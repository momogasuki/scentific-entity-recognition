In	O
NLP	O
,	O
reproduction	B-TaskName
studies	O
generally	O
address	O
the	O
following	O
question	O
:	O
if	O
we	O
create	O
and/or	O
evaluate	O
this	O
system	O
multiple	O
times	O
,	O
will	O
we	O
obtain	O
the	O
same	O
results	O
?	O

Recently	O
,	O
transformer	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O
2017)-based	O
language	O
models	O
have	O
been	O
successfully	O
applied	O
in	O
the	O
field	O
of	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
.	O

(	O
4	O
)	O
In	O
order	O
to	O
maximize	O
syntactic	O
and	O
lexical	O
diversity	O
of	O
the	O
pairs	O
of	O
paraphrased	O
sentences	O
,	O
we	O
perform	O
an	O
analysis	O
based	O
on	O
word	O
overlap	O
between	O
the	O
semantically	O
similar	O
pair	O
sentences	O
(	O
i.e.	O
,	O
the	O
output	O
of	O
the	O
previous	O
step	O
)	O
.	O

We	O
demonstrate	O
these	O
gains	O
over	O
three	O
datasets	O
:	O
YELP	B-MethodName
(	O
Zhao	O
et	O
al	O
.	O
,	O
2018b	O
)	O
,	O
IMDB	B-DatasetName
(	O
Dai	O
et	O
al	O
.	O
,	O
2019	O
)	O
and	O
POLITICAL	B-DatasetName
(	O
Prabhumoye	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
generating	O
six	O
constraints	O
including	O
lexical	O
,	O
syntactic	O
and	O
domainspecific	O
constraints	O
.	O

For	O
example	O
,	O
sentences	O
of	O
similar	O
lengths	O
(	O
irrespective	O
of	O
their	O
domains	O
)	O
should	O
be	O
closer	O
together	O
.	O

We	O
further	O
use	O
the	O
bounding	O
box	O
averaged	O
over	O
the	O
whole	O
sequence	O
to	O
crop	O
the	O
ROI	B-MetricName
area	O
,	O
which	O
roughly	O
denotes	O
the	O
signing	O
region	O
of	O
a	O
signer	O
.	O

Following	O
the	O
previous	O
work	O
(	O
Zhou	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
we	O
call	O
this	O
setting	O
under	O
weak	O
supervision	O
.	O

6.1	O
)	O
,	O
the	O
action	O
scorer	O
"	O
forgets	O
"	O
the	O
experience	O
/	O
skills	O
dealing	O
with	O
simple	O
games	O
and	O
the	O
model	O
fails	O
to	O
generalize	O
on	O
unseen	O
simple	O
games	O
.	O

In	O
order	O
to	O
build	O
our	O
paraphrasing	O
benchmark	O
component	O
(	O
ARGEN	B-DatasetName
PPH	I-DatasetName
)	O
,	O
we	O
use	O
the	O
following	O
three	O
datasets	O
:	O
AraPara	B-DatasetName
.	O

Among	O
encoder	O
-	O
decoder	O
models	O
,	O
BART	B-MethodName
conducts	O
NLU	B-TaskName
tasks	O
by	O
feeding	O
the	O
same	O
input	O
into	O
the	O
encoder	O
and	O
decoder	O
,	O
and	O
taking	O
the	O
final	O
hidden	O
states	O
of	O
the	O
decoder	O
.	O

Following	O
TAT	B-DatasetName
-	I-DatasetName
QA	I-DatasetName
,	O
the	O
hypothetical	O
questions	O
are	O
also	O
labeled	O
with	O
four	O
answer	O
types	O
:	O
arithmetic	O
,	O
span	O
,	O
count	O
,	O
and	O
multi	O
-	O
span	O
,	O
three	O
types	O
of	O
answer	O
sources	O
:	O
table	O
,	O
text	O
and	O
table	O
-	O
text	O
,	O
and	O
a	O
derivation	O
on	O
how	O
the	O
answer	O
is	O
derived	O
from	O
the	O
context	O
.	O

great	O
food	O
.	O

We	O
argue	O
that	O
this	O
update	O
process	O
effectively	O
learns	O
the	O
high	O
-	O
order	O
semantics	O
inherent	O
in	O
each	O
hypergraph	O
and	O
the	O
high	O
-	O
order	O
associations	O
between	O
two	O
hypergraphs	O
.	O

Note	O
that	O
if	O
a	O
word	O
is	O
divided	O
into	O
several	O
subwords	O
after	O
tokenization	O
,	O
then	O
only	O
the	O
first	O
subword	O
is	O
considered	O
in	O
the	O
loss	O
function	O
.	O

We	O
train	O
our	O
model	O
on	O
5	B-HyperparameterValue
%	I-HyperparameterValue
,	O
10	B-HyperparameterValue
%	I-HyperparameterValue
,	O
and	O
20	B-HyperparameterValue
%	I-HyperparameterValue
of	O
the	O
training	O
data	O
and	O
compared	O
with	O
other	O
baselines	O
on	O
end	O
-	O
to	O
-	O
end	O
dialogue	O
task	O
,	O
Table	O
2	O
list	O
the	O
results	O
.	O

For	O
instance	O
,	O
TAPT	B-MethodName
required	O
a	O
total	O
of	O
seven	O
hours	O
of	O
training	O
,	O
while	O
DoKTRa	B-MethodName
was	O
completed	O
in	O
only	O
1.1	O
hours	O
for	O
the	O
ChemProt	B-DatasetName
task	O
.	O

ii	O
)	O
Learning	O
a	O
complex	O
reasoning	O
process	O
is	O
difficult	O
especially	O
in	O
a	O
condition	O
where	O
only	O
QA	B-TaskName
is	O
provided	O
without	O
extra	O
supervision	O
on	O
how	O
to	O
capture	O
any	O
evidence	O
from	O
the	O
KB	O
and	O
infer	O
based	O
on	O
them	O
.	O

The	O
warmup	B-HyperparameterName
step	O
we	O
use	O
is	O
4000	B-HyperparameterValue
.	O

Following	O
Geng	O
et	O
al	O
.	O
(	O
2019	O
)	O
,	O
we	O
select	O
12	O
tasks	O
from	O
4	O
domains	O
(	O
Books	O
,	O
DVD	O
,	O
Electronics	O
,	O
Kitchen	O
)	O
for	O
meta	O
-	O
testing	O
tasks	O
,	O
and	O
the	O
support	O
sets	O
of	O
these	O
tasks	O
are	O
fixed	O
(	O
Yu	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

,	O
pre	O
-	O
update	O
)	O
θ	O
according	O
to	O
the	O
performance	O
of	O
f	O
θ	O
i	O
on	O
the	O
query	O
set	O
D	O
q	O
i	O
as	O
in	O
Eq.1	O
:	O

,	O
y	O
|Y	O
|	O
)	O
,	O
we	O
estimate	O
the	O
following	O
conditional	O
probability	O
:	O

The	O
cloze	O
questions	O
and	O
other	O
details	O
can	O
be	O
found	O
in	O
Appendix	O
B.1	O
.	O

All	O
our	O
models	O
are	O
trained	O
on	O
8	O
NVIDIA	O
V100	O
GPUs	O
.	O

Yin	O
et	O
al	O
.	O
(	O
2020	O
)	O
build	O
an	O
information	O
bottleneck	O
to	O
the	O
model	O
,	O
while	O
this	O
approach	O
decreases	O
the	O
model	O
performance	O
with	O
this	O
passive	O
regularization	O
.	O

For	O
all	O
models	O
and	O
baselines	O
,	O
across	O
all	O
tasks	O
,	O
we	O
identify	O
the	O
best	O
model	O
on	O
the	O
respective	O
Dev	O
data	O
and	O
blind	O
-	O
test	O
it	O
on	O
Test	O
data	O
.	O

There	O
's	O
still	O
space	O
to	O
improve	O
the	O
pre	O
-	O
trained	O
modules	O
.	O

The	O
weight	O
for	O
the	O
contrastive	B-HyperparameterValue
loss	I-HyperparameterValue
is	O
0.1.Overall	B-HyperparameterValue
performance	O
.	O

Another	O
observation	O
is	O
that	O
the	O
cloze	O
formulation	O
is	O
critical	O
for	O
GLM	B-MethodName
's	O
performance	O
on	O
NLU	B-TaskName
tasks	O
.	O

Quality	O
measures	O
coherence	O
,	O
fluency	O
,	O
and	O
informativeness	O
.	O

The	O
non	O
-	O
Arabic	O
text	O
is	O
sometimes	O
foreign	O
language	O
advertising	O
or	O
even	O
full	O
translation	O
of	O
the	O
Arabic	O
text	O
in	O
some	O
cases	O
.	O

We	O
also	O
link	O
to	O
individual	O
ARGEN	B-DatasetName
datasets	O
through	O
our	O
public	O
repository	O
.	O

In	O
order	O
to	O
better	O
leverage	O
the	O
retrieved	O
memory	O
and	O
enhance	O
the	O
dependence	O
of	O
our	O
model	O
on	O
support	O
sets	O
,	O
we	O
propose	O
an	O
imitation	O
module	O
to	O
encourage	O
the	O
imitation	O
of	O
support	O
sets	O
behaviors	O
when	O
making	O
predictions	O
on	O
query	O
sets	O
.	O

Different	O
from	O
their	O
work	O
,	O
we	O
pre	O
-	O
train	O
language	O
models	O
with	O
blank	O
infilling	O
objectives	O
and	O
evaluate	O
their	O
performance	O
in	O
downstream	O
NLU	B-TaskName
and	O
generation	B-TaskName
tasks	O
.	O

9	O
For	O
all	O
these	O
datasets	O
,	O
we	O
use	O
the	O
same	O
splits	O
as	O
Sajjad	O
et	O
al	O
.	O
(	O
2020	O
)	O
in	O
our	O
experiments	O
.	O

The	O
first	O
positional	O
i	O
d	O
represents	O
the	O
position	O
in	O
the	O
corrupted	O
text	O
x	O
corrupt	O
.	O

where	O
the	O
subscript	O
i	O
denotes	O
the	O
i	O
-	O
th	O
index	O
of	O
column	O
vectors	O
in	O
each	O
matrix	O
.	O

it	O
was	O
very	O
dry	O
.	O

We	O
can	O
obtain	O
this	O
module	O
through	O
supervised	O
pre	O
-	O
training	O
,	O
and	O
decouple	O
it	O
from	O
reinforcement	O
learning	O
to	O
yield	O
better	O
sample	O
efficiency	O
.	O

In	O
order	O
to	O
decouple	O
language	O
learning	O
from	O
decision	O
making	O
,	O
which	O
further	O
improves	O
the	O
sample	O
efficiency	O
,	O
we	O
propose	O
to	O
acquire	O
the	O
world	O
-	O
perceiving	O
modules	O
through	O
supervised	O
pre	O
-	O
training	O
.	O

However	O
,	O
prior	O
methods	O
have	O
been	O
evaluated	O
under	O
a	O
disparate	O
set	O
of	O
protocols	O
,	O
which	O
hinders	O
fair	O
comparison	O
and	O
measuring	O
progress	O
of	O
the	O
field	O
.	O

To	O
demonstrate	O
the	O
effectiveness	O
of	O
our	O
approach	O
,	O
we	O
designed	O
the	O
following	O
ablation	O
studies	O
.	O

The	O
first	O
is	O
a	O
contrastive	B-HyperparameterValue
loss	I-HyperparameterValue
and	O
the	O
second	O
is	O
a	O
classification	B-HyperparameterValue
loss	I-HyperparameterValue
-aiming	O
to	O
regularize	O
the	O
latent	O
space	O
further	O
and	O
bring	O
similar	O
sentences	O
across	O
domains	O
closer	O
together	O
.	O

We	O
set	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
neighbors	I-HyperparameterName
N	I-HyperparameterName
=	O
10	B-HyperparameterValue
and	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
local	I-HyperparameterName
adaptation	I-HyperparameterName
steps	I-HyperparameterName
L	B-HyperparameterName
=	O
20	B-HyperparameterValue
.	O

Then	O
,	O
MemIML	B-MethodName
conducts	O
the	O
global	O
optimization	O
of	O
the	O
value	O
predictor	O
over	O
these	O
key	O
-	O
value	O
pairs	O
.	O

ARAE	B-MethodName
regularizes	O
this	O
latent	O
space	O
utilizing	O
a	O
GAN	O
-	O
like	O
setup	O
that	O
includes	O
an	O
implicit	O
prior	O
obtained	O
from	O
a	O
parameterized	O
generator	O
network	O
enc	O
ψ	O
:	O
N	O
(	O
0	O
,	O
I	O
)	O
→	O
Z.	O
Here	O
,	O
enc	O
ψ	O
maps	O
a	O
noise	O
sample	O
s	O
∼	O
N	O
(	O
0	O
,	O
I	O
)	O
to	O
the	O
corresponding	O
prior	O
latent	O
codez	O
=	O
enc	O
ψ	O
(	O
s	O
)	O
∼	O
Pz	O
.	O

The	O
world	O
-	O
perceiving	O
modules	O
,	O
which	O
are	O
pre	O
-	O
trained	O
with	O
simple	O
games	O
,	O
help	O
to	O
train	O
a	O
decision	O
module	O
that	O
adapts	O
well	O
on	O
unseen	O
games	O
.	O

From	O
the	O
results	O
,	O
we	O
find	O
that	O
the	O
attention	O
mechanism	O
between	O
question	O
and	O
knowledge	O
is	O
crucial	O
for	O
complex	B-TaskName
QA	I-TaskName
.	O

Note	O
that	O
we	O
only	O
change	O
the	O
teacher	O
's	O
attention	B-HyperparameterName
temperature	I-HyperparameterName
during	O
inference	O
time	O
.	O

One	O
straightforward	O
solution	O
is	O
to	O
model	O
counterfactual	O
thinking	O
as	O
a	O
generation	O
procedure	O
with	O
the	O
fact	O
and	O
assumption	O
as	O
inputs	O
by	O
using	O
a	O
generation	O
model	O
such	O
as	O
GPT	B-MethodName
(	O
Brown	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

Moreover	O
,	O
T5	B-MethodName
always	O
predicts	O
spans	O
in	O
a	O
fixed	O
left	O
-	O
to	O
-	O
right	O
order	O
.	O

A	O
game	O
is	O
considered	O
simple	O
,	O
if	O
it	O
consists	O
of	O
only	O
a	O
few	O
subtasks	O
,	O
and	O
complex	O
if	O
it	O
consists	O
of	O
more	O
subtasks	O
.	O

Knowledge	O
distillation	O
is	O
a	O
class	O
of	O
methods	O
that	O
leverage	O
the	O
output	O
of	O
a	O
(	O
large	O
)	O
teacher	O
model	O
to	O
guide	O
the	O
training	O
of	O
a	O
(	O
small	O
)	O
student	O
model	O
.	O

On	O
the	O
other	O
hand	O
,	O
MemNN	B-MethodName
†	I-MethodName
,	O
HAN	B-MethodName
(	O
Kim	O
et	O
al	O
.	O
,	O
2020	O
)	O
,	O
and	O
BAN	B-MethodName
(	O
Kim	O
et	O
al	O
.	O
,	O
2018	O
)	O
achieve	O
comparatively	O
high	O
performance	O
because	O
MemNN	B-MethodName
†	I-MethodName
adopts	O
question	O
-	O
guided	O
soft	O
attention	O
over	O
knowledge	O
memories	O
.	O

•	O
Deriving	O
head	O
.	O

Teacher	O
/	O
Student	O
model	O
settings	O
We	O
use	O
BART	B-MethodName
Large	I-MethodName
(	O
Lewis	O
et	O
al	O
.	O
,	O
2020	O
)	O
as	O
our	O
teacher	O
model	O
,	O
which	O
has	O
12	B-HyperparameterValue
layers	O
in	O
the	O
encoder	O
and	O
decoder	O
.	O

We	O
address	O
aforementioned	O
shortcomings	O
with	O
following	O
key	O
contributions	O
:	O

Notice	O
that	O
the	O
interpolation	O
not	O
only	O
works	O
on	O
the	O
prediction	O
output	O
but	O
also	O
guides	O
the	O
training	O
via	O
gradient	O
descent	O
based	O
on	O
the	O
interpolated	O
output	O
.	O

1	O
(	O
b	O
)	O
shows	O
an	O
example	O
of	O
our	O
decision	O
making	O
process	O
.	O

Smith	O
played	O
for	O
the	O
Michigan	O
Wolverines	O
football	O
team	O
from	O
1959	O
to	O
1963	O
.	O

Apart	O
from	O
having	O
different	O
topics	O
,	O
the	O
IMDB	B-DatasetName
dataset	O
is	O
more	O
formal	O
compared	O
to	O
the	O
more	O
colloquial	O
YELP	O
.	O

We	O
expect	O
that	O
the	O
performance	O
will	O
be	O
improved	O
when	O
the	O
entity	O
linking	O
module	O
is	O
enhanced	O
.	O

We	O
consider	O
a	O
two	O
-	O
phase	O
training	O
strategy	O
to	O
decouple	O
these	O
two	O
regimes	O
to	O
further	O
improve	O
the	O
sample	O
efficiency	O
(	O
Hill	O
et	O
al	O
.	O
,	O
2021	O
)	O
.	O

Several	O
works	O
have	O
been	O
proposed	O
to	O
tackle	O
the	O
memorization	O
overfitting	O
issue	O
for	O
regression	B-TaskName
and	O
image	B-TaskName
classification	I-TaskName
tasks	O
.	O

Most	O
notably	O
,	O
the	O
governor	O
is	O
president	O
of	O
the	O
senate	O
and	O
governor	O
.	O

Fig	O
.	O

i	O
)	O
We	O
propose	O
Hypergraph	B-MethodName
Transformer	I-MethodName
which	O
enhances	O
multi	O
-	O
hop	O
reasoning	O
ability	O
by	O
encoding	O
high	O
-	O
order	O
semantics	O
in	O
the	O
form	O
of	O
a	O
hypergraph	O
and	O
learning	O
inter	O
-	O
and	O
intrahigh	O
-	O
order	O
associations	O
in	O
hypergraphs	O
using	O
the	O
attention	O
mechanism	O
.	O

In	O
this	O
work	O
,	O
we	O
extend	O
NDR	B-TaskName
to	O
hypothetical	B-TaskName
question	B-TaskName
answering	I-TaskName
(	O
HQA	B-TaskName
)	O
,	O
where	O
the	O
question	O
consists	O
of	O
an	O
assumption	O
beyond	O
the	O
context	O
(	O
Figure	O
1	O
)	O
.	O

In	O
1962	O
,	O
he	O
set	O
the	O
Wolverines	O
'	O
all	O
-	O
time	O
interception	O
record	O
with	O
13	O
,	O
and	O
was	O
second	O
overall	O
in	O
the	O
1962	O
season	O
's	O
Heisman	O
Trophy	O
voting	O
.	O

As	O
pointed	O
out	O
by	O
(	O
Yang	O
et	O
al	O
.	O
,	O
2019	O
)	O
,	O
BERT	B-MethodName
fails	O
to	O
capture	O
the	O
interdependencies	O
of	O
masked	O
tokens	O
due	O
to	O
the	O
independence	O
assumption	O
of	O
MLM	O
.	O

The	O
output	O
of	O
this	O
optimized	O
using	O
binary	B-HyperparameterValue
crossentopy	I-HyperparameterValue
loss	I-HyperparameterValue
described	O
in	O
Eqn:4	O
.	O

Table	O
6	O
shows	O
our	O
ablation	O
analysis	O
for	O
GLM	B-MethodName
.	O

Notably	O
,	O
our	O
approach	O
even	O
outperformed	O
RoBERTa	B-MethodName
-	I-MethodName
PM	I-MethodName
on	O
two	O
tasks	O
and	O
demonstrated	O
comparable	O
performances	O
on	O
the	O
others	O
.	O

Xent	B-HyperparameterValue
loss	I-HyperparameterValue
,	O
we	O
use	O
one	O
that	O
is	O
amenable	O
to	O
multiple	O
positive	O
instances	O
(	O
Khosla	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

We	O
therefore	O
,	O
introduce	O
a	O
memory	O
module	O
and	O
an	O
imitation	O
module	O
to	O
enhance	O
such	O
dependence	O
.	O

(	O
2	O
)	O
MTMT	B-MethodName
w/o	I-MethodName
weighting	I-MethodName
,	O
which	O
set	O
the	O
α	B-HyperparameterName
(	O
•	O
)	O
,	O
β	B-HyperparameterName
and	O
γ	B-HyperparameterName
all	O
to	O
be	O
1	O
in	O
the	O
loss	O
of	O
student	O
learning	O
.	O

Use	O
of	O
under	O
-	O
specified	O
reward	O
will	O
often	O
lead	O
to	O
policy	O
that	O
suffers	O
from	O
high	O
variance	O
(	O
Agarwal	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

The	O
margin	B-HyperparameterName
µ	I-HyperparameterName
of	O
the	O
activation	B-HyperparameterName
transfer	I-HyperparameterName
loss	I-HyperparameterName
was	O
set	O
to	O
1.0	B-HyperparameterValue
.	O

We	O
fine	O
-	O
tune	O
our	O
three	O
models	O
and	O
22	B-MethodName
MARBERT	I-MethodName
outperform	O
both	O
multilingual	O
encoder	O
-	O
only	O
Transformers	O
mBERT	B-MethodName
,	O
XLM	B-MethodName
-	I-MethodName
RBase	I-MethodName
,	O
XLM	B-MethodName
-	I-MethodName
RLarge	I-MethodName
,	O
and	O
Arabicspecific	O
BERT	B-MethodName
-	I-MethodName
based	I-MethodName
AraBERT	B-MethodName
(	O
Antoun	O
et	O
al	O
.	O
,	O
2020	O
)	O
,	O
AR	B-MethodName
-	I-MethodName
BERT	I-MethodName
(	O
Abdul	O
-	O
Mageed	O
et	O
al	O
.	O
,	O
2021	O
)	O
.	O

Our	O
framework	O
is	O
consist	O
of	O
two	O
models	O
:	O
teacher	O
training	O
model	O
learned	O
from	O
the	O
source	O
language	O
and	O
teacher	O
-	O
student	O
distillation	O
learning	O
model	O
learned	O
from	O
the	O
target	O
language	O
.	O

We	O
highlight	O
that	O
PQL	B-DatasetName
is	O
more	O
challenging	O
dataset	O
than	O
PQ	B-DatasetName
in	O
that	O
PQL	B-DatasetName
not	O
only	O
covers	O
more	O
knowledge	O
facts	O
but	O
also	O
has	O
fewer	O
QA	O
instances	O
.	O

UniLM	B-MethodName
combines	O
different	O
pretraining	O
objectives	O
under	O
the	O
autoencoding	O
framework	O
by	O
changing	O
the	O
attention	O
mask	O
among	O
bidirectional	O
,	O
unidirectional	O
,	O
and	O
cross	O
attention	O
.	O

For	O
ACC	B-MetricName
,	O
1	O
indicates	O
that	O
the	O
target	O
sentence	O
has	O
only	O
the	O
source	O
sentence	O
style	O
while	O
2	O
indicates	O
good	O
transfer	O
to	O
the	O
target	O
style	O
.	O

Under	O
the	O
wyoming	O
state	O
constitution	O
,	O
the	O
governor	O
can	O
veto	O
the	O
actions	O
of	O
the	O
other	O
members	O
of	O
the	O
wyoming	O
house	O
of	O
representatives	O
.	O

For	O
example	O
,	O
in	O
the	O
biomedical	O
domain	O
,	O
several	O
domainspecific	O
PLMs	O
trained	O
with	O
large	O
biomedical	O
texts	O
,	O
such	O
as	O
BioBERT	B-MethodName
,	O
PubMedBERT	B-MethodName
(	O
Gu	O
et	O
al	O
.	O
,	O
2020	O
)	O
and	O
BlueBERT	B-MethodName
(	O
Peng	O
et	O
al	O
.	O
,	O
2019	O
)	O
,	O
have	O
been	O
successfully	O
used	O
as	O
strong	O
baselines	O
for	O
several	O
downstream	O
tasks	O
.	O

GLM	B-MethodName
differs	O
in	O
three	O
aspects	O
:	O

Example	O
D.2	O
.	O
Jonathan	O
Terry	O
is	O
a	O
television	O
and	O
film	O
actor	O
.	O

We	O
present	O
results	O
of	O
students	O
trained	O
with	O
gold	O
labels	O
(	O
Gold	O
)	O
and	O
regular	O
pseudo	O
labels	O
(	O
Regular	O
)	O
as	O
well	O
as	O
pseudo	O
labels	O
with	O
higher	O
and	O
random	O
attention	B-HyperparameterName
temperatures	I-HyperparameterName
(	O
PLATE	B-MethodName
B12	I-MethodName
-	I-MethodName
3	I-MethodName
λ=1.5	I-MethodName
,	O
PLATE	B-MethodName
B12	I-MethodName
-	I-MethodName
3	I-MethodName
λ=2.0	I-MethodName
and	O
PLATE	B-MethodName
B12	I-MethodName
-	I-MethodName
3	I-MethodName
rnd	I-MethodName
)	O
.	O

62106275).Proof	O
of	O
inequality	O
in	O
Eqn	O
.	O

most	O
standard	O
toppings	O
cost	O
extra	O
too	O
.	O

AR	B-DatasetName
-	I-DatasetName
LUE	I-DatasetName
score	O
is	O
a	O
simply	O
macro	O
-	O
average	O
of	O
the	O
different	O
scores	O
across	O
all	O
task	O
clusters	O
,	O
where	O
each	O
task	O
is	O
weighted	O
equally	O
following	O
(	O
Wang	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

In	O
each	O
case	O
,	O
the	O
samples	O
are	O
high	O
-	O
quality	O
,	O
informative	O
,	O
and	O
fluent	O
.	O

For	O
multi	O
-	O
head	O
attention	O
,	O
the	O
attended	O
outputs	O
with	O
different	O
heads	O
are	O
concatenated	O
and	O
fed	O
into	O
a	O
single	O
layer	O
feedforward	O
layer	O
to	O
make	O
a	O
final	O
representation	O
.	O

However	O
,	O
there	O
is	O
a	O
dilemma	O
to	O
adapt	O
the	O
siamese	O
network	O
to	O
tokenlevel	B-TaskName
recognition	I-TaskName
tasks	O
such	O
as	O
NER	B-TaskName
.	O

Few	O
-	O
shot	O
learning	O
for	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
(	O
NLU	B-TaskName
)	O
has	O
been	O
significantly	O
advanced	O
by	O
pretrained	O
language	O
models	O
(	O
PLMs	O
;	O
Brown	O
et	O
al	O
.	O
,	O
2020;Schick	O
and	O
Schütze	O
,	O
2021a	O
,	O
b	O
)	O
.	O

While	O
we	O
focus	O
on	O
simple	O
constraints	O
at	O
the	O
sentence	O
-	O
and	O
word	O
-	O
level	O
,	O
future	O
work	O
can	O
add	O
phrase	O
-	O
level	O
and	O
more	O
fine	O
-	O
grained	O
constraints	O
.	O

Instead	O
,	O
we	O
reformulate	O
NLU	B-TaskName
classification	I-TaskName
tasks	O
as	O
generation	O
tasks	O
of	O
blank	B-TaskName
infilling	I-TaskName
,	O
following	O
PET	O
(	O
Schick	O
and	O
Schütze	O
,	O
2020a	O
)	O
.	O

This	O
gives	O
us	O
our	O
seventh	O
ARGEN	B-DatasetName
MT	I-DatasetName
dataset	O
,	O
which	O
we	O
call	O
(	O
7	O
)	O
OPUS	B-DatasetName
-	I-DatasetName
X	I-DatasetName
-	I-DatasetName
Ara	I-DatasetName
.	O

If	O
the	O
sentence	O
itself	O
has	O
no	O
sentiment	O
then	O
chose	O
2	O
Political	O
Orientation	O
1	O
-Talks	O
about	O
topics	O
with	O
the	O
other	O
orientation	O
.	O

Following	O
the	O
standard	O
structure	O
of	O
the	O
transformer	O
,	O
we	O
build	O
up	O
guided	O
-	O
attention	O
block	O
and	O
selfattention	O
block	O
where	O
each	O
block	O
consists	O
of	O
each	O
attention	O
operation	O
with	O
layer	B-HyperparameterName
normalization	I-HyperparameterName
,	O
residual	B-HyperparameterName
connection	I-HyperparameterName
,	O
and	O
a	O
single	O
feed	B-HyperparameterName
-	I-HyperparameterName
forward	I-HyperparameterName
layer	I-HyperparameterName
.	O

Empirically	O
,	O
we	O
have	O
found	O
that	O
the	O
15	B-HyperparameterValue
%	I-HyperparameterValue
ratio	O
is	O
critical	O
for	O
good	O
performance	O
on	O
downstream	O
NLU	B-TaskName
tasks	O
.	O

The	O
semantic	O
labels	O
of	O
visual	O
concepts	O
or	O
named	O
entities	O
are	O
then	O
linked	O
with	O
knowledge	O
entities	O
in	O
the	O
knowledge	O
base	O
using	O
exact	O
keyword	O
matching	O
.	O

Note	O
that	O
we	O
used	O
the	O
RoBERTa	B-MethodName
-	I-MethodName
base	I-MethodName
model	O
in	O
this	O
section	O
because	O
of	O
the	O
training	O
stability	O
.	O

We	O
first	O
let	O
the	O
player	O
to	O
go	O
through	O
each	O
simple	O
game	O
,	O
then	O
construct	O
the	O
datasets	O
upon	O
the	O
interaction	O
data	O
.	O

In	O
this	O
paper	O
,	O
the	O
young	O
investor	O
stands	O
for	O
a	O
standard	O
meta	O
-	O
learning	O
algorithm	O
(	O
e.g.	O
,	O
MAML	B-MethodName
)	O
,	O
which	O
is	O
prone	O
to	O
memorization	O
overfitting	O
,	O
and	O
the	O
old	O
investor	O
is	O
a	O
memory	O
module	O
we	O
integrate	O
into	O
the	O
method	O
,	O
carrying	O
information	O
of	O
support	O
sets	O
.	O

To	O
ablate	O
the	O
calibrated	O
teacher	O
training	O
,	O
we	O
trained	O
the	O
teacher	O
model	O
using	O
only	O
L	O
CE	O
.	O

We	O
check	O
the	O
increase	O
of	O
mutual	B-MetricName
information	I-MetricName
between	O
predictions	O
of	O
query	O
sets	O
with	O
the	O
provided	O
support	O
-	O
set	O
information	O
after	O
augmented	O
with	O
the	O
memory	O
information	O
M.	O

Compared	O
with	O
the	O
textbased	O
agents	O
(	O
Narasimhan	O
et	O
al	O
.	O
,	O
2015;Adolphs	O
and	O
Hofmann	O
,	O
2020;Jain	O
et	O
al	O
.	O
,	O
2020;Yin	O
and	O
May	O
,	O
2019;Xu	O
et	O
al	O
.	O
,	O
2020a;Guo	O
et	O
al	O
.	O
,	O
2020	O
)	O
,	O
which	O
take	O
the	O
raw	O
textual	O
observations	O
as	O
input	O
to	O
build	O
state	O
representations	O
,	O
the	O
KGbased	O
agents	O
construct	O
the	O
knowledge	O
graph	O
and	O
leverage	O
it	O
as	O
the	O
additional	O
input	O
(	O
Ammanabrolu	O
and	O
Riedl	O
,	O
2019;Xu	O
et	O
al	O
.	O
,	O
2020b	O
)	O
.	O

Given	O
the	O
observation	O
o	O
t	O
and	O
the	O
task	O
candidate	O
set	O
T	O
,	O
we	O
use	O
the	O
task	O
selector	O
to	O
first	O
obtain	O
a	O
subset	O
of	O
currently	O
available	O
subtasks	O
T	O
t	O
⊆	O
T	O
,	O
then	O
select	O
a	O
subtask	O
T	O
t	O
∈	O
T	O
t	O
.	O

In	O
addition	O
,	O
systematic	O
issues	O
have	O
been	O
discovered	O
in	O
multilingual	O
corpora	O
on	O
which	O
language	O
models	O
have	O
been	O
trained	O
(	O
Kreutzer	O
et	O
al	O
.	O
,	O
2021	O
)	O
.	O

In	O
the	O
previous	O
section	O
,	O
GLM	B-MethodName
masks	O
short	O
spans	O
and	O
is	O
suited	O
for	O
NLU	B-TaskName
tasks	O
.	O

The	O
choice	O
-	O
based	O
agents	O
circumvent	O
this	O
challenge	O
by	O
assuming	O
the	O
access	O
to	O
a	O
set	O
of	O
admissible	O
actions	O
at	O
each	O
game	O
state	O
(	O
He	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

(	O
Narasimhan	O
and	O
Schwing	O
,	O
2018	O
;	O
proposed	O
memory	O
-	O
based	O
methods	O
that	O
represent	O
knowledge	O
facts	O
in	O
the	O
form	O
of	O
memory	O
and	O
calculate	O
soft	O
attention	O
scores	O
of	O
the	O
memory	O
with	O
a	O
question	O
.	O

Nevertheless	O
,	O
additional	O
pre	O
-	O
training	O
has	O
several	O
limitations	O
,	O
such	O
as	O
the	O
need	O
for	O
sufficient	O
training	O
data	O
and	O
resources	O
,	O
and	O
a	O
longer	O
training	O
time	O
.	O

Thus	O
,	O
Hypergraph	B-MethodName
Transformer	I-MethodName
can	O
mitigate	O
the	O
well	O
-	O
known	O
over	O
-	O
smoothing	O
problem	O
in	O
the	O
previous	O
graph	O
-	O
based	O
methods	O
exploiting	O
the	O
message	O
passing	O
scheme	O
.	O

We	O
compute	O
the	O
ARLUE	B-DatasetName
score	O
(	O
i.e.	O
,	O
overall	O
macro	O
-	O
average	O
)	O
for	O
each	O
of	O
our	O
three	O
models	O
(	O
i.e.	O
,	O
AraT5	B-MethodName
MSA	I-MethodName
,	O
AraT5	B-MethodName
Tw	I-MethodName
,	O
and	O
AraT5	B-MethodName
)	O
and	O
the	O
baseline	O
(	O
mT5	B-MethodName
)	O
.	O

good	O
sake	O
is	O
ready	O
.	O

Note	O
that	O
we	O
use	O
Q	O
,	O
K	O
,	O
and	O
V	O
for	O
query	O
,	O
key	O
,	O
value	O
,	O
and	O
q	O
,	O
k	O
as	O
subscripts	O
to	O
represent	O
question	O
and	O
knowledge	O
,	O
respectively	O
.	O

We	O
use	O
teacher	O
forcing	O
and	O
consider	O
the	O
prediction	O
correct	O
only	O
when	O
all	O
the	O
predicted	O
tokens	O
are	O
correct	O
.	O

We	O
consider	O
the	O
following	O
four	O
models	O
,	O
and	O
compare	O
with	O
more	O
variants	O
in	O
ablation	O
studies	O
:	O
KG	O
-	O
based	O
RL	O
agent	O
,	O
which	O
is	O
the	O
benchmark	O
model	O
for	O
cooking	B-TaskName
games	I-TaskName
.	O

We	O
report	O
the	O
average	O
accuracy	B-MetricName
of	O
five	O
repeated	O
runs	O
on	O
different	O
data	O
split	O
:	O
76.55	B-HyperparameterValue
as	O
top-1	O
accuracy	B-MetricName
(	O
average	O
of	O
76.93	B-HyperparameterValue
,	O
75.92	B-HyperparameterValue
,	O
76.24	B-HyperparameterValue
,	O
76.16	B-HyperparameterValue
,	O
and	O
77.50	B-HyperparameterValue
)	O
and	O
82.20	B-MetricValue
as	O
top-3	O
accuracy	B-MetricName
(	O
average	O
of	O
82.90	B-HyperparameterValue
,	O
81.45	B-HyperparameterValue
,	O
81.70	B-HyperparameterValue
,	O
81.74	B-HyperparameterValue
and	O
83.20	B-MetricValue
)	O
.	O

Here	O
,	O
we	O
highlight	O
four	O
aspects	O
as	O
follows	O
:	O
1	O
)	O
KVQA	B-DatasetName
dataset	O
covers	O
the	O
large	O
number	O
of	O
entities	O
(	O
at	O
least	O
5	O
times	O
more	O
)	O
and	O
knowledge	O
facts	O
(	O
at	O
least	O
17	O
times	O
more	O
)	O
than	O
FVQA	B-DatasetName
,	O
PQ	B-DatasetName
and	O
PQL	B-DatasetName
.	O

We	O
further	O
investigate	O
the	O
generalization	O
performance	O
of	O
our	O
model	O
on	O
simple	O
games	O
,	O
considering	O
that	O
simple	O
games	O
are	O
not	O
engaged	O
in	O
our	O
RL	O
training	O
.	O

Disentanglement	O
approaches	O
are	O
the	O
prevalent	O
approach	O
to	O
tackle	O
unsupervised	O
attribute	B-TaskName
transfer	I-TaskName
:	O
attributes	O
and	O
content	O
are	O
separated	O
in	O
latent	O
dimension	O
.	O

Memory	O
Writing	O
constructs	O
the	O
memory	O
using	O
the	O
information	O
of	O
samples	O
in	O
the	O
support	O
set	O
D	O
s	O
i	O
.	O

Experimental	O
Setup	O
.	O

The	O
value	O
of	O
Equation	O
4	B-HyperparameterValue
directly	O
refers	O
to	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
neurons	I-HyperparameterName
activated	O
differently	O
than	O
the	O
teacher	O
model	O
.	O

In	O
particular	O
,	O
we	O
use	O
the	O
version	O
GATA	B-MethodName
-	I-MethodName
GTF	I-MethodName
,	O
which	O
takes	O
only	O
the	O
KG	O
-	O
based	O
observation	O
,	O
and	O
denote	O
it	O
as	O
GATA	B-MethodName
for	O
simplicity	O
.	O

These	O
results	O
are	O
promising	O
,	O
and	O
hence	O
we	O
plan	O
to	O
further	O
investigate	O
multi	O
-	O
task	O
learning	O
with	O
our	O
new	O
models	O
in	O
the	O
future	O
.	O

Summaries	O
generated	O
by	O
abstractive	O
models	O
may	O
be	O
ungrammatical	O
or	O
unfaithful	O
to	O
the	O
original	O
document	O
.	O

To	O
demonstrate	O
the	O
hypothesis	O
,	O
we	O
test	O
our	O
method	O
against	O
baseline	O
in	O
a	O
low	O
sample	O
complexity	O
regime	O
.	O

Perplexity	B-HyperparameterValue
is	O
the	O
exponentiation	O
of	O
the	O
average	O
cross	B-HyperparameterName
entropy	I-HyperparameterValue
of	O
a	O
corpus	O
.	O

In	O
cross	B-TaskName
-	I-TaskName
lingual	I-TaskName
NER	I-TaskName
,	O
the	O
training	O
set	O
without	O
entity	O
label	O
of	O
the	O
target	O
language	O
is	O
also	O
available	O
when	O
training	O
the	O
model	O
.	O

This	O
reveals	O
the	O
potential	O
limitations	O
of	O
our	O
method	O
and	O
training	O
using	O
transformers	O
is	O
a	O
future	O
work	O
.	O

The	O
output	O
representation	O
of	O
i	O
-	O
th	O
layer	O
is	O
O	O
i	O
=	O
j	O
p	O
ij	O
o	O
ij	O
where	O
o	O
is	O
the	O
another	O
embeddings	O
of	O
knowledge	O
facts	O
different	O
from	O
m.	O
The	O
updated	O
question	O
representation	O
is	O
q	O
k+1	O
=	O
O	O
k+1	O
+	O
q	O
k	O
,	O
and	O
based	O
on	O
the	O
output	O
representation	O
and	O
question	O
representation	O
,	O
answer	O
is	O
predicted	O
as	O
follows	O
:	O
a	O
=	O
softmax(f	O
(	O
O	O
K	O
+	O
q	O
K−1	O
)	O
)	O
where	O
f	O
is	O
a	O
single	O
layer	B-HyperparameterName
feed	I-HyperparameterName
-	I-HyperparameterName
forward	I-HyperparameterName
layer	I-HyperparameterName
.	O

In	O
this	O
paper	O
,	O
we	O
introduce	O
a	O
concept	O
of	O
hypergraph	O
to	O
encode	O
highlevel	O
semantics	O
of	O
a	O
question	O
and	O
a	O
knowledge	O
base	O
,	O
and	O
to	O
learn	O
high	O
-	O
order	O
associations	O
between	O
them	O
.	O

For	O
HAN	B-MethodName
,	O
the	O
hyperedges	O
sampled	O
by	O
stochastic	O
graph	O
walk	O
are	O
fed	O
into	O
the	O
co	O
-	O
attention	O
mechanism	O
.	O

Therefore	O
,	O
the	O
result	O
shows	O
that	O
the	O
NDR	B-TaskName
model	O
can	O
achieve	O
simple	O
counterfactual	O
thinking	O
by	O
learning	O
to	O
answer	O
hypothetical	O
questions	O
.	O

6	O
Results	O
and	O
analysisTable	O
1	O
shows	O
the	O
performance	O
of	O
the	O
above	O
approaches	O
on	O
the	O
two	O
datasets	O
.	O

In	O
the	O
teacher	O
training	O
model	O
,	O
there	O
are	O
two	O
sub	O
-	O
models	O
,	O
i.e.	O
an	O
entity	O
recognizer	O
teacher	O
and	O
a	O
similarity	O
evaluator	O
teacher	O
.	O

Position	O
1	O
1	O
2	O
3	O
4	O
5	O
5	O
5	O
5	O
3	O
3	O
Position	O
2	O
0	O
0	O
0	O
0	O
0	O
1	O
2	O
3	O
1	O
2	O
output	O
respectively	O
.	O

This	O
result	O
thus	O
reflects	O
the	O
advantage	O
of	O
the	O
unified	O
operator	O
framework	O
adopted	O
by	O
the	O
L2I	B-MethodName
module	O
,	O
which	O
is	O
consistent	O
with	O
previous	O
work	O
(	O
Andor	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

Also	O
,	O
our	O
model	O
shows	O
significant	O
improvement	O
in	O
spatial	O
question	O
compared	O
to	O
other	O
models	O
.	O

To	O
predict	O
an	O
answer	O
,	O
we	O
first	O
concatenate	O
the	O
representation	O
z	O
k	O
and	O
z	O
q	O
obtained	O
from	O
the	O
attention	O
blocks	O
and	O
feed	O
into	O
a	O
single	O
feed	B-HyperparameterName
-	I-HyperparameterName
forward	I-HyperparameterName
layer	I-HyperparameterName
(	O
i.e.	O
,	O
R	O
2dv	O
→	O
R	O
w	O
)	O
to	O
make	O
a	O
joint	O
representation	O
z.	O
We	O
then	O
consider	O
two	O
types	O
of	O
answer	O
predictor	O
:	O
multi	O
-	O
layer	O
perceptron	O
and	O
similarity	O
-	O
based	O
answer	O
predictor	O
.	O

We	O
present	O
the	O
detailed	O
content	O
of	O
the	O
example	O
in	O
Section	O
1	O
in	O
table	O
12.We	O
present	O
more	O
examples	O
of	O
student	O
models	O
'	O
outputs	O
and	O
cross	O
attention	O
visualization	O
here	O
.	O

The	O
classifier	O
predicts	O
an	O
overriding	O
majority	O
of	O
the	O
data	O
(	O
99.83	B-HyperparameterValue
%	I-MetricValue
)	O
as	O
MSA	O
.	O

We	O
design	O
an	O
RL	O
agent	O
that	O
is	O
capable	O
of	O
automatic	O
task	O
decomposition	O
and	O
subtask	O
-	O
conditioned	O
action	O
pruning	O
,	O
which	O
brings	O
two	O
branches	O
of	O
benefits	O
.	O

We	O
also	O
investigate	O
multitask	O
learning	O
(	O
Caruana	O
,	O
1997;Ruder	O
,	O
2017	O
)	O
with	O
our	O
AraT5	B-MethodName
models	O
.	O

In	O
comparison	O
,	O
Transformer	B-MethodName
(	I-MethodName
SA+GA	I-MethodName
)	I-MethodName
strongly	O
attends	O
to	O
the	O
knowledge	O
entities	O
which	O
appear	O
repetitive	O
in	O
the	O
knowledge	O
facts	O
.	O

Specifically	O
,	O
we	O
normalize	O
the	O
token	O
positions	O
of	O
each	O
document	O
to	O
(	O
0.0	B-HyperparameterValue
,	O
1.0	B-HyperparameterValue
]	O
and	O
divide	O
the	O
normalized	O
positions	O
into	O
five	O
bins	O
.	O

Shleifer	O
and	O
Rush	O
(	O
2020	O
)	O
compare	O
pseudolabeling	O
(	O
BART	B-MethodName
-	I-MethodName
PL	I-MethodName
)	O
,	O
knowledge	O
distillation	O
using	O
both	O
output	O
and	O
intermediate	O
layers	O
(	O
BART	B-MethodName
-	I-MethodName
KD	I-MethodName
)	O
as	O
well	O
as	O
shrink	O
and	O
fine	O
-	O
tuning	O
(	O
BART	B-MethodName
-	I-MethodName
SFT	I-MethodName
)	O
methods	O
.	O

We	O
train	O
GLM	B-MethodName
Base	I-MethodName
and	O
GLM	B-MethodName
Large	I-MethodName
with	O
the	O
same	O
architectures	O
as	O
BERT	B-MethodName
Base	I-MethodName
and	O
BERT	B-MethodName
Large	I-MethodName
,	O
containing	O
110	B-HyperparameterValue
M	I-HyperparameterValue
and	O
340	B-HyperparameterValue
M	I-HyperparameterValue
parameters	B-HyperparameterName
respectively	O
.	O

A	O
POMDP	B-MethodName
can	O
be	O
described	O
by	O
a	O
tuple	O
G	O
=	O
⟨S	O
,	O
A	O
,	O
P	O
,	O
r	O
,	O
Ω	O
,	O
O	O
,	O
γ⟩	O
,	O
with	O
S	O
representing	O
the	O
state	O
set	O
,	O
A	O
the	O
action	O
set	O
,	O
P	O
(	O
s	O
′	O
|s	O
,	O
a	O
)	O
:	O
S	O
×	O
A	O
×	O
S	O
→	O
R	O
+	O
the	O
state	O
transition	O
probabilities	O
,	O
r(s	O
,	O
a	O
)	O
:	O
S	O
×	O
A	O
→	O
R	O
the	O
reward	O
function	O
,	O
Ω	O
the	O
observation	O
set	O
,	O
O	O
the	O
conditional	O
observation	O
probabilities	O
,	O
and	O
γ	B-HyperparameterName
∈	O
(	O
0	O
,	O
1	O
]	O
the	O
discount	O
factor	O
.	O

by	O
far	O
the	O
best	O
breakfast	O
tacos	O
in	O
the	O
area	O
.	O

We	O
also	O
evaluate	O
on	O
our	O
two	O
synthetic	B-TaskName
CST	I-TaskName
datasets	O
,	O
MSA	B-MethodName
-	I-MethodName
EN	I-MethodName
and	O
MSA	B-MethodName
-	I-MethodName
FR	I-MethodName
,	O
one	O
time	O
with	O
EN	B-MetricName
/	I-MethodName
FR	I-MethodName
as	O
target	O
(	O
e.g.	O
,	O
MSA	B-MethodName
-	I-MethodName
EN→EN	I-MethodName
)	O
and	O
another	O
with	O
MSA	B-MetricName
as	O
target	O
(	O
e.g.	O
,	O
MSA	B-MethodName
-	I-MethodName
EN→MSA	I-MethodName
)	O
.	O

To	O
fully	O
capture	O
the	O
interdependencies	O
between	O
different	O
spans	O
,	O
we	O
randomly	O
permute	O
the	O
order	O
of	O
the	O
spans	O
,	O
similar	O
to	O
the	O
permutation	O
language	O
model	O
(	O
Yang	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

Based	O
on	O
how	O
actions	O
are	O
selected	O
,	O
the	O
RL	O
agents	O
can	O
also	O
be	O
divided	O
as	O
parser	O
-	O
based	O
agents	O
,	O
choice	O
-	O
based	O
agents	O
,	O
and	O
template	O
-	O
based	O
agents	O
.	O

All	O
experiments	O
were	O
repeated	O
three	O
times	O
with	O
different	O
random	O
seeds	O
,	O
and	O
the	O
average	O
performances	O
and	O
standard	O
deviations	O
have	O
been	O
reported	O
.	O

In	O
this	O
work	O
,	O
the	O
data	O
source	O
we	O
use	O
is	O
from	O
a	O
published	O
dataset	O
and	O
does	O
not	O
involve	O
privacy	O
issues	O
for	O
the	O
data	O
collection	O
.	O

Additional	O
pre	O
-	O
training	O
with	O
in	O
-	O
domain	O
text	O
has	O
been	O
proposed	O
to	O
provide	O
the	O
PLMs	O
with	O
domain	O
-	O
specific	O
knowledge	O
.	O

Of	O
these	O
tasks	O
,	O
in	O
this	O
work	O
we	O
focus	O
on	O
dialogue	B-TaskName
policy	I-TaskName
management	I-TaskName
to	O
improve	O
the	O
endto	O
-	O
end	O
performance	O
of	O
ToD.	B-TaskName
The	O
need	O
for	O
sample	O
efficiency	O
is	O
key	O
for	O
learning	O
offline	O
task	B-TaskName
-	B-TaskName
oriented	I-TaskName
dialogue	I-TaskName
system	O
,	O
as	O
access	O
to	O
data	O
are	O
finite	O
and	O
expensive	O
.	O

NLU	B-TaskName
as	B-TaskName
Generation	I-TaskName
.	O

e	O
k	O
=	O
ϕ	B-HyperparameterName
k	O
•	O
f	O
k	O
(	O
h	O
k	O
)	O
∈	O
R	O
d	O
,	O
e	O
q	O
=	O
ϕ	O
q	O
•	O
f	O
q	O
(	O
h	O
q	O
)	O
∈	O
R	O
d	O
where	O
h	O
[	O
•	O
]	O
is	O
a	O
hyperedge	O
in	O
E	O
[	O
•	O
]	O
.	O

BERTSUM	B-MethodName
(	O
Liu	O
and	O
Lapata	O
,	O
2019	O
)	O
employs	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O
2019	O
)	O
as	O
its	O
encoder	O
and	O
uses	O
randomly	O
initialized	O
decoder	O
.	O

It	O
contains	O
English	O
reviews	O
of	O
23	O
types	O
of	O
Amazon	O
products	O
,	O
where	O
each	O
product	O
consists	O
of	O
three	O
different	O
binary	O
classification	O
tasks	O
.	O

In	O
Figure	O
4	O
,	O
we	O
show	O
that	O
removing	O
the	O
second	B-HyperparameterName
dimension	I-HyperparameterName
of	O
2D	B-HyperparameterValue
positional	I-HyperparameterValue
encoding	I-HyperparameterValue
hurts	O
the	O
performance	O
of	O
long	B-TaskName
text	I-TaskName
generation	I-TaskName
.	O

The	O
Rams	O
waived	O
Smith	O
during	O
the	O
September	O
1	O
,	O
1972	O
offseason	O
.	O

A	O
hyperedge	O
is	O
flexible	O
to	O
encode	O
different	O
kinds	O
of	O
semantics	O
in	O
the	O
underlying	O
graph	O
without	O
the	O
constraint	O
of	O
length	O
.	O

The	O
mean	O
proportions	O
of	O
evident	O
attentions	O
for	O
all	O
bins	O
are	O
shown	O
in	O
Figure	O
2	O
.	O

In	O
the	O
pre	O
-	O
training	O
phase	O
,	O
we	O
collect	O
human	O
interaction	O
data	O
from	O
the	O
simple	O
games	O
,	O
and	O
design	O
QA	O
datasets	O
to	O
train	O
the	O
worldperceiving	O
modules	O
through	O
supervised	O
learning	O
.	O

To	O
alleviate	O
the	O
influence	O
of	O
the	O
compound	O
error	O
,	O
we	O
assign	O
time	O
-	O
awareness	O
to	O
subtasks	O
.	O

As	O
fingerspelling	O
occurs	O
sparsely	O
in	O
the	O
signing	O
stream	O
,	O
explicit	O
detection	O
of	O
fingerspelling	O
could	O
potentially	O
improve	O
search	O
performance	O
by	O
removing	O
unrelated	O
signs	O
.	O

Specifically	O
,	O
GLM	B-MethodName
RoBERTa	I-MethodName
outperforms	O
T5	B-MethodName
Large	I-MethodName
but	O
is	O
only	O
half	O
its	O
size	O
.	O

Nonetheless	O
,	O
these	O
approaches	O
suffer	O
from	O
the	O
memorization	O
overfitting	O
issue	O
,	O
where	O
the	O
model	O
tends	O
to	O
memorize	O
the	O
meta	O
-	O
training	O
tasks	O
while	O
ignoring	O
support	O
sets	O
when	O
adapting	O
to	O
new	O
tasks	O
.	O

In	O
this	O
study	O
,	O
we	O
proposed	O
the	O
DoKTra	B-MethodName
framework	O
as	O
a	O
domain	O
knowledge	O
transfer	O
method	O
for	O
PLMs	O
.	O

We	O
will	O
treat	O
the	O
IL	O
-	O
based	O
method	O
as	O
a	O
baseline	O
and	O
conduct	O
comparisons	O
in	O
the	O
experiments	O
.	O

For	O
Attn	B-MethodName
-	I-MethodName
KWS	I-MethodName
,	O
the	O
model	O
outputs	O
an	O
attention	O
vector	O
,	O
which	O
we	O
convert	O
to	O
segments	O
as	O
in	O
(	O
Shi	O
et	O
al	O
.	O
,	O
2021	O
)	O
.	O

told	O
the	O
manager	O
about	O
my	O
allergies	O
and	O
that	O
all	O
i	O
wanted	O
was	O
vegetable	O
fried	O
rice	O
no	O
soy	O
sauce	O
they	O
could	O
n't	O
even	O
handle	O
that	O
!	O
!	O
!	O
amateur	O
hour	O
here	O
do	O
n't	O
waste	O
your	O
time	O
.	O

1	O
)	O
Appropriateness	O
:	O
Are	O
the	O
generated	O
responses	O
appropriate	O
for	O
the	O
given	O
context	O
in	O
the	O
dialogue	O
turn	O
?	O
2	O
)	O
Fluency	B-MetricName
:	O
Are	O
the	O
generated	O
responses	O
coherent	O
and	O
comprehensible	O
?	O

Proof	O
.	O
Experimental	O
Setup	O
.	O

However	O
,	O
in	O
the	O
future	O
we	O
plan	O
to	O
compare	O
our	O
models	O
under	O
the	O
full	O
data	O
setting	O
.	O

He	O
later	O
appeared	O
as	O
a	O
regular	O
for	O
the	O
show	O
's	O
final	O
six	O
seasons	O
,	O
and	O
has	O
been	O
a	O
frequent	O
guest	O
in	O
the	O
show	O
since	O
.	O

7	O
We	O
pre	O
-	O
train	O
each	O
model	O
for	O
1	B-HyperparameterValue
M	I-HyperparameterValue
steps	B-HyperparameterName
.	O

Dataset	O
.	O

The	O
work	O
is	O
also	O
supported	O
by	O
the	O
project	O
no	O
.	O

Settings	O
and	O
Evaluation	O
.	O

As	O
both	O
TAPT	B-MethodName
and	O
DoK	B-MethodName
-	I-MethodName
Tra	I-MethodName
only	O
utilize	O
the	O
task	O
-	O
specific	O
training	O
data	O
,	O
they	O
can	O
be	O
fairly	O
compared	O
in	O
terms	O
of	O
performance	O
and	O
training	O
resources	O
.	O

More	O
transfer	O
results	O
are	O
mention	O
in	O
Table	O
8	O
.	O

These	O
models	O
mainly	O
update	O
node	O
representations	O
in	O
the	O
hypergraph	O
through	O
a	O
message	O
passing	O
process	O
using	O
graph	O
convolution	O
operation	O
.	O

Optimization	O
-	O
based	O
meta	O
-	O
learning	O
algorithms	O
achieve	O
promising	O
results	O
in	O
low	O
-	O
resource	O
scenarios	O
by	O
adapting	O
a	O
well	O
-	O
generalized	O
model	O
initialization	O
to	O
handle	O
new	O
tasks	O
.	O

3	O
The	O
resulting	O
numbers	O
of	O
document	O
-	O
summary	O
pairs	O
for	O
training	O
,	O
validation	O
,	O
and	O
test	O
are	O
287,227	B-HyperparameterValue
,	O
13,368	O
,	O
and	O
11,490	O
,	O
respectively	O
.	O

Encoder	O
-	O
decoder	O
models	O
adopt	O
bidirectional	O
attention	O
for	O
the	O
encoder	O
,	O
unidirectional	O
attention	O
for	O
the	O
decoder	O
,	O
and	O
cross	O
attention	O
between	O
them	O
(	O
Song	O
et	O
al	O
.	O
,	O
2019;Bi	O
et	O
al	O
.	O
,	O
2020	O
;	O
.	O

We	O
show	O
that	O
the	O
NLU	B-TaskName
tasks	O
can	O
be	O
formulated	O
as	O
conditional	B-TaskName
generation	I-TaskName
tasks	O
,	O
and	O
therefore	O
solvable	O
by	O
autoregressive	O
models	O
.	O

As	O
mentioned	O
before	O
,	O
our	O
best	O
model	O
outperformed	O
the	O
BioBERT	B-MethodName
(	O
teacher	O
)	O
model	O
on	O
four	O
of	O
the	O
five	O
tasks	O
.	O

6	O
.	O

Our	O
method	O
aims	O
to	O
make	O
these	O
large	O
models	O
faster	O
.	O

And	O
β	B-HyperparameterName
is	O
set	O
such	O
that	O
it	O
is	O
high	O
when	O
the	O
output	O
of	O
the	O
entity	O
similarity	O
teacher	O
is	O
close	O
to	O
0	O
or	O
1	O
,	O
and	O
it	O
is	O
low	O
when	O
the	O
output	O
is	O
close	O
to	O
0.5	B-HyperparameterValue
.	O

Rastogi	O
et	O
al	O
.	O
(	O
2019	O
)	O
and	O
Hosseini	O
-	O
Asl	O
et	O
al	O
.	O
(	O
2020	O
)	O
frame	O
dialogue	O
policy	O
learning	O
as	O
language	B-TaskName
modeling	I-TaskName
task	O
.	O

Following	O
(	O
Tjong	O
Kim	O
Sang	O
,	O
2002	O
)	O
,	O
we	O
use	O
the	O
entity	O
level	O
F1	B-MetricName
-	O
score	O
as	O
the	O
evaluation	O
metric	O
.	O

The	O
student	O
model	O
learns	O
less	O
from	O
unreasonable	O
results	O
,	O
and	O
it	O
can	O
make	O
more	O
accurate	O
entity	O
recognition	O
for	O
the	O
target	O
language	O
.	O

Our	O
future	O
studies	O
would	O
focus	O
on	O
developing	O
the	O
proposed	O
framework	O
as	O
a	O
task	O
-	O
agnostic	O
method	O
and	O
evaluating	O
it	O
on	O
various	O
tasks	O
.	O

since	O
p(Ŷ	O
q	O
,	O
Z	O
)	O
does	O
not	O
rely	O
on	O
the	O
variable	O
M.	O
Hence	O
,	O
we	O
can	O
just	O
write	O
EŶ	O
q	O
,	O
Z	O
,	O
M	O
as	O
E	O
for	O
short	O
.	O

Sajjad	O
et	O
al	O
.	O
(	O
2020	O
)	O
introduce	O
AraBench	B-DatasetName
,	O
an	O
evaluation	O
suite	O
for	O
MSA	B-TaskName
and	O
dialectal	B-TaskName
Arabic	I-TaskName
to	O
English	B-TaskName
MT	I-TaskName
consisting	O
of	O
five	O
publicly	O
available	O
datasets	O
:	O
(	O
3	O
)	O
ADPT	B-MethodName
:	O
Arabic	B-TaskName
-	I-TaskName
Dialect	I-TaskName
/	O
English	O
Parallel	O
Text	O
(	O
Zbib	O
et	O
al	O
.	O
,	O
2012	O
)	O
,	O
(	O
4	O
)	O
MADAR	B-MethodName
:	O
Multi	O
-	O
Arabic	O
Dialect	O
Applications	O
and	O
Resources	O
dataset	O
(	O
Bouamor	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
(	O
5	O
)	O
QAraC	B-DatasetName
:	O
Qatari	O
-	O
English	O
speech	O
corpus	O
(	O
Elmahdy	O
et	O
al	O
.	O
,	O
2014	O
)	O
,	O
and	O
(	O
6	O
)	O
Bible	O
:	O
The	O
English	O
Bible	O
translated	O
into	O
MSA	O
,	O
Moroccan	O
,	O
and	O
Tunisian	O
Arabic	O
dialects	O
.	O

Table	O
5	O
:	O
Example	O
outputs	O
generated	O
by	O
the	O
best	O
system	O
according	O
to	O
AGG	B-MetricName
score	O
.	O

The	O
target	O
is	O
the	O
Egyptian	O
transliteration	O
of	O
these	O
message	O
.	O

Learning	B-HyperparameterName
rates	I-HyperparameterName
are	O
tuned	O
on	O
validation	O
sets	O
(	O
choose	O
from	O
1e-5	B-HyperparameterValue
,	O
3e-5	B-HyperparameterValue
,	O
5e-5	B-HyperparameterValue
,	O
7e-5	B-HyperparameterValue
)	O
.	O

We	O
translate	O
these	O
manually	O
into	O
monolingual	O
French	O
.	O

For	O
the	O
5	O
single	O
-	O
token	O
tasks	O
,	O
the	O
score	O
is	O
defined	O
to	O
be	O
the	O
logit	O
of	O
the	O
verbalizer	O
token	O
.	O

D	O
provides	O
more	O
experimental	O
results	O
.	O

This	O
shows	O
the	O
potential	O
application	O
of	O
domain	O
transferred	O
sentences	O
as	O
adversarial	O
examples	O
.	O

In	O
addition	O
,	O
explicit	O
fingerspelling	O
detection	O
(	O
Ext	B-MethodName
-	I-MethodName
Det	I-MethodName
,	O
FSS	B-MethodName
-	I-MethodName
Net	I-MethodName
)	O
improves	O
performance	O
over	O
implicit	O
fin-	O
Of	O
the	O
models	O
that	O
do	O
n't	O
use	O
such	O
supervision	O
,	O
Attn	B-MethodName
-	I-MethodName
KWS	I-MethodName
is	O
the	O
best	O
performer	O
given	O
enough	O
data	O
,	O
but	O
is	O
still	O
far	O
behind	O
FSS	B-MethodName
-	I-MethodName
Net	I-MethodName
.	O

Then	O
,	O
we	O
distilled	O
for	O
10	B-HyperparameterValue
epochs	B-HyperparameterName
with	O
learning	B-HyperparameterName
rates	I-HyperparameterName
of	O
{	O
6e-6	B-HyperparameterValue
,	O
8e-6	B-HyperparameterValue
,	O
1e-5	B-HyperparameterValue
}	O
.	O

11	O
shows	O
the	O
action	O
predicting	O
process	O
.	O

We	O
note	O
that	O
T5	B-MethodName
is	O
pretrained	O
with	O
a	O
similar	O
blank	O
infilling	O
objective	O
.	O

We	O
introduce	O
an	O
attention	O
mechanism	O
over	O
two	O
hypergraphs	O
based	O
on	O
guided	O
-	O
attention	O
(	O
Tsai	O
et	O
al	O
.	O
,	O
2019	O
)	O
and	O
self	O
-	O
attention	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

Green	O
refers	O
to	O
good	O
translation	O
.	O

We	O
give	O
a	O
case	O
study	O
to	O
show	O
that	O
the	O
failed	O
cases	O
of	O
baseline	O
models	O
can	O
be	O
corrected	O
by	O
our	O
model	O
.	O

To	O
evaluate	O
a	O
pure	O
reasoning	O
ability	O
of	O
the	O
models	O
,	O
we	O
conduct	O
experiments	O
in	O
the	O
oracle	O
setting	O
.	O

All	O
the	O
other	O
baselines	O
are	O
of	O
similar	O
size	O
to	O
BERT	B-MethodName
Large	I-MethodName
.	O

Figure	O
1	O
illustrates	O
one	O
such	O
example	O
,	O
where	O
a	O
sentence	O
from	O
the	O
BOOKS	O
domain	O
is	O
translated	O
to	O
the	O
MOVIE	O
domain	O
.	O

We	O
evaluate	O
the	O
quality	O
of	O
different	O
summarization	O
systems	O
using	O
ROUGE	B-MetricName
.	O

Subtasks	O
"	O
denote	O
the	O
average	O
number	B-HyperparameterName
of	I-HyperparameterName
subtask	I-HyperparameterName
candidates	I-HyperparameterName
T	O
,	O
and	O
the	O
average	O
number	B-HyperparameterName
of	I-HyperparameterName
available	I-HyperparameterName
subtasks	O
T	O
t	O
,	O
respectively	O
)	O
.	O

Note	O
that	O
if	O
an	O
n	O
-	O
gram	O
appears	O
in	O
the	O
summary	O
,	O
but	O
not	O
in	O
the	O
original	O
document	O
,	O
we	O
call	O
it	O
a	O
novel	O
n	O
-	O
gram	O
.	O

In	O
Table	O
3	O
,	O
the	O
huge	O
performance	O
drop	O
shows	O
that	O
even	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
NDR	B-TaskName
model	O
lacks	O
counterfactual	O
thinking	O
ability	O
.	O

Besides	O
,	O
the	O
performance	O
difference	O
between	O
SIM	B-MetricName
and	O
MLP	B-MetricName
in	O
one	O
-	O
shot	O
answer	O
(	O
appeared	O
in	O
the	O
only	O
one	O
time	O
in	O
training	O
phase	O
)	O
is	O
more	O
than	O
18	B-HyperparameterValue
%	I-HyperparameterValue
.	O

The	O
maximum	B-HyperparameterName
passage	B-HyperparameterName
length	I-HyperparameterName
is	O
464	B-HyperparameterValue
and	O
the	O
maximum	B-HyperparameterName
question	B-HyperparameterName
length	I-HyperparameterName
is	O
48	B-HyperparameterValue
.	O

Figure	O
1	O
intuitively	O
shows	O
the	O
effect	O
of	O
using	O
higher	O
attention	B-HyperparameterName
temperatures	I-HyperparameterName
.	O

We	O
also	O
do	O
not	O
require	O
additional	O
handcrafted	O
operations	O
in	O
reward	O
shaping	O
(	O
Bahdanau	O
et	O
al	O
.	O
,	O
2019).There	O
have	O
been	O
a	O
wide	O
range	O
of	O
work	O
studying	O
pre	O
-	O
training	O
methods	O
or	O
incorporating	O
pre	O
-	O
trained	O
modules	O
to	O
facilitate	O
reinforcement	O
learning	O
(	O
Eysenbach	O
et	O
al	O
.	O
,	O
2018;Hansen	O
et	O
al	O
.	O
,	O
2019;Sharma	O
et	O
al	O
.	O
,	O
2019;Gehring	O
et	O
al	O
.	O
,	O
2021;Liu	O
et	O
al	O
.	O
,	O
2021;Schwarzer	O
et	O
al	O
.	O
,	O
2021	O
)	O
.	O

The	O
models	O
are	O
optimized	O
via	O
Double	B-MethodName
DQN	I-MethodName
(	O
epsilon	O
decays	O
from	O
1.0	B-HyperparameterValue
to	O
0.1	B-HyperparameterValue
in	O
20,000	B-HyperparameterValue
episodes	B-HyperparameterName
,	O
Adam	B-HyperparameterValue
optimizer	I-HyperparameterName
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.001	B-HyperparameterValue
)	O
with	O
Pritorized	O
Experience	B-HyperparameterName
Replay	I-HyperparameterName
(	O
replay	B-HyperparameterName
buffer	I-HyperparameterName
size	I-HyperparameterName
500,000	B-HyperparameterValue
)	O
.	O

Each	O
hyperedge	O
connects	O
an	O
arbitrary	O
number	O
of	O
nodes	O
and	O
has	O
partial	O
order	O
itself	O
,	O
i.e.	O
,	O
h	O

The	O
TAPT	B-MethodName
approach	O
additionally	O
pre	O
-	O
trains	O
an	O
existing	O
PLM	O
before	O
fine	O
-	O
tuning	O
it	O
with	O
the	O
training	O
samples	O
of	O
each	O
task	O
.	O

In	O
general	O
,	O
the	O
models	O
with	O
more	O
accurate	O
localization	O
also	O
have	O
higher	O
search	O
and	O
retrieval	O
performance	O
,	O
as	O
seen	O
by	O
comparing	O
Table	O
2	O
with	O
Table	O
1	O
.	O

Then	O
,	O
it	O
infers	O
an	O
answer	O
by	O
attending	O
to	O
knowledge	O
evidence	O
with	O
high	O
attention	O
scores	O
.	O

We	O
also	O
want	O
to	O
thank	O
MindSpore	B-DatasetName
1	O
for	O
the	O
partial	O
suppoort	O
of	O
this	O
work	O
,	O
which	O
is	O
a	O
new	O
deep	O
learning	O
computing	O
framework	O
.	O

In	O
our	O
work	O
,	O
user	O
embeddings	O
are	O
learned	O
in	O
a	O
different	O
approach	O
,	O
and	O
we	O
focus	O
on	O
how	O
to	O
use	O
similarity	O
calculated	O
from	O
user	O
embeddings	O
to	O
build	O
better	O
LMs	O
.	O

We	O
find	O
that	O
AGG	B-MetricName
is	O
the	O
highest	O
with	O
2	O
positives	O
per	O
sample	O
as	O
also	O
used	O
by	O
Khosla	O
et	O
al	O
.	O
(	O
2020	O
)	O
.	O

We	O
train	O
DANN	B-MethodName
for	O
sentiment	B-TaskName
analysis	I-TaskName
on	O
amazon	O
reviews	O
dataset	O
(	O
He	O
and	O
McAuley	O
,	O
2016	O
)	O
with	O
DVD	O
as	O
source	O
and	O
ELECTRONICS	B-DatasetName
as	O
the	O
tar	O
-	O
get	O
domain	O
-achieving	O
an	O
accuracy	O
of	O
83.75	B-MetricValue
%	I-MetricValue
on	O
ELECTRONICS	B-DatasetName
.	O

simple	O
menu	O
.	O

During	O
decoding	O
,	O
we	O
use	O
beam	B-HyperparameterValue
search	I-HyperparameterName
with	O
beam	B-HyperparameterName
size	I-HyperparameterName
of	O
5	B-HyperparameterValue
and	O
remove	O
repeated	O
trigrams	O
.	O

These	O
results	O
validate	O
that	O
it	O
is	O
meaningful	O
to	O
consider	O
not	O
only	O
knowledge	O
but	O
also	O
question	O
as	O
hypergraphs	O
.	O

2	O
Given	O
a	O
query	O
video	O
clip	O
v	O
and	O
a	O
list	O
of	O
n	O
words	O
w	O
1	O
:	O
n	O
,	O
FWS	B-DatasetName
is	O
the	O
task	O
of	O
finding	O
which	O
(	O
if	O
any	O
)	O
of	O
w	O
1	O
:	O
n	O
are	O
present	O
in	O
v.	O
Conversely	O
,	O
in	O
FVS	B-DatasetName
the	O
input	O
is	O
a	O
query	O
word	O
w	O
and	O
n	O
video	O
clips	O
v	O
1	O
:	O
n	O
,	O
and	O
the	O
task	O
consists	O
of	O
finding	O
all	O
videos	O
containing	O
the	O
fingerspelled	O
word	O
w.	O
We	O
consider	O
an	O
openvocabulary	O
setting	O
where	O
the	O
word	O
w	O
is	O
not	O
constrained	O
to	O
a	O
pre	O
-	O
determined	O
set	O
.	O

Example	O
D.3	O
.	O
Corona	O
was	O
a	O
station	O
along	O
the	O
port	O
Washington	O
branch	O
of	O
the	O
long	O
island	O
rail	O
road	O
in	O
the	O
Corona	O
section	O
of	O
queens	O
,	O
New	O
York	O
City	O
.	O

When	O
applying	O
L2I	B-MethodName
to	O
an	O
existing	O
NDR	B-TaskName
method	O
,	O
we	O
keep	O
its	O
question	B-TaskName
-	I-TaskName
answering	I-TaskName
objective	O
unchanged	O
.	O

The	O
candidate	O
labels	O
y	O
∈	O
Y	O
are	O
also	O
mapped	O
to	O
answers	O
to	O
the	O
cloze	O
,	O
called	O
verbalizer	O
v(y	O
)	O
.	O

The	O
GATA	B-MethodName
and	O
IL	B-MethodName
models	O
are	O
equipped	O
with	O
similar	O
modules	O
.	O

Pfister	O
et	O
al	O
.	O
(	O
2013	O
)	O
;	O
employ	O
mouthing	O
to	O
detect	O
keywords	O
in	O
sign	O
-	O
interpreted	O
TV	O
programs	O
with	O
coarsely	O
aligned	O
subtitles	O
.	O

11	O
The	O
Arabic	O
part	O
includes	O
summaries	O
for	O
29.2	B-HyperparameterValue
K	I-HyperparameterValue
articles	B-HyperparameterName
,	O
which	O
we	O
split	O
into	O
80	B-HyperparameterValue
%	I-HyperparameterValue
Train	B-HyperparameterName
(	O
23.4	B-HyperparameterValue
K	I-HyperparameterValue
)	O
,	O
10	B-HyperparameterValue
%	I-HyperparameterValue
Dev	B-HyperparameterName
(	O
2.9	B-HyperparameterValue
K	I-HyperparameterValue
)	O
,	O
and	O
10	B-HyperparameterValue
%	I-HyperparameterValue
Test	B-HyperparameterName
(	O
2.9K).The	B-HyperparameterValue
purpose	O
of	O
the	O
news	B-TaskName
title	I-TaskName
generation	I-TaskName
(	O
NTG	B-TaskName
)	O
task	O
is	O
to	O
produce	O
proper	O
news	O
article	O
titles	O
(	O
Liang	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

We	O
find	O
that	O
the	O
proposed	O
method	O
facilitates	O
insights	O
into	O
causes	O
of	O
variation	O
between	O
reproductions	O
,	O
and	O
allows	O
conclusions	O
to	O
be	O
drawn	O
about	O
what	O
changes	O
to	O
system	O
and/or	O
evaluation	O
design	O
might	O
lead	O
to	O
improved	O
reproducibility	O
.	O

Third	O
,	O
encoder	O
-	O
decoder	O
models	O
are	O
pretrained	O
for	O
sequence	O
-	O
to	O
-	O
sequence	O
tasks	O
(	O
Song	O
et	O
al	O
.	O
,	O
2019;Bi	O
et	O
al	O
.	O
,	O
2020;Zhang	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

These	O
turn	O
out	O
to	O
be	O
almost	O
always	O
natural	O
code	O
-	O
switching	O
involving	O
many	O
foreign	O
languages	O
(	O
e.g.	O
,	O
English	O
,	O
French	O
,	O
Korean	O
,	O
etc	O
.	O

In	O
summary	O
,	O
the	O
best	O
performance	O
achieved	O
by	O
QWA	B-MethodName
demonstrates	O
that	O
our	O
model	O
can	O
generalize	O
well	O
on	O
games	O
with	O
different	O
complexities	O
.	O

We	O
confirm	O
that	O
our	O
model	O
works	O
effectively	O
as	O
a	O
general	O
reasoning	O
framework	O
without	O
considering	O
characteristics	O
of	O
different	O
knowledge	O
sources	O
(	O
i.e.	O
,	O
Wikidata	B-DatasetName
for	O
KVQA	B-DatasetName
,	O
DBpedia	B-DatasetName
,	O
ConceptNet	B-DatasetName
,	O
WebChild	B-DatasetName
for	O
FVQA).To	B-DatasetName
required	O
to	O
answer	O
a	O
given	O
question	O
is	O
unknown	O
.	O

Fig	O
.	O

For	O
instance	O
,	O
it	O
can	O
be	O
a	O
combination	O
of	O
the	O
cross	B-HyperparameterValue
-	I-HyperparameterValue
entropy	I-HyperparameterValue
(	O
CE	B-HyperparameterValue
)	B-HyperparameterValue
loss	I-HyperparameterValue
over	O
the	O
operand	O
look	O
-	O
up	O
and	O
the	O
CE	B-HyperparameterValue
loss	I-HyperparameterValue
over	O
the	O
choice	O
of	O
discrete	O
operation	O
(	O
Herzig	O
et	O
al	O
.	O
,	O
2020;Zhu	O
et	O
al	O
.	O
,	O
2021	O
)	O
.	O

For	O
FWS	B-TaskName
,	O
we	O
use	O
all	O
words	O
in	O
the	O
test	O
set	O
as	O
the	O
test	O
vocabulary	O
w	O
1	O
:	O
n	O
.	O

With	O
these	O
extremely	O
large	O
models	O
,	O
we	O
can	O
obtain	O
state	O
-	O
of	O
-	O
theart	O
summarization	O
results	O
,	O
but	O
they	O
are	O
slow	O
for	O
online	O
inference	O
,	O
which	O
makes	O
them	O
difficult	O
to	O
be	O
used	O
in	O
the	O
production	O
environment	O
even	O
with	O
cutting	O
-	O
edge	O
hardware	O
.	O

The	O
datasets	O
come	O
from	O
both	O
MSA	O
and	O
Arabic	O
dialects	O
,	O
and	O
range	O
between	O
600	B-HyperparameterValue
-	B-HyperparameterValue
138	I-HyperparameterValue
K	I-HyperparameterValue
sentences	B-HyperparameterName
(	O
details	O
in	O
Table	O
C.2	O
in	O
Appendix	O
)	O
.	O

We	O
truncate	O
all	O
documents	O
and	O
summaries	O
to	O
1024	B-HyperparameterValue
sub	B-HyperparameterName
-	I-HyperparameterName
word	B-HyperparameterName
tokens	I-HyperparameterName
.	O

where	O
the	O
last	O
inequality	O
holds	O
due	O
toŶ	O
q	O
is	O
dependent	O
on	O
M.	O

Examples	O
of	O
translation	O
from	O
the	O
multi	O
-	O
attribute	O
dataset	O
is	O
shown	O
in	O
Table	O
10.For	O
FL	B-MetricName
,	O
0	B-HyperparameterValue
indicates	O
not	O
fluent	O
at	O
all	O
,	O
1	O
indicates	O
somewhat	O
fluent	O
and	O
2	O
is	O
a	O
completely	O
fluent	O
sentence	O
.	O

We	O
reproduce	O
end	O
-	O
to	O
-	O
end	O
memory	B-MethodName
networks	I-MethodName
(	O
Sukhbaatar	O
et	O
al	O
.	O
,	O
2015	O
)	O
proposed	O
as	O
a	O
baseline	O
model	O
in	O
.	O

We	O
propose	O
MemIML	B-MethodName
to	O
enhance	O
the	O
dependence	O
of	O
the	O
model	O
on	O
the	O
support	O
sets	O
for	O
task	O
adaptation	O
.	O

Note	O
that	O
BERT	B-MethodName
-	I-MethodName
f	I-MethodName
performs	O
better	O
than	O
our	O
model	O
on	O
the	O
Chinese	O
dataset	O
due	O
to	O
their	O
re	O
-	O
tokenization	O
of	O
the	O
dataset	O
.	O

Assume	O
that	O
NDR	B-TaskName
model	O
equipped	O
with	O
L2I	B-MethodName
can	O
answer	O
the	O
hypothetical	O
questions	O
requiring	O
one	O
-	O
iteration	O
derivation	O
(	O
i.e.	O
,	O
c	O
i	O
→	O
c	O
i	O
)	O
.	O

they	O
did	O
a	O
great	O
job	O
last	O
time	O
we	O
were	O
there	O
since	O
our	O
party	O
had	O
specific	O
requirements	O
like	O
<	O
unk	O
>	O
free	O
and	O
<	O
unk	O
>	O
.	O

It	O
is	O
crucial	O
to	O
the	O
value	O
predictor	O
since	O
removing	O
it	O
from	O
the	O
value	O
predictor	O
results	O
in	O
an	O
even	O
worse	O
performance	O
than	O
removing	O
the	O
value	O
predictor	O
.	O

We	O
randomly	O
blank	O
out	O
continuous	O
spans	O
of	O
tokens	O
from	O
the	O
input	O
text	O
,	O
following	O
the	O
idea	O
of	O
autoencoding	O
,	O
and	O
train	O
the	O
model	O
to	O
sequentially	O
reconstruct	O
the	O
spans	O
,	O
following	O
the	O
idea	O
of	O
autoregressive	O
pretraining	O
(	O
see	O
Figure	O
1	O
)	O
.	O

All	O
of	O
the	O
feature	O
encoders	O
mentioned	O
in	O
this	O
paper	O
use	O
pre	O
-	O
trained	O
mBERT	B-MethodName
model	O
(	O
Devlin	O
et	O
al	O
.	O
,	O
2019	O
)	O
in	O
HuggingFace	B-MethodName
Transformer	I-MethodName
1	I-MethodName
,	O
which	O
has	O
12	B-HyperparameterValue
Transformer	B-HyperparameterName
blocks	I-HyperparameterName
,	O
12	B-HyperparameterValue
attention	B-HyperparameterName
heads	I-HyperparameterName
,	O
and	O
768	B-HyperparameterValue
hidden	B-HyperparameterName
units	I-HyperparameterName
.	O

While	O
satisfying	O
the	O
constraints	O
our	O
methods	O
brings	O
significant	O
improvements	O
in	O
overall	O
score	O
.	O

Retrieve	O
N	O
nearest	O
neighbors	O
of	O
K	O
q	O
j	O
from	O
Mi	O
.	O

To	O
encourage	O
teacher	O
models	O
to	O
generate	O
pseudo	O
labels	O
with	O
more	O
diversity	O
,	O
we	O
further	O
propose	O
to	O
use	O
a	O
random	O
λ	B-HyperparameterName
for	O
each	O
input	O
document	O
(	O
λ	O
∼	O
U	O
[	O
a	O
,	O
b	O
]	O
)	O
.	O

However	O
,	O
we	O
are	O
interested	O
in	O
pretraining	O
a	O
single	O
model	O
that	O
can	O
handle	O
both	O
NLU	B-TaskName
and	O
text	B-TaskName
generation	I-TaskName
.	O

We	O
observe	O
that	O
the	O
accuracy	B-MetricName
on	O
PQL-3H	B-DatasetName
is	O
relatively	O
lower	O
than	O
the	O
other	O
splits	O
.	O

In	O
the	O
literature	O
,	O
there	O
are	O
mainly	O
two	O
kinds	O
of	O
methods	O
for	O
summarization	B-TaskName
:	O
extractive	B-TaskName
summarization	I-TaskName
and	O
abstractive	B-TaskName
summarization	I-TaskName
(	O
Nenkova	O
and	O
McKeown	O
,	O
2011	O
)	O
.	O

We	O
explicitly	O
ask	O
the	O
annotators	O
to	O
consider	O
semantic	O
similarity	O
for	O
SIM	B-MetricName
,	O
irrespective	O
of	O
whether	O
the	O
target	O
sentence	O
shares	O
some	O
phrases	O
with	O
the	O
source	O
sentence	O
,	O
with	O
1	O
indicating	O
no	O
semantic	O
similarity	O
and	O
3	O
indicating	O
complete	O
semantic	O
similarity	O
.	O

The	O
experimental	O
results	O
on	O
diverse	O
split	O
of	O
PQ	B-DatasetName
and	O
PQL	B-DatasetName
datasets	O
are	O
provided	O
in	O
Table	O
2	O
.	O

Based	O
on	O
the	O
attention	O
map	O
A	O
,	O
the	O
joint	O
feature	O
is	O
obtained	O
as	O
follows	O
:	O
z	O

In	O
this	O
paper	O
,	O
we	O
focus	O
on	O
the	O
task	O
which	O
is	O
called	O
knowledge	B-TaskName
-	I-TaskName
based	I-TaskName
visual	I-TaskName
question	I-TaskName
answering	I-TaskName
,	O
To	O
answer	O
the	O
given	O
question	O
,	O
the	O
multiple	O
reasoning	O
evidences	O
(	O
marked	O
as	O
orange	O
)	O
are	O
required	O
.	O

Each	O
document	O
is	O
ensured	O
to	O
be	O
annotated	O
by	O
3	O
different	O
subjects	O
.	O

These	O
dissimilar	O
slots	O
bring	O
much	O
noise	O
,	O
which	O
makes	O
the	O
predictions	O
of	O
query	O
samples	O
inaccurate	O
.	O

Direct	O
applications	O
of	O
language	O
models	O
(	O
LM	O
)	O
include	O
predictive	O
text	O
,	O
authorship	O
attribution	O
,	O
and	O
dialog	O
systems	O
used	O
to	O
model	O
the	O
style	O
of	O
an	O
individual	O
or	O
profession	O
(	O
e.g.	O
,	O
therapist	O
,	O
counselor	O
)	O
.	O

As	O
shown	O
in	O
Figure	O
2(a	O
)	O
,	O
entity	O
linking	O
module	O
first	O
links	O
concepts	O
from	O
query	O
(	O
a	O
given	O
image	O
-	O
question	O
pair	O
)	O
to	O
knowledge	O
base	O
.	O

We	O
note	O
that	O
using	O
single	O
-	O
wordunit	O
-	O
based	O
input	O
format	O
for	O
both	O
knowledge	O
and	O
question	O
is	O
the	O
standard	O
settings	O
for	O
the	O
Transformer	B-MethodName
network	I-MethodName
and	O
using	O
hyperedge	O
-	O
based	O
input	O
format	O
for	O
both	O
is	O
the	O
proposed	O
model	O
,	O
Hypergraph	B-MethodName
Transformer	I-MethodName
.	O

First	O
,	O
we	O
see	O
that	O
adding	O
the	O
cooperative	B-HyperparameterValue
losses	I-HyperparameterValue
on	O
both	O
the	O
generator	O
and	O
the	O
critic	O
is	O
crucial	O
for	O
the	O
overall	O
performance	O
.	O

The	O
estimated	O
valueV	O
q	O
t	O
from	O
local	O
adaptation	O
helps	O
the	O
base	O
model	O
to	O
infer	O
the	O
final	O
outputŶ	O
q	O
t	O
.Experiments	O
on	O
personalized	B-TaskName
dialogue	I-TaskName
generation	I-TaskName
and	O
multi	O
-	O
domain	O
sentiment	B-TaskName
classification	I-TaskName
verify	O
our	O
model	O
on	O
text	B-TaskName
generation	I-TaskName
and	B-TaskName
classification	I-TaskName
,	O
respectively	O
,	O
where	O
we	O
use	O
Persona	B-DatasetName
-	I-DatasetName
Chat	I-DatasetName
and	O
ARSC	B-DatasetName
datasets	O
.	O

Our	O
objective	O
is	O
to	O
leverage	O
the	O
labeled	O
data	O
collected	O
from	O
simple	O
games	O
to	O
speed	O
up	O
RL	O
training	O
in	O
complex	O
games	O
,	O
thus	O
obtaining	O
an	O
agent	O
capable	O
of	O
complex	O
games	O
.	O

Furthermore	O
,	O
whenever	O
a	O
new	O
PLM	O
emerges	O
,	O
it	O
must	O
be	O
re	O
-	O
trained	O
to	O
create	O
more	O
advanced	O
domain	O
-	O
specific	O
models	O
.	O

This	O
gives	O
us	O
122	B-HyperparameterValue
K	I-HyperparameterValue
paraphrase	B-HyperparameterName
pairs	I-HyperparameterName
.	O

QRA	B-TaskName
produces	O
a	O
single	O
score	O
estimating	O
the	O
degree	O
of	O
reproducibility	O
of	O
a	O
given	O
system	O
and	O
evaluation	O
measure	O
,	O
on	O
the	O
basis	O
of	O
the	O
scores	O
from	O
,	O
and	O
differences	O
between	O
,	O
different	O
reproductions	O
.	O

In	O
total	O
,	O
we	O
obtain	O
8,283	O
hypothetical	O
questions	O
,	O
naming	O
it	O
as	O
TAT	B-DatasetName
-	I-DatasetName
HQA	I-DatasetName
.	O

This	O
problem	O
setting	O
is	O
an	O
important	O
application	O
of	O
text	B-TaskName
transfer	I-TaskName
,	O
as	O
enforcing	O
constraints	O
of	O
identity	O
can	O
help	O
maintain	O
the	O
brand	O
identity	O
when	O
the	O
product	O
descriptions	O
are	O
mapped	O
from	O
one	O
commercial	O
product	O
to	O
another	O
.	O

We	O
use	O
the	O
uncased	O
wordpiece	O
tokenizer	O
of	O
BERT	B-MethodName
with	O
30k	B-HyperparameterValue
vocabulary	B-HyperparameterName
.	O

Results	O
show	O
that	O
our	O
models	O
achieve	O
best	O
BLEU	B-MetricName
score	O
in	O
37	O
out	O
of	O
the	O
42	O
tests	O
splits	O
.	O

With	O
the	O
release	O
of	O
multi	O
-	O
domain	O
,	O
multi	O
-	O
turn	O
Multi	B-DatasetName
-	B-DatasetName
Woz2.0	I-DatasetName
dataset	O
(	O
Budzianowski	O
et	O
al	O
.	O
,	O
2018a	O
)	O
,	O
there	O
has	O
been	O
flurry	O
of	O
recent	O
works	O
,	O
of	O
which	O
Zhang	O
et	O
al	O
.	O
(	O
2019	O
)	O
uses	O
data	O
augmentation	O
.	O

Fig	O
.	O

In	O
pre	O
-	O
training	O
,	O
the	O
VGG-19	O
layers	O
are	O
first	O
pre	O
-	O
trained	O
on	O
ImageNet	B-MethodName
(	O
Deng	O
et	O
al	O
.	O
,	O
2009	O
)	O
and	O
the	O
image	O
features	O
further	O
go	O
through	O
a	O
1-	B-HyperparameterValue
layer	B-HyperparameterName
Bi	B-HyperparameterValue
-	I-HyperparameterName
LSTM	I-HyperparameterName
with	O
512	B-HyperparameterValue
hidden	B-HyperparameterName
units	I-HyperparameterName
per	I-HyperparameterName
direction	I-HyperparameterName
.	O

In	O
this	O
setting	O
we	O
use	O
both	O
stochastic	O
,	O
L	O
sto	O
and	O
deterministic	O
,	O
L	O
det	O
loss	O
functions	O
on	O
dialogue	O
act	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
our	O
approach	O
is	O
the	O
first	O
to	O
introduce	O
cooperative	B-HyperparameterValue
losses	I-HyperparameterValue
in	O
a	O
GAN	O
-	O
like	O
setup	O
for	O
NLG.Task	B-TaskName
Setup	O
:	O
We	O
consider	O
two	O
sets	O
of	O
sentences	O
(	O
or	O
corpora	O
)	O
S=	O
{	O
x	O
1	O
src	O
,	O
x	O
2	O
src	O
,	O
.	O

The	O
second	O
positional	O
i	O
d	O
represents	O
the	O
intra	O
-	O
span	O
position	O
.	O

However	O
,	O
generative	O
models	O
require	O
much	O
more	O
parameters	O
to	O
work	O
due	O
to	O
the	O
limit	O
of	O
unidirectional	O
attention	O
.	O

In	O
the	O
second	O
phase	O
,	O
we	O
deploy	O
the	O
agent	O
in	O
games	O
with	O
the	O
pretrained	O
modules	O
frozen	O
,	O
and	O
train	O
the	O
agent	O
through	O
reinforcement	O
learning	O
.	O

To	O
optimize	O
the	O
L2I	B-MethodName
module	O
,	O
we	O
incorporate	O
supervision	O
on	O
the	O
classifiers	O
in	O
the	O
tagging	O
head	O
and	O
deriving	O
head	O
.	O

We	O
introduced	O
the	O
world	O
-	O
perceiving	O
modules	O
,	O
which	O
are	O
capable	O
of	O
automatic	O
task	O
decomposition	O
and	O
action	O
pruning	O
through	O
answering	O
questions	O
about	O
the	O
environment	O
.	O

All	O
the	O
datasets	O
used	O
total	O
158	B-HyperparameterValue
GB	I-HyperparameterValue
of	O
uncompressed	B-HyperparameterName
texts	I-HyperparameterName
,	O
close	O
in	O
size	O
to	O
RoBERTa	B-MethodName
's	O
160	B-HyperparameterValue
GB	I-HyperparameterValue
datasets	O
.	O

Another	O
challenge	O
is	O
the	O
large	O
discrete	O
action	O
space	O
:	O
the	O
agent	O
may	O
waste	O
both	O
time	O
and	O
training	O
data	O
if	O
attempting	O
irrelevant	O
or	O
inferior	O
actions	O
(	O
Dulac	O
-	O
Arnold	O
et	O
al	O
.	O
,	O
2015;Zahavy	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

We	O
notice	O
that	O
GLM	B-MethodName
Doc	I-MethodName
slightly	O
underperforms	O
GLM	B-MethodName
Large	I-MethodName
,	O
which	O
is	O
consistent	O
with	O
our	O
observations	O
in	O
the	O
seq2seq	O
experiments	O
.	O

To	O
answer	O
this	O
question	O
for	O
a	O
given	O
specific	O
system	O
,	O
typically	O
(	O
Wieling	O
et	O
al	O
.	O
,	O
2018;Arhiliuc	O
et	O
al	O
.	O
,	O
2020;Popović	O
and	O
Belz	O
,	O
2021	O
)	O
an	O
original	O
study	O
is	O
selected	O
and	O
repeated	O
more	O
or	O
less	O
closely	O
,	O
before	O
comparing	O
the	O
results	O
obtained	O
in	O
the	O
original	O
study	O
with	O
those	O
obtained	O
in	O
the	O
repeat	O
,	O
and	O
deciding	O
whether	O
the	O
two	O
sets	O
of	O
results	O
are	O
similar	O
enough	O
to	O
support	O
the	O
same	O
conclusions	O
.	O

However	O
,	O
because	O
the	O
hidden	B-HyperparameterName
embedding	I-HyperparameterName
dimensions	I-HyperparameterName
of	O
teachers	O
and	O
students	O
are	O
different	O
in	O
our	O
setting	O
,	O
we	O
applied	O
a	O
linear	O
transformation	O
to	O
the	O
teacher	O
's	O
classification	O
embedding	O
to	O
match	O
the	O
dimension	O
with	O
the	O
student	O
model	O
.	O

We	O
first	O
assign	O
a	O
name	O
of	O
the	O
detected	O
faces	O
with	O
the	O
label	O
of	O
the	O
closest	O
distance	O
compared	O
to	O
all	O
of	O
the	O
face	O
embeddings	O
of	O
18,880	O
named	B-HyperparameterName
entities	O
.	O

Wang	O
et	O
al	O
.	O
(	O
2021	O
)	O
reuse	O
learned	O
features	O
stored	O
in	O
the	O
memory	O
on	O
the	O
few	O
-	O
shot	O
slot	O
tagging	O
.	O

LAVA	B-MethodName
(	O
Lubis	O
et	O
al	O
.	O
,	O
2020	O
)	O
,	O
reduces	O
the	O
action	O
space	O
of	O
policy	O
in	O
end	O
-	O
to	O
-	O
end	O
ToD	B-TaskName
,	O
by	O
using	O
the	O
latent	O
space	O
of	O
a	O
variational	O
model	O
with	O
an	O
informed	O
prior	O
.	O

The	O
choice	O
of	O
action	O
in	O
reward	O
function	O
R(s	O
t	O
,	O
a	O
t	O
,	O
g	O
)	O
can	O
either	O
be	O
dialogue	O
act	O
or	O
generate	O
response	O
,	O
we	O
refer	O
corresponding	O
variants	O
of	O
metrics	O
as	O
M	B-MetricName
(	O
act	O
)	O
and	O
M	B-MetricName
(	O
resp	O
)	O
.	O

I(Ŷ	O
q	O
i	O
;	O
[	O
D	O
s	O
i	O
,	O
M	O
i	O
]	O
|	O
θ	O
,	O
X	O
q	O
i	O
)	O
>	O
I(Ŷ	O
q	O
i	O
;	O
D	O
s	O
i	O
|	O
θ	O
,	O
X	O
q	O
i	O
)	O
,	O
(	O
In	O
the	O
meta	O
-	O
training	O
phase	O
(	O
shown	O
in	O
Alg	O
.	O

In	O
other	O
words	O
,	O
domain	O
-	O
specific	O
knowledge	O
can	O
be	O
transferred	O
into	O
advanced	O
models	O
without	O
a	O
time	O
-	O
consuming	O
pre-	O
training	O
and	O
perturbing	O
the	O
model	O
's	O
efficacy	O
in	O
the	O
general	O
domain	O
.	O

We	O
assume	O
that	O
each	O
word	O
unit	O
(	O
a	O
word	O
or	O
named	O
entity	O
)	O
of	O
the	O
question	O
is	O
defined	O
as	O
a	O
node	O
,	O
and	O
has	O
edges	O
to	O
adjacent	O
nodes	O
.	O

where	O
α	B-HyperparameterName
is	O
the	O
inner	B-HyperparameterName
loop	O
learning	B-HyperparameterName
rate	I-HyperparameterName
.	O

Inspired	O
by	O
Pattern	O
-	O
Exploiting	O
Training	O
(	O
PET	O
)	O
(	O
Schick	O
and	O
Schütze	O
,	O
2020a	O
)	O
,	O
we	O
reformulate	O
NLU	B-TaskName
tasks	O
as	O
manually	O
-	O
crafted	O
cloze	O
questions	O
that	O
mimic	O
human	O
language	O
.	O

In	O
the	O
second	O
example	O
,	O
our	O
model	O
attends	O
to	O
the	O
correct	O
knowledge	O
hyperedges	O
considering	O
the	O
multi	O
-	O
hop	O
facts	O
about	O
place	O
of	O
birth	O
of	O
the	O
people	O
shown	O
in	O
the	O
given	O
image	O
,	O
and	O
infers	O
the	O
correct	O
answer	O
.	O

On	O
other	O
metrics	O
we	O
perform	O
on	O
par	O
or	O
better	O
than	O
our	O
competing	O
systems	O
.	O

When	O
the	O
learned	O
model	O
ignores	O
support	O
sets	O
to	O
predict	O
query	O
sets	O
,	O
I(Ŷ	O
q	O
i	O
;	O
D	O
s	O
i	O
)	O
|θ	O
,	O
X	O
q	O
i	O
)	O
=	O
0	O
occurs	O
,	O
which	O
indicates	O
the	O
complete	O
memorization	O
overfitting	O
in	O
metalearning	O
(	O
Yin	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

ask	O
for	O
the	O
spicy	O
chicken	O
,	O
and	O
they	O
have	O
a	O
great	O
selection	O
.	O

friendly	O
staff	O
.	O

Hence	O
,	O
we	O
acquire	O
the	O
prediction	O
of	O
a	O
query	O
-	O
set	O
sample	O
viâ	O
Y	O
q	O
j	O
=	O
Decoder([V	O
q	O
j	O
;	O
Encoder(X	O
q	O
j	O
)	O
]	O
)	O
.	O

We	O
perform	O
a	O
data	O
quality	O
study	O
confirming	O
the	O
findings	O
of	O
Kreutzer	O
et	O
al	O
.	O
(	O
2021	O
)	O
.	O

Given	O
the	O
absence	O
of	O
an	O
Arabic	B-TaskName
QG	I-TaskName
dataset	O
,	O
we	O
create	O
a	O
new	O
Arabic	B-TaskName
QG	I-TaskName
dataset	O
(	O
ARGEN	B-DatasetName
QG	I-DatasetName
)	O
using	O
a	O
publicly	O
available	O
Arabic	B-TaskName
question	I-TaskName
answering	I-TaskName
(	O
QA	B-TaskName
)	O
resource	O
.	O

Reproduction	O
studies	O
are	O
becoming	O
more	O
common	O
in	O
Natural	B-TaskName
Language	I-TaskName
Processing	I-TaskName
(	O
NLP	O
)	O
,	O
with	O
the	O
first	O
shared	O
tasks	O
being	O
organised	O
,	O
including	O
RE	B-MethodName
-	I-MethodName
PROLANG	I-MethodName
(	O
Branco	O
et	O
al	O
.	O
,	O
2020	O
)	O
and	O
ReproGen	B-MethodName
(	O
Belz	O
et	O
al	O
.	O
,	O
2021b	O
)	O
.	O

Our	O
proposed	O
method	O
does	O
not	O
include	O
inference	O
or	O
judgments	O
about	O
individuals	O
and	O
does	O
not	O
generate	O
any	O
discriminatory	O
,	O
insulting	O
responses	O
.	O

best	O
mexican	O
food	O
in	O
the	O
area	O
.	O

In	O
addition	O
,	O
we	O
refine	O
a	O
list	O
of	O
de	O
-	O
tected	O
named	O
entities	O
by	O
matching	O
the	O
associated	O
image	O
caption	O
(	O
i.e.	O
,	O
Wikipedia	O
caption	O
)	O
.	O

Positive	O
<	O
l	O
a	O
t	O
e	O
x	O
i	O
t	O
s	O
h	O
a	O
1	O
_	O
b	O
a	O
s	O
e	O
6	O
4	O
=	O
"	O
c	O
b	O
5	O
S	O
9	O

We	O
implement	O
our	O
model	O
based	O
on	O
the	O
transformer	O
(	O
Dehghani	O
et	O
al	O
.	O
,	O
2018;Vaswani	O
et	O
al	O
.	O
,	O
2017	O
)	O
with	O
pre	O
-	O
trained	O
Glove	O
embedding	O
(	O
Pennington	O
et	O
al	O
.	O
,	O
2014	O
)	O
following	O
(	O
Madotto	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

Inspired	O
by	O
the	O
observation	O
that	O
human	O
be-	O
The	O
decision	O
making	O
process	O
.	O

1	O
shows	O
our	O
model	O
architecture	O
.	O

Meanwhile	O
,	O
HAN	B-MethodName
employs	O
stochastic	O
graph	O
walk	O
in	O
a	O
knowledge	O
and	O
question	O
graph	O
to	O
encode	O
high	O
-	O
order	O
semantics	O
(	O
e.g.	O
,	O
knowledge	O
facts	O
and	O
question	O
phrases	O
)	O
,	O
and	O
considers	O
attention	O
scores	O
between	O
knowledge	O
facts	O
and	O
question	O
phrases	O
.	O

To	O
further	O
investigate	O
the	O
difference	O
in	O
answering	O
factual	O
and	O
hypothetical	O
questions	O
,	O
we	O
test	O
TAGOP	B-MethodName
-	I-MethodName
L2I	I-MethodName
on	O
TAT	B-DatasetName
-	I-DatasetName
QA	I-DatasetName
.	O

(	O
Auer	O
et	O
al	O
.	O
,	O
2007	O
)	O
,	O
ConceptNet	B-DatasetName
(	O
Liu	O
and	O
Singh	O
,	O
2004	O
)	O
,	O
and	O
WebChild	B-DatasetName
(	O
Tandon	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

The	O
sample	O
selection	O
is	O
according	O
to	O
a	O
diversity	O
-	O
based	O
selection	O
criterion	O
(	O
Xie	O
et	O
al	O
.	O
,	O
2015	O
)	O
to	O
ensure	O
the	O
diversity	O
and	O
representativeness	O
of	O
the	O
memory	O
content	O
.	O

Arabic	B-DatasetName
SemEval	I-MethodName
Paraphrasing	I-MethodName
(	O
ASEP	B-DatasetName
)	O
.	O

For	O
DST	B-TaskName
and	O
response	B-TaskName
generation	I-TaskName
,	O
we	O
retain	O
the	O
cross	B-HyperparameterValue
entropy	I-HyperparameterValue
loss	I-HyperparameterValue
as	O
is	O
from	O
DAMD	B-MethodName
(	O
Zhang	O
et	O
al	O
.	O
,	O
2019).On	O
the	O
other	O
extreme	O
of	O
model	O
complexity	O
,	O
we	O
use	O
the	O
Task	B-TaskName
oriented	I-MethodName
Dialogue	I-TaskName
model	O
,	O
MinTL	B-MethodName
(	O
Lin	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

Let	O
λ	B-HyperparameterName
enc	O
,	O
λ	B-HyperparameterName
cross	O
,	O
and	O
λ	B-HyperparameterName
dec	O
denote	O
the	O
attention	B-HyperparameterName
temperature	I-HyperparameterName
coefficient	I-HyperparameterName
of	O
the	O
encoder	O
self	O
-	O
attention	O
module	O
,	O
the	O
decoder	O
cross	O
-	O
attention	O
module	O
,	O
and	O
the	O
decoder	O
self	O
-	O
attention	O
module	O
,	O
respectively	O
.	O

We	O
use	O
the	O
similarity	O
-	O
based	O
answer	O
predictor	O
for	O
KVQA	B-DatasetName
,	O
and	O
MLP	O
for	O
the	O
others	O
.	O

The	O
key	O
to	O
achieving	O
counterfactual	O
thinking	O
in	O
NDR	B-TaskName
lies	O
in	O
:	O
1	O
)	O
parsing	O
the	O
assumption	O
to	O
identify	O
the	O
target	O
fact	O
to	O
intervene	O
;	O
and	O
2	O
)	O
deriving	O
the	O
assumed	O
value	O
to	O
construct	O
the	O
counterfactual	O
context	O
.	O

Her	O
/	O
his	O
task	O
is	O
to	O
follow	O
the	O
recipe	O
to	O
prepare	O
the	O
meal	O
.	O

they	O
have	O
good	O
margaritas	O
and	O
good	O
food	O
.	O

T	O
where	O
x	O
v	O
is	O
the	O
v	O
-	O
th	O
word	O
embedding	O
of	O
each	O
en	O
-	O
tity	O
in	O
the	O
knowledge	O
and	O
question	O
graph	O
,	O
a	O

Besides	O
,	O
our	O
method	O
prevails	O
when	O
pre	O
-	O
trained	O
on	O
simple	O
tasks	O
rather	O
than	O
complicated	O
ones	O
,	O
making	O
it	O
more	O
feasible	O
for	O
human	O
to	O
interact	O
and	O
annotate	O
(	O
Arumugam	O
et	O
al	O
.	O
,	O
2017;Mirchandani	O
et	O
al	O
.	O
,	O
2021	O
)	O
.	O

For	O
instance	O
,	O
in	O
personalized	B-TaskName
dialogue	I-TaskName
generation	I-TaskName
,	O
this	O
implies	O
that	O
the	O
dialog	O
model	O
can	O
not	O
adapt	O
to	O
individual	O
users	O
based	O
on	O
short	O
conversation	O
histories	O
and	O
hence	O
fails	O
to	O
generate	O
personalized	O
responses	O
.	O

Especially	O
,	O
when	O
we	O
convert	O
the	O
one	O
of	O
both	O
hyperedge	O
-	O
level	O
representation	O
to	O
single	O
-	O
word	O
-	O
unit	O
-	O
based	O
representation	O
,	O
the	O
mean	B-HyperparameterName
accuracy	I-MetricName
of	O
QA	B-TaskName
is	O
82.7	B-MetricValue
%	I-MetricValue
and	O
88.7	B-MetricValue
%	I-MetricValue
,	O
respectively	O
.	O

Data	O
samples	O
are	O
extracted	O
from	O
the	O
Dev	O
datasets	O
.	O

We	O
consider	O
the	O
following	O
two	O
objectives	O
:	O

Its	O
results	O
are	O
not	O
too	O
bad	O
,	O
indicating	O
that	O
the	O
memory	O
module	O
helps	O
to	O
mitigate	O
the	O
memorization	O
overfitting	O
problem	O
.	O

Our	O
language	O
models	O
are	O
not	O
pretrained	O
on	O
foreign	O
data	O
,	O
but	O
we	O
include	O
vocabulary	O
from	O
11	O
foreign	O
languages	O
.	O

If	O
we	O
do	O
not	O
limit	O
the	O
use	O
of	O
human	O
knowledge	O
in	O
this	O
phase	O
,	O
we	O
can	O
also	O
treat	O
T	O
t	O
as	O
a	O
goal	O
with	O
either	O
hand	O
-	O
crafted	O
(	O
Jiang	O
et	O
al	O
.	O
,	O
2019	O
)	O
or	O
learnt	O
reward	O
function	O
(	O
Colas	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

In	O
particular	O
,	O
entity	O
anonymization	O
is	O
applied	O
to	O
all	O
relation	B-TaskName
extraction	I-TaskName
datasets	O
,	O
which	O
replace	O
the	O
entity	O
mentions	O
with	O
anonymous	O
tokens	O
(	O
e.g.	O
,	O
@GENE$	O
,	O
@DISEASE$	O
)	O
to	O
avoid	O
confusion	O
in	O
using	O
complex	O
entity	O
names	O
.	O

The	O
two	O
tasks	O
correspond	O
to	O
two	O
directions	O
of	O
search	O
(	O
video−→text	O
and	O
text−→video	O
)	O
,	O
as	O
is	O
standard	O
practice	O
in	O
other	O
retrieval	O
work	O
such	O
as	O
video	B-TaskName
-	I-TaskName
text	I-TaskName
search	I-TaskName
(	O
Zhang	O
et	O
al	O
.	O
,	O
2018;Ranjay	O
et	O
al	O
.	O
,	O
2017;Ging	O
et	O
al	O
.	O
,	O
2020).We	O
propose	O
a	O
single	O
model	O
,	O
FSS	B-MethodName
-	I-MethodName
Net	I-MethodName
(	O
for	O
"	O
Finger	B-MethodName
-	I-MethodName
Spelling	I-MethodName
Search	I-MethodName
Network	I-MethodName
"	O
)	O
,	O
summarized	O
in	O
Fig-	B-MethodName
Image	O
encoding	O
The	O
input	O
image	O
frames	O
are	O
encoded	O
into	O
a	O
sequence	O
of	O
feature	O
vectors	O
via	O
an	O
image	O
encoder	O
,	O
which	O
consists	O
of	O
the	O
VGG-19	O
(	O
Simonyan	O
and	O
Zisserman	O
,	O
2015	O
)	O
convolutional	O
layers	O
followed	O
by	O
a	O
Bi	O
-	O
LSTM	O
.	O

For	O
all	O
comparative	O
models	O
,	O
we	O
use	O
the	O
same	O
knowledge	O
hypergraph	O
extracted	O
by	O
the	O
3	O
-	O
hop	O
graph	O
walk	O
.	O

It	O
is	O
more	O
accurate	O
than	O
ours	O
around	O
9.4	B-MetricValue
%	I-MetricValue
in	O
the	O
recall	O
metric	O
.	O

The	O
weight	B-HyperparameterName
decay	I-HyperparameterName
is	O
set	O
to	O
0.0001	B-HyperparameterValue
.	O

1	O
2	O

To	O
this	O
end	O
,	O
we	O
propose	O
an	O
end	O
-	O
to	O
-	O
end	O
model	O
,	O
FSS	B-MethodName
-	I-MethodName
Net	I-MethodName
,	O
which	O
jointly	O
detects	O
fingerspelling	O
from	O
unconstrained	O
signing	O
video	O
and	O
matches	O
it	O
to	O
text	O
queries	O
.	O

Both	O
enhancements	O
are	O
based	O
on	O
pre	O
-	O
trained	O
language	O
models	O
.	O

Automatic	O
processing	O
of	O
sign	O
language	O
videos	O
"	O
in	O
the	O
wild	O
"	O
has	O
not	O
been	O
addressed	O
until	O
recently	O
,	O
and	O
is	O
still	O
restricted	O
to	O
tasks	O
like	O
isolated	O
sign	O
recognition	O
Joze	O
and	O
Koller	O
,	O
2019;Li	O
et	O
al	O
.	O
,	O
2020	O
)	O
and	O
fingerspelling	O
recognition	O
(	O
Shi	O
et	O
al	O
.	O
,	O
2018(Shi	O
et	O
al	O
.	O
,	O
,	O
2019	O
.	O

For	O
our	O
analysis	O
,	O
we	O
randomly	O
sample	O
1	O
M	O
paragraphs	O
from	O
the	O
Arabic	O
part	O
of	O
mC4	B-DatasetName
.	O

We	O
postulate	O
that	O
the	O
failure	O
is	O
due	O
to	O
unable	O
of	O
imagining	O
the	O
counterfactual	O
context	O
according	O
to	O
the	O
assumption	O
(	O
Figure	O
1	O
)	O
.	O

Sec	O
.	O

We	O
acknowledge	O
our	O
models	O
may	O
still	O
be	O
misused	O
in	O
real	O
world	O
.	O

The	O
test	O
set	O
is	O
constructed	O
by	O
including	O
the	O
9,076	O
articles	O
published	O
after	O
January	O
1	O
,	O
2007	O
.	O

Both	O
empirical	O
and	O
theoretical	O
results	O
demonstrate	O
that	O
our	O
method	O
MemIML	B-MethodName
effectively	O
alleviates	O
the	O
memorization	O
overfitting	O
problem	O
.	O

We	O
observe	O
that	O
using	O
pseudo	O
-	O
labeling	O
methods	O
with	O
higher	O
attention	B-HyperparameterName
temperatures	I-HyperparameterName
consistently	O
improves	O
over	O
its	O
counterpart	O
with	O
normal	O
attention	B-HyperparameterName
temperatures	I-HyperparameterName
(	O
Regular	O
)	O
across	O
all	O
three	O
datasets	O
,	O
and	O
the	O
differences	O
between	O
them	O
are	O
almost	O
always	O
significant	O
measured	O
with	O
the	O
ROUGE	B-MetricName
script	I-MetricName
5	O
(	O
see	O
details	O
in	O
Table	O
2	O
)	O
.	O

Multitask	O
.	O

Then	O
we	O
build	O
T	O
as	O
the	O
Cartesian	O
product	O
of	O
the	O
ingredients	O
and	O
the	O
verbs	O
{	O
chop	O
,	O
dice	O
,	O
slice	O
,	O
fry	O
,	O
get	O
,	O
grill	O
,	O
roast	O
}	O
plus	O
two	O
special	O
subtasks	O
"	O
get	O
knife	O
"	O
and	O
"	O
make	O
meal	O
"	O
.	O

In	O
the	O
outer	O
loop	O
,	O
each	O
sample	O
of	O
the	O
query	O
set	O
reads	O
the	O
memory	O
to	O
retrieve	O
the	O
most	O
similar	O
memory	O
slots	O
.	O

For	O
example	O
,	O
the	O
false	O
predictions	O
made	O
by	O
the	O
binary	O
classifier	O
in	O
the	O
task	O
selector	O
may	O
lead	O
to	O
a	O
wrong	O
T	O
t	O
,	O
which	O
affects	O
A	O
t	O
and	O
a	O
t	O
in	O
turn	O
.	O

We	O
design	O
AR	B-DatasetName
-	I-DatasetName
GEN	I-DatasetName
using	O
both	O
existing	O
datasets	O
and	O
new	O
datasets	O
that	O
we	O
create	O
for	O
this	O
work	O
.	O

The	O
base	O
model	O
is	O
a	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O
2019	O
)	O
followed	O
by	O
a	O
fully	O
-	O
connected	O
network	O
.	O

We	O
devise	O
a	O
Learning	O
to	O
Imagine	O
module	O
to	O
model	O
counterfactual	O
thinking	O
(	O
Section	O
3.1	O
)	O
,	O
and	O
then	O
incorporate	O
the	O
L2I	B-MethodName
module	O
(	O
Section	O
3.2	O
)	O
into	O
existing	O
NRD	O
methods	O
,	O
followed	O
by	O
a	O
discussion	O
about	O
potential	O
extensions	O
(	O
Section	O
3.3).Functionally	O
speaking	O
,	O
the	O
L2I	B-MethodName
module	O
aims	O
to	O
construct	O
a	O
counterfactual	O
context	O
based	O
on	O
the	O
factual	O
context	O
and	O
the	O
assumption	O
.	O

We	O
report	O
the	O
performance	O
of	O
standard	O
finetuning	O
(	O
i.e.	O
classification	O
on	O
the	O
[	O
CLS	O
]	O
token	O
representation	O
)	O
.	O

Our	O
work	O
is	O
closely	O
related	O
to	O
task	B-TaskName
decomposition	I-TaskName
(	O
Oh	O
et	O
al	O
.	O
,	O
2017;Shiarlis	O
et	O
al	O
.	O
,	O
2018;Sohn	O
et	O
al	O
.	O
,	O
2018	O
)	O
and	O
hierarchical	O
reinforcement	O
learning	O
(	O
Dayan	O
and	O
Hinton	O
,	O
1992;Kulkarni	O
et	O
al	O
.	O
,	O
2016;Vezhnevets	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

All	O
the	O
numbers	O
of	O
CASPI	B-MethodName
reported	O
in	O
this	O
work	O
are	O
median	O
of	O
5	B-HyperparameterValue
runs	O
with	O
different	O
seeds	O
.	O

We	O
can	O
also	O
train	O
a	O
task	O
scorer	O
via	O
a	O
metapolicy	O
for	O
adaptive	O
task	O
selection	O
(	O
Xu	O
et	O
al	O
.	O
,	O
2021).After	O
obtaining	O
the	O
subtask	O
T	O
t	O
,	O
we	O
conduct	O
action	O
pruning	O
conditioned	O
on	O
it	O
(	O
or	O
on	O
both	O
T	O
t	O
and	O
o	O
t	O
)	O
to	O
reduce	O
the	O
action	O
space	O
,	O
tackling	O
the	O
challenge	O
of	O
large	O
action	O
space	O
.	O

We	O
explore	O
the	O
trade	O
-	O
offs	O
between	O
the	O
amount	O
of	O
available	O
data	O
from	O
existing	O
users	O
,	O
the	O
number	O
of	O
existing	O
users	O
and	O
new	O
users	O
,	O
and	O
how	O
our	O
similarity	O
metrics	O
and	O
methods	O
scale	O
.	O

Note	O
that	O
RoBERTa	B-MethodName
-	I-MethodName
PM	I-MethodName
has	O
an	O
advantage	O
in	O
the	O
i2b2	B-DatasetName
task	O
since	O
its	O
pre	O
-	O
training	O
data	O
contains	O
MIMIC	B-DatasetName
-	I-DatasetName
III	I-DatasetName
clinical	O
text	O
data	O
,	O
while	O
our	O
teacher	O
model	O
was	O
pretrained	O
with	O
only	O
biomedical	O
texts	O
.	O

The	O
ability	O
of	O
HQA	B-TaskName
will	O
undoubtedly	O
enhance	O
the	O
practical	O
use	O
of	O
NDR	B-TaskName
due	O
to	O
the	O
universality	O
of	O
hypothetical	O
questions	O
.	O

Paraphrasing	B-TaskName
,	O
Transliteration	B-TaskName
,	O
and	O
Title	B-TaskName
Generation	I-TaskName
Output	O
.	O

We	O
refer	O
to	O
this	O
dataset	O
as	O
ARGEN	B-DatasetName
TR	I-DatasetName
.Baselines	O
and	O
Procedure	O
.	O

We	O
provide	O
a	O
summary	O
of	O
the	O
dataset	O
statistics	O
in	O
Table	O
1	O
.	O

Our	O
code	O
is	O
available	O
at	O
https://github	O
.	O

Fig	O
.	O

It	O
clearly	O
demonstrates	O
the	O
advantage	O
of	O
our	O
method	O
in	O
NLU	B-TaskName
tasks	O
.	O

)	O
could	O
be	O
found	O
at	O
Appendix	O
B.	O
We	O
train	O
the	O
task	O
selector	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
256	B-HyperparameterValue
,	O
and	O
the	O
action	O
(	O
Adhikari	O
et	O
al	O
.	O
,	O
2020	O
)	O
to	O
conduct	O
reinforcement	O
learning	O
.	O

Although	O
pre	O
-	O
trained	O
with	O
∼	O
49	B-HyperparameterValue
%	I-HyperparameterValue
less	O
data	O
,	O
our	O
new	O
models	O
perform	O
significantly	O
better	O
than	O
mT5	B-MethodName
on	O
all	O
ARGEN	B-DatasetName
tasks	O
(	O
in	O
52	O
out	O
of	O
59	O
test	O
sets	O
)	O
and	O
set	O
several	O
new	O
SOTAs	O
.	O

,	O
post	O
-	O
update	O
)	O
model	O
f	O
θ	O
i	O
is	O
first	O
obtained	O
for	O
each	O
task	O
T	O
i	O
via	O
gradient	O
descent	O
over	O
its	O
support	O
set	O
D	O
s	O
i	O
.	O

A	O
n	B-HyperparameterValue
-	I-HyperparameterValue
gram	O
phrase	O
is	O
considered	O
as	O
a	O
hyperedge	O
in	O
the	O
question	O
hypergraph	O
(	O
see	O
Figure	O
2(b)).To	O
consider	O
high	O
-	O
order	O
associations	O
between	O
knowledge	O
and	O
question	O
,	O
we	O
devise	O
structural	O
semantic	O
matching	O
between	O
the	O
query	O
-	O
aware	O
knowledge	O
hypergraph	O
and	O
the	O
question	O
hypergraph	O
.	O

Arbitrary	O
methods	O
can	O
be	O
used	O
for	O
optimizing	O
Adhikari	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

It	O
includes	O
19	O
different	O
datasets	O
with	O
59	B-HyperparameterValue
test	B-HyperparameterName
splits	I-HyperparameterName
and	O
covers	O
seven	O
tasks	O
:	O
machine	B-TaskName
translation	I-TaskName
(	O
MT	B-TaskName
)	O
,	O
codeswitched	B-TaskName
translation	I-TaskName
(	O
CST	B-TaskName
)	O
,	O
text	B-TaskName
summarization	I-TaskName
(	O
TS	B-TaskName
)	O
,	O
news	B-TaskName
title	I-TaskName
generation	I-TaskName
(	O
NGT	B-TaskName
)	O
,	O
question	B-TaskName
generation	I-TaskName
(	O
QG	B-TaskName
)	O
,	O
transliteration	B-TaskName
(	O
TR	B-TaskName
)	O
,	O
and	O
paraphrasing	B-TaskName
(	O
PPH	B-TaskName
)	O
.	O

In	O
this	O
way	O
,	O
the	O
model	O
has	O
to	O
access	O
the	O
support	O
set	O
by	O
memory	O
imitation	O
each	O
time	O
it	O
makes	O
a	O
prediction	O
on	O
a	O
query	O
-	O
set	O
sample	O
,	O
hence	O
it	O
's	O
no	O
longer	O
feasible	O
for	O
the	O
model	O
to	O
memorize	O
all	O
meta	O
tasks	O
.	O

T5	B-MethodName
has	O
no	O
direct	O
match	O
in	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
for	O
BERT	B-MethodName
Large	I-MethodName
,	O
so	O
we	O
present	O
the	O
results	O
of	O
both	O
T5	B-MethodName
Base	I-MethodName
(	O
220	B-HyperparameterValue
M	I-HyperparameterValue
parameters	B-HyperparameterName
)	O
and	O
T5	B-MethodName
Large	I-MethodName
(	O
770	B-HyperparameterValue
M	I-HyperparameterValue
parameters	B-HyperparameterName
)	O
.	O

The	O
result	O
in	O
Figure	O
4	O
shows	O
that	O
training	O
on	O
TAT	B-DatasetName
-	I-DatasetName
HQA	I-DatasetName
causes	O
a	O
performance	O
drop	O
in	O
counting	O
,	O
span	O
and	O
multi	O
-	O
span	O
groups	O
of	O
TAT	B-DatasetName
-	I-DatasetName
QA	I-DatasetName
,	O
and	O
performs	O
similar	O
on	O
the	O
in	O
arithmetic	O
group	O
.	O

1	O
The	O
code	O
and	O
pre	O
-	O
trained	O
models	O
are	O
available	O
at	O
https	O
:	O
//github.com	O
/	O
THUDM	B-DatasetName
/	I-MethodName
GLM	B-MethodName
In	O
general	O
,	O
existing	O
pretraining	O
frameworks	O
can	O
be	O
categorized	O
into	O
three	O
families	O
:	O
autoregressive	O
,	O
autoencoding	O
,	O
and	O
encoder	O
-	O
decoder	O
models	O
.	O

(	O
2	O
)	O
Knowledge	O
base	O
information	O
is	O
not	O
well	O
exploited	O
and	O
incorporated	O
into	O
semantic	O
parsing	O
.	O

The	O
ASL	O
fingerspelling	O
alphabet	O
,	O
from	O
(	O
Keane	O
,	O
2014	O
)	O
Table	O
6	O
shows	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
video	B-HyperparameterName
clips	I-HyperparameterName
in	O
the	O
two	O
datasets	O
.	O

During	O
inference	O
,	O
we	O
need	O
to	O
either	O
know	O
or	O
enumerate	O
the	O
length	O
of	O
the	O
answer	O
,	O
the	O
same	O
problem	O
as	O
BERT	B-MethodName
.	O

ARAE	B-MethodName
consists	O
of	O
an	O
auto	O
-	O
encoder	O
with	O
a	O
deterministic	O
encoder	O
enc	O
θ	O
:	O
X	O
→	O
Z	O
that	O
encodes	O
sentences	O
into	O
a	O
latent	O
space	O
;	O
i.e.	O
,	O
z	O
=	O
enc	O
θ	O
(	O
x	O
)	O
∼	O
P	O
z	O
,	O
and	O
a	O
conditional	O
decoder	O
p	O
φ	O
(	O
x|z	O
)	O
that	O
generates	O
a	O
sentence	O
given	O
a	O
latent	O
code	O
.	O

The	O
qualitative	O
results	O
indicate	O
that	O
our	O
model	O
draws	O
reasonable	O
inferences	O
across	O
diverse	O
question	O
categories	O
.	O

The	O
aim	O
of	O
the	O
task	O
selector	O
is	O
to	O
identify	O
a	O
subset	O
of	O
available	O
subtasks	O
T	O
t	O
⊆	O
T	O
,	O
and	O
then	O
select	O
one	O
subtask	O
T	O
t	O
∈	O
T	O
t	O
.	O

Meta	O
-	O
Learning	O
aims	O
to	O
improve	O
the	O
learning	O
algorithm	O
itself	O
based	O
on	O
the	O
previously	O
learned	O
experience	O
(	O
Thrun	O
and	O
Pratt	O
,	O
1998;Hospedales	O
et	O
al	O
.	O
,	O
2021	O
)	O
.	O

For	O
example	O
,	O
the	O
AP@IoU	B-MetricName
of	O
Ext	B-DatasetName
-	I-MethodName
Det	I-MethodName
(	O
0.344	B-HyperparameterValue
)	O
is	O
an	O
order	O
of	O
magnitude	O
higher	O
than	O
that	O
of	O
Attn	B-MethodName
-	I-MethodName
KWS	I-MethodName
(	O
0.035	B-HyperparameterValue
)	O
while	O
their	O
FVS	B-DatasetName
mAP	I-MethodName
results	O
are	O
much	O
closer	O
(	O
0.593	B-HyperparameterValue
vs.	O
0.573	B-HyperparameterValue
)	O
.	O

We	O
find	O
that	O
removing	O
the	O
2D	B-HyperparameterValue
positional	I-HyperparameterValue
encoding	I-HyperparameterName
leads	O
to	O
lower	O
accuracy	B-MetricName
and	O
higher	O
perplexity	B-MetricName
in	O
language	O
modeling	O
.	O

At	O
test	O
time	O
,	O
we	O
use	O
beam	O
search	O
to	O
generate	O
a	O
list	O
of	O
hypothesesŵ	O
1	O
:	O
M	O
for	O
the	O
target	O
video	O
clip	O
I	O
1	O
:	O
T	O
.	O

Specifically	O
,	O
we	O
first	O
introduce	O
the	O
knowledge	O
distillation	O
to	O
build	O
entity	O
recognizer	O
and	O
similarity	O
evaluator	O
teachers	O
in	O
the	O
source	O
language	O
and	O
transfer	O
the	O
learned	O
patterns	O
to	O
the	O
student	O
in	O
the	O
target	O
language	O
.	O

These	O
methods	O
mainly	O
adopt	O
an	O
iterative	O
message	O
passing	O
process	O
to	O
propagate	O
information	O
between	O
adjacent	O
nodes	O
in	O
the	O
graph	O
.	O

Multi	O
-	O
hop	O
graph	O
walk	O
connects	O
multiple	O
facts	O
by	O
setting	O
the	O
arrival	O
node	O
(	O
tail	O
)	O
of	O
the	O
preceding	O
walk	O
as	O
the	O
starting	O
(	O
head	O
)	O
node	O
of	O
the	O
next	O
walk	O
,	O
thus	O
,	O
n	O
-	O
hop	O
graph	O
walk	O
combines	O
n	O
facts	O
as	O
a	O
hyperedge	O
.	O

We	O
can	O
directly	O
apply	O
the	O
pretrained	O
GLM	B-MethodName
for	O
unconditional	B-TaskName
generation	I-TaskName
,	O
or	O
finetune	O
it	O
on	O
downstream	O
conditional	B-TaskName
generation	I-TaskName
tasks	O
.	O

Although	O
the	O
T5	B-MethodName
model	O
,	O
originally	O
pre	O
-	O
trained	O
for	O
English	O
,	O
was	O
recently	O
extended	O
to	O
the	O
multilingual	O
setting	O
as	O
mT5	B-MethodName
(	O
Xue	O
et	O
al	O
.	O
,	O
2020	O
)	O
,	O
it	O
is	O
not	O
clear	O
how	O
suited	O
it	O
is	O
to	O
individual	O
languages	O
(	O
and	O
varieties	O
of	O
these	O
languages	O
)	O
.	O

External	O
Memory	O
for	O
Few	O
-	O
shot	O
Learning	O
.	O

We	O
verify	O
the	O
effectiveness	O
of	O
the	O
interpolation	O
in	O
Appendix	O
.	O

In	O
light	O
of	O
these	O
findings	O
,	O
we	O
will	O
consider	O
more	O
powerful	O
pre	O
-	O
training	O
methods	O
as	O
a	O
future	O
direction	O
.	O

The	O
student	O
model	O
learns	O
two	O
source	O
language	O
patterns	O
of	O
entity	O
recognition	O
and	O
entity	O
similarity	O
evaluation	O
.	O

We	O
then	O
use	O
a	O
similar	O
way	O
to	O
obtain	O
a	O
changeable	O
task	O
set	O
,	O
which	O
is	O
a	O
combination	O
of	O
the	O
verb	O
set	O
{	O
chop	O
,	O
dice	O
,	O
slice	O
,	O
fry	O
,	O
make	O
,	O
get	O
,	O
grill	O
,	O
roast	O
}	O
and	O
the	O
ingredient	O
set	O
,	O
where	O
the	O
construction	O
details	O
are	O
provided	O
in	O
Appendix	O
B.	O
Table	O
4	O
and	O
Table	O
5	O
show	O
the	O
KG	O
-	O
based	O
observations	O
o	O
t	O
,	O
corresponding	O
subtask	O
candidates	O
T	O
and	O
action	O
candidates	O
A.	O
Table	O
6	O
and	O
Table	O
7	O
show	O
more	O
examples	O
of	O
subtasks	O
and	O
actions	O
,	O
respectively	O
.	O

Then	O
in	O
the	O
second	O
phase	O
,	O
the	O
action	O
selector	O
is	O
fine	O
-	O
tuned	O
through	O
reinforcement	O
learning	O
.	O

In	O
addition	O
,	O
the	O
governor	O
can	O
appoint	O
members	O
of	O
the	O
Wyoming	O
house	O
of	O
representatives	O
.	O

However	O
,	O
this	O
usage	O
simply	O
aggre	O
-	O
gates	O
the	O
support	O
set	O
information	O
into	O
the	O
query	O
set	O
,	O
which	O
is	O
not	O
as	O
precise	O
as	O
learning	O
the	O
information	O
required	O
by	O
the	O
query	O
set	O
itself	O
.	O

For	O
the	O
3	O
multi	O
-	O
token	O
tasks	O
,	O
we	O
use	O
the	O
sum	O
of	O
the	O
log	O
-	O
probabilities	O
of	O
the	O
verbalizer	O
tokens	O
.	O

To	O
facilitate	O
comparison	O
,	O
the	O
network	O
architecture	O
for	O
the	O
visual	O
and	O
text	O
encoding	O
in	O
all	O
baselines	O
is	O
the	O
same	O
as	O
in	O
FSS	B-MethodName
-	I-MethodName
Net	I-MethodName
.	O

my	O
chicken	O
chow	O
mean	O
fried	O
rice	O
just	O
looked	O
and	O
tasted	O
like	O
last	O
weeks	O
rice	O
.	O

First	O
,	O
XLNet	B-MethodName
uses	O
the	O
original	O
position	O
encodings	O
before	O
corruption	O
.	O

In	O
1966	O
,	O
the	O
NFL	O
gave	O
players	O
$	O
300,000	O
a	O
season	O
to	O
play	O
football	O
.	O

Our	O
work	O
aims	O
at	O
uncovering	O
the	O
extent	O
to	O
which	O
mT5	B-MethodName
can	O
serve	O
Arabic	O
's	O
different	O
varieties	O
.	O

To	O
leverage	O
the	O
similarity	O
between	O
the	O
tokens	O
of	O
the	O
source	O
languages	O
,	O
we	O
design	O
an	O
multiple	O
-	O
task	O
and	O
multiple	O
-	O
teacher	O
model	O
(	O
short	O
as	O
MTMT	B-MethodName
,	O
as	O
shown	O
in	O
Figure	O
1	O
)	O
,	O
which	O
helps	O
the	O
NER	B-TaskName
learning	O
process	O
on	O
the	O
target	O
languages	O
.	O

The	O
experimental	O
results	O
from	O
the	O
biomedical	O
,	O
clinical	O
,	O
and	O
financial	O
domain	O
downstream	O
tasks	O
demonstrated	O
that	O
our	O
proposed	O
framework	O
could	O
transfer	O
domain	O
-	O
specific	O
knowledge	O
into	O
a	O
PLM	O
,	O
while	O
preserving	O
its	O
own	O
expressive	O
advantages	O
without	O
any	O
further	O
pre	O
-	O
training	O
with	O
additional	O
in	O
-	O
domain	O
data	O
.	O

Since	O
these	O
MT	B-TaskName
models	O
are	O
only	O
fine	O
-	O
tuned	O
on	O
parallel	O
monolingual	O
data	O
,	O
we	O
refer	O
to	O
these	O
experiments	O
as	O
zeroshot	O
.	O

We	O
propose	O
Learning	O
to	O
Imagine	O
,	O
where	O
the	O
counterfactual	O
thinking	O
is	O
implemented	O
with	O
two	O
intervening	O
steps	O
:	O
1	O
)	O
identifying	O
the	O
facts	O
to	O
intervene	O
,	O
and	O
2	O
)	O
deriving	O
the	O
result	O
of	O
intervention	O
.	O

In	O
this	O
paper	O
,	O
we	O
consider	O
approaches	O
to	O
finetuning	O
and	O
interpolation	O
that	O
are	O
novel	O
in	O
that	O
they	O
leverage	O
data	O
from	O
similar	O
users	O
to	O
boost	O
personalized	O
LM	O
performance	O
.	O

More	O
details	O
on	O
contrasting	O
the	O
merits	O
and	O
limitations	O
of	O
these	O
methods	O
can	O
be	O
found	O
in	O
Sec	O
:	O
A.1	O

Model	O
architecture	O
All	O
models	O
are	O
implemented	O
based	O
on	O
GATA	B-MethodName
's	O
released	O
code	O
¶	O
.	O

Note	O
that	O
trivially	O
,	O
we	O
have	O

Detailed	O
performance	O
.	O
To	O
further	O
investigate	O
the	O
effectiveness	O
of	O
the	O
proposed	O
L2I	B-MethodName
module	O
,	O
we	O
perform	O
a	O
detailed	O
comparison	O
between	O
TAGOP	B-MethodName
-	I-MethodName
L2I	I-MethodName
and	O
TAGOP	B-MethodName
w.r.t	O
.	O

The	O
maximum	O
document	B-HyperparameterName
length	I-HyperparameterName
is	O
192	B-HyperparameterValue
and	O
the	O
maximum	O
summary	B-HyperparameterName
length	I-HyperparameterName
is	O
32	B-HyperparameterValue
.	O

3	O
Background	B-DatasetName
POMDP	I-DatasetName
Text	I-DatasetName
-	I-TaskName
based	I-TaskName
games	I-TaskName
can	O
be	O
formulated	O
as	O
a	O
Partially	O
Observable	O
Markov	O
Decision	O
Processes	O
(	O
POMDPs	B-MethodName
)	O
.	O

As	O
such	O
,	O
ARGEN	B-DatasetName
has	O
wide	O
-	O
coverage	O
both	O
in	O
terms	O
of	O
the	O
number	O
of	O
tasks	O
and	O
datasets	O
.	O

We	O
pre	O
-	O
process	O
every	O
classification	O
dataset	O
except	O
for	O
GAD	B-TaskName
in	O
the	O
same	O
manner	O
as	O
the	O
BLUE	B-DatasetName
(	O
Peng	O
et	O
al	O
.	O
,	O
2019	O
)	O
benchmark	O
.	O

The	O
acquired	O
representation	O
is	O
regarded	O
as	O
the	O
key	O
K	O
s	O
j	O
for	O
X	O
s	O
j	O
(	O
K	O
q	O
j	O
for	O
X	O
q	O
j	O
)	O
.	O

3	O
-Talks	O
about	O
topics	O
with	O
the	O
correct	O
orientation	O
.	O

Notable	O
successes	O
have	O
been	O
achieved	O
by	O
metalearning	O
on	O
low	O
-	O
resource	O
NLP	O
tasks	O
,	O
such	O
as	O
multidomain	B-TaskName
sentiment	I-TaskName
classification	I-TaskName
(	O
Yu	O
et	O
al	O
.	O
,	O
2018;Geng	O
et	O
al	O
.	O
,	O
2019	O
)	O
and	O
personalized	B-TaskName
dialogue	I-TaskName
generation	I-TaskName
(	O
Madotto	O
et	O
al	O
.	O
,	O
2019;Song	O
et	O
al	O
.	O
,	O
2020;Zheng	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

In	O
a	O
language	O
-	O
informed	O
RL	O
system	O
,	O
in	O
contrast	O
,	O
the	O
agent	O
is	O
required	O
to	O
conduct	O
both	O
language	O
learning	O
and	O
decision	O
making	O
regimes	O
,	O
where	O
the	O
former	O
can	O
be	O
considered	O
as	O
prior	O
knowledge	O
and	O
is	O
much	O
slower	O
than	O
the	O
later	O
(	O
Hill	O
et	O
al	O
.	O
,	O
2021	O
)	O
.	O

It	O
indicates	O
that	O
the	O
teachers	O
can	O
produce	O
more	O
concise	O
and	O
abstractive	O
summaries	O
,	O
which	O
matches	O
the	O
goal	O
of	O
abstractive	O
summarization	O
.	O

We	O
adopt	O
the	O
Seq2Seq	B-MethodName
Transformer	I-MethodName
(	O
Vaswani	O
et	O
al	O
.	O
,	O
2017	O
)	O
model	O
.	O

The	O
objective	O
aims	O
for	O
long	B-TaskName
text	I-TaskName
generation	I-TaskName
.	O

x	O
m	O
src	O
}	O
and	O
T	O
=	O
{	O
x	O
1	O
trg	O
,	O
x	O
2	O
trg	O
,	O
.	O

Therefore	O
,	O
the	O
student	O
model	O
is	O
better	O
suited	O
to	O
the	O
target	O
language	O
with	O
learning	O
fewer	O
low	O
-	O
confidence	O
misrecognitions	O
for	O
the	O
target	O
language	O
.	O

We	O
propose	O
a	O
novel	O
data	O
-	O
augmentation	O
technique	O
for	O
neural	B-TaskName
machine	I-TaskName
translation	I-TaskName
based	O
on	O
ROT	O
-	O
k	O
ciphertexts	O
.	O

We	O
conduct	O
experiments	O
on	O
Fact	B-TaskName
-	I-TaskName
based	I-TaskName
Visual	I-TaskName
Question	I-TaskName
Answering	I-TaskName
(	O
FVQA	B-DatasetName
)	O
as	O
an	O
additional	O
benchmark	O
dataset	O
for	O
knowledge	B-TaskName
-	I-TaskName
based	I-TaskName
VQA	I-TaskName
.	O

14	O
show	O
the	O
pre	O
-	O
training	O
performance	O
of	O
QWA	B-MethodName
's	O
task	O
selector	O
and	O
action	O
validator	O
,	O
respectively	O
.	O

iii	O
)	O
Domain	O
specific	O
-number	O
of	O
domain	O
-	O
specific	O
attributes	O
(	O
Li	O
et	O
al	O
.	O
,	O
2018	O
)	O
(	O
categorical	O
up	O
to	O
5	O
)	O
.	O

In	O
order	O
to	O
have	O
an	O
intuitive	O
feeling	O
,	O
we	O
select	O
a	O
rep	O
-	O
resentative	O
example	O
1	O
and	O
visualize	O
its	O
cross	O
attention	O
weights	O
2	O
(	O
see	O
the	O
left	O
graph	O
in	O
Figure	O
1	O
)	O
.	O

Recent	O
advancements	O
in	O
off	O
-	O
policy	O
reinforcement	O
learning	O
methods	O
that	O
uses	O
offline	O
data	O
as	O
against	O
a	O
simulator	O
has	O
proven	O
to	O
be	O
sample	O
efficient	O
(	O
Thomas	O
and	O
Brunskill	O
,	O
2016	O
)	O
.	O

Figure	O
7	O
shows	O
image	O
examples	O
from	O
the	O
following	O
three	O
data	O
sources	O
:	O
YouTube	O
,	O
DeafVIDEO	O
,	O
misc	O
.	O

Obtain	O
the	O
keys	O
K	O
q	O
j	O
for	O
each	O
sample	O
X	O
q	O
j	O
12	O
:	O

We	O
want	O
the	O
student	O
model	O
to	O
learn	O
from	O
the	O
two	O
teachers	O
as	O
follows	O
:	O
the	O
higher	O
the	O
prediction	O
of	O
the	O
entity	O
recognizer	O
teacher	O
is	O
(	O
the	O
further	O
away	O
from	O
0.5	O
the	O
prediction	O
of	O
the	O
entity	O
similarity	O
teacher	O
is	O
,	O
the	O
higher	O
the	O
consistency	B-HyperparameterName
level	I-HyperparameterName
is	O
)	O
,	O
the	O
more	O
accurate	O
the	O
prediction	O
is	O
,	O
thus	O
the	O
more	O
attention	O
the	O
student	O
model	O
pays	O
attention	O
to	O
the	O
input	O
tokens	O
,	O
and	O
vice	O
versa	O
.	O

Each	O
image	O
is	O
resized	O
to	O
160	B-HyperparameterValue
×	O
160	B-HyperparameterValue
before	O
feeding	O
into	O
the	O
model	O
.	O

Autoencoding	O
models	O
,	O
such	O
as	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O
2019	O
)	O
,	O
learn	O
bidirectional	O
context	O
encoders	O
via	O
denoising	O
objectives	O
,	O
e.g.	O
Masked	B-MethodName
Language	I-MethodName
Model	I-MethodName
(	O
MLM	B-MethodName
)	O
.	O

However	O
,	O
it	O
is	O
difficult	O
to	O
capture	O
multi	O
-	O
hop	O
relationships	O
containing	O
long	O
-	O
distance	O
nodes	O
from	O
the	O
graph	O
due	O
to	O
the	O
well	O
-	O
known	O
over	O
-	O
smoothing	O
problem	O
,	O
where	O
repetitive	O
message	O
passing	O
process	O
to	O
propagate	O
information	O
across	O
long	O
distance	O
makes	O
features	O
of	O
connected	O
nodes	O
too	O
similar	O
and	O
undiscriminating	O
(	O
Li	O
et	O
al	O
.	O
,	O
2018	O
;	O
.	O

While	O
,	O
the	O
streaming	O
models	O
need	O
to	O
balance	O
the	O
latency	O
and	O
quality	O
and	O
generate	O
translations	O
based	O
on	O
the	O
partial	O
utterance	O
,	O
as	O
shown	O
in	O
Figure	O
1	O
.	O

Empirically	O
,	O
compared	O
with	O
standalone	O
baselines	O
,	O
GLM	B-MethodName
with	O
multi	O
-	O
task	O
pretraining	O
achieves	O
improvements	O
in	O
NLU	B-TaskName
,	O
conditional	B-TaskName
text	I-TaskName
generation	I-TaskName
,	O
and	O
language	B-TaskName
modeling	I-TaskName
tasks	O
altogether	O
by	O
sharing	O
the	O
parameters	O
.	O

The	O
results	O
on	O
GLUE	B-DatasetName
and	O
SQuAD	B-DatasetName
are	O
shown	O
in	O
Tables	O
9	O
and	O
10	O
.	O

For	O
tokens	O
in	O
Part	O
B	O
,	O
they	O
range	O
from	O
1	B-HyperparameterValue
to	O
the	O
length	B-HyperparameterName
of	I-HyperparameterName
the	I-HyperparameterName
span	O
.	O

Given	O
this	O
,	O
we	O
hypothesize	O
that	O
cycle	O
consistency	O
might	O
be	O
too	O
restrictive	O
for	O
sentence	O
-	O
level	O
tasks	O
.	O

To	O
validate	O
the	O
effectiveness	O
of	O
our	O
proposed	O
L2I	B-MethodName
module	O
,	O
we	O
apply	O
it	O
to	O
TAGOP	B-MethodName
,	O
obtaining	O
an	O
NDR	B-TaskName
model	O
for	O
HQA	B-TaskName
,	O
named	O
TAGOP	B-MethodName
-	I-MethodName
L2I.	I-MethodName
In	O
addition	O
to	O
the	O
vanilla	O
TAGOP	B-MethodName
,	O
we	O
compare	O
our	O
method	O
against	O
representative	O
methods	O
of	O
traditional	B-TaskName
QA	I-TaskName
,	O
numerical	B-TaskName
QA	I-TaskName
,	O
tabular	B-TaskName
QA	I-TaskName
,	O
and	O
hybrid	B-TaskName
QA	I-TaskName
.	O

These	O
constraints	O
can	O
be	O
defined	O
at	O
various	O
levels	O
of	O
a	O
sentence	O
:	O
lexical	O
,	O
syntactic	O
and	O
domain	O
-	O
specific	O
.	O

It	O
's	O
a	O
more	O
direct	O
idea	O
to	O
change	O
the	O
softmax	B-HyperparameterName
temperature	I-HyperparameterName
in	O
the	O
final	O
decoder	O
layer	O
rather	O
than	O
attention	B-HyperparameterName
temperatures	I-HyperparameterName
,	O
namely	O
changing	O
the	O
T	B-HyperparameterName
in	O
equation	O
5	O
to	O
some	O
other	O
values	O
rather	O
than	O
the	O
default	O
value	O
1.0	B-HyperparameterValue
.	O

Hallucinates	O
Sen	O
Booker	O
which	O
appears	O
frequently	O
in	O
the	O
dataset	O
Target	O
by	O
far	O
,	O
the	O
best	O
spot	O
for	O
ramen	O
.	O

Terry	O
first	O
appeared	O
in	O
the	O
TV	O
series	O
"	O
theKnowledge	B-TaskName
-	I-TaskName
based	I-TaskName
visual	I-TaskName
question	I-TaskName
answering	I-TaskName
(	O
QA	B-TaskName
)	O
aims	O
to	O
answer	O
a	O
question	O
which	O
requires	O
visually	O
-	O
grounded	O
external	O
knowledge	O
beyond	O
image	O
content	O
itself	O
.	O

The	O
model	O
predicts	O
the	O
missing	O
tokens	O
in	O
the	O
spans	O
from	O
the	O
corrupted	O
text	O
in	O
an	O
autoregressive	O
manner	O
,	O
which	O
means	O
when	O
predicting	O
the	O
missing	O
tokens	O
in	O
a	O
span	O
,	O
the	O
model	O
has	O
access	O
to	O
the	O
corrupted	O
text	O
and	O
the	O
previously	O
predicted	O
spans	O
.	O

p(y|x	O
)	O
=	O
p(v(y)|c(x	O
)	O
)	O
y	O
∈Y	O
p(v(y	O
)	O
|c(x	O
)	O
)	O
(	O
3	O

θ	O
:	O
=	O
θ	O
−	O
R(s	O
t	O
,	O
a	O
t	O
,	O
g)∇π	O
blackbox	O
(	O
a	O
t	O
|s	O
t	O
;	O
θ	O
)	O
(	O
6	O
)	O
Hence	O
we	O
believe	O
our	O
pairwise	O
casual	O
reward	O
learning	O
and	O
associated	O
improvement	O
in	O
sample	O
efficiency	O
are	O
independent	O
of	O
model	O
architecture	O
.	O

Fixing	O
the	O
parameter	O
of	O
PLM	B-MethodName
largely	O
impedes	O
the	O
performance	O
of	O
TAGOP	B-MethodName
-	I-MethodName
L2I	I-MethodName
on	O
TAT	B-DatasetName
-	I-DatasetName
HQA	I-DatasetName
,	O
showing	O
that	O
encoding	O
factual	O
and	O
hypothetical	O
questions	O
requires	O
different	O
mechanisms	O
.	O

Another	O
disadvantage	O
of	O
BERT	B-MethodName
is	O
that	O
it	O
can	O
not	O
fill	O
in	O
the	O
blanks	O
of	O
multiple	O
tokens	O
properly	O
.	O

It	O
's	O
really	O
[	O
MASK	O
]	O
"	O
.	O

Human	O
Evaluation	O
We	O
conduct	O
human	O
evaluation	O
following	O
Song	O
et	O
al	O
.	O
(	O
2020	O
)	O
considering	O
two	O
aspects	O
Quality	O
and	O
Consistency	O
where	O
five	O
welleducated	O
volunteers	O
annotate	O
250	O
generated	B-HyperparameterName
responses	O
for	O
each	O
model	O
.	O

We	O
do	O
not	O
freeze	O
any	O
layers	O
and	O
we	O
use	O
the	O
output	O
of	O
the	O
last	O
layer	O
as	O
our	O
hidden	O
feature	O
vector	O
.	O

We	O
train	O
Transformer	B-MethodName
for	O
100	B-HyperparameterValue
epochs	B-HyperparameterName
and	O
select	O
the	O
best	O
model	O
w.r.t	O
.	O

Though	O
the	O
turns	O
have	O
varying	O
levels	O
of	O
importance	O
,	O
each	O
of	O
the	O
turns	O
are	O
treated	O
equally	O
in	O
imitation	O
learning	O
.	O

5	O
Quantitative	B-DatasetName
ResultsWe	O
all	O
settings	O
.	O

Note	O
that	O
although	O
the	O
action	O
space	O
is	O
reduced	O
,	O
it	O
still	O
remains	O
challenging	O
as	O
the	O
agent	O
may	O
encounter	O
unseen	O
action	O
candidates	O
(	O
Chandak	O
et	O
al	O
.	O
,	O
2019(Chandak	O
et	O
al	O
.	O
,	O
,	O
2020	O
.	O

For	O
the	O
training	O
of	O
recognition	O
teacher	O
model	O
and	O
similarity	O
teacher	O
model	O
,	O
we	O
set	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
to	O
be	O
1e-5	B-HyperparameterValue
and	O
5e-6	B-HyperparameterValue
separately	O
.	O

However	O
,	O
UniLM	B-MethodName
always	O
replaces	O
masked	O
spans	O
with	O
[	O
MASK	O
]	O
tokens	O
,	O
which	O
limits	O
its	O
ability	O
to	O
model	O
the	O
dependencies	O
between	O
the	O
masked	O
spans	O
and	O
their	O
context	O
.	O

The	O
global	O
optimization	O
keeps	O
updating	O
in	O
the	O
whole	O
meta	O
-	O
training	O
phase	O
.	O

Considering	O
the	O
available	O
baseline	O
results	O
,	O
we	O
use	O
the	O
Gigaword	B-DatasetName
dataset	O
(	O
Rush	O
et	O
al	O
.	O
,	O
2015	O
)	O
for	O
abstractive	B-TaskName
summarization	I-TaskName
and	O
the	O
SQuAD	B-DatasetName
1.1	I-DatasetName
dataset	O
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O
2016	O
)	O
for	O
question	B-TaskName
generation	I-TaskName
(	O
Du	O
et	O
al	O
.	O
,	O
2017	O
)	O
as	O
the	O
benchmarks	O
for	O
models	O
pretrained	O
on	O
BookCorpus	B-DatasetName
and	O
Wikipedia	B-DatasetName
.	O

We	O
would	O
also	O
like	O
to	O
acknowledge	O
the	O
support	O
of	O
the	O
NExT	O
research	O
grant	O
funds	O
,	O
supported	O
by	O
the	O
National	O
Research	O
Foundation	O
,	O
Prime	O
Ministers	O
Office	O
,	O
Singapore	O
under	O
its	O
IRC@	O
SG	O
Funding	O
Initiative	O
,	O
and	O
to	O
gratefully	O
acknowledge	O
the	O
support	O
of	O
NVIDIA	O
Corporation	O
with	O
the	O
donation	O
of	O
the	O
GeForce	O
GTX	O
Titan	O
XGPU	O
used	O
in	O
this	O
research	O
.	O

ARAEs	B-DatasetName
(	O
Zhao	O
et	O
al	O
.	O
,	O
2018b	O
)	O
are	O
the	O
auto	O
-	O
encoder	O
variants	O
of	O
the	O
Generative	B-MethodName
Adversarial	I-MethodName
Network	I-MethodName
(	O
GAN	B-MethodName
)	O
(	O
Goodfellow	O
et	O
al	O
.	O
,	O
2014	O
)	O
framework	O
.	O

However	O
,	O
it	O
does	O
not	O
explicitly	O
maintain	O
other	O
attributes	O
between	O
the	O
source	O
and	O
translated	O
text	O
,	O
for	O
e.g.	O
,	O
text	O
length	O
and	O
descriptiveness	O
.	O

Our	O
model	O
responds	O
by	O
focusing	O
on	O
{	O
second	O
⪯	O
from	O
⪯	O
left	O
}	O
phrase	O
of	O
the	O
question	O
and	O
four	O
facts	O
having	O
a	O
left	O
relation	O
among	O
86	O
knowledge	O
hyperedges	O
.	O

For	O
question	O
hypergraph	O
,	O
each	O
word	O
unit	O
is	O
used	O
as	O
a	O
start	O
node	O
of	O
a	O
graph	O
walk	O
.	O

The	O
others	O
were	O
Joe	O
Namath	O
,	O
Bill	O
Nelsen	O
,	O
and	O
Jerry	O
Kramer	O
.	O

In	O
particular	O
,	O
our	O
analyses	O
are	O
for	O
the	O
following	O
tasks	O
:	O
machine	B-TaskName
translation	I-TaskName
,	O
code	B-TaskName
-	I-TaskName
switched	I-TaskName
translation	I-TaskName
,	O
paraphrasing	B-TaskName
,	O
transliteration	B-TaskName
,	O
and	O
news	B-TaskName
title	I-TaskName
generation	I-TaskName
.	O

Our	O
combined	O
MSA	B-DatasetName
and	O
Twitter	B-DatasetName
data	O
make	O
up	O
29B	B-HyperparameterValue
tokens	B-HyperparameterName
,	O
and	O
hence	O
is	O
∼	O
49	B-HyperparameterValue
%	I-HyperparameterValue
less	O
than	O
Arabic	O
tokens	O
on	O
which	O
mT5	B-MethodName
is	O
pre	O
-	O
trained	O
(	O
57B	B-HyperparameterValue
Arabic	B-HyperparameterName
tokens	I-HyperparameterName
)	O
.	O

Then	O
in	O
the	O
reinforcement	O
fine	O
-	O
tuning	O
phase	O
,	O
we	O
freeze	O
the	O
task	O
selector	O
and	O
fine	O
-	O
tune	O
the	O
action	O
selector	O
through	O
reinforcement	O
learning	O
,	O
where	O
the	O
experiment	O
setting	O
is	O
same	O
with	O
QWA	B-MethodName
and	O
GATA	B-MethodName
.	O

(	O
Wang	O
et	O
al	O
.	O
,	O
2020	O
)	O
is	O
first	O
to	O
argue	O
the	O
use	O
of	O
automated	O
evaluation	O
metrics	O
directly	O
as	O
reward	O
is	O
under	O
-	O
specified	O
for	O
ToD	B-TaskName
policy	O
learning	O
.	O

We	O
also	O
use	O
label	B-HyperparameterName
smoothing	I-HyperparameterName
with	O
rate	B-HyperparameterName
0.1	B-HyperparameterValue
(	O
Pereyra	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

AdvPicker	B-MethodName
proposes	O
a	O
adversarial	O
discriminator	O
for	O
cross	B-TaskName
-	I-TaskName
lingual	I-TaskName
NER	I-TaskName
.	O

Joint	O
representation	O
is	O
obtained	O
based	O
on	O
the	O
attention	O
as	O
well	O
.	O

The	O
improvement	O
for	O
POLITICAL	B-DatasetName
is	O
less	O
;	O
we	O
find	O
these	O
source	O
sentences	O
themselves	O
are	O
less	O
fluent	O
and	O
contain	O
many	O
U.S.	O
political	O
acronyms	O
,	O
and	O
that	O
our	O
system	O
produces	O
many	O
outof	O
-	O
vocabulary	O
words	O
affecting	O
fluency	O
.	O

We	O
conjecture	O
the	O
performance	O
drop	O
in	O
the	O
first	O
three	O
groups	O
is	O
because	O
the	O
question	O
-	B-TaskName
answering	I-TaskName
label	O
in	O
TAT	B-DatasetName
-	I-DatasetName
HQA	I-DatasetName
under	O
the	O
same	O
c	O
and	O
q	O
is	O
different	O
from	O
TAT	B-DatasetName
-	I-DatasetName
QA	I-DatasetName
.	O

This	O
issue	O
is	O
particularly	O
pronounced	O
in	O
unsupervised	O
attribute	B-TaskName
transfer	I-TaskName
due	O
to	O
lack	O
of	O
parallel	O
sentences	O
between	O
S	O
and	O
T	O
.	O

Jaques	O
et	O
al	O
.	O
(	O
2019	O
)	O
and	O
Wang	O
et	O
al	O
.	O
(	O
2020	O
)	O
uses	O
Batch	B-MethodName
-	I-MethodName
RL	I-MethodName
for	O
dialogue	B-TaskName
policy	I-TaskName
learning	I-TaskName
.	O

This	O
objective	O
aims	O
for	O
seq2seq	O
tasks	O
whose	O
predictions	O
are	O
often	O
complete	O
sentences	O
or	O
paragraphs	O
.	O

Translation	O
based	O
models	O
generate	O
pseudo	O
labeled	O
target	O
language	O
data	O
to	O
train	O
the	O
cross	B-TaskName
-	I-TaskName
lingual	I-TaskName
NER	I-TaskName
model	O
,	O
but	O
the	O
noise	O
from	O
translation	O
process	O
restrains	O
its	O
performance	O
.	O

Meta	O
-	O
learning	O
-	O
based	O
methods	O
(	O
Thrun	O
and	O
Pratt	O
,	O
2012	O
)	O
have	O
been	O
commonly	O
used	O
in	O
such	O
scenarios	O
owing	O
to	O
their	O
fast	O
adaptation	O
ability	O
.	O

Our	O
models	O
are	O
publicly	O
available	O
.	O

In	O
order	O
to	O
enhance	O
the	O
interaction	O
between	O
semantic	O
parsing	O
and	O
knowledge	O
base	O
,	O
we	O
incorporate	O
entity	O
triples	O
from	O
the	O
knowledge	O
base	O
into	O
a	O
knowledgeaware	O
entity	O
disambiguation	O
module	O
.	O

However	O
,	O
since	O
the	O
autoencoding	O
and	O
autoregressive	O
objectives	O
differ	O
by	O
nature	O
,	O
a	O
simple	O
unification	O
can	O
not	O
fully	O
inherit	O
the	O
advantages	O
of	O
both	O
frameworks	O
.	O

This	O
causes	O
a	O
performance	O
drop	O
across	O
all	O
languages	O
due	O
to	O
two	O
single	O
teachers	O
can	O
not	O
make	O
a	O
difference	O
with	O
the	O
combination	O
.	O

GLM	B-MethodName
Sent	I-MethodName
can	O
perform	O
better	O
than	O
GLM	B-MethodName
Large	I-MethodName
,	O
while	O
GLM	B-MethodName
Doc	I-MethodName
performs	O
slightly	O
worse	O
than	O
GLM	B-MethodName
Large	I-MethodName
.	O

good	O
prices	O
.	O

Cooperatively	O
reducing	O
the	O
contrastive	O
or	O
the	O
classification	O
loss	O
is	O
better	O
than	O
ARAE	B-MethodName
.	O

Figure	O
17	O
:	O
The	O
RL	O
performance	O
of	O
models	O
with	O
respect	O
to	O
training	O
episodes	O
(	O
the	O
full	O
result	O
of	O
Fig	O
.	O

This	O
demonstrates	O
GLM	B-MethodName
's	O
advantage	O
in	O
handling	O
variable	O
-	O
length	O
blank	O
.	O

TOF	B-MethodName
(	O
Zhang	O
et	O
al	O
.	O
,	O
2021	O
)	O
transfers	O
knowledge	O
from	O
three	O
aspects	O
for	O
cross	B-TaskName
-	I-TaskName
lingual	I-TaskName
NER	I-TaskName
.	O

We	O
suspect	O
the	O
reason	O
is	O
that	O
the	O
operation	O
of	O
SWAP	O
MIN	B-MethodName
NUM	I-MethodName
is	O
very	O
close	O
to	O
SWAP	O
,	O
which	O
may	O
confuse	O
the	O
deriving	O
head	O
when	O
making	O
classification	O
over	O
the	O
operators	O
.	O

Following	O
the	O
paper	O
,	O
we	O
split	O
the	O
dataset	O
into	O
train	O
,	O
validation	O
,	O
and	O
test	O
sets	O
with	O
a	O
proportion	O
of	O
8:1:1	B-HyperparameterValue
,	O
and	O
report	O
the	O
average	O
accuracy	O
of	O
five	O
repeated	O
runs	O
on	O
different	O
data	O
split	O
.	O

The	O
positve	O
/	O
negative	B-HyperparameterName
threshold	I-HyperparameterName
of	O
the	O
anchors	O
are	O
0.6/0.3	B-HyperparameterValue
respectively	O
.	O

We	O
conduct	O
a	O
pilot	O
study	O
on	O
the	O
generalization	O
ability	O
of	O
existing	O
NDR	B-TaskName
models	O
on	O
hypothetical	O
questions	O
.	O

Besides	O
,	O
existing	O
work	O
assumes	O
that	O
unlimited	O
interaction	O
data	O
can	O
be	O
obtained	O
to	O
train	O
the	O
whole	O
model	O
through	O
RL	O
.	O

The	O
most	O
similar	O
answer	O
to	O
the	O
joint	O
representation	O
is	O
selected	O
as	O
an	O
answer	O
among	O
the	O
answer	O
candidates	O
.	O

We	O
demonstrate	O
our	O
method	O
MemIML	B-MethodName
meets	O
the	O
above	O
criterion	O
(	O
See	O
details	O
in	O
Appendix	O
.	O

However	O
,	O
in	O
our	O
case	O
,	O
since	O
we	O
are	O
training	O
an	O
ARAE	B-MethodName
,	O
it	O
would	O
involve	O
an	O
additional	O
inference	O
and	O
auto	O
-	O
encoder	O
training	O
step	O
which	O
is	O
expensive	O
and	O
we	O
defer	O
exploring	O
this	O
.	O

Cycle	O
Consistency	O
Loss	O
:	O
a	O
)	O
In	O
Latent	O
Spaces	O
-Cycle	O
consistency	O
in	O
latent	O
spaces	O
has	O
been	O
shown	O
to	O
improve	O
word	O
-	O
level	O
tasks	O
,	O
such	O
as	O
cross	B-TaskName
-	I-TaskName
lingual	I-TaskName
dictionary	I-TaskName
construction	I-TaskName
(	O
Mohiuddin	O
and	O
Joty	O
,	O
2019	O
)	O
and	O
topic	B-TaskName
modeling	I-TaskName
.	O

The	O
model	O
architectures	O
of	O
Transformer	B-MethodName
(	I-MethodName
SA	I-MethodName
)	I-MethodName
and	O
Transformer	B-MethodName
(	I-MethodName
SA+GA	I-MethodName
)	I-MethodName
presented	O
in	O
this	O
paper	O
are	O
the	O
same	O
as	O
Hypergraph	B-MethodName
Transformer	I-MethodName
.	O

For	O
low	O
-	O
resource	O
scenarios	O
in	O
NLP	O
,	O
optimization	O
-	O
based	O
meta	O
-	O
learning	O
methods	O
achieved	O
promising	O
results	O
on	O
tasks	O
such	O
as	O
personalized	B-TaskName
dialog	I-TaskName
generation	I-TaskName
(	O
Madotto	O
et	O
al	O
.	O
,	O
2019;Song	O
et	O
al	O
.	O
,	O
2020	O
;	O
,	O
lowresource	B-TaskName
machine	B-TaskName
translation	I-TaskName
(	O
Gu	O
et	O
al	O
.	O
,	O
2018;Sharaf	O
et	O
al	O
.	O
,	O
2020	O
)	O
and	O
question	B-TaskName
answering	I-TaskName
,	O
few	O
-	O
shot	O
slot	B-TaskName
tagging	I-TaskName
(	O
Wang	O
et	O
al	O
.	O
,	O
2021	O
)	O
,	O
and	O
so	O
on	O
.	O

With	O
λ	B-HyperparameterName
=	O
1.5	B-HyperparameterValue
,	O
we	O
obtain	O
a	O
BLEU	B-MetricName
of	O
27.90	B-HyperparameterValue
,	O
while	O
the	O
result	O
of	O
the	O
regular	O
pseudo	O
-	O
labeling	O
is	O
27.79	B-HyperparameterValue
(	O
more	O
details	O
are	O
in	O
Appendix	O
A	O
)	O
.	O

Figure	O
6	O
shows	O
the	O
distribution	O
of	O
fingerspelling	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
in	O
the	O
two	O
datasets	O
.	O

saving	O
80	B-HyperparameterValue
%	I-HyperparameterValue
of	O
the	O
online	O
interaction	O
data	O
in	O
complex	O
games	O
.	O

Fig	O
.	O

Some	O
RL	O
-	O
based	O
game	O
agents	O
have	O
been	O
developed	O
recently	O
and	O
proven	O
to	O
be	O
effective	O
in	O
handling	O
challenges	O
such	O
as	O
language	O
representation	O
learning	O
and	O
partial	O
observability	O
(	O
Narasimhan	O
et	O
al	O
.	O
,	O
2015;Fang	O
et	O
al	O
.	O
,	O
2017;Ammanabrolu	O
and	O
Riedl	O
,	O
2019	O
)	O
.	O

The	O
datasets	O
are	O
mainly	O
extracted	O
from	O
transcriptions	O
of	O
TED	O
talks	O
between	O
2010	O
and	O
2016	O
,	O
and	O
the	O
QCRI	B-DatasetName
Educational	I-DatasetName
Domain	I-DatasetName
Corpus	I-DatasetName
(	O
QED	O
2016	O
)	O
(	O
Abdelali	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

Without	O
generative	O
objective	O
during	O
pretraining	O
,	O
GLM	B-MethodName
Large	I-MethodName
can	O
not	O
complete	O
the	O
language	O
modeling	O
tasks	O
,	O
with	O
perplexity	B-HyperparameterName
larger	O
than	O
100	B-HyperparameterValue
.	O

For	O
a	O
fair	O
comparison	O
,	O
we	O
compare	O
our	O
model	O
against	O
the	O
version	O
of	O
TOF	B-MethodName
w/o	I-MethodName
continual	O
learning	O
(	O
Zhang	O
et	O
2021	O
)	O
,	O
RIKD	B-MethodName
w/o	I-MethodName
IKD	I-MethodName
(	O
Liang	O
et	O
al	O
.	O
,	O
2021	O
)	O
and	O
Unitrans	B-MethodName
w/o	I-MethodName
translation	O
(	O
Wu	O
et	O
al	O
.	O
,	O
2020b	O
)	O
as	O
reported	O
in	O
their	O
paper	O
.	O

Our	O
AraT5	B-MethodName
model	O
outperforms	O
mT5	B-MethodName
,	O
even	O
though	O
it	O
is	O
pre	O
-	O
trained	O
with	O
49	B-HyperparameterValue
%	I-HyperparameterValue
less	O
data	O
(	O
see	O
§	O
2.1	O
)	O
.	O

Compared	O
to	O
ARAE	B-MethodName
,	O
our	O
model	O
performs	O
well	O
except	O
for	O
in	O
YELP	O
.	O

Also	O
,	O
our	O
model	O
produces	O
sentences	O
where	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
proper	I-HyperparameterName
nouns	I-HyperparameterName
are	O
retained	O
(	O
Chris	O
Klein	O
vs.	O
Robert	O
De	O
Niro	O
)	O
,	O
whereas	O
ARAE	B-MethodName
does	O
not	O
.	O

,	O
@	O
,	O
as	O
well	O
as	O
the	O
blank	O
symbol	O
for	O
CTC	B-DatasetName
.	O

we	O
love	O
the	O
atmosphere	O
,	O
the	O
service	O
and	O
obviously	O
the	O
food	O
.	O

Again	O
,	O
our	O
models	O
establish	O
new	O
SOTA	O
on	O
the	O
majority	O
of	O
language	O
understanding	O
tasks	O
.	O

For	O
TAPT	B-MethodName
,	O
we	O
additionally	O
pre	O
-	O
trained	O
the	O
RoBERTa	B-MethodName
-	I-MethodName
large	I-MethodName
model	O
with	O
each	O
pre	O
-	O
processed	O
downstream	O
task	O
's	O
training	O
data	O
.	O

Similar	O
to	O
our	O
efforts	O
,	O
Jiang	O
et	O
al	O
.	O
(	O
2019	O
)	O
and	O
Xu	O
et	O
al	O
.	O
(	O
2021	O
)	O
designed	O
a	O
meta	O
-	O
policy	O
for	O
task	O
decomposition	O
and	O
subtask	O
selection	O
,	O
and	O
a	O
sub	O
-	O
policy	O
for	O
goal	O
-	O
conditioned	O
decision	O
making	O
.	O

In	O
absence	O
of	O
comparisons	O
with	O
monolingual	O
pre	O
-	O
trained	O
language	O
models	O
that	O
serve	O
different	O
non	O
-	O
English	O
contexts	O
,	O
it	O
remains	O
unknown	O
how	O
multilingual	O
models	O
really	O
fare	O
against	O
languagespecific	O
models	O
.	O

Note	O
that	O
U	O
[	O
a	O
,	O
b	O
]	O
is	O
a	O
uniform	O
distribution	O
and	O
we	O
typically	O
set	O
a	O
=	O
1.0	B-HyperparameterValue
and	O
b	O
=	O
2.0.We	B-HyperparameterValue
conduct	O
our	O
experiments	O
on	O
three	O
popular	B-TaskName
document	I-TaskName
summarization	I-TaskName
datasets	O
:	O
CNN	B-DatasetName
/	I-DatasetName
DailyMail	I-DatasetName
(	O
Hermann	O
et	O
al	O
.	O
,	O
2015	O
)	O
,	O
XSum	B-DatasetName
(	O
Narayan	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
and	O
New	B-DatasetName
York	I-DatasetName
Times	I-DatasetName
(	O
Sandhaus	O
,	O
2008	O
)	O
.	O

The	O
key	O
is	O
the	O
sentence	O
representation	O
of	O
a	O
sample	O
input	O
from	O
support	O
sets	O
obtained	O
from	O
an	O
introduced	O
key	O
network	O
.	O

In	O
the	O
setting	O
of	O
RoBERTa	B-MethodName
Large	I-MethodName
,	O
GLM	B-MethodName
RoBERTa	I-MethodName
can	O
still	O
achieve	O
improvements	O
over	O
the	O
baselines	O
,	O
but	O
with	O
a	O
smaller	O
margin	O
.	O

For	O
text	B-TaskName
generation	I-TaskName
tasks	O
,	O
the	O
given	O
context	O
constitutes	O
the	O
Part	O
A	O
of	O
the	O
input	O
,	O
with	O
a	O
mask	O
token	O
appended	O
at	O
the	O
end	O
.	O

They	O
are	O
typically	O
deployed	O
in	O
conditional	B-TaskName
generation	I-TaskName
tasks	O
,	O
such	O
as	O
text	B-TaskName
summarization	I-TaskName
and	O
response	B-TaskName
generation	I-TaskName
.	O

Effect	O
of	O
multi	O
-	O
hop	O
graph	O
walk	O
We	O
compare	O
the	O
performances	O
with	O
different	O
number	B-HyperparameterName
of	I-HyperparameterName
graph	I-HyperparameterName
walks	I-HyperparameterName
used	O
to	O
construct	O
a	O
knowledge	O
hypergraph	O
(	O
i.e.	O
,	O
1	B-HyperparameterValue
-	I-HyperparameterValue
hop	I-HyperparameterValue
,	O
2	B-HyperparameterValue
-	I-HyperparameterValue
hop	I-HyperparameterValue
,	O
and	O
3	B-HyperparameterValue
-	I-HyperparameterValue
hop	I-HyperparameterValue
)	O
.	O

The	O
soft	O
attention	O
over	O
the	O
knowledge	O
facts	O
and	O
the	O
given	O
question	O
is	O
computed	O
as	O
follows	O
:	O
p	O
ij	O
=	O
softmax(q	O
T	O
i−1	O
m	O
ij	O
)	O
where	O
m	O
is	O
the	O
embeddings	O
of	O
knowledge	O
facts	O
,	O
i	O
is	O
a	O
number	B-HyperparameterName
of	I-HyperparameterName
layer	I-HyperparameterName
and	O
j	O
is	O
an	O
index	O
of	O
knowledge	O
facts	O
.	O

4	O
)	O
The	O
performance	O
achieved	O
is	O
still	O
low	O
w.r.t	O
.	O

)	O
where	O
σ	B-HyperparameterName
is	O
a	O
logistic	O
sigmoid	O
function	O
,	O
and	O
W	O
[	O
•	O
]	O
and	O
U	O
[	O
•	O
]	O
are	O
learnable	O
parameters	O
.	O

To	O
generate	O
proposals	O
,	O
we	O
first	O
transform	O
the	O
feature	O
sequence	O
via	O
a	O
1D	B-HyperparameterValue
-	I-MethodName
CNN	I-MethodName
with	O
the	O
following	O
architecture	O
:	O
conv	B-HyperparameterValue
layer	I-HyperparameterName
(	O
512	B-HyperparameterValue
output	B-HyperparameterName
dimension	I-HyperparameterName
,	O
kernel	B-HyperparameterName
width	I-HyperparameterName
8)	B-HyperparameterValue
,	O
max	B-HyperparameterName
pooling	I-HyperparameterName
(	O
kernel	B-HyperparameterName
width	I-HyperparameterName
8	B-HyperparameterValue
,	O
stride	B-HyperparameterName
4	I-HyperparameterName
)	O
,	O
conv	B-HyperparameterValue
layer	I-HyperparameterName
(	O
256	B-HyperparameterValue
output	B-HyperparameterName
dimension	I-HyperparameterName
,	O
kernel	B-HyperparameterName
width	I-HyperparameterName
3	B-HyperparameterValue
)	O
and	O
conv	B-HyperparameterValue
layer	I-HyperparameterName
(	O
256	B-HyperparameterValue
output	B-HyperparameterName
dimension	I-HyperparameterName
,	O
kernel	B-HyperparameterName
width	I-HyperparameterName
3	B-HyperparameterValue
)	O
.	O

3	O
)	O
TAGOP	B-MethodName
-	I-MethodName
L2I	I-MethodName
achieves	O
the	O
worst	O
performance	O
on	O
SWAP	B-DatasetName
MIN	I-MethodName
NUM	I-DatasetName
,	O
which	O
is	O
merely	O
comparable	O
to	O
TAGOP	B-MethodName
.	O

On	O
the	O
contrary	O
,	O
our	O
method	O
update	O
node	O
representations	O
via	O
hyperedge	O
matching	O
of	O
hypergraphs	O
instead	O
of	O
message	O
passing	O
scheme	O
.	O

The	O
model	O
is	O
trained	O
with	O
CTC	O
loss	O
(	O
Graves	O
et	O
al	O
.	O
,	O
2006	O
)	O
.	O

Row	O
6	O
shows	O
that	O
removing	O
the	O
span	O
shuffling	O
(	O
always	O
predicting	O
the	O
masked	O
spans	O
from	O
left	O
to	O
right	O
)	O
leads	O
to	O
a	O
severe	O
performance	O
drop	O
on	O
SuperGLUE	B-DatasetName
.	O

The	O
second	O
and	O
third	O
attended	O
knowledge	O
entities	O
are	O
the	O
other	O
person	O
(	O
Q7141361	O
)	O
and	O
Iran	O
.	O

We	O
create	O
two	O
human	O
written	O
(	O
natural	O
)	O
code	O
-	O
switched	O
parallel	O
datasets	O
:	O
(	O
1	O
)	O
ALG	B-MethodName
-	I-DatasetName
CST	I-DatasetName
.	O

Applying	O
our	O
framework	O
to	O
ALBERT	B-MethodName
allowed	O
us	O
to	O
obtain	O
a	O
student	O
model	O
with	O
performance	O
comparable	O
to	O
that	O
of	O
the	O
teacher	O
with	O
half	O
the	O
parameters	O
.	O

We	O
define	O
a	O
triplet	O
as	O
a	O
basic	O
unit	O
of	O
graph	O
walk	O
to	O
preserve	O
high	O
-	O
order	O
semantics	O
inherent	O
in	O
knowledge	O
graph	O
,	O
i.e.	O
,	O
every	O
single	O
graph	O
walk	O
contains	O
three	O
nodes	O
{	O
head	O
,	O
predicate	O
,	O
tail	O
}	O
,	O
rather	O
than	O
having	O
only	O
one	O
of	O
these	O
three	O
nodes	O
.	O

Existing	O
models	O
can	O
be	O
separated	O
into	O
three	O
categories	O
,	O
shared	O
feature	O
space	O
based	O
,	O
translation	O
based	O
and	O
knowledge	O
distillation	O
based	O
.	O

The	O
training	O
is	O
fairly	O
fast	O
.	O

Current	O
methods	O
either	O
refine	O
representations	O
stored	O
in	O
the	O
memory	O
(	O
Ramalho	O
and	O
Garnelo	O
,	O
2018	O
)	O
or	O
refining	O
parameters	O
using	O
the	O
memory	O
(	O
Munkhdalai	O
and	O
Yu	O
,	O
2017;Cai	O
et	O
al	O
.	O
,	O
2018	O
;	O
.	O

The	O
results	O
are	O
overall	O
consistent	O
with	O
the	O
perceived	O
relative	O
visual	O
qualities	O
of	O
these	O
categories	O
.	O

All	O
models	O
except	O
ours	O
show	O
slightly	O
lower	O
performance	O
on	O
the	O
3	O
-	O
hop	O
graph	O
than	O
on	O
the	O
2	O
-	O
hop	O
graph	O
.	O

The	O
first	O
example	O
in	O
Fig	O
:	O
9	O
demonstrate	O
this	O
behaviour	O
.	O

Similar	O
to	O
the	O
task	O
selector	O
,	O
we	O
pre	O
-	O
train	O
this	O
module	O
through	O
question	O
answering	O
.	O

Our	O
model	O
shows	O
notable	O
strengths	O
especially	O
on	O
complex	O
problems	O
such	O
as	O
Comparison	O
,	O
Multi	O
-	O
entity	O
or	O
Subtraction	O
.	O

The	O
memory	O
-	O
based	O
methods	O
represent	O
knowledge	O
facts	O
in	O
a	O
form	O
of	O
memory	O
and	O
calculate	O
soft	O
attention	O
scores	O
of	O
each	O
memory	O
with	O
respect	O
to	O
a	O
question	O
.	O

The	O
conversations	O
span	O
across	O
7	O
domains	O
including	O
attraction	O
,	O
hospital	O
,	O
hotel	O
,	O
police	O
,	O
restaurant	O
,	O
taxi	O
and	O
train	O
.	O

The	O
RL	O
agents	O
for	O
text	B-TaskName
-	I-TaskName
based	I-TaskName
games	I-TaskName
can	O
be	O
divided	O
as	O
text	O
-	O
based	O
agents	O
and	O
KG	O
-	O
based	O
agents	O
based	O
on	O
the	O
form	O
of	O
observations	O
.	O

All	O
the	O
suggestions	O
for	O
stabilizing	O
training	O
are	O
mostly	O
obtained	O
from	O
(	O
Arjovsky	O
and	O
Bottou	O
,	O
2017	O
)	O
.	O

We	O
use	O
Adam	B-HyperparameterValue
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
optimizer	B-HyperparameterName
for	O
both	O
inner	O
and	O
outer	O
loop	O
update	O
with	O
learning	B-HyperparameterName
rate	I-HyperparameterName
2e	B-HyperparameterValue
−5	I-HyperparameterValue
and	O
1e	B-HyperparameterValue
−5	I-HyperparameterValue
respectively	O
,	O
and	O
we	O
set	O
β	B-HyperparameterName
=	O
0.2	B-HyperparameterValue
in	O
Eqn	O
.	O

We	O
measure	O
performance	O
via	O
AP@IoU	B-MetricName
,	O
a	O
commonly	O
used	O
evaluation	O
metric	O
for	O
action	B-TaskName
detection	I-TaskName
(	O
Idrees	O
et	O
al	O
.	O
,	O
2016;Heilbron	O
et	O
al	O
.	O
,	O
2015	O
)	O
that	O
has	O
also	O
been	O
used	O
for	O
fingerspelling	B-TaskName
detection	I-TaskName
(	O
Shi	O
et	O
al	O
.	O
,	O
2021	O
)	O
.	O

Our	O
method	O
,	O
which	O
builds	O
on	O
top	O
of	O
pseudo	O
-	O
labeling	O
,	O
is	O
conceptually	O
simple	O
and	O
improves	O
pseudo	O
-	O
labeling	O
across	O
different	O
summarization	O
datasets	O
.	O

Each	O
hypothesisŵ	O
m	O
is	O
split	O
into	O
a	O
list	O
of	O
words	O
{	O
ŵ	O
n	O
m	O
}	O
1≤n≤N	O
separated	O
by	O
<	O
x	O
>	O
.	O

Second	O
,	O
autoregressive	O
models	O
are	O
trained	O
with	O
a	O
left	O
-	O
to	O
-	O
right	O
language	O
modeling	O
objective	O
(	O
Radford	O
et	O
al	O
.	O
,	O
2018a	O
,	O
b;Brown	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

However	O
,	O
the	O
fi-	O
nal	O
performance	O
of	O
this	O
variant	O
is	O
still	O
comparable	O
.	O

Our	O
own	O
fine	O
-	O
tuning	O
version	O
of	O
BART	B-MethodName
(	I-MethodName
BART	B-MethodName
(	O
ours	O
)	O
)	O
is	O
comparable	O
or	O
slightly	O
better	O
than	O
the	O
original	O
reported	O
BART	B-MethodName
results	O
,	O
and	O
we	O
use	O
it	O
as	O
the	O
teacher	O
model	O
on	O
the	O
three	O
datasets	O
.	O

The	O
numbers	O
of	O
clips	O
in	O
the	O
various	O
splits	O
can	O
be	O
found	O
in	O
the	O
Appendix	O
.	O

The	O
messages	O
(	O
sources	O
)	O
were	O
natively	O
written	O
in	O
either	O
romanized	O
Arabizi	O
or	O
Egyptian	O
Arabic	O
orthography	O
.	O

The	O
three	O
seq2seq	O
models	O
are	O
one	O
each	O
for	O
belief	B-TaskName
state	I-TaskName
,	O
dialogue	B-TaskName
act	I-TaskName
and	O
response	B-TaskName
generation	I-TaskName
modules	O
.	O

1	O
(	O
a	O
)	O
shows	O
an	O
example	O
of	O
the	O
textual	O
observation	O
and	O
the	O
corresponding	O
KG	O
-	O
based	O
observation	O
.	O

To	O
address	O
this	O
issue	O
,	O
we	O
introduce	O
an	O
evaluation	O
framework	O
that	O
improves	O
previous	O
evaluation	O
procedures	O
in	O
three	O
key	O
aspects	O
,	O
i.e.	O
,	O
test	O
performance	O
,	O
dev	O
-	O
test	O
correlation	O
,	O
and	O
stability	O
.	O

In	O
addition	O
to	O
these	O
short	O
comings	O
,	O
the	O
direct	O
use	O
of	O
automatic	O
evaluation	O
metric	O
as	O
reward	O
for	O
policy	O
learning	O
is	O
not	O
desirable	O
,	O
since	O
these	O
automatic	O
evaluation	O
metrics	O
are	O
often	O
for	O
the	O
entire	O
dialogue	O
and	O
not	O
per	O
turn	O
.	O

Interestingly	O
,	O
our	O
student	O
models	O
PLATE	B-MethodName
B12	I-MethodName
-	I-MethodName
3	I-MethodName
λ=2.0	I-MethodName
and	O
PLATE	B-MethodName
B12	I-MethodName
-	I-MethodName
6	I-MethodName
λ=2.0	I-MethodName
outperform	O
all	O
models	O
in	O
comparison	O
(	O
including	O
student	O
models	O
and	O
even	O
the	O
teacher	O
model	O
)	O
on	O
CNNDM	B-DatasetName
.	O

Following	O
Durrett	O
et	O
al	O
.	O
(	O
2016	O
)	O
;	O
Liu	O
and	O
Lapata	O
(	O
2019	O
)	O
,	O
we	O
report	O
limited	O
-	O
length	O
recall	O
based	O
ROUGE-1	B-MetricName
,	O
ROUGE-2	B-MetricName
,	O
and	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
,	O
where	O
generated	O
summaries	O
are	O
truncated	O
to	O
the	O
lengths	O
of	O
gold	O
summaries	O
.	O

Here	O
,	O
we	O
consider	O
the	O
two	O
types	O
of	O
input	O
format	O
,	O
which	O
are	O
single	O
-	O
wordunit	O
and	O
hyperedge	O
-	O
based	O
representations	O
.	O

Ground	O
-	O
truth	O
Labels	O
The	O
present	O
research	O
was	O
supported	O
by	O
Zhejiang	O
Lab	O
(	O
No	O
.	O

While	O
we	O
can	O
not	O
directly	O
compare	O
GLM	B-MethodName
with	O
T5	B-MethodName
due	O
to	O
the	O
differences	O
in	O
training	O
data	O
and	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
parameters	I-HyperparameterName
,	O
the	O
results	O
in	O
Tables	O
1	O
and	O
6	O
have	O
demonstrated	O
the	O
advantage	O
of	O
GLM.Pretrained	B-MethodName
Language	O
Models	O
.	O

On	O
September	O
10	O
,	O
1968	O
,	O
he	O
was	O
traded	O
back	O
to	O
Los	O
Angeles	O
for	O
a	O
second	O
round	O
pick	O
in	O
the	O
1970	O
draft	O
.	O

In	O
calibrated	O
teacher	O
training	O
,	O
we	O
trained	O
for	O
3	B-HyperparameterValue
-	O
10	B-HyperparameterValue
epochs	B-HyperparameterName
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
2e-5	B-HyperparameterValue
.	O

Length	B-HyperparameterName
and	O
novel	B-HyperparameterValue
n	I-MetricName
-	I-MetricName
grams	I-MetricName
We	O
first	O
analyze	O
the	O
pseudo	O
summaries	O
generated	O
by	O
the	O
teacher	O
models	O
.	O

Being	O
guided	O
by	O
some	O
questions	O
,	O
the	O
agent	O
first	O
decomposes	O
the	O
task	O
to	O
obtain	O
a	O
set	O
of	O
available	O
subtasks	O
,	O
and	O
selects	O
one	O
from	O
them	O
.	O

Speech	B-TaskName
translation	I-TaskName
(	O
ST	B-TaskName
)	O
aims	O
at	O
translating	O
from	O
source	O
language	O
speech	O
into	O
target	O
language	O
text	O
,	O
which	O
is	O
widely	O
helpful	O
in	O
various	O
scenarios	O
such	O
as	O
conference	O
speeches	O
,	O
business	O
meetings	O
,	O
crossborder	O
customer	O
service	O
,	O
and	O
overseas	O
travel	O
.	O

We	O
ask	O
human	O
players	O
to	O
play	O
the	O
simple	O
games	O
,	O
and	O
answer	O
the	O
yes	O
-	O
or	O
-	O
no	O
questions	O
based	O
on	O
the	O
observations	O
.	O

It	O
corresponds	O
to	O
the	O
simplest	O
imagination	O
since	O
the	O
assumed	O
value	O
(	O
i.e.	O
,	O
c	O
i	O
)	O
is	O
explicitly	O
mentioned	O
in	O
the	O
assumption	O
.	O

With	O
the	O
goal	O
of	O
learning	O
a	O
new	O
task	O
with	O
very	O
few	O
(	O
usually	O
less	O
than	O
a	O
hundred	O
)	O
samples	O
,	O
few	O
-	O
shot	O
learning	O
benefits	O
from	O
the	O
prior	O
knowledge	O
stored	O
in	O
PLMs	O
.	O

Despite	O
the	O
effectiveness	O
,	O
there	O
are	O
two	O
major	O
challenges	O
for	O
RL	O
-	O
based	O
agents	O
,	O
preventing	O
them	O
from	O
being	O
deployed	O
in	O
real	O
world	O
applications	O
:	O
the	O
low	O
sample	O
efficiency	O
,	O
and	O
the	O
large	O
action	O
space	O
(	O
Dulac	O
-	O
Arnold	O
et	O
al	O
.	O
,	O
2021	O
)	O
.	O

In	O
this	O
work	O
we	O
take	O
a	O
step	O
further	O
and	O
study	O
search	O
and	O
retrieval	O
of	O
arbitrary	O
fingerspelled	O
content	O
in	O
real	O
-	O
world	O
American	O
Sign	O
Language	O
(	O
ASL	O
)	O
video	O
(	O
see	O
Figure	O
1	O
)	O
.	O

The	O
experimental	O
results	O
show	O
that	O
QWA	B-MethodName
achieves	O
high	O
sample	O
efficiency	O
in	O
solving	O
complex	O
games	O
.	O

The	O
model	O
is	O
trained	O
for	O
30	B-HyperparameterValue
epochs	B-HyperparameterName
and	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
decayed	O
to	O
0.001	B-HyperparameterValue
after	O
20	B-HyperparameterValue
epochs	B-HyperparameterName
.	O

This	O
work	O
is	O
supported	O
partly	O
by	O
the	O
Fundamental	O
Research	O
Funds	O
for	O
the	O
Central	O
Universities	O
and	O
by	O
the	O
State	O
Key	O
Laboratory	O
of	O
Software	O
Development	O
Environment	O
.	O

Here	O
,	O
we	O
note	O
that	O
BLSTM	B-MethodName
and	O
MemNN	B-MethodName
of	O
the	O
first	O
section	O
in	O
the	O
table	O
are	O
based	O
on	O
the	O
different	O
entity	O
linking	O
modules	O
with	O
top-1	O
precision	O
81.1	B-MetricValue
%	I-MetricValue
and	O
top-1	O
recall	O
82.2	B-MetricValue
%	I-MetricValue
1	O
.	O

We	O
find	O
a	O
sizable	O
amount	O
of	O
the	O
data	O
(	O
i.e.	O
,	O
13.59	B-HyperparameterValue
%	I-HyperparameterValue
)	O
to	O
be	O
non	O
-	O
Arabic	O
(	O
mostly	O
English	O
or	O
French	O
)	O
.	O

Irrespective	O
of	O
the	O
use	O
of	O
an	O
alternative	O
version	O
(	O
Equation	O
5	O
)	O
during	O
the	O
training	O
,	O
the	O
extent	O
to	O
which	O
the	O
activation	O
pattern	O
is	O
distilled	O
can	O
be	O
intuitively	O
observed	O
by	O
calculating	O
the	O
original	O
"	O
activation	B-HyperparameterValue
transfer	I-HyperparameterValue
loss	I-HyperparameterValue
"	O
(	O
Equation	O
4	O
)	O
.	O

YouTube	O
videos	O
are	O
mostly	O
ASL	O
lectures	O
with	O
high	O
resolution	O
.	O

In	O
the	O
fifth	O
block	O
,	O
we	O
additionally	O
conduct	O
selfdistillation	O
experiments	O
,	O
which	O
is	O
not	O
the	O
focus	O
of	O
this	O
work	O
.	O

.	O

ROT	B-MethodName
-	I-MethodName
k	O
is	O
a	O
simple	O
letter	O
substitution	O
cipher	O
that	O
replaces	O
a	O
letter	O
in	O
the	O
plaintext	O
with	O
the	O
kth	O
letter	O
after	O
it	O
in	O
the	O
alphabet	O
.	O

In	O
the	O
above	O
process	O
,	O
instead	O
of	O
sampling	O
s	O
from	O
a	O
noise	O
distribution	O
like	O
N	O
(	O
0	O
,	O
I	O
)	O
and	O
passing	O
it	O
through	O
a	O
generator	O
enc	O
ψ	O
,	O
we	O
feed	O
it	O
text	O
from	O
the	O
target	O
domain	O
T	O
and	O
a	O
decoder	O
dec	O
η	O
that	O
decodes	O
text	O
in	O
T	O
.	O

Minimizing	O
the	O
second	O
term	O
encourages	O
g	O
ω	O
q	O
j	O
to	O
better	O
estimate	O
the	O
retrieved	O
memory	O
values	O
{	O
V	O
s	O
l	O
}	O
N	O
l=1	O
.	O

Experimental	O
results	O
show	O
that	O
our	O
method	O
achieves	O
improved	O
performance	O
with	O
high	O
sample	O
efficiency	O
.	O

Using	O
a	O
pair	O
of	O
dialogue	O
rewards	O
R(τ	O
1	O
)	O
and	O
R(τ	O
2	O
)	O
,	O
we	O
compute	O
the	O
probabilistic	O
preference	O
between	O
the	O
roll	O
-	O
outs	O
P	O
[	O
τ	O
1	O
≻	O
τ	O
2	O
]	O
either	O
by	O
standard	O
normalization	O
or	O
a	O
softmax	O
function	O
.	O

Module	O
Design	O
.	O
Based	O
on	O
the	O
two	O
-	O
step	O
formulation	O
,	O
we	O
then	O
design	O
the	O
L2I	B-MethodName
module	O
as	O
neural	O
network	O
operations	O
.	O

We	O
train	O
the	O
modules	B-HyperparameterName
with	O
batch	B-HyperparameterName
size	I-HyperparameterName
128	B-HyperparameterValue
for	O
up	O
to	O
50	B-HyperparameterValue
epochs	B-HyperparameterName
.	O

We	O
followed	O
the	O
hyperparameters	O
used	O
in	O
TAPT	B-MethodName
except	O
for	O
batch	B-HyperparameterName
size	I-HyperparameterName
and	O
the	O
maximum	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
because	O
we	O
used	O
the	O
same	O
computing	O
resource	O
as	O
DoKTra	B-MethodName
for	O
a	O
fair	O
comparison	O
.	O

Our	O
results	O
confirm	O
the	O
utility	O
of	O
dedicated	O
language	O
models	O
as	O
compared	O
to	O
multilingual	O
models	O
such	O
as	O
mT5	B-MethodName
(	O
101	B-HyperparameterValue
+	I-HyperparameterValue
languages	B-HyperparameterName
)	O
.	O

We	O
find	O
that	O
the	O
accuracy	O
of	O
DANN	B-MethodName
on	O
the	O
ELECTRONICS	B-DatasetName
domain	O
reduces	O
by	O
∼3	B-HyperparameterName
points	I-HyperparameterValue
.	O

To	O
mitigate	O
the	O
two	O
issues	O
,	O
we	O
propose	O
a	O
knowledge	O
-	O
aware	O
fuzzy	O
semantic	O
parsing	O
framework	O
(	O
KaFSP	B-MethodName
)	O
.	O

the	O
smallest	O
portion	O
of	O
poke	O
possible	O
,	O
<	O
unk	O
>	O
overcooked	O
rice	O
,	O
and	O
barely	O
got	O
any	O
ponzu	O
.	O

Unlike	O
the	O
competing	O
losses	O
used	O
in	O
GANs	O
,	O
we	O
introduce	O
cooperative	B-HyperparameterValue
losses	I-HyperparameterValue
where	O
the	O
discriminator	O
and	O
the	O
generator	O
cooperate	O
and	O
reduce	O
the	O
same	O
loss	O
.	O

MSA	B-DatasetName
Vs	O
.	O

The	O
weights	O
are	O
set	O
as	O
follows	O
:	O
α	B-HyperparameterName
1	I-HyperparameterName
(	O
α	B-HyperparameterName
2	I-HyperparameterName
)	O
is	O
an	O
increasing	O
function	O
concerning	O
the	O
output	O
of	O
the	O
entity	O
recognizer	O
teacher	O
as	O
shown	O
in	O
Figure	O
4	O
.	O

The	O
training	O
procedure	O
includes	O
the	O
global	O
optimization	O
shared	O
across	O
tasks	O
and	O
the	O
local	O
adaptation	O
for	O
each	O
specific	O
task	O
.	O

We	O
find	O
the	O
data	O
to	O
have	O
4.14	B-HyperparameterValue
%	I-HyperparameterValue
non	O
-	O
Arabic	O
.	O

Note	O
that	O
there	O
is	O
no	O
overlapping	O
games	O
between	O
the	O
simple	O
set	O
and	O
the	O
medium	O
/	O
hard	O
game	O
sets	O
.	O

For	O
the	O
masked	O
spans	O
,	O
it	O
is	O
the	O
position	O
of	O
the	O
corresponding	O
[	O
MASK	O
]	O
token	O
.	O

6	O
shows	O
the	O
results	O
,	O
where	O
"	O
+	O
expTS	O
"	O
(	O
"	O
+	O
expAV	O
"	O
)	O
denotes	O
that	O
the	O
model	O
uses	O
an	O
expert	O
task	O
selector	O
(	O
action	O
validator	O
)	O
.	O

The	O
input	O
is	O
the	O
sentence	O
representation	O
of	O
the	O
sample	O
in	O
query	O
sets	O
encoded	O
by	O
the	O
key	O
network	O
,	O
and	O
the	O
output	O
is	O
the	O
memory	O
slots	O
similar	O
to	O
the	O
query	O
sample	O
.	O

We	O
then	O
study	O
a	O
multi	O
-	O
task	O
pretraining	O
setup	O
,	O
in	O
which	O
a	O
second	O
objective	O
of	O
generating	O
longer	O
text	O
is	O
jointly	O
optimized	O
with	O
the	O
blank	O
infilling	O
objective	O
.	O

We	O
train	O
the	O
GATA	B-MethodName
through	O
reinforcement	O
learning	O
,	O
the	O
experiment	O
setting	O
is	O
same	O
with	O
Sec	O
.	O

Besides	O
the	O
attention	B-HyperparameterName
temperatures	I-HyperparameterName
,	O
we	O
can	O
also	O
tune	O
the	O
temperature	B-HyperparameterName
T	O
in	O
the	O
decoder	O
output	O
softmax	O
layer	O
.	O

We	O
keep	O
using	O
the	O
same	O
subtask	O
T	O
over	O
time	O
until	O
it	O
is	O
not	O
included	O
in	O
T	O
t	O
,	O
as	O
we	O
do	O
not	O
want	O
the	O
agent	O
to	O
switch	O
subtasks	O
too	O
frequently	O
.	O

Moreover	O
,	O
we	O
conduct	O
each	O
experiment	O
5	O
times	O
and	O
report	O
the	O
mean	O
F1	B-MetricName
-	O
score	O
.	O

The	O
acquired	O
support	O
set	O
information	O
leveraged	O
by	O
the	O
imitation	O
module	O
augments	O
the	O
model	O
initialization	O
learning	O
,	O
enhancing	O
the	O
dependence	O
of	O
the	O
model	O
's	O
task	O
adaptation	O
on	O
support	O
sets	O
.	O

mT5	B-MethodName
(	O
Xue	O
et	O
al	O
.	O
,	O
2020	O
)	O
is	O
the	O
multilingual	O
version	O
of	O
Textto	B-TaskName
-	I-MethodName
Text	I-MethodName
Transfer	I-MethodName
Transformer	I-MethodName
model	O
(	O
T5	B-MethodName
)	O
(	O
Raffel	O
et	O
al	O
.	O
,	O
2019	O
As	O
we	O
have	O
demonstrated	O
,	O
our	O
resulting	O
models	O
are	O
better	O
equipped	O
to	O
power	O
applications	O
involving	O
several	O
varieties	O
of	O
Arabic	O
as	O
well	O
as	O
code	O
-	O
switched	O
language	O
use	O
involving	O
Arabic	O
.	O

AraT5	B-MethodName
MSA	I-MethodName
excels	O
with	O
20.61	B-MetricValue
%	I-MetricValue
BLEU	B-MetricName
on	O
ARGEN	B-DatasetName
NTG	I-DatasetName
and	O
AraT5	B-MethodName
is	O
at	O
16.99	B-MetricValue
%	I-MetricValue
on	O
ARGEN	B-DatasetName
QG	I-DatasetName
.For	O
the	O
paraphrasing	O
task	O
,	O
we	O
fine	O
-	O
tune	O
and	O
validate	O
on	O
our	O
new	O
AraPra	B-DatasetName
dataset	O
and	O
blind	O
-	O
test	O
on	O
both	O
APB	B-DatasetName
and	O
ASEP	B-DatasetName
datasets	O
(	O
described	O
in	O
§	O
3.6	O
)	O
.	O

Table	O
1	O
and	O
2	O
shows	O
the	O
statistics	O
of	O
all	O
datasets	O
.	O

Each	O
GCN	B-MethodName
model	O
consists	O
of	O
two	B-HyperparameterValue
propagation	B-HyperparameterName
layers	I-HyperparameterName
and	O
a	O
sum	B-HyperparameterName
pooling	I-HyperparameterName
layer	I-HyperparameterName
across	O
the	O
nodes	O
in	O
the	O
graph	O
.	O

1.We	O
introduce	O
pairwise	O
causal	O
reward	O
learning	O
to	O
learn	O
fine	O
grained	O
per	O
turn	O
reward	O
that	O
reason	O
the	O
intention	O
of	O
human	O
utterance	O
.	O

CASPI(MinTL	B-MethodName
)	I-MethodName
trained	O
only	O
on	O
20	B-HyperparameterValue
%	I-HyperparameterValue
of	O
data	O
was	O
able	O
to	O
out	O
perform	O
previous	O
state	O
of	O
the	O
art	O
method	O
,	O
LAVA	B-MethodName
(	O
Lubis	O
et	O
al	O
.	O
,	O
2020	O
)	O
and	O
MINTL	B-MethodName
(	O
Lin	O
et	O
al	O
.	O
,	O
2020	O
)	O
trained	O
on	O
100	B-HyperparameterValue
%	O
data	O
on	O
two	O
of	O
the	O
three	O
performance	O
metrics	O
.	O

We	O
fine	O
-	O
tune	O
the	O
off	O
-	O
the	O
-	O
shelf	O
pre	O
-	O
trained	O
BERT	B-MethodName
on	O
the	O
masked	B-TaskName
language	O
modeling	O
task	O
following	O
(	O
Dopierre	O
et	O
al	O
.	O
,	O
2021	O
)	O
as	O
it	O
greatly	O
improves	O
embeddings	O
'	O
quality	O
.	O

The	O
most	O
recent	O
is	O
the	O
creation	O
of	O
a	O
six	O
-	O
seat	O
district	O
that	O
includes	O
all	O
or	O
part	O
of	O
the	O
following	O
:	O
In	O
the	O
2009	O
elections	O
,	O
the	O
state	O
senate	O
members	O
were	O
elected	O
to	O
six	O
-	O
year	O
terms	O
.	O

We	O
remove	O
diacritics	O
and	O
replace	O
URLs	O
and	O
user	O
mentions	O
with	O
<	O
URL	O
>	O
and	O
<	O
USER	O
>	O
.	O

We	O
formulate	O
it	O
as	O
:	O
c	O
=	O
g(c	O
,	O
a	O
)	O
,	O
where	O
the	O
counterfactual	O
context	O
c	O
is	O
the	O
status	O
of	O
the	O
context	O
c	O
after	O
the	O
assumption	O
a	O
is	O
executed	O
.	O

It	O
was	O
one	O
of	O
two	O
stations	O
built	O
by	O
the	O
flushing	O
railroad	O
in	O
Corona	O
,	O
this	O
one	O
having	O
been	O
at	O
Grand	O
Avenue	O
(	O
later	O
called	O
National	O
Avenue	O
,	O
now	O
National	O
Street	O
)	O
and	O
45th	O
Avenue	O
.	O

We	O
adopt	O
Adam	O
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
to	O
optimize	O
all	O
learnable	O
parameters	O
in	O
the	O
model	O
.	O

For	O
each	O
episode	O
,	O
we	O
sample	O
a	O
game	O
from	O
the	O
training	O
set	O
to	O
interact	O
with	O
.	O

Thus	O
,	O
we	O
ablate	O
the	O
calibrated	O
teacher	O
training	O
steps	O
in	O
our	O
framework	O
and	O
compare	O
the	O
final	O
performances	O
and	O
loss	O
values	O
.	O

This	O
is	O
reasonable	O
since	O
the	O
average	O
length	O
of	O
both	O
assumption	O
and	O
question	O
are	O
only	O
around	O
10	B-HyperparameterValue
words	O
(	O
cf	O
.	O

To	O
handle	O
this	O
issue	O
,	O
we	O
reconstruct	O
the	O
data	O
to	O
pair	O
format	O
.	O

Two	O
of	O
these	O
are	O
natural	O
and	O
two	O
are	O
synthetic	O
,	O
as	O
follows	O
:	O
Natural	O
Code	O
-	O
Switched	O
Data	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
an	O
unsupervised	O
multipletask	O
and	O
multiple	O
-	O
teacher	O
model	O
for	O
cross	B-TaskName
-	I-TaskName
lingual	I-TaskName
NER	I-TaskName
.	O

That	O
demonstrates	O
the	O
benefits	O
of	O
our	O
proposed	O
MTMT	B-MethodName
model	O
,	O
compared	O
to	O
direct	O
model	O
transfer	O
(	O
Wu	O
and	O
Dredze	O
,	O
2019	O
)	O
.	O

We	O
observe	O
that	O
success	O
rate	O
,	O
if	O
used	O
as	O
is	O
,	O
will	O
result	O
in	O
non	O
-	O
markovian	O
and	O
stochastic	O
per	O
turn	O
reward	O
function	O
.	O

There	O
are	O
two	O
kinds	O
of	O
application	O
scenarios	O
,	O
including	O
the	O
non	O
-	O
streaming	O
translation	O
and	O
the	O
streaming	O
one	O
.	O

The	O
model	O
architecture	O
and	O
detailed	O
operation	O
of	O
hypergraph	B-MethodName
attention	I-MethodName
networks	I-MethodName
are	O
similar	O
to	O
that	O
of	O
BAN	B-MethodName
.	O

Our	O
framework	O
reveals	O
new	O
insights	O
:	O
(	O
1	O
)	O
both	O
the	O
absolute	O
performance	O
and	O
relative	O
gap	O
of	O
the	O
methods	O
were	O
not	O
accurately	O
estimated	O
in	O
prior	O
literature	O
;	O
(	O
2	O
)	O
no	O
single	O
method	O
dominates	O
most	O
tasks	O
with	O
consistent	O
performance	O
;	O
(	O
3	O
)	O
improvements	O
of	O
some	O
methods	O
diminish	O
with	O
a	O
larger	O
pretrained	O
model	O
;	O
and	O
(	O
4	O
)	O
gains	O
from	O
different	O
methods	O
are	O
often	O
complementary	O
and	O
the	O
best	O
combined	O
model	O
performs	O
close	O
to	O
a	O
strong	O
fully	O
-	O
supervised	O
baseline	O
.	O

Searching	O
in	O
an	O
open	O
-	O
vocabulary	O
setting	O
,	O
including	O
proper	O
nouns	O
,	O
typically	O
requires	O
searching	O
for	O
fingerspelling	O
.	O

The	O
station	O
closed	O
on	O
september	O
15	O
,	O
1927	O
,	O
with	O
the	O
train	O
service	O
transferred	O
from	O
Grand	O
Avenue	O
to	O
45th	O
Avenue	O
.	O

Each	O
dataset	O
D	O
i	O
consists	O
of	O
a	O
support	O
set	O

•	O
Consistency	B-MetricName
:	O
C	B-MetricName
score	O
(	O
Madotto	O
et	O
al	O
.	O
,	O
2019	O
)	O
measures	O
the	O
consistency	O
between	O
the	O
generated	O
responses	O
and	O
persona	O
descriptions	O
through	O
a	O
pretrained	O
natural	O
language	O
inference	O
model	O
.	O

First	O
,	O
the	O
subtasks	O
are	O
easier	O
to	O
solve	O
,	O
as	O
the	O
involved	O
temporal	O
dependencies	O
are	O
usually	O
shortterm	O
.	O

The	O
first	O
section	O
in	O
the	O
table	O
includes	O
fully	O
-	O
supervised	O
models	O
which	O
require	O
a	O
ground	O
-	O
truth	O
path	O
annotation	O
as	O
an	O
additional	O
supervision	O
.	O

It	O
takes	O
about	O
45	O
minutes	O
for	O
one	O
epoch	O
,	O
and	O
we	O
need	O
6	B-HyperparameterValue
epochs	B-HyperparameterName
in	O
total	O
.	O

Note	O
that	O
a	O
weighting	O
strategy	O
is	O
also	O
provide	O
therein	O
to	O
take	O
into	O
consideration	O
of	O
the	O
reliability	O
of	O
the	O
teachers	O
.	O

The	O
fine	O
-	O
tuned	O
BERT	B-MethodName
is	O
then	O
used	O
as	O
the	O
initialization	O
for	O
all	O
few	O
-	O
shot	O
models	O
.	O

Therefore	O
,	O
it	O
is	O
still	O
inferior	O
to	O
our	O
model	O
.	O

i	O
=	O
(	O
M	O
q	O
W	O
q	O
)	O
i	O
⊤	O
A(M	O
k	O
W	O
k	O
)	O
i	O

The	O
attention	O
mechanism	O
enables	O
the	O
model	O
to	O
implicitly	O
localize	O
frames	O
relevant	O
to	O
the	O
text	O
.	O

In	O
other	O
words	O
,	O
calibration	O
on	O
the	O
teacher	O
training	O
clearly	O
aids	O
the	O
supervision	O
of	O
the	O
teacher	O
in	O
activation	O
boundary	O
distillation	O
,	O
even	O
though	O
the	O
output	O
probability	O
information	O
is	O
not	O
directly	O
used	O
in	O
distillation	O
.	O

Consistency	B-MethodName
measures	O
the	O
task	O
consistency	O
between	O
the	O
generated	O
responses	O
and	O
the	O
person	O
's	O
persona	O
description	O
.	O

We	O
also	O
clean	O
the	O
data	O
by	O
removing	O
HTML	O
tags	O
,	O
elongation	O
,	O
and	O
the	O
hash	O
signs	O
.	O

All	O
our	O
Dev	O
results	O
are	O
in	O
Section	O
C.2	O
in	O
the	O
Appendix	O
.	O

For	O
multi	O
-	O
task	O
pretraining	O
,	O
we	O
train	O
two	O
Largesized	O
models	O
with	O
a	O
mixture	O
of	O
the	O
blank	O
infilling	O
objective	O
and	O
the	O
document	O
-	O
level	O
or	O
sentencelevel	O
objective	O
,	O
denoted	O
as	O
GLM	B-MethodName
Doc	I-MethodName
and	O
GLM	B-MethodName
Sent	I-MethodName
.	O

Then	O
,	O
the	O
two	O
graph	O
representations	O
are	O
concatenated	O
and	O
fed	O
into	O
a	O
single	B-HyperparameterValue
layer	I-HyperparameterValue
feed	I-HyperparameterName
-	I-HyperparameterName
forward	I-HyperparameterName
layer	I-HyperparameterName
to	O
get	O
joint	O
representation	O
.	O

We	O
first	O
generate	O
multiple	O
ROT	O
-	O
k	O
ciphertexts	O
using	O
different	O
values	O
of	O
k	O
for	O
the	O
plaintext	O
which	O
is	O
the	O
source	O
side	O
of	O
the	O
parallel	O
data	O
.	O

The	O
conclusions	O
also	O
hold	O
for	O
ChicagoF	B-DatasetName
-	I-DatasetName
SWild+.In	I-DatasetName
the	O
previous	O
section	O
we	O
have	O
seen	O
that	O
models	O
that	O
explicitly	O
detect	O
and	O
localize	O
fingerspelling	O
outperform	O
ones	O
that	O
do	O
not	O
.	O

We	O
typically	O
(	O
i.e.	O
,	O
in	O
all	O
our	O
experiments	O
)	O
identify	O
the	O
best	O
checkpoint	O
for	O
each	O
model	O
on	O
the	O
development	O
set	O
,	O
and	O
report	O
its	O
performance	O
on	O
both	O
development	O
and	O
test	O
data	O
.	O

For	O
example	O
,	O
T5	B-MethodName
(	O
Raffel	O
et	O
al	O
.	O
,	O
2020	O
)	O
is	O
pre	O
-	O
trained	O
by	O
predicting	O
corrupted	O
text	O
spans	O
.	O

We	O
find	O
that	O
λ	B-HyperparameterName
=	O
1.5	B-HyperparameterValue
or	O
λ	B-HyperparameterName
=	O
2.0	B-HyperparameterValue
usually	O
works	O
well	O
in	O
practice	O
.	O

†	O
Work	O
is	O
done	O
while	O
at	O
ByteDance	O
.	O

In	O
this	O
work	O
,	O
we	O
offer	O
the	O
first	O
comparison	O
of	O
the	O
mT5	B-MethodName
model	O
to	O
similar	O
encoder	O
-	O
decoder	O
models	O
dedicated	O
to	O
Arabic	O
.	O

Local	O
adaptation	O
fine	O
-	O
tunes	O
the	O
value	O
predictor	O
on	O
those	O
retrieved	O
slots	O
.	O

Figure	O
3	O
shows	O
that	O
introducing	O
the	O
cooperative	B-HyperparameterValue
losses	I-HyperparameterValue
significantly	O
outperform	O
DRG	B-MethodName
and	O
ARAE	B-MethodName
in	O
maintaining	O
constraints	O
.	O

The	O
statistics	O
of	O
the	O
pre	O
-	O
processed	O
downstream	O
task	O
datasets	O
are	O
listed	O
in	O
For	O
the	O
experiments	O
,	O
we	O
used	O
the	O
pre	O
-	O
trained	O
BioBERT	B-MethodName
-	I-MethodName
base	I-MethodName
model	O
(	O
L=12	B-HyperparameterName
,	O
H=768	B-HyperparameterName
,	O
A=12	B-HyperparameterName
)	O
as	O
the	O
initial	O
teacher	O
model	O
.	O

These	O
include	O
:	O
1	O
)	O
inform	O
ratemeasures	O
the	O
fraction	O
of	O
dialogue	O
,	O
the	O
system	O
has	O
provided	O
the	O
correct	O
entity	O
,	O
2	O
)	O
success	O
rate	O
-fraction	O
of	O
dialogues	O
,	O
the	O
system	O
has	O
answered	O
all	O
the	O
requested	O
information	O
and	O
3	O
)	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
.	O
,	O
2002	O
)	O
-measures	O
the	O
fluency	O
of	O
the	O
generated	O
response	O
.	O

According	O
to	O
the	O
ACM	O
's	O
definitions	O
(	O
Association	O
for	O
Computing	O
Machinery	O
,	O
2020	O
)	O
,	O
results	O
have	O
been	O
reproduced	O
if	O
obtained	O
in	O
a	O
different	O
study	O
by	O
a	O
different	O
team	O
using	O
artifacts	O
supplied	O
in	O
part	O
by	O
the	O
original	O
authors	O
,	O
and	O
replicated	O
if	O
obtained	O
in	O
a	O
different	O
study	O
by	O
a	O
different	O
team	O
using	O
artifacts	O
not	O
supplied	O
by	O
the	O
original	O
authors	O
.	O

To	O
imitate	O
the	O
zero	O
-	O
resource	O
cross	B-TaskName
lingual	I-TaskName
NER	I-TaskName
case	O
,	O
following	O
(	O
Wu	O
and	O
Dredze	O
,	O
2019	O
)	O
,	O
we	O
used	O
English	O
as	O
the	O
source	O
language	O
and	O
other	O
languages	O
as	O
the	O
target	O
language	O
.	O

Hence	O
,	O
we	O
also	O
use	O
a	O
soft	O
version	O
of	O
the	O
metric	O
M	O
sof	O
t	O
,	O
where	O
the	O
success	B-MetricName
rate	O
measures	O
a	O
fraction	O
of	O
requested	O
information	O
provided	O
in	O
a	O
dialogue	O
.	O

we	O
had	O
a	O
party	O
of	O
10	O
and	O
they	O
were	O
very	O
accommodating	O
to	O
our	O
group	O
of	O
us	O
.	O

Comparison	O
with	O
sampling	O
and	O
tuning	O
output	O
layer	O
temperature	O
Sampling	O
based	O
methods	O
can	O
produce	O
more	O
diverse	O
and	O
richer	O
outputs	O
than	O
its	O
beam	O
search	O
based	O
counterpart	O
and	O
has	O
been	O
proven	O
useful	O
in	O
back	O
translation	O
(	O
Edunov	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

Through	O
multi	O
-	O
task	O
learning	O
of	O
different	O
pretraining	O
objectives	O
,	O
a	O
single	O
GLM	B-MethodName
can	O
excel	O
in	O
both	O
NLU	B-TaskName
and	O
(	O
conditional	B-TaskName
and	O
unconditional	O
)	O
text	B-TaskName
generation	I-TaskName
.	O

It	O
is	O
a	O
large	O
scale	O
multidomain	O
,	O
task	O
oriented	O
dataset	O
generated	O
by	O
human	O
-	O
to	O
-	O
human	O
conversation	O
,	O
where	O
one	O
participant	O
plays	O
the	O
role	O
of	O
a	O
user	O
while	O
the	O
other	O
plays	O
the	O
agent	O
.	O

Specifically	O
,	O
we	O
use	O
a	O
two	B-HyperparameterValue
-	O
layer	O
fully	O
-	O
connected	O
network	O
g	O
ω	O
with	O
parameters	O
ω	O
to	O
build	O
the	O
mapping	O
.	O

Specifically	O
,	O
we	O
select	O
two	O
PLMs	O
as	O
the	O
initial	O
student	O
model	O
:	O
ALBERT	B-MethodName
-	I-MethodName
xlarge	I-MethodName
(	O
Lan	O
et	O
al	O
.	O
,	O
2019	O
)	O
,	O
which	O
has	O
a	O
smaller	O
number	O
of	O
parameters	O
but	O
performs	O
better	O
than	O
BERT	B-MethodName
,	O
and	O
RoBERTa	B-MethodName
-	I-MethodName
large	I-MethodName
,	O
which	O
has	O
a	O
larger	O
number	O
of	O
parameters	O
and	O
is	O
known	O
to	O
outperform	O
BERT	B-MethodName
significantly	O
for	O
most	O
of	O
the	O
tasks	O
.	O

The	O
hyperparameters	O
of	O
the	O
activation	O
boundary	O
distillation	O
for	O
the	O
RoBERTa	B-MethodName
student	O
are	O
searched	O
in	O
the	O
same	O
manner	O
with	O
the	O
ALBERT	B-MethodName
and	O
summarized	O
in	O
Table	O
A3.In	O
this	O
section	O
,	O
we	O
report	O
on	O
the	O
details	O
of	O
two	O
financial	O
downstream	O
task	O
datasets	O
,	O
the	O
experimental	O
details	O
,	O
and	O
hyperparameters	O
of	O
the	O
financial	O
task	O
experiments	O
.	O

For	O
example	O
,	O
XL	B-MethodName
-	I-MethodName
Net	I-MethodName
(	O
Yang	O
et	O
al	O
.	O
,	O
2019	O
)	O
encodes	O
the	O
original	O
position	O
so	O
that	O
it	O
can	O
perceive	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
missing	I-HyperparameterName
tokens	I-HyperparameterName
,	O
and	O
SpanBERT	B-MethodName
(	O
Joshi	O
et	O
al	O
.	O
,	O
2020	O
)	O
replaces	O
the	O
span	O
with	O
multiple	O
[	O
MASK	O
]	O
tokens	O
and	O
keeps	O
the	O
length	O
unchanged	O
.	O

Potential	O
future	O
work	O
may	O
explore	O
reinforcement	O
learning	O
losses	O
to	O
directly	O
optimize	O
the	O
constraints	O
.	O

In	O
this	O
paper	O
,	O
we	O
proposed	O
Hypergraph	B-MethodName
Transformer	I-MethodName
for	O
multi	O
-	O
hop	O
reasoning	O
over	O
knowledge	O
graph	O
under	O
weak	O
supervision	O
.	O

The	O
transition	O
data	O
for	O
AP	B-TaskName
task	O
is	O
collected	O
from	O
the	O
FTWP	B-DatasetName
game	O
set	O
and	O
is	O
provided	O
by	O
GATA	B-MethodName
's	O
released	O
code	O
.	O

As	O
shown	O
in	O
Table	O
7	O
of	O
Appendix	O
E	O
,	O
our	O
model	O
shows	O
the	O
best	O
performances	O
for	O
both	O
original	O
and	O
paraphrased	O
questions	O
.	O

•	O
We	O
construct	O
a	O
challenging	O
HQA	B-TaskName
dataset	O
and	O
conduct	O
extensive	O
experiments	O
on	O
the	O
dataset	O
,	O
where	O
the	O
performance	O
validates	O
the	O
rationality	O
and	O
effectiveness	O
of	O
the	O
proposed	O
L2I.In	B-MethodName
the	O
general	O
setting	O
of	O
machine	O
reading	O
comprehension	O
,	O
the	O
task	O
is	O
to	O
answer	O
a	O
question	O
according	O
to	O
the	O
facts	O
in	O
a	O
given	O
context	O
.	O

An	O
α	B-HyperparameterName
of	O
0.4	B-HyperparameterValue
is	O
considered	O
good	O
agreeement	O
(	O
Hedayatnia	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

The	O
summaries	O
are	O
extremely	O
abstractive	O
.	O

Parameter	O
settings	O
.	O
We	O
implement	O
TAGOP	B-MethodName
-	I-MethodName
L2I	I-MethodName
based	O
on	O
TAGOP	B-MethodName
4	I-MethodName
.	O

For	O
a	O
fair	O
comparison	O
with	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O
2019	O
)	O
,	O
we	O
use	O
BooksCorpus	B-DatasetName
(	O
Zhu	O
et	O
al	O
.	O
,	O
2015	O
)	O
and	O
English	B-DatasetName
Wikipedia	I-DatasetName
as	O
our	O
pretraining	O
data	O
.	O

We	O
uses	O
three	O
evaluations	O
metrics	O
proposed	O
by	O
(	O
Budzianowski	O
et	O
al	O
.	O
,	O
2018a	O
)	O
.	O

Pre	O
-	O
Training	O
.	O

On	O
November	O
15	O
,	O
1970	O
,	O
the	O
Los	O
Angeles	O
Rams	O
acquired	O
Smith	O
from	O
the	O
Lions	O
in	O
exchange	O
for	O
Linebacker	O
Tony	O
Harris	O
.	O

These	O
results	O
are	O
striking	O
since	O
our	O
language	O
models	O
are	O
pre	O
-	O
trained	O
on	O
Arabic	O
data	O
only	O
(	O
although	O
they	O
include	O
English	O
vocabulary	O
and	O
marginal	O
amounts	O
of	O
code	O
-	O
switching	O
;	O
see	O
§	O
2.1	O
)	O
.	O

λ	B-HyperparameterName
det	I-HyperparameterName
in	O
equation	O
4	O
is	O
0.1	B-HyperparameterValue
(	O
chosen	O
from	O
{	O
0.1	B-HyperparameterValue
,	O
0.5	B-HyperparameterValue
,	O
1.0	B-HyperparameterValue
}	O
)	O
.	O

Here	O
the	O
user	O
has	O
requested	O
for	O
a	O
taxi	O
,	O
before	O
enough	O
information	O
such	O
as	O
destination	O
or	O
time	O
of	O
departure	O
are	O
gathered	O
,	O
the	O
agent	O
books	O
the	O
taxi	O
.	O

BAN	B-MethodName
calculates	O
soft	O
attention	O
scores	O
between	O
knowledge	O
entities	O
and	O
question	O
words	O
as	O
follows	O
:	O

It	O
consists	O
of	O
1	O
,	O
010	O
Arabic	O
sentence	O
pairs	O
that	O
are	O
collected	O
from	O
different	O
Arabic	O
books	O
.	O

Thanks	O
to	O
the	O
autoregressive	O
blank	O
infilling	O
mechanism	O
we	O
proposed	O
,	O
we	O
can	O
obtain	O
all	O
the	O
log	O
-	O
probabilities	O
in	O
one	O
pass	O
.	O

)	O
.	O

We	O
tried	O
different	O
temperatures	B-HyperparameterName
of	O
scaling	O
the	O
softmax	O
(	O
Guo	O
et	O
al	O
.	O
,	O
2017	O
)	O
-0.4	O
,	O
0.5	B-HyperparameterValue
,	O
0.6	B-HyperparameterValue
,	O
0.7	B-HyperparameterValue
and	O
chose	O
the	O
one	O
that	O
produced	O
the	O
best	O
result	O
on	O
the	O
dev	O
set	O
.	O

Models	O
with	O
blank	O
-	O
infilling	O
objectives	O
,	O
such	O
as	O
T5	B-MethodName
and	O
our	O
GLM	B-MethodName
,	O
benefits	O
more	O
from	O
converting	O
the	O
NLU	B-TaskName
tasks	O
into	O
cloze	O
questions	O
.	O

Personalized	B-TaskName
Language	I-TaskName
Modeling	I-TaskName
.	O

Then	O
MAML	B-MethodName
updates	O
its	O
initialization	O
(	O
a.k.a	O
.	O

We	O
compare	O
ARAE	B-MethodName
seq2seq	I-MethodName
with	O
the	O
following	O
baselines	O
:	O
a	O
)	O
DRG	B-MethodName
:	O
The	O
Delete	O
,	O
Retrieve	O
,	O
Generate	O
method	O
that	O
deletes	O
domain	O
specific	O
attributes	O
,	O
retrieves	O
a	O
template	O
and	O
generates	O
the	O
target	O
domain	O
text	O
(	O
Li	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

However	O
,	O
such	O
uncontrollable	O
model	O
(	O
Zou	O
et	O
al	O
.	O
,	O
2021	O
)	O
can	O
hardly	O
generate	O
high	O
-	O
quality	O
context	O
for	O
two	O
reasons	O
:	O
1	O
)	O
the	O
context	O
is	O
more	O
complex	O
than	O
plain	O
text	O
,	O
which	O
can	O
include	O
a	O
table	O
(	O
Figure	O
1	O
)	O
;	O
and	O
2	O
)	O
NDR	B-TaskName
requires	O
a	O
precise	O
context	O
with	O
the	O
correct	O
numbers	O
(	O
Figure	O
1	O
,	O
$	O
132,935	O
for	O
the	O
finished	O
goods	O
in	O
2019	O
)	O
.	O

Usually	O
,	O
the	O
assumption	O
appears	O
either	O
before	O
of	O
after	O
the	O
factual	O
question	O
.	O

GLM	B-MethodName
uses	O
a	O
single	B-HyperparameterValue
Transformer	O
with	O
several	O
modifications	O
to	O
the	O
architecture	O
:	O
(	O
1	O
)	O
we	O
rearrange	O
the	O
order	O
of	O
layer	O
normalization	O
and	O
the	O
residual	O
connection	O
,	O
which	O
has	O
been	O
shown	O
critical	O
for	O
large	O
-	O
scale	O
language	O
models	O
to	O
avoid	O
numerical	O
errors	O
(	O
Shoeybi	O
et	O
al	O
.	O
,	O
2019	O
)	O
;	O
(	O
2	O
)	O
we	O
use	O
a	O
single	B-HyperparameterValue
linear	B-HyperparameterName
layer	I-HyperparameterName
for	O
the	O
output	O
token	O
prediction	O
;	O

He	O
was	O
also	O
a	O
voice	O
actor	O
for	O
"	O
the	O
Simpsons	O
"	O
as	O
well	O
as	O
"	O
the	O
marvelous	O
misadventures	O
of	O
superman	O
.	O

Among	O
many	O
self	O
-	O
supervised	O
metric	O
losses	O
such	O
as	O
Triplet	B-MetricName
Loss	I-MetricName
(	O
Hoffer	O
and	O
Ailon	O
,	O
2015	O
)	O
and	O
NT-	B-MetricName
2	I-MethodName
)	O
Train	O
the	O
Critic	O
:	O

1	O
(	O
right	O
part	O
)	O
illustrates	O
the	O
mechanism	O
of	O
local	O
adaptation	O
.	O

The	O
evaluation	O
metrics	O
are	O
the	O
scores	O
of	O
BLEU-1	B-MetricName
,	O
BLEU-2	B-MetricName
,	O
BLEU-3	B-MetricName
,	O
BLEU-4	B-MetricName
(	O
Papineni	O
et	O
al	O
.	O
,	O
2002	O
)	O
,	O
METEOR	B-MethodName
(	O
Denkowski	O
and	O
Lavie	O
,	O
2014	O
)	O
and	O
Rouge	B-MetricName
-	I-DatasetName
L	I-DatasetName
(	O
Lin	O
,	O
2004	O
)	O
.	O

i	O
ordered	O
the	O
chicken	O
chimichanga	O
and	O
it	O
was	O
just	O
plain	O
gross	O
.	O

It	O
is	O
not	O
surprising	O
that	O
the	O
variant	O
"	O
IL	B-MethodName
w/o	I-MethodName
FT	I-MethodName
"	O
also	O
performs	O
well	O
on	O
simple	O
games	O
,	O
since	O
they	O
are	O
only	O
pre	O
-	O
trained	O
with	O
simple	O
games	O
.	O

While	O
we	O
find	O
some	O
of	O
these	O
to	O
be	O
actual	O
dialectal	O
text	O
(	O
usually	O
short	O
belonging	O
to	O
either	O
Egyptian	O
or	O
Saudi	O
dialects	O
)	O
from	O
web	O
fora	O
,	O
in	O
the	O
majority	O
of	O
cases	O
the	O
text	O
is	O
simply	O
names	O
of	O
soap	O
operas	O
or	O
advertisements	O
.	O

Recent	O
work	O
has	O
suggested	O
that	O
there	O
are	O
several	O
benefits	O
to	O
personalized	O
models	O
in	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
over	O
one	O
-	O
size	O
-	O
fits	O
-	O
all	O
solutions	O
:	O
they	O
are	O
more	O
accurate	O
for	O
individual	O
users	O
;	O
they	O
help	O
us	O
understand	O
communities	O
better	O
;	O
and	O
they	O
focus	O
the	O
attention	O
of	O
our	O
evaluations	O
on	O
the	O
enduser	O
(	O
Flek	O
,	O
2020	O
)	O
.	O

That	O
is	O
,	O
the	O
teacher	O
model	O
has	O
the	O
same	O
as	O
the	O
neural	O
network	O
structure	O
of	O
the	O
student	O
model	O
.	O

there	O
was	O
only	O
one	O
other	O
person	O
in	O
the	O
<	O
unk	O
>	O
We	O
would	O
like	O
to	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
useful	O
suggestions	O
.	O

However	O
we	O
find	O
that	O
with	O
higher	O
values	O
of	O
p	O
,	O
there	O
is	O
a	O
trade	O
-	O
off	O
with	O
SIM	B-MetricName
resulting	O
in	O
a	O
lower	O
AGG	B-MetricName
score	O
overall	O
-similar	O
to	O
Krishna	O
et	O
al	O
.	O
(	O
2020).The	O
number	B-HyperparameterName
of	O
positive	O
and	O
negative	O
samples	O
used	O
for	O
contrastive	O
learning	O
(	O
Eq	O
.	O

Then	O
we	O
can	O
acquire	O
the	O
locally	O
adapted	O
value	O
prediction	O
network	O
g	O
ω	O
q	O
j	O
with	O
parameters	O
ω	O
q	O
j	O
=	O
arg	O
miñ	O
ω	O
L	O
loc	O
(	O
ω	O
)	O
.	O

Following	O
(	O
Wu	O
and	O
Dredze	O
,	O
2019	O
)	O
,	O
all	O
datasets	O
are	O
annotated	O
using	O
the	O
BIO	O
entity	O
labelling	O
scheme	O
.	O

16	O
We	O
observe	O
that	O
our	O
model	O
outperforms	O
mT5	B-MethodName
in	O
the	O
four	O
X	O
→	O
Arabic	O
sub	O
-	O
tasks	O
with	O
an	O
average	O
of	O
+1.12	O
and	O
+0.86	O
BLEU	B-MetricName
points	O
on	O
Dev	O
and	O
Test	O
,	O
respectively	O
.	O

As	O
Table	O
B.2	O
shows	O
,	O
our	O
AraT5	B-MethodName
model	O
achieves	O
the	O
highest	O
AR	B-DatasetName
-	I-DatasetName
LUE	I-DatasetName
score	O
(	O
77.52	O
)	O
,	O
followed	O
by	O
AraT5	B-MethodName
MSA	I-MethodName
(	O
77.50	B-MetricValue
)	O
and	O
AraT5	B-MethodName
TW	I-MethodName
(	O
75.33	B-HyperparameterValue
)	O
.	O

In	O
order	O
to	O
analyze	O
MSA	O
-	O
dialect	O
distribution	O
in	O
our	O
Twitter	B-DatasetName
data	O
,	O
we	O
run	O
the	O
binary	O
(	O
MSA	O
-	O
dialect	O
)	O
classifier	O
introduced	O
in	O
Abdul	O
-	O
Mageed	O
et	O
al	O
.	O
(	O
2020b	O
)	O
on	O
a	O
random	O
sample	O
of	O
100	B-HyperparameterValue
M	I-HyperparameterValue
tweets	B-HyperparameterName
.	O

The	O
parser	O
-	O
based	O
agents	O
generate	O
actions	O
word	O
by	O
word	O
,	O
leading	O
to	O
a	O
huge	O
combinatorial	O
action	O
space	O
(	O
Kohita	O
et	O
al	O
.	O
,	O
2021	O
)	O
.	O

It	O
is	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
method	O
on	O
TAT	B-DatasetName
-	I-DatasetName
QA	I-DatasetName
dataset	O
.	O

it	O
's	O
not	O
much	O
flavor	O
,	O
but	O
the	O
meat	O
is	O
dry	O
.	O

We	O
speculate	O
the	O
reason	O
may	O
be	O
that	O
,	O
unlike	O
summarization	B-TaskName
,	O
outputs	O
of	O
the	O
machine	B-TaskName
translation	I-TaskName
task	O
are	O
relatively	O
fixed	O
.	O

AP@IoU	B-MetricName
measures	O
the	O
average	O
precision	O
of	O
a	O
detector	O
under	O
the	O
constraint	O
that	O
the	O
overlap	O
of	O
its	O
predicted	O
segments	O
with	O
the	O
ground	O
truth	O
is	O
above	O
some	O
threshold	O
Intersectionover	O
-	B-MetricName
Union	I-MetricName
(	O
IoU	B-MetricName
)	O
value	O
.	O

External	O
detector	O
(	O
Ext	O
-	O
Det	O
)	O
This	O
baseline	O
uses	O
the	O
off	O
-	O
the	O
-	O
shelf	O
fingerspelling	O
detectors	O
of	O
(	O
Shi	O
et	O
al	O
.	O
,	O
2021	O
)	O
to	O
generate	O
fingerspelling	O
proposals	O
,	O
instead	O
of	O
our	O
proposal	O
generator	O
,	O
and	O
is	O
otherwise	O
identical	O
to	O
FSS	B-MethodName
-	I-MethodName
Net	I-MethodName
.	O

Table	O
4	O
shows	O
the	O
performance	O
of	O
the	O
compared	O
methods	O
on	O
the	O
TAT	B-DatasetName
-	I-DatasetName
HQA	I-DatasetName
dataset	O
.	O

Paraphrasing	B-TaskName
,	O
Transliteration	B-TaskName
,	O
and	O
Title	B-TaskName
Generation	I-TaskName
.	O

worst	O
chinese	O
food	O
experience	O
i	O
ever	O
had	O
.	O

Our	O
model	O
outperforms	O
all	O
of	O
the	O
alternatives	O
.	O

Our	O
work	O
defines	O
the	O
different	O
constraints	O
that	O
should	O
be	O
preserved	O
and	O
adds	O
simple	O
differentiable	O
contrastive	O
learning	O
losses	O
to	O
preserve	O
them	O
.	O

Although	O
we	O
only	O
collect	O
labeled	O
data	O
from	O
the	O
simple	O
games	O
,	O
it	O
is	O
still	O
burdensome	O
for	O
human	O
players	O
to	O
go	O
through	O
the	O
games	O
and	O
answer	O
the	O
questions	O
.	O

12	O
We	O
only	O
include	O
titles	O
with	O
at	O
least	O
three	O
words	O
in	O
this	O
dataset	O
.	O

We	O
finetune	O
GLM	B-MethodName
LARGE	I-MethodName
on	O
the	O
training	O
set	O
for	O
4	B-HyperparameterValue
epochs	B-HyperparameterName
with	O
AdamW	B-HyperparameterValue
optimizer	B-HyperparameterName
.	O

Thus	O
,	O
we	O
apply	O
a	O
confidence	O
penalty	O
regularization	O
in	O
the	O
refinement	O
step	O
.	O

We	O
propose	O
a	O
general	O
pretraining	O
framework	O
GLM	B-MethodName
based	O
on	O
a	O
novel	O
autoregressive	O
blank	O
infilling	O
objective	O
.	O

We	O
include	O
datasets	O
of	O
varied	O
length	O
and	O
complexity	O
.	O

Examples	O
where	O
our	O
system	O
fails	O
with	O
plausible	O
explanation	O
are	O
given	O
in	O
Table	O
9	O
.	O

We	O
conjecture	O
that	O
mT5	B-MethodName
good	O
performance	O
on	O
English	O
code	O
-	O
switched	O
data	O
is	O
due	O
to	O
it	O
being	O
pre	O
-	O
trained	O
on	O
very	O
large	O
amounts	O
of	O
English	O
rather	O
than	O
natural	O
code	O
-	O
switching	O
.	O

We	O
use	O
paragraphs	O
rather	O
than	O
whole	O
documents	O
for	O
a	O
more	O
fine	O
-	O
grained	O
analysis	O
that	O
is	O
more	O
comparable	O
to	O
our	O
own	O
data	O
(	O
especially	O
in	O
the	O
case	O
of	O
Twitter	O
)	O
.	O

As	O
to	O
TAGOP	B-MethodName
-	I-MethodName
L2I	I-MethodName
,	O
the	O
gap	O
between	O
arithmetic	O
question	O
and	O
other	O
types	O
of	O
question	O
largely	O
reduces	O
,	O
validating	O
the	O
effectiveness	O
of	O
learning	O
intervention	O
with	O
discrete	O
operators	O
and	O
neural	O
network	O
modules	O
.	O

They	O
are	O
denoted	O
by	O
BART	B-MethodName
12	I-MethodName
-	I-MethodName
6	I-MethodName
,	O
BART	B-MethodName
12	I-MethodName
-	I-MethodName
3	I-MethodName
,	O
and	O
BART	B-MethodName
12	I-MethodName
-	I-MethodName
12	I-MethodName
with	O
6	B-HyperparameterValue
,	O
3	B-HyperparameterValue
,	O
and	O
12	B-HyperparameterValue
decoder	B-HyperparameterName
layers	I-HyperparameterName
,	O
respectively	O
.	O

After	O
feeding	O
the	O
memory	O
reading	O
output	O
of	O
a	O
query	O
-	O
set	O
sample	O
to	O
this	O
network	O
,	O
we	O
perform	O
local	O
adaptation	O
and	O
employ	O
the	O
adapted	O
network	O
to	O
estimate	O
the	O
value	O
for	O
the	O
query	O
sample	O
.	O

We	O
leverage	O
our	O
unlabeled	O
MSA	B-DatasetName
and	O
Twitter	B-DatasetName
data	O
described	O
in	O
§	O
2.1	O
to	O
pretrain	O
three	O
models	O
:	O
AraT5	B-MethodName
MSA	I-MethodName
on	O
MSA	B-DatasetName
data	O
,	O
AraT5	B-MethodName
TW	I-MethodName
on	O
twitter	O
data	O
,	O
and	O
AraT5	B-MethodName
on	O
both	O
MSA	O
and	O
twitter	O
data	O
using	O
the	O
T5	B-MethodName
Base	I-MethodName
encoderdecoder	O
architecture	O
(	O
Raffel	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

14	O
For	O
experiments	O
,	O
we	O
use	O
the	O
same	O
split	O
proposed	O
by	O
Shazal	O
et	O
al	O
.	O
(	O
2020	O
)	O
(	O
58.9	B-HyperparameterValue
K	I-HyperparameterValue
for	O
Train	O
and	O
5.4	B-HyperparameterValue
K	I-HyperparameterValue
for	O
Dev	O
and	O
Test	O
each	O
)	O
.	O

Among	O
the	O
works	O
that	O
uses	O
reinforcement	O
learning	O
.	O

In	O
order	O
to	O
fully	O
leverage	O
information	O
from	O
support	O
sets	O
,	O
we	O
construct	O
key	O
-	O
value	O
pairs	O
from	O
support	O
-	O
set	O
samples	O
and	O
store	O
them	O
in	O
the	O
memory	O
module	O
.	O

There	O
have	O
been	O
several	O
workshops	O
and	O
initiatives	O
on	O
reproducibility	O
,	O
including	O
workshops	O
at	O
ICML	B-DatasetName
2017	O
and	O
2018	O
,	O
the	O
reproducibility	O
challenge	O
at	O
ICLR	B-DatasetName
2018	O
and	O
2019	O
,	O
and	O
at	O
NeurIPS	B-DatasetName
2019	O
and	O
2020	O
,	O
the	O
RE	B-MethodName
-	I-MethodName
PROLANG	I-MethodName
(	O
Branco	O
et	O
al	O
.	O
,	O
2020	O
)	O
initiative	O
at	O
LREC	B-DatasetName
2020	O
,	O
and	O
the	O
ReproGen	O
shared	O
task	O
on	O
reproducibility	O
in	O
NLG	B-TaskName
(	O
Belz	O
et	O
al	O
.	O
,	O
2021b	O
)	O
.	O

The	O
ACM	O
originally	O
had	O
these	O
definitions	O
the	O
other	O
way	O
around	O
until	O
asked	O
by	O
ISO	O
to	O
bring	O
them	O
in	O
line	O
with	O
the	O
scientific	O
standard	O
(	O
ibid	O
.	O

there	O
was	O
one	O
chunk	O
of	O
chicken	O
and	O
<	O
unk	O
>	O
pieces	O
of	O
egg	O
in	O
the	O
food	O
was	O
just	O
ok	O
.	O

Compared	O
to	O
BERT	B-MethodName
with	O
cloze	O
-	O
style	O
finetuning	O
,	O
GLM	B-MethodName
benefits	O
from	O
the	O
autoregressive	O
pretraining	O
.	O

In	O
particular	O
,	O
the	O
unified	O
framework	O
that	O
converts	O
all	O
text	O
-	O
based	O
language	O
problems	O
into	O
a	O
text	O
-	O
to	O
-	O
text	O
format	O
presented	O
through	O
the	O
T5	B-MethodName
model	O
(	O
Raffel	O
et	O
al	O
.	O
,	O
2019	O
)	O
is	O
attractive	O
.	O

We	O
fix	O
the	O
maximum	B-HyperparameterName
vocabulary	B-HyperparameterName
size	I-HyperparameterName
for	O
YELP	B-DatasetName
,	O
IMDB	B-DatasetName
and	O
POLITICAL	B-DatasetName
at	O
30	B-HyperparameterValue
K	I-HyperparameterValue
which	O
is	O
also	O
the	O
default	O
maximum	B-HyperparameterName
vocab	B-HyperparameterName
size	I-HyperparameterName
used	O
in	O
(	O
Zhao	O
et	O
al	O
.	O
,	O
2018b	O
7	O
63.4	B-HyperparameterValue
36.7	B-HyperparameterValue
20.2	B-HyperparameterValue
96.0	B-HyperparameterValue
73.6	B-HyperparameterValue
35.4	B-HyperparameterValue
26.2	B-HyperparameterValue
98.6	B-HyperparameterValue
55.0	B-HyperparameterValue
44.4	B-HyperparameterValue
25.5	B-HyperparameterValue
nucleus(p	B-HyperparameterName
=	I-HyperparameterName
0.6	B-HyperparameterValue
)	O
85.6	B-HyperparameterValue
63.0	B-HyperparameterValue
36.6	B-HyperparameterValue
20.0	B-HyperparameterValue
95.8	B-HyperparameterValue
72.8	O
35.3	O
25.7	O
98.6	O
54.4	O
44.2	O
25	B-HyperparameterName
=	O
0.6	B-HyperparameterValue
)	O
89.4	O
68.6	O
32.8	O
20.4	O
97.1	O
82.6	O
33.6	O
27.4	O
99.0	O
56.0	O
41.6	O
24.4	O
Table	O
2	O
:	O
Evaluation	O
of	O
ARAE	B-MethodName
seq2seq	I-MethodName
with	O
ACC	B-MetricName
(	O
transfer	B-MetricName
accuracy	I-MetricName
)	O
,	O
FL	B-MetricName
(	O
fluency	B-MetricName
)	O
and	O
SIM	B-MetricName
(	O
semantic	B-MetricName
similarity	I-MetricName
)	O
,	O
AGG	B-MetricName
(	O
aggregate	B-MetricName
metric	I-MetricName
)	O
.	O

(	O
1	O
)	O
Source	O
:	O
:	O
MSA	O
Target	O
:	O

The	O
margin	O
m	O
,	O
number	B-HyperparameterName
of	I-HyperparameterName
negative	I-HyperparameterName
samples	I-HyperparameterName
in	O
N	O
v	O
and	O
N	O
w	O
are	O
tuned	O
to	O
be	O
0.45	B-HyperparameterValue
,	O
5	B-HyperparameterValue
and	O
5	B-HyperparameterValue
.	O

He	O
has	O
appeared	O
in	O
music	O
videos	O
for	O
the	O
killers	O
in	O
1993	O
,	O
the	O
pretenders	O
in	O
1995	O
,	O
and	O
in	O
the	O
TV	O
shows	O
"	O
the	O
royal	O
"	O
and	O
"	O
the	O
bill	O
"	O
.	O

We	O
set	O
our	O
hyperparameters	O
empirically	O
following	O
(	O
Wu	O
et	O
al	O
.	O
,	O
2020c	O
)	O
with	O
some	O
modifications	O
.	O

The	O
practices	O
are	O
different	O
from	O
the	O
generative	O
pretraining	O
task	O
,	O
leading	O
to	O
inconsistency	O
between	O
pretraining	O
and	O
finetuning	O
.	O

Such	O
traversal	O
,	O
called	O
graph	O
walk	O
,	O
starts	O
from	O
the	O
node	O
linked	O
from	O
the	O
previous	O
module	O
(	O
see	O
section	O
3.2	O
)	O
and	O
considers	O
all	O
entity	O
nodes	O
associated	O
with	O
the	O
start	O
node	O
.	O

(	O
b	O
-	O
e	O
)	O
.	O

For	O
BART	B-MethodName
12	I-MethodName
-	I-MethodName
6	I-MethodName
(	O
or	O
BART	B-MethodName
12	I-MethodName
-	I-MethodName
3	I-MethodName
)	O
,	O
the	O
decoder	O
is	O
initialized	O
from	O
the	O
first	O
6	O
(	O
or	O
3	O
)	O
layers	O
or	O
the	O
maximally	O
spaced	O
6	B-HyperparameterValue
(	O
or	O
3	B-HyperparameterValue
)	O
layers	O
of	O
BART	B-MethodName
decoder	O
.	O

Section	O
5	O
is	O
an	O
analysis	O
and	O
discussion	O
of	O
our	O
results	O
.	O

However	O
,	O
we	O
hope	O
the	O
models	O
will	O
be	O
deployed	O
in	O
domains	O
such	O
as	O
education	O
,	O
disaster	O
management	O
,	O
health	O
,	O
recreation	O
,	O
travel	O
,	O
etc	O
.	O

Besides	O
the	O
domain	O
gap	O
in	O
terms	O
of	O
the	O
observation	O
space	O
,	O
there	O
is	O
also	O
a	O
gap	O
between	O
domains	O
in	O
terms	O
of	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
available	I-HyperparameterName
subtasks	I-HyperparameterName
−	O
while	O
there	O
's	O
always	O
one	O
available	O
subtask	O
per	O
time	O
step	O
in	O
simple	O
games	O
,	O
the	O
model	O
will	O
face	O
more	O
available	O
subtasks	O
in	O
the	O
medium	O
/	O
hard	O
games	O
.	O

Appendix	O
.	O
This	O
supplementary	O
material	O
provides	O
additional	O
information	O
not	O
described	O
in	O
the	O
main	O
text	O
due	O
to	O
the	O
page	O
limit	O
.	O

The	O
modules	O
are	O
optimized	O
via	O
cross	B-HyperparameterValue
entropy	I-HyperparameterValue
loss	I-HyperparameterValue
and	O
Adam	B-HyperparameterValue
optimizer	I-HyperparameterName
with	O
learning	B-HyperparameterName
rate	I-HyperparameterName
0.001	B-HyperparameterValue
.	O

We	O
repeatedly	O
sample	O
new	O
spans	O
until	O
at	O
least	O
15	B-HyperparameterValue
%	I-HyperparameterValue
of	O
the	O
original	O
tokens	O
are	O
masked	O
.	O

(	O
Tsai	O
et	O
al	O
.	O
,	O
2016	O
)	O
48.12	O
60.55	O
61.56	O
WS	O
(	O
Ni	O
et	O
al	O
.	O
,	O
2017	O
)	O
58.50	O
65.10	O
65.40	O
TMP	B-MethodName
(	O
Jain	O
et	O
al	O
.	O
,	O
2019	O
)	O
61.50	O
73.50	O
69.9	O
BERT	B-MethodName
-	I-MethodName
f	I-MethodName
(	O
Wu	O
and	O
Dredze	O
,	O
2019	O
)	O
69.56	O
74.96	O
77.57	O
AdvCE	B-MethodName
(	O
Keung	O
et	O
al	O
.	O
,	O
2019	O
)	O
71.90	O
74.3	O
77.6	O
TSL	B-MetricName
(	O
Wu	O
et	O
al	O
.	O
,	O
2020a	O
)	O
73.16	O
76.75	O
80.44	O
Unitrans	B-MethodName
(	O
Wu	O
et	O
al	O
.	O
,	O
2020b	O
)	O
74	B-MethodName
TSL	I-MethodName
(	O
Wu	O
et	O
al	O
.	O
,	O
2020c	O
)	O
proposes	O
a	O
teacher	O
-	O
student	O
learning	O
model	O
for	O
cross	B-TaskName
-	I-TaskName
lingual	I-TaskName
NER.Unitrans	I-TaskName
(	O
Wu	O
et	O
al	O
.	O
,	O
2020b	O
)	O
unifies	O
a	O
data	O
transfer	O
and	O
model	O
transfer	O
for	O
cross	B-TaskName
-	I-TaskName
lingual	I-TaskName
NER	I-TaskName
.	O

Specifically	O
,	O
we	O
propose	O
a	O
Memory	B-MethodName
-	I-MethodName
Imitation	I-MethodName
Meta	I-MethodName
-	I-MethodName
Learning	I-MethodName
(	O
MemIML	B-MethodName
)	O
method	O
that	O
forces	O
query	O
set	O
predictions	O
to	O
depend	O
on	O
their	O
corresponding	O
support	O
sets	O
by	O
dynamically	O
imitating	O
behaviors	O
of	O
the	O
latter	O
.	O

We	O
randomly	O
sample	O
1.5B	B-HyperparameterValue
Arabic	B-HyperparameterName
tweets	I-HyperparameterName
(	O
178	B-HyperparameterValue
GB	I-HyperparameterValue
)	O
from	O
a	O
large	O
in	O
-	O
house	O
dataset	O
of	O
∼	O
10B	B-HyperparameterValue
tweets	B-HyperparameterName
.	O

To	O
address	O
the	O
above	O
limitation	O
,	O
we	O
propose	O
a	O
novel	O
method	O
,	O
Hypergraph	B-MethodName
Transformer	I-MethodName
,	O
which	O
exploits	O
hypergraph	O
structure	O
to	O
encode	O
multi	O
-	O
hop	O
relationships	O
and	O
transformer	O
-	O
based	O
attention	O
mechanism	O
to	O
learn	O
to	O
pay	O
attention	O
to	O
important	O
knowledge	O
evidences	O
for	O
a	O
question	O
.	O

They	O
learn	O
smooth	O
latent	O
spaces	O
(	O
by	O
imposing	O
implicit	O
priors	O
)	O
to	O
ease	O
the	O
sampling	O
of	O
latent	O
sentences	O
.	O

The	O
analysis	O
below	O
is	O
done	O
on	O
ChicagoFSWild	B-DatasetName
for	O
simplicity	O
.	O

The	O
memory	O
module	O
is	O
task	O
-	O
specific	O
,	O
recording	O
the	O
mapping	O
behaviors	O
between	O
inputs	O
and	O
outputs	O
of	O
support	O
sets	O
for	O
each	O
task	O
.	O

We	O
introduce	O
a	O
method	O
for	O
such	O
constrained	O
unsupervised	O
text	B-TaskName
style	I-TaskName
transfer	I-TaskName
by	O
introducing	O
two	O
complementary	O
losses	O
to	O
the	O
generative	B-MethodName
adversarial	I-MethodName
network	I-MethodName
(	O
GAN	B-MethodName
)	O
family	O
of	O
models	O
.	O

We	O
have	O
two	O
considerations	O
for	O
the	O
module	O
design	O
:	O
1	O
)	O
the	O
module	O
should	O
recognize	O
the	O
semantic	O
connection	O
between	O
the	O
assumption	O
and	O
the	O
context	O
,	O
and	O
2	O
)	O
the	O
module	O
should	O
uniformly	O
support	O
various	O
discrete	O
operations	O
to	O
enable	O
accurate	O
derivation	O
.	O

These	O
two	O
models	O
are	O
two	O
parallel	O
tasks	O
,	O
wherein	O
the	O
entity	O
recognition	O
teacher	O
focuses	O
on	O
identifying	O
the	O
named	O
entities	O
and	O
the	O
similarity	O
evaluator	O
teacher	O
is	O
to	O
decide	O
if	O
two	O
tokens	O
are	O
in	O
the	O
same	O
type	O
.	O

Our	O
main	O
contributions	O
are	O
as	O
follows	O
:	O

In	O
addition	O
to	O
introducing	O
the	O
task	O
,	O
we	O
address	O
the	O
research	O
question	O
of	O
whether	O
the	O
explicit	O
temporal	O
localization	O
of	O
fingerspelling	O
can	O
help	O
its	O
search	O
and	O
retrieval	O
,	O
and	O
how	O
best	O
to	O
localize	O
it	O
.	O

We	O
exploit	O
the	O
UN	O
multi	O
-	O
parallel	O
data	O
(	O
Ziemski	O
et	O
al	O
.	O
,	O
2016	O
)	O
using	O
the	O
Arabic	O
-	O
English	O
and	O
Arabic	O
-	O
French	O
test	O
splits	O
(	O
4	B-HyperparameterValue
,	I-HyperparameterValue
000	I-HyperparameterValue
sentences	B-HyperparameterName
each	O
,	O
described	O
in	O
§	O
3.1	O
)	O
to	O
generate	O
our	O
two	O
code	O
-	O
switched	O
test	O
sets	O
(	O
3	O
)	O
MSA	B-MethodName
-	I-MethodName
EN	I-MethodName
and	O
(	O
4	O
)	O
MSA	B-MethodName
-	I-MethodName
FR	I-MethodName
.	O

Concretely	O
,	O
optimization	O
-	O
based	O
meta	O
-	O
learning	O
algorithms	O
aim	O
to	O
learn	O
a	O
well	O
-	O
generalized	O
global	O
model	O
initialization	O
θ	O
that	O
can	O
quickly	O
adapt	O
to	O
new	O
tasks	O
within	O
a	O
few	O
steps	O
of	O
gradient	O
updates	O
.	O

Figure	O
18	O
:	O
The	O
RL	O
performance	O
of	O
our	O
model	O
and	O
the	O
variant	O
without	O
time	O
-	O
awareness	O
(	O
the	O
full	O
result	O
of	O
Fig	O
.	O

this	O
movie	O
is	O
a	O
very	O
poor	O
attempt	O
to	O
make	O
money	O
using	O
a	O
classical	O
theme	O
.	O

All	O
the	O
other	O
results	O
of	O
well	O
studied	O
for	O
language	O
modeling	O
.	O

Multi	O
-	O
domain	O
Sentiment	O
Classification	O
.	O

1	O

The	O
corresponding	O
value	O
is	O
constructed	O
to	O
store	O
the	O
information	O
of	O
the	O
sample	O
output	O
(	O
ground	O
truth	O
)	O
as	O
in	O
Sec	O
.	O

In	O
all	O
our	O
experiments	O
,	O
we	O
use	O
K	B-HyperparameterName
=	O
10	B-HyperparameterValue
.	O

Extensive	O
experiments	O
on	O
various	O
datasets	O
,	O
KVQA	B-DatasetName
,	O
FVQA	B-DatasetName
,	O
PQ	B-DatasetName
,	O
and	O
PQL	B-DatasetName
validated	O
that	O
Hypergraph	B-MethodName
Transformer	I-MethodName
conducts	O
accurate	O
inference	O
by	O
focusing	O
on	O
knowledge	O
evidences	O
necessary	O
for	O
question	O
from	O
a	O
large	O
knowledge	O
graph	O
.	O

We	O
further	O
compare	O
GLM	B-MethodName
with	O
BERT	B-MethodName
on	O
the	O
two	O
benchmarks	O
.	O

Several	O
Arabic	O
-	O
to	O
-	O
English	O
parallel	O
datasets	O
were	O
released	O
during	O
IWSLT	B-DatasetName
evaluation	O
campaigns	O
(	O
Federico	O
et	O
al	O
.	O
,	O
2012;Cettolo	O
et	O
al	O
.	O
,	O
2013Cettolo	O
et	O
al	O
.	O
,	O
,	O
2014Cettolo	O
et	O
al	O
.	O
,	O
,	O
2016	O
.	O

Although	O
not	O
covered	O
in	O
this	O
paper	O
,	O
an	O
interesting	O
future	O
work	O
is	O
to	O
construct	O
heterogeneous	O
knowledge	O
graph	O
that	O
includes	O
more	O
diverse	O
knowledge	O
sources	O
(	O
e.g.	O
documents	O
on	O
web	O
)	O
.	O

The	O
contents	O
of	O
this	O
appendix	O
are	O
as	O
follows	O
:	O
In	O
Section	O
A	O
,	O
we	O
show	O
the	O
detailed	O
statistics	O
for	O
the	O
diverse	O
splits	O
of	O
four	O
benchmark	O
datasets	O
,	O
i.e.	O
,	O
KVQA	B-DatasetName
,	O
FVQA	B-DatasetName
,	O
PQ	B-DatasetName
and	O
PQL	B-DatasetName
.	O

V	O
q	O
j	O
=	O
g	O
ω	O
q	O
j	O
(	O
K	O
q	O
j	O
)	O
,	O
(	O
4	O
)	O

and	O
help	O
the	O
model	O
make	O
better	O
predictions	O
.	O

To	O
investigate	O
this	O
question	O
,	O
we	O
apply	O
mT5	B-MethodName
on	O
a	O
language	O
with	O
a	O
wide	O
variety	O
of	O
dialects	O
-	O
Arabic	O
.	O

awesome	O
mexican	O
food	O
,	O
a	O
little	O
on	O
the	O
corner	O
of	O
a	O
<	O
unk	O
>	O
.	O

Shared	O
feature	O
space	O
based	O
models	O
generally	O
train	O
a	O
language	O
-	O
independent	O
encoder	O
using	O
source	O
and	O
target	O
language	O
data	O
(	O
Tsai	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

In	O
this	O
paper	O
,	O
we	O
study	O
two	O
issues	O
of	O
semantic	O
parsing	O
approaches	O
to	O
conversational	B-TaskName
question	I-TaskName
answering	I-TaskName
over	O
a	O
large	O
-	O
scale	O
knowledge	O
base	O
:	O
(	O
1	O
)	O
The	O
actions	O
defined	O
in	O
grammar	O
are	O
not	O
sufficient	O
to	O
handle	O
uncertain	O
reasoning	O
common	O
in	O
real	O
-	O
world	O
scenarios	O
.	O

•	O
We	O
highlight	O
the	O
importance	O
of	O
counterfactual	O
thinking	O
in	O
NDR	B-TaskName
and	O
formulate	O
counterfactual	O
thinking	O
as	O
an	O
intervening	O
procedure	O
to	O
achieve	O
precise	O
imagination	O
.	O

We	O
follow	O
the	O
experimental	O
settings	O
suggested	O
in	O
(	O
Wang	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

The	O
MLP	O
uses	O
17	B-HyperparameterValue
%	I-HyperparameterValue
more	O
parameters	O
than	O
SIM	B-MetricName
because	O
KVQA	B-DatasetName
has	O
a	O
large	O
number	O
of	O
answer	O
candidates	O
(	O
19,360	O
)	O
.	O

Moreover	O
,	O
DoKTra	B-MethodName
required	O
less	O
training	O
time	O
than	O
TAPT	B-MethodName
while	O
both	O
methods	O
were	O
task	O
-	O
specific	O
.	O

This	O
is	O
due	O
to	O
the	O
insufficient	O
number	O
of	O
training	O
QA	O
pairs	O
in	O
PQL-3H.	B-DatasetName
When	O
we	O
use	O
PQL-3H	B-DatasetName
-	I-DatasetName
More	I-DatasetName
which	O
has	O
twice	O
more	O
QA	O
pairs	O
(	O
1031	B-HyperparameterValue
→	O
2062	O
)	O
on	O
the	O
same	O
knowledge	O
base	O
as	O
PQL-3H	B-DatasetName
,	O
our	O
model	O
achieves	O
95.4	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
.	O

Moreover	O
,	O
to	O
guarantee	O
the	O
student	O
learning	O
performance	O
,	O
we	O
also	O
propose	O
a	O
weighting	O
strategy	O
to	O
take	O
into	O
consideration	O
the	O
reliability	O
of	O
the	O
teachers	O
.	O

We	O
compare	O
with	O
two	O
baselines	O
:	O
(	O
1	O
)	O
BERT	B-MethodName
,	O
which	O
learns	O
a	O
left	O
-	O
to	O
-	O
right	O
language	O
model	O
to	O
generate	O
the	O
masked	O
tokens	O
on	O
top	O
of	O
the	O
blank	O
representation	O
,	O
and	O
(	O
2	O
)	O
BLM	B-MethodName
proposed	O
by	O
(	O
Shen	O
et	O
al	O
.	O
,	O
2020	O
)	O
,	O
which	O
can	O
fill	O
in	O
the	O
blank	O
with	O
arbitrary	O
trajectories	O
.	O

The	O
observation	O
encoder	O
is	O
implemented	O
based	O
on	O
the	O
Relational	B-MethodName
Graph	I-MethodName
Convolutional	I-MethodName
Networks	I-MethodName
(	O
R	B-MethodName
-	I-MethodName
GCNs	I-MethodName
)	O
(	O
Schlichtkrull	O
et	O
al	O
.	O
,	O
2018	O
)	O
by	O
taking	O
into	O
account	O
both	O
nodes	O
and	O
edges	O
.	O

For	O
each	O
language	O
,	O
we	O
pick	O
1	B-HyperparameterValue
M	I-HyperparameterValue
sentences	B-HyperparameterName
for	O
training	O
and	O
5	B-HyperparameterValue
K	I-HyperparameterValue
sentences	B-HyperparameterName
for	O
each	O
of	O
development	O
and	O
test	O
splits	O
.	O

We	O
report	O
different	O
ROUGE	B-MetricName
scores	O
(	O
Lin	O
,	O
2004	O
)	O
in	O
Table	O
5	O
.	O

5.1	O
Multilingual	O
vs.	O
Dedicated	O
Models	O
.	O

i	O
was	O
one	O
of	O
two	O
customers	O
who	O
was	O
not	O
chinese	O
.	O

In	O
existing	O
work	O
on	O
sign	O
language	O
video	O
processing	O
,	O
search	O
and	O
retrieval	O
tasks	O
have	O
been	O
studied	O
much	O
less	O
than	O
sign	O
language	O
recognition	O
(	O
mapping	O
from	O
sign	O
language	O
video	O
to	O
gloss	O
labels	O
)	O
(	O
Koller	O
et	O
al	O
.	O
,	O
2017;Forster	O
et	O
al	O
.	O
,	O
2016	O
)	O
and	O
translation	O
(	O
mapping	O
from	O
sign	O
language	O
video	O
to	O
text	O
in	O
another	O
language	O
)	O
(	O
Yin	O
and	O
Read	O
,	O
2020;Camgöz	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

We	O
also	O
conduct	O
experiments	O
on	O
fixing	O
the	O
parameter	O
of	O
PLM	O
during	O
training	O
on	O
TAT	B-DatasetName
-	I-DatasetName
HQA	I-DatasetName
as	O
initialized	O
by	O
TAT	B-DatasetName
-	I-DatasetName
QA	I-DatasetName
.	O

Whereas	O
spatial	O
question	O
is	O
quite	O
simple	O
,	O
it	O
is	O
required	O
to	O
understand	O
a	O
correct	O
spatial	O
relationship	O
between	O
multiple	O
entities	O
in	O
a	O
given	O
image	O
.	O

With	O
the	O
help	O
of	O
transfer	O
learning	O
(	O
Ruder	O
et	O
al	O
.	O
,	O
2019	O
)	O
and	O
multilingual	O
BERT	B-MethodName
(	O
short	O
as	O
mBERT	B-MethodName
)	O
(	O
Devlin	O
et	O
al	O
.	O
,	O
*	O
NER	B-TaskName
/	O
NER	B-TaskName
tea	O
:	O
learned	B-TaskName
NER	I-TaskName
model	O
for	O
source	O
language	O
;	O
NER	B-TaskName
stu	O
:	O
learned	B-TaskName
NER	I-TaskName
model	O
for	O
target	O
language	O
;	O
SIM	B-MetricName
tea	O
learned	O
similarity	O
model	O
for	O
source	O
language	O
;	O
{	O
X	O
,	O
Y	O
}	O
src	O
:	O
labeled	O
data	O
in	O
source	O
language	O
;	O
{	O
X	O
}	O
tgt	O
:	O
unlabeled	O
data	O
in	O
target	O
language	O
;	O
{	O
X	O
,	O
P	O
}	O
tgt	O
:	O
labeled	O
data	O
in	O
target	O
language	O
with	O
probability	O
;	O
{	O
X	O
,	O
S	O
}	O
tgt	O
:	O
labeled	O
data	O
in	O
target	O
language	O
with	O
entity	O
similarity	O
score	O
.	O

In	O
contrast	O
,	O
our	O
model	O
performs	O
very	O
well	O
and	O
achieves	O
over	O
80	B-HyperparameterValue
%	I-HyperparameterValue
of	O
the	O
scores	O
.	O

We	O
manually	O
inspect	O
∼	O
100	B-HyperparameterValue
random	B-HyperparameterName
samples	B-HyperparameterName
of	O
the	O
data	O
predicted	O
as	O
non	O
-	O
Arabic	O
.	O

•	O
We	O
devise	O
the	O
L2I	B-MethodName
module	O
,	O
which	O
is	O
designed	O
as	O
neural	O
network	O
operations	O
and	O
can	O
be	O
seamlessly	O
incorporated	O
into	O
the	O
NDR	B-TaskName
model	O
for	O
answering	O
hypothetical	O
questions	O
.	O

Some	O
previous	O
work	O
also	O
considered	O
task	O
decomposition	O
Hu	O
et	O
al	O
.	O
,	O
2019	O
)	O
,	O
but	O
the	O
related	O
module	O
is	O
obtained	O
through	O
imitating	O
human	O
demonstrations	O
,	O
which	O
is	O
directly	O
related	O
to	O
decision	O
making	O
instead	O
of	O
world	O
perceiving	O
.	O

For	O
all	O
three	O
of	O
our	O
pre	O
-	O
trained	O
models	O
,	O
we	O
use	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.01	B-HyperparameterValue
,	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
128	B-HyperparameterValue
sequences	B-HyperparameterName
,	O
and	O
a	O
maximum	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
of	O
512	B-HyperparameterValue
,	O
except	O
for	O
AraT5	B-MethodName
TW	I-MethodName
where	O
the	O
maximum	B-HyperparameterName
sequence	I-HyperparameterName
is	O
128	B-HyperparameterValue
.	O

Empirically	O
,	O
we	O
show	O
that	O
with	O
the	O
same	O
amount	O
of	O
parameters	O
and	O
computational	O
cost	O
,	O
GLM	B-MethodName
significantly	O
outperforms	O
BERT	B-MethodName
on	O
the	O
SuperGLUE	B-DatasetName
benchmark	O
by	O
a	O
large	O
margin	O
of	O
4.6	B-MetricValue
%	I-MetricValue
-5.0	B-MetricValue
%	I-MetricValue
and	O
outperforms	O
RoBERTa	B-MethodName
and	O
BART	B-MethodName
when	O
pretrained	O
on	O
a	O
corpus	O
of	O
similar	O
size	O
(	O
158	B-HyperparameterValue
GB	I-HyperparameterValue
)	O
.	O

Wyoming	O
's	O
constitution	O
provides	O
that	O
the	O
governor	O
can	O
appoint	O
a	O
member	O
of	O
the	O
wyoming	O
state	O
senate	O
to	O
the	O
wyoming	O
supreme	O
court	O
,	O
and	O
the	O
chairman	O
of	O
the	O
wyoming	O
senate	O
.	O

no	O
one	O
ever	O
came	O
to	O
eat	O
here	O
.	O

Random	O
initialization	O
is	O
applied	O
when	O
a	O
word	O
for	O
a	O
node	O
does	O
not	O
exist	O
in	O
the	O
vocabulary	O
of	O
GloVe	B-MethodName
.	O

All	O
prior	O
work	O
on	O
keyword	O
search	O
for	O
sign	O
language	O
has	O
been	O
done	O
in	O
a	O
closed	O
-	O
vocabulary	O
setting	O
,	O
which	O
assumes	O
that	O
only	O
words	O
from	O
a	O
pre	O
-	O
determined	O
set	O
will	O
be	O
queried	O
.	O

Simpsons	O
"	O
as	O
the	O
character	O
captain	O
Billy	O
Higgledypig	O
,	O
but	O
his	O
character	O
was	O
only	O
a	O
one	O
-	O
time	O
recurring	O
character	O
in	O
the	O
series	O
'	O
first	O
six	O
seasons	O
.	O

Recently	O
,	O
PET	B-MethodName
(	O
Schick	O
and	O
Schütze	O
,	O
2020a	O
,	O
b	O
)	O
proposes	O
to	O
reformulate	O
input	O
examples	O
as	O
cloze	O
questions	O
with	O
patterns	O
similar	O
to	O
the	O
pretraining	O
corpus	O
in	O
the	O
few	O
-	O
shot	O
setting	O
.	O

We	O
observe	O
that	O
attention	O
weights	O
form	O
three	O
"	O
lines	O
"	O
,	O
which	O
indicates	O
very	O
time	O
the	O
decoder	O
predicts	O
the	O
next	O
word	O
,	O
its	O
attention	O
points	O
to	O
the	O
next	O
word	O
in	O
the	O
input	O
document	O
.	O

The	O
memory	O
module	O
is	O
task	O
-	O
specific	O
,	O
storing	O
representative	O
information	O
of	O
support	O
sets	O
.	O

Perplexity	O
is	O
an	O
evaluation	O
criterion	O
that	O
has	O
been	O
changes	O
to	O
the	O
senate	O
.	O

(	O
3	O
)	O
MTMT	B-MethodName
w/o	I-MethodName
similarity	I-MethodName
,	O
which	O
removes	O
the	O
similarity	O
teacher	O
model	O
.	O

With	O
the	O
same	O
amount	O
of	O
parameters	O
,	O
encoding	O
the	O
context	O
with	O
bidirectional	O
attention	O
can	O
improve	O
the	O
performance	O
of	O
language	O
modeling	O
.	O

13	O
and	O
Fig	O
.	O

We	O
check	O
the	O
validity	O
of	O
memory	O
imitation	O
by	O
examining	O
whether	O
the	O
criterion	O
in	O
Section	O
4.4	O
is	O
met	O
.	O

To	O
limit	O
GPU	B-TaskName
needs	O
during	O
our	O
experiments	O
,	O
especially	O
given	O
the	O
time	O
-	O
consuming	O
fine	O
-	O
tuning	O
process	O
typical	O
of	O
T5	O
models	O
,	O
we	O
do	O
not	O
fine	O
-	O
tune	O
the	O
models	O
on	O
the	O
full	O
amounts	O
of	O
available	O
parallel	O
data	O
.	O

Because	O
the	O
entropy	B-MetricName
regularizer	O
in	O
calibrated	O
teacher	O
training	O
issues	O
penalties	O
based	O
on	O
the	O
output	O
probability	O
distribution	O
,	O
it	O
is	O
difficult	O
to	O
intuitively	O
understand	O
how	O
it	O
positively	O
affects	O
activation	O
boundary	O
distillation	O
,	O
which	O
uses	O
hidden	O
representation	O
.	O

We	O
present	O
the	O
attention	O
map	O
from	O
the	O
guided	O
-	O
attention	O
block	O
,	O
and	O
visualize	O
top	O
-	O
k	O
attended	O
knowledge	O
facts	O
or	O
entities	O
with	O
the	O
attention	O
scores	O
.	O

For	O
example	O
,	O
for	O
Rougier	O
et	O
al	O
.	O
(	O
2017	O
)	O
,	O
reproducing	O
a	O
result	O
means	O
running	O
the	O
same	O
code	O
on	O
the	O
same	O
data	O
and	O
obtaining	O
the	O
same	O
result	O
,	O
while	O
replicating	O
the	O
result	O
is	O
writing	O
and	O
running	O
new	O
code	O
based	O
on	O
the	O
information	O
provided	O
by	O
the	O
original	O
publication	O
.	O

We	O
can	O
thus	O
derive	O
the	O
value	O
of	O
successors	O
(	O
e.g.	O
,	O
c	O
i	O
→	O
c	O
j	O
)	O
by	O
forming	O
a	O
simple	O
hypothetical	O
question	O
:	O
"	O
What	O
c	O
j	O
would	O
be	O
if	O
c	O
i	O
is	O
c	O
i	O
?	O
"	O
and	O
answering	O
it	O
with	O
the	O
NDR	B-TaskName
model	O
.	O

They	O
are	O
retained	O
verbatim	O
after	O
the	O
delete	O
operation	O
.	O

Data	O
quality	O
is	O
another	O
challenge	O
for	O
multilingual	O
models	O
.	O

One	O
issue	O
we	O
are	O
concerned	O
about	O
is	O
the	O
compound	O
error	O
−	O
the	O
prediction	O
error	O
from	O
imperfect	O
pre	O
-	O
trained	O
modules	O
will	O
adversely	O
affect	O
RL	O
training	O
(	O
Talvitie	O
,	O
2014;Racanière	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

We	O
use	O
a	O
simple	O
method	O
to	O
build	O
the	O
subtask	O
set	O
T	O
from	O
o	O
t	O
:	O
As	O
shown	O
in	O
Fig	O
.	O

A	O
subtask	O
is	O
bounded	O
by	O
a	O
time	O
limit	O
[	O
0	O
,	O
ξ	O
]	O
.	O

Compared	O
with	O
the	O
left	O
graph	O
,	O
the	O
right	O
graph	O
with	O
higher	O
attention	B-HyperparameterName
temperature	I-HyperparameterName
has	O
shorter	O
lines	O
(	O
less	O
copy	O
bias	O
)	O
with	O
high	O
attention	O
weights	O
,	O
and	O
positions	O
of	O
high	O
attention	O
weights	O
extend	O
to	O
the	O
first	O
450	O
words	O
(	O
less	O
leading	O
bias	O
)	O
.	O

Following	O
a	O
review	O
of	O
related	O
research	O
(	O
Section	O
2	O
)	O
,	O
we	O
present	O
the	O
method	O
(	O
Section	O
3	O
)	O
,	O
tests	O
and	O
results	O
(	O
Section	O
4	O
)	O
,	O
discuss	O
method	O
and	O
results	O
(	O
Section	O
5	O
)	O
,	O
and	O
finish	O
with	O
some	O
conclusions	O
(	O
Section	O
6).The	O
situation	O
memorably	O
caricatured	O
by	O
Pedersen	O
(	O
2008	O
)	O
still	O
happens	O
all	O
the	O
time	O
:	O
you	O
download	O
some	O
code	O
you	O
read	O
about	O
in	O
a	O
paper	O
and	O
liked	O
the	O
sound	O
of	O
,	O
you	O
run	O
it	O
on	O
the	O
data	O
provided	O
,	O
only	O
to	O
find	O
that	O
the	O
results	O
are	O
not	O
the	O
same	O
as	O
reported	O
in	O
the	O
paper	O
,	O
in	O
fact	O
they	O
are	O
likely	O
to	O
be	O
worse	O
(	O
Belz	O
et	O
al	O
.	O
,	O
2021a	O
)	O
.	O

the	O
only	O
thing	O
i	O
did	O
n't	O
like	O
was	O
the	O
<	O
unk	O
>	O
.	O

He	O
was	O
also	O
traded	O
to	O
the	O
St.	O
Louis	O
Cardinals	O
for	O
a	O
second	O
round	O
pick	O
in	O
the	O
1970	O
draft	O
.	O

To	O
summarize	O
,	O
our	O
model	O
is	O
robust	O
to	O
limited	O
pretraining	O
data	O
and	O
largely	O
alleviates	O
the	O
burden	O
of	O
human	O
annotations	O
.	O

Attention	O
-	O
based	O
keyword	O
search	O
(	O
Attn	B-MethodName
-	I-MethodName
KWS	I-MethodName
)	O
This	O
model	O
is	O
adapted	O
from	O
(	O
Tamer	O
and	O
Saraçlar	O
,	O
2020b	O
)	O
's	O
approach	O
for	O
keyword	O
search	O
in	O
sign	O
language	O
.	O

Also	O
,	O
no	O
samples	O
of	O
the	O
non	O
-	O
Arabic	O
included	O
real	O
code	O
-	O
switching	O
.	O

In	O
the	O
1980s	O
,	O
two	O
stations	O
were	O
constructed	O
on	O
the	O
line	O
,	O
Corona	O
Road	O
and	O
Corona	O
Park	O
.	O

Although	O
increasing	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
negatives	O
is	O
beneficial	O
for	O
contrastive	O
learning	O
,	O
when	O
more	O
than	O
one	O
positive	O
example	O
is	O
available	O
,	O
using	O
them	O
brings	O
further	O
improvements	O
(	O
Khosla	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

Besides	O
making	O
the	O
agent	O
robust	O
against	O
errors	O
,	O
another	O
benefit	O
by	O
introducing	O
time	O
-	O
awareness	O
to	O
subtasks	O
is	O
that	O
it	O
improves	O
the	O
subtask	O
selection	O
diversity	O
,	O
which	O
helps	O
the	O
agent	O
to	O
avoid	O
getting	O
stuck	O
in	O
local	O
minima	O
(	O
Pong	O
et	O
al	O
.	O
,	O
2020;Campero	O
et	O
al	O
.	O
,	O
2020).We	O
conduct	O
experiments	O
on	O
cooking	O
games	O
provided	O
by	O
the	O
rl.0.2	O
game	O
set	O
†	O
and	O
the	O
FTWP	B-DatasetName
game	O
set	O
‡	O
,	O
which	O
share	O
the	O
vocabulary	O
set	O
.	O

α	B-HyperparameterName
(	O
•	O
)	O
=	O
(	O
max(ŷ	O
T	O
i	O
)	O
)	O
2	O
β	B-HyperparameterName
=	O
(	O
2	O
t	O
T	O
(	O
x	O
T	O
,	O
x	O
T	O
,	O
i	O
,	O
j	O
)	O
−	O
1	O
)	O
2	O
γ	B-HyperparameterName
=	O
1	O
−	O
|σ(cos(ŷ	O
T	O
i	O
,	O
ŷ	O
T	O
j	O
)	O
)	O
−t	O
T	O
(	O
x	O
T	O
,	O
x	O
T	O
,	O
i	O
,	O
j)|In	O
this	O
section	O
,	O
we	O
evaluate	O
our	O
multiple	O
-	O
task	O
and	O
multiple	O
-	O
teacher	O
model	O
for	O
cross	B-TaskName
-	I-TaskName
lingual	I-TaskName
NER	I-TaskName
and	O
compare	O
our	O
model	O
with	O
a	O
series	O
of	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
.	O

T5	B-MethodName
proposes	O
a	O
similar	O
blank	O
infilling	O
objective	O
to	O
pretrain	O
an	O
encoder	O
-	O
decoder	O
Transformer	O
.	O

Then	O
,	O
we	O
optimize	O
θ	O
based	O
on	O
the	O
performance	O
of	O
θ	O
i	O
on	O
a	O
query	O
set	O
(	O
i.e.	O
,	O
another	O
set	O
of	O
samples	O
in	O
task	O
i	O
)	O
.	O

GLM	B-MethodName
:	O
He	O
was	O
a	O
voice	O
actor	O
for	O
the	O
"	O
X	O
-	O
Men	O
"	O
cartoon	O
series	O
.	O

The	O
new	O
Corona	O
station	O
opened	O
in	O
1988	O
,	O
and	O
the	O
original	O
Corona	O
station	O
was	O
demolished	O
.	O

Table	O
3	O
shows	O
the	O
classification	O
performance	O
of	O
BioBERT	B-MethodName
,	O
RoBERTa	B-MethodName
-	I-MethodName
PM	I-MethodName
,	O
and	O
our	O
approach	O
in	O
five	O
biomedical	O
and	O
clinical	O
tasks	O
.	O

GLM	B-MethodName
515	I-MethodName
M	I-MethodName
(	O
1.5×	O
of	O
GPT	B-MethodName
Large	I-MethodName
)	O
can	O
further	O
outperform	O
GPT	B-MethodName
Large	I-MethodName
.	O

)	O

We	O
design	O
a	O
memory	O
module	O
M	O
i	O
for	O
each	O
task	O
T	O
i	O
and	O
incorporate	O
it	O
in	O
the	O
MAML	B-MethodName
framework	O
.	O

All	O
models	O
are	O
under	O
the	O
same	O
setting	O
of	O
ORG+3	B-MetricName
-	O
hop	O
reported	O
in	O
Table	O
1.We	O
analyze	O
QA	B-TaskName
performances	O
over	O
different	O
question	O
categories	O
in	O
Table	O
5	O
.	O

Table	O
1	O
shows	O
the	O
results	O
.	O

Comparison	O
with	O
T5	B-MethodName
(	O
Raffel	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

As	O
shown	O
in	O
Table	O
4	O
,	O
using	O
large	O
attention	B-HyperparameterName
temperature	I-HyperparameterName
coefficients	I-HyperparameterName
(	O
2.0	B-HyperparameterValue
)	O
for	O
all	O
three	O
types	O
of	O
attention	O
modules	O
leads	O
to	O
the	O
best	O
result	O
.	O

The	O
results	O
are	O
tabulated	O
at	O
Table	O
:1	O
.	O

Then	O
we	O
evaluate	O
the	O
GLM	B-MethodName
's	O
performance	O
in	O
a	O
multi	O
-	O
task	O
setting	O
(	O
Section	O
2.1	O
)	O
.	O

The	O
student	O
models	O
with	O
our	O
method	O
(	O
λ	B-HyperparameterName
=	O
1.5	B-HyperparameterValue
and	O
λ	B-HyperparameterName
=	O
2.0	B-HyperparameterValue
)	O
slightly	O
outperform	O
the	O
student	O
with	O
regular	O
pseudo	O
-	O
labeling	O
method	O
(	O
λ	B-HyperparameterName
=	O
1.0	B-HyperparameterValue
)	O
.	O

Mistakes	O
made	O
by	O
the	O
model	O
can	O
be	O
attributed	O
to	O
poor	O
understanding	O
of	O
the	O
original	O
semantics	O
,	O
lack	O
of	O
diversity	O
,	O
and	O
not	O
producing	O
attribute	O
-	O
specific	O
words	O
.	O

the	O
only	O
thing	O
that	O
was	O
<	O
unk	O
>	O
was	O
the	O
chicken	O
burrito	O
.	O

Given	O
the	O
transition	O
data	O
,	O
the	O
task	O
is	O
to	O
predict	O
the	O
action	O
a	O
t	O
∈	O
A	O
given	O
the	O
current	O
observation	O
o	O
t	O
,	O
and	O
the	O
next	O
observation	O
o	O
t+1	O
after	O
executing	O
a	O
t	O
.	O

Each	O
corpus	O
-which	O
we	O
interpret	O
as	O
domains	O
-contain	O
discernable	O
attributes	O
,	O
ranging	O
from	O
sentiment	O
(	O
e.g.	O
,	O
positive	O
vs.	O
negative	O
)	O
,	O
topics	O
,	O
political	O
slant	O
(	O
e.g.	O
,	O
democratic	O
vs.	O
republican	O
)	O
,	O
or	O
some	O
combination	O
(	O
Li	O
et	O
al	O
.	O
,	O
2018;Lample	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

The	O
matching	O
score	O
between	O
video	O
I	O
1	O
:	O
T	O
and	O
w	O
is	O
defined	O
as	O
:	O

-A	O
good	O
English	O
Sentence	O
Similarity	O
:	O
Indicate	O
how	O
semantically	O
similar	O
the	O
target	O
sentence	O
is	O
.	O

We	O
introduce	O
each	O
dataset	O
briefly	O
here.(1	O
)	O
United	O
Nations	O
Parallel	B-DatasetName
Corpus	I-DatasetName
.	O

Here	O
,	O
we	O
set	O
up	O
the	O
model	O
as	O
three	B-HyperparameterValue
layers	O
with	O
adjacent	O
and	O
layer	O
-	O
wise	O
weight	O
tying	O
.	O

Hyperparameters	O
are	O
chosen	O
to	O
maximize	O
the	O
mAP	B-MetricName
on	O
the	O
dev	O
set	O
,	O
independently	O
for	O
the	O
two	O
tasks	O
(	O
though	O
ultimately	O
,	O
the	O
best	O
hyperparameter	O
values	O
in	O
our	O
search	O
are	O
identical	O
for	O
both	O
tasks	O
)	O
.	O

In	O
this	O
paper	O
,	O
we	O
address	O
the	O
memorization	O
overfitting	O
issue	O
by	O
enhancing	O
the	O
model	O
's	O
dependence	O
on	O
support	O
sets	O
when	O
learning	O
the	O
model	O
initialization	O
,	O
which	O
forces	O
the	O
model	O
to	O
better	O
leverage	O
information	O
from	O
support	O
sets	O
.	O

BART	B-MethodName
use	O
as	O
a	O
standard	O
encoder	O
decoder	O
transformer	O
architecture	O
with	O
a	O
bidirectional	O
encoder	O
and	O
an	O
autoregressive	O
decoder	O
.	O

We	O
set	O
10	B-HyperparameterValue
%	I-HyperparameterValue
of	O
the	O
entire	O
data	O
as	O
the	O
test	O
set	O
,	O
which	O
is	O
similar	O
to	O
FPB	B-DatasetName
.	O

i	O
only	O
ordered	O
two	O
of	O
them	O
.	O

We	O
achieve	O
accuracy	B-MetricName
of	O
97.9	B-MetricValue
for	O
YELP	B-DatasetName
,	O
96.9	B-MetricValue
for	O
IMDB	B-DatasetName
and	O
97.1	B-MetricValue
for	O
POLITICAL	B-DatasetName
.	O

Given	O
T	O
t	O
and	O
the	O
action	O
candidate	O
set	O
A	O
,	O
we	O
use	O
the	O
action	O
validator	O
to	O
get	O
an	O
action	O
subset	O
A	O
t	O
⊆	O
A	O
,	O
which	O
contains	O
only	O
those	O
relevant	O
to	O
the	O
subtask	O
T	O
t	O
.	O

In	O
the	O
first	O
example	O
,	O
both	O
model	O
,	O
Hypergraph	B-MethodName
Transformer	I-MethodName
and	B-MethodName
Transformer	I-MethodName
(	I-MethodName
SA+GA	I-MethodName
)	I-MethodName
,	O
infer	O
the	O
correct	O
answer	O
,	O
Q5075293	O
.	O

We	O
follow	O
the	O
standard	O
pre	O
-	O
processing	O
steps	O
described	O
in	O
See	O
et	O
al	O
.	O
(	O
2017	O
)	O
;	O
Liu	O
and	O
Lapata	O
(	O
2019	O
)	O
.	O

We	O
now	O
describe	O
our	O
pretraining	O
setup	O
and	O
the	O
evaluation	O
of	O
downstream	O
tasks	O
.	O

We	O
study	O
the	O
contribution	O
of	O
the	O
subtask	O
timeawareness	O
by	O
comparing	O
our	O
full	O
model	O
with	O
the	O
variant	O
without	O
this	O
technique	O
.	O

In	O
contrast	O
,	O
we	O
consider	O
the	O
more	O
practical	O
situation	O
where	O
the	O
interaction	O
data	O
is	O
limited	O
,	O
and	O
focus	O
on	O
improving	O
the	O
RL	O
agent	O
's	O
data	O
efficiency	O
.	O

It	O
has	O
been	O
successfully	O
applied	O
to	O
transfer	O
learning	O
such	O
as	O
one	O
-	O
shot	O
image	B-TaskName
recognition	O
(	O
Koch	O
et	O
al	O
.	O
,	O
2015	O
)	O
,	O
text	B-TaskName
similarity	I-TaskName
(	O
Neculoiu	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

In	O
particular	O
,	O
we	O
evaluate	O
TAGOP	B-MethodName
(	O
Zhu	O
et	O
al	O
.	O
,	O
2021	O
)	O
,	O
which	O
is	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
model	O
on	O
TAT	B-DatasetName
-	I-DatasetName
QA	I-DatasetName
(	O
see	O
detailed	O
settings	O
in	O
Section	O
4.1	O
)	O
by	O
training	O
on	O
TAT	B-DatasetName
-	I-DatasetName
QA	I-DatasetName
and	O
testing	O
on	O
TAT	B-DatasetName
-	I-DatasetName
HQA	I-DatasetName
.	O

The	O
template	O
-	O
based	O
agents	O
achieve	O
a	O
trade	O
-	O
off	O
between	O
the	O
huge	O
action	O
space	O
and	O
the	O
assumption	O
of	O
admissible	O
action	O
set	O
by	O
introducing	O
the	O
template	O
-	O
based	O
action	O
space	O
,	O
where	O
the	O
agent	O
selects	O
first	O
a	O
template	O
,	O
and	O
then	O
a	O
verb	O
-	O
object	O
pair	O
either	O
individually	O
or	O
conditioned	O
on	O
the	O
selected	O
template	O
.	O

Nonetheless	O
,	O
our	O
model	O
shows	O
robust	O
reasoning	O
performance	O
when	O
a	O
large	O
and	O
noisy	O
knowledge	O
facts	O
are	O
given	O
.	O

We	O
call	O
the	O
two	O
biases	O
above	O
the	O
copy	O
bias	O
and	O
the	O
leading	O
bias	O
.	O

We	O
use	O
Transformer	B-MethodName
-	I-MethodName
Big	I-MethodName
model	O
as	O
the	O
teacher	O
and	O
Transformer	B-MethodName
-	I-MethodName
Base	I-MethodName
as	O
the	O
student	O
.	O

Second	O
,	O
by	O
acquiring	O
the	O
skills	O
to	O
solve	O
subtasks	O
,	O
the	O
agent	O
will	O
be	O
able	O
to	O
learn	O
to	O
solve	O
a	O
new	O
task	O
more	O
quickly	O
by	O
reusing	O
the	O
learnt	O
skills	O
(	O
Barreto	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

In	O
addition	O
to	O
the	O
triplet	O
-	O
based	O
graph	O
walks	O
,	O
a	O
multihop	O
graph	O
walk	O
is	O
proposed	O
to	O
encode	O
multiple	O
relational	O
facts	O
that	O
are	O
interconnected	O
.	O

The	O
detailed	O
statistics	O
of	O
the	O
datasets	O
are	O
shown	O
in	O
Appendix	O
A.Each	O
node	O
in	O
the	O
knowledge	O
hypergraph	O
and	O
the	O
question	O
hypergraph	O
is	O
represented	O
as	O
a	O
300dimensional	B-HyperparameterValue
vector	O
(	O
i.e.	O
,	O
w	B-HyperparameterName
=	O
300	B-HyperparameterValue
)	O
initialized	O
using	O
GloVe	B-MethodName
(	O
Pennington	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

These	O
meaningful	O
potential	O
use	O
cases	O
are	O
behind	O
our	O
decision	O
to	O
release	O
the	O
models	O
.	O

In	O
this	O
study	O
,	O
we	O
selected	O
the	O
FinBERT	B-MethodName
(	O
Yang	O
et	O
al	O
.	O
,	O
2020	O
)	O
model	O
as	O
a	O
teacher	O
in	O
the	O
DoKTra	B-MethodName
framework	O
and	O
evaluated	O
our	O
approach	O
on	O
two	O
tasks	O
,	O
the	O
Financial	B-DatasetName
PhraseBank	I-DatasetName
(	O
FPB	B-DatasetName
)	O
and	O
Fin	B-DatasetName
-	I-DatasetName
TextSen	I-DatasetName
(	O
FTS	B-DatasetName
)	O
.	O

Our	O
main	O
results	O
are	O
shown	O
in	O
Table	O
2	O
.	O

While	O
ARAE	B-MethodName
is	O
an	O
auto	O
-	O
encoder	O
that	O
recreates	O
input	O
x	O
→x	O
,	O
our	O
requirement	O
is	O
to	O
translate	O
sentences	O
from	O
one	O
domain	O
to	O
another	O
.	O

The	O
possible	O
maximum	O
pre	O
-	O
training	O
batch	B-HyperparameterName
size	I-HyperparameterName
with	O
the	O
given	O
computing	O
resource	O
for	O
the	O
RoBERTa	B-MethodName
-	I-MethodName
large	I-MethodName
model	O
was	O
36	B-HyperparameterValue
.	O

However	O
,	O
for	O
arithmetic	O
questions	O
,	O
the	O
question	B-TaskName
-	B-TaskName
answering	I-TaskName
label	O
for	O
one	O
pair	O
of	O
c	O
and	O
q	O
remains	O
the	O
same	O
between	O
TAT	B-DatasetName
-	I-DatasetName
HQA	I-DatasetName
and	O
TAT	B-DatasetName
-	I-DatasetName
QA	I-DatasetName
,	O
and	O
the	O
intervention	O
is	O
achieved	O
explicitly	O
by	O
deriving	O
operators	O
and	O
tagging	O
head	O
.	O

Learning	B-HyperparameterName
rates	I-HyperparameterName
are	O
picked	O
from	O
1e-4	B-HyperparameterValue
,	O
3e-4	B-HyperparameterValue
,	O
5e-4	B-HyperparameterValue
,	O
7e-4	B-HyperparameterValue
accord	O
-	O
ing	O
to	O
validation	O
sets	O
.	O

TC20210528011	O
)	O
.	O

Typically	O
,	O
these	O
works	O
either	O
assume	O
the	O
access	O
to	O
a	O
set	O
of	O
available	O
subtasks	O
,	O
or	O
decompose	O
a	O
task	O
through	O
pre	O
-	O
defined	O
rules	O
,	O
while	O
we	O
aim	O
to	O
achieve	O
automatic	O
task	O
decomposition	O
through	O
pre	O
-	O
training	O
,	O
and	O
remove	O
the	O
requirement	O
for	O
expert	O
knowledge	O
during	O
reinforcement	O
learning	O
.	O

Each	O
GGNN	B-MethodName
model	O
consists	O
of	O
three	O
gated	O
recurrent	O
propagation	O
layers	O
and	O
a	O
graphlevel	O
aggregator	O
.	O

where	O
Y	O
is	O
the	O
label	O
set	O
.	O

Our	O
own	O
pre	O
-	O
training	O
data	O
in	O
the	O
case	O
of	O
Twitter	O
,	O
in	O
comparison	O
,	O
involve	O
much	O
more	O
dialectal	O
content	O
(	O
28.39	B-HyperparameterValue
%	O
as	O
listed	O
in	O
§	O
2.1	O
)	O
.	O

As	O
shown	O
in	O
Figure	O
1	O
,	O
a	O
hypothetical	O
question	O
includes	O
an	O
assumption	O
,	O
e.g.	O
,	O
"	O
if	O
the	O
amount	O
in	O
2019	O
was	O
$	O
132,935	O
thousand	O
instead	O
"	O
.	O

We	O
consider	O
a	O
huge	O
number	O
of	O
knowledge	O
facts	O
in	O
the	O
KB	O
as	O
a	O
huge	O
knowledge	O
graph	O
,	O
and	O
construct	O
a	O
hypergraph	O
by	O
traversing	O
the	O
knowledge	O
graph	O
.	O

B	O
illustrates	O
the	O
process	O
for	O
constructing	O
the	O
pre	O
-	O
training	O
datasets	O
.	O

Both	O
new	O
objectives	O
are	O
defined	O
in	O
the	O
same	O
way	O
as	O
the	O
original	O
objective	O
,	O
i.e.	O
Eq	O
.	O

The	O
model	O
structures	O
of	O
these	O
applications	O
are	O
basically	O
the	O
same	O
,	O
except	O
for	O
the	O
following	O
three	O
points	O
:	O
the	O
base	O
model	O
,	O
the	O
way	O
to	O
get	O
the	O
value	O
V	O
s	O
l	O
stored	O
in	O
the	O
memory	O
module	O
,	O
and	O
the	O
way	O
to	O
leverage	O
the	O
outputV	O
q	O
j	O
of	O
Sec	O
.	O

fast	O
service	O
.	O

Sample	B-TaskName
QAs	I-TaskName
have	O
been	O
shown	O
in	O
Fig	O
.	O

To	O
sum	O
up	O
,	O
Hypergraph	B-MethodName
Transformer	I-MethodName
takes	O
graph	O
-	O
level	O
inputs	O
,	O
i.e.	O
,	O
hyperedge	O
,	O
and	O
conducts	O
semantic	O
matching	O
between	O
hyperedges	O
by	O
the	O
attention	O
mechanism	O
.	O

We	O
set	O
the	O
subtask	O
time	O
limit	O
ξ	O
=	O
5	B-HyperparameterValue
.	O

Pre	O
-	O
training	O
of	O
each	O
model	O
took	O
∼	O
80	O
days	O
on	O
one	O
Google	O
Cloud	O
TPU	O
with	O
8	B-HyperparameterValue
cores	O
(	O
v3.8	O
)	O
from	O
TensorFlow	B-DatasetName
Research	I-DatasetName
Cloud	I-DatasetName
(	O
TFRC	B-DatasetName
)	O
.	O

It	O
leverages	O
the	O
pretrained	O
T5	B-MethodName
and	O
BART	B-MethodName
(	O
Lewis	O
et	O
al	O
.	O
,	O
2019	O
)	O
as	O
backbone	O
for	O
model	O
architecture	O
.	O

We	O
follow	O
to	O
conduct	O
a	O
two	O
-	O
phase	O
training	O
process	O
:	O
imitation	O
pre	O
-	O
training	O
and	O
reinforcement	O
fine	O
-	O
tuning	O
.	O

POLITICAL	B-DatasetName
i	O
wish	O
u	O
would	O
bring	O
change	O
and	O
i	O
wish	O
you	O
would	O
help	O
bring	O
democracy	O
and	O
i	O
'	O
m	O
not	O
sure	O
mr.trump	O
.	O

We	O
include	O
our	O
code	O
in	O
the	O
supplementary	O
material	O
.	O

True	O
objective	O
of	O
ToD	B-TaskName
is	O
human	O
experience	O
while	O
interacting	O
with	O
the	O
dialogue	O
systems	O
,	O
which	O
automatic	O
evaluation	O
metrics	O
might	O
fall	O
short	O
to	O
capture	O
.	O

In	O
this	O
task	O
,	O
each	O
sample	O
consists	O
of	O
an	O
input	O
utterance	O
and	O
a	O
ground	O
truth	O
utterance	O
,	O
so	O
the	O
value	O
V	O
s	O
l	O
stored	O
in	O
the	O
memory	O
is	O
obtained	O
from	O
the	O
ground	O
truth	O
utterance	O
Y	O
s	O
l	O
of	O
a	O
support	O
-	O
set	O
sample	O
,	O
which	O
is	O
embedded	O
by	O
the	O
key	O
network	O
followed	O
by	O
an	O
LSTM	B-MetricName
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
.	O

2	O
10	O
:	O
for	O
(	O
X	O
q	O
j	O
,	O
Y	O
q	O
j	O
)	O
in	O
D	O
q	O
i	O
do	O
11	O
:	O

GLM	B-MethodName
formulates	O
NLU	B-TaskName
tasks	O
as	O
cloze	O
questions	O
that	O
contain	O
task	O
descriptions	O
,	O
which	O
can	O
be	O
answered	O
by	O
autoregressive	O
generation	O
.	O

The	O
hidden	B-HyperparameterName
size	I-HyperparameterName
of	O
each	O
layer	O
is	O
1024	B-HyperparameterValue
,	O
and	O
each	O
layer	O
contains	O
16	B-HyperparameterValue
attention	B-HyperparameterName
heads	I-HyperparameterName
with	O
a	O
hidden	B-HyperparameterName
size	I-HyperparameterName
of	O
64	B-HyperparameterValue
.	O

To	O
train	O
GLM	B-MethodName
RoBERTa	I-MethodName
,	O
we	O
follow	O
the	O
pretraining	O
datasets	O
of	O
RoBERTa	B-MethodName
,	O
which	O
consist	O
of	O
BookCorups	B-DatasetName
(	O
Zhu	O
et	O
al	O
.	O
,	O
2015),Wikipedia	O
(	O
16	B-HyperparameterValue
GB	I-HyperparameterValue
)	O
,	O
CC	B-DatasetName
-	I-DatasetName
News	I-DatasetName
(	O
the	O
English	O
portion	O
of	O
the	O
Com	B-DatasetName
-	I-DatasetName
monCrawl	I-DatasetName
News	I-DatasetName
dataset	O
3	O
76	B-HyperparameterValue
GB	I-HyperparameterValue
)	O
,	O
OpenWebText	B-DatasetName
(	O
web	O
content	O
extracted	O
from	O
URLs	O
shared	O
on	O
Reddit	B-DatasetName
with	O
at	O
least	O
three	O
upvotes	O
(	O
Gokaslan	O
and	O
Cohen	O
,	O
2019	O
)	O
,	O
38	B-HyperparameterValue
GB	I-HyperparameterValue
)	O
and	O
Stories	O
(	O
subset	O
of	O
Common	B-DatasetName
-	I-DatasetName
Crawl	I-DatasetName
data	O
filtered	O
to	O
match	O
the	O
story	O
-	O
like	O
style	O
of	O
Winograd	O
schemas	O
(	O
Trinh	O
and	O
Le	O
,	O
2019	O
)	O
,	O
31	B-HyperparameterValue
GB	I-HyperparameterValue
)	O
.	O

Dist	B-MethodName
-	I-MethodName
n	I-MethodName
(	O
Li	O
et	O
al	O
.	O
,	O
2016	O
)	O
evaluates	O
the	O
response	O
diversity	O
by	O
counting	O
unique	B-HyperparameterValue
n	I-MetricName
-	I-MetricName
grams	I-MetricName
.	O

Our	O
model	O
,	O
MemIML	B-MethodName
,	O
performs	O
the	O
best	O
in	O
most	O
aspects	O
,	O
including	O
quality	O
,	O
diversity	O
,	O
and	O
task	O
consistency	O
.	O

Namely	O
,	O
the	O
student	O
is	O
refined	O
with	O
L	O
cls	O
after	O
the	O
distillation	O
steps	O
.	O

While	O
content	B-TaskName
retention	I-TaskName
is	O
not	O
explicitly	O
defined	O
in	O
the	O
literature	O
,	O
we	O
design	O
this	O
new	O
task	O
of	O
constrained	O
unsupervised	O
attribute	B-TaskName
transfer	I-TaskName
that	O
assigns	O
explicit	O
constraints	O
C	O
=	O
{	O
c	O
1	O
,	O
c	O
2	O
,	O
.	O

•	O
Quality	B-MetricName
:	O
BLEU	B-MetricName
-	I-MethodName
n	I-MethodName
(	O
Papineni	O
et	O
al	O
.	O
,	O
2002	O
)	O
,	O
CIDEr	B-MethodName
(	O
Vedantam	O
et	O
al	O
.	O
,	O
2015	O
,	O
and	O
ROUGE	B-MethodName
(	O
Lin	O
,	O
2004	O
)	O
measures	O
the	O
n	B-HyperparameterValue
-	I-MetricName
gram	I-HyperparameterValue
matching	O
between	O
the	O
generated	O
response	O
and	O
ground	O
truth	O
.	O

B.2	O
.	O

We	O
use	O
the	O
BART	B-MethodName
12	I-MethodName
-	I-MethodName
6	I-MethodName
as	O
the	O
student	O
model	O
,	O
and	O
the	O
distillation	O
results	O
on	O
CNNDM	B-DatasetName
are	O
in	O
Table	O
5	O
.	O

With	O
the	O
same	O
amount	O
of	O
training	O
data	O
,	O
GLM	B-MethodName
consistently	O
outperforms	O
BERT	B-MethodName
on	O
most	O
tasks	O
with	O
either	O
base	O
or	O
large	O
architecture	O
.	O

Our	O
paraphrase	O
samples	O
also	O
tightly	O
capture	O
the	O
meaning	O
of	O
the	O
source	O
sentences	O
.	O

Attention(Q	O
k	O
,	O
K	O
k	O
,	O
V	O
k	O
)	O
.	O

On	O
the	O
other	O
hand	O
,	O
Transformer	B-MethodName
(	I-MethodName
SA+GA	I-MethodName
)	I-MethodName
strongly	O
attends	O
to	O
the	O
knowledge	O
entity	O
of	O
person	O
(	O
Q2439789	O
)	O
presented	O
in	O
the	O
image	O
with	O
undesired	O
attention	O
score	O
0.788	O
.	O

Fingerspelling	O
boundary	O
information	O
is	O
again	O
not	O
used	O
in	O
this	O
baseline	O
.	O

Our	O
best	O
performing	O
student	O
model	O
PLATE	B-MethodName
B12	I-MethodName
-	I-MethodName
3	I-MethodName
λ=1.5	I-MethodName
outperforms	O
BART	B-MethodName
-	I-MethodName
PL	I-MethodName
,	O
BART	B-MethodName
-	I-MethodName
SFT	I-MethodName
,	O
and	O
BART	B-MethodName
-	I-MethodName
KD	I-MethodName
on	O
XSum	B-DatasetName
.	O

The	O
sign	O
language	O
videos	O
are	O
untrimmed	O
,	O
i.e.	O
they	O
include	O
regular	O
signs	O
in	O
addition	O
to	O
fingerspelling	O
,	O
and	O
are	O
downsampled	O
here	O
for	O
visualization	O
.	O

All	O
datasets	O
were	O
annotated	O
with	O
four	O
entity	O
types	O
:	O
LOC	B-DatasetName
,	O
MISC	B-DatasetName
,	O
ORG	O
,	O
and	O
PER	B-MethodName
.	O

However	O
,	O
current	O
NDR	B-TaskName
models	O
face	O
severe	O
generalization	O
failure	O
on	O
hypothetical	O
questions	O
.	O

However	O
,	O
the	O
sequence	O
-	O
level	O
knowledge	O
of	O
teacher	O
mod	O
-	O
els	O
is	O
not	O
well	O
utilized	O
.	O

In	O
Section	O
E	O
,	O
we	O
depict	O
the	O
implementation	O
details	O
of	O
comparative	O
models	O
for	O
KVQA.The	B-DatasetName
diverse	O
split	O
statistics	O
for	O
four	O
benchmark	O
datasets	O
,	O
KVQA	B-DatasetName
,	O
FVQA	B-DatasetName
(	O
Wang	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
PQ	B-DatasetName
and	O
PQL	B-DatasetName
(	O
Zhou	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
are	O
shown	O
in	O
Table	O
4	O
.	O

Despite	O
this	O
growing	O
body	O
of	O
research	O
,	O
no	O
consensus	O
has	O
emerged	O
about	O
standards	O
,	O
terminology	O
and	O
definitions	O
.	O

He	O
was	O
a	O
guest	O
lecturer	O
at	O
King	O
's	O
College	O
London	O
,	O
and	O
then	O
took	O
two	O
years	O
of	O
acting	O
courses	O
at	O
the	O
brit	O
school	O
of	O
acting	O
to	O
prepare	O
for	O
his	O
future	O
career	O
in	O
the	O
entertainment	O
industry	O
.	O

where	O
the	O
adapted	O
parameters	O
ω	O
q	O
j	O
are	O
discarded	O
thereafter	O
,	O
and	O
the	O
model	O
does	O
not	O
back	O
-	O
propagate	O
throughV	O
q	O
j	O
.	O

For	O
each	O
task	O
T	O
i	O
,	O
the	O
task	O
-	O
specific	O
memory	O
M	O
i	O
consists	O
of	O
N	O
i	O
memory	O
slots	O
(	O
i.e.	O
key	O
-	O
value	O
pairs	O

Paraphrasing	O
was	O
performed	O
manually	O
using	O
six	O
transformation	O
procedures	O
(	O
i.e.	O
,	O
addition	O
,	O
deletion	O
,	O
expansion	O
,	O
permutation	O
,	O
reduction	O
,	O
and	O
replacement).Transliteration	O
involves	O
mapping	O
a	O
text	O
written	O
with	O
orthographic	O
symbols	O
in	O
a	O
given	O
script	O
into	O
another	O
(	O
Beesley	O
,	O
1998	O
)	O
.	O

Therefore	O
,	O
we	O
remove	O
the	O
Stories	O
dataset	O
and	O
replace	O
OpenWebText	B-DatasetName
with	O
OpenWebText2	B-DatasetName
5	I-DatasetName
(	O
66	B-HyperparameterValue
GB	I-HyperparameterValue
)	O
.	O

To	O
solve	O
the	O
task	O
,	O
two	O
pioneering	O
studies	O
(	O
Wang	O
et	O
al	O
.	O
,	O
2017(Wang	O
et	O
al	O
.	O
,	O
,	O
2018	O
suggested	O
logical	O
parsing	O
-	O
based	O
methods	O
which	O
convert	O
a	O
question	O
to	O
a	O
KB	O
logic	O
query	O
using	O
predefined	O
query	O
templates	O
and	O
execute	O
the	O
generated	O
query	O
on	O
KB	O
for	O
searching	O
an	O
answer	O
.	O

go	O
to	O
china	O
blossom	O
worst	O
experience	O
ever	O
.	O

This	O
LSTM	B-MetricName
is	O
optimized	O
with	O
the	O
base	O
model	O
.	O

Each	O
hypothetical	O
question	O
is	O
related	O
to	O
one	O
factual	O
question	O
from	O
TAT	B-DatasetName
-	I-DatasetName
QA	I-DatasetName
,	O
but	O
each	O
factual	O
question	O
in	O
TAT	B-DatasetName
-	I-DatasetName
QA	I-DatasetName
is	O
not	O
guaranteed	O
to	O
have	O
one	O
hypothetical	O
question	O
.	O

where	O
a	O
massive	O
number	O
of	O
knowledge	O
facts	O
from	O
a	O
general	O
knowledge	O
base	O
(	O
KB	O
)	O
is	O
given	O
with	O
an	O
image	O
-	O
question	O
pair	O
.	O

BAN	B-MethodName
calculates	O
soft	O
attention	O
scores	O
between	O
knowledge	O
entities	O
and	O
question	O
words	O
.	O

Fig	O
.	O

Finally	O
,	O
we	O
compare	O
GLM	B-MethodName
variants	O
with	O
different	O
pretraining	O
designs	O
to	O
understand	O
their	O
importance	O
.	O

The	O
multi	O
-	O
hop	O
graph	O
walk	O
is	O
conducted	O
in	O
the	O
same	O
manner	O
as	O
the	O
knowledge	O
hypergraph	O
.	O

There	O
is	O
rising	O
interest	O
in	O
translating	O
code	O
-	O
switched	O
data	O
(	O
Nagoudi	O
et	O
al	O
.	O
,	O
2021	O
)	O
.	O

Further	O
,	O
advanced	O
pre	O
-	O
trained	O
language	O
models	O
(	O
PLMs	O
)	O
with	O
improved	O
architectures	O
or	O
training	O
methods	O
continue	O
to	O
emerge	O
,	O
including	O
ALBERT	B-MethodName
(	O
Lan	O
et	O
al	O
.	O
,	O
2019	O
)	O
or	O
RoBERTa	B-MethodName
.	O

all	O
the	O
meta	O
-	O
training	O
tasks	O
satisfy	O
this	O
criterion	O
,	O
the	O
generalization	O
ability	O
of	O
the	O
model	O
initialization	O
improves	O
.	O

We	O
validate	O
the	O
model	O
performance	O
on	O
the	O
three	O
commonly	O
-	O
used	O
datasets	O
across	O
7	O
languages	O
and	O
the	O
experimental	O
results	O
show	O
the	O
superiority	O
of	O
our	O
presented	O
MTMT	B-MethodName
model	O
.	O

These	O
are	O
MSR	B-MethodName
-	I-MethodName
Paraphrase	I-MethodName
(	O
510	B-HyperparameterValue
pairs	B-HyperparameterName
)	O
,	O
MSR	B-MethodName
-	I-MethodName
Video	I-MethodName
(	O
368	B-HyperparameterValue
pairs	B-HyperparameterName
)	O
,	O
and	O
SMTeuroparl	B-MethodName
(	O
203	B-HyperparameterValue
pairs	B-HyperparameterName
)	O
.	O

On	O
the	O
two	O
benchmarks	O
,	O
GLM	B-MethodName
can	O
still	O
outperform	O
BERT	B-MethodName
with	O
the	O
same	O
amount	O
of	O
parameters	O
,	O
but	O
with	O
a	O
smaller	O
margin	O
.	O

We	O
control	O
the	O
memory	B-HyperparameterName
size	I-HyperparameterName
through	O
|M	O
|	O
=	O
store	B-HyperparameterName
ratio	I-HyperparameterName
×	O
|D	O
s	O
|	O
.	O

For	O
similarity	B-TaskName
-	I-TaskName
based	I-TaskName
answer	O
,	O
we	O
calculate	O
a	O
dot	O
product	O
similarity	O
p	O
=	O
zC	O
T	O
between	O
z	O
and	O
answer	O
candidate	O
set	O
C	O
∈	O
R	O
|A|×w	O
where	O
|A|	O
is	O
a	O
number	O
of	O
candidate	O
answers	O
and	O
w	O
is	O
a	O
dimension	O
of	O
representation	O
for	O
each	O
answer	O
.	O

ii	O
)	O
Syntactic	O
:	O
Presence	O
of	O
personal	O
pronouns	O
(	O
binarized	O
to	O
indicate	O
the	O
presence	O
of	O
a	O
personal	O
pronoun	O
)	O
;	O
number	B-HyperparameterName
of	I-HyperparameterName
adjectives	I-HyperparameterName
(	O
categorical	O
up	O
to	O
5	B-HyperparameterValue
)	O
;	O
number	B-HyperparameterName
of	I-HyperparameterName
proper	I-HyperparameterName
nouns	I-HyperparameterName
(	O
categorical	O
up	O
to	O
3	B-HyperparameterValue
)	O
;	O
syntactic	B-HyperparameterName
tree	I-HyperparameterName
height	I-HyperparameterName
(	O
categorical	O
up	O
to	O
10	B-HyperparameterValue
)	O
.	O

The	O
hyperparameters	O
for	O
GLM	B-MethodName
Doc	I-MethodName
and	O
GLM	B-MethodName
Sent	I-MethodName
are	O
the	O
same	O
as	O
those	O
of	O
GLM	B-MethodName
Large	I-MethodName
.	O

Each	O
dialogue	O
is	O
generated	O
by	O
users	O
with	O
a	O
defined	O
goal	O
which	O
may	O
cover	O
1	O
-	O
5	O
domains	O
with	O
a	O
maximum	O
of	O
13	O
turns	O
in	O
a	O
conversation	O
.	O

)	O
.	O
To	O
build	O
these	O
memory	O
slots	O
,	O
we	O
select	O
samples	O
from	O
support	O
sets	O
and	O
write	O
their	O
information	O
into	O
the	O
memory	O
.	O

The	O
only	O
difference	O
is	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
spans	I-HyperparameterName
and	O
the	O
span	B-HyperparameterName
lengths	I-HyperparameterName
.	O

We	O
also	O
add	O
a	O
few	O
refinement	O
steps	O
to	O
refine	O
the	O
classification	O
layer	O
of	O
the	O
student	O
model	O
.	O

AraT5	B-MethodName
Models	O
Release	O
.	O

Specifically	O
,	O
given	O
the	O
key	O
representation	O
K	O
q	O
j	O
of	O
a	O
sample	O
X	O
q	O
j	O
∈	O
D	O
q	O
i	O
,	O
we	O
retrieve	O
the	O
top	O
N	O
most	O
similar	O
slots	O
from	O
its	O
task	O
-	O
specific	O
memory	O
M	O
i	O
.	O

The	O
source	O
code	O
of	O
KaFSP	B-MethodName
is	O
available	O
at	O
https	O
:	O
//github.com	O
/	O
tjunlp	O
-	O
lab	O
/	O
KaFSP	B-MethodName
.	O

Each	O
of	O
Tables	O
D.3	O
,	O
D.4	O
,	O
and	O
D.5	O
(	O
Appendix	O
D	O
)	O
shows	O
two	O
output	O
samples	O
from	O
our	O
paraphrasing	O
,	O
transliteration	B-TaskName
,	O
and	O
title	B-TaskName
generation	O
models	O
,	O
respectively	O
.	O

We	O
thus	O
believe	O
that	O
TAGOP	B-MethodName
-	I-MethodName
L2I	I-MethodName
can	O
generalize	O
well	O
to	O
more	O
deriving	O
operations	O
by	O
simply	O
incorporating	O
the	O
operators	O
,	O
as	O
long	O
as	O
the	O
corresponding	O
training	O
questions	O
are	O
not	O
rare	O
.	O

That	O
may	O
be	O
the	O
reason	O
why	O
multiple	O
continuous	O
spans	O
of	O
text	O
are	O
copied	O
.	O

The	O
scores	O
across	O
all	O
settings	O
drop	O
when	O
GA	O
or	O
SA	O
is	O
removed	O
.	O

As	O
we	O
proposed	O
,	O
applying	O
both	O
calibrated	O
teacher	O
training	O
and	O
activation	O
boundary	O
distillation	O
resulted	O
in	O
a	O
superior	O
performance	O
.	O

β	B-HyperparameterName
is	O
tuned	O
to	O
1	B-HyperparameterValue
(	O
chosen	O
from	O
{	O
0.5	B-HyperparameterValue
,	O
1	B-HyperparameterValue
,	O
2	B-HyperparameterValue
,	O
3	B-HyperparameterValue
}	O
)	O
.	O

During	O
inference	O
,	O
as	O
common	O
wisdom	O
,	O
we	O
apply	O
beam	O
search	O
.	O

Specifically	O
,	O
we	O
train	O
a	O
recognizer	O
to	O
output	O
a	O
sequence	O
of	O
symbols	O
consisting	O
of	O
either	O
fingerspelled	O
letters	O
or	O
a	O
special	O
non	O
-	O
fingerspelling	O
symbol	O
<	O
x	O
>	O
.	O

It	O
projects	O
the	O
raw	O
content	O
into	O
latent	O
representation	O
.	O

Hence	O
we	O
create	O
ARGEN	B-DatasetName
CST	I-DatasetName
,	O
our	O
code	B-TaskName
-	B-TaskName
switched	I-TaskName
translation	I-TaskName
benchmark	O
component	O
,	O
using	O
four	O
sub	O
-	O
test	O
sets	O
.	O

2	O
)	O
PQ	B-DatasetName
and	O
PQL	B-DatasetName
datasets	O
have	O
annotations	O
of	O
a	O
ground	O
-	O
truth	O
reasoning	O
path	O
to	O
answer	O
a	O
given	O
question	O
.	O

Formally	O
,	O
it	O
is	O
to	O
learn	O
a	O
function	O
y	O
=	O
f	O
(	O
q	O
,	O
c	O
)	O
,	O
where	O
y	O
,	O
q	O
,	O
and	O
c	O
are	O
the	O
word	O
list	O
representing	O
the	O
answer	O
,	O
the	O
question	O
,	O
and	O
the	O
context	O
2	O
respectively	O
.	O

This	O
student	O
is	O
randomly	O
initialized	O
and	O
denoted	O
by	O
Transformer	B-MethodName
.	O

The	O
input	O
of	O
the	O
key	O
network	O
is	O
the	O
sample	O
input	O
sentence	O
X	O
s	O
j	O
∈	O
D	O
s	O
i	O
(	O
X	O
q	O
j	O
∈	O
D	O
q	O
i	O
)	O
,	O
and	O
the	O
output	O
is	O
the	O
encoded	O
representation	O
of	O
the	O
first	O
token	O
(	O
i.e.	O
[	O
CLS	O
]	O
token	O
)	O
of	O
the	O
sentence	O
.	O

Fot	O
the	O
text	B-TaskName
summarization	I-TaskName
task	O
,	O
we	O
use	O
the	O
dataset	O
Gigaword	B-MethodName
(	O
Rush	O
et	O
al	O
.	O
,	O
2015	O
)	O
for	O
model	O
fine	O
-	O
tuning	O
and	O
evaluation	O
.	O

RoBERTa	B-MethodName
's	O
performance	O
was	O
already	O
similar	O
to	O
the	O
teacher	O
model	O
in	O
the	O
initial	O
fine	O
-	O
tuning	O
stage	O
because	O
it	O
was	O
pre	O
-	O
trained	O
with	O
more	O
data	O
than	O
BERT	B-MethodName
and	O
exhibited	O
a	O
greater	O
robustness	O
.	O

FWS	B-DatasetName
and	O
FVS	B-DatasetName
respectively	O
consist	O
of	O
detecting	O
fingerspelled	O
words	O
within	O
a	O
given	O
raw	O
ASL	O
video	O
stream	O
and	O
detecting	O
video	O
clips	O
of	O
interest	O
containing	O
a	O
given	O
fingerspelled	O
word	O
.	O

In	O
FSS	B-MethodName
-	I-MethodName
Net	I-MethodName
,	O
the	O
visual	O
features	O
output	O
from	O
convolutional	O
layers	O
are	O
passed	O
through	O
a	O
1layer	B-HyperparameterValue
Bi	B-MetricName
-	I-HyperparameterName
LSTM	I-HyperparameterName
with	O
256	B-HyperparameterValue
hidden	B-HyperparameterName
units	I-HyperparameterName
per	B-HyperparameterName
direction	I-HyperparameterName
to	O
capture	O
temporal	O
information	O
.	O

We	O
follow	O
the	O
split	O
of	O
training	O
,	O
testing	O
and	O
validation	O
set	O
of	O
TAT	B-DatasetName
-	I-DatasetName
QA	I-DatasetName
as	O
shown	O
in	O
Table	O
2	O
.	O

T5	B-MethodName
uses	O
independent	O
positional	B-HyperparameterValue
encodings	I-HyperparameterValue
for	O
the	O
encoder	O
and	O
decoder	O
,	O
and	O
relies	O
on	O
multiple	O
sentinel	O
tokens	O
to	O
differentiate	O
the	O
masked	O
spans	O
.	O

5.3	O
.	O

If	O
the	O
current	O
subtask	O
T	O
is	O
not	O
finished	O
within	O
its	O
time	O
limit	O
,	O
we	O
force	O
the	O
agent	O
to	O
re	O
-	O
select	O
a	O
new	O
subtask	O
T	O
t	O
∈	O
T	O
t	O
\	O
{	O
T	O
}	O
,	O
regardless	O
whether	O
T	O
is	O
still	O
available	O
.	O

For	O
evaluation	O
,	O
we	O
introduce	O
a	O
novel	O
benchmark	O
for	O
ARabic	B-DatasetName
language	I-DatasetName
GENeration	I-TaskName
(	O
ARGEN	B-DatasetName
)	O
,	O
covering	O
seven	O
important	O
tasks	O
.	O

We	O
use	O
string	O
matching	O
to	O
only	O
include	O
tweets	O
with	O
at	O
least	O
3	O
Arabic	O
words	O
,	O
regardless	O
whether	O
the	O
tweet	O
has	O
non	O
-	O
Arabic	O
string	O
or	O
not	O
.	O

Details	O
about	O
ARGEN	B-DatasetName
NTG	I-DatasetName
are	O
in	O
Table	O
C.1	O
(	O
Appendix	O
)	O
.	O

The	O
ASL	O
videos	O
in	O
the	O
two	O
datasets	O
are	O
collected	O
from	O
online	O
resources	O
and	O
include	O
a	O
variety	O
of	O
viewpoints	O
and	O
styles	O
,	O
such	O
as	O
webcam	O
videos	O
and	O
lectures	O
.	O

The	O
only	O
difference	O
between	O
these	O
two	O
methods	O
is	O
that	O
TAGOP	B-MethodName
-	I-MethodName
CLO	I-MethodName
incorporates	O
an	O
extra	O
CLO	O
.	O

The	O
beam	B-HyperparameterName
size	I-HyperparameterName
,	O
length	B-HyperparameterName
penalty	I-HyperparameterName
,	O
and	O
minimal	B-HyperparameterName
length	I-HyperparameterName
are	O
4	B-HyperparameterValue
,	O
2.0	B-HyperparameterValue
,	O
and	O
55	B-HyperparameterValue
on	O
CNNDM	B-DatasetName
;	O
6	B-HyperparameterValue
,	O
0.1	B-HyperparameterValue
,	O
and	O
1	B-HyperparameterValue
on	O
XSum	B-DatasetName
;	O
and	O
4	B-HyperparameterValue
,	O
0.7	B-HyperparameterValue
,	O
and	O
80	B-HyperparameterValue
on	O
NYT	B-DatasetName
,	O
respectively	O
.	O

SuperGLUE	B-DatasetName
.	O

In	O
the	O
meta	O
-	O
training	O
process	O
,	O
we	O
first	O
train	O
θ	O
on	O
a	O
support	O
set	O
(	O
i.e.	O
,	O
a	O
few	O
training	O
samples	O
of	O
a	O
new	O
task	O
i	O
)	O
to	O
obtain	O
task	O
-	O
specific	O
parameters	O
θ	O
i	O
.	O

We	O
consider	O
that	O
the	O
reason	O
why	O
Hypergraph	B-MethodName
Transformer	I-MethodName
failed	O
to	O
infer	O
the	O
correct	O
answer	O
despite	O
focusing	O
on	O
the	O
exact	O
knowledge	O
fact	O
is	O
that	O
the	O
correct	O
answer	O
word	O
(	O
Myocardial	O
Infarction	O
)	O
appears	O
rarely	O
in	O
QA	O
pairs	O
.	O

He	O
appeared	O
in	O
the	O
first	O
few	O
episodes	O
of	O
"	O
"	O
as	O
the	O
character	O
major	O
Jack	O
Ryan	O
.	O

The	O
detailed	O
description	O
of	O
this	O
criterion	O
is	O
in	O
Appendix	O
.	O

Pretraining	O
largescale	O
language	O
models	O
significantly	O
improves	O
the	O
performance	O
of	O
downstream	O
tasks	O
.	O

Entity	O
linking	O
setting	O
We	O
also	O
present	O
the	O
experimental	O
results	O
on	O
the	O
entity	O
linking	O
setting	O
where	O
the	O
named	O
entities	O
are	O
not	O
provided	O
as	O
the	O
oracle	O
setting	O
,	O
but	O
detected	O
by	O
the	O
module	O
as	O
described	O
in	O
Section	O
3.2	O
.	O

The	O
Stories	O
dataset	O
is	O
no	O
longer	O
publicly	O
available	O
4	O
.	O

They	O
report	O
57B	B-HyperparameterValue
Arabic	B-HyperparameterName
tokens	I-HyperparameterName
(	O
almost	O
double	O
our	O
token	B-HyperparameterName
size	I-HyperparameterName
)	O
from	O
53	B-HyperparameterValue
M	I-HyperparameterValue
webpages	B-HyperparameterName
,	O
making	O
1.66	B-HyperparameterValue
%	I-HyperparameterValue
of	O
all	O
mT5	B-MethodName
data	O
.	O

Next	O
,	O
to	O
generate	O
sentences	O
,	O
we	O
consider	O
two	O
decodersx	O
src	O
∼	O
p	O
φ	O
(	O
x|z	O
)	O
andx	O
tgt	O
∼	O
p	O
η	O
(	O
x|z	O
)	O
.	O

