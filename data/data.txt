Abstract We propose a touch - based editing method for translation , which is more Ô¨Çexible than traditional keyboard - mouse - based translation postediting . 
This approach relies on touch actions that users perform to indicate translation errors . 
We present a dual - encoder model to handle the actions and generate reÔ¨Åned translations . 
To mimic the user feedback , we adopt the TER algorithm comparing between draft translations and references to automatically extract the simulated actions for training data construction . 
Experiments on translation datasets with simulated editing actions show that our method signiÔ¨Åcantly improves original translation of Transformer ( up to 25.31 BLEU ) and outperforms existing interactive translation methods ( up to 16.64 BLEU ) . 
We also conduct experiments on post - editing dataset to further prove the robustness and effectiveness of our method . 
1 Introduction Neural machine translation ( NMT ) has made great success during the past few years ( Sutskever et al . 
, 2014 ; Bahdanau et al . 
, 2014 ; Wu et al . 
, 2016 ; Vaswani et al . 
, 2017 ) , but automatic machine translation is still far from perfect and can not meet the strict requirements of users in real applications ( Petrushkov et al . 
, 2018 ) . 
Many notable humanmachine interaction approaches have been proposed for allowing professional translators to improve machine translation results ( Wuebker et al . 
, 2016 ; Knowles and Koehn , 2016 ; Hokamp and Liu , 2017 ) . 
As an instance of such approaches , post - editing directly requires translators to modify outputs from machine translation ( Simard et al . 
, 2007 ) . 
However , traditional post - editing requires intensive keyboard interaction , which is inconvenient on mobile devices . 
Grangier and Auli ( 2018 ) suggest a one - time interaction approach with lightweight editing ef - forts , QuickEdit , in which users are asked to simply mark incorrect words in a translation hypothesis for one time in the hope that the system will change them . 
QuickEdit delivers appealing improvements on draft hypotheses while maintaining the Ô¨Çexibility of human - machine interaction . 
Unfortunately , only marking incorrect words is far from adequate : for example , it does not indicate the missing information beyond the original hypothesis , which is a typical issue called under - translation in machine translation ( Tu et al . 
, 2016 ) . 
In this paper , we propose a novel one - time interaction method called Touch Editing , which is Ô¨Çexible for users and more adequate for a system to generate better translations . 
Inspired by human editing process , the proposed method relies on a series of touch - based actions including SUBSTITU TION , DELETION , INSERTION and REORDERING . 
These actions do not include lexical information and thus can be Ô¨Çexibly provided by users through various of gestures on touch screen devices . 
By using these actions , our method is able to capture the editing intention from users to generate better translations : for instance , INSERTION indicates a word is missing at a particular position , and our method is expected to insert the correct word . 
To this end , we present a neural network model by augmenting Transformer ( Vaswani et al . 
, 2017 ) with an extra encoder for a hypothesis and its actions . 
Since it is impractical to manually annotate large - scale action dataset to train the model , we thereby adopt the algorithm of TER ( Snover et al . 
, 2006 ) to automatically extract actions from a draft hypothesis and its reference . 
To evaluate our method , we conduct simulated experiments on translation datasets the same as in other works ( Denkowski et al . 
, 2014 ; Grangier and Auli , 2018 ) , The results demonstrate that our method can address the well - known challenging issues in machine translation including over-1 QuickEditHypothesis   y‚Äôtravel    far    does    not     necessary    to    proctor    for     food     supply   .Resulttravel    far    does    not       require      to    proctor             food    supplies   .Source   xweite wegem√ºsse proctorf√ºr die nahrungsmittelbeschaffungnicht gehen . 
Reference   yproctor    does    not    have    to    travel    far    to    buy    food    . 
Touch EditingHypothesis   y‚Äôtravel    fardoes    not     necessary    to    proctorfor         food     supply   . 
Modified   ùëöùëö(y ‚Äô ) proctordoes    not    necessary    to    travel    farfor    < INS > food    supply   . 
Action Sequence   a --- S            ---S         I             -D       - Resultproctor    does    not        have       to    travel    far      to      buy       food                 .Figure 1 : Example of interaction methods . 
QuickEdit allows users to mark incorrect words . 
Our method introduces more Ô¨Çexible actions . 
m(y / prime)is modiÔ¨Åed from y / primeby applying reordering actions and inserting a special token /angbracketleftINS / angbracketrightto keep alignment with the action sequence awhich contains actions like SUBSTITUTION , INSERTION and DELETION . 
‚Äú - ‚Äù denotes the word in that position is unmarked . 
Our method then generates a reÔ¨Åned translation based on the modiÔ¨Åed hypothesis m(y / prime)and the action sequence a. translation , under - translation and mis - ordering , and thus it outperforms Transformer and QuickEdit by a margin up to 25.31 and 16.64 BLEU points respectively . 
In addition , experiments on post - editing dataset further prove the effectiveness and robustness of our method . 
Finally , we implement a real application on mobile phones to discuss the usability in real senarios . 
2 Touch Editing Approach 2.1 Actions QuickEdit allows translators to mark incorrect words which they expect the system to change ( Grangier and Auli , 2018 ) . 
However , as shown in Figure 1 , the information is inadequate for a system to correct a translation hypothesis , especially when it comes to under - translation , in which the system is hardly to predict missing words into hypotheses . 
To achieve better adequacy , we take human editing habits into consideration . 
As shown in Figure 1 , a human translator may insert , delete , substitute or reorder some words to correct errors of undertranslation , over - translation , mis - translation and mis - ordering in an original translation hypothesis . 
Based on human editing process , we deÔ¨Åne a set of actions to represent human editing intentions : ‚Ä¢INSERTION : a new word should be inserted into a given position . 
‚Ä¢DELETION : a word at a speciÔ¨Åc position should be deleted.‚Ä¢SUBSTITUTION : a word should be substituted by another word . 
‚Ä¢REORDERING : a segment of words should be moved to another position . 
In Touch Editing , these actions can be performed by human translators on a given machine hypothesis to indicate translation errors . 
To keep the Ô¨Çexibility of interactions , for SUBSTITUTION and INSERTION actions , our method allows users to only indicate which word should be substitute or in which position a word should be inserted . 
The light - weight interaction in Touch Editing is nonlexical , i.e. , it does not require any keyboard inputs , and thus can be adopted to mobile devices with touch screens . 
2.2 Model Our model seeks to correct translation errors of an original hypothesis y / primebased on actionsAprovided by human translator . 
To make full use of the actions , we Ô¨Årstly modify the original hypothesis by applying Aony / primeto obtainA(y / prime ): A(y / prime ) = /angbracketleftm(y / prime),a / angbracketright . 
( 1 ) SpeciÔ¨Åcally , as shown in Figure 1 , m(y / prime)is modiÔ¨Åed from y / primeby reordering the segment in gray color and inserting a token /angbracketleftINS / angbracketright , and thus theREORDERING actions is implicitly included in2 Target EmbeddingLinear & SoftmaxOutput Probabilities TargetTransformerDecoder Source EmbeddingTransformer Encoder(source ) SourceModified Hypothesis Action SequenceAction PositionalEmbedding Target EmbeddingTransformerEncoder(hypothesis ) Figure 2 : Model architecture . 
We add a hypothesis encoder ( the right part ) into Transformer which differs from source encoder ( the left part ) in positional embedding . 
We use learned action positional embedding instead of the sinusoids . 
INSERTION Positional Embedding012 ‚Ä¶ ùë≥ùë≥ DELETION Positional Embedding012 ‚Ä¶ ùë≥ùë≥ SUBSTITUTION Positional Embedding012 ‚Ä¶ ùë≥ùë≥ None ActionPositional Embedding012 ‚Ä¶ ùë≥ùë≥ Action SequenceI      D     S      -Figure 3 : Action positional embedding . 
The model Ô¨Årstly chooses an embedding matrix according to the action at position i , then lookups the ith row of the matrix as the positional embedding of position i. Lis the maximum length of sentences . 
m(y / prime ) . 
The action sequence abelowm(y / prime)contains SUBSTITUTION , INSERTION andDELETION at the corresponding position . 
We then use a neural network model to generate a translation yfor the source sentence x , the hypothesis y / primeand the actionsA : P(y|x , y / prime , A;Œ∏ ) = N / productdisplay n=1P(yn|y < n , x , m(y / prime),a;Œ∏).(2 ) As shown in Figure 2 , the neural network model we developed is a dual encoder model based on Transformer similar to Tebbifakhr et al . 
( 2018 ) . 
SpeciÔ¨Åcally , besides encoding the source sentence xwith source encoder ( the left part of Figure 2 ) , our model additionally encodes A(y / prime)with an extrahypothesis encoder ( the right part of Figure 2 ) and integrates the encoded representations into decoding network using dual multi - head attention . 
EncodingA(y / prime)As shown in the right part of Figure 2 , the hypothesis encoder Ô¨Årstly embeds m(y / prime)with lengthlin distributed space using the same word embedding as in decoder , which is denoted as w={w1,¬∑¬∑¬∑,wl } . 
Then it encodes a={a1,¬∑¬∑¬∑,al}with learned positional embedding according to the speciÔ¨Åc actions . 
As shown in Figure 3 , the action positional embedding includes four embedding matrixes corresponding to three action types and a none action for positions without any action . 
For the ith position of a , the encoder chooses an embedding matrix based on the action type ofaiand selects the ith row of the matrix as the positional embedding vector , which is denoted aspi : pi=Ô£± Ô£¥Ô£¥Ô£≤ Ô£¥Ô£¥Ô£≥PE INSERTION ( i ) ifai = I PE DELETION ( i ) ifai =D PE SUBSTITUTION ( i)ifai = S PE None(i ) ifai=-(3 ) WherePE‚àódenote the action positional embedding matrixes in Figure 3 . 
The learned action positional embedding is used in hypothesis encoder to replace the Ô¨Åxed sinusoids positional encoding in Transformer encoder . 
Next , the encoder adds the word embedding wand the action positional embedding pto obtain input embedding e={w1+p1,¬∑¬∑¬∑,wl+pl } . 
The following part ofhypothesis encoder lies the same as Transformer encoder . 
Decoding The output of hypothesis encoder , together with the output of source encoder , are fed into the decoder . 
To combine both of the encoders ‚Äô outputs , we apply dual multi - head attention in each layer of decoder : the attention sub - layer attends to both encoders ‚Äô outputs by performing multi - head attention respectively : Asrc = MultiHead ( Qtgt , Ksrc , Vsrc ) Ahyp = MultiHead ( Qtgt , Khyp , Vhyp)(4 ) WhereQtgtis coming from previous layer of the decoder , KsrcandVsrcmatrixes are Ô¨Ånal representations of the source encoder whileKhypandVhyp matrixes are Ô¨Ånal representations of the hypothesis encoder . 
The two attention vectors AsrcandAhyp are then averaged to replace encoder - decoder attention in Transformer , resulting in the input of next layer.3 Training The overall model , which includes a source encoder , ahypothesis encoder with action positional embedding , and a decoder , is jointly trained . 
We maximize the log - likelihood of the reference sentence ygiven the source sentence x , the initial hypothesis y / prime , and the corresponding actions A. By applyingAony / prime , the training objective becomes : ÀÜŒ∏= arg max Œ∏ / braceleftBigg / summationdisplay DlogP(y|x , m(y / prime),a;Œ∏)/bracerightBigg . 
( 5 ) whereDis the training dataset consists of quadruplets like ( source x , modiÔ¨Åed hypothesis m(y / prime ) , action sequence a , target y ) . 
We use Adam optimizer ( Kingma and Ba , 2014 ) , an extension of stochastic gradient descent ( Bottou , 1991 ) , to train the model . 
After training , the model with parameter ÀÜŒ∏is then used in inference phase to generate reÔ¨Åned translations for test data , which consists of triplets like ( source x , modiÔ¨Åed hypothesis m(y / prime ) , action sequence a ) . 
3 Automatic Data Annotation The actions we deÔ¨Åned in Section 2.1 can be provided by human translators in real applications . 
However , it is impractical to manually collect a large scale annotated dataset for training our model . 
Thus we resort to propose an approach to automatically extract editing actions from a machine translation hypothesis and its corresponding reference . 
To make our method powerful , the number of editing actions which convert a hypothesis to its Algorithm 1 Extracting actions with TER Input : hypothesis y / prime , reference y m(y / prime)‚Üêy / prime a‚ÜêEmpty action sequence repeat Find reordering rthat most reduces min - editdistance(m(y / prime),y ) ifrreduces edit distance then m(y / prime)‚Üêapplyingrtom(y / prime ) end if until no beneÔ¨Åcial reordering remains a‚Üêmin - edit(m(y / prime),y ) m(y / prime)‚Üêinsert / angbracketleftINS / angbracketrightintom(y / prime)based on a Output : m(y / prime),areference is minimal as presented in Section 2.1 . 
Snover et al . 
( 2006 ) study this problem and point out that its optimal solution is NP - hard ( Lopresti and Tomkins , 1997 ; Shapira and Storer , 2002 ) . 
To optimize the number of editing actions , they instead propose an approximate algorithm based on minimal edit distance . 
The basic idea of their algorithm can be explained as follows . 
It repeatedly modiÔ¨Åes the intermediate string by applying reordering actions which is greedily found to mostly reduce the edit distance between the intermediate string and the reference , until no more beneÔ¨Åcial reordering remains . 
In this paper , we adopt the basic idea of Snover et al . 
( 2006 ) to automatically extract actions . 
As shown in Algorithm 1 , given a reference and a hypothesis , the algorithm repeatedly reorders words to reduce the word - level minimal edit distance between reference yand modiÔ¨Åed hypothesis m(y / prime ) until no beneÔ¨Åcial reordering remains . 
With the modiÔ¨Åed hypothesis m(y / prime ) , the algorithm then calculates the editing action sequence athat minimize the word - level edit distance between m(y / prime)andy ( see Action Sequence ain Figure 1 ) . 
It Ô¨Ånally inserts special token / angbracketleftINS / angbracketrightto keep alignment between the modiÔ¨Åed hypothesis and the action sequence ( see ModiÔ¨Åed m(y / prime)in Figure 1 ) . 
The output of the algorithm , which is a tuple of modiÔ¨Åed hypothesis and action sequence , together with the source sentence and its reference , are used to train our model as described in Section 2.2 . 
4 Experiment We conduct simulated experiment on translation datasets . 
SpeciÔ¨Åcally , we translate the source sentences in translation datasets with a pre - trained Transformer model and build the training data with simulated human feedback using algorithm described in Section 3 . 
4.1 Dataset and Settings The experiment is conducted on three translation datasets : the IWSLT‚Äô14 English - German dataset ( Cettolo et al . 
, 2014 ) , the WMT‚Äô14 EnglishGerman dataset ( Bojar et al . 
, 2014 ) and the WMT‚Äô17 Chinese - English dataset ( Ondrej et al . 
, 2017 ) . 
The IWSLT‚Äô14 English - German dataset consists of 170k sentence pairs from TED talk subtitles . 
We use dev2010 as validation set which contains 887 sentent pairs , and a concatenation of tst2010 , tst2011 andtst2012 as test set which con-4 ModelIWSLT‚Äô14 WMT‚Äô14 WMT‚Äô17 EN - DE DE - EN EN - DE DE - EN EN - ZH ZH - EN BLEU TER BLEU TER BLEU TER BLEU TER BLEU TER BLEU TER ConvS2S‚Ä†24.20 - 27.40 - 25.20 - 29.70 - - - - QuickEdit‚Ä†30.80 - 34.60 - 36.60 - 41.30 - - - - Transformer 27.40 0.52 33.17 0.45 26.69 0.56 31.73 0.48 32.53 0.55 21.89 0.61 QuickEdit‚Ä°34.33 0.43 40.13 0.39 37.00 0.43 41.48 0.39 41.20 0.43 29.78 0.51 Touch Baseline 34.48 0.42 40.09 0.35 33.92 0.43 39.47 0.37 38.96 0.42 29.17 0.51 Touch Editing 44.25 0.32 50.39 0.29 50.49 0.28 56.47 0.24 57.84 0.28 45.67 0.33 Table 1 : Results of different systems measured in BLEU and TER.‚Ä†denotes the results from Quick Edit . 
QuickEdit‚Ä°is our reimplementation based on Transformer . 
Touch baseline is the result modiÔ¨Åed from initial hypothesis by deleting and reordering words . 
Touch Editing is our model trained with all actions described in Section 2.1 . 
tains 4698 sentence pairs . 
For WMT‚Äô14 EnglishGerman dataset , we use the same data and preprocessing as ( Luong et al . 
, 2015 ) . 
The dataset consists of 4.5 M sentence pairs for training1 . 
We take newstest2013 for validation and newstest2014 for testing . 
For Chinese to English dataset , we use CWMT portion which is a subset of WMT‚Äô17 training data containing 9 M sentence pairs . 
We validate onnewsdev2017 and test on newstest2017 . 
As for vocabulary , the English and German datasets are encoded using byte - pair encoding ( Sennrich et al . 
, 2015 ) with a shared vocabulary of 8k tokens for IWSLT‚Äô14 and 32k tokens for WMT‚Äô14 . 
For Chinese to English dataset , the English vocabulary is set to 30k subwords , while the Chinese data is tokenized into character level and the vocabulary is set to 10k characters . 
Note that even with subword units or character units , the actions are marked in word level , i.e. all units from a given word share the same actions . 
We train the models with two settings . 
For the larger WMT English - German and English - Chinese dataset , we borrow the Transformer base parameter set of Vaswani et al . 
( 2017 ) , which contains 6 layers for encoders and decoder respectively . 
The multi - head attention of each layer contains 8 heads . 
The word embedding size is set to 512 and the feedforward layer dimension is 2048 . 
For the smaller IWSLT dataset , we use 3 layers for each component and multi - head attention with 4 heads in each layer . 
The word embedding size is 256 and the feedforward layers ‚Äô hidden size is 1024 . 
We also apply label smoothing œÉls= 0.1and dropout pdropout = 0.1during training . 
All models are 1We use the pre - processed data from https://nlp . 
stanford.edu/projects/nmt/trained from scratch with corresponding training data , e.g. , parallel data for Transformer baseline model and annotated data for Touch Editing . 
4.2 Main Results We report the results of different systems including Transformer and QuickEdit . 
The Transformer model is tested on bitext data , i.e. , the model directly generates translations based on source sentences . 
As for the QuickEdit , we followed the settings of Grangier and Auli ( 2018 ) , in which they mark all words in initial translation results that do not appear in the references as incorrect , and use the QuickEdit model to generate reÔ¨Åned translations . 
In Touch Baseline setting , we use the algorithm described in Section 3 to obtain the actions respect to initial translations and references , and then apply reordering and deletion actions to obtain reÔ¨Åned translations . 
The Touch Edit setting accesses the same information as Touch Baseline but uses the neural model described in Section 2.2 to handle the actions . 
Note that the original QuickEdit model is based on ConvS2S , and thus we reimplement it based on Transformer to keep the fairness of comparison2 . 
As shown in Table 1 , our model strongly outperforms other systems . 
As for BLEU score , our model achieves up to +25.31 than Transformer and +16.64 than QuickEdit . 
Our model also signiÔ¨Åcantly reduces TER by -0.28 and -0.18 comparing to Transformer and QuickEdit . 
We also notice that the improvement on the smaller IWSLT‚Äô14 dataset ( up to 17.22 ) is not as 2In fact , the comparison is still unfair because QuickEdit and our mothod access more supervised information than Transformer form simulated human feedback , which is the nature of interaction settings.5 Reordering RIBES Transformer 4672 79.97 QuickEdit 4799 84.33 Touch Editing 650 90.50 Table 2 : Word reordering quality , measured in number of word reorderings required to align to references , and RIBES score . 
signiÔ¨Åcant as that on the larger WMT‚Äô14 dataset ( up to 24.74 ) and WMT‚Äô17 dataset ( up to 25.31 ) . 
This observation is in consistent with QuickEdit , which also gains lower improvement on the smaller dataset . 
The reason , as described in Grangier and Auli ( 2018 ) , is that the underlying machine translation model is overÔ¨Åtted on the smaller 170k dataset . 
Thus the translation output requires less edits on which we build simulated editing action dataset . 
The limited supervised data further impacts the model quality and Ô¨Ånal results . 
4.3 Analysis To further investigate the model capacity , we conduct four experiments on WMT‚Äô14 English to German dataset . 
We analyze the factors that bring the remarkable improvement by modeling coverage , reordering quality and accuracy of each action type . 
We also test our model with limited number of actions to evaluate the model usability with partial feedback . 
Reordering We evaluate the word reordering quality of our model , compared with Transformer and QuickEdit . 
We adopt two automatic evaluation metrics . 
One metric is based on monolingual alignment . 
We Ô¨Årstly align model hypotheses and references with TER , and then count the number of words that should be reordered . 
As shown by Reordering in Table 2 , the output of our model requires less word reorderings to align with reference . 
The other metric is RIBES ( Isozaki et al . 
, 2010 ) , which is based on rank correlation . 
As shown in Table 2 , our method outperforms the other two systems with 90.50versus 79.97for Transformer and84.33for QuickEdit . 
Accuracy As described in Section 2.1 , the actions of our method represent human editing intentions , i.e. , they indicate errors in original hypothesis and our model is expected to correct these errors based on editing actions . 
To evaluate the accuracyTotal Correct Accuracy Quick EditDeletion 6438 4440 68.97 % Insertion 4430 681 15.37 % Substitution 20858 5030 24.12 % Touch EditingDeletion 6438 6383 99.15 % Insertion 4430 1609 36.32 % Substitution 20858 6645 31.86 % Table 3 : Accuracy of actions . 
Total means number of actions to transform the draft machine translations into references . 
Correct means how many words are corrected ( or deleted ) by the model . 
ofINSERTION , DELETION and SUBSTITUTION , we Ô¨Årst use TER to align machine translation hypotheses and references , as well as our model ‚Äôs outputs and references . 
With the references as intermediates , we then align our model ‚Äôs outputs and original machine translations . 
With the alignment result , we directly check whether the words with actions are corrected or not to calculate the accuracy of the three actions . 
To make a complete comparison , we also analyze the results of QuickEdit and calculate the accuracy . 
As shown in Table 3 , our model achieves the accuracy of 99.15 % for deletion3,36.32 % for insertion and 31.86 % for substitution . 
The high deletion accuracy shows that our model indeed learns to delete over - translated words . 
For insertion and substitution , the actions only indicate where to insert or substitute , and do not provide any ground truth . 
Since the self - attention mechanism in Transformer is good at word sense disambiguation ( Tang et al . 
, 2018a , b ) , our model is able to select correct words to insert or substitute . 
Partial Feedback The model we train and test is based on all actions , i.e. , all translation errors of the initial hypotheses are marked out . 
However , a human translator may not provide all marks . 
In fact , the feedback of human translators is hard to predict , and vary with different translators . 
In this case , we test our model with simulated partial feedback . 
We train our model with all actions and randomly select 0%,5%, ... 100 % of actions in test set to simulate human behavior . 
To further investigate the effect of partial feedback 3We do not explicitly remove words that marked as DELE TION and the neural model is responsible for making Ô¨Ånal decision whether these words should be deleted . 
It might slightly hurt BLEU and accuracy but potentially generates more Ô¨Çuent translations.6 253035404550550%20%40%60%80%100%BLEU Percentage of Actionsalltypeswithout REORDERINGSUBSTITUTIONDELETIONINSERTIONFigure 4 : Results of partial feedback measured in BLEU score . 
We train Ô¨Åve models to investigate the effects of partial feedback on different actions . 
on different actions , we train three extra models with speciÔ¨Åc kinds of actions : INSERT , DELETE and SUBSTITUTE . 
We then randomly select part of each kind of actions to test the model . 
Note that theREORDERING actions are always enabled since they are operated on a segment of words and can not be partially disabled . 
To investigate the effect of REORDERING actions , we also train a model without reordering and partially select three kinds of actions to test the model . 
As shown in Figure 4 , for the model trained with all actions , the BLEU scores increases from 29.43 ( with reordering only ) to 50.49 ( with all actions ) as more actions are provided . 
For the models trained with speciÔ¨Åc kinds of actions and the model trained without reordering , the observation is similar . 
4.4 Experiments on Post - Editing Data In previous sections , our model is tested and analyzed on automatic machine translation datasets . 
However , in post - editing scenarios , our model faces three major challenges : action inconsistency , data inconsistency and model inconsistency . 
For action inconsistency , the editing actions to train our model are extracted from machine predictions and references . 
The references in our training data are written by human from scratch , while in post - editing the references ( human post - edited results ) are revisions of machine translations , and thus the editing actions might be different . 
For data inconsistency , our model is trained on dataset of News domain ( WMT ) or TED talks ( IWSLT ) . 
However in real world , data may be from any other domains . 
For model inconsistency , we use Transformer to build our training data while the translation model usedWMT 16 WMT 17 BLEU TER BLEU TER MT 62.48 0.24 62.83 0.24 QuickEdit 67.14 0.19 69.22 0.18 Touch Editing 82.05 0.09 82.88 0.09 Table 4 : Results on post - editing dataset in terms of BLEU and TER . 
in real applications may be different . 
To investigate the performance facing the three challenges , we test our model on WMT EnglishGerman Automatic Post - Editing ( APE ) dataset in IT domain using data from WMT‚Äô16 ( Bojar et al . 
, 2016 ) and WMT‚Äô17 ( Ondrej et al . 
, 2017 ) . 
The test data consists of triplets like ( source , machine translation , human post - edit ) , in which the machine translation is generated with a PBSMT system . 
We use the algorithm of Section 3 to extract actions from machine translations and human post - edited sentences . 
With the actions and original machine translations , we use the model trained on WMT‚Äô14 English - German dataset in Section 4 to generate reÔ¨Åned translations . 
To make a comparison , we also evaluate QuickEdit with the same setting . 
Table 4 summarizes the results on post - editing dataset . 
It is clear to see that even with the three kinds of inconsistency , our model still gains signiÔ¨Åcant improvements of up to 20.05 BLEU than the raw machine translation system ( PBSMT ) . 
As for QuickEdit , the improvement on post - editing dataset ( about 4 - 7 BLEU ) is smaller than that on translation dataset ( about 11 BLEU ) . 
We conjecture that the stable improvement of our method is due to more Ô¨Çexible action types . 
With the detailed editing actions , the model is competent to correct various of errors in draft machine translations , and thus leads to the robustness and effectiveness of our method . 
4.5 Discussion on Real Scenarios So far , the experiments we conducted are based on simulated human feedbacks , in which the actions are extracted from initial machine translation results and their corresponding references to simulate human editing actions . 
Thus in our simulated setting , the references are used in inference phase to simulate human behavior , as in other interaction methods ( Denkowski et al . 
, 2014 ; Marie and Max , 2015 ; Grangier and Auli , 2018 ) . 
These experiments show that our method can signiÔ¨Åcantly7 improve the initial translation with similated actions . 
However , whether the actions are convenient to perform is a key point in real applications . 
To investigate the usability and applicable scenarios of our method , we implement a real mobile application on iPhone , in which the actions can be performed on multi - touch screens . 
For a given source sentence , the application provides an initial machine translation . 
The text area of translation can response to several gestures4 : Tap indicated a missing word should be inserted into the nearest space between two words ; Swipe on a word indicated that the word should be deleted ; LongPress a word means the word should be substituted with other word ; Pan can drag a word to another position . 
We conduct a free - use study with four participants , in which the participants are asked to translate 20 sentences randomly selected from LDC Chinese - English test set with ( 1 ) Touch Editing or ( 2 ) keyboard input after 5 minutes to get familiar with the application . 
We observe that the users with Touch Editing tends to correct an error for multiple times when the system can not predict a word they want , while the users with keyboard input tends to modify more content of initial translation and spend more time on choosing words . 
We then conduct an unstructured interview on the usability of our method . 
The result of the interview shows that Touch Editing is convenient and intuitive but lack of ability of generating Ô¨Ånal accurate translation . 
It can be treated as a light - weight proofreading method , and suitable for Pre - Post - Editing ( Marie and Max , 2015 ) . 
5 Related Work Post - editing is a pragmatic method that allows human translators to directly correct errors in draft machine translations ( Simard et al . 
, 2007 ) . 
Comparing to purely manual translation , it achieves higher productivity while maintaining the human translation quality ( Plitt and Masselot , 2010 ; Federico et al . 
, 2012 ) . 
Many notable works introduce different levels of human - machine interactions in post - editing . 
Barrachina et al . 
( 2009 ) propose a preÔ¨Åx - based interactive method which enable users to correct the Ô¨Årst translation error from left to right in each iteration . 
4These gestures are explicit and directly supported by Apple iOS devices : https://developer.apple.com/ documentation / uikit / uigesturerecognizerGreen et al . 
( 2014 ) implement a preÔ¨Åx - based interactive translation system and Huang et al . 
( 2015 ) adopt the preÔ¨Åx constrained translation candidates into a novel input method for translators . 
Peris et al . 
( 2017 ) further extend this idea to neural machine translation . 
The preÔ¨Åx - based protocol is inÔ¨Çexible since users have to follow the left - to - right order . 
To overcome the weakness of preÔ¨Åx - based approach , Gonz ¬¥ alez - Rubio et al . 
( 2016 ) ; Cheng et al . 
( 2016 ) introduce interaction methods that allow users to correct errors at arbitrary position in a machine hypothesis , while Weng et al . 
( 2019 ) also preventing repeat mistakes by memorizing revision actions . 
Hokamp and Liu ( 2017 ) propose grid beam search to incorporate lexical constraints like words and phrases provided by human translators and force the constraints to appear in hypothesis . 
Recently , some researchers resort to more Ô¨Çexible interactions , which only require mouse click or touch actions . 
For example , Marie and Max ( 2015 ) ; Domingo et al . 
( 2016 ) propose interactive translation methods which ask user to select correct or incorrect segments of a translation with mouse only . 
Similar to our work , Grangier and Auli ( 2018 ) propose a mouse based interactive method which allows users to simply mark the incorrect words in draft machine hypotheses and expect the system to generate reÔ¨Åned translations . 
Herbig et al . 
( 2019 , 2020 ) propose a multi - modal interface for post - editors which takes pen , touch , and speech modalities into consideration . 
The protocol that given an initial translation to generate a reÔ¨Åned translation , is also used in polishing mechanism in machine translation ( Xia et al . 
, 2017 ; Geng et al . 
, 2018 ) and automatic post - editing ( APE ) task ( Lagarda et al . 
, 2009 ; Pal et al . 
, 2016 ) . 
The idea of multi - source encoder is also widely used in the Ô¨Åeld of APE research ( Chatterjee et al . 
, 2018 , 2019 ) . 
In human - machine interaction scenarios , the human feedback is used as extra information in polishing process . 
6 Conclusion In this paper , we propose Touch Editing , a Ô¨Çexible and effective interaction approach which allows human translators to revise machine translation results via touch actions . 
The actions we introduce can be provided with gestures like tapping , panning , swiping or long pressing on touch screens to represent human editing intentions . 
We present a8 simulated action extraction method for constructing training data and a dual - encoder model to handle the actions to generate reÔ¨Åned translations . 
We prove the effectiveness of the proposed interaction approach and discuss the applicable scenarios with a free - use study . 
For future works , we plan to conduct large scale real world experiments to evaluate the productivity of different interactive machine translation methods . 
Abstract Multilingual pretrained language models ( such as multilingual BERT ) have achieved impressive results for cross - lingual transfer . 
However , due to the constant model capacity , multilingual pre - training usually lags behind the monolingual competitors . 
In this work , we present two approaches to improve zero - shot cross - lingual classiÔ¨Åcation , by transferring the knowledge from monolingual pretrained models to multilingual ones . 
Experimental results on two cross - lingual classiÔ¨Åcation benchmarks show that our methods outperform vanilla multilingual Ô¨Åne - tuning . 
1 Introduction Supervised text classiÔ¨Åcation heavily relies on manually annotated training data , while the data are usually only available in rich - resource languages , such as English . 
It requires great effort to make the resources available in other languages . 
Various methods have been proposed to build cross - lingual classiÔ¨Åcation models by exploiting machine translation systems ( Xu and Yang , 2017 ; Chen et al . 
, 2018 ; Conneau et al . 
, 2018 ) , and learning multilingual embeddings ( Conneau et al . 
, 2018 ; Yu et al . 
, 2018 ; Artetxe and Schwenk , 2019 ; Eisenschlos et al . 
, 2019 ) . 
Recently , multilingual pretrained language models have shown surprising cross - lingual effectiveness on a wide range of downstream tasks ( Devlin et al . 
, 2019 ; Conneau and Lample , 2019 ; Conneau et al . 
, 2020 ; Chi et al . 
, 2020a , b ) . 
Even without using any parallel corpora , the pretrained models can still perform zero - shot cross - lingual classiÔ¨Åcation ( Pires et al . 
, 2019 ; Wu and Dredze , 2019 ; Keung et al . 
, 2019 ) . 
That is , these models can be Ô¨Åne - tuned in a source language , and then directly evaluated in other target languages . 
Despite ‚àóContribution during internship at Microsoft Research.the effectiveness of cross - lingual transfer , the multilingual pretrained language models have their own drawbacks . 
Due to the constant number of model parameters , the model capacity of the richresource languages decreases if we adds languages for pre - training . 
The curse of multilinguality results in that the multilingual models usually perform worse than their monolingual competitors on downstream tasks ( Arivazhagan et al . 
, 2019 ; Conneau et al . 
, 2020 ) . 
The observations motivate us to leverage monolingual pretrained models to improve multilingual models for cross - lingual classiÔ¨Åcation . 
In this paper , we propose a multilingual Ô¨Ånetuning method ( M ONOX ) based on the teacherstudent framework , where a multilingual student model learns end task skills from a monolingual teacher . 
Intuitively , monolingual pretrained models are used to provide supervision of downstream tasks , while multilingual models are employed for knowledge transfer across languages . 
We conduct experiments on two widely used cross - lingual classiÔ¨Åcation datasets , where our methods outperform baseline models on zero - shot cross - lingual classiÔ¨Åcation . 
Moreover , we show that the monolingual teacher model can help the student multilingual model for both the source language and target languages , even though the student model is only trained in the source language . 
2 Background : Multilingual Fine - Tuning We use multilingual BERT ( Devlin et al . 
, 2019 ) for multilingual pretrained language models . 
The pretrained model uses the BERT - style Transformer ( Vaswani et al . 
, 2017 ) architecture , and follows the similar Ô¨Åne - tuning procedure as BERT for text classiÔ¨Åcation , which is illustrated in Figure 1(a ) . 
To be speciÔ¨Åc , the Ô¨Årst input token of the models is always a special classiÔ¨Åcation token12 Figure 1 : Illustration of multilingual LM Ô¨Åne - tuning . 
( a ) The original multilingual LM Ô¨Åne - tuning procedure for cross - lingual classiÔ¨Åcation . 
( b ) The Ô¨Åne - tuning procedure of our proposed M ONOX via knowledge distillation ( M ONOX - K D ) . 
Notice that M ONOX does not use any target language data during Ô¨Åne - tuning . 
[ CLS ] . 
During Ô¨Åne - tuning , the Ô¨Ånal hidden state of the special token is used as the sentence representation . 
In order to output predictions , an additional softmax classiÔ¨Åer is built on top of the sentence representation . 
Denoting Das the training data in the source language , the pretrained models are Ô¨Åne - tuned with standard cross - entropy loss : LCE(Œ∏;D ) = ‚àí/summationdisplay ( x , y)‚ààDlogp(y|x;Œ∏ ) whereŒ∏represents model parameters . 
Then the model is directly evaluated on other languages for cross - lingual classiÔ¨Åcation . 
3 Methods As shown in Figure 1(b ) , we Ô¨Årst Ô¨Åne - tune the monolingual pretrained model in the source language . 
Then we transfer task knowledge to the multilingual pretrained model by soft ( Section 3.1 ) or hard ( Section 3.2 ) labels . 
We describe two variants of our proposed method ( M ONOX ) as follows . 
3.1 Knowledge Distillation In order to transfer task - speciÔ¨Åc knowledge from monolingual model to multilingual model , we propose to use knowledge distillation ( Hinton et al . 
,2015 ) under our M ONOX framework , where a student modelsis trained with soft labels generated by a better - learned teacher model t. The loss function of the student model is : LKD(Œ∏s;D , Œ∏t ) = ‚àí/summationdisplay ( x , y)‚ààDK / summationdisplay k=1q(y = k|x;Œ∏t ) logp(y = k|x;Œ∏s ) wherep(¬∑)andq(¬∑)represent the probability distribution over Kcategories , predicted by the studentsand the teacher t , respectively . 
Notice that only the student model parameters Œ∏sare updated during knowledge distillation . 
As shown in Figure 1(b ) , we Ô¨Årst use the Ô¨Åne - tuned monolingual pretrained model as a teacher , which is learned by minimizing LCE(Œ∏t;D ) . 
Then we perform knowledge distillation for the student model withLKD(Œ∏s;DC , Œ∏t)as the loss function , where DCis the concatenation of training dataset and the unlabeled dataset in the source language . 
We denote this implementation as M ONOX - K D. 3.2 Pseudo - Label In addition to knowledge distillation , we also consider implementing M ONOX by training the student multilingual model with pseudo - label ( Lee , 2013 ) . 
SpeciÔ¨Åcally , after Ô¨Åne - tuning the monolingual pretrained model on the training data as teacher , we apply the teacher model on the unlabeled data in the source language to generate pseudo labels . 
Next , we Ô¨Ålter the pseudo labels by a prediction conÔ¨Ådence threshold , and only keep the examples with higher conÔ¨Ådence scores . 
Notice that the pseudo training data are assigned with hard labels . 
Finally , we concatenate the original training data and the pseudo data as the Ô¨Ånal training set for the student model . 
We denote this implementation as M ONOX - P L. 4 Experiments 4.1 Experimental Setup In the following experiments , we consider the zero - shot cross - lingual setting , where models are trained with English data and directly evaluated on all target languages . 
Datasets We conduct experiments on two widely used datasets for cross - lingual evaluation : ( 1 ) Cross - Lingual Sentiment ( CLS ) dataset ( Prettenhofer and Stein , 2010 ) , containing Amazon13 reviews in three domains and four languages ; ( 2 ) Cross - Lingual NLI ( XNLI ) dataset ( Conneau et al . 
, 2018 ) , containing development and test sets in 15 languages and a training set in English for the natural language inference task . 
Pretrained Language Models We use multilingual BERT BASE1for cross - lingual transfer . 
For monolingual pretrained language model , the English - version RoBERTa LARGE2is employed . 
All the pretrained models used in our experiments are cased models . 
Baselines We compare our methods ( M ONOXKD , and M ONOX - P L ) with the following models : ‚Ä¢MBERT : directly Ô¨Åne - tuning the multilingual BERT BASE with English training data . 
‚Ä¢MBERT - ST : Ô¨Åne - tuning the multilingual BERT BASE by self - training , i.e. , alternately Ô¨Åne - tuning mBERT and updating the training data by labeling English unlabeled examples . 
4.2 ConÔ¨Åguration For the CLS dataset , we randomly select 20 % examples from training data as the development set and use the remaining examples as the training set . 
For XNLI , we randomly sample 20 % examples from training data as the training set , and regard the other examples as the unlabeled set . 
We use the vocabularies provided by the pretrained models , which are extracted by Byte - Pair Encoding ( Sennrich et al . 
, 2016 ) . 
The input sentences are truncated to 256 tokens . 
For both datasets , we use Adam optimizer with a learning rate of 5√ó10‚àí6 , and a batch size of 8 . 
We train models with epoch size of 200 and 2,500 steps for CLS and XNLI , respectively . 
For M ONOX - K D , the softmax temperature of knowledge distillation is set to 0.1 . 
For M ONOX - P L , the conÔ¨Ådence threshold is set to zero , which means all of the generated pseudo labels are used as training data . 
4.3 Results and Discussion Preliminary Experiments To see how much monolingual pretrained models is better than multilingual pretrained models , we Ô¨Ånetune several different pretrained language models on the two datasets under the aforementioned conÔ¨Åguration , 1https://github.com/google-research/ bert / blob / master / multilingual.md 2https://github.com/pytorch/fairseq/ tree / master / examples / robertaParameters CLS XNLI Multilingual Pretrained Models MBERT 110 M 86.37 77.07 Monolingual Pretrained Models BERT BASE 110 M 90.10 80.46 RoBERTa BASE 125 M 93.82 85.09 RoBERTa LARGE 355 M 95.77 89.24 Table 1 : Preliminary experiments results . 
Models are Ô¨Ånetuned with English training data of CLS and XNLI under the conÔ¨Åguration ( see Section 3.2 ) , and only evaluated in English . 
The results on CLS are averaged over three domains . 
and only evaluate them in English . 
As shown in Table 1 , the gap between multilingual and monolingual pretrained models is large , even when using the same size of parameters . 
It is not hard to explain because MBERT is trained in 104 languages , where different languages tend to confuse each other . 
Sentiment ClassiÔ¨Åcation We evaluate our method on the zero - shot cross - lingual sentiment classiÔ¨Åcation task . 
The goal of sentiment classiÔ¨Åcation is to classify input sentences to positive or negative sentiments . 
In Table 2 we compare the results of our methods with baselines on CLS . 
It can be observed that our M ONOX method outperforms baselines in all evaluated languages and domains , providing 4.91 % improvement of averaged accuracy to the original multilingual BERT Ô¨Åne - tuning method . 
Notice that MBERTSTis trained under the same condition with our method , i.e. , using the same labeled and unlabeled data as ours . 
However , we only observe a slight improvement over MBERT , which demonstrates that the performance improvement of M ONOX mainly beneÔ¨Åts from its end task knowledge transfer rather than the unlabeled data . 
Natural Language Inference We also evaluate our method on the zero - shot cross - lingual NLI task , which is more challenging than sentiment classiÔ¨Åcation . 
The goal of NLI is to identify the relationship of a pair of input sentences , including a premise and a hypothesis with an entailment , contradiction , orneutral relationship between them . 
As shown in Table 3 , we present the evaluation results on XNLI . 
Unsurprisingly , both M ONOXPLand M ONOX - K Dperform better than baseline methods , showing that our method success-14 en de fr ja Books DVD Music Books DVD Music Books DVD Music Books DVD Music avg MBERT 87.75 86.60 84.75 79.55 75.90 77.05 81.45 80.35 80.35 75.15 76.90 75.90 80.14 MBERT - ST 88.20 85.50 88.00 79.65 76.70 80.00 84.85 83.25 80.55 74.60 75.80 76.90 81.17 MONOX - P L94.00 92.75 91.80 83.20 79.25 82.95 86.00 84.95 84.55 78.85 80.00 79.35 84.80 MONOX - K D93.90 91.40 92.25 84.20 81.50 83.65 85.40 85.90 83.95 78.95 79.15 80.30 85.05 Table 2 : Evaluation results of zero - shot cross - lingual sentiment classiÔ¨Åcation on the CLS dataset . 
ar bg de el en es fr hi ru sw th tr ur vi zh avg MBERT 61.2 67.4 65.8 61.6 77.1 70.7 68.6 53.4 67.0 50.6 44.6 56.3 57.8 43.6 67.8 60.9 MBERT - ST 60.9 67.6 65.4 61.0 77.6 70.4 68.9 53.1 65.9 50.6 41.8 55.2 56.8 43.6 67.9 60.5 MONOX - P L63.5 70.1 69.8 61.7 80.9 74.1 72.1 52.5 68.4 51.2 42.3 57.9 58.0 44.0 70.2 62.5 MONOX - K D62.2 69.3 69.3 62.1 79.6 72.9 72.0 52.8 68.6 52.3 41.7 57.9 58.5 45.9 70.8 62.4 Table 3 : Evaluation results of zero - shot cross - lingual NLI on the XNLI dataset . 
Note that 20 % of the original training data are used as training set , and the other 80 % are used as unlabeled set . 
101102103104105 Training Data Size354045505560Averaged Accuracy mBert MonoX - PL MonoX - KD Figure 2 : Averaged accuracy scores on zero - shot XNLI with different training data sizes . 
( 20 % and 80 % of the training data are regraded training and unlabeled set . 
) fully helps the multilingual pretrained model gain end task knowledge from the monolingual pretrained model for cross - lingual classiÔ¨Åcation . 
It is also worth mentioning that the performance of MBERT - STis similar to MBERT . 
We believe the reason is that XNLI has more training data than CLS , which wakens the impact of self - training . 
Effects of Training Data Size We conduct a study on how much multilingual pretrained model can learn from monolingual pretrained model for different training data size . 
We cut the training data to 10 , 100 , 1 K , 10 K and 78 K ( full training data in our setting ) examples , and keep other hyper - parameters Ô¨Åxed . 
In Figure 2 , we show the averaged accuracy scores for zero - shot XNLI with different training data sizes . 
We observe that MONOX outperforms MBERTon all data sizes except the 10 - example setting . 
When the training data is relatively small ( ‚â§104 ) , our method shows 103 102 101 100101102 Distillation Temperature61.562.062.5Averaged Accuracy Figure 3 : Averaged accuracy scores on the development set for zero - shot XNLI with different softmax temperatures of M ONOX - K D. a great improvement . 
Effects of Distillation Temperature Figure 3 presents XNLI averaged accuracy scores of MONOX - K Dwith different softmax temperatures in knowledge distillation . 
Even though the temperature varies from 10‚àí3to102 , all of the results are higher than baseline scores , which indicates MONOX - K Dis nonsensitive to the temperature . 
When the temperature is set to 10‚àí1 , we observe the best results on the development set . 
Therefore we set temperature as 0.1 in other experiments . 
5 Conclusion In this work , we investigated whether a monolingual pretrained model can help cross - lingual classiÔ¨Åcation . 
Our results have shown that , with a RoBERTa model pretrained in English , we can boost the classiÔ¨Åcation performance of a pretrained multilingual BERT in other languages . 
For future work , we will explore whether mono-15 lingual pretrained models can help other crosslingual NLP tasks , such as natural language generation ( Chi et al . 
, 2020a ) . 
Acknowledgements Prof. Heyan Huang is the corresponding author . 
The work is supported by National Key R&D Plan ( No . 
2016QY03D0602 ) , NSFC ( No . 
U19B2020 , 61772076 , 61751201 and 61602197 ) and NSFB ( No . 
Z181100008918002 ) . 
Abstract Aspect - level sentiment analysis(ASC ) predictseach speciÔ¨Åc aspect term ‚Äôs sentiment polar - ity in a given text or review . 
Recent stud - ies used attention - based methods that can ef - fectively improve the performance of aspect - level sentiment analysis . 
These methods ig - nored the syntactic relationship between the as - pect and its corresponding context words , lead - ing the model to focus on syntactically unre - lated words mistakenly . 
One proposed solu - tion , the graph convolutional network ( GCN),cannot completely avoid the problem . 
While itdoes incorporate useful information about syn - tax , it assigns equal weight to all the edgesbetween connected words . 
It may still incor - rectly associate unrelated words to the targetaspect through the iterations of graph convo - lutional propagation . 
In this study , a graphattention network with memory fusion is pro - posed to extend GCN ‚Äôs idea by assigning dif - ferent weights to edges . 
Syntactic constraintscan be imposed to block the graph convolu - tional propagation of unrelated words . 
A con - volutional layer and a memory fusion were ap - plied to learn and exploit multiword relationsand draw different weights of words to im - prove performance further . 
Experimental re - sults on Ô¨Åve datasets show that the proposedmethod yields better performance than exist - ing methods . 
The code of this paper is avail - abled at https://github.com/YuanLi95/GATT-For-Aspect . 
1 Introduction Aspect - level sentiment classiÔ¨Åcation is a Ô¨Ånegrained subtask in sentiment analysis ( Wang et al . 
, 2019;Peng et al . 
,2020 ) . 
Given a sentence andan aspect that appears in the sentence , ASC aimsto determine the sentiment polarity of that aspect(e.g . 
, negative , neutral , or positive ) . 
For example , a review of a restaurant ‚Äú The price is reasonable although the service is poor . 
‚Äù expresses apositive sentiment for thepriceaspect , but also conveys anegativesentiment for theserviceaspect , as shown in Figure1 . 
Such a technique is widely used toanalyze online posts reviews , mainly from Ama - zon reviews or Twitter , to help raise the ability to understand consumer needs or experiences with a product , guiding a manufacturer towards productimprovement . 
Aspect - level sentiment classiÔ¨Åcation is much more complicated than sentence - level sentiment classiÔ¨Åcation . 
ASC task is necessary to identify the parts of the sentence that describe the correspondence between multiple aspects . 
Traditional methods mostly use shallow machine learning models with hand - crafted features to build sentiment classiÔ¨Åers for the ASC task ( Jiang et al . 
,2011;Wagner et al . 
,2014).However , the process for manual feature engineering is time - consuming and labor - intensive as well as limited in classiÔ¨Åcation performance Recently , with the development of deep learning techniques , various attention - based neural models have achieved remarkable success in ASC . 
( Wang et al . 
,2016;Ma et al . 
,2017;Chen et al . 
,2017;Gu et al . 
,2018;Tang et al . 
,2019 ) . 
However , these methods ignored the syntactic dependence betweencontext words and aspects in a sentence . 
As a result , the current attention model may inappropriately focus on syntactically unrelated context words . 
As shown in Figure1 , when predicting the emotional polarity ofprice , the attention mechanism may focus on the wordpoor , which is not related to its syntax . 
To address this issue , Zhang et al.(2019 ) built a graph convolutional network ( GCN ) over a dependency tree to exploit syntactical information andword dependencies . 
However , the model assigns equal weight to the edges connected between words so that words may mistakenly associate syntactically unrelated words to the target aspect through iterations of graph convolutional propagation . 
As indicated in Figure1 , after three iterations , bothreasonable(yellow lines ) andpoor(red lines ) may be27 Theprice is resonable although ' ( 71281 $ 8 ; $ ' - 6&21- ' ( 7 1281the serviceispoorDFRPSGHW QVXEM DFRPSDGYFO $ 8 ; $ ' Figure 1 : Grammatical Relational Examples . 
identiÔ¨Åed as descriptors of the aspectprice , which is incorrect . 
As a result , the model will falsely classify the aspectpriceas a negative sentiment . 
In this paper , a graph attention model with memory fusion was proposed . 
This model extends theidea of graph convolutional networks in two as - pects . 
First , the graph attention mechanism is applied to assign different weights to the edge , so the syntactical constraints can be imposed to block thepropagation of syntactically unrelated words to the target aspect . 
Second , a convolutional operation is applied to extract local information to exploit multiword relations , such asnot goodandfar from perfect , which can further improve the performance . 
To integrate all features , a memory fusion layer , which is similar to a memory network , is appliedto draw different weights for words according to their contribution to the Ô¨Ånal classiÔ¨Åcation . 
Experiments are conducted on Ô¨Åve datasets demonstrate how the proposed model outperforms baselines for aspect - level sentiment analysis . 
The remainder of this paper is organized asfollows . 
Section 2 brieÔ¨Çy reviews the existing works for aspect - level sentiment analysis . 
Section 3 presents a detailed description of the proposedgraph attention model with memory fusion . 
Section 4 summarizes the implementation details and experimental results . 
The conclusions of this study are Ô¨Ånally drawn in Section 5 . 
2 Related Works Aspect - level sentiment classiÔ¨Åcation is an impor - tant branch of sentiment classiÔ¨Åcation , aiming toidentify the sentiment polarity of an aspect targetin a sentence . 
ASC methods can be divided into traditional and deep learning methods . 
Traditional methods usually used feature - based machine learning algorithms , such as a feature - based supportvector machine ( SVM ) ( Kiritchenko et al . 
,2014).Due to the inefÔ¨Åciency of manually constructed features , several neural network methods have been proposed for aspect - level sentiment analysis ( Jiang et al . 
,2011 ) , which are mainly based on long shortterm memory ( LSTM ) ( Tang et al . 
,2016a;Wanget al . 
,2020).Tang et al.(2016b ) indicated thatthe ASC task ‚Äôs challenge is to identify better thesemantic correlation between context words andaspect words so that several recent works widely applied an attention mechanism and achieved good performance . 
Ma et al.(2017 ) used an interac - tive attention network to obtain a two - way atten - tion representation of context words and aspect words . 
Huang et al.(2018 ) proposed a joint model based on an attention mechanism to model aspects and sentences . 
Tang et al.(2019 ) proposed a self - supervised attention model that can dynamically update attention weights . 
Yao et al.(2019 ) introduced the graph convo - lutional network into the sentiment classiÔ¨Åcation task and achieved good performance . 
Subsequently , Zhang et al.(2019 ) proposed to use GCN on the dependency tree of a sentence to exploit the longrange syntactic information for the ASC task . 
3 Graph Attention Network withMemory Fusion The proposed graph attention network with memory fusion is mainly composed of the followingfour parts : a context encoder , a graph attentionlayer , a convolutional layer and a memory fusionlayer , as shown in Figure2.The context encoderemploys a vanilla bidirectional LSTM to capture the textual features . 
It contains a word embedding layer and a BiLSTM layer to produce a hidden representation of the text . 
Taking the hidden representation as input , the graph attention layer ( G - ATT)28 ƒÇ Word EmbeddingsƒÇ Context Encoder ƒÇ copyƒÇ copyƒÇ L - Layer G - ATTƒÇ Aspect- specific   Masking ƒÇ ƒÇ   2 - Layer ConvMemory FusionConcatenateSoftmax Aspect - specific Masking Attention Vector Hidden Representation Hidden Representation Figure 2 : The overall architecture of the proposed graph attention network with memory fusion . 
is trained on the dependency tree to mine explicit structural information between words . 
The convolutional layer was used to extract the local informa - tion around the sentiment word , which can dynami - cally deal with non - single word aspects such as not goodandfar from perfect , instead of only takingthe average of its vectors . 
To merge all features , we adopt a memory fusion layer similar to a memory network ( Tang et al . 
,2016b ) , which can assign different weights to the context words according to their contribution to the Ô¨Ånal classiÔ¨Åcation . 
The detailed description is presented as follows.3.1 Context Encoder Given a sentence x=[x 1,x2,¬∑¬∑¬∑,xœÑ+1,¬∑¬∑¬∑,xœÑ+m , ¬∑ ¬∑ ¬∑ xn]containingnwords , the target aspect starts from the ( œÑ+1 ) -th word with a length ofm . 
ABiLSTM was applied as context encoder , which can capture long - distance dependencies within the sentence . 
We average the hidden representation ofboth the forward direction and backward direction to obtain the contextual representation , deÔ¨Åned as,(/vectorh Ei,/vectorcEi)=LSTM(x i,/vectorhEi‚àí1,/vectorcEi‚àí1)(1 ) ( ‚Üê hEi,‚ÜêcEi)=LSTM(x i,‚Üê hEi+1,‚ÜêcEi+1)(2)h i=(‚àí‚Üíhi‚äï‚Üê‚àíhi)/2(3 ) where‚äïis an element - wise addition operator;‚àí‚Üíh i‚ààRdh,‚Üê‚àíh i‚ààRdhandhi‚ààRdharethe forward , backward and output representa - tion , respectively ; anddhis the dimension ofhidden state . 
Thus , the Ô¨Ånal representation ofthe context encoder can be denoted as HE=[h E1,hE2,¬∑¬∑¬∑,hEœÑ+1,¬∑¬∑¬∑,hEœÑ+m,¬∑¬∑¬∑,hEœÑ+m].3.2 Graph Attention Layer The graph attention ( G - ATT ) layer learns syntac - tically relevant words to the target aspect on the dependency tree1 , which is widely used in several NLP tasks to effectively identify the relationships and roles of words . 
After parsing the given sen - tence as a dependency tree , the adjacency matrixwas built from the tree topology . 
It is worth not - ing that the dependency tree is a directed graph . 
Therefore , the graph attention mechanism was applied with consideration of the direction , but themechanism could be adapted to the undirection - aware scenario . 
Therefore , we propose a variant on dependency graphs that are undirectional . 
The obtained hidden state HE‚ààRn√ódhwas fed into a stacked G - ATT model , which was performed in a multilayer fashion with anLgraph attention layer . 
In practice , the representation in the l - the layer was not immediately fed into the G - ATT layer . 
To enhance the relevance of the context words to the corresponding aspect , we adopted a position weight function to the representation of word iin layerl , 1We use spaCy toolkit : https://spacy.io/.29 which is widely used in previous works ( Li et al . 
, 2018;Zhang et al . 
,2019 ) , deÔ¨Åned as , q i=‚éß‚é®‚é©1‚àíœÑ+1‚àíi n1‚â§i < œÑ+10œÑ+1‚â§i‚â§œÑ+m1‚àí i‚àíœÑ‚àím nœÑ+m < i‚â§n(4)ÀÜh li = qihli(5)whereq i‚ààRis the position weight to wordi . 
In each layer , an attention coefÔ¨Åcient Œ±lijwas applied to measure the importance between word iand wordj , deÔ¨Åned as , Œ±li , j = exp / parenleftBigLeakyReLU(aT[WlŒ±ÀÜhli||WlŒ±ÀÜhlj])/parenrightBig /summationtext k‚ààN iexp / parenleftBigLeakyReLU(aT[WlŒ±ÀÜhli||WlŒ±ÀÜhlj])/parenrightBig(6 ) whereNiis the set of the neighbor of wordiand WlŒ±‚ààRdh√ódhis a shared weight matrix applied toperform linear transformation to each word in order to obtain sufÔ¨Åcient express ability of high - levelrepresentation . 
||is the concatenation operator , a‚ààR2dhis a weight vector , and the leaky rectiÔ¨Åed linear unit ( LeakyReLU ) is the non - linearity . 
To stabilize the learning process of the graph ‚Äôs attention , we implement Kdifferent attention with the same parameter settings , which is similar tothe multi - head attention mechanism proposed by Vaswani et al.(2017 ) . 
Thus , the Ô¨Ånal representation hl+1iof wordiin layerl+1 can be obtained as , h l+1i= ReLU(1KK / summationdisplay k=1 / summationdisplay j‚ààNiŒ±l , ki , jWlkÀÜhlj)(7 ) whereŒ±l , ki , jis thek - th attention coefÔ¨Åcientscomputed by Eq . 
( 6 ) , Wlkis the correspondingweight matrix ofk - th attention in l - th GAT layer , and the nonlinear function is ReLU . 
The Ô¨Ånalrepresentation of the L - layer G - ATT is denotedas HL=[hL1,hL2,¬∑¬∑¬∑,hLœÑ+1¬∑¬∑¬∑,hLœÑ+m,¬∑¬∑¬∑,hLn ] , hLi‚ààRdh.3.3 Convolutional Layer The convolutional layer ( Conv ) was applied to extract local n - gram information which are composed of multiple sentiment words ( e.g , not goodand far from perfect ) , in order to improve the learning ability of then - gram features . 
The hidden repre - sentation of context encoder HEis fed into two convolutional layers . 
In each layer , we use Fconvolution Ô¨Ålters to learn local n - gram features . 
In awindow of œâwordshi : i+œâ‚àí1 , the Ô¨Ålter f - th generates the feature mapcfias follows , c fi= ReLU(Wf ‚ó¶ hEi : i+œâ‚àí1+bf)(8 ) where ‚ó¶ is a convolutional operator , Wf‚ààRœâ√ódh andbf‚ààRdhrespectively denote the weight ma - trix and bias , œâis the length of the Ô¨Ålter , and thenon - linearity is ReLU . 
By concatenating all fea - ture maps , the representation for word iwill be hci=[c1i , c2i,¬∑¬∑¬∑,cfi,¬∑¬∑¬∑,cFi ] . 
To ensure that the shape of the output is consistent with the shape of the input in the convolutional layer , we set Fto dhand pad each sentence with zero vectors to the maximum input length in the corpora . 
Then , we send the feature maps to the second convolutional layer , which has a similar structure , to obtain theÔ¨Ånal representation of convolutional layer HC=[h C1,hC2,¬∑¬∑¬∑,hCœÑ+1,¬∑¬∑¬∑,hCœÑ+m,¬∑¬∑¬∑hCn],hCi‚ààdh . 
3.4 Aspect - SpeciÔ¨Åc Masking The aspect - speciÔ¨Åc masking layer aims to learn aspect - speciÔ¨Åc content for memory fusion and the Ô¨Ånal classiÔ¨Åcation . 
Therefore , we mask out thehidden state vectors of the input from the G - ATTand Conv layer , i.e. , HLandHC . 
Formally , we set all the vectors of non - aspect words to zero and leave the vectors of the aspect words unchanged , deÔ¨Åned as , h i=/braceleftbigg01‚â§i < œÑ+1,œÑ+m < i‚â§nh iœÑ+1‚â§i‚â§œÑ+m(9 ) The output vector of the G - ATT layer af - ter the mask operation is HLmasked =[ 0,¬∑¬∑¬∑,h LœÑ+1,¬∑¬∑¬∑,hLœÑ+m,¬∑¬∑¬∑,0 ] , which hasperceived contexts around the aspect so bothsyntactical dependencies and the long - rangemultiword relations can be considered . 
Sim - ilarly , the output representation of the con - volutional layer after the mask operation is HCmask= [ 0,¬∑¬∑¬∑,hCœÑ+1,¬∑¬∑¬∑,hCœÑ+m,¬∑¬∑¬∑,0].3.5 Memory Fusion Memory fusion aims to learn the Ô¨Ånal representa - tion related to the meaning of aspect words . 
Theidea is to retrieve signiÔ¨Åcant features that are se - mantically relevant to the aspect words from thehidden representation by aligning the vectors of both G - ATT and Conv to the hidden vectors . 
Formally , we calculate the attention score for the i - th word inHEandj - th word inHL , deÔ¨Åned as,30 Dataset Positive Neutral Negative Total Max Length Mean Length TwitterTrain 1561 3127 1560 6248 43 19 Test 173 346 173 692 39 19 Lap14Train 994 464 870 2328 81 21 Test 341 169 128 638 70 17 Rest14Train 2164 637 807 3608 77 18 Test 728 196 196 1120 68 17 Rest15Train 912 36 256 1204 72 15 Test 326 34 182 542 61 17 Rest16Train 1240 69 439 1748 72 16 Test 469 30 117 616 77 18 Table 1 : The summary of datasets ei = n / summationdisplay j=1hLiTWlhEj = œÑ+m / summationdisplay j = œÑ+1hLiTWlhEj(10 ) where Wl‚ààRdh√ódhis a bilinear term that inter - acts with these two vectors and captures the speciÔ¨Åc semantic relations . 
According toSocher et al . 
( 2013 ) , such a tensor operator can be used to model complicated compositions between those vectors . 
Therefore , the attention score weight and Ô¨Ånal representation of G - ATT are computed as , Œ± i = exp(ei)/summationtextnk=1exp(ek)(11)s g = n / summationdisplay i=1Œ±ihEi(12 ) Accordingly , the Ô¨Ånal representation of the Conv layer is computed as , r i = n / summationdisplay j=1hCiTWchEj = œÑ+m / summationdisplay j = œÑ+1hCiTWchEj(13)Œ≤ i = exp(ri)/summationtextnk=1exp(rk)(14)s c = n / summationdisplay i=1Œ≤ihEi(15 ) 3.6 Sentiment ClassiÔ¨Åcation After obtaining representation sgandsc , they are fed into a fully connected layer and then a softmax layer to generate a probability distribution over the classes,ÀÜy= softmax(W s[sg||sc]+bs)(16 ) where Wsandbsrespectively denote the weights and bias in the output layer . 
Thus , given a trainingset / braceleftbigx(t),y(t)/bracerightbigTt=1=1 , where x(t)is a training sample , y(t)is the corresponding actual sentiment label , and Tis the number of training samples inthe corpus . 
The training goal is to minimize the cross - entropyL cls(Œ∏)deÔ¨Åned as , Lcls(Œ∏)=‚àí1TT / summationdisplay t=1logp(ÀÜy(t)|x(t);Œ∏)+Œª / bardblŒ∏ / bardbl22(17 ) whereŒ∏denotes all trainable parameters . 
To avoid overÔ¨Åtting , an L2 - regularization Œª / bardblŒ∏ / bardbl22is also introduced to the loss function in the training phase , whereŒªis the decay factor . 
4 Experimental Results This section conducts comparative experiments on Ô¨Åve corpora against several previously proposedmethods for aspect - level sentiment analysis . 
The experimental setting and empirical results are then presented in detail.4.1 Dataset To compare the proposed model with other aspectlevel sentiment analysis models , we conduct ex - periments on the following Ô¨Åve commonly used datasets : Twitter was originally proposed byDong et al.(2014 ) and contains several Twitter posts , while the other four corpora ( Lap14 , Rest14 , Rest15 , Rest16 ) were respectively retrieved from SemEval 2014 task 4 ( Pontiki et al . 
,2014 ) , Se - mEval 2015 task 12 ( Pontiki et al . 
,2015 ) and Se - mEval 2016 Task 5 ( Pontiki et al . 
,2016 ) , whichinclude two types of data , i.e. , reviews of laptops and restaurants . 
The statistical descriptions of these corpora are shown in Table1 . 
We use accuracy and Macro - average F1 - score as evaluation metrics ; these are commonly used in ASC task ( Huang and Carley,2019;Zhang et al . 
,2019 ) . 
A higher accuracy orF1 - score indicates better prediction performance31 ModelTwitter Lap14 Rest14 Rest15 Rest16 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 LSTM 69.56 67.70 69.29 63.09 78.13 67.47 77.37 55.17 86.80 63.88 TD - LSTM 70.81 69.11 70.45 64.78 79.47 69.01 78.23 57.25 87.17 64.89 MemNet 71.48 69.90 70.64 65.17 79.61 69.64 77.31 58.28 85.44 65.99 IAN 72.50 70.81 72.05 67.38 79.26 70.09 78.54 52.65 84.74 55.21 RAM 69.36 67.30 74.4971.35 80.23 70.80 78.8561.97 88.92 68.23 AOA 72.30 70.20 72.62 67.52 79.97 70.42 78.17 57.02 87.50 66.21 TNet - LF 72.98 71.43 74.61 70.14 80.42 71.03 78.47 59.47 89.07 70.43 ASGCN 72.15 70.40 75.5571.05 80.77 72.02 79.8961.89 88.99 67.48 G - ATT - U 73.6072.12 76.18 72.23 81.59 72.65 81.18 64.07 89.0671.97 G - ATT - D 73.8971.82 75.75 71.52 80.89 71.68 80.93 64.03 88.8172.36 Table 2 : Model comparison results ( % ) . 
In the case of random initialization , the average accuracy of the 3 runsand the macroF 1 - score . 
The best results of its baseline model and our model are shown in bold . 
ModelTwitter Lap14 Rest14 Rest15 Rest16 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 ASGCN - DG 72.15 70.40 75.55 71.05 80.77 72.02 79.89 61.89 88.99 67.48 G - ATT - U 73.6072.12 76.18 72.23 81.59 72.65 81.18 64.07 89.06 71.97 G - ATT - U w/o Pos 73.74 72.00 75.13 71.26 81.82 73.91 80.07 62.42 88.6969.54 G - ATT - U w/o Mask 73.36 71.47 75.2470.70 80.15 70.49 79.8962.78 88.5370.34 G - ATT - U w/o GAT 73.03 71.04 74.56 71.23 80.21 71.16 80.38 61.31 87.66 68.27 G - ATT - U w/o Conv 73.23 71.22 74.8271.35 80.86 71.77 80.5462.02 87.39 69.22 Table 3 : Ablation study results ( % ) . 
Accuracy and macro F1 - scores are the average value over 3 runs with randominitialization . 
4.2 Implementation Details To comprehensively evaluate the proposed model , we selected the following baseline methods , which are introduced as follows:‚Ä¢LSTM ( Tang et al . 
,2016a ) uses the standard LSTM model to send the state of the last layer to thesoftmaxlayer to obtain the output of sentiment probability.‚Ä¢TD - LSTM ( Tang et al . 
,2016a ) connects as - pect word embedding and context word em - bedding to obtain the Ô¨Ånal word embedding representation , and the two sides of the aspect word are respectively modeled by LSTM to obtain the hidden layer representation.‚Ä¢MemNet ( Tang et al . 
,2016b ) consists of a multilevel memory network , which effectively retains context and aspect information.‚Ä¢IAN ( Ma et al . 
,2017 ) exchanges information between context and aspect as an interactive attention model.‚Ä¢RAM ( Chen et al . 
,2017 ) learns sentencerepresentation by layers consisting of an attention - based aggregation of word features and a GRU cell with multilayer architecture.‚Ä¢AOA(Huang et al . 
,2018 ) captures the inter - action between context and aspect words by jointly modeling aspects and sentences.‚Ä¢TNet - LFT ( Li et al . 
,2018 ) increases the retention of context information through a context retention conversion mechanism.‚Ä¢ASGCN ( Zhang et al . 
,2019 ) uses externalgrammatical information through the graph convolution neural network , while aspect obtains syntax - related context information.‚Ä¢G - ATT uses either undirectional ( G - ATT - U ) or directional ( G - ATT - D ) graphs to represent the parsed tree - structure as the proposed model . 
For all the models , the 300 - dimensional GloVe vector ( Pennington et al . 
,2014 ) pretrained on 840B Common Crawl was used as the initial word embedding . 
Words that do not appear in GloVe were initialized with a uniform distribution of U(-0.25 , 0.25 ) . 
The hidden layer vectors ‚Äô dimensions are all 300 , and all model weights are initialized with the Xaviernormalization ( Glorot and Bengio,2010 ) . 
RMSprop was used as the optimizer with a learning rate of 0.001 to train all the models . 
We set the L2regularization decay factor to 1e-4 and the batch size to 40 . 
The negative input slope of LeakyReLUin the G - ATT layer is set to 0.2 . 
All aforementioned32 Aspect Model Attention Visualization Prediction Label OSASGCN neutral positive Convpositive positive G - ATT CajunshrimpASGCN negative positive Convpositive positive G - ATT PlaceASGCN neutral negative Convnegative negative G - ATT Table 4 : Visualization of the proposed model . 
hyperparameters are selected using a grid - search strategy . 
The epoch was set depending on an early stop strategy . 
The training processing stops after Ô¨Åve epochs if there is no improvement . 
The experimental results are obtained by averaging the results of three random initialization runs.4.3 Comparative Results Table2shows the comparative results of G - ATT - D and G - ATT - U against several baselines . 
As indicated , G - ATT - U outperformed all baseline models by usingF1 - score as a criterion . 
In terms of accuracy , except results slightly lower than the TNet - LF model on Rest16 , both G - ATT - D and G - ATT - Uachieved better performance . 
The rational reasonis that the proposed model can capture both syntactic and local information , thus improving performance . 
In addition , the improvement of the F1 - score of the proposed model on Rest15 and Rest16 is huge compared to the baselines , which is 2.1 % and 1.5%,respectively . 
The possible reason is that the syntactical structure of the texts in Rest15 and Rest16 is more complicated than those in Twitter , Lap14 and Rest14 . 
The performance of directional ver - sion ( G - ATT - D ) is slightly higher than the undi - rectional version ( G - ATT - U ) on Twitter , Rest15 and Rest16 , while performance is slightly lower on Lap14 andRest14 , indicating an undirectional syntax relationship that is more appropriate on those datasets.4.4 Ablation Experiment Table3shows the ablation experiments to investigate further how the models can beneÔ¨Åt from each component . 
As indicated , removing the position weight ( i.e. , G - ATT - U w/o Pos ) causes the performance on Lap14 , Rest15 and Rest16 to decrease . 
However , the performance of G - ATT - U w/o Posincreases F1 - score by 1.26 % when used on Twit- ter and Rest14 since the local information is lessimportant than syntactic . 
Removing the mask op-33 ( a)Twitter ( b)Lap14 ( c)Rest14 ( d)Rest15 ( e)Rest16 Figure 3 : Effect of the number of G - ATT Layers eration ( i.e. , G - ATT - U w/o mask ) reduces the performance , which shows that the mask operation prevents the noise word from entering the Ô¨Ånal representation . 
Further , Twitter , Lap14 , and Rest14 are less syntactical , so the integration of position weight does not beneÔ¨Åt or can even negatively beneÔ¨Åt the results . 
Besides , it is observed that G - ATT - U w/o Conv is generally better than G - ATT - U w/o G - ATT , which shows that the GAT layer beneÔ¨Åts for the model are greater than the Conv layer , indicating that the contextual syntax - related information is more important than local information . 
Compared withASGCN - DG , the proposed G - ATT - U w/o Conv achieved better performance , especially on Twitter and Rest16 , withF1 - score improvements of 0.64 % and 1.74 % , respectively . 
This result shows that GATT - U w/o Conv outperformed the ASGCN modelin most cases , indicating that graph attention layers with different edge weights are more effective than graph convolution layers with equal edge weights.4.5 Visualization Memory fusion can capture both syntax - relatedand local information with the attention mechanism . 
For visualization , we selected three examples from Lap14 and Rest16 that are signiÔ¨Åcantly improved by the proposed G - ATT model against the ASGCN - DG model . 
We conducted a visualization experiment using a heat map to show the attention score offered by parameters Œ±andŒ≤in Eq.(11 ) and Eq.(14 ) , respectively , as shown in Table4 . 
Thecolor density is the attention score of each token . 
Adeeper color indicates that more weight is assigned to the token according to its contribution to the Ô¨Ånal classiÔ¨Åcation . 
As indicated , ASGCN allows the syntactically unrelated words to be associated with the target aspect by assigning equal weight to the edge , such asgreatfor OS , goodforCajun shrimp andnot invitingforplace . 
Conversely , G - ATT - U tends to block graph convolution propagation from unrelated words to the target aspect by assignedattention weights to the edges . 
The convolution operation can also exploit some explicit structure , such asnot greatandnot inviting . 
Such phrases are expressive and task - speciÔ¨Åc , thus improve performance.4.6 Number of GAT layers Since G - ATT involves Llayers of graph attention , we investigate whether the number of layers can determine the proposed model ‚Äôs performance . 
As indicated , the best performance can be achieved whenLi s2o n Twitter , 7o n Lap14 , 3o n Rest14 a n d6o n Rest15 and Rest16 . 
WhenLis greaterthan 7 , a decreasing trend in both metrics is pre - sented . 
As Lreaches 10 , the model contains toomany parameters and becomes more difÔ¨Åcult to train . 
5 Conclusions In this study , a graph attention network with memory fusion is proposed for aspect - level sentiment analysis . 
A graph attention layer was implemented34 to capture a context word ‚Äôs syntactic relationshipto the target aspect by learning different weights for edges to block the propagation from unrelated words . 
Moreover , a convolutional layer and a memory fusion were used to learn the local information and draw different weights for context words . 
Experimental results show that the G - ATT model yields better performance than the existing methods for aspect - based sentiment analysis . 
Besides , ablation studies and case studies are provided to prove the effectiveness of the proposed model further . 
Future works will improve the graph attention layerand dynamic to learn the attention score , so the proposed model can better integrate syntax - related context information . 
Acknowledgments This work was supported by the National Natural Science Foundation of China ( NSFC ) under Grant No . 
61966038 , 61702443 and 61762091 , and inpart by the Ministry of Science and Technology , Taiwan , ROC , under Grant No . 
MOST107 - 2628 - E-155 - 002 - MY3 . 
The authors would like to thank the anonymous reviewers for their constructive comments . 
Abstract Unlike non - conversation scenes , emotion recognition in dialogues ( ERD ) poses more complicated challenges due to its interactive nature and intricate contextual information . 
All present methods model historical utterances without considering the content of the target utterance . 
However , different parts of a historical utterance may contribute differently to emotion inference of different target utterances . 
Therefore we propose Fine - grained Extraction and Reasoning Network ( FERNet ) to generate target - speciÔ¨Åc historical utterance representations . 
The reasoning module effectively handles both local and global sequential dependencies to reason over context , and updates target utterance representations to more informed vectors . 
Experiments on two benchmarks show that our method achieves competitive performance compared with previous methods . 
1 Introduction With the development of human - machine interaction ( HMI ) applications , textual dialogue scenes appear more frequently . 
These scenes request effective and high - performance emotion recognition systems helping in building empathetic machines ( Young et al . 
, 2018 ) . 
Therefore , emotion recognition in dialogues ( ERD ) is getting growing attention from both academic and business community . 
Different from non - conversation scenes , the ERD task poses a more complicated challenge of modeling context - sensitive dependencies . 
Most of existing approaches adopt Convolution Neural Network ( CNN ) ( Krizhevsky et al . 
, 2012 ) , followed by a max - pooling layer to obtain utterance representations ( Kim , 2014 ; Torres , 2018 ; Hazarika et al . 
, 2018a , b ; Majumder et al . 
, 2019 ; Ghosal et al . 
, 2019 ) . 
The process proceeds without the guidance of the target utterance , thus generated historicalutterance representations are indistinguishable toward different target utterances . 
Emotion recognition may fail in cases where historical utterances express various emotions toward various targets , which may confuse the emotion recognition of target utterances . 
As Figure 1 shows , for different target utterances B1andB2 , the model should attend the words ‚Äú good service ‚Äù and ‚Äú bad food ‚Äù in A1 , separately . 
In a word , it is desired to pay different attention to different words of a certain historical utterance to generate the target - speciÔ¨Åc historical utterance representation . 
Therestaurantwherewehaddinnerprovided[goodservice]but[badfood ] . 
üòê Whataboutthefood?Ican‚Äôtagreewithyoumore . 
üòê üòû üòÑ Yeah , Iwasimpressedbytheservice.ùê¥"ùê¥#ùêµ"ùêµ # Figure 1 : A dialogue shows that modeling intricate contextual information is crucial for emotion recognition . 
In this paper , we propose Fine - grained Extraction and Reasoning Network ( FERNet ) to generate target - speciÔ¨Åc historical utterance representations conditioned on the content of target utterances by using the multi - head attention mechanism ( Vaswani et al . 
, 2017 ) , extracting more Ô¨Åne - grained , relevant and contributing information for emotion recognition . 
Besides , we devise the reasoning module , which employs historical utterances as a sequence of triggers , and updates the representation of the target utterance to a more informed vector as it observes historical utterances through time . 
In the reasoning process , the module models both short - term and long - term sequential dependencies effectively . 
We demonstrate the effectiveness of our method on two benchmarks . 
Experimental results show that our method achieves competitive performance compared with previous methods.37 2 Related Work Primitive approaches deal with the ERD task as simple solely - sentence emotion recognition task with no consideration of the historical information ( Joulin et al . 
, 2016 ; Chen et al . 
, 2016 ; Yang et al . 
, 2016 ; Chatterjee et al . 
, 2019 ) . 
To exploit contextual information , Poria et al . 
( 2017 ) ; Huang et al . 
( 2019 ) ; Jiao et al . 
( 2019 ) ; Hazarika et al . 
( 2018a , b ) ; Torres ( 2018 ) use RNN architecture , Hazarika et al . 
( 2018b , a ) use conversational memory networks ( Sukhbaatar et al . 
, 2015 ) , Torres ( 2018 ) ; Jiao et al . 
( 2019 ) use attention mechanism and Ghosal et al . 
( 2019 ) uses graph neural network . 
Besides , Majumder et al . 
( 2019 ) ; Hazarika et al . 
( 2018a ) propose to keep track of states of individual speakers throughout the dialogue and Ghosal et al . 
( 2019 ) incorporates speaker information into edge types . 
Some of these works consider the context following the target utterance such as Luo et al . 
( 2018 ) ; Saxena et al . 
( 2018 ) ; Ghosal et al . 
( 2019 ) and some variants of Majumder et al . 
( 2019 ) . 
However , this condition is quite incompatible with some practical situations like real - time dialogue systems in which we possess no future utterances while handling the target utterance . 
So in our paper , we only focus on the setting that only historical utterances can be utilized . 
3 Proposed Model Each dialogue Dconsists of two parts denoted as D={(U , S ) } , whereU= [ u1,u2, ... ,u n]is a sequence of utterances ordered based on their temporal occurrence . 
S= [ s1,s2, ... ,s n]denotes corresponding speakers and nis the number of utterances in the dialogue . 
The ERD task aims to predict Y= [ y1,y2, ... ,y n ] , whereyi‚ààC(1‚â§i‚â§n ) denotes the underlying emotion of the utterance ui . 
Cis the set of candidate emotion categories . 
The FERNet consists of four successive modules : feature extraction module , attention module , reasoning module and output module . 
Figure 2 presents the overall architecture of the proposed model . 
3.1 Feature Extraction Module We use two multi - layer bidirectional Gated Recurrent Unit ( bi - GRU ) Networks ( Tang et al . 
, 2015 ) to accumulate contextual information from two directions for each word of target utterances and historical utterances , separately . 
The inputs consist of Multi - layerBi - GRUMulti - layerBi - GRUMulti - headAttentionReasoningModuleùë¢ " ùë¢#ùêæùëâùëÑùë•"ùêª"ùëÖùëÖ""ùëÖ""Figure 2 : The overall architecture of the model . 
300 dimensional pre - trained GloVe vectors ( Pennington et al . 
, 2014 ) . 
The k‚àíth contextual word representation hl k= [ ‚àí ‚Üí hl k;‚Üê ‚àí hl k]is generated by concatenating the hidden states of the k‚àíth time steps of forward and backward GRU , where lis the number of layers . 
3.2 Attention Module We utilize multi - head attention mechanism ( Vaswani et al . 
, 2017 ) to focus on more relevant parts of each historical utterance according to the target utterance . 
We also employ residual connection ( He et al . 
, 2016 ) followed by layer normalization ( Ba et al . 
, 2016 ) to make model training easier . 
The target - speciÔ¨Åc representations of historical utterances are obtained by : X = Concat ( head 1, ... head t)WO(1 ) head i = Attention ( QWQ i , KWK i , VWV i)(2 ) Attention ( Q , K , V ) = softmax ( QKT ‚àödk)V(3 ) where queries Q = Rare representations of target utterances , keys Kand valuesVare contextual word representations for words of historical utterances . 
WQ i‚ààRd√ódk , WK i‚ààRd√ódk , WV i‚ààRd√ódv andWO‚ààRtdv√ódare parameter matrices , where dkis the dimension of queries and keys , dvis the dimension of values , dis the dimension of the output of feature extraction module and tis the number of heads . 
X= [ x1,x2, ... ,x n‚àí1]are target - speciÔ¨Åc representations of historical utterances . 
3.3 Reasoning Module The reasoning module takes target - speciÔ¨Åc historical utterance representations [ x1,x2, ... x n‚àí1]and target utterance representations Ras inputs . 
Target utterance representations are updated through time and layers . 
Each unit in this module takes two inputs : R andxi(1‚â§i‚â§n‚àí1 ) . 
Thet‚àíth unit updates R according to xtby : zt = Œ±(xt , R ) = œÉ(Wz(xt ‚ó¶ R ) + bz ) ( 4)38 rt = Œ≤(xt , R ) = œÉ(Wr(xt ‚ó¶ R ) + br)(5 ) ÀúRt = œÅ(xt , R ) = tanh(Wh[xt;R ] + bh)(6 ) Rt = ztrtÀúRt+ ( 1‚àízt)Rt‚àí1 ( 7 ) whereztis the update gate , rtis a reset function , ÀúRtis the candidate of updated representation of target utterance and Rtis the updated representation after observing the t‚àíth historical utterance . 
œÉis sigmoid activation , tanh is hyperbolic tangent activation, ‚ó¶ is element - wise vector multiplication , and [ ; ] is vector concatenation along the last dimension . 
Wz‚ààRd√ód , Wr‚ààRd√ód , Wh‚ààRd√ó2dare weight matrices , bz‚ààRd , br‚ààRd , bh‚ààRdare bias terms . 
SpeciÔ¨Åcally , ztmeasures the relevance between the target utterance representation and the tth historical utterance representation for Ô¨Åne - controlled gating . 
Compared with global attention computed over all historical utterances , the gate can be considered as local attention which models short - term sequential dependency . 
rtis a reset function to determine how much previous information should be ignored by resetting the candidate of updated representation of target utterance . 
As shown in Sukhbaatar et al . 
( 2015 ) , multi - hop can perform reasoning over multiple facts more effectively . 
So we stack several layers with outputs of the current layer used as inputs to the next layer . 
Besides , to model more abundant information , we compute‚àí ‚Üí Rl tand‚Üê ‚àí Rl tin both forward and backward directions and add them together to get Rl tas the updated representation of the t‚àíth unit inl‚àíth layer : Rl t=‚àí ‚ÜíRl t+‚Üê ‚àíRl t ( 8) Finally , we get the updated representation of target utterance Rupdate = RL n‚àí1 , wheren‚àí1and Lare the number of units and the number of layers in the reasoning module , respectively . 
3.4 Output Module After the feature extraction and reasoning modules , we obtain the updated representation of target utterance . 
To preserve original semantic content , we concatenate the updated representation and the original representation together : Rfinal = [ Rupdate;R ] ( 9 ) We use a fully connected layer with softmax as activation to calculate emotion - class probabilities : P = softmax ( WfRfinal + bf ) ( 10)whereWf‚ààRdclass√ó2dis a weight matrix , bf‚àà Rdclass is a bias term and P‚ààRdclass are emotionclass probabilities . 
4 Experiment Datasets We perform experiments on two benchmarks : IEMOCAP ( Busso et al . 
, 2008 ) and A VEC ( Schuller et al . 
, 2012 ) . 
They are multimodal datasets involved in two - way dynamic conversations . 
In this paper , we only focus on using textual modality to recognize the emotion . 
The data distribution is shown in Appendices . 
Evaluation Metrics We use accuracy ( Acc . 
) , F1score ( F1 ) and weighted average F1 - socre ( Average ) as evaluation metrics for IEMOCAP dataset . 
Mean Absolute Error ( MAE ) and Pearson correlation coefÔ¨Åcient ( r ) are used as metrics for A VEC dataset . 
Baselines We compare the FERNet with following existing approaches : CNN ( Kim , 2014 ) , c - LSTM ( Poria et al . 
, 2017 ) , c - LSTM+Attention ( Poria et al . 
, 2017 ) , Memnet ( Ba et al . 
, 2016 ) , CMN ( Hazarika et al . 
, 2018b ) , DialogueRNN ( Majumder et al . 
, 2019 ) . 
Training Details The training details such as hyper - parameters and settings we used are shown in Appendices . 
4.1 Results The overall results of experiments are shown in Table 1 . 
We can see that our model outperforms baselines signiÔ¨Åcantly on all evaluation metrics of both datasets . 
SpeciÔ¨Åcally , our model surpasses DialogueRNN by 1.69 % on weighted average F1score . 
For A VEC dataset , our model lower mean absolute error by 0.03 , 0.027 , 0.009 and 0.31 for valence , arousal , expectancy and power , separately . 
We attribute the enhancement to the fundamental improvement of FERNet , which are generating target - speciÔ¨Åc representations of historical utterances and handling both short - term and long - term sequential dependencies . 
4.2 Discussion and Analysis Parameters We conduct experiments with different values of the number of historical utterances ( N ) and the number of layers of reasoning module ( L ) on the IEMOCAP dataset . 
Results are shown in Figure 4 . 
We observe that as Nincreases , the performance of the model tends to be improved . 
This trend shows that adequate historical information39 methodsIEMOCAP A VEC Happy Sad Neutral Angry Excited Frustrated Average Valence Arousal Expectancy Power Acc . 
F1 Acc . 
F1 Acc . 
F1 Acc . 
F1 Acc . 
F1 Acc . 
F1 Acc . 
F1 MAE r MAE r MAE r MAE r CNN 27.22 29.86 57.14 53.83 34.33 40.14 61.17 52.44 46.15 50.09 62.99 55.75 48.92 48.18 0.545 -0.01 0.542 0.01 0.605 -0.01 8.71 0.19 c - LSTM 29.17 34.43 57.14 60.87 54.17 51.81 57.06 56.73 51.17 57.95 67.19 58.92 55.21 54.95 0.194 0.14 0.212 0.23 0.201 0.25 8.90 -0.04 c - LSTM+Attention 30.56 35.63 56.73 62.90 57.55 53.00 59.41 59.24 52.84 58.85 65.88 59.41 56.32 56.19 0.189 0.16 0.213 0.25 0.190 0.24 8.67 0.10 Memnet 25.72 33.53 55.53 61.77 58.12 52.84 59.32 55.39 51.50 58.30 67.2 59.00 55.72 55.10 0.202 0.16 0.211 0.24 0.216 0.23 8.97 0.05 CMN 25.00 30.38 55.92 62.41 52.86 52.39 61.76 59.83 55.52 60.25 71.13 60.69 56.56 56.13 0.192 0.23 0.213 0.29 0.195 0.26 8.74 -0.02 DialogueRNN‚àó31.25 33.83 66.12 69.83 63.02 57.76 61.76 62.50 61.54 64.45 59.58 59.46 59.33 59.89 0.188 0.28 0.201 0.36 0.188 0.32 8.19 0.31 FERNet 38.89 40.14 72.65 70.22 67.19 61.50 66.47 62.43 68.90 68.21 50.39 58.63 61.80 61.58 0.158 0.44 0.174 0.43 0.179 0.37 7.88 0.36 Table 1 : Performance of FERNet compared with baselines on the IEMOCAP dataset and A VEC dataset . 
Bold font denotes the best performances.‚àópresents the state - of - the - art method in the setting that only historical utterances can be utilized . 
I‚Äômgettingmarried.[excited]Noway.[excited ] . 
                     [ 1]Noway , when?When , when , whendidithappen?[excited][1]Justacoupledaysago.[excited]Ican‚Äôtbelieveit.[2]Inever thought you would get married.[excited][2]Iknowmeneither.[excited ] Noway , when?When , when , whendidithappen?<EOS > Figure 3 : Average attention vectors across all attention heads for words of a historical utterance with regard to different target utterances . 
[ 1 ] shows the attention vector for the sentence ‚Äù Just a couple days ago ‚Äù ; [ 2 ] shows the attention vector for the sentence ‚Äù I know me either ‚Äù . 
contributes to the performance of emotion recognition . 
However , a further increase of Ndegrades the performance of the model . 
It is mainly due to that there is too much - unrelated information confusing the model . 
As for L , the trend is similar to the parameterN. Models with hops in the range of 2 - 8 outperform the single layer variant . 
However , with Lincreasing , the reasoning module deepens and may cause the gradient vanishing problem which damages the performance of the model . 
55.2455.5356.9558.0458.8858.3959.4260.2961.5860.8558.915052545658606264 234510152025303540The number of historical utterances(N)57.4659.4260.7358.658.458.8157.8858.2856.4356.3350525456586062 12345678910The number of hops(L)(ùëé)(ùëè)ùëäùëíùëñùëî‚Ñéùë°ùëíùëë 	 ùêπ1‚àíùë†ùëêùëúùëüùëíùëäùëíùëñùëî‚Ñéùë°ùëíùëë 		 ùêπ1‚àíùë†ùëêùëúùëüùëí Figure 4 : Performance of FERNet with different values ofNandL. In ( a),L= 2and in ( b),N= 20 . 
Ablation Study In order to demonstrate the effect of each module , we perform ablation studies . 
We compare the attention - based model with the attention - free model and replace the reasoning module with a memory network . 
As shown in Table 2 , attention module and reasoning module both have a positive impact on model performance . 
Case Study and Error Analysis We analyze the predicted results and Ô¨Ånd that misclassiÔ¨Åcation often occurs when utterances are short . 
For example , our model classiÔ¨Åes ‚Äú what ? ‚Äù as ‚Äú neutral ‚Äù , but the label is ‚Äú excited ‚Äù . 
We think it is due to the lack of visual and audio modality . 
In this utterance , methods Acc . 
F1 FERNet without attention 58.84 58.58 FERNet with memory network 59.77 59.33 FERNet 61.80 61.58 Table 2 : Performance of variants of FERNet on the IEMOCAP dataset . 
Bold font denotes the best performances . 
high pitched audio can provide vital information for recognizing the emotion . 
Besides , we Ô¨Ånd our model misclassiÔ¨Åes several ‚Äú excited ‚Äù utterances as ‚Äú happy ‚Äù utterances , several ‚Äú sad ‚Äù utterances as ‚Äú frustrated ‚Äù utterances , and vice versa . 
The reason is that it is hard for the model to distinguish the subtle difference between these similar emotions . 
Besides , we perform qualitative visualization of the attention module . 
The dialogue in Figure 3 shows that for different target utterances , the model allocates different attention to words of a historical utterance . 
It demonstrates the effectiveness of the attention module . 
5 Conclusion In this paper , we propose FERNet to solve the ERD task . 
The model generates target - speciÔ¨Åc historical utterances according to the content of the target utterance using attention mechanism . 
The reasoning module effectively handles both local and global sequential dependencies to update the original representation of the target utterance to a more informed vector . 
Our model achieves competitive performance on two benchmarks.40 A Appendices Dataset Partition # of utterances # of dialogues IEMOCAPtrain 5810 120 test 1623 31 A VECtrain 4368 63 test 1430 32 Table 3 : Data distribution of IEMOCAP and A VEC datasets . 
Training Details We use 10 % of the training set as the validation set for hyper - parameters tuning . 
All tokens are lowercased with removal of stop words , symbols and digits , and sentences are zero - padded to the length of the longest sentence in the dataset . 
We alter the weight that each training instance carries when computing the loss to mitigate the inÔ¨Çuence of data imbalance . 
The weights are speciÔ¨Åc factors depending on corresponding emotions . 
Hyper - parameters IEMOCAP A VEC Optimizer Adam Adam Learning rate 0.001 0.001 Batch size 16 16 Bi - GRU layer 2 2 Reasoning module layer 2 2 Historical utterance 30 20 GRU hidden size 150 150 Attention head 4 2 Attention hidden size 256 256 Table 4 : Hyper - parameters and settings used for the two datasets.43 Proceedings of the 1st Conference of the Asia - PaciÔ¨Åc Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing , pages 44‚Äì53 December 4 - 7 , 2020 . 
¬© 2020 Association for Computational Linguistics SentiRec : Sentiment Diversity - aware Neural News Recommendation Chuhan Wu‚Ä†Fangzhao Wu‚Ä°Tao Qi‚Ä†Yongfeng Huang‚Ä† ‚Ä†Department of Electronic Engineering & BNRist , Tsinghua University , Beijing 100084 , China ‚Ä°Microsoft Research Asia , Beijing 100080 , China { wuchuhan15,wufangzhao , taoqi.qt } @gmail.com yfhuang@tsinghua.edu.cn Abstract Personalized news recommendation is important for online news services . 
Many news recommendation methods recommend news based on their relevance to users ‚Äô historical browsed news , and the recommended news usually have similar sentiment with browsed news . 
However , if browsed news is dominated by certain kinds of sentiment , the model may intensively recommend news with the same sentiment orientation , making it difÔ¨Åcult for users to receive diverse opinions and news events . 
In this paper , we propose a sentiment diversity - aware neural news recommendation approach , which can recommend news with more diverse sentiment . 
In our approach , we propose a sentiment - aware news encoder , which is jointly trained with an auxiliary sentiment prediction task , to learn sentiment - aware news representations . 
We learn user representations from browsed news representations , and compute click scores based on user and candidate news representations . 
In addition , we propose a sentiment diversity regularization method to penalize the model by combining the overall sentiment orientation of browsed news as well as the click and sentiment scores of candidate news . 
Extensive experiments on real - world dataset show that our approach can effectively improve the sentiment diversity in news recommendation without performance sacriÔ¨Åce . 
1 Introduction Online news websites such as Google news1have gained huge popularity for consuming digital news ( Das et al . 
, 2007 ) . 
However , it is difÔ¨Åcult for users to Ô¨Ånd their interested news information due to the huge volume of news emerging every day ( Okura et al . 
, 2017 ) . 
Thus , personalized news recommendation is important for news websites to 1https://news.google.com/ Driving the Highway 1   North of San Francisco ' Snack Man ' Helps Feed   The Homeless 3 dead , 3 injured in plane   crash west of Michigan   airport1 killed in shooting at   Washington state   apartment building4 Dead In Horrific Wrong Way Crash On Highway   101 In San Francisco Clicked News Candidate News Recommend ‚Ä¶ ‚Ä¶ Early morning Christmas   fire displaces 250 at   shelter for homelessFigure 1 : Several news browsed by two users of MSN News and the candidate news recommended to them . 
target user interest and alleviate information overload ( Wu et al . 
, 2019a ) . 
Many existing news recommendation methods rank candidate news based on their relevance to the interests of users inferred from their historical browsed news ( Okura et al . 
, 2017 ; Wu et al . 
, 2019c ) . 
For example , Okura et al . 
( 2017 ) proposed to learn news representations from news texts via autoencoders , and learn user representations from browsed news using a gated recurrent unit ( GRU ) network . 
They ranked candidate news based on the inner product of the user representation and candidate news representation . 
Wu et al . 
( 2019c ) proposed to learn news and user representations using multi - head self - attention networks . 
They ranked news based on the click scores computed by the dot product between news and user representations . 
The news articles recommended by these methods are usually similar to those previously browsed by a user in many aspects , such as content and sentiment . 
For example , in Fig . 
1 the two candidate news articles are recommended to both users . 
The Ô¨Årst user browses a news about the highway in San Francisco and a news about a person helping the homeless , which has inherent relatedness with the content of the candidate news . 
The second user browses several news about deadly accidents and44 crime , which has the same sentiment orientation as the candidate news . 
However , like the recommendations for the second user in Fig . 
1 , if a user mainly browses news articles that have a certain kind of sentiment ( e.g. , negative sentiment ) , many existing methods may intensively recommend news with the same sentiment orientation , which is not beneÔ¨Åcial for this user to receive diverse opinions and news events that convey other sentiments . 
In this paper , we propose a sentiment diversityaware news recommendation approach named SentiRec , which can improve the sentiment diversity of news recommendation by considering the sentiment orientation of candidate and browsed news . 
In our approach , we propose a sentiment - aware news encoder , which is jointly trained with an auxiliary news sentiment prediction task , to incorporate sentiment information into news modeling and generate sentiment - aware news representations . 
We learn user representations from the representations of browsed news , and compute the click scores of candidate news based on their relevance to the user representations . 
In addition , to enhance the sentiment diversity of news recommendation , we propose a sentiment diversity regularization method to penalize our model during model training , which is based on the overall sentiment orientation of browsed news as well as the sentiment scores and click scores of candidate news . 
We conduct extensive experiments on a real - world benchmark dataset , and the results show that our approach can achieve better sentiment diversity and recommendation accuracy than many baseline methods . 
The contributions of this paper are summarized as follows : ‚Ä¢To the best of our knowledge , this is the Ô¨Årst work that explores to improve the sentiment diversity of news recommendation . 
‚Ä¢We propose a sentiment - aware news encoder that incorporates an auxiliary news sentiment prediction task to encode sentiment - aware news representations . 
‚Ä¢We propose a sentiment diversity regularization method to encourage the model to recommend news with diverse sentiment from the browsed news . 
‚Ä¢Extensive experiments on real - world benchmark dataset verify that our approach can recommend news with diverse sentiment without performance loss.2 Related Work News recommendation is an important technique for online news websites to provide personalized news reading services ( Zheng et al . 
, 2018 ) . 
A core problem in news recommendation is building accurate representations of news and users and further ranking candidate news according to news and user representations ( Okura et al . 
, 2017 ) . 
In many news recommendation methods , news ranking is based on the representations of news and users built by manual feature engineering ( Liu et al . 
, 2010 ; Capelle et al . 
, 2012 ; Son et al . 
, 2013 ; Karkali et al . 
, 2013 ; Garcin et al . 
, 2013 ; Bansal et al . 
, 2015 ; Ren et al . 
, 2015 ; Chen et al . 
, 2017 ; Zihayat et al . 
, 2019 ) . 
For example , Liu et al . 
( 2010 ) proposed to use topic categories and interest features generated by a Bayesian model to build news and user representations . 
They ranked candidate news based on the product of a content - based score computed from news representations and a Ô¨Ålter - based score computed by collaborative Ô¨Åltering . 
Son et al . 
( 2013 ) proposed an Explicit Localized Semantic Analysis ( ELSA ) model for location - based news recommendation . 
They proposed to represent news and users by extracting topic and location features from Wikipedia pages , and ranked news based on the cosine distance between the representations of news and user . 
Lian et al . 
( 2018 ) proposed to use various handcrafted features to represent news and users , such as title length , news categories , user proÔ¨Åles and features extracted from user behavior histories . 
They ranked candidate news based on the click scores computed by a neural factorization machine . 
However , these methods rely on manual feature engineering to build news and user representations , which usually necessitate massive expertise . 
In addition , handcrafted features may not be optimal in representing news content and user interest . 
In recent years , several news recommendation methods based on deep learning techniques are proposed ( Okura et al . 
, 2017 ; Khattar et al . 
, 2018 ; Wang et al . 
, 2018 ; Wu et al . 
, 2019a ; An et al . 
, 2019 ; Wu et al . 
, 2019b , c ; Ge et al . 
, 2020 ) . 
For example , Okura et al . 
( 2017 ) proposed to learn Ô¨Årst news representations from news bodies using autoencoders , and then learn representations of users from their clicked news with a GRU network . 
Candidate news are ranked based on the click scores computed by the dot products between news and user representations . 
Wang et al . 
( 2018 ) proposed to learn news representations from news titles and45 their entities via a knowledge - aware CNN network , and learn user representations from clicked news with a candidate - aware attention network . 
They ranked candidate news based on the click scores computed from the concatenation of news and user representations via a feed - forward neural network . 
Wu et al . 
( 2019c ) proposed to learn news and user representations with a combination of multi - head self - attention and additive attention networks . 
They also used dot product to compute click scores for news ranking . 
These methods tend to recommend news articles which are similar with the news users previously browsed ( Lin et al . 
, 2014 ) . 
Thus , these methods may recommend news with similar sentiment orientation with those previously browsed by users , which is not beneÔ¨Åcial for users to receive diverse news information . 
Different from these methods , our approach can effectively recommend news with diverse sentiment to users by incorporating sentiment information into news modeling via a sentiment - aware news encoder and regularizing the model based on the sentiment orientation of browsed and candidate news . 
3 Our Approach In this section , we Ô¨Årst present the formal deÔ¨Ånitions of the problem explored in this paper , then introduce the details of our sentiment diversity - aware news recommendation ( SentiRec ) approach . 
3.1 Problem DeÔ¨Ånition The problem studied in this paper is deÔ¨Åned as follows . 
Given a user uwith her news browsing historyH= [ D1,D2, ... ,D N]and a set of candidate news2C= [ Dc 1,Dc 2, ... ,Dc P](NandPrespectively denote the number of browsed news and candidate news ) , the goal of the news recommendation model is to predict the personalized click scores [ ÀÜy1,ÀÜy2, ... ,ÀÜyP]of these candidate news , which are further used for ranking and display . 
We denote the sentiment labels of the browsed news and candidate news as [ s1,s2, ... ,s N]and[sc 1,sc 2, ... ,sc P ] , respectively . 
In this paper we assume the sentiment labels are real values from -1 to 1 , which indicate the sentiment polarity of news articles . 
We denote the overall sentiment orientation of browsed news ass . 
The sentiment diversity is deÔ¨Åned as the differences between the sentiment orientation of recommended news and the overall sentiment 2The candidate news set is usually recalled from the entire news pool.of browsed news.3The sentiment diversity of the news ranking results C / primefor the useruis measured by a function d = f(C / prime , s ) . 
The recommendation diversity is better if more top ranked news in C / prime have the different sentiment orientation with s. 3.2 News Recommendation Framework In this section , we introduce the general news recommendation framework of our SentiRec approach , as shown in Fig . 
2 . 
There are three core components in this framework for news recommendation , i.e. , sentiment - aware ( SA ) news encoder , user encoder , and click predictor . 
The sentimentaware news encoder aims to learn representations of news articles from their texts , where their sentiments are taken into consideration . 
We apply the sentiment - aware news encoder to the browsed news [ D1,D2, ... ,D N]and the candidate news Dcto encode their sentiment - aware representations , which are respectively denoted as [ r1,r2, ... ,rN]andrc . 
The user encoder aims to learn representations of users from the sentiment - aware representations of their browsed news . 
Motivated by ( Vaswani et al . 
, 2017 ) , we use Transformer to capture the relatedness between browsed news and learn a uniÔ¨Åed representation ufor each user . 
The click predictor aims to compute the personalized click scores of candidate news by measuring the relevance between user and candidate news representations . 
Following many previous works ( Okura et al . 
, 2017 ; Wu et al . 
, 2019b ) , we use dot product to implement the click predictor , and the click score ÀÜyis predicted by ÀÜy = u / latticetoprc . 
3.3 Sentiment - Aware News Encoder In this section , we introduce the details of the sentiment - aware news encoders in our SentiRec approach . 
Its architecture is shown in Fig . 
3 . 
Motivated by the news encoder in ( Wu et al . 
, 2019c ) , we Ô¨Årst use a word embedding layer to convert the sequence of words in a news title into a sequence of semantic vectors , and then use a Transformer ( Vaswani et al . 
, 2017 ) to capture the contexts of words and build a uniÔ¨Åed rrepresentation of news texts . 
However , the news representations directly learned by the Transformer are usually not sentiment - bearing . 
In fact , the sentiment information of news is very important for understanding 3We do not strictly require the recommendation results in an impression to be diverse in sentiment . 
We expect the sentiment of recommended news in a long term ( e.g. , multiple impressions in months ) is diverse.46 ùê∑1 Browsed Newsùê∑2 ùê∑ùëêùíñClick Predictorùëù Click   Score Candidate Newsùíìùüè SA News   Encoderùíìùëµ SANews   EncoderùíìùüêUser Encoder SA News   Encoderùíìùëê ùë†ùëê ùë†1 ùë†2Sentiment   Monitor‡∑ùùíöSentiment Diversity Score Sentiment   AnalyzerSentiment   AnalyzerSentiment   Analyzer“ßùë† ùê∑ùëÅSANews   Encoderùë†ùëÅ Sentiment   Analyzer Figure 2 : The framework of our SentiRec approach . 
Transformer Word EmbeddingùíÜ1ùíÜ2 ùíÜùëÄùíì Sentiment   Predictor∆∏ùë†Sentiment ScoreNews Representation Driving the Francisco Figure 3 : The architecture of the sentiment - aware news encoder . 
the content of news . 
For example , in Fig . 
1 , although the news ‚Äú Early morning ... ‚Äù and ‚Äú Snack Man ... ‚Äù are both related to the homeless , they have opposite sentiment polarity , and modeling the sentiment of them can help understand their content better . 
In addition , the sentiment of news can also provide useful clues for user modeling and news ranking . 
For example , if a user frequently clicks negative news as the second user in Fig . 
1 , it may be more appropriate to recommend several positive news to this user rather than continuously recommending similar negative news . 
Thus , modeling news sentiment has the potential to enhance news recommendation . 
However , since the sentiment scores of news are numerical variables , simply re - garding them as model input may be not optimal . 
Thus , we propose an auxiliary sentiment prediction task , and we jointly train the news encoder with this task to encourage it to learn sentiment - aware news representations . 
The real - valued sentiment score ÀÜsis predicted as follows : ÀÜs = Vs√ór+vs , ( 1 ) where Vsandvsare parameters . 
The loss function of sentiment prediction we use is the mean absolute error ( MAE ) , which is formulated as follows : Lsenti=1 SS / summationdisplay i=1|ÀÜsi‚àísi| , ( 2 ) where ÀÜsiandsirespectively stand for the predicted sentiment score and sentiment label of the i - th news , andSdenotes the number of news . 
The sentiment labels are obtained by the sentiment analyzer modules in Fig . 
2 , which can be implemented by many sentiment analysis methods . 
3.4 Sentiment Diversity Regularization To further improve the sentiment diversity of news recommendation , we propose a sentiment diversity regularization method to penalize the recommendation model according to the overall sentiment score of browsed news , the sentiment score of candidate news , and its predicted click score . 
As shown in Fig . 
2 , we Ô¨Årst use the sentiment analyzer to obtain the sentiment scores of the candidate47 news ( denoted as sc ) and browsed news ( denoted as[s1,s2, ... ,s N ] ) . 
We then compute an overall sentiment score4of browsed news to indicate the historical sentiment preference of a user as follows : ¬Øs=1 NN / summationdisplay i=1si . 
( 3 ) A positive ¬Øsindicates that the user has read news with more positive sentiment and a negative ¬Øsindicates the negative sentiment is dominant in the browsed news . 
If the news recommender intensively recommends news with the same sentiment polarity with the overall sentiment sof a user ‚Äôs browsed news , it is difÔ¨Åcult for this user to receive diverse news information . 
Thus , it is important to recommend news with diverse sentiment to users . 
To solve this problem , we propose a sentiment diversity regularization method . 
We Ô¨Årst propose to compute a sentiment diversity score pwith a sentiment monitor , which is formulated as follows : p = max(0,¬ØsscÀÜy ) , ( 4 ) where a larger score of pindicates less sentiment diversity . 
In this formula , for a candidate news that shares the same sentiment polarity with s , the score pis larger if the model assigns it a higher click score or its sentiment and the overall browsed news sentiment are more intense , which indicate that the recommendation is less diverse in sentiment . 
Then , we propose a sentiment diversity loss function to regularize our model as follows : Ldiv=1 |S|/summationdisplay i‚ààSpi , ( 5 ) whereSis the data set for model training , and pi denotes the sentiment diversity score of the i - th sample inS. 3.5 Model Training In this section , we introduce how to train the models in our SentiRec approach . 
Following ( Huang et al . 
, 2013 ; Wu et al . 
, 2019c ) , we use negative sampling techniques to construct labeled data for the news recommendation task from the user impression logs . 
More speciÔ¨Åcally , for each news clicked by a user , we randomly sample Knews displayed in the same impression which are not clicked by 4We do not incorporate the numbers of positive and negative news because they can not take the sentiment intensity into consideration.this user . 
We denote the click scores of the i - th clicked news as ÀÜy+ iand the associated Knon - click news as [ ÀÜy‚àí i,1,ÀÜy‚àí i,2, ... ,ÀÜy‚àí i , K ] . 
we use the click predictor to jointly predict these scores , and normalize these scores via the softmax function to compute the click probability scores . 
The news recommendation loss we used is the negative log - likelihood of the clicked news samples , which is computed as : Lrec=/summationdisplay i‚ààSlog(exp(ÀÜy+ i ) exp(ÀÜy+ i ) + /summationtextK j=1exp(ÀÜy‚àí i , j)),(6 ) whereSis the data set for model training . 
We jointly train the news recommendation model with the auxiliary sentiment prediction task and meanwhile regularize it using the sentiment diversity loss . 
The Ô¨Ånal uniÔ¨Åed loss function of our approach is a weighted summation of the three loss functions , which is formulated as follows : L = Lrec+ŒªLsenti+¬µLdiv , ( 7 ) whereŒªand¬µare coefÔ¨Åcients to control the relative importance of the sentiment prediction loss and sentiment diversity regularization loss . 
4 Experiments 4.1 Datasets and Experimental Settings Our experiments were conducted on a real - world news recommendation dataset provided by ( Wu et al . 
, 2019b ) , which is constructed from MSN News5logs from Oct. 31 , 2018 to Jan. 29 , 2019 . 
We use the logs in the last week as the test set and the rest are used for training and validation , where the split ratio is 9:1.6To obtain the sentiment labels of the news in this dataset , we use the V ADER algorithm ( Hutto and Gilbert , 2014 ) as the sentiment analyzer in our approach.7It is a famous sentiment analysis method based on a set of sentiment lexicons such as LIWC ( Pennebaker et al . 
, 2001 ) , ANEW ( Nielsen , 2011 ) and GI ( Stone et al . 
, 1966 ) . 
We use V ADER to compute an overall sentiment orientation score of each news as the gold label , and these scores are ranged in [ -1 , 1 ] . 
The detailed statistics of the news recommendation dataset are shown in Table 1 . 
We also plot the distribution of news sentiment scores and the overall sentiment orientation of users ‚Äô browsed news 5https://www.msn.com/en-us/news 6The numbers of constructed samples for training and validation are 277,811 and 30,868 , respectively . 
The number of samples for test is 1,707,588 . 
7We choose this algorithm because it can compute the real - valued sentiment scores rather than polarity only.48 # users 10,000 avg . 
# words per title 11.29 # news 42,255 # click samples 489,644 # impressions 445,230 # non - click samples 6,651,940 # samples 7,141,584 avg . 
sentiment score 0.0314 Table 1 : Statistics of the dataset . 
-1.0 -0.6 -0.2 0.2 0.6 1.0Sentiment Polarity Score012345Density ( a ) News sentiment polarity scores . 
-1.0 -0.6 -0.2 0.2 0.6 1.0Overall Sentiment Polarity Score012345Density(b ) Overall sentiment orientation of users ‚Äô browsed news . 
Figure 4 : Distributions of the news sentiment polarity scores and the overall sentiment orientation of users ‚Äô browsed news in our dataset . 
in Figs . 
4(a ) and 4(b ) , respectively . 
We Ô¨Ånd that although positive and negative news are almost balanced , the overall sentiment orientation of users ‚Äô browsed news is negative . 
In addition , we show the average click - through rate ( CTR ) of news with different ranges of sentiment scores in Fig . 
5 . 
As the saying goes , ‚Äú evil news rides fast , while good news baits later ‚Äù . 
We Ô¨Ånd it is interesting that more negative news have higher CTRs , which indicates that negative news has stronger ability in attracting news clicks . 
Thus , it is important to recommend news with diverse sentiment to avoid overwhelming users with too much negative news information . 
Following Wu et al . 
( 2019b ) , in our experiments the word embeddings were initialized by the 300dimensional Glove embeddings ( Pennington et al . 
, 2014 ) . 
The negative sampling ratio Kwas set to 4 . 
Adam ( Kingma and Ba , 2014 ) was chosen as the optimizer and the size of a minibatch was 30 . 
In addition , the loss weights Œªand¬µwere respectively set to 0.4 and 10 . 
These hyperparameters were tuned on the validation set . 
To evaluate the performance of news recommendation , we use metrics including AUC , MRR , nDCG@5 and nDCG@108 . 
Since there is no off - the - shelf metric to evaluate the sentiment diversity of news recommendation , motivated by the MRR and hit ratio metrics , we propose three metrics named Senti MRR , Senti @5 andSenti @10 to quantitatively measure sentiment 8The relevance grade is binary , i.e. , 0 for non - clicked news and 1 for clicked news . 
[ -1.0,-0.6 ) [ -0.6,-0.2 ) [ -0.2,0.2 ) [ 0.2,0.6 ) [ 0.6,1.0 ] Sentiment Score0.000.010.020.030.040.050.06CTRFigure 5 : Click - through rates of news with different sentiment polarity scores . 
diversity . 
They are computed as follows : Senti MRR = max(0,¬ØsC / summationdisplay i=1sc i i ) , Senti @5 = max(0,¬Øs5 / summationdisplay i=1sc i ) , Senti @10 = max(0,¬Øs10 / summationdisplay i=1sc i),(8 ) whereCis the number of candidate news in an impression , sc idenotes the sentiment score of the candidate news with the i - th highest click score . 
In these metrics , higher scores indicate that the recommendation results are less diverse from the browsed news in their sentiment.9We repeated each experiment 10 times and reported the average results over all impressions in terms of the recommendation performance and sentiment diversity . 
4.2 Performance Evaluation We evaluate the recommendation performance and sentiment diversity of our approach by comparing it with several baseline methods , including : ( 1)LibFM ( Rendle , 2012 ) , a feature - based recommendation method based on factorization machine . 
TF - IDF features are used to represent the textual content of news . 
( 2 ) EBNR ( Okura et al . 
, 2017 ) , an embedding - based neural news recommendation method . 
It uses denoising autoencoders to learn news representations and a GRU network to encode user representations . 
( 3 ) DKN ( Wang et al . 
, 2018 ) , a knowledge - aware news recommendation method , which learns news representations via knowledgeaware CNN networks and learns user representations with a candidate - aware attention network . 
9The scores are positive if the top ranked news have the same sentiment orientation with the overall sentiment , and are higher if these sentiments are more intensive.49 Methods AUC MRR nDCG@5 nDCG@10 LibFM 0.5661 0.2414 0.2689 0.3552 EBNR 0.6102 0.2811 0.3035 0.3952 DKN 0.6032 0.2744 0.2967 0.3873 Conv3D 0.6051 0.2765 0.2987 0.3904 DAN 0.6154 0.2860 0.3093 0.3996 NPA 0.6240 0.2952 0.3185 0.4094 NAML 0.6205 0.2902 0.3144 0.4060 NRMS 0.6275 0.2985 0.3217 0.4139 SentiRec 0.6294 0.3013 0.3237 0.4165 SentiRec - same 0.6299 0.3017 0.3240 0.4171 Table 2 : Results of recommendation performance . 
Higher scores indicate better performance . 
( 4)Conv3D ( Khattar et al . 
, 2018 ) , a neural news recommendation method which learns news representations using 2 - D CNN models and learns user representations using a 3 - D CNN model . 
( 5 ) DAN ( Zhu et al . 
, 2019 ) , a neural news recommendation method which learns news representations from title and entities with two independent CNN models and learns user representations using attentive LSTM network . 
( 6 ) NPA ( Wu et al . 
, 2019b ) , a neural news recommendation method which learns news and user representations via personalized attention mechanism . 
( 7 ) NAML ( Wu et al . 
, 2019a ) , a neural news recommendation method which learns news representations with CNN models and learns user representations using attention networks . 
( 8) NRMS ( Wu et al . 
, 2019c ) , a neural news recommendation method which learns news and representations using multi - head self - attention and additive attention networks . 
For fair comparison , in all methods we used news titles to learn news representations . 
In addition , we compare the sentiment diversity of random news ranking , which aims to show the benchmark sentiment diversity without news and user modeling . 
Besides , we also compare a variant of our method ( denoted as SentiRec - same ) which only recommends the news with the same sentiment polarity with the browsed news ( Ô¨Ålter the candidate news with different sentiment polarity ) , which aims to show the scores of an extreme case with minimal sentiment diversity . 
The results of recommendation performance and sentiment diversity are summarized in Tables 2 and 3 . 
From these results , we Ô¨Ånd that neural news recommendation approaches achieve better recommendation performance than LibFM . 
This is probably because neural networks can learn more informative news and user representations than traditional matrix factorization methods . 
However , compared with random ranking , we Ô¨Ånd that the diver - Methods Senti MRRSenti @5Senti @10 Random 0.0262 0.0442 0.0687 LibFM 0.0843 0.1192 0.2579 EBNR 0.0989 0.1476 0.2868 DKN 0.0954 0.1389 0.2810 Conv3D 0.0973 0.1431 0.2830 DAN 0.1005 0.1520 0.2897 NPA 0.1044 0.1583 0.3015 NAML 0.1030 0.1569 0.2967 NRMS 0.1066 0.1592 0.3034 SentiRec 0.0046 0.0083 0.0115 SentiRec - same 0.3271 0.4963 0.9373 Table 3 : Results of sentiment diversity . 
Lower scores indicate better sentiment diversity . 
sity scores of all baseline methods are much larger , especially those based on neural networks . 
This is probably because the compared baseline methods mainly recommend news based on the relevance between candidate news and browsed news , and will tend to recommend news with similar sentiment orientation with browsed news , which is harmful for users to receive diverse news information . 
Different from baseline methods , our SentiRec approach can achieve much better sentiment diversity even than random ranking . 
These results show that our approach can actively recommend news with diverse sentiment from browsed news . 
In addition , our approach can also achieve better recommendation performance than baseline methods . 
These results validate that our approach can achieve the goal of improving sentiment diversity in news recommendation without hurting the recommendation performance . 
Besides , by comparing SentiRec and its variant SentiRec - same , although the recommendation performance of SentiRec - same is slightly better , the sentiment of its recommendation results are minimally diverse from browsed news , which may amplify the problem of Ô¨Ålter bubble and hurt user experience . 
4.3 Ablation Study In this section , we conduct ablation studies to verify the inÔ¨Çuence of the auxiliary sentiment prediction task in the sentiment - aware news encoder and the sentiment diversity regularization method on the recommendation performance and sentiment diversity . 
The results are shown in Fig . 
6 . 
From Fig . 
6 , we Ô¨Ånd that the sentiment prediction task can improve both sentiment diversity and recommendation performance . 
This may be because this auxiliary task can encourage the news encoder to50 AUC61.562.062.563.063.5AUCNone + Sentiment Prediction + Diversity Regularization SentiRecFigure 6 : InÔ¨Çuence of the sentiment prediction task and sentiment diversity regularization method . 
0.00.10.20.30.40.50.662.562.662.862.963.0AUC AUC Figure 7 : InÔ¨Çuence of the hyperparameter Œª . 
exploit the sentiment information in news texts to encode sentiment - aware news representations , which is beneÔ¨Åcial for predicting news clicks more accurately and further improving sentiment diversity by modeling users ‚Äô dynamic preferences on news sentiment . 
In addition , the sentiment diversity regularization can also effectively improve the sentiment diversity of news recommendation and meanwhile keep the recommendation performance . 
This is because this regularization method can enforce the model to recommend news with different sentiment orientations with the browsed news . 
Moreover , combining both techniques can further improve sentiment diversity , which veriÔ¨Åes the effectiveness of our SentiRec method . 
4.4 InÔ¨Çuence of Hyperparameters In this section , we will explore the inÔ¨Çuence of two important hyperparameters on our approach , i.e. , the loss coefÔ¨Åcients Œªand¬µin Eq . 
( 7 ) on the performance and sentiment diversity of our approach . 
Since there are two hyperparameters , we Ô¨Årst vary the value of Œªto Ô¨Ånd the optimal one to learn sentiment - aware news representations in our approach . 
The results are illustrated in Fig . 
7 . 
According to Fig.7 , we Ô¨Ånd both sentiment diversity and recommendation performance of our approach 0 5 10 15 2062.062.262.462.662.863.0AUC AUCFigure 8 : InÔ¨Çuence of ¬µunderŒª= 0.4 . 
improves when Œªincreases from 0 . 
This is probably because when Œªis too small , the useful sentiment information in news can not be fully exploited . 
However , the performance of our approach starts to decline when Œªis too large . 
This may be because when Œªis too large , the auxiliary sentiment prediction task is over - emphasized and the news recommendation task is not fully respected . 
Thus , a moderateŒª(e.g . 
, 0.4 ) is more appropriate for our approach to make a tradeoff between recommendation performance and sentiment diversity . 
Then , we vary the value of ¬µunderŒª= 0.4 to evaluate the recommendation performance and sentiment diversity of our approach.10The results are illustrated in Fig . 
8 . 
According to the results , we Ô¨Ånd the sentiment diversity can be consistently improved when ¬µincreases . 
This is probably because when ¬µis larger , the model is regularized more intensively and may tend to recommend more news with diverse sentiment from browsed news . 
However , when ¬µgoes too large , the performance in terms of AUC declines signiÔ¨Åcantly , which may hurt user experience . 
Thus , a moderate selection on ¬µ(e.g . 
, 10 ) is appropriate to achieve the goal of recommending news with diverse sentiment and meanwhile keep good recommendation performance . 
4.5 Case Study In this section , we present several case studies to better demonstrate the effectiveness of our approach in improving sentiment diversity of news recommendation . 
The clicked news of a randomly selected user as well as the top ranked candidate news recommended by a state - of - the - art method NRMS and our SentiRec approach are shown in Table 4 . 
We can see that the historical browsed news of this user are mainly about negative topics such 10We Ô¨Ånd that the scale of the regularization loss is relatively small and the magnitude of ¬µneeds to be larger.51 Browsed NewsTop Ranked Candidate News NRMS SentiRec Woman Arrested for Alleged California WildÔ¨Åre ScamSheriff : California ofÔ¨Åcer ‚Äôs killer is in the US illegallyEight 2018 Fashion Trends We ‚Äôre Ready to Move On From Guns are the second leading killer of kids , after carsProfessional golfer and his caddie arrested for poaching at a tiger reserveJosh Duhamel Wants to Date Someone Young Enough to Have Kids From international fashion model to suspect in racist attack on Kansas toddlerTrump threatens years - long shutdown for his wall as GOP support begins to fracture58 Amazing After - Christmas Deals Happening Right Now Table 4 : The browsed news of a user and top ranked candidate news provided by different methods . 
as crime , which usually convey negative sentiment . 
However , the NRMS method still intensively recommends news with negative sentiment such as ‚Äú Sheriff : California ofÔ¨Åcer ‚Äôs killer ... ‚Äù . 
It indicates thatNRMS tends to recommend news with similar sentiment to the browsed news , which is not suitable for users to acquire diverse news information . 
Different from NRMS , our approach can effectively recommend news with diverse sentiment from browsed news , and the recommended news also has some inherent relatedness with browsed news in their content ( e.g. , both the Ô¨Årst candidate news and the third browsed news mention ‚Äú fashion ‚Äù ) . 
It shows that our approach can improve the sentiment diversity of news recommendation and meanwhile keep recommendation accuracy . 
5 Conclusion and Future Work In this paper , we propose a sentiment diversityaware neural news recommendation approach which can effectively recommend news with diverse sentiment from browsed news . 
We propose a sentiment - aware news encoder to learn sentimentaware news representations by jointly training it with an auxiliary sentiment prediction task . 
We learn user representations from representations of browsed news , and compute click scores based on user and candidate news representations . 
In addition , we propose a sentiment diversity regularization method to regularize the model according to the overall sentiment orientation of browsed news as well as the click scores and sentiment scores of candidate news . 
Extensive experiments on realworld benchmark dataset validate that our approach can effectively enhance the sentiment diversity of news recommendation without hurting the recommendation performance . 
In our future work , we plan to analyze the sentiment on the entities in news and explore to improve the entity - level sentiment diversity of news recommendation . 
In addition , we plan to extend sentiment polarities to more kinds of emotions , such asangry , happiness , sad and surprise , to enhance the emotion diversity of news recommendation . 
Acknowledgments This work was supported by the National Key Research and Development Program of China under Grant number 2018YFB2101501 , and the National Natural Science Foundation of China under Grant numbers U1936208 and U1936216 . 
Abstract Similarity search is to Ô¨Ånd the most similar items for a certain target item . 
The ability of similarity search at large scale plays a significant role in many information retrieval applications and has received much attention . 
Text hashing is a promising strategy , which utilizes binary encoding to represent documents , and is able to obtain attractive performance . 
This paper makes the Ô¨Årst attempt to utilize Bayesian Clustering for TextHashing , dubbed as BCTH . 
SpeciÔ¨Åcally , BCTH can map documents to binary codes by utilizing multiple Bayesian Clusterings in parallel , where each Bayesian Clustering is responsible for one bit . 
Our approach employs the bit - balanced constraint to maximize the amount of information in each bit . 
Meanwhile , the bit - uncorrelated constraint is adopted to keep independence among all bits . 
The time complexity of BCTH is linear , where the hash codes and hash functions are jointly learned . 
Based on four widely - used datasets , the experimental results demonstrate that BCTH is competitive compared with currently competitive baselines from the perspective of both precision and training speed . 
1 Introduction The task of similarity search , also called nearest neighbor search , aims to Ô¨Ånd the most similar objects for a given query item ( Gionis et al . 
, 1999 ; Andoni and Indyk , 2006 ) . 
It plays a signiÔ¨Åcant role in many information retrieval applications , such as document clustering , content - based retrieval , collaborative Ô¨Åltering ( Wang et al . 
, 2016 ) , etc . 
With the development of many intelligent terminals , massive textual data has been produced over the past several decades . 
Huge challenges exist in applying text similarity algorithms ( Conneau et al . 
, 2017 ; Le et al . 
, 2018 ) to large - scale corpora , since these ‚á§ means the corresponding author.methods require complicated numerical computation . 
Text hashing ( Severyn and Moschitti , 2015 ) is a promising strategy and has obtained much attention . 
It maps semantically similar documents to hash codes with similar semantics through designing binary codes in a low - dimensional space . 
A hashing representation of each document usually needs only a few bits to be stored . 
The calculation of the similarity between two hash codes can be executed by a bit - wise XOR operation . 
Therefore , text hashing is an effective strategy to accelerate similarity queries and reduce data storage . 
Most of the traditional text hashing methods consist of two stages ( Zhang et al . 
, 2010 ; Lin et al . 
, 2014b ; Severyn and Moschitti , 2015 ) . 
The Ô¨Årst step is to learn hash code , preserving similarity among neighbors . 
Then the hash function is trained through the self - taught method , with the text features and hash codes as the input ( Wang et al . 
, 2013b ) . 
However , for mdocuments , O(m2)training time complexity is needed to generate the pairwise similarity matrix used to preserve the similarity information . 
On the other hand , due to the success of deep learning , researchers have attempted to study text hashing through deep neural networks ( Xu et al . 
, 2015 ) . 
Some of the most representative works include VDSH ( Chaidaroon and Fang , 2017 ) and NASH ( Kalchbrenner et al . 
, 2014 ) . 
The NASH model studies text hashing through an endto - end Neural Architecture , which treats the hash codes as the latent factor . 
The VDSH model introduces a latent factor for documents to capture the semantic information . 
Even though these methods have achieved attractive performance , the training time is unsatisfactory , making them unscalable to large - scale datasets . 
Motivated by the above observations , this paper attempts to utilize Bayesian Clustering for Text Hashing , dubbed as BCTH . 
SpeciÔ¨Åcally , BCTH54 can map documents to binary codes by using multiple Bayesian Clusterings in parallel , where each Bayesian Clustering is responsible for one bit . 
Our approach employs the bit - balanced constraint to maximize the amount of information in each bit . 
Meanwhile , the bit - uncorrelated constraint is adopted to keep independence among all bits . 
Experimental results prove that our approach is competitive in the perspective of both precision and training speed . 
Our contributions are summarized as follows : ‚Ä¢We propose a novel Text Hashing based on the Bayesian Clustering framework , dubbed as BCTH , for learning effective hash codes from documents . 
To the best of our knowledge , this is the Ô¨Årst work that utilizes Bayesian Clustering in text hashing . 
‚Ä¢The time complexity of our method is linear , where the hash codes and hash function are jointly learned . 
What ‚Äôs more , we visualize the hash codes and prove that BCTH can obtain effective semantics from the original documents . 
‚Ä¢We conduct extensive experiments on four public text datasets . 
Based on four widelyused datasets , the experimental results demonstrate that BCTH is competitive compared with currently competitive baselines from the perspective of both precision and training speed . 
2 Model The approach of our proposed BCTH is introduced in this section . 
As is shown in Fig . 
1 , BCTH is a general learning idea , which utilizes Bayesian Clustering that is based on the latent factor framework in Text Hashing . 
BCTH can map documents to binary codes by using multiple Bayesian Clusterings in parallel , where each Bayesian Clustering is responsible for one bit . 
During this process , the bit - balanced constraint is to maximize the amount of information in each bit . 
Meanwhile , the bituncorrelated constraint is adopted to keep independence among all bits . 
2.1 Preliminaries Given a set of mdocuments X={x(i)}m i=1 , where x(i)is the feature representation of the i - th document . 
The binary code for the i - th document isexpressed as b(i)={b(i ) k , b(i ) k2 {  1,1}}r k=1 , and ris the length of the hash codes . 
Unlike the existing approaches ( Liu et al . 
, 2011;Zhang et al . 
, 2010 ; Xu et al . 
, 2015 ) that aim to preserve the pair - wise similarity among all the documents , we use Naive Bayes to extract the semantic information of the i - th document as : P ‚á£ b(i ) k = ck|x(i ) ‚åò = P ‚á£ x(i)|b(i ) k = ck ‚åò P ‚á£ b(i ) k = ck ‚åò P(x(i ) ) ( 1 ) The Naive Bayes method assumes the conditional independence for the conditional probability distribution , and therefore , we obtain the following equation : P ‚á£ x(i)|b(i ) k = ck ‚åò = nY j=1P(wj = l(i ) j|b(i ) k = ck)(2 ) where ckrepresents the k - th bit ‚Äôs value of the hash codes of the i - th document , ck2 {  1,1 } , andnis the size of the vocabulary . 
The l(i ) jdenotes whether the j - th word of the vocabulary appears in thei - th document , and l(i ) j2{0,1 } . 
The previous formula adopts the cumulative multiplication of all words ‚Äô probabilities to calculate the likelihood of a particular document . 
However , since many words will not appear in a speciÔ¨Åc document , to avoid redundant calculation , we consider using the cumulative multiplication of the probabilities of words that appear in that particular document to calculate the probability of that document . 
P ‚á£ x(i)|b(i ) k = ck ‚åò = Y j2 (i)P(wj=1|b(i ) k = ck)(3 ) In the above equation ,  (i)is a set of words that appear in the i - th document . 
Each hash code can be learned through an unsupervised iteration process . 
By utilizing multiple Bayesian Clusterings to calculate all hash codes of documents in parallel , we obtain the following objective function : P(B|X)=P(X|B)P(B ) P(X)(4 ) where hash codes of documents are expressed as B={b(i ) k , k=1,2 .. r , i = 1,2 .. , m } . 
In order to obtain high - quality hash codes , the bit - balanced and the bit - uncorrelated constraints are introduced . 
In addition , we transform the probability from the interval [ 0,1]to the interval [  1,1]55 1 0 Input documents ùëëùëúùëêùë¢ùëöùëíùëõùë°m 1 0 1 0 1 0 0 0 0 1 1 0 1 1 ‚Ä¶ ‚Ä¶ ùëëùëúùëêùë¢ùëöùëíùëõùë°1Bayesian ClusteringHash codesFigure 1 : Illustration of how to learn the hash codes through multiple Bayesian Clusterings jointly from mdocuments . 
The size of hash codes in the illustration is r=4 . 
by the function f(P)= 2P 1 . 
Therefore , we obtain the following loss function : minrX k=1mX i=1   b(i ) k p(i ) k   2 ( ) kB Pk s.t . 
B2 {  1,1}r ‚á• m B1=0,BBT = mIr ‚á• r(5 ) where p(i ) k = f ‚á£ P(b(i ) k = ck|x(i ) ) ‚åò andP= { p(i ) k , k=1,2 .. r , i = 1,2 .. , m } . 
The 1denotes a vector with all of its elements equal to 1 . 
The equalityB1=0denotes the bit - balanced constraint , which aims to maximize the amount of information in each bit . 
The equality BBT = mIr ‚á• rdenotes the bit - uncorrelated constraint , aiming to keep the independence among all bits . 
However , the Eq . 
( 5 ) is difÔ¨Åcult to solve directly . 
Following the prior work in discrete graph hashing ( Liu et al . 
, 2014 ) , let us deÔ¨Åne the constraint space as ‚å¶ =  Y2Rr ‚á• m|Y1=0,YYT = mIr ‚á• r   . 
Then we formulate a more general framework which softens the two hard constraints in Eq . 
( 5 ) as : minkB Pk2+ kB Yk2 s.t . 
B2 {  1,1}r ‚á• m Y1=0,YYT = mIr ‚á• r(6 ) where   0is a hyper parameter and Y is relaxation factor . 
If problem ( 5 ) is feasible , we can enforce B1=0,BBT = mIr ‚á• rin Eq.(5 ) by setting an extremely large value to   , thereby converting problem ( 6 ) into problem ( 5 ) . 
2.2 Learning The learning process aims to Ô¨Ånd the desirable hash codes that can optimize the Eq . 
( 6 ) . 
Similar to ( Liuet al . 
,2014 ) , we utilize a tractable alternating minimization algorithm , which is an unsupervised iteration process , including alternately solving three sub - problems . 
W - subproblem : Let us initialize the hash codes B randomly , and the parameter W={p(wj= 1|bk = ck),p(bk = ck)},j2{1,2 , . 
. 
. 
, n } , k2 { 1,2 , . 
. 
. 
, r } can be calculated by Naive Bayes . 
The document is represented by the one - hot method . 
The variable Pis calculated in the following way , speciÔ¨Åcally , through the conditional probability and prior probability . 
The formula is as follow : p(bk = ck)=Pm i=1I(b(i ) k = ck ) m , k2{1,2 , . 
. 
. 
, r } ( 7 ) where the p(bk = ck)is the ratio of the number of documents with the k - th hash code equal to ck to the total number of documents . 
Iis the indicator function . 
If the input value is true , it returns 1 , else returns 0 . 
The calculation process of the conditional probability P(wj=1|bk = ck ) , which includes the strategy of Laplace smoothing , is as follow : P(wj=1|bk = ck)= Pm i=1I(w(i ) j=1\b(i ) k = ck)+1 Pn j=1Pm i=1I(w(i ) j=1\b(i ) k = ck)+n(8 ) wherePm i=1I(w(i ) j=1\b(i ) k = ck)is the number of documents whose k - th hash code value is ck , which contains the word wj . 
The ‚Äù \‚Äùsymbol means ‚Äù and ‚Äù . 
Y - subproblem : Given the value of B , the continuous variable Ycan be calculated by Eq . 
( 10),56 the details are as follows : min YkB Yk2()min Y2(m tr(BTY ) ) s.t . 
Y1=0,YYT = mIr ‚á• r ( 9 ) Where tris solving the trace of a matrix and Ir ‚á• ris an identity matrix . 
Minimizing 2(m tr(BTY))is equivalent to maximizing the trace of the BTY , and it can be solved by performing singular value decomposition ( SVD ) operation on the matrix Bwhere every element is calculated by : b(i ) k = b(i ) k 1 mPm i=1b(i ) k. TheUbandVb , therein satisfying [ Vb1]TbVb=0 , are stacked by the left and right singular vectors respectively from the result of SVD . 
After performing Gram - Schmidt process on UbandVb , we obtainUbandVb . 
Finally , according to ( Zhang et al . 
, 2016 ) , the Yis updated by : Y = pmh UbbUbih VbbVbiT(10 ) B - subproblem : Given the value of Pand the continuous variable Y , the value of Bcan be calculated by minimizing Eq . 
( 12 ) , and the details are as follows : min BkB Pk2+ kB Yk2 s.t . 
B2 {  1,1}r ‚á• m(11 ) Since Eq . 
( 12 ) is a simple binary optimization process , we can update Bby updating each element of it in parallel according to : b(i ) k= argmin b(i ) k2{ 1,1}   b(i ) k p(i ) k   2 +     b(i ) k y(i ) k   2 ( 12 ) The whole algorithm implementation process is shown in algorithm 1 . 
2.3 Complexity Analysis In this section , we analyze the space and time complexity of BCTH . 
The learning algorithm of BCTH is shown in Algorithm 1 . 
For space complexity , Algorithm 1 requires O(mn+mr+nr)to store the training datasets , hash codes , and parameters . 
Asris usually less than 1024 , we can easily store the above variables at large - scale in memory . 
For time complexity , we Ô¨Årst analyze each of the sub - problems . 
For W - subproblem , it takes O(mnr ) to calculate parameter Wand updateAlgorithm 1 : Learning algorithm of BCTH Input : Training data : X2Rm ‚á• n code length : r hyperparameter :   ; Output : W={p(wj=1|bk= ck),p(bk = ck)},j2 { 1,2 , . 
. 
. 
, n } , k2{1,2 , . 
. 
. 
, r } ; 1Initialize : Bby randomization ; 2repeat 3 W - step : 4 Solve WandPin W - subproblem 5 Y - step : 6 Solve Yin Y - subproblem 7 B - step : 8 Solve Bby B - subproblem 9until convergence ; 10return W , B ; probability P. For Y - subproblem , it requires O(r2n)to perform the SVD , Gram - Schmidt orthogonalization , and matrix multiplication . 
For B - subproblem , it requires O(mn)to update each b(i ) kofB. The time complexity of the whole Algorithm 1 is O(t(mnr + r2n+mn ) ) , where tis the number of iterations needed for convergence . 
In our experiments , tis set to 10by default ( See section 3.8 ) . 
It can be seen that the time complexity of BCTH is linear . 
3 Experiments 3.1 Datasets Following prior works ( Chaidaroon and Fang , 2017 ) , we experiment on four public text datasets . 
‚Ä¢Reuters Corpus Volume I ( RCV1 ): The RCV1 is a large collection of manually labeled 800,000 newswire stories provided by Reuters . 
The full - topics version is available at the LIBSVM website1 . 
‚Ä¢Reuters21578 ( Reuters)2 : This dataset is a widely - used text corpus for text classiÔ¨Åcation . 
This collection contains 10,788 documents with 90 categories and 7,164 unique words . 
‚Ä¢TMC3 : This dataset has 22 labels , 21,519 1https://www.csie.ntu.edu.tw/ cjlin / libsvmtools / datasets/ multilabel.html 2http://www.nltk.org/book/ch02.html 3https://catalog.data.gov/dataset/siam-2007-text-miningcompetition-dataset57 training set , 3,498 test set , and 3,498 documents for the validation set . 
This dataset is used as part of the SIAM text mining competition and contains the air traffc reports provided by NASA . 
‚Ä¢20Newsgroups4 : The 20 Newsgroups dataset is a collection of 18828 newsgroup documents . 
It is divided into different newsgroups , each corresponding to a speciÔ¨Åc topic . 
3.2 Baselines and Evaluation Metrics We compare BCTH with the following competitive unsupervised methods since BCTH also belongs to unsupervised methods . 
‚Ä¢LSH : This approach applies ( Datar et al . 
, 2004 ) random projections as the hash function to transform the data points from its original space to the binary hash space . 
More hash bits are needed to guarantee the precision on account of the randomness of the hash function . 
‚Ä¢SH : This baseline ( Weiss et al . 
, 2008 ) calculates the bits through thresholding a subset of eigenvectors of the Laplacian of the similarity graph . 
‚Ä¢STH : STH ( Zhang et al . 
, 2010 ) aims to Ô¨Ånd the best l - bit binary codes for all documents in the corpus via unsupervised learning . 
‚Ä¢AGH : This method ( Liu et al . 
, 2011 ) discovers the neighborhood structure hidden in the data to learn proper compact codes . 
To make the method computationally feasible , it utilizes Anchor Graphs to gain tractable lowrank adjacency matrices . 
‚Ä¢VDSH : ( Chaidaroon and Fang , 2017 ) presents a series of deep learning models for text hashing , including VDSH , VDSH - S , and VDSH - SP . 
The VDSH - S and VDSH - SP models are supervised by utilizing document labels / tags for the hashing process . 
For the comparison ‚Äôs fairness , the VDSH is adopted as the baseline since our method is also unsupervised . 
To better evaluate the effectiveness of hash codes used in the Ô¨Åeld of similarity search , every document in the test set is adopted as the query document . 
The similarity between the query document 4http://ana.cachopo.org/datasets-for-single-label-textcategorizationand each target similar document , which is utilized to retrieve relevant documents , is calculated by the Hamming distance of their hash codes respectively . 
The performance is measured by Precision , which is the ratio of the number of the similar documents to the number of total retrieved documents . 
The retrieved document that shares any common test label with the query document is denoted as a relevant document . 
Similar to previous works ( Chaidaroon and Fang , 2017 ) , the precision for the top 100 ( pre@100 ) is employed as the main criterion . 
The Ô¨Ånal results are averaged over all the test documents . 
3.3 Experimental Setup We randomly split each dataset into two subsets for training and testing , which account for 90 % and 10 % , respectively . 
The training data is used to learn the mapping from the document to the hash code . 
Each document in the test set is used to retrieve similar documents based on the mapping , and the results are evaluated . 
The similar as ( Chaidaroon and Fang , 2017 ) , we use one - hot encoding as the default representation of the raw document . 
The hyper - parameter  = [ 0,0.001 , 0.01,0.1 ] , where the number in bold denotes the default setting ( see Section 3.7 ) . 
The number of iterations is set to 10 ( see Section 3.8 ) . 
Our codes are available at the open - source code repository5 . 
In addition , the settings of the SH6 , AGH7 , STH8 and VDSH9remain unchanged with original paper . 
We run Ô¨Åve trials for each methods and an average of Ô¨Åve trials is reported to avoid bias introduced by randomness . 
All of the methods are run on Windows with 1 Intel i7 - 7500 CPU and 1 GeForce GTX 1050Ti GPU . 
3.4 Comparison Results To examine the competitiveness of BCTH , we compared our method with competitive baselines , including traditional techniques and deep learning models from the perspective of both precision and training speed . 
Table 2 reports the training time on the 20Newsgroups dataset . 
From the table , we can derive the following interesting conclusions : ( 1 ) Compared 5https://github.com/myazi/SemHash 6https://github.com/superhans/SpectralHashing 7https://github.com/ColumbiaDVMM/Anchor-GraphHashing 8http://www.dcs.bbk.ac.uk/ dell / publications / dellzhang sigir2010/ sthv1.zip 9https://github.com/unsuthee/VariationalDeepSemanticHashing58 Table 1 : Precision of the top 100 retrieved documents on four datasets with different numbers of hashing bits . 
The bold font denotes the best result at that number of bits . 
MethodsRCV1 Reuters 8bits 16bits 32bits 64bits 128bits 8bits 16bits 32bits 64bits 128bits LSH 0.4180 0.4352 0.4716 0.5214 0.5877 0.2802 0.3215 0.3862 0.4667 0.5194 SH 0.5321 0.5658 0.6786 0.7337 0.7064 0.4016 0.4201 0.4631 0.4590 0.4622 STH 0.6992 0.7688 0.8016 0.8098 0.8037 0.6955 0.7239 0.7576 0.7486 0.7240 AGH 0.4257 0.4976 0.5457 0.5698 0.5799 0.6552 0.7046 0.7313 0.7189 0.7043 VDSH 0.7285 0.7718 0.8165 0.7720 0.6630 0.6642 0.7118 0.7335 0.7083 0.7079 BCTH 0.7339 0.7989 0.8389 0.8641 0.8690 0.6827 0.7307 0.7584 0.7669 0.7889 Methods20Newsgroups TMC 8bits 16bits 32bits 64bits 128bits 8bits 16bits 32bits 64bits 128bits LSH 0.0578 0.0597 0.0666 0.0770 0.0949 0.4388 0.4393 0.4514 0.4553 0.4773 SH 0.0699 0.1096 0.2010 0.2732 0.2632 0.5999 0.6206 0.6108 0.5813 0.5612 STH 0.2035 0.3481 0.4581 0.5129 0.5247 0.7278 0.7520 0.7633 0.7569 0.7411 AGH 0.2435 0.3531 0.3861 0.3796 0.3579 0.6000 0.6334 0.6443 0.6423 0.6273 VDSH 0.3514 0.3848 0.4667 0.2219 0.0651 0.6503 0.6640 0.7062 0.6567 0.5868 BCTH 0.3089 0.4497 0.5216 0.5534 0.5830 0.7076 0.7351 0.7651 0.7804 0.7926 Methods 8bits 16bits 32bits 64bits 128bits SH 28.1 28.9 32.2 37.8 47.1 STH 16.3 16.5 17.9 20.3 28.3 AGH 10.3 10.8 11.4 12.8 15.5 VDSH 100 + 100 + 100 + 100 + 100 + BCTH 0.5 0.9 2.0 5.0 10.8 Table 2 : Training time ( second ) of different methods on20Newsgroups dataset . 
with these methods , BCTH costs less training time among all different hash bits . 
The reason can be attributed to the joint learning of hash codes and hash function , without needing to build the pairwise similarity matrix and the linear time complexity of BTCH . 
( 2 ) It consumes extremely more time to train the VDSH model than train a traditional model . 
It shows that deep learning methods with sophisticated network architecture bring many parameters , thus requiring much more time to complete the training process . 
( 3 ) The SH , STH , and AGH spend less time on the training process , which indicates that the traditional methods has its advantage over the deep learning method in training time . 
Apart from comparing the 20Newsgroups dataset , we also compare over four datasets from the perspective of precision . 
Table 1 reports the comparison results with various methods over different numbers of bits . 
From this table , we can derive the following interesting conclusions : ( 1 ) Our proposed BCTH outperforms nearly all baselines among all different hash bits on four datasets . 
It demonstrates that BCTH , which introduces the bit - balanced and the bit - uncorrelated constraints , can learn effectively hash code from documents . 
( 2 ) The VDSH model outperforms traditional methods in almost all situations . 
It denotes that deep learning techniques can capture inherent hidden text semantics , which are beneÔ¨Åcial to generate the text hash codes . 
Although our method does not get the best results in some datasets under the circumstance of short hashing bit , it is approximating the best ones . 
Since our method utilizes the bit - balanced and the bit - uncorrelated constraints to make each bit capture independent semantics for documents , it is worthwhile to study the relationship between the length of the hash codes and the effect of our method . 
3.5 Impact of the Length of Hash Codes Previous works usually limit the length of the hash code to 128 bits on account of data storage . 
To study the effectiveness of the hash codes ‚Äô size , we conduct experiments on hash codes ranging from 8 bits to 128 bits and extend hash codes to 1024 bits in this section . 
Figure 2 reports the compared results on four datasets . 
From this Ô¨Ågure , we can Ô¨Ånd the following phenomena : ( 1 ) when the length of the hash codes is equal to or over 128 bits , the effect of most other methods starts to decline . 
( 2 ) the performance of our method always increases with the length of the hash codes increasing over all datasets . 
The59 23242526272829210code length0.40.50.60.70.80.91Pre@100RCV1 23242526272829210code length0.20.30.40.50.60.70.80.9Reuters 23242526272829210code length00.10.20.30.40.50.60.720Newsgroups 23242526272829210code length0.40.450.50.550.60.650.70.750.80.85TMCLSHSHSTHAGHVDSHBCTH Figure 2 : Precision@100 curve on four datasets with hash codes length from 8 to 1024 . 
reason is that our approach , which introduces the bit - balanced and the bit - uncorrelated constraints , can better keep independent semantic for all bits . 
3.6 Qualitative Analysis Figure 3 : Visualization of the 1024 - dimensional document latent semantic vectors by BCTH on the 20Newsgroup dataset using t - SNE . 
To evaluate whether our presented BCTH model can preserve the original documents ‚Äô semantics , we visualize the documents ‚Äô low - dimensional representations on the 20Newsgroups dataset in this section . 
In particular , the hash codes , obtained by BCTH , can be regarded as the latent semantic vectors of documents . 
We use t - SNE10tool to generate the scatter plots through 1024 - bit hash codes . 
Figure 3 shows the results . 
Different colors represent different categories based on the ground truth . 
As we can see from Ô¨Ågure 3 , BCTH generates wellseparated clusters with each corresponding to a true category . 
It shows that our method can effectively learn low - dimensional representations for documents . 
3.7 Impact of Parameters Our method is involved with a critical parameter   , which is used to control the bit - balanced and the bit10https://lvdmaaten.github.io/tsne/Datasets  =0  =0.001  =0.01  =0.1 RCV1 0.8476 0.8641 0.8526 0.8269 Reuters 0.7577 0.7669 0.7668 0.7532 20Newsgroups 0.5528 0.5534 0.5464 0.5502 TMC 0.7073 0.7172 0.7070 0.7025 Table 3 : The effect of  on four datasets with 64hashing bits . 
uncorrelated constraints . 
We here study the impact of hyper - parameter  in this section . 
Table 3 shows the results , which are obtained by using 64 hash bits . 
From this table , we can Ô¨Ånd that : ( 1 ) With  varying from 0 to 0.1 , BCTH is able to achieve relatively desirable results over all four datasets , which means that  is universally applicable ; ( 2 ) With  set to 0.001 , BCTH obtains the optimal result , and therefore , 0.001 is set as the default value for our method . 
3.8 Convergence Speed 1 3 5 7 9 11 13 15Iteration0400800120016002000Loss / bitsBCTHvariant8bits16bits32bits64bits128bits1 3 5 7 9 11 13 15Iteration0400800120016002000Loss / bitsBCTH8bits16bits32bits64bits128bitsFigure 4 : Convergence curve of the loss on the 20Newsgroups . 
In order to evaluate the convergence performance of our proposed BCTH algorithm , we performed convergence experiments on the 20Newsgroups dataset . 
Considering the different loss scales produced under different hash bits , we consider the ratio of the loss to the hash length to make60 it comparable at the same scale for different iterations . 
Note that , in this set of experiments , we also test one variant of BCTH methods , which is known asBCTH variant , which calculates the conditional probability through Eq . 
( 2 ) . 
The result is reported in Ô¨Ågure 4 , and we can Ô¨Ånd that : ( 1 ) both the BCTH and the BCTH variant converge after approximately 10iterations , and therefore , 10is set as the default value for our method ; ( 2 ) the convergence effect of BCTH is better than BCTH variant , which has a lower loss . 
This demonstrates that BTCH is effective . 
4 Related Work Hashing methods can be divided into dataindependent methods and data - dependent methods ( Chang et al . 
, 2012 ) . 
The well - known dataindependent methods include locality sensitive hashing ( LSH ) ( Datar et al . 
, 2004 ) and its variants . 
Data - dependent hashing methods are also known as learning to hash ( L2H ) methods by learning a hash function from data ( Li et al . 
, 2016 ) . 
At present , the main L2H methods ( Wang et al . 
, 2018 ) can be divided into three categories : pairwise similarity preserving , multiwise similarity preserving , and implicit similarity preserving . 
The pairwise similarity - preserving methods aim to build a pairwise similarity matrix between two points , such as spectral hashing ( SH ) ( Weiss et al . 
, 2008 ) , hashing with graphs ( AGH ) ( Liu et al . 
, 2011 ) , discrete graph hashing ( DGH ) ( Liu et al . 
, 2014 ) , fast supervised hashing ( FastH ) ( Lin et al . 
, 2014a ) and column - sampling - based discrete supervised hashing ( COSDISH ) ( Kang et al . 
, 2016 ) . 
The multiwise similarity - preserving is similar to pairwise similarity , which uses three or more samples as a group to deÔ¨Åne generalized similarity measures ( Norouzi et al . 
, 2012 ; Wang et al . 
, 2013a ) . 
The implicit similarity - preserving methods maintain the similarity in an equivalent manner that adopts the idea of preserving the similarity of local neighbors ( Irie et al . 
,2014 ) . 
Compared with this line of works , although our work also focuses on the nearest neighbor search , our work is different from theirs since ( 1 ) most of these works focus on images data , and ( 2 ) Bayesian Clustering is not covered in these works . 
Another line of works discuss text hashing , is related to our work since our work also aims to learn binary code from documents effectively . 
For example , ( Zhang et al . 
, 2010 ) presented the Self - TaughtHashing ( STH ) method for efÔ¨Åciently learning semantic hashing . 
( Zhang et al . 
, 2010 ) incorporated both the tag information and the similarity information from probabilistic topic modeling . 
However , many of these models rely on pairwise similaritypreserving technique , which the time complexity is unavoidable O(m2)where m is the number of documents . 
On the other hand , researchers have attempted to study text hashing ( Xu et al . 
, 2015 ) via deep neural networks owing to the success of deep learning . 
For example , ( Chaidaroon and Fang , 2017 ) introduces a latent factor for documents to capture the semantic information . 
( Kalchbrenner et al . 
,2014 ) proposed an end - to - end Neural Architecture for Semantic Hashing ( NASH ) , which treats the hashing codes as latent variables . 
Compared to this line of works , our work shares several common features : ( 1 ) our work also learns hashing by introducing latent factor , and ( 2 ) our work also aims to the issues related to text hashing . 
Nevertheless , our work differs from theirs in several features : ( 1 ) most of these works are based on complex nonlinear functions like convolutional neural networks , and training time complexity is enormous , and ( 2 ) Bayesian Clustering is not covered in these works . 
In this paper , we make the Ô¨Årst attempts to utilize Bayesian Clustering for text hashing and gain training time ‚Äôs linear complexity . 
5 Conclusion This paper presents a general learning framework that utilizes multiple Bayesian Clusterings jointly for text hashing . 
We introduce two constraints to make the hash code effectively . 
SpeciÔ¨Åcally , the bit - balanced constraint is employed to maximize the amount of information in each bit , and the bituncorrelated constraint is adopted to keep the independence among all bits . 
The time complexity of our method is linear . 
Based on four widelyused datasets , the experiment results demonstrate that BCTH is competitive compared with current competitive baselines from the perspective of both precision and training speed . 
Abstract Recently , BERT has become an essential ingredient of various NLP deep models due to its effectiveness and universal - usability . 
However , the online deployment of BERT is often blocked by its large - scale parameters and high computational cost . 
There are plenty of studies showing that the knowledge distillation is efÔ¨Åcient in transferring the knowledge from BERT into the model with a smaller size of parameters . 
Nevertheless , current BERT distillation approaches mainly focus on taskspeciÔ¨Åed distillation , such methodologies lead to the loss of the general semantic knowledge of BERT for universal - usability . 
In this paper , we propose a sentence representation approximating oriented distillation framework that can distill the pre - trained BERT into a simple LSTM based model without specifying tasks . 
Consistent with BERT , our distilled model is able to perform transfer learning via Ô¨Åne - tuning to adapt to any sentencelevel downstream task . 
Besides , our model can further cooperate with task - speciÔ¨Åc distillation procedures . 
The experimental results on multiple NLP tasks from the GLUE benchmark show that our approach outperforms other taskspeciÔ¨Åc distillation methods or even much larger models , i.e. , ELMO , with efÔ¨Åciency well - improved . 
1 Introduction As one of the most important progress in the Natural Language Processing Ô¨Åeld recently , the Bidirectional Encoder Representation from Transformers ( BERT ) ( Devlin et al . 
, 2019 ) has been proved to be effective in improving the performances of various NLP tasks by providing a powerful pre - trained language model based on large - scale unlabeled corpora . 
Recent studies have shown that BERT ‚Äôs capability can be further enhanced by utilizing deeper architectures or performing the pre - training on larger ‚àó * Equal contribution during the internship at Tencent.corpora with appropriate guidance ( Radford et al . 
, 2019 ; Yang et al . 
, 2019 ; Liu et al . 
, 2019b ) . 
Despite its strength in building distributed semantic representations of sentences and supporting various NLP tasks , BERT holds a huge amount of parameters that raises the difÔ¨Åculty of conducting online deployment due to its unsatisfying computational efÔ¨Åciency . 
To address this issue , various studies have been done to utilize the knowledge distillation ( Hinton et al . 
, 2015 ) for compressing BERT and meanwhile keep its semantic modeling capability as much as possible ( Chia et al . 
, 2019 ; Tsai et al . 
, 2019 ) . 
The distilling methodologies include simulating BERT with a much smaller model ( e.g. , LSTM ) ( Tang et al . 
, 2019b ) and reducing some of the components , such as transformers , attentions to obtain the smaller BERT based model ( Sun et al . 
, 2019 ; Barkan et al . 
, 2019 ) . 
Nevertheless , the current methods highly rely on a labeled dataset upon a speciÔ¨Åed task . 
Firstly , BERT is Ô¨Åne - tuned on the speciÔ¨Åed task to get the teaching signal for distillation , and the student model with simpler architectures attempts to Ô¨Åt the task - speciÔ¨Åed Ô¨Åne - tuned BERT afterward . 
Such methodologies can achieve satisfying results by capturing the task - speciÔ¨Åed biases ( McCallum and Nigam , 1999 ; Godbole et al . 
, 2018 ; Min et al . 
, 2019 ) , which are inherited by the tuned BERT ( Niven and Kao , 2019 ; McCoy et al . 
, 2019 ) . 
Unfortunately , the powerful generalization nature of BERT tends to be lost . 
Apparently , distilling BERT ‚Äôs original motivation is to obtain a lightweight substitution of BERT for online implementations , and BERT ‚Äôs general semantic knowledge , which plays a signiÔ¨Åcant role in some NLP tasks like sentence similarity quantiÔ¨Åcation , is expected to be maintained accordingly . 
Meanwhile , for many NLP tasks , manual labeling is quite a high - cost work , and large amounts of annotated data can not be guaranteed to obtain . 
Thus , it70 is of great necessity to compress BERT with the non - task - speciÔ¨Åc training procedure on unlabeled datasets . 
For achieving the Non - task - speciÔ¨Åc Distillation from BERT , this paper proposes a distillation loss function to approximate sentence representations by minimizing the cosine distance between the sentence representation given by the student network and the one from BERT . 
As a result , a student network with a much smaller scale of parameters is produced . 
Since the distilling strategy purely focuses on the simulation of sentence embeddings from BERT , which is not directly related to any speciÔ¨Åc NLP task , the whole training procedure takes only a large amount of sentences without any manual labeling work . 
Similar to BERT , the smaller student network can also perform transfer learning to any sentence - level downstream tasks , such as text classiÔ¨Åcation and sentence matching . 
The proposed methodology is evaluated on the open platform of General Language Understanding Evaluation ( GLUE ) ( Wang et al . 
, 2019 ) , including the Single Sentence ( SST-2 ) , Similarity and Paraphrase ( QQP and MRPC ) , and Natural Language Inference ( MNLI ) tasks . 
The experimental results show that our proposed model outperforms the models distilled from a BERT Ô¨Åne - tuned on a speciÔ¨Åc task . 
Moreover , our model inferences more efÔ¨Åciently than other transformer - based distilled models . 
2 Related Works With the propose of ELMo ( Peters et al . 
, 2018 ) , various studies take the representation given by pretrained language models as additional features to improve the performances . 
Howard and Ruder ( 2018 ) propose Universal Language Model Finetuning ( ULMFiT ) , an effective transfer learning method that can be applied to any NLP task and accordingly , using pre - trained language models in downstream tasks became one of the most exciting directions . 
On this basis , developing with deeper network design and more effective training methods , pre - trained models ‚Äô performances improved continuously ( Devlin et al . 
, 2019 ; Radford et al . 
, 2019 ; Yang et al . 
, 2019 ; Liu et al . 
, 2019b ) . 
Since the release of BERT ( Devlin et al . 
, 2019 ) , the stateof - the - art ( SOTA ) results on 11 NLP tasks have been produced consequently . 
With the improvement in performances , the computing cost increases , and the inference procedure becomes slower accordingly . 
Thus , various stud - ies focused on the model compression upon BERT . 
Among the most common model compression techniques , the knowledge distillation ( Hinton et al . 
, 2015 ) has been proven to be efÔ¨Åcient in transferring the knowledge from large - scaled pre - trained language models into another one ( Liu et al . 
, 2019a ; Wang et al . 
, 2020 ; Jiao et al . 
, 2019 ; Sun et al . 
, 2020 ) . 
With the help of proposed distillation loss , Sun et al . 
( 2019 ) compressed BERT into fewer layers by shortening the distance of internal representations between student and teacher BERTs . 
For the sentence - pair modeling , Barkan et al . 
( 2019 ) found the cross - attention function across sentences is consuming and tried to remove it with distillation on sentence - pair tasks . 
Different from these studies distilling BERT into transformer - based models , Chia et al . 
( 2019 ) proposed convolutional student architecture to distill GPT for efÔ¨Åcient text classiÔ¨Åcation . 
Moreover , focusing on the sequence labeling tasks , ( Tsai et al . 
, 2019 ) derived a BiLSTM or MiniBERT from BERT via standard distillation procedure to simulate the prediction on each token . 
Besides , Tang et al . 
( 2019a , b ) proposed to distill BERT into a BiLSTM based model with penalizing the mean square error between the student ‚Äôs logits and the ones given by BERT as the objective on speciÔ¨Åc tasks , and introduced various data augmentation methods during distillation . 
3 Method As introduced in Section 1 , our proposed method consists of two procedures . 
Firstly , we distill BERT into a smaller student model via approximating the representation of sentences given by BERT . 
Afterward , similar to BERT , the student model can be Ô¨Åne - tuned on any sentence - level task , such as text classiÔ¨Åcation and sentence matching . 
3.1 Distillation Procedure Suppose x={w1 , w2 , ¬∑ ¬∑ ¬∑ , wi,¬∑¬∑¬∑wn|i‚àà[1 , n ] } stands for a sentence containing ntokens ( wiis the i - th token of x ) , and let T : x‚ÜíTx‚ààRdbe the teacher model which encodes xintod - dimensional sentence embedding Tx , the goal of the sentence approximation oriented distillation is to train a student model S : x‚ÜíSx‚ààRdgenerating Sxas the approximation of Tx . 
In our proposed distillation architecture , as shown in Figure 1a , we take the BERT as the teacher model T , and the hidden representation Cis extracted from the top transformer layer upon71 ECLS EN ‚ãØE2 [ CLS ] Tok 1 Tok 2 Tok N‚ãØC T1 TN ‚ãØT2 BERTloss=1 2(1 -C‚ãÖH CH ) E1 Single Sentence‚ãØ ‚ãØE1 Tok 1E2 Tok 2EN Tok NLSTM LSTM ‚ãØ LSTMLSTM LSTM ‚ãØ LSTM Single SentenceFC layerH ùêÅùê¢ùêãùêíùêìùêåùêíùêëùêÄ(a ) Distilling BERT based on Representation Approximation . 
FC layerClass Label H ‚ãØ Tok 1 Tok 2 Tok N Single SentenceùêÅùê¢ùêãùêíùêìùêåùêíùêëùêÄ ( b ) Tuning ( sentence classiÔ¨Åcation ) . 
Matching FC layer ‡∑©HH H‚àí‡∑©HH‚äô‡∑©H H ‚ãØ Tok 1 Tok 2 Tok N Sentence 1ùêÅùê¢ùêãùêíùêìùêåùêíùêëùêÄ‡∑©H ‚ãØ Tok 1 Tok 2 Tok M Sentence 2ùêÅùê¢ùêãùêíùêìùêåùêíùêëùêÄ(c ) Tuning ( sentence - pair oriented ) . 
Figure 1 : The illustration of the proposed BERT distillation architecture including the distilling and tuning procedures . 
Sub-Ô¨Ågure ( a ) demonstrates the distillation procedure taking BERT as the teacher model and BiLSTM as the student model , with the objective of approximating the sentence representations given by BERT . 
( b ) and ( c ) show two types of Ô¨Åne - tuning frameworks , in which ( b ) addresses the sentence classiÔ¨Åcation task with the single sentence as the input , and ( c ) goes for the sentence - pair - oriented tasks , i.e. , sentence similarity quantiÔ¨Åcation , natural language inference . 
the [ CLS]1token as Tx . 
For the student model , a standard bidirectional LSTM ( BiLSTM ) is Ô¨Årst employed to encode the sentence into a Ô¨Åxed - size vectorH. After that , a fully connected layer without bias terms is built upon the BiLSTM layer to map Hinto a d - dimensional representation , followed by atanh activation that normalizes the values of previous representation between -1 and 1 as the Ô¨ÅnalSx . 
As our non - task - speciÔ¨Åc distillation task has no labeling data , and the signal given by the teacher is a real value vector , it is not feasible to minimize the cross - entropy loss over the soft labels and ground truth labels ( Sun et al . 
, 2019 ; Barkan et al . 
, 2019 ; Tang et al . 
, 2019b ) . 
On this basis , we propose an adjusted cosine similarity between the two real value vectors TxandSxto perform the sentence representation approximation . 
Our distillation objective is computed as follows : Ldistill = 1 2(1‚àíTx¬∑Sx /bardblTx / bardbl / bardblSx / bardbl ) ( 1 ) Heretanh is chosen as the activation function since most values ( more than 98 % according to our statics ) in Txobtained from BERT are within range of tanh ( -1 to 1 ) . 
The choice of using cosine similarity based loss is mainly based on the following two considerations . 
Firstly , since 2 % values in Txare outside the range of [ -1 , 1 ] , it is more reasonable 1[CLS ] is a special symbol added in front of other tokens in BERT , and the Ô¨Ånal hidden state corresponding to this token is usually used as the aggregate sequence representation.to use a scalable measurement , such as cosine similarity , to deal with these deviations . 
Secondly , it is meaningful to compute the cosine similarity between sentence embeddings given by BERT ( Xiao , 2018 ) . 
Overall , after the distillation procedure , we obtained a BiLSTM based ‚Äú BERT ‚Äù , which is smaller in parameter scale and more efÔ¨Åcient in generating a sentence ‚Äôs semantic representation . 
Distilling data As our distillation procedure needs no dependency on sentence type or labeling resources but only standard sentences available everywhere , the distillation data selection follows the existing literature on language model pre - training as well as BERT . 
We use the English Wikipedia to perform the distillation . 
Furthermore , as the proposed method focus on the sentence representation approximation , the document is segmented into sentences using spacy ( Honnibal and Montani , 2017 ) . 
3.2 Fine - tuning the Student Model The Ô¨Åne - tuning on sentence - level tasks is straightforward . 
The downstream tasks discussed in this paper can be summarized as type judgment on a single sentence and predicting the relationship between two sentences ( same as all GLUE tasks ) . 
Figure 1b illustrates the model architecture for single sentence classiÔ¨Åcation tasks . 
The student model S is utilized to provide sentence representation . 
After that , a multilayer perceptron ( MLP ) based classiÔ¨Åer72 using Relu as activation of hidden layers is applied for the speciÔ¨Åc task . 
For the sentence pair tasks , as shown in Figure 1c , the representations Hand ÀúHfor the sentence pair are obtained by transforming two sentences into two BiLSTM based student models with shared weights respectively . 
Then , following the baseline BiLSTM model reported by GLUE ( Wang et al . 
, 2019 ) , we apply a standard concatenate - compare operation between the two sentence embeddings and get an interactive vector as[H,ÀúH,|H‚àíÀúH| , H‚äôÀúH ] , where the‚äôdemotes for the element - wise multiplication . 
Then , same as the single sentence task , an MLP based classiÔ¨Åer is built upon the interactive representation . 
For both types of tasks , MLP layers are initialized randomly , and the rest parameters are inherited from the distilled student model . 
Meanwhile , all parameters are optimized through the training procedure for the speciÔ¨Åc task . 
4 Experimental Setups 4.1 Datasets & Evaluation Tasks To evaluate the performance of our proposed non - task - speciÔ¨Åc distilling method , we conduct experiments on three types of sentence - level tasks : sentiment classiÔ¨Åcation ( SST-2 ) , similarity ( QQP , MRPC ) , and natural language inference ( MNLI ) . 
All the tasks come from the GLUE benchmark ( Wang et al . 
, 2019 ) . 
SST-2 Based on the Stanford Sentiment Treebank dataset ( Socher et al . 
, 2013 ) , the SST-2 task is to predict the binary sentiment of a given single sentence . 
The dataset contains 64k sentences for training and remains 1k for testing . 
QQP The Quora Question Pairs2dataset consists of pairs of questions , and the corresponding task is to determine whether each pair is semantically equivalent . 
MNLI The Multi - Genre Language Inference Corpus ( Williams et al . 
, 2018 ) is a crowdsourced collection of sentence pairs with textual entailment annotations . 
There are two sections of the test dataset : matched ( in - domain , noted as MNLI - m ) and mismatched ( cross - domain , noted as MNLImm ) . 
MRPC The Microsoft Research Paraphrase Corpus ( Dolan and Brockett , 2005 ) is similar to the 2https://www.quora.com/q/quoradata/ First - Quora - Dataset - Release - Question - PairsQQP dataset . 
This dataset consists of sentence pairs with binary labels denoting their semantic equivalence . 
4.2 Model Variations BERT ( Devlin et al . 
, 2019 ) with two variants : BERT BASE and BERT LARGE , containing 12 and 24 layers of Transformer respectively . 
ELMO Baseline ( Wang et al . 
, 2019 ) is a BiLSTM based model , taking ELMo ( Peters et al . 
, 2018 ) embeddings in place of word embeddings . 
BERT - PKD ( Sun et al . 
, 2019 ) proposes a patient knowledge distillation approach to compress BERT into a BERT with fewer layers . 
BERT 3 - PKD and BERT 6 - PKD stand for the student models consisting of 3 and 6 layers of Transformer , respectively . 
DSE ( Barkan et al . 
, 2019 ) is a sentence embedding model based on knowledge distillation from cross - attentive models . 
For each single sentence modeling , the 24 - layers BERT is employed . 
BiLSTM KD(Tang et al . 
, 2019b ) introduces a new distillation objective to distill a BiLSTM based model from BERT for a speciÔ¨Åc task . 
BiLSTM KD+TS ( Tang et al . 
, 2019a ) donates the distilling procedure performed with the proposed data augmentation strategies . 
BiLSTM SRA stands for the Sentence Representation Approximation based distillation model proposed in this paper . 
BiLSTM SRA + KD donates performing knowledge distillation method proposed by Tang et al . 
( 2019b ) during Ô¨Åne - tuning on a speciÔ¨Åc task , and BiLSTM SRA + KD + TS demonstrates using the same augmented dataset to perform the distillation . 
4.3 Hyperparameters For the student model in our proposed distilling method , we employ the 300 - dimension GloVe ( 840B Common Crawl version ; Pennington et al . 
, 2014 ) to initialize the word embeddings . 
The number of hidden units for the bi - directional LSTM is set to 512 , and the size of the task - speciÔ¨Åc layers is set to 256 . 
All the models are optimized using Adam ( Kingma and Ba , 2015 ) . 
In the distilling procedure , we choose the learning rate as 1√ó10‚àí3with the batch size=1024 . 
During Ô¨Ånetuning , the best learning rate on the validation set is picked from{2,3,5,10}√ó10‚àí4 . 
For the data73 # ModelsSST-2 QQP MNLI - m / mm MRPC Acc F 1 / Acc Acc F 1 / Acc 1 BiLSTM ( report by GLUE ) 85.9 61.4 / 81.7 70.3 / 70.8 79.4 / 69.3 2 BiLSTM ( report by Tang et al . 
( 2019b ) ) 86.7 63.7 / 86.2 68.7 / 68.3 80.9 / 69.4 3 BiLSTM ( our implementation ) 84.5 60.3 / 81.6 70.8 / 69.4 80.2 / 69.7 4 ELMO Baseline ( Wang et al . 
, 2019 ) 90.2 65.6 / 85.7 72.9 / 73.4 84.9 / 78.0 5 BERT BASE ( Devlin et al . 
, 2019 ) 93.5 71.2 / 89.2 84.6 / 83.4 88.9 / 84.8 6 BERT LARGE ( Devlin et al . 
, 2019 ) 94.9 72.1 / 89.3 86.7 / 85.9 89.3 / 85.4 7 DSE ( Barkan et al . 
, 2019 ) - 68.5 / 86.9 80.9 / 80.4 86.7 / 80.7 8 BERT 6 - PKD ( Sun et al . 
, 2019 ) 92.0 70.7 / 88.9 81.5 / 81.0 85.0 / 79.9 9 BERT 3 - PKD ( Sun et al . 
, 2019 ) 87.5 68.1 / 87.8 76.7 / 76.3 80.7 / 72.5 10 BiLSTM KD(Tang et al . 
, 2019a ) 88.4 - / - - / - 78.0 / 69.7 11 BiLSTM SRA(Ours ) 90.0 64.4 / 86.2 72.6 /72.5 83.1 /75.1 12 BiLSTM SRA + KD 90.2 67.7 /87.8 72.3 / 72.0 80.2 / 72.8 13 BiLSTM KD+TS ( Tang et al . 
, 2019b ) 90.7 68.2 / 88.1 73.0 / 72.6 82.4 / 76.1 14 BiLSTM SRA + KD + TS 91.1 68.4 /88.6 73.0 /72.9 83.8 /76.2 Improvements obtained by performing different knowledge distillations 15 PKD ( Sun et al . 
, 2019 ) +1.1 +2.3 / +0.9 +1.9 / +2.0 +0.2 / -0.1 16 KD ( Tang et al . 
, 2019a ) +1.7 - / - - / - -2.9 / +0.3 17 SRA ( Ours ) +5.5 +4.1 / +4.6 +1.8 / +3.1 +2.9 /+5.4 18 SRA ( Ours ) + KD +5.7 +7.4 /+6.2 +1.5 / +2.6 0 . 
/ +3.1 19 KD+TS ( Tang et al . 
, 2019a ) +4.0 +4.5 / +1.9 +4.3 /+4.2 +1.5 / +6.7 20 SRA ( Ours ) + KD+TS +6.6 +8.1 /+7.0 +2.2 / +3.5 +3.6 / +6.5 Table 1 : Evaluation results with scores given by the ofÔ¨Åcial evaluation server3 . 
augmentation , we use the rule - based method originally suggested by Tang et al . 
( 2019b ) . 
Notably , on the SST-2 and MRPC dataset , we stop data augmenting when the transfer set achieves 800 K samples following the setting of their follow - up research ( Tang et al . 
, 2019a ) . 
Besides , inspired by the comparisons in the research of Sun et al . 
( 2019 ) , we Ô¨Ånd BERT BASE can provide more instructive representations than BERT LARGE . 
So that , we chose BERT BASE as our teacher model to train the non - task - speciÔ¨Åed BiLSTM SRA . 
5 Results and Analysis 5.1 Model Performance Analysis For a comprehensive experiment analysis , we collect data and implement comparative experiments on various published BERT and BERT - distillation methods . 
Table 1 shows the results of our proposed BiLSTM SRAand the baselines on the four datasets . 
All models in the Ô¨Årst block ( row 1 - 6 ) belong to base methods without implementing distillation , the second ( row 7 - 9 ) and third ( row 10 - 12 ) blocks 3https://gluebenchmark.com/leaderboardshow the performances of distillation models using BERT and BiLSTM structures , respectively . 
Moreover , the fourth block ( row 13 - 14 ) displays the inÔ¨Çuences of textual data augmentation approach on our BiLSTM SRAand BiLSTM KDdistillation baseline . 
The last two blocks contain the results of pure improvements obtained by different distillation methods . 
To analyze the effectiveness of BiLSTM SRAthoroughly , we break down the analyses into the following two perspectives . 
5.1.1 Comparison Between Models Taking those non - distillation methods in the Ô¨Årst block as references , BiLSTM SRAperforms on par with ELMO on all tasks . 
Especially , BiLSTM SRA + KD + TS outperforms the ELMO baseline by approximately 3 % on QQP and 1 % on SST2 ( row 14 vs 4 ) . 
Such fact shows our compressed ‚Äú BERT ‚Äù can provide as good pre - trained representations as ELMO on the sentence - level tasks . 
For those distillation methods , both our model and BiLSTM KDdistill knowledge from BERT into a simple BiLSTM based model , while BERT - PKD focuses on distilling with the BERT of fewer lay-74 ers . 
Despite the powerful BERT based student model and large - scale parameters used by BERTPKD , our proposed BiLSTM SRAstill outperforms BERT 3 - PKD on SST-2 and MRPC dataset ( row 12 vs. 9 ) . 
For BiLSTM KD , it proposes a rule - based textual data augmentation approach ( noted as TS ) to construct transfer sets for the task - speciÔ¨Åc knowledge distillation . 
We also employ such method upon BiLSTM SRA + KD . 
With and without the data augmentation , BiLSTM SRA consistently outperforms BiLSTM KDon all tasks ( row 12 vs 10 ; row 14 vs 13 ) . 
Coworking with the standard knowledge distillation and data augmentation methods , our proposed model is sufÔ¨Åcient to distill semantic representation modeled from pre - training tasks as well as the task - speciÔ¨Åc knowledge included in a Ô¨Åne - tuned BERT . 
Besides , DSE ‚Äôs overall architecture is similar to our method for modeling the sentence matching task , except DSE does not reduce the parameter size because it employs the pretrained BERT LARGE to give sentence representations . 
Thus , on the sentence - pair level tasks , DSE somehow is an upper bound of the distilled models without utilizing any cross attention to model the two sentences ‚Äô interaction . 
Comparing with DSE achieved an averaged 80.7 score on all sentencepair level tasks , BiLSTM SRA + KD + TS can also obtain 77.2 that only 3.5 points lower ( row 7 vs. 14 ) . 
Analyzing from this fact , our proposed model has distilled a much smaller ‚Äú BERT ‚Äù with acceptable performances . 
5.1.2 Distillation Effectiveness Because in each paper , the performances of student models used for distillation vary from each other . 
To further evaluate the distillation effectiveness , we also report each distillation method ‚Äôs improvement upon the corresponding student directly trained without distillation ( in row 15 - 20 ) . 
It can be observed that SRA improves the scores by over 3.9 % on average , while PKD and KD only provide less than 1.2 % increase ( row 17/16 vs. 15 ) . 
Since our distillation method is unrelated to speciÔ¨Åc tasks , KD can also be performed upon BiLSTM SRAduring Ô¨Åne - tuning on a given dataset . 
This operation provides a notable boost on the QQP task , but damages the performance on both MNLI and MRPC datasets ( row 17 vs. 18 ) . 
We attribute these differences to the following aspects : a ) the QQP dataset has more obvious task - speciÔ¨Åed bi - Models # of Par . 
Inference Time BERT LARGE 309 ( 64x ) 1461.9 ( 54.4x ) BERT BASE 87 ( 18x ) 479.7 ( 17.7x ) ELMO 93 ( 19x ) - ( 23.7x ) BERT 3 - PKD 21 ( 4x ) - ( 4.8x ) BERT 6 - PKD 42 ( 9x ) - ( 9.2x ) DSE 309 ( 64x ) - ( 109.1x ) BiLSTM KD 2.4 ( 0.5x ) 31.9 ( 1.2x ) BiLSTM SRA 4.8 ( 1x ) 26.8 ( 1x ) Table 2 : Comparisons of model size and inference speed . 
# of Par . 
denotes the number of millions of parameters , and the inference time is in seconds . 
The factors inside the brackets are computed comparing to our proposed model . 
ases during the sampling process4 . 
A pre - trained BERT can not learn such biases ; b ) a Ô¨Åne - tuned BERT on the MNLI can not further provide more easy - to - use information to guide the student training after performing SRA ; c ) MRPC does not include enough data to complete KD , which is also indicated by the decreased F1 score shown in row 16 in Table 1 . 
These phenomena reÔ¨Çect that the pre - distillation without paying attention to a speciÔ¨Åc task can help to learn more useful semantic information from the teacher model . 
Different from obtaining the best results on the MNLI dataset , SRA+KD+TS brings few improvements compared to KS+TS ( row 19 vs. 20 ) . 
We attribute this to the difference in the results of pure student BiLSTM between our implementation and the one of Tang et al . 
( 2019b ) , though our scores are more constant with the baselines given by the GLUE benchmark ( Wang et al . 
, 2019 ) . 
5.2 Model EfÔ¨Åciency Analysis To compare the inference speeds of different models , we also implement experiments on 100k samples from the QQP dataset . 
The results are shown in Table 2 . 
All the inference procedures are performed on a single P40 GPU with a batch size of 1024 , respectively . 
As the inference time is affected by the test machine ‚Äôs computing power , for fair comparisons with ELMO , BERT 3 - PKD , BERT 6 - PKD , and DSE , we inherit the speed - up factors from previous papers . 
Besides , the numbers of parameters reported in Table 2 exclude those 4https://www.kaggle.com/c/ quora - question - pairs / discussion/32819 # latest-18949375 Models 20 % 30 % 50 % 100 % BERT LARGE 91.9 92.5 93.5 93.7 BiLSTM 80.7 81.0 83.6 84.5 BiLSTM KD 81.9 83.2 84.8 86.3 BiLSTM SRA 85.9 87.3 88.1 89.2 Table 3 : The accuracy scores evaluated on the SST-2 validation set . 
The models are trained with different proportions of the training data . 
from the embedding layers , since such components do not affect the inference speed and are positively related to the vocabulary sizes , i.e. , usually few words appeared for a speciÔ¨Åc task . 
From the results shown in Table 2 , it can be observed that the BiLSTM based distilled models have fewer parameters than BERT , ELMO , as well as the other transformer - based models . 
Compared to the lightest model , both the BERT BASE and ELMO are around 20 times larger in parameter size and 20 times slower in inference speed . 
Even the smallest transformer based model BERT 3PKD is also four times larger than our proposed BiLSTM SRA . 
Comparing with BiLSTM KD , although our proposed BiLSTM SRAis larger in parameter size due to the restriction of the sentence embedding ‚Äôs dimension given by the teacher BERT , it stills inferences more efÔ¨Åciently . 
This is mainly due to the fact that the more hidden units in BiLSTM SRAare more accessible to calculated in parallel by the GPU core , while the larger word embedding size in BiLSTM KDslows down its inference efÔ¨Åciency . 
In conclusion , the cost and production per second of BiLSTM KDand BiLSTM SRA are within the same scale , but our method achieves better results on GLUE tasks according to the comparison shown in Table 1 . 
5.3 InÔ¨Çuence of Task - speciÔ¨Åc Data Size Since pre - trained language models have wellinitialized parameters and only learn a few parameters from scratch , these models usually converge faster and are less dependent on large - scale annotations . 
Correspondingly , the non - task - speciÔ¨Åc distillation method proposed in this paper also aims to obtain a compressed pre - trained BERT and keep these desirable properties . 
To evaluate it , in this section , we discuss the inÔ¨Çuence of the task - speciÔ¨Åc training data and learning iterations on the performance of our model and the others . 
As illustrated in Table 3 , we experiment in train 0.7 0.75 0.8 0.85 0.9 0.95   1   3   5   7   9Validation Accuracy Iteration Number BiLSTM + 30 % dataBiLSTM + 100 % data BiLSTM SRA   + 30 % dataBiLSTM SRA   + 100 % dataFigure 2 : Learning curve on the QQP dataset . 
ing the models using different proportions of the dataset . 
BERT LARGE trained on the corresponding data stands for the teacher model of each BiLSTM KD . 
No doubt , all the models can achieve better results using more training data , while BERT performs the best . 
BERT even successfully predicts 91.9 % of validation samples under only 20 % training data . 
Comparing with the pure BiLSTM models , the BiLSTM KDmodels slightly improve the performances by 1 % ‚àº2 % , whereas BiLSTM SRA outperforms the best BiLSTM model as well as the BiLSTM KDtrained with 20 % and 30 % percent data respectively . 
Besides , similar to BERT , the difference of accuracy between BiLSTM SRAtrained with 20 % and the one using 100 % corpus is relatively small . 
This phenomenon indicates that our model converges faster and is less dependent on the amount of training data for downstream tasks . 
Such conclusions are also reÔ¨Çected in the comparison in Figure 2 of the models ‚Äô learning curves on QQP . 
Even though QQP is a large dataset to train a good BiLSTM model , it can be observed that BiLSTM SRAtrained with 30 % data performs equivalent to BiLSTM using the whole corpus . 
Moreover , using 100 % training data , BiLSTM SRAeven outperforms the converged BiLSTM after the Ô¨Årst epoch . 
Besides , all the BiLSTM SRAmodels converge in much fewer epochs . 
5.4 InÔ¨Çuence of Distilling Data Size Despite the task - speciÔ¨Åed data , Wikipedia corpus is used in the distillation procedure of our proposed method . 
We also pre - train different BiLSTM SRA base models using{1 , 2 , 4}million Wikipedia data , and the corresponding Ô¨Åne - tuning performances on SST-2 and MNLI are reported in Table 4 . 
It can be observed that both the performances of76 SizeDistillation SST-2 MNLI - m Loss Acc Acc 0 M - 84.5 70.23 1 M 0.0288 88.9 ( +4.4 ) 72.01 ( +1.78 ) 2 M 0.0257 89.3 ( +4.8 ) 72.09 ( +1.86 ) 4 M 0.0241 89.4 ( +4.9 ) 72.45 ( +2.22 ) Table 4 : The distillation losses on the Wikipedia validation set and the accuracy scores of the downstream tasks various with the distillation data sizes . 
BiLSTM SRAon SST-2 and MNLI are proportional to the distillation loss . 
This observation indicates the effectiveness of our proposed distillation process and objective . 
Besides , distilling with adequate data is sufÔ¨Åcient to produce more BERT - like sentence representations as well as achieve better performance on the downstream tasks . 
Nevertheless , different from the fact that more training data has a signiÔ¨Åcant beneÔ¨Åt in a particular task , four times the distilling data can only improve around 0.5 points on both SST-2 and MNLI - m tasks . 
Thus , our method does not require a vast amount of training data and a long training time to obtain good sentence representations . 
Furthermore , the second column ‚Äôs loss scores suggest BiLSTM SRAcan generate more than 95 % similar sentence embeddings with the ones given by BERT under the measure of the cosine similarity . 
5.5 Analysis on the Untuned Sentence Representations A notable characteristic of the pre - trained language models , such as ELMO , BERT , and certainly the non - task oriented distillation models , lies in the capability of providing sentence representations for quantifying similarities of sentences , without any tuning operation based on speciÔ¨Åc tasks . 
In this subsection , we conduct the comparisons among models by directly extracting their sentence embeddings without Ô¨Åne - tuning upon sentence similarity oriented tasks . 
Table 5 lists the results of models on the QQP dataset . 
It should be noted that , in this table , ELMO , BERT BASE ( CLS ) and BERT BASE ( averaged ) are introduced as the comparison basis , since they can give the SOTA untuned sentence representations for the similarity measurement . 
The comparison mainly focuses on the performances ofModels Acc F 1 ELMO 65.1 64.4 BERT BASE ( CLS ) 63.9 61.0 BERT BASE ( averaged ) 66.4 64.1 BiLSTM KD 56.3 56.6 BiLSTM SRA 62.9 61.0 Table 5 : Results of untuned sentence representing models on QQP dataset . 
our proposed BiLSTM SRAand BiLSTM KD . 
For a thorough comparison , we deÔ¨Åne the training objective of BiLSTM KDas Ô¨Åtting the cosine similarity score of the sentence pair directly given by the pretrained BERT BASE , which means both the teacher BERT and distilled models do not utilize the labels of QQP dataset . 
Even though the training goal of BiLSTM KDis more direct than BiLSTM SRA , it can be seen that our BiLSTM SRAoutperforms the former on the metrics . 
Furthermore , it achieves scores closed to those of BERT BASE . 
Besides , we can also observe that , for sentence similarity quantiÔ¨Åcation , averaging the context word embeddings as the sentence representation ( ELMO and BERT BASE ( averaged ) ) works better than taking the Ô¨Ånal hidden state corresponding to the [ CLS ] token ( BERT BASE ( CLS ) ) . 
6 Conclusions In this paper , we have presented a sentence representation approximating oriented method for distilling the pre - trained BERT model into a much smaller BiLSTM without specifying tasks , so as to inherit the general semantic knowledge of BERT for better generalization and universal - usability . 
The experiments conducted based on the GLUE benchmark have shown that our proposed nontask - speciÔ¨Åc distillation methodology can improve the performances on multiple sentence - level downstream tasks . 
From the experimental results , the following conclusions can be drawn : 1 ) for a speciÔ¨Åed task , our proposed distillation method can bring the 5 % improvement to the pure BiLSTM model on average ; 2 ) the proposed model can outperform the state - of - the - art BiLSTM based pre - trained language model , which contains much more parameters ; 3 ) compared to the task - speciÔ¨Åc distillation , our distilled model is less dependent on the corpus size of the downstream task with satisfying performances guaranteed.77 Abstract We propose a simple and effective method for incorporating word clusters into the Continuous Bag - of - Words ( CBOW ) model . 
SpeciÔ¨Åcally , we propose to replace infrequent input and output words in CBOW model with their clusters . 
The resulting cluster - incorporated CBOW model produces embeddings of frequent words and a small amount of cluster embeddings , which will be Ô¨Åne - tuned in downstream tasks . 
We empirically show our replacing method works well on several downstream tasks . 
Through our analysis , we show that our method might be also useful for other similar models which produce word embeddings . 
1 Introduction Word embeddings have been widely applied to various natural language processing ( NLP ) tasks . 
These embeddings can be pretrained on a large corpus and carry useful semantic information . 
One of the most well - known methods for obtaining word embeddings is based on Continuous Bag - of - Words ( CBOW ) ( Mikolov et al . 
, 2013a ) and there have been many research efforts to extend it . 
In this paper , we focus on incorporating word clusters into CBOW model . 
Each word cluster consists of words that function similarly . 
By aggregating such words , we can alleviate data sparsity , even though each of those words is infrequent . 
In the past few years , word clusters have been applied to various tasks , such as named - entity recognition ( Ritter et al . 
, 2011 ) , machine translation ( Wuebker et al . 
, 2013 ) and parsing ( Kong et al . 
, 2014 ) . 
Many word clustering algorithms can be applied to a raw corpus with different languages and help us obtain word clusters easily without additional language resources . 
In our method , we keep only very frequent words and replace the other words with their clusters for both input and output words in the CBOW model . 
This is motivated by the fact that word clusters are more reliable than infrequent words . 
Thus , only very frequent word embeddings and a small amount of cluster embeddings are produced as the output . 
When Ô¨Åne - tuning the trained embeddings on downstream tasks , the embeddings of infrequent words within one cluster are initialized by the embedding of their cluster to increase the coverage of pretrained word embeddings . 
Since word embeddings are usually trained on the large - scale dataset . 
For making clusters on the large - scale dataset , we choose bidirectional , interpolated , reÔ¨Åning , and alternating ( BIRA ) predictive exchange algorithm ( Dehdari et al . 
, 2016)1as our clustering method . 
Because BIRA was reported to be faster than many other methods . 
Notably , it can produce 800 clusters on 1 billion English tokens in 1.4 hours . 
We evaluate our cluster - incorporated word embeddings2on downstream tasks , in which Ô¨Ånetuning of word embeddings is involved . 
The evaluation for frequent words , for which our method also works well , on word similarity tasks can be found in appendix A. For the downstream tasks , we choose language modeling ( LM ) tasks , which are a fundamental task in NLP , as well as two machine translation ( MT ) tasks . 
To verify the effect of word clusters across different languages , 8 typologically diverse languages are further selected for the LM task . 
Finally , an analysis is provided for our method . 
In summary , our replacing method can be used to improve the embeddings of frequent and infrequent words , to reduce the number of word embeddings and to make training more effective . 
1We used ClusterCat ( https://github.com/jonsafari/clustercat ) as the implementation . 
2https://github.com/yukunfeng/cluster-cbow80 2 Related Work A number of related research efforts have been done to help to learn better word embeddings aiming at different aspects . 
For example , Neelakantan et al . 
( 2014 ) proposed an extension that learns multiple embeddings per word type . 
Ammar et al . 
( 2016 ) proposed methods for estimating embeddings for different languages in a single shared embedding space . 
There is also a lot of work that incorporates internal information of words , such as character - level information ( Chen et al . 
, 2015 ; Bojanowski et al . 
, 2017 ) and morpheme information ( Luong et al . 
, 2013 ; Qiu et al . 
, 2014 ) . 
Our research aims at another aspect and focuses on incorporating word clusters into the CBOW model , which has not been studied before . 
There have also been some previous researches that utilized word clusters for reducing the number of word embeddings . 
Botha et al . 
( 2017 ) used word clusters to reduce the network size for the part - of - speech tagging task . 
Shu and Nakayama ( 2018 ) attempted to compress word embeddings without losing performance by constructing the embeddings with a few basic vectors . 
Our goal is different from the previous work in that we attempt to learn better word embeddings and do not aim at reducing the parameters when our embeddings are Ô¨Åne - tuned in downstream tasks . 
Nonetheless , the reduction of the number of word embeddings from the CBOW model before Ô¨Åne - tuning is still one of our goals as we can save space to store these embeddings and save time to download them . 
For example , Google News Vectors have around 3 million words , and we need only 2 % of the number of the word embeddings if we choose 100 K most frequent words and 10 K word clusters in our method . 
3 Our Method 3.1 CBOW Model Letwtdenote thet - th word in a given text . 
We adopt the basic CBOW model architecture for learning word embeddings . 
The CBOW model predicts the output word wtgiven the input words in the window which precede or follow the output word . 
When the window size is 2 , as an example , the input words are wt‚àí2,wt‚àí1,wt+1,wt+2 . 
We denote the input and output embeddings of word wirespectively as / vector xiand / vector oi . 
The CBOW model computesthe hidden representation as follows : /vectorh=1 2cc / summationdisplay i=‚àíc , i / negationslash=0 / vector xt+i , ( 1 ) wherecis the window size . 
We use negative sampling ( Mikolov et al . 
, 2013b ) to train the CBOW model by maximizing the following objective function : logœÉ(/vectorhT / vector ot ) + k / summationdisplay j=1logœÉ(‚àí/vectorhT / vector oj ) , ( 2 ) wherekis the size of the negative sample , /vector ojis thej - th noise word embedding and œÉis the sigmoid function . 
Each word in the negative sample is drawn from the unigram distribution . 
AYeragereplaceZordembeddingsclXsWerembeddings replaceWargeWnoiseinpXW ZordsoXWpXW Zords Figure 1 : CBOW architecture with our replacing method for input and output words trained with negative sampling . 
Suppose that wt‚àí2,wt+1,o1ando3are infrequent words . 
3.2 Replacing Methods As a method for incorporating word clusters , we propose to replace infrequent words with their clusters for the input and output . 
The architecture is shown in Figure 1 . 
This is motivated by the intuition that the embeddings of clusters should be more reliable than those of infrequent words . 
We denote the embedding of the cluster for word wt+i as / vectordt+i . 
We present the following two replacing methods : ‚Ä¢ReIn : In the input , /vector xt+iin Eq . 
( 1 ) will be replaced with /vectordt+iif the frequency of wt+iis less than threshold fin . 
‚Ä¢ReOut : In the output , output words whose frequency is less than foutare replaced with their clusters . 
Thus , in negative sampling , a noise word will be sampled from clusters and frequent words.81 As with the standard CBOW model , we use the input word embeddings and input cluster embeddings for downstream tasks . 
Thresholds finand foutare set to 100 in all experiments . 
Due to this large value , each cluster contains many infrequent words , which share the same embedding . 
We use two methods together , which is referred to as ReIn+ReOut in the following experiments . 
3.2.1 Motivation of ReIn and ReOut Since the embeddings of clusters are learned by aggregating many infrequent words , they are more robust than the embeddings of the infrequent words . 
During the Ô¨Åne - tuning process for a downstream task , the embeddings of infrequent words are Ô¨Årst initialized with the embeddings of their clusters . 
As most of these infrequent words appear only a few times , these embeddings will not be updated far away from each other within one cluster . 
The visualization of these embeddings before and after Ô¨Åne - tuning can be found in the appendix B. As a result , these embeddings for infrequent words become more reliable since originally most infrequent word embeddings are updated only several times and are not far away from where they were randomly initialized . 
Since the context of frequent words becomes less noisy by replacing all the infrequent words with their clusters , the learned frequent word embeddings are also better , as shown later in our experiments . 
The standard CBOW model is usually trained with negative sampling , which is designed for speeding up the training process . 
By using ReOut , infrequent noise words will be replaced with their clusters , which contain more noise words than the original CBOW model . 
As a result , ReOut makes the training of the CBOW model more effective , as shown later in our experiments . 
4 Experiments on LM and MT We applied our embeddings to downstream tasks : language modeling ( LM ) and low - resource machine translation ( MT ) . 
When applying to the downstream tasks , we only used the training data of the speciÔ¨Åc task to obtain word clusters and embeddings without any extra data . 
We then used the learned embeddings to initialize the lookup table of word embeddings for the task . 
In this paper , we limit the applications of our model to relatively small datasets to demonstrate the usefulness of our method . 
We plan to conduct larger - scale experiments on more downstream tasks in future work . 
Inthe following tables , CBOW and ReIn+ReOut indicate that they are initialization methods for speciÔ¨Åc downstream tasks . 
4.1 Hyper - parameter Settings In this section , we describe the hyper - parameters for producing word clusters and word embeddings . 
As we mentioned before , we obtained word clusters through the ClusterCat software . 
For most hyperparameters , we used its default values . 
We set the number of clusters to 600 in all our experiments . 
Since our work involves many tasks in total , it is hard to choose the optimal number of word clusters for each task . 
We experimented with several values ( 600 , 800 and 1000 ) and observed the same trend . 
Thus , we simply chose 600 , for convenience , for all tasks . 
For producing word embeddings , our implementation was based on the fasttext3 . 
Our cluster - incorporated CBOW model and the standard CBOW model were trained under the same hyper - parameters . 
We set most hyper - parameters as its default values . 
Namely , we set the training epoch to 5 , the number of negative examples to 5 , the window size to 5 , and the minimum count of word occurrence to 54 . 
4.2 LM on Standard English Datasets We test ReIn+ReOut based on the recent state - ofthe - art awd - lstm - lm codebase5(Merity et al . 
, 2018 ) using two standard language modeling datasets : Penn Treebank ( PTB ) and WikiText-2 ( Wiki2 ) . 
We followed exactly the same setting in the source code . 
The results are shown in Table 1 , and we found that our ReIn+ReOut is effective even with the strong baseline . 
PTB Wiki2 AWD - LSTM w/o Ô¨Åne - tuning ( Merity et al . 
, 2018)58.80 66.00 CBOW 58.39 65.48 ReIn+ReOut 57.85 63.93 Table 1 : Perplexity results on PTB and Wiki2 . 
4.3 Low - resource NMT We applied our method to the standard long - short term memory networks ( LSTMs ) based sequenceto - sequence ( seq2seq ) model on two datasets : German - English ( de - en ) with 153 K sentence pairs 3https://github.com/facebookresearch/fastText 4When we set the minimum count of word occurrence to 1 , the standard CBOW does not perform well . 
5https://github.com/salesforce/awd-lstm-lm82 from IWSLT 2014 ( Cettolo et al . 
, 2014 ) , EnglishVietnamese ( en - vi ) with 133 K sentence pairs from IWSLT 2015 ( Cettolo et al . 
, 2012 ) . 
The detailed data statistics of two low - resource NMT datasets is in Table 2 . 
We used the opennmt - py toolkit6with a 2 - layer bidirectional LSTM with hidden size of 500 and set the training epoch to 30 . 
The word embedding size is set to 500 and the batch size is 64 . 
We trained the seq2seq models by the SGD optimizer with start learning rate being 1.0 , which will be decayed by 0.5 if perplexity does not decrease on the validation set . 
Other hyper - parameters were kept default . 
We also include some published results based on LSTM - based seq2seq models to gauge the result of our baseline . 
As shown in Table 3 , without any extra language pair resources , the ReIn+ReOut initialization improves the BLEU score over the baseline by 1.29 and 0.51 points on de - en , en - vi respectively . 
de - en en - vi # Training pairs 153,348 133,317 # Test pairs 6,750 1,268 # Valid pairs 6,970 1,553 Train V ocab ( source ) 103,796 54,169 Train V ocab ( target ) 50,045 25,615 Table 2 : Data statistics of two low - resource NMT datasets . 
de - en en - vi seq2seq with attention ( Luong and Manning , 2015 ) - 23.3 AC+LL ( Bahdanau et al . 
, 2017 ) 28.53 NPMT ( Huang et al . 
, 2018 ) 29.92 27.69 Our seq2seq with attention 28.95 28.16 CBOW 29.25 28.24 Our ReIn+ReOut 30.24 28.67 Table 3 : BLEU scores on two low - resource MT datasets . 
NPMT in Huang et al . 
( 2018 ) used a neural phrase - based machine translation model and AC+LL in Bahdanau et al . 
( 2017 ) used a one - layer GRU encoder and decoder with attention . 
4.4 LM in Diverse Languages To verify the effect of word clusters on different languages , we selected 8 datasets containing typologically diverse languages from LM datasets released by Gerz et al . 
( 2018 ) . 
The data statistics of 8 LM datasets is in Table 5 . 
We basically used standard LSTMs instead of AWD - LSTM - LM to save time . 
We chose the available standard LSTM - LM code7 . 
Hyper - parameters of our standard LSTM model on 6https://github.com/OpenNMT/OpenNMT-py 7https://github.com/pytorch/examples/tree/master/ word language modellanguage modeling tasks is in Table 4 . 
The results are shown in Table 6 . 
Our LSTM - LM obtained better results than the one from Gerz et al . 
( 2018 ) on all datasets . 
As we see , ReIn+ReOut is effective for typologically diverse languages and also requires a smaller input vocabulary . 
For example , the input vocabulary of ReIn+ReOut for en dataset contains 1.3 K words while the full vocabulary 50K. Embedding size 200 Epochs 40 LSTM layers 2 Optimizer SGD LSTM sequence length 35 Learning rate 20 LSTM hidden unit 200 Learning rate decay 4 Param . 
init : rand uniform [ -0.1,0.1 ] Gradient clipping 0.25 Dropout 0.2 Batch size 20 Table 4 : Hyper - parameters of our standard LSTM model on language modeling task . 
5 Analysis In this section , we analyse ReIn+ReOut on the basis of LM experiments with en and de datasets . 
5.1 Targeted Perplexity Results To show the gain for frequent and infrequent words , we measured the perplexity for frequent and infrequent words in the test data separately . 
SpeciÔ¨Åcally , we calculated the perplexity of the next word , when an infrequent word is given as the current word . 
A similar analysis on language models can be found in Vania and Lopez ( 2017 ) . 
Our analysis do not contain new words in the test dataset . 
The results are shown in Table 7 . 
As we see , ReIn+ReOut is more effective than CBOW in learning both the embeddings of frequent and infrequent words , as we explained in Sec . 
3.2.1 . 
5.2 Ablation Study The results of ablation study are in Table 8 . 
Comparing the methods ReIn and CBOW , we found replacing only input infrequent words in CBOW also works better than the original CBOW . 
We can also conclude that replacing only output infrequent words in CBOW works better than the original CBOW , by comparing ReOut and CBOW . 
Both ReIn and ReOut work well even when they are used alone . 
As mentioned in the motivation of ReOut , it makes the training more effective . 
To verify83 TypologyTrain vocab#Train tokens#Test tokens#Valid tokens#Input vocab of ReIn+ReOut zh ( Chinese ) Isolating 43674 746 K 56.8 K 56.9 K 1661 vi ( Vietnamese ) Isolating 32065 754 K 61.9 K 64.8 K 1716 de ( German ) Fusional 80743 682 K 51.3 K 52.6 K 1163 en ( English ) Fusiona 55522 783 K 59.5 K 57.3 K 1381 ar ( Arabic ) IntroÔ¨Çexive 89091 723 K 54.7 K 55.2 K 1431 he ( Hebrew ) IntroÔ¨Çexive 83223 719 K 54.7 K 52.9 K 1345 et ( Estonian ) Agglutinative 94184 556 K 38.6 K 40.0 K 1285 tr ( Turkish ) Agglutinative 90847 627 K 45.2 K 47.4 K 1241 Table 5 : Data statistics of 8 language modeling datasets and size of input vocabulary of our ReIn+ReOut . 
Dataset Random CBOW ReIn+ReOut zh 555 527 494 vi 153 145 138 de 609 542 484 en 365 317 289 ar 1647 1447 1305 he 1482 1236 1175 et 1451 1157 1004 tr 1379 1220 1148 Table 6 : Perplexity results of standard LSTM LM on 8 datasets with different initialization methods . 
Freq . 
Infreq . 
All enCBOW 340 198 283 ReIn+ReOut 316 184 264 deCBOW 591 352 489 ReIn+ReOut 564 318 458 Table 7 : Targeted perplexity results of standard LSTM LM with different initializations . 
this , we increased the number of negative samples for ReIn and CBOW . 
The training will be more effective if we increase the number of negative samples , while training the model will also take longer time . 
As we increased the size of negative samples , we obtained better results for both ReIn and CBOW . 
We increased it only to 30 because we did not observe improvements when we made it further larger . 
This result indicates that we can use word clusters to obtain better results with a small amount of negative samples . 
In reality , we can also use off - the - shelf word clusters to avoid spending time for producing word clusters . 
en de en de ReIn 300 528 CBOW 317 542 ReIn neg+10 293 499 CBOW neg+10 309 523 ReIn neg+30 300 494 CBOW neg+30 312 554 ReIn+ReOut 289 484 ReOut 312 515 Table 8 : Perplexity results of LSTM LM by changing the number of negative samples . 
‚Äô + neg ‚Äô represents the number of negative samples , which is 5 at default.5.3 LM Results on Off - the - shelf Vectors To gauge the improvements , we used off - the - shelf pretrained word vectors in English : GloVe vectors ( Pennington et al . 
, 2014 ) and Google News Vectors8 . 
We obtained 258 , 290 and 289 perplexity scores on en with Google News Vectors , Glove vectors and ReIn+ReOut respectively . 
Although ReIn+ReOut underperforms Google News Vectors , which were trained on 100 billion tokens , it obtained the results comparable to Glove Vectors , trained on 6 billion tokens . 
This indicates that our ReIn+ReOut is effective even without extra training data ( only 783 K training tokens in en ) . 
en Google News Vectors 258 GloVe Vectors 290 ReIn+ReOut 289 Table 9 : Perplexity results of standard LSTM compared with off - the - shelf vectors . 
6 Conclusion We proposed a simple and effective method to incorporate word clusters into the CBOW model . 
Our method is effective on several downstream tasks . 
For future work , we will test our methods on larger corpora and also add more downstream tasks . 
We will also study how to combine word clusters and subword information . 
Acknowledgments We would like to thank anonymous reviewers for their constructive comments and Hu also thanks his support from China Scholarship Council . 
Abstract The recently introduced pre - trained language model BERT advances the state - of - the - art on many NLP tasks through the Ô¨Åne - tuning approach , but few studies investigate how the Ô¨Åne - tuning process improves the model performance on downstream tasks . 
In this paper , we inspect the learning dynamics of BERT Ô¨Åne - tuning with two indicators . 
We use JS divergence to detect the change of the attention mode and use SVCCA distance to examine the change to the feature extraction mode during BERT Ô¨Åne - tuning . 
We conclude that BERT Ô¨Åne - tuning mainly changes the attention mode of the last layers and modiÔ¨Åes the feature extraction mode of the intermediate and last layers . 
Moreover , we analyze the consistency of BERT Ô¨Åne - tuning between different random seeds and different datasets . 
In summary , we provide a distinctive understanding of the learning dynamics of BERT Ô¨Ånetuning , which sheds some light on improving the Ô¨Åne - tuning results . 
1 Introduction BERT ( Bidirectional Encoder Representations from Transformers ; Devlin et al . 
2019 ) is a large pre - trained language model . 
It obtains state - of - theart results on a wide array of Natural Language Processing ( NLP ) tasks . 
Unlike other previous pretrained language models ( Peters et al . 
, 2018a ; Radford et al . 
, 2018 ) , BERT employs the multi - layer bidirectional Transformer encoder as the model architecture and proposes two novel pre - training tasks : the masked language modeling and the next sentence prediction . 
There are two approaches to adapt the pretrained language representations to the downstream tasks . 
One is the feature - based approach , where the parameters of the original pre - trained ‚àóContribution during internship at Microsoft Research.model are frozen when applied on the downstream tasks ( Mikolov et al . 
, 2013 ; Pennington et al . 
, 2014 ; Peters et al . 
, 2018a ) . 
Another one is the Ô¨Åne - tuning approach , where the pre - trained model and the taskspeciÔ¨Åc model are trained together ( Dai and Le , 2015 ; Howard and Ruder , 2018 ; Radford et al . 
, 2018 ) . 
Take the classiÔ¨Åcation task as an example , the new parameter added for BERT Ô¨Åne - tuning is a task - speciÔ¨Åc fully - connected layer , then all parameters of BERT and the classiÔ¨Åcation layer are trained together to minimize the loss function . 
Peters et al . 
( 2019 ) demonstrate that the Ô¨Ånetuning approach of BERT generally outperforms the feature - based approach . 
We know that BERT encodes task - speciÔ¨Åc representations during Ô¨Ånetuning , but it is unclear about the learning dynamics of BERT Ô¨Åne - tuning , i.e. , how Ô¨Åne - tuning helps BERT to improve performance on downstream tasks . 
We investigate the learning dynamics of BERT Ô¨Åne - tuning with two indicators . 
First , we use Jensen - Shannon divergence to measure the change of the attention mode during BERT Ô¨Åne - tuning . 
Second , we use Singular Vector Canonical Correlation Analysis ( SVCCA ; Raghu et al . 
( 2017 ) ) distance to measure the change of the feature extraction mode . 
We conclude that during the Ô¨Åne - tuning procedure , BERT mainly changes the attention mode of the last layers , and modiÔ¨Åes the feature extraction mode of intermediate and last layers . 
At the same time , BERT has the ability to avoid catastrophic forgetting of knowledge in low layers . 
Moreover , we also analyze the consistency of the Ô¨Åne - tuning procedure . 
Across different random seeds and different datasets , we observe that the changes of low layers ( 0 - 9th layer ) are generally consistent , which indicates that BERT has learned some common transferable language knowledge in low layers during the pre - training process , while the task - speciÔ¨Åc87 information is mostly encoded in intermediate and last layers . 
2 Experimental Setup We employ the BERT - large model1on a diverse set of NLP tasks : natural language inference ( NLI ) , sentiment analysis ( SA ) and paraphrase detection ( PD ) . 
For NLI , we use both the Multi - Genre Natural Language Inference dataset ( MNLI ; Williams et al . 
2018 ) and the Recognizing Textual Entailment dataset ( RTE ; aggregated from Dagan et al . 
2006 , Haim et al . 
2006 , Giampiccolo et al . 
2007 , Bentivogli et al . 
2009 ) . 
For SA , we use the binary version of the Stanford Sentiment Treebank dataset ( SST-2 ; Socher et al . 
2013 ) . 
For PD , we use the Microsoft Research Paraphrase Corpus dataset ( MRPC ; Dolan and Brockett 2005 ) . 
Dataset LR BS NE MNLI 3e-5 64 3 RTE 1e-5 32 5 SST-2 3e-5 64 4 MRPC 1e-5 16 5 Table 1 : Hyperparameter conÔ¨Åguration for BERT Ô¨Ånetuning . 
LR : learning rate , BS : batch size , NE : number of epochs . 
The hyperparameter choice for Ô¨Åne - tuning is task - speciÔ¨Åc . 
We choose relatively optimal parameters for every dataset as suggested in Devlin et al . 
( 2019 ) . 
The detailed hyperparameter conÔ¨Åguration is shown in Table 1 . 
Moreover , we use Adam optimizer with the slanted triangular learning rate schedule ( Howard and Ruder , 2018 ) and keep the dropout probability at 0.1 . 
3 Fine - tuning changes the attention mode of the last layers The model architecture of BERT is essentially based on the multi - layer bidirectional Transformer , the core function of which is the self - attention mechanism ( Vaswani et al . 
, 2017 ) . 
We use JensenShannon divergence between two attention scores to detect changes of the attention mode in different layers during Ô¨Åne - tuning . 
Jensen - Shannon divergence JS divergence is a method of measuring the distance between two 1github.com/google-research/bertprobability distributions , it is deÔ¨Åned as : DJS(P||Q ) = 1 2DKL(P||R ) +1 2DKL(Q||R ) wherePandQare two different probability distributions , R = P+Q 2is the average probability distribution of them and DKLrepresents the Kullback - Leibler divergence . 
For every layer of BERT , there are 16 attention heads , each head produces an attention score of the input sequence . 
Each attention score is a probability distribution about how much attention a target word pays to other words . 
We compute JS divergence of attention scores between the original BERT model M0and the Ô¨Åne - tuned model Mton the development set , by calculating the average of the sum of JS divergence at each word and each attention head for every layer , the speciÔ¨Åc calculation formula is as follows : DJS(Mt||M0 ) = 1 N1 HN / summationdisplay n=1H / summationdisplay h=11 WW / summationdisplay i=1 DJS(Ah t(word i)||Ah 0(word i ) ) whereNdenotes the number of development examples , Hdenotes the number of attention heads , Wdenotes the number of tokens in a sequence andAh t(word i)denotes the attention score of the attention head hatword iin modelMt . 
1357911131517192123 Layer Index0.00.10.20.3JS Divergence SST-2 MRPC MNLI Figure 1 : JS divergence of attention scores of every layer between the original BERT model and the Ô¨Ånetuned model . 
We present JS divergence results in Figure 1 , from which we observe the attention mode in low layers and intermediate layers do not change seriously , while the attention mode of last layers changes drastically . 
It indicates that the Ô¨Åne - tuning procedure has the ability to keep the attention mode of low layers consistent with the original BERT model , and changes the attention mode of the last layers to adapt BERT on speciÔ¨Åc tasks.88 4 Fine - tuning modiÔ¨Åes the feature extraction mode of the intermediate and the last layers While the attention score implies the inherent dependencies between different words , the output representation of every layer is the practical feature that the model extracts . 
We use SVCCA distance ( Raghu et al . 
, 2017 ) to quantify the change of these output representations during Ô¨Åne - tuning , which indicates the change of the feature extraction mode of BERT . 
Singular Vector Canonical Correlation Analysis . 
SVCCA distance is used as a metric to measure the differences of hidden representations between the original BERT model M0and the Ô¨Ånetuned model Mtat a target layer . 
It is calculated by : DSV CCA ( Mt||M0 ) = 1‚àí1 cc / summationdisplay i=1œÅ(i ) wherecdenotes the hidden size of BERT , œÅis the Canonical Correlation Analysis ( CCA ) resulting in a value between 0 and 1 , which indicates how well correlated the two representations derived by two models are . 
For a detailed explanation of SVCCA , please see Raghu et al . 
( 2017 ) . 
1357911131517192123 Layer Index0.00.20.4SVCCA Distance SST-2 MRPC MNLI Figure 2 : SVCCA distance of individual layers between the original BERT model and the Ô¨Åne - tuned model . 
From Figure 2 , we observe that changes in SVCCA distance in higher layers are more distinct than lower layers . 
This phenomenon is reasonable because the output representation of higher layers undergoes more transformations , so the change of SVCCA distance in higher layers is more dramatic . 
As the output representation of the last layer is directly used for classiÔ¨Åcation , we aim to compare the effect of each layer on the Ô¨Ånal output representation respectively . 
We replace the parameters of /uni00000013 / uni00000015 / uni00000017 / uni00000019 / uni0000001b / uni00000014 / uni00000013 / uni00000014 / uni00000015 / uni00000014 / uni00000017 / uni00000014 / uni00000019 / uni00000014 / uni0000001b / uni00000015 / uni00000013 / uni00000015 / uni00000015 /uni0000002f / uni00000044 / uni0000005c / uni00000048 / uni00000055 / uni00000003 / uni0000002c / uni00000051 / uni00000047 / uni00000048 / uni0000005b / uni00000036 / uni00000036 / uni00000037 / uni00000010 / uni00000015 /uni00000030 / uni00000035 / uni00000033 / uni00000026 /uni00000030 / uni00000031 / uni0000002f / uni0000002c / uni00000013 / uni00000011 / uni00000013 / uni00000015 /uni00000013 / uni00000011 / uni00000013 / uni00000017 /uni00000013 / uni00000011 / uni00000013 / uni00000019 /uni00000013 / uni00000011 / uni00000013 / uni0000001b /uni00000013 / uni00000011 / uni00000014 / uni00000013 Figure 3 : SVCCA distance of the last layer between the original Ô¨Åne - tuned model and the Ô¨Åne - tuned model with parameters of a target layer replaced with their pretrained values . 
every layer in the Ô¨Åne - tuned model with their original values in the BERT model before Ô¨Åne - tuning and compute the SVCCA distance of the last layer output representation . 
The results are shown in Figure 3 , we observe that whether the low layers ( 0 - 10 ) are replaced with their original values or not , it has little effect on the Ô¨Ånal output representation . 
Moreover , the change in the intermediate and last layers will increase the SVCCA distance , which reÔ¨Çects that Ô¨Åne - tuning mainly changes the feature extraction mode of intermediate and last layers . 
5 Consistency of Fine - tuning In this section , we investigate the consistency of different Ô¨Åne - tuning procedures , including the consistency between different random seeds and the consistency between different datasets . 
5.1 Consistency between different random seeds We Ô¨Åne - tune two models on every dataset with the same hyperparameters but different random seeds . 
We compute the pairwise JS divergence and SVCCA distance of each layer between the two models with different random seeds . 
As shown in Figure 4 , for large dataset MNLI and SST-2 , the attention mode of low and intermediate layers is basically consistent between two different random seeds , whereas the attention mode of last layers is relatively divergent . 
For MRPC , the attention mode appears to be divergent at the 9th layer . 
Figure 5 illustrates SVCCA distance between different random seeds , we observe that the SVCCA distance gradually increases in all layers . 
For MNLI and SST-2 , the increase of last layers is89 1357911131517192123 Layer Index0.00.10.20.3JS Divergence SST-2 MRPC MNLIFigure 4 : JS divergence between two models with different random seeds . 
1357911131517192123 Layer Index0.00.20.4SVCCA Distance SST-2 MRPC MNLIFigure 5 : SVCCA distance between two models with different random seeds . 
1357911131517192123 Layer Index0.00.10.20.30.40.5JS Divergence MNLI&RTE MRPC&RTE MRPC&SST-2 Figure 6 : JS divergence between different datasets . 
1357911131517192123 Layer Index0.00.20.4SVCCA Distance MNLI&RTE MRPC&RTE MRPC&SST-2Figure 7 : SVCCA distance between different datasets . 
more obvious , and for MRPC , the increase appears to be obvious from the 13th layer . 
5.2 Consistency between different datasets Besides the consistency between different random seeds , we also aim to investigate the consistency between different datasets . 
We Ô¨Åne - tune two models on two different datasets then evaluate on a combined dataset containing 200 examples respectively from both two datasets . 
For different datasets of the same domain , we use two models Ô¨Åne - tuned on RTE and MNLI dataset . 
For different domains , we examine the consistency between MRPC and RTE , which both have pairwise input sequences , and the consistency between MRPC and SST-2 , which have different patterns of input sequences . 
The JS divergence results and SVCCA distance results between different datasets are shown in Figure 6 and Figure 7 . 
Figure 6 and Figure 7 demonstrate that no matter two datasets are from the same domain or the different domain , the attention mode and the feature extraction mode of low layers ( 0 - 7 layer ) are consistent , which indicates BERT studies some common language knowledge during the pre - training procedure and low layers are stable to change their original modes . 
JS divergence of the attention scoresand SVCCA distance of the output representations in intermediate and last layers between two models are more distinct when the difference between two training datasets increases . 
The consistency between datasets from similar tasks like RTE and MNLI is still relatively strong in last layers compared to the consistency between datasets from the different domain . 
And when the input sequence pattern and the domain of two datasets are different , the consistency of intermediate and last layers is weak as expected . 
6 Related Work Pre - trained language models ( Radford et al . 
, 2018 ; Devlin et al . 
, 2019 ; Liu et al . 
, 2019 ; Dong et al . 
, 2019 ; Yang et al . 
, 2019 ; Clark et al . 
, 2020 ; Bao et al . 
, 2020 ) stimulate the research interest on the interpretation of these black - box models . 
Peters et al . 
( 2018b ) show that the biLM - based models learn representations that vary with network depth , the lower layers specialize in local syntactic relationships and the higher layers model longer range relationships . 
Kovaleva et al . 
( 2019 ) propose a methodology and offer the analysis of BERTs capacity to capture different kinds of linguistic information by encoding it in its self - attention weights . 
Hao et al . 
( 2019 ) visualize the loss landscapes and90 optimization trajectories of the BERT Ô¨Åne - tuning procedure and Ô¨Ånd that low layers of the BERT model are more invariant and transferable across tasks . 
Merchant et al . 
( 2020 ) Ô¨Ånd that Ô¨Åne - tuning primarily affects the top layers of BERT , but with noteworthy variation across tasks . 
Hao et al . 
( 2020 ) propose a self - attention attribution method to interpret information Ô¨Çow within Transformer . 
7 Discussions We use JS divergence to detect the change of the attention mode in different layers during BERT Ô¨Åne - tuning and use SVCCA distance to detect the change of the feature extraction mode . 
We observe that BERT Ô¨Åne - tuning mainly changes the attention mode of last layers and modiÔ¨Åes the feature extraction mode of intermediate and last layers . 
We also demonstrate that the changes of low layers are consistent between different random seeds and different datasets , which indicates that BERT learns common transferable language knowledge in low layers . 
In future research , we would like to explore learning dynamics for cross - lingual pretrained models ( Conneau and Lample , 2019 ; Conneau et al . 
, 2020 ; Chi et al . 
, 2020 ) . 
Acknowledgements The work was partially supported by National Natural Science Foundation of China ( NSFC ) [ Grant No . 
61421003 ] . 
Abstract In this paper , we propose second - order graphbased neural dependency parsing using message passing and end - to - end neural networks . 
We empirically show that our approaches match the accuracy of very recent state - ofthe - art second - order graph - based neural dependency parsers and have signiÔ¨Åcantly faster speed in both training and testing . 
We also empirically show the advantage of second - order parsing over Ô¨Årst - order parsing and observe that the usefulness of the head - selection structured constraint vanishes when using BERT embedding . 
1 Introduction Graph - based dependency parsing is a popular approach to dependency parsing that scores parse components of a sentence and then Ô¨Ånds the highest scoring tree through inference . 
First - order graphbased dependency parsing takes individual dependency edges as the components of a parse tree , while higher - order dependency parsing considers more complex components consisting of multiple edges . 
There exist both exact inference algorithms ( Carreras , 2007 ; Koo and Collins , 2010 ; Ma and Zhao , 2012 ) and approximate inference algorithms ( McDonald and Pereira , 2006 ; Smith and Eisner , 2008 ; Gormley et al . 
, 2015 ) to Ô¨Ånd the best parse tree . 
Recent work focused on neural network based graph dependency parsers ( Kiperwasser and Goldberg , 2016 ; Wang and Chang , 2016 ; Cheng et al . 
, 2016 ; Kuncoro et al . 
, 2016 ; Ma and Hovy , 2017 ; Dozat and Manning , 2017 ) . 
Dozat and Manning ( 2017 ) proposed a Ô¨Årst - order graph - based neural dependency parsing approach with a simple headselection training objective . 
It uses a biafÔ¨Åne function to score dependency edges and has high efÔ¨Åciency and good performance . 
Subsequent work ‚àóKewei Tu is the corresponding author.introduced second - order inference into their parser . 
Ji et al . 
( 2019 ) proposed a graph neural network that captures second - order information in token representations , which are then used for Ô¨Årst - order parsing . 
Very recently , Zhang et al . 
( 2020 ) proposed an efÔ¨Åcient second - order tree CRF model for dependency parsing and achieved state - of - the - art performance . 
In this paper , we Ô¨Årst show how a previously proposed second - order semantic dependency parser ( Wang et al . 
, 2019 ) can be applied to syntactic dependency parsing with simple modiÔ¨Åcations . 
The parser is an end - to - end neural network derived from message passing inference on a conditional random Ô¨Åeld that encodes the second - order parsing problem . 
We then propose an alternative conditional random Ô¨Åeld that incorporates the head - selection constraint of syntactic dependency parsing , and derive a novel second - order dependency parser . 
We empirically compare the two second - order approaches and the Ô¨Årst - order baselines on English Penn Tree Bank 3.0 ( PTB ) , Chinese Penn Tree Bank 5.1 ( CTB ) and datasets of 12 languages in Universal Dependencies ( UD ) . 
We show that our approaches achieve state - of - the - art performance on both PTB and CTB and our approaches are signiÔ¨Åcantly faster than recently proposed second - order parsers . 
We also make two interesting observations from our empirical study . 
First , it is a common belief that contextual word embeddings such as ELMo ( Peters et al . 
, 2018 ) and BERT ( Devlin et al . 
, 2019 ) already conveys sufÔ¨Åcient high - order information that renders high - order parsing less useful , but we Ô¨Ånd that second - order decoding is still helpful even with strong contextual embeddings like BERT . 
Second , while Zhang et al . 
( 2019 ) previously found that incoperating the head - selection constraint is helpful in Ô¨Årst - order parsing , we Ô¨Ånd that with a better loss function design and hyper - parameter tun-93 ing both Ô¨Årst- and second - order parsers without the head - selection constraint can match the accuracy of parsers with the head - selection constraint and can even outperform the latter when using BERT embedding . 
Our approaches are closely related to the work of Gormley et al . 
( 2015 ) , which proposed a nonneural second - order parser based on Loopy Belief Propagation ( LBP ) . 
Our work differs from theirs in that : 1 ) we use Mean Field Variational Inference ( MFVI ) instead of LBP , which Wang et al . 
( 2019 ) found is faster and equally accurate in practice ; 2 ) we add the head - selection constraint and do not include the global tree constraint that is shown to produce only slight improvement ( Zhang et al . 
, 2019 ) but would complicate our neural network design and implementation ; 3 ) we employ modern neural encoders and achieve much better parsing accuracy . 
Our approaches are also closely related to the very recent work of Fonseca and Martins ( 2020 ) . 
The main difference is that we use MFVI while they use the dual decomposition algorithm AD3(Martins et al . 
, 2011 , 2013 ) for approximate inference . 
2 Approach Zhang et al . 
( 2019 ) categorized different kinds of graph - based dependency parsers based on their structured output constraints according to the normalization for output scores . 
A Local approach views dependency parsing as a head - selection problem , in which each word selects exactly one dependency head . 
A Single approach places no structured constraint , viewing the existence of each possible dependency edge as an independent binary classiÔ¨Åcation problem . 
The second - order semantic dependency parser of Wang et al . 
( 2019 ) is an end - to - end neural network derived from message passing inference on a conditional random Ô¨Åeld that encodes the second - order parsing problem . 
It is clearly a Single approach because of the lack of structured constraints in semantic dependency parsing . 
We can apply this approach to syntactic dependency parsing with two minor modiÔ¨Åcations . 
First , co - parents , one of the three types of second - order parts , become invalid and hence are removed . 
Second , for the approach to output valid parse trees during testing , we run maximum spanning tree ( MST ) ( McDonald et al . 
, 2005 ) based on the posterior edge probabilities predicted by the approach . 
Inspired by Wang et al . 
( 2019 ) , below we propose a Local second - order parsing approach . 
While the Single approach uses Boolean random variables to represent existence of possible dependency edges , our Local approach deÔ¨Ånes a discrete random variable for each word specifying its dependency head , thus enforcing the head - selection constraint and leading to different formulation of the message passing inference steps . 
2.1 Scoring Following Dozat and Manning ( 2017 ) , we predict edge existence and edge labels separately . 
Suppose the input sentence is w= [ w0,w1,w2, ... ,w n ] wherew0is a dummy root . 
We feed word representations outputted by the BiLSTM encoder into a biafÔ¨Åne function to assign score s(edge ) ij to edge wi‚Üíwj . 
We use a Trilinear function to assign scores(sib ) ij , ikto the siblings part consisting of edges wi‚Üíwjandwi‚Üíwk , and another Trilinear function to assign score s(gp ) ij , jkto the grandparent part consisting of edges wi‚Üíwjandwj‚Üíwk . 
For edge labels , we use a biafÔ¨Åne function to predict label scores of each potential edge and use a softmax function to compute the label distribution P(y(label ) ij|w ) , wherey(label ) ijrepresents the possible label for edge wi‚Üíwj . 
2.2 Message Passing The head - selection structured constraint requires that each word except the root has exactly one head . 
We deÔ¨Åne variable Xj‚àà{0,1,2, ... ,n}to indicate the head of word wj . 
We then deÔ¨Åne a conditional random Ô¨Åeld ( CRF ) over [ X1, ... ,X n ] . 
For each variable Xj , the unary potential is deÔ¨Åned by : œÜu(Xj = i ) = exp(s(edge ) ij ) Given two variables XjandXl , the binary potential is deÔ¨Åned by : œÜp(Xj = i , Xl = k ) = Ô£± Ô£¥Ô£≤ Ô£¥Ô£≥exp(s(sib ) ij , kl)k = i exp(s(gp ) ij , kl)k = j 1 Otherwise We use MFVI for approximate inference on this CRF . 
The algorithm updates the factorized poste-94 rior distribution Qj(Xj)of each word iteratively . 
M(t‚àí1 ) j(i ) = /summationdisplay k / negationslash = i , jQ(t‚àí1 ) k(i)s(sib ) ij , ik + Q(t‚àí1 ) k(j)s(gp ) ij , jk+Q(t‚àí1 ) i(k)s(gp ) ki , ij Q(t ) j(i ) = exp{s(edge ) ij + M(t‚àí1 ) j(i ) } n / summationtext k=0exp{s(edge ) kj+M(t‚àí1 ) j(k ) } Att= 0,Q(t ) j(Xj)is initialized by normalizing the unary potential . 
The iterative update steps can be unfolded as recurrent neural network layers parameterized by part scores , thus forming an end - to - end neural network . 
Compared with the update formula in the Single approach , here the posterior distributions are deÔ¨Åned over head - selections and are normalized over all possible heads . 
The computational complexity remains the same . 
2.3 Learning We deÔ¨Åne the cross entropy losses by : L(edge)=‚àí/summationdisplay ilog[Qi(y‚àó(edge ) i|w ) ] L(label)=‚àí/summationdisplay i , j1(y‚àó(edge ) j = i ) log(P(y‚àó(label ) ij|w ) ) L = ŒªL(label)+ ( 1‚àíŒª)L(edge ) wherey‚àó(edge ) i is the head of word wiandy‚àó(label ) ij is the label of edge wi‚Üíwjin the golden parse tree , Œªis a hyper - parameter and 1(x)is an indicator function that returns 1whenxis true and 0 otherwise . 
3 Experiments 3.1 Setups Following previous work ( Dozat and Manning , 2017 ; Ma et al . 
, 2018 ) , we use PTB 3.0 ( Marcus et al . 
, 1993 ) , CTB 5.1 ( Xue et al . 
, 2002 ) and 12 languages in Universal Dependencies ( Nivre et al . 
, 2018 ) ( UD ) 2.2 to evaluate our parser . 
Punctuation is ignored in all the evaluations . 
We use the same treebanks and preprocessing as Ma et al . 
( 2018 ) for PTB , CTB , and UD . 
For all the datasets , we remove sentences longer than 90 words in training sets for faster computation . 
We use GNN , Local1O , Single1O , Local2O andSingle2O to represent the approaches of Ji et al . 
( 2019 ) , Dozat and Manning ( 2017 ) , DozatHidden Layer Hidden Sizes Word / GloVe / Char 100 POS 50 GloVe Linear 125 BERT Linear 125 BiLSTM 3 * 600 Char LSTM 1 * 400 Unary Arc ( UD ) 500 Local1O /Local2O Unary Arc ( Others ) 450 Single1O /Single2O Unary Arc ( Others ) 550 Label 150 Binary Arc 150 Dropouts Dropout Prob . 
Word / GloVe / POS 20 % Char LSTM ( FF / recur ) 33 % Char Linear 33 % BiLSTM ( FF / recur ) 45%/25 % Unary Arc / Label 25%/33 % Binary Arc 25 % Optimizer & Loss Value Local1O /Local2O Interpolation ( Œª ) 0.40 Single1O /Single2O Interpolation ( Œª ) 0.07 AdamŒ≤1 0 AdamŒ≤2 0.95 Decay Rate 0.85 Decay Step ( without devimprovement ) 500 Weight Initialization Mean / Stddev Unary weight 0.0/1.0 Binary weight 0.0/0.25 Table 1 : Hyper - parameter for Local1O , Single2O and Local2O in our experiment . 
and Manning ( 2018 ) , and our two second - order approaches respectively . 
For all the approaches , we use the MST algorithm to guarantee treestructured output in testing . 
We use the concatenation of word embeddings , character - level embeddings and part - of - speech ( POS ) tag embeddings to represent words and additionally concatenate BERT embeddings for experiments with BERT . 
For a fair comparison with previous work , we use GloVe ( Pennington et al . 
, 2014 ) and BERTLarge - Uncased model for PTB , and structuredskipgram ( Ling et al . 
, 2015 ) and BERT - BaseChinese model for CTB . 
For UD , we use fastText embeddings ( Bojanowski et al . 
, 2017 ) and BERTBase - Multilingual - Cased model for different languages . 
We set the default iteration number for our approaches to 3 because we Ô¨Ånd no improvement on more or less iterations . 
ForGNN1 , we rerun the code based on the ofÔ¨Åcial release of Ji et al . 
( 2019 ) . 
For Single1O , Local1O2,Single2O3 , we implement these ap1https://github.com/AntNLP/ gnn - dep - parsing 2https://github.com/tdozat/Parser-v3 3https://github.com/wangxinyu0922/ Second_Order_SDP95 PTB CTB UAS LAS UAS LAS Dozat and Manning ( 2017 ) 95.74 94.08 89.30 88.23 Ma et al . 
( 2018) ‚ô† 95.87 94.19 90.59 89.29 F&G ( 2019) ‚ô† 96.04 94.43 - GNN 95.87 94.15 90.78 89.50 Single1O 95.75 94.04 90.53 89.28 Local1O 95.83 94.23 90.59 89.28 Single2O 95.86 94.19 90.75 89.55 Local2O 95.98 94.34 90.81 89.57 Ji et al . 
( 2019)‚Ä†95.97 94.31 - Zhang et al . 
( 2020)‚Ä†‚Ä°96.14 94.49 - Local2O‚Ä†‚Ä°96.12 94.47 - + BERT Zhou and Zhao ( 2019) ‚ô£ 97.20 95.72 Clark et al . 
( 2018)/diamondmath96.60 95.00 - Single1O 96.82 95.20 92.73 91.64 Local1O 96.86 95.32 92.47 91.30 Single2O 96.86 95.31 92.78 91.69 Local2O 96.91 95.34 92.55 91.38 Table 2 : Comparison of our approaches and the previous state - of - the - art approaches on PTB and CTB . 
We report our results averaged over 5 runs.‚Ä† : These approaches perform model selection based on the score on the development set.‚Ä° : These approaches do not use POS tags as input./diamondmath : Clark et al . 
( 2018 ) uses semisupervised multi - task learning with ELMo embeddings . 
‚ô† : These approaches use structured - skipgram embeddings instead of GloVe embeddings for PTB . 
‚ô£ : For reference , Zhou and Zhao ( 2019 ) utilized both dependency and constituency information in their approach . 
Therefore , the results are not comparable to our results . 
proaches based on the ofÔ¨Åcial release code of Wang et al . 
( 2019 ) and we implement Local2O based on this code . 
In speed comparison , we implement the second - order approaches based on an PyTorch implementation biafÔ¨Åne parser4implemented by Zhang et al . 
( 2020 ) for a fair speed comparison with their approach5 . 
Since we Ô¨Ånd that the accuracy of our approaches based on PyTorch implementation on PTB does not change , we only report scores based on Wang et al . 
( 2019 ) . 
3.2 Hyper - parameters The hyper - parameters we used in our experiments is shown in Table 1 . 
We tune the the hidden size for calculating s(edge ) ij(Unary Arc in the table ) separately for PTB and CTB . 
Following Qi et al . 
( 2018 ) , we switch to AMSGrad ( Reddi et al . 
, 2018 ) after 5,000 iterations without improvement . 
We train models for 75,000 iterations with batch sizes of 4https://github.com/yzhangcs/parser 5At the time we Ô¨Ånished the paper , the ofÔ¨Åcial code for the second - order tree CRF parser have not release yet . 
We believe it is a fair comparison since we use the same settings and GPU as Zhang et al . 
( 2020).6000 tokens and stopped the training early after 10,000 iterations without improvements on development sets . 
Different from previous approaches such as Dozat and Manning ( 2017 ) and Ji et al . 
( 2019 ) , we use Adam ( Kingma and Ba , 2015 ) with a learning rate of 0.01 and anneal the learning rate by 0.85 for every 500 iterations without improvement on the development set for optimization . 
For GNN , we train the models with the same setting as in Ji et al . 
( 2019 ) . 
We do not use character embeddings and our optimization settings for GNN because we Ô¨Ånd they do not improve the accuracy . 
For the edge loss of Single approaches , Zhang et al . 
( 2019 ) proposed to sample a subset of the negative edges to balance positive and negative examples , but we Ô¨Ånd that using a relatively small interpolation Œª(shown in Table 1 ) on label loss can improve the accuracy and the sampling does not help further improve the accuracy . 
3.3 Results Table 2 shows the Unlabeled Attachment Score ( UAS ) and Labeled Attachment Score ( LAS ) of all the approaches as well as the reported scores of previous state - of - the - art approaches on PTB and CTB . 
It can be seen that without BERT , our Local2O achieves state - of - the - art performance on CTB and has almost the same accuracy as the very recent work of Zhang et al . 
( 2020 ) on PTB . 
With BERT embeddings , Local2O performs the best on PTB while Single2O has the best accuracy on CTB . 
Table 3 shows the results of the Ô¨Åve approaches on UD in addition to PTB and CTB . 
We make the following observations . 
First , our second - order approaches outperform GNN and the Ô¨Årst - order approaches both with and without BERT embeddings , showing that second - order decoders are still helpful in neural parsing even with strong contextual embeddings . 
Second , without BERT , Local slightly outperforms Single , although the difference between the two is quite small6 ; when BERT is used , however , Single clearly outperforms Local , which is quite interesting and warrants further investigation in the future . 
Third , the relative strength ofLocal andSingle approaches varies over treebanks , suggesting varying importance of the headselection constraint . 
6Note that Zhang et al . 
( 2019 ) reports higher difference in accuracy between Ô¨Årst - order Local andSingle approaches . 
The discrepancy is most likely caused by our better designed loss function and tuned hyper - parameters.96 PTB CTB bg ca cs de en es fr it nl no ro ru Avg . 
GNN 94.15 89.50‚Ä†90.33 92.39 90.95 79.73 88.43 91.56 87.23 92.44 88.57 89.38 85.26 91.20 89.37 Single1O 94.04 89.28 90.05 92.72‚Ä†92.07 81.73 89.55 92.10 88.27 92.64 89.57 91.81 85.39 92.60 90.13 Local1O 94.23 89.28 90.30 92.56 92.15 81.42 89.43 91.99 88.26 92.49 89.76 91.91 85.27 92.72 90.13 Single2O 94.19 89.55‚Ä†90.24 92.82‚Ä†92.13 81.99‚Ä†89.64‚Ä†92.17‚Ä†88.69 92.83‚Ä†89.97‚Ä†91.90 85.53‚Ä†92.58 90.30‚Ä† Local2O 94.34‚Ä†‚Ä°89.57‚Ä†90.53‚Ä†92.83‚Ä†92.12 81.73 89.72‚Ä†92.07 88.53 92.78 90.19‚Ä†91.88 85.88‚Ä†‚Ä°92.67 90.35‚Ä† + BERT Single1O 95.20 91.64‚Ä†90.87 93.55‚Ä†92.01 81.95‚Ä†90.44‚Ä†92.56‚Ä†89.35 93.44‚Ä†90.89 91.78 86.13‚Ä†92.51 90.88‚Ä† Local1O 95.32 91.30 91.03 93.17 91.93 81.66 90.09 92.32 89.26 93.05 90.93 91.62 85.67 92.51 90.70 Single2O 95.31 91.69‚Ä†‚Ä°91.30‚Ä†93.60‚Ä†‚Ä°92.09‚Ä†82.00‚Ä†‚Ä°90.75‚Ä†‚Ä°92.62‚Ä†‚Ä°89.32 93.66‚Ä†91.21 91.74 86.40‚Ä†92.61 91.02‚Ä†‚Ä° Local2O 95.34 91.38 91.13 93.34‚Ä†92.07‚Ä†81.67 90.43‚Ä†92.45‚Ä†89.26 93.50‚Ä†90.99 91.66 86.09‚Ä†92.66 90.86‚Ä† Table 3 : LAS and standard deviations on test sets . 
We report results averaged over 5 runs . 
We use ISO 639 - 1 codes to represent languages from UD . 
‚Ä†means that the model is statistically signiÔ¨Åcantly better than the Local1O model by Wilcoxon rank - sum test with a signiÔ¨Åcance level of p < 0.05 . 
We use‚Ä°to represent winner of the signiÔ¨Åcant test between the Single2O andLocal2O models . 
System Train Test Time Complexity GNN 392 464 O(n2d ) Zhang et al . 
( 2020 ) 200 400 O(n3 ) Single1O 616 1123 O(n2 ) Local1O 625 1150 O(n2 ) Single2O 481 966 O(n3 ) Local2O 486 1006 O(n3 ) Table 4 : Comparison of training and testing speed ( sentences per second ) and the time complexity of the decoders of different approaches on PTB . 
3.4 Speed Comparison We evaluate the speed of different approaches on a single GeForce GTX 1080 Ti GPU following the setting of Zhang et al . 
( 2020 ) . 
As shown in Table 4 , our Local approach and Single approach have almost the same speed . 
Our second - order approaches only slow down the training and testing speed in comparison with the Ô¨Årst - order approaches by 23 % and 12 % respectively . 
They are also signiÔ¨Åcantly faster than previous state - of - theart approaches . 
Our Local approach is 1.2 and 2.3 times faster than GNN in training and testing respectively and is 2.4 and 2.9 times faster than the second - order tree CRF approach of Zhang et al . 
( 2020 ) . 
In terms of time complexity , our second - order decoders have a time complexity of O(n3)7 ; while the time complexity of GNN isO(n2d ) , the hidden sized(500 by default ) is typically much larger than sentence length n ; and the decoder of Zhang et al . 
( 2020 ) has a time complexity of O(n3)as well , but it requires sequential computation over the input sentence while our decoders can be parallelized 7The MST algorithm has a time complexity of O(n2)and we follow Dozat et al . 
( 2017 ) only using the MST algorithm when the argmax predictions of structured output are not trees.over words of the input sentence . 
4 Conclusion We propose second - order graph - based dependency parsing based on message passing and end - toend neural networks . 
We modify a previous approach that predicts dependency edges independently and also design a new approach that incorporates the head - selection structured constraint . 
Our experiments show that our second - order approaches have better overall performance than the Ô¨Årst - order baselines ; they achieve competitive accuracy with very recent start - of - the - art second - order graph - based parsers and are signiÔ¨Åcantly faster . 
Our empirical comparisons also show that secondorder decoders still outperform Ô¨Årst - order decoders even with BERT embeddings , and that the usefulness of the head - selection constraint is limited , especially when using BERT embeddings . 
Our code is publicly avilable at https://github.com/ wangxinyu0922 / Second_Order_Parsing . 
Acknowledgements This work was supported by the National Natural Science Foundation of China ( 61976139 ) . 
Abstract Current end - to - end semantic role labeling is mostly accomplished via graph - based neural models . 
However , these all are Ô¨Årst - order models , where each decision for detecting any predicate - argument pair is made in isolation with local features . 
In this paper , we present a high - order reÔ¨Åning mechanism to perform interaction between all predicate - argument pairs . 
Based on the baseline graph model , our highorder reÔ¨Åning module learns higher - order features between all candidate pairs via attention calculation , which are later used to update the original token representations . 
After several iterations of reÔ¨Ånement , the underlying token representations can be enriched with globally interacted features . 
Our high - order model achieves state - of - the - art results on Chinese SRL data , including CoNLL09 and Universal Proposition Bank , meanwhile relieving the long - range dependency issues . 
1 Introduction Semantic role labeling ( SRL ) , as the shallow semantic parsing aiming to detect the semantic predicates and their argument roles in texts , plays a core role in natural language processing ( NLP ) community ( Pradhan et al . 
, 2005 ; Zhao et al . 
, 2009 ; Lei et al . 
, 2015 ; Xia et al . 
, 2019b ) . 
SRL is traditionally handled by two pipeline steps : predicate identiÔ¨Åcation ( Scheible , 2010 ) and argument role labeling ( Pradhan et al . 
, 2005 ) . 
More recently , growing interests are paid for developing end - to - end SRL , achieving both two subtasks , i.e. , recognizing all possible predicates together with their arguments jointly , via one single model ( He et al . 
, 2018a ) . 
The end - to - end joint architecture can greatly alleviate the error propagation problem , thus helping to achieve better task performance . 
Currently , the end - to - end SRL methods largely are graph - based ‚àóCorresponding author.neural models , enumerating all possible predicates and their arguments exhaustively ( He et al . 
, 2018a ; Cai et al . 
, 2018 ; Li et al . 
, 2019 ) . 
However , these Ô¨Årst - order models that only consider one predicateargument pair at a time can be limited to short - term features and local decisions , thus being subjective to long - range dependency issues existing at large surface distances between arguments ( Chen et al . 
, 2019 ; Lyu et al . 
, 2019 ) . 
This makes it imperative to capture the global interactions between multiple predicates and arguments . 
In this paper , based on the graph - based model architecture , we propose to further learn the higherorder interaction between all predicate - argument pairs by performing iterative reÔ¨Åning for the underlying token representations . 
Figure 1 illustrates the overall framework of our method . 
The BiLSTM encoder ( Hochreiter and Schmidhuber , 1997 ) Ô¨Årst encodes the inputs into the initial token representations for producing predicate and argument representations , respectively . 
The biafÔ¨Åne attention then exhaustively calculates the score representations for all the candidate predicate - argument pairs . 
Based on all these score representations , our highorder reÔ¨Åning module generates high - order feature for each corresponding token via an attention mechanism , which is then used for upgrading the raw token representation . 
After total Niterations of the above reÔ¨Åning procedure , the information between the predicates and the associated arguments can be fully interacted , and thus results in global consistency for SRL . 
On the other hand , most of the existing SRL studies focus on the English language , while there is little work in Chinese , mainly due to the limited amount of annotated data . 
In this study , we focus on the Chinese SRL . 
We show that our proposed high - order reÔ¨Åning mechanism can be especially beneÔ¨Åcial for such lower - resource language . 
Meanwhile , our proposed reÔ¨Åning process is fully100 FFNs ..... + .+‚äïw1 w2 b. . 
.++ ... .... . 
. 
... ... BiLSTM encoderBiaffine attention Input   representationToken representationArgument   representationToken representationHigh -order feature representationRefined t oken representation Score   representation i - th refining   iteration Predicate   representationFigure 1 : The overview of the graph - based high - order model for end - to - end SRL . 
The dotted - line green box is our proposed high - order reÔ¨Åning module . 
parallel and differentiable . 
We conduct experiments on the dependencybased Chinese SRL datasets , including CoNLL09 ( Haji Àác et al . 
, 2009 ) , and Universal Proposition Bank ( Akbik et al . 
, 2015 ; Akbik and Li , 2016 ) . 
Results show that the graph - based end - to - end model with our proposed high - order reÔ¨Åning consistently brings task improvements , compared with baselines , achieving state - of - the - art results for Chinese end - to - end SRL . 
2 Related Work Gildea and Jurafsky ( 2000 ) pioneer the task of semantic role labeling , as a shallow semantic parsing . 
Earlier efforts are paid for designing hand - crafted discrete features with machine learning classiÔ¨Åers ( Pradhan et al . 
, 2005 ; Punyakanok et al . 
, 2008 ; Zhao et al . 
, 2009 ) . 
Later , a great deal of work takes advantages of neural networks with distributed features ( FitzGerald et al . 
, 2015 ; Roth and Lapata , 2016 ; Marcheggiani and Titov , 2017 ; Strubell et al . 
, 2018 ) . 
On the other hand , many previous work shows that integrating syntactic tree structure can greatly facilitate SRL ( Marcheggiani et al . 
, 2017 ; He et al . 
, 2018b ; Zhang et al . 
, 2019 ; Fei et al . 
, 2020b ) . 
Prior studies traditionally separate SRL into two individual subtasks , i.e. , predicate disambiguation and argument role labeling , mostly conducting only the argument role labeling based on the pre - identiÔ¨Åed predicate ( Pradhan et al . 
, 2005 ; Zhao et al . 
, 2009 ; FitzGerald et al . 
, 2015 ; He et al . 
, 2018b ; Fei et al . 
, 2020a ) . 
More recently , several researches consider the end - to - end solution that handles both two subtasks by one single model . 
All of them employs graph - based neural model , exhaustively enumerating all the possible predicate and argument mentions , as well as their relations ( He et al . 
, 2018a ; Cai et al . 
, 2018 ; Li et al . 
, 2019 ; Xia et al . 
, 2019a ) . 
Most of these end - to - end models , however , are Ô¨Årst - order , considering merely one predicate - argument pair at a time . 
In this work , we propose a high - order reÔ¨Åning mechanism to reinforce the graph - based end - to - end method . 
Note that most of the existing SRL work focuses on the English language , with less for Chinese , mainly due to the limited amount of annotated data ( Xia et al . 
, 2019a ) . 
In this paper , we aim to improve the Chinese SRL and make compensation of the data scarcity by our proposed high - order model . 
3 Framework Task formulation . 
Following prior end - to - end SRL work ( He et al . 
, 2018a ; Li et al . 
, 2019 ) , we treat the task as predicate - argument - role triplets prediction . 
Given an input sentence S= { w1,¬∑¬∑¬∑,wn } , the system is expected to output a set of tripletsY ‚àà P√óA√óR , whereP= { p1,¬∑¬∑¬∑,pm}are all possible predicate tokens , A={a1,¬∑¬∑¬∑,al}are all associated argument tokens , andRare the corresponding role labels for eachai , including a null label /epsilon1indicating no relation between a pair of predicate argument . 
3.1 Baseline Graph - based SRL Model Our baseline SRL model is mostly from He et al . 
( 2018a ) . 
First , we obtain the vector representation xw tof each word wtfrom pre - trained embeddings . 
We then make use of the part - of - speech ( POS ) tag for each word , and use its embedding xpos t. A convolutional neural networks ( CNNs ) is used to encode Chinese characters inside a word xc t. We concatenate them as input representations : xt= [ xw t;xpos t;xc t ] . 
Thereafter , a multi - layer bidirectional LSTM ( BiLSTM ) is used to encode the input representations into contextualized token representations : h1,¬∑¬∑¬∑,hn = BiLSTM ( x1,¬∑¬∑¬∑,xn ) . 
Based on the token representations , we further generate the separate predicate representations and argument representations : vp t = FFN(ht),va t = FFN(ht).101 Then , a biafÔ¨Åne attention ( Dozat and Manning , 2016 ) is used for scoring the semantic relationships exhaustively over all the predicate - argument pairs : vs(pi , aj ) = vp i¬∑W1¬∑va j+W2¬∑[vp i;va j]+b,(1 ) where W1,W2andbare parameters . 
Decoding and learning . 
Once a predicateargument pair ( pi , aj)(i.e . 
, the role label r / negationslash=/epsilon1 ) is determined by a softmax classiÔ¨Åer , based on the score representation vs(pi , aj ) , the model outputs this tuple ( p , a , r ) . 
During training , we optimize the probability PŒ∏(ÀÜy|S)of the tupley(pi , aj , r)over a sentence S : PŒ∏(y|S ) = /productdisplay p‚ààP , a‚ààA , r‚ààRPŒ∏(y(p , a , r)|S ) = /productdisplay p‚ààP , a‚ààA , r‚ààRœÜ(p , a , r ) /summationtext ÀÜr‚ààRœÜ(p , a,ÀÜr),(2 ) whereŒ∏is the parameters of the model and œÜ(p , a , r ) represents the total unary score from : œÜ(p , a , r ) = WpReLU ( vp ) + WaReLU ( va ) + WsReLU ( vs(p , a ) ) . 
( 3 ) The Ô¨Ånal objective is to minimize the negative loglikelihood of the golden structure : L=‚àílogPŒ∏(y|S ) . 
( 4 ) 3.2 Higher - order ReÔ¨Åning The baseline graph model is a Ô¨Årst - order model , since it only considers one predicate - argument pair ( as in Eq . 
3 ) at a time . 
This makes it limited to short - term and local decisions , and thus subjective to long - distance dependency problem wherever there are larger surface distances between arguments . 
We here propose a higher - order reÔ¨Åning mechanism for allowing a deep interactions between all predicate - argument pairs . 
Our high - order model is shown in Figure 1 . 
Compared with the baseline model , the main difference lies in the high - order reÔ¨Åning module . 
Our motivation is to inform each predicate - argument pair with the information of the other rest of pairs from the global viewpoint . 
We reach this by reÔ¨Åning the underlying token representations htwith reÔ¨Åned ones which carry high - order interacted features . 
Concretely , we take the baseline as the initiation , performing reÔ¨Ånement iteratively . 
At the i - th reÔ¨Åning iteration , we can collect the score representations Vi , s={vi , s 1,¬∑¬∑¬∑,vi , s K}of all candidatepredicate - argument pairs , where K(i.e . 
,/parenleftbign 2 / parenrightbig ) are the total combination number of these pairs . 
Based onVi , s , we then generate the high - order feature vector oi tby using an attention mechanism guided by the current token representation hi‚àí1 tfor word wtat last turn , i.e. , the ( i-1)-th iteration : ui k = tanh(W3hi‚àí1 t+W4vi , s k ) , Œ±i k = softmax ( ui k ) , oi t=/summationtextK k=1Œ±i kvi , s k,(5 ) where W3andW4are parameters . 
We then concatenate the raw token representation and highorder feature representation together , and obtain the reÔ¨Åned token representation after a non - linear projection ÀÜhi t = FFN([oi t;hi‚àí1 t ] ) . 
Finally , we use ÀÜhi t to update the old one hi‚àí1 t. After totalNiterations of high - order reÔ¨Ånement , we expect the model to capture more informative features at global scope and achieve the global consistency . 
4 Experiments 4.1 Settings Our method is evaluated on the Chinese SRL benchmarks , including CoNLL091and Universal Proposition Bank ( UPB)2 . 
Each dataset comes with its own training , development and test sets . 
Precision , recall and F1 score are used as the metrics . 
We use the pre - trained Chinese fasttext embeddings3 . 
The BiLSTM has hidden size of 350 , with three layers . 
The kernel sizes of CNN are [ 3,4,5 ] . 
We adopt the Adam optimizer with initial learning rate of 1e-5 . 
We train the model by mini - batch size in [ 16,32 ] with early - stop strategy . 
We also use the contextualized Chinese word representations , i.e. , ELMo4and BERT ( Chinese - base - version)5 . 
4.2 Main Results We mainly make comparisons with the recent endto - end SRL models , as well as the pipeline methods on standalone argument role labeling given the gold predicates . 
Table 1 shows the results on the Chinese CoNLL09 . 
We Ô¨Årst Ô¨Ånd that the joint detection for predicates and arguments can be more beneÔ¨Åcial 1https://catalog.ldc.upenn.edu/ LDC2012T03 2https://github.com/System-T/ UniversalPropositions 3https://fasttext.cc/ 4https://github.com/HIT-SCIR/ ELMoForManyLangs 5https://github.com/google-research/ bert102 Arg . 
Prd . 
P R F1 F1 ‚Ä¢Pipeline method Zhao et al . 
( 2009 ) 80.4 75.2 77.7 Bj¬®orkelund et al . 
( 2009 ) 82.4 75.1 78.6 Roth and Lapata ( 2016 ) 83.2 75.9 79.4 Marcheggiani and Titov ( 2017 ) 84.6 80.4 82.5 He et al . 
( 2018b ) 84.2 81.5 82.8 Cai and Lapata ( 2019)‚Ä°85.4 84.6 85.0 ‚Ä¢End - to - end method He et al . 
( 2018a ) 82.6 83.6 83.0 85.7 Cai et al . 
( 2018 ) 84.7 84.0 84.3 86.0 Li et al . 
( 2019 ) 84.9 84.6 84.8 86.9 Xia et al . 
( 2019a ) 84.6 85.7 85.1 87.2 + BERT 88.0 89.1 88.5 89.6 Ours 85.7 86.2 85.9 88.6 + ELMo 86.4 87.6 87.1 88.9 + BERT 87.4 89.3 88.8 90.3 Table 1 : Performances on CoNLL09 . 
Results with‚Ä° indicates the additional resources are used . 
P R F1 He et al . 
( 2018a ) 64.8 65.3 64.9 Cai et al . 
( 2018 ) 65.0 66.4 65.8 Li et al . 
( 2019 ) 65.4 67.2 66.0 Xia et al . 
( 2019a ) 65.2 67.6 66.1 Ours 67.5 68.8 67.9 + ELMo 68.0 70.6 68.8 + BERT 70.0 73.0 72.4 Table 2 : Performances by end - to - end models for the argument role labeling on UPB . 
than the pipeline detection of SRL , notably with 85.1 % F1 score on argument detection by Xia et al . 
( 2019a ) . 
Most importantly , our high - order end - toend model outperforms all these baselines on both two subtasks , with 85.9 % F1 score for argument role labeling and 88.6 % F1 score for predicate detection . 
When the contextualized word embeddings are available , we Ô¨Ånd that our model can achieve further improvements , i.e. , 88.8 % and 90.3 % F1 scores for two subtasks , respectively . 
Table 2 shows the performances on UPB . 
Overall , the similar trends are kept as that on CoNLL09 . 
Our high - order model still performs the best , yielding 67.9 % F1 score on argument role labeling , verifying its prominent capability for the SRL task . 
Also with BERT embeddings , our model further wins a great advance of performances . 
4.3 Analysis High - order reÔ¨Ånement . 
We take a further step , looking into our proposed high - order reÔ¨Åning 1 2 3 4 583858789 IterationsF1(%)Arg.(Ours ) Prd.(Ours ) Arg.(He et al . 
( 2018 ) ) Prd.(He et al . 
( 2018))Figure 2 : Performances by varying reÔ¨Åning iterations . 
1 2 3 - 4 5 - 6 7 - 8 ‚â•9607590 Distance from predicate to argument ( words)F1(%)Ours He et al . 
( 2018 ) Li et al . 
( 2019 ) Figure 3 : Argument recognition under varying surface distance between predicates and arguments . 
mechanism . 
We examine the performances under varying reÔ¨Åning iterations in Figure 2 . 
Compared with the Ô¨Årst - order baseline model by He et al . 
( 2018a ) , our high - order model can achieve better performances for both two subtasks . 
We Ô¨Ånd that our model can reach the peak for predicate detection with total 2 iterations of reÔ¨Ånement , while the best iteration number is 4 for argument labeling . 
Long - distance dependencies . 
Figure 3 shows the performances of argument recognition by different surface distance between predicates and arguments . 
The overall results decrease when arguments are farther away from the predicates . 
Nevertheless , our high - order model can beat against such drop signiÔ¨Åcantly . 
Especially when the distance grows larger , e.g. , distance ‚â•7 , the winning score by our model even becomes more notable . 
5 Conclusion We proposed a high - order end - to - end model for Chinese SRL . 
Based on the baseline graph - based model , our high - order reÔ¨Åning module performed interactive learning between all predicate - argument pairs via attention calculation . 
The generated higher - order featured token representations then were used to update the original ones . 
After total Niterations of reÔ¨Ånement , we enriched the underlying token representations with global interactions , and made the learnt features more informative . 
Our103 high - order model brought state - of - the - art results on Chinese SRL data , i.e. , CoNLL09 and Universal Proposition Bank , meanwhile relieving the longrange dependency issues . 
Acknowledgments We thank the anonymous reviewers for their valuable and detailed comments . 
This work is supported by the National Natural Science Foundation of China ( No . 
61772378 , No . 
61702121 ) , the National Key Research and Development Program of China ( No . 
2017YFC1200500 ) , the Research Foundation of Ministry of Education of China ( No . 
18JZD015 ) , the Major Projects of the National Social Science Foundation of China ( No . 
11&ZD189 ) , the Key Project of State Language Commission of China ( No . 
ZDI135 - 112 ) and Guangdong Basic and Applied Basic Research Foundation of China ( No . 
2020A151501705 ) . 
Abstract Answer selection ( AS ) is an important subtask of document - based question answering ( DQA ) . 
In this task , the candidate answers come from the same document , and each answer sentence is semantically related to the given question , which makes it more challenging to select the true answer . 
WordNet provides powerful knowledge about concepts and their semantic relations , so we employ WordNet to enrich the abilities of paraphrasing and reasoning of the network - based question answering model . 
SpeciÔ¨Åcally , we exploit the synset and hypernym concepts to enrich the word representation and incorporate the similarity scores of two concepts that share the synset or hypernym relations into the attention mechanism . 
The proposed WordNet - enhanced hierarchical model ( WEHM ) consists of four modules , including WordNet - enhanced word representation , sentence encoding , WordNetenhanced attention mechanism , and hierarchical document encoding . 
Extensive experiments on the public WikiQA and SelQA datasets demonstrate that our proposed model signiÔ¨Åcantly improves the baseline system and outperforms all existing state - of - the - art methods by a large margin . 
1 Introduction Answer selection ( AS ) is a challenging subtask of document - based question answering ( DQA ) in natural language processing ( NLP ) . 
The AS task is to select a whole answer sentence from the document and can be regarded as a ranking problem , which is different from the machine reading comprehension ( MRC ) task on the SQuAD and MS - MARCO datasets . 
Compared with a single word or phrase , returning the full sentence often adds more value as the user can easily verify the correctness without reading a lengthy document ( Yih et al . 
, 2013 ) . 
In ‚àóCorresponding author.this paper , we focus on the AS task of DQA . 
Table 1 gives a real example of this task . 
Lots of fruits on answer selection have been achieved via deep learning models , including convolutional neural network ( CNN ) ( Yang et al . 
, 2015 ) , recurrent neural network ( RNN ) ( Tan et al . 
, 2015 ) , attention - way ( Wang et al . 
, 2016 ) and generative adversarial networks ( GAN ) ( Wang et al . 
, 2017a ) . 
Recently proposed models often consist of an embedding layer , an encoding layer , an interaction layer , and an answer layer ( Weissenborn et al . 
, 2017 ; Wang et al . 
, 2017b ; Hewlett et al . 
, 2017 ) . 
Different from other question answering like community - based question answering , the candidate answers of DQA come from the same document , and each candidate answer is semantically related to the question . 
From the example in Table 1 , we can see that almost every candidate answer contains the information related to the word ‚Äú food ‚Äù and ‚Äú afghan ‚Äù in the given question . 
As a result , it is difÔ¨Åcult for the existing network - based models to choose the right answer , since the power generation ability of the networks may have transformed the sentences into similar meanings in the latent space . 
To tackle this challenge , we propose to leverage WordNet knowledge base into the neural network model . 
Our hypothesis is that the ability of paraphrase and reasoning is essential to the questionanswering task . 
WordNet is a semantic network ( Fellbaum , 1998 ) , where the words that are related in meanings are interlinked by means of pointers , which stand for different semantic relations . 
It organizes concepts mainly with the is - a relation , where a concept is a set of word senses ( synset ) . 
On the one hand , we apply the synset information to enrich the sentence ‚Äôs paraphrase representation , which could distinguish the candidate answers in the latent semantic space to some degree . 
On the other hand , we apply the hypernym information to capture reasoning knowledge . 
The real case106 Question : what food is in afghan ? Document : [ 1]A table setting of Afghan food in Kabul . 
[ 2]Afghan cuisine is largely based upon the nation ‚Äôs chief crops ; cereals like wheat , maize , barley and rice . 
[ 3 ] ...... [ 4]Afghanistan ‚Äôs culinary specialties reÔ¨Çect its ethnic and geographic diversity . 
[ 5]Though it has similarities with neighboring countries , Afghan cuisine is undeniably unique . 
[ 6 ] ...... Reference Answer : Afghan cuisine is largely based upon the nation ‚Äôs chief crops ; cereals like wheat , maize , barley and rice . 
Table 1 : An example from the WikiQA data . 
The text is shown in its original form , which may contain errors in typing . 
from the WikiQA dataset in table 1 shows that if our model has the ability of reasoning on common sense , like ‚Äú wheat is a kind of food ‚Äù , ‚Äú maize is a kind of food ‚Äù and so on , it would be of great help for choosing the right answer with respect to the question ‚Äú what food is in afghan ? ‚Äù . 
The overall framework of our proposed model is shown in Figure 1 , which mainly consists of four modules . 
First , we apply the synset and hypernym information to enrich the word representation . 
Second , we use an RNN module to encode the WordNet - enhanced word representation . 
Third , we propose to use the synset ‚Äôs and hypernym ‚Äôs relation score based on two senses ‚Äô path in the WordNet to enrich the attention mechanism . 
SpeciÔ¨Åcally , the attention similarity matrix is not only measured by a similarity score over hidden vectors produced by CNN or RNN networks but also measured based on the synset and hypernym relation scores of two concepts in Wordnet . 
And then following the compareaggregate framework ( Wang and Jiang , 2016 ) , we combine the original representation with the attention representation . 
Finally , considering the strong relations among context sentences , we employ a hierarchical neural network for answer sentence selection . 
We conduct extensive experiments on the public WikiQA and SelQA datasets . 
The results show that our proposed WordNet - enhanced hierarchical model outperforms the baseline models by a large margin and achieves state - of - the - art performance on both datasets . 
On the WikiQA data , it obtains a MAP of 77.02 , which beats the existing best result by 1.62 points ; on the SelQA data , it achieves a MAP of 91.71 , which outperforms the previous best result by 2.57 points . 
2 Model Description Given a question qand the sentences ai , i= 1,2, ... ,S in a document d , our model aimsto select the best sentence which could answer the question . 
2.1 WordNet - Enhanced Word Representation Firstly , we map each word into the vector space . 
Different from directly using word embedding or the concatenation of word embedding and sum of its character embeddings , we propose to exploit the word ‚Äôs hypernym and synset in the WorNet to enrich the word representation . 
Suppose wjis the jthword in a sequence , ksjandkhjrepresent the hypernym and synset in the WordNet with respect to the wordwj . 
The WordNet - enhanced word embedding is computed as follows : kj= [ wj;ksj;khj ] ( 1 ) ksj=1 |S|/summationdisplay|S| i=1wksj i ( 2 ) khj=1 |H|/summationdisplay|H| i=1wkhj i ( 3 ) wherewksj iandwkhj irepresent word embeddings in the synset and hypernym concepts respectively ; |S|and|H|denote the number of concepts in the synset and hypernym respectively . 
And ; means the concatenation operation . 
We usekq jandkai jto represent the jthword ‚Äôs WordNet - enhanced embedding of the question and theithcandidate answer sentence respectively . 
2.2 Sentcene Encoding We encode the question and each sentence in the document into latent vectors using a Bi - directional Gated Recurrent Unit ( Bi - GRU ) network . 
The formulas of a GRU ( Cho et al . 
, 2014 ) are as follows : rj = œÉ(Wrkj+Urhj‚àí1+br ) ( 4 ) zj = œÉ(Wzkj+Uzhj‚àí1+bz ) ( 5)107 ! " # $ % & " # $ soorr%'"#$%"#$!(#$%&(#$%'(#$%(#$!)#$%&)#$%')#$%)#$ ‚Ä¶ ... ‚Ä¶ ... ‚Ä¶ ... ‚Ä¶ ... !"*%&"*%'"*%"*!(*%&(*%'(*%(*!+*%&+*%'+*%+* ‚Ä¶ ... ‚Ä¶ ... ‚Ä¶ ... ‚Ä¶ ... WordNet - EnhancedWordRepresentationWordNet - EnhancedAttentionMechanismSentenceEncoding!,#$%&,#$%',#$%,#$-."#$-.(#$-.)#$ ‚Ä¶ ... -.,#$'+/0+/1HierarchicalDocumentEncoding2#32#$2#4softmax wordnetAfghancuisineis ‚Ä¶ ‚Ä¶ riceQuestionCandidateAnswerWhatfood ‚Ä¶ ‚Ä¶ afghanFigure 1 : Framework of our proposed WordNet - enhanced hierarchical model ( WEHM ) . 
/tildewidehj= tanh(Whkj+Uh(rj‚äôhj‚àí1 ) + bh)(6 ) hj= ( 1‚àízj)‚äôhj‚àí1+zj‚äô/tildewidehj ( 7 ) where‚äôis element - wise multiplication . 
rjandzj are the reset and update gates respectively . 
And Wr , Wz , Wh‚ààRH√óE , Ur , Uz , Uh‚ààRH√óHand br , bz , bh‚ààRH√ó1are parameters to be learned . 
A Bi - GRU processes the sequence in both forward and backward directions to produce two sequences [ hf 1,hf 2 , ... , hf S ] and [ hb 1,hb 2 , ... , hb S ] . 
The Ô¨Ånal output ofhjis the concatenation of hf jandhb j. We usehq jandhai jto represent jthword ‚Äôs hidden vector produced by sentence encoding in the question and in the ithcandidate answer sentence respectively . 
2.3 WordNet - Enhanced Attention Mechanism Different from the vanilla attention mechanism , where the attention score is only measured by hidden vectors , we propose to employ the synset and hypernym relation scores of two concepts in WordNet to enhance the attention mechanism , which can capture more rich interaction information between two sequences . 
The sketch of our proposed WordNet - enhanced attention mechanism is shown in Figure 2 , which consists of three parts : the standard attention score , the synset relation score , and the hypernym relation score . 
As for the standard attention mechanism , we adopt the Luong attention ( also known as bilinear function attention mechanism ) ( Luong et al . 
, 2015 ) , which is widely used in NLP . 
In our model , Mh |ai|,|q| represents the attention score between the question and one of its candidate answers . 
The formulas of computing each element are as follows : Mh n , m = hainWhq mT(8 ) Mh n , m= exp(Mh n , m)//summationdisplay|q| k=1exp(Mh n , k)(9 ) wherehainandhq mrepresent the nthandmthword hidden vector in the candidate answer and the question respectively , |ai|and|q|are the candidate answer ‚Äôs length and the question ‚Äôs length respectively . 
Besides the standard attention , we employ two kinds of WordNet - enhanced mechanism to measure the attention score . 
Lots of studies have been done on computing lexical similarity based on WordNet ( Pedersen et al . 
, 2004 ) . 
Wu - Palmer Similarity ( Wu and Palmer , 1994 ) denotes how similar two words senses are , based on the depth of the two senses in the taxonomy and that of their Least Common Subsumer . 
Leacock - Chodorow Similarity ( Leacock and Chodorow , 1998 ) denotes how similar twoword senses are , based on the shortest path that connects the senses in the is - a ( hypernym / hyponym ) taxonomy.108 QueryAttentionValue!"#$%&'()*"$!"#$%+!"#$,'()*"-'()*". ‚Ä¶ ‚Ä¶ !"#-%&!"#-%+!"#-,!"#.%&!"#.%+!"#.,Figure 2 : Sketch of our proposed WordNet - enhanced attention mechanism . 
Keyh jmeans the attention score derived by two hidden vectors . 
Keyks jandKeykh jrepresent the attention score derived by synset relation and hypernym relation respectively . 
Vlaue jmeans the hidden vector of question , and Query means the candidate answer . 
We use Wu - Palmer Similarity to compute the attention score with the synset relation . 
Mks |ai||q| represents the attention matrix between the question and one of its candidate answers , where each elementMksn , mis computed as : Mksn , m= 2‚àóNc/(Nain+Nqm+ 2‚àóNc)(10 ) Mksn , m= exp(Mksn , m)//summationdisplay|q| k=1exp(Mks n , k)(11 ) whereai nandqmrepresent the corresponding concepts of the nth word of the ith candidate answer and themth word of the question respectively , cis the least common superconcept of ai nandqm , Nain is the number of nodes on the path from ai ntoc , Nqmis the number of nodes on the path from Nqm toc , Ncis the number of nodes on the path from c to root . 
We use Leacock - Chodorow Similarity to measure the attention score with hypernym relation . 
LetMkh |ai||q|denote the attention matrix between the question and one of its candidate answers , where each element Mkhn , mcan be computed as : Mkhn , m=‚àílog(path(ai n , qm)/2L ) ( 12 ) Mkhn , m= exp(Mkhn , m)//summationdisplay|q| k=1exp(Mkhn , m)(13 ) wherepath(ai n , qm)is the shortest path length connecting two concepts and Lis the whole taxonomy depth . 
Finally , we combine all the three similarity matrixes . 
The formulas are as follows : Mn , m = Mh n , m+Mksn , m+Mkhn , m ( 14 ) Mn , m= exp(Mn , m)//summationdisplay|q| k=1exp(Mn , k)(15)Equipped with the WordNet - enhanced similarity matrixM , we apply the attention mechanism between the question encoding hqand the sentence encodinghaito obtain a new sentence representationvai , which is a weighted sum of hidden vectors of the question . 
We then aggregate the vectors of haiandvai . 
Formulas are as follows : vai = M¬∑hq(16 ) ÀÜvai= [ vai;hai;vai‚äôhai;vai+hai;vai‚àíhai ] ( 17 ) where ; is the concatenation operation , + is element - wise addition , ‚àíis element - wise subtraction and‚äôis element - wise multiplication . 
2.4 Hierarchical Document Encoding Inspired by the work ( Bian et al . 
, 2017 ) , we also adopt a list - wise method to model the answer selection task . 
But different from their model , we employ a hierarchical Bi - GRU architecture to compare candidate sentences by ranking them with respect to a given question . 
Considering that candidate answers all come from a whole document , the hierarchical Bi - GRU architecture can capture contextual features among sentences and make the understanding of a document more coherent . 
We Ô¨Årst encode each candidate answer ÀÜvaiand then extract features among sentences ‚Äô hidden vectors . 
Then we again encode the document based on each candidate answer ‚Äôs extracted features . 
The BiGRU is the same to that mentioned in our sentence encoding section . 
uai j = BiGRU / parenleftBig uai j‚àí1,ÀÜvai j / parenrightBig ( 18 ) uaiavg=1 |ai|/summationdisplay|ai| j=1uai j , uaimax=|ai|max j=1uai j(19 ) fai=/bracketleftBig uaiavg;uaimax / bracketrightBig ( 20 ) ÀÜud i = BiGRU / parenleftBig ÀÜud i‚àí1,fai / parenrightBig ( 21 ) wherejis thejthword in the ithsentence in the candidate answers , faiis theithsentence extracted features and ÀÜud iis theithsentence ‚Äôs hidden vector after the document encoding phase . 
At last , we use a softmax layer to choose the right answer among every step ‚Äôs output of the document ‚Äôs RNN layer . 
The model is trained to minimize the cross - entropy loss function : Àúai = œÉ(FC(ÀÜud i ) ) ( 22)109 Dataset Split # Questions # Pairs WikiQATRAIN 873 8672 DEV 126 1130 TEST 243 2351 SelQATRAIN 5529 66438 DEV 785 9377 TEST 1590 19435 Table 2 : Statistical distribution of two benchmark datasets . 
C=‚àí1 |d|/summationdisplay i‚àà|d|[ailog Àúai+ ( 1‚àíai ) log ( 1‚àíÀúai ) ] ( 23 ) whereFC is a feed - forward neural network , i means the sentence index in the document , |d|is the document ‚Äôs length in terms of sentences , aiis the true label ( 0 or 1 ) from the training data and Àúaiis the predicted probability score by our model . 
The sentence with the highest probability score is regarded as the right answer . 
3 Experiments 3.1 Datasets and Baselines We use two different datasets to conduct our answer selection experiments : WikiQA ( Yang et al . 
, 2015 ) and SelQA ( Jurczyk et al . 
, 2016 ) . 
Both datasets contain open - domain questions whose answers were extracted from Wikipedia articles . 
In the AS task , it is assumed that there is at least one correct answer for a question . 
In the WikiQA , there are some questions which have no answer , we removed these questions , just like other researches do . 
Table 2 shows the statistical distribution of the two datasets . 
As for the WikiQA dataset , it has been well studied by lots of literature . 
Baselines adopted are as follows : ‚Ä¢CNN - Cnt : this model combines sentence representations produced by a convolutional neural network with the logistic regression ( Yang et al . 
, 2015 ) . 
‚Ä¢ABCNN : this model is an attention - based convolutional neural network ( Yin et al . 
, 2015 ) . 
‚Ä¢IARNN - Occam : this model adds regularization on the attention weights ( Wang et al . 
, 2016).‚Ä¢IARNN - Gate : this model uses the question representation to build GRU gates for each candidate answer ( Wang et al . 
, 2016 ) . 
‚Ä¢CubeCNN : this model builds a CNN on all pairs of word similarities ( He and Lin , 2016 ) . 
‚Ä¢CA - Network : this model applies a compareaggregate neural network to model question answering problem ( Wang and Jiang , 2016 ) . 
‚Ä¢IWAN - Skip : this model measures the similarity of sentence pairs by focusing on the interaction information ( Shen et al . 
, 2017b ) . 
‚Ä¢Dynamic - Clip : this model proposes a novel attention mechanism named DynamicClip Attention , which is then directly integrated into the Compare - Aggregate framework . 
( Bian et al . 
, 2017 ) . 
As for the SelQA dataset , besides the above mentioned CNN - Cnt model , Jurczyk et al . 
( 2016 ) also re - implement CNN - Tree and two attention RNN models . 
Other baselines are as follows : ‚Ä¢CNN - hinge : this is a re - implemented CNNbased model with hinge loss function ( dos Santos et al . 
, 2017 ) . 
‚Ä¢CNN - DAN : dos Santos et al . 
( 2017 ) propose a CNN - based model trained with a DAN framework , which is to learn loss functions for predictors and also implements semisupervised learning . 
‚Ä¢AdaQA : Shen et al . 
( 2017a ) propose an adaptive question answering ( AdaQA ) model , which consists of a novel two - way feature abstraction mechanism to encapsulate codependent sentence representations . 
The answer selection task can be considered as a ranking problem , and so two evaluation metrics are used : mean average precision ( MAP ) and mean reciprocal rank ( MRR ) . 
3.2 Experiment Setup The proposed models are implemented with TensorFlow . 
The dimension of word embeddings is set to 300 . 
The word embeddings are initialized by 300D GloVe 840B ( Pennington et al . 
, 2014 ) , and out - of - vocabulary words are initialized randomly . 
We Ô¨Åx the embeddings during training . 
We train the model with the Adam optimization algorithm with110 Model MAP MRR CNN - Cnt ( Yang et al . 
, 2015 ) 65.20 66.52 ABCNN ( Yin et al . 
, 2015 ) 69.21 71.08 CubeCNN ( He and Lin , 2016 ) 70.90 72.34 IARNN - Gate ( Wang et al . 
, 2016 ) 72.58 73.94 IARNN - Occam ( Wang et al . 
, 2016 ) 73.41 74.18 CA - Network ( Wang and Jiang , 2016 ) 74.33 75.45 IWAN - Skip ( Shen et al . 
, 2017b ) 73.30 75.00 Dynamic - Clip ( Bian et al . 
, 2017 ) 75.40 76.40 WEHM ( Proposed ) 77.02 78.82 Table 3 : Experimental results on the WikiQA dataset Model MAP MRR CNN - Cnt ( Jurczyk et al . 
, 2016 ) 84.00 84.94 CNN - Tree ( Jurczyk et al . 
, 2016 ) 84.66 85.68 RNN : one - way ( Jurczyk et al . 
, 2016 ) 82.06 83.18 RNN : attn - pool ( Jurczyk et al . 
, 2016 ) 86.43 87.59 CNN - DAN ( dos Santos et al . 
, 2017 ) 86.55 87.30 CNN - hinge ( dos Santos et al . 
, 2017 ) 87.58 88.12 AdaQA ( Shen et al . 
, 2017a ) 89.14 89.83 WEHM ( Proposed ) 91.71 92.22 Table 4 : Experimental results on the SelQA dataset a learning rate of 0.001 . 
Our models are trained in mini - batches ( with a batch size of 10 ) . 
We Ô¨Åx the length of the question and each sentence in the document according to their sentence ‚Äôs max length in each mini - batch , and any sentences not enough to this range are padded . 
The hidden vector size is set to 150 for a single RNN . 
We conduct word sense disambiguation for ambiguous words via the nltk tool . 
3.3 Results and Analysis 3.3.1 Performance We compare our model with state - of - the - art methods on the WikiQA and SelQA dataset in Table 3 and Table 4 , respectively . 
Our proposed model not only obtains state - of - the - art performance on two datasets but also makes a signiÔ¨Åcant improvement . 
Compared with the existing best method DynamicClip , our model yields nearly 1.6 % improvement in MAP and 2.4 % in MRR on the WikiQA dataset . 
On the SelQA dataset , our model improves 2.6 % in MAP and 2.4 % in MRR , compared with the previous best method AdaQA . 
It is a challenging task for answer selection , especially for the WikiQA dataset . 
As is shown in Table 3 , the notable CA - network outperforms the IARNN - Occam approach only by 0.92 MAP points , and our best result ( 77.02 ) achieves a performance gain of 1.6 MAP points over the Dynamic - Clip . 
In this sense , the improvement of our model is valuable . 
Model MAP ‚àÜ/% without WordNet knowledge 84.87 ( 1 ) only hypernym token 85.35 0.48 ( 2 ) only synset token 85.17 0.30 ( 3 ) only hypernym&synset token 86.32 1.45 ( 4 ) only hypernym attention 90.21 5.34 ( 5 ) only synset attention 89.99 5.12 ( 6 ) only hypernym&synset attention 90.49 5.62 WEHM 91.71 6.84 Table 5 : Ablation study on the SelQA dataset 3.3.2 Ablation Study We further conduct an ablation study to explore different WordNet - enhanced components in our model , including WordNet - enhanced word embedding and WordNet - enhanced Attention Mechanism . 
Table 5 reports the experimental results . 
We Ô¨Årst remove all knowledge components from our model , denoted as without WordNet knowledge , which can be regarded as the baseline model . 
In the baseline model , we only use the original word embeddings and the conventional Luong attention mechanism . 
Then we evaluate the WordNetenhanced word embedding by adding the hypernym , synset , and the combination of both to the word embeddings , shown in ( 1)-(3 ) of Table 5 . 
To evaluate the WordNet - enhanced attention mechanism , we also add the synset relation score , the hypernym score or its combination to the original hidden vectors ‚Äô score based on the baseline model , shown in ( 4)-(6 ) of Table 5 . 
Compared with the baseline model , the WordNet knowledge brings consistent performance gain both for the WorNet - enhanced word embedding and WordNet - enhanced attention mechanism . 
As for the Knowledge - enhanced word embedding , the hypernym and synset improve 0.48 % and 0.30 % in MAP , respectively , and the combination of them improves 1.45 % in MAP . 
As for the Knowledgeenhanced attention mechanism , the hypernym and synset improve 5.34 % and 5.12 % in MAP respectively , and the combination of them improves 5.62 % in MAP . 
At the result , our full proposed model WEHM yields a signiÔ¨Åcant performance gain of 6.84 MAP points . 
We could Ô¨Ånd that the knowledge - enhanced attention mechanism is more effective than the simple knowledge - enriched word embedding , perhaps because computing the similarity scores of two concepts takes into account much information , like the shortest path between them and the depth of the concept in the taxonomy . 
Moreover , the combina-111 ( a)Mvector   ( b)Mwordnet Figure 3 : Attention score matrixes Mvector andMwordnet of a real case on the WikiQA dataset . 
The matrix Mwordnet not only captures the paraphrase information like ‚Äù food ‚Äù and ‚Äô cuisine ‚Äô , but also enhances relations between the question ‚Äôs word ‚Äú food ‚Äù and some of the sentence ‚Äôs words , like ‚Äú crops ‚Äù , ‚Äú cereals ‚Äù , ‚Äú wheat ‚Äù and ‚Äú rice ‚Äù . 
tion of hypernym and synset is better than the single hypernym or synset information in both knowledge components because it captures more diverse information . 
Interestingly , the hypernym information is more effective than the synset information in the question - answering task . 
3.3.3 Case Study To make a detailed analysis of the effectiveness of our proposed model , we give a case study to visualize the different attention score matrix Mvector andMwordnet , by a heatmap in Figure 3 . 
Mvector is only computed by hidden vectors , and Mwordnet is calculated by our proposed model . 
When answering the question , our proposed model not only captures the information of ‚Äù food ‚Äù and ‚Äù afghan ‚Äù , but also pays more attention to the related meaning of ‚Äù wheat - food ‚Äù , ‚Äù rice - food ‚Äù and so on , which brings vital information to the prediction , while the baseline method performs weakly on capturing this information . 
3.3.4 Error Analysis We further make an error analysis of our model for further improvements . 
Table 6 is a wrong prediction produced by our proposed model ( WEHM ) . 
‚Äù Cardiovascular disease ‚Äù is another name for ‚Äù heart disease ‚Äù . 
However , ‚Äù Cardiovascular disease ‚Äù is n‚Äôt mentioned in the given question . 
Although we have enriched the model with WordNet knowledge , it is still hard for the model to capture the lexical gap between these two words , for that their concepts are not the same in WordNet . 
From this analysis , we ‚Äôd like to employ more Ô¨Åne - grained knowledge , like the clariÔ¨Åcation for proper nouns . 
3.3.5 Comparison with other knowledge - enhanced models To the best of our knowledge , we are the Ô¨Årst to explore the WordNet knowledge to enhance theQuestion : what causes heart disease ? Document : [ 1]Cardiovascular disease ( also called heart disease ) is a class of diseases that involve the heart or blood vessels ( arteries , capillaries , and veins ) . 
[ 2 ] ...... [ 3]The causes of cardiovascular disease are diverse but atherosclerosis and hypertension are the most common . 
[ 4 ] ...... Reference Answer : The causes of cardiovascular disease are diverse but atherosclerosis and hypertension are the most common . 
Table 6 : The error prediction of our proposed model . 
The text is shown in its original form , which may contain errors in typing . 
Our proposed model predict the Ô¨Årst sentence is the right answer , however it is wrong . 
neural network model for the DQA problem . 
There are also some other knowledge - enhanced models designed for speciÔ¨Åc tasks , in which the natural language inference ( NLI ) task is somewhat similar to the QA task . 
In order to compare with our proposed WEHN model , we re - run the KEM model on the WikiQA dataset by using its public codes , which is designed for NLI task by Chen et al . 
( 2018 ) . 
ESIM ( Chen et al . 
, 2017 ) is the basic model of KEM without knowledge . 
KEM uses feature vectors of speciÔ¨Åc dimensions in WordNet , while our WEHM model directly employs synset and hypernym relation scores to enrich the attention score and also use their concepts to enrich the word representation . 
Table 7 shows the results of the WikiQA dataset . 
We could see that our proposed model outperforms the KEM model by a large margin . 
Besides , when comparing the improvements produced by the enriched knowledge , our proposed model is still better than KEM , with nearly 4 % gain versus about 3 % gain in MAP.112 Model MAP MRR ESIM ( Lan and Xu , 2018 ) 65.20 66.40 KEM ( Chen et al . 
, 2018 ) 68.03 69.58 WEHM ( without knowledge ) 73.17 74.63 WEHM ( Proposed ) 77.02 78.82 Table 7 : Experimental results on the WikiQA dataset . 
We list the reported results of ESIM in the paper ( Lan and Xu , 2018 ) , and re - run the public code of KEM proposed in the paper ( Chen et al . 
, 2018 ) to produce its results . 
4 Related Work In the NLP Ô¨Åeld , many problems involve matching two or more sequences to make a decision . 
For the DQA task , most of the studies also consider this problem as text matching , and they compute the semantic similarity between the question and candidate answers to decide whether a sentence in the document could answer the question . 
There have been various deep neural network models proposed to tackle sentence pairs matching . 
Two kinds of matching strategies have been considered : the Ô¨Årst is to convert the whole source and target sentences into embedding vectors in the latent spaces respectively , and then calculate the similarity score between them ; the second is to calculate the similarities among all possible local positions of the source and target sentences and then summarize the local scores into the Ô¨Ånal similarity value . 
As for works using the Ô¨Årst strategy , Qiu and Huang ( 2015 ) apply a tensor transformation layer on CNN - based embeddings to capture the interactions between the question and answer . 
Tan et al . 
( 2015 ) employ the long short - term memory ( LSTM ) network to address this problem . 
In the second strategy , Pang et al . 
( 2016 ) build hierarchical convolution layers on the word similarity matrix between sentences , and Yin and Sch ¬®utze ( 2015 ) propose MultiGranCNN to integrate multiple granularity levels of matching models . 
For the DQA task , the notable work is the compare - aggregate structure , which is Ô¨Årst proposed by Wang and Jiang ( 2016 ) . 
Following this structure , Bian et al . 
( 2017 ) propose the dynamicclip way to compute the attention score . 
Our basic model also adopts this structure , but with a different implementation . 
What ‚Äôs more , we employ a hierarchical module to capture inter - sentence relations . 
Exploiting the background knowledge and common sense to improve NLP tasks ‚Äô performancehas long been a heated research topic . 
To facilitate NLP tasks , various semantic knowledge bases ( KBs ) have been constructed , ranging from manually annotated semantic networks like WordNet ( Fellbaum , 1998 ) to semi - automatically or automatically constructed knowledge graphs like Freebase ( Bollacker et al . 
, 2008 ) . 
Recently , several approaches have been proposed to leverage the prior knowledge in neural networks on different tasks ( Yang and Mitchell , 2017 ; Chen et al . 
, 2018 ; Wu et al . 
, 2018 ; Wang et al . 
, 2019 ) . 
Wu et al . 
( 2018 ) fuse the prior knowledge into word representations with a knowledge gate by using question categories for the QA task and topics for the conversation task . 
Yang and Mitchell ( 2017 ) propose a KBLSTM network architecture , which incorporates the background knowledge into LSTM to improve machine reading . 
Unlike the two approaches , our model directly employs the synset and hypernym concepts information to enrich the word representation . 
Chen et al . 
( 2018 ) use WordNet to measure the semantic relatedness of word pairs for the natural language inference task , including synonym , antonym , hypernym , and same hypernym . 
Each of these features is denoted as a real number and is incorporated into the neural networks . 
Compared to the feature vectors derived from the WordNet , our model directly employ the synset and hypernym relation scores to enrich the attention mechanism . 
Wang et al . 
( 2019 ) present an entailment model for solving the Natural Language Inference ( NLI ) problem that utilizes ConceptNet as an external knowledge source , while our method mainly focus on the WordNet . 
5 Conclusion In this paper , we exploit a WordNet - enhanced hierarchical model to address the answer selection problem . 
Based on WordNet ‚Äôs prior knowledge , the proposed model applies the synset and hypernym concepts to enrich word representations and uses synset and hypernym relation scores between two concepts to enhance the traditional attention score . 
Extensive experiments conducted on two benchmark datasets demonstrate that our method signiÔ¨Åcantly improves the baseline model and outperforms state - of - the - art results by a large margin . 
Our approach obtains 1.62 % improvement and 2.57 % improvement in MAP on the WikiQA and SelQA datasets , respectively , compared to the stateof - the - art results . 
In the future , we would like to113 explore more knowledge in the neural networks to deal with different NLP tasks . 
Acknowledgments This work is supported by the National Natural Science Foundation of China ( 61773026 ) and the Key Project of Natural Science Foundation of China ( 61936012 ) . 
Abstract In this paper , we propose a simple method to predict salient locations from news article text using a knowledge base ( KB ) . 
The proposed method uses a dictionary of locations created from the KB to identify occurrences of locations in the text and uses the hierarchical information between entities in the KB for assigning appropriate saliency scores to regions . 
It allows prediction at arbitrary region units and has only a few hyperparameters that need to be tuned . 
We show using manually annotated news articles that the proposed method improves the f - measure by > 0.12compared to multiple baselines . 
1 Introduction Predicting relevant locations from news articles can result in numerous useful applications . 
For example , it enables the delivery of news related to a speciÔ¨Åc city that is of user interest , or facilitates the prediction of a disease outbreak in a speciÔ¨Åc region when used with event detection techniques . 
In this paper , we focus on predicting relevant locations from news articles . 
The goal of this task is to identify locations that are salient to the article , not those that simply appeared in the article . 
For example , consider the following excerpt : ‚Äú The Aoi Festival is one of the three major festivals in Kyoto . 
It originated as a series of rites to calm down angry gods . 
A visitor from Australia said ... ‚Äù In this example , the phrase ‚Äú Kyoto ‚Äù is highly relevant to the article , but ‚Äú Australia ‚Äù is not . 
Traditional methods to predict locations require speciÔ¨Åc data , such as a training dataset or phrase distribution , that match the application domain and the granularity of the prediction . 
However , it is costly to prepare such data for individual applications . 
We propose a knowledge base ( KB)-based method that only requires a general - purpose KBinstead of a labeled dataset for training . 
It propagates phrase - level importance to region entities following their relationship in the KB . 
It can theoretically be applied to predictions at an arbitrary level of granularity ( e.g. countries , prefectures , cities ) without dedicated training data . 
In this study , we focus on Japanese news articles and report the performance of predictions at the Japanese prefecture - level . 
We provide practical tips to tackle un - tokenized language like Japanese . 
2 Related Works Depending on the objective , geolocation prediction tasks from texts are roughly divided into two types . 
One type is for detecting and identifying mentions of points - of - interest ( POIs ) in the text , well known by entity linking ( Nadeau and Sekine , 2007 ; Shen et al . 
, 2015 ) . 
This task focuses on extracting all mentions regardless of their saliency . 
The other type is for estimating the author ‚Äôs current location or home town from his or her posts ( Huang and Carley , 2019 ) . 
It is mainly performed to complement user proÔ¨Åles in services that deal with user - generated content ( e.g. SNS ) . 
In this case , in addition to text , various user metadata such as the relationship between authors is available ( Backstrom et al . 
, 2010 ) . 
Our work is similar to the former type in terms of purpose , but we focus on identifying salient regions rather than extracting all of the individual POIs . 
There are two main approaches to predicting location . 
One is the dictionary - based approach ( Berggren et al . 
, 2015 ; Han et al . 
, 2012 ; Li et al . 
, 2014 ) , where dictionaries of location indicative words are created in advance and used for the prediction . 
In addition to explicit region names , the choice of which words to add to the dictionary is a hot topic of discussion ( Han et al . 
, 2014 ) . 
The other is the machine learning(ML)-based ap-116 proach ( Zhou and Luo , 2012 ; Miyazaki et al . 
, 2018 ) . 
Methods based on this approach usually perform well if sufÔ¨Åcient training data is available . 
However , in practice , it is difÔ¨Åcult to prepare data whose granularity matches the requirements of the application . 
In particular , estimating regions that are rarely found in the training data is one of the weaknesses of machine learning . 
3 Task Setting and Baseline 3.1 Task LetAbe the set of articles and Rbe the set of candidate regions , e.g. , Japanese prefectures . 
Our goal is to construct a function F : A‚ÜíP(R ) such thatr‚àà F(a)if and only if there are mentions of region rin the article aand regionris salient to the text , where P(R)denotes the power set ofR. Note that even when there are mentions ofrina , ifris not a main topic in a , F(a)does not containr . 
Similarly , when ais not a locationaware article , then F(a)is‚àÖ. In this paper , we assumeAto be a set of Japanese news articles and Rto be a set of Japanese prefectures . 
3.2 Gazetteer baseline The baseline method we adopted is similar to the baseline used in ( Berggren et al . 
, 2015 ) and consists of the following three steps : 1 ) Create a gazetteer from an external data source . 
2 ) Identify the strings contained in the gazetteer from the given text . 
3 ) Aggregate the results and return the relevant locations . 
In practice , there are several choices regarding step 3 ) . 
For example , we could return all the regions mentioned in the text , return the region mentioned most frequently in the text , or return only the region that appears earliest in the text . 
We decided to return locations that appear in the Ô¨Årst 20 % of the text . 
4 KB - based Methods 4.1 Knowledge base A KB consists of information about entities expressed in a structured , machine - readable graph format . 
YAGO and DBpedia are examples of KBs ( Hoffart et al . 
, 2011 ; Lehmann et al . 
, 2015 ) . 
Entities are assigned class(es ) that represent what kind of entity they are . 
Examples of possible entity classes include person , place , and company . 
Mount Fuji is an example of a place entity . 
A KB can be regarded as an edge - labeled directed graph . 
Entities correspond to nodes in the graph , and the relationships between entities correspond to edges in the graph . 
These relationships are given relation - type labels called predicates . 
We focus on the subgraph of KB that is useful in terms of location prediction . 
Let us reduce the graph by keeping only Place class entities , and vertexes connected to such entities with inclusive relations such as containedBy predicates and notation relations such as name , alsoKnownAs predicates , and name the resulting graph G= ( V , E ) . 
The notation relations in the graph will be used in ¬ß 4.3to create a gazetteer , while the inclusive relations will be used in ¬ß 4.5and¬ß4.6to determine the set of corresponding entities for each mention and calculate the corresponding score for different candidate regions , respectively . 
4.2 Overview of the proposed method The overview of our proposed method is shown in Fig . 
1 . 
It consists of four steps , three of which correspond to those in ¬ß 3.2and the rest is the entity linking step performed between 2 ) and 3 ) . 
As shown in the following sections , we add the efÔ¨Åcient use of KB information at each step . 
4.3 Create gazetteer Create a dictionary Dwith location names as keys and corresponding entities as values using the notation relations in the KB . 
Note that multiple entities may have the same name , so D(m)is a set of entities that belong to Vfor each key m. However , in practice , if we use all of the notation relations for D , it may adversely affect the prediction . 
For example , ‚Äú the park ‚Äù can be an alias for all parks in the world , but due to some inconsistencies in KB entries , some parks have such an alias in the KB and others do not . 
Therefore , by using inclusive relations between entities in the KB , we systematically extract phrases that appear as the preÔ¨Åx / postÔ¨Åx of entity names in wildly distant multiple regions and create a blacklist of phrases by manually reviewing them . 
In addition , we manually added the names of central ministries to the blacklist , since occurrences of such names rarely indicate the locality of the news . 
The blacklist currently consists of 151 words . 
4.4 Phrase identiÔ¨Åcation When given an article a , identify phrases that serve as clues by the following three steps:117 l‡¢Åﬂ¥‡¨úz l‹∞“™‡≠îzl‡Øï‡ª∫›ùz 1.0 l‡¥®‡¢≥Õ∑‡≠åz # region name , entity i d ‡±¶⁄ò‡±é  ‹∞“™‡≠î  ‡¢Åﬂ¥‡¨ú     ú‡¢Åﬂ¥‡¨úÕ∞Õ∏  ‡Øï‡ª∫›ùÕ∑‹∞“™ ‡≠î‡ØäÕ∑ Æ‡¥®‡¢≥ Õ∑‡≠å ØÕ∞Õ∏…∫ ‡¢Åﬂ¥‡¨úÕ∞Õ∏  ‡Øï‡ª∫›ùÕ∑‹∞“™ ‡≠î‡ØäÕ∑ Æ‡¥®‡¢≥ Õ∑‡≠å ØÕ∞Õ∏…∫ Shiraito Falls ID:617 Shiraito Falls ID:660Takayama VLG ID:134Takayama VLG ID:312 Karuizawa Town ID:811 Fujinomiya City ID:950Gunma Pref . 
ID : 200 Nagano Pref . 
ID:300 Shizuoka Pref . 
ID:400Extract only hierarchical relations   between entities 0.9 0.5 0.30.30.9 0.82.7 KB 1 . 
Create   gazetteer 2 . 
Phrase identification3 . 
Entity linking ( in red ) 4 . 
Scoring &   propagation Figure 1 : Overview of the proposed method . 
1 . 
Tokenize text of the article into morphemes . 
2 . 
Chunk the list of morphemes so that it results in as long and as many matches for keys in D. 3 . 
Perform named entity recognition ( NER ) and only keep phrases that at least partially overlap with named entities whose IREX1category is LOCATION , ARTIFACT , orORGANIZATION . 
The additional NER step is necessary to avoid confusion between the names of persons and places . 
There are many family names in Japanese that have similar characters to region names . 
The reason we do not limit the phrases to those categorized as LOCATION is that some ORGANIZATION orARTIFACT entities may contain region names in their names , e.g. , small local businesses . 
The sets of phrases identiÔ¨Åed from article aiin this step are represented by { mi}hereafter . 
4.5 Entity linking Entity linking is the task of mapping entity mentions to the corresponding entities in a KB . 
The purpose of this step is to reduce the candidate entities formifromD(mi)using the contexts of articlea . 
Da(mi)denotes the candidates remaining after the following steps . 
1 . 
If|D(mi)|= 1 , we have nothing to do . 
2 . 
IfD(mi)containses.t.‚àÉmjina , D(mj ) = { e } , we remove other candidates for mi . 
3 . 
IfD(mi)doesn‚Äôt satisfy the above but containses.t.‚àÉmjina , D(mj ) = { e‚Ä≤},eande‚Ä≤ 1https://nlp.cs.nyu.edu/irex/NE/df990214.txtare both contained by the same region r‚ààR , we remove other candidates for mi . 
In short , we give preference to entities when there is relevant evidence elsewhere in the article . 
Unlike in traditional entity linking tasks , for our purposes , if the procedure fails to resolve the phrase to a single entity , but Ô¨Ånds a list of candidates , it is still quite useful in terms of location prediction . 
As discussed later in the paper , such phrases and corresponding candidate entities will be taken properly into account in the later steps . 
4.6 Scoring and propagation In this step , we score each phrase occurrence mi and propagate the score to corresponding entities . 
First , we deÔ¨Åne phrase scores œÜaas : œÜa(mi ) = length(mi ) log(pos(mi ) + C ) , where pos(mi)means the number of words that precedemiin the article and Cis a positive constant . 
Next , we calculate entity scores œàaas : œàa(ei ) = ‚àë ( ej , ei)‚ààEœàa(ej ) |{(ej,¬∑)‚ààE}|+‚àë ei‚ààDa(mj)œÜa(mj ) |Da(mj)| , whereEhas a DAG structure because it is composed of inclusive relations and the calculation order is naturally determined . 
Finally , return regions that satisfy certain criteria as F(a ) . 
There are several possibilities for the actual criteria to determine which regions to return , such as F(a ) = { r‚ààR|œàa(r)>T}(absolute ) , F(a ) = { r‚ààR|œàa(r ) max r‚Ä≤(œàa(r‚Ä≤))>Œ±}(relative ) , F(a ) = { r‚ààR|rank(œàa(r))‚â§N}(rank).118 # of articles 1,711 # of candidate prefs 47 # of salient prefs / article 1.29 articles with no salient prefs 28.4 % Table 1 : Statistics on dataset . 
After experiments , we decided to adopt the intersection of the above three criteria with parameters T= 0.5,Œ±= 0.7 , andN= 2 . 
5 Experiments 5.1 Knowledge base resource We implemented the method proposed in ¬ß 4using an in - house KB of Yahoo Japan Corporation ( Yamazaki et al . 
, 2019 ) and in - house morphological analysis / NER tools for the following experiments . 
In short , the KB consists of data integrated from various open data , data purchased from our suppliers , and information extracted from web crawling . 
When open data is available in multiple languages ( e.g. , Wikipedia ) , a Japanese data dump is used to construct the KB . 
For historical reasons , the entities that correspond to regions in the KB were little used , and there were problems regarding the quality of data in this domain . 
Therefore , we incorporated various ofÔ¨Åcial data sources containing lists of regions , regional codes , and zip codes into the KB , as the accuracy / completeness of regions and the inclusive relations between them play a crucial role in location prediction . 
5.2 Dataset Since there is no publicly available Japanese corpus of salient locations , we asked a team of professional annotators to label a total of 1,711 news articles with relevant prefectures . 
There are 47 prefectures in Japan . 
The details of this dataset are shown in Table 1 . 
The team consists of Ô¨Åve annotators independent of us . 
Although each article is labeled by one annotator , the annotation team created an annotation guideline in an iterative way as follows to ensure consistency of the annotation : First , create a temporary annotation guideline and annotate a relatively small number of articles . 
Then , share the annotated results and discuss whether an annotation guideline needs to be updated . 
This iteration was repeated until a reasonable annotation guideline is Ô¨Åxed . 
Note that the annotation guideline was Ô¨Ånalized before the development of the proposed method started . 
Although our method enables prediction with Ô¨Åner granularity , we evaluate only at the prefecture - level in this Ô¨Årst research due to the cost of annotation . 
5.3 Metrics We evaluate the prediction performance by microaveraged precision ( pm ) , micro - averaged recall ( rm ) , and article - averaged f - measure ( fA ) calculated as : pm=‚àë a‚ààA|Ra‚à© F(a)|‚àë a‚ààA|F(a)|,fA=1 |A|‚àë a‚ààA2para pa+ra , rm=‚àë a‚ààA|Ra‚à© F(a)|‚àë a‚ààA|Ra| , whereRais the set of salient prefectures for articleain the ground truth and pa=|Ra‚à© F(a)|/|F(a)|,ra=|Ra‚à© F(a)|/|Ra|are articlelevel precision and recall , respectively . 
We considerpa= 1.0ifF(a ) = ‚àÖ , andra= 1.0if Ra=‚àÖ. Hence , when ais not a location - aware article , the harmonic average of the two is 1.0 if and only if the method returns an empty set , and 0.0 otherwise . 
5.4 Baseline methods We adopted two different baseline methods to demonstrate the validity of the proposed method , the baseline method that relies on gazetteer described in 3.2and ML - based method . 
The second ML - based baseline method treats location prediction as a multi - label classiÔ¨Åcation problem ( i.e. , an article can have multiple subject regions ) . 
In this setting , the classiÔ¨Åer assigns different labels that correspond to Japanese prefectures to each article . 
We used fastText ( Joulin et al . 
, 2017 ) library for this task and tokenized the text for each article using the same in - house morphological tool described in 5.1 . 
5.5 Comparison with baselines The results for the proposed and baseline methods are listed in Table 2 . 
As shown , the proposed method outperformed the baseline methods in all performance metrics . 
Note that the evaluation metrics for fastText baseline are calculated in a slightly different way from other methods and are meant as approximate reference values . 
It was calculated by taking an average of models obtained119 methods pmrmfA gazetteer baseline 0.501 0.476 0.708 fastText baseline 0.660 * 0.420 * 0.430 * proposed 0.824 0.515 0.830 proposed + BL 0.856 0.515 0.852 Table 2 : Results of proposed and baseline methods . 
* The average over nested 4 - fold cross - validation . 
by nested 4 - fold cross - validation over the evaluation dataset . 
We tuned the hyperparameters to optimize article - averaged f - measure ( fA ) in each inner loop of cross - validation . 
For the proposed method and the gazetteer baseline that require no dataset for training , the evaluation metrics are calculated using the entire dataset . 
The gazetteer baseline suffers from low precision . 
We give the following example to demonstrate how the proposed method ‚Äôs output improved over that of the gazetteer baseline . 
Yokohama most often represented the well - known city in Kanagawa Pref . 
but on rare occasions represented the small town with a similar name in Aomori Pref . 
The gazetteer baseline is not able to prioritize between them and returns both locations . 
The proposed method considered the other entity mentions to resolve Yokohama into the correct region . 
The fastText baseline performs differently for different kinds of articles . 
While most of the articles in the evaluation dataset are labeled one or two prefectures , some articles contain phrases that collectively refer to multiple Japanese prefectures2 and are labeled a large number of prefectures . 
Since such phrases have only a limited number of variations and appear in the dataset repeatedly , it is relatively easy for the ML - based approach to learn such expressions . 
Therefore it performs relatively well in terms of micro - averaged metrics heavily weighted to articles with a high number of relevant prefectures . 
However , the article - averaged metric fAis incredibly low compared to other methods . 
This is because the knowledge of names of individual prefectures or cities is essential in order to make correct predictions for the rest of the articles . 
We found that fastText classiÔ¨Åer often fails to predict locations for such articles even when names of prefectures are explicitly stated in the article . 
We conclude that it is practically impossible to learn all the necessary region names from a few thou2Examples include ‚Äú T ¬Øohoku region ‚Äù that refers to 6 prefectures and ‚Äú Western Japan ‚Äù that refers to > 20 prefectures.0.50.60.70.80.9 0.3 0.5 0.7 0.9pm , r m , f A Œ±pmrmfA 0.50.60.70.80.9 0.3 0.5 0.7 TpmrmfA Figure 2 : Effect of varying Œ±(left ) andT(right ) . 
sand articles and that utilizing external resources such as the KB is the critical element in achieving good performance . 
As another example , there is ‚Äú Tokyo Disneyland ‚Äù that is actually located in Chiba Pref . 
, not in Tokyo Pref . 
It is crucial to treat it as an entity and not be confused by their apparent region names . 
When we added the blacklist created in ¬ß 4.3to the proposed method , there was a huge improvement in precision . 
This highlights the incompleteness of the aliases in the KB and indicates that care must be taken when applying entries in KB to a real service . 
5.6 Impact of hyperparameters The hyperparameters that govern the performance of our proposed method are T , Œ±andN(introduced in ¬ß 4.6 ) . 
We can see the effect of varying these hyperparameters in Fig . 
2 . 
These results demonstrate that the precision / recall tradeoff can be adjusted by varying hyperparameters . 
6 Conclusion In this paper , we presented a simple KB - based method to predict relevant locations from articles . 
The proposed method requires no training data or maintenance of a dictionary thanks to a freshly generated KB , and it can be used to make predictions at an arbitrary level of granularity , as long as the corresponding data is present in the KB . 
We demonstrated the effectiveness of this method at predicting salient Japanese prefectures using manually annotated articles . 
In future work , we plan to make location predictions at the city - level and evaluate its performance . 
Acknowledgments We thank our teammates for help in developing and deploying our location prediction system . 
We are grateful to the annotation team for providing us with an evaluation dataset . 
We wish to thank the anonymous referees for helpful comments.120 Abstract Most existing approaches for goal - oriented dialogue policy learning used reinforcement learning , which focuses on the target agent policy and simply treats the opposite agent policy as part of the environment . 
While in realworld scenarios , the behavior of an opposite agent often exhibits certain patterns or underlies hidden policies , which can be inferred and utilized by the target agent to facilitate its own decision making . 
This strategy is common in human mental simulation by Ô¨Årst imaging a speciÔ¨Åc action and the probable results before really acting it . 
We therefore propose an opposite behavior aware framework for policy learning in goal - oriented dialogues . 
We estimate the opposite agent ‚Äôs policy from its behavior and use this estimation to improve the target agent by regarding it as part of the target policy . 
We evaluate our model on both cooperative and competitive dialogue tasks , showing superior performance over state - of - the - art baselines . 
1 Introduction In goal - oriented dialogue systems , dialogue policy plays a crucial role by deciding the next action to take conditioned on the dialogue state . 
This problem is often formulated using reinforcement learning ( RL ) in which the user serves as the environment ( Levin et al . 
, 1997 ; Rieser and Lemon , 2011 ; Lemon and Pietquin , 2012 ; Young et al . 
, 2013 ; Fatemi et al . 
, 2016 ; Zhao and Eskenazi , 2016 ; Dhingra et al . 
, 2016 ; Su et al . 
, 2016 ; Li et al . 
, 2017 ; Williams et al . 
, 2017 ; Liu and Lane , 2017 ; Lipton et al . 
, 2018 ; Liu et al . 
, 2018 ; Gao et al . 
, 2019 ; Takanobu et al . 
, 2019 , 2020 ; Jhunjhunwala et al . 
, 2020 ) . 
However , different from symbolic - based and simulation - based RL tasks , such as chess ( Silver et al . 
, 2016 ) and video games ( Mnih et al . 
, ‚àóCorresponding author.2015 ) , which can get vast amounts of training interactions in low cost , dialogue systems require to learn directly from real users , which is too expensive . 
Therefore , there are some efforts using simulation methods to provide an affordable training environment . 
One principle direction for mitigating this problem is to leverage human conversation data to build a user simulator , and then to learn the dialogue policy by making simulated interactions with the simulator ( Schatzmann et al . 
, 2006 ; Li et al . 
, 2016 ; G ¬®ur et al . 
, 2018 ) . 
However , there always exist discrepancies between simulated users and real users due to the inductive biases of the simulation model , which can lead to a sub - optimal dialogue policy ( Dhingra et al . 
, 2016 ) . 
Another direction is to learn the dynamics of dialogue environment during interacting with real user , and concurrently use the learned dynamics for RL planning ( Peng et al . 
, 2018 ; Su et al . 
, 2018 ; Wu et al . 
, 2018 ; Zhang et al . 
, 2019b ) . 
Most of these works are based on Deep Dyna - Q ( DDQ ) framework ( Sutton , 1990 ) , where a world model is introduced to learn the dynamics ( which is much like a simulated user ) from real experiences . 
The target agent ‚Äôs policy is trained using both real experiences via direct RL and simulated experiences via a world - model . 
In the above methods , both the simulated user and world model facilitate target policy learning by providing more simulated experiences and remain a black box for the target agent . 
That is , the target agent ‚Äôs knowledge about the simulated agents is still passively obtained through interaction and implicitly learned by the policy model updating as indirect try - and - error with real user . 
However , we argue that from the angle of a target agent , actively exploring the world with proper estimation would not only make user simulation122 Policy ModelUserEstimatorUserHumanConversational Data RealExperienceSupervisedLearningImitationLearningDirectRLActingùíÇùíï#ùíÇùíï$ùüèùíê 	 ( HumanConversational DataSupervisedLearningImitationLearningPolicy ModelUserRealExperienceWorld ModelActingDirect RLWorld ModelLearningPlanningActingDirect RLUser   / SimulatorHumanConversational DataPolicy ModelDirect RLDDQOPPAFigure 1 : A comparison of dialogue policy learning a ) with real / simulated user , b ) with real user via DDQ and c ) with real user guided by active user estimation . 
more efÔ¨Åcient but also improve the target agent ‚Äôs performance . 
In agreement with the Ô¨Åndings from cognitive science , humans often maintain models of other people they interact with to capture their goals ( Harper , 2014 ; Premack and Woodruff , 1978 ) . 
And humans manage to use their mental process to simulate others ‚Äô behavior ( Gordon , 1986 ; Gallese and Goldman , 1998 ) . 
Therefore , to carefully treat and model the behaviors of other agents would be full of potential . 
For example , in competitive tasks such as chess , the player often sees a number of moves ahead by considering the possible reaction of the other player . 
In goal - oriented dialogues for a hotel booking task , the agent can reduce interaction numbers and improve user experience by modeling users as business travellers with strict time limit or backpackers seeking adventure . 
In this paper , we propose a new dialogue policy learning method with OPPosite agent Awareness ( OPPA ) , where the agent maintains explicit modeling of the opposite agent or user for facilitating its own policy learning . 
Different from DDQ , the estimated user model is not utilized as a simulator to produce simulated experiences , but as an auxiliary component of the target agent ‚Äôs policy to guide the next action . 
Figure 1(c ) shows the framework of our model . 
SpeciÔ¨Åcally , whenever the system needs to output an action , it foresees a candidate action ÀÜatand consequently estimates the user ‚Äôs response behaviorao t+1 / prime . 
On top of this estimation , as well as the dialogue context , it makes better decisions with a dynamic estimation of the user ‚Äôs strategy . 
To further regulate the behavior of the system agent , we mitigate the difference between the real system actionatand the sampled action ÀÜatwith decay for better robustness and consistency . 
Without any constraint on the type of agents ( either competitive or cooperative ) , the proposed OPPA method can be applied to both cooperative and non - cooperative goal - oriented dialogues . 
To summarize , our contributions are three - fold : ‚Ä¢We propose a new dialogue policy learning setting where the agent shifts from passively learning to actively estimating the opposite agent or user for more efÔ¨Åcient simulations , thereby obtaining better performance . 
‚Ä¢We mitigate the difference between real system agent action and the sampled action with decay to further enhance estimated system agent behaviors . 
‚Ä¢Extensive experiments on both cooperative and competitive goal - oriented dialogues indicate that the proposed model can achieve better dialogue policies than baselines . 
2 Related Work 2.1 RL - based Dialogue Policy Learning Policy learning plays a central role in building goaloriented dialogue systems by deciding the next action , which is often formulated using the RL framework . 
Early methods used probabilistic graph model , such as partially observable Markov decision process ( POMDP ) , to learn dialogue policy by modeling the conditional dependences between observation , belief states and actions ( Williams and Young , 2007 ) . 
However , these methods require manual work to deÔ¨Åne features and state representation , which leads to poor domain adaptation . 
More recently , deep learning methods are applied in dialogue policy learning , including DQN ( Mnih et al . 
, 2015 ) and Policy Gradient ( Sutton et al . 
, 2000 ) methods , which mitigate the problem of domain adaptation through function approximation and representation learning ( Zhao and Eskenazi , 2016 ) . 
Recently , there are some efforts focused on multi - domain dialogue policy . 
An intuitive way is to learn independent policies for each speciÔ¨Åc domain and aggregate them ( Wang et al . 
, 2014 ; Ga Àási¬¥c123 et al . 
, 2015 ; Cuay ¬¥ ahuitl et al . 
, 2016 ) . 
There are also some works using hierarchical RL , which decomposes the complex task into several sub - tasks ( Peng et al . 
, 2017 ; Casanueva et al . 
, 2018 ) according to pre - deÔ¨Åned domain structure and cross - domain constraints . 
Nevertheless , most of the above works regard the opposite agent as part of the environment without explicitly modeling its behavior . 
Planning based RL methods are also introduced to make a trade - off between reducing human interaction cost and learning a more realistic simulator . 
Peng et al . 
( 2018 ) proposed to use Deep Dynamic Q - network , in which a world model is co - trained with the target policy model . 
By training the world model with the real system - human interaction data , it consistently approaches the performance of real users , which provides better simulated experience for planning . 
Adversarial methods are applied to dynamically control the proportion of simulated and real experience during different stages of training ( Su et al . 
, 2018 ; Wu et al . 
, 2018 ) . 
Still , these methods work from the opposite agents ‚Äô angle . 
2.2 Dialogue User Simulation In RL - based dialogue policy learning methods , a user simulator is often required to provide affordable training environments due to the high cost of collecting real human corpus . 
Agenda - based simulation ( Schatzmann et al . 
, 2007 ; Li et al . 
, 2016 ) is a widely applied rule - based method , which starts with a randomly generated user goal that is unknown to the system . 
During a dialogue session , it remains a stack data structure known as user agenda , which holds some pending user intentions to achieve . 
In the stack update process , machine learning or expert - deÔ¨Åned methods can be applied . 
There are also some model - based methods that learn a simulator from real conversation data . 
The seq2seq framework has recently been introduced by encoding dialogue history and generates the next response or dialogue action ( Asri et al . 
, 2016 ; Kreyssig et al . 
, 2018 ) . 
By incorporating a variational step to the seq2seq network , it can introduce meaningful diversity into the simulator ( G ¬®ur et al . 
, 2018 ) . 
Our work tackles the problem from a different point of view . 
We let the target agent approximate an opposite agent model to save user simulation efforts . 
3 Model In this section , we introduce our proposed OPPA model . 
There are two agents in our framework , oneis the system agent we want to optimize , and the other is the user agent . 
We refer to these two agents astarget andopposite agents in the following sections . 
Note that the proposed model works at dialog act level , and it can also work at natural language level when equipped with natural language understanding ( NLU ) and natural language generation ( NLG ) modules . 
3.1 Overview As shown in Figure 2 , the proposed model consists of two key components : a target agent QfunctionQ(s , a)and an opposite agent policy estimatorœÄo(s , a ) . 
SpeciÔ¨Åcally , each time before the target agent needs to take an action , the model samples a candidate action ÀÜat . 
Then the opposite estimatorœÄoestimates the opposite agent ‚Äôs response behaviorao t+1 / prime , which is then aggregated with the original dialog state stto generate a new state ÀÜst . 
On top of this new state , the target policy Q(s , a ) gets the next target action . 
In more detail , a brief script of our proposed OPPA model is shown in Algorithm 1 . 
3.2 Opposite Action Estimation One essential target of the opposite estimator is to measure how the opposite agent reacts given its preceding target agent action and state . 
In OPPA , we implement the opposite estimation model using a two - layer feed - forward neural network followed by a softmax layer . 
It takes as input the current statest , a sampled target action ÀÜat , and predicts an opposite action ao t+1 / primeas below : ao t+1 / prime = œÄo(st,ÀÜat ) . 
( 1 ) Note that we regard the opposite action estimation task as a classiÔ¨Åcation problem , and ao t+1 / primeis an action label . 
It has been shown effective in other studies like Su et al . 
( 2018 ) . 
We also carried out preliminary experiments on other more complicated designs such as Weber et al . 
( 2017 ) . 
However , results have shown MLP ‚Äôs superior performance in our dialogue policy learning task . 
3.3 Opposite Aware Q - Learning After obtaining the estimated opposite reaction ao t+1 / prime , it serves as an extra input to the DQN - based policy component besides the original dialogue state representation st . 
Therefore , we form a new state representation ÀÜstas below : ÀÜst= [ st , Eoao t+1 / prime ] , ( 2)124 Natural LanguageUnderstanding Natural LanguageGenerationDialogue StateTrackingDialogue Stateùë†"Sample Target Action ùëé"$Opposite Estimatorùúã&(ùë†"$,ùëé"$)DQN Policy a+=argmùëéùë•1ùëÑ(ùë†"$,ùëé)Estimated Opposite Action ùëé"34 & 	 6Target Action ùëé"UserDialogue Stateùë† " ùë•47ùë•87ùë•9:7Dialogue ContextsFF Layer + Attention‚Ñé7ùëõoutputs ‚Ä¶ ùëú4ùëú9Reward Function HumanRewardorFigure 2 : The proposed OPPA model and the reward function . 
Note that the reward for policy model can be either from real user or the reward function depending on whether real reward is available . 
in whichEoao t+1 / primeintroduces the knowledge of opposite agent into our policy learning . 
Eois the opposite action embedding matrix which maps the action into speciÔ¨Åc vector representation . 
Given the outputatofargmaxa / primeQ ( ÀÜst , a / prime ) , the agent chooses an action to execute using an /epsilon1 - greedy policy that selects a random action with probability /epsilon1or otherwise follows the output at . 
We update the Qfunction by minimizing the mean - squared loss function , which is deÔ¨Åned as L1(Œ∏ ) = E(s , a , r , s / prime)‚àºDL[(yi‚àíQ(s , a))2],(3 ) yi = r+Œ≥max a / primeQ / prime(s / prime , a / prime ) , ( 4 ) whereŒ≥‚àà[0,1]is a discount factor , DLis the replay buffer and yirepresents the expected reward computed based on the transition . 
3.4 Target Action Sampling In this subsection , we explain how the action ÀÜatis sampled utilizing the above modules . 
For generating the true target action at , we predict it using a deep Q - network which takes as input an estimated opposite action ao t+1 / primeand the dialogue state he t. However , we can not get ao t+1 / primewithout ÀÜat . 
Therefore , we further leverage this Q - network at hand . 
Specifcally , we feed an constant opposite action placeholder aoto the Q - function : ÀÜat= argmax a / primeQ([he t , Eoao],a / prime ) ( 5 ) whereaoserves as a constant opposite action . 
In our experiment , aocorresponds to the general actions which do not inÔ¨Çuence business logic , such asHello andThanks . 
3.5 Action Regularization with Decay In our method , ÀÜatis sampled from a distribution . 
At the very beginning of training , since the model isnot well trained , the sampled ÀÜatmay perform badly , which would lead to slow convergence . 
Therefore , we apply action regularization to mitigate the difference between ÀÜatand realat . 
As the training progress goes on , such guidance becomes less effective , and we hope to encourage the model to explore more in the action space . 
Therefore , we adopt a decay mechanism inspired by ( Zhang et al . 
, 2019a ) . 
The regularization term is deÔ¨Åned as the cross entropy of ÀÜatand real action : L2(Œ∏ ) = ‚àíŒ≤ / summationdisplay tatlog ( ÀÜat ) , ( 6 ) whereŒ≤is the decay coefÔ¨Åcient . 
The value of Œ≤ decreases along with time by applying a discount factorŒ≥in each epoch . 
As a consequence , a strict constraint on the sampled action is applied to avoid large action sampling performance drop at the beginning stage . 
After that , the constraint is continuously relaxed so that the model can explore more actions for better strategy . 
To sum up , the Ô¨Ånal loss function for training our OPPA model is the weighted sum of the DQN loss and action regularization loss : L(Œ∏ ) = L1(Œ∏ ) + ŒªL2(Œ∏ ) . 
( 7 ) whereŒªis an adjustable hyperparameter . 
3.6 Reward Function When a dialogue session is completed , what we get are several dialogue acts or natural language utterances ( when paired with NLU and NLG ) . 
For most goal - oriented dialogues , the reward signal can be obtained from the user simulator or real user ratings . 
However , when that reward is not available , an output prediction model is required which takes as input the whole dialogue session125 Algorithm 1 OPPA for Dialogue Policy Learning Require:/epsilon1,C 1 : initializeœÄo(s , a;Œ∏œÄ)andQ(s , a;Œ∏Q)by supervised and imitation learning 2 : initializeQ / prime(s , a , Œ∏Q / prime)withŒ∏Q / prime = Œ∏Q 3 : initialize replay buffer D 4 : for each iteration do 5 : user actsau 6 : initialize state s 7 : while notdone do 8 : e = random ( 0,1 ) 9 : ife</epsilon1 then 10 : select a random action a 11 : else 12 : sample ÀÜat 13 : est . 
user action ao t+1 / prime = œÄo(s,ÀÜat ) 14 : ÀÜs= [ s , Eoao t+1 / prime ] 15 : a = argmaxa / primeQ(ÀÜs , a / prime;Œ∏Q ) 16 : end if 17 : executea 18 : get user response aoand rewardr 19 : state updated to s / prime 20 : store ( s , a , r , s / prime)toD 21 : end while 22 : sample minibatches of ( s , a , r , s / prime)fromD 23 : updateŒ∏Qaccording to Equation 4 24 : each everyCiterations set Œ∏Q / prime = Œ∏Q 25 : end for X={xs 1,xs 2, ... ,xs n}whereXis a sequence of tokens , and outputs structured result to calculate the reward . 
We use a bi - directional GRU model with an attention mechanism to learn a summarization hsof the whole session : ho j= BiGRU(ho j‚àí1,[Exs j , hj ] ) , ( 8) ha j = Wa[tanh(Whho j ) ] , ( 9 ) Œ±j = exp(w¬∑ha j)/summationtext t / primeexp(w¬∑ha j / prime ) , ( 10 ) hs= tanh(Ws[hg,/summationdisplay jŒ±jhj ] ) . 
( 11 ) Note that in this process , we concatenated all the utterances by time order , and the subscript j indicates the index of word in the concatenated sequence . 
In addition , there may be multiple aspects of the output . 
For example , in a negotiation goaloriented dialogue with multiple issues ( we denote the book or hat items to negotiate on as issues ) , weneed to get the output of each issue to calculate the total reward . 
Therefore , for each issue oi , a speciÔ¨Åc softmax classiÔ¨Åer is applied : pŒ∏(oi|x0 ... T , g ) = softmax ( Woihs ) . 
( 12 ) After the structured output is predicted , we can obtain the Ô¨Ånal reward by applying the task - speciÔ¨Åc reward function on the output . 
r = fR(o1,o2, ... ,oNo ) , ( 13 ) whereNois the number of output aspects and fR is the reward function which is often manually deÔ¨Åned according to the task . 
4 Experiment Depending on the task , dialogues can be divided into cooperative and competitive ones . 
In a cooperative task , the aim can be reducing unnecessary interactions by inferring the opposite person ‚Äôs intention . 
While in competitive tasks , the aim is usually to maximize their own interests by considering the opposite agents ‚Äô possible reactions . 
To test our method ‚Äôs wide suitability , we evaluated it on both cooperative and competitive tasks . 
4.1 Dataset For the cooperative task , we used MultiWOZ ( Budzianowski et al . 
, 2018 ) , a large - scale linguistically rich multi - domain goal - oriented dialogue dataset , which contains 7 domains , 13 intents and 25 slot types . 
There are 10,483 sessions and 71,544 turns , which is at least one order of magnitude larger than previous annotated task - oriented dialogue dataset . 
Among all the dialogue sessions , we used 1,000 each for validation and test . 
SpeciÔ¨Åcally , in the data collection stage , the user follows a speciÔ¨Åc goal to converse with the agent but is encouraged to change his / her goal dynamically during the session , which makes the dataset more challenging . 
For the competitive task , we used a bilateral negotiation dataset ( Lewis et al . 
, 2017 ) , where there are 5,808 dialogues from 2,236 scenarios . 
In each session , there are two people negotiating to divide some items , such as books , hats and balls . 
Each kind of item is of different value to each person , thus they can give priority to valuable items in the negotiation . 
For example , a hat may worth 5 for personAand3for personB , soBcan give up some hat in order to get other valuable items . 
To conduct our experiment , we further labeled the dataset with system dialogue actions.126 4.2 Experimental Settings We implemented the model using PyTorch ( Paszke et al . 
, 2017 ) . 
The hyper - parameters were decided using validation set . 
The dimension of GRUohidden state is 256 , and the hidden state size of GRUg andGRUware 64 and 128 respectively . 
The size ofhsis 256 . 
As for the Q - function , the size of st is 256./epsilon1 - greedy is applied for exploration . 
The buffer size of Dis set to 500 and the update step C is 1 . 
Note that due to the complexity of MultiWOZ , the error propagation problem caused by NLU and NLG is serious . 
Therefore , the cooperative experiment is conducted on the dialogue act level . 
In the experiment , our proposed model interacts with a robust rule - based user simulator , which appends an agenda - based model ( Schatzmann et al . 
, 2007 ) with extensive manual rules . 
The simulator gives user response , termination signal and goal - completion feedback during training . 
For the competitive task , the experiment is on natural language level . 
Following ( Lewis et al . 
, 2017 ) , we built a seq2seq language model for the NLU and NLG module , which is pre - trained on the negotiation corpus . 
Our proposed model was Ô¨Årst pre - trained with supervised learning ( SL ) . 
SpeciÔ¨Åcally , we pretrained the opposite estimator œÄoand the QfunctionQ(s , a)via supervised learning and imitation learning . 
We then Ô¨Åne - tuned the model using reinforcement learning ( RL ) . 
The reward of the MultiWOZ experiment consists of two parts : a ) a small negative value in each turn to encourage shorter sessions and b ) a large positive reward when the session ends successfully . 
Note that the task completion signal is obtained from the user . 
For the negotiation experiment , the reward is the total value of item items that the agent Ô¨Ånally got . 
In the negotiation dataset , the reward is given by the proposed output model described in the Reward Function section . 
4.3 Baselines To demonstrate the effectiveness of our proposed model , we compared it with the following baselines . 
For the MultiWOZ task , we compared with : ‚Ä¢DQN : The conventional DQN ( Mnih et al . 
, 2015 ) algorithm with a 2 - layer fully - connected network for Q - function . 
‚Ä¢REINFORCE : The REINFORCE algorithm ( Williams , 1992 ) with a 2 - layer fully - connected policy network.‚Ä¢PPO : Proximal Policy Optimization ( Schulman et al . 
, 2017 ) , a policy - based RL algorithm using a constant clipping mechanism . 
‚Ä¢DDQ : The Deep Dyna - Q ( Peng et al . 
, 2018 ) algorithm which introduced a world - model for RL planning . 
Note that the DQN can be seen as our proposed model without opposite estimator ( OPPA w/o OBE ) . 
For the negotiation task , we compared with : ‚Ä¢SL RNN : A supervised learning method that is based on an RNN language generation model . 
‚Ä¢RL RNN : The reinforcement learning extension of SL RNN by reÔ¨Åning the model parameters after SL pretraining . 
‚Ä¢ROL : SL RNN with goal - based decoding in which the model Ô¨Årst generates several candidate utterances and chooses the one with the highest expected overall reward after rolling out several sessions . 
‚Ä¢RL ROL : The extension of RL RNN with rollout decoding . 
‚Ä¢HTG : A hierarchical text generation model with planning ( Yarats and Lewis , 2018 ) , which learns explicit turn - level representation before generating a natural language response . 
Note that the rollout mechanism used in ROL and RL ROL also endows them with the ability of ‚Äú seeing ahead ‚Äù in which the candidate actions ‚Äô rewards are predicted using a random search algorithm , while our OPPA explicitly models the opposite ‚Äôs behavior . 
RL RNN , RL ROL and HTG used the REINFORCE ( Williams , 1992 ) algorithm for reinforcement learning on both strategy and language level , while in OPPA we used the DQN ( Mnih et al . 
, 2015 ) algorithm only on strategy level . 
To further examine the effectiveness of our proposed action regularization with decay , we did an ablation study by removing the regularization with decay part in Equation 6 ( OPPA w/o A ) . 
4.4 Evaluation Metric For the evaluation of experiments on MultiWOZ , we used the number of turns , inform F1 score , match rate and success rate . 
The Number of turns is the averaged number on all sessions , and less turns in cooperative goal - oriented task can promote user satisfaction . 
Inform F1 evaluates whether all the slots of an entity requested by the user have been successfully informed . 
We use F1 score because it considers both the precision and recall so127 that a policy which greedily informs all slot information of an entity wo n‚Äôt get a high score . 
Match rate evaluates whether the booked entities match the goals in all domains . 
The score of a domain is 1 only when its entity is successfully booked . 
Finally , a session is considered successful only if all the requested slots are informed ( recall = 1 ) and all entities are correctly booked . 
For the negotiation task , we used the averaged scores ( total values of items ) of all the sessions and those with an agreement as the primary evaluation metrics following Lewis et al . 
( 2017 ) . 
The percentage of agreed and Pareto optimal‚àósessions are also reported . 
Method # Turn Inform F1 Match Success DQN 10.50 78.23 60.31 51.7 REINFORCE 9.49 81.73 67.41 58.1 PPO 9.83 83.34 69.09 59.0 DDQ 9.31 81.49 63.10 62.7 OPPA w/o A 8.19 88.45 77.18 75.2 OPPA 8.47 91.68 79.62 81.6 Human 7.37 66.89 95.29 75.0 Table 1 : The results on MultiWoZ dataset , a large scale multi - domain task - oriented dialog dataset . 
We used a rule - based method for DST and Agenda - based user simulator . 
The DQN method can be regard as OPPA w/o OBE . 
Human - human performance from the test set serves as the upper bound . 
4.5 Cooperative Dialogue Analysis The results on MultiWOZ dataset are shown in Table 1 . 
OPPA shows superior performance on task success rate than other baseline methods due to the considerable improvement in Inform F1 and Match rate . 
By Ô¨Årst infer the next action of the opposite agent , the target agent policy can make better choices to match the reward signal during training . 
When compared with human performance , OPPA even achieves a higher success rate , although the number of turns is still higher . 
This might be due to the fact that the user is sensitive to the dialogue length . 
When a dialogue becomes intolerably long , many user will leave without completing the dialogue . 
By taking actions in account of the inferred opposite action , the target agent can also make the dialogue more efÔ¨Åciently by avoiding some lengthy interactions , which is extremely important in applications where the user is sensitive to dialogue length . 
Meanwhile , DDQ achieves higher task success rate than other baseline models since it also mod‚àóA dialogue is Pareto optimal if neither agent ‚Äôs score can be improved without lowering the other ‚Äôs score.els the behavior of opposite agent through world model . 
However , it makes use of the learned world model by providing more simulated experiences , which does not give a direct hint on how to act in the middle of a session . 
Therefore , in its experiments , it still gets longer dialogue sessions and a lower success rate than OPPA . 
If we remove the action regularization mechanism , we can see an obvious decline on performance , which is as expected . 
The action regularization is introduced to mitigate the difference between sampled ÀÜatand realat , so there can be a large discrepancy between the sampled and real actions if we remove it at the early training stage . 
When the ÀÜatis not reliable , the consequent estimated opposite action a / prime t+1also becomes noisy , which leads to performance drop . 
4.6 Competitive Dialogue Analysis Table 2 shows the scores for all sessions and for only agreed ones . 
When comparing with the seq2seq models , OPPA achieves signiÔ¨Åcantly better results . 
This can be attributed to the hierarchical structure of OPPA . 
The sequence models only take as input ( and outputs ) the word - level natural language utterances , without explicitly modeling turn - level dialogue actions . 
In this way , the parameters for linguistic and strategy functions are tangled together , and the back - propagation errors can inÔ¨Çuence both sides . 
As for the two ROL models , although they can predict the value of a candidate action in advance , they still can not beat OPPA . 
The reason is that the rollout method did not explicitly maintain an estimation of the opposite agent as our OPPA did . 
Instead , it just estimates the candidate acitons ‚Äô rewards based on Monte Carlo search by using its own model for predicting future movements . 
Therefore , when the opposite model ‚Äôs behavior is not very familiar to the target agent , the estimated reward becomes unreliable . 
The HTG model also used a hierarchical framework by learning an explicit turn - level latent representation . 
By doing this , it obtains higher scores than the seq2seq models . 
However , it does not make any assumptions about the opposite agent . 
Therefore , its scores are still lower than OPPA , although the discrepancy narrows down . 
By removing the opposite estimator , we Ô¨Ånd that the performance of OPPA w/o OBE drops signiÔ¨Åcantly compared to that of OPPA . 
This ablation study directly veriÔ¨Åes the effectiveness of our proposed opposite behavior estimator . 
There fore,128 Methodvs . 
SL RNN vs. RL RNN vs. ROL vs. RL ROL All Agreed All Agreed All Agreed All Agreed SL RNN 5.4 vs. 5.5 6.2 vs. 6.2 - - - - - RL RNN 7.1 vs. 4.2 7.9 vs. 4.7 5.5 vs. 5.6 5.9 vs. 5.8 - - - ROL 7.3 vs. 5.1 7.9 vs. 5.5 5.7 vs. 5.2 6.2 vs. 5.6 5.5 vs. 5.4 5.8 vs. 5.9 - RL ROL 8.3 vs. 4.2 8.8 vs. 4.5 5.8 vs. 5.0 6.5 vs. 5.5 6.2 vs. 4.9 7.0 vs. 5.4 5.9 vs. 5.8 6.4 vs. 6.3 HTG 8.7 vs. 4.4 8.8 vs 4.5 6.0 vs. 5.1 6.9 vs. 5.5 6.5 vs. 5.0 6.9 vs. 5.3 6.5 vs. 5.6 7.0 vs. 6.3 OPPA w/o OE 8.2 vs. 4.2 8.8 vs. 4.7 6.1 vs. 5.2 6.8 vs. 5.6 6.5 v.s. 4.8 7.0 v.s. 5.3 5.7 v.s. 5.8 6.5 v.s. 6.4 OPPA w/o A 8.7 vs. 4.1 8.9 vs. 4.3 6.3 vs. 5.0 7.2 vs. 5.4 6.5 vs. 4.8 7.2 vs. 5.4 6.5 vs. 6.1 7.1 vs. 6.8 OPPA 8.8 vs. 3.9 9.0 vs. 4.1 6.7 vs. 4.6 7.3 vs. 5.2 6.8 vs. 4.2 7.4 vs. 5.1 6.7 vs. 6.0 7.2 vs. 6.6 Table 2 : The results of our proposed OPPA and the baselines on the negotiation dataset . 
AllandAgreed indicates averaged scores for all sessions and only the agreed sessions respectively . 
Methodvs . 
SL RNN vs. RL RNN vs. ROL vs. RL ROL Agreed(% ) PO(% ) Agreed(% ) PO(% ) Agreed(% ) PO(% ) Agreed(% ) PO(% ) SL RNN 87.9 49.6 - - - - - RL RNN 89.9 58.6 81.5 60.3 - - - ROL 92.9 63.7 87.4 65.0 85.1 67.3 - RL ROL 94.4 74.8 85.7 74.6 71.2 76.4 67.5 77.2 HTG 94.8 75.1 88.3 75.4 83.2 77.8 66.1 73.2 OPPA w/o OBE 94.6 74.6 87.9 75.2 79.3 78.2 73.7 77.9 OPPA w/o A 95.6 77.9 91.9 77.4 82.4 78.8 78.0 79.5 OPPA 95.7 77.7 91.4 77.2 82.3 79.1 78.2 79.7 Table 3 : The proportion of agreed and Pareto optimal ( PO ) sessions for our proposed OPPA and the baselines on the negotiation dataset . 
modeling the opposite policy in one ‚Äôs mind is a crucial source to achieve better results in competitive dialogue policy learning . 
When comparing with OPPA w/o A which removed action regularization , we can see that the OPPA model gets better results . 
This veriÔ¨Åes the importance of regularizing the action sampling . 
By controlling the difference between real and model generated actions , we can keep the opposite model consistent with the real opposite agent at the early training stage . 
The percentage of agreed and Pareto optimal session are shown in Table 3 . 
As we can see , the percentage of Pareto optimal increases in our method , showing that the OPPA model can explore the solution space more effectively . 
However , the agreement rate decreases when the opposite model gets stronger . 
This phenomenon is also found in Lewis et al . 
( 2017 ) when they change the opposite agent from SL RNN to real human . 
This can be attributed to the aggressiveness of the agent : when both agents act aggressively , they are less likely to reach an agreement . 
The SL RNN model simply imitates the behavior in the dialogue corpus , while the ROL and RL mechanisms both help the agent to explore more spaces , which makes them more aggressive on action selection.4.7 Human Evaluation To better validate our propositions , we further conducted human evaluation by making our model conversing with real user . 
We only conducted human evaluation on the negotiation task since the MultiWOZ model is implemented on the dialogue act level . 
We tested the models on a total of 1,000 dialogue sessions . 
In the evaluation , the users conversed with the agent , and the total item values are used as the evaluation metrics . 
The results are shown in Table 4 . 
We can see that our proposed OPPA outperforms the baseline models . 
The system score are lower than that in Table 2 , and the discrepancy between AllandAgreed results is large . 
This can be due to the high intelligence and aggressiveness of real humans who want to get as more values as possible and do not make compromises easily . 
Due to this reason , the sessions become considerably lengthy , and the target agent exceeds our length limit before reaching an agreement . 
Method All Agreed RL ROL 4.5 vs. 5.2 7.8 vs. 7.1 HTG 4.8 vs. 4.7 8.0 vs. 7.2 OPPA w/o A 4.7 vs. 4.9 8.4 vs. 6.7 OPPA 5.2 vs. 5.1 8.2 vs. 6.5 Table 4 : The rewards of each model vs. human user . 
5 Conclusion In this work , we present an opposite agent - aware dialogue policy model which actively estimates the129 opposite agent instead of doing passive learning from experiences . 
We have shown that it is possible to harvest a reliable model of the opposite agent through more efÔ¨Åcient dialogue interactions . 
By incorporating the estimated model output as part of dialogue state , the target agent shows signiÔ¨Åcant improvement on both cooperative and competitive goal - oriented tasks . 
As future work , we will explore multi - party dialogue modeling in which multi - agent learning techniques can be applied . 
Acknowledgments This work was jointly supported by the NSFC projects ( key project with No . 
61936010 and regular project with No . 
61876096 ) , and the Guoqiang Institute of Tsinghua University with Grant No . 
2019GQG1 . 
This work was also supported by Beijing Academy of ArtiÔ¨Åcial Intelligence , BAAI and Beijing Nova Program ( Z201100006820068 ) from Beijing Municipal Science & Technology Commission . 
We thank THUNUS NExT Joint - Lab for the support . 
Abstract Typically , tokenization is the very Ô¨Årst step in most text processing works . 
As a token serves as an atomic unit that embeds the contextual information of text , how to deÔ¨Åne a token plays a decisive role in the performance of a model . 
Even though Byte Pair Encoding ( BPE ) has been considered the de facto standard tokenization method due to its simplicity and universality , it still remains unclear whether BPE works best across all languages and tasks . 
In this paper , we test several tokenization strategies in order to answer our primary research question , that is , ‚Äú What is the best tokenization strategy for Korean NLP tasks ? ‚Äù Experimental results demonstrate that a hybrid approach of morphological segmentation followed by BPE works best in Korean to / from English machine translation and natural language understanding tasks such as KorNLI , KorSTS , NSMC , and PAWS - X. As an exception , for KorQuAD , the Korean extension of SQuAD , BPE segmentation turns out to be the most effective . 
Our code and pre - trained models are publicly available at https://github.com/ kakaobrain / kortok . 
1 Introduction Tokenization is the very Ô¨Årst step in most text processing works . 
Not surprisingly , tremendous academic efforts have been made to Ô¨Ånd the best tokenization method for various NLP tasks . 
For the past few years , Byte Pair Encoding ( BPE ) ( Gage , 1994 ) has been considered the de facto standard tokenization technique since it was reintroduced by Sennrich et al . 
( 2016a ) . 
Besides the fact that BPE turns out to be very effective in the machine translation task , another important reason BPE has gained ‚àó*Equal contribution.such popularity is that BPE is a data - driven statistical algorithm so it is independent of language . 
However , it is still not clear whether BPE works best across all languages , irrespective of tasks . 
In this paper we study various tokenization strategies for Korean , a language which is morphologically by far richer than English . 
Concretely , we empirically examine what is the best tokenization strategy for Korean to English / English to Korean machine translation tasks , and natural language understanding ( NLU ) tasks ‚Äî machine reading comprehension ( MRC ) , natural language inference ( NLI ) , semantic textual similarity ( STS ) , sentiment analysis , and paraphrase identiÔ¨Åcation . 
We are particularly interested in how complementary BPE and linguistically motivated segmentation are . 
2 Background 2.1 MeCab - ko : A Korean Morphological Analyzer MeCab ( Kudo , 2006 ) is an open - source morphological analyzer based on Conditional Random Fields ( CRFs ) . 
It is originally designed for Japanese , but also serves generic purposes so it can be applied to other languages . 
MeCab - ko1 , a Korean extension of MeCab , started from the idea that MeCab can be easily extended to the Korean language due to the close similarity between Japanese and Korean in terms of morphology or syntax . 
MeCab - ko trained its model on the Sejong Corpus ( Kang and Kim , 2001 ) , arguably the largest Korean corpus morphologically annotated by many experts , using MeCab . 
Ever since released in 2013 , MeCab - ko has been widely used for many Korean NLP tasks due to its high accuracy and good usability . 
For example , the Workshop on Asian Transla1https://bitbucket.org/eunjeon/ mecab - ko133 Tokenization Tokenized Sequence Raw Text /uni1102 /uni1161 / uni1105.1 / uni1161.10 / uni11BC.10 / uni1109.27 /uni116D.15 / uni1111.36 / uni1175.5 /uni11BC.2 / uni1112 /uni1161 / uni110C /uni1161 . 
CV ( 4.1 ) /uni3134//uni314F//uni3139//uni314F//uni3147/‚ãÜ//uni3145//uni315B//uni314D//uni3163//uni3147//uni314E//uni314F//uni3148//uni314F/. Syllable ( 4.2 ) /uni1102 /uni1161//uni1105.1 / uni1161.10 / uni11BC.10/‚ãÜ//uni1109.27 /uni116D.15//uni1111.36 / uni1175.5 /uni11BC.2//uni1112 /uni1161//uni110C /uni1161/. Morpheme ( 4.3 ) /uni1102 /uni1161//uni1105.1 / uni1161.10 / uni11BC.10/‚ãÜ//uni1109.27 /uni116D.15 / uni1111.36 / uni1175.5 /uni11BC.2//uni1112 /uni1161//uni110C /uni1161/. Subword ( 4.4 ) /uni1102 /uni1161 / uni1105.1 / uni1161.10 / uni11BC.10//uni1109.27 /uni116D.15//uni1111.36 / uni1175.5 /uni11BC.2 / uni1112 /uni1161//uni110C /uni1161/. Morpheme - aware Subword ( 4.5 ) /uni1102 /uni1161//uni1105.1 / uni1161.10 / uni11BC.10/‚ãÜ//uni1109.27 /uni116D.15//uni1111.36 / uni1175.5 /uni11BC.2//uni1112 /uni1161//uni110C /uni1161/. Word ( 4.6 ) /uni1102 /uni1161 / uni1105.1 / uni1161.10 / uni11BC.10//uni1109.27 /uni116D.15 / uni1111.36 / uni1175.5 /uni11BC.2 / uni1112 /uni1161 / uni110C /uni1161 /. Table 1 : An input sentence /uni1102 /uni1161 / uni1105.1 / uni1161.10 / uni11BC.10 / uni1109.27 /uni116D.15 / uni1111.36 / uni1175.5 /uni11BC.2 / uni1112 /uni1161 / uni110C /uni1161 . 
‚Äò Let ‚Äôs go shopping with me . 
‚Äô is differently tokenized depending on the various tokenization strategies . 
Slashes ( / ) are token separators . 
tion ( WAT ) has adopted it as the ofÔ¨Åcial segmentation tool for evaluating Korean machine translation results since 2015 . 
( Nakazawa et al . 
, 2015 , 2016 , 2017 , 2018 , 2019 ) . 
2.2 Byte Pair Encoding Byte Pair Encoding ( BPE ) is a simple data compression technique that iteratively replaces the most frequent pair of bytes in text with a single , unused byte ( Gage , 1994 ) . 
Since Sennrich et al . 
( 2016b ) successfully applied it to neural machine translation models , it has been regarded as the standard tokenization method across languages . 
Korean is not an exception ; Park et al . 
( 2019b ) applied BPE to the Korean text in the Korean to Japanese task of WAT 2019 and ranked Ô¨Årst . 
In addition , most recent Korean neural language models ( e.g. , KoBERT2 ) used BPE to tokenize the training text . 
3 Related Work There have been extensive studies about tokenization techniques for machine translation . 
Several papers claimed that a hybrid of linguistically informed segmentation and a data - driven method like BPE or unigram language modeling performs the best for non - English languages . 
Banerjee and Bhattacharyya ( 2018 ) combined an off - the - shelf morphological segmenter and BPE in Hindi and Bengali translations against English . 
TawÔ¨Åk et al . 
( 2019 ) used a retrained version of linguistically motivated segmentation model along with statistical segmentation methods for Arabic . 
Pinnis et al . 
( 2017 ) adopted linguistic guidance to BPE for English - Latvian translation . 
Particularly ( Park et al . 
, 2019a ) is close to ours , but their main focus is on preprocessing techniques for neural machine 2https://github.com/SKTBrain/KoBERTtranslation like parallel corpus Ô¨Åltering rather than on tokenization strategies per se . 
Compared with the tokenization studies for machine translation , those for NLU tasks have gained less attention . 
Among them is Bostrom and Durrett ( 2020 ) , which compared the Ô¨Åne - tuning task performance of BERT ( Devlin et al . 
, 2019 ) pre - trained with BPE and unigram language modeling . 
Moon and Okazaki ( 2020 ) proposed a novel encoding method for Korean and showed its efÔ¨Åciency in vocabulary compression with a few Korean NLU datasets . 
4 Tokenization Strategies We introduce assorted Korean tokenization strategies arranged from the smallest to the largest unit . 
Each of them induces different tokenization results , as illustrated in Table 1 . 
4.1 Consonant and Vowel ( CV ) In Hangul , the standard Korean writing system , consonants and vowels , called Jamo in Korean , corresponding to Latin letters are assembled to form a syllable character . 
For example , a Hangul consonant / uni314E / h/ ( U+314E ) is combined with a vowel /uni314F /a/ ( U+314F ) to make a syllable character /uni1112 /uni1161 / ha/ ( U+558 ) . 
Readers who are not familiar with such a mechanism can think of Jamo and syllables as atoms and molecules respectively . 
As a molecule H2Ocan be decomposed into two Hatoms and an Oatom , a syllable / uni1112 /uni1161 / ha/ can be decomposed into its constituent consonant /uni314E / h/ and vowel / uni314F / a/. The Ô¨Årst syllable / uni1102 /uni1161 / na/ of the raw text in Table 1 is tokenized into / uni3134 / n/ and / uni314F / a/ , and the second syllable / uni1105.1 / uni1161.10 / uni11BC.10 / lang/ is tokenized into /uni3139 / l/,/uni314F / a/ , and / uni3147 / ng/ , and so on . 
A whitespace is replaced by a special symbol ‚ãÜ.134 4.2 Syllable We can tokenize a sentence at the syllable level . 
A whitespace is replaced by the special symbol ‚ãÜ. 4.3 Morpheme MeCab - ko provides a convenient tokenization option in the command line interface3 . 
For example , it returns A , B , and Cgiven an input text AB C , where A - Crepresent morphemes . 
Note that the original space between ABandCis missing in the output token list . 
Accordingly , it is NOT possible to recover the original text from the tokenized result . 
This can be problematic in some tasks that require us to restore the input text such as machine translation whose target language is Korean , or machine reading comprehension where we are expected to suggest a certain phrase in the given text as the answer . 
For this reason , we insert a special token ‚ãÜ ( U+2B51 ) to the original whitespace position . 
As a result , in the above example , the tokenized sequence looks like A , B,‚ãÜ , andD. 4.4 Subword We learn and apply BPE using the SentencePiece ( Kudo and Richardson , 2018 ) library . 
It prepends ‚Äò ‚Äô ( U+2581 ) to every word to mark the original whitespace , then tokenizes text into subword pieces . 
As seen in Table 1 , /uni1102 /uni1161 / uni1105.1 / uni1161.10 / uni11BC.10 / uni1109.27 /uni116D.15 / uni1111.36 / uni1175.5 /uni11BC.2 / uni1112 /uni1161 / uni110C /uni1161 . 
can be split into / uni1102 /uni1161 / uni1105.1 / uni1161.10 / uni11BC.10,/uni1109.27 /uni116D.15,/uni1111.36 / uni1175.5 /uni11BC.2 / uni1112 /uni1161,/uni110C /uni1161 , and . 
( period ) . 
4.5 Morpheme - aware Subword Motivated by the combined methods of dataand linguistically - driven approaches ( Banerjee and Bhattacharyya , 2018 ; Park et al . 
, 2019a ; Pinnis et al . 
, 2017 ; TawÔ¨Åk et al . 
, 2019 ) , we apply MeCabko and BPE in sequence to make morpheme - aware subwords . 
According to this strategy , since BPE is applied after the original text is split into morphemes , tokens spanning multiple morphemes ( e.g. , /uni1111.36 / uni1175.5 /uni11BC.2 / uni1112 /uni1161 in the Section 4.4 ) are not generated . 
Instead , the BPE algorithm further segments morphemes into frequent pieces . 
4.6 Word We can simply split text by whitespaces . 
Note that punctuation marks are split into separate tokens . 
Check that / uni1102 /uni1161 / uni1105.1 / uni1161.10 / uni11BC.10 / uni1109.27 /uni116D.15 / uni1111.36 / uni1175.5 /uni11BC.2 / uni1112 /uni1161 / uni110C /uni1161 . 
is tokenized into / uni1102 /uni1161 / uni1105.1 / uni1161.10 / uni11BC.10 , /uni1109.27 /uni116D.15 / uni1111.36 / uni1175.5 /uni11BC.2 / uni1112 /uni1161 / uni110C /uni1161 and . 
( period ) in Table 1 . 
3 % mecab -O wakatiLang PairVocab SizeKorean BPE Training DataDev Test Ko - En 32KAI Hub ( 130 MB ) 35.79 36.06 Wiki ( 613 MB ) 39.05 38.69 En - Ko 32KAI Hub ( 130 MB ) 37.19 36.98 Wiki ( 613 MB ) 37.11 36.98 Table 2 : BLEU scores of Korean to English ( Ko - En ) and English to Korean ( En - Ko ) translation models with different BPE training data . 
Note that the English sentences are tokenized using a 32 K BPE model trained on the English Wiki . 
5 Experiments 5.1 Korean to / from English Machine Translation 5.1.1 Dataset To date , there have yet been few open source benchmark datasets for Korean - English machine translation , not to mention that Korean is not in the language list of WMT4or IWSLT5 . 
Park et al . 
( 2019a ) used OpenSubtitles ( Lison and Tiedemann , 2016 ) , a collection of crowd - sourced movie subtitles across 65 different languages , for English to Korean translation , but they are too noisy to serve as a translation benchmark dataset.6 Recently , a Korean - English parallel corpus was publicly released by AI Hub7 , which was gathered from various sources such as news , government web sites , legal documents , etc . 
We download the news data , which amount to 800 K sentence pairs , and randomly split them into 784 K ( train ) , 8 K ( dev ) , and 8 K ( test ) . 
5.1.2 BPE Modeling Prior to training , we do simple preliminary experiments to decide which dataset to use for learning BPE . 
There are two choices : AI Hub news training data and open source large text such as Wiki . 
AI Hub training data is relatively small in size ( 130 MB ) , but can be optimal as its lexical distribution will be close to that of the test data , considering both of them are from the same source . 
On the other hand , Wiki is larger , but it is not news per se , so can be not as appropriate as AI Hub data for 4https://www.aclweb.org/anthology/ venues / wmt 5http://iwslt.org/doku.php?id=start 6Park et al . 
( 2019a ) reported BLEU scores of 7 - 12 . 
7http://www.aihub.or.kr/aidata/87135 Tokenization Vocab SizeKo - En En - KoOOV Rate ( % ) Avg . 
LengthDev Test Dev Test CV 166 39.11 38.56 36.52 36.45 0.02 142.75 Syllable 2 K 39.30 38.75 38.64 38.45 0.06 69.20 Morpheme8 K 31.59 31.24 32.44 32.19 7.51 49.19 16 K 34.38 33.80 35.74 35.52 4.67 49.19 32 K 36.19 35.74 36.51 36.12 2.72 49.19 64 K 37.88 37.37 37.51 37.03 1.40 49.19 Subword4 K 39.18 38.75 38.31 38.18 0.07 48.02 8 K 39.16 38.75 38.09 37.94 0.08 38.44 16 K 39.22 38.77 37.64 37.34 0.10 33.69 32 K 39.05 38.69 37.11 36.98 0.11 30.21 64 K 37.02 36.46 35.77 35.64 0.12 27.50 Morpheme - aware Subword4 K 39.41 38.95 39.29 39.13 0.06 65.17 8 K 39.42 39.06 39.78 39.61 0.06 56.79 16 K 39.84 39.41 40.23 40.04 0.07 53.30 32 K 41.00 40.34 40.43 40.41 0.07 51.38 64 K 39.62 39.34 38.63 38.42 0.07 50.27 Word 64 K 7.04 7.07 18.68 18.42 26.20 18.96 Table 3 : BLEU scores of Korean to English ( Ko - En ) and English to Korean ( En - Ko ) translation models of various tokenization strategies . 
Note that we use an 32 K Subword model for English for all of them . 
The OOV rate values in the table are obtained from the test set , but there is no meaningful difference between the test and the dev set in terms of the OOV rate . 
The best BLEU scores in each column ( global ) and group ( local ) are bold - faced and underlined , respectively . 
BPE modeling . 
To investigate this , Ô¨Årst we train a 32 K Korean BPE model ( A ) using SentencePiece with the Korean sentences in the AI Hub training data . 
Then we download the latest Wikipedia Korean8 / English9dumps , and extract plain texts using WikiExtractor10 . 
Next , we make 32 K BPE models for Korean ( B ) and English ( C ) with them . 
Finally , we train Korean to English ( Ko - En ) and English to Korean ( En - Ko ) translation models on the AI Hub training data with the two different Korean BPE models ( A , B ) . 
The training details are explained in Section 5.1.3 . 
For comparison , we use the same English BPE model ( C ) for both . 
The results are shown in Table 2 . 
For Ko - En translation , the Wiki - based BPE model performs better in both dev and test sets by 2 - 3 points . 
For En - Ko translation , there is no practical difference in performance between the Wiki and AI Hub - based models . 
It is also worth considering the BPE models are used for NLU tasks as well as machine translation . 
All things taken together , we opt for 8https://dumps.wikimedia.org/kowiki 9https://dumps.wikimedia.org/enwiki 10https://github.com/attardi/ wikiextractorthe Wiki - based BPE model . 
5.1.3 Training We test the tokenization strategies in Section 4 with various vocabulary sizes on the AI Hub news dataset . 
We use the Transformer ( Vaswani et al . 
, 2017 ) , the state - of - the - art model for neural machine translation . 
We mostly follow the base model conÔ¨Åguration : 6 blocks of 512 - 2048 units with 8 attention heads . 
We run all of our experiments using FAIRSEQ11(Ott et al . 
, 2019 ) , a PyTorch based deep learning library for sequence to sequence models . 
Each model is trained using a Tesla V100 GPU with batch size 128 , dropout rate 0.3 , label smoothing 0.1 , and the Adam ( Kingma and Ba , 2015 ) optimizer . 
We set the learning rate to 5e-4 with the inverse square - root schedule . 
We train all models for 50 epochs and save the checkpoint Ô¨Åles at every epoch . 
5.1.4 Results After all training stages are Ô¨Ånished , we evaluate the saved checkpoint Ô¨Åles of each model on 11https://github.com/pytorch/fairseq136 Vocab Size # Tokens # Tokens Spanning Morpheme Boundaries 4 K 387,088 25,458 ( 6.58 % ) 8 K 309,360 50,029 ( 16.17 % ) 16 K 271,334 62,861 ( 23.17 % ) 32 K 242,736 73,609 ( 30.26 % ) 64 K 221,530 82,324 ( 37.16 % ) Table 4 : Number of tokens spanning morpheme boundaries in Subword models . 
the dev set to Ô¨Ånd the best one , which is subsequently used for the Ô¨Ånal test . 
In Table 3 we report BLEU scores on both the dev and test sets using the Moses12multi-bleu.perl script . 
Following WAT 2019 ( Nakazawa et al . 
, 2019 ) , Moses tokenizer and MeCab - ko are used for tokenizing the evaluation data . 
For both Ko - En and En - Ko , overall , the Subword models ( 35.64 - 39.22 ) and the Syllable models ( 38.45 - 39.30 ) are superior to the Morpheme models ( 31.59 - 37.37 ) or the Word models ( 7.04 - 18.42 ) in performance . 
It is highly likely to come from the lower OOV rates of the Subword models ( 0.070.12 ) and the Syllable models ( 0.06 ) compared to those of the Morpheme models ( 1.40 - 7.51 ) and the Word models ( 26.20 ) . 
While BPE tends to split rare words into subword pieces , MeCab - ko is ignorant of statistics so it splits words into morphemes by linguistic knowledge instead . 
That the Morpheme and Word models generate many OOVs suggests Korean has so large types of morphemes or word forms that even 64 K vocabulary is not enough to cover them all . 
CV models are tiny in vocabulary size ( 166 ) so they show the lowest OOV rate ( 0.02 ) . 
However , their performance is not as good as the Syllable or Subword models . 
We speculate this is because a single consonant or vowel must bear too much contextual information in the CV models . 
Morpheme - aware Subword 32 K models achieve the best BLEU scores . 
Each Subword model , as shown in Table 4 , contains 6 - 37 % of tokens spanning morpheme boundaries in the test set , which implies that subword segmentation by BPE is not optimal and morpheme boundaries are meaningful in tokenization . 
To sum up , morpheme - aware subword tokenization that makes the best use of linguistic knowledge and statistical information is the best for Korean machine translation . 
12http://www.statmt.org/mosesHyperparamKorQuAD KorNLI KorSTS NSMC PA WS Epoch 5 3 5 3 5 Batch 16 64 64 64 64 Œ∑ 5e-5 1e-4 5e-5 5e-5 1e-4 Dropout 0.1 0.1 0.1 0.1 0.1 Warm - up 0.1 0.1 0.1 0.1 0.1 Max Seq.‚Ä†128 128 128 128 128 Table 5 : Fine - tuning hyper - parameters for NLU tasks . 
Œ∑ : learning rate.‚Ä† : Max sequence length is 256 for CV models in all tasks . 
5.2 Korean Natural Language Understanding Tasks Large pre - trained language models have proven their effectiveness in many downstream tasks ( Peters et al . 
, 2018 ; Devlin et al . 
, 2019 ; Liu et al . 
, 2019 ) . 
We pre - train BERT ( Devlin et al . 
, 2019 ) models with various tokenization strategies , and Ô¨Åne - tune them on Ô¨Åve different Korean NLU tasks . 
5.2.1 Machine Reading Comprehension : KorQuAD 1.0 Dataset The KorQuAD 1.0 dataset ( Lim et al . 
, 2019 ) is a Korean adaptation of SQuAD 1.0 ( Rajpurkar et al . 
, 2016 ) , a popular reading comprehension dataset . 
KorQuAD 1.0 consists of 10,645 passages and their paired 66,181 questions ( 60,407 for training + 5,774 for development13 ) . 
Like SQuAD 1.0 , KorQuAD 1.0 involves answering a question given a passage . 
The answer must be a phrase within the passage . 
5.2.2 Natural Language Inference : KorNLI Dataset The KorNLI Dataset ( Ham et al . 
, 2020 ) is a Korean NLI dataset sourced from three different NLI datasets : SNLI ( Bowman et al . 
, 2015 ) , MNLI ( Williams et al . 
, 2018 ) , and XNLI ( Conneau et al . 
, 2018 ) . 
It is composed of 950,354 sentence pairs : 942,854 for training , 2,490 for development , and 5,010 for test . 
A model receives a pair of sentences ‚Äî a premise and a hypothesis ‚Äî and classiÔ¨Åes their relationship into one out of three categories : entailment , contradiction , and neutral . 
5.2.3 Semantic Textual Similarity : KorSTS Dataset The KorSTS Dataset ( Ham et al . 
, 2020 ) is a Korean STS dataset translated from the STS - B dataset ( Cer et al . 
, 2017 ) . 
It comprises 8,628 sentence 13The test dataset is not included.137 TokenizationVocab SizeKorQuAD KorNLI KorSTS NSMC PA WS - X Dev ( EM / F1 ) Dev Test Dev Test Dev Test Dev Test CV 166 59.66 / 73.91 70.60 71.20 77.22 71.47 87.97 87.89 58.00 55.20 Syllable 2 K 69.10 / 83.29 73.98 73.47 82.70 75.86 88.94 89.07 68.65 67.20 Morpheme32 K 68.05 / 83.82 74.86 74.37 82.37 76.83 87.87 88.04 69.30 67.20 64 K 70.68 / 85.25 75.06 75.69 83.21 77.38 88.72 88.88 73.40 68.65 Subword4 K 71.48 / 83.11 74.38 74.03 83.37 76.80 89.08 89.30 72.00 69.60 8 K 72.91 / 85.11 74.18 74.65 83.23 76.42 89.08 89.19 73.45 69.00 16 K 73.42 / 85.75 74.46 75.15 83.30 76.41 88.89 88.88 73.40 70.70 32 K 74.04 / 86.30 74.74 74.29 83.02 77.01 89.39 89.38 74.05 70.95 64 K 74.04 /86.66 73.73 74.55 83.52 77.47 88.80 89.19 75.85 72.10 Morpheme - aware Subword4 K 67.53 / 81.93 73.53 73.45 83.34 76.03 88.93 89.32 69.75 67.45 8 K 70.90 / 84.57 74.14 73.95 83.71 76.07 89.37 89.29 73.40 71.30 16 K 69.47 / 83.36 75.02 74.99 83.22 76.59 89.33 89.41 75.05 71.70 32 K 72.65 / 86.35 74.10 75.13 83.65 78.11 89.53 89.65 74.60 71.60 64 K 69.48 / 83.73 76.39 76.61 84.29 76.78 89.82 89.66 76.15 74.00 Word 64 K 1.54 / 8.86 64.06 65.83 69.00 60.41 70.10 70.58 58.25 55.30 Table 6 : Performance of various models on several Korean natural language understanding tasks . 
The evaluation metrics are as follows : KorQuAD : Exact Match / F1 , KorNLI : accuracy ( % ) , KorSTS : 100 √óSpearman correlation , NSMC : accuracy ( % ) , PAWS - X : accuracy ( % ) . 
The best scores in each column ( global ) and group ( local ) are bold - faced and underlined , respectively . 
pairs‚Äî5,749 for training , 1,500 for development , and 1,379 for test . 
The task assesses the gradations of semantic similarity between two sentences with a scale from 0 to 5 . 
5.2.4 Sentiment Analysis : NSMC Dataset NSMC14is a movie review dataset scraped from Naver MoviesTM . 
It consists of 200 K samples of which 150 K are the training set and the rest 50 K are the test set . 
Each sample is labeled with 0 ( negative ) or 1 ( positive ) . 
We hold out 10 percent of the training data for development . 
5.2.5 Paraphrase IdentiÔ¨Åcation : PA WS - X Dataset The PAWS - X dataset ( Yang et al . 
, 2019 ) is a challenging paraphrase identiÔ¨Åcation dataset in six languages including Korean . 
The Korean portion amounts to 53,338 sentence pairs ( 49,410 for training , 1,965 for development , and 1,972 for test ) . 
Like the NSMC dataset , each sentence pair is annotated with either 0 ( negative ) or 1 ( positive ) . 
For each tokenization strategy , we pre - train a BERT - Base model on a large corpus and Ô¨Åne - tune it on the training sets of the Ô¨Åve NLU tasks independently . 
Pre - training . 
Because the Korean Wiki corpus is not enough in volume , 640 MB , for the pre14https://github.com/e9t/nsmctraining purpose , we additionally download the recent dump of Namuwiki15 , a Korean Wiki , and extract plain texts using Namu Wiki Extractor16 . 
On the resulting Namuwiki corpus ( 5.5 GB ) along with the Wiki corpus ( 640 MB ) , pre - training is performed with a Cloud TPU v3 - 8 for 1 M steps using the ofÔ¨Åcial BERT training code17 , which is based on TensorFlow . 
We set the training hyperparameters of all models as follows : batch size = 1024 , max sequence length = 128 , optimizer = AdamW ( Loshchilov and Hutter , 2019 ) , learning rate = 5e-5 , warm - up steps = 10K. Fine - tuning . 
After converting each of the pretrained models in TensorFlow into PyTorch , we Ô¨Åne - tune it using HuggingFace Transformers18 ( Wolf et al . 
, 2019 ) . 
The hyper - parameters for each task are shown in Table 5 . 
5.2.6 Results In Table 6 we report the evaluation results of the various models on the dev and test sets . 
Since KorQuAD lacks the test set , we report the results on the dev set only . 
15http://dump.thewiki.kr 16https://github.com/jonghwanhyeon/ namu - wiki - extractor 17https://github.com/google-research/ bert 18https://github.com/huggingface/ transformers138 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Avg . 
# syllables per token3234363840BLEU scores on test set 8k16k32k64k4k 8k 16k 32k 64k4k8k16k32k 64k 2k166 CV Syllable Morpheme Subword Morpheme - aware Subword(a ) Ko - En 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Avg . 
# syllables per token3234363840BLEU scores on test set 8k16k32k64k4k 8k 16k 32k 64k4k8k16k32k 64k 2k 166 CV Syllable Morpheme Subword Morpheme - aware Subword ( b ) En - Ko Figure 1 : Translation performance over the average number of syllables per token As for KorQuAD , Subword 64 K models achieve the highest Exact Match ( EM ) and F1 scores . 
The scores in the Subword and Morpheme models increase monotonically as the vocabulary size grows . 
On the other hand , the 32 K models outperform the others in the Morpheme - aware Subword models ; no clear correlation is found between performance and vocabulary sizes in them . 
For all the other four tasks , Morpheme - aware Subword 64 K models show the best scores . 
One noteworthy phenomenon is that the scores tend to increase as the vocabulary size grows across the tokenization groups . 
This is discordant with the machine translation results in Section 5.1.4 , where a larger vocabulary size does not guarantee better performance for the Subword and Morpheme - aware Subword models . 
6 Discussion We further examine which factors with respect to tokenization affect the Ko - En and En - Ko translation performance . 
6.1 Token Length Because tokenization involves splitting a text into shorter segments , we Ô¨Ånd it important to Ô¨Ågure out how much information each segment bears . 
To this end , based on the assumption that the longer a text is , the more information it is likely to have , we plot the BLEU scores by the average number of syllables per Korean token in the translation testsets in Figure 1 . 
The BLEU scores of the subword models ‚Äî Syllable , Morpheme , Subword , and Morphemeaware Subword ‚Äî are mostly higher than those of the CV models , which are plotted as dotted lines . 
In particular , the Syllable , Subword , and Morphemeaware Subword models between 1.00 and 1.50 show the best scores both in Ko - En and in En - Ko . 
When a token has more than 1.5 syllables on average , the scores begin to decrease , and the Word models which has more than 2.5 syllables in a token performs the worst ( 7.07 for Ko - En and 18.42 for En - Ko ) . 
Note that they are not in the Ô¨Ågures due to space constraints . 
6.2 Linguistic Awareness Obviously token length is not the only key factor in tokenization strategies . 
Let us compare the Morpheme - aware Subword 16 K models ( green markers ) and Subword 8 K models ( red markers ) in the shaded regions in Figure 1 . 
Although they have the same average token length around 1.4 , the Morpheme - aware Subword models outperform the Subword models . 
We believe this is evidence to support that linguistic awareness is another important factor in Korean tokenization strategies for machine translation . 
6.3 Under - trained Tokens In section 5.1.4 , we pointed out high OOV rates are highly likely to degrade the performance of139 n=1 n=20 n=40 n=60 n=80 n=1000.00.51.01.52.02.53.03.5Percentage of tokens that appear in training set less than n times ( % ) CV 166 Syllable 2 K Morpheme 32 K Subword 32 K Morpheme - aware Subword 32KFigure 2 : Percentage of under - trained tokens in various tokenization strategies Morpheme models . 
It is also worth noting that in Figure 1 as most of the orange markers denoting Morpheme models are below the dotted lines . 
OOVs are the tokens that appear only in the test set . 
They are an extreme case of under - trained tokens ‚Äî test set ‚Äôs tokens that appear in the training set for the limited number of times . 
Figure 2 shows how much under - trained tokens account for in each model , ranging from n= 1ton= 100 , wheren is the frequency of the under - trained tokens in the training set . 
Clearly , the curve of the Morpheme 32 K model is far above that of the others , indicating that it suffers from the problem of under - trained tokens the most . 
7 Conclusion We explored various Korean tokenization strategies on machine translation and Ô¨Åve NLU tasks . 
In machine translation Morpheme - aware Subword models with a vocabulary size worked best for both Korean to English and English to Korean settings . 
By contrast , there was no single best tokenization strategy for the NLU tasks . 
Instead , Subword 64 K models showed the best performance on KorQuAD , whereas Morpheme - aware Subword 64 K models turned out to be optimal for the other KorNLI , KorSTS , NSMC , and PAWS - X tasks . 
Acknowledgments We are grateful to the anonymous reviewers for their valuable comments . 
For pre - training models , we used Cloud TPUs provided by TensorFlow Research Cloud program . 
References Tamali Banerjee and Pushpak Bhattacharyya . 
2018 . 
Meaningless yet meaningful : Morphology grounded subword - level NMT . 
In Proceedings of the Second Workshop on Subword / Character LEvel Models , pages 55‚Äì60 , New Orleans . 
Association for Computational Linguistics . 
Kaj Bostrom and Greg Durrett . 
2020 . 
Byte pair encoding is suboptimal for language model pretraining . 
arXiv preprint arXiv:2004.03720 . 
Samuel R. Bowman , Gabor Angeli , Christopher Potts , and Christopher D. Manning . 
2015 . 
A large annotated corpus for learning natural language inference . 
InProceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 632‚Äì642 , Lisbon , Portugal . 
Association for Computational Linguistics . 
Daniel Cer , Mona Diab , Eneko Agirre , I Àúnigo LopezGazpio , and Lucia Specia . 
2017 . 
SemEval-2017 task 1 : Semantic textual similarity multilingual and crosslingual focused evaluation . 
In Proceedings of the 11th International Workshop on Semantic Evaluation ( SemEval-2017 ) , pages 1‚Äì14 , Vancouver , Canada . 
Association for Computational Linguistics . 
Alexis Conneau , Ruty Rinott , Guillaume Lample , Adina Williams , Samuel Bowman , Holger Schwenk , and Veselin Stoyanov . 
2018 . 
XNLI : Evaluating cross - lingual sentence representations . 
In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 2475‚Äì2485 , Brussels , Belgium . 
Association for Computational Linguistics . 
Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 
2019 . 
BERT : Pre - training of deep bidirectional transformers for language understanding . 
In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171‚Äì4186 , Minneapolis , Minnesota . 
Association for Computational Linguistics . 
Philip Gage . 
1994 . 
A new algorithm for data compression . 
C Users J. , 12(2):23‚Äì38 . 
Jiyeon Ham , Yo Joong Choe , Kyubyong Park , Ilji Choi , and Hyungjoon Soh . 
2020 . 
KorNLI and KorSTS : New benchmark datasets for korean natural language understanding . 
arXiv preprint arXiv:2004.03289 . 
Beom - mo Kang and Hung - gyu Kim . 
2001 . 
21st century sejong project - compiling korean corpora . 
In Proceedings of the 19th International Conference on Computer Processing of Oriental Languages ( ICCPOL 2001 ) , pages 180‚Äì183 . 
Diederik P Kingma and Jimmy Ba . 
2015 . 
Adam : A method for stochastic optimization . 
3rd International Conference on Learning Representations , ICLR 2015 .140 Taku Kudo . 
2006 . 
Mecab : Yet another part - ofspeech and morphological analyzer . 
https:// sourceforge.net/projects/mecab/ . 
Taku Kudo and John Richardson . 
2018 . 
SentencePiece : A simple and language independent subword tokenizer and detokenizer for neural text processing . 
In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing : System Demonstrations , pages 66‚Äì71 , Brussels , Belgium . 
Association for Computational Linguistics . 
Seungyoung Lim , Myungji Kim , and Jooyoul Lee . 
2019 . 
KorQuAD 1.0 : Korean qa dataset for machine reading comprehension . 
arXiv preprint arXiv:1909.07005 . 
Pierre Lison and J ¬®org Tiedemann . 
2016 . 
OpenSubtitles2016 : Extracting large parallel corpora from movie and TV subtitles . 
In Proceedings of the Tenth International Conference on Language Resources and Evaluation ( LREC‚Äô16 ) , pages 923‚Äì929 , PortoroÀáz , Slovenia . 
European Language Resources Association ( ELRA ) . 
Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov . 
2019 . 
Roberta : A robustly optimized bert pretraining approach . 
arXiv preprint arXiv:1907.11692 . 
Ilya Loshchilov and Frank Hutter . 
2019 . 
Decoupled weight decay regularization . 
7th International Conference on Learning Representations , ICLR 2019 . 
Sangwhan Moon and Naoaki Okazaki . 
2020 . 
Jamo pair encoding : Subcharacter representation - based extreme Korean vocabulary compression for efÔ¨Åcient subword tokenization . 
In Proceedings of The 12th Language Resources and Evaluation Conference , pages 3490‚Äì3497 , Marseille , France . 
European Language Resources Association . 
Toshiaki Nakazawa , Chenchen Ding , Hideya Mino , Isao Goto , Graham Neubig , and Sadao Kurohashi . 
2016 . 
Overview of the 3rd workshop on Asian translation . 
In Proceedings of the 3rd Workshop on Asian Translation ( WAT2016 ) , pages 1‚Äì46 , Osaka , Japan . 
The COLING 2016 Organizing Committee . 
Toshiaki Nakazawa , Nobushige Doi , Shohei Higashiyama , Chenchen Ding , Raj Dabre , Hideya Mino , Isao Goto , Win Pa Pa , Anoop Kunchukuttan , Shantipriya Parida , Ond Àárej Bojar , and Sadao Kurohashi . 
2019 . 
Overview of the 6th workshop on Asian translation . 
In Proceedings of the 6th Workshop on Asian Translation , pages 1‚Äì35 , Hong Kong , China . 
Association for Computational Linguistics . 
Toshiaki Nakazawa , Shohei Higashiyama , Chenchen Ding , Hideya Mino , Isao Goto , Hideto Kazawa , Yusuke Oda , Graham Neubig , and Sadao Kurohashi . 
2017 . 
Overview of the 4th workshop on Asian translation . 
In Proceedings of the 4th Workshop on Asian Translation ( WAT2017 ) , pages 1‚Äì54 , Taipei , Taiwan . 
Asian Federation of Natural Language Processing . 
Toshiaki Nakazawa , Hideya Mino , Isao Goto , Graham Neubig , Sadao Kurohashi , and Eiichiro Sumita . 
2015 . 
Overview of the 2nd workshop on Asian translation . 
In Proceedings of the 2nd Workshop on Asian Translation ( WAT2015 ) , pages 1‚Äì28 , Kyoto , Japan . 
Workshop on Asian Translation . 
Toshiaki Nakazawa , Katsuhito Sudoh , Shohei Higashiyama , Chenchen Ding , Raj Dabre , Hideya Mino , Isao Goto , Win Pa Pa , Anoop Kunchukuttan , and Sadao Kurohashi . 
2018 . 
Overview of the 5th workshop on Asian translation . 
In Proceedings of the 32nd PaciÔ¨Åc Asia Conference on Language , Information and Computation : 5th Workshop on Asian Translation : 5th Workshop on Asian Translation , Hong Kong . 
Association for Computational Linguistics . 
Myle Ott , Sergey Edunov , Alexei Baevski , Angela Fan , Sam Gross , Nathan Ng , David Grangier , and Michael Auli . 
2019 . 
fairseq : A fast , extensible toolkit for sequence modeling . 
In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics ( Demonstrations ) , pages 48‚Äì53 , Minneapolis , Minnesota . 
Association for Computational Linguistics . 
Chanjun Park , Gyeongmin kim , and Heuiseok Lim . 
2019a . 
Parallel corpus Ô¨Åltering and koreanoptimized subword tokenization for machine translation . 
In Proceedings of the 31st Annual Conference on Human & Cognitive Language Technology . 
Cheoneum Park , Young - Jun Jung , Kihoon Kim , Geonyeong Kim , Jae - Won Jeon , Seongmin Lee , Junseok Kim , and Changki Lee . 
2019b . 
KNUHYUNDAI ‚Äôs NMT system for scientiÔ¨Åc paper and patent tasks on WAT 2019 . 
In Proceedings of the 6th Workshop on Asian Translation , pages 81‚Äì89 , Hong Kong , China . 
Association for Computational Linguistics . 
Matthew Peters , Mark Neumann , Mohit Iyyer , Matt Gardner , Christopher Clark , Kenton Lee , and Luke Zettlemoyer . 
2018 . 
Deep contextualized word representations . 
In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long Papers ) , pages 2227‚Äì2237 , New Orleans , Louisiana . 
Association for Computational Linguistics . 
M¬Øarcis Pinnis , Rihards Kri Àáslauks , Daiga Deksne , and Toms Miks . 
2017 . 
Neural machine translation for morphologically rich languages with improved subword units and synthetic data . 
In International Conference on Text , Speech , and Dialogue , pages 237 ‚Äì 245 . 
Springer . 
Pranav Rajpurkar , Jian Zhang , Konstantin Lopyrev , and Percy Liang . 
2016 . 
SQuAD : 100,000 + questions for machine comprehension of text . 
In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 2383‚Äì2392 , Austin , Texas . 
Association for Computational Linguistics.141 Rico Sennrich , Barry Haddow , and Alexandra Birch . 
2016a . 
Improving neural machine translation models with monolingual data . 
In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 86‚Äì96 , Berlin , Germany . 
Association for Computational Linguistics . 
Rico Sennrich , Barry Haddow , and Alexandra Birch . 
2016b . 
Neural machine translation of rare words with subword units . 
In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1715 ‚Äì 1725 , Berlin , Germany . 
Association for Computational Linguistics . 
Ahmed TawÔ¨Åk , Mahitab Emam , Khaled Essam , Robert Nabil , and Hany Hassan . 
2019 . 
Morphology - aware word - segmentation in dialectal Arabic adaptation of neural machine translation . 
In Proceedings of the Fourth Arabic Natural Language Processing Workshop , pages 11‚Äì17 , Florence , Italy . 
Association for Computational Linguistics . 
Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , ≈Å ukasz Kaiser , and Illia Polosukhin . 
2017 . 
Attention is all you need . 
In Advances in Neural Information Processing Systems 30 , pages 5998‚Äì6008 . 
Curran Associates , Inc. Adina Williams , Nikita Nangia , and Samuel Bowman . 
2018 . 
A broad - coverage challenge corpus for sentence understanding through inference . 
In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long Papers ) , pages 1112‚Äì1122 . 
Association for Computational Linguistics . 
Thomas Wolf , Lysandre Debut , Victor Sanh , Julien Chaumond , Clement Delangue , Anthony Moi , Pierric Cistac , Tim Rault , R‚Äôemi Louf , Morgan Funtowicz , and Jamie Brew . 
2019 . 
Huggingface ‚Äôs transformers : State - of - the - art natural language processing . 
ArXiv , abs/1910.03771 . 
Yinfei Yang , Yuan Zhang , Chris Tar , and Jason Baldridge . 
2019 . 
PAWS - x : A cross - lingual adversarial dataset for paraphrase identiÔ¨Åcation . 
In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLP - IJCNLP ) , pages 3687‚Äì3692 , Hong Kong , China . 
Association for Computational Linguistics.142 Proceedings of the 1st Conference of the Asia - PaciÔ¨Åc Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing , pages 143‚Äì153 December 4 - 7 , 2020 . 
¬© 2020 Association for Computational Linguistics BERT - Based Neural Collaborative Filtering and Fixed - Length Contiguous Tokens Explanation Reinald Adrian Pugoy1,2and Hung - Yu Kao1 1National Cheng Kung University , Tainan City , Taiwan 2University of the Philippines Open University , Los Ba Àúnos , Philippines rdpugoy@up.edu.ph , hykao@mail.ncku.edu.tw Abstract We propose a novel , accurate , and explainable recommender model ( BENEFICT ) that addresses two drawbacks that most reviewbased recommender systems face . 
First is their utilization of traditional word embeddings that could inÔ¨Çuence prediction performance due to their inability to model the word semantics ‚Äô dynamic characteristic . 
Second is their black - box nature that makes the explanations behind every prediction obscure . 
Our model uniquely integrates three key elements : BERT , multilayer perceptron , and maximum subarray problem to derive contextualized review features , model user - item interactions , and generate explanations , respectively . 
Our experiments show that BENEFICT consistently outperforms other state - of - the - art models by an average improvement gain of nearly 7 % . 
Based on the human judges ‚Äô assessment , the BENEFICT - produced explanations can capture the essence of the customer ‚Äôs preference and help future customers make purchasing decisions . 
To the best of our knowledge , our model is one of the Ô¨Årst recommender models to utilize BERT for neural collaborative Ô¨Åltering . 
1 Introduction In recommender systems research , collaborative Ô¨Åltering ( CF ) is the dominant state - of - the - art recommendation model , which primarily focuses on learning accurate representations of users ( user preferences ) and items ( item characteristics ) ( Chen et al . 
, 2018 ; Tay et al . 
, 2018 ) . 
The earliest recommender models learned these representations based on user - given numeric ratings that each item received ( Mnih and Salakhutdinov , 2008 ; Koren et al . 
, 2009 ) . 
However , ratings , which are values on a single discrete scale , oversimplify user preferences and item characteristics ( Musto et al . 
, 2017 ) . 
The large amount of users and items in a typical online platform consequently results in a highlysparse rating matrix , making it hard to learn accurate representations ( Zheng et al . 
, 2017 ) . 
To alleviate these issues , review texts have instead been utilized to model such representations for subsequent recommendation and rating prediction , and this approach has attracted growing attention in research ( Catherine and Cohen , 2017 ; Zheng et al . 
, 2017 ) . 
The main advantage of reviews as the source of features is that they can cover user opinions ‚Äô multi - faceted substance . 
Because users can explain their reasons underlying their given ratings , reviews contain a large amount of latent information that is both rich and valuable , and that can not be otherwise obtained from ratings alone ( Chen et al . 
, 2018 ; Wang et al . 
, 2019 ) . 
Recently , models that incorporate user reviews have yielded state - of - the - art performances ( Zheng et al . 
, 2017 ; Chen et al . 
, 2018 ) . 
These approaches learn user and item representations by using traditional word embeddings ( e.g. , word2vec , GloVe ) to map each word in the review into its corresponding vector . 
The review is transformed into an embedded matrix before being fed to a convolutional neural network ( CNN ) ( Chen et al . 
, 2018 ) . 
CNNs have been shown to effectively model reviews and have illustrated outstanding results in numerous natural language processing tasks ( Wang et al . 
, 2018a ) . 
Nevertheless , there are drawbacks that most review - based recommender models experience . 
First is the utilization of traditional or mainstream word embeddings to learn review features . 
Their static nature is a hindrance , as each word sense is associated with the same embedding regardless of the context . 
In other words , such embeddings can not identify the dynamic nature of each word ‚Äôs semantics . 
For review - based recommenders , this could be an issue in modeling users and items , which could , in turn , affect recommendation performance ( Pilehvar and Camacho - Collados , 2019 ) . 
Also , once a CNN is fed with the matrix of word embeddings , the word frequency information of contextual fea-143 tures , said to be crucial for modeling reviews , will be lost ( Wang et al . 
, 2018a ) . 
Another drawback is the inherent black - box nature of deep learning - based models that makes the explanations behind every prediction obscure ( Ribeiro et al . 
, 2016 ; Wang et al . 
, 2018b ) . 
The complex architecture of hidden layers has opaqued the models ‚Äô internal decision - making processes ( Peake and Wang , 2018 ) . 
Providing explanations could help persuade users to make decisions and develop trust in a recommender system ( Zhang et al . 
, 2014 ; Ribeiro et al . 
, 2016 ; Costa et al . 
, 2018 ; Peake and Wang , 2018 ) . 
However , this leads us to a dilemma , i.e. , a trade - off between accuracy and explainability . 
Usually , the most accurate models are inherently complicated , non - transparent , and unexplainable ( Zhang and Chen , 2018 ) . 
The same is also true for explainable and straightforward methods that sacriÔ¨Åce accuracy . 
Formulating models that are both explainable and accurate is a challenging yet critical research agenda for the machine learning community to ensure that we derive beneÔ¨Åts from machine learning fairly and responsibly ( Peake and Wang , 2018 ) . 
In this paper , we propose a unique model : BERT - Based Neural Collaborative Filtering and Fixed - Length Contiguous Tokens Explanation ( BENEFICT ) . 
Our model learns user and item representations simultaneously using two parallel networks . 
To address the Ô¨Årst drawback , we incorporate BERT as a key component in each parallel network . 
BERT affords us to extract more meaningful , contextualized features adaptable to arbitrary contexts ; such features can not be derived from mainstream word embeddings ( Pilehvar and CamachoCollados , 2019 ; Zakbik et al . 
, 2019 ) . 
BERT can also retain the word frequency information that makes CNN an unnecessary component of our model . 
Once user and item representations are learned , they are concatenated together in a shared hidden space before being Ô¨Ånally fed to an optimal stack of multilayer perceptron ( MLP ) layers that serve as BENEFICT ‚Äôs interaction function . 
To address the second drawback , we introduce a novel component in our model that integrates BERT ‚Äôs self - attention and an implementation of the Ô¨Åxed - length maximum subarray problem ( MSP ) , which is considered to be a classic computer science problem . 
BERT applies self - attention in each encoder layer that consequently produces selfattention weights for each token . 
These are passedto the successive encoder layers through feedforward networks . 
We argue that these self - attention weights can be the basis for explaining rating predictions . 
Based on this premise , MSP then selects a segment or subarray of consecutive tokens that has the maximum possible sum of self - attention weights . 
1.1 Contributions Our work aims to Ô¨Åll the research gap by implementing a solution that is both accurate and explainable . 
We propose a novel model that uniquely integrates three vital elements , i.e. , BERT , MLP , and MSP , to derive review features , model useritem interactions , and produce possible explanations . 
To the best of our knowledge , BENEFICT is one of the Ô¨Årst review - based recommender models to utilize BERT for neural CF . 
Also , to the extent of our knowledge , BENEFICT is one of the Ô¨Årst models to repurpose a portion of the Neural Collaborative Filtering ( NCF ) framework ( He et al . 
, 2017 ) as the user - item interaction function for review - based , explicit CF . 
Moreover , our experiments have demonstrated that our model achieves better rating prediction results than the other stateof - the - art recommender models . 
2 Related Work and Concepts Designing a CF model involves two crucial steps : learning user and item representations and modeling user - item interactions based on those representations ( He et al . 
, 2018 ) . 
Before the advancements provided by neural networks , matrix factorization ( MF ) was the dominant model representing users and items as vectors of latent factors ( called embeddings ) and models user - item interactions using the inner product operation . 
The said operation leads to poor performance because it is sub - optimal for learning rich yet complicated patterns from realworld data ( He et al . 
, 2018 ) . 
To address this scenario , neural networks ( NN ) have been integrated into recommender architectures . 
One of the initial works that have laid the foundation in employing NN for CF is NCF ( He et al . 
, 2017 ) . 
Their framework , originally implemented for rating - based , implicit CF , learns non - linear interactions between users and items by employing MLP layers as their interaction function , granting it a high degree of non - linearity and Ô¨Çexibility to learn meaningful interactions . 
Two common designs have emerged when it comes to leveraging MLP layers : placing144 an MLP above either the concatenated user - item embeddings ( He et al . 
, 2017 ; Bai et al . 
, 2017 ) or the element - wise product of user and item embeddings ( Zhang et al . 
, 2017 ; Wang et al . 
, 2017 ) . 
As far as rating prediction is concerned , two notable recommender models have yielded signiÔ¨Åcant state - of - the - art prediction performances . 
DeepCoNN is the Ô¨Årst deep model that represents users and items from reviews jointly ( Zheng et al . 
, 2017 ) . 
It consists of two parallel , CNN - powered networks . 
One network learns user behavior by examining all reviews that he has written , and the other network models item properties by exploring all reviews that it has received . 
A shared layer connects these two networks , and factorization machines capture user - item interactions . 
The second model is NARRE , which shares certain similarities with DeepCoNN . 
NARRE is also composed of two parallel networks for user and item modeling with respective CNNs to process reviews ( Chen et al . 
, 2018 ) . 
Rather than concatenating reviews to one long sequence the same way that DeepCoNN does , their model introduces an attention mechanism that learns review - level usefulness in the form of attention weights . 
These weights are integrated into user and item representations to enhance the embedding quality and the subsequent prediction accuracy . 
Both DeepCoNN and NARRE employ traditional word embeddings . 
Other relevant studies have claimed to provide explanations for recommendations such as EFM ( Zhang et al . 
, 2014 ) , sCVR ( Ren et al . 
, 2017 ) , and TriRank ( He et al . 
, 2015 ) . 
These models initially extract aspects and opinions by performing phraselevel sentiment analysis on reviews . 
Afterward , they generate feature - level explanations according to product features that correspond to user interests ( Chen et al . 
, 2018 ) . 
However , these models have some limitations ; manual preprocessing is required for sentiment analysis and feature extraction , and the explanations are simple extraction of words or phrases from the review text ( Zhang et al . 
, 2014 ; Ren et al . 
, 2017 ) . 
This also has the unintended effect of distorting the reviews ‚Äô original meaning ( Ribeiro et al . 
, 2016 ; Chen et al . 
, 2018 ) . 
Another limitation is that textual similarity is solely based on lexical similarity ; this implies that semantic meaning is ignored ( Zheng et al . 
, 2017 ; Chen et al . 
, 2018).3 Methodology BENEFICT , as illustrated in Figure 1 , has two parallel networks to model user and item embeddings that both utilize BERT . 
Hereafter , we will only illustrate the user modeling process because the same is also observed for item modeling , with their inputs as the only difference . 
3.1 Input Layer and BERT Encoding Given an input set of user - written reviews Vu= { Vu1,Vu2, ... ,V uj}wherejis the total number of reviews from user u , Vuis fed to a pre - trained BERT BASE model to encode the reviews and obtain their respective contextualized representations . 
BERT BASE consists of 12 encoder layers and 12 self - attention heads ( Devlin et al . 
, 2018 ) . 
It also has a hidden size of 768 , which we will directly utilize later as the Ô¨Åxed embedding dimension . 
Furthermore , BERT requires every review to follow a particular format . 
For this purpose , the model applies WordPiece tokenization to the review ‚Äôs input sequence ( Wu et al . 
, 2016 ) . 
The format is comprised of token embeddings , segment embeddings , position embeddings , and padding masks . 
Because rating prediction is not a sentence pairing task , BERT takes each review as a single segment of contiguous text . 
Typically , BERT supports a maximum sequence length of 512 tokens . 
In this study , we use a shorter length of 256 tokens to save substantial memory . 
As such , each input sequence is truncated or padded accordingly . 
The newly - formatted input sequence then passes through a stack of Transformer encoders to obtain the contextualized representations of reviews : h[CLS ] , u={h[CLS ] , u1,h[CLS ] , u2, ... ,h [ CLS ] , uj } , whereh[CLS ] , u‚ààRj√ó768 . 
We utilize the hidden state of the special [ CLS ] token to serve as the review ‚Äôs aggregate sequence representation or pooled contextualized embedding ( Devlin et al . 
, 2018 ) . 
In theory , any encoder layer may be selected to provide the hidden state of [ CLS ] as the review ‚Äôs representation . 
We select the twelfth layer for our approach ; prior studies have illustrated that its predictive capability is the best among the other layers ( Sun et al . 
, 2019 ) . 
3.2 Embedding Generation , Multilayer Perceptron , and Prediction The user embedding ( user feature vector ) Pu‚àà R1√ó768is obtained by calculating the average of the [ CLS ] representations of the reviews written by145 useru , given by the formula below . 
Similarly , the item embedding ( item feature vector ) Qi‚ààR1√ó768 can be generated from the item modeling network . 
Pu=1 jj / summationdisplay t=1h[CLS ] , ut ( 1 ) Furthermore , the purpose of incorporating an MLP is to learn the interactions between user and item representations and to model the CF effect , which will not be properly covered by solely using vector concatenation or element - wise product ( He et al . 
, 2017 ) . 
Adding a certain number of hidden layers on top of the concatenated user - item embedding provides further Ô¨Çexibility and non - linearity . 
Formally , the MLP component of BENEFICT is deÔ¨Åned as follows : h0=/bracketleftbig Pu , Qi / bracketrightbigT h1 = ReLU ( W1h0+b1 ) hL = ReLU ( WLhL‚àí1+bL ) ÀÜRui = WL+1hL+bL+1(2 ) whereh0‚ààR1√ó1536is the concatenated user - item embedding in the shared hidden space ; hLrepresents theL - th MLP layer ; WLandbLpertain to the weight matrix and bias vector of the L - th layer , respectively ; and ÀÜRuidenotes the predicted rating that userugives to item i. For the activation function of the MLP layers , we choose the rectiÔ¨Åed linear unit ( ReLU ) , which generally yields better performance than other activation functions such as tanh and sigmoid ( Glorot et al . 
, 2011 ; He et al . 
, 2016 , 2017 ) . 
Concerning the structure , our model ‚Äôs MLP component follows a tower pattern where the bottom layer is the widest , and every subsequent top layer has a smaller number of neurons . 
The rationale behind this is that the MLP can learn more abstractive data features by utilizing fewer hidden units for the top layers ( He et al . 
, 2016 ) . 
In our implementation for a three - layered MLP , the number of neurons from the bottom layer to the top layer follows this pattern : 1536 ( concatenated embedding ) ‚Üí768 ( MLP layer 1)‚Üí384 ( MLP layer 2 ) ‚Üí192 ( MLP layer 3)‚Üí1 ( prediction layer ) 3.3 Learning In training the model , the loss function is the mean squared error ( MSE ) given by this formula : MSE = 1 |Tr|/summationdisplay u , i‚ààTr(Rui‚àíÀÜRui)2(3)whereTrrefers to the training samples or instances , andRuiis the ground - truth rating given by user u to itemi . 
Moreover , we employ the Adaptive Moment Estimation with weight decay or AdamW ( Loshchilov and Hutter , 2018 ) to optimize the loss function . 
Based on the original Adam optimizer , AdamW also leverages the power of adaptive learning rates during training . 
This makes the selection of a proper learning rate less cumbersome that consequently leads to faster convergence ( Chen et al . 
, 2018 ) . 
Unlike Adam , AdamW implements a weight decay Ô¨Åx , a regularization technique that prevents weights from growing too huge and is proven to yield better training loss and generalization error ( Loshchilov and Hutter , 2018 ) . 
3.4 Explanation Generation The stack of BERT ‚Äôs Transformer encoders also provides sets of self - attention weights that a token gives to every token found in the review text . 
We are particularly interested in the attention that [ CLS ] gives to each review token using the twelfth layer ‚Äôs multiple attention heads . 
Given an input sequence of tokens Fujproduced by WordPiece tokenization from review Vuj , a set of attention weights is represented as : Œ±[CLS ] , uj={Œ±k 1(Fuj),Œ±k 2(Fuj), ... ,Œ±k g(Fuj ) } ( 4 ) wherekis the speciÔ¨Åc attention head in a particular encoder layer , and Œ±k gis the attention that [ CLS ] gives to the g - th WordPiece token over the input sequenceFuj . 
There are 12 attention heads in an encoder layer which translate to 12 different attention weights that each token receives from the [ CLS ] token . 
For a given token g , the following formula is applied to compress the weights into a single value : ComAtt g=12 / summationdisplay k=1Œ±k g(Fuj ) ( 5 ) We then reformulate the task of generating explanations as a Ô¨Åxed - length MSP . 
In its vanilla sense , MSP selects a segment of consecutive array elements ( i.e. , a contiguous subarray of tokens ) that has the maximum possible sum over all other segments ( Bae , 2007 ) . 
In this paper , we introduce constraintNto the MSP ; Nis a Ô¨Åxed value that pertains to the length of the explanation . 
Formally , the set of compressed attention weights per review is given by the following array : Auj= [ ComAtt 1,ComAtt 2, ... ,ComAtt g](6)146 MLP Layer 1MLP Layer 2MLP Layer L ReLUReLUMSE Loss ... Concatenation ... BERT ... BERTUser / Item Embedding Generation BERT ¬†  Encoding Input Layer User Modeling Item ModelingCompressed AttentionMSP Compressed AttentionMSPExplanation Generation Explanation ¬†  GenerationMultilayer PerceptronFigure 1 : The proposed BENEFICT architecture . 
Dataset # Reviews # Users # Items Toys and Games 167,597 19,412 11,924 Digital Music 64,706 5,541 3,568 Yelp - Dense 159,114 8,919 7,122 Yelp - Sparse 229,907 45,981 11,537 Table 1 : Statistics summary of the datasets . 
The goal is to Ô¨Ånd token indices xandythat maximize : y / summationdisplay t = xAuj[t ] ( 7 ) This is subject to the requirements that 1‚â§x < y‚â§256 and(y‚àíx ) + 1 = N. Finally , the generated explanation for review Vujis represented as : EXP uj = Concat ( Fuj , x , Fuj , x +1, ... ,F uj , y ) ( 8) 4 Experiments In this section , we perform relevant experiments intending to answer the following research questions : RQ1 : Does BENEFICT outperform other stateof - the - art recommender models?RQ2 : What is the optimal conÔ¨Åguration for learning user - item interactions ? RQ3 : Can our model produce explanations acceptable to humans ? 4.1 Datasets and Experimental Settings Table 1 summarizes the four public datasets from different domains used in our study . 
Two of these datasets are Amazon 5 - core1 : Toys and Games , which consists of nearly 168 thousand reviews , and Digital Music , which contains about 65 thousand reviews ( McAuley et al . 
, 2015 ) . 
These datasets are said to be 5 - core wherein users and items have Ô¨Åve reviews each . 
We also utilize Yelp2 , a large - scale dataset for restaurant feedback and ratings . 
We both employ its original , sparse version and its 5core , dense version with about 160 thousand and 230 thousand reviews , respectively . 
The ratings in all datasets are in the range of [ 1 , 5 ] . 
We randomly split each dataset of user - item pairs into training ( 80 % ) , validation ( 10 % ) , and test ( 10 % ) sets . 
In our experiments , we perform an exhaustive grid search over the following hyperparameters : number of epochs [ 1 , 20 ] and number of MLP layers [ 0 , 3 ] . 
The learning rate and weight decay are both set to 1http://jmcauley.ucsd.edu/data/amazon/ 2https://github.com/danielfrg/kaggle-yelp-recruitingcompetition147 Model Toys and GamesDigital MusicYelpDenseYelpSparseAverage DeepCoNN 0.8971 0.8972 1.0311 1.2006 1.0065 NARRE 0.8840 0.8997 1.0312 1.1770 0.9979 BENEFICT 0.8348 0.8750 0.9963 0.9764 0.9206 ‚àÜBENEFICT 5.57 % 2.47 % 3.38 % 17.04 % 7.11 % Table 2 : RMSE comparison of the recommender models . 
The best RMSE values are highlighted in bold . 
The last row shows the improvement gained by BENEFICT against the better performing baseline . 
0.001 . 
Due to memory limitations , the batch size is Ô¨Åxed at 32 . 
We select the model conÔ¨Åguration ( i.e. , a grid point ) with the best root mean square error ( RMSE ) on the validation set . 
We use the test set for evaluating the model ‚Äôs Ô¨Ånal performance . 
4.2 Baselines and Evaluation Metric To validate the effectiveness of BENEFICT , we select two other state - of - the - art models as baselines : ‚Ä¢DeepCoNN ( Zheng et al . 
, 2017 ): It is a deep collaborative neural network model based on two parallel CNNs to learn user and item feature vectors in a joint manner . 
‚Ä¢NARRE ( Chen et al . 
, 2018 ): Similar to DeepCoNN , it is a neural attentional regression model that integrates two parallel CNNs and an attention mechanism to model latent features . 
Afterward , we calculate the RMSE , a widely used metric for rating prediction , to evaluate the models ‚Äô respective performances . 
RMSE = /radicalBigg 1 |Ts|/summationdisplay u , i‚ààTs(Rui‚àíÀÜRui)2(9 ) In the formula , Tsdenotes the test samples or instances of user - item pairs . 
4.3 Prediction Results and Discussion Table 2 reports the RMSE values of BENEFICT and the two baselines , with the last row ( represented by ‚àÜBENEFICT ) indicating the improvement gained by our model compared with the better baseline . 
The results show that BENEFICT consistently outperforms the baselines across all datasets ; our model has an average RMSE score of 0.9206 , as opposed to 1.0065 and 0.9979 for DeepCoNN and NARRE , respectively . 
On average , this has Figure 2 : RMSE comparison of BENEFICT variants using different user - item interaction functions . 
The solid lines pertain to the concatenation - MLP interaction function . 
On the other hand , the broken lines refer to the interaction function based on the element - wise product ( EWP ) and MLP . 
resulted in the improvement gained by BENEFICT of nearly 7 % . 
These results validate our hypothesis that using BERT - derived embeddings and representations , considered to be more semantically meaningful than their traditional counterparts , can signiÔ¨Åcantly improve rating prediction accuracy and that BERT can likewise offset the limitations of mainstream word embeddings and CNN . 
Moreover , the rationale of employing two versions of Yelp is to compare the recommender models ‚Äô performances on both dense and sparse datasets . 
As illustrated in the fourth and Ô¨Åfth columns of Table 2 , both the RMSE values of DeepCoNN and NARRE worsen when they attempt to perform predictions on the original , sparse Yelp . 
For DeepCoNN , from the dense version ‚Äôs RMSE of 1.0311 , it increases to 1.2006 . 
The same is148 Figure 3 : Distribution of the judges ‚Äô given usefulness scores based on US1 . 
also true for NARRE , whose RMSE increases to 1.1770 from 1.0312 . 
Interestingly , BENEFICT produces an entirely different observation ; its RMSE decreases to 0.9764 from 0.9963 . 
Our model ‚Äôs improvement is 17.04 % , greater than ‚àÜBENEFICT for the three other datasets . 
We attribute these Ô¨Åndings to the greater amount of information in Yelp - Sparse that can be successfully utilized by BENEFICT for modeling reviews . 
It should be noted that Yelp - Sparse has nearly 230 thousand reviews , while Yelp - Dense has almost 160 thousand . 
In conclusion , these results provide evidence that our model is best equipped and capable of performing predictions regardless of a dataset ‚Äôs inherent sparsity or density . 
4.3.1 Optimal Interaction Function BENEFICT employs an MLP above the concatenated user - item embeddings in the shared hidden space . 
We compare it against another variant of our model , which utilizes an MLP on top of the element - wise product of user and item representations . 
We examine their performances using a different number of hidden layers [ 0 , 3 ] . 
It should be noted that an MLP with zero layers pertains to the shared hidden space ‚Äôs direct projection to the prediction layer . 
Figure 2 demonstrates that BENEFICT ‚Äôs utilization of concatenation exceeds the element - wise product by a signiÔ¨Åcant margin across all MLP layers and datasets . 
This result veriÔ¨Åes the positive effect of feeding the concatenated features Figure 4 : Distribution of the judges ‚Äô given usefulness scores based on US2 . 
to the MLP to learn user - item interactions . 
Furthermore , consistent with the Ô¨Åndings of He et al . 
( 2017 ) , stacking more layers is indeed beneÔ¨Åcial and effective for neural explicit collaborative Ô¨Åltering as well . 
There appears to be a trend : increasing the hidden layers implies decreasing ( and better ) RMSE values . 
Simply projecting the shared hidden space to the prediction layer is insufÔ¨Åcient and weak , as evidenced by its relatively high RMSE scores . 
On the contrary , using three MLP layers has generally resulted in the lowest RMSE scores . 
The only exception is with the Digital Music dataset wherein utilizing two layers produces the best RMSE value . 
Furthermore , even though the element - wise product is more inferior than concatenation , the former also beneÔ¨Åts from increasing the MLP layers . 
In summary , all these Ô¨Åndings validate the necessity of incorporating the MLP as an integral part of the whole BENEFICT model . 
5 Explainability Study 5.1 Human Assessment of Explanations To validate the helpfulness of BENEFICTproduced explanations in real life , we also generate possible explanations using TF - IDF andTextRank . 
Applying TF - IDF determines which words are more favorable or relevant in a corpus of documents ( Rajaraman and Ullman , 2011 ) . 
To make the assessment fair , we only select words with the topNTF - IDF scores , where the value of Nis the same as the constraint introduced in BENEFICT‚Äôs149 Explanation US Scores TF - IDF : Some ofthe tracks were really quite ... dare I say it , catchy . 
And there was even a Top 30 - friendly single on the album ( ‚Äô Only Time will tell ‚Äô ) . 
But was n‚Äôt this Carl Palmer ‚Äì he ofthe70striple album and seriousdevo teeofclassical percussionist James Blades ? And was n‚Äôt this also Steve Hose ‚Äì he ofanother 70striple album and several serious solo albums . 
And had n‚Äôt John Wetton starred on theseriously serious ‚Äô Red ‚Äô in 74 ? How could the three come together yet produce this Adult - Oriented stadium Rock?Let ‚Äôs not forget Palmer ‚Äôs beginnings in the Crazy World of Arthur Brown and Atomic Rooster . 
Or Wetton‚Äôsbizarre phase with Uriah Heep . 
And Geoff Downes was nominally half of ‚Äô Buggles ‚Äô , whose minimal output was unashamed pop . 
The style of this , Asia ‚Äôs debut album was n‚Äôt a million miles from UK ‚Äôs eponymous LP of 1978 , although it was distinctly more mainstream . 
I like this album , the best of all theAsia output that I ‚Äôve heard . 
I would have preferred the music to be a little more ambitious ; there ‚Äôs a sense in which it ‚Äôs all been concocted to maximise the commercial return , which you could n‚Äôt say ofUK . 
But it ‚Äôs a good , undemand ing listen . 
US1 : 1.5 US2 : 1.5 TextRank : Some of the tracks were really quite ... dare I say it , catchy . 
And there was even a Top 30 - friendly single on the album ( ‚Äô Only Time will tell ‚Äô ) . 
Butwasn‚Äôt thisCarl Palmer ‚Äì heofthe70striple album andseriousdevo teeofclassicalpercussionistJames Blades ? And was n‚Äôt this also Steve Hose .... US1 : 2 US2 : 2 BENEFICT : ..... The style of this , Asia ‚Äôs debut album was n‚Äôt a million miles from UK ‚Äôs eponymous LP of 1978 , although it was distinctly more mainstream . 
I likethisalbum , thebestofalltheAsia outputthatI‚Äôve heard . 
Iwould have preferred the music to be a little more ambitious ; there ‚Äôs a sense in which it ‚Äôs all been concocted to maximise the commercial return , which you could n‚Äôt say of UK ..... US1 : 4 US2 : 4 Table 3 : Sample explanations ( highlighted in yellow ) generated by TF - IDF , TextRank , and BENEFICT from a speciÔ¨Åc user review . 
The second column includes the average judge - given US1 and US2 scores . 
explanation generation module . 
On the other hand , TextRank is a fully unsupervised , graph - based extractive summarization algorithm ( Mihalcea and Tarau , 2004 ) . 
Its goal is to rank entire sentences that comprise a given review text . 
Also , to make the assessment consistent , we only take the top sentence with a length of less than or equal to Nfor each review . 
We then ask two human judges to evaluate a total of 90 explanations , 30 explanations each for TF - IDF , TextRank , and BENEFICT , with N= 20 . 
We instruct them to score each explanation based on the following usefulness statements ( US ) on a Ô¨Åve - point Likert scale , ranging from 1 ( strongly disagree ) to 5 ( strongly agree ) . 
US1 : The explanation captures the essence of the customer ‚Äôs preference ( like or dislike ) in the review . 
US2 : The explanation is helpful for you or any customer to decide to purchase that particular item in the future . 
We further examine the human assessment results by determining the strength of agreement between the two judges . 
This is done by calculatingthe Quadratic Weighted Kappa ( QWK ) statistic . 
It measures inter - rater agreement and is suitable for ordinal or ranked variables . 
The Kappa metric lies on a scale of -1 to 1 , where 1 implies perfect agreement , 0 indicates random agreement , and negative values mean that the agreement is less than chance , such as disagreement . 
SpeciÔ¨Åcally , a coefÔ¨Åcient of 0.01 - 0.20 indicates slight agreement , 0.21 - 0.40 implies fair agreement , 0.41 - 0.60 refers to moderate agreement , 0.61 - 0.80 pertains to substantial agreement , and 0.81 - 0.99 denotes nearly perfect agreement ( Borromeo and Toyama , 2015 ) . 
5.2 Explainability Results and Discussion 5.2.1 Overall Assessment Figure 3 summarizes the judges ‚Äô given scores on their assessment of explanations based on US1 . 
They Ô¨Ånd that nearly 58 % of BENEFICT - derived explanations capture the essence of the customer ‚Äôs preference ( i.e. , those with usefulness scores of either four or Ô¨Åve ) . 
It is followed by TextRank , with almost 52 % of its produced explanations , and TFIDF , with only 1.67 % of its generated explanations . 
With respect to the inter - rater agreement on US1150 in Table 5 , the judges express fair agreement on BENEFICT ( having a Kappa value of 0.2019 ) . 
On the other hand , they slightly agree with each other on both TF - IDF and TextRank , with QWK values of 0.1924 and 0.0625 , respectively . 
As Table 4 indicates , our model has a mean usefulness score of 3.45 , better than TextRank ( 3.26 ) and TF - IDF ( 2.05 ) . 
Figure 4 shows the judges ‚Äô assessment scores based on US2 . 
Interestingly , the judges express that nearly 63 % of the explanations generated by BENEFICT and TextRank are helpful for any future customers . 
Upon including the low - scoring explanations , BENEFICT is still better than TextRank ; the former has a mean usefulness score of 3.61 against the latter ‚Äôs 3.40 . 
Furthermore , the judges moderately agree as far as our model ‚Äôs generated explanation is concerned ( with a Kappa value of 0.4705 ) . 
At the same time , they express less than chance agreement for TextRank ( obtaining a Kappa value of -0.0073 ) . 
This statement means that the large majority of TextRank ‚Äôs high assessment scores come from one judge alone . 
Lastly , the judges observe that only 8.33 % of the explanations from TF - IDF are helpful , with a mean usefulness score of 2.18 and a QWK value of 0.1921 , which implies their slight agreement . 
These results indicate that BENEFICT ‚Äôs explanation generation module can effectively provide useful explanations that capture the essence of the customer ‚Äôs preference and help future customers make purchasing decisions . 
5.2.2 SpeciÔ¨Åc Example Comparison Given an example , we highlight words that serve as the explanations in Table 3 . 
The explanation produced by TF - IDF can capture a few important words , such as unashamed andundemanding . 
However , due to its bag - of - words property , it includes several other unnecessary words that may not contribute to the explanation . 
Therefore , the judges do not Ô¨Ånd it to be helpful . 
Next , the TextRank - generated explanation also does not appear to capture the essence of the user ‚Äôs like or dislike . 
It does not seem useful for customers to decide whether to purchase that item in the future . 
Still , the judges give TextRank higher usefulness scores than TF - IDF , even though the latter captures more adjectives and important words . 
We attribute this to human ‚Äôs natural bias toward less noisy sentences that express complete thoughts . 
Lastly , the BENEFICT - produced explanation con - Method US1 Mean US2 Mean TF - IDF 2.05 2.18 TextRank 3.26 3.40 BENEFICT 3.45 3.61 Table 4 : Mean usefulness scores of explanations assessed by the judges , based on US1 and US2 . 
Method US1 QWK US2 QWK TF - IDF 0.1924 0.1921 TextRank 0.0625 -0.0073 BENEFICT 0.2019 0.4705 Table 5 : The strength of inter - judge agreement for both US1 and US2 given by the QWK values . 
veys a near - complete thought ; take note that it is not a sentence but a segment of contiguous tokens that maximize the sum of attention weights . 
This enables BENEFICT to capture important phrases such as like this album andthe best of all . 
Hence , the judges agree that it captures the essence of the customer ‚Äôs preference and helps customers make purchasing decisions in the future . 
6 Conclusion and Future Work We have successfully implemented a novel recommender model that uniquely integrates BERT , MLP , and MSP . 
BENEFICT ‚Äôs predictive capability is validated by experiments performed on Amazon and Yelp datasets , consistently outperforming other state - of - the - art models . 
Moreover , its explanation generation capability is veriÔ¨Åed by human judges . 
We argue that our work offers an avenue to help bridge the research gap between accuracy and explainability . 
In the future , we will consider incorporating other neural components , such as attention mechanisms , in improving the user - item modeling process . 
We also intend to enhance the expressiveness and the overall quality of the generated explanations . 
Acknowledgment First and foremost , we extend our gratitude to the hardworking anonymous reviewers for their valuable insights and suggestions . 
Likewise , we sincerely thank the judges in our explainability study , Dr. Ria Mae Borromeo and Ms. Verna Banasihan , for their time and participation.151 Abstract By predicting chemical compound structures from their names , we can better comprehend chemical compounds written in text and identify the same chemical compound given different notations for database creation . 
Previous methods have predicted the chemical compound structures from their names and represented them by SimpliÔ¨Åed Molecular Input Line Entry System ( SMILES ) strings . 
However , these methods mainly apply handcrafted rules , and can not predict the structures of chemical compound names not covered by the rules . 
Instead of handcrafted rules , we propose Transformer - based models that predict SMILES strings from chemical compound names . 
We improve the conventional Transformer - based model by introducing two features : ( 1 ) a loss function that constrains the number of atoms of each element in the structure , and ( 2 ) a multi - task learning approach that predicts both SMILES strings and InChI strings ( another string representation of chemical compound structures ) . 
In evaluation experiments , our methods achieved higher Fmeasures than previous rule - based approaches ( Open Parser for Systematic IUPAC Nomenclature and two commercially used products ) , and the conventional Transformer - based model . 
We release the dataset used in this paper as a benchmark for the future research1 . 
1 Introduction Knowledge of chemical substances is necessary for developing new materials and drugs , and for synthesizing products from new materials . 
To utilize such knowledge , researchers have created databases containing the physical property values of chemical substances and the interrelationships among chemical substances . 
It is thought that several billions of chemical compounds exist ( Lahana , 1999 ; Hoffmann and 1http://aiweb.cs.ehime-u.ac.jp/ pred - chem - structGastreich , 2019 ) , but only a portion of these are entered into chemical databases . 
Even PubChem2 , one of the largest databases of chemical compounds , includes the information of only approximately 100 million chemical compounds . 
Moreover , databases for chemical domains are manually maintained , which consumes much time and cost . 
One of the time consuming processes is the integration of the same chemical compounds with different notations . 
For instance , a chemical structure can be derived from partial structures which are given notational variants , or the notation can Ô¨Çuctuate for a given chemical compound ( Watanabe et al . 
, 2019 ) . 
Therefore , a system that automatically predicts a chemical compound structure from its chemical compound names would improve the database creation procedure . 
Structures are most commonly predicted from their notations by rule - based conversion methods ( Lowe et al . 
, 2011 ) . 
Although rule - based conversion can accurately predict the structures of chemical compounds based on systematic nomenclatures such as the International Union of Pure and Applied Chemistry ( IUPAC)3nomenclature , it often fails the structure prediction of chemical compound names that violate these nomenclatures ( e.g. , Synonyms4 ) . 
To improve the low prediction performance of compounds with non - IUPAC names , we propose neural network - based models that predict chemical compound structures represented as SimpliÔ¨Åed Molecular Input Line Entry System ( SMILES ) ( Weininger , 1988 ) strings from chemical compound names categorized as Synonyms5 . 
In this work , we use the Transformer - based sequence2https://pubchem.ncbi.nlm.nih.gov/ 3https://iupac.org 4PubChem ‚Äôs deÔ¨Ånition of chemical compound names other than IUPAC names 5Our Synonyms excludes DATABASE IDs from the original deÔ¨Ånition of Synonyms because DATABASE IDs can be efÔ¨Åciently recognized by rules.154 Name Type Name IUPAC 2 - acetyloxybenzoic acid DATABASE ID ( CAS registry number ) 50 - 78 - 2 ABBREVIATION ASA COMMON aspirin Table 1 : Examples of ‚Äú aspirin ‚Äù representations . 
In this table , ABBREVIATION and COMMON are Synonyms . 
to - sequence neural network model ( Vaswani et al . 
, 2017 ) for machine translation , which achieves a state - of - the - art performance in various tasks among the sequence - to - sequence neural network models such as recurrent neural network - based models . 
To improve the conventional Transformer - based model , we introduce the following two chemicalstructure oriented features : 1.A loss function considering the constraints on the number of atoms of each element in the chemical structure . 
2.A multi - task learning for predicting both SMILES strings and IUPAC International Chemical IdentiÔ¨Åer ( InChI ) ( Heller et al . 
, 2015 ) strings , which are representations for denoting chemical compound structures as strings . 
For our experiments , we created a dataset from PubChem for predicting chemical compound structures represented by SMILES strings from Synonyms . 
The experimental results demonstrate the Transformer - based conversion methods achieve higher F - measures than the existing rule - based methods . 
In addition , our two proposals ( i.e. , constraining the number of atoms of each element and multi - task learning of both SMILES strings and InChI strings ) improve the performance of the conventional Transformer - based method . 
2 Preliminary 2.1 Chemical Compound Names In PubChem , the text names of chemical compounds are represented by three main types of notational categories : IUPAC , DATABASE ID , and Synonyms . 
IUPAC is a systematic nomenclature for chemical compound names . 
DATABASE ID is the unique identiÔ¨Åer of a chemical compound in a database . 
An example is the Chemical Abstracts Service ( CAS)6registry number . 
The Synonyms 6https://www.cas.org/ O OH NH2HO[Chemical Structure ] [ SMILES ] [ InChI]N[C@@H](Cc1ccc(O)cc1)C(=O)O InChI=1S / C9H1 1NO3 / c10 - 8(9(12)13)5 - 6 - 1 - 3 - 7(1 1)4 - 2 - 6 /h1 - 4,8,11H,5,10H2,(H,12,13)/t8-/m0 / s1Figure 1 : Chemical structure of L - tyrosine ( top ) , and its SMILES ( middle ) and InChI ( bottom ) representations naming category in PubChem includes ABBREVIATION and COMMON . 
As an example , Table 1 shows various ‚Äú aspirin ‚Äù representations . 
The IUPAC nomenclature provides a systematic naming under standardized rules , which are easily and accurately converted by rule - based conversion methods ( Lowe et al . 
, 2011 ) ; ( Heller et al . 
, 2015 ) . 
Provided they are registered in the database , DATABASE IDs are easily converted to their corresponding chemical compounds using dictionary - lookup methods . 
However , neither rulebased nor dictionary - based approach can convert chemical compound names that are not covered by the rules or dictionaries . 
Unlike IUPAC and DATABASE ID notations , the naming patterns of Synonyms are complex and widely variable . 
In many cases , the chemical compound names appearing in documents can not be converted by rule - based or dictionary - based approaches . 
Consequently , the prediction performance of chemical compound names is worse in Synonyms than in IUPAC , as shown in section 6.1 . 
In our preliminary experiments , the highest F - measure obtained with an existing tool exceeded 0.96 on IUPAC data , but was reduced to 0.75 on Synonyms data.155 BPE Tokenizer amino gall ane cesium picrate hex dec en olid non an al oxime ¬∑ ¬∑ ¬∑ ( a ) Learning TokenizationCompound Name EncoderSMILES DecoderC = C O C C O C C Cl < s > 2 - ( 2 - C h l o ro eth ox y ) ethyl v inyl ether < s > C = C O C C O C C Cl BPE TokenizerElement - wise & Character - wise Tokenizer 2-(2 - Chloroethoxy)ethylvinyl ether C = COCCOCCCl ( b ) Training ModelCompound Name EncoderSMILES DecoderC = C O C C O C C Cl < s > 2 - ( 2 - C h l o ro eth ox y ) ethyl v inyl ether < s > BPE Tokenizer 2-(2 - Chloroethoxy)ethylvinyl ether ( c ) Inference Figure 2 : Overview of Transformer - based prediction of SMILES strings from chemical compound names 2.2 Representation of Chemical Compound Structures For multi - task learning , we represented chemical compound structures as SMILES strings and InChI strings . 
These two representations are major notations of chemical compound structures . 
We use SMILES strings as the target representation because they are simpler than InChI strings but were sufÔ¨Åciently representative for our purpose ( i.e. , creating a chemical compound database ) . 
The SMILES ( Weininger , 1988 ) notation system was designed for modern chemical information processing . 
Based on the principles of molecular graph theory , SMILES allows rigorous structure speciÔ¨Åcation using a very small and natural grammar . 
SMILES strings are composed of atoms and symbols representing their bonds , branches , rings , and other structural features , assembled into a linear expression of the two - dimensional structure of a molecule . 
An example of a SMILES string is shown in Figure 1 . 
In this work , we used Canonical SMILES because it uniquely determines the correspondence between chemical structures and SMILES strings . 
In the InChI ( Heller et al . 
, 2015 ) representation , the information of a chemical compound structure is represented by Ô¨Åve layers . 
In Figure 1 , the layers are separated by ‚Äú / ‚Äù symbols . 
Each layer adds detailed information to the following layer . 
Because these layers are interrelated , InChI strings are more complex than SMILES strings . 
3 Proposed Methods This section presents our proposed methods , namely , our tokenizer training method and sequence - to - sequence models . 
Let XandTbe a set of chemical compound names and a setof SMILES strings , respectively . 
We deÔ¨Åne a training dataset consisting of nsamples asD= /angbracketleft(X1,T1), ... ,(Xn , Tn)/angbracketright , whereXi‚ààX is a chemical compound name and Ti‚ààT is the SMILES string ofXifor1‚â§i‚â§n . 
Our objective is to learn a mapping function fthat realizes f(Xi)= TifromD. Figure 2 overviews the Transformer - based prediction of SMILES strings from chemical compound names , where < s > is a special symbol denoting the start and end of a sequence . 
Chemical compound names , SMILES , and InChI are long strings without explicit boundaries ( such as white spaces in English text ) . 
Therefore , to convert chemical compound names to SMILES strings , we propose ( a ) training of a tokenizer and ( b ) a Transformer - based approach . 
3.1 Tokenizer Chemical compound names can be tokenized by the Open Parser for Systematic IUPAC Nomenclature ( OPSIN ) ( Lowe et al . 
, 2011 ) tokenizer , a rule - based parser that generates SMILES and InChI strings from chemical compound names ( mainly , from IUPAC names ) . 
However , some chemical compound names , especially Synonyms , can not be tokenized by rule - based tokenizers such as OPSIN . 
In particular , the OPSIN tokenizer is limited to chemical compound names covered by its dictionary and rules ; meanwhile ( as mentioned above ) chemical compound names lack explicit word - boundary markers . 
To overcome these restrictions , we propose a method that trains tokenizers for Synonyms , SMILES , and InChI representations . 
Note that InChI is used in a multi - task learning . 
To eliminate the unknown tokens , our tokenizer learning method is unsupervised and covers a large156 set of chemical compound names . 
The tokenization is performed by byte pair encoding ( BPE ) ( Sennrich et al . 
, 2016)7 . 
The BPE - based tokenizer was learned by fastBPE8 . 
First , the chemical compound names obtained by the OPSIN tokenizer were segmented because fastBPE requires segmented input text . 
By virtue of the newly obtained BPE dictionary , the BPE - based tokenizer can tokenize chemical compound names that can not be handled by the OPSIN tokenizer . 
When tokenizing the SMILES strings , each element ( e.g. , ‚Äú C ‚Äù , ‚Äú O ‚Äù , ‚Äú Cl ‚Äù ) identiÔ¨Åed by regular expressions was regarded as one token . 
The remaining symbols not covered by regular expressions were divided into single characters , each regarded as one token . 
For tokenizing InChI strings , the model was learned on SentencePiece ( Kudo and Richardson , 2018 ) , a unigram - based unsupervised training method for word segmentation . 
Note that InChI strings can not be tokenized by BPE because the segmentations of InChI strings are not preliminarily given . 
3.2 Transformer - based Prediction of SMILES Strings from Chemical Compound Names The Transformer model consists of stacked encoder and decoder layers . 
Based on self - attention , it attends to tokens in the same sequence , i.e. , a single input sequence or a single output sequence . 
The encoder maps an input sequence to a sequence of vector representations . 
From this vector representations , the decoder generates an output sequence . 
The Transformer - based model predicts SMILES strings from chemical compound names , so its input is a chemical compound name and its output is a SMILES string . 
During the learning process , the following objective function is minimized : Lsmiles = ‚àílogP(T|X;Œ∏enc , Œ∏smiles ) , ( 1 ) whereŒ∏encandŒ∏smiles are the parameter sets of the compound name encoder and SMILES decoder , respectively , and X=/angbracketleftx1,x2, ... ,x n / angbracketrightis the word sequence of a chemical compound name segmented by the BPE model . 
T=/angbracketleftt1,t2, ... ,t m / angbracketrightis the 7In preliminary experiments , BPE achieved a higher Fmeasure than SentencePiece ( Kudo and Richardson , 2018 ) . 
Therefore , it was used for tokenizing the chemical compound names . 
8https://github.com/glample/fastBPE ùìõùíÇùíïùíêùíé=1 ùê¥ùëÅ"C"‚àíùë¶1ùëùùëüùëíùëë2+ùëÅ"O"‚àíùë¶3ùëùùëüùëíùëë2+‚ãØ = 1 ùê¥2‚àí2.272 + 1‚àí0.842+‚ãØLoss for ‚Äú C ‚Äù Loss for ‚Äú O‚Äù012 C = O ùíöùüè ùíöùüê ùíöùüë01 C = O01 C = O ùíöùíëùíìùíÜùíÖ= + + ùíöùüí+01 C = O01 C = Oùë¶1ùëùùëüùëíùëëùë¶3ùëùùëüùëíùëë ùê¥=ùëé|"C","O",‚ãØ , " = " ‚àâùê¥ùëÅ"C":2,ùëÅ"O":1Figure 3 : Calculating the constraints on the number of atoms of each element sequence of elements and symbols in the correct SMILES string of X. 3.3 Training with a Constraint on the Number of Atoms To correctly predict the chemical structure from a chemical compound name , the number of atoms of each element included in the chemical structure must be Ô¨Åxed . 
In this subsection , we propose a softmax - based loss function that constrains the number of atoms of each element , that is , we minimize the difference between the numbers of atoms of each element in the predicted and correct SMILES strings . 
The differences are measured by their squared errors . 
The squared errors are computed using the Gumbel softmax ( Jang et al . 
, 2016 ) function , which obtains the probability distribution of the number of atoms of each element in a predicted SMILES string . 
Let œÄi= ( œÄi1,œÄi2, ... ,œÄ i|V| ) be the probability distribution of the i - th output token from the Transformer model . 
Then , yi= ( yi1,yi2, ... ,y i|V|)for thei - th output token with Gumbel softmax is calculated as follows : yij = exp ( ( log(œÄij ) + gij)/œÑ ) /summationtext|V| k=1exp ( ( log(œÄik ) + gik)/œÑ),(2 ) gij=‚àílog(‚àílog(uij ) ) , uij‚àºUniform(0,1 ) , whereVrepresents the vocabulary set of SMILES , andœÑis a hyperparameter of Gumbel softmax . 
The distributionyiapproximates an one - hot vector as œÑ decreases , and a uniform distribution as œÑincreases . 
In this work , œÑwas set to 0.1 . 
Using Equation 2 , the loss function under the157 proposed constraints is given by Latom = 1 |A|/summationdisplay a‚ààA(Na(T)‚àíypred idx(a))2,(3 ) ypred = y1+y2+¬∑¬∑¬∑+ym = ( ypred 1,ypred 2, ... ,ypred |V| ) , whereAis a set of elements , Na(T)is a function that returns the number of atoms of element ain SMILES string T , andidx(a)is a function that returns the index of element ainV. Note that Acontains only elemental symbols , and the other features such as symbols representing bonds are absent . 
More formally , ‚Äú C ‚Äù , ‚Äú O ‚Äù ‚ààA , ‚Äú = ‚Äù , ‚Äú # ‚Äù /‚ààA , andV‚äÉA. Each dimension of ypredis an estimation of the frequency of the corresponding token of the vocabulary Vin the predicted SMILES . 
The proposed constraint calculation uses only the estimation of the elements in V. The frequencies of elements not included in the correct SMILES are set to 0 . 
As an example , Figure 3 shows how the number of atoms of each element is constrained when the correct SMILES string is ‚Äú CC = O ‚Äù . 
As ‚Äú C ‚Äù and ‚Äú O ‚Äù are elements and ‚Äú = ‚Äù is a subsidiary symbol representing a double bond , the proposed constraint function treats the number of atoms of each element ( ‚Äú C ‚Äù and ‚Äú O ‚Äù ) as the error to be minimized , and disregards the ‚Äú = ‚Äù symbol . 
The objective function under the proposed constraints is deÔ¨Åned as follows : Lsmiles + ŒªatomLatom , ( 4 ) whereŒªatom is a hyperparameter that controls the degree of considering Latom . 
3.4 Multi - task Learning for Predicting both SMILES Strings and InChI Strings The same chemical structure is differently represented in a SMILES string and an InChI string . 
Assuming that the models for predicting SMILES and InChI strings compensate each other , we propose a multi - task learning method that shares the encoder of the name - to - SMILES and name - to - InChI conversion models , and trains both models at the same time . 
LetIbe the set of InChI strings . 
We deÔ¨Åne a training dataset consisting of nsamples as ÀúD= /angbracketleft(X1,T1,I1 ) , ... , , ( Xn , Tn , In)/angbracketright , whereXi‚ààX , Ti‚ààT , andIi‚ààIfor1‚â§i‚â§n . 
The objectiveCompound Name EncoderInChI DecoderSMILES DecoderC = C O C C O C C Cl < s>1 S / C 6 H 1 1 Cl O 2/ c 1 - 2- 8- 56- 9 - 4- 3 - 7 / h 2 H , 1 , 3 - 6 H 2 < s > 2 - ( 2 - C h l o ro eth ox y ) ethyl v inyl ether < s > C = C O C C O C C Cl < s>1 S / C 6 H 1 1 Cl O 2/ c 1 - 28- 5 - 6- 9 - 4- 3 - 7 / h 2 H , 1 , 3 - 6 H 2 BPE TokenizerElement - wise & Character - wise TokenizerUnigram Model Tokenizer 2-(2 - Chloroethoxy)ethylvinyl etherC = COCCOCCCl 1S / C6H11ClO2 / c1 - 2 - 85 - 6 - 9 - 4 - 3 - 7 / h2H,1,3 - 6H2 Figure 4 : Overview of multi - task learning for predicting both SMILES strings and InChI strings Split Size Training 5,000,000 Development 1,113 Test 11,194 Table 2 : Sizes of the training , development , and test datasets is to learn a function Àúffrom ÀúD.Àúf(Xi)predicts bothTiandIi . 
SpeciÔ¨Åcally , the proposed multi - task learning minimizes the following objective function : Lsmiles + ŒªinchiLinchi , ( 5 ) Linchi = ‚àílogP(I|X;Œ∏enc , Œ∏inchi ) , whereŒ∏inchi andŒ∏encare parameter sets for the InChI decoder and shared encoder , respectively , andŒªinchi is a hyperparameter that controls the degree of considering Linchi . 
Lsmiles is calculated by Eq . 
1 . 
The method is overviewed in Figure 4 . 
4 Experimental Settings 4.1 Data Set In all experiments , the data comprised a chemical compound name and a correct SMILES string . 
Using the dump data of PubChem9(97 M compound records ) , the chemical compound names were converted to Synonyms associated with each CID10 , and the correct SMILES strings were converted from isomeric SMILES strings11to canon9ftp://ftp.ncbi.nlm.nih.gov/pubchem/ 10PubChem ‚Äôs compound identiÔ¨Åer for a unique chemical structure 11SMILES strings written with isotopic and chiral speciÔ¨Åcations158 method recall precision F - measure Rule - based OPSIN 0.693 0.836 0.758 tool A 0.711 0.797 0.752 tool B 0.653 0.800 0.719 Transformer - based transformer 0.793 0.806 0.799 ( BPE ) atomnum 0.798 0.808 0.803 inchigen 0.810 0.819 0.814 Transformer - based transformer 0.763 0.873 0.814 ( OPSIN - TK + BPE ) atomnum 0.768 0.876 0.818 inchigen 0.779 0.886 0.829 Transformer - based transformer 0.755 0.868 0.808 ( OPSIN - TK ) atomnum 0.757 0.867 0.808 inchigen 0.754 0.869 0.807 Table 3 : Evaluation results of each converter for Synonyms . 
Transformer - based ones are our proposed methods . 
We evaluated the Transformer - based ones with different three tokenizers , BPE , OPSIN - TK+BPE , and OPSIN - TK . 
ical SMILES strings using RDKit12 . 
Note that in PubChem , the Synonyms includes the IUPAC names , common names , and IDs of the compounds in chemical compound databases . 
Here , we used the isomeric SMILES strings because they least overlap with their corresponding CIDs . 
In the multi - task learning , the InChI strings are also associated with CIDs . 
From the dump data , 10,000 CIDs and 100,000 CIDs were randomly selected as the development and test datasets , respectively , and only the two chemical compound names with the longest edit distance were assigned to each CID . 
To create Synonyms in the development and test data , chemical compound names like IDs in the chemical compound databases were removed using manually created regular expressions . 
In the development and test datasets , duplicate chemical compound names with different CIDs were removed13 . 
From the development and test datasets , we removed 820 and 8,241 duplicates , respectively . 
As the training dataset , we selected chemical compound names that were categorized as Synonyms that could be tokenized by the OPSIN tokenizer . 
The size of each dataset is listed in Table 2 . 
4.2 Parameter Settings The hyperparameters of the Transformer model were set as follows : number of stacks in the encoder and decoder layers = 6 , number of heads 12https://github.com/rdkit/rdkit 13The same chemical compound name may have more than one CID.= 8 , embedding dimension = 512 , and dropout probability = 0.1 . 
The loss functions Lsmiles and Linchi were computed using a label - smoothing cross entropy with the smoothing parameter /epsilon1set to 0.1 . 
The learning rate was linearly increased to 0.0005 over the Ô¨Årst 4,000 steps . 
In later steps , it was decreased proportionally to the inverse square root of the step number . 
The optimizer was an Adam ( Kingma and Ba , 2015 ) optimizer with Œ≤1= 0.9 , Œ≤2= 0.98,and / epsilon1= 10‚àí8 . 
The model parameters were updated 300,000 times . 
The hyperparameters Œªatom andŒªinchi for controlling the degree of constraint consideration were set to 0.7 and0.3 , respectively . 
The number of merge operations for the BPE - based tokenizer of chemical compound names was set to 500 . 
The vocabulary size for the tokenizer of InChI strings was set to 1,000 . 
We tuned the hyperparameters for our constraints and subword on the development data . 
To present the results of our Transformer - based models , we averaged the last 10 checkpoints ( saved at 1,000 - step intervals ) of the Transformer models . 
We used beam search with a beam size of 4 and length penalty Œ±= 0.6(Vaswani et al . 
, 2017 ) . 
The maximum output length of an inference was set to 200 . 
5 Experimental Results 5.1 Prediction Performance The results are shown in Table 3 . 
Here , tool A and tool B are two commercially available tools , atomnum indicates the method based on the number of atoms described in section 3.3 , and inchigen denotes the multitask learning method159 Figure 5 : Histogram of Jaccard similarities between incorrect structures generated by inchigen with BPE and their correct structures described in section 3.4 . 
The notations BPE and OPSIN - TK indicate the use of the BPE - based and OPSIN tokenizers , respectively . 
As conÔ¨Årmed in Table 3 , the proposed methods attained higher prediction performance the existing rule - based methods and the conventional Transformer - based model . 
inchigen with BPE showed 0.056 , 0.062 , and 0.095 points higher Fmeasure than OPSIN , tool A , and tool B , respectively . 
The F - measure was further improved by combining the two tokenizers ( see the results of OPSINTK+BPE in Table 3 ) . 
In the OPSIN - TK+BPE method , the Transformer - based method with BPE predicted the structures from chemical compound names that could be tokenized by the OPSIN tokenizer . 
The highest F - measure and precision ( 0.829 and 0.886 , respectively ) were achieved by inchigen with OPSIN - TK+BPE . 
In the Transformer - based models , the OPSIN tokenizer obtained higher precision than the BPEbased tokenizers because approximately 11.5 % ( 1,293/11,194 ) of the chemical compounds in the test set could not be tokenized by OPSIN . 
Consequently , the precision was improved by the reduced number of outputs . 
In contrast , the recall was lower than in the BPE - based tokenizers . 
These results clarify the impact of tokenizer outputs on the recall , precision , and F - measure scores . 
5.2 Error Analysis Most of the predictions in the Transformer - based approach were grammatically correct SMILES strings . 
In this context , ‚Äú grammatically correct‚Äùmeans that the chemical structure can be visualized from the predicted SMILES string using RDKit , and does not require the correct SMILES string of a chemical compound name . 
In particular , inchigen with BPE achieved grammatically correct predictions for 99 % of the test data , 10.6 ‚Äì 17.4 % higher than OPSIN , tool A , and tool B. To evaluate the usefulness of the Transformer - based approach , we also analyzed the proportion of incorrect structure predictions that were grammatically correct SMILES strings but did not match the correct SMILES strings . 
To this end , we measured the Jaccard similarity ( Tanimoto similarity)14between each structure that was incorrectly predicted by inchigen with BPE and the correct structure . 
The Jaccard similarity , a common technique for measuring chemical compound similarities , is deÔ¨Åned as follows : J(X , Y ) = vX¬∑vY |vX+vY|‚àívX¬∑vY , where thevXandvYare binary chemical Ô¨Ångerprints of chemical compounds X and Y , respectively , represented by binary vectors . 
|v|is the L1 norm ofv , andvX¬∑vYis the inner product of vX andvY. Here , a chemical Ô¨Ångerprint expresses a chemical compound structure as a calculable vector . 
A famous type of Ô¨Ångerprint is a series of binary digits ( bits ) that represent the presence or absence of particular partial structures in the chemical compound . 
For example , the Molecular Access System key ( Durant et al . 
, 2002 ) , which is used as the Ô¨Ångerprints in the present evaluation , comprises 166 partial structures of chemical compounds . 
Figure 5 is a histogram of the Jaccard similarity scores obtained in this analysis . 
We Ô¨Ånd that most of the incorrect SMILES strings generated by inchigen with BPE possessed high Jaccard similarities to the correct SMILES strings . 
The average Jaccard similarity was 0.753 . 
An incorrect structure generated by inchigen with BPE is compared with its correct structure in Figure 6 . 
The two structures differed only by whether ethylsulfanylbutane or methanethiol was bonded in the partial structures enclosed by the red ellipses . 
In other words , the two structures are very similar ( Jaccard similarity = 0.76 ) . 
From this result , we observe that even when the proposed method generates an incorrect structure , 14Jaccard similarity , also called the Tanimoto similarity , measures the similarities between pairs of chemical compounds.160 ONH HSO HOO ( a ) Predicted by inchigen with BPE S NH OO OHO   ( b ) Correct Chemical Structure Figure 6 : Example of a chemical structure mistakenly for ‚Äú fmoc - l - buthionine ‚Äù . 
The red - edged ellipses enclose the partial structures that differ between the two chemical structures . 
the outcome does not deviate greatly from the correct structure . 
6 Related Work 6.1 Predicting SMILES Strings from Chemical Compound Names OPSIN ( Lowe et al . 
, 2011 ) is a rule - based parser that generates SMILES strings and InChI strings from chemical compound names ( mainly from IUPAC names ) . 
The OPSIN tokenization approach is based on regular grammar . 
From a tokenized chemical name , an XML parse tree is constructed . 
Stepwise operations on this tree are continued until the structure has been reconstructed from the name . 
The construction is performed on substructures associated with the terms . 
As mentioned earlier , many of chemical compound names described in papers and patents do not comply with IUPAC names or other systematic nomenclatures , so are difÔ¨Åcult to reconstruct using rule - based methods . 
In our preliminary experiments using OPSIN and commercially available tools , the F - measures of predicting the IUPAC names in the dataset ranged from 0.878 to 0.960 . 
However , on the Synonyms dataset , the F - measures fell to 0.719 - 0.758 . 
6.2 Deep Learning methods using SMILES Recently , SMILES strings have been applied to chemical reaction prediction ( Nam and Kim , 2016 ; Schwaller et al . 
, 2019 ) . 
The method of Nam and Kim ( 2016 ) predicts SMILES strings representing products from SMILES strings representing reactants and reagents . 
This method employs a sequence - to - sequence model with an attention mechanism based on a recurrent neural network ( Bahdanau et al . 
, 2015 ) . 
Schwaller et al . 
( 2019 ) achieved higher accuracy than Nam andKim ( 2016 ) ‚Äôs model by applying the conventional Transformer model ( Vaswani et al . 
, 2017 ) . 
Similarly to our study , their models adapt SMILES strings to sequence - to - sequence models , but our target task ( predicting chemical structures from their chemical compound names ) differs from theirs . 
To improve the accuracy of our target task , we will improve the update speed and quality of our chemical compounds databases . 
We also intend to solve other chemistry problems , including chemical reactions , by predictive machine learning . 
7 Conclusions This paper introduced our Transformer - based prediction methods , which convert chemical compound names to SMILES strings trained with the constraint of the number of atoms of each element in the SMILES string . 
We also proposed a multitask learning approach that simultaneously learns the conversions to SMILES strings and InChI strings . 
In an experimental comparison evaluation , our proposed method achieved higher F - measures than the existing methods . 
In future work , we intend to explore various tokenization methods , and further improve the prediction performance . 
We also hope to apply the proposed loss function to multi - task learning . 
Acknowledgments The research results were achieved by the RIKEN AIP - FUJITSU Collaboration Center , Japan . 
Abstract In recent years , pre - trained models have been extensively studied , and several downstream tasks have beneÔ¨Åted from their utilization . 
In this study , we verify the effectiveness of two methods that incorporate a BERT - based pre - trained model developed by Cui et al . 
( 2020 ) into an encoder - decoder model on Chinese grammatical error correction tasks . 
We also analyze the error type and conclude that sentence - level errors are yet to be addressed . 
1 Introduction Grammatical error correction ( GEC ) can be regarded as a sequence - to - sequence task . 
GEC systems receive an erroneous sentence written by a language learner and output the corrected sentence . 
In previous studies that adopted neural models for Chinese GEC ( Ren et al . 
, 2018 ; Zhou et al . 
, 2018 ) , the performance was improved by initializing the models with a distributed word representation , such as Word2Vec ( Mikolov et al . 
, 2013 ) . 
However , in these methods , only the embedding layer of a pretrained model was used to initialize the models . 
In recent years , pre - trained models based on Bidirectional Encoder Representations from Transformers ( BERT ) have been studied extensively ( Devlin et al . 
, 2019 ; Liu et al . 
, 2019 ) , and the performance of many downstream Natural Language Processing ( NLP ) tasks has been dramatically improved by utilizing these pre - trained models . 
To learn existing knowledge of a language , a BERTbased pre - trained model is trained on a large - scale corpus using the encoder of Transformer ( Vaswani et al . 
, 2017 ) . 
Subsequently , for a downstream task , a neural network model is initialized with the weights learned by a pre - trained model that has the same structure and is Ô¨Åne - tuned on training data of ‚àóCurrently at Nomura Research Institute , Ltd. ‚Ä†Currently at Retrieva , Inc. Figure 1 : Two methods for incorporating a pre - trained model into the GEC model . 
the downstream task . 
Using this two - stage method , the performance is expected to improve because downstream tasks are informed by the knowledge learned by the pre - trained model . 
Recent works ( Kaneko et al . 
, 2020 ; Kantor et al . 
, 2019 ) show that BERT helps improve the performance on the English GEC task . 
As the Chinese pre - trained models are developed and released continuously ( Cui et al . 
, 2020 ; Zhang et al . 
, 2019 ) , the Chinese GEC task may also beneÔ¨Åt from using those pre - trained models . 
In this study , as shown in Figure 1 , we develop a Chinese GEC model based on Transformer with a pre - trained model using two methods : Ô¨Årst , by initializing the encoder with the pre - trained model ( BERT - encoder ) ; second , by utilizing the technique proposed by Zhu et al . 
( 2020 ) , which uses the pre - trained model for additional features ( BERTfused ) ; on the Natural Language Processing and Chinese Computing ( NLPCC ) 2018 Grammatical Error Correction shared task test dataset ( Zhao et al . 
, 2018 ) , our single models obtain F0.5scores of 29.76 and 29.94 respectively , which is similar to the performance of ensemble models developed163 by the top team of the shared task . 
Moreover , using a 4 - ensemble model , we obtain an F0.5score of 35.51 , which outperforms the results from the top team by a large margin . 
We annotate the error types of the development data ; the results show that word - level errors dominate all error types and that sentence - level errors remain challenging and require a stronger approach . 
2 Related Work Given the success of the shared tasks on English GEC at the Conference on Natural Language Learning ( CoNLL ) ( Ng et al . 
, 2013 , 2014 ) , a Chinese GEC shared task was performed at the NLPCC 2018 . 
In this task , approximately one million sentences from the language learning website Lang81were used as training data and two thousand sentences from the PKU Chinese Learner Corpus ( Zhao et al . 
, 2018 ) were used as test data . 
Here , we brieÔ¨Çy describe the three methods with the highest performance . 
First , Fu et al . 
( 2018 ) combined a 5 - gram language model - based spell checker with subwordlevel and character - level encoder - decoder models using Transformer to obtain Ô¨Åve types of outputs . 
Then , they re - ranked these outputs using the language model . 
Although they reported a high performance , several models were required , and the combination method was complex . 
Second , Ren et al . 
( 2018 ) utilized a convolutional neural network ( CNN ) , such as in Chollampatt and Ng ( 2018 ) . 
However , because the structure of the CNN is different from that of BERT , it can not be initialized with the weights learned by the BERT . 
Last , Zhao and Wang ( 2020 ) proposed a dynamic masking method that replaces the tokens in the source sentences of the training data with other tokens ( e.g. [ PAD ] token ) . 
They achieved stateof - the - art results on the NLPCC 2018 Grammar Error Correction shared task without using any extra knowledge . 
This is a data augmentation method that can be a supplement for our study . 
3 Methods In the proposed method , we construct a correction model using Transformer , and incorporate a Chinese pre - trained model developed by Cui et al . 
( 2020 ) in two ways as described in the following sections . 
1https://lang-8.com/3.1 Chinese Pre - trained Model We use a BERT - based model as our pre - trained model . 
BERT is mainly trained with a task called Masked Language Model . 
In the Masked Language Model task , some tokens in a sentence are replaced with masked tokens ( [ MASK ] ) and the model has to predict the replaced tokens . 
In this study , we use the Chinese - RoBERTawwm - ext model developed by Cui et al . 
( 2020 ) . 
The main difference between Chinese - RoBERTawwm - ext and the original BERT is that the latter uses whole word masking ( WWM ) to train the model . 
In WWM , when a Chinese character is masked , other Chinese characters that belong to the same word should also be masked . 
3.2 Grammatical Error Correction Model In this study , we use Transformer as the correction model . 
Transformer has shown excellent performance in sequence - to - sequence tasks , such as machine translation , and has been widely adopted in recent studies on English GEC ( Kiyono et al . 
, 2019 ; Junczys - Dowmunt et al . 
, 2018 ) . 
However , a BERT - based pre - trained model only uses the encoder of Transformer ; therefore , it can not be directly applied to sequence - to - sequence tasks that require both an encoder and a decoder , such as GEC . 
Hence , we incorporate the encoderdecoder model with the pre - trained model in two ways as described in the following subsections . 
BERT - encoder We initialize the encoder of Transformer with the parameters learned by Chinese - RoBERTa - wwm - ext ; the decoder is initialized randomly . 
Finally , we Ô¨Åne - tune the initialized model on Chinese GEC data . 
BERT - fused Zhu et al . 
( 2020 ) proposed a method that uses a pre - trained model as the additional features . 
In this method , input sentences are fed into the pre - trained model and representations from the last layer of the pre - trained model are acquired Ô¨Årst . 
Then , the representations will interact with the encoder and decoder by using attention mechanism . 
Kaneko et al . 
( 2020 ) veriÔ¨Åed the effectiveness of this method on English GEC tasks . 
4 Experiments 4.1 Experimental Settings Data In this study , we use the data provided by the NLPCC 2018 Grammatical Error Correction164 shared task . 
We Ô¨Årst segment all sentences into characters because the Chinese pre - trained model we used is character - based . 
In the GEC task , source and target sentences do not tend to change signiÔ¨Åcantly . 
Considering this , we Ô¨Ålter the training data by excluding sentence pairs that meet the following criteria : i ) the source sentence is identical to the target sentence ; ii ) the edit distance between the source sentence and the target sentence is greater than 15 ; iii ) the number of characters of the source sentence or the target sentence exceeds 64 . 
Once the training data were Ô¨Åltered , we obtained 971,318 sentence pairs . 
Because the NLPCC 2018 Grammatical Error Correction shared task did not provide development data , we opted to randomly extract 5,000 sentences from the training data as the development data following Ren et al . 
( 2018 ) . 
The test data consist of 2,000 sentences extracted from the PKU Chinese Learner Corpus . 
According to Zhao et al . 
( 2018 ) , the annotation guidelines follow the minimum edit distance principle ( Nagata and Sakaguchi , 2016 ) , which selects the edit operation that minimizes the edit distance from the original sentence . 
Model We implement the Transformer model using fairseq 0.8.0.2and load the pre - trained model using pytorch transformer 2.2.0.3 We then train the following models based on Transformer . 
Baseline : a plain Transformer model that is initialized randomly without using a pre - trained model . 
BERT - encoder : the correction model introduced in Section 3.2 . 
BERT - fused : the correction model introduced in Section 3.2 . 
We use the implementation provided by Zhu et al . 
( 2020).4 Finally , we train a 4 - ensemble BERT - encoder model and a 4 - ensemble BERT - fused model . 
More details on the training are provided in the appendix A. Evaluation As the evaluation is performed on word - unit , we strip all delimiters from the system output sentences and segment the sentences using 2https://github.com/pytorch/fairseq 3https://github.com/huggingface/ transformers 4https://github.com/bert-nmt/bert-nmt[Our models ] P R F0.5 Baseline 25.14 14.34 21.85 BERT - encoder 32.67 22.19 29.76 BERT - fused 32.11 23.57 29.94 BERT - encoder ( 4 - ensemble ) 41.94 22.02 35.51 BERT - fused ( 4 - ensemble ) 32.20 23.16 29.87 [ SOTA Result ] Zhao and Wang ( 2020 ) 44.36 22.18 36.97 [ NLPCC 2018 ] Fu et al . 
( 2018 ) 35.24 18.64 29.91 Ren et al . 
( 2018 ) 41.73 13.08 29.02 Ren et al . 
( 2018 ) ( 4 - ensemble ) 47.63 12.56 30.57 Table 1 : Experimental results on the NLPCC 2018 Grammatical Error Correction shared task . 
the pkunlp5provided in the NLPCC 2018 Grammatical Error Correction shared task . 
Based on the setup of the NLPCC 2018 Grammatical Error Correction shared task , the evaluation is conducted using MaxMatch ( M2).6 4.2 Evaluation Results Table 1 summarizes the experimental results of our models . 
We run the single models four times , and report the average score . 
For comparison , we also cite the result of the state - of - the - art model ( Zhao and Wang , 2020 ) and the results of the models developed by two teams in the NLPCC 2018 Grammatical Error Correction shared task . 
The performances of BERT - encoder and BERTfused are signiÔ¨Åcantly superior to that of the baseline model and are comparable to those achieved by the two teams in the NLPCC 2018 Grammatical Error Correction shared task , indicating the effectiveness of adopting the pre - trained model . 
The BERT - encoder ( 4 - ensemble ) model yields anF0.5score nearly 5 points higher than the highest - performance model in the NLPCC 2018 Grammatical Error Correction shared task . 
However , there is no improvement for the BERT - fused ( 4 - ensemble ) model compared with the single BERT - fused model . 
We Ô¨Ånd that the performance of the BERT - fused model depends on the warm - up model . 
Compared with Kaneko et al . 
( 2020 ) using a state - of - the - art model to warm - up their BERTfused model , we did not use a warm - up model in this work . 
The performance noticeably drops when we try to warm - up the BERT - fused model from a weak baseline model , therefore , the BERT - fused model may perform better when warmed - up from a 5http://59.108.48.12/lcwm/pkunlp/ downloads / libgrass - ui.tar.gz 6https://github.com/nusnlp/m2scorer165 src ÊåÅ ÊåÅ ÊåÅÂà´ Âà´ Âà´ÊòØÂåó‰∫¨ÔºåÊ≤°Êúâ‚ÄúËá™ÁÑ∂‚ÄùÁöÑÊÑüËßâ „ÄÇ ‰∫∫‰ª¨Âú®‰∏ÄËæàÂ≠êÁªè Áªè ÁªèÈ™å È™å È™åÂæàÂ§ö‰∫ãÊÉÖ „ÄÇ gold Áâπ Áâπ ÁâπÂà´ Âà´ Âà´ÊòØÂåó‰∫¨ÔºåÊ≤°Êúâ‚ÄúËá™ÁÑ∂‚ÄùÁöÑÊÑüËßâ „ÄÇ ‰∫∫‰ª¨Âú®‰∏ÄËæàÂ≠êÁªè Áªè ÁªèÂéÜ ÂéÜ ÂéÜÂæàÂ§ö‰∫ãÊÉÖ „ÄÇ baseline ÊåÅ ÊåÅ ÊåÅÂà´ Âà´ Âà´ÊòØÂåó‰∫¨ÔºåÊ≤°Êúâ‚ÄúËá™ÁÑ∂‚ÄùÁöÑÊÑüËßâ „ÄÇ ‰∫∫‰ª¨Âú®‰∏ÄËæàÂ≠êÁªè Áªè ÁªèÂéÜ ÂéÜ ÂéÜ‰∫ÜÂæàÂ§ö‰∫ãÊÉÖ „ÄÇ BERT - encoder Áâπ Áâπ ÁâπÂà´ Âà´ Âà´ÊòØÂåó‰∫¨ÔºåÊ≤°Êúâ‚ÄúËá™ÁÑ∂‚ÄùÁöÑÊÑüËßâ „ÄÇ ‰∫∫‰ª¨ ‰∏ÄËæàÂ≠ê‰ºö ‰ºö ‰ºöÁªè Áªè ÁªèÂéÜ ÂéÜ ÂéÜÂæàÂ§ö‰∫ãÊÉÖ „ÄÇ Translation Especially in Beijing , there is no natural feeling . 
People experience many things in their lifetime . 
Table 2 : Source sentence , gold edit , and output of our models . 
Error TypeNumber of errorsExamples B 9ÊúÄÂêéÔºåË¶ÅÂÖ≥‰∏ª{ÂÖ≥ ÂÖ≥ ÂÖ≥Ê≥® Ê≥® Ê≥®}‰∏Ä‰∫õÂÖ≥‰∫éÂ§©Ê∞îÈ¢ÑÊä•ÁöÑÊñ∞Èóª „ÄÇ ( Finally , pay attention to some weather forecast news . 
) CC 35Êúâ‰∏ÄÂ§©Êôö‰∏ä ‰ªñ ‰∏ã ‰∫Ü ÂÜ≥ÂÆö{ÂÜ≥ ÂÜ≥ ÂÜ≥ÂøÉ ÂøÉ ÂøÉ}ÂêëÂØå‰∏ΩÂ†ÇÁöá ÁöÑÂÆ´ÊÆøÈáåËµ∞ÔºåÂÅ∑ÂÅ∑ÁöÑ{Âú∞ Âú∞ Âú∞}ËøõÂÖ• ÂÆ´ÂÜÖ„ÄÇ(One night he decided to walk to the magniÔ¨Åcent palace , and sneaked in it secretly . 
) CQ 30Âú®‰∏äÊµ∑ÊàëÊÄªÊòØ‰ΩèNONE{Âú® Âú® Âú®}‰∏ÄÂÆ∂ÁâπÂÆöNONE{ÁöÑ ÁöÑ ÁöÑ}ÈÖíÂ∫ó „ÄÇ ( I always stay in the same hotel in Shanghai . 
) CD 21 ÊàëÂæàÂñúÊ¨¢Âøµ{NONE}ËØªÂ∞èËØ¥ . 
( I like to read novels . 
) CJ 35 . 
. 
. 
. 
. 
.‰ΩÜÊòØÂêåÊó∂‰πüÂØπÁéØÂ¢ÉÈóÆÈ¢ò{NONE}Êó•Áõä‰∏•ÈáçÈÄ†Êàê‰∫Ü{ÈÄ† ÈÄ† ÈÄ†Êàê Êàê Êàê‰∫Ü ‰∫Ü ‰∫ÜÊó• Êó• Êó•Áõä Áõä Áõä‰∏• ‰∏• ‰∏•Èáç Èáç ÈáçÁöÑ ÁöÑ ÁöÑ } Á©∫Ê∞î Ê±°ÊüìÈóÆÈ¢ò„ÄÇ(But on the meanwhile , it also aggravated the problem of air pollution . 
) Table 3 : Examples of each error type . 
The underlined tokens are detected errors that should be replaced with the tokens in braces . 
TypeDetection Correction P RF0.5 P RF0.5 BERT - encoder B 80.0 55.6 73.5 80.0 55.6 73.5 CC 62.5 31.4 52.2 43.8 20.0 35.4 CQ 65.0 43.3 59.1 45.0 30.0 40.9 CD 58.3 28.6 48.3 50.0 28.6 43.5 CJ 56.5 42.9 53.1 4.3 2.9 3.9 BERT - fused B 80.0 44.4 69.0 80.0 44.4 69.0 CC 61.9 42.9 56.9 38.1 22.9 33.6 CQ 69.0 63.3 67.8 44.8 46.7 45.2 CD 71.4 42.9 63.0 57.1 38.1 51.9 CJ 63.2 34.3 54.1 15.8 8.6 13.5 Table 4 : Detection and correction performance of BERT - encoder and BERT - fused models on each type of error . 
stronger model ( e.g. , the model proposed by Zhao and Wang ( 2020 ) ) . 
For the state - of - the - art result achieved by Zhao and Wang ( 2020 ) , both the precision and the recall are comparatively high , and they therefore obtain the best F0.5score . 
Additionally , the precision of the models that used a pre - trained model is lower than that of the models proposed by the two teams ; conversely , the recall is signiÔ¨Åcantly higher . 
5 Discussion Case Analysis Table 2 shows the sample outputs . 
In the Ô¨Årst example , the spelling error ÊåÅÂà´is accurately corrected to ÁâπÂà´(which means especially ) by the proposed model , whereas it is not corrected by the baseline model . 
Hence , it appearsthat the proposed model captures context more efÔ¨Åciently by using the pre - trained model through the WWM strategy . 
In the second example , the output of the proposed model is more Ô¨Çuent , although the correction made by the proposed model is different from the gold edit . 
The proposed model not only changed the wrong word ÁªèÈ™å(which usually means the noun experience ) toÁªèÂéÜ(which usually means the verb experience ) , but also added a new word ‰ºö ( would , could ) ; this addition makes the sentence more Ô¨Çuent . 
It appears that the proposed model can implement additional changes to the source sentence because the pre - trained model is trained with a large - scale corpus . 
However , this type of change may affect the precision because the gold edit in this dataset followed the principle of minimum edit distance ( Zhao et al . 
, 2018 ) . 
Error Type Analysis To understand the error distribution of Chinese GEC , we annotate 100 sentences of development data and obtain 130 errors ( one sentence may contain more than one error ) . 
We refer to the annotation of the HSK learner corpus7and adopt Ô¨Åve categories of error : B , CC , CQ , CD , and CJ . 
B denotes character - level errors , which are mainly spelling and punctuation errors . 
CC , CQ , and CD are word - level errors , which are word selection , missed word , and redundant word errors , respectively . 
CJ denotes sentence - level errors which contain several complex errors , such as word order and lack of subject errors . 
Several 7http://hsk.blcu.edu.cn/166 examples are presented in Table 3 . 
Based on the number of errors , it is evident that word - level errors ( CC , CQ , and CD ) are the most frequent . 
Table 4 lists the detection and correction results of the BERT - encoder and BERT - fused models for each error type . 
The two models perform poorly on sentence - level errors ( CJ ) , which often involve sentence reconstructions , demonstrating that this is a difÔ¨Åcult task . 
For character - level errors ( B ) , the models achieve better performance than for other error types . 
Compared with the correction performance , the systems indicate moderate detection performance , demonstrating that the systems address error positions appropriately . 
With respect to the difference in performance of the two systems on each error type , we can conclude that BERTencoder performs better on character - level errors ( B ) , and BERT - fused performs better on other error types . 
6 Conclusion In this study , we incorporated a pre - trained model into an encoder - decoder model using two methods on Chinese GEC tasks . 
The experimental results demonstrate the usefulness of the BERT - based pretrained model in the Chinese GEC task . 
Additionally , our error type analysis showed that sentencelevel errors remain to be addressed . 
Acknowledgments This work has been partly supported by the programs of the Grant - in - Aid for ScientiÔ¨Åc Research from the Japan Society for the Promotion of Science ( JSPS KAKENHI ) Grant Numbers 19K12099 and 19KK0286 . 
A Appendices Table 5 shows the training details for each model . 
Baseline Architecture Encoder ( 12 - layer ) , Decoder ( 12 - layer ) Learning rate 1√ó10‚àí5 Batch size 32 Optimizer Adam ( Œ≤1= 0.9,Œ≤2= 0.999,/epsilon1= 1√ó10‚àí8 ) Max epochs 20 Loss function cross - entropy Dropout 0.1 BERT - encoder Architecture Encoder ( 12 - layer ) , Decoder ( 12 - layer ) Learning rate 3√ó10‚àí5 Batch size 32 Optimizer Adam ( Œ≤1= 0.9,Œ≤2= 0.999,/epsilon1= 1√ó10‚àí8 ) Max epochs 20 Loss function cross - entropy Dropout 0.1 BERT - fused Architecture Transformer ( big ) Learning rate 3√ó10‚àí5 Batch size 32 Optimizer Adam ( Œ≤1= 0.9,Œ≤2= 0.98,/epsilon1= 1√ó10‚àí8 ) Max epochs 20 Loss function label smoothed cross - entropy ( /epsilon1ls= 0.1 ) Dropout 0.3 Table 5 : Training details for each model.168 Proceedings of the 1st Conference of the Asia - PaciÔ¨Åc Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing , pages 169‚Äì180 December 4 - 7 , 2020 . 
¬© 2020 Association for Computational Linguistics Neural Gibbs Sampling for Joint Event Argument Extraction Xiaozhi Wang1‚àó , Shengyu Jia3‚àó , Xu Han1 , Zhiyuan Liu1,2‚Ä† , Juanzi Li1,2,Peng Li4,Jie Zhou4 1Department of Computer Science and Technology , BNRist ; 2KIRC , Institute for ArtiÔ¨Åcial Intelligence ; 3Department of Electrical Engineering , Tsinghua University , Beijing , 100084 , China 4Pattern Recognition Center , WeChat AI , Tencent Inc , China { wangxz20 , jsy20 , hanxu17 } @mails.tsinghua.edu.cn Abstract Event Argument Extraction ( EAE ) aims at predicting event argument roles of entities in text , which is a crucial subtask and bottleneck of event extraction . 
Existing EAE methods either extract each event argument roles independently or sequentially , which can not adequately model the joint probability distribution among event arguments and their roles . 
In this paper , we propose a Bayesian model named Neural Gibbs Sampling ( NGS ) to jointly extract event arguments . 
SpeciÔ¨Åcally , we train two neural networks to model the prior distribution and conditional distribution over event arguments respectively and then use Gibbs sampling to approximate the joint distribution with the learned distributions . 
For overcoming the shortcoming of the high complexity of the original Gibbs sampling algorithm , we further apply simulated annealing to efÔ¨Åciently estimate the joint probability distribution over event arguments and make predictions . 
We conduct experiments on the two widely - used benchmark datasets ACE 2005 and TAC KBP 2016 . 
The Experimental results show that our NGS model can achieve comparable results to existing state - of - the - art EAE methods . 
The source code can be obtained from https:// github.com/THU-KEG/NGS . 
1 Introduction Event argument extraction ( EAE ) is a crucial subtask of Event Extraction , which aims at predicting entities and their event argument roles in event mentions . 
For instance , given the sentence ‚Äú Fox ‚Äôs stock price rises after the acquisition of its entertainment businesses by Disney ‚Äù , the event detection ( ED ) model will Ô¨Årst identify the trigger word ‚Äú acquisition ‚Äù triggering a Transfer - Ownership event . 
Then , with the trigger word and event type , ‚àóindicates equal contribution ‚Ä†Corresponding author : Z.Liu ( liuzy@tsinghua.edu.cn )         's stock price rises after the acquisition of its                                            by             .Transfer - OwnershipFoxentertainment businessesDisneyBuyerSellerArtifactTextEventArgument RoleEvent TypeacquisitionFoxTrigger WordArgumentFigure 1 : An example of event extraction , including event detection and event argument extraction . 
the EAE model is required to identify that ‚Äú Fox ‚Äù and ‚Äú Disney ‚Äù are event arguments whose roles are ‚Äú Seller ‚Äù and ‚Äú Buyer ‚Äù respectively . 
As ED is well - studied in recent years ( Liu et al . 
, 2018a ; Nguyen and Grishman , 2018 ; Zhao et al . 
, 2018 ; Wang et al . 
, 2019a ) , EAE becomes the bottleneck and has drawn growing attention . 
As EAE is the bottleneck of event extraction , especially is also important for various NLP applications ( Yang et al . 
, 2003 ; Basile et al . 
, 2014 ; Cheng and Erk , 2018 ) , intensive efforts have already been devoted to designing effective EAE systems . 
The early feature - based methods ( Patwardhan and Riloff , 2009 ; Gupta and Ji , 2009 ) manually design sophisticated features and heuristic rules to extract event arguments . 
As the development of neural networks , various neural methods adopt convolutional ( Chen et al . 
, 2015 ) or recurrent ( Nguyen et al . 
, 2016 ) neural networks to automatically represent sentence semantics with lowdimensional vectors , and independently determine argument roles with the vectors . 
Recently , some advanced techniques have also been adopted to further enhance the performance of EAE models , such as zero - shot learning ( Huang et al . 
, 2018 ) , multimodal integration ( Zhang et al . 
, 2017 ) and weak supervision ( Chen et al . 
, 2017 ) . 
However , above - mentioned methods do not model the correlation among event arguments in169 event mentions . 
As shown in Figure 1 , all event arguments are correlated with each other . 
It is more likely to see a ‚Äú Seller ‚Äù when you have seen a ‚Äú Buyer ‚Äù and an ‚Äú Artifact ‚Äù in event mentions , and vice versa . 
Formally , with xidenoting the random variable of the i - th event argument candidate , the required probability distribution for EAE isP(x1,x2, ... ,x n|o ) , whereois the observation from sentence semantics of event mentions . 
The existing methods which independently extract event arguments solely model P(xi|o ) , totally ignoring the correlation among event arguments , which may lead models to trapping in a local optimum . 
Recently , some proactive works view EAE as a sequence labeling problem ( Yang and Mitchell , 2016 ; Nguyen et al . 
, 2016 ; Zeng et al . 
, 2018 ) and adopt conditional random Ô¨Åeld ( CRF ) with the Viterbi algorithm ( Rabiner , 1989 ) to solve the problem . 
These explorations consider the correlation of event arguments unintentionally . 
Yet limited by the Markov property , their linear - chain CRF only considers the correlation between two adjacent event arguments in the sequence and Ô¨Ånds a maximum likelihood path to model the joint distribution , i.e , these sequence models can not adequately handle the complex situation that each event argument is correlated with each other in event mentions , just like the example shown in Figure 1 . 
To adequately model the genuine joint distributionP(x1,x2, ... ,x n|o)rather than / producttextn iP(xi|o ) for EAE , we propose a Bayesian method named Neural Gibbs Sampling ( NGS ) inspired by previous work ( Finkel et al . 
, 2005 ; Sun et al . 
, 2014 ) . 
Gibbs sampling ( Geman and Geman , 1987 ) is a Markov Chain Monte Carlo ( MCMC ) algorithm , which deÔ¨Ånes a Markov chain in the space of possible variable assignments whose stationary distribution is the desired joint distribution . 
Then , a Monte Carlo method is adopted to sample a sequence of observations , and the sampled sequence can be used to approximate the joint distribution . 
More speciÔ¨Åcally , for NGS , we Ô¨Årst adopt a neural network to model the prior distribution Pp(xi|o)and independently predict an argument role for each event argument candidate to get an initial state for the random variable sequence x1,x2, ... ,x n , which is similar to the previous methods . 
Then , we train a special neural network to model the conditional probability distribution Pc(xi|x1,x2, ... ,x i‚àí1,xi+1, ... ,x n , o)and iteratively change the sequence state by this conditionaldistribution . 
Intuitively , the network modeling the conditional probability distribution aims to predict unknown argument roles based on both sentence semantics and some known argument roles . 
After enough steps , the state of the sequence will accurately follow the posterior joint distribution P(x1,x2, ... ,x n|o ) , and the most frequent state in history will be the best result of EAE . 
Considering that it will take many steps to accurately estimate the shape of the joint distribution and each step uses neural networks for inference , it is time - consuming and impractical . 
Due to what we want for EAE is the max - likelihood state of the argument roles , we follow Geman and Geman ( 1987 ) and adopt simulated annealing ( Kirkpatrick et al . 
, 1983 ) to efÔ¨Åciently Ô¨Ånd the max - likelihood state based on the Gibbs sampling . 
To conclude , our main contributions can be summarized as follows : ( 1 ) Our NGS method combines both the advantages of neural networks and the Gibbs sampling method . 
The neural networks have shown their strong ability to Ô¨Åt a distribution from data . 
Gibbs sampling has remarkable advantages in performing Bayesian inference and modeling the complex correlation among event arguments . 
( 2 ) Considering the shortcoming of high complexity of the original Gibbs sampling algorithm , we further apply simulated annealing to efÔ¨Åciently estimate the joint probability distribution and Ô¨Ånd the max - likelihood state for NGS . 
( 3 ) Experimental results on the widely - used benchmark datasets ACE 2005 and TAC KBP 2016 show that our NGS works well to consider the correlation among event arguments and achieves the state - of - the - art results . 
The experiments also show that the simulated annealing method can signiÔ¨Åcantly improve the convergence speed and the stability of Gibbs sampling , which demonstrate that our NGS is both effective and efÔ¨Åcient . 
2 Related Work Event Extraction ( EE ) aims to extract structured information from plain text , which is a challenging task in the Ô¨Åeld of information extraction . 
EE consists of two subtasks , one is event detection ( ED ) to detect words triggering events and identify event types , the other is event argument extraction ( EAE ) to extract argument entities in event mentions and identify event argument roles . 
As EE is important and beneÔ¨Åcial for various downstream170 Gibbs SamplingPrior Neural ModelPrior Distribution ‚Ä¶ Pp(xi|o)Prior StateInitialization x2xn ‚Ä¶ x1 ConditionalNeural ModelConditionalDistribution ‚Ä¶ SamplingProcess .x(t)1x(t)2x(t)nx(t)i.t step statet+1 step statex(t+1)i.x(t)1x(t)2x(t)n . 
Pc(x(t+1)i|X(t) i , o)i ‚á† max(Pc(x(t+1)i|X(t) i , o))1 / cPnj=1max(Pc(x(t+1)j|X(t) j , o))1 / cSimulated Annealingdecreasec        's stock price rises after the acquisition of its entertainment businesses by             .Transfer - OwnershipFoxDisneyBuyerSeller ArtifactFox 's stock price rises after the acquisition of its entertainment businesses by Disney . 
TextResult ConvergeInitializationFigure 2 : Overall framework of our Neural Gibbs Sampling model . 
NLP tasks , e.g. , question answering ( Yang et al . 
, 2003 ) , information retrieval ( Basile et al . 
, 2014 ) , and reading comprehension ( Cheng and Erk , 2018 ) , it has attracted wide attentions recently . 
ED has been well - studied by the previous works due to its simple and clear deÔ¨Ånition , including feature - based and rule - based methods ( Ahn , 2006 ; Ji and Grishman , 2008 ; Gupta and Ji , 2009 ; Riedel et al . 
, 2010 ; Hong et al . 
, 2011 ; McClosky et al . 
, 2011 ; Huang and Riloff , 2012a , b ; Araki and Mitamura , 2015 ; Li et al . 
, 2013 ; Yang and Mitchell , 2016 ; Liu et al . 
, 2016b ) , neural methods ( Chen et al . 
, 2015 ; Nguyen and Grishman , 2015 ; Nguyen et al . 
, 2016 ; Duan et al . 
, 2017 ; Nguyen et al . 
, 2016 ; Ghaeini et al . 
, 2016 ; Lin et al . 
, 2018 ) , the methods with external heterogeneous knowledge ( Liu et al . 
, 2016a , 2017 ; Zhang et al . 
, 2017 ; Duan et al . 
, 2017 ; Zhao et al . 
, 2018 ; Liu et al . 
, 2018b ) . 
Some advanced architectures , such as graph convolutional networks ( Nguyen and Grishman , 2018 ) and adversarial training ( Hong et al . 
, 2018 ; Wang et al . 
, 2019a ) , have also been applied recently . 
As ED models has achieved relatively promising results , the more difÔ¨Åcult EAE becomes the bottleneck of EE , and have drawn growing research interests . 
The early works ( Patwardhan and Riloff , 2009 ; Gupta and Ji , 2009 ; Liao and Grishman , 2010b , a ; Huang and Riloff , 2012b ; Li et al . 
, 2013 ) focus on designing hand - crafted features and heuristic rules to extract event arguments , which suffer from the problem of both implementation complexity and low recall . 
As the rapid develop - ment of neural networks , various neural methods have been proposed , such as utilizing convolutional models ( Chen et al . 
, 2015 ) , utilizing recurrent models ( Nguyen et al . 
, 2016 ; Sha et al . 
, 2018 ) , and Ô¨Ånetuning pre - trained language model BERT ( Wang et al . 
, 2019b ) . 
As compared with the early featurebased and rule - based methods , neural methods automatically represent sentence semantics with lowdimensional vectors , and independently determine argument roles with the vectors , leading to getting rid of designing sophisticated features and rules . 
Recently , some works adopt some advanced techniques to further improve EAE models in different scenarios , including zero - shot learning ( Huang et al . 
, 2018 ) , multi - modal integration ( Zhang et al . 
, 2017 ) , cross - lingual ( Subburathinam et al . 
, 2019 ) , end - to - end ( Wadden et al . 
, 2019 ) , and weak supervision ( Chen et al . 
, 2017 ; Zeng et al . 
, 2018 ) . 
The current methods for EAE have achieved some promising results . 
However , they focus on independently handling each argument entity to predict its role . 
Because of ignoring to capture rich correlated knowledge among event arguments , the above - mentioned methods are easy to trap in a local optimum and make some inexplicable mistakes . 
Inspired by some methods in named entity recognition ( Huang et al . 
, 2015 ) and relation extraction ( Miwa and Bansal , 2016 ) , some recent proactive works view EAE as a sequence labeling problem . 
Following the methods for sequence labeling problem ( Ma and Hovy , 2016 ) , these sequential EAE models ( Yang and Mitchell , 2016 ; Zeng et al . 
,171 2018 ) adopt conditional random Ô¨Åeld ( CRF ) with the Viterbi algorithm ( Rabiner , 1989 ) , and unintentionally consider the correlation of event arguments . 
Limited by the Markov property , the linear - chain CRF sequentially considers the correlation between two adjacent event arguments , which can not adequately handle the complex situation in EAE that each argument and any other arguments may be correlated . 
To this end and inspired by some proactive works ( Finkel et al . 
, 2005 ; Sun et al . 
, 2014 ) , we adapt Gibbs sampling ( Geman and Geman , 1987 ) for EAE to perform approximate inference from the joint distribution . 
Moreover , we incorporate simulated annealing ( Kirkpatrick et al . 
, 1983 ) to accelerate the sampling process , leading to an effective and efÔ¨Åcient method . 
3 Methodology 3.1 Framework For convenience , we denote X={x1, ... ,x n } andX‚àíi={x1, ... ,x i‚àí1,xi+1, ... ,x n } . 
Figure 2 shows the overall framework of our Neural Gibbs Sampling ( NGS ) method , consisting of the following modules : The neural models , including a prior neural model to model the prior distribution Pp(xi|o ) , and a conditional neural model to model the conditional distributionPc(xi|X‚àíi , o ) . 
The prior neural model is similar with existing EAE methods , which takes the event mention text as input and outputs the labels of event argument candidates . 
The labels will serve as the prior state for the Gibbs sampling module . 
The conditional neural model takes the text and the results of the last step as input and outputs the probability distribution over labels for each event argument candidate . 
The Gibbs sampling module to sample variable assignments XwithPp(xi|o)and Pc(xi|X‚àíi , o ) , which gradually match the implicit posterior joint distribution . 
The simulated annealing method to efÔ¨Åciently Ô¨Ånd the optimal state in the Markov chain of Gibbs sampling . 
It uses a ‚Äú temperature ‚Äù parameter to control the sharpness of the transition distribution . 
With the ‚Äú temperature ‚Äù decreasing , the algorithm will more and more tend to choose the max - likelihood state as the next state . 
3.2 Neural Models The Prior Neural Model is to model the prior distribution Pp(xi|o ) . 
In this paper , we use DM - CNN ( Chen et al . 
, 2015 ) and DMBERT as the prior neural models . 
Given a sentence consisting of several words{w1, ... ,t, ... ,w i, ... ,w n } , wheret andwidenote the trigger word and the candidate argument entity respectively . 
DMCNN transfers each word in the word sequence into an input embedding ei , which consists of word embedding , event type embedding , and position embedding . 
Then , DMCNN feeds the input embeddings into a convolutional encoding layer to automatically learn the features and a dynamic multi - pooling layer to aggregate the features into a uniÔ¨Åed sentence observation embedding to predict an argument role xiforwi . 
DMBERT is a variation of BERT ( Devlin et al . 
, 2019 ) proposed by Wang et al . 
( 2019b ) . 
It adopts a pre - trained BERT to represent the word sequence as feature vectors and also uses a dynamic multipooling mechanism like DMCNN to aggregate the features into an instance embedding for prediction . 
It inserts special tokens around the event argument candidates to indicate their positions . 
We sample an argument role following Pp(xi|o ) for each argument candidate and Ô¨Ånally predict an initial argument role state X(0)={x(0 ) 1, ... ,x(0 ) n } as the start point of Gibbs sampling . 
Note that , our NGS method does not have any special requirements for the prior neural model , any other neural networks can also be used . 
Conditional Neural Model is to model the conditional distribution Pc(xi|X‚àíi , o)for the state transition in Gibbs sampling . 
Considering that it requires to integrate the argument role information of X‚àíito computePc(xi|X‚àíi , o ) , we set an argument role embedding aifor each word wito represent whether it is an event argument and which role it is of . 
Then , we modify the input layer of DMCNN and DMBERT to feed the argument role embeddings in . 
More speciÔ¨Åcally , DMCNN concatenates the original input embedding eiwith the argument role embedding aias new inputs . 
DMBERT utilizes the pre - trained parameters and adds aiinto the input embedding . 
3.3 Gibbs Sampling Module The Gibbs sampling module aims at sampling from the implicit joint distribution P(X|o ) . 
As Algorithm 1 shows , we use the prior neural model to initialize an initial state X(0 ) . 
In stept , for each random variable xi , we input the other random variables ‚Äô states X(t‚àí1 ) ‚àíiinto the conditional neu-172 Algorithm 1 Neural Gibbs sampling Input : Initial state X(0)={x(0 ) 1, ... ,x(0 ) n}predicted by the prior neural network Result : Nsamples matching the joint distribution P(X|o ) Train the conditional neural model to Ô¨Åt Pc(xi|X‚àíi , o ) fort‚Üê1toNdo // iteratively change the state fori‚Üê1tondo x(t ) i‚Üêsample / parenleftBig Pc / parenleftbig x(t ) i|X(t‚àí1 ) ‚àíi , o / parenrightbig / parenrightBig end X(t)‚Üê{x(t ) 1, ... ,x(t ) n } end ReturnX(1), ... ,X(N ) ral model to get the distribution Pc / parenleftbig x(t ) i|X(t‚àí1 ) ‚àíi , o / parenrightbig . 
Then we sample x(t ) ifrom the distribution , and Ô¨Ånally get the new state X(t ) . 
We can approximately sampleNsamplesX(1), ... ,X(N)with the Gibbs sampling module . 
Our Appendix gives the proof that the samples will accurately follow the joint distribution after enough steps . 
Geman and Geman ( 1987 ) have shown that the samples from the beginning of the Markov chain ( the burn - in period ) may not accurately follow the desired distribution , hence we choose the most frequent state from X(N 2), ... ,X(N)as the result . 
3.4 Simulated Annealing Method The Gibbs sampling module is to accurately estimate the shape of P(X|o ) , which will take many steps to reach the convergence . 
As what we want for EAE is only the max - likelihood state , we adopt a simulated annealing method to efÔ¨Åciently Ô¨Ånd the optimal state following Geman and Geman ( 1987 ) . 
As shown in Algorithm 2 , in step t , the simulated annealing method randomly sample an ifrom the distributionmax / parenleftbig Pc / parenleftbig x(t ) i|X(t‚àí1 ) ‚àíi , o / parenrightbig / parenrightbig1 / c /summationtextn j=1max / parenleftbig Pc / parenleftbig x(t ) j|X(t‚àí1 ) ‚àíj , o / parenrightbig / parenrightbig1 / c. The probability of ibeing chosen has positive correlation with the probability of the max - likelihood state in the conditional distribution of xi . 
Then we only need to update xiwith its max - likelihood state in conditional distribution Pc / parenleftbig x(t ) i|X(t‚àí1 ) ‚àíi / parenrightbig modeled by the conditional neural model to get the next stateX(t ) , which is more efÔ¨Åcient than the original Gibbs sampling method . 
The simulated annealing method adopts a time - varying parameter cAlgorithm 2 NGS + simulated annealing Input : Initial state X(0)={x(0 ) 1, ... ,x(0 ) n}predicted by the prior neural network Result : The max - likelihood state X(N ) Train the conditional neural model to Ô¨Åt Pc(xi|X‚àíi , o ) c= 1 fort‚Üê1toNdo // randomly choose ito transit i‚Üêsample / parenleftBigg max / parenleftBig Pc / parenleftBig x(t ) i|X(t‚àí1 ) ‚àíi , o / parenrightBig / parenrightBig1 / c /summationtextn j=1max / parenleftBig Pc / parenleftBig x(t ) j|X(t‚àí1 ) ‚àíj , o / parenrightBig / parenrightBig1 / c / parenrightBigg x(t ) i‚Üêarg max / parenleftBig Pc / parenleftbig x(t ) i|X(t‚àí1 ) ‚àíi , o / parenrightbig / parenrightBig X(t)‚ÜêX(t‚àí1 ) ‚àíi‚à™{x(t ) i } decreasec end ReturnX(N ) to control the sharpness of the distribution . 
With c gradually decreasing , the algorithm more and more tends to transit in the max - likelihood way and will quickly reach the max - likelihood state . 
When cis large , it performs like the original Gibbs sampling , so that can avoid falling into suboptimal results . 
4 Experiments 4.1 Datasets and Evaluation Metrics We evaluate the proposed models on two real - world datasets : the most widely - used ACE 2005 ( Walker et al . 
, 2006 ) and the newly - developed TAC KBP 2016 ( Ellis et al . 
, 2015 ) . 
They are both often used as the benchmark in the previous works . 
ACE 20051is the most widely - used dataset in EE , consisting of 599 documents , 8 event types , 33 event subtypes , and 35 argument roles . 
We evaluate our models by the performance of argument classiÔ¨Åcation . 
When testing models , an argument is correctly classiÔ¨Åed only if its event subtype , offsets and argument role match the annotation results . 
For fair comparison with the previous works ( Liao and Grishman , 2010b ; Chen et al . 
, 2015 ) , we follow them to use the same test set containing 40 newswire documents , the similar development set with 30 randomly selected documents and training set with the remaining 529 documents . 
TAC KBP 20162indicates the data of the TAC KBP 2016 Event Argument Extraction track , which is the latest benchmark dataset in EE . 
Different 1https://catalog.ldc.upenn.edu/LDC2006T06 2https://tac.nist.gov//2016/KBP/173 from ACE 2005 , this competition only annotates difÔ¨Åcult test data but no training data . 
Accordingly , they encourage participants to construct training data from any other sources by themselves . 
Considering the argument roles of TAC KBP 2016 are almost the same with ACE 2005 expect TAC KBP 2016 merges all the time - related roles in ACE 2005 . 
We use the ACE 2005 dataset as our training data , which is also provided to the participants of the competition . 
Hence we can have a fair comparison with the baselines . 
For fair comparison with the baselines , we use the same evaluation metrics with previous works : ( 1)Precision ( P ) , which is deÔ¨Åned as the number of correct argument predictions divided by the number of all argument predictions returned by the model . 
( 2 ) Recall ( R ) , which deÔ¨Åned as the number of correct argument predictions divided by the number of all correct golden results in the test set . 
( 3 ) F1 score ( F1 ) , which is deÔ¨Åned as the harmonic mean of the precision and recall . 
F1 score is the most important metric to evaluate EAE performance . 
4.2 Baselines To directly show the improvement of our method from the comparisons , we reproduce DMCNN and DMBERT as baselines on both of the two datasets . 
In addition , we also select some state - of - the - art baselines on the two datasets respectively . 
OnACE 2005 , we compare our models with various state - of - the - art baselines , including : ( 1 ) Feature - based methods . 
Li ‚Äôs joint ( Li et al . 
, 2013 ) adopts structure prediction to extract events , which is the best traditional feature - based method . 
RBPB ( Sha et al . 
, 2016 ) adopts a regularization - based method to balance the effect of features and patterns , and also consider the relationship between argument candidates . 
( 2 ) Vanilla neural network methods . 
JRNN ( Nguyen et al . 
, 2016 ) jointly conducts event detection and event argument extraction with bidirectional recurrent neural networks . 
( 3 ) Advanced neural network method with external information . 
The dbRNN ( Sha et al . 
, 2018 ) utilizes a recurrent neural network with dependency bridges to carry syntactically related information between words , which considers not only sequence structures but also tree structures of the sentences . 
TheHMEAE ( Wang et al . 
, 2019b ) leverages the latent concept hierarchy among argument roles with neural module networks , which considers the labelLearning Rate 10‚àí3 Batch Size 60 Dropout Probability 0.5 Hidden Layer Dimension 300 Kernel Size 3 Word Embedding Dimension 100 Position Embedding Dimension 5 Event Type Embedding Dimension 5 Argument Role Embedding Dimension 5 Table 1 : Hyperparameter settings for CNN models . 
Learning Rate 6√ó10‚àí5 Batch Size 50 Warmup Rate for the Prior Neural Model 0.1 Warmup Rate for the Conditional Nueral Model 0.05 Argument Role Embedding Dimension 768 Table 2 : Hyperparameter settings for BERT models . 
dependency but still classify each event argument independently . 
OnTAC KBP 2016 , we compare our models with the top systems of the competition , including : DISCERN - R ( Dubbin et al . 
, 2016 ) , CMU CS Event1 ( Hsi et al . 
, 2016 ) , Washington1 and Washington4 ( Ferguson et al . 
, 2016 ) . 
4.3 Hyperparameter Settings Our methods with DMCNN and DMBERT as the prior and conditional neural networks are named as NGS ( CNN ) andNGS ( BERT ) respectively . 
They both transit for 200 steps and the clinearly decrease from 1to0 . 
As our work focuses on extracting event arguments and their roles and our methods do not involve the event detection stage ( to identify the trigger and determine the event type ) , we conduct EAE based on the event detection models in ( Chen et al . 
, 2015 ) and ( Wang et al . 
, 2019a ) for the CNN and BERT models respectively . 
ForNGS ( CNN ) , the hyperparameters of the prior and conditional neural networks are set as the same as in the original DMCNN ( Chen et al . 
, 2015 ) . 
We also use the pre - trained word embeddings learned by Skip - Gram ( Mikolov et al . 
, 2013 ) as the initial word embeddings . 
The detailed hyperparameters are shown in Table 1 . 
ForNGS ( BERT ) , the two BERT models for the prior and conditional probability distributions are both based on the BERT BASE model in Devlin et al . 
( 2019 ) . 
We apply the pre - trained model3to initialize the parameters . 
To utilize the event type information in our model , we append a special token into each input sequence for BERT to indicate 3github.com/google-research/bert174 MethodTrigger ClassiÔ¨ÅcationArgument Role ClassiÔ¨Åcation P R F1 P R F1 Li ‚Äôs Joint 73.7 62.3 67.5 64.7 44.4 52.7 DMCNN 75.6 63.6 69.1 62.2 46.9 53.5 RBPB 70.3 67.5 68.9 54.1 53.5 53.8 JRNN 66.0 73.0 69.3 54.2 56.7 55.4 HMEAE ( CNN ) 75.6 63.6 69.1 57.3 54.2 55.7 DMBERT 77.6 71.8 74.6 58.8 55.8 57.2 dbRNN 74.1 69.8 71.9 66.2 52.8 58.7 HMEAE ( BERT ) 77.6 71.8 74.6 62.2 56.6 59.3 NGS ( CNN ) 75.6 63.6 69.1 61.3 51.3 55.9 NGS ( BERT ) 77.6 71.8 74.6 59.9 59.1 59.5 Table 3 : The overall EAE results ( % ) of various baselines and NGS on ACE 2005 . 
EAE performances are inÔ¨Çuenced by the trigger quality , hence we also provide the trigger classiÔ¨Åcation ( event detection ) results . 
Note that as our work does not involve the event detection stage , the NGS ( CNN ) and NGS ( BERT ) use the triggers predicted by DMCNN and DMBERT respectively . 
MethodArgument Role ClassiÔ¨Åcation P R F1 DISCERN - R ( Dubbin et al . 
, 2016 ) 7.9 7.4 7.7 Washington4 ( Ferguson et al . 
, 2016 ) 32.1 5.0 8.7 CMU CS Event1 ( Hsi et al . 
, 2016 ) 31.2 4.9 8.4 Washington1 ( Ferguson et al . 
, 2016 ) 26.5 6.8 10.8 DMCNN ( Chen et al . 
, 2015 ) 17.9 16.0 16.9 HMEAE ( CNN ) ( Wang et al . 
, 2019b ) 15.3 22.5 18.2 DMBERT ( Wang et al . 
, 2019b ) 22.6 24.7 23.6 HMEAE ( BERT ) ( Wang et al . 
, 2019b ) 24.8 25.4 25.1 NGS ( CNN ) 21.5 16.2 18.5 NGS ( BERT ) 25.5 25.1 25.3 Table 4 : The overall EAE results ( % ) of various baseline methods and our NGS on TAC KBP 2016 Event Argument Task . 
All the models use golden triggers . 
the event type . 
Additional hyperparameters used in our experiments are shown in Table 2 . 
4.4 Overall Evaluation Results The overall results of various baseline methods and NGS on ACE 2005 are shown in Table 3 . 
And the results on TAC KBP 2016 are shown in Table 4 . 
From the results , we observe that : ( 1 ) NGS ( CNN ) and NGS ( BERT ) achieve signiÔ¨Åcant improvements as compared with DMCNN and DMBERT respectively . 
Meanwhile , our models still outperform other baseline methods , which are either the typical EAE models or the recent state - of - the - art models . 
It indicates that our Gibbs sampling with simulated annealing works well to improve EAE with the help of adequately model / uni00000013 /uni00000015 / uni00000013 / uni00000013 /uni00000017 / uni00000013 / uni00000013 /uni00000019 / uni00000013 / uni00000013 /uni0000001b / uni00000013 / uni00000013 /uni00000014 / uni00000013 / uni00000013 / uni00000013 /uni00000014 / uni00000015 / uni00000013 / uni00000013 /uni00000036 / uni00000057 / uni00000048 / uni00000053 / uni00000013 / uni00000011 / uni00000018 / uni00000014 / uni00000013 / uni00000011 / uni00000018 / uni00000015 / uni00000013 / uni00000011 / uni00000018 / uni00000016 / uni00000013 / uni00000011 / uni00000018 / uni00000017 / uni00000013 / uni00000011 / uni00000018 / uni00000018 / uni00000013 / uni00000011 / uni00000018 / uni00000019 / uni00000029 / uni00000014 /uni0000000e / uni00000036 / uni0000004c / uni00000050 / uni00000058 / uni0000004f / uni00000044 / uni00000057 / uni00000048 / uni00000047 / uni00000003 / uni00000024 / uni00000051 / uni00000051 / uni00000048 / uni00000044 / uni0000004f / uni0000004c / uni00000051 / uni0000004a /uni0000002a / uni0000004c / uni00000045 / uni00000045 / uni00000056 / uni00000003 / uni00000036 / uni00000044 / uni00000050 / uni00000053 / uni0000004f / uni0000004c / uni00000051 / uni0000004a /uni00000013 /uni00000015 / uni00000013 / uni00000013 /uni00000017 / uni00000013 / uni00000013 /uni00000019 / uni00000013 / uni00000013 /uni0000001b / uni00000013 / uni00000013 /uni00000014 / uni00000013 / uni00000013 / uni00000013 /uni00000014 / uni00000015 / uni00000013 / uni00000013 /uni00000036 / uni00000057 / uni00000048 / uni00000053 / uni00000013 / uni00000011 / uni00000014 / uni0000001a / uni00000013 / uni00000013 / uni00000011 / uni00000014 / uni0000001a / uni00000018 / uni00000013 / uni00000011 / uni00000014 / uni0000001b / uni00000013 / uni00000013 / uni00000011 / uni00000014 / uni0000001b / uni00000018 / uni00000029 / uni00000014 /uni0000000e / uni00000036 / uni0000004c / uni00000050 / uni00000058 / uni0000004f / uni00000044 / uni00000057 / uni00000048 / uni00000047 / uni00000003 / uni00000024 / uni00000051 / uni00000051 / uni00000048 / uni00000044 / uni0000004f / uni0000004c / uni00000051 / uni0000004a /uni0000002a / uni0000004c / uni00000045 / uni00000045 / uni00000056 / uni00000003 / uni00000036 / uni00000044 / uni00000050 / uni00000053 / uni0000004f / uni0000004c / uni00000051 / uni0000004aFigure 3 : F1 - step curves of NGS ( CNN ) with the simulated annealing method and the original Gibbs sampling on ACE 2005 ( left ) and TAC KBP 2016 ( right ) . 
ing the correlation between event arguments . 
This demonstrates that our method is effective . 
( 2 ) As NGS enhances both CNN models and BERT models on different datasets , it shows that our Gibbs sampling with simulated annealing is independent of EAE models . 
In other words , our method can be easily adapted for other EAE models to enhance their extraction performances . 
( 3 ) From the experimental results on both ACE 2005 and TAC KBP 2016 , we can Ô¨Ånd that the recall scores and F1 scores of our models are much better than the baseline models . 
The precision scores of our models do not achieve such obvious improvements . 
This is consistent with what we mention in the previous sections . 
We argue that the baseline models focusing on independently handling each event argument candidates may sever the constraints among argument roles , and may trap in a local optimum or over-Ô¨Åt the training set . 
The models without considering argument correlations may predict various argument roles with high conÔ¨Ådence , even make some inexplicable mistakes . 
Hence the precision scores of these models may increase , but their recall scores and F1 scores may decrease . 
Our models adopt Gibbs sampling for EAE to perform approximate inference from the joint distribution , and make the most of the corrleation and constraints among argument roles . 
Accordingly , our models can avoid these issues and achieve the state - of - the - art results . 
4.5 Ablation Study In order to verify the effectiveness of our method , especially for the simulated annealing method and the prior neural network , we conduct ablation studies on ACE 2005 and TAC KBP 2016 . 
Effectiveness of the Simulated Annealing To demonstrate the effectiveness of the simulated annealing method , we show the F1 - step curves of175 Type : Justice Subtype : Appeal Text : Malaysia ‚Äôs second highest court onFriday rejected an appeal by ... Anwar Ibrahim against his conviction and nine - year prison sentence for sodomy . 
Event Argument Candidate Malaysia court Friday Anwar Ibrahim sodomy DMCNN Place / check Adjudicator /check Time - Within /check Plaintiff /check N / A√ó NGS ( CNN ) Place / check Adjudicator /check Time - Within /check Plaintiff /check Crime / check Table 5 : Top : An example sentence highlighting the event argument candidates , which is sampled from ACE 2005 . 
Bottom : EAE results of DMCNN and NGS ( CNN ) . 
NGS ( CNN ) correctly classiÔ¨Åes ‚Äú sodomy ‚Äù into Crime with the help of correlations among event arguments . 
/uni00000013 /uni00000015 / uni00000018 /uni00000018 / uni00000013 /uni0000001a / uni00000018 /uni00000014 / uni00000013 / uni00000013 /uni00000014 / uni00000015 / uni00000018 /uni00000014 / uni00000018 / uni00000013 /uni00000036 / uni00000057 / uni00000048 / uni00000053 / uni00000013 / uni00000011 / uni00000015 / uni00000013 / uni00000013 / uni00000011 / uni00000015 / uni00000018 / uni00000013 / uni00000011 / uni00000016 / uni00000013 / uni00000013 / uni00000011 / uni00000016 / uni00000018 / uni00000013 / uni00000011 / uni00000017 / uni00000013 / uni00000013 / uni00000011 / uni00000017 / uni00000018 / uni00000013 / uni00000011 / uni00000018 / uni00000013 / uni00000013 / uni00000011 / uni00000018 / uni00000018 / uni00000013 / uni00000011 / uni00000019 / uni00000013 / uni00000029 / uni00000014 /uni00000033 / uni00000055 / uni0000004c / uni00000052 / uni00000055 / uni00000003 / uni00000031 / uni00000048 / uni00000058 / uni00000055 / uni00000044 / uni0000004f / uni00000003 / uni00000030 / uni00000052 / uni00000047 / uni00000048 / uni0000004f / uni00000003 / uni0000002c / uni00000051 / uni0000004c / uni00000057 / uni0000004c / uni00000044 / uni0000004f / uni0000004c / uni0000005d / uni00000044 / uni00000057 / uni0000004c / uni00000052 / uni00000051 /uni00000035 / uni00000044 / uni00000051 / uni00000047 / uni00000052 / uni00000050 / uni00000003 / uni0000002c / uni00000051 / uni0000004c / uni00000057 / uni0000004c / uni00000044 / uni0000004f / uni0000004c / uni0000005d / uni00000044 / uni00000057 / uni0000004c / uni00000052 / uni00000051 /uni00000013 /uni00000015 / uni00000013 /uni00000017 / uni00000013 /uni00000019 / uni00000013 /uni0000001b / uni00000013 /uni00000014 / uni00000013 / uni00000013 /uni00000014 / uni00000015 / uni00000013 /uni00000036 / uni00000057 / uni00000048 / uni00000053 / uni00000013 / uni00000011 / uni00000014 / uni00000018 / uni00000013 / uni00000011 / uni00000014 / uni00000019 / uni00000013 / uni00000011 / uni00000014 / uni0000001a / uni00000013 / uni00000011 / uni00000014 / uni0000001b / uni00000029 / uni00000014 /uni00000033 / uni00000055 / uni0000004c / uni00000052 / uni00000055 / uni00000003 / uni00000031 / uni00000048 / uni00000058 / uni00000055 / uni00000044 / uni0000004f / uni00000003 / uni00000030 / uni00000052 / uni00000047 / uni00000048 / uni0000004f / uni00000003 / uni0000002c / uni00000051 / uni0000004c / uni00000057 / uni0000004c / uni00000044 / uni0000004f / uni0000004c / uni0000005d / uni00000044 / uni00000057 / uni0000004c / uni00000052 / uni00000051 /uni00000035 / uni00000044 / uni00000051 / uni00000047 / uni00000052 / uni00000050 / uni00000003 / uni0000002c / uni00000051 / uni0000004c / uni00000057 / uni0000004c / uni00000044 / uni0000004f / uni0000004c / uni0000005d / uni00000044 / uni00000057 / uni0000004c / uni00000052 / uni00000051 Figure 4 : F1 - step curves of NGS ( CNN ) with prior neural network initialization and random initialization on ACE 2005 ( left ) and TAC KBP 2016 ( right ) . 
Gibbs sampling with and without the simulated annealing in Figure 3 . 
We can observe that : ( 1 ) The simulated annealing method can signiÔ¨Åcantly improve the convergence speed and the stability . 
Our methods just require quarter to half of the steps to reach the convergence . 
( 2 ) The simulated annealing method does not weaken the performance of our models . 
Although the methods with the simulated annealing are much more efÔ¨Åcient than those without the simulated annealing , their results are comparable . 
Effectiveness of the Prior Neural Network As the mathematical proof in the Appendix shows , a prior distribution is not necessary for Gibbs sampling . 
To demonstrate the effectiveness of the prior neural model , we show the F1 - step curves of the prior neural model initialization and a random initialization for our NGS method ( with simulated annealing ) in Figure 4 . 
As it shows in Ô¨Ågures , our NGS models with the prior neural network initialization take much fewer steps to reach the convergence than those models with random initialization , which is important and meaningful for the application . 
Combining the prior neural network initialization and the simulated annealing for our NGS will lead to a more efÔ¨Åcient model.#arguments 1 - 2 3 - 4 > 5 DMCNN 55.3 54.1 61.8 NGS ( CNN ) 56.7 ( +1.4 ) 57.9 ( +3.8 ) 69.5 ( +7.7 ) Table 6 : F1 scores ( % ) of DMCNN and NGS ( CNN ) on different parts of ACE 2005 dev set with different event argument numbers per sentence . 
4.6 Analysis on Modeling Event Argument Correlations To analyze whether NGS can successfully capture the event argument correlations and further improve EAE performance , we conduct a case study in Table 5 and a quantitative analysis in Table 6 . 
The sentence in Table 5 is a real sentence containing an Appeal event , which is sampled from the test set of ACE 2005 . 
From the EAE results , we can see that the vanilla DMCNN correctly classiÔ¨Åes most of the event argument candidates . 
But because ‚Äú sodomy ‚Äù is a rare word , it misclassiÔ¨Åed ‚Äú sodomy ‚Äù into ‚Äú N / A ‚Äù ( not an event argument ) . 
With the help of our NGS method ‚Äôs ability to model the joint distribution among event arguments , NGS ( CNN ) can infer that ‚Äú sodomy ‚Äù is a crime from the event argument correlations as it has known there are some crime - related arguments ( adjudicator and plaintiff ) in the sentence . 
On the other side , we show the comparisons between the basic model DMCNN and NGS ( CNN ) on data with different numbers of event arguments in Table 6 . 
With the increase of event argument number , our improvements signiÔ¨Åcantly rise , which demonstrates our improvements come from modeling the correlations among event arguments . 
Note that the F1 scores are higher than the overall F1 scores , which is due to we Ô¨Ålter out the negative instances without event arguments.176 5 Conclusion and Future Work In this paper , we propose a novel Neural Gibbs Sampling ( NGS ) method to adequately model the correlation between event arguments and argument roles , which combines the advantages of the Gibbs sampling method to model the joint distribution among random variables and the neural network models to automatically learn the effective representations . 
Considering the shortcoming of high complexity of Gibbs sampling algorithm , we further apply simulated annealing to accelerate the whole estimation process , which lead our method to being both effective and efÔ¨Åcient . 
The experimental results on two widely - used real - world datasets show that NGS can achieve comparable results to existing state - of - the - art EAE methods . 
The empirical analyses and ablation studies further verify the effectiveness and efÔ¨Åciency of our method . 
In the future : ( 1 ) We will try to extend NGS to other tasks and scenarios to evaluate its general effectiveness of modeling the latent correlations . 
( 2 ) We will also explore more effective and simple methods to consider the correlations . 
Acknowledgement We thank Hedong ( Ben ) Hou for his help in the mathematical proof . 
This work is supported by the Key - Area Research and Development Program of Guangdong Province ( 2019B010153002 ) , NSFC Key Projects ( U1736204 , 61533018 ) , a grant from Institute for Guo Qiang , Tsinghua University ( 2019GQB0003 ) and THUNUS NExT CoLab . 
This work is also supported by the Pattern Recognition Center , WeChat AI , Tencent Inc. Xiaozhi Wang is supported by Tsinghua University Initiative ScientiÔ¨Åc Research Program . 
Abstract Despite the success of neural machine translation ( NMT ) , simultaneous neural machine translation ( SNMT ) , the task of translating in real time before a full sentence has been observed , remains challenging due to the syntactic structure difference and simultaneity requirements . 
In this paper , we propose a general framework for adapting neural machine translation to translate simultaneously . 
Our framework contains two parts : preÔ¨Åx translation that utilizes a consecutive NMT model to translate source preÔ¨Åxes and a stopping criterion that determines when to stop the preÔ¨Åx translation . 
Experiments on three translation corpora and two language pairs show the efÔ¨Åcacy of the proposed framework on balancing the quality and latency in adapting NMT to perform simultaneous translation . 
1 Introduction Simultaneous translation ( F ¬®ugen et al . 
, 2007 ; Oda et al . 
, 2014 ; Grissom et al . 
, 2014 ; Niehues et al . 
, 2016 ; Cho and Esipova , 2016 ; Gu et al . 
, 2017 ; Ma et al . 
, 2018 ) , the task of producing a partial translation of a sentence before the whole input sentence ends , is useful in many scenarios including outbound tourism , international summit and multilateral negotiations . 
Different from the consecutive translation in which translation quality alone matters , simultaneous translation trades off between translation quality and latency . 
The syntactic structure difference between the source and target language makes simultaneous translation more challenging . 
For example , when translating from a verb-Ô¨Ånal ( SOV ) language ( e.g. , Japanese ) to a verb - media ( SVO ) language ( e.g. , English ) , the verb appears much later in the source sequence ‚àóPart of the work was done when Yun is working at Huawei Noah ‚Äôs Ark Lab.than in the target language . 
Some premature translations can lead to signiÔ¨Åcant loss in quality ( Ma et al . 
, 2018 ) . 
Recently , a number of researchers have endeavored to explore methods for simultaneous translation in the context of NMT ( Bahdanau et al . 
, 2015 ; Vaswani et al . 
, 2017 ) . 
Some of them propose sophisticated training frameworks explicitly designed for simultaneous translation ( Ma et al . 
, 2018 ; Arivazhagan et al . 
, 2019 ) . 
These approaches are either memory inefÔ¨Åcient during training ( Ma et al . 
, 2018 ) or with hyper - parameters hard to tune ( Arivazhagan et al . 
, 2019 ) . 
Others utilize a full - sentence base model to perform simultaneous translation by modiÔ¨Åcations to the encoder and the decoding process . 
To match the incremental source context , they replace the bidirectional encoder with a left - to - right encoder ( Cho and Esipova , 2016 ; Satija and Pineau , 2016 ; Gu et al . 
, 2017 ; Alinejad et al . 
, 2018 ) or recompute the encoder hidden states ( Zheng et al . 
, 2019 ) . 
On top of that , heuristic algorithms ( Cho and Esipova , 2016 ; Dalvi et al . 
, 2018 ) or a READ / WRITE model trained with reinforcement learning ( Satija and Pineau , 2016 ; Gu et al . 
, 2017 ; Alinejad et al . 
, 2018 ) or supervised learning ( Zheng et al . 
, 2019 ) are used to decide , at every step , whether to wait for the next source token or output a target token . 
However , these models either can not directly use a pretrained consecutive neural machine translation ( CNMT ) model with bidirectional encoder as the base model or work in a sub - optimal way in the decoding stage . 
In this paper , we study the problem of adapting neural machine translation to translate simultaneously . 
We formulate simultaneous translation as two nested loops : an outer loop that updates input buffer with newly observed source tokens and an inner loop that translates source tokens in the buffer updated at each outer step . 
For the outer loop , the input buffer can be updated by an ASR system with191 an arbitrary update schedule . 
For the inner loop , we translate using the pretrained CNMT model and stop translation with a stopping controller . 
Such formulation is different from previous work ( Satija and Pineau , 2016 ; Gu et al . 
, 2017 ; Alinejad et al . 
, 2018 ; Zheng et al . 
, 2019 ) which deÔ¨Åne simultaneous translation as sequentially making interleaved READ or WRITE decisions . 
We argue that our formulation is better than the previous one in two aspects : ( i ) Our formulation can better utilize the available source tokens . 
Under previous formulation , the number of source tokens observed by the CNMT model is determined by the number of READ actions that has been produced by the policy network . 
It is likely that the CNMT model does not observe all the available source tokens produced by the ASR system . 
In contrast , the CNMT model observes all the available source tokens when performing inner loop translation in our framework . 
( ii ) Previous formulation makes TŒ∑+TœÑREAD or WRITE decisions regardless of the ASR update schedule , where TŒ∑andTœÑare source sentence and translation length , respectively . 
For an ASR system that outputs multiple tokens at a time , this is computational costly . 
Consider an extreme case where the ASR system outputs a full source sentence at a time . 
Previous work translates with a sequence of TŒ∑+TœÑactions , while we translate with a sequence ofTœÑdecisions ( TœÑ‚àí1CONTINUE and 1 STOP ) . 
Under our proposed framework , we present two schedules for simultaneous translation : one stops the inner loop translation with heuristic and one with a stopping controller learned in a reinforcement learning framework to balance translation quality and latency . 
We evaluate our method on IWSLT16 German - English ( DE - EN ) translation in both directions , WMT15 English - German ( EN - DE ) translation in both directions , and NIST Chineseto - English ( ZH‚ÜíEN ) translation . 
The results show our method with reinforced stopping controller consistently improves over the de - facto baselines , and achieves low latency and reasonable BLEU scores . 
2 Background Given a set of source ‚Äì target sentence pairs /angbracketleftxm , y‚àó m / angbracketrightM m=1 , a consecutive NMT model can be trained by maximizing the log - likelihood of the target sentence from its entire source side context : ÀÜœÜ= argmax œÜ / braceleftbiggM / summationdisplay m=1logp(y‚àó m|xm;œÜ)/bracerightbigg , ( 1)whereœÜis a set of model parameters . 
At inference time , the NMT model Ô¨Årst encodes a source language sentence x={x1, ... ,xTŒ∑}with its encoder and passes the encoded representations h={h1, ... ,hTŒ∑}to a greedy decoder . 
Then the greedy decoder generates a translated sentence in target language by sequentially choosing the most likely token at each step t : yt= argmaxyp(y|y < t , x ) . 
( 2 ) The distribution of next target word is deÔ¨Åned as : p(y|y < t , x)‚àùexp [ œÜOUT(zt ) ] zt = œÜDEC(yt‚àí1,z < t , h ) , ( 3 ) whereztis the decoder hidden state at position t. In consecutive NMT , once obtained , the encoder hidden states hand the decoder hidden state ztare not updated anymore and will be reused during the entire decoding process . 
3 Simultaneous NMT In SNMT , we receive streaming input tokens , and learn to translate them in real - time . 
We formulate simultaneous translation as two nested loops : the outer loop that updates an input buffer with newly observed source tokens and the inner loop that translates source tokens in the buffer updated at each outer step . 
More precisely , suppose at the end of outer step s‚àí1 , the input buffer is xs‚àí1={x1, ... ,xŒ∑[s‚àí1 ] } , and the output buffer is ys‚àí1={y1, ... ,yœÑ[s‚àí1 ] } . 
Then at outer step s , the system translates with the following steps : 1The system observes cs>0new source tokens and updates the input buffer to be xs= { x1, ... ,xŒ∑[s]}whereŒ∑[s ] = Œ∑[s‚àí1 ] + cs . 
2Then , the system starts inner loop translation and writes ws>= 0 target tokens to the output buffer . 
The output buffer is updated to be ys={y1, ... ,yœÑ[s]}whereœÑ[s ] = œÑ[s‚àí1 ] + ws . 
The simultaneous decoding process continues until no more source tokens are added in the outer loop . 
We deÔ¨Åne the last outer step as the terminal outer stepS , and other outer steps as non - terminal outer steps . 
For the outer loop , we make no assumption about the value of cs , while all previous work assumes192 cs= 1 . 
This setting is more realistic because ( i ) increasing cscan reduce the number of outer steps , thus reducing computation cost ; ( ii ) in a real speech translation application , an ASR system may generate multiple tokens at a time . 
For the inner loop , we adapt a pretrained vanilla CNMT model to perform partial translation with two important concerns : 1.PreÔ¨Åx translation : given a source preÔ¨Åx xs= { x1, ... ,xŒ∑[s]}and a target preÔ¨Åx ys œÑ[s‚àí1]= { y1, ... ,yœÑ[s‚àí1 ] } , how to predict the remaining target tokens ? 2.Stopping criterion : since the NMT model is trained with full sentences , how to design the stopping criterion for it when translating partial source sentcnes ? 3.1 PreÔ¨Åx Translation At an outer step s , given encoder hidden states hs for source preÔ¨Åx xs={x1, ... ,xŒ∑[s]}and decoder hidden states zs œÑ[s‚àí1]for target preÔ¨Åx ys œÑ[s‚àí1]= { y1, ... ,yœÑ[s‚àí1 ] } , we perform preÔ¨Åx translation sequentially with a greedy decoder : zs t = œÜDEC(yt‚àí1,zs < t , hs ) p(y|y < t , xs)‚àùexp [ œÜOUT(zs t ) ] yt= argmaxyp(y|y < t , xs ) , ( 4 ) wheretstarts fromt = œÑ[s‚àí1 ] + 1 . 
The preÔ¨Åx translation terminates when a stopping criterion meets , yielding a translation ys={y1, ... ,yœÑ[s ] } . 
However , a major problem comes from the above translation method : how can we obtain the encoder hidden states hsand decoder hidden states zs œÑ[s‚àí1 ] at the beginning of preÔ¨Åx translation ? We propose to rebuild all encoder and decoder hidden states with hs = œÜENC / parenleftbig xs / parenrightbig , ( 5 ) zs œÑ[s‚àí1]=œÜDEC / parenleftbig ys œÑ[s‚àí1],hs / parenrightbig . 
( 6 ) During full sentence training , all the decoder hidden states are computed conditional on the same source tokens . 
By rebuilding encoder and decoder hidden states , we also ensure that the decoder hidden states are computed conditional on the same source . 
This strategy is different from previous work that reuse previous encoder ( Cho and Esipova , 2016 ; Gu et al . 
, 2017 ; Dalvi et al . 
, 2018 ; Alinejad et al . 
, 2018 ) or decoder ( Cho and Esipova , 2016 ; Gu et al . 
, 2017 ; Dalvi et al . 
, 2018 ; Ma et al . 
, 2018 ) 1ÊôìËéπ‚Üíxiaoying2ÊôìËéπ‰Ω†‚Üí xiaoyingyou3ÊôìËéπ ‰Ω†Â•Ω‚Üí xiaoying you are good4ÊôìËéπ ‰Ω† Â•Ω„ÄÇ‚Üí xiaoying you are good.src transFigure 1 : Failure case when using EOS alone as the stopping criterion . 
hidden states . 
We carefully compare the effect of rebuilding hidden states in Section 4.2 and experiment results show that rebuilding all hidden states beneÔ¨Åts translation . 
3.2 Stopping Criterion In consecutive NMT , the decoding algorithm such as greedy decoding or beam search terminates when the translator predicts an EOS token or the length of the translation meets a predeÔ¨Åned threshold ( e.g. 200 ) . 
The decoding for most source sentences terminates when the translator predicts the EOS token.1In simultaneous decoding , since we use a NMT model pretrained on full sentences to translate partial source sentences , it tends to predict EOS when the source context has been fully translated . 
However , such strategy could be too aggressive for simultaneous translation . 
Fig . 
1 shows such an example . 
At outer step 2 , the translator predicts ‚Äú you EOS ‚Äù , emiting target token ‚Äú you ‚Äù . 
However , ‚Äú you ‚Äù is not the expected translation for ‚Äú ‰Ω† ‚Äù in the context of ‚Äú ‰Ω†Â•Ω „ÄÇ ‚Äù . 
Therefore , we hope preÔ¨Åx translation at outer step 2can terminate without emitting any words . 
To alleviate such problems and do better simultaneous translation with pretrained CNMT model , we propose two novel stopping criteria for preÔ¨Åx translation . 
3.2.1 Length and EOS Control In consecutive translation , the decoding process stops mainly when predicting EOS . 
In contrast , for preÔ¨Åx translation at non - terminal outer step , we stop the translation process when translation length isdtokens behind source sentence length : œÑ[s ] = Œ∑[s]‚àíd . 
SpeciÔ¨Åcally , at the beginning of outer steps , we have source preÔ¨Åx xs={x1, ... ,xŒ∑[s ] } and target preÔ¨Åx ys œÑ[s‚àí1]={y1, ... ,yœÑ[s‚àí1 ] } . 
PreÔ¨Åx translation terminates at inner step wswhen 1We conduct greedy decoding on the validation set of WMT15 EN ‚ÜíDE translation with fairseq - py , and Ô¨Ånd that 100 % translation terminates with EOS predicted.193 Figure 2 : Framework of our proposed model with the TN controller . 
predicting an EOS token or satisfying : ws=/braceleftBigmax(0,Œ∑[s]‚àíœÑ[s‚àí1]‚àíd)s < S 200‚àíœÑ[s‚àí1]s = S ( 7 ) wheredis a non - negative integer that determines the translation latency of the system . 
We call this stopping criterion as Length and EOS ( LE ) stopping controller . 
3.2.2 Learning When to Stop Although simple and easy to implement , LE controller lacks the capability to learn the optimal timing with which to stop preÔ¨Åx translation . 
Therefore , we design a small trainable network called trainable ( TN ) stopping controller to learn when to stop preÔ¨Åx translation for non - terminal outer step . 
Fig . 
2 shows the illustration . 
At each inner decoding step kfor non - terminal outer steps , the TN controller utilizes a stochastic policyœÄŒ∏parameterized by a neural network to make the binary decision on whether to stop translation at current step : œÄŒ∏(aœÑ[s‚àí1]+k|zs œÑ[s‚àí1]+k ) = fŒ∏(zs œÑ[s‚àí1]+k),(8 ) wherezs œÑ[s‚àí1]+kis the current decoder hidden state . 
We implement fŒ∏with a feedforward network with two hidden layers , followed by a softmax layer . 
The preÔ¨Åx translation stops if the TN controller predictsaœÑ[s‚àí1]+k= 1 . 
Our TN controller is much simpler than previous work ( Gu et al . 
, 2017 ) which implements the READ / WRITE policy network using a recurrent neural network whose input is the combination of the current context vector , the current decoder state and the embedding vector of the candidate word . 
To train the TN controller , we freeze the NMT model with pretrained parameters , and optimizethe TN network with policy gradient for reward maximizationJ = EœÄŒ∏(/summationtextTœÑ t=1rt ) . 
With a trained TN controller , preÔ¨Åx translation stops at inner decoding step wswhen predicting an EOS token or satisfying : /braceleftBigaœÑ[s‚àí1]+ws= 1s < S ws= 200‚àíœÑ[s‚àí1]s‚â§S.(9 ) In the following , we talk about the details of the reward function and the training with policy gradient . 
Reward To trade - off between translation quality and latency , we deÔ¨Åne the reward function at inner decoding step kof outer step sas : rt = rQ t+Œ±¬∑rD t , ( 10 ) wheret = œÑ[s‚àí1]+k , andrQ tandrD tare rewards related to quality and delay , respectively . 
Œ±‚â•0 is a hyper - parameter that we adjust to balance the trade - off between translation quality and delay . 
Similar to Gu et al . 
( 2017 ) , we utilize sentencelevel BLEU ( Papineni et al . 
, 2002 ; Lin and Och , 2004 ) with reward shaping ( Ng et al . 
, 1999 ) as the reward for quality : rQ t=/braceleftBig‚àÜBLEU ( y‚àó,y , t)k / negationslash = wsors / negationslash = S BLEU ( y‚àó,y)k = wsands = S ( 11 ) where ‚àÜBLEU ( y‚àó,y , t ) = BLEU ( y‚àó,yt)‚àíBLEU ( y‚àó,yt‚àí1)(12 ) is the intermediate reward . 
Note that the higher the values of BLEU are , the more rewards the TN controller receives . 
Following Ma et al . 
( 2018 ) , we use average lagging ( AL ) as the reward for latency : rD t=Ô£± Ô£≤ Ô£≥0 k / negationslash = wsors / negationslash = S ‚àí‚åäd(x , y)‚àíd‚àó‚åã+k = wsands = S ( 13 ) where d(x , y ) = 1 teœÑe / summationdisplay t=1l(t)‚àít‚àí1 Œª . 
( 14 ) l(t)is the number of observed source tokens when generating the t - th target token , te=194 Dataset Train Validation Test IWSLT16 193,591 993 1,305 WMT15 3,745,796 3,003 2,169 NIST 1,252,977 878 4,103 Table 1 : # sentences in each dataset . 
argmint(l(t ) = |x|)denotes the earliest point when the system observes the full source sentence , Œª=|y| |x|represents the target - to - source length ratio andd‚àó‚â•0is a hyper - parameter called target delay that indicates the desired system latency . 
Note that the lower the values of AL are , the more rewards the TN controller receives . 
Policy Gradient We train the TN controller with policy gradient(Sutton et al . 
, 1999 ) , and the gradients are : ‚àáŒ∏J = EœÄŒ∏ / bracketleftBiggTœÑ / summationdisplay t=1Rt‚àáŒ∏logœÄŒ∏(at|¬∑)/bracketrightBigg , ( 15 ) whereRt=/summationtextTœÑ i = triis the cumulative future rewards for the current decision . 
We can adopt any sampling approach ( Chen et al . 
, 2017 , 2018 ; Shen et al . 
, 2018 ) to estimate the expected gradient . 
In our experiments , we randomly sample multiple action trajectories from the current policy œÄŒ∏and estimate the gradient with the collected accumulated reward . 
We try the variance reduction techniques by subtracting a baseline average reward estimated by a linear regression model from Rt and Ô¨Ånd that it does not help to improve the performance . 
Therefore , we just normalize the reward in each mini - batch without using baseline reward for simplicity . 
4 Experiments 4.1 Settings Dataset We compare our approach with the baselines on WMT15 German - English2(DE - EN ) translation in both directions . 
This is also the most widely used dataset to evaluate SNMT ‚Äôs performance ( Cho and Esipova , 2016 ; Gu et al . 
, 2017 ; Ma et al . 
, 2018 ; Arivazhagan et al . 
, 2019 ; Zheng et al . 
, 2019 ) . 
To further evaluate our approach ‚Äôs efÔ¨Åcacy in trading off translation quality and latency on other language pair and spoken language , we also 2http://www.statmt.org/wmt15/conduct experiments with the proposed LE and TN methods on NIST Chinese - to - English3(ZH‚ÜíEN ) translation and IWSLT16 German - English4(DEEN ) translation in both directions . 
For WMT15 , we use newstest2014 for validation and newstest2015 for test . 
For NIST , we use MT02 for validation , and MT05 , MT06 , MT08 for test . 
For IWSLT16 , we use tst13 for validation and tst14 for test . 
All the data is tokenized and segmented into subword symbols using byte - pair encoding ( Sennrich et al . 
, 2016 ) to restrict the size of the vocabulary . 
We use 40,000 joint merge operations on WMT15 , and 24,000 on IWSLT16 . 
For NIST , we use 30,000 merge operations for source and target side separately . 
Without explicitly mention , we simulate simultaneous translation scenario at inference time with these datasets by assuming that the system observes one new source token at each outer step , i.e. ,cs= 1 . 
Table 1 shows the data statistics . 
Pretrained NMT Model We use Transformer ( Vaswani et al . 
, 2017 ) trained with maximum likelihood estimation as the pretrained CNMT model and implement our method based on fairseq - py.5We follow the setting in transformer iwslt deen for IWSLT16 dataset , and transformer wmtende for WMT15 and NIST dataset . 
Fairseq - py adds an EOS token for all source sentences during training and inference . 
Therefore , to be consistent with the CNMT model implemented with fairseq - py , we also add an EOS token at the end of the source preÔ¨Åx for preÔ¨Åx translation and Ô¨Ånd that the EOS helps translation . 
TN Controller To train the TN controller , we use a mini - batch size of 8,16,16 and sample 5,10,10 trajectories for each sentence pair in a batch for IWSLT16 , WMT15 and NIST , respectively . 
We set the number of newly observed source tokens at each outer step to be 1during the training for simplicity . 
We set Œ±to be 0.04 , andd‚àóto be 2,5,8 . 
All our TN controllers are trained with policy gradient using Adam optimizer ( Kingma and Ba , 2015 ) with 30,000 updates . 
We select the last model as our Ô¨Ånal TN controller . 
Baseline We compare our model against three baselines that utilize a pretrained CNMT model to 3These sentence pairs are mainly extracted from LDC2002E18 , LDC2003E07 , LDC2003E14 , Hansards portion of LDC2004T07 , LDC2004T08 and LDC2005T06 4https://workshop2016.iwslt.org/ 5https://github.com/pytorch/fairseq195 Figure 3 : Comparison with the baselines on the test set of WMT15 EN ‚ÜíDE and WMT15 DE ‚ÜíEN translations . 
The shown points from left to right on the same line are the results of simultaneous greedy decoding with d‚àó‚àà { 2,5,8}for TN , d‚àà{0,2,4,6,8}for LE , œÅ‚àà{0.65,0.6,0.55,0.5,0.45,0.4}for SL , k‚àà{1,3,5,7,9}for testtime waitk andCW‚àà{2,5,8}for RWAgent . 
The scores of Greedy decoding : BLEU= 25.16 , AL= 28.10for WMT15 EN‚ÜíDE translation and BLEU= 26.17 , AL= 31.20for WMT15 DE‚ÜíEN translation . 
Figure 4 : Performance on the test set of IWSLT16 EN ‚ÜíDE translation , IWSLT16 DE ‚ÜíEN translation and NIST ZH‚ÜíEN translation . 
The shown points from left to right on the same line are the results of d‚àó‚àà{2,5,8}for TN andd‚àà{0,2,4,6,7}for LE . 
# $ : full - sentence ( greedy and beam - search ) . 
perform simultaneous translation : ‚Ä¢testtime waitk ( Ma et al . 
, 2018 ): the method that decodes with a waitk policy with a CNMT model . 
We report the results when k‚àà { 1,3,5,7,9 } . 
‚Ä¢SL(Zheng et al . 
, 2019 ): the method that adapts CNMT to SNMNT by learning an adaptive READ / WRITE policy from oracle READ / WRITE sequences generated with heuristics . 
We report the results with thresholdœÅ‚àà{0.65,0.6,0.55,0.5,0.45,0.4 } . 
‚Ä¢RWAgent ( Gu et al . 
, 2017 ): the adaptation of Gu et al . 
( 2017 ) ‚Äôs full - sentence model and reinforced READ / WRITE policy network to Transformer by Ma et al . 
( 2018 ) . 
We report the results when using CW‚àà{2,5,8}as the target delay . 
We report the result with d‚àà{0,2,4,6,8}for our proposed LE method and d‚àó‚àà{2,5,8}for our proposed TN method . 
For all baselines , we cite the results reported in Zheng et al . 
( 2019).6 4.2 Results We compare our methods with the baselines on the test set of WMT15 EN ‚ÜíDE and DE‚ÜíEN translation tasks , as shown in Fig . 
3 . 
The points closer to the upper left corner indicate better overall performance , namely low latency and high quality . 
We observe that as latency increases , all methods improve in quality . 
the TN method signiÔ¨Åcantly outperforms all the baselines in both translation tasks , 6Since Zheng et al . 
( 2019 ) did not mention the details of data preprocessing , we can not compare the BLEU and AL scores directly with theirs . 
Therefore , we normalize the BLEU and AL scores with its corresponding upper bound , i.e. the BLEU and AL scores obtained when the pretrained Transformer performs standard greedy decoding ( Greedy ) .196 Figure 5 : Comparison of whether to reuse previous encoder or decoder hidden states on WMT15 EN ‚ÜíDE test set with the LE controller . 
The left Y axis is the BLEU score and the right Y axis is the length ratio : the translation length divided by the reference length . 
The points on the same line are the results of d‚àà{0,2,4,6,8}.none : rebuild all encoder / decoder hidden states ; decoder : reuse decoder hidden states and rebuild all encoder hidden states ; encoder : reuse previous encoder hidden states and rebuild all decoder hidden states . 
demonstrating that it indeed learns the appropriate timing to stop preÔ¨Åx translation . 
LE outperforms the baselines on WMT15 EN ‚ÜíDE translation at high latency region and performs similarly or worse on other cases . 
We show the methods ‚Äô efÔ¨Åcacy in trading off quality and latency on other language pair and spoken language in Fig . 
4 . 
TN outperforms LE on all translation tasks , especially at the low latency region . 
It obtains promising translation quality with acceptable latency : with a lag of < 7tokens , TN obtains 96.95 % , 97.20 % and 94.03 % BLEU with respect to consecutive greedy decoding for IWSLT16 EN‚ÜíDE , IWSLT16 DE‚ÜíEN and NIST ZH‚ÜíEN translations , respectively . 
4.3 Analyze We analyze the effect of different ways to obtain the encoder and decoder hidden states at the beginning of preÔ¨Åx translation with the LE controller . 
Fig . 
5 shows the result . 
We try three variants : a ) dynamically rebuild all encoder / decoder hidden states ( none ) ; b ) reuse decoder hidden states and rebuild all encoder hidden states ( decoder ) ; c ) reuse previous encoder hidden states and rebuild all decoder hidden states ( encoder ) . 
The left Y axis and X axis show BLEU - vs - AL curve . 
We observe that if reusing previous encoder hidden states ( encoder ) , the translation fails . 
We ascribe this to the discrepancy between training and decoding for the encoder . 
We also observe that when d‚àà0,2 , reusing decoder hidden states ( decoder ) obtain negative AL.To analyze this , we plot the translation to reference length ratio versus AL curve with the right Y axis and X axis . 
It shows that with decoder , the decoding process stops too early and generates too short translations . 
Therefore , to avoid such problem and to be consistent with the training process of the CNMT model , it is important to dynamically rebuild all encoder / decoder hidden states for preÔ¨Åx translation . 
Since we make no assumption about the cs , i.e. , the number of newly observed source tokens at each outer step , we also test the effect of differentcs . 
Fig . 
6 shows the result with the LE and TN controllers on the test set of WMT15 EN ‚ÜíDE translation . 
We observe that as csincreases , both LE and TN trend to improve in quality and worsen in latency . 
When cs= 1 , LE controller obtains the best balance between quality and latency . 
In contrast , TN controller obtains similar quality and latency balance with different cs , demonstrating that TN controller successfully learns the right timing to stop regardless of the input update schedule . 
We also analyze the TN controller ‚Äôs adaptability by monitoring the initial delay , i.e. , the number of observed source tokens before emitting the Ô¨Årst target token , on the test set of WMT15 EN ‚ÜíDE translation , as shown in Fig . 
7 . 
d‚àóis the target delay measured with AL ( used in Eq . 
13 ) . 
It demonstrates that the TN controller has a lot of variance in it ‚Äôs initial delay . 
The distribution of initial delay changes with different target delay : with higher target delay , the average initial delay is larger . 
For most sentences , the initial delay is within 1‚àí7 . 
In speech translation , listeners are also concerned with long silences during which no translation occurs . 
Following Gu et al . 
( 2017 ) ; Ma et al . 
( 2018 ) , we use Consecutive Wait ( CW ) to measure this : CW(x , y ) = /summationtextS s=1cs / summationtextS s=1 1ws>0 . 
( 16 ) Fig . 
8 shows the BLEU - vs - CW plots for our proposed two methods . 
The TN controller has higher CW than the LE controller . 
This is because TN controller prefers consecutive updating output buffer ( e.g. , it often produces wsas 0 0 0 0 3 0 0 0 0 0 5 0 0 0 0 4 ... ) while the LE controller often updates its output buffer following the input buffer ( e.g. , it often produces wsas 0 0 0 0 1 1 1 1 1 1 ... whend= 4 ) . 
Although larger than LE , the CW for TN ( < 6 ) is acceptable for most speech translation scenarios.197 Figure 6 : Performance on the test set of WMT15 EN ‚ÜíDE translation with different input buffer update schedule . 
Points on the same line are obtained by increasing d‚àà0,2,4,6,8for ( a ) andd‚àó‚àà2,5,8for ( b ) . 
Figure 7 : Number of observed source tokens before emitting the Ô¨Årst target token for the TN controller on the test set of WMT15 EN‚ÜíDE translation . 
Figure 8 : Average consecutive write length on the test set of WMT15 EN‚ÜíDE translation . 
4.4 Translation Examples Fig . 
9 shows two translation examples with the LE and TN controllers on the test set of NIST ZH ‚ÜíEN and WMT15 EN‚ÜíDE translation . 
In manual inspection of these examples and others , we Ô¨Ånd that the TN controller learns a conservative timing for stopping preÔ¨Åx translation . 
For example , in example 1 , TN outputs translation ‚Äú wu bangguo attended the signing ceremony ‚Äù when observing ‚Äú Âê¥ÈÇ¶ÂõΩÂá∫Â∏≠Á≠æÂ≠ó‰ª™ÂºèÂπ∂ ‚Äù , instead of a more radical translation ‚Äú wu bangguo attended the signing ceremony and ‚Äù . 
Such strategy helps to alleviate the problem of premature translation , i.e. , translating before observing enough future context.5 Related Work A number of works in simultaneous translation divide the translation process into two stages . 
A segmentation component Ô¨Årst divides the incoming text into segments , and then each segment is translated by a translator independently or with previous context . 
The segmentation boundaries can be predicted by prosodic pauses detected in speech ( F¬®ugen et al . 
, 2007 ; Bangalore et al . 
, 2012 ) , linguistic cues ( Sridhar et al . 
, 2013 ; Matusov et al . 
, 2007 ) , or a classiÔ¨Åer based on alignment information ( Siahbani et al . 
, 2014 ; Yarmohammadi et al . 
, 2013 ) and translation accuracy ( Oda et al . 
, 2014 ; Grissom et al . 
, 2014 ; Siahbani et al . 
, 2018 ) . 
Some authors have recently endeavored to perform simultaneous translation in the context of NMT . 
Niehues et al . 
( 2018 ) ; Arivazhagan et al . 
( 2020 ) adopt a re - translation approach where the source is repeatedly translated from scratch as it grows and propose methods to improve translation stability . 
Cho and Esipova ( 2016 ) ; Dalvi et al . 
( 2018 ) ; Ma et al . 
( 2018 ) introduce a manually designed criterion to control when to translate . 
Satija and Pineau ( 2016 ) ; Gu et al . 
( 2017 ) ; Alinejad et al . 
( 2018 ) extend the criterion into a trainable agent198 1 2 3 4 5 6 7 8 9 Âê¥ÈÇ¶ÂõΩÂá∫Â∏≠Á≠æÂ≠ó‰ª™ÂºèÂπ∂ Âú®ÂçèËÆÆ‰∏ä Á≠æÂ≠ó LE wu bangguo attended the signing ceremony and signed the agreement TN wu bangguo attended the signing ceremony and signed the agreement Greedy wu bangguo attended the signing ceremony and signed the agreement Ref wu bangguo attends signing ceremony and signs agreement NATO does not want to break agreements with Russia LE Die NATO m ¬®ochte keine Abkommen mit Russland brechen TN Die NATO will keine Abkommen mit Russland brechen Greedy Die NATO m ¬®ochte keine Abkommen mit Russland brechen Ref NATO will Vereinbarungen mit Russland nicht brechen Figure 9 : Translation examples from the test set of NIST ZH ‚ÜíEN ( example 1 ) and WMT15 EN ‚ÜíDE translation ( example 2 ) . 
We compare LE with d= 4 and TN with d‚àó= 5 because these two models achieve similar latency . 
Greedy and Ref represent the greedy decoding result from consecutive translation and the reference , respectively . 
in a reinforcement learning framework . 
However , these work either develop sophisticated training frameworks explicitly designed for simultaneous translation ( Ma et al . 
, 2018 ) or fail to use a pretrained consecutive NMT model in an optimal way ( Cho and Esipova , 2016 ; Dalvi et al . 
, 2018 ; Satija and Pineau , 2016 ; Gu et al . 
, 2017 ; Alinejad et al . 
, 2018 ; Zheng et al . 
, 2019 ) . 
In contrast , our work is signiÔ¨Åcantly different from theirs in the way of using pretrained consecutive NMT model to perform simultaneous translation and the design of the two stopping criteria . 
6 Conclusion We have presented a novel framework for improving simultaneous translation with a pretrained consecutive NMT model . 
The basic idea is to translate partial source sentence with the consecutive NMT model and stops the translation with two novel stopping criteria . 
Extensive experiments demonstrate that our method with trainable stopping controller outperforms the state - of - the - art baselines in balancing between translation quality and latency . 
Acknowledgments We thank the anonymous reviewers for their insightful feedback on this work . 
Yun Chen is partially supported by the Fundamental Research Funds for the Central Universities and the funds of Beijing Advanced Innovation Center for Language Resources ( No . 
TYZ19005 ) . 
Abstract Chinese and Japanese share many characters with similar surface morphology . 
To better utilize the shared knowledge across the languages , we propose UnihanLM , a self - supervised Chinese - Japanese pretrained masked language model ( MLM ) with a novel two - stage coarse - to-Ô¨Åne training approach . 
We exploit Unihan , a ready - made database constructed by linguistic experts to Ô¨Årst merge morphologically similar characters into clusters . 
The resulting clusters are used to replace the original characters in sentences for the coarse - grained pretraining of the MLM . 
Then , we restore the clusters back to the original characters in sentences for the Ô¨Ånegrained pretraining to learn the representation of the speciÔ¨Åc characters . 
We conduct extensive experiments on a variety of Chinese and Japanese NLP benchmarks , showing that our proposed UnihanLM is effective on both mono- and cross - lingual Chinese and Japanese tasks , shedding light on a new path to exploit the homology of languages.1 1 Introduction Recently , Pretrained Language Models have shown promising performance on many NLP tasks ( Peters et al . 
, 2018 ; Devlin et al . 
, 2019 ; Liu et al . 
, 2019 ; Yang et al . 
, 2019c ; Lan et al . 
, 2020 ) . 
Many attempts have been made to train a model that supports multiple languages . 
Among them , Multilingual BERT ( mBERT ) ( Devlin et al . 
, 2019 ) is released as a part of BERT . 
It directly adopts the same model architecture and training objective , and is trained on Wikipedia in different languages . 
XLM ( Lample and Conneau , 2019 ) is proposed with an additional language embedding and a new training ‚àóThis work was done during Canwen ‚Äôs internship at Microsoft Research Asia . 
1The code and pretrained weights are available at https : //github.com / JetRunner / unihan - lm .JA / uni53F01 / uni98A82 / uni306F / uni71B13 / uni5E2F4 / uni4F4E / uni6C175 / uni57276 / uni306E / uni4E00 / uni7A2E7 / uni3067 / uni3059 / uni3002 T - ZHÈ¢±1È¢®2ÊòØÁÜ±3Â∏∂4Ê∞£5ÊóãÁöÑ‰∏ÄÁ®Æ7 „ÄÇ S - ZHÂè∞1È£é2ÊòØÁÉ≠3Â∏¶4‰ΩéÊ∞î5Âéã6ÁöÑ‰∏ÄÁßç7 „ÄÇ EN Typhoon is a type of tropical depression . 
Table 1 : A sentence example in Japanese ( JA ) , Traditional Chinese ( T - ZH ) and SimpliÔ¨Åed Chinese ( S - ZH ) with its English translation ( EN ) . 
The characters that already share the same Unicode are marked with an underline . 
In this work , we further match characters with identical meanings but different Unicode , then merge them . 
Characters eligible to be merged together are marked with the same superscript . 
objective ( translation language modeling , TLM ) . 
XLM - R ( Conneau et al . 
, 2019 ) has a larger size and is trained with more data . 
Based on XLM , Unicoder ( Huang et al . 
, 2019 ) collects more data and uses multi - task learning to train on three supervised tasks . 
The census of cross - lingual approaches is to allow lexical information to be shared between languages . 
XLM and mBERT exploit shared lexical information by Byte Pair Encoding ( BPE ) ( Sennrich et al . 
, 2016 ) and WordPiece ( Wu et al . 
, 2016 ) , respectively . 
However , these automatically learned shared representations have been criticized by recent work ( K et al . 
, 2020 ) , which reveals their limitations in sharing meaningful semantics across languages . 
Also , words in both Chinese and Japanese are short , which prohibits an effective learning of sub - word representations . 
Different from European languages , Chinese and Japanese naturally share Chinese characters as a subword component . 
Early work ( Chu et al . 
, 2013 ) shows that shared characters in these two languages can beneÔ¨Åt Examplebased Machine Translation ( EBMT ) with a statistical based phrase extraction and alignment . 
For Neural Machine Translation ( NMT ) , ( Zhang and Ko-201 machi , 2019 ) exploited such information by learning a BPE representation over sub - character ( i.e. , ideograph and stroke ) sequence . 
They applied this technique to unsupervised Chinese - Japanese machine translation and achieved state - of - the - art performance . 
However , this approach greatly relies on unreliable automatic BPE learning and may suffer from the noise brought by various variants . 
To facilitate lexical sharing , we propose Unihan Language Model ( UnihanLM ) , a cross - lingual pretrained masked language model for Chinese and Japanese . 
We propose a two - stage coarse - to-Ô¨Åne pretraining procedure to empower better generalization and take advantages of shared characters in Japanese , Traditional and SimpliÔ¨Åed Chinese . 
First , we let the model exploit maximum possible shared lexical information . 
Instead of learning a shared sub - word vocabulary like the prior work , we leverage Unihan database ( Jenkins et al . 
, 2019 ) , a ready - made constituent of the Unicode standard , to extract the shared lexical information across the languages . 
By exploiting this database , we can effectively merge characters with the similar surface morphology but independent Unicodes , as shown in Table 1 into thousands of clusters . 
The clusters will be used to replace the characters in sentences during the Ô¨Årst - stage coarse - grained pretraining . 
After the coarse - grained pretraining Ô¨Ånishes , we restore the clusters back to the original characters and initialize their representation with their corresponding cluster ‚Äôs representation and then learn their speciÔ¨Åc representation during the second - stage Ô¨Ånegrained pretraining . 
In this way , our model can make full use of shared characters while maintaining a good sense for nuances of similar characters . 
To verify the effectiveness of our approach , we evaluate on both lexical and semantic tasks in Chinese and Japanese . 
On word segmentation , our model outperforms monolingual and multilingual BERT ( Devlin et al . 
, 2019 ) and shows a much higher performance on cross - lingual zeroshot transfer . 
Also , our model achieves state - of - theart performance on unsupervised Chinese - Japanese machine translation , and is even comparable to the supervised baseline on Chinese - to - Japanese translation . 
On classiÔ¨Åcation tasks , our model achieves a comparable performance with monolingual BERT and other cross - lingual models trained with the same scale of data . 
To summarize , our contributions are three - fold : ( 1 ) We propose UnihanLM , a cross - lingual pre - trained language model for Chinese and Japanese NLP tasks . 
( 2 ) We pioneer to apply the language resource ‚Äì the Unihan Database to help model pretraining , allowing more lexical information to be shared between the two languages . 
( 3 ) We devise a novel coarse - to-Ô¨Åne two - stage pretraining strategy with different granularity for Chinese - Japanese language modeling . 
2 Preliminaries 2.1 Chinese Character Chinese character is a pictograph used in Chinese and Japanese . 
These characters often share the same background and origin . 
However , due to historic reasons , Chinese characters have developed into different writing systems , including Japanese Kanji , Traditional Chinese and SimpliÔ¨Åed Chinese . 
Also , even in a single text , multiple variants of the same characters can be used interchangeably ( e.g. , ‚Äú Âè∞ÁÅ£ ‚Äù and ‚Äú Ëá∫ÁÅ£ ‚Äù for ‚Äú Taiwan ‚Äù , in Traditional Chinese ) . 
These characters have identical or overlapping meanings . 
Thus , it is critical to better exploit such information for modeling both cross - lingual ( i.e. , between Chinese and Japanese ) , cross - system ( i.e. , between Traditional and SimpliÔ¨Åed Chinese ) and cross - variant semantics . 
Both Chinese and Japanese have no delimiter ( e.g. , white space ) to mark the boundaries of words . 
There have always been debates over whether word segmentation is necessary for Chinese NLP . 
Recent work ( Li et al . 
, 2019 ) concludes that it is not necessary for various NLP tasks in Chinese . 
Previous cross - lingual language models use different methods for tokenization . 
mBERT adds white spaces around Chinese characters and lefts Katakana / Hiragana Japanese ( also known as kanas ) unprocessed . 
Different from mBERT , XLM uses Stanford Tokenizer2and KyTea3to segment Chinese and Japanese sentences , respectively . 
After tokenization , mBERT and XLM use WordPiece ( Wu et al . 
, 2016 ) and Byte Pair Encoding ( Sennrich et al . 
, 2016 ) for sub - word encoding , respectively . 
Nevertheless , both approaches suffer from obvious drawbacks . 
For mBERT , the kanas and Chinese characters are treated differently , which causes a mismatch for labeling tasks . 
Also , leaving kanas untokenized may cause the data sparsity problem . 
For XLM , as pointed out in ( Li et al . 
, 2019 ) , an 2https://nlp.stanford.edu/software/ tokenizer.html 3http://www.phontron.com/kytea/202 Variant Description Example Traditional Variant The traditional versions of a simpliÔ¨Åed Chinese character . 
Âèë‚ÜíÈ´Æ(hair),Áôº(to burgeon ) SimpliÔ¨Åed Variant The simpliÔ¨Åed version of a traditional Chinese character . 
Âúò‚ÜíÂõ¢(group ) Z - Variant Same character with different unicodes only for compatibility . 
Ë™™‚Üî/uni8AAC(say ) Semantic Variant Characters with identical meaning . 
/uni514E‚ÜîÂÖî(rabbit ) Specialized Semantic Variant Characters with overlapping meaning . 
/uni4E3C(rice bowl , well)‚Üî‰∫ï(well ) Table 2 : The Ô¨Åve types of variants in the Unihan database . 
external word segmenter would introduce extra segmentation errors and compromise the performance of the model . 
Also , as a word - based model , it is difÔ¨Åcult to share cross - lingual characters unless the segmented words in both Chinese and Japanese are exactly matched . 
Furthermore , both approaches would enlarge the vocabulary size and thus introduce more parameters . 
2.2 Unihan Database Chinese , Japanese and Korean ( CJK ) characters share a common origin from the ancient Chinese characters . 
However , with the development of each language , both the shape and semantics of characters drastically change . 
When exchanging information , different codings of the same character hinders the text processing . 
Thus , as the result of Han uniÔ¨Åcation4 , the database of CJK UniÔ¨Åed Ideographs , Unihan ( Jenkins et al . 
, 2019 ) , is constructed by human experts tracing the sources of each character . 
As part of the Unicode Standard , Unihan merges the Unicode for some characters from different languages and provides extra variant information between different characters . 
In previous studies ( Zhang and Komachi , 2019 ; Lample and Conneau , 2019 ; Devlin et al . 
, 2019 ) , Unicode is used by default . 
However , due to the ‚Äú Source Separation Rule ‚Äù of Unicode , to remain the compatibility with prior encoding systems , a single character can have multiple Unicodes with different glyphs . 
For example , for the character ‚Äú Êà∂ ‚Äù , there are three unicodes : U+6236 , U+6237 and U+6238 . 
This feature could be useful for message exchange but is undoubtedly undesirable for NLP and may bring the problems of data sparsity and prevent the alignment of a crosslingual language model . 
Fortunately , Unihan database also provides 12,373 entries of variant information in Ô¨Åve types , as listed in Table 2 . 
Note that one character may have multiple types of variants and each type may 4https://en.wikipedia.org/wiki/Han _ unificationTokenization Scheme Result BERT ( 2019 ) /uni53F0 / uni98A8//uni306F / uni3072 / uni3069 / uni3044 XLM ( 2019 ) /uni53F0 / uni98A8//uni306F//uni3072 / uni3069 / uni3044 UnihanLM /uni53F0//uni98A8//uni306F//uni3072//uni3069//uni3044 Table 3 : Different tokenization schemes used in recent work and ours . 
Note that the tokenized results of both BERT and XLM in this table are before WordPiece / BPE applied . 
WordPiece / BPE may further split a token . 
have multiple variant characters ( e.g. , the traditional variants of ‚Äú Âèë ‚Äù in Table 2 ) . 
Such information forms a complex graph structure . 
3 UnihanLM In this section , we introduce the tokenization , character merging and training procedure for our proposed UnihanLM . 
3.1 Tokenization As analyzed in Section 2.1 , the tokenization scheme is tricky and critical for East Asian languages . 
Although recent work ( Li et al . 
, 2019 ) reveals that tokenization is unnecessary for most highlevel NLU and NLG tasks , many downstream labeling tasks ( e.g. , Part - of - speech Tagging , Named Entity Recognition ) still require an implicit or explicit segmentation . 
To enable all NLP tasks , we tokenize the sentences by treating every character ( including Japanese Kana ) as a token . 
Thus , our model is capable of processing all tasks , from the lowest - level Chinese and Japanese word segmentation to high - level NLU tasks . 
We summarize the different tokenization schemes used in recent work and ours in Table 3 . 
We do not further apply BPE to our tokenized sentences for two reasons . 
First , a character is the atomic element in both Chinese and Japanese grammars which should not be further split . 
Second , character itself is naturally a sub - word semantic element , e.g. , ‚Äú Ëá™ ‚Äù ( self ) + ‚Äú ‰ø°‚Äù(belief ) = ‚Äú Ëá™‰ø°‚Äù203 U+53F0U+6AAFU+81FAU+98B1TraditionalSimpliÔ¨ÅedTraditionalTraditionalTraditionalSemanticU+5113SemanticU+67B1SemanticSimpliÔ¨ÅedSimpliÔ¨Åed SemanticFigure 1 : A connected subgraph of Unihan database . 
For example , for the word ‚Äú typhoon ‚Äù , ‚Äú Âè∞ ‚Äù is used in Japanese and SimpliÔ¨Åed Chinese while ‚Äú È¢± ‚Äù is used in Traditional Chinese . 
( conÔ¨Ådence ) ; ‚Äú Ëá™ ‚Äù ( self ) + ‚Äú Â∞ä‚Äù(respect ) = ‚Äú Ëá™Â∞ä ‚Äù ( self - esteem ) . 
3.2 Character Merging To reduce the vocabulary size and align the Chinese characters in Traditional Chinese , SimpliÔ¨Åed Chinese and Japanese to the greatest extent , it is important to merge as many characters as possible while ensuring only merging characters with the identical or overlapping meanings . 
Thus , we use Unihan database , which includes character variant information collected and approved by human experts . 
We use four types of variants including Traditional Variant , SimpliÔ¨Åed Variant , Z - Variant and Semantic Variant . 
Note that we exclude Specialized Semantic Variant which may raise ambiguity problem since it is not very common and the semantics of the two characters are merely overlapping , not identical . 
However , merging characters is still challenging since the variant information in Unihan database is a complex graph , as illustrated in Figure 1 . 
To merge the characters as much as possible , we convert Unihan database to a large undirected graph and use Union Find Algorithm ( Galler and Fischer , 1964 ) to Ô¨Ånd all maximal connected subgraph . 
For example , the whole Figure 1 is a subgraph in the Unihan graph found by the algorithm . 
We call all characters in a maximal connected subgraph belong to a ‚Äú cluster ‚Äù . 
After this merging procedure , the 12,373 variant entries yield a total of 4,001 clusters . 
3.3 Training Procedure As illustrated in Figure 2 , the model is a Transformer based model with three embeddings as inTransformerPositionEmbedding>V@>0$6.@ŒÖ>0$6.@ÕøÕö#1Œà -$-$-$-$-$-$-$++++++++++++++LanguageEmbedding CharacterEmbeddingClusterEmbedding>V@-$++ TransformerPositionEmbedding>V@›£>0$6.@ŒÖŒà>0$6.@ÕöÊ∞±Õø -$-$-$-$-$-$-$++++++++++++++LanguageEmbedding>V@-$++    &OXVWHUOHYHO3UHWUDLQLQJ    &KDUDFWHUOHYHO3UHWUDLQLQJCharacter EmbeddingInitialization#1›£Figure 2 : The model architecture of UnihanLM . 
( 1 ) We merge characters to clusters and use cluster indices when doing cluster - level pretraining . 
In the Ô¨Ågure , ‚Äú # 1 ‚Äù and ‚Äú # 2 ‚Äù indicate indices of the clusters which ‚Äú Âè∞ ‚Äù and ‚Äú È¢® ‚Äù belong to , respectively . 
( 2 ) We initialize the embedding of each character in a cluster with the cluster embedding and do character - level pretraining to predict each character . 
put and the training procedure is composed of two phases . 
3.3.1 Model Our model is a Transformer - based Masked Language Model ( Devlin et al . 
, 2019 ) which learns to predict the randomly masked words with the context . 
Also , following ( Lample and Conneau , 2019 ) , we add language embedding to help the model distinguish between Chinese and Japanese , especially when we share the characters between these two languages . 
The detailed hyperparameter settings are described in Section 4.1 . 
3.3.2 Coarse - grained Cluster - level Pretraining To maximize the shared lexicon and force them to share a representation , we leverage clusters to pretrain our models on a coarse - grained cluster level.204 We Ô¨Årst append the cluster indices to the vocabulary . 
During cluster - level pretraining , we substitute the character index with its corresponding cluster index if the character is in the Unihan database . 
For Japanese kanas , punctuation , number and other characters not in Unihan database , we keep its original token index . 
In this way , we employ human prior knowledge to the pretraining procedure and allow the model to roughly model the semantic knowledge . 
3.3.3 Fine - grained Character - level Pretraining Although the clusters training is effective , there are two problems remaining unsolved . 
First , Traditional Variant could be ambiguous . 
As shown in Table 2 , a character ( most likely one used in SimpliÔ¨Åed Chinese ) may have multiple Traditional Variants . 
Although it should not have a signiÔ¨Åcant negative effect for understanding the language ( since a SimpliÔ¨Åed Chinese user can disambiguate between different meanings of a character based on its context ) , it still makes sense to improve the overall performance by distinguish the characters explicitly ( Navigli et al . 
, 2017 ) . 
Also , in tasks involving decoding ( e.g. , machine translation ) , they must be processed independently . 
Thus , character disambiguation can be naturally used as a selfsupervised task . 
Second , when using the trained model for translation , it would be important for the model to decode the right character for different languages and writing systems . 
For example , for the word meaning ‚Äú typhoon ‚Äù , ‚Äú /uni53F0 / uni98A8 ‚Äù , ‚Äú È¢±È¢® ‚Äù , ‚Äú Âè∞ È£é ‚Äù should be used in Japanese , Traditional Chinese and SimpliÔ¨Åed Chinese , respectively . 
Consequently , we leave these nuances of characters to a Ô¨Åne - grained character - level pretraining . 
Since during the cluster - level pretraining , all characters in Unihan database are preserved in the vocabulary but their embedding is untrained , we initialize their embedding with their corresponding cluster embedding trained in cluster - level pretraining stage . 
In the character - level pretraining stage , we discard the clusters in the vocabulary and do not substitute any character since then . 
In this way , the model can handle each character case by case , with a Ô¨Åne granularity . 
We restart the training with a smaller learning rate to allow the model to learn to disambiguate . 
Model # Layer # Param . 
BERT - Mono - ZH ( 2019 ) 12 110 M mBERT ( 2019 ) 12 179 M XLM ( 2019 ) 16 571 M UnihanLM 12 176 M Table 4 : The numbers of layers and parameters for different models . 
4 Experiments In this section , we compare UnihanLM with other self - supervised pretrained language models . 
All of our baselines ( monolingual BERT , mBERT and XLM ) use Wikipedia for self - supervised pretraining . 
Note that we do not compare our model to XLM - R ( Conneau et al . 
, 2019 ) and Unicoder ( Huang et al . 
, 2019 ) since they are trained with much more data and even on supervised tasks . 
4.1 Training Details We use the mixture of Chinese and Japanese Wikipedia5as the unparalleled pretraining corpus . 
We sample 5,000sentences as validation set for model selection and use the rest for training . 
Our model uses 12 layers of Transformer blocks with 16 attention heads . 
The hidden size is set to 1,024 . 
The vocabulary size is 24,044 . 
Shown in Table 4 , our model has a similar size to mBERT . 
We train our model on 8 Nvidia V100 32 GB GPUs to optimize Masked Language Model ( MLM ) objective ( Devlin et al . 
, 2019 ) with an Adam ( Kingma and Ba , 2015 ) optimizer . 
The masking probability is set to 15 % . 
We add a L2 regularization of 0.01 . 
We warm up the Ô¨Årst 30,000 steps for each stage of pretraining by an inverse square root function . 
The batch size is set to 64 per GPU . 
The maximum sequence length is limited to 256tokens . 
We add dropout ( Srivastava et al . 
, 2014 ) for both feedforward network and attention with a drop rate of 0.1 . 
The learning rate for cluster - level pretraining is set to 1√ó10‚àí4 . 
After 264 hours of clusterlevel pretraining until convergence , we perform character - level pretraining with a smaller learning rate of 5√ó10‚àí5for another 43hours . 
We choose the best model according to its perplexity on validation set . 
For downstream tasks ( to be detailed shortly ) , we Ô¨Åne - tune UnihanLM with a learning rate of 5√ó10‚àí7,1√ó10‚àí4,2.5√ó10‚àí5and a batch 5https://dumps.wikimedia.org/205 Method PKU ( ZH ) KWDLC ( JA ) Standard training mBERT ( 2019 ) 95.0 96.3 BERT - Mono - ZH ( 2019 ) 96.5 UnihanLM 96.6 98.2 Cross - lingual zero - shot transfer mBERT ( 2019 ) 82.0 63.1 UnihanLM 85.7 74.1 Table 5 : F1 scores on Chinese Word Segmentation ( CWS ) and Japanese Word Segmentation ( JWS ) tasks . 
‚Äú Cross - lingual zero - shot transfer ‚Äù indicates that the model is trained on CWS and zero - shot tested on JWS , vice versa . 
size of 20,24,16for word segmentation , unsupervised machine translation and classiÔ¨Åcation tasks , respectively . 
4.2 Word Segmentation Word segmentation is a fundamental task in both Chinese and Japanese NLP . 
It is often recognized as the Ô¨Årst step for further processing in many systems . 
Thus , we evaluate Chinese Word Segmentation ( CWS ) and Japanese Word Segmentation ( JWS ) on PKU dataset ( Emerson , 2005 ) and KWDLC ( Kawahara et al . 
, 2014 ) . 
We use Multilingual BERT and monolingual Chinese BERT ( Devlin et al . 
, 2019 ) as baselines . 
We use pretrained checkpoints provided by Google6 . 
Following previous work , we treat the word segmentation task as a sequence labeling task . 
Note that XLM ( Lample and Conneau , 2019 ) uses pre - segmented sentences as input , making it inapplicable for this task . 
As shown in Table 5 , our proposed UnihanLM outperforms mBERT and monolingual BERT by 1.6and0.1 in terms of F1 score on CWS , respectively . 
On JWS , our model outperforms mBERT by 1.9on F1 . 
Additionally , we conduct zero - shot transfer experiments to determine how much lexical knowledge is shared within Chinese and Japanese for each model . 
We use the weights trained on CWS and JWS for zero - shot transferring on the other language . 
Our model drastically outperforms mBERT on this task by 3.7and 11.0on CWS and JWS , respectively . 
This proves that our model can better capture the lexical knowledge shared between Chinese and Japanese . 
Also , it is notable that zero - shot JWS has a prominently poorer performance than zero - shot CWS . 
As we 6https://github.com/google-research/ bertMethod ZH‚ÜíJA JA‚ÜíZH Supervised baseline OpenNMT ( Klein et al . 
, 2017 ) 42.12 40.63 Fine - tuned on Wikipedia XLM ( Lample and Conneau , 2019 ) 14.58 15.06 UnihanLM 33.53 28.70 Fine - tuned on shufÔ¨Çed ASPEC - JC training set Stroke ( Zhang and Komachi , 2019 ) 33.81 31.66 UnihanLM 44.59 40.58 Table 6 : BLEU scores of Chinese - Japanese unsupervised translation on ASPEC - JC dataset . 
analyze , the criterion for segmenting Chinese characters can be learned with a Japanese corpus and then transferred to CWS . 
However , since no kana is present in CWS , the model can not successfully segment kanas , when performing zero - shot inference on JWS . 
4.3 Unsupervised Machine Translation A Chinese speaker who never learned Japanese can roughly understand a Japanese text ( and vice versa ) , due to the similarity between the writing systems of these two languages . 
On the other hand , only a few parallel corpora between Chinese and Japanese are publicly available , and they are usually small in size . 
Thus , Unsupervised Machine Translation ( UMT ) is very promising and meaningful on the Chinese - Japanese translation task . 
We evaluate on Asian ScientiÔ¨Åc Paper Excerpt Corpus Japanese - Chinese ( ASPEC - JC)7 , the most widely - used Chinese - Japanese Machine Translation dataset . 
We perform our experiments under two settings : ( 1 ) Chinese and Japanese Wikipedia is used as the monolingual corpora , following the setting of ( Lample and Conneau , 2019 ) . 
( 2 ) ShufÔ¨Çed unparalleled ASPEC - JC training set is used as the monolingual corpora , following the settings in ( Zhang and Komachi , 2019 ) . 
Except for XLM , we choose ( Zhang and Komachi , 2019 ) , the current state - of - the - art ChineseJapanese UMT model as a strong baseline . 
They decomposed a Chinese character in both Chinese and Japanese into strokes and then learn a shared token in the stroke sequence to increase the shared tokens in the vocabulary . 
However , this method relies on an unsupervised BPE ( Sennrich et al . 
, 2016 ) to learn shared stroke tokens from a long noisy 7http://orchid.kuee.kyoto-u.ac.jp/ ASPEC/206 MethodPAWS - X ZH JA BOW 54.5 55.1 ESIM ( Chen et al . 
, 2017a ) 60.3 59.6 mBERT ( Devlin et al . 
, 2019 ) 82.3 79.2 XLM ( Lample and Conneau , 2019 ) 82.5 79.5 UnihanLM 82.7 80.5 Table 7 : Accuracy scores on PAWS - X dataset . 
stroke sequence , which is rather unreliable compared to our solution . 
For example , ‚Äú ‰∏ë ‚Äù ( ugly ) and ‚Äú ‰∫î ‚Äù ( Ô¨Åve ) have a very similar stroke sequence but completely different meanings . 
Following ( Lample and Conneau , 2019 ) , we use our pretrained weights to initialize the translation model and train the model with denoising auto - encoding loss and online back - translation loss . 
Note that both baselines use Wikipedia as the unsupervised data and are based on the same UMT method ( Lample et al . 
, 2018c ) . 
We use character - level BLEU ( Papineni et al . 
, 2002 ) as the evaluation metric . 
We demonstrate the results in Table 6 . 
As we analyzed , XLM suffers from a severe out - ofvocabulary ( OOV ) problem on AESPEC - JC , a dataset composed of scientiÔ¨Åc papers , containing many new terminologies which do not show up in the pretraining corpus of XLM . 
As a word - based model , XLM is not able to handle these new words and thus yields a rather poor result . 
When Ô¨Ånetuned on unparalleled training set of ASPEC - JC , our model outperforms the previous state - of - the - art model ( Zhang and Komachi , 2019 ) by a large margin of 10.78and8.92 in terms of BLEU . 
Also notably , UnihanLM even outperforms the supervised baseline on Chinese - to - Japanese translation and has a performance in close proximity on Japaneseto - Chinese task , compared to an early supervised machine translation model , OpenNMT ( Klein et al . 
, 2017 ) , trained on the paired training set of ASPECJC . 
4.4 Text ClassiÔ¨Åcation To further evaluate our model , we perform our experiments on Cross - lingual Paraphrase Aversaries from Word Scrambling ( PAWS - X ) ( Yang et al . 
, 2019b ) , a newly proposed cross - lingual text classiÔ¨Åcation dataset supporting seven languages including Chinese and Japanese . 
This dataset consists of challenging English paraphrase identiÔ¨Åcation pairs from Wikipedia and Quora . 
Then the human trans - lators translate the text into the other six languages . 
We test under the setting of TRANSLATE - TRAIN ( i.e. , we use the provided translation of the training set for both Chinese and Japanese and test in the same language ) . 
Shown in Table 7 , UnihanLM outperforms all baselines in ( Yang et al . 
, 2019b ) , including mBERT . 
4.5 Ablation Study To verify the effectiveness of our two - stage pretraining procedure , we conduct an ablation study . 
A character - level model is trained from scratch without the cluster - level pretraining and marked as ‚Äú ‚àícluster ‚Äù . 
On the other hand , we use the model trained in cluster - level stage for downstream tasks and mark it as ‚Äú ‚àícharacter ‚Äù . 
Note that since the objective for cluster - level stage is to predict the masked cluster , it can not be used for unsupervised translation . 
Shown in Figure 8 , both cluster - level and character - level pretraining play an essential role on classiÔ¨Åcation tasks . 
On translation task , cluster - level pretraining is more important when Ô¨Åne - tuned on Wikipedia but has a relatively smaller impact when using shufÔ¨Çed ASPEC - JC training set . 
To analyze the success of our two - stage training strategy , we would like to emphasize two strengths . 
First , as mentioned before , our easy - to - hard training procedure matches the core idea of Curriculum Learning ( Bengio et al . 
, 2009 ) , which smooths the training and help the model generalize better . 
Second , the two - stage procedure inherently introduces a new self - supervised task , which could take the advantage of Multitask Learning ( Caruana , 1993 ) . 
5 Related Work Multilingual Representation Learning Learning cross - lingual representations are useful for downstream tasks such as cross - lingual classiÔ¨Åcation ( Conneau et al . 
, 2018 ; Yang et al . 
, 2019b ) , cross - lingual retrieval ( Zweigenbaum et al . 
, 2017 ; Artetxe and Schwenk , 2019 ) and cross - lingual QA ( Artetxe et al . 
, 2019 ; Lewis et al . 
, 2019 ; Clark et al . 
, 2020 ) . 
Earlier work on multilingual representations exploiting parallel corpora ( Luong et al . 
, 2015 ; Gouws et al . 
, 2015 ) or a bilingual dictionary to learn a linear mapping ( Mikolov et al . 
, 2013 ; Faruqui and Dyer , 2014 ) . 
Subsequent methods explored self - training ( Artetxe et al . 
, 2017 ) and unsupervised learning ( Zhang et al . 
, 2017 ; Artetxe et al . 
, 2018 ; Lample et al . 
, 2018b ) . 
Recently , multilingual pretrained encoders have shown its effec-207 MethodPAWS - XASPEC - JC Wiki ShufÔ¨Çed - train ZH JA ZH‚ÜíJA JA‚ÜíZH ZH‚ÜíJA JA‚ÜíZH UnihanLM 82.7 80.5 33.53 28.70 44.59 40.58 ‚àícluster 81.5 79.2 29.33 20.93 42.34 39.24 ‚àícharacter 82.0 80.1 - - - Table 8 : The results of ablation study on text classiÔ¨Åcation and UMT . 
‚Äú -cluster ‚Äù and ‚Äú -character ‚Äù indicate the model trained without the cluster - level pretraining and character - level pretraining , respectively . 
The metrics for PAWS - X and ASPEC - JC are accuracy and BLEU , respectively . 
tiveness for learning deep cross - lingual representations ( Eriguchi et al . 
, 2018 ; Pires et al . 
, 2019 ; Wu and Dredze , 2019 ; Lample and Conneau , 2019 ; Conneau et al . 
, 2019 ; Huang et al . 
, 2019 ) . 
Word Segmentation Word segmentation is often formalized as a sequence tagging task . 
It requires lexical knowledge to split a character sequence into a word list that can be used for downstream tasks . 
This step is necessary for many earlier NLP systems for Chinese and Japanese . 
Recent work on Chinese Word Segmentation ( Wang and Xu , 2017 ; Zhou et al . 
, 2017 ; Yang et al . 
, 2017 ; Cai et al . 
, 2017 ; Chen et al . 
, 2017b ; Yang et al . 
, 2019a ) and Japanese Word Segmentation ( Kaji and Kitsuregawa , 2014 ; Fujinuma and II , 2017 ; Kitagawa and Komachi , 2018 ) exploit deep neural networks and focus on building end - to - end sequence tagging models . 
Unsupervised Machine Translation Recently , machine translation systems have demonstrated near human - level performance on some languages . 
However , it depends on the availability of large amounts of parallel sentences . 
Unsupervised Machine Translation addresses this problem by exploiting monolingual corpora which can be easily constructed . 
Lample et al . 
( 2018a ) proposed a UMT model by learning to reconstruct in both languages from a shared feature space . 
Lample et al . 
( 2018c ) exploited language modeling and back - translation and thus proposed a neural unsupervised translation model and a phase - based translation model . 
Different from European languages ( e.g. , English ) , Chinese and Japanese naturally share Chinese characters . 
Zhang and Komachi ( 2019 ) exploited such information by learning a BPE representation over sub - character ( i.e. , ideograph and stroke ) sequence . 
They applied this technique to unsupervised Chinese - Japanese machine translation and achieved state - of - the - art performance . 
This information is also shown to beeffective by ( Xu et al . 
, 2019 ) . 
6 Discussion and Future Work There is still space to improve for our method . 
First , as we analyze , except for Chinese characters , English words often appear in both Chinese and Japanese texts . 
In our current model , they are treated as normal characters without any special processing . 
However , such a rough processing may harm the performance of the model on some tasks . 
For example , in PAWS - X , many entities remain untranslated and this may have a negative effect on the performance of our model . 
Also , loan words ( i.e. , Gairaigo ) , especially from English , constitute a large part of nouns in modern Japanese ( Miller , 1998 ) . 
These words are written with kanas , instead of Chinese characters which makes it inapplicable to be shared with our approach . 
Thus , it may be reasonable to involve English in cross - lingual modeling of Asian languages , as well . 
Similarly , Chinese characters exist in Korean and Vietnamese but are now written in Hangul ( Korean alphabet ) and Vietnamese alphabet , respectively . 
Our future work will explore the possibility to generalize the idea to more Asian languages including Korean and Vietnamese . 
7 Conclusion In this paper , we exploit the ready - made Unihan database constructed by linguistic experts and propose a novel Chinese - Japanese cross - lingual language model trained by a two - stage coarse - to-Ô¨Åne procedure . 
Our extensive experiments on word segmentation , unsupervised machine translation and text classiÔ¨Åcation verify the effectiveness of our model . 
Our approach sheds some light on the linguistic features that receive insufÔ¨Åcient attention recently and showcases a novel way to fuse human linguistic knowledge and exploit the similarity between two languages.208 Acknowledgments We are grateful for the insightful comments from the anonymous reviewers . 
We would like to thank Longtu Zhang and Mamoru Komachi from Tokyo Metropolitan University for their help with the MT baseline . 
Abstract In order to combat overÔ¨Åtting and in pursuit of better generalization , label smoothing is widely applied in modern neural machine translation systems . 
The core idea is to penalize over - conÔ¨Ådent outputs and regularize the model so that its outputs do not diverge too much from some prior distribution . 
While training perplexity generally gets worse , label smoothing is found to consistently improve test performance . 
In this work , we aim to better understand label smoothing in the context of neural machine translation . 
Theoretically , we derive and explain exactly what label smoothing is optimizing for . 
Practically , we conduct extensive experiments by varying which tokens to smooth , tuning the probability mass to be deducted from the true targets and considering different prior distributions . 
We show that label smoothing is theoretically wellmotivated , and by carefully choosing hyperparameters , the practical performance of strong neural machine translation systems can be further improved . 
1 Introduction In recent years , Neural Network ( NN ) models bring steady and concrete improvements on the task of Machine Translation ( MT ) . 
From the introduction of sequence - to - sequence models ( Cho et al . 
, 2014 ; Sutskever et al . 
, 2014a ) , to the invention of the attention mechanism ( Bahdanau et al . 
, 2015 ; Luong et al . 
, 2015 ) , end - to - end sequence learning with attention becomes the dominant design choice for Neural Machine Translation ( NMT ) models . 
From the study of convolutional sequence to sequence learning ( Gehring et al . 
, 2017a , b ) , to the prosperity of self - attention networks ( Vaswani et al . 
, 2017 ; Devlin et al . 
, 2019 ) , modern NMT systems , especially Transformer - based ones ( Vaswani et al . 
, 2017 ) , often deliver state - of - the - art performances(Bojar et al . 
, 2018 ; Barrault et al . 
, 2019 ) , even under the condition of large - scale corpora ( Ott et al . 
, 2018 ; Edunov et al . 
, 2018 ) . 
In Transformer - based models , label smoothing is a widely applied method to improve model performance . 
Szegedy et al . 
( 2016 ) initially introduce the method when making reÔ¨Ånements to the Inception ( Szegedy et al . 
, 2015 ) model , with the motivation to combat overÔ¨Åtting and improve adaptability . 
In principle , label smoothing discounts a certain probability mass from the true label and redistributes it uniformly across all the class labels . 
This lowers the difference between the largest probability output and the others , effectively discouraging the model to generate overly conÔ¨Ådent predictions . 
Since information entropy ( Shannon , 1948 ) can be thought of as a conÔ¨Ådence measure of a probability distribution , Pereyra et al . 
( 2017 ) add a negative entropy regularization term to the conventional cross entropy training criterion and compare it with uniform smoothing and unigram smoothing . 
M ¬®uller et al . 
( 2019 ) deliver further insightful discussions about label smoothing , empirically investigating it in terms of model calibration , knowledge distillation and representation learning . 
Label smoothing itself is an interesting topic that brings insights about the general learnability of a neural model . 
While existing methods are rather heuristical in their nature , the fact that simply discounting some probability mass from the true label and redistributing it with some prior distribution ( see Figure 1 for an illustration ) works in practice , is worth to be better understood . 
In this paper , we raise two high - level research questions to outline our work : 1.Theoretically , what is label smoothing ( or the related conÔ¨Ådence penalty ) optimizing for ? 2.Practically , what is a good recipe in order to apply label smoothing successfully in NMT?212 V v01 Vm(a ) uniform distribution V v0AB ( b ) conÔ¨Ådence penalty V v0rvm ( c ) arbitrary distribution Figure 1 : An illustration of label smoothing with various prior distributions . 
mandBare discounted probabiltiy masses . 
Vis the vocabulary size and v0is the correct target word.1 V , Aandrvare prior distributions . 
Smoothing with ( a),mis equally redistributed across the vocabulary . 
Smoothing with ( b ) , Ais implicitly1 Veverywhere as well , and the exact value of Bcan be obtained ( Section 3.2 ) . 
Smoothing with ( c ) , mgoes to each class in proportion to an arbitrary smoothing prior rv(Section 4.3 ) . 
The presentation of our results is organized into three major sections : ‚Ä¢First , we introduce a generalized formula for label smoothing and derive the theoretical solution to the training problem . 
‚Ä¢Second , we investigate various aspects that affect the training process and show an empirically good recipe to apply label smoothing . 
‚Ä¢Finally , we examine the implications in search and scoring and motivate further research into the mismatch between training and testing . 
2 Related Work The extensive use of NNs in MT ( Bojar et al . 
, 2016 , 2017 , 2018 ; Barrault et al . 
, 2019 ) is a result of many pioneering and inspiring works . 
Continuousvalued word vectors lay the foundation of modern Natural Language Processing ( NLP ) NNs , capturing semantic and syntactic relations and providing numerical ways to calculate meaningful distances among words ( Bengio et al . 
, 2001 ; Schwenk et al . 
, 2006 ; Schwenk , 2007 ; Sundermeyer et al . 
, 2012 ; Mikolov et al . 
, 2013a , b ) . 
The investigations of sequence - to - sequence learning ( Cho et al . 
, 2014 ; Sutskever et al . 
, 2014b ) , the studies of attention mechanism ( Bahdanau et al . 
, 2015 ; Luong et al . 
, 2015 ) and the explorations into convolutional and self - attention NNs ( Gehring et al . 
, 2017a , b ; Vaswani et al . 
, 2017 ) mark steady and important steps in the Ô¨Åeld of NMT . 
Since the introduction of BERT ( Devlin et al . 
, 2019 ) , the Transformer model ( Vaswani et al . 
, 2017 ) becomes the de facto architectural choice for many competitive NLP systems . 
Among the numerous ingredients that make Transformer networks successful , label smoothing is one that must not be overlooked and shall be the focus of this work . 
The idea of smoothing is not new in itself . 
For instance , many smoothing heuristics and functions are investigated in the context of count - based language modeling ( Jelinek and Mercer , 1980 ; Katz , 1987 ; Church and Gale , 1991 ; Kneser and Ney , 1995 ; Chen and Goodman , 1996 ) . 
Interestingly , when training NNs , the idea of smoothing comes in a new form and is applied on the empirical one - hot target distributions . 
Proposed to counteract overÔ¨Åtting and pursue better generalization , label smoothing ( Szegedy et al . 
, 2016 ) Ô¨Ånds its Ô¨Årst applications in NNs in the Ô¨Åeld of computer vision . 
Later , the method is shown to be effective in MT ( Vaswani et al . 
, 2017 ) . 
Furthermore , it is also helpful when applied in other scenarios , e.g. Generative Adversarial Networks ( GANs ) ( Salimans et al . 
, 2016 ) , automatic speech recognition ( Chiu et al . 
, 2018 ) , and person re - identiÔ¨Åcation ( Ainam et al . 
, 2019 ) . 
Since the method centralizes on the idea of avoiding over - conÔ¨Ådent model outputs on training data , it is reanalyzed in Pereyra et al . 
( 2017 ) . 
The authors include an additional conÔ¨Ådence penalty regularization term in the training loss , and compare it to standard label smoothing with uniform or unigram prior . 
While label smoothing boosts performance signiÔ¨Åcantly compared to using hard target labels , the difference in performance gains when comparing different smoothing methods is relatively small . 
M ¬®uller et al . 
( 2019 ) bring recent advancements towards better intuitive understandings of label smoothing . 
They observe a clustering effect of learned features and argue that label smoothing improves model calibration , yet hurting knowledge distillation when the model is used as a teacher for another student network . 
As a regularization technique in training , label smoothing can be compared against other methods such as dropout ( Srivastava et al . 
, 2014 ) and Dis-213 turbLabel ( Xie et al . 
, 2016 ) . 
Intuitively , dropout can be viewed as ensembling different model architectures on the same data and DisturbLabel can be viewed as ensembling the same model architecture on different data , as pointed out in Xie et al . 
( 2016 ) . 
Interestingly , label smoothing can also be understood as estimating the marginalized label dropout during training ( Pereyra et al . 
, 2017 ) . 
In this paper , we propose two straightforward extensions to label smoothing , examining token selection and prior distribution . 
Salimans et al . 
( 2016 ) and Zhou et al . 
( 2017 ) investigate a similar issue to the former . 
In the context of GANs , they select only those positive examples to smooth while we consider the task of MT , discussing how many tokens to smooth and how they should be selected . 
Pereyra et al . 
( 2017 ) and Gao et al . 
( 2019 ) talk about ideas similar to the latter . 
In their respective contexts , one experiments with unigram probabilities for label smoothing and the other uses Language Model ( LM ) posteriors to softly augment the source and target side of MT training data . 
3 Solving the Training Problem The standard label smoothing ( STN ) loss , as used by Vaswani et al . 
( 2017 ) , can be expressed as : LSTN=‚àíN / summationdisplay n=1V / summationdisplay v=1 / parenleftbigg ( 1‚àím)pv+m1 V / parenrightbigg logqv ( 1 ) whereLSTNdenotes the cross entropy with standard label smoothing , nis a running index in the total number of training tokens N , vis a running index in the target vocabulary V , mis the hyperparameter that controls the amount of probability mass to discount , pvis the one - hot true target distribution and qvis the output distribution of the model . 
The conÔ¨Ådence penalty ( CFD ) loss , as used by Pereyra et al . 
( 2017 ) , can be expressed as : LCFD=‚àíN / summationdisplay n=1V / summationdisplay v=1 / parenleftbig pv‚àím / primeqv / parenrightbig logqv ( 2 ) whereLCFDdenotes the conÔ¨Ådence - penalized cross entropy , m / primein this case is the hyperparameter that controls the strength of the conÔ¨Ådence penalty and thus differs from min Equation 1 . 
In both cases , the outer summation is over all of the training tokens N , implicating that all of the target token probabilities are smoothed . 
Thedependencies of qvandpvonnare omitted for simplicity . 
Additionally for Equation 1 , authors of both papers ( Vaswani et al . 
, 2017 ; Pereyra et al . 
, 2017 ) point out that the uniform prior can be replaced with alternative distributions over the target vocabulary . 
One more thing to notice is the negative sign in front of the non - negative term m / primein Equation 2 , which means that pv‚àím / primeqvis not a probability distribution anymore . 
One can nonetheless apply tricks to normalize the term inside the parentheses so that it becomes a probability distribution , e.g. : LCFD normalized 1=‚àíN / summationdisplay n=1V / summationdisplay v=1logqv ¬∑ ( pv‚àím / primeqv)‚àímin(pv‚àím / primeqv)/summationtextV v / prime=1(pv / prime‚àím / primeqv / prime)‚àímin(pv / prime‚àím / primeqv / prime)(3 ) or LCFD normalized 2=‚àíN / summationdisplay n=1V / summationdisplay v=1logqv ¬∑ exp(pv‚àím / primeqv)/summationtextV v / prime=1exp(pv / prime‚àím / primeqv / prime)(4 ) and implement it as an additional layer of activation during training , where v / primeis an alternative running index in the vocabulary . 
In any case , the integration of Equation 2 into the form of Equation 1 can not be done without signiÔ¨Åcantly modifying the original conÔ¨Ådence penalty , and we leave it for future work . 
3.1 Generalized Formula In an effort to obtain a uniÔ¨Åed view , we propose a simple generalized formula and make two major changes . 
First , we separate the outer summation over the tokens and divide it into two summations , namely ‚Äú not to smooth ‚Äù and ‚Äú to smooth ‚Äù . 
Second , we modify the prior distribution to allow it to depend on the position , current token and model output . 
In this case , rcould be the posterior from some helper model ( e.g. an LM ) , and during training , obtaining it on - the-Ô¨Çy is not expensive , as previously shown ( Bi et al . 
, 2019 ; Wang et al . 
, 2019 ) . 
The generalized label smoothing ( GNR ) loss can be expressed as : LGNR=‚àí/summationdisplay n‚ààAV / summationdisplay v=1pvlogqv‚àí /summationdisplay n‚ààBV / summationdisplay v=1((1‚àím)pv+mrv , qv ) logqv ( 5)214 whereLGNRdenotes the generalized cross entropy , Ais the set of tokens not to smooth , Bis the set of tokens to smooth , rv , qvis an arbitrary prior distribution for smoothing and again we drop the dependencies ofpv , qvandrv , qvonnfor simplicity . 
A natural question when explicitly writing out A andB , s.t . 
A‚à©B = ‚àÖand|A‚à™B| = N , is which tokens to include in B. Here , we consider two simple ideas : uniform random sampling ( RND ) and an entropy - based uncertainty heuristic ( ENT ) . 
The former chooses a certain percentage of tokens to smooth by sampling tokens uniformly at random . 
The latter prioritizes those tokens whose prior distributions have higher entropy . 
The logic behind the ENT formulation is that when the prior distribution is Ô¨Çattened out , yielding a higher entropy , the helper model is uncertain about the current position , and the model output should thus be smoothed . 
Formally , the two heuristics can be expressed as : BRND={n;œÅn‚àºU(0,1),œÅn‚â§œÄ } ( 6 ) BENT={b1,b2, ... ,b ‚åàœÄN‚åâ } ( 7 ) whereœÅnis a sample from the uniform distribution Uin[0,1],œÄis a hyperparameter controlling the percentage of tokens to smooth and { b1,b2, ... ,bN } is a permutation of data indices { 1,2, ... N}in descending order of the entropy of prior r , i.e.‚àÄ1‚â§ i‚â§j‚â§N,‚àí/summationtext Vrbilogrbi‚â•‚àí/summationtext Vrbjlogrbj . 
The hyperparameter min Equation 5 deserves some further notice . 
This is essentially the parameter that controls the strength of the label smoothing procedure . 
When it is zero , no smoothing is done . 
When it is one and |B|=N , the model is optimized to output the prior distribution r. One can obviously further generalize it so that mdepends also onn , vandqv . 
However in this work , we focus on the outer summation in Nand alternative priors r , and leave the exploration of adaptive smoothing strengthmn , r , qvfor future work . 
3.2 Theoretical Solution When it comes to the analysis of label smoothing , previous works focus primarily on intuitive understandings . 
Pereyra et al . 
( 2017 ) observe that both label smoothing and conÔ¨Ådence penalty lead to smaller gradient norms during training . 
M ¬®uller et al . 
( 2019 ) argue that label smoothing helps beamsearch by improving model calibration . 
They further visualize the learned features and show a clustering effect of features from the same class . 
In this work , we concentrate on Ô¨Ånding a theoreticalsolution to the training problem , and show exactly what label smoothing and conÔ¨Ådence penalty are optimizing for . 
Consider the optimization problem when training with Equation 1 : min q1,q2, ... ,qVLSTN n , s.t . 
V / summationdisplay v=1qv= 1 ( 8) While in practice we use gradient optimizers to obtain a good set of parameters of the NN , the optimization problem actually has well - deÔ¨Åned analytical solutions locally : ÀúqSTN v= ( 1‚àím)pv+m1 V(9 ) which is simply a linear interpolation between the one - hot target distribution pvand the smoothing prior1 V , withm‚àà[0,1]being the interpolation weight . 
One can use either the divergence inequality or the Lagrange multiplier method to obtain this result ( see Appendix A ) . 
Consider the optimization problem when training with Equation 2 : min q1,q2, ... ,qVLCFD n , s.t . 
V / summationdisplay v=1qv= 1 ( 10 ) The problem becomes harder because now the regularization term also depends on qv . 
Introducing the Lagrange multiplier Œªand solving for optima will result in a transcendental equation . 
Making use of the Lambert Wfunction ( Corless et al . 
, 1996 ) , the solution can be expressed as ( see Appendix A for detailed derivation ): ÀúqCFD v = pv m / primeW0 / parenleftBig pv m / primee1+Œª m / prime / parenrightBig ( 11 ) whereW0is the principal branch of the Lambert W function and Œªis the Lagrange multiplier , which is numerically solvable1when non - negative m / primeand probability distribution pvare given . 
Equation 11 essentially gives a non - linear relationship between ÀúqCFD v andpv , controlled by the hyperparameter m / prime . 
Now that theoretical solutions are presented in Equation 9 and 11 , it is possible to plot the graphs of optimal Àúqv , with respect to mandm / prime . 
Shown in Figure 2 , as expected for both STN and CFD , the overall effect is to decrease qvwhenpv= 1and increaseqvwhenpv= 0 . 
Whenmorm / primegets large 1One can use limm / prime‚Üí0ÀúqCFD v to avoid division by zero.215 10‚àí210‚àí1100101102 morm‚Ä≤00.20.40.60.81ÀúqvÀúqCFD vvs.m‚Ä≤ ÀúqSTD vvs.m(a)pv= 1 10‚àí210‚àí1100101102 morm‚Ä≤00.71.42.12.83.5Àúqv√ó10‚àí5 1 V ÀúqCFD vvs.m‚Ä≤ ÀúqSTD vvs.m ( b)pv= 0 Figure 2 : Graphs of optimal Àúqvw.r.t.morm/prime . 
Note the logarithmic scale in horizontal axes , with m‚àà[0,1 ] andm / prime‚â•0 . 
In order to obtain numerical solutions for ÀúqSTN v andÀúqCFD v , we setV= 32000 , which is a common vocabulary size when operating on sub - word levels . 
enough , the total probability mass is discounted and1 Vis redistributed to each token in the vocabulary . 
The graph of GRN2is similar to STD , only changing the limit from1 Vtorvasmapproaches one , and not included here for brevity . 
One last thing to notice is that the outer summation over the tokens is ignored . 
If it is taken into consideration , Àúq is dragged towards the empirical distribution given by the corpus3 . 
4 Finding a Good Recipe In this section , we describe our results and insights towards a good recipe to successfully apply label smoothing . 
We experiment with six IWSLT2014 datasets : German ( de ) , Spanish ( es ) , Italian ( it ) , Dutch ( nl ) , Romanian ( ro ) , Russian ( ru ) to English ( en ) , and one WMT2014 dataset : English to German . 
The statistics of these datasets are summarized in Table 1 . 
To prepare the subword tokens , we adopt joint byte pair encoding ( Sennrich et al . 
, 2016 ) , and use 10 K and 32 K merge operations on IWSLT and WMT , respectively . 
When preprocessing IWSLT , we remove sentences longer than 175 words , lowercase both source and target sides , randomly subsample roughly 4.35 % of the training sentence pairs as development data and concatenate all previously available development and test sets as test data , similar to Gehring et al . 
( 2017a ) . 
As for the preprocessing of WMT , we follow the setup in Ott et al . 
( 2018 ) . 
Using the Transformer architec2Assuming ronly depends on n , vand not qv . 
In the latter case , one needs to solve the optimization problem ignoring the outer summation and reusing the Lagrange multiplier . 
3For an intuitive understanding , consider the case when two sentence pairs have the exact same context up to a certain target position but the next tokens are different ( e.g. ‚Äú Danke . 
‚Äù in German being translated to ‚Äú Thank you . 
‚Äù and ‚Äú Thank you very much . 
‚Äù in English , the period in the Ô¨Årst translation and ‚Äú very ‚Äù in the second translation have the same context.)ture ( Vaswani et al . 
, 2017 ) , we apply the base setup for IWSLT and the big setup for WMT . 
For all language pairs , we share all three embedding matrices . 
All helper models are also Transformer - based . 
We conduct all experiments using fairseq ( Ott et al . 
, 2019 ) , monitor development set perplexity during training , and report BLEU ( Papineni et al . 
, 2002 ) scores on test sets after beam search . 
4.1 Token Selection The Ô¨Årst thing to determine is how to select tokens for smoothing and how many tokens to smooth . 
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 œÄ-0.5 + 0.0 + 0.5 + 1.0BLEURND ENT Figure 3 : Smoothing with RND versus ENT on de - en . 
mis set to 0.1 . 
The development and test perplexities of the helper LM are 53.8 and 46.5 . 
For this purpose , we begin by considering models smoothed with an LM helper . 
The helper LM is trained on target sentences from the corresponding parallel data till convergence . 
Figure 3 shows a comparison between RND and ENT , varying the percentage of smoothed tokens œÄand using the absolute performance improvements in BLEU as the vertical axis . 
Since the two methods only affect the order in which tokens are selected , they should yield the exact same results when all tokens are selected . 
This can be clearly seen from the Ô¨Ågure and serves as a sanity check for the correctness of216 dataset IWSLT WMT language pair de - en es - en it - en nl - en ro - en ru - en en - de number of sentence pairstrain 160 K 169 K 167 K 154 K 168 K 153 K 4.50 M valid 7.3 K 7.7 K 7.6 K 7.0 K 7.6 K 7.0 K 3.0 K test 6.8 K 5.6 K 6.6 K 5.4 K 5.6 K 5.5 K 3.0 K Table 1 : Data statistics of IWSLT and WMT datasets . 
the implementation . 
The RND and ENT curves follow a similar trend , increasing with the number of smoothed tokens . 
From the curves , neither selection method is consistently better than the other , indicating that the entropy - based selection heuristics is probably an oversimpliÔ¨Åcation considering the stochasticity introduced when altering the number of smoothed tokens . 
We continue to examine the uphill trend seen in Figure 3 in other cases . 
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 œÄ+0.0 + 0.5 + 1.0 + 1.5 + 2.0BLEUde - en es - en it - en nl - en ro - en ru - en ( a ) uniform as rv , m= 0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 œÄ+0.0 + 0.5 + 1.0 + 1.5 + 2.0BLEUde - en es - en it - en nl - en ro - en ru - en ( b ) unigram as rv , m= 0.1 Figure 4 : Smoothing different percentages of tokens . 
Figure 4 reveals the relationship between absolute BLEU improvements and œÄ , when smoothing with uniform or unigram ( RND ) distributions . 
While for each language pair the actual changes in BLEU differ , it is clear to conclude that , the more tokens smoothed , the better the performance . 
This conclusion is rather universal and holds true for the majority of our experiment settings ( varying m andr ) . 
From here on , we smooth all tokens , i.e. |B|=N , by default.4.2 Probability Mass Our next goal is to Ô¨Ånd good values of m. 0 0.2 0.4 0.6 0.8 1 m363738394041BLEU es - en nl - en ( a ) uniform as rv 0.0 0.1 0.2 0.3 m+0.0 + 0.5 + 1.0 + 1.5 + 2.0BLEUde - en es - en it - en nl - en ro - en ru - en ( b ) unigram as rv Figure 5 : Discounting different probability masses . 
The discounted probability mass mis a tunable hyperparameter that is set to 0.1 in the original Transformer ( Vaswani et al . 
, 2017 ) paper . 
We vary this parameter in the case of uniform smoothing and unigram smoothing , and plot the results in Figure 5 . 
As shown in Figure 5a , the BLEU score immediately improves at m= 0.1 , then plateaus when m‚àà[0.3,0.6 ] , slowly decreases whenm‚àà[0.7,0.9]and quickly drops to zero whenmapproaches one . 
When m= 1 , the model is optimized towards a uniform distribution and completely ignores the training data . 
Because perplexity can be thought of as the effective vocabulary size of a model , we examine the perplexities when m= 1for both language pairs . 
As expected , the development perplexities are around 10 K , which is in the same order of magnitude as the corresponding vocabulary sizes . 
Another interesting observation is that the BLEU scores only drop when217 mgets close to one and the model produces acceptable translations elsewhere . 
This indicates that NN models trained with gradient optimizers are very good at picking out the effective training signals even when they are buried in much stronger noise signals ( the uniform smoothing priors in the case of Figure 5a ) . 
This could be further related to multi - task learning ( Ruder , 2017 ) , where the system performances are also related to the regularization weights of the auxiliary losses . 
For unigram , we varymin{0.1 , 0.2 , 0.3 } . 
As seen in Figure 5b , while smoothing with m= 0.1gives a large improvement over no smoothing , setting m= 0.3 further boosts the performance , consistently for all six IWSLT language pairs . 
4.3 Prior Distribution Furthermore , we explore the use of LM and MT posteriors as prior distributions for smoothing . 
1.0 31.0 52.9 100.2 195.6 helper model perplexity36.53737.53838.53939.5BLEULM cheating LM normal no smoothing uniform unigram ( a)ro - en , LM posterior as rv , m= 0.1 4.62 4.72 4.85 4.93 5.02 5.15 5.29 helper model perplexity33.53434.535BLEUMT no smoothing uniform unigram ( b)de - en , MT posterior as rv , m= 0.1 Figure 6 : Smoothing with LM and MT posteriors . 
We train systems using Transformer LMs and MT models of different qualities for label smoothing , as in Figure 6 . 
To obtain very good LMs , we train them with test data and mark the cheating LMs in Figure 6a . 
We additionally plot the BLEU scores of models with no smoothing , smoothed with uniform and unigram , as horizontal lines to compare the absolute performances . 
Intuitively , the curve should follow a downhill trend , meaning that the worse the helper model performs , the worse themodel smoothed with it performs . 
This is loosely the case for LM , with cheating LMs giving better performances than uniform and unigram , and normal LMs lacking behind . 
As for MT , improvement over the no smoothing case is seen in Figure 6b . 
However , neither the downhill trend nor the competence over other priors in terms of BLEU , is seen . 
This suggests that the model is probably not utilizing the information in the soft distribution effectively . 
Related to knowledge distillation ( Hinton et al . 
, 2015 ; Kim and Rush , 2016 ) , a trainable teacher ( the helper model in our case ) might be further beneÔ¨Åcial ( Bi et al . 
, 2019 ; Wang et al . 
, 2018 ) . 
One important thing to mention is that , while neither LM nor MT outperforms uniform or unigram in terms of test BLEU score in our experiments , we see signiÔ¨Åcant drops in development set perplexities when smoothing with LM or MT . 
This signals a mismatch between training and testing , and suggests that smoothing with LM or MT indeed works well for the optimization criterion , but not as much for the Ô¨Ånal metric , the calculation of which involves beam search and scoring of the discrete tokens . 
4.4 Final Results Finally , we report BLEU scores of our best systems across all language pairs in Table 2 . 
While applying uniform label smoothing signiÔ¨Åcantly improves over the baselines , by using a good recipe , an additional improvement of around +0.5 BLEU is obtained across all language pairs . 
For the hyperparameters , we Ô¨Ånd that smoothing all tokens by m=0.3with a unigram prior is a good recipe , consistently giving one of the best BLEU scores . 
5 Analyzing the Mismatch As discussed in Section 4.3 , models smoothed with LMs or MT model posteriors yield very good development set perplexities but no big improvements in terms of test BLEU scores . 
Here , we further investigate this phenomenon in terms of search and scoring . 
5.1 Search We Ô¨Årst plot the test BLEU scores with respect to the beam size used during search . 
In Figure 7 , we see that the dashed curves for ‚Äú no smoothing ‚Äù , ‚Äú uniform ‚Äù and ‚Äú unigram ‚Äù initially increase and then plateau , which is an expected shape ( see Figure 8218 dataset IWSLT WMT language pair de - en es - en it - en nl - en ro - en ru - en en - de no label smoothing 33.6 39.3 31.2 36.5 37.0 22.3 28.0 Vaswani et al . 
( 2017 ) 34.4 40.8 32.4 37.5 38.5 23.4 28.4 our best recipe 35.0 41.5 32.8 38.0 39.0 23.9 29.0 Table 2 : BLEU scores can be signiÔ¨Åcantly improved with good label smoothing recipes . 
The Ô¨Årst row of numbers corresponds to using only the cross entropy criterion for training . 
The second row of numbers corresponds to the Transformer baselines . 
The last row contains scores obtained with our best hyperparameters . 
1 2 3 4 5 6 7 8 9 10 beam size32333435BLEU no smoothing uniform , m= 0.1 unigram , m= 0.3 LM , m= 0.3 Figure 7 : BLEU versus beam size on de - en . 
in Zhou et al . 
( 2019 ) ) . 
However , the solid curve for LM drops quickly as beam size increases ( see Stahlberg and Byrne ( 2019 ) for more insight ) . 
A possible explanation is that models smoothed with LMs generate search spaces that are richer in probability variations and more diversiÔ¨Åed , compared to e.g. uniform label smoothing . 
As search becomes stronger , hypotheses that have higher probabilities , but not necessarily closer to the true targets , are found . 
This suggests that the mismatch in development set perplexity and test BLEU is a complex phenomenon and calls for more analysis . 
5.2 Scoring We further examine test BLEU with respect to development ( dev ) BLEU and dev perplexity . 
As shown in Figure 8a , test BLEU is nicely correlated with dev BLEU , indicating that there is no mismatch between dev and test in the dataset itself . 
However , as in Figure 8b , although test BLEU increases with a decreasing dev perplexity , in regions of low dev perplexities , there exist many systems with very different test performances ranging from 39.3 BLEU to 41.5 BLEU . 
Despite perplexity being directly related to the cross entropy training criterion , this is an example where it fails to be a good proxy for the Ô¨Ånal BLEU metric . 
Against this mismatch between training and testing , either a more BLEU - related dev score or a more perplexityrelated test metric needs to be considered . 
41 41.5 42 42.5 43 43.5 dev BLEU3939.54040.54141.5test BLEU data Ô¨Åtted curve(a ) Dev BLEU is a good proxy for test BLEU . 
0 5 10 15 20 25 30 dev perplexity3939.54040.54141.5test BLEUdata Ô¨Åtted curve ( b ) Dev perplexity is a bad proxy for test BLEU . 
Figure 8 : Relationships between test BLEU and dev metrics . 
79 converged es - enmodels with different label smoothing hyperparameters are scattered . 
6 Conclusion In this work , we investigate label smoothing in neural machine translation . 
Considering important aspects in label smoothing : token selection , probability mass and prior distribution , we introduce a generalized formula and derive theoretical solutions to the training problem . 
Examining the effect of various hyperparameter choices , practically we show that with a good label smoothing recipe , one can obtain consistent improvements over strong baselines . 
Delving into search and scoring , we Ô¨Ånally emphasize the mismatch between training and testing , and motivate future research . 
Reassuring that label smoothing brings concrete improvements and considering that it only operates at the output side of the model , our next step is to explore similar smoothing ideas at the input side.219 Acknowledgements This work has received funding from the European Research Council ( ERC ) ( under the European Union ‚Äôs Horizon 2020 research and innovation programme , grant agreement No 694537 , project ‚Äú SEQCLAS ‚Äù ) and the Deutsche Forschungsgemeinschaft ( DFG ; grant agreement NE 572/8 - 1 , project ‚Äú CoreTec ‚Äù ) . 
The GPU computing cluster was supported by DFG ( Deutsche Forschungsgemeinschaft ) under grant INST 222/1168 - 1 FUGG . 
Abstract In linguistics and cognitive science , Logical metonymies are deÔ¨Åned as type clashes between an event - selecting verb and an entitydenoting noun ( e.g. The editor Ô¨Ånished the article ) , which are typically interpreted by inferring a hidden event ( e.g. reading ) on the basis of contextual cues . 
This paper tackles the problem of logical metonymy interpretation , that is , the retrieval of the covert event via computational methods . 
We compare different types of models , including the probabilistic and the distributional ones previously introduced in the literature on the topic . 
For the Ô¨Årst time , we also tested on this task some of the recent Transformer - based models , such as BERT , RoBERTa , XLNet , and GPT-2 . 
Our results show a complex scenario , in which the best Transformer - based models and some traditional distributional models perform very similarly . 
However , the low performance on some of the testing datasets suggests that logical metonymy is still a challenging phenomenon for computational modeling . 
1 Introduction The phenomenon of logical metonymy is deÔ¨Åned as a type clash between an event - selecting metonymic verb ( e.g. , begin ) and an entity - denoting nominal object ( e.g. , the book ) , which triggers the recovery of a hidden event ( e.g. , reading ) . 
Logical metonymies have been widely studied , on the one hand , in theoretical linguistics as they represent a challenge to traditional theories of compositionality ( Asher , 2015 ; Pustejovsky and Batiukova , 2019 ) . 
On the other hand , they received extensive attention in cognitive research on human sentence processing as they determine extra processing costs during online sentence comprehension ( McElree et al . 
, 2001 ; Traxler et al . 
, 2002 ) , apparently relatedto ‚Äú the deployment of operations to construct a semantic representation of the event ‚Äù ( Frisson and McElree , 2008).1 Logical metonymy has also been explained in terms of the words - as - cues hypothesis proposed by Jeffrey Elman ( Elman , 2009 , 2014 ) . 
This hypothesis relies on the experimental evidence that human semantic memory stores knowledge about events and their typical participants ( see McRae and Matsuki ( 2009 ) for an overview ) and claims that words act like cues to access event knowledge , incrementally modulating sentence comprehension . 
The results obtained in a probe recognition experiment by Zarcone et al . 
( 2014 ) , in line with this explanation , suggest that speakers interpret logical metonymies by inferring the most likely event the sentences could refer to , given the contextual cues . 
Previous research in NLP on logical metonymy has often been inÔ¨Çuenced by such theoretical explanation ( Zarcone and Pad ¬¥ o , 2011 ; Zarcone et al . 
, 2012 ; Chersoni et al . 
, 2017 ) . 
In our contribution , we propose a general comparison of different classes of computational models for logical metonymy . 
To begin with , we tested two approaches that have been previously introduced in the literature on the topic : probabilistic and distributional models ( Zarcone et al . 
, 2012 ) . 
We also examined the Structured Distributional Model ( SDM ) by Chersoni et al . 
( 2019 ) , which represents sentence meaning with a combination of formal structures and distributional embeddings to dynamically integrate knowledge about events and their typical participants , as they are activated by lexical items . 
Finally , to the best of our knowledge , we are the Ô¨Årst ones to include the recent Transformer language models into a contrastive study on 1Notice however that the evidence is not uncontroversial : Delogu et al . 
( 2017 ) report that coercion costs largely reÔ¨Çect word surprisal , without any speciÔ¨Åc effect of type shift in the early processing measures.224 logical metonymy . 
Transformers ( Vaswani et al . 
, 2017 ; Devlin et al . 
, 2019 ) are the dominant class of NLP systems in the last few years , since they are able to generate ‚Äú dynamic ‚Äù representations for a target word depending on the sentence context . 
As the interpretation of logical metonymy is highly sensitive to context , we deem that the contextual representations built by Transformers might be able to integrate the covert event that is missing in the surface form of the sentence . 
All models are evaluated on their capability ofassigning the correct interpretation to a metonymic sentence , that is , recovering the verb that refers to the correct interpretation . 
This task is hard for computational models , as they must exploit contextual cues to distinguish covert events with a high typicality ( e.g. , The pianist begins the symphony‚Üíplaying ) from plausible but less typical ones ( ‚Üícomposing ) . 
2 Related Work 2.1 Computational Models of Logical Metonymy According to Zarcone et al . 
( 2013 ) , the phenomenon of logical metonymy can be explained in terms of the thematic Ô¨Åt , that is , the degree of compatibility between the verb and one of its arguments ( the direct object , in this case ) . 
On the one hand , a low thematic Ô¨Åt between an event - selecting verb and an entity - denoting argument triggers the recovery of a covert event , while on the other hand , the recovered event is often the best Ô¨Åtting one , given the information available in the sentence . 
Research in NLP on logical metonymy initially focused on the problem of covert event retrieval , which was tackled by means of probabilistic models ( Lapata and Lascarides , 2003 ; Shutova , 2009 ) , or by using Distributional Semantic Models ( DSMs ) that identify the candidate covert event with the one that has the highest thematic Ô¨Åt with the arguments in the sentence ( Zarcone et al . 
, 2012 ) . 
Following the psycholinguistic works by McElree et al . 
( 2001 ) and Traxler et al . 
( 2002 ) , which reported increased reading times and longer Ô¨Åxations in eye - tracking for the metonymic sentences , Zarcone et al . 
( 2013 ) proposed a distributional model of the thematic Ô¨Åt between verb and object , and showed that it accurately reproduces the differences between the experimental conditions in the data from the two original studies . 
A general distributional model for sentence com - prehension was used by Chersoni et al . 
( 2017 ) to simultaneously tackle both these two aspects of logical metonymy ( covert event retrieval and increased processing times ) , although at the cost of a highly - elaborated compositional model . 
The authors recently introduced a more up - to - date and reÔ¨Åned version of their sentence comprehension model ( Chersoni et al . 
, 2019 ) , but it has not been tested on the logical metonymy task so far . 
2.2 Transformer Models in NLP The traditional approach in Distributional Semantics has been the building of a single , stable vector representation for each word type in the corpus ( Turney and Pantel , 2010 ; Lenci , 2018 ) . 
Lately , a new generation of embeddings has emerged , in which each occurrence of a word in a speciÔ¨Åc sentence context gets a unique representation ( Peters et al . 
, 2018 ) . 
The most recent systems typically rely on an LSTM or a Transformer architecture for getting word representations : they are trained on large amounts of textual data and the word vectors are learned as a function of the internal states of the encoder , such that a word in different sentence contexts determines different activation states and is represented by a different vector . 
Thus , embeddings generated by these new models are said to be contextualized , as opposed to the static vectors generated by the earlier frameworks , and they aim at modeling the speciÔ¨Åc sense assumed by the word in context . 
One of the most popular and successful contextualized model is probably BERT ( Devlin et al . 
, 2019 ) , whose key technical innovation is applying the bidirectional training of Transformer , a popular attention model , to language modelling . 
This is in contrast to previous efforts which looked at a text sequence either from left to right or combined left - to - right and right - toleft training . 
The results of the paper show that a language model with bidirectional training can have a deeper sense of language context and structure than single - direction language models . 
An interesting aspect of Transformer models like BERT is that they are trained via masked language modeling , that is , they have to retrieve a word that has been masked in a given input sentence . 
Since interpreting logical metonymy implies the retrieval of an event that is not overtly expressed and that humans retrieve integrating the lexical cues in the sentence , these models are potentially a very good Ô¨Åt for this task . 
To draw an analogy , we could225 imagine that the covert event is a verb that has been ‚Äô masked ‚Äô in the linguistic input and that we ask BERT - like models to make a guess . 
It is important to point out that not all Transformers are used for masked language modeling : among those tested for this study , BERT and RoBERTa are directly trained with this objective , XLNet is trained with permutation language modeling , but can still retrieve a hidden word given a bidirectional context , and GPT-2 works similarly to a traditional , unidirectional language model . 
3 Experimental Settings 3.1 Task Our research question focuses on how computational models can interpret metonymic sentences . 
To explore this issue , we deÔ¨Åne the task of logical metonymy interpretation as a covert event recoverytask . 
More speciÔ¨Åcally , given a sentence like The architect Ô¨Ånished the house , the computational model has to return the most likely hidden verb for the sentence , i.e. the covert event representing its interpretation . 
Despite the architectural differences , all tested models compute a plausibility score of a verb as expressing the covert event associated with a < subject , metonymic verb , object > triple . 
We evaluate the scores returned by a model against human judgments using the standard measures of accuracy and correlation depending if the dataset contains categorical or continuous variables . 
3.2 Datasets In our experiments , we use three datasets designed for previous psycholinguistic studies , and a newly created one by means of an elicitation task . 
TheMcElree dataset ( MC ) comprises the stimuli from the sentences of the self - paced reading experiment of McElree et al . 
( 2001 ) and includes 30 pairs of tuples . 
Each pair has the same subject , metonymic verb , object , just the covert verb varies . 
As in the conditions of the original experiment , the hidden verb could be either highly plausible , or plausible but less typical , given the subject and the object of the tuple . 
The Traxler dataset ( TR ) results from the sentences of the eye - tracking experiment of Traxler et al . 
( 2002 ) and includes 36 pairs of tuples . 
The format is the same as the McElree dataset . 
On these two datasets , the models have to perform a binary classiÔ¨Åcation task , with the goal of assigning a higher score to the covert event in the typical condition . 
TheLapata - Lascarides dataset ( L&L ) ( Lapata and Lascarides , 2003 ) includes 174 tuples , each composed by a metonymic verb , an object and a potential covert verb . 
The authors collected plausibility ratings for each metonymy by turning the tuples into sentences and used the Magnitude Estimation Paradigm ( Stevens , 1957 ) to ask human subjects to rate the plausibility of the interpretation of the metonymic verb . 
Finally , the mean ratings have been normalized and log - transformed . 
A further dataset of recovered covert events ( CE ) was collected by the authors . 
The metonymic sentences used in the McElree and Traxler experiments were turned into 69templates with an empty slot corresponding to the covert event ( e.g. , The student began the book late in the semester ) . 
Thirty subjects recruited with crowdsourcing were asked to produce two verbs that provided the most likely Ô¨Ållers for the event slot . 
Out of the 4,084 collected verbs , we selected those with a production frequency‚â•3for a given stimulus . 
The Ô¨Ånal dataset comprises 285items each consisting of a subject ‚Äì metonymic verb ‚Äì object tuple t and a covert event eassociated with a salience score corresponding to the event conditional probability given the tuple P(e|t)(i.e . 
, the production frequency of enormalized by the total events produced for t ) . 
In the case of the latter two datasets , for each model we compute the Spearman ‚Äôs correlation between the probabilities generated by the model and the human judgements . 
Examples from these datasets are provided in Table 1 . 
While collecting the data for CE , we also run a statistical comparison between the production frequencies of the verbs in the typical and in the atypical condition that appear in the binary classiÔ¨Åcation datasets , to ensure that humans genuinely agree on the higher typicality of the former . 
The result conÔ¨Årmed this assumption : according to the Wilcoxon signed rank test with continuity correction , the frequencies of production of the typical verbs for the MC dataset were signiÔ¨Åcantly higher ( W= 424 , p < 0.001 ) , and the same holds for the typical verbs in the TRdataset ( W= 526 .5 , p < 0.001 ) . 
3.3 Models In the following section , we describe the general aspects of the computational models that we tested on logical metonymy interpretation.226 Dataset Subject - verb - object Covert event Condition / Score Size MC chef start dinnerprepare HIGH TYP30 ( pairs)eat LOW TYP TR dieter resist cakeeat HIGH TYP36 ( pairs)taste LOW TYP L&L ‚Äî start experimentimplement 0.1744174study 0.0184 CE architect start housedraw 0.348258build 0.087 Table 1 : Examples of stimuli from each dataset . 
3.3.1 Probabilistic Model As a baseline model , we adopt the simple probabilistic approach proposed by Lapata and Lascarides ( 2003 ) and replicated by Zarcone et al . 
( 2012 ) as the SOpmodel , which was reported as the best performing probabilistic model on the task . 
The interpretation of a logical metonymy ( e.g. , The pianist began the symphony ) is modelled as the joint distribution P(s , v , o , e ) of the variables s(the subject , pianist ) , v(the metonymic verb , began ) , o ( the object , symphony ) , and the covert event e(e.g . 
, play ) . 
We compute that probability considering the metonymic verb constant : P(s , v , o , e ) ‚âàP(e)P(o|e)P(s|e ) The verb Erepresenting the preferred interpretation of the metonymy is the verb emaximizing the following equation : E = argmax eP(e)P(o|e)P(s|e ) We computed the statistics from a 2018 dump of the English Wikipedia , parsed with the Stanford CoreNLP toolkit ( Manning et al . 
, 2014 ) . 
Dataset Coverage MC 19/30 ( pairs ) TR 21/36 ( pairs ) L&L 151/174 ( items ) CE 195/285 ( items ) Table 2 : Coverage for the probabilistic model . 
3.3.2 Logical Metonymy as Thematic Fit Distributional models of logical metonymy assume that the event recovery task can be seen as a thematic Ô¨Åt task : recovering the covert event means identifying the verb with the highest thematic Ô¨Åt with the metonymic sentence . 
We reimplement thedistributional model by Zarcone et al . 
( 2012 ) with the following procedure : ‚Ä¢we retrieve the n(= 500 ) 2most strongly associated verbs for the subject and the object respectively , and we take the intersection of the two lists ; ‚Ä¢we update their association scores using either the sum ( add ) or the product ( prod ) function ; ‚Ä¢we select the embeddings corresponding to the Ô¨Årst m(= 20 ) verbs in this list and we add them together to create the prototype vector of the verb given the subject and the object ; ‚Ä¢the thematic Ô¨Åt of the covert event ewith respect to the nominal entities is computed as the similarity score of its corresponding lexical vector /vector ewith the prototype vector . 
As we did the probabilistic model , we discard the metonymic verb from this computation.3 We test two variations of this model , TF - add andTF - prod , which differ for the Ô¨Åller selection update function . 
Statistics were extracted from Wikipedia 2018 , and the vectors were the publiclyavailable Wikipedia embeddings4trained with the FastText model ( Bojanowski et al . 
, 2017 ) . 
The verb-Ô¨Åller association score is the Local Mutual Information ( Evert , 2008 ) . 
Similarly , the scores for the subject Ô¨Ållers are deÔ¨Åned as : LMI ( s , e ) = f(esbj‚Üê‚àí‚àís)log2p(s|e ) p(s)p(e ) 2We set a high value for this parameter in order to maximize the coverage . 
3Zarcone et al . 
( 2012 ) show that , for both the probabilistic and the distributional model , including the metonymic verb does not help too much in terms of performance and leads to coverage issues . 
4https://fasttext.cc/docs/en/ english - vectors.html227 where sis the subject , ethe covert event , and f(esbj‚Üê‚àí‚àís)indicates the frequency of ewith the subject . 
The scores for the object position are computed with the following formula : LMI ( o , e ) = f(eobj‚Üê‚àí‚àío)log2p(o|e ) p(o)p(e ) where ois the object and f(eobj‚Üê‚àí‚àío)represents the joint frequency of ewith the object . 
3.3.3 Structured Distributional Model The Structured Distributional Model ( SDM ) proposed by Chersoni et al . 
( 2019 ) consists of two components : a Distributional Event Graph ( henceforth , DEG ) , and a meaning composition function . 
DEG represents event knowledge as a graph automatically built from parsed corpora , where the nodes are words associated to a numeric vector , and the edges are labeled with syntactic relations and weighted using statistic association measures . 
Each event is represented as a path in DEG , that is , a sequence of edges ( relations ) which joins a sequence of vertices ( words ) . 
Thus , given a lexical cue w , it is possible to identify the associated events and to generate expectations about incoming inputs on both the paradigmatic and the syntagmatic axis . 
The composition function makes use of two semantic structures ( inspired by DRT ( Kamp , 2013 ) ): the linguistic condition ( LC ) , a contextindependent tier of meaning , and the active context ( AC ) , which accumulates contextual information available during sentence processing or activated by lexical items . 
The crucial aspect is that the model associates a vectorial representation to these formal structures : /vectorLCis the sum of the embeddings of the lexical items of a sentence ; /vectorAC , for each syntactic slot , is represented as the centroid vector built out of the role vectors /vector r1 , ... , /vector r navailable in AC , i.e. the syntactic associates of the lexical items that have been already processed . 
In our implementation of SDM , the DEG is constructed by extracting syntactic relations from the same dump of Wikipedia adopted in the previous models , and we chose as lexical embeddings the same FastText Wikipedia vectors . 
Following the same assumption of the previous experiment , we model the covert event recovery task as a thematic Ô¨Åt task : the goal is to predict the hidden verb on the basis of the subject and the object , treating the metonymic verb as a constant . 
SpeciÔ¨Åcally , the model builds a semantic representation for eachModel settings Data size L H A P BERT large - cased24 1024 16 340 M 16 GB RoBERTa large24 1024 16 355 M 160 GB XLNet large - cased24 1024 16 340 M 113 GB GPT-2 extra - large48 1600 25 1542 M 40 GB Table 3 : Comparison between transformer models . 
Model details : L : number of layers , H : dimension of hidden states , A : attention head numbers , and P : total parameter size . 
tuple in the dataset . 
The linguistic condition vector /vectorLCcontains the sum of the subject and object embeddings . 
At the same time , the event knowledge vector /vectorACcontains the prototypical embedding for the main verb , using DEG to retrieve the most associated verbs for the subject and the object , as in Chersoni et al . 
( 2019 ) . 
The scoring function has been adapted to the event recovery task as follows : cos(/vector e,/vectorLC(sent ) ) + cos(/vector e,/vectorAC(sent ) ) where sentrefers to the metonymic test tuple . 
In other words , we quantify the typicality of a verb for a tuple subject - object as the sum of i. ) the cosine similarity between the event embedding and the additive combination of the other argument vectors ( /vectorLC ) and ii . 
) the cosine similarity between the event embedding and the prototype vector representing the active context ( /vectorAC ) . 
3.3.4 Transformer - based Models We experiment with four Transformer models which have been shown to obtain state - of - the - art performances on several NLP benchmarks . 
The popular BERT model ( Devlin et al . 
, 2019 ) was the Ô¨Årst to adopt the bidirectional training of Transformer for a language modeling task . 
To make this kind of training possible , BERT introduced a masked language modeling objective function : random words in the input sentences are replaced by a [ MASK ] token and the model attempts to predict the masked token based on the surrounding context . 
Simultaneously , BERT is optimized on a next sentence prediction task , as the model receives sentence pairs in input and has to predict whether the second sentence is subsequent to the228 Ô¨Årst one in the training data.5BERT has been trained on a concatenation of the BookCorpus and the English Wikipedia , for a total of 3300 M tokens ca . 
In our experiments , we used the larger pre - trained version , called BERT - large - cased . 
RoBERTa ( Liu et al . 
, 2019 ) has the same architecture as BERT , but it introduces several parameter optimization choices : it makes use of dynamic masking ( compared to the static masking of the original model ) , of a larger batch - size and a larger vocabulary size . 
Moreover , the input consists of complete sentences randomly extracted from one or multiple documents , and the next sentence prediction objective is removed . 
Besides the optimized design choice , another key difference of RoBERTa with the other models is the larger training corpus , which consists of a concatenation of the BookCorpus , CCNEWS , OpenWebText , and STORIES . 
With a total 160 GB of text , RoBERTa has access to more potential knowledge than the other models . 
For our tests , we used the large pre - trained model . 
XLNet ( Yang et al . 
, 2019 ) is a generalized autoregressive ( AR ) pretraining method which uses the context words to predict the next word . 
The AR architecture is constrained to a single direction ( either forward or backwards ) , that is , context representation takes in consideration only the tokens to the left or to the right of the i - th position , while BERT representation has access to the contextual information on both sides . 
To capture bidirectional contexts , XLNet is trained with a permutation method as language modeling objective , where all tokens are predicted but in random order . 
XLNet ‚Äôs training corpora were the same as BERT plus Giga5 , ClueWeb 2012 - B and Common Crawl , for a total of 32.89B subword piece . 
Also in this case , we used the large pre - trained model . 
GPT-2 ( Radford et al . 
, 2019 ) , a variation of GPT , is a uni - directional transformer language model , which means that the training objective is to predict the next word , given all of the previous words . 
Compared with GPT , GPT-2 optimizes the layer normalization , expands the vocabulary size to 50,257 , increases the context size from 512 to 1024 tokens , and optimizes with a larger batch size of 512 . 
In addition , GPT-2 is pre - trained on WebText , which was created by scraping web pages , for a total of 8 million documents of data ( 40 GB ) . 
We 5Notice that the usefulness of this secondary objective function was questioned , and it was indeed removed in more recent models ( Yang et al . 
, 2019 ; Liu et al . 
, 2019 ; Joshi et al . 
, 2020).used the XL version of GPT-2 for our experiments . 
The parameters of the Transformer models are reported in Table 3 . 
BERT , RoBERTa and XLNet are used to perform a word prediction task : given a sentence and a masked word in position k , they compute the probability of a word wkgiven the context k : P(wi|context k ) . 
For our experiments , the context is the entire sentence Swith the k - th word ( the covert event ) being replaced by a special token ‚Äò [ MASK ] ‚Äô . 
Therefore , we turned the test tuples into full sentences , masking the verb as in the example below : The architect Ô¨Ånishes [ MASK ] house.6We then compute the probability of a hidden verb to occur in that position , and we expect the preferred verb to get a high value . 
We performed this task using the packages of the HappyTransformer library.7 As GPT-2 works as a traditional language model , we adopted this model to calculate the probability of the entire sentence ( instead of the probability of the hidden verb given the context ) . 
In this case , we expect that sentences evoking more typical events get higher values . 
We adopted the lm - scorer package to compute sentence probabilities.8 4 Evaluation Results Table 5 and 4 report the Ô¨Ånal evaluation scores . 
The performance of the probabilistic model is in line with previous studies , and it outperforms distributional models in some cases , proving that it is indeed a hard baseline to beat . 
However , accuracy and correlation are computed only on a subgroup of the test items : actually , the model covers about 60 % of the datasets ‚Äô tuples ( 86.8 % for L&L ) , as we reported in Table 2 . 
Coverage is the main issue probabilistic models have to face ( Zarcone et al . 
, 2013 ) , while distributional models do not experience such limitation . 
Regarding the thematic Ô¨Åt models , we observe that there is no difference between the TF - add and TF - prod models , as they obtain similar scores . 
6One of the anonymous reviewers argues that the performance of the Transformer - based models might be inÔ¨Çuenced by the prompt sentence and suggest more variations of the input sentences . 
We indeed tested several manipulations of the inputs before feeding them to the transformers , changing 1 ) the tense of the metonymic verb ( using the past tense ) and 2 ) the number of the direct object ( we used the plurals of the dataset nouns ) . 
However , the results did not show any consistent trend . 
7https://github.com/EricFillion/ happy - transformer 8https://pypi.org/project/lm-scorer/229 Probabilistic Distributional Transformer - based SOp TF - add TF - prod SDM BERT RoBERTa XLNet GPT-2 MC 0.68 0.70 0.73 0.77 0.70 0.80 0.40 0.87 TR 0.48 0.53 0.53 0.72 0.47 0.72 0.39 0.69 O. P. 0.58 0.62 0.63 0.75 0.59 0.76 0.40 0.78 Table 4 : Results for binary classiÔ¨Åcation task . 
Probabilistic Distributional Transformer - based SOp TF - add TF - prod SDM BERT RoBERTa XLNet GPT-2 L&L 0.53 0.41 0.41 0.53 0.61 0.73 0.04 0.43 CE 0.36 0.26 0.22 0.40 0.27 0.39 0.18 0.31 O. P. 0.45 0.34 0.32 0.47 0.44 0.56 0.11 0.37 Table 5 : Results for correlation task . 
However , we need to point out that , when the system computes the intersection of the two lists of the top verbs for subjects and objects , sometimes the number of retrieved items is less than 20(the model parameter for the verb embedding selection , cf . 
Section 3.3.2 ) . 
Therefore , independently of the selected function , the verbs used to compute the prototypical vector are eventually all those belonging to the intersection . 
Moreover , TF - models are often close to , and never signiÔ¨Åcantly outperform the probabilistic baseline . 
Among the distributional models , SDM is the one that obtains a considerable performance across all the datasets . 
This model performs close to RoBERTa both in the Traxler and in the CE dataset . 
This result is surprising , considering that SDM is trained just on a dump of Wikipedia , while RoBERTa is trained on 160 GB of text and implements advanced deep learning techniques . 
This outcome conÔ¨Årms that SDM , which has been designed to represent event knowledge and the dynamic construction of sentence meaning , is able to adequately model the typicality of events . 
This aspect has been suggested to be one of the core components of the language processing system ( Baggio and Hagoort , 2011 ; Baggio et al . 
, 2012 ; Chersoni et al . 
, 2019 ) . 
On the other hand , Transformers also provided interesting results . 
RoBERTa achieves the best score for the L&L dataset , reaching a statistical signiÔ¨Åcance of the improvement over SDM ( p < 0.01).9More importantly , it is the only Transformer that consistently obtains good results across all datasets , while the scores from other 9Thep - value is computed with Fisher ‚Äôs r - to - z transformation , one - tailed test . 
Transformer models are highly Ô¨Çuctuating . 
We believe that the gigantic size of the training corpus is a factor that positively affects its performance . 
At the same time , GPT-2 achieves the highest score for MC dataset ( 0.87 ) ( but the improvement over RoBERTa and SDM does not reach statistical signiÔ¨Åcance ) , although it performs signiÔ¨Åcantly lower on the other benchmarks10 . 
For the sake of completeness , we also report the overall performance of each model over the two tasks . 
Results identify RoBERTa and GPT-2 as the best models for the correlation and classiÔ¨Åcation tasks , respectively . 
However , we wonder if the average score is a valid measure to identify the best model . 
These two models tend to have a wavering behavior , which results in large differences between the two datasets scores . 
SpeciÔ¨Åcally , Roberta achieves 0.75 for the L&L dataset , but only 0.39 for the CE one , with 0.36 points of difference . 
Similarly , GPT-2 reaches 0.89 scores for the MC dataset , but its performance goes down by 0.16 . 
On the contrary , SDM behavior is more stable , with a smaller gap between the two datasets ‚Äô scores ( 0.13 point difference for the correlation task and just 0.05 for the accuracy task ) . 
4.1 Error analysis Binary classiÔ¨Åcation task For the MC and TR datasets , we evaluate the models for their capability of assigning a higher probability to the verb in the typical condition . 
It is important to empha10We determine the signiÔ¨Åcance of differences between models for MC and TR datasets with a McNemar ‚Äôs Chi - Square Test , applied to a 2x2 contingency matrix containing the number of correct and incorrect answers ( replicating the approach of Zarcone et al . 
( 2012)).230 size that both verbs are plausible in the context , but one describes a more likely event given the subject and the object . 
This remark is essential , because it explains the performance of all models , distributional and Transformer ones . 
To identify which tuples are the most difÔ¨Åcult ones , we built a heat map visualizing the correctlypredicted ones in blue , and the wrong ones in yellow ( see Figures 1 and 2 ) . 
We do not consider the accuracy values obtained by the probabilistic model for its partial coverage . 
Figure 1 : Heat map for error analysis over MC dataset . 
Figure 2 : Heat map for error analysis over TR dataset . 
This visualization technique reveals that some pairs are never predicted correctly , corresponding to the fully vertical yellow lines in the Ô¨Ågures . 
In what follow we report the tuples that are consistently mistaken for MC ( 1 ) and TR ( 2 ) datasets . 
( 1 ) a. The teenager starts the novel . 
b. The worker begins the memo.(2 ) a. The editor Ô¨Ånishes the newspaper . 
b. The director starts the script . 
c. The teenager begins the novel . 
In all the above cases , a model must discriminate between the verb read ( HIGH TYP ) and write ( LOW TYP).11It is interesting to notice that , for many of the read - write pairs in the binary classiÔ¨Åcation data , the production frequencies of typical and atypical verb are much closer than on average , suggesting that the interpretation requires understanding of subtle nuances of context - sensitive typicality , which might not be trivial even for humans . 
Furthermore , in Figure 2 we observe that for two TR ‚Äôs pairs , SDM is the only one picking the right choice : The stylist starts the braid andThe auditor begins the taxes . 
It seems that models regularly tend to prefer a verb with a more generic and undetermined meaning ( make anddo , respectively ) , while only SDM correctly assigns the HIGH TYP class to the verbs that indicate more precisely the manner of doing something ( braid andaudit ) . 
On the other hand , GPT-2 and RoBERTa managed to pick the right choice for a few of the readwrite items on which SDM is mistaken . 
Correlation task Correlation is a more complex task compared to classiÔ¨Åcation , as the lower scores also reveal . 
To better understand our results , we select the best model for the CE ( i.e. , SDM ) and L&L ( i.e. , RoBERTa ) datasets , and we plot the linear relationship between the human ratings and the model - derived probabilities.12For CE , Figure 3 reveals 1 ) a small positive correlation between the two variables , 2 ) a large amount of variance , and 3 ) a few outliers . 
As for L&L in Figure 4 , the majority of the points follow a roughly linear relationship , and there is a small variation around the trend . 
Nevertheless , this result could be inÔ¨Çuenced by the form of the input sentences . 
For all the other datasets , we masked the token between the verb and the object , and the corresponding hidden verb had to be in the progressive form ( The chef starts [ cooking ] dinner ) . 
For L&L , instead , we chose to insert the prepositiontoafter the verb since lots of the metonymic verbs ( want , try , etc . 
) require to be followed by the inÔ¨Ånitive verb . 
Thus , the context gives a higher 11Except for the sentence in 2.a , where the typical verb is edit . 
12We apply the logarithmic transformation of data for visualization purposes.231 probability to verbs as masked tokens , while different parts of speech could be equally plausible for the other conditions . 
Figure 3 : SDM correlation for CE . 
Figure 4 : RoBERTa correlation for L&L. 5 Discussion and Conclusions In this paper , we have presented a comparative evaluation of several computational models on the task of logical metonymy interpretation . 
We frame this problem as the retrieval of an event that is not overtly expressed in the surface form of the sentence . 
According to Elman ‚Äôs Words - as - Cues framework , human subjects can infer the covert event in logical metonymy thanks to the generalized knowledge about events and participants stored in their semantic memory . 
Hence , during sentence processing , words in the sentence create a network of mutual expectations that triggers the retrieval of typical events associated with lexical items and gen - erates expectations about the upcoming words ( Elman , 2014 ) . 
To tackle the task of logical metonymy interpretation , computational models must be able to recover unexpressed relationships between the words , using a context - sensitive representation of meaning that captures this event knowledge . 
The most compelling outcome of the reported experiments is probably the performance of SDM , which achieves the best score for the TR and the CE datasets . 
These results demonstrate the signiÔ¨Åcance of encoding event structures outside the embeddings ( which are treated as nodes in a distributional graph ) , and the ability of the SDM compositional function to dynamically update the semantic representation for a sentence . 
However , the evaluation scores are not very high , especially in the correlation task . 
Results reveal that the contextualized information used by computational models is useful to recall plausible events connected to the arguments , but this is still not sufÔ¨Åcient . 
Even Transformer models , which currently report state - of - theart performances on several NLP benchmarks , are not performing signiÔ¨Åcantly better than the SDM model , which is trained on a smaller corpus and without any advanced deep learning technique . 
Error analysis highlights that they are able to identify the plausible scenarios in which the participants could occur , but they still struggle in perceiving different nuances of typicality . 
Our experiments show how the logical metonymy task can be seen as a testing ground to check whether computational models encode common - sense event knowledge . 
Future work might follow two directions . 
On the one hand , expanding the coverage of the graph could favourably increase the performance of SDM . 
On the other hand , Transformer models could be tested with new experimental settings , such as the Ô¨Åne - tuning of the pre - trained weights on thematic Ô¨Åt - related ( Lenci , 2011 ; Sayeed et al . 
, 2016 ; Santus et al . 
, 2017 ) or semantic role classiÔ¨Åcation tasks ( Collobert et al . 
, 2011 ; Zapirain et al . 
, 2013 ; Roth and Lapata , 2015 ) . 
6 Acknowledgements This work , carried out within the Institut Convergence ILCB ( ANR-16 - CONV-0002 ) , has beneÔ¨Åted from support from the French government , managed by the French National Agency for Research ( ANR ) and the Excellence Initiative of AixMarseille University ( A*MIDEX ) . 
We thank the anonymous reviewers for their insightful feedback.232 Abstract Structured semantic sentence representations such as Abstract Meaning Representations ( AMRs ) are potentially useful in various NLP tasks . 
However , the quality of automatic parses can vary greatly and jeopardizes their usefulness . 
This can be mitigated by models that can accurately rate AMR quality in the absence of costly gold data , allowing us to inform downstream systems about an incorporated parse ‚Äôs trustworthiness or select among different candidate parses . 
In this work , we propose to transfer the AMR graph to the domain of images . 
This allows us to create a simple convolutional neural network ( CNN ) that imitates a human judge tasked with rating graph quality . 
Our experiments show that the method can rate quality more accurately than strong baselines , in several quality dimensions . 
Moreover , the method proves to be efÔ¨Åcient and reduces the incurred energy consumption . 
1 Introduction The goal of sentence meaning representations is to capture the meaning of sentences in a well - deÔ¨Åned format . 
One of the most prominent frameworks for achieving this is Abstract Meaning Representation ( AMR ) ( Banarescu et al . 
, 2013 ) . 
In AMR , sentences are represented as directed acyclic and rooted graphs . 
An example is displayed in Figure 1 , where we see three equivalent displays of an AMR that represents the meaning of the sentence ‚Äú The baby is sleeping well ‚Äù . 
In AMR , nodes are variables or concepts , while ( labeled ) edges express their relations . 
Among other phenomena , this allows AMR to capture coreference ( via re - entrant structures ) and semantic roles ( via : argnrelation ) . 
Furthermore , AMR links sentences to KBs : e.g. , predicates are mapped to PropBank ( Palmer et al . 
, 2005 ; Kingsbury and Palmer , 2002 ) , while named ws bsleep-1 baby arg0mod    ( s / sleep-01      : arg0 ( b / baby )      : mod ( w / well ) ) Penman notation well { < s , instance , sleep > ,     < s , arg0 , b > ,    < b , instance , baby > ,    < w , instance , well > ,    < s , mod , w > } Triples Figure 1 : Equivalent representations of the AMR for ‚Äú The baby is sleeping well ‚Äù . 
( p4 ¬† / ¬† possible-01 : arg1 ¬† ( d5 ¬† / ¬† destabilize-01 ¬†¬†¬†¬†¬†¬†¬†¬† : arg0 [: arg1 ] ¬† ( c3 ¬† / ¬† country ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†       : quant ¬† ( w2 ¬† / ¬† whole ) ) ) ¬†¬†¬†¬† : condition ¬† ( e1 ¬† / ¬† economy    [: poss c3 ]  ¬†¬†¬†¬†¬†¬†¬†¬† : arg0 - of ¬† ( f0 ¬† / ¬† function-01    [: pol - ] ) ) ) Figure 2 : Parse of Without a functioning economy , the whole country may destabilize with errors outlined . 
entities are linked to Wikipedia . 
From a logical perspective , AMR is closely related to Ô¨Årst - order logic ( FOL , see Bos ( 2016 , 2019 ) for translation mechanisms ) . 
Currently , AMRs are leveraged to enhance a variety of natural language understanding tasks . 
E.g. , they have enhanced commonsense reasoning and question answering ( Mitra and Baral , 2016 ) , machine translation ( Song et al . 
, 2019 ) , text summarization ( Liao et al . 
, 2018 ; Dohare et al . 
, 2017 ) and paraphrasing ( Issa et al . 
, 2018 ) . 
However , there is a critical issue with automatically generated AMRs ( parses ): they are often deÔ¨Åcient . 
These deÔ¨Åciencies can be quite severe , even when high - performance parsers are used . 
For example , in Figure 2 , a neural parser ( Lyu and Titov , 2018 ) conducts several errors when parsing Without a functioning economy the whole country may destabilize . 
E.g. , it misses a negative polarity and classiÔ¨Åes a patient argument as the agent by failing235 graph representation computer processing human understanding well - deÔ¨Åned triples (e.g . 
, GNN )   graph visualization   ( short sentences )  PENMAN , linearized string (e.g . 
, LSTM )   PENMAN , indents (this work )   Table 1 : Four ( equivalent ) AMR representations and their accessibility with respect to human or computer (  : ‚Äò okay ‚Äô ,  : ‚Äò perhaps possible , but difÔ¨Åcult ‚Äô ) . 
to see that destabilize here functions as an ergative verb ( parser : the country is the causer of destabilize ; correct : the country is the object that is destabilized ) . 
In sum , the parse has misrepresented the sentence ‚Äôs meaning.1However , assessing such deÔ¨Åciencies via comparison against a gold reference ( as in classical parser evaluation ) is often infeasible in practice : it takes a trained annotator and appr . 
10 minutes to manually create one AMR ( Banarescu et al . 
, 2013 ) . 
To mitigate these issues , we would like to automatically rate the quality of AMRs without the costly gold graphs . 
This would allow us to signal downstream task systems the incorporated graphs ‚Äô trustworthiness or select among different candidate graphs from different parsing systems . 
To achieve this , we propose a method that imitates a human rater , who is inspecting the graphs . 
We show that the method can efÔ¨Åciently rate the quality of the AMRs in the absence of gold graphs . 
The remainder of the paper is structured as follows : in Section 2 , we outline our idea to exploit the textual multi - line string representation of AMRs , allowing for efÔ¨Åcient and simple AMR processing while preserving vital graph structure . 
In Section 2.2 , we instantiate this idea in a lightweight CNN that predicts the quality of AMR graphs along multiple dimensions of interest . 
In our experiments ( Section 3 ) , we show that this framework is efÔ¨Åcient and performs better than strong baselines . 
Our code is available at https://github . 
com / flipz357 / amr - quality - rater . 
2 AMR as image with latent channels In this section , we Ô¨Årst motivate to treat AMRs as images with latent channels in order to rate them efÔ¨Åciently . 
Second , we brieÔ¨Çy describe the task at hand : Rating the quality of AMR graphs in the absence of gold graphs . 
Finally , to solve this task , we create a lightweight CNN that evaluates AMR quality in multiple dimensions of interest . 
1?With a functioning economy , the whole country may cause something to destabilize . 
( p4 ¬† / ¬† possible-01      : pol      : arg1 ¬† ( d5 ¬† / ¬† destabilize-01 ¬†¬†¬†¬†¬†¬†¬†  : arg1 ¬† ( c3 ¬† / ¬† country ¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† : quant ¬† ( w2 ¬† / ¬† whole ) ) ) ¬†¬†¬†¬† : condition ¬† ( e1 ¬† / ¬† economy          : poss c3 ¬†¬†¬†¬†¬†¬†¬†¬† : arg0 - of ¬† ( f0 ¬† / ¬† function-01)))possible p4-   pol d5 c3w2e1 arg1                     condition poss        arg1            quant                f0                    arg0 - of       destabilize-01 countrywholeeconomy function-01 Figure 3 : Different displays for an AMR structure of a sentence that has medium length ( left : P ENMAN notation , right : graphical visualization ) The P ENMAN notation and its ( hidden ) advantages The native AMR notation is called PENMAN -notation or Sentence Plan Language ( Kasper , 1989 ; Mann , 1983 ) . 
Provably , an advantage of this notation is that it allows for secure AMR storage in text Ô¨Åles . 
However , we argue that it has more advantages . 
For example , due to its clear structure , it allows humans a fairly quick understanding even of medium - sized to large AMR structures ( Figure 3 , left ) . 
On the other hand , we argue that a graphical visualization of such medium - sized to large AMRs ( Figure 3 , right ) could hamper intuitive understanding , since the abundant visual signals ( circles , arrows , etc . 
) may more easily overwhelm humans . 
Moreover , in every display , one would depend on an algorithm that needs to determine a suitable ( and spacious ) arrangement of the nodes , edges and edge labels . 
It may be for these reasons , that in the AMR annotation tool2 , the graph that is under construction is always shown in PENMAN notation to the human user . 
In sum , we Ô¨Ånd that the indented multi - line PENMAN form possesses three key advantages ( Table 1 ): ( i ) it enables fairly easy human understanding , ( ii ) , it is well - deÔ¨Åned and ( iii ) , which is what we will show next , it can be computationally exploited to better rate AMR quality . 
AMR as image to preserve graph structure Figure 4 describes our proposed sentence representation treatment . 
After non - degenerate AMR graph simpliÔ¨Åcation ( more details in Preprocessing , 3.1 ) , we Ô¨Årst project the PENMAN representation onto a small grid ( ‚Äò image ‚Äô ) . 
Each AMR token ( e.g. , a node or an edge ) is represented as a ‚Äò categorical pixel ‚Äô . 
Second , Œ¶adds latent ‚Äò channels ‚Äô to the categorical pixels , which can be learned incrementally in an application . 
In other words , every AMR token is represented by a Ô¨Åxed - sized vector of real 2https://www.isi.edu/cgi-bin/div3/mt/ amr - editor / login - gen - v1.7.cgi236 ws bsleep-1 baby arg0mod ( s / sleep-01        : arg0 ( b / baby )        : mod ( w / well ) ) ( sleep-01      : arg0 baby      : mod well ) wellsleep-01 babyarg0modsleep-01 : arg0 : modbaby wellsleep-01 : arg0 : modbaby well AMR as imageAMR as image With latent channelswell Figure 4 : We transform the ( simpliÔ¨Åed ) P ENMAN representation to an image and use Œ¶to add latent channels . 
numbers . 
These vectors are arranged such that the original graph structure is fully preserved . 
2.1 Task : Rating the quality of AMR graphs We aim at rating the quality of AMR graphs ( ‚Äò parses ‚Äô ) in the absence of gold graphs . 
This boils down to answering the following question : how well does a candidate AMR graph capture a given natural language sentence ? Therefore , the exact goal in this task is to learn a mapping f : S√óG‚Üí Rd , ( 1 ) that maps a sentence s‚ààStogether with a candidate AMR graph g‚ààG ontodscores , which describe the AMR with regard to dquality dimensions of interest . 
A successful mapping function should strongly correlate with the gold scores as they would emerge from evaluation against gold graphs . 
We proceed by describing the targeted dimensions in more detail . 
Main AMR quality dimensions The main quality dimensions that we desire our model to predict are estimated Smatch F1 /recall /precision . 
Smatch is the canonical AMR metric , assessing the triple overlap between two graphs , after an alignment step ( Cai and Knight , 2013 ) . 
AMR sub - task quality dimensions However , we predict also other quality dimensions to assess various AMR aspects ( Damonte et al . 
, 2017 ) . 
In this place , we can merely provide a brief overview : ( i)Unlabeled : Smatch F1 when disregarding edgelabels . 
( ii ) No WSD : Smatch F1 when ignoring ProbBank senses . 
( iii ) Frames : PropBank frame identiÔ¨Åcation F1 ( iii ) WikiÔ¨Åcation : KB linking F1 score on : wiki relations . 
( iv ) Negations : negation detection F1 . 
( v ) NamedEnt : NER F1 . 
( vi ) NS frames : F1 score for ProbBank frame identiÔ¨Åcation when disregarding the sense . 
( vii ) Concepts F score for concept identiÔ¨Åcation ( viii ) SRL : Smatch F1 computed on arg - i roles only . 
( ix ) Reentrancy : Smatch F1 computed on re - entrant edges only . 
( x)IgnoreVars : F1 when variable nodes are ignored . 
( xi)Concepts : F1 for concept detection . 
2.2 A lightweight CNN to rate AMR quality We want to model f(Eq . 
1 ) in order to estimate a suite of quality scores y‚ààRdfor any automatically generated AMR graph , given only the graph and the sentence from whence it is derived . 
Following Opitz and Frank ( 2019 ) , we will contrast the AMR against the sentence ‚Äôs dependency parse , exploiting observed structural similarities between these two types of information ( Wang et al . 
, 2015 ) . 
Our proposed method allows this in a simple way by processing dependency and AMR graphs in parallel . 
The architecture is outlined in Figure 5 . 
Symbol embedding The latent channels of AMR and dependency ‚Äò pixels ‚Äô represent the embeddings of the ‚Äò tokens ‚Äô or ‚Äò symbols ‚Äô contained in the AMR and dependency vocabulary . 
These symbols represent nodes or edges . 
We use two special tokens : the < tab > token , which represents the indention level , and the < pad > token , which Ô¨Ålls the remaining empty ‚Äò pixels ‚Äô . 
By embedding lookup , we obtain AMR and dependency images with 128 latent channels and 45x15 ‚Äò pixels ‚Äô ( Œ¶in Figure 5 ; the amount of pixels is chosen such that more than 95 % of training AMRs can be fully captured ) . 
Encoding local graph regions Given AMR and dependency images with 128 latent channels and 45x15 pixels , we apply to each of the two images 256 Ô¨Ålters of size 3x3 , which is a standard type of kernel in CNNs . 
This converts both graphs to 256 feature maps each ‚ààR45√ó15(samepadding ) , obtaining two three - dimensional tensors L1 amr , L1 dep‚ààR45√ó15√ó256 . 
From here , we construct our Ô¨Årst joint representation , which matches local dependency regions with local AMR regions : jres = GPF ( L1 amr‚äóL1 dep ) , ( 2 ) wherex‚äóy= [ x‚äôy;x / circleminusy]denotes the concatenation of element - wise multiplication and element - wise subtraction . 
GPF is an operation that performs global pooling and vectorization ( ‚Äò Ô¨Çattening ‚Äô ) of any input tensor . 
This means that jres‚ààR512is a joint representation of the locally matched dependency and AMR graph regions . 
This intermediate process is outlined in Figure 5 by ‚äó ( left ) and GPF . 
Finally , we reduce the dimensions of the two intermediate three - dimensional representationsL1 amrandL1 depwith 3x3 max - pooling and237 Figure 5 : Our proposed architecture for efÔ¨Åcient AMR quality assessment . 
obtainL2 amrandL2 dep‚ààR15√ó5√ó256 Encoding global graph regions For a moment , we put the joint residual ( jres ) aside and proceed by processing the locally convolved feature maps with larger Ô¨Ålters . 
While the Ô¨Årst convolutions allowed us to obtain abstract local graph regions L2 amrand L2 dep , we now aim at matching more global regions . 
More precisely , we use 128 2D Ô¨Ålters of shape 10x5 , followed by a 5x5 max - pooling operations on L2 amrandL2 dep . 
Thus , we have obtained vectorized abstract global graph representations gamr , gdep‚àà R384 . 
Then , we construct a joint representation ( right‚äó , Figure 5 ): jglob = gamr‚äógdep . 
( 3 ) At this point , together with the joint residual representation from the local region matching , we have arrived at two joint vector representations jgloband jres . 
We concatenate them ( [ ¬∑ ; ¬∑ ] in Figure 5 ) to form one joint representation j‚ààR1280 : j= [ jres;jglob ] ( 4 ) Quality prediction The shared representation j is further processed by a feed - forward layer with ReLU activation functions ( FF + ReLU , Figure 5 ) and a consecutive feed - forward layer with sigmoid activation functions ( FF + sigm , Figure 5 ): y = sigm ( ReLU ( jTA)B ) , ( 5 ) whereA‚ààR1280√óh , B‚ààRh√ódim ( out)are parameters of the model and sigm ( x ) = ( 1 1+e‚àíx1, ... ,1 1+e‚àíxdim ( out))projectsxonto[0,1]dim ( out ) . 
When estimating the main AMR metric scores we instantiate three output neurons ( dim(out ) = 3 ) that represent estimated Smatch precision , Smatch recall and Smatch F1 . 
In the case where we are interested in a more Ô¨Åne - grained assessment of AMR quality ( e.g. , knowledge - base linking quality ) , we have 33 output neurons representing expected scores for various semantic aspects involved in AMR parsing ( we predict precision , recall and F1 of 11 aspects , as outlined in¬ß2.1 ) . 
To summarize , the residual joint representation should capture local similarities . 
On the other hand , the second joint representation aims to capture the more global and structural properties of the two graphs . 
Both types of information inform the Ô¨Ånal quality assessment of our model in the last layer . 
3 Experiments In this section , we Ô¨Årst describe the data , changes to the data that target the reduction of biases , and the baseline . 
After discussing our main results , we conduct further analyses . 
( i ) , we study the effects of our data - debiasing steps . 
( ii ) , we assess the performance of our model in a classiÔ¨Åcation task ( distinguishing good from bad parses ) . 
( iii ) , we assess the model performance when we only provide the candidate AMR and the sentence ( dependency tree ablation ) . 
( iv ) , we provide detailed measurements of the method ‚Äôs computational cost . 
3.1 Experimental setup Data We use the data from Opitz and Frank ( 2019 ) . 
The data set consists of more than 15,000238 sentences with more than 60,000 corresponding parses , by three different automatic parsing systems and a human . 
More precisely , the data set D={(si , gi , yi)}N i=1consists of tuples ( si , gi , yi ) , wheresi‚àà S is a natural language sentence , gi‚ààGis a ‚Äò candidate ‚Äô AMR graph and yi‚ààRdis a 36 - dimensional vector containing scores which represent the quality of the AMR graph in terms of precision , recall and F1 with respect to 12 different tasks captured by AMR ( as outlined in ¬ß 2.1 ) . 
Debiasing of the data We observe three biases in the data . 
First , the graphs in the training section of our data are less deÔ¨Åcient than in the development and testing data , because the parsers were trained on ( sentence , gold graph ) pairs from the training section . 
For our task , this means that the training section ‚Äôs target scores are higher , on average , than the target scores in the other data partitions . 
To achieve more balance in this regard , we re - split the data randomly on the sentence - id level ( such that a sentence does not appear in more than one partition with different parses ) . 
Second , we observe that the data contains some superÔ¨Åcial hidden clues that could give away the parse ‚Äôs source . 
This bears the danger that a model does not learn to assess the parse quality , but to assess the source of the parse . 
And since some parsers are better or worse than others , the model could exploit this bias . 
For example , consider that one parser prefers to write ( r / run-01 : arg1 ( c / cat ) : polarity - ) , while the other parser prefers to write ( r / run-01 : polarity - : arg1 ( c / cat ) ) . 
These two structures are semantically equivalent but differ on the surface . 
Hence , the arrangement of the output may provide unwanted clues on the source of the parse . 
To alleviate this issue , we randomly re - arrange all parses on the surface , keeping their semantics.34 A third bias stems from a design choice in the metric scripts used to calculate the target scores . 
More precisely , the extended Smatch metric script , per default , assigns a parse that does not contain a certain edge - type ( e.g. , : arg n ) the score 0 with respect to the speciÔ¨Åc quality dimension ( in this case , SRL : 0.00 Precision / Recall / F1 ) . 
However , if the gold parse also does not contain an 3Technically , this is achieved by reformatting the parses such that in the depth-Ô¨Årst writing - traversal at node nthe out - going edges of nwill be traversed in random order . 
4Different variable names , e.g. , ( r / run-01 ) and ( x / run-01 ) are not an issue in this work since the variables are handled via ( van Noord and Bos , 2017a ) . 
See also Preprocessing , ¬ß 3.1edge of this type ( i.e. , : arg n ) , then we believe that the correct default score should be 1 , since the parse is , in the speciÔ¨Åc dimension , in perfect agreement with the gold ( i.e. , SRL : 1.00 Precision / Recall / F1 ) . 
Therefore , we set all sub - task scores , where the predicted graph agrees with the gold graph in the absence of a feature , from 0 to 1 . 
Preprocessing Same as prior work , we dependency - parse and tokenize the sentences with spacy ( Honnibal and Montani , 2017 ) and replace variables with corresponding concepts ( e.g. , ( j / jump-01 : arg0 ( g / girl ) ) is translated to ( jump-01 : arg0 ( girl ) ) . 
Re - entrancies are handled with pointers according to van Noord and Bos ( 2017a ) , which ensures non - degenerate AMR simpliÔ¨Åcation.5Furthermore , we lower - case all tokens , remove quotation marks and join sub - structures that represent names.6The vocabulary encompasses all tokens of frequency ‚â•5 , remaining ones are set to < unk > . 
Training All parameters are initialized randomly . 
We train for 5 epochs and select the parameters Œ∏from the epoch where maximum development scores were achieved ( with respect to average Pearson‚ÄôsœÅover the quality dimensions ) . 
In training , we reduce the squared error with gradient descent ( Adam rule ( Kingma and Ba , 2019 ) , learning rate = 0.001 , mini batch size = 64 ): Œ∏‚àó= arg min Œ∏|D|/summationdisplay i=1|M|/summationdisplay j=1(yi , j‚àífŒ∏(si , gi)j)2,(6 ) whereMis the set of target metrics . 
Baseline Our main baseline is the model of previous work , henceforth denoted by LG - LSTM . 
The method works in the following steps : Ô¨Årst , it uses a depth-Ô¨Årst graph traversal to linearize the automatic AMR graph and the corresponding dependency tree of the sentence . 
Second , it constructs a joint representation and predicts the score estimations . 
To further improve its performance , the baseline uses some extra - features ( e.g. , a shallow alignment from 5For example , consider the sentence The cat scratches itself and its graph ( x / scratch-01 : arg0 ( y / cat ) : arg1 y ) ) . 
Replacing the variables with concepts would come at the cost of an information loss w.r.t . 
to coreference : ( scratch-01 : arg0 cat : arg1 cat ) ‚Äî does the cat scratch itself or another cat ? Hence , pointers are used to translate the graph into ( scratch-01 : arg0 * 0 * cat : arg1 * 0 * ) ) . 
6E.g . 
, : name ( name : op1 ‚Äò Barack ‚Äô : op2 ‚Äò Obama ‚Äô ) is translated to : name barack obama .239 Smatch Ridge GNN LG - LSTM ours change % P‚ÄôsœÅF1 0.428 0.659 0.662¬±0.000.696¬±0.00 + 5.14‚Ä†‚Ä° Precision 0.348 0.601 0.600¬±0.000.623¬±0.01 + 3.83‚Ä† Recall 0.463 0.667 0.676¬±0.000.719¬±0.00 + 6.36‚Ä†‚Ä°RMSEF1 0.155 0.132 0.130¬±0.000.128¬±0.00 - 1.54 Precision 0.146 0.127 0.126¬±0.000.126¬±0.00 + -0.0 Recall 0.169 0.141 0.142¬±0.000.136¬±0.00 - 4.23 Table 2 : Main results . 
Pearson ‚Äôs corr . 
coefÔ¨Åcient ( row 1 - 3 ) is better if higher ; root mean square error ( RMSE , row 4 - 6 ) is better if lower . 
The quality dimensions are explained in¬ß2.1.‚Ä†(‚Ä° ): p<0.05 ( p<0.005 ) , significant difference in the correlations with two - tailed test using Fisher œÅto z transformation ( Fisher , 1915 ) . 
dependency tokens to AMR tokens).7Generally speaking , the baseline is a model that works based on graph linearizations . 
Such type of model , despite its apparent simplicity , has proven to be an effective baseline or state - of - the - art method in various works about converting texts into graphs ( Konstas et al . 
, 2017 ; van Noord and Bos , 2017b ) , or converting graphs into texts ( Bastings et al . 
, 2017 ; Beck et al . 
, 2018 ; Song , 2019 ; Pourdamghani et al . 
, 2016 ; Song et al . 
, 2018 ; Vinyals et al . 
, 2015 ; Mager et al . 
, 2020 ) , or performing mathematically complex tasks modeled as graph - to - graph problems , such as symbolic integration ( Lample and Charton , 2020 ) . 
However , in our main results , we also display the results of two additional baselines : GNN ( Song et al . 
, 2018 ) , where we encode the dependency tree and the AMR with a graph - recurrent encoder and perform regression on the joint averaged node embedding vectors.8And Ridge , an l2 - regularized linear regression that is based on shallow graph statistics.9 3.2 Results Main AMR quality dimensions The main quality of an AMR graph is estimated in expected triple match ratios ( Smatch F1 , Precision and Recall ) . 
The results , averaged over 10 runs , are displayed in Table 2 . 
With regard to estimated Smatch F1 , we 7Furthermore , the baseline uses auxiliary losses to achieve a slight performance gain in predicting the Smatch metrics . 
For the sake of simplicity , we do not use these auxiliary losses , except in one experiment , where we show that our method achieves a similar small gain with the auxiliary losses . 
8 / bracketleftbigg 1 |VA|/summationtext v‚ààVAemb(v)/bracketrightbigg ‚äó/bracketleftbigg 1 |VD|/summationtext v‚ààVDemb(v)/bracketrightbigg . 
9For the dependency graph ( D ) and the AMR graph ( A ) we both compute œÜ(A|D)= [ density , avg . 
node degree , node count , edge count , ( arg0 |subj ) count , ( arg1|obj ) count ] , the Ô¨Ånal feature vector then is deÔ¨Åned as Œ¶(x)= [ œÜ(A ) -œÜ(D ) ; œÜ(D);œÜ(A);|lemmas(D)‚à©concepts(A)| |lemmas(D)‚à™concepts(A)|]Quality Dim . 
LG - LSTM ours change % F1 Pearson ‚Äôs œÅConcepts 0.508¬±0.010.545¬±0.01 + 7.28‚Ä† Frames 0.420¬±0.010.488¬±0.01 + 16.19‚Ä†‚Ä† IgnoreVars 0.627¬±0.010.665¬±0.00 + 6.06‚Ä†‚Ä† NamedEnt . 
0.429¬±0.020.460¬±0.01 + 7.23‚Ä† Negations 0.685¬±0.020.746¬±0.01 + 8.91‚Ä†‚Ä† NoWSD 0.640¬±0.010.680¬±0.00 + 6.25‚Ä†‚Ä† NS - frames 0.419¬±0.020.505¬±0.01 + 20.53‚Ä†‚Ä† Reentrancies 0.508¬±0.010.602¬±0.00 + 18.50‚Ä†‚Ä† SRL 0.519¬±0.010.581¬±0.01 + 11.95‚Ä†‚Ä† Unlabeled 0.628¬±0.010.663¬±0.00 + 5.57‚Ä†‚Ä† WikiÔ¨Åcation 0.901¬±0.000.904¬±0.00 + 0.33F1 RMSEConcepts 0.117¬±0.000.114¬±0.00 - 2.56 Frames 0.186¬±0.000.182¬±0.00 - 2.15 IgnoreVars 0.195¬±0.000.186¬±0.00 - 4.62 NamedEnt . 
0.159¬±0.000.156¬±0.00 - 1.89 Negations 0.197¬±0.000.180¬±0.00 - 8.63 NoWSD 0.132¬±0.000.126¬±0.00 - 4.55 NS - frames 0.157¬±0.000.155¬±0.00 - 1.27 Reentrancies 0.285¬±0.000.265¬±0.00 - 7.02 SRL 0.189¬±0.000.181¬±0.00 - 4.23 Unlabeled 0.124¬±0.000.121¬±0.00 - 2.42 WikiÔ¨Åcation 0.165¬±0.000.162¬±0.00 - 1.82 Table 3 : Results for AMR quality rating w.r.t . 
various sub - tasks.‚Ä†(‚Ä° ): signiÔ¨Åcance ( c.f . 
caption Table 2 ) . 
achieve a correlation with the gold scores of 0.695 Pearson‚ÄôsœÅ . 
This constitutes a signiÔ¨Åcant improvement of appr . 
5 % over LG - LSTM . 
Similarly , recall and precision correlations improve by 6.36 % and 3.83 % ( from 0.676 to 0.719 and 0.600 to 0.623 ) . 
While the improvement in predicted recall is significant at p<0.05 and p<0.005 , the improvement in predicted precision is signiÔ¨Åcant at p < 0.05 . 
When we consider the root mean square error ( RMSE ) , we Ô¨Ånd that the method improves over the best baseline by -1.54 % in estimated Smatch F1 and -4.23 % in estimated Smatch recall . 
On the other hand , the RMS error in estimated precision remains unchanged . 
AMR subtask quality Our model can also rate the quality of an AMR graph in a more Ô¨Åne - grained way . 
The results are displayed in Table 3 . 
Over almost every dimension we see considerable improvements . 
For instance , a considerable improvement in Pearson ‚Äôs œÅis achieved for assessment of frame prediction quality ( ‚Äò NSFrames ‚Äô in Table 3 , +20.5%œÅ ) and coreference quality ( ‚Äò Reentrancies ‚Äô in Table 3 , +18.5 % ) . 
A substantial error reduction is achieved in polarity ( ‚Äò Negations ‚Äô , Table 3 ) , where we reduce the RMSE of the estimated F1 score by -8.6 % . 
When rating the SRL - quality of an AMR parse , our model reduces the RMSE by appr . 
4 % . 
In general,240 Pearson‚ÄôsœÅ error data method P R F1 RMSE ( F1 ) 0 2LG - LSTM 0.72 0.78 0.77 0.138 LG - LSTM + aux 0.74 0.79 0.78 0.137 ours 0.75 0.80 0.79 0.133 ours + aux 0.76 0.81 0.80 0.132 1 2LG - LSTM 0.67 0.73 0.72 0.120 ours 0.68 0.75 0.74 0.117 2 2LG - LSTM 0.60 0.68 0.66 0.130 ours 0.62 0.72 0.70 0.128 Table 4 : Performance - effects of data debiasing steps . 
+ auxindicates a model variant that is trained using auxiliary losses that incorporate information about the other AMR aspects in the training process ( see Fn.7 ) . 
improvements are obtained over almost all tested quality dimensions , both in RMSE reduction and increased correlation with the gold scores . 
3.3 Analysis Effect of data debiasing We want to study the effect of the data set cleaning steps by analyzing the performance of our method and the baseline on three different versions of the data , with respect to estimated Smatch scores . 
The three versions are ( i ) 0 2= A MRQUALITY , which is the original data ; ( ii ) 1 2 , which is the data after the random re - split and score correction ; ( iii)2 2= A MRQUALITY CLEAN which is our main data after the Ô¨Ånal debiasing step ( shallow structure debiasing ) has been applied . 
The results are shown in Table 4 . 
We can make three main observations : ( i ) from the Ô¨Årst to the second debiasing step , the baseline and our model have in common that Pearson ‚Äôs œÅand the error decrease . 
While we can not exactly explain why œÅ decreases , it is somewhat in line with recent research that observed performance drops when data was re - split ( Gorman and Bedrick , 2019 ) . 
On the other hand , the error decrease can be explained by the random re - split that balances the target scores . 
( ii ) The second debiasing step leads to a decrease in œÅand an increase in error , for both models . 
This indicates that we have successfully removed shallow biases from the data that can give away the parse ‚Äôs source . 
( iii ) On all considered versions of the data , the method performs better than the baseline . 
AMRs : telling the good from the bad In this experiment , we want to see how well the model can discriminate between good and bad graphs . 
To this aim , we create a Ô¨Åve - way classiÔ¨Åcation task : graphs are assigned the label ‚Äò very bad ‚Äô ( Smatch F1 < 0.25 ) , ‚Äò bad ‚Äô ( 0.25‚â•Smatch F1<0.5 ) , ‚Äò good‚Äômajority random LG - LSTM ours avg . 
F1 0.13 0.20 0.40 0.44‚Ä†‚Ä° quadr . 
kappa 0.0 0.03 0.53 0.60‚Ä†‚Ä° Table 5 : Graph quality classiÔ¨Åcation task . 
‚Ä†(‚Ä° ) significance with paired t - test at p < 0.05 ( p<0.005 ) over 10 random inititalizations . 
Quality Dim . 
LG - LSTM ours ours ( no dep.)P‚ÄôsœÅSmatch F1 0.662¬±0.000.696¬±0.000.682¬±0.01 Smatch precision 0.600¬±0.000.623¬±0.010.614¬±0.01 Smatch recall 0.676¬±0.000.719¬±0.000.702¬±0.01RMSESmatch F1 0.130¬±0.000.128¬±0.000.128¬±0.00 Smatch precision 0.126¬±0.000.126¬±0.000.129¬±0.00 Smatch recall 0.142¬±0.000.136¬±0.000.139¬±0.00 Table 6 : Right column : results of our system when we abstain from feeding the dependency tree , and only show the sentence together with the candidate AMR . 
( 0.5‚â•Smatch F1<0.75 ) , ‚Äò very good ‚Äô ( 0.75 ‚â• Smatch F1 < 0.95 ) and ‚Äò excellent ‚Äô ( Smatch F1 ‚â•0.95 ) . 
Here , we do not retrain the models with a classiÔ¨Åcation objective but convert the estimated Smatch F1 to the corresponding label . 
Since the classes are situated on a nominal scale , and ordinary classiÔ¨Åcation metrics would not fully reÔ¨Çect the performance , we also use quadratic weighted kappa ( Cohen , 1968 ) for evaluation . 
The results are shown in Table 5 . 
All baselines , including LG - LSTM , are signiÔ¨Åcantly outperformed by our approach , both in terms of macro F1 ( +4 points , 10 % improvement ) and quadratic kappa ( +7 points , 13 % improvement ) . 
How important is the dependency information ? To investigate this question , instead of feeding the dependency tree of the sentence , we only feed the sentence itself . 
To achieve this , we simply insert the tokens in the Ô¨Årst row of the former dependency input image , and pad all remaining empty ‚Äò pixels ‚Äô . 
In this mode , the sentence encoding is similar to standard convolutional sentence encoders as they are typically used in many tasks ( Kim , 2014 ) . 
The results are shown in the right column of Table 6 . 
The performance drops are small but consistent across all analyzed dimensions , both in terms of error ( 0 to 2.2 % increase ) and Pearson ‚Äôs œÅ(1.4 to 2.4 % decrease ) . 
This indicates that the dependency trees contain information that can be exploited by our model to better judge the AMR quality . 
We hypothesize that this is due to similarities between relations such as subj / obj(syntactic ) or arg0 / arg1 ( semantic ) , etc . 
Yet , we see that this simpler model,241 GPU type GTX Titan GTX 1080 method LG - LSTM ours LG - LSTM ours avg . 
ep . 
time 722s 59s 1582s 64s avg . 
W 105 166 45 128 kWh per epoch 0.021 0.003 0.020 0.002 Table 7 : EfÔ¨Åciency analysis of two approaches . 
time / epoch ( hours ) kWh / epoch0.00.10.20.30.40.50.6cost LG - LSTM ours Figure 6 : Training cost diagram of two approaches . 
which does not see the dependency tree , still outperforms the baseline , except in estimated precision , where the error is increased by 2.4 % . 
EfÔ¨Åciency analysis Recently , in many countries , there have been efforts to reduce energy consumption and carbon emission . 
Since deep learning typically requires intensive GPU computing , this aspect is of increasing importance to researchers and applicants ( Strubell et al . 
, 2019 ; Tang et al . 
, 2019 ; Ganguly et al . 
, 2019 ) . 
To investigate energy consumption of our method and previous work , we monitor their GPU usage during training , assessing the following quantities : ( i ) avg . 
time per epoch , ( ii ) avg . 
watts GPU usage , ( iii ) kilowatts per epoch ( in kWh ) . 
The results of this analysis are displayed in Table 7 and outlined in Figure 6 . 
Our method consumes approximately 6.6 times less total kWh on a GTX Titan ( 10 times less on a GTX 1080 ) . 
Directly related , it also reduces the training time : prior work requires appr . 
1500s training time per epoch ( GTX 1080 ) , while our method requires appr . 
60s per epoch ( GTX 1080 ) . 
The main reason for this is that our model does not depend on recurrent operations and proÔ¨Åts more from parallelism . 
4 Related work Quality measurement of structured predictions Since evaluating structured representations against human annotations is costly , systems have been developed that attempt an automatic quality assessment of these structures . 
Due to its popularity , much work has been conducted in machine translation ( MT ) under the umbrella of quality estimation ( QE ) . 
QE can take place either on a wordlevel ( Martins et al . 
, 2017 ) , sentence - level ( Spe - cia et al . 
, 2009 ) , or document - level ( Scarton et al . 
, 2015 ) . 
The conference on Machine Translation ( WMT ) has a long - standing workshop and shared task - series on MT quality assessment ( Bojar et al . 
, 2013 , 2014 , 2015 , 2016 , 2017 ; Specia et al . 
, 2018 ; Fonseca et al . 
, 2019 ) . 
Quality estimation for neural language generation has been investigated , i.a . 
, by Scarton et al . 
( 2016 ) , and recently by Du Àásek et al . 
( 2019 ) , who design a model that jointly learns to rate and rank generations , or by Zopf ( 2018 ) , who predicts pair - wise preferences for generated summaries . 
Furthermore , automatic techniques for the quality assessment of syntactic parses have been proposed . 
For instance , Ravi et al . 
( 2008 ) formulate the task as a single - variable regression problem to assess the quality of constituency trees . 
A major difference to our work is that they try to assess the performance of a single parser , while we aim at a parser - agnostic setting where candidate parses stem from different parsers . 
Similarly , Kawahara and Uchimoto ( 2008 ) predict a binary label that reÔ¨Çects whether the tree - quality lies above a certain threshold ( or not ) . 
When multiple candidate parses are available , tree ranking methods ( Zhu et al . 
, 2015 ; Zhou et al . 
, 2016 ) may also be interpreted as some form of parse quality assessment ( see Do and Rehbein ( 2020 ) for a recent overview ) . 
Compared with assessing the quality of ( abstract ) meaning representations , judging about syntactic trees perhaps is a conceptually slightly simpler task , since the syntactic graphs are more directly grounded in the sentence10 , and therefore it may be easier to judge whether graph components are correct , redundant , missing , or false . 
In comparison to MT , automatic quality assessment of meaning representations is insufÔ¨Åciently researched . 
Opitz and Frank ( 2019 ) propose an LSTM based model that performs a multi - variate quality analysis of AMRs ( constituting the baseline which we compared against ) . 
We believe that quality estimation approaches may also prove valuable for other meaning representation formalisms ( MRs ) , such as , e.g. , discourse representations ( Kamp and Reyle , 1993 ; Kamp , 2008 ; Abzianidze et al . 
, 2019 ) or universal semantic dependencies ( Reisinger et al . 
, 2015 ; Stengel - Eskin et al . 
, 2020 ) . 
For example , since the manual creation of MRs 10In dependency trees , nodes are words ; in constituency trees , nodes are ( labeled ) phrases ; in meaning representations , words or phrases may be projected to abstract semantic nodes , or they may be omitted.242 is a notoriously laborious task , automatic quality assessment tools could assist humans in the annotation process ( e.g. , by serving as a cheap annotation quality check or by Ô¨Åltering automatic parses in active learning ) . 
AMR metrics When a gold graph is available , it can be used to compute the canonical AMR metric Smatch ( Cai and Knight , 2013 ) that assesses matching triples . 
Furthermore , Damonte et al . 
( 2017 ) have extended Smatch to inspect various aspects of AMR . 
In this work , we have shown that our model can predict the expected outcomes of these metrics in the absence of the gold graph . 
Recently , more AMR metrics have been proposed , for example the Bleu - based ( Papineni et al . 
, 2002 ) SemBleu metric ( Song and Gildea , 2019 ) , Sema ( Anchi ÀÜeta et al . 
, 2019 ) or S2match ( Opitz et al . 
, 2020 ) , a variant of Smatch . 
We plan to extend our model such that it also predicts these metrics . 
AMR parsing Recent advances in AMR parsing have been achieved by parsers that either predict latent alignments jointly with nodes ( Lyu and Titov , 2018 ) , or by transducing a graph from a sequence with a minimum spanning tree ( MST ) decoding algorithm ( Zhang et al . 
, 2019 ) , or by focusing on core semantics in a top - down fashion ( Cai and Lam , 2019 ) , or by performing auto - regressive decoding with a graph encoder ( Cai and Lam , 2020 ) . 
Other approaches apply statistical machine translation ( Pust et al . 
, 2015 ) or sequence - to - sequence models , which tend to suffer from data scarcity issues and need considerable amounts of silver data to improve results ( van Noord and Bos , 2017c ; Konstas et al . 
, 2017 ) . 
Previously , alignment - based pipeline models have proved effective ( Flanigan et al . 
, 2014 ) or transition - based approaches that convert dependency trees step - by - step to AMR graphs ( Wang et al . 
, 2015 , 2016 ; Lindemann et al . 
, 2020 ) . 
5 Conclusion In this work , we have developed an approach to rate the quality of AMR graphs in the absence of costly gold data . 
Our model imitates a human judge that is confronted , ‚Äò on paper ‚Äô , with the AMR in its native multi - line Penman format . 
We saw how this setup allowed efÔ¨Åcient AMR processing with convolutions . 
Our experiments indicate that the method rates AMR quality more accurately and more efÔ¨Åciently than previous work . 
Acknowledgments I am grateful to the anonymous reviewers for their valuable thoughts and comments . 
Moreover , I am grateful to Anette Frank for her thoughtful feedback on an earlier draft of this paper and her general guidance throughout my studies . 
Abstract Commonsense explanation generation aims to empower the machine ‚Äôs sense - making capability by generating plausible explanations to statements against commonsense . 
While this task is easy to human , the machine still struggles to generate reasonable and informative explanations . 
In this work , we propose a method that Ô¨Årst extracts the underlying concepts which are served as bridges in the reasoning chain and then integrates these concepts to generate the Ô¨Ånal explanation . 
To facilitate the reasoning process , we utilize external commonsense knowledge to build the connection between a statement and the bridge concepts by extracting and pruning multi - hop paths to build a subgraph . 
We design a bridge concept extraction model that Ô¨Årst scores the triples , routes the paths in the subgraph , and further selects bridge concepts with weak supervision at both the triple level and the concept level . 
We conduct experiments on the commonsense explanation generation task and our model outperforms the state - of - the - art baselines in both automatic and human evaluation.1 1 Introduction Machine commonsense reasoning has been widely acknowledged as a crucial component of artiÔ¨Åcial intelligence and a considerable amount of work has been dedicated to evaluate this ability from various aspects in natural language processing ( Levesque et al . 
, 2011 ; Talmor et al . 
, 2018 ; Sap et al . 
, 2019 ) . 
A large proportion of existing tasks frame commonsense reasoning as multi - choice reading comprehension problems , which lack direct assessment to machine commonsense ( Wang et al . 
, 2019 ) and impede its practicability to realistic scenarios ( Lin ‚àóCorresponding author 1The source code is available at https://github . 
com / cdjhz / CommExpGen .Statement : Theschool was open for summer . 
Explanation : Summertime is typically vacation time forschool . 
Figure 1 : Generating a reasonable and informative explanation involves generating bridge concepts likevacation by identifying the relation to the source concepts , i.e. school andsummer in the statement . 
et al . 
, 2019b ) . 
Recently , Wang et al . 
( 2019 ) proposed a commonsense explanation generation challenge that directly tests machine ‚Äôs sense - making capability via commonsense reasoning . 
In this paper , we focus on the challenging explanation generation task where the goal is to generate a sentence to explain the reasons why the input statement is against commonsense , as shown in Figure 1 . 
Generating a reasonable explanation for a statement faces two main challenges : 1 ) Trivial and uninformative explanations . 
As this task can be formulated as a sequence - to - sequence generation task , existing neural language generation models tend to generate trivial and uninformative explanations . 
For example , one of the existing neural models generates an explanation ‚Äú The school was n‚Äôt open for summer ‚Äù to the statement in Figure 1 . 
Although it is sometimes reasonable , simple modiÔ¨Åcation of the statement to the negation form with no additional information can not explain the reasons why the statement conÔ¨Çicts with commonsense . 
2 ) Noisy commonsense knowledge grounding . 
It ‚Äôs still challenging for most existing language generation models to generate explanations that are faithful to commonsense ( Lin et al . 
, 2019b ) . 
Thus , explicitly incorporating external knowledge sources is necessary for this task . 
Since the nature of the explanation generation task involves using underlying commonsense knowledge to explain , locating useful commonsense knowledge from large - scale knowledge graph is not trivial and generally requires multi - hop reasoning.248 To address the above challenges , we propose a two - stage generation framework that Ô¨Årst extracts the critical concepts served as bridges between the statement and the explanation from an external commonsense knowledge graph , and then generates plausible explanations with these concepts . 
We Ô¨Årst retrieve multi - hop reasoning paths from ConceptNet ( Speer et al . 
, 2017 ) and heuristically prune the paths to maintain the coverage to plausible concepts while keeping the scale of the subgraph tractable . 
Before the extraction stage , we initialize the representation of each node on the subgraph by fusing both the contextual and graph information . 
Then , we design a bridge concept extraction model that scores triples , propagates the probabilities along multi - hop paths to the connected concepts and further extracts plausible concepts . 
In the second stage , we use a pre - trained language model ( Radford et al . 
, 2019 ) to generate the explanation by integrating both the statement and the extracted concept representations . 
Experimental results show that our framework outperforms knowledge - aware text generation baselines and GPT-2 ( Radford et al . 
, 2019 ) in both automatic and human evaluation . 
Particularly , our model generates explanations with more informative content and provides reasoning paths on the knowledge graph for concept extraction . 
To summarize , our contributions are two - fold : ‚Ä¢We analyze the under - explored commonsense explanation generation task and investigate the challenges in incorporating external knowledge graph to aid the generation problem . 
To the best of our knowledge , this is the Ô¨Årst work on generating explanations for counter - commonsense statements . 
‚Ä¢We propose a two - stage generation method that Ô¨Årst extracts the bridge concepts from reasoning paths and then generates the explanation based on these concepts . 
Our model outperforms state - of - the - art baselines on the commonsense explanation generation task in both automatic and human evaluation . 
2 Related Work 2.1 Machine Commonsense Reasoning Previous work on machine commonsense reasoning mainly focuses on the tasks of inference ( Levesque et al . 
, 2011 ) , question answering ( Talmor et al . 
, 2018 ; Sap et al . 
, 2019 ) andknowledge base completion ( Bosselut et al . 
, 2019 ) . 
While the ultimate goals of these tasks are different from ours , we argue that performing explicit commonsense reasoning is also critical to generation . 
A line of work ( Bauer et al . 
, 2018 ; Lin et al . 
, 2019a ) resorts to structured commonsense knowledge and builds graph - aware representations along with the contextualized word embeddings to tackle the commonsense question answering problem . 
In our work , we focus on reasoning over structured knowledge to explicitly infer discrete bridge concepts that are further used for text generation . 
Another line of work ( Rajani et al . 
, 2019 ; Khot et al . 
, 2019 ) identiÔ¨Åes the knowledge gap critical for the complete reasoning chain and Ô¨Ålls the gap by writing general explanation or acquiring Ô¨Åne - grained annotations with human effort . 
While sharing a similar motivation , our method differs from theirs in the sense that we acquire distant supervisions for the bridge concepts to extract reasoning paths and generate plausible explanations without the need of additional human annotation . 
2.2 Knowledge - Grounded Text Generation Existing work that utilizes structured knowledge graphs to generate texts mainly lies in conversation generation ( Zhou et al . 
, 2018 ; Tuan et al . 
, 2019 ; Moon et al . 
, 2019 ) , story generation ( Guan et al . 
, 2019 ) and language modeling ( Ahn et al . 
, 2016 ; Logan et al . 
, 2019 ; Hayashi et al . 
, 2019 ) . 
Zhou et al . 
( 2018 ) and Guan et al . 
( 2019 ) propose to use graph attention that incorporates the information of neighbouring concepts into context representations to help generate the target sentence . 
Yang et al . 
( 2019 ) resort to a dynamic concept memory that updates during essay generation . 
Guan et al . 
( 2020 ) conduct post - training on knowledge triples to enhance the GPT-2 with commonsense knowledge . 
Since one - hop graphs of concepts in the statement have low coverage to the concepts in the explanation , merely leveraging information of individual concepts or triples is not suitable for this task . 
Another direction that utilizes more complex graph is to model multi - hop reasoning by performing random walk ( Moon et al . 
, 2019 ) on the knowledge graph or simulating a Markov process on the pre - extracted knowledge paths ( Tuan et al . 
, 2019 ) . 
While in our task , we do n‚Äôt have access to a parallel grounded knowledge source nor the bridge concepts , which makes the problem even more challenging.249 TheZaVforschoolopensXmmerReaVRQiQg PaWh ReWUieYaOCRQceSWNeWVchRROSRXUce CRQceSWVVchRROVXPPeUVchRROVXPPeUThe ...... VchoolVXmmerYacaWionbreakTranVformerSXPPeUWiPe iV W\SicaOO\ YacaWiRn¬¨WiPe ... TUiSOe ScRUiQgPaWh RRXWiQgCRQceSW SeOecWiRQ ... TRS - UaQNed CRQceSWVYacaWiRQbUeakWimehXmaQ VXPPeUSWaWePeQWFigure 2 : The inference process of our model . 
In the reasoning path retrieval stage ( ¬ß 3.3 ) , a subgraph is Ô¨Årstly retrieved from the ConceptNet given the source concepts ( Cx ) , where each node representation is fused with both textual and graph - aware representations ( ¬ß 3.4 ) . 
Then the model scores each triple on the subgraph , routes the path by propagating the probabilities along paths to the connected nodes , and selects concepts from activated nodes ( ¬ß 3.5 ) . 
Finally , the model generates the explanation by integrating the token embeddings of both the statement and the top - ranked concepts ( ¬ß 3.6 ) . 
3 Methodology 3.1 Task DeÔ¨Ånition The commonsense explanation generation task is deÔ¨Åned as generating an explanation given a statement against commonsense . 
Let x = x1¬∑¬∑¬∑xN be the input statement with Nwords and y= y1¬∑¬∑¬∑yMbe the explanation with Mwords . 
A simple sequence - to - sequence formulation which learns a mapping from xtoycan be adopted in this task : P(y|x ) = M / productdisplay t=1P(yt|y < t , x ) . 
( 1 ) 3.2 Model Overview Formally , our model generates the explanation by Ô¨Årstly extracting the critical bridge concepts con a retrieved knowledge graph Gxgiven the statement xand then integrating the bridge concepts and the statement to generate a proper explanation y , which can be formulated as follows : P(y , c|x ) = P(c|x)P(y|x , c ) ( 2 ) where the bridge concepts care deÔ¨Åned as the unique concepts delivered in the explanation but not mentioned in the statement . 
Figure 2 presents the overview of our model framework . 
Firstly , we retrieve multi - hop reasoning paths from the ConceptNet based on the statement , and heuristically prune the noisy connections to obtain a subgraph for further concept extraction ( ¬ß 3.3 ) . 
To score the paths and concepts , we obtain the fused concept representation for each node on the subgraph by considering both the contextual and graph information ( ¬ß 3.4 ) . 
Secondly , we design a path routing algorithm to propagate the triple probabilities alongmulti - hop paths to the connected concepts and further extract plausible concepts ( ¬ß 3.5 ) . 
Finally , our model generates the explanation by integrating the statement representation and the selected concept representation as inputs ( ¬ß 3.6 ) . 
3.3 Reasoning Path Retrieval In this section , we demonstrate how we retrieve and prune the reasoning paths to form a subgraph . 
We also acquire distant supervision for uncovering the bridge concepts in the subgraph to supervise the concept extraction in the next stage . 
Given an external commonsense knowledge graphG= ( V , E ) , for each statement x , we extract source conceptsCx={ci x}fromxby aligning the surface texts in xto the concepts in V. We also use the stem form of the surface texts to enable soft alignment and Ô¨Ålter out stop words . 
At the training phase , we extract the target concepts Cy={cj y } from the explanation ywith a similar procedure . 
Starting with the source concepts , we then retrieve reasoning paths from the knowledge graph to form a subgraph that has relatively high coverage to the bridge concepts with a tractable scale . 
We Ô¨Årst examine the minimum length of paths that connect source concepts Cxwith each concept in the explanation set Cy‚àíCx . 
As shown in Figure 3 , over 80 % of the examples require two or three hops of connection from the source concepts to the concepts that are merely mentioned in the explanation , which indicates the necessity for multi - hop reasoning . 
We then count the number of concepts covered by subgraphs with different numbers of hops starting from the source concepts ( We only consider concepts in the training data ) . 
As Figure 3 shows , the average number of nodes covered by 3 - hop sub-250 02000400060008000#Nodes1234#Hops020406080Proportion ( % ) Figure 3 : The left axis presents the distribution of the minimum required number of hops to reach the concepts in the explanation set Cy‚àíCxfrom the source concepts inCx . 
The right axis shows the number of nodes in the subgraph with different number of hops . 
graph exceeds 6,000 , indicating the need of path pruning to keep the scale tractable . 
Therefore , we design a heuristic algorithm to retrieve a subgraph Gx={Vx , Ex}from the ConceptNet by expanding the source concepts with 3 hops to cover most bridge concepts . 
To keep the scale of the subgraph tractable , at each iterating step , we enlarge VxwithBneighbour concepts most commonly visited by concepts in Vx . 
Intuitively , the salient bridge concepts should be in a reasonable distance from the source concepts on the graph to maintain the semantic relation and should be commonly visited nodes that support the information Ô¨Çow on the graph . 
We distantly label the bridge concepts as the unique concepts in the explanation that could be covered by the subgraph : Bx‚Üíy={c|c‚ààCy‚àíCx , c‚ààVx } ( 3 ) 3.4 Fused Concept Representation We initialize each node on the subgraph with a fused concept representation hcby considering both the contextual feature of the concept and the graph - aware information . 
We Ô¨Årst obtain the contextualized statement representation Hx‚ààRN√ód1 using a multi - layer bi - directional Transformer encoder ( Vaswani et al . 
, 2017 ) . 
H0 x = onehot(x)¬∑We+Wp ( 4 ) Hl x = trmblock ( Hl‚àí1 x ) , l= 1, ... ,L ( 5 ) where Weis the token embedding matrix , Wpis the position embedding matrix , trmblock ( ¬∑ ) is the transformer block with bi - directional attention andLis the number of Transformer blocks . 
We typically choose the output of the last layer HL xas the statement representation Hx . 
Then we consider the following embeddings:‚Ä¢Context - aware token embedding . 
In order to enhance the contextual dependency of the conceptcto the statement x , we utilize a biattention network ( Seo et al . 
, 2016 ) that models the cross interaction between the concept and the statement . 
Htok c = onehot(c)¬∑We ( 6 ) Hcon c = bi - attention ( Htok c , Hx)(7 ) Then we integrate Htok candHcon cby max pooling and linear transformation to obtain a Ô¨Åxed - length representation that encodes the textual information of the concept c : htext c = mlp / parenleftBig max / parenleftbig [ Htok c;Hcon c]/parenrightbig / parenrightBig ( 8) ‚Ä¢Concept distance embedding . 
To encode the graph - aware structure information into the node representation , we design a concept distance embedding hdist c‚ààRd1that encodes the relative distance from concept cto the source conceptsCxon the subgraph . 
SpeciÔ¨Åcally , the concept distance for concept cis deÔ¨Åned as the minimum length of the path that can be reached from one source concept in Cx : dc= min cx‚ààCxDist(cx , c ) ( 9 ) The concept distance is then used as an index to look up a trainable matrix Wdand obtain thehdist c‚ààRd1 . 
Finally , the fused concept representation hcis obtained by concatenating the context - aware token embedding and the concept distance embedding . 
hc= [ htext c;hdist c ] ( 10 ) 3.5 Bridge Concept Extraction We describe the core component of our method in this section , which extracts the bridge concepts for further explanation generation . 
It Ô¨Årst scores triples on the subgraph to downweight the noisy paths . 
Then it aggregates the path scores to each connected concepts by a path routing process and deactivates the nodes with low routing scores . 
Finally it selects top - ranked bridge concepts from the activated nodes.251 3.5.1 Triple Scoring Firstly , we calculate the triple scores according to the representation of triples and the input statement . 
For each triple e= ( ce , head , re , ce , tail ) wherece , head /ce , tail indicates the head / tail concept andredenotes the relation , we can obtain its representation by concatenating the representations of the head concept , the relation and the tail concept : he= [ hce , head;hre;hce , tail ] ( 11 ) Both the head and the tail representations are calculated by Equation ( 10 ) and the relation representation is acquired by indexing a trainable relation embedding matrix Wr . 
Then we use the statement representation to query each triple representation by taking the bilinear dot - product attention and calculate the selection probability for each triple : hx = max - pooling ( Hx)‚ààRd1(12 ) P(e|x ) = œÉ(heW2hT x ) ( 13 ) We adopt weak supervision to supervise the triple scoring process . 
For each concept c‚ààBx‚Üíy , we obtain the set of the shortest paths Px‚Üícusing the breadth-Ô¨Årst search from each concept ofCxtoc . 
We consider all these shortest paths Px‚Üíy=/uniontext c‚ààBx‚ÜíyPx‚Üícas the supervision of our triple scoring process as they connect the reasoning chain from the statement to the explanation with minimum distractive information . 
Accordingly , other triples in Gxwhich do n‚Äôt belong to Px‚Üíyare regarded as negative samples . 
The loss function of triple scoring is devised as follows : Ltriple = ‚àí/summationdisplay e‚ààGxI(e‚ààPx‚Üíy ) logP(e|x ) + [ 1‚àíI(e‚ààPx‚Üíy ) ] log[1‚àíP(e|x ) ] ( 14 ) where I(e‚ààPx‚Üíy)is an indicator function that takes the value 1 iff e‚ààPx‚Üíy , and 0 otherwise . 
3.5.2 Path Routing Next , we describe the path routing process which involves propagating the scores along the paths to each concept on the subgraph from the source concepts . 
For each path pretrieved from the subgraph Gx , we calculate a path score s(p)by aggregating the triple score P(e|x)along the path : s(p ) = 1 |p|/summationdisplay e‚ààpP(e|x ) ( 15)For each concept c , we consider all the shortest paths Px‚Üícthat starts with the source concepts and ends with cmonotonically , i.e. , the concept distance of each node on the path increases monotonically along the path . 
Then we calculate the routing score for the concept cby averaging the path scores of Px‚Üíc . 
s(c ) = 1 |Px‚Üíc|/summationdisplay p‚ààPx‚Üícs(p ) ( 16 ) Intuitively , this process disseminates the triple scores and aggregates them to the connected concepts . 
Then we deactivate some paths based on the path routing results and obtain Vx‚Üíyby preserving concepts with the top- K1routing scores . 
3.5.3 Concept Selection Finally , we conduct concept selection based on the concept representation and the statement representation . 
For each concept in Vx‚Üíy , we calculate the selection probability for it by taking the dotproduct attention and adopt a similar cross - entropy loss with supervision from bridge concepts Bx‚Üíy : P(c|x ) = œÉ(hcW3hT x ) ( 17 ) Lconcept = ‚àí/summationdisplay c‚ààVx‚ÜíyI(c‚ààBx‚Üíy ) logP(c|x ) + [ 1‚àíI(c‚ààBx‚Üíy ) ] log[1‚àíP(c|x ) ] ( 18 ) where the indicator function is similar to that of Equation ( 14 ) . 
Finally , the bridge concepts with top- K2probabilityP(c|x)are selected as the additional input to the generation model . 
3.6 Explanation Generation We utilize a pre - trained Transformer decoder ( Radford et al . 
, 2019 ) as our generation model which shares the parameter with the Transformer encoder . 
Essentially , it takes the statement xand the conceptscas input and auto - regressively generates the explanation y : P(y|x , c ) = P(y|x , c1,¬∑¬∑¬∑,cK2 ) = M / productdisplay t=1P(yt|x , c1,¬∑¬∑¬∑,cK2,y < t ) ( 19 ) Lgeneration = ‚àílogP(y|x , c1,¬∑¬∑¬∑,cK2)(20)252 As shown in Figure 2 , the input to the Transformer decoder is the token embeddings of both the statement and the selected concepts concatenated along the sequence length dimension . 
To model bi - directional attention on the input side while preserving the causal dependency of the generated sequence , we adopt a hybrid attention mask where each token on the input side could attend to all the tokens in the input sequence while the generated token at each time step only attends to the input sequence and the previously generated tokens . 
3.7 Training and Inference To train the model , we optimize the Ô¨Ånal loss function which is the sum of the three loss functions : Lfinal = Lgeneration + Œª1Ltriple + Œª2Lconcept ( 21 ) As for the inference process , Figure 2 demonstrates how our model retrieves reasoning paths given the statement , extracts bridge concepts and Ô¨Ånally generates the explanation . 
4 Experiment 4.1 Dataset and Experimental Setup 4.1.1 Commonsense Explanation Dataset We adopt the dataset from the Commonsense Validation and Explanation Challenge2which consists of three subtasks , i.e. , commonsense validation , commonsense explanation selection and commonsense explanation generation . 
We focus on the explanation generation subtask in this paper . 
The commonsense explanation generation subtask contains 10,000statements that are against commonsense . 
For each statement , three human - written explanations are provided . 
To evaluate our proposed model and other baselines , we randomly split 10 % data as the test set , 5 % as the development set and the latter as the training set . 
Note that we further split each example in the training set into three statement - explanation pairs , while for the development set and the test set we use the three corresponding explanations as references for each statement . 
This results in our Ô¨Ånal data split ( 25,596 / 476 / 992 ) denoted as ( train / dev / test ) . 
2https://competitions.codalab.org/ competitions/210804.1.2 Commonsense Knowledge Graph We use the English version ConceptNet as our external commonsense knowledge graph . 
It contains triples in the form of ( h , r , t ) wherehandtrepresent head and tail concepts and ris the relation type . 
We follow Lin et al . 
( 2019a ) to merge the original 42 relation types into 17 types . 
We additionally deÔ¨Åne 17 reverse types corresponding to the original 17 relation types to distinguish the direction of the triples on the graph . 
4.2 Automatic Evaluation Metrics To automatically evaluate the performance of the generation models , we use the BLEU-3/4 ( Papineni et al . 
, 2001 ) , ROUGE-2 / L ( Lin , 2004 ) , METEOR ( Banerjee and Lavie , 2005 ) as our main metrics . 
We also propose Concept F1 to evaluate the accuracy of the unique concepts in the generated explanation that do not occur in the statement . 
SpeciÔ¨Åcally , given the generated explanation ÀÜy and the reference explanation y , we extract a set of conceptsCÀÜyandCyfrom the generated explanation and the reference explanation respectively using the method in¬ß3.3 . 
We denote the sets of unique concepts in the explanation as Uy = Cy‚àíCxand UÀÜy = CÀÜy‚àíCx . 
Then we can compute the Concept F1 as the harmonic mean of recall andprecision . 
recall = |UÀÜy‚à©Uy| |Uy| , precision = |UÀÜy‚à©Uy| |UÀÜy| ( 22 ) 4.3 Implementation Details For the reasoning path retrieval process , we set the maximum number of neighbours B= 300 at each hop . 
For each example , we restrict the concepts of the subgraph to those only appeared in the training and development set . 
We use a pre - trained Transformer language model GPT-2 ( Radford et al . 
, 2019 ) as the initialization of the Transformer model . 
We set the hidden dimensiond1= 768 identical to the hidden size of the Transformer . 
We empirically set the following hyperparameters by tuning the model on the development set : selection threshold K1= 30,K2= 3 , loss coefÔ¨Åcients Œª1= 1,Œª2= 1 , number of epochs = 3 , batch size = 4 , learning rate = 4√ó10‚àí5and use the Adam optimizer ( Kingma and Ba , 2015 ) with 10 % warmup steps . 
We select the model with the highest BLEU-4 score on the development set and evaluate it on the test set . 
At the decoding253 Model B-3/4 R-2 / L M Concept F1 Seq2Seq 10.7/6.1 9.9/25.8 11.4 11.1 MemNet 10.2/5.7 8.8/25.7 11.0 11.5 Transformer 10.0/5.8 9.6/26.0 12.0 11.7 GPT-2 - FT 23.4/15.7 18.9/36.5 17.7 17.4 Ours 24.7/17.1 20.2/37.9 18.3 20.1 Table 1 : Automatic evaluation of explanation generation in terms of BLEU ( B ) , ROUGE ( R ) , METEOR ( M ) and Concept F1 . 
Setting BLEU-4 Concept F1 Ours 17.1 20.1 w/o Context Emb . 
16.0 18.6 w/o Distance Emb . 
16.4 18.5 w/o Path Routing 16.5 19.2 # Hop = 2 16.2 18.3 # Hop = 1 15.9 17.3 Table 2 : Ablation study of our framework on the test set . 
We present the model ablation results in the upper block and the data ablation results in the lower block . 
phase , we use beam search with a beam size of 3 for all models . 
4.4 Baseline Models We compare with the following baseline models : ‚Ä¢Seq2Seq : a sequence - to - sequence model based on gated recurrent unit ( GRU ) ( Cho et al . 
, 2014 ) and attention mechanism , which is widely used in text generation tasks ( Bahdanau et al . 
, 2015 ) . 
‚Ä¢MemNet : a knowledge - grounded sequenceto - sequence model ( Ghazvininejad et al . 
, 2018 ) . 
In our experimental setting , we regard all the concepts which are connected with those in the statements as knowledge facts . 
‚Ä¢Transformer : an encoder - decoder framework commonly used in machine translation tasks ( Vaswani et al . 
, 2017 ) . 
‚Ä¢GPT-2 : a multi - layer Transformer decoder pre - trained on WebText ( Radford et al . 
, 2019 ) which is then directly Ô¨Åne - tuned on our dataset . 
4.5 Experimental Results As shown in Table 1 , our model achieves the best performance in terms of all the automatic evaluation metrics , which demonstrates that our model 12345Selection thresold k1015202530P / R@Nprecisionrecall 12345Selection thresold k1718192021Concept F1K2 K2Figure 4 : P / R@N measures the precision / recall of the top - Nselected bridge concepts . 
Concept F1 measures the F1 - score of concepts in the generated explanations . 
can generate high quality explanations . 
SpeciÔ¨Åcally , our model achieves a 2.7 % gain on Concept F1 compared with GPT-2 which indicates that explicitly extracting bridge concepts enhances the informativeness of the generated explanation . 
To evaluate the effects of different modules in our method , we conduct ablation studies on both the model components and the external knowledge base . 
For the model components , we test the following variants : ( 1 ) without the context - aware token embeddings ( w/o Context Emb . 
) ; ( 2 ) without the concept distance embeddings ( w/o Distance Emb . 
) ; ( 3 ) without the path routing process ( w/o Path Routing ) . 
As for the data ablation , we sample subgraphs by restricting the maximum number of hops to 2 ( # Hop=2 ) and 1 ( # Hop=1 ) . 
As shown in Table 2 , each module contributes to the Ô¨Ånal results . 
Particularly , discarding the context - aware embeddings leads to the most remarkable performance drop , which indicates the signiÔ¨Åcance for context modeling in multi - hop reasoning . 
Besides , the data ablation results demonstrate that as the subgraph has less coverage , the generation model will suffer from the noisy concepts and thus deteriorate the generation results . 
We additionally present the results of the selected and generated concepts with different concepts selection threshold K2 . 
As shown in the upper part of the Figure 4 , as the number of selected concepts increases , more true positives are selected , resulting in the increase of the recall ( Recall@N ) while the inclusion of more false positives leads to254 Error Type Ratio ( % ) Input Output Repetition 7.7 She begins working for relaxation . 
People work to relax , not relax . 
Overstatement 19.2 Less people seek knowledge . 
People do n‚Äôt seek knowledge . 
Unrelated 26.9 The simplest carbohydrates are amino acid . 
Alkaloids are not found in bread . 
Chaotic 11.5 Giving assistance is for revenge . 
If you help someone , you are grateful . 
Table 3 : Distribution and typical cases of different error types of the explanations generated by our model . 
Underlined texts denote the error types including repetition , overstatement , unrelated words and chaotic expression . 
ModelFluency Reasonability Informativeness Win Lose Win Lose Win Lose vs. Seq2seq 0.41 0.02 0.86 0.04 0.84 0.05 vs. MemNet 0.48 0.00 0.84 0.03 0.87 0.03 vs. Transformer 0.33 0.01 0.71 0.03 0.72 0.03 vs. GPT-2 0.20 0.10 0.40 0.27 0.34 0.15 Table 4 : Human evaluation results . 
The scores are the percentages of win andlose of our model in pair - wise comparison ( tiecan be calculated by 1‚àíwin‚àílose ) . 
Our model is signiÔ¨Åcantly better ( sign test , p - value < 0.005 ) than all the baseline models on all three criteria . 
the decrease of the precision ( Precision@N ) . 
The Concept F1 reaches maximum when K2= 3(see the lower part ) , which demonstrates that the model learns to extract critical concepts for explanation generation while keeping out most noisy candidates with an appropriate selection threshold . 
4.6 Human Evaluation To further evaluate the quality of the generated explanations , we conduct the human evaluation and recruit Ô¨Åve annotators to perform pair - wise comparisons . 
Each annotator is given 100 paired explanations ( one generated by our model and the other by a baseline model , along with the statement ) and is required to give a preference among ‚Äú win ‚Äù , ‚Äú tie ‚Äù , and ‚Äú lose ‚Äù according to three criteria : ( 1 ) Fluency which measures the grammatical correctness and the readability of the explanation . 
( 2 ) Reasonabilitywhich measures whether the explanation is reasonable and accords with the commonsense . 
( 3 ) Informativeness which measures the amount of new information delivered in the explanation that helps explain the statement . 
The results are shown in Table 4 , our model outperforms all the baseline models signiÔ¨Åcantly on all three criteria ( sign test , p - value<0.005 ) . 
SpeciÔ¨Åcally , our model wins GPT-2 substantially in terms of reasonability and informativeness . 
To evaluate the inter - rater agreement for each criterion , we calculate the Fleiss ‚Äô kappa ( Fleiss , 1971 ) . 
For Reasonability /Informativeness , the kappa is 0.429 / 0.433 respectively indicating aStatement 1 : I buy popcorn and knife when I go to the cinema . 
Seq2Seq : A person can not buy a person to watch a movie . 
MemNet : A toothbrush is not a place to play a movie . 
Transformer : A fridge is not a place to store groceries . 
GPT-2 : You do n‚Äôt buy popcorn and knife at the cinema . 
Ours : Knives are not sold at the cinema . 
Top-3 reasoning paths : ( buy‚Üíantonym ‚Üísell ) , ( popcorn ‚Üírelated to ‚Üífood ) , ( cinema ‚Üírelated to ‚Üí movie ) Selected concepts : sell , place , movie Statement 2 : He eats his chips with toothpaste . 
Seq2Seq : Chopsticks are not edible . 
MemNet : A potato is too soft to eat juice with your teeth . 
Transformer : You do not eat sand with a cup . 
GPT-2 : Toothpaste is not edible . 
Ours : Toothpaste is used to clean teeth . 
Top-3 reasoning paths : ( eat‚Üírelated to ‚Üítooth ) , ( toothpaste ‚Üírelated to ‚Üípaste‚Üírelated to ‚Üíuse ) , ( eat ‚Üíhas subevent ‚Üíwork‚Üírelated to ‚Üíuse ) Selected concepts : use , tooth , food Table 5 : Examples of generated explanations . 
Irrelevant contents are in red and critical concepts for explanation are in green . 
moderate agreement among annotators . 
In terms of Fluency , annotators show diverse preferences ( Œ∫= 0.245 ) since GPT-2 has strong ability in generating Ô¨Çuent texts . 
4.7 Case Study Table 5 presents the generated explanations . 
Our model is capable to generate reasonable and informative explanations by utilizing the extracted bridge concepts . 
SpeciÔ¨Åcally , in the Ô¨Årst case our model extracts bridge concepts ‚Äú sell ‚Äù and identiÔ¨Åes the incompatibility between ‚Äú knives ‚Äù and ‚Äú cinema ‚Äù . 
In the second case , our model clariÔ¨Åes the function of the ‚Äú toothpaste ‚Äù by extracting ‚Äú use ‚Äù from two reasoning paths and provides more information rather than simply negative phrasing.255 4.8 Error Analysis To analyze the error types of the explanations generated by our model , we manually check all the failed cases3 in the pair - wise comparison between our model and the strong baseline GPT-2 . 
The number of these cases is 26 in all 100 explanations . 
We manually annotated four types of errors from the failed explanations : repetition ( words repeating ) , overstatement ( overstate the points ) , unrelated concepts towards the statement ( the explanation itself may be reasonable ) , chaotic sentences ( difÔ¨Åcult to understand ) . 
As shown in Table 3 , it is still challenging for the model to generate explanations highly related to the statement with accurate wording . 
5 Conclusion In this paper , we analyze the challenges in incorporating external knowledge graph to aid the commonsense generation problem and propose a twostage method that Ô¨Årst extracts bridge concepts from a retrieved subgraph and then generates the explanation by integrating the extracted concepts . 
Experimental results show that our model outperforms baselines including the strong pre - trained language model GPT-2 in both automatic and manual evaluation . 
Acknowledgments This work was jointly supported by the NSFC projects ( key project with No . 
61936010 and regular project with No . 
61876096 ) , and the Guoqiang Institute of Tsinghua University with Grant No . 
2019GQG1 . 
We thank THUNUS NExT Joint - Lab for the support . 
Abstract The KB - to - text task aims at generating texts based on the given KB triples . 
Traditional methods usually map KB triples to sentences via a supervised seq - to - seq model . 
However , existing annotated datasets are very limited and human labeling is very expensive . 
In this paper , we propose a method which trains the generation model in a completely unsupervised way with unaligned raw text data and KB triples . 
Our method exploits a novel dual training framework which leverages the inverse relationship between the KB - to - text generation task and an auxiliary triple extraction task . 
In our architecture , we reconstruct KB triples or texts via a closed - loop framework via linking a generator and an extractor . 
Therefore the loss function that accounts for the reconstruction error of KB triples and texts can be used to train the generator and extractor . 
To resolve the cold start problem in training , we propose a method using a pseudo data generator which generates pseudo texts and KB triples for learning an initial model . 
To resolve the multiple - triple problem , we design an allocated reinforcement learning component to optimize the reconstruction loss . 
The experimental results demonstrate that our model can outperform other unsupervised generation methods and close to the bound of supervised methods . 
1 Introduction Knowledge Base ( KB)-to - text task focuses on generating plain text descriptions from given knowledge bases ( KB ) triples which makes them accessible to users . 
For instance , given a KB triple < 101 Helena , discoverer , James Craig Watson > , it is expected to generate a description sentence such as ‚àóThe work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region , China ( Project Codes : 14204418).‚Äú101 Helena is discovered by James Craig Watson . 
‚Äù . 
Recently , many research works have been proposed for this task . 
For example , Gardent et al . 
( 2017a , b ) create the WebNLG dataset to generate description for triples sampled from DBPedia ( Auer et al . 
, 2007 ) . 
Lebret et al . 
‚Äôs ( 2016 ) method generates people ‚Äôs biographies from extracted Wikipedia infobox . 
Novikova et al . 
( 2017 ) propose to generate restaurant reviews by some given attributes and Fu et al . 
( 2020a ) create the WikiEvent dataset to generate text based on an event chain . 
However , the works mentioned above usually map structured triples to text via a supervised seq - to - seq ( Sutskever et al . 
, 2014 ) model , in which large amounts of annotated data is necessary and the annotation is very expensive and time - consuming . 
We aim to tackle the problem of completely unsupervised KB - to - text generation which only requires a text corpus and a KB corpus and does not assume any alignment between them . 
We propose a dual learning framework based on the inverse relationship between the KB - to - text generation task and the triple extraction task . 
SpeciÔ¨Åcally , the KBto - text task generates sentences from structured triples while the task of triple extraction extracts multiple triples from plain texts . 
Such a relationship enables the design of a closed - loop learning framework in which we link KB - to - text generation and its dual task of triple extraction so as to reconstruct the unaligned KB triples and texts . 
The non - differentiability issue of picking words from our neural model before reconstruction makes it hard to train the extractor or generator effectively using backpropagation . 
To solve this issue , we apply Reinforcement Learning ( RL ) based on policy gradients into our dual learning framework to optimize our extractor or generator according to the rewards . 
Some semi - supervised works ( He et al . 
, 2016 ; Cao et al . 
, 2019 ) have been proposed to generate258 101 Helena is discovered by James Craig Watson who was born in Canada . 
< 101 Helena , discoverer , James Craig Watson>“Ö < James Craig Watson , nationality , Canada > , < James Craig Watson , profession , Writer > , < James Craig Watson , deathPlace , Australia > 101 Helena is discovered by James Craig Watson who was born in Canada and died in Australia . 
EG101 Helena is discovered by James Craig Watson . 
  James Craig Watson was born in Canada . 
James Craig Watson is a writer . 
James Craig Watson died in Australia . 
1.0 1.0 0.5 0.50.9GTraditional RLOur Proposed ARLFigure 1 : Illustration of the multiple - triple problem , in whichEandGare extractor and generator respectively . 
The left part is the traditional RL methods and the right is our proposed ARL method . 
Four triples are extracted by the extractor . 
The top two triples are right and the others are wrong . 
Traditional RL methods give a single reward ( 0.9 ) for all the four triples while our proposed ARL gives each triple a different reward . 
Then the right triples and the wrong triples will be distinguished and optimized differently . 
plain texts from data of certain forms in other domains ( e.g. , translation , semantic parsing ) with limited annotated resources . 
These models contain two major steps . 
Firstly , they pre - train a weak model based on the labeled data . 
Secondly , they use an iterative model whose aim is to improve the weak model using the unlabeled data . 
In each iteration , the input sequence of the original data form is transformed into another form by the original model . 
Then , it is transformed back to the original data form by an inverse model . 
However , there are still some challenges applying the existing methods into KB - to - text directly : ( 1 ) Cold start problem . 
Existing approaches pre - train the model with labeled data and then Ô¨Åne - tune their models via unlabelled data . 
Such a mechanism still needs annotated data which is more difÔ¨Åcult and expensive to obtain in KB - to - text task . 
( 2 ) Multiple - triple problem . 
As shown in Fig . 
1 , multiple triples might be extracted from a text example , and inevitably , the neural extractor could extract some wrong triples . 
The traditional dual learning approaches ( He et al . 
, 2016 ; Cao et al . 
, 2019 ) , if directly applied , will regard all these triples as one unit and calculate a single reward for all the triples regardless of whether they are correct or not . 
It not only results in the slow convergence of RL , but also leads to unsatisfactory model performance . 
We propose a novel Extractor- Generator Dual ( EGD ) framework which exploits the inverse relationship between KB - to - text generation and auxiliary triple extraction . 
Our model can resolve the KB - to - text task in a totally unsupervised way . 
To cope with the cold start problem , we propose apseudo data generator ( PDG ) which can generate pseudo text and pseudo KB triples based on the given unaligned KB triples and text respectively with prior knowledge . 
The extractor and the generator are then pre - trained with the generated pseudo data . 
To resolve the multiple - triple problem , we propose a novel Allocated Reinforcement Learning ( ARL ) component . 
Different from traditional RL methods in which one reward is calculated for the whole sequence , ARL allocates different rewards to different sub - parts of the sequence ( Fig . 
1 right ) . 
Therefore , our model can distinguish the quality of each triple and optimize the extractor and the generator more accurately . 
We compare our framework with existing dual learning methods and the experimental results demonstrate that our model can outperform other unsupervised generation methods and close to the bound of supervised methods . 
2 Related Works Recently many tasks and methods have been proposed to transform existing data into humanreadable text . 
WebNLG ( Gardent et al . 
, 2017a , b ) is proposed to describe a list of triples sampled from DBPedia ( Auer et al . 
, 2007 ) . 
Except for the KB triples , many other types of data have also been investigated for how to generate text from them . 
For example , E2E ( Novikova et al . 
, 2017 ) aims at generating text from some restaurants ‚Äô attributes . 
Wikibio ( Lebret et al . 
, 2016 ) proposes to generate biographies for the Wikipedia infobox while WikiEvent ( Fu et al . 
, 2020a ) proposes to generate text based on an event chain . 
Besides , Chen and Mooney ( 2008 ) ; Wiseman et al . 
( 2017 ) propose to generate a summarization of a match based on the scores and Liang et al . 
( 2009 ) propose to generate weather reports based on the records . 
All these tasks require an elaborately annotated dataset which is very expensive to prepare . 
Many methods have been proposed to tackle the dataset insufÔ¨Åciency problem in other tasks . 
Fu et al . 
( 2020c ) propose to directly train the model on partially - aligned data in which the data and the text are not necessarily exactly math , and it can be built automatically . 
He et al . 
( 2016 ) ; Sennrich et al . 
( 2016 ) ; Yi et al . 
( 2017 ) propose dual learning frameworks . 
They pre - train a weak model with parallel data and reÔ¨Åne the model with monolingual data . 
This strategy has been applied in many related tasks including semantic parsing ( Cao et al . 
, 2019 ) , summarization ( Baziotis et al . 
, 2019 ) and259 KB1KB2KBn ‚Ä¶ TextTextKBGEŒ∏PDGGœÜEEŒ∏EGœÜPDGEŒ∏GGœÜpre - train kb2kbARLGtxt2txtARLETextText1Text2Textn ‚Ä¶ KBKB1KB2KBn ‚Ä¶ KBsplitKBKB1KB2KBn ‚Ä¶ TextText1Text2Textn ‚Ä¶ KBsplitLE < latexit sha1_base64="Y8Oe2xpmNErGDbs8P67Jf4pRbrs=">AAAB+XicbVDLSsNAFJ3UV62vVFfSzWARXJWkRZuCi4IILly0YGuhCWEynbZDJw9mJoUS8iduXCji1v9w4U6 / xknbhVYPDBzOuZd75ngRo0IaxqeWW1vf2NzKbxd2dvf2D / TiYVeEMcekg0MW8p6HBGE0IB1JJSO9iBPke4zce5OrzL+fEi5oGNzJWUQcH40COqQYSSW5um77SI4xYslt6ibXacHVy0alYRm1RhUaFcu6qNbPFTHmgOaSlJtFu / TVPn5vufqHPQhx7JNAYoaE6JtGJJ0EcUkxI2nBjgWJEJ6gEekrGiCfCCeZJ0 / hqVIGcBhy9QIJ5+rPjQT5Qsx8T01mOcWql4n / ef1YDi0noUEUSxLgxaFhzKAMYVYDHFBOsGQzRRDmVGWFeIw4wlKVlZVgrn75L+lWK2atUm2b5eYlWCAPSuAEnAET1EET3IAW6AAMpuABPIFnLdEetRftdTGa05Y7R+AXtLdvgOyWgw==</latexit > LARLG < latexit sha1_base64="kU43c7Im1Fe8lkxpEhyB0d0rxeA=">AAAB / HicdVBNSwJBGJ61L7OvNU / hZUiCTsuuZiZ0MDrUwYNGmqCyzI6zOjj7wcxsIIv9lS4diujaz+jQrX5Ns1pQUQ8M78Pzvi / vM48TMiqkab5pqYXFpeWV9GpmbX1jc0vPbrdFEHFMWjhgAe84SBBGfdKSVDLSCTlBnsPIlTM+TfpX14QLGviXchKSvoeGPnUpRlJJtp7reUiOMGJxfWrHJxf1s2nG1gumUS1ViuUyNI2kHlQVOVTFLELLMGco1LK9 / Htz56Vh66 + 9QYAjj / gSMyRE1zJD2Y8RlxQzMs30IkFChMdoSLqK+sgjoh / PzE / hnlIG0A24er6EM / X7Row8ISaeoyYTq+J3LxH/6nUj6R71Y+qHkSQ+nh9yIwZlAJMk4IBygiWbKIIwp8orxCPEEZYqrySEr5 / C/0m7aFglo9i0CrVjMEca5MEu2AcWqIAaOAcN0AIYTMAtuAcP2o12pz1qT / PRlPa5kwM / oD1 / AHTjl5Y=</latexit > LG < latexit sha1_base64="y / LVboRZvPiT5759n9zigPQZrw8=">AAAB+XicdVDLSsNAFJ34rPWV6kq6CRbBVUjqoy5cFFzowkUL9gFNCJPppB06mYSZSaGE / IkbF4q49T9cuNOvcdJWsKIHLhzOuZd7OH5MiZCW9aEtLa+srq0XNoqbW9s7u3ppry2ihCPcQhGNeNeHAlPCcEsSSXE35hiGPsUdf3SV+50x5oJE7E5OYuyGcMBIQBCUSvJ03QmhHCJI09vMS6+zoqdXbNOawrDMs9r5ac1WZK58W5V6ySl / Ng / eGp7 + 7vQjlISYSUShED3biqWbQi4JojgrOonAMUQjOMA9RRkMsXDTafLMOFJK3wgiroZJY6r+vEhhKMQk9NVmnlP89nLxL6+XyODCTQmLE4kZmj0KEmrIyMhrMPqEYyTpRBGIOFFZDTSEHCKpyloo4X / Srpr2iVlt2pX6JZihAMrgEBwDG9RAHdyABmgBBMbgHjyCJy3VHrRn7WW2uqTNb / bBArTXL07GlmE=</latexit > LARLE < latexit sha1_base64="pL0RyuW8v2vDVjBk9FKEysQokYo=">AAAB / HicdVDLSgMxFM3UV62vqV1JN8EiuBoyLfQBLioiuOiiFfuAtpRMmrahmQdJRihD / RU3LhRx62e4cKdfY6ZVUNEDgcM593JPjhNwJhVCb0ZiZXVtfSO5mdra3tndM9P7LemHgtAm8bkvOg6WlDOPNhVTnHYCQbHrcNp2pmex376mQjLfu1KzgPZdPPbYiBGstDQwMz0XqwnBPKrNB9HpZe18nhqYOWQVUQyILFQpo0JFk0KxXMyXoG0tHJSrpnvZ98bBS31gvvaGPgld6inCsZRdGwWqH2GhGOF0nuqFkgaYTPGYdjX1sEtlP1qEn8MjrQzhyBf6eQou1O8bEXalnLmOnoyjyt9eLP7ldUM1Kvcj5gWhoh5ZHhqFHCofxk3AIROUKD7TBBPBdFZIJlhgonRfcQlfP4X / k1besgtWvmHnqidgiSTIgkNwDGxQAlVwAeqgCQiYgVtwDx6MG+POeDSelqMJ43MnA37AeP4AU8SXgA==</latexit > Data from corpusE / GExtractor / generator modelEŒ∏ / GœÜModel with updatable parametersCalculate with gradient   back - propagationCalculate without gradientFigure 2 : The extractor - generator dual ( EGD ) framework . 
It contains three processes namely a pre - train process , a kb2 kb process and a txt2txt process . 
information narration ( Sun et al . 
, 2018 ) . 
However , as indicated in Hoang et al . 
( 2018 ) , the dual learning approach is not easy to train . 
Moreover , these methods still need some aligned data to pre - train the weak model . 
Another line of research proposes to use some extra annotations instead of using aligned data . 
Lample et al . 
( 2018a , b ) propose to train an unsupervised NMT system based on few annotated word pairs ( Conneau et al . 
, 2018 ) . 
Luo et al . 
( 2019 ) propose to generate pseudo data with a rule - based template ( Li et al . 
, 2018 ) . 
However , these models can not be directly applied in our scenario since our dataset is too complicated to make these annotations . 
Fu et al . 
( 2020b ) propose to utilize topic information from a dynamic topic tracker to solve the dataset insufÔ¨Åciency problem . 
Cheng et al . 
( 2020 ) propose to generate better text description for a few entities by exploring the knowledge from KB and distill the useful part . 
In the Ô¨Åeld of computer vision , Zhu et al . 
( 2017 ) propose cycleGAN which uses a cycled training method that transforms the input into another data form and then transforms it back , minimizing the recover loss . 
The method works well in the image domain but has some problems in text generation considering the non - differentiable discrete layer . 
We follow the ideas of cycleGAN to train the whole model without supervised data and adopt the RL method proposed in dual learning methods . 
Reinforcement Learning ( RL ) has been utilized to solve the infeasibility of backpropagation through discrete tokens layer . 
Li et al . 
( 2016 ) propose to use RL to focus on the long term target and thus improve the performance . 
Yu et al . 
( 2017 ) propose to use the RL in generative adversarial networks to solve the discrete tokens problem . 
He et al . 
( 2016 ) ; Sun et al . 
( 2018 ) propose to use RL in dual training . 
As far as we know , no studies of RL have been conducted for KB triples in whichthe reward is different for each triple considering multiple - triple problem . 
3 Method 3.1 Problem DeÔ¨Ånition Formally , we denote the KB corpus as K= { Ki|‚àÄi}in whichKi= [ k(i ) 1,k(i ) 2,¬∑¬∑¬∑,k(i ) ni]is the ith KB triple list containing nitriples.k(i ) j= ( h(i ) j , r(i ) j , t(i ) j)represents the jth KB triple in Ki containing the head , relation and tail entity respectively . 
We denote the texts corpus as T={Ti|‚àÄi } in whichTi= [ t(i ) 1,t(i ) 2,¬∑¬∑¬∑,t(i ) ni]is theith sentence andt(i ) jis thejth word in the sentence . 
In our problem , we are only given a collection of KB triplesKt‚äÇK and a collection of text Tt‚äÇT without any alignment information between them . 
The ultimate goal is to train a model that generates the corresponding text in Tdescribing the given triple list fromK. 3.2 Extractor - Generator Dual Framework Our proposed Extractor - Generator Dual ( EGD ) framework is composed of a generator Gand an extractorEthat translate data in one form to another . 
We denote all trainable parameters in E andGasŒ∏andœÜ , respectively . 
The generator generates text representation for each KB triple asT / prime = G(K),K‚àà K , T / prime‚àà T while the extractor extracts KB triples from raw text as K / prime= E(T),T‚ààT , K / prime‚ààK. Our EGD framework is trained in an unsupervised manner and it contains three processes , as shown in Fig . 
2 . 
The Ô¨Årst process is a pre - train process in which both EandG are trained with the pseudo data generated by the pseudo generator . 
The second process is the kb2 kb process which generates description text based on the given KB triples with Gand then recovers the KB triples from the generated text with E. The260 third process is called txt2txt which extracts KB triples from the given text with Eand then recovers the text from the generated KB triples with G. In order to overcome the multiple - mapping problem , we propose a novel allocated reinforcement learning component in kb2 kb and txt2txt , respectively . 
The EGD framework Ô¨Årstly pre - trains the extractor and generator with the data generated by the pseudo data generator ( PDG ) . 
For the text corpusTt , we generate corresponding pseudo KB triples asK / prime t={K = PK(T)|‚àÄT‚àà Tt } , in whichPKis the pseudo KB generator . 
We pretrain the generator Gto transform K‚àà K / prime tto T‚àà Tt . 
Similarly , we generate pseudo text as T / prime t={T = PT(K)|‚àÄK‚ààKt } , in whichPTis the pseudo text generator . 
Then , we train the extractor to transform T‚ààT / prime ttoK‚ààKt . 
AfterG andEhave been pre - trained , the kb2 kb process and the txt2txt process are conducted alternately to further improve the performance . 
In the kb2 kb process , the input KB triples are Ô¨Årstly Ô¨Çattened and concatenated one by one as K= [ k1,k2,¬∑¬∑¬∑,knk ] = [ w1,w2,¬∑¬∑¬∑,wnw]in whichkiis theith triple inKwhilewidenotes theith words in the concatenated word list . 
nk is the number of triples while nwis the number of the words . 
Kis then sent into the generator Gto get a text description Tm= [ t1,t2,¬∑¬∑¬∑,tnt ] , wheretiis theith word in the sentence Tmand ntis the length of Tm . 
Afterwards , The extractor takes the sentences Tmas input and outputs the triple sequence K / prime= [ w / prime 1,w / prime 2,¬∑¬∑¬∑,w / prime n / primew ] , in which w / prime iis theith word inK / primewhilen / prime wis the length ofK / prime . 
The target is to make K / primeas close toKas possible . 
Therefore , in the training step , the loss function for the extractor is deÔ¨Åned as the negative log probability of each word in K : LE=‚àínw / summationdisplay i=1logpŒ∏(w / prime i = wi|Tm , w1,¬∑¬∑¬∑,wi‚àí1 ) . 
We can also use the output to improve the generator . 
SinceTmis discrete , the gradient can not be passed to the generator as the cycleGAN ( Zhu et al . 
, 2017 ) does . 
To tackle this problem , we propose an Allocated Reinforcement Learning for Generator ( ARLG ) component to utilize the extractor ‚Äôs result to optimize the generator . 
Different rewards are allocated to different parts of the generator output . 
The gradient for the generator is denoted as ‚àáœÜLARLG which will be introduced in the later section . 
In the txt2txt process , the input text T= [ t1,t2,¬∑¬∑¬∑,tnt]is transformed into its KB representationKm= [ k1,k2,¬∑¬∑¬∑,knm]by the extractor E. Kmis then transformed to T / prime= [ t / prime 1,t / prime 2,¬∑¬∑¬∑,t / prime nt ] by the generator and the loss is deÔ¨Åned as : LG=‚àínt / summationdisplay i=1logpœÜ(t / prime i = ti|Km , t1,¬∑¬∑¬∑,ti‚àí1 ) . 
Similarly , we also propose an Allocated Reinforcement Learning for Extractor ( ARLE ) to utilize the generator ‚Äôs result to optimize the extractor . 
Different rewards are allocated to different parts of the extractor output . 
Let the gradient for the extractor be denoted as ‚àáŒ∏LARLE . 
The Ô¨Ånal gradient for extractor ‚Äôs parameters Œ∏is formulated as ‚àáŒ∏LE+‚àáŒ∏LARLE while the gradient for generator ‚Äôs parameters œÜis‚àáœÜLG+‚àáœÜLARLG . 
We use the Adam ( Kingma and Ba , 2014 ) as the optimizer to optimize all the parameters . 
3.3 Background of Transformer The extractor and the generator are both backboned by the prevalent Transformer ( Vaswani et al . 
, 2017 ) model , which is a variant of the seq - to - seq model . 
It takes a sequence as input and generates another sequence as output . 
The Transformer model contains two parts , namely an encoder and a decoder . 
Both of them are built with several attention layers . 
We refer readers to the original paper ( Vaswani et al . 
, 2017 ) for more details . 
3.4 Pseudo Data Generator To handle the cold start problem , we propose a novel pseudo data generator ( PDG ) to generate pseudo data . 
It contains two components , namely a pseudo text generator and a pseudo KB generator . 
Pseudo Text Generator generates pseudo text for each KB and forms a pseudo supervised training data for pre - training the extractor and thus solving the cold start problem . 
We compute a statistics of the word count in the training set Ttand calculate the empirical distribution for each word as : p(w ) = # w / summationtext w / prime‚ààTt#w / prime , where # w stands for the total word count for winTt . 
For a list of KB triples K = [ k1,k2,¬∑¬∑¬∑,knk ] = [ h1,r1,t1,h2,r2,t2,¬∑¬∑¬∑,hnk , rnk , tnk ] , we Ô¨Årstly sample head entities and tail entities asKs= [ h1,t1,h2,t2,¬∑¬∑¬∑;hn , tn ] . 
The Ô¨Ånal261 sequence is generated by sampling from both Ksandp(w ) . 
When generating each word ÀúTi , a random number generator is used to generate a random number riuniformly . 
riis used to compare with a threshold parameter Œ± . 
Ifri > Œ± , ÀúTiis sampled with the word distribution p(w ) , otherwise , it is sampled form the next token in Ks . 
This process can be expressed mathematically as : ÀúTi=Ô£± Ô£¥Ô£¥Ô£≤ Ô£¥Ô£¥Ô£≥w‚àºp(w ) ri > Œ± Ks[1 + i‚àí1 / summationdisplay j=11(ÀúTj‚ààKs)]otherwise , in which 1(C ) = 1 if condition Cis true and 0 otherwise . 
ÀúTj‚ààKsindicates whether the word ÀúTj is sampled from Ks . 
This pseudo text data is used to solve the cold start problem when training the extractor . 
Pseudo KB Generator generates pseudo KB triples for each text and form a pseudo supervised training data . 
This data is used to solve the cold start problem when pre - training the generator . 
Similar with the work of Freitag and Roy ( 2018 ) , for an input sequence Twe randomly remove words in the input text with a probability Œ≤1and sample new words by sampling words from a distribution with a probability Œ≤2 . 
The generated sequence ÀúK is the pseudo KB sequence for each text . 
Similar to the Pseudo Text Generator , we randomly add some words by sampling from the distribution p(w ) . 
We do not use the probability calculated from Ktsince it may sample some wrong relations or wrong entity names which undermines the performance . 
Mathematically , it can be expressed as : ÀúKi=Ô£± Ô£¥Ô£¥Ô£≤ Ô£¥Ô£¥Ô£≥w‚àºp(w ) ri < Œ≤ 2 Ts[1 + i‚àí1 / summationdisplay j=11(ÀúKj‚ààTs)]otherwise , in whichTs = s(T)ands(¬∑)is a sample function deÔ¨Åned as : s(T ) = Ô£± Ô£¥Ô£≤ Ô£¥Ô£≥T /bardblT / bardbl= 0 [ T1;s(T2:/bardblT / bardbl)]r < Œ≤ 1,/bardblT / bardbl / negationslash= 0 s(T2:/bardblT / bardbl ) otherwise , where / bardblT / bardbldenotes the length of the sequence T whileT2:/bardblT / bardblstands for the sub - sequence from the second to the last of T.3.5 Allocated Reinforcement Learning Traditional reinforcement learning for sequence generation calculates a reward for the whole sequence ( He et al . 
, 2016 ; Hoang et al . 
, 2018 ; Keneshloo et al . 
, 2018 ) and uses the policy gradient ( Sutton et al . 
, 2000 ) algorithm to optimize the parameters . 
It suffers from the multiple - triple problem as discussed above . 
We propose an allocated reinforcement learning method to allocate different rewards for different KB triples and thus alleviate this problem . 
In the kb2 kb process , the RL model is called the Allocated Reinforcement Learning for Generator ( ARLG ) since it optimizes the parameters in the generator while in the txt2txt process , it is called Allocated Reinforcement Learning for Extractor ( ARLE ) accordingly . 
ARLE is shown in Fig . 
2 . 
The main idea is to recover and evaluate the KB triples separately which inherently has the following beneÔ¨Åts : 1 ) Each triple is given a distinct reward as discussed above ; 2 ) Traditional RL is more likely to ignore some triples ( e.g. , 3rd triple in Fig . 
1 ) since it handles several triples at once while our method alleviates such problem by handling triples one by one . 
It Ô¨Årstly sends the input text T= [ t1,t2,¬∑¬∑¬∑,tnt ] into the extractor and get the extracted triples : Km = E(T ) = [ k(1 ) m , k(2 ) m,¬∑¬∑¬∑,k(nk ) m].The corresponding probability for each token is denoted asp(i ) j , in whichidenotes the ith triple and j denotes the jth word in the triple . 
Afterwards , the generator is applied on each triple in Km to recover the corresponding text , which denotes as : T / prime= [ G(k(1 ) m),G(k(2 ) m),¬∑¬∑¬∑,G(k(nk ) m ) ] = [ t / prime 1,t / prime 2,¬∑¬∑¬∑,t / prime nk ] . 
We calculate the reward for each k(i ) mas the recall for each corresponding t / prime ireferring toT : R(k(i ) m ) = /summationtext / bardblt / prime i / bardbl j=1 1(t / prime(j ) i‚ààT ) /bardblt / prime i / bardbl , in which / bardblt / prime i / bardbldenotes the length of t / prime iand t / prime(j ) iis thejth word in t / prime i. The reward for each sentence in Kmis denoted as : Re= [ R(k(1 ) m),R(k(2 ) m),¬∑¬∑¬∑,R(k(nk ) m)].Different from the traditional policy gradient algorithm ( Sutton et al . 
, 2000 ) , our RL uses a different reward for each generated triple . 
The gradient is calculated as : ‚àáŒ∏LARLE = ‚àíE[nk / summationdisplay i=1R(k(i ) m)/bardblk / prime i / bardbl / summationdisplay j=1‚àáŒ∏logp(i ) j].262 Since the RL model only guides the model with some reward scores which is only one aspect of the result . 
It misleads the model into generating some sequences which have a high reward while actually perform worse . 
To prevent this , we propose to conduct the gradient descent together with the kb2 kb process simultaneously in which the extractor is trained with a supervised sequence . 
ARLG is applied in the kb2 kb process . 
The input KB triples is Ô¨Årstly splitted into nktriplesK= [ k1,k2,¬∑¬∑¬∑,knk]which is then sent into the generator separately and get the corresponding description sentences : Tm= [ G(k1),G(k2),¬∑¬∑¬∑,G(knk ) ] = [ t(1 ) m , t(2 ) m,¬∑¬∑¬∑,t(nk ) m].The corresponding probability for thejth word in the ith sentence is denoted asp(i ) j. Afterwards , the text is sent into the extractor to recover the input KB triple for each t(i ) m : K / prime= [ E(t(1 ) m),E(t(2 ) m),¬∑¬∑¬∑,E(t(nk ) m ) ] = [ k / prime 1,k / prime 2,¬∑¬∑¬∑,k / prime nk].We calculate the reward for each t(i ) mas the precision for each corresponding k / prime ireferring tokiinK : P(t(i ) m ) = /summationtext / bardblk / prime i / bardbl j=1 1(k / prime(j ) i‚ààki ) /bardblk / prime i / bardbl , in which / bardblk / prime i / bardbldenotes the total word number count ofk / prime i. The reward for each sentence in Tmis denoted as : Rg= [ P(t(1 ) m),P(t(2 ) m),¬∑¬∑¬∑,P(t(nk ) m ) ] . 
We use RL to maximize the expected reward for each KB triple t(i ) mwith corresponding reward . 
The gradient is : ‚àáœÜLARLG = ‚àíE[nk / summationdisplay i=1P(t(i ) m)/bardblt / prime i / bardbl / summationdisplay j=1‚àáŒ∏logp(i ) j ] . 
Similar to ARLE , we also train the model with the txt2txt process to give a targeted sequence to guide the training together with the reward score . 
4 Experiments 4.1 Dataset We adopt the WebNLG v2 dataset ( Gardent et al . 
, 2017a)1 . 
It samples KB triples from DBpedia and annotates corresponding texts by crowdsourcing . 
In order to show that our model can work under the unsupervised setting , we split the original dataset into two parts , namely the KB part and the text part . 
We do not assume any alignment between 1https://gitlab.com/shimorina/webnlg-dataset#triples 1 2 3 4 5 6 7 Total train 7,429 6,717 7,377 6,888 4,982 488 471 34,352 dev 924 842 919 877 632 64 58 4,316 test 931 831 903 838 608 58 55 4,224 Table 1 : Statistics for the dataset . 
The number of instances with different number of triples are listed . 
KB and text . 
Table 1 shows the statistics of instances with different number of triples . 
In this dataset , one sentence can be mapped to at most seven triples . 
We use the same dev and test set as the original WebNLG . 
The training set has 34,352 samples in total while the dev set and the test set have 4,316 and 4,224 samples respectively . 
It can be observed that there are 78.2 % sentences mapped with multiple - triple . 
4.2 Comparison Models We compare our model against the following baseline methods : PDG uses the Pseudo Data Generator to generate the pseudo data for pre - training both extractor and generator . 
PDG does not conduct the subsequent dual learning process and thus illustrates the capability of PDG . 
DLuses the dual learning process proposed in He et al . 
( 2016 ) ; Zhu et al . 
( 2017 ) . 
It is Ô¨Åne - tuned on the PDG model and iterates alternatively between txt2txt and kb2 kb processes . 
Here , we do not use any reinforcement learning component . 
DL - RL1 uses the dual learning process together with an RL component . 
It is similar to the dual learning method proposed in He et al . 
( 2016 ) ; Zhu et al . 
( 2017 ) . 
We use the PDG ‚Äôs data to train the weak model . 
It uses the log - likelihood of the recover process ‚Äôs output sequence as the reward . 
DL - RL2 follows the settings of Sun et al . 
( 2018 ) . 
Different from DL - RL1 , this model uses the ROUGEL(Lin , 2004 ) score of the recovered sequence instead of using the log - likelihood as the reward . 
SEG is a Supervised Extractor - Generator using the original setting of WebNLG for both generator and extractor . 
It utilizes all the alignment information between KB and text and thus provides an upper bound for our experiment . 
4.3 Experimental settings We evaluate the performances of the generator and the extractor with several metrics including BLEU ( Papineni et al . 
, 2002 ) , NIST ( Dodding-263 Generator Extractor BLEU NIST METEOR ROUGE LCIDEr BLEU NIST METEOR ROUGE LCIDEr Precision Recall F1 PDG 0.322 7.06 0.349 0.505 2.63 0.489 6.01 0.351 0.618 3.97 0.635 0.465 0.510 DL 0.352 7.71 0.347 0.528 2.96 0.735 10.4 0.502 0.743 5.67 0.644 0.691 0.646 DL - RL1 0.356 7.73 0.350 0.532 3.00 0.760 10.8 0.501 0.755 5.92 0.670 0.687 0.658 DL - RL2 0.356 7.75 0.350 0.533 2.99 0.757 10.7 0.503 0.755 5.90 0.668 0.691 0.659 EGD 0.369 7.77 0.364 0.541 3.13 0.775 11.1 0.503 0.772 6.25 0.704 0.691 0.680 EGD w/o ARLE 0.351 7.72 0.347 0.529 2.97 0.770 10.9 0.501 0.764 6.11 0.683 0.682 0.665 EGD w/o ARLG 0.353 7.77 0.348 0.531 2.99 0.729 10.4 0.505 0.746 5.61 0.639 0.695 0.645 EGD w/o PDG 0.010 0.82 0.037 0.119 0.02 0.020 0.42 0.026 0.042 0.08 0.011 0.008 0.007 SEG 0.406 8.31 0.385 0.585 3.66 0.848 11.8 0.595 0.867 7.43 0.783 0.830 0.796 Table 2 : Results for generator ( left ) and extractor ( right ) , which are evaluated with generation metrics . 
For the extractor , precision , recall , and F1 scores are also calculated at triple ‚Äôs level . 
The performances of our EGD method without different components and the supervised method SEG are shown in the bottom . 
RatioGenerator Extractor BLEU ROUGE L BLEU ROUGE L 0.10 0.235 0.439 0.335 0.557 0.15 0.281 0.49 0.655 0.708 0.20 0.308 0.506 0.746 0.757 0.25 0.347 0.524 0.71 0.764 PDG 0.322 0.505 0.489 0.618 Table 3 : Compare our PDG framework with semisupervised models at different labeling ratios . 
ton , 2002 ) , METEOR ( Banerjee and Lavie , 2005 ) , ROUGEL(Lin , 2004 ) , and CIDEr ( Vedantam et al . 
, 2015 ) . 
These metrics are calculated with the evaluation code provided in Novikova et al . 
( 2017 ) . 
Moreover , we also evaluate the performance of the extractor with precision , recall , and F1 scores ( Manning et al . 
, 2010 ) . 
In PDG , we set Œ±= 0.8,Œ≤1= 0.2,Œ≤2= 0.6 . 
We Ô¨Årstly pre - train the extractor and the generator in the PDG model with the data generated by PDG until convergence . 
All other models are Ô¨Åne - tuned on the PDG model . 
For the DL model , we train the generator for 5 steps with the txt2txt process and train the extractor with the kb2 kb process for another 5 steps with the new generator . 
We iterate this process 10 times . 
For all transformers , we set clip norm to 1.0 , label smoothing to 0.1 , and dropout to 0.3 . 
We use Adam ( Kingma and Ba , 2014 ) as our optimizer and set the learning rate for the extractor to 2e-4 and generator to 5e-4 . 
All hyper - parameters are tuned on the dev dataset with grid search . 
4.4 Experimental Results The performances of our KB - to - text generator and triple extractor are shown in the left and right of Table 2 respectively . 
Both generator and extractor of our model outperform all baseline models signiÔ¨Åcantly and consistently . 
The comparison between our EGD model and the supervised SEG model indicates that our unsupervised EGD model 2 4 60.40.60.8Extractor BLEU 2 4 60.40.6 ROUGEL 2 4 60.30.40.5Generator 2 4 60.350.400.450.50 PDG DL EGD SEGFigure 3 : The inÔ¨Çuence of KB triples count . 
The xaxis represents the KB triples count while the y - axis represents the scores . 
is close to the bound of the supervised methods . 
Compared with the PDG model , our EGD model has a much better performance with the dual learning framework and the ARL component . 
Moreover , Our EGD model outperforms the DL - RL1 and DLRL2 model , which indicates that our proposed ARL component can handle the multiple - triple problem between triples and texts . 
In the traditional RL models , the reward is the same for a whole sequence including all the triples while in our ARL model , the reward is calculated for several subparts of the sequence , which is more accurate and effective . 
By comparing PDG with SEG , we found that the model trained with our proposed pseudo data generator ( PDG ) ‚Äôs output achieves acceptable results . 
It indicates that using the PDG ‚Äôs output is a feasible alternative to initialize the model and can handle the cold start problem . 
Ablation Study . 
We also conduct some ablation studies to show that each component contributes264 Extractor Generator Gold(1634 : The Ram Rebellion , mediaType , E - book ) ( 1634 : The Ram Rebellion , author , Virginia DeMarce)Virginia DeMarce is the author of 1634 : The Ram Rebellion , which can be found as an e - book . 
SEG(1634 : The Ram Rebellion , mediaType , E - book ) ( 1634 : The Ram Rebellion , author , Virginia DeMarce)1634 : The Ram Rebellion was written by Virginia DeMarce and has the ISBN number 1 - 4165 - 2060 - 0 . 
PDG ( 1634 : The Ram Rebellion , mediaType , E - book)1634 : The Ram Rebellion was followed by 1634 : The Galileo Affair and its author is Virginia DeMarce . 
DL(1634 : The Ram Rebellion , mediaType , E - book ) ( 1634 : The Ram Rebellion , author , Virginia DeMarce ) ( 1634 : The Ram Rebellion , ISBN number , 1 - 4165 - 2060 - 0)1634 : The Ram Rebellion is available as an E - Book . 
EGD(1634 : The Ram Rebellion , mediaType , E - book ) ( 1634 : The Ram Rebellion , author , Virginia DeMarce)Virginia DeMarce is the author of 1634 : The Ram Rebellion , currently in print . 
Table 4 : Case study . 
The input KB and text are listed in the Ô¨Årst row . 
to the Ô¨Ånal performance . 
The results are shown at the bottom part of Table 2 . 
By comparing the model EGD w/o ARLE and EGD w/o ARLG with the EGD model , we can see that both the ARLE and ARLG components are effective to handle the multiple - triple problem and help improve the performance . 
It is interesting to see that the result of EGD w/o PDG is extremely poor showing the importance of our PDG component . 
The EGD w/o PDG removes the pre - train stage with the pseudo data generator and conducts the iterations between txt2txt and kb2 kb directly . 
Without PDG , we observe that the models trend to learn some ‚Äú own language ‚Äù without a good initialization which is incomprehensible to human . 
The InÔ¨Çuence of the KB triples Number . 
We analyze the inÔ¨Çuence of the KB triples ‚Äô number on the performance . 
The results are shown in Fig . 
3 . 
As expected , the SEG model performs the best over all numbers since it is fully supervised . 
The PDG model performs the worst since it only uses pseudo data to train . 
The DL model improves signiÔ¨Åcantly comparing with the PDG model over all numbers , especially in the extractor model . 
It shows that using dual learning ‚Äôs iteration approach does improve the model of training solely based on PDG ‚Äôs data . 
Our proposed EGD model outperforms the DL model and the PDG model . 
This shows that the ARL model does help to give more information to train the model . 
Nearly all generators ‚Äô scores decrease as the number increases . 
This is because if the sequence is long , it has more ways to express those triples which may be different from the gold standard sentence . 
However , when extracting triples from the text , it only has one correct way and thus the extractor ‚Äôs scores are similar in all lengths . 
Error Analysis . 
We conduct an error analysis experiment for the top 20 mentioned relations in Figure 4 : Error analysis for top 20 mentioned relations . 
the extractor which is shown in Fig . 
4 . 
We focus on two kinds of errors . 
The Ô¨Årst kind of error is called ‚Äú false negative ‚Äù which means when extracting , some correct triples are ignored . 
The second kind of error is called ‚Äú false positive ‚Äù which means that the extractor generates some incorrect triples that the text does not mention . 
It can be observed that the ‚Äú false negative ‚Äù problem is much more severe than the ‚Äú false positive ‚Äù problem for the PDG model , while the DL model and the EGD model alleviate this problem a lot . 
This reason is that the pseudo text data is made by sampling entities in KB ignoring relation information . 
Iterating alternately between txt2txt and kb2 solves the problem since the missing information is supplemented . 
It can also be observed that when comparing with the DL model , our EGD model mainly solves the ‚Äú false positive ‚Äù problem . 
The reason is that the RL can penalize the wrong generated triples but can not give speciÔ¨Åc guidance on which missing triples the model should generate . 
Comparison with Semi - Supervised Learning . 
To measure the quality of the initialization via PDG , we compare our PDG method against the semisupervised learning method . 
We sample labeled data from the original dataset with different ratios to train the models and compare the results with the PDG model . 
The result is shown in Table 3 . 
It can be concluded from the result that training the265 extractor with the PDG ‚Äôs data outperforms training with 10 % aligned data and it also outperforms 20 % aligned data for the generator . 
It shows that our PDG component does provide usable data and it can be boosted a lot in the subsequent dual iteration process . 
Case Study . 
Table 4 shows a case study for 4 models . 
For the extractor , the input is ‚Äú Virginia DeMarce is the author of 1634 : The Ram Rebellion , which can be found as an e - book . 
‚Äù . 
For the generator , the input is ‚Äú ( 1634 : The Ram Rebellion , mediaType , E - book ) ( 1634 : The Ram Rebellion , author , Virginia DeMarce ) ‚Äù . 
It can be observed that for the PDG model , it omits the second triple . 
It also shows that the PDG model has a severe false negative problem which has been mentioned in the error analysis sub - section . 
The DL model alleviates this problem but it introduces more triples causing the false positive problem . 
Our EGD model solves the false positive problem by the RL component . 
All models make some mistakes in the generation process including the supervised SEG model . 
The result of the generator shows that it is more difÔ¨Åcult to generate a sequence than extracting triples . 
5 Conclusions We propose a new challenging task , namely , unsupervised KB - to - text generation . 
To solve this task , we propose an extractor - generator dual framework which exploits the inverse relationship between the KB - to - text generation task and the auxiliary triple extraction task . 
To handle the cold start problem and the multiple - triple problem respectively , we propose a novel pseudo data generator and an allocated reinforcement learning component . 
Experimental results show that our proposed method successfully resolves the observed problems and outperforms all the baseline models . 
Abstract Despite the recent achievements made in the multi - modal emotion recognition task , two problems still exist and have not been well investigated : 1 ) the relationship between different emotion categories are not utilized , which leads to sub - optimal performance ; and 2 ) current models fail to cope well with low - resource emotions , especially for unseen emotions . 
In this paper , we propose a modality - transferable model with emotion embeddings to tackle the aforementioned issues . 
We use pre - trained word embeddings to represent emotion categories for textual data . 
Then , two mapping functions are learned to transfer these embeddings into visual and acoustic spaces . 
For each modality , the model calculates the representation distance between the input sequence and target emotions and makes predictions based on the distances . 
By doing so , our model can directly adapt to the unseen emotions in any modality since we have their pre - trained embeddings and modality mapping functions . 
Experiments show that our model achieves stateof - the - art performance on most of the emotion categories . 
Besides , our model also outperforms existing baselines in the zero - shot and few - shot scenarios for unseen emotions1 . 
1 Introduction Multi - modal emotion recognition is an increasingly popular but challenging task . 
One main challenge is that labelled data is difÔ¨Åcult to come by as humans Ô¨Ånd it time - consuming to discern emotion categories from either speech or video . 
Indeed we humans express emotions through a combination of modalities , including the way we speak , the words we use , facial expressions and sometimes gestures . 
It is also much more comfortable for humans to understand each other ‚Äôs emotions when they can both 1Code is available at https://github.com/ wenliangdai / Modality - Transferable - MER happysurprised Acoustic 	 Spacehappy surprised T extual 	 Spacehappy surprised V isual 	 Space He 	 pr oposed . 
		 W ell , 	 and 	 I 	 said 	 yes , 	 of 	 course . 
Textual Visual Acoustic I 'm 	 going 	 to 	 miss 	 you . 
Textual Visual Acoustic . 
	 . 
	 . 
sad sad sad ( happy 	 and 	  surprised ) ( sad)Seen 	 Emotion 	 Example Unseen 	 Emotion 	 Example . 
	 . 
	 . 
Figure 1 : An intuitive example of our method . 
In the upper image , the relative positions of GloVe emotion embeddings ( happy , surprised ) are shown in the textual space , which are then projected to acoustic and visual spaces by two mapping functions ( ft‚Üíaandft‚Üív ) . 
Our model learns to group the representations of input sentences (   , ) based on their corresponding emotion embeddings . 
Examples are shown in the lower image . 
When a sample has both happy andsurprised emotions , its representation gets close to these two emotion embeddings in all three spaces . 
If an unseen emotion sad ( ) comes , the model processes it with the same Ô¨Çow and recognizes corresponding data samples . 
hear and see the other person . 
It follows that multimodal emotion recognition can , therefore , yield more reliable results than restricting machines to a single modality . 
In the past few years , much research has been done to better understand intra - modality and intermodality dynamics , and modality fusion is a widely studied approach . 
For example , Zadeh et al . 
( 2017 ) proposed a tensor fusion network that combines three modalities from vectors to a tensor using the Cartesian product . 
In addition , the attention269 mechanism is commonly used to do modality fusion ( Zadeh et al . 
, 2018a ; Wang et al . 
, 2018 ; Liang et al . 
, 2018 ; Hazarika et al . 
, 2018 ; Pham et al . 
, 2018 ; Tsai et al . 
, 2019a ) . 
Although signiÔ¨Åcant improvements have been made on the multi - modal emotion recognition task , however , the relationship between emotions has not been well modelled , which can lead to sub - optimal performance . 
Also , the problem of low - resource multi - modal emotion recognition is not adequately studied . 
Multi - modal emotion recognition data is hard to collect and annotate , especially for low - resource emotions ( e.g. , surprise ) that are rarely seen in daily life , which motivates us to investigate this problem . 
In this paper , we propose a modality - transferable network with cross - modality emotion embeddings to model the relationship between emotions . 
Given that emotion embeddings contain semantic information and emotion relations in the vector space , we use them to represent emotion categories and measure the similarity of the representations between the input sentence and target emotions to make predictions . 
Concretely , for the textual modality , we use the pre - trained GloVe ( Pennington et al . 
, 2014 ) embeddings of emotion words as the emotion embeddings . 
As there are no pre - trained emotion embeddings for the visual and acoustic modalities , the model learns two mapping functions , ft‚Üívand ft‚Üía , to transfer the emotion embeddings from the textual space to the visual and acoustic spaces ( Figure 1 ) . 
Therefore , for each modality , there will be a dedicated set of emotion embeddings . 
The distances computed in all modalities will be Ô¨Ånally fused , and the model will make predictions based on that . 
BeneÔ¨Åting from this prediction mechanism , our model can easily carry out zero - shot learning ( ZSL ) to identify unseen emotion categories using the embeddings from unseen emotions . 
The intuition behind it is that the pre - trained and projected emotion embeddings form a semantic knowledge space , which is shared by both the seen and unseen classes . 
Furthermore , with the help of embedding mapping functions , the model can also perform ZSL on a single modality during inference time . 
When a few samples from unseen emotions are available , our model can adapt to new emotions without forgetting the previous emotions by using joint training and continual learning ( Lopez - Paz and Ranzato , 2017 ) . 
Our contributions in this work are three - fold:‚Ä¢We introduce a simple but effective end - toend model for the multi - modal emotion recognition task . 
It learns the relationship of different emotion categories using emotion embeddings . 
‚Ä¢To the best of our knowledge , this paper is the Ô¨Årst to investigate multi - modal emotion recognition in the low - resource scenario . 
Our model can directly adapt to an unseen emotion , even if only one modality is available . 
‚Ä¢Experimental results show that our model achieves state - of - the - art results on most emotion categories . 
We also provide a thorough analysis of zero - shot and few - shot learning . 
2 Related Works 2.1 Multi - modal Emotion Recognition Since the early 2010s , multi - modal emotion recognition has drawn more and more attention with the rise of deep learning and its advances in computer vision and natural language processing ( BaltruÀásaitis et al . 
, 2018 ) . 
Schuller et al . 
( 2011 ) proposed the Ô¨Årst Audio - Visual Emotion Challenge and Workshop ( A VEC ) , which focused on multimodal emotion analysis for health . 
In recent years , most achievements in this area aimed to Ô¨Ånd a better modality fusion method . 
Zadeh et al . 
( 2017 ) introduced a tensor fusion network that combined data representation from each modality to a tensor by performing the Cartesian product . 
In addition , the attention mechanism ( Bahdanau et al . 
, 2015 ) has been widely applied to do modality fusion and emphasis ( Zadeh et al . 
, 2018a ; Pham et al . 
, 2018 ; Tsai et al . 
, 2019a ) . 
Furthermore , Liu et al . 
( 2018 ) proposed a low - rank architecture to decrease the problem complexity , and Tsai et al . 
( 2019b ) introduced a modality re - construction method to generate occasional missing data in a modality . 
Although prior works have made progress on this task , the relationship between emotion categories has not been well modelled in previous works , except by Xu et al . 
( 2020 ) , who captured emotion correlations using graph networks for emotion recognition . 
However , the model is only based on a single textual modality . 
Additionally , the previous studies have not put much effort toward unseen and lowresource emotion categories , which is a problem of multi - modal emotion data by nature.270 LSTMV LSTMV LSTMV LSTMV LSTMAIntra - Modality 	 Encoder 	 Networks Thank 	   excited you I V isual 	 emotion 	 embeddings Acoustic 	 emotion 	 embeddingsT extual 	 emotion 	 embeddingsFusionAttention Attention Attention ... 		  ... 	 ... Modality 	 Mapping Modality 	 Fusion Loss ... 	  LSTM T LSTM T LSTM T LSTM T LSTMA LSTMA LSTMAFigure 2 : The architecture of our proposed multi - modal emotion recognition model . 
It consists of three LSTM networks , one emotion embedding mapping module , and one modality fusion module . 
For each modality , the input is a sequence of length T. Each modality has a set of emotion embeddings by mapping the GloVe textual emotion embeddings to the other modalities using ft‚Üívandft‚Üía . 
The whole architecture is optimized end - to - end . 
2.2 Zero / Few - Shot and Continual Learning Zero - shot and few - shot learning methods , which address the data scarcity scenario , have been applied to many popular machine learning tasks where zero or only a few training samples are available for the target tasks or domains , such as machine translation ( Johnson et al . 
, 2017 ; Gu et al . 
, 2018 ) , dialogue generation ( Zhao and Eskenazi , 2018 ; Madotto et al . 
, 2019 ) , dialogue state tracking ( Liu et al . 
, 2019c ; Wu et al . 
, 2019 ) , slot Ô¨Ålling ( Bapna et al . 
, 2017 ; Liu et al . 
, 2019b , 2020 ) , and accented speech recognition ( Winata et al . 
, 2020 ) . 
They have also been adopted in multiple cross - lingual tasks , such as named entity recognition ( Xie et al . 
, 2018 ; Ni et al . 
, 2017 ) , part - ofspeech tagging ( Wisniewski et al . 
, 2014 ; Huck et al . 
, 2019 ) , and question answering ( Liu et al . 
, 2019a ; Lewis et al . 
, 2019 ) . 
Recently , several methods have been proposed for continual learning ( Rusu et al . 
, 2016 ; Kirkpatrick et al . 
, 2017 ; Lopez - Paz and Ranzato , 2017 ; Fernando et al . 
, 2017 ; Lee et al . 
, 2017 ) , and these were applied to some NLP tasks , such as opinion mining ( Shu et al . 
, 2016 ) , document classiÔ¨Åcation ( Shu et al . 
, 2017 ) , and dialogue state tracking ( Wu et al . 
, 2019 ) . 
3 Methodology As shown in Figure 2 , our model consists of three parts : intra - modal encoder networks , emotion em - bedding mapping modules , and an inter - modal fusion module . 
In this section , we Ô¨Årst deÔ¨Åne the problem , and then we introduce the details of our model . 
3.1 Problem DeÔ¨Ånition We deÔ¨Åne the input multi - modal data samples as X={(ti , ai , vi)}I i=1 , in which Idenotes the total number of samples , and t , a , vdenote the textual , acoustic , and visual modalities , respectively . 
For each modality , there is a set of emotion embeddings that represent the semantic meanings for the emotion categories to be recognized . 
In the textual modality , we have Et={et k}K k=1 , which is from the pre - trained GloVe embeddings . 
In acoustic and visual modalities , we have Ea={ea k}K k=1 andEv={ev k}K k=1 , which are mapped from Etby the mapping function ft‚Üívandft‚Üía . 
Kdenotes the number of emotion categories , and it can be changed to Ô¨Åt different tasks and zero - shot learning . 
Y={yi}I i=1denotes the annotations for multilabel emotion recognition , where yiis a vector of length Kwith binary values . 
3.2 Intra - modality Encoder Networks As shown in Figure 2 , for each data sample , there are three sequences of length Tfrom the three modalities . 
For each modality , we use a bi - directional Long - Short Term Memory ( LSTM ) ( Hochreiter and Schmidhuber , 1997 ) net-271 work as the encoder to process the sequence and get a vector representation . 
In other words , for the ith data sample , we will have three vectors , r(i ) t‚ààRdt , r(i ) a‚ààRda , andr(i ) v‚ààRdv , that represent the textual , acoustic , and visual modalities . 
Here , dt , da , anddvare the dimensions of the emotion embeddings of the textual , acoustic , and visual modalities , respectively . 
3.3 Modality Mapping Module As mentioned in Section 1 , previous works do not consider the connections in different emotion categories , and the only information about emotions is in the annotations . 
In our model , we use emotion word embeddings to inject the semantic information of emotions into the model . 
Additionally , emotion embeddings also contain the relationships between emotion categories . 
For the textual modality , we use pre - trained GloVe ( Pennington et al . 
, 2014 ) embeddings of Kemotion words , denoted asEt‚ààRK√ódt . 
For the other two modalities , because there are no off - the - shelf pre - trained emotion embeddings , our model learns two mapping functions which project the vectors from the textual space into the acoustic and visual spaces : Ea = ft‚Üía(Et)‚ààRK√óda(1 ) Ev = ft‚Üív(Et)‚ààRK√ódv . 
( 2 ) 3.4 Modality Fusion and Prediction To predict the emotions for input sentences , we calculate the similarity scores between the sequence representation and the emotion embeddings for each modality . 
As shown in Eq.3 , for a data sample i , every modality will have a vector of similarity scores of length Kby dot product attention . 
We further add a modality fusion module to weighted sum all the vectors , in which the weights are also optimized end - to - end ( Eq.4 ) . 
Finally , as the datasets are multi - labelled , the sigmoid activation function is applied to each score in the fused vector s(i ) , and a threshold is used to decide whether an emotion exists or not . 
s(i ) t = Etr(i ) t , s(i ) a = Ear(i ) a , s(i ) v = Evr(i ) v ( 3 ) s(i)=Sigmoid ( wts(i ) t+was(i ) a+wvs(i ) v)(4 ) 4 Unseen Emotion Prediction Collecting numerous training samples for a new emotion , especially for a low - resource emotion , is expensive and time - consuming . 
Therefore , inthis section , we concentrate on the ability of our model to generalize to an unseen target emotion by considering the scenario where we have zero or only a few training samples in an unseen emotion . 
4.1 Zero - Shot Emotion Prediction Ideally , our model is able to directly adapt to a new emotion based on its embedding . 
Given a new text emotion embedding et k+1 , we can generate the visual and acoustic emotion embeddings ev k+1 andea k+1 , respectively , using the already learned mapping functions ft‚Üívandft‚Üía . 
After that , the similarity scores between the input sentence and the new emotion can be computed for each modality . 
4.2 Few - Shot Emotion Prediction In this section , we assume 1 % of the positive training samples in a new emotion are available , and to balance the training samples , we take the same amount of negative training samples for the new emotion . 
However , the model could lose its ability to predict the original emotions when we simply Ô¨Åne - tune it on the training samples of a new emotion . 
To cope with this issue , we propose two Ô¨Ånetuning settings . 
First , after we obtain the trained model in the source emotions , we jointly train it with the training samples of the source emotions and the new target emotion . 
Second , we utilize a continual learning method , gradient episodic memory ( GEM ) ( Lopez - Paz and Ranzato , 2017 ) , to prevent the catastrophic forgetting of previously learned knowledge . 
The purpose of using continual learning is that we do not need to retrain with all the data from previously learned emotions since the data might not be available . 
We describe the training process of GEM as follows : We deÔ¨Åne ŒòSas the model ‚Äôs parameters trained in the source emotions , and Œòdenotes the current optimized parameters based on the target emotion data . 
GEM keeps a small number of samples N from the source emotions , and a constraint is applied on the gradient to prevent the loss on the stored samples from increasing when the model learns the new target emotion . 
The training process can be formulated as Minimize ŒòL(Œò ) Subject to L(Œò , N)‚â§L(ŒòS , N ) , where L(Œò , N)is the loss value of the N stored samples.272 5 Experiments In this section , we Ô¨Årst introduce the two public datasets we use and data feature extraction . 
Then , we discuss our evaluation metrics , including their advantages and defects . 
Finally , we introduce the baselines and our experimental settings . 
5.1 Datasets CMU - MOSEI CMU Multimodal Opinion Sentiment and Emotion Intensity ( CMUMOSEI ) ( Zadeh et al . 
, 2018b ) is currently the largest public dataset for multi - modal sentiment analysis and emotion recognition . 
It comprises 23,453 annotated data samples extracted from 3228 videos . 
For emotion recognition , it consists of six basic categories : anger , disgust , fear , happy , sad , andsurprise . 
For zero - shot and few - shot learning evaluation , we use four relatively low - resource categories among them ( anger , disgust , fear , surprise ) . 
The model is trained on the other Ô¨Åve categories when evaluating one zero - shot category . 
A detailed statistical table about these categories is included in Appendix A. IEMOCAP The Interactive Emotional Dyadic Motion Capture ( IEMOCAP ) ( Busso et al . 
, 2008 ) dataset was created for multi - modal human emotion analysis , and was collected from dialogues performed by ten actors . 
It is also a multi - labelled emotion recognition dataset which contains nine emotion categories . 
For comparison with prior works ( Wang et al . 
, 2018 ; Liang et al . 
, 2018 ; Pham et al . 
, 2018 ; Tsai et al . 
, 2019a ) where four ( out of the nine ) emotion categories are selected for training and evaluating the models , we also follow the same four categories , namely , happy , sad , angry , and neutral , to train our model . 
For zero - shot learning evaluation , we consider three low - resource categories from the remaining Ô¨Åve , namely , excited , surprised , and frustrated , as unseen emotions . 
5.2 Data Feature Extraction We use CMU - Multimodal SDK ( Zadeh et al . 
, 2018c ) for downloading and pre - processing the datasets . 
It helps to do data alignment and earlystage feature extraction for each modality . 
The textual data is tokenized in word level and represented using GloVe ( Pennington et al . 
, 2014 ) embeddings . 
Facial action units are extracted by the Facet ( iMotions , 2017 ) to indicate muscle movements and expressions ( Ekman et al . 
, 1980 ) . 
These are a commonly used type of feature for facial expressionrecognition ( Fan et al . 
, 2020 ) . 
For acoustic data , COV AREP ( Degottex et al . 
, 2014 ) is used to extract fundamental features , such as mel - frequency cepstral coefÔ¨Åcients ( MFCCs ) , pitch tracking , glottal source parameters , etc . 
5.3 Evaluation Metrics Weighted Accuracy Due to the imbalanced nature of the emotion recognition dataset ( for each emotion category , there are many more negative samples than positive samples ) , we use binary weighted accuracy ( Tong et al . 
, 2017 ) on each category to better measure the model ‚Äôs performance . 
The formula is Weighted Acc . 
= TP√óN / P + TN 2N where P means total positive , TP true positive , N total negative , and TN true negative . 
Weighted F1 In prior works ( Zadeh et al . 
, 2018b ; Akhtar et al . 
, 2019 ; Tsai et al . 
, 2019a ) , the binary weighted F1 score metric is used on the CMUMOSEI dataset , and its formula is shown in Eq.5 . 
Weighted F1 = P I√óF1p+N I√óF1n(5 ) Here , F1pis the F1 score that treats positive samples as positive , while F1ntreats negative samples aspositive , and they are weighted by their portion of the data . 
However , there is one defect of using binary weighted F1 in this task . 
As there are many more negative samples than positive ones , we Ô¨Ånd that with the increase of the threshold , the weighted F1 score will also increase because the true negativeincreases . 
Therefore , in this paper , we do not report this metric . 
A detailed analysis of this is given in Appendix B. AUC Score To eliminate the effect of threshold and mitigate the defect of the weighted F1 score , we also report Area under the ROC Curve ( AUC ) scores . 
The AUC score considers classiÔ¨Åcation performance on both positive and negative samples , and it is scale- and threshold - invariant . 
5.4 Baselines For both the CMU - MOSEI and IEMOCAP datasets , we use Early Fusion LSTM ( EF - LSTM ) and Late Fusion LSTM ( LF - LSTM ) as two baseline models . 
Additionally , for CMU - MOSEI , the Graph Memory Fusion Network ( Graph - MFN ) ( Zadeh et al . 
, 2018b ) and a multi - task learning ( MTL)273 Emotion Anger Disgust Fear Happy Sad Surprise Average Metrics W - Acc AUC W - Acc AUC W - Acc AUC W - Acc AUC W - Acc AUC W - Acc AUC W - Acc AUC EF - LSTM 58.5 62.2 59.9 63.9 50.1 69.8 65.1 68.9 55.1 58.6 50.6 54.3 56.6 63.0 LF - LSTM 57.7 66.5 61.0 71.9 50.7 61.1 63.9 68.6 54.3 59.6 51.4 61.5 56.5 64.9 Graph - MFN 62.6 - 69.1 - 62.0 - 66.3 - 60.4 - 53.7 - 62.3 MTL 66.8 68.0‚Ä†72.7 76.7‚Ä†62.2 42.9‚Ä†53.6 71.4‚Ä†61.4 57.6‚Ä†60.6 65.1‚Ä†62.8 63.6‚Ä† Ours 67.0 71.7 72.5 78.3 65.4 71.6 67.9 73.9 62.6 66.7 62.1 66.4 66.2 71.4 Table 1 : Results of multi - modal emotion recognition on the CMU - MOSEI dataset . 
Baselines ( EF - LSTM , LFLSTM ) and previous state - of - the - art models ( Graph - MFN ( Zadeh et al . 
, 2018b ) , MTL ( Akhtar et al . 
, 2019 ) ) are compared . 
Results marked by‚Ä†are re - run and Ô¨Åne - tuned by us as they are not reported in the original paper . 
Emotion Happy Sad Angry Neutral Metrics Acc AUC Acc AUC Acc AUC Acc AUC EF - LSTM 85.8 70.7 83.7 85.8 75.8 90.3 67.1 74.1 LF - LSTM 85.2 71.7 83.4 84.4 79.5 86.8 66.5 72.2 RMFN ( Liang et al . 
, 2018 ) 87.5 - 83.8 - 85.1 - 69.5 RA VEN ( Wang et al . 
, 2018 ) 87.3 - 83.4 - 87.3 - 69.7 MCTN ( Pham et al . 
, 2018 ) 84.9 - 80.5 - 79.7 - 62.3 MulT ( Tsai et al . 
, 2019a ) 83.5‚Ä†71.2‚Ä†85.0‚Ä†89.3‚Ä†85.5‚Ä†92.4‚Ä†71.0‚Ä†77.2‚Ä† Ours 85.0 74.2 86.6 88.4 88.1 93.2 71.1 76.7 Table 2 : Multi - modal emotion recognition results on IEMOCAP . 
We re - run MulT ( marked by‚Ä† ) with its reported best hyper - parameters to get the AUC scores . 
model ( Akhtar et al . 
, 2019 ) are included for comparison with previous state - of - the - art models . 
For IEMOCAP , the Recurrent Multistage Fusion Network ( RMFN ) ( Liang et al . 
, 2018 ) , Recurrent Attended Variation Embedding Network ( RA VEN ) ( Wang et al . 
, 2018 ) , and the Multimodal Transformer ( MulT ) ( Tsai et al . 
, 2019a ) are included . 
To compare the AUC scores and zeroshot performance with baselines , we re - run the MTL and MulT models based on their reported best hyper - parameters , and we also carry out hyperparameter search for a fair comparison . 
CMU - MOSEI IEMOCAP Best Epoch 15 16 Batch size 512 32 Learning rate 1e-4 1e-3 # LSTM layers 2 2 Hidden Size 300/200/100 300/200/100 Dropout 0.15 0.15 Gradient Clip 10.0 1.0 Random Seed 0 0 Table 3 : The hyper - parameters of our best models . 
The hidden size means the size of the LSTM hidden state of the textual / acoustic / visual modality , respectively . 
5.5 Training Details The model is trained end - to - end with the Adam optimizer ( Kingma and Ba , 2015 ) and a scheduler that will reduce the learning rate by a factor of 0.1 when the optimization stays on a plateau for more than 5 epochs . 
The best hyper - parameters inour training for both datasets are shown in Table 3 . 
Also , we use the largest GloVe word embeddings ( glove.840B.300d2 ) for both the input text data and the emotion embeddings in the textual modality . 
The weights of the textual embeddings are frozen during training to keep the pre - trained relations , which is also essential for doing zero - shot learning . 
6 Analysis 6.1 Results Table 1 shows our model ‚Äôs performance on the CMU - MOSEI dataset . 
Compared to existing baselines , our model surpasses them by a large margin . 
The weighted accuracy ( W - Acc ) and AUC score are used for evaluation , with a threshold set to 0.5 to calculate the W - Acc . 
As discussed in Section 5.3 , we do not follow the previous papers in using the weighted F1 - score ( W - F1 ) because it does not provide an effective evaluation when the dataset is very imbalanced . 
For example , the weakest baseline , EF - LSTM , can even achieve 90 % W - F1 by predicting almost all samples as negative . 
More plots and analysis of this defect of W - F1 are included in Appendix B. We further test our model on a second dataset called IEMOCAP , and the results are shown in Table 2 . 
Similarly , our model achieves better results on most emotion categories , except happy . 
For a 2https://nlp.stanford.edu/projects/ glove/274 Metrics W - Acc AUC W - Acc AUC W - Acc AUC W - Acc AUC Unseen emotion Anger ( unseen ) Disgust ( unseen ) Fear ( unseen ) Surprise ( unseen ) Zero - ShotEF - LSTM 50.6 50.9 50.3 48.2 45.8 42.3 50.2 46.9 LF - LSTM 48.4 49.2 49.7 44.2 47.4 47.3 48.6 48.3 Ours 55.9 61.6 67.5 72.7 41.8 40.6 53.4 55.5 1 % Few - ShotFT ( Ours ) 58.9 61.9 67.9 71.5 43.1 43.1 51.8 53.9 CL ( Ours ) 58.9 61.5 68.7 72.8 42.6 42.7 50.6 52.5 JT ( Ours ) 59.0 61.1 69.2 74.2 41.9 41.7 55.2 58.1 Average on all categories Except Anger Except Disgust Except Fear Except Surprise Zero - shot Ours 65.6 70.6 64.4 69.3 65.9 70.9 67.2 71.4 1 % Few - ShotFT ( Ours ) 64.4 69.8 63.7 68.5 65.4 70.7 65.1 71.4 CL ( Ours ) 64.6 69.8 63.8 68.9 65.6 70.9 65.5 71.5 JT ( Ours ) 64.3 69.3 63.5 68.8 65.9 70.8 66.1 71.5 Table 4 : Zero / few - shot results on low - resource emotion categories in CMU - MOSEI dataset . 
Here , FT , CL , and JT stand for Fine - Tuning , Continual Learning , and Joint Training respectively . 
FT directly Ô¨Åne - tunes the trained model on the unseen emotions , and CL and JT are two different settings introduced in Section 4.2 . 
Note that in the few - shot settings , we select the model based on the average performance of all emotions ( including the unseen emotion ) to ensure good overall performance of our model . 
Figure 3 : Euclidean distances between different emotion embeddings in the textual , acoustic , and visual spaces . 
Although the absolute values are different , the relative distances between emotion categories are well reserved . 
This indicates that the two mapping functions ft‚Üívandft‚Üíatransfer the relationships of emotion categories well . 
fair comparison on IEMOCAP , we use accuracy instead of W - Acc , following the previous works compared in the table . 
6.2 Effects of Emotion Embeddings Quantitatively , our model makes a large improvement in the multi - modal emotion recognition task . 
We think it beneÔ¨Åts greatly from the emotion embeddings , which can model the relationships ( or distances ) between emotion categories . 
This is especially important for emotion recognition , which is a multi - label task by nature , as people can have multiple emotions at the same time . 
For example , if a person is surprised , it is more likely that this person is also happy andexcited and is less likely to bedisgusted orsad . 
This kind of information is expected to be modelled and captured by emotion embeddings . 
Intuitively , in the textual space , related emotions ( e.g. , angry anddisgusted ) tend to have closer word vectors than unrelated emotions ( angry andhappy ) . 
To ensure the effectiveness of word embeddings , for each emotion word , we investi - gated multiple forms of it . 
For example , for surprised , we also tried with Surprised , ( S / s)urprising , ( S / s)urprise . 
Generally , they all show a similar trend , and in most cases , the word form that is used to describe human shows the best results . 
In our Ô¨Ånal setting , we iterate and pick the best performing form for each emotion category . 
Moreover , our model can transfer the relationship of emotion categories from the textual space to the acoustic and visual spaces using end - to - end optimized mapping functions . 
In Figure 3 , we show the Euclidean distances of emotion embeddings between categories . 
The relative positions are preserved very well after being transferred from the textual space to the visual and acoustic spaces . 
This indicates that the learned mapping functions ( ft‚Üív andft‚Üía ) are effective . 
Although it is not the main focus of this paper , we think improving the pre - trained textual emotion embeddings is an essential direction for future work . 
It can beneÔ¨Åt all modalities and further enhance the overall performance . 
For example , incorporate semantic emotion275 information ( Xu et al . 
, 2018 ) to the original word embeddings . 
6.3 Zero / Few - Shot Results BeneÔ¨Åting from the pre - trained textual emotion embeddings and learned mapping functions , our model can recognize unseen emotion categories to a certain extent . 
We evaluate our model ‚Äôs zero - shot learning ability on the low - resource categories in CMU - MOSEI ( shown in Table 4 ) and IEMOCAP ( shown in Table 5 ) . 
For a fair comparison , we use the same training setting that is used in Table 1 . 
This can ensure that no downgrade happens on the seen emotions , and the model is not selected to overÔ¨Åt a single unseen category . 
As we can see , the zero - shot results of the baselines are similar to random guesses , because the weights related to that unseen emotion in the model are randomly initialized and have never been optimized . 
For our model , the zero - shot performance is much better than that of the baselines in almost all emotions . 
This is because our model learns to classify emotion categories based on the similarity between the sentence representation and emotion embeddings , which enables our model better generalization ability to other unseen emotions since emotion embeddings contain semantic information in the vector space . 
Furthermore , we perform few - shot learning using only 1 % of data of these low - resource categories . 
As we can see from Table 4 , using very few training samples , our model can adapt to unseen emotions without losing the performance in the source emotions . 
In addition , we observe that simply Ô¨Åne - tuning ( FT ) our trained model sometimes obtains inferior performance . 
This is because our model will gradually lose the ability to classify the source emotions , and we have to early stop the Ô¨Åne - tuning process , which leads to inferior performance . 
We can see that CL and JT prevent our model from catastrophic forgetting and improve the few - shot performance in the unseen emotion . 
Moreover , JT achieves slightly better performance than CL . 
This can be attributed to the fact that CL might still result in performance drops in source emotions since our model only observes partial samples from them . 
At the same time , JT directly optimizes the model on the data samples of such emotions . 
Unseen emotionExcited ( unseen)Surprised ( unseen)Frustrated ( unseen ) Metrics Acc F1 Acc F1 Acc F1 EF - LSTM 13.1 23.1 11.3 5.1 22.9 37.2 LF - LSTM 14.0 23.3 2.6 5.1 23.7 37.4 MulT 45.1 27.3 41.4 7.5 48.7 40.9 Ours ( TA V ) 82.0 56.1 78.8 13.1 73.6 57.9 Ours ( TA ) 79.9 52.7 79.3 14.4 75.1 60.1 Ours ( TV ) 75.9 42.7 58.6 9.1 54.1 13.6 Ours ( A V ) 89.1 69.9 65.7 13.2 83.9 73.6 Ours ( T ) 72.9 37.1 67.7 3.1 55.3 9.0 Ours ( A ) 76.9 52.8 82.1 16.8 86.1 74.8 Ours ( V ) 82.1 35.0 81.1 6.2 68.6 44.6 Table 5 : Zero - shot results on the IEMOCAP dataset . 
T ( textual ) , A ( acoustic ) , and V ( visual ) indicate the existence of that modality during inference time . 
Metric W - Acc Emotion Anger Disgust Fear Happy Sad Surprise T+A+V 67.0 72.5 65.4 67.9 62.6 62.1 T+A 65.0 71.9 64.8 66.0 63.0 59.9 T+V 64.9 71.2 66.7 67.6 61.0 60.4 A+V 63.8 71.1 65.5 64.5 61.3 55.2 Only T 61.5 69.0 64.3 64.2 59.7 61.2 Only A 61.9 71.5 66.9 62.7 61.0 54.8 Only V 63.4 69.7 63.2 63.2 58.5 53.3 Table 6 : Ablation study on CMU - MOSEI dataset . 
Different combinations of subsets of modalities are used . 
6.4 Ablation Study To further investigate how each individual modality inÔ¨Çuences the model , we perform comprehensive ablation studies on supervised multi - modal emotion recognition and also zero - shot prediction . 
In Table 6 , we enumerate different subsets of the ( textual , acoustic , visual ) modalities to evaluate the effect of each single modality . 
Generally , the performance will increase if more modalities are available . 
Compared to single - modal data , multimodal data can provide supplementary information , which leads to more accurate emotion recognition . 
In terms of a single modality , we Ô¨Ånd that textual andacoustic are more effective than visual . 
Similarly , in Table 5 , we show the zero - shot performance with different combinations of modalities during the inference time ( all modalities exist in the training phase ) . 
As there are many more negative samples than positive ones in the ZSL setting , we also evaluate the models with the unweighted F1 score . 
Because if a model has high accuracy but a low F1 , it is heavily biased to the negative samples so it can not do classiÔ¨Åcation effectively . 
Empirical results indicate that zero - shot on only one modality is possible . 
Moreover , if the data of an emotion category has strong characteristics in276 one modality and is ambiguous in other modalities , single - modality can even surpass multi - modality on zero - shot prediction . 
For example , the performance of single - modality zero - shot prediction using the acoustic modality on the surprised category is better than using all modalities . 
7 Conclusion In this paper , we introduce a modality - transferable model that leverages cross - modality emotion embeddings for multi - modal emotion recognition . 
It makes predictions by measuring the distances between input data and target emotion categories , which is especially effective for a multi - label problem . 
The model also learns two mapping functions to transfer pre - trained textual emotion embeddings to acoustic and visual spaces . 
The empirical results demonstrate that it exhibits state - of - the - art performance on most of the categories . 
Enabled by the utilization of emotion embeddings , our model can carry out zero - shot learning for unseen emotion categories and can quickly adapt few - shot learning without downgrading trained categories . 
Acknowledgement This work is funded by MRP/055/18 of the Innovation Technology Commission , the Hong Kong SAR Government . 
Abstract In this paper , we aim at learning the relationships and similarities of a variety of tasks , such as humour detection , sarcasm detection , offensive content detection , motivational content detection and sentiment analysis on a somewhat complicated form of information , i.e. ,memes . 
We propose a multi - task , multi - modal deep learning framework to solve multiple tasks simultaneously . 
For multi - tasking , we propose two attention - like mechanisms viz . 
, Inter - task Relationship Module ( iTRM ) and Inter - class Relationship Module ( iCRM ) . 
The main motivation of iTRM is to learn the relationship between the tasks to realize how they help each other . 
In contrast , iCRM develops relations between the different classes of tasks . 
Finally , representations from both the attentions are concatenated and shared across the Ô¨Åve tasks ( i.e. , humour , sarcasm , offensive , motivational , and sentiment ) for multi - tasking . 
We use the recently released dataset in the Memotion Analysis task @ SemEval 2020 , which consists of memes annotated for the classes as mentioned above . 
Empirical results on Memotion dataset show the efÔ¨Åcacy of our proposed approach over the existing state - of - theart systems ( Baseline and SemEval 2020 winner ) . 
The evaluation also indicates that the proposed multi - task framework yields better performance over the single - task learning . 
1 Introduction The content and form of content shared on online social media platforms have changed rapidly over time . 
Currently , one of the most popular forms of media shared on such platforms is ‚Äô Memes ‚Äô . 
According to its deÔ¨Ånition from Oxford Dictionary , a meme is a piece of data , often in the form of images , text or videos that carry cultural information through an imitable phenomenon with a mimicked theme , that is shared ( sometimes with slight modiÔ¨Åcation ) rapidly by internet users . 
Every meme can be associated with Ô¨Åve affect values , namely humour ( Hu ) , sarcastic ( Sar ) , offensive ( Off ) , motivational ( Mo ) , and sentiment ( Sent ) . 
Hence , in a broad sense , memes can be categorized into four intersecting setsviz.humorous memes , sarcastic memes , offensive memes , and motivational memes . 
Humour refers to the quality of being amusing or comic . 
Formally , humour is deÔ¨Åned as the nature of experiences to induce laughter and provide amusement . 
Humourous memes are the most popular and widely used on social media platforms . 
An example for humourous memes is shown in Figure 1a . 
Sarcasm is often used to convey thinly veiled disapproval humorously . 
A sarcastic meme is a meme where an incongruity exists between the intended meaning and the way it is expressed . 
These are generally used to express dissatisfaction or to veil insult through humour . 
As we can see in Figure 1a , the person on the right is made fun of , without explicitly expressing it , which is a typical example of a sarcastic meme . 
Offensive content include a lot of insulting , derogatory terms . 
It is contrary to the moral sense or good . 
As social media expands , offensive language has become a huge headache to maintain sanity on social media . 
As memes are growing to become more and more popular , detecting offensive memes on such platforms is becoming an important and challenging task . 
Figure 1a , Figure 1c and Figure 1d are the instances of Offensive memes . 
Motivation is derived from the word ‚Äô motive ‚Äô which means needs or desires within the individuals . 
It is the process of stimulating people to actions to achieve their goals . 
By its deÔ¨Ånition , motivational memes are those that beneÔ¨Åt a certain group of people to achieve their plans or goals . 
Motivation can be both either positive or negative.281 ( a)Humour , sarcasm , offensive . 
  ( b)Motivational , positive . 
( c)Sarcasm , offensive , Negative . 
  ( d)Sarcasm , offensive , Funny . 
Figure 1 : Few examples from the Memotion dataset to show the inter - dependency between different tasks . 
However , we usually consider motivation in a positive sense . 
Figure 1b is an excellent example for the positive motivation . 
Sentiment analysis refers to the process of computationally identifying and categorizing opinions expressed in a piece of communication , especially to determine whether the writer ‚Äôs attitude towards a particular topic , product , etc . 
is positive , negative , orneutral . 
This has been a very prominent and important task in Natural Language Processing . 
Sentiment analysis on memes refers to the task of systematically extracting its emotional tone in understanding the opinion expressed by the meme . 
Figure 1b is an example for positive sentiment towards the government and Figure 1c for negative sentiment towards Ph.D. in Electrical Engineering . 
Generally , speciÔ¨Åc labels of one task have a strong relation to the other labels of sarcasm , offensive , humour or motivational tasks . 
Through proper representation , training , and evaluation , these relations can be modelled to help each other for better classiÔ¨Åcation . 
For example , in Figure 1b , just by seeing text , the meme can be either sarcastic or motivational , but the image in the meme conÔ¨Årms that this has an overall positive sentiment and hence motivational . 
Similarly , in Figure 1c , knowing that the meme is sarcastic and has a negative sentiment makes it highly probable to being offensive . 
As seen above , humorous , motivational , offensive , and sarcastic nature of the memes are closely related . 
Thus , a multi - task learning framework would be extremely beneÔ¨Åcial in such scenarios . 
In this paper , we exploit these relationships and similarities in the tasks of humour detection , sarcasm detection , offensive content detection , motivational content detection , and sentiment in a multi - task manner . 
The main contributions and/or attributes are as follows : ( a).We propose a multi - task multimodal deep learning framework to leverage the util - ity of each task to help each other in a multi - task framework ; ( b).We propose two attention mechanisms viz . 
iTRM and iCRM to better understand the relationship between the tasks and between the classes of tasks , respectively ; and ( c).We present the state - of - the - art results for meme prediction in the multi - modal scenario . 
2 Related Work Sentiment analysis and its related tasks , such as humour detection , sarcasm detection , and offensive content detection , are the topics of interest due to their needs in recent times . 
There has been a phenomenal growth in multi - modal information sources in social media , such as audio , video , and text . 
Multi - modal information analysis has attracted the attention of researchers and developers due to their complexity , and multi - tasking has been of keen interest in the Ô¨Åeld of affect analysis . 
Humour : Early feature - based models attempt to solve humour include the models based on word overlap with jokes , presence of ambiguity , and word overlap with common idioms ( Sj ¬®obergh and Araki , 2007 ) , human - centeredness , and negative polarity ( Mihalcea and Pulman , 2007 ) . 
Some of the recent multi - modal approaches include utilizing information from the various modalities , such as acoustic , visual , and text , using deep learning models ( Bertero and Fung , 2016 ; Yang et al . 
, 2019 ; Swamy et al . 
, 2020 ) . 
Yang et al . 
( 2020 ) employs a paragraph decomposition technique coupled with Ô¨Åne - tuning BERT ( Devlin et al . 
, 2018 ) model for humour detection on three languages ( Chinese , Spanish and Russian ) . 
Sarcasm : Starting from the traditional approaches , such as rule - based methods ( Veale and Hao , 2010 ) , lexical features ( Carvalho et al . 
, 2009 ) , and incongruity ( Joshi et al . 
, 2015 ) to all the way up to multi - modal deep learning techniques ( Schi-282 fanella et al . 
, 2016 ) , sarcasm detection has been showing its presence . 
Castro et al . 
( 2019 ) created a multi - modal conversational dataset , MUStARD from the famous TV shows , and provided baseline SVM approaches for sarcasm detection . 
Recently , Chauhan et al . 
( 2020 ) proposed a multi - task learning framework for multi - modal sarcasm , sentiment and emotion analysis to explore how sentiment and emotion helps sarcasm . 
The author used the MUStARD dataset and extended the MUStARD dataset with sentiment ( implicit and explicit ) and emotion ( implicit and explicit ) labels . 
Offensive : Razavi et al . 
( 2010 ) used a threelevel classiÔ¨Åcation model taking advantage of various features from statistical models and rulebased patterns and various dictionary - based features . 
Chen et al . 
( 2012 ) proposed a feature - based Lexical Syntactic Feature ( LSF ) architecture to detect the offensive contents . 
Gomez et al . 
( 2020 ) created a multi - modal hate - speech dataset from Twitter ( MMHS150 K ) to introduce a deep - learningbased multi - modal Textual Kernels Model ( TKM ) and compare it with various existing deep learning architectures on the proposed MMHS150 K dataset . 
Motivation : Swieczkowska et al . 
( 2020 ) proposes a novel chaining method of neural networks for identifying motivational texts where the output from one model is passed on to the second model . 
Sentiment : An important task to leverage multimodality information effectively is to combine them using various strategies . 
Mai et al . 
( 2019 ) employs a hierarchical feature fusion strategy , Divide , Conquer , and Combine for affective computing . 
Chauhan et al . 
( 2019 ) uses the Inter - modal Interaction Module ( IIM ) to combine information from a pair of modalities for multi - modal sentiment and emotion analysis . 
Some of the other techniques include a contextual inter - modal attention based framework for multi - modal sentiment classiÔ¨Åcation ( Ghosal et al . 
, 2018 ; Akhtar et al . 
, 2019 ) . 
Multi - task : Some of the early attempts to correlate the tasks like sarcasm , humour , and offensive statements include a features based classiÔ¨Åcation using various syntactic and semantic features , such as frequency of words , the intensity of adverbs and adjectives , the gap between positive and negative terms , the structure of the sentence , synonyms and others ( Barbieri and Saggion , 2014 ) . 
More recently , Badlani et al . 
( 2019 ) proposed a convolution - based model to extract the embedding by Ô¨Åne - tuning the same for the tasks of sentiment , sarcasm , humour , and hate - speech and then concatenating these representations to be used in a sentiment classiÔ¨Åer . 
In our current work , we propose a multi - task multi - modal deep learning framework to simultaneously solve the tasks of sarcasm , humour , offensive , and motivational on memes . 
Further , to the best of our knowledge , this is the very Ô¨Årst attempt at solving the multi - modal affect analysis on memes in a multi - task deep learning framework . 
We demonstrate through a detailed empirical evaluation that a multi - task learning framework can improve the performance of individual tasks over a single task learning framework . 
3 Proposed Methodology We propose an attention - based deep learning model to solve the problem of multi - task affect analysis of memes . 
The inputs to the model are the meme itself and the manually corrected text extracted through OCR . 
The overall architecture is depicted in Figure 2 . 
The source code is available at http://www . 
iitp.ac.in/ Àúai - nlp - ml / resources.html . 
3.1 Input Layer : We now describe the input features for our proposed model . 
3.1.1 Text Input Given Nnumber of samples , where each sample is associated with meme image and the corresponding text . 
Let us assume , in each sample , there are nTnumber of words w1 : nT = w1 , ... , w nT , where wj‚ààRdT , dT= 768 , and wjis obtained using BERT ( Devlin et al . 
, 2018 ) . 
The maximum number of words for ithsample across the dataset is 189 . 
3.1.2 Image Input Image is the prime component of any meme and contains the majority of the information . 
To leverage this information effectively , feature vectors from average pooling layer ( avgpool ) of the ImageNet pre - trained ResNet-152 ( He et al . 
, 2016 ) image classiÔ¨Åcation model are extracted . 
Each image is Ô¨Årst pre - processed by resizing to 224√ó224 and then normalized . 
The extracted feature vector for image of ithsample is represented by Vi‚ààRdv anddv= 2048 . 
3.2 Attention Modules These vectors are concatenated and then passed through a set of four dense layers to obtain the vectors of equal length drepresented by TVt‚ààRd,283 Figure 2 : Overall architecture of the proposed multi - modal multi - task affect analysis framework for Memes . 
Here Vrefers to the Meme Image andTrefers to the text extracted from the Meme . 
where tis a task‚àà{humour , sarcasm , offensive , motivational } . 
These vectors are then passed through the Inter - class Relationship Module and Inter - task Relationship module . 
The output is then concatenated and passed through another set of four dense layers , and a layer of softmax is applied to obtain the Ô¨Ånal output . 
3.2.1 Inter - class Relationship Module This module is used to learn the relationship between the classes of all the tasks . 
This is done by passing TVtthrough another dense layer and softmax ( conÔ¨Ådence score ) . 
For each task , we Ô¨Årst group all the classes into two classes for the hierarchical classiÔ¨Åcation of the sample . 
At this level , the sample is labelled with either positive or negative for all the tasks . 
For instance , a sample will be labelled as either sarcastic or not sarcastic for sarcasm tasks . 
A loss is back - propagated using these conÔ¨Ådence scores for the corresponding tasks . 
This is done in order to control each dense layer so that it aligns with the respective tasks . 
Meanwhile , a dot - product of the softmax scores of each task is obtained and used to form the Score Matrix . 
This is then Ô¨Çattened and passed forward . 
3.2.2 Inter - task Relationship Module While the above module is used to Ô¨Ånd the correlation between the individual classes , this module is used to Ô¨Ånd the relationship between the different tasks in the model . 
This is done by initially Ô¨Ånding the cosine - similarity between TVtvectors . 
And apooling layer is used to collect information between the tasks and then normalized by the corresponding cosine - similarity score . 
The output from the pooling layer is then Ô¨Çattened and passed forward . 
3.3 Output Unit The Ô¨Çattened vectors from iTRM andiCRM are concatenated and then branched into four dense layers for each task . 
This is then forwarded through a softmax layer to obtain the Ô¨Ånal output for each task , and the loss is back - propagated to learn the parameters . 
In this layer , the information from both iCRM andiTRM modules will be leveraged and used to predict the Ô¨Ånal outcome . 
Please note that , there are two sets of loss used in the model , one in the iCRM module and second at the end the of Output Unit . 
4 Dataset We perform experiments using the dataset released in the Memotion Analysis 1.0 @SemEval 2020 Task ( Sharma et al . 
, 2020)1 . 
This dataset consists of 6992 samples . 
Each sample consists of an image , corrected text extracted from the meme , and the Ô¨Åve labels associated with the Ô¨Åve tasks , viz . 
, Humour , Sarcasm , Offensive , Motivational , and Overall Sentiment . 
The distribution of the classes associated with each of the Ô¨Åve tasks with label is shown in Table 1 and Table 2 . 
1https://competitions.codalab.org/com petitions/20629284 Task Classes Count RC ( % ) T - A Sentvery negative 1033 17.34Ngnegative 3127 52.48 neutral 2201 36.94 Nu positive 480 8.06Psvery positive 151 2.53 Table 1 : Dataset Distribution of Task - A , where RCand T - Adenotes the relative count and abbreviation for labels of Task - A , respectively . 
Task Classes Count RC ( % ) T - C T - B Hunotfunny 1651 30.91 NfNh funny 2452 45.91 Fn Hm very funny 2238 41.90 Vf hilarious 651 12.19 Hr Sarnotsarcastic 1544 22.08 NsNs general 3507 50.16 Gr Sr twisted meaning 1547 22.13 Tm very twisted 394 5.64 Vt Offnotoffensive 2713 38.80 NoNo slight 2592 37.07 Sg Of very offensive 1466 20.97 Vo hateful offensive 221 3.16 Ho Monotmotivational 4525 64.72 NmNm motivational 2467 35.28 MoMo Table 2 : Dataset Distribution of Task - B and Task - C , where RC , T - BandT - Cdenotes the relative count , abbreviation for labels of Task - B , and abbreviation for labels of Task - C respectively . 
We address 5 multi - modal affective analysis problems , namely humour classiÔ¨Åcation , sarcasm classiÔ¨Åcation , offensive classiÔ¨Åcation , motivational classiÔ¨Åcation , and sentiment classiÔ¨Åcation . 
A. Humour classiÔ¨Åcation : There are four classes associated with the humour task , namely not funny , funny , very funny , and hilarious , which are labelled as 0 , 1 , 2 , and 3 , respectively . 
B. Sarcasm classiÔ¨Åcation : There are four classes associated with the sarcasm task , namely not sarcastic , general , twisted meaning , and very twisted which are labelled as 0 , 1 , 2 , and 3 respectively . 
C. Offensive classiÔ¨Åcation : There are four classes associated with the offensive task , namely not offensive , slight , very offensive , and hateful offensive which are labelled as 0 , 1 , 2 , and 3 , respectively . 
D. Motivational classiÔ¨Åcation : There are two classes associated with the motivational task , namely not motivational and motivational , which are labelled as 0 and 1 , respectively . 
E. Sentiment classiÔ¨Åcation : There are Ô¨Åve classes associated with the sentiment task , namely very negative , negative , neutral , positive , and very positive , which are labelled as 0 , 1 , 2 , 3 , and 4 , respectively . 
5 Experimental setup In accordance with the SemEval 2020 ( Sharma et al . 
, 2020 ) , the project is organized into three sets of tasks2 . 
‚Ä¢Task A : Sentiment ClassiÔ¨Åcation : In this task , memes are classiÔ¨Åed into 3 classes viz . 
, -1 ( negative , very negative ) , 0 ( neutral ) and +1 ( positive , very positive ) . 
‚Ä¢Task B : Binary - class ClassiÔ¨Åcation : In this set of tasks , the memes are classiÔ¨Åed as follows ( c.f . 
T - B in Table 2 ) ; 1.Humour ( funny , very funny , hilarious ) and Non - humour ( not funny ) . 
2.Sarcasm ( general , twisted meaning , very twisted ) and Non - sarcasm ( non sarcastic ) 3.Offensive ( slight , very offensive , hateful offensive ) and Non - Offensive ( not offensive ) , and 4.Motivational ( motivational ) and Nonmotivational ( not motivational ) . 
‚Ä¢Task C : Multi - class ClassiÔ¨Åcation : In this set of task , the original labels are used as described in the dataset ( c.f . 
T - C in Table 2 ) for the tasks of Humour , Sarcasm , Offensive and Motivational . 
Please note that , in Task A , as it is not a multitask scenario , iCRM andiTRM are not applicable . 
For all the other sets of tasks , the entire network is shown in Figure 2 . 
We evaluate our proposed model on the multimodal Memotion dataset . 
We perform grid search to Ô¨Ånd the optimal hyper - parameters ( c.f . 
Table 3 ) . 
Though we aim for a generic hyper - parameter conÔ¨Åguration for all the experiments , in some cases , a different choice of the parameter has a signiÔ¨Åcant effect . 
Therefore , we choose different parameters for a different set of experiments . 
2https://competitions.codalab.org/com petitions/20629#learn thedetails - task - la bels - format285 Parameters Task - A Task - B Task - C Activations ReLu Optimizer Adam ( lr=0.001 ) Output Softmax Loss Categorical cross - entropy Batch 16 Epochs 30 Dropout - p 0.3 0.5 0.7 # neurons ( Dense ) 50 200 200 Table 3 : Model conÔ¨Ågurations We implement our proposed model on the open source machine learning library PyTorch3 . 
Hugging Face4library is used for BERT implementation . 
As the evaluation metric , we employ precision ( P ) , recall ( R ) , macro - F1 ( Ma - F1 ) , and micro - F1 ( Mi- F1 ) for all the tasks i.e. , humour , sarcasm , offensive , motivational , and sentiment . 
We use Adam as an optimizer , Softmax as a classiÔ¨Åer , and the categorical cross - entropy as a loss function for all the tasks . 
6 Results and Analysis We evaluate our proposed architecture with bimodal inputs ( i.e. , text and visual ) . 
We show the obtained results for Task - A ( i.e. , sentiment analysis)in Table 4 . 
LabelsTask - A P RMa - F1Mi - F1 Sentiment 36.99 35.70 35.81 50.58 Table 4 : Memes : Sentiment ClassiÔ¨Åcation ( Task A ) Task - B has four different tasks , i.e. , humour , sarcasm , offensive , and sentiment with binary - class labels ( c.f . 
binary - class classiÔ¨Åcation in Section 5 ) . 
The results are shown in Table 5 . 
LabelsTask - B ( Binary ClassiÔ¨Åcation ) STL MTL P RMa - F1Mi - F1 P RMa - F1Mi - F1 Hu 55.44 53.77 53.74 71.29 55.52 53.84 53.84 71.29 Sa 51.94 51.34 50.98 70.76 52.99 52.48 52.52 70.94 Of 52.33 52.19 52.13 56.28 51.35 51.37 51.36 54.10 Mo 53.56 53.49 53.51 57.18 55.86 56.44 56.12 57.44 Table 5 : Memes : Single - task vs Multi - task ( Task B ) Task - C has also four different tasks , i.e. , humour , sarcasm , offensive , and sentiment with multi - class labels ( c.f . 
multi - class classiÔ¨Åcation in Section 5 ) . 
The results are shown in Table 6 . 
3https://pytorch.org/ 4https://github.com/huggingface/trans formersLabelsTask - C ( Multi - class ClassiÔ¨Åcation ) STL MTL P RMa - F1Mi - F1 P RMa - F1Mi - F1 Hu 26.83 26.89 26.75 29.76 27.23 27.29 27.03 32.00 Sa 25.16 26.71 25.74 36.52 26.30 27.33 26.80 39.94 Of 27.21 27.30 26.93 35.30 25.05 26.04 25.53 35.94 Mo 53.32 52.89 52.65 58.46 54.14 53.31 53.72 59.79 Table 6 : Memes : Single - task vs Multi - task ( Task C ) In both the tasks B and C , we outline the comparison between the multi - task ( MTL ) and single - task ( STL ) learning frameworks in Table 5 and Table 6 . 
We observe that MTL shows better performance over the STL setups . 
For the offensive task , we Ô¨Ånd that STL performs better than MTL . 
We hypothesize that this is due to the model getting confused between the offensive and sarcastic ( or humorous ) memes . 
From Table 9 , under Sarcasm , we can see that for the class Vt , MTL predicts a few samples as sarcastic , whereas in actuality it belongs to the other classes . 
However , we can see a decrease in performance for class Ho under Offensive . 
This is due to the lack of a larger dataset for the complex model to disambiguate the same . 
In the example , BRB ... GOT TO TAKE CARE OF SOME SH*T IN UKRAIN ( c.f . 
Figure 1d ) , the actual set of labels are Fn , Gn , Sg , Nm . 
The predicted labels in STL are Vf , Gn , Sg , Moand in MTL are Vf , Tm , Vo , Mo. This is supposed to be slightly offensive but got it confused with the sarcastic . 
7 Comparative Analysis We compare the results obtained in our proposed model against the baseline model and SemEval 2020 winner , which also made use of the same dataset . 
The comparative analysis is shown in Table 7 . 
Our proposed multi - modal framework achieves the best macro - F1 of 35.8 % ( 0.4%‚Üë ) and micro - F1 of 50.6 % ( 1.9%‚Üë ) as compared to macro - F1 of 35.4 % and micro - F1 of 48.7 % of the state - of - the - art system ( i.e. , SemEval 2020 Winner ) for Task - A. Similarly , for Task - B , we obtain the macro - F1 of 53.5 % ( 1.7%‚Üë ) and micro - F1 of 63.4 % ( 2.0%‚Üë ) as compared to the macro - F1 of 51.8 % and micro - F1 of 61.4 % of the state - ofthe - art system , whereas for Task - C , we obtain the macro - F1 of 33.3 % ( 1.1%‚Üë ) and micro - F1 of 41.9 % ( 4.1%‚Üë ) as compared to the macro - F1 of 32.2 % and micro - F1 of 37.8 % of the state - of - theart system . 
It is evident from Table 5 and Table 6 that multitask learning framework successfully leverages the286 SystemsTask A Task B Task C Ma - F1Mi - F1Ma - F1Mi - F1Ma - F1Mi - F1 Baseline 21.76 30.77 50.02 56.86 30.08 33.28 SE‚Äô20 Winner 35.46 48.72 51.83 61.44 32.24 37.79 Proposed 35.81 50.58 53.46 63.44 33.27 41.92 Table 7 : Comparative Analysis of the proposed approach with recent state - of - the - art systems . 
Here , SE‚Äô20 denotes the SemEval 2020 winner , and ‚Äô Proposed ‚Äô refers to the models described in the paper for the respective tasks . 
Sentiment NgNuPs Ng17 19 127 Nu 25 170 399 Ps58 290 763 ( a ) Task - ASetups Humour Sarcasm Offensive Motivational NhHm NsSr NoOf NmMo STLNh 91 354 Ns68 353 No252 455Nm 801 387 Hm 185 1248 Sa196 1261 Of366 805Mo417 273 MTLNh 92 353 Ns90 331 No285 422Nm 801 387 Hm 186 1247 Sa239 1218 Of440 731Mo431 259 ( b ) Task - B Table 8 : Confusion Matrix for Task - A and Task - B ( Refer Table 1 and Table 2 for Label deÔ¨Ånitions ) . 
Setups Humour Sarcasm Offensive Motivational NfFnVfHr NsGrTmVt NoSgVoHo NmMo STLNf122 143 130 50Ns117 182 122 0No254 307 111 35Nm 878 310Fn140 218 205 91Gr234 427 276 0Sg224 340 105 40 Vf129 201 193 82Tm 94 188 142 0Vo109 198 62 18Mo470 220Hr36 65 47 26Vt 19 52 25 0Ho 20 37 11 7 MTLNf147 147 136 21Ns125 206 87 3No350 219 138 0Nm 924 264Fn173 240 208 33Gr222 525 172 18Sg330 250 129 0 Vf172 195 204 34Tm 112 210 100 2Vo181 131 75 0Mo491 199Hr51 70 43 10Vt 23 57 16 0Ho 43 22 10 0 Table 9 : Confusion Matrix for Task C ( Refer Table 2 for Label deÔ¨Ånitions ) . 
inter - dependence between all the tasks in improving the overall performance in comparison to the single - task learning . 
We also show the confusion matrices corresponding to each set of tasks in Table 8a , Table 8b , and Table 9 , respectively . 
8 Error Analysis We perform error analysis ( i.e. for Task - C ) on the predictions of our proposed model . 
We take some utterances ( c.f . 
Table 10 ) with corresponding image ( c.f . 
Figure 3 ) , where we show that MTL is predicting correct while STLis not able to predict the right labels . 
We also present the attention heatmaps for iCRM andiTRM of the multi - task learning framework in Figure 4 and Figure 5 , respectively . 
We take the Ô¨Åfth utterance from Table 10 ( c.f . 
Figure 3e ) to illustrate the heatmap . 
For iCRM ( c.f . 
Figure 4 ) , there are six matrices which show the interdependency between humour and sarcasm ( HuSar ) , humour and offensive ( Hu - Off ) , humour and motivational ( Hu - Mo ) , sarcasm and offensive ( saroff ) , sarcasm and motivational ( Sar - Mo ) , and offensive and motivational ( Off - Mo ) , respectively , where ( a ) 1 . 
  ( b ) 2 . 
  ( c ) 3 . 
( d ) 4 . 
  ( e ) 5 . 
  ( f ) 6 . 
Figure 3 : Few examples for Human Error Analysis corresponding to Table 10 . 
the light shade to dark shade shows the amount of contributions in ascending sequence . 
The main objective of iCRM is to develop the relationship between the classes of tasks . 
Figure 4 shows the established relationship between the tasks . 
We see the established relationship between the classes of tasks in Figure 4 . 
For predicting the Ô¨Åfth utterance correctly in Table 10 , humour and287 UtterancesSTL MTL Hu Sar Off Mo Hu Sar Off Mo 1 my name is giovanni giorgio but everybody calls me giorgio . 
NfGrNoNmVfTmVoMo 2 i ‚Äôm in shape . 
unfortunately that shape is a potato VfNsNoMoFnGrSgNm 3 obama i ‚Äôm coming after ur job as president memeshappen . 
Com FnGrNoNmVfTmVoMo 4 look at me I ‚Äôm the captain now . 
VfTmVoMoFnGrSgNm 5 freshmen .0000000000127 seconds after the bell mr . 
bean go zoom zoom . 
HrNsSgMoFnGrMoNm 6 sorry i was working . 
FnTmVoMoVfGrSgNm Table 10 : Comparison between multi - task learning and single - task learning frameworks .Few error cases where MTL framework performs better than the STL framework . 
not sarcasm ( Figure 4a ) , humour and not offensive ( Figure 4b ) etc . 
are helping each other . 
( a)Hu - Sar   ( b)Hu - Off   ( c)Hu - Mo ( d)Sar - Off   ( e)Sar - Mo   ( f)Off - Mo Figure 4 : iCRM attention for Figure 3e under Task C Similarly , the main objective of iTRM is to develop the relationship between the tasks . 
Figure 5 shows the established relationship between the tasks , and we see that attention put more weight on sarcasm and offensive pair while less weight on humour and sarcasm . 
It is clear from the deÔ¨Ånition of sarcasm and humour ( c.f . 
Section 1 ) that both of them have a very different meaning when used in a sentence while the actual sentence looks similar . 
Hence sarcasm and humour is found not be helping each other . 
Figure 5 : iTRM attention for Figure 3e under Task C 9 Conclusion and Future Work In this paper , we have successfully established the concept of obtaining effective relationshipsbetween inter - tasks and between inter - classes for multi - modal affect analysis . 
We have proposed a deep attentive multi - task learning framework which helps to obtain very effective inter - tasks and interclasses relationship . 
To capture the interdependence , we have proposed two attention - like mechanisms viz . 
,Inter - task Relationship Module ( iTRM ) and Inter - class Relationship Module ( iCRM ) . 
The main motivation of iTRM is to learn the relationship between the tasks , i.e. which task helps the other tasks . 
In contrast , iCRM develops the relations between the classes of tasks . 
We have evaluated our proposed approach on a recently published Memotion dataset . 
Experimental results suggest the efÔ¨Åcacy of the proposed model over the existing state - of - the - art systems ( Baseline and SemEval 2020 winner ) . 
The evaluation shows that the proposed multi - task framework yields better performance over single - task learning . 
The dataset used for the experiments is relatively small for training an effective deep learning model and is heavily biased . 
Therefore , assembling a large , and more balance dataset with quality annotations is an important job . 
Moreover , the memes are a complicated form of data which includes both text and image that repeat over numerous memes ( meme templates ) . 
Hence quality representation of memes for affect analysis is challenging future work . 
Acknowledgement The research reported here is partially supported by SkyMap Global India Private Limited . 
Dushyant Singh Chauhan acknowledges the support of Prime Minister Research Fellowship ( PMRF ) , Govt . 
of India . 
Asif Ekbal acknowledges the Young Faculty Research Fellowship ( YFRF ) , supported by Visvesvaraya PhD scheme for Electronics and IT , Ministry of Electronics and Information Technology ( Meit/8Y ) , Government of India , being implemented by Digital India Corporation ( formerly Media Lab Asia).288 Abstract We propose Implicit Quote Extractor , an endto - end unsupervised extractive neural summarization model for conversational texts . 
When we reply to posts , quotes are used to highlight important part of texts . 
We aim to extract quoted sentences as summaries . 
Most replies do not explicitly include quotes , so it is difÔ¨Åcult to use quotes as supervision . 
However , even if it is not explicitly shown , replies always refer to certain parts of texts ; we call them implicit quotes . 
Implicit Quote Extractor aims to extract implicit quotes as summaries . 
The training task of the model is to predict whether a reply candidate is a true reply to a post . 
For prediction , the model has to choose a few sentences from the post . 
To predict accurately , the model learns to extract sentences that replies frequently refer to . 
We evaluate our model on two email datasets and one social media dataset , and conÔ¨Årm that our model is useful for extractive summarization . 
We further discuss two topics ; one is whether quote extraction is an important factor for summarization , and the other is whether our model can capture salient sentences that conventional methods can not . 
1 Introduction As the amount of information exchanged via online conversations is growing rapidly , automated summarization of conversations is in demand . 
Neuralnetwork - based models have achieved great performance on supervised summarization , but its application to unsupervised summarization is not sufÔ¨Åciently explored . 
Supervised summarization requires tens of thousands of human - annotated summaries . 
Because it is not realistic to prepare such large datasets for every domain , there is a growing requirement for unsupervised methods . 
Previous research proposed diverse methods of unsupervised summarization . 
Graph - centrality Figure 1 : Example of a post and a reply with a quote and a reply with no quote . 
Implicit quote is the part of post that reply refers to , but not explicitly shown in the reply . 
based on the similarity of sentences ( Mihalcea and Tarau , 2004 ; Erkan and Radev , 2004 ; Zheng and Lapata , 2019 ) has long been a strong feature for unsupervised summarization , and is also used to summarize conversations ( Mehdad et al . 
, 2014 ; Shang et al . 
, 2018 ) . 
Apart from centrality , centroid of vectors ( Gholipour Ghalandari , 2017 ) , KullbackLeibler divergence ( Haghighi and Vanderwende , 2009 ) , reconstruction loss ( He et al . 
, 2012 ; Liu et al . 
, 2015 ; Ma et al . 
, 2016 ) , and path scores of word graphs ( Mehdad et al . 
, 2014 ; Shang et al . 
, 2018 ) , are leveraged for summarization . 
The premise of these methods is that important topics appear frequently in a document . 
Therefore , if important topics appear only a few times , these methods fail to capture salient sentences . 
For more accurate summarization , relying solely on the frequency is not sufÔ¨Åcient and we need to focus on other aspects of texts . 
As an alternative aspect , we propose ‚Äú the probability of being quoted ‚Äù . 
When one replies to an email or a post , a quote is used to highlight the291 important parts of the text ; an example is shown in Figure 1 . 
The reply on the bottom includes a quote , which generally starts with a symbol ‚Äú > ‚Äù . 
If we can predict quoted parts , we can extract important sentences irrespective of how frequently the same topic appears in the text . 
Thus , we aim to extract quotes as summaries . 
Previous research assigned weights to words that appear in quotes , and improved the centroidbased summarization ( Carenini et al . 
, 2007 ; Oya and Carenini , 2014 ) . 
However , most replies do not include quotes , so it is difÔ¨Åcult to use quotes as the training labels of neural models . 
We propose a model that can be trained without explicit labels of quotes . 
The model is Implicit Quote Extractor ( IQE ) . 
As shown in Figure 1 , implicit quotes are sentences of posts that are not explicitly quoted in replies , but are those the replies most likely refer to . 
The aim of our model is to extract these implicit quotes for extractive summarization . 
We use pairs of a post and reply candidate to train the model . 
The training task of the model is to predict if a reply candidate is an actual reply to the post . 
IQE extracts a few sentences of the post as a feature for prediction . 
To predict accurately , IQE has to extract sentences that replies frequently refer to . 
Summaries should not depend on replies , so IQE does not use reply features to extract sentences . 
The model requires replies only during the training and not during the evaluation . 
We evaluate our model with two datasets of Enron mail ( Loza et al . 
, 2014 ) , corporate and private mails , and verify that our model outperforms baseline models . 
We also evaluated our model with Reddit TIFU dataset ( Kim et al . 
, 2019 ) and achieved results competitive with those of the baseline models . 
Our model is based on a hypothesis that the ability of extracting quotes leads to a good result . 
Using the Reddit dataset where quotes are abundant , we obtain results that supports the hypothesis . 
Furthermore , we both quantitatively and qualitatively analyzed that our model can capture salient sentences that conventional frequency - based methods can not . 
The contributions of our research are as follows : ‚Ä¢We veriÔ¨Åed that ‚Äú the possibility of being quoted ‚Äù is useful for summarization , and demonstrated that it reÔ¨Çects an important aspect of saliency that conventional methods do not.‚Ä¢We proposed an unsupervised extractive neural summarization model , Implicit Quote Extractor ( IQE ) , and demonstrated that the model outperformed or achieved results competitive to baseline models on two mail datasets and a Reddit dataset . 
‚Ä¢Using the Reddit dataset , we veriÔ¨Åed that quote extraction leads to a high performance of summarization . 
2 Related Works Summarization methods can be roughly grouped into two methods : extractive summarization and abstractive summarization . 
Most unsupervised summarization methods proposed are extractive methods . 
Despite the rise of neural networks , conventional non - neural methods are still powerful in the Ô¨Åeld of unsupervised extractive summarization . 
The graph - centrality - based method ( Mihalcea and Tarau , 2004 ; Erkan and Radev , 2004 ; Zheng and Lapata , 2019 ) and centroid - based method ( Gholipour Ghalandari , 2017 ) have been major methods in this Ô¨Åeld . 
Other models use reconstruction loss ( He et al . 
, 2012 ; Liu et al . 
, 2015 ; Ma et al . 
, 2016 ) , Kullback - Leibler divergence ( Haghighi and Vanderwende , 2009 ) or path score calculation ( Mehdad et al . 
, 2014 ; Shang et al . 
, 2018 ) based on multi - sentence compression algorithm ( Filippova , 2010 ) . 
These methods assume that important topics appear frequently in a document , but our model focuses on a different aspect of texts : the probability of being quoted . 
That is , our model can extract salient sentences that conventional methods fail to . 
A few neural - network - based unsupervised extractive summarization methods were proposed ( KÀöageb ¬®ack et al . 
, 2014 ; Yin and Pei , 2015 ; Ma et al . 
, 2016 ) . 
However , these methods use pretrained neural network models as a feature extractor , whereas we propose an end - to - end neural extractive summarization model . 
As for end - to - end unsupervised neural models , a few abstractive models have been proposed . 
For sentence compression , Fevry and Phang ( 2018 ) employed the task to reorder the shufÔ¨Çed word order of sentences . 
Baziotis et al . 
( 2019 ) employed the reconstruction task of the original sentence from a compressed one . 
For review abstractive summarization , Isonuma et al . 
( 2019 ) revealed parent nodes of tree structures induce summaries , Chu and Liu292 ÊöóÈªôÁöÑ„Å™ÂºïÁî®„ÅÆÂ≠¶ÁøíÔºöÊñáÊäΩÂá∫„Çí‰Ωø„ÅÜÊâãÊ≥ï ! Post ‚Ä¶ Reply CandidateCoattentionMatrixSplit to sentencesDecomposableAttentionPredict if Reply Candidate is an actual replyBiLSTMBiLSTMBiLSTMEncoderLSTMExtractor Extracted SentencesPredictor ( only used during training ) ! " ! ! # ! ! $ ! ! " % & ' ! # % & ' ! ( % & ' ‚Ä¶ ! " ) ! # ) ! * ) ‚Ä¶ ‚Ä¶ ‚Ä¶ " " ! Split to sentencesAttention & Gumbel Softmax#"%&'‚âì!+!#,%&'#(%&'‚âì!-!"#!"$!"")"#)"*)BiLSTMBiLSTMBiLSTM ‚Ä¶ Figure 2 : Description of our model , Implicit Quote Extractor ( IQE ) . 
The Extractor extracts sentences and uses them as summaries . 
kandjare indices of the extracted sentences . 
( 2019 ) generated summaries from mean vectors of review vectors , and Amplayo and Lapata ( 2020 ) employed the prior distribution of Variational AutoEncoder to induce summaries . 
Another research employed a task to reconstruct masked sentences for summarization ( Laban et al . 
, 2020 ) . 
Research on the summarization of online conversations such as mail , chat , social media , and online discussion fora has been conducted for a long time . 
Despite the rise of neural summarization models , most research on conversation summarization is based on non - neural models . 
A few used path scores of word graphs ( Mehdad et al . 
, 2014 ; Shang et al . 
, 2018 ) . 
Dialogue act classiÔ¨Åcation is a classiÔ¨Åcation task that classiÔ¨Åes sentences depending on what their functions are ( e.g. : questions , answers , greetings ) , and has also been applied for summarization ( Bhatia et al . 
, 2014 ; Oya and Carenini , 2014 ) . 
Quotes are also important factors of summarization . 
When we reply to a post or an email and when we want to emphasize a certain part of it , we quote the original text . 
A few studies used these quotes as features for summarization . 
Some previous work ( Carenini et al . 
, 2007 ; Oya and Carenini , 2014 ) assigned weights to words that appeared in quotes , and improved the conventional centroidbased methods . 
The previous research used quotes as auxiliary features . 
In our research , we solely focus on quotes , and do not directly use quotes as supervision ; rather , we aim to extract implicit quotes.3 Model We propose Implicit Quote Extractor ( IQE ) , an unsupervised extractive summarization model . 
Figure 2 shows the structure of the model . 
The inputs to the model during training are a post and reply candidate . 
A reply candidate can be either a true or a false reply to the post . 
The training task of the model is to predict whether a reply candidate is true or not . 
The model comprises an Encoder , an Extractor , and a Predictor . 
The Encoder computes features of posts , the Extractor extracts sentences of a post to use for prediction , and the Predictor predicts whether a reply candidate is an actual reply or not . 
We describe each component below . 
Encoder The Encoder computes features of posts . 
First , the post is split into Nsentences { sp 1,sp 2, ... ,sp N } . 
Each sentence sp icomprises KiwordsWp i={wp i1,wp i2, ... ,wp iK i } . 
Words are embedded to continuous vectors Xp i= { xp i1,xp i2, ... ,xp iK i}through word embedding layers . 
We compute the features of each sentence hp iby inputting embedded vectors to Bidirectional Long Short - Term Memory ( BiLSTM ) and concatenating the last two hidden layers : hp i = BiLSTM ( Xp i ) ( 1 ) Extractor The Extractor extracts a few sentences of a post for prediction . 
For accurate prediction , the Extractor learns to extract sentences that replies frequently refer to . 
Note that the Extractor does not use reply features for extraction . 
This is because293 summaries should not depend on replies . 
IQE requires replies only during the training and can induce summaries without replies during the evaluation . 
We employ LSTM to sequentially compute features on the Extractor . 
We set the mean vector of the sentence features of the Encoder hp ias the initial hidden state of the Extractor hext 0 . 
hext 0=1 NN / summationdisplay i=1hp i ( 2 ) The Extractor computes attention weights using the hidden states of the Extractor hext tand the sentence featureshp icomputed on the Encoder . 
The sentence with the highest attention weight is extracted . 
During the training , we use Gumbel Softmax ( Jang et al . 
, 2017 ) to make this discrete process differentiable . 
By adding Gumbel noise gusing noise u from a uniform distribution , the attention weights a become a one - hot vector . 
The discretized attention weightsŒ±are computed as follows : ui‚àºUniform ( 0,1 ) ( 3 ) gi=‚àílog ( ‚àílogui ) ( 4 ) ati = cTtanh(hext t+hp i ) ( 5 ) œÄti = expati / summationtextN k=1expatk(6 ) Œ±ti = exp ( logœÄti+gi)/œÑ / summationtextN k=1exp ( logœÄtk+gk)/œÑ(7 ) cis a parameter vector , and the temperature œÑis set to 0.1 . 
We input the linear sum of the attention weightsŒ±and the sentence vectors hp ito LSTM and update the hidden state of the Extractor . 
We repeat this step Ltimes . 
xext t = N / summationdisplay i=1Œ±tihp i ( 1‚â§t‚â§L)(8 ) hext t+1 = LSTM ( xext t ) ( 0‚â§t‚â§L‚àí1 ) ( 9 ) The initial input vector xext 0of the Extractor is a parameter , and Lis deÔ¨Åned by a user depending on the number of sentences required for a summary . 
Predictor Then , using only the extracted sentences and a reply candidate , the Predictor predicts whether the candidate is an actual reply or not . 
We labeled actual replies as positive , and randomly sampled posts as negative . 
Suppose a reply candidateR={sr 1,sr 2, ... ,sr M}hasMsentences . 
Sentence vectors{hr j}of each sentence{sr j}on the reply are computed similarly to the equation 1 . 
To compute the relation between the post and the reply candidate , we employ Decomposable Attention ( Parikh et al . 
, 2016 ) . 
From this architecture , we obtain the probability of binary - classiÔ¨Åcation ythrough the sigmoid function . 
y = sigmoid ( DA(xext 1, ... ,xext L‚àí1,hr 1, ... ,hr M ) ) ( 10 ) where DA denotes Decomposable Attention . 
The detail of the computation is described in Appendix A.1 . 
Decomposable Attention . 
The loss of this classiÔ¨Åcation Lrepis obtained by cross entropy as follows where trepis 1 when a reply candidate is an actual reply , and otherwise 0 . 
Lrep=‚àítreplogy‚àí(1‚àítrep ) log ( 1‚àíy ) ( 11 ) Reranking As we mentioned in the Introduction , we are seeking for a criterion that is different from conventional methods . 
To take advantage of our method and conventional methods , we employ reranking ; we simply reorder summaries ( 3 sentences ) extracted by our model based on the ranking of TextRank ( Mihalcea and Tarau , 2004 ) . 
4 Experiment We train and evaluate the model on two domains of datasets . 
One is a mail dataset , and the other is a dataset from the social media platform , Reddit . 
4.1 Mail Dataset We use Avocado collection1for the training . 
The Avocado collection is a public dataset that comprises emails obtained from 279 custodians of a defunct information technology company . 
From this dataset , we use post - and - reply pairs to train our model . 
We exclude pairs where the number of words in a post or a reply is smaller than 50 or 25 . 
After the preprocessing , we have 56,174 pairs . 
We labeled a pair with an actual reply as positive and a pair with a wrong reply that is randomly sampled from the whole dataset as negative . 
The number of positive labels and negative labels are equal . 
Therefore , we have 112,348 pairs in total . 
For evaluation , we employ the Enron Summarization dataset ( Loza et al . 
, 2014 ) . 
This dataset 1https://catalog.ldc.upenn.edu/LDC2015T03294 DataSample sizeSummary Source # of references # of sentences # of words # of sentences # of words # of words per sentence ECS 109 2 4.7 78.0 11.0 179.4 16.3 EPS 103 2 5.8 88.0 19.3 217.1 11.2 tldr 3000 1 1.3 19.7 15.1 311.9 20.7 Table 1 : Overview of the evaluation datasets . 
has two types of evaluation datasets : ECS ( Enron Corporate Single ) and EPS ( Enron Personal Single ) . 
An overview of these datasets is summarized in Table 1 . 
Because the evaluation datasets do not have validation datasets , we use the ECS dataset as a validation dataset for the EPS dataset , and vice versa . 
We use the validation datasets to decide which model to use for the evaluation . 
4.2 Reddit TIFU Dataset The Reddit TIFU dataset ( Kim et al . 
, 2019 ) is a dataset that leverages tldr tags for the summarization task , which is the abbreviation of ‚Äú too long did n‚Äôt read ‚Äù . 
On the discussion forum Reddit TIFU , users post a tldr along with the post . 
tldr brieÔ¨Çy explains what is written in the original post and thus can be regarded as a summary . 
We preprocess the TIFU dataset similarly as the mail datasets . 
Because the TIFU dataset does not include replies , we collected replies of the posts included in the TIFU dataset using praw2 . 
As a consequence , we obtained 183,500 correct pairs of posts and replies and the same number of wrong pairs . 
We use that 367,000 pairs of posts and replies as the training dataset . 
We use 3,000 posts and tldrs that are not included in the training dataset as the validation dataset , and the same number of posts and tldrs as the evaluation dataset . 
An overview of the TIFU evaluation dataset is also summarized in Table 1 . 
4.3 Training The dimensions of the embedding layers and hidden layers of the LSTM are 100 . 
The size of the vocabulary is set to 30,000 . 
We tokenize each email or post into sentences and each sentence into words using the nltk tokenizer3 . 
The upper limit of the number of sentences is set to 30 , and that of words in each sentence is set to 200 . 
The epoch size is 10 , and we use Adam ( Kingma and Ba , 2015 ) as an optimizer . 
In the Ô¨Årst few epochs , we do not use the Extractor ; all the post sentences are used for the prediction 2https://praw.readthedocs.io/ 3https://www.nltk.orgof post - reply relations . 
This is to train the Extractor and the Predictor efÔ¨Åciently . 
The Extractor learns to extract proper sentences and the Predictor learns to predict the relation between a post and a reply candidate . 
Models with several components generally achieve better results if each component is pretrained separately ( Hashimoto et al . 
, 2017 ) . 
Thus , we train the Predictor in the Ô¨Årst few epochs before training the Extractor . 
We set this threshold as 4 . 
During training , L , the number of sentences the Extractor extracts is randomly set from 1 to 4 , so that the model can extract an arbitrary number of sentences . 
We replace the named entities on the text data with tags ( person , location , and organization ) using the Stanford Named Entity Recognizer ( NER)4 , to prevent the model from simply using named entities as a hint for the prediction . 
We pretrain word embeddings of the model with Skipgram , using the same data as the training . 
We conduct the same experiment Ô¨Åve times and use the average of the results to mitigate the effect of randomness rooting in initialization and optimization . 
4.4 Evaluation In the evaluation phase , we only use the Encoder and Extractor and do not use the Predictor . 
Each model extracts 3 sentences as a summary . 
Following previous work , we report the average F1 of ROUGE-1 , ROUGE-2 , and ROUGE - L for the evaluation ( Lin , 2004 ) . 
We use the Ô¨Årst 20 , 40 , and 60 words of the extracted sentences . 
For ROUGE computation , we use ROUGE 2.0 ( Ganesan , 2015 ) . 
As a validation metric , we use an average of ROUGE1 - F , ROUGE-2 - F , and ROUGE - L - F. 4.5 Baseline As baseline models , we employ TextRank ( Mihalcea and Tarau , 2004 ) , LexRank ( Erkan and Radev , 2004 ) , KLSum ( Haghighi and Vanderwende , 2009 ) , PacSum ( Zheng and Lapata , 2019 ) , Lead , and Random . 
TextRank and LexRank are graph - centrality based methods that have long been considered as 4https://nlp.stanford.edu/software/CRF-NER.shtml295 ModelROUGE-1 - F ROUGE-2 - F ROUGE - L - F # of words # of words # of words 20 40 60 20 40 60 20 40 60 Lead 0.217 0.351 0.413 0.115 0.198 0.240 0.212 0.290 0.321 TextRank 0.231 0.365 0.434 0.123 0.199 0.243 0.223 0.294 0.336 LexRank 0.234 0.359 0.423 0.127 0.199 0.240 0.220 0.290 0.323 Random 0.193 0.317 0.365 0.089 0.163 0.190 0.199 0.285 0.303 KLSum 0.235 0.344 0.383 0.125 0.183 0.204 0.220 0.273 0.303 PacSum 0.230 0.367 0.435 0.125 0.211 0.256 0.220 0.287 0.326 IQETextRank 0.213 0.336 0.394 0.104 0.172 0.208 0.211 0.287 0.315 IQE 0.241 0.374 0.445 0.130 0.206 0.251 0.220 0.292 0.333 IQE + reranking 0.242 0.374 0.443 0.131 0.207 0.246 0.227 0.298 0.332 Table 2 : Results on ECS data . 
The best results are bolded and the second best results are underlined . 
ModelROUGE-1 - F ROUGE-2 - F ROUGE - L - F # of words # of words # of words 20 40 60 20 40 60 20 40 60 Lead 0.128 0.204 0.230 0.045 0.084 0.099 0.150 0.208 0.221 TextRank 0.172 0.272 0.317 0.080 0.129 0.151 0.185 0.260 0.290 LexRank 0.161 0.254 0.299 0.068 0.113 0.136 0.173 0.245 0.275 Random 0.144 0.213 0.238 0.058 0.086 0.099 0.158 0.213 0.232 KLSum 0.191 0.287 0.321 0.093 0.141 0.153 0.184 0.254 0.277 PacSum 0.179 0.275 0.330 0.082 0.127 0.151 0.171 0.250 0.287 IQETextRank 0.158 0.252 0.291 0.069 0.115 0.136 0.169 0.242 0.268 IQE 0.189 0.292 0.342 0.091 0.143 0.168 0.189 0.268 0.302 IQE + reranking 0.185 0.290 0.340 0.087 0.138 0.164 0.189 0.264 0.299 Table 3 : Results on EPS data . 
The best results are bolded and the second best results are underlined . 
strong methods for unsupervised summarization . 
PacSum is an improved model of TextRank , which harnesses the position of sentences as a feature . 
KLSum employs the Kullbuck ‚Äì Leibler divergence to constrain extracted sentences and the source text to have the similar word distribution . 
Lead is a simple method that extracts the Ô¨Årst few sentences from the source text but is considered as a strong baseline for the summarization of news articles . 
PacSum and LexRank leverage idf . 
We compute idf using the validation data . 
As another baseline , we employ IQETextRank ; the TextRank model that leverages cosine similarities of sentence vectors of IQE ‚Äôs Encoder as similarities between sentences . 
This is added to verify that the success of our model is not only because our model uses neural networks . 
5 Results and Discussion Experimental results for each evaluation dataset are listed in Table 2 , 3 and 4 . 
Our model outperforms baseline models on the mail datasets ( ECS and EPS ) in most metrics . 
On Reddit TIFU dataset , IQE with reranking outperforms most baseline models except TextRank . 
Reranking improves the accuracy on ECS and TIFU but not on EPS . 
PacSum signiÔ¨Åcantly outperformed TextRank onthe news article dataset ( Zheng and Lapata , 2019 ) but does not work well on our datasets where the sentence position is not an important factor . 
IQETextRank performed worse than IQE with the mail datasets . 
This indicates that the performance of our model does not result from the use of neural networks . 
Our model outperforms the baseline models more with the EPS dataset than the ECS dataset . 
The overview of the datasets in Table 1 explains the reason . 
The average number of words each sentence has is smaller in EPS . 
Baseline models such as LexRank and TextRank compute similarity of sentences using the co - occurrence of words . 
Thus , if the lengths of sentences are short , it fails to build decent co - occurrence networks and to capture the saliency of the sentences . 
IQE did not outperform TextRank on TIFU dataset . 
It is conceivable that Reddit users are less likely to refer to important topics on the post , given that anyone can reply . 
5.1 The Performance of Summarization and Quote Extraction Our model performed well on the Mail datasets but two questions remain unclear . 
First , because we did not use quotes as supervision , it is not clear how well our model extracts quotes . 
Second , following296 ModelROUGE-1 - F ROUGE-2 - F ROUGE - L - F # of words # of words # of words 20 40 60 20 40 60 20 40 60 Lead 0.128 0.150 0.149 0.017 0.023 0.024 0.107 0.122 0.125 TextRank 0.161 0.179 0.173 0.027 0.034 0.035 0.126 0.140 0.142 LexRank 0.149 0.165 0.163 0.021 0.026 0.029 0.119 0.131 0.134 Random 0.136 0.156 0.158 0.018 0.024 0.026 0.112 0.128 0.131 KL - Sum 0.142 0.159 0.157 0.020 0.026 0.029 0.115 0.127 0.131 PacSum 0.143 0.161 0.161 0.021 0.026 0.028 0.117 0.132 0.135 IQETextRank 0.152 0.169 0.166 0.023 0.030 0.032 0.122 0.136 0.139 IQE 0.153 0.172 0.169 0.024 0.031 0.033 0.122 0.136 0.139 IQE + reranking 0.161 0.177 0.171 0.026 0.033 0.034 0.126 0.138 0.139 Table 4 : Results on TIFU tldr data . 
The best results are bolded and the second best results are underlined . 
Model MRR LexRank 0.094 TextRank 0.109 Random 0.081 IQE 0.135 Table 5 : Ability of extracting quotes . 
Model ROUGE-1 - F ROUGE-2 - F ROUGE - L - F IQEquote 0.184 0.030 0.126 IQEnonquote 0.168 0.020 0.118 Table 6 : ROUGE scores of extracted sentences that coincide with quote ( IQEquote ) and that does not coincide with quotes ( IQEnonquote ) . 
The ROUGE scores become higher when IQE succeeded in extracting quotes . 
Carenini ‚Äôs work ( Carenini et al . 
, 2007 ; Oya and Carenini , 2014 ) , we assumed quotes were useful for summarization but it is not clear whether the quote extraction leads to better results of summarization . 
To answer these questions , we conduct two experiments . 
For the experiments , we use the Reddit TIFU dataset and replies extracted via praw as described in 4.2 . 
From the dataset , we extract replies that contain quotes , which start with the symbol ‚Äú > ‚Äù . 
In total , 1,969 posts have replies that include quotes . 
We label sentences of the posts that are quoted by the replies and verify how accurately our model can extract the quoted sentences . 
How well our model extracts quotes ? To assess the ability of quote extraction , we regard the extraction of quotes as an information retrieval task and evaluate with Mean Reciprocal Rank ( MRR ) . 
We compute MRR as follows . 
MRR = /braceleftbigg1 R(q)(R(q)‚â§4 ) 0 ( R(q)>4)(12 ) The function Rdenotes the rank of the saliency scores a model computes ; our model does not compute the scores but sequentially extracts sentences , and the order is regarded as the rank here . 
If a model extracts quotes as salient sentences , the rank becomes higher . 
Therefore , the MRR in our study indicates the capability of a model to extract quotes . 
As explained in the section 4.3 , we trained our model to extract up to four sentences . 
Thus we setthe threshold at four ; if R(q)is larger than 4 we set MRR 0 . 
For each data , we compute MRR and use the mean value as a result . 
Table 5 shows the results . 
IQE is more likely to extract quotes than TextRank , LexRank and Random . 
Does extracting quotes lead to good summarization ? Next , we validate whether the ROUGE scores become better when our model succeeded in extracting quotes . 
We compute ROUGE scores when our model succeeds or fails in quote extraction ( which means when MRR equals 1 or otherwise ) . 
IQEquote indicates the data where the extracted sentence coincides with a quote , and IQEnonquote vice versa . 
The result in the Table 6 shows ROUGE scores are higher when the extracted sentence coincides with a quote . 
The results of the two analyses support the claim that our model is more likely to extract quotes and that the ability of extracting quotes leads to better summarization . 
5.2 Ablation Tests Effect of replacing named entities As explained in the section 4.3 , our models shown in Tables 2 , 3 and 4 all use the Stanford NER . 
To validate the effect of NER , we experiment without replacing named entities . 
Table 7 lists the results . 
The table indicates that replacing named entities improves the performance on the mail datasets . 
This is because names of people , locations , and organizations can be signiÔ¨Åcant hints for distinguishing297 Dataset ModelROUGE-1 - F ROUGE-2 - F ROUGE - L - F # of words # of words # of words 20 40 60 20 40 60 20 40 60 ECSIQE 0.241 0.374 0.445 0.130 0.206 0.251 0.220 0.292 0.333 IQE w/o NER 0.215 0.351 0.424 0.110 0.189 0.237 0.208 0.290 0.329 IQE w/o Pretraining 0.223 0.355 0.420 0.113 0.190 0.231 0.210 0.288 0.323 EPSIQE 0.189 0.292 0.342 0.091 0.143 0.168 0.189 0.268 0.302 IQE w/o NER 0.170 0.271 0.312 0.076 0.127 0.149 0.188 0.268 0.295 IQE w/o Pretraining 0.176 0.274 0.318 0.078 0.124 0.147 0.186 0.260 0.291 TIFUIQE 0.153 0.172 0.169 0.024 0.031 0.033 0.122 0.136 0.139 IQE w/o NER 0.154 0.172 0.170 0.024 0.030 0.033 0.122 0.136 0.139 IQE w/o Pretraining 0.143 0.161 0.160 0.020 0.027 0.029 0.116 0.131 0.133 Table 7 : Results of ablation tests correct replies . 
For example , if a post and a reply candidate refer to the same person ‚Äôs name , the model extracts sentences that contain the person ‚Äôs name . 
The replacement of named entities encourages the model to extract sentences semantically relevant to replies rather than simply extracting sentences that include named entities . 
However , on the Reddit TIFU dataset , NER did not affect the accuracy . 
Reddit is an anonymized social media platform , and the posts are less likely to refer to people ‚Äôs names . 
Thus , named entities will not be hints to predict reply - relation . 
Effect of pretraining Predictor As explained in the section 4.3 , we pretrained the Predictor in the Ô¨Årst few epochs so that the model can learn the extraction and the prediction separately . 
Table 7 shows the effect of pretraining . 
Without pretraining , the accuracy decreased . 
This shows the importance of the separate training of each component . 
5.3 Difference from Conventional Methods As explained in the Introduction , most conventional unsupervised summarization methods are based on the assumption that important topics appear frequently in a document . 
TextRank is a typical example ; TextRank is a centrality - based method that extracts sentences with high PageRank as the summary . 
A sentence having high PageRank indicates that the sentence has high similarity with many other sentences , meaning that many sentences refer to the same topic . 
We suspected that important topics are not always referred to frequently , and suggested another criterion : the frequency of being referred to in replies . 
Comparing with TextRank , we verify that our method can capture salient sentences that the centrality - based method fails to . 
Figure 3 shows the correlation between the maximum PageRank in each post of ECS / EPS and ROUGE-1 - F scores Figure 3 : Correlation between ROUGE-1 - F score and maximum PageRank of each post on ECS and EPS datasets . 
X - axis shows rounded maximum PageRank , and Y - axis shows ROUGE-1 - F and the error bar represents the standard error . 
of IQE and TextRank . 
As shown in the Figure , the ROUGE-1 - F scores of our model are higher than those of TextRank when the maximum PageRank in the sentence - similarity graph is low . 
This supports our hypothesis that our model can capture salient sentences even when the important topic is referred to only few times . 
Table 8 shows a demonstrative example of extracted summaries of IQE and TextRank . 
The sample is from the EPS dataset . 
The summary includes descriptions regarding a promotion and that the sender is having a baby . 
However , those words298 Source Text Just got your email address from Rachel . 
Congrats on your promotion . 
I ‚Äôm sure it ‚Äôs going to be alot different for you but it sounds like a great deal . 
My hubby and ‚Äô I moved out to Katy a few months ago . 
I love it there - my parents live about 10 minutes away . 
New news from me - I ‚Äôm having a baby - due in June . 
I ca n‚Äôt even believe it myself . 
The thought of me being a mother is downright scary but I Ô¨Ågure since I ‚Äôm almost 30 , I probably need to start growing up . 
I ‚Äôm really excited though . 
Rachel is coming to visit me in a couple of weeks . 
You planning on coming in for any of the rodeo stuff ? You ‚Äôll never guess who I got in touch with about a month ago . 
It was the weirdest thing - heather evans . 
I had n‚Äôt talked to her in about 10 years . 
Seems like she ‚Äôs doing well but I can never really tell with her . 
Anyway , I ‚Äôll let you go . 
Got ta get back to work . 
Looking forward to hearing back from ya . 
Summary ( Gold ) The sender wants to congratulate the recipient for his / her new promotion , as well as , updating him / her about her life . 
The sender just move out to Katy few months ago . 
She is having a baby due in June . 
She is scared of being a mother but also pretty exited about it . 
Rachel is coming to visit her in couple of weeks and she is asking if he / she will join for any of the rodeo stuff . 
She run into heather evans which she had n‚Äôt talked in 10 years . 
Table 8 : Example of sentences extracted by Implicit Quote Extractor ( IQE ) ( bold ) andTextRank ( italic ) . 
appear only once in the source text ; thus TextRank fails to capture the salient sentences . 
Our model , by contrast , can capture them because they are topics that replies often refer to . 
6 Conclusion This paper proposes Implicit Quote Extractor , a model that extracts implicit quotes as summaries . 
We evaluated our model with two mail datasets , ECS and EPS , and one social media dataset TIFU , using ROUGE as an evaluation metric , and validated that our model is useful for summarization . 
We hypothesized that our model is more likely to extract quotes and that ability improved the performance of our model . 
We veriÔ¨Åed these hypotheses with the Reddit TIFU dataset , but not with the email datasets , because few emails included annotated summaries , and those emails did not have replies with quotes . 
For future work , we will examine whether our hypotheses are valid for emails and other datasets . 
References Reinald Kim Amplayo and Mirella Lapata . 
2020 . 
Unsupervised opinion summarization with noising and denoising . 
In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 1934‚Äì1945 , Online . 
Association for Computational Linguistics . 
Christos Baziotis , Ion Androutsopoulos , Ioannis Konstas , and Alexandros Potamianos . 
2019 . 
SEQÀÜ3 : Differentiable sequence - to - sequence - to - sequence autoencoder for unsupervised abstractive sentence compression . 
In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 673‚Äì681 . 
Association for Computational Linguistics . 
Sumit Bhatia , Prakhar Biyani , and Prasenjit Mitra . 
2014 . 
Summarizing online forum discussions ‚Äì can dialog acts of individual messages help ? In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 2127‚Äì2131 . 
Association for Computational Linguistics . 
Giuseppe Carenini , Raymond T. Ng , and Xiaodong Zhou . 
2007 . 
Summarizing email conversations with clue words . 
In Proceedings of the 16th International Conference on World Wide Web , WWW ‚Äô 07 , pages 91‚Äì100 . 
ACM . 
Eric Chu and Peter J. Liu . 
2019 . 
Meansum : A neural model for unsupervised multi - document abstractive summarization . 
In Proceedings of the 36th International Conference on Machine Learning , ICML 2019 , 9 - 15 June 2019 , Long Beach , California , USA , pages 1223‚Äì1232 . 
G¬®unes Erkan and Dragomir R. Radev . 
2004 . 
Lexrank : Graph - based lexical centrality as salience in text summarization . 
J. Artif . 
Int . 
Res . 
, 22(1):457‚Äì479 . 
Thibault Fevry and Jason Phang . 
2018 . 
Unsupervised sentence compression using denoising autoencoders . 
In Proceedings of the 22nd Conference on Computational Natural Language Learning , pages 413‚Äì422 . 
Association for Computational Linguistics . 
Katja Filippova . 
2010 . 
Multi - sentence compression : Finding shortest paths in word graphs . 
In Proceedings of the 23rd International Conference on Computational Linguistics ( Coling 2010 ) , pages 322‚Äì330 , Beijing , China . 
Coling 2010 Organizing Committee . 
Kavita Ganesan . 
2015 . 
Rouge 2.0 : Updated and improved measures for evaluation of summarization tasks . 
Demian Gholipour Ghalandari . 
2017 . 
Revisiting the centroid - based method : A strong baseline for multi - document summarization . 
In Proceedings of the Workshop on New Frontiers in Summarization , 299 pages 85‚Äì90 . 
Association for Computational Linguistics . 
Aria Haghighi and Lucy Vanderwende . 
2009 . 
Exploring content models for multi - document summarization . 
In Proceedings of Human Language Technologies : The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics , pages 362‚Äì370 , Boulder , Colorado . 
Association for Computational Linguistics . 
Kazuma Hashimoto , Caiming Xiong , Yoshimasa Tsuruoka , and Richard Socher . 
2017 . 
A joint many - task model : Growing a neural network for multiple NLP tasks . 
In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 1923‚Äì1933 , Copenhagen , Denmark . 
Association for Computational Linguistics . 
Zhanying He , Chun Chen , Jiajun Bu , Can Wang , Lijun Zhang , Deng Cai , and Xiaofei He . 
2012 . 
Document summarization based on data reconstruction . 
InProceedings of the Twenty - Sixth AAAI Conference on ArtiÔ¨Åcial Intelligence , AAAI‚Äô12 , pages 620 ‚Äì 626 . 
AAAI Press . 
Masaru Isonuma , Junichiro Mori , and Ichiro Sakata . 
2019 . 
Unsupervised neural single - document summarization of reviews via learning latent discourse structure and its ranking . 
In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 2142‚Äì2152 . 
Association for Computational Linguistics . 
Eric Jang , Shixiang Gu , and Ben Poole . 
2017 . 
Categorical reparameterization with gumbel - softmax . 
In 5th International Conference on Learning Representations , ICLR 2017 , Toulon , France , April 24 - 26 , 2017 , Conference Track Proceedings . 
Mikael K Àöageb ¬®ack , Olof Mogren , Nina Tahmasebi , and Devdatt Dubhashi . 
2014 . 
Extractive summarization using continuous vector space models . 
In Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality ( CVSC ) , pages 31‚Äì39 . 
Association for Computational Linguistics . 
Byeongchang Kim , Hyunwoo Kim , and Gunhee Kim . 
2019 . 
Abstractive summarization of Reddit posts with multi - level memory networks . 
In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 2519‚Äì2531 . 
Association for Computational Linguistics . 
Diederik P. Kingma and Jimmy Ba . 
2015 . 
Adam : A method for stochastic optimization . 
In 3rd International Conference on Learning Representations , ICLR 2015 , San Diego , CA , USA , May 7 - 9 , 2015 , Conference Track Proceedings . 
Philippe Laban , Andrew Hsi , John Canny , and Marti A. Hearst . 
2020 . 
The summary loop : Learning to writeabstractive summaries without examples . 
In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 5135 ‚Äì 5150 , Online . 
Association for Computational Linguistics . 
Chin - Yew Lin . 
2004 . 
ROUGE : A package for automatic evaluation of summaries . 
In Text Summarization Branches Out , pages 74‚Äì81 . 
Association for Computational Linguistics . 
He Liu , Hongliang Yu , and Zhi - Hong Deng . 
2015 . 
Multi - document summarization based on two - level sparse representation model . 
In Proceedings of the Twenty - Ninth AAAI Conference on ArtiÔ¨Åcial Intelligence , AAAI‚Äô15 , pages 196‚Äì202 . 
AAAI Press . 
Vanessa Loza , Shibamouli Lahiri , Rada Mihalcea , and Po - Hsiang Lai . 
2014 . 
Building a dataset for summarization and keyword extraction from emails . 
In Proceedings of the Ninth International Conference on Language Resources and Evaluation ( LREC-2014 ) , pages 2441‚Äì2446 . 
European Languages Resources Association ( ELRA ) . 
Shulei Ma , Zhi - Hong Deng , and Yunlun Yang . 
2016 . 
An unsupervised multi - document summarization framework based on neural document model . 
In Proceedings of COLING 2016 , the 26th International Conference on Computational Linguistics : Technical Papers , pages 1514‚Äì1523 . 
The COLING 2016 Organizing Committee . 
Yashar Mehdad , Giuseppe Carenini , and Raymond T. Ng . 
2014 . 
Abstractive summarization of spoken and written conversations based on phrasal queries . 
InProceedings of the 52nd Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1220‚Äì1230 . 
Association for Computational Linguistics . 
Rada Mihalcea and Paul Tarau . 
2004 . 
TextRank : Bringing order into text . 
In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing , pages 404‚Äì411 . 
Association for Computational Linguistics . 
Tatsuro Oya and Giuseppe Carenini . 
2014 . 
Extractive summarization and dialogue act modeling on email threads : An integrated probabilistic approach . 
InProceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue ( SIGDIAL ) , pages 133‚Äì140 . 
Association for Computational Linguistics . 
Ankur Parikh , Oscar T ¬®ackstr ¬®om , Dipanjan Das , and Jakob Uszkoreit . 
2016 . 
A decomposable attention model for natural language inference . 
In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 2249‚Äì2255 . 
Association for Computational Linguistics . 
Guokan Shang , Wensi Ding , Zekun Zhang , Antoine Tixier , Polykarpos Meladianos , Michalis Vazirgiannis , and Jean - Pierre Lorr ¬¥ e. 2018 . 
Unsupervised abstractive meeting summarization with multisentence compression and budgeted submodular300 maximization . 
In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 664‚Äì674 . 
Association for Computational Linguistics . 
Wenpeng Yin and Yulong Pei . 
2015 . 
Optimizing sentence modeling and selection for document summarization . 
In Proceedings of the 24th International Conference on ArtiÔ¨Åcial Intelligence , IJCAI‚Äô15 , pages 1383‚Äì1389 . 
AAAI Press . 
Hao Zheng and Mirella Lapata . 
2019 . 
Sentence centrality revisited for unsupervised summarization . 
In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 6236‚Äì6247 , Florence , Italy . 
Association for Computational Linguistics.301 A Appendices A.1 Decomposable Attention As explained in section 3 , the Predictor uses Decomposable Attention for prediction . 
Decomposable Attention computes a two - dimensional attention matrix , computed by two sets of vectors , and thus , captures detailed information useful for prediction . 
The computation uses the following equations : Etj= ( xext t)Thr j ( 13 ) Œ≤t = M / summationdisplay j=1exp ( Etj)/summationtextM k=1exp ( Etk)hr j ( 14 ) Œ±j = L / summationdisplay t=1exp ( Etj)/summationtextL k=1exp ( Ekj)xext t ( 15 ) The computation of xext tandhr jare explained in section 3 . 
First , we compute a co - attention matrix Eas in ( 13 ) . 
The weights of the co - attention matrix are normalized row - wise and column - wise in the equations ( 14 ) and ( 15 ) . 
Œ≤iis a linear sum of reply featureshr jthat is aligned to xext tand vice versa forŒ±j . 
v1,t = G([xext t;Œ≤t])v2,j = G([hr j;Œ±j])(16 ) v1 = L / summationdisplay t=1v1,tv2 = M / summationdisplay j=1v2,j(17 ) y = sigmoid ( H([v1;v2]))(18 ) Next , we separately compare the aligned phrases Œ≤tandxext t , Œ±jandhr j , using a function G.G denotes a feed - forward neural network , and [ ; ] denotes concatenation . 
Finally , we concatenate v1 andv2and obtain binary - classiÔ¨Åcation result y through a linear layer Hand the sigmoid function.302 Proceedings of the 1st Conference of the Asia - PaciÔ¨Åc Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing , pages 303‚Äì312 December 4 - 7 , 2020 . 
¬© 2020 Association for Computational Linguistics Unsupervised Aspect - Level Sentiment Controllable Style Transfer Mukuntha N S¬ß , Zishan Ahmad¬ß , Asif Ekbal , Pushpak Bhattacharyya Department of Computer Science and Engineering Indian Institute of Technology Patna Bihar , India { mukuntha.cs16,1821cs18,asif , pb } @iitp.ac.in Abstract Unsupervised style transfer in text has previously been explored through the sentiment transfer task . 
The task entails inverting the overall sentiment polarity in a given input sentence , while preserving its content . 
From the Aspect - Based Sentiment Analysis ( ABSA ) task , we know that multiple sentiment polarities can often be present together in a sentence with multiple aspects . 
In this paper , the task of aspect - level sentiment controllable style transfer is introduced , where each of the aspect - level sentiments can individually be controlled at the output . 
To achieve this goal , a BERT - based encoder - decoder architecture with saliency weighted polarity injection is proposed , with unsupervised training strategies , such as ABSA masked - languagemodelling . 
Through both automatic and manual evaluation , we show that the system is successful in controlling aspect - level sentiments . 
1 Introduction With a rapid increase in the quality of generated text , due to the rise of neural text generation models ( Kalchbrenner and Blunsom , 2013 ; Cho et al . 
, 2014 ; Sutskever et al . 
, 2014 ; Vaswani et al . 
, 2017 ) , controllable text generation is quickly becoming the next frontier in the Ô¨Åeld of text generation . 
Controllable text generation is the task of generating realistic sentences whose attributes can be controlled . 
The attributes to control can be : ( i ) . 
Stylistic : Like politeness , sentiment , formality etc , ( ii ) . 
Content : Like information , entities , keywords etc . 
or ( iii ) . 
Ordering : Like ordering of information , events , plots etc . 
Controlling sentence level polarity has been well explored as a style transfer task . 
Zhang et al . 
( 2018 ) used unsupervised machine translation techniques for polarity transfer in sentences . 
Yang et al . 
( 2018 ) ¬ß equal contribution The service   was speedy and the   salads   were great , but the chicken    was bland and stale . 
  The service   was slow , but the   salads   were great and the chicken    was tasty and fresh . 
Service - Positive   Salads - Positive   Chicken - Negative   Service - Negative   Salads - Positive   Chicken - Positive Query :   Service - Negative   Salads - Positive   Chicken - Positive Figure 1 : An example of the proposed aspect - level sentiment style transfer task used language models as discriminators to achieve style ( polarity ) transfer in sentences . 
Li et al . 
( 2018a ) proposed a simpler method where they deleted the attribute markers and devise a method to replace or generate the target attribute - key phrases in the sentence . 
In this paper we explore a more Ô¨Åne - grained style transfer task , where each aspect ‚Äôs polarities can be changed individually . 
Recent interest in Aspect - Based Sentiment Analysis ( ABSA ) ( Pontiki et al . 
, 2014 ) has shown that sentiment information can vary within a sentence , with differing sentiments expressed towards different aspect terms of target entities ( e.g. ‚Äò food ‚Äô , ‚Äò service ‚Äô in a restaurant domain ) . 
We introduce the task of aspect - level sentiment transfer - the task of rewriting sentences to transfer them from a given set of aspect - term polarities ( such as ‚Äò positive sentiment ‚Äô towards the service of a restaurant and a ‚Äò positive sentiment ‚Äô towards the taste of the food ) to a different set of aspect - term polarities ( such as ‚Äò negative sentiment ‚Äô towards the service of a restaurant and a ‚Äò positive ‚Äô sentiment towards the taste of the food ) . 
This is a more challenging task than regular style transfer as the style attributes here are not the overall attributes for the whole sentence , but are localized to speciÔ¨Åc parts of the sentence , and multiple opposing at-303 tributes could be present within the same sentence . 
The target of the transformation made needs to be localized and the other content expressed in the rest of the sentence need to be preserved at the output . 
An example of the task is shown in Figure 1 . 
For successful manipulation of the generated sentences , a few challenges need to be addressed : ( i ) . 
The model should learn to associate the right polarities with the right aspects . 
( ii ) . 
The model needs to be able to correctly process the aspectpolarity query and accordingly delete , replace and generate text sequence to satisfy the query . 
( iii ) . 
The polarities of the aspects not in the query should not be affected . 
( iv ) . 
The non - attribute content and Ô¨Çuency of the text should be preserved . 
We explore this task in an unsupervised setting ( as is common with most style - transfer tasks due to the lack of an aligned parallel corpus ) using only monolingual unaligned corpora . 
In this work , a novel encoder - decoder architecture is proposed to perform unsupervised aspect - level sentiment transfer . 
A BERT ( Devlin et al . 
, 2019 ) based encoder is used that is trained to understand aspect - speciÔ¨Åc polarity information . 
We also propose using a ‚Äò polarity injection ‚Äô method , where saliency - weighted aspect - speciÔ¨Åc polarity information is added to the hidden representations from the encoder to complete the query for the decoder . 
1.1 Motivation The Aspect - Based Sentiment Analysis ( ABSA ) task shows that differing sentiments can be present within the same sentence , localized to different entities or parts of the text . 
The notion of styles in natural language can be used to refer to the attributes , such as sentiment , formality in content , emotion , sarcasm , etc . 
Similar to the sentiment , these other attributes can also be present localized to different entities taking differing values at each location . 
If we consider the style ‚Äò emotion ‚Äô with the example ‚Äú Although Alice infuriates me with her prattle and Bob scares me , I am quite happy about how things are turning out . 
‚Äù - A single piece of text ( such as a single sentence ) can express an emotion , such as ‚Äò happiness ‚Äô about an event while expressing ‚Äò fear ‚Äô towards some entity and ‚Äò anger ‚Äô towards a second entity . 
This shows that style transfer in language needs a more nuanced understanding . 
Especially when generating larger pieces of text , multiple such styles could intermingle , and differing styles can often be present together when discussing differenttopics and entities . 
Our work intends to take the Ô¨Årst step towards a more controllable form of Ô¨Ånegrained style transfer with the task of aspect - level sentiment style transfer . 
2 Related Work In this section we present an overview of the related literature . 
2.1 Sentiment Transfer To the best of our knowledge , our current work is the Ô¨Årst to tackle aspect - level sentiment transfer . 
Most of the previous works involving sentiment transfer ( Li et al . 
, 2018b ; Yang et al . 
, 2018 ; Shen et al . 
, 2017 ; Xu et al . 
, 2018 ; Prabhumoye et al . 
, 2018 ; Wu et al . 
, 2019 ) consider the style that is present throughout the sentence and seek to transfer only the overall sentiment polarities expressed . 
Tian et al . 
( 2018 ) proposed a new training objective for content preservation during style transfer . 
They used Part - of - Speech ( PoS ) tagging to collect nouns at inputs , and expect them to be present at the output for content preservation . 
To achieve this , they proposed a PoS preservation constraint and ‚Äò Content Conditional Language Modelling ‚Äô . 
They tested their system on sentiment style transfer task . 
Wang et al . 
( 2019 ) proposed a method that can also control the degree of polarity transfer in a sentence with multiple aspect categories present in it . 
Unlike their task which deals with predeÔ¨Åned aspect categories , our task deals with opinion target expressions . 
Aspect categories are coarse entities that are few in number and predeÔ¨Åned for a certain domain , while aspect - terms or opinion target expressions are Ô¨Åne - grained entities that are present in the text . 
They also did not investigate selectively transferring the polarity over a subset of aspects with multiple differing polarities at the output and only invert the overall polarity expressed by the sentence . 
Our method works across thousands of unique opinion target expressions ( Table 1 shows the number unique target aspects present in each of our datasets).Our method also does not need these to be predeÔ¨Åned , and so could be used to control the polarities of previously unseen target expressions as well . 
2.2 Unsupervised Machine Translation Previous works in unsupervised neural machine translation ( Artetxe et al . 
, 2017 ) and unsupervised style transfer ( Zhang et al . 
, 2018 ) have shown that,304 with only monolingual data , using a denoising autoencoder loss and an on - the-Ô¨Çy back - translation loss can be very successful in achieving transfer . 
Both of these training steps are used as part of our method to train the network in an unsupervised fashion . 
2.3 Natural Language Generation Architecture Lai et al . 
( 2019 ) proposed an adversarial training mechanism for Gated Recurrent Unit ( GRU ) based encoder - decoder model for sentiment polarity transfer and multiple - attribute transfer tasks . 
They split the training mechanism of their model into two phases , viz.(i ) . 
Style transfer phase and ( ii ) . 
Reconstruction phase . 
Pryzant et al . 
( 2020 ) proposed a method to remove subjective bias in the sentences . 
They proposed adding a ‚Äò join - embedding ‚Äô weighted by a word subjective - bias probability to automatically edit the hidden states from the encoder . 
We adapt this ‚Äò join - embedding ‚Äô method to inject weighted polarities into our encoder outputs as described in Section 3.5 . 
2.4 Aspect Based Sentiment Analysis Aspect based sentiment analysis ( ABSA ) has been explored in a series of SemEval shared tasks . 
The task consists of both aspect term extraction and aspect sentiment prediction . 
Tay et al . 
( 2018 ) proposed ‚Äò Aspect Fusion LSTM ‚Äô to attend on the associative relationships between sentence words and aspect words to classify aspect polarities . 
Xu et al . 
( 2019 ) proposed BERT based models for aspect term extraction and aspect - polarity classiÔ¨Åcation tasks . 
We build similar BERT based aspect termextraction and aspect - polarity classiÔ¨Åcation models and use them to label Yelp reviews dataset . 
This dataset is then used for aspect - level sentiment controllable style transfer task in this paper . 
3 Methodology 3.1 Problem Statement Let us assume we have access to a corpora of labelled sentences D= ( x1,l1) ... (xn , ln ) , wherexiis a sentence , and li = { ( ti1,pi1) ... (tim , pim ) } . 
Here , tijis an aspect - target or ‚Äò Opinion Target Expression ‚Äô ( Pontiki et al . 
, 2014 ) , and pijis the corresponding sentiment - polarity expressed towards tij , where pij‚àà{‚Äúpositive ‚Äù , ‚Äú negative ‚Äù } . 
A model is to be learned that takes as input ( x , ltgt)wherexis the source sentence expressing some aspectpolarity set lsrc , and outputs ythat retain all the non - polarity content in xwhile expressing the aspect - polarity set ltgt . 
This is to be performed in an unsupervised manner , where we do not assume access to an aligned set of parallel sentences with the same content but different aspect - polarities . 
The overall architecture consists of a Transformer ( Vaswani et al . 
, 2017 ) encoder - decoder neural network , where the encoder is BERT ( Devlin et al . 
, 2019 ) . 
In this section , we describe the architecture and the training methodology used . 
The inputs provided to the model are the sentence , a list of aspects , their corresponding desired target polaritiesltgtand their corresponding per - token weights ( explained in Section 3.5 ) . 
3.2 ABSA Input Representation The BERT model ( Devlin et al . 
, 2019 ) was originally trained with two objectives : ( i ) . 
A cloze objective where the classiÔ¨Åer predicts missing words in a sequence , and ( ii ) . 
A sentence - pair classiÔ¨Åcation objective . 
For the sentence - pair objective , BERT was trained to take inputs as segment - pairs , where each segment has a different embedding added to it and are separated by a [ SEP ] token . 
For our input representation , we construct such segment - pairs . 
The Ô¨Årst segment consists of an aspect - polarity sequence SEG A= ‚Äú T1P1[SEP ASP] ... T kPk[SEP ASP ] ‚Äù , whereTiis the tokenized target aspect term and Pi‚àà{[POS ] , [ NEG ] } is a polarity corresponding to it . 
[ SEP ASP]‚Äùis a separator token . 
[ POS ] , [ NEG ] are the special tokens corresponding to the ‚Äô positive ‚Äô and ‚Äô negative ‚Äô sentiments , for which the unused tokens from the BERT vocabulary were used . 
The second segment SEG Bconsists of a sentence expressing some sentiment towards these targets . 
3.3 Preconditioning the BERT - encoder for ABSA Input Representations We precondition the BERT encoder to better understand the ABSA task and to learn the tokenembeddings for [ POS ] and[NEG ] with MLM pre - training ( a cloze objective ) ( c.f . 
Figure 2 ) . 
For each data instance , with an equal probability , we randomly mask out either ( i ) . 
all the polarity tokens from aspect - polarity sequence ( SEG A ) , or ( ii ) . 
random tokens from the sentence ( SEG B ) and train the encoder to correctly predict the masked-305 [ MASK ] service was [ MASK ] but the [ MASK ] was delicious.service [ NEG ] food [ POS ]   SEGA : Source aspect - polarity sequence   constructed from lsrc SEGB : Randomly masked source sentence tokens ( x )   ( expressing aspect - level sentiments lsrc)[SEP ] BERT Encoder The slow food The service was slow but the food was delicious.service [ MASK ] food [ MASK ]   SEGA : Aspect - polarity sequence with   Masked polarities , constructed from lsrc SEGB : Source sentence tokens ( x ) ( expressing   aspect - level sentiments lsrc)[SEP ] BERT Encoder [ POS ] [ NEG ] Figure 2 : BERT Encoder pre - training x : input sentence Asp1 [ pol1 ] [ SEPASP ] ‚Ä¶ Aspk [ polk ] [ SEPASP ] SEGA : Goal aspect - polarity sequence   constructed from ltgt SEGB : source sentence tokens   ( expressing aspect - level sentiments lsrc)[SEP ] BERT Encoder + + + + + + + + vivi‚àà{vpos , vneg } ‚Ä¢‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢ ‚Ä¢   pi HH‚ÄôTransformer   Decoder y } ( For each   aspect ) ‚àÄ i From classifier   saliency - maps ( expressing   Aspect - level   sentiments ltgt)vposvneg Figure 3 : The encoder - decoder network used , with the polarity injection . 
out tokens . 
When the polarities get masked , the encoder learns to correctly understand the aspectlevel sentiment polarities from a sentence . 
When the words from the sentence get masked , the encoder also learns to correctly predict attribute markers corresponding to a given aspect and a sentiment . 
For example , it would learn associations between the markers , such as ‚Äò personable ‚Äô ( or ‚Äò rude ‚Äô ) when given an aspect - term , such as ‚Äò staff ‚Äô with a polarity ‚Äò [ POS ] ‚Äô ( or ‚Äò [ NEG ] ‚Äô ) as opposed to an aspectmarker , such as ‚Äò delicious ‚Äô , which can not be used with ‚Äò staff ‚Äô . 
3.4 Encoder - Decoder Architecture To convert a sentence from one set of aspectlevel polarities to another , the input to the encoder consists of the target aspect - level polarities ltgt asSEG Awith the source sentence xpassed as SEG B. The full architecture is shown in Figure 3 . 
The source sentence xexpresses some source aspect - level polarities lsrcwhich is not provided to the model . 
The polarity - injection ( explained in Section 3.5 ) adds the weighted target polarities ltgtinto the hidden - representation Hfrom the BERTencoder to obtain H / primewhich is passed to the decoder . 
The decoder is trained to output the target sentence ywhich consists of the same content as present in xbut expressing the target aspect - level polarities ltgt . 
This architecture is trained in an unsupervised fashion as explained in Section 3.6 . 
3.5 Polarity Injection In Pryzant et al . 
( 2020 ) , authors showed that the hidden states of the encoder can be edited by adding weighted vectors to indicate subjective - bias , before being input to the decoder . 
They proposed this as a method to join the results from two sub - modules in their system . 
Here , we extend this to cover multiple attributes - the ‚Äò positive ‚Äô and ‚Äò negative ‚Äô sentiments , and substitute the supervised model they train with saliency - based weights . 
We inject ( add ) weighted amounts of two vectors corresponding to these two attributes to edit the hidden states output by the encoder . 
For each aspect , the vector added corresponds to the desired target polarity of this aspect , and the amount added to a given token depends on the saliency - based weight for this token calculated from the gradient for this aspect ‚Äôs polarity from a classiÔ¨Åcation model ( described in Section 4.1.1 ) . 
More formally , the polarity injection is calculated from equation 1 . 
H= [ h1,h2, ... h k]that denotes the hidden - state output from the encoder , andH / prime= [ h / prime 1,h / prime 2, ... h / prime k]are the new hidden - states calculated after polarity - injection . 
The number pij denotes the saliency - based weightage for token j with respect to aspect i. Figure 3 shows the polarityinjection architecture . 
vposandvnegare the special vector - embeddings , which have the same size as the hidden dimension , and trained to denote the positive and negative sentiment , respectively . 
h / prime j = hj+k / summationdisplay i=1pij¬∑vi ( 1 ) vi=/braceleftBigg vpos ifpolidesired is positive vneg ifpolidesired is negative(2 ) wherepoliis the target ( desired ) polarity from ltgt for the ithaspect - term . 
For calculating pij , saliencymaps obtained for each aspect from the polarity classiÔ¨Åer described in 4.1 are used . 
Saliency - maps ( Simonyan et al . 
, 2014 ) are calculated with the gradient of the loss at the input , as given in equation306 Dataset No . 
of Sentences No . 
of Target Aspects No . 
of Unique Target Aspects SemEval ( Train and Validation ) 2,242 4,016 1,437 SemEval ( Test ) 401 513 269 Yelp ( Train and Validation ) 361,968 471,820 47,750 Table 1 : Data distribution for the restaurant domain . 
The Yelp dataset does not contain target aspects and their polarities extracted , and these were extracted with a classiÔ¨Åer trained on the SemEval training data 3 . 
Thestokvalues for all the tokens tokin the sentencexare normalized between 0 and 1 for each ( targett , sentencex ) pair to obtain the pijvalues . 
Since the saliency - maps produce high values for the tokens that are important in calculating the sentiment ‚Äôs polarity , adding the ‚Äò positive ‚Äô or ‚Äò negative ‚Äô embedding weighted by these probabilities would provide hints to the decoder about the important words to be rewritten with the required sentiment . 
Thepijvalues over Segment - A is set to 1 over all the tokens corresponding to the ithaspect term ( TiPi ) ( see Section 3.2 ) and 0 over the other tokens . 
stok=/vextendsingle / vextendsingle / vextendsingle / vextendsingle‚àÇL(yt;x , t , Œ∏ ) ‚àÇemb tok / vextendsingle / vextendsingle / vextendsingle / vextendsingle;‚àÄtok‚ààx ( 3 ) One - Zero Alternative to Saliency : To test for performance in the absence of any saliency information , we also propose using a one - zero setup . 
Here , pijis set to 1 over the tokens corresponding to the ithaspect - term and 0 elsewhere . 
So in this setup , vposgets added to the tokens corresponding to the positive aspects and vneggets added to the tokens corresponding to the negative aspects . 
For example , in Figure 1 , vposgets added to the subword tokens corresponding to the word ‚Äò salads ‚Äô and ‚Äò chicken ‚Äô , while vneggets added to the sub - word tokens corresponding to the word ‚Äò service ‚Äô . 
3.6 Unsupervised Training For training the model in an unsupervised setting , we alternate training steps between a denoising auto - encoding objective and a back - translation objective . 
During the denoising step , we add random noise to the sentence part of the input SEG B. We also randomly mask the polarities in the aspectpolarity sequence in SEG Awith a small probability to ensure the model learns to generate outputs using the polarity injection clues . 
During the backtranslation step , a random query ltgtaspect - polarity sequence is used to produce an intermediate translation ( using the model ) , and the same model is trained to regenerate the original input when provided the aspect - polarity sequence from the original input sentence.4 Experiments In this section we report the datasets used for the experiments and the implementation details . 
4.1 Datasets Text generation tasks require huge amounts of data , however there are no aspect - sentiment annotated datasets that are large enough for our task . 
Fortunately , aspect extraction and aspect - sentiment classiÔ¨Åcation tasks have been well explored and have several publicly available datasets . 
We used datasets ( only restaurant domain ) from SemEval 2014 , 2015 and 2016 ( ABSA task ) to train BERT based aspect extraction and aspect - sentiment classiÔ¨Åcation systems . 
We only consider positive and negative polarities for our experiments . 
For the task of aspect - level sentiment style transfer , we use Yelp dataset . 
Since this dataset does not contain aspect - level polarity information or the target - aspects extracted , we use our BERT - based target - extraction model and BERT - based polarity classiÔ¨Åcation model which were trained on the SemEval ABSA training data , to generate aspect - level sentiment data from the Yelp reviews dataset . 
Table 1 shows some statistics from the datasets . 
4.1.1 Aspect based Sentiment Analysis with BERT A pipeline of BERT - based models was trained for target - extraction and aspect - level polarity classiÔ¨Åcation over the SemEval dataset . 
These are the models used to extract target - aspects and their polarities from the Yelp dataset . 
The target extraction task was posed as a sequential token classiÔ¨Åcation problem with BERT using the IOB2 format ( SANG , 1999 ) . 
This BERT model was fed the whole sentence as the input segment and it obtained an F1 - score of 0.8012 ( evaluation carried out similar to Sang and Buchholz ( 2000 ) ) . 
The sentiment - polarity prediction task is posed as a sentence - pair classiÔ¨Åcation problem using BERT , with the sentence provided as the Ô¨Årst segment and the aspect - term as the second segment . 
This model obtained an F1 - score of 0.9080 for the positive po-307 ModelClassiÔ¨Åer Score ( Overall)ClassiÔ¨Åer Score ( 1 - Aspect)ClassiÔ¨Åer Score ( 2 - Aspects)ClassiÔ¨Åer Score ( 3 - or - more Aspects)BLEU Score BERT - Baseline ( BB ) 0.5158 0.4983 0.5448 0.5036 36.0683 BB + MLM pretraining ( BBMLM)0.5298 0.5433 0.5310 0.5145 35.4601 BB - MLM + one - zero polarity injection0.5415 0.5675 0.5276 0.5290 35.8244 BB - MLM + saliency - based polarity injection0.5918 0.6125 0.5828 0.5797 39.3838 Table 2 : Results of automatic evaluation . 
The overall classiÔ¨Åer score is calculated over all queries . 
The other columns show the score calculated only on queries with one aspect , two aspects or three or more aspects . 
The classiÔ¨Åer scores are calculated on the full test set , while the BLEU scores are measured with reference - outputs for a subset of 100 queries . 
Model Att Con Gra BERT - Baseline ( BB ) 2.48 3.99 3.96 BB + MLM pretraining ( BB - MLM ) 2.64 3.95 4.04 BB - MLM + one - zero polarity injection 2.80 4.00 4.05 BB - MLM + saliency - based polarity injection 2.98 4.08 4.05 Table 3 : Results of manual evaluation . 
Here , ‚Äò Att ‚Äô stands for attribute match , ‚Äò Con ‚Äô stands for content preservation and ‚Äò Gra ‚Äô stands for grammaticality or Ô¨Çuency . 
Manual evaluation is performed on a subset of 100 queries from all the test set queries , and averaged scores are shown . 
larity and 0.8239 for the negative polarity on the ABSA restaurant dataset . 
Using this classiÔ¨Åer , for each ( Sentence , Target ) pair the gradient of the loss was taken at the input token embeddings and normalized to obtain the saliency - based weights used for polarity - injection . 
4.2 Implementation Details All the models were implemented using PyTorch ( Paszke et al . 
, 2017 ) . 
The BERT model was implemented using the transformers library ( Wolf et al . 
, 2019 ) . 
Models are trained with an initial learning rate of 1e-4 with a linear schedule and a warmup ( Vaswani et al . 
, 2017 ) , using the Adam Optimizer ( Kingma and Ba , 2019 ) . 
Mini - batches of size 32 were used during training . 
A linear schedule was used for the weight of the loss from the denoising auto - encoding step , which was set to decrease from 1 to 0.1 for the Ô¨Årst 30,000 optimization steps and then decrease linearly to 0 over the next 70,000 steps . 
The models were each trained for 8 epochs on the Yelp dataset . 
The random masking probability used during pre - training was 0.25 . 
During the denoising step , a probability of 0.25 was used for dropping words , and words were shufÔ¨Çed with a window - size of 3.5 Results and Analysis 5.1 Evaluation The evaluation metrics we use are an extension of the metrics used for evaluating the sentiment transfer task by previous work ( such as Li et al . 
( 2018b ) ; Wang et al . 
( 2019 ) ) . 
The evaluation was done with the SemEval test dataset . 
Queries were generated from this data by randomly inverting a subset ( non - null , improper subset ) of the polarities expressed at the input . 
For queries with 2 or more aspects , as many queries were generated as there were aspects in the sentence with different random inversions , resulting in a total of 513 evaluation queries . 
A sample consisting of 100 queries from the test set was used for manual evaluation . 
5.1.1 Automatic Evaluation For automatic evaluation , we use a classiÔ¨Åer score and a BLEU score . 
The results for automatic evaluation are shown in Table 2 . 
ClassiÔ¨Åer Score : We use an aspect - level sentiment polarity classiÔ¨Åer to measure how many of the outputs express the necessary target polarities ( Li et al . 
, 2018b ) . 
We use the classiÔ¨Åer described in 4.1.1 for the polarity prediction . 
We deÔ¨Åne the classiÔ¨Åer score to be the fraction of aspect - level sentiment polarities ( predicted by the classiÔ¨Åer from the output ) that match with the desired aspect - level polarity ( from the query ) . 
While averaging , each308 Input Query Model Output overall , decent food at a good price , with friendly people.food - negative people - positiveoverall , mediocre food at a good price , with friendly people . 
food - positive people - negativeoverall , decent food at a good price , with rude people . 
the waiter was attentive , the food was delicious and the views of the city were greatwaiter - negative food - positive views of the city - positivethe waiter was inattentive , the food was delicious and the views of the city were great . 
waiter - positive food - negative views of the city - positivethe waiter was attentive , the food was disappointing but the views of the city were great . 
waiter - positive food - negative views of the city - negativethe waiter was attentive , the food was disappointing but the views of the city were terrible . 
Table 4 : Example outputs from the full model with saliency - based polarity injection with different aspect - level polarity queries . 
aspect - level sentiment in a query was treated as a separate instance . 
BLEU Scores : Like in Li et al . 
( 2018b ) ; Gan et al . 
( 2017 ) , human reference outputs were written for 100 of the queries . 
Three Human experts were asked to rewrite the reviews with as much content preserved as possible , without compromising Ô¨Çuency . 
These experts had good language abilities and having satisfactory knowledge in the relevant area . 
We report BLEU scores for the models against these references . 
A BLEU score could be treated as a measure of content preservation from the input or the output Ô¨Çuency . 
5.1.2 Manual Evaluation Following the previous methods ( Li et al . 
, 2018b ; Wang et al . 
, 2019 ) for manual evaluation of style transfer , workers were asked to rate the output sentences on the Likert - scale ( 1 to 5 ) for three criteria - Attribute match to the query set of aspect - level polarities ( Att ) , Fluency ( Gra ) measuring the naturalness of the output and Content preservation ( Con ) . 
They were shown the source sentence , the query aspect - level polarities and the model output . 
The results of manual evaluation are shown in Table 31 5.2 Error Analysis The importance of each component in our model is shown through an ablation study in Table 2 and Table 3 . 
From the classiÔ¨Åer - based score , we see that the full model with saliency - based polarity injection is the most successful in transferring sentimentlevel polarities . 
Polarity injection , even without 1Inter - annotator agreement measured through the Krippendorff ‚Äôs alpha was found to be 0.92 , 0.82 , 0.87 for ‚Äò Att ‚Äô , ‚Äò Con ‚Äô , and ‚Äò Gra ‚Äô respectively.saliency information is seen to be useful . 
The models with polarity injection are especially better at transferring sentiments when three or more aspects are present , showing that the polarity signals are useful in localizing the style attributes with multiple targets present . 
The model using saliencybased weighting for the polarity injection has a signiÔ¨Åcantly higher classiÔ¨Åer score . 
This could be because of the saliency information acting like an adversarial white - box attack on the classiÔ¨Åer , making it easier to obtain higher classiÔ¨Åer scores . 
The Content preservation ( Con ) scores and BLEU scores for the baseline models are significantly high , but these models also show poor Attribute match ( Att ) scores . 
This means that many of the sentiments at the output were left untransferred resulting in the poor Att score , while large parts of the input text were copied over to the output resulting in the larger BLEU and Content preservation scores . 
The improved Content preservation ( Con ) scores and the Fluency ( Gra ) from the model without saliency information to the model with saliencybased weighting shows that the attribute transfer with saliency - based info is more successful in inverting the correct polarities , while maintaining the content and Ô¨Çuency , due to the added information about the words to be edited . 
The Table 5 shows how the model outputs change with different components of the model are ablated . 
With queries involving mostly positive or negative attributes , the saliency - based polarity injection supports the localized inversion of sentiment in the output . 
Outputs also show how polarity injection helps produce the required change with more content and Ô¨Çuency preserved , by selectively editing the correct words.309 Input the veal and the mushrooms were cooked perfectly . 
Query veal - positive , mushrooms - negative BERT - Baseline ( BB ) the veal and the mushrooms were not cooked perfectly . 
BB + MLM pretraining ( BB - MLM ) the veal and the mushrooms were over cooked perfectly . 
BB - MLM + one - zero polarity injection the veal was gross and the mushrooms were over cooked . 
BB - MLM + saliency - based polarity injection loved the veal and the mushrooms were over cooked . 
Inputthe waiter was attentive , the food was delicious and the views of the city were great . 
Query waiter - negative , food - positive , views of the city - positive BERT - Baseline ( BB)the waiter was attentive , the food was delicious and the city were great . 
BB + MLM pretraining ( BB - MLM)the waiter was attentive , the food was delicious and the views of the city were great . 
BB - MLM + one - zero Polarity injectionthe waiter was attentive , the food was delicious and the views of the city were great . 
BB - MLM + saliency - based polarity injectionthe waiter was inattentive , the food was delicious and the views of the city were great . 
Input for 7 years they have put out the most tasty , most delicious food and kept it that way ... Query food - negative BERT - Baseline ( BB ) for 7 years they have put out the most tasty food and kept it that way . 
BB + MLM pretraining ( BB - MLM ) for 7 years they have put out the most greasy food and bland food that way ... BB - MLM + one - zero polarity injection for 6 years they have put out the most bland food and kept it that way ... BB - MLM + saliency - based polarity injection for 7 years they have put out the most bland food and kept it that way ... Table 5 : Example outputs from the SemEval data showing aspect - level sentiment transfer from the ablated models . 
Aspects colored red ( negative ) or green ( positive ) indicate their sentiment . 
Input i must say i am surprised by the bad reviews of the restaurant earlier in the year , though . 
Query restaurant - negative Output i must say i am surprised by the bad reviews of the restaurant earlier in the year , though . 
Comment No change . 
Sentiment here is implied and latent . 
Inputthe space is limited so be prepared to wait up to 45 minutes - 1 hour , but be richly rewarded when you savor the delicious indo - chinese food Query space - positive , indo - chinese food - positive Output the space is extensive so be prepared to 10 - 15 - 20 + minutes , but delicious chinese food . 
CommentDisÔ¨Çuency and dropped content due to the length of input and the negative sentiment implied through the word ‚Äò waiting ‚Äô . 
Input i ‚Äôd be horriÔ¨Åed if my staff were turning away customers so early and so rudely ! Query staff - positive Output i ‚Äôd be delighted if my staff were turning away customers so early and nicely ! Comment Lower naturalness of the output from real - world knowledge that turning away customers is bad . 
Input i had Ô¨Åsh and my husband had the Ô¨Ålet - both of which exceeded our expectations . 
Query Ô¨Åsh - negative , Ô¨Ålet - positive Output i had Ô¨Åsh and my husband had the Ô¨Ålet - both of which exceeded our expectations . 
CommentThe attribute markers for Ô¨Åsh and Ô¨Ålet are shared , making transfer difÔ¨Åcult . 
SigniÔ¨Åcant rewriting of the input in needed to produce acceptable Ô¨Çuent output . 
Table 6 : Example sentences that show difÔ¨Åculty in transferring sentiment . 
To understand the errors in outputs better , the outputs marked with low Att , Con and Gra scores were examined . 
Some of these outputs are shown and discussed in Table 6 . 
Many of the failures in Attribute match were found to be due to the complexity involved in the language , such as when the sentiment expressed towards a target is implicit from the content of the review . 
The absence ofattribute markers also makes it harder to convert sentiment . 
Most outputs with low Con and Gra scores were found to contain very long sentences . 
The models were trained on the Yelp dataset which mostly contained smaller sentences . 
Failed examples with multiple different polarities at the output were often also due to the attribute markers towards different aspects being shared in the input sentence.310 Such examples require signiÔ¨Åcant rewriting and reordering to produce sentences of acceptable Ô¨Çuency , and our method seems most successful when making localized changes such as with word replacements . 
6 Conclusion and Future Work In this paper , the task of aspect - level sentiment style transfer has been introduced , where stylistic attributes can be localized to different parts of a sentence . 
We have proposed a BERT - based encoder - decoder architecture with saliency - based polarity injection and show that it can be successful at the task when trained in an unsupervised setting . 
The experiments have been conducted on an aspect level polarity tagged benchmark dataset related to the restaurant domain . 
This work is hopefully an important initial step in developing a Ô¨Åne - grained controllable style transfer system . 
In the future , we would like to explore the ability to transfer such systems to data - sparse domains , and explore injecting attributes such as emotions to targets attributes in larger pieces of text . 
Acknowledgments Authors duly acknowledge the support from the Project titled Sevak - An Intelligent Indian Language Chatbot , Sponsored by SERB , Govt . 
of India ( IMP/2018/002072 ) . 
Abstract Abstractive community detection is an important spoken language understanding task , whose goal is to group utterances in a conversation according to whether they can be jointly summarized by a common abstractive sentence . 
This paper provides a novel approach to this task . 
We Ô¨Årst introduce a neural contextual utterance encoder featuring three types of self - attention mechanisms . 
We then train it using the siamese and triplet energybased meta - architectures . 
Experiments on the AMI corpus show that our system outperforms multiple energy - based and non - energy based baselines from the state - of - the - art . 
Code and data are publicly available1 . 
1 Introduction Today , large amounts of digital text are generated by spoken or written conversations , let them be human - human ( customer service , multi - party meetings ) or human - machine ( chatbots , virtual assistants ) . 
Such text comes in the form of transcriptions . 
A transcription is a list of time - ordered text fragments called utterances . 
Abstractive summarization of conversations is an open problem in NLP . 
Previous work ( Mehdad et al . 
, 2013 ; Oya et al . 
, 2014 ; Banerjee et al . 
, 2015 ; Shang et al . 
, 2018 ) decomposes this task into two subtasks a andbas shown in Fig . 
1 . 
Subtask a , orAbstractive Community Detection ( ACD ) , is the focus of this paper . 
It consists in grouping utterances according to whether they can be jointly summarized by a common abstractive sentence ( Murray et al . 
, 2012 ) . 
Such groups of utterances are called abstractive communities . 
Once they are obtained , an abstractive sentence is generated for each group ( subtask b ) , thus forming the Ô¨Ånal summary . 
ACD includes , but is a more 1https://bitbucket.org/guokan_shang/ abscomm transcription abstractive   communities abstractive   summary extracted   utterances a1a a2 bFigure 1 : Abstractive summarization of conversations . 
general problem than , topic clustering . 
Indeed , as shown in Fig . 
2 , communities should capture more complex relationship than simple semantic similarity . 
Also , two utterances may be part of the same community even if they are not close to each other in the transcription . 
Finally , a given utterance may belong to more than one community , which results in overlapping groupings ( e.g. , A and D in Fig . 
2 ) , or be a singleton community ( B in Fig . 
2 ) . 
In this paper , we depart from previous work and argue that the ACD subtask should be broken down into two steps , a1anda2 in Fig . 
1 . 
That is , summary - worthy utterances should Ô¨Årst be extracted from the transcription ( a1 ) , and then , grouped into abstractive communities ( a2 ) . 
This process is more consistent with the way humans treat the summarization task . 
E.g. , during the creation of the AMI corpus ( McCowan et al . 
, 2005 ) , annotators were Ô¨Årst asked to extract summaryworthy utterances from the transcription , and then to link the selected utterances to the sentences in the abstractive summary ( links in Fig . 
2 ) , i.e. , create communities . 
Abstractive summaries comprise four sections : ABSTRACT , ACTIONS , PROBLEMS , and DECISIONS . 
Step a1plays an important Ô¨Åltering role , since in practice , only a small part of the original utterances are used to construct the abstractive communities ( 17 % on average for AMI ) . 
However , this step is closely related to extractive summarization , which has been extensively studied in the conversational domain ( Murray et al . 
,313 UI : ¬† But ¬† what ¬† if ¬† we ¬† ha ¬† what ¬† if ¬† we ¬† had ¬† like ¬† a ¬† Spongy sort ¬† of ¬† like ¬† stress ¬† balley ¬† kinda ¬† [ disfmarker ] PM : ¬† If ¬† you ¬† have ¬† like ¬† that ¬† stress ¬† ball ¬† material ¬† kind ¬†  of ¬† as ¬† what ¬† you ‚Äôre ¬† actually ¬† holding ¬† in ¬† your ¬† hand , PM : ¬† and ¬† then ¬† there ‚Äôs ¬† more ¬† of ¬† a ¬† hard ¬† plastic ¬† thing ¬†  where ¬† that ¬† thing ¬† is . 
PM : ¬† And ¬† on ¬† that ¬† hard ¬† plastic ¬† thing ¬† you ¬† can ¬† change either ¬† the ¬† colour ¬† or ¬† the ¬† fruit ¬† or ¬† vegetable ¬† that ‚Äôs ¬†  on ¬† there . 
ME : ¬† see ¬† you ‚Äôre ¬† thinking , ¬† it ‚Äôs ¬† weird , ¬† you ‚Äôre ¬† thinking the ¬† opposite ¬† of ¬† me ME : ¬† Because ¬† I ¬† was ¬† thinking ¬† if ¬† you ¬† have ¬† a ¬† cover ¬† for the ¬† squashy ¬† bit , UI : ¬† oh ¬† so ¬† so ¬† you ‚Äôre ¬† saying ¬† the ¬† squishy ¬† part ‚Äôs ¬† like detachable , UI : ¬† so ¬† so ¬† maybe ¬† one ¬† you ¬† know ¬† [ disfmarker ] ¬† you ¬†  can ¬† have ¬† like ¬† the ¬† broccoli ¬† squishy ¬† thing , ¬† and ¬† then you ¬† could ¬† have ¬† like ¬† the ¬† banana ¬† squishy ¬† thing PM : ¬† Oh ¬† when ¬† we ¬† move ¬† on , ¬† you ¬† two ¬† are ¬† going ¬† to ¬†  be ¬† playing ¬† with ¬† play¬≠dough . 
ABSTRACT Some ¬† part ¬† of ¬† the ¬† casing ¬† will ¬† be ¬† made ¬† of ¬† a ¬† spongy material . 
... ACTIONS The ¬† Project ¬† Manager ¬† instructed ¬† the ¬† User ¬† Interface ¬†  Designer ¬† and ¬† the ¬† Industrial ¬† Designer ¬† to ¬† construct ¬†  the ¬† prototype . 
... DECISIONS The ¬† remote ¬† will ¬† feature ¬† a ¬† changeable ¬† outer ¬† casing . 
... PROBLEMS The ¬† group ¬† wanted ¬† to ¬† include ¬† a ¬† changeable ¬† outer ¬†  casing ¬† but ¬† could ¬† not ¬† decide ¬† whether ¬† the ¬† spongy or ¬† the ¬† hard ¬† plastic ¬† component ¬† should ¬† be ¬† the ¬†  removable ¬† casing . 
... ‚¨á timeAA BB CC DD ‚¨Ü Figure 2 : Example of ground truth human annotations from the ES2011c AMI meeting . 
Successive grey nodes on the left denote utterances in the transcription . 
Black nodes correspond to the utterances judged important . 
Sentences ( e.g. , A , B , C , D ) from the abstractive summary are shown on the right . 
All utterances linked to the same abstractive sentence form one community . 
2005 ; Garg et al . 
, 2009 ; Tixier et al . 
, 2017 ) . 
Rather , we focus in this paper on the rarely explored a2utterance clustering step , which we think is an important spoken language understanding problem , as it plays a crucial role of bridge between two major types of summaries : extractive and abstractive . 
2 Departure from previous work Prior work performed ACD either in a supervised ( Murray et al . 
, 2012 ; Mehdad et al . 
, 2013 ) or unsupervised way ( Oya et al . 
, 2014 ; Banerjee et al . 
, 2015 ; Singla et al . 
, 2017 ; Shang et al . 
, 2018 ) . 
In the supervised case , Murray et al . 
( 2012 ) train a logistic regression classiÔ¨Åer with handcrafted features to predict extractive - abstractive links , then build an utterance graph whose edges represent the binary predictions of the classiÔ¨Åer , and Ô¨Ånally apply an overlapping community detection algorithm to the graph . 
Mehdad et al . 
( 2013 ) add to the previous approach by building an entailment graph for each community , where edges are entailment relations between utterances , predicted by a SVM classiÔ¨Åer trained with handcrafted features on an external dataset . 
The entailment graph allows less informative utterances to be eliminated from each community . 
On the other hand , unsupervised approaches to ACD do not make use of extractive - abstractive links . 
Oya et al . 
( 2014 ) ; Banerjee et al . 
( 2015 ) ; Singla et al . 
( 2017 ) assume that disjoint topic segments ( Galley et al . 
, 2003 ) align with abstractive communities , while Shang et al . 
( 2018 ) use the classical vector space representation with TF - IDF weights , and apply k - means to the LSAcompressed utterance - term matrix . 
To sum up , prior ACD methods either train multiple models on different labeled datasets and heavily rely on handcrafted features , or are incapable of capturing the complicated structure of abstractive communities described in the introduction . 
Motivated by the recent success of energy - based approaches to similarity learning tasks such as face veriÔ¨Åcation ( Schroff et al . 
, 2015 ) and sentence matching ( Mueller and Thyagarajan , 2016 ) , we introduce in this paper a novel utterance encoder , and train it within the siamese ( Chopra et al . 
, 2005 ) and triplet ( Hoffer and Ailon , 2015 ) energy - based meta - architectures . 
Our Ô¨Ånal network is able to accurately capture the complexity of abstractive community structure , while at the same time , it is trainable in an end - to - end fashion without the need for human intervention and handcrafted features . 
Our contributions are threefold : ( 1 ) we formalize ACD , a crucial subtask for abstractive summarization of conversations , and publicly release a version of the AMI corpus preprocessed for this subtask , to foster research on this topic , ( 2 ) we propose one of the Ô¨Årst applications of energy - based learning to spoken language understanding , and ( 3 ) we introduce a novel utterance encoder featuring three types of self - attention mechanisms and taking contextual and temporal information into account . 
3 Energy framework Energy - Based Modeling ( EBM ) ( LeCun and Huang , 2005 ; Lecun et al . 
, 2006 ) is a uniÔ¨Åed framework that can be applied to many machine learning314 X X Y X Y Z Y   ( a ) ( b ) ( c ) Figure 3 : Three EBM architectures . 
When all Gs andWs are equal , ( b ) and ( c ) correspond to the siamese / triplet cases . 
problems . 
In EBM , an energy function assigns a scalar called energy to each pair of random variables ( X , Y ) . 
The energy can be interpreted as the incompatibility between the values of XandY. Training consists in Ô¨Ånding the parameters W‚àóof the energy function EWthat , for all ( Xi , Yi)in the training setSof sizeP , assign low energy to compatible ( correct ) combinations and high energy to all other incompatible ( incorrect ) ones . 
This is done by minimizing a loss functional2L : W‚àó= arg min W‚ààWL(EW(X , Y ) , S ) ( 1 ) For a given X , prediction consists in Ô¨Ånding the value ofYthat minimizes the energy . 
3.1 Single architecture In the EBM framework , a regression problem can be formulated as shown in Fig . 
3a , where the input Xis passed through a regressor model GWand the scalar output is compared to the desired outputYwith a dissimilarity measure Dsuch as the squared error . 
Here , the energy function is the loss functional to be minimized . 
L=1 PP / summationdisplay i=1EW(Xi , Yi ) = 1 PP / summationdisplay i=1 / bardblGW(Xi)‚àíYi / bardbl2(2 ) 3.2 Siamese architecture In the regression problem previously described , the dependence between XandYis expressed by a direct mapping Y = f(X ) , and there is a single bestY‚àófor everyX. However , when XandY are not in a predictor / predictand relationship but are exchangeable instances of the same family of objects , there is no such mapping . 
E.g. , in paraphrase identiÔ¨Åcation , a sentence may be similar 2the loss functional is passed the output of the energy function , unlike a loss function which is directly fed the output of the model.to many other ones , or , in language modeling , a givenn - gram may be likely to be followed by many different words . 
Thereby , Lecun et al . 
( 2006 ) introduced EBM for implicit regression orconstraint satisfaction ( see Fig . 
3b ) , in which a constraint that XandYmust satisfy is deÔ¨Åned , and the energy function measures the extent to which that constraint is violated : EW1,W2(X , Y ) = D(GW1(X),GW2(Y ) ) ( 3 ) whereGW2andGW1are two functions parameterized byW1andW2 . 
WhenGW1 = GW2and W1 = W2 , we obtain the well - known siamese architecture ( Bromley et al . 
, 1994 ; Chopra et al . 
, 2005 ) , which has been applied with success to many tasks , including sentence similarity ( Mueller and Thyagarajan , 2016 ) . 
Here , the constraint is determined by a collection - level set of binary labels { Ci}P i=1.Ci= 0indicates that ( Xi , Yi)is agenuine pair ( e.g. , two paraphrases ) , while Ci= 1indicates that ( Xi , Yi ) is an impostor pair ( e.g. , two sentences with different meanings ) . 
The function GWprojects objects into an embedding space such that the deÔ¨Åned dissimilarity measureD(e.g . 
, Euclidean distance ) in that space reÔ¨Çects the notion of dissimilarity in the input space . 
Thus , the energy function can be seen as a metric to be learned . 
We experiment with deep neural network encoders asGW , and , following ( Mueller and Thyagarajan , 2016 ) , we adopt the exponential negative Manhattan distance as dissimilarity measure and the mean squared error as loss functional : EW(X , Y ) = 1‚àíexp(‚àí/bardblGW(X)‚àíGW(Y)/bardbl1)(4 ) L=1 PP / summationdisplay i=1 / bardblEW(Xi , Yi)‚àíCi / bardbl2(5 ) 3.3 Triplet architecture The triplet architecture ( Schroff et al . 
, 2015 ; Hoffer and Ailon , 2015 ; Wang et al . 
, 2014 ) , as can be seen in Fig . 
3c , is a direct extension of the siamese architecture that takes as input a triplet ( X , Y , Z ) in lieu of a pair ( X , Y ) .X , Y , andZare referred to as the positive , anchor , and negative objects , respectively . 
XandYare similar , while both being dissimilar toZ. Learning consists in jointly minimizing the positive - anchor energy EW(Xi , Yi)while maximizing the anchor - negative energy EW(Yi , Zi ) . 
Here , we use the softmax triplet loss ( Hoffer and Ailon , 2015 ) as our loss functional:315 L=1 2PP / summationdisplay i=1 / parenleftbig /bardblne+‚àí0 / bardbl2+/bardblne‚àí‚àí1 / bardbl2 / parenrightbig ( 6 ) ne+=eEW(Xi , Yi ) eEW(Xi , Yi)+eEW(Yi , Zi)(7 ) ne‚àí=eEW(Yi , Zi ) eEW(Xi , Yi)+eEW(Yi , Zi)(8 ) wherenestands for normalized energy , and the dissimilarity measure is the Euclidean distance , i.e. , EW(Xi , Yi ) = /bardblGW(Xi)‚àíGW(Yi)/bardbl2 . 
Essentially , the softmax triplet loss is the mean squared error between the normalized energy vector [ ne+,ne‚àí ] and[0,1 ] . 
We justify our choice of loss functionals in App . 
F. 3.4 Sampling procedures We sample tuples from the ground truth abstractive communities to train our utterance encoder GW under the siamese and triplet meta - architectures as follows . 
Pair sampling . 
All utterances belonging to the same community are paired as genuine pairs , while impostor pairs are any two utterances coming from different communities . 
Triplet sampling . 
Utterances from the same community provide positive and anchor items , while the negative item is taken from any other community . 
4 Proposed utterance encoder Notation . 
The timet(as superscript ) denotes the position of a given utterance in the conversation of lengthT , and the positioni(as subscript ) denotes the position of a token within a given utterance of lengthN. E.g. , ut 1is the representation of the Ô¨Årst token of Ut , thetthutterance in the transcription . 
Word encoder . 
As shown in the upper right corner of Fig . 
4 , we obtain ut iby concatenating the pre - trained vector of the corresponding token with the discourse features of Ut(role , position and dialogue act ) , and passing the resulting vector to a dense layer . 
Utterance encoder . 
As shown in the center of Fig . 
4 , we represent Utas a sequence of N ddimensional token representations / braceleftbig ut 1, ... ,ut N / bracerightbig . 
In addition , because there is a strong time dependence between utterances ( see Fig . 
2 ) , we inform the model about the preceding and following utterances when encoding Ut . 
To accomplish this , we prepend ( resp . 
append ) to Uta context vector containing information about theprevious ( resp . 
next ) utterances , Ô¨Ånally obtainingUt=/braceleftbig ut pre , ut 1, ... ,ut N , ut post / bracerightbig ‚ààR(N+2)√ód . 
We then use a non - stacked bidirectional Recurrent Neural Network ( RNN ) with Gated Recurrent Units ( GRU ) ( Cho et al . 
, 2014 ) to transform Ut into a sequence of annotations Ht‚ààR(N+2)√ó2d . 
In practice , the pre and post - context vectors indirectly initialize the left - to - right and right - to - left RNNs with information about the utterances preceding and following Ut . 
This is similar in spirit to the warm - start method of Wang et al . 
( 2017 ) , that directly initializes the hidden states of the RNNs with the context vectors . 
Self - attention . 
We then pass the annotations Htto a self - attention mechanism ( Vaswani et al . 
, 2017 ; Lin et al . 
, 2017 ) . 
More precisely , Htgo through a dense layer and the output is compared via dot product with a trainable vector uŒ≥ , initialized randomly . 
Then , a probability distribution over the N+ 2tokens in Utis obtained via a softmax : Œ≥Œ≥Œ≥t= softmax ( uŒ≥¬∑tanh(WŒ≥Ht ) ) ( 9 ) The attentional vector for Utis Ô¨Ånally computed as a weighted sum of its annotations , and , as shown in Fig . 
4 , is Ô¨Ånally passed to a dense layer to obtain the utterance embedding ut‚ààRdf : ut= dense / parenleftBiggN+2 / summationdisplay i=1Œ≥t iht i / parenrightBigg ( 10 ) uŒ≥can be interpreted as a learned representation of the ‚Äú ideal word ‚Äù , on average . 
The more similar a token vector is to this representation , the more attention the model pays to the token . 
Context encoder : level 1 . 
The pre and postcontext vectors that we prepend and append to Ut are obtained by aggregating information from the Cutterances preceding and following Ut : ut pre‚Üêaggregate pre / parenleftbig / braceleftbig Ut‚àíC, ... ,Ut‚àí1 / bracerightbig / parenrightbig ( 11 ) ut post‚Üêaggregate post / parenleftbig / braceleftbig Ut+1, ... ,Ut+C / bracerightbig / parenrightbig ( 12 ) whereC , the context size , is a hyperparameter . 
Since ut preandut postwill become part of utterance Utwhich is a sequence of token vectors , and fed to the RNN , we need them to live in the same space as any other token vector . 
This forbids the use of any nonlinear or dimension - changing transformation inaggregate , such as convolutional or recurrent operations . 
Therefore , we use self - attention only . 
More precisely , we propose a two - level hierarchical architecture that makes use of a different type of self - attention at each level ( see left part of Fig . 
4 ) . 
The pre and post - context encoders share the exact316 utterance encoder ùë¢1ùë°‚àíùê∂ùë¢2ùë°‚àíùê∂ùë¢ùëÅùë°‚àíùê∂ ‚Ä¶ ùë¢1ùë°‚àí1ùë¢2ùë°‚àí1ùë¢ùëÅùë°‚àí1 ‚Ä¶‚Ä¶ ùë¢1ùë°‚àí2ùë¢2ùë°‚àí2ùë¢ùëÅùë°‚àí2 ‚Ä¶ Œ≤   Œ±   Œ±   Œ±pre - context encoder ( CEpre )   Œ±   Œ≤   Œ≥content - aware self - attention time - aware self - attention self - attention ùëàùë°‚àí1ùëàùë°‚àí2ùëàùë°‚àíùê∂DA wordvecdense embword encoder ( WE ) pos roleùë¢ùëñùë°to energy functionùë¢ùëùùëüùëíùë°   Œ≥ ‚Ä¶ bidirGRU ùë¢1ùë°ùë¢ùëÅùë°ùë¢ùëùùëüùëíùë° WEùë¢2ùë° WE WE ‚Ä¶ CEpre CEpostùë¢ùëùùëúùë†ùë°ùë° ùëàùë°denseFigure 4 : Our proposed utterance encoder GW . 
Only the pre - context encoder is shown . 
Cis the context size . 
same architecture , so we only describe the precontext encoder in what follows . 
Content - aware self - attention . 
At level 1 , we apply the same attention mechanism to each utterance in / braceleftbig Ut‚àíC, ... ,Ut‚àí1 / bracerightbig . 
E.g. , for Ut‚àí1 : Œ±Œ±Œ±t‚àí1= softmax / parenleftbigg uŒ±¬∑tanh / parenleftBig WŒ±Ut‚àí1+W / primeN / summationdisplay i=1ut i / parenrightBig / parenrightbigg ( 13 ) This mechanism is the same as in Eq . 
9 , except for two differences . 
First , we operate directly on the matrix of token vectors of the previous utterance Ut‚àí1rather than on RNN annotations . 
Second , there is an extra input that consists of the elementwise sum of the token vectors of the current utterance Ut . 
The latter modiÔ¨Åcation is inspired by the coverage vectors used in translation and summarization to address under(over)-translation and repetition , e.g. , ( Tu et al . 
, 2016 ; See et al . 
, 2017 ) . 
In our case , we hope that by letting the model know about the tokens in the current utterance Ut , it will be able to extract complementary -rather than redundant- information from its context , and thus produce a richer embedding . 
To recapitulate , the content - aware attention mechanism transforms the sequence of utterance matrices / braceleftbig Ut‚àíC, ... ,Ut‚àí1 / bracerightbig ‚ààRC√óN√ódinto a sequence of vectors / braceleftbig ut‚àíC, ... ,ut‚àí1 / bracerightbig ‚ààRC√ód . 
These vectors are then aggregated into a single precontext vector ut pre‚ààRdas described next . 
Note that since there is no inherent difference between preceding and following utterances3 , we use the same content - aware self - attention mechanism for the pre and post contexts . 
This also gives us a more parsimonious and faster model . 
One should note , however , that the pre and post - context encoders still differ in terms of their time - aware attention mechanisms at level 2 , described next . 
3indeed , the latter become the former as we slide the window over the transcriptionContext encoder : level 2 . 
As can be seen in Fig . 
2 , two utterances close to each other in time are much more likely to be related ( e.g. , adjacency pair , elaboration ... ) than any two randomly selected utterances . 
To enable our model to capture such time dependence , we use the trainable universal time - decay attention mechanism of Su et al . 
( 2018 ) . 
Time - aware self - attention . 
The mechanism combines three types of time - decay functions via weightswi . 
The attentional coefÔ¨Åcient for ut‚àí1 is : Œ≤t‚àí1 = w1Œ≤convt‚àí1+w2Œ≤lint‚àí1+w3Œ≤conct‚àí1(14 ) = w1 a(dt‚àí1)b+w2[edt‚àí1+k]++w3 1 + /parenleftbigdt‚àí1 D0 / parenrightbigl where [ ‚àó]+=max(‚àó,0)(ReLU),dt‚àí1is the offset between the positions of Ut‚àí1andUt , i.e. ,dt‚àí1= |t‚àí(t‚àí1)|= 1 , and thewi‚Äôs , a , b , e , k , D0 , andl are scalar parameters learned during training . 
The convex ( conv ) , linear ( lin ) , and concave ( conc ) terms respectively assume the strength of dependence to weaken rapidly , linearly , and slowly , as the distance in time increases . 
The post - context mechanism can be obtained by symmetry . 
It has different parameters . 
5 Community detection Once the utterance encoder GWpresented in Section 4 has been trained within the siamese or triplet meta - architecture presented in Section 3 , it is used to project the summary - worthy utterances from a given test transcription to a compact embedding space . 
We assume that if training was successful , the distance in that space encodes community structure , so that a basic clustering algorithm such as k - means ( MacQueen , 1967 ) is enough to capture it . 
However , since we need to detect overlapping communities , we use a probabilistic version of k - means , the Fuzzy c - Means ( FCM ) algorithm ( Bezdek et al . 
,317 1984 ) . 
FCM returns a probability distribution over all communities for each utterance . 
More details are provided in App . 
E. 6 Experimental setup 6.1 Dataset We experiment on the AMI corpus ( McCowan et al . 
, 2005 ) , with the manual annotations v1.6.2 . 
The corpus contains data for more than 100 meetings , in which participants play 4 roles within a design team whose task is to develop a prototype of TV remote control . 
Each meeting is associated with the annotations described in the introduction and shown in Fig . 
2 . 
There are 2368 unique abstractive communities in total , whose statistics are shown in Table 1 . 
We adopt the ofÔ¨Åcially suggested scenario - only partition4 , which provides 97 , 20 , and 20 meetings respectively for training , validation and testing . 
We use manual transcriptions , and do not apply any particular preprocessing except Ô¨Åltering out speciÔ¨Åc ASR tags , such as vocalsound . 
typeabstract action problem decision total unique 1147 247 380 594 2368 disjoint 528 124 69 45 766 overlapping 349 17 163 149 678 singleton 49 162 38 244 493 Table 1 : Statistics of abstractive communities . 
6.2 Baselines Full baseline details are provided in App . 
B. ‚Ä¢Encoders . 
First , we evaluate our utterance encoder against two encoders that are trained within the energy framework : ( 1 ) LD(Lee and Dernoncourt , 2016 ) , a sequential sentence encoder developed for dialogue act classiÔ¨Åcation ; and ( 2 ) HAN ( Yang et al . 
, 2016 ) , a hierarchical self - attentive network for document embedding . 
Note that to be fair , we ensure that both LD and HAN have access to context ( see details in App . 
B ) . 
We also compare our full pipeline against unsupervised and supervised systems . 
‚Ä¢Unsupervised systems . 
In ( 1 ) tf - idf , we combine the TF - IDF vectors of the current utterance and the context utterances , each concatenated with their discourse features , and apply FCM . 
In ( 2)w2v , we repeat the same approach with the word2vec centroids of the words in each utterance . 
We also compare our full pipeline against LCseg ( Galley et al . 
, 2003 ) , a lexical - cohesion based topic 4http://groups.inf.ed.ac.uk/ami/ corpus / datasets.shtmlsegmenter that directly clusters utterances without computing embeddings . 
‚Ä¢Supervised systems . 
Finally , here , we use an approach similar to that of Murray et al . 
( 2012 ) . 
More precisely , we train a MLP to learn abstractive links between utterances , and then apply the CONGA community detection algorithm to the utterance graph . 
We also considered 4 variants of our model : ( 1 ) CA - S : we replace the time - aware self - attention mechanism of the context encoder with basic selfattention . 
( 2 ) S - S : we replace both the contentaware and the time - aware self - attention mechanisms of the context encoder with basic selfattention . 
( 3 ) ( 0,0 ) : our model , without using the contextual encoder . 
( 4 ) ( 3,0 ) : our model , using only pre - context , with a small window of 3 , to enable fair comparison with the LD baseline . 
6.3 Training details Word encoder . 
Discourse features consist of two one - hot vectors of dimensions 4 and 16 , respectively for speaker role and dialogue act . 
The positional feature is a scalar in [ 0,1 ] , indicating the normalized position of the utterance in the transcription . 
We used the pre - trained vectors learned on the Google News corpus with word2vec by ( Mikolov et al . 
, 2013 ) , and randomly initialized out - of - vocabulary words ( 1645 out of 12412 ) . 
As a preprocessing step , we reduced the dimensionality of the pre - trained word vectors from 300 to 21 with PCA , in order to give equal importance to discourse and textual features . 
In the end , tokens are thus represented by a d= 42 -dimensional vector . 
Layer sizes . 
For our model , and the LD and HAN baselines , we set df= 32 ( output dimension of the Ô¨Ånal dense layer ) . 
LD . 
We set d1=3 and d2=0 , which is very close to ( 2,0 ) , the best conÔ¨Åguration reported in the original paper . 
HAN . 
Again , for the sake of fairness , we give the HAN baseline access to contextual information , by feeding it the current utterance surrounded by the Cbpreceding and Cbfollowing utterances in the transcription , where Cbdenotes the best context size reported in Section 7 . 
Training details . 
The exact same token representations and settings were used for our model , its variants , and the baselines . 
Models were trained on the training set for 30 epochs with the Adam ( Kingma and Ba , 2015 ) optimizer . 
The best epoch318 was selected as the one associated with the lowest validation loss . 
Batch size and dropout ( Srivastava et al . 
, 2014 ) were set to 16 and 0.5 . 
Dropout was applied to the word embedding layer only . 
To account for randomness , we average results over 10 runs . 
Also , following ( Hoffer and Ailon , 2015 ; Liu et al . 
, 2019 ) , we use a different , small subset of all possible triplets for training at each epoch ( more precisely , 15594 triplets ) . 
This intelligently maximizes data usage while preventing overÔ¨Åtting . 
To enable fair comparison with the siamese approach , 15594 genuine and 15594 impostor pairs were sampled at the beginning of each epoch , since we consider that one triplet essentially equates one genuine pair and one impostor pair . 
Performance evaluation . 
We evaluate performance at the distance and the clustering level , using respectively precision , recall , and F1 score at k , and the Omega Index ( Collins and Dent , 1988 ) following the previous ACD work of Murray et al . 
( 2012 ) . 
For P , R , and F1 , we evaluate the quality of the ranking of the closest utterances to a given query utterance . 
We use a Ô¨Åxed k=10 and also a variablek(denoted as k = v ) , wherekis equal to the size of the community of the query utterance minus one . 
In that case , P = R = F1 . 
More details and examples are given in Appendices C and D. For the Omega Index , we report results with a Ô¨Åxed number of communities |Q|=11 , and also a variable|Q|(|Q|=v ) , where|Q|is equal to the number of ground truth communities . 
More details and examples are given in App . 
C. Due to the stochastic nature of the FCM algorithm , we select the run yielding the smallest objective function value over 20 runs . 
7 Results Context sizes . 
Larger contexts bring richer information , but increase the risk of considering unrelated utterances . 
Using our proposed encoder within the triplet meta - architecture , we tried different values of Con the validation set , under two settings : ( pre , post ) = ( C,0 ) , and ( pre , post ) = ( C , C ) . 
Results are shown in Fig . 
5 . 
We can observe that increasing Calways brings improvement , with diminishing returns . 
Results also clearly show that considering the following utterances in addition to the preceding ones is useful . 
Note that the curves look similar for F1@k= 10 . 
In the end , we selected ( 11,11 ) as our best context sizes . 
Quantitative results . 
Final test set results are 0 3 5 7 9 11 130.480.490.500.510.52 with pre context only with pre and post contextsFigure 5 : Impact of context size on the validation P@k = v , for our model trained within the triplet meta - architecture . 
shown in Table 2 . 
All variants of our model signiÔ¨Åcantly outperform LD . 
While HAN is much stronger than LD , our model and its variants using best context sizes manage to outperform it everywhere , except in the siamese / P@k = v case ( row j ) . 
One of the reasons for the superiority of our utterance encoder is probably that it considers contextual information while encoding the current utterance , while HAN and LD take as input the context utterances together with the current utterance , without distinguishing between them . 
Moreover , we use an attention mechanism dedicated to temporality , whereas HAN is only able to capture an implicit notion of time through the use of recurrence ( RNN ) , and LD , with its dense layers , completely ignores it . 
Also , all variants of our model using best context sizes ( 11,11 ) outperform the ones using reduced ( 3,0 ) or no ( 0,0 ) context , regardless of the metaarchitecture . 
This conÔ¨Årms the value added by our context encoder . 
For siamese , our model outperforms its two variants ( CA - S andS - S ) for all metrics , indicating that both the content - aware and the time - aware selfattention mechanisms are useful . 
However , it is interesting to note that when training under the triplet conÔ¨Åguration , the CA - S variant of our model is better , suggesting that in that case , the content - aware mechanism is beneÔ¨Åcial , but the time - aware one is not . 
LCseg ( row m ) and tf - idf ( 11,11 ) ( row n3 ) are the best of all ( un)supervised baseline systems , but both perform signiÔ¨Åcantly worse than all energybased approaches , highlighting that training with the energy framework is beneÔ¨Åcial . 
In terms of Omega Index , supervised baseline systems are logically better than unsupervised ones . 
w2v generally outperforms tf - idf when there is no context ( rows k1,l1,n1,o1 ) or short context ( k2,l2,n2,o2 ) , but not with large contexts ( k3,l3,n3,o3 ) . 
Results also show that overall , using319 ( pre , P P R F1 Omega index√ó100 post ) @k = v @k= 10 |Q|=v|Q|= 11 a1 ) our model ( 0 , 0 ) 54.59 46.05 62.45 43.18 49.09 48.81 a2 ) our model ( 3 , 0 ) 55.17 46.17 62.80 43.25 49.78 49.70 a3 ) our model ( 11 , 11 ) 58.58 46.73 63.82 43.83 49.90 49.28 Triplet b ) our model ( CA - S ) ( 11 , 11 ) 59.52‚ãÜ46.98‚ãÜ64.01‚ãÜ44.06‚ãÜ50.11 49.73 c ) our model ( S - S ) ( 11 , 11 ) 58.96 46.81 63.65 43.87 49.59 49.88 d ) LD ( 3 , 0 ) 52.04 44.82 60.41 41.82 48.70 48.14 e ) HAN ( 11 , 11 ) 58.72 45.76 62.60 42.89 49.32 48.88 f1 ) our model ( 0 , 0 ) 53.01 45.10 60.97 42.12 50.56 49.65 f2 ) our model ( 3 , 0 ) 53.78 45.54 61.33 42.48 51.01 50.00 f3 ) our model ( 11 , 11 ) 56.64 46.47 62.54 43.40 52.44‚ãÜ51.88‚ãÜ Siamese g ) our model ( CA - S ) ( 11 , 11 ) 56.46 46.08 61.92 43.02 51.60 50.98 h ) our model ( S - S ) ( 11 , 11 ) 55.68 45.64 61.17 42.53 52.26 51.11 i ) LD ( 3 , 0 ) 52.13 44.83 60.85 41.86 51.18 50.70 j ) HAN ( 11 , 11 ) 58.54 45.72 61.55 42.74 50.51 49.82 k1 ) tf - idf ( 0 , 0 ) 29.28 26.67 34.69 24.19 13.12 13.66 k2 ) tf - idf ( 3 , 0 ) 34.77 30.27 40.83 27.79 10.22 10.17 k3 ) tf - idf ( 11 , 11 ) 58.94 43.94 61.36 41.45 38.09 39.47 Unsupervised l1 ) w2v ( 0 , 0 ) 29.02 27.46 37.39 25.11 13.89 13.50 l2 ) w2v ( 3 , 0 ) 34.11 29.92 39.55 27.32 10.61 10.77 l3 ) w2v ( 11 , 11 ) 58.30 44.08 61.59 41.59 37.75 38.28 m ) LCSeg - - - - - 38.98 41.57 n1 ) tf - idf ( 0 , 0 ) - - - - 25.04 25.14 n2 ) tf - idf ( 3 , 0 ) - - - - 27.33 26.95 Supervised n3 ) tf - idf ( 11 , 11 ) - - - - 45.26 44.91 o1 ) w2v ( 0 , 0 ) - - - - 25.32 25.25 o2 ) w2v ( 3 , 0 ) - - - - 29.14 29.02 o3 ) w2v ( 11 , 11 ) - - - - 43.31 43.08 Table 2 : Results ( averaged over 10 runs).‚ãÜ : best score per column . 
Bold : best score per section . 
- : does not apply as the method does not produce utterance embeddings . 
larger contexts always brings improvement . 
Qualitative results . 
We visualize in App . 
A that the three self - attention mechanisms behave in a cooperative manner to produce a meaningful utterance representation . 
We also inspect the closest utterances to a given query utterance in App . 
D. We also visualize the time - aware self - attention coefÔ¨Åcients in Fig . 
6 , and Ô¨Ånd that interestingly , the distributions over the pre and post - context are not symmetric . 
Indeed , only the utterances immediately following Ut(t+ 1‚Üít+ 5 ) seem to matter , while the attention weights are much more uniform across the utterances preceding Ut . 
This suggests that in dialogues , considering a long history of preceding utterances helps understanding the current one . 
Overall , the three terms ( see Eq . 
14 ) altogether do produce a universal function that decreases as time distance increases , which is in accordance with intuition . 
SimpliÔ¨Åed task . 
Finally , we also experimented on a much simpler task , where only the communities of type ABSTRACT were considered . 
This makes ACD much simpler , because most of the overlapping communities are of the other types ( see Table 1 ) . 
For this simpliÔ¨Åed task , we have 1147 unique -1 - 2 - 3 - 4 - 5 - 6 - 7 - 8 - 9 - 10 - 11 t+1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 + 110.000.050.100.150.200.250.300.35   convex linear concav universalFigure 6 : Normalized time - aware self - attention weights for pre and post - contexts , averaged over 10 runs . 
communities , of which 78.99 % are disjoint . 
our model achieves 72.09 in terms of P@k = vand 55.67 in terms of Omega Index when |Q|=v . 
P , R , F 1@k= 15 are respectively equal to 55.07 , 74.37 , and 54.00 , and the Omega Index is 54.30 when|Q|= 8 . 
8 Conclusion This paper proposes one of the Ô¨Årst applications of energy - based learning to ACD . 
Using the siamese and triplet meta - architectures , we showed that our novel contextual utterance encoder learns better distance and communities than state - of - the - art competitors.320 Acknowledgments This research was supported in part by the OpenPaaS::NG and LinTo ( Lorr ¬¥ e et al . 
, 2019 ) projects . 
Abstract Modern task - oriented dialog systems need to reliably understand users ‚Äô intents . 
Intent detection is even more challenging when moving to new domains or new languages , since there is little annotated data . 
To address this challenge , we present a suite of pretrained intent detection models which can predict a broad range of intended goals from many actions because they are trained on wikiHow , a comprehensive instructional website . 
Our models achieve state - of - the - art results on the Snips dataset , the Schema - Guided Dialogue dataset , and all 3 languages of the Facebook multilingual dialog datasets . 
Our models also demonstrate strong zero- and few - shot performance , reaching over 75 % accuracy using only 100 training examples in all datasets.1 1 Introduction Task - oriented dialog systems like Apple ‚Äôs Siri , Amazon Alexa , and Google Assistant have become pervasive in smartphones and smart speakers . 
To support a wide range of functions , dialog systems must be able to map a user ‚Äôs natural language instruction onto the desired skill or API . 
Performing this mapping is called intent detection . 
Intent detection is usually formulated as a sentence classiÔ¨Åcation task . 
Given an utterance ( e.g. ‚Äú wake me up at 8 ‚Äù ) , a system needs to predict its intent ( e.g. ‚Äú Set an Alarm ‚Äù ) . 
Most modern approaches use neural networks to jointly model intent detection and slot Ô¨Ålling ( Xu and Sarikaya , 2013 ; Liu and Lane , 2016 ; Goo et al . 
, 2018 ; Zhang et al . 
, 2019 ) . 
In response to a rapidly growing range of services , more attention has been given to zero - shot intent detection ( Ferreira et al . 
, 2015a , b ; Yazdani and Henderson , 2015 ; Chen et al . 
, 2016 ; Kumar et al . 
, 2017 ; Gangadharaiah and 1The data and models are available at https:// github.com/zharry29/wikihow-intent .Narayanaswamy , 2019 ) . 
While most existing research on intent detection proposed novel model architectures , few have attempted data augmentation . 
One such work ( Hu et al . 
, 2009 ) showed that models can learn much knowledge that is important for intent detection from massive online resources such as Wikipedia . 
We propose a pretraining task based on wikiHow , a comprehensive instructional website with over 110,000 professionally edited articles . 
Their topics span from common sense such as ‚Äú How to Download Music ‚Äù to more niche tasks like ‚Äú How to Crochet a Teddy Bear . 
‚Äù We observe that the header of each step in a wikiHow article describes an action and can be approximated as an utterance , while the title describes a goal and can be seen as an intent . 
For example , ‚Äú Ô¨Ånd good gas prices ‚Äù in the article ‚Äú How to Save Money on Gas ‚Äù is similar to the utterance ‚Äú where can I Ô¨Ånd cheap gas ? ‚Äù with the intent ‚Äú Save Money on Gas . 
‚Äù Hence , we introduce a dataset based on wikiHow , where a model predicts the goal of an action given some candidates . 
Although most of wikiHow ‚Äôs domains are far beyond the scope of any present dialog system , models pretrained on our dataset would be robust to emerging services and scenarios . 
Also , as wikiHow is available in 18 languages , our pretraining task can be readily extended to multilingual settings . 
Using our pretraining task , we Ô¨Åne - tune transformer language models , achieving state - of - the - art results on the intent detection task of the Snips dataset ( Coucke et al . 
, 2018 ) , the Schema - Guided Dialog ( SGD ) dataset ( Rastogi et al . 
, 2019 ) , and all 3 languages ( English , Spanish , and Thai ) of the Facebook multilingual dialog datasets ( Schuster et al . 
, 2019 ) , with statistically signiÔ¨Åcant improvements . 
As our accuracy is close to 100 % on all these datasets , we further experiment with zero- or few - shot settings . 
Our models achieve over 70 % accuracy with no in - domain training data on Snips328 and SGD , and over 75 % with only 100 training examples on all datasets . 
This highlights our models ‚Äô ability to quickly adapt to new utterances and intents in unseen domains . 
2 WikiHow Pretraining Task 2.1 Corpus We crawl the wikiHow website in English , Spanish , and Thai ( the languages were chosen to match those in the Facebook multilingual dialog datasets ) . 
We deÔ¨Åne the goal of each artcle as its title stripped of the preÔ¨Åx ‚Äú How to ‚Äù ( and its equivalent in other languages ) . 
We extract a set of steps for each article , by taking the bolded header of each paragraph . 
2.2 WikiHow Pretraining Dataset A wikiHow article ‚Äôs goal can approximate an intent , and each step in it can approximate an associated utterance . 
We formulate the pretraining task as a 4choose-1 multiple choice format : given a step , the model infers the correct goal among 4 candidates . 
For example , given the step ‚Äú let check - in agents and Ô¨Çight attendants know if it ‚Äôs a special occasion ‚Äù and the candidate goals : A. Get Upgraded to Business Class B. Change a Flight Reservation C. Check Flight Reservations D. Use a Discount Airline Broker the correct goal would be A. This is similar to intent detection , where a system is given a user utterance and then must select a supported intent . 
We create intent detection pretraining data using goal - step pairs from each wikiHow article . 
Each article contributes at least one positive goal - step pair . 
However , it is challenging to sample negative candidate goals for a given step . 
There are two reasons for this . 
First , random sampling of goals correctly results in true negatives , but they tend to be so distant from the positive goal that the classiÔ¨Åcation task becomes trivial and the model does not learn sufÔ¨Åciently . 
Second , if we sample goals that are similar to the positive goal , then they might not be true negatives , since there are many steps in wikiHow often with overlapping goals . 
To sample high - quality negative training instances , we start with the correct goal and search in its article ‚Äôs ‚Äú related articles ‚Äù section for an article whose title has the least lexical overlap with the current goal . 
We recursively do this until we have enough candidates . 
Empirically , examples created this way are mostly clean , with an example shown above . 
We select onepositive goal - step pair from each article by picking its longest step . 
In total , our wikiHow pretraining datasets have 107,298 English examples , 64,803 Spanish examples , and 6,342 Thai examples . 
3 Experiments We Ô¨Åne - tune a suite of off - the - shelf language models pretrained on our wikiHow data , and evaluate them on 3 major intent detection benchmarks . 
3.1 Models We Ô¨Åne - tune a pretrained RoBERTa model ( Liu et al . 
, 2019 ) for the English datasets and a pretrained XLM - RoBERTa model ( Conneau et al . 
, 2019 ) for the multilingual datasets . 
We cast the instances of the intent detection datasets into a multiple - choice format , where the utterance is the input and the full set of intents are the possible candidates , consistent with our wikiHow pretraining task . 
For each model , we append a linear classiÔ¨Åcation layer with cross - entropy loss to calculate a likelihood for each candidate , and output the candidate with the maximum likelihood . 
For each intent detection dataset in any language , we consider the following settings : + in - domain ( + ID ): a model is only trained on the dataset ‚Äôs in - domain training data ; + wikiHow + in - domain ( + WH+ID ): a model is Ô¨Årst trained on our wikiHow data in the corresponding language , and then trained on the dataset ‚Äôs indomain training data ; + wikiHow zero - shot ( + WH 0 - shot ): a model is trained only on our wikiHow data in the corresponding language , and then applied directly to the dataset ‚Äôs evaluation data . 
For non - English languages , the corresponding wikiHow data might suffer from smaller sizes and lower quality . 
Hence , we additionally consider the following cross - lingual transfer settings for non - English datasets : + en wikiHow + in - domain ( + enWH+ID ) , a model is trained on wikiHow data in English , before it is trained on the dataset ‚Äôs in - domain training data ; + en wikiHow zero - shot ( + enWH 0 - shot ) , a model is trained on wikiHow data in English , before it is directly applied to the dataset ‚Äôs evaluation data . 
3.2 Datasets We consider the 3 following benchmarks : The Snips dataset ( Coucke et al . 
, 2018 ) is a single - turn English dataset . 
It is one of the most cited dialog benchmarks in recent years , containing329 Training SizeValid . 
SizeTest SizeNum . 
Intents Snips 2,100 700 N / A 7 SGD 163,197 24,320 42,922 4 FB - en 30,521 4,181 8,621 12 FB - es 3,617 1,983 3,043 12 FB - th 2,156 1,235 1,692 12 Table 1 : Statistics of the dialog benchmark datasets . 
utterances collected from the Snips personal voice assistant . 
While its full training data has 13,784 examples , we Ô¨Ånd that our models only need its smaller training split consisting of 2,100 examples to achieve high performance . 
Since Snips does not provide test sets , we use the validation set for testing and the full training set for validation . 
Snips involves 7 intents , including Add to Playlist , Rate Book , Book Restaurant , Get Weather , Play Music , Search Creative Work , and Search Screening Event . 
Some example utterances include ‚Äú Play the newest melody on Last Fm by Eddie Vinson , ‚Äù ‚Äú Find the movie schedule in the area , ‚Äù etc . 
The Schema - Guided Dialogue dataset ( SGD ) ( Rastogi et al . 
, 2019 ) is a multi - turn English dataset . 
It is the largest dialog corpus to date spanning dozens of domains and services , used in the DSTC8 challenge ( Rastogi et al . 
, 2020 ) with dozens of team submissions . 
Schemas are provided with at most 4 intents per dialog turn . 
Examples of these intents include Buy Movie Tickets for a Particular show , Make a Reservation with the Therapist , Book an Appointment at a Hair Stylist , Browse attractions in a given city , etc . 
At each turn , we use the last 3 utterances as input . 
An example : ‚Äú That sounds fun . 
What other attractions do you recommend ? There is a famous place of worship called Akshardham . 
‚Äù The Facebook multilingual datasets ( FBen / es / th ) ( Schuster et al . 
, 2019 ) is a single - turn multilingual dataset . 
It is the only multilingual dialog dataset to the best of our knowledge , containing utterances annotated with intents and slots in English ( en ) , Spanish ( es ) , and Thai ( th ) . 
It involves 12 intents , including Set Reminder , Check Sunrise , Show Alarms , Check Sunset , Cancel Reminder , Show Reminders , Check Time Left on Alarm , Modify Alarm , Cancel Alarm , Find Weather , Set Alarm , and Snooze Alarm . 
Some example utterances are ‚Äú Is my alarm set for 10 am today ? ‚Äù ‚Äú Colocar una alarma para ma Àúnana a las 3 am , ‚Äù   etc . 
Snips SGD FB - en ( Ren and Xue , 2020 ) .993 N / A .993 ( Ma et al . 
, 2019 ) N / A .948 N / A + in - domain ( + ID ) .990 .942 .993 ( ours ) + WH+ID .994 .951‚Ä† .995‚Ä† ( ours ) + WH 0 - shot .713 .787 .445 Chance .143 .250 .083 Table 2 : The accuracy of intent detection on English datasets using RoBERTa . 
State - of - the - art performances are in bold ; ‚Ä†indicates statistically signiÔ¨Åcant improvement from the previous state - of - the - art . 
FB - en FB - es FB - th ( Ren and Xue , 2020 ) .993 N / A N / A ( Zhang et al . 
, 2019 ) N / A .978 .967 + in - domain ( + ID ) .993 .986 .962 ( ours ) + WH+ID .995 .988 .971 ( ours ) + enWH+ID .995 .990‚Ä† .976‚Ä† ( ours ) + WH 0 - shot .416 .129 .119 ( ours ) + enWH 0 - shot .416 .288 .124 Chance .083 .083 .083 Table 3 : The accuracy of intent detection on multilingual datasets using XLM - RoBERTa . 
Statistics of the datasets are shown in Table 1 . 
3.3 Baselines We compare our models with the previous state - ofthe - art results of each dataset : ‚Ä¢Ren and Xue ( 2020 ) proposed a Siamese neural network with triplet loss , achieving state - of - the - art results on Snips and FB - en ; ‚Ä¢Zhang et al . 
( 2019 ) used multi - task learning to jointly learn intent detection and slot Ô¨Ålling , achieving state - of - the - art results on FB - es and FB - th ; ‚Ä¢Ma et al . 
( 2019 ) augmented the data via backtranslation to and from Chinese , achieving state - ofthe - art results on SGD . 
3.4 Modelling Details After experimenting with base and large models , we use RoBERTa - large for the English datasets and XLM - RoBERTa - base for the multilingual dataset for best performances . 
All our models are implemented using the HuggingFace Transformer library2 . 
We tune our model hyperparameters on the validation sets of the datasets we experiment with . 
However , in all cases , we use a uniÔ¨Åed setting 2https://github.com/huggingface/ transformers330 00.20.40.60.81 .953 .470Snips ( RoBERTa ) .531.755SGD ( RoBERTa ) .458.884FB - en ( RoBERTa ) 10 100 1,00000.20.40.60.81.894 .481FB - en ( XLM - RoBERTa ) 10 100 1,000.845 .663 .349FB - es ( XLM - RoBERTa ) 10 100 1,000.853 .851 .341FB - th ( XLM - RoBERTa ) + ID ( ours)+WH+ID ( ours)+enWH+ID Chance Figure 1 : Learning curves of models in low - resource settings . 
The vertical axis is the accuracy of intent detection , while the horizontal axis is the number of in - domain training examples of each task , distorted to log - scale . 
which empirically performs well , using the Adam optimizer ( Kingma and Ba , 2014 ) with an epsilon of1e‚àí8 , a learning rate of 5e‚àí6 , maximum sequence length of 80 and 3 epochs . 
We variate the batch size from 2 to 16 according to the number of candidates in the multiple - choice task , to avoid running out of memory . 
We save the model every 1,000 training steps , and choose the model with the highest validation performance to be evaluated on the test set . 
We run our experiments on an NVIDIA GeForce RTX 2080 Ti GPU , with half - precision Ô¨Çoating point format ( FP16 ) with O1 optimization . 
Each epoch takes up to 90 minutes in the most resource intensive setting , i.e. running a RoBERTa - large on around 100,000 training examples of our wikiHow pretraining dataset . 
3.5 Results The performance of RoBERTa on the English datasets ( Snips , SGD , and FB - en ) are shown in Table 2 . 
We repeat each experiment 20 times , report the mean accuracy , and calculate its p - value against the previous state - of - the - art result , using a one - sample and one - tailed t - test with a signiÔ¨Åcance level of 0.05 . 
Our models achieve state - of - the - art results using the available in - domain training data . 
Moreover , our wikiHow data enables our models todemonstrate strong performances in zero - shot settings with no in - domain training data , implying our models ‚Äô strong potential to adapt to new domains . 
The performance of XLM - RoBERTa on the multilingual datasets ( FB - en , FB - es , and FB - th ) are shown in Table 3 . 
Our models achieve state - of - theart results on all 3 languages . 
While our wikiHow data in Spanish and Thai does improve models ‚Äô performances , its effect is less salient than the English wikiHow data . 
Our experiments above focus on settings where all available in - domain training data are used . 
However , modern task - oriented dialog systems must rapidly adapt to burgeoning services ( e.g. Alexa Skills ) in different languages , where little training data are available . 
To simulate low - resource settings , we repeat the experiments with exponentially increasing number of training examples up to 1,000 . 
We consider the models trained only on in - domain data ( + ID ) , those Ô¨Årst pretrained on our wikiHow data in corresponding languages ( + WH+ID ) , and those Ô¨Årst pretrained on our English wikiHow data ( + enWH+ID ) for FB - es and FB - th . 
The learning curves of each dataset are shown in Figure 1 . 
Though the vanilla transformers models ( + ID ) achieve close to state - of - the - art performance with access to the full training data ( see Table 2331 and 3 ) , they struggle in the low - resource settings . 
When given up to 100 in - domain training examples , their accuracies are less than 50 % on most datasets . 
In contrast , our models pretrained on our wikiHow data ( + WH+ID ) can reach over 75 % accuracy given only 100 training examples on all datasets . 
4 Discussion and Future Work As our model performances exceed 99 % on Snips and FB - en , the concern arises that these intent detection datasets are ‚Äú solved ‚Äù . 
We address this by performing error analysis and providing future outlooks for intent detection . 
4.1 Error Analysis Our model misclassiÔ¨Åes 7 instances in the Snips test set . 
Among them , 6 utterances include proper nouns on which intent classiÔ¨Åcation is contingent . 
For example , the utterance ‚Äú please open Zvooq ‚Äù assumes the knowledge that Zvooq is a streaming service , and its labelled intent is ‚Äú Play Music . 
‚Äù Our model misclassiÔ¨Åes 43 instances in the FBen test set . 
Among them , 10 has incorrect labels : e.g. the labelled intent of ‚Äú have alarm go off at 5 pm ‚Äù is ‚Äú Show Alarms , ‚Äù while our model prediction ‚Äú Set Alarm ‚Äù is in fact correct . 
28 are ambiguous : e.g. the labelled intent of ‚Äú repeat alarm every weekday ‚Äù is ‚Äú Set Alarm , ‚Äù whereas that of ‚Äú add an alarm for 2:45 on every Monday ‚Äù is ‚Äú Modify Alarm . 
‚Äù We only Ô¨Ånd 1 example an interesting edge case : the gold intent of ‚Äú remind me if there will be a rain forecast tomorrow ‚Äù is ‚Äú Find Weather , ‚Äù while our model incorrectly chooses ‚Äú Set Reminder . 
‚Äù By performing manual error analyses on our model predictions , we observe that most misclassiÔ¨Åed examples involve ambiguous wordings , wrong labels , or obscure proper nouns . 
Our observations imply that Snips and FB - en might be too easy to effectively evaluate future models . 
4.2 Open - Domain Intent Detection State - of - the - art models now achieve greater than 99 % percent accuracy on standard benchmarks for intent detection . 
However , intent detection is far from being solved . 
The standard benchmarks only have a dozen intents , but future dialog systems will need to support many more functions with intents from a wide range of domains . 
To demonstrate that our pretrained models can adapt to unseen , open - domain intents , we hold out 5,000 steps ( as utterances ) with their corresponding goals ( as intents ) from our wikiHow dataset as a proxy of anintent detection dataset with more than 100,000 possible intents ( all goals in wikiHow ) . 
For each step , we sample 100 goals with the highest embedding similarity to the correct goal , as most other goals are irrelevant . 
We then rank them for the likelihood that the step helps achieve them . 
Our RoBERTa model achieves a mean reciprocal rank of 0.462 and a 36 % accuracy of ranking the correct goal Ô¨Årst . 
As a qualitative example , given the step ‚Äú Ô¨Ånd the order that you want to cancel , ‚Äù the top 3 ranked steps are ‚Äú Cancel an Order on eBay ‚Äù , ‚Äú Cancel an Online Order ‚Äù , ‚Äú Cancel an Order on Amazon . 
‚Äù This hints that our pretrained models ‚Äô can work with a much wider range of intents than those in current benchmarks , and suggests that future intent detection research should focus on open domains , especially those with little data . 
5 Conclusion By pretraining language models on wikiHow , we attain state - of - the - art results in 5 major intent detection datasets spanning 3 languages . 
The wideranging domains and languages of our pretraining resource enable our models to excel with few labelled examples in multilingual settings , and suggest open - domain intent detection is now feasible . 
Acknowledgments This research is based upon work supported in part by the DARPA KAIROS Program ( contract FA8750 - 19 - 2 - 1004 ) , the DARPA LwLL Program ( contract FA8750 - 19 - 2 - 0201 ) , and the IARPA BETTER Program ( contract 2019 - 19051600004 ) . 
Approved for Public Release , Distribution Unlimited . 
The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the ofÔ¨Åcial policies , either expressed or implied , of DARPA , IARPA , or the U.S. Government . 
We thank the anonymous reviewers for their valuable feedback . 
Abstract This work studies the widely adopted ancestral sampling algorithms for auto - regressive language models , which is not widely studied in the literature . 
We use the quality - diversity ( QD ) trade - off to investigate three popular sampling algorithms ( top- k , nucleus and tempered sampling ) . 
We focus on the task of open - ended language generation . 
We Ô¨Årst show that the existing sampling algorithms have similar performance . 
After carefully inspecting the transformations deÔ¨Åned by different sampling algorithms , we identify three key properties that are shared among them : entropy reduction , order preservation , and slope preservation . 
To validate the importance of the identiÔ¨Åed properties , we design two sets of new sampling algorithms : one set in which each algorithm satisÔ¨Åes all three properties , and one set in which each algorithm violates at least one of the properties . 
We compare their performance with existing sampling algorithms , and Ô¨Ånd that violating the identiÔ¨Åed properties could lead to drastic performance degradation , as measured by the Q - D trade - off . 
On the other hand , we Ô¨Ånd that the set of sampling algorithms that satisÔ¨Åes these properties performs on par with the existing sampling algorithms.1 1 Introduction A language model ( LM ) is a central module for natural language generation ( NLG ) tasks ( Young et al . 
, 2018 ) such as machine translation ( Wu et al . 
, 2018 ) , dialogue response generation ( Li et al . 
, 2017 ) , image captioning ( Lin et al . 
) , and related tasks . 
Given a trained LM , Ô¨Ånding the best way to generate a sample from it has been an important challenge for NLG applications . 
‚àóEqual contribution . 
1Our data and code are available at https://github.com/moinnadeem/ characterizing - sampling - algorithms . 
Figure 1 : Human evaluation ( y - axis : quality , x - axis : diversity , both are the bigger the better ) shows that the generation performance of existing sampling algorithms are on par with each other . 
Decoding , i.e. , Ô¨Ånding the most probable output sequence from a trained model , is a natural principle for generation . 
The beam - search decoding algorithm approximately Ô¨Ånds the most likely sequence by performing breadth-Ô¨Årst search over a restricted search space . 
It has achieved success in machine translation , summarization , image captioning , and other subÔ¨Åelds . 
However , in the task of open - ended language generation ( which is the focus of this work ) , a signiÔ¨Åcant degree of diversity is required . 
For example , conditioned on the prompt ‚Äú The news says that ... ‚Äù , the LM is expected to be able to generate a wide range of interesting continuations . 
While the deterministic behavior of decoding algorithms could give high - quality samples , they suffer from a serious lack of diversity . 
This need for diversity gives rise to a wide adoption of various sampling algorithms . 
Notably , topksampling ( Fan et al . 
, 2018 ) , nucleus sampling ( Holtzman et al . 
, 2020 ) , and tempered sampling ( Caccia et al . 
, 2020 ) have been used in open - ended334 generation ( Radford et al . 
, 2018 ; Caccia et al . 
, 2020 ) , story generation ( Fan et al . 
, 2018 ) , and dialogue response generation ( Zhang et al . 
, 2020b ) . 
However , the sampling algorithm and the hyperparameter are usually chosen via heuristics , and a comprehensive comparison between existing sampling algorithm is lacking in the literature . 
More importantly , the underlying reasons behind the success of the existing sampling algorithms still remains poorly understood . 
In this work , we begin by using the qualitydiversity ( Q - D ) trade - off ( Caccia et al . 
, 2020 ) to compare the three existing sampling algorithms . 
For automatic metrics , we use the BLEU score for quality and n - gram entropy for diversity . 
We also correlate these automatic metrics with human judgements . 
The Ô¨Årst observation we draw is that top - k , nucleus and tempered sampling perform on par in the Q - D trade - off , as shown in Figure 1 . 
Motivated by this result , we extract three key properties by inspecting the transformations deÔ¨Åned by the sampling algorithms : ( 1 ) entropy reduction , ( 2 ) order preservation and ( 3 ) slope preservation . 
We prove all three properties hold for the three existing sampling algorithms . 
We then set out to systematically validate the importance of the identiÔ¨Åed properties . 
To do so , we design two sets of new sampling algorithms in which each algorithm either violates one of the identiÔ¨Åed properties , or satisÔ¨Åes all properties . 
Using the Q - D trade - off , we compare their efÔ¨Åcacy against existing algorithms , and Ô¨Ånd that violating these identiÔ¨Åed properties could result in signiÔ¨Åcant performance degradation . 
More interestingly , we Ô¨Ånd that the set of sampling algorithms that satisÔ¨Åes these properties has generation performance that matches the performance of existing sampling algorithms . 
2 Sampling Algorithms for Autoregressive Language Models 2.1 Autoregressive Language Modeling The task of autoregressive language modeling is to learn the probability distribution of the ( l+ 1)-th wordWl+1 in a sentence Wconditioned on the word history W1 : l:= ( W1, ... ,Wl)and contextC. Here , we use Wi‚ààVto denote a discrete random variable distributed across a Ô¨Åxed vocabulary V. In this work , the vocabulary is constructed on subword level ( Sennrich et al . 
, 2016 ) . 
Given a training set D , maximum likelihood es - timation ( MLE ) has been the most popular framework to train an autoregressive LM ( Mikolov et al . 
, 2010 ) . 
MLE training minimizes the negative loglikelihood ( NLL ) objective below : LMLE=1 |D|/summationdisplay ( W , C)‚ààD‚àíŒ£L‚àí1 l=0logPŒ∏(Wl+1|W1 : l , C ) , ( 1 ) whereŒ∏denotes model parameters , and PŒ∏(¬∑|W1 : l ) denotes the conditional model distribution of Wl+1 given a preÔ¨Åx W1 : l. For simplicity , we assume all sentences are of length Lin the formulations . 
Since this work focuses on sampling from a given model instead of training it , in the rest of the paper , we abbreviatePŒ∏(¬∑)asP(¬∑)for brevity . 
2.2 Existing Sampling Algorithms Given a trained LM and a context C , an ancestral sampling algorithm seeks to generate a sequence fromP(W|C)by sampling token - by - token from a transformed version of P(Wl+1|W1 .. l , C ) . 
We now review and formulate three popular sampling algorithms : top - k(Fan et al . 
, 2018 ) , nucleus ( Holtzman et al . 
, 2020 ) , and tempered ( Ackley et al . 
, 1985 ; Caccia et al . 
, 2020 ) sampling . 
We view these algorithms as different transformations applied to the distribution P(Wl+1|W1 .. l , C ) . 
First , we treat the conditional distribution P(Wl+1|W1 .. l , C)as a sorted vector pof length|V| . 
By sorting , we rearrange the elements such that if i < j‚Üípi>=pj.2We list the transformations and their intuition below : DeÔ¨Ånition 2.1 . 
( Top - k ) In top - ksampling , we only sample from the top Ktokens : ÀÜpi = pi ¬∑ 1{i‚â§K}/summationtextK j=1pj , ( 2 ) where 1is the indicator function , and K(1‚â§K‚â§ |V| ) is the hyperparameter . 
DeÔ¨Ånition 2.2 . 
( Nucleus ) With a hyperparameter P(0 < P‚â§1 ) , in nucleus sampling , we sample from the top- Pmass of p : ÀÜpi = p / prime i / summationtext|V| j=1p / prime j , ( 3 ) wherep / prime i = pi ¬∑ 1{/summationtexti‚àí1 j=1pj < P } . 
2The token indexes are also permutated accordingly.335 DeÔ¨Ånition 2.3 . 
( Tempered ) In tempered sampling , the log probabilities are scaled by1 T : ÀÜpi = exp(log(pi)/T ) /summationtext|V| j=1exp(log(pj)/T ) . 
( 4 ) In this work , we assume 0 < T < 1 , i.e. , the distribution is only made sharper3 . 
We additionally experiment with a combined version of top- kand tempered sampling : DeÔ¨Ånition 2.4 . 
( Tempered Top- k ) We combine the transformation deÔ¨Åned by top- kand tempered sampling : ÀÜpi = p / prime i / summationtext|V| j=1p / prime j , ( 5 ) wherep / prime i= exp(log(pi)/T ) ¬∑ 1{i‚â§K } . 
We set 1‚â§K‚â§|V|and0 < T < 1 . 
Throughout this work we use ÀÜpto denote the normalized version of the transformed distribution . 
All algorithms have hyperparameters to control the entropy of the transformed distribution . 
For example , Kin top - ksampling controls the size of the support of the resulting distribution . 
We will formalize this statement in Property 1 below . 
3 Properties of Sampling Algorithms As we will show in Section 5.1 ( also Figure 1 ) , top - k , nucleus and tempered sampling perform on par with each other under our evaluation . 
This key observation makes us question : What are the core principles underlying the different algorithms that lead to their similar performance ? To answer this question , in this section , we identify three core properties that are provably shared by the existing sampling algorithms . 
We then design experiments to validate their importance . 
3.1 Identifying Core Properties By inspecting the transformations listed in DeÔ¨Ånition 2.1 , 2.2 and 2.3 , we extract the following three properties : Property 1 . 
( Entropy Reduction ) : The transformation strictly decrease the entropy of the distribution . 
Formally , H(ÀÜp)<H(p ) , whereH(p ) = ‚àí/summationtext|V| i=1pilogpi . 
3One could also use T > 1 , but it does not work well in practice . 
Property 2 . 
( Order Preservation ) : The order of the elements in the distribution is preserved . 
Formally , pi‚â•pj‚ÜíÀÜpi‚â•ÀÜpj . 
Property 3 . 
( Slope Preservation ) : The ‚Äú slope ‚Äù of the distribution is preserved . 
Formally , ‚àÄÀÜpi > ÀÜpj>ÀÜpk>0(i.e . 
, they are not truncated ) , we have logpi‚àílogpj logpj‚àílogpk = log ÀÜpi‚àílog ÀÜpj log ÀÜpj‚àílog ÀÜpk . 
The order preservation property implies that truncation can only happen in the tail of the distribution , which aligns with top- kand nucleus sampling . 
The slope preservation property is stronger than the order preservation property in that not only the ordering , but also the relative magnitude of the elements in the distribution needs to be somewhat preserved by the transformation . 
All these three properties are shared by the three existing sampling algorithms : Proposition 1 . 
Property 1 , 2 and 3 hold for the topk , nucleus and tempered sampling transformations formulated in DeÔ¨Ånitions 2.1 , 2.2 and 2.3 . 
Proof . 
See Appendix B. We then set out to validate the importance of these identiÔ¨Åed properties in the aspects of necessityandsufÔ¨Åciency . 
To do so , we design two sets of new sampling algorithms in which each algorithm either violates one of the identiÔ¨Åed properties , or satisÔ¨Åes all properties . 
We list them in the next section . 
3.2 Designed Sampling Algorithms Property - violating algorithms To validate the necessity of each property , we design several sampling algorithms which violate at least one of the identiÔ¨Åed properties . 
In our experiments , we check whether that violation leads to a signiÔ¨Åcant degradation in performance . 
We list them below : DeÔ¨Ånition 3.1 . 
( Target Entropy ) Based on tempered sampling , target entropy sampling tunes the temperature tsuch that the transformed distribution has entropy value equal to the hyperparameter E ( 0 < E‚â§log|V| ) . 
We formulate it below : ÀÜpi = exp(log(pi)/t ) /summationtext|V| j=1exp(log(pj)/t ) , ( 6 ) wheretis selected such that H(ÀÜp ) = E. Target entropy sampling violates entropy reduction , because when H(p)<E , the entropy will be tuned up ( i.e. , H(ÀÜp)>H(p)).336 DeÔ¨Ånition 3.2 . 
( Random Mask ) In random mask sampling , we randomly mask out tokens in the distribution with rate R. We formluate it below : ÀÜpi = p / prime i / summationtext|V| j=1p / prime j , ( 7 ) wherep / prime i = pi ¬∑ 1{i= 1 orui > R}and ui‚àºU(0,1 ) . 
The hyperparameter R(0 < R‚â§1 ) controls the size of the support of the resulting distribution . 
In Appendix A , we show it is crucial that the token which is assigned the largest probability ( p1 ) is never be masked . 
Random mask sampling is different from top- k or nucleus sampling in that the masking not only happens in the tail of the distribution . 
Therefore , it violates the order preservation property . 
DeÔ¨Ånition 3.3 . 
( Noised Top- k ) We add a sorted noise distribution to the result from top- Ktransformation , and the weight of the noise distribution is controlled by a hyperparameter W(0‚â§W‚â§1 ) . 
We formulate it below : ÀÜp= ( 1‚àíW)ÀÜptop - K+Wpnoise - K , ( 8) where pnoise - Kis a uniformly sampled sorted Ksimplex , which satisÔ¨Åes / summationtextK i=1pnoise - K i = 1andi < j‚Üípnoise - K i‚â•pnoise - K j‚â•0 . 
The sorted nature of the noise distribution pnoise - Kmaintains order preservation . 
However , it violates slope preservation , and the noise weight Wcontrols the degree of the violation . 
Property - satisfying algorithms To validate the sufÔ¨Åciency of the identiÔ¨Åed properties , we design two new sampling algorithms for which all three properties hold . 
And in our experiments we check whether their performance is on par with the existing sampling algorithms . 
We list them below : DeÔ¨Ånition 3.4 . 
( Random Top- k ) We design a randomized version of top- ksampling : At each time step , we sample a uniformly random Ô¨Çoat number u‚àºU(0,1 ) , and use it to specify a top- ktruncation : ÀÜpi = pi ¬∑ 1{i‚â§k}/summationtextk j=1pj , ( 9 ) wherek=‚åä1 + M¬∑u‚åã. The hyperparameter M ( 1‚â§M < |V| ) controls the maximum truncation threshold . 
DeÔ¨Ånition 3.5 . 
( Max Entropy ) Max entropy sampling is similar to target entropy sampling ( Definition 3.1 ) . 
However to match entropy reduction ( Property 1 ) , we only tune the temperature whenH(p)>E , whereEis the hyperparameter ( 0 < E‚â§log|V| ): ÀÜpi=Ô£± Ô£≤ Ô£≥exp(log(pi)/t)/summationtext|V| j=1exp(log(pj)/t),ifH(p)>E pi , otherwise , ( 10 ) wheretis selected so thatH(ÀÜp ) = E. It is easy to prove that Property 1 , 2 , and 3 holds for the transformations deÔ¨Åned by random top- k and max entropy sampling , and we omit the proof for brevity . 
4 Experiment Setup In this section , we Ô¨Årst establish evaluation protocols , and then describe the model and data we use for the open - ended language generation task . 
4.1 Evaluation via the Q - D Trade - off How to efÔ¨Åciently measure the generation performance of a NLG model has been an important open question . 
Most existing metrics either measure the quality aspect ( e.g. BLEU score ) or the diversity ( e.g. n - gram entropy ) aspect . 
To make the situation more complicated , each sampling algorithm has its own hyperparameters which controls the trade - off between quality and diversity . 
To address the challenges above , we adopt the quality - diversity trade - off proposed by Caccia et al . 
( 2020 ) . 
In the Q - D trade - off , we perform a Ô¨Ånegrained sweep of hyperparameters for each sampling algorithm , and compute the quality and diversity score for each conÔ¨Åguration . 
We report two pairs of Q / D metrics , with one pair using automatic evaluation and the other using human evaluation . 
In the next two sections , we describe the metrics we use , and refer readers to Caccia et al . 
( 2020 ) for more intuition behind the Q - D trade - off . 
4.1.1 Automatic Evaluation For automatic metrics , we adopt the corpus - BLEU ( Yu et al . 
, 2016 ) metric to measure quality and the self - BLEU ( Zhu et al . 
, 2018 ) metric to measure diversity . 
We formulate them below . 
Given a batch of generated sentences Sgenand a batch of sentences from ground - truth data as referencesSref , corpus - BLEU returns the average337 BLEU score ( Papineni et al . 
, 2002 ) of every model generated sentence against the reference set : corpus - BLEU ( Sgen , Sref ) = 1 |Sgen|/summationdisplay W‚ààSgenBLEU ( W , S ref ) . 
( 11 ) A higher corpus - BLEU score means that the generated sequences has better quality in that it has higher ngram - level overlap with the reference data . 
Based on the same intuition , we deÔ¨Åne the selfBLEU metric to quantify the diversity aspect : self - BLEU ( Sgen ) = corpus - BLEU ( Sgen , Sgen),(12 ) where a lower self - BLEU score means that the samples have better diversity . 
In our experiments , we feed the Ô¨Årst ten subwords of every sample from test set to the model , and compare the model - generated sequences to the reference samples in the validation set . 
We use 10,000 samples to compute corpus - BLEU or selfBLEU , i.e. ,|Sgen|=|Sref|= 10,000 . 
Automatic evaluation enables us to do a Ô¨Ånegrained sweep of the hyperparameters for each sampling algorithm , and compare them in the qualitydiversity trade - off . 
However , observations from automatic evaluation could be misaligned with human evaluation ( Belz and Reiter , 2006 ) . 
Therefore , we conÔ¨Årm our key observations with human evaluation . 
4.1.2 Human Evaluation Quality We ask a pool of 602 crowdworkers on Amazon Mechanical Turk to evaluate various sampling conÔ¨Ågurations in the quality aspect . 
Each worker is presented a set of ten samples along with the prompts ( preÔ¨Åxes ) . 
They are then asked to rate how likely the sentence would appear in a news article between 0 and 5 ( Invalid , Confusing , UnspeciÔ¨Åc , Average , Expected , and Very Expected respectively ) . 
We focus on the Gigaword dataset for human evaluation since news articles are ubiquitous and do not often require expert knowledge for quality judgement . 
For each conÔ¨Åguration ( sampling algorithm and hyperparameter pair ) we ask crowdworkers to rate 200 samples in total . 
To get an accurate rating for each sample , we enlist 25 different crowdworkers to rate each sample . 
We report mean and standard deviation from 5 independent runs ( each with 40 samples ) as error bar . 
By manual inspection , we Ô¨Ånd that the time spent in the annotations is a good indicator of the qualityof the rating . 
Therefore , we estimate the human judgement score for a sample as the average rating of the 20 crowdworkers ( out of 25 ) who took the most time to rate the samples . 
We provide further details about our setup in Appendix C and D. Diversity It is difÔ¨Åcult for human annotators to estimate diversity of text ( Hashimoto et al . 
, 2019 ) . 
Therefore , we use the n - gram entropy metric ( Zhang et al . 
, 2018 ; He and Glass , 2019 ) . 
Given Sgenwhich contains a large number of samples , we measure its diversity using the following formulation : Hn - gram(Sgen ) = /summationdisplay g‚ààGn‚àír(g ) logr(g),(13 ) whereGnis the set of all n - grams that appeared inSgen , andr(g)refers to the ratio ( frequency ) of n - gramgw.r.t . 
all n - grams in the Sgen . 
For the estimation of n - gram entropy , we generate 50,000 samples from each sampling conÔ¨Åguration . 
We will report human quality score either paired with n - gram entropy or with self - BLEU as diversity metric . 
We Ô¨Ånd they give similar observations . 
4.2 Model and Datasets We separately Ô¨Åne - tune GPT2 - small ( Radford et al . 
, 2018 ; Wolf et al . 
, 2019 ) ( 110 M parameters ) on the Gigaword ( Graff et al . 
, 2003 ; Napoles et al . 
, 2012 ) and the Wikitext-103 ( Merity et al . 
, 2017 ) datasets . 
We use the same tokenization as GPT-2 , and add additional padding and end - of - sequence tokens ( [ EOS ] ) to the sentences . 
To generate a sequence , we feed a length-10 preÔ¨Åx from test data into the Ô¨Åne - tuned GPT-2 model , and use a sampling algorithm to complete the sentence . 
Since shorter samples are more difÔ¨Åcult to judge in quality ( Ippolito et al . 
, 2020 ) , we Ô¨Ålter all generated sentence completions to be between 40 and 50 subwords , and Ô¨Ålter our validation and test set to meet the same requirements . 
To permit validation and test sets that are large enough to preÔ¨Åx 10,000 sentences for the corpus - BLEU metric , we re - chunk the Ô¨Årst 80 % of the Gigaword dataset for the training set , 15 % for validation , and the last 5 % for the test set . 
Similarly , we re - chunk the Ô¨Årst 97 % of the Wikitext-103 dataset for training , and leave 1.5 % for validation and 1.5 % for test . 
5 Empirical Results First , we compare existing sampling algorithms , and then move on to validate the necessity and338 Figure 2 : The performance ( x - axis : quality , y - axis : diversity , both are the smaller the better ) of top- k , nucleus , tempered and tempered top- ksampling are on par on the Gigaword dataset , as shown by automatic evaluation . 
sufÔ¨Åciency of the identiÔ¨Åed properties . 
5.1 Comparison of Existing Algorithms We compare top- k , nucleus , and tempered sampling via automatic and human evaluation . 
We do a Ô¨Åne - grained sweep of hyperparameters for each sampling algorithm on the Gigaword dataset . 
The results are shown in Figure 1 ( human evaluation ) and Figure 2 ( automatic evaluation ) . 
We also show the quality and diversity score for human text in the test data for reference , which is labeled as gold . 
Both automatic and human evaluations demonstrate that the performance of top- k , nucleus and tempered sampling are on par with each other , with no signiÔ¨Åcant gap . 
When the hyperparameters ( K , PandT ) are tuned so that different sampling has the same diversity ( measured by self - BLEU or ngram entropy ) , their quality ( measured by corpusBLEU or human rating ) are close . 
Additionally , we compare tempered top- ksampling with the existing algorithm also in Figure 2 . 
We Ô¨Ånd that adding the tempered transformation only moves top- ksampling along the Q - D tradeoff , instead of yielding a better or a worse sampling algorithm . 
For example , the performance of the K= 500,T= 0.8conÔ¨Åguration for tempered top - ksampling is very close to the K= 30 conÔ¨Åguration for the top- ksampling . 
Motivated by these observations , we identify three core properties ( elaborated in Section 3.1 ) that are shared among the sampling algorithms : entropy reduction , order preservation andslope preservation . 
In the following two sections , we Figure 3 : Automatic evaluation of the noised top- k , target entropy , and random mask sampling proposed to validate the necessity of the identiÔ¨Åed properties . 
The results show that violation of entropy reduction and slope preservation could lead to drastic performance degradation , while the order preservation property could be further relaxed . 
present experiments validating the necessity or sufÔ¨Åciency aspect of the properties . 
5.2 Property - violating Algorithms In Figure 3 , we compare the generation performance of the property - violating sampling algorithms ( designed in Section 3.2 ) , against the existing algorithms using automatic evaluation on the Gigaword dataset . 
We make the following observations : First , the target entropy sampling , which violates entropy reduction , has signiÔ¨Åcantly worse performance ; Second , even with small noise weight W , the performance of noised top- ksampling degrades from the original top- ksampling , and the gap becomes larger as Wincreases ; Last , the random mask sampling is on par with the existing sampling algorithms in performance . 
We further conÔ¨Årm this observation with human evaluation in Figure 5 . 
These results suggest that the violation of entropy reduction or slope preservation could lead to drastic performance degradation . 
On the other hand , the competitive performance of random mask sampling suggests that order preservation could be further relaxed . 
In the next section , we investigate the sufÔ¨Åciency aspect of the identiÔ¨Åed properties . 
5.3 Property - satisfying Algorithms We now compare the generation performance of the property - satisfying sampling algorithms ( designed in Section 3.2 ) with the existing sampling algorithms . 
The results from the Gigaword dataset339 Sampling Conditional Samples Existing Sampling Algorithms Top - k ( K = 30)steven spielberg ‚Äôs dreamworks movie studio said monday it was Ô¨Åling a lawsuit , accusing us studio executives of defrauding hundreds of thousands of dollars in refunds and other damages . 
Nucleus ( P = 0.80)steven spielberg ‚Äôs dreamworks movie studio has failed to attract the kind of business and development investors that jeffrey hutchinson dreamed up in the past . 
Tempered ( T = 0.85)steven spielberg ‚Äôs dreamworks movie studio plans to spend the rest of the year producing the high - speed thriller ‚Äù the earth ‚Äôs path ‚Äù and an upcoming sequel , the studio announced on wednesday . 
Property - satisfying Sampling Algorithms Random Top- k ( R = 90)steven spielberg ‚Äôs dreamworks movie studio is planning to make a movie about a young man who is a < unk > , a man who has a dream of being the Ô¨Årst man to be born with the ability to walk on water . 
Max Entropy ( E = 2.75)steven spielberg ‚Äôs dreamworks movie studio has agreed to pay $ # . 
# million to director john nichols ( ¬£ # . 
# million , # # # , a record in the studio circulation ) , the studio announced sunday .. Property - violating Sampling Algorithms Random Mask ( R = 0.75)steven spielberg ‚Äôs dreamworks movie studio scored a big win with a $ # # . 
# million ( euro # # . 
# million ) direct - to - video ( dvds ) deal to develop the # # # # short story ‚Äù the rose garden ‚Äù . 
Noised Top - k ( K=50 , W=5e-3)steven spielberg ‚Äôs dreamworks movie studio is in disarray and has a few directors and a lot of stock involved , leaving it only a matter of time before spielberg ‚Äôs departure from the nobel peace prize . 
Target Entropy ( E = 2.75)steven spielberg ‚Äôs dreamworks movie studio production scored an action boost m boom , nabbing an ‚Äôd after the # # th instal specialization with nominations of fritz , ika , ivan english ape and evlyn mcready . 
Table 1 : Generated sequences with the same preÔ¨Åx steven spielberg ‚Äôs dreamworks movie studio by different sampling algorithms . 
The hyperparameters are chosen such that the algorithms yield roughly the same diversity measured by self - BLEU . 
The poor - quality spans are higlighted in red . 
Figure 4 : The proposed random top- kand max entropy schedulers , which meet the identiÔ¨Åed properties , are on par in performance with existing methods in automatic evaluation on the Gigaword dataset . 
are shown in Figure 3 ( for automatic evaluation ) and Figure 5 ( for human evaluation ) . 
For completeness , we also replicate Figure 5 with self - BLEU as the diversity measure in Appendix F. We also present results from automatic evaluation on the Wikitext-103 dataset in Figure 6 , with consistent observations . 
The evaluations consistently show that the performance of random top- kand max entropy sampling ( and random mask sampling in last section ) is on par with top- k , nucleus , and tempered sampling . 
These results strengthen the importance of the idenFigure 5 : Human evaluation also shows that the proposed sampling algorithms has performance on par with the existing methods on the Gigaword dataset . 
Appendix F repeats this plot with self - BLEU . 
tiÔ¨Åed properties in that , new sampling algorithms could get competitive generation performance as long as they meet the identiÔ¨Åed properties . 
5.4 Qualitative Analysis We list samples from the proposed sampling algorithms and compare them with the existing ones in Table 1 . 
We choose the hyperparameter of each sampling algorithm so that each algorithm exhibits a similar level of diversity ( as measured by selfBLEU ) . 
By manual inspection , we Ô¨Ånd that the quality of samples from property - satisfying sam-340 Figure 6 : Automatic evaluation on the Wikitext-103 dataset : The performance of proposed sampling algorithms are on par with top- k , nucleus , and tempered sampling . 
pling algorithms is on par with samples from the existing algorithms . 
In particular , the samples from random top- k , max entropy , and random masked sampling are all coherent and informative . 
In contrast , the samples from noised top- kand target entropy algorithms , tend to be less semantically and syntatically coherent . 
In particular , the target entropy sampling algorithm , which obtains the lowest quality score measured by corpusBLEU , lacks basic language structure . 
In comparison to target entropy , noised top- kis syntatically coherent , but exhibits logical and factual inconsistencies . 
These observations aligns with the results we get from automatic evaluation . 
6 Related Works Despite the popularity of sampling algorithms in natural language generation , a rigorous comparison or scrutiny of existing algorithms is lacking in the literature . 
Ippolito et al . 
( 2019 ) provides a comparison between sampling and decoding algorithms . 
Holtzman et al . 
( 2020 ) proposes nucleus sampling , and compare it with top- ksampling ( Fan et al . 
, 2018 ) . 
However , only a few hyperparameter conÔ¨Ågurations are tested . 
In Hashimoto et al . 
( 2019 ) and Caccia et al . 
( 2020 ) , temperature sampling is used and the hyperparameter Tis tuned to trade - off between diversity and quality , but it lacks comparisons with other sampling algorithms . 
Welleck et al . 
( 2020 ) studies the consistency of existing sampling and decoding algorithms , without comparing the generation performance . 
In this work we mainly use the quality - diversity trade - off ( Caccia et al . 
, 2020 ) to conduct a compar - ison of different sampling algorithms . 
Parallel to our work , Zhang et al . 
( 2020a ) also uses the qualitydiversity trade - off to compare top- k , nucleus , and tempered sampling . 
Their observation is similar to ours : The performance of the existing algorithms are close with no signiÔ¨Åcant gap . 
More importantly , the underlying reasons for the success of various sampling algorithms remain poorly understood . 
Zhang et al . 
( 2020a ) proposes theselective sampling algorithm , which fails to outperform existing approaches . 
This failed attempt suggests the need for a better understanding of the strengths and weaknesses of existing methods . 
To the best of our knowledge , our work provides the Ô¨Årst systematic characterization of sampling algorithms , where we attribute the success of existing sampling algorithms to a shared set of properties . 
We show that we can propose novel sampling algorithms based on the identiÔ¨Åed properties , and reach competitive generation performance as measured by both automatic and human evaluation . 
7 Limitations and Future Work Our core contribution is the three properties of sampling algorithms that we conjecture are crucial for competitive generation performance . 
While we design a set of experiments to validate their necessity and sufÔ¨Åciency , the observations we make are still empirical . 
We emphasize that it is completely possible that there exists some crucial property , that is yet to be discovered , and can lead to signiÔ¨Åcantly better generation performance . 
Therefore , the exploration of novel sampling algorithms ( Zhang et al . 
, 2020a ) should still be encouraged . 
On the other hand , to provide a comprehensive study , we focus on the open - ended language generation task with the GPT-2 model . 
As future work , it would be interesting to check whether our observations also hold on other tasks such story generation or dialogue response generation , or with weaker language models in low - resource setting . 
8 Conclusion This work studies sampling algorithms for the openended language generation task . 
We show that the existing algorithms , namely top- k , nucleus , and tempered sampling , have similar generation performance as measured by the quality - diversity tradeoff evaluation . 
Motivated by this result , we identify three key properties that we prove are shared by341 the existing algorithms . 
To validate the importance of these identiÔ¨Åed properties , we design a set of new sampling algorithms , and compare their performance with the existing sampling algorithms . 
We Ô¨Ånd that violation of the identiÔ¨Åed properties may lead to drastic performance degradation . 
On the other hand , we propose several novel algorithms , namely random top- kand max entropy sampling , that meet the identiÔ¨Åed properties . 
We Ô¨Ånd that their generation performance is on par with the existing algorithms . 
Acknowledgments The authors sincerely thank Yixin Tao , Jingzhao Zhang and Yonatan Belinkov for useful discussions . 
This work was partly supported by Samsung Advanced Institute of Technology ( Next Generation Deep Learning : from pattern recognition to AI ) , Samsung Electronics ( Improving Deep Learning using Latent Structure ) . 
Kyunghyun Cho thanks CIFAR , Naver , eBay , NVIDIA and Google for their support . 
This research was sponsored in part by the United States Air Force Research Laboratory and was accomplished under Cooperative Agreement Number FA8750 - 19 - 2 - 1000 . 
The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the ofÔ¨Åcial policies , either expressed or implied , of the United States Air Force or the U.S. Government . 
The U.S. Government is authorized to reproduce and distribute reprints for Government purposes not withstanding any copyright notation herein . 
A Auxiliary Plots We show the importance of preserving the token with the largest probability ( p1 ) in the proposed random mask sampling . 
For comparison , we relax the constraint and deÔ¨Åne the random mask - all sampling : DeÔ¨Ånition A.1 . 
( Random Mask - all ) The only difference between random mask - all sampling and random mask sampling is that we allow the p1token to be masked . 
We formulate it below : ÀÜpi = p / prime i / summationtext|V| j=1p / prime j , ( 14 ) wherep / prime i = pi ¬∑ 1{ui > R}andui‚àºU(0,1 ) . 
In Figure 7 , we show that if p1is allowed to be masked , the generation performance will be seriously degraded . 
Figure 7 : The random mask - all sampling , where p1is allowed to be masked , is shown to have worse performance than the random mask sampling . 
The dataset is Giagword . 
B Proof for Proposition 1 In this section we prove Proposition 1 . 
Firstly , it is straightforward to prove that Property 2 ( order preservation ) holds for the top- k , nucleus and tempered sampling and we omit the proof here . 
For Property 3 ( slope preservation ) , it holds trivially for nucleus and top- ksampling . 
We prove it for tempered sampling in the following lemma : Lemma B.1 . 
Property 3 holds for tempered sampling ( DeÔ¨Ånition 2.3 ) . 
Proof . 
Remember that the tempered sampling with hyperparameter TdeÔ¨Ånes the follow transformation : ÀÜpi = p / prime i / summationtext jp / prime j , wherep / prime i= exp(log(pi)/T).We setZ=/summationtext jp / prime j , then‚àÄÀÜpi>ÀÜpj>ÀÜpk>0we have log ÀÜpi‚àílog ÀÜpj log ÀÜpj‚àílog ÀÜpk = logp / prime i‚àílogZ‚àílogp / prime j+ logZ logp / prime j‚àílogZ‚àílogp / prime k+ logZ = logp / prime i‚àílogp / prime j logp / prime j‚àílogp / prime k(logZis cancelled ) = log(pi)/T‚àílog(pj)/T log(pj)/T‚àílog(pk)/T = log(pi)‚àílog(pj ) log(pj)‚àílog(pk)(15 ) Only Property 1 ( entropy reduction ) is left . 
We now prove it holds for top- k/ nucleus sampling : Lemma B.2 . 
Property 1 holds for transformations deÔ¨Åned by top- kor nucleus sampling ( DeÔ¨Ånition 2.1 and 2.2 ) . 
Proof . 
We Ô¨Årst consider the change of entropy when the token with the smallest probability ( p|V| ) is removed from the original distribution ( ÀÜpi= pi / summationtext|V|‚àí1 j=1pi,1‚â§i<|V| ): ‚àíH(p ) = V / summationdisplay i=1pilogpi = V‚àí1 / summationdisplay i=1pilogpi+p|V|logp|V| = ( 1‚àíp|V|)V‚àí1 / summationdisplay i=1pi 1‚àíp|V|logpi+p|V|logp|V| = V‚àí1 / summationdisplay i=1pi 1‚àíp|V|logpi 1‚àíp|V|+ log(1‚àíp|V|)/bracehtipupleft / bracehtipdownright / bracehtipdownleft / bracehtipupright < 0 + p|V|Ô£´ Ô£≠logp|V|‚àíV‚àí1 / summationdisplay i=1pi 1‚àíp|V|logpiÔ£∂ Ô£∏ < V‚àí1 / summationdisplay i=1ÀÜpilog ÀÜpi+p|V|Ô£´ Ô£¨Ô£¨Ô£≠logp|V|‚àíV‚àí1 / summationdisplay i=1pi 1‚àíp|V|logpi / bracehtipupleft / bracehtipdownright / bracehtipdownleft / bracehtipupright > p|V|Ô£∂ Ô£∑Ô£∑Ô£∏ < V‚àí1 / summationdisplay i=1ÀÜpilog ÀÜpi+p|V|Ô£´ Ô£¨Ô£¨Ô£¨Ô£¨Ô£¨Ô£≠logp|V|‚àíV‚àí1 / summationdisplay i=1pi 1‚àíp|V|logp|V| /bracehtipupleft / bracehtipdownright / bracehtipdownleft / bracehtipupright = logp|V|Ô£∂ Ô£∑Ô£∑Ô£∑Ô£∑Ô£∑Ô£∏ = V‚àí1 / summationdisplay i=1ÀÜpilog ÀÜpi=‚àíH(ÀÜp ) ( 16 ) Therefore , we getH(ÀÜp)<H(p ) . 
By induction ( iteratively removing the last token ) , it is now easy to see that the top- kor nucleus344 transformation strictly decrease the entropy of the sampling distribution . 
Finally , we prove Property 1 ( entropy reduction ) holds for tempered sampling : Lemma B.3 . 
Property 1 holds for the transformation deÔ¨Åned by tempered sampling ( DeÔ¨Ånition 2.3 ) . 
Proof . 
For convenience , we Ô¨Årst rewrite the Temperature transformation : ÀÜpi = pŒ± i = exp(‚àíŒ±ei)/summationtext jexp(‚àíŒ±ej)(17 ) whereei=‚àílog(pi)andŒ±=1 T. The entropy can be written as : H(pŒ± ) = ‚àí/summationdisplay iexp(‚àíŒ±ei)/summationtext jexp(‚àíŒ±ej)logexp(‚àíŒ±ei)/summationtext jexp(‚àíŒ±ej ) = log / summationdisplay jexp(‚àíŒ±ej ) + Œ± / summationdisplay ieiexp(‚àíŒ±ei)/summationtext jexp(‚àíŒ±ej ) ( 18 ) Next , we take derivative w.r.t Œ± : ‚àÇH ‚àÇŒ±=‚àí/summationdisplay ieiexp(‚àíŒ±ei)/summationtext jexp(‚àíŒ±ej)+/summationdisplay ieiexp(‚àíŒ±ei)/summationtext jexp(‚àíŒ±ej ) /bracehtipupleft /bracehtipdownright / bracehtipdownleft /bracehtipupright = 0 + Œ±‚àÇ ‚àÇŒ± / summationdisplay ieiexp(‚àíŒ±ei)/summationtext jexp(‚àíŒ±ej ) = Œ± / summationdisplay iei / bracketleftBigg ‚àÇ ‚àÇŒ±logexp(‚àíŒ±ei)/summationtext jexp(‚àíŒ±ej)/bracketrightBigg / bracketleftBigg exp(‚àíŒ±ei)/summationtext jexp(‚àíŒ±ej)/bracketrightBigg /bracehtipupleft /bracehtipdownright / bracehtipdownleft /bracehtipupright log - derivative trick = Œ± / summationdisplay ieiÔ£Æ Ô£∞‚àíei+/summationdisplay j / primeej / primeexp(‚àíŒ±ei)/summationtext jexp(‚àíŒ±ej)Ô£π Ô£ª /bracketleftBigg exp(‚àíŒ±ei)/summationtext jexp(‚àíŒ±ej)/bracketrightBigg = ‚àíŒ±EpŒ± / bracketleftBig e2 i‚àíeiEpŒ±[ei]/bracketrightBig = ‚àíŒ± / bracehtipupleft / bracehtipdownright / bracehtipdownleft / bracehtipupright > 0 / parenleftBig EpŒ±[e2 i]‚àíEpŒ±[ei]2 / parenrightBig /bracehtipupleft / bracehtipdownright / bracehtipdownleft / bracehtipupright = VarpŒ±[ei]‚â•0 < 0 ( 19 ) We can now easily get‚àÇH ‚àÇT=‚àÇH ‚àÇŒ±‚àÇŒ± ‚àÇT>0 . 
Therefore , when we apply a tempered transformation withT < 1 , the entropy will strictly decrease comaparing to the original distribution ( where T= 1 ) . 
C Mechanical Turk Setup Our crowdworkers were required to have a HIT acceptance rate higher than 95 % , and be locatedin the United States . 
In total , 602 crowdworkers completed our tasks . 
In order to ensure that we had quality data , we Ô¨Åltered the crowdworker annotations for workers that spent at least 45 seconds on the aggregate task ( or 4.5 seconds rating each sentence ) . 
51 crowdworkers were Ô¨Åltered out through this process . 
Screenshots of our instructions and task are available in Figure(s ) 8 and 9 respectively . 
Figure 8 : Our instructions for crowdworker task . 
Figure 9 : An example of the task given to crowdworkers . 
D Convergence of Human Evaluation When we conduct human evaluation , we provide crowdworkers with 200 generated samples for some conÔ¨Åguration , and ask 25 different crowdworkers to evaluate the same sample . 
However , a reasonable question is whether our human evaluations are converging to some underlying true rating , or whether we need more samples or replicas . 
Figure 10 and 11 show that the average scores have roughly converged around 150 samples per conÔ¨Åguration , or around 15 replicas per sample . 
The two Ô¨Ågures demonstrate this for nucleus sampling , and this holds true for human evaluations of all sampling algorithms . 
E Additional Model - Generated Samples Table 2 shows some additional samples from each of the sampling algorithms described in the paper . 
Similarly , we have chosen hyperparameters for each sampling method that yields a similar diversity ( measured by self - BLEU ) to the top- kconÔ¨Åguration whereK= 15 . 
We observe that all sampling345 Sampling Conditional Samples Existing Sampling Algorithms Top - K ( K = 15)as the rest of his denver broncos teammates prepared for the game against denver , jay kasey could not help but think of his teammates and friends who worked hard in preparation for that night ‚Äôs game . 
Nucleus ( P = 0.65)as the rest of his denver broncos teammates slumped and buried themselves in their work , broncos quarterback leon johnson moved to the locker room monday and called his parents . 
Temperature ( T = 0.7)as the rest of his denver broncos teammates gathered in an auditorium to watch more stretching drills , ben holtz gave an emotional speech : we ‚Äôre running out of time to win a championship ring . 
Property - satisfying Sampling Algorithms Random Top - K ( R = 30)as the rest of his denver broncos teammates battled through their own stretch of the nÔ¨Ç playoffs , the quarterback began throwing the ball in the fourth quarter . 
Max Entropy ( E = 2.75)steven spielberg ‚Äôs dreamworks movie studio has agreed to pay $ # . 
# million to director john nichols ( ¬£ # . 
# million , # # # , a record in the studio circulation ) , the studio announced sunday .. Property - violating Sampling Algorithms Random Mask ( R = 0.75)as the rest of his denver broncos teammates connect with a player that the team did n‚Äôt expect to become a starter , quarterback james crosby speaks out about colin peterson ‚Äôs passion for the game . 
Noised Top - K ( K=20 , W=5e-3)as the rest of his denver broncos teammates start making room for nerdy bundles or twiggy pitchers , coach william perez might have to cut a big , bold note cut ready to console wife join them in iraq . 
Target Entropy ( E = 2.5)as the rest of his denver broncos teammates scratched out their locker rooms , cleanDeath Yo Communities wander edge extingustretched cords429 Mohnegie wildÔ¨Åres . 
Table 2 : The samples conditioned on as the rest of his denver broncos teammates , and the hyperparameters for a given sampling algorithm . 
The poor quality spans are higlighted in red . 
Figure 10 : We see that we obtain a reasonable estimate of sample quality around 150 samples per conÔ¨Åguration . 
Figure 11 : We see that we obtain a reasonable estimate of sample quality with around 15 ratings per sample . 
algorithms except for noised top- kand target entropy , yield similar quality samples . 
For noised top - kand target entropy , we see that these samples tend to degenerate towards the end of the sentence , indicating violation of the identiÔ¨Åed properties may possibly lead towards degraded performance . 
F Human Evaluation with Self - BLEU as Diversity Metric Figure 12 : Using self - BLEU as a diversity metric provides similar conclusions as to using n - gram entropy . 
Figures 1 and 5 measures diversity in terms of 3gram entropy , while the rest of our work measures diversity in terms of self - BLEU . 
For completeness , we provide Figure 12 where self - BLEU is used for diversity metric . 
This Ô¨Ågure demonstrates that similar trends can be observed using either 3 - gram entropy or self - BLEU.346 Proceedings of the 1st Conference of the Asia - PaciÔ¨Åc Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing , pages 347‚Äì357 December 4 - 7 , 2020 . 
¬© 2020 Association for Computational Linguistics Chinese Content Scoring : Open - Access Data Sets and F eatures on Different Segmentation Levels Y uning Ding Andrea Horbach Haoshi W ang Language T echnology Lab , University Duisburg - Essen ( yuning.ding|andrea.horbach|torsten.zesch)@uni - due.deXuefeng Song T orsten Zesch Abstract In this paper , we analyse the challenges of Chinese content scoring in comparison to English . 
As a review of prior work for Chinese content scoring shows a lack of openaccess data in the field , we present two short - answer data sets for Chinese . 
The Chinese Educational Short Answers data set ( CESA ) contains 1800 student answers for five science - related questions . 
As a second data set , we collected ASAP - ZH with 942 answers by re - using three existing prompts from the ASAP data set . 
W e adapt a state - of - the - art content scoring system for Chinese and evaluate it in several settings on these data sets . 
Results show that features on lower segmentation levels such as character n - grams tend to have better performance than features on token level . 
1 Introduction Short answer questions are a type of educational assessment that requires respondents to give natural language answers in response to a question or some reading material ( Rademakers et al . 
, 2005 ) . 
The applications used to automatically score such questions are usually thought of as content scoring systems , because content ( and not linguistic form ) is taken into consideration for automatic scoring ( Ziai et al . 
, 2012 ) . 
While there is a large research body for English content scoring , there is less research for Chinese.1The largest obstacle for more research on Chinese is the lack of publicly available data sets of Chinese short answer questions . 
1In this work , we use the term ‚Äò Chinese ‚Äô as abbreviation for Mandarin Chinese , which includes simplified and traditional written Chinese . 
Cantonese , W u , Min Nan and other dialects are not included . 
W orking with Chinese poses substantially different challenges than work on English data . 
Unlike English , which uses spaces as natural separators between words , segmentation of Chinese texts into tokens is challenging ( Chen and Liu , 1992 ) . 
F urthermore , there are more options on which level to segment Chinese text . 
Apart from tokenization and segmentation into characters , which are two options also available and often used for English , segmentation into components , radicals and even individual strokes are additionally possible for Chinese . 
T able 1gives an example for the segmentation options in both languages . 
Orthographic variance can be challenging in both languages , but behaves very differently . 
Nonword errors , which is the main source of orthographic problems in English ( Mitton , 1987 ) , can by definition not happen in Chinese , due to the input modalities . 
Language Level Unigrams English word panda characters p , a , n , d , a Chinese word ‡æß‡™≠ characters ‡æßƒë‡™≠ components radicals ·ßØ ƒë·óØ strokes   ... T able 1 : Comparison of segmentation possibilities in English and Chinese In the remainder of this paper , we will discuss these challenges in more detail ( Section 2 ) . 
W e review prior work on Chinese content scoring ( Section 3 ) and present two new freely - available data sets of short answers in Chinese ( Section 4 ) . 
In Section 5 , we adapt a347 machine learning pipeline for automatic scoring with state - of - art NLP tools for Chinese . 
W e investigate the extraction of n - gram features on all possible segmentation levels . 
In addition , we use features based on the Pinyin transcription of Chinese texts and experiment with the removal of auxiliary words as an equivalent to lemmatization in English . 
W e evaluate these features on our new data sets as well as , for comparison , an English data set translated into Chinese . 
2 Challenges in Chinese Content Scoring In this section , we highlight the main challenges when processing Chinese learner data in comparison to English data sets . 
W e first focus on segmentation , as tokenization is more difficult in Chinese than in English and there are more linguistic levels on which to segment a Chinese text compared to English . 
Next , we discuss variance in learner answers , which is a challenge for content scoring in any language but manifests itself in Chinese differently than in English . 
2.1 Segmentation English has an alphabetic writing system with some degree of grapheme - to - phoneme correspondence . 
The Chinese language , in contrast , uses a logosyllabic writing system , where characters represent lexical morphemes . 
Chinese words can be formed by one or more characters ( Chen , 1992 ) . 
Unlike English , where words are separated by white - spaces , the fact that Chinese writing does not mark word boundaries makes word segmentation a much harder task in Chinese NLP ( e.g. , Chen and Liu ( 1992 ) ; Huang et al . 
( 1996 ) ) . 
According to a recent literature review on Chinese word segmentation ( Zhao et al . 
, 2019 ) , the best - performing segmentation tool has an average F1 - value of only around 97 % . 
A major challenge is the handling of out - of - vocabulary words . 
In English content scoring , word level features such as word n - grams or word embeddings have proven to be effective ( e.g. , Sakaguchi et al . 
( 2015 ) ; Riordan et al . 
( 2017 ) ) . 
Additionally , character features are frequently used to capture orthographic as well as morphological variance ( e.g. , Heilman and Mad - nani ( 2013 ) ; Zesch et al . 
( 2015 ) ) . 
In the light of the tokenziation challenges mentioned above , it is surprising that although most prior work on Chinese also applies word - level features ( see Section 3 ) , the performance of their tokenizers are barely discussed and character - level features are neglected altogether . 
Apart from words and characters , there are more possibilities of segmentation in Chinese as discussed above . 
Consider , for example , a Chinese bi - morphemic word such aspanda bear ‡æß‡™≠ . 
It can additionally be segmented on the stroke , component and radical level as shown in T able 1 . 
It has been argued that the morphological information of characters in Chinese consists of the sequential information hidden in stroke order and the spatial information hidden in character components ( T ao et al . 
, 2019 ) . 
Each Chinese character can directly be mapped into a series of strokes ( with a particular order ) . 
On the component level , it has been estimated that about 80 % of modern Chinese characters are phonetic - logographic compounds , each of which consists of two components : One carries the sound of the character ( the stem ) and the other the meaning of the character ( the radical ) ( Li,1977 ) . 
W e argue that , together with strokes , both kinds of components may be used as features in content scoring . 
Note that in some cases , a character has only one component , which in the extreme case consists of one stroke only , so that for the characterone·ÅÇ , all four segmentation levels yield the same result , somewhat comparable to an English onecharacter word , such as ‚Äú I ‚Äù . 
2.2 Linguistic V ariance V ariance in learner answers has a major influence on content scoring performance ( Horbach and Zesch , 2019 ) , i.e. , the more variance between the answers to a specific prompt , the harder it is to score automatically . 
If we ignore cases of conceptually different answers , variance means different realizations with approximately the same semantic meaning . 
As shown in T able 2 , if we have a question about the eating habits of pandas , Chinese short answers can contain similar variance as in English , which is realized as both orthographic348 variance caused by spelling errors as well as variance of linguistic expression . 
Note that these types of variance should not influence the score of an answer as it depends only from the content of the answer . 
Both types of variance are further discussed in the following . 
Spelling errors in English can be classified into non - word and real - word spelling errors . 
In our example , ‚Äú bambu ‚Äù is a non - word , while ‚Äú beer ‚Äù is a real word spelling error . 
Both error types occur frequently in English short answer data sets , with non - word errors being more frequent ( Mitton , 1987 , 1996 ) . 
A content scoring system must therefore be able to generalize by taking variance in spelling into account ( Leacock and Chodorow , 2003 ) . 
T o do so , many systems for English data use character - level features ( Heilman and Madnani , 2013 ; Horbach et al . 
, 2017 ) , such that ‚Äú bamboo ‚Äù and ‚Äú bambu ‚Äù , while being different tokens , share , for example , the character 3 - grams mbamnand mambn . 
F or Chinese , the situation is entirely different . 
Non - word spelling errors are rare and even impossible for digitized data because of the input modalities typically used for Chinese text . 
When entering a Chinese text on the computer , a writer would normally type the phonetic transcription Pinyin , which is the Romanization of Chinese characters based on their pronunciation . 
After typing a Pinyin , the writer is shown all corresponding characters from which they choose the right one . 
As this selection list contains only valid Chinese characters , non - word errors can not occur by definition . 
Even if the original data set was collected in hand - written format , the transcription process forces transcribers to correct any non - word error that might occur in the data . 
F or example , if the learner accidentally wrotepanda bear ‡æß‡™≠ as   , the transcriber has no choice but to correct such an error , since the non - word character simply does not exist in the Chinese character set . 
There are two steps in the writing / transcription process where errors can still occur : typing letters to spell a Pinyin and choosing a character out of a list for this Pinyin . 
Previous experiments showed that people usually do not check Pinyin for errors , but wait until the Chinese characters start to show up ( Chenand Lee , 2000 ) . 
This behaviour generates two types of real - word spelling errors . 
In our example , spelling errors like confusingpoor ‡±´ ( qi«íng ) withbear ‡æß ( xi«íng ) are normally caused by wrong letters typed in the first step . 
The other error type , i.e. , choosing a wrong word from the homophones , leads to spelling errors like pearl ·á®·à∞ ( zh≈´ zi ) instead ofbamboo ·á∞·à∞ ( zh√∫ zi ) . 
Researchers found that nearly 95 % of errors are due to the misuse of homophones ( Y ang et al . 
, 2012 ) , i.e. , are errors of the second type . 
In order to reduce the influence of these errors in content scoring , introducing features presented as Pinyin might be beneficial . 
V ariance of linguistic expression is obviously found in both English and Chinese short answers . 
As shown in T able 2 , nearly the same content can be expressed using different lexical and syntactic choices . 
Human annotators can usually abstract away from these differences and treat all answers the same . 
However , linguistic variance is a challenge for automatic scoring systems . 
In English content scoring , lemmatization is often considered a useful method to reduce part of the variance ( Koleva et al . 
, 2014 ) . 
In this process , words are reduced to their base forms , such as substituting ‚Äú ate ‚Äù with ‚Äú eat ‚Äù and deleting the ‚Äú s ‚Äù after ‚Äú bamboo ‚Äù . 
In Chinese , similar grammatical morphemes such as ‚Äú ‡®î ‚Äù and ‚Äú ‡´å ‚Äù , termed auxiliary words ( Zan and Zhu , 2009 ) , which indicate the past tense and plural , can also be deleted in a preprocessing step to achieve a similar effect . 
Another type of variance is caused by synonyms . 
F or such cases of lexical variance , external knowledge is often needed to decide that two different words are interchangeable . 
However , as we can see in T able 2 , some synonyms , such as ‚Äú panda bears ‚Äù vs. ‚Äú pandas ‚Äù andbamboo ·á∞·à∞ vs.bamboo ·á∞ share some character(s ) . 
Such similarities can be covered by character features , but not token n - grams . 
In summary , there is the challenge of the segmentation of Chinese texts into tokens . 
F eatures extracted on other segmentation levels might be more robust and therefore helpful for automatic scoring . 
At the same time , NLP techniques which are useful to reduce variance349 English Chinese Reference Answer Panda bears eat bamboo.panda bear ‡æß‡™≠eat ”πbamboo ·á∞·à∞ b Orthographic V ariance Panda beers eat bambu .poor ‡±´cat ‡™≠eat ”πpearl ·á®·à∞b Expression V ariance Panda bears ate bamboos .panda ‡æßbear ‡™≠eat ”π<grammatical morpheme for past tense > ›ñ bamboo ·á∞·à∞ < grammatical morpheme for plural > ‡´å b Pandas eat bamboo.panda bear ‡æß‡™≠eat ”πbamboo ·á∞b T able 2 : Example answers showing variance in English and Chinese for the question : What do panda bears eat ? in English , especially lemmatization , have not yet been transferred to Chinese . 
Thus , we will explore in our experiments both n - gram features on different levels and the removal of auxiliary words . 
3 Prior W ork on Chinese Content Scoring As shown in T able 3 , all prior work on Chinese content scoring uses lexical features on the word level , such as word n - grams and sentence length in tokens . 
They are not only used in shallow learning methods like support vector machines ( SVM ) or support vector regression ( SVR ) ( W ang et al . 
, 2008 ; W u and Shih , 2018 ) , but also applied to deep learning methods like long - short term memory recurrent neural networks ( LSTM ) ( Y ang et al . 
, 2017 ; Huang et al . 
, 2018 ) or deep autoencoders ( Y ang et al . 
, 2018 ) . 
Also for neural models using word embeddings , word - level tokenization is necessary . 
W u and Y eh ( 2019 ) train 300 - dimensional word2vec word embeddings on sentences from their data set along with Chinese Wikipedia articles and classify student answers with a convolution neural network ( CNN ) . 
Li et al . 
( 2019 ) use a Bidirectional Long ShortT erm Memory ( Bi - LSTM ) network for semantic feature extraction from pre - trained 300dimensional word embeddings ( Li et al . 
, 2018 ) and score student answers based on their similarity to the reference answer using a mutual attention mechanism . 
F or segmentation , most prior work uses the jieba tokenizer2for pre - processing . 
However , 2https://github.com/fxsjy/jiebathe performance of the tokenization is rarely discussed . 
W e also notice that no related work uses segmentation on character or component level . 
Y ang et al . 
( 2018 ) perform stop word removal , but they do not mention if it included some kind of removal of grammatical markers . 
4 Chinese Scoring Data Sets In this section , we review existing Chinese content scoring data sets . 
They are not publicly available , which is a major obstacle to reproducibility in the field . 
W e thus produce two new Chinese data sets ( see detailed description in Section 4.2 ) , which are available online3to foster future research . 
4.1 Existing Data Sets Horbach and Zesch ( 2019 ) give an overview of publicly available data sets for content scoring , five of which are for English , and compare them based on properties such as prompt type , learner population and data set size . 
Unfortunately , we did not find any freely available Chinese content scoring data sets . 
Since we could not access the data sets used in related work , we can only compare them based on their brief descriptions , according to the aspects of comparison mentioned above . 
Results are shown in T able 4 . 
The Debris Flow Hazard ( DFH ) data set is used in the earliest work . 
It contains more than 1000 answers for 2 prompts in a creative problem - solving task . 
The learner population are high - school students from T aiwan , who speak native Chinese ( W ang et al . 
, 2008 ) . 
3https://github.com/ltlude/ChineseShortAnswerDatasets350 Reference Data Set Preprocessing F eatures Classifier Evaluation W ang et al . 
( 2008 ) DFH tasktokenization , POS taggingword uni-/bigrams , POS bigramsSVM r=.92 W u and Shih ( 2018)SCB - ZHMT CS - ENMTtokenization ( jieba)sentence length , word unigrams , BLEU scoreSVR , SVMacc=.60 RMSE=1.17 Y ang et al . 
( 2017 ) CRCCtokenization ( jieba)word unigrams LSTMacc=.76 , Cohen ‚Äôs Œ∫=.61 Y ang et al . 
( 2018 ) CRCCpunctuation and stop word removal , tokenization ( jieba)word unigramsAutoencoderacc=.74 , qwk=.64 Huang et al . 
( 2018 ) CRCCtokenization ( jieba)word vector trained on CBOWLSTMacc=.74 , qwk=.62 W u and Y eh ( 2019)ML_SQA SCB - ZHMTtokenization ( jieba)300D pre - trained word embeddingCNNacc=.91 , recall=.82 Li et al . 
( 2019 ) Law Questions tokenization300D pre - trained word embeddingBi - LSTM acc=.88 T able 3 : Overview of related work in Chinese content scoring . 
The Chinese Reading Comprehension Corpus ( CRCC ) ( Y ang et al . 
, 2018 ) , contains five reading comprehension questions . 
Each question has on average 2500 answers from students in grade 8 . 
Instead of collecting and annotating a data set from scratch , W u and Shih ( 2018 ) translated the English SciEntBank ( Dzikovska et al . 
, 2013 ) and the computer science ( CS ) ( Mohler and Mihalcea , 2009 ) data sets to Chinese . 
The data set was first translated using machine translation . 
In order to solve word usage and grammar problems , 12 % of the sentences were manually corrected . 
In their most recent work , the authors also collected a data set with 12 short answer questions and overall 600 answers related to machine learning ( ML_SQA ) to compare with the CS - ZHMT data set ( W u and Y eh , 2019 ) . 
In the most recent work ( Li et al . 
, 2019 ) , a large data set containing 85.000 student and reference answers was collected from a national specialty examination related to law . 
4.2 Collection of Open - access Data Sets As part of the contribution in this paper , we collected two new data sets for Chinese content scoring : Chinese Short Answer ( CESA ) and ASAP - ZH . 
In addition , we provide a machine - translated version of the the originalASAP - SAS English data , ASAP - ZHMT . 
T able 4shows key properties , while T able 5gives example answers of each data set . 
Chinese Educational Short Answers ( CESA ) contains five questions from the physics and computer science domain ( see T able 6 ) . 
Answers are collected from 360 students in the computer science department of Zhengzhou University . 
Each participant was required to answer each question with a maximum of 20 characters , resulting in an average answer length of 13.5 characters . 
T wo annotators speaking native Chinese with computer science background scored the answers into three classes , 0 , 1 and 2 points , with an average inter - annotator agreement of 0.9 quadratically weighted kappa ( QWK ) . 
ASAP - ZH This data set is based on the ASAP short - answer scoring data set released by the Hewlett F oundation.4ASAP contains ten short answer prompts covering different subjects and about 2000 student answers per prompt . 
Prompt 1 , 2 and 10 are sciencerelated tasks , which do not have a strong cultural background , and are therefore considered as appropriate to be transferred to other languages . 
Therefore , we collected answers in Chinese 4http://www.kaggle.com/c/asap-sas351 Data Set Type # Answers # Prompts Labels Level DFHcreative problem solving2,698 2 [ 0,1, ... ,28 ] high school CRCCreading comprehension12,528 5 [ 0,1,2,3,(4),(5 ) ] middle school SciEntsBank - ZHMTscience 9,804 197 binary&diagnostic high school CS - ZHMTcomputer science 630 21 [ 0 , 0.5 , ... , 5 ] university ML_SQA computer science 608 12 binary university Law Questions law 85,000 2 [ 0,1.5,3]/[0,1,1.5 ] CESAphysics , computer science1,800 5 [ 0,1,2 ] university ASAP - ZH science 942 3 [ 0,1,2,(3 ) ] high school ASAP - ZHMTscience 6,119 3 [ 0,1,2,(3 ) ] high school T able 4 : Chinese content scoring data sets : data sets from previous work ( upper part ) and our new data sets ( lower part ) for these three prompts after manually translating the prompt material . 
The data collection provider BasicFinder5helped us to collect 942 answers altogether , 314 answers for each prompt . 
They are collected from students in high school from grades 9 - 12 , which is comparable with the set of English answers in the ASAP - SAS data set . 
The answers are transcribed into digital form manually after being collected in handwriting . 
After reaching an acceptable agreement on a set of answers from the original ASAP - SAS , two annotators speaking native Chinese scored the ASAP - ZH data on a scale from 0 to 3 points ( prompt 1 and 2 ) or 0 to 2 points ( prompt 10 ) with an average QWK of 0.7 . 
Key statistics for the data set can be found in T able 7 . 
ASAP - ZHMTF or comparison , we also translated the English answers in prompts 1,2 and 10 in the original ASAP - SAS data set to Chinese using the Google T ranslate API.6The examples in T able 5show that some translation errors can be found , especially when errors exist already in the original text . 
W ords containing spelling errors like ‚Äú wat ‚Äù instead of ‚Äù what ‚Äù are simply not translated at all . 
The overall translation quality is also not perfect , for example , the word ‚Äú coolest ‚Äù is wrongly translated intomost ·âãfashioned ‡•Æ÷• instead of the correctmost ·âãcoldest ‡ßè÷• . 
  5https://www.basicfinder.com/en 6https://cloud.google.com/translateAs shown in T ables 7 and 8 , the average length of the translated answers is larger than the length of the original Chinese answers to the same prompt in our re - collected data set . 
One explanation could be that paid crowd workers are less motivated than actual students and therefore write shorter answers . 
5 Experimental Setup In this section , we adapt a state - of - the - art content scoring system to Chinese . 
W e evaluate it in six settings with different feature sets on the data sets described above in order to investigate different options for segmentation of Chinese text . 
T able 9gives an example for the different segmentation options , which will also be detailed in Section 5.2 . 
Additionally , we add a pre - processing step to remove all auxiliary words in the data in order to simulate the effect of lemmatization in English content scoring . 
5.1 General Experimental Setup F or all our experiments , we use the ESCRITO ( Zesch and Horbach , 2018 ) toolkit and extended it with readers and tokenization for Chinese text . 
ESCRITO is a publicly available general - purpose scoring framework based on DKPro TC ( Daxenberger et al . 
, 2014 ) , which uses an SVM classifier ( Cortes and V apnik , 1995 ) using the SMO algorithm as provided by WEKA ( Witten et al . 
, 1999 ) . 
F or all kinds of features , we use the top 10000 most frequent352 Data Set ID Score Example CESA 52 The machine summarizes a large amount of data and finds the pattern from it ‡†è‡∞ñ·àπ‡¢≤’∂‡®à‡∂î‡§åƒë’ñ·áè·Ö≥‹ø÷û‡©∞ 1 Machines can learn things by themselves ‡†è‡∞ñ‡¨ø·à±‡†≠‡øê ‡ºù ◊™‡ºÜ 0 Let the machine learn human thinking ability ‡≤û‡†è‡∞ñ‡øê ‡ºù ‡≤¶÷•‡∂±‡Ωò‡¨ø‡ßØ ASAP - ZH 10 2 White : make the indoor temperature not too high , œ¢‡≥§‡µê‡µ©‡¨Ω‡∞ó‡ªë “Ç ‡∑æ€ö experiments show that white has the lowest light energy absorption rate ‡µå·Äí—ñ‡´ºœ¢‡≥§÷•‹ªÿì‡¨ø‡®à‡ºã‡µ¨‡©±·âã÷Æ 1 Black allows the doghouse to absorb more heat in the light , making it warm ﬁë‡≥§‡¨ø‡≤û‹ê‡ª†·Ñù‹ª‡ºØ‡ºã÷•ÿü€∑‡≤£ƒë‡µê‡∞É‡ªë ‡≠∞ 0 Dark gray : keep the temperature unchanged , ‡¥ßﬂß‡≥§ƒü ƒü‡µê‡ªëÿá “Ç —çƒë the lighter the color , the lower the temperature ‡øæ‡≥§·ÑÄ‡∞∞‡ªëÿá·ÑÄ÷Æ ASAP - ZHMT10 2 white : : having white paint would make the dog house colder , œ¢‡≥§ : : ·Çµœ¢‡≥§·Ç≤‡∞Ä ﬂ∂ ‡µê‹ê‡ª¨€∑‡ßèƒë so in the summer the dog would not be hot . 
‡∑Æ·Åõ·Ñù‡º±‡∏ø‹ê “Ç ﬁì ﬂ∂ ‡≤£b The average for white is the coolest temperature ( 42 ( DEG ) ) œ¢‡≥§÷•‡Øú‡§®·Ü¥‡µû·âã‡•Æ÷• ‡ªëÿáƒç 42ƒçDEGƒé ƒé 1 black : : Because , the darker the lid color , ﬁë‡≥§ : : ·Åπ‡∫πƒë€Ç·à∞‡øæ‡≥§·ÑÄ‡¥ßƒë the greater the increase in the air temperature in the glass jar . 
—™‡ß∞‹∑·áè‡•¢‡∞ó‡ªë÷•ÿá‡¥∂‡£º€ö·ÑÄ’∂b 0 light gray : : The light grey will effect the doghouse by making it more noticable ‡∞∞ﬂß‡≥§ƒü‡∞∞ﬂß‡≥§ ﬂ∂ ‡µê‡°Ü€∑‹ê‹ê‡ΩÅ·ÄÑƒë and plus dogs can only see black , white and grey . 
‡°Ü‡¥à‹ê·Ü∫‡¨ø‡•Åﬁë÷û‡≥§ƒëœ¢‡≥§ﬂßﬁÑ‡≥§b T able 5 : Example answers in our data sets . 
1- to 5 - grams . 
Due to the limited amount of data , we use 10 - fold cross - validation on both data sets . 
F or evaluation , we use accuracy , i.e. , the percentage of student answers scored correctly , as well as QWK , which does not only consider whether an answer is classified correctly or not , but also how far it is from the gold standard classification . 
5.2 F eature Sets T oken Baseline As a baseline , we follow previous work and use tokenization as segmentation , based on the HanLP tokenizer ( He , 2020 ) . 
Pinyin F eatures In order to reduce the variance caused by spelling errors , we transcribe the text into Pinyin using cnchar ( Chen , 2020 ) and extract ngrams on the level of transcribed characters . 
Note that we did not include information about tones in Pinyinon purpose , in order to cover spelling errors caused by homophones . 
Character F eatures F or this segmentation level , we simply split a text into individual characters . 
Component F eatures T o extract these features on sub - character level , we use a dictionary with 17,803 Chinese characters7and their components to decompose all characters . 
Radical F eatures Remember that radicals are only those components carrying the meaning of characters and might therefore be particularly useful in content scoring . 
W e use XMNLP ( Li,2019 ) to extract the radicals of each character and use only those as features . 
Note that some radicals as defined by the ‚Äú T able of Indexing Chinese Character Compo7https://github.com/kfcd/chaizi353 ID Prompt IAA avg . 
Distribution ( QWK ) Length 1why ‡∫π‡µâ‡™πwe ‡ª°‡´åcan ‡¨øuse ·Ç®diamond ·âá‡µÜcut ‡±çglass —™‡ß∞ ? .94 9.6 2why ‡∫π‡µâ‡™πred ﬁ£‡≥§clothes ·Åâ⁄õlooks ‡•Å‡∞è‡¶üas ‡µûred ﬁ£‡≥§÷• . 
? 83 14.7 3what ‡µâ‡™πis ‡µûartificial ‡≤¶ €Ω intelligence ·áÜ‡¨ø ? .91 15.3 4what ‡µâ‡™πis ‡µûnatural ·à±‡≤ñlanguage ·Éî‡øΩ ? .93 12.1 5what ‡µâ‡™πis ‡µûmachine ‡†è‡∞ñlearning ‡øê ‡ºù ? .89 15.7 T able 6 : Overview of prompts in CESA ID IAA avg . 
Distribution ( QWK ) Length 1 .72 35.3 2 .70 38.2 10 .69 37.6 T able 7 : Overview of prompts in ASAP - ZH ID IAA avg . 
Distribution ( QWK ) Length 1 .96 68 2 .94 94 10 .91 61 T able 8 : Overview of prompts in ASAP - ZHMT nents‚Äù8can consist of more than one component , therefore the radicals are not a proper subset of the components extracted above . 
Stroke F eatures W e use the cnchar tool to represent each answer as a sequence of individual strokes , following the stroke order for each character . 
Although we show the strokes in their original shapes in T able 9 , a letter encoding is used in the experiment for an efficient processing . 
Auxiliary W ords Removal Based on the knowledge database released by Han et al . 
( 2011 ) , which contains 45 common auxiliary words in modern Chinese , we remove all these grammatical morphemes on token level to reduce the influence of expression variance . 
In our example shown in T able 9 , the possessive 8http://www.moe.gov.cn/s78/A19/yxs_left/moe _ 810 / s230/201001 / t20100115_75694.htmlAnswerdiamond ·âá‡µÜ ‚Äôs ÷•hardness ·Çóÿágreat ’∂ T okens ·âá‡µÜƒë÷•ƒë·Çóÿáƒë’∂ PinyinZuan , Shi , De , Ying , Du , Da Characters ·âáƒë‡µÜƒë÷•ƒë·Çóƒëÿáƒë’∂ Components‡£Å·Öùƒë·ÅÇ·â•‡•ßƒëœ¢‡¥êƒë ‡µÜ€∑ƒë‹º·âõ·Çªƒë‡≤¶·ÅÇ Radicals ·™éƒë‡µÜƒëœ¢ƒë‡µÜƒë‹ºƒë’∂ Strokes T able 9 : Different segmentation levels for an answer in CESA , prompt 1 . 
markerms ÷• is eliminated . 
6 Results and Discussion T able 10 shows the performance of the different system configurations for the individual data sets , per prompt as well as averaged over all prompts from the same data set . 
First , we see that all feature sets were able to learn something meaningful from the training data . 
Although the performance of different feature sets is quite close to each other , we see a slight but significant advantage across data sets of component and character features over the token baseline . 
In order to check if tokenization caused354 Data Set CESA ASAP - ZH ASAP - ZHMT Prompt 1 2 3 4 5 avg . 
1 2 10 avg . 
1 2 10 avg . 
T oken .91 .84 .59 .66 .48 .70 .54 .40 .50 .48 .66 .59 .63 .63 Pinyin-.02+.03-.03+.01-.03+.01**+.13+.01+.04+.09**-.02+.01+.01¬±0 Character-.01+.03 ¬±0+.11+.05+.04**+.13+.03+.06+.07**¬±0+.04+.04+.02 * Component-.03+.03-.01+.10+.02+.02**+.17+.04+.08+.10**-.01 ¬±0+.04+.01 * * Radical-.02+.02+.03+.07 ¬±0+.02**+.08+.08+.02+.06**+.02-.02+.04+.01 Stroke-.01-.02-.02+.06-.04-.01**+.14+.07+.04+.08**-.01-.02-.03-.02 * * - Auxiliary ¬±0¬±0+.03+.02+.01+.01**-.01 ¬±0-.01-.01**-.01-.01+.01-.01 * * * * p < 0.01,*p < 0.05 T able 10 : Classification results on different feature sets in QWK values . 
problems in scoring , we manually inspected 100 answers from prompt 1 and 4 in CESA . 
However , we found that tokenization was only erroneous in 12 cases . 
Surprisingly , most of them occurred in prompt 1 , where the token baseline even outperformed the character features and not in prompt 4 , where character features performed better . 
W e also had a closer look at a number of student answers which are assigned a wrong score by the token baseline model but not by models with more fine - grained features . 
7 out of 18 instances contain indeed variants of more frequent words in the data set . 
F or example , human ‡≤¶‡´å andhuman ‡≤¶ are less - frequently seen variants ofhuman ‡≤¶‡ßã , all of which are indicators of a correct answer . 
This supports the assumption that , like in English , character - level features can capture variance in learner answers , in this case by handling variance in lexical choice . 
The usage of Pinyin did not bring the expected benefit , possibly because the amount of spelling errors is not substantial enough in the data . 
Similarly , removing auxiliary words appears to have little influence on scoring performance . 
7 Summary & F uture W ork In this paper , we discussed the main challenges in Chinese content scoring in comparison with English , namely segmentation and a different form of linguistic variance . 
W e reviewed related work in Chinese content scoring and saw a need for open - access scoring data sets in Chinese . 
Therefore , we collected two new datasets , CESA and ASAP - ZH , and release them for research in the future . 
While previous work has been limited to word - level features , we conducted a comparison of features on different segmentation levels . 
Although the difference between feature sets was in general small , we found that some answers with unusual expressions have a tendency to be better scored with models trained on lower level features , such as character ngrams . 
In the future , we will extend our comparison of segmentation levels also to a deep learning setting , using embeddings of different granularity ( Yin et al . 
, 2016 ) . 
Abstract This paper presents a new dataset , B - SHARP , that can be used to develop NLP models for the detection of Mild Cognitive Impairment ( MCI ) known as an early sign of Alzheimer ‚Äôs disease . 
Our dataset contains 1 - 2 min speech segments from 326 human subjects for 3 topics , ( 1 ) daily activity , ( 2 ) room environment , and ( 3 ) picture description , and their transcripts so that a total of 650 speech segments are collected . 
Given theB - SHARP dataset , several hierarchical text classiÔ¨Åcation models are developed that jointly learn combinatory features across all 3 topics . 
The best performance of 74.1 % is achieved by an ensemble model that adapts 3 types of transformer encoders . 
To the best of our knowledge , this is the Ô¨Årst work that builds deep learningbased text classiÔ¨Åcation models on multiple contents for the detection of MCI . 
1 Introduction Alzheimer ‚Äôs Disease ( AD ) is a progressive neurodegenerative disorder that is associated with memory loss and declines in major brain functions including semantic and pragmatic levels of language processing ( Vestal et al . 
, 2006 ; Ferris and Farlow , 2013 ) . 
Traditional cognitive assessments such as positron emission tomography or cerebrospinal Ô¨Çuid analysis are expensive and time - consuming ( Fyffe et al . 
, 2011 ) . 
This may cause delay in treating AD , known to be irreversible and incurable ( Korczyn , 2012 ) , and put an increasing pressure on public health , especially for seniors whose life expectancy is rapidly growing yet are more likely to develop AD . 
Thus , it is crucial to Ô¨Ånd a more intelligent way of detecting AD in the earliest stage possible ( Karr et al . 
, 2018 ) . 
Mild Cognitive Impairment ( MCI ) is considered the Ô¨Årst phase that patients start having biomarker evidence of brain changes that can eventually lead to AD ( Albert et al . 
, 2011 ) . 
MCI involves subtle language changes from impairment in reasoningthat may not be noticeable to people other than friends and relatives . 
Because of this , the detection of MCI is a much more challenging task than detecting dementia ( Suzman and Beard , 2011 ) . 
Recent studies in NLP have shown that it is possible to detect early stages of AD by analyzing patients ‚Äô language patterns ; however , most previous works have focused on the detection of dementia instead and researches tackling the detection of MCI have been based on relatively small datasets ( Section 2 ) . 
This paper presents a new dataset that comprises three types of speech segments from both normal controls and MCI patients ( Section 3 ) . 
Then , a hierarchical text classiÔ¨Åcation model is proposed , which jointly learns features from all three types of speech segments to determine whether or not each subject has MCI ( Section 4 ) . 
Individual and ensemble models using three types of transformer encoders are evaluated on our dataset and show that different transformer encoders reveal strengths in distinct types of speeches ( Section 5 ) . 
We believe that this work takes the initiative of deep learningbased NLP for detecting MCI that will be broadly beneÔ¨Åcial to global public health . 
2 Related Work Only few studies have tackled the detection of MCI using NLP.1Asgari et al . 
( 2017 ) conducted interviews with ( 27 C , 14 M ) , and developed SVM and random forest models on their transcribed speeches . 
Beltrami et al . 
( 2018 ) conducted three speech tasks with ( 48 C , 32 M , 16 D ) , and analyzed phonetic and linguistic features of their speeches and transcripts . 
Fraser et al . 
( 2019 ) conducted 3 language tasks with ( 29C , 26 M ) , and built a cascade model to learn multimodal features such as audio , text , eye - tracking . 
Gosztolya et al . 
( 2019 ) conducted question answer1#C : the number of normal controls , # M / D / A : the number of MCI / Dementia / AD patients.358 Tokens Sentences Nouns Verbs Conjuncts Complex Discourse Q1Control 186.6 ( ¬±60.4 ) 10.4 ( ¬±4.5 ) 28.1 ( ¬±9.6 ) 30.4 ( ¬±11.5 ) 8.5 ( ¬±4.5 ) 2.3 ( ¬±1.7 ) 8.1 ( ¬±5.4 ) MCI 175.6 ( ¬±54.5 ) 9.8 ( ¬±4.1 ) 23.7 ( ¬±8.3 ) 29.3 ( ¬±10.4 ) 8.5 ( ¬±4.2 ) 2.0 ( ¬±1.6 ) 9.2 ( ¬±6.0 ) Q2Control 191.5 ( ¬±11.8 ) 11.7 ( ¬±4.7 ) 41.1 ( ¬±13.3 ) 24.3 ( ¬±11.2 ) 6.6 ( ¬±4.5 ) 3.6 ( ¬±2.7 ) 7.1 ( ¬±4.8 ) MCI 178.6 ( ¬±11.7 ) 11.6 ( ¬±4.7 ) 36.7 ( ¬±12.1 ) 23.2 ( ¬±10.6 ) 6.4 ( ¬±4.4 ) 2.9 ( ¬±2.3 ) 8.4 ( ¬±5.3 ) Q3Control 193.4 ( ¬±63.4 ) 12.6 ( ¬±5.4 ) 39.5 ( ¬±13.5 ) 28.4 ( ¬±10.1 ) 8.0 ( ¬±4.8 ) 3.3 ( ¬±2.1 ) 6.1 ( ¬±5.5 ) MCI 187.8 ( ¬±63.4 ) 12.7 ( ¬±5.1 ) 36.2 ( ¬±13.2 ) 27.7 ( ¬±10.9 ) 7.2 ( ¬±4.2 ) 2.6 ( ¬±2.0 ) 7.3 ( ¬±5.5 ) AllControl 578.1 ( ¬±149.8 ) 34.5 ( ¬±10.7 ) 110.5 ( ¬±27.9 ) 84.2 ( ¬±25.4 ) 23.5 ( ¬±10.1 ) 9.3(¬±4.5 ) 21.4 ( ¬±13.0 ) MCI 548.7 ( ¬±140.6 ) 34.0 ( ¬±10.5 ) 98.1 ( ¬±26.1 ) 81.2 ( ¬±24.1 ) 22.5 ( ¬±9.7 ) 7.7 ( ¬±4.2 ) 25.3 ( ¬±15.0 ) p 0.0110 0.5541 < 0.0001 0.1277 0 .2046 < 0.0001 0 .0006 Table 1 : Average counts and their standard deviations of linguistic features per transcript in the B - SHARP dataset . 
Complex : occurrences of complex structures ( e.g. , relative clauses , non-Ô¨Ånite clauses ) , Discourse : occurrences of discourse elements ( e.g. , interjections , disÔ¨Çuency ) . 
ing sessions with ( 25 C , 25 M , 25 A ) , and trained a SVM model using acoustic and linguistic features . 
All of the previous works were based on fewer than 100 subjects using traditional linguistic features to develop NLP models , compared to our work that is based on 326 subjects and 650 recordings using the latest transformer - based deep neural models . 
The task of dementia detection has been more explored by the NLP community . 
Becker et al . 
( 1994 ) presented the DementiaBank , that consists of 552 audio recordings describing the picture called ‚Äú The Boston Cookie Theft ‚Äù from 99 normal controls and 194 dementia patients , that have been used by the following works . 
Orimaye et al . 
( 2016 ) presented deep - deep neural network language models using higher - order n - grams and skip - grams . 
Pou - Prom and Rudzicz ( 2018 ) leveraged linguistic features and multiview embeddings by applying generalized canonical correlation analysis . 
Karleka et al . 
( 2018 ) proposed a model based on convolutional and recurrent neural networks and gave interpretations of this model to explain linguistic characteristics for detecting dementia . 
Our work is distinguished as : ‚Ä¢We tackle the detection of MCI , not dementia , ‚Ä¢Our documents are multi - contents compared to single - content documents in the DementiaBank . 
‚Ä¢Our approach is based on the latest contextualized embeddings compared to the distributional embeddings adapted by the previous works . 
3 Dataset 3.1 B - SHARP Our work is based on data collected as part of the Brain , Stress , Hypertension , and AgingResearch Program ( B - SHARP ) .2In this dataset , 185 normal 2B - SHARP : http://medicine.emory.edu/bsharpcontrols and 141 MCI patients are selected based on neuropsychological and clinical assessments . 
Every subject has been examined with multiple cognitive tests including the Montreal Cognitive Assessment ( MoCA ; Nasreddine et al . 
2005 ) and the Boston Naming Test ( BNT ; Kaplan et al . 
1983 ) , followed by a speech task protocol for recording . 
51.5 % and 23.9 % of the subjects have so far come back for their 2nd and 3rd visits to take new voice recordings , respectively . 
B - SHARP is an ongoing program ; recordings of 20 - 25 subjects are taken every month ; thus , the data is still growing . 
Sbj 2nd 3rd Rec MoCA BNT C 185 100 50 385 26.2 ( ¬±2.6 ) 14.2 ( ¬±1.2 ) M 141 68 28 265 21.5 ( ¬±3.5 ) 13.4 ( ¬±1.5 ) Œ£ 326 168 78 650 24.2 ( ¬±3.8 ) 13.9 ( ¬±1.4 ) Table 2 : Statistics of control ( C ) and MCI ( M ) groups . 
Sbj : # of subjects , 2nd/3rd : # of subjects who made the 2nd/3rd visits , Rec : # of voice recordings , MoCA / BNT : average scores and stdevs from MoCA / BNT . 
Note that subjects with the 2nd/3rd visits take one / two additional recordings ; thus , Rec = Sbj + 1 ¬∑ ( 2nd ) + 2¬∑(3rd ) . 
Table 2 shows the statistics of the control and the MCI groups in B - SHARP . 
Note that when subjects make multiple visits , there is a year gap in between so that subjects generally do not remember so much from their previous visits . 
Thus , speeches from the same subject are not necessarily more similar than ones from the other subjects . 
In fact , most speeches across subjects , regardless of their groups , are very similar when they are transcribed since all subjects follow the same speech protocol in Section 3.2.3 3.2 Speech Task Protocol A speech task protocol has been conducted to collect recordings of both control and MCI subjects 3A.3 compares B - SHARP with the DementiaBank in details.359 Transformer1 ( T1)w11w12 / uni22EFw1n[CLS1]w21w22 / uni22EFw2n[CLS2]w31w32 / uni22EFw3n[CLS3 ] c 1 e 11 e 12 / uni22EF e 1 n c 2 e 21 e 22 / uni22EF e 2 n c 3 e 31 e 32 / uni22EF e 3 n c 1 c 2 c 3MLP1MLP2MLP3 / uni2295 / uni2295o2o1o3MLPeoe Transformer2 ( T2)Transformer3 ( T3)Figure 1 : Overview of hierarchical transformer to combine content features from the three types of speech tasks . 
who are asked to speak about Q1 : daily activity , Q2 : room environment , and Q3 : picture description for 1 - 2 minutes each . 
All subjects are provided with the same instructions in A.2 , and visual abilities of the subjects are conÔ¨Årmed before recording . 
To reduce potential variance , the subjects are guided to follow similar activities before Q1 , located to similar room settings before Q2 , and shown the same picture in Fig 2 , ‚Äú The Circus Procession ‚Äù , for Q3 . 
The collected voice recordings are automatically transcribed by the online tool called Temi.4Table 1 shows linguistic features about our dataset analyzed by the open - source NLP toolkit , ELIT.5Transcripts from the control group depict signiÔ¨Åcantly higher numbers of tokens , nouns , and complex structures while transcripts from the MCI group gives signiÔ¨Åcantly more discourse elements , implying that the control subjects are more expressive while the MCI subjects include more disÔ¨Çuency in their speeches . 
4 Hierarchical Transformer Although transformer encoders have recently established the state - of - the - art results on most document classiÔ¨Åcation tasks , they have a limit on the input size . 
As in Table 1 , the average number of tokens in our input documents well - exceeds 512 when combining transcripts from all three tasks , which is the max - number of tokens that the pretrained models of these transformers allow in general . 
This makes it difÔ¨Åcult to simply join all transcripts together and feed into a transformer encoder . 
Thus , this section presents a hierarchical transformer to overcome the challenge of long documents while jointly training transcript contents from all three tasks ( Figure 1 ) . 
4Temi : https://www.temi.com 5ELIT : https://github.com/elitcloud/elitLetWi={wi1 , . 
. 
. 
, w in}be a transcript , where wijrepresents the j‚Äôth token in the transcript produced by the i‚Äôth task Qi(in our case , i={1,2,3 } ) . 
Wiis prepended by the special token [ CLS i]that is used to learn the transcript embedding , and fed into the transformer Ti . 
The transformer then generates Ei={ci , ei1 , . 
. 
. 
, e in } , where ciandeijare the embeddings for [ CLS i]andwij , respectively . 
ci‚ààRdis used to make two types of predictions . 
First , ciis fed into a multilayer perceptron layer , MLP i , that generates the output vector oi‚ààR2to predict whether or not the subject has MCI based on the transcript from Qialone . 
Second , the transcript embeddings from all three tasks are concatenated such that ce = c1‚äïc2‚äïc3‚ààR3d , which gets fed into another MLP eto generate the output vector oe‚ààR2 , and makes the binary decision based on the transcripts from all three tasks , Q1,Q2andQ3 . 
5 Experiments There are 650 recordings in our dataset ( Table 2 ) , that is rather small to divide into train , development , and test sets . 
Thus , 5 - fold cross - validation ( CV ) is used to evaluate the performance of our models . 
Table 5 shows the distributions of the Ô¨Åve CVsets for our experiments , where the transcript of each recording is treated as an independent document . 
Notice that the distributions are calculated based on analysis of the last MLP layer instead of simple majority vote on individual models . 
It is worth mentioning that all recordings from the same subject given multiple visits are assigned to the same CVset ; thus , there is no overlap in terms of subjects across these CVsets . 
This allows us to avoid potential inÔ¨Çation in accuracy due to unique language patterns used by individual subjects.360 BERT RoBERTa ALBERT Q1 Q2 Q3 Q1 Q2 Q3 Q1 Q2 Q3 ACC 67.6 ( ¬±0.4 ) 69.0 ( ¬±1.2 ) 67.7 ( ¬±0.7 ) 69.0 ( ¬±1.5 ) 69.9 ( ¬±0.2 ) 65.2 ( ¬±0.3 ) 67.6 ( ¬±1.5 ) 69.5 ( ¬±0.3 ) 66.6 ( ¬±1.3 ) SEN 48.9 ( ¬±1.8 ) 57.1 ( ¬±2.5 ) 41.5 ( ¬±3.6 ) 44.3 ( ¬±4.5 ) 55.3 ( ¬±1.2 ) 37.1 ( ¬±3.7 ) 45.9 ( ¬±1.9 ) 52.2 ( ¬±0.6 ) 37.4 ( ¬±3.3 ) SPE 80.4 ( ¬±1.2 ) 77.3 ( ¬±2.8 ) 85.2 ( ¬±3.0 ) 85.8 ( ¬±2.1 ) 79.7 ( ¬±0.7 ) 84.5 ( ¬±3.0 ) 82.6 ( ¬±3.7 ) 81.4 ( ¬±0.3 ) 86.8 ( ¬±3.3 ) Table 3 : Model performance on the individual tasks . 
ACC : accuracy , SEN : sensitivity , SPE : speciÔ¨Åcity . 
CNN BERT e RoBERTa e ALBERT e Be+ Re Ae+ Re Be+ Ae+ Re ACC 69.5 ( ¬±0.2 ) 69.9 ( ¬±1.1 ) 71.6 ( ¬±1.5 ) 69.7 ( ¬±2.9 ) 72.2 ( ¬±0.7 ) 71.5 ( ¬±1.9 ) 74.1 ( ¬±0.3 ) SEN 49.2 ( ¬±0.8 ) 57.6 ( ¬±3.4 ) 48.5 ( ¬±6.1 ) 46.2 ( ¬±8.3 ) 56.5 ( ¬±2.5 ) 51.7 ( ¬±1.3 ) 60.9 ( ¬±5.2 ) SPE 83.5 ( ¬±0.9 ) 77.4 ( ¬±4.8 ) 87.5 ( ¬±1.8 ) 85.4 ( ¬±0.5 ) 83.1 ( ¬±0.9 ) 86.7 ( ¬±3.4 ) 84.0 ( ¬±2.4 ) Table 4 : Performance of ensemble models . 
Bert e / RoBERTa e / ALBERT euse transcript embeddings from all 3 tasks trained by the BERT / RoBERTa / ALBERT models in Table 3 , respectively . 
B e+Reuses transcript embeddings from both Bert eand RoBERTa e(so the total of 6 embeddings ) , A e+Reuses transcript embeddings from both ALBERT e and RoBERTa e(6 embeddings ) , and B e+Ae+Reuses transcript embeddings from all three models ( 9 embeddings ) . 
Three transformer encoders are used , BERT ( Devlin et al . 
, 2019 ) , RoBERTa ( Liu et al . 
, 2020 ) , and ALBERT ( Lan et al . 
, 2019 ) for our experiments . 
Every model is trained 3 times and its average performance with the standard deviation are reported.6 CV0CV1CV2CV3CV4ALL CRec 77 77 77 77 77 385 MRec 53 53 53 53 53 265 CSbj 37 37 37 37 37 185 MSbj 27 28 28 29 29 141 Table 5 : Statistics of the CVsets for our experiments . 
Rec / Sbj : # of recordings / subjects , C / M : in control / MCI group . 
CVi : thei‚Äôth set . 
ALL:/summationtext4 i=0CVi . 
5.1 Performance of Individual Models Individual models are built by training transcripts from each task separately using MLP iin Section 4 . 
Table 3 shows the performance of the 3 transformer models on the individual tasks . 
The performance onQ2shows the highest accuracy for all three models , achieving 69.9 % with RoBERTa , implying that the room environment task of Q2 , involving many spatial descriptions , are the most effective to distinguish the MCI group . 
The highest sensitivity of 57.1 % is achieved by BERT on Q2 , and the highest speciÔ¨Åcity of 86.8 % is achieved by ALBERT on Q3 . 
Such a low sensitivity and a high speciÔ¨Åcity imply that it is easier to recognize the normal controls but not the MCI patients given the short speeches . 
5.2 Performance of Ensemble Models Ensemble models are developed by jointly training multiple transcript embeddings from the individual models using MLP ein Section 4 . 
Table 4 shows the 6Details about the experimental settings are provided in A.1.model performance of the ensemble models . 
Additionally , results from a model that takes transcripts from the 3 tasks as one input document and trains a convolutional neural network ( CNN ) are provided for comparison to Karleka et al . 
( 2018).7Reshows 1.7 % improvement on accuracy over the RoBERTa model in Table 3 although its sensitivity is worse . 
Table 6 shows the voting distributions of each task combination ; given the samples correctly predicted by RoBERTa e , we count how often the individual models are correct for those samples by comparing the weights in MLP eand estimate the percentages . 
The combination of ( Q1,Q3 ) shows the highest percentage of 30 % , meaning that 30 % of the corrected predicted samples are voted by both Q1andQ3 . 
Q1Q2Q3Q1,2Q1,3Q2,3Q1,2,3 5.8 6.4 2.8 19.5 30.0 8.8 26.1 Table 6 : V oting distributions of each task combination for RoBERTa e. Qi : % of only the Qimodel is correct , Qi , i , j : % of all Qi , Qi , andQjmodels are correct . 
A similar analysis is done for B e+Re+Aealthough displaying the distributions is quite infeasible since it involves 29 - 1 combinations . 
Among the samples correctly predicted by B e+Re+Ae , 86 % are derived from majority votes ; in other words , at least 5 out of 9 individual models agree with the predictions . 
V otes from 6 and 5 models are the largest groups , showing 35 % and 28 % , respectively . 
Only 0.21 % are agreed by all 9 models . 
No case of votes from 3 or less models is found , implying that no individual model dominates the Ô¨Ånal decision of B e+Re+Ae . 
7We also experiemented with LSTM - RNN and CNN - LSTM models as suggested by Karleka et al . 
( 2018 ) ; however , the CNN model gave the highest accuracy on our dataset.361 6 Conclusion This paper presents the B - SHARP dataset , that is the largest dataset for the task of MCI detection feasible to develop robust deep neural models . 
Our best ensemble model using hierarchical transformer gives the accuracy of 74 % to distinguish MCI patients from normal controls that is very promising . 
We will also explore models to make a longevity analysis per patient with this dataset.8 A Appendix A.1 Experimental Settings Table 7 shows the conÔ¨Åguration of the transformer encoders in Section 5 . 
The base pre - trained models are used for all encoders . 
Transformer L AH IC HC P BERT 12 12 768 768 108 M RoBERTa 12 12 768 768 125 M ALBERT 12 12 768 128 12 M Table 7 : ConÔ¨Ågurations of the BERT , RoBERTa , and ALBERT encoders for our experiments . 
L : # of layers , AH : # of attended heads , IC : # of input cells , HC : # of hidden cells , P : # of parameters . 
Individual Models For training the BERT and RoBERTa models , the batch size of 5 , the learning rate of 5¬∑10‚àí6 , and the gradient clip of norm 0.5 are used with the Adam optimizer . 
A dropout rate of0.15is applied to all layers . 
For the ALBERT model , the batch size of 8 is used . 
All three models are trained for 30 epochs . 
Ensemble Models For training the two model ensembles , B e+Reand A e+Re , the batch size of 72 and the learning rate of 5¬∑10‚àí5are used with the Adam optimizer for 200 epochs . 
A dropout rate of 0.25is also applied . 
For training the B e+Ae+Re model , the dropout rate is set to 0.3 . 
A.2 Speech Task Protocol Table 8 describes the instructions provided to the subjects for the three speech tasks in Section 3.2 . 
Task Instruction Q1I would like you to describe to me everything we did from the moment we met today until now . 
Please try to recall as many details as possible in the order the events actually happened where we met , what we did , what we saw , where we went , and what you felt or thought during each of these events . 
Q2I would like you to describe everything that you see in this room . 
Q3I am going to show you a picture and ask you to describe what you see in as much detail as possible . 
You can describe the activities , characters , and colors of things you see in this picture . 
Table 8 : Instructions of the 3 speech tasks , Q1,Q2,Q3 , provided to the subjects . 
Figure 2 illustrates the image of the picture called ‚Äú The Circus Procession ‚Äù for the picture description task , Q3 , copyrighted by the McLoughlin Brothers as part of the Juvenile Collection . 
Figure 2 : The picture of ‚Äú The Circus Procession ‚Äù used in the B - SHARP dataset . 
A.3 B - SHARP Compared to DementiaBank DementiaBank is the largest public dataset for dementia detection that comprises recordings for 4 language tasks , picture description , verbal Ô¨Çuency , story recall , and sentence construction , from a large longitudinal study ( Becker et al . 
, 1994 ) . 
Subjects in this study are divided into two groups , normal controls and dementia patients . 
Among the four tasks , data from only the picture description task can be used for classiÔ¨Åcation since the other tasks give data of dementia patients only.9The design of this task is similar to Q3inB - SHARP ( Section 3.2 ) ; each subject is shown ‚Äú The Boston Cookie Theft ‚Äù picture in Figure 3 to describe for 1 - 2 minutes . 
Figure 3 : The picture of ‚Äú The Boston Cookie Theft ‚Äù used in the DementiaBank . 
Table 10 shows the statistics of the DementiaBank in comparison to Table 2 in Section 3 . 
Subjects in this study made up to 5 visits compared to 3 in B - SHARP although the number of subjects in each visit is larger in B - SHARP .B - SHARP has‚âà100 9The verbal Ô¨Çuency task gives 1 audio recording of a normal control , that is still not enough to train classiÔ¨Åcation models.364 Tokens Sentences Nouns Verbs Conjuncts Complex Discourse Control 124.0 ( ¬±59.7 ) 12.6 ( ¬±5.1 ) 23.7 ( ¬±11.8 ) 27.1 ( ¬±11.9 ) 2.8 ( ¬±2.8 ) 1.6 ( ¬±1.6 ) 1.5 ( ¬±1.6 ) Dementia 114.3 ( ¬±61.3 ) 12.1 ( ¬±6.4 ) 18.7 ( ¬±10.4 ) 23.9 ( ¬±12.9 ) 2.4 ( ¬±2.4 ) 1.4 ( ¬±1.4 ) 2.8(¬±2.9 ) p 0.0625 0 .3204 < 0.0001 0 .0029 0.0715 0 .1184 < 0.0001 Table 9 : Average counts and standard deviations of linguistic features per transcript in the DementiaBank . 
See the caption in Table 1 for the column descriptions . 
more recordings than the DementiaBank , more importantly , B - SHARP is still growing , which makes it the largest dataset for NLP research related to the detection of Alzheimer ‚Äôs Disease . 
Unlike DementiaBank where 66.2 % of the subjects are dementia patients , 43.3 % of the subjects belong to the MCI group in B - SHARP ; this makes sense because MCI is closer to the preclinical phase that involves a much fewer number of patients reported in general . 
Group Sbj 2nd 3rd 4th 5th Rec Control 99 29 28 9 8 243 Dementia 194 53 13 8 3 309 All 293 82 41 17 11 552 Table 10 : Statistics of the control and the dementia groups in the DementiaBank . 
Sbj : # of subjects , i‚Äôth : # of subjects who made i‚Äôth visits , Rec : # of voice recordings . 
Note that subject with i‚Äôth visits take ( i‚àí1)additional recordings ; thus , Rec = Sbj + /summationtext5 i=2(i‚àí1)‚Äôth . 
Table 9 shows the statistics of linguistic features in comparison to Table 1 in Section 3 . 
The same tools , Temi and ELIT , are used to measure them . 
Unlike B - SHARP , the control group in the DementiaBank does not reveal a signiÔ¨Åcantly greater number of tokens than the dementia group . 
The document size in the DementiaBank is 4.9 times smaller thanB - SHARP on average . 
In both datasets , the noun and discourse counts are signiÔ¨Åcantly different between the control and the other groups . 
It is interesting that a signiÔ¨Åcant difference is found in verbs whereas it is not the case for complex structures in the DementiaBank , which is opposite in B - SHARP . 
This may imply that the verb usage deteriorates as it progresses from MCI to dementia , but more thorough research is needed for further veriÔ¨Åcation.365 Proceedings of the 1st Conference of the Asia - PaciÔ¨Åc Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing , pages 366‚Äì377 December 4 - 7 , 2020 . 
¬© 2020 Association for Computational Linguistics An Exploratory Study on Multilingual Quality Estimation Shuo Sun,1*Marina Fomicheva,2*Fr¬¥ed¬¥eric Blain,2 Vishrav Chaudhary,3Ahmed El - Kishky,3Adithya Renduchintala,3Francisco Guzm ¬¥ an,3Lucia Specia2,4 1Johns Hopkins University,2University of ShefÔ¨Åeld,3Facebook AI 4Imperial College London 1ssun32@jhu.edu 2{m.fomicheva , f.blain , l.specia } @sheffield.ac.uk 3{vishrav , ahelk , adirendu , fguzman } @fb.com Abstract Predicting the quality of machine translation has traditionally been addressed with language - speciÔ¨Åc models , under the assumption that the quality label distribution or linguistic features exhibit traits that are not shared across languages . 
An obvious disadvantage of this approach is the need for labelled data for each given language pair . 
We challenge this assumption by exploring different approaches to multilingual Quality Estimation ( QE ) , including using scores from translation models . 
We show that these outperform singlelanguage models , particularly in less balanced quality label distributions and low - resource settings . 
In the extreme case of zero - shot QE , we show that it is possible to accurately predict quality for any given new language from models trained on other languages . 
Our Ô¨Åndings indicate that state - of - the - art neural QE models based on powerful pre - trained representations generalise well across languages , making them more applicable in real - world settings . 
1 Introduction Quality Estimation ( QE ) ( Blatz et al . 
, 2004a ; Specia et al . 
, 2009 ) is the task of predicting the quality of an automatically generated translation at test time , when no reference translation is available for comparison . 
Instead of reference translations , QE turns to explicit quality indicators that are either provided by the Machine Translation ( MT ) system itself ( the so - called glass - box features ) or extracted from both the source and the target texts ( the socalled black - box features ) ( Specia et al . 
, 2018b ) . 
In the current QE approaches , black - box features are learned representations extracted by Ô¨Ånetuning pre - trained multilingual or cross - lingual sentence encoders such as BERT ( Devlin et al . 
, 2018 ) , ‚àóEqual contribution . 
XLM - R ( Conneau et al . 
, 2019 ) or LASER ( Artetxe and Schwenk , 2019 ) . 
These supervised approaches have led to the state - of - the - art ( SOTA ) results in this task ( Kepler et al . 
, 2019 ; Fonseca et al . 
, 2019 ) , similarly to what has been observed for a myriad of other downstream natural language processing applications that rely on cross - lingual sentence similarity . 
Glass - box features are usually obtained by extracting various types of information from the MT system , e.g. lexical probability or language model probability in the case of statistical MT systems ( Blatz et al . 
, 2004b ) , or more recently softmax probability and attention weights from neural MT models ( Fomicheva et al . 
, 2020 ) . 
Glass - box approach is potentially useful for low resource or zeroshot scenarios as it does not require large amounts of labelled data for training , but it does not perform as well as SOTA supervised models . 
QE is therefore generally framed as a supervised machine learning problem , with models trained on data labelled for quality for each language pair . 
Training data publicly available to build QE models is constrained to very few languages , which has made it difÔ¨Åcult to assess how well QE models generalise across languages . 
Therefore QE work to date has been addressed as a language - speciÔ¨Åc task . 
The recent availability of multilingual QE data in a diverse set of language pairs ( see Section 4.1 ) has made it possible to explore the multilingual potential of the QE task and SOTA models . 
In this paper , we posit that it is possible and beneÔ¨Åcial to extend SOTA models to frame QE as a languageindependent task . 
We further explore the role of in - language supervision in comparison to supervision coming from other languages in a multi - task setting . 
Finally , we propose for the Ô¨Årst time to model QE as a zero - shot cross - lingual transfer task , enabling new avenues of research in which multilingual models366 can be trained once and then serve a multitude of languages . 
The main contributions of this paper are : ( i ) we propose new multi - task learning approaches for multilingual QE ( Section 3 ) ; ( ii ) we show that multilingual system outperforms single language ones ( Section 5.1.1 ) , especially in low - resource and less balanced label distribution settings ( Section 5.1.3 ) , and ‚Äì counter - intuitively ‚Äì that sharing a source or target language with the test case does not prove beneÔ¨Åcial ( Section 5.1.2 ) ; and ( iii ) we study black - box and glass - box QE in a multilingual setting and show that zero - shot QE is possible for both ( Section 5.1.3 and 5.2 ) . 
2 Related Work QE Early QE models were trained upon a set of explicit features expressing either the conÔ¨Ådence of the MT system , the complexity of the source sentence , the Ô¨Çuency of the translation in the target language or its adequacy with regard to the source sentence ( Specia et al . 
, 2018b ) . 
Current SOTA models are learnt with the use of neural networks ( NN ) ( Specia et al . 
, 2018a ; Fonseca et al . 
, 2019 ) . 
The assumption is that representations learned can , to some extent , account for source complexity , target Ô¨Çuency and source - target adequacy . 
These are Ô¨Åne - tuned from pre - trained word representations extracted using multilingual or cross - lingual sentence encoders such as BERT ( Devlin et al . 
, 2018 ) , XLM - R ( Conneau et al . 
, 2019 ) or LASER ( Artetxe and Schwenk , 2019 ) . 
Kim et al . 
( 2017 ) propose the Ô¨Årst breakthrough in neural - based QE with the Predictor - Estimator modular architecture . 
The Predictor model is an encoder - decoder Recurrent Neural Network ( RNN ) model trained on a huge amount of parallel data for a word prediction task . 
Its output is fed to the Estimator , a unidirectional RNN trained on QE data , to produce the quality estimates . 
Kepler et al . 
( 2019 ) use a similar architecture where the Predictor model is replaced by pretrained contextualised word representations such as BERT ( Devlin et al . 
, 2018 ) or XLM - R ( Conneau et al . 
, 2019 ) . 
Despite achieving strong performances , such models are resource heavy and need to be Ô¨Åne - tuned for each language - pair under consideration . 
In a very different approach , Fomicheva et al . 
( 2020 ) propose exploiting information provided by the NMT system itself . 
By exploring uncertainty quantiÔ¨Åcation methods , they show that theconÔ¨Ådence with which the NMT system produces its translation correlates well with its quality . 
Although not performing as well as SOTA supervised models , their approach has the main advantage to be unsupervised and not rely on labelled data . 
Multilinguality Multilinguality allows training a single model to perform a task from and to multiple languages . 
This principle has been successfully applied to NMT ( Dong et al . 
, 2015 ; Firat et al . 
, 2016b , a ; Nguyen and Chiang , 2017 ) . 
Aharoni et al . 
( 2019 ) stretches this approach by translating up to 102 languages from and to English using a Transformer model ( Vaswani et al . 
, 2017 ) . 
They show that multilingual many - to - many models are effective in low resource settings . 
Multilinguality also allows for zero - shot translation ( Johnson et al . 
, 2017 ) . 
With a simple encoder - decoder architecture and without explicit bridging between source and target languages , they show that their model is able to build a form of inter - lingual representation between all involved language pairs . 
Shah and Specia ( 2016 ) is the only work in QE that attempted to explore models for more than one language . 
They use multitask learning with annotators or languages as multiple tasks . 
In a traditional black - box feature - based approach with Gaussian Processes as learning algorithm , their results suggest that adequately modelling the additional data is as important as the additional data itself . 
The multilingual models led to marginal improvements over bilingual ones . 
In addition , the experiments were only conducted with English translation into two closely related languages ( French and Spanish ) . 
3 Multilingual QE In this section , we describe the QE models we propose and experiment with . 
They build upon pretrained representations and represent the SOTA in QE , as we will show in Section 5 . 
Pre - trained contextualised representations such as BERT ( Devlin et al . 
, 2018 ) and XLM - R ( Conneau et al . 
, 2019 ) are deep contextualised language models based on the transformer neural architecture ( Vaswani et al . 
, 2017 ) . 
These models are pre - trained on a large amount of texts in multiple languages and optimised with self - supervised loss functions . 
They use shared subword vocabularies that directly support more than a hundred languages without the need for367 Figure 1 : Baseline QE model . 
any language - speciÔ¨Åc pre - processing . 
We explore QE models built on top of XLM - R , a pre - trained contextualised language model that achieves SOTA performance on multiple benchmark datasets . 
Baseline QE model ( BASE ) Given a source sentence sXin language Xand a target sentence sY in language Y , we model the QE function fby stacking a 2 - layer multilayer perceptron ( MLP ) on the vector representation of the [ CLS ] token from XLM - R : f(sX , sY ) = W2¬∑ReLU ( W1¬∑Ecls(sX , sY ) + b1 ) + b2(1 ) where W2‚ààR1√ó4096,b2‚ààR , W1‚ààR4096√ó1024 andb1‚ààR4096.Eclsis a function that extracts the vector representation of the [ CLS ] token after encoding the concatenation of sXandsYwith XLM - R and ReLU is the RectiÔ¨Åed Linear Unit activation function . 
We explore two training strategies : Thebilingual ( BL ) strategy trains a QE model for every language pair while the multilingual ( ML ) strategy trains a single multilingual QE model for all language pairs , where the training data is simply pooled together without any language identiÔ¨Åer . 
We note that this multilingual model here corresponds to a pooled , single - task learning approach . 
Multi - task Learning QE Model ( MTL ) Multitask learning has shown promising results in different NLP tasks ( Ruder , 2017 ) . 
Here , we want to explore whether having parameter sharing across languages is beneÔ¨Åcial , and to what extent having language - speciÔ¨Åc predictors can boost performance . 
Therefore , we experiment with a simple multi - task approach where we concurrently optimise multiple QE BASE models that use a language - speciÔ¨Åc ( LS ) training strategy . 
To allow for testing in zero - shot conditions , we also train Figure 2 : Multi - task learning QE model ( MTL ) with a shared XLM - R encoder . 
a language - agnostic ( LA ) component , which receives sampled data from every language . 
We refer to these two models as MTL - LA andMTL - LS . 
As seen in Figure 2 , the MTL - LS submodels and MTL - LA submodel share a common XLM - R encoder , while each submodel has its own dedicated language - speciÔ¨Åc MLP . 
The intuition of this approach is that it can result in improved learning efÔ¨Åciency and prediction accuracy by exploiting the similarities and differences in the QE tasks for different language directions ( Thrun , 1996 ; Baxter , 2000 ) . 
At training time , we iterate through the MTL - LS submodels in a round - robin fashion and alternate between training the MTL - LA submodel and training the chosen MTL - LS submodel . 
At test time , we can evaluate a test set with either the MTL - LA submodel or the MTL - LS submodel trained on the same language pair as the test set . 
4 Experimental Setup 4.1 QE Dataset We use the ofÔ¨Åcial data from the WMT 2020 QE Shared Task 11 . 
This dataset contains sentences extracted from Wikipedia ( Fomicheva et al . 
, 2020 ) and Reddit for Ru - En , translated to and from English for a total of 7 language pairs . 
The language pairs are divided into 3 categories : the high - resource English ‚Äì German ( En - De ) , English ‚Äì Chinese ( En - Zh ) and Russian ‚Äì English ( Ru - En ) pairs ; the medium - resource Romanian ‚Äì English ( Ro - En ) and Estonian ‚Äì English ( Et - En ) pairs ; and 1http://statmt.org/wmt20/ quality - estimation - task.html368 the low - resource Sinhala ‚Äì English ( Si - En ) and Nepali ‚Äì English ( Ne - En ) pairs . 
Each translation was produced with SOTA transformer - based NMT models and manually annotated for quality using an annotation scheme inspired by the Direct Assessment ( DA ) methodology proposed by Graham et al . 
( 2013 ) . 
SpeciÔ¨Åcally , translations were annotated on a 0 - 100 scale , where the 0 - 10 range represents an incorrect translation ; 11 - 29 , a translation with few correct keywords , but the overall meaning is different from the source ; 30 - 50 , a translation with major mistakes ; 51 - 69 , a translation which is understandable and conveys the overall meaning of the source but contains typos or grammatical errors ; 70 - 90 , a translation that closely preserves the semantics of the source sentence ; and 90 - 100 , a perfect translation . 
Figure 3 shows the distribution of DA scores for the different language pairs . 
Figure 3 : Distribution of DA judgments for different language pairs . 
4.2 Settings We train and test our models in the following conditions : Data splits we use the training and development sets provided for the WMT2020 shared task on QE.2Since the test set is not publicly available , we further split the 7,000 - instance training set for each language pair by using the Ô¨Årst 6,000 instances for training and the last 1,000 instances for development , and report results on the ofÔ¨Åcial ( 1,000 ) development set . 
Training details We optimise our models with Adam ( Kingma and Ba , 2015 ) and use the same 2http://www.statmt.org/wmt20/ quality-estimation-task.htmllearning rate ( 1e‚àí6 ) for all experiments . 
We use a batch size of 8 and train on Nvidia V100 GPUs for 20 epochs . 
Each model is trained 5 times with different random seeds . 
Evaluation All results in this paper are in terms of the average Pearson ‚Äôs correlation for predicted QE scores against gold QE scores over the 5 different runs . 
Pearson correlation is the standard metric for this task , but we also compute error using Root Mean Squared Error ( RMSE ) ( see Appendix ) . 
5 Results In what follows , we pose and discuss various hypotheses on multilinguality for QE . 
First we focus on our black - box approach from Section 3 ( Section 5.1 ) . 
Second , we examine the behavior of a glassbox approach which does not directly model the source and target texts in multilingual settings ( Section 5.2 ) . 
In all cases , we deÔ¨Åne TrainL as the set of language pairs used for training the QE model , andTestL as the set of language pairs used at test time . 
5.1 Black - box QE Approach 5.1.1 Multilingual models are better than bilingual models As we can see from the results in Table 13 , the average Pearson ‚Äôs correlation scores of the multilingual models are always higher the bilingual ones , in some cases by a large margin . 
This is particularly true for En - De where the best BL model performs at Pearson ‚Äôs correlation of 0.39 , while both BASEML and MTL - LA achieve 0.47 , which is a 20.5 % relative improvement over the best BL model . 
Furthermore , the average score of Base - ML across all TestL is 0.69 , 0.03 ( 4.5 % ) higher than the average score ( 0.66 ) of the best BASE - BL scores across all TestL ( diagonal in the top part of Table 1 ) . 
The results clearly show that multilingual models generally outperform bilingual models , even when the latter are optimised individually for different TestL. An interesting observation in Table 1 is that some BASE - BL models trained on different TrainL than TestL can perform almost as well as the models trained on the same TrainL as TestL. For example , 3The best results for BASE - BL are underlined and bold marks the best results across all models . 
SigniÔ¨Åcant improvements over BASE BL are marked with * . 
We use the HotellingWilliams test for dependent correlations to compute signiÔ¨Åcance of the difference between correlations ( Williams , 1959 ) with p - value < 0.05.369 Model Strategy TrainLTestL En - De En - Zh Et - En Ro - En Si - En Ne - En Ru - En Avg BASEBLEn - De 0.39 ( -0.17 ) ( -0.39 ) ( -0.51 ) ( -0.32 ) ( -0.51 ) ( -0.35 ) 0.34 En - Zh ( -0.02 ) 0.47 ( -0.19 ) ( -0.36 ) ( -0.16 ) ( -0.24 ) ( -0.17 ) 0.50 Et - En ( -0.10 ) ( -0.08 ) 0.75 ( -0.20 ) ( -0.07 ) ( -0.10 ) ( -0.08 ) 0.57 Ro - En ( -0.10 ) ( -0.14 ) ( -0.02 ) 0.89 ( -0.02 ) ( -0.04 ) ( -0.08 ) 0.60 Si - En ( -0.13 ) ( -0.13 ) ( -0.08 ) ( -0.15 ) 0.66 ( -0.05 ) ( -0.07 ) 0.57 Ne - En ( -0.10 ) ( -0.11 ) ( -0.06 ) ( -0.08 ) ( -0.01 ) 0.77 ( -0.08 ) 0.60 Ru - En ( -0.04 ) ( -0.09 ) ( -0.19 ) ( -0.26 ) ( -0.11 ) ( -0.16 ) 0.70 0.54 ML All 0.47 * 0.49 0.78 * 0.89 0.70 * 0.78 0.73 0.69 MTLLS All 0.45 0.48 0.77 0.89 0.66 0.79 0.72 0.68 LA All 0.47 * 0.49 0.76 0.89 0.66 0.78 0.72 0.68 LS En- * 0.41 0.46 - - - - - LA En- * 0.45 0.46 - - - - - LS * -En - - 0.78 * 0.90 0.69 0.79 0.73 LA * -En - - 0.78 * 0.89 0.69 0.78 0.73 ‚Ä°BERT - BiRNN ( Fomicheva et al . 
, 2020 ) 0.27 0.37 0.64 0.76 0.47 0.55 - ‚Ä°WMT20 QE Shared Task 1 Leaderboard ( June 2020 ) 0.47 0.48 0.79 0.90 0.65 0.79 0.78 0.69 Table 1 : Results for BASE and MTL QE models . 
We train different BASE - BL models for every language pair and a single BASE - ML model on all language pairs . 
We also train a single MTL QE model consists of multiple MTL - LS and MTL - LA submodels . 
For each TestL , we evaluate it with the MTL - LS submodel trained on the same language pair . 
We bold the best results across all models . 
SigniÔ¨Åcant improvements over BASE BL are marked with * .‚Ä°identiÔ¨Åes systems trained on the full 7,000 - instance training set with performances reported on the ofÔ¨Åcial test set of the WMT‚Äô20 QE Shared Task 1 ( https://competitions.codalab.org/competitions/24447 ) , which we assume to come from the same distribution as the dev set . 
a BASE - BL model trained on En - Zh and tested on En - De performs at average Pearson ‚Äôs correlation of 0.37 , which is only 0.02 below the best result . 
We hypothesize that XLM - R might be capturing certain traits in TrainL that can generalise well to other TestL , i.e. the complexity of source sentences or the Ô¨Çuency of the target sentences ( Sun et al . 
, 2020 ) . 
5.1.2 There is little beneÔ¨Åt from specialisation Here we investigate whether having specialised language - speciÔ¨Åc sub - models which can beneÔ¨Åt from the shared supervision from other languages while keeping their focus on a language - speciÔ¨Åc task can help to improve performance . 
Furthermore , it is possible that multi - task learning works better when language pairs share certain characteristics . 
Therefore , we also investigate whether combining language pairs that share either source or target languages can be more beneÔ¨Åcial . 
For that , we use the MTL models but with a reduced set of languages . 
From the results in Table 1 , we observe that language - specialised predictors do not help improve performance . 
There is no clear advantage in using the multi - task learning QE approach ( MTL - LS and MTL - LA ) where each language pair is treated as a separate task ; over the simple singletask multi - lingual learning approach ( BASE - ML ) , despite the former having more parameters and language - speciÔ¨Åc MLP layers . 
In the table , we compare MTL models trained on language pairs that share the source language ( En- * ) or the target language ( * -En ) against MTL models trained on all languages ( All ) . 
As we can see from the results , the MTL model trained on En * perform worse than the MTL model trained on all language pairs . 
In contrast , the MTL model trained on * -En performs a little bit better than the MTL model trained on all language pairs on 4 out of the 5 language pairs and is comparable to Base - ML on those language directions . 
5.1.3 Multilingual models help zero- and few - shot QE To test whether a multilingual model for QE can generalise beyond the language pairs observed during training , we also conduct experiments varying amounts of in - language data ( i.e. 0 % ‚Äì zeroshot , 5 % , 10 % , 25 % , 50 % , 75 % and 100 % ) . 
We build and compare BASE - BL and BASE - ML models . 
We train BASE - BL models only on the sub-370 TestL % in - lang Model Strategy En - De En - Zh Et - En Ro - En Si - En Ne - En Ru - En Avg 0 BASE ML 0.45 0.42 0.75 0.80 0.68 0.76 0.68 0.65 5 BASEBL 0.13 0.39 0.65 0.70 0.58 0.63 0.63 0.53 ML 0.38 0.44 0.74 0.85 0.67 0.76 0.71 0.65 10 BASEBL 0.24 0.43 0.69 0.85 0.56 0.68 0.64 0.58 ML 0.37 0.46 0.75 0.87 0.64 0.77 0.71 0.65 25 BASEBL 0.27 0.45 0.70 0.87 0.61 0.72 0.70 0.62 ML 0.40 0.46 0.75 0.88 0.66 0.76 0.71 0.66 50 BASEBL 0.33 0.47 0.74 0.88 0.62 0.74 0.69 0.64 ML 0.41 0.48 0.76 0.89 0.69 0.77 0.72 0.67 75 BASEBL 0.39 0.47 0.75 0.88 0.64 0.76 0.70 0.66 ML 0.46 0.49 0.78 0.89 0.70 0.78 0.71 0.69 100 BASEBL 0.39 0.47 0.75 0.89 0.66 0.77 0.70 0.66 ML 0.47 0.49 0.78 0.89 0.70 0.78 0.73 0.69 Table 2 : Results of BASE QE models for different portions of training data ( % data ) . 
For BASE - ML , we train the models on subsampled training data in the test language pair and all training data in other language pairs . 
For BASE - BL , we train the models on only subsampled training data in the test language pair . 
We underline the best results for each % data setting . 
sampled in - language training data and train BASEML on both sub - sampled in - language training data and all training data in other language pairs . 
In other words , we want to know whether multilingual QE helps if we have limited or no training data in our desired test language pair . 
Results are shown in Table 2 . 
For ease of visualisation , we also plot the Pearson ‚Äôs correlation results against the percentage of in - language training data in Figure 4 . 
As seen in Table 2 , the multilingual model performs better than the bilingual models on all language pairs for every conÔ¨Åguration of training data . 
Moreover , in 3 out of 7 cases , the zero - shot models perform better than the fully - trained bilingual models . 
This provides strong evidence that the QE task can be solved in a multilingual way , without loss of performance compared to bilingual performance . 
It also shows strong evidence for the zero - shot applicability of our models . 
5.2 Glass - box QE Approach Having pre - trained representations can help build state - of - the - art multilingual systems . 
However , these representations are costly to compute in practice , which limits their applicability for building QE systems for real - time scenarios . 
Glass - box approaches to QE extract information from the NMT system itself to predict quality , without directly relying on the source and target text or using any external resources . 
To test how well this information can generalise across different languages , we lever-0 5 10 25 50 75 10000.20.40.60.81 % DataCorrelationEn - De En - Zh Et - En Ro - En Si - En Ne - En Ru - En Avg Figure 4 : Results of BASE QE models for various zeroshot and few - shot cross - lingual transfer settings . 
The solid lines represent the BASE ML models while the dashed lines are the BASE BL models . 
age existing work on glass - box QE by Fomicheva et al . 
( 2020 ) that explores NMT output distribution to capture predictive uncertainty as a proxy for MT quality . 
We use the following 5 best - performing glass - box indicators from their work : ‚Ä¢Average NMT log - probability of the translated sentence ; ‚Ä¢Variance of word - level log - probabilities ; ‚Ä¢Entropy of NMT softmax output distribution ; ‚Ä¢NMT log - probability of translations generated with Monte Carlo dropout ( Gal and Ghahramani , 2016);4 4This method consists in performing several forward371 TrainLTestL En - De En - Zh Et - En Ro - En Si - En Ne - En En - De 0.24 ( -0.25 ) ( -0.36 ) ( -0.22 ) ( -0.24 ) ( -0.32 ) En - Zh ( +0.08 ) 0.44 ( -0.05 ) ( -0.04 ) ( -0.03 ) ( -0.08 ) Et - En ( +0.07 ) ( -0.03 ) 0.61 ( -0.02 ) ( -0.02 ) ( -0.06 ) Ro - En ( +0.05 ) ( -0.05 ) ( -0.03 ) 0.76 ( -0.02 ) ( -0.06 ) Si - En ( +0.06 ) ( -0.04 ) ( -0.04 ) ( -0.03 ) 0.54 ( -0.03 ) Ne - En ( -0.00 ) ( -0.09 ) ( -0.09 ) ( -0.09 ) ( -0.04 ) 0.58 All langs 0.32 0.44 0.60 0.75 0.55 0.56 Best feature 0.26 0.32 0.64 0.69 0.51 0.60 Table 3 : Pearson correlation for regression models based on glass - box features trained on each language pair and evaluated either on the same language pair or other language pairs . 
For testing on a different language pair we report the difference in Pearson correlation with respect to training and testing on the same language pair . 
For comparison we show the correlation individual best performing feature with no learning involved . 
‚Ä¢Lexical similarity between MT hypotheses generated with Monte Carlo dropout . 
We train an XGboost regression model ( Chen and Guestrin , 2016)5to combine these features to predict DA judgments and test the performance of the model in multilingual settings . 
Table 3 shows Pearson correlation for the regression models trained on each language pair and evaluated either on the same language pair or other language pairs.6The ‚Äô All langs ‚Äô row indicates the results when training on all language pairs , whereas ‚Äô Best feature ‚Äô indicates the correlation obtained by the best performing feature individually . 
Comparing these results to the results for pre - trained representations in Table 1 we can make three observations . 
5.2.1 Glass - box features are more comparable across languages First , although the correlation is generally lower for the glass - box approach , performance degradation when testing on different language pairs is smaller . 
For all language pairs except EnglishGerman , we observe a relatively small decrease in performance ( up to 0.09 ) when training and test language pairs are different . 
This suggests that the indicators extracted from the NMT model are more passes through the network , collecting posterior probabilities generated by the model with parameters perturbed by dropout and using the resulting distribution to approximate model uncertainty . 
5We chose a regression model over an NN given the smaller number of features available . 
6These experiments do not include Russian - English , as the corresponding NMT system is an ensemble and it is not evident how the glass - box features proposed by Fomicheva et al . 
( 2020 ) should be extracted in this case.comparable across languages than input features from pre - trained representations . 
We note that the NMT systems in MLQE dataset were all based on Transformer architecture but trained using different amount of data and have different overall output quality . 
Interestingly , the results of this experiment indicate that glass - box information extracted from these systems could be language - independent . 
More experiments are needed to conÔ¨Årm if this observation can be extrapolated to other datasets , language pairs , domains and MT systems . 
5.2.2 Multilingual gains are limited by learning algorithm Second , by contrast to the results in Table 1 where multilingual training brings signiÔ¨Åcant improvements , we do not see any gains in performance from training with all available data . 
The reason could be that training a regression model with a small number of features does not require large amounts of training data , and therefore performance does not improve with additional data . 
English - German is an exception with a large gain in correlation when training on all language pairs . 
5.2.3 The output label distribution matters Finally , similarly to the black - box approach in Table 1 , the performance for English - German beneÔ¨Åts from using the data from other language pairs for training . 
This indicates that the results are affected by factors that are independent of the approach used for prediction . 
To better understand these results we look at the distribution of NMT logprobabilities ( Figure 5 ) and the distribution of DA scores ( Figure 3).372 Figure 5 : Distribution of NMT log - probabilities for different language pairs While log - probability distributions are comparable across language pairs , the distributions of DA scores are very different . 
We suggest , therefore , that the decrease in performance when testing on a different language is related to a higher extent to the shift in the output distribution across languages ( i.e. DA judgments ) than to the shift in the input features . 
This also explains the difÔ¨Åculty for training and predicting on English - German data where the distribution of DA scores is highly skewed with minimal variability in the quality range . 
6 Discussion and Conclusions From our various experiments , one setting that stood out is that of English - German . 
We suggest that the difÔ¨Åculty for predicting quality for this language pair was exacerbated by the metric used for evaluation . 
Because of its sample - dependence , Pearson correlation can be more sensitive to the output distribution . 
In contrast , an error - based metric like RMSE will be less sensitive to these variations . 
To illustrate these effects , in Figure 6 , we show the hierarchical clustering of language directions obtained by using the metric value from training on one direction and testing on another one as a notion of distance . 
In subÔ¨Ågure ( a ) , we observe the clusters based on Pearson correlation as shown in Table 1 . 
In subÔ¨Ågure ( b ) , we observe the same clustering done based on RMSE . 
It should be noted that in the former , En - De is a clear outlier , whereas in the latter , we have a clustering that is more consistent with the general maturity of the language pairs : Ne - En and Si - En are low resource , Ro - En and Et - En are medium resource , etc . 
We explored the use of multilingual contextual ( a ) Pearson correlation ( b ) RMSE Figure 6 : Language hierarchical clustering according to the results of training on one language and testing on another . 
In subÔ¨Ågure ( a ) we plot the clustering based on Pearson correlation . 
In subÔ¨Ågure ( b ) we plot the same clustering based on RMSE . 
The y axis denotes thedistance between language pairs according to each evaluation . 
representations to build state - of - the - art multilingual QE models . 
From our experiments , we observed that : 1 ) multilingual systems are always better than bilingual systems ; 2 ) having multi - task models , which share parts of the model across languages and specialise others , does not necessarily yield better results ; and 3 ) multilingual systems for QE generalise well across languages and are powerful even in zero - shot scenarios . 
We also contrasted the use of pre - trained representations which are costly to obtain , to the use of glass - box features which can be extracted from the NMT system . 
We observed that glass - box features are very comparable across languages , and training multilingual systems with them adds little value . 
Finally , we observed that the distribution of the output labels matters for the evaluation of QE.373 Acknowledgments Marina Fomicheva , Fr ¬¥ ed¬¥eric Blain and Lucia Specia were supported by funding from the Bergamot project ( EU H2020 Grant No . 
825303 ) . 
Abstract Approaching named entities transliteration as a Neural Machine Translation ( NMT ) problem is common practice . 
While many have applied various NMT techniques to enhance machine transliteration models , few focus on the linguistic features particular to the relevant languages . 
In this paper , we investigate the effect of incorporating phonetic features for English - to - Chinese transliteration under the multi - task learning ( MTL ) setting ‚Äî where we deÔ¨Åne a phonetic auxiliary task aimed to improve the generalization performance of the main transliteration task . 
In addition to our system , we also release a new English - toChinese dataset and propose a novel evaluation metric which considers multiple possible transliterations given a source name . 
Our results show that the multi - task model achieves similar performance as the previous state of the art with a model of a much smaller size.1 1 Introduction Transliteration , the act of mapping a name from the orthographic system of one language to another , is directed by the pronunciation in the source and target languages , and often by historical reasons or conventions . 
It plays an important role in tasks like information retrieval and machine translation ( Marton and Zitouni , 2014 ; Hermjakob et al . 
, 2008 ) . 
Over the recent years , many have addressed transliteration using sequence - to - sequence ( seq2seq ) deep learning models ( Rosca and Breuel , 2016 ; Merhav and Ash , 2018 ; Grundkiewicz and HeaÔ¨Åeld , 2018 ) , enhanced with several NMT techniques ( Grundkiewicz and HeaÔ¨Åeld , 2018 ) . 
However , this recent work neglects the most crucial feature for transliteration , i.e. pronunciation . 
To * Work done at The University of Edinburgh . 
1Our code and data are available at https://github . 
com / Lawhy / Multi - task - NMTransliteration .English IPA Chinese Pinyin A /"eI./ Ëâæ ` ai my /mi/ Á±≥ mÀáƒ± Table 1 : An example of English - to - Chinese transliteration , from Amy toËâæÁ±≥ . 
Each row presents a group of corresponding subsequences in different representations . 
bridge this gap , we deÔ¨Åne a phonetic auxiliary task that shares the sound information with the main transliteration task under the multi - task learning ( MTL ) setting . 
Depending on the speciÔ¨Åc language , the written form of a word reveals its pronunciation to various extents . 
For alphabetical languages such as English and French , a letter , or a sequence of letters , usually reÔ¨Çects the word pronunciation . 
For example , the word Amy ( in the International Phonetic Alphabet , IPA , /"eI.mi/ ) has the sub - word Acorresponding to /"eI./ andmycorresponding to /mi/ . 
In contrast , characters in a logographic2writing system for languages like Chinese or Japanese do not explicitly indicate sound ( Xing et al . 
, 2006 ) . 
In this paper , we give a treatment to the problem of transliteration from English ( alphabet ) to Chinese3(logogram ) using an RNN - based MTL model with a phonetic auxiliary task . 
We transform each Chinese character to the alphabetical representation of its pronunciation via the ofÔ¨Åcial phonetic writing system , Pinyin,4which uses Latin letters with four diacritics denoting tones to represent the sounds . 
2A logogram is an individual character that represents a whole word or phrase . 
3The Chinese language we mention in this paper refers explicitly to Mandarin , which is the ofÔ¨Åcial language originated from the northern dialect in China . 
4Pinyin is the ofÔ¨Åcial romanization system for Standard Chinese ( Mandarin ) in mainland China and to some extent in Taiwan . 
It does not apply to other Chinese dialects.378 For example , the Chinese transliteration for Amy is ËâæÁ±≥and the associated Pinyin representation is ` ai mÀáƒ± . 
We summarize the correspondences occurring in this example in Table 1 . 
Due to the similarity between the source name and the Pinyin representation , Jiang et al . 
( 2009 ) proposed a sequential transliteration model that uses Pinyin as an intermediate representation before transliterating a Chinese name to English . 
In contrast , our idea is to build a model with a shared encoder and dual decoders , that can learn the mapping from English to Chinese and Pinyin simultaneously . 
By jointly learning source - to - target and source - to - sound mappings , the encoder is expected to generalize better ( Ruder , 2017 ) and pass more reÔ¨Åned information to the decoders . 
Transliteration datasets are often extracted from dictionaries , or aligned corpus generated from applying named entity recognition ( NER ) system to parallel newspaper articles in different languages ( Sproat et al . 
, 2006 ) . 
We use two datasets for our experiments , one taken from NEWS Machine Transliteration Shared Task ( Chen et al . 
, 2018 ) and the other extracted from a large dictionary . 
We evaluate the transliteration system using both the conventional word accuracy and a novel metric designed for English - to - Chinese transliteration ( see Section 5 ) . 
Our contributions are as follows : 1.We make available a new English - to - Chinese named entities dataset ( ‚Äú DICT ‚Äù ) particular to names of people . 
This dataset is based on the dictionary A Comprehensive Dictionary of Names in Roman - Chinese ( Xinhua News Agency , 2007 ) . 
2.We propose a substitution - based metric called Accuracy with Alternating Character Table ( ACC - ACT ) , which gives a better estimation of the system ‚Äôs quality than the traditional word accuracy ( ACC ) . 
3.We propose a multi - task learning transliteration model with a phonetic auxiliary task , and run experiments to demonstrate that it attains better scores than single - main - task or single - auxiliarytask models . 
We report accuracy and F - score of 0.299and 0.6799 , respectively , on the NEWS dataset , with a model of size 22 M parameters , compared to the previous state of the art ( Grundkiewicz and HeaÔ¨Åeld , 2018 ) , which achieves accuracy and F - score of 0.304and0.6791 , respectively , with a model of size133 M parameters . 
On the DICT dataset , forSource ( x)Target ( y)Pinyin ( p ) Caleigh ÂáØËéâ kai li Table 2 : An example data point under our multi - task learning setting . 
the same model sizes , we report accuracy of 0.729 as compared to their 0.732 . 
2 Problem Formulation We use the word vocabulary to describe the set of characters for the purpose of our task speciÔ¨Åcation . 
LetVsrcandVtgtdenote the source and target vocabularies , respectively . 
For a source word xof lengthIand a target word yof lengthJ , we have : x= ( x1,x2, ... ,x I)‚ààVI src , y= ( y1,y2, ... ,y J)‚ààVJ tgt . 
where thekth element in the vector denotes a character at position k. We formulate the task of transliteration as a supervised learning problem : given a collection of n training examples , { ( x(i),y(i))}n i=0 , the objective is to learn a predictor function , f : x‚Üíy , of which the parameter space maximizes the following conditional probability : P(y|x)Chain Rule = J / productdisplay j=1P(yj|y1, ... ,y j‚àí1,x ) . 
For our multi - task transliteration model , the predictor becomes fMTL : x‚Üí(y , p ) , where pdenotes the written representation of the pronunciation of the target word y. For decoding , we maximize the conditional probabilities , P(p|x,Àú y)and P(y|x,Àú p ) , where Àú yandÀú prefers to the implicit information channeled by one task to the other . 
The phonetic information we use for our task refers to the Pinyin version of the name in Chinese , without tone marks,5because they are often removed for spelling Chinese names in an alphabetical language . 
We present an example data point in the form of ( x , y , p)in Table 2 . 
3 Dataset Preparation We experiment with two different English - toChinese datasets . 
For simplicity , we denote the one 5For example , the Pinyins , ch¬Øƒ± , ch¬¥ƒ± , chÀáƒ±andch`ƒ± , are all transformed to chi . 
Note that this process will decrease the vocabulary size.379 taken from NEWS Machine Transliteration Shared Task ( Chen et al . 
, 2018 ) as ‚Äú NEWS , ‚Äù and the one extracted from the dictionary ( Xinhua News Agency , 2007 ) as ‚Äú DICT . 
‚Äù 3.1 NEWS Dataset We use the preprocessing script6created by Grundkiewicz and HeaÔ¨Åeld ( 2018 ) to construct the NEWS dataset from raw data provided in the Shared Task ( Chen et al . 
, 2018 ) . 
This script merges the raw English - to - Chinese and Chinese - to - English datasets into a single one , then transforms it to uppercase7and tokenizes all names into sequences of characters ( words are treated as sentences , characters are treated as words ) . 
In addition , it takes 513 examples from the training data to form the internal development set and uses the ofÔ¨Åcial development set as the internal test set . 
To make the Ô¨Ånal comparison , we download the source - side data of the ofÔ¨Åcial test set from the Shared Task ‚Äôs website,8and submit the transliteration results ( see Section 6.4 ) . 
3.2 DICT Dataset The source dictionary contains approximately 680 K name pairs for transliteration from other languages than Chinese . 
We extracted 58,456 pairs that originated in English and performed the following preprocessing steps : 1.For the source side ( English ) , we remove the inverted commas and white spaces from names that contain them ( e.g. A‚ÄôCourt , Le Gresley ) . 
2.For both sides , we lowercase9all the words and tokenize them into sequences of characters . 
3.Name pairs with multiple target transliterations are removed from the dataset and saved in a separate Ô¨Åle for the construction of the ACT ( see next paragraph ) . 
As such , every name pair becomes unique in our preprocessed dataset . 
We randomly divide the rest into the ratio of 8 : 1 : 1 , to form training , development and test sets . 
We report the Ô¨Ånal partitions of both datasets in Table 3 . 
6Available at https://github.com/snukky/ news - translit - nmt . 
7We lowercase all the words in both NEWS and DICT datasets as evaluating transliteration is case - insensitive . 
8The ofÔ¨Åcial test set with task ID T - EnCh is available at : http://workshop.colips.org/news2018/ dataset.html . 
9Lowercasing does not affect Chinese characters as they are not alphabetical . 
Source Train Dev Test NEWS 81,252 513 1,000 DICT 46,620 5,828 5,828 Table 3 : Numbers of data points in training , development and test sets of NEWS and DICT datasets . 
Dev and Test for the NEWS dataset ( Ô¨Årst row ) refer to the internal development and test set , respectively . 
3.3 Alternating Character Table Chinese characters10that sound alike can often replace each other in the transliteration of a name from other languages . 
Unlike an alphabetical language where a similar pronunciation is bounded to sub - words of various lengths , characters in Chinese have concrete and independent pronunciations . 
Thus , we can conveniently build the Alternating Character Table ( ACT ) with each row storing a list of interchangeable characters . 
We construct the ACT based on the DICT dataset because it contains less noise after applying significant data cleansing . 
In total , 449English names from the DICT dataset have more than one transliterations in Chinese . 
We purposely removed all these names from the DICT data during the preprocessing so as to ensure that we are not using any knowledge from the test set . 
The Ô¨Ånal ACT contains 29 rows ( see Appendix ) and we use it with our adaptive evaluation metric ( see Section 5 ) . 
3.4 Pinyin Conversion In transliteration , the pronunciations of the Chinese characters are often unique ( even for a polyphonic character , e.g. ‰ªÄ , that has more than one Pinyins , sh¬¥ƒ±andsh¬¥en , only sh¬¥ƒ±is commonly used in transliteration ) . 
Therefore , we can directly transform each Chinese character into a unique Pinyin , thus forming the target data for the auxiliary task . 
The procedure is as follows : for each character ytin the target name y , we use the Python package pypinyin11 to mapytto the corresponding Pinyin ( without the tone mark ) . 
The tool will generate the most frequently used Pinyin for each ytbased on dictionary data . 
We then apply further manual correction on the Pinyins because the most frequent Pinyin is not necessarily the one used in transliteration . 
10Limited to the set of characters ( with size ‚âà1 K out of 80 K ) commonly used in transliteration . 
11Available at : https://github.com/mozillazg/ python - pinyin . 
We use the lazy pinyin feature to generate Pinyins without tone marks.380 Figure 1 : Visualization of the Seq2MultiSeq model . 
The left half illustrates the components involved in the main task and the right half is for the auxiliary task . 
The shared part is the encoder that consists of a source embedding layer and a stacked biRNN ( top middle ) . 
4 Model Our model is intent on solving English - to - Chinese transliteration through joint supervised learning of source - to - target ( main ) and source - to - Pinyin ( auxiliary ) tasks . 
Training closely related tasks together can help the model to learn information that is often ignored in single - task learning , thus obtaining a better representation in the shared layers ( in our case , encoder ) . 
Moreover , the auxiliary task implicitly provides the phonetic information that is not easily learned through the single main task given the characteristics of Chinese ( see Section 1 ) . 
Our model has a sequence - to - multiple - sequence ( Seq2MultiSeq ) architecture that contains a shared encoder and dual decoders . 
Between the encoder and decoder is a bridge layer12that transforms the 12We call it ‚Äú bridge ‚Äù because it connects the shared encoder to each decoder . 
It allows Ô¨Çexible choices of the hidden sizes of the encoder and decoder and serves as the intermediate ‚Äú buffer ‚Äù before passing the encoder Ô¨Ånal state to each decoder.encoder ‚Äôs Ô¨Ånal state into the decoder ‚Äôs initial state ( see Figure 1 ) . 
The encoder has an embedding layer with dropout ( Hinton et al . 
, 2012 ) , followed by a 2layer biLSTM ( Schuster and Paliwal , 1997 ) . 
The bridge layer consists of a linear layer followed by tanh activation . 
The shared encoder passes its Ô¨Ånal state to the main - task decoder and the auxiliarytask decoder via separate bridge layers . 
In each decoder , we use additive attention ( Bahdanau et al . 
, 2015 ) to compute the context vector ( weighted sum of the encoder outputs according to the attention scores ) , then concatenate it with the target embedding to form the input of the subsequent 2 - layer feed - forward LSTM . 
The prediction is made by feeding the concatenation of the LSTM ‚Äôs output , the context vector and the target embedding into a linear layer followed by log - softmax . 
Our model is expected to simultaneously maximize the conditional probabilities mentioned in Section 2 . 
To achieve this goal , we use the linear combination of the main - task decoder ‚Äôs loss13 ( negative log likelihood ; ly ) and the auxiliary - task decoder ‚Äôs loss ( lp ) as the model ‚Äôs objective function : lMTL = Œª¬∑ly+ ( 1‚àíŒª)¬∑lp , where the subscript MTL stands for multi - task learning and 0 < Œª < 1 . 
Note that for Œª= 0 andŒª= 1 , it is equivalent to train on a single auxiliary task and a single main task , respectively . 
The whole system is implemented using the deep learning framework PyTorch ( Paszke et al . 
, 2019).14 5 Adaptive Evaluation Metrics We evaluate the transliteration system using word accuracy ( ACC ) and its variants on the 1 - best output : ACC = 1 N / summationdisplay ( y,ÀÜy)Icriterion ( ÀÜy , y ) , whereNis the total number of test - set samples , Icriterion ( ÀÜy , y)is an indicator function with value 1if the prediction ( top candidate ) ÀÜymatches the referenceyunder certain criterion . 
The simplest criterion is exact string match between ÀÜyandy . 
If the test set contains multiple target words for a single source word , we let indicator be 1if the prediction matches one of the references ( Chen et al . 
, 2018 ) . 
13We use nn . 
NLLLoss ( ) from the PyTorch library . 
14Available at https://pytorch.org/ .381 Source Target ( F ) Target ( M ) MED Mona Ëé´Â®ú Ëé´Á∫≥ 1 Colina ÁßëËéâÂ®úÁßëÂà©Á∫≥ 2 Table 4 : Examples of a single source name with more than one target transliterations , with ( F ) and ( M ) indicating female and male , respectively . 
We use ACC and ACC+ to denote the original accuracy and its variant with multiple references . 
The drawback of ACC is that it may underestimate the quality of the system because it neglects the possibility of having more than one transliteration for a given source name , as is the case for English - to - Chinese transliteration . 
For example in Table 4 , if the test set only includes Target ( F ) for a Source while the model predicts Target ( M ) , ACC will mistakenly count it as wrong . 
Although ACC+ considers the alternatives appearing in the dataset , it is unrealistic to expect the dataset to contain all possible references . 
To resolve this issue , we propose a new variant of word accuracy speciÔ¨Åc to English - to - Chinese transliteration . 
Based on the knowledge of a native Chinese speaker , we analyze the English - to - Chinese dataset and summarize the key observations for source names with multiple target transliterations as follows : the minimum edit distance ( MED ) between any two target names ‚â§2 , and the lengths are the same ; for any two such target names , distinct characters occur in the same position , and they often indicate the gender of the name ( see Table 4 ) . 
To use ACT in accord with the above observations , we propose the following criterion for the accuracy indicator function ( we refer to it as ACC - ACT ) . 
Let subscript tdenote the position of a character , then Icriterion ( ÀÜy , y)= 1 if either MED ( ÀÜy , y ) = 0 ( which covers all the cases for ACC ) or the following conditions are met in order : 1.ÀÜyandyare of the same length , L ; 2.MED ( ÀÜy , y)‚â§2and distinct characters of ÀÜyand ymust occur in the same position(s ) ; 3.IfÀÜyt / negationslash = ytfor1‚â§t‚â§L , replace ÀÜytby looking up the ACT and this condition will be satisÔ¨Åed if any of the modiÔ¨Åed ÀÜy(s ) can match yexactly . 
There is no guarantee that characters that are interchangeable according to ACT can replace each other in every scenario . 
But since we only applyEnc Dec - M Dec - A Emb.h256 256 128 Œ¥0.1 0.1 0.1 RNNh512 512 128 Œ¥0.2 0.2 0.1 Table 5 : Illustration of the model settings , where Emb . 
and RNN stand for the embedding layers and RNN units in each part ( column ) of the model , handŒ¥are the hidden size and dropout value , respectively . 
The column names ( from left to right ) stand for encoder , main - task decoder and auxiliary - task decoder . 
substitution on the output predictions rather than the references , we are not manipulating the test set by creating any new instance . 
This new metric ( ACC - ACT ) will ensure cases like in Table 4 are captured without requiring extra data in the test set , thus giving a more reasonable estimate of the system ‚Äôs quality than both ACC and ACC+ . 
6 Experimental Setup Recall from Section 4 that we use Œªto denote the weighting of the two tasks we train . 
We set the single - main - task ( Œª= 1 ) and the single - auxiliarytask ( Œª= 0 ) models as the baselines , and compare the multi - task models of different weightings ( Œª‚àà{1 6,1 4,1 2,2 3,5 6,8 9 } ) against them . 
We conduct experiments on both the NEWS and DICT datasets and select the best model for each of them to compare to the previous state of the art . 
6.1 Model and Training Settings The conÔ¨Ågurations of hidden sizes and dropout values of embedding layers and RNN units are presented in Table 5 . 
The type of all RNN units is LSTM and the number of layers is set to 2 . 
Besides the bridge layer that transforms the encoder ‚Äôs Ô¨Ånal hidden state to the decoder ‚Äôs initial hidden state , we add another one to carry the Ô¨Ånal cell state for using LSTM ( in total , we have 4‚Äúbridges ‚Äù ) . 
We use the Adam optimizer ( Kingma and Ba , 2015 ) with the batch size set to 64 . 
Evaluation of the development set is carried out on every 500 batches . 
We record the validation score ( ACC ) and decrease the learning rate ( initially set to 0.003 ) by 90 % if the score does not surpass the previous best . 
We pick the Ô¨Ånal model that attains the highest validation score within 100training epochs . 
For decoding in the training phase , we apply teacher forcing ( Williams and Zipser , 1989 ) with382 NEWS DICT Main Auxiliary Main Auxiliary Œª ACC ACC+ ACC - ACT ACC ACC ACC - ACT ACC 1 0.723 0.731 0.746 NA 0.725 0.748 NA 1/6 0.666 0.672 0.688 0.698 0.728 0.750 0.744 1/4 0.734 0.743 0.751 0.755 0.725 0.747 0.746 1/2 0.724 0.733 0.740 0.738 0.723 0.748 0.739 2/3 0.698 0.707 0.715 0.705 0.722 0.746 0.739 5/6 0.739 0.749 0.760 0.757 0.729 0.752 0.746 8/9 0.670 0.679 0.686 0.705 0.722 0.746 0.734 0 NA NA NA 0.743 NA NA 0.743 Table 6 : Experiment results on NEWS internal test set and DICT development set , where Œª= 1 andŒª= 0 are baselines of main task and auxiliary task , respectively . 
Maximum score in each metric is is bold . 
Figure 2 : The plots of main - task ACC against auxiliary - task ACC on the NEWS ( left ) and DICT ( right ) development sets . 
Colors indicate which multi - task model ( by Œªvalue ) the evaluation points belong to . 
To highlight the dense regions , we set the minimum of the x - axis to 0.5and0.6for NEWS and DICT datasets , respectively . 
the following empirical decay function : tfr= max / parenleftbigg 1‚àí10 + epoch√ó1.5 50,0.2 / parenrightbigg , where tfrrefers to the teacher forcing ratio , i.e. the probability of feeding the true reference instead of the predicted token . 
We use beam search decoding with beam size 10and length normalization ( Wu et al . 
, 2016 ) for evaluation . 
6.2 Evaluation We use ACC and ACC - ACT to evaluate the performance on the main task and ACC on the auxiliary task . 
Note that since the only data portion we have that contains multiple references given a source word is the internal test set of NEWS data , we apply ACC+ on this particular set exclusively.6.3 Model Selection In the experiments in this section , we tune Œªon the NEWS internal test set and DICT development set , and select the model with the highest ACC on the main task . 
The experiment results in Table 6 show that Œª=5 6yields the best models on both datasets . 
We observe a signiÔ¨Åcant improvement against the baselines on NEWS while a less noticeable increase on DICT . 
Besides , the models are more sensitive to Œªon NEWS than DICT ( with standard deviation 0.03 and 0.003 on ACC , respectively ) . 
Furthermore , we investigate the relationship between the main and the auxiliary tasks based on the evaluation points of the development set . 
In Figure 2 , we observe a nearly - total positive linear correlation between the main - task ACC and auxiliary - task ACC , and this is further evident in the Pearson cor-383 Internal Test OfÔ¨Åcial Test Main Auxiliary Main System ACC ACC+ ACC - ACT ACC ACC+ Baseline 0.724 0.733 0.742 0.736 NA Multi - task 0.739 0.749 0.760 0.757 0.299 BiDeep 0.731 0.739 0.746 0.740 NA BiDeep+ NA 0.765 NA NA 0.304 Table 7 : Experiment results on the NEWS internal test ( ofÔ¨Åcial development ) set and ofÔ¨Åcial test set , where ‚Äú Baseline ‚Äù refers to the single - task model and ‚Äú BiDeep+ ‚Äù refers to the best system Grundkiewicz and HeaÔ¨Åeld ( 2018 ) submitted to the NEWS workshop , and the corresponding scores are taken from their paper . 
Main Auxiliary System ACC ACC - ACT ACC Baseline 0.726 0.748 0.738 Multi - task 0.729 0.751 0.749 BiDeep 0.732 0.755 0.760 Table 8 : Experiment results on the DICT test set , where Baseline refers to the single - task model . 
User ACC+ F - score romang 0.3040 ( 1 ) 0.6791 ( 2 ) Ours 0.2990 ( 2 ) 0.6799 ( 1 ) saeednajaÔ¨Å 0.2820 ( 3 ) 0.6680 ( 3 ) soumyadeep 0.2610 ( 4 ) 0.6603 ( 4 ) Table 9 : Table of the NEWS leaderboard ( available at https://competitions.codalab.org/ competitions/18905#results , accessed 19 June 2020 ) . 
User ‚Äú romang ‚Äù refers to Grundkiewicz and HeaÔ¨Åeld ( 2018 ) . 
relation coefÔ¨Åcients15 , which are 0.982and0.992 for NEWS and DICT , respectively . 
This means the multi - task model improves the performance on both tasks simultaneously . 
6.4 Test - set Results and System Comparison We submit our 1 - best transliteration results on the NEWS ofÔ¨Åcial test set through the CodaLab link provided by the Shared Task ‚Äôs Committee and we present the leaderboard partially in Table 9 . 
Note that in addition to ACC+ , the leaderboard also 15Computed by pearsonr ( ) fromScipy library , which is available at : https://docs.scipy.org/doc/ scipy-0.14.0 / reference / generated / scipy . 
stats.pearsonr.html .records mean F - score16on which we rank Ô¨Årst . 
We report the test - set performance of our best multi - task model on NEWS in Table 7 and DICT in Table 8 , in comparison to the system built by Grundkiewicz and HeaÔ¨Åeld ( 2018 ) . 
The baseline model of their work employs the RNN - based BiDeep17architecture ( Miceli Barone et al . 
, 2017 ) which consists of 4 bidirectional alternating stacked encoder , each with a 2 - layer transition RNN cell , and 4 stacked decoders with base RNN of depth 2 and higher RNN of depth 4 ( Zhou et al . 
, 2016 ; Pascanu et al . 
, 2014 ; Wu et al . 
, 2016 ) . 
Besides , they strengthen the model by applying layer normalization ( Ba et al . 
, 2016 ) , skip connections ( Zhang et al . 
, 2016 ) and parameter tying ( Press and Wolf , 2017 ) . 
We reproduce their model without changing any conÔ¨Ågurations in their paper ( Grundkiewicz and HeaÔ¨Åeld , 2018 ) , and train it on both tasks separately . 
In Table 7 , we can see that the multi - task model performs signiÔ¨Åcantly better than both the singletask baseline and the BiDeep model in all metrics on NEWS . 
Note that the BiDeep model we reproduce achieves the same ACC+ as reported in the work of Grundkiewicz and HeaÔ¨Åeld ( 2018 ) and ACC+ is the only evaluation metric used in their paper . 
‚Äú BiDeep+ ‚Äù in the third row refers to the Ô¨Ånal system they submitted to the Shared Task , on which they adopted additional NMT techniques including ensemble modeling for re - ranking and synthetic data generated from back translation ( Sennrich et al . 
, 2017 ) . 
Our ACC+ score on 16The F - score metric measures the similarity between the target prediction and reference . 
Precision and Recall in this particular F - score are computed based on the length of the Longest Common Subsequence . 
See details in the NEWS whitepaper ( Chen et al . 
, 2018 ) . 
17Implemented with the Marian toolkit available at https : //marian - nmt.github.io / docs/ .384 Source Output ( ST ) Output ( MT ) ocallaghan Â••Âç°ÊãâÊ†πÂ••Âç°ÊãâÊ±â / check holleran ÈúçÂ∞î‰º¶ ÈúçÂãí‰º¶ / check ajemian ÈòøËµ´Á±≥ÂÆâÈòøÊù∞Á±≥ÂÆâ Table 10 : Example outputs and the corresponding source words of our systems , where ‚Äú ST ‚Äù and ‚Äú MT ‚Äù refer to ‚Äú single - task ‚Äù and ‚Äú multi - task ‚Äù models . 
The tick symbols indicate which outputs match the references . 
the anonymized ofÔ¨Åcial test set is 0.299which is slightly worse than their 0.304 . 
However , we attain a better F - score ( 0.6799 ) than them ( 0.6791 ) as shown in Table 9 . 
Moreover , our model is of size22 M parameters , which is much smaller than their baseline BiDeep of size 133 M parameters,18 and we do not apply as many NMT techniques as they did . 
Nevertheless , on the DICT test set , there is no prominent difference among the single - task baseline , multi - task and BiDeep model , possibly because the noise pattern in the DICT dataset is not complex enough to reÔ¨Çect the learning ability of these models . 
7 Discussion In our experiments , a system has ACCACT > ACC+>ACC because both ACC - ACT and ACC+ consider the cases of ACC but ACC - ACT can capture more acceptable transliterations . 
Despite a consistent ranking given by the three metrics , ACC - ACT reveals different information from ACC and ACC+ . 
For example , in Table 6 , the model of Œª=5 6outperforms Œª=1 2by 0.015and0.016 in ACC and ACC+ , respectively , but the difference is 0.020 in ACC - ACT , on the NEWS dataset . 
This suggests a more prominent gap between these two models . 
In contrast , by looking at the same two rows but on the DICT dataset , ACC - ACT indicates a smaller gap ( 0.004 ) than ACC ( 0.006 ) . 
If we conduct experiments on another dataset , the disagreement among the metrics might be signiÔ¨Åcant enough to render an inconsistent ranking . 
Furthermore , we present some typical examples in which the multi - task model generates better predictions than the single - task in Table 10 . 
In the Ô¨Årst 18We compute the size of our multi - task model by counting the number of trainable parameters extracted from model.parameters ( ) ; For the BiDeep model , we use thenumpy package to load the model in .npz format and calculate the number of parameters via a simple for-loop.example , the single - task model wrongly maps the sub - word ghan toÊ†π(emphasizing on the character g ) while the multi - task model correctly maps han toÊ±â . 
The erroneous grouping of the English characters also occurs in the second example where the single - task model maps ertoÂ∞îinstead of more reasonably lertoÂãí . 
Even in the third example where both outputs are mismatched , the multi - task model predicts the character Êù∞ , which is closer to the source sub - word jethan the single - task model ‚Äôs Ëµ´in terms of pronunciation . 
Overall , it seems that the multi - task model can capture the source - word pronunciation better than the single - task one . 
Still , the multi - task model does not consistently handle all names better than the single - task model ‚Äì especially for exceptional names that do not have a regular transliteration . 
For instance , the name Fyleman is transliterated into Ê≥ï‰ºäÂ∞îÊõº , but the character‰ºädoes not have any source - word correspondence if we consider the pronunciation of the source name . 
Finally , our model can be generalized to other transliteration tasks by replacing Pinyin with other phonetic representations such as IPA for English and r ¬Øomaji for Japanese . 
In addition , ACC - ACT can be extended to alphabetical languages by , for instance , constructing the Alternating Sub - word Table which stores lists of interchangeable subsequences . 
Another possible future work is to redesign the objective function by treating Œªas a trainable parameter or including the correlation information ( Papasarantopoulos et al . 
, 2019 ) . 
8 Related Work Previous work has demonstrated the effectiveness of using MTL on models through joint learning of various NLP tasks such as machine translation , syntactic and dependency parsing ( Luong et al . 
, 2016 ; Dong et al . 
, 2015 ; Li et al . 
, 2014 ) . 
In most of this work , underlies a similar idea to create a uniÔ¨Åed training setting for several tasks by sharing the core parameters . 
Besides , machine transliteration has a long history of using phonetic information , for example , by mapping a phrase to its pronunciation in the source language and then convert the sound to the target word ( Knight and Graehl , 1997 ) . 
There is also relevant work that uses both graphemes and phonemes to various extents for transliteration , such as the correspondence - based ( Oh et al . 
, 2006 ) and G2P - based ( Le and Sadat , 2018 ) approaches . 
Our work is inspired by the intu-385 itive understanding that pronunciation is essential for transliteration , and the success of incorporating phonetic information such as Pinyin ( Jiang et al . 
, 2009 ) and IPA ( Salam et al . 
, 2011 ) , in the model design . 
9 Conclusion We argue in this paper that language - speciÔ¨Åc features should be used when solving transliteration in a neural setting , and we exemplify a way of using phonetic information as the transferred knowledge to improve a neural machine transliteration system . 
Our results demonstrate that the main transliteration task and the auxiliary phonetic task are indeed mutually beneÔ¨Åcial in English - to - Chinese transliteration , and we discuss the possibility of applying this idea on other language pairs . 
Acknowledgements We thank the anonymous reviewers for their insightful feedback . 
We would also like to thank Zheng Zhao , Zhijiang Guo , Waylon Li and Pinzhen Chen for their help and comments . 
Abstract Attention - based encoder - decoder models have achieved great success in neural machine translation tasks . 
However , the lengths of the target sequences are not explicitly predicted in these models . 
This work proposes length prediction as an auxiliary task and set up a sub - network to obtain the length information from the encoder . 
Experimental results show that the length prediction sub - network brings improvements over the strong baseline system and that the predicted length can be used as an alternative to length normalization during decoding . 
1 Introduction In recent years , neural network ( NN ) models have achieved great improvements in machine translation ( MT ) tasks . 
Sutskever et al . 
( 2014 ) introduced the encoder - decoder network , Bahdanau et al . 
( 2015 ) developed the attention - based architecture , and Vaswani et al . 
( 2017 ) proposed the transformer model with self - attentions , which delivers state - of - the - art performances . 
Despite the success achieved in neural machine translation ( NMT ) , current NMT systems do not model the length of the output explicitly , and thus various length normalization approaches are often used in decoding . 
Length normalization is a common technique used in the beam search of NMT systems to enable a fair comparison of partial hypotheses with different lengths . 
Without any form of length normalization , regular beam searches will prefer shorter hypotheses to longer ones on average , as a negative logarithmic probability is added at each step , resulting in lower ( more negative ) scores for longer sentences . 
The simplest way is to normalize the score of the current partial hypothesis ( ei 1 ) by its length ( |i| ): s(ei 1,fJ 1 ) = logp(ei 1|fJ 1 ) |i|(1)wherefJ 1is the source sequence . 
To use a softer approach , the denominator |i|can also be raised to the power of a number between 0 and 1 or replaced by more complex functions , as proposed in Wu et al . 
( 2016 ) . 
Moreover , a constant word reward is used in He et al . 
( 2016 ) as an alternative to length normalization . 
All of these approaches tackle the length problem in decoding , and all NMT systems use at least one of them to ensure the performance . 
In addition to investigating various types of length normalization , their rationality is rarely explored . 
Although length normalization appears to be simple and effective , it is still an additional technique to help a ‚Äú weak ‚Äù machine translation model that can not handle the hypothesis length properly . 
In this work it is proposed to model the target length using the neural network itself in a multi - task learning way . 
The estimated length information can either be implicitly included in the network to ‚Äú guide ‚Äù translation , or it can be used explicitly as an alternative to length normalization during decoding . 
The experimental results on various datasets show that the proposed system achieves improvements compared to the baseline model and the predicted length can easily be used to replace the length normalization . 
2 Related Work Multi - task learning is an important training strategy that aims to improve the generalization performance of the main task with some other related tasks ( Luong et al . 
, 2016 ; Mart ¬¥ ƒ±nez Alonso and Plank , 2017 ) . 
With regard to deep learning , multitask learning is applied successfully in many areas , such as natural language processing ( Liu et al . 
, 2015 ) , computer vision ( Donahue et al . 
, 2014 ) , and speech processing ( Heigold et al . 
, 2013 ) . 
In this work , the prediction of the target length while generating translation hypotheses can be seen as a389 multi - task learning application . 
Murray and Chiang ( 2018 ) and Stahlberg and Byrne ( 2019 ) attribute the fact that beam search prefers shorter candidates due to the local normalization of NMT . 
To address this problem , in addition to the standard length normalization technique , Wu et al . 
( 2016 ) propose a more complicated correction with a hyperparameter that can be adjusted for different language pairs . 
In He et al . 
( 2016 ) , a word reward function is proposed that simulates the coverage vector in statistical machine translation so that the decoder prefers a long translation . 
Huang et al . 
( 2017 ) and Yang et al . 
( 2018 ) suggest variations of this reward that provide better guarantees during search . 
There are also works on target vocabulary prediction in the encoder - decoder model that implicitly predicts the target length ( Weng et al . 
, 2017 ; Suzuki and Nagata , 2017 ) . 
In our work , the target length is explicitly modeled by the neural network itself , which indicates that the entire system relies more on statistics rather than heuristics . 
3 Neural Length Model To predict the target length based on the standard transformer architecture ( Vaswani et al . 
, 2017 ) , we build a multi - layer sub - network that only requires information from the source sequence ( or the encoder ) . 
In this work the length prediction task is considered as a classiÔ¨Åcation task for different lengths . 
Other methods , such as directly generating a real number , binarizing the length , or performing multiple binary classiÔ¨Åcation tasks , are also being tested , but the classiÔ¨Åcation method performs best . 
3.1 Modeling We predict the length of the target sequence by a classiÔ¨Åer in the range of [ 0,200 ] , the input of which is a single vector without time dimension , which is extracted from the encoder . 
To obtain this vector , we Ô¨Årst concatenate the encoder output and the embedding of the source tokens , followed by a linear layer with an activation function to map the vectors to the same dimension as the original encoder output . 
Then we set the length of the concatenated vectors to 200 by clipping or zero padding , in order to have a Ô¨Åxed length of time dimension , which could be compressed to a single vector by convolution and max - pooling . 
Then , the vectors run through a convolutional layer with an activation function and a max - pooling layer . 
A linear layer is encoder output word embeddinglinear layer ( f dim)convolutional layer ( j dim)activation ( ReLU)max pooling ( j dim)linear layer ( j dim ) source lengthlinear layer ( f dim ) length embeddinglinear layer ( f dim)softmaxFigure 1 : The architecture of the length prediction subnetwork . 
then used to project the max - pooled vector into a single vector . 
We also embed the length of source sequence into a 201 dimension vector with a length embedding matrix , which is initialized by the empirical distribution of the length . 
This length embedding is then concatenated with the output logit of the length prediction sub - network . 
Again , this concatenated vector is projected through a linear layer onto a vectorswith 201 dimensions . 
Finally , the length distribution qlis given by a softmax over s. And the predicted length lpredis thelwith the highest probability . 
The complete structure of the proposed length prediction sub - network is illustrated in Figure 1 . 
When we train the model with the translation and length prediction tasks jointly , the gradient of the length model will propagate to the translation model ( referred to as no - connection in this paper ) . 
Thus , these two models will inÔ¨Çuence each other during multi - task training . 
In addition , the translation model could beneÔ¨Åt from concatenating the length prediction output vector sto the outputs of each decoder layer ( referred to as cross - concat in this paper ) . 
After the vector is concatenated , a linear projection is run through to maintain the feature dimension of the vector as the original one , so that it can be used without modifying the rest of the original transformer model . 
Here we detach sfrom the backpropagation graph so that the length prediction is not affected by this connection . 
In this method , we think that with the concatenation , the390 length information could be passed to the decoder and used implicitly . 
3.2 Training During training , Kullback ‚Äì Leibler ( KL ) divergence ( Kullback and Leibler , 1951 ) is used as the loss of the length prediction task : Loss length = DKL(P||Q ) = /summationdisplay lpllogpl ql(2 ) whereqlis the probability from model output . 
Supposeltarget is the actual length of the target sequence , plis the target distribution given by a Gaussian function added with a neighborhood reward d(l , ltarget ) . 
Formally , plis given as : pl = al / summationtext l / primeal / prime(3 ) where al= exp / parenleftBigg ‚àí/parenleftbiggl‚àíltarget œÉ / parenrightbigg2 / parenrightBigg + d(l , ltarget)(4 ) where d(l , ltarget ) = Ô£± Ô£≤ Ô£≥1 ifl = ltarget 0.1 if|l‚àíltarget|= 1 0 others ( 5 ) hereœÉis a constant and is used to control the shape of the distribution . 
In contrast to cross entropy with label smoothing , in which there is only one true label with a high probability and others are treated equally , the probability plbecomes smaller if lis further away from ltarget , which creates the desired relationship between each class in the classiÔ¨Åer . 
We use cross entropy with label smoothing as the training loss for the translation task . 
We linearly combine the translation loss with the length loss , so that the training loss is given by Loss all = Œª1Loss translation + Œª2Loss length ( 6 ) 3.3 Decoding Besides using the length information implicitly ( as the two methods mentioned above ) , we can also guide the decoding step with the length prediction explicitly . 
With the help of the length prediction , we have a mathematically reasonable control of the output length in comparison to the length normalization in beam search . 
Since the predicted target length can not be 100 % accurate and a sourcesentence can have multiple possible translations of different lengths , we control the length of the inference by penalizing the score ( logarithmic probability ) of the end - of - sentence ( EOS ) token during beam search , rather than forcing the length of the inference to match the predicted length . 
More specifically , if the length of the hypothesis is shorter than the predicted length , the EOS token score is penalized ; if the hypothesis is longer than the predicted length , the EOS token score is rewarded to facilitate the selection of the EOS token in beam search to Ô¨Ånalize the hypothesis . 
A logarithmic linear penalty is introduced , which is added to the score of EOS token at each time step during beam search : P = Œ±logLhyp Lpred(7 ) whereLhypis the length of the hypothesis , Lpredis the predicted length of the target sentence , and Œ± is a hyperparameter to control the penalty . 
4 Experiments 4.1 Experimental Setup We Ô¨Årst conduct experiments on a relatively small dataset , IWSLT2014 German ‚ÜíEnglish ( 160k sentence pairs ) ( Cettolo et al . 
, 2014 ) , to tune hyperparameters and analyze the performance . 
Then we train our model on other four different language pairs , which are Spanish - English ( es - en ) , Italian - English ( it - en ) , Dutch - English ( nl - en ) and Romanian - English ( ro - en ) . 
At last , the experiments are carried out on the WMT ( Barrault et al . 
, 2019 ) German‚ÜîEnglish ( 4 M sentence pairs ) datasets in order to compare our system with the baseline model . 
All datasets used in this work are preprocessed by fairseq1(Ott et al . 
, 2019 ) . 
Data statistics can be found in Table 1 . 
data set language pairnumber of sentence pairs train valid test IWSLTde - en 160k 7.3k 6.8k es - en 169k 7.7k 5.6k it - en 167k 7.6k 6.6k nl - en 154k 7.0k 5.4k ro - en 168k 7.6k 5.6k WMT en‚Üîde 4.5 M 3.0k 3.0k Table 1 : Data statistics of IWSLT and WMT datasets . 
We employ the transformer base architecture ( Vaswani et al . 
, 2017 ) as the baseline model and 1https://github.com/pytorch/fairseq391 this work is implemented in fairseq . 
All model hyperparameters of the baseline model for IWSLT match the settings in fairseq . 
For the WMT experiments , the settings are the same as for the original base transformer model . 
The sub - network used for the length prediction only increases the number of free parameters by less than 10 % , the inÔ¨Çuence on the training and decoding speed is also marginal . 
Experimental performance is measured using BLEU ( Papineni et al . 
, 2002 ; Post , 2018 ) and CHARAC TER(Wang et al . 
, 2016 ) ( CTER ) metrics . 
4.2 Experimental Results For the length prediction task , the inference length does not have to correspond exactly to the reference length , since there can be multiple correct translations with different lengths . 
Therefore , we consider the predictions that fulÔ¨Åll |lpredict‚àíltarget|/ltarget‚â§ Tto be accurate , where Tis a threshold . 
Œª1Œª2model acc.[%]BLEU[% ] 1 0baseline - 34.8 0 1length model 83.4 1 1no - connection 86.7 35.3 cross - concat 86.1 35.3 Table 2 : Accuracy rate and B LEU scores of the proposed system with the length model on the IWSLT German‚ÜíEnglish task . 
The accuracy of the length prediction task is reported on the validation dataset . 
language pair es - en it - en nl - en ro - en baseline 41.2 32.6 37.8 38.4 no - connection 41.3 32.8 37.8 38.8 cross - concat 41.3 32.7 38.3 38.7 Table 3 : Performance ( in B LEU[%]scores ) using different methods for different language pairs . 
Table 2 shows the experiments carried out with the standard translation model ( Œª1= 1andŒª2= 0 ) , the pure length model ( Œª1= 0 andŒª2= 1 ) and the combination of the two models ( Œª1= 1 andŒª2= 1 ) . 
For the accuracy , here we choose the thresholdT= 20 % . 
It is observed that the joint training of the two models performs better for both the translation and the length prediction task . 
Due to the multi - task learning , although the translation task does not explicitly inÔ¨Çuence the length prediction , it helps to bring model parameters to a better local optimum . 
We use no - connection , cross - concat model totrain on other language pairs with the same hyperparameters as on IWSLT de - en to test the performance , as shown in Table 3 . 
For cross - concat , theBLEU score of the nl - en system is improved by 0.5 % . 
For other language pairs , the results of two methods are almost the same , all of which are better than the baseline . 
Figure 2 shows the relative differences ‚àÜlbetween the predicted and actual lengths for different target sequence lengths . 
When ltarget is between about 10and40 , the prediction is pretty good : for mostltarget in[10,40],‚àÜlis less than 15 % . 
When ltarget is in[40,100 ] , the prediction becomes worse , but most of them are still less than 20 % . 
After 100 , the prediction is pretty bad . 
There are two reasons for this : Ô¨Årst , the length of most target sequences in training data is between 10 and 40 , so the model does not often see the cases that the sequence is too long ; second , there are very few long sequences in the validation dataset , so the results for these points lack statistical meaning . 
Figure 2 : The left y - axis shows the relative difference between the target and the predicted length and the righty - axis is for the empirical distribution of ltarget . 
Table 4 shows the comparison between the proposed approach and the baseline model . 
Here we setŒ±= 10 according to the experiments that are carried out on the IWSLT dataset . 
The additional sub - network for length prediction improves the BLEU score by up to 0.9%over the strong baseline model . 
Moreover , the predicted length successfully serves as an alternative to length normalization . 
Regardless of whether the inference tends to be longer or shorter than the reference , the ratio when using the predicted length is slightly better than using the length normalization , which shows better control of the length.392 model English‚ÜíGerman German‚ÜíEnglish newstest2014 newstest2017 BLEU[%]‚ÜëCTER[%]‚Üìlen . 
ratio BLEU[%]‚ÜëCTER[%]‚Üìlen . 
ratio baseline 27.3 45.8 1.024 33.0 41.8 0.974 + len . 
model no - connection 27.6 45.5 1.020 33.4 41.5 0.972 cross - concat 27.4 45.7 1.024 33.4 41.3 0.970 - len . 
norm . 
no - connection 27.6 45.6 1.018 33.9 40.9 0.973 cross - concat 27.3 45.8 1.021 33.7 41.2 0.974 Table 4 : Comparison between the proposed system and the baseline model . 
‚Äú + len . 
model ‚Äù indicates that the length prediction sub - network is added to the baseline architecture . 
‚Äú - len . 
normalization ‚Äù denotes that the predicted length is used during decoding as an alternative to the length normalization as described in Section 3.3 . 
‚Äú len . 
ratio ‚Äù gives the length ratio between the hypothesis length and the reference length : the closer to 1 , the better . 
Figure 3 : Length prediction model outperforms the baseline model in length prediction test accuracy . 
Figure 3 shows the relationship between the length prediction accuracy of the baseline model and the length prediction model , and the threshold Tfor calculating accuracy . 
Since the transformer baseline model does not predict the target length , the length prediction of baseline is obtained from the average ratio of source sentence length to target sentence length . 
For the length prediction task , the accuracy of our model is always better than the baseline , which indicates that on WMT data , our model can still predict the target length well . 
5 Conclusion In this paper , we propose a length prediction subnetwork based on the transformer architecture , and a method of using the length prediction information on the decoder side , namely cross - concat . 
In decoding , we use the predicted length to calculate a logarithmic linear penalty in the beam search in order to replace the length normalization . 
Experimental results show that the sub - network canpredict target length well and further improve translation quality . 
In addition , the predicted length can be used to replace the length normalization with a better and more mathematically explainable control of the output length . 
For future work , the use of length prediction in positional encoding ( Lakew et al . 
, 2019 ; Takase and Okazaki , 2019 ) and nonautoregressive ( or partially autoregressive ) NMT ( Gu et al . 
, 2017 ; Lee et al . 
, 2018 ; Stern et al . 
, 2019 ) could be further investigated . 
Acknowledgements This work has received funding from the European Research Council ( ERC ) ( under the European Union ‚Äôs Horizon 2020 research and innovation programme , grant agreement No 694537 , project ‚Äú SEQCLAS ‚Äù ) and the Deutsche Forschungsgemeinschaft ( DFG ; grant agreement NE 572/8 - 1 , project ‚Äú CoreTec ‚Äù ) . 
The GPU computing cluster was supported by DFG ( Deutsche Forschungsgemeinschaft ) under grant INST 222/1168 - 1 FUGG . 
Abstract Recent work in unsupervised parsing has tried to incorporate visual information into learning , but results suggest that these models need linguistic bias to compete against models that only rely on text . 
This work proposes grammar induction models which use visual information from images for labeled parsing , and achieve state - of - the - art results on grounded grammar induction on several languages . 
Results indicate that visual information is especially helpful in languages where high frequency words are more broadly distributed . 
Comparison between models with and without visual information shows that the grounded models are able to use visual information for proposing noun phrases , gathering useful information from images for unknown words , and achieving better performance at prepositional phrase attachment prediction.1 1 Introduction Recent grammar induction models are able to produce accurate grammars and labeled parses with raw text only ( Jin et al . 
, 2018b , 2019 ; Kim et al . 
, 2019b , a ; Drozdov et al . 
, 2019 ) , providing evidence against the poverty of the stimulus argument ( Chomsky , 1965 ) , and showing that many linguistic distinctions like lexical and phrasal categories can be directly induced from raw text statistics . 
However , as computational - level models of human syntax acquisition , they lack semantic , pragmatic and environmental information which human learners seem to use ( Gleitman , 1990 ; Pinker and MacWhinney , 1987 ; Tomasello , 2003 ) . 
This paper proposes novel grounded neuralnetwork - based models of grammar induction which take into account information extracted from images in learning . 
Performance comparisons show 1The system implementation and translated datasets used in this work can be found at https://github.com/ lifengjin / imagepcfg . 
( a ) friend as companion   ( b ) friend as condiment Figure 1 : Examples of disambiguating information provided by images for the prepositional phrase attachment of the sentence Mary eats spaghetti with a friend ( Gokcen et al . 
, 2018 ) . 
that the proposed models achieve state - of - the - art results on multilingual induction datasets , even without help from linguistic knowledge or pretrained image encoders . 
Experiments show several speciÔ¨Åc beneÔ¨Åts attributable to the use of visual information in induction . 
First , as a proxy to semantics , the co - occurrences between objects in images and referring words and expressions , such as the word spaghetti and the plate of spaghetti in Figure 1,2 provide clues to the induction model about the syntactic categories of such linguistic units , which may complement distributional cues from word collocation which normal grammar inducers rely on solely for induction . 
Also , pictures may help disambiguate di Ô¨Äerent syntactic relations : induction models are not able to resolve many prepositional phrase attachment ambiguities with only text ‚Äî for example in Figure 1 , there is little information in the text of Mary eats spaghetti with a friend for the induction models to induce a high attachment structure where a friend is a companion ‚Äî and images may provide information to resolve these ambiguities . 
Finally , images may provide grounding information for unknown words when their syntactic properties are not clearly indicated by sentential context . 
2https://github.com/ajdagokcen/ madlyambiguous - repo396 2 Related work Existing unsupervised PCFG inducers exploit naturally - occurring cognitive and developmental constraints , such as punctuation as a proxy to prosody ( Seginer , 2007 ) , human memory constraints ( Noji and Johnson , 2016 ; Shain et al . 
, 2016 ; Jin et al . 
, 2018b ) , and morphology ( Jin and Schuler , 2019 ) , to regulate the posterior of grammars which are known to be extremely multimodal ( Johnson et al . 
, 2007 ) . 
Models in Shi et al . 
( 2019 ) also match embeddings of word spans to encoded images to induce unlabeled hierarchical structures with a concreteness measure ( Hill et al . 
, 2014 ; Hill and Korhonen , 2014 ) . 
Additionally , visual information is observed to provide grounding for words describing concrete objects , helping to identify and categorize such words . 
This hypothesis is termed ‚Äò noun bias ‚Äô in language acquisition ( Gentner , 1982 , 2006 ; Waxman et al . 
, 2013 ) , through which the early acquisition of nouns is attributed to nouns referring to observable objects . 
However , the models in Shi et al . 
( 2019 ) also rely on language - speciÔ¨Åc branching bias to outperform other text - based models , and images are encoded by pretrained object classiÔ¨Åers trained with large datasets , with no ablation to show the beneÔ¨Åt of visual information for unsupervised parsing . 
Visual information has also been used for joint training of prepositional phrase attachment models ( Christie et al . 
, 2016 ) suggesting that visual information may contain semantic information to help disambiguate prepositional phrase attachment . 
3 Grounded Grammar Induction Model The full grounded grammar induction model used in these experiments , ImagePCFG , consists of two parts : a word - based PCFG induction model and a vision model , as shown in Figure 2 . 
The two parts have their own objective functions . 
The PCFG induction model , called NoImagePCFG when trained by itself , can be trained by maximizing the marginal probability P(œÉ ) of sentences œÉ . 
This part functions similarly to previously proposed PCFG induction models ( Jin et al . 
, 2018a ; Kim et al . 
, 2019a ) where a PCFG is induced through maximization of the data likelihood of the training corpus marginalized over latent syntactic trees . 
The image encoder - decoder network in the vision model is trained to reconstruct the original image after passing through an information bottleneck . 
The latent encoding from the image encoder may be seen as a compressed representation of vi - sual information in the image , some of which is semantic , relating to objects in the image . 
We hypothesize that semantic information can be helpful in syntax induction , potentially through helping three tasks mentioned above . 
In contrast to the full model where the encoded visual representations are trained from scratch , the ImagePrePCFG model uses image embeddings encoded by pretrained image classiÔ¨Åers with parameters Ô¨Åxed during induction training . 
We hypothesize that pretrained image classiÔ¨Åers may provide useful information about objects in an image , but for grammar induction it is better to allow the inducer to decide which kind of information may help induction . 
The two parts are connected through a syntacticvisual loss function connecting a syntactic sentence embedding projected from word embeddings and an image embedding . 
We hypothesize that visual information in the encoded images may help constrain the search space of syntactic embeddings of words with supporting evidence of lexical attributes such as concreteness for nouns or correlating adjectives with properties of objects.3 3.1 Induction model The PCFG induction model is factored into three submodels : a nonterminal expansion model , a terminal expansion model and a split model , which distinguishes terminal and nonterminal expansions . 
The binary - branching non - terminal expansion rule probabilities,4and unary - branching terminal expansion rule probabilities in a factored Chomskynormal - form PCFG can be parameterized with these three submodels . 
Given a tree as a set œÑ of nodesŒ∑undergoing non - terminal expansions cŒ∑‚ÜícŒ∑1cŒ∑2(whereŒ∑‚àà{1,2}‚àóis a Gorn address specifying a path of left or right branches from the root ) , and a set œÑ / primeof nodesŒ∑undergoing terminal expansions cŒ∑‚ÜíwŒ∑(where wŒ∑is the word at nodeŒ∑ ) in a parse of sentence œÉ , the marginal 3The syntactic nature of word embeddings indicates that any lexical - speciÔ¨Åc semantic information in these embeddings may be abstract , which is generally not su Ô¨Écient for visual reconstruction . 
Experiments with syntactic embeddings show that it is di Ô¨Écult to extract semantic information from them and present visually . 
4These include the expansion rules generating the top node in the tree.397 a giraffe is eating leaves    Syntactic - visual projectorImage encoderImage decoderGrammarInductionModelembeddingsInside / ViterbiSNPVP ‚Ä¶ P(  ) , < latexit sha1_base64="/t+RkoYiTPP2qCO / l471Vfuq114=">AAACHnicbZBLSwMxFIUz9VXrq9alm2ARKkiZKQV1V3DjsoJ9QGcomTTThuYxJBm1DPNXxJ3+EnfiVn+Ie9PHQlsPBA7n3EsuXxgzqo3rfjm5tfWNza38dmFnd2//oHhYamuZKExaWDKpuiHShFFBWoYaRrqxIoiHjHTC8fW079wTpakUd2YSk4CjoaARxcjYqF8sQSs / VjJMfU2HHGXn / WLZrbozwVXjLUwZLNTsF7/9gcQJJ8JghrTueW5sghQpQzEjWcFPNIkRHqMh6VkrECc6SGe3Z / DUJgMYSWWfMHCW / t5IEdd6wkM7yZEZ6eVuGv7X9RITXQYpFXFiiMDzj6KEQSPhFAQcUEWwYRNrEFbU3grxCCmEjcVV8AV5wJJzJAbpDE / W84LUMopgM6uUvbOsYDl5y1RWTbtW9erVq9t6uVFbEMuDY3ACKsADF6ABbkATtAAGj+AJvIBX59l5c96dj / lozlnsHIE / cj5 / ANoooQY=</latexit > e  < latexit sha1_base64="D1DjNBk1Y1GUg9GLi2rV5g4KEO4=">AAACHXicbVDJSgNBFOyJW4xb1KOXxiDoJcwEQb0JXjxGMImQGUNP503SpJehu0cJw3yKeNMv8SZexQ / xbmc5uBU8KKre4xUVp5wZ6 / sfXmlhcWl5pbxaWVvf2Nyqbu+0jco0hRZVXOmbmBjgTELLMsvhJtVARMyhE48uJn7nDrRhSl7bcQqRIAPJEkaJdVKvuh0KYodxkkNxGxo2EKRXrfl1fwr8lwRzUkNzNHvVz7CvaCZAWsqJMd3AT22UE20Z5VBUwsxASuiIDKDrqCQCTJRPoxf4wCl9nCjtRlo8Vb9f5EQYMxax25wENb+9ifif181schrlTKaZBUlnj5KMY6vwpAfcZxqo5WNHCNXMZcV0SDSh1rVVCSXcUyUEkf08TLWKi24Q5XloEtwsDmvBUVFxPQW / W / lL2o16cFw / uzqunTfmjZXRHtpHhyhAJ+gcXaImaiGK7tEDekLP3qP34r16b7PVkje/2UU/4L1 / ASCSodI=</latexit > em < latexit sha1_base64="qRMW3nBDU4fMlvosuWaXb4Q9BLs=">AAACFnicbVC7SgNBFJ2NrxhfUUubwSDEJuyGgNoFbCwjGBWyq8xO7sbBeSwzs0pY9jfETr / ETmxt / RB7Z5MUvg5cOJxzL / dw4pQzY33 / w6vMzS8sLlWXayura+sb9c2tc6MyTaFPFVf6MiYGOJPQt8xyuEw1EBFzuIhvj0v/4g60YUqe2XEKkSAjyRJGiXVSGApib+Ikh+JKXNcbfsufAP8lwYw00Ay96 / pnOFQ0EyAt5cSYQeCnNsqJtoxyKGphZiAl9JaMYOCoJAJMlE8yF3jPKUOcKO1GWjxRv1 / kRBgzFrHbLDOa314p / ucNMpscRjmTaWZB0umjJOPYKlwWgIdMA7V87AihmrmsmN4QTah1NdVCCfdUCUHkMA9TreJiEER5HpoE94pmI9gvaq6n4Hcrf8l5uxV0WkennUa3PWusinbQLmqiAB2gLjpBPdRHFKXoAT2hZ+/Re / FevbfpasWb3WyjH / DevwB / t59v</latexit > L( ,m ) < latexit sha1_base64="XhjR5wFMxemIrYcvEDz / Tq5oUls=">AAACFXicbVDLSsNAFJ3Ud31VXboZLEILUpJSUHcFNy5cVLBaSEKZTCft0HmEmYlSQj5D3OmXuBO3rv0Q905rFtp64MLhnHu5954oYVQb1/10SkvLK6tr6xvlza3tnd3K3v6tlqnCpIslk6oXIU0YFaRrqGGklyiCeMTIXTS+mPp390RpKsWNmSQk5GgoaEwxMlbyr2qBpkOOTni9X6m6DXcGuEi8glRBgU6/8hUMJE45EQYzpLXvuYkJM6QMxYzk5SDVJEF4jIbEt1QgTnSYzU7O4bFVBjCWypYwcKb+nsgQ13rCI9vJkRnpeW8q / uf5qYnPwoyKJDVE4J9FccqgkXD6PxxQRbBhE0sQVtTeCvEIKYSNTakcCPKAJedIDLIgUTLKfS / MskDHsJPXql49L9ucvPlUFslts+G1GufXrWq7WSS2Dg7BEagBD5yCNrgEHdAFGEjwCJ7Bi / PkvDpvzvtPa8kpZg7AHzgf3yH / nhI=</latexit > L(em , e  ) < latexit sha1_base64="Z6eDJl1Ek+4i / v+xmfjhKsWlDjU=">AAACLXicbVC7SgNBFJ31GeMramkzGoQIEnaDoHaCjYVFBKNCdg2zk7txcB7LzKwSlq39GrHTL7EQxNYvsHfyKHwduHDmnHuZe0+ccmas7796E5NT0zOzpbny / MLi0nJlZfXcqExTaFHFlb6MiQHOJLQssxwuUw1ExBwu4pujgX9xC9owJc9sP4VIkJ5kCaPEOqlT2TiphYLY6zjJobgSO98eoWE9QbY7lapf94fAf0kwJlU0RrNT+Qy7imYCpKWcGNMO / NRGOdGWUQ5FOcwMpITekB60HZVEgIny4SkF3nJKFydKu5IWD9XvEzkRxvRF7DoHm5rf3kD8z2tnNtmPcibTzIKko4+SjGOr8CAX3GUaqOV9RwjVzO2K6TXRhFqXXjmUcEeVEER28zDVKi7aQZTnoUlws6hVg+2i7HIKfqfyl5w36sFu / eB0t3rYGCdWQutoE9VQgPbQITpGTdRCFN2jB / SEnr1H78V7895HrRPeeGYN / YD38QWGq6gx</latexit > NoImagePCFGImagePrePCFGImagePCFGFigure 2 : Di Ô¨Äerent conÔ¨Ågurations of PCFG induction models : the model without vision ( NoImagePCFG ) , the model with a pretrained image encoder ( ImagePrePCFG ) and the model with images ( ImagePCFG . 
) probability of œÉcan be computed as : P(œÉ)=/summationdisplay œÑ , œÑ / prime / productdisplay Œ∑‚ààœÑP(cŒ∑‚ÜícŒ∑1cŒ∑2)¬∑/productdisplay Œ∑‚ààœÑ / primeP(cŒ∑‚ÜíwŒ∑ ) ( 1 ) We Ô¨Årst deÔ¨Åne a set of Bernoulli distributions that distribute probability mass between terminal and nonterminal rules , so that the lexical expansion model can be tied to the image model ( see Section 4.2 ): P(Term|cŒ∑)=softmax { 0,1}(ReLU ( WsplxB , cŒ∑+bspl ) ) , ( 2 ) where cŒ∑is a non - terminal category , Wspl‚ààR2√óh andbspl‚ààR2are model parameters for hidden vectors of size h , and xB , cŒ∑‚ààRhthe result of a multilayered residual network ( Kim et al . 
, 2019a ) . 
The residual network consists of Barchitecturally identical residual blocks . 
For an input vector xb‚àí1,c each residual block bperforms the following computation : xb , c = ReLU ( WbReLU ( W / prime bxb‚àí1,c+b / prime b ) + bb)+xb‚àí1,c , ( 3 ) with base case : x0,c = ReLU ( W0EŒ¥c+b0 ) ( 4 ) whereŒ¥cis a Kronecker delta function ‚Äì a vector with value one at index cand zeros everywhere else ‚Äì and E‚ààRd√óCis an embedding matrix for eachnonterminal category cwith embedding size d , and W0‚ààRh√ód , Wb , W / prime b‚ààRh√óhandb0,bb , b / prime b‚ààRh are model parameters with latent representations of size h. Bis set to 2 in all models following Kim et al . 
( 2019a ) . 
Binary - branching non - terminal expansion rule probabilities for each non - terminal category cŒ∑and left and right children cŒ∑1cŒ∑2are deÔ¨Åned as : P(cŒ∑‚ÜícŒ∑1cŒ∑2)=P(Term = 0|cŒ∑ ) ¬∑ softmax cŒ∑1,cŒ∑2(WnontEŒ¥cŒ∑+bnont),(5 ) where Wnont‚ààRC2√ódandbnont‚ààRC2are parameters of the model . 
The lexical unary - expansion rule probabilities for a preterminal category cŒ∑and a word wŒ∑at nodeŒ∑are deÔ¨Åned as : P(cŒ∑‚ÜíwŒ∑)=P(Term = 1|cŒ∑)¬∑exp(ncŒ∑ , wŒ∑ ) /summationtext wexp(ncŒ∑ , w ) ( 6 ) nc , w = ReLU ( w / latticetop lexnB , c , w+blex ) ( 7 ) where wis the generated word type , and wlex‚ààRh andblex‚ààRare model parameters . 
Similarly , nb , c , w = ReLU ( W / prime / prime bReLU ( W / prime / prime / prime bnb‚àí1,c , w+b / prime / prime / prime b ) + b / prime / prime b)+nb‚àí1,c , w , ( 8) with base case : n0,c , w = ReLU ( W / prime 0 / bracketleftBiggEŒ¥c LŒ¥w / bracketrightBigg ) + b / prime 0 ) ( 9)398 where W / prime 0‚ààRh√ó2d , W / prime / prime b , W / prime / prime / prime b‚ààRh√óhand b0,b / prime / prime b , b / prime / prime / prime b‚ààRhare model parameters for latent representations of size h. Lis a matrix of syntactic word embeddings for all words in vocabulary . 
4 Vision model The vision model consists of an image encoderdecoder network and a syntactic - visual projector . 
The image encoder - decoder network encodes an image into an image embedding and then decodes that back into the original image . 
This reconstruction constrains the information in the image embedding to be closely representative of the original image . 
The syntactic - visual projector projects word embeddings used in the calculation of lexical expansion probabilities into the space of image embeddings , building a connection between the space of syntactic information and the space of visual information . 
4.1 The image encoder - decoder network The image encoder employs a ResNet18 architecture ( He et al . 
, 2016 ) which encodes an image with 3 channels into a single vector . 
The encoder consists of four blocks of residual convolutional networks . 
The image decoder decodes an image from a visual vector generated by the image encoder . 
The image decoder used in the joint model is the image generator from DCGAN ( Radford et al . 
, 2016 ) , where a series of transposed convolutions and batch normalizations attempts to recover an image from an image embedding.5 4.2 The syntactic - visual projector The projector model is a CNN - based neural network which takes a concatenated sentence embedding matrix MœÉ‚ààR|œÉ|√ódas input , where embeddings in MœÉare taken from L , and returns the syntactic - visual embedding eœÉ . 
The jth full lengthwise convolutional kernel is deÔ¨Åned as a matrix Kj‚ààRuj√ókjdwhich slides across the sentence matrixMto produce a feature map , where ujis the number of channels in the kernel , kjis the width of the kernel , and dis the height of the kernel which is equal to the size of the syntactic word embeddings . 
Because the kernel is as high as the embeddings , it produces one vector of length ujfor each window . 
The full feature map Fj‚ààRuj√óHj , where Hjis total 5Details of these models can be found in the cited work and the appendix.number of valid submatrices for the kernel , is : Fj=/summationdisplay h(Kjvec(MœÉ[h .. kj+h‚àí1,‚àó])+bj)Œ¥ / latticetop h.(10 ) Finally , an average pooling layer and a linear transform are applied to feature maps from di Ô¨Äerent kernels : ÀÜf=[mean ( F1) ... mean ( Fj)]/latticetop , ( 11 ) eœÉ = tanh(WpoolReLU ( ÀÜf)+bpool ) . 
( 12 ) AllKs , bs and Ws here are parameters of the projector . 
5 Optimization There are three di Ô¨Äerent kinds of objectives used in the optimization of the full grounded induction model . 
The Ô¨Årst loss is the marginal likelihood loss for the PCFG induction model described in Equation 1 , which can be calculated with the Inside algorithm . 
The second loss is the syntactic - visual loss . 
Given the encoded image embedding emand the projected syntactic - visual embedding eœÉof a sentenceœÉ , the syntactic - visual loss is the mean squared error of these two embeddings : L(em , eœÉ)=(em‚àíeœÉ)/latticetop(em‚àíeœÉ ) . 
( 13 ) The third loss is the reconstruction loss of the image . 
Given the original image represented as a vector imand the reconstructed image ÀÜim , the reconstruction objective is the mean squared error of the corresponding pixel values of the two images : L(m)=(im‚àíÀÜim)/latticetop(im‚àíÀÜim ) . 
( 14 ) Models with di Ô¨Äerent sets of input optimize the three losses di Ô¨Äerently for clean ablation . 
NoImagePCFG , which learns from text only , optimizes the negative marginal likelihood loss ( the negative of Equation 1 ) using gradient descent . 
The model with pretrained image encoders , ImagePrePCFG , optimizes the negative marginal likelihood and the syntactic - visual loss ( Equation 13 ) simultaneously . 
The full grounded grammar induction model ImagePCFG learns from text and images jointly by minimizing all three objectives : negative marginal likelihood , syntactic - visual loss and image reconstruction loss ( Equation 14 ): L(œÉ , m)=‚àíP(œÉ)+L(em , eœÉ)+L(m ) . 
( 15)399 6 Experiment methods Experiments described in this paper use the MSCOCO caption data set ( Lin et al . 
, 2015 ) and the Multi30k dataset ( Elliott et al . 
, 2016 ) , which contains pairs of images and descriptions of images written by human annotators . 
Captions in the MSCOCO data set are in English , whereas captions in the Multi30k dataset are in English , German and French . 
Captions are automatically parsed ( Kitaev and Klein , 2018 ) to generate a version of the reference set with constituency trees.6In addition to these datasets with captions generated by human annotators , we automatically translate the English captions into Chinese , Polish and Korean using Google Translate,7and parse the resulting translations into constituency trees , which are then used in experiments to probe the interactions between visual information and grammar induction . 
Results from models proposed in this paper ‚Äî NoImagePCFG , ImagePrePCFG and ImagePCFG ‚Äî are compared with published results from Shi et al . 
( 2019 ) , which include PRPN ( Shen et al . 
, 2018 ) , ON - LSTM ( Shen et al . 
, 2019 ) as well as the grounded VG - NSL models which uses either head Ô¨Ånal bias ( VG - NSL + H ) or head Ô¨Ånal bias and Fasttext embeddings ( VG - NSL + H+F ) as inductive biases from external sources . 
All of these models only induce unlabeled structures and have been evaluated with unlabeled F1 scores . 
We additionally report the labeled evaluation score Recall - Homogeneity ( Rosenberg and Hirschberg , 2007 ; Jin and Schuler , 2020 ) for better comparison between the proposed models . 
All evaluation is done on Viterbi parse trees of the test set from 5 di Ô¨Äerent runs . 
Details about hyper - parameters and results on development data sets can be found in the appendix . 
However , importantly , the tuned hyperparameters for the grammar induction model are the same across the three proposed models , which facilitates direct comparisons among these models to determine the eÔ¨Äect of visual information on induction . 
6.1 Standard set : no replication of e Ô¨Äect for visual information Both unlabeled and labeled evaluation results are shown in Table 1 with left- and right - branching baselines . 
First , trees induced by the PCFG induction models are more accurate than trees induced 6The multilingual parsing accuracy for all languages used in this work has been validated in Fried et al . 
( 2019 ) and veriÔ¨Åed in Shi et al . 
( 2019 ) . 
7https://translate.google.com/ .with all other models , showing that the family of PCFG induction models is better at capturing syntactic regularities and provides a much stronger baseline for grammar induction . 
Second , using the NoImagePCFG model as a baseline , results from both the ImagePCFG model , where raw images are used as input , and the ImagePrePCFG model , where images encoded by pretrained image classiÔ¨Åers are used as input , do not show strong indication of beneÔ¨Åts of visual information in induction . 
The baseline NoImagePCFG outperforms other models by signiÔ¨Åcant margins on all languages in unlabeled evaluation . 
Compared to seemingly large gains between text - based models like PRPN and ON - LSTM8and the grounded models like VGNSL + Hon French and German observed by Shi et al . 
( 2019 ) , the only positive gain between NoImagePCFG and ImagePCFG shown in Table 1 is the labeled evaluation on French where ImagePCFG outperforms NoImagePCFG by a small margin . 
Because the only di Ô¨Äerence between NoImagePCFG and ImagePCFG models is whether the visual information inÔ¨Çuences the syntactic word embeddings , the results indicate that on these languages , visual information does not seem to help induction . 
The gain seen in previous results may therefore be from external inductive biases . 
Finally , the ImagePrePCFG model performs at slightly lower accuracies than the ImagePCFG model consistently across di Ô¨Äerent languages , datasets and evaluation metrics , showing that the information needed in grammar induction from images is not the same as information needed for image classiÔ¨Åcation , and such information can be extracted from images without annotated image classiÔ¨Åcation data . 
6.2 Languages with wider distribution of high - frequency word types : positive eÔ¨Äect One potential advantage of using visual information in induction is to ground nouns and noun phrases . 
For example , if images like in Figure 1 are consistently presented to models with sentences describing spaghetti , the models may learn the categorize words and phrases which could be linked with objects in images as nominal units and then bootstrap other lexical categories . 
However , in the test languages above , a narrow set of very high fre8PCFG induction models where a grammar is induced generally perform better in parsing evaluation than sequence models where only syntactic structures are induced ( Kim et al . 
, 2019a ; Jin et al . 
, 2019).400 ModelsMSCOCO Multi30k English * * English * * German * * French * * F1 RH F1 RH F1 RH F1 RH Left - branching 23.3 - 22.6 - 34.7 - 19.0 Right - branching 21.4 - 11.3 - 12.1 - 11.0 PRPN 52.5 ¬±2.6- 30.8 ¬±17.9- 31.5 ¬±8.9- 27.5 ¬±7.0ON - LSTM 45.5 ¬±3.3- 38.7 ¬±12.7- 34.9 ¬±12.3- 27.7 ¬±5.6VG - NSL + H53.3¬±0.2- 38.7 ¬±0.2- 38.3 ¬±0.2- 38.1 ¬±0.6VG - NSL + H+F54.4¬±0.4- - - - - - NoImagePCFG 60.0¬±8.247.6¬±10.059.4¬±7.751.6¬±8.548.1¬±5.253.7¬±5.244.3¬±5.143.8¬±5.2 ImagePrePCFG 55.6 ¬±7.542.3¬±7.347.0¬±7.040.5¬±7.246.2¬±7.451.1¬±8.042.6¬±10.343.4¬±10.8 ImagePCFG 55.1 ¬±2.742.5¬±1.548.2¬±4.940.5¬±5.047.0¬±5.551.8¬±8.443.6¬±5.544.5¬±6.3 Table 1 : Averages and standard deviations of labeled Recall - Homogeneity and unlabeled F1 scores of various unsupervised grammar inducers on the MSCOCO and Multi30k caption datasets . 
VG - NSL + H : VG - NSL system with head Ô¨Ånal bias . 
VG - NSL + H+F : VG - NSL system with head Ô¨Ånal bias and Fasttext word embeddings . 
( * * : the unlabeled performance di Ô¨Äerence between NoImagePCFG and ImagePCFG is signiÔ¨Åcant p<0.01 . 
) quency words such as determiners provide strong identifying information for nouns and noun phrases , which may greatly diminish the advantage contributed by visual information . 
In such cases , visual information may even be harmful , as models may attend to other information in images which is irrelevant to induction . 
Korean , Polish and Chinese are chosen as representatives of languages with no deÔ¨Ånite articles , and in which statistical information provided by high frequency words is less reliable because there are more such word types . 
Table 2 shows the performance scores of the three proposed systems on these languages . 
Comparing to results in Table 1 , the models with visual information in the input signiÔ¨Åcantly outperform the baseline model , NoImagePCFG , on a majority of the additional test datasets . 
Figure 3 shows the correlation between the RH di Ô¨Äerence between the ImagePCFG model and the NoImagePCFG model on each language in an image dataset , and the distribution of high frequency words in that language , deÔ¨Åned as the number of word types needed to account for 10 % of the number of word tokens in the Universal Dependency ( Nivre et al . 
, 2016 ) corpus of a language.9The Ô¨Ågure shows that the largest gain brought by visual information in induction is on Korean , where the number of high frequency word types is also highest . 
Results on Chinese and Polish 9Korean has 41 , Chinese and Polish have 5 , German has 4 , English has 3 and French has 2 . 
101 log # High Freq Words6 4 2 024MSCOCO RH Diff English PolishKorean Chinese 101 log # High Freq Words10 5 0510Multi30k RH Diff EnglishPolishKorean ChineseFrench GermanFigure 3 : The correlation between number of word types needed to account for 10 % of word tokens in a language ( log # High Freq Words ) and the RH gain from NoImagePCFG to ImagePCFG on di Ô¨Äerent languages on the two di Ô¨Äerent image datasets . 
also show a beneÔ¨Åt for visual information , although the gain is much smaller and less consistent . 
It also shows that when there is a trend of positive correlation between the number of high frequency words and the gain brought by visual information , factors other than high frequency words are at play as well in determining the Ô¨Ånal induction outcome for each dataset in each language in the visually grounded setup , which are left for investigation in future work . 
7 Analysis of advantages of visual information We hypothesize three speciÔ¨Åc ways that visual information may help grammar induction . 
First , a strong correlation between words and objects in images can help identiÔ¨Åcation and categorization401 Models on MSCOCOKorean * * Polish * * Chinese * * F1 RH F1 RH F1 RH NoImagePCFG 38.1 ¬±8.522.3¬±6.858.9¬±3.747.1¬±3.861.2¬±3.548.5¬±3.7 ImagePrePCFG 39.0 ¬±4.123.5¬±3.260.5¬±1.849.8¬±3.360.0¬±4.647.2¬±4.5 ImagePCFG 45.0¬±2.227.1¬±2.653.6¬±8.341.3¬±7.864.9¬±6.651.2¬±8.6 Models on Multi30kKorean * * Polish Chinese * * F1 RH F1 RH F1 RH NoImagePCFG 30.7 ¬±5.622.8¬±3.149.6¬±4.639.9¬±5.159.1¬±3.353.2¬±4.7 ImagePrePCFG 27.1 ¬±4.419.9¬±3.448.4¬±3.138.3¬±2.957.9¬±7.051.0¬±7.7 ImagePCFG 44.9¬±1.333.8¬±2.149.7¬±7.240.4¬±6.158.5¬±3.252.8¬±4.6 Table 2 : Averages and standard deviations of labeled Recall - Homogeneity and unlabeled F1 scores of various unsupervised grammar inducers on the MSCOCO and Multi30k caption datasets in the additional languages with high numbers of high - frequency word types . 
( * * : the unlabeled performance di Ô¨Äerence between NoImagePCFG and ImagePCFG is signiÔ¨Åcant p<0.01 . 
) of nouns and noun phrases , especially on languages where nouns and noun phrases are not readily identiÔ¨Åable by neighboring high frequency words . 
Second , visual information may provide bottom - up information for unknown word embeddings . 
Languages where neighboring words can reliably predict the grammatical category of an unknown word may build robust representations of unknown word embeddings , but the construction of the UNK embedding may also beneÔ¨Åt from bottom - up information from images , especially when sentential context is not enough to build informative UNK embeddings . 
Finally , semantic information inside images may be helpful in solving syntactic ambiguities like prepositional phrase attachment in languages like English . 
Results from experiments described below with the ImagePCFG and NoImagePCFG models show evidence of all three ways . 
7.1 Grounding of nouns and noun phrases The ‚Äò Noun bias ‚Äô hypothesis ( Gentner , 1982 ) postulates that visual information in the induction process may impact how words are categorized grammatically , and nouns may receive an advantage because they correspond to objects in images . 
However , objects in images are often described with phrases , not single words . 
For example , captions likea red car is parked on the street , are common in both caption datasets , where the objects in the image may associate more strongly with modiÔ¨Åer words like redthan the head noun car . 
Evaluations are carried out on the parsed sentences of all languages from two caption datasetsusing a part - of - speech homogeneity metric ( Rosenberg and Hirschberg , 2007 ) for measuring the partof - speech accuracy , and an unlabeled NP recall score for measuring how many noun phrases in gold annotation are also found in the induced trees . 
Results in Figure 4 Ô¨Årst show that the POS homogeneity scores from di Ô¨Äerent models on the same induction dataset are extremely close to each other . 
Given that nouns are one of the categories with the most numerous tokens , the almost identical performance of POS homogeneity across di Ô¨Äerent models indicates that the unsupervised clustering accuracy for nouns across di Ô¨Äerent models is also very close , in contrast to substantial RH score differences on English and Korean . 
However , NP recall scores show a pattern of performance ranking that resembles the ranking observed in Tables 1 and 2 . 
For all datasets except for the Polish Multi30k dataset , when the RH score of ImagePCFG is higher than NoImagePCFG , the NP recall score for the ImagePCFG model is also higher . 
SigniÔ¨Åcance testing with permutation sampling shows that all performance di Ô¨Äerences are signiÔ¨Åcant ( p<0.01).10High accuracy on noun phrases is crucial to high accuracy of other constituents such as prepositional phrases and verb phrases , which usually contain noun phrases , and eventually leads to high overall accuracy . 
This result suggests that the beneÔ¨Åt contributed by visual information works at phrasal levels , most likely 10SigniÔ¨Åcance testing is not done on POS homogeneity due to the possibility that the same induced POS label may mean diÔ¨Äerent things in di Ô¨Äerent induced grammars.402 English KoreanPolishChinese German English French KoreanPolishChinese0.00.20.40.60.81.0POS Homogeneity MSCOCO Multi30kNoImagePCFG ImagePCFG English KoreanPolishChinese German English French KoreanPolishChinese0.00.20.40.60.81.0NP Recall MSCOCO Multi30k * * * * * * * * * * * * * * * * * * * * NoImagePCFG ImagePCFGFigure 4 : The POS Homogeneity and NP Recall scores for the ImagePCFG and NoImagePCFG models across the test languages ( * * : p<0.01 ) . 
Total High LowTotal High Low0.00.20.40.60.81.0Accuracy MSCOCO Multi30k******NoImagePCFG ImagePCFG Figure 5 : The average overall accuracy as well as accuracies for high and low attachment sentences in PP attachment evaluation for models with and without visual information ( * * : p<0.01 , * : p<0.05 ) . 
grounding phrases , not words , with objects in images . 
7.2 Informativeness of the UNK embedding The informativeness of unknown word embeddings is tested among the induction models across different languages . 
An UNK test set is created by randomly replacing one word in one sentence with an UNK symbol if the sentence has no unknown words present . 
Table 3 shows the labeled evaluation results on the multilingual datasets.11First , performance on the UNK test sets on all languages is lower than on the normal test sets , showing that replacing random words with UNK symbols does impact performance . 
The performance ranking of the models on a majority of the languages is consistent with the ranking on the normal test set . 
The ranking of the models on one dataset , the Chinese Multi30k , is reversed on the UNK test set , where the ImagePCFG models show signiÔ¨Åcantly higher performance than the NoImagePCFG models ( Chinese : p<0.01 , permutation test on unlabeled F1 ) . 
This result indicates that the ImagePCFG model in which visual information is supplied during train11The unlabeled evaluation results can be found in the appendix.ing may have built more informative embeddings for the unknown word symbols , helping the model to outperform the model without visual information on a majority of datasets where UNK symbols are frequent . 
7.3 Prepositional phrase attachment Finally , visual information may provide semantic information to resolve structural ambiguities . 
Word quintuples such as ( a ) hotel caught Ô¨Åre during ( a ) storm were extracted from English Wikipedia and the attachment locations were automatically labeled either as ‚Äò n ‚Äô for low attachment , where the prepositional phrase adjoins the direct object , or ‚Äò v ‚Äô for high attachment , where the prepositional phrase adjoins the main verb ( Nakashole and Mitchell , 2015 ) . 
168 test items are selected by human annotators for evaluation , within which 119 are sentences with high attached PPs and 49 are with low attached PPs . 
For evaluation of PP attachment with induced trees , one test item is labeled correct when the induced tree puts the main verb and the direct object into one constituent and it is labeled as ‚Äò v ‚Äô . 
For example , if the induced tree has caught Ô¨Åre as a constituent , it counts as correct for the above example with high attachment . 
Low attachment trees must have a constituent with the direct object and the prepositional phrase . 
For example , for the sentence ( a ) guide gives talks about animals , the induced tree must have talks about animals . 
Average accuracies for all sentences as well as for sentences with high attachment or low attachment with induced grammars are shown in Figure 5 . 
Results show that the models trained with visual information on both datasets show signiÔ¨Åcantly higher performance on the PP attachment task in most of the categories , except for the low attachment category with Multi30k models where the performance from both models is not signiÔ¨Åcantly di Ô¨Äerent . 
This is in contrast to the403 ModelsMSCOCO Multi30k En Ko Pl Zh De En Fr Ko Pl Zh NoImagePCFG 46.2 21.7 45.8 46.0 52.8 49.9 42.2 22.8 38.9 51.6 ImagePCFG 41.2 26.4 40.2 48.1 51.3 39.9 42.6 33.2 39.7 53.2 Table 3 : Average labeled Recall - Homogeneity of the NoImagePCFG and ImagePCFG models on the MSCOCO and Multi30k caption datasets with random words replaced by the UNK symbol . 
Standard deviations across the datasets are similar to what is reported in Table 1 and 2 . 
Chinese Multi30k is the one on which the NoImagePCFG model outperforms the ImagePCFG model on the normal test set but not on the UNK test set . 
higher performance of the NoImagePCFG models on unlabeled F1 and labeled RH than that of the ImagePCFG models on English from both caption datasets . 
Results indicate that induction models use visual information for weighting competing latent syntactic trees for a sentence , which is consistent with the third hypothesized advantage of visual information for induction . 
This also indicates that the reason that the overall parsing performance of ImagePCFG on English is lower than NoImagePCFG lies within other syntactic structures , which is left for future work . 
8 Conclusion This work proposed several novel neural networkbased models of grammar induction which take into account visual information in induction . 
These models achieve state - of - the - art results on multilingual induction datasets without any help from linguistic knowledge or pretrained image encoders . 
Further analyses isolated three hypothesized beneÔ¨Åts of visual information : it helps categorize noun phrases , represent unknown words and resolve syntactic ambiguities . 
Acknowledgments The authors would like to thank the anonymous reviewers for their helpful comments . 
Computations for this project were partly run on the Ohio Supercomputer Center ( 1987 ) . 
This work was supported by the Presidential Fellowship from the Ohio State University . 
The content of the information does not necessarily reÔ¨Çect the position or the policy of the Government , and no o Ô¨Écial endorsement should be inferred . 
This work was also supported by the National Science Foundation grant # 1816891 . 
All views expressed are those of the authors and do not necessarily reÔ¨Çect the views of the National Science Foundation . 
References Noam Chomsky . 
1965 . 
Aspects of the Theory of Syntax . 
MIT Press , Cambridge , MA . 
Gordon Christie , Ankit Laddha , Aishwarya Agrawal , Stanislaw Antol , Yash Goyal , Kevin Kochersberger , and Dhruv Batra . 
2016 . 
Resolving language and vision ambiguities together : Joint segmentation & Prepositional attachment resolution in captioned scenes . 
In EMNLP 2016 - Conference on Empirical Methods in Natural Language Processing , Proceedings , pages 1493‚Äì1503 . 
Jia Deng , Wei Dong , Richard Socher , Li - Jia Li , Kai Li , and Li Fei - Fei . 
2009 . 
Imagenet : A large - scale hierarchical image database . 
In 2009 IEEE conference on computer vision and pattern recognition , pages 248‚Äì255 . 
Ieee . 
Andrew Drozdov , Patrick Verga , Yi - Pei Chen , Mohit Iyyer , and Andrew McCallum . 
2019 . 
Unsupervised labeled parsing with deep inside - outside recursive autoencoders . 
In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLPIJCNLP ) , pages 1507‚Äì1512 , Hong Kong , China . 
Association for Computational Linguistics . 
Desmond Elliott , Stella Frank , Khalil Sima‚Äôan , and Lucia Specia . 
2016 . 
Multi30 K : Multilingual EnglishGerman Image Descriptions . 
In Proceedings of the 5th Workshop on Vision and Language , pages 70 ‚Äì 74 , Berlin , Germany . 
Association for Computational Linguistics . 
Daniel Fried , Nikita Kitaev , and Dan Klein . 
2019 . 
Cross - domain generalization of neural constituency parsers . 
In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 323‚Äì330 , Florence , Italy . 
Association for Computational Linguistics . 
Dedre Gentner . 
1982 . 
Why nouns are learned before verbs : Linguistic relativity versus natural partitioning . 
Language development : Vol . 
2 . 
Language , thought , and culture , 2(1):301‚Äì334 . 
Dedre Gentner . 
2006 . 
Why Verbs Are Hard to Learn . 
In K. Hirsh - Pasek and R. Golinko Ô¨Ä , editors , Action Meets Word : How Children Learn Verbs , pages 544 ‚Äì 564 . 
Oxford University Press.404 Lila Gleitman . 
1990 . 
The Structural Sources of Verb Meanings . 
Language Acquisition , 1(1):3‚Äì55 . 
Ajda Gokcen , Ethan Hill , and Michael White . 
2018 . 
Madly ambiguous : A game for learning about structural ambiguity and why it ‚Äôs hard for computers . 
In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Demonstrations , pages 51‚Äì55 , New Orleans , Louisiana . 
Association for Computational Linguistics . 
Kaiming He , Xiangyu Zhang , Shaoqing Ren , and Jian Sun . 
2016 . 
Deep residual learning for image recognition . 
In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition , volume 2016 - Decem , pages 770‚Äì778 . 
Felix Hill and Anna Korhonen . 
2014 . 
Concreteness and subjectivity as dimensions of lexical meaning . 
In52nd Annual Meeting of the Association for Computational Linguistics , ACL 2014 - Proceedings of the Conference , volume 2 , pages 725‚Äì731 . 
Felix Hill , Roi Reichart , and Anna Korhonen . 
2014 . 
Multi - Modal Models for Concrete and Abstract Concept Meaning . 
Transactions of the Association for Computational Linguistics , 2:285‚Äì296 . 
Lifeng Jin , Finale Doshi - Velez , Timothy Miller , William Schuler , and Lane Schwartz . 
2018a . 
Depthbounding is e Ô¨Äective : Improvements and evaluation of unsupervised PCFG induction . 
In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 2721‚Äì2731 , Brussels , Belgium . 
Association for Computational Linguistics . 
Lifeng Jin , Finale Doshi - Velez , Timothy Miller , William Schuler , and Lane Schwartz . 
2018b . 
Unsupervised grammar induction with depth - bounded PCFG . 
Transactions of the Association for Computational Linguistics , 6:211‚Äì224 . 
Lifeng Jin , Finale Doshi - Velez , Timothy Miller , Lane Schwartz , and William Schuler . 
2019 . 
Unsupervised learning of PCFGs with normalizing Ô¨Çow . 
InProceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 2442‚Äì2452 , Florence , Italy . 
Association for Computational Linguistics . 
Lifeng Jin and William Schuler . 
2019 . 
Variance of average surprisal : A better predictor for quality of grammar from unsupervised PCFG induction . 
In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 2453‚Äì2463 , Florence , Italy . 
Association for Computational Linguistics . 
Lifeng Jin and William Schuler . 
2020 . 
The Importance of Category Labels in Grammar Induction with Child - directed Utterances . 
In Proceedings of 16th International Conference on Parsing Technologies , Seattle , USA . 
Association for Computational Linguistics . 
Mark Johnson , Thomas L. Gri Ô¨Éths , and Sharon Goldwater . 
2007 . 
Bayesian Inference for PCFGs via Markov chain Monte Carlo . 
Proceedings of Human Language Technologies : The Conference of the North American Chapter of the Association for Computational Linguistics , pages 139‚Äì146 . 
Yoon Kim , Chris Dyer , and Alexander Rush . 
2019a . 
Compound probabilistic context - free grammars for grammar induction . 
In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 2369‚Äì2385 , Florence , Italy . 
Association for Computational Linguistics . 
Yoon Kim , Alexander Rush , Lei Yu , Adhiguna Kuncoro , Chris Dyer , and G ¬¥ abor Melis . 
2019b . 
Unsupervised recurrent neural network grammars . 
In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 1105‚Äì1117 , Minneapolis , Minnesota . 
Association for Computational Linguistics . 
Nikita Kitaev and Dan Klein . 
2018 . 
Constituency parsing with a self - attentive encoder . 
In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 2676‚Äì2686 , Melbourne , Australia . 
Association for Computational Linguistics . 
Tsung - Yi Lin , Michael Maire , Serge Belongie , Lubomir Bourdev , Ross Girshick , James Hays , Pietro Perona , Deva Ramanan , C. Lawrence Zitnick , and Piotr Doll ¬¥ ar . 
2015 . 
Microsoft COCO : Common Objects in Context . 
In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition , pages 3686‚Äì3693 . 
Ndapandula Nakashole and Tom M Mitchell . 
2015 . 
A knowledge - intensive model for prepositional phrase attachment . 
In ACL - IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing , Proceedings of the Conference , volume 1 , pages 365‚Äì375 . 
Joakim Nivre , Marie - Catherine De Marne Ô¨Äe , Filip Ginter , Yoav Goldberg , Jan Haji Àác , Christopher D Manning , Ryan Mcdonald , Slav Petrov , Sampo Pyysalo , Natalia Silveira , Reut Tsarfaty , and Daniel Zeman . 
2016 . 
Universal Dependencies v1 : A Multilingual Treebank Collection . 
In Proceedings of Language Resources and Evaluation Conference . 
Hiroshi Noji and Mark Johnson . 
2016 . 
Using Leftcorner Parsing to Encode Universal Structural Constraints in Grammar Induction . 
In Proceedings of the Conference on Empirical Methods in Natural Language Processing , pages 33‚Äì43 . 
The Ohio Supercomputer Center . 
1987 . 
Ohio Supercomputer Center . 
\url{http://osc.edu /ark:/19495 /f5s1ph73}.405 Steven Pinker and B MacWhinney . 
1987 . 
The bootstrapping problem in language acquisition . 
Mechanisms of language acquisition , pages 399‚Äì441 . 
Alec Radford , Luke Metz , and Soumith Chintala . 
2016 . 
Unsupervised representation learning with deep convolutional generative adversarial networks . 
In Proceedings of the 4th International Conference on Learning Representations . 
International Conference on Learning Representations , ICLR . 
Andrew Rosenberg and Julia Hirschberg . 
2007 . 
Vmeasure : A conditional entropy - based external cluster evaluation measure . 
In Proceedings of the 2007 joint conference on empirical methods in natural language processing and computational natural language learning ( EMNLP - CoNLL ) . 
Yoav Seginer . 
2007 . 
Fast Unsupervised Incremental Parsing . 
In Proceedings of the Annual Meeting of the Association of Computational Linguistics , pages 384‚Äì391 . 
Cory Shain , William Bryce , Lifeng Jin , Victoria Krakovna , Finale Doshi - Velez , Timothy Miller , William Schuler , and Lane Schwartz . 
2016 . 
Memory - bounded left - corner unsupervised grammar induction on child - directed input . 
In Proceedings of COLING 2016 , the 26th International Conference on Computational Linguistics : Technical Papers , pages 964‚Äì975 , Osaka , Japan . 
The COLING 2016 Organizing Committee . 
Yikang Shen , Zhouhan Lin , Chin - Wei Huang , and Aaron Courville . 
2018 . 
Neural Language Modeling by Jointly Learning Syntax and Lexicon . 
In ICLR . 
Yikang Shen , Shawn Tan , Alessandro Sordoni , and Aaron Courville . 
2019 . 
Ordered Neurons : Integrating Tree Structures into Recurrent Neural Networks . 
InICLR . 
Haoyue Shi , Jiayuan Mao , Kevin Gimpel , and Karen Livescu . 
2019 . 
Visually grounded neural syntax acquisition . 
In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 1842‚Äì1861 , Florence , Italy . 
Association for Computational Linguistics . 
Michael Tomasello . 
2003 . 
Constructing a language : A usage - based theory of language acquisition . 
Harvard University Press , Cambridge , MA , US . 
Sandra Waxman , Xiaolan Fu , Sudha Arunachalam , Erin Leddon , Kathleen Geraghty , Hyun - Joo Song , Child Dev , and Perspect Author . 
2013 . 
Are Nouns Learned Before Verbs ? Infants Provide Insight into a Longstanding Debate NIH Public Access Author Manuscript . 
Child Dev Perspect , 7(3).406 A Details of datasets The MSCOCO caption dataset used in Shi et al . 
( 2019 ) contains 413,915 sentences in the training set , and 5000 sentences in the development and test sets respectively.12Every image is accompanied by 5 captions , and there are 82,783 images in total in the training set . 
The image embeddings of size 2048 used in Shi et al . 
( 2019 ) are encoded by an image classiÔ¨Åer with ResNet128 architecture trained with on the ImageNet classiÔ¨Åcation task ( Deng et al . 
, 2009 ) . 
The Multi30k caption dataset contains 29,000 sentences in the training set , and 1,014 sentences in the development and 1,000 in the test set in four diÔ¨Äerent languages , all of which except Czech are used in this work thanks to the availability of high accuracy constituency parsers in these languages.13 There are as many images as there are captions in the training set . 
The image embeddings of size 2048 provided with the dataset are encoded by an image classiÔ¨Åer with ResNet50 architecture also trained with on the ImageNet classiÔ¨Åcation task . 
For data preprocessing , following Shi et al . 
( 2019 ) , the size of the vocabulary is limited to 10,000 for all languages and datasets . 
All raw images are resized to 3 √ó64√ó64 and normalized with means [ 0 .485,0.456,0.406 ] and standard deviations [ 0.229,0.224,0.225 ] , calculated from images in ImageNet . 
B Hyperparameters The hyperparameters used in all proposed models are tuned with the MSCOCO English development set . 
For the grammar induction model , the size of word and syntactic category embeddings , as well as the size of hidden intermediary representations is 64 . 
The size of the image embedding in the ImagePCFG system is also 64 . 
All out - ofvocabulary words are replaced by the UNK symbol . 
Sentences with more than 40 words in the training set are trimmed down to 40 words . 
For the projector model , Ô¨Åve di Ô¨Äerent convolutional kernels , from ( 1,64 ) to ( 5,64 ) , are used with 128 output channels . 
The trainable image encoder employs a 12The data set can be found at https://github.com/ ExplorerFreda / VGNSL along with image embeddings encoded by pretrained image encoders . 
13The data set can be found at https://github.com/ multi30k / dataset along with image embeddings encoded by pretrained image encoders . 
ResNet18 architecture,14and the decoder employs the decoder architecture in the DCGAN model.15 A batch size of 2 is used in training . 
Adam is used as the optimizer , with the initial learning rate at 5√ó10‚àí4 . 
The loss on the validation set is checked every 20000 batches , and training is stopped when the validation loss has not been lowered for 10 checkpoints . 
The model with the lowest validation loss is used as the candidate model for test evaluation , where best parses are generated with the Viterbi algorithm on an inside chart . 
C Development Table 4 and 5 report unlabeled F1 and labeled RH results on the development sets in the multilingual caption datasets . 
Results show that development and test results are very similar , indicating that the general characteristics of the two sets are very close . 
14https://pytorch.org/docs/stable/_modules/ torchvision / models / resnet.html#resnet18 15https://github.com/pytorch/examples/blob/ master / dcgan / main.py407 ModelsEnglish Korean Polish Chinese F1 RH F1 RH F1 RH F1 RH NoImagePCFG 60.3 ¬±8.246.4¬±11.038.6¬±8.722.6¬±6.959.5¬±3.847.5¬±3.9 ImagePrePCFG 55.7 ¬±7.539.6¬±5.439.5¬±4.224.1¬±3.461.2¬±1.650.1¬±3.3 ImagePCFG 55.4 ¬±2.743.2¬±1.845.1¬±2.327.5¬±2.654.3¬±8.341.6¬±7.9 Table 4 : Averages and standard deviations of labeled Recall - Homogeneity and unlabeled F1 scores of various unsupervised grammar inducers on the MSCOCO caption development datasets . 
ModelsGerman English French F1 RH F1 RH F1 RH NoImagePCFG 47.2 ¬±5.753.6¬±5.759.1¬±8.152.2¬±8.543.8¬±4.943.2¬±5.2 ImagePrePCFG 44.8 ¬±7.950.0¬±8.346.7¬±7.340.7¬±7.542.3¬±10.342.8¬±10.5 ImagePCFG 45.6 ¬±5.250.6¬±8.547.7¬±5.440.9¬±5.243.1¬±5.143.9¬±5.5 ModelsKorean Polish Chinese F1 RH F1 RH F1 RH NoImagePCFG 30.6 ¬±5.722.2¬±3.049.4¬±4.940.0¬±5.359.7¬±3.353.6¬±4.7 ImagePrePCFG 27.0 ¬±4.819.2¬±3.648.5¬±3.138.5¬±3.155.5¬±9.348.3¬±10.4 ImagePCFG 45.1 ¬±1.133.4¬±1.949.5¬±7.640.8¬±6.358.3¬±3.252.1¬±4.3 Table 5 : Averages and standard deviations of labeled Recall - Homogeneity and unlabeled F1 scores of various unsupervised grammar inducers on the Multi30k caption development datasets.408 Proceedings of the 1st Conference of the Asia - PaciÔ¨Åc Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing , pages 409‚Äì424 December 4 - 7 , 2020 . 
¬© 2020 Association for Computational Linguistics Heads - up ! Unsupervised Constituency Parsing via Self - Attention Heads Bowen Li‚Ä†Taeuk Kim‚Ä°Reinald Kim Amplayo‚Ä†Frank Keller‚Ä† ‚Ä†ILCC , School of Informatics , University of Edinburgh , UK ‚Ä°Dept . 
of Computer Science and Engineering , Seoul National University , Korea { bowen.li , reinald.kim } @ed.ac.uk taeuk@europa.snu.ac.kr keller@inf.ed.ac.uk Abstract Transformer - based pre - trained language models ( PLMs ) have dramatically improved the state of the art in NLP across many tasks . 
This has led to substantial interest in analyzing the syntactic knowledge PLMs learn . 
Previous approaches to this question have been limited , mostly using test suites or probes . 
Here , we propose a novel fully unsupervised parsing approach that extracts constituency trees from PLM attention heads . 
We rank transformer attention heads based on their inherent properties , and create an ensemble of high - ranking heads to produce the Ô¨Ånal tree . 
Our method is adaptable to low - resource languages , as it does not rely on development sets , which can be expensive to annotate . 
Our experiments show that the proposed method often outperform existing approaches if there is no development set present . 
Our unsupervised parser can also be used as a tool to analyze the grammars PLMs learn implicitly . 
For this , we use the parse trees induced by our method to train a neural PCFG and compare it to a grammar derived from a human - annotated treebank . 
1 Introduction Pre - trained language models ( PLMs ) , particularly BERT ( Devlin et al . 
, 2019 ) and others ( Yang et al . 
, 2019 ; Liu et al . 
, 2019b ; Radford et al . 
, 2019 ) based on the transformer architecture ( Vaswani et al . 
, 2017 ) , have dramatically improved the state of the art in NLP . 
Such models make it possible to train a large , generic language model on vast unannotated datasets , and then Ô¨Åne - tune it for a speciÔ¨Åc task using a small amount of annotated data . 
The success of PLMs has led to a large literature investigating the linguistic knowledge that PLMs learn implicitly during pre - training ( Liu et al . 
, 2019a ; Clark et al . 
, 2019 ; Kovaleva et al . 
, 2019 ; Pimentel et al . 
, 2020 ) , sometimes referred to as BERTology ( Rogers et al . 
, 2020).BERTology has been particularly concerned with the question whether BERT - type models learn syntactic structure . 
Typical approaches include test suites of sentences that instantiate speciÔ¨Åc syntactic structures ( Goldberg , 2019 ) , general probes ( also known as diagnostic classiÔ¨Åers , Belinkov and Glass 2019 ) or structural probes ( Hewitt and Manning , 2019 ) . 
All of these approaches are limited : the Ô¨Årst one requires the laborious compilation of languageand construction - speciÔ¨Åc suites of sentences ; the second one sometimes fails to adequately reÔ¨Çect differences in representations ( Zhang and Bowman , 2018 ; Hewitt and Liang , 2019 ; V oita and Titov , 2020 ) ; the third one involves designing a novel extraction model that is not applicable to tasks other than probing ( Maudslay et al . 
, 2020 ) . 
It is therefore natural to use a parsing task to test whether the representations learned by PLMs contain usable syntactic information . 
This enables us to test syntactic structure in general , rather than speciÔ¨Åc constructions , and does n‚Äôt require a specialized probe . 
In this paper , we will therefore use PLM attention heads to construct an unsupervised constituency parser . 
Previously , related approaches have been proposed under the heading of zero - shot constituency parsing ( Kim et al . 
, 2020a , b).1However , this prior work crucially relies on an annotated development set in order to identify transformer heads that are sensitive to syntactic structure . 
Existing approaches therefore are not truly unsupervised . 
For most low resource languages , no such annotated data is available , and often not even an annotation scheme exists . 
Thus , assuming a development set is not a realistic experimental setup ( Kann et al . 
, 2019 ) . 
If a suitable development set is available , Shi et al . 
( 2020 ) shows that an existing supervised parser trained on a few - shot setting can outperform all the unsupervised parsing 1Like Kim et al . 
( 2020b ) , we use zero - shot to refer to the transfer from language modeling to constituency parsing.409 methods by a signiÔ¨Åcant margin . 
It strongly challenges tuning on an annotated development set for unsupervised parsing . 
In this paper , we propose a novel approach to build a PLM - based unsupervised parser that does not require a development set : we rank transformer heads based on their inherent properties , such as how likely tokens are to be grouped in a hierarchical structure . 
We then ensemble the top- Kheads to produce constituency trees . 
We evaluate our approach and previous zero - shot approaches on the English Penn Treebank ( PTB ) and eight other languages on the SPMRL dataset . 
On the one hand , if the development set is absent , our approach largely outperforms previous zero - shot approaches on the English PTB . 
On the other hand , if previous zero - shot approaches are equipped with the development set , our approach can still match the parsing performance of these approaches using the single best head or layerwise ensembling . 
For the multilingual experiment , we take advantage of the top- Kheads selected in English and directly parse other languages using our approach . 
Surprisingly , on Ô¨Åve out of nine languages , this crosslingual unsupervised parser matches previous approaches that rely on a development set in each target language with the single best head or layer - wise ensembling . 
However , our fully unsupervised method lags behind the previous state - of - the - art zero - shot parser if a top- Kensemble is used . 
Furthermore , our approach can be use as a tool to analyze the capability of PLMs in learning syntactic knowledge . 
As no human annotation is required , our approach has the potential to reveal the grammar PLMs learn implicitly . 
Here , we use the tree structures generated by our parser to train a neural PCFG . 
We evaluate the learned grammar against the English PTB on internal tags and production rules both qualitatively and quantitatively . 
2 Related Work Recently , neural models have renewed interest in grammar induction . 
Earlier work ( Choi et al . 
, 2018 ; Williams et al . 
, 2018 ) attempted to induce grammar by optimizing a sentence classiÔ¨Åcation objective , while follow - up work ( Htut et al . 
, 2018 ; Shen et al . 
, 2018a , 2019 ) showed that a language modeling objective performs better . 
Latest work employed autoencoders or probabilistic grammars ( Drozdov et al . 
, 2019 ; Kim et al . 
, 2019a , b ; Zhu et al . 
, 2020).A new line of work is zero - shot constituency parsing , whose goal is to automatically extract trees from PLMs in a parameter - free fashion . 
The top - down zero - shot parser ( Kim et al . 
, 2020a ) utilizes the concept of syntactic distance ( Shen et al . 
, 2018b ) , where trees are induced by an algorithm that recursively splits a sequence of words in a topdown manner . 
However , this approach suffers from its greedy search mode , failing to take into account all possible subtrees . 
The chart - based zero - shot parser ( Kim et al . 
, 2020b ) applies chart parsing to address this problem . 
Wu et al . 
( 2020 ) introduced a parameter - free probing technique to analyze PLMs via perturbed masking . 
There is also prior work on extracting constituency trees from self - attention mechanisms of transformers . 
Mare Àácek and Rosa ( 2018 ) proposed heuristic approaches to convert attention weights to trees . 
Mare Àácek and Rosa ( 2019 ) introduced a chartbased tree extraction method in transformer - based neural machine translation encoders and provide a quantitative study . 
3 Zero - shot Constituency Parsing via PLMs In this section , we brieÔ¨Çy review the chart - based zero - shot parser and then introduce our rankingbased zero - shot parser . 
3.1 Chart - based Zero - shot Parsing In chart - based zero - shot parsing , a real - valued scorestree(t)is assigned for each tree candidate t , which decomposes as : stree(t ) = /summationdisplay ( i , j)‚ààtsspan(i , j ) , wheresspan(i , j)is the score ( or cost ) for a constituent that is located between positions iandj ( 1‚â§i‚â§j‚â§n , wherenis the length of the sentence ) . 
SpeciÔ¨Åcally , for a span of length 1 , sspan(i , j)is deÔ¨Åned as 0 when i = j. For a span longer than 1 , the following recursion applies : sspan(i , j ) = scomp(i , j ) + mini‚â§k < jssplit(i , k , j ) ( 1 ) ssplit(i , k , j ) = sspan(i , k ) + sspan(k+ 1,j),(2 ) wherescomp(¬∑,¬∑)measures the validity or compositionality of the span ( i , j)itself , whilessplit(i , k , j ) indicates how plausible it is to split the span ( i , j ) at positionk . 
Two alternatives have been developed in Kim et al . 
( 2020b ) for scomp ( ¬∑ , ¬∑ ): the pair410 score function sp(¬∑,¬∑)and the characteristic score functionsc ( ¬∑ , ¬∑ ) . 
The pair score function sp(¬∑,¬∑)computes the average pair - wise distance in a given span : sp(i , j ) = 1 / parenleftbigj‚àíi+1 2 / parenrightbig / summationdisplay ( wx , wy)‚ààpair ( i , j)f(g(wx),g(wy)),(3 ) where pair ( i , j)returns a set consisting of all combinations of two words ( e.g. , wx , wy ) inside the span ( i , j ) . 
Functionsf(¬∑,¬∑)andg(¬∑)are the distance measure function and the representation extractor function , respectively . 
For g , givenlas the number of layers in a PLM , gis actually a set of functions g={gd ( u , v)|u= 1, ... ,l , v = 1, ... ,a } , each of which outputs the attention distribution of the vth attention head on the uthlayer of the PLM.2In case of the function f , there are also two options , Jensen - Shannon ( JSD ) and Hellinger ( HEL ) distance . 
Thus , f={JSD , HEL } . 
The characteristic score function sc(¬∑,¬∑)measures the distance between each word in the constituent and a predeÔ¨Åned characteristic value c ( e.g. , the center of the constituent ): sc(i , j ) = 1 j‚àíi+ 1 / summationdisplay i‚â§x‚â§jf(g(wx),c),(4 ) where c=1 j‚àíi+1 / summationtext i‚â§y‚â§jg(wy ) . 
Sincescomp(¬∑,¬∑)is well deÔ¨Åned , it is straightforward to compute every possible case of sspan(i , j ) using the CKY algorithm ( Cocke , 1969 ; Kasami , 1966 ; Younger , 1967 ) . 
Finally , the parser outputs ÀÜt , the tree that requires the lowest score ( cost ) to build , as a prediction for the parse tree of the input sentence : ÀÜt= arg mintstree(t ) . 
For attention heads ensembling , both a layerwise ensemble and a top- Kensemble are considered . 
The Ô¨Årst one averages all attention heads from a speciÔ¨Åc layer , while the second one averages the top - Kheads from across different layers . 
At test time , separate trees produced by different heads are merged to one Ô¨Ånal tree via syntactic distance .3 The chart - based zero - shot parser achieves the state of the art in zero - shot constituency parsing . 
2The hidden representations of the given words can also serve as an alternative for g. But Kim et al . 
( 2020a ) show that the attention distributions provide more syntactic clues under the zero - shot setting . 
3Details can be found in Kim et al . 
( 2020b ) . 
For the ensemble parsing , marrying chart - based parser and top - down parser yields better results than averaging the attention distributions.3.2 Ranking - based Zero - shot Parsing The chart - based zero - shot parser relies on the existing development set of a treebank ( e.g. , the English PTB ) to select the best conÔ¨Åguration , i.e. , the combination of{g|gd ( u , v),u= 1, ... ,l , v = 1, ... ,a } , { f|JSD , HEL},{scomp|sp , sc } , and heads ensemble that achieves the best parsing accuracy . 
Such a development set always contains hundreds of sentences , hence considerable annotation effort is still required . 
From the perspective of unsupervised parsing , such results arguably are not fully unsupervised.4Another argument against using a development set is that the linguistic assumptions inherent in the expert annotation required to create the development set potentially restrict our exploration of how PLMs model the constituency structures . 
It could be that the PLM learns valid constituency structures , which however do not match the annotation guidelines that were used to create the development set . 
Here , we take a radical departure from the previous work in order to extract constituency trees from PLMs in a fully unsupervised manner . 
We propose a two - step procedure for unsupervised parsing : ( 1 ) identify syntax - related attention heads directly from PLMs without relying on a development set of a treebank ; ( 2 ) ensemble the selected top- Kheads to produce the constituency trees . 
For identiÔ¨Åcation of the syntax - related attention heads , we rank all heads by scoring them with a chart - based ranker . 
We borrow the idea of the chartbased zero - shot parser to build our ranker . 
Given an input sentence and a speciÔ¨Åc choice of fandscomp , each attention head gd ( u , v)in the PLM yields one unique attention distribution . 
Using the chart - based zero - shot parser in Section 3.1 , we can obtain the score of the best constituency tree as:5 sparsing ( u , v ) = stree(ÀÜt ) = /summationtext ( i , j)‚ààÀÜtsspan(i , j),(5 ) where ÀÜt= arg mintstree(t ) . 
It is obvious 4Some previous work ( Shen et al . 
, 2018a , 2019 ; Drozdov et al . 
, 2019 ; Kim et al . 
, 2019a ) also use a development set to tune hyperparameters or early - stop training . 
5Our ranking method works approximately as a maximum a posteriori probability ( MAP ) estimate , since we only consider the best tree the attention head generates . 
In unsupervised parsing , marginalization is a standard method for model development . 
We have tried to apply marginalization to our ranking algorithm where all possible trees are considered and the sum score is calculated ( using the logsumexp trick ) for ranking . 
But marginalization does not work well for attention distributions , where an ‚Äú attending broadly ‚Äù head with higher entropy is more favorable than a syntax - related head with lower entropy . 
So we only consider the score of the best tree.411 that all combinations of { f|JSD , HEL}and { scomp|sp , sc}will produce multiple scores for a given head . 
Here we average the scores of all such combinations to get one single score . 
Then we rank all attention heads and select the syntax - related heads for parsing . 
However , directly applying the chart - based zero - shot parser in Section 3.1 for ranking delivers a trivial , ill - posed solution . 
The recursion in Eq . 
( 2 ) only encourages the intra - similarity inside the span . 
Intuitively , one attention head that produces the same attention distribution for each token ( e.g. , a uniform attention distribution or one that forces every token to attend to one speciÔ¨Åc token ) will get the lowest score ( cost ) and the highest ranking.6 To address this issue , we Ô¨Årst introduce intersimilarity into the recursion in Eq . 
( 2 ) and get the following : ssplit(i , k , j ) = sspan(i , k ) + sspan(k+ 1,j)‚àíscross(i , k , j ) , ( 6 ) where the cross score scross(i , k , j ) is the similarity between two subspans ( i , k)and(k+ 1,j ) . 
However , this formulation forces the algorithm to go to the other extreme : one attention head that produces a totally different distribution for each token ( e.g. , force each token to attend to itself or the previous / next token ) will get the highest ranking . 
To balance the inter- and intra - similarity and avoid having to introduce a tunable coefÔ¨Åcient , we simply add a length - based weighting term to Eq . 
( 1 ) and get : sspan(i , j ) = j‚àíi+ 1 n(scomp(i , j ) + min i‚â§k < jssplit(i , k , j ) ) , ( 7 ) wherej‚àíi+ 1 is the length of the span ( i , j ) . 
The length ratio functions as a regulator to assign larger weights to longer spans . 
This is motivated by the fact that longer constituents should contribute more to the scoring of the parse tree , since the inter - similarity always has strong effects on shorter spans . 
In this way , the inter- and intra - similarity can be balanced . 
With respect to the choice for scross(i , k , j ) , we follow the idea of spandscin Eq . 
( 3 ) and ( 4 ) 6Such cases do exist in PLMs . 
Clark et al . 
( 2019 ) shows that BERT exhibits clear surface - level attention patterns . 
Some of these patterns will deliver ill - posed solutions in ranking : attend broadly , attend to a special tokens ( e.g. , [ SEP ] ) , attend to punctuation ( e.g. , period ) . 
One can also observe these patterns using the visualization tool provided by Vig ( 2019).and propose the pair score function spxand the characteristic score function scx7for cross score computation . 
spxis deÔ¨Åned as : spx(i , j ) = 1 ( k‚àíi+ 1)(j‚àík)/summationdisplay ( wx , wy)‚ààprod ( i , k , j)f(g(wx),g(wy ) ) , where prod ( i , k , j ) returns a set of the product of words from the two subspans ( i , k)and(k+ 1,j ) . 
Andscxis deÔ¨Åned as : scx(i , j ) = f(ci , k , ck+1,j ) , where ci , k=1 k‚àíi+1 / summationtext i‚â§x‚â§kg(wx),ck+1,j= 1 j‚àík / summationtext k+1‚â§y‚â§jg(wy ) . 
We average all the combinations of { f|JSD , HEL},{scomp|sp , sc}and { scross|spx , scx}to rank all the attention heads and select the top- Kheads . 
After the ranking step , we perform constituency parsing by ensembling the selected heads . 
We simply employ the ensemble method in Section 3.1 and average all the combinations of { f|JSD , HEL } and{scomp|sp , sc}to get a single predicted parse tree for a given sentence . 
3.3 How to select K For ensemble parsing , Kim et al . 
( 2020b ) proposed three settings : the best head , layerwise ensemble , and top - Kensemble . 
To prevent introducing a tunable hyperparameter , we propose to select a value forKdynamically based on a property of the ranking score in Eq . 
( 5 ) . 
Since we use a similarity - based distance , the lower the ranking score , the higher the ranking . 
Assuming that scores are computed for all attention heads , we can sort the scores in ascending order . 
Intuitively , given the order , we would like to choose thekfor which ranking score increases the most , which means syntactic relatedness drops the most . 
Supposesparsing ( k)is the ranking score where kis the head index in the ascending order , then this is equivalent to Ô¨Ånding the kwith the greatest gradient on the curve of the score . 
We Ô¨Årst estimate the gradient of sparsing ( k)and then Ô¨Ånd the kwith the greatest gradient . 
Finally , Kis computed as : K= arg max k / summationdisplay k‚àíŒ¥‚â§j‚â§k+Œ¥ j / negationslash = ksparsing ( k+j)‚àísparsing ( k ) j , 7Subscripts in the naming of functions in this paper : p ‚Äì pair score , c ‚Äì characteristic score , x ‚Äì cross score.412 where we smooth the gradient by considering Œ¥ steps . 
Here , we set Œ¥= 3 . 
In practice , we Ô¨Ånd that the greatest gradient always happens in the head or the tail of the curve . 
For the robustness , we select the Kfrom the middle range of the score function curve , i.e. , starting from 30 and ending with 75 % of all heads.8We also provide a lazyoption forKselection , which simply assume a Ô¨Åxed value of 30 for the top- Kensemble . 
4 Grammar Learning We are also interested in exploring to what extent the syntactic knowledge acquired by PLMs resembles human - annotated constituency grammars . 
For this exploration , we infer a constituency grammar , in the form of probabilistic production rules , from the trees induced from PLMs . 
This grammar can then be analyzed further , and compared to humanderived grammars . 
Thanks to the recent progress in neural parameterization , neural PCFGs have been successfully applied to unsupervised constituency parsing ( Kim et al . 
, 2019a ) . 
We harness this model9 to learn probabilistic constituency grammars from PLMs by maximizing the joint likelihood of sentences and parse trees induced from PLMs . 
In the following , we Ô¨Årst brieÔ¨Çy review the neural PCFG and then introduce our training algorithm . 
4.1 Neural PCFGs A probabilistic context - free grammar ( PCFG ) consists of a 5 - tuple grammar G= ( S , N , P , Œ£ , R ) and rule probabilities œÄ={œÄr}r‚ààR , whereSis the start symbol , Nis a Ô¨Ånite set of nonterminals , Pis a Ô¨Ånite set of preterminals , Œ£is a Ô¨Ånite set of terminal symbols , and Ris a Ô¨Ånite set of rules associated with probabilities œÄ . 
The rules are of the form : S‚ÜíA , A‚ààN A‚ÜíBC , A‚ààN , B , C‚ààN‚à™P T‚Üíw , T‚ààP , w‚ààŒ£. 8Although our ranking algorithm can Ô¨Ålter out noisy heads , by observing the attention heatmaps , we Ô¨Ånd that noisy heads sometimes still rank high . 
We do not do any post - processing to further Ô¨Ålter out the noisy heads , so we empirically search kstarting at 30 . 
9A more advanced version of the neural PCFG , the compound PCFG , has also been developed in Kim et al . 
( 2019a ) . 
In this model variant , a compound probability distribution is built upon the parameters of a neural PCFG . 
In preliminary experiments , we found the compound PCFG learns similar grammars as the neural PCFG . 
So we only use the more lightweight neural PCFG in this work . 
AssumingTGis the set of all possible parse trees ofG , the probability of a parse tree t‚àà TGis deÔ¨Åned asp(t ) = /producttext r‚ààtRœÄr , wheretRis the set of rules used in the derivation of t. A PCFG also deÔ¨Ånes the probability of a given sentence x(string of terminals x‚ààŒ£‚àó ) viap(x ) = /summationtext t‚ààTG(x)p(t ) , whereTG(x ) = { t|yield ( t ) = x } , i.e. , the set of trees tsuch that t ‚Äôs leaves are x. The traditional way to parameterize a PCFG is to assign a scalar to each rule œÄrunder the constraint that valid probability distributions must be formed . 
For unsupervised parsing , however , this parameterization has been shown to be unable to learn meaningful grammars from natural language data ( Carroll and Charniak , 1992 ) . 
Distributed representations , the core concept of the modern deep learning , have been introduced to address this issue ( Kim et al . 
, 2019a ) . 
SpeciÔ¨Åcally , embeddings are associated with symbols and rules are modeled based on such distributed and shared representations . 
In the neural PCFG , the log marginal likelihood : logpŒ∏(x ) = log / summationdisplay t‚ààTG(x)pŒ∏(t ) can be computed by summing out the latent parse trees using the inside algorithm ( Baker , 1979 ) , which is differentiable and amenable to gradient based optimization . 
We refer readers to the original paper of Kim et al . 
( 2019a ) for details on the model architecture and training scheme . 
4.2 Learning Grammars from Induced Trees Given the trees induced from PLMs ( described in Section 3.2 ) , we use neural PCFGs to learn constituency grammars . 
In contrast to unsupervised parsing , where neural PCFGs are trained solely on raw natural language data , we train them on the sentences and the corresponding tree structures induced from PLMs . 
Note that this differs from a fully supervised parsing setting , where both tree structures and internal constituency tags ( nonterminals and preterminals ) are provided in the treebank . 
In our case , the trees induced from PLMs have no internal annotations . 
For the neural PCFG training , the joint likelihood is given by : logp(x,ÀÜt ) = /summationdisplay r‚ààÀÜtRlogœÄr , where ÀÜtis the induced tree and ÀÜtRis the set of rules applied in the derivation of ÀÜt . 
Although tree struc-413 tures are given during training , marginalization is still involved : all internal tags will be marginalized to compute the joint likelihood . 
Therefore , the grammars learned by our method are anonymized : nonterminals and preterminals will be annotated as NT - idand T - id , respectively , where idis an arbitrary ID number . 
5 Experiments We conduct experiments to evaluate the unsupervised parsing performance of our ranking - based zero - shot parser on English and eight other languages ( Basque , French , German , Hebrew , Hungarian , Korean , Polish , Swedish ) . 
For the grammars learned from the induced parse trees , we perform qualitative and quantitative analysis on how the learned grammars resemble the human - crafted grammar of the English PTB . 
5.1 General Setup We prepare the PTB ( Marcus et al . 
, 1993 ) for English and the SPMRL dataset ( Seddah et al . 
, 2013 ) for eight other languages . 
We adopt the standard split of each dataset to divide it into development and test sets . 
For preprocessing , we follow the setting in Kim et al . 
( 2019a , b ) . 
We run our ranking algorithm on the development set to select the syntax - related heads and the ensemble parsing algorithm on the test set . 
We only use the raw sentences in the development set , without any syntactic annotations . 
We average all conÔ¨Ågurations both for ranking ( f , scomp andscross ) and parsing ( fandscomp ) ; hence we do not tune any hyperparameters for our algorithm . 
ForKselection , we experiment with Ô¨Åxed top- K ( i.e. , top-30 ) and dynamically searching the best Kdescribed in Section 3.3 , dubbed dynamic K. We report the unlabeled sentence - level F1score to evaluate the extent to which the induced trees resemble the corresponding gold standard trees . 
For neural PCFG training , we modify some details but keep most of the model conÔ¨Ågurations of Kim et al . 
( 2019a ) ; we refer readers to the original paper for more information . 
We train the models on longer sentences for more epochs . 
SpeciÔ¨Åcally , we train on sentences of length up to 30 in the Ô¨Årst epoch , and increase this length limit by Ô¨Åve until the length reaches 80 . 
We train for 30 epochs and use a learning rate scheduler . 
Model Top - down Chart - based Our ranking - based ConÔ¨ÅgurationSingle Single Top Top Top Dynamic Full /Layer‚Ä†/Layer‚Ä†-K -K‚Ä°-K K heads w/ dev trees w/o dev trees BERT - base - cased 32.6 37.5 42.7 29.3 34.8 37.1 35.8 BERT - large - cased 36.7 41.5 44.6 21.5 36.1 38.7 33.2 XLNet - base - cased 39.0 40.5 46.4 38.4 41.2 42.7 42.4 XLNet - large - cased 37.3 39.7 46.4 34.1 40.6 41.1 41.2 RoBERTa - base 38.0 41.0 45.0 35.9 41.7 42.1 39.6 RoBERTa - large 33.8 38.6 42.8 30.2 33.1 37.5 35.7 GPT2 35.4 34.5 38.5 21.9 26.1 27.2 26.1 GPT2 - medium 37.8 38.5 39.8 19.4 29.1 29.1 27.2 A VG 36.3 39.0 43.3 28.8 35.3 36.9 35.1 A VG w/o GPT2 * 36.2 39.8 44.7 31.6 37.9 39.8 38.0 Table 1 : Unlabeled sentence - level parsing F1scores on the English PTB test set . 
‚Ä† : the best results of the top single head and layer - wise ensemble . 
‚Ä° : directly applying the chart - based parser for ranking ( no development set trees ) and ensembling the top- Kheads for parsing . 
* : averageF1scores without GPT2 and GPT2 - medium . 
Bold Ô¨Ågures highlight the best scores for the two different groups : with and without development trees . 
Model F1SBAR NP VP PP ADJP ADVP Balanced 18.5 7 27 8 18 27 25 Left branching 8.7 5 11 0 5 2 8 Right branching 39.4 68 2471 42 27 38 BERT - base - cased 37.1 36 49 30 42 40 69 BERT - large - cased 38.7 38 50 30 46 42 72 XLNet - base - cased 42.7 45 58 31 46 46 72 XLNet - large - cased 41.1 44 54 30 42 48 64 RoBERTa - base 42.1 38 58 3147 42 71 RoBERTa - large 37.5 35 53 29 33 36 54 Table 2 : Unlabeled parsing scores and recall scores on six constituency tags of trivial baseline parse trees as well as ones achieved by our parser using dynamic K on different PLMs . 
5.2 Results on the English PTB We Ô¨Årst evaluate our ranking - based zero - shot parser on the English PTB dataset . 
We apply our methods to four different PLMs for English : BERT ( Devlin et al . 
, 2019 ) , XLNet ( Yang et al . 
, 2019 ) , RoBERTa ( Liu et al . 
, 2019b ) , and GPT2 ( Radford et al . 
, 2019).10 Table 1 shows the unlabeled F1scores for our ranking - based zero - shot parser as well as for previous zero - shot parsers in two settings , with and without an annotated development set . 
We employ the chart - based parser in a setting without development trees , where Eqs . 
( 1 ) and ( 2 ) are used for 10We follow previous work ( Kim et al . 
, 2020a , b ) in using two variants for each PLM , where the X - base variants consist of 12 layers , 12 attention heads , and 768 hidden dimensions , while the X - large ones have 24 layers , 16 heads , and 1024 dimensions . 
With regard to GPT2 , the GPT2 model corresponds to X - base while GPT2 - medium to X - large.414 0 20 40 60 80 100 120 140 k heads3132333435363738parsing f1 0.6 0.5 0.4 0.3 0.2 score parsing f1 score(a ) BERT - base - cased 0 20 40 60 80 100 120 140 k heads353637383940414243parsing f1 0.7 0.6 0.5 0.4 0.3 0.2 score parsing f1 score ( b ) XLNet - base - cased 0 20 40 60 80 100 120 140 k heads373839404142parsing f1 0.6 0.5 0.4 0.3 0.2 score parsing f1 score ( c ) RoBERTa - base 0 50 100 150 200 250 300 350 400 k heads313233343536373839parsing f1 0.6 0.5 0.4 0.3 0.2 0.1 score parsing f1 score ( d ) BERT - large - cased 0 50 100 150 200 250 300 350 400 k heads3637383940414243parsing f1 0.7 0.6 0.5 0.4 0.3 0.2 score parsing f1 score ( e ) XLNet - large - cased 0 50 100 150 200 250 300 350 400 k heads333435363738parsing f1 0.6 0.5 0.4 0.3 0.2 score parsing f1 score ( f ) RoBERTa - large Figure 1 : Relation between Kfor top - Kand parsing performance on different PLMs . 
The blue curve shows the ranking score of heads where heads are sorted in an ascending order . 
The red curve shows the parsing performance that is evaluated on the PTB test set given every 10 heads . 
The green dashed line indicates the dynamic K. ranking and ensembling the top- K(i.e . 
, top-30 ) heads . 
Compared to our method under the same conÔ¨Åguration , its poor performance conÔ¨Årms the effectiveness of our ranking algorithm . 
With respect to the Kselection , our dynamic Kmethod beats both Ô¨Åxed top-30 and full heads . 
Surprisingly , using all attention heads for ensemble parsing yields nearly the same performance as using top-30 heads . 
This suggests that although our ranking algorithm Ô¨Ålters out some noisy heads , it is still not perfect . 
On the other hand , the ensemble parsing method is robust to noisy heads when full attention heads are used . 
Figure 1 shows how the ensemble parsing performance changes given different Kselection . 
We can identify a roughly concave shape of the parsing performance curve , which indicates why our ranking algorithm works . 
Interestingly , the parsing performance does not drop too much when Kreaches the maximum for XLNet . 
We conjecture that syntactic knowledge is more broadly distributed across heads in XLNet . 
Our ranking - based parser performs badly on GPT2 and GPT2 - medium , which is not unexpected . 
Unlike other PLMs , models in the GPT2 category are auto - regressive language models , whose attention matrix is strictly lower triangular . 
It makes it hard for our ranking algorithm to work properly . 
But for top - down and chart - based zero - shot parsers , tuning against an annotated development set canalleviate this problem . 
We focus on BERT , XLNet and RoBERTa and only evaluate these three models in the rest of our experiments . 
Except for GPT2 variants , our parser with dynamic Koutperforms the top - down parser in all cases . 
On average ( without GPT2 variants ) , even though our parser only requires raw sentence data , it still matches the chartbased parser with the top single head or layer - wise ensemble . 
To explore the limit of the chart - based parser , we also present the results by selecting the top - K(i.e . 
, top-20 ) heads using the annotated development set ( Kim et al . 
, 2020b).11Note that in this setting , the best conÔ¨Åguration , i.e. , the combination ofg , fandscomp as well asKare selected against the development set . 
This setting serves as an upper bound of the chart - based zero - shot parsing and largely outperforms our ranking - based method . 
Table 2 presents the parsing scores as well as recall scores on different constituents of trivial baselines and our parser . 
It indicates that trees induced from XLNet - base - cased , XLNet - large - cased and RoBERTa - base can outperform the right - branching baseline without resembling it . 
This conÔ¨Årms that PLMs can produce non - trivial parse trees . 
Large gains on NP , ADJP and ADVP compared to the 11Selecting heads against a development set ensures the quality of high ranking heads ; top-20 heads are optimal in this setting ( Kim et al . 
, 2020b ) , unlike top-30 in our setting.415 Model English Basque French German Hebrew Hungarian Korean Polish Swedish A VG Trivial baselines Balanced 18.5 24.4 12.9 15.2 18.1 14.0 20.4 26.1 13.3 18.1 Left branching 8.7 14.8 5.4 14.1 7.7 10.6 16.5 28.7 7.6 12.7 Right branching 39.4 22.4 1.3 3.0 0.0 0.0 21.1 0.7 1.7 10.0w/ dev treesChart - based ( Single / Layer)‚Ä† M - BERT 41.2 38.1 30.6 32.1 31.9 30.4 46.4 43.5 27.5 35.7 XLM 43.0 35.3 35.6 41.6 39.9 34.5 35.7 51.7 33.7 39.0 XLM - R 44.4 40.4 31.0 32.8 34.1 32.4 47.5 44.7 29.2 37.4 XLM - R - large 40.8 36.5 26.4 30.2 32.1 26.8 45.6 47.9 25.8 34.7 A VG 42.4 37.6 30.9 34.2 34.5 31.0 43.8 46.9 29.1 36.7 Chart - based ( top- K)‚Ä† M - BERT 45.0 41.2 35.9 35.9 37.8 33.2 47.6 51.1 32.6 40.0 XLM 47.7 41.3 36.7 43.8 41.0 36.3 35.7 58.5 36.5 41.9 XLM - R 47.0 42.2 35.8 37.7 40.1 36.6 51.0 52.7 32.9 41.8 XLM - R - large 45.1 40.2 29.7 37.1 36.2 31.0 46.9 47.9 27.8 38.0 A VG 46.2 41.2 34.5 38.6 38.8 34.3 45.3 52.6 32.5 40.4w / o dev treesCrosslingual ranking - based ( Dynamic K)‚Ä° M - BERT 40.7 38.2 31.0 31.0 29.0 27.1 43.3 30.7 25.8 33.0 XLM 44.9 26.6 35.8 39.7 39.6 32.9 28.0 50.1 34.1 36.9 XLM - R 45.5 38.2 34.0 35.5 36.7 33.5 45.2 39.4 29.9 37.6 XLM - R - large 41.0 37.9 28.0 28.0 31.3 24.6 44.4 32.2 24.9 32.5 A VG 43.0 34.7 32.4 33.5 35.0 29.8 40.4 39.2 29.2 35.3 Table 3 : Parsing results on nine languages with multilingual PLMs.‚Ä† : attention heads are selected on the development trees in the target language.‚Ä° : attention heads are selected on raw sentences in English . 
Bold Ô¨Ågures highlight the best scores for the two different groups : with and without development trees . 
right branching baseline show that PLMs can better identify such constituents . 
5.3 Results for Languages other than English Low - resource language parsing is one of the main motivations for the development of unsupervised parsing algorithms , which makes a multilingual setting ideal for evaluation . 
Multilingual PLMs are attractive in this setting because they are trained to process over one hundred languages in a languageagnostic manner . 
Kim et al . 
( 2020b ) has investigated the zero - shot parsing capability of multilingual PLMs assuming that a small annotated development set is available . 
Here , by taking advantage of our ranking - based parsing algorithm , we use a more radical crosslingual setting . 
We rank attention heads only on sentences in English and directly apply the parser to eight other languages . 
We follow Kim et al . 
( 2020b ) and use four multilingual PLMs : a multilingual version of the BERT - base model ( M - BERT , Devlin et al . 
2019 ) , the XLM model ( Conneau and Lample , 2019 ) , the XLM - R and XLM - R - large models ( Conneau et al . 
, 2020 ) . 
Each multilingual PLM differs in architecture and pre - training data , and we refer readers to the original papers for more details . 
In Table 3 , our crosslingual parser outperforms the trivial baselines in all cases by a large margin . 
Compared with the chart - based parser with the top head or layer - wise ensemble , our crosslingual parser can match the performance on Ô¨Åve out of nine languages . 
Among four model variants , XLM - R and XLM - R - large have identical training settings and pre - training data , and so form a controlled experiment . 
By directly comparing XLM - R and XLM - R - large , we conjecture that , as the capacity of the PLM scales , the model has more of a chance to learn separate hidden spaces for different languages . 
This is consistent with a recent study on multilingual BERT ( Dufter and Sch ¬®utze , 2020 ) showing that underparameterization is one of the main factors that contribute to multilinguality . 
Again , our method lags behind the chart - base zero - shot parser with a top- Kensemble . 
More experimental results including using target language for head selection in our method can be found in Appendix A.1 . 
5.4 Grammar Analysis By not relying on an annotated development set , we have an unbiased way of investigating the tree structures as well as the grammars that are inher-416 TreesPreterminal Rule Parsing Acc‚Ä†Acc‚Ä°F1 Gold * 66.1 46.2 BERT - base - cased 64.4 24.8 37.1 BERT - large - cased 64.0 22.3 38.7 XLNet - base - cased 67.7 26.1 42.7 XLNet - large - cased 65.8 27.3 41.1 RoBERTa - base 65.7 27.2 42.1 RoBERTa - large 62.4 25.1 37.5 Table 4 : Preterminal ( PoS tag ) and production rule accuracies of PCFG PLMand PCFG Goldon the entire PTB . 
‚Ä† : PoS tagging accuracy using the many - to - one mapping ( Johnson , 2007 ) . 
‚Ä° : production rule accuracy where anonymized nonterminals and preterminals are mapped to the gold tags using the many - to - one mapping . 
* : PCFG Gold . 
ent in PLMs . 
SpeciÔ¨Åcally , we Ô¨Årst parse the raw sentences using our ranking - based parser described in Section 3.2 and then train a neural PCFG given the induced trees using the method in Section 4.2 . 
We conduct our experiments on the English PTB and evaluate how the learned grammar resembles PTB syntax in a quantitative way on preterminals ( PoS tags ) and production rules . 
We visualize the alignment of preterminals and nonterminals of the learned grammar and the gold labels in Appendix A.2 as a qualitative study . 
We also showcase parse trees of the learned grammar to get a glimpse of some distinctive characteristics of the learned grammar in Appendix A.3 . 
For brevity , we refer to a neural PCFG learned from trees induced of a PLM as PCFG PLMand to a neural PCFG learned from the gold parse trees as PCFG Gold . 
In Table 4 , we report preterminal ( unsupervised PoS tagging ) accuracies and production rule accuracies of PCFG PLM and PCFG Goldon the corpus level . 
For preterminal evaluation , we map the anonymized preterminals to gold PoS tags using many - to - one ( M-1 ) mapping ( Johnson , 2007 ) , where each anonymized preterminal is matched onto the gold PoS tag with which it shares the most tokens . 
For production rule evaluation , we map both nonterminals and preterminals to gold tags using M-1 mapping to get the binary production rules.12We Ô¨Ånd that all PCFG PLMgrammars except for PCFG RoBERTa - large outperform a discrete HMM baseline ( 62.7 , He et al . 
2018 ) but are far from the state of the art for neural grammar induc12For the gold annotations , we drop all unary rules . 
For n - ary rules ( n > 2 ) , we convert them to binary rules by right branching and propagating the parent tag . 
For example , a n - ary rule A‚ÜíB C D yields A‚ÜíB A andA‚ÜíC D .tion ( 80.8 , He et al . 
2018 ) . 
All PCFG PLMproduce similar accuracies on preterminals as PCFG Gold . 
However , for the production rules , PCFG PLMlags behind PCFG Goldby a large margin . 
This makes sense as presumably the tree structures heavily affect nonterminal learning . 
We also present the parsingF1scores of corresponding trees against the gold trees in Table 4 for comparison . 
We observe that for all PCFG PLM , both preterminal accuracies and production rule accuracies correlate well with the parsingF1scores of the corresponding trees . 
6 Conclusion In this paper , we set out to analyze the syntactic knowledge learned by transformer - based pretrained language models . 
In contrast to previous work relying on test suites and probes , we proposed to use a zero - shot unsupervised parsing approach . 
This approach is able to parse sentences by ranking the attention heads of the PLM and ensembling them . 
Our approach is able to completely do away with a development set annotated with syntactic structures , which makes it ideal in a strictly unsupervised setting , e.g. , for low resource languages . 
We evaluated our method against previous methods on nine languages . 
When development sets are available for previous methods , our method can match them or produce competitive results if they use the top single head or layer - wise ensembling of attention heads , but lags behind them if they ensemble the top - Kheads . 
Furthermore , we present an analysis of the grammars learned by our approach : we use the induced trees to train a neural PCFG and evaluate the pre - terminal and non - terminal symbols of that grammar . 
In future work , we will develop further methods for analyzing the resulting grammar rules . 
Another avenue for follow - up research is to use our method to determine how the syntactic structures inherent in PLMs change when these models are Ô¨Åne - tuned on a speciÔ¨Åc task . 
Acknowledgments We thank the reviewers for their valuable suggestions regarding this work . 
A Appendix A.1 More Results on Languages other than English We present a comprehensive analysis of the chartbased parser and our ranking - based parser on the multilingual setting . 
In addition to Table 3 , for our method , we conduct experiments using target language for head selection with both Top- K(i.e . 
, top-30 ) ensemble and dynamic Kensemble . 
In Table 5 , we Ô¨Ånd that our ranking - based parser with Top - Kensemble performs slightly better than that using dynamic K. In contrast to the superiority of dynamicKon English PLMs in Table 1 , multilingual PLMs produce similar parsing performance with a lazy top-30 ensemble . 
We conjecture that there could be no clear concave pattern ( like Figure 1 ) in the relation of Kand parsing performance in this crosslingual setting . 
We also experimented with another setting for our ranking - based parser : selecting attention heads based on the sentences in the target language . 
Interestingly , we observe a considerable parsing performance drop on both top- Kand dynamic Kensemble . 
We suspect that our chart - based ranking algorithm ( e.g. , the inherent context free grammar assumption ) does not work equally well in all languages , at least for the annotation scheme provided by the SPMRL dataset . 
In this scenario , using English for head selection has a better chance to capture syntax - related attention heads . 
Again , as we discussed before , using annotated trees in the target language can always ensure the quality of selected top- Kheads . 
A.2 Visualization of the Alignment for Internal Tags Since the recall scores in Table 2 have shown ability of PLMs to identify different nonterminals , here we visualize the alignment between PCFG internal tags and corresponding gold labels in Figures 2 and 3 . 
For the nonterminal alignment , some of the learned nonterminals clearly align to gold standard labels , in particular for frequent ones like NP and VP . 
Compared to PCFG Gold , PCFG PLMlearns a more uncertain grammar and resulting in overall lower precision . 
But for the preterminal ( PoS tag ) alignment , no clear difference can be identiÔ¨Åed between PCFG Goldand PCFG PLM . 
This is consistent with the Ô¨Ånding in Table 4 that all PCFG PLMproduce similar accuracies on preterminals as PCFG Gold . 
A.3 Parse tree samples In Figure 4 , we show parse trees obtained by PCFG Gold , PCFG PLM and the gold standard reference on a sample sentence . 
In this sample , PCFG Goldpredicts the constituency tree structure accurately . 
On the development set , PCFG Gold reaches around 72 unlabeled F1score , as it is supervised by the PTB trees . 
Although this is a low F1 - score , it is not untypical for PCFG - based models , which are limited by their insufÔ¨Åciently Ô¨Çexible rules and their lack of lexicalization . 
Also note that the oracle trees only yield 84.3 F1 . 
PCFG PLMperform worse than PCFG Goldwhen compared against the gold tree . 
They are able to identify short NPs , but do n‚Äôt work well for larger constituents . 
We also observe some frequent incorrect patterns which are also present in this example , e.g. , grouping VBD with the preceding NP , or IN with the preceding VBD.420 Language English Basque French German Hebrew Hungarian Korean Polish Swedish A VG Trivial baselines Balanced 18.5 24.4 12.9 15.2 18.1 14.0 20.4 26.1 13.3 18.1 Left branching 8.7 14.8 5.4 14.1 7.7 10.6 16.5 28.7 7.6 12.7 Right branching 39.4 22.4 1.3 3.0 0.0 0.0 21.1 0.7 1.7 10.0Target language for head selectionChart - based ( Single / Layer)‚Ä† M - BERT 41.2 38.1 30.6 32.1 31.9 30.4 46.4 43.5 27.5 35.7 XLM 43.0 35.3 35.6 41.6 39.9 34.5 35.7 51.7 33.7 39.0 XLM - R 44.4 40.4 31.0 32.8 34.1 32.4 47.5 44.7 29.2 37.4 XLM - R - large 40.8 36.5 26.4 30.2 32.1 26.8 45.6 47.9 25.8 34.7 A VG 42.4 37.6 30.9 34.2 34.5 31.0 43.8 46.9 29.1 36.7 Chart - based ( Top- K)‚Ä† M - BERT 45.0 41.2 35.9 35.9 37.8 33.2 47.6 51.1 32.6 40.0 XLM 47.7 41.3 36.7 43.8 41.0 36.3 35.7 58.5 36.5 41.9 XLM - R 47.0 42.2 35.8 37.7 40.1 36.6 51.0 52.7 32.9 41.8 XLM - R - large 45.1 40.2 29.7 37.1 36.2 31.0 46.9 47.9 27.8 38.0 A VG 46.2 41.2 34.5 38.6 38.8 34.3 45.3 52.6 32.5 40.4 Ranking - based ( Top- K)‚Ä° M - BERT 41.5 38.9 33.9 30.2 36.3 30.9 39.0 18.4 26.3 31.7 XLM 44.6 21.0 29.8 39.2 30.5 25.2 23.8 55.2 30.3 31.9 XLM - R 44.8 36.0 34.1 31.8 36.4 32.5 40.3 29.6 26.7 33.4 XLM - R - large 41.1 36.8 30.3 26.8 33.4 24.9 37.4 17.5 26.3 29.2 A VG 43.0 33.2 32.0 32.0 34.2 28.4 35.1 30.2 27.4 31.6 Ranking - based ( Dynamic K)‚Ä° M - BERT 40.7 39.1 28.4 25.5 26.9 31.2 41.3 22.2 21.3 29.5 XLM 44.9 20.8 29.9 40.3 34.4 27.7 23.6 55.1 31.2 32.9 XLM - R 45.5 37.3 30.7 31.5 31.8 34.1 40.8 36.0 27.4 33.7 XLM - R - large 41.0 36.5 29.0 30.1 32.6 25.3 43.9 30.0 25.5 31.6 A VG 43.0 33.4 29.5 31.9 31.4 29.6 37.4 35.8 26.4 31.9English for head selectionCrosslingual ranking - based ( Top- K)‚Ä° M - BERT - 37.9 33.4 31.2 31.5 29.4 45.3 33.4 27.2 34.5 XLM - 25.9 34.4 39.2 39.5 31.9 27.5 50.4 34.2 36.4 XLM - R - 37.9 33.9 35.1 36.8 33.3 44.7 39.7 30.3 37.4 XLM - R - large - 35.7 28.5 28.5 34.7 25.5 44.5 36.9 27.1 33.6 A VG - 34.3 32.6 33.5 35.6 30.0 40.5 40.1 29.7 35.5 Crosslingual ranking - based ( Dynamic K)‚Ä° M - BERT - 38.2 31.0 31.0 29.0 27.1 43.3 30.7 25.8 33.0 XLM - 26.6 35.8 39.7 39.6 32.9 28.0 50.1 34.1 36.9 XLM - R - 38.2 34.0 35.5 36.7 33.5 45.2 39.4 29.9 37.6 XLM - R - large - 37.9 28.0 28.0 31.3 24.6 44.4 32.2 24.9 32.5 A VG - 34.7 32.4 33.5 35.0 29.8 40.4 39.2 29.2 35.3 Table 5 : Parsing results on nine languages with multilingual PLMs . 
Except for the trivial baselines , all experimental results are divided into two groups : using target language for head selection and using English for head selection ( crosslingual).‚Ä† : results of the best conÔ¨Ågurations of f , g , scomp andKare decided on an annotated development set.‚Ä° : results where only raw sentences are required . 
For top- K , 20 is used for chart - based and 30 is used for our ranking - based . 
Bold Ô¨Ågures highlight the best scores for the two different groups : using target language and English for head selection.421 S SBARNP VP PP ADJP ADVPOthersprecNT- 1 NT- 2 NT- 3 NT- 4 NT- 5 NT- 6 NT- 7 NT- 8 NT- 9 NT-10 NT-11 NT-12 NT-13 NT-14 NT-15 NT-16 NT-17 NT-18 NT-19 NT-20 NT-21 NT-22 NT-23 NT-24 NT-25 NT-26 NT-27 NT-28 NT-29 NT-30(a ) PCFG Gold S SBARNP VP PP ADJP ADVPOthersprecNT- 1 NT- 2 NT- 3 NT- 4 NT- 5 NT- 6 NT- 7 NT- 8 NT- 9 NT-10 NT-11 NT-12 NT-13 NT-14 NT-15 NT-16 NT-17 NT-18 NT-19 NT-20 NT-21 NT-22 NT-23 NT-24 NT-25 NT-26 NT-27 NT-28 NT-29 NT-30 ( b ) PCFG BERT - base - cased S SBARNP VP PP ADJP ADVPOthersprecNT- 1 NT- 2 NT- 3 NT- 4 NT- 5 NT- 6 NT- 7 NT- 8 NT- 9 NT-10 NT-11 NT-12 NT-13 NT-14 NT-15 NT-16 NT-17 NT-18 NT-19 NT-20 NT-21 NT-22 NT-23 NT-24 NT-25 NT-26 NT-27 NT-28 NT-29 NT-30 ( c ) PCFG BERT - large - cased S SBARNP VP PP ADJP ADVPOthersprecNT- 1 NT- 2 NT- 3 NT- 4 NT- 5 NT- 6 NT- 7 NT- 8 NT- 9 NT-10 NT-11 NT-12 NT-13 NT-14 NT-15 NT-16 NT-17 NT-18 NT-19 NT-20 NT-21 NT-22 NT-23 NT-24 NT-25 NT-26 NT-27 NT-28 NT-29 NT-30 ( d ) PCFG XLNet - base - cased S SBARNP VP PP ADJP ADVPOthersprecNT- 1 NT- 2 NT- 3 NT- 4 NT- 5 NT- 6 NT- 7 NT- 8 NT- 9 NT-10 NT-11 NT-12 NT-13 NT-14 NT-15 NT-16 NT-17 NT-18 NT-19 NT-20 NT-21 NT-22 NT-23 NT-24 NT-25 NT-26 NT-27 NT-28 NT-29 NT-30 ( e ) PCFG XLNet - large - cased S SBARNP VP PP ADJP ADVPOthersprecNT- 1 NT- 2 NT- 3 NT- 4 NT- 5 NT- 6 NT- 7 NT- 8 NT- 9 NT-10 NT-11 NT-12 NT-13 NT-14 NT-15 NT-16 NT-17 NT-18 NT-19 NT-20 NT-21 NT-22 NT-23 NT-24 NT-25 NT-26 NT-27 NT-28 NT-29 NT-30 ( f ) PCFG RoBERTa - base S SBARNP VP PP ADJP ADVPOthersprecNT- 1 NT- 2 NT- 3 NT- 4 NT- 5 NT- 6 NT- 7 NT- 8 NT- 9 NT-10 NT-11 NT-12 NT-13 NT-14 NT-15 NT-16 NT-17 NT-18 NT-19 NT-20 NT-21 NT-22 NT-23 NT-24 NT-25 NT-26 NT-27 NT-28 NT-29 NT-30 ( g ) PCFG RoBERTa - large Figure 2 : Alignment of induced nonterminals of PCFG PLMand PCFG Goldon the entire PTB . 
The last column prec shows the precision that a nonterminal predicts a particular gold constituent.422 DTJJ NNS VBDNN CC RB IN JJSNNPCD TO JJRVBG POS VBP VBN RBR WRBPRPPRP$ WDTEX MD VBVBZNNPSWP RPPDT WP$ RBSFW UHSYMLST- 1 T- 2 T- 3 T- 4 T- 5 T- 6 T- 7 T- 8 T- 9 T-10 T-11 T-12 T-13 T-14 T-15 T-16 T-17 T-18 T-19 T-20 T-21 T-22 T-23 T-24 T-25 T-26 T-27 T-28 T-29 T-30 T-31 T-32 T-33 T-34 T-35 T-36 T-37 T-38 T-39 T-40 T-41 T-42 T-43 T-44 T-45 T-46 T-47 T-48 T-49 T-50 T-51 T-52 T-53 T-54 T-55 T-56 T-57 T-58 T-59 T-60(a ) PCFG Gold DTJJ NNS VBDNN CC RB IN JJSNNPCD TO JJRVBG POS VBP VBN RBR WRBPRPPRP$ WDTEX MD VBVBZNNPSWP RPPDT WP$ RBSFW UHSYMLST- 1 T- 2 T- 3 T- 4 T- 5 T- 6 T- 7 T- 8 T- 9 T-10 T-11 T-12 T-13 T-14 T-15 T-16 T-17 T-18 T-19 T-20 T-21 T-22 T-23 T-24 T-25 T-26 T-27 T-28 T-29 T-30 T-31 T-32 T-33 T-34 T-35 T-36 T-37 T-38 T-39 T-40 T-41 T-42 T-43 T-44 T-45 T-46 T-47 T-48 T-49 T-50 T-51 T-52 T-53 T-54 T-55 T-56 T-57 T-58 T-59 T-60 ( b ) PCFG BERT - base - cased DTJJ NNS VBDNN CC RB IN JJSNNPCD TO JJRVBG POS VBP VBN RBR WRBPRPPRP$ WDTEX MD VBVBZNNPSWP RPPDT WP$ RBSFW UHSYMLST- 1 T- 2 T- 3 T- 4 T- 5 T- 6 T- 7 T- 8 T- 9 T-10 T-11 T-12 T-13 T-14 T-15 T-16 T-17 T-18 T-19 T-20 T-21 T-22 T-23 T-24 T-25 T-26 T-27 T-28 T-29 T-30 T-31 T-32 T-33 T-34 T-35 T-36 T-37 T-38 T-39 T-40 T-41 T-42 T-43 T-44 T-45 T-46 T-47 T-48 T-49 T-50 T-51 T-52 T-53 T-54 T-55 T-56 T-57 T-58 T-59 T-60 ( c ) PCFG BERT - large - cased DTJJ NNS VBDNN CC RB IN JJSNNPCD TO JJRVBG POS VBP VBN RBR WRBPRPPRP$ WDTEX MD VBVBZNNPSWP RPPDT WP$ RBSFW UHSYMLST- 1 T- 2 T- 3 T- 4 T- 5 T- 6 T- 7 T- 8 T- 9 T-10 T-11 T-12 T-13 T-14 T-15 T-16 T-17 T-18 T-19 T-20 T-21 T-22 T-23 T-24 T-25 T-26 T-27 T-28 T-29 T-30 T-31 T-32 T-33 T-34 T-35 T-36 T-37 T-38 T-39 T-40 T-41 T-42 T-43 T-44 T-45 T-46 T-47 T-48 T-49 T-50 T-51 T-52 T-53 T-54 T-55 T-56 T-57 T-58 T-59 T-60 ( d ) PCFG XLNet - base - cased DTJJ NNS VBDNN CC RB IN JJSNNPCD TO JJRVBG POS VBP VBN RBR WRBPRPPRP$ WDTEX MD VBVBZNNPSWP RPPDT WP$ RBSFW UHSYMLST- 1 T- 2 T- 3 T- 4 T- 5 T- 6 T- 7 T- 8 T- 9 T-10 T-11 T-12 T-13 T-14 T-15 T-16 T-17 T-18 T-19 T-20 T-21 T-22 T-23 T-24 T-25 T-26 T-27 T-28 T-29 T-30 T-31 T-32 T-33 T-34 T-35 T-36 T-37 T-38 T-39 T-40 T-41 T-42 T-43 T-44 T-45 T-46 T-47 T-48 T-49 T-50 T-51 T-52 T-53 T-54 T-55 T-56 T-57 T-58 T-59 T-60 ( e ) PCFG XLNet - large - cased DTJJ NNS VBDNN CC RB IN JJSNNPCD TO JJRVBG POS VBP VBN RBR WRBPRPPRP$ WDTEX MD VBVBZNNPSWP RPPDT WP$ RBSFW UHSYMLST- 1 T- 2 T- 3 T- 4 T- 5 T- 6 T- 7 T- 8 T- 9 T-10 T-11 T-12 T-13 T-14 T-15 T-16 T-17 T-18 T-19 T-20 T-21 T-22 T-23 T-24 T-25 T-26 T-27 T-28 T-29 T-30 T-31 T-32 T-33 T-34 T-35 T-36 T-37 T-38 T-39 T-40 T-41 T-42 T-43 T-44 T-45 T-46 T-47 T-48 T-49 T-50 T-51 T-52 T-53 T-54 T-55 T-56 T-57 T-58 T-59 T-60 ( f ) PCFG RoBERTa - base DTJJ NNS VBDNN CC RB IN JJSNNPCD TO JJRVBG POS VBP VBN RBR WRBPRPPRP$ WDTEX MD VBVBZNNPSWP RPPDT WP$ RBSFW UHSYMLST- 1 T- 2 T- 3 T- 4 T- 5 T- 6 T- 7 T- 8 T- 9 T-10 T-11 T-12 T-13 T-14 T-15 T-16 T-17 T-18 T-19 T-20 T-21 T-22 T-23 T-24 T-25 T-26 T-27 T-28 T-29 T-30 T-31 T-32 T-33 T-34 T-35 T-36 T-37 T-38 T-39 T-40 T-41 T-42 T-43 T-44 T-45 T-46 T-47 T-48 T-49 T-50 T-51 T-52 T-53 T-54 T-55 T-56 T-57 T-58 T-59 T-60 ( g ) PCFG RoBERTa - large Figure 3 : Alignment of induced preterminals ( PoS tags ) of PCFG PLMand PCFG Goldon the entire PTB.423 Gold standard PCFG Gold S VP SBAR - TMP S VP PP - CLR NP NNS currenciesJJ majorJJS mostIN againstVBD weakenedNP - SBJ NN dollarDT theIN asVBD surgedNP - SBJ NNS bondsJJ ForeignNT-1 [ S ] NT-29 [ VP ] NT-30 [ PP ] NT-1 [ S ] NT-29 [ VP ] NT-30 [ PP ] NT-17 [ NP ] NT-10 [ NP ] T-12 [ NNS ] currenciesT-41 [ NN ] majorT-37 [ JJ ] mostT-57 [ IN ] againstT-25 [ VBD ] weakenedNT-23 [ NP ] T-27 [ NN ] dollarT-17 [ DT ] theT-38 [ IN ] asT-25 [ VBD ] surgedNT-23 [ NP ] T-47 [ NNS ] bondsT-42 [ JJ ] Foreign PCFG BERT - base - cased PCFG BERT - large - cased NT-21 [ S ] NT-1 [ VP ] NT-13 [ NP ] T-33 [ NN ] currenciesT-53 [ JJ ] majorNT-5 [ PP ] T-23 [ DT ] mostT-28 [ IN ] againstNT-30 [ NP ] NT-9 [ NP ] T-55 [ RB ] weakenedNT-28 [ NP ] T-22 [ NN ] dollarT-23 [ DT ] theNT-4 [ NP ] T-26 [ TO ] asNT-15 [ S ] T-15 [ VBD ] surgedNT-28 [ NP ] T-40 [ NNS ] bondsT-56 [ JJ ] foreignNT-1 [ S ] NT-27 [ VP ] NT-14 [ NP ] NT-8 [ NP ] T-49 [ NNS ] currenciesT-3 [ JJ ] majorT-11 [ JJ ] mostNT-26 [ NP ] T-26 [ IN ] againstT-4 [ VBD ] weakenedNT-12 [ NP ] NT-25 [ PP ] NT-15 [ NP ] T-53 [ NN ] dollarT-23 [ DT ] theT-26 [ IN ] asNT-21 [ S ] T-4 [ VBD ] surgedNT-15 [ NP ] T-49 [ NNS ] bondsT-3 [ JJ ] foreign PCFG XLNet - base - cased PCFG XLNet - large - cased NT-21 [ S ] NT-1 [ VP ] NT-12 [ PP ] NT-14 [ NP ] NT-13 [ NP ] T-16 [ NNS ] currenciesT-17 [ JJ ] majorT-50 [ JJ ] mostT-26 [ IN ] againstNT-4 [ S ] T-58 [ VBD ] weakenedNT-3 [ NP ] NT-7 [ NP ] T-22 [ NN ] dollarT-23 [ DT ] theT-24 [ IN ] asNT-30 [ NP ] T-58 [ VBD ] surgedNT-3 [ NP ] T-16 [ NNS ] bondsT-17 [ JJ ] foreignNT-1 [ S ] NT-27 [ VP ] NT-4 [ NP ] NT-23 [ NP ] T-16 [ NN ] currenciesT-9 [ JJ ] majorT-23 [ DT ] mostNT-26 [ NP ] T-7 [ IN ] againstT-54 [ VBD ] weakenedNT-12 [ NP ] NT-11 [ PP ] NT-28 [ NP ] T-49 [ NN ] dollarT-23 [ DT ] theNT-26 [ NP ] T-7 [ IN ] asT-54 [ VBD ] surgedNT-9 [ NP ] T-47 [ NNS ] bondsT-45 [ JJ ] foreign PCFG RoBERTa - base PCFG RoBERTa - large NT-11 [ S ] NT-27 [ VP ] NT-22 [ NP ] NT-19 [ NP ] T-47 [ NNS ] currenciesT-41 [ JJ ] majorT-41 [ JJ ] mostNT-10 [ VP ] T-26 [ IN ] againstT-30 [ VBD ] weakenedNT-16 [ NP ] NT-18 [ PP ] NT-15 [ NP ] T-22 [ NN ] dollarT-23 [ DT ] theT-26 [ IN ] asNT-9 [ S ] T-30 [ VBD ] surgedNT-19 [ NP ] T-36 [ NNS ] bondsT-51 [ NN ] foreignNT-21 [ S ] NT-1 [ VP ] NT-18 [ NP ] T-4 [ NN ] currenciesT-50 [ JJ ] majorNT-7 [ VP ] T-14 [ PRP$ ] mostT-26 [ IN ] againstNT-30 [ NP ] NT-23 [ NP ] NT-13 [ NP ] T-53 [ NN ] weakenedT-39 [ NN ] dollarT-23 [ DT ] theNT-4 [ PP ] T-57 [ IN ] asNT-15 [ S ] T-7 [ VBD ] surgedNT-13 [ NP ] T-22 [ NN ] bondsT-9 [ JJ ] foreign Figure 4 : Parse tree samples of gold standard , PCFG Gold , and PCFG PLM . 
The mapped tag ( marked in red ) for each anonymized nonterminal and preterminal is obtained via many - to - one mapping.424 Proceedings of the 1st Conference of the Asia - PaciÔ¨Åc Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing , pages 425‚Äì434 December 4 - 7 , 2020 . 
¬© 2020 Association for Computational Linguistics Building Location Embeddings from Physical Trajectories and Textual Representations Laura Biester , Carmen Banea , Rada Mihalcea Computer Science & Engineering , University of Michigan , USA { lbiester , carmennb , mihalcea } @umich.edu Abstract Word embedding methods have become the defacto way to represent words , having been successfully applied to a wide array of natural language processing tasks . 
In this paper , we explore the hypothesis that embedding methods can also be effectively used to represent spatial locations . 
Using a new dataset consisting of the location trajectories of 729 students over a seven month period and text data related to those locations , we implement several strategies to create location embeddings , which we then use to create embeddings of the sequences of locations a student has visited . 
To identify the surface level properties captured in the representations , we propose a number of probing tasks such as the presence of a speciÔ¨Åc location in a sequence or the type of activities that take place at a location . 
We then leverage the representations we generated and employ them in more complex downstream tasks ranging from predicting a student ‚Äôs area of study to a student ‚Äôs depression level , showing the effectiveness of these location embeddings . 
1 Introduction Due to the rising adoption of smartphones over the past decade , the number of services with full or partial information about people ‚Äôs spatial mobility has skyrocketed . 
Inspired by the natural language processing ( NLP ) literature , we investigate various properties of location embeddings . 
We explore whether valuable information is encoded in individual location embeddings , as well as embeddings that encompass a sequence of locations . 
We begin by exploring whether they are able to represent aspects such as location presence or location functionality . 
Ultimately , we test the hypothesis that if enough underlying information is encoded , embedding models should aid in predicting user - centered descriptors , such as area of study , academic status , or mental health . 
Location data can be used by university administrators for applications that improve student life . 
From the frequency and the type of locations accessed in one ‚Äôs daily routine , we may be able to identify someone who is depressed or someone who is overworked . 
Importantly , opt - in frameworks can be established to supplement existing counseling and advising ofÔ¨Åces , allowing for early intervention in the case of mental health and academic concerns . 
With proper privacy safeguards in place , such models could readily be applied on most university campuses , as WiFi connection data ( from which we infer location ) is likely already available . 
In addition , universities could use this data in an aggregate form to better understand student life and well - being , and Ô¨Ånd ways to promote healthy and engaging behaviors on campus . 
Such aggregate location information can also be used by architectural Ô¨Årms or municipalities to help with the selection of buildings ‚Äô locations , architecture , and design ; with road and pedestrian trafÔ¨Åc optimization ; or for emergency response . 
We also know that such data is already available to large technology companies that track their users , and it is important to spread awareness about the personal information that can be gleaned . 
Research like ours helps inform users about privacy concerns , and may open up a path to stricter legislation regarding the use of such data in the future . 
While we envision numerous positive applications of these methods , there are clear privacy drawbacks that the public should be aware of in the current technological environment . 
Our work focuses on building an understanding of what information is encoded in location embeddings . 
In addition to creating embeddings using location trajectories , we propose an alternative method that synthesizes text from online sources to build representations that we hypothesize will better encode certain properties of locations . 
We425 show that using dense location embeddings that incorporate both movement patterns and text data improves our ability to model downstream tasks . 
We see that although we are not able to recover as much surface level information from embeddings of location sequences as we are from a simpler representation , the additional semantic information that is encoded allows us to better predict some user attributes . 
2 Related Work Embedding Evaluation and Probing . 
Word embeddings are now widely used to create word representations using methods such as word2vec ( Mikolov et al . 
, 2013 ) , GloVe ( Pennington et al . 
, 2014 ) , ELMo ( Peters et al . 
, 2018 ) , and BERT ( Devlin et al . 
, 2019 ) . 
BERT and ELMo can be used to create contextualized word embeddings , in which the vector representing an individual word varies depending on the context in which it appears . 
Previous methods including word2vec and GloVe did not make this distinction ; adding context helped BERT achieve state - of - the - art results on many downstream NLP tasks . 
One traditional benchmark for word embeddings is performance on synthetic tasks , such as word similarity and word analogy tasks ( Mikolov et al . 
, 2013 ; Pennington et al . 
, 2014 ) . 
However , word embeddings are widely used because of their superior performance on a variety of downstream NLP tasks when compared to other word representations . 
Performance on downstream tasks has been used to evaluate sentence embeddings , however such approaches can not gauge the content that is actually captured in the embeddings . 
To systematically ascertain what information is encoded in sentence vectors , researchers have turned to probing tasks ( Shi et al . 
, 2016 ; Adi et al . 
, 2017 ; Conneau et al . 
, 2018 ) . 
These are meant to address the question ‚Äú what information is encoded in a sentence vector ‚Äù at a higher level . 
In our work , we Ô¨Ånd inspiration in the research by Conneau et al . 
( 2018 ) , who propose a formalized evaluation technique for sentence embeddings using a suite of ten classiÔ¨Åcation tasks focusing on : ( 1 ) surface information ( e.g. , length , word content ) , ( 2 ) syntactic information ( e.g. , bigram shift , tree depth ) , and ( 3 ) semantic information ( e.g. , tense ) . 
The deep learning methods gave the best results overall , but the bag - of - vectors approach was a solid baseline for the word content task , where it outper - formed the deep learning models . 
Applications of Embeddings for Location Data . 
Liu et al . 
( 2016 ) were among the Ô¨Årst to use the skip - gram model on location data . 
They use locations visited before and after a target location as context to create location embeddings . 
These are then used in a personalized location recommendation system . 
Feng et al . 
( 2017 ) similarly create embeddings of check - in data , but use the CBOW model . 
Their application task is reversed , predicting future visitors for a location instead of predicting locations that a user will visit . 
Chang et al . 
( 2018 ) also predict next check - ins for users using a model based on skip - gram . 
Their work is uniquely related to ours in that they also build prediction of the text content of check - ins into the objective function . 
Zhu et al . 
( 2019 ) trained a skip - gram model to build location embeddings , and use them to understand the Ô¨Çow between urban locations . 
Crivellari and Beinat ( 2019 ) explore location embeddings from the perspective of geoinformatics , paving the way for our probing tasks . 
The work of Solomon et al . 
( 2018 ) is most similar to our own . 
They use GPS data from cell phones as input to create embeddings and use data from a university setting . 
Our work differs in that we use the skip - gram model and incorporate text - based embeddings . 
We also propose probing tasks to better understand the embeddings that we create , and predict additional user attributes from our new dataset that go beyond demographic information . 
3 Data 3.1 Student and Location Data Our dataset consists of location data collected from 729 undergraduate university students who agreed to participate in our study in 2018 and 2019 over a period of seven months.1Two - thirds of the students participated during the winter semester , and the other third during the fall semester . 
Dataset statistics are presented in Table 1 . 
Due to the sensitivity and scope of the data , it is infeasible for our study to include other universities ; nonetheless , we believe that similar patterns would hold on other campuses as well . 
Because of privacy concerns , we are not able to publicly release this dataset . 
1The data was collected as part of a study that underwent a full board review and was approved by the IRB at the University of Michigan ( study number HUM00126298 ) . 
All participants in the study have signed an informed consent form.426 Number of Participants 729 Valid Location Visits After Pre - Processing 478,329 Unique Locations 194 Mean Locations per Participant 656.2 Mean Locations per Day 4.7 Table 1 : Statistical summary of the location dataset . 
While most similar research uses GPS ( Solomon et al . 
, 2018 ) , mobile check - ins ( Feng et al . 
, 2017 ; Liu et al . 
, 2016 ) , or cell phone pings ( Zhu et al . 
, 2019 ) for location tracking , we collect location data from WiFi access logs . 
WiFi access logs provide a strong and unbiased location signal on campus , as most students carry their smart phones with them at all times ; however , a downside is that we do not have location data for large time chunks when students are not connected to the campus WiFi . 
The original data consists of 20,766,750 WiFi session updates across all the students . 
We only consider connections with uninterrupted updates from a single building ( without a connection to a network in another building ) for at least ten minutes . 
This ensures that a student ‚Äôs location will not be mapped to multiple points during overlapping time spans , and that locations where a student does not spend a notable amount of time are excluded . 
After collecting this list of locations , start , and stop times , we perform a merging operation on the data , sorted by start time . 
If spans for the same location occur consecutively in the series with start and stop times less than 30 minutes apart , those spans are merged together . 
After this pre - processing , we are left with 478,329 valid location spans with start and stop times . 
Since our dataset covers a single campus ( 194 locations ) , each location was manually labeled with its functionality , for a total of thirteen functionalities . 
The Ô¨Åve most frequent are : class , study , dorm , lab , and library . 
While there are 194 locations in the location dataset , we utilize 132 in our analysis because this set of locations appears in all of the text - based datasets ( described in Section 3.2 ) ; the ones that are left out are not among the most frequently visited . 
In addition to location data , we collected a rich dataset containing information about the 729 students , consisting of a series of extensive surveys taken by the students throughout the semester and academic data from the registrar . 
From the survey data , we use information on class year , gender , depression , and sleep satisfaction . 
From theCampus Dataset Website Reddit Twitter Overall Tokens 581 K 882 K 655 K Unique Tokens GloVe 9 K 11 K 18 K Median Instances Per Loc . 
3.5 20.0 166.5 Start Date ( year - month ) N / A 2011 - 05 2010 - 09 End Date ( year - month ) 2019 - 05 2019 - 07 2019 - 08 Table 2 : Statistical information about text datasets . 
academic data , we utilize the GPA and the school where the student is enrolled . 
These combined data sources are used for our downstream classiÔ¨Åcation tasks . 
We chose students for the study covering all undergraduate class years , genders , and academic disciplines . 
3.2 Text Data In addition to location trajectories , we use text data from three sources ( campus website , Reddit , Twitter ) that illustrate various ways in which text can be used to represent places . 
Statistics of the text datasets are shown in Table 2 . 
Campus Website . 
With this dataset , we capture how people formally deÔ¨Åne locations . 
The university hosts a building search website that links to pages containing information about campus buildings , including the departments hosted inside . 
We manually link the locations in our dataset with building pages on this site , then scrape the Ô¨Årst Google search result constrained within the university domain for each listed department , and use that text to represent the location . 
In addition to the departments , some pages directly link to a website ( e.g. , a gym links to recreational sports ) , from which we also scrape text . 
Reddit . 
With this dataset , we capture how people informally discuss locations . 
From the university Reddit page , we search for building names . 
We increase the search term list using OpenStreetMap,2 which lists alternate names for many buildings . 
We include text from posts and comments that speciÔ¨Åcally mention a building . 
Twitter . 
With this dataset , we capture how people express themselves in various locations . 
We collect tweets that have been geotagged with GPS points within 0.05 kilometers of campus buildings . 
2https://www.openstreetmap.org/427 4 Representing Locations We use location trajectories and text data to create vector representations of locations and , subsequently , embeddings of sequences of locations that are visited by a single person . 
After pre - processing using the method described in Section 3.1 , the location input data consists of a series of sorted , nonoverlapping locations for a number of users with start and end times . 
We discuss multiple methods to create vector representations based on this data . 
4.1 Location Trajectory - Based Representations To create embeddings of locations , we make use of the temporal nature of the location trajectories to create a sequence of names of locations visited by a user over a period of time ( e.g. , the seven month period of our data collection , see Section 3.1 ) . 
A skip - gram model is trained to use a location to predict locations around it in a user ‚Äôs schedule , creating location embeddings that we expect will encode semantic information about locations.3 We represent each hour during the data collection period as a distinct token in the input trajectories . 
If a user has visited a single location in one hour , that location will be used in the slot for the hour ; if they visited multiple locations , their predominant location will be used . 
If we do not have any location data for the user during that hour , we use the EXTERNAL token . 
This approach gives an exact meaning to the distance between locations in a sequence , while a raw sequence would ignore gaps in the data . 
The approach of using one token per set time interval is also used in Zhu et al . 
( 2019 ) . 
We refer to the method as Loc2V , and show a visualization in Figure 1 . 
4.2 Text - Based Representations In addition to creating location representations from trajectories in the physical world , we explore the idea of using relevant text to deÔ¨Åne locations . 
Such text can reveal information about locations that may not be discernible from location trajectories , e.g. that people meet friends in a certain place . 
Therefore , for the same locations that appear in 3We use the default window size of 5 and generate embeddings with 25 dimensions . 
While 25 dimensions is fairly small in the context of word embeddings , since our dataset has fewer than two hundred locations that we seek to embed , higher values can not be considered as leading to a dimensionality reduction . 
We use a negative sampling value of 20 , as is suggested by Mikolov et al . 
( 2013 ) for small datasets . 
Figure 1 : A sample sequence of locations , and the corresponding sequences that are used as Loc2V input . 
our trajectories , we collect textual data that enables us to derive text - based representations from three sources as described in Section 3.2 . 
Using each textual data source , we map a location name to a set of relevant words . 
We calculate tf.idf ( Salton and Buckley , 1988 ) weights for each word , then use those weights to compute a weighted average of pre - trained word embeddings . 
Because our datasets are primarily from social media , we use pre - trained GloVe embeddings that were obtained from Twitter data.4The resulting vector is used as a location representation . 
4.3 Combining Representations We hypothesize that trajectory based and text - based representations may encode different aspects of locations . 
Therefore , in addition to representing locations using text and physical trajectories , we experiment with combining the two . 
Our Ô¨Årst method concatenates embedding vectors created from physical trajectories and vectors created from text data . 
Our second method performs retroÔ¨Åtting on top of text - based vectors . 
In the context of embeddings , ‚Äú retroÔ¨Åtting ‚Äù describes the process of modifying vectors that have already been created to better encode additional criteria . 
We Ô¨Ånd inspiration in the method from Faruqui et al . 
( 2015 ) , which retroÔ¨Åts word embeddings to a graph representing a semantic lexicon . 
In our work , we retroÔ¨Åt text embeddings to the graph that represents the transitions between locations ; the nodes are locations , and the edges are weighted by the number of times there was a transition between those two locations in our dataset . 
The retroÔ¨Åtting method takes a matrix ÀÜQ , the initial vectors , and updates matrix Q(initialized to ÀÜQ ) using a location transition graph . 
The objective 4https://nlp.stanford.edu/projects/glove/428 Figure 2 : Comparison of the concatenation and retroÔ¨Åtting methods . 
function incorporates the set of edges E , bringing vectors that share an edge closer together in the vector space : Œ®(Q ) = /summationtextn i=1 / bracketleftBig Œ±i / bardblqi‚àíÀÜqi / bardbl2+/summationtext ( i , j)‚ààEŒ≤ij / bardblqi‚àíqj / bardbl2 / bracketrightBig An iterative method is used to update matrix Q : qi=/summationtext j:(i , j)‚ààEŒ≤ijqj+Œ±iÀÜqi / summationtext j:(i , j)‚ààEŒ≤ij+Œ±i We perform ten iterations , as was done in previous work . 
The parameters Œ±andŒ≤control the relative importance of the two components ( initial vectors and location graph ) . 
In their implementation , Faruqui et al . 
set Œ±i= 1andŒ≤ij = degree ( i)‚àí1 . 
As the graph we use is weighted , we introduce a weighted version that incorporates edge weights W , using a weighted inverse degree for Œ≤ . 
The retroÔ¨Åtting method enhances the text - based information by adding the assumption that locations that are visited sequentially are similar ( in the sense that a person who visits one would visit the other ) , bringing them closer in the vector space . 
This method aims to infuse the text - based representations with information related to the cooccurrence of locations in a student ‚Äôs trajectory ; locations that co - occur may be suggestive of , for instance , areas of campus that tend to be visited by engineering students . 
It is not used on the trajectorybased representations , as these already incorporate location transitions . 
Figure 2 compares the concatenation and retroÔ¨Åtting methods . 
As outlined above , the concatenation method directly combines the two vectors into one with the same content , while the retroÔ¨Åtting method takes information from a graph structure representing trajectories into account to create a modiÔ¨Åed version of the original vector . 
Figure 3 : Fictional examples of locations visited by students ; a larger pin reÔ¨Çects more time spent at a location . 
4.4 Representing a Sequence of Locations To represent a sequence of locations , we use a vector representing the locations that a person has visited in a month , instead of the individual locations . 
We settled on this time interval since a shorter time span ( such as a day ) contains very little predictive information , while a longer span ( one semester ) groups together distinct time spans that may lead to divergent behaviors , such as exam periods . 
We create a sequence embedding by taking a weighted average of the location vectors included in the sequence , using the time spent at each location as weights , thus increasing the importance of locations at which the person spent more time . 
5 Probing Location Representations While some of the methods we use ( i.e. , skip - gram ) have been used in the past to represent locations for certain tasks , there has been less work studying them intrinsically . 
We propose surface level tasks to probe the properties encoded in location embeddings , which are important to gain a deeper understanding of the type of information they capture . 
We split surface level tasks into two categories : those that focus on individual locations and those that focus on location sequences . 
In addition to these surface level tasks , we propose a set of downstream prediction tasks to validate the utility of such embeddings . 
5.1 Surface Level Location Tasks With these tasks , we examine two properties that should be encoded in location representations : location functionality and physical proximity . 
To directly compare how well each method encodes these semantic properties , we propose a metric to measure each property . 
We are inspired by Ye and Skiena ( 2019 ) , who use similar methods to analyze properties of name embeddings ( representations of429 people ‚Äôs names ) . 
We borrow their method of analysis , measuring overlaps in the Nnearest neighbors for various values of N , but they analyze a different property , namely the gender associated with the name . 
Functionality Overlap . 
Each location in our dataset is annotated with its functionality , including two functionalities for mixed - use buildings , e.g. , a class building that also contains labs . 
For each location , we calculate the percentage of its nearest neighbors in the vector space that share at least one functionality ; a higher value indicates that the embeddings more distinctly capture functionality . 
We compute nearest neighbors using cosine similarity . 
Physical Distance . 
We compute the distance in kilometers between a location and its nearest neighbors , and average the distances . 
This allows us to measure exactly how far a location is from its nearest neighbors ; a lower number for this metric correlates with an increased physical proximity . 
5.2 Surface Level Sequence Tasks Our surface level sequence tasks are inspired by the methodology proposed by Conneau et al . 
( 2018 ) to probe sentence embeddings . 
Many of those tasks focus on syntax , which is not relevant for our use case , but we adapt their task for location - presence and propose probing for functionality - presence . 
Location Presence . 
We propose a binary locationpresence classiÔ¨Åcation task ( LocPres ) . 
We create classiÔ¨Åers for each location , predicting if the location appears in a sequence . 
We average the results across all locations with at least one hundred positive and negative examples ( resulting in being able to assess 83 locations out of 132 ) . 
Functionality Presence . 
We also propose a functionality - presence task ( FuncPres ) . 
Given a sequence embedding , we predict if it includes locations of a certain functionality . 
We use a binary classiÔ¨Åcation setup that mirrors the one used for the location - presence task . 
We treat the classiÔ¨Åcation of either the primary or secondary functionalities assigned to locations as correct . 
As with the location - presence task , we average results over all functionalities with at least one hundred training instances from each class ( accounting for 11 functionalities out of 13 ) . 
5.3 Downstream Application - Based Tasks In addition to surface level tasks , we want to understand what other human - centric information isencoded in location sequence embeddings . 
Our hypothesis is that the way in which students spend their time may be indicative of certain information about them ; an example of students ‚Äô diverse behavior on campus is shown in Figure 3 . 
Using the dataset described in Section 3.1 , we propose seven classiÔ¨Åcation tasks : Ô¨Åve tasks with two classes ( major depression , all depression , gender , sleep satisfaction , and GPA ) , one task with three classes ( to predict which school a student is enrolled in , e.g. business or engineering ) , and one task with four classes ( to predict class year ) . 
Sleep satisfaction is reported in a survey ( Section 3.1 ) on a Ô¨Åve - point Likert scale ; the top three responses are mapped to a positive class , and the bottom two to a negative class . 
As semester GPA is continuous , we formulate the binary classiÔ¨Åcation as less than or greater than 3.5 ( between Aand B+ ) . 
Depression is measured using the standard PHQ-8 survey ; using a clinically validated algorithm ( Kroenke et al . 
, 2001 ) , we classify major depression ( binary ) , along with major and other depression ( a weaker diagnosis ) ; we label the former as ‚Äú major depression ‚Äù and the latter as ‚Äú all depression . 
‚Äù For the other tasks , we Ô¨Ålter out underpopulated classes , going from 18 to three classes for school , from Ô¨Åve to two for gender , and from Ô¨Åve to four for class year . 
We use a classiÔ¨Åcation approach over regression because we hope that this work can be used to identify at - risk students . 
6 Experimental Setup We perform 10 - fold cross validation on 729 instances , where each instance represents a student . 
Preliminary classiÔ¨Åcation experiments were conducted on a small subset using SVM with linear and RBF kernels , random forests , decision trees , and Na ¬®ƒ±ve Bayes , yet linear SVM had the most robust performance . 
Accordingly , our experiments consist of classiÔ¨Åcation tasks using linear SVM . 
As many of the classes are unbalanced , we more heavily weight updates for the minority class(es ) by modifying the loss function to use a weight that is inversely proportional to the class ‚Äôs prevalence . 
To predict a student attribute , we create one vector for each month of data collection pertaining to each student , using the process described in Section 4.4 . 
Our training framework is illustrated in Figure 4 . 
We start by feeding the sequence vectors through a SVM classiÔ¨Åer , which predicts monthlevel labels . 
These are then concatenated to form a430 L4L9L7L1L1L7 L9L4locationvectorsmonthsequencevectorsLabel 1 SVMLabel 2meta classifierSVMJanFebJanFebPredicted Label ‚Ä¶ ‚Ä¶ Œ£Œ£Figure 4 : The framework for downstream prediction tasks . 
student instance and are passed to a meta - classiÔ¨Åer that decides the Ô¨Ånal class label for that student . 
We use the meta - classiÔ¨Åcation approach to allow the Ô¨Årst classiÔ¨Åer more data to learn from ; without this approach , the number of input samples is relatively small ( 729 ) . 
The process for surface level sequence tasks is similar , but no meta - classiÔ¨Åer is used , as the gold standard labels have a month - level granularity . 
7 Results and Discussion Figure 5 and Tables 3 and 5 show the results obtained for the probing tasks . 
In addition to the loc2vec trajectory and text - based models , we run our experiments with two combination models , using the methods discussed in Section 4.3 . 
We employ the Reddit variation for these combination models due to its strong performance on downstream tasks ; we incorporate one model using concatenation and a model using retroÔ¨Åtting . 
We refer to these models as ‚Äú Loc2V - Reddit , ‚Äù and ‚Äú RedditRetroÔ¨Åt , ‚Äù respectively . 
We compare our classiÔ¨Åcation performance against a random baseline . 
In order to introduce a stronger supervised baseline for our methods , we employ simpler location representations , in the form of one - hot vectors , which are passed as input in our supervised evaluation framework ( Figure 4 ) . 
We take the mean of those one - hot vectors to create month sequence vectors as we do for the embeddings . 
7.1 Surface Level Location Tasks For these tasks , we include an overall average baseline , where we compute the metric for all locaFigure 5 : Results on surface level location tasks . 
tions . 
The results , shown in Figure 5 , lead to two unsurprising Ô¨Åndings : text - based methods are better at encoding functionality , and the methods rooted in physical location are better at encoding distance . 
The results are somewhat skewed for the text - based representations such as ‚Äú CampusWebsite , ‚Äù as some locations share a single page ; however , this effect alone does not entirely explain the performance of that model on the functionality overlap task , as it is outperformed on the physical distance task . 
One fascinating result is that the Twitter embeddings offer the best performance on the physical distance task by a method that does not utilize physical trajectories , which may be because this data is collected using geotags . 
People may tweet as they move between buildings , blurring the line between tweets in adjacent locations . 
We also observe that the methods that account for physical trajectories and text data can outperform those that use only text data ; this is especially clear from the results for Loc2V - Reddit , which show stronger performance than Loc2Vec andReddit individually for functionality overlap , and slightly stronger performance than Reddit for physical distance . 
This demonstrates one way in which we can create more robust representations of locations . 
7.2 Surface Level Sequence Tasks Overall , we note that all of our methods are easily able to surpass the random baseline . 
However , when it comes to the supervised one - hot vectorial representation , we see that traditional ways of representing text are able to best encode surface level information . 
This is because the sparse one - hot representation explicitly encodes information necessary for solving each task ; location - presence is denoted by a value greater than one for the particular dimension , and functionality - presence is denoted by a value greater than one for various431 Loc Pres Func Pres Random Baseline 41.0 45.0 One - Hot Avg 61.4 62.6 Loc2V 54.8 55.6 Twitter 56.9 57.8 Reddit 56.9 58.3 Campus - Website 55.8 57.8 Loc2V - Reddit 57.9 59.7 Reddit - RetroÔ¨Åt 55.2 56.5 Table 3 : Macro F1 scores ( % ) on surface level sequence tasks . 
Task # Cls Inst % in minority class Class Year 4 721 22.33 Gender 2 714 49.44 School 3 522 9.77 Sleep 2 729 41.02 GPA 2 729 38.13 All Depression 2 729 18.93 Major Depression 2 729 11.66 Table 4 : Class balance for downstream tasks . 
Instances are reported after Ô¨Åltering small classes . 
dimensions . 
We Ô¨Ånd that the text - based methods lead to stronger performance , as compared to their location - trajectory - based counterpart . 
This conÔ¨Årms that the superior encoding of functionality discussed in Section 7.1 is still discernible with aggregated sequence vectors . 
Among all of our proposed methods , the concatenation of trajectory - embeddings and text - based embeddings ( Loc2V - Reddit ) leads to the strongest results on these tasks . 
The results on both tasks are completely unmatched by the other methods , indicating that the additional semantic information from concatenation leads to stronger representations . 
7.3 Downstream Tasks We evaluate our embedding methods on the seven downstream tasks introduced in Section 5.3 : class year , gender , school enrollment , sleep satisfaction , GPA , all depression , and major depression . 
These tasks were designed to demonstrate the utility of various location representations in predicting a diverse set of attributes . 
The overall results for each model are listed in Table 5 ; we use macro F1 score as our metric . 
Table 4 shows the size of the minority class for each task . 
This imbalance and ourrelatively small data size made it challenging to achieve strong results on some tasks , although we generally were able to improve upon the baselines . 
Across all the tasks , predicting depression has the most potential for real - world impact , but also showcases the most imbalanced data distribution . 
With more data , we believe that patterns could be learned in a more robust way . 
For the task of school prediction , we greatly improve upon the random baseline even though the data is very imbalanced ; this could be because this attribute is clearly linked to where people go on campus , as is class year . 
For example , freshmen typically live in dorms and eat in dining halls , while seniors often live off campus ; computer science students attend classes in different places than English students . 
The strong performance on the gender prediction task may be explained by the real - world bias entailed in the school of enrollment ; e.g. , fewer women are enrolled in engineering , so they are less likely to visit engineering buildings . 
The strong performance on predicting class year with one - hot encodings can be directly linked to the surface level task improvement : freshmen are more likely to visit certain types of locations like dorms ( functionalitypresence ) ; performance is best among freshmen . 
Among text - based methods , we see that the Reddit embeddings enable the best performance on most downstream tasks . 
Reddit contains the most expressive language compared to the other venues , because its users are able to write at length without a strict character limit or other formalities imposed by media such as Twitter . 
Furthermore , from manually examining a sample of the posts , the community seems to primarily encompass current and former undergraduate students , therefore establishing a community that is above all else a place for students to share and discuss their daily lives . 
Meanwhile , the tweets that we link to locations may encompass musings from faculty or visiting scholars , and brief statements that are unrelated to campus life . 
The campus website data is the furthest from the student experience , as it is devoid of any dynamic content , written in the dry format of informational style . 
As a result , it seems intuitive that Reddit , in addition to providing definitional information about locations ( e.g. , there are many posts comparing and discussing dormitories ) , also provides student ‚Äôs emotional perspectives on them . 
We hypothesize that this closeness to student thoughts and feelings is what yields bet-432 Depression Class Year Gender School Sleep GPA All Major Random Baseline 25.0 50.0 30.0 50.0 49.0 45.0 41.0 One - Hot Avg 52.1 56.8 61.8 49.4 51.8 48.2 46.6 Loc2V 50.8 61.0 62.0 52.9 51.9 49.6 43.6 Twitter 49.4 57.4 65.4 49.3 51.9 48.5 44.8 Reddit 50.2 59.8 66.3 52.7 49.1 50.5 47.7 Campus - Website 48.8 58.1 60.1 46.4 51.9 49.4 42.9 Loc2V - Reddit 50.3 59.4 64.5 53.7 52.7 50.8 44.7 Reddit - RetroÔ¨Åt 50.2 60.8 66.0 52.6 47.7 48.7 39.6 Table 5 : Macro F1 scores ( % ) on downstream tasks . 
ter performance when predicting student attributes , compared to the other text - based methods . 
Overall , while results vary between different tasks , we Ô¨Ånd that a method that accounts for both physical location trajectories and text data describing locations ( Loc2V - Reddit ) has a strong overall performance . 
Notably , it is the best performing model on three tasks and achieves large improvements over the supervised baseline on two additional tasks . 
Such a model should be considered in future work on location embeddings because of its robustness on varied tasks . 
8 Conclusions In this paper , we addressed the task of building and probing location embeddings . 
We investigated several strategies to construct them , as well as a suite of probing tasks to understand the type of information encoded within . 
First , we showed that while all embedding methods encode both physical distance and functionality , methods using trajectories yield better spatial representations and methods using text data better encode location functionality . 
We showed that , like in the case of sentence embeddings from natural language , sequence embeddings of location data are able to encode surface level information ( location - presence , and functionalitypresence ) , as well as information that can be effectively used in downstream tasks . 
Overall , we found that an embedding model that accounts for both location trajectories and text related to locations ( Loc2V - Reddit ) gives the best performance over a diverse range of downstream tasks , from prediction of depression or sleep to prediction of academic area of study . 
Importantly , we also found that embeddings of locations tend to underperform more traditional one - hot encodings on surface - level tasks , yet they generally outperform these representations on downstream tasks . 
This suggests that while such embeddings do not explicitly record distinct locations that people visit ( thus being more privacy preserving and counteracting negative actions like stalking ) , they may be more effective for downstream applications that can yield positive outcomes , such as population - level mental health tracking or opt - in tracking for individuals who are in therapy . 
Our code is publicly available at http://lit . 
eecs.umich.edu/downloads.html . 
Acknowledgment We are grateful to Shiyu Qu , Jiaxin Ye , and Xinyi Zheng for assisting us with this study . 
This material is based in part upon work supported by the Precision Health initiative at the University of Michigan , by the Michigan Institute for Data Science ( MIDAS ) , by the National Science Foundation ( grant # 1815291 ) , and by the John Templeton Foundation ( grant # 61156 ) . 
Any opinions , Ô¨Åndings , and conclusions or recommendations expressed in this material are those of the author and do not necessarily reÔ¨Çect the views of the Precision Health initiative , MIDAS , the National Science Foundation , or John Templeton Foundation . 
Abstract Pairwise data automatically constructed from weakly supervised signals has been widely used for training deep learning models . 
Pairwise datasets such as parallel texts can have uneven quality levels overall , but usually contain data subsets that are more useful as learning examples . 
We present two methods to reÔ¨Åne data that are aimed at obtaining that kind of subsets in a self - supervised way . 
Our methods are based on iteratively training dualencoder models to compute similarity scores . 
We evaluate our methods on de - noising parallel texts and training neural machine translation models . 
We Ô¨Ånd that : ( i ) The self - supervised reÔ¨Ånement achieves most machine translation gains in the Ô¨Årst iteration , but following iterations further improve its intrinsic evaluation . 
( ii ) Machine translations can improve the de - noising performance when combined with selection steps . 
( iii ) Our methods are able to reach the performance of a supervised method . 
Being entirely self - supervised , our methods are well - suited to handle pairwise data without the need of prior knowledge or human annotations . 
1 Introduction Deep learning models are widely adopted and have demonstrated their usefulness in many areas and applications . 
Despite their diversity , one common characteristic of these models is the large number of parameters that need to be adjusted during training ( some recent models that have billions of parameters include T5 ( Raffel et al . 
, 2019 ) and GPT2 ( Radford et al . 
, 2019 ) ) . 
This leads to the need of collecting large amounts of training examples . 
Pairwise data , that captures the relationship in two modalities , is used to train deep learning models such as Neural Machine Translation ( NMT ) ( Wu et al . 
, 2016 ) , Question Answering ( Wang et al . 
, 2007 ) , Image Captioning ( Sharma et al . 
, 2018 ) , etc . 
To train this kind of models , large - scale data can often be obtained from weak signals like text cooccurrence ( Yang et al . 
, 2018 ) or dictionary n - gram matching ( Uszkoreit et al . 
, 2010 ) . 
For example , in the machine translation community , the large amount of multilingual text available on the internet has naturally led to the idea of using internet data to train NMT models ( Resnik , 1999 ) . 
This approach has proven advantageous but it has the drawback that data mined this way is intrinsically noisy ( Resnik and Smith , 2003 ) . 
Despite the poor quality , usually this kind of data contains a helpful subset that can be recovered through a process of data cleaning or reÔ¨Ånement . 
Data cleaning could be implemented with linguistic knowledge such as its script , vocabulary , syntax , etc . 
Alternatively , a model can be trained on ‚Äú clean ‚Äù or ‚Äú trusted ‚Äù pairs that are veriÔ¨Åed through manual annotation . 
Both options can be highly effective , but the former is limited in scope and error - prone , while the latter can be costly due to the number of required annotated examples . 
In this paper we introduce two self - supervised methods to obtain data subsets from noisy pairwise data that can be helpful to train dual - encoder ( D - E ) and neural machine translation ( NMT ) models . 
As noisy pairwise data , in our experiments we use parallel texts mined from the internet . 
Our methods do not require external knowledge ( e.g. syntactic rules ) , language - dependent heuristics ( e.g. script veriÔ¨Åcation ) or synthetic positive or negative training examples . 
By eliminating the need of annotations , our methods directly address the data labelling bottleneck . 
Our methods employ D - E models ( Gillick et al . 
, 2018 ) to learn a shared embedded space from the co - located text in the sentence pairs mined from the internet . 
Following Chidambaram et al . 
( 2018 ) we use the embedding distance in the learned space as a measure of cross - lingual similarity between sentences . 
Our hypothesis is that435 if higher scores are associated with cross - lingual similarity , pairs with higher scores will be closer to be actual translations of each other and , in that case , may be part of the data subset useful to train the models . 
In our experiments , our methods show effective reÔ¨Åning parallel texts mined from the internet . 
Much of the gains in the downstream evaluation are achieved in the Ô¨Årst iteration of the method , but later iterations keep improving the D - E models . 
Despite being self - supervised , our methods show competitive performance when compared against a de - noising method that uses supervision . 
2 Related Work One line of the research that directly relates to our work is corpus Ô¨Åltering for training NMT models . 
Below we classify the related work into two categories depending on the amount of supervision needed ( e.g. high quality parallel texts ) . 
( Semi-)Supervised Methods Some data denoising methods simply use Ô¨Åltering rules or heuristics such as language identiÔ¨Åcation of both the source and target texts , vocabulary checks , language model ( syntactic ) veriÔ¨Åcation , and so on . 
In contrast to rule - based approaches , approaches like Chen and Huang ( 2016 ) and Wang et al . 
( 2018c ) train classiÔ¨Åers to distinguish in - domain vs. outof - domain ( or clean vs. noisy ) data with a small parallel corpus , while other approaches build reference models on larger amounts of high - quality data ( Junczys - Dowmunt , 2018 ; Defauw et al . 
, 2019 ) . 
There are approaches that combine rules and heuristics with probabilistic models to determine the amount of noise in each sentence pair . 
In some cases these systems are designed as targeted efforts to denoise a particular dataset . 
Bicleaner ( S¬¥anchez - Cartagena et al . 
, 2018 ) , in relationship to the ParaCrawl ( Espl ` a et al . 
, 2019 ) data , is an example of that approach . 
Unsupervised Methods In contrast to the supervised methods , unsupervised methods do not require good - quality data to be available . 
Recent work ( Zhang et al . 
, 2020 ) leverages pre - trained language models and synthetic data ( Vyas et al . 
, 2018 ) , in place of true supervision . 
Some efforts focus on using monolingual corpora and align them through bootstrapping in order to generate sentence pairs ( Tran et al . 
, 2020 ; Ruiter et al . 
, 2020 ) , while others train a model with noisy data directly to gen - erate embeddings and score the data ( Chaudhary et al . 
, 2019 ) . 
Wang et al . 
( 2018b ) use twoNMT models taken from two training epochs to decide which data to use in order to improve the training efÔ¨Åciency and to show a de - noising effect . 
Our methods here try to take advantages of all of these approaches . 
Koehn et al . 
( 2018 ) and Koehn et al . 
( 2019 ) summarize Ô¨Åndings of the WMT corpus Ô¨Åltering efforts , though our work here primarily examines a self - supervised method in the context of de - noising , rather than on a targeted Ô¨Åltering effort . 
Our methods are unsupervised . 
We use dualencoder models , rather than an encoder - decoder architecture , to model pairwise data and let the model self - supervise itself or , further , be co - trained with an NMT model to reÔ¨Åne the training data . 
3 Dual - Encoder Model Dual - encoder ( D - E ) models have demonstrated to be an effective learning framework applied to both supervised ( Henderson et al . 
, 2017 ; Gillick et al . 
, 2019 ) and unsupervised tasks ( Cer et al . 
, 2018 ; Chidambaram et al . 
, 2018 ) . 
A multi - task D - E model consists of two encoders and a combination function for each of the tasks . 
In the context of the D - E framework , the selection of bilingual text can be interpreted as a ranking problem where , with yias the true target of source sentence xi , P(yi|xi)is ranked above all the other target candidates in Y. P(yi|xi)can be expressed as a log - linear model but , for practical reasons , we approximate the full set of target candidates Ywith a sample ( Henderson et al . 
, 2017 ) . 
When training in a batch , P(yi|xi ) can be approximated as : P(yi|xi)‚âàeœÜ(xi , yi ) eœÜ(xi , yi)+ Œ£N n=1,n /negationslash = ieœÜ(xi , yn)(1 ) whereNis the size of a batch and œÜis a similarity function . 
In such a way , model training can be done by optimizing a log - likelihood loss function : L= ‚àí1 N / summationtextN i=1logeœÜ(xi , yi ) eœÜ(xi , yi)+/summationtextN n=1,n / negationslash = ieœÜ(xi , yn)(2 ) Based on the results of Yang et al . 
( 2019a ) with additive margin softmax ( Wang et al . 
, 2018a ) , we modify our loss function to include margin m:436 Source   Encoder   Source   Negatives   Target   Encoder    Target   Target   Encoder   Target   Negatives 0.010.02 ... 0.01 0.010.02 ... 0.01 ... ... ... ... 0.2 0.01 ... 0.010.1 0.01 ... 0.05 0.020.1 ... 0.02 ... ... ... ... 0.1 0.06 ... 0.01 Batched   Input   Additive Margin   Softmax   Dual   Encoder   Source   Encoder   Source 0.6 0.1 ... 0.01 0.010.8 ... 0.01 ... ... ... ... 0.2 0.01 ... 0.7Figure 1 : D - E model training with hard negatives . 
The encoders with the same color share parameters . 
The dot product scoring function makes it easy to compute pairwise scores by doing matrix multiplications . 
The highlighted diagonal indicates the dot products of the source and target texts . 
The additive margin softmax is applied at every row ( source‚Üítarget ) and column ( target ‚Üísource ) . 
Lams= ‚àí1 N / summationtextN i=1logeœÜ(xi , yi)‚àím eœÜ(xi , yi)‚àím+/summationtextN n=1,n / negationslash = ieœÜ(xi , yn)(3 ) When using the dot product as similarity functionœÜ , a single matrix multiplication can be used to efÔ¨Åciently compute scores for all the examples in the batch . 
When set to learn from clean crosslingual paired texts , a D - E model can be used to learn strong cross - lingual embeddings for bitext retrieval as shown in Guo et al . 
( 2018 ) and Yang et al . 
( 2019a ) . 
The challenge is to learn similar embeddings when training D - E models on noisy data . 
3.1 Model ConÔ¨Åguration In our experiments we use D - E models with hard negatives sampling ( Guo et al . 
, 2018 ) . 
Similar to Yang et al . 
( 2019a ) , our models are trained bidirectional so the rankings in both directions , source to target and target to source , are optimized . 
But in contrast to Yang et al . 
( 2019a ) we do not share the parameters between the source and target encoders . 
In our initial experiments training NMT models we found that , under noisy conditions , there is improvement of close to 1 BLEU point when using D - E models that use speciÔ¨Åc encoders for each language . 
Figure 1 illustrates our training approach . 
For our encoders we use 3 - layer transformers ( Vaswani et al . 
, 2017 ) in the encoders with hidden layers of size 512 and 8 attention heads . 
We build vocabularies for each language separately . 
Given the noise in the data , the vocabulariesmight not include all words in the source or target languages . 
We control the prevalence of words in the expected language with the vocabulary size . 
Our reasoning is that large vocabularies are more likely to include words in languages other than the expected . 
200k most frequent words are used and 200k extra buckets are reserved for the outof - vocabulary words found in the training . 
We use character- and word - level features to model the source and target inputs . 
For character - level representations , we decompose each word into all character n - grams within a range . 
For word - level representation , we sum the embeddings for its character n - grams and its word embedding . 
The Ô¨Ånal sentence representation is the output of the transformer layers as a 500 - dimensional vector . 
We train the D - E models using SGD for 40 M steps with a learning rate of 0.001 . 
A Ô¨Åxed value of margin 0.2 is used in equation 3 . 
4 Our Approach : Self - Supervised Learning for Data ReÔ¨Ånement 4.1 Training with Hard Negatives As described in equation 1 , and illustrated in Ô¨Ågure 1 , for every source sentence we use all target sentences , except its own , as negatives in a batch . 
We also augment the batch with hard negatives to improve the contrast between true translations and any other random sentence pairing . 
We mine the hard negatives using a separate D - E model to retrieve , for every sentence , the top Ncandidates that are not its counterpart in the pair . 
It is important437 Initial Dual   Encoder   Model Noisy data   Score , Rank ,   Select   Selected   data Train Dual   Encoder   Dual Encoder   Model Train Initial   Dual Encoder ( a ) Iterative Ô¨Åltering . 
Score , rank ,   select Selected data Train NMT NMT Model Initial NMT   Model Noisy data   Translate   Translated   data Train Dual   Encoder   Dual Encoder   Model Train Initial   NMT ( b ) Machine translation iterative Ô¨Åltering . 
Figure 2 : [ Iterative Ô¨Åltering ( IF ) ] : the scores of the dual - encoder are used to select the training material for the next model . 
[ Machine translation iterative Ô¨Åltering ( MT - IF ) ] : The D - E model is used to score the forwardtranslations from the NMT model , only the top - ranking sentence pairs are used to train the next NMT model . 
to notice that the hard negatives in our method are retrieved , not generated or synthesized . 
We mine the hard negatives ofÔ¨Çine from the sentences in the ParaCrawl v1.0 data , or from the translations only when using translations as target sentences . 
Our negative - mining D - E has DNN layers , instead of transformer ones , with a reduced embedding size ( 25 - dimensional ) . 
We mine hard negatives for both the source and target sentences . 
As shown in Ô¨Ågure 1 , the hard negatives are speciÔ¨Åc to each one of the sentence pairs but , when added to the batch , we use them as additional random negatives for all the other source sentences in the batch . 
We use a batch size of 128 examples and 5 hard - negatives per example . 
We augment the batch row - wise with hard negatives mined for the target sentences , and column - wise with hard negatives for the source . 
In our self - supervised approach , we train D - E models with one dataset and use the models that we train to score the same data . 
Our hypothesis is that the scores are useful to rank the data in a way that makes it easy to Ô¨Ålter out the noise . 
It is natural to believe that , in principle , a data - model cycle like this may not lead to much improvement because the trained models tend memorize the training data , including the noise . 
We break this cycle by adding a selection step to the process and avoiding to train the models with the same examples all the time . 
We propose a self - supervised method for pairwise data reÔ¨Ånement based on data ‚Äú iterative Ô¨Åltering ‚Äù ( IF ) . 
With this method we reÔ¨Åne data that we use to train NMT models . 
By including the downstream task in our method , we formulate a second method asan extension of the Ô¨Årst one . 
We regard this second method as ‚Äú machine translation - iterative Ô¨Åltering ‚Äù ( MT - IF ) . 
Both methods are illustrated in Ô¨Ågure 2 . 
4.2 Iterative Filtering We use the dot product between source and target embeddings as proxy of cross - lingual similarity . 
Once we score and select data to train one model , we can use that model to score and select data for the next one in an iterative way . 
The details of this method are shown in Ô¨Ågure 2a and explained in algorithm 1 . 
We bootstrap this method by training an initial D - E model with all the pre-Ô¨Åltered data . 
It is important to notice that in each iteration we train the D - E model with a subset of the data ( the selected data ) , but we score the entire set . 
This allows the method to recover useful data that may have been discarded in earlier iterations . 
Algorithm 1 Iterative Ô¨Åltering 1 : œÑ‚Üêselection threshold 2 :D - E = TrainDualEncoder(data ) 3 : while D - E improves do 4 : scored data = Score(data ; D - E ) 5 : ranked data = Rank(scored data ) 6 : selected data = Select(ranked data ; œÑ ) 7 : D - E = TrainDualEncoder(selected data ) 8 : end while438 4.3 Machine Translation Iterative Filtering In this method , the D - E model selects data to train an NMT model , rather than to train another D - E model . 
The NMT model then produces translations to train the D - E model . 
This way , the D - E and NMT models boost each other in a ‚Äú co - training ‚Äù way . 
The key to this method is to use the NMT model to generate the training data for the D - E model in order to improve its de - noising capabilities . 
Algorithm 2 explains this idea and Ô¨Ågure 2b illustrates it . 
As before , in every iteration the whole dataset is scored and ranked so sentence pairs that ranked low early on can be recovered in later iterations . 
In principle , forward - translation does not seem to be a good way to generate training data . 
One can anticipate that the models are prone to mimic the training data , including the noise . 
Just as in our Ô¨Årst method , we break the cycle by adding a selection step based on the D - E scores and using only the top - ranking data to train the next NMT model . 
5 Experimental Setup Machine Translation Model To assess if we can recover useful subsets from noisy data , we train Transformer - Big ( Vaswani et al . 
, 2017 ) NMT models using data reÔ¨Åned with our methods . 
To train the models , we split the source and target texts into pieces using bilingual sentence piece models ( Kudo and Richardson , 2018 ) that were trained with the ParaCrawl v1.0 data only . 
We train for a maximum of 200k steps using ( Shazeer and Stern , 2018 ) and pick the best checkpoint according to the performance on a validation set . 
The models are trained on Google ‚Äôs Cloud TPU v3 with batch size 3072 . 
In all our experiments , the conÔ¨Åguration of the NMT models is kept the same with the only difference being the training data . 
Algorithm 2 Machine translation iterative Ô¨Åltering 1 : œÑ‚Üêselection threshold 2 : NMT = TrainNMT(data ) 3 : while D - E improves orNMT improves do 4 : translated data = Translate(data ; NMT ) 5 : D - E = TrainDualEncoder(translated data ) 6 : scored data = Score(data ; D - E ) 7 : ranked data = Rank(scored data ) 8 : selected data = Select(ranked data ; œÑ ) 9 : NMT = TrainNMT(selected data ) 10 : end whileen - fr en - de All sentence pairs 4,235 M 4,591 M Pre-Ô¨Åltered 289 M 282 M 70th percentile ( for NMT ) 87 M 85 M 80th percentile ( for D - E ) 58 M 56 M Table 1 : Number of sentence pairs in the ParaCrawl v1.0 data , and after preÔ¨Åltering and selection . 
Data In our experiments we use two language pairs : English to French ( en - fr ) and English to German ( en - de ) . 
We use ParaCrawl v1.0 ( Espl ` a et al . 
, 2019 ) as training data . 
We apply light - weight preÔ¨Åltering steps to remove sentence pairs that : ( i ) are duplicated , ( ii ) have identical source and target texts , ( iii ) have empty sentences , or ( iv ) have a large difference in the number of tokens . 
For the last case , we compute the ratio of source over target tokens as : œÅ = nS+Œ± nT+Œ±withnSandnTbeing the number of tokens in the source and in the target respectively , and Œ±a token count tolerance . 
With anŒ±of 15 , we discard a sentence pair if œÅis greater than 1.5 . 
Similarly for the ratio of target over source tokens . 
We use WMT newstest 20122013 ( Bojar et al . 
, 2014 ) as the development set and we evaluate on two sets : WMT newstest 2014 and news discussion test 2015 for en - fr ; WMT newstest 2014 and 2015 for en - de . 
Evaluation As described in section 3 , we trained the D - E models as rankers . 
Thus , we use the BUCC 2018 mining task ( Zweigenbaum et al . 
, 2018 ) as an intrinsic metric for the model . 
The task data consists of corpora for four language pairs including fr - en and de - en . 
For each language pair , the shared task provides a monolingual corpus for each language and a ground truth list containing true translation pairs . 
The task is to construct a list of translation pairs from the monolingual corpora , and evaluate them in terms of the F1 compared to the ground truth . 
To test the end - performance of the NMT models in terms BLEU scores , we compute the detokenized and case - sensitive BLEU scores against the original references using an in - house reimplementation of themteval-v14.pl script . 
Iterative Selection In our experiments we ran 3 iterations of the IFmethod and 3 iterations of the MT - IF one . 
To deÔ¨Åne the value of the selection thresholds , we conducted initial experiments to explore the439 impact of the threshold when selecting the data to train the D - E models . 
Figure 3 shows the BUCC results , in terms of the best F1 measure and the area under the precision - recall curve ( AUCPR ) , for D - E models trained with data selected using different thresholds . 
Even though there is not a single threshold that works best for both languages , models trained with data selected from the 70th or 80th percentiles produce the best results . 
Using either very low ( below 0.2 ) or very high thresholds ( above 0.95 ) leads to D - E models with lower results . 
We set the selection thresholds for the data to train the D - E models and to train the NMT models separately . 
For the former we use data on the 80th percentile , and on the 70th percentile for the latter . 
Our intuition was that we can be more stringent when selecting data to train the D - E because only high - ranking examples may be true translations to learn from . 
Table 1 shows the number of sentences in the ParaCrawl v1.0 en - fr and en - de datasets and the amount of sentences that the pre-Ô¨Åltering and selection steps , at the different thresholds , let through . 
The large number of sentence pairs that are eliminated via pre-Ô¨Åltering give an indication of how much noise there is in the data . 
It is worth noticing that the subset of data that we deem ‚Äú useful ‚Äù is two orders of magnitude smaller than the original data . 
6 Results 6.1 Intrinsic Dual - Encoder Evaluation Table 2 shows the BUCC mining task results for the D - E models trained with our methods in terms of F1 AUCPR / Best F1 0 0.2 0.4 0.6 0.8 1.000.20.40.60.81.0 Threshold ( œÑ)en - fr AUCPR en - fr Best F1 en - de AUCPR en - de Best F1 Figure 3 : BUCC mining results of dual encoder models trained with data selected at different thresholds.and AUCPR . 
As baseline we include the results of a D - E model trained with all the ParaCrawl v1.0 data after pre-Ô¨Åltering . 
The baseline performs poorly in both en - fr and en - de . 
The D - E models trained with theIFdata produce good mining results starting from the very initial models , i.e. when using D - E models trained using hard negatives but no selection yet . 
The signiÔ¨Åcant gains of IF0over the baseline conÔ¨Årm our observations about the positive impact of hard negatives in cross - lingual tasks ( Guo et al . 
, 2018 ) . 
In subsequent iterations ( indices 1 to 3 in table 2 ) selection is used and the D - E models show steady improvement . 
The improvement in the AUCPR and F1 of the D - E models trained with the MT - IFdata is quite remarkable . 
The performance for models trained with data from the Ô¨Årst iteration of this method surpass the performance of models trained with the the third iteration of the IFdata and keep improving , but seem to plateau around the second iteration . 
For reference , we include in table 2 the AUCPR and F1 from embeddings generated with the public ‚Äú universal - sentence - encodermultilingual - large ‚Äù v2 ( Yang et al . 
, 2019b ) from TFHub1to show the performance of a D - E model trained on multiple large and non - public industry datasets . 
As expected , training on this kind of data is far better than de - noising , but the evaluation shows that our methods do a good job reÔ¨Åning data , especially considering how much noise there is in the ParaCrawl datasets to start with . 
6.2 Translation Evaluation To illustrate the end - performance of our methods , table 3 shows the BLEU scores ( Papineni et al . 
, 2002 ) of NMT models trained with data subsets selected with our methods . 
The D - E models used to score the data in each iteration correspond to the same models reported in table 2 . 
As baseline we use an NMT model trained with all the sentence pairs just after pre-Ô¨Åltering , i.e. selection is not used yet . 
For both our methods the NMT models show considerable improvement over the baseline . 
It is interesting that the initial NMT ( IF0 in table 3 ) , shows good improvement in spite of using a D - E whose only difference over baseline is the use of hard negatives . 
There is also noticeable improvement between the IF0andIF1results pointing to the fact that our process of scoring , ranking and selection is also useful to improve the 1https://tfhub.dev/google/universal-sentence-encodermultilingual-large/2440 Methoden - fr en - de AUCPR Best F1 AUCPR Best F1 Pre-Ô¨Åltered data ( baseline ) 0.068 0.149 0.020 0.069 IF0 0.246 0.330 0.094 0.179 IF1 0.380 0.445 0.291 0.359 IF2 0.570 0.600 0.372 0.415 IF3 0.622 0.642 0.390 0.432 MT - IF1 0.641 0.673 0.545 0.566 MT - IF2 0.664 0.697 0.600 0.620 MT - IF3 0.676 0.707 0.593 0.608 USE multi - lingual 0.824 0.812 0.861 0.815 Table 2 : BUCC mining results of the dual - encoder models . 
The index in each experiment denotes the iteration . 
The USE multi - lingual model was trained using non - public industry datasets . 
Methoden - fr en - de newstest2014 newsdiscusstest2015 newstest2014 newstest2015 Pre-Ô¨Åltered data ( baseline ) 0.303 0.297 0.196 0.239 IF0 0.324 0.315 0.237 0.276 IF1 0.342 0.343 0.239 0.281 IF2 0.340 0.352 0.237 0.279 IF3 0.342 0.348 0.236 0.283 Forward - translated data 0.305 0.306 0.203 0.243 MT - IF1 0.342 0.346 0.237 0.280 MT - IF2 0.343 0.348 0.235 0.283 MT - IF3 0.346 0.349 0.236 0.286 Table 3 : BLEU scores of the trained NMT models and the baseline models . 
The index in each experiment denotes the iteration . 
NMT models . 
The second half of table 3 shows the BLEU scores when the NMT models are added to the reÔ¨Ånement process in the MT - IFmethod . 
For a better reference , we train an NMT model with forward - translated sentence pairs using the baseline NMT model . 
Crucially , there is no selection on the forward - translated data to train this model . 
This NMT model does not show improvement relative to the baseline NMT model and conÔ¨Årms that distilling new training examples from forward translations provides little or no gain . 
In contrast to the BUCC evaluation from table 2 , the downstream task does not seem to require several iterations to show good results . 
The BLEU scores of later iterations in the process only improve marginally as opposed to the steady improvement observed in the BUCC task . 
6.3 Supervised vs Self - Supervised We use Bicleaner to compare our methods against a supervised approach on the task of de - noising the ParaCrawl data , with the important caveat thaten - fr en - de 70th percentile after lang ID 29 M 27 M Bicleaner v1.2 25 M 17 M Table 4 : Number of sentence pairs of selected data after language identiÔ¨Åcation and in Bicleaner v1.2 . 
Bicleaner is not only supervised but tailored to denoise this data . 
In that sense , our method would be in disadvantage especially because our D - E models were not trained with any signal related to the identity of the language . 
To add this missing component to our method , we use language identiÔ¨Åcation as a post - processing step on the reÔ¨Åned data . 
We use a pre - trained language identiÔ¨Åcation method from Zhang et al . 
( 2018 ) to Ô¨Ålter out pairs where the source or target texts do not match the expected language . 
As around 30 % of the training data gets discarded ( table 1 vs table 4 ) , the scores of the remaining data need to be re - ranked in preparation for the selection step . 
We train new NMT models using only the sentence pairs that get ranked in the441 70th percentile and Ô¨Åltered by the language identiÔ¨Åcation . 
We compare the models against similar NMT models trained with the Bicleaner v1.2 data downloaded from the ParaCrawl website2 . 
Table 4 shows the number of sentence pairs used to train the NMT models after applying language identiÔ¨Åcation and in the Bicleaner v1.2 data . 
To isolate the effects of language identiÔ¨Åcation , we compare NMT models trained with data from our methods against similar models trained with data that went through language identiÔ¨Åcation also but , as in previous baselines , no selection was used . 
As shown in Table 5 , using language identiÔ¨Åcation on the training data boosts the performance . 
The NMT models trained with the data reÔ¨Åned with our methods still show considerable improvement over not using selection , making evident that there is still much room for data reÔ¨Ånement after language identiÔ¨Åcation . 
Our method shows very competitive results against the NMT models trained using the Bicleaner v1.2 data , surpassing the BLEU scores in en - fr and getting very similar performance in en - de . 
It is interesting that , with the addition of language identiÔ¨Åcation , our self - supervised method can remove noise just as effectively as a targeted effort to denoise the ParaCrawl data . 
6.4 Iterative Data ReÔ¨Ånement To verify the effectiveness of our methods in Ô¨Ånding useful subsets contained in the noisy data , we analyze the results of our models when scoring true sentence pairs versus scoring pairs that are not actual translations . 
For this analysis , we leverage the BUCC mining task and compute the dot products of ‚Äú ground truth ‚Äù pairs using our D - E models . 
Figure 4 shows box plots of the dot products for both en - fr and en - de BUCC data . 
For reference , we compute the dot products of the ‚Äú nearest negative ‚Äù of each source sentence . 
We reuse the retrieval results from the D - E intrinsic evaluation ( subsection 6.1 ) to deÔ¨Åne the nearest negative as the target sentence with the highest dot product that is not its actual translation . 
This leads to 9,086 ground truth and nearest negative dot products for en - fr and 9,580 for en - de whose distributions are displayed in the box plots in Ô¨Ågure 4 . 
Starting with the baseline D - E models , the dot products of the ground truth and the nearest negative are very close in value . 
This is evident by the fact that their difference ( also plotted in Ô¨Ågure 4 ) is very close to 0 . 
The differ2https://paracrawl.eu/v1ence starts to grow with the IF0models , showing that hard - negatives are useful to increase the separation between the dot products of both classes . 
For theIFmethod , the difference between ground truth and nearest negative keeps growing steadily with every iteration . 
This conÔ¨Årms the progression observed in the AUCPR and F1 measures in table 2 . 
For theMT - IFmodels , the score difference between ground truth and nearest negative is already signiÔ¨Åcant in the Ô¨Årst iteration , but it does not progress much further in later iterations . 
This also conÔ¨Årms the observations for these models in the BUCC mining results from table 2 . 
The fact that the dot products of our models show good levels of separation of each class corroborate , from the data analysis standpoint , that both our methods are effective in separating useful data samples from the noisy dataset . 
6.5 Discussion Intrinsic vs downstream evaluations Our selfsupervised methods seem to naturally improve the quality of the reÔ¨Åned data , as measured by the results of the BUCC parallel text mining task . 
However , most of the BLEU score gains are achieved on the Ô¨Årst iteration . 
One possible explanation is that the BUCC evaluation is a closer match to the ranking task used to train the D - E model . 
Another possibility is that , given that different sequences can produce the same BLEU scores , there may be improvements in the translation quality that the BLEU scores do not reÔ¨Çect . 
Making the method more aware of the downstream translation task and gaining insight into the translation quality are interesting lines of future work . 
Language identiÔ¨Åcation impact In noisy data , language identiÔ¨Åcation seems to play a signiÔ¨Åcant role . 
In our experiments we applied it as a postprocess but we are interested in applying it as part of the pre-Ô¨Åltering process , or integrated as part of our scoring models in the future . 
Breaking the data - model memorization cycle Training NMT models directly with translated data did not produce gains over the baseline . 
But we found signiÔ¨Åcant gains when instead we used the translated data to train D - E models and used the models to score and select data to in turn train the NMT models . 
We see this as conÔ¨Årmation that it is possible to break the data - model memorization cycle by co - training models using different training goals.442 Methoden - fr en - de newstest2014 newsdiscusstest2015 newstest2014 newstest2015 Pre-Ô¨Åltered data lang ID 0.336 0.346 0.239 0.279 Bicleaner v1.2 data 0.363 0.370 0.274 0.316 IF1 0.369 0.373 0.263 0.306 IF2 0.369 0.369 0.267 0.308 IF3 0.366 0.372 0.269 0.314 MT - IF1 0.361 0.365 0.263 0.308 MT - IF2 0.363 0.370 0.262 0.303 MT - IF3 0.360 0.364 0.259 0.303 Table 5 : BLEU scores of the NMT models using language identiÔ¨Åcation and compared against Bicleaner . 
Figure 4 : Dot product distributions for the ground truth and nearest negative from the BUCC mining task . 
The box plots represent the ( 5,25,50,75,95)-percentile of the dot product distribution for each method and iteration . 
7 Conclusions We introduced two self - supervised methods to reÔ¨Åne pairwise data aimed at selecting useful subsets from noisy data . 
In our experiments we used parallel texts mined from the internet as example of the weakly constructed pairwise data to reÔ¨Åne . 
Our methods do not require linguistic knowledge or human annotated data . 
They use iterative selection of the data to train two kinds of models . 
Our Ô¨Årst method is based on self - boosting dualencoder models iteratively . 
We applied this method to denoise data to train NMT models . 
Our second method integrates the NMT models into the iterative process to generate translations that , after a selection step , are used to train the dual - encoder models . 
Our results show that most of the gains in terms of BLEU score can be achieved in the Ô¨Årst iteration of our methods , but later iterations keep improving the performance of the dual - encoder models in the BUCC evaluation . 
In our experiments , using translated text in combination with a selection step helped to improve the de - noising capabilities of the dual - encoder models . 
We observed that selection is effective to break the model - datamemorization cycle . 
One characteristic that our self - supervised methods do not seem to capture well is an indication of the language identity . 
If we use language identiÔ¨Åcation on the denoised data as a post - processing step , the performance , in terms of BLEU scores , turns very competitive against supervised targeted efforts tailored to remove noise from the dataset . 
These results encourage us to pursue future lines of work that include using cross - attention in the pairwise data to better capture the relationship in the pairs . 
Also , speciÔ¨Åc to parallel sentences mined from the internet , we would like to explore ways to include language identiÔ¨Åcation in the models . 
On the other hand , it seems natural to leverage the self - supervision characteristics of our methods and apply them to language pairs where noisy internet data may be available but annotated data is not . 
Lastly , we are interested in expanding our methods to other pairwise data such as text - image pairs . 
Acknowledgments The authors would like to thank Noah Constant and the anonymous reviewers for their valuable feedback and suggestions to improve this paper.443 Abstract Recent years have seen important advances in the quality of state - of - the - art models , but this has come at the expense of models becoming less interpretable . 
This survey presents an overview of the current state of Explainable AI ( XAI ) , considered within the domain of Natural Language Processing ( NLP ) . 
We discuss the main categorization of explanations , as well as the various ways explanations can be arrived at and visualized . 
We detail the operations and explainability techniques currently available for generating explanations for NLP model predictions , to serve as a resource for model developers in the community . 
Finally , we point out the current gaps and encourage directions for future work in this important research area . 
1 Introduction Traditionally , Natural Language Processing ( NLP ) systems have been mostly based on techniques that are inherently explainable . 
Examples of such approaches , often referred to as white box techniques , include rules , decision trees , hidden Markov models , logistic regressions , and others . 
Recent years , though , have brought the advent and popularity of black box techniques , such as deep learning models and the use of language embeddings as features . 
While these methods in many cases substantially advance model quality , they come at the expense of models becoming less interpretable . 
This obfuscation of the process by which a model arrives at its results can be problematic , as it may erode trust in the many AI systems humans interact with daily ( e.g. , chatbots , recommendation systems , information retrieval algorithms , and many others ) . 
In the broader AI community , this growing understanding of the importance of explainability has created an emerging Ô¨Åeld called Explainable AI ( XAI ) . 
However , just as tasks in different Ô¨Åelds are more amenable to particular approaches , explainabilitymust also be considered within the context of each discipline . 
We therefore focus this survey on XAI works in the domain of NLP , as represented in the main NLP conferences in the last seven years . 
This is , to the best of our knowledge , the Ô¨Årst XAI survey focusing on the NLP domain . 
As will become clear in this survey , explainability is in itself a term that requires an explanation . 
While explainability may generally serve many purposes ( see , e.g. , Lertvittayakumjorn and Toni , 2019 ) , our focus is on explainability from the perspective of an end user whose goal is to understand how a model arrives at its result , also referred to as theoutcome explanation problem ( Guidotti et al . 
, 2018 ) . 
In this regard , explanations can help users of NLP - based AI systems build trust in these systems ‚Äô predictions . 
Additionally , understanding the model ‚Äôs operation may also allow users to provide useful feedback , which in turn can help developers improve model quality ( Adadi and Berrada , 2018 ) . 
Explanations of model predictions have previously been categorized in a fairly simple way that differentiates between ( 1 ) whether the explanation is for each prediction individually or the model ‚Äôs prediction process as a whole , and ( 2 ) determining whether generating the explanation requires post - processing or not ( see Section 3 ) . 
However , although rarely studied , there are many additional characterizations of explanations , the most important being the techniques used to either generate or visualize explanations . 
In this survey , we analyze the NLP literature with respect to both these dimensions and identify the most commonly used explainability and visualization techniques , in addition to operations used to generate explanations ( Sections 4.1 - Section 4.3 ) . 
We brieÔ¨Çy describe each technique and point to representative papers adopting it . 
Finally , we discuss the common evaluation techniques used to measure the quality of explanations ( Section 5 ) , and conclude with a discussion of gaps and challenges in developing success-447 ful explainability approaches in the NLP domain ( Section 6 ) . 
Related Surveys : Earlier surveys on XAI include Adadi and Berrada ( 2018 ) and Guidotti et al . 
( 2018 ) . 
While Adadi and Berrada provide a comprehensive review of basic terminology and fundamental concepts relevant to XAI in general , our goal is to survey more recent works in NLP in an effort to understand how these achieve XAI and how well they achieve it . 
Guidotti et al . 
adopt a four dimensional classiÔ¨Åcation scheme to rate various approaches . 
Crucially , they differentiate between the ‚Äú explanator ‚Äù and the black - box model it explains . 
This makes most sense when a surrogate model is used to explain a black - box model . 
As we shall subsequently see , such a distinction applies less well to the majority of NLP works published in the past few years where the same neural network ( NN ) can be used not only to make predictions but also to derive explanations . 
In a series of tutorials , Lecue et al . 
( 2020 ) discuss fairness and trust in machine learning ( ML ) that are clearly related to XAI but not the focus of this survey . 
Finally , we adapt some nomenclature from Arya et al . 
( 2019 ) which presents a software toolkit that can help users lend explainability to their models and ML pipelines . 
Our goal for this survey is to : ( 1 ) provide the reader with a better understanding of the state of XAI in NLP , ( 2 ) point developers interested in building explainable NLP models to currently available techniques , and ( 3 ) bring to the attention of the research community the gaps that exist ; mainly a lack of formal deÔ¨Ånitions and evaluation for explainability . 
We have also built an interactive website providing interested readers with all relevant aspects for every paper covered in this survey.1 2 Methodology We identiÔ¨Åed relevant papers ( see Appendix A ) and classiÔ¨Åed them based on the aspects deÔ¨Åned in Sections 3 and 4 . 
To ensure a consistent classiÔ¨Åcation , each paper was individually analyzed by at least two reviewers , consulting additional reviewers in the case of disagreement . 
For simplicity of presentation , we label each paper with its main applicable category for each aspect , though some papers may span multiple categories ( usually with varying degrees of emphasis . 
) All relevant aspects for every 1https://xainlp2020.github.io/xainlp/ ( we plan to maintain this website as a contribution to the community.)paper covered in this survey can be found at the aforementioned website ; to enable readers of this survey to discover interesting explainability techniques and ideas , even if they have not been fully developed in the respective publications . 
3 Categorization of Explanations Explanations are often categorized along two main aspects ( Guidotti et al . 
, 2018 ; Adadi and Berrada , 2018 ) . 
The Ô¨Årst distinguishes whether the explanation is for an individual prediction ( local ) or the model ‚Äôs prediction process as a whole ( global ) . 
The second differentiates between the explanation emerging directly from the prediction process ( selfexplaining ) versus requiring post - processing ( posthoc ) . 
We next describe both of these aspects in detail , and provide a summary of the four categories they induce in Table 1 . 
3.1 Local vs Global Alocal explanation provides information or justiÔ¨Åcation for the model ‚Äôs prediction on a speciÔ¨Åc input ; 46 of the 50 papers fall into this category . 
Aglobal explanation provides similar justiÔ¨Åcation by revealing how the model ‚Äôs predictive process works , independently of any particular input . 
This category holds the remaining 4 papers covered by this survey . 
This low number is not surprising given the focus of this survey being on explanations that justify predictions , as opposed to explanations that help understand a model ‚Äôs behavior in general ( which lie outside the scope of this survey ) . 
3.2 Self - Explaining vs Post - Hoc Regardless of whether the explanation is local or global , explanations differ on whether they arise as part of the prediction process , or whether their generation requires post - processing following the model making a prediction . 
A self - explaining approach , which may also be referred to as directly interpretable ( Arya et al . 
, 2019 ) , generates the explanation at the same time as the prediction , using information emitted by the model as a result of the process of making that prediction . 
Decision trees and rule - based models are examples of global self - explaining models , while feature saliency approaches such as attention are examples of local self - explaining models . 
In contrast , a post - hoc approach requires that an additional operation is performed after the predictions are made . 
LIME ( Ribeiro et al . 
, 2016 ) is448 an example of producing a local explanation using a surrogate model applied following the predictor ‚Äôs operation . 
A paper might also be considered to span both categories ‚Äì for example , ( Sydorova et al . 
, 2019 ) actually presents both self - explaining and post - hoc explanation techniques . 
Local Post - HocExplain a single prediction by performing additional operations ( after the model has emitted a prediction ) Local SelfExplainingExplain a single prediction using the model itself ( calculated from information made available from the model as part of making the prediction ) Global Post - HocPerform additional operations to explain the entire model ‚Äôs predictive reasoning Global SelfExplainingUse the predictive model itself to explain the entire model ‚Äôs predictive reasoning ( a.k.a . 
directly interpretable model ) Table 1 : Overview of the high - level categories of explanations ( Section 3 ) . 
4 Aspects of Explanations While the previous categorization serves as a convenient high - level classiÔ¨Åcation of explanations , it does not cover other important characteristics . 
We now introduce two additional aspects of explanations : ( 1 ) techniques for deriving the explanation and ( 2 ) presentation to the end user . 
We discuss the most commonly used explainability techniques , along with basic operations that enable explainability , as well as the visualization techniques commonly used to present the output of associated explainability techniques . 
We identify the most common combinations of explainability techniques , operations , and visualization techniques for each of the four high - level categories of explanations presented above , and summarize them , together with representative papers , in Table 2 . 
Although explainability techniques and visualizations are often intermixed , there are fundamental differences between them that motivated us to treat them separately . 
Concretely , explanation derivation - typically done by AI scientists and engineers - focuses on mathematically motivated justiÔ¨Åcations of models ‚Äô output , leveraging various explainability techniques to produce ‚Äú raw explanations ‚Äù ( such as attention scores ) . 
On the other hand , explanation presentation - ideally done by UX engineers focuses on how these ‚Äú raw explanations ‚Äù are best presented to the end users using suitable visualization techniques ( such as saliency heatmaps).4.1 Explainability Techniques In the papers surveyed , we identiÔ¨Åed Ô¨Åve major explainability techniques that differ in the mechanisms they adopt to generate the raw mathematical justiÔ¨Åcations that lead to the Ô¨Ånal explanation presented to the end users . 
Feature importance . 
The main idea is to derive explanation by investigating the importance scores of different features used to output the Ô¨Ånal prediction . 
Such approaches can be built on different types of features , such as manual features obtained from feature engineering ( e.g. , V oskarides et al . 
, 2015 ) , lexical features including word / tokens and n - gram ( e.g. , Godin et al . 
, 2018 ; Mullenbach et al . 
, 2018 ) , or latent features learned by NNs ( e.g. , Xie et al . 
, 2017 ) . 
Attention mechanism ( Bahdanau et al . 
, 2015 ) and Ô¨Årst - derivative saliency ( Li et al . 
, 2015 ) are two widely used operations to enable feature importance - based explanations . 
Text - based features are inherently more interpretable by humans than general features , which may explain the widespread use of attention - based approaches in the NLP domain . 
Surrogate model . 
Model predictions are explained by learning a second , usually more explainable model , as a proxy . 
One well - known example is LIME ( Ribeiro et al . 
, 2016 ) , which learns surrogate models using an operation called input perturbation . 
Surrogate model - based approaches are model - agnostic and can be used to achieve either local ( e.g. , Alvarez - Melis and Jaakkola , 2017 ) or global ( e.g. , Liu et al . 
, 2018 ) explanations . 
However , the learned surrogate models and the original models may have completely different mechanisms to make predictions , leading to concerns about the Ô¨Ådelity of surrogate model - based approaches . 
Example - driven . 
Such approaches explain the prediction of an input instance by identifying and presenting other instances , usually from available labeled data , that are semantically similar to the input instance . 
They are similar in spirit to nearest neighbor - based approaches ( Dudani , 1976 ) , and have been applied to different NLP tasks such as text classiÔ¨Åcation ( Croce et al . 
, 2019 ) and question answering ( Abujabal et al . 
, 2017 ) . 
Provenance - based . 
Explanations are provided by illustrating some or all of the prediction derivation process , which is an intuitive and effective explainability technique when the Ô¨Ånal prediction is the result of a series of reasoning steps . 
We observe several question answering papers adopt such ap-449 Category Explainability Operations to Visualization Representative ( # ) Technique Enable Explainability Technique # Paper(s ) Local Post - Hoc ( 11)feature importanceÔ¨Årst derivative saliency , example drivensaliency 5 ( Wallace et al . 
, 2018 ; Ross et al . 
, 2017 ) surrogate model Ô¨Årst derivative saliency , layer - wise relevance propagation , input perturbationsaliency 4 ( Alvarez - Melis and Jaakkola , 2017 ; Poerner et al . 
, 2018 ; Ribeiro et al . 
, 2016 ) example driven layer - wise relevance propagation , explainability - aware architectureraw examples 2 ( Croce et al . 
, 2018 ; Jiang et al . 
, 2019 ) Local Self - Exp ( 35)feature importanceattention , Ô¨Årst derivative saliency , LSTM gating signals , explainabilityaware architecturesaliency 22 ( Mullenbach et al . 
, 2018 ; Ghaeini et al . 
, 2018 ; Xie et al . 
, 2017 ; Aubakirova and Bansal , 2016 ) induction explainability - aware architecture , rule inductionraw declarative representation6 ( Ling et al . 
, 2017 ; Dong et al . 
, 2019 ; Pezeshkpour et al . 
, 2019a ) provenance template - based natural language , other3 ( Abujabal et al . 
, 2017 ) surrogate model attention , input perturbation , explainability - aware architecturenatural language3 ( Rajani et al . 
, 2019a ; Sydorova et al . 
, 2019 ) example driven layer - wise relevance propagation raw examples 1 ( Croce et al . 
, 2019 ) Global Post - Hoc ( 3)feature importanceclass activation mapping , attention , gradient reversalsaliency 2 ( Pryzant et al . 
, 2018a , b ) surrogate model taxonomy induction raw declarative representation1 ( Liu et al . 
, 2018 ) Global Self - Exp ( 1)induction reinforcement learning raw declarative representation1 ( Pr ¬®ollochs et al . 
, 2019 ) Table 2 : Overview of common combinations of explanation aspects : columns 2 , 3 , and 4 capture explainability techniques , operations , and visualization techniques , respectively ( see Sections 4.1 , 4.2 , and 4.3 for details ) . 
These are grouped by the high - level categories detailed in Section 3 , as shown in the Ô¨Årst column . 
The last two columns show the number of papers in this survey that fall within each subgroup , and a list of representative references . 
proaches ( Abujabal et al . 
, 2017 ; Zhou et al . 
, 2018 ; Amini et al . 
, 2019 ) . 
Declarative induction . 
Human - readable representations , such as rules ( Pr ¬®ollochs et al . 
, 2019 ) , trees ( V oskarides et al . 
, 2015 ) , and programs ( Ling et al . 
, 2017 ) are induced as explanations . 
As shown in Table 2 , feature importance - based and surrogate model - based approaches have been in frequent use ( accounting for 29 and 8 , respectively , of the 50 papers reviewed ) . 
This should not come as a surprise , as features serve as building blocks for machine learning models ( explaining the proliferation of feature importance - based approaches ) and most recent NLP papers employ NNbased models , which are generally black - box models ( explaining the popularity of surrogate modelbased approaches ) . 
Finally note that a complex NLP approach consisting of different componentsmay employ more than one of these explainability techniques . 
A representative example is the QA system QUINT ( Abujabal et al . 
, 2017 ) , which displays the query template that best matches the user input query ( example - driven ) as well as the instantiated knowledge - base entities ( provenance ) . 
4.2 Operations to Enable Explainability We now present the most common set of operations encountered in our literature review that are used to enable explainability , in conjunction with relevant work employing each one . 
First - derivative saliency . 
Gradient - based explanations estimate the contribution of input itowards output oby computing the partial derivative ofowith respect to i. This is closely related to older concepts such as sensitivity ( Saltelli et al . 
, 2008 ) . 
First - derivative saliency is particularly con-450 venient for NN - based models because these can be computed for any layer using a single call to auto - differentiation , which most deep learning engines provide out - of - the - box . 
Recent work has also proposed improvements to Ô¨Årst - derivative saliency ( Sundararajan et al . 
, 2017 ) . 
As suggested by its name and deÔ¨Ånition , Ô¨Årst - derivative saliency can be used to enable feature importance explainability , especially on word / token - level features ( Aubakirova and Bansal , 2016 ; Karlekar et al . 
, 2018 ) . 
Layer - wise relevance propagation . 
This is another way to attribute relevance to features computed in any intermediate layer of an NN . 
DeÔ¨Ånitions are available for most common NN layers including fully connected layers , convolution layers and recurrent layers . 
Layer - wise relevance propagation has been used to , for example , enable feature importance explainability ( Poerner et al . 
, 2018 ) and example - driven explainability ( Croce et al . 
, 2018 ) . 
Input perturbations . 
Pioneered by LIME ( Ribeiro et al . 
, 2016 ) , input perturbations can explain the output for input xby generating random perturbations of xand training an explainable model ( usually a linear model ) . 
They are mainly used to enable surrogate models ( e.g. , Ribeiro et al . 
, 2016 ; Alvarez - Melis and Jaakkola , 2017 ) . 
Attention ( Bahdanau et al . 
, 2015 ; Vaswani et al . 
, 2017 ) . 
Less an operation and more of a strategy to enable the NN to explain predictions , attention layers can be added to most NN architectures and , because they appeal to human intuition , can help indicate where the NN model is ‚Äú focusing ‚Äù . 
While previous work has widely used attention layers ( Luo et al . 
, 2018 ; Xie et al . 
, 2017 ; Mullenbach et al . 
, 2018 ) to enable feature importance explainability , the jury is still out as to how much explainability attention provides ( Jain and Wallace , 2019 ; Serrano and Smith , 2019 ; Wiegreffe and Pinter , 2019 ) . 
LSTM gating signals . 
Given the sequential nature of language , recurrent layers , in particular LSTMs ( Hochreiter and Schmidhuber , 1997 ) , are commonplace . 
While it is common to mine the outputs of LSTM cells to explain outputs , there may also be information present in the outputs of the gates produced within the cells . 
It is possible to utilize ( and even combine ) other operations presented here to interpret gating signals to aid feature importance explainability ( Ghaeini et al . 
, 2018 ) . 
Explainability - aware architecture design . 
One way to exploit the Ô¨Çexibility of deep learning is to devise an NN architecture that mimics the processhumans employ to arrive at a solution . 
This makes the learned model ( partially ) interpretable since the architecture contains human - recognizable components . 
Implementing such a model architecture can be used to enable the induction of human - readable programs for solving math problems ( Amini et al . 
, 2019 ; Ling et al . 
, 2017 ) or sentence simpliÔ¨Åcation problems ( Dong et al . 
, 2019 ) . 
This design may also be applied to surrogate models that generate explanations for predictions ( Rajani et al . 
, 2019a ; Liu et al . 
, 2019 ) . 
Previous works have also attempted to compare these operations in terms of efÔ¨Åcacy with respect to speciÔ¨Åc NLP tasks ( Poerner et al . 
, 2018 ) . 
Operations outside of this list exist and are popular for particular categories of explanations . 
Table 2 mentions some of these . 
For instance , Pr ¬®ollochs et al . 
( 2019 ) use reinforcement learning to learn simple negation rules , Liu et al . 
( 2018 ) learns a taxonomy post - hoc to better interpret network embeddings , and Pryzant et al . 
( 2018b ) uses gradient reversal ( Ganin et al . 
, 2016 ) to deconfound lexicons . 
4.3 Visualization Techniques An explanation may be presented in different ways to the end user , and making the appropriate choice is crucial for the overall success of an XAI approach . 
For example , the widely used attention mechanism , which learns the importance scores of a set of features , can be visualized as raw attention scores or as a saliency heatmap ( see Figure 1a ) . 
Although the former is acceptable , the latter is more user - friendly and has become the standard way to visualize attention - based approaches . 
We now present the major visualization techniques identiÔ¨Åed in our literature review . 
Saliency . 
This has been primarily used to visualize the importance scores of different types of elements in XAI learning systems , such as showing input - output word alignment ( Bahdanau et al . 
, 2015 ) ( Figure 1a ) , highlighting words in input text ( Mullenbach et al . 
, 2018 ) ( Figure 1b ) or displaying extracted relations ( Xie et al . 
, 2017 ) . 
We observe a strong correspondence between feature importancebased explainability and saliency - based visualizations ; namely , all papers using feature importance to generate explanations also chose saliency - based visualization techniques . 
Saliency - based visualizations are popular because they present visually perceptive explanations and can be easily understood by different types of end users . 
They are there-451 ( a ) Saliency heatmap ( Bahdanau et al . 
, 2015 ) ( b ) Saliency highlighting ( Mullenbach et al . 
, 2018 ) ( c ) Raw declarative rules ( Pezeshkpour et al . 
, 2019b ) ( d ) Raw declarative program ( Amini et al . 
, 2019 )   ( e ) Raw examples ( Croce et al . 
, 2019 ) Figure 1 : Examples of different visualization techniques fore frequently seen across different AI domains ( e.g. , computer vision ( Simonyan et al . 
, 2013 ) and speech ( Aldeneh and Provost , 2017 ) ) . 
As shown in Table 2 , saliency is the most dominant visualization technique among the papers covered by this survey . 
Raw declarative representations . 
As suggested by its name , this visualization technique directly presents the learned declarative representations , such as logic rules , trees , and programs ( Figure 1c and 1d ) . 
Such techniques assume that end users can understand speciÔ¨Åc representations , such as Ô¨Årstorder logic rules ( Pezeshkpour et al . 
, 2019a ) and reasoning trees ( Liang et al . 
, 2016 ) , and therefore may implicitly target more advanced users . 
Natural language explanation . 
The explanation is verbalized in human - comprehensible natural language ( Figure 2 ) . 
The natural language can be generated using sophisticated deep learning models , e.g. , by training a language model with human natural language explanations and coupling with a deep generative model ( Rajani et al . 
, 2019a ) . 
It can also be generated by using simple templatebased approaches ( Abujabal et al . 
, 2017 ) . 
In fact , many declarative induction - based techniques can use template - based natural language generation ( Reiter and Dale , 1997 ) to turn rules and programs into human - comprehensible language , and this minor extension can potentially make the explanation more accessible to lay users . 
Table 2 references some additional visualization techniques , such as using raw examples to Figure 2 : Template - based natural language explanation for a QA system ( Abujabal et al . 
, 2017 ) . 
present example - driven approaches ( Jiang et al . 
, 2019 ; Croce et al . 
, 2019 ) ( e.g. , Figure 1e ) , and dependency parse trees to represent input questions ( Abujabal et al . 
, 2017 ) . 
5 Explanation Quality Following the goals of XAI , a model ‚Äôs quality should be evaluated not only by its accuracy and performance , but also by how well it provides explanations for its predictions . 
In this section we discuss the state of the Ô¨Åeld in terms of deÔ¨Åning and measuring explanation quality . 
5.1 Evaluation Given the young age of the Ô¨Åeld , unsurprisingly there is little agreement on how explanations should be evaluated . 
The majority of the works reviewed ( 32 out of 50 ) either lack a standardized evaluation or include only an informal evaluation , while a smaller number of papers looked at more formal evaluation approaches , including leveraging ground truth data and human evaluation . 
We next present the major categories of evaluation tech-452 niques we encountered ( summarized in Table 3 ) . 
None or Informal Comparison to Human Examination only Ground Truth Evaluation 32 12 9 Table 3 : Common evaluation techniques and number of papers adopting them , out of the 50 papers surveyed ( note that some papers adopt more than one technique ) Informal examination of explanations . 
This typically takes the form of high - level discussions of how examples of generated explanations align with human intuition . 
This includes cases where the output of a single explainability approach is examined in isolation ( Xie et al . 
, 2017 ) as well as when explanations are compared to those of other reference approaches ( Ross et al . 
, 2017 ) ( such as LIME , which is a frequently used baseline ) . 
Comparison to ground truth . 
Several works compare generated explanations to ground truth data in order to quantify the performance of explainability techniques . 
Employed metrics vary based on task and explainability technique , but commonly encountered metrics include P / R / F1 ( Carton et al . 
, 2018 ) , perplexity , and BLEU ( Ling et al . 
, 2017 ; Rajani et al . 
, 2019b ) . 
While having a quantitative way to measure explainability is a promising direction , care should be taken during ground truth acquisition to ensure its quality and account for cases where there may be alternative valid explanations . 
Approaches employed to address this issue involve having multiple annotators and reporting inter - annotator agreement or mean human performance , as well as evaluating the explanations at different granularities ( e.g. , token - wise vs phrasewise ) to account for disagreements on the precise value of the ground truth ( Carton et al . 
, 2018 ) . 
Human evaluation . 
A more direct way to assess the explanation quality is to ask humans to evaluate the effectiveness of the generated explanations . 
This has the advantage of avoiding the assumption that there is only one good explanation that could serve as ground truth , as well as sidestepping the need to measure similarity of explanations . 
Here as well , it is important to have multiple annotators , report inter - annotator agreement , and correctly deal with subjectivity and variance in the responses . 
The approaches found in this survey vary in several dimensions , including the number of humans involved ( ranging from 1 ( Mullenbach et al . 
, 2018 ) to 25 ( Sydorova et al . 
, 2019 ) humans ) , as well as thehigh - level task that they were asked to perform ( including rating the explanations of a single approach ( Dong et al . 
, 2019 ) and comparing explanations of multiple techniques ( Sydorova et al . 
, 2019 ) ) . 
Other operation - speciÔ¨Åc techniques . 
Given the prevalence of attention layers ( Bahdanau et al . 
, 2015 ; Vaswani et al . 
, 2017 ) in NLP , recent work ( Jain and Wallace , 2019 ; Serrano and Smith , 2019 ; Wiegreffe and Pinter , 2019 ) has developed speciÔ¨Åc techniques to evaluate such explanations based on counterfactuals or erasure - based tests ( Feng et al . 
, 2018 ) . 
Serrano and Smith repeatedly set to zero the maximal entry produced by the attention layer . 
If attention weights indeed ‚Äú explain ‚Äù the output prediction , then turning off the dominant weights should result in an altered prediction . 
Similar experiments have been devised by others ( Jain and Wallace , 2019 ) . 
In particular , Wiegreffe and Pinter caution against assuming that there exists only one true explanation to suggest accounting for the natural variance of attention layers . 
On a broader note , causality has thoroughly explored such counterfactualbased notions of explanation ( Halpern , 2016 ) . 
While the above overview summarizes how explainability approaches are commonly evaluated , another important aspect is what is being evaluated . 
Explanations are multi - faceted objects that can be evaluated on multiple aspects , such as Ô¨Ådelity ( how much they reÔ¨Çect the actual workings of the underlying model ) , comprehensibility ( how easy they are to understand by humans ) , and others . 
Therefore , understanding the target of the evaluation is important for interpreting the evaluation results . 
We refer interested readers to ( Carvalho et al . 
, 2019 ) for a comprehensive presentation of aspects of evaluating approaches . 
Many works do not explicitly state what is being evaluated . 
As a notable exception , ( Lertvittayakumjorn and Toni , 2019 ) outlines three goals of explanations ( reveal model behavior , justify model predictions , and assist humans in investigating uncertain predictions ) and proposes human evaluation experiments targeting each of them . 
5.2 Predictive Process Coverage An important and often overlooked aspect of explanation quality is the part of the prediction process ( starting with the input and ending with the model output ) covered by an explanation . 
We have observed that many explainability approaches explain only part of this process , leaving it up to the end453 user to Ô¨Åll in the gaps . 
As an example , consider the MathQA task of solving math word problems . 
As readers may be familiar from past education experience , in math exams , one is often asked to provide a step - by - step explanation of how the answer was derived . 
Usually , full credit is not given if any of the critical steps used in the derivation are missing . 
Recent works have studied the explainability of MathQA models , which seek to reproduce this process ( Amini et al . 
, 2019 ; Ling et al . 
, 2017 ) , and have employed different approaches in the type of explanations produced . 
While ( Amini et al . 
, 2019 ) explains the predicted answer by showing the sequence of mathematical operations leading to it , this provides only partial coverage , as it does not explain how these operations were derived from the input text . 
On the other hand , the explanations produced by ( Ling et al . 
, 2017 ) augment the mathematical formulas with text describing the thought process behind the derived solution , thus covering a bigger part of the prediction process . 
The level of coverage may be an artifact of explainability techniques used : provenance - based approaches tend to provide more coverage , while example - driven approaches , may provide little to no coverage . 
Moreover , while our math teacher would argue that providing higher coverage is always beneÔ¨Åcial to the student , in reality this may depend on the end use of the explanation . 
For instance , the coverage of explanations of ( Amini et al . 
, 2019 ) may be potentially sufÔ¨Åcient for advanced technical users . 
Thus , higher coverage , while in general a positive aspect , should always be considered in combination with the target use and audience of the produced explanations . 
6 Insights and Future Directions This survey showcases recent advances of XAI research in NLP , as evidenced by publications in major NLP conferences in the last 7 years . 
We have discussed the main categorization of explanations ( Local vs Global , Self - Explaining vs Post - Hoc ) as well as the various ways explanations can be arrived at and visualized , together with the common techniques used . 
We have also detailed operations and explainability techniques currently available for generating explanations of model predictions , in the hopes of serving as a resource for developers interested in building explainable NLP models . 
We hope this survey encourages the researchcommunity to work in bridging the current gaps in the Ô¨Åeld of XAI in NLP . 
The Ô¨Årst research direction is a need for clearer terminology and understanding of what constitutes explainability and how it connects to the target audience . 
For example , is a model that displays an induced program that , when executed , yields a prediction , and yet conceals the process of inducing the program , explainable in general ? Or is it explainable for some target users but not for others ? The second is an expansion of the evaluation processes and metrics , especially for human evaluation . 
The Ô¨Åeld of XAI is aimed at adding explainability as a desired feature of models , in addition to the model ‚Äôs predictive quality , and other features such as runtime performance , complexity or memory usage . 
In general , trade - offs exist between desired characteristics of models , such as more complex models achieving better predictive power at the expense of slower runtime . 
In XAI , some works have claimed that explainability may come at the price of losing predictive quality ( Bertsimas et al . 
, 2019 ) , while other have claimed the opposite ( Garneau et al . 
, 2018 ; Liang et al . 
, 2016 ) . 
Studying such possible trade - offs is an important research area for XAI , but one that can not advance until standardized metrics are developed for evaluating the quality of explanations . 
The third research direction is a call to more critically address the issue of Ô¨Ådelity ( or causality ) , and to ask hard questions about whether a claimed explanation is faithfully explaining the model ‚Äôs prediction . 
Finally , it is interesting to note that we found only four papers that fall into the global explanations category . 
This might seem surprising given that white box models , which have been fundamental in NLP , are explainable in the global sense . 
We believe this stems from the fact that because white box models are clearly explainable , the focus of the explicit XAI Ô¨Åeld is in explaining black box models , which comprise mostly local explanations . 
White box models , like rule based models and decision trees , while still in use , are less frequently framed as explainable or interpretable , and are hence not the main thrust of where the Ô¨Åeld is going . 
We think that this may be an oversight of the Ô¨Åeld since white box models can be a great test bed for studying techniques for evaluating explanations . 
Acknowledgments We thank the anonymous reviewers for their valuable feedback . 
We also thank Shipi Dhanorkar,454 Yunyao Li , Lucian Popa , Christine T Wolf , and Anbang Xu for their efforts at the early stage of this work . 
Abstract Fine - tuning ( FT ) pre - trained sentence embedding models on small datasets has been shown to have limitations . 
In this paper we show that concatenating the embeddings from the pretrained model with those from a simple sentence embedding model trained only on the target data , can improve over the performance of FT for few - sample tasks . 
To this end , a linear classiÔ¨Åer is trained on the combined embeddings , either by freezing the embedding model weights or training the classiÔ¨Åer and embedding models end - to - end . 
We perform evaluation on seven small datasets from NLP tasks and show that our approach with end - to - end training outperforms FT with negligible computational overhead . 
Further , we also show that sophisticated combination techniques like CCA and KCCA do not work as well in practice as concatenation . 
We provide theoretical analysis to explain this empirical observation . 
1 Introduction Fine - tuning ( FT ) powerful pre - trained sentence embedding models like BERT ( Devlin et al . 
, 2018 ) has recently become the de - facto standard for downstream NLP tasks . 
Typically , FT entails jointly learning a classiÔ¨Åer over the pre - trained model while tuning the weights of the latter . 
While FT has been shown to improve performance on tasks like GLUE ( Wang et al . 
, 2018 ) having large datasets ( QQP , MNLI , QNLI ) , similar trends have not been observed on small datasets , where one would expect the maximum beneÔ¨Åts of using a pre - trained model . 
Several works ( Phang et al . 
, 2018 ; Garg et al . 
, 2019 ; Dodge et al . 
, 2020 ; Lee et al . 
, 2020 ) have demonstrated that FT with a few target domain samples is unstable with high variance , thereby often leading to sub - par gains . 
Furthermore , this ‚àóEqual contribution by authors ‚Ä†Work completed at the University of Wisconsin - Madisonissue has also been well documented in practice1 . 
Learning with low resources has recently become an active research area in NLP , and arguably one of the most interesting scenarios for which pre - trained models are useful ( e.g. , ( Cherry et al . 
, 2019 ) ) . 
Many practical applications have small datasets ( e.g. , in social science , medical studies , etc ) , which are different from large - scale academic benchmarks having hundreds of thousands of training samples ( e.g , DBpedia ( Lehmann et al . 
, 2015 ) , Sogou News ( Wang et al . 
, 2008 ) , etc ) . 
This necessitates effective transfer learning approaches using pre - trained sentence embedding models for fewsample tasks . 
In this work , we show that concatenating sentence embeddings from a pre - trained model and those from a smaller model trained solely on the target data , can improve over the performance of FT . 
SpeciÔ¨Åcally , we Ô¨Årst learn a simple sentence embedding model on the target data . 
Then we concatenate ( CAT ) the embeddings from this model with those from a pre - trained model , and train a linear classiÔ¨Åer on the combined representation . 
The latter can be done by either freezing the embedding model weights or training the whole network ( classiÔ¨Åer plus the two embedding models ) end - to - end . 
We evaluate our approach on seven small datasets from NLP tasks . 
Our results show that our approach with end - to - end training can signiÔ¨Åcantly improve the prediction performance of FT , with less than a 10 % increase in the run time . 
Furthermore , our approach with frozen embedding models performs better than FT for very small datasets while reducing the run time by 30%‚àí50 % , and without the requirement of large memory GPUs . 
We also conduct evaluations of multiple techniques for combining the pre - trained and domainspeciÔ¨Åc embeddings , comparing concatenation to 1Issues numbered 265 , 1211 onhttps://github.com/huggi ngface / transformers / issues/460 CCA and KCCA . 
We observe that the simplest approach of concatenation works best in practice . 
Moreover , we provide theoretical analysis to explain this empirical observation . 
Finally , our results also have implications on the semantics learning ability of small domainspeciÔ¨Åc models compared to large pre - trained models . 
While intuition dictates that a large pre - trained model should capture the entire semantics learned by a small domain - speciÔ¨Åc model , our results show that there exist semantic features captured solely by the latter and not by the former , in spite of pretraining on billions of words . 
Hence combining the embeddings can improve the performance of directly FT the pre - trained model . 
Related Work Recently , several pre - trained models have been studied , of which some provide explicit sentence embeddings ( Conneau et al . 
, 2017 ; Subramanian et al . 
, 2018 ) , while others provide implicit ones ( Howard and Ruder , 2018 ; Radford et al . 
, 2018 ) . 
Peters et al . 
( 2019 ) compare the performance of feature extraction ( by freezing the pre - trained weights ) and FT . 
There exists other more sophisticated transferring methods , but they are typically much more expensive or complicated . 
For example , Xu et al . 
( 2019 ) ‚Äú post - train ‚Äù the pretrained model on the target dataset , Houlsby et al . 
( 2019 ) inject speciÔ¨Åcally designed new adapter layers , Arase and Tsujii ( 2019 ) inject phrasal paraphrase relations into BERT , Sun et al . 
( 2019 ) use multi - task FT , and Wang et al . 
( 2019 ) Ô¨Årst train a deep network classiÔ¨Åer on the Ô¨Åxed pre - trained embedding and then Ô¨Åne - tune it . 
Our focus is to propose alternatives to FT with similar simplicity and computational efÔ¨Åciency , and study conditions where it has signiÔ¨Åcant advantages . 
While the idea of concatenating multiple embeddings has been previously used ( Peters et al . 
, 2018 ) , we use it for transfer learning in a low resource target domain . 
2 Methodology We are given a set of labeled training sentences S={(si , yi)}m i=1from a target domain and a pretrained sentence embedding model f1 . 
Denote the embedding of sfromf1byv1s = f1(s)‚ààRd1 . 
Here f1is assumed to be a large and powerful embedding model such as BERT . 
Our goal is to transfer f1effectively to the target domain using S. We propose to use a second sentence embedding model f2 , which is different from and typically much smaller thanf1 , which has been trained solely on S. Thesmall size of f2is necessary for efÔ¨Åcient learning on the small target dataset . 
Let v2s = f2(s)‚ààRd2 denote the embedding for sobtained from f2 . 
Our method CATconcatenates v1sandv2sto get an adaptive representation ¬Øvs=[v / latticetop 1s , Œ±v / latticetop 2s]/latticetopfors . 
HereŒ±>0is a hyper - parameter to modify emphasis onv1sandv2s . 
It then trains a linear classiÔ¨Åer c(¬Øvs ) usingSin the following two ways : ( a ) Frozen Embedding Models ·Ωë2Only training the classiÔ¨Åer cwhile Ô¨Åxing the weights of embedding models f1andf2 . 
This approach is computationally cheaper than FT f1since onlycis trained . 
We denote this by C AT·Ωë2(Lockedf1,f2weights ) . 
( b ) Trainable Embedding Models /lock - open Jointly training classiÔ¨Åer c , and embedding models f1,f2 in an end - to - end fashion . 
We refer to this as C AT / lock - open . 
The inspiration for combining embeddings from two different models f1,f2stems from the impressive empirical gains of ensembling ( Dietterich , 2000 ) in machine learning . 
While typical ensembling techniques like bagging and boosting aggregate predictions from individual models , C AT·Ωë2and CAT / lock - openaggregate the embeddings from individual models and train a classiÔ¨Åer using Sto get the predictions . 
Note that C AT·Ωë2keeps the model weights off1,f2frozen , while C AT / lock - openinitializes the weights off2after initially training on S2 . 
One of the beneÔ¨Åts of C AT·Ωë2and C AT / lock - openis that they treatf1as a black box and do not access its internal architecture like other variants of FT ( Houlsby et al . 
, 2019 ) . 
Additionally , we can theoretically guarantee that the concatenated embedding will generalize well to the target domain under assumptions on the loss function and embedding models . 
2.1 Theoretical Analysis Assume there exists a ‚Äú ground - truth ‚Äù embedding vectorv‚àó sfor each sentence swith labelys , and a ‚Äú ground - truth ‚Äù linear classiÔ¨Åer f‚àó(s)=/angbracketleftw‚àó,v‚àó s / angbracketright with a small loss L(f‚àó)=Es[/lscript(f‚àó(s),ys)]w.r.t . 
some loss function /lscript(such as cross - entropy ) , where Esdenotes the expectation over the true data distribution . 
The superior performance of C AT / lock - openin practice ( see Section 3 ) suggests that there exists a linear relationship between the embeddings v1s , v2s andv‚àó s. Thus we assume a theoretical model : v1s = P1v‚àó s+/epsilon11;v2s = P2v‚àó s+/epsilon12where / epsilon1i ‚Äôs are noises independent of v‚àó swith variances œÉ2 i ‚Äôs . 
If we denoteP / latticetop=[P / latticetop 1,P / latticetop 2]and / epsilon1 / latticetop=[/epsilon1 / latticetop 1,/epsilon1 / latticetop 2 ] , then 2We empirically observe that C AT / lock - openby randomly initializing weights of f2performs similar to Ô¨Åne - tuning only f1461 the concatenation ¬Øvs=[v / latticetop 1s , v / latticetop 2s]/latticetopis¬Øvs = Pv‚àó s+/epsilon1 . 
LetœÉ=/radicalbig œÉ2 1+œÉ2 2 . 
We present the following theorem which guarantees the existence of a ‚Äú good ‚Äù classiÔ¨Åer ¬Øfover¬Øvs : Theorem 1 . 
If the loss function LisŒª - Lipschitz for the Ô¨Årst parameter , and Phas full column rank , then there exists a linear classiÔ¨Åer ¬Øfover¬Øvssuch thatL(¬Øf)‚â§L(f‚àó ) + ŒªœÉ / bardbl(P‚Ä†)/latticetopw‚àó/bardbl2whereP‚Ä†is the pseudo - inverse of P. Proof . 
Let¬Øfhave weight ¬Øw= ( P‚Ä†)/latticetopw‚àó. Then /angbracketleft¬Øw,¬Øvs / angbracketright=/angbracketleft(P‚Ä†)/latticetopw‚àó,Pv‚àó s+/epsilon1 / angbracketright = /angbracketleft(P‚Ä†)/latticetopw‚àó,Pv‚àó s / angbracketright+/angbracketleft(P‚Ä†)/latticetopw‚àó,/epsilon1 / angbracketright = /angbracketleftw‚àó,P‚Ä†Pv‚àó s / angbracketright+/angbracketleft(P‚Ä†)/latticetopw‚àó,/epsilon1 / angbracketright = /angbracketleftw‚àó,v‚àó s / angbracketright+/angbracketleft(P‚Ä†)/latticetopw‚àó,/epsilon1 / angbracketright . 
( 1 ) Then the difference in the losses is given by L(¬Øf)‚àíL(f‚àó ) = Es[/lscript(¬Øf(s),ys)‚àí/lscript(f‚àó(s),ys ) ] ‚â§ŒªEs|¬Øf(s)‚àíf‚àó(s)| ( 2 ) = ŒªEs|/angbracketleft(P‚Ä†)/latticetopw‚àó,/epsilon1 / angbracketright| . 
‚â§Œª / radicalBig Es / angbracketleft(P‚Ä†)/latticetopw‚àó,/epsilon1 / angbracketright2 ( 3 ) ‚â§Œª / radicalBig Es / bardbl(P‚Ä†)/latticetopw‚àó/bardbl2 2 / bardbl / epsilon1 / bardbl2 2 ( 4 ) = ŒªœÉ / bardbl(P‚Ä†)/latticetopw‚àó/bardbl2 where we use the Lipschitz - ness of Lin Equation 2 , Jensen ‚Äôs inequality in Equation 3 , and CauchySchwarz inequality in Equation 4 . 
More intuitively , if the SVD of P = UŒ£V / latticetop , then /bardbl(P‚Ä†)/latticetopw‚àó/bardbl2=/bardbl(Œ£‚Ä†)/latticetopV / latticetopw‚àó/bardbl2 . 
So if the top right singular vectors in Valign well with w‚àó , then /bardbl(P‚Ä†)/latticetopw‚àó/bardbl2will be small in magnitude . 
This means that if P1andP2together cover the directionw‚àó , they can capture information important for classiÔ¨Åcation . 
And thus there exists a good classiÔ¨Åer¬Øfon¬Øvs . 
Additional explanation is presented in Appendix A.1 . 
2.2 Do Other Combination Methods Work ? There are several sophisticated techniques to combinev1sandv2sother than concatenation . 
Since v1sandv2smay be in different dimensions , a dimension reduction technique which projects them on the same dimensional space might work better at capturing the general and domain speciÔ¨Åc information . 
We consider two popular techniques : CCA Canonical Correlation Analysis ( Hotelling , 1936 ) learns linear projections Œ¶1andŒ¶2into dimensiondto maximize the correlations betweenthe projections{Œ¶1v1si}and{Œ¶2v2si } . 
We use ¬Øv / latticetop s=1 2Œ¶1v1si+1 2Œ¶2v2siwithd= min{d1,d2 } . 
KCCA Kernel Canonical Correlation Analysis ( Sch ¬®olkopf et al . 
, 1998 ) Ô¨Årst applies nonlinear projections g1andg2and then CCA on { g1(v1si)}m i=1and{g2(v2si)}m i=1 . 
We used= min{d1,d2}and¬Øv / latticetop s=1 2g1(v1si ) +1 2g2(v2si ) . 
We empirically evaluate C CA·Ωë2and K CCA·Ωë2and our results ( see Section 3 ) show that the former two perform worse than C AT·Ωë2 . 
Further , C CA·Ωë2performs even worse than the individual embedding models . 
This is a very interesting negative observation , and below we provide an explanation for this . 
We argue that even when v1sandv2scontain information important for classiÔ¨Åcation , CCA of the two embeddings can eliminate this and just retain the noise in the embeddings , thereby leading to inferior prediction performance . 
Theorem 2 constructs such an example . 
Theorem 2 . 
Let¬Øvsdenote the embedding for sentencesobtained by concatenation , and Àúvsdenote that obtained by CCA . 
There exists a setting of the data andw‚àó,P,/epsilon1 such that there exists a linear classiÔ¨Åer ¬Øfon¬Øvswith the same loss as f‚àó , while CCA achieves the maximum correlation but any classiÔ¨Åer on Àúvsis at best random guessing . 
Proof . 
Suppose we perform CCA to get ddimensional Àúvs . 
Supposev‚àó shasd+ 2dimensions , each dimension being an independent Gaussian . 
Supposew‚àó=[1,1,0 , ... , 0]/latticetop , and the label for the sentencesisys=1if / angbracketleftw‚àó,v‚àó s / angbracketright‚â•0andys=0otherwise . 
Suppose /epsilon1=0,P1 = diag(1,0,1 , ... , 1 ) , and P2 = diag(0,1,1 , ... , 1 ) . 
Let the linear classiÔ¨Åer ¬Øfhave weights [ 1,0,0,0,1,0]/latticetopwhere 0is the zero vector ofddimensions . 
Clearly , ¬Øf(s)=f‚àó(s)for anys , so it has the same loss as f‚àó. For CCA , since the coordinates of v‚àó sare independent Gaussians , v1sandv2sonly have correlation in the lastddimensions . 
Solving the CCA optimization , the projection matrices for both embeddings are the same œÜ = diag(0,0,1 , ... , 1 ) which achieves the maximum correlation . 
Then the CCA embedding is Àúvs= [ 0,0,(v‚àó s)3:(d+2)]where ( v‚àó s)3:(d+2)are the lastddimensions of v‚àó s , which contains no information about the label . 
Therefore , any classiÔ¨Åer on Àúvsis at best random guessing . 
The intuition for this is that v1sandv2sshare com-462 mon information while each has some special information for the classiÔ¨Åcation . 
If the two sets of special information are uncorrelated , then they will be eliminated by CCA . 
Now , if the common information is irrelevant to the labels , then the best any classiÔ¨Åer can do with the CCA embeddings is just random guessing . 
This is a fundamental drawback of the unsupervised CCA technique , clearly demonstrated by the extreme example in the theorem . 
In practice , the common information can contain some relevant information , so CCA embeddings are worse than concatenation but better than random guessing . 
KCCA can be viewed as CCA on a nonlinear transformation of v1sandv2swhere the special information gets mixed non - linearly and can not be separated out and eliminated by CCA . 
This explains why the poor performance of C CA·Ωë2 is not observed for K CCA·Ωë2 in Table 2 . 
We present additional empirical veriÔ¨Åcation of Theorem 2 in Appendix A.2 . 
3 Experiments Datasets We evaluate our approach on seven low resource datasets from NLP text classiÔ¨Åcation tasks like sentiment classiÔ¨Åcation , question type classiÔ¨Åcation , opinion polarity detection , subjectivity classiÔ¨Åcation , etc . 
We group these datasets into 2 categories : the Ô¨Årst having a few hundred training samples ( which we term as very small datasets for the remainder of the paper ) , and the second having a few thousand training samples ( which we term as small datasets ) . 
We consider the following 3 very small datasets : Amazon ( product reviews ) , IMDB ( movie reviews ) and Yelp ( food article reviews ) ; and the following 4 small datasets : MR ( movie reviews ) , MPQA ( opinion polarity ) , TREC ( question - type classiÔ¨Åcation ) and SUBJ ( subjectivity classiÔ¨Åcation ) . 
We present the statistics of the datasets in Table 1 and provide the details and downloadable links in Appendix B.1 . 
Dataset c N |V| Test Amazon ( Sarma et al . 
, 2018 ) 2 1000 1865 100 IMDB ( Sarma et al . 
, 2018 ) 2 1000 3075 100 Yelp ( Sarma et al . 
, 2018 ) 2 1000 2049 100 MR ( Pang and Lee , 2005 ) 2 10662 18765 1067 MPQA ( Wiebe and Wilson , 2005 ) 2 10606 6246 1060 TREC ( Li and Roth , 2002 ) 6 5952 9592 500 SUBJ ( Pang and Lee , 2004 ) 2 10000 21323 1000 Table 1 : Dataset statistics . 
c : Number of classes , N : Dataset size,|V| : V ocabulary size , Test : Test set size ( if no standard test set is provided , we use a random train / dev / test split of 80 / 10 / 10 % ) Amazon Yelp IMDB BERT No - FT 93.1 90.2 91.6 BERT FT 94.0 91.7 92.3 Adapter 94.3 93.5 90.5 CNN - R 91.1 92.7 93.2 CCA·Ωë2(CNN - R ) 79.1 71.5 80.8 KCCA·Ωë2(CNN - R ) 91.5 91.5 94.1 CAT·Ωë2(CNN - R ) 93.2 96.5 96.2 CAT / lock - open(CNN - R ) 94.0 96.2 97.0 CNN - S 94.7 95.2 96.6 CCA·Ωë2(CNN - S ) 83.6 67.8 83.3 KCCA·Ωë2(CNN - S ) 94.3 91.9 97.9 CAT·Ωë2(CNN - S ) 95.3 97.1 98.1 CAT / lock - open(CNN - S ) 95.7 97.2 98.3 CNN - NS 95.9 95.8 96.8 CCA·Ωë2(CNN - NS ) 81.3 69.4 85.0 KCCA·Ωë2(CNN - NS ) 95.8 96.2 97.2 CAT·Ωë2(CNN - NS ) 96.4 98.3 98.3 CAT / lock - open(CNN - NS ) 96.8 98.3 98.4 Table 2 : Evaluation on very small datasets . 
C CA·Ωë2 ( ¬∑ ) / KCCA·Ωë2 ( ¬∑ ) / C AT·Ωë2 ( ¬∑ ) / C AT / lock - open ( ¬∑ ) refers to using a speciÔ¨Åc CNN variant as f2 . 
Best results for each CNN variant in boldface . 
Models for Evaluation We use the BERT ( Devlin et al . 
, 2018 ) base uncased model as the pre - trained modelf1 . 
We choose a Text - CNN ( Kim , 2014 ) model as the domain speciÔ¨Åc model f2with 3 approaches to initialize the word embeddings : randomly initialized ( C NN - R ) , static GloVe ( Pennington et al . 
, 2014 ) vectors ( C NN - S ) and trainable GloVe vectors ( C NN - NS ) . 
We use a regularized logistic regression as the classiÔ¨Åer c. We present the model and training details along with the chosen hyperparameters in Appendix B.2 - B.3 . 
We also present results with two other popular pre - trained models : GenSen and InferSent in Appendix C.2 . 
We consider two baselines : ( i ) BERT Ô¨Ånetuning ( denoted by BERT FT ) and ( ii ) learningcover frozen pre - trained BERT weights ( denoted by BERT No - FT ) . 
We also present the Adapter ( Houlsby et al . 
, 2019 ) approach as a baseline , which injects new adapters in BERT followed by selectively training the adapters while freezing the BERT weights , to compare with C AT·Ωë2since neither Ô¨Åne - tunes the BERT parameters . 
Results on Very Small Datasets On the 3 very small datasets , we present results averaged over 10 runs in Table 2 . 
The key observations are summarized as follows : ( i ) C AT·Ωë2and C AT / lock - openalmost always beat the accuracy of the baselines ( BERT FT , Adapter ) showing their effectiveness in transferring knowledge from the463 MR MPQA SUBJ TREC BERT No - FT 83.26 87.44 95.96 88.06 BERT FT 86.22 90.47 96.95 96.40 Adapter 85.55 90.40 97.40 96.55 CNN - NS 80.93 88.38 89.25 92.98 CAT·Ωë2(CNN - NS ) 85.60 90.06 95.92 96.64 CAT / lock - open(CNN - NS)87.15 91.19 97.60 97.06 Table 3 : Performance of C AT·Ωë2and C AT / lock - openusing C NN - NS and BERT on small datasets . 
Best results in boldface . 
general domain to the target domain . 
( ii ) Both the C CA·Ωë2 , K CCA·Ωë2(computationally expensive ) get inferior performance than C AT·Ωë2 . 
Similar trends for GenSen and InferSent in Appendix C.2 . 
( iii ) C AT / lock - openperforms better than C AT·Ωë2 , but at an increased computational cost . 
The execution time for the latter is the time taken to train the text - CNN , extract BERT embeddings , concatenate them , and train a classiÔ¨Åer on the combination . 
On an average run on the Amazon dataset , C AT·Ωë2requires about 125 s , reducing around 30 % of the 180 s for BERT FT . 
Additionally , C AT·Ωë2has small memory requirements as it can be computed on a CPU in contrast to BERT FT which requires , at minimum , a 12 GB memory GPU . 
The total time for C AT / lock - openis 195 s , which is less than a 9%increase over FT . 
It also has a negligible 1.04 % increase in memory ( the number of parameters increases from 109,483,778 to 110,630,332 due to the text - CNN ) . 
Results on Small Datasets We use the best performing C NN - NS model and present the results in Table 3 . 
Again , C AT / lock - openachieves the best performance on all the datasets improving the performance of BERT FT and Adapter . 
C AT·Ωë2can achieve comparable test accuracy to BERT FT on all the tasks while being much more computationally efÔ¨Åcient . 
On an average run on the MR dataset , C AT·Ωë2(290 s ) reduces the time of BERT FT ( 560 s ) by about 50 % , while C AT / lock - open(610 s ) only incurs an increase of about 9%over BERT FT . 
Comparison with Adapter CAT·Ωë2can outperform Adapter for very small datasets and perform comparably on small datasets having 2 advantages : ( i ) We do not need to open the BERT model and access its parameters to introduce intermediate layers and hence our method is modular applicable to multiple pre - trained models . 
( ii ) On very small datasets like Amazon , C AT·Ωë2introduces roughly only 1%extra parameters as compared to the 3‚àí4%of Adapter thereby being more parameter efÔ¨Åcient . 
However note that this increase Figure 1 : Comparing test accuracy of C AT·Ωë2and C AT / lock - open on MR dataset with varying training dataset size . 
in the number of parameters due to the text - CNN is a function of the vocabulary size of the dataset as it includes the word embeddings which are fed as input to the text - CNN . 
For a dataset having a larger vocabulary size like SUBJ3 , Adapter might be more parameter efÔ¨Åcient than C AT·Ωë2 . 
Effect of Dataset Size We study the effect of size of data on the performance of our method by varying the training data of the MR dataset via random sub - sampling . 
From Figure 1 , we observe that C AT / lock - opengets the best results across all training data sizes , signiÔ¨Åcantly improving over BERT FT . 
CAT·Ωë2gets performance comparable to BERT FT on a wide range of data sizes , from 500 points on . 
We present qualitative analysis and complete results with error bounds in Appendix C. 4 Conclusion In this paper we have proposed a simple method for transferring a pre - trained sentence embedding model for text classiÔ¨Åcation tasks . 
We empirically show that concatenating pre - trained and domain speciÔ¨Åc sentence embeddings , learned on the target dataset , with or without Ô¨Åne - tuning can improve the classiÔ¨Åcation performance of pre - trained models like BERT on small datasets . 
We have also provided theoretical analysis identifying the conditions when this method is successful and to explain the experimental results . 
Acknowledgements This work was supported in part by FA9550 - 181 - 0166 . 
The authors would also like to acknowledge the support provided by the University of Wisconsin - Madison OfÔ¨Åce of the Vice Chancellor for Research and Graduate Education with funding from the Wisconsin Alumni Research Foundation . 
3For SUBJ , the embeddings alone contribute 6,396,900 additional parameters ( 5.84 % of parameters of BERT - Base)464 Abstract Learning speciÔ¨Åc hands - on skills such as cooking , car maintenance , and home repairs increasingly happens via instructional videos . 
The user experience with such videos is known to be improved by meta - information such as time - stamped annotations for the main steps involved . 
Generating such annotations automatically is challenging , and we describe here two relevant contributions . 
First , we construct and release a new dense video captioning dataset , VideoTimeline Tags ( ViTT ) , featuring a variety of instructional videos together with time - stamped annotations . 
Second , we explore several multimodal sequenceto - sequence pretraining strategies that leverage large unsupervised datasets of videos and caption - like texts . 
We pretrain and subsequently Ô¨Ånetune dense video captioning models using both YouCook2 and ViTT . 
We show that such models generalize well and are robust over a wide variety of instructional videos . 
1 Introduction YouTube recently reported that a billion hours of videos were being watched on the platform every day ( YouTubeBlog , 2017 ) . 
In addition , the amount of time people spent watching online videos was estimated to grow at an average rate of 32 % a year between 2013 and 2018 , with an average person forecasted to watch 100 minutes of online videos per day in 2021 ( ZenithMedia , 2019 ) . 
An important reason for this fast - growing video consumption is information - seeking . 
For instance , people turn to YouTube ‚Äú hungry for how - to and learning content ‚Äù ( O‚ÄôNeil - Hart , 2018 ) . 
Indeed , compared to traditional content format such as text , video carries richer information to satisfy such ‚àóThis work was done while Gabriel Huang was an intern at Google Research . 
Groundtruth Varying stiching speeds √ò - Pretraining Showing other parts MASS - Pretraining Explaining how to do a stitch Figure 1 : Dense video captioning using ViTT ‚Äì trained models . 
For the given video scene , we show the ViTT annotation ( Groundtruth ) and model outputs ( no pretraining and MASS - based pretraining ) . 
needs . 
But as a content media , videos are also inherently more difÔ¨Åcult to skim through , making it harder to quickly target the relevant part(s ) of a video . 
Recognizing this difÔ¨Åculty , search engines started showing links to ‚Äú key moments ‚Äù within videos in search results , based on timestamps and short descriptions provided by the content creators themselves.1This enables users to get a quick sense of what the video covers , and also to jump to a particular time in the video if so desired . 
This effort echoes prior work in the literature showing how users of instructional videos can beneÔ¨Åt from human - curated meta - data , such as a timeline pointing to the successive steps of a tutorial ( Kim et al . 
, 2014 ; Margulieux et al . 
, 2012 ; Weir et al . 
, 2015 ) . 
Producing such meta - data in an automatic way would greatly scale up the efforts of providing easier information access to videos . 
This task is closely related to the dense video captioning task considered in prior work ( Zhou et al . 
, 2018a , c ; Krishna et al . 
, 2017 ) , where an instructional video is Ô¨Årst segmented into its main steps , followed by segment - level caption generation . 
To date , the YouCook2 data set ( Zhou et al . 
, 2018a ) is the largest annotated data set for dense 1https://www.blog.google/products/ search / key - moments - video - search/470 video captioning . 
It contains annotations for 2,000 cooking videos covering 89 recipes , with per - recipe training / validation split . 
Restricting to a small number of recipes is helpful for early exploratory work , but such restrictions impose barriers to model generalization and adoption that are hard to overcome . 
We directly address this problem by constructing a larger and broader - coverage annotated dataset that covers a wide range of instructional topics ( cooking , repairs , maintenance , etc . 
) We make the results of our annotation efforts publicly available as VideoTimeline Tags ( ViTT ) 2 , consisting of around 8,000 videos annotated with timelines ( on average 7.1 segments per video , each segment with a short free - text description ) . 
Using YouCook2 and the new ViTT dataset as benchmarks for testing model performance and generalization , we further focus on the subproblem of video - segment ‚Äì level caption generation , assuming segment boundaries are given ( Hessel et al . 
, 2019 ; Sun et al . 
, 2019b ; Luo et al . 
, 2020 ) . 
Motivated by the high cost of collecting human annotations , we investigate pretraining a video segment captioning model using unsupervised signals ‚Äì ASR ( Automatic Speech Recognition ) tokens and visual features from instructional videos , and unpaired instruction steps extracted from independent sources : Recipe1 M ( Marin et al . 
, 2019 ) and WikiHow ( Koupaee and Wang , 2018 ) . 
In contrast to prior work that focused on BERT - style pretraining of encoder networks ( Sun et al . 
, 2019b , a ) , our approach entails jointly pretraining both multimodal encoder and text - based decoder models via MASSstyle pretraining ( Song et al . 
, 2019 ) . 
Our experiments show that pretraining with either text - only or multi - modal data provides signiÔ¨Åcant gains over no pretraining , on both the established YouCook2 benchmark and the new ViTT benchmark . 
The results we obtain establish state - of - the - art performance on YouCook2 , and present strong performance numbers on the ViTT benchmark . 
These Ô¨Åndings help us conclude that the resulting models generalize well and are quite robust over a wide variety of instructional videos . 
2 Related Work Text - only Pretraining . 
Language pretraining models based on the Transformer neural net2Available at https://github . 
com / google - research - datasets/ Video - Timeline - Tags - ViTTwork architecture ( Vaswani et al . 
, 2017a ) such as BERT ( Devlin et al . 
, 2018 ) , GPT ( Radford et al . 
, 2018 ) , RoBERTa ( Liu et al . 
, 2019 ) , MASS ( Song et al . 
, 2019 ) and ALBERT ( Lan et al . 
, 2020 ) have achieved state - of - the - art results on many NLP tasks . 
MASS ( Song et al . 
, 2019 ) has been recently proposed as a joint encoder - decoder pretraining strategy . 
For sequence - to - sequence tasks , this strategy is shown to outperform approaches that separately pretrain the encoder ( using a BERT - style objective ) and the decoder ( using a language modeling objective ) . 
UniLM ( Dong et al . 
, 2019 ) , BART ( Lewis et al . 
, 2019 ) , and T5 ( Raffel et al . 
, 2019 ) propose uniÔ¨Åed pretraining approaches for both understanding and generation tasks . 
Multimodal Pretraining . 
VideoBERT ( Sun et al . 
, 2019b ) , CBT ( Sun et al . 
, 2019a ) and ActBERT ( Zhu and Yang , 2020 ) use a BERT - style objective to train both video and ASR text encoders . 
Alayrac et al . 
( 2016 ) and Miech et al . 
( 2020 ) use margin - based loss functions to learn joint representations for video and ASR , and evaluate them on downstream tasks such as video captioning , action segmentation and anticipation , and action localization . 
An independent and concurrent work ( UniViLM ) by Luo et al . 
( 2020 ) is closely related to ours in that we share some similar pretraining objectives , some of the pretraining setup ‚Äì HowTo100 M ( Alayrac et al . 
, 2016 ) , and the down - stream video captioning benchmark using YouCook2 ( Zhou et al . 
, 2018a ) . 
The main difference is that they use BERT - style pretraining for encoder and language - modeling style pretraining for decoder , whereas we use MASS - style pre - training to pretrain encoder and decoder jointly . 
Other approaches such as ViLBERT ( Lu et al . 
, 2019 ) , LXMERT ( Tan and Bansal , 2019 ) , Unicoder - VL ( Li et al . 
, 2019 ) , VL - BERT ( Su et al . 
, 2019 ) , and UNITER ( Chen et al . 
, 2019 ) focus on pretraining joint representations for text and image , evaluating them on downstream tasks such as visual question answering , image - text retrieval and referring expressions . 
Dense Video Captioning . 
In this paper , we focus on generating captions at the segment - level , which is a sub - task of the so - called dense video captioning task ( Krishna et al . 
, 2017 ) , where Ô¨Ånegrained captions are generated for video segments , conditioned on an input video with pre - deÔ¨Åned471 Name Type # segments Pretraining datasets YT8M - cook ASR+video 186 K HowTo100 M ASR+video 8.0 M Recipe1 M CAP - style 10.8 M WikiHow CAP - style 1.3 M Finetuning datasets YouCook2 ASR+video+CAP 11.5 K ViTT - All ASR+video+CAP 88.5 K Table 1 : Datasets used in this work , along with size of the data measured by the total number of segments . 
event segments . 
This is different from the video captioning models that generate a single summary for the entire video ( Wang et al . 
, 2019 ) . 
Hessel et al . 
( 2019 ) make use of ASR and video for segment - level captioning on YouCook2 and show that most of the performance comes from ASR . 
Shi et al . 
( 2019 ) ; Luo et al . 
( 2020 ) train their dense video captioning models on both video frames and ASR text and demonstrate the beneÔ¨Åts of adding ASR as an input to the model . 
There are also a number of video captioning approaches that do not use ASR directly ( Zhou et al . 
, 2018c ; Pan et al . 
, 2020 ; Zheng et al . 
, 2020 ; Zhang et al . 
, 2020 ; Lei et al . 
, 2020 ) . 
Instructional video captioning data sets . 
In addition to YouCook2 ( Zhou et al . 
, 2018a ) , there are two other smaller data sets in the instructional video captioning category . 
Epic Kitchen ( Damen et al . 
, 2018 ) features 55 hours of video consisting of 11.5 M frames , which were densely labeled for a total of 39.6 K action segments and 454.3 K object bounding boxes . 
How2 ( Sanabria et al . 
, 2018 ) consists of instructional videos with video - level ( as opposed to segment - level ) descriptions , authored by the video creators themselves . 
3 Data We present the datasets used for pretraining , Ô¨Ånetuning , and evaluation in Table 1 . 
We also describe in detail the newly introduced dense video captioning dataset , VideoTimeline Tags ( ViTT ) . 
3.1 Dense Video - Captioning Datasets Our goal is to generate captions ( CAP ) for video segments . 
We consider two datasets with segment - level captions for Ô¨Åne - tuning and evaluating ASR+Video‚ÜíCAP models . 
YouCook2 . 
Up to this point , YouCook2 ( Zhou et al . 
, 2018a ) has been the largest human - annotated dense - captioning dataset of instructional videos publicly available . 
It originally contained 2,000 cooking videos from YouTube . 
Starting from 110 recipe types ( e.g. , ‚Äú shrimp tempura ‚Äù ) , 25 unique videos per recipe type were collected ; the recipe types that did not gather enough videos were dropped , resulting in a total of 89 recipe types in the Ô¨Ånal dataset . 
In addition , Zhou et al . 
( 2018b ) ‚Äú randomly split the videos belonging to each recipe into 67%:23%:10 % as training , validation and test sets3 , ‚Äù which effectively guarantees that videos in the validation and test sets are never about unseen recipes . 
Annotators were then asked to construct recipe steps for each video ‚Äî that is , identify the start and end times for each step , and provide a recipe - like description of each step . 
Overall , they reported an average of 7.7 segments per video , and 8.8 words per description . 
After removing videos that had been deleted by users , we obtained a total of 11,549 segments . 
ViTT . 
One limitation of the YouCook2 dataset is the artiÔ¨Åcially imposed ( almost ) uniform distribution of videos over 89 recipes . 
While this may help making the task more tractable , it is difÔ¨Åcult to judge whether performance on its validation / test sets can be generalized to unseen topics . 
The design of our ViTT dataset annotation process is aimed at Ô¨Åxing some of these drawbacks . 
We started by collecting a large dataset of videos containing a broader variety of topics to better reÔ¨Çect topic distribution in the wild . 
SpeciÔ¨Åcally , we randomly sampled instructional videos from the YouTube-8 M dataset ( Abu - El - Haija et al . 
, 2016 ) , a large - scale collection of YouTube videos that also contain topical labels . 
Since much of prior work in this area revolved around cooking videos , we aimed at sampling a signiÔ¨Åcant proportion of our data from videos with cooking labels ( speciÔ¨Åcally , ‚Äú Cooking ‚Äù and ‚Äú Recipe ‚Äù ) . 
Aside from the intentional bias regarding cooking videos , the rest of the videos were selected by randomly sampling non - cooking videos , including only those that were considered to be instructional videos by our human annotators . 
Once candidate videos were identiÔ¨Åed , timeline annotations and descriptive tags were collected . 
3Note that no annotations are provided for the test split ; we conducted our own training / dev / test split over available videos.472 Our motivation was to enable downstream applications to allow navigating to speciÔ¨Åc content sections . 
Therefore , annotators were asked to identify the main steps in a video and mark their start time . 
They were also asked to produce a descriptive - yetconcise , free - text tag for each step ( e.g. , ‚Äú shaping the cookies ‚Äù , ‚Äú removing any leftover glass ‚Äù ) . 
A subset of the videos has received more than one complete annotation ( main steps plus tags ) . 
The resulting ViTT dataset consists of a total of 8,169 videos , of which 3,381 are cooking - related . 
A total of 5,840 videos have received only one annotation , and have been designated as the training split . 
Videos with more than one annotation have been designated as validation / test data . 
Overall , there are 7.1 segments per video on average ( max : 19 ) . 
Given the dataset design , descriptions are much shorter in length compared to YouCook2 : on average there are 2.97 words per tag ( max : 16 ) ‚Äî 20 % of the captions are single - word , 22 % are two - words , and 25 % are three words . 
Note that the average caption length is signiÔ¨Åcantly shorter than for YouCook2 , which is not surprising given our motivation of providing short and concise timeline tags for video navigation . 
We standardized the paraphrases among the top-20 most frequent captions . 
For instance,{‚Äúintro ‚Äù , ‚Äú introduction ‚Äù } ‚Üí ‚Äú intro ‚Äù . 
Otherwise , we have preserved the original tags asis , even though additional paraphrasing most definitely exists . 
Annotators were instructed to start and end the video with an opening and closing segment as possible . 
As a result , the most frequent tag ( post - standardization ) in the dataset is ‚Äú intro ‚Äù , which accounts for roughly 11 % of the 88,455 segments . 
More details on the data collection process and additional analysis can be found in the Supplementary Material ( Section A.1 ) . 
Overall , this results in 56,027 unique tags , with a vocabulary size of 12,509 token types over 88,455 segments . 
In this paper , we consider two variants : the full dataset ( ViTT - All ) , and the cooking subset ( ViTT - Cooking ) . 
3.2 Pretraining Datasets : ASR+Video We consider two large - scale unannotated video datasets for pretraining , as described below . 
Timestamped ASR tokens were obtained via YouTube Data API,4and split into ASR segments if the timestamps of two consecutive words are more 4https://developers.google.com/ youtube / v3 / docs / captionsthan 2 seconds apart , or if a segment is longer than a pre - speciÔ¨Åed max length ( in our case , 320 words ) . 
They were paired with concurrent video frames in the same segment . 
YT8M - cook We extract the cooking subset of YouTube-8 M ( Abu - El - Haija et al . 
, 2016 ) by taking , from its training split , videos with ‚Äú Cooking ‚Äù or ‚Äú Recipe ‚Äù labels , and retain those with English ASR , subject to YouTube policies . 
After preprocessing , we obtain 186 K ASR+video segments with an average length of 64 words ( 24 seconds ) per segment . 
HowTo100M. This is based on the 1.2 M YouTube instructional videos released by Miech et al . 
( 2019 ) , covering a broad range of topics . 
After preprocessing , we obtain 7.99 M ASR+video segments with an average of 78 words ( 28.7 seconds ) per segment . 
3.3 Pretraining Datasets : CAP - style We also consider two text - only datasets for pretraining , containing unpaired instruction steps similar in style to the target captions . 
Recipe1 M is a collection of 1 M recipes scraped from a number of popular cooking websites ( Marin et al . 
, 2019 ) . 
We use the sequence of instructions extracted for each recipe in this dataset , and treat each recipe step as a separate example during pretraining . 
This results in 10,767,594 CAP - style segments , with 12.8 words per segment . 
WikiHow is a collection of 230,843 articles extracted from the WikiHow knowledge base ( Koupaee and Wang , 2018 ) . 
Each article comes with a title starting with ‚Äú How to ‚Äù . 
Each associated step starts with a step summary ( in bold ) followed by a detailed explanation . 
We extract the all step summaries , resulting in 1,360,145 CAP - style segments , with 8.2 words per segment . 
Again , each instruction step is considered as a separate example during pretraining . 
3.4 Differences between Pretraining and Finetuning Datasets First , note that video segments are deÔ¨Åned differently for pretraining and Ô¨Ånetuning datasets , and may not match exactly . 
For ASR+Video pretraining datasets , which are unsupervised , the segments are divided following a simple heuristic ( e.g. , two consecutive words more than 2 seconds apart ) , whereas for Ô¨Ånetuning ASR+Video ‚ÜíCAP datasets , which are supervised , the segments are deÔ¨Åned by473 human annotators to correspond to instruction steps . 
Otherwise , the ASR data are relatively similar between pretraining and Ô¨Ånetuning datasets , as both come from instructional videos and are in the style of spoken language . 
Second , compared to the target captions in Ô¨Ånetuning datasets , the CAP - like pretraining datasets are similar in spirit ‚Äî they all represent summaries ofsteps , but they may differ in length , style and granularity . 
In particular , the CAP - like pretraining datasets are closer in style to captions in YouCook2 , where annotators were instructed to produce a recipe - like description for each step . 
This is reÔ¨Çected in their similar average length ( YouCook2 : 8.8 words , Recipe1 M : 12.8 words , WikiHow : 8.2 words ) ; whereas captions in ViTT are signiÔ¨Åcantly shorter ( 2.97 words on average ) . 
Despite these differences ‚Äî some are inevitable due to the unsupervised nature of pretraining datasets ‚Äî the pretraining data is very helpful for our task as shown in the experimental results . 
4 Method To model segment - level caption generation , we adopt MASS - style pretraining ( Song et al . 
, 2019 ) with Transformer ( Vaswani et al . 
, 2017b ) as the backbone architecture . 
For both pre - training and Ô¨Åne - tuning objectives , we have considered two variants : text - only and multi - modal . 
They are summarized in Table 2 and more details are given below . 
4.1 Separate - Modality Architecture Both ASR tokens and video segment features are given as input in the multimodal variants . 
We consider an architecture with a separate transformer for each modality ( text or video ) , see Figure 2 for details . 
When available , the text and video encoders attend to each other at every layer using cross - modal attention , as in ViLBERT ( Lu et al . 
, 2019 ) . 
The text decoder attends over the Ô¨Ånal - layer output of both encoders . 
We discuss in more detail the differences between using a separate - modality architecture vs. a vanilla - Transformer approach for all modalities in Appendix A.2 . 
The inputs to the text encoder is the sum of three components : text token embeddings , positional embeddings and the corresponding style embeddings,5depending on the style of the text ( ASR or Caption - like ) . 
The inputs to the video encoder 5This is similar to the way language - ID embeddings are used in machine translation.could be either precomputed frame - level 2D CNN features or 3D CNN features , pretrained on the Kinetics ( Carreira and Zisserman , 2017 ; Kay et al . 
, 2017 ) data set . 
The visual features are projected with fully - connected layers to the same dimension as the text embeddings . 
The main architecture we consider is a 2 - layer encoder ( E2 ) , 6 - layer decoder ( D6 ) Transformer . 
We use E2D6 to refer to the text - only version , and E2vidD6 to refer to the multimodal version with an active video encoder . 
We also experiment with E2D2 and E2vidD2 ( 2 - layer decoder).6 4.2 Pretraining with Text - only MASS Text - only pretraining is essentially the unsupervised learning of the style transfer between ASRstyle and caption - style texts using unpaired data sources : ASR strings from video segments in YT8M - cook or HowTo100 M ; and CAP - style instruction steps found in Recipe1 M or HowTo100M. Just like using MASS for unsupervised machine translation involves pretraining the model on unpaired monolingual datasets , we alternate between ASR‚ÜíASR and CAP‚ÜíCAP MASS steps during our pretraining stage , which does not require the ‚Äú source ‚Äù ( ASR+Video ) and ‚Äú target ‚Äù ( CAP - style ) data to be aligned . 
In an ASR‚ÜíASR step , we mask a random subsequence of the ASR and feed the masked ASR to the text encoder . 
The text decoder must reconstruct the hidden subsequence while attending to the encoder output . 
A CAP‚ÜíCAP step works similarly by trying to reconstruct a masked sequence of a CAP - style text . 
The encoder and decoder are trained jointly using teacher - forcing on the decoder . 
We denote this text - only strategy as MASS in the experiments . 
4.3 Pretraining with Multimodal MASS During multimodal pretraining , we alternate between text - only CAP‚ÜíCAP MASS steps and multimodal MASS steps . 
During each multimodal MASS step ASR+video‚ÜíASR , we feed a masked ASR to the text - encoder and the co - occurring video features to the video - encoder . 
The text decoder must reconstruct the masked ASR subsequence . 
We denote this pretraining strategy as MASSvid in the experiments . 
This trains cross - modal attention between the text - encoder and video - encoder 6We found in a preliminary study that using 6 - layer encoders did not improve performance for our application.474 + + + + + + + + + + Text Encoder Layer 1   text CLStext after text spread@ text ingText Encoder Layer 2   Video Encoder Layer 1   video   emb . 
video   emb . 
video   emb . 
video   emb . 
Video Encoder Layer 2   [ CLS ]    after spread@@   ing     the   text the Text Embedding Layer Feature Projection Layers   ( Masked ) ASR   ‚Äú after spreading the   [ MASK ] [ MASK ] the bread ‚Äù Tokenize & Truncate f0 f1 f2 f3pos   0pos   1pos   2pos   3pos   4style    asrstyle    asrstyle    capstyle    cap Pretrained Feature Extractor + + + + pos   0pos   1pos   2pos   3Cross - Modal   Attention CLS output text output text output text output text output video   output video   output video   output video   output Segment   Alignment   0/1Segment   Ordering   0/1 Decoder ( Text - only ) Captioning ( asr to cap ) ‚Äú spread butter ‚Äù   ( Masked ) Cap   ‚Äú spread butter ‚Äù or orEncoder - Decoder   Multimodal Attention   Decoder Input ( teacher forcing ) MASS for ASR      ‚Äú butter on ‚Äù   MASS for Cap   ‚Äú spread butter ‚Äù   Pretraining Objectives Fine - tuning Objectives   Reverse Captioning ( cap to asr ) ‚Äú after spread@@ ing the ‚Äù   Encoder   ( Multimodal ) + + + + + + + + + + Text Decoder Layer 1   text CLStext after text spread@ text ingText Decoder Layer 2   [ CLS ]    after spread@@   ing     the   text the Text Embedding Layer   ( Masked ) ASR   ‚Äú [ BOS ] butter ‚Äù Tokenize & Truncate pos   0pos   1pos   2pos   3pos   4 ( Masked ) Cap   ‚Äú spread butter ‚Äù or or Text Encoder Input Video Encoder Input style    asrstyle    asrstyle    capstyle    capFigure 2 : A diagram for the separate - modality architecture . 
It consists of a two - stream ( text and video inputs ) encoder with cross - modal attention and a text - only decoder , jointly trained using the MASS objective . 
at every layer , jointly with the text decoder that attends to the output layer of both the text and video encoders.7 To force more cross - modal attention between encoder and decoder , we also investigate a strategy of hiding the text - encoder output from the decoder for some fraction of training examples . 
We refer to this strategy as MASSdrop in the experiments . 
4.4 Pretraining with Alignment and Ordering Tasks We also explore encoder - only multimodal pretraining strategies . 
We take the last - layer representation for the CLS ( beginning of sentence ) token from the encoder , and add a multi - layer perceptron on top of it for binary predictions ( Figure 2 ) . 
Given a pair of ASR and video segment , we train the encoder to predict the following objectives : ‚Ä¢Segment - Level Alignment . 
An ( ASR , video ) pair is aligned if they occur in the same pretraining segment ; negative examples are constructed by sampling pairs from the same video but at least 2 segments away . 
7In preliminary experiments , we had attempted to directly adapt the MASS objective ( Song et al . 
, 2019 ) to video reconstruction ‚Äî by masking a subsequence of the input video and making the video decoder reconstruct the input using the Noise Constrastive Estimator Loss ( Sun et al . 
, 2019a ) . 
Due to limited success , we did not further pursue this approach.‚Ä¢Segment - Level Ordering . 
We sample ( ASR , video ) pairs that are at least 2 segments away , and train the model to predict whether the ASR occurs before or after the video clip . 
During this MASSalign pretraining stage , we alternate between two text - only MASS steps ( CAP‚ÜíCAP , ASR‚ÜíASR ) and the two binary predictions ( Alignment andOrdering ) described above . 
4.5 Finetuning on Video Captioning For text - only Ô¨Ånetuning , we feed ASR to the text encoder and the decoder has to predict the corresponding CAP ( ASR‚ÜíCAP ) . 
For multimodal Ô¨Ånetuning , we also feed additional video representations to the video encoder ( ASR+video‚ÜíCAP ) . 
When Ô¨Ånetuning a multimodal model from textonly pretraining , everything related to video ( weights in the video encoder and any cross - modal attention modules ) will be initialized randomly . 
In addition to these uni - directional ( UniD ) Ô¨Ånetuning , we also experiment with several variants of bidirectional ( BiD ) Ô¨Ånetuning ( Table 2 ) . 
For instance , adding CAP‚ÜíASR ( predicting ASR from CAP ) to text - only Ô¨Ånetuning . 
In the experiments , we Ô¨Ånd some variants of bidirectional Ô¨Ånetuning beneÔ¨Åcial whether training from scratch or Ô¨Ånetuning from a pretrained model.475 Pretraining Objectives Name T V Input ‚ÜíOutput MASS   CAP‚ÜíCAP , ASR‚ÜíASR MASSvid   CAP‚ÜíCAP , ASR+video‚ÜíASR MASSdrop   CAP‚ÜíCAP , ASR+video‚ÜíASR MASSalign  CAP‚ÜíCAP , ASR‚ÜíASR , ASR+video‚Üí{0,1 } Finetuning Objectives Name T V Input ‚ÜíOutput UniD   ASR‚ÜíCAP BiD   ASR‚ÜíCAP , CAP‚ÜíASR UniD   ASR+video‚ÜíCAP BiD   ASR+video‚ÜíCAP , CAP‚ÜíASR BiDalt   ASR+video‚ÜíCAP , CAP+video‚ÜíASR Table 2 : Pretraining and Fine - tuning objectives . 
For each strategy , indicates whether the text ( T ) and video ( V ) encoders are active , followed by a summary of training objectives involved in one training step . 
5 Experiments 5.1 Implementation Details We tokenize ASR and CAP inputs using byte - pair ‚Äì encoding subwords ( Sennrich et al . 
, 2015 ) , and truncate them to 240 subwords . 
We truncate video sequences to 40 frames ( 40 seconds of video ) , compute the 128 - dim features proposed by Wang et al . 
( 2014 ) ( which we will refer to as Compact 2D features ) , and project them to the embedding space using a two - layer perceptron with layer normalization and GeLU activations . 
We instantiate the E2xDx models deÔ¨Åned in Section 4.1 with 128 - dimensional embeddings and 8 heads respectively for self - attention , encoderdecoder , and cross - modal attention modules . 
We deÔ¨Åne each epoch to be 3,125 iterations , where each iteration contains one repetition of each training step as represented in Table 2 . 
We pretrain for 200 epochs and Ô¨Ånetune for 30 epochs . 
For evaluation , we consider BLEU-4 ( Papineni et al . 
, 2002 ) , METEOR ( Denkowski and Lavie , 2014 ) , ROUGE -L ( Lin and Och , 2004 ) and CIDEr ( Vedantam et al . 
, 2015 ) metrics . 
Please refer to Appendix A.3 for full implementation details , hyperparameters and computation cost . 
Notes on ViTT evaluation : With the exception ofROUGE -L , all other metrics are sensitive to short groundtruth . 
67 % of the groundtruth tags in ViTT have less than 4 words , where a perfect prediction will not yield a full score in , say , BLEU-4 . 
Thus , wefocus mainly on ROUGE -L , report BLEU-1instead ofBLEU-4 for ViTT , and provide the other two metrics only as reference points . 
We had originally decided to use videos with multiple annotations as validation and test data , so that we could explore evaluation with multiple reference groundtruth captions . 
But as annotators do not always yield the same set of segment boundaries , this became tricky . 
Instead , we simply treat each segment as a separate instance with one single reference caption . 
Note that all segments annotated for the same video will be in either validation or test to ensure no content overlap . 
5.2 Main Results We run several variants of our method on YouCook2 , ViTT - All and ViTT - Cooking , using different architectures , modalities , pretraining datasets , pretraining and Ô¨Ånetuning strategies . 
Comparing with other methods on YouCook2 For YouCook2 , we report our method alongside several methods from the literature ( Hessel et al . 
, 2019 ; Sun et al . 
, 2019b ; Zhou et al . 
, 2018c ; Lei et al . 
, 2020 ) , as well as state - of - the - art concurrent work ( Luo et al . 
, 2020 ) . 
The related work is provided for reference and to give a ballpark estimate of the relative performance of each method , but results are not always strictly and directly comparable . 
Beyond the usual sources of discrepancy in data processing , tokenization , or even different splits , an additional source of complication comes from the fact that videos are regularly deleted by content creators , causing video datasets to shrink over time . 
Additionally , when comparing to other work incorporating pretraining , we could differ in ( videos available in ) pretraining datasets , segmentation strategies , etc . 
To this end , we perform an extensive ablation study , which at least helps us to understand the effectiveness of different components in our approach . 
Effect of pretraining The main experimental results for the three datasets we consider are summarized in Table 3 ( YouCook2 ) and Table 4 ( ViTT - All and ViTT - Cooking ) . 
Across all three datasets , the best performance is achieved by Ô¨Ånetuning a multimodal captioning model under the Multimodal Pretraining condition . 
For instance , on YouCook2 , E2vidD6 - MASSvid - BiD improves over the nopretraining model E2vidD6 - BiD by 4.37 ROUGE -L , a larger improvement than UniViLM with pretraining ( # 5 ) vs without ( # 2 ) ( Luo et al . 
, 2020 ) . 
This476 Method Input Pretraining B LEU-4 M ETEOR ROUGE -L CIDE R Constant Pred ( Hessel et al . 
, 2019 ) - - 2.70 10.30 21.70 0.15 MART ( Lei et al . 
, 2020 ) Video - 8.00 15.90 - 0.36 EMT ( Zhou et al . 
, 2018c ) Video - 4.38 11.55 27.44 0.38 CBT ( Sun et al . 
, 2019a ) Video Kinetics + HowTo100 M 5.12 12.97 30.44 0.64 AT ( Hessel et al . 
, 2019 ) ASR - 8.55 16.93 35.54 1.06 AT+Video ( Hessel et al . 
, 2019 ) Video + ASR - 9.01 17.77 36.65 1.12 UniViLM # 1 ( Luo et al . 
, 2020 ) Video - 6.06 12.47 31.48 0.64 UniViLM # 2 ( Luo et al . 
, 2020 ) Video + ASR - 8.67 15.38 35.02 1.00 UniViLM # 5 ( Luo et al . 
, 2020 ) Video + ASR HowTo100 M 10.42 16.93 38.02 1.20 √ò Pretraining E2D6 - BiD ASR - 7.90 15.70 34.86 0.93 E2vidD6 - BiD Video + ASR - 8.01 16.19 34.66 0.91 Text Pretraining E2D6 - MASS - BiD ASR YT8M - cook + Recipe1 M 10.60 17.42 38.08 1.20 E2vidD6 - MASS - BiD Video + ASR YT8M - cook + Recipe1 M 11.47 17.70 38.80 1.25 Multimodal Pretraining E2vidD6 - MASSalign - BiD Video + ASR YT8M - cook + Recipe1 M 11.53 17.62 39.03 1.22 E2vidD6 - MASSvid - BiD Video + ASR YT8M - cook + Recipe1 M 12.04 18.32 39.03 1.23 E2vidD6 - MASSdrop - BiD Video + ASR YT8M - cook + Recipe1 M 10.45 17.74 38.82 1.22 Human ( Hessel et al . 
, 2019 ) Video + ASR - 15.20 25.90 45.10 3.80 Table 3 : Segment - level captioning results on YouCook2 . 
We use YT8M - cook and Recipe1 M for pretraining . 
The numbers for the related work ( Ô¨Årst group ) are directly reported from the corresponding papers . 
The last line is an estimate of human performance as reported by Hessel et al . 
( 2019 ) , and can be taken as a rough upper bound of the best performance achievable . 
improvement also holds in ViTT - Cooking ( +4.22 inROUGE -L ) and ViTT - All ( +2.97 in ROUGE -L ) . 
We do not observe consistent and signiÔ¨Åcant trends among the different multimodal pretraining strategies : MASS pretraining with video ( MASSvid ) , with video and droptext ( MASSdrop ) , or with alignment tasks ( MASSalign ) .8Furthermore , we observe that most of the pretraining improvement is achievable via text - only MASS pretraining . 
Across all three datasets , while Multimodal Pretraining ( E2vidD6 - MASSvid - BiD ) is consistently better than Text Pretraining ( E2vidD6 - MASS - BiD ) , the differences are quite small ( under one ROUGE -L point ) . 
It ‚Äôs worthy noting that for MASSalign , the best validation accuracies for the pretraining tasks are reasonably high : for YT8M - cook , we observed 90 % accuracy for the alignment task , and 80 % for the ordering task ( for HowTo100 M : 87 % and 71.4 % , respectively ) , where random guess would yield 50 % . 
This suggests that our video features are reasonably strong , and the Ô¨Åndings above are not due to weak visual representations . 
8Limited improvement with MASSalign suggests that such alignment tasks are better suited for retrieval ( Luo et al . 
, 2020).Effect of other modeling choices We experiment with 2 - layer decoder ( D2 ) vs 6 - layer decoder ( D6 ) , combined with either unidirectional Ô¨Ånetuning ( UniD ) or bidirectional Ô¨Åne - tuning ( BiD ) . 
Table 5 shows ablation results of the four possible combinations when Ô¨Ånetuning a multimodal model using text - only pretraining on YouCook2 ( a more complete list of results can be found in Appendix A.5 , showing similar trends ) . 
The D6xBiD combination tends to yield the best performance , with the differences among the four conÔ¨Ågurations being relatively small ( under one ROUGE -L point ) . 
For visual features , we also explored using 3D features ( Xie et al . 
, 2018 ) instead of 2D features during Ô¨Ånetuning ( with no pretraining or text - only pretraining ) , and do not Ô¨Ånd much difference in model performance on YouCook2 . 
As a result , we use the simpler 2D features in our multimodal pretraining . 
We leave more extensive experiments with visual features as future work . 
Generalization implications An important motivation for constructing the ViTT dataset and evaluating our models on it has been related to generalization . 
Since the YouCook2 benchmark is restricted to a small number of cooking recipes , there is little to be understood about how well models477 Method InputViTT - All ViTT - Cooking BLEU-1 M ETEOR ROUGE -L CIDEr B LEU-1 M ETEOR ROUGE -L CIDEr Constant baseline ( ‚Äú intro ‚Äù ) - 1.42 3.32 11.15 0.28 1.16 2.93 10.21 0.25 √ò Pretraining E2D6 - BiD ASR 19.60 9.12 27.88 0.68 20.77 10.08 28.63 0.72 E2vidD6 - BiD Video + ASR 19.49 9.23 28.53 0.69 20.45 9.88 28.88 0.69 Text Pretraining E2D6 - MASS - BiD ASR 21.93 10.60 30.45 0.79 24.79 12.25 32.40 0.88 E2vidD6 - MASS - BiD Video + ASR 22.44 10.83 31.27 0.81 24.22 12.22 32.60 0.89 Multimodal Pretraining E2vidD6 - MASSalign - BiD Video + ASR 22.31 10.66 31.13 0.79 24.92 12.25 33.09 0.90 E2vidD6 - MASSvid - BiD Video + ASR 22.45 10.76 31.49 0.80 24.87 12.43 32.97 0.90 E2vidD6 - MASSdrop - BiD Video + ASR 22.37 11.00 31.40 0.82 24.48 12.22 33.10 0.89 Human Video + ASR 43.34 33.56 41.88 1.26 41.61 32.50 41.59 1.21 Table 4 : Segment - level captioning results on ViTT . 
For ViTT - All we pretrain on HowTo100 M and WikiHow ; for ViTT - Cooking we pretrain on YT8M - cook and Recipe1M. We report baseline scores for predicting the most common caption ‚Äú intro ‚Äù . 
We also estimate the human performance as a rough upper bound ( details in Supplementary Material A.1 ; Table 9 ) . 
Method B LEU-4 M ETEOR ROUGE -L CIDEr D2 - UniD 10.84 17.39 38.24 1.16 D6 - UniD 11.39 18.00 38.71 1.22 D2 - BiD 11.38 18.04 38.67 1.19 D6 - BiD 11.47 17.70 38.80 1.25 D6 - BiDalt 11.07 17.68 38.43 1.22 D6 - BiD ( S3D ) 11.64 18.04 38.75 1.24 Table 5 : Ablation study on YouCook2 . 
We Ô¨Ånetune a multimodal captioning model ( E2vid ) with either 2 - layer decoder ( D2 ) or 6 - layer decoder ( D6 ) using YT8M - cook /Recipe1 M for MASS pretraining , combined with either unidirectional ( UniD ) or bidirectional ( BiD ) Ô¨Ånetuning . 
We Ô¨Ånd no signiÔ¨Åcant difference between using 2D and 3D features ( marked as S3D ) . 
trained and evaluated on it generalize . 
In contrast , the ViTT benchmark has a much wider coverage ( for both cooking - related videos and general instructional videos ) , and no imposed topic overlap between train / dev / test . 
As such , there are two Ô¨Åndings here that are relevant with respect to generalization : ( a ) the absolute performance of the models on the ViTT benchmark is quite high ( ROUGE - L scores above 0.30 are usually indicative of decent performance ) , and ( b ) the performance on ViTT vs. YouCook2 is clearly lower ( 31.5 ROUGE - L vs. 39.0 ROUGE - L , reÔ¨Çecting the increased difÔ¨Åculty of the new benchmark ) , but it is maximized under similar pretraining and Ô¨Ånetuning conditions , which allows us to claim that the resulting models generalize well and are quite robust over a wide variety of instructional videos.6 Conclusions Motivated to improve information - seeking capabilities for videos , we have collected and annotated a new dense video captioning dataset , ViTT , which is larger with higher diversity compared to YouCook2 . 
We investigated several multimodal pretraining strategies for segment - level video captioning , and conducted extensive ablation studies . 
We concluded that MASS - style pretraining is the most decisive factor in improving the performance on all the benchmarks used . 
Even more to the point , our results indicate that most of the performance can be attributed to leveraging the ASR signal . 
We achieve new state - of - the - art results on the YouCook2 benchmark , and establish strong performance baselines for the new ViTT benchmark , which can be used as starting points for driving more progress in this direction . 
Acknowledgements We send warm thanks to Ashish Thapliyal for helping the Ô¨Årst author debug his code and navigate the computing infrastructure , and to Sebastian Goodman for his technical help ( and lightning fast responses ! ) . 
We also thank the anonymous reviewers for their comments and suggestions . 
A Appendix Supplementary Material for ‚Äú Multimodal Pretraining for Dense Video Captioning ‚Äù . 
A.1 The ViTT dataset Sampling video for annotation . 
The goal of the ViTT dataset design is to mirror topic distribution in the ‚Äú wild ‚Äù . 
Therefore , instead of starting from speciÔ¨Åc how - to instructions and searching for corresponding videos , we sampled videos from the validation set of the YouTube-8 M dataset ( Abu - El - Haija et al . 
, 2016 ) , a large - scale collection of YouTube videos with topical labels , subject to YouTube policies . 
Exclusion criteria were lack of English ASR and the topic label ‚Äú Game ‚Äù . 
The latter was motivated by the fact that in this type of videos , the visual information predominantly features video games , while the ViTT dataset was intended to contain only videos with real - world human actions . 
Cooking videos can be easily identiÔ¨Åed by sampling videos that came with ‚Äú Cooking ‚Äù or ‚Äú Recipe ‚Äù topic labels . 
Given the convenience and the fact that much of prior work in this area had focused on cooking videos , approximately half of the dataset was designed to include cooking videos only , while the remaining videos would be randomly sampled non - cooking videos , as long as they were veriÔ¨Åed as instructional by human annotators . 
Annotation process Annotators were presented with a video alongside its timestamped , automatic transcription shown in sentence - length paragraphs . 
They were asked to watch the video and Ô¨Årst judge whether the video was instructional . 
For the purpose of our dataset , we determine that a video is instructional if it focuses on real - world human actions that are accompanied by procedural language explaining what is happening on screen , in reasonable details . 
Also for our purposes , instructional videos need to be grounded in real life , with a real person in the video exemplifying the action being verbally described . 
For videos judged to be instructional , annotators were then asked to : ‚Ä¢Delimit the main segments of the video . 
‚Ä¢Determine their start time if different from the automatically suggested start time ( explained below).‚Ä¢Provide a label summarizing or explaining the segment . 
Annotation guidelines Annotators were instructed to identify video segments with two potential purposes : ‚Ä¢Allow viewers to jump straight to the start of a segment for rewatch . 
‚Ä¢Present viewers with an index to decide whether to watch the video in full or directly skip to the segment of interest . 
Our guidelines suggested a range of Ô¨Åve to ten segments as long as the the structure and content of the video permitted . 
For short videos , the direction was to prioritize quality over quantity and to only deÔ¨Åne those segments that formed the narrative structure of the video , even if the resulting number of segments was below 5 . 
To help annotators determine segment start times , transcriptions were shown in ‚Äú sentences ‚Äù ‚Äî we expected that sentence start times might be good candidates for segment start times . 
We obtained sentence boundaries automatically as follows . 
Given the stream of timestamped ASR tokens for a video , we Ô¨Årst separated them into blocks by breaking two consecutive tokens whenever they were more than 2 seconds apart . 
We then used a punctuation prediction model to identify sentence boundaries in each resulting block . 
Each sentence was shown with the timestamp corresponding to its Ô¨Årst token . 
Annotators were advised that transcriptions had been automatically divided into paragraphs that may or may not correspond to a video segment ‚Äî if they decided that a segment started from a particular sentence , they could choose to use the start time of the sentence as the start time for the segment , or , if needed , they could put in an adjusted start time instead . 
Once the start time had been identiÔ¨Åed , annotators were asked to provide a free - text label to summarize each segment . 
We instructed the annotators to use nouns or present participles ( -ing form of verbs ) to write the labels for the video segments , whenever possible . 
Additionally , we asked that the labels be succinct while descriptive , using as few words as possible to convey as much information as possible . 
Data statistics and post - processing The resulting dataset consists of 8,169 instructional videos that received segment - level annotations , of which482 3,381 are cooking - related . 
Overall there are an average of 7.1 segments per video ( max : 19 ) . 
Given our instructions , the descriptions are much shorter in lengths compared to a typical captioning dataset : on average there are 2.97 words per description ( max : 16 ) ; 20 % of the captions are single - word , 22 % are two - words , and 25 % are three words . 
We refer to these descriptions as ‚Äú tags ‚Äù given how short they are . 
When possible , annotators were also asked to start and end the video with an opening and closing segment . 
As a result , most annotations start with an introduction segment : this accounts for roughly 11 % of the 88455 segments in the dataset ( ‚Äú intro ‚Äù : 8 % , ‚Äú introduction ‚Äù : 2.3 % ) . 
Note that while ‚Äú intro ‚Äù and ‚Äú introduction ‚Äù are clearly paraphrases of each other , an automatic metric will penalize a model predicting ‚Äú intro ‚Äù when the groundtruth is ‚Äú introduction ‚Äù . 
Similarly , the ending segment was described in several varieties : ‚Äú outro ‚Äù : 3.4 % , ‚Äú closing ‚Äù : 1 % , ‚Äú closure ‚Äù , ‚Äú conclusion ‚Äù , ‚Äú ending ‚Äù , ‚Äú ‚Äò end of video ‚Äù : each under 1 % . 
Penalizing paraphrases of the ground truth is an inherent weakness of automatic metrics . 
To mitigate this , we decided to reduce the chance of this happening for the most frequent tags in the dataset . 
That is , in our experiments , we identiÔ¨Åed three groups of tags among the top-20 most frequent tags , and standardized them as follows . 
intro intro , introduction , opening outro outro , closing , closure , conclusion , ending , end of video , video closing result Ô¨Ånished result , Ô¨Ånal result , results Table 6 : Standardization of top tags Note that this does not mean we can solve this problem as a classiÔ¨Åcation task like in visual question answering ( VQA ): overall , there are 56,027 unique tags with a vocabulary size of 12,509 for the 88,455 segments ; 51,474 tags appeared only once in the dataset , making it infeasible to reduce the segment - level captioning problem into a pure classiÔ¨Åcation task . 
Table 7 shows the top 10 most frequent tags after standardization . 
Estimate of human performance . 
A subset of the candidate videos were given to three annotators9 , to help us understand variations in human annotations . 
5,840 videos received dense captioning 9A small set were unintentionally given to six annotators . 
Tag % of segments intro 11.4 outro 6.6 result 0.9 ingredients 0.8 listing ingredients 0.2 supplies 0.2 mixing ingredients 0.2 materials 0.1 what you ‚Äôll need 0.1 lining the eyes 0.1 Table 7 : 10 most frequent tags after standardization . 
from exactly one annotator and were used as training data . 
Videos with more than one annotation were used as validation / test data . 
Note that not all the videos with multiple timeline annotations have exactly three sets of them ‚Äî in fact , 1368 videos received 3 - way segment - level annotations . 
This is because not all annotators agreed on whether a video was instructional . 
Computing annotator agreement for the annotated timelines is non - trivial . 
Here we focus on an estimate of tagging agreement when a pair of annotators agreed over the segment start time . 
SpeciÔ¨Åcally , we go through each video that received multiple segment - level annotations . 
For each segment where two annotators chose the same ASR sentence as its starting point , we take the tags they produced for this segment and consider one of them as groundtruth , the other as prediction , and add that into our pool of ( groundtruth , prediction ) pairs . 
We can then compute standard automatic evaluations metrics over this pool . 
The results are as follows . 
BLEU-1 METEOR ROUGE - L CIDEr 43.34 33.56 41.88 1.26 Table 8 : Estimate of human performance for the segment - level captioning on ViTT - All ( computed over 7528 pairs ) . 
BLEU-1 METEOR ROUGE - L CIDEr 41.61 32.50 41.59 1.21 Table 9 : Estimate of human performance for the segment - level captioning on ViTT - Cooking ( computed over 2511 pairs ) . 
Note that METEOR , and CIDEr scores are both penalized by the lack of n - grams for higher n. That483 is , when both groundtruth and prediction are singleword , say , ‚Äú intro ‚Äù , this pair will not receive a full score from any of these metrics . 
But the ROUGE -L score is in the same ballpark as estimate of human performance in prior work ( Hessel et al . 
, 2019 ) . 
One might note that perhaps this pool of label pairs contains a higher share of ‚Äú intro ‚Äù , since annotators might be more likely to agree over where an opening segment starts . 
Indeed , 20 % of the time , one of the tags is ‚Äú intro ‚Äù . 
Interestingly , in spite of standardization of top tags , 14 % of the time one tag is ‚Äú intro ‚Äù , the other tag is not‚Äúintro ‚Äù : they can be less frequent paraphrases ( e.g. , ‚Äú welcoming ‚Äù , ‚Äú greeting ‚Äù , ‚Äú opening and welcoming ‚Äù ) or something semantically different ( e.g. , ‚Äú using dremel tool ‚Äù ) . 
A.2 Separated vs. Concatenated - Modality Architecture Prior work has explored both concatenating different modalities and feeding them into the same multimodal Transformer encoder ( Sun et al . 
, 2019b ; Hessel et al . 
, 2019 ) , as well as separating them into unimodal transformers ( Sun et al . 
, 2019a ; Lu et al . 
, 2019 ) . 
We opt for the separated architecture because it offers more Ô¨Çexibility . 
First , the concatenated architecture requires embedding the text and video features into the same space . 
When the video features are projected using a simple network , there is no guarantee that we can meaningfully project them into the text embedding space . 
VideoBERT ( Sun et al . 
, 2019b ) gives more Ô¨Çexibility to the video embeddings by quantizing video features and learning an embedding for each codeword . 
However , the quantization step has subsequently been claimed to be detrimental ( Sun et al . 
, 2019a ) . 
Moreover , the concatenated architecture uses the same sets of forward and attention weights to process text and video , and performs layer normalization jointly between the two modalities , which is not necessarily meaningful . 
Finally , the separated architecture makes it easy to switch between variable length text - only , video - only , or text+video modalities , whereas concatenated architectures might rely on separating tokens , modalities embeddings , and using Ô¨Åxed sequence lengths ( Luo et al . 
, 2020 ) . 
A.3 Additional Implementation Details We optimize all models on a nVidia v100 GPU using the Adam optimizer with inverse square root schedule , batch size 32 , warm - up period of 4,000iterations , and maximum learning rate of 0.0001 , following MASS ( Song et al . 
, 2019 ) . 
The positional embeddings are initialized randomly . 
We use dropout and attention dropout with probabilities0.1 . 
With E2vidD6 , pretraining takes 3 - 6 days depending on the objective and bidirectional Ô¨Ånetuning takes up to 1.5 days , however those times could be improved by optimizing the data pipeline . 
A.4 Example Predictions We show examples of good andbadpredictions on YouCook2 ( Figure 5 and ViTT - All ( Figure 4 and 5 ) . 
The captions are generated by E2vidD6 - BiD ( no pretraining ) and E2vidD6 - MASS - BiD ( text - only MASS pretraining ) . 
A.5 Full result tables We present here tables with all the ablation results that we run . 
There are two main takeaway messages from the results involving the pretraining approach : ( a ) the accuracy improvements , as measured across all the metrics we use , indicate the value of using a pretraining approach to this problem , speciÔ¨Åcally one that is capable of leveraging the ASR signals at both pretraining and Ô¨Ånetuning stages , and ( b ) the training speedup achieved from pretraining is impressive , as a pretrained model converges much faster than training from scratch . 
This is especially visible on ViTT - All where Ô¨Ånetuning after MASS pretraining reaches best ROUGE -L score at epoch 2 , whereas it takes around 11 epochs to converge when training from scratch.484 Sample Frame Ground Truth √ò - Pretraining MASS - Pretraining Comments crush and grate the garlicgrate garlic and add to bowl ( good ) crush ginger and garlic(good ) ginger is correct despite not appearing in ground truth . 
crimp shut with fork place the Ô¨Ålling on the wrapper ( ok)seal the edges of the wrapper ( good ) pretrained model is more speciÔ¨Åc place wings on the baking sheet and cook Ô¨Çippingbake the pizza in the oven ( bad)cook the wings on the grill ( good ) only pretrained model predicted correct food add the pork back into the hot oiladd the rice to the pot ( bad)place the meat on the pan ( good ) √ò model hallucinates the rice and pot add thyme bay leaves onion and clam juice and boil the mixtureadd diced tomatoes tomato puree and mix well ( bad)add thyme thyme onion and clam juice to the pot and stir ( ok)√ò hallucinates a lot of nonexistent ingredients cook bacon in a pot with oil and pepperadd chopped tomatoes to pan and stir ( bad)add bacon and stir ( ok)both models missed oil and pepper ( not mentioned in ASR ) pour dressing on top of the salad and tossadd dressing to the bowl ( good ) serve the soup over the salad ( bad)pretrained model referred to dressing as ‚Äú soup ‚Äù slice the ginger into piecesslice a celery ( bad ) slice the chicken ( bad ) both models had wrong ingredients ( ASR segment does not mention what is being sliced ) Figure 3 : Example good and bad predictions on YouCook2 . 
The pretrained model is generally but not always better . 
Note that there are no ‚Äú intro ‚Äù or ‚Äú outro‚Äù-like labels on YouCook2 because the dataset was speciÔ¨Åcally curated to only contain actual recipe steps.485 Sample Frame Ground Truth √ò - Pretraining MASS - Pretraining Comments tightening extra loop tightening the loop ( good ) tightening the loop ( good ) both models perform well adding eyeshadow blending eye shadow ( good ) applying eye shadow ( good ) both models perform well showcasing the Ô¨Ånished lookshowing Ô¨Ånished look ( good ) showing Ô¨Ånished look ( good ) both models perform well rolling and folding the clayrolling and blending ( ok)rolling and folding the clay ( good ) MASS is a bit more speciÔ¨Åc highlighting brow bone applying eye shadow ( ok)brushing on the brows ( good ) MASS is a bit more speciÔ¨Åc covering the chicken and cookingcooking the bread ( bad)cooking the chicken ( good ) only MASS got the right ingredient connecting spray hose and sprayerconnecting the new cover ( ok)connecting the valve ( good ) spray hose is more speciÔ¨Åc than valve implementing second layershowing Ô¨Ånished product ( ok)showing second layer ( good ) MASS is more speciÔ¨Åc making decorative trim cutting the edges ( good ) cutting the fabric ( good ) both models yield good predictions checking bleach containeroutro ( bad ) checking the container ( good ) MASS is a bit more speciÔ¨Åc demonstrating the Ô¨Çip checking the battery ( bad)Ô¨Çipping the board ( good ) √ò model got inÔ¨Çuenced by car mechanics tutorials tilting board setting up the oven ( bad)turning the board ( good ) √ò overÔ¨Åtted on cooking videos Figure 4 : Example good predictions on ViTT - All ( Part 1 ) . 
The pretrained model is generally but not always better.486 Sample Frame Ground Truth √ò - Pretraining MASS - Pretraining Comments securing the bar in place removing the cover ( bad)checking for the other side ( bad)predictions are not speciÔ¨Åc enough starting with unlocking barsopening the box ( bad ) pulling the car on ( bad)predictions are incorrect or not speciÔ¨Åc enough demonstrating techniqueattaching paper ( bad ) stamping paper ( good ) the technique is about stamping the paper spritzing in additional waterpouring water into the water ( ok)adding water to water ( ok)understandable but ungrammarly checking for leaks checking for the new new new new new new new new new new new new new new new ( bad)checking the process ( ok)√ò got into a loop , MASS not speciÔ¨Åc enough displaying materials neededintro ( bad ) removing paste ( ok ) prediction makes sense because narrator is displaying thermal paste remover sketching on the swirls drawing the lines ( good ) drawing on the eyes ( bad)pretrained model overÔ¨Åtted on makeup tutorials crimping wire and completing projectattaching the screws ( bad)attaching the wire to the wire ( ok)both models have trouble with the concept of crimping a wire cutting with guide line cutting the top of the top of the top of the top of the top of the top ( bad)explaining process ( ok ) √ò model got into a loop , MASS model is not speciÔ¨Åc enough Figure 5 : Example okandbad predictions on ViTT ( Part 2 ) . 
The pretrained model is generally but not always better.487 Method Input Pretraining BLEU-4 METEOR ROUGE - L CIDEr Constant Pred ( Hessel et al . 
, 2019 ) - - 2.70 10.30 21.70 0.15 MART ( Lei et al . 
, 2020 ) Video - 8.00 15.90 - 0.36 DPC ( Shi et al . 
, 2019 ) Video + ASR - 2.76 18.08 - EMT ( Zhou et al . 
, 2018c ) Video - 4.38 11.55 27.44 0.38 CBT ( Sun et al . 
, 2019a ) Video Kinetics + HowTo100 M 5.12 12.97 30.44 0.64 AT ( Hessel et al . 
, 2019 ) ASR - 8.55 16.93 35.54 1.06 AT+Video ( Hessel et al . 
, 2019 ) Video + ASR - 9.01 17.77 36.65 1.12 UniViLM # 1 ( Luo et al . 
, 2020 ) Video - 6.06 12.47 31.48 0.64 UniViLM # 2 ( Luo et al . 
, 2020 ) Video + ASR - 8.67 15.38 35.02 1.00 UniViLM # 5 ( Luo et al . 
, 2020 ) Video + ASR HowTo100 M 10.42 16.93 38.02 1.20 √ò Pretraining E2D2 - UniD ASR - 7.42 15.15 33.26 0.85 E2D6 - UniD ASR - 7.88 15.29 34.10 0.87 E2D2 - BiD ASR - 6.85 15.64 34.26 0.91 E2D6 - BiD ASR - 7.90 15.70 34.86 0.93 E2vidD2 - UniD Video + ASR - 7.47 15.11 34.77 0.90 E2vidD6 - UniD Video + ASR - 7.61 15.57 34.28 0.89 E2vidD2 - BiD Video + ASR - 8.39 15.36 34.54 0.91 E2vidD6 - BiD Video + ASR - 8.01 16.19 34.66 0.91 E2vidD2 - BiDalt Video + ASR - 8.12 15.83 34.83 0.93 E2vid , D6 - BiDalt Video + ASR - 7.70 16.11 34.78 0.91 E2vidD2 - BiD ( S3D ) Video + ASR - 8.04 16.17 36.01 0.96 E2vidD6 - BiD ( S3D ) Video + ASR - 7.91 16.28 35.23 0.93 Text Pretraining E2D2 - MASS - UniD ASR YT8M - cook + Recipe1 M 10.52 17.14 37.39 1.14 E2D6 - MASS - UniD ASR YT8M - cook + Recipe1 M 10.72 17.74 37.85 1.17 E2D2 - MASS - BiD ASR YT8M - cook + Recipe1 M 10.84 17.44 37.20 1.13 E2D6 - MASS - BiD ASR YT8M - cook + Recipe1 M 10.60 17.42 38.08 1.20 E2vidD2 - MASS - UniD Video + ASR YT8M - cook + Recipe1 M 10.84 17.39 38.24 1.16 E2vidD6 - MASS - UniD Video + ASR YT8M - cook + Recipe1 M 11.39 18.00 38.71 1.22 E2vidD2 - MASS - BiD Video + ASR YT8M - cook + Recipe1 M 11.38 18.04 38.67 1.19 E2vidD6 - MASS - BiD Video + ASR YT8M - cook + Recipe1 M 11.47 17.70 38.80 1.25 E2vid , D2 - MASS - BiDalt Video + ASR YT8M - cook + Recipe1 M 11.49 17.85 38.60 1.18 E2vid , D6 - MASS - BiDalt Video + ASR YT8M - cook + Recipe1 M 11.07 17.68 38.43 1.22 E2vidD2 - MASS - BiD ( S3D ) Video + ASR YT8M - cook + Recipe1 M 11.13 17.71 38.57 1.12 E2vidD6 - MASS - BiD ( S3D ) Video + ASR YT8M - cook + Recipe1 M 11.64 18.04 38.75 1.24 Multimodal Pretraining E2vidD2 - MASSalign - BiD Video + ASR YT8M - cook + Recipe1 M 11.54 17.57 37.70 1.15 E2vidD6 - MASSalign - BiD Video + ASR YT8M - cook + Recipe1 M 11.53 17.62 39.03 1.22 E2vidD2 - MASSvid - BiD Video + ASR YT8M - cook + Recipe1 M 11.17 17.71 38.32 1.17 E2vidD6 - MASSvid - BiD Video + ASR YT8M - cook + Recipe1 M 12.04 18.32 39.03 1.23 E2vidD2 - MASSdrop - BiD Video + ASR YT8M - cook + Recipe1 M 11.21 17.99 38.72 1.23 E2vidD6 - MASSdrop - BiD Video + ASR YT8M - cook + Recipe1 M 10.45 17.74 38.82 1.22 Human ( Hessel et al . 
, 2019 ) Video + ASR - 15.20 25.90 45.10 3.80 Table 10 : Video Captioning Results on YouCook2 . 
We use YT8M - cook / Recipe1 M for pretraining . 
All video features are Compact 2D ( Wang et al . 
, 2014 ) except when marked as S3D ( Xie et al . 
, 2018).488 Method Input Pretraining BLEU-1 METEOR ROUGE - L CIDEr Constant baseline ( ‚Äú intro ‚Äù ) - - 1.42 3.32 11.15 0.28 √ò Pretraining E2D2 - UniD ASR - 17.94 8.55 27.06 0.64 E2D6 - UniD ASR - 18.91 8.96 27.80 0.67 E2D2 - BiD ASR - 18.81 8.82 27.63 0.65 E2D6 - BiD ASR - 19.60 9.12 27.88 0.68 E2vidD2 - UniD Video + ASR - 18.94 8.99 28.05 0.67 E2vidD6 - UniD Video + ASR - 19.29 9.15 27.97 0.69 E2vidD2 - BiD Video + ASR - 19.37 9.21 28.56 0.69 E2vidD6 - BiD Video + ASR - 19.49 9.23 28.53 0.69 Text Pretraining E2D2 - MASS - UniD ASR HowTo100 M + WikiHow 21.53 10.24 29.95 0.77 E2D6 - MASS - UniD ASR HowTo100 M + WikiHow 22.09 10.58 30.67 0.79 E2D2 - MASS - BiD ASR HowTo100 M + WikiHow 20.73 10.20 30.15 0.76 E2D6 - MASS - BiD ASR HowTo100 M + WikiHow 21.93 10.60 30.45 0.79 E2vidD2 - MASS - UniD Video + ASR HowTo100 M + WikiHow 21.46 10.45 30.56 0.78 E2vidD6 - UniD Video + ASR HowTo100 M + WikiHow 22.21 10.75 30.86 0.81 E2vidD2 - MASS - BiD Video + ASR HowTo100 M + WikiHow 21.78 10.64 30.72 0.79 E2vidD6 - MASS - BiD Video + ASR HowTo100 M + WikiHow 22.44 10.83 31.27 0.81 Multimodal Pretraining E2vidD2 - MASSalign - BiD Video + ASR HowTo100 M + WikiHow 22.07 10.33 30.60 0.77 E2vidD6 - MASSalign - BiD Video + ASR HowTo100 M + WikiHow 22.31 10.66 31.13 0.79 E2vidD2 - MASSvid - BiD Video + ASR HowTo100 M + WikiHow 22.15 10.75 31.06 0.80 E2vidD6 - MASSvid - BiD Video + ASR HowTo100 M + WikiHow 22.45 10.76 31.49 0.80 E2vidD2 - MASSdrop - BiD Video + ASR HowTo100 M + WikiHow 21.84 10.55 31.10 0.79 E2vidD6 - MASSdrop - BiD Video + ASR HowTo100 M + WikiHow 22.37 11.00 31.40 0.82 Human estimate Video + ASR - 43.34 33.56 41.88 1.26 Table 11 : Video captioning results on ViTT - All . 
We use HowTo100M / WikiHow for pretraining . 
We also estimate human performance ( details in Appendix A.1 ; Table 9).489 Method Input Pretraining BLEU-1 METEOR ROUGE - L CIDEr Constant baseline ( ‚Äú intro ‚Äù ) - - 1.16 2.93 10.21 0.25 √ò Pretraining E2D2 - UniD ASR - 19.73 9.43 27.95 0.69 E2D6 - UniD ASR - 20.24 9.93 28.59 0.71 E2D2 - BiD ASR - 19.73 9.72 27.92 0.68 E2D6 - BiD ASR - 20.77 10.08 28.63 0.72 E2vidD2 - UniD Video + ASR - 19.97 9.75 28.30 0.69 E2vidD6 - UniD Video + ASR - 20.46 9.93 28.62 0.69 E2vidD2 - BiD Video + ASR - 20.60 10.08 29.45 0.71 E2vidD6 - BiD Video + ASR - 20.45 9.88 28.88 0.69 Text Pretraining E2D2 - MASS - UniD ASR YT8M - cook + Recipe1 M 22.89 11.53 31.62 0.84 E2D6 - MASS - UniD ASR YT8M - cook + Recipe1 M 24.47 12.22 32.51 0.90 E2D2 - MASS - BiD ASR YT8M - cook + Recipe1 M 22.75 11.63 31.54 0.84 E2D6 - MASS - BiD ASR YT8M - cook + Recipe1 M 24.79 12.25 32.40 0.88 E2vidD2 - MASS - UniD Video + ASR YT8M - cook + Recipe1 M 23.86 11.85 32.32 0.86 E2vidD6 - MASS - UniD Video + ASR YT8M - cook + Recipe1 M 24.32 12.32 32.90 0.90 E2vidD2 - MASS - BiD Video + ASR YT8M - cook + Recipe1 M 22.93 11.68 32.15 0.87 E2vidD6 - MASS - BiD Video + ASR YT8M - cook + Recipe1 M 24.22 12.22 32.60 0.89 Multimodal Pretraining E2vidD2 - MASSalign - BiD Video + ASR YT8M - cook + Recipe1 M 24.02 11.91 32.73 0.86 E2vidD6 - MASSalign - BiD Video + ASR YT8M - cook + Recipe1 M 24.92 12.25 33.09 0.90 E2vidD2 - MASSvid - BiD Video + ASR YT8M - cook + Recipe1 M 24.15 12.10 32.96 0.88 E2vidD6 - MASSvid - BiD Video + ASR YT8M - cook + Recipe1 M 24.87 12.43 32.97 0.90 E2vidD2 - MASSdrop - BiD Video + ASR YT8M - cook + Recipe1 M 23.70 12.01 32.71 0.88 E2vidD6 - MASSdrop - BiD Video + ASR YT8M - cook + Recipe1 M 24.48 12.22 33.10 0.89 Human estimate Video + ASR - 41.61 32.50 41.59 1.21 Table 12 : Video captioning results on ViTT - Cooking . 
We use YT8M - cook and Recipe1 M for optional pretraining.490 Proceedings of the 1st Conference of the Asia - PaciÔ¨Åc Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing , pages 491‚Äì503 December 4 - 7 , 2020 . 
¬© 2020 Association for Computational Linguistics Systematic Generalization on gSCAN with Language Conditioned Embedding Tong Gao‚àóQi Huang‚àóRaymond J. Mooney Department of Computer Science University of Texas at Austin { gaotong , qhuang , mooney } @cs.utexas.edu Abstract Systematic Generalization refers to a learning algorithm ‚Äôs ability to extrapolate learned behavior to unseen situations that are distinct but semantically similar to its training data . 
As shown in recent work , state - of - the - art deep learning models fail dramatically even on tasks for which they are designed when the test set is systematically different from the training data . 
We hypothesize that explicitly modeling the relations between objects in their contexts while learning their representations will help achieve systematic generalization . 
Therefore , we propose a novel method that learns objects ‚Äô contextualized embedding with dynamic message passing conditioned on the input natural language and is end - to - end trainable with other downstream deep learning modules . 
To our knowledge , this model is the Ô¨Årst one that signiÔ¨Åcantly outperforms the provided baseline and reaches state - of - the - art performance ongrounded SCAN ( gSCAN ) , a grounded natural language navigation dataset designed to require systematic generalization in its test splits . 
1 Introduction Systematic Generalization refers to a learning algorithm ‚Äôs ability to extrapolate learned behavior to unseen situations that are distinct but semantically similar to its training data . 
It has long been recognized as a key aspect of humans ‚Äô cognitive capacities ( Fodor et al . 
, 1988 ) . 
SpeciÔ¨Åcally , humans ‚Äô mastery of systematic generalization is prevalent in grounded natural language understanding . 
For example , humans can reason about the relations between all pairs of concepts from two domains , even if they have only seen a small subset of pairs during training . 
If a child observes ‚Äù red squares ‚Äù , ‚Äù green squares ‚Äù and ‚Äù yellow circles ‚Äù , he or she can ‚àó ( * ) denotes co-Ô¨Årst authorship , authors contribute equally and are listed in alphabetical order.recognize ‚Äù red circles ‚Äù at their Ô¨Årst encounter . 
Humans can also contextualize their reasoning about objects ‚Äô attributes . 
For example , a city being referred to as ‚Äù the larger one ‚Äù within a state might be referred to as ‚Äù the smaller one ‚Äù nationwide . 
In the past decade , deep neural networks have shown tremendous success on a collection of grounded natural language processing tasks , such as visual question answering ( VQA ) , image captioning , and vision - and - language navigation ( Hudson and Manning , 2018 ; Anderson et al . 
, 2018a , b ) . 
Despite all the success , recent literature shows that current deep learning approaches are exploiting statistical patterns discovered in the datasets to achieve high performance , an approach that does not achieve systematic generalization . 
Gururangan et al . 
( 2018 ) discovered that annotation artifacts like negation words or purpose clauses in natural language inference data can be used by simple text classiÔ¨Åcation categorization model to solve the given task . 
Jia and Liang ( 2017 ) demonstrated that adversarial examples can fool reading comprehension systems . 
Indeed , deep learning models often fail to achieve systematic generalizations even on tasks on which they are claimed to perform well . 
As shown by Bahdanau et al . 
( 2018 ) , state - of - the - art Visual Questioning Answering ( VQA ) ( Hudson and Manning , 2018 ; Perez et al . 
, 2018 ) models fail dramatically even on a synthetic VQA dataset designed with systematic difference between training and test sets . 
In this work , we focus on approaching systematic generalization in grounded natural language understanding tasks . 
We experiment with a recently introduced synthetic dataset , grounded SCAN ( gSCAN ) , that requires systematic generalization to solve ( Ruis et al . 
, 2020 ) . 
For example , after observing how to ‚Äù walk hesitantly ‚Äù to a target object in a grid world , the learning agent is tested with instruction that requires it to ‚Äù pull hesitantly ‚Äù , therefore testing its ability to generalize adverbs to491 unseen adverb - verb combinations . 
When presented with a world of objects with different attributes , and natural language sentences that describe such objects , the goal of the model is to generalize its ability to understand unseen sentences describing novel combinations of observed objects , or even novel objects with observed attributes . 
One of the essential steps in achieving this goal is to obtain good object embeddings to which natural language can be grounded . 
By considering each object as a bag of its descriptive attributes , this problem is further transformed into learning good representations for those attributes based on the training data . 
This requires : 1 ) learning good representations of attributes whose actual meanings are contextualized , for example , ‚Äù smaller ‚Äù and ‚Äù lighter ‚Äù , etc . 
; 2 ) learning good representations for attributes so that conceptually similar attributes , e.g. , ‚Äù yellow ‚Äù and ‚Äù red ‚Äù , have similar representations . 
We hypothesize that explicitly modeling the relations between objects in their contexts , i.e. , learning contextualized object embeddings , will help achieve systematic generalization . 
This is intuitively helpful for learning concepts with contextualized meaning , just as learning to recognize the ‚Äù smaller ‚Äù object in a novel pair requires experience of comparison between semantically similar object pairs . 
Learning contextualized object embeddings can also be helpful for obtaining good representations for semantically similar concepts when such concepts are the only difference between two contexts . 
Inspired by Hu et al . 
( 2019 ) , we propose a novel method that learns an object ‚Äôs contextualized embedding with dynamic message passing conditioned on the input natural language . 
At each round of message passing , our model collects relational information between each object pair , and constructs an object ‚Äôs contextualized embedding as a weighted combination of them . 
Such weights are dynamically computed conditioned on the input natural sentence . 
This contextualized object embedding scheme is trained end - to - end with downstream deep modules for speciÔ¨Åc grounded natural language processing tasks , such as navigation . 
Experiments show that our approach signiÔ¨Åcantly outperforms a strong baseline on gSCAN . 
2 Related Work Research on deep learning models ‚Äô systematic generalization behavior has gained traction in recent years , with particular focus on natural languageprocessing tasks . 
2.1 Compositionality An idea that is closely related to systematic generalization is compositionality . 
Kamp and Partee ( 1995 ) deÔ¨Åne the principle of compositionality as ‚Äú The meaning of a whole is a function of the meanings of the parts and of the way they are syntactically combined ‚Äù . 
Hupkes et al . 
( 2020 ) synthesizes different interpretations of this abstract principle into 5 theoretically grounded tests to evaluate a model ‚Äôs ability to represent compositionality : 1 ) Systematicity : if the model can systematically recombine known parts and rules ; 2 ) Productivity : if the model can extend their predictions beyond what they have seen in the training data ; 3 ) Substitutivity ; if the model is robust to synonym substitutions ; 4 ) Localism : if the model ‚Äôs composition operations are local or global ; and 5 ) Overgeneralisation : if the model favors rules or exceptions during training . 
The gSCAN dataset focuses more on capturing the Ô¨Årst three tests in a grounded natural language understanding setting , and our proposed model achieves signiÔ¨Åcant performance improvement on test sets relating to systematicity and substitutivity . 
2.2 Systematic Generalization Datasets Many systematic generalization datasets have been proposed in recent years ( Bahdanau et al . 
, 2018 ; Chevalier - Boisvert et al . 
, 2019 ; Hill et al . 
, 2020 ; Lake and Baroni , 2017 ; Ruis et al . 
, 2020 ) . 
This paper is conceptually most related to the SQOOP dataset proposed by Bahdanau et al . 
( 2018 ) , the SCAN dataset proposed by Lake and Baroni ( 2017 ) , and the gSCAN dataset proposed by Ruis et al . 
( 2020 ) . 
The SQOOP dataset consists of a random number of MNIST - style alphanumeric characters scattered in an image with speciÔ¨Åc spatial relations ( ‚Äù left ‚Äù , ‚Äù right ‚Äù , ‚Äù up ‚Äù , ‚Äù down ‚Äù ) among them ( Bahdanau et al . 
, 2018 ) . 
The algorithm is tested with a binary decision task of reasoning about whether a speciÔ¨Åc relation holds between a pair of alphanumeric characters . 
Systematic difference is created between the testing and training set by only providing supervision on relations for a subset of digit pairs to the learner , while testing its ability to reason about relations between unseen alphanumeric character pairs . 
For example , the algorithm is tested with questions like ‚Äú is S above T ‚Äù while it never sees a relation involving both S and T during train-492 ing . 
Therefore , to fully solve this dataset , it must learn to generalize its understanding of the relation ‚Äú above ‚Äù to unseen pairs of characters . 
Lake and Baroni ( 2017 ) proposed the SCAN dataset and its related benchmark that tests a learning algorithm ‚Äôs ability to perform compositional learning and zeroshot generalization on a natural language command translation task . 
Given a natural language command with a limited vocabulary , an algorithm needs to translate it into a corresponding action sequence consisting of action tokens from a Ô¨Ånite token set . 
Compared to SQOOP , SCAN tests the algorithm ‚Äôs ability to learn more complicated linguistic generalizations like ‚Äù walk around left ‚Äù to ‚Äù walk around right ‚Äù . 
SCAN also ensures that the target action sequence is unique , and an oracle solution exists by providing an interpreter function that can unambiguously translate any given command to its target action sequence . 
Going beyond SCAN that focuses purely on syntactic aspects of systematic generalization , the gSCAN dataset proposed by Ruis et al . 
( 2020 ) is an extension of SCAN . 
It contains a series of systematic generalization tasks that require the learning agent to ground its understanding of natural language commands in a given grid world to produce the correct action token sequence . 
We choose gSCAN as our benchmark dataset , as its input command sentences are linguistically more complex , and requires processing multi - modal input . 
2.3 Systematic Generliazation Algorithms Bahdanau et al . 
( 2018 ) demonstrated that modular networks , with a carefully chosen module layout , can achieve nearly perfect systematic generalization on SQOOP dataset . 
Our approach can be considered as a conceptual generalization of theirs . 
Each object ‚Äôs initial embedding can be considered as a simple afÔ¨Åne encoder module , and we learn the connection scheme among these modules conditioned on natural language instead of handdesigning it . 
Gordon et al . 
( 2019 ) proposed solving the SCAN benchmark by hard - coding their model to be equivariant to all permutations of SCAN ‚Äôs verb primitives . 
Andreas ( 2020 ) proposed GECA ( ‚Äú Good - Enough Compositional Augmentation ‚Äù ) that systematically augments the SCAN dataset by identifying sentence fragments with similar syntactic context , and permuting them to generate novel training examples . 
This line of permutationinvariant approaches is shown to not generalizewell on the gSCAN dataset ( Ruis et al . 
, 2020 ) . 
At the time of submission , our method was the Ô¨Årst to outperform the strong baseline provided in the gSCAN benchmark , and also the Ô¨Årst one to apply language - conditioned message passing to learn contextualized input embedding for systematic generalization tasks . 
Concurrent to our work , Kuo et al . 
( 2020 ) proposed a family of parse - tree - based compositional RNN networks to enable systematic generalization , and heavily relies on off - the - shelf parsers to produce the network hierarchy . 
HeinzeDeml and Bouchacourt ( 2020 ) use an attentionbased prediction of the target object ‚Äôs location as an auxilary training task to regularize the model . 
However , it only improves over the baseline model in Ruis et al . 
( 2020 ) in a limited subset of test splits . 
For completeness , we also compare our model ‚Äôs result with the above two concurrent works . 
3 Problem DeÔ¨Ånition & Algorithm 3.1 Task DeÔ¨Ånition gSCAN contains a series of systematic generalization tasks in a grounded natural language understanding setting . 
In gSCAN , the learning agent is tested with the task of following a given natural language instruction to navigate in a twodimensional grid world with objects . 
This is achieved in the form of generating a sequence of action tokens from a Ô¨Ånite action token set A={walk , push , pull , stay , L turn , Rturn}that brings the agent from its starting location to the target location . 
An object in gSCAN ‚Äôs world state is encoded with a one - hot encoding describing its attributes in three property types : 1 ) colorC={red , green , blue , yellow } 2 ) shape S={circle , square , cylinder } 3 ) sizeD= { 1,2,3,4 } . 
The agent is also encoded as an ‚Äú object ‚Äù in the grid world , with properties including orientationO={left , right , up , down } and a binary variableB={yes , no}denoting the presence of the agent . 
Therefore , the whole grid is represented as a tensor xS‚ààRd√ód√óc , wheredis the dimension of the grid , and c=|C|+|S|+|D|+ |O|+|B| . 
Mathematically , given an input tuple x= ( xc , xS ) , wherexc={xc 1,xc 2, ... ,xc n}represents the navigation instruction , the agent needs to predict the correct output action token sequence y={y1,y2, ... ,y m } . 
Despite its simple form , this task is quite challenging . 
For one , generating the correct action token sequence requires understanding the instruction within the context of the agent‚Äôs493 current grid world . 
It also involves connecting speciÔ¨Åc instructions with complex dynamic patterns . 
For example , ‚Äú pulling ‚Äù a square will be mapped to a ‚Äú pull ‚Äù command when the square has a size of 1 or 2 , but to ‚Äú pull pull ‚Äù when the square has a size of 3 or 4 ( a ‚Äú heavy ‚Äù square ) ; ‚Äú move cautiously ‚Äù requires the agent to turn left then turn right before making the actual move . 
gSCAN also introduces a series of test sets that have systematic differences from the training set . 
Computing the correct action token sequences on these test sets requires the model to learn to combine seen concepts into novel combinations , including novel object property combinations , novel contextual references , etc .. 3.2 Model Architecture The overview of our model architecture is shown in Figure 1 . 
At the highest level , it follows the same encoder - decoder framework used by the baseline model in Ruis et al . 
( 2020 ) to extract information from the input sentence / grid - world representation and to output navigation instructions . 
However , there is a paradigm shift in how we represent and encode the grid world . 
Instead of viewing the grid world as a whole , we treat it as a collection of objects whose semantic meanings should be contextualized by their relations with one another . 
We also hypothesize that inter - object relations that are salient in a given grid world can be inferred from the accompanied natural language instruction . 
Therefore , we expand the vanilla CNN - based grid world encoder with a message passing module guided by the accompanied natural language instruction to obtain the contextualized grid - world embedding . 
3.2.1 Input Extraction Given the input sentence and the grid world state , we Ô¨Årst project them into higher dimensional embedding . 
For the input instruction I= { w1,w2, ... ,w S}wherewiis the embedding vector of wordi , following the practice of Ruis et al . 
( 2020 ) and Hu et al . 
( 2019 ) , we Ô¨Årst encode it as the hidden states{hs}S s=1and the summary vector sobtained by feeding the input Ito a Bi - LSTM as : [ h1,h2, ... ,h S ] = BiLSTM ( I)ands= [ h1;hS](1 ) Where we use semi - colon to represent concatenation , andhi= [ ‚àí ‚Üíhi;‚Üê ‚àíhi]is the concatenation of the forward and backward direction of the LSTM hidden state for input word i. For each round of mes - sage passing between the objects embedding , we further apply a transformation using a multi - step textual attention module similar to that of Hudson and Manning ( 2018 ) and Hu et al . 
( 2018 ) to extract the round - speciÔ¨Åc textual context . 
Given a round - speciÔ¨Åc projection matrix Wt 2 , the textual attention score for word iat message passing round tis computed as : Œ±t , i = softmax s(W1(hi‚äô(Wt 2ReLU ( W3s ) ) ) ) ( 2 ) The Ô¨Ånal textual context embedding for message passing round tis computed as : ct = S / summationdisplay i=1Œ±t , i¬∑hi ( 3 ) Details of the message passing mechanism will be described in later sections . 
As for the grid - world representation , from each grid , we extract one - hot representations of color C , shapeS , sizeDand agent orientation O , and embed each property with a 16 - dimensional vector . 
We Ô¨Ånally concatenate them back into one vector and use this vector as the object ‚Äôs local embedding . 
3.2.2 Language - conditioned Message Passing After extracting a textual context embedding and the objects ‚Äô local embedding , we perform a language - conditioned iterative message passing forTrounds to obtain the contextualized object embeddings , where Tis a hyper - parameter . 
1 ) Denoting the extracted object local embedding asxloc , and previous round ‚Äôs object context embedding as xctx , we Ô¨Årst construct a fused representation of an object iat roundtby concatenating its local , context embedding as well their elementwise product : xfuse i , t= [ xloc i , xctx i , t‚àí1,(W4xloc i)‚äô(W5xctx i , t‚àí1 ) ] ( 4 ) We use an object ‚Äôs local embedding to initialize its context embedding at round 0 . 
2 ) For each pair of objects ( i , j ) , we use their fused representations , together with this round ‚Äôs textual context embedding to compute their message passing weight as : wt i , j = softmax s(W6xfuse j , t)T((W7xfuse i , t)‚äô(W8ct))(5 ) Note that the computation of the raw weight logits is asymmetric.494 Figure 1 : Model Overview 3 ) We consider all the objects in a grid world as nodes , and they together form a complete graph . 
Each nodeicomputes its message to receiver node jas : mt i , j = wt i , j¬∑((W9xfuse i , t‚äô(W10ct ) ) ( 6 ) and each receiver node jupdates its context embedding as : xctx j , t = W11[xctx j , t‚àí1;N / summationdisplay i=1mt i , j ] ( 7 ) AfterTrounds of iterative message passing , the Ô¨Ånal contextualized embedding for object iwill be : xout i = W12[xloc i;xctx i , T ] ( 8) 3.2.3 Encoding the Grid World After obtaining contextualized embeddings for all objects in a grid world xsas{xout}n= { xout 1,xout 2, ... ,xout n}each of dimensionality Rout , we map them back to their locations in the grid world , and construct a new grid world representationXs / prime‚ààRd√ód√óoutby zero - padding cells without any object . 
This is then fed into three parallel single convolutional layers with different kernel sizes to obtain a grid world ‚Äôs embedding at multiple scales , as done by Wang and Lake ( 2019 ) . 
The Ô¨Ånal grid world encoding is as follows : Hs= [ Hs 1;Hs 2;Hs 3 ] , Hs i = Conv i(Xs / prime ) ( 9)whereConv idenotes the ith convolutional network , andHs‚ààRd2√óhid . 
3.2.4 Decoding Action Sequences We use a Bi - LSTM with multi - modal attention to both the grid world embedding and the input instruction embedding to decode the Ô¨Ånal action sequence , following the baseline model provided by Ruis et al . 
( 2020 ) . 
At each step i , the hidden state of the decoder hd iis computed as : hd i = LSTM ( [ ed i;cc i;cs i],hd i‚àí1 ) ( 10 ) whereed iis the embedding of the previous output action token yi‚àí1,cc iis the instruction context computed with attention over textual encoder ‚Äôs hidden states [ hc 1,hc 2, ... ,hc S ] , andcs iis the grid world context computed with attention over all locations in the grid world embedding HS . 
We set the decoder ‚Äôs hidden size to 64 so that it aligns with the textual encoder , and use the attention implementation proposed by Bahdanau et al . 
( 2016 ) . 
The instruction context is computed as : ec ij = vT ctanhW c(hd i‚àí1+hc j ) ( 11 ) Œ±c ij = exp(ec ij ) /summationtextS j=1exp(ec ij)(12 ) cc i = S / summationdisplay j=1Œ±c ijhc j,‚àÄj‚àà{1,2, ... ,S } ( 13)495 Similarly , the grid world context is computed as : es ij = vT stanhW s(hd i‚àí1+cc i ) ( 14 ) Œ±s ij = exp(es ij ) /summationtextd2 j=1exp(es ij)(15 ) cs i = d2 / summationdisplay j=1Œ±s ijhs j,‚àÄj‚àà{1,2, ... ,d2 } ( 16 ) wherevs , vc , Wc , Wsare learnable parameters , and hs jis the embedding of grid jobtained from HS . 
The distribution of next action token can then be computed as p(yi|x , y 1,y2, ... ,y i‚àí1 ) = softmax ( Wohd i ) . 
4 Experimental Evaluation 4.1 Methodology & Implementation We run experiments to test the hypothesis that contextualized embeddings help systematic generalization1 . 
Since this task has a limited vocabulary size , word - level accuracy is no longer a proper metric to reÔ¨Çect the model ‚Äôs performance . 
We follow the baseline and use the exact match percentage as our metric , where an exact match means that the produced action token sequence is exactly the same as the target sequence . 
We compare our model with the baseline on different test sets , and use early stopping based on the exact match score on the validation set . 
We set the learning rate as 1e-4 , decaying by 0.9 every 20,000 steps . 
We choose the number of message passing iterations to be 4 . 
Our model is trained for 6 separate runs , and the average performance as well as the standard deviation are reported . 
Our encoder / decoder model is implemented in PyTorch ( Paszke et al . 
, 2017 ) and the message passing graph network is backed by DGL ( Wang et al . 
, 2020 ) . 
For comparison , we use test set , validation set , and baseline model released by Ruis et al . 
( 2020 ) . 
4.2 Results Table 1 is an overview of 7 test splits used for evaluation , and table 2 shows our experiment results as well as other models ‚Äô performance for comparison . 
In the following sections , we present the results on each systematic generalization test split , and also introduce the conÔ¨Åguration of test splits . 
Note that test split A is a random split set that has no systematic difference from the training set . 
1Code is available hereSplit B : This tests the model ‚Äôs ability to generalize to navigation in a novel direction . 
For example , a testing example would require the agent to move to a target object that is to its southwest , even though during training target objects are never placed south - west of the agent . 
Although our model manages to predict some correct action sequences compared to the baseline ‚Äôs complete failure , our model still fails on the majority of cases . 
We further analyze the failure on Split B in the discussion section . 
Split C , G : Split C tests the model ‚Äôs ability to generalize to novel contextual references . 
In the training set , a circle of size 2 is never referred to as ‚Äú the small circle ‚Äù , while in the test set the agent needs to generalize the notion ‚Äú small ‚Äù to it based on its size comparison with other circles in the grid world . 
The message passing mechanism helps the model comprehend the relative sizes of objects , and boost the performance on split C. Besides , our model shows promising results on exploring the interrelationship between an agent and other objects in the scene , as well as learning abstract concepts by contextual comparison as shown in split G. This test split asks the model to push a square of size 3 . 
An object with the size of 3 or 4 is deÔ¨Åned as ‚Äú heavy ‚Äù , according to the conÔ¨Åguration , and requires two consecutive push / pull actions applied on it before it actually moves . 
The challenge here is that the model has been trained to‚Äúpull ‚Äù heavy squares and ‚Äú push ‚Äù squares with size of 4 , but was never trained to ‚Äú push ‚Äù a size-3 square . 
Thus , it needs to generalize the concept of ‚Äú heavy ‚Äù and act accordingly . 
Split D , E : Split D and E are similar , as they both deÔ¨Åne the target object with novel combinations of color and shape . 
Split E is generally easier because the target object , a yellow square , appears as the target in training examples , but is only referred to as ‚Äú the square ‚Äù , ‚Äú the smaller square ‚Äù , or ‚Äú the bigger square ‚Äù . 
Split D increases the difÔ¨Åculty by referring to the red square , which never appears in the training set as a target but does appear as a background object . 
We Ô¨Ånd that while the baseline model understands the concept of ‚Äú square ‚Äù , it gets confused by target objects with a new color - shape combination . 
In contrast , our model can generalize to novel compositions of object properties and correctly Ô¨Ånd the target object , performing signiÔ¨Åcantly better on these two splits . 
Split F : This split is designed to test the model‚Äôs496 Split Description A : Random Randomly split test sets B : Novel Direction Target object is to the South - West of the agent C : Relativity Target object is a size 2circle , referred to with the small modiÔ¨Åer D : Red Squares Red squares are the target object E : Yellow Squares Yellow squares are referred to with a color and a shape at least F : Adverb to Verb All examples with the adverb ‚Äô while spinning ‚Äô and the verb ‚Äô pull ‚Äô G : Class Inference All examples where the agent needs to push a square of size 3 Table 1 : Description of test splits Split Baseline Kuo et al . 
( 2020 ) Heinze - Deml and Bouchacourt ( 2020 ) Ours A : Random 97.69¬±0.22 97.32 94.19¬±0.71 98.6¬±0.95 B : Novel Direction 0¬±0 5.73 N / A 0.16¬±0.12 C : Relativity 35.02¬±2.35 75.19 43.43¬±7.0 87.32¬±27.38 D : Red Squares 23.51¬±21.82 80.16 81.07¬±10.12 80.31¬±24.51 E : Yellow Squares 54.96¬±39.39 95.35 86.45¬±6.28 99.08¬±0.69 F : Adverb to Verb 22.7¬±4.59 0 N / A 33.6¬±20.81 G : Class Inference 92.95¬±6.75 98.63 N / A 99.33¬±0.46 Table 2 : Exact match accuracy of test splits ability to generalize to novel adverb - verb combinations , where the model is tested under different situations but always with the terms ‚Äú while spinning ‚Äù and ‚Äú pull ‚Äù in the commands . 
However , they never appear in the training set together , consequently the model needs to generalize to this novel combination of adverb and verb . 
The results shows that our model does a bit better than the baseline , but suffers from high variance across different runs . 
Comparing to the two concurrent works Kuo et al . 
( 2020 ) and Heinze - Deml and Bouchacourt ( 2020 ) , our model yields better performance in general . 
Notice that Heinze - Deml and Bouchacourt ( 2020 ) and our model also report the standard deviation of multiple runs , while Kuo et al . 
( 2020 ) does not . 
4.3 Discussion Model Comparison . 
We reveal the strength of our model by analyzing two test examples where it succeeds and the baseline fails . 
For each example , we visualize the grid world that the agent is in , where each cell is colored with different grey - scale levels indicating its assigned attention score . 
For reader ‚Äôs convenience , we also visualize the model ‚Äôs prediction and the target sequence by the red path and green path , respectively . 
Figure 2 from split G visualizes the prediction sequence as well as the attention weights generated by the baseline . 
The baseline attends to the position of the target object but is unable to capture the dynamic relationship between the target object and the green cylinder . 
It tries to push the target object over it , while our model correctly predicts Figure 2 : While the target is correctly chosen , the baseline did not stop pushing even after encountering an obstacle . 
the incoming collision and stops at the right time . 
Another example from split D where our model outperforms the baseline is shown in Figure 3 . 
The baseline model incorrectly attends to two small blue squares and picks one as the target rather than the correct small red square . 
Note that the model has seen blue and green squares as targets in the training set , but has never seen a red square . 
This is a common mistake since the baseline struggles to choose target objects with novel property combinations when there are similar objects in the scene that were seen during training . 
On the contrary,497 Figure 3 : Baseline can not distinguish the correct square from similar candidates . 
our model handles these cases well , demonstrating its ability to generalize to novel color - shape combinations with the help of contextualized object embedding . 
Split No Message Passing Full Model A 91.07¬±0.61 98.6¬±0.95 B 0.16¬±0.04 0.16¬±0.12 C 50.26¬±5.9 87.32¬±27.38 D 35.95¬±13.13 80.31¬±24.51 E 44.18¬±24.56 99.08¬±0.69 F 44.82¬±1.95 33.6¬±20.81 G 93.02¬±0.33 99.33¬±0.46 Table 3 : Ablation study Ablation Study . 
We conduct an ablation study to test the signiÔ¨Åcance of the languageconditioned message passing component in our network . 
We built a model whose architecture and hyper - parameters are the same as our full model , except that we remove the language - conditioned message passing module described in section 3.2.2 . 
That is , we follow all the steps in section 3.2.1 and obtain every object ‚Äôs local embedding , then map new embedding back to the their locations as stated in section 3.2.3 . 
The results in Table 3 indicate that language - conditioned message passing does help achieve higher exact match accuracy in many test splits , though it sometimes hurts the performance on split F. We conclude that the model is getting better at understanding object - related com - mands ( ‚Äú pull ‚Äù moves the object ) , sacriÔ¨Åcing some ability to discover the meaning of easy - to - translate adverbs that are irrelevant to the interaction with objects ( ‚Äú while spinning ‚Äù only describes the behavior of agent with no impact on the scene ) . 
Failure on Split B. Here we analyze a failure case to understand why split B is notably difÔ¨Åcult for our model . 
Figure 4 demonstrates an example that leads to both models ‚Äô failure . 
The attention scores indicate that the model has identiÔ¨Åed the correct target position , but does not know the correct action sequence to get there . 
The LSTM decoder can not generalize the meaning of action tokens that direct the agent towards an unseen direction . 
We can observe from our model ‚Äôs output prediction that , even if it manages to correctly predict the Ô¨Årst few steps ( ‚Äù turn left turn left walk ‚Äù ) , it quickly gets lost and fails to navigate to the target location . 
The model only observes the initial world state and the command , then generates a sequence of actions toward the target . 
In other words , it is blindly generating the action sequence with only a static image of the agent and the target ‚Äôs location , not really modeling the movement of the agent . 
However , humans usually do not handle navigation in a novel direction in this way . 
Instead , they will Ô¨Årst turn to the correct direction , and transform the novel task into a familiar task ( ‚Äù walk southwest is equivalent to turn southwest then walk the same as you walk north ‚Äù ) . 
This naturally requires a change of perspective and conditioning on the agent ‚Äôs previous action . 
A possible improvement is to introduce clues to inform the model of possible changes in its view as it takes actions . 
5 Conclusion and Future Work In this paper , we proposed a language - conditioned message passing model for a grounded language navigation task that can dynamically extract contextualized embeddings based on input command sentences , and can be trained end - to - end with the downstream action - sequence decoder . 
We showed that obtaining such contextualized embeddings improves performance on a recently introduced challenge problem , gSCAN , signiÔ¨Åcantly outperforming the state - of - the - art across several test splits designed to test a model ‚Äôs ability to represent novel concept compositions and achieve systematic generalization . 
Nonetheless , our model ‚Äôs fairly poor performance on split B and F shows that challenges still498 Figure 4 : Failure case on split B , prediction and attention scores were generated by our model . 
remain . 
As explained in the discussion section , our model is falling short of estimating the effect of each action on the agent ‚Äôs state . 
An alternative view of this problem is as a reinforcement learning task with sparse reward . 
Sample - efÔ¨Åcient model - based reinforcement learning ( Buckman et al . 
, 2018 ) could then be used , and its natural ability to explicitly model environment change should improve performance on this task . 
It would also be beneÔ¨Åcial to visualize the dynamically generated edge weights during message passing to have a more intuitive understanding of what contextual information is integrated during the message passing phase . 
Currently , we consider all objects appearing on the grid , including the agent , as homogeneous nodes during message passing , and all edges in the message passing graph are modelled in the same way . 
However , intuitively , we should model the relation between different types of objects differently . 
For example , the relation between the agent and the target object of pulling might be different from the relation between two objects on the grid . 
Inspired by Bahdanau et al . 
( 2018 ) , it would be interesting to try modeling different edge types explicitly with neural modules , and perform type - speciÔ¨Åc message passing to obtain better contextualized embeddings . 
References Peter Anderson , Xiaodong He , Chris Buehler , Damien Teney , Mark Johnson , Stephen Gould , and Lei Zhang . 
2018a . 
Bottom - up and top - down attention for image captioning and visual question answering . 
InProceedings of the IEEE conference on computer vision and pattern recognition , pages 6077‚Äì6086 . 
Peter Anderson , Qi Wu , Damien Teney , Jake Bruce , Mark Johnson , Niko S ¬®underhauf , Ian Reid , Stephen Gould , and Anton van den Hengel . 
2018b . 
Visionand - language navigation : Interpreting visuallygrounded navigation instructions in real environments . 
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 3674‚Äì3683 . 
Jacob Andreas . 
2020 . 
Good - enough compositional data augmentation . 
In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 7556‚Äì7566 , Online . 
Association for Computational Linguistics . 
Dzmitry Bahdanau , Kyunghyun Cho , and Yoshua Bengio . 
2016 . 
Neural machine translation by jointly learning to align and translate . 
Dzmitry Bahdanau , Shikhar Murty , Michael Noukhovitch , Thien Huu Nguyen , Harm de Vries , and Aaron C. Courville . 
2018 . 
Systematic generalization : What is required and can it be learned ? CoRR , abs/1811.12889 . 
Jacob Buckman , Danijar Hafner , George Tucker , Eugene Brevdo , and Honglak Lee . 
2018 . 
SampleefÔ¨Åcient reinforcement learning with stochastic ensemble value expansion . 
In Advances in Neural Information Processing Systems , pages 8224‚Äì8234 . 
Maxime Chevalier - Boisvert , Dzmitry Bahdanau , Salem Lahlou , Lucas Willems , Chitwan Saharia , Thien Huu Nguyen , and Yoshua Bengio . 
2019 . 
Babyai : A platform to study the sample efÔ¨Åciency of grounded language learning . 
Jerry A Fodor , Zenon W Pylyshyn , et al . 
1988 . 
Connectionism and cognitive architecture : A critical analysis . 
Cognition , 28(1 - 2):3‚Äì71 . 
Jonathan Gordon , David Lopez - Paz , Marco Baroni , and Diane Bouchacourt . 
2019 . 
Permutation equivariant models for compositional generalization in language . 
In International Conference on Learning Representations . 
Suchin Gururangan , Swabha Swayamdipta , Omer Levy , Roy Schwartz , Samuel Bowman , and Noah A. Smith . 
2018 . 
Annotation artifacts in natural language inference data . 
In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 2 ( Short Papers ) , pages 107‚Äì112 , New Orleans , Louisiana . 
Association for Computational Linguistics.499 Christina Heinze - Deml and Diane Bouchacourt . 
2020 . 
Think before you act : A simple baseline for compositional generalization . 
Felix Hill , Andrew Lampinen , Rosalia Schneider , Stephen Clark , Matthew Botvinick , James L. McClelland , and Adam Santoro . 
2020 . 
Environmental drivers of systematicity and generalization in a situated agent . 
Ronghang Hu , Jacob Andreas , Trevor Darrell , and Kate Saenko . 
2018 . 
Explainable neural computation via stack neural module networks . 
In Proceedings of the European conference on computer vision ( ECCV ) , pages 53‚Äì69 . 
Ronghang Hu , Anna Rohrbach , Trevor Darrell , and Kate Saenko . 
2019 . 
Language - conditioned graph networks for relational reasoning . 
2019 IEEE / CVF International Conference on Computer Vision ( ICCV ) . 
Drew Arad Hudson and Christopher D. Manning . 
2018 . 
Compositional attention networks for machine reasoning . 
In International Conference on Learning Representations . 
Dieuwke Hupkes , Verna Dankers , Mathijs Mul , and Elia Bruni . 
2020 . 
Compositionality decomposed : How do neural networks generalise ? Journal of ArtiÔ¨Åcial Intelligence Research , 67:757‚Äì795 . 
Robin Jia and Percy Liang . 
2017 . 
Adversarial examples for evaluating reading comprehension systems . 
InProceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 2021‚Äì2031 , Copenhagen , Denmark . 
Association for Computational Linguistics . 
Hans Kamp and Barbara Partee . 
1995 . 
Prototype theory and compositionality . 
Cognition , 57(2):129 ‚Äì 191 . 
Yen - Ling Kuo , Boris Katz , and Andrei Barbu . 
2020 . 
Compositional networks enable systematic generalization for grounded language understanding . 
Brenden M. Lake and Marco Baroni . 
2017 . 
Generalization without systematicity : On the compositional skills of sequence - to - sequence recurrent networks . 
Adam Paszke , Sam Gross , Soumith Chintala , Gregory Chanan , Edward Yang , Zachary DeVito , Zeming Lin , Alban Desmaison , Luca Antiga , and Adam Lerer . 
2017 . 
Automatic differentiation in pytorch . 
Ethan Perez , Florian Strub , Harm De Vries , Vincent Dumoulin , and Aaron Courville . 
2018 . 
Film : Visual reasoning with a general conditioning layer . 
In Thirty - Second AAAI Conference on ArtiÔ¨Åcial Intelligence . 
Laura Ruis , Jacob Andreas , Marco Baroni , Diane Bouchacourt , and Brenden M. Lake . 
2020 . 
A benchmark for systematic generalization in grounded language understanding . 
Minjie Wang , Da Zheng , Zihao Ye , Quan Gan , Mufei Li , Xiang Song , Jinjing Zhou , Chao Ma , Lingfan Yu , Yu Gai , Tianjun Xiao , Tong He , George Karypis , Jinyang Li , and Zheng Zhang . 
2020 . 
Deep graph library : A graph - centric , highly - performant package for graph neural networks . 
Ziyun Wang and Brenden M. Lake . 
2019 . 
Modeling question asking using neural program generation.500 A Appendix A.1 Implementation Details Our implementation is based on the gSCAN dataset used by the Ruis et al . 
( 2020 ) and the world size is d= 6 . 
For equation 1 , each token is embedded to a randomly initialized vector of size 32 , and the hidden size of the encoder BiLSTM is 32 . 
For equation 9 , we use three convolutional networks with kernel size k= 1,5,7and padding size ‚åäk 2‚åã= 0,2,3to ensure that the resulting dimensionality is synchronized with input . 
They share the same Ô¨Ålter size of 64 . 
The concatenation of Hs i is also Ô¨Çattened to the shape of 36√ó192 . 
Table 4 presents the shapes of other trainable parameters mentioned in section 3 . 
We simply set dcmd = dh = dloc = dm = ds = dctx= 64 . 
Parameter Shape W1 1√ódcmd W2,W 3dcmd√ódcmd W4dh√ódloc W5dh√ódctx W6,W 7dh√ó(dloc+dctx+dh ) W8dh√óds W9dm√ó(dloc+dctx+dh ) W10dm√óds W11dctx√ódctx W12dh√ó(dloc+dctx ) Table 4 : Parameter Shapes A.2 Example Visualization Here we present more examples demonstrating our model ‚Äôs strengths and weaknesses . 
Figures 5 - 8 are cases where our model ‚Äôs prediction exactly matches the target while the baseline ‚Äôs does not . 
Some of the common failures of our model are illustrated in Figures 9 - 11 . 
Figure 5 : Baseline incorrectly picked a yellow square as the target . 
Figure 6 : Baseline incorrectly picked a red square as the target.501 Figure 7 : Baseline falsely predicted the consequential interaction and decided not to push . 
Figure 8 : Baseline incorrectly picked the bigger circle instead of the smaller one . 
Figure 9 : Getting lost along a long sequence : Our model fails when the target sequence repeats the same actions several times . 
Figure 10 : Incorrect path plan : Our model generates the path plan in a partially - reversed order.502 Figure 11 : Early stop before reaching boundary : Our model stops pushing when the target object is next to the boundary grid.503 Proceedings of the 1st Conference of the Asia - PaciÔ¨Åc Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing , pages 504‚Äì515 December 4 - 7 , 2020 . 
¬© 2020 Association for Computational Linguistics Are scene graphs good enough to improve Image Captioning ? Victor Milewski1Marie - Francine Moens1Iacer Calixto2,3 1KU Leuven2New York University3ILLC , University of Amsterdam { victor.milewski , sien.moens } @cs.kuleuven.be iacer.calixto@nyu.edu Abstract Many top - performing image captioning models rely solely on object features computed with an object detection model to generate image descriptions . 
However , recent studies propose to directly use scene graphs to introduce information about object relations into captioning , hoping to better describe interactions between objects . 
In this work , we thoroughly investigate the use of scene graphs in image captioning . 
We empirically study whether using additional scene graph encoders can lead to better image descriptions and propose a conditional graph attention network ( CGAT ) , where the image captioning decoder state is used to condition the graph updates . 
Finally , we determine to what extent noise in the predicted scene graphs inÔ¨Çuence caption quality . 
Overall , we Ô¨Ånd no signiÔ¨Åcant difference between models that use scene graph features and models that only use object detection features across different captioning metrics , which suggests that existing scene graph generation models are still too noisy to be useful in image captioning . 
Moreover , although the quality of predicted scene graphs is very low in general , when using high quality scene graphs we obtain gains of up to 3.3 CIDEr compared to a strong Bottom - Up Top - Down baseline.1 1 Introduction Scene understanding is a complex and intricate activity which humans perform effortlessly but that computational models still struggle with . 
An important backbone of scene understanding is being able to detect objects and relations between objects in an image , and scene graphs ( Johnson et al . 
, 2015 ; Anderson et al . 
, 2016 ) are a closely related data 1We open source the codebase to reproduce all our experiments in https://github.com/iacercalixto/ butd - image - captioning .structure that explicitly annotates an image with its objects and relations in context . 
Scene graphs can be used to improve important visual tasks that require scene understanding , e.g. image indexing and search ( Johnson et al . 
, 2015 ) or scene construction and generation ( Johnson et al . 
, 2017 , 2018 ) , and there is evidence that they can also be used to improve image captioning ( Yang et al . 
, 2019 ; Li and Jiang , 2019 ) . 
However , the de facto standard in top - performing image captioning models to date use strong object features only , e.g. obtained with a pretrained Faster R - CNN ( Ren et al . 
, 2015 ) , and no explicit relation information ( Anderson et al . 
, 2018 ; Lu et al . 
, 2018 ; Yu et al . 
, 2019a ) . 
One possible explanation to this observation is that by using detected objects we already capture the more important information that characterises a scene , and that relation information is already implicitly learned in such models . 
Another explanation is that relations are simply not as important as we hypothesise and that we gain no valuable extra information by adding them . 
In this work , we investigate these empirical observations in more detail and strive to answer the following research questions : ( i ) Can we improve image captioning by explicitly supervising a model with information about object relations ? ( ii ) How does the content of the captions improve when utilising scene graphs ? ( iii ) How does scene graph quality impact the quality of the captions ? The most recent best - performing image captioning models make use of the Transformer architecture ( Vaswani et al . 
, 2017 ; Li et al . 
, 2019 ; Yu et al . 
, 2019b ) . 
However , in this paper we build upon the inÔ¨Çuential Bottom - Up Top - Down architecture ( Anderson et al . 
, 2018 ) which uses LSTMs , and since we want to measure to what extent scene graphs are helpful or not , we remove any ‚Äú extras ‚Äù to make model comparison easier , e.g. reinforcement learning step after cross - entropy training , ensembling at504 inference time , etc . 
Scene graph generation ( SGG ) is the task where given an image a model predicts a graph with its objects and their relations . 
We use a pretrained SGG model ( Xu et al . 
, 2017 ) to obtain and inject explicit relation information into image captioning , and investigate different image captioning model architectures that incorporate object and relation features , similarly to Li and Jiang ( 2019 ) ; Wang et al . 
( 2019 ) . 
We propose an extension to graph attention networks ( Veli Àáckovi ¬¥ c et al . 
, 2018 ) which we call conditional graph attention ( C - GAT ) , where we condition the updates of the scene graph features on the current image captioning decoder state . 
Finally , we conduct an in - depth analysis of the captions produced by different models and determine if scene graphs actually improve the content of the captions . 
Our approach is illustrated in Figure 1 . 
Our main contributions are : ‚Ä¢We investigate different graph - based architectures to fuse object and relation information derived from scene graph generation models in the context of image captioning . 
‚Ä¢We introduce conditional graph attention networks to condition scene graph updates on the current state of an image captioning decoder and Ô¨Ånd that it leads to improvements of up to 0.8 CIDEr . 
‚Ä¢We compare the quality of the generated scene graphs and the quality of the corresponding captions and Ô¨Ånd that by using high quality scene graphs we can improve captions quality by up to 3.3 CIDEr . 
‚Ä¢We systematically analyse captions generated by standard image captioning models and by models with access to scene graphs using SPICE scores for objects and relations ( Anderson et al . 
, 2016 ) and Ô¨Ånd that when using scene graphs there is an increase of 0.4 F1 for relations and decrease of 0.1 F1 for objects . 
2 Background 2.1 Object Detection Object detection is a task where given an input image the goal is to locate and label all its objects . 
TheFaster R - CNN , which builds on the R - CNN and Fast R - CNN ( Girshick et al . 
, 2014 ; Girshick , 2015 ) , is a widely adopted model proposed for object detection ( Ren et al . 
, 2015 ) . 
It uses a pretrained convolutional neural network ( CNN ) as a backbone to extract feature maps for an input image . 
A regionproposal network ( RPN ) uses these feature maps to propose a set of regions with a high likelihood of containing an object . 
For each region , a feature vector is generated using the feature map , which is then passed to an object classiÔ¨Åcation layer . 
In our experiments , we use the Faster R - CNN with a ResNet-101 backbone ( He et al . 
, 2016 ) . 
2.2 Graph Neural Networks Graph neural networks ( GNNs ; Battaglia et al . 
, 2018 ) are neural architectures designed to operate on arbitrarily structured graphs G= ( V , E ) , whereVandEare the set of vertices and edges inG , respectively . 
In GNNs , representations for a vertex v‚ààV are computed by using information from neighbouring vertices N(v)which are deÔ¨Åned to include all vertices connected through an edge . 
In this work , we use a neighbourhood N(v1)that contains vertices v2connected through incoming edges , i.e. v2‚Üív1‚ààE. Graph Attention Networks Graph attention networks ( GATs ; Veli Àáckovi ¬¥ c et al . 
, 2018 ) combine features from neighbour vertices N(v1)through an attention mechanism ( Bahdanau et al . 
, 2014 ) to generate representations for vertex v1 . 
Vertex v1 ‚Äôs statevt‚àí1 1at time step t‚àí1is used as the query to soft - select the information from neighbours relevant to its updated state vt 1 . 
2.3 Scene Graphs Scene graphs consist of a data structure devised to annotate an image with its objects and the existing relations between objects and were Ô¨Årst introduced for image retrieval ( Johnson et al . 
, 2015 ) . 
We consider scene graphs Gfor an image with two types of vertices : objects and relations.2Object vertices describe the different objects in the image , and relation vertices describe how different objects interact with each other . 
This gives us the following rules for edges E:(i)All existing edges are between an object vertex and a relation vertex ; ( ii)If an object o1is connected to another object o2 via a relation vertex r3 , then vertex r3hasonly two connected edges : one incoming from o1and one outgoing to o2 . 
Finally , object ( relation ) vertices are also associated to an object ( relation ) label . 
Scene Graph Generation Scene graph generation ( SGG ) was introduced by Xu et al . 
( 2017 ) and 2Attributes are also originally present in scene graphs as vertices , but we do not use them.505 LSTM1 LSTM2 dog on cake chair   fruitnext   tobehind behind   C - GAT    a    dog   on     ‚Ä¶ Object   Detection Scene Graph   Generation   Hier : scene   graph first Hier : objects   firstFlatAttention Figure 1 : We use object features from an object detection model and scene graph features from a scene graph generation model in image captioning . 
We use conditional graph attention ( C - GAT ) to encode scene graph features , and Ô¨Çat vs. hierarchical attention mechanisms are used to incorporate both feature sets into an LSTM decoder . 
has since received growing attention ( Zellers et al . 
, 2018 ; Li et al . 
, 2018 ; Yang et al . 
, 2018 ; Knyazev et al . 
, 2020 ) . 
One can compare it to object detection ( Section 2.1 ) , where instead of only predicting objects a model must additionally predict which objects have relations and what are these relations . 
This similarity makes it natural that SGG models build on object detection architectures . 
Most SGG models use a pretrained Faster R - CNN or similar architecture to predict objects and have an additional component to predict relations for pairs of objects . 
In addition to the original object loss components in the Faster R - CNN , they include a mechanism to update object feature representations using neighbourhood information , and a component to predict relations and their label . 
Iterative Message Passing The Iterative Message Passing SGG model ( Xu et al . 
, 2017 ) keeps two sets of states , i.e. for object vertices and relation vertices . 
The object vertices are initialised directly from Faster R - CNN features , while a relation vertex is computed by the box union of each of its two objects boxes , which is encoded with the Faster R - CNN to obtain a relation vector . 
Hidden states in each set are updated using an attention mechanism over neighbour vertices , i.e. objects are informed by all connected relation vertices , and relations are informed by the two objects it links . 
Since there are two sets of states it is easy to efÔ¨Åciently send messages from one set to the other by the means of an adjacency matrix . 
This procedure is repeated for kiterations , and Xu et al . 
( 2017 ) found that k= 2gives optimal results . 
Relation proposal network ( RelPN ) Xu et al . 
( 2017 ) Ô¨Årst proposed to build a fully connected graph connecting all object pairs and scoring relations between all possible object pairs ; however , this model is expensive and grows exponentially with the number of objects . 
Yang et al . 
( 2018 ) introduced a relation proposal network ( RelPN ) , which works similarly to an object detection RPN but that selectively proposes relations between pairs of objects . 
In all our experiments , we use the Iterative Message Passing model trained using a RelPN . 
3 Conditional Graph Attention ( C - GAT ) Standard graph neural architectures encode information about neighbour nodes N(v)into representations of node v‚ààV. Therefore , these GNNs arecontextual because they encode graph - internal context . 
We propose the conditional graph attention ( C - GAT ) architecture , a novel extension for graph attention networks ( Veli Àáckovi ¬¥ c et al . 
, 2018).3Our goal is to make these networks conditional in addition to contextual . 
By conditional we mean that a C - GAT layer is conditioned on external context , e.g. a vector representing knowledge that is not part of the original input graph . 
Our motivation is that when using graph - based inputs such as a scene graph , a C - GAT layer allows us to condition the message propagation between connected nodes in the graph on the current state of the model , e.g. on the decoder state in the 3This architecture is novel to the best of our knowledge.506   vMAN   eABO VE eRIGHT ‚Äâ  OF   vTREE   vS UN   q LEA VE S   TREE   MAN S UN   B A G RIGHT   OF HOLDS HAS SKY   FIELD ON ON IN   ABO VE   vMAN    ( l Ôºã1)Figure 2 : C - GAT layer where we illustrate the update of vertex MAN by combining the features of all incoming relations ( and objects ) through an attention mechanism . 
The attention scores are conditioned on the external query vector q. captioning decoder in Figure 1 . 
Whereas a standard GAT layer contextually updates object hidden states , it can not condition on context outside the scene graph.4With a C - GAT layer , we provide a mechanism for the model to learn to update object hidden states in the context of the current state of the decoder language model , which we expect to lead to better contextual features . 
In Figure 2 , a C - GAT layer is applied to an input scene graphGand conditioned on a query vector q , i.e. the decoder state . 
We illustrate the update of vertex vman‚ààV using features from its neighbourhoodN(vman ) . 
The self - relation is assumed to always be present and for readability is not shown . 
Neighbour nodes ‚Äô features are combined with an MLP attention mechanism ( Bahdanau et al . 
, 2014 ) and scores are computed using query q. As described in Section 2.3 , neighbours of an object vertex vi‚àà V in a scene graph only include relation vertices . 
To include neighbour object features as well as relation features , we collect features for all vj‚àà N obj(vi ) , deÔ¨Åned as nodes accessible by all relation vertices vrsuch that{vi‚Üêvr , vr‚Üêvj}‚ààE . 
4 Model Setup In this section , we Ô¨Årst introduce image captioning models that do not explicitly use relation features ( Section 4.1 ) and contrast them with those that use explicit relation features ( Section 4.2 ) . 
4.1 Baseline Image Captioning ( IC ) Bottom - Up Top - Down ( BUTD ) The bottom - up top - down ( BUTD ; Anderson et al . 
, 2018 ) model 4This is generally true for standard GNN architectures and not just GATs.consists of a Faster R - CNN image encoder ( Ren et al . 
, 2015 ) that computes object proposal features for an input image , and a 2 - layer LSTM language model decoder with a MLP attention mechanism over the object features that generates a caption for the image ( Hochreiter and Schmidhuber , 1997 ; Bahdanau et al . 
, 2014 ) . 
We denote the set of object features X‚ààRn√ód , where nis the number of objects in the image and dthe features dimensionality . 
The 2 - layer LSTM is designed so that the Ô¨Årst layer is used to compute an attention over the image features and the second layer is used to generate the captions ‚Äô tokens . 
LSTM states at time steptare denoted as h(t ) 1andh(t ) 2for layer 1 and 2 , respectively . 
The hidden state of LSTM 1is used to derive an attention over image features : x(t)=Att(X , h(t ) 1 ) , ( 1 ) where x(t)‚ààRdis the output of the attention layer and denotes the image features used at time step t. Update rules for each LSTM layer are deÔ¨Åned by : h(t ) 1 = LSTM 1([h(t‚àí1 ) 2;w(t‚àí1);¬ØX],h(t‚àí1 ) 1 ) , ( 2 ) h(t ) 2 = LSTM 2([h(t ) 1;x(t)],h(t‚àí1 ) 2 ) , ( 3 ) where w(t‚àí1)is the embedding of the previously generated word , and ¬ØX‚ààRdare the mean image features . 
Next word probabilities are computed using a softmax over the vocabulary and parameterised by a linear projection of the hidden state of LSTM 2 : p(w(t)=k|w(1):(t‚àí1))‚àùexp(W h(t ) 2 ) . 
4.2 Relation - aware Image Captioning ( RIC ) We now describe models that incorporate explicit relation information into image captioning by using scene graphs as additional inputs . 
We use the pretrained Iterative Message Passing model with a relation proposal network ( Xu et al . 
, 2017 ; Yang et al . 
, 2018 ) to obtain scene graph features for all images . 
Scene graph features for an image are denoted Y‚ààR(o+r)√ók , where ois the number of objects , rthe number of relations between objects , and kis the object / relation feature dimensionality . 
We follow Wang et al . 
( 2019 ) who have found that only using scene graph features led to poor results compared to using Faster R - CNN features only . 
Therefore , we propose to integrate scene507 graph features Yand Faster R - CNN object features Xby experimenting with ( i)using Ydirectly , applying a GAT layer on Y , or applying a C - GAT layer on Yprior to feeding scene graph features into the decoder , and ( ii)using a Ô¨Çat attention mechanism versus a hierarchical attention mechanism . 
GAT over Scene Graphs We propose a model that encodes the scene graph features Ywith a standard GAT layer prior to using them in LSTM 2 in the decoder . 
C - GAT over Scene Graphs In this setup , we apply a C - GAT layer on scene graph features Y using the current decoder state h(t ) 1from LSTM 1 as the external context , and use the output of the C - GAT layer in LSTM 2 in the decoder . 
Flat Attention The Ô¨Çat attention ( FA ) consists of two separate attention heads , one over scene graph features Yand the other over Faster R - CNN features X. We use two standard MLP attention mechanisms ( Bahdanau et al . 
, 2014 ) , each using the hidden state from LSTM 1as the query : x(t)=Attx(X , h(t ) 1 ) , y(t)=Atty(Y , h(t ) 1 ) . 
Each LSTM layer is now deÔ¨Åned as follows : h(t ) 1 = LSTM 1([h(t‚àí1 ) 2;w(t‚àí1);¬ØX;¬ØY],h(t‚àí1 ) 1 ) , h(t ) 2 = LSTM 2([h(t ) 1;x(t);y(t)],h(t‚àí1 ) 2),(4 ) where x(t)andy(t)are computed by the two attention heads AttxandAtty , respectively , and ¬ØY denote the mean scene graph features . 
Hierarchical Attention In a hierarchical attention ( HA ) mechanism the output of the Ô¨Årst attention head is used as input to derive the attention of the second head . 
We again have two sets of inputs , scene graph features Yand Faster R - CNN object features X. We experiment Ô¨Årst using Yas input to the Ô¨Årst head , and its output y(t)as additional input to the second head : y(t)=Atty(Y , h(t ) 1 ) , x(t)=Attx(X,[h(t ) 1;y(t ) ] ) . 
( 5 ) This setup is similar to the cascade attention from Wang et al . 
( 2019 ) . 
We also try using Xas input to the Ô¨Årst head , and the Ô¨Årst head ‚Äôs output x(t)as additional input to the second head : x(t)=Attx(X , h(t ) 1 ) . 
y(t)=Atty(Y,[h(t ) 1;x(t ) ] ) . 
( 6 ) In both cases , the hidden states for LSTM 1and LSTM 2are computed as in Equation 4 . 
5 Experimental Setup and Results We compare our models with the following external baselines with no access to scene graphs : ( 1 ) the adaptive attention model Add - Att which determines at each decoder time step how much of the visual features should be used ( Lu et al . 
, 2017 ) ; ( 2 ) the Neural Baby Talk model NBT generates a sentence with gaps and Ô¨Ålls the gaps using detected object labels ( Lu et al . 
, 2018 ) ; ( 3 ) and the BUTD model ( Anderson et al . 
, 2018 ) described in Section 4.1 . 
We also compare with the following baselines that use scene graphs : ( 1 ) The ‚Äú Know more , say less ‚Äù model KMSL extracts features for objects and relations based on the scene graph , which are passed through two attention heads and Ô¨Ånally combined using a Ô¨Çat attention head ( Li and Jiang , 2019 ) ; and ( 2 ) the Cascade model ( Wang et al . 
, 2019 ) which is similar to our hierarchical attention model with a GAT layer , but that instead uses a relational graph convolutional network ( Marcheggiani et al . 
, 2017 ) . 
We do not discuss model variants / results that are trained with an additional reinforcement learning step ( Rennie et al . 
, 2017 ; Yang et al . 
, 2019 ) and only compare single model results , since training and performing inference with such models is very costly and orthogonal to our research questions . 
Our proposed models are : Ô¨Çat attention ( FA ) , hierarchical attention with scene graph Ô¨Årst ( HA - SG ) following Equation 5 , hierarchical attention with objects detected Ô¨Årst ( HA - IM ) following Equation 6 , HA - SG with graph attention network ( HASG+GAT ) , and HA - SG with conditional graph attention ( HA - SG+C - GAT ) . 
We choose the last two variants to extend HA - SG following the setup used by ( Wang et al . 
, 2019 ) . 
We evaluate captions generated by different models by investigating their SPICE scores ( Anderson et al . 
, 2016 ) , i.e. an F1 based semantic captioning evaluation metric computed over scene graphs . 
It uses the semantic structure of the scene graph to508 B4 C R S Add - Att‚àó‚Ä†33.2 108.5 ‚Äî ‚Äî NBT‚Ä†34.7 107.2 ‚Äî 20.1 BUTD‚Ä†36.2 113.5 56.4 20.3 BUTD 34.8 109.2 55.7 20.0 Cascade‚Ä†34.1 108.6 55.9 20.3 KMSL‚Ä†33.8 110.3 54.9 19.8 FA 33.7 102.5 54.7 18.8 HA - IM 35.7 109.9 55.9 19.9 HA - SG 35.0 109.1 55.7 19.8 + GAT 34.7 106.4 55.4 19.4 + C - GAT 35.5 109.9 56.0 19.8 Table 1 : Results on the MSCOCO test set , with models selected on the validation set ( karpathy splits ) . 
Models in the upper section do not use scene graphs , while those in the bottom section do . 
All models are trained to convergence for a maximum of 50 epochs . 
Metrics reported are : BLEU-4 ( B4 ) , CIDEr ( C ) , ROUGE - L ( R ) , and SPICE ( S ) . 
See Section 5 for details on all models and acronyms . 
We bold - face the best and underscore the second - best scores per metric ( models that use scene graph).‚àóModel uses features from last convolutional layer in CNN , i.e. no Faster R - CNN features . 
‚Ä†Results reported in the authors ‚Äô original papers . 
compute scores over several dimensions ( object , relation , attribute , colour , count , and size ) . 
We use the MSCOCO karpathy split ( Lin et al . 
, 2014 ; Karpathy and Fei - Fei , 2015 ) which has 5k images each in validation and test sets , and we use the remaining 113k images for training . 
We build a vocabulary based on all words in the train split that occur at least 5 times . 
We use MSCOCO evaluation scripts ( Lin et al . 
, 2014 ) and report BLEU4 ( B4 ; Papineni et al . 
, 2002 ) , CIDEr ( C ; Vedantam et al . 
, 2015 ) , ROUGE - L ( R ; Lin , 2004 ) , and SPICE ( S ; Anderson et al . 
, 2016 ) . 
See Appendix A for extra information on our implementation and training procedures . 
5.1 Image Captioning without Relational Features Our re - implementation of the BUTD baseline scores slightly worse compared to the results reported by Anderson et al . 
( 2018 ) . 
This difference can be attributed to the Faster R - CNN features used , i.e. we always use 36 objects per image whereas Anderson et al . 
( 2018 ) use a variable number of objects per image ( i.e. 10 to 100 ) , and there are other smaller differences in their training procedure . 
Since all our models use these settings , in further experiments we compare to our implementation of the BUTD baseline . 
5.2 Image Captioning with relational features We notice that the KMSL model by Li and Jiang ( 2019 ) slightly outperforms the other models according to CIDEr , while it performs worse in all other metrics . 
Li and Jiang ( 2019 ) found performance increases when restricting the number of relations and report scores using this restriction , whereas we decided to use the full set of relations to test the effect of scene graph quality ( see Section 5.4 ) . 
Furthermore , the features used in the KMSL model are not directly extracted from the SGG model as is the case for the other models , but an additional architecture is used for computing stronger features . 
Flat vs. Hierarchical attention According to Table 1 , FA performs worse not only compared to HA models , but also compared to other baselines . 
The HA model using Faster R - CNN object features in the Ô¨Årst head , i.e. HA - IM setup , performs better than using the scene graph features Ô¨Årst , i.e. HA - SG setup . 
We hypothesise that this difference comes from the additional guidance from x(t)helping with a better attention selection over possibly more noisy features present in Y. Additional GNN updates Directly using a GAT layer over scene graph features negatively impacts model performance . 
Comparing these results to the related Cascade model from Wang et al . 
( 2019 ) , we hypothesise that the R - GCN architecture works better in this setting , although compared to other models it still has lower scores according to most metrics . 
The reason may be that the Cascade model by Wang et al . 
( 2019 ) was undertrained or could have used better hyperparameters , as indicated by our BUTD baseline performing comparably or better than their strongest model.5 Combining a C - GAT layer on the decoder improves overall results according to most metrics , though by a small margin . 
This suggests that using additional GNNs in the context of image captioning have a positive effect . 
Furthermore , graph features learned using C - GAT always outperform standard GAT , which coincides with our intuition that taking 5Our BUTD baseline scores 109.2 CIDEr , whereas their best model achieves 108.6 CIDEr.509 All Obj Rel BUTD 19.8 36.0 5.2 FA 18.5 34.7 5.0 HA - IM 19.5 35.9 5.2 HA - SG 19.5 35.9 5.3 + GAT 19.2 35.5 5.6 + C - GAT 19.4 35.8 5.3 Table 2 : Breakdown of overall SPICE scores ( All ) into object ( Obj ) and relation ( Rel ) F1 scores . 
See Section 5 for details on all models and acronyms . 
We bold - face the best overall scores and underline the best scores obtained by our models . 
the current decoder hidden context into consideration can improve graph features . 
5.3 SPICE breakdown In our analysis , in addition to the overall SPICE F1 score for an entire caption , we break it down into scores over objects and over relations.6This allows us to investigate how models are better or worse on describing objects and relations independently . 
These results , computed for the validation split , are shown in Table 2 . 
When we look at individual scores for objects and relations , we notice a small and consistent gain in relation F1 by using scene graphs independently of the attention architecture or other design choices , but also observe lower object F1 scores with respect to the BUTD baseline . 
When object and relation scores are combined into a single F1 measure , it results in worse overall scores suggesting that the small increase in the relation scores is not sufÔ¨Åcient to have a positive impact on captioning insofar . 
5.4 Scene Graph Quality Since scene graph features are generated with a pretrained SGG model , we expect them to introduce a considerable amount of noise into the model . 
In this section , we investigate the effect that the quality of the scene graph has on the quality of captions . 
VG - COCO In this set of experiments , we need images with both captions and scene graph annotations . 
Thus , we use a subset of MSCOCO which overlaps with Visual Genome ( Krishna et al . 
, 2017 ) , using captions from the former and scene graphs 6The SPICE score also includes the components attribute , colour , count , and size , but we do not report them directly . 
Figure 3 : Distribution of the scores for the scene graphs in the validation split of the VG - COCO dataset . 
from the latter . 
We refer to this dataset as VGCOCO , as similarly done by Li and Jiang ( 2019 ) . 
We compute scores for each scene graph predicted by the Iterative Message Passing model using the common SGDet recall@100 as deÔ¨Åned by Yang et al . 
( 2018 ) . 
SGDet recall@100 is computed by using the 100 highest scoring triplets among all triplets predicted by the model,7and reporting the percentage of gold - standard triplets . 
The distribution of scores across images ( Figure 3 ) shows that most scene graphs have extremely low scores close to zero , thus containing a lot of noise . 
We separate images in the VG - COCO validation set in three groups : low ( R <33 % ) , average ( 33%‚â§R<67 % ) , and high scoring graphs ( 67%‚â§R ) , where R is SGDet recall@100 . 
For each set of images in each of these groups , we compute captioning metrics and also report a SPICE breakdown in Table 3 . 
Effect of scene graph quality Due to the imbalance in scene graph quality , the low , average , and high quality subsets have around 1000 , 500 , and 200 images , respectively . 
By reporting results for the BUTD baseline , we show the performance a strong baseline obtains on the same set of images . 
In Table 3b , scores across all metrics are similar and only model FA performs clearly worse than others . 
Though the BUTD baseline never performs best , it is often not more than a point behind the best performing model ( except for CIDEr where it is 2.5 points lower compared to HA - IM ) . 
When comparing Table 3b to Table 3c , we observe that all models tend to increase scores , and that BUTD tends to perform best overall . 
In Table 3d , we see an increase in the difference between 7A triplet is an object - predicate - subject phrase.510 SPICE Captioning All Obj Rel B4 C R BUTD 19.8 36.0 5.0 35.4 109.8 56.0 FA 18.5 34.9 4.8 33.2 103.6 54.8 HA - IM 19.6 36.0 5.2 35.0 110.8 55.7 HA - SG 19.8 36.2 5.5 35.7 111.0 56.0 + GAT 19.4 35.5 5.6 35.0 108.3 55.6 + C - GAT 19.5 35.8 5.2 35.7 110.4 56.0 ( a ) Full VG - COCO datasetSPICE Captioning All Obj Rel B4 C R 19.5 35.6 4.7 34.0 109.2 55.2 18.2 34.9 4.4 32.4 102.7 54.3 19.5 36.0 5.2 34.3 111.7 55.5 19.6 36.0 5.0 34.6 110.3 55.4 19.5 35.7 5.7 34.8 109.5 55.4 19.5 35.7 5.1 34.8 109.9 55.7 ( b ) Low VG - COCO dataset SPICE Captioning All Obj Rel B4 C R BUTD 20.5 37.0 5.5 38.4 117.5 57.1 FA 18.8 35.0 5.3 34.6 111.1 55.1 HA - IM 19.8 36.2 5.3 36.2 115.1 55.3 HA - SG 19.6 35.9 5.4 37.0 114.4 56.3 + GAT 19.4 35.8 5.7 36.2 112.7 56.0 + C - GAT 19.5 36.0 5.1 37.6 116.4 56.1 ( c ) Average VG - COCO datasetSPICE Captioning All Obj Rel B4 C R 20.9 36.8 5.1 37.2 126.5 57.0 19.8 35.3 5.6 35.9 117.4 56.6 20.3 36.2 5.8 36.2 124.1 56.8 20.9 37.1 6.0 38.1 129.8 57.6 19.7 35.3 5.9 36.2 123.6 57.1 20.8 36.6 6.0 37.2 127.3 56.9 ( d ) High VG - COCO dataset Table 3 : SPICE breakdown and captioning metrics for images in VG - COCO validation split . 
Results for the full VG - COCO , and for subsets of images collected according to the quality of their corresponding predicted scene graphs : low , average , and high . 
See Sections 5 for details on all models and acronyms . 
Metrics reported are : overall SPICE F1 score ( All ) , object ( Obj ) and relation ( Rel ) F1 score components , BLEU-4 ( B4 ) , CIDEr ( C ) , and ROUGE - L ( R ) . 
We bold - face the best and underline the second - best overall scores per metric and per data subset . 
the baseline and our best models according to all metrics . 
All these gains are very promising and suggest that when we have high quality scene graphs , we can expect a consistent positive transfer into image captioning models . 
However , the overall SPICE score is the highest for both BUTD and HA - SG , while BUTD has lower scores for objects and relations F - measure . 
That suggests that other components part of SPICE were worsened with the addition of scene graphs . 
Since this is not the focus of this paper , we did not investigate this further and leave that for future work . 
Overall , these results show that indiscriminately using scene graphs from pretrained SGG models downstream on image captioning can be harmful because of the amount of noise present in these scene graphs . 
However , when this noise is smaller and the scene graphs of higher quality , our Ô¨Åndings together suggest that scene graphs can be useful in image captioning models . 
Ground - truth graphs Finally , we also conduct a small - scale experiment using ground - truth scenegraphs and evaluate how using these instead of predicted scene graphs at inference time impacts models , which can be found in Appendix B. Qualitative Results Here , we try to determine if there is a clear difference in the difÔ¨Åculty in captioning images in low , average , and high quality sets , which might help explain the result in Table 3 . 
In Figures 4 and 5 we show some images for the low and high scoring graphs , respectively . 
At a Ô¨Årst glance , images from both sets appear equally cluttered with objects ( i.e. , which we hypothesise should correlate with the image being harder to describe ) . 
Furthermore , for both low and high scoring scene graphs , the average number of objects and relations is 23 and 22 respectively . 
However , we note that even scene graphs in the high quality set often include tiny objects and details , e.g. the image in the right of Figure 4 shows a single aircraft , but there are 17 annotated objects describing components such as wings , windows , etc.511 Figure 4 : Images , captions and ground - truth number of objects and relations for high scoring scene graph . 
Figure 5 : Images , captions and ground - truth number of objects and relations for lowscoring scene graph . 
6 Conclusions and Future Work In this work , we investigate the impact scene graphs have on image captioning . 
We introduced conditional graph attention ( C - GAT ) networks and applied it to image captioning , and report promising results ( Table 1 ) . 
Overall , we found that improvements in captioning when using scene graphs generated with publicly available SGG models are minor . 
We observe a very small increase in the ability to describe relations as measured by relation SPICE F - scores , however , this is associated with models producing worse overall descriptions and producing lower object SPICE F - scores . 
In an in - depth analysis , we found that the predicted scene graphs contain a large amount of noise which harms the captioning process . 
When this noise is reduced , large gains can be achieved across all image captioning metrics , e.g. 3.3 CIDEr points in the high VG - COCO split ( Table 3d ) . 
This indicates that with further research and improved scene graph generation models , we will likely be able to observe consistent gains in image captioning and possibly other tasks by leveraging silver - standard scene graphs . 
Future work In further research , we will conduct an in - depth analysis of our proposed condi - tional graph attention to determine what tasks other than image captioning we can apply it to . 
We envision using it for visual question - answering also with generated scene graphs , and on syntax - aware neural machine translation ( Bastings et al . 
, 2017 ) , fake news detection ( Monti et al . 
, 2019 ) , and question answering ( Zhang et al . 
, 2018 ) . 
In a focused qualitative analysis , we found that the scene graphs represent objects and relations in images sometimes with great detail . 
We plan to investigate how to account for such highly detailed objects / relations in the context of image captioning . 
Finally , we will look into a method to use predicted scene graphs selectively according to their estimated quality , possibly selecting the best graph between those generated by different SGG models . 
Acknowledgments We would like to thank the COST Action CA18231 for funding a research visit to collaborate on this project . 
This work is funded by the European Research Council ( ERC ) under the ERC Advanced Grant 788506 . 
IC has received funding from the European Union ‚Äôs Horizon 2020 research and innovation program under the Marie Sk≈Çodowska - Curie grant agreement No 838188.512 Abstract Our analysis of large summarization datasets indicates that redundancy is a very serious problem when summarizing long documents . 
Yet , redundancy reduction has not been thoroughly investigated in neural summarization . 
In this work , we systematically explore and compare different ways to deal with redundancy when summarizing long documents . 
SpeciÔ¨Åcally , we organize the existing methods into categories based on when and how the redundancy is considered . 
Then , in the context of these categories , we propose three additional methods balancing non - redundancy and importance in a general and Ô¨Çexible way . 
In a series of experiments , we show that our proposed methods achieve the state - of - the - art with respect to ROUGE scores on two scientiÔ¨Åc paper datasets , Pubmed and arXiv , while reducing redundancy signiÔ¨Åcantly.1 1 Introduction Summarization is the task of shortening a given document(s ) while maintaining the most important information . 
In general , a good summarizer should generate a summary that is syntactically accurate , semantically correct , coherent , and non - redundant ( Saggion and Poibeau , 2013 ) . 
While extractive methods tend to have better performance on the Ô¨Årst two aspects , they are typically less coherent andmore redundant than abstractive ones , where new sentences are often generated by sentence fusion and compression , which helps detecting and removing redundancy ( Lebanoff et al . 
, 2019 ) . 
Although eliminating redundancy has been initially and more intensely studied in the Ô¨Åeld of multidocument summarization ( Lloret and Sanz , 2013 ) , because important sentences selected from multiple documents ( about the same topic ) are more 1Our code can be found here - http://www.cs . 
ubc.ca/cs-research/lci/research-groups/ natural - language - processing / likely to be redundant than sentences from the same document , generating a non - redundant summary should still be one of the goals for single document summarization ( Lin et al . 
, 2009 ) . 
Generally speaking , there is a trade - off between importance and diversity ( non - redundancy ) ( Jung et al . 
, 2019 ) , which is reÔ¨Çected in the two phases , sentence scoring and sentence selection ( Zhou et al . 
, 2018 ) in which extractive summarization task can be naturally decomposed . 
The former typically scores sentences based on importance , while the latter selects sentences based on their scores , but also possibly taking other factors ( including redundancy ) into account . 
Traditionally , in non - neural approaches the tradeoff between importance and redundancy has been carefully considered , with sentence selection picking sentences by optimizing an objective function that balances the two aspects ( Carbonell and Goldstein , 1998 ; Ren et al . 
, 2016 ) . 
In contrast , more recent works on neural extractive summarization models has so far over - emphasized sentence importance and the corresponding scoring phase , while paying little attention to how to reduce redundancy in the selection phase , where they simply apply a greedy algorithm to select sentences ( e.g. ,Cheng and Lapata ( 2016 ) ; Xiao and Carenini ( 2019 ) ) . 
Notice that this is especially problematic for long documents , where redundancy tends to be a more serious problem , as we have observed in key datasets . 
Improving redundancy reduction in neural extractive summarization for long documents is a major goal of this paper . 
Indeed , some recently proposed neural methods aim to reduce redundancy , but they either do that implicitly or inÔ¨Çexibly and only focusing on short documents ( e.g. , news ) . 
For instance , some models learn to reduce redundancy when predicting the scores ( Nallapati et al . 
, 2016a ) , or jointly learn to score and select sentences ( Zhou et al . 
, 2018 ) in516 an implicit way . 
However , whether these strategies actually help reducing redundancy is still an open empirical question . 
The only neural attempt of explicitly reduce redundancy in the sentence selection phase is the Trigram Blocking technique , used in recent extractive summarization models on news datasets ( e.g. , ( Liu and Lapata , 2019 ) ) . 
However , the effectiveness of such strategy on the summarization of long documents has not been tested . 
Finally , a very recent work by Bi et al . 
( 2020 ) attempts to reduce redundancy in more sophisticated ways , but still focusing on news . 
Furthermore , since it relies on BERT , such model is unsuitable to deal with long documents ( with over 3,000 words ) . 
To address this rather confusing situation , characterized by unclear connections between all the proposed neural models , by their limited focus on short documents , and by spotty evaluations , in this paper we systematically organize existing redundancy reduction methods into three categories , and compare them with respect to the informativeness and redundancy of the generated summary for long documents . 
In particular , to perform a fair comparison we re - implement all methods by modifying a common basic model ( Xiao and Carenini , 2019 ) , which is a top performer on long documents without considering redundancy . 
Additionally , we propose three new methods that we argue will reduce redundancy more explicitly and Ô¨Çexibly in the sentence scoring and sentence selection phase by deploying more suitable decoders , loss functions and/or sentence selection algorithms , again building for a fair comparison on the common basic model ( Xiao and Carenini , 2019 ) . 
To summarize , our main contributions in this paper are : we Ô¨Årst examine popular datasets , and show that redundancy is a more serious problem when summarizing long documents ( e.g. , scientiÔ¨Åc papers ) than short ones ( e.g. news ) . 
Secondly , we not only reorganize and re - implement existing neural methods for redundancy reduction , but we also propose three new general and Ô¨Çexible methods . 
Finally , in a series of experiments , we compare existing and proposed methods on long documents ( i.e. , the Pubmed and arXiv datasets ) , with respect to ROUGE scores ( Lin , 2004 ) and redundancy scores ( Peyrard et al . 
, 2017 ; Feigenblat et al . 
, 2017 ) . 
As a preview , empirical results reveal that the proposed methods achieve state - of - the - art performance on ROUGE scores , on the two scientiÔ¨Åc paper datasets , while also reducing the redundancysigniÔ¨Åcantly . 
2 Related Work In traditional extractive summarization , the process is treated as a discrete optimization problem balancing between importance scores and redundancy scores , with techniques like Maximal Marginal Relevance(MMR)(Carbonell and Goldstein , 1998 ) , redundancy - aware feature - based sentence classiÔ¨Åers ( Ren et al . 
, 2016 ) and graph - based submodular selection ( Lin et al . 
, 2009 ) . 
In recent years , researchers have explored neural extractive summarization solutions , which score sentences by training the neural models on a large corpus , and simply apply a greedy algorithm for sentence selection ( Cheng and Lapata , 2016 ; Nallapati et al . 
, 2016a ) . 
Although a model with a sequence decoder might plausibly encode redundancy information implicitly , Kedzie et al . 
( 2018 ) empirically show that this is not the case , since non auto - regressive models ( the ones scoring each sentence independently ) , perform on par with models with a sequence decoder . 
In one of our new methods , to effectively capture redundancy information , we specify a new loss that explicitly consider redundancy when training the neural model . 
Beyond a greedy algorithm , the Trigram Blocking is frequently used to explicitly reduce redundancy in the sentence selection phase ( Liu and Lapata , 2019 ) . 
In essence , a new sentence is not added to the summary if it shares a 3 - gram with the previously added one . 
Paulus et al . 
( 2017 ) Ô¨Årst adopt the strategy for abstractive summarization , which forces the model not to produce the same trigram twice in the generated summaries , as a simpliÔ¨Åed version of MMR ( Carbonell and Goldstein , 1998 ) . 
Arguably , this method is too crude for documents with relatively long sentences or speciÔ¨Åc concentrations ( e.g. scientiÔ¨Åc papers ) , where some technical terms , possibly longer than 2 - grams , are repeated frequently in the ‚Äô important sentences ‚Äô ( even in the reference summaries ) . 
To address this limitation , we propose a neural version of MMR to deal with redundancy within the sentence selection phase in a more Ô¨Çexible way , that can be tuned to balance importance and non - redundancy as needed . 
The idea of MMR has also inspired Zhou et al . 
( 2018 ) , who propose a model jointly learning to score and select the sentences . 
Yet , this work not only focuses on summarizing short documents ( i.e. , news ) , but also uses MMR implicitly , and arguably517 sub - optimally , by learning a score that only indirectly captures the trade - off between relevance and redundancy . 
To improve on this approach , in this paper we propose a third new method , in which importance and redundancy are explicitly weighted , while still making the sentence scoring and selection beneÔ¨Åt from each other by Ô¨Åne tuning the trained neural model through a Reinforcement Learning ( RL ) mechanism . 
Finally , Bi et al . 
( 2020 ) is the most recent ( still unpublished ) work on reducing redundancy in neural single document summarization . 
However , their goal is very different form ours , since they focus on relatively short documents in the news domain . 
3 Measuring Redundancy : metrics and comparing long vs. short documents We use the following two relatively new metrics to measure redundancy in the source documents and in the generated summaries . 
Unique n - gram ratio2 : proposed in Peyrard et al . 
( 2017 ) , it measures n - grams uniqueness ; the lower it is , the more redundant the document is . 
Uniqngramratio = count ( uniqngram ) count ( ngram ) Normalized Inverse of Diversity ( NID ): captures redundancy , as the inverse of a diversity metric with length normalization . 
Diversity is deÔ¨Åned as the entropy of unigrams in the document ( Feigenblat et al . 
, 2017 ) . 
Since longer documents are more likely to have a higher entropy , we normalize the diversity with the maximum possible entropy for the document log(|D| ) . 
Thus , we have : NID = 1‚àíentropy ( D ) log(|D| ) Note that higher NID indicates more redundancy . 
When we compare the redundancy of long vs. short documents with respect to these two metrics on four popular datasets for summarization ( CNNDM ( Nallapati et al . 
, 2016b ) , Xsum ( Narayan et al . 
, 2018 ) , Pubmed and arXiv ( Cohan et al . 
, 2018 ) ) , we observe that long documents are substantially more redundant than short ones ( as it was already pointed out in the past ( Stewart and Carbonell , 1998 ) ) . 
Table 1 shows the basic statistics of each dataset , along with the average NID 2In this paper , all the unique n - gram ratios are shown in percentage . 
Figure 1 : The average unique n - gram ratio in the documents across different datasets . 
To reduce the effect of length difference , stopwords were removed . 
scores , while Figure 1 shows the average Unique n - gram Ratio for the same datasets . 
These observations provide further evidence that redundancy is a more serious problem in long documents . 
In addition , notice that the sentences in the scientiÔ¨Åc paper datasets are much longer than in the news datasets , which plausibly makes it even harder to balance between importance and non - redundancy . 
Datasets # Doc . 
# words / doc . 
# words / sent . 
NID Xsum 203k 429 22.8 0.188 CNNDM 270k 823 19.9 0.205 Pubmed 115k 3142 35.1 0.255 arXiv 201k 6081 29.2 0.267 Table 1 : Longer documents are more redundant 4 Redundancy Reduction Methods We systematically organize neural redundancy reduction methods into three categories , and compare prototypical methods from each category . 
AThe decoder is designed to implicitly take redundancy into account . 
BIn the sentence scoring phase , explicitly learn to reduce the redundancy . 
CIn the sentence selection phase , select sentences with less redundancy . 
In this section , we describe different methods from each category . 
To compare them in a fair way , we build all of them on a basic ExtSum - LG model ( see¬ß4.1 ) , by modifying the decoder and the loss function in the sentence selection phase or the sentence selection algorithm . 
In Table 2 , we summarize the architecture ( Encoder , Decoder , Loss Function and sentence selection algorithm ) of all the methods we compare.518 Categ . 
MethodsSent . 
Scor . 
Sent . 
Sel . 
Encoder Decoder Loss Func . 
- Naive MMR Cosine Similarity MMR Select - ExtSum - LG Encoder - LG MLP Cross Entropy ( CE ) Greedy A + SR Decoder Encoder - LG SR Decoder CE Greedy A + NeuSum Decoder Encoder - LG NeuSum Decoder KL Divergence NeuSum Decoder B + RdLoss Encoder - LG MLP CE + Red . 
Loss1 Greedy C + Trigram Blocking Encoder - LG MLP CE Trigram Blocking C + MMR - Select Encoder - LG MLP CE MMR Select C + MMR - Select+ Encoder - LG MLP CE + Red . 
Loss2 MMR Select Table 2 : The architecture of redundancy reduction methods . 
Bold methods are proposed in this paper . 
4.1 Baseline Models We consider two baseline models . 
One is an inÔ¨Çuential unsupervised method explicitly balancing importance and redundancy ( Naive MMR ) . 
The other is our basic neural supervised model not dealing with redundancy at all ( ExtSum - LG ) , to which we add different redundancy reduction mechanisms . 
Naive MMR MMR ( Carbonell and Goldstein , 1998 ) is a traditional extractive summarization method , which re - ranks the candidate sentences with a balance between query - relevance(importance ) and information novelty(non - redundancy ) . 
Given a document D , at each step , MMR selects one sentence from the candidate set D\ÀÜSthat is relevant with the queryQ , while containing little redundancy with the current summary ÀÜS. Note that if there is no speciÔ¨Åc query , then the query is the representation of the whole document . 
The method can be formally speciÔ¨Åed as : MMR = arg max si‚ààD\ÀÜS[ŒªSim 1(si , Q ) ‚àí(1‚àíŒª ) max sj‚ààÀÜSSim 2(si , sj ) ] whereSim 1(si , Q)measures the similarity between the candidate sentence siand the query , indicating the importance of si , while maxsj‚ààÀÜSSim 2(si , sj)measures the similarity between the candidate sentence siand the current summary ÀÜS , representing the redundancy , and Œª is the balancing factor . 
In this work , all the Sim are computed as the cosine similarity between the embeddings of the sentences . 
ExtSum - LG For the basic model , we use the current state - ofthe - art model ( Xiao and Carenini , 2019 ) on the summarization of long documents . 
It is a novel extractive summarization model incorporating local context and global context in the encoder , withan MLP layer as decoder and cross - entropy as the loss function . 
For the sentence selection phase , it greedily picks the sentences according to the score predicted by the neural model . 
In this method , redundancy is not considered , so it is a good testbed for adding and comparing redundancy reduction methods . 
SpeciÔ¨Åcally , for a document D = { s1,s2, ... ,s n } , the output of the encoder is hifor each sentence si , and the decoder gives outputP(yi)as the conÔ¨Ådence score on the importance of sentence si . 
Finally , the model is trained on the Cross Entropy Loss : Lce=‚àín / summationdisplay i=1(yilogP(yi)+(1‚àíyi ) log ( 1‚àíP(yi ) ) 4.2 Implicitly Reduce Redundancy in the neural model ( Category A , Table 2 ) In this section , we describe two decoders from previous work , in which the redundancy of the summary is considered implicitly . 
SummaRuNNer Decoder : Nallapati et al . 
( 2016a ) introduce a decoder that computes a sentence score based on its salience , novelty(nonredundancy ) and position to decide whether it should be included in the summary . 
Formally : P(yi ) = œÉ(Wchi # Content + hiWsd # Salience ‚àíhT iWrtanh(summ i ) # Novelty + Wappa i+Wrppr i # Position + b ) # Bias wherehiis the hidden state of sentence ifrom the encoder , dis the document representation , summ i is the summary representation , updated after each decoding step , and pa i , pr iare absolute and relative position embeddings , respectively . 
Once P(yi)is obtained for each sentence i , a greedy algorithm selects the sentences to form the Ô¨Ånal summary.519 Notice that although SummaRuNNer does contain a component assessing novelty , it would be inappropriate to view this model as explicitly dealing with redunadany because the novelty component is not directly supervised . 
NeuSum Decoder : One of the main drawback of SummaRuNNer decoder is that it always score the sentences in order , i.e. , the former sentences are not inÔ¨Çuenced by the latter ones . 
In addition , it only considers redundancy in the sentence scoring phase , while simply using a greedy algorithm to select sentences according to the resulting scores . 
To address these problems , Zhou et al . 
( 2018 ) propose a new decoder to identify the relative gain of sentences , jointly learning to score and select sentences . 
In such decoder , instead of feeding the sentences and getting the scores in order , they use a mechanism similar to the pointer network ( Vinyals et al . 
, 2015 ) to predict the scores of all the sentences at each step , select the sentence with the highest score , and feed it to the next step of sentence selection . 
As for the loss function , they use the KL divergence between the predicted score distribution and the relative ROUGE F1 gain at each step . 
To be speciÔ¨Åc , the loss computed at step tis : Lt = DKL(Pt||Qt ) Pt(yi ) = exp(œÉ(hi))/summationtextn j=1exp(œÉ(hj ) ) Qt(yi ) = exp(œÑÀúgt(yi))/summationtextn j=1exp(œÑÀúgt(yj ) ) gt(yi ) = r1(St‚àí1‚à™si)‚àír1(St‚àí1 ) wherePt , Qtare the predicted and ground truth relative gain respectively , gt(yi)is the ROUGE F1 gain with respect to the current partial summary St‚àí1for sentence si , and Àúgt(yi)is the Min - Max normalizedgt(yi).œÑis a smoothing factor , which is set to 200empirically on the Pubmed dataset.3 4.3 Explicitly Reduce Redundancy in Sentence Scoring ( Category B , Table 2 ) We propose a new method to explicitly learn to reduce redundancy when scoring the sentences . 
RdLoss : Although Zhou et al . 
( 2018 ) jointly train the decoder to score and select sentences , it still learns to reduce redundancy implicitly , and the method does not allow controlling the degree of redundancy . 
To address this limitation , we propose a rather simple method to explicitly force the model 3Due to the complexity of generating the target distribution Q , we only experiment with this method on Pubmed.to reduce redundancy in the sentence scoring phase by adding a redundancy loss term to the original loss function , motivated by the success of a similar strategy of adding a bias loss term in the gender debiasing task ( Qian et al . 
, 2019 ) . 
Our new loss termLrdis naturally deÔ¨Åned as the expected redundancy contained in the resulting summary , as shown below : L = Œ≤Lce+ ( 1‚àíŒ≤)Lrd Lrd = n / summationdisplay i=1n / summationdisplay j=1P(yi)P(yj)Sim(si , sj ) whereP(yi),P(yj)are the conÔ¨Ådence scores of sentenceiandjon whether to select the sentences in the generated summary , and Sim(si , sj)is the similarity , i.e. redundancy between sentence iand j.4By adding the redundancy loss term , we penalize it more if two sentences are similar to each other and both of them have high conÔ¨Ådence scores . 
Œ≤is a balance factor , controlling the degree of redundancy . 
4.4 Explicitly Reduce Redundancy in Sentence Selection ( Category C , Table 2 ) We Ô¨Årst introduce an existing method and then propose two novel methods that explicitly reduce redundancy in the sentence selection phase . 
Trigram Blocking is widely used in recent extractive summarization models on the news dataset ( e.g. Liu and Lapata ( 2019 ) ) . 
Intuitively , it borrows the idea of MMR to balance the importance and non - redundancy when selecting sentences . 
In particular , given the predicted sentence scores , instead of just selecting sentences greedily according to the scores , the current candidate is added to the summary only if it does not have trigram overlap with the previous selected sentences . 
Otherwise , the current candidate sentence is ignored and the next one is checked , until the length limit is reached . 
MMR - Select : Inspired by the existence of a relevance / redundancy trade - off , we propose MMRSelect , a simple method to eliminate redundancy when a neural summarizer selects sentences to form a summary , in a way that is arguably more Ô¨Çexible than Trigram Blocking with a balance factor Œª . 
With the conÔ¨Ådence score computed by the basic model , P={P(y1),P(y2), ... ,P ( yn ) } , instead of picking sentences greedily , we pick the sentences according to the MMR - score , which is deÔ¨Åned 4Noting that we deÔ¨Åne Sim ( si , si)as0520 Figure 2 : The pipeline of the MMR - Select+ method , where ÀÜS,ÀÜYand¬ØS,¬ØYare the summary and labels generated by the MMR - Select algorithm and the normal greedy algorithm , respectively . 
SandYare the ground truth summary and the oracle labels . 
based on MMR and updated after each single sentence being selected . 
MMR - Select = arg max si‚ààD\ÀÜS[MMR - score i ] MMR - score i = ŒªP(yi)‚àí(1‚àíŒª ) max sj‚ààÀÜSSim(si , sj ) ] The main difference between the Naive MMR and MMR - Select falls into the computation of the importance score . 
In the Naive MMR , the importance score is the similarity between each sentence and the query , or the whole document , while in MMR - Select , the importance score is computed by a trained neural model . 
MMR - Select+ : The main limitation of MMRSelect is that the sentence scoring phase and the sentence selection phase can not beneÔ¨Åt from each other , because they are totally separate . 
To promote synergy between these two phases , we design a new method , MMR - Select+ , shown in Figure 2 , which synergistically combines three components : the basic model , the original crossentropy loss Lce(in blue ) , and an RL mechanism ( in green ) whose loss is Lrd . 
The neural model is then trained on a mixed objective loss LwithŒ≥as the scaling factor . 
Zooming on the details of the RL component , it Ô¨Årst generates a summary ÀÜSby applying the MMR selection described for MMRSelect , which is to greedily pick sentences according to MMR - score , as well as the corresponding label assignment ÀÜY={ÀÜy1,ÀÜy2, ... ,ÀÜyn}(ÀÜyi= 1 if siis selected , ÀÜyi= 0 otherwise ) . 
Then , the expected reward is computed based on the ROUGE score between ÀÜSand the gold - standard human abstractive summary Sweighted by the probability of the ÀÜYlabels . 
Notice that we also adopt the self - critical strategy ( Paulus et al . 
, 2017 ) to help accelerating the convergence by adding a baseline summary ¬ØS , which is generated by greedily pickingthe sentences according to P.r(¬ØS)is the reward of this baseline summary and it is subtracted from r(ÀÜS)to only positively reward summaries which are better than the baseline . 
Formally , the whole MMR - Select+ model can be speciÔ¨Åed as follows : L = Œ≥Lrd+ ( 1‚àíŒ≥)Lce Lrd=‚àí(r(ÀÜS)‚àír(¬ØS))n / summationdisplay i=1logP ( ÀÜyi ) r(S / prime ) = 1 3 / summationdisplay k‚àà{1,2,L}ROUGE - k ( S / prime , S ) 5 Experiments In this section , we describe the settings , results and analysis of the experiments of different methods on the Pubmed and arXiv datasets . 
5.1 Model Settings Following previous work , we use GloVe ( Pennington et al . 
, 2014 ) as word embedding , and the average word embedding as the distributed representation of sentences . 
To be comparable with Xiao and Carenini ( 2019 ) , we set word length limit of the generated summaries as 200on both datasets . 
6We tune the hyperparameter ŒªandŒ≤in the respective methods on the validation set , and set Œª= 0.6,Œ≤= 0.3for both datasets . 
Following previous work ( e.g. , Li et al . 
( 2019 ) ) , Œ≥was set to0.99 . 
For training MMR - Select+ , the learning rate islr= 1e‚àí6 ; we start with the pretrained ExtSumm - LG model . 
As for the evaluation metric , we use ROUGE scores as the measurement of importance while using the Unique N - gram Ratio and NID deÔ¨Åned in Section 3 as the measurements of redundancy . 
5The results of ExtSum - LG were obtained by re - running their model . 
6A document representation in Unsupervised MMR is similarly computed by averaging the embeddings of all the words.521 Figure 3 : The average ROUGE scores , average unique n - gram ratios , and average NID scores with different Œªused in the MMR - Select on the validation set . 
Remember that the higher the Unique n - gram Ratio , the lower NID , the less redundancy contained in the summary . 
Categ . 
ModelPubmed arXiv ROUGE-1 ROUGE-2 ROUGE - L ROUGE-1 ROUGE-2 ROUGE - L C Naive MMR 37.46 11.25 32.22 33.74 8.50 28.36 - ExtSum - LG545.18 20.20 40.72 43.77 17.50 38.71 A + SR Decoder 45.18 20.16 40.69 43.92 17.65 38.83 A + NeuSum Decoder 44.54 19.66 40.42 - - B + RdLoss 45.30‚Ä†20.42‚Ä†40.95‚Ä†44.01‚Ä†17.79‚Ä†39.09‚Ä† C + Trigram Blocking 43.33 17.67 39.01 42.75 15.73 37.85 C + MMR - Select 45.29‚Ä†20.30‚Ä†40.90‚Ä†43.81 17.41 38.94 C + MMR - Select+ 45.39‚Ä†20.37‚Ä†40.99‚Ä†43.87‚Ä†17.50 38.97‚Ä† - Oracle 55.05 27.48 49.11 53.89 23.07 46.54 Table 3 : Rouge score of different summarization models on the Pubmed and arXiv datasets . 
‚Ä†indicates signiÔ¨Åcantly better than the ExtSum - LG with conÔ¨Ådence level 99 % on the Bootstrap SigniÔ¨Åcance test . 
Green numbers means it‚Äôsbetter than ExtSum - LG on the certain metric , and the red numbers means worse . 
Categ . 
ModelPubmed arXiv Unigram% Bigram% Trigram% NID Unigram% Bigram% Trigram% NID C Naive MMR 56.55 90.93 96.95 0.1881 53.01 88.82 96.28 0.1992 - ExtSum - LG 53.02 87.29 94.37 0.2066 52.17 87.19 95.38 0.2088 A + SR Decoder 52.88 87.17 94.32 0.2070 51.98 87.08 95.31 0.2097 A + NeuSum Decoder 54.88‚Ä†88.71‚Ä†95.13‚Ä†0.1993‚Ä†- - - B + RdLoss 53.23‚Ä†87.41 94.43 0.2052‚Ä†52.17 87.20 95.36 0.2085 C + Trigram Blocking 57.58‚Ä†‚Ä°93.05‚Ä†‚Ä°98.56‚Ä†‚Ä°0.1818‚Ä†‚Ä°56.12‚Ä†‚Ä°92.38‚Ä†‚Ä°98.94‚Ä†‚Ä°0.1876‚Ä†‚Ä° C + MMR - Select 53.76‚Ä†88.04‚Ä†94.96‚Ä†0.2022 52.80‚Ä†87.64‚Ä†95.40 0.2055‚Ä† C + MMR - Select+ 53.93‚Ä†88.32 95.14 0.2014 52.76‚Ä†87.78‚Ä†95.70‚Ä†0.2055‚Ä† - Oracle 56.66 89.25 95.55 0.2036 56.74 90.81 96.82 0.2029 - Reference 56.69 89.45 95.95 0.2005 58.92 90.13 97.02 0.1970 Table 4 : Unique n - gram ratio and NID score on the two datasets . 
‚Ä†indicates signiÔ¨Åcant differences from ( Xiao and Carenini , 2019 ) with conÔ¨Ådence level 99 % , while ‚Ä°indicates signiÔ¨Åcant differences from all the other models with conÔ¨Ådence level 99 % on the Bootstrap SigniÔ¨Åcance test . 
Noting the higher the Unique n - gram Ratio , the lower NID , the less redundancy contained in the summary . 
Green numbers means it ‚Äôs better than ExtSum - LG on the certain metric , and the red numbers means worse . 
5.2 Finetuning Œª Consistently with previous work ( Jung et al . 
, 2019 ) , when we Ô¨Ånetune Œªof MMR Select on the validation set , we pinpoint the trade off between importance and non - redundancy in the generated summary ( see Figure 3 ) . 
For Œª‚â§0.6 , as we increase the weight of importance score , the average ROUGE scores continuously increase while the redundancy / diversity increases / drops rapidly . 
But since extractive methods can only reuse sentences from the input document , there is an upper bound on how much the generated summary can match the ground - truth summary , so whenŒª>0.6 , the ROUGE score even drops by a small margin , while the redundancy / diversity still decreases / drops . 
Then the problem to solve for future work is how to increase the peak , which could be done by either applying Ô¨Åner units ( e.g. , clauses instead of sentences ) or further improve the model that predicts the importance score . 
5.3 Overall Results and Analysis The experimental results for the ROUGE scores are shown in Table 3 , whereas results for redundancy scores ( Unique N - gram Ratio and NID score ) are shown in Table 4 . 
With respect to the balance be-522 tween importance and non - redundancy , despite the trade - off between the two aspects , all of the three methods we propose can reduce redundancy signiÔ¨Åcantly while also improving the ROUGE score signiÔ¨Åcantly compared with the ExtSum - LG basic neural model . 
In contrast , the NeuSum Decoder and Trigram Blocking effectively reduce redundancy , but in doing that they hurt the importance aspect considerably . 
Even worse , the SR Decoder is dominated by the basic model on both aspects . 
Focusing on the redundancy aspect ( Table 4 ) , Trigram Blocking makes the largest improvement on redundancy reduction , but with a large drop in ROUGE scores . 
This is in striking contrast with results on news datasets ( Liu and Lapata , 2019 ) , where Trigram Blocking reduced redundancy while also improving the ROUGE score signiÔ¨Åcantly . 
Plausibly , the difference between the performances across datasets might be the result of the inÔ¨Çexibility of the method . 
In both Pubmed and arXiv datasets , the sentences are much longer than those in the news dataset ( See Table 1 ) , and therefore , simply dropping candidate sentences with 3 - gram overlap may lead to incorrectly missing sentences with substantial important information . 
Furthermore , another insight revealed in Table 4 is that dealing with redundancy in the sentence selection phase is consistently more effective than doing it in the sentence scoring phase , regardless of whether this happens implicitly ( NeuSum > SR Decoder ) or explicitly ( Trigram Blocking , MMRSelect/+>RdLoss ) . 
Moving to more speciÔ¨Åc Ô¨Åndings about particular systems , we already noted that while the NeuSum Decoder reduces redundancy effectively , it performs poorly on the ROUGE score , something that did not happen with news datasets . 
A possible explanation is that the number of sentences selected for the scientiÔ¨Åc paper datasets ( on average 6 - 7 sentences ) is almost twice the number of sentences selected for news ; and as it was mentioned in the original paper ( Zhou et al . 
, 2018 ) , the precision of NeuSum drops rapidly after selecting a few sentences . 
Other results conÔ¨Årm established beliefs . 
The considerable difference between Naive MMR and MMR - Select was expected given the recognized power of neural network over unsupervised methods . 
Secondly , the unimpressive performance of the SR decoder conÔ¨Årms that the in - order sequence scoring is too limited for effectively predicting im - portance score and reducing redundancy . 
5.4 More Insights of the Experiments In addition to the main experiment results discussed above , we further explore the performance on informativeness ( ROUGE score ) and redundancy ( Unique N - gram Ratio ) of different redundancy reduction methods under two different conditions , namely the degree of redundancy and the length of the source documents . 
Figure 4 shows the results on the Pubmed dataset , while further results of a similar analysis on the arXiv dataset can be found in the Appendices . 
With respect to the degree of redundancy , ( upper part of Figure 4 ) , the less redundant the document is , the less impact the redundancy reduction methods have . 
Among all the methods , although Trigram Blocking works the best with respect to reducing redundancy , it hurts the informativeness the most . 
However , it is still a good choice for a rather less redundant document ( e.g. the documents in the last two bins with avg Unique N - gram Ratio over 0.7 ) , which is also consistent with the previous works showing the Trigram Blocking works well on the news datasets , which tends to be less redundant ( see ¬ß 3 ) . 
As for all the other methods , although they have the same trends , MMR - Select+ performs the best on both informativeness and redundancy reduction , especially for the more redundant documents . 
Regarding to the length of the source document ( bottom part of Figure 4 ) , as the document become longer , both informativeness and redundancy in the summary generated by all methods increases and then decrease once hitting the peak . 
MMRSelect+ and MMR - Select are the best choices to balance between the informativeness and redundancy - they are the only two methods having the higher ROUGE scores and higher Unique N - gram ratios across different lengths , even for the short documents with less than 50 sentences . 
Besides , we also conduct experiments on generating summaries with different length limit , where we found that our new methods are stable across different summary lengths ( Figure . 
5 ) . 
6 Conclusion and Future work Balancing sentence importance and redundancy is a key problem in extractive summarization . 
By examining large summarization datasets , we Ô¨Ånd that longer documents tend to be more redundant . 
Therefore in this paper , we systematically explore and compare existing and newly proposed methods523 Figure 4 : Comparing the average ROUGE scores and average unique n - gram ratios of different models on the Pubmed dataset , conditioned on different degrees of redundancy and lengths of the document ( extremely long documents - i.e. , 1 % of the dataset are not shown because of space constraints).7 Figure 5 : Comparing the average ROUGE scores and average unique n - gram ratios of different models with different word length limits on the Pubmed dataset . 
See Appendices for similar results on arXiv.for redundancy reduction in summarizing long documents . 
Experiments indicate that our novel methods achieve SOTA on the ROUGE scores , while signiÔ¨Åcantly reducing redundancy on two scientiÔ¨Åc paper datasets ( Pubmed and arXiv ) . 
Interestingly , we show that redundancy reduction in sentence selection is more effective than in the sentence scoring phase , a Ô¨Ånding to be further investigated . 
Additional venues for future work include experimenting with generating summaries at Ô¨Åner granularity than sentences , as suggested by our analysis of the Œªparameter . 
We also intend to explore other ways to assess redundancy , moving from computing the cosine similarity between sentence embeddings , to a pre - trained neural model for sentence similarity . 
Finally , we plan to run human evaluations to assess the quality of the generated summaries . 
This is quite challenging for scientiÔ¨Åc papers , as it requires participants to possess sophisticated domain - speciÔ¨Åc background knowledge . 
Acknowledgments We thank reviewers and the UBC - NLP group for their insightful comments . 
This research was supported by the Language & Speech Innovation Lab of Cloud BU , Huawei Technologies Co. , Ltd.524 A Appendices In these Appendices , we show more analysis of the experimental results . 
A.1 Analysis on arXiv Dataset under conditions Figure 6 shows the performance on informativeness ( ROUGE score ) and redundancy ( Unique N - gram Ratio ) of different redundancy reduction methods under different conditions on the arXiv dataset . 
Comparing with the Pubmed dataset , the documents in the arXiv dataset tend to be longer and more redundant , as the majority of the documents in the Pubmed dataset have less than 100 sentences with average Unique N - gram Ratio in the 0.5‚àí0.6interval , while the majority of the documents in the arXiv dataset have number of sentences in the range 100 to 300 with average Unique N - gram Ratio in the0.6‚àí0.7interval . 
Consistent with the result on the Pubmed dataset , the Trigram Blocking method is the best choice for rather less redundant documents ( with average Unique N - gram Ratio larger than0.7 ) , and the MMR - Select+ is the one better or equivalent to the original model across different degree of redundancy , ignoring the outliers . 
With respect to the length of the documents , the MMRSelect+ and MMR - Select are consistently the most effective methods for balancing redundancy and informativeness on documents with different length . 
A.2 Analysis on Selection Overlap To explore the difference made by applying different redundancy reduction methods on the original method(ExtSumLG ) , we compare the selected sentences by all the methods , and show the overlap ratios between every two methods , as well as the total number and the average length of selected sentences in the test set , in Table 5 and Table 6 for Pubmed dataset and arXiv dataset respectively . 
As we can see from the tables , except for the SR Decoder , all the other methods tend to select more and shorter sentences than the original summarizer . 
Regarding the overlap between the original method and the others , we observe that among all the three categories , the methods in category A tend to produce large differences , since these methods change the structure of the original model . 
Comparing the methods in Category C , around 36 % of the sentences are regarded as redundant by Trigram Blocking , which means 36 % of the sentences have trigram - overlap with other selected sentences , while only around 10 % sentences are regarded as redundant by MMR - Select . 
As the ROUGE scores of MMR - Select are much better than Trigram Blocking on both datasets , this is in line with our analysis in Section 5.3 , Triagram Blocking dropping some important sentences incorrectly . 
Interestingly , we notice that the overlap ratio between Trigram Block and MMR - Select is considerably larger than the overlap ratio of Trigram Block with original method ( ExtSumLG ) on both datasets . 
This indicates that there are some sentences , not selected by the original method , which are considered to be important by both the Trigram Blocking and MMR - Select methods.526 Figure 6 : Comparing the average ROUGE scores and average unique n - gram ratios of different models on the arXiv dataset , conditioned on different degrees of redundancy and lengths of the document.8 - ExtSumLG + SR + NeuSum + RdLoss + Tri - Block + MMR - Select + MMR - Select+ ExtSumLG 100.00 72.84 52.00 77.70 60.77 87.71 85.75 + SR 72.66 100.00 49.73 70.29 52.24 69.78 70.64 + Neusum 60.44 57.94 100.00 60.77 48.47 60.38 61.07 + RdLoss 80.84 73.32 54.40 100.00 57.67 79.03 80.08 + Tri - Block 64.85 55.89 44.51 59.15 100.00 64.72 64.38 + MMR - Select 90.49 72.17 53.59 78.37 62.56 100.00 91.15 + MMR - Select+ 88.66 73.22 54.33 79.58 62.38 91.35 100.00 # Sent . 
Sel . 
36979 36888 42981 38476 39463 38151 38236 # words / Sent 40.66 40.84 33.38 38.95 37.21 39.35 39.31 Table 5 : Micro overlap ratio ( % ) between the selections of different methods and the total number and the average length of selected sentences in the test set of Pubmed . 
A.3 Analysis on Recall and Precision of ROUGE Scores We also provide the Precision and Recall of the ROUGE scores in the main experiment , the results of Pubmed and arXiv datasets are shown in Table 7 and Table 8 , respectively . 
It is interesting to see that the NeuSum Decoder tends to have a high precision but low recall , indicating that the generated summaries tend to be shorter and contain less useful information than the original method . 
A.4 Analysis on the Relative Position of Selections We also show the relative position distribution of the selected sentences on both datasets in Figure 7 to verify if any redundancy reduction method has aparticular tendency to select sentences in particular position of the documents . 
However , as shown in the Ô¨Ågure , the trends are all rather similar for all methods.527 - ExtSumLG + SR + NeuSum + RdLoss + Tri - Block + MMR - Select + MMR - Select+ ExtSumLG 100.00 72.06 - 76.51 56.22 75.04 80.21 + SR 73.84 100.00 - 69.07 49.16 62.82 67.03 + Neusum - - - - - - + RdLoss 79.88 70.38 - 100.00 53.00 67.81 72.57 + Tri - Block 64.59 55.12 - 58.33 100.00 60.34 62.55 + MMR - Select 88.93 72.65 - 76.97 62.23 100.00 93.13 + MMR - Select+ 89.96 73.36 - 77.96 61.06 88.14 100.00 # Sent . 
Sel . 
39698 40681 - 41448 45611 47045 44526 # words / Sent 36.26 35.52 - 34.50 30.86 30.73 32.40 Table 6 : Micro overlap ratio ( % ) between the selections of different methods and the total number and the average length of selected sentences in the test set of arXiv . 
Categ . 
ModelPubmed ROUGE-1 ROUGE-2 ROUGE - L Prec . 
Recall Prec . 
Recall Prec . 
Recall C Naive MMR 36.45 42.56 11.05 12.64 31.39 36.53 - ExtSum - LG944.05 51.08 19.82 22.71 39.74 45.97 A + SR Decoder 44.00 51.10 19.75 22.68 39.66 45.96 A + NeuSum Decoder 44.36 49.24 19.74 21.58 40.29 44.62 B + RdLoss 44.30 51.09 20.11 22.88 40.09 46.11 C + Trigram Blocking 42.67 48.54 17.51 19.73 38.45 43.64 C + MMR - Select 44.25 51.09 19.98 22.75 40.08 46.07 C + MMR - Select+ 44.28 51.27 20.01 22.86 40.03 46.24 Table 7 : Rouge Recall and Precision of different summarization models on the Pubmed dataset . 
Green numbers means it ‚Äôs better than ExtSum - LG on the certain metric , and the red numbers means worse . 
Categ . 
ModelArxiv ROUGE-1 ROUGE-2 ROUGE - L Prec . 
Recall Prec . 
Recall Prec . 
Recall C Naive MMR 29.61 42.69 7.45 10.78 24.92 35.82 - ExtSum - LG1038.60 54.64 15.38 22.00 34.17 48.26 A + SR Decoder 38.65 54.99 15.47 22.28 34.24 48.64 A + NeuSum Decoder - - - - - B + RdLoss 38.92 54.77 15.68 22.29 34.60 48.59 C + Trigram Blocking 38.04 52.71 13.98 19.47 33.71 46.61 C + MMR - Select 38.85 54.33 15.39 21.74 34.56 48.24 C + MMR - Select+ 38.75 54.67 15.41 21.96 34.44 48.51 Table 8 : Rouge Recall and Precision of different summarization models on the Pubmed dataset . 
Green numbers means it ‚Äôs better than ExtSum - LG on the certain metric , and the red numbers means worse . 
Figure 7 : The relative position distribution of different redundancy reduction methods on Pubmed(left ) and arXiv(right ) datasets.528 Proceedings of the 1st Conference of the Asia - PaciÔ¨Åc Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing , pages 529‚Äì535 December 4 - 7 , 2020 . 
¬© 2020 Association for Computational Linguistics A Cascade Approach to Neural Abstractive Summarization with Content Selection and Fusion Logan Lebanoff, ‚ô† Franck Dernoncourt, ‚ô¶ Doo Soon Kim, ‚ô¶ Walter Chang, ‚ô¶ Fei Liu ‚ô† ‚ô† Computer Science Department , University of Central Florida , Orlando , FL ‚ô¶ Adobe Research , San Jose , CA loganlebanoff@knights.ucf.edu { dernonco , dkim , wachang } @adobe.com feiliu@cs.ucf.edu Abstract We present an empirical study in favor of a cascade architecture to neural text summarization . 
Summarization practices vary widely but few other than news summarization can provide a sufÔ¨Åcient amount of training data enough to meet the requirement of end - to - end neural abstractive systems which perform content selection and surface realization jointly to generate abstracts . 
Such systems also pose a challenge to summarization evaluation , as they force content selection to be evaluated along with text generation , yet evaluation of the latter remains an unsolved problem . 
In this paper , we present empirical results showing that the performance of a cascaded pipeline that separately identiÔ¨Åes important content pieces and stitches them together into a coherent text is comparable to or outranks that of end - to - end systems , whereas a pipeline architecture allows for Ô¨Çexible content selection . 
We Ô¨Ånally discuss how we can take advantage of a cascaded pipeline in neural text summarization and shed light on important directions for future research . 
1 Introduction There is a variety of successful summarization applications but few can afford to have a large number of annotated examples that are sufÔ¨Åcient to meet the requirement of end - to - end neural abstractive summarization . 
Examples range from summarizing radiology reports ( Jing et al . 
, 2019 ; Zhang et al . 
, 2020 ) to congressional bills ( Kornilova and Eidelman , 2019 ) and meeting conversations ( Mehdad et al . 
, 2013 ; Li et al . 
, 2019 ; Koay et al . 
, 2020 ) . 
The lack of annotated resources suggests that end - toend systems may not be a ‚Äú one - size-Ô¨Åts - all ‚Äù solution to neural text summarization . 
There is an increasing need to develop cascaded architectures to allow for customized content selectors to be combined with general - purpose neural text generatorsto realize the full potential of neural abstractive summarization . 
We advocate for explicit content selection as it allows for a rigorous evaluation and visualization of intermediate results of such a module , rather than associating it with text generation . 
Existing neural abstractive systems can perform content selection implicitly using end - to - end models ( See et al . 
, 2017 ; Celikyilmaz et al . 
, 2018 ; Raffel et al . 
, 2019 ; Lewis et al . 
, 2020 ) , or more explicitly , with an external module to select important sentences or words to aid generation ( Tan et al . 
, 2017 ; Gehrmann et al . 
, 2018 ; Chen and Bansal , 2018 ; Kry ¬¥ sci¬¥nski et al . 
, 2018 ; Hsu et al . 
, 2018 ; Lebanoff et al . 
, 2018 , 2019b ; Liu and Lapata , 2019 ) . 
However , content selection concerns not only the selection of important segments from a document , but also the cohesiveness of selected segments and the amount of text to be selected in order for a neural text generator to produce a summary . 
In this paper , we aim to investigate the feasibility of a cascade approach to neural text summarization . 
We explore a constrained summarization task , where an abstract is created one sentence at a time through a cascaded pipeline . 
Our pipeline architecture chooses one or two sentences from the source document , then highlights their summary - worthy segments and uses those as a basis for composing a summary sentence . 
When a pair of sentences are selected , it is important to ensure that they are fusible ‚Äî there exists cohesive devices that tie the two sentences together into a coherent text ‚Äî to avoid generating nonsensical outputs ( Geva et al . 
, 2019 ; Lebanoff et al . 
, 2020 ) . 
Highlighting sentence segments allows us to perform Ô¨Åne - grained content selection that guides the neural text generator to stitch selected segments into a coherent sentence . 
The contributions of this work are summarized as follows.529 SentPredHighlightNon - Highlight [ CLS]aduke thursdaySentenceSelectionFine - GrainedContent Selection Highlight studentNon - Highlight ... ... ...... aduke thursdaystudent++++ duke < START > duke studentstudent has ... Encoder Decoder wordembeddinghighlightembeddingGeneration Figure 1 : Model architecture . 
We divide the task between two main components : the Ô¨Årst component performs sentence selection and Ô¨Åne - grained content selection , which are posed as a classiÔ¨Åcation problem and a sequencetagging problem , respectively . 
The second component receives the Ô¨Årst component ‚Äôs outputs as supplementary information to generate the summary . 
A cascade architecture provides the necessary Ô¨Çexibility to separate content selection from surface realization in abstractive summarization . 
‚Ä¢We present an empirical study in favor of a cascade architecture for neural text summarization . 
Our cascaded pipeline chooses one or two sentences from the document and highlights their important segments ; these segments are passed to a neural generator to produce a summary sentence . 
‚Ä¢Our quantitative results show that the performance of a cascaded pipeline is comparable to or outranks that of end - to - end systems , with added beneÔ¨Åt of Ô¨Çexible content selection . 
We discuss how we can take advantage of a cascade architecture and shed light on important directions for future research.1 2 A Cascade Approach Our cascaded summarization approach focuses on shallow abstraction . 
It makes use of text transformations such as sentence shortening , paraphrasing and fusion ( Jing and McKeown , 2000 ) and is in contrast to deep abstraction , where a full semantic analysis of the document is often required . 
A shallow approach helps produce abstracts that convey important information while , crucially , remaining faithful to the original . 
In what follows , we describe our approach to select single sentences and sentence pairs from the document , highlight summary - worthy segments and perform summary generation conditioned on highlights . 
Selection of Singletons and Pairs Our approach iteratively selects one or two sentences from the input document ; they serve as the basis for composing a single summary sentence . 
Previous research suggests that 60 - 85 % of human - written summary 1Our code is publicly available at https://github . 
com / ucfnlp / cascaded - summsentences are created by shortening a single sentence or merging a pair of sentences ( Lebanoff et al . 
, 2019b ) . 
We adopt this setting and present a coarse - to-Ô¨Åne strategy for content selection . 
Our strategy begins with selecting sentence singletons and pairs , followed by highlighting important segments of the sentences . 
Importantly , the strategy allows us to control which segments will be combined into a summary sentence‚Äî‚Äúcompatible ‚Äù segments come from either a single document sentence or a pair of fusible sentences . 
In contrast , when all important segments of the document are provided to a neural generator all at once ( Gehrmann et al . 
, 2018 ) , it can happen that the generator arbitrarily stitches together text segments from unrelated sentences , yielding a summary that contains hallucinated content and fails to retain the meaning of the original document ( Falke et al . 
, 2019 ; Lebanoff et al . 
, 2019a ; Kryscinski et al . 
, 2019 ) . 
We expect a sentence singleton or pair to be selected from the document if it contains salient content . 
Moreover , a pair of sentences should contain content that is compatible with each other . 
Given a sentence or pair of sentences from the document , our model predicts whether it is a valid instance to be compressed or merged to form a summary sentence . 
We follow ( Lebanoff et al . 
, 2019b ) to use BERT ( Devlin et al . 
, 2019 ) to perform the classiÔ¨Åcation . 
BERT is a natural choice since it takes one or two sentences and generates a classiÔ¨Åcation prediction . 
It treats an input singleton or pair of sentences as a sequence of tokens . 
The tokens are fed to a series of Transformer block layers , consisting of multi - head self - attention modules . 
The Ô¨Årst Transformer layer creates a contextual representation for each token , and each successive layer further reÔ¨Ånes those representations . 
An additional530 R - L F - score ( % ) R-2 F - score ( % ) R-1 F - score ( % ) Probability ThresholdingProportional to Input ( All Sents)Proportional to Input ( 1~2 Sents)Threshold ValueThreshold ValueThreshold Value Percentage of Words ( % ) Percentage of Words ( % ) Percentage of Words ( % ) Figure 2 : Comparison of various highlighting strategies . 
Thresholding obtains the best performance . 
[ CLS ] token is added to contain the sentence representation . 
BERT is Ô¨Åne - tuned for our task by adding an output layer on top of the Ô¨Ånal layer representation hL [ CLS ] for sequence s , as seen in Eq . 
( 1 ) . 
psent(s ) = œÉ(u / latticetophL [ CLS ] ) ( 1 ) where uis a vector of weights and œÉis the sigmoid function . 
The model predicts psent ‚Äì whether the sentence singleton or pair is an appropriate one based on the [ CLS ] token representation . 
We describe the training data for this task in ¬ß 3 . 
Fine - Grained Content Selection It is interesting to note that the previous architecture can be naturally extended to perform Ô¨Åne - grained content selection by highlighting important words of sentences . 
When two sentences are selected to generate a fusion sentence , it is desirable to identify segments of text from these sentences that are potentially compatible with each other . 
The coarse - toÔ¨Åne method allows us to examine the intermediate results and compare them with ground - truth . 
Concretely , we add a classiÔ¨Åcation layer to the Ô¨Ånal layer representation hL ifor each token wi(Eq . 
( 2 ) ) . 
The per - target - word loss is then interpolated with instance prediction ( one or two sentences ) loss using a coefÔ¨Åcient Œª . 
Such a multi - task learning objective has been shown to improve performance on a number of tasks ( Guo et al . 
, 2019 ) . 
phighlight ( wi ) = œÉ(v / latticetophL i ) ( 2 ) where vis a vector of weights and œÉis the sigmoid function . 
The model predicts phighlight for each token ‚Äì whether the token should be included in the output fusion , calculated based on the given token ‚Äôs representation . 
Information Fusion Given one or two sentences taken from a document and their Ô¨Åne - grained highlights , we proceed by describing a fusion process that generates a summary sentence from the selected content . 
Our model employs an encoderdecoder architecture based on pointer - generatornetworks that has shown strong performance on its own and with adaptations ( See et al . 
, 2017 ; Gehrmann et al . 
, 2018 ) . 
We feed the sentence singleton or pair to the encoder along with highlights derived by the Ô¨Åne - grained content selector , the latter come in the form of binary tags . 
The tags are transformed to a ‚Äú highlight - on ‚Äù embedding for each token if it is chosen by the content selector , and a ‚Äú highlight - off ‚Äù embedding for each token not chosen . 
The highlight - on / off embeddings are added to token embeddings in an element - wise manner ; both highlight and token embeddings are learned . 
An illustration is shown in Figure 1 . 
Highlights provide a valuable intermediate representation suitable for shallow abstraction . 
Our approach thus provides an alternative to methods that use more sophisticated representations such as syntactic / semantic graphs ( Filippova and Strube , 2008 ; Banarescu et al . 
, 2013 ; Liu et al . 
, 2015 ) . 
It is more straightforward to incorporate highlights into an encoder - decoder fusion model , and obtaining highlights through sequence tagging can be potentially adapted to new domains . 
3 Experimental Results Data and Annotation To enable direct comparison with end - to - end systems , we conduct experiments on the widely used CNN / DM dataset ( See et al . 
, 2017 ) to report results of our cascade approach . 
We use the procedure described in Lebanoff et al . 
( 2019b ) to create training instances for the sentence selector and Ô¨Åne - grained content selector . 
Our training data contains 1,053,993 instances ; every instance contains one or two candidate sentences . 
It is a positive instance if a groundtruth summary sentence can be formed by compressing or merging sentences of the instance , negative otherwise . 
For positive instances , we highlight all lemmatized unigrams appearing in the summary , excluding punctuation . 
We further add smoothing to the labels by highlighting single words that con-531 System R-1 R-2 R - L SumBasic ( Vanderwende et al . 
, 2007 ) 34.11 11.13 31.14 LexRank ( Erkan and Radev , 2004 ) 35.34 13.31 31.93 Pointer - Generator ( See et al . 
, 2017 ) 39.53 17.28 36.38 FastAbsSum ( Chen and Bansal , 2018 ) 40.88 17.80 38.54 BERT - Extr ( Lebanoff et al . 
, 2019b ) 41.13 18.68 37.75 BottomUp ( Gehrmann et al . 
, 2018 ) 41.22 18.68 38.34 BERT - Abs ( Lebanoff et al . 
, 2019b ) 37.15 15.22 34.60 Cascade - Fusion ( Ours ) 40.10 17.61 36.71 Cascade - Tag ( Ours ) 40.24 18.33 36.14 GT - Sent + Sys - Tag 50.40 27.74 46.25 GT - Sent + Sys - Tag + Fusion 51.33 28.08 47.50 GT - Sent + GT - Tag 74.80 48.21 67.40 GT - Sent + GT - Tag + Fusion 72.70 48.33 67.06(SYSTEM SENTS ) ADuke student hasadmittedtohang inganoose made ofrope from atreenear astudent union , university ofÔ¨Åcials said Thursday . 
Thestudent was identiÔ¨ÅedduringaninvestigationbycampuspoliceandtheofÔ¨Åceofstudent affairs andadmittedtoplacingthe noose onthetreeearly Wednes day , the university said . 
( CASCADE -FUSION ) ADuke student wasidentiÔ¨ÅedduringaninvestigationbycampuspoliceandtheofÔ¨Åceofstudent affairs andadmittedto placingthenoose onthetreeearly Wednes day . 
( GT S ENTS ) In a news release , it said the student wasnolonger oncampusandwillface student conduct review . 
Duke Universityis a private college with about 15,000 students in Durham , North Carolina . 
( GT S ENTS + F USION ) Duke Universitystudent wasnolonger oncampusandwillface student conduct review . 
( REFERENCE ) Student isnolonger onDuke Universitycampusandwill face disciplinary review . 
Table 1 : ( L EFT ) Summarization results on CNN / DM test set . 
Our cascade approach performs comparable to strong extractive and abstractive baselines ; oracle models using ground - truth sentences and segment highlights perform the best . 
( R IGHT ) Example source sentences and their fusions . 
Dark highlighting is content taken from the Ô¨Årst sentence , and light highlighting comes from the second . 
Our Cascade - Fusion approach effectively performs entity replacement by replacing ‚Äú student ‚Äù in the second sentence with ‚Äú a Duke student ‚Äù from the Ô¨Årst sentence . 
nect two highlighted phrases and by dehighlighting isolated stopwords . 
At test time , four highestscored instances are selected per document ; their important segments are highlighted by content selector then passed to the fusion step to produce a summary sentence each . 
The hyperparameter Œªfor weighing the per - target - word loss is set to 0.2 and highlighting threshold value is 0.15 . 
The model hyperparameters are tuned on the validation split . 
Summarization Results We show experimental results on the standard test set and evaluated by ROUGE metrics ( Lin , 2004 ) in Table 1 . 
The performance of our cascade approaches , Cascade - Fusion andCascade - Tag , is comparable to or outranks a number of extractive and abstractive baselines . 
Particularly , Cascade - Tag does not use a fusion step ( ¬ß 2 ) and is the output of Ô¨Åne - grained content selection . 
Cascade - Fusion provides a direct comparison against BERT - Abs ( Lebanoff et al . 
, 2019b ) that uses sentence selection and fusion but lacks a Ô¨Åne - grained content selector . 
Our results suggest that a coarse - to-Ô¨Åne content selection strategy remains necessary to guide the fusion model to produce informative sentences . 
We observe that the addition of the fusion model has only a moderate impact on ROUGE scores , but the fusion process can reorder text segments to create true and grammatical sentences , as shown in Table 1 . 
We analyze the performance of a number of oracle models that use ground - truth sentence selection ( GT - Sent ) and tagging ( GT - Tag ) . 
When given ground - truth sentences as input , our cascademodels achieve‚àº10 points of improvement in all ROUGE metrics . 
When the models are also given ground - truth highlights , they achieve an additional 20 points of improvement . 
In a preliminary examination , we observe that not all highlights are included in the summary during fusion , indicating there is space for improvement . 
These results show that cascade architectures have great potential to generate shallow abstracts and future emphasis may be placed on accurate content selection . 
How much should we highlight ? It is important to quantify the amount of highlighting required for generating a summary sentence . 
Highlighting too much or too little can be unhelpful . 
We experiment with three methods to determine the appropriate amount of words to highlight . 
Probability Thresholding chooses a set threshold whereby all words that have a probability higher than the threshold are highlighted . 
When Proportional to Input is used , the highest probability words are iteratively highlighted until a target rate is reached . 
The amount of highlighting can be proportional to the total number of words per instance ( one or two sentences ) or per document , containing all sentences selected for the document . 
We investigate the effect of varying the amount of highlighting in Figure 2 . 
Among the three methods , probability thresholding performs the best , as it gives more freedom to content selection . 
If the model scores all of the words in sentences highly , then we should correspondingly highlight all of the words . 
If only very few words score highly , then532 we should only pick those few . 
Highlighting a certain percentage of words tend to perform less well . 
On our dataset , a threshold value of 0.15‚Äì0.20 produces the best ROUGE scores . 
Interestingly , these thresholds end up highlighting 58‚Äì78 % of the words of each sentence . 
Compared to what the generator was trained on , which had a median of 31 % of each sentence highlighted , the system ‚Äôs rate of highlighting is higher . 
If the model ‚Äôs highlighting rate is set to be similar to that of the ground - truth , it yields much lower ROUGE scores ( cf . 
threshold value of 0.3 in Figure 2 ) . 
This observation suggests that the amount of highlighting can be related to the effectiveness of content selector and it may be better to highlight more than less . 
4 Conclusion We present a cascade approach to neural abstractive summarization that separates content selection from surface realization . 
Importantly , our approach makes use of text highlights as intermediate representation ; they are derived from one or two sentences using a coarse - to-Ô¨Åne content selection strategy , then passed to a neural text generator to compose a summary sentence . 
A successful cascade approach is expected to accurately select sentences and highlight an appropriate amount of text , both can be customized for domain - speciÔ¨Åc tasks . 
Acknowledgments We are grateful to the anonymous reviewers for their comments and suggestions . 
This research was supported in part by the National Science Foundation grant IIS-1909603 . 
Abstract Cross - lingual Summarization ( CLS ) aims at producing a summary in the target language for an article in the source language . 
Traditional solutions employ a twostep approach , i.e. translate ‚Üísummarize or summarize ‚Üítranslate . 
Recently , end - to - end models have achieved better results , but these approaches are mostly limited by their dependence on large - scale labeled data . 
We propose a solution based on mixed - lingual pretraining that leverages both cross - lingual tasks such as translation and monolingual tasks like masked language models . 
Thus , our model can leverage the massive monolingual data to enhance its modeling of language . 
Moreover , the architecture has no task - speciÔ¨Åc components , which saves memory and increases optimization efÔ¨Åciency . 
We show in experiments that this pre - training scheme can effectively boost the performance of cross - lingual summarization . 
In Neural Cross - Lingual Summarization ( NCLS ) ( Zhu et al . 
, 2019b ) dataset , our model achieves an improvement of 2.82 ( English to Chinese ) and 1.15 ( Chinese to English ) ROUGE-1 scores over state - of - the - art results . 
1 Introduction Text summarization can facilitate the propagation of information by providing an abridged version for long articles and documents . 
Meanwhile , the globalization progress has prompted a high demand of information dissemination across language barriers . 
Thus , the cross - lingual summarization ( CLS ) task emerges to provide accurate gist of articles in a foreign language . 
Traditionally , most CLS methods follow the twostep pipeline approach : either translate the article into the target language and then summarize it ( Leuski et al . 
, 2003 ) , or summarize the article in the source language and then translate it ( Wan ‚àóEqual contributionet al . 
, 2010 ) . 
Although this method can leverage off - the - shelf summarization and MT models , it suffers from error accumulation from two independent subtasks . 
Therefore , several end - to - end approaches have been proposed recently ( Zhu et al . 
, 2019b ; Ouyang et al . 
, 2019 ; Duan et al . 
, 2019 ) , which conduct both translation and summarization simultaneously . 
Easy to optimize as these methods are , they typically require a large amount of cross - lingual summarization data , which may not be available especially for low - resource languages . 
For instance , NCLS ( Zhu et al . 
, 2019b ) proposes to co - train on monolingual summarization ( MS ) and machine translation ( MT ) tasks , both of which require tremendous labeling efforts . 
On the other hand , the pre - training strategy has proved to be very effective for language understanding ( Devlin et al . 
, 2018 ; Holtzman et al . 
, 2019 ) and cross - lingual learning ( Lample and Conneau , 2019 ; Chi et al . 
, 2019 ) . 
One of the advantages of pre - training is that many associated tasks are selflearning by nature , which means no labeled data is required . 
This greatly increases the amount of training data exposed to the model , thereby enhancing its performance on downstream tasks . 
Therefore , we leverage large - scale pre - training to improve the quality of cross - lingual summarization . 
Built upon a transformer - based encoderdecoder architecture ( Vaswani et al . 
, 2017 ) , our model is pre - trained on both monolingual tasks including masked language model ( MLM ) , denoising autoencoder ( DAE ) and monolingual summarization ( MS ) , and cross - lingual tasks such as crosslingual masked language model ( CMLM ) and machine translation ( MT ) . 
This mixed - lingual pretraining scheme can take advantage of massive unlabeled monolingual data to improve the model ‚Äôs language modeling capability , and leverage crosslingual tasks to improve the model ‚Äôs cross - lingual representation . 
We then Ô¨Ånetune the model on the536 downstream cross - lingual summarization task . 
Furthermore , based on a shared multi - lingual vocabulary , our model has a shared encoder - decoder architecture for all pre - training and Ô¨Ånetuning tasks , whereas NCLS ( Zhu et al . 
, 2019b ) sets aside taskspeciÔ¨Åc decoders for machine translation , monolingual summarization , and cross - lingual summarization . 
In the experiments , our model outperforms various baseline systems on the benchmark dataset NCLS ( Zhu et al . 
, 2019b ) . 
For example , our model achieves 3.27 higher ROUGE-1 score in Chinese to English summarization than the state - of - the - art result and 1.28 higher ROUGE-1 score in English to Chinese summarization . 
We further conduct an ablation study to show that each pretraining task contributes to the performance , especially our proposed unsupervised pretraining tasks . 
2 Related Work 2.1 Pre - training Pre - training language models ( Devlin et al . 
, 2018 ; Dong et al . 
, 2019 ) have been widely used in NLP applications such as question answering ( Zhu et al . 
, 2018 ) , sentiment analysis ( Peters et al . 
, 2018 ) , and summarization ( Zhu et al . 
, 2019a ; Yang et al . 
, 2020 ) . 
In multi - lingual scenarios , recent works take input from multiple languages and shows great improvements on cross - lingual classiÔ¨Åcation ( Lample and Conneau , 2019 ; Pires et al . 
, 2019 ; Huang et al . 
, 2019 ) and unsupervised machine translation ( Liu et al . 
, 2020 ) . 
Artetxe and Schwenk ( 2019 ) employs the sequence encoder from a machine translation model to produce cross - lingual sentence embeddings . 
Chi et al . 
( 2019 ) uses multi - lingual pre - training to improve cross - lingual question generation and zero - shot cross - lingual summarization . 
Their model trained on articles and summaries in one language is directly used to produce summaries for articles in another language , which is different from our task of producing summaries of one language for an article from a foreign language . 
2.2 Cross - lingual Summarization Early literatures on cross - lingual summarization focus on the two - step approach involving machine translation and summarization ( Leuski et al . 
, 2003 ; Wan et al . 
, 2010 ) , which often suffer from error propagation issues due to the imperfect modular systems . 
Recent end - to - end deep learning models have greatly enhanced the performance . 
Shen et al.(2018 ) presents a solution to zero - shot cross - lingual headline generation by using machine translation and summarization datasets . 
Duan et al . 
( 2019 ) leverages monolingual abstractive summarization to achieve zero - shot cross - lingual abstractive sentence summarization . 
NCLS ( Zhu et al . 
, 2019b ) proposes a cross - lingual summarization system for large - scale datasets for the Ô¨Årst time . 
It uses multitask supervised learning and shares the encoder for monolingual summarization , cross - lingual summarization , and machine translation . 
However , each of these tasks requires a separate decoder . 
In comparison , our model shares the entire encoder - decoder architecture among all pre - training and Ô¨Ånetuning tasks , and leverages unlabeled data for monolingual masked language model training . 
A concurrent work by Zhu et al . 
( 2020 ) improves the performance by combining the neural model with an external probabilistic bilingual lexicon . 
3 Method 3.1 Pre - training Objectives We propose a set of multi - task pre - training objectives on both monolingual and cross - lingual corpus . 
For monolingual corpus , we use the masked language model ( MLM ) from Raffel et al . 
( 2019 ) . 
The input is the original sentence masked by sentinel tokens , and the target is the sequence consists of each sentinel token followed by the corresponding masked token . 
The other monolingual task is the denoising auto - encoder ( DAE ) , where the corrupted input is constructed by randomly dropping , masking , and shufÔ¨Çing a sentence and the target is the original sentence . 
Since our Ô¨Ånal task is summarization , we also include monolingual summarization ( MS ) as a pre - training task . 
To leverage cross - lingual parallel corpus , we introduce the cross - lingual masked language model ( CMLM ) . 
CMLM is an extension of MLM on the parallel corpus . 
The input is the concatenation of a sentence in language A and its translation in language B. We then randomly select one sentence and mask some of its tokens by sentinels . 
The target is to predict the masked tokens in the same way as MLM . 
Different from MLM , the masked tokens in CMLM are predicted not only from the context within the same language but also from their translations in another language , which encourages the model to learn language - invariant representations . 
Note that CMLM is similar to the Translation Language Model ( TLM ) loss proposed in Lample537 Objective Supervised Multi - lingual Inputs Targets Masked Language Model France < X > Morocco in < Y > exhibition match . 
< X > beats < Y > an Denoising Auto - Encoder France beats < M > in < M > exhibition . 
France beats Morocco in an exhibition match . 
Monolingual Summarization /checkWorld champion France overcame a stuttering start to beat Morocco 1 - 0 in a scrappy exhibition match on Wednesday night . 
France beats Morocco in an exhibition match . 
Cross - lingual MLM /check /checkFrance < X > Morocco in < Y > exhibition match . 
Ê≥ïÂõΩÈòüÂú®‰∏ÄÂú∫Ë°®ÊºîËµõ‰∏≠ÂáªË¥•Êë©Ê¥õÂì•Èòü„ÄÇ < X > beats < Y > an Cross - lingual MLM /check /checkFrance beats Morocco in an exhibition match . 
< X > ÈòüÂú®‰∏ÄÂú∫Ë°®ÊºîËµõ‰∏≠ < Y > Êë©Ê¥õÂì•Èòü„ÄÇ < X > Ê≥ïÂõΩ < Y > ÂáªË¥• Machine Translation /check /check France beats Morocco in an exhibition match . 
Ê≥ïÂõΩÈòüÂú®‰∏ÄÂú∫Ë°®ÊºîËµõ‰∏≠ÂáªË¥•Êë©Ê¥õÂì•Èòü „ÄÇ Table 1 : Examples of inputs and targets used by different objectives for the sentence ‚Äú France beats Morocco in an exhibition match ‚Äù with its Chinese translation . 
We use < X > and < Y > to denote sentinel tokens and < M > to denote shared mask tokens . 
and Conneau ( 2019 ) . 
The key differences are : 1 ) TLM randomly masks tokens in sentences from both languages , while CMLM only masks tokens from one language ; 2 ) TLM is applied on encoderonly networks while we employ CMLM on the encoder - decoder network . 
In addition to CMLM , we also include standard machine translation ( MT ) objective , in which the input and output are the unchanged source and target sentences , respectively . 
The examples of inputs and targets used by our pre - training objectives are shown in Table 1 . 
3.2 UniÔ¨Åed Model for Pre - training and Finetuning While NCLS ( Zhu et al . 
, 2019b ) uses different decoders for various pre - training objectives , we employ a uniÔ¨Åed Transformer ( Vaswani et al . 
, 2017 ) encoder - decoder model for all pre - training and Ô¨Ånetuning tasks . 
This makes our model learn a crosslingual representation efÔ¨Åciently . 
A shared dictionary across all languages is used . 
To accommodate multi - task and multilingual objectives , we introduce language i d symbols to indicate the target language , and task symbols to indicate the target task . 
For instance , for the CMLM objective where the target language is Chinese , the decoder takes < cmlm > and < zh > as the Ô¨Årst two input tokens . 
We empirically Ô¨Ånd that our model does not suffer from the phenomenon of forgetting target language controllability as in Chi et al . 
( 2019 ) , which requires manual freezing of encoder or decoder during Ô¨Ånetuning . 
After pretraining , we conduct Ô¨Ånetuning on cross - lingual summarization data.4 Experiments 4.1 Dataset We conduct our experiment on NCLS dataset ( Zhu et al . 
, 2019b ) , which contains paired data of English articles with Chinese summaries , and Chinese articles with English summaries . 
The cross - lingual training data is automatically generated by a machine translation model . 
For Ô¨Ånetuning and testing , we followed the same train / valid / test split of the original dataset . 
We refer readers to Table 1 in Zhu et al . 
( 2019b ) for detailed statistics of the dataset . 
For pre - training , we obtain monolingual data for English and Chinese from the corresponding Wikipedia dump . 
There are 83 million sentences for English monolingual corpus and 20 million sentences for Chinese corpus . 
For parallel data between English and Chinese , we use the parallel corpus from Lample and Conneau ( 2019 ) , which contains 9.6 million paired sentences . 
For monolingual summarization objective , we use CNN / DailyMail dataset ( Nallapati et al . 
, 2016 ) for English summarization and LCSTS dataset ( Hu et al . 
, 2015 ) for Chinese summarization . 
4.2 Implementation Details Our transformer model has 6 layers and 8 heads in attention . 
The input and output dimensions dmodel for all transformer blocks are 512 and the inner dimensiondffis 2048 . 
We use a dropout probability of 0.1 on all layers . 
We build a shared SentencePiece ( Kudo and Richardson , 2018 ) vocabulary of size 33,000from a balanced mix of the monolingual Wikipedia corpus . 
The model has approximately 61 M parameters . 
For MLM we use a mask probability of 0.15 . 
For DAE , we set both the mask and drop out rate538 English ‚ÜíChinese Chinese ‚ÜíEnglish ROUGE-1 ROUGE-2 ROUGE - L ROUGE-1 ROUGE-2 ROUGE - L TETran 26.15 10.60 23.24 23.09 7.33 18.74 GETran 28.19 11.40 25.77 24.34 9.14 20.13 TLTran 30.22 12.20 27.04 33.92 15.81 29.86 GLTran 32.17 13.85 29.43 35.45 16.86 31.28 NCLS 36.82 18.72 33.20 38.85 21.93 35.05 NCLS - MS 38.25 20.20 34.76 40.34 22.65 36.39 NCLS - MT 40.23 22.32 36.59 40.25 22.58 36.21 XNLG 39.85 24.47 28.28 38.34 19.65 33.66 ATS 40.68 24.12 36.97 40.47 22.21 36.89 Ours 43.50 25.41 29.66 41.62 23.35 37.26 Table 2 : ROUGE-1 , ROUGE-2 , ROUGE - L for English to Chinese and Chinese to English summarization on NCLS dataset . 
to 0.1 . 
For all pre - training and Ô¨Ånetuning we use RAdam optimizer ( Liu et al . 
, 2019 ) with Œ≤1= 0.9 , Œ≤2= 0.999 . 
The initial learning rate is set to 10‚àí9for pre - training and 10‚àí4for Ô¨Ånetuning . 
The learning rate is linearly increased to 0.001with 16,000warmup steps followed by an exponential decay . 
For decoding , we use a beam size of 6 and a maximum generation length of 200 tokens for all experiments . 
English ‚ÜíChinese ROUGE-1 ROUGE-2 ROUGE - L Ours 43.50 25.41 29.66 - MS 42.48 24.45 28.49 - MT 42.12 23.97 28.74 - MLM , DAE 41.82 23.85 28.40 - All Pretraining 41.12 23.67 28.53 Table 3 : Finetuning performance on English ‚ÜíChinese summarization starting with various ablated pre - trained models . 
4.3 Baselines We Ô¨Årst include a set of pipeline methods from Zhu et al . 
( 2019b ) which combines monolingual summarization and machine translation . 
TETran Ô¨Årst translates the source document and then uses LexRank ( Erkan and Radev , 2004 ) to summarize the translated document . 
TLTran Ô¨Årst summarizes the source document and then translates the summary . 
GETran andGLTran replace the translation model in TETran and TLTran with Google Translator1respectively . 
We also include three strong baselines from Zhu et al . 
( 2019b ): NCLS , NCLS - MS andNCLS - MT . 
1https://translate.google.com/NCLS trains a standard Transformer model on the cross - lingual summarization dataset . 
NCLS - MS and NCLS - MT both use one encoder and multiple decoders for multi - task scenarios . 
NCLS - MS combines the cross - lingual summarization task with monolingual summarization while NCLS - MT combines it with machine translation . 
We Ô¨Ånetune XNLG model from Chi et al . 
( 2019 ) on the same cross - lingual summarization data . 
We Ô¨Ånetune all layers of XNLG in the same way as our pretrained model . 
Finally , we include the result of ATS from the concurrent work of Zhu et al . 
( 2020 ) . 
4.4 Results Table 2 shows the ROUGE scores of generated summaries in English - to - Chinese and Chinese - toEnglish summarization . 
As shown , pipeline models , although incorporating state - of - the - art machine translation systems , achieve sub - optimal performance in both directions , proving the advantages of end - to - end models . 
Our model outperforms all baseline models in all metrics except for ROUGE - L in English - toChinese . 
For instance , our model achieves 2.82 higher ROUGE-1 score in Chinese to English summarization than the previously best result and 1.15 higher ROUGE-1 score in English to Chinese summarization , which shows the effectiveness of utilizing multilingual and multi - task data to improve cross - lingual summarization . 
4.5 Ablation Study Table 3 shows the ablation study of our model on English to Chinese summarization . 
We remove539 0.005.0010.0015.0020.0025.0030.0035.0040.0045.0050.00 1 K 10 K 364 K ( Full ) Ours Ours w/o Pretraining # Train Data ( a ) English ‚ÜíChinese ROUGE-1 0.005.0010.0015.0020.0025.0030.0035.0040.0045.00 1 K 10 K 1693 K ( Full ) Ours Ours w/o Pretraining # Train Data ( b ) Chinese ‚ÜíEnglish ROUGE-1 Figure 1 : ROUGE-1 performance on NCLS dataset when the cross - lingual summarization training data is subsampled to size of 1k and 10k . 
The result on the full dataset is also shown . 
from the pre - training objectives i ) all monolingual unsupervised tasks ( MLM , DAE ) , ii ) machine translation ( MT ) , iii ) monolingual summarization ( MS ) , and iv ) all the objectives . 
Note that ‚Äù - All Pretraining ‚Äù and NCLS both only train on the cross - lingual summarization data . 
The performance difference between the two is most likely due to the difference in model size , vocabulary , and other hyperparameters . 
As shown , the pre - training can improve ROUGE1 , ROUGE-2 , and ROUGE - L by 2.38 , 1.74 , and 1.13 points respectively on Chinese - to - English summarization . 
Moreover , all pre - training objectives have various degrees of contribution to the results , and the monolingual unsupervised objectives ( MLM and DAE ) are relatively the most important . 
This veriÔ¨Åes the effectiveness of leveraging unsupervised data in the pre - training . 
Low - resource scenario . 
We sample subsets of size 1 K and 10 K from the training data of crosslingual summarization and Ô¨Ånetune our pre - trained model on those subsets . 
Figure 1 shows the the performance of the pre - trained model and the model trained from scratch on the same subsets . 
As shown , the gain from pre - training is larger when the size of training data is relatively small . 
This proves the effectiveness of our approach to deal with low - resource language in cross - lingual summarization . 
5 Conclusion We present a mix - lingual pre - training model for cross - lingual summarization . 
We optimize a shared encoder - decoder architecture for multi - lingual and multi - task objectives . 
Experiments on a benchmarkdataset show that our model outperforms pipelinebased and other end - to - end baselines . 
Through an ablation study , we show that all pretraining objectives contribute to the model ‚Äôs performance . 
Abstract Point - of - Interest ( POI ) oriented question answering ( QA ) aims to return a list of POIs given a question issued by a user . 
Recent advances in intelligent virtual assistants have opened the possibility of engaging the client software more actively in the provision of location - based services , thereby showing great promise for automatic POI retrieval . 
Some existing QA methods can be adopted on this task such as QA similarity calculation and semantic parsing using pre - deÔ¨Åned rules . 
The returned results , however , are subject to inherent limitations due to the lack of the ability for handling some important POI related information , including tags , location entities , and proximityrelated terms ( e.g. ‚Äú nearby ‚Äù , ‚Äú close ‚Äù ) . 
In this paper , we present a novel deep learning framework integrated with joint inference to capture both tag semantic and geographic correlation between question and POIs . 
One characteristic of our model is to propose a special cross attention question embedding neural network structure to obtain question - to - POI and POI - to - question information . 
Besides , we utilize a skewed distribution to simulate the spatial relationship between questions and POIs . 
By measuring the results offered by the model against existing methods , we demonstrate its robustness and practicability , and supplement our conclusions with empirical evidence . 
1 Introduction Point - of - Interest ( POI ) oriented question answering ( QA ) problem is a special QA task which aims to answer users ‚Äô questions by generating a list of POIs . 
With the rapid development of smart agents ( e.g. Amazon Echo ) and intelligent virtual assistants ( e.g. Apple Siri and Google Assistant ) , there are many POI oriented queries being requested everyday . 
Some examples of these questions are ‚Äú Where can I take my kid to have fun nearby New York City ‚Äù or ‚Äú Where can we go in LA with my friends ‚Äù . 
The answers are typically a list of POIssuch as parks , malls , or restaurants corresponding to the details provided by users in the questions . 
According to some statistics , there are millions of POI oriented QA questions being requested per day on a mobile search engine in China . 
Generally speaking , semantic parsing and similarity matching methods are utilized to tackle the POI oriented QA problem in current solutions . 
Nevertheless , both of them are subject to inherent limitations and deserve to be improved . 
Semantic parsing based methods convert the questions to formal representations ( such as SQL queries ) using pre - deÔ¨Åned rules , then get the POI results from the query . 
DifÔ¨Åculties arise , however , when the form of the questions varies from person to person . 
Besides , due to ambiguous expressions of a speciÔ¨Åc tag , semantic parsing methods always fail to match mentioned tags to the POI database . 
Furthermore , based only on tag information , it is almost impossible for semantic parsing methods to make use of the distance correlation between questions and POIs . 
Another line of solution is adopting similarity matching models for calculating the similarity score between questions and POIs . 
Recent years have witnessed rapid growth in various kinds of semantic similarity based QA systems such as Convolutional Neural Network Architecture ( Hu et al . 
, 2014 ) , LSTM Based Answer Selection ( Tan et al . 
, 2015 , 2016 ) , and Cross - Attention Based Question Answering System ( Hao et al . 
, 2017 ) . 
Despite the success in common landscapes , most existing studies of this family can not work well for POI oriented QA , since it is ineffective for them to handle the unique properties of POI elements such as tags and locations . 
As a result , a signiÔ¨Åcant gap remains between academic proposals and the industry standard of implementing location based services . 
It is nontrivial to extend existing QA models to handle the challenges of POI oriented QA . 
In general , the unique challenges for this problem mainly542 Figure 1 : The overall architecture of our model . 
Generally , the model is made up of two parts , namely tag semantic moduleptand distance correlation module pd . 
The model takes the Question - POI pair as input , and the probability of choosing a POI given the question as output . 
come from two aspects . 
First of all , when asking POI oriented questions , people tend to emphasize certain needs , which correspond to some POI properties , such as the popular users of the POI , the service provided by the POI , and the types of the POI , etc . 
Hereafter we name all such POI properties as tags . 
Identifying such information in the question that is related to the tags of POI is crucial in this task , thus creating a bottleneck . 
Take the question ‚Äú Where can children go nearby New York City ‚Äù as an example , the word ‚Äú children ‚Äù , being regarded as both a question term and a POI tag , plays an important role in identifying the corresponding POIs . 
Second , proximity - related terms such as ‚Äú nearby ‚Äù and ‚Äú close ‚Äù deserve special treatment . 
Considering the same example , if there is ‚Äú nearby ‚Äù in the question , the candidate POIs should be mainly located outside New York City ; whereas if without , the candidate POIs should be within New York City . 
Furthermore , for different location entities such as ‚Äú nearby New York City ‚Äù v.s. ‚Äú nearby Manhattan ‚Äù , the distance scopes of ‚Äú nearby ‚Äù are also different . 
In contrast , traditional QA methods are not able to treat these terms in their models properly and thus leading to a poor performance on POI oriented QA . 
In this paper , we propose a POI oriented QA model with JointInference ( named as PJI for short ) to tackle the challenges mentioned before . 
PJI mainly has two modules which are named as tag semantic module and distance correlation module . 
The tag semantic module is used to automatically search for relevant POIs based on semantic tag in - formation . 
Besides , in order to capture speciÔ¨Åc patterns buried in questions and POIs , we develop a novel cross attention based question embedding structure . 
Therefore the mutual inÔ¨Çuence between questions and POIs is taken into account . 
In the distance correlation module , we adopt a skewed distribution on three - level locations including city , district , Area of Interest ( AOI ) to Ô¨Åt the distance distribution between candidate POIs and mentioned location terms in the question . 
Both modules are fused together and optimized in an end - to - end manner for retrieving the Ô¨Ånal POI list . 
Our major contributions can be summarized as follows : ‚Ä¢We tackle the POI oriented QA problem by proposing a new deep learning model with joint inference . 
‚Ä¢We leverage two neural network modules to build a bridge between questions and POIs on both POI tags and question location terms . 
We also adopt a skewed distribution method to deal with proximity - related terms . 
‚Ä¢We design a special embedding structure using cross attention mechanism to obtain a more precise and Ô¨Çexible representation of questions . 
‚Ä¢We conduct comprehensive experiments on two real - world datasets enabling the evaluation of the results from different perspectives . 
Experimental results demonstrate signiÔ¨Åcant improvements of PJI over all the state - of - theart baselines.543 2 Related Work QA with Semantic Parser Semantic parsing shines at handling complex linguistic constructions and obtains reasonable performance on question answering problems . 
Traditionally , semantic parsers like AMR ( Banarescu et al . 
, 2012 ) and SQL ( Androutsopoulos et al . 
, 1995 ) map sentences to formal representations of their underlying meaning ( Shen and Lapata , 2007 ; Yao et al . 
, 2014 ; Hill et al . 
, 2015 ; Talmor et al . 
, 2017 ) . 
By leveraging a knowledge base , semantic parsing is reduced to query graph generation and stage searching . 
Neural Approaches for QA With the recent development in deep learning , neural networks have achieved great success in question answer problems ( Salakhutdinov and Hinton , 2009 ; Collobert et al . 
, 2011 ; Socher et al . 
, 2012 ; Hu et al . 
, 2014 ; Tan et al . 
, 2015 , 2016 ) . 
Most of these models use a deep neural network like GRU ( Chung et al . 
, 2014 ) and LSTM ( Hochreiter and Schmidhuber , 1997 ) to handle the long texts required for QA . 
Further improvements like attention mechanism are applied to focus on the most relevant facts ( Hao et al . 
, 2017 ; Zhao et al . 
, 2019 ) . 
The relevance score of each QA pair is the cosine similarity of the semantic vectors . 
The Ô¨Ånal answers to each question are then sorted by the similarity score . 
Probabilistic Deep Learning Models The base of probabilistic deep learning models is to use the neural network as a conditional model parameterised by the weights in the network when some inputs are given . 
The output is obtained by optimizing the parameters in the model with the estimates provided by Bayesian framework . 
Several probabilistic models have been used in tasks like question answering with knowledge graph and link prediction ( Wang et al . 
, 2007 , 2014 ; Zhang et al . 
, 2018 ) . 
The main advantage of this complete separation of the neural network from Bayesian model is that the good features generated by the network are well used to make predictions , which gives the model high Ô¨Çexibility and accuracy . 
3 Our Model 3.1 Preliminaries Point of interest ( POI ) is a dedicated geographic entity on an online map where someone may Ô¨Ånd useful information , like a restaurant , a hotel , or a travel spot . 
Compared with the common entities inknowledge graph , POI has two important properties which are tags and location . 
Tags refer to a short text ( one or several words ) in a POI describing its service ( e.g. fast food or entertainment ) , its major users ( e.g. kids or lovers ) , its types ( e.g. restaurants or shopping mal ) , etc . 
Users are greatly facilitated by informative tags when searching for the POIs . 
In addition , each POI has three location properties named as location entities recording the POI located city , district , and area of interest ( AOI ) . 
Here AOI refers to a polygonal area in a 2D map which usually contains several POIs , New York Central Park for example . 
For each location entity , it is possible to Ô¨Ånd a set of POIs within the entity . 
Given a question q , the POI oriented question answering seeks to parse the question , then return a set of POIs which can be seen as the answer result according to the question . 
For example when qis the question ‚Äú Where can children go on weekend in New York City ? ‚Äù , the answer is a set of POIs which are places for kids to play in New York City satisfying the information request conveyed by the user . 
It is possible to further rank the POIs according to some POI recommendation algorithms but it is beyond the scope of this paper . 
3.2 Model Overview In our framework , the dataset is a set of question - POI pairs which can be represented as D = { qi , ai}N i=1 , whereqirefers to a question , ai refers to a POI answering the question . 
Our model PJI aims to retrieve the correct POIs with respect to each question which corresponds to the function P(ai|qj)returning the probability that POI aisatisÔ¨Åes the question qj . 
The overall structure of our model is illustrated in Fig 1 . 
Our model consists of two neural network modules , as described below : Tag Semantic Module In the POI oriented QA , some question terms correspond to POI tags and thus serving as a bridge between questions and POIs . 
In Fig 1 , ‚Äú children ‚Äù is both a term in the question and a tag of POI . 
However , it is not easy to match the query terms to POI tags directly . 
For example , terms such as ‚Äú kids ‚Äù , ‚Äú baby ‚Äù can also correspond to the tag ‚Äú children ‚Äù . 
In our model , for each question qj , we learn the probability that a tagyais included in the question qj , which can be represented as P(ya|qj ) . 
Given the question embedding and tags , POIs with the corresponding tags are chosen as the answers at the tag level . 
We then develop a neural network module specialized for544 calculatingP(ai|ya , qj ) , which is the likelihood of POIaibeing selected given the tag yaand the questionqj . 
Above all , the likelihood of choosing POI aias the answer to the question qjis the marginal probability mass function over all tags : pt(ai|qj , Œ∏t ) = /summationdisplay ya‚ààVtP(ai|ya , qj)‚àóP(ya|qj)(1 ) which sums out all possibilities of tag variables , whereVtrefers to the tag set . 
Distance Correlation Module Apart from tags , there also exists a distance correlation between questions and POIs . 
In this module , we Ô¨Årst extract the location entity using NER tools , since the location entity vocabulary is very large and has Ô¨Åxed names . 
The questions usually contain some proximity - related terms , such as ‚Äú nearby ‚Äù and ‚Äú close to ‚Äù . 
It is hard to conÔ¨Ådently determine whether a POI is in or out an extracted location entity polygon considering such proximity - related terms . 
Thus , instead of directly identifying the candidate POIs by the location entity appeared in the question , we also calculate the probability of candidate POIs considering both the distance to the extracted entity polygon and the question context . 
With this motivation , we introduce another probability function P(ai|yl , qj ) , which captures the probability of POI aibeing the answer of the questionqjif the location entity ylappears inqj . 
We denote the likelihood of choosing POI aigiven the question qjbased on distance correlation as : pd(ai|qj , Œ∏d ) = /summationdisplay yl‚ààVdP(ai|yl , qj)‚àóP(yl|qj)(2 ) whereP(yl|qj ) = 1 ifylappears inqj , otherwise P(yl|qj ) = 0 , yl‚ààVdandVdrefers to the location entity set . 
Overall Formulation With the two modules above , the parameters of the function p(ai|qj)can be estimated by maximizing the log - likelihood as follows : max Œ∏t , Œ∏d(1 NN / summationdisplay i=1logpt+1 NN / summationdisplay i=1logpd ) ( 3 ) 3.3 Neural Network Module for Tag Semantic Matching Due to the linguistic diversity of describing a certain tag , it is almost impossible to recognize the tag with exact matching . 
Therefore , we build atag recognizer which can be jointly trained with the model . 
After that , we can get the POIs given the tag and question representations with a cross attention architecture . 
QA Embedding We use two dense ddimensional vector representations of questions in the module . 
The Ô¨Årst one is represented as fent ( ¬∑ ) : q‚ÜíRd , which takes the Word2Vec vectors as input , then feeds them into a Bi LSTM neural network with a pooling layer . 
It helps to capture the sequence information in the question and is used in POI tag recognition . 
The other one is denoted as fpr ( ¬∑ ) : q‚ÜíRd , which leverages attention mechanism to distinguish and catch the most important information in questions . 
Rather than apply a simple attention layer , we introduce a special cross attention mechanism tailored to this task originally Ô¨Årst brought by Hao et al . 
( 2017 ) . 
The answer POI is embedded with function g ( ¬∑ ) : a‚ÜíRd , which calculates the average value of POI tag vectors obtained from Word2Vec . 
Cross Attention Mechanism Similar tofent , the structure fprconsists of a Bi LSTM network with a pooling layer , whereas the output of it interacts with the POI representation and takes the attention weights into account . 
The Ô¨Ånal attentive embedding consists of POI - towards - question embedding and question - towards - POI embedding . 
In POI - towards - question step , we train weights between every state in the Bi LSTM hidden layer and POI tag , then get a set of weighted question vectors regarding each POI tag . 
The following formulas are proposed to calculate the vectors : Œ±mn = softmax ( h(WT[hn;em ] + b ) ) ( 4 ) fpr(q)m=/summationdisplay nŒ±mnhn ( 5 ) wherehndenotes the question hidden layer vector . 
emdenotes the POI tag embedding vector . 
Œ±mnis the weight of attention from the tag emto thenth word in the question . 
h(¬∑)is an activation function . 
In question - towards - POI step , we learn a set of weights between the question pooling layer vector and POI tag . 
Using the weighted question vectors in the Ô¨Årst step and the weights in the second step , we can then get the Ô¨Ånal weighted double - sided attentive question vector by multiplying and adding them up . 
fpr(q ) = /summationdisplay mŒ≤mfpr(q)m ( 6)545 Œ≤m = softmax ( h(WT[fent(q);em ] + b))(7 ) whereŒ≤mdenotes the attention of question towards answer aspects . 
POI Tag Recognition We exploit the question context to build the tag recognizer . 
For instance , if the question contains the word ‚Äú dating ‚Äù , it means that the target audience is lovers and the POI type should be like parks and restaurants . 
SpeciÔ¨Åcally , we embed the question to a ddimensional vector using embedding function fent ( ¬∑ ) : q‚ÜíRdas described above . 
Then given the embedding vector of the question q , we set the likelihood of choosing tagyaby adding a softmax layer as follows : P(ya|q ) = softmax ( WT yfent(q ) ) ( 8) = exp(WT yfent(q))/summationtext y / prime‚ààVtexp(WT y / primefent(q))(9 ) whereVtrefers to the tag set in the POI dataset . 
Tag Based POI Retrieval Since the number of POIs in the dataset is often very large , it is necessary to obtain some candidate POIs based on tag information and discard the irrelevant ones . 
Having P(ya|q ) , we can get POI tag yawith the highest score . 
We then Ô¨Ålter out POIs with the tag yafrom the dataset and form the candidate set . 
Precisely , we introduce a Dirac delta function /epsilon1to accomplish this process . 
For POIs with ya , the function /epsilon1ya(a ) is set to 1 , while for POIs without , /epsilon1ya(a)is set to 0 . 
The Ô¨Åltering of POI greatly reduces the workload of subsequent process , and has a signiÔ¨Åcant effect for large - scale data . 
After obtaining the tag yain the question qj and the function /epsilon1 , the next step is to retrieve the corresponding POIs , which is represented as P(ai|ya , qj)in Section 3.2 . 
Suppose questions are embedded using the embedding function fpr ( ¬∑ ) : q‚ÜíRd . 
In this function , the Ô¨Ånal output question embedding is the weighted cross - attentive vectors where informative patterns in questions are strongly focused . 
The likelihood of choosing ai given question answer embedding and POI tag can be represented as follows : P(ai|ya , q ) = sigmoid ( fpr(q)Tg(ai))¬∑/epsilon1ya(ai ) ( 10)3.4 Neural Network Module for Distance Correlation This section mainly discusses the approach for matching the POIs to the question based on the aspect of distance correlation . 
As discussed in Section 3.2 , the Ô¨Årst step of distance correlation module is to Ô¨Ånd location entities in the question . 
In our model , we assume that there are three types of location entities : city , district , AOI ( Area of Interest ) . 
AOI is a location entity on the map with boundaries ( e.g. Central Park ) which usually belongs to a district ( e.g. Manhattan ) of a city ( e.g. New York ) . 
We Ô¨Årst build a dictionary storing all of the location entities and their corresponding scopes as well as types . 
For every question , we extract the location terms in the question with an NER ( named entity recognition ) tool before mapping them onto the dictionary . 
Note that the location term extracted directly from questions can be hierarchical . 
For example , AOIs may appear in the form of District+AOI ( e.g. Manhattan Central Park ) or City+AOI ( New York Central Park ) or City+District+AOI ( New York Manhattan Central Park ) or just itself ( Central Park ) . 
Thus , we set the priority order to AOI > district > city when conducting entity mapping . 
Proximity - related Terms While retrieving the POIs according the location entity , another factor we should consider is whether the question contains some proximity - related terms such as ‚Äú nearby ‚Äù , ‚Äú close to ‚Äù , or ‚Äú neighboring ‚Äù . 
When these terms appear in the question , people are actually expecting POIs which are close to , or outside the location boarder . 
It implies that the model should avoid simply returning POIs within the location polygon . 
Fig 2(a ) shows the real - data distribution of POI with respect to questions with and without proximityrelated terms according to real - world data used in our experiments . 
In addition , concerning questions with proximity - related terms , the area of the location entity also has an important impact on the probability distribution of the distance between the selected POI coordinate and location entity polygon . 
As shown in Fig 2(b ) , when asking city - level questions with proximity - related terms ( e.g. ‚Äú Where can children go nearby New York ? ‚Äù ) , the result may contain POIs located in city suburban district or outside the city ; while as for AOI - level questions ( e.g. ‚Äú Where can children go nearby Central Park ? ‚Äù ) , the result may only contain POIs outside546 ( a )   ( b ) Figure 2 : The POI probability distribution concerning distance . 
X axis is the log distance between the POI and the location entity polygon . 
If POI is outside the polygon , the distance is positive , otherwise is negative . 
Y axis is the probability the POI is recommended . 
( PRT denotes proximity - related terms . 
but close to the border of the AOI . 
This is because the area of a city is much bigger than that of an AOI . 
With different area sizes of the location entity , the probability distribution functions are quite different . 
Distance Correlation Calculation The probability of choosing POI aigiven the location entity in questionqjhas a proportional relationship with the distance between the POI and the location entity polygon . 
That is , if a POI is very far away from the expected location , the probability we recommend it is close to zero . 
Apart from the distance , as discussed above , proximity - related terms and location entity areas should also be taken into account when calculating the likelihood . 
Given the location entity ylextracted from the questionqj , all factors , including the distance between the polygon of yland POIai , the area size ofyland proximity - related terms , have an impact on the likelihood distribution . 
SpeciÔ¨Åcally , we propose a skewed distribution based model , which takes the distance from POI to the location entity d(ai , yl ) , indicator function œÑ(qj ) , as well as the area of location entity s(yl)as inputs.œÑ(qj)is the indicator function that œÑ(qj ) = ‚àí1if the question contains proximity - related terms , otherwise œÑ(qj ) = 1 . 
The probability of choosing POI ai having the location entity yland the question qjis : P(ai|yl , qj ) = sigmoid ( Wdf(œÑ(qj)d(ai , yl ) sy ) ) ( 11 ) f(x ) = 2 œâœÜ(x‚àíŒæ œâ)Œ¶(Œ±x‚àíŒæ œâ ) ( 12)œÜ(x ) = 1‚àö 2œÄe‚àíx2 2 ( 13 ) Œ¶(x ) = /integraldisplayx ‚àí‚àûœÜ(t)dt=1 2(1 + erf(x‚àö 2))(14 ) Wheref(x)is the skewed normal distribution of x , Wdis what we want to optimize . 
Note that Œ± , Œæ , œâare hyper parameters . 
Given the formulation above , we can see if the questions do not contain proximity - related terms , œÑ(qj)value is equal to 1 , POIs inside the polygon scope are what we need . 
As for questions containing proximityrelated terms , the smaller polygon area syis , the steeper the distribution curve will be , as a result , POIs closer to the polygon boundary will be more likely to be selected . 
3.5 Inference During inference , ideally we want to Ô¨Ånd the candidate POIs given the question qj . 
In the aspect of POI tags , we select the tag yareceiving the maximum score from P(ya|qj ) . 
Then we reduce the candidate POI number by Ô¨Åltering out the POIs whose corresponding tag is equal to ya . 
After that , we calculate the semantic probability of choosing the POI as the answer . 
In distance correlation stage , the computation is quadratic in the number of location entities and thus is too expensive . 
We Ô¨Årst extract the location entity ylby NER and calculate the distance from POI coordinates to the polygon ofylafterwards . 
Take the question in Fig 1 as an example , after the extraction step , we obtain the location entity ‚Äú New York City ‚Äù . 
Finally , we select the top 5 candidate POIs with top scores as the result . 
4 Experiments 4.1 Experiment Setup Datasets We construct two large - scale datasets , both of which are based on queries extracted from query logs of a widely used mobile search engine App and POIs obtained from an online map service provider . 
In order to Ô¨Ålter the POI related questions from the search engine App , we design a set of templates such as ‚Äú where can [ * ] go in [ * ] ‚Äù and keep all the queries that match with the templates . 
To construct the ground truth of question - POI pairs used in the training period , given questions satisfying the templates , we crawl the related website clicked547 by the user inquiring the question , then calculate the similarity between the website text and POIs . 
Finally we choose POIs that are most similar to the website as the answer POI . 
For determining the answer POIs , we sort the POIs according to their probability and choose the top - K result as the Ô¨Ånal output . 
Moreover , all the datasets are anonymized due to privacy concerns . 
‚Ä¢Dataset A. This dataset mainly contains POI related questions whose geographic entities are located in Beijing . 
We sample questions out of one month records satisfying the template , and construct 11,000 question - POI pairs . 
The question data is divided into two parts randomly . 
The training set contains 10,900 question - POI pairs . 
The testing set is made up of 100 questions which do not appear in the training set . 
‚Ä¢Dataset B. In this dataset , the location of the questions is not restricted in Beijing . 
We randomly sample questions to construct the question - POI pairs which covers most of the cities and many popular visited districts and AOIs in China . 
Similarly , there are 350,900 question - POI pairs in the training set , and 100 questions in the testing set . 
On average , the length of questions in 2 datasets is 37.8 Chinese characters . 
The average length of POIs including its tag information ( name , tags , city , district and AOI ) is 30.3 characters . 
We later evaluate our model on these two datasets by the percent of hits at K ( % hits@K ) which is the percent of question - POI pairs whose POI appears in top - K retrieved POI . 
Baselines We compare our model with several state - of - the - art baselines to show the effectiveness of our model . 
The Ô¨Årst two are semantic parser based methods using tag information and the left ones are deep learning methods based on semantic matching . 
‚Ä¢Template Matching Method ( TMM ) This method Ô¨Årst converts the questions into SQL queries according to the templates , then retrieves POIs from database . 
‚Ä¢StanfordCoreNLP Stanford CoreNLP is an integrated NLP toolkit providing a wide range of linguistic analysis tools . 
We use it as a Chinese semantic parser to recognize the tags . 
Based on the tool , we can turn the question into SQL queries according to the semantic characteristics of the tags . 
‚Ä¢Bi - LSTM It is a basic deep neural network model which takes the Word2Vec vectors of query and answer as input and their cosine similarity as output ( Tan et al . 
, 2015 ) . 
It utilizes a Bi - LSTM layer to capture question semantic features and then feed them into a pooling layer . 
This model takes the max margin hinge loss as the loss function . 
‚Ä¢Bi - LSTM+ATT ( AQA ) Compared with BiLSTM , in this model , each Bi - LSTM output vector will be multiplied by a softmax weight , which is determined by the answer embedding . 
‚Ä¢Bi - LSTM+C - ATT ( CAQA ) This is a state - ofthe - art end - to - end neural question answering model introduced by ( Hao et al . 
, 2017 ) . 
It considers the double - sided attention containing question - to - answer attention and answer - toquestion attention . 
4.2 Experiment Results Overall Performance We compare our model with all the baselines whose results are shown in Table 1 . 
Conclusions observed are listed as follows . 
( 1 ) Compared with typical neural network based models , semantic parsing based methods have a higher % hits@K rate on the whole . 
However , with the lack of Ô¨Çexibility , their % hits@K rate is worse than our PJI model . 
( 2 ) In general , models with attention mechanism reach better performance than models without . 
Bidirectional attention models achieve higher % hits@K rate than unidirectional one , which indicates there exists several parts in the questions as well as POI attributes that should be put emphasis on . 
( 3 ) Our model achieves the best overall performance among all the models . 
In terms of % hits@K rate , no matter what K is , the rate of our model is beyond 95 % . 
Our model utilizes several neural network modules instead of calculating the semantic similarity directly . 
Moreover , thanks to the cross - attention question embedding structure , our model puts strong emphasis on the distance and tag related patterns of both questions and POIs . 
In addition , we use a special probability distribution to handle questions with proximityrelated geographic terms which are treated the same as normal questions in the baselines.548 Dataset A Dataset B hits@1 hits@3 hits@5 hits@1 hits@3 hits@5 TMM 84.9 % 84.9 % 84.9 % 82.8 % 82.8 % 82.8 % CoreNLP 88.9 % 88.9 % 88.9 % 86.5 % 86.5 % 86.5 % Bi - LSTM 55.6 % 56.0 % 61.7 % 29.1 % 29.4 % 31.1 % AQA 65.2 % 67.3 % 68.8 % 37.5 % 38.9 % 35.6 % CAQA 69.2 % 56.1 % 68.1 % 42.1 % 42.8 % 49.0 % PJI 98.6 % 98.9 % 99.0 % 97.2 % 97.9 % 99.1 % Table 1 : The overall performance over two datasets . 
Three - level Location Performance . 
Table 2 shows the % hits@5 of two datasets where questions contain city , district and AOI location entities , respectively . 
As shown in the table , no matter which model we use , city level questions obtain the best result compared to other two types . 
The reason is that the number of cities in the whole nation is rather small and there is almost no duplicate city names among them . 
However , both AOI and district names can have a lot of duplications thus causing ambiguity and noise . 
Moreover , due to the hierarchical nature of the location entity , AOI names appear in different formats , which increases the difÔ¨Åculty of POI retrieval . 
Therefore , the template - based method and the end - to - end similarity matching method may be far from meeting the real - world demands of POI oriented QA . 
Despite the challenges we mentioned above , our model still outperforms all the baselines on city , district and AOI questions . 
Dataset A Dataset B Cit . 
Dis . 
AOI Cit . 
Dis . 
AOI TMM 94.1 % 92.0 % 91.3 % 93.2 % 91.6 % 90.2 % CoreNLP 96.0 % 93.4 % 58.3 % 90.3 % 90.1 % 31.8 % Bi - LSTM 92.1 % 76.2 % 8.4 % 90.9 % 62.1 % 3.5 % AQA 92.4 % 78.5 % 9.1 % 92.2 % 62.8 % 9.3 % CAQA 92.9 % 79.4 % 9.7 % 92.6 % 63.1 % 9.7 % PJI 100 % 99.6 % 97.3 % 99.8 % 99.2 % 97.0 % Table 2 : The % hits@5 rate on questions containing different location entities . 
4.3 Proximity - related Term Analysis Fig 3 shows the % hits@5 with and without proximity - related terms on Dataset A and B. From the result we can conclude that all existing baselines can not handle questions with proximityrelated geographic terms . 
For traditional neural network QA models , the model has no idea how important these words are and considers them just as normal words . 
As a result , the results returned do not make sense to the users . 
Nevertheless , this problem gets tackled by the distance probability module in our model . 
Therefore , our model outperforms the baselines when it comes to these kinds of problems to a great extent . 
( a ) Dataset A   ( b ) Dataset B Figure 3 : The % hits@5 rate concerning questions with and without proximity - related terms on two datasets . 
5 Conclusion In this paper , we propose a novel deep learning framework with joint inference to solve the POI oriented question answering task . 
Our main contributions lie in three aspects . 
First , this model handles the POI oriented QA with the help of tag semantic module and distance correlation module . 
Second , by introducing a cross attention based question embedding structure , we achieve a precise and Ô¨Çexible representation of questions . 
Third , the proposed model can overcome several challenges of POI oriented QA including POI tag recognition , proximity - related term processing and diverse distance correlation . 
Extensive experiments on two real - world datasets are carried out to demonstrate the effectiveness of our model . 
The result shows that our approach outperforms all the baselines and state - of - the - art models . 
Acknowledgments The work described in this paper is partially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region , China ( Project Codes : 14200719 ) . 
Abstract We show that leveraging metadata information from web pages can improve the performance of models for answer passage selection / reranking . 
We propose a neural passage selection model that leverages metadata information with a Ô¨Åne - grained encoding strategy , which learns the representation for metadata predicates in a hierarchical way . 
The models are evaluated on the MS MARCO ( Nguyen et al . 
, 2016 ) and Recipe - MARCO datasets . 
Results show that our models signiÔ¨Åcantly outperform baseline models , which do not incorporate metadata . 
We also show that the Ô¨Ånegrained encoding ‚Äôs advantage over other strategies for encoding the metadata . 
1 Introduction Question answering ( QA ) is a long - standing task in NLP and IR . 
Having QA systems that perform well on real - world questions is of signiÔ¨Åcant value for search engines and intelligent assistants . 
While some of the earliest work tackled the task of answering questions based on a large corpus ( V oorhees and Tice , 2000 ; V oorhees , 2003 ; Wang et al . 
, 2007 ) ( albeit mostly focusing on simple fact - oriented questions ) , much of the recent work on QA has focused on answering questions in a less realistic setting ‚Äì drawing the answer from a paragraph of text ( Rajpurkar et al . 
, 2016 ; Joshi et al . 
, 2017 ) , which is commonly referred to as machine reading comprehension ( MRC ) . 
In this work , we tackle the more realistic problem ‚Äî candidate answers passages selection / reranking for real - world questions on the web . 
In contrast to both MRC and early work on QA from a large corpus , web pages often provide an additional source of knowledge . 
In particular , and thanks in part to the Semantic Web initiative ( Berners - Lee ‚àóWork conducted during internship at Microsoft Research . 
‚Ä¶ Textual ObjectPredicateClassic Meatloaf / recipe / name20 minutes / recipe / preptime1 hour , 10 minutes / recipe / cookTime1 celery rib , ‚Ä¶ /recipe / ingredients Metadata Object - predicate Pairs : Figure 1 : Metadata Example from SimplyRecipes . 
RecipecookTimeprepTimerecipeInstructionsrecipeyieldetc . 
Figure 2 : Hierarchy diagram showing properties of ‚Äú recipe ‚Äù from schema.org/recipe . 
et al . 
, 2001 ) , it is estimated that a non - trivial portion of web pages contain metadata annotations that provide a deeper understanding of the website content . 
The Web Data Commons project ( M√ºhleisen and Bizer , 2012 ) estimates that 0.9 billion HTML pages out of the 2.5 billion pages ( 37.1 % ) in the Common Crawl web corpus1contain structured metadata . 
Figure 1 shows an example of this metadata which comes in the form of objectpredicate pairs annotated with schema.org tags ‚Äì a set of tags / predicates deÔ¨Åned in the schema.org2 hierarchy . 
In the example , the hierarchical metadata is used to add more structure to the web page of a recipe , providing meaning to the otherwise unstructured content . 
This makes several aspects of the recipe explicit ‚Äì the preparation time ( PREP TIME ) , cooking time ( COOK TIME ) , ingredients ( INGREDIENTS ) , etc . 
Figure 2 shows the ‚Äú recipe ‚Äù object in schema.org ; it contains several properties such as COOK TIME , PREP TIME , 1http://commoncrawl.org 2http://schema.org551 Is selected URL Passage Text allrecipes.com ... Preheat oven to350degrees F andlightly grease a ... instructions simplyrecipes.com ...... Bake for 1hour and10min cookTime or until a meat thermometer inserted ...  thekitchn.com ... Any ground meat can be used to make meatloaf : beef , pork , veal ingredients ...  livestrong.com ...... loaf to stand for 10to15min cookTime before slicing and servingitto4 - 6 yield ... Table 1 : Example of answer passage selection on the Web . 
There are 4 candidate passages the query ‚Äú How long should I cook ground beef meat loaf in the oven ? ‚Äù RECIPE INSTRUCTIONS , etc . 
We hypothesize that leveraging this metadata , in addition to the textual content , will improve the performance of QA systems on the Web . 
Table 1 presents an example of a query and several candidate passages . 
The candidate answer passages are decorated by colored spans that denote a corresponding schema.org predicate property . 
The correct answer ( ‚Äú 1 hour and 10 min ‚Äù ) could be inferred from the metadata tag COOK TIME . 
While it seems clear from the example that the hierarchical schema.org metadata can be exploited in web QA , it will only be of true beneÔ¨Åt if the use of metadata is prevalent in web pages . 
Luckily , this is the case as shown by Guha et al . 
who studied a sample of 10 billion web pages and showed that one third ( 31.3 % ) of the pages have schema.org markup . 
To date , the end - to - end web QA systems have not made use of this metadata information . 
We Ô¨Årst explore how to incorporate ( and the effect of incorporating ) semantic web hierarchical metadata into statistical NLP models for web - based QA . 
More speciÔ¨Åcally , we introduce a Ô¨Åne - grained encoding method for metadata predicates , to better leverage the semantic information in it . 
We evaluate the models on the answer passage selection / re - ranking task of MS MARCO ( Nguyen et al . 
, 2016 ) , that contains real user queries sampled from the Bing search engine , with the answer passages extracted from real - world web pages . 
Results show that our approaches outperform the baseline systems substantially , with more signiÔ¨Åcant gains on the subset of queries whose candidate passages contain richer metadata tags . 
Our work demonstrates the importance of encoding metadata information for QA , and veriÔ¨Åes our hypothesis that the metadata knowledge can signiÔ¨Åcantly beneÔ¨Åt the performance of the neural models . 
We also provide qualitative analysis that includes performance comparisons acrossdomains . 
Our Ô¨Åndings further provide motivation for webmasters to annotate their web pages with semantic schema.org markup and for question answering systems developer to leverage them . 
2 Related Work Our work is related to several directions of work in semantic web , NLP and ML . 
Metadata for NLP and ML Metadata like time stamp ( Blei and Lafferty , 2006 ) and rating ( Mcauliffe and Blei , 2008 ) have been successfully incorporated in document modeling . 
In community question answering , metadata is often used as hard features to improve the model performance ‚Äì category metadata ( Cao et al . 
, 2010 ; Zhou et al . 
, 2015 ) and user - level information and question- and answer - speciÔ¨Åc data ( Joty et al . 
, 2018 ; Xu et al . 
, 2018 ) . 
For answer quality prediction , author information ( Burel et al . 
, 2012 ; Suggu et al . 
, 2016 ) has been often incorporated . 
In our work , we investigate how to leverage the general metadata knowledge from schema.org in web answer passage selection . 
Our metadata schema used , as compared to prior work mentioned , is structural and hierarchical , and applies to general web pages . 
The metadata could provide rich information to better understand the textual content on the web . 
Semantic Web Berners - Lee et al . 
( 2001 ) described the vision of the Semantic Web . 
The authors envisioned an extension of the World Wide Web , in which information is given well - deÔ¨Åned meaning by bringing structure to the content of web pages . 
Ten years later , several major search engines have come together to launch the schema.org initiative , that to focus on creating , maintaining and promoting a common set of schemas for structured data markup on web pages . 
Webmasters use this schema to add metadata tags to their websites in order to help search engines understand the content . 
The use of such metadata has gained more popularity over the years . 
3 Leveraging Metadata for Answer Passage Selection In our setting of answer passage selection , the input to the system is a set of candidate passages p1 , ... , p n , and a query q , the goal is to identify the passage that best answers the question.552 For each candidate passage pi , we have the URL iof the web page from where it is extracted . 
The web document from URL i , may contain a list of metadata object - predicate pairs ( obj1,pred1 ) , ... , ( objm , predm ) . 
The detailed approach of obtaining the pairs is presented in Section ( 3.1 ) . 
Each predicate pred jconsists of a root rjand a property pro j(e.g . 
, RECIPE and COOK TIME for / RECIPE /COOKTIME , respectively ) . 
We denote the path between rjandpro jasptj . 
3.1 Generate Metadata - Decorated Passages Algorithm 1 generates the decorated answer passages with metadata . 
The example for a decorated passage is shown immediately after the algorithm . 
The spans are marked up with the metadata predicate features . 
The decorated results are later used as input for our models . 
To be more speciÔ¨Åc , given the queryPsgExample ( including query , candidate answer passage , URL , label of whether is selected ) and metadata object - predicate pairs as input , we aim to obtain the queryPsgExamples whose candidate answer passages are decorated . 
We Ô¨Årst obtain all the metadata pairs ( matchingMetaPairs ) for the URL where the passage text appears ( line 1 ) . 
Then , for each metadata pair in matchingMetaPairs , we employ a similarity function ( MetaSim in line 6 ) to Ô¨Årst compute the similarity between all possible text spans of the passage and the object text in the metadata object - predicate pair ; afterwards the function records the start and end offset of the text spans which have a similarity score higher than the threshold . 
In our case , we use BLEU-4 ( Papineni et al . 
, 2002 ) as MetaSim . 
It calculates a score for up to 4 - grams overlap using uniform weights . 
A metadata - decorated candidate passage with the algorithm is presented in Table 2 . 
Algorithm 1 : How to obtain for metadata for each URL and generate metadata - decorated passage Data : queryPsgEg ( query , psgText , URL , label ) , metaPairs ( subj , pred , obj , URL ) ; 1matchingMetaPairs ‚ÜêJoin(queryPsgEg[URL ] = = metaPairs[URL ] ) ; 2foreach pair‚ààmatchingMetaPairs do 3 ifpair[obj ] is not text then 4 continue ; 5 else 6 startOffsets , endOffsets , score ‚Üê MetaSim ( queryPsgEg[psgText ] , pair[obj ] ) ; 7 Decorate ( queryPsgEg[psgText ] , startOffsets , endOffsets , pred ) ; 8 end 9endword Rinse tilapia Ô¨Ållets in cold water ... Season both sides with salt and pepper pred . 
O B_R_ING I_R_ING O O O O feature O O O O B_R_ING I_R_ING I_R_ING Table 2 : Metadata - Decorated Candidate Passage 3.2 Neural Passage Selection with Fine - grained Metadata Encoding We propose a simple but effective neural network structure for building our base neural passage selector ( NPS ) . 
Similar to the neural reader ( Hermann et al . 
, 2015 ; Chen et al . 
, 2017 ) for MRC , we Ô¨Årst obtain a feature - rich ( including the Ô¨Åne - grained encoding of the metadata ) contextualized representation for each token in the passage and query . 
The output layer takes the passage and query representations as input and makes the prediction . 
Fine - grained metadata embedding each predicate feature pred ( e.g. , /RECIPE /COOK TIME ) includes the root r(RECIPE ) and the property pro ( COOK TIME ) . 
To leverage this information , we propose to leverage the hierarchy present on the predicate by learning the root embedding Er , the property embedding Epro , as well as the path embedding Ept(RECIPE‚ÜíCOOK TIME ) , instead of only learning an embedding of the entire predicate ( /RECIPE /COOK TIME ) . 
Thus , the Ô¨Ånal predicate feature encoding for token tiis the concatenation of the three components : Epred(pred i ) = concat ( Er(ri),Epro(pro i),Ept(pti ) ) . 
Passage & Query encoding We Ô¨Årst represent each token tiin the passage with a vector representation and pass it through a multi - layer BiLSTM ( Hochreiter and Schmidhuber , 1997 ) network to get the contextualized representation for each token ( t1,t2 , ... ) , where tiis the concatenation of : ‚Ä¢(Contextualized ) word embedding : GloVe 840B.300d ( Pennington et al . 
, 2014 ) embeddings is used to initialize the embedding layer and is Ô¨Åne - tuned during training , we denote it as Àútifor token ti . 
Besides , we also use the pretrained contextualized representations produced by BERT ( Devlin et al . 
, 2019 ) , ÀÜ q1 , ... , ÀÜ qm , ... , ÀÜt1 , ... , ÀÜtn= BERT ( [ CLS ] , q1 , ... , q m,[SEP ] , t1 , ... , t n ) . 
For the ithtoken , the word embedding E(ti ) is the concatenation of the two . 
‚Ä¢Metadata predicate embedding : We use the Ô¨Åne - grained predicate encoding of metadata553 pair ( Epred(pred i ) ) , as described above . 
Embedding for beginning ( B _ ) and intermediate ( I _ ) tokens of a decorated span are different and learned during training ; For the other passage tokens that are not metadata - decorated , their predicate ( O ) embedding are Ô¨Ålled with zero vectors . 
‚Ä¢Aligned query embedding : Similar to ( Chen et al . 
, 2017 ) , we also incorporate the aligned query embedding . 
This feature is intended to capture the similarity between tiand each query word qj . 
For the ithtoken ti . 
It is calculated as:/summationtext jE(qj)‚àósim(E(ti),E(qj ) ) . 
The encoding pkfor candidate passage kis the sum of the token representations after the BiLSTM . 
Similarly , query token embedding qjis the concatenation of its contextualized word embedding ( ÀÜ qj ) and the GloVe embedding . 
We pass it through another BiLSTM , and use the sum operation to obtain the query encoding q. Prediction Finally , the ‚Äú Is_selected ‚Äù score for passage kis calculated as a function of the passage encoding pkand the query encoding q : score ( k ) = softmax ( pkWq ) . 
At test time , we calculate score ( 1 ) , ... , score ( n)for all the candidate answer passages , and select the passage with highest score : argmaxk(score ( k ) ) . 
4 Experiments and Analysis This section Ô¨Årst presents the QA dataset that is used for evaluation , and then describe results comparing different methods ( with or without leveraging the metadata information ) . 
4.1 Datasets and Models We evaluate our models on the passage selection task of MS MARCO ( Nguyen et al . 
, 2016 ) , to our knowledge , this is currently the only large - scale real - world QA / MRC dataset on general web pages , that is paired with URLs from which the candidate passages are extracted . 
To measure how the models perform when trained and tested on a subset of queries from a focused domain , where the usage of schema.org metadata is more prevalent , we extract the QA pairs of the recipes domain from MS MARCO dataset and extend it with extra QA pairs in this domain ( Recipe - MARCO ) . 
Table 3 shows the number of queries for the datasets . 
Although WikiQA ( Yang et al . 
, 2015 ) and Natural Questions ( Kwiatkowski et al . 
, 2019 ) also containMARCO Recipe MARCO Train 82,326 7515 Dev 10,047 835 Test 9650 846 Table 3 : Statistics of Datasets . 
queries from real users , their answer candidates are restricted to be from Wikipedia . 
However , the adoption of schema.org tags in Wikipedia pages is very low ( < 2.2%3 ) . 
This is signiÔ¨Åcantly less than general web pages where the adoption rate of schema.org metadata is around 31.3 % . 
Thus we do not use these datasets for evaluation . 
We follow previous work ( Yang et al . 
, 2015 ; Tan et al . 
, 2018 ) on reporting precision@1 ( P@1 ) and Mean Reciprocal Rank ( MRR ) . 
P@1 measures whether the highest scoring answer passage returned matches the correct passage . 
MRR ( V oorhees and Tice , 2000 ) evaluates the relative rank of the correct passage in the candidate passages . 
We compare our models to several baselines , S - Net ( Tan et al . 
, 2018 ) is a prior state - of - theart model on MS MARCO , it also produces synthetic answers and use text generation metrics ( e.g. , BLEU and ROUGE - L ) . 
In this work , we only compare to its capability of passage re - ranking . 
NPS is the baseline ‚Äú neural passage selector ‚Äù which does not encode metadata information . 
It ‚Äôs similar to the implementation in Dai and Callan ( 2019 ) . 
B - NPS is a version of our model which builds upon NPS anddirectly encodes the entire predicate . 
F - NPS is our main model ‚Äì Ô¨Åne - grained metadata encoding enriched neural passage selector . 
We also report the results of selecting the Ô¨Årst and a random passage . 
4.2 Results and Analysis Table 4 shows the comparison of different methods on the candidate passage selection task . 
We see that : ( 1 ) By leveraging the metadata , both versions of our model ( B - NPS and F - NPS ) outperform the baseline NPS model ; ( 2 ) With Ô¨Åne - grained encoding , F - NPS signiÔ¨Åcantly outperforms all models in both P@1 and MRR . 
Particularly , F - NPS achieves higher P@1 than NPS by around 2 % ; ( 3 ) From the ablation study , we see the BERT pretrained representations consistently improve the performance , and leveraging the metadata information further improves it . 
We also present the results of different methods when trained and tested on Recipe3http://webdatacommons.org/ structureddata/2018 - 12 / stats / stats.html554 MARCO Recipe - MARCO P@1 MRR P@1 MRR First Passage 13.89 - 15.13 Random 13.76 34.76 11.35 30.67 S - Net ( Tan et al . 
, 2018 ) 28.30 - - NPS 32.80 51.72 41.68 59.73 w/o BERT 29.57 50.10 40.24 58.39 B - NPS 33.52 52.83 43.58 61.37 F - NPS 34.70‚àó54.21 44.37‚àó62.46 w/o BERT 33.01 52.96 43.42 61.13 Table 4 : Evaluation results on datasets . 
Statistic signiÔ¨Åcance is indicated with‚àó(p < 0.05 ) . 
Prop . 
( % ) NPS F - NPS book 6.37 29.06 32.81 medical 13.20 30.69 34.46 person 11.75 29.30 32.51 organization 13.32 30.12 33.86 review 3.09 27.42 35.48 Table 5 : Analysis of P@1 performance for models w/ and w/o metadata information in diverse domains . 
MARCO . 
We see that the relative increase of performances for F - NPS is more substantial . 
Finally , we provide analysis on both the models and the effect of encoding metadata . 
Since not all web pages come with metadata , we turn our attention to the results describing the model performance on the portion of queries of MS MARCO that come with at least one metadata item ( ‚Äú MRich - MARCO ‚Äù ) . 
We Ô¨Årst perform analysis to understand how often the web pages in the dataset contain markup and how it affects the models performance . 
We see that for each query in MS MARCO , there are around 7.9 metadata pairs for its candidate passages ; and 31.6 for queries in MRich - MARCO . 
On M - Rich - MARCO , the results we get on P@1 ( F - NPS : 33.13 , NPS 28.79 ) demonstrate that the performance gap between the model that leverages the metadata is larger than the general case . 
This , once again , demonstrates the effect of encoding metadata knowledge . 
To better understand how the models perform and the effect of metadata on speciÔ¨Åc web domains , we report in Table 5 P@1 of models ( trained on entireMS MARCO ) on domains that are richer with metadata ( i.e. , book , medical , person , organization and review ) . 
We observe that queries in ‚Äú medical ‚Äù , ‚Äù person ‚Äù and ‚Äù organization ‚Äù domains have a larger presence in the dataset ( > 10 % ) . 
The table also shows the performance of NPS and F - NPS on each domain . 
We see that F - NPS outperform NPS across all these domains . 
And the improvement ismore substantial as compared to evaluating on the entire test set ( the second column of Table 4 ) . 
5 Conclusion We demonstrate beneÔ¨Åts of incorporating metadata information from web pages for improving answer passage selection model . 
We describe methods for obtaining metadata and decorating passages with metadata object - predicate pairs , and a Ô¨Ånegrained encoding strategy for leveraging metadata information in neural models . 
For future work , we ‚Äôll investigate metadata for other tasks such as web entity linking and extraction . 
Acknowledgments We thank the anonymous reviewers for helpful feedback and comments . 
Abstract Intermediate - task training‚ÄîÔ¨Åne - tuning a pretrained model on an intermediate task before Ô¨Åne - tuning again on the target task ‚Äî often improves model performance substantially on language understanding tasks in monolingual English settings . 
We investigate whether English intermediate - task training is still helpful onnon - English target tasks . 
Using nine intermediate language - understanding tasks , we evaluate intermediate - task transfer in a zeroshot cross - lingual setting on the XTREME benchmark . 
We see large improvements from intermediate training on the BUCC and Tatoeba sentence retrieval tasks and moderate improvements on question - answering target tasks . 
MNLI , SQuAD and HellaSwag achieve the best overall results as intermediate tasks , while multi - task intermediate offers small additional improvements . 
Using our best intermediate - task models for each target task , we obtain a 5.4 point improvement over XLM - R Large on the XTREME benchmark , setting the state of the art1as of June 2020 . 
We also investigate continuing multilingual MLM during intermediate - task training and using machine - translated intermediatetask data , but neither consistently outperforms simply performing English intermediate - task training . 
1 Introduction Zero - shot cross - lingual transfer involves training a model on task data in one set of languages ( or language pairs , in the case of translation ) and evaluating the model on the same task in unseen languages ( or pairs ) . 
In the context of natural language understanding tasks , this is generally done using a pretrained multilingual language - encoding model ‚á§ ‚á§ Equal contribution . 
1The state of art on XTREME at the time of Ô¨Ånal publication in September 2020 is held by Fang et al . 
( 2020 ) , who introduce an orthogonal method.such as mBERT ( Devlin et al . 
, 2019a ) , XLM ( Conneau and Lample , 2019 ) or XLM - R ( Conneau et al . 
, 2020 ) that has been pretrained with a masked language modeling ( MLM ) objective on large corpora of multilingual data , Ô¨Åne - tune it on task data in one language , and evaluate the tuned model on the same task in other languages . 
Intermediate - task training ( STILTs ; Phang et al . 
, 2018 ) consists of Ô¨Åne - tuning a pretrained model on a data - rich intermediate task , before Ô¨Åne - tuning a second time on the target task . 
Despite its simplicity , this two - phase training setup has been shown to be helpful across a range of Transformer models and target tasks ( Wang et al . 
, 2019a ; Pruksachatkun et al . 
, 2020 ) , at least within English settings . 
In this work , we propose to use intermediate training on English tasks to improve zero - shot cross - lingual transfer performance . 
Starting with a pretrained multilingual language encoder , we perform intermediate - task training on one or more English tasks , then Ô¨Åne - tune on the target task in English , and Ô¨Ånally evaluate zero - shot on the same task in other languages . 
Intermediate - task training on English data introduces a potential issue : We train the pretrained multilingual model extensively on only English data before evaluating it on non - English target task data , potentially causing the model to lose the knowledge of the other languages that was acquired during pretraining ( Kirkpatrick et al . 
, 2017 ; Yogatama et al . 
, 2019 ) . 
To mitigate this issue , we experiment with mixing in multilingual MLM training updates during the intermediate - task training . 
In the same vein , we also conduct a case study where we machine - translate intermediate task data from English into three other languages ( German , Russian and Swahili ) to investigate whether intermediate training on these languages improves target task performance in the same languages . 
Concretely , we use the pretrained XLM - R ( Con-557 Self - supervisedmultilingual pre - trainingIntermediate tasktrainingin EnglishTarget taskfine - tuningin EnglishXLM - RENÂÆø–ñÿ®MLM ÂÆø–ñÿ®Target taskevaluationin each language ... #1#MTarget Task : # mENTarget Task : # mÂÆøSingle - task fine - tuning on task # n ... # 1#NInterm . 
Task : # nEN Interm . 
Task : # 1 to # NInterm . 
Task : # n & MLMÂÆø–ñÿ®EN Multi - task fine - tuningOREnglishMultilingualMasked Language Modeling trainingTarget Task : # m–ñTarget Task : # mÿ®OR ENFigure 1 : We investigate the beneÔ¨Åt of injecting an additional phase of intermediate - task training on English language task data . 
We also consider variants using multi - task intermediate - task training , as well as continuing multilingual MLM during intermediate - task training . 
Best viewed in color . 
neau et al . 
, 2020 ) encoder and perform experiments on 9 target tasks from the recently introduced XTREME benchmark ( Hu et al . 
, 2020 ) , which aims to evaluate zero - shot cross - lingual transfer performance across diverse target tasks across up to 40 languages each . 
We investigate how training on 9 different intermediate tasks , including question answering , sentence tagging , sentence completion , paraphrase detection , and natural language inference impacts zero - shot cross - lingual transfer performance . 
We Ô¨Ånd the following : ‚Ä¢Intermediate - task training on SQuAD , MNLI , and HellaSwag yields large target - task improvements of 8.2 , 7.5 , and 7.0 points on the development set , respectively . 
Multi - task intermediate - task training on all 9 tasks performs best , improving by 8.7 points . 
‚Ä¢Applying intermediate - task training to BUCC and Tatoeba , the two sentence retrieval target tasks that have no training data of their own , yields dramatic improvements with almost every intermediate training conÔ¨Åguration . 
TyDiQA shows consistent improvements with many intermediate tasks , whereas XNLI does not see beneÔ¨Åts from intermediate training . 
‚Ä¢Evaluating our best performing models for each target task on the XTREME benchmark yields an average improvement of 5.4 points , setting the state of the art as of writing.‚Ä¢Training on English intermediate tasks outperforms the more complex alternatives of ( i ) continuing multilingual MLM during intermediate - task training , and ( ii ) using machine - translated intermediate - task data . 
2 Approach We follow a three - phase approach to training , illustrated in Figure 1 : ( i ) we use a publicly available model pretrained on raw multilingual text using MLM ; ( ii ) we perform intermediate - task training on one or more English intermediate tasks ; and ( iii ) we Ô¨Åne - tune the model on English target - task training data , before evaluating it on target - task test data in each target language . 
In phase ( ii ) , our intermediate tasks have English input data . 
In Section 2.4 , we investigate an alternative where we machine - translate intermediate - task data to other languages , which we use for training . 
We experiment with both single- and multi - task training for intermediate - task training . 
We use target tasks from the recent XTREME benchmark for zero - shot cross - lingual transfer . 
2.1 Intermediate Tasks We study the effect of intermediate - task training ( STILTs ; Phang et al . 
, 2018 ) with nine different English intermediate tasks , described in Table 1 . 
We choose the tasks below based to cover a variety of task formats ( classiÔ¨Åcation , question answering , and multiple choice ) and based on evidence558 Name |Train || Dev|| Test| Task Genre / Source Intermediate tasks ANLI+1,104,934 22,857 ‚Äì natural language inference Misc . 
MNLI 392,702 20,000 ‚Äì natural language inference Misc . 
QQP 363,846 40,430 ‚Äì paraphrase detection Quora questions SQuAD v2.0 130,319 11,873 ‚Äì span extraction Wikipedia SQuAD v1.1 87,599 10,570 ‚Äì span extraction Wikipedia HellaSwag 39,905 10,042 ‚Äì sentence completion Video captions & Wikihow CCG 38,015 5,484 ‚Äì tagging Wall Street Journal Cosmos QA 25,588 3,000 ‚Äì question answering Blogs CommonsenseQA 9,741 1,221 ‚Äì question answering Crowdsourced responses Target tasks ( XTREME Benchmark ) XNLI 392,702 2,490 5,010 natural language inference Misc . 
PAWS - X 49,401 2,000 2,000 paraphrase detection Wiki / Quora POS 21,253 3,974 47‚Äì20,436 tagging Misc . 
NER 20,000 10,000 1,000‚Äì10,000 named entity recognition Wikipedia XQuAD 87,599 34,726 1,190 question answering Wikipedia MLQA 87,599 34,726 4,517‚Äì11,590 question answering Wikipedia TyDiQA - GoldP 3,696 634 323‚Äì2,719 question answering Wikipedia BUCC ‚Äì ‚Äì 1,896‚Äì14,330 sentence retrieval Wiki / news Tatoeba ‚Äì ‚Äì 1,000 sentence retrieval Misc . 
Table 1 : Overview of the intermediate tasks ( top ) and target tasks ( bottom ) in our experiments . 
For target tasks , Train andDevcorrespond to the English training and development sets , while Testshows the range of sizes for the target - language test sets for each task . 
XQuAD , TyDiQA and Tateoba do not have separate held - out development sets . 
of positive transfer from literature . 
Pruksachatkun et al . 
( 2020 ) shows that MNLI ( of which ANLI+is a superset ) , CommonsenseQA , Cosmos QA and HellaSwag yield positive transfer to a range of downstream English - language tasks in intermediate training . 
CCG involves token - wise prediction and is similar to the POS and NER target tasks . 
Both versions of SQuAD are widely - used questionanswering tasks , while QQP is semantically similar to sentence retrieval target tasks ( BUCC and Tatoeba ) as well as PAWS - X , another paraphrasedetection task . 
ANLI + MNLI + SNLI ( ANLI+)The Adversarial Natural Language Inference dataset ( Nie et al . 
, 2020 ) is collected using model - in - the - loop crowdsourcing as an extension of the Stanford Natural Language Inference ( SNLI ; Bowman et al . 
, 2015 ) and Multi - Genre Natural Language Inference ( MNLI ; Williams et al . 
, 2018 ) corpora . 
We follow Nie et al . 
( 2020 ) and use the concatenated ANLI , MNLI and SNLI training sets , which we refer to as ANLI+ . 
For all three natural language inference tasks , examples consist of premise and hypothesis sentence pairs , and the task is to classify the relationship between the premise and hypothesis as entailment , contradiction , or neutral . 
CCG CCGbank ( Hockenmaier and Steedman , 2007 ) is a conversion of the Penn Treebank into Combinatory Categorial Grammar ( CCG ) derivations . 
The CCG supertagging task that we use consists of assigning lexical categories to individual word tokens , which together roughly determine a full parse.2 CommonsenseQA CommonsenseQA ( Talmor et al . 
, 2019 ) is a multiple - choice QA dataset generated by crowdworkers based on clusters of concepts from ConceptNet ( Speer et al . 
, 2017 ) . 
Cosmos QA Cosmos QA is multiple - choice commonsense - based reading comprehension dataset ( Huang et al . 
, 2019b ) generated by crowdworkers , with a focus on the causes and effects of events . 
HellaSwag HellaSwag ( Zellers et al . 
, 2019 ) is a commonsense reasoning dataset framed as a fourway multiple choice task , where examples consist of an incomplete paragraph and four choices of spans , only one of which is a plausible continuation of the scenario . 
It is built using adversarial Ô¨Åltering ( Zellers et al . 
, 2018 ; Le Bras et al . 
, 2020 ) with BERT . 
2If a word is tokenized into sub - word tokens , we use the representation of the Ô¨Årst token for the tag prediction for that word as in Devlin et al . 
( 2019a ) .559 MNLI In additional to the full ANLI+ , we also consider the MNLI task as a standalone intermediate task because of its already large and diverse training set . 
QQP Quora Question Pairs3is a paraphrase detection dataset . 
Examples in the dataset consist of two questions , labeled for whether they are semantically equivalent . 
SQuAD Stanford Question Answering Dataset ( Rajpurkar et al . 
, 2016 , 2018 ) is a questionanswering dataset consisting of passages extracted from Wikipedia articles and crowd - sourced questions and answers . 
In SQuAD version 1.1 , each example consists of a context passage and a question , and the answer is a text span from the context . 
SQuAD version 2.0 includes additional questions with no answers , written adversarially by crowdworkers . 
We use both versions in our experiments . 
2.2 Target Tasks We use the 9 target tasks from the XTREME benchmark , which span 40 different languages ( hereafter referred to as the target languages ): Crosslingual Question Answering ( XQuAD ; Artetxe et al . 
, 2020b ) ; Multilingual Question Answering ( MLQA ; Lewis et al . 
, 2020 ) ; Typologically Diverse Question Answering ( TyDiQA - GoldP ; Clark et al . 
, 2020 ) ; Cross - lingual Natural Language Inference ( XNLI ; Conneau et al . 
, 2018 ) ; Crosslingual Paraphrase Adversaries from Word Scrambling ( PAWS - X ; Yang et al . 
, 2019 ) ; Universal Dependencies v2.5 ( Nivre et al . 
, 2018 ) POS tagging ; Wikiann NER ( Pan et al . 
, 2017 ) ; BUCC ( Zweigenbaum et al . 
, 2017 , 2018 ) , which requires identifying parallel sentences from corpora of different languages ; and Tatoeba ( Artetxe and Schwenk , 2019 ) , which involves aligning pairs of sentences with the same meaning . 
Among the 9 tasks , BUCC and Tatoeba are sentence retrieval tasks that do not include training sets , and are scored based on the similarity of learned representations ( see Appendix A ) . 
XQuAD , TyDiQA and Tatoeba do not include development sets separate from the test sets.4For all XTREME tasks , we follow the training and evaluation protocol described in the benchmark paper ( Hu et al . 
, 2020 ) 3http://data.quora.com/ First - Quora - DatasetRelease - Question - Pairs 4UDPOS also does not include development sets for Kazakh , Thai , Tagalog or Yoruba.and their sample implementation.5Intermediateand target - task statistics are shown in Table 1 . 
2.3 Multilingual Masked Language Modeling Our setup requires that we train the pretrained multilingual model extensively on English data before using it on a non - English target task , which can lead to the catastrophic forgetting of other languages acquired during pretraining . 
We investigate whether continuing to train on the multilingual MLM pretraining objective while Ô¨Åne - tuning on an English intermediate task can prevent catastrophic forgetting of the target languages and improve downstream transfer performance . 
We construct a multilingual corpus across the 40 languages covered by the XTREME benchmark using Wikipedia dumps from April 14 , 2020 for each language and the MLM data creation scripts from thejiant 1.3 library ( Phang et al . 
, 2020 ) . 
In total , we use 2 million sentences sampled across all 40 languages using the sampling ratio from Conneau and Lample ( 2019 ) with ‚Üµ = 0.3 . 
2.4 Translated Intermediate - Task Training Large - scale labeled datasets are rarely available in languages other than English for most languageunderstanding benchmark tasks . 
Given the availability of increasingly performant machine translation models , we investigate if using machinetranslated intermediate - task data can improve samelanguage transfer performance , compared to using English intermediate task data . 
We translate training and validation data of three intermediate tasks : QQP , HellaSwag , and MNLI . 
We choose these tasks based on the size of the training sets and because their examplelevel ( rather than word - level ) labels can be easily mapped onto translated data . 
To translate QQP and HellaSwag , we use pretrained machine translation models from OPUS - MT ( Tiedemann and Thottingal , 2020 ) . 
These models are trained with Marian - NMT ( Junczys - Dowmunt et al . 
, 2018 ) on OPUS data ( Tiedemann , 2012 ) , which integrates several resources depending on the available corpora for the language pair . 
For MNLI , we use the publicly available machine - translated training data of XNLI provided by the XNLI authors.6We use German , Russian , and Swahili translations of 5https://github.com/google-research/ xtreme 6According to Conneau et al . 
( 2018 ) , these data are translated using a Facebook internal machine translation system.560 all three datasets instead of English data for the intermediate - task training . 
3 Experiments and Results 3.1 Models We use the pretrained XLM - R Large model ( Conneau et al . 
, 2020 ) as a starting point for all our experiments , as it currently achieves state - of - theart performance on many zero - shot cross - lingual transfer tasks.7Details on intermediate- and targettask training can be found in Appendix A. XLM - R For our baseline , we directly Ô¨Åne - tune the pretrained XLM - R model on each target task ‚Äôs English training data ( if available ) and evaluate zero - shot on non - English data , closely following the sample implementation for the XTREME benchmark . 
XLM - R + Intermediate Task In our main approach , as described in Figure 1 , we include an additional intermediate - task training phase before training and evaluating on the target tasks as described above . 
We also experiment with multi - task training on all available intermediate tasks . 
We follow Raffel et al . 
( 2020 ) and sample batches of examples for each task with probability rm = min(em , K)P(min(em , K ) , where emis the number of examples in task mand the constant K=217limits the oversampling of data - rich tasks . 
XLM - R + Intermediate Task + MLM To incorporate multilingual MLM into the intermediatetask training , we treat multilingual MLM as an additional task for intermediate training , using the same multi - task sampling strategy as above . 
XLM - R + Translated Intermediate Task We translate intermediate - task training and validation data for three tasks and Ô¨Åne - tune XLM - R on translated intermediate - task data before we train and evaluate on the target tasks . 
3.2 Software Experiments were carried out using the jiant ( Phang et al . 
, 2020 ) library ( 2.0 alpha ) , based on PyTorch ( Paszke et al . 
, 2019 ) and Transformers ( Wolf et al . 
, 2019 ) . 
7XLM - R Large ( Conneau et al . 
, 2020 ) is a 550m - parameter variant of the RoBERTa masked language model ( Liu et al . 
, 2019b ) trained on a cleaned version of CommonCrawl on 100 languages . 
Notably , Yoruba is used in the POS and NER XTREME tasks but not is not in the set of 100 languages.3.3 Results We train three versions of each intermediate - task model with different random seeds . 
For each run , we compute the average target - task performance across languages , and report the median performance across the three random seeds . 
Intermediate - Task Training As shown in Table2 , no single intermediate task yields positive transfer across all target tasks . 
The target tasks TyDiQA , BUCC and Tatoeba see consistent gains from most or all intermediate tasks . 
In particular , BUCC and Tatoeba , the two sentence retrieval tasks with no training data , beneÔ¨Åt universally from intermediate - task training . 
PAWS - X , NER , XQuAD and MLQA also exhibit gains with the additional intermediate - task training on some intermediate tasks . 
On the other hand , we Ô¨Ånd generally no or negative transfer to XNLI and POS . 
Among the intermediate tasks , we Ô¨Ånd that MNLI performs best ; with meaningful improvements across the PAWS - X , TyDiQA , BUCC and Tatoeba tasks . 
ANLI+ , SQuAD v1.1 , SQuAD v2.0 and HellaSwag also show strong positive transfer performance : SQuAD v1.1 shows strong positive transfer across all three QA tasks , SQuAD v2.0 shows the most positive transfer to TyDiQA , while HellaSwag shows the most positive transfer to NER and BUCC tasks . 
ANLI+does not show any improvement over MNLI ( of which it is a superset ) , even on XNLI for which it offers additional directly relevant training data . 
This mirrors negative Ô¨Åndings from Nie et al . 
( 2020 ) on NLI evaluations and Bowman et al . 
( 2020 ) on transfer within English . 
QQP signiÔ¨Åcantly improves sentence retrieval - task performance , but has broadly negative transfer to the other target tasks.8CCG also has relatively poor transfer performance , consistent with Pruksachatkun et al . 
( 2020 ) . 
Among our intermediate tasks , both SQuAD v1.1 and MNLI also serve as training sets for target tasks ( for XNLI and XQuAD / MLQA respectively ) . 
While both tasks show overall positive transfer , SQuAD v1.1 actually markedly improves the performance in XQuAD and MLQA , while MNLI slightly hurts XNLI performance . 
We hypothesize that the somewhat surprising improvements to XQuAD and MLQA performance from SQuAD v1.1 arise due to the baseline XQuAD and MLQA 8For QQP , on 2 of the 3 random seeds the NER model performed extremely poorly , leading to the large negative transfer of -45.4.561 Target tasksXNLI PAWS - X POS NER XQuAD MLQA TyDiQA BUCC Tatoeba Avg . 
Metricacc . 
acc . 
F1 F1 F1 / EM F1 / EM F1 / EM F1 acc . 
‚Äì # langs.15 7 33 40 11 7 9 5 37 ‚Äì XLM - R 80.186.5 75.7 62.8 76.1 / 60.0 70.1 / 51.5 65.6 / 48.2 71.5 31.0 67.2Without MLMANLI+- 0.8- 0.0- 1.4- 3.5- 1.1/- 0.5- 0.6/- 0.8- 0.6/- 3.0 + 19.9 + 48.2 + 6.6MNLI- 1.2 + 1.4- 0.7 + 0.5- 0.3/- 0.1 + 0.2/+ 0.2- 1.0/- 1.6 + 20.0 + 48.8 + 7.5QQP- 4.4- 4.8- 6.5 - 45.4- 3.8/- 3.8- 3.9/- 4.4 - 11.1/-10.2 + 17.1 + 49.5- 1.5SQuADv1.1- 1.9 + 1.2- 0.8- 0.4 + 1.8/+ 2.5 + 2.2/+ 2.6 + 9.7/+10.8 + 18.9 + 41.3 + 8.1SQuADv2- 1.6 + 1.9- 1.1 + 0.8- 0.5/+ 0.7- 0.4/+ 0.1 + 10.4/+11.3 + 19.3 + 43.4 + 8.2HellaSwag- 7.1 + 1.8- 0.7 + 1.6- 0.0/+ 0.5- 0.1/+ 0.2- 0.0/- 1.0 + 20.3 + 47.6 + 7.0CCG- 2.6- 3.4- 2.0- 1.5- 1.5/- 1.3- 1.6/- 1.5- 2.8/- 6.2 + 11.7 + 41.9 + 4.1CosmosQA- 2.1- 0.3- 1.4- 1.5- 0.9/- 1.3- 1.5/- 2.0 + 0.5/- 0.6 + 19.2 + 43.9 + 6.1CSQA- 2.9- 2.8- 1.7- 1.6- 1.0/- 1.8- 1.0/- 0.6 + 3.5/+ 2.9 + 18.1 + 48.6 + 6.5Multi - task- 0.9 + 1.7- 1.0 + 1.8 + 0.3/+ 0.9 + 0.2/+ 0.5 + 5.8/+ 6.0 + 19.6 + 49.9 + 8.7With MLMANLI+- 1.1 + 1.4 + 0.0 + 0.4- 1.9/- 1.7- 0.7/- 0.6 + 0.9/+ 0.5 + 18.6 + 46.2 + 7.1MNLI- 0.7 + 1.6- 1.6 + 1.0- 0.7/+ 0.1 + 0.4/+ 0.8- 1.8/- 3.2 + 17.1 + 44.3 + 6.6QQP- 1.3- 1.1- 2.4- 0.9- 0.3/- 0.2 + 0.0/+ 0.2- 1.6/- 4.2 + 14.4 + 39.8 + 5.0SQuADv1.1- 2.6 + 0.3- 2.0- 0.9 + 0.2/+ 1.6 + 0.1/+ 1.1 + 8.5/+ 9.5 + 16.0 + 40.3 + 6.8SQuADv2- 1.7 + 2.1- 1.4 + 1.0- 0.8/+ 0.1- 0.8/- 0.5 + 8.3/+ 8.9 + 15.6 + 31.3 + 6.1HellaSwag- 3.3 + 2.0- 0.7 + 0.8- 0.8/- 0.0 + 0.1/+ 0.6 + 0.3/+ 1.0 + 6.3 + 22.3 + 3.1CCG- 1.0- 1.3- 1.2- 1.9- 1.9/- 2.2- 2.1/- 2.6- 5.5/- 6.2 + 8.8 + 36.1 + 3.3CosmosQA- 1.0- 1.0- 1.6- 3.8- 3.1/- 3.3- 3.7/- 4.2- 0.6/- 3.2 + 15.5 + 42.7 + 4.7CSQA- 0.5 + 0.3- 1.0- 0.7- 0.9/- 1.0- 0.7/- 0.6 + 2.1/+ 0.4 + 11.6 + 17.2 + 2.9XTREME Benchmark Scores‚Ä†XLM - R ( Hu et al . 
,2020)79.2 86.4 72.665.476.6 / 60.8 71.6 / 53.2 65.1 / 45.0 66.0 57.3 68.1XLM - R ( Ours)79.5 86.2 74.0 62.6 76.1 / 60.0 70.2 / 51.2 65.6 / 48.2 64.5 31.0 64.8Our Best Models‚Ä°80.0 87.9 74.464.078.7/63.3 72.4/53.7 76.0/59.5 71.9 81.2 73.5Human ( Hu et al . 
,2020)92.8 97.5 97.0 - 91.2 / 82.3 91.2 / 82.3 90.1 / - - - -Table 2 : Intermediate - task training results . 
We compute the average target task performance across all languages , and report the median over 3 separate runs with different random seeds . 
Multi - task experiments use all intermediate tasks . 
We underline the best results per target task with and without intermediate MLM co - training , and bold - face the best overall scores for each target task.‚Ä† : XQuAD , TyDiQA and Tatoeba do not have held - out test data and are scored using development sets in the benchmark.‚Ä° : Results obtained with our best - performing intermediate task conÔ¨Åguration for each target task , selected based on the development set . 
The results for individual languages can be found in Appendix B. models being under - trained . 
For all target - task Ô¨Ånetuning , we follow the sample implementation for target task training in the XTREME benchmark , which trains on SQuAD for only 2 epochs . 
This may explain why an additional phase of SQuAD training can improve performance . 
Conversely , the MNLI - to - XNLI model might be over - trained , given the MNLI training set is approximately 4 times as large as the SQuAD v1.1 training set . 
Multi - Task Training Multi - task training on all intermediate tasks attains the best overall average performance on the XTREME tasks , and has the most positive transfer to NER and Tatoeba tasks . 
However , the overall margin of improvement over the best single intermediate - task model is relatively small ( only 0.3 , over MNLI ) , while requiring signiÔ¨Åcantly more training resources . 
Many single intermediate - task models also outperform the multitask model in individual target tasks . 
Wang et al . 
( 2019b ) also found more mixed results from a having an initial phase of multi - task training , albeitonly among English language tasks across a different set of tasks . 
On the other hand , multi - task training precludes the need to do intermediate - task model selection , and is a useful method for incorporating multiple , diverse intermediate tasks . 
MLM Incorporating MLM during intermediatetask training shows no clear trend . 
It reduces negative transfer , as seen in the cases of CommonsenseQA and QQP , but it also tends to somewhat reduce positive transfer . 
The reductions in positive transfer are particularly signiÔ¨Åcant for the BUCC and Tatoeba tasks , although the impact on TyDiQA is more mixed . 
On balance , we do not see that incorporating MLM improves transfer performance . 
XTREME Benchmark Results At the bottom of Table 2 , we show results obtained by XLM - R on the XTREME benchmark as reported by Hu et al . 
( 2020 ) , results obtained with our reimplementation of XLM - R ( i.e. our baseline ) , and results obtained with our best models , which use intermediate - task conÔ¨Åguration selected according562 TL Model XNLI PAWS - X POS NER XQuAD MLQA TyDiQA BUCC TatoebaEnglishXLM - R 89.393.4 95.9 81.686.3/ 74.2 81.6 / 68.6 70.4 / 56.6 ‚Äì ‚Äì MNLIen- 1.2 + 1.6 + 0.3 + 2.6- 2.1/- 1.6 + 1.1/+ 1.4 + 1.1/+ 1.1‚Äì‚ÄìQQPen- 3.2- 0.4- 2.2- 5.8- 4.0/- 3.6- 2.6/- 2.6- 6.2/- 5.0‚Äì‚ÄìHellaSwagen- 0.8 + 1.5 + 0.6 + 2.7- 0.2/+ 1.4 + 1.8/+ 2.3 + 1.7/+ 2.5‚Äì‚ÄìGermanXLM - R 83.888.1 88.6 78.6 77.7 / 61.269.1/ 52.0 ‚Äì 77.7 63.9MNLIen- 0.8 + 0.9- 0.1- 0.8- 0.3/- 1.0- 1.0/- 0.2‚Äì+16.5 + 32.7MNLIde- 0.4 + 0.5- 0.3- 0.9 + 0.2/- 0.3- 2.4/- 2.0‚Äì+17.0 + 33.7QQPen- 2.2- 4.2- 3.2- 7.3- 4.5/- 4.7- 6.7/- 6.4‚Äì+16.5 + 32.6QQPde- 2.6- 9.1- 3.2 - 22.9- 6.6/- 5.9- 7.7/- 6.6‚Äì+16.0 + 33.5HellaSwagen- 0.3 + 0.3 + 0.1 + 0.5 + 1.0/+ 0.2- 0.3/+ 0.4‚Äì+16.9 + 33.8HellaSwagde- 0.2 + 0.2- 0.4- 0.4 + 0.2/- 0.2- 3.5/- 2.5‚Äì+16.3 + 33.5RussianXLM - R79.2 ‚Äì 89.569.3 77.7 / 59.8 ‚Äì 65.4 / 43.6 79.2 42.1MNLIen+ 0.3‚Äì- 0.0 + 0.8 + 0.1/+ 1.5‚Äì- 1.5/- 4.6 + 14.3 + 47.1MNLIru- 0.6‚Äì- 0.3 + 1.9- 0.4/+ 1.3‚Äì+11.2/+16.1 + 13.1 + 48.3QQPen- 0.7‚Äì- 2.9 - 18.6- 3.5/- 2.4‚Äì- 8.1/- 5.4 + 14.1 + 49.5QQPru- 3.0‚Äì-10.6 - 59.1- 5.2/- 3.9‚Äì-14.4/-12.1 + 13.3 + 46.7HellaSwagen- 0.9‚Äì- 0.0 + 1.4 + 0.8/+ 2.9‚Äì- 4.0/-10.6 + 14.7 + 49.9HellaSwagru- 0.3‚Äì- 0.4 + 2.8 + 0.2/+ 0.2‚Äì+ 8.5/+13.2 - 71.6 - 23.5SwahiliXLM - R 72.4 ‚Äì ‚Äì 69.8 ‚Äì ‚Äì 67.2 / 48.7 ‚Äì 7.9MNLIen- 3.0‚Äì‚Äì+ 0.6‚Äì‚Äì- 0.3/- 0.2‚Äì+24.9MNLIsw- 1.1‚Äì‚Äì- 2.4‚Äì‚Äì+13.8/+23.4‚Äì+47.9QQPen- 2.8‚Äì‚Äì- 4.6‚Äì‚Äì-12.7/-12.2‚Äì+27.2QQPsw- 7.1‚Äì‚Äì-32.1‚Äì‚Äì- 7.0/- 0.4‚Äì+41.8HellaSwagen- 0.4‚Äì‚Äì+ 0.1‚Äì‚Äì- 0.9/- 0.4‚Äì+27.2HellaSwagsw- 9.8‚Äì‚Äì+ 0.4‚Äì‚Äì+15.6/+26.3‚Äì- 0.5Table 3 : Experiments with translated intermediate - task training and validation data evaluated on all XTREME target tasks . 
In each target language ( TL ) block , models are evaluated on a single target language . 
We show results for models trained on original intermediate - task training data ( en ) and compare it to models trained on translated data{de , ru , sw } . 
‚Äò ‚Äì ‚Äô indicates that target task data is not available for that target language . 
to development set performance on each target task . 
Based on the results in Table 2 , which reÔ¨Çect the median over 3 runs , we pick the best intermediatetask conÔ¨Åguration for each target task , and then choose the best model out of the 3 runs . 
Scores on the XTREME benchmark are computed based on the respective test sets where available , and based on development sets for target tasks without separate held - out test sets . 
We are generally able to replicate the best reported XLM - R baseline results , except for Tatoeba , where our implementation signiÔ¨Åcantly underperforms the reported scores in Hu et al . 
( 2020 ) , and TyDiQA , where our implementation outperforms the reported scores . 
We also highlight that there is a large margin of difference between development and test set scores for BUCC ‚Äì this is likely because BUCC is evaluated based on sentence retrieval over the given set of input sentences , and the test sets for BUCC are generally much larger than the development sets . 
Our best models show gains in 8 out of the 9 XTREME tasks relative to both baseline implementations , attaining an average score of 73.5 across target tasks , a 5.4 point improvement over the pre - vious best reported average score of 68.1 . 
We set the state of the art on the XTREME benchmark as of June 2020 , though Fang et al . 
( 2020 ) achieve higher results and hold the state of the art using an orthogonal approach at the time of our Ô¨Ånal publication in September 2020 . 
Translated Intermediate - Task Training Data In Table 3 , we show results for experiments using machine - translated intermediate - training data , and evaluated on the available target - task languages . 
Surprisingly , even when evaluating inlanguage , using target - language intermediate - task data does not consistently outperform using English intermediate - task data in any of the intermediate tasks on average . 
In general , cross - lingual transfer to XNLI is negative regardless of the intermediate - task or the target language . 
In contrast , we observe mostly positive transfer on BUCC , and Tatoeba , with a few notable exceptions where models fail catastrophically . 
TyDiQA exhibits positive transfer where the intermediate- and target - task languages aligned : intermediate training on Russian or German helps TyDiQA performance in that respective language,563 whereas intermediate training on English hurts nonEnglish performance somewhat . 
For the remaining tasks , there appears to be little correlation between performance and the alignment of intermediateand target - task languages . 
English language QQP already has mostly negative transfer to all target tasks except for BUCC and Tatoeba ( see Table 2 ) , and also shows a similar trend when translated into any of the three target languages . 
We note that the quality of translations may affect the transfer performance . 
While validation performance on the translated intermediate tasks ( Table 15 ) for MNLI and QQP is only slightly worse than the original English versions , the performance for the Russian and Swahili HellaSwag is much worse and close to chance . 
Despite this , intermediate - task training on Russian and Swahili HellaSwag improve performance on PAN - X and TyDiQA , while we see generally poor transfer performance from QQP . 
The interaction between translated intermediate - task data and transfer performance continues to be a complex open question . 
Artetxe et al . 
( 2020a ) found that translating or back - translating training data for a task can improve zero - shot cross - lingual performance for tasks such as XNLI depending on how the multilingual datasets are created . 
In contrast , we train on translated intermediate - task data and then Ô¨Åne - tune on a target task with English training data ( excluding BUCC2018 and Tatoeba ) . 
The authors of the XTREME benchmark have also recently released translated versions of all the XTREME task training data , which we hope will prompt further investigation into this matter . 
4 Related work Sequential transfer learning using pretrained Transformer - based encoders ( Phang et al . 
, 2018 ) has been shown to be effective for many text classiÔ¨Åcation tasks . 
This setup generally involves Ô¨Ånetuning on a single task ( Pruksachatkun et al . 
, 2020 ; Vu et al . 
, 2020 ) or multiple tasks ( Liu et al . 
, 2019a ; Wang et al . 
, 2019b ; Raffel et al . 
, 2020 ) , sometimes referred to as the intermediate task(s ) , before Ô¨Ånetuning on the target task . 
We build upon this line of work , focusing on intermediate - task training for improving cross - lingual transfer . 
Early work on cross - lingual transfer mostly relies on the availability of parallel data , where one can perform translation ( Mayhew et al . 
, 2017 ) or project annotations from one language into another(Hwa et al . 
, 2005 ; Agi¬¥c et al . 
, 2016 ) . 
For dependency parsing , McDonald et al . 
( 2011 ) use delexicalized parsers trained on source languages and labeled training data for parsing target - language data . 
Agi¬¥c(2017 ) proposes a parser selection method to select the single best parser for a target language . 
For large - scale cross - lingual transfer outside NLU , Johnson et al . 
( 2017 ) train a single multilingual neural machine translation system with up to 7 languages and perform zero - shot translation without explicit bridging between the source and target languages . 
Aharoni et al . 
( 2019 ) expand this approach to cover over 100 languages in a single model . 
Recent works on extending pretrained Transformer - based encoders to multilingual settings show that these models are effective for cross - lingual tasks and competitive with strong monolingual models on the XNLI benchmark ( Devlin et al . 
, 2019b ; Conneau and Lample , 2019 ; Conneau et al . 
, 2020 ; Huang et al . 
, 2019a ) . 
More recently , Artetxe et al . 
( 2020a ) showed that cross - lingual transfer performance can be sensitive to translation artifacts arising from a multilingual datasets ‚Äô creation procedure . 
Finally , Pfeiffer et al . 
( 2020 ) propose adapter modules that learn language and task representations for cross - lingual transfer , which allow adaptation to languages not seen during pretraining . 
5 Conclusion We evaluate the impact of intermediate - task training on zero - shot cross - lingual transfer . 
We investigate 9 intermediate tasks and how intermediate - task training impacts the zero - shot cross - lingual transfer to the 9 target tasks in the XTREME benchmark . 
Overall , intermediate - task training signiÔ¨Åcantly improves the performance on BUCC and Tatoeba , the two sentence retrieval target tasks in the XTREME benchmark , across almost every intermediate - task conÔ¨Åguration . 
Our best models obtain 5.9 and 23.9 point gains on BUCC and Tatoeba , respectively , compared to the best available XLM - R baseline scores ( Hu et al . 
, 2020 ) . 
We also observed gains in question - answering tasks , particularly using SQuAD v1.1 and v2.0 as intermediate tasks , with absolute gains of 2.1 F1 for XQuAD , 0.8 F1 for MLQA , and 10.4 for F1 TyDiQA , again over the best available baseline scores . 
We improve over XLM - R by 5.4 points on average on the XTREME benchmark . 
Additionally , we found multi - task training on all 9 intermedi-564 ate tasks to slightly outperform individual intermediate training . 
On the other hand , we found that neither incorporating multilingual MLM into the intermediate - task training phase nor translating intermediate - task data consistently led to improved transfer performance . 
While we have explored the extent to which English intermediate - task training can improve crosslingual transfer , a clear next avenue of investigation for future work is how the choice of intermediateand target - task languages inÔ¨Çuences transfer across different tasks . 
Acknowledgments This project has beneÔ¨Åted from support to SB by Eric and Wendy Schmidt ( made by recommendation of the Schmidt Futures program ) , by Samsung Research ( under the project Improving Deep Learning using Latent Structure ) , by Intuit , Inc. , by NVIDIA Corporation ( with the donation of a Titan V GPU ) , by Google ( with the donation of Google Cloud credits ) . 
IC has received funding from the European Union ‚Äôs Horizon 2020 research and innovation program under the Marie Sk≈Çodowska - Curie grant agreement No 838188 . 
This project has beneÔ¨Åted from direct support by the NYU IT High Performance Computing Center . 
This material is based upon work supported by the National Science Foundation under Grant No . 
1922658 . 
Any opinions , Ô¨Åndings , and conclusions or recommendations expressed in this material are those of the author(s ) and do not necessarily reÔ¨Çect the views of the National Science Foundation . 
Abstract Slot-Ô¨Ålling , Translation , Intent classiÔ¨Åcation , and Language identiÔ¨Åcation , or STIL , is a newly - proposed task for multilingual Natural Language Understanding ( NLU ) . 
By performing simultaneous slot Ô¨Ålling and translation into a single output language ( English in this case ) , some portion of downstream system components can be monolingual , reducing development and maintenance cost . 
Results are given using the multilingual BART model ( Liu et al . 
, 2020 ) Ô¨Åne - tuned on 7 languages using the MultiATIS++ dataset . 
When no translation is performed , mBART ‚Äôs performance is comparable to the current state of the art system ( Cross - Lingual BERT by Xu et al . 
( 2020 ) ) for the languages tested , with better average intent classiÔ¨Åcation accuracy ( 96.07 % versus 95.50 % ) but worse average slot F1 ( 89.87 % versus 90.81 % ) . 
When simultaneous translation is performed , average intent classiÔ¨Åcation accuracy degrades by only 1.7 % relative and average slot F1 degrades by only 1.2 % relative . 
1 Introduction Multilingual Natural Language Understanding ( NLU ) , also called cross - lingual NLU , is a technique by which an NLU - based system can scale to multiple languages . 
A single model is trained on more than one language , and it can accept input from more than one language during inference . 
In most recent high - performing systems , a model is Ô¨Årst pre - trained using unlabeled data for all supported languages and then Ô¨Åne tuned for a speciÔ¨Åc task using a small set of labeled data ( Conneau and Lample , 2019 ; Pires et al . 
, 2019 ) . 
Two typical tasks for goal - based systems , such as virtual assistants and chatbots , are intent classiÔ¨Åcation and slot Ô¨Ålling ( Gupta et al . 
, 2006 ) . 
Though intent classiÔ¨Åcation creates a language agnostic output ( the intent of the user ) , slot Ô¨Ålling does not . 
Input ‰ªéÁõêÊπñÂüéÂà∞Âä†Â∑ûÂ••ÂÖãÂÖ∞ÁöÑËà™Áè≠ Traditional Outputintent : Ô¨Çight slots : ( ÁõêÊπñÂüé , fromloc.cityname ) , . 
. 
. 
( Â••ÂÖãÂÖ∞ , toloc.cityname ) , . 
. 
. 
( Âä†Â∑û , toloc.statename ) STIL Outputintent : Ô¨Çight slots : ( salt lake city , fromloc.cityname ) , . 
. 
. 
( oakland , toloc.cityname ) , . 
. 
. 
( california , toloc.statename ) lang : zh Table 1 : Today ‚Äôs slot Ô¨Ålling systems do not translate the slot content , as shown in ‚Äú Traditional Ouput . 
‚Äù With a STIL model , the slot content is translated and language identiÔ¨Åcation is performed . 
Instead , a slot-Ô¨Ålling model outputs the labels for each of input tokens from the user . 
Suppose the slot-Ô¨Ålling model can handle Llanguages . 
Downstream components must therefore handle all L languages for the full system to be multilingual acrossLlanguages . 
Machine translation could be performed before the slot Ô¨Ålling model at system runtime , though the latency would be fully additive , and some amount of information useful to the slotÔ¨Ålling model may be lost . 
Similarly , translation could occur after the slot-Ô¨Ålling model at runtime , but slot alignment between the source and target language is a non - trivial task ( Jain et al . 
, 2019 ; Xu et al . 
, 2020 ) . 
Instead , the goal of this work was to build a single model that can simultaneously translate the input , output slotted text in a single language ( English ) , classify the intent , and classify the input language ( See Table 1 ) . 
The STIL task is deÔ¨Åned such that the input language tag is not given to the model as input . 
Thus , language identiÔ¨Åcation is necessary so that the system can communicate back to the user in the correct language . 
Contributions of this work include ( 1 ) the introduction of a new task for multilingual NLU , namely simultaneous Slot Ô¨Ålling , Translation , Intent clas-576 Example Input Example Output Ô¨Ç¬®uge von salt lake city nach oakland kaliforniensalt < B - fromloc.city name > lake < I - fromloc.city name > city < I - fromloc.city name > oakland < B-toloc.city name > california < B-toloc.state name > < intent-Ô¨Çight > < lang - de > ‰ªéÁõêÊπñÂüéÂà∞Âä†Â∑ûÂ••ÂÖãÂÖ∞ ÁöÑËà™Áè≠salt < B - fromloc.city name > lake < I - fromloc.city name > city < I - fromloc.city name > oakland < B-toloc.city name > california < B-toloc.state name > < intent-Ô¨Çight > < lang - zh > Table 2 : Two text - to - text STIL examples . 
In all STIL cases , the output is in English . 
Each token is followed by its BIO - tagged slot label . 
The sequence of tokens and slots are followed by the intent and then the language . 
siÔ¨Åcation , and Language identiÔ¨Åcation ( STIL ) ; ( 2 ) both non - translated and STIL results using the mBART model ( Liu et al . 
, 2020 ) trained using a fully text - to - text data format ; and ( 3 ) public release of source code used in this study , with a goal toward reproducibility and future work on the STIL task1 . 
2 Dataset The Airline Travel Information System ( ATIS ) dataset is a classic benchmark for goal - oriented NLU ( Price , 1990 ; Tur et al . 
, 2010 ) . 
It contains utterances focused on airline travel , such as how much is the cheapest Ô¨Çight from Boston to New York tomorrow morning ? The dataset is annotated with 17 intents , though the distribution is skewed , with 70 % of intents being the Ô¨Çight intent . 
Slots are labeled using the Beginning Inside Outside ( BIO ) format . 
ATIS was localized to Turkish and Hindi in 2018 , forming MultiATIS ( Upadhyay et al . 
, 2018 ) , and then to Spanish , Portuguese , German , French , Chinese , and Japanese in 2020 , forming MultiATIS++ ( Xu et al . 
, 2020 ) . 
In this work , Portuguese was excluded due to a lack of Portuguese pretraining in the publicly available mBART model , and Japanese was excluded due to a current lack of alignment between Japanese and English samples in MultiATIS++ . 
Hindi and Turkish data were taken from MultiATIS , and the training data were upsampled by 3x for Hindi and 7x for Turkish . 
Prior to any upsampling , there were 4,488 training samples for English , Spanish , German , French , and Chinese . 
The test sets contained 893 samples for all languages except Turkish , which had 715 samples . 
For English , Spanish , German , French , and Chinese , validation sets of 490 samples were used in all cases . 
Given the smaller data quantities for Hindi and Turkish , two training and validation set conÔ¨Ågurations were considered . 
The Ô¨Årst conÔ¨Åguration 1https://github.com/jgmÔ¨Åtz/stil-mbart-multiatisppaacl2020matched that of Xu et al . 
( 2020 ) , using training sets of 1,495 for Hindi and 626 for Turkish along with validation sets of 160 for Hindi and 60 for Turkish . 
In the second conÔ¨Åguration , no validation sets were made for Hindi and Turkish ( though there were still validation sets for the other languages ) , and the training sets of 1,600 Hindi samples and 638 samples from MultiATIS were used . 
Two output formats are considered , being ( 1 ) the non - translated , traditional case , in which translation of slot content is not performed , and ( 2 ) the translated , STIL case , in which translation of slot content is performed . 
In both cases , the tokens , the labels , the intent , and the detected language are all output from the model as a single ordered text sequence , as shown in Table 2 . 
3 Related Work Previous approaches for intent classiÔ¨Åcation and slot Ô¨Ålling have used either ( 1 ) separate models for slot Ô¨Ålling , including support vector machines ( Moschitti et al . 
, 2007 ) , conditional random Ô¨Åelds ( Xu and Sarikaya , 2014 ) , and recurrent neural networks of various types ( Kurata et al . 
, 2016 ) or ( 2 ) joint models that diverge into separate decoders or layers for intent classiÔ¨Åcation and slot Ô¨Ålling ( Xu and Sarikaya , 2013 ; Guo et al . 
, 2014 ; Liu and Lane , 2016 ; Hakkani - T ¬®ur et al . 
, 2016 ) or that share hidden states ( Wang et al . 
, 2018 ) . 
In this work , a fully text - to - text approach similar to that of the T5 model was used , such that the model would have maximum information sharing across the four STIL sub - tasks . 
Encoder - decoder models , Ô¨Årst introduced in 2014 ( Sutskever et al . 
, 2014 ) , are a mainstay of neural machine translation . 
The original transformer model included both an encoder and a decoder ( Vaswani et al . 
, 2017 ) . 
Since then , much of the work on transformers focuses on models with only an encoder pretrained with autoencoding techniques ( e.g. BERT by Devlin et al . 
( 2018 ) ) or auto - regressive models with only a decoder ( e.g.577 GPT by Radford ( 2018 ) ) . 
In this work , it was assumed that encoder - decoder models , such as BART ( Lewis et al . 
, 2019 ) and T5 ( Raffel et al . 
, 2019 ) , are the best architectural candidates given the translation component of the STIL task , as well as past state of the art advancement by encoder - decoder models on ATIS , cited above . 
Rigorous architectural comparisons are left to future work . 
4 The Model 4.1 The Pretrained mBART Model The multilingual BART ( mBART ) model architecture was used ( Liu et al . 
, 2020 ) , as well as the pretrained mBART.cc25 model described in the same paper . 
The model consists of 12 encoder layers , 12 decoder layers , a hidden layer size of 1,024 , and 16 attention heads , yielding a parameter count of 680M. The mBART.cc25 model was trained on 25 languages for 500k steps using a 1.4 TB corpus of scraped website data taken from Common Crawl ( Wenzek et al . 
, 2019 ) . 
The model was trained to reconstruct masked tokens and to rearrange scrambled sentences . 
SentencePiece tokenization ( Kudo and Richardson , 2018 ) was used for mBART.cc25 with a sub - word vocabulary size of 250k . 
4.2 This Work The same vocabulary as that of the pretrained model was used for this work , and SentencePiece tokenization was performed on the full sequence , including the slot tags , intent tags , and language tags . 
For all mBART experiments and datasets , data from all languages were shufÔ¨Çed together . 
The fairseq library was used for all experimentation ( Ott et al . 
, 2019 ) . 
Training was performed on 8 Nvidia V100 GPUs ( 16 GB ) using a batch size of 32 , layer normalization for both the encoder and the decoder ( Xu et al . 
, 2019 ) ; label smoothed cross entropy with /epsilon1= 0.2 ( Szegedy et al . 
, 2016 ) ; the ADAM optimizer with Œ≤1= 0.9andŒ≤2= 0.999(Kingma and Ba , 2014 ) ; an initial learning rate of 3√ó10‚àí5with polynomial decay over 20,000 updates after 1 epoch of warmup ; attention dropout of 0.1 and dropout of 0.2 elsewhere ; and FP16 type for weights . 
Each model was trained for 19 epochs , which took 5 - 6 hours . 
5 Results and Discussion Results from the models are given in Table 3 . 
Statistical signiÔ¨Åcance was evaluated using the Wilsonmethod ( Wilson , 1927 ) with 95 % conÔ¨Ådence . 
5.1 Comparing to Xu et al . 
( 2020 ) Examining the Ô¨Årst training conÔ¨Åguration ( 1,496 samples for Hindi and 626 for Turkish ) , the nontranslated mBART ‚Äôs macro - averaged intent classiÔ¨Åcation ( 96.07 % ) outperforms Cross - Lingual BERT by Xu et al . 
( 2020 ) ( 95.50 % ) , but slot F1 is worse ( 89.87 % for non - translated mBART and 90.81 % for Cross - Lingual BERT ) . 
The differences are statistically signiÔ¨Åcant in both cases . 
5.2 With and Without Translation When translation is performed ( the STIL task ) , intent classiÔ¨Åcation accuracy degrades by 1.7 % relative from 96.07 % to 94.40 % , and slot F1 degrades by 1.2 % relative from 89.87 % to 88.79 % . 
The greatest degradation occurred for utterances involving Ô¨Çight number , airfare , and airport name ( in that order ) . 
5.3 Additional Hindi and Turkish Training Data Adding 105 more Hindi and 12 more Turkish training examples results in improved performance for the translated , STIL mBART model . 
Macro - averaged intent classiÔ¨Åcation improves from 94.40 % to 95.94 % , and slot F1 improves from 88.79 % to 90.10 % , both of which are statistically signiÔ¨Åcant . 
By adding these 117 samples , the STIL mBART model matches the performance ( within conÔ¨Ådence intervals ) of the non - translated mBART model . 
This Ô¨Ånding suggests that the STIL models may require more training data than traditional , non - translated slot Ô¨Ålling models . 
Additionally , by adding more Hindi and Turkish data , both the intent accuracy and the slot Ô¨Ålling F1 improves for every individual language of the translated , STIL models , suggesting that some portion of the internal , learned representation is language agnostic . 
Finally , the results suggest that there is a trainingsize - dependent performance advantage in using a single output language , as contrasted with the nontranslated mBART model , for which the intent classiÔ¨Åcation accuracy and slot F1 does not improve ( with statistical signiÔ¨Åcance ) when using the additional Hindi and Turkish training samples . 
5.4 Language IdentiÔ¨Åcation Language identiÔ¨Åcation F1 is above 99.7 % for all languages , with perfect performance in many cases.578 Intent accuracy en es de zh fr hi tr Mac Avg Cross - Lingual BERT ( Xu et al . 
, 2020 ) 97.20 96.77 96.86 95.54 97.24 92.70 tr=149592.20 tr=62695.50 Seq2Seq - Ptr ( Rongali et al . 
, 2020 ) 97.42 Stack Propagation ( Qin et al . 
, 2019 ) 97.5 Joint BERT + CRF ( Chen et al . 
, 2019 ) 97.9 Non - translated mBART , with hi - tr val 96.98 96.98 97.09 96.08 97.65 95.07 tr=149592.73 tr=62696.07 Translated / STIL mBART , with hi - tr val 95.86 94.62 95.63 93.84 95.97 93.84 tr=149591.05 tr=62694.40 Non - translated mBART , no hi - tr val 97.09 97.20 97.20 96.30 97.42 94.74 tr=160094.27 tr=63896.32 Translated / STIL mBART , no hi - tr val 96.98 96.53 96.64 96.42 97.31 94.85 tr=160092.87 tr=63895.94 Slot F1 en es de zh fr hi tr Mac Avg Bi - RNN ( Upadhyay et al . 
, 2018 ) 95.2 80.6 tr=60078.9 tr=60084.90 Cross - Lingual BERT ( Xu et al . 
, 2020 ) 95.90 87.95 95.00 93.67 90.39 86.73 tr=149586.04 tr=62690.81 Stack Propagation ( Qin et al . 
, 2019 ) 96.1 Joint BERT ( Chen et al . 
, 2019 ) 96.1 Non - translated mBART , with hi - tr val 95.03 86.76 94.42 92.13 89.31 86.91 tr=149584.53 tr=62689.87 Translated / STIL mBART , with hi - tr val 93.81 90.38 91.41 85.93 91.24 83.98 tr=149584.79 tr=62688.79 Non - translated mBART , no hi - tr val 95.00 86.87 94.14 92.22 89.32 87.42 tr=160084.33 tr=63889.90 Translated / STIL mBART , no hi - tr val 94.66 91.55 92.61 87.73 92.15 86.74 tr=160085.23 tr=63890.10 Language IdentiÔ¨Åcation F1 en es de zh fr hi tr Mac Avg Translated / STIL mBART , with hi - tr val 100.00 98.87 100.00 100.00 98.95 100.00 99.93 99.68 Translated / STIL mBART , no hi - tr val 99.78 99.83 100.00 100.00 99.72 100.00 99.86 99.88 Table 3 : Results are shown for intent accuracy , slot F1 score , and language identiÔ¨Åcation F1 score . 
For English , Spanish , German , Chinese , and French in all of the models shown above ( including other work ) , training sets were between 4,478 and 4,488 samples , and validation sets were between 490 and 500 samples . 
In this work , two training set sizes were used for Hindi and Turkish , denoted by ‚Äú tr= ‚Äù and ‚Äú with hi - tr val[idation set ] ‚Äù or ‚Äú no hi - tr val[idation set ] ‚Äù . 
Across all work shown above , the tests sets contained 893 samples for all languages except Turkish , for which the test set was 715 samples . 
Perfect performance on Chinese and Hindi is unsurprising given their unique scripts versus the other languages tested . 
6 Conclusion This preliminary work demonstrates that a single NLU model can perform simultaneous slot Ô¨Ålling , translation , intent classiÔ¨Åcation , and language identiÔ¨Åcation across 7 languages using MultiATIS++ . 
Such an NLU model would negate the need for multiple - language support in some portion of downstream system components . 
Performance is not irreconcilably worse than traditional slot-Ô¨Ålling models , and performance is statistically equivalent with a small amount of additional training data . 
Looking forward , a more challenging dataset is needed to further develop the translation compo - nent of the STIL task . 
The English MultiATIS++ test set only contains 455 unique entity - slot pairs . 
An ideal future dataset would include freeform and varied content , such as text messages , song titles , or open - domain questions . 
Until then , work remains to achieve parity with English - only ATIS models . 
Acknowledgments The author would like to thank Saleh Soltan , Gokhan Tur , Saab Mansour , and Batool Haider for reviewing this work and providing valuable feedback . 
Abstract Simultaneous text translation and end - to - end speech translation have recently made great progress but little work has combined these tasks together . 
We investigate how to adapt simultaneous text translation methods such as wait - kand monotonic multihead attention to end - to - end simultaneous speech translation by introducing a pre - decision module . 
A detailed analysis is provided on the latency - quality trade - offs of combining Ô¨Åxed and Ô¨Çexible predecision with Ô¨Åxed and Ô¨Çexible policies . 
We also design a novel computation - aware latency metric , adapted from Average Lagging.1 1 Introduction Simultaneous speech translation ( SimulST ) generates a translation from an input speech utterance before the end of the utterance has been heard . 
SimulST systems aim at generating translations with maximum quality and minimum latency , targeting applications such as video caption translations and real - time language interpreter . 
While great progress has recently been achieved on both end - to - end speech translation ( Ansari et al . 
, 2020 ) and simultaneous text translation ( SimulMT ) ( Grissom II et al . 
, 2014 ; Gu et al . 
, 2017 ; Luo et al . 
, 2017 ; Lawson et al . 
, 2018 ; Alinejad et al . 
, 2018 ; Zheng et al . 
, 2019b , a ; Ma et al . 
, 2020 ; Arivazhagan et al . 
, 2019 , 2020 ) , little work has combined the two tasks together ( Ren et al . 
, 2020 ) . 
End - to - end SimulST models feature a smaller model size , greater inference speed and fewer compounding errors compared to their cascade counterpart , which perform streaming speech recognition followed by simultaneous machine translation . 
In addition , it has been demonstrated that end - to - end SimulST systems can have lower latency than cascade systems ( Ren et al . 
, 2020 ) . 
1The code is available at https://github.com/ pytorch / fairseqIn this paper , we study how to adapt methods developed for SimulMT to end - to - end SimulST . 
To this end , we introduce the concept of pre - decision module . 
Such module guides how to group encoder states into meaningful units prior to making a READ / WRITE decision . 
A detailed analysis of the latency - quality trade - offs when combining a Ô¨Åxed or Ô¨Çexible pre - decision module with a Ô¨Åxed or Ô¨Çexible policy is provided . 
We also introduce a novel computation - aware latency metric , adapted from Average Lagging ( AL ) ( Ma et al . 
, 2019 ) . 
2 Task formalization A SimulST model takes as input a sequence of acoustic features X= [ x1, ... x|X|]extracted from speech samples every Tsms , and generates a sequence of text tokens Y= [ y1, ... ,y|Y|]in a target language . 
Additionally , it is able to generate yi with only partial input X1 : n(yi)= [ x1, ... xn(yi ) ] , wheren(yi)‚â§|X|is the number of frames needed to generate the i - th target token yi . 
Note thatnis a monotonic function , i.e. n(yi‚àí1)‚â§n(yi ) . 
A SimulST model is evaluated with respect to quality , using BLEU ( Papineni et al . 
, 2002 ) , and latency . 
We introduce two latency evaluation methods for SimulST that are adapted from SimulMT . 
We Ô¨Årst deÔ¨Åne two types of delays to generate the wordyi , a computation - aware ( CA ) and a non computation - aware ( NCA ) delay . 
The CA delay ofyi , dCA(yi ) , is deÔ¨Åned as the time that elapses ( speech duration ) from the beginning of the process to the prediction of yi , while the NCA delay foryidCA(yi)is deÔ¨Åned by dNCA(yi ) = Ts¬∑n(yi ) . 
Note thatdNCAis an ideal case for dCAwhere the computational time for the model is ignored . 
Both delays are measured in milliseconds . 
Two types of latency measurement , LCAandLNCA , are calculated accordingly : L = C(D)whereCis a latency metric and D= [ d(y1), ... ,d ( y|Y|)].582 To better evaluate the latency for SimulST , we introduce a modiÔ¨Åcation to AL . 
We assume an oracle system that can perform perfect simultaneous translation for both latency and quality , while in Ma et al . 
( 2019 ) the oracle is ideal only from the latency perspective . 
We evaluate the lagging based on time rather than steps . 
The modiÔ¨Åed AL metric is deÔ¨Åned in Eq . 
( 1 ): AL=1 œÑ(|X|)œÑ(|X|)/summationdisplay i=1d(yi)‚àí|X| |Y‚àó|¬∑Ts¬∑(i‚àí1)(1 ) where|Y‚àó|is the length of the reference translation , œÑ(|X|)is the index of the Ô¨Årst target token generated when the model read the full input . 
There are two beneÔ¨Åts from this modiÔ¨Åcation . 
The Ô¨Årst is that latency is measured using time instead of steps , which makes it agnostic to preprocessing and segmentation . 
The second is that it is more robust and can prevent an extremely low and trivial value when the prediction is signiÔ¨Åcantly shorter than the reference . 
3 Method 3.1 Model Architecture End - to - end ST models directly map a source speech utterance into a sequence of target tokens . 
We use the S - Transformer architecture proposed by ( Di Gangi et al . 
, 2019b ) , which achieves competitive performance on the MuST - C dataset ( Di Gangi et al . 
, 2019a ) . 
In the encoder , a two - dimensional attention is applied after the CNN layers and a distance penalty is introduced to bias the attention towards short - range dependencies . 
We investigate two types of simultaneous translation mechanisms , Ô¨Çexible and Ô¨Åxed policy . 
In particular , we investigate monotonic multihead attention ( Ma et al . 
, 2020 ) , which is an instance of Ô¨Çexible policy and the preÔ¨Åx - to - preÔ¨Åx model ( Ma et al . 
, 2019 ) , an instance of Ô¨Åxed policy , designated by wait - kfrom now on . 
Monotonic Multihead Attention ( MMA ) ( Ma et al . 
, 2020 ) extends monotonic attention ( Raffel et al . 
, 2017 ; Arivazhagan et al . 
, 2019 ) to Transformer - based models . 
Each head in each layer has an independent step probability pijfor theith target andjth source step , and then uses a closed form expected attention for training . 
A weighted average and variance loss were proposed to control the behavior of the attention heads and thus the trade - offs between quality and latency . 
Wait - k(Ma et al . 
, 2019 ) is a Ô¨Åxed policy that waits forksource tokens , and then reads and writes alternatively . 
Wait- kcan be a special case of Monotonic InÔ¨Ånite - Lookback Attention ( MILk ) ( Arivazhagan et al . 
, 2019 ) or MMA where the stepwise probability pij= 0ifj‚àíi < k elsepij= 1 . 
Figure 1 : Simul - ST architecture with pre - decision module . 
Blue states in the Ô¨Ågure indicate the point Simul - SST model triggers the simultaneous making process 3.2 Pre - Decision Module In SimulMT , READ or WRITE decisions are made at the token ( word or BPE ) level . 
However , with speech input , it is unclear when to make such decisions . 
For example , one could choose to read or write after each frame or after generating each encoder state . 
Meanwhile , a frame typically only covers 10ms of the input while an encoder state generally covers 40ms of the input ( assuming a subsampling factor of 4 ) , while the average length of a word in our dataset is 270ms . 
Intuitively , a policy like wait- kwill not have enough information to write a token after reading a frame or generating an encoder state . 
In principle , a Ô¨Çexible or modelbased policy such as MMA should be able to handle granulawhile MMA is more robust tr input . 
Our analysis will show , however , that o the granularity of the input , it also performs poorly when the input is too Ô¨Åne - grained . 
In order to overcome these issues , we introduce the notion of pre - decision module , which groups frames or encoder states , prior to making a decision . 
A pre - decision module generates a series of trigger probabilities ptron each encoder states to indicate whether a simultaneous decision should be583 made . 
Ifptr>0.5 , the model triggers the simultaneous decision making , otherwise keeps reading new frames . 
We propose two types of pre - decision module . 
Fixed Pre - Decision A straightforward policy for a Ô¨Åxed pre - decision module is to trigger simultaneous decision making every Ô¨Åxed number of frames . 
Let ‚àÜtbe the time corresponding to this Ô¨Åxed number of frames , with ‚àÜta multiple of Ts , andre = int(|X|/|H|).ptrat encoder step jis deÔ¨Åned in Eq . 
( 2 ): ptr(j ) = /braceleftBigg 1if mod ( j¬∑re¬∑Ts,‚àÜt ) = 0 , 0Otherwise.(2 ) Flexible Pre - Decision We use an oracle Ô¨Çexible pre - decision module that uses the source boundaries either at the word or phoneme level . 
Let Abe the alignment between encoder states and source labels ( word or phoneme ) . 
A(hi)represents the token thathialigns to . 
The trigger probability can then be deÔ¨Åned in Eq . 
( 3 ): ptr(j ) = /braceleftBigg 0ifA(hj ) = A(hj‚àí1 ) 1Otherwise.(3 ) 4 Experiments We conduct experiments on the English - German portion of the MuST - C dataset ( Di Gangi et al . 
, 2019a ) , where source audio , source transcript and target translation are available . 
We train on 408 hours of speech and 234k sentences of text data . 
We use Kaldi ( Povey et al . 
, 2011 ) to extract 80 dimensional log - mel Ô¨Ålter bank features , computed with a 25mswindow size and a 10 mswindow shift . 
For text , we use SentencePiece ( Kudo and Richardson , 2018 ) to generate a unigram vocabulary of size 10,000 . 
We use Gentle2to generate the alignment between source text and speech as the label to generate the oracle Ô¨Çexible predecision module . 
Translation quality is evaluated with case - sensitive detokenized BLEU with SACRE BLEU ( Post , 2018 ) . 
The latency is evaluated with our proposed modiÔ¨Åcation of AL ( Ma et al . 
, 2019 ) . 
All results are reported on the MuSTC dev set . 
All speech translation models are Ô¨Årst pretrained on the ASR task where the target vocabulary is character - based , in order to initialize the 2https://lowerquality.com/gentle/encoder . 
We follow the same hyperparameter settings from ( Di Gangi et al . 
, 2019b ) . 
We follow the latency regularization method introduced by ( Ma et al . 
, 2020 ; Arivazhagan et al . 
, 2019 ) , The objective function to optimize is L=‚àílog(P(Y|X ) ) + Œªmax(C(D),0 ) ( 4 ) WhereCis a latency metric ( AL in this case ) and Dis described in Section 2 . 
Only samples with AL>0are regularized to avoid overÔ¨Åtting . 
For the models with monotonic multihead attention , we Ô¨Årst train a model without latency with Œªlatency = 0 . 
After the model converges , Œªlatency is set to a desired value and the model is continue trained until convergence . 
The latency - quality trade - offs of the 4 types of model from the combination of Ô¨Åxed or Ô¨Çexible predecision with Ô¨Åxed or Ô¨Çexible policy are presented in Fig . 
2 . 
The non computation - aware delays are used to calculate the latency metric in order to evaluate those trade - offs from a purely algorithmic perspective . 
Fixed Pre - Decision + Fixed Policy3(Fig . 
2a ) . 
As expected , both quality and latency increase with step size and lagging . 
In addition , the latencyquality trade - offs are highly dependent on the step size of the pre - decision module . 
For example , with step size 120ms , the performance is very poor even with largekbecause of very limited information being read before writing a target token . 
Large step sizes improve the quality but introduce a lower bound on the latency . 
Note that step size 280ms , which provides an effective latency - quality tradeoff compared to other step sizes , also matches the average word length of 271ms . 
This motivates the study of a Ô¨Çexible pre - decision module based on word boundaries . 
Fixed Pre - Decision + Flexible Policy4(Fig . 
2b ) Similar to wait- k , MMA obtains very poor performance with a small step size of 120ms . 
For other step sizes , MMA obtains similar latency - quality trade - offs , demonstrating some form of robustness to the step size . 
Flexible Pre - Decision Curve‚ãÜand in Ô¨Ågure Fig . 
2 show latency - quality trade - offs when the predecision module is determined by oracle word or phoneme boundaries . 
Note that a SimulST model 3k= 1,2,3,4,5,6,7,8,9,10 4Œª= 0.001,0.004,0.01,0.02,0.04,0.06,0.08,0.1584 1000 2000 3000 4000 5000 AL0246810121416BLEU ( a)Wait - k 1000 2000 3000 4000 5000 AL46810121416BLEU Step size : 120ms Step size : 200ms Step size : 280ms Step size : 360ms Step size : 440ms Step size : word Step size : phoneme ( b)MMA Figure 2 : Latency - Quality trade - off curves . 
The unit of AL is millisecond 1000 2000 3000 4000 5000 6000 AL46810121416BLEU Fixed PD + wait - k Fixed PD + MMA Flexible PD ( oracle word ) + wait - k Flexible PD ( oracle word ) + MMA Figure 3 : Comparison of best models in four settings 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 AL0246810121416BLEU Step size : 120ms Step size : 200ms Step size : 280ms Step size : 360ms Step size : 440ms Figure 4 : Computation - aware latency for Ô¨Åxed pre - decision + wait - kpolicy . 
Points on dotted lines are computation - aware , without lines are non - computation - aware would not normally have access to this information and that the purpose of this experiment is to guide future design of a Ô¨Çexible pre - decision model . 
First , as previously observed , the granularity of the pre - decision greatly inÔ¨Çuences the latency - quality trade - offs . 
Models using phoneme boundaries obtain very poor translation quality because those boundaries are too granular , with an average phoneme duration of 77ms . 
In addition , comparing MMA and wait- kwith phoneme boundaries , MMA is found to be more robust to the granularity of the pre - decision . 
Best Curves The best settings for each approachare compared in Fig . 
3 . 
For Ô¨Åxed pre - decision , we choose the setting that has the best quality for each latency bucket of 500ms , while for the Ô¨Çexible pre - decision we use oracle word boundaries . 
For both wait - kand MMA , the Ô¨Çexible pre - decision module outperforms the Ô¨Åxed pre - decision module . 
This is expected since the Ô¨Çexible pre - decision module uses oracle information in the form of precomputed word boundaries but provides a direction for future research . 
The best latency - quality trade - offs are obtained with MMA and Ô¨Çexible predecision from word boundaries . 
4.1 Computation Aware Latency We also consider the computation - aware latency described in Section 2 , shown in Fig . 
4 . 
The focus is on Ô¨Åxed pre - decision approaches in order to understand the relation between the granularity of the pre - decision and the computation time . 
Fig . 
4 shows that as the step size increases , the difference between the NCA and the CA latency shrinks . 
This is because with larger step sizes , there is less overhead of recomputing the bidirectional encoder states5 . 
We recommend future work on SimulST to make use of CA latency as it reÔ¨Çects a more realistic evaluation , especially in low - latency regimes , and is able to distinguish streaming capable systems . 
5 Conclusion We investigated how to adapt SimulMT methods to end - to - end SimulST by introducing the concept of pre - decision module . 
We also adapted Average Lagging to be computation - aware . 
The effects of combining a Ô¨Åxed or Ô¨Çexible pre - decision module 5This is a common practice in SimulMT where the input length is signiÔ¨Åcantly shorter than in SimulST ( Arivazhagan et al . 
, 2019 ; Ma et al . 
, 2019 ; Arivazhagan et al . 
, 2020)585 with a Ô¨Åxed or Ô¨Çexible policy were carefully analyzed . 
Future work includes building an incremental encoder to reduce the CA latency and design a learnable pre - decision module . 
Abstract Automatically generating stories is a challenging problem that requires producing causally related and logical sequences of events about a topic . 
Previous approaches in this domain have focused largely on one - shot generation , where a language model outputs a complete story based on limited initial input from a user . 
Here , we instead focus on the task of interactive story generation , where the user provides the model mid - level sentence abstractions in the form of cue phrases during the generation process . 
This provides an interface for human users to guide the story generation . 
We present two content - inducing approaches to effectively incorporate this additional information . 
Experimental results from both automatic and human evaluations show that these methods produce more topically coherent and personalized stories compared to baseline methods . 
1 Introduction Automatic story generation requires composing a coherent and Ô¨Çuent passage of text about a sequence of events . 
Prior studies on story generation mostly focused on symbolic planning ( Lebowitz , 1987 ; P ¬¥ erez y P ¬¥ erez and Sharples , 2001 ; Porteous and Cavazza , 2009 ; Riedl and Young , 2010 ) or case - based reasoning ( Gerv ¬¥ as et al . 
, 2005 ) that heavily relied on manual knowledge engineering . 
Recent state - of - the - art methods for story generation ( Martin et al . 
, 2018 ; Clark et al . 
, 2018a ) are based on sequence - to - sequence models ( Sutskever et al . 
, 2014 ) that generate a story in one go . 
In this setting , the user has little control over the generated story . 
On the other hand , when humans write , they incrementally edit and reÔ¨Åne the text they produce . 
Motivated by this , rather than generating the entire story at once , we explore the problem of interactive story generation . 
In this setup , a user can provide Figure 1 : Interactive story generation : the user inputs the Ô¨Årst sentence of the story ( prompt ) , and provides guiding cue phrases as the system generates the story one sentence at a time . 
the model mid - level sentence abstractions in the form of cue phrases as the story is being generated . 
Cue phrases enable the user to inform the system of what they want to happen next in the story and have more control over what is being generated . 
To achieve our goal , this paper primarily focuses on approaches for smoothly and effectively incorporating user - provided cues . 
The schematic in Fig . 
1 illustrates this scenario : the system generates the story one sentence at a time , and the user guides the content of the next sentence using cue phrases . 
We note that the generated sentences need to Ô¨Åt the context , and also be semantically related to the provided cue phrase . 
A fundamental advantage of using this framework as opposed to a fully automated one is that it can provide an interactive interface for human users to incrementally supervise the generation by giving signals to the model throughout the story generation process . 
This human - computer collaboration can result in generating richer and personalized stories . 
In particular , this Ô¨Åeld of research can be used in addressing the literacy needs of learners with disabilities and enabling children to explore588 creative writing at an early age by crafting their own stories . 
In this paper , we present two content - inducing approaches based on the Transformer Network ( Vaswani et al . 
, 2017 ) for interactively incorporating external knowledge when automatically generating stories . 
Here , our external knowledge is in the form of cue phrases provided by the user to enable interaction , but can readily be replaced with knowledge accessible through other means1 . 
SpeciÔ¨Åcally , our models fuse information from the story context and cue phrases through a hierarchical attention mechanism . 
The Ô¨Årst approach , Cued Writer , employs two independent encoders ( for incorporating context and cue phrases ) and an additional attention component to capture the semantic agreement between the cue phrase and output sentence . 
The second approach , Relevance Cued Writer , additionally measures the relatedness between the context and cue phrase through a contextcue multi - head unit . 
In both cases , we introduce different attention units in a single end - to - end neural network . 
Our automatic and human evaluations demonstrate that the presented models outperform strong baselines and can successfully incorporate cues in generated stories . 
This capability is one step closer to an interactive setup , and unlike one - shot generation , it lets users have more control over the generation . 
Our contributions are twofold : ‚Ä¢Two novel content - inducing approaches to incorporate additional information , in this case cue phrases , into the generation phase . 
‚Ä¢Experiments demonstrating utility of contentinducing approaches using automatic and human evaluations . 
2 Related Work Automatic story generation is a longstanding problem in AI , with early work dating back to the 1970s based on symbolic planning ( Lebowitz , 1987 ; P¬¥erez y P ¬¥ erez and Sharples , 2001 ; Porteous and Cavazza , 2009 ; Riedl and Young , 2010 ) and casebased reasoning using ontologies ( Gerv ¬¥ as et al . 
, 2005 ) . 
Li et al . 
( 2013 ) extended prior works toward learning domain models ( via corpus and/or crowdsourcing ) to support open story generation about any topic . 
1For example , the user - provided cues can be replaced by the outputs of an automatic planner . 
Our models are Ô¨Çexible enough to work in other setups . 
With the advent of deep learning there has been a major shift towards using seq2seq models ( Sutskever et al . 
, 2014 ; Bahdanau et al . 
, 2015 ) for various text generation tasks , including storytelling ( Roemmele , 2016 ; Jain et al . 
, 2017 ; Hu et al . 
, 2020 ) . 
However , these models often fail to ensure coherence in the generated story . 
To address this problem , Clark et al . 
( 2018a ) incorporated entities given their vector representations , which get updated as the story unfolds . 
Similarly , Liu et al . 
( 2020 ) proposed a character - centric story generation by learning character embeddings directly from the corpus . 
Fan et al . 
( 2018 ) followed a twostep process to Ô¨Årst generate the premise and then condition on that to generate the story . 
Yu et al . 
( 2020 ) proposed a multi - pass CV AE to improve wording diversity and content consistency . 
Previous work has explored the potential of creative writing with a machine in the loop . 
Clark et al . 
( 2018b ) found that people generally enjoy collaborating with a machine . 
Traditional methods proposed to write stories collaboratively using a case - based reasoning architecture ( Swanson and Gordon , 2012 ) . 
Recent work ( Roemmele and Gordon , 2015 ) extended this to Ô¨Ånd relevant suggestions for the next sentence in a story from a large corpus . 
Other methods proposed GUI and tools to facilitate co - creative narrative generation ( Manjavacas et al . 
, 2017 ; Kapadia et al . 
, 2015 ) . 
Unlike us , these approaches explore the value of and tools for interaction rather than designing methods for incorporating user input into the model . 
Another line of research decomposes story generation into two steps : story plot planning and plot - to - surface generation . 
Previous work produces story - plans based on sequences of events ( Martin et al . 
, 2018 ; Tambwekar et al . 
, 2019 ; Ammanabrolu et al . 
, 2020 ) , critical phrases ( Xu et al . 
, 2018 ) or both events and entities ( Fan et al . 
, 2019 ) . 
Yao et al . 
( 2019 ) model the story - plan as a sequence of keywords . 
They proposed Static andDynamic paradigms that generate a story based on these story - plans . 
Goldfarb - Tarrant et al . 
( 2019 ) adopted thestatic model proposed in Yao et al . 
( 2019 ) to supervise story - writing . 
A major focus of these works is on generating a coherent plan for generating the story . 
In contrast , our contribution is complementary since we do not focus on planning but on generation . 
We present approaches to effectively incorporate external knowledge in the form of cue - phrases during589 Figure 2 : Overall model architecture for Cued Writer andRelevance Cued Writer . 
generation , and conduct extensive experiments to compare our models with those of Yao et al . 
( 2019 ) by modifying them to work in our setup . 
3 Interactive Story Generation We design models to generate a story one sentence at a time . 
Given the generated context so far ( as a sequence of tokens ) X={x1, ... ,xT } , and the cue phrase for the next sentence c={c1, ... ,cK } , our models generate the tokens of the next sentence of the storyY={y1, ... ,yM } . 
We train the models by minimizing the cross - entropy loss : LŒ∏=‚àíM / summationdisplay i=1logP(yi|X , c , Œ∏ ) ( 1 ) Here , Œ∏refers to model parameters . 
Note that when generating the n - th sentence , the model takes the Ô¨Årstn‚àí1sentences in the story as the context along with the cue phrase . 
In the rest of this section , we describe our two novel content - inducing approaches for addressing the interactive story generation task : the Cued Writer , and the Relevance Cued Writer . 
These models share an overall encoder - decoder based architecture shown in Fig . 
2 . 
They adopt a dual encoding approach where two separate but architecturally similar encoders are used for encoding the context ( Context Encoder represented in the green box ) and the cue phrase ( Cue Encoder represented in the purple box ) . 
Both these encoders Figure 3 : ( a)Encoder Block consists of MultiHead and FFN . 
( b)MultiHead Attention . 
( c)Attention Module . 
advise the Decoder ( represented in the blue box ) , which in turn generates the next sentence . 
The two proposed models use the same encoding mechanism ( described in ¬ß 3.1 ) and differ only in their decoders ( described in ¬ß 3.2 ) . 
3.1 Encoder Our models use the Transformer encoder introduced in Vaswani et al . 
( 2017 ) . 
Here , we provide a generic description of the encoder architecture followed by the inputs to this architecture for the Context and Cue Encoders in our models . 
Each encoder layer lcontains architecturally identical Encoder Blocks , referred to as ENCBLOCK ( with unique trainable parameters ) . 
Fig . 
3(a ) shows an Encoder Block which consists of a Multi - Head attention and an FFN that applies the following operations : Àúol= M ULTIHEAD(hl‚àí1 ) ( 2a ) ol= LAYER NORM(Àúol+hl‚àí1 ) ( 2b ) Àúhl= FFN(ol ) ( 2c ) hl= LAYER NORM(Àúhl+ol ) ( 2d ) Where MULTI HEAD represents Multi - Head Attention ( described below ) , FFN is a feed - forward neural network with ReLU activation ( LeCun et al . 
, 2015 ) , and LAYER NORM is a layer normalization ( Ba et al . 
, 2016 ) . 
In the rest of the paper , LAYER NORM ( also shown as Add & Norm in Ô¨Ågures ) is always applied after MULTI HEAD and FFN , but we do not explicitly mention that in text or equations for simplicity . 
Multi - Head Attention The multi - head attention , shown in Fig . 
3(b ) , is similar to that used in Vaswani et al . 
( 2017 ) . 
It is made of multiple Attention heads , shown in Fig . 
3(c ) . 
The Attention head has three types of inputs : the query sequence,590 Q‚ààRnq√ódk , the key sequence , K‚ààRnk√ódk , and the value sequence , V‚ààRnv√ódk . 
The attention module takes each token in the query sequence and attends to tokens in the key sequence using a scaled dot product . 
The score for each token in the key sequence is then multiplied by the corresponding value vector to form a weighted sum : ATTN(Q , K , V ) = softmax / parenleftbiggQKT ‚àödk / parenrightbigg V ( 3 ) For each head , all Q , K , andVare passed through a head - speciÔ¨Åc projection prior to the attention being computed . 
The output of a single head is : Hi= A TTN(QWQ i , KWK i , VWV i ) ( 4 ) WhereWs are head - speciÔ¨Åc projections . 
Attention headsHiare then concatenated : MULTIH(Q , K , V ) = [ Hi; ... ;Hm]WO(5 ) WhereWOis an output projection . 
In the encoder , all query , key , and value come from the previous layer and thus : MULTIHEAD(hl‚àí1 ) = M ULTIH(hl‚àí1,hl‚àí1,hl‚àí1 ) ( 6 ) Encoder Input The Encoder Blocks described above form the constituent units of the Context and Cue Encoders , which process the context and cue phrase respectively . 
Each token in the context , xi , and cue phrase , ci , is assigned two kinds of embeddings : token embeddings indicating the meaning andposition embeddings indicating the position of each token within the sequence . 
These two are summed to obtain individual input vectors , X0 , and c0 , which are then fed to the Ô¨Årst layer of Context and Cue encoders , respectively . 
Thereafter , new representations are constructed through layers of encoder blocks : Xl+1= ENCBLOCK ( Xl , Xl , Xl ) ( 7a ) cl+1= ENCBLOCK ( cl , cl , cl ) ( 7b ) wherel‚àà[0,L‚àí1]denotes different layers . 
In Eqn . 
7a and 7b , the output of the previous layer ‚Äôs Encoder Block is used as Q , K , andVinput for the multi - head attention of the next block . 
3.2 Content - Inducing Decoders We now describe the decoders for our models . 
Cued Writer The main intuition behind our Ô¨Årst model , Cued Writer , is that since cue phrases ( a)Cued Writer ( b)Rel . 
Cued Writer Figure 4 : Decoder architectures . 
XLandcLare the outputs of the top - layers of the Context and Cue encoders respectively , and KandVare the corresponding keys and values . 
indicate users ‚Äô expectations of what they want to see in the next sentence of the story , they should be used by the model at the time of generation , i.e. , in the decoder . 
Below , we describe the decoder used by the Cued Writer . 
After processing the two types of inputs in the Context and Cue Encoders , the model includes their Ô¨Ånal encoded representations ( XLandcL ) in the decoder . 
The decoder consists of Llayers with architecturally identical Decoder Blocks . 
Each Decoder Block contains Enc - Dec MultiHead and the Cue MultiHead units ( see Fig . 
4(a ) ) , which let the decoder to focus on the relevant parts of the context and the cue phrase , respectively . 
GivenY0as the word - level embedding represen-591 tation for the output sentence , our Decoder Block is formulated as : Yl+1 self= M ULTIH(Yl , Yl , Yl ) ( 8a ) Yl+1 dec= M ULTIH(Yl+1 self , XL , XL ) ( 8b ) Yl+1 cued= M ULTIH(Yl+1 self , cL , cL ) ( 8c ) Eqn . 
8a is standard self - attention , which measures the intra - sentence agreement for the output sentence and corresponds to the MultiHead unit in Fig . 
4(a ) . 
Eqn . 
8b , describing the Enc - Dec MultiHead unit , measures the agreement between context and output sentence , where queries come from the decoder Multi - Head unit ( Yself ) , and the keys and values come from the top layer of the context encoder ( XL ) . 
Similarly , Eqn . 
8c captures the agreement between output sentence and cue phrase through Cue MultiHead unit . 
Here , keys and values come from the top layer of the Cue encoder ( cL ) . 
Lastly , we adapt a gating mechanism ( Sriram et al . 
, 2018 ) to integrate the semantic representations from both YdecandYcuedand pass the resulting output to FFN function : gl+1 = œÉ(W1[Yl+1 dec;Yl+1 cued ] ) ( 9a ) Yl+1 int = W2(gl+1 ‚ó¶ [Yl+1 dec;Yl+1 cued ] ) ( 9b ) Yl+1= FFN(Yl+1 int ) ( 9c ) the representation from YdecandYcuedare concatenated to learn gates , g. The gated hidden layers are combined by concatenation and followed by a linear projection with the weight matrix W2 . 
Relevance Cued Writer The decoder of Cued Writer described above captures the relatedness of the context and the cue phrase to the generated sentence but does not study the relatedness or relevance of the cue phrase to the context . 
We incorporate this relevance in the decoder of our next model , Relevance Cued Writer . 
Its Decoder Block ( shown in Fig . 
4(b ) ) is similar to that of Cued Writer except for two additional units : the Context - Cue and Relevance MultiHead units . 
The intuition behind theContext - Cue MultiHead unit ( Eqn . 
10a ) is to characterize the relevance between the context and the cue phrase , so as to highlight the effect of words in the cue phrase that are more relevant to the context thereby promoting topicality and Ô¨Çuency . 
This relevance is then provided to the decoder using theRelevance MultiHead unit ( Eqn . 
10b ): Xl+1 rel= M ULTIH(XL , cL , cL ) ( 10a ) Yl+1 rel= M ULTIH(Yl+1 self , Xl+1 rel , Xl+1 rel)(10b)We fuse the information from all three sources using a gating mechanism and pass the result to FFN : gl+1 = œÉ(W1[Yl+1 dec;Yl+1 cued;Yl+1 rel ] ) ( 11a ) Yl+1 int = W2(gl+1 ‚ó¶ [Yl+1 dec;Yl+1 cued;Yl+1 rel])(11b ) Yl+1= FFN(Yl+1 int ) ( 11c ) Finally , for both models , a linear transformation and a softmax function ( shown in Fig . 
2 ) is applied to convert the output produced by the stack of decoders to predicted next - token probabilities : P(yi|y < i , X , c , Œ∏ ) = softmax ( YL iWy)(12 ) whereP(yi|y < i , X , c , Œ∏ ) is the likelihood of generatingyigiven the preceding text ( y < i ) , context and cue , andWyis the token embedding matrix . 
4 Empirical Evaluation 4.1 Dataset We used the ROCStories corpus ( Mostafazadeh et al . 
, 2016 ) for experiments . 
It contains 98,161 Ô¨Åve - sentence long stories with a rich set of causal / temporal sequences of events . 
We held out 10 % of stories for validation and 10 % for test set . 
4.2 Baselines SEQ2SEQ Our Ô¨Årst baseline is based on a LSTM sentence - to - sentence generator with attention ( Bahdanau et al . 
, 2015 ) . 
In order to incorporate userprovided cue phrases , we concatenate context and cue phrase with a delimiter token ( < $ > ) before passing it to the encoder . 
DYNAMIC This is the Dynamic model proposed by Yao et al . 
( 2019 ) modiÔ¨Åed to work in our setting . 
For a fair comparison , instead of generating a plan , we provide the model with cue phrases and generate the story one sentence at a time . 
STATIC TheSTATIC model ( Yao et al . 
, 2019 ) gets all cue phrases at once to generate the entire story2 . 
By design , it has additional access to all , including future , cue phrases . 
Our models and other baselines do not have this information . 
VANILLA To verify the effectiveness of our content - inducing approaches , we use a Vanilla Transformer as another baseline and concatenate context and cue phrase using a delimiter token . 
2We used the implementation available at : https:// bitbucket.org/VioletPeng/language-model/592 Models PPL ( ‚Üì)BLEU-1 ( ‚Üë)BLEU-2 ( ‚Üë)BLEU-3 ( ‚Üë)GM ( ‚Üë)Repetition-4 ( ‚Üì ) DYNAMIC ( Yao et al . 
, 2019 ) 29.49 30.05 9.16 4.59 0.73 44.36 STATIC ( Yao et al . 
, 2019 ) 20.81 33.25 9.64 4.77 0.75 26.26 SEQ2SEQ 20.97 33.91 10.01 3.09 0.82 33.23 VANILLA 15.78 40.30 16.09 7.19 0.89 20.87 Cued Writer 14.80 41.50 16.72 7.25 0.92 15.08 Rel . 
Cued Writer 14.66 42.65 17.33 7.59 0.94 16.23 Table 1 : Automatic evaluation results . 
Our models outperform all baselines across all metrics ( p<0.05 ) . 
4.3 Training details Following previous work ( Vaswani et al . 
, 2017 ) , we initialize context encoders and decoders with 6layers ( 512dimensional states and 8attention heads ) . 
Our models contain 3 - layer encoders for encoding cue phrases ( all other speciÔ¨Åcations are the same ) . 
For the position - wise feed - forward networks , we use 2048 dimensional inner states . 
We use the Adam optimizer ( Kingma and Ba , 2015 ) with a learning rate of 0.0001 and residual , embedding , and attention dropouts with a rate of 0.1 for regularization . 
Models are implemented in PyTorch , trained for 30epochs with early stopping on validation loss . 
Cue - phrases for Training and Automatic Evaluation : For training all models , we need cue phrases , which are , in principle , to be entered by a user . 
However , to scale model training , we automatically extracted cue phrases from the target sentences in the training set using the previously proposed RAKE algorithm ( Rose et al . 
, 2010 ) . 
It is important to note that cue phrases can represent a variety of information , and many other methods can be used to extract them for training purposes . 
For example , topic words , distinctive entities or noun phrases in the sentence , the headword in the dependency parse of the sentence , etc . 
Our automatic evaluations were done on a largescale , and so we followed a similar approach for extracting cue - phrases . 
Cue - phrases for Human Evaluation : In the interest of evaluating the interactive nature of our models , cue - phrases were provided manually during our interactive evaluations3 . 
General Statistics on Cue - phrases : Automatically extracted cue phrases has the vocabulary size of22,097 , and 6,189on the train and test set , respectively with the average 10 % coverage over the entire target sentence . 
Cue - phrases are typically 1 - 2 words . 
Comparing user - provided vs automati3We left the deÔ¨Ånition of cue - phrase open - ended to enable Ô¨Çexibility in user interaction . 
They are typically 1 - 2 words . 
Figure 5 : Inter - story ( left ) and Intra - story ( right ) repetition scores . 
The proposed models have better scores . 
cally extracted cue - phrases , the average length of user - provided cue - phrases in interactive evaluation is1.56 , with a vocabulary size of 206 , whereas these numbers are 1.59and214for their corresponding automatically extracted cue phrases . 
4.4 Automatic Evaluation Following previous credible works ( Martin et al . 
, 2018 ; Fan et al . 
, 2018 ) , we compare various methods using Perplexity and BLEU ( Papineni et al . 
, 2002 ) on the test set . 
We reported BLEU - n for n=1,2,3 . 
From Table 1 , we can see that both our models outperform DYNAMIC andSTATIC by large margins on perplexity and BLEU scores . 
The proposed models are also superior to the SEQ2SEQ andVANILLA baseline on both measures . 
Comparing the last two rows of Table 1 , we also see an additive gain from modeling the relevance in Rel . 
Cued Writer . 
All improvements are statistically signiÔ¨Åcant ( approximate randomization ( Noreen , 1989),p<0.05 ) . 
To evaluate how well the story generation model incorporates the cues , we use an embedding - based greedy matching score ( GM ) ( Liu et al . 
, 2016 ) . 
The score measures the relatedness of the generated story with cues by greedily matching them with each token in a story based on the cosine similarity of their word embeddings ( Yao et al . 
, 2019 ) . 
We can see from the 5th column in Table 1 that our models generate stories that are more related to the cue phrases.593 Prompt ( Ô¨Årst sentence ): Jordan was watching TV on her couch . 
Cue phrases : watch football - change channel - comedy show - very funny She was trying to watch football on TV . 
Then she went to change channel . 
Finally , she decided to watch a comedy show . 
She saw the comedy that was playing and did n‚Äôt like . 
Cue phrases : soccer - cook - order pizza - tasty dinner Her brother was playing in a soccer . 
She was n‚Äôt able to cook . 
Instead , she ordered pizza . 
Her brother was happy with the tasty dinner . 
Table 2 : Example of stories generated in interactive evaluation using two models given the same prompt and different set of cue - phrase . 
Previous works have shown that neural generation models suffer from repetition issue ; and so we additionally evaluate the models using repetition-4 which measures the percentage of generated stories that repeat at least one 4 - gram ( Shao et al . 
, 2019 ) and inter- and intra - story repetition scores ( Yao et al . 
, 2019 ) . 
A lower value is better for these scores . 
The result of repetition-4 is reported in the last column of Table 1 . 
The proposed models signiÔ¨Åcantly outperform all baselines , and among the twoCued Writer is better . 
Inter and intra repetition scores are depicted in Fig . 
5 . 
Our two proposed models are almost comparable on these metrics but they show a general superior performance compared to all baselines . 
In particular , Rel . 
Cued Writer achieves a signiÔ¨Åcant performance increase of16 % and46 % on these scores over the stronger model of Yao et al . 
( 2019)4 . 
4.5 Human Evaluation Automatic metrics can not evaluate all aspects of open - ended text generation ( Fan et al . 
, 2018 ) , and so we also conduct several human evaluations . 
Interactive Evaluation In this experiment , human subjects compare our best model , Rel . 
Cued Writer , with the strongest baseline from the automatic evaluations ( VANILLA ) in an interactive , real - time setup . 
For robust evaluation , it is essential that the users generate a wide variety of stories . 
Since generating different prompts ( Ô¨Årst sentence ) requires creativity on the part of human judges and can be challenging , we provided participants with initial prompts that were randomly selected from the test set . 
For each prompt , the participants generated stories using both models by interactively provid4Note that the result of our SEQ2SEQbaseline is not directly comparable with that of Inc - S2S in ( Yao et al . 
, 2019 ) , since we included cue phrases as additional input whereas Inc - S2S generate the whole story conditioned on the title . 
( a )   ( b ) Figure 6 : Human evaluations on story - level ( left ) and sentence - level ( right ) . 
We Ô¨Ånd that human judges preferred stories generated by Rel . 
Cued Writer . 
ing cue - phrases5 . 
They were then asked to choose which story they prefer . 
Participants preferred stories generated by Rel . 
Cued Writer over VANILLA in57.5%of the cases ( 80stories in total , p‚àº0.1 ) . 
Judges also rated the stories in terms of Ô¨Çuency and coherence on a 5 - point Likert scale . 
Rel . 
Cued Writer achieved a higher Ô¨Çuency score of 4.22compared with 3.80achieved by VANILLA .VANILLA attained a slightly higher coherence score ( 3.40 vs.3.35 ) . 
On manually inspecting the generated stories , we found that our model generates longer sentences ( avg . 
9.18words ) with more complex language , whereas VANILLA generated relatively shorter sentences ( avg . 
7.46words ) which might improve coherence . 
This experiment is promising but inconclusive because for the same prompt , the participants could provide different sets of cue - phrases for different models , resulting in generated stories that are too different to be comparable ( Table 2 shows an example ) . 
This led us to conduct the following more controlled evaluations . 
Story - level Evaluation In this experiment , we again make pairwise comparisons , but both models are provided the same prompts , and sets of cue phrases6.3judges evaluated 100pairs of stories ( in shufÔ¨Çed order)7 . 
Fig . 
6(a ) shows the percentage of preference for our stronger model , Rel . 
Cued Writer , over the baselines . 
Judges prefer our model over all other baselines . 
Also , judges preferred Rel . 
Cued Writer over Cued Writer , which demonstrates the effec5We included instructions and examples for participants . 
The order of the presentation of the models was random . 
The judges were self - identiÔ¨Åed native English speakers . 
6Cue - phrases were extracted automatically to alleviate any biases , then curated manually . 
7For all AMT evaluations , we only allow judges who have at least 500previous jobs , and at least 97 % approval rate.594 Prompt I was coughing very badly . 
|Cue phrases notice blood - emergency room - waited - okay DYNAMIC I noticed that I had a headache . 
I went to the doctor . 
The doctor told me that I had to go to the doctor . 
The doctor said I was Ô¨Åne . 
[ waited ] STATIC Shenoticed a huge blood pressure . 
She went to the doctor . 
The doctor told her to stop . 
She waited for a while . 
[ okay ] SEQ2SEQI noticed blood and noticed blood in the closet . 
Now I have emergency room foremergency room . 
I found it and I waited until the next day . 
I feel guilty and now i do n‚Äôt have to do it again . 
[ okay ] VANILLA I noticed blood in the bathroom . 
I had to go to the emergency room . 
I waited in the bathroom for a while . 
Luckily the doctor was okay . 
Cued Writer I noticed blood in my mouth . 
I had to go to the emergency room . 
I waited there a few hours . 
I felt okay . 
Rel Cued Writer I noticed blood on my shirt . 
I went to the emergency room . 
I waited for the doctor . 
He came and I was okay . 
Prompt Harry really wanted to see the new Star Wars movie . 
|Cue phrases opening day - ticket - entire movie - really DYNAMIC He went to the theater to show up . 
He bought a ticket . 
He had a great time . 
He had a great time . 
[ opening day ] [ entire movie ] STATICOne day , shewas in a hurry to get to the theater . 
she was n‚Äôt sure if she would like to go . 
She was excited to try the new movie . 
She was so excited to see the movie . 
[ opening day ] [ ticket ] SEQ2SEQThe day daywas opening day and his family was opening theopening day . 
Harry had bought a ticket and the ticket was n‚Äôt very good . 
The entire movie was very happy . 
Harry became very really disappointed . 
VANILLAOn opening day , Harry was very nervous . 
He bought a ticket tothe theater . 
He bought Harry ticket tickets to the theater . 
He really did n‚Äôt like the movie . 
[ entire movie ] Cued Writer On opening day , he went to the theater . 
He bought a ticket at the theater . 
The entire movie was great . 
He really was excited . 
Rel Cued Writer He decided to watch it on opening day . 
He got to the theater and got a ticket . 
He watched the entire movie . 
He was really excited about it . 
Table 3 : Sample stories generated by different models . 
We highlight in different color the [ missing ] cue phrase , incoherent or unÔ¨Çuent , and repetitive parts of each story . 
We see that compared to baselines , our models correctly mention cue phrases and generate better stories . 
tiveness of the additional Context - Cue and Relevance Multi - Head units . 
All improvements are statistically signiÔ¨Åcant ( app . 
rand . 
, p<0.05 ) . 
Sentence - level Evaluation We also performed a more Ô¨Åne - grained evaluation of the models by evaluating generated sentences while the model is generating a story . 
The generated sentences are evaluated in light of the ( incomplete ) story . 
Specifically , we provide an ( incomplete ) story passage and a manually provided cue phrase to the two models to generate the next sentence . 
We asked human judges to identify which of the two sentences is better based on their Ô¨Çuency and semantic relevance to ( 1 ) the input ( incomplete ) story and ( 2 ) the cue phrase . 
We did this experiment for a set of 100randomly selected stories ( 400sentences . 
3different judges evaluated each sentence pair . 
Fig . 
6(b ) shows that the Rel . 
Cued Writer model was preferred over SEQ2SEQandVANILLA in72 % and64 % of the cases , respectively . 
Comparing the two proposed models , we again see additive gain by modeling Cue - Context relevance . 
All improvements are statistically signiÔ¨Åcant ( app . 
rand . 
, p<0.001 ) . 
5 Qualitative Results and Error Analysis Table 3 presents examples of stories generated by different models for the same prompt and cue phrases . 
We highlight the [ missing ] cue phrases , incoherent or unÔ¨Çuent , and repetitive parts of eachoff - topic : Kelly and her friends went to a new ice - cream shop . 
They decided to try the new Ô¨Çavors . 
They all tried on many different restaurants . 
To their surprise , they thought it tasted good . 
They were glad to Ô¨Ånd one online . 
Not - logically - consistent : Avery received a homework assignment due in two weeks . 
He immediately read it . 
When he turned it in , he made schedule . 
He completed tasks and turned it in time . 
When he Ô¨Ånished early , he was disappointed . 
non - coreferent - pronouns : Rob has never been on a rollercoaster . 
They go on all the way to six Ô¨Çags . 
He got on with a free ticket . 
Rob joined the rollercoaster . 
There was a long line of people in the line . 
Table 4 : Examples of errors made by our model . 
story . 
Note that we did not highlight [ missing ] , if the model mentions part of the cue phrase or incorporates it semantically . 
As we observe , all of the baselines suffered from several issues ; however , our novel content inducing approaches generate more causally related sentences , which Ô¨Åt the given prompt and cue phrases more naturally . 
We also manually reviewed 50stories , generated from our models and analyzed common errors . 
Table 4 shows sample stories that depict different types of errors including ‚Äú getting off - topic ‚Äù , ‚Äú not - logically - connected ‚Äù and ‚Äú non - coreferent pronouns ‚Äù . 
The last type of error represents the cases where the model generates pronouns that do not refer to any previously mentioned entity . 
The examples demonstrate that there are still many challenges in this domain.595 6 Conclusion and Future Work This paper explored the problem of interactive storytelling , which leverages human and computer collaboration for creative language generation . 
We presented two content - inducing approaches that take user - provided inputs as the story progresses and effectively incorporate them in the generated text . 
Experimental results show that our methods outperform competitive baselines . 
However , there are several other signiÔ¨Åcant aspects to be considered in story generation , such as modeling of discourse relations , and representation of key narrative elements , which lie beyond the scope of this investigation . 
Also , while we received encouraging feedback from users on this setup during the interactive evaluation , we did not explore important questions about user interfaces , design , and human computer interaction . 
Future work can explore these questions and also explore other forms of natural language interaction . 
Abstract In this paper , we introduce a large - scale Indonesian summarization dataset . 
We harvest articles from Liputan6.com , an online news portal , and obtain 215,827 document ‚Äì summary pairs . 
We leverage pre - trained language models to develop benchmark extractive and abstractive summarization methods over the dataset with multilingual and monolingual BERT - based models . 
We include a thorough error analysis by examining machinegenerated summaries that have low ROUGE scores , and expose both issues with ROUGE itself , as well as with extractive and abstractive summarization models . 
1 Introduction Despite having the fourth largest speaker population in the world , with 200 million native speakers,1 Indonesian is under - represented in NLP . 
One reason is the scarcity of large datasets for different tasks , such as parsing , text classiÔ¨Åcation , and summarization . 
In this paper , we attempt to bridge this gap by introducing a large - scale Indonesian corpus for text summarization . 
Neural models have driven remarkable progress in summarization in recent years , particularly for abstractive summarization . 
One of the Ô¨Årst studies was Rush et al . 
( 2015 ) , where the authors proposed an encoder ‚Äì decoder model with attention to generate headlines for English Gigaword documents ( Graff et al . 
, 2003 ) . 
Subsequent studies introduced pointer networks ( Nallapati et al . 
, 2016b ; See et al . 
, 2017 ) , summarization with content selection ( Hsu et al . 
, 2018 ; Gehrmann et al . 
, 2018 ) , graph - based attentional models ( Tan et al . 
, 2017 ) , and deep reinforcement learning ( Paulus et al . 
, 2018 ) . 
More recently , we have seen the widespread adoption 1https://www.visualcapitalist.com/ 100 - most - spoken - languages/ .of pre - trained neural language models for summarization , e.g. BERT ( Liu and Lapata , 2019 ) , BART ( Lewis et al . 
, 2020 ) , and PEGASUS ( Zhang et al . 
, 2020a ) . 
Progress in summarization research has been driven by the availability of large - scale English datasets , including 320 K CNN / Daily Mail document ‚Äì summary pairs ( Hermann et al . 
, 2015 ) and 100k NYT articles ( Sandhaus , 2008 ) which have been widely used in abstractive summarization research ( See et al . 
, 2017 ; Gehrmann et al . 
, 2018 ; Paulus et al . 
, 2018 ; Lewis et al . 
, 2020 ; Zhang et al . 
, 2020a ) . 
News articles are a natural candidate for summarization datasets , as they tend to be well - structured and are available in large volumes . 
More recently , English summarization datasets in other Ô¨Çavours / domains have been developed , e.g. XSum has 226 K documents with highly abstractive summaries ( Narayan et al . 
, 2018 ) , BIGPATENT is a summarization dataset for the legal domain ( Sharma et al . 
, 2019 ) , Reddit TIFU is sourced from social media ( Kim et al . 
, 2019 ) , and Cohan et al . 
( 2018 ) proposed using scientiÔ¨Åc publications from arXiv and PubMed for abstract summarization . 
This paper introduces the Ô¨Årst large - scale summarization dataset for Indonesian , sourced from theLiputan6.com online news portal over a 10year period . 
It covers various topics and events that happened primarily in Indonesia , from October 2000 to October 2010 . 
Below , we present details of the dataset , propose benchmark extractive and abstractive summarization methods that leverage both multilingual and monolingual pre - trained BERT models . 
We further conduct error analysis to better understand the limitations of current models over the dataset , as part of which we reveal not just modelling issues but also problems with ROUGE . 
To summarize , our contributions are : ( 1 ) we release a large - scale Indonesian summarization corpus with over 200 K documents , an order of mag-598 Dokumen : 	  Liputan6.com , 	 Jakarta 	 : 	 Gara - gara 	  berusaha 	 kabur 	 saat 	 diminta menunjukkan 	 barang 	 hasil 	 curian , 	 Rosihan 	 bin 	 Usman , 	  tersangka pencurian 	 tas 	 wisatawan 	 asing , 	 baru - baru 	 ini , 	 tersungkur 	  ditembak aparat 	 Kepolisian 	 Resor 	 Denpasar 	 Barat , 	 Bali . 
	 Sebelumnya , 	 Rosihan ditangkap 	 massa 	 setelah 	 mencuri 	 tas 	 Nicholas 	 Dreyden , 	 wisatawan asing 	 asal 	 Inggris . 
	 T as 	 yang 	 berisi 	 dokumen 	 keimigrasian 	 dan 	 surat penting 	 itu 	 diambil 	 Rosihan 	 setelah 	 mengelabui 	 korban . 
	  [ 7 	 kalimat 	 dengan 	 78 	 kata 	 setelahnya 	 tidak 	 ditampilkan ] Ringkasan : Seorang 	 pencuri 	 tas 	 wisatawan 	 asing 	 ditembak 	 polisi . 
	 Ia 	 berusaha kabur 	 saat 	 diminta 	 menunjukan 	 hasil 	 curian . 
	 Karena 	 itu , 	 polisi menembaknya . 
Example-2 Document : 	  Liputan6.com , 	 Jakarta : 	 Because 	 of 	  trying 	 to 	 escape 	 when 	 asked 	 to show 	 stolen 	 goods , 	 Rosihan 	 bin 	 Usman , 	  a 	 suspect 	 of 	 the 	 theft 	 of 	 a foreign 	 tourist 	 bag , 	 recently 	 fell 	 down , 	  shot 	 by 	 the 	 W est 	 Denpasar Resort 	 Police , 	 Bali . 
	 Previously , 	 Rosihan 	 was 	 arrested 	 by 	 the 	 mob 	 after stealing 	 the 	 bag 	 of 	 Nicholas 	 Dreyden , 	 a 	 foreign 	 tourist 	 from 	 England . 
The 	 bag 	 containing 	 immigration 	 documents 	 and 	 important 	 letters 	 was taken 	 by 	 Rosihan 	 after 	 tricking 	 the 	 victim . 
	  [ 7 	 sentences 	 with 	 78 	 words 	 are 	 abbreviated 	 from 	 here ] Summary : A 	 foreign 	 tourist 	 bag 	 thief 	 was 	 shot 	 by 	 police . 
	 He 	 tried 	 to 	 run 	 away when 	 asked 	 to 	 show 	 the 	 loot . 
	 Because 	 of 	 this , 	 the 	 police 	 shot 	 him . 
Dokumen : 	  Liputan6.com , 	 Jakarta 	 : 	 Or ganisasi 	 Negara - negara 	 Pengekspor Minyak 	 ( OPEC ) 	 mengakui 	 mengalami 	 kesulitan 	 untuk 	 menjaga stabilitas 	 har ga 	 minyak 	 dunia . 
	 Itu 	 lantaran 	 har ga 	 minyak 	 terus melonjak 	 sepanjang 	 tahun 	 ini . 
	 Hingga 	 kini 	 har ga 	 minyak 	 mentah dunia 	 masih 	 mencapai 	 tingkat 	 tertinggi 	 sejak 	 pecah 	 perang 	 teluk sepuluh 	 tahun 	 silam . 
[ 3 	 kalimat 	 dengan 	 57 	 kata 	 tidak 	 ditampilkan ] Padahal 	 , 	 sebelumnya 	 OPEC 	 telah 	 merevisi 	 produksi 	 minyak sebanyak 	 tiga 	 kali 	 dalam 	 enam 	 bulan 	 terakhir . 
	 Pertama , 	 April 	 hingga Juni 	 dengan 	 kenaikan 	 mencapai 	 500 	 ribu 	 barel 	 dan 	 terakhir , September 	 ini , 	 OPEC 	 kembali 	 menaikkan 	 produksi 	 sebesar 	 800 	 ribu barel 	 per 	 hari . 
	  [ 5 	 kalimat 	 dengan 	 96 	 kata 	 setelahnya 	 tidak 	 ditampilkan ] Ringkasan : OPEC 	 kesulitan 	 menjaga 	 stabilitas 	 har ga 	 minyak 	 dunia 	 lantaran 	 har ga minyak 	 dipasaran 	 terus 	 melonjak . 
	 Padahal , 	 OPEC 	 telah 	 tiga 	 kali menaikkan 	 produksi 	 dalam 	 enam 	 bulan 	 terakhir .Example-1 Document : 	  Liputan6.com , 	 Jakarta : 	 The 	 Or ganization 	 of 	 Petroleum 	 Exporting Countries 	  ( OPEC ) 	 has 	 admitted 	 that 	 it 	 is 	 having 	 dif ficulty 	 maintaining the 	 stability 	 of 	 world 	 oil 	 prices . 
	 That 's 	 because 	 oil 	 prices 	 continue 	 to soar 	 this 	 year . 
	 Until 	 now 	 world 	 crude 	 oil 	 prices 	 have 	 still 	 reached 	 the highest 	 level 	 since 	 the 	 gulf 	 war 	 broke 	 out 	 ten 	 years 	 ago . 
[ 3 	 sentences 	 with 	 57 	 words 	 are 	 abbreviated 	 from 	 here ] In 	 fact , 	 OPEC 	 had 	 previously 	 revised 	 oil 	 production 	 three 	 times 	 in 	 the last 	 six 	 months . 
	 First , 	 April 	 to 	 June 	 with 	 an 	 increase 	 of 	 500 	 thousand barrels 	 and 	 last , 	 this 	 September , 	 OPEC 	 has 	 again 	 increased 	 production by 	 800 	 thousand 	 barrels 	 per 	 day . 
[ 5 	 sentences 	 with 	 96 	 words 	 are 	 abbreviated 	 from 	 here ] Summary : OPEC 	 is 	 struggling 	 to 	 maintain 	 the 	 stability 	 of 	 world 	 oil 	 prices because 	 oil 	 prices 	 on 	 the 	 market 	 continue 	 to 	 soar . 
	 In 	 fact , 	 OPEC 	 has raised 	 production 	 three 	 times 	 in 	 the 	 past 	 six 	 months . 
Figure 1 : Example articles and summaries from Liputan6 . 
To the left is the original document and summary , and to the right is an English translation ( for illustrative purposes ) . 
We additionally highlight sentences that the summary is based on ( noting that such highlighting is not available in the dataset ) . 
nitude larger than the current largest Indonesian summarization dataset and one of the largest nonEnglish summarization datasets in existence;2(2 ) we present statistics to show that the summaries in the dataset are reasonably abstractive , and provide two test partitions , a standard test set and an extremely abstractive test set ; ( 3 ) we develop benchmark extractive and abstractive summarization models based on pre - trained BERT models ; and ( 4 ) we conduct error analysis , on the basis of which we share insights to drive future research on Indonesian text summarization . 
2 Data Construction Liputan6.com is an online Indonesian news portal which has been running since August 2000 , and provides news across a wide range of topics including politics , business , sport , technology , health , and entertainment . 
According to the Alexa ranking of websites at the time of writing,3 Liputan6.com is ranked 9th in Indonesia and 112th globally . 
The website produces daily articles along 2The data can be accessed at https://github.com/ fajri91 / sum_liputan6 3https://www.alexa.com/topsiteswith a short description for its RSS feed . 
The summary is encapsulated in the javascript variablewindow.kmklabs.article and the key shortDescription , while the article is in the main body of the associated HTML page . 
We harvest this data over a 10 - year window ‚Äî from October 2000 to October 2010 ‚Äî to create a largescale summarization corpus , comprising 215,827 document ‚Äì summary pairs . 
In terms of preprocessing , we remove formatting and HTML entities ( e.g. & quot , and ) , lowercase all words , and segment sentences based on simple punctuation heuristics . 
We provide example articles and summaries , with English translations for expository purposes ( noting that translations are not part of the dataset ) , in Figure 1 . 
As a preliminary analysis of the document ‚Äì summary pairs over the 10 - year period , we binned the pairs into 5 chronologically - ordered groups containing 20 % of the data each , and computed the proportion of novel n - grams ( order 1 to 4 ) in the summary ( relative to the source document ) . 
Based on the results in Figure 2 , we can see that the proportion of novel n - grams drops over time , implying that the summaries of more recent articles are less599 Variant#Doc % of Novel n - grams Train Dev Test 1 2 3 4 Canonical 193,883 10,972 10,972 16.2 52.5 71.8 82.4 Xtreme 193,883 4,948 3,862 22.2 66.7 87.5 96.6 Table 1 : Statistics for the canonical and Xtreme variants of our data . 
The percentage of novel n - grams is based on the combined Dev and Test set . 
TimePercentage 0255075100 Oct 2000 - Jul   2003Jul 2003 - Aug   2006Aug 2006 - Apr   2008Apr 2008 - Aug   2009Aug 2009 - Oct   20101 - gram 2 - gram 3 - gram 4 - gram Figure 2 : Proportion of novel n - grams over time in the summaries . 
abstractive . 
For this reason , we decide to use the earlier articles ( October 2000 to Jan 2002 ) as the development and test documents , to create a more challenging dataset . 
This setup also means there is less topic overlap between training and development / test documents , allowing us to assess whether the summarization models are able to summarize unseen topics . 
For the training , development and test partitions , we use a splitting ratio of 90:5:5 . 
In addition to this canonical partitioning of the data , we provide an ‚Äú Xtreme ‚Äù variant ( inspired by Xsum ; Narayan et al . 
( 2018 ) ) whereby we discard development and test document ‚Äì summary pairs where the summary has fewer than 90 % novel 4 - grams ( leaving the training data unchanged ) , creating a smaller , more challenging data conÔ¨Åguration . 
Summary statistics for the ‚Äú canonical ‚Äù and ‚Äú Xtreme ‚Äù variants are given in Table 1 . 
We next present a comparison of Liputan6 ( canonical partitioning ) and IndoSum ( the current largest Indonesian summarization dataset , as detailed in Section 6 ; Kurniawan and Louvan ( 2018 ) ) in Table 2 . 
In terms of number of documents , Liputan6 is approximately 11 times larger than IndoSum ( the current largest Indonesian summarization dataset ) , although articles and summaries in Liputan6 are slightly shorter . 
To understand the abstractiveness of the summaries in the two datasets , in Table 3 we presentROUGE scores for the simple baseline of using the Ô¨ÅrstNsentences as an extractive summary ( ‚Äú LEAD - N ‚Äù ) , and the percentage of novel n - grams in the summary.4We use LEAD-3andLEAD-2 for IndoSum and Liputan6 respectively , based on the average number of sentences in the summaries ( Table 2 ) . 
We see that Liputan6 has consistently lower ROUGE scores ( R1 , R2 , and RL ) for LEADN ; it also has a substantially higher proportion of noveln - grams . 
This suggests that the summaries in Liputan6 are more abstractive than IndoSum . 
To create a ground truth for extractive summarization , we follow Cheng and Lapata ( 2016 ) and Nallapati et al . 
( 2016a ) in greedily selecting the subset of sentences in the article that maximizes the ROUGE score based on the reference summary . 
As a result , each sentence in the article has a binary label to indicate whether they should be included as part of an extractive summary . 
Extractive summaries created this way will be referred to as ‚Äú ORACLE ‚Äù , to denote the upper bound performance of an extractive summarization system . 
3 Summarization Models We follow Liu and Lapata ( 2019 ) in building extractive and abstractive summarization models using BERT as an encoder to produce contextual representations for the word tokens . 
The architecture of both models is presented in Figure 3 . 
We tokenize words with WordPiece , and append [ CLS ] ( preÔ¨Åx ) and [ SEP ] ( sufÔ¨Åx ) tokens to each sentence . 
To further distinguish the sentences , we add even / odd segment embeddings ( TA / TB ) based on the order of the sentence to the word embeddings . 
For instance , for a document with sentences [ s1,s2,s3,s4 ] , the segment embeddings are [ TA , TB , TA , TB ] . 
Position embeddings ( P ) are also used to denote the position of each token . 
The WordPiece , segment , and position embeddings are summed together and provided as input to BERT . 
BERT produces a series of contextual representations for the word tokens , which we feed into a ( second ) transformer encoder / decoder for the extractive / abstractive summarization model . 
We detail the architecture of these two models in Sections 3.1 and 3.2 . 
Note that this second transformer is initialized with random parameters ( i.e. it is not pre - trained ) . 
For the pre - trained BERT encoder , we use mul4All statistics are based on the entire dataset , encompassing the training , dev , and test data.600 Dataset#Doc Article Summary Train Dev Test ¬µ(Word)¬µ(Sent ) # Vocab ¬µ(Word)¬µ(Sent ) # Vocab IndoSum 14,252 750 3,762 347.23 18.37 117 K 68.09 3.47 53 K Liputan6 193,883 10,972 10,972 232.91 12.60 311 K 30.43 2.09 100 K Table 2 : A comparison of IndoSum and Liputan6 . 
¬µ(Word ) and¬µ(Sent ) denote the average number of words and sentences , respectively . 
DatasetLEAD - N % of Novel n - grams R1 R2 RL 1 2 3 4 IndoSum 65.6 58.9 64.8 3.1 10.8 16.2 20.3 Liputan6 41.2 27.1 38.7 12.9 41.6 57.6 66.9 Table 3 : Abstractiveness of the summaries in IndoSum and Liputan6 . 
tilingual BERT ( mBERT ) and our own IndoBERT ( Koto et al . 
, to appear).5IndoBERT is a BERTBase model we trained ourselves using Indonesian documents from three sources : ( 1 ) Indonesian Wikipedia ( 74 M words ) ; ( 2 ) news articles ( 55 M words ) from Kompas,6Tempo ( Tala et al . 
, 2003),7and Liputan6;8and ( 3 ) the Indonesian Web Corpus ( 90 M words ; Medved and Suchomel ( 2017 ) ) . 
In total , the training data has 220 M words . 
We implement IndoBERT using the Huggingface framework,9and follow the default conÔ¨Åguration of BERT - Base ( uncased ): hidden size = 768d , hidden layers = 12 , attention heads = 12 , and feed - forward = 3,072d . 
We train IndoBERT with 31,923 WordPieces ( vocabulary ) for 2 million steps . 
3.1 Extractive Model After the document is processed by BERT , we have a contextualized embedding for every word token in the document . 
To learn inter - sentential relationships , we use the [ CLS ] embeddings ( [ xS1,xS2, .. ,x Sm ] ) to represent the sentences , to which we add a sentence - level positional embedding ( P ) , and feed them to a transformer encoder ( Figure 3 ) . 
An MLP layer with sigmoid activation is applied to the output of the transformer encoder to predict whether a sentence should be extracted ( i.e.ÀúyS‚àà{0,1 } ) . 
We train the model with binary 5The pre - trained mBERT is sourced from : https:// github.com/google-research/bert . 
6https://kompas.com 7https://koran.tempo.co 8For Liputan6 , we use only the articles from the training partition . 
9https://huggingface.co/ mBERT 	 / 	 IndoBER T [ CLS]Sent1[SEP][CLS]Sent2[SEP ] [ CLS]Sentm[SEP][PAD]TA TA TATBTBTB TA TA TATPP2 P1xS 1xS 2xS m W ord Segment Position Bert 	 Out P4 P3 P6 P5 Pn-3 Pn-1 Pn-2 PnxS 1xS e n t 1xS E PxS 2xS e n t 2xS E PxS mxS e n t mxS E PxP A DExtractive 	 Model Transformer 	 Encoder·ªπS 1 ·ªπS m MLP 	 Layer ... ... ...... Pre - trained 	 model·ªπS 2 ...... xS 1xS e n t 1xS e n t mxS E PxP A D ... Transformer 	 DecoderSoftmax 	 LayerGeneration 	 of 	  YAbstractive 	 ModelLearned 	 from 	 scratchP2 P1 Pm ... P2 P1 ... Pn-1 Pn-2 Pn PositionFigure 3 : Architecture of the extractive and abstractive summarization models . 
cross entropy , and update all model parameters ( including BERT ) during training . 
Note that the parameters in the transformer encoder and the MLP layer are initialized randomly , and learned from scratch . 
The transformer encoder is conÔ¨Ågured as follows : layers = 2 , hidden size = 768 , feed - forward = 2,048 , and heads = 8 . 
In terms of training hyperparameters , we train using the Adam optimizer with learning rate lr= 2e‚àí3¬∑min(step‚àí0.5,step ¬∑ warmup‚àí1.5)where warmup = 10,000 . 
We train for 50,000 steps on 3 √óV100 16 GB GPUs , and perform evaluation on the development set every 2,500 steps . 
At test time , we select sentences for the extractive summary according to two conditions : the summary must consist of : ( a ) at least two sentences , and ( b ) at least 15 words . 
These values were set based on the average number of sentences and the minimum number of words in a summary . 
We also apply trigram blocking to reduce redundancy ( Paulus et al . 
, 2018 ) . 
Henceforth , we refer to this model as ‚Äú B ERTEXT‚Äù.601 3.2 Abstractive Model Similar to the extractive model , we have a second transformer to process the contextualized embeddings from BERT . 
In this case , we use a transformer decoder instead ( i.e. an attention mask is used to prevent the decoder from attending to future time steps ) , as we are learning to generate an abstractive summary . 
But unlike the extractive model , we use the BERT embeddings for all tokens as input to the transformer decoder ( as we do not need sentence representations ) . 
We add to these BERT embeddings a second positional encoding before feeding them to the transformer decoder ( Figure 3 ) . 
The transformer decoder is initialized with random parameters ( i.e. no pre - training ) . 
The transformer decoder is conÔ¨Ågured as follows : layers = 6 , hidden size = 768 , feed - forward = 2,048 , and heads = 8 . 
Following Liu and Lapata ( 2019 ) , we use a different learning rate for BERT and the decoder when training the model : lr= 2e‚àí3¬∑min(step‚àí0.5,step¬∑20,000‚àí1.5)and 0.1¬∑min(step‚àí0.5,step¬∑10,000‚àí1.5)for BERT and the transformer decoder , respectively . 
Both networks are trained with the Adam optimizer for 200,000 steps on 4 √óV100 16 GB GPUs and evaluated every 10,000 steps . 
For summary generation , we use beam width = 5 , trigram blocking , and a length penalty ( Wu et al . 
, 2016 ) to generate at least two sentences and at least 15 words ( similar to the extractive model ) . 
Henceforth the abstractive model will be referred to as ‚Äú BERTABS ‚Äù . 
We additionally experiment with a third variant , ‚Äú BERTEXTABS ‚Äù , where we use the weights of the Ô¨Åne - tuned BERT in BERTEXTfor the encoder ( instead of off - the - shelf BERT weights ) . 
4 Experiment and Results We use three ROUGE ( Lin , 2004 ) F-1 scores as evaluation metrics : R1 ( unigram overlap ) , R2 ( bigram overlap ) , and RL ( longest common subsequence overlap ) . 
In addition , we also provide BERTS CORE ( F-1 ) , as has recently been used for machine translation evaluation ( Zhang et al . 
, 2020b).10We use the development set to select the best checkpoint during training , and report the evaluation scores for the canonical and Xtreme test sets in Table 4 . 
For both test sets , the summarization models are trained using the same training 10https://github.com/Tiiiger/bert_scoreset , but they are tuned with a different development set ( see Section 2 for details ) . 
In addition to the BERT models , we also include two pointergenerator models ( See et al . 
, 2017 ): ( 1 ) the base model ( PTG EN ) ; and ( 2 ) the model with coverage penalty ( PTG EN+COV).11 We Ô¨Årst look at the baseline LEAD - NandORACLEresults . 
LEAD-2is the best LEAD - Nbaseline for Liputan6 . 
This is unsurprising , given that in Table 2 , the average summary length was 2 sentences . 
We also notice there is a substantial gap between ORACLE andLEAD-2 : 12‚Äì15 points for R1 and 5‚Äì7 points for BERTS CORE , depending on the test set . 
This suggests that the baseline of using the Ô¨Årst few sentences as an extractive summary is ineffective . 
Comparing the performance between the canonical and Xtreme test sets , we see a substantial drop in performance for both LEAD - Nand ORACLE , highlighting the difÔ¨Åculty of the Xtreme test set due to its increased abstractiveness . 
For the pointer - generator models , we see little improvement when including the coverage mechanism ( PTG EN+COVvs . 
PTG EN ) , implying that there is minimal repetition in the output of PTG EN . 
We suspect this is due to the Liputan6 summaries being relatively short ( 2 sentences with 30 words on average ) . 
A similar observation is reported by Narayan et al . 
( 2018 ) for XSum , where the summaries are similarly short ( a single sentence with 23 words , on average ) . 
Next we look at the BERT models . 
Overall they perform very well , with both the mBERT and IndoBERT models outperforming the LEAD - Nbaselines and PTG ENmodels by a comfortable margin . 
IndoBERT is better than mBERT ( approximately 1 ROUGE point better on average over most metrics ) , showing that a monolingually - trained BERT is a more effective pre - trained model than the multilingual variant . 
The best performance is achieved by IndoBERT ‚Äôs BERTEXTABS . 
In the canonical test set , the improvement over LEAD-2is+4.4 R1 , +2.62 R2 , +4.3 R3 , and +3.4BERTS CORE points . 
In the Xtreme test set , BERTEXTABSsuffers a substantial drop compared to the canonical test set ( 6‚Äì7 ROUGE and 2 BERTS CORE points ) , although the performance gap between it and LEAD-2is about the same . 
11We use the default hyper - parameter conÔ¨Åguration recommended by the original authors for the pointer - generator models.602 ModelCanonical Test Set Xtreme Test Set R1 R2 RL BS R1 R2 RL BS LEAD-1 32.67 18.50 29.40 72.62 27.27 11.56 23.60 71.19 LEAD-2 36.68 20.23 33.71 74.58 31.10 12.78 27.63 72.98 LEAD-3 34.49 18.84 32.06 74.31 29.54 12.05 26.68 72.78 ORACLE 51.54 30.56 47.75 79.24 43.69 18.57 38.84 76.75 PTG EN 36.10 19.19 33.56 75.92 30.41 12.05 27.51 74.10 PTG EN+COV 35.53 18.56 32.92 75.75 30.27 11.81 27.26 74.11 BERTEXT(mBERT ) 37.51 20.15 34.57 75.22 31.83 12.63 28.37 73.62 BERTABS(mBERT ) 39.48 21.59 36.72 77.19 33.26 13.82 30.12 75.40 BERTEXTABS(mBERT ) 39.81 21.84 37.02 77.39 33.86 14.13 30.73 75.69 BERTEXT(IndoBERT ) 38.03 20.72 35.07 75.33 31.95 12.74 28.47 73.64 BERTABS(IndoBERT ) 40.94 23.01 37.89 77.90 34.59 15.10 31.19 75.84 BERTEXTABS(IndoBERT ) 41.08 22.85 38.01 77.93 34.84 15.03 31.40 75.99 Table 4 : ROUGE results for the canonical and Xtreme test sets . 
All ROUGE ( ‚Äú R1 ‚Äù , ‚Äú R2 ‚Äù , and ‚Äú RL ‚Äù ) scores have a conÔ¨Ådence interval of at most ¬±0.3 , as reported by the ofÔ¨Åcial ROUGE script . 
‚Äú BS ‚Äù is BERS CORE computed withbert - base - multilingual - cased ( layer 9 ) , as suggested by Zhang et al . 
( 2020b ) . 
5 Error Analysis In this section , we analyze errors made by the extractive ( BERTEXT ) and abstractive ( BERTEXTABS ) models to better understand their behaviour . 
We use the mBERT version of these models in our analysis.12 5.1 Error Analysis of Extractive Summaries We hypothesized that the disparity between ORACLE andBERTEXT(14.03 point difference for R1 in the canonical test set ) was due to the number of extracted sentences . 
To test this , when extracting sentences with BERTEXT , we set the total number of extracted sentences to be the same as the number of sentences in the ORACLE summary . 
However , we found minimal beneÔ¨Åt using this approach , suggesting that the disparity is not a result of the number of extracted sentences . 
To investigate this further , we present the frequency of sentence positions that are used in the summary in ORACLE andBERTEXTfor the canonical test set in Figure 4a . 
We can see that BERTEXT tends to over - select the Ô¨Årst two sentences as the summary . 
In terms of proportion , 65.47 % of 12The error analysis is based on mBERT rather than IndoBERT simply because this was the best - performing model at the time the error analysis was performed . 
While IndoBERT ultimately performed slightly better , given that the two models are structurally identical , we would expect to see a similar pattern of results . 
BERTEXTsummaries involve the Ô¨Årst two sentences . 
In comparison , only 42.54 % of ORACLE summaries use sentences in these positions . 
One may argue that this is because the training and test data have different distributions under our chronological partitioning strategy ( recall that the test set is sampled from the earliest articles ) , but that does not appear to be the case : as Figure 4b shows , the distribution of sentence positions in the training data is very similar to the test data ‚Äî 43.14 % of ORACLE summaries involve the Ô¨Årst two sentences . 
5.2 Error Analysis of Abstractive Summaries To perform error analysis for BERTEXTABS , we randomly sample 100 documents with an R1 score < 0.4 in the canonical test set ( which accounts for nearly 50 % of the test documents ) . 
Two native Indonesian speakers examined these 100 samples to manually assess the quality of the summaries , and score them on a 3 - point ordinal scale : ( 1 ) bad ; ( 2 ) average ; and ( 3 ) good . 
Each annotator is presented with the source document , the reference summary , and the summary generated by BERTEXTABS . 
In addition to the overall quality evaluation , we also asked the annotators to analyze a number of ( Ô¨Ånegrained ) attributes in the summaries : ‚Ä¢Abbreviations : the system summary uses abbreviations that are different to the reference summary.603 Position02000400060008000 1 2 3 4 5 6 8 10 12 14 15 16 17 18 19 20 > 20GOLD PRED(a ) Distribution of sentence positions for ORACLE and BERTEXTin the canonical test set . 
Position0250005000075000100000125000 1234567891011121314151617181920>20(b ) Distribution of sentence positions for ORACLE in the training set . 
Figure 4 : Position of O RACLE and/or Predicted Extractive Summaries Category Bad Avg . 
Good # Samples ( 100 ) 32 8 60 Abbreviation ( % ) 21.9 25.0 40.0 Morphology ( % ) 12.5 25.0 36.7 Paraphrasing ( % ) 50.0 87.5 86.7 Lack of coverage ( % ) 90.6 100.0 40.0 Wrong focus ( % ) 68.8 0.00 8.3 Un . 
details ( from doc ) ( % ) 90.6 75.0 75.0 Un . 
details ( not from doc ) ( % ) 18.8 12.5 5.0 Table 5 : Error analysis for 100 samples with R1 < 0.4 . 
‚Ä¢Morphology : the system summary uses morphological variants of the same lemmas contained in the reference summary . 
‚Ä¢Synonyms / paraphrasing : the system summary contains paraphrases of the reference summary . 
‚Ä¢Lack of coverage : the system summary lacks coverage of certain details that are present in the reference summary . 
‚Ä¢Wrong focus : the system summarizes a different aspect / focus of the document to the reference summary . 
‚Ä¢Unnecessary details ( from document ): the system summary includes unimportant but factually correct information . 
‚Ä¢Unnecessary details ( not from document ): the system summary includes unimportant and factually incorrect information ( hallucinations ) . 
We present a breakdown of the different error types in Table 5 . 
Inter - annotator agreement for the overall quality assessment is high ( Pearson ‚Äôs r=0.69 ) . 
Disagreements in the quality label ( bad , average , good ) are resolved as follows : ( 1 ) { bad , average}‚Üí bad ; and ( 2){good , average}‚Üí good . 
We only have four examples with { bad , good}disagreement , which we resolved through discussion . 
Interestingly , more than half ( 60 ) of our samples were found to have good summaries . 
The primary reasons why these summaries have low ROUGE scores are paraphrasing ( 86.7 % ) , and the inclusion of additional ( but valid ) details ( 75.0 % ) . 
Abbreviations and morphological differences also appear to be important factors . 
These results underline a problem with the ROUGE metric , in that it is unable to detect good summaries that use a different set of words to the reference summary . 
One way forward is to explore metrics that consider sentence semantics beyond word overlap such as METEOR ( Banerjee and Lavie , 2005 ) and BERTS CORE , 13 and question - answering system based evaluation such as APES ( Eyal et al . 
, 2019 ) and QAGS ( Wang et al . 
, 2020 ) . 
Another way is to create more reference summaries ( which will help with the issue of the system summaries including [ validly ] different details to the single reference ) . 
Looking at the results for average summaries ( middle column ) , BERTEXTABSoccasionally fails to capture salient information : 100 % of the summaries have coverage issues , and 75.0 % contain unnecessary ( but valid ) details . 
They also tend to use paraphrases ( 87.5 % ) , which further impacts on a lower ROUGE score . 
Finally , the bad system summaries have similar coverage issues , and also tend to have a very different focus compared to the 13Indeed , we suggest that BERTS CORE should be used as the canonical evaluation metric for the dataset , but leave empirical validation of its superiority for Indonesian summarization evaluation to future work.604 Dokumen : 	  Liputan6.com 	 , 	 Jakarta 	 : 	 Langkah 	 reshuf fle 	 yang 	 dilakukan 	 Presiden Abdurrahman 	 W ahid 	 , 	 agaknya 	 tak 	 mendapat 	 restu 	 . 
	 Buktinya 	 , 	 W akil Presiden 	 Megawati 	 Sukarnoputri 	 kembali 	 tidak 	 hadir 	 dalam pelantikan 	 tiga 	 menteri 	 bidang 	 ekonomi 	 , 	 Rabu 	 ( 	 13/6 	 ) 	 . 
	  [ 8 	 kalimat 	 dengan 	 1 13 	 kata 	 setelahnya 	 tidak 	 ditampilkan ] Ringkasan 	 manusia : wapres 	 megawati 	 sukarnoputri 	 , 	 kembali 	 tidak 	 hadir 	  dalam 	 pelantikan tiga 	 menteri 	 baru 	 . 
	 dalam 	 reshufle 	 1 	 juni 	 , 	 megawati 	 juga 	 tak 	 muncul dalam 	 pelantikan 	 , 	  karena 	 merasa 	 tak 	 dilibatkan 	 dalam 	 reshuf fle kabinet 	  . 
Ringkasan 	 sistem 	 [ Bad ] : presiden 	 abdurrahman 	 wahid 	 kembali 	 tidak 	 hadir 	 dalam 	 pelantikan tiga 	 menteri 	 bidang 	 ekonomi 	 . 
	  ketidaksepakatan 	 soal 	 perombakan kabinet 	 itu 	 juga 	 terjadi 	 1 	 juni 	 silam 	 . 
	  presiden 	 meminta 	 mereka 	 lebih menjaga 	 koordinasi 	 antarmenteri 	 .Example-2 	 of 	 error 	 analysis 	 ( Lack 	 of 	 coverage , 	 wrong 	 focus , 	 and 	  details 	 that 	 are 	 not 	 from 	 the 	 document ) Document : 	  Liputan6.com , 	 Jakarta : 	 The 	 reshuf fle 	 step 	 was 	 taken 	 by 	 President Abdurrahman 	 W ahid , 	 apparently 	 did 	 not 	 get 	 the 	 blessing . 
	 The 	 proof , V ice 	 President 	 Megawati 	 Sukarnoputri 	 was 	 again 	 not 	 present 	 at 	 the inauguration 	 of 	 three 	 ministers 	 in 	 the 	 economic 	 sector , 	 W ednesday ( 6/13 ) . 
	  [ 8 	 sentences 	 with 	 1 13 	 words 	 are 	 abbreviated 	 from 	 here ] Gold 	 Summary : V ice 	 President 	 Megawati 	 Sukarnoputri , 	 is 	 not 	 present 	 a t 	 the inauguration 	 of 	 three 	 new 	 ministers 	 again . 
	 In 	 the 	 reshuf fle 	 on 	 June 	 1 , Megawati 	 also 	 did 	 not 	 appear 	 in 	 the 	 inauguration , 	  because 	 she 	 felt 	 not involved 	 in 	 the 	 cabinet 	 reshuf fle . 
System 	 Summary 	 [ Bad ] : President 	 Abdurrahman 	 W ahid 	 was 	 again 	 absent 	  from 	 the inauguration 	 of 	 three 	 ministers 	 in 	 the 	 economic 	 sector . 
	 disagreement about 	 the 	 cabinet 	 reshuf fle 	 also 	 occurred 	 1 	 June 	 ago . 
	 the 	 president asked 	 them 	 to 	 maintain 	 more 	 coordination 	 between 	 ministries . 
Dokumen : 	  Liputan6.com 	 , 	 Jakarta 	 : 	 Protes 	 masih 	 ber gema 	 menyambut Keputusan 	 Menteri 	 T enaga 	 Kerja 	 dan 	 T ransmigrasi 	 Nomor 	 78 	 T ahun 2001 	 . 
	 Kebijakan 	 yang 	 sengaja 	 dikeluarkan 	 sebagai 	 wujud 	 perubahan keputusan 	 sebelumnya 	 ini 	 , 	 sampai 	 sekarang 	 , 	 masih 	 mengundang kecaman 	 keras 	 dari 	 pekerja 	 di 	 Indonesia 	 . 
	 Itulah 	 sebabnya 	 , 	 mereka menuntut 	 Kepmenakertrans 	 baru 	 ini 	 dicabut 	 karena 	 dinilai 	 merugikan pekerja 	 . 
	  [ 19 	 kalimat 	 dengan 	 406 	 kata 	 tidak 	 ditampilkan ] Sementara 	 itu 	 , 	  SPSI 	 secara 	 tegas 	 menolak 	 segala 	 bentuk 	 negosiasi 	  . 
	  [ 3 	 kalimat 	 dengan 	 45 	 kata 	 setelahnya 	 tidak 	 ditampilkan ] Ringkasan 	 manusia : pemberlakuan 	  kepmenakertrans 	 78/2001 	 masih 	 mengundang 	 rasa tidak 	 puas 	 di 	 dada 	 sejumlah 	 pekerja 	 indonesia 	 . 
	 maka 	 , 	 lahirlah tuntutan 	 agar 	 peraturan 	 yang 	 dinilai 	 merugikan 	 ini 	 dicabut 	 . 
Ringkasan 	 sistem 	 [ Good ] : 	  keputusan 	 menteri 	 tenaga 	 kerja 	 dan 	 transmigrasi 	 nomor 	 78 	 tahun 2001 	 mengundang 	 kecaman 	 keras 	 dari 	 pekerja 	 di 	 indonesia 	 . 
	 mereka menuntut 	 kepmenakertrans 	 dicabut 	 karena 	 dinilai 	 merugikan 	 pekerja 	 . 
spsi 	 menolak 	 negosiasi 	  .Example-1 	 of 	 error 	 analysis 	 ( Abbreviation , 	 morphoplogy , 	 synonyms / paraphrashing , 	 and 	  details 	 from 	 the 	 document ) Document : 	  Liputan6.com , 	 Jakarta : 	 Protests 	 still 	 resonate 	 with 	 welcoming Minister 	 of 	 Manpower 	 and 	 T ransmigration 	 Decree 	 No . 
	 78/2001 . 
	 This policy , 	 which 	 was 	 deliberately 	 issued 	 as 	 an 	 amendment 	 to 	 the previous 	 decision , 	 until 	 now , 	 still 	 invites 	 harsh 	 criticism 	 from 	 workers in 	 Indonesia . 
	 That 	 is 	 why 	 they 	 demand 	 to 	 revoke 	 the 	 new Kepmenakertrans 	 because 	 it 	 is 	 considered 	 detrimental 	 to 	 workers . 
[ 19 	 sentences 	 with 	 406 	 words 	 are 	 abbreviated 	 from 	 here ] Meanwhile , 	  SPSI 	 firmly 	 rejected 	 all 	 forms 	 of 	 negotiation . 
[ 3 	 sentences 	 with 	 45 	 words 	 are 	 abbreviated 	 from 	 here ] Gold 	 Summary : The 	 enactment 	 of 	  Kepmenakertrans 	 78/2001 	 still 	 invites 	 the dissatisfaction 	 of 	 Indonesian 	 workers . 
	 hence , 	  demands 	 to 	 revoke 	 the regulation 	 arose 	 as 	 it 	 was 	 considered 	 to 	 be 	 detrimental . 
System 	 Summary 	 [ Good ] : Minister 	 of 	 Manpower 	 and 	 T ransmigration 	 Decree 	  number 	 78 	 of 	 2001 invited 	 strong 	 criticism 	 from 	 workers 	 in 	 Indonesia . 
	 They 	  demand 	 to revoke 	 Kepmenakertrans 	 because 	 it 	 is 	 considered 	 detrimental 	 to workers . 
	  SPSI 	 rejects 	 negotiations . 
Figure 5 : Two examples to highlight error categories used in our error analysis . 
reference summary ( 90.6 % ) . 
In Figure 5 we show two representative examples from BERTEXTABS . 
The Ô¨Årst example is considered good by our annotators , but due to abbreviations , morphological differences , paraphrasing , and additional details compared to the reference summary , the ROUGE score is < 0.4 . 
In this example , the gold summary uses the abbreviation kepmenakertrans while BERTEXTABSgenerates the full phrase keputusan menteri tenaga kerja dan transmigrasi ( which is correct ) . 
The example also uses paraphrases ( invites strong criticism to explain dissatisfaction ) , and there are morphological differences in words such as tuntutan ( noun ) vs. menuntut(verb ) . 
The low ROUGE score here highlights the fact that the bigger issue is with ROUGE itself rather than the summary . 
The second example is considered to be bad , with the following issues : lack of coverage , wrong focus , and contains unnecessary details that are not from the article . 
The Ô¨Årst sentence President Abdurrahman Wahid was absent has nothing to dowith the original article , creating a different focus ( and confusion ) in the overall summary . 
To summarize , coverage , focus , and the inclusion of other details are the main causes of low quality summaries . 
Our analysis reveals that abbreviations and paraphrases are another cause of summaries with low ROUGE scores , but that is an issue with ROUGE rather than the summaries . 
Encouragingly , hallucination ( generating details not in the original document ) is not a major issue for these models ( notwithstanding that almost 20 % of badsamples contain hallucinations ) . 
6 Related Datasets Previous studies on Indonesian text summarization have largely been extractive and used small - scale datasets . 
Gunawan et al . 
( 2017 ) developed an unsupervised summarization model over 3 K news articles using heuristics such as sentence length , keyword frequency , and title features . 
In a similar vein , Najibullah ( 2015 ) trained a naive Bayes model to extract summary sentences in a 100 - article dataset.605 Aristoteles et al . 
( 2012 ) and Silvia et al . 
( 2014 ) apply genetic algorithms to a summarization dataset with less than 200 articles . 
These studies do not use ROUGE for evaluation , and the datasets are not publicly available . 
Koto ( 2016 ) released a dataset for chat summarization by manually annotating chat logs from WhatsApp .14However , this dataset contains only 300 documents . 
The largest summarization data to date is IndoSum ( Kurniawan and Louvan , 2018 ) , which has approximately 19 K news articles with manually - written summaries . 
Based on our analysis , however , the summaries of IndoSum are highly extractive . 
Beyond Indonesian , there is only a handful of non - English summarization datasets that are of sufÔ¨Åcient size to train modern deep learning summarization methods over , including : ( 1 ) LCSTS ( Hu et al . 
, 2015 ) , which contains 2 million Chinese short texts constructed from the Sina Weibo microblogging website ; and ( 2 ) ES - News ( Gonzalez et al . 
, 2019 ) , which comprises 270k Spanish news articles with summaries . 
LCSTS documents are relatively short ( less than 140 Chinese characters ) , while ES - News is not publicly available . 
Our goal is to create a benchmark corpus for Indonesian text summarization that is both large scale and publicly available . 
7 Conclusion We release Liputan6 , a large - scale summarization corpus for Indonesian . 
Our dataset comes with two test sets : a canonical test set and an ‚Äú Xtreme ‚Äù variant that is more abstractive . 
We present results for several benchmark summarization models , in part based on IndoBERT , a new pre - trained BERT model for Indonesian . 
We further conducted extensive error analysis , as part of which we identiÔ¨Åed a number of issues with ROUGE - based evaluation for Indonesian . 
Acknowledgments We are grateful to the anonymous reviewers for their helpful feedback and suggestions . 
In this research , Fajri Koto is supported by the Australia Awards Scholarship ( AAS ) , funded by the Department of Foreign Affairs and Trade ( DFAT ) , Australia . 
This research was undertaken using the LIEF HPC - GPGPU Facility hosted at The University of 14https://www.whatsapp.com/ .Melbourne . 
This facility was established with the assistance of LIEF Grant LE170100200 . 
Abstract Sports game summarization focuses on generating news articles from live commentaries . 
Unlike traditional summarization tasks , the source documents and the target summaries for sports game summarization tasks are written in quite different writing styles . 
In addition , live commentaries usually contain many named entities , which makes summarizing sports games precisely very challenging . 
To deeply study this task , we present S PORTS SUM1 , a Chinese sports game summarization dataset which contains 5,428 soccer games of live commentaries and the corresponding news articles . 
Additionally , we propose a two - step summarization model consisting of a selector and a rewriter for S PORTS SUM . 
To evaluate the correctness of generated sports summaries , we design two novel score metrics : name matching score and event matching score . 
Experimental results show that our model performs better than other summarization baselines on ROUGE scores as well as the two designed scores . 
1 Introduction There are a large number of sports games playing every day . 
Apparently , manually writing sports news articles to summarize every game is laborintensive and infeasible . 
How to automatically generate sports summaries , therefore , becomes a popular and demanding task . 
Recently , generating news from live commentaries has gradually attracted attention in the academic community ( Zhang et al . 
, 2016 ; Yao et al . 
, 2017 ) . 
At the same time , several trials have been done in the industry such as sports news from Toutiao ‚Äôs Xiaoming Bot2 , Sohu Ruibao3 and AI football news4 . 
1The dataset is available at https://github.com/ ej0cl6 / SportsSum 2http://www.nbd.com.cn/columns/803 3https://mp.sohu.com/profile?xpt= c29odW1wMzZpdDlzQHNvaHUuY29 t 4https://www.51zhanbao.comLive Commentary Time Scores Commentary Sentence 66 ‚Äô 0 - 0Â§öÁâπËíôÂæ∑ÁêÉÂëòÊ†ºÁ≠ñÊãºÊä¢ÁäØËßÑ , ÂØπÊâãËé∑ÂæóÊéßÁêÉÊùÉ . 
Dortmund ‚Äôs player G ¬®otze fouled , and the opponent got the possession of the ball . 
66 ‚Äô 0 - 0ÊñΩÈ≠èÂõ†ÊñØÊ≥∞Ê†º‰∏∫Êãú‰ªÅÊÖïÂ∞ºÈªëËµ¢Âæó‰∏Ä‰∏™‰ªªÊÑèÁêÉ . 
Schweinsteiger got a free kick for Bayern Munich . 
67 ‚Äô 1 - 0ËøõÁêÉÂï¶ÔºÅÔºÅÔºÅÊãú‰ªÅÊÖïÂ∞ºÈªëÁêÉÂëòÂÖãÁΩóÊñØÂ§ßÁ¶ÅÂå∫Â§ñ Â∑¶ËÑöÂ∞ÑÈó® , ÁêÉ‰ªéÂè≥‰∏ãËßíÈ£ûËøõÁêÉÈó® , ÁêÉËøõ‰∫Ü!Âä©ÊîªÁöÑ ÊòØÁ©ÜÂãí . 
Êãú‰ªÅÊÖïÂ∞ºÈªë1 - 0Â§öÁâπËíôÂæ∑ . 
Goal ! ! ! Bayern Munich ‚Äôs player Kroos shot with his left foot from the outside of the penalty area . 
The ball Ô¨Çew into the goal through the lower right corner . 
The ball went in ! Muller gave the assist . 
Bayern Munich 1 - 0 Dortmund . 
71 ‚Äô 1 - 0Êãú‰ªÅÊÖïÂ∞ºÈªëÁêÉÂëòÈáåË¥ùÈáåÂ§ßÁ¶ÅÂå∫Â∑¶‰æßÂ∞ùËØïÂè≥ËÑöÂ∞Ñ Èó® , ÂèØÊÉúÁöÆÁêÉÈ´òÂá∫ÁêÉÈó® . 
Áªô‰ªñ‰º†ÁêÉÁöÑÊòØÊãâÂßÜ . 
Bayern Munich ‚Äôs player Ribery tried to shoot with his right foot from the penalty area ‚Äôs left side , but the ball was higher than the crossbar . 
Lahm passed the ball to him . 
Sports News Ariticle ÂºÄÂú∫3ÂàÜÈíüÔºåÂÖãÁΩóÊñØÂ∑¶‰æß‰ªªÊÑèÁêÉË¢´È°∂Âà∞ÂêéÁÇπÔºåÈáåË¥ùÈáåÁ¶ÅÂå∫ËæπÁºòÊäΩÂ∞Ñ ÂÅèÂá∫ËøëÈó®Êü±„ÄÇÁ¨¨8ÂàÜÈíüÔºåÁ©ÜÂãíÂè≥Ë∑Ø‰∏éÊõºÊú±Âü∫Â•áÊâìÂá∫Ë∏¢Â¢ôÈÖçÂêàÔºåÂú®Èó® Ââç12Á±≥Â§ÑÊé®Â∞ÑË¢´ËãèÂçöËíÇÂ•áÈì≤Âá∫Â∫ïÁ∫ø„ÄÇÁ¨¨13ÂàÜÈíüÔºåÈáåË¥ùÈáåÂè≥Ë∑ØÂ°ûÁêÉ Ôºå ÂÖãÁΩóÊñØÂú®Èó®Ââç27Á±≥Â§ÑÊäΩÂ∞ÑÂÅèÂá∫ËøëÈó®Êü± „ÄÇ ( ... ) In the 3rd minutes , Kroos ‚Äôs free kick on the left was tipped to the back , and Ribery ‚Äôs shot from the penalty area missed . 
In the 8th minute , Muller and Mandzukic had teamwork , and Muller ‚Äôs shot from the 12 meters ahead the goal line was touched out by Suboti ¬¥ c. In the 13th minute , Ribery passed the ball from the right , and Kroos ‚Äôs shot near the 27 meters ahead the goal line missed . 
( ... ) Table 1 : An example of S PORTS SUMdataset . 
Unlike traditional text summarization tasks ( Hermann et al . 
, 2015 ; Rush et al . 
, 2015 ) , the source documents and the target summaries for sports game summarization tasks are written in quite different styles . 
Live commentaries are the real - time transcripts of the commentators . 
Accordingly , commentary sentences are more colloquial and informal . 
In contrast , news summaries are usually more narrative and well - organized since they are written after the games . 
In addition , commentaries contain a large number of player names . 
One player can be referred to multiple times in the whole game , and one commentary sentence may mention multiple player names simultaneously . 
Those properties609 make sports games summarization tasks very challenging . 
In this paper , we present SPORTS SUM , a Chinese dataset for studying sports game summarization tasks . 
We collect 5,428 pairs of live commentaries and news articles from seven famous soccer leagues . 
To the best of our knowledge , SPORTS SUMis the largest Chinese sports game summarization dataset . 
In addition , we propose a two - step summarization model for SPORTS SUM , which learns a selector to extract important commentary sentences and trains a rewriter to convert the selected sentences to a news article . 
To encourage the model to capture the relations between players and actions better , we replace all the player names in the training sentences with a special token and train the proposed model on the modiÔ¨Åed template - like sentences . 
The proposed model performs better than existing extractive and abstractive summarization baseline models in ROUGE scores ( Lin , 2004 ) . 
However , we observe that ROUGE scores can not evaluate the correctness of generated summaries very well . 
Therefore , we design two new scores , name matching score andevent matching score , as the auxiliary metrics for SPORTS SUM . 
Our experimental results demonstrate that the proposed model is superior to the baseline models in all the metrics . 
Summarizing documents between two articles written in different styles and involving many named entities is not limited to the sports game summarization tasks . 
There are many possible applications , such as summarizing events from tweets and summarizing trends from forum comments . 
We hope that SPORTS SUMprovides a potential research platform to develop advanced techniques for this type of summarization tasks . 
2 Dataset We present SPORTS SUM , a sports game summarization dataset in Chinese . 
Data collection . 
We crawl the records of soccer games from Sina Sports Live5 . 
The collected records contain soccer games in seven different leagues ( Bundesliga , CSL , Europa , La Liga , Ligue 1 , PL , Series A , UCL ) from 2012 to 2018 . 
For each game , we have a live commentary document Cand a news article R , as illustrated in Table 1 . 
The live commentary document Ccon5https://match.sports.sina.com.cn/League # of games Bundesliga 453 CSL 1371 Europa 143 La Liga 713 Ligue 1 161 Premier League 1220 Serie A 890 UCL 477 All 5428 Table 2 : The number of games in different leagues . 
SourceAvg . 
# charsAvg . 
# wordsAvg . 
# sent . 
Total # vocab Commentary 3459.97 1825.63 193.77 43482 News 801.11 427.98 23.80 21294 Table 3 : Statistics of S PORTS SUMdataset . 
sists of a series of tuples ( ti , si , ci ) , where tiis the timeline information , sirepresents the current scores , and cidenotes the commentary sentence . 
The news article Rconsists of several news sentences ri . 
In addition to commentaries and news reports , we also include some metadata , such as rosters , starting lineups , and player positions , which is potentially helpful for sports game summarization tasks . 
Data cleaning . 
The crawled live commentary documents and news articles are quite noisy . 
Therefore , we apply multiple steps of data cleaning to improve the quality of the dataset . 
We Ô¨Årst remove all the HTML tags from the commentary documents and the news articles . 
Then , we observe that there are usually some descriptions that can not be directly inferred from the commentaries at the beginning of news articles , such as matching history . 
Hence , we design a heuristic rule to remove those descriptions . 
We identify several starting keywords which can indicate the start of a game , such as ‚Äú ‰∏Ä ÂºÄÂú∫(at the beginning of the game ) ‚Äù and ‚Äú ÂºÄÂú∫ Âêé(after the game started ) ‚Äù . 
The full list of starting keywords can be found in Appendix A. Once we see a starting keyword appearing in a news report , we remove all the sentences before the starting keyword . 
Finally , we discard those games with the number of news sentences being less than 5 and the number of commentary sentences being less than 20 . 
After data cleaning , we have 5,428 games remaining ( detailed numbers of games are shown in Table 2 ) . 
Notice that SPORTS SUM(5,428 games ) is much larger than the only public sports game summariza-610 tion dataset ( 150 games ) ( Zhang et al . 
, 2016 ) . 
Statistics and properties . 
Table 3 shows the statistics of SPORTS SUM . 
On average , there are 193.77 sentences per commentary document and 23.80 sentences per news article . 
After applying word segmentation by pyltp tool6 , the average numbers of words for commentary documents and news reports are 1825.63 and 427.98 , respectively . 
As mentioned in Section 1 , commentary sentences and news sentences are in quite different writing styles . 
Commentary sentences are more colloquial and informal , while news sentences are more narrative and well - organized . 
Also , commentaries contain a large number of player names , which makes the model easy to generate news reports with incorrect facts , as shown in Section 3 . 
3 Sports Game Summarization The goal of sports game summarization is to generate a sports news report ÀúR={Àúr1,Àúr2 , .. , Àúrn } from a given live commentary document C= { ( t1 , s1 , c1 ) , ... , ( tm , sm , cm ) } . 
The generated news report ÀúRis expected to cover most of the important events in the games and describe those events correctly . 
In this paper , we propose a twostep model for SPORTS SUM . 
The proposed model Ô¨Årst learns a selector to extract important commentary sentences and then utilizes a rewriter to convert the selected sentences to a news article . 
Sentence mapping . 
To train the selector and rewriter , we need some labels to indicate the importance of commentary sentences and the corresponding news sentences . 
To obtain the labels , we consider the timeline information and BERTScore ( Zhang et al . 
, 2020 ) , a metric to measure the sentence similarity , and map each news sentence to a commentary sentence . 
Although we have no explicit timeline information for news sentences , we observe that many news sentences start with ‚Äú in the n - th minute ‚Äù and thus we can extract the timeline information for some news sentences . 
We map sentences by the following steps : 1 ) For each news sentence ri , we extract the timeline information hiif possible . 
Otherwise , we do not map this news sentence . 
2)We consider those commentary sentences cjwithtjbeing close tohi . 
More speciÔ¨Åcally , we consider C(i)= { ck , ck+1 , ... c k+l } , where cjis the commentary sentence with timeline information tj‚àà[hi , hi+3 ] 6https://github.com/HIT-SCIR/pyltpfork‚â§j‚â§k+l.3)We compute BERTScore of the news sentence riand all the commentary sentences in C(i ) . 
The commentary sentence cj‚ààC(i ) with the highest score is considered to be mapped with the news sentences ri . 
With the above mapping process , we obtain a set of mapped commentary sentences and news sentencesD={(¬Øc1,¬Ør1),(¬Øc2,¬Ør2 ) , ... , ( ¬Øcs,¬Ørs ) } , which can be used for training our selector and rewriter . 
Selector . 
There are many commentary sentences in a live commentary document , but only few of them contain valuable information and should be reported in the news article . 
Therefore , we learn aselector to pick up those important sentences . 
More speciÔ¨Åcally , Given a commentary document C={(t1 , s1 , c1 ) , ... , ( tm , sm , cm ) } , the selector outputs a set Cselect = { Àúc1,Àúc2 , ... , Àúcn}which contains only important commentary sentences . 
We train a binary classiÔ¨Åer as the selector to choose important commentary sentences . 
When training , for each commentary sentence ciinC , we assign a positive label if cican be mapped with a news sentence by the aforementioned mapping process . 
Otherwise , we give a negative label . 
Rewriter . 
The rewriter converts the selected commentary sentences Cselect = { Àúc1,Àúc2 , ... , Àúcn}to a news report ÀúR={Àúr1,Àúr2 , .. , Àúrn } . 
We focus on the sentence - level rewriter . 
That is , we convert each selected commentary sentence Àúcito a news sentence Àúri . 
An intuitive way to learn the sentencelevel rewriter is training a sequence - to - sequence ( seq2seq ) model , such as LSTM ( Hochreiter and Schmidhuber , 1997 ) and Transformer ( Vaswani et al . 
, 2017 ) , on the mapped sentences D. However , as illustrated in Table 4 , we observe that the seq2seq model tends to generate high - frequency player names rather than the correct player names even though the high - frequency player names do not appear in the commentary sentences . 
We call this situation name mismatch problem . 
To solve the name mismatch problem , we train the rewriter in a template - to - template ( tem2tem ) way instead of in a seq2seq way . 
We Ô¨Årst build a dictionary of player names from the lineup data ( metadata ) . 
Next , for each ( ¬Øci,¬Øri)inD , we replace all the player names in ¬Øciand¬Øriwith a special token ‚Äú [ player ] ‚Äù so that the new sentence is like a template . 
If there are multiple player names in a sentence , we append a number to the special token to distinguish them , as shown in Table 5.611 Live Commentary SentenceÈáå Èáå ÈáåË¥ù Ë¥ù Ë¥ùÈáå Èáå ÈáåÁ¶ÅÂå∫Â∑¶‰æßÂ∞ùËØïÂè≥ËÑöÂ∞ÑÈó® , ÁöÆÁêÉÈ´òÂá∫ÁêÉÈó® . 
Áªô‰ªñ‰º†ÁêÉÁöÑÊòØÊãâ Êãâ ÊãâÂßÜ ÂßÜ ÂßÜ . 
Ribery tried to shoot with his right foot from the left side of the penalty area , but the ball was higher than the crossbar . 
Lahm passed the ball to him . 
Gound Truth News SentenceÊãâ Êãâ ÊãâÂßÜ ÂßÜ ÂßÜËΩ¨ÁßªÂà∞Â∑¶‰æßÔºåÈáå Èáå ÈáåË¥ù Ë¥ù Ë¥ùÈáå Èáå ÈáåÁ™ÅÂÖ•Á¶ÅÂå∫Â∑¶‰æßË∑ùÈó®12Á±≥Â§ÑÊäΩÂ∞ÑÈ´òÂá∫ „ÄÇ Lahm passed the ball to the left , and Ribery cut in the left penalty area and shot from 12 meters ahead the goal line . 
The shot was too high . 
News Sentence Generated by Seq2seq ModelÈáå Èáå ÈáåË¥ù Ë¥ù Ë¥ùÈáå Èáå Èáå‰º†ÁêÉÔºåÊõº Êõº ÊõºÊú± Êú± Êú±Âü∫ Âü∫ Âü∫Â•á Â•á Â•áÁ¶ÅÂå∫Â∑¶‰æßÂ∞ÑÈó®ÂÅèÂá∫ËøúÈó®Êü± „ÄÇ Ribery passed the ball and Mandzukic ‚Äôs shot from the left side of the penalty area was out of the goalpost . 
Table 4 : An example of the name mismatch problem . 
Seq2seq model tends to generate high - frequency player names rather than the correct names . 
Input Sentence Output Sentence Seq2seqÂ∞ÑÈó®!!!ÈáåË¥ùÈáåÁêÉÈó®Á∫øË∑üÂâçÂè≥ËÑöÂ∞ÑÈó® , Ë¢´ÈòøÂæ∑Âãí Ê®™Ë∫´ÊâëÂá∫ . 
Áªô‰ªñ‰º†ÁêÉÁöÑÊòØÊãâÂßÜ . 
ÊãâÂßÜÂè≥Ë∑Ø‰Ωé‰º†ÔºåÈáåË¥ùÈáåÂâçÁÇπÈì≤Â∞ÑË¢´ÈòøÂæ∑ÂãíÂ∞Å Âá∫ „ÄÇ Shoot ! ! ! Ribery ‚Äôs right foot shot in front of the goal line was saved by Adler . 
Lahm passed the ball to him . 
Lahm made a low pass on the right and Ribery ‚Äôs shot from the front was blocked by Adler . 
Tem2temÂ∞ÑÈó®!!![player1 ] ÁêÉÈó®Á∫øË∑üÂâçÂè≥ËÑöÂ∞ÑÈó® , Ë¢´[player2 ] Ê®™Ë∫´ÊâëÂá∫ . 
Áªô‰ªñ‰º†ÁêÉÁöÑÊòØ[player3 ] .[player3]Âè≥Ë∑Ø‰Ωé‰º†Ôºå[player1]ÂâçÁÇπÈì≤Â∞Ñ Ë¢´[player2 ] Â∞ÅÂá∫ „ÄÇ Shoot ! ! ! [ player1 ] ‚Äôs right foot shot in front of the goal line was saved by [ player2 ] .[player3 ] passed the ball to him.[player3 ] made a low pass on the right and [ player1 ] ‚Äôs shot from the front was blocked by [ player2 ] . 
Table 5 : Training models by seq2seq versus training models by tem2tem . 
After converting ¬Øciand¬Ørito the template sentences , we train a seq2seq model on the template sentences . 
By training models in a tem2tem way , the model focuses more on the relations between players and actions and is less inÔ¨Çuenced by the high - frequency player names . 
When predicting , for each commentary sentence ÀúciinCselect , we use the aforementioned way to convert Àúcito a commentary template sentence . 
Then , we generate a news template sentence by the rewriter and replace all the special tokens in the sentence with the original player names . 
4 Experiments SPORTS SUMcontains 5,428 games and we split them into three sets : training ( 4,828 games ) , validation ( 300 games ) , and testing ( 300 games ) sets . 
Evaluation . 
We consider ROUGE scores ( Lin , 2004 ) , which are standard metrics for summarization tasks . 
More precisely , we focus on ROUGE-1 , ROUGE-2 , and ROUGE - L. However , we observe that ROUGE scores can not accurately evaluate the correctness of summaries . 
Some summaries may get high ROUGE scores but contain many incorrect facts . 
Therefore , we design two metrics : name matching score ( NMS ) and event matching score ( EMS).The name matching score evaluates the closeness of the player names in the ground truth news article Rand the generated summaries ÀúR. LetNgandNp denote the set of the player names appearing in R andÀúR , respectively . 
We deÔ¨Åne the name matching score as NMS ( R,ÀúR ) = F - score ( Ng , Np ) . 
Similarly , the event matching score evaluates the closeness of the events in RandÀúR. We deÔ¨Åne an event as a pair ( subject , verb ) in the sentence . 
Two pairs ( subject1,verb 1)and ( subject2,verb 2 ) are viewed as equivalent if and only if 1)subject1 is the same as subject2and2)verb 1andverb 2are synonym7to each other . 
Let EgandEprepresent the set of events in RandÀúR , respectively , the event matching score is deÔ¨Åned as EMS(R,ÀúR ) = F - score ( Eg , Ep ) . 
Implementations and Models . 
We consider the convolutional neural network ( Kim , 2014 ) as the selector . 
For the rewriter , we consider the following : ( 1)LSTM : a bidirectional LSTM with attention mechanism ( Bahdanau et al . 
, 2015 ) . 
( 2 ) Transformer . 
( Vaswani et al . 
, 2017 ) ( 3 ) PGNet : pointergenerator network , an encoder - decoder model with copy mechanism ( See et al . 
, 2017 ) . 
7Details to decide synonyms can be found in Appendix B.612 Method Model ROUGE-1 ROUGE-2 ROUGE - L NMS EMS Extractive ModelsRawSent 26.52 7.64 25.42 57.33 36.17 LTR 24.44 6.39 23.19 51.63 29.03 Abstractive ModelsAbs - LSTM 30.54 10.16 29.78 10.87 14.03 Abs - PGNet 34.02 11.09 33.13 17.87 19.76 Selector + Rewriter ( Seq2seq)LSTM 41.39 16.99 40.53 28.48 25.19 Transformer 41.71 18.10 40.96 35.63 30.94 PGNet 43.17 18.66 42.27 48.18 36.94 Selector + Rewriter ( Tem2tem)LSTM 41.71 17.08 40.82 59.54 40.34 Transformer 41.47 17.18 40.54 58.26 39.33 PGNet 41.95 17.09 41.01 59.35 40.46 Table 6 : Evaluation results . 
NMS and EMS represent the name matching score and the event matching score . 
For comparison , we consider two extractive summarization baselines : ( 1 ) RawSent : the raw sentences selected by the selector without rewriting . 
( 2)LTR : the learning - to - rank approach for sports game summarization proposed by the previous work ( Zhang et al . 
, 2016 ) . 
In addition , we train a bidirectional LSTM with attention mechanism ( Abs - LSTM ) and a pointergenerator network ( Abs - PGNet ) on the paired commentaries and news articles as two simple abstractive summarization baselines . 
More implementation details can be found in Appendix C. Results . 
Table 6 shows the experimental results . 
We observe that the extractive models ( RawSent and LTR ) get low ROUGE scores but high NMS and EMS . 
That means the extractive models can generate summaries with correct information , but the writing style is different from the ground truth . 
On the contrary , the abstractive models get higher ROUGE scores but lower NMS and EMS . 
That implies the summaries generated by the abstractive models usually contain incorrect facts . 
Our proposed two - step model performs better than the extractive models and the abstractive models on ROUGE scores , NMS , and EMS . 
This veriÔ¨Åes our design of the selector and the rewriter . 
In addition , we observe that when training the model in a tem2tem way , we can get better NMS and EMS , which implies that training by tem2tem can improve the correctness of summaries . 
5 Related Work Text summarization . 
Existing approaches can be grouped into two families : extractive models and abstractive models . 
Extractive models select a part of sentences from the source document as the summary . 
Traditional approaches ( Carbonell and Goldstein , 1998 ; Erkan and Radev , 2004 ; Mc - Donald , 2007 ) utilize graph or optimization techniques . 
Recently , neural models achieve good performance ( Cheng and Lapata , 2016 ; Nallapati et al . 
, 2017 ; Jadhav and Rajan , 2018 ) . 
Abstractive summarization models aim to rephrase the source document . 
Most work applies neural models for this task . 
( Rush et al . 
, 2015 ; Chopra et al . 
, 2016 ; Nallapati et al . 
, 2016 ; Zeng et al . 
, 2016 ; See et al . 
, 2017 ; Gehrmann et al . 
, 2018 ) . 
Factual correctness of summaries . 
There is a lot of work focusing on evaluation and improvement of the factual correctness of summaries ( Falke et al . 
, 2019 ; Kryscinski et al . 
, 2019 ; Wang et al . 
, 2020 ; Maynez et al . 
, 2020 ; Zhu et al . 
, 2020 ) . 
Data - to - Text generation . 
Recently , generating news articles from different kinds of data - records becomes a popular research direction . 
Wiseman et al . 
( 2017 ) ; Puduppully et al . 
( 2019 ) focus on generating news from boxed - data . 
Zhang et al . 
( 2016 ) and Yao et al . 
( 2017 ) study generating sports news from live commentaries , but their methods are based on hand - crafted features . 
6 Conclusion We present SPORTS SUM , a Chinese dataset for sports game summarization , as well as a model that consists of a selector and a rewriter . 
To improve the quality of generated news , we train the model in a tem2tem way . 
We design two metrics to evaluate the correctness of generated summaries . 
The experimental results demonstrate that the proposed model performs well on ROUGE scores and the two designed scores . 
Acknowledgments We thank Tecent AI Lab , UCLA - NLP group , and anonymous reviewers for their feedback.613 A Starting Keywords We consider the following regular expressions as the starting keywords : ‚Ä¢‰∏ÄÂºÄÂú∫ ‚Ä¢ÂºÄÂú∫Âêé ‚Ä¢ÂºÄÂú∫[\d]+ÂàÜÈíü ‚Ä¢ÂºÄÂßã[\d]+ÂàÜÈíü ‚Ä¢ÂºÄÂú∫[‰ªÖ][\d]+Áßí ‚Ä¢[\d]+Áßí ‚Ä¢Á¨¨[\d]+ÂàÜÈíü ‚Ä¢[\d]+ÂàÜÈíü ‚Ä¢[\d]+[Á±≥Á†Å ] B Event Matching Score We pick up the top 300 most frequent verbs and ask human to annotate if the verb is an important verb for soccer games or not . 
Then , we ask human to cluster those important verbs based on their meanings . 
When calculating the event matching score , we only consider those verbs . 
Two verbs are viewed as the synonym to each other if they are in the same group . 
The groups of verbs are as follows : ‚Ä¢Shooting : Â∞ÑÈó® , ÊâìÈó® , ÊîªÈó® , ÊäΩÂ∞Ñ , Êé®Â∞Ñ , Âä≤ Â∞Ñ , ËøúÂ∞Ñ , ‰ΩéÂ∞Ñ , Ë°•Â∞Ñ , Êâ´Â∞Ñ , ÊñúÂ∞Ñ , ÊçÖÂ∞Ñ , Â∞Ñ , ÊÄíÂ∞Ñ , Ëµ∑ËÑö , Èì≤Â∞Ñ , Âû´Â∞Ñ , ÂêäÂ∞Ñ , ÊåëÂ∞Ñ , ÂºπÂ∞Ñ , ÂãæÂ∞Ñ , ÁàÜÂ∞Ñ , Â§¥ÁêÉ , Áî©Â§¥ ‚Ä¢Missed Shot : ÂÅèÂá∫ , È´òÂá∫ , ÊâìÂÅè , ÂºπÂá∫ , ÊâìÈ´ò , ÂºπÂõû , ÊâìÈ£û , È°∂È´ò , È°∂ÂÅè , Ë∂ÖÂá∫ , Â∞ÑÂÅè , Ëπ≠ÂÅè , Ëπ≠Âá∫ , ÊªëÂá∫ ‚Ä¢Passing : ‰º†‰∏≠ , ‰º†ÁêÉ , Êñú‰º† , ÈÄÅÂá∫ , Â§¥ÁêÉÊëÜÊ∏° , Áõ¥Â°û , Ê®™‰º† , Êåë‰º† , Áõ¥‰º† , ‰Ωé‰º† , Ê®™Êï≤ , ÁªôÂà∞ , ‰º†ÂÖ• , ‰º† , ‰º†Âà∞ , Â¶ô‰º† , ÊñúÂ°û , Èïø‰º† , Áü≠‰º† , Âõû ‰º† , ÂõûÊï≤ , ÂõûÁÇπ , ÂàÜÁêÉ ‚Ä¢Blocking : ÊâëÂá∫ , Êå°Âá∫ , Ê≤°Êî∂ , Â∞ÅÂ†µ , ÂæóÂà∞ , Â∞Å Âá∫ , ÊâòÂá∫ , Êâë‰Ωè , Êïë‰∏ã , Êä±‰Ωè , ÊïëÂá∫ ‚Ä¢Defense : Ëß£Âõ¥ , Á†¥Âùè , Èì≤Âá∫ , ÂåñËß£ ‚Ä¢Foul : ÁäØËßÑ , ÂêÉÂà∞ , Ë≠¶Âëä , Âà§ÁΩö , Ë¢´Âà§ , È¢ÜÂà∞ , ÁΩö‰∏ã , Âá∫Á§∫ , Ë¢´ÁΩö C Implementation Details For the selector , we consider CNN with the same architecture in ( Kim , 2014 ) and set the learning rate to 10‚àí3 . 
For the rewriter , the implementation details are as follows:‚Ä¢LSTM : we use a bidirectional LSTM with the attention mechanism ( Bahdanau et al . 
, 2015 ) . 
The size of hidden state is set to 300 . 
We set the learning rate to 10‚àí3 . 
‚Ä¢Transformer : we use the Transformer with the same architecture in the original paper ( Vaswani et al . 
, 2017 ) . 
We set the learning rate to 10‚àí4 . 
‚Ä¢PGNet : we implement the pointer - generator network ( See et al . 
, 2017 ) and set the size of hidden state to 300 . 
We set the learning rate to10‚àí3 . 
‚Ä¢LSTM - abs : we use a bidirectional LSTM with the attention mechanism ( Bahdanau et al . 
, 2015 ) . 
The size of hidden state is set to 300 . 
We set the learning rate to 10‚àí3 . 
‚Ä¢PGNet - abs : we implement the pointergenerator network ( See et al . 
, 2017 ) and set the size of hidden state to 300 . 
We set the learning rate to 10‚àí3 . 
For all the models , we use the 200 - dimensional pre - trained Chinese word embedding from Tecent AI Lab8 . 
8https://ai.tencent.com/ailab/nlp/ embedding.html615 Proceedings of the 1st Conference of the Asia - PaciÔ¨Åc Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing , pages 616‚Äì625 December 4 - 7 , 2020 . 
¬© 2020 Association for Computational Linguistics Massively Multilingual Document Alignment with Cross - lingual Sentence - Mover ‚Äôs Distance Ahmed El - Kishky Facebook AI ahelk@fb.comFrancisco Guzm ¬¥ an Facebook AI fguzman@fb.com Abstract Document alignment aims to identify pairs of documents in two distinct languages that are of comparable content or translations of each other . 
Such aligned data can be used for a variety of NLP tasks from training cross - lingual representations to mining parallel data for machine translation . 
In this paper we develop an unsupervised scoring function that leverages cross - lingual sentence embeddings to compute the semantic distance between documents in different languages . 
These semantic distances are then used to guide a document alignment algorithm to properly pair cross - lingual web documents across a variety of low , mid , and high - resource language pairs . 
Recognizing that our proposed scoring function and other state of the art methods are computationally intractable for long web documents , we utilize a more tractable greedy algorithm that performs comparably . 
We experimentally demonstrate that our distance metric performs better alignment than current baselines outperforming them by 7 % on high - resource language pairs , 15 % on mid - resource language pairs , and 22 % on low - resource language pairs . 
1 Introduction While the Web provides a large amount of monolingual text , cross - lingual parallel data is more difÔ¨Åcult to obtain . 
Despite its scarcity , parallel cross - lingual data plays a crucial role in a variety of tasks in natural language processing such as machine translation . 
Previous works have shown that training on sentences extracted from parallel or comparable documents mined from the Web can improve machine translation models ( Munteanu and Marcu , 2005 ) or learning word - level translation lexicons ( Fung and Yee , 1998 ; Rapp , 1999 ) . 
Other tasks that leverage these parallel texts include cross - lingual information retrieval , document classiÔ¨Åcation , and multilingual representations such as SourceTargetFigure 1 : Documents in a source and target langauge in the same web - domain . 
Solid lines indicate crosslingual document pairs . 
XLM ( Lample and Conneau , 2019 ) . 
Document alignment is a method for obtaining cross - lingual parallel data that seeks to pair documents in different languages such that pairs are translations or near translations of each other . 
As seen in Figure 1 , this involves a one - to - one pairing of documents in a source language with documents in a target language . 
To automate and scale the process of identifying these documents pairs , we introduce an approach to accurately mine comparable web documents across a variety of low , mid , and high - resource language directions . 
Previous approaches have been applied to homogeneous corpora , however mining the Web involves analyzing a variety of heterogeneous data sources ( Koehn et al . 
, 2002 ) . 
Other approaches rely on corpus - speciÔ¨Åc features such as metadata and publication date which can be inconsistent and unreliable ( Munteanu and Marcu , 2005 ; AbduI - Rauf and Schwenk , 2009 ) . 
Related methods utilize document structure when calculating document similarity ( Resnik and Smith , 2003 ; Chen and Nie , 2000 ) . 
However , when mining large , unstructured collections of web documents these features are often missing or unreliable . 
As such , we introduce an approach that aligns documents based solely on semantic distances between their textual content . 
For our approach , we Ô¨Årst decompose documents into sentences , and encode each sentence into a cross - lingual semantic space yielding a bag-616 of - sentences representation . 
Utilizing the dense , cross - lingual representation of sentences , we then compute document distances using a variant of earth mover ‚Äôs distance where probability mass is moved from the source document to the target document . 
We then leverage these document distances as a guiding metric for identifying cross - lingual document pairs and demonstrate experimentally that our proposed method outperforms state - of - theart baselines that utilize cross - lingual document representations . 
2 Related Works Crawling and mining the web for parallel data has been previously explored by Resnik ( 1999 ) where the focus is on identifying parallel text from multilingual data obtained from a single source . 
For example , parallel corpora were curated from the United Nations General Assembly Resolutions ( Rafalovitch et al . 
, 2009 ; Ziemski et al . 
, 2016 ) and from the European Parliament ( Koehn , 2005 ) . 
However , curating from homogeneous sources by deriving domain - speciÔ¨Åc rules does not generalize to arbitrary web - domains . 
Other approaches rely on metadata for mining parallel documents in unstructured web corpora . 
Some methods leveraged publication date and other temporal heuristics to identifying parallel documents ( Munteanu and Marcu , 2005 , 2006 ; Udupa et al . 
, 2009 ; Do et al . 
, 2009 ; AbduI - Rauf and Schwenk , 2009 ) . 
However , temporal features are often sparse , noisy , and unreliable . 
Another class of alignment methods rely on document structure ( Resnik and Smith , 2003 ; Chen and Nie , 2000 ) yet these structure signals can be sparse and may not generalize to new domains . 
In the WMT-2016 bilingual document alignment shared task ( Buck and Koehn , 2016a ) , many techniques were proposed to retrieve , score , and align cross - lingual document pairs . 
However this shared task only considered English to French ‚Äì a highresource direction and the proposed techniques were not readily extendable to more languages . 
Several approaches translate the target corpus into the source language , then apply retrieval and matching approaches on translated 2 - grams and 5grams to query , retrieve , and align documents ( Dara and Lin , 2016 ; Gomes and Lopes , 2016 ) . 
These methods rely on high - quality translation systems to translate , however such models may not exist , especially for low - resource language directions . 
Ad - ditionally , these methods leverage rare n - grams to identify likely candidates , yet low - frequency words and phrases that are likely to be mistranslated by machine translation systems . 
In the shared task , many document similarity measures were investigated for use in aligning English to French web documents . 
One method utilized a phrase table from a phrase - based statistical machine translation system to compute coverage scores , based on the ratio of phrase pairs covered by a document pair ( Gomes and Lopes , 2016 ) . 
Other methods utilize the translated content of the target ( French ) document , and Ô¨Ånd the source ( English ) corresponding document based on n - gram matches in conjunction with a heuristic document length ratio ( Dara and Lin , 2016 ; Shchukin et al . 
, 2016 ) . 
Other methods translate the target documents into the source language and apply cosine similarity between tf / idf weighted vectors on unigrams and n - grams ( Buck and Koehn , 2016b ; Medve Àád et al . 
, 2016 ; Jakubina and Langlais , 2016 ) . 
Finally , several methods were introduced that score pairs using metadata in each document such as links to documents , URLs , digits , and HTML structure ( Espl ` aGomis et al . 
, 2016 ; Papavassiliou et al . 
, 2016 ) . 
Recently , the use of neural embedding methods has been explored for bilingual alignment of text at the sentence and document level . 
One method proposes using hierarchical document embeddings , constructed from sentence embeddings , for bilingual document alignment ( Guo et al . 
, 2019 ) . 
Another method leverages a multilingual sentence encoder to embed individual sentences from each document , then performs a simple vector average across all sentence embeddings to form a dense document representation with cosine similarity guiding document alignment ( El - Kishky et al . 
, 2019 ) . 
Word mover ‚Äôs distance ( WMD ) is an adaptation of earth mover ‚Äôs distance ( EMD ) ( Rubner et al . 
, 1998 ) that has been recently used for document similarity and classiÔ¨Åcation ( Kusner et al . 
, 2015 ; Huang et al . 
, 2016 ; Atasu et al . 
, 2017 ) . 
Other methods have leveraged the distance for cross - lingual document retrieval ( Balikas et al . 
, 2018 ) . 
However these methods treat individual words as the base semantic unit for comparison which are intractable for large web - document alignment . 
Finally , sentence mover ‚Äôs similarity has been proposed for automatically evaluating machinegenerated texts outperforming ROUGE ( Clark et al . 
, 2019 ) . 
This method is purely monolingual617 and sentence representations are constructed by summing individual word embeddings . 
3 Problem DeÔ¨Ånition Given a set of source documents , Dsand a set of target documents Dt , there exist|Ds|√ó|Dt| potential pairs of documents of the form ( ds , dt ) . 
LetPbe the set of all candidate pairs ( Ds√óDt ) . 
Then cross - lingual document alignment aims to Ô¨Ånd the largest mapping from source documents to target documents , P / prime‚äÇP , s.t . 
given an DsandDt where , without a loss of generality , |Ds|‚â§|Dt| , the largest injective function mapping betweenDs andDt : ‚àÄa , b‚ààDs,(a , c)‚ààP / prime‚àß(b , c)‚ààP / prime=‚áía = b In other words , each source document and target document can only be used in at most a single pair . 
This can be seen in Figure 1 where within the same web - domain , given source and target documents , the task is to match each source document to a unique target document where possible . 
To Ô¨Ånd the best possible mapping between Ds andDtwe require two components : 1 ) a similarity functionœÜ(ds , dt)which is used to score a set of candidate document pairs according to their semantic relatedness ; and 2 ) an alignment or matching algorithm which uses the scores for each of the pairs inDs√óDtto produce an alignment of size min(|Ds|,|Dt|)representing the best mapping according toœÜ(ds , dt ) . 
4 Cross - Lingual Sentence Mover ‚Äôs Distance WMD fails to generalize to our use case for two reasons : ( 1 ) it relies on monolingual word representations which fail to capture the semantic distances between different language documents ( 2 ) intractability due to long web documents or lack word boundaries in certain languages . 
To address this , we introduce cross - lingual sentence mover ‚Äôs distance ( SMD ) and show that representing each document as a bag - of - sentences ( BOS ) and leveraging recent improvements in multilingual sentence representations , SMD can better identify cross - lingual document pairs . 
4.1 Cross - Lingual Sentence Mover ‚Äôs Distance Our proposed SMD solves the same optimization problem as WMD , but utilizes cross - lingual sentence embeddings instead of word embeddings asthe base semantic . 
In particular , we utilize LASER sentence representations ( Artetxe and Schwenk , 2019 ) . 
LASER learns to simultaneously embed 93 languages covering 23 different alphabets into a joint embedding space by training a sequence - tosequence system on many language pairs at once using a shared encoder and a shared byte - pair encoding ( BPE ) vocabulary for all languages . 
Utilizing LASER , each sentence is encoded using an LSTM encoder into a Ô¨Åxed - length dense representation . 
We adapt EMD to measure the distance between two documents by comparing the distributions of sentences within each document . 
More speciÔ¨Åcally , SMD represents each document as a normalized bag - of - sentences ( nBOS ) where each sentence has associated with it some probability mass . 
As distances can be computed between dense sentence embeddings , the overall document distance can then be computed by examining how close the distribution of sentences in the source document is to sentences in the target document . 
We formulate this distance as the minimum cost of transforming one document into the other . 
For our basic formulation of SMD , each document is represented by the relative frequencies of sentences , i.e. , for the ithsentence in the document , dA , i = cnt(i)/|A| ( 1 ) where|A|is the total number of sentence in document A , and d B , iis deÔ¨Åned similarly for document B. Under this assumption , each individual sentence in a document is equally important and probability mass is allocated uniformly to each sentence . 
Later , we will investigate alternative schemes to allocating probability mass to sentences . 
Now let the ithsentence be represented by a vectorvi‚ààRm . 
This length - m dense embedding representation for each sentence allows us to deÔ¨Åne distances between the i thand j thsentences . 
We denote ‚àÜ(i , j)as the distance between the ith andjthsentences and let Vdenote the vocabulary size where the vocabulary is the unique set of sentences within a document pair . 
We follow previous works ( Kusner et al . 
, 2015 ) and use the Euclidean distance , ‚àÜ(i , j ) = ||vi‚àívj|| . 
The SMD between a document pair is then the solution to the linear program : SMD ( A , B ) = min T‚â•0V / summationdisplay i=1V / summationdisplay j=1Ti , j√ó‚àÜ(i , j)(2)618 subject to : ‚àÄiV / summationdisplay j=1Ti , j = dA , i ‚àÄjV / summationdisplay i=1Ti , j = dB , j WhereT‚ààRV√óVis a nonnegative matrix , where each Ti , jdenotes how much of sentence iin document Ais assigned to sentences jin documentB , and constraints ensure the Ô¨Çow of a given sentence can not exceed its allocated mass . 
Specifically , SMD ensures the the entire outgoing Ô¨Çow from sentence iequalsdA , i , i.e./summationtext jTi , j = dA , i. Additionally , the amount of incoming Ô¨Çow to sentencejmust match dB , j , i.e. ,/summationtext iTi , j = dB , j. 4.2 Alternative Sentence Weighting Schemes In Equation 1 , each document is represented as a normalized bag - of - sentences ( nBOS ) where sentences are equally weighted . 
However , we posit that some sentences may be more semantically important than others . 
Sentence Length Weighting The Ô¨Årst insight we investigate is that documents will naturally be segmented into sentences of different lengths based on the language , content , and choice of segmentation . 
While Equation 1 , treats each sentence equally , we posit that longer sentences should be assigned larger weighting than shorter sentences . 
As such , we weight each sentence by the number of tokens in the sentence relative to the total number of tokens in the entire document , i.e. , for theithsentence in the document A , we compute the weighting SL(i ) as follows : dA , i = cnt(i)¬∑|i|//summationdisplay s‚ààAcnt(s)¬∑|s| ( 3 ) where|i|and|s|indicate the number of tokens in sentencesiandsrespectively . 
As such , longer sentence receive larger probability mass than shorter sentences . 
IDF Weighting The second insight we investigate is that text segments such as titles and navigation text is ubiquitous in crawled data yet less semantically informative . 
Based on this insight , we apply a variant of inverse document frequency ( IDF ) ‚Äì a weighting scheme common in the information retrieval space ‚Äì to individual sentences ( Robertson , 2004 ) . 
Under this scheme , themore common a sentence is within a webdomain , the less mass the sentence will be allocated . 
For sentence iin a web - domain D , we compute IDF(i ) as follows : dA , i= 1 + log|D| |{d‚ààD : i‚ààd}|(4 ) where|{d‚ààD : s‚ààd}|is the number of documents where the sentence soccurs and smoothing by1is performed to prevent 0 IDF . 
SLIDF Weighting Finally , we propose combining both sentence length and inverse document frequency into a joint weighting scheme : dA , i = SL(i)¬∑IDF ( i ) ( 5 ) In this scheme , each sentence is weighted proportionally to the number of tokens it contains as well as by the IDF of the sentence within the domain . 
This weighting scheme is reminiscent of the use of tf - idf to determine word relevance ( Ramos et al . 
, 2003 ) , but instead sentence length and idf are used to determine sentence importance . 
4.3 Fast Distance Approximation While EMD and other variants have demonstrated superior performance in many retrieval and classiÔ¨Åcation tasks , they have also been shown to suffer from high computational complexity O(p3logp ) , wherepdenotes the number of unique semantic units in a document pair . 
As such , we investigate techniques to speed up this computation . 
Relaxed SMD Given the scalability challenges for computing WMD , simpliÔ¨Åed version of WMD was proposed that relaxes one of the two constraints in the original formulation ( Kusner et al . 
, 2015 ) . 
Applying the same principle to SMD , we formulate : SMD ( A , B ) = min T‚â•0V / summationdisplay i=1V / summationdisplay j=1Ti , j√ó‚àÜ(i , j ) subject to:‚àÄi / summationtextV j=1Ti , j = dA , i. Analogous to the relaxed - WMD , this relaxed problem yields a lower - bound to the SMD as every SMD solution satisfying both constraints remains a feasible solution if one constraint is removed . 
The optimal solution can be found by simply allocating the mass in each source sentence to the closest sentence in the target document.619 The same computation can be performed in the reverse direction by removing the second constraint:‚àÄj / summationtextV i=1Ti , j = dB , j. Similarly , the optimal solution allocates the mass sentences in the target document to the closest sentence in the source document . 
Both these distances can be calculated by computing the distance matrix between all pairs of sentences inO(p2)time . 
For a tighter estimate of distance , the maximum of the two resultant distances can be used . 
Greedy Mover ‚Äôs Distance We introduce an alternative to the relaxed - EMD variant wherein we keep both constraints in the transportation problem , but identify an approximate transportation scheme . 
This greedy mover ‚Äôs distance ( GMD ) Ô¨Ånds the closest sentence pair between the source and target and moves as much mass between the two sentences as possible ; the algorithm moves to the next closest until all mass has been moved while maintaining both constraints . 
Algorithm 1 : Greedy Mover ‚Äôs Distance Input : ds , dt , ws , wt Output : ‚àÜ(ds , dt ) 1pairs‚Üê{(ss , st)forss , st‚ààds√ódt } in ascending order by /bardblss‚àíst / bardbl 2distance‚Üê0.0 3forss , st‚ààpairs do 4 Ô¨Çow‚Üêmin(ws[ss ] , wt[st ] ) 5 ws[ss]‚Üêws[ss]‚àíÔ¨Çow 6 wt[st]‚Üêwt[st]‚àíÔ¨Çow 7 distance‚Üêdistance + /bardblss‚àíst / bardbl√óÔ¨Çow 8end 9return total As seen in Algorithm 1 , the algorithm takes a source document ( ds ) and a target document ( dt ) as well as the probability mass for the sentences in each : respectively wsandwt . 
The algorithm Ô¨Årst computes the euclidean distance between each sentence pair from source to target and sorts these pairs in ascending order by their euclidean distance . 
The algorithm then iteratively chooses the closest sentence pair and moves the mass of the smallest sentence from the source to the target and subtracting this moved math from both . 
The algorithm terminates when all moveable mass has been moved . 
Unlike the exact solution to EMD , the runtime complexity is a more tractable O(|ds||dt|√ólog(|ds||dt|))which is dominated by the cost of sorting all candidate pairs . 
Unlike the relaxation , both constraints are satisÔ¨Åed but the transport is not necessarily optimal . 
As such , GMDyields an upper - bound to the exact computation . 
We experimentally compare the effect of both approximation strategies on downstream document alignment in Section 7 . 
5 Document Matching Algorithm In addition to a distance metric ( i.e. SMD ) , we need a document matching algorithm to determine the best mapping between documents in two languages . 
In our case , this works as follows : for any given webdomain , each document in the source document set , Dsis paired with each document in the target set , Dt , yielding|Ds√óDt|scored pairs ‚Äì a fully connected bipartite graph representing all candidate pairings . 
Similar to previous works ( Buck and Koehn , 2016b ) , the expected output assumes that each webpage in the non - dominant language has a translated or comparable counterpart . 
As visualized in Figure 1 , this yields a min(|Ds|,|Dt| ) expected number of aligned pairs . 
While an optimal matching maximizing scoring can be solved using the Hungarian algorithm ( Munkres , 1957 ) , the complexity of this algorithm isO(max(|Ds||Dt|)3)which is intractable to even moderately sized web domains . 
As such , similar to the work in ( Buck and Koehn , 2016b ) , a one - to - one matching between English and nonEnglish documents is enforced by applying , competitive matching , a greedy bipartite matching algorithm . 
Algorithm 2 : Competitive Matching Input : P={(ds , dt)|ds‚ààDs , dt‚ààDt } Output : P / prime={(ds , i , dt , i ) , ... } ‚äÇP 1scored‚Üê{(p , score ( p))forp‚ààP } 2sorted‚Üêsort(scored ) in ascending order 3aligned‚Üê‚àÖ 4Ss‚Üê‚àÖ 5St‚Üê‚àÖ 6fords , dt‚ààsorted do 7 ifds/‚ààSs‚àßdt/‚ààStthen 8 aligned‚Üêaligned‚à™{(ds , dt ) } 9 Ss‚ÜêSs‚à™ds 10 St‚ÜêSt‚à™dt 11end 12return aligned In Algorithm 2 , the algorithm Ô¨Årst scores each candidate document pair using a distance function and then sorts pairs from closest to farthest . 
The algorithm then iteratively selects the closest document pair as long as the dsanddtof each pair have not been used in a previous ( closer ) pair . 
The620 algorithm terminates when min(|Ds|,|Dt|)pairs have been selected . 
Unlike the Hungarian algorithm , the runtime complexity is a more tractable O(|Ds||Dt|√ólog(|Ds||Dt|))which is dominated by the cost of sorting all candidate pairs . 
6 Experiments and Results In this section , we explore the question of whether SMD can be used as a dissimilarity metric for the document alignment problem . 
Moreover , we explore which sentence weighting schemes yield the best results . 
6.1 Experimental Setup Dataset We evaluate on the test set from the URL - Aligned CommonCrawl dataset ( El - Kishky et al . 
, 2019 ) across 47 language directions . 
Baseline Methods For comparison , we implemented two existing and intuitive document scoring baselines from ( El - Kishky et al . 
, 2019 ) . 
The direct embedding ( DE ) , directly embeds the entire content of a document using LASER . 
The second method sentence averaging ( SA ) embeds all sentences in a document using LASER and averages all embeddings to get a document representation . 
Cosine similarity on the embedded representation is used to compare documents . 
SMD Weightings We evaluate four weighting schemes for SMD : ( 1 ) vanilla SMD with each sentence equally weighted(2 ) weighting by sentence length ( SL ) where SMD is computed under a scheme where each sentence is weighted by its length ( number of tokens ) normalized by the length of the entire document ( 3 ) weighting by inverse document frequence ( IDF ) where SMD is computed under a scheme where each sentence is weighted by the idf of the sentence ( 4 ) computing SMD under a scheme where each sentence is weighted by both sentence length and inverse document frequency ( SLIDF ) . 
Under all these schemes , all weights are normalized to unit measure . 
Distance approximation We use the greedy mover ‚Äôs distance approximation for all variants reported . 
In Section 7 we further explore the performance of the full distance computation and relaxed variants that were described in Section 4.3 . 
Evaluation Metric for Document Alignment Because the ground - truth document pairs only reÔ¨Çect a high - precision set of web - document pairsthat are translations or of comparable content , there may be many other valid cross - lingual document pairs within each web - domain that are not included in the ground truth set . 
As such , we evaluate each method ‚Äôs generated document pairs solely on the recall ( i.e. what percentage of the aligned pages in the test set are found ) from the ground truth pairs . 
For each scoring method , we score document pairs from the source and target languages within the same web - domain using the proposed document distance metrics described above . 
For the alignment , we report the performance for each distance metric after applying the competitive matching alignment algorithm as described in Algorithm 2 . 
6.2 Results In Table 1 , we Ô¨Årst notice that constructing document representations by directly embedding ( DE ) the entire content of each document and computing document similarity using cosine similarity of the representation severely under - performs compared to individually embedding sentences and constructing the document representations by averaging the individual sentence representations within the document ( SA ) . 
This is intuitive as LASER embeddings were trained on parallel sentences and embedding much larger documents directly using LASER results in poorer representations than by Ô¨Årst embedding smaller sentences and combining them into the Ô¨Ånal document representation . 
Comparing the basic SMD to the best performing baseline ( SA ) , we see a 4 % , 12 % , and 20 % improvement across high , mid , and low - resource directions respectively . 
This improvement suggests that summing sentence embeddings into a single document representation degrades the quality of the resultant document distances over computing document distances by keeping all sentence representations separate and computing distances between individual sentence pairs and combining these distances into a Ô¨Ånal document distance . 
This is more pronounced in lower - resource over higher - resource pairs which may be due to poorer lower - resource embeddings due to LASER being trained on fewer low - resource sentence pairs . 
As such averaging is more destructive to these representations while SMD avoids this degradation . 
Further analysis veriÔ¨Åed the intuition that different sentences should be allocated different weighting in SMD . 
Assigning mass proportional to the621 Recall Language DE SA SMD SL IDF SLIDF French 0.39 0.84 0.81 0.84 0.83 0.85 Spanish 0.34 0.53 0.59 0.63 0.62 0.64 Russian 0.06 0.64 0.69 0.69 0.70 0.71 German 0.52 0.74 0.78 0.76 0.77 0.77 Italian 0.22 0.47 0.55 0.56 0.56 0.59 Portuguese 0.17 0.36 0.39 0.41 0.38 0.40 Dutch 0.28 0.49 0.54 0.54 0.54 0.56 Indonesian 0.11 0.47 0.49 0.52 0.51 0.53 Polish 0.17 0.38 0.45 0.45 0.46 0.46 Turkish 0.12 0.38 0.52 0.56 0.57 0.59 Swedish 0.19 0.40 0.44 0.44 0.46 0.45 Danish 0.27 0.62 0.63 0.69 0.65 0.69 Czech 0.15 0.40 0.43 0.44 0.44 0.43 Bulgarian 0.07 0.43 0.52 0.54 0.55 0.52 Finnish 0.06 0.47 0.51 0.51 0.54 0.52 Norwegian 0.13 0.33 0.37 0.39 0.42 0.41 A VG 0.20 0.50 0.54 0.56 0.56 0.57 ( a ) High - resource directions . 
Recall Language DE SA SMD SL IDF SLIDF Romanian 0.15 0.40 0.44 0.43 0.45 0.43 Vietnamese 0.06 0.28 0.29 0.29 0.29 0.32 Ukrainian 0.05 0.68 0.67 0.78 0.78 0.82 Greek 0.05 0.31 0.47 0.48 0.49 0.49 Korean 0.06 0.34 0.60 0.54 0.61 0.60 Arabic 0.04 0.32 0.63 0.59 0.65 0.61 Croatian 0.16 0.37 0.40 0.40 0.41 0.40 Slovak 0.20 0.41 0.46 0.46 0.46 0.44 Thai 0.02 0.19 0.41 0.33 0.47 0.41 Hebrew 0.05 0.18 0.39 0.43 0.41 0.41 Hindi 0.04 0.27 0.34 0.54 0.52 0.53 Hungarian 0.15 0.49 0.50 0.54 0.51 0.54 Lithuanian 0.11 0.73 0.79 0.79 0.80 0.80 Slovenian 0.13 0.33 0.34 0.35 0.36 0.36 Persian 0.06 0.32 0.56 0.57 0.53 0.59 A VG 0.09 0.37 0.49 0.50 0.52 0.52 ( b ) Mid - resource directions . 
Recall Language DE SA SMD SL IDF SLIDF Estonian 0.28 0.52 0.69 0.66 0.74 0.72 Bengali 0.05 0.32 0.78 0.72 0.77 0.79 Albanian 0.23 0.56 0.66 0.65 0.65 0.66 Macedonian 0.02 0.33 0.32 0.36 0.38 0.33 Urdu 0.06 0.22 0.60 0.60 0.49 0.56 Serbian 0.06 0.59 0.75 0.74 0.74 0.71 Azerbaijani 0.08 0.34 0.74 0.74 0.75 0.74 Armenian 0.02 0.18 0.32 0.35 0.34 0.38 Belarusian 0.07 0.47 0.67 0.69 0.73 0.71 Georgian 0.06 0.24 0.46 0.48 0.45 0.45 Tamil 0.02 0.20 0.51 0.45 0.51 0.53 Marathi 0.02 0.11 0.43 0.46 0.33 0.39 Kazakh 0.05 0.31 0.44 0.46 0.45 0.45 Mongolian 0.03 0.13 0.18 0.22 0.21 0.23 Burmese 0.01 0.10 0.26 0.33 0.46 0.46 Bosnian 0.18 0.64 0.61 0.69 0.65 0.72 A VG 0.08 0.33 0.53 0.54 0.54 0.55 ( c ) Low - resource directions . 
Table 1 : Alignment recall on URL - aligned CommonCrawl dataset . 
number of tokens in the sentence ( SL ) , we see a 2 % , 1 % and 1 % absolute improvement in recall in high , mid , and low - resource directions over assigning equal probability mass . 
This supports the claim that longer sentences should be allocated higher importance weight over shorter sentences as they contain more semantic content . 
The second assumption we investigated is that sentences that are common within a webdomain have less semantic importance and should be allocated less probability mass when computing SMD . 
After computing SMD with each sentence allocated mass according to inverse document frequency ( IDF ) and normalized to unit measure , we see a 2 % , 3 % , and 1 % improvement over SMD for high , mid , and lowresource directions . 
Finally , when combining both sentence length and inverse document frequency ( SLIDF ) and normalizing to unit measure , we see a 3 % , 3 % and 2 % absolute improvement in recall for high , mid , and low - resource directions . 
Overall , our SMD with SLIDF weighting scheme outperforms the sentence averaging baseline by 7 % on high - resource directions , 15 % on mid - resource directions , and 22 % on low - resource directions . 
7 Discussion Although using sentences over words as the base semantic unit drastically reduces the overall cost of computing EMD - based metrics , the cubic computation still prohibits its use as a fast distance metric for large - scale alignment efforts . 
As such , in Section 4.3 we described two faster approximations to EMD computation : ( 1 ) a relaxation of constraints resulting in a lower bound and ( 2 ) a greedy algo - rithm for computing assigning transport representing an upper bound . 
We Ô¨Årst analyze and compare the distances from each approximation scheme to the exact SMD computation . 
Method Tau Recall MAE Runtime ( s ) Exact - SMD 1.00 0.69 0.000 0.402 Relaxed - SMD 0.70 0.58 0.084 0.031 Greedy - SMD 0.98 0.69 0.010 0.107 Table 2 : Comparing exact SMD computation to approximation schemes for computing SMD on 10 webdomains . 
In Figure 3 , we see that the distance computations for exact SMD and the greedy SMD approximation are highly correlated with small variance , while the relaxed approximation is less so with high variance . 
Additionally , as discussed in Section 4.3 , the visualizations empirically suggest that our greedy approximation is a fairly tight upper bound while the relaxed approximation is a looser lower bound . 
In Table 2 , we compare quantitative metrics for the relaxed and greedy approximations to the exact solution of SMD on ten webdomains . 
Our Ô¨Årst evaluation investigates how the approximate computation of distances affects the resultant ordering of document pairs . 
For the ten selected webdomains , we sort the document pairs in order by their computed distances and compare the ordering to the ordering induced by the exact computation of SMD . 
We evaluate the orderings using the Kendall - Tau metric ( Kendall , 1938 ) which measures the agreement between the two rankings ; if the agreement between the two rankings is perfect ( i.e. , the two622 French German Russian Danish Spanish Italian Dutch Turkish Bulgarian Indonesian Finnish Swedish Polish Czech Portugese Norwegian0.00.20.40.60.8RecallHigh - Resource Relaxed vs Greedy Mover 's Distance Greedy Mover 's Distance Relaxed Mover 's Distance(a ) High - resource directions . 
Lithuanian Ukrainian Persian Arabic Hindi Hungarian Korean Slovak Romanian Croatian Hebrew Greek Slovenian Thai Vietnamese0.00.20.40.60.8RecallMid - Resource Relaxed vs Greedy Mover 's Distance Greedy Mover 's Distance Relaxed Mover 's Distance ( b ) Mid - resource directions . 
Serbian Bengali Belarusian Bosnian Estonian Albanian Urdu Azerbaijani Kazakh Tamil Georgian Burmese Marathi Macedonian Armenian Mongolian0.00.20.40.6RecallLow Resource Relaxed vs Greedy Mover 's Distance Greedy Mover 's Distance Relaxed Mover 's Distance ( c ) Low - resource directions . 
Figure 2 : Document alignment results for different distance approximation techniques . 
0 100 200 300 400 500 Document / uni00A0Pairs0.20.40.60.81.0Distances Distance / uni00A0Computations Exact / uni00ADXLSMD Relaxed / uni00ADXLSMD Greedy / uni00ADXLSMD Figure 3 : Exact , relaxed , and greedy - SMD distances sorted by Exact - SMD for a random selection of document pairs . 
rankings are the same ) the coefÔ¨Åcient has value 1 and if the disagreement between the two rankings is perfect ( i.e. , one ranking is the reverse of the other ) the coefÔ¨Åcient has value -1 . 
Intuitively , we would like the distances computed by an approximation to induce a similar ordering to the ordering by the exact distance computation . 
Comparing the KendallTau for the relaxed and greedy approximations in relation to the exact computation shows that the order induced by the greedy approximation is very similar to the ordering induced by the exact computation while the relaxed approximation varies considerably . 
Additionally , the relaxed approximation demonstrates fairly high mean absolute error ( MAE ) and results in lower document alignment recall when compared to the exact computation of SMD , while our greedy approximation performs comparably and shows insigniÔ¨Åcant MAE . 
Finally , while the runtime of the relaxed computation is the fastest at 13 times faster than the exact computation , our greedy algorithm is approximately 4 times faster while delivering comparable document alignment performance to the exact computation and superior performance to the relaxed computation . 
To ensure that the greedy algorithm consistently outperforms the relaxed algorithm on document alignment , we investigate the effect of using eachapproximation method on the downstream document alignment performance across 47 language pairs of varying resource availability . 
Approximation Low Mid High All Relaxed - SMD 0.44 0.43 0.50 0.46 Greedy - SMD 0.54 0.50 0.56 0.54 Table 3 : Document alignment performance of fast methods for approximating the same variant of SMD . 
As seen in Figure 2 , in 45 of the 47 evaluated language pairs , our proposed Greedy Mover ‚Äôs Distance approximation yielded higher downstream recall in our alignment task over using the relaxed distance proposed for use in WMD ( Kusner et al . 
, 2015 ) . 
In Table 3 , we see a 10 % , 7 % , and 6 % improvement in downstream recall across low , mid , and high - resource directions respectively . 
These results indicate that relaxing one of the two constraints in EMD is too lax for measuring an accurate distance . 
We posit this is because there are many sentences that can be considered ‚Äú hubs ‚Äù that are semantically close to many other sentences . 
These sentences can have a lot of probability mass allocated to them , resulting in a lower approximate EMD . 
Our greedy approximation ensures that both constraints are maintained even if the Ô¨Ånal result does not reÔ¨Çect the optimal transport . 
8 Conclusion In this paper , we introduce SMD a cross - lingual sentence mover ‚Äôs distance metric for automatically assessing the semantic similarity of two documents in different languages . 
We leverage state - of - the - art multilingual sentence embeddings and apply SMD to the task of cross - lingual document alignment . 
We demonstrate that our new metric outperforms other unsupervised metrics by a margin , especially in medium and low - resourced conditions.623 Abstract Topic segmentation is critical in key NLP tasks and recent works favor highly effective neural supervised approaches . 
However , current neural solutions are arguably limited in how they model context . 
In this paper , we enhance a segmenter based on a hierarchical attention BiLSTM network to better model context , by adding a coherence - related auxiliary task and restricted self - attention . 
Our optimized segmenter1outperforms SOTA approaches when trained and tested on three datasets . 
We also the robustness of our proposed model in domain transfer setting by training a model on a large - scale dataset and testing it on four challenging real - world benchmarks . 
Furthermore , we apply our proposed strategy to two other languages ( German and Chinese ) , and show its effectiveness in multilingual scenarios . 
1 Introduction Topic segmentation is a fundamental NLP task that has received considerable attention in recent years ( Barrow et al . 
, 2020 ; Glavas and Somasundaran , 2020 ; Lukasik et al . 
, 2020 ) . 
It can reveal important aspects of a document semantic structure by splitting the document into topical - coherent textual units . 
Taking the Wikipedia article in Table 1 as an example , without the section marks , a reliable topic segmenter should be able to detect the correct boundaries within the text and chunk this article into the topical - coherent units T1,T2and T3 . 
The results of topic segmentation can further beneÔ¨Åt other key downstream NLP tasks such as document summarization ( Mitra et al . 
, 1997 ; Riedl and Biemann , 2012a ; Xiao and Carenini , 2019 ) , question answering ( Oh et al . 
, 2007 ; Diefenbach et al . 
, 2018 ) , machine reading ( van Dijk , 1981 ; 1Our code will be publicly available at www.cs . 
ubc.ca/cs-research/lci/research-groups/ natural - language - processing / Preface : Marcus is a city in Cherokee County , Iowa , United States . 
[ T1 ] History : S1 : The Ô¨Årst building in Marcus was erected in 1871 . 
S2 : Marcus was incorporated on May 15 , 1882 . 
[ T2 ] Geography : S3 : Marcus is located at ( 42.822892 , -95.804894 ) . 
S4 : According to the United States Census Bureau , the city has a total area of 1.54 square miles , all land . 
[ T3 ] Demographics : S5 : As of the census of 2010 , there were 1,117 people , 494 households , and 310 families residing in the city . 
... ... Table 1 : A Wikipedia sample article about City Marcus covering three topics : T1,T2andT3 Saha et al . 
, 2019 ) and dialogue modeling ( Xu et al . 
, 2020 ; Zhang et al . 
, 2020 ) . 
A wide variety of techniques have been proposed for topic segmentation . 
Early unsupervised models exploit word statistic overlaps ( Hearst , 1997 ; Galley et al . 
, 2003 ) , Bayesian contexts ( Eisenstein and Barzilay , 2008 ) or semantic relatedness graphs ( Glava Àás et al . 
, 2016 ) to measure the lexical or semantic cohesion between the sentences or paragraphs and infer the segment boundaries from them . 
More recently , several works have framed topic segmentation as neural supervised learning , because of the remarkable success achieved by such models in most NLP tasks ( Wang et al . 
, 2016 , 2017 ; Sehikh et al . 
, 2017 ; Koshorek et al . 
, 2018 ; Arnold et al . 
, 2019 ) . 
Despite minor architectural differences , most of these neural solutions adopt Recurrent Neural Network ( Schuster and Paliwal , 1997 ) and its variants ( RNNs ) as their main framework . 
On the one hand , RNNs are appropriate because topic segmentation can be modelled as a sequence labeling task where each sentence is either the end of a segment or not . 
On the other hand , this choice makes these neural models limited in how to model the context . 
Because some sophisticated RNNs ( eg . 
,626 LSTM , GRU ) are able to preserve long - distance information ( Lipton et al . 
, 2015 ; Sehikh et al . 
, 2017 ; Wang et al . 
, 2018 ) , which can largely help language models . 
But for topic segmentation , it is critical to supervise the model to focus more on the local context . 
As illustrated in Table 1 , the prediction of the segment boundary between T1andT2hardly depends on the content in T3 . 
Bringing in excessive long - distance signals may cause unnecessary noise and hurt performance . 
Moreover , text coherence has strong relation with topic segmentation ( Wang et al . 
, 2017 ; Glavas and Somasundaran , 2020 ) . 
For instance , in Table 1 , sentence pairs from the same segment ( like < S1,S2 > or < S3,S4 > ) are more coherent than sentence pairs across segments ( like S2andS3 ) . 
Arguably , with a proper way of modeling the coherence between adjacent sentences , a topic segmenter can be further enhanced . 
In this paper , we propose to enhance a state - ofthe - art ( SOTA ) topic segmenter ( Koshorek et al . 
, 2018 ) based on hierarchical attention BiLSTM network to better model the local context of a sentence in two complementary ways . 
First , we add a coherence - related auxiliary task to make our model learn more informative hidden states for all the sentences in a document . 
More speciÔ¨Åcally , we reÔ¨Åne the objective of our model to encourage smaller coherence for the sentences from different segments and larger coherence for the sentences from the same segment . 
Secondly , we enhance context modeling by utilizing restricted self - attention ( Wang et al . 
, 2018 ) , which enables our model to pay attention to the local context and make better use of the information from the closer neighbors of each sentence ( i.e. , with respect to a window of explicitly Ô¨Åxed size k ) . 
Our empirical results show ( 1 ) that our proposed context modeling strategy signiÔ¨Åcantly improves the performance of the SOTA neural segmenter on three datasets , ( 2 ) that the enhanced segmenter is more robust in domain transfer setting when applied to four challenging real - world test sets , sampled differently from the training data , ( 3 ) that our context modeling strategy is also effective for the segmenters trained on other challenging languages ( eg . 
, German and Chinese ) , rather than just English . 
2 Related Work Topic Segmentation Early unsupervised models exploit the lexical overlaps of sentences tomeasure the lexical cohesion between sentences or paragraphs ( Hearst , 1997 ; Galley et al . 
, 2003 ; Eisenstein and Barzilay , 2008 ; Riedl and Biemann , 2012b ) . 
Then , by moving two sliding windows over the text , the cohesion between successive text units could be measured and a cohesion drop would signal a segment boundary . 
Even if these models do not require any training data , they only show limited performance in practice and are not general enough to handle the temporal change of the languages ( Huang and Paul , 2019 ) . 
More recently , neural - based supervised methods have been devised for topic segmentation because of their more accurate predictions and greater efÔ¨Åciency . 
One line of research frames topic segmentation as a sequence labeling problem and builds neural models to predict segment boundaries directly . 
Wang et al . 
( 2016 ) proposed a simple BiLSTM model to label if a sentence is a segment boundary or not . 
They demonstrated that along with engineered features based on cue phrases ( eg . 
, ‚Äò Ô¨Årst of all ‚Äô , ‚Äò second ‚Äô ) , their model can achieve marginally better performance than early unsupervised methods . 
Later , Koshorek et al . 
( 2018 ) proposed a hierarchical neural sequence labeling model for topic segmentation and showed its superiority compared with their selected supervised and unsupervised baselines . 
Around the same time , Badjatiya et al . 
( 2018 ) proposed an attention - based BiLSTM model to classify whether a sentence was a segment boundary or not , by considering the context around it . 
The work we present in this paper can be seen as pushing this line of research even further by encouraging the model to more explicitly consider contextual coherence , as well as to prefer more information from the neighbor context through restricted self - attention . 
Another rather different line of works Ô¨Årst trains neural models for other tasks , and then uses these models ‚Äô outputs to predict boundaries . 
Wang et al . 
( 2017 ) trained a Convolutional Neural Network ( CNN ) network to predict the coherence scores for text pairs . 
Sentences in a pair with large cohesion are supposed to belong to the same segment . 
However , their ‚Äú learning to rank ‚Äù framework asks for the pre - deÔ¨Åned number of segments , which limits their model ‚Äôs applicability in practice . 
Our selected framework overcomes this constraint by tuning a conÔ¨Ådence threshold during the training stage . 
A sentence with the output probability above this threshold will be predicted as the end of a seg-627 ment . 
Following a very different approach , Arnold et al . 
( 2019 ) introduced a topic embedding layer into a BiLSTM model . 
After training their model to predict the sentence topics , the learned topic embeddings can be utilized for topic segmentation . 
However , one critical Ô¨Çaw of their method is that it requires a complicated pre - processing pipeline , which includes topic extraction and synset clustering , whose errors can propagate to the main topic segmentation task . 
In contrast , our proposal only requires the plain content of the training data without any complex pre - processing . 
Coherence Modeling Early works on coherence modeling merely predict the coherence score for documents by tracking the patterns of entities ‚Äô grammatical role transition ( Barzilay and Lapata , 2005 , 2008 ) . 
More recently , researchers started modeling the coherence for sentence pairs by their semantic similarities and used them for higher level coherence prediction or even other tasks , including topic segmentation . 
Wang et al . 
( 2017 ) demonstrated the strong relation between text - pair coherence modeling and topic segmentation . 
They assumed that ( 1 ) a pair of texts from the same document should be ranked more coherent than a pair of texts from different documents ; ( 2 ) a pair of texts from the same segment should be ranked more coherent than a pair of texts from different segments of a document . 
With these assumptions , they created a ‚Äú quasi ‚Äù training corpus for text - pair coherence prediction by assigning different coherence scores to the texts from the same segment , different segments but the same document , and different documents . 
Then they proposed the corresponding model , and further use this model to directly conduct topic segmentation . 
Following their second assumption , we propose a neural solution in which by injecting a coherence - related auxiliary task , topic segmentation and sentence level coherence modeling can mutually beneÔ¨Åt each other . 
3 Neural Topic Segmentation Model Since RNN - based topic segmenters have shown success with high - quality training data , we adopt a state - of - the - art RNN - based topic segmenter enhanced with attention and BERT embeddings as our basic model . 
Then , we extend such model to make better use of the local context , something that can not be done effectively within the RNN framework ( Wang et al . 
, 2018 ) . 
In particular , we add a coherence - related auxiliary task and a restricted Figure 1 : The architecture of our basic model . 
seiis the produced sentence embedding for sentence Si . 
self - attention mechanisms to the basic model , so that predictions are more strongly inÔ¨Çuenced by the coherence between the nearby sentences . 
As a preview of this section , we Ô¨Årst deÔ¨Åne the problem of topic segmentation and introduce the basic model . 
In the next section , we motivate and describe our proposed extensions . 
3.1 Problem DeÔ¨Ånition Topic segmentation is usually framed as a sequence labeling task . 
More precisely , given a document represented as a sequence of sentences , our model will predict the binary label for each sentence to indicate if the sentence is the end of a topical coherent segment or not . 
Formally , Given : A document din the form of a sequence of sentences{s1,s2,s3, ... ,s k } . 
Predict : A sequence of labels assigned to a sequence of sentences { l1,l2,l3, ... ,l k‚àí1 } , wherelis a binary label , 1means the corresponding sentence is the end of a segment , 0means the corresponding sentence is not the end of a segment . 
We do not predict the label for the last sentence sk , since it is always the end of the last segment . 
3.2 Basic Model : Enhanced Hierarchical Attention Bi - LSTM Network ( HAN ) Figure 1 illustrates the detailed architecture of our basic model comprising the two steps of sentence encoding and label prediction . 
Formally , a sentence encoding network returns sentence embeddings from pre - trained word embeddings . 
Then a label prediction network processes the sentence embeddings generated earlier and outputs the probabilities to indicate if sentences are the segment628 boundaries or not . 
Finally , to convert the numerical probabilities into binary labels , we follow the greedy decoding strategy in Koshorek et al . 
( 2018 ) by setting a threshold œÑ . 
All the sentences with their probabilities over œÑwill be labeled 1 , and 0 otherwise . 
This parameter œÑis set in the validation stage . 
For training , we compute the cross - entropy loss between the ground truth labels Y= { y1, ... ,y k‚àí1}and our predicted probabilities P= { p1, ... ,p k‚àí1}for a document with ksentences : L1=‚àík‚àí1 / summationdisplay i=1[yilogpi+ ( 1‚àíyi ) log(1‚àípi)](1 ) Looking at the details of the architecture in Figure 1 , our basic model constitutes a strong baseline by extending the segmenter presented in Koshorek et al . 
( 2018 ) in two ways ( colored parts ) ; namely , by improving the sentence encoder with an attention mechanism ( orange ) and with BERT embeddings ( blue ) . 
Enhancing Task - SpeciÔ¨Åc Sentence Representations - While Koshorek et al . 
( 2018 ) applied maxpooling to build sentence embeddings from sentence encoding network , we applied an attention mechanism ( Yang et al . 
, 2016 ) to make the model better capture task - wise sentence semantics . 
The beneÔ¨Åt of this enhancement is veriÔ¨Åed empirically by the results in Table 2 . 
As it can be seen , replacing the max - pooling with the attention based BiLSTM sentence encoder yields better performance . 
Enhancing Generality with BERT Embeddings In order to better deal with unseen text in test data and hence improve the model ‚Äôs generality , we utilize a pre - trained BERT sentence encoder2 which complements our sentence encoding network . 
The transformer - based BERT model ( Devlin et al . 
, 2019 ) was trained on multi - billion sentences publicly available on the web for several generic sentence - level semantic tasks , such as Natural Language Inference and Question Answering , which implies that it can arguably capture more general aspects of sentence semantics in a reliable way . 
To combine task - speciÔ¨Åc information with generic semantic signals from BERT , we simply concatenate the BERT sentence embeddings with the sentence embeddings derived from our encoder . 
Such concatenation then becomes the input of the next level 2github.com/hanxiao/bert-as-service . 
For languages other than English , we use their corresponding pretrained BERT models . 
Dataset CHOI RULES SECTION MEAN MaxPooling 1.04 7.74 12.62 7.14 BiLSTM 0.92 7.47 11.60 6.66 BERT 0.93 8.35 12.08 7.12 BiLSTM+BERT 0.81 6.90 11.30 6.34 Table 2 : Pkerror score ( lower the better , see Section 4.3 for details ) of different sentence encoding strategies on three datasets ( Section 4.1 ) . 
To Ô¨Åt in the table , we shorten Att - BiLSTM to BiLSTM . 
Results in bold are the best performance across the comparisons . 
network ( see Figure 1 ) . 
The beneÔ¨Åt of injecting BERT embedding is also veriÔ¨Åed empirically by the results reported in Table 2 . 
We can see that concatenating BERT embedding and the output of Att - BiLSTM yields the best performance compared with only BERT embedding or the output of Att - BiLSTM . 
3.3 Auxiliary Task Learning In a well - structured document , the semantic coherence of a pair of sentences from the same segment should tend to be greater than the coherence of a pair of sentences from different segments . 
This observation provides us with an alternative way to enable better context modeling by formulating a coherence - related auxiliary task whose objective can be jointly optimized with our original objective ( Equation 1 ) . 
This task thereby is to predict the consecutive sentence - pair coherence by using the sentence hidden states generated from the BiLSTM network . 
Concurrently minimizing the loss of this task can regulate our model to learn better semantic coherence relation between sentences by reducing the semantic coherence scores for the sentence pairs across segments and increasing the semantic coherence scores for the sentence pairs within a segment . 
To obtain the ground truth for our introduced auxiliary task ( sentence - pair coherence prediction ) , we leverage the ground truth of our segmented training set rather than requiring external annotations . 
For a document which contains msentences , there arem‚àí1consecutive sentence pairs . 
If this document hasnsegment boundaries , then among those m‚àí1sentence pairs , nsentence pairs are from different segments , while the remaining m‚àín‚àí1 sentence pairs are from the same segment . 
In order to concurrently minimize the coherence of the sentences from different segments and maximize the coherence of the sentences in the same segment , we give a sentence pair < si , si+1 > a coherence label629 Figure 2 : Our full model with context modeling components : restricted self - attention , auxiliary task module . 
li= 1if sentences in this pair are from the same segment , and li= 0otherwise . 
The embeddings ei andei+1of adjacent sentences pairs < si , si+1 > used for coherence computing are calculated from BiLSTM forward and backward hidden states‚àí ‚Üíh and‚Üê ‚àíh , following the equations below : ei= tanh(We(‚àí ‚Üíhi‚àí‚àí‚àí‚Üíhi‚àí1 ) + be ) ( 2 ) ei+1= tanh(We(‚Üê‚àí‚àíhi+1‚àí‚Üê‚àí‚àíhi+2 ) + be)(3 ) However , notice that instead of using the conventional [ ‚àí ‚Üíhi;‚Üê ‚àíhi]as the embedding of sentence i , here , similarly to Wang and Chang ( 2016 ) , we subtract forward / backward states to focus on the semantics of sentences in the current sentence pair . 
The semantic coherence between two sentence embeddings is then computed as the sigmoid of their cosine similarity : Coh i = œÉ(cos(ei , ei+1 ) ) ( 4 ) We use binary cross - entropy loss to formulate the objective of our auxiliary task . 
For a document withksentences , the loss can be calculated as : L2=‚àík‚àí1 / summationdisplay i=1,li=1logCoh i‚àík‚àí1 / summationdisplay i=1,li=0log(1‚àíCoh i ) ( 5 ) which penalizes high Coh across segments and low Coh within segments . 
Combining Equation 1 and 5 , we form the loss function of our new segmenter as : Ltotal = Œ±L1 + ( 1‚àíŒ±)L2 ( 6)with the trade - off parameter Œ±tuned in validation stage , topic segmentation and the coherence - related auxiliary task are jointly optimized . 
The architecture of the auxiliary task module and its integration in our segmenter is shown in red in Figure 2 . 
3.4 Sentence - Level Restricted Self - Attention The self - attention mechanism ( Vaswani et al . 
, 2017 ) has been widely applied to many sequence labeling tasks due to its superiority in modeling long - distance dependencies in text . 
However , when the task mainly requires modelling local context , long - distance dependencies will instead introduce noise . 
Wang et al . 
( 2018 ) noticed this problem for discourse segmentation , where the crucial information for a clause - like Elementary Discourse Unit ( EDU ) boundary prediction comes usually only from the adjacent EDUs . 
Thus , they proposed aword - level restricted self - attention mechanism by adding a Ô¨Åxed size window constraint on the standard self - attention . 
In essence , this mechanism encourages the model to absorb more information directly from adjacent context words within a Ô¨Åxed range of neighborhood . 
We hypothesize that the similar restricted dependencies also play a dominant role in topic segmentation due to their close relation . 
Hence , instead of at word - level , we add asentence - level restricted self - attention on top of the label prediction network of the basic model , as shown in green in Figure 2 . 
In particular , once hidden states are obtained for all the sentences of document d , we compute the630 Dataset CHOI RULES SECTION WIKI-50 CITIES ELEMENTS CLINICAL documents 920 4,461 21,376 50 100 118 227 # sent / seg 7.4 7.4 7.2 13.6 5.2 3.3 28.0 # seg / doc 10.0 16.6 7.9 3.5 12.2 6.8 5.0 real world Table 3 : Statistics of all the English topic segmentation datasets used in our experiments . 
Dataset EN DE ZH documents 21,376 12,993 10,000 # sent / seg 7.2 6.3 5.1 # seg / doc 7.9 7.0 6.4 real world Table 4 : Statistics of the the WIKI - SECTION data in English(EN ) , German(DE ) and Chinese(ZH ) . 
similarities between the current sentence iand its nearby sentences within a window of size S. For example , the similarity between sentence siandsj which is within the window size is computed as : simi , j = Wa[hi;hj ; ( hi‚äôhj ) ] + ba ( 7 ) wherehi , hjare the hidden state of siandsj . 
Wa andbaare both attention parameters . 
; is the concatenation operation and ‚äôis the dot product operation . 
The attention weights for all the sentences in the Ô¨Åxed window are : ai , j = esimi , j /summationtextS s=‚àíSesimi , i+s(8 ) The output for sentence iafter the restricted selfattention mechanism is the weighted sum of all the sentence hidden states within the window : ci = S / summationdisplay s=‚àíSai , i+shi+s ( 9 ) wherecidenotes the local context embedding of sentenceigenerated by restricted self - attention . 
After getting the local context embeddings for all the sentences , we concatenate them with the original sentence hidden states and input them to another BiLSTM layer ( top of Figure 2 ) . 
4 Experimental Setup In order to comprehensively evaluate the effectiveness of our context modeling strategy of adding a coherence - related auxiliary task and a restricted self - attention mechanisms to the basic model , we conduct three sets of experiments for evaluation:(i ) Intra Domain : we train and test the models in the same domain , repeating this evaluation for three different domains ( datasets ) . 
( ii ) Domain Transfer : we train the models on a large dataset which covers a variety of topics and test them on four challenging real - world datasets . 
( iii ) Multilingual : we train and test our model on three datasets within different languages ( English , German and Chinese ) , to assess our proposed strategy ‚Äôs generality within different languages . 
4.1 Datasets Data for Intra - Domain Evaluation High quality training dataset for topic segmentation usually satisÔ¨Åes the following criteria : ( 1 ) large size ; ( 2 ) cover a variety of topics ; ( 3 ) contains real documents with reliable segmentation either from human annotations or already speciÔ¨Åed in the documents e.g. , sections . 
In order to comprehensively evaluate the effectiveness of our context modeling strategy when dealing with data of different quality , we train and test models on the following three datasets : CHOI ( Choi , 2000 ) whose articles are synthesized artiÔ¨Åcially by stitching together different sources ( i.e. , they were not written as one document by one author ) . 
Hence , it does not really reÔ¨Çect naturally occurring topic drifts . 
While the quality of this dataset is low , it is an early but popular benchmark for topic segmentation evaluation . 
We include this dataset to allow comparison with the previous work . 
RULES ( Bertrand et al . 
, 2018 ) is a dataset collected from the U.S. Federal Register issues3 . 
When U.S. federal agencies make changes to regulations or other policies , they must publish a document called a ‚Äú Rule ‚Äù in the Federal Register . 
The Rule describes what is being changed and discusses the motivation and legal justiÔ¨Åcation for the action . 
Since each paragraph in a document discusses one topic , we consider the last sentence of each paragraph as a ground truth topic boundary . 
The discussion paragraphs usually cover diverse topics in formal , 3www.govinfo.gov/631 technical language that can be hard to Ô¨Ånd online , so we deem it as an additional well - labelled dataset for testing topic segmentation to complement our other datasets which contain more informal use of the language . 
WIKI - SECTION ( Arnold et al . 
, 2019 ) is a newly released dataset which was originally generated from the most recent English and German Wikipedia dumps . 
To better align with the purpose of intra - domain experiment , we only select the English samples for training and the German samples will be used in the experiments of multilingual evaluation . 
The English WIKI - SECTION ( labeled SECTION in the tables ) consists of Wikipedia articles from domain diseases andcities . 
We deem this dataset as the most reliable training source among the three datasets . 
It has the largest size and the two domains ( cities anddiseases ) cover news - based samples and scientiÔ¨Åc - based samples respectively . 
We split CHOI andRULES into 80 % training , 10 % validation , 10 % testing . 
For SECTION , we follow Arnold et al . 
( 2019 ) and split it into 70 % training , 10 % validation , 20 % testing . 
Table 3 ( left ) contains the statistical details for these three sets . 
Data for Domain Transfer Evaluation We pick WIKI - SECTION as our training set in this line of experiments , due to its largest size and variety of covered topics . 
Following previous work , we evaluate our model and baselines on four datasets that originate from different source distributions : WIKI-50 ( Koshorek et al . 
, 2018 ) which consists of 50 samples randomly generated from the latest English Wikipedia dump , with no overlap with training and validation data . 
Cities ( Chen et al . 
, 2009 ) which consists of 100 samples generated from Wikipedia about cities . 
We also ensure that this dataset has no overlap with training and validation data . 
Elements ( Chen et al . 
, 2009 ) which consists of 118 samples generated from Wikipedia about chemical elements . 
Clinical Books ( Malioutov and Barzilay , 2006 ) which consists of 227 chapters from a medical textbook . 
Table 3 ( right ) gives more detailed statistics for these datasets . 
Data For Multilingual Evaluation In order to test the effectiveness of our context modeling strategy across languages , besides the English WIKISECTION , we train and test our model on two other Wikipedia datasets in German and Chinese : SECTION - DE which was released together with English WIKI - SECTION in Arnold et al . 
( 2019 ) . 
ItDataset CHOI RULES SECTION MEAN Random 49.4 50.6 51.3 50.4 BayesSeg 20.8 41.5 39.5 33.9 GraphSeg 6.6 39.3 44.9 30.3 TextSeg 1.0 7.7 12.6 7.1 Sector - - 12.7 Transformer 4.8 9.6 13.6 9.3 Basic Model 0.81 7.0 11.3 6.4 + AUX 0.64‚Ä†6.1‚Ä†10.4‚Ä†5.7 + RSA 0.72‚Ä†6.3‚Ä†10.0‚Ä†5.7 + AUX+RSA 0.54‚Ä†5.8‚Ä†9.7‚Ä†5.3 Table 5 : Pkerror score on three datasets . 
Results in bold indicate the best performance across all comparisons . 
Underlined results indicate the best performance in the bottom section . 
‚Ä†indicates the result is signiÔ¨Åcantly different ( p<0.05 ) from basic model . 
also contains articles about cities and diseases . 
The section marks are used as the ground truth labels . 
SECTION - ZH which was randomly generated from the Chinese Wikipedia dump4mentioned in Hao and Paul ( 2020 ) . 
As before , section marks are also used here as ground truth boundaries . 
The statistical details of these two datasets can be found in Table 4 . 
4.2 Baselines These include two popular unsupervised topic segmentation methods , BayesSeg ( Eisenstein and Barzilay , 2008 ) and GraphSeg ( Glava Àás et al . 
, 2016 ) , as well as the three recently proposed supervised neural models , TextSeg ( Koshorek et al . 
, 2018 ) ( from which we derive our basic model ) , Sector ( Arnold et al . 
, 2019 ) and Hierarchical Transformer ( labeled Transformer in the tables ) ( Glavas and Somasundaran , 2020 ) . 
We use the original implementation of BayesSeg , GraphSeg andTextSeg . 
We reimplement the Hierarchical Transformer ourselves . 
In Table 6 , we adopt the results of BayesSeg , GraphSeg andSector from Arnold et al . 
( 2019)5 . 
4.3 Evaluation Metric We use the standard Pkerror score ( Beeferman et al . 
, 1999 ) as our evaluation metric , since it has become the standard for comparing topic segmenters . 
Pkis calculated as : Pk(ref , hyp ) = /summationtextn‚àík i=0Œ¥ref(i , i+k)/negationslash = Œ¥hyp(i , i+k ) 4https://linguatools.org/tools/ corpora / wikipedia - monolingual - corpora/ 5Arnold et al . 
( 2019 ) reported Sector ‚Äôs performance on multiple model settings . 
Here we pick the performance of the model trained on wikifull to be close to our training setting.632 Dataset Wiki-50 Cities Elements Clinical Random 52.7 47.1 50.1 44.1 BayesSeg 49.2 36.2 35.6 57.2 GraphSeg 63.6 40.0 49.1 64.6 TextSeg 28.5 19.8 43.9 36.6 Sector 28.6 33.4 42.8 36.9 Transformer 29.3 20.2 45.2 35.6 Basic Model 28.7 17.9 43.5 33.8 + AUX 27.9 17.0‚Ä†41.8‚Ä†31.5‚Ä† + RSA 27.8‚Ä†16.8‚Ä†42.7 31.9‚Ä† + AUX+RSA 26.8‚Ä†16.1‚Ä†39.4‚Ä†30.5‚Ä† Table 6 : Pkerror score on four test sets . 
Results in bold indicate the best performance across all comparisons . 
Underlined results indicate the best performance in the bottom section . 
‚Ä†indicates the result is signiÔ¨Åcantly different ( p<0.05 ) from basic model . 
whereŒ¥is an indicator function which is 1 if sentenceiandi+kare in the same segment , 0 otherwise . 
It measures the probability of mismatches between the ground truth segments ( ref ) and model predictions ( hyp ) within a sliding window k. As a standard setting which has been used in previous work , window size kis the average segment length ofref . 
SincePkis a penalty metric , lower score indicates better performance . 
4.4 Neural Model Setup Following Koshorek et al . 
( 2018 ) , our initial word embeddings are GoogleNews word2vec ( d= 300 ) . 
We also use word2vec embeddings ( d= 300 ) and Fasttext embeddings ( d= 300 ) , which are both derived from Wikipedia corpora for German and Chinese respectively . 
We use the Adam optimizer , setting the learning rate to 0.001 and batch size to 8 . 
The BiLSTM hidden state size is 256 following Koshorek et al . 
( 2018 ) . 
Model training is done for 10 epochs and performance is monitored over the validation set . 
We generate BERT sentence embeddings with the pre - trained 12 - layer model released by Google AI ( embedding size 768 ) . 
The window size of restricted self - attention is 3 and Œ± is 0.8 . 
These were tuned on the validation sets of the datasets we use . 
5 Results and Discussion 5.1 Intra - Domain Evaluation Table 5 shows the models ‚Äô performance on the three datasets , when all supervised models are trained and evaluated on the training and test set from the same domain . 
To investigate the effectiveness of auxiliary task ( AUX ) and restricted self - attentionDataset EN DE ZH Random 51.3 48.7 52.2 Basic Model 11.3 18.2 20.5 + AUX 10.4‚Ä†17.7 20.5 + RSA 10.0‚Ä†16.6‚Ä†19.8‚Ä† + AUX+RSA 9.7‚Ä†15.9‚Ä†20.0‚Ä† Table 7 : Pkerror score on the datasets in three languages ( English , German and Chinese ) . 
( RSA ) , Table 5 also shows the results of individually adding each component to our basic segmenter . 
The most important observation from the table is that our model enhanced by context modeling outperforms all the supervised and unsupervised baselines with a substantial performance gain . 
With our context modeling strategy , the average Pkscores of our model over the three datasets improves on the best model ( TextSeg ) among the baselines by 25 % . 
Compared with the basic model , adding AUX or RSA equally gives signiÔ¨Åcant and consistent improvement across all three sets . 
Adding both AUX and RSA results in the biggest improvement by up to 17 % on the mean across the three datasets . 
5.2 Domain Transfer Evaluation Table 6 compares the performance of the baselines and our model on four challenging real - world test datasets . 
All supervised models are trained on the training set of WIKI - SECTION . 
One important observation is that our model enhanced by context modeling outperforms all the baseline methods on three out of four test sets with a substantial performance gap . 
Admittedly , BayesSeg performs better on Elements , possibly because that merely word embedding similarity is sufÔ¨Åcient to indicate segment boundaries in this dataset . 
However , BayesSeg is completely dominated by our model on the other test sets . 
Overall , this indicates that our proposed context modeling strategy can not only enhance the model under the intra - domain setting , but also produce robust models that transfer to other unseen domains . 
Furthermore , we observe that AUX and RSA are both necessary for our model , since they do not only improve performance individually , but they achieve the best results when synergistically combined . 
5.3 Multilingual Evaluation Table 7 shows results for our context modeling strategy across three different languages : English ( EN ) , German ( DE ) and Chinese ( ZH ) . 
Remark-633 ably , even our basic model without any add - on component outperforms the random baseline by a wide margin . 
Looking at the gains from AUX and RSA , for German we observe a pattern similar to English , with our complete context modeling strategy ( AUX+RSA ) delivering the strongest gains . 
However , the performance on Chinese is not as strong as on English and German . 
Employing RSA still achieves a statistically signiÔ¨Åcant 0.7 Pk score drop , but introducing AUX does not help . 
One possible reason is that the sentences in the Chinese Wikipedia pages are relatively short and fragmented . 
Thus , the semantics of these sentences may be too simple to sufÔ¨Åciently guide the coherence auxiliary task . 
In general , when comparing the behavior of our context modeling strategy across these three languages , RSA appears to yield stable beneÔ¨Åts , while the effectiveness of AUX seems to depend more on peculiarities of the dataset in the target language . 
6 Conclusions and Future Work We address a serious limitation of current neural topic segmenters , namely their inability to effectively model context . 
To this end , we propose a novel neural model that adds a coherence - related auxiliary task and restricted self - attention on top of a hierarchical BiLSTM attention segmenter to make better use of the contextual information . 
Experimental results of intra - domain on three datasets show that our strategy is effective within domains . 
Further , results on four challenging real - world benchmarks demonstrate its effectiveness in domain transfer settings . 
Finally , the application to other two languages ( German and Chinese ) suggests that our strategy has its potential in multilingual scenarios . 
As future work , we will investigate whether our proposed context modeling strategy is also effective for segmenting dialogues ( Takanobu et al . 
, 2018 ) rather than just standard articles . 
Secondly , we will explore how to capture even more accurate and informative contextual information by integrating document structures or sentence dependencies obtained from other NLP tasks ( e.g. , discourse parsing ( Huber and Carenini , 2019 , 2020 ) or discourse role labeling ( Zeng et al . 
, 2019 ) ) . 
Acknowledgments We thank the anonymous reviewers and the UBCNLP group for their insightful comments . 
References Sebastian Arnold , Rudolf Schneider , Philippe Cudr ¬¥ eMauroux , Felix A. Gers , and Alexander L ¬®oser . 
2019 . 
Sector : A neural model for coherent topic segmentation and classiÔ¨Åcation . 
Transactions of the Association for Computational Linguistics , 7:169‚Äì184 . 
Pinkesh Badjatiya , Litton J. Kurisinkel , Manish Gupta , and Vasudeva Varma . 
2018 . 
Attention - based neural text segmentation . 
In European Conference on Information Retrieval 2018 , pages 180‚Äì193 . 
Joe Barrow , Rajiv Jain , Vlad Morariu , Varun Manjunatha , Douglas Oard , and Philip Resnik . 
2020 . 
A joint model for document segmentation and segment labeling . 
In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 313‚Äì322 . 
Regina Barzilay and Mirella Lapata . 
2005 . 
Modeling local coherence : An entity - based approach . 
In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ( ACL‚Äô05 ) , pages 141‚Äì148 . 
Regina Barzilay and Mirella Lapata . 
2008 . 
Modeling local coherence : An entity - based approach . 
Computational Linguistics , 34(1):1‚Äì34 . 
Doug Beeferman , Adam Berger , and John Lafferty . 
1999 . 
Statistical models for text segmentation . 
Machine Learning , 34(1):177‚Äì210 . 
Marianne Bertrand , Matilde Bombardini , Raymond Fisman , Bradley Hackinen , and Francesco Trebbi . 
2018 . 
Hall of mirrors : Corporate philanthropy and strategic advocacy . 
Technical report , National Bureau of Economic Research . 
Harr Chen , S.R.K. Branavan , Regina Barzilay , and David R. Karger . 
2009 . 
Global models of document structure using latent permutations . 
In Proceedings of Human Language Technologies : The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics , pages 371‚Äì379 . 
Freddy Y . 
Y . 
Choi . 
2000 . 
Advances in domain independent linear text segmentation . 
In 1st Meeting of the North American Chapter of the Association for Computational Linguistics . 
Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 
2019 . 
BERT : Pre - training of deep bidirectional transformers for language understanding . 
In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171‚Äì4186 . 
Dennis Diefenbach , Vanessa Lopez , Kamal Singh , and Pierre Maret . 
2018 . 
Core techniques of question answering systems over knowledge bases : a survey . 
Knowledge and Information Systems , 55(3):529 ‚Äì 569.634 Teun van Dijk . 
1981 . 
Episodes as units of discourse analysis . 
Analyzing Discourse : Text and Talk . 
Jacob Eisenstein and Regina Barzilay . 
2008 . 
Bayesian unsupervised topic segmentation . 
In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing , pages 334‚Äì343 . 
Michel Galley , Kathleen McKeown , Eric FoslerLussier , and Hongyan Jing . 
2003 . 
Discourse segmentation of multi - party conversation . 
In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1 , pages 562 ‚Äì 569 . 
Goran Glava Àás , Federico Nanni , and Simone Paolo Ponzetto . 
2016 . 
Unsupervised text segmentation using semantic relatedness graphs . 
In Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics , pages 125‚Äì130 . 
Association for Computational Linguistics . 
Goran Glavas and Swapna Somasundaran . 
2020 . 
Twolevel transformer and auxiliary coherence modeling for improved text segmentation . 
In The ThirtyFourth AAAI Conference on ArtiÔ¨Åcial Intelligence ( AAAI-20 ) , pages 2306‚Äì2315 . 
Shudong Hao and Michael J. Paul . 
2020 . 
An empirical study on crosslingual transfer in probabilistic topic models . 
Computational Linguistics , 46(1):95‚Äì134 . 
Marti A. Hearst . 
1997 . 
Text tiling : Segmenting text into multi - paragraph subtopic passages . 
Computational Linguistics , 23(1):33‚Äì64 . 
Xiaolei Huang and Michael J. Paul . 
2019 . 
Neural temporality adaptation for document classiÔ¨Åcation : Diachronic word embeddings and domain adaptation models . 
In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 4113‚Äì4123 . 
Association for Computational Linguistics . 
Patrick Huber and Giuseppe Carenini . 
2019 . 
Predicting discourse structure using distant supervision from sentiment . 
In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ( EMNLPIJCNLP ) , pages 2306‚Äì2316 . 
Patrick Huber and Giuseppe Carenini . 
2020 . 
Mega rst discourse treebanks with structure and nuclearity from scalable distant sentiment supervision . 
In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing . 
Association for Computational Linguistics . 
Omri Koshorek , Adir Cohen , Noam Mor , Michael Rotman , and Jonathan Berant . 
2018 . 
Text segmentation as a supervised learning task . 
In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 2 ( Short Papers ) , pages 469‚Äì473.Zachary C. Lipton , John Berkowitz , and Charles Elkan . 
2015 . 
A critical review of recurrent neural networks for sequence learning . 
CoRR , abs/1506.00019 . 
Michal Lukasik , Boris Dadachev , Gonc ¬∏alo Sim Àúoes , and Kishore Papineni . 
2020 . 
Text segmentation by cross segment attention . 
CoRR , abs/2004.14535 . 
Igor Malioutov and Regina Barzilay . 
2006 . 
Minimum cut model for spoken lecture segmentation . 
In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics , pages 25‚Äì32 . 
Mandar Mitra , Amit Singhal , and Chris Buckley . 
1997 . 
Automatic text summarization by paragraph extraction . 
In Intelligent Scalable Text Summarization . 
HyoJung Oh , Sung Hyon Myaeng , and Myung - Gil Jang . 
2007 . 
Semantic passage segmentation based on sentence topics for question answering . 
Information Sciences , 177(18):3696‚Äì3717 . 
Martin Riedl and Chris Biemann . 
2012a . 
How text segmentation algorithms gain from topic models . 
In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 553‚Äì557 . 
Martin Riedl and Chris Biemann . 
2012b . 
Topictiling : A text segmentation algorithm based on lda . 
In Proceedings of ACL 2012 Student Research Workshop , pages 37‚Äì42 . 
Swarnadeep Saha , Malolan Chetlur , Tejas Indulal Dhamecha , W M Gayathri K Wijayarathna , Red Mendoza , Paul Gagnon , Nabil Zary , and Shantanu Godbole . 
2019 . 
Aligning learning outcomes to learning resources : A lexico - semantic spatial approach . 
In Proceedings of the Twenty - Eighth International Joint Conference on ArtiÔ¨Åcial Intelligence , IJCAI-19 , pages 5168‚Äì5174 . 
Mike Schuster and Kuldip K. Paliwal . 
1997 . 
Bidirectional recurrent neural networks . 
IEEE Transactions on Signal Processing , 45:2673‚Äì2681 . 
Imran Sehikh , Dominique Fohr , and Irina Illina . 
2017 . 
Topic segmentation in asr transcripts using bidirectional rnns for change detection . 
In 2017 IEEE Automatic Speech Recognition and Understanding Workshop ( ASRU ) . 
Ryuichi Takanobu , Minlie Huang , Zhongzhou Zhao , Fenglin Li , Haiqing Chen , Xiaoyan Zhu , and Liqiang Nie . 
2018 . 
A weakly supervised method for topic segmentation and labeling in goal - oriented dialogues via reinforcement learning . 
In Proceedings of the Twenty - Seventh International Joint Conference on ArtiÔ¨Åcial Intelligence , IJCAI-18 , pages 4403‚Äì4410.635 Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , ≈Å ukasz Kaiser , and Illia Polosukhin . 
2017 . 
Attention is all you need . 
In Advances in Neural Information Processing Systems 30 , pages 5998‚Äì6008 . 
Liang Wang , Sujian Li , Yajuan Lv , and Houfeng Wang . 
2017 . 
Learning to rank semantic coherence for topic segmentation . 
In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pages 1340‚Äì1344 . 
Liang Wang , Sujian Li , Xinyan Xiao , and Yajuan Lyu . 
2016 . 
Topic segmentation of web documents with automatic cue phrase identiÔ¨Åcation and blstm - cnn . 
InNatural Language Understanding and Intelligent Applications , pages 177‚Äì188 . 
Wenhui Wang and Baobao Chang . 
2016 . 
Graph - based dependency parsing with bidirectional LSTM . 
In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 2306‚Äì2315 . 
Yizhong Wang , Sujian Li , and Jingfeng Yang . 
2018 . 
Toward fast and accurate neural discourse segmentation . 
In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 962‚Äì967 . 
Wen Xiao and Giuseppe Carenini . 
2019 . 
Extractive summarization of long documents by combining global and local context . 
In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing , pages 3009‚Äì3019 . 
Yi Xu , Hai Zhao , and Zhuosheng Zhang . 
2020 . 
Topic - aware multi - turn dialogue modeling . 
CoRR , abs/2009.12539 . 
Zichao Yang , Diyi Yang , Chris Dyer , Xiaodong He , Alex Smola , and Eduard Hovy . 
2016 . 
Hierarchical attention networks for document classiÔ¨Åcation . 
In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 1480‚Äì1489 . 
Jichuan Zeng , Jing Li , Yulan He , Cuiyun Gao , Michael R. Lyu , and Irwin King . 
2019 . 
What you say and how you say it : Joint modeling of topics and discourse in microblog conversations . 
Transactions of the Association for Computational Linguistics , 7:267‚Äì281 . 
Hainan Zhang , Yanyan Lan , Liang Pang , Hongshen Chen , Zhuoye Ding , and Dawei Yin . 
2020 . 
Modeling topical relevance for multi - turn dialogue generation . 
In Proceedings of the Twenty - Ninth International Joint Conference on ArtiÔ¨Åcial Intelligence , IJCAI-20 , pages 3737‚Äì3743 . 
International Joint Conferences on ArtiÔ¨Åcial Intelligence Organization.636 Proceedings of the 1st Conference of the Asia - PaciÔ¨Åc Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing , pages 637‚Äì642 December 4 - 7 , 2020 . 
¬© 2020 Association for Computational Linguistics Contextualized End - to - End Neural Entity Linking Haotian Chen BlackRock haotian.chen@blackrock.comAndrej Zukov - Gregoric BlackRock andrej.zukovgregoric@blackrock.com Xi ( David ) Li BlackRock david.li@blackrock.comSahil Wadhwa University of Illinois at Urbana - Champaign‚àó sahilw2@illinois.edu Abstract We propose an entity linking ( EL ) model that jointly learns mention detection ( MD ) and entity disambiguation ( ED ) . 
Our model applies task - speciÔ¨Åc heads on top of shared BERT contextualized embeddings . 
We achieve stateof - the - art results across a standard EL dataset using our model ; we also study our model ‚Äôs performance under the setting when handcrafted entity candidate sets are not available and Ô¨Ånd that the model performs well under such a setting also . 
1 Introduction Entity linking ( EL)1 , in our context , refers to the joint task of recognizing named entity mentions in text through mention detection ( MD ) and linking each mention to a unique entity in a knowledge base ( KB ) through entity disambiguation ( ED)2 . 
For example , in the sentence ‚Äú The Times began publication under its current name in 1788 , ‚Äù the span The Times should be detected as a named entity mention and then linked to the corresponding entity : TheTimes , a British newspaper . 
However , an EL model which disjointly applies MD and ED might easily mistake this mention with TheNew York Times , an American newspaper . 
Since our model jointly learns MD and ED from the same contextualized BERT embeddings , its Ô¨Ånal EL prediction is partially informed by both . 
As a result , it is able to generalize better . 
Another common approach employed in previous EL research is candidate generation , where for each detected mention , a set of candidate entities is generated and the entities within it are ranked by a model to Ô¨Ånd the best match . 
Such sets are ‚àóWork done while at BlackRock . 
1Also known as A2 KB task in GERBIL evaluation platform ( R ¬®oder et al . 
, 2018 ) and end - to - end entity linking in some literature 2Also known as D2 KB task in GERBILbuilt using hand - crafted rules which deÔ¨Åne which entities make it in and which do not . 
This risks ( 1 ) skipping out on valid entities which should be in the candidate set and ( 2 ) inÔ¨Çating model performance since often times candidate sets contain only one or two items . 
These sets are almost always used at prediction time and sometimes even during training . 
Our model has the option of not relying on them during prediction , and never uses them during training . 
We introduce two main contributions : ( i)We propose a new end - to - end differentiable neural EL model that jointly performs MD and ED and achieves state - of - the - art performance . 
( ii)We study the performance of our model when candidate sets are removed to see whether EL can perform well without them . 
2 Related Work Neural - network based models have recently achieved strong results across standard EL datasets . 
Research has focused on learning better entity representations and extracting better local and global features through novel model architectures . 
Entity representation . 
Good KB entity representations are a key component of most ED and EL models . 
Representation learning has been addressed by Yamada et al . 
( 2016 ) , Ganea and Hofmann ( 2017 ) , Cao et al . 
( 2017 ) and Yamada et al . 
( 2017 ) . 
Sil et al . 
( 2018 ) and Cao et al . 
( 2018 ) extend it to the cross - lingual setting . 
More recently , Yamada and Shindo ( 2019 ) have suggested learning entity representations using BERT which achieves state - of - the - art results in ED . 
Entity Disambiguation ( ED ) . 
The ED task assumes already - labelled mention spans which are then disambiguated . 
Recent work on ED has focused on extracting global features ( Ratinov et al . 
,637 2011 ; Globerson et al . 
, 2016 ; Ganea and Hofmann , 2017 ; Le and Titov , 2018 ) , extending the scope of ED to more non - standard datasets ( Eshel et al . 
, 2017 ) , and positing the problem in new ways such as building separate classiÔ¨Åers for KB entities ( Barrena et al . 
, 2018 ) . 
Entity Linking ( EL ) . 
Early work by Sil and Yates ( 2013 ) , Luo et al . 
( 2015 ) and Nguyen et al . 
( 2016 ) introduced models that jointly learn NER and ED using engineered features . 
More recently , Kolitsas et al . 
( 2018 ) propose a neural model that Ô¨Årst generates all combinations of spans as potential mentions and then learns similarity scores over their entity candidates . 
MD is handled implicitly by only considering mention spans which have non - empty candidate entity sets . 
Martins et al . 
( 2019 ) propose training a multi - task NER and ED objective using a Stack - LSTM ( Dyer et al . 
, 2015 ) . 
Finally , Poerner et al . 
( 2019 ) and Broscheit ( 2019 ) both propose end - to - end EL models based onBERT . 
Poerner et al . 
( 2019 ) model the similarity between entity embeddings and contextualized word embeddings by mapping the former onto the latter whereas Broscheit ( 2019 ) in essence do the opposite . 
Our work is different in three important ways : our training objective is different in that we explicitly model MD ; we analyze the performance of our model when candidate sets are expanded to include the entire universe of entity embeddings ; and we outperform both models by a wide margin . 
3 Model Description Given a document containing a sequence of ntokensw={w1, ... ,w n}with mention label indicators3ymd={I , O , B}nand entity IDs yed= { j‚ààZ : j‚àà[1,k]}nwhich index a pre - trained entity embedding matrix E‚ààRk√ódof entity universe sizekand entity embedding dimension d , the model is trained to tag each token with its correct mention indicator and link each mention with its correct entity ID . 
3.1 Text Encoder The text input to our model is encoded by BERT ( Devlin et al . 
, 2019 ) . 
We initialize the pre - trained weights from BERT - BASE .4The text input is tokenized by the cased WordPiece ( Johnson et al . 
, 3We use standard inside - outside - beginning ( IOB ) tagging format introduced by ( Ramshaw and Marcus , 1995 ) 4https://github.com/google-research/bert2017 ) sub - word tokenizer . 
The text encoder outputsncontextualized WordPiece embeddings h which are grouped to form the embedding matrix H‚ààRn√óm , wheremis the embedding dimension . 
In the case of BERT - BASE , mis equal to 768 . 
The transformation from word level to WordPiece sub - word level labels is handled similarly to theBERT NER task , where the head WordPiece token represents the entire word , disregarding tail tokens . 
BERT comes in two settings : feature - based and Ô¨Åne - tuned . 
Under the feature - based setting , BERT parameters are not trainable in the domain task ( EL ) , whereas the Ô¨Åne - tuned setting allows BERT parameters to adapt to the domain task . 
3.2 EL model MD is modeled as a sequence labelling task . 
Contextualized embeddings hare passed through a feed - forward neural network and then softmaxed for classiÔ¨Åcation over IOB : mmd = Wmdh+bmd ( 1 ) pmd = softmax ( mmd ) ( 2 ) where bmd‚ààR3is the bias term , Wmd‚ààR3√óm is a weight matrix , and pmd‚ààR3is the predicted distribution across the { I , O , B}tag set . 
The predicted tag is then simply : ÀÜ ymd= arg max i{pmd(i ) } ( 3 ) EDis modeled by Ô¨Ånding the entity ( during inference this can be from either the entire entity universe or some candidate set ) closest to the predicted entity embedding . 
We do this by applying an additional ED - speciÔ¨Åc feed - forward neural network to h : med = tanh(Wedh+bed ) ped = s(med , E ) ÀÜ yed= arg max j{ped(j)}(4 ) where bed‚ààRdis the bias term , Wed‚ààRd√ómis a weight matrix , and med‚ààRdis the same size as the entity embedding and sis any similarity measure which relates medto every entity embedding inE. In our case , we use cosine similarity . 
Our638 O 4123 Leicester # # shire beat Somerset County Cricket Club [ CLS ] [ SEP]BERThL e i c e s t e r h # # s h i r e hb e a t hS o m e r s e t hC o u n t y hC r i c k e t hC l u b h [ C L S ] h [ S E P ] B 1622318 I 1622318 O 3221 B 1622178 I 2221 I 2221 I 2221 0 1223 Output Layer FFNMD hS o m e r s e tB 1622178 FFNEDleicestershire county cricket_clubleicestershire county cricket_club - somerset county cricket_clubsomerset county cricket_clubsomerset county cricket_clubsomerset county cricket_club- -Figure 1 : Architecture of the proposed model . 
WordPiece tokens are passed through BERT forming contextualized embeddings . 
Each contextualized embedding is passed through two task - speciÔ¨Åc feed - forward neural networks for MD and ED , respectively . 
Entity ID prediction on the ‚Äò B ‚Äô MD tag is extended to the entire mention span . 
predicted entity is the index of pedwith the highest similarity score . 
We use pre - trained entity embeddings from wikipedia2vec ( Yamada et al . 
, 2018 ) , as pretraining optimal entity representation is beyond the scope of this work . 
Ideally , pre - trained entity embeddings should be from a similar architecture to our EL model , but experiments show strong results even if they are not . 
The wikipedia2vec entity embeddings used in our model are trained on the 2018 Wikipedia with 100dimensions and link graph support.5 During inference , after receiving results for each token from both the MD and ED tasks , the mention spans are tagged with { B , I}tags as shown in Figure 1 . 
For each mention span , the entity ID prediction of Ô¨Årst token represents the entire mention span . 
The remaining non - mention and non-Ô¨Årst entity ID prediction are masked out . 
Such behavior is facilitated by the training objective below . 
During training , we minimize the following multi - task objective which is inspired by Redmon and Farhadi ( 2017 ) from the object detection domain:6 J(Œ∏ ) = ŒªLmd(Œ∏ ) + ( 1‚àíŒª)Led(Œ∏ ) ( 5 ) whereLmdis the cross entropy between predicted and actual distributions of IOB and Ledis the cosine similarity between projected entity embeddings and actual entity embeddings . 
We tentatively explored triplet loss and contrastive loss with some simple negative mining strategies for ED but did not observe signiÔ¨Åcant gains in performance . 
The two loss functions are weighted by 5https://wikipedia2vec.github.io/wikipedia2vec/pretrained/ 6Similar to EL , object detection has two sub - tasks : locating bounding boxes and identifying objects in each box.a hyperparameter Œª(in our case Œª= 0.1 ) . 
Note thatLmdis calculated for all non - pad head WordPiece tokens butLedis calculated only for the Ô¨Årst WordPiece token of every labeled entity mention with a linkable and valid entity ID label . 
4 Experiments 4.1 Dataset and Performance Metrics We train and evaluate our model on the widely used AIDA / CoNLL dataset ( Hoffart et al . 
, 2011 ) . 
It is a collection of news articles from Thomson Reuters , which is split into training , validation ( testa ) and test ( testb ) sets . 
Following convention , the evaluation metric is strong - matching span - level InKB micro and macro F1 score over gold mentions , where entity annotation is available ( R ¬®oder et al . 
, 2018 ) . 
Note that ED models are evaluated by accuracy metric while EL models are evaluated by F1 , which penalizes the tagging of non - mention spans as entity mentions . 
4.2 Candidate Sets All EL models cited rely on candidate sets . 
As for our model , mentions can be efÔ¨Åciently disambiguated with respect to the entire entity universe , which we take to be the one million most frequent entities in 2018 Wikipedia . 
Consequently , our model can circumvent candidate generation , as well as the external knowledge that comes with it . 
In order to study the impact of candidate sets on our model , we apply candidate sets from Hoffart et al . 
( 2011 ) backed by the YAGO knowledge graph ( Suchanek et al . 
, 2007 ) . 
Importantly , we do not arbitrarily limit the size of the candidate sets . 
4.3 Training Details and Settings We train the EL model on the training split with a batch size of 4 for 50,000 steps . 
As in the original BERT paper , the model is optimized by the Adam639 optimizer ( Kingma and Ba , 2014 ) with the same hyperparameters except the learning rate , which we set to be 2e-5 . 
Training was performed on a Tesla V100 GPU . 
A 0.1dropout rate was used on the prediction heads . 
Experiments are repeated three times to calculate an error range . 
4.4 Results Comparison with Other EL Models . 
We compare our model with six of the most recent , and best performing , EL models in Table 1 . 
We study the performance of our model with , and without candidate sets ( see Section 4.2 ) . 
We Ô¨Ånd that when candidate sets are provided , our model outperforms existing models by a signiÔ¨Åcant margin . 
One of the problems of comparing results in the EL and ED space is that candidate sets are usually paper - speciÔ¨Åc and many works suggest their own methodologies for generating them . 
In addition to using candidate sets from Hoffart et al . 
( 2011 ) ( which makes us comparable to Kolitsas et al . 
( 2018 ) who use the same sets ) , we impose no arbitrary limit on candidate set size . 
This means that many of our candidate sets have more than the standard 20 - 30 candidates , which are normally considered in past works . 
Without candidate sets our model also shows good results and validation performance is on par with recent work by Martins et al . 
( 2019 ) who used stack LSTMs with candidate sets . 
We improve upon work by Broscheit ( 2019 ) who , like us , do not use candidate sets . 
We use a larger overall entity universe ( 1 M instead of 700 K ) . 
Interestingly , Broscheit ( 2019 ) note that during their error analysis only 3 % of wrong predictions were due to erroneous span detection . 
This could potentially explain our margin of improvement in the test set since our model is span - aware unlike theirs . 
For more details on the properties of the AIDA dataset we recommend Ilievski et al . 
( 2018 ) . 
OverÔ¨Åtting . 
There are considerable drops in performance between validation and test both when BERT is Ô¨Åne - tuned or Ô¨Åxed , pointing to potential problems with overÔ¨Åtting . 
Identical behaviour is seen in Broscheit ( 2019 ) and Poerner et al . 
( 2019 ) , who propose similar BERT -based models . 
Whether overÔ¨Åtting is due to BERT or the downstream models requires further research . 
Even more considerable drops in performance between validation and test are experienced when candidates sets are not used and entities are linkedAIDA / testa F1 ( val ) AIDA / testb F1 ( test ) Macro Micro Macro Micro Martins et al . 
( 2019 ) 82.8 85.2 81.2 81.9 Kolitsas et al . 
( 2018 ) 86.6 89.4 82.6 82.4 Cao et al . 
( 2018 ) 77.0 79.0 80.0 80.0 Nguyen et al . 
( 2016 ) - - - 78.7 Broscheit ( 2019 ) - 76.5 - 67.8 Poerner et al . 
( 2019 ) 89.1 90.8 84.2 85.0 Fine - tuned BERT with candidate sets 92.6¬±0.293.6¬±0.287.5¬±0.387.7¬±0.3 Fine - tuned BERT without candidate sets 82.6 ¬±0.283.5¬±0.270.7¬±0.369.4¬±0.3 Table 1 : Strong - matching span - level InKB macro & micro F1 results on validation and test splits of AIDA / CoNLL dataset . 
Note that the other models cited all use candidate sets . 
We run our models three times with different seeds to get bounds around our results . 
Ablation Validation F1 Test F1 Macro Micro Macro Micro Feature - based BERT with candidate sets 87.1 ¬±0.190.3¬±0.183.5¬±0.384.8¬±0.4 Feature - based BERT without candidate sets 63.3 ¬±1.164.1¬±0.257.2¬±0.254.1¬±0.3 With fasttext entity embedding 90.4 91.4 82.8 82.9 Table 2 : Ablation results on validation and test sets of AIDA / CoNLL . 
By feature - based BERT we mean BERT which is not Ô¨Åne - tuned to the task . 
across the entire entity universe . 
We can not be sure whether these drops are speciÔ¨Åc to BERT since no non - BERT works cite results over the entire entity universe . 
Ablation Study . 
We perform a simple ablation study , the results of which are shown in Table 2 . 
We note that performance suffers in the EL task when BERT is not Ô¨Åne - tuned but still maintains strong results comparable to the state - of - theart . 
Without Ô¨Åne - tuning , validation set performance decreases and becomes more comparable to test set performance , indicating that the Ô¨Ånetuned BERT overÔ¨Åts in such a setting - we Ô¨Ånd this to be an interesting future direction of study . 
Other Results . 
Finally , during research , we swapped the Wikipedia2Vec entities with averaged - out 300 - dimensional FastText embeddings ( Bojanowski et al . 
, 2017 ) to see what the impact of not having entity - speciÔ¨Åc embeddings would be . 
To our surprise , the model performs on par with existing results which , we think , points to a combination of ( 1 ) BERT already having internal knowledge of entity - mentions given their context ; and ( 2 ) many AIDA mentions being easily linkable by simply considering their surface - form . 
We think this too is an interesting direction of future study . 
Point ( 2 ) speciÔ¨Åcally points to the need for better EL datasets than AIDA , which was originally meant to be an ED dataset . 
A great study of point ( 1 ) can be found in Poerner et al . 
( 2019).640 5 Conclusions and Future Work We propose an EL model that jointly learns the MD and ED task , achieving state - of - the - art results . 
We also show that training and inference without candidate sets is possible . 
We think that interesting future directions of study include a better understanding of how BERT already comprehends entities in text without reference to external entity embeddings . 
Finally , we think that moving forward , reducing the EL community ‚Äôs dependence on candidate sets could be a good thing and requires more research . 
Dropping candidate sets could make models more easily comparable . 
Abstract Research in building intelligent agents have emphasized the need for understanding characteristic behavior of people . 
In order to reÔ¨Çect human - like behavior , agents require the capability to comprehend the context , infer individualized persona patterns and incrementally learn from experience . 
In this paper , we present a model called D APPER that can learn to embed persona from natural language and alleviate task or domain - speciÔ¨Åc data sparsity issues related to personas . 
To this end , we implement a text encoding strategy that leverages a pretrained language model and an external memory to produce domain - adapted persona representations . 
Further , we evaluate the transferability of these embeddings by simulating low - resource scenarios . 
Our comparative study demonstrates the capability of our method over other approaches towards learning rich transferable persona embeddings . 
Empirical evidence suggests that the learnt persona embeddings can be effective in downstream tasks like hate speech detection . 
1 Introduction With increasing human - machine hybrid technologies , the real - world interactions with AI systems are often stilted . 
This shortcoming can be attributed to the lack of shared common knowledge about how people will act , communicate and react under different circumstances . 
Several studies in the Ô¨Åeld of psychology ( Goldberg , 1990 ; Barrick and Mount , 1993 , 1991 ) have established the role of personas in governing how people process information , attend to and interpret life - experiences , and respond to social situations . 
SpeciÔ¨Åcally , the relationship between personality and natural language have been widely studied ( Digman and TakemotoChock , 1981 ; Pennebaker et al . 
, 2003 ) . 
For example , a narcissistic person might make frequent use of Ô¨Årst - person expressions ( I , me , myself , forme , etc . 
) . 
Therefore , endowing machines with the persona information can lead to the development of psychologically plausible intelligent systems . 
Though computational models of personality have generally followed prior psychological models or theories ( Hjelle and Ziegler , 1992 ; Costa and PAUL , 1996 ) , multiple deÔ¨Ånitions of personas have been in use depending on the nature of the domain or task at hand . 
There has been considerable amount of interest in the past that used NLP tools to conduct persona analysis of Ô¨Åctional characters in literary texts ( Flekova and Gurevych , 2015 ; Mairesse et al . 
, 2007 ) . 
Motivated by such works , we focus on deriving persona representations that explain human social behavior categorized according to their inÔ¨Çuences on language , conversations and actions in different social contexts . 
In this work , we deÔ¨Åne persona as the sum total of mental , emotional , and social characteristics of an individual ( Soloff , 1985 ) . 
This broad deÔ¨Ånition , while basing on theoretical foundations , allows us to learn persona embeddings from annotated text that span across multiple domains and social contexts . 
Often these persona - annotated domain data are either too small or not representative of all the domain aspects of persona . 
Therefore , we address these challenges by formulating our representation learning problem through the lens of domain adaptation . 
We propose a model called DAPPER1to learn a domain - adapted persona embedding that promotes positive knowledge transfer across multiple text domains : movies dialogue , forum discussion posts and personal life stories or essays . 
Towards this goal , we use a pretrained BERTmodel to extract rich semantic features from text and Ô¨Ånetune them by introducing Adaptive Knowledge Transformer that serve as adaptive layers on top of the representations obtained from BERTmodel . 
1Short for Domain Adapted Pretraining - based PErsona Representation643 These adaptive layers enrich the representations with domain - related persona knowledge . 
We explore variants of Transformer encoder layer as our adaptive layers . 
In our experiments , we compare our Transformer - based DAPPER model with RNNbased techniques on data from three different text domains . 
Finally , we showcase the advantages of using our representations in a downstream hate speech detection task . 
Thus , our contributions are as follows : ‚Ä¢We propose a model called DAPPER that integrates pretrained language model with adaptive knowledge Transformer layers to learn better domain - adapted representation of personas . 
‚Ä¢We evaluate our model on texts from multiple text domains : Movies dialogue ( Chu et al . 
, 2018 ) , forum discussion posts and personal essays or life stories ( Pennebaker and King , 1999 ) . 
Our DAPPER model outperforms the baseline models signiÔ¨Åcantly across these domains . 
‚Ä¢We determine how our model performs in domains with limited labeled data by simulating such scenarios within our existing datasets . 
We show that our domain - knowledge enriched persona representations are capable of adapting to such domains . 
Further , they show promise in an unrelated downstream hate speech detection task . 
2 Related Work Considering that personality compels a tendency on a lot of aspects of human behavior , there have been several studies intended to model personality traits from text . 
An earlier work by ( Pennebaker and King , 1999 ) compiled stream - of - consciousness essay dataset for an automated personality detection task . 
Since the Five Factor Model is widely accepted , several attempts have been made to detect personality from these essays including LIWC features or deep learning techniques ( Majumder et al . 
, 2017 ; Mairesse et al . 
, 2007 ) . 
( Chaudhary et al . 
, 2013 ) compared different machine learning models to predict Myers - Brigg Type Indicator . 
Another line of work ( Liu et al . 
, 2016 ) related to personas focused on developing a language independent and compositional model for personality trait recognition for short tweets . 
Additionally , there have beenDatasets Label Type Size # Categories Personal Essays Big - Five 2,400 5 Forum Posts MBTI 52,648 16 Movies Dialogue Tropes 17,342 72 Table 1 : Details of the datasets from different domains other efforts that model personas of movie characters and incorporate speaker persona in dialogue models based on speaking style characterized by natural language sentences ( Bamman et al . 
, 2013 ) . 
We observe that most of these works use different theories and deÔ¨Ånitions for modeling personas ‚Äì ranging from widely accepted psychological tests to simple emotion states of people as means of ascertaining personality ( Shuster et al . 
, 2018 ) . 
However , there is very limited work ( Li et al . 
, 2016 ; Chu et al . 
, 2018 ) focusing on persona embeddings that can be adapted to different domains . 
In this work , our goal is to produce general purpose persona embeddings computed using texts from various domains . 
3 Datasets Towards learning a domain - adapted persona embedding , we aggregate different forms of text data : ( a ) personal stories / essays , ( b ) dialogues and ( c ) discussion forum posts . 
Each of these datasets have distinct persona categories . 
Table 1 shows the details of the dataset . 
We elaborate them in the following sections . 
3.1 Personal Essays Corpus Personal stories or reÔ¨Çections explain important parts of one ‚Äôs personality including their goals and values ( McAdams and Manczak , 2015 ) . 
For our purpose , we make use of personal essays originally from Pennebaker et al . 
( Pennebaker and King , 1999 ) . 
This corpus consists of 2400 essays collected between 1997 and 2004 . 
Students who produced these texts were assessed based on Big Five2Questionnaires . 
To obtain labels from the self - assessments , z - scores were computed from them by ( Mairesse et al . 
, 2007 ) and the resulting scores were discretized to categories by ( Celli et al . 
, 2013 ) . 
3.2 Forum Posts Corpus One of the most commonly administered psychological tests is Myers - Briggs Type Indicator 2https://en.wikipedia.org/wiki/Big Five personality traits644 ( MBTI3 ) . 
Based on Jung ‚Äôs theory of psychological types , 16 personality types were recognized as useful reference points to understand one ‚Äôs personality . 
In this work , we collect a text corpus from a discussion forum called PersonalityCafe4 , that has dedicated communities for each of the 16MBTI personality types . 
The members of these communities generally self - identify with the corresponding personality type and post various forms of text including those written in a stream - of - consciousness style . 
To obtain these posts , we crawled speciÔ¨Åc sections of the forum related to each personality type . 
Further , we Ô¨Ålter the posts that are too short ( i.e. less than 75characters in length ) and replace explicit mentions of their personality type in the text with markers . 
Though the prevalence of MBTI personality types in general population is highly disproportional , the forum posts might not always reÔ¨Çect that distribution . 
Therefore , we create a more or less balanced dataset to avoid any skewed representation of personality types . 
In total , our Forum Posts dataset contains 52,648 posts . 
The dataset will be made publicly available . 
3.3 Movies Dialogue Corpus In a contrast to prior datasets which has welldeÔ¨Åned persona categories based on personality tests / theories , we use a dataset that views character tropes as a proxy for persona labels . 
In the context of Ô¨Åction , character trope refers to the aspects of a story that conveys information about a character including its role in the plot , personality , motivations and perceived behavior . 
Thus , we utilize the IMDB dialogue snippet dataset5(Chu et al . 
, 2018 ) containing utterances of characters in movies obtained from CMU Movie Summary datasets ( Bamman et al . 
, 2013 ) . 
Each of the 433characters in the dataset is associated with one among 72 different trope labels . 
Additionally , we collect more personarelated domain - speciÔ¨Åc knowledge from TVTropes . 
TVTropes is a wiki that collects document descriptions about plot conventions and devices . 
It also contains useful notes describing MBTI6and Big Five7personality traits with references to character tropes that closely relate to each of those categories . 
3https://en.wikipedia.org/wiki/Myers‚ÄìBriggs Type Indicator 4https://www.personalitycafe.com 5https://pralav.github.io/emnlp personas/ 6https://tvtropes.org/pmwiki/pmwiki.php/UsefulNotes/ MyersBriggs 7https://tvtropes.org/pmwiki/pmwiki.php/UsefulNotes/ BigFivePersonalityTraitsFigure 1 displays samples from the datasets used in this work . 
Using these datasets and persona category knowledge , we focus on developing domainadapted persona embeddings . 
# 13 ‚Ä¢ May 15 , 2011 I ‚Äôm tired of people making ad hominem attacks . 
I ‚Äôm tired of people thinking they ‚Äôre better than me because I ‚Äôm an F. I still do n‚Äôt believe that Americans care as much about ‚Äú immigration status ‚Äù as they care about the color of your skin . 
Forum 	 Posts 	 Corpus : 	 PersonalityCafe 	 ‚Äî 	 ISFJ Stacks Edwards : What time is it ? Tommy DeVito : It ‚Äôs eleven thirty , we ‚Äôre supposed to be there by nine . 
Stacks Edwards : Be ready in a minute . 
Tommy DeVito : Yeah , you were always fuckin ‚Äô late , you were late for your own fuckin ‚Äô funeral . 
  [ shoots 	 him]Movie 	 Dialogue 	 Corpus : 	 IMDB 	 Dialogue 	 Snippet ‚Ä¶ . 
I have some really random thoughts . 
I want the best things . 
But I fear that I want too much ! What if I fall Ô¨Çat on my face and do n‚Äôt amount to anything . 
But I feel like I was born to do BIG things on this earth . 
But who knows ‚Ä¶ There is this Persian party today . 
My neck hurts ‚Ä¶ .Personal 	 Essays 	 Corpus : 	 PersonalityCafe 	 ‚Äî 	 Extrovert Figure 1 : Samples from different datasets used for learning domain - adapted persona embeddings . 
4 Problem Setup The overall goal of our model is to learn persona embeddings using documents from different domainsD : dialogue utterances , forum posts and personal essays . 
This persona representation learning problem is formulated as a supervised classiÔ¨Åcation problem . 
Let us denote the /u1D456 / u1D461 / uni210Einput document asI(/u1D456)=[I(/u1D456 ) 1,I(/u1D456 ) 2, ... ,I(/u1D456 ) |/u1D43C| ] . 
Here , a document refers to a list of sentences from the personal essays or forum Posts corpus and dialogue snippets in case of movies dialogue corpus ( explained in Section 3 ) . 
Each input I(/u1D456)in our data is associated with their domain - speciÔ¨Åc persona label /u1D45D(/u1D456 ) /u1D458 where / u1D458‚àà{1,2, .. |D| } , /u1D45D(/u1D456 ) /u1D458‚ààY / u1D458andY / u1D458is the personal categories related to the /u1D458 / u1D461 / uni210E - domain . 
5 Proposed Model In this work , we explore the idea of leveraging a pretrained BERTmodel towards our goal of learning domain - adaptive persona embeddings . 
Instead of relying only on the domain - speciÔ¨Åc training data , we allow additional domain knowledge to be injected into our model using an external memory . 
Our model architecture is illustrated in Figure 2 . 
5.1 Input Processing The input to our DAPPER model can take different forms depending on the domain under considera-645 Persona Repr esentationsFeed - ForwardSelf - Attention KnowledgeAttentionB ER TInput 	 Pr ocessor K VMS K VDomain 	 KnowledgeN adapt 	  x Adaptive 	 Knowledge Memory[CLS ] snippet1 [ SEP ] [ CLS ] dialog snippet2 [ SEP ] Es n i p p e t 1E [ SEP ] E [ CLS ] E dialog Es n i p p e t 2E [ SEP ] EA EA EAEA EB EB EBEBInput 	 with Special 	 T okens T oken 	 Embeddings Interval 	 Segment Embeddings Position 	 Embeddingsdialog E [ CLS ] E dialog E 1 E 2 E3E 4 E 5 E 5 E 6 E7E 8Figure 2 : Illustration of our D APPER model . 
tion : ( a ) long essays or forum posts containing several sentences representing personal details , goals and values , and ( b ) dialogue snippets having character ‚Äôs own lines and additional contextual information such as narrator or interacting characters ‚Äô lines . 
The varying nature of the data from these domains can pose a challenge to our modeling objective . 
In order to represent data from these domains , we deÔ¨Åne the following procedure : ‚Ä¢For Personal Essays and Forum Posts Corpus , we insert a special [ /u1D436 / u1D43F / u1D446]token at the beginning of each sentence /u1D460 / u1D457 in an essay or post with an intention that each [ /u1D436 / u1D43F / u1D446]token will accumulate the features of the tokens following it . 
‚Ä¢For Dialogue Corpus , we introduce a [ /u1D436 / u1D43F / u1D446 ] before every dialogue snippet /u1D451 / u1D457while the character ‚Äôs own lines and additional context are separated by a[/u1D446 / u1D438 / u1D443]token . 
‚Ä¢Next , we apply interval segment embeddings , /u1D438 / u1D434or / u1D438 / u1D435 , to distinguish sentences or dialogue snippets in our data . 
This is done by alternating assignments between two consecutive sentences or dialogue snippets . 
For example , we would assign[/u1D438 / u1D434,/u1D438 / u1D435,/u1D438 / u1D434,/u1D438 / u1D435]to a list of dialogue snippets denoted as [ /u1D4511,/u1D4512,/u1D4513,/u1D4514 ] . 
We also incorporate position embeddings into ourinput data processing step . 
Thus , we obtain a uniform way of representing our inputs texts from different domains . 
This allows us to hierarchically learn abstract persona representations . 
5.2 Encoder Our input document I(/u1D456)is passed to our input processing module /u1D453I ( ¬∑ ) . 
The output of this module is a document representation augmented with special tokens and processed with interval segment and position embeddings . 
The processed input is passed to the pretrained BERT model . 
Formally , this is computed as : /u1D43B(/u1D456)=BERT(/u1D453I(I(/u1D456 ) ) ) ( 1 ) where / u1D453Iis the input processing function , /u1D43B(/u1D456 ) contains contextualized embeddings related to each token in the processed input document . 
We obtain /u1D457 / u1D461 / uni210Esentence or snippet embeddings by extracting the corresponding vector of /u1D457 / u1D461 / uni210E[/u1D436 / u1D43F / u1D446]token from the topmost BERTlayer . 
We denote this as /u1D445(/u1D456)‚àà R|/u1D43C|√ó/u1D451 / uni210E,/u1D451 / uni210Eis the set to the hidden dimensions of the B ERTmodel . 
5.3 Adaptive Knowledge Transformer Inspired by a prior work by ( Miller et al . 
, 2016 ; Zhang et al . 
, 2017 ) , we integrate an external memory module with the Transformer architecture and refer it as Adaptive Knowledge Transformer ( AKT).646 This component aids to create persistent latent embeddings related to persona categories and further accumulate more knowledge as we process data from new domains . 
We conceptualize this component to be composed of : ( a ) a Key - Value Memory Store ( KVMS ) that speciÔ¨Åcally facilitates adaptivity to new domains or data ( b ) Transformerbased adaptive layers that attends over the contents of the memory to enrich the representation with persona - related domain knowledge . 
By feeding the computed / u1D445(/u1D456)into our AKT , we obtain domainknowledge enriched persona embeddings . 
This is given as : P(/u1D456)=AKT(/u1D445(/u1D456 ) ) ( 2 ) 5.3.1 K VMS : Key - Value Memory Store OurKVMS module consists of a mutable key matrix ( K ‚àà R / u1D441 / u1D440√ó/u1D451 / u1D43E ) that accumulates personarelated knowledge across multiple domains and a non - updatable value matrix ( V‚ààR / u1D441 / u1D440√ó/u1D451 / u1D449 ) containing a learnable persona category embedding . 
The key matrix , K , is initialized with representations of text descriptions of character tropes , MBTI types and Big - Five traits collected from TVTropes wiki ( explained in 3.3 ) while the value matrix , V , is set to their corresponding learnable persona category embeddings . 
We feed the text descriptions through the input processing model and compute the sum of the sentence embeddings obtained from the topmost layer of B ERT . 
5.3.2 Knowledge - Attention Conventionally , a Transformer encoder layer consists of two sub - layers : ( a ) a multi - headed selfattention network and ( b ) a point - wise fullyconnected network . 
Each sub - layer has a residual connection followed by layer normalization . 
For the sake of brevity , we avoid the residual connections and layer normalization functions in our model illustration ( Figure 2 ) and explanation . 
Our Transformer - based adaptive layers contain an additional sub - layer to integrate the personarelevant domain knowledge into the contextual representation obtained from the encoder . 
We refer to this sub - layer as Knowledge - Attention . 
This is Ô¨Åne - tuned using domain - speciÔ¨Åc categories based on a supervised classiÔ¨Åcation objective . 
The steps involved in Transformer adaptive layers are givenas follows : /u1D444(/u1D45B)=MHA(/u1D436(/u1D45B‚àí1),/u1D436(/u1D45B‚àí1),/u1D436(/u1D45B‚àí1))(3 ) /u1D434(/u1D45B)=MHA(/u1D444(/u1D45B),K , V ) ( 4 ) /u1D436(/u1D45B)=FFN(/u1D434(/u1D45B ) ) ( 5 ) P(/u1D456)=/u1D436 / u1D441 / u1D44E / u1D451 / u1D44E / u1D45D / u1D461 ( 6 ) where MHAis a multi - head attention function as explained in ( Vaswani et al . 
, 2017 ) , /u1D45B= { 1,2, .. ,/u1D441 / u1D44E / u1D451 / u1D44E / u1D45D / u1D461},/u1D436(0)=/u1D445(/u1D456),/u1D436(/u1D45B‚àí1)is the output from the previous Transformer layer , /u1D434(/u1D45B ) is the output from the knowledge - attention sublayer . 
Our knowledge - attention mechanism identiÔ¨Åes the most correlated and relevant knowledge from the KVMS component with respect to the input document embeddings . 
The resulting domain knowledge - enhanced representations are fed to the point - wise feed - forward sub - layer ( FFN ) . 
We stack such adaptive layers on top of each other and the output from /u1D441 / u1D461 / uni210E /u1D44E / u1D451 / u1D44E / u1D45D / u1D461layer is our Ô¨Ånal domainadapted persona representation , P(/u1D456 ) . 
5.3.3 Memory Update Intuitively , accumulation of persona - related knowledge extracted from the training documents into our memory store can enhance the quality of the learned persona embeddings . 
Therefore , we perform a memory update operation on selective rows in the key matrixKbased on the persona - related features derived from the input document and its corresponding ground truth persona labels . 
The update step is deÔ¨Åned as follows : /u1D706=/u1D70E(/u1D44A / u1D458K[/u1D454 / u1D457]+/u1D44A / u1D45F / u1D719(P(/u1D456 ) ) ( 7 ) K[/u1D454 / u1D457]=/u1D706‚äôK[/u1D454 / u1D457]+(1‚àí/u1D706)‚äô/u1D719(P(/u1D456))(8 ) where / u1D454 / u1D457refers to the indices of the rows in KVMS containing knowledge about ground truth persona label / u1D45D(/u1D456 ) /u1D458,/u1D719is aggregation function that compresses the information from P(/u1D456)into a single vector . 
We Ô¨Ånd from preliminary experiments that the mean[/u1D436 / u1D43F / u1D446]token embedding serves as an effective alternative to computing an average embedding related to the tokens in the input document . 
5.4 Training Objective Our model learns persona embeddings using a supervised classiÔ¨Åcation objective . 
We feed the output of the aggregation function /u1D719to a domainspeciÔ¨Åc softmax layer to get /u1D45E , where / u1D45E=647 /u1D460 / u1D45C / u1D453 / u1D461 / u1D45A / u1D44E / u1D465(/u1D453 / u1D45E(/u1D719(P(/u1D456 ) ) ) ) . 
Note that the categories vary across each domain . 
L / u1D436 / u1D438=/u1D441 / u1D458 / summationdisplay.1 /u1D457=1‚àí/u1D45D / u1D457 / u1D459 / u1D45C / u1D454(/u1D45E / u1D457 ) ( 9 ) L / u1D44E / u1D461 / u1D461 / u1D45B=1 /u1D440 / u1D440 / summationdisplay.1 /u1D457=1‚àí/u1D459 / u1D45C / u1D454(/u1D45F / u1D457[/u1D454 / u1D457 ] ) ( 10 ) L=/u1D6FC1L / u1D436 / u1D438+/u1D6FC2L / u1D44E / u1D461 / u1D461 / u1D45B ( 11 ) whereL / u1D436 / u1D438is the cross - entropy loss , /u1D6FC1,/u1D6FC2are learnable parameters , /u1D45D / u1D457‚àà { 0,1}denotes the ground - truth label that reÔ¨Çects if the input document belongs to /u1D457 / u1D461 / uni210Epersona category , L / u1D44E / u1D461 / u1D461 / u1D45Bis the attention loss that promotes focus on rows with ground truth persona , /u1D45F / u1D457[/u1D454 / u1D457]is the attention score for the row inKreÔ¨Çecting / u1D45D / u1D456 /u1D458 ‚Äôs knowledge . 
6 Experiments In this section , we describe the various evaluations settings : datasets , baselines , our model variants , modes and metrics . 
Our experiments are designed to study the following research questions : RQ1 : How well does our DAPPER model perform in comparison to baselines and its variants on domain - speciÔ¨Åc persona classiÔ¨Åcation task ? RQ2 : Is our model capable of adapting to new domains with limited labeled data ? RQ3 : How good are the learned persona embeddings ? Do they exhibit transfer capability to a downstream task ? 6.1 Dataset Preparation We evaluate our models using persona - related datasets from different domains : movies dialogue , forum posts and personal essays as explained in Section 3 . 
Using a 70 - 10 - 20 split , we divide our persona dataset associated with each domain into training , validation and test sets . 
6.2 Baselines & Model Variants ( RQ1 ) Since we collect persona datasets from different domains , we also compare our model ‚Äôs performance to domain - speciÔ¨Åc baseline methods . 
All these methods are enlisted as follows : ‚Ä¢AFF2VEC(Khosla et al . 
, 2018 ) is a method for enriched word embeddings that are representative of affective interpretations of words . 
‚Ä¢CNN(Kim , 2014 ) is a single - layer CNNwhere the input document is passed in entirety without any additional knowledge . 
For PersonalEssays corpus , we report the best results from ( Majumder et al . 
, 2017 ) as they use additional features to improve persona classiÔ¨Åcation task . 
‚Ä¢AMN(Chu et al . 
, 2018 ) learns persona embeddings from movies dialogue using a multilevel attention mechanism augmented with prior knowledge about persona categories . 
Note that this model is one of the closest relevant work to our model . 
For movies dialogue corpus , we report scores only for the best performing conÔ¨Åguration , i.e. , /u1D45B / u1D451 / u1D456 / u1D44E / u1D459 / u1D45C / u1D454 = 32 . 
For the remaining datasets , we treat each sentence from the text as a character utterance and train the model accordingly . 
‚Ä¢TTSis a non - pretrained Transformer baseline trained with the same settings as ( Vaswani et al . 
, 2017 ) . 
We do not feed additional domain knowledge to this model . 
It is randomly initialized and trained for our task from the scratch . 
‚Ä¢BERTFT(Devlin et al . 
, 2018 ) is a Ô¨Åne - tuned ( FT ) version of BERT / u1D44F / u1D44E / u1D460 / u1D452 model . 
We do not feed additional domain knowledge to this model . 
We refrain from training BERT / u1D459 / u1D44E / u1D45F / u1D454 / u1D452 due to memory constraints . 
‚Ä¢BERT+ G RUFT(Devlin et al . 
, 2018 ; Chung et al . 
, 2014 ) is a similar to our DAPPER model , but applies GRU - based adaptive for persona classiÔ¨Åcation task . 
For this setting , we experiment with and without additional knowledge using a sufÔ¨Åx ‚Äú + K ‚Äù . 
In ‚Äú + K ‚Äù setting , we use GRU as the controller and apply an approach similar to AMN to enrich the learnt embeddings with domain knowledge . 
Without the sufÔ¨Åx , GRU is used for Ô¨Åne - tuning only . 
‚Ä¢DAPPER is our complete model by default . 
We also experiment with its variants using sufÔ¨Åx ‚Äú -K ‚Äù indicating no knowledge attention . 
The various BERT - based models can be considered as variants of our DAPPER model . 
While we report /u1D4391 - scores for movies dialogue and forum discussion post datasets , we report accuracy scores for personal essays corpus in order to remain consistent with prior work evaluation metrics ( Majumder et al . 
, 2017).648 6.3 Model Modes ( RQ2 ) We attribute the domain adaptive capability of our DAPPER model to three main aspects : pretrained language model , domain knowledge enrichment and joint training across multiple datasets . 
However , this ability can be demonstrated only when we apply it to domains with limited labeled data . 
Therefore , we run our model in ‚Äú ADAPT ‚Äù mode which simulates low - data regimes to analyze the importance of some of the above mentioned aspects . 
InADAPT mode , we restrain the amount of training data for only one of the domains while retaining the complete set for the remaining domains . 
Further , we vary the percentage of training examples from one domain to understand how early our models adapt to that domain ( with decent performance ) . 
We refer to the default model mode for experiments in Section 6.2 as ‚Äú FULL ‚Äù . 
For this experiment , we plot the average prediction performance ( /u1D4391 ) for varying percentages of domain - speciÔ¨Åc training set . 
6.4 Other Experimental Settings For baselines , we initialize our word embedding layers using GloVe ( Pennington et al . 
, 2014 ) embeddings . 
We use the publicly released pre - trained model parameters for BERT variants . 
We perform a grid - search and optimize the hyperparameters using the validation set . 
In our experiments , /u1D441 / u1D44E / u1D451 / u1D44E / u1D45D / u1D461 =3 , resulted in best outcomes . 
We use Adam ( Kingma and Ba , 2014 ) as our optimizer . 
In FULL mode , the model achieves the best performance after training for 50 epochs with a learning rate of / u1D6FC=0.00001 . 
For ADAPT mode , we perform a Ô¨Åxed number of epochs to train each variant . 
We use PyTorch to implement our model and train it on on 4 GPUs . 
In order to alleviate the problem of unbalanced datasets , we utilize class weights in categorical cross - entropy loss for each domain based on the training and validation sets . 
6.5 Results 6.5.1 D APPER Performance ( RQ1 ) Table 2 presents the results of our evaluation under complete training data settings ( FULL ) . 
Our DAPPER model achieves an absolute improvement of 14.53 % over previously reported model baseline ( AMN ) in the dialogues domain . 
While several models have shown only marginal improvement in prediction performance on Personal Essays corpus , our model shows promise by recording an improvement of 8.67 % in comparison to thepreviously reported CNNbaseline . 
Overall , our DAPPER model outperforms the baselines across all the three datasets signiÔ¨Åcantly . 
Effect of Architecture Choices ( RQ1 ): Pretrained BERT - based models have consistently outperformed all the previous baselines including the non - pretrained TTSmodel . 
Moreover , the Transformer - based adaptive layers , with an average improvement of 6.1 % ( with knowledge - attention ) and 4.2 % ( without knowledge - attention ) , are much more powerful than RNN - based adaptive layers . 
Further , we observe that BERT+ G RUFTrecords only marginal gains over BERTwhen there is no knowledge - attention . 
Effect of Knowledge - Attention ( RQ1 ): From our results in Table 2 , we analyze the importance of the knowledge - attention to the overall performance gain . 
We compute percentage performance gain between similar models with and without knowledgeattention sub - layer(eg . 
DAPPER , DAPPER‚àí/u1D43E ) . 
We Ô¨Ånd that the performance boost provided by the knowledge - attention module is noteworthy . 
We posit that the higher percentage gain ( 7.38 % ) for Forum Posts dataset is due to the additional domain knowledge ( MBTI - related ) ingested into our KVMS ( explained in Section 3 ) . 
Inspecting further within individual domain , the percentage increase in prediction performance almost doubles8for Transformer - based adaptive layers ( as in DAPPER ) in comparison with RNN - based adaptive layers ( BERT+ G RUFT + K ) . 
The reason for this phenomenon can be ascribed to the multi - hop knowledge enrichment facilitated by /u1D441 / u1D44E / u1D451 / u1D44E / u1D45D / u1D461 encoder layers commonly observed in Memory networks literature ( Miller et al . 
, 2016 ) . 
6.5.2 A DAPT Mode Performance ( RQ2 ) Figure 3a and 3b show the mean prediction performance on movies dialogue and forum posts datasets respectively . 
We measure the domain adaptive capability of models based on the distance from its lifetime best performance . 
By varying the percentage of training data , we notice that our DAPPER model stabilizes early and outperforms the other variants with limited amount of training data . 
Notably , AMN model performs better than TTSmodel under low - data regimes . 
The improved performance of AMN is due to the domain knowledge enrichment via an external memory module . 
8 % increase - RNN vs Transformer - based adaptive layers : Movies dialogue corpus : 1.6 % vs 3.24 % ( dialogue ) , 5.86 % vs 8.9 % ( posts ) , 1.6 % to 2.4 % ( essays)649 Models Domain - related Persona Datasets Movies Dialogues ( /u1D4391)Forum Posts ( /u1D4391)Personal Essays ( /u1D434 / u1D450 / u1D450 . 
) AFF2VEC 0.579 * CNN 0.628 0.391 0.588 * AMN 0.750 * 0.453 0.591 TTS 0.776 0.496 0.593 BERTFT 0.804 0.539 0.607 BERT+ G RUFT + K 0.820 0.579 0.616 BERT+ G RUFT 0.807 0.547 0.608 DAPPER 0.859 0.636 0.639 DAPPER‚àíK 0.832 0.584 0.624 ( a)Models /u1D46D1 Text Only BCA 0.744 * CNN - CHAR 0.735 * 1 - Extra Feature BCA+P 0.776 BCA+ SC 0.784 * All Features BCA+ SC+P(>/u1D44E / u1D461 / u1D461)0.812 BCA+ SC+P(</u1D44E / u1D461 / u1D461)0.824 ( b ) Table 2 : Evaluation results of different models on : ( a ) three different Persona - related domain datasets in F ULL mode , and ( b ) a downstream application ‚Äì Hate Speech detection . 
Results with * are taken from prior studies using the model on that dataset . 
This feature is absent in TTS . 
Furthermore , we note thatDAPPER‚àíKmodel is able to maintain a good performance even under low - data settings . 
We intuit that pretraining involved in DAPPER‚àíKmodel is one of the reasons behind this behavior . 
Therefore , we Ô¨Ånd that our DAPPER model is able to learn general purpose persona embeddings that can adapt to low - data settings . 
Moreover , the combination of pretraining and adaptive knowledge transformer facilitates domain adaptation effectively . 
F1 score00.180.360.540.720.9 Percentage of    Training data20%40%60%80%100 % DAPPER DAPPER - K Transformer AMNF1 score00.140.280.420.560.7 Percentage of    Training data20%40%60%80%100 % DAPPER DAPPER - K Transformer AMN ( a ) Movies Dialogue Corpus(b ) Forum Posts Corpus Figure 3 : Evaluation of D APPER model in A DAPT mode . 
We report the mean prediction performance ( /u1D4391 ) on Movies Dialogue and Forum Posts dataset . 
6.6 Cluster Analysis ( RQ3 ) In order to demonstrate the capabilities of our persona embedding , we Ô¨Årst perform a simple cluster analysis . 
Following prior studies ( Bamman et al . 
, 2013 ; Chu et al . 
, 2018 ) , we measure the ability to recover persona - based clusters using our embeddings through the purity scores as in ( Bamman et al . 
, 2013 ) . 
We compute the overlap between clusters as:/u1D443 / u1D462 / u1D45F / u1D456 / u1D461 / u1D466 = 1 /u1D441 / summationtext.1 /u1D45B / u1D45A / u1D44E / u1D465 / u1D457|/u1D466 / u1D45B‚à©/u1D450 / u1D457| , where / u1D466 / u1D45B is the / u1D45B - th ground truth cluster , /u1D441is total number of characters , /u1D450 / u1D457is the / u1D457 / u1D461 / uni210Epredicted cluster . 
By applying simple agglomerative clustering on our persona embeddings ( /u1D458clusters ) , we report thesek AMN DP DAPPER 25 48.4 39.63 68.6 50 48.1 31.0 65.3 100 45.2 24.4 63.4 Table 3 : Cluster purity scores . 
DP is the Dirichlet Persona as reported in ( Bamman et al . 
, 2013 ) purity scores for movies dialogue corpus . 
SpeciÔ¨Åcally , we compare the results with AMN . 
Results in Table 3 indicate that our DAPPER model sharpens the persona embeddings so as to form much better clusters . 
7 Application : Hate Speech Detection With concerns about hate crimes , harassment , and intimidation on the rise , the role of online hate in exacerbating such violence can not be discounted . 
Hence , there is an growing need to identify and counter the problem of hateful content on social media . 
While most prior modeling approaches have attempted to capture the semantics of hate from text , a few of them ( Vijayaraghavan et al . 
) have used multi - modal information to detect hateful content . 
Few attempts have been made to study the personality of targets and instigators of hate . 
Since our DAPPER model learns persona embeddings from different forms of text such as dialogues , posts or personal essays , we deem it Ô¨Åt to explore how well our persona embeddings transfer knowledge to a hate speech detection task involving texts from a different domain ( in our case , Twitter ) . 
There are several publicly available labeled hate speech datasets ( de Gibert et al . 
, 2018 ; Waseem , 2016 ) but very few include author metadata or650 tweets . 
In this work , we take advantage of the models and datasets introduced by ( Vijayaraghavan et al . 
) ( hereafter referred as M M - HATE ) . 
This weakly - labeled dataset contains author information and additional metadata about potential hate groups . 
Instead of training a powerful hate speech system from the scratch , we augment their base architecture with our persona embeddings and evaluate the prediction performance on the task at hand . 
We compute persona representations ( P ) for an author based on their past tweets . 
We train MM - HATE ‚Äôs best performing model , BIGRU+C HAR+ATTN ( BCA ) , under the following settings : ( a ) BCA+P , which combines our persona embeddings with the extracted text features , ( b ) BCA+SC+P(>/u1D44E / u1D461 / u1D461 ) , which integrates the persona embeddings at the penultimate layer . 
Note that the text and sociocultural ( SC ) features are already fused at that layer , and ( c ) BCA+SC+P(</u1D44E / u1D461 / u1D461)fuses the extracted text and socio - cultural features with persona embeddings using an attention layer ( as in M M - HATE ) . 
Table 2b summarizes the results of our evaluation on hate speech detection task . 
We observe that SC - fused model ( BCA+SC ) performs marginally better than our persona - fused model ( BCA+P ) . 
This result can be ascribed to the domain speciÔ¨Åcity ofSCfeatures . 
We also note that the combination of all the extracted features leads to a marked improvement in prediction performance , and even more so when the persona embeddings are fed to the fusion layer ( BCA+SC+P(</u1D44E / u1D461 / u1D461 ) ) . 
Thus , our DAPPER model is able to extract behavioral features from user texts allowing positive knowledge transfer to various domains and applications . 
8 Conclusion We proposed a DAPPER model that learns a domain adapted pretraining - based persona representation . 
Our DAPPER model leverages pretrained BERT model and Ô¨Åne - tunes it with additional domainadaptive layers . 
By introducing a knowledgeattention mechanism , we allow the domain knowledge to be integrated into our persona embeddings . 
The proposed model achieves signiÔ¨Åcant gains across persona classiÔ¨Åcation task in different domains . 
Our evaluations validate that our model is capable of adapting to a new domain with limited labeled data . 
We also highlight the transferability of our persona embeddings in a downstream hate speech detection task . 
References David Bamman , Brendan O‚ÄôConnor , and Noah A Smith . 
2013 . 
Learning latent personas of Ô¨Ålm characters . 
In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 352‚Äì361 . 
Murray R Barrick and Michael K Mount . 
1991 . 
The big Ô¨Åve personality dimensions and job performance : a meta - analysis . 
Personnel psychology , 44(1):1‚Äì26 . 
Murray R Barrick and Michael K Mount . 
1993 . 
Autonomy as a moderator of the relationships between the big Ô¨Åve personality dimensions and job performance . 
Journal of applied Psychology , 78(1):111 . 
Fabio Celli , Fabio Pianesi , David Stillwell , and Michal Kosinski . 
2013 . 
Workshop on computational personality recognition : Shared task . 
In Seventh International AAAI Conference on Weblogs and Social Media . 
Shristi Chaudhary , Ritu Singh , Syed Tausif Hasan , and Ms Inderpreet Kaur . 
2013 . 
A comparative study of different classiÔ¨Åers for myers - brigg personality prediction model . 
Linguistic analysis , page 21 . 
Eric Chu , Prashanth Vijayaraghavan , and Deb Roy . 
2018 . 
Learning personas from dialogue with attentive memory networks . 
arXiv preprint arXiv:1810.08717 . 
Junyoung Chung , Caglar Gulcehre , KyungHyun Cho , and Yoshua Bengio . 
2014 . 
Empirical evaluation of gated recurrent neural networks on sequence modeling . 
arXiv preprint arXiv:1412.3555 . 
JR Costa and T PAUL . 
1996 . 
of personality theories : Theoretical contexts for the Ô¨Åve - factor model . 
The Ô¨Åve - factor model of personality : Theoretical perspectives , 51 . 
Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 
2018 . 
Bert : Pre - training of deep bidirectional transformers for language understanding . 
arXiv preprint arXiv:1810.04805 . 
John M Digman and Naomi K Takemoto - Chock . 
1981 . 
Factors in the natural language of personality : Re - analysis , comparison , and interpretation of six major studies . 
Multivariate behavioral research , 16(2):149‚Äì170 . 
Lucie Flekova and Iryna Gurevych . 
2015 . 
Personality proÔ¨Åling of Ô¨Åctional characters using sense - level links between lexical resources . 
In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 1805‚Äì1816 . 
Ona de Gibert , Naiara Perez , Aitor Garc ¬¥ ƒ±a - Pablos , and Montse Cuadros . 
2018 . 
Hate speech dataset from a white supremacy forum . 
arXiv preprint arXiv:1809.04444 .651 Lewis R Goldberg . 
1990 . 
An alternative ‚Äù description of personality ‚Äù : the big-Ô¨Åve factor structure . 
Journal of personality and social psychology , 59(6):1216 . 
Larry A Hjelle and Daniel J Ziegler . 
1992 . 
Personality theories : Basic assumptions , research , and applications . 
McGraw - Hill Book Company . 
Sopan Khosla , Niyati Chhaya , and Kushal Chawla . 
2018 . 
Aff2vec : Affect ‚Äì enriched distributional word representations . 
arXiv preprint arXiv:1805.07966 . 
Yoon Kim . 
2014 . 
Convolutional neural networks for sentence classiÔ¨Åcation . 
arXiv preprint arXiv:1408.5882 . 
Diederik P Kingma and Jimmy Ba . 
2014 . 
Adam : A method for stochastic optimization . 
arXiv preprint arXiv:1412.6980 . 
Jiwei Li , Michel Galley , Chris Brockett , Georgios P Spithourakis , Jianfeng Gao , and Bill Dolan . 
2016 . 
A persona - based neural conversation model . 
arXiv preprint arXiv:1603.06155 . 
Fei Liu , Julien Perez , and Scott Nowson . 
2016 . 
A language - independent and compositional model for personality trait recognition from short texts . 
arXiv preprint arXiv:1610.04345 . 
Franc ¬∏ois Mairesse , Marilyn A Walker , Matthias R Mehl , and Roger K Moore . 
2007 . 
Using linguistic cues for the automatic recognition of personality in conversation and text . 
Journal of artiÔ¨Åcial intelligence research , 30:457‚Äì500 . 
Navonil Majumder , Soujanya Poria , Alexander Gelbukh , and Erik Cambria . 
2017 . 
Deep learning - based document modeling for personality detection from text . 
IEEE Intelligent Systems , 32(2):74‚Äì79 . 
Dan P McAdams and Erika Manczak . 
2015 . 
Personality and the life story . 
Alexander Miller , Adam Fisch , Jesse Dodge , AmirHossein Karimi , Antoine Bordes , and Jason Weston . 
2016 . 
Key - value memory networks for directly reading documents . 
arXiv preprint arXiv:1606.03126 . 
James W Pennebaker and Laura A King . 
1999 . 
Linguistic styles : Language use as an individual difference . 
Journal of personality and social psychology , 77(6):1296 . 
James W Pennebaker , Matthias R Mehl , and Kate G Niederhoffer . 
2003 . 
Psychological aspects of natural language use : Our words , our selves . 
Annual review of psychology , 54(1):547‚Äì577 . 
Jeffrey Pennington , Richard Socher , and Christopher D Manning . 
2014 . 
Glove : Global vectors for word representation . 
In Proceedings of the 2014 conference on empirical methods in natural language processing ( EMNLP ) , pages 1532‚Äì1543.Kurt Shuster , Samuel Humeau , Antoine Bordes , and Jason Weston . 
2018 . 
Engaging image chat : Modeling personality in grounded dialogue . 
arXiv preprint arXiv:1811.00945 . 
Paul H Soloff . 
1985 . 
Personality disorders . 
In Diagnostic interviewing , pages 131‚Äì159 . 
Springer . 
Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , ≈Åukasz Kaiser , and Illia Polosukhin . 
2017 . 
Attention is all you need . 
In Advances in neural information processing systems , pages 5998‚Äì6008 . 
Prashanth Vijayaraghavan , Hugo Larochelle , and Deb Roy . 
Interpretable multi - modal hate speech detection . 
Zeerak Waseem . 
2016 . 
Are you a racist or am i seeing things ? annotator inÔ¨Çuence on hate speech detection on twitter . 
In Proceedings of the Ô¨Årst workshop on NLP and computational social science , pages 138 ‚Äì 142 . 
Jiani Zhang , Xingjian Shi , Irwin King , and Dit - Yan Yeung . 
2017 . 
Dynamic key - value memory networks for knowledge tracing . 
In Proceedings of the 26th international conference on World Wide Web , pages 765‚Äì774.652 Proceedings of the 1st Conference of the Asia - PaciÔ¨Åc Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing , pages 653‚Äì663 December 4 - 7 , 2020 . 
¬© 2020 Association for Computational Linguistics Event Coreference Resolution with Non - Local Information Jing Lu and Vincent Ng Human Language Technology Research Institute University of Texas at Dallas Richardson , TX 75083 - 0688 { ljwinnie , vince } @hlt.utdallas.edu Abstract Existing event coreference resolvers have largely focused on exploiting the information extracted from the local contexts of the event mentions under consideration . 
Hypothesizing that non - local information could also be useful for event coreference resolution , we present two extensions to a state - of - the - art joint event coreference model that involve incorporating ( 1 ) a supervised topic model for improving trigger detection by providing global context , and ( 2 ) a preprocessing module that seeks to improve event coreference by discarding unlikely candidate antecedents of an event mention using discourse contexts computed based on salient entities . 
The resulting model yields the best results reported to date on the KBP 2017 English and Chinese datasets . 
1 Introduction Event coreference resolution is the task of determining the event mentions in a document that refer to the same real - world event . 
One of its major challenges concerns error propagation : since the event coreference resolution component typically lies towards the end of the standard information extraction pipeline , the performance of an event coreference resolver can be adversely affected by errors propagated from its upstream components . 
The upstream component that has the largest impact on event coreference performance is arguably trigger detection . 
Recall that the goal of a trigger detector is to identify event triggers and assign an event subtype to each of them . 
Failure to detect triggers could therefore limit the upper bound on event coreference performance . 
To address error propagation , one way that has been shown to be effective for a variety of NLP tasks is to develop joint models , which allow crosstask output constraints to be learned from annotated training data . 
For event coreference , a learnercan easily learn , for instance , that two coreferent event mentions must have the same event subtype , thereby allowing event coreference to inÔ¨Çuence trigger detection . 
Unfortunately , the vast majority of existing event coreference resolvers have adopted a pipeline architecture where trigger detection precedes event coreference . 
In particular , joint models are both under - studied and under - exploited for event coreference given the usefulness they have demonstrated for other NLP tasks . 
One exception is Lu and Ng ‚Äôs ( 2017a ) joint model , which jointly learns trigger detection and event coreference and has achieved state - of - the - art results . 
As a structured conditional random Ô¨Åeld , the model employs unary factors to encode the features speciÔ¨Åc for each task and binary / ternary factors to capture the interaction between each pair of tasks . 
The use of binary / ternary factors is a particularly appealing aspect of this model : it allows these cross - task interactions to be captured in a softmanner , enabling the learner to learn which combinations of values of the output variables are more probable . 
We hypothesize that the power of this joint event coreference model has not been fully exploited and seek to extend it in this paper . 
Our extensions are based on the observation that the strength of a joint model stems from its ability to facilitate cross - task knowledge transfer . 
In other words , the better we can model each task involved , the more we can potentially get out of joint modeling . 
Given this observation , we seek to improve the modeling of these tasks in this joint model as follows . 
First , we improve trigger detection by exploiting topic information . 
State - of - the - art trigger detectors , including those based on deep neural networks ( e.g. , Nguyen et al . 
( 2016 ) ) , classify each candidate trigger using local information and largely ignore the fact that the topic of the document in which a trigger appears plays an important role in determining its event subtype . 
To understand the usefulness653 Three journalists at The New York Times on Tuesday announced plans to { leave}ev1the newspaper . 
The { departures}ev2 follow moves last month by several other Times employees , all of whom were { leaving}ev3to join digital companies . 
Pakistan ‚Äôs Interior Ministry has ordered New York Times Reporter to { leave}ev4 . 
The ministry gave no explanation for the expulsion order . 
‚Äú You are therefore advised to { leave}ev5the country within 72 hours , ‚Äù the order stated . 
Table 1 : Event coreference resolution examples . 
of document topics , consider the examples in Table 1 : although all Ô¨Åve events have similar trigger words , we can see that the meaning of the triggers and their event subtypes are different in different contexts . 
Hence , if an event coreference model knows that the topics of these two documents are different , it can exploit this information to more accurately classify their event subtypes . 
In particular , we propose to train a supervised topic model to infer the topic of each word in a test document , with the goal of understanding each candidate trigger using its global in addition to local context . 
Second , we improve event coreference by exploiting discourse information . 
SpeciÔ¨Åcally , we introduce a preprocessing component for event coreference resolution where we prune the candidate antecedents of an event mention that are unlikely to be its correct antecedent based on discourse context . 
In essence , this discourse - based preprocessing step seeks to simplify the job of the event coreference model by reducing the number of candidate antecedents it has to consider for a given event mention . 
We encode the discourse context of an event mention using the entities that are salient at the point of the discourse in which the event mention appears . 
To our knowledge , we are the Ô¨Årst to show that event coreference performance can be improved using discourse contexts that are encoded using salient discourse entities . 
In sum , the contributions of this paper are twofold . 
First , while existing event coreference resolvers have largely focused on exploiting the information extracted from the local contexts of the event mentions under consideration , we show how a state - of - the - art joint event coreference model can be improved using the non - local information provided by a supervised topic model and salient discourse entities . 
Second , the resulting model achieves the best results to date on the KBP 2017 English and Chinese event coreference datasets . 
2 DeÔ¨Ånitions and Corpora 2.1 DeÔ¨Ånitions We employ the following deÔ¨Ånitions in our discussion of trigger detection and event coreference:‚Ä¢Anevent trigger is a string of text that most clearly expresses the occurrence of an event , usually a word or a multi - word phrase . 
‚Ä¢Anevent mention is an explicit occurrence of an event consisting of a textual trigger , arguments or participants ( if any ) , and the event type / subtype . 
‚Ä¢Anevent coreference chain ( a.k.a . 
an event hopper ) is a group of event mentions that refer to the same real - world event . 
They must have the same event ( sub)type . 
To understand these deÔ¨Ånitions , consider the example in Table 1 , which contains Ô¨Åve event mentions from two documents . 
The Ô¨Årst one consists of three event mentions of subtype Personnel . 
Endposition , among which ev1andev2 , which are triggered by ‚Äú leave ‚Äù and ‚Äú departures ‚Äù respectively , are coreferent since they describe the event that three journalists resign . 
The second one consists of two coreferent event mentions , ev4andev5 , both of which are triggered by ‚Äú leave ‚Äù and have subtype Movement . 
Transport Person . 
2.2 Corpora We employ the English and Chinese corpora used in the TAC KBP 2017 Event Nugget Detection and Coreference task for evaluation , which are composed of two types of documents , newswire documents and discussion forum documents . 
There are no ofÔ¨Åcial training sets : the task organizers have simply made available a number of event coreference - annotated corpora for training . 
For English , we use LDC2015E29 , E68 , E73 , E94 , and LDC2016E64 for training . 
Together they contain 817 documents with 22894 event mentions distributed over 13146 coreference chains . 
For Chinese , we use LDC2015E78 , E105 , E112 , and LDC2016E64 for training . 
Together they contain 548 documents with 7388 event mentions distributed over 5526 coreference chains . 
The KBP 2017 English test set consists of 167 documents with 4375 event mentions distributed over 2963 coreference chains . 
The Chinese test set consists of 167 documents with 3884 event mentions distributed over 2558 coreference chains.654 3 Model Following Lu and Ng ( 2017a ) , we employ a structured conditional random Ô¨Åeld , which operates at the document level . 
SpeciÔ¨Åcally , given a test document , we Ô¨Årst extract from it all single- and multiword nouns and verbs that have appeared at least once as a trigger in the training data . 
We treat each of these extracted nouns and verbs as a candidate event mention . 
The goal of the model is to make joint predictions for the candidate event mentions in a document . 
Three predictions will be made for each candidate event mention that correspond to the three tasks in the model : its trigger subtype , its induced topic , and its antecedent . 
Given this formulation , we deÔ¨Åne three types of output variables . 
The Ô¨Årst type consists of event subtype variables s= ( s1, ... ,s n ) . 
Eachsitakes a value in the set of the 18 event subtypes deÔ¨Åned in KBP 2017 or NONE , which indicates that the event mention is not a trigger . 
The second type consists of coreference variables c= ( c1, ... ,c n ) , where ci‚àà{1 , . 
. 
. 
, i‚àí1,NEW } . 
In other words , the value of each ciis the i d of its antecedent , which can be one of the preceding event mentions , or NEW(if the mention underlying cistarts a new cluster ) . 
The third type consists of topic variables t = ( t1, ... ,t n ) . 
Eachtitakes a value in a 19 - element set in which the topics have a one - to - one correspondence with the event subtype labels deÔ¨Åned above . 
Despite this one - to - one mapping , these two types of labels should not be interpreted in the same manner . 
As we will see , a word ‚Äôs induced topic label is inÔ¨Çuenced by our supervised topic model , whereas a word ‚Äôs subtype is not . 
Each candidate event mention is associated with one coreference variable , one event subtype variable , and one topic variable . 
Our model induces a probability distribution over these variables : p(s , c , t|x ; Œò)‚àùexp(/summationdisplay iŒ∏ifi(s , c , t , x ) ) whereŒ∏i‚ààŒòis the weight associated with feature functionfiandxis the input document . 
3.1 Independent Models 3.1.1 Trigger Detection Model Each instance for training the trigger detection model corresponds to a candidate trigger in the training set , which is created as follows . 
For each wordwthat appears as a true trigger at least once in the training data , we create a candidate triggerfrom each occurrence of win the training data . 
If a given occurrence of wis a true trigger in the associated document , the class label of the corresponding training instance is its subtype label . 
Otherwise , we label the instance as N ONE . 
Each candidate trigger mis represented using features generated from the following feature templates : m ‚Äôs word , m ‚Äôs lemma , word bigrams formed with a window size of three from m ; feature conjunctions created by pairing m ‚Äôs lemma with each of the following features : the head word of the entity syntactically closest to m , the head word of the entity textually closest to m , the entity type of the entity that is syntactically closest to m , and the entity type of the entity that is textually closest to m.1In addition , for event mentions with verb triggers , we use the head words and the entity types of their subjects and objects as features , where the subjects and objects are extracted from the dependency parses produced by Stanford CoreNLP ( Manning et al . 
, 2014 ) . 
For event mentions with noun triggers , we create the same features except that we replace the subjects and verbs with heuristically extracted agents and patients . 
3.1.2 Topic Model Our Ô¨Årst extension to Lu and Ng ‚Äôs ( 2017a ) model seeks to improve trigger detection using topic information . 
We train a supervised topic model to infer the topic of each word in a test document , with the goal of understanding each candidate trigger using itsglobal in addition to local context . 
Like the trigger detection model , each training instance corresponds to a candidate trigger . 
The class label is the topic label of the candidate trigger . 
We have 19 topic labels in total : there is a one - to - one correspondence between the 18 subtype labels and 18 of the topic labels . 
The remaining topic label is OTHER , which is reserved for those words that do not belong to any of the 18 topics . 
Topic labels can be derived directly from subtype labels given the one - to - one correspondence between them . 
Each candidate trigger is represented using 19 features , which correspond to the 19 topic labels . 
The value of a feature , which is derived from the output of a LabeledLDA model ( Ramage et al . 
, 2009 ) , encodes the probability that the candidate trigger belongs to the corresponding topic . 
To train the LabeledLDA model , we Ô¨Årst apply LabeledLDA using the Mallet toolkit ( McCallum , 1We use an in - house CRF - based entity extraction model to jointly identify the entity mentions and their types.655 2002 ) to the training documents , which learns a distribution over words for each topic , Œ≤ . 
We represent each training document using the candidate triggers as well as the context words that are useful for distinguishing the topics.2To get the useful context words , we rank the words in the training documents by their weighted log - likelihood ratios : P(wi|mj , vk ) logP(wi|mj , vk ) P(wi|mj,¬¨vk ) wherewi , mjandvkdenote theith word in the vocabulary , the jth candidate trigger word and the kth subtype ( including NONE ) , respectively . 
Intuitively , a word wiwill have a high rank with respect to a candidate trigger word mjof subtypevkif it appears frequently with mjof subtypevkand infrequently with mjof other subtypes . 
We employ as the useful context words the top 125 words ranked by the weighted log likelihood ratio w.r.t . 
each pair of trigger and subtype . 
The label set of each training document is the set of subtypes collected from all the triggers in the document plus N ONE . 
After training , we apply the resulting LabeledLDA model to a test document , which is represented using the candidate triggers and the useful context words , as deÔ¨Åned above . 
SpeciÔ¨Åcally , given a test document , we ( 1 ) apply the model to infer the distribution of topics in the document , and then ( 2 ) compute the posterior distribution of topics given each candidate trigger in the document using Bayes rule as follows : P(z|m)‚àùP(m|z : Œ≤)P(z ) whereP(z)is the distribution of topic zin the test document , P(m|z : Œ≤)is the topic - dependent distribution of candidate triggers mthat is learned from the training documents , and P(z|m)is the posterior distribution of zgivenmin the test document . 
We use this posterior distribution to generate features for representing each instance for training / testing the topic model , as described above . 
Note that while the label sets used by the trigger detector and the topic model are functionally equivalent , they are trained using different feature sets . 
The features used by the trigger detector encodes a candidate trigger ‚Äôs local context , while the features used by the topic model encodes its global context ( e.g. , its relationship with other words ) . 
2If a candidate trigger is a multi - word phrase , we treat it as a ‚Äú word ‚Äù by concatenating its constituent words using underscores ( e.g. ,‚Äústep down ‚Äù is represented as ‚Äú step down‚Äù).3.1.3 Event Coreference Model Our event coreference model is an adaptation of Durrett and Klein ‚Äôs ( 2013 ) mention - ranking model , which was originally developed for entity coreference , to the task of event coreference . 
This model selects the most probable antecedent for a mention to be resolved from its set of candidate antecedents ( or N EWif the mention is non - anaphoric ) . 
We employ two types of feature templates to represent the candidate antecedents for the event mention to be resolved , mj . 
The Ô¨Årst type is composed of features that represent the NULL candidate antecedent.3These include : mj ‚Äôs word , mj ‚Äôs lemma , a conjoined feature created by pairing mj ‚Äôs lemma with the number of sentences preceding mj , and another conjoined feature created by pairing mj ‚Äôs lemma with the number of mentions precedingmjin the document . 
The second type is composed of features that represent a non- NULL candidate antecedent , mi . 
These include mi ‚Äôs word , mi ‚Äôs lemma , whether miandmjhave the same lemma , and the following feature conjunctions : ( 1 ) mi ‚Äôs word paired with mj ‚Äôs word , ( 2)mi ‚Äôs lemma paired withmj ‚Äôs lemma , ( 3 ) the sentence distance betweenmiandmjpaired with mi ‚Äôs lemma and mj ‚Äôs lemma , ( 4 ) the mention distance between mi andmjpaired withmi ‚Äôs lemma and mj ‚Äôs lemma , ( 5 ) a quadruple consisting of miandmj ‚Äôs subjects and their lemmas , and ( 6 ) a quadruple consisting ofmiandmj ‚Äôs objects and their lemmas . 
Our second extension to Lu and Ng ‚Äôs ( 2017a ) model involves leveraging discourse information to improve this event coreference model . 
Specifically , we introduce a preprocessing component for event coreference resolution where we prune the candidate antecedents of an event mention that are unlikely to be its correct antecedent based on discourse context . 
The idea is to ( 1 ) encode the discourse context of each event mention in a document using the entities that are salient at the point of the discourse in which the event mention appears , and by hypothesizing that two event mentions that appear in different discourse contexts are unlikely to be coreferent , we ( 2 ) prune any candidate antecedent of an event mention mwhose discourse context is different from that of m , allowing the event coreference model to resolve an event mention to one of the candidate antecedents that survive this discourse - based Ô¨Åltering step . 
In essence , this 3Resolving a mention to the NULL antecedent is the same as having the mention starts a N EWcluster.656 preprocessing step seeks to simplify the job of the event coreference model by reducing the number of candidate antecedents it has to consider for a given event mention . 
Since we aim to encode the discourse context of each event mention using the entities that are salient at the point of the discourse in which the event mention appears , we need to compute the salience score of each entity Ew.r.t . 
each event mentionm . 
We employ the following formula , which was proposed by Chen and Ng ( 2015b ): /summationdisplay e‚ààEg(e)√ódecay ( e ) In this formula , eis a mention of entity Ethat appears in either the same sentence as mor one of its preceding sentences . 
g(e)is a score that is computed based on the grammatical role of ein the sentence : 4 if eis a subject , 2 if it is an object , and 1 otherwise . 
decay ( e)is a decay factor that is set to 0.5dis , wheredisis the sentence distance between eandm . 
We compute discourse entities using Stanford CoreNLP ‚Äôs neural entity coreference resolver and grammatical roles using CoreNLP ‚Äôs syntactic dependency parser . 
Next , we deÔ¨Åne the discourse context of an event mentionmto be the list of entities whose salience score is at least 1 when computed w.r.t . 
m. As noted before , we aim to prune the unlikely candidate antecedents of an event mention m , namely those candidates whose discourse contexts are different from that of m. Rather than heuristically deÔ¨Åning a function for computing the similarity between two different discourse contexts , we train a ranker that ranks the candidate antecedents of m based on two types of features derived from their discourse contexts : Salience score ratios ( SSRs ): For each entity E that appears in the discourse contexts of both candidate antecedent candm , we Ô¨Årst compute E ‚Äôs SSR as the ratio of E ‚Äôs salience score computed w.r.t.mtoE ‚Äôs salience score computed w.r.t . 
c. ( If this ratio is less than 1 , we take its reciprocal . 
) Then , for each ( c , m)pair , we create Ô¨Åve features that encode the number of entities whose SSR falls into each of these Ô¨Åve intervals : [ 1,1 ] , ( 1 , 2 ] , ( 2 , 3 ] , ( 3,4 ] , ( 4,5 ] , and [ 5 , inf ] . 
Intuitively , c ‚Äôs andm ‚Äôs discourse contexts tend to be more similar if they have more entities in the lower buckets . 
Lexical features : For each mention em1of each entity in candidate antecedent c ‚Äôs discourse conFigure 1 : Unary factors for the three tasks , the variables they are connected to , and the possible values of the variables . 
text and each mention em2of each entity in m ‚Äôs discourse context , we create a lexical feature that pairsem1 ‚Äôs head with em2 ‚Äôs head . 
To train this ranker , we employ the same loglinear model as the one used for the event coreference model , where the training objective is to maximize the likelihood of selecting the correct antecedent for each event mention . 
After training , we apply this ranker to prune all but the topkcandidate antecedents of each event mention in a test document . 
These kcandidate antecedents , together with the NULL candidate antecedent , will be ranked by the event coreference model , and the highest - ranked candidate will be selected as the antecedent of the event mention under consideration.4We treatkas a hyperparameter and tune it on the development set . 
It is worth noting that we prune the candidate antecedents of the event mentions not only in the test set but also in the training set . 
We produce the topkcandidate antecedents of each event mention in the training set via Ô¨Åve - fold cross - validation over the training documents . 
Figure 1 illustrates the unary factors , which encode the features used in the three independent models . 
SpeciÔ¨Åcally , the sentence fragment at the bottom of the Ô¨Ågure contains two event mentions , one triggered by leave and the other by departure . 
Each of them is associated with three variables , one for each of the three models . 
Next to each variable is the set of possible values of that variable . 
3.2 Joint Learning To perform joint training over the three models described in the previous subsection , we need to 4The discourse preprocessing module does not handle NULL candidate antecedents , so they will always be available to the event coreference model.657 Figure 2 : Binary and ternary factors . 
deÔ¨Åne ( 1 ) features that capture the interaction between the two tasks , ( 2 ) the joint training scheme , and ( 3 ) the inference mechanism . 
3.2.1 Cross - Task Interaction Features Our cross - task interaction features , which capture thepairwise interaction between our tasks , are associated with ternary factors , as described below . 
Trigger detection and coreference . 
We deÔ¨Åne our joint coreference and trigger detection factors such that the features deÔ¨Åned on subtype variables siandsjare Ô¨Åred only if current mention mjis coreferent with preceding mention mi . 
These features are : ( 1 ) the pair of miandmj ‚Äôs subtypes ; ( 2 ) the pair ofmj ‚Äôs subtype and mi ‚Äôs word ; and ( 3 ) the pair ofmi ‚Äôs subtype and mj ‚Äôs word . 
Trigger detection and topic modeling . 
We Ô¨Åre features ( encoded as binary factors ) that conjoin each candidate event mention ‚Äôs event subtype , its topic and the lemma of its trigger . 
Topic modeling and coreference . 
Our joint coreference and topic modeling factors and features are the same as those for trigger detection and coreference , except that event subtype labels are replaced with topic labels . 
In other words , the features are deÔ¨Åned on the topic labels . 
Figure 2 shows the cross - task interaction features . 
The green factor is binary , connecting a subtype variable and a topic variable . 
The red factor is ternary , connecting two subtype variables to a coreference variable . 
Finally , the blue factor is also ternary , connecting topic with coreference . 
3.2.2 Training The joint training scheme seeks to learn the model parameters Œòfrom a set of dtraining documents , where document icontains content xi , gold trigger annotations s‚àó i , topic labels t‚àó iinferred from the LabeledLDA model using Gibbs sampling , andgold event coreference partition C‚àó i , by maximizing the following conditional likelihood of the training data with L 1regularization:5 L(Œò ) = d / summationdisplay i=1log / summationdisplay c‚àó‚ààA(C‚àó i)p / prime(s‚àó i , t‚àó i , c‚àó|xi ; Œò ) + Œª / bardblŒò / bardbl1 wherep / prime(s‚àó,t‚àó,c‚àó|x ; Œò)isp(s‚àó,t‚àó,c‚àó|x ; Œò)augmented with task - speciÔ¨Åc loss functions . 
SpeciÔ¨Åcally , p / prime(s‚àó,t‚àó,c‚àó|x ; Œò)‚àùp(s‚àó,t‚àó,c‚àó|x ; Œò ) exp [ Œ±sls(s , s‚àó ) + Œ±tlt(t , t‚àó ) + Œ±clc(c , C‚àó ) ] wherels , ltandlcare task - speciÔ¨Åc loss functions6 , andŒ±s , Œ±tandŒ±care the associated weight parameters that specify the relative importance of the three tasks in the objective function.7We use AdaGrad ( Duchi et al . 
, 2011 ) to optimize our objective function with Œª= 0.001 . 
3.2.3 Inference Inference , which is performed during training and decoding , involves computing the marginals for a variable or a set of variables to which a factor connects . 
For efÔ¨Åciency , we perform approximate inference using belief propagation , running it until convergence . 
We use minimum Bayes risk decoding , where we compute the marginals for each variable in our model and independently return the most likely setting of each variable . 
Marginals typically converge in 3‚Äì5 iterations of belief propagation , so we use 5 iterations in our experiments . 
4 Evaluation 4.1 Experimental Setup We perform training and evaluation on the KBP 2017 English and Chinese corpora . 
For English , 5In the conditional log likelihood function , A(C‚àó i)is the set of antecedent structures that are consistent with C‚àó i. Since our model needs to be trained on antecedent vectors c‚àóbut the gold coreference annotation for each document iis provided in the form of a clustering C‚àó i , we need to sum over all consistent antecedent structures . 
6The loss function for event coreference , which is introduced by Durrett and Klein ( 2013 ) for entity coreference resolution , is a weighted sum of ( 1 ) the number of anaphoric mentions misclassiÔ¨Åed as non - anaphoric , ( 2 ) the number of non - anaphoric mentions misclassiÔ¨Åed as anaphoric , and ( 3 ) the number of incorrectly resolved mentions . 
The loss function for trigger detection is parameterized in a similar way , having three parameters associated with ( 1 ) the number of nontriggers misclassiÔ¨Åed as triggers , ( 2 ) the number of triggers misclassiÔ¨Åed as non - triggers , and ( 3 ) the number of triggers labeled with the wrong subtype . 
The loss function for topic detection is deÔ¨Åned in a similar way as trigger detection . 
7These weight parameters , as well as those that are used within the loss functions , are tuned on the development set using grid search.658 Event Coreference Trigger Detection English MUCB3CEAF eBLANC A VG - F ‚àÜ P R F ‚àÜ 1 Huang et al . 
( 2019 ) 35.7 43.2 40.0 32.4 36.8 56.8 46.4 51.1 2 Full 37.11 44.49 40.03 29.93 37.89 64.45 46.92 54.30 3‚àíTopic 34.16 43.76 40.78 28.20 36.72‚àí1.17 64.39 46.67 54.11‚àí0.19 4‚àíDiscourse 34.53 43.06 40.07 27.95 36.40‚àí1.49 62.15 47.49 53.84‚àí0.46 5‚àíBoth 31.94 42.84 40.21 26.49 35.37‚àí2.52 63.57 45.87 53.29‚àí0.89 Event Coreference Trigger Detection Chinese MUCB3CEAF eBLANC A VG - F ‚àÜ P R F ‚àÜ 6 Lu and Ng ( 2017b ) 27.07 34.18 32.22 18.57 28.01 46.61 46.91 46.76 7 Full 27.89 40.95 39.49 22.00 32.58 51.81 54.81 53.27 8‚àíTopic 26.39 40.43 38.75 21.18 31.69‚àí0.89 51.81 53.28 52.53‚àí0.74 9‚àíDiscourse 26.13 40.78 39.31 21.02 31.81‚àí0.77 51.65 54.65 53.11‚àí0.16 10‚àíBoth 25.93 37.50 34.24 19.92 29.40‚àí3.18 56.78 44.63 49.98‚àí3.29 Table 2 : Results of event coreference and trigger detection on the KBP 2017 English and Chinese test sets . 
Baseline results ( rows 1 and 6 ) are copied verbatim from the original papers . 
we train models on 646 of the training documents , tune parameters on 171 training documents , and report results on the ofÔ¨Åcial KBP 2017 English test set . 
For Chinese , we train models on 438 of the training documents , tune parameters on 110 training documents , and report results on the ofÔ¨Åcial KBP 2017 Chinese test set . 
Results of event coreference and trigger detection are obtained using version 1.8 of the ofÔ¨Åcial scorer provided by the KBP 2017 organizers . 
To evaluate event coreference performance , the scorer employs four commonly - used scoring measures , namely MUC ( Vilain et al . 
, 1995 ) , B3(Bagga and Baldwin , 1998 ) , CEAF e(Luo , 2005 ) and BLANC ( Recasens and Hovy , 2011 ) , as well as the unweighted average of their F - scores ( A VG - F ) . 
The scorer reports event mention detection performance in terms of Precision ( P ) , Recall ( R ) and F - score , considering a mention correctly detected if it has an exact match with a gold mention in terms of boundary and event subtype . 
4.2 Results Results on the English test set are shown in the top half of Table 2 . 
SpeciÔ¨Åcally , row 1 shows the results of Huang et al . 
‚Äôs ( 2019 ) resolver , which has produced best results to date on this test set . 
Row 2 shows the results of our full model , which substantially outperforms the baseline system ( row 1 ) , yielding an improvement of 1.09 points in A VG - F for event coreference and 3.2 points in F - score for trigger detection . 
Note that the improvement in the MUC and B3F - scores is largely offset by the precipitation in the BLANC F - score . 
Results on the Chinese test set are shown in the bottom half of Table 2 . 
SpeciÔ¨Åcally , row 6 shows the results of Lu and Ng ‚Äôs ( 2017b ) resolver , which is the top KBP 2017 system for Chinese and hasproduced the best results to date on this test set . 
Our full model ( row 7 ) outperforms this baseline by 4.57 points in A VG - F for event coreference and 6.51 points in F - score for trigger detection . 
Despite the large improvement in A VG - F , the MUC F - score only increases by 0.82 points . 
Since MUC F - scores are computed solely based on coreference links , these results suggest that the improvement in A VG - F can largely be attributed to successful identiÔ¨Åcation singleton clusters rather than successful identiÔ¨Åcation of coreference links . 
4.3 Model Ablations To evaluate the importance of each of the two extensions in the full model , we perform ablation experiments . 
Rows 3‚Äì5 and rows 8‚Äì10 in Table 2 show the English and Chinese results obtained using models that are retrained after one or both of the extensions are removed from the full model . 
The changes in A VG - F as a result of the ablations are shown in the ‚àÜcolumns for both tasks . 
Similar conclusions can be drawn from the ablation results for both languages . 
First , ablating each of the two extensions causes a drop in performance for both event coreference and trigger detection . 
These results suggest that topic modeling and discourse pruning are both useful for the two tasks . 
Second , ablating both extensions causes a more abrupt drop in performance than ablating one of the extensions . 
This implies that each extension is providing useful information for each task that can not be provided by the other extension . 
Third , when both extensions are ablated , the resulting models still outperform the baselines for both tasks . 
Nevertheless , we can see that for English , discourse pruning contributes more to the performance of our full model than topic modeling , whereas the reverse is true for Chinese.659 English Chinese Training Test Training Test 1 Number of candidate event mentions to be resolved 52370 9494 39758 9918 2 Number of candidate antecedents before pruning 371718 48750 124292 26406 3 Number of candidate antecedents after pruning 119416 20956 83378 20109 4 Number ( % ) of anaphoric event mentions4362 ( 8.3%)914 ( 9.6%)1713 ( 4.3%)821 ( 8.3 % ) 5Number ( % ) of anaphoric event mentions whose correct antecedent are among the candidates before pruning4317 ( 99.0%)803 ( 87.8%)1671 ( 97.6%)585 ( 71.3 % ) 6Number ( % ) of anaphoric event mentions whose correct antecedent are among the candidates after pruning3171 ( 72.7%)670 ( 73.3%)1610 ( 94.0%)565 ( 68.8 % ) Table 3 : Statistics on salience - based candidate pruning . 
4.4 Analysis of Salience - Based Pruning To gain insights into the effectiveness of discourse modeling in terms of pruning candidate antecedents , Table 3 shows some statistics on the candidate antecedents before and after applying pruning . 
Concretely , row 1 shows the total number of event mentions to be resolved in the English and Chinese training and test sets . 
For English , as we can see in rows 2‚Äì3 , only 32.1 % and 43.0 % of the candidate antecedents remain in the training and test sets respectively after pruning . 
This can be attributed to the fact that we aggressively prune the candidate antecedents by allowing k(the number of top candidate antecedents that can survive the pruning for each event mention ) to be in the range of 1 to 5 during parameter tuning.8Row 4 shows that among all event mentions to be resolved , only 8.3 % of them are anaphoric . 
Row 5 shows that before pruning , the correct antecedent of almost all of the anaphoric event mentions in the training set is among the set of candidate antecedents , whereas the corresponding number on the test set is only 87.8 % due to the presence of unseen event mentions . 
Row 6 shows that 72.7 % and 73.3 % of the correct antecedents on the training set and the test set survive the pruning , respectively . 
Similar trends can be observed for the Chinese datasets . 
Overall , these statistics shed light on why discourse - based pruning is beneÔ¨Åcial : the percentage of correct antecedents that survive the pruning is far greater than the percentage of candidate antecedents that are pruned . 
4.5 Discussion One thing that the reader may not be able to appreciate just by looking at the performance numbers in Table 2 is that our two extensions are starting to attack some of the non - trivial aspects of event 8The bestkaccording to the development set is 2 for English and 3 for Chinese.coreference that involve semantics and discourse , as opposed to those previous approaches that focus on low - level issues ( e.g. , string matching ) . 
For this reason , we will take a look at some of the errors addressed by our extensions below . 
Let us Ô¨Årst consider the kind of errors topic modeling allows us to address . 
Consider the Ô¨Årst two sentences in Table 4 , both of which contain the trigger candidate ‚Äú struck ‚Äù . 
While ‚Äú struck ‚Äù triggers a ‚Äú ConÔ¨Çict . 
Attack ‚Äù event in the Ô¨Årst sentence , neither of its occurrences in the second sentence corresponds to a true trigger ( and therefore their subtypes should both be NONE ) . 
Without topic modeling , the model predicts all occurrences of ‚Äú struck ‚Äù in these sentences as belonging to ConÔ¨Çict . 
Attack ( and hence misclassiÔ¨Åes the subtypes ofm2andm3 ) . 
The reasons are that ( 1 ) ‚Äú struck ‚Äù is most frequently associated with ‚Äú ConÔ¨Çict . 
Attack ‚Äù in the training data , and ( 2 ) since the two sentences have a similar syntactic structure and contain entities of the same type , the model fails to identify their differences . 
In contrast , with topic modeling , our model correctly predicts the topic of the document in which the second example appears as Contact . 
Meeting . 
Since the model manages to learn that the subtype of ‚Äú struck ‚Äù should be NONE when the topic is Contact . 
Meeting and that its subtype should be ‚Äú ConÔ¨Çict . 
Attack ‚Äù when the topic is ‚Äú ConÔ¨Çict . 
Attack ‚Äù , it correctly predicts m2and m3as having subtype NONE and , as a result , it also correctly determines that they are not coreferent . 
In other words , by using global information encoded by the topic model , our model can distinguish between words that have different meanings in different contexts . 
Next , consider the last example in Table 4 , which aims to give the reader an idea of the usefulness of discourse - based pruning . 
In this example , m4,m5 , andm8refer to the event of the French soldier being stabbed and are coreferent , whereas m6andm7660 A barrage of US missile { struck}m1Pakistan ‚Äôs North Waziristan tribal district on Tuesday , killing at least 15 militants . 
President Vladimir Putin sent his condolences to U.S. President Barack Obama on Tuesday over the deadly tornado that { struck}m2Oaklahoma City . 
The tornado { struck}m3the southern suburbs of the Oklahoma state capital Monday afternoon , killing at least 51 people and injuring at least 140 others . 
The French police said they were continuing to search for the man responsible for { stabbing}m4a uniformed soldier in the neck Saturday evening . 
The soldier was { stabbed}m5 in the back of the neck with a box cutter or short knife as he patrolled with two colleagues through the transport station of La D ¬¥ efense , a business area in a suburb of Paris . 
The police suggested that the deed may have been inspired by the { attack}m6on a British soldier in a London street Wednesday . 
A spokesman for the police union UNSA , Christophe Cr ¬¥ epin , said there were similarities with the London { attack}m7 . 
The case of the { wounded}m8soldier , Pfc . 
C ¬¥ edric Cordier , 23 , is being handled by France ‚Äôs anti - terrorism court , ofÔ¨Åcials said Sunday . 
Table 4 : Examples illustrating the usefulness of topic modeling and salience - based pruning . 
refer to the attack on the British solider and form another coreference cluster . 
Without discourse - based pruning , the model mistakenly links m8withm7 because they both have subtype ‚Äú ConÔ¨Çict . 
Attack ‚Äù . 
In contrast , discourse - based pruning ranks m4and m5higher than m6andm7inm8 ‚Äôs list of candidate antecedents , the reason being that m4,m5 , and m8share the same entity ( realized as ‚Äú a uniformed soldier ‚Äù , ‚Äú The soldier ‚Äù , and ‚Äú the wounded soldier ‚Äù ) in their contexts . 
Since the model retains only the top two candidate antecedents for English , m6and m7are being pruned , and the model successfully resolvesm8tom5 . 
5 Related Work Using topics and salience . 
For event coreference , the notion of ‚Äú topics ‚Äù has thus far been exploited only for cross - document event coreference , where documents are clustered by topics so that no cross - document coreference links can be established between documents in different clusters ( Lee et al . 
, 2012 ; Choubey and Huang , 2017 ) . 
These resolvers , unlike ours , are pipelined systems , meaning that topic detection can inÔ¨Çuence event coreference resolution but not the other way round . 
As for discourse salience , we are not aware of any event coreference work that attempts to explicitly model it , although one can argue that existing systems may have implicitly encoded it in a shallow manner via exploiting features that encode the distance between two event mentions ( Liu et al . 
, 2014 ; Cybulska and V ossen , 2015 ) . 
Computing argument compatibility . 
In addition to discourse - based pruning , candidate antecedents can be pruned based on how compatible the arguments of the two event mentions are . 
To capture argument compatibility , argument features have been extensively exploited . 
Basic features such as the number of overlapping arguments and the number of unique arguments , and a binary feature encoding whether arguments are conÔ¨Çictinghave been proposed ( Chen et al . 
, 2009 ; Chen and Ji , 2009 ; Chen and Ng , 2016 ) . 
More sophisticated features based on different kinds of similarity measures have also been considered , such as the surface similarity based on Dice coefÔ¨Åcient and the WuPalmer WordNet similarity between argument heads ( McConky et al . 
, 2012 ; Cybulska and V ossen , 2013 ; Araki et al . 
, 2014 ; Krause et al . 
, 2016 ) . 
These features are computed using either the outputs of event argument extractors and entity coreference resolvers ( Ahn , 2006 ; Chen and Ng , 2014 , 2015a ; Lu and Ng , 2016 ) or the outputs of semantic parsers ( Bejan and Harabagiu , 2014 ; Yang et al . 
, 2015 ; Peng et al . 
, 2016 ) , and therefore suffer from error propagation ( see Lu and Ng ( 2018 ) ) . 
Several previous works proposed joint models to address this problem ( Lee et al . 
, 2012 ; Lu et al . 
, 2016 ) , while others utilized iterative methods to propagate argument information ( Liu et al . 
, 2014 ; Choubey and Huang , 2017 ) in order to alleviate this issue . 
Nevertheless , argument extraction remains a very challenging task , especially when the arguments do not appear in the same sentence as the trigger . 
Our discourse - based pruning method can be thought of as a way of approximating argument compatibility without performing argument extraction . 
6 Conclusion We incorporated non - local information into a stateof - the - art joint model for event coreference resolution via topic modeling and discourse - based pruning . 
The resulting model not only signiÔ¨Åcantly outperforms the independent models but also achieves the best results to date on the KBP 2017 English and Chinese event coreference corpora . 
Acknowledgments We thank the three anonymous reviewers for their detailed and insightful comments on an earlier draft of the paper . 
This work was supported in part by NSF Grants IIS-1528037 and CCF-1848608.661 Abstract This paper evaluates the utility of Rhetorical Structure Theory ( RST ) trees and relations in discourse coherence evaluation . 
We show that incorporating silver - standard RST features can increase accuracy when classifying coherence . 
We demonstrate this through our tree - recursive neural model , namely RST - Recursive , which takes advantage of the text ‚Äôs RST features produced by a state of the art RST parser . 
We evaluate our approach on the Grammarly Corpus for Discourse Coherence ( GCDC ) and show that when ensembled with the current state of the art , we can achieve the new state of the art accuracy on this benchmark . 
Furthermore , when deployed alone , RST - Recursive achieves competitive accuracy while having 62 % fewer parameters . 
1 Introduction Discourse coherence has been the subject of much research in Computational Linguistics thanks to its widespread applications ( Lai and Tetreault , 2018 ) . 
Most current methods can be described as either stemming from explicit representations based on the Centering Theory ( Grosz et al . 
, 1994 ) , or deep learning approaches that learn without the use of hand - crafted linguistic features . 
Our work explores a third research avenue based on the Rhetorical Structure Theory ( RST ) ( Mann and Thompson , 1988 ) . 
We hypothesize that texts of low / high coherence tend to adhere to different discourse structures . 
Thus , we pose that using even silver - standard RST features should help in separating coherent texts from incoherent ones . 
This stems from the deÔ¨Ånition of the coherence itself as the writer of a document needs to follow speciÔ¨Åc rules for building a clear narrative or argument structure in which the role of each constituent of the document should be appropriate with respect ‚àóAuthors contributed equally œÉœÉœÉtanhxh_left , r_left œÉh_right , r_right c_leftc_rightxx+xtanhc_out h_outf_leftf_rightinputupdateoutput people think he is smartbecause he did well in schoolLSTMLSTM ENSYou know Mark , BNSLSTMTree LSTMTree LSTM ... R ......... RST TreeRST - RecursiveNN RST NetworkRST Networkh_lefth_rightFCSoftmax Classifier Right BranchLeft BranchCoherence EmbeddingCoherence Level EvidenceBackgroundFigure 1 : Overview of RST - Recursive ; EDU embeddings are generated for the leaf nodes using the EDU network . 
Subsequently , the RST tree is recursively traversed bottom - up using the RST network . 
to its local and global context , and even existing discourse parsers should be able to predict a plausible structure that is consistent across all coherent documents . 
However , if a parser has difÔ¨Åculty interpreting a given document , it will be more likely to produce unrealistic trees with improbable patterns of discourse relations between constituents . 
This idea was Ô¨Årst explored by Feng et al . 
( 2014 ) , who followed an approach similar to Barzilay and Lapata ( 2008 ) by estimating entity transition likelihoods , but instead using discourse relations ( predicted by a state of the art discourse parser ( Feng and Hirst , 2014 ) ) that entities participate in as opposed to their grammatical roles . 
Their method achieved signiÔ¨Åcant improvements in performance even when using silver - standard discourse trees , showing potential in the use of parsed RST features for classifying textual coherence . 
Our work , however , is the Ô¨Årst to develop and test a neural approach to leveraging RST discourse representations in coherence evaluation . 
Furthermore , Feng et al . 
( 2014 ) only tested their proposal on the664 œÉœÉœÉtanhxœÉc_leftc_rightxx+tanhc_out h_outf_leftf_rightinputupdateoutput people think he is smartbecause he did well in schoolLSTMLSTM ENSYou know Mark , BNSLSTMTree LSTMTree LSTM ... R ......... RST TreeRST - RecursiveNN RST NetworkRST Networkh_lefth_rightFCSoftmax Classifier Right BranchLeft BranchCoherence EmbeddingCoherence Level EvidenceBackgroundh_left , r_left h_right , r_right xFigure 2 : Recursive LSTM architecture used in RSTRecursive adapted from ( Tai et al . 
, 2015 ) . 
sentence permutation task , which involves ranking a sentence - permuted text against the original . 
As noted by Lai and Tetreault ( 2018 ) , this is not an accurate proxy for realistic coherence evaluation . 
We evaluate our method on their more realistic Grammarly Corpus Of Discourse Coherence ( GCDC ) , where the model needs to classify a naturally produced text into one of three levels of coherence . 
Our contributions involve : ( 1)RST - Recursive , an RST - based neural tree - recursive method for coherence evaluation that achieves 2 % below the state of the art performance on the GCDC while having 62 % fewer parameters . 
( 2)When ensembled with the current state of the art , namely Parseq ( Lai and Tetreault , 2018 ) , we achieve a notable improvement over the plain ParSeq model . 
( 3)We demonstrate the usefulness of silver - standard RST features in coherence classiÔ¨Åcation , and establish our results as a lower - bound for performance improvements to be gained using RST features . 
2 Related Work 2.1 Coherence Evaluation of Text Centering Theory ( Grosz et al . 
, 1994 ) states that subsequent sentences in coherent texts are likely to continue to focus on the same entities ( i.e. , subjects , objects , etc . 
) as within the previous sentences . 
Building on top of this , Barzilay and Lapata ( 2008 ) were the Ô¨Årst to propose the Entity - Grid model that constructs a two - dimensional array Gn , mfor a text ofnsentences and mentities , which are used to estimate transition probabilities for entity occurrence patterns . 
More recently , Elsner and Charniak ( 2011 ) extended Entity - Grid using entity - speciÔ¨Åc features , while Tien Nguyen and Joty ( 2017 ) used a Convolutional Neural Network ( CNN ) on top of Entity - Grid to learn more hierarchical patterns . 
On the other hand , feature - free deep neural techniques have dominated recent research . 
Li and Jurafsky ( 2017 ) applied Recurrent Neural Networks ( RNNs ) to model the coherent generation of the œÉœÉœÉtanhxh_leftXœÉh_rightc_leftc_rightxx+xtanhc_out h_outf_leftf_rightinputupdateoutput people think he is smartbecause he did well in schoolEDU NetworkEDU Network ENSYou know Mark , BNSEDU NetworkRST NetworkRST Network ... R ......... RST TreeRST - RecursiveNN RST NetworkRST Networkh_lefth_rightFCSoftmax Classifier Right BranchLeft BranchCoherence EmbeddingCoherence LevelFigure 3 : Overview of the classiÔ¨Åcation layer in RSTRecursive ; At the root of the RST tree , children ‚Äôs hidden states are concatenated to form the document representation d= [ hl , hr]which is then transformed into a 3 - dimensional vector of Softmax probabilities . 
next sentence given the current sentence and viceversa . 
Mesgar and Strube ( 2018 ) constructed a local coherence model that encodes patterns of changes on how adjacent sentences within the text are semantically related . 
Recently , Moon et al . 
( 2019 ) used a multi - component model to capture both local and global coherence perturbations . 
Lai and Tetreault ( 2018 ) developed a hierarchical neural architecture named ParSeq with three stacked LSTM Networks , designed to encode the coherence at sentence , paragraph and document levels . 
2.2 Rhetorical Structure Theory ( RST ) RST describes the structure of a text in the following way : Ô¨Årst , the text is segmented into elementary discourse units ( EDUs ) , which describe spans of text constituting clauses or clause - like units ( Mann and Thompson , 1988 ) . 
Second , the EDUs are recursively structured into a tree hierarchy where each node deÔ¨Ånes an RST relation between the constituting sub - trees . 
The sub - tree with the central purpose is called the nucleus , and the one bearing secondary intent is called the satellite while a connective discourse relation is assigned to both . 
An example of a ‚Äú nucleus - satellite ‚Äù relation pairing is presented in Figure 1 where a claim is followed by the evidence for the claim ; RST posits an ‚Äú Evidence ‚Äù relation between these two spans with the left sub - tree being the ‚Äú nucleus ‚Äù and the right sub - tree as ‚Äú satellite ‚Äù . 
3 Method 3.1 RST - Recursive We parse silver - standard RST trees for documents using the CODRA ( Joty et al . 
, 2015 ) RST parser , which we then employ as input to our recursive neural model , RST - Recursive . 
The overall procedure665 for RST - Recursive is shown in Figure 1 . 
Given a document of nEDUsE1 : nwith each EDUEirepresented as a list of GloVe embeddings ( Pennington et al . 
, 2014 ) , we use an LSTM to process each Ei , using the Ô¨Ånal hidden state as the EDU embedding ei = LSTM ( Ei)for each leaf iof the document ‚Äôs RST tree . 
Afterwards , we apply a recursive LSTM architecture ( Figure 2 ) that traverses the RST tree bottom - up . 
At each node s , we use the children ‚Äôs sub - tree embeddings [ hl , cl , rl]and[hr , cr , rr]to form the node ‚Äôs sub - tree embedding : [ hs , cs ] = TreeLSTM ( [ hl , cl , rl],[hr , cr , rr ] ) ( 1 ) where hl / clandhr / crare the LSTM hidden and cell states from the left and right sub - trees respectively . 
The relation embeddings of the children sub - trees , rlandrr , are learned vector embeddings for each of the 31 pre - deÔ¨Åned relation labels in the form of ‚Äú [ relation ] [ nucleus / satellite ] ‚Äù ( e.g. , ‚Äú Evidence Satellite ‚Äù for the last EDU in Figure 1 ) . 
At the root of the tree , the output hidden states from both children are concatenated into a single document embedding d= [ hl , hr ] . 
As shown in Figure 3 , a fully connected layer is applied to this representation before using a Softmax function to obtain the coherence class probabilities . 
3.2 Ensemble : ParSeq + RST - Recursive To evaluate if the addition of silver - standard RST features to existing methods can improve coherence evaluation , we ensemble RST - Recursive with the current state of the art coherence classiÔ¨Åer : ParSeq . 
A deep learned non - linguistic classiÔ¨Åer , ParSeq employs three layers of LSTMs that intend to capture coherence at different granularities . 
An overview of the ParSeq architecture is presented in Figure 4 . 
First , LSTM 1(not shown ) produces a single sentence embedding for each sentence in the text . 
Next , LSTM 2generates paragraph embeddings using the corresponding sentence embeddings from LSTM 1 . 
Finally , LSTM 3reads the paragraph embeddings , generating the Ô¨Ånal document embedding , which is passed to a fully connected layer to produce Softmax label probabilities . 
In this augmented variation of our model , we operate ParSeq on the document independently until a document level embedding dpis obtained at the highest - level LSTM . 
This document embedding is then concatenated to the RST - Recursive coherence embedding d= [ hl , hr , dparseq ] in Figure Figure 4 : The architectural overview of ParSeq ; an illustration of ParSeq ‚Äôs structure , taken directly from the original paper ( Lai and Tetreault , 2018 ) . 
3 to produce class probabilities . 
Note that in this ensemble variation , we initialize tree leaves e1 : n with zero - vectors as opposed to EDU embeddings since ParSeq is sufÔ¨Åciently capable of capturing semantic information on its own , and early experiments using 5 - fold cross - validation on the training set revealed model overÔ¨Åtting when training with EDU embeddings simultaneously . 
4 Experiments 4.1 Dataset We evaluate RST - Recursive and Ensemble on the GCDC dataset ( Lai and Tetreault , 2018 ) . 
This dataset consists of 4 separate sub - datasets : Clinton emails , Enron emails , Yahoo answers , and Yelp reviews , each containing 1000 documents for training and 200 documents for testing . 
Each document is assigned a discrete coherence label of incoherent ( 1 ) , neutral ( 2 ) , and coherent ( 3 ) . 
We parse RST trees for each example within the GCDC dataset using CODRA ( Joty et al . 
, 2015 ) . 
Due to CODRA ‚Äôs imperfect parsing of documents , RST trees could not be obtained for approximately 1.5%-2 % of the documents , which were then excluded from the study . 
In addition , we re - evaluated ParSeq on only the RST - parsed portion of documents to assure consistent comparability of results . 
For more details , see Appendix A / B. Our code and dataset can be accessed below1 , and the access to the original GCDC corpus can be obtained here2 . 
We can share RST - parsings of GCDC examples with interested readers upon request once access to the GCDC dataset has also been obtained . 
1https://github.com/grig-guz/coherence-rst 2https://github.com/aylai/GCDC-corpus666 MODEL T NS R E CLINTON ENRON YAHOO YELP AVERAGE MAJORITY 55.33 44.39 38.02 54.82 48.14 RST - R EC / check 55.33¬±0.00 44.39¬±0.00 38.02¬±0.00 54.82¬±0.00 48.14¬±0.00 RST - R EC / check /check 53.74¬±0.14 44.67¬±0.07 44.61¬±0.09 53.76¬±0.11 49.20¬±0.07 RST - R EC / check /check /check 54.07¬±0.10 43.99¬±0.07 49.39¬±0.10 54.39¬±0.12 50.46¬±0.05 RST - R EC / check /check /check /check 55.70¬±0.08 53.86¬±0.11 50.92¬±0.13 51.70¬±0.16 53.04¬±0.09 PARSEQ 61.05¬±0.13 54.23¬±0.10 53.29¬±0.14 51.76¬±0.21 55.09¬±0.09 ENSEMBLE /check * 61.12¬±0.13 54.20¬±0.12 52.87¬±0.16 51.52¬±0.22 54.93¬±0.10 ENSEMBLE /check /check * 60.82¬±0.13 54.01¬±0.10 52.92¬±0.15 51.63¬±0.24 54.85¬±0.10 ENSEMBLE /check /check /check * 61.17¬±0.12 53.99¬±0.10 53.99¬±0.14 52.40¬±0.21 55.39¬±0.09 Table 1 : Overall and sub - dataset speciÔ¨Åc coherence classiÔ¨Åcation accuracy on the GCDC dataset . 
Error boundaries describe 95 % conÔ¨Ådence intervals . 
Values in bold describe statistically signiÔ¨Åcant state of the art performance . 
* indicates availability of EDU - level semantic information through the ensembling with ParSeq . 
MODEL T NS R E CLINTON ENRON YAHOO YELP AVERAGE MAJORITY 39.42 27.29 20.95 38.82 31.62 RST - R EC / check 39.42¬±0.00 27.29¬±0.00 20.95¬±0.00 38.82¬±0.00 31.62¬±0.00 RST - R EC / check /check 39.20¬±0.03 30.81¬±0.16 35.67¬±0.18 39.93¬±0.08 36.40¬±0.09 RST - R EC / check /check /check 41.08¬±0.07 31.21¬±0.13 41.97¬±0.14 42.27¬±0.09 39.13¬±0.08 RST - R EC / check /check /check /check 45.90¬±0.12 44.33¬±0.16 43.85¬±0.18 43.13¬±0.10 44.30¬±0.08 PARSEQ 52.12¬±0.21 44.90¬±0.15 46.22¬±0.18 43.36¬±0.09 46.65¬±0.10 ENSEMBLE /check * 52.35¬±0.22 44.92¬±0.16 45.48¬±0.22 43.70¬±0.11 46.61¬±0.11 ENSEMBLE /check /check * 51.90¬±0.22 44.76¬±0.14 45.48¬±0.22 43.83¬±0.13 46.49¬±0.10 ENSEMBLE /check /check /check * 52.42¬±0.19 44.69¬±0.15 46.88¬±0.17 43.94¬±0.09 46.98¬±0.09 Table 2 : Overall and sub - dataset speciÔ¨Åc coherence classiÔ¨Åcation F1 scores on the GCDC dataset . 
Error boundaries describe 95 % conÔ¨Ådence intervals . 
Values in bold describe statistically signiÔ¨Åcant state of the art performance . 
F1 scores are calculated by macro - averaging the corresponding class - wise F1 scores . 
* indicates availability of EDUlevel semantic information through the ensembling with ParSeq . 
4.2 Training We train all models with hyperparameter settings consistent with that of ParSeq reported by ( Lai and Tetreault , 2018 ) . 
SpeciÔ¨Åcally , we use a learning rate of 0.0001 , hidden size of 100 , relation embedding size of 50 , and 300 - dimensional pre - trained GloVe embeddings ( Pennington et al . 
, 2014 ) . 
We train with the Adam optimizer ( Kingma and Ba , 2014 ) for 2 epochs . 
For every model / variation , the reported results represent the corresponding accuracies and F1 scores averaged over 1000 independent runs , each initialized with a different random seed . 
4.3 RST - Recursive ‚Äôs Performance Our full model incorporates the RST Tree ( T ) structure , nucleus / satellite properties ( nuclearity ) of subtrees ( NS ) , RST speciÔ¨Åc connective relations ( R ) , and EDU embeddings at leaves of the RST tree ( E),as previously described in 3.1 . 
Here , ( T ) deÔ¨Ånes the tree traversal operation and ( NS ) and ( R ) are learned vector embeddings for nuclearity and relations . 
We examine three ablations , each removing one of ( NS ) , ( R ) and ( E ) from the model . 
The results are provided in Tables 1 and 2 . 
As shown , the complete model is able to achieve a competitive overall accuracy and F1 at 53.04 % and 44.30 % respectively , which is close to the state of the art . 
Although this lags behind ParSeq by a noticeable 2 % margin , RST - Recursive is able to achieve this performance with 62 % fewer parameters ( 1,230k vs. 3,241k ) , demonstrating the usefulness of linguistically - motivated features . 
Removing EDU embeddings reduces accuracy and F1 scores to 50.46 % and 39.13 % . 
This is still signiÔ¨Åcantly better than the majority class baseline , signifying that even without any semantic infor-667 Recall Precision F1 Recall Precision F1 Recall Precision F1 0.000.250.500.751.00 Incoherent Incoherent Incoherent Neutral Neutral Neutral Coherent Coherent CoherentParSeq + RST - Recursive ( w / out E ) ParSeq RST - Recursive ( w / out E)Figure 5 : Comparison of Recall , Precision and F1 on overall classiÔ¨Åcation of each coherence level . 
mation about the text and its contents , it is still possible to evaluate coherence using just the silverstandard RST features of the text . 
Removing RST relations and nuclearity , however , decreases performance substantially , dropping to the majority class level . 
This indicates that an RST tree structure alone ( of the quality delivered by silver - standard parsers ) is not sufÔ¨Åcient to classify coherence . 
It must also be noted that since we employ silverstandard RST parsing as performed by CODRA ( Joty et al . 
, 2015 ) , the reported results act as a lower bound which we would expect to improve as parsing quality increases . 
4.4 Ensemble ‚Äôs Performance We examine three variations of the Ensemble . 
The full model augments ParSeq with the text ‚Äôs RST tree , relations and nuclearity . 
This model is able to achieve the new state of the art performance , at 55.39 % accuracy and 46.98 % F1 . 
Using Ô¨Ånal layer concatenation for ensembling is widely applicable to many other neural methods , and serves as a lower bound for the accuracy / F1 boost to be appreciated by incorporating RST features into the model . 
Removing the RST relations and/or nuclearity information completely eliminates the performance gain , which shows that the RST tree on its own is not sufÔ¨Åcient as an RST source of information for distinguishing coherence , even when ensembled with ParSeq . 
4.5 ClassiÔ¨Åcation Trends As demonstrated in Figure 5 , coherence classiÔ¨Åers have difÔ¨Åculty predicting the neutral class ( 2 ) , experiencing modal collapse towards the extreme ends in the best performing models . 
Early experiments using alternative objective functions such as the Ordinal Loss or Mean Squared Error resulted in a similar modal collapse or poor overall performance . 
We leave further exploration of this problem to future research . 
Furthermore , RST - Recursive shows a notably stronger recall on the coherent class ( 3 ) as compared to ParSeq . 
On the other hand , ParSeq has a higher recall / precision on class ( 1 ) and slightly higher precision on class ( 3 ) . 
The Ensemble method , however , is able to take the best of both , achieving better recall , precision and F1 on both the incoherent and coherent classes as compared to ParSeq . 
5 Conclusions and Future Work In this paper , we explore the usefulness of silverstandard parsed RST features in neural coherence classiÔ¨Åcation . 
We propose two new methods , RSTRecursive and Ensemble . 
The former achieves reasonably good performance , only 2 % short of state of the art , while more robust with 62 % fewer parameters . 
The latter demonstrates the added advantage of RST features in improving classiÔ¨Åcation accuracy of the existing state of the art methods by setting new state of the art performance with a modest but promising margin . 
This signiÔ¨Åes that the document ‚Äôs rhetorical structure is an important aspect of its perceived clarity . 
Naturally , this improvement in performance is bounded by the quality of parsed RST features and could increase as better discourse parsers are developed . 
In the future , exploring other RST - based architectures for coherence classiÔ¨Åcation , as well as better RST ensemble schemes and improving RST parsing can be avenues of potentially fruitful research . 
Additional research on multipronged approaches that draw from Centering Theory , RST and deep learning all together can also be of value . 
Abstract Large - scale natural language inference ( NLI ) datasets such as SNLI or MNLI have been created by asking crowdworkers to read a premise and write three new hypotheses , one for each possible semantic relationships ( entailment , contradiction , and neutral ) . 
While this protocol has been used to create useful benchmark data , it remains unclear whether the writing - based annotation protocol is optimal for any purpose , since it has not been evaluated directly . 
Furthermore , there is ample evidence that crowdworker writing can introduce artifacts in the data . 
We investigate two alternative protocols which automatically create candidate ( premise , hypothesis ) pairs for annotators to label . 
Using these protocols and a writing - based baseline , we collect several new English NLI datasets of over 3k examples each , each using a Ô¨Åxed amount of annotator time , but a varying number of examples to Ô¨Åt that time budget . 
Our experiments on NLI and transfer learning show negative results : None of the alternative protocols outperforms the baseline in evaluations of generalization within NLI or on transfer to outside target tasks . 
We conclude that crowdworker writing still the best known option for entailment data , highlighting the need for further data collection work to focus on improving writing - based annotation processes . 
1 Introduction Research on natural language understanding has beneÔ¨Åted greatly from the availability of largescale , annotated data , especially for tasks like reading comprehension and natural language inference , which lend themselves to non - expert crowdsourcing . 
These datasets are useful in three settings : evaluation ( Williams et al . 
, 2018 ; Rajpurkar et al . 
, 2018 ; Zellers et al . 
, 2019 ) ; pretraining ( Phang et al . 
, 2018 ; Conneau et al . 
, 2018 ; Pruksachatkun et al . 
,2020 ) ; and as training data for downstream tasks ( Trivedi et al . 
, 2019 ; Portelli et al . 
, 2020 ) . 
Natural language inference ( NLI ) , also known as recognizing textual entailment ( RTE ; Dagan et al . 
, 2005 ) is the problem of determining whether or not a hypothesis semantically entails a premise . 
The two largest NLI corpora , SNLI ( Bowman et al . 
, 2015 ) and MNLI ( Williams et al . 
, 2018 ) are created by asking crowdworkers to write three labeled hypothesis sentences given a premise sentence taken from a preexisting text corpus . 
While these datasets have been widely used as benchmarks for NLU , there have been no studies evaluating writing - based annotation for collecting NLI data . 
Moreover , there is growing evidence that human writing can introduce annotation artifacts , which enable models to perform moderately well just by learning spurious statistical patterns in the data ( Gururangan et al . 
, 2018 ; Tsuchiya , 2018 ; Poliak et al . 
, 2018a ) . 
This paper explores the possibility of collecting high - quality NLI data without asking crowdworkers to write hypotheses . 
We introduce two alternative protocols ( Figure 1 ) which substitute crowdworker writing with fully - automated pipelines to generate premise - hypothesis sentence pairs , which annotators then simply label . 
The Ô¨Årst protocol uses a sentence - similarity - based method to pair similar sentences from large unannotated corpora . 
The second protocol uses parallel sentences and uses machine translation systems to generate sentence pairs . 
Using the MNLI protocol as our baseline , we collect Ô¨Åve datasets using premises taken from Gigaword news text ( Parker et al . 
, 2011 ) and Wikipedia . 
We then compare models trained using these datasets for their generalization performance within NLI and for transfer learning to other tasks . 
We start from the assumption that writing a new hypothesis takes more time and effort than simply labeling a presented hypothesis . 
As a result , it is plausible that our protocols could offer some value672 Similarity Retrieval   Unstructured Source Text   ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì , ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì . 
  ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì . 
‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ...    ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì . 
‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì . 
‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì . 
  Unlabeled Sentence Pairs   ( ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì , ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì . 
, ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì . 
) ( ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì , ‚Äì ‚Äì ‚Äì ‚Äì , ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì . 
, ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì , ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì . 
) Use FAISS and FastText to pair - up   similar sentences . 
  Crowdworker Labeling   P : ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì , ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì . 
, H : ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì . 
  ‚ö™ entailment     ‚ö™ neutral     ‚ö™ contradiction Use a tuned automatic Ô¨Ålter to identify a   diverse set of pairs to annotate . 
  ? ?    ? ? MNLI - Style Baseline   Unstructured Source Text   ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì , ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì . 
  ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì . 
‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ...   ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì . 
‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì . 
‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì . 
  Sample individual sentences to annotate . 
  Crowdworker Writing   P : ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì , ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì . 
   entailment :   contr adiction :   neutr al : | ? ?    ? ? Translation   Unlabeled Sentence Pairs   ( ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì , ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì . 
, ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì . 
) ( ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì , ‚Äì ‚Äì ‚Äì ‚Äì , ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì . 
, ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì , ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì . 
) Identify pairs of similar sentences from   existing bilingual comparable corpora . 
  Translate the non - English sentence to   English automatically . 
  Crowdworker Labeling   P : ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì , ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì . 
, H : ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì . 
  ‚ö™ entailment     ‚ö™ neutral     ‚ö™ contradiction Use a tuned automatic Ô¨Ålter to identify a   diverse set of pairs to annotate . 
  ? ?    ? ? Aligned Bilingual Text   Eng . 
:   ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì , ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì . 
  ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì . 
  Êó•Êú¨Ë™û : ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì . 
‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì ‚Äì . 
  Using the sampled sentence as a   premise , collect a matching hypothesis   for each label . 
  Collect a label for each pair . 
  Collect a label for each pair . 
Figure 1 : We introduce two new protocols for natural language inference data collection . 
Both use fully - automated pipelines to generate pairs of semantically - related sentences , which crowdworker annotators then label . 
even if the quality of the data they produce is no better than a writing - based baseline . 
To study the cost trade - off , we collect each dataset under the same Ô¨Åxed annotation budget with a Ô¨Åxed ( ‚àºUS $ 15 ) hourly wage . 
Using this constraint , we collect approximately twice as many examples from our new protocols . 
Our main results on natural language inference and transfer learning are clearly negative . 
Humanconstructed examples appear to be far superior to automatically - constructed examples in both settings . 
While crowdworker writing in data collection has known issues , it produces better training data than our automatic methods , or any known comparable methods which intervene the writingbased protocol to help crowdworkers with the writing process ( Bowman et al . 
, 2020 ) . 
This strongly suggests that future work on data quality should focus on improving human - based generation processes . 
2 Collecting NLI Data We compare three protocols for collecting NLI data : ( 1 ) a baseline MNLI - style protocol ( BASE ) , ( 2 ) a sentence - similarity - based protocol ( SIM ) , and ( 3 ) a translation - based protocol ( TRANSLATE ) . 
To test generalization performance across domains , we collect two datasets for BASE andSIM , using text from Gigaword ( news ) and Wikipedia ( wiki ) domains.1ForTRANSLATE , we collect a dataset from WikiMatrix ( Schwenk et al . 
, 2019 ) , a collection of Wikipedia parallel sentences . 
Table 1 shows examples of sentence pairs collected using 1The premise sentences for each protocol can be different although they come from the same source.each protocol . 
Our new protocols ( Figure 1 ) share a similar automated pipeline . 
Given an unstructured text , we automatically collect similar sentence pairs which annotators then label . 
There are two key differences between our new protocols and BASE . 
First , our automatically paired sentences are unlabeled , and thus require a further data labeling process ( Section 2.4 ) . 
Second , our protocols might produce datasets with imbalanced label distributions . 
This is in contrast to BASE , which ensures each premise will have one hypothesis for each label in the annotation . 
The following subsections describe each protocol in more detail . 
2.1 Baseline ( BASE ) Our BASE protocol closely follows that used for MNLI . 
We randomly sample premise sentences from Gigaword and Wikipedia and ask crowdworkers to write three new hypotheses , one for each relation type.2 2.2 Sentence Similarity ( SIM ) Our SIMprotocol exploits the fact that , in large corpora , it should be easy to Ô¨Ånd pairs of sentences that describe similar events or situations . 
For example , in Gigaword , one event might be written differently by different news sources in ways that yield any of our three relationships . 
We collect similar sentences and automatically match them to form sentence pairs which annotators then label . 
The whole pipeline consists of three steps : 2Our instructions can be found in the Appendix A , and our FAQs are available at https://sites.google.com/ nyu.edu/nlu-mturk-faq/writing-sentences .673 Dataset Label Premise Hypothesis Base - News E The city reconsidered that position on Wednesday , saying it was seeking to raise an additional $ 1.5 million to extend Mardi Gras over two weekends and to pay for overtime on several days . 
The city is looking to get more money for Mardi Gras . 
Base - Wiki C Service books were not included and a note at the end mentions many other books in French , English and Latin which were then considered worthless . 
Service books were included . 
Sim - News N All of them run out like college football players before a big bowl game . 
Pray before a college football game . 
Sim - Wiki C His work was heavily criticised as unscientiÔ¨Åc by his contemporaries . 
His work was recognized and admired by his contemporaries . 
Translate - Wiki E This was used to indicate a positive response , or truth , or approval of the item in front of it . 
This was used to indicate yes , true , or conÔ¨Årmed on items in a list . 
Table 1 : Examples of sentence pairs chosen randomly from each test set , along with their assigned labels . 
E : entailment , C : contradiction , N : neutral . 
indexing and retrieval , reranking , and crowdworker labeling . 
Indexing and Retrieval Given a raw text , we Ô¨Årst split it into sentences.3We encode each sentence as a 300 - dimensional vector using fastText ( Bojanowski et al . 
, 2017 ) and index them using FAISS ( Johnson et al . 
, 2019 ) , an open - source library for large - scale similarity search on vectors.4 Since Gigaword and Wikipedia consist of billions of sentences , we perform dimensionality reduction using PCA and cluster the search space to allow efÔ¨Åcient index and retrieval . 
We randomly sample query sentences from the text corpus and retrieve the top 1k most similar sentences for each query . 
This is done by building an index with type " PCAR64,IVFx , Flat " in FAISS terms , where xvaries depending on the corpus size . 
Details of our indexing and retrieval procedures can be found in Appendix A.1 . 
Reranking FastText uses a Continuous Bag - ofWords ( CBoW ) model to learn word representations . 
This means given a query , we will sometimes have top matches which are syntactically similar but describe different events or situations . 
While unrelated sentences can be contradictory or neutral , directly using the top- nsentences from FAISS will give us too few entailment pairs . 
Furthermore , because we use randomly sampled sentences as queries , there could be no good match at all for a given query . 
3We use Spacy ‚Äôs " encore weblg " model to segment sentences and extract noun phrase and entities for later use in reranking . 
4https://github.com/facebookresearch/ faissTo collect a set of sentence pairs with a reasonable label distribution , for each query , we retrieve top - Kmatches and rerank the ( query , retrieved sentence ) pairs using the following features : ‚Ä¢FAISS similarity score : The raw similarity score from FAISS . 
‚Ä¢Word types : The proportion of word types in the query sentence seen in the retrieved sentence . 
‚Ä¢Noun phrase : The proportion of noun phrases in the query sentence seen in the retrieved sentence . 
‚Ä¢Subjects : The proportion of complete subject spans ( some sentences with embedded clauses can have more than one subject ) in the query sentence seen in the retrieved sentence . 
‚Ä¢Named entity : The proportion of named entities in the query sentence seen in the retrieved sentence . 
‚Ä¢Time : A boolean feature which denotes whether two sentences are written in the same month and year ( only for Gigaword ) ‚Ä¢Wiki article : A boolean feature which denotes whether the pairs come from the same article . 
( only for Wikipedia ) ‚Ä¢Wiki link : The proportion of hyperlink tokens in the query sentence seen in the retrieved sentence ( only for Wikipedia ) The choice of these hand - crafted features will likely impact the distribution of our Ô¨Ånal dataset , but we674 do n‚Äôt expect these choices to inject signiÔ¨Åcant labelassociation artifacts , since our methods play no role in setting labels . 
We calculate the score for each sentence pair using a weighted sum of these features . 
We populate pairs from all queries and sort them based on their feature scores . 
We then select the top N% pairs as our Ô¨Ånal pairs . 
We use a Bayesian hyperparameter optimization to tune the feature weights , K , andN. In an ideal case , we want our dataset to have a balanced distribution so that all classes will be represented equally . 
To push for this , we tune these parameters to minimize the Kullback ‚Äì Leibler ( KL ) divergence between a uniform distribution across three entailment classes , P(x ) , and an empirical distribution , Q(x ) , computed based on the predictions of an NLI model . 
We run Bayesian optimization for 100 iterations using Optuna ( Akiba et al . 
, 2019 ) . 
For the NLI model , we use a RoBERTa Large model Ô¨Åne - tuned on a combination of SNLI , MNLI , and ANLI . 
2.3 Translation ( TRANSLATE ) Multilingual comparable corpora contain similar texts in at least two different languages . 
If they are sentence - aligned , we can automatically translate text from one language to one of the others to yield candidate sentence pairs . 
Since the alignment behind the corpus can be noisy , the resulting sentence pairs range almost continuously from being parallel to being semantically unrelated , potentially Ô¨Åtting any of the three entailment relationships . 
In the TRANSLATE protocol , we investigate whether we can use such sentence pairs as entailment data . 
We use WikiMatrix ( Schwenk et al . 
, 2019 ) , a collection of 135 million Wikipedia parallel sentences , which was constructed by aligning similar sentences in different languages in a joint sentence embedding space ( Schwenk , 2018 ; Artetxe and Schwenk , 2019 ) . 
It is a mix of translated sentence pairs and comparable sentences written independently about the same information . 
We collect parallel sentences where one of the sentences is in English , sE. For the paired non - English languages , we pick 5 languages : German , French , Indonesian , Japanese , and Czech . 
We then translate the aligned non - English sentence into an English sentence , sÀÜEusing the OPUS - MT ( Tiedemann and Thottingal , 2020 ) machine translation systems , and treat ( sE , sÀÜE)as a sentence pair . 
The diverse set of languages allows us to collect a more diverse setIndividual = = Gold No Gold Label MNLI ( Full ) 88.7 % 1.8 % Base - News 78.7 % 13.1 % Base - Wiki 76.4 % 10.0 % Sim - News 72.9 % 15.8 % Sim - Wiki 74.1 % 11.9 % Translate - Wiki 72.8 % 14.6 % Table 2 : Validation statistics for each protocol , compared to MNLI Full . 
of sentence pairs coming from the structural differences across languages . 
We do not perform any reranking as our predictions using an NLI model on the initially retrieved data ( the same one that we used in¬ß2.2 ) shows a near - balanced distribution . 
2.4 Data Labeling We use Amazon Mechanical Turk to label the automatically - collected sentence pairs ( SIMand TRANSLATE ) . 
We hire crowdworkers which have completed at least 5000 HITs with at least a 99 % acceptance rate . 
In each task , we present crowdworkers with a sentence pair and ask them to provide a single label ( entailment , contradiction , neutral or‚ÄúI do n‚Äôt understand ‚Äù ) for the pair . 
The latter is used if there are problems with either sentence , e.g. , because of errors during preprocessing . 
We collect one label per sentence pair . 
We use the same HIT setup for validating our test sets ( Section 3 ) . 
3 The Resulting Datasets Using BASE , we collect 3k examples for BaseNews and Base - Wiki.5ForSIMandTRANSLATE , we increase the number of pairs to exhaust the same budget that was used for the corresponding baseline dataset ( $ 1,791 for Base - News and $ 1,445 for Base - Wiki ) , allowing us to collect around twice as many examples for each protocol.6 For each dataset , we randomly select 250 sentence pairs as the test set and use the rest as the training set . 
To ensure accurate labeling , we perform an additional round of annotation on the test sets . 
We ask four crowdworkers to label each pair using the same instructions that we use for data labeling , giving us a total of 5 annotations per example . 
We assign the majority vote as the gold 5Our preliminary experiments on subsets of MNLI show that RoBERTa performance starts to stabilize once we use at least 3k training examples . 
6The resulting datasets are available at https:// github.com/nyu-mll/semi-automatic-nli . 
We provide anonymized worker - ids.675 # Pairs Label Distribution HL E HL C HL N Word Type Overlap E C N ¬µ ( œÉ ) ¬µ ( œÉ ) ¬µ ( œÉ ) E C NTrainingMNLI-3k 2750 33.4 33.9 32.7 9.7 4.4 9.4 4.0 11.0 4.4 25.2 17.3 15.4 Base - News 2734 33.5 33.4 33.2 12.1 6.0 11.8 5.8 12.4 6.2 23.5 18.4 18.1 Base - Wiki 2740 33.3 33.7 33.0 11.1 7.7 10.5 4.5 11.6 7.1 31.2 23.4 22.7 Sim - News 6627 21.8 39.1 39.2 23.2 9.7 22.7 10.0 23.3 9.9 46.6 21.8 23.0 Sim - Wiki 6174 23.5 40.4 36.1 12.8 6.0 12.7 5.2 13.1 5.3 52.7 31.7 29.7 Translate - Wiki 6189 34.7 31.4 34.0 18.6 9.6 14.2 7.5 16.0 8.8 41.3 20.0 24.6TestMNLI-3k 250 29.2 37.6 33.2 10.6 4.6 9.4 3.7 10.7 4.2 26.3 14.6 15.9 Base - News 226 38.1 33.2 28.8 12.8 5.7 11.5 5.1 11.6 4.6 22.8 14.4 13.5 Base - Wiki 234 32.5 32.1 35.5 12.5 8.6 11.7 8.2 11.5 4.8 32.9 24.6 21.1 Sim - News 219 20.1 44.3 35.6 22.5 11.1 24.9 11.1 23.9 10.9 69.3 20.9 20.6 Sim - Wiki 229 20.5 45.0 34.5 12.6 7.6 13.7 5.8 12.0 4.5 60.5 32.8 28.7 Translate - Wiki 222 40.5 29.3 30.2 18.7 8.5 13.0 6.9 14.3 6.7 46.3 15.1 21.1 Table 3 : Dataset statistics . 
HLdenotes the average andstandard deviation of the hypothesis length of each label . 
label . 
Table 2 shows the agreement statistics for each protocol . 
BASE shows a higher agreement than SIMandTRANSLATE , although it is lower than MNLI . 
Compared to MNLI , all of our datasets show higher number of examples with no gold label ( no consensus between annotators ) . 
As we strictly follow the MNLI protocol for BASE , this suggests that the different population of crowdworkers is likely responsible for these differences.7 3.1 Dataset Statistics Table 3 shows the statistics of our collected data . 
As anticipated , datasets collected using SIMand TRANSLATE have slightly unbalanced distributions compared to BASE . 
In particular , for SIM , we observe that the entailment class has the lowest distribution in the training and test data . 
One clear difference between BASE and our new protocols is the hypothesis length . 
SIMand TRANSLATE tend to create longer hypothesis than BASE . 
We suspect that this is an artifact of the sentence - similarity method , which prefers identicalsentences ( both syntax and semantics ) over semantically similar sentences . 
Across domains , we observe that sentences from news texts are longer than Wikipedia . 
Recent work by McCoy et al . 
( 2019 ) shows that popular NLI models might learn a simple lexical overlap heuristic for predicting entailment labels . 
While this heuristic is natural for entailment , it can affect the model ‚Äôs generalization especially when it is strongly reÔ¨Çected in the data . 
We calculate word type overlap by using the intersection of premise 7MNLI used an organized group of crowdworkers hired through Hybrid ( gethybrid.io).and hypothesis word types , divided by the union of the two sets . 
The last three columns in Table 3 reports word type overlap in each dataset for each entailment label . 
We Ô¨Ånd that word type overlap is amuch stronger predictor of the label in our new protocols than in BASE . 
This could be a signiÔ¨Åcant driver of our results and might hurt the generalization performance of models trained using our new protocols ‚Äô data . 
3.2 Annotation Cost We use the FairWork platform to set payment for each of our HITs ( Whiting et al . 
, 2019 ) . 
FairWork surveys workers to estimate the time that each HIT takes and adjusts pay to a target of US $ 15 / hr . 
Based on its estimation , we pay $ 0.4and $ 0.3for each written hypothesis of Base - News and Base - Wiki , respectively . 
For Sim - News , Sim - Wiki , and Translate - Wiki , we pay $ 0.175,$0.15,$0.15 for each labeled sentence pair , respectively . 
In total , we spend $ 1791 for each dataset collected from Gigaword and $ 1445 for each dataset collected from Wikipedia . 
4 Experiments We aim to test whether our alternative protocols can produce high - quality data that yield models that generalize well within NLI and in transfer learning . 
For the NLI evaluation , we evaluate each model on nine test sets : ( i ) the Ô¨Åve new individual test sets , each containing ‚àº250 examples ; ( ii ) the MNLI development set ; and ( iii ) the three development sets of Adversarial NLI ( ANLI ; Nie et al . 
, 2020 ) , collected from three rounds of annotation ( A1 , A2 , A3 ) . 
ANLI is collected using an iterative adversarial approach that follows MNLI but encourages676 Test Data Training Data BN BW SN SW TW MNLI A1 A2 A3 Avg . 
CBoWBase - News 33.4 37.8 32.4 30.1 35.8 35.6 32.8 32.8 33.4 34.0 Base - Wiki 34.1 33.1 37.9 35.4 39.0 35.6 33.1 31.6 33.2 34.8 Sim - News 35.4 35.9 32.0 32.3 37.8 35.8 33.1 32.8 33.4 34.3 Sim - Wiki 32.3 37.2 52.1 49.1 44.6 36.6 33.1 32.4 32.1 38.8 Translate - Wiki 37.4 39.3 35.4 35.8 45.5 35.4 33.0 32.9 32.8 36.4RoBERTaMNLI-3k 79.0 61.3 76.7 57.5 58.1 83.9 33.4 27.0 28.7 56.2 Base - News 79.4 76.1 57.5 61.6 58.1 83.1 35.8 29.5 28.0 56.6 Base - Wiki 77.0 74.2 58.5 62.0 61.3 54.0 30.9 31.8 33.1 53.6 Sim - News 53.3 56.0 65.8 59.8 66.2 79.5 35.8 30.2 28.2 52.8 Sim - Wiki 62.0 62.8 64.8 64.9 69.1 64.7 32.2 32.0 31.5 53.8 Translate - Wiki 48.5 54.9 60.7 58.1 67.1 50.9 32.5 32.7 33.2 48.7 Average per test set 52.0 51.7 52.2 49.7 53.0 54.1 33.2 31.4 31.6 45.4 Table 4 : Model performance on individual test sets , as a median over 10 random restarts . 
BN : Base - News , BW : Base - Wiki , SN : Sim - News , SW : Sim - Wiki , TW : Translate - Wiki . 
The last row shows the average performance across models on each test set . 
Test Data Training Data BN BW SN SW TW MNLI A1 A2 A3 Avg . 
MNLI-3k 46.5 50.4 33.3 38.4 36.2 52.8 33.3 33.1 33.0 39.7 Base - News 47.8 46.6 33.8 33.6 37.4 51.5 32.5 33.3 33.1 38.8 Base - Wiki 33.2 32.1 44.3 45.0 29.3 32.8 33.3 33.3 33.0 35.1 Sim - News 33.2 35.5 38.8 38.9 29.3 32.8 33.3 33.3 33.5 34.3 Sim - Wiki 33.2 30.8 44.3 44.6 28.8 32.8 33.3 33.3 33.0 34.9 Translate - Wiki 31.4 34.6 34.3 34.5 32.4 33.6 33.3 33.3 33.5 33.4 Average per test set 37.5 38.3 38.1 39.2 32.2 39.4 33.2 33.3 33.2 36.0 Table 5 : RoBERTa performance on individual test sets for hypothesis - only models . 
crowdworkers to write sentences that are difÔ¨Åcult for a trained NLI model . 
We experiment with two sentence encoders : a CBoW baseline initialized with fastText embeddings ( Bojanowski et al . 
, 2017 ) , and a more powerful RoBERTa Large ( Liu et al . 
, 2019 ) model , Ô¨Ånetuned on individual training sets . 
We perform a hyperparameter sweep , varying the learning rate ‚àà{1e‚àí3,1e‚àí4,1e‚àí5}and the dropout rate ‚àà { 0.1,0.2 } . 
We use batch size of 16 and 4 for CBoW and RoBERTA , respectively . 
We train each model using the best hyperparameters for 10 epochs , with 10 random restarts . 
In initial experiments , we Ô¨Ånd that this setup yields sTable performance given our relatively small datasets , especially when using RoBERTa.8 For transfer learning , we test whether each dataset can improve downstream task performance when it is used as intermediate - task data ( Phang et al . 
, 2018 ; Pruksachatkun et al . 
, 2020 ) . 
As our col8This is consistent with the recent Ô¨Åndings of Zhang et al . 
( 2020 ) and Mosbach et al . 
( 2020 ) regarding Ô¨Åne - tuning BERTstyle models on small data.lected datasets are fairly small ( < 10 K examples ) , we use Ô¨Åve data - poor downstream target tasks in the SuperGLUE benchmark ( Wang et al . 
, 2019a ): COPA ( Roemmele et al . 
, 2011 ) ; WSC ( Levesque et al . 
, 2012 ) ; RTE ( Dagan et al . 
, 2005 , et seq ) , WiC ( Pilehvar and Camacho - Collados , 2019 ) ; and MultiRC ( Khashabi et al . 
, 2018 ) . 
We experiment with the BERT Large ( Devlin et al . 
, 2019 ) and RoBERTa Large models . 
We follow Pruksachatkun et al . 
( 2020 ) for training hyperparameters . 
We use the Adam optimizer ( Kingma and Ba , 2015 ) . 
We run experiments using the jiant toolkit ( Wang et al . 
, 2019b ) , which is the recommended baseline package for SuperGLUE , and is based on Pytorch ( Paszke et al . 
, 2019 ) , HuggingFace Transformers ( Wolf et al . 
, 2020 ) , and AllenNLP ( Gardner et al . 
, 2017 ) . 
4.1 NLI Experiments Table 4 reports the model performance on individual test sets . 
We include a baseline training data , a 3k randomly sampled training examples from MNLI ( MNLI-3k ) . 
We observe that all the677 Intermediate COPA MultiRC RTE WiC WSCAvg.training data acc . 
F1 Œ± acc . 
acc . 
acc . 
BERTNone 70.0 70.9 73.3 72.7 62.5 69.9 MNLI-3k +0.0 -0.1 +4.0 -0.8 -2.9 +0.0 Base - News +1.0 -0.5 +4.3 -1.7 +1.0 +0.8 Base - Wiki +2.0 +0.3 +3.2 -1.2 -1.0 +0.7 Sim - News +3.0 -0.3 +2.2 -2.3 +0.0 +0.5 Sim - Wiki +7.0 -0.2 +4.0 -2.6 -3.8 +0.9 Translate - Wiki +4.0 +0.1 +2.5 -3.7 0.0 +0.6RoBERTaNone 88.0 77.0 85.2 71.9 67.3 77.9 MNLI-3k -4.0 -0.1 +0.7 +0.2 -3.8 -1.5 Base - News +1.0 +0.4 +1.1 +0.7 -1.9 +0.3 Base - Wiki -2.0 -1.2 +1.1 +0.5 -1.0 -0.5 Sim - News -6.0 -3.6 -6.1 -0.1 -3.8 -3.9 Sim - Wiki -5.0 -1.9 -2.2 -1.2 -16.3 -5.3 Translate - Wiki -5.0 -2.7 -2.5 -1.8 -6.7 -3.7 Table 6 : Results on using each collected dataset as intermediate training data on Ô¨Åve SuperGLUE tasks . 
We report the median performance over 3 random restarts on the intermediate NLI models . 
None denotes experiments without intermediate - task training , i.e. , direct Ô¨Åne - tuning on target tasks . 
The last column shows the average score across the Ô¨Åve tasks . 
We report the difference with respect to None using BERT and RoBERTa . 
CBoW baselines obtain near chance performance . 
Using RoBERTa , the top performing models are all trained on datasets collected using BASE : BaseNews and MNLI-3k . 
We Ô¨Ånd that models trained using Translate - Wiki obtain the worst performance . 
On average across all training sets , ANLI development sets seem to be the hardest , while MNLI seems to be the easiest . 
Unsurprisingly , we do not Ô¨Ånd a single training set which yields the best model across all test sets . 
We observe that models trained on Base - News perform the best for Base - News and Base - Wiki test sets . 
Similarly , Sim - Wiki performs the best on both Sim - Wiki and Sim - News test sets . 
We Ô¨Ånd that all models do poorly on all ANLI development sets . 
Overall , we Ô¨Ånd that Base - News outperforms all other datasets . 
However , it is also better than SIMandTRANSLATE which suggests that our new protocols failed . 
The lower accuracy for SIMand TRANSLATE on their respective test sets also suggests that they produce datasets with noisier labels . 
4.2 Hypothesis - Only Results Next , we experiment with a hypothesis - only model ( Poliak et al . 
, 2018b ) to investigate spurious statistical patterns in the hypotheses which might signal the actual labels to the model . 
Table 5 reports the results for all Ô¨Åve datasets and MNLI . 
On the Ô¨Åve new test sets , we observe that MNLI and BaseNews are the most solvable by the hypothesis - only models , though their numbers are still much lowerthan with SNLI with accuracy 69.17 . 
On average across all test sets , none of the training sets obtain much higher performance than chance . 
All models achieve chance performance on ANLI . 
However , all of our training sets are fairly small , and these numbers might not be very informative . 
This also explains why these numbers are relatively lower than other NLI datasets ( Poliak et al . 
, 2018b ) . 
Across all training sets , we again see that the MNLI test set is the most solvable by the hypothesis - only models . 
Our new protocols show lower performance than theBASE , but that may just be because they are of lower overall quality and not because they are less solvable by the hypothesis - only models . 
We verify this by looking at their transfer learning performance in the following section . 
4.3 Transfer Learning Table 6 shows our results when using each collected data as intermediate - training data on the Ô¨Åve target tasks . 
We report the median performance of three random restarts on the validation sets . 
Using BERT , we observe that all our new datasets yield models with better performance than plain BERT or MNLI-3k as intermediate - training data . 
We see less positive transfer when we use RoBERTa . 
If we look at individual target task performance , both Base - News and Base - Wiki data give consistent positive transfer for RTE , a natural language inference task . 
We also see some positive trans-678 Entailment Contradiction NeutralM-3klooked 0.44 no 1.03 also 0.75 capital 0.43 never 0.95 because 0.71 population 0.43 any 0.88 better 0.63B - Newsaccording 0.58 never 1.07 also 0.62 position 0.45 no 1.02 many 0.52 set 0.42 any 0.90 most 0.52B - Wikiboth 0.45 never 1.18 most 0.78 named 0.38 not 1.01 well 0.64 early 0.35 any 0.96 many 0.56S - Gigasummit 0.53 points 0.66 very 0.54 roads 0.51 we 0.65 research 0.48 weighted 0.46 ‚Äì 0.59 weeks 0.48S - Wikidivision 0.56 census 0.88 through 0.57 team 0.48 population 0.86 such 0.54 candidate 0.47 2010 0.82 number 0.49T - Wiki ; 0.68 brought 0.45 each 0.57 album 0.58 maintain 0.40 { 0.56 f 0.55 will 0.39 } 0.56 Table 7 : Top three words most associated with each label by PMI . 
M : MNLI , B : Base , S : Sim , T : Translate . 
fer for COPA , however since its validation set is very small ( 100 examples ) , we can not conclude anything with conÔ¨Ådence . 
Overall , our BASEshows better transfer learning performance compared to MNLI , suggesting that our setup is sound . 
However , we also see that our new protocols perform worse than BASE , showing that they produce less useful training data than the strong baseline of crowdworker writing . 
5 Dataset Analysis 5.1 Annotation Artifacts Following Gururangan et al . 
( 2018 ) , we compute the PMI between each hypothesis word and label in the training set to examine whether certain words have high associations with its inference label . 
For a fair comparison , we only use ‚àº3k training examples from each dataset , and sub - sample data collected using S IMand T RANSLATE . 
Table 7 shows the top three most associated words for each label , sorted by their PMI scores . 
We Ô¨Ånd that BASE has similar associations to MNLI , especially for the neutral and contradiction labels where we found many negations and adverbs . 
We observe that both SIMandTRANS LATE are less susceptible to this artifact . 
However , this might be a side - effect of high word overlap in the data , which prefers similar words in the premise and hypothesis . 
This is also a well - known artifact for NLI data ( McCoy et al . 
, 2019).5.2 Qualitative Analysis Our new protocols use a vector - distance based measurement to Ô¨Ånd similar sentences , and we Ô¨Ånd that many of the sentence pairs share similar syntactic structure in their premise and hypothesis , even when both describe different events or entities . 
We also Ô¨Ånd that hypothesis in several SimNews examples differs by only a few words with its premise . 
For Translate - Wiki , we observe some effects of translation divergence , where the translation of the sentence changes semantically because of cross - linguistic distinctions between languages . 
We provide some examples of these observations in Table 8 . 
6 Related Work There is a large body of work on constructing data for natural language inference . 
The Ô¨Årst test suite for entailment problems , FraCas ( Consortium et al . 
, 1996 ) , is a very small set created manually by experts to isolate phenomena of interest . 
The RTE challenge corpora ( Dagan et al . 
, 2005 , et seq ) were built by asking human annotators to judge whether a text entails a hypothesis . 
The SICK dataset ( Marelli et al . 
, 2014 ) is constructed by mining existing paraphrase sentence pairs from image and video captions , which annotators then label . 
Some recent works also use automatic methods for generating sentence pairs for entailment data . 
Zhang et al . 
( 2017 ) propose a framework to generate hypotheses based on context from general world knowledge or neural sequence - to - sequence methods . 
The DNC corpus ( Poliak et al . 
, 2018a ) is an NLI dataset with ordinal judgments constructed by recasting several NLP datasets to NLI examples and labeling them using custom automatic procedures . 
QA - NLI ( Demszky et al . 
, 2018 ) is an NLI dataset derived from existing QA datasets . 
Similar to ours , both DNC and QA - NLI use automatic methods to generate sentence pairs . 
However , neither of them explicitly evaluates whether machinegenerated pairs are better than human - generated pairs . 
Bowman et al . 
( 2020 ) propose four potential modiÔ¨Åcations to the SNLI / MNLI protocol , all still involving crowdworker writing , and show that none yields improvements in the resulting data . 
SWAG ( Zellers et al . 
, 2018 ) and HellaSwag ( Zellers et al . 
, 2019 ) construct sentence pairs from speciÔ¨Åc data sources and use language models to generate challenging negative examples.679 Type Dataset Premise Hypothesis Label Syntactic structureSim - NewsFor many people , choosing wallpaper is one of decorating ‚Äôs more stressful experiences , fraught with anxiety over color , pattern and cost . 
For many people , anxiety about decorating stems from not understanding the language of furniture , fabrics and decorative styles . 
E Sim - WikiIts Ô¨Çowers are pale yellow towhite andspherical . 
Its Ô¨Çowers are funnel - shaped and pink towhite . 
C Translate - WikiBut now , in the early 1990s , the Jakarta - Begor railway had turned into a double rail . 
However , by the early 1990s , McCreery ‚Äôs position within the UDA became less secure . 
N Lexical overlapSim - NewsGrandMet owns Burger King , the world ‚Äôs second - biggest hamburger chain , as well as US frozen foods manufacturer Pillsbury , which produces the luxury ice - cream HaagenDaazs . 
GrandMet owns Burger King , the world ‚Äôs second - biggest hamburger chain , as well as US food group Pillsbury , which produces the luxury icecream Haagen - Daazs . 
E Translation divergenceTranslate - WikiMarcus Claudius then abducted her while shewas on herway to school . 
Marcus Claudius then kidnapped him while hewas on hisway to school . 
N Table 8 : Dataset observations from our new protocols . 
On the topic of cost - effective crowdsourcing , Gao et al . 
( 2015 ) develop a method to reduce redundant translations when collecting human translated data . 
When the annotation budget is Ô¨Åxed , Khetan et al . 
( 2018 ) suggest that it is better to label collect single label per training example as many as possible , rather than collecting less training examples with multiple labels . 
7 Conclusion In this paper , we introduce two data collection protocols which use fully - automatic pipelines to collect hypotheses , replacing crowdworker writing in the MNLI baseline protocol . 
We Ô¨Ånd that switching to a writing - free process with the same source data and annotator pool yields poor - quality data . 
Our main experiments show strong negative results both in NLI generalization and transfer learning , and mixed results on annotation artifacts , suggesting that MNLI - style crowdworker writing examples are broadly better than automatically paired ones . 
This Ô¨Ånding dovetails with that of Bowman et al . 
( 2020 ) , who Ô¨Ånd that they are unable to improve upon a base MNLI - style prompt when introducing aids meant to improve annotator speed or creativity . 
Future work along this line might focus on crowdsourcing strategies ( beyond the basic HIT design ) which encourage crowdworkers to produce high - quality data with reduced artifacts . 
While our fully - automatic methods to construct sentence pairs yield negative results , we have not exhausted all possible automatic techniques for collecting similar sentences . 
However , giventhat we use state - of - the - art tools including FAISS , RoBERTa , and OPUS , and reÔ¨Åne our methods with several rounds of piloting and tuning , we are skeptical that there is low - hanging fruit in the two directions we explored . 
A more radically different direction might involve generating pairs from scratch , using a large language model like GPT-3 ( Brown et al . 
, 2020 ) . 
However , this would still require training data from crowdworker - written dataset , and might add a major source of potentially difÔ¨Åcult - todiagnose bias . 
Finally , despite its known issues , we Ô¨Ånd that MNLI - style data is still the most effective for both NLI evaluation and transfer learning , and future efforts to create similar data should work from that starting point . 
Acknowledgments This project has beneÔ¨Åted from Ô¨Ånancial support to SB by Eric and Wendy Schmidt ( made by recommendation of the Schmidt Futures program ) , by Samsung Research ( under the project Improving Deep Learning using Latent Structure ) , by Intuit , Inc. , and in - kind support by the NYU HighPerformance Computing Center and by NVIDIA Corporation ( with the donation of a Titan V GPU ) . 
This material is based upon work supported by the National Science Foundation under Grant No . 
1922658 . 
Any opinions , Ô¨Åndings , and conclusions or recommendations expressed in this material are those of the author(s ) and do not necessarily reÔ¨Çect the views of the National Science Foundation.680 A Appendices A.1 Indexing and Retrieval Gigaword The corpus contains texts from seven news sources : afp eng , apw eng , cna eng , ltw eng , nyteng , wpb eng , and xin eng . 
We build one index for each news source with type ‚Äú PCAR64,IVFx , Flat ‚Äù , where xdeÔ¨Ånes the number of clusters in the index . 
This type of index allows faster retrieval , however it requires a training stage to assign a centroid to each cluster . 
We refer readers to FAISS documentation for more detail explanations.9 For each news source , we randomly sample 100 sentences from its monthly articles and use them as seed sentences to train the clusters . 
We then set the number of clusters xtoN 100(rounded to the nearest hundred ) , where Nis the number of seed sentences . 
Table 9 lists the number of seed sentences and clusters used for each news source index . 
Source # seed sentences x afpeng 111,147 1,100 apw eng 146,119 1,400 cnaeng 125,508 1,200 ltweng 90,195 900 nyteng 136,827 1,300 wpb eng 9,144 100 xineng 157,760 1,500 Table 9 : Number of seed sentences and number of clusters for each news source index . 
During retrieval , for each query , we retrieve top 1000 sentences from each index and perform reranking on the combined list , i.e. , 7,000 sentence pairs , as described in Section 2.2 . 
Wikipedia We build one index for the whole Wikipedia corpus . 
For seed sentences , we use sentences taken from the Ô¨Årst paragraph of each article as it usually contains the summary of the article . 
We set the number of clusters xto 15,000 . 
9https://github.com/facebookresearch/ faiss684 A.2 Writing HIT Instructions ‚óè ‚óè ‚óè Prompt :   ‚Äú Security and reliability are two important aspects of this service because of the sensitivity and urgency of the data sent over . 
‚Äù   Definitely correct   Example : For the prompt ‚Äú The cottages near the shoreline , styled like plantation homes with large covered porches , are luxurious within ; some come with private hot tubs . 
‚Äù , you   could write ‚Äú The shoreline has plantation style homes near it , which are luxurious and often have covered porches or hot tubs . 
‚Äù   Maybe correct   Example : For the prompt ‚Äú Government Executive magazine annually presents Government Technology Leadership Awards to recognize federal agencies and state governments   for their excellent performance with information technology programs . 
‚Äù , you could write ‚Äú In addition to their annual Government Technology Leadership Award , Government   Executive magazine also presents a cash prize for best dressed agent from a federal agency . 
‚Äù   Definitely incorrect   Example : For the prompt ‚Äú Yes , he ‚Äôs still under arrest , which is why USAT ‚Äôs front - page refer headline British Court Frees Chile ‚Äôs Pinochet is a bit off . 
‚Äù , you could write ‚Äú The   headline ` British Court Frees Chile ‚Äôs Pinochet ` is correct , since the man is freely roaming the streets . 
‚Äù   Problems ( optional ) If something is wrong with the prompt that makes it difficult to understand , let us know here . 
  Figure 2 : Writing HIT instructions.685 A.3 Data Labeling and Validation HIT Instructions ‚óè ‚óè ‚óè ‚óè Figure 3 : Data Labeling and Validation HIT instructions . 
We collect one annotation per example for data labeling and Ô¨Åve annotations per example for validation.686 Proceedings of the 1st Conference of the Asia - PaciÔ¨Åc Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing , pages 687‚Äì695 December 4 - 7 , 2020 . 
¬© 2020 Association for Computational Linguistics MaP : A Matrix - based Prediction Approach to Improve Span Extraction in Machine Reading Comprehension Huaishao Luo1‚àó , Yu Shi2 , Ming Gong3 , Linjun Shou3 , Tianrui Li1 1School of Information Science and Technology , Southwest Jiaotong University 2Microsoft Cognitive Services Research Group 3Microsoft STCA NLP Group huaishaoluo@gmail.com , trli@swjtu.edu.cn { yushi , migon , lisho } @microsoft.com Abstract Span extraction is an essential problem in machine reading comprehension . 
Most of the existing algorithms predict the start and end positions of an answer span in the given corresponding context by generating two probability vectors . 
In this paper , we propose a novel approach that extends the probability vector to a probability matrix . 
Such a matrix can cover more start - end position pairs . 
Precisely , to each possible start index , the method always generates an end probability vector . 
Besides , we propose a sampling - based training strategy to address the computational cost and memory issue in the matrix training phase . 
We evaluate our method on SQuAD 1.1 and three other question answering benchmarks . 
Leveraging the most competitive models BERT and BiDAF as the backbone , our proposed approach can get consistent improvements in all datasets , demonstrating the effectiveness of the proposed method . 
1 Introduction Machine reading comprehension ( MRC ) , which requires the machine to answer comprehension questions based on the given passage of text , has been studied extensively in the past decades ( Liu et al . 
, 2019 ) . 
Due to the increase of various large - scale datasets ( e.g. , SQuAD ( Rajpurkar et al . 
, 2016 ) and MS MARCO ( Nguyen et al . 
, 2016 ) ) , and the enhancement of pre - trained models ( e.g. , ELMo ( Peters et al . 
, 2018 ) , BERT ( Devlin et al . 
, 2019 ) , and XLNet ( Yang et al . 
, 2019 ) ) , remarkable advancements have been made recently in this area . 
Among various MRC tasks , span extraction is one of the essential tasks . 
Given the context and question , the span extraction task is to extract a span of the most plausible text from the corresponding context as a ‚àóThis work was done during the Ô¨Årst author ‚Äôs internship at Microsoft Passage : ,Begun as a one - page journal in September 1876 , the Scholastic magazine is issued twice monthly and ... Question : When did the Scholastic Magazine of Notre dame begin publishing?Answer : September 1876 condi ÔøΩ onal condi ÔøΩ onal p(end|start)p(end|start)p(start|end)p(start|end ) Vector - BasedMatrix - BasedFigure 1 : An illustration of a machine reading comprehension framework . 
Most of previous works are vectorbased approaches shown as the left part . 
Our matrixbased conditional approach is shown in the right part . 
In our setting , every start ( or end ) position has an end ( or start ) probability vector , which leads that the output probabilities is a matrix ( best seen in color ) . 
candidate answer . 
Although there exist unanswerable cases beyond the span extraction , the spanbased task is still fundamental and signiÔ¨Åcant in the MRC Ô¨Åeld . 
Previous methods used to predict the start and end position of an answer span can be divided into two categories . 
The Ô¨Årst one regards the generation of begin position and end position independently . 
We refer to this category as independent approach . 
It can be written as p‚àó=p(‚àó|H‚àó ) , where‚àó‚àà { s , e } , thesandedenote start and end , respectively . 
H‚àóis the hidden representation , in which HsandHeusually have shared features . 
The other one constructs a dependent route from the start position when predicting the end position . 
We refer to this category asconditional approach . 
It can be formalized as ps = p(s|Hs),pe = p(e|s , He ) . 
This category usu-687 ally reuses the predicted position information ( e.g. , s ) to assist in the subsequent prediction . 
The difference between these two approaches is that the conditional approach considers the relationship between start and end positions , but the independent approach does not . 
In the literature , AMANDA ( Kundu and Ng , 2018b ) , QANet ( Yu et al . 
, 2018 ) , and SEBert ( Keskar et al . 
, 2019 ) can be regarded as the independent approach , where the probabilities of the start and end positions are calculated separately with different representations . 
DCN ( Xiong et al . 
, 2017 ) , R - NET ( Wang et al . 
, 2017 ) , BiDAF1 ( Seo et al . 
, 2017 ) , Match - LSTM ( Wang and Jiang , 2017 ) , S - Net ( Tan et al . 
, 2018 ) , SDNet ( Zhu et al . 
, 2018 ) , and HAS - QA ( Pang et al . 
, 2019 ) belong to the conditional approach . 
The probabilities are generated in sequence . 
The conditional approach empirically has an advantage over the independent approach . 
However , the output distributions of the previous conditional approaches are two probability vectors . 
It ignores some more possible start - end pairs . 
As an extension , every possible start ( or end ) position should have an end ( or start ) probability vector . 
Thus , the output conditional probabilities is a matrix . 
We propose a Matrix - based Prediction approach ( MaP ) based on the above consideration in this paper . 
As Figure 1 shown , the key point is to consider as many probabilities as possible in training and inference phases . 
SpeciÔ¨Åcally , we calculate a conditional probability matrix instead of a probability vector to expand the choices of start - end pairs . 
Because of more values contained in a matrix than a vector , there is a big challenge in the training phase of the MaP. That is the high computational cost and memory issues if the input sequence is long . 
As an instance , the matrix contains 262 , 144probability values if the sequence length is 512 . 
Therefore , we propose a sampling - based training strategy to speed up the training and reduce the memory cost . 
The main contributions of our work are fourfold . 
‚Ä¢A novel conditional approach is proposed to address the limitation of the probability vector generated by the vector - based conditional approach . 
It increases the likelihood of hitting the ground - truth start and end positions . 
‚Ä¢A sampling - based training strategy is pro1We classify BiDAF as a conditional approach by its ofÔ¨Åcial implementation : https://github.com/allenai/ bi - att - flowposed to overcome the computation and memory issues in the training phase of the matrixbased conditional approach . 
‚Ä¢An ensemble approach on both start - to - end and end - to - start directions of conditional probability is investigated to improve the accuracy of the answer span . 
‚Ä¢We evaluate our strategy on SQuAD 1.1 and three other question answering benchmarks . 
The implementation of the matrix - based conditional approach is designed based on the BERT and BiDAF , which are the most competitive models , to test the generalization of our strategy . 
The consistent improvements in all datasets demonstrate the effectiveness of the strategy . 
2 Methodology In this section , we Ô¨Årst give the problem deÔ¨Ånition . 
Then we introduce a typical vector - based conditional approach . 
Next , we mainly introduce our matrix - based conditional approach and samplingbased training strategy . 
Finally , an ensemble approach on both start - to - end and end - to - start directions of conditional probability is discussed . 
2.1 Problem Statement Given the passage P={t1,t2 , ¬∑ ¬∑ ¬∑ , tn}and the questionQ={q1,q2 , ¬∑ ¬∑ ¬∑ , qm } , the span extraction task needs to extract the continuous subsequence A= { ts , ¬∑ ¬∑ ¬∑ , te}(1‚â§s‚â§e‚â§n ) from the passage as the right answer to the question , where nandmare the length of the passage and question respectively , sandeare the start and end position in the passage . 
Usually , the objective to predict a= ( s , e)is maximizing the conditional probability p(a|P , Q ) . 
2.2 A Typical Vector - based Approach We summarize a typical implementation of the vector - based conditional approach shown in Figure 2 . 
Previous mentioned R - NET , BiDAF , MatchLSTM , S - Net , and SDNet can be regarded as such implementation . 
Its backbone is the Pointer Network proposed by Vinyals et al . 
( 2015 ) . 
The interactive representation H‚ààRn√ódbetween the given question Qand passage Pis calculated as follows , H = M(Q , P ) , ( 1 ) whereMis a neural network , e.g. , Match - LSTM , QANet , BERT , and XLNet , dis the dimension size688 Model Ques ÔøΩ on PassageRNNso ÔøΩ max so ÔøΩ max(|)psH ( |,)pesH specsheh Init Œó QHFigure 2 : A typical implementation of the vector - based conditional approach . 
of the representation . 
After generating the interactive representation , the next step is to predict the answer span . 
The main architecture of the span prediction is an RNN . 
As an instance , LSTM is used in ( Wang and Jiang , 2017 ) , and GRU is adopted in ( Tan et al . 
, 2018 ; Zhu et al . 
, 2018 ) . 
Take the hidden representationhe‚ààRkof end position as an example , which is calculated as follows , he = RNN(hs , ce ) , ( 2 ) ce = H / latticetopps , ( 3 ) where ps = p(s|H)is the start probability and ps‚ààRn , kis the dimension size of he . 
Then pe = p(e|s , H)(pe‚ààRn)can be calculated using heas follows , p(e|s , H ) = softmax / parenleftBig v / latticetoptanh / parenleftbig VH / latticetop+/llbracketWehe / rrbracketn / parenrightbig / parenrightBig ( 4 ) where / llbracket¬∑/rrbracketnis an operation that generates a matrix by repeating the vector on the left ntimes , v‚ààRl , V‚ààRl√ód , and We‚ààRl√ókare parameters to be learned . 
The calculation of p(s|H)is similar to p(e|s , H ) . 
The key is to obtain the hidden state hs . 
A choice is to use an attention approach to condense the question representation into a vector . 
The process is as follows , pinit = softmax / parenleftBig v / latticetop Qtanh / parenleftbig VQH / latticetop Q / parenrightbig / parenrightBig , ( 5 ) hs = H / latticetop Qpinit , ( 6 ) where HQ‚ààRm√ódis the representation corresponding to Q , vQ‚ààRl , and VQ‚ààRl√ódare parameters . 
Œó i Œó Pn[]iHÔÇ©ÔÇ¨ÔÇ™ÔÇ≠ÔÇ´ÔÇÆFigure 3 : Matrix - based conditional approach . 
There is a vast number of works on MRC . 
However , most of these works focus on the design of M and generate the answer span based on the vectorbased conditional approach . 
In this paper , we expand the vector to a probability matrix . 
Thus , many more possibilities can be covered . 
It is also a natural manner because that every start ( or end ) position should have an end ( or start ) probability vector . 
2.3 Matrix - based Conditional approach As the previous description , the implementation of the vector - based conditional approach has a uniÔ¨Åed and important implementation step : create a ‚Äò condition ‚Äô . 
Take the forward direction ( ‚Äò condition ‚Äô constructed from the start position to end position ) of the vector - based conditional approach as an example , the ‚Äò condition ‚Äô is the probability vector ps . 
The end probability vector pecan not be calculated until generating ps . 
However , there is only one probability vector pewhatever the start position is . 
In this paper , we keep the ‚Äò condition ‚Äô step but propose calculating an individual pefor each start position . 
SpeciÔ¨Åcally , the probability matrix Pe‚ààRn√ónis calculated as follows , P(i ) e = softmax / parenleftBigg v / latticetoptanh / parenleftbigg V / bracketleftbigg H / latticetop;/Largellbracket / parenleftbig H[i]/parenrightbig / latticetop / Largerrbracketn / bracketrightbigg / parenrightbigg / parenrightBigg ( 7 ) where P(i ) edenotes the i - th row of Pe,[;]is a concatenate operation , /llbracket¬∑/rrbracketnis an operation that generates a matrix by repeating the vector on the left n times , [ i]means to choose the i - th row from the matrix H , v‚ààRlandV‚ààRl√ó2dare parameters . 
Figure 3 illustrates the calculation process of Eq . 
( 7 ) . 
Although the calculation is brief and can cover more probabilities than the vector - based approach , there is a big question on computation cost and memory occupation . 
The main computation cost comes from the matrix multiplication between V and / bracketleftbigg H / latticetop;/Largellbracket / parenleftbig H[i]/parenrightbig / latticetop / Largerrbracketn / bracketrightbigg in Eq . 
( 7 ) , totally ntimes689 such computation for Pe . 
The number of probabilities is also ntimes bigger than the vector - based conditional approach . 
It also causes the issue of out of memory ( OOM ) , especially with a big n , due to intermediate gradient values needing cache in the training phase . 
We propose a sampling - based training strategy to solve the above issues . 
2.4 Sampling - based Training Strategy In order to train the probability matrix effectively , we propose a sampling - based strategy in the training phase . 
Given the hyper - parameter k , we Ô¨Årst choose the indexes ÀÜIof top k‚àí1possibilities from p(- ÀÜs ) s , ÀÜI = top / parenleftBig p(- ÀÜs ) s , k‚àí1 / parenrightBig , ( 8) where top(p , v)is an operation used to get the indexes of top vvalues in p , p(-w)contains all but w - th value of p , and ÀÜsis the truth start position used as the supervised information in the training phase . 
Then , the ÀÜ smust merge to ÀÜI , I=ÀÜI+{ÀÜs } , ( 9 ) whereIcontains kindexes . 
Eq . 
( 8) and Eq . 
( 9 ) promise that the sampled start probabilities must contain and only contain the target probability which we need to train in each iteration . 
The target probability is the ÀÜs - th value in ps , and the bigger , the better . 
After sampling the start probability vector , the computation cost of Pedecrease . 
For each i‚ààI , executing Eq . 
( 7 ) repeatedly can generate a samplingbased end probability matrix . 
It is noted that this sampling - based matrix is a part of the original Pe . 
We refer to it as ÀúPe , and ÀúPe‚ààRk√ón . 
It is still a big issue of computation cost and memory occupation for ÀúPewith a long sequence . 
So , we carry out similar operations in Eq . 
( 8) and Eq . 
( 9 ) for each row of ÀúPeusing ÀÜeinstead of ÀÜs , where ÀÜeis the end truth position . 
Finally , the sampling - based matrix ÀÜPe‚ààRk√ókis generated . 
It is small enough to train compared with Pe . 
Figure 4 shows the sampling results colored with a yellow background on the left and corresponding ground truth matrix on the right . 
2.5 Training In the training phase , the objective function is to minimize the cross - entropy error averaged over start and end positions , L=1 2(Ls+Le ) , ( 10 )           Ground Truthij e ePsÔÄ§ÔÄ§Figure 4 : A sampling of probability matrix . 
Left : the calculated probability matrix with sampled top four positions ( in both row and column directions colored with yellow background ) . 
Right : the ground truth matrix , where position ( ÀÜs,ÀÜe)with the red background has probability 1 . 
Ls=‚àí1 NN ‚àë i=1 / parenleftBig I(ÀÜs)/parenleftbig log(ps)/parenrightbig / latticetop / parenrightBig , ( 11 ) Le=‚àí1 NN ‚àë i=1 / parenleftbigg T / parenleftbig I(ÀÜs,ÀÜe)/parenrightbig / parenleftBig log / parenleftbig T(ÀÜPe)/parenrightbig / parenrightBig / latticetop / parenrightbigg , ( 12 ) where Nis the number of data , I(ÀÜs)means the onehot vector of ÀÜs , I(ÀÜs,ÀÜe)means a zero matrix with a value of 1 in row ÀÜsand column ÀÜe , andT()is a row wise Ô¨Çatten operation . 
The Ô¨Çatten operation makes the loss function on matrix - based distribution similar to that on vector - based distribution . 
As the introduction of the sampling - based training strategy , there are limited end probabilities that could be trained in each iteration . 
The extreme situation is kequals to n , which makes all probability matrix calculate each time . 
As our previous argumentation , it is almost impossible for time and memory limitations . 
However , there is a question of what makes sampling strategy works . 
The following content gives some explanation based on gradient backpropagation . 
The gradient of the cross - entropy L‚àóto the predicted logits z‚àóis , ‚àÇL‚àó ‚àÇz‚àó=/braceleftBigg p(i ) ‚àó‚àí1,ifiis the ground - truth ; p(j ) ‚àó , others(13 ) where p‚àó=softmax ( z‚àó)is probabilities in which values are between 0 and 1 ( exclusion ) . 
Thus p(i ) ‚àó‚àí1is negative , and p(j ) ‚àóis positive in most cases . 
As the parameters Œ∏update usually follows Œ∏t = Œ∏t‚àí1‚àíŒ∑¬∑‚àáŒ∏L(Œ∏)and learning rate Œ∑is a positive value , the probability in ground - truth position should go up , and the probabilities in other sampled positions should go down.690 Itera ÔøΩ on # 1Ground - truth Itera ÔøΩ on # 2 Itera ÔøΩ on # 3Figure 5 : Sampling - based probabilities training ( k=5 ) . 
Block with red color is the ground - truth , blocks with blue color are the sampled probabilities . 
Probabilities with a gray background will not change their values in each iteration . 
Figure 5 illustrates the sampling - based training process , where the parameter kis set to 5 . 
It means that there are extra top-4 probabilities ( blue background ) except ground - truth ( red background ) will be chosen to calculate . 
With the iteration going from # 1 to # 3 , the probability in ground - truth position goes up , and that in sampled top-4 positions goes down . 
Such a sampling - based training approach has the same goal with the training on the whole probabilities , thus should have proximity results . 
2.6 Ensemble for Inference The vector - based conditional approach usually searches the span ( s , e)via the computation of p(i ) s√óp(j ) eunder the condition of i‚â§j , and choices the(i‚àó,j‚àó)with the highest p(i‚àó ) s√óp(j‚àó ) eas the output in the inference phase . 
The matrix - based conditional approach follows the same idea , but the calculation of the probability is p(i ) s√óP(i , j ) einstead ofp(i ) s√óp(j ) e. The p(i ) sis the i - th probability in ps , andP(i , j ) eis the probability in row i , column jof Pe . 
The above inference strategy only involves one direction , e.g. , start - to - end direction ( generate start position Ô¨Årstly , then generate end position ) , which is the most cases in previous works . 
An ensemble of both start - to - end and end - to - start directions is a good choice to improve the performance . 
The difference in end - to - start direction is that Eqs . 
( 712 ) should be repeated in the opposite direction . 
In other words , the start is replaced by e , and the end is replaced by s. Totally , there are two groups of probabilities , ( ps , Pe)and(pe , Ps ) . 
In this paper , we design a type of ensemble strategy , which Ô¨Årst chooses top kpairs F={(if , jf)}withAlgorithm 1 MaP Training Algorithm Input : Npairs of passage Pand question Q , k used to choose top probabilities ; Output : Learned MaP model 1 : Initialize all learnable parameters Œò ; 2 : repeat 3 : Select a batch of pairs from corpus ; 4 : foreach pair ( P , Q)do 5 : Use a neural network Mto generate the representation H ; ( Eq . 
1 ) 6 : Compute start probability vector ps ; ( Eqs . 
4 - 6 ) 7 : Sample indexesIby choosing top k‚àí1 probabilities of ps ; ( Eqs . 
8,9 ) 8 : Compute end probability matrix Pe ; ( Eq . 
7 ) 9 : Compute objective L ; ( Eq . 
10 - 12 ) 10 : end for 11 : Use the backpropagation algorithm to update parameters Œòby minimizing the objective with the batch update mode 12 : until stopping criteria is met highest probability p(if ) s√óP(if , jf ) e , then chooses topkpairs B={(jb , ib)}with highest probabilityp(jb ) e√óP(jb , ib ) s . 
It is noted that some pairs may have the same position , e.g. , ( 3f,5f)and(5b,3b ) . 
If there are the same elements , we prune away them inB. Then , we choose the ( i‚àó,j‚àó)with highest probability in F‚à™B. The overall training procedure of MaP is summarized in Algorithm 1 . 
3 Experiments In this section , we conduct experiments to evaluate the effectiveness of the proposed MaP. 3.1 Datasets We Ô¨Årst evaluate our strategy on SQuAD 1.1 , which is a reading comprehension benchmark . 
The benchmark beneÔ¨Åts to our evaluation compared with its augmented version SQuAD 2.0 due to its questions always have a corresponding answer in the given passages . 
We also evaluate our strategy on three other datasets from the MRQA 2019 Shared Task2 : NewsQA ( Trischler et al . 
, 2017 ) , HotpotQA ( Yang et al . 
, 2018 ) , Natural Questions ( Kwiatkowski et al . 
, 2019 ) . 
As the SQuAD 1.1 dataset , the format of 2https://github.com/mrqa/ MRQA - Shared - Task-2019691 ModelsSQuAD NewsQA HotpotQA Natural Questions EM F1 EM F1 EM F1 EM F1 BERT - Base InD 81.24 88.38 52.59 67.12 59.01 75.69 67.31 78.96 MaP F 81.78 88.59 52.66 66.50 59.82 75.81 67.68 78.99 MaP E82.12 88.63 53.06 67.37 60.55 76.12 68.21 79.09 BERT - Large InD 84.05 90.85 54.46 69.61 62.26 78.18 69.44 80.93 MaP F 84.50 90.89 54.84 68.73 63.19 78.99 69.56 80.49 MaP E84.79 90.89 55.29 69.98 63.70 79.25 69.91 81.22 BiDAF VCP 68.57 78.23 44.04 58.07 47.31 62.42 56.95 68.79 MaP F 68.85 78.06 44.19 58.65 50.25 65.21 57.04 68.87 MaP E69.55 78.91 44.25 58.91 51.45 66.74 57.21 69.08 Table 1 : The performance ( % ) of EM and F1 on SQuAD 1.1 and three MRQA extractive question answering tasks . 
MaP Fis the matrix - based conditional approach calculating on start - to - end direction . 
MaP Emeans the ensemble of both directions of matrix - based conditional approach . 
InD denotes the independent approach . 
VCP is vector - based conditional approach . 
the task is extractive question answering . 
It contains no unanswerable or non - span answer questions . 
Besides , the fact that these datasets vary in both domain and collection pattern beneÔ¨Åts for the evaluation of our strategy on generalization across different data distributions . 
Table 2 shows the statistics of these datasets . 
Dataset Training Development SQuAD 1.1 86,588 10,507 NewsQA 74,160 4,212 HotpotQA 72,928 5,904 Natural Questions 104,071 12,836 Table 2 : The statistics of datasets . 
3.2 Baselines To validate the effectiveness and generalization of our proposed strategy on the span extraction , we implement it using two strong backbones , BERT and BiDAF . 
SpeciÔ¨Åcally , we borrow their main bodies except the top layer to implement the proposed strategy to Ô¨Ånish the span extraction on different datasets . 
Some more tests on other models , e.g. , XLNet ( Yang et al . 
, 2019 ) and SpanBERT ( Joshi et al . 
, 2019 ) , and datasets will be our future work . 
‚Ä¢BERT : BERT is an empirically powerful language model , which obtained state - of - the - art results on eleven natural language processing tasks in the past ( Devlin et al . 
, 2019 ) . 
The original implementation in their paper on the span prediction task belongs to the independent approach . 
Both BERT - base and BERT - large withuncased pre - trained weights are used in comparison to investigating the effect of the ability of language model on span extraction with different prediction approaches . 
‚Ä¢BiDAF : BiDAF is used as a baseline of the vector - based conditional approach ( Seo et al . 
, 2017 ) . 
The use of a multi - stage hierarchical process and a bidirectional attention Ô¨Çow mechanism makes its representation powerful . 
There are four strategies of span extraction involved in our comparison : InD denotes the independent approach ; VCP is the vector - based conditional approach ; MaP Fis our matrix - based conditional approach calculating on start - to - end direction ; MaP Emeans the ensemble of both directions of matrix - based conditional approach . 
The InD is used to compare with MaP Fand MaP Ein BERT , and the VCP is used to compare with MaP Fand MaP Ein BiDAF . 
3.3 Experimental Settings We implement the BERT and BiDAF following the ofÔ¨Åcial settings for a fair comparison . 
For the BERT , we train for 3 epochs with a learning rate of5e-5and a batch size of 32 . 
The max sequence length is 384 for SQuAD 1.1 and 512 for other datasets , and a sliding window of size 128 is used for all datasets is the sentence is longer than the max length . 
For the BiDAF , we keep all original settings except a difference that we use ADAM ( Kingma and Ba , 2015 ) optimizer with a learning rate of 1e-3 in the training phase instead of AdaDelta ( Zeiler , 2012 ) for a stable performance.692 Following the work from ( Rajpurkar et al . 
, 2016 ) , we evaluate the results using Exact Match ( EM ) and Macro - averaged F1 score . 
The sampling parameter kis set to 20 for our strategy . 
We implement our model in python using the pytorch - transformers library3for BERT and the AllenNLP library4for BiDAF . 
The reported results are average scores of 5 runs with different random seeds . 
All computations are done on 4 NVIDIA Tesla V100 GPUs . 
3.4 Main Results The results of our strategies as well as the baselines are shown in Table 1 . 
All these values come from the evaluation of the development sets in each dataset due to the test sets are withheld . 
Nevertheless , our strategy achieves a consistent improvement compared with the independent approach and the vector - based conditional approach . 
The values with a bold type mean the winner across all strategies . 
As we can observe , the MaP Ewins 16 out of 16 in both BERT - base and BERT - large groups . 
It proves that the ensemble of both directions is helpful for the span extraction . 
In the BiDAF group , The MaP Eis also the best on all datasets compared with VCP . 
It shows the robustness of our matrix - based conditional approach in language models . 
The fact that the MaP Fwins 12 out of 12 in EM , and 8 out of 12 in F1 demonstrates that the matrix - based conditional approach is capable of predicting a clean answer span that matches human judgment exactly . 
We suppose the reason is that more start - end position pairs considered in the probability matrix can enhance the interaction and constraint between the start and end , thus , make the MaP Fperform more consistently in EM than in F1 . 
05001000150020002500 0.020.040.060.080.0 123456789 > 9 NO . 
of Samples by Answer LengthEM & F1 ( % ) Answer Length of Ground - truthNO . 
VCP - EM VCP - F1 MaP - EM MaP - F1 Figure 6 : EM and F1 of MaP Fand VCP based on BiDAF under different answer length . 
3https://github.com/huggingface/ pytorch - transformers 4https://github.com/allenai/allennlp3.5 Strategy Analysis Figure 6 shows how the performance changes with respect to the answer length , which is designed on HotpotQA . 
We can see that the matrix - based conditional approach works better than the vectorbased conditional approach as the span decrease in length . 
Since the short answers have a high rate in all answer spans , so the matrix - based conditional approach is better for the answer span task . 
In other words , this observation supports the ensemble of both directions as Edoes . 
The MaP Ecombining the MaP F ‚Äôs advantage in short answers and the VCP ‚Äôs advantage in long answers can get a better result than any of them . 
81.281.481.681.882.088.088.288.488.688.8102030405060708090100 EM ( % ) F1 ( % ) F1 EM k Figure 7 : Impact of hyper - parameter kin MaP Fon SQuAD 1.1 with BERT - base as the backbone . 
We investigate the impact of kused to choose the top probabilities in the training phase . 
The results are shown in Figure 7 . 
With the increase of k , the EM and F1 show a downtrend . 
The best performance happens at k=20 . 
We guess that choosing more probabilities makes the training difÔ¨Åcult and brings extra noises to the candidate positions . 
E.g. , ifkis set to 30 , the number of candidate probabilities will be 900 , which is larger than the sequence length 512 in vector - based conditional approach . 
02468 05k10k15k20k25k30k35kTraining Loss Training StepsNo - Sampling ( Vector - Based ) Sampling - Top20 ( Vector - Based ) Figure 8 : Convergence of sampling - based training strategy on BERT . 
We analyze the convergence of the sampling-693 based training strategy on SQuAD 1.1 . 
Due to the effectiveness of the sampling - based training strategy is proved in MaP , we conduct an further experiment under the VCP to prove its generalization . 
Figure 8 demonstrates the results . 
As our expectation , the sampling - based training strategy optimizes the model as training in whole samples . 
However , it will cost longer training steps to get the same loss compared with standard training . 
So our samplingbased training strategy is good for the training of the matrix - based conditional approach . 
4 Related Work Machine reading comprehension is an important topic in the NLP community . 
More and more neural network models are proposed to tackle this problem , including DCN ( Xiong et al . 
, 2017 ) , RNET ( Wang et al . 
, 2017 ) , BiDAF ( Seo et al . 
, 2017 ) , Match - LSTM ( Wang and Jiang , 2017 ) , S - Net ( Tan et al . 
, 2018 ) , SDNet ( Zhu et al . 
, 2018 ) , QANet ( Yu et al . 
, 2018 ) , HAS - QA ( Pang et al . 
, 2019 ) . 
Among various MRC tasks , span extraction is a typical task that extracting a span of text from the corresponding passage as the answer of a given question . 
It can well overcome the weakness that words or entities are not sufÔ¨Åcient to answer questions ( Liu et al . 
, 2019 ) . 
Previous models proposed for span extraction mostly focus on the design of architecture , especially on the representation of question and passage , and the interaction between them . 
There are few works devoted to the top - level design of span output , which refers to the probabilities generation from the representation . 
We divide the previous toplevel design into two categories , independent approach and conditional approach . 
The independent approach is to predict the start and end positions in the given passage independently ( Kundu and Ng , 2018a ; Yu et al . 
, 2018 ) . 
Although the independent approach has a simple assumption , it works well when the input features are strong enough , e.g. , combining with BERT ( Devlin et al . 
, 2019 ) , XLNet ( Yang and Song , 2019 ) , and SpanBERT ( Joshi et al . 
, 2019 ) . 
Nevertheless , since there is a kind of dependency relationship between start and end positions , the conditional approach has advancements over the independent approach . 
A typical work on the conditional approach comes from Wang and Jiang ( 2017 ) . 
They proposed two different models based on the Pointer Network . 
One is the sequence model which produces a se - quence of answer tokens as the Ô¨Ånal output , and another is the boundary model which produces only the start token and the end token of the answer . 
The experimental results demonstrate that the boundary model ( span extraction ) is superior to the sequence model on both EM and F1 . 
The R - NET ( Wang et al . 
, 2017 ) , BiDAF ( Seo et al . 
, 2017 ) , S - Net ( Tan et al . 
, 2018 ) , SDNet ( Zhu et al . 
, 2018 ) have the same output layer and inference phase with the boundary model in ( Wang and Jiang , 2017 ) . 
Lee et al . 
( 2016 ) presented an architecture that builds Ô¨Åxed length representations of all spans in the passage with a recurrent network to address the answer extraction task . 
The computation cost is decided by the max - length of the possible span and the sequence length . 
The experimental results show an improvement on EM compared with the endpoints prediction that independently predicts the two endpoints of the answer span . 
However , previous works related to the conditional approach are always based on a probability vector . 
We investigate another possible matrixbased conditional approach in this paper . 
Besides , a well - matched training strategy is proposed to our approach , and forward and backward conditional possibilities are also integrated to improve the performance . 
5 Conclusion In this paper , we Ô¨Årst investigate different approaches of span extraction in MRC . 
To improve the current vector - based conditional approach , we propose a matrix - based conditional approach . 
More careful consideration of the dependencies between the start and end positions of the answer span can predict their values better . 
We also propose a sampling - based training strategy to address the training process of the matrix - based conditional approach . 
The Ô¨Ånal experimental results on a wide of datasets demonstrate the effectiveness of our approach and training strategy . 
Acknowledgments This work was supported by National Key R&D Program of China ( 2019YFB2101802 ) and Sichuan Key R&D project ( 2020YFG0035 ) . 
Abstract Providing instant response for product - related questions in E - commerce question answering platforms can greatly improve users ‚Äô online shopping experience . 
However , existing product question answering ( PQA ) methods only consider a single information source such as user reviews and/or require large amounts of labeled data . 
In this paper , we propose a novel framework to tackle the PQA task via exploiting heterogeneous information including natural language text and attribute - value pairs from two information sources of the concerned product , namely product details and user reviews . 
A heterogeneous information encoding component is then designed for obtaining uniÔ¨Åed representations of information with different formats . 
The sources of the candidate snippets are also incorporated when measuring the question - snippet relevance . 
Moreover , the framework is trained with a speciÔ¨Åcally designed weak supervision paradigm making use of available answers in the training phase . 
Experiments on a real - world dataset show that our proposed framework achieves superior performance over state - of - the - art models . 
1 Introduction To help potential consumers address their concerns during online shopping , many E - commerce sites now provide a community question answering ( CQA ) platform , where users can post questions for a speciÔ¨Åc product , and others can voluntarily answer them . 
Very often , it takes a long time for an asker to wait for an answer on such platforms . 
Therefore , automatically providing a proper response to a product - related question can greatly improve user online shopping experience and stimulate purchase decisions . 
‚àóThe work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region , China ( Project Code : 14200719).Several efforts have been made to tackle such product - related question answering ( PQA ) task ( McAuley and Yang , 2016 ; Yu et al . 
, 2018a ; Gao et al . 
, 2019 ; Chen et al . 
, 2019b ; Deng et al . 
, 2020b ) . 
The existing methods can be generally categorized regarding the involved information source , i.e. , from where the responses are obtained . 
A pioneer work by McAuley and Yang ( McAuley and Yang , 2016 ) investigates answer selection via detecting clues from user reviews . 
From then on , the review set becomes a commonly used auxiliary information for predicting the answer types or distinguishing true answers from randomly sampled ones ( Wan and McAuley , 2016 ; Yu and Lam , 2018 ) . 
However , these methods are not feasible for newly - posted questions without candidate answers . 
A recent approach for PQA task is to directly extract review sentences as the response for a given question ( Chen et al . 
, 2019a ) . 
But it requires a large number of labeled question - review pairs , whose annotation is a time - consuming and laborious work . 
Other information sources , such as existing QA collections , are also exploited ( Yu et al . 
, 2018b ) , but relevant QA pairs are assumed to be always available for a new question in their setting , which is uncommon in practice . 
Besides user reviews , another kind of information , namely product details provided by the manufacturer are always available and can be an important information source for addressing productrelated questions . 
For example , considering the question ‚Äú How large is the keyboard ‚Äù for the product shown in Figure 1 , the attribute - value pair ‚Äú Item Dimensions : 10.9√ó4.8√ó0.6 in ‚Äù from the speciÔ¨Åcation table can be a good response . 
Such information can be essential for questions looking for factual type information due to their reliability and preciseness , but they are often underutilized in previous works . 
The above scenario motivates our task of answering product - related questions via exploit-696 Logitech K380   Multi - Device Bluetooth KeyboardProduct Details Product DetailsUser Reviews QA PairsFigure 1 : A sample E - commerce product associated with its product details , user reviews , and QA pairs ing the information from both product details and user reviews to obtain relevant snippets serving as responses for improving user satisfaction . 
This task presents some new research challenges : ( i ) The heterogeneity of candidate information needs to be appropriately handled . 
From the above example , we can see that there exists both attributevalue pairs and natural language texts as candidate responses , which implies that typical answer selection approaches ( Tan et al . 
, 2016 ; Wang et al . 
, 2017 ; Rao et al . 
, 2019 ) are incapable of handling the concerned task . 
( ii ) Product details and user reviews contain different types of information , which are suitable for answering questions with different information needs . 
Returning to the example in Figure 1 , considering a more subjective question asking about user experience ‚Äú How is the key travel ‚Äù , snippets from reviews such as ‚Äú ... good key travel and solid feel .. ‚Äù can provide more appropriate responses . 
Thus , we can observe that questions with different intents can be better answered by snippets from different sources , which should be exploited when measuring the question - snippet relevance . 
( iii ) Training a model to capture the relevance between a question and a candidate snippet with typical supervised paradigms requires a large volume of labeled data . 
However , it is very timeconsuming to manually label the question - snippet pairs in the PQA task due to the product - speciÔ¨Åc nature of questions and candidate snippets ( Chen et al . 
, 2019a ) , which demands a better solution for training such models . 
To tackle these challenges , we propose a novel framework for the PQA task using Heterogenous Information via a Weak Supervision paradigm ( HIWS ) . 
Given a product - related question , HIWS exploits the corresponding product details and user reviews to return a ranked snippet list serving as the response . 
SpeciÔ¨Åcally , a heterogeneous infor - mation encoding component is Ô¨Årst developed to encode different information formats into a uniÔ¨Åed representation composed of a free text sentence and a set of focused aspects . 
Then for measuring the question - snippet relevance , a gated fusion approach is designed to get aspect - enhanced representations . 
Also , a question intent analysis module is designed to better determine which information source is more suitable for providing responses . 
To handle the shortage of labeled data for model training , we develop a weak supervision paradigm making use of the original user - posted answers during training . 
Some external resources including pre - trained language models such as BERT ( Devlin et al . 
, 2019 ) are utilized to obtain weak supervision signals to facilitate the training process . 
Our main contributions are as follows : ‚Ä¢We explore to utilize heterogeneous information including attribute - value pairs and natural language sentences from both product details and user reviews to tackle the PQA task . 
‚Ä¢To handle the lack of labeled data , we design an effective weak supervision paradigm making use of available answers in training phase . 
‚Ä¢Experiments on real - world E - commerce dataset show that our proposed model achieves superior performance over state - of - the - art models . 
2 The Proposed Framework For a product p , its associated information can be represented as a tuple Cp= ( A , D , R ) , where A={(ai , vi)}is a set of attribute - value pairs extracted from the corresponding speciÔ¨Åcation table . 
D={di}denotes the textual product description snippets represented by di , R={ri}denotes the review set composed of review snippets represented byri . 
Now given a question qregarding the product p , our task is to automatically rank the candidate697 snippets inCp , which can either be a textual sentence fromDorR , or an attribute - value pair from Afor providing responses to the question q. As shown in Figure 2 , HIWS mainly consists of three components : heterogeneous information encoding , question - snippet relevance matching , and automatic label construction . 
Concretely , the candidate snippets are Ô¨Årst transformed into uniÔ¨Åed representations . 
Then we measure the questionsnippet relevance both from their aspect - enhanced representations and the intent matching . 
The overall model is then trained using the automaticallyconstructed labels via making use of the original answer to the given question . 
2.1 Heterogeneous Information Encoding Heterogeneous Information UniÔ¨Åcation Given the heterogeneous candidate snippets including natural language sentences and attribute - value pairs , we transform them into uniÔ¨Åed representations . 
It can be observed that these two types of information are actually complementary to each other where the attribute term in an attribute - value pair can well indicate the major focus of such snippet , while a textual sentence can usually provide more detailed semantic information . 
To highlight the focus of a natural language sentence ¬Øc‚ààD‚à™R , we can extract maspect terms : ca={ca 1,ca 2, ... ,ca m}= AE(¬Øc ) ( 1 ) where AE(¬∑)refers to a reasonable aspect extraction algorithm such as ( He et al . 
, 2017 ) used in our experiments . 
caare the extracted maspects . 
These extracted aspects are typically not exactly the same as the terms in the attribute set , but they play a similar role as characterizing the focus of the candidate snippet . 
For an attribute - value pair ( ai , vi)‚ààA , since the main focus of such a snippet is already highlighted by the attribute term ai , we directly treat aias the aspectcaand construct a pesudo - sentence ctby concatenating the attribute and value terms . 
To this end , any raw snippet ÀÜc‚ààC p , regardless of its original information type ( i.e. , whether it is an attribute - value pair or a natural language sentence ) , is mapped to a uniÔ¨Åed representation , denoted as c , as follows : c= ( ct , ca),whereca={ca 1,ca 2, ... ,ca m}(2 ) wherectis the textual sentence of ÀÜc . 
Such a uniÔ¨Åed representation facilitates effective processing ofdifferent input formats and also enriches the input representation for later process . 
Snippet Encoding We next encode the uniÔ¨Åed candidate snippet representation cand the questionqto vector representations . 
We Ô¨Årst employ an embedding layer to transform each word into their corresponding word vector . 
The embedding of the word wis denoted as ew= [ ec w;eg w ] , which is a concatenation of character - level embedding ec w and word - level embedding eg w. A bidirectional long short - term memory ( Bi - LSTM ) network is then employed to encode the local context information for each word in the question and the textual sentence ctof the candidate snippet , which generates the context - aware question and snippet representations as follows : hq i= Bi - LSTM ( eq i , hq i‚àí1),i‚àà[1,lq](3 ) hc i= Bi - LSTM ( ec i , hc i‚àí1),i‚àà[1,lc](4 ) whereh‚àó iis the hidden state of the encoder at thei - th time step . 
lqandlcare the length of the corresponding sequence . 
We denote the contextaware question and snippet representation as Hq‚àà Rlq√ódhandHc‚ààRlc√ódhrespectively , where dhis the number of hidden units of the LSTM network . 
Besides the free text part , there are also maspects for each candidate snippet c. They are useful when measuring the relevance between qandc since they can be regarded as the most salient part of the candidate snippet . 
Unlike a textual sentence , aspect terms are often quite short , so we directly employ the character - level embedding to transform each aspect term ca ito a vector representation denoted asha i : ha i = ec ca i= MaxPool(Conv ( ca i ) ) ( 5 ) where MaxPool(¬∑)andConv(¬∑)denote the maxpooling and convolutional operations ( Kim , 2014 ) . 
2.2 Question - Snippet Relevance Matching Aspect - enhanced Representations To utilize the aspect information , we design a gated attention mechanism to highlight the relevant information in the question q. SpeciÔ¨Åcally , for the k - th word in the context - aware question representation , denoted asHq k , we measure the relative importance Œ±k[i]of this word given the i - th aspect term : Œ±k[i]=exp / parenleftbig ( Hq k)Tha i / parenrightbig /summationtextlq j=1exp / parenleftBig ( Hq j)Tha i / parenrightBig ( 6)698 concat HqHcha1ha2 ‚Ä¶ ÀúHqfwdxqc ( info source)Bilinear Attentionu‚äóvqvcSemantic EncodingoqocAŒ≤ÃÇyOriginal QA Pair(q , a)weak   supervisionyLLoss QuestionqWord - level EmbeddingChar - level EmbeddingContext - aware Encodingfree textctaspectsca1,ca2, ... ,camRaw snippet ÃÇcHeterogeneous Information DistillingHeterogeneous   Info EncodingQ - S Relevance   MatchingAutomatic Label   ConstructionFigure 2 : The architecture of proposed HIWS model Since there are in total maspects for a given candidate snippet c , we can similarly obtain Œ±k[1],Œ±k[2], ... ,Œ± k[m]attention scores for the k - th question word . 
These attention scores reÔ¨Çect different relative associations of the concerned word with different aspects . 
Then for every word in the questionq , we can obtain these attention scores , giving us an attention matrix A‚ààRlq√óm . 
To get one compositive attention weight for each word in the question , we apply a gated fusion approach to combine these aspects . 
SpeciÔ¨Åcally , a linear transformation is employed as a gate to learn an appropriate combination between these different attention weights as follows : Œ≤ = tanh(WaAT+ba ) ( 7 ) whereŒ≤‚ààRlqdenotes the relative importance of each word in the question q , Waandbaare trainable parameters . 
Then we can utilize the combined attention weight to obtain an aspect - enhanced question representation oq : oq=/summationdisplaylq k=1Hq k¬∑Œ≤k ( 8) Hereoqrepresents the question representation with an enhancement from multiple aspects of the candidate snippet , which captures the relevance information between qandcfrom the view of aspect terms . 
Based on the intuition that explicitly highlighting these aspects in ctis also helpful to capture its major information , we apply similar operations to Hc , giving an aspect - aware snippet representation oc . 
Question Intent Analysis for Multi - source Candidate Information The question intent helpsidentify what type of information the user is looking for and how to respond them . 
For example , it can be much more helpful to respond a question asking about personal experience with snippets from reviews . 
In contrast , the product details will be more suitable and convincing for a question looking for concrete product speciÔ¨Åcations . 
Thus , a question intent matching module is designed to detect such matching signals . 
It can be observed that the beginning words of a question often have stronger ability for indicating the question intent . 
Thus , given the question representationHq , a weight decay function fwd()is applied on it to emphasize the importance of the beginning words . 
Precisely , for the i - th word in the question , we multiply Hq ibyni , wheren‚àà(0,1 ) can be set in advance such as n= 0.9used in our experiments or learned with the model . 
Then we can obtain the encoded question representation rq as follows : /tildewideHq i = fwd i(Hq i ) = ni‚äóHq i ( 9 ) rq=/summationdisplaylq i=1 / tildewideHq i ( 10 ) where‚äórefers to the element - wise multiplication . 
We denote the question representation after such transformation as rq . 
Then given a one - hot feature vectoru‚ààR2of the candidate snippet cindicating its information source i.e. , from product details or user reviews . 
A bilinear attention layer is employed to achieve the question intent matching analysis : xqc= tanh ( rqWmu+bm ) ( 11 ) whereWmandbmare trainable parameters , xqcdenotes a low - dimensional vector reÔ¨Çecting the intent matching between the question and the candidate snippet . 
Matching Signal Aggregation and Prediction After obtaining the aspect - enhanced representations and the question intent matching signals , we also employ a Siamese architecture to encode Hq andHcwith another Bi - LSTM encoder for capturing their main semantic information : vq= Bi - LSTM lq(Hq ) ( 12 ) vc= Bi - LSTM lc(Hc ) ( 13 ) We usel‚àóas the subscripts in the above equations to differentiate it from Equation ( 3 ) indicating that only the last hidden state is taken as the encoded699 representation . 
By utilizing the same sentence encoder , it helps map them into the same semantic space for determining their semantic relevance . 
Then these different matching signals can be aggregated and fed to a MLP layer to make the Ô¨Ånal prediction ÀÜy : ÀÜy= MLP([vq;vc;oq;oc;xqc ] ) ( 14 ) where the aggregated vector contains matching features from different perspectives including the core semantic information vqandvc , the aspectenhanced representations oqandocwhich highlight the major focuses discussed in each sequence , as well as the question intent matching signals xqc containing information about which information source is better for answering the concerned question regarding its intent . 
The overall model is then trained to minimize the cross entropy loss between the predicted relevance score ÀÜyand the automatically - constructed label y which will be introduced in the next section : L=‚àí1 NN / summationdisplay n=1[ÀÜynlogyn+ ( 1‚àíÀÜyn ) log ( 1‚àíyn ) ] ( 15 ) where ÀÜynandyndenote the prediction and label of then - th training instance , Nis the total number of training instances . 
2.3 Automatic Label Construction In order to learn a matching function between the question and candidates , the most typical approach is to utilize a large number of annotated sentence pairs ( Chen et al . 
, 2019a ) to conduct the training . 
However , this manual solution is not effective in PQA settings due to the large volume of candidate snippets and the product - speciÔ¨Åc nature of questions and candidates . 
Fortunately , we can take advantage of the original user - posted answers to their corresponding questions via a weak supervision paradigm during the training phase which has been successfully applied to provide imperfect labels but with far more less human efforts in many NLP tasks such as knowledge - base completion ( Hoffmann et al . 
, 2011 ) and sentiment analysis ( Severyn and Moschitti , 2015b ) etc . 
Given a question q , we have its answer aduring the training phase as auxiliary information to obtain the labelyfor the candidate snippet c. To make use of the information of the whole QA pair , the entire QA pair ( q , a)is Ô¨Årst fused to an integrated textualsnippetpqawith some heuristic rules ( details are given in Sec 3.3 ) . 
Then the problem of obtaining the relevance label between candqare cast as measuring the relation between ctwithpqa . 
We measure such relation from two perspectives , namely , syntactic relevance and semantic relevance . 
Syntactic Relevance . 
Word overlapping between two text items can be a strong signal indicating their relevance . 
Here we adopt the idea of ROUGE ( Lin , 2004 ) which is initially proposed for computing a recall - based word overlapping score to compute the syntactic - level relevance score s1 : s1= ROUGE-1 ( ct , pqa ) + ROUGE-2 ( ct , pqa ) ( 16 ) where ROUGE - N refers to the overlap of N - grams betweenctandpqa . 
Semantic Relevance . 
To address the issue of the semantic gap between two text items , many word and sentence embedding models have been proposed and successfully applied to many NLP tasks recently . 
Here , we utilize some pre - trained text embedding models to compute the semantic relevance between the integrated QA snippet pqaandct : si= cos(Pre - TE i(pqa),Pre - TE i(ct ) ) ( 17 ) where Pre - TE refers to a pre - trained text encoder . 
We adopt GloVe ( Pennington et al . 
, 2014 ) , Elmo ( Peters et al . 
, 2018 ) and BERT ( Devlin et al . 
, 2019 ) in our experiments . 
cos(¬∑)denotes the cosine similarity score between the two encoded sentence representations . 
We denote the computed relevance scores with the aforementioned pre - trained models ass2,s3,s4respectively . 
After obtaining these relevance signals , a small amount of human - annotated question - snippet pairs are used to train a simple classiÔ¨Åer for learning to combine these signals into the single label y1 . 
Note that it seems to be unnecessary to design any framework if a simple classiÔ¨Åer with a few amount of labeled data and some pre - trained models can achieve a high accuracy . 
This is because we use the information of the entire QA pair to obtain the labelydenoting the question - snippet relevance , which is different when we only have the question qand needs to retrieve relevant snippets during the testing phase . 
Thus a simple classiÔ¨Åer with a few amount of labeled data can learn to integrate these relevance scores for the construction of ‚Äú gold ‚Äù labels with the help of original answers . 
140 questions with their candidate snippets are annotated for this purpose , a SVM classiÔ¨Åer is used in our experiment.700 3 Experiments 3.1 Dataset We perform experiments on real - world data to validate the model effectiveness . 
The question - answer pairs and reviews are drawn from the Amazon QA dataset ( McAuley and Yang , 2016 ) and Amazon review dataset ( Ni et al . 
, 2019 ) . 
Product details are crawled from the corresponding products ‚Äô pages and incorporated into our dataset . 
In this way , we construct a heterogeneous dataset , which includes in total 5,395 QA pairs of 3,840 products spanning three product categories , namely , ‚Äú Cell Phones and Accessories ‚Äù , ‚Äú Sports and Outdoors ‚Äù and‚ÄúTools and Home Improvement ‚Äù . 
For each question , we Ô¨Årst utilize the BM25 algorithm to conduct an initial Ô¨Åltering and collect the 50 top - ranked snippets from the corresponding product information as candidate snippets . 
After discarding empty or meaningless strings , we obtain 219,563 question - candidate snippet pairs in total . 
The dataset is split for training / validation / testing as 4,023 / 779 / 593 questions respectively , which results in 163,063 / 32,178 / 24,322 question - snippet pairs in each set . 
To obtain training and validation set , we utilize the weak supervision paradigm described in Sec 2.3 to automatically construct labels . 
For the testing set , in order to evaluate the effectiveness of the whole framework , the relevance labels between the questions and candidate snippets are annotated manually by two trained human annotators , the disagreements of the annotations are resolved by another experienced annotator 3.2 Baselines and Evaluation Metrics To compare with our proposed framework , we adopt several strong baseline and state - of - the - art question answering models , including CNN ( Severyn and Moschitti , 2015a ) , QA - LSTM ( Tan et al . 
, 2016 ) , MatchPyramid ( Pang et al . 
, 2016 ) , BiMPM ( Wang et al . 
, 2017 ) , Conv - KNRM ( Dai et al . 
, 2018 ) , HCAN ( Rao et al . 
, 2019 ) for comparisons . 
These models take the question and natural language sentence part of the candidate snippet as input , and are trained using the same automaticallyconstructed labels derived from original QA pairs as our proposed HIWS framework . 
Two retrieval - based unsupervised models are also adopted : ( 1 ) BM25 : It is a widely - used bagof - words retrieval model . 
( 2 ) QCEM : Question Candidate Embedding Matching is an unsupervised method that sums the word vectors of each sentenceTable 1 : Response Selection Performance MAP MRR P@5 P@10 BM25 0.417 0.549 0.296 0.234 QCEM 0.479 0.623 0.385 0.278 CNN 0.576 0.665 0.430 0.329 QA - LSTM 0.561 0.656 0.419 0.327 MatchPyramid 0.630 0.700 0.466 0.353 BiMPM 0.613 0.683 0.458 0.336 Conv - KNRM 0.615 0.696 0.457 0.337 HCAN 0.632 0.710 0.459 0.339 HIWS 0.674 0.749 0.498 0.363 as the sentence embedding , and cosine similarity is utilized for predicting sentence relevance . 
For evaluation metrics , Mean Average Precision ( MAP ) , Mean Reciprocal Rank ( MRR ) , and Precision at N ( P@N ) are used to measure the performance . 
Precision at N ( P@N ) is the precision of the N retrieved snippets . 
We set N=5 and N=10 which correspond to P@5 and P@10 respectively in our experiments . 
3.3 Implementation Details For the automatic label construction , we Ô¨Årst utilize the user - posted answer to paraphrase the question for obtaining the integrated snippet pqaaccording to the part - of - speech tags and syntactic structure of the question with heuristic rules . 
For example , for a question ‚Äú does it have a front - facing camera ? ‚Äù with the answer ‚Äú No . 
‚Äù , it will be combined to ‚Äú It does not have a front - facing camera ‚Äù . 
For the network architecture , we initialize the word embedding layer with the pre - trained 300D GloVe word vectors ( Pennington et al . 
, 2014 ) . 
The sizes of the CNN Ô¨Ålters in the character - level embedding are set to [ 2,3,4,5 ] , each with 75 Ô¨Ålters , resulting in 300D character - level embedding for each word . 
The hidden dimension of the contextaware Bi - LSTM encoder is set to 150 , with the dropout rate being 0.3 . 
The hidden dimension of the sentence encoder in Eq . 
( 12 ) is set to 64 , with the dropout rate also being 0.3 . 
The hidden dimensions of the MLP layer in the Ô¨Ånal prediction layer are set to 300 and 100 respectively , with ReLU as the activation function . 
All models are trained with the batch size of 100 . 
The number of aspects m for each candidate snippet is set to be 3 which is a moderate number for a single sentence.701 Table 2 : Effectiveness of Weak Supervision Paradigm BiMPM HCAN HIWS MAP MRR MAP MRR MAP MRR with QA 0.338 0.409 0.329 0.402 0.310 0.393 with SQS 0.443 0.492 0.432 0.495 0.479 0.556 with WS 0.613 0.683 0.632 0.710 0.674 0.749 3.4 Quantitative Evaluation Results Response Selection Performance The evaluation results are presented in Table 1 , which demonstrates that our proposed HIWS achieves the best performance among all evaluation metrics compared with both retrieval - based solutions and supervised QA matching methods . 
We can observe that some simple QA models such as QA - LSTM and unsupervised models such as QCEM can still achieve reasonable performance . 
For those state - of - theart models such as BiMPM and HCAN , although equipped with complicated network architecture , they do not perform as promising as expected . 
Such a result is due to the fact that these QA models merely focus on the matching between text items and ignore some important characteristics in the Ecommerce scenario such as the heterogeneous information formats and multiple information sources of the candidate snippets . 
HIWS exploits such characteristics and utilize the extracted aspects to obtain enriched representations , leading to its superior performance . 
Effectiveness of Proposed Weak Supervision Paradigm We investigate two alternative strategies for tackling the shortage of labeled data and compare them with our proposed weak supervision strategy to examine its effectiveness . 
The results on the same test set are reported in Table 2 . 
SpeciÔ¨Åcally , we train HIWS and two baselines , namely BiMPM and HCAN with different methods : ‚Äú with QA ‚Äù denotes training with the QA pairs instead of question - snippet pairs as in Table 1 . 
We treat questions with their original answers as the positive samples and other randomly selected answers as negative samples for model training ; ‚Äú with SQS ‚Äù refers to models which are Ô¨Årst trained with QA pairs , then the Small number of annotated Question- Snippet pairs introduced in Sec 2.3 are used to Ô¨Åne - tune the model ; ‚Äú with WS ‚Äù means the model is trained with the proposed weak supervision approach . 
Comparing these model variants , we can observe that models trained with the original QA pairs perform quite worse , showing theTable 3 : Ablation study for components in HIWS Ablation of HIWS MAP MRR w/o syntactic relevance score 0.273 0.434 w/o semantic relevance score 0.543 0.626 w/o question intent matching 0.667 0.737 w/o aspect - enhanced representations 0.631 0.704 HIWS 0.674 0.749 semantic gap between the original answers and the candidate snippets needs to be handled properly . 
Models with SQS outperform models with QA via Ô¨Åne - tuning with proper data , but it still failed to achieve satisfactory results due to the limited amount of labeled data . 
However , performance for all models can be improved with our proposed weak supervision paradigm , demonstrating its effectiveness on utilizing original answer information for bridging the connection between the question and snippets in the E - commerce settings . 
Ablation Analysis We conduct ablation analysis to investigate the effectiveness of some important components in HIWS as shown in Table 3 . 
We Ô¨Årst create two sets of training labels whose construction step only involves one kind of relevance scores introduced in Sec 2.3 , denoted as ‚Äú w/o syntactic relevance score ‚Äù and ‚Äú w/o semantic relevance score ‚Äù respectively . 
It can be observed that these two kinds of linguistic considerations , especially the syntactic relevance , are quite essential for automatically obtaining the labels for conducting training and thus directly inÔ¨Çuence the Ô¨Ånal performance of our model . 
Another two important components in HIWS are the aspect - enhanced representations and the question intent matching . 
As shown in Table 3 , these two components contribute to some performance boost , especially the aspect - enhanced module . 
For constructing the variant model without aspect - enhanced representations , we still feed the embedded aspect ha iinto the aggregation layer . 
Thus , even without considering the interaction between aspects and the question as in HIWS , this variant still outperforms some baselines . 
Performance with Different Amount of Data We further investigate the robustness of HIWS via examining its performance with different amount of training data . 
The MAP and MRR scores under each product category are reported in Figure 3 , where ‚Äù w / ndata ‚Äù refers to HIWS trained with n proportion of the entire training data . 
It can be observed that even when we use a moderate amount of training data such as 3/4 training data , the per-702 Figure 3 : Performance with Different Amount of Data formance does not drop signiÔ¨Åcantly . 
Such results show the robustness of our proposed model implying that it can effectively utilize the available QA pairs to automatically construct useful training signals for learning the question - snippet relevance relation . 
3.5 Case Study To gain some insights into HIWS , we present two sample questions with the top - one responses given by HIWS and two strong existing methods in Table 4 . 
The information sources of each snippet are marked , whereA , D , Rrefers to attribute - value pairs , product textual descriptions and reviews respectively . 
Following each information source symbol , the correctness of the retrieved response is given . 
From the results , we can observe that HIWS successfully handles candidate snippets from different sources to answer the product questions with different information needs . 
For example , it precisely retrieves the corresponding attribute of the product for Question-1 , which is more reliable and precise than the snippet retrieved from the review set by the existing models . 
Moreover , HIWS correctly handles the second question while the focused aspect is missing in responses from other methods . 
This is likely due to the aspect - enhanced representations for highlighting the major focus in the question and snippets . 
This result shows the necessity of effectively exploring different types of information of the concerned product instead of considering a single information source as in previous works . 
4 Related Work In recent years , many deep learning based methods have been proposed for the answer selection task in community question answering ( CQA ) platforms . 
These models can be generally categorized into two types according to their network architecture ( Lai et al . 
, 2018 ) , namely Siamese networks ( Tan et al . 
, 2016 ; Mueller and Thyagarajan , 2016 ) and Compare - Aggregate networks ( WangTable 4 : Two sample questions with the top - one responses returned by HIWS and two existing models . 
The information sources and the gold labels of the snippets are also marked out in the parentheses at the end of each snippet respectively . 
Question-1 : What is the overall length of this bulb ? HIWS : Product Dimensions : 6.5 x 2.5 x 2.5 inches ( A ) ( /check ) MatchPyramid : I decided to try using these before i went more expensive route , the bulb are indeed quite large the length of a hand perhaps . 
( R ) ( √ó ) HCAN : I will update this review to render my durability opinion , one last note pay attention to the length of these bulb . 
( R ) ( √ó ) Question-2 : Will this work with my unlocked Ô¨Åre phone i have straight talk i want to switch to the amazon Ô¨Åre phone . 
HIWS : Sim card will only work with an att compatible or unlocked gsm phone ( D ) ( /check ) MatchPyramid : Keep your current phone number . 
Works with SIMs , IM , social networks , email , and web . 
( D ) ( √ó ) HCAN : I have tmobile and the service is not good in my area so i want to switch to straight talk ( R ) ( √ó ) et al . 
, 2017 ; Rao et al . 
, 2019 ; Deng et al . 
, 2020a ) . 
Product - related Question Answering ( PQA ) problem has drawn a lot of attention recently , due to the increasing popularity of online shopping . 
Most of the existing works utilize reviews as their major information to provide responses for a given question . 
McAuley and Yang ( 2016 ) treat reviews as ‚Äú experts ‚Äù to handle the answer selection task . 
Later , product aspects are considered to further improve the performance ( Yu and Lam , 2018 ) . 
Chen et al . 
( 2019a ) propose to tackle PQA task by directly retrieving review sentences as answers . 
However , it requires a large number of labeled questionreview pairs . 
Yu et al . 
( 2018b ) assume that relevant QA pairs are always available for a given question which can be utilized to provide the responses . 
Some other works formulate the PQA task as a reading comprehension problem ( Xu et al . 
, 2019 ) , where the main focus is to extract a text span as the answer given a relevant review , which is unavailable in many cases . 
Given some successful applications of text generation models such as text summarization ( Rush et al . 
, 2015 ) and response generation ( Tao et al . 
, 2018 ) , some models are proposed to generate an answer sentence ( Gao et al . 
, 2019 ; Chen et al . 
, 2019b ) given relevant product information , some later works speciÔ¨Åcally consider the user opinion information during such generation process ( Deng et al . 
, 2020b ) . 
Since most product - related questions are looking for diverse answers , we argue that information extracted from reliable sources is more effective and explainable703 solution for the PQA task . 
More recently , some studies consider the answer helpfulness prediction task ( Zhang et al . 
, 2020b ) and answer ranking problem ( Zhang et al . 
, 2020a ) in the context of PQA , assuming the existence of user - provided answers to a given question . 
Different from them , we aim to provide instant responses for a newly - posted question in E - commerce . 
5 Conclusions We propose a novel framework for answering product - related questions via exploiting heterogeneous information including attribute - value pairs and free text sentences from both product details and user reviews . 
To tackle the shortage of labeled data , we design a weak supervision paradigm by making use of the existing QA pairs to automatically construct labels for training . 
Extensive experiments conducted on a real - word dataset demonstrate the superiority of our proposed framework . 
Abstract An NLP model ‚Äôs ability to reason should be independent of language . 
Previous works utilize Natural Language Inference ( NLI ) to understand the reasoning ability of models , mostly focusing on high resource languages like English . 
T o address scarcity of data in low - resource languages such as Hindi , we use data recasting to create four NLI datasets from existing four text classification datasets in Hindi language . 
Through experiments , we show that our recasted dataset1is devoid of statistical irregularities and spurious patterns . 
W e study the consistency in predictions of the textual entailment models and propose a consistency regulariser to remove pairwise - inconsistencies in predictions . 
F urthermore , we propose a novel two - step classification method which uses textual - entailment predictions for classification task . 
W e further improve the classification performance by jointly training the classification and textual entailment tasks together . 
W e therefore highlight the benefits of data recasting and our approach2 with supporting experimental results . 
1 Introduction T extual entailment ( TE ) is the task of determining if a hypothesis sentence can be inferred from a given context sentence . 
Figure 1shows examples of context - hypothesis pairs for TE . 
Previous works ( W ang and Zhang , 2009 ; T atu and Moldovan , 2005 ; Sammons et al . 
, 2010 ) investigated several semantic approaches for TE and demonstrated how they can be used to evaluate inference - related tasks such as Ques1https://github.com/midas-research/ hindi - nli - data 2https://github.com/midas-research/ hindi - nli - codetion Answering ( QA ) , reading comprehension ( RC ) and paraphrase acquisition ( PA ) . 
Context - Hypothesis Label p : The kid exclaimed with joy . 
entailed h : The kid is happy . 
p : I am feeling happy . 
not - entailed h : I am angry . 
( contradictory ) T able 1 : Example illustrating context ( c ) - hypothesis ( h ) pairs for the task of textual entailment . 
Researchers have curated many resources3 and benchmark datasets for TE in English ( Bowman et al . 
, 2015 ; Williams et al . 
, 2018 ; Khot et al . 
, 2018 ) . 
However , to our knowledge , there is only one TE dataset ( XNLI ) in Hindi , which was created by translating English data ( Conneau et al . 
, 2018 ) and another in HindiEnglish code - switched setting ( Khanuja et al . 
, 2020 ) . 
Hindi is the language with the fourth most native speakers in the world4 . 
Despite its wide prevalence , Hindi is still considered a low - resource language by NLP practitioners because there are a rather limited number of publicly available annotated datasets . 
Developing models that can accurately process text from low - resource languages , such as Hindi , is critical for the proliferation and broader adoption of NLP technologies . 
Creating a high - quality labeled corpus for TE in Hindi through crowd - sourcing could be challenging . 
In this paper , we employ a recasting technique from Poliak et al . 
( 2018a , b ) to convert four publicly available text classification datasets in Hindi and pose them as TE problems . 
In this recasting process , we build template hypotheses for each class in the label taxonomy . 
Then , we pair the original anno3https://aclweb.org/aclwiki/Textual _ Entailment_Resource_Pool 4https://en.wikipedia.org/wiki/List_of _ languages_by_number_of_native_speakers706 tated sentence with each of the template hypotheses to create TE samples . 
Unlike XNLI , our dataset is based on the original Hindi text and is not translated . 
F urthermore , the multiple annotation artefacts ( T an et al . 
, 2019 ) present in the original classification data are leveled out for the T extual entailment task on the recasted data due to label balance5 . 
W e evaluated state - of - the - art language models ( Conneau et al . 
, 2019 ) performance on the recasted TE data . 
W e then combine the predictions of related pairs ( same premise ) from TE task to predict the classification labels of the original data ( premise sentence ) , a twostep classification . 
W e observed that a better TE performance on the recasted data leads to higher accuracy on the followed classification task . 
W e also observed that TE models can make inconsistent predictions across samples derived from the same context sentence . 
Driven by these observations , we propose two improvements to TE and classification modeling . 
First , we introduce a regularisation constraint based on the work of ( Li et al . 
, 2019 ) that enforces consistency across pairs of training samples , thus correcting inconsistent predictions . 
Second , we propose a joint objective for training TE and classification simultaneously . 
Our results demonstrate that the regularization constraint and joint training helps improve the performance of both the TE models and the followed classification task . 
Though our work demonstrates the use of recasting and modeling improvements for TE in Hindi , we expect these techniques can be applied to other low - resource languages and other semantic phenomenon beyond textual classification . 
F ollowing are the main contributions of this work : 1 . 
W e develop new NLI datasets for a lowresource language Hindi using recasting ( Section 3 ) and evaluated state - of - the - art language models on them ( Section 4.1 ) . 
2 . 
Based on our analysis of inconsistencies in the predictions of TE models , we propose a new regularisation constraint ( Section 4.1.1 ) . 
5See Appendix Section A.4 for other benefits of recasting data.3 . 
W e propose a two - step classification approach that uses TE predictions from context - hypothesis pairs to predict the labels of the original classification task ( Section 4.2 ) . 
4 . 
W e propose a novel joint - training objective paired with consistency regularisation to obtain state - of - the - art performance for text classification on four Hindi datasets ( Section 4.2.1 ) . 
2 Related W ork In this section , we list some of the related works in the field of NLI as well as challenges encountered in low - resource settings . 
2.1 Natural Language Inference Recent studies in the field of NLI have emphasized the role of TE for estimating language comprehensibility of the models . 
White et al . 
( 2017 ) takes into consideration the need to leverage the existing pool of annotated collections as targeted textual inference examples ( such as pronoun resolution and sentence paraphrasing ) . 
Poliak et al . 
( 2018b ) discussed existing biases in NLI datasets which helps the models to perform well on Hypothesisonly baselines . 
Poliak et al . 
( 2018a ) analysed NLI datasets based on various semantic phenomenon to verify the ability of a model to perform unique , varied levels of reasoning . 
It performs data recasting on existing classification datasets to obtain a conventional context / hypothesis / label for common NLI tasks . 
Several modifications have been tried over baseline models for enhanced NLI and NLU . 
Liu et al . 
( 2019 ) focuses on NLU over crosstask data to achieve generalisability over new unseen tasks . 
Li et al . 
( 2018 ) incorporates attention mechanism to capture semantic relations in between individual words of the sentence for robust encodings . 
However , NLI has mostly revolved around English language . 
Our approach is motivated by such studies to analyse NLU using current embeddings for low - resource languages likeHindi . 
Bhattacharyya ( 2012 ) discusses some of the key challenges associated with Hindi , for example , grammatical constraints for most words to be masculine / feminine ( similar to F rench and unlike English ) , which makes707 semantic tasks like pronoun resolution , paraphrasing tough . 
2.2 NLP for Low - Resource Languages In a plethora of diverse languages , only a handful of them have plenty of labeled resources for data - driven analysis and advancements ( Joshi et al . 
, 2020 ) . 
Data in low - resource languages is either unlabeled or resides in spoken dialect than texts . 
There have been recent efforts using curriculum learning for making pretrained language models for several multi - lingual tasks ( Conneau et al . 
, 2018 , 2019 ) . 
However , many such languages give rise to creoles , building new mixed languages at the interface of existing languages . 
One such example is Hinglish ( Hindi + English ) that has widely been taken over in the form of tweets and social media messages . 
Attempts have been made to study linguistic tasks like language identification , NER ( Singh et al . 
, 2018 ) and detection of hate speech from social media ( Mathur et al . 
, 2018 ) . 
( Sitaram et al . 
, 2019 ) looks at the challenges and opportunities of code - switching . 
Joshi et al . 
( 2019 ) compares the current deep learning methods for classification tasks in Hindi and concludes the need of more efficient models for the same . 
Apart from that , low - resource languages also challenge us to shift from data - driven modelling to intelligent neural modelling . 
This improves language understanding from limited available data and also diminishes the need of hand - engineered feature representations similar to generative modelling . 
Some such efforts have been put forth by Kumar et al . 
( 2019 ) and Akhtar et al . 
( 2016 ) . 
Keeping these challenges in mind , this work is a step towards understanding of a lowresource language - Hindi using TE . 
3 Recasting Classification Datasets One of the main challenges for TE evaluation for low - resource languages is the lack of labeled data . 
In this work , we employ recasting to convert annotated classification datasets in Hindi to labeled TE samples . 
As in ( Poliak et al . 
, 2018a ) , we selected four different datasets for recasting thus introducing linguistic diversity in the resulting TE dataset . 
Product Review - The first dataset ( PR ) contains 5,417 samples of online user reviewsin Hindi for different products ( Akhtar et al . 
, 2016 ) . 
These samples were annotated into one of the following four sentiment classes : positive , negative , neutral , andconflict . 
F or recasting the samples in this dataset , we first built 8 hypothesis templates : 2 per class label . 
F or each label , we create one positive and one negative hypothesis which roughly translate to:‚ÄòThis product got < label > reviews ‚Äô and ‚Äò This product did not get < label > reviews ‚Äô . 
Given a sample from the PR dataset , we treat it as the context sentence and combine with the 8 hypotheses sentences to create NLI samples . 
If the < label > of the premise matches that of the positive hypothesis , then the NLI sample is marked as ‚Äò entailed ‚Äô . 
Likewise , if the < label > of the premise does not match the negative hypothesis , then the NLI sample is also marked as ‚Äò entailed ‚Äô . 
F or the remaining cases , the sample is marked as ‚Äò non - entailed ‚Äô . 
This process is summarized with an example in Figure 1 . 
F or more detailed recasting illustration , see Appendix Section A.1 Figure 5 . 
BHAA V - The second dataset BHAA V ( BH ) ( Kumar et al . 
, 2019 ) contains 20,304 sentences from Hindi short stories annotated for one of the following five emotion categories : joy , anger , suspense , sad , andneutral . 
W e used a similar process as PR to recast BH using the following templates to create the hypothesis : ‚Äò It is a matter of great < label > ‚Äô and‚ÄòIt is not a matter of great < label > ‚Äô . 
Hindi Discourse Modes Dataset ( HDA ) - This dataset ( Dhanwal et al . 
, 2020 ) consists of10,472 sentences from Hindi short stories annotated for five different discourse modes argumentative , narrative , descriptive , dialogic and informative . 
Hindi BBC News Dataset ( BBC ) - This dataset6contains 4,335 Hindi news headlines tagged across 14 categories : India , Pakistan , news , International , entertainment , sport , science , China , learning english , social , southasia , business , institutional , multimedia . 
W e processed this dataset to combine two sets of relevant but low prevalence classes . 
Namely , we merged the samples from Pakistan , China , international , andsouthasia as one class called 6https://tinyurl.com/y8hxtbn8708      RECASTED NLI DATASET   c1 : Has good streaming quality . 
c1 ‚Äô : Has good streaming quality . 
  h1 : The product got positive h1 ‚Äô : The product did not get positive          reviews from its users . 
               reviews from its users . 
  TE label : entailed TE label : not - entailed   c2 : Has good streaming quality . 
c2 ‚Äô : Has good streaming quality . 
  h2 : The product got negative h2 ‚Äô : The product did not get negative          reviews from its users . 
               reviews from its users . 
  TE label : not - entailed TE label : entailed   c3 : Has good streaming quality . 
c3 ‚Äô : Has good streaming quality . 
  h3 : The product got neutral h3 ‚Äô : The product did not get neutral          reviews from its users . 
               reviews from its users . 
  TE label : not - entailed TE label : entailed   c4 Has good streaming quality . 
c4 ‚Äô : Has good streaming quality . 
  h4 : The product got conÔ¨Çicting h4 ‚Äô : The product did not get conÔ¨Çicting          reviews from its users . 
               reviews from its users . 
  TE label : not - entailed TE label : entailed     ORIGINAL DATASET   Sentence : Has good streaming quality . 
  Annotation : Positive   Set of classes : Positive , Negative , Neutral , ConÔ¨Çict   CLASSIFICATION Recasting Textual   Entailment   Model p1 p1 ‚Äô p2 p2 ‚Äô p3 p3 ‚Äô p4 p4 ‚Äô Entailment   Vector ClassiÔ¨Åer 1 - p1   1 - p2   1 - p3   1 - p4 CONSTRAINT   REGULARISATION      c : Context           h : Hypothesis          TE : Textual Entailment   DIRECT CLASSIFICATION TWO - STEP CLASSIFICATION            Entailment probability > = 0.5                      Entailment probability < 0.5 Figure 1 : Illustration of the proposed approach international . 
Likewise , we also merged samples from news , business , social , learning english , andinstitutional as news . 
Lastly , we also removed the class multimedia because there were very few samples . 
T able 2shows statistics about the datasets and T able 3shows examples from each . 
Datasets PR BH HDA BBC Original datasets # Classes 4 5 5 6 # T rain 4334 16243 8377 3889 # Dev 541 2030 1047 216 # T est 542 2031 1048 217 Recasted TE data # Classes 2 2 2 2 # T rain 17336 64972 33508 15556 # Dev 4328 20300 10470 2592 # T est 4336 20310 10480 2604 T able 2 : Statistics of the original classification data and recasted NLI data . 
4 Methodology Our objective in this paper is not only to use recasting to create a NLI dataset in low - resource settings but also to understand how different models are effective in both TE and classification task . 
F urthermore , we also discuss our novel two - step classification technique with joint objective and regularization constraints . 
4.1 T extual Entailment One straightforward application of NLI comes with evaluating the task of T extual Entailment(TE ) . 
It analyses if the TE model can draw reasonable inferences from the context to hypothesise over other related / unrelated data , as shown in T able 1 . 
However , apart from being correct / incorrect , certain times , TE models are not always consistent with their own beliefs ( Li et al . 
, 2019 ) due to spurious patterns in the dataset ( Poliak et al . 
, 2018a ) . 
Consider two context - hypothesis pairs P and P‚Ä≤generated from the same context sentence and opposing hypotheses statements ( as illustrated in Figure 1 ) . 
Consequently , P and P‚Ä≤would have opposing TE labels . 
When a TE model makes predictions on these two pairs , there are three possibilities ( T able 5 ) . 
The model can get both predictions right , in which case the predictions are consistent . 
It can also get both predictions wrong but still they are consistent . 
Lastly , it can get one of the predictions wrong , in which case they are inconsistent7 . 
T o mitigate this inconsistency problem , we propose consistency regularisation loss . 
4.1.1 Consistency Regularisation ( CR ) T o enforce this pairwise - consistency , we add a regularisation loss8 , inspired from ( Li et al . 
, 7See Appendix Section A.3 T able 11 for additional inconsistency examples . 
8Other suitable loss function also works ( Li et al . 
, 2019 ) .709 Dataset Sentence ( Hindi ) Sentence ( English ) Sentiment PR —û‡§´‡§≤‡§π‡§æ‡§≤ , ‡§á‡§∏‡§Æ”í ‡§ï‡•ã‡§à ‡§µ‡•Ä—û‡§°‡§Ø‡•ã ‡§Ø‡§æ ‡§µ‡•â‡§Ø‡§∏ ‡§ï‡•â‡§≤ ‡§∏‡§™‡•ã‡§ü À®‡§®‡§π“∞‡§Ç‡§π‡•à‡•§At the moment , there is no video or voice call support.negative BH ‡§á‡§§‡§®‡•Ä —†‡§Æ‡§†‡§æ‡§á‡§Ø‡§æ ‡§Å‡§≤“∞‡§Ç , ‡§Æ‡•Å‡§ù‡•á—û‡§ï‡§∏‡•Ä ‡§® ‡•á‡§è‡§ï ‡§≠‡•Ä ‡§® ‡§¶“∞‡•§T ook so many sweets , nobody gave me one.anger HDA ‡§∏‡•å‡§∞ ‡§Æ ‡§Ç‡§°‡§≤ ‡§ï ‡•á‡§∏‡§æ‡§∞‡•á‡§Æ‡§π‡•á ‡§¨‡•É‡§π‡§É‡§™‡§ø‡§§ ‡§Æ”í ‡§∏‡§Æ‡§æ ‡§Ç ‡§∏‡§ï‡§§‡•á ‡§π ”î|All the planets in the solar system can be contained within the Jupiter.informative BBC ‡§Ö‡§ñ‡§¨‡§æ‡§∞ ‡§® ‡•á‡§¨‡§§‡§æ‡§Ø‡§æ —û‡§ï ‡§´ ‡•á ‡§∏‡§¨ ‡•Å‡§ï ‡§™‡§∞ —†‡§Æ‡§≤ ‡•á‡§ó‡•Ä ‡§Ö‡§∏‡§≤ ‡§ú‡§æ‡§¶ ‡•Ç‡§ï“¥ ‡§ùÕ©‡§™‡•Ä ”î‡•§The newspaper said that real magic hug will be found on F acebook.entertainment T able 3 : Sample sentences from the four datasets and the corresponding annotation labels . 
2019 ) , for our settings , where the entailment probabilities pandp‚Ä≤of pairs P andP‚Ä≤respectively , is required to always sum up to one as illustrated in Figure 1 . 
Mathematically , we define the regularisation term as depicted in Equation 1 . 
Lreg=/vextenddouble / vextenddoublep+p‚Ä≤‚àí1 / vextenddouble / vextenddouble2 2(1 ) Our regularisation is different from ( Li et al . 
, 2019 ) in terms of different consistency problem being considered , which in - term diversifies a very different inductive bias from former . 
4.2 T wo - step classification W e further extend the knowledge accumulated by TE predictions for multi - class classification . 
Consider a TE model with binary output where 1 ( entailed ) represents entailed and 0 ( not - entailed ) represents not - entailed . 
One can co - relate model predictions for related TE pairs with same context but different hypothesis during prediction ( inference ) to retrieve the classification label . 
This is depicted by an example in T able 4 . 
W e call our approach a two - step classification method , where we obtain TE predictions in the first step and use them to obtain classification label in step two . 
F or demarcation , we refer to the straightforward task ( without the recasted data ) as direct classification . 
Therefore , a perfect TE model would lead to a 100 % accuracy over the two - step classification task . 
However , having a completely accurate TE model is often a bottleneck due to inaccurate and inconsistent predictions . 
Here , inconsistency can even occur across pairs , for example , two different pairs can predict two different labels . 
So instead of binary outputs , we use soft TE probabilities ( pi ) of each context - hypothesis pair ( ci - hi ) and concatenate them together to form an entailment vector ( E ) , see Figure 1 . 
The classifier C : E ‚Üí    ‡§≤ ‡•á ‡§ï‡§®        ‡§ï‡§∞‡§§‡§æ           ‡§∏‡§ï‡§§‡§æ         ‡§¨‡•à‡§ü‡§∞‡•Ä          ‡§π‡§æ‡§≤‡§æ‡§Å‡§ï     but          does            can          battery      however     ‡§≤ ‡•á ‡§ï‡§®        ‡§ï‡§∞‡§§‡§æ           ‡§∏‡§ï‡§§‡§æ         ‡§¨‡•à‡§ü‡§∞‡•Ä          ‡§π‡§æ‡§≤‡§æ‡§Å‡§ï     but          does            can          battery      however Figure 2 : Plot showing statistics of unigram patterns in PR dataset for train ( top ) and test ( bottom ) across different classes for some sentiment as well as non - sentiment keywords . 
The x - axis represents the keyword with the percentage of occurrence on the y - axis . 
Y , then takes as input the entailment vector ( E ) to retrieve the classification label ( Y ) . 
Here , the entailment vector works as an added weaker supervision at the group level ( group of all recasted pairs for a given context ) to the classifier . 
Thus the classifier identify the correct boundary for the final classification task . 
F urthermore , two - step classification adds an interpretable advantage over the direct classification . 
This is because , direct - classifcation is driven by a lot of spurious unigram patterns present in the original dataset . 
These patterns are leveled in the two - step classification approach due to the balanced set of text tokens710 for both entailed and not - entailed pairs ( both labels ) with data recasting . 
Figure 2shows some of the unigram statistics for PR dataset over some sentiment as well as non - sentiment words to depict the type of artefact patterns in the classification datasets , similar to ( T an et al . 
, 2019 ) . 
These annotation aretefacts are nullified in the recasted TE task due to balanced label balanced for every premise tokens . 
4.2.1 Joint Objective ( JO ) One simple method for two - step classification is to first train a TE model and then train the classifier on its predictions . 
However , using a fixed TE model prediction imposes a prior bottleneck on the classification accuracy . 
Since both the tasks i.e. the TE and the follow - up classification , can influence each other , thus we propose a joint training objective as shown in Equation 2 Ljoint = LT E+ŒªLclf ( 2 ) where Œªis the weight of the follow - up classification loss , LT E andLclf are cross - entropy loss for the task of TE and classification respectively as defined in Equations 3and 4 . 
LT E=/summationdisplay km / summationdisplay j=1‚àíptrue k , j logpk , j ( 3 ) Lclf=/summationdisplay km / summationdisplay j=1‚àíctrue k , j logck , j ( 4 ) Here , m represents the total classes , ptrue k , j andctrue k , jrepresent the binary label of sample k to belong to class j , and pk , j andck , j represent the probability of predicted label for sample k to be class j. Benefit of Joint Objective . 
Satisfying the joint objective not only ensures that the model predictions are correct but also ensures that they are correct for the right reasons . 
The true classification label can be retrieved from the entailment vector only when the model draws necessary inferences correctly . 
Otherwise the multi - class classification would fail . 
F urthermore , combining the joint objective ( Equation 2 ) with consistency regulariser ( Equation 1 ) for the intermediate TE prediction further force pairwise - consistency between prediction of related TE pairs . 
Context sentence : He cried over his lost pet . 
Hypotheses TE Prediction 1 . 
He is happy . 
not - entailed 2 . 
He is not happy . 
entailed 3 . 
He is angry . 
not - entailed 4 . 
He is not angry . 
entailed 5 . 
He is sad . 
entailed 6 . 
He is not sad . 
not - entailed Inferred label : Sad T able 4 : An example demonstrating inference of the label for the original classification task based on predictions from TE model . 
5 Experiments Most of the sentence embedding models have been designed and evaluated to perform well onEnglish language . 
The experiments in this work are motivated to answer the following questions for a low - resource language , Hindi : ‚Ä¢ Are these representations effective to derive logical entailment in contexthypothesis pairs on recasted data ? . 
F urthermore , how consistent / inconsistent are such models with their own decisions ? Also , does consistency regulariser help to mitigate model inconsistency ? ‚Ä¢ Do sentence representation models work well for direct classification ? Can models trained on recasted NLI data be used to retrieve ground truth classification annotations using two - step classification ? Does our joint training objective with consistency regularization improve performance ? Baselines - F or evaluating our approach , we use the following baselines : InferSent ( Conneau et al . 
, 2017 ) , Sent2V ec ( Pagliardini et al . 
, 2018 ) , Bag - of - words ( BoW ) and XLMRoBER T a ( Conneau et al . 
, 2019 ) which is state - of - the - art for multilingual language modelling . 
Also , we evaluate a hypothesis - only analogue for each one of them as well . 
F or experiments with recasted data , we use embeddings of context - hypothesis pair for baselines whereas for the hypothesis - only ( Poliak et al . 
, 2018b ) models , we only use embeddings of thehypothesis sentence , keeping it blind to thecontext . 
Hypothesis only Baselines - Evaluating hypothesis - only models is motivated by irregularities and biases presented in entailment711 Context ( Hindi ): ‡§µ‡§π ‡§∞‡•ã‡§Ø‡§æ ‡§ú‡§¨ ‡§â‡§∏‡§® ‡•á‡§Ö‡§™‡§®‡§æ ‡§™‡§æ‡§≤‡§§ ‡•Ç‡§ñ‡•ã —û‡§¶‡§Ø‡§æ Emotion class ( Hindi ): ‡§¶‡•Å‡§ñ ( English ): He cried over his lost pet . 
( English ): Sad Hypothesis ( Hindi ) Hypothesis ( English ) TE label Consistency Prediction h1 : ‡§µ‡§π ‡§ñ ‡•Å‡§∂ ‡§π‡•à h1 : He is happy . 
not - entailedConsistentCorrect h1‚Ä≤:‡§µ‡§π ‡§ñ ‡•Å‡§∂ ‡§®‡§π“∞ ‡§Ç‡§π‡•à h1‚Ä≤:He is not happy . 
entailed Correct h1 : ‡§µ‡§π ‡§ñ ‡•Å‡§∂ ‡§π‡•à h1 : He is happy . 
not - entailedInconsistentCorrect h1‚Ä≤:‡§µ‡§π ‡§ñ ‡•Å‡§∂ ‡§®‡§π“∞ ‡§Ç‡§π‡•à h1‚Ä≤:He is not happy . 
not - entailed Incorrect h1 : ‡§µ‡§π ‡§ñ ‡•Å‡§∂ ‡§π‡•à h1 : He is happy . 
entailedInconsistentIncorrect h1‚Ä≤:‡§µ‡§π ‡§ñ ‡•Å‡§∂ ‡§®‡§π“∞ ‡§Ç‡§π‡•à h1‚Ä≤:He is not happy . 
entailed Correct h1 : ‡§µ‡§π ‡§ñ ‡•Å‡§∂ ‡§π‡•à h1 : He is happy . 
entailedConsistentIncorrect h1‚Ä≤:‡§µ‡§π ‡§ñ ‡•Å‡§∂ ‡§®‡§π“∞ ‡§Ç‡§π‡•à h1‚Ä≤:He is not happy . 
not - entailed Incorrect T able 5 : A simple example illustrating the concept of consistency in model prediction for TE task for the task of emotion analysis . 
datasets . 
Such biases often lead to high performance over NLI tasks without completely comprehending the semantic reasonings in data and language . 
When the accuracy of a hypothesis - only model is much lower than the baseline and closer to random ( 50 % ) , it exhibits that learning is not boosted due to statistical irregularities in data such as word count , unigram / bi - gram pattern or any other spurious pattern ( artefacts ) . 
W e achieve this using our approach since recasting ensures label balance for the augmentations of each class label for every sentence and its tokens . 
Experimental Settings - F or each of the models , we use the initial learning rate 1 x 10‚àí3and a decay rate of 0.9 , using Adam optimizer with the embedding dimension kept as 1024 for all the models . 
F or all the experiments associated with XLM - RoBER T a , W e use XLM - RoBER T a large with 1024 - hidden . 
F or InferSent and Sent2V ec we use the default parameter for NLI model architecture as stated in the paper . 
F or hypothesis only baseline we use the single sent model of XLMRoBER T a , InferSent and Sent2V ec as reported in paper for binary classification . 
After the embeddings are obtained , we use an MLP classifier for performing all the classification experiments . 
F or a hypothesis - only baseline , only the hypothesis embedding is passed as an input to the MLP , whereas for a premise - hypothesis baseline , we concatenate the embeddings of premise , hypothesis , as well as their element - wise product and elementwise subtraction . 
F or the joint objective training ( see Eq . 
2 ) , we use Œª=2.0 . 
W e train our model for 15 epochs on a machine with GeF orce R TX 2080 GPU using the PyT orch framework.5.1 T extual Entailment Results F or all four semantic phenomenon considered , we use recasted data to predict the performance on textual entailment task . 
While training , we use four context - hypothesis pairs - with hypothesis having true classification label , its negation ( hypothesis 5 and 6 in T able 4 ) , a random label from the remaining classes and its negation ( hypothesis 1 and 2 in T able 4 ) . 
This ensures that neither original classification label nor the negation ( we choose only one random pair ) correlate with entailment labels . 
F or development and test sets , we use all possible 2nrecasted pairs ( where nis the number of classes in classification data ) since ideally , while testing we have no prior knowledge of the ground - truth label . 
Context - Hypothesis Baselines Sentence Dataset Representation PR BH HDA BBC BoW 47.32 51.00 54.20 57.00 Sent2V ec 61.21 62.67 64.00 65.42 InferSent 68.00 65.04 67.9 68.84 XLM - RoBER T a 74.02 74.48 75.29 73.56 Hypothesis - only Baselines BoW 44.89 47.01 44.82 43.00 Sent2V ec 51.91 50.84 50.88 48.80 InferSent 54.32 52.14 53.54 51.08 XLM - RoBER T a 55.00 52.60 53.92 55.00 T able 6 : TE classification accuracies using different sentence embeddings for all four datasets . 
With T able 6 , we establish that XLMRoBER T a ( Conneau et al . 
, 2019 ) gives the best performance as compared to all the other baselines . 
Therefore , we use it for all the following experiments . 
Also , random performance on hypothesis - only baseline ensures that our recasted data does not contain hypothesis - bias.712 Consistency - W e analyse the effect of consistency regulariser ( CR ) by comparing the percentage of inconsistent model predictions for TE models with and without CR . 
Figure 3 clearly depicts that the constraint regularisation helps in reducing the percentage of inconsistent pairs and hence makes the model predictions congruent with its own internal representation in the model parameters . 
5.2 T wo - step Classification Results W e now use the TE model to perform two - step classification as explained in section 4.2 . 
T able 10 shows the classification accuracies obtained via direct as well as two - step classification with consistency regularisation and joint - objective . 
As reported in T able 9and 10 , we observe a jump in both the TE as well as two - step classification accuracies with the addition of consistency regularisation . 
Such a constraint restricts the model predictions to be either correct or incorrect but not pairwise - inconsistent with its other beliefs . 
Joint Objective - In T able 9and 10 , we observe that joint objective proves to be much more beneficial than independent TE and classifier training . 
The two - step classification accuracy with joint - objective ( + JO+CR ) surpasses the direct classification performance . 
W e observe an increment of 5 % in TE and 2 % in classification accuracy across all the datasets . 
F urthermore , from Figure 3 , we observe that , JO also improve the prediction consistency across all the datasets . 
T able 7shows the exact percentage of correct / incorrect and inconsistent pairs . 
Improved Performance Analysis - The two - step classification is able to achieve overall improvement over direct classification approach mainly due to following two factors . 
Firstly , the joint objective ( JO ) helps in creating a feedback loop with the two tasks of textual entailment and classification , which enforce consistency in the model predictions for the two tasks . 
Secondly , the consistency regularisation ( CR ) for the TE helps in making the model decisions congruent across same context premise but different related hypothesis . 
Thus , both the JO and CR imposes indirect and direct inductive bias through constrained loss objective which improves model performance Figure 3 : Plot depicting percentage ( % ) of inconsistent predictions for all the datasets using XLMRoBERTa with and without consistency regularisation ( CR ) and Joint Objective ( JO ) . 
compared to the direct classification task . 
5.3 Direct vs T wo - Step Classification W e analyse the classification predictions obtained by direct as well as two - step classification to compare the differences . 
Figure 4 shows the percentage ( % ) of correct and incorrect predictions obtained for the two approaches considered . 
More generally , we see a maximum consensus across the main diagonal between the two approaches . 
However , there are irregularities wherein one of the predictions contradicts the other . 
As illustrated in T able 8 , we depict qualitative examples corresponding to these irregularities . 
W e analyse their entailment vectors to interpret intermediate predictions and realise that the high entailments corresponding to the gold label and certain incorrect label lead to incorrect predictions . 
F or example , for the first sentence in T able 8 , we observe that the context - hypothesis pairs with hypothesis corresponding to The product received negative reviews from its users , and‚ÄòThe product received conflicting reviews from its users ‚Äô get the entailment probabilities 0.64 and 0.58 , respectively . 
This shows that apart from the gold label i.e. negative here , there is an inclination towards the class label conflict . 
Moreover , we see certain statistical word patterns like the usage of the keyword but in most of the sentences corresponding to the classconflict , thereby ensuring a certain degree of artefact learning which governs the decisions in direct classification . 
One advan-713 Dataset Correct Incorrect Inconsistent TE + CR + JO + CR TE + CR + JO + CR TE + CR + JO + CR + JO + JO + JO PR 71.43 72.18 72.50 74.00 13.82 18.6 18.6 18.2 14.75 9.22 8.90 7.80 BH 73.20 74.50 74.76 75.80 14.32 17.50 17.66 17.99 12.48 8.00 7.58 6.21 HDA 72.00 74.88 75.22 76.8 11.50 14.66 14.78 13.9 16.50 10.46 10.00 9.30 BBC 71.17 74.56 74.84 76.00 17.75 18.2 18.16 17.2 11.08 7.24 7.00 6.80 T able 7 : Percentage ( % ) of correct , incorrect and inconsistent prediction pairs for all the datasets using XLM - RoBER T a. Sentence T rue Label Direct clf . 
T wo - step clf . 
‡§Ø‡§π‡§æ‡§Å‡§ñ‡§æ‡§®‡§æ ‡§™‡•Ä‡§®‡§æ ‡§â‡§§‡§®‡§æ ‡§Æ ‡§Å‡§π‡§ó‡§æ ‡§®‡§π“∞ ‡§Ç‡§™‡§∞ ‡§∞‡§π‡§®‡§æ ‡§ú ‡•á‡§¨ ‡§ï‡•ã ‡§ï‡§æ‡§´“¥ ‡§≠‡§æ‡§∞“∞ ‡§™‡•ú‡§§‡§æ ‡§π ‡•à‡•§negative conflict negative English : Drinking here is not that expensive but living on the pocket is very heavy . 
‡§∞‡§æ‡§ú‡§ó‡•Å“ä , ‡§Æ‡§π‡§æ‡§∞‡§æ‡§ú ‡§ï ‡•É ‡§Ç‡§£‡§¶ ‡•á‡§µ ‡§∞‡§æ‡§Ø ‡§ï‡•ã ‡§ï‡§π‡§§‡•á ‡§π ‡•à‡§ï‡•á‡§§‡•á‡§®‡§æ‡§≤“∞‡§∞‡§æ‡§Æ ‡§ù ‡•Ç‡§† ‡§¨‡•ã‡§≤ ‡§∞‡§π‡•á ‡§π ‡•à|anger anger sad English : Rajguru tells Maharaja Krishnadeva Raya that T enaliram is lying . 
T able 8 : Qualitative examples where direct and two - step classification methods contradict predictions . 
DatasetT extual Entailment w/o CR / JO + CR + JO + CR+JO PR 74.02 77.80 78.40 81.40 BH 74.48 76.57 77.01 80.05 HDA 75.29 78.00 78.22 81.67 BBC 73.56 76.24 77.69 79.22 T able 9 : TE accuracies for all the four datasets using XLM - RoBER T a ( Conneau et al . 
, 2019 ) . 
DatasetDirect T wo - step clf . 
clf . 
TE TE+ CRTE+ JOTE+ CR+JO PR 71.65 66.24 69.38 70.58 73.70 BH 73.03 68.06 70.91 71.82 74.80 HDA 74.25 68.22 71.45 72.45 75.96 BBC 70.22 65.98 68.20 70.30 72.18 T able 10 : Classification ( direct and two - step ) accuracies for all the four datasets using XLMRoBER T a ( Conneau et al . 
, 2019 ) . 
tage of two - step classification is that it is more transparent about it ‚Äôs predictions . 
This ensures more interpretability in the model decisions . 
W e also compare class - wise accuracies of both the approaches for each of the datasets and see improvements with the twostep method in all classes9 . 
6 Conclusion In this work , we share the first recasted NLI dataset in a low - resource language Hindi , and show how a large - scale NLI data can be developed for low - resource languages without un9See Appendix Section A.2 Figure 6for class - wise results 65 12.5 5 17.568 12 6.5 12.570 10 5 1563 15.5 4 17.5DIRECT CLASSIFICATION TWO - STEP CLASSIFICATION ‚úî        ‚ùå PR   BH    HDA   BBC   Correct : ‚úî   Incorrect : ‚ùå ‚úî ‚ùå ‚úî        ‚ùå   ‚úî        ‚ùå   ‚úî        ‚ùå Figure 4 : Correct vs Incorrect Predictions ( % ) for Direct and T wo - Step classification . 
dergoing costly and time taking human annotations . 
W e perform TE experiments and introduce a consistency regulariser to avoid pairwise - inconsistent TE predictions . 
F urthermore , we propose a two - step classification approach with a joint training objective . 
Our results with the joint objective shows significant improvement in performance . 
As a future work , we aim to analyse the proposed methodology which is language independent on other low - resource languages . 
W e also aim to use more generalisable templates for linguistic diversity in recating data . 
It would be interesting to analyse how extending textual entailment knowledge especially the consistency regularization constraint affect other downstream NLP tasks apart from textual classification , not only in terms of the performance , but also in enhancing the model interpretability .714 A Appendix A.1 Illustration of Recasting Approach W e illustrate the proposed recasting approach in more detail with example templates in Fig - ure 5 . 
W e show how each classification sentence is used to create a context - hypothesis pair for NLI task for different datasets corresponding to the diverse semantic phenomenon considered . 
A.2 Additional Results Development Set Results - W e report the results on development set for textual entailment as well as classification in T able 12 and 13 respectively . 
W e observe similar trends in the development set as depicted in the test set performance for both the tasks of textual entailment as well as the two - step classification task . 
Class - wise Performance - In Figure 6 , we show class - wise accuracies obtained by the two classification approaches - direct vs two - step . 
Broadly , we obtain a considerable improvement in the performance of two - step classification over direct classification , over all classes across all the four datasets . 
This ensures that the obtained performance improvement is balanced across all classes . 
Semi - supervised setting - W e extend our analysis to a semi - supervised setting ( with fewer labels ) wherein we retain the true labels for only 40 % , 60 % and 80 % of the data while training and analyse its effect on the performance of TE and classification tasks . 
T able 14,16 and 18 show the results obtained with different ablations with 80 % , 60 % and 40 % of the labelled data respectively for the TE task . 
Similarly , T able 15,17 and 19 report the results for direct and two - step classification in the semi - supervised approach highlighting the effect of joint objective and consistency regularisation in obtaining improvement . 
Although , we utilize the consistency regularisation , since it does not depend on the true label , rather operated on pairwise contexthypothesis groupings . 
W e observe that TE with consistency regularisation and joint objective surpasses the trivial TE task without any added constraints . 
This depicts that our regularisation and joint objective approach add robust improvements in TE model performance even with minimum supervision.716 Recasting Datasets   Original Sentence Sentiment Label   Recasting Template   The product got < label >   reviews from its users . 
It is a matter of < label > . 
  Context :   Original Sentence Premise :   Recasting Template The product did not get < label >   reviews from its users . 
It is not a matter of < label > . 
  TE Label Context :   Original Sentence Premise :   Recasting Template   < ground truth label > : entailed   < any other label > : not - entailed < ground truth label > : not - entailed   < any other label > : entailed PR BH The sentence depicts < label >   statement . 
  The sentence does not depicts   < label > statement . 
  HDA Positive Hypothesis   Negative Hypothesis Figure 5 : Illustration of the proposed recasting approach . 
Original Sentence(Hindi ) Original Sentence ( English ) Sentiment ‡§á‡§® ‡§™—ü‡§µ‡§Ω ‡§≠‡§æ‡§µ”ñ ‡§∏ ‡•á‡§â‡§∏‡§ï“¥ ‡§ÜÕ§‡§Æ‡§æ —ü‡§µ—ã‡§≤ ‡§π‡•ã ‡§ó‡§Ø‡•Ä‡•§His soul was overwhelmed by these holy feelings . 
Joy Model Consistency / Inconsistency Contradictory TE pairs ( Hindi ) Contradictory TE pairs ( English ) Prediction Label p - h1p - h2 p : ‡§á‡§® ‡§™—ü‡§µ‡§Ω ‡§≠‡§æ‡§µ”ñ ‡§∏ ‡•á‡§â‡§∏‡§ï“¥ ‡§ÜÕ§‡§Æ‡§æ —ü‡§µ—ã‡§≤ ‡§π‡•ã ‡§ó‡§Ø‡•Ä‡•§p : His soul was overwhelmed by these holy feelings.e e Inconsistent h1:‘π‡§æ ‡§Ø‡§π ‡§ñ ‡•Å‡§∂‡•Ä ‡§ï“¥ ‡§¨‡§æ‡§§ ‡§π ‡•à ? h1 : Is this a matter of joy ? e ne Correct p : ‡§á‡§® ‡§™—ü‡§µ‡§Ω ‡§≠‡§æ‡§µ”ñ ‡§∏ ‡•á‡§â‡§∏‡§ï“¥ ‡§ÜÕ§‡§Æ‡§æ —ü‡§µ—ã‡§≤ ‡§π‡•ã ‡§ó‡§Ø‡•Ä‡•§p : His soul was overwhelmed by these holy feelings.ne e Incorrect h2:‘π‡§æ ‡§Ø‡§π ‡§ñ ‡•Å‡§∂‡•Ä ‡§ï“¥ ‡§¨‡§æ‡§§ ‡§®‡§π“∞ ‡§Ç‡§π‡•à ? h2 : Is this not a matter of joy ? ne ne Inconsistent T able 11 : Example sentences for contradictory premise ( p ) - ( h ) pairs for measuring inconsistency in the recasted model predictions with erepresenting entailed andne representing not - entailed . 
DatasetT extual Entailment ‚Üë w/o + CR + JO + CR+JO PR 74.26 78.44 78.02 80.60 BH 73.88 76.46 76.82 80.95 HDA 75.90 78.54 78.48 81.86 BBC 73.45 76.48 77.96 79.02 T able 12 : TE accuracies for all the four datasets using XLM - RoBER T a on the development set . 
DatasetDirect T wo - step clf . 
‚Üë clf . 
TE TE+ CRTE+ JOTE+ CR+JO PR 71.40 65.48 68.76 70.84 72.98 BH 73.50 69.24 70.88 71.46 75.66 HDA 74.85 68.46 72.34 73.50 75.56 BBC 71.36 66.40 68.38 70.47 73.08 T able 13 : Classification ( direct and two - step ) accuracies for all the four datasets using XLM - RoBER T a on the development set.717 HDA BBC   PR BH   Figure 6 : Class - wise comparison of Direct vs T wo - Step Classification . 
DatasetT extual Entailment ‚Üë w/o + CR + JO + CR+JO PR 69.23 72.68 70.48 74.04 BH 70.65 71.09 70.99 73.98 HDA 70.29 72.23 71.32 74.67 BBC 70.36 73.84 71.65 74.52 T able 14 : TE accuracies for all the four datasets using XLM - RoBER T a with fewer labels ( 80%).DatasetDirect T wo - step clf . 
‚Üë clf . 
TE TE+ CRTE+ JOTE+ CR+JO PR 67.20 61.28 64.87 62.49 68.98 BH 68.51 64.22 66.71 71.46 69.46 HDA 68.82 62.62 65.13 63.75 69.95 BBC 66.93 60.94 63.14 61.47 67.73 T able 15 : Classification ( direct and two - step ) accuracies for all the four datasets using XLM - RoBER T a with fewer labels ( 80 % ) . 
DatasetT extual Entailment ‚Üë w/o + CR + JO + CR+JO PR 65.12 67.46 65.58 70.06 BH 66.12 68.57 67.22 70.69 HDA 65.29 67.25 66.34 70.59 BBC 66.87 68.22 67.19 71.42 T able 16 : TE accuracies for all the four datasets using XLM - RoBER T a with fewer labels ( 60%).DatasetDirect T wo - step clf . 
‚Üë clf . 
TE TE+ CRTE+ JOTE+ CR+JO PR 60.29 61.82 62.37 62.00 63.98 BH 61.52 62.14 64.18 62.45 64.81 HDA 61.82 63.47 63.94 63.33 65.56 BBC 60.23 61.24 62.16 62.09 64.73 T able 17 : Classification ( direct and two - step ) accuracies for all the four datasets using XLM - RoBER T a with fewer labels ( 60 % ) . 
DatasetT extual Entailment ‚Üë w/o + CR + JO + CR+JO PR 57.12 58.46 58.08 59.56 BH 59.12 59.57 59.22 60.69 HDA 59.29 59.25 60.19 60.78 BBC 58.42 58.70 58.10 59.02 T able 18 : TE accuracies for all the four datasets using XLM - RoBER T a with fewer labels ( 40%).DatasetDirect T wo - step clf . 
‚Üë clf . 
TE TE+ CRTE+ JOTE+ CR+JO PR 55.29 56.28 56.48 57.00 59.89 BH 58.52 59.17 59.18 59.59 60.11 HDA 58.82 58.43 58.94 59.23 60.68 BBC 55.23 57.24 56.46 58.01 60.78 T able 19 : Classification ( direct and two - step ) accuracies for all the four datasets using XLM - RoBER T a with fewer labels ( 40%).718 A.3 Another Inconsistency Example In T able 11 , we explain the concept of pairwise consistencies and inconsistencies in the context - hypothesis pairs in the recasted data with an example . 
It depicts how different entailment results for the same context but different hypothesis can lead to inconsistencies within the model predictions . 
A.4 Benefits of Data Recasting There are several benefits of data recasting ( Conneau et al . 
, 2019 ) especially for lowresource languages ‚Ä¢ Recasting is an automated process and hence remove the need of expensive human annotation to labelled data . 
‚Ä¢ Uniform procedure of recasting data has equal number of context - hypothesis pairs for each label , hence making it neutral to statistical irregularities ( see hypothesis bias experiments in Section 5 ) . 
‚Ä¢ Diverse semantic phenomenon for various classification tasks can be unified as a single task using data recasting.719 Proceedings of the 1st Conference of the Asia - PaciÔ¨Åc Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing , pages 720‚Äì725 December 4 - 7 , 2020 . 
¬© 2020 Association for Computational Linguistics Explaining Word Embeddings via Disentangled Representation Keng - Te Liao National Taiwan University d05922001@ntu.edu.twCheng - Syuan Lee National Taiwan University r07922055@ntu.edu.tw Zhong - Yu Huang National Taiwan University r06944047@ntu.edu.twShou - de Lin National Taiwan University sdlin@csie.ntu.edu.tw Abstract Disentangled representations have attracted increasing attention recently . 
However , how to transfer the desired properties of disentanglement to word representations is unclear . 
In this work , we propose to transform typical dense word vectors into disentangled embeddings featuring improved interpretability via encoding polysemous semantics separately . 
We also found the modular structure of our disentangled word embeddings helps generate more efÔ¨Åcient and effective features for natural language processing tasks . 
1 Introduction Disentangled representations are known to represent interpretable factors in separated dimensions . 
This property can potentially help people understand or discover knowledge in the embeddings . 
In natural language processing ( NLP ) , works of disentangled representations have shown notable impacts on sentence and document - level applications . 
For example , Larsson et al . 
( 2017 ) and Melnyk et al . 
( 2017 ) proposed to disentangle sentiment and semantic of sentences . 
By manipulating sentiment factors , the machine can rewrite a sentence with different sentiment . 
Brunner et al . 
( 2018 ) also demonstrated sentence generation while more focusing on syntactic factors such as part - of - speech tags . 
For document - level applications , Jain et al . 
( 2018 ) presented a learning algorithm which embeds biomedical abstracts disentangling populations , interventions and outcomes . 
Regarding word - level disentanglement , Athiwaratkun and Wilson ( 2017 ) proposed mixture of Gaussian models which can disentangle meanings of polysemous words into two or three clusters . 
It has a connection with unsupervised sense representations ( Camacho - Collados and Pilehvar , 2018 ) which is an active research topic in the community . 
In this work , we focus on word - level disentanglement and introduce an idea of transforming dense word embeddings such as GloVe ( Pennington et al . 
, 2014 ) or word2vec ( Mikolov et al . 
, 2013b ) into disentangled word embeddings ( DWE ) . 
The main feature of our DWE is that it can be segmented into multiple sub - embeddings or sub - areas as illustrated in Figure 1 . 
In the Ô¨Ågure , each sub - area encodes information relevant to one speciÔ¨Åc topical factor such as Animal orLocation . 
As an example , we found words similar to ‚Äú turkey ‚Äù are ‚Äú geese ‚Äù , ‚Äú Ô¨Çock ‚Äù and ‚Äú goose ‚Äù in the Animal area , and the similar words turn into ‚Äú Greece ‚Äù , ‚Äú Cyprus ‚Äù and ‚Äú Ankara ‚Äù in the Location area . 
Figure 1 : Disentangled embedding with factors Animal , Location andUnseen . 
We also found our DWE generally satisÔ¨Åes the Modularity andCompactness properties proposed by Higgins et al . 
( 2018 ) and Ridgeway and Mozer ( 2018 ) which can be a deÔ¨Ånition of general - purpose disentangled representations . 
Also , our DWE can have the following advantages : ‚Ä¢Explaining Underlying Knowledge The multi - senses of words can be extracted and separately encoded despite the learning algorithm of the original word embeddings ( e.g. GloVe ) does not do disambiguation . 
As a result , the encoded semantic can be presented in an intuitive way for examination . 
‚Ä¢Modular and Compact Features Each sub - area of our DWE can itself be informative features . 
The advantage is that people720 are free to abandon features in sub - areas irrelevant to the given downstream tasks while still achieving competitive performance . 
In Section 4 , we show that using the compact features is not only efÔ¨Åcient but also helps improve performance on downstream tasks . 
‚Ä¢Quality Preservation In addition to higher interpretability , our DWE preserves co - occurrence statistics information in the original word embeddings . 
We found it also helps preserve the performance on downstream tasks including word similarity , word analogy , POS - tagging , chunking , and named entity recognition . 
2 Obtaining Disentangled Word Representations 2.1 Problem DeÔ¨Ånition Our goal is transforming N d - dimensional dense word vectors X‚ààRN√ódinto disentangled embeddingsZ‚ààRN√ódby leveraging a set of binary attributesA={a1, ... ,aM}labelled on words . 
Zis expected to have two properties . 
The Ô¨Årst one is preserving word features encoded in X. More speciÔ¨Åcally , we require XXT‚âàZZTas pointed out by Levy and Goldberg ( 2014 ) that typical dense word embeddings can be regarded as factorizing co - ocurrence statistics matrices . 
The second property is that Zcan be decomposed intoM+1sub - embedding sets Za1, ... ,ZaM andZunseen , where each sub - embedding set encodes information only relevant to the corresponding attribute . 
For example , Za1is expected to be relevant toa1and irrelevant to a2, ... ,aM. Information inXnot relevant to any attributes in Ais then encoded inZunseen . 
An example of transforming XintoZwith two attributes , Animal andLocation , is illustrated in Figure 1 . 
For modelling the relevance between subembeddings and attributes , we use mutual informationI(Za , a)as learning objectives , where ais an arbitrary attribute in A. 2.2 Transformation with Quality Preservation We obtain Zby transforming Xby a matrix W‚ààRd√ód . 
That is , Z = XW . 
To ensure XXT‚âàZZT , an additional constraint WWT= Iis included . 
ZZT= ( XW)(XW)T= X(WWT)XT = XXTifWWT = Iholds.2.3 OptimizingI(Za , a ) Letza , ibe thei - th row inZa . 
By derivation , I(Za , a ) = Œ£N i=1p(zi)p(a|za , i)/bracketleftbig logp(a|za , i)‚àílogp(a)/bracketrightbig ‚âà1 NŒ£N i=1p(a|za , i)/bracketleftbig logp(a|za , i)‚àílogp(a)/bracketrightbig We let logp(a)be constant and replace p(a|z)with a parametrized model qŒ∏(a|z ) . 
By experiments , we found logistic regression with parameter Œ∏is sufÔ¨Åcient to be qŒ∏(a|z ) . 
Intuitively , highI(Za , a ) meansZaare informative features for a classiÔ¨Åer to distinguish whether words has attribute a. When increasing I(Za , a)by optimizing qŒ∏(a|z ) , we found a strategy helping generate higher quality Z. The strategy is letting Zabe features to reconstruct original vectors for words having attribute a. For words with a , the approach becomes a semi - supervised learning architecture which attempts to predict labels and reconstruct inputs simultaneously . 
The loss function L(W , Œ∏ , œÜ ) for maximizing I(Za , a)is as follow : ‚àí1 NŒ£N i=1qŒ∏(a|za , i ) + ŒªIa , i||xi‚àíœÜ(za , i)||2 2 Ia , i=/braceleftBigg 1wheni - th word has attribute a 0wheni - th word does not have a ( 1 ) whereœÜis single and fully - connected layer , xiis the original i - th word ‚Äôs vector in X , andŒªis a hyper - parameter . 
We set Œª=1 din all experiments . 
2.4 Learning to Generate Sub - embedding Za As discussed in 2.3 that high I(Za , a)indicates Zaare informative features for classiÔ¨Åcation , we propose to regard sub - embedding generation as a feature selection problem . 
More speciÔ¨Åcally , we apply sparsity constraint on Z. Ideally , when predictinga , a smaller number of dimensions of Z are selected as the informative features , which are regarded asZa . 
In this work , we use Variational Dropout ( Kingma et al . 
, 2015 ; Molchanov et al . 
, 2017 ) as the sparsity constraint . 
At each iteration of training , a set of multiplicative noise Œæis sampled from a normal distribution N(1,Œ±a = pa 1‚àípa)and injected onZ. That is , the prediction and reconstruction is done by Œ∏(Œæ‚äôZ)andœÜ(Œæ‚äôZ ) . 
The parameterŒ±a‚ààRdis jointly learned with W , Œ∏ , andœÜ . 
Afterwards , d - dimensional dropout rates721 pa = sigmoid ( logŒ±a)can be obtained . 
For each attributeainA , the dimensions with dropout rates lower than 50 % are normally regarded as Za . 
We would like to emphasize that the learned dropout rates are not binary values . 
Therefore , deciding the length of sub - embeddings can actually depend on users preferences or tasks requirements . 
For example , users can obtain more compact and pureZaby selecting dimensions with dropout rates lower than 10 % , or get more thorough yet less disentangledZaby setting the threshold be 70 % . 
To encourage disentanglement when handling multiple attributes , we include additional loss functions on dropout rates . 
Let a M - dimensional vector Pbe1‚àípafor allainAin a speciÔ¨Åc dimension . 
The idea is to minimize / producttextM i=1Piwith constraint Œ£M i=1Pi= 1 . 
The optimal solution is that the dimension is relevant to only one attribute a / primewhere 1‚àípa / prime‚âà1 . 
In implementation , we minimize the following loss function Œ£M i=1logPi+Œ≤||Œ£M i=1Pi‚àí1||2 2 ( 2 ) We setŒ≤= 1 in the experiments , and equation 1 and 2 are optimized jointly . 
To generate Zunseen , we initially select a set of dimensions and constrain their dropout rates be always larger than 50 % . 
The number of dimensions ofZunseen is a hyper - parameter . 
After selection , we do not apply equation 2 on the selected dimensions . 
3 Evaluation 3.1 Word Embeddings and Attributes We transform 300 - dimensional GloVe1into DWE . 
The 300 - dimensional GloVe is denoted by GloVe300 . 
For word attributes A , we use labels in WordStat2 . 
WordStat contains 45 kinds of attributes labeled on 70,651 words . 
Among the attributes , we select 5 high - level and easily understandable attributes : Artifact , Location , Animal , Adjective ( ADJ ) andAdverb ( ADV ) for our experiments . 
The number of words labelled with these 5 attributes is 13,337 . 
After training , all pre - trained GloVe vectors are transformed by the learned matrix W(i.e . 
XW ) for downstream evaluations . 
The number of learned dimensions for each attribute is illustrated in Figure 2 , where the threshold of dropout rates for dimension selection is 50 % . 
1https://nlp.stanford.edu/projects/glove/ 2https://provalisresearch.com/products/content-analysissoftware/ Figure 2 : Disentangled embedding with Ô¨Åve attributes : Artifact , Location , Animal , Adjective andAdverb . 
The remaining dimensions are viewed as Unseen . 
MEN SimLex BATS GA GloVe-300 0.749 0.369 18.83 63.58 DWE 0.764 0.390 18.75 62.30 Table 1 : Word similarity and analogy performance . 
POS Chunking NER GloVe-300 65.0 64.9 65.2 DWE 67.2 66.3 66.1 Table 2 : POS - tag , chunking and NER performance . 
3.2 Evaluation of Quality Preservation We Ô¨Årstly examine whether DWE can preserve features encoded in GloVe-300 . 
The examination is done by intrinsic evaluations including the following tasks and datasets . 
‚Ä¢Word Similarity : Marco , Elia and Nam ( MEN ) ( Bruni et al . 
, 2014 ) and SimLex-999 ( Hill et al . 
, 2015 ) . 
‚Ä¢Word Analogy : Bigger Analogy Test Set ( BATS ) ( Gladkova et al . 
, 2016 ) , Google Analogy ( GA ) ( Mikolov et al . 
, 2013a ) . 
‚Ä¢POS tagging , Chunking and Named Entity Recognition ( NER ): CoNLL 2003 ( Sang and Meulder , 2003 ; Li et al . 
, 2017 ) . 
‚Ä¢QVEC - CCA3(Tsvetkov et al . 
, 2015 ): The performance is measured by semantic and syntactic CCA . 
As shown in Table 1 , 2 and 3 , DWE can preserve performance of GloVe-300 on various NLP tasks . 
Probably due to the additional information of word attributes , DWE can have slightly better performance than GloVe-300 on seven of the tasks 3.3 Attribute ClassiÔ¨Åcation We design an attribute classiÔ¨Åcation task for examining whether the DWE can meet requirements described in Section 2.3 . 
We use logistic regression and take sub - embeddings Zaas input features for 3https://github.com/ytsvetko/qvec722 Semantic Syntactic GloVe-300 0.473 0.341 DWE 0.474 0.348 Table 3 : QVEC - CCA evaluation . 
Artifact Location Animal ADJ ADV Zartifact 77.8 71.0 68.0 65.5 71.2 Zlocation 59.2 83.8 64.0 60.5 69.8 Zanimal 58.5 67.5 84.2 60.2 71.0 Zadj 69.8 70.7 68.2 82.0 72.5 Zadv 59.0 72.5 71.8 71.5 84.2 Zunseen 54.8 70.0 66.5 60.2 68.8 Table 4 : Attribute classiÔ¨Åcation accuracies ( % ) . 
verifying the performance of classiÔ¨Åcation by crossvalidation . 
For each attribute , We randomly sample 400 data for testing . 
The numbers of positive and negative data for testing are balanced . 
Therefore , a random predictor would get around 50 % accuracy in each classiÔ¨Åcation task . 
The binary classiÔ¨Åcation accuracies are shown in Table 4 . 
Take the second column of Table 4 for example . 
For distinguishing whether a word can be location , taking Zlocation as features for training a classiÔ¨Åer achieves the highest accuracy 83.8 % . 
On the other hand , the accuracy reported in the second row of Table 4 implies that Zlocation are less informative features for other attributes . 
Similar results can also be observed for other attributes . 
3.4 Disentangled Interpretability We provide some examples to demonstrate that words having ambiguous or different aspects of semantics can be disentangled . 
Table 5 shows the results of nearby words . 
As can be seen , querying a word inZawith different attributes can help discover the ambiguous semantics implicitly encoded in the original word vectors X. The results also show thatZunseen does capture meaningful information having little relevance to given attributes . 
4 Application : Compact Features for Downstream Tasks Here we demonstrate an application of the modularity andcompactness properties of our DWE . 
We Ô¨Årstly aim to show the sub - embeddings can directly be informative features and can outperform GloVe with the same number of dimensions . 
With the high interpretability , selecting relevantQuery Vectors Nearby Words turkeyZanimal geese , Ô¨Çock , goose turkeyZlocation greece , cyprus , ankara mouseZanimal mice , rat , rats mouseZartifact keyboard , joystick , buttons japanZlocation korea , vietnam , singapore japanZunseen japanese , yakuza , yen appleZartifact macintosh , software , mac appleZunseen mango , cherry , tomato Table 5 : Results of nearby words . 
sub - embeddings could be intuitive . 
Secondly , we will demonstrate that if deciding to Ô¨Åne - tune word vectors for a given downstream task , by using our DWE , we can focus on updating the relevant subembedding instead of the whole embedding . 
The advantage is that it reduces the number of learning parameters . 
Also , it could be regarded as a dimensional and interpretable regularization technique reducing overÔ¨Åtting . 
We take a sentiment analysis task , IMDB movie review classiÔ¨Åcation(Maas et al . 
, 2011 ) , for experiments . 
Intuitively , ADJ andADV should be the most relevant attributes in A. We then select 50 dimensions from Z‚ààR300with the lowest dropout rates in ADJ andADV sub - areas for comparing with 50 - dimensional GloVe4(GloVe-50 ) . 
The embeddings with the selected dimensions are denoted byZadj+adv-50 . 
When tuning our DWE with the classiÔ¨Åer , we update the 52 dimensions ( Zadj‚ààR23andZadv‚ààR29 ) of DWE and compare it with GloVe-300 . 
The document representations for classiÔ¨Åcation is averaged word embeddings . 
The classiÔ¨Åer is a logistic regression . 
When tuning the input word embeddings , we update the embeddings with gradient propagated from the classiÔ¨Åer . 
The results are listed in Table 6 . 
From the table , we can see Zadj+adv-50 directly outperforms GloVe-50 without tuning . 
A possible explanation is that GloVe-50 is forced to encode information less relevant to the sentiments , making it less effective thanZadj+adv-50 in this task . 
In the Ô¨Åne - tuning experiments , DWE can show slightly higher accuracy than GloVe-300 by updating only 52 instead of 300 dimensional features . 
4https://nlp.stanford.edu/projects/glove/723 Feature Without Tuning After Tuning GloVe-50 76.55 86.72 Zadj+adv-50 79.78 87.60 GloVe-300 83.85 87.72 DWE 83.67 87.84 Table 6 : ClassiÔ¨Åcation accuracies ( % ) on IMDB dataset . 
5 Conclusion In this work , we propose a new deÔ¨Ånition and learning algorithm for obtaining disentangled word representations . 
As a result , the disentangled word vectors can show higher interpretability and preserve performance on various NLP tasks . 
We can also see the ambiguous semantics hidden in typical dense word embeddings can be extracted and separately encoded . 
Finally , we showed the disentangled word vectors can help generate compact and effective features for NLP applications . 
In the future , we would like to investigate whether similar effects can be found from non - distributional or contextualized word embeddings . 
Abstract Most previous work on knowledge graph completion conducted single - view prediction or calculation for candidate triple evaluation , based only on the content information of the candidate triples . 
This paper describes a novel multi - view classiÔ¨Åcation model for knowledge graph completion , where multiple classiÔ¨Åcation views are performed based on both content and context information for candidate triple evaluation . 
Each classiÔ¨Åcation view evaluates the validity of a candidate triple from a speciÔ¨Åc viewpoint , based on the content information inside the candidate triple and the context information nearby the triple . 
These classiÔ¨Åcation views are implemented by a uniÔ¨Åed neural network and the classiÔ¨Åcation predictions are weightedly integrated to obtain the Ô¨Ånal evaluation . 
Experiments show that , the multi - view model brings very signiÔ¨Åcant improvements over previous methods , and achieves the new state - of - the - art on two representative datasets . 
We believe that , the Ô¨Çexibility and the scalability of the multi - view classiÔ¨Åcation model facilitates the introduction of additional information and resources for better performance . 
1 Introduction Knowledge graph ( KG ) is a typical kind of graphstructured knowledge base ( KB ) . 
Nowdays , there exist many famous KGs such as YAGO ( Suchanek et al . 
, 2007 ) , Freebase ( Bollacker et al . 
, 2008 ) and DBpedia ( Lehmann et al . 
, 2015 ) . 
Large - scale KGs are widely used in many applications such as semantic searching ( Kasneci et al . 
, 2008 ; Schuhmacher and Ponzetto , 2014 ; Xiong et al . 
, 2017 ) , question answering ( Zhang et al . 
, 2016 ; Hao et al . 
, 2017 ) and machine reading ( Yang and Mitchell , ‚àóJoint Ô¨Årst author . 
Guo participated in the optimization of this work during the internship in Baidu.2017 ) . 
A KG contains a set of triples indicating facts , each of which is composed of a head entity , a tailentity , and a relation indicating the relationship between the two entities . 
It is nearly impossible to collect a complete set of facts or triples for a KG , especially in open domains . 
In fact , many valuable valid triples are missing even for the existing wellbuilt large - scale KGs such as Freebase ( Socher et al . 
, 2013 ; West et al . 
, 2014 ) . 
Many researchers devote their efforts to the problem of knowledge graph completion ( KGC ) , the core operation of which is to evaluate the validity of candidate triples . 
Previous work on KGC mainly include two groups , embedding - based methods and classiÔ¨Åcation - based methods . 
Embedding - based models learn embeddings for entities and relations , and evaluate candidate triples based on the embeddings and speciÔ¨Åc distance metrics . 
Representative models include TransE ( Bordes et al . 
, 2013 ) and its extensions ( Wang et al . 
, 2014 ; Lin et al . 
, 2015b ; Ji et al . 
, 2015 ; Nguyen et al . 
, 2016 ) , DistMult ( Yang et al . 
, 2015 ) and ComplEx ( Trouillon et al . 
, 2016 ) . 
ClassiÔ¨Åcation - based models learn neural networks to evaluate the validity of candidate triples . 
Representative models include ConvE ( Dettmers et al . 
, 2018 ) and ConvKB ( Nguyen , 2017 ) . 
The major advantage of classiÔ¨Åcation - based methods is that they directly model the evaluation of the validity of candidate triples , probably leading to better performance . 
Most of these previous work conducted single - view prediction based on content information , that is , evaluating a candidate triple according to a single distance metric or classiÔ¨Åcation schema , resorting to information restricted in the scope of the candidate triple . 
We believe that multiple learning views for triple evaluation as well as context information of the candidate triple would contribute to better performance . 
In this work , we propose for KGC a novel multiview classiÔ¨Åcation model , where multiple classiÔ¨Å-726 Figure 1 : Illustration of sub - graphs corresponding to the learning views . 
The colored nodes indicate the head and tail entities of the candidate triple . 
The bold nodes and edges are the elements in the retrieved sub - graphs . 
The question marks indicate the elements to be predicted . 
cation views are performed to estimate the validity of a candidate triple , based on both content and context information of the triple . 
There are four classiÔ¨Åcation views for candidate triple evaluation . 
Each of the Ô¨Årst three views performs component prediction , where a speciÔ¨Åc component of the candidate triple is predicted according to the other two components as well as its nearby triples . 
The last view performs plausibility prediction , where the plausibility of the candidate triple is predicted according to its components as well as its nearby triples . 
The prediction conditions of these views investigate both content and context information of the candidate triple , that is , the components in the candidate triple , and the triples nearby the candidate triple . 
The content and context information can be represented as a sub - graph surrounding the candidate triple . 
These classiÔ¨Åcation views are implemented by a uniÔ¨Åed neural network with shared embedding and encoding layers and separated prediction layers , and the classiÔ¨Åcation predictions are integrated by a weighted integration procedure for better candidate triple evaluation . 
In the uniÔ¨Åed neural network , the sub - graphs indicating the content and context of the candidate triples are encoded in a sequencial manner , by converting the sub - graphs into sequential tree representations . 
It facilitates the utilization of advanced encoders such as BiLSTM or Transformer . 
We experiment on two widely used benchmark datasets , FB15k-237 and WN18RR , speciÔ¨Åc versions of Freebase and WordNet . 
We Ô¨Ånd that the multi - view model achieves the new state - of - the - art , signiÔ¨Åcantly outperforming pervious work on KGC . 
We also Ô¨Ånd that we can promote the efÔ¨Åciency of the multi - view model in realistic applications , by a coarse - to-Ô¨Åne strategy where the Ô¨Årst two views are performed to give a list of candidates , and theoverall model is then performed to evaluate these candidates . 
We believe that , the Ô¨Çexibility and the scalability of the multi - view classiÔ¨Åcation model facilitates the introduction of additional information and resources for better performance . 
2 Related Work Most existing KGC models are based on KG embeddings , which aims at learning distributed representations for entities and relations in a KG . 
In these models , the candidate triples are evaluated by some speciÔ¨Åc distance metrics based on the embeddings . 
These models perform embedding learning with local information in individual triples , including translation - based models ( Bordes et al . 
, 2013 ; Wang et al . 
, 2014 ; Lin et al . 
, 2015b ) , semantic matching models ( Yang et al . 
, 2015 ; Nickel et al . 
, 2016 ; Trouillon et al . 
, 2016 ) , and neural network models ( Dettmers et al . 
, 2018 ; Jiang et al . 
, 2019 ; Nguyen , 2017 ) . 
There also exist KGC models based on classiÔ¨Åcation , where classiÔ¨Åers are learnt to evaluate the validity of candidate triples ( Dettmers et al . 
, 2018 ; Nguyen , 2017 ) . 
Both kinds of previous work consider only one view , with simple distance metrics and classiÔ¨Åcation operations . 
In contrast , multi - view learning enables the incorporation of much more views that utilize internal and external information for triple evaluation . 
In recent years , many efforts were devoted to embedding learning based on non - local information such as multi - hop paths ( Lin et al . 
, 2015a ; Das et al . 
, 2017 ) and k - degree neighborhoods ( Feng et al . 
, 2016 ; Schlichtkrull et al . 
, 2017 ) . 
Some researchers also investigated graph embeddings in social network and other areas ( Perozzi et al . 
, 2014 ; Grover and Leskovec , 2016 ; Ristoski and Paulheim , 2016 ; Cochez et al . 
, 2017 ) . 
Compared with these work , our method not only learns em-727 View Type Instance from gv Instance from g‚àí v hr‚Üítghr‚Üít=/angbracketleftG(h , r,?),t / angbracketrightg‚àí hr‚Üít=/angbracketleftG(S(h , r,?)),none / angbracketright , s.t . 
S(h , r,?)/‚ààKG rt‚Üíhgrt‚Üíh=/angbracketleftG(?,r , t),h / angbracketrightg‚àí rt‚Üíh=/angbracketleftG(S(?,r , t)),none / angbracketright , s.t . 
S(?,r , t)/‚ààKG ht‚Üírght‚Üír=/angbracketleftG(h,?,t),r / angbracketrightg‚àí ht‚Üír=/angbracketleftG(S(h,?,t)),none / angbracketright , s.t . 
S(h,?,t)/‚ààKG hrt‚Üíghrt‚Üí=/angbracketleftG(h , r , t ) , true / angbracketrightg‚àí hrt‚Üí=/angbracketleftG(S(h , r , t ) ) , false / angbracketright , s.t . 
S(h , r , t ) /‚ààKG Table 1 : Instance generation for each learning view . 
The Ô¨Årst / second part in an instance is used as the input / output for classiÔ¨Åcation . 
The function Gretrieves the sub - graph surrounding the candidate triple with the maximum height and width limitations . 
The function Sreceives a tuple and returns a randomly corrupted tuple that not exists in the KG , by randomly replacing a known component which is not denoted by the question mark . 
The operator ‚àà incidates that a tuple is equal to orinside of a triple . 
beddings for individual entities and relations based on non - local information , but also obtains representations for sub - graphs resorting to complicated neural encoders . 
This manner probably brings better KGC performance by leveraging global information more effectively . 
3 Method : Multi - view ClassiÔ¨Åcation A knowledge graph KGcontains a set of triples indicating facts,{(h , r , t ) } ‚äÜE√óR√óE . 
Each triple ( h , r , t ) consists of two entities handtreferred to the subject and object of the triple , and a relation rreferred to the relationship between the two entities . 
EandRindicates the possible entity set and the possible relation set , respectively . 
The fundamental problem for KGC is to deÔ¨Åne a candidate triple evaluation model f : E√óR√óE‚Üí R , giving each candidate triple ( h , r , t ) a score indicting the validity of the triple . 
3.1 ClassiÔ¨Åcation Views We adopt a multi - view classiÔ¨Åcation model for KGC , where a candidate triple is evaluated from four different views . 
The Ô¨Årst three views adopt the generative methodology , each view predicts a speciÔ¨Åc component of the candidate triple according to the other two components and the nearby triples . 
The last view adopts the discriminative methodology , it predicts the plausibility of the whole triple according to its components as well as its nearby triples . 
In the prediction conditions of these views , the components in the candidate triple are content information inside the triple , and the triples nearby the candidate triple are context information outside the triple . 
In details , the Ô¨Årst view hr‚Üítpredictstbased onh , rand their context , the second view rt‚Üíh predictshbased onr , tand their context , the third view ht‚Üírpredictsrbased onh , tand their context , and the fourth view hrt‚Üípredicts the plausi - bility givenh , r , tand their context . 
We denote the view set asV , containing the four views mentions above . 
These views evaluate the candidate triple from different viewpoints and can be integrated to give better prediction . 
In the prediction condition of each view , the context information includes the entities and relations nearby the candidate triple , and excludes the entities and relations that can only be reached by way of the entity or relation to be predicted . 
The content and context can be jointly represented as the subgraph surrounding the candidate triple . 
For each of the Ô¨Årst three views , the entity of relation to be predicted is replaced by a speciÔ¨Åc placeholder . 
The sub - graph can be extracted by breadth-Ô¨Årst traversal from the candidate triple , without passing by the entity or relation to be predicted . 
In the traversal procedure , two hyperparameters dandware introduced to restrict the depth and width of the sub - graph . 
SpeciÔ¨Åcally , ddeÔ¨Ånes the maximum distance between an entity and the candidate triple , andwdeÔ¨Ånes the maximum branch count when passing by an entity . 
The sub - graphs can be linearized as sequences of of symbols with paired brackets in speciÔ¨Åc positions . 
The linearization facilitates the sequential encoding of graphic structures , which is proved to be effective and efÔ¨Åcient in syntactic parsing . 
Table 1 shows the learning views and Figure 1 shows the content and context information for each view . 
3.2 Instance Generation Given a learning view v‚ààV , we deÔ¨Åne a pair of instance generation functions , gvandg‚àí v , to generate positive and negative classiÔ¨Åcation instances for a candidate triple under this view . 
The instances are used as classiÔ¨Åcation instances for triple evaluation . 
In an instance /angbracketleftx , y / angbracketright , the source part xis a linearized sequence representing a sub - graph , and the target part yis a label indicating an entity , a728 Figure 2 : The overall multi - task learning architecture for the multi - view learning model . 
relation or a boolean symbol . 
They correspond to the input and output for the learning of the classiÔ¨Åcation models . 
For a given triple and a given viewv , we always generate one positive view instance , but only generate a negative instance with a certain frequency œÅv . 
The frequencies for the Ô¨Årst three views should be much smaller than 1 in order to balance the instances with respect to the classiÔ¨Åcation labels . 
The positive instances are generated directly according to the schemas of the views . 
The negative instances are necessary for the learning of the triple evaluation model especially for the forth view . 
The source part of a negative instance can be generated by replacing a random component in the tuple with a random symbol of the same type , to satisfy the condition that the changed tuple is not equal to or inside of a triple in the KG . 
The target part for a negative instance is none for the Ô¨Årst three views , andfalse for the fourth view . 
Table 1 shows the instance generation functions and their instances . 
For each learning view , both positive and negative instances generated from the training triples are used for training , while only positive instances generated from the candidate triple are needed for testing . 
The classiÔ¨Åcation models for the learning views can be trained with separated classiÔ¨Åers or in a multi - task framework . 
To promote the information sharing and interaction between learning views , we realized the multi - view model in a multi - task learning architecture , where each subtask takes charge of a speciÔ¨Åc learning view . 
In the multi - task architecture , the instances for a trainingor testing triple are simultaneously assigned to the sub - tasks according to their corresponding views . 
The details for realization will be described in the next section . 
3.3 Triple Evaluation Given a candidate triple , four classiÔ¨Åcation instances are generated for the learning views by the corresponding positive instance generation functions . 
The evaluation given by each learning view is obtained by evaluating the corresponding instance with the corresponding classiÔ¨Åer . 
The evaluation given by the whole multi - view model is the weightedly summation of the evaluations given by these views : f(h , r , t ) = /summationdisplay v‚ààVwvfv(h , r , t ) The function fvand the hyperparameter wvindicate the view - speciÔ¨Åc evaluation function and its weighting coefÔ¨Åcient , respectively . 
The view - speciÔ¨Åc evaluation function invokes the classiÔ¨Åcation model of the view with the source part of the instance as input , and returns the prediction score corresponding to the target part of the instance : fv(h , r , t ) = /summationdisplay v‚ààVwvFv(g+ v(h , r , t ) ¬∑ x)[g+ v(h , r , t ) ¬∑ y ] The functionFindicates the classiÔ¨Åcation procedure of the sub - task corresponding to a speciÔ¨Åc learning view , it takes the source part of the instance as input and gives the prediction scores729 on all possible labels . 
The operator ¬∑ indexes the source or target part of the instance , and the operator [ ] indexes the score corresponding to the target part . 
For each triple in the testing set , we should compare its validity with those of the candidate triples , which are generated by replacing the head or tail entity with another entity . 
This means that , for a KG with millions of entities , millions of candidate triples should be evaluated by the multi - view model for each testing triple . 
To promote the efÔ¨Åciency of the multi - view model , we adopt a coarse - to-Ô¨Åne strategy in testing , where the Ô¨Årst or second view is performed to give a list of k - best candidates , and the overall model is then performed to evaluate these candidates . 
4 Realization : Multi - task Architecture We implement the multi - view learning in a multitask architecture , where each sub - task takes charge of a speciÔ¨Åc learning view . 
The multi - task learning strategy enables information sharing and interaction between the sub - tasks , thus leading to better performance . 
4.1 Overall Pipeline We design a uniÔ¨Åed neural multi - task learning architecture for the multi - view model . 
The overall procedure of the multi - task architecture is shown in Figure 2 . 
The overall procedure is composed of three stages , instance generation , instance classiÔ¨Åcation and prediction integration . 
The instance generation stage takes as input the given triple , and generates classiÔ¨Åcation instances for all learning views by the instance generation functions . 
The instance classiÔ¨Åcation stage takes as input the source parts of these instances , and predicts the labels for each input with the corresponding view - speciÔ¨Åc classiÔ¨Åcation model . 
The prediction integration stage takes as input the predictions of all the classiÔ¨Åcation models , and computes the overall training cost and evaluation score according to the target parts of the instances . 
Note that we need not compute the overall evaluation score for training , nor generate the negative instances for testing . 
In the instance classiÔ¨Åcation stage , all the classiÔ¨Åcation models follow the same pipeline composed of embedding , encoding and predicting . 
For predicting , these models adopt separated predicting layers due to their essentially different learning objects . 
For embedding and encoding , these modelsadopt the shared layers following the conventional strategy in NLP multi - task learning work . 
This is reasonable because the relationship between an instance and its components is analogous to that between a sentence and its words . 
The architecture in Figure 2 shows the multi - task learning architecture with shared embedding and encoding layers . 
We add a speciÔ¨Åc symbol indicating the learning view at the beginning of the source part of the instance . 
This is similar to the idea in multilingual NMT that a speciÔ¨Åc markup is added at the beginning of a source language sentence to indicate the target language . 
The marked source parts of the instances are input into the same encoding layer . 
According to the added markups , the neural network learns and applies different information propagation regularities for instances of different views , while sharing network parameters as much as possible . 
4.2 Neural ClassiÔ¨Åer We use multi - layer Transformer as the encoding layers and logistic regression with softmax as the classiÔ¨Åcation layers . 
Given the source part of an instance , x= ( x1,x2, ... ,x n ) , which is a sequence of entities and relations with paired brackets indicating an linearized sub - graph , we construct the representation for each element xi‚ààxas : h0 i = xe i+xp i wherexe iis the element embedding and xp ithe position embedding , indicating the current element and its position in the sequence , respectively . 
We feed these representations into a stack of Lsuccessive Transformer encoders as : hl i = Transformer ( hl‚àí1 i ) , l= 1,2, ... ,L where hl iis the hidden state of xiafter thel - th encoding layer . 
We omit the detailed description of Transformer since it is already ubiquitous recently . 
The representation used for the subsequent classiÔ¨Åcation layer is the concatenation of the Ô¨Ånal hidden states corresponding to the components of the triple for evaluation . 
Note that for the Ô¨Årst three views , one of the three components is a placeholder . 
The training procedure aims to Ô¨Ånd the parameters minimizing the cross - entropy loss : L(Œ∏ ) = /summationdisplay z‚ààKG / summationdisplay v‚ààV / summationdisplay /angbracketleftx , y / angbracketright‚àà{g+ v(z),g‚àí v(z)}C(Fv(x , Œ∏),y)730 SettingFB15k-237 WN18RR Content + Context Content + Context MR MRR H@10 MR MRR H@10 MR MRR H@10 MR MRR H@10 V- hr‚Üít161 .267 .431 209 .289 .485 2420 .408 .477 2262 .412 .498 V- rt‚Üíh155 .277 .443 178 .296 .476 3318 .377 .437 3573 .393 .473 V- ht‚Üír150 .294 .468 215 .310 .481 2824 .424 .491 2713 .462 .522 V- hrt‚Üí 156 .290 .475 161 .335 .492 3011 .421 .477 2713 .436 .509 V 139 .330 .491 151 .359 .521 2193 .446 .526 2210 .484 .540 Table 2 : The contributions of the individual views to the overall model , evaluated on the development sets . 
FB15k-237 WN18RR Statistics # entrity 14,541 40,943 # relation 237 11 Train 272,115 86,835 Partition Develop 17,535 3,034 Test 20,466 3,134 Table 3 : The statistics of FB15k-237 and WN18RR , including number of entities , relations , and triples in each partition . 
Here , we useFto indicate the feedforward procedure , Cto indicate the cross - entropy cost function , andKGto indicate the set of training triples . 
In the testing procedure , only positive instances are used for a testing triple . 
The testing procedure evaluates a triple by integrating the four views as mentioned before . 
5 Experiments 5.1 Datasets and Evaluation Protocol We evaluate the multi - view model on two widely used benchmark datasets , FB15k-237 and WN18RR , which are subsets of two common datasets FB15k and WN18 . 
The original FB15k and WN18 are easy for KGC due to the reversible relations , it could not reÔ¨Çect the real performance of KGC models . 
Therefore , researchers create FB15k-237 and WN18RR to Ô¨Åx the reversible relation problem , and make the KGC task more realistic ( Toutanova and Chen , 2015 ; Dettmers et al . 
, 2018 ) . 
The statistics of the datasets are summarized in Table 3 . 
The purpose of KGC is to predict a missing entity given a relation and another entity . 
Following Bordes et al . 
( 2013 ) , for every testing triple , we replace the head or tail entities with all entities existed in the knowledge graph , and rank these triples in ascending order according to the triple evaluation function , following the Ô¨Åltered setting protocolwhich does not consider any corrupted triples that appear in the original KG . 
Following ( Nguyen , 2017 ) , we use three common evaluation metrics , mean rank ( MR ) , mean reciprocal rank ( MRR ) , and the proportion of the valid test triples ranking in topnpredictions ( H@n ) with n‚àà{1,3,10 } . 
5.2 Details for Training and Testing The multi - view model is trained with instances generated from the training triples , and is used to evaluate the instances generated from the testing triples . 
There are parameters to be tuned in the procedures of instance generation , model training , and model testing . 
For the deÔ¨Ånition of the subgraph indicating the content and context of a triple , the maximum depth dand widthwwill be determined in the developing procedure . 
For the transformer used for classiÔ¨Åcation , the number of Transformer blocks isL= 6 , the number of self - attention heads is A= 4 , and the hidden size and the feed - forward size areD= 256 and2D= 512 , respectively . 
The dropout strategy is applied on embedding and encoding layers with dropout rate 0.5 . 
We adopt the Adam algorithm ( Kingma and Ba , 2014 ) for tuning with a learning rate Œ∑= 5√ó10‚àí4 . 
The multi - view model is trained with batch size B= 256 for at most 1000 epochs . 
For the coarse - to-Ô¨Åne prediction strategy in the testing procedure , the number k of best candidates given by the Ô¨Årst or second view is determined on the development set . 
We choose œÅ= [ 0.001,0.001,0.01,1.0]for negative instance generation , d= 2 andw= 3 for sub - graph retrieval , and w= [ 0.30,0.30,0.25,0.15]for view combination by grid search experiments on development sets . 
The above models are implemented on PaddlePaddle1 . 
1https://github.com/PaddlePaddle/Paddle731 ModelFB15k-237 WN18RR MR MRR H@1 H@3 H@10 MR MRR H@1 H@3 H@10 R - GCN+ - .249 .151 .264 .417 - - - - KB - LRN 209 .309 .219 - .493 - - - - ConvE 246 .316 .239 .350 .491 5277 .460 .390 .430 .480 ConvR - .350 .261 .385 .528 - .475 .443 .489 .537 RotatE 177 .338 .241 .375 .533 3340 .476 .428 .492 .571 TuckER - .358 .266 .394 .544 - .470 .443 .482 .526 pLogicNet 173 .332 .237 .367 .524 3408 .441 .398 .446 .537 SimpleClassiÔ¨Åcation 161 .307 .223 .382 .525 2193 .446 .393 .456 .522 MultiView 134 .320 .276 .412 .544 1738 .463 .462 .494 .549 Table 4 : Performance of multi - view learning compared with previous methods , on the testing sets of FB15k237 and WN18RR . 
R - GCN+ : ( Schlichtkrull et al . 
, 2017 ) , KB - LRN : ( Garcia - Duran and Niepert , 2017 ) , ConvE : ( Dettmers et al . 
, 2018 ) , ConvR : ( Jiang et al . 
, 2019 ) , RotatE : ( Sun et al . 
, 2019 ) , TuckER : ( Bala Àázevi¬¥c et al . 
, 2019 ) , pLogicNet : ( Qu and Tang , 2019 ) . 
SimpleClassiÔ¨Åcation : multi - view model based on simple classiÔ¨Åcation ( hr‚Üít and rt‚Üíh ) , MultiView : multi - view model with all components ( hr‚Üít+rt‚Üíh+ht‚Üír+hrt‚Üí ) . 
5.3 Main Results and Analysis We verify the effectiveness of the multi - view model , by investigating the contributions of the learning views to the overall model . 
Table 2 shows the performance on the development sets of the two datasets . 
Note that for each experimental setting , the model is retrained on the classiÔ¨Åcation instances generated according to the views in the setting . 
We Ô¨Ånd that each of the learning views contributes to the Ô¨Ånal performance , and context information brings further improvement . 
The performance of the multi - view model on the testing sets of the two datasets is shown in Table 4 , where the performance of methods in previous work is also listed . 
The multi - view learning model achieves the new state - of - the - art on both benchmark datasets . 
Compared with previous work , it gives signiÔ¨Åcantly better MR on both datasets . 
It reveals that in the multi - view model , the answers are high in the ranked lists on average . 
Considering that it does not use any optimization tricks , we think that it still has potential for further improvement by intruding additional information and resources , such as pre - trained embeddings , text descriptions and surface morphologies of entities and relations . 
We also Ô¨Ånd that , the simple classiÔ¨Åcation model based on the Ô¨Årst two views , which brutally predict the head and tail entities according to the rest components , achieves very promising results . 
In other words , the Ô¨Årst two views lead to simple but effective classiÔ¨Åcation - based KGC models . 
The simple classiÔ¨Åcation model works very fast in evaluation of candidate triples , since direct pre200 400 600 800 1000 1200 1400 1600 1800 2000 k - best707580859095Recall FB15k-237 WN18RRFigure 3 : The recall curves of k - best pre-Ô¨Åltering . 
diction of the missing entities is equivalent to evaluating thousands of candidate triples simultaneously . 
We can adopt a coarse - to-Ô¨Åne strategy in realistic applications . 
It pre - selects the k - best candidates by the Ô¨Årst two views , and reranks the candidates by the whole multi - view model . 
Figure 3 shows the experimental results . 
The quality of the candidate list is measured with recall , indicating the percentage of the instances for which the candidate lists contain the answers . 
We Ô¨Ånd that the pre - selection of2000 -best list achieves very high recalls on the two datasets , especially on FB15k-237 . 
Therefore , we can safely Ô¨Ålter out most of the candidates with little loss of Ô¨Ånal precision . 
It facilitates the introduction of more features in the multi - view model by restricting the search space to a small but precise k - best list.732 6 Conclusion We propose a novel multi - view classiÔ¨Åcation model for knowledge graph completion , where multiple classiÔ¨Åcation views are performed based on both content and context information for candidate triple evaluation . 
The multi - view model is implemented with a simple and uniÔ¨Åed multi - task learning architecture where the parameters are shared across all the learning views . 
It achieves the new stateof - the - art although without using any optimization tricks . 
The multi - view model can be improve from two perspectives in the future . 
First , the multi - view model can leverage more kinds of information and resources for better performance , such as the descriptions of the entities and relations , as well as related information in external knowledge bases . 
Second , the multi - task learning architecture can introduce different kinds of neural networks to better model different kinds of information , for example , sequential neural networks for sequences and graph neural networks for graphs . 
Acknowledgments The authors Chen and Xu were also supported by the National Nature Science Foundation of China ( No . 
61876198 , 61976015 , 61370130 and 61473294 ) , the Fundamental Research Funds for the Central Universities ( No . 
2018YJS025 ) , the Beijing Municipal Natural Science Foundation ( No . 
4172047 ) , and the International Science and Technology Cooperation Program of China under Grant No . 
K11F100010 . 
We sincerely thank Quan Wang in Baidu for the enlightening suggestions in the research procedure , and the anonymous reviewers for their valuable comments and suggestions . 
Abstract Named entity disambiguation is an important task that plays the role of bridge between text and knowledge . 
However , the performance of existing methods drops dramatically for short text , which is widely used in actual application scenarios , such as information retrieval and question answering . 
In this work , we propose a novel knowledge - enhanced method for named entity disambiguation . 
Considering the problem of information ambiguity and incompleteness for short text , two kinds of knowledge , factual knowledge graph and conceptual knowledge graph , are introduced to provide additional knowledge for the semantic matching between candidate entity and mention context . 
Our proposed method achieves signiÔ¨Åcant improvement over previous methods on a large manually annotated short - text dataset , and also achieves the state - of - the - art on three standard datasets . 
The short - text dataset and the proposed model will be publicly available for research use . 
1 Introduction Name entity disambiguation ( NED ) aims to associate each entity mention in the text with its corresponding entity in the knowledge graph ( KG ) . 
It plays an important role in many text - related artiÔ¨Åcial intelligent tasks such as recommendation and conversation , since it works as a bridge between text and knowledge . 
In decades , researchers devoted their efforts to NED in many ways , including the rule - based methods ( Shen et al . 
, 2014 ) , the conventional statistic methods ( Shen et al . 
, 2014 ) and the deep learning methods ( OctavianEugen Ganea , 2017 ) . 
On formal text , state - of - theart methods achieve high performance thanks to the well - written utterance and rich context . 
However , experiments show that the performance of these methods degrades dramatically on informal text , for example , the short text widely used in many real application scenarios such as information retrieval and human - machine interaction . 
It is difÔ¨Åcult for existing methods to make decisions on the non - standard utterance without adequate context . 
The discrimination procedure of NED depends on sufÔ¨Åcient context in the input text , which is usually noisy and scarce in the short text used in information retrieval and human - machine interaction . 
For example , an analysis based on search engine logs demonstrates that a search query contains 2.35 words on average ( Yi Fang , 2011 ) . 
Such short text could not provide adequate context which is necessary for NED models . 
In recent years , many efforts improve NED by exploiting more powerful models and richer context information ( Shen et al . 
, 2014 ) . 
These methods mainly focus on the better utilization of existing context . 
Therefore , they can not improve NED effectively on short text since the problem of information shortage still exists . 
Intuitively , it is hard for NED to achieve essential improvement on short text if it can not exploit external information to enhance the recognition procedure . 
In information scarce situations , human beings can still perform recognition by association with related external information , such as commonsense or domain - speciÔ¨Åc knowledge . 
It inspires us that , NED on short text could be improved if appropriate external knowledge can be retrieved and considered . 
We propose a novel knowledge - enhanced NED model , where the prediction procedure of NED is enhanced by two kinds of knowledge formalized as two different KGs . 
The one kind is conceptual knowledge formalized as a conceptual KG , it is used to augment the representation of the entity mention by giving the mention a concept embedding . 
The other kind is factual knowledge formalized as a factual KG , it is used to augment the representation of each candidate entity by giving the entity an entity embedding . 
The augmented735 Which singer sang Li Bai ? ( Wedding photos of Li Bai and Sa Beining ? ) ( QWLW\'LVDPELJXDWLRQ IRU6KRUW7H[W‡π´·åÆ[·Éã‡±≠·•Ø·úã]‡π´·åÆ[·¶∏’à]‡π´·åÆ[‡∏Å‡∏à]‡π´·åÆ[‡æà‡πÉ ] ‡π´·åÆ[’à·áî ] ‡π´·åÆ[·ä™·•§€Ç ] * Li Bai ( ‡π´·åÆ ) has 21 ambiguous entities   in KB . 
. 
. 
.‡π´·åÆ‡∏é ·ß° ‡†ñ·å±‡æà·ùï·µú‡π´·åÆ‡ØÜ‘ç·àª‡π´·åÆŸü·¨¶ﬂ∫‘∂‡πç›∑·å±·¶∏‡π´·åÆ”®‡∂Ü·®¨‡®ò·å±·ïÆ‡ßí·Üô . 
QRZOHGJH%DVH ( How to play the hero Li Bai ? ) ( What famous poems written by Li Bai ? ) ( Li Bai [ Song ] ) ( Li Bai [ Game Character ] ) ( Li Bai [ Poet ] ) ( Li Bai [ Celebrity ] ) ( Li Bai [ Person ] ) ( Li Bai [ Tvshow ] ) isa ,                             Songisa ,   Game Character , Heroisa ,                 Poet , PersonDynamic Conceptualizaition Li BaiPoet ( ·¶∏’à)Li BaiCelebrity ( ‡∏Å‡∏à)Sa Beining‡∂Ü·®¨‡®ò|‡©ú‡∂ÜspousePoemrelatedWritingrelatedEntity Context EmbeddingFigure 1 : Short text entity disambiguation and our method . 
We solve the problem of entity disambiguation of sparse short texts through dynamic conceptualization and entity context embedding . 
representations of the mention context and the candidate entities are used in a matching network for better NED prediction . 
We validate the knowledge - enhanced NED model on three public NED datasets for short text ( NEEL , KORE50 and FUDAN ) as well as our new dataset ( DUEL ) , which is constructed for information acquiring scenarios and will be publicly available for research use . 
Experiments show that the knowledge - enhance NED model performs signiÔ¨Åcantly better than previous methods . 
It shows the effectiveness of external knowledge in improving the prediction of NED in information scarce situations . 
The contribution of our work includes two aspects . 
First , we introduce conceptual and factual knowledge to improve NED for short text for the Ô¨Årst time , and achieve signiÔ¨Åcant improvement . 
Second , we release a large - scale good - quality NED dataset for short text for information acquisition scenarios , which is complementary existing datasets . 
The rest of this paper is organized as follows . 
We Ô¨Årst introduce the NED task and the baseline method ( section 2 ) , and then describe the architecture and details of our knowledge - enhanced model ( section 3 ) . 
After giving the detailed experimental analysis ( section 4 ) , we give the related work ( section 5 ) and conclude the work . 
2 Task DeÔ¨Ånition and Baseline Model NED is a fundamental task in the area of natural language processing and knowledge base . 
It aims toassociate each entity mention in the given text with its corresponding entity in the given KG . 
Formally , given a KGGand a piece of textT , it assigns each mentionm‚ààT with an entity e‚ààG indicating thatmrefers toe , or with the symbol œÜindicating that there is no corresponding entity . 
The disambiguation procedure can be formalized as matching between the context of the mention and each candidate entities . 
f(m ) = Ô£± Ô£≤ Ô£≥arg maxe‚ààe(m)(s(e , c(m ) ) ) , e(m)/negationslash=‚àÖ œÜ , otherwise(1 ) Here , the function ereturns the entity candidate set for a given mention , and the function creturns the context of the given mention . 
The function s is used to evaluate the matching degree between context and candidate , and is usually implemented as matching networks . 
If the entity candidate set is empty or the highest matching score is bellow a given threshold , the function freturnsœÜfor the given mention . 
The conditions GandTin the functions eandcare omitted in the equation for simplicity . 
We adopt the deep structured semantic model ( DSSM ) ( Huang et al . 
, 2013 ) as the baseline model for NED(Nie and Pan , 2018 ) . 
Based on a selfattention matching network , DSSM maps the candidate entities and the context to the same semantic space , and Ô¨Ånds the candidate entity that best semantically matches the context . 
The representation learning for both entities and contexts is enhanced736 Product AttentionFCSim Loss Self AttentionLSTMembeddingCandidateEntity ( CE)Mention and Context ( MC)Entity Context EmbeddingEntity Text MetaEntity RelationEntity Embedding Knowledge Feature NetworkGraph ConstructionConceptualizeDynamic ConceptualizationEntity Context Emb Conceptualization EmbEntity Emb Context EmbKnowledge BaseScore(CE , MC)Who is the singer of Li Bai?Li Bai6HOI$WWHQWLRQ0DWFKLQJ1HWZRUNEDVHG'660Figure 2 : The architecture of our proposed entity disambiguation model K - NED . 
by word2vec ( Tomas Mikolov , 2013 ) . 
In this work , we focus on the problem of NED itself , that is , predicting the right entity candidate for each given entity mention . 
The entity mentions needed for NED are simply derived from the results of named entity recognition ( NER ) . 
3 Knowledge - Enhanced NED Model For short text used in information acquisition scenarios such as information retrieval and question answering , the lack of both lexical and syntactical information obstacles the precise disambiguation of entity mentions . 
For human beings , however , the problem of information scarcity does not hinder the disambiguation procedure . 
This is because there are many implicit assumptions and apriori knowledges in these information acquisition scenarios , which can be effectively considered by association and imagination during disambiguation procedure . 
Inspired by this , we propose a knowledge - enhanced NED model ( K - NED ) for short text , where two kinds of knowledge are introduce to provide additional information for better disambiguation performance . 
Figure 2 gives the overall architecture of the model . 
The overall procedure of the K - NED model is a pipeline including feature extraction and semantic matching , where the former is composed of two sub - procedures , taking charge of feature extraction for mention context and candidate entity , respectively . 
Rather than considering only the utterance of the input text , the feature extraction procedure also considers external knowledge for betterrepresentation . 
In details , the feature extraction sub - procedure for mention context is enhanced by conceptual knowledge formalized as a conceptual KG , which augments the representation of the mention context by giving each word a concept embedding ; while the sub - procedure for candidate entity is enhanced by factual knowledge formalized as a factual KG , which augments the representation of each candidate entity by giving the entity an entity embedding . 
The augmented representation is used in the following semantic matching procedure for better prediction . 
The major difference of the K - NED model is the introduction of external knowledge in the representation learning procedure . 
For the representation learning procedure , it simply uses the pre - trained word2vec language model to take charge of the conventional utterance representation learning . 
For the semantic matching procedure , it directly adopt the self - attention matching network based on DSSM . 
Given a mention mand a candidate entity e , the word2vec - based module gives two representation vectors , rlm mandrlm e , while the KG - based modules give another two representation vectors , rkg mand rkg e. The concatenation of the four representation vectors is fed into the matching network to obtain the matching degree . 
Based on the utterance of more , the word2vec - based representation vector rlm morrlm eis obtained by averaging the hidden representations for the words or characters in the utterance . 
We omit the detailed descriptions of the word2vec - based feature extraction and the DSSM-737 based semantic matching owing to space limitations . 
In the following subsections , we describe in details the computation procedures for the KGbased feature extraction . 
3.1 KG - enhanced Representation of Mention The concepts in a conceptual KG can be treated as upper classes of the entities in the factual KG . 
A concept is a name or label representing a concrete or material existence such as a person , a place or a thing . 
For example , the entity apple , maybe corresponds to the concept of fruits , companies and songs . 
For a mention , we label the mention word with a concept and use the concept representation as additional feature representation of the mention . 
Intuitively , the concept labeling procedure works as a semantic bridge between the mentions and the entities . 
Different from traditional methods where mentions are classiÔ¨Åed into coarse - grained entity types , the concept labeling procedure in our work classiÔ¨Åes the mentions into Ô¨Åne - grained concepts , which can better utilize the context of the mentions and provide more information for disambiguation . 
We adopt a graph - based labeling algorithm for concept labeling , as shown in Figure 3 . 
Given a short sentence , it Ô¨Årst builds a knowledge feature network ( KFN ) based on the short sentence and reference conceptual / factual KGs , and then searches for the appropriate concept for the mention by a random walking algorithm . 
The KFN is built according to the correspondence between the symbols in the short sentence and the reference KGs . 
The symbols include words , entity mentions and candidate concepts , where the words and mentions are obtained by lexical analysis and entity recognition , and the concepts are obtained by matching on the reference KGs . 
The KFN describes three kinds of relationships , that is , the entity - concept relationship , the concept - concept relationship and the word - concept relationship . 
The concept - entity relationship is represented by the generation probability from concept cto entity e. The probability p(c|e)is calculated based on the page - view ( PV ) statistics of the Wikipedia entity pages : P(c|e ) = NPV(e)/summationtext e / prime‚ààcNPV(e / prime)(2 ) The concept - concept relationship is represented by the transition probability between two concepts , isA networkconcept cooc networklexical networkKnowledge BaseNQRZOHGJHIHDWXUHQHWZRUNVXEJUDSKFRQVWUXFWLRQ wordsegmententity recognitionsubgraphbuilding random walk with restartFRQFHSWXDOL]DWLRQ Short text : Which singer sang LiBai?LiBaisingersangSongGameCharacterPoetCelebrityPerson0203040.030.070.020.1conceptsignal wordentity Figure 3 : Architecture of Ô¨Åne - grained conceptualization , which consists of three parts : ( a ) Knowledge Feature Network . 
( b ) Sub - graph construction . 
( c ) Conceptualization . 
ciandcj . 
The probability P(ci|cj)is calculated based on the co - occurrence frequencies of the entities under the two concepts : P(ci|cj ) = /summationtext ej‚ààcj , ei‚ààciN(ej , ei ) /summationtext c‚ààC / summationtext ej‚ààc , ei‚ààcN(ej , ei)(3 ) where the co - occurrence frequency N(ej , ei ) , is calculated based on the statistics of anchor links of Baidu Encyclopedia , and wis the size of the window that counts the co - occurrences frequencies of the entity pair in Baidu Encyclopedia . 
In this paper , wis set to 25 . 
N(ej , ei ) = freq w(ej , ei ) ( 4 ) The word - concept relationship is represented by the labeling probability between the word wand the related concept c. The probability is calculated based on the word frequency and word - concept co - occurrence frequency : P(c|w ) = N(c , w ) N(w)(5 ) We perform a random walk algorithm ( JiaYu Pan , 2004 ) on the KFN to get the appropriate concept of entity mention . 
First , we initialize the weights of the nodes and the edges by : E0(e ) = /braceleftbiggP(c|t)ifeisc‚Üít P(ci|cj)ifeiscj‚Üíci(6 ) N0(n ) = /braceleftbigg1/|T|ifnisentity 0 ifnisconcept(7 ) Second , we iteratively update the node and edge by : Nk= ( 1‚àíŒ±)E / prime√óNk‚àí1+Œ±N0(8)738 Ek‚Üê(1‚àíŒ≤)Nk+Œ≤Ek(9 ) whereŒ±andŒ≤are hyper - parameters tuned on developing sets . 
Finally , we normalize the edge weights and obtain the concept type with the highest weight : c‚àó= arg max cP(c|t ) = arg max cE(t‚Üíc)/summationtext ciE(t‚Üíci)(10 ) 3.2 KG - enhanced Representation of Entity The conventional representation for an entity is the textual representation of the entity . 
Inspired by the wide usage of distributed representation of KG entities in many NLP applications , we think that such knowledge representation is also helpful in NED . 
In this work , we use both textual and knowledge representation to better represent the semantics of candidate entities . 
We propose a novel representation learning method which can simultaneously learns both kinds of knowledge . 
Based on the related textual context and other information of the entities , it uses the CBOW model with a sigmoid layer to generate the distributed representation of the entities . 
Knowledge BaseText entity embedding(entity , desc)Positive Samplekeytexts and entities related to entitiesNegative SampleHierachical negative sampleCBOW + single layer NNskipgramsequence of ( entity , , entity , entity≈è)pretrain vectorentity embeddingb ) entity relationsa ) entity text meta      Figure 4 : Entity context embedding architecture which combines entity relations and the entity context . 
The detailed training process for the two models will now be introduced . 
Figure 4 shows that the entityeand its description generate the entity embedding . 
First , a positive sample is generated by entity description from KB ( Wikipedia and Baidu Encyclopedia ) , and then word segmentation is applied to the entity description text . 
We have counted the word frequency in positive samples , and negative samples are generated by band - frequency random sampling . 
In order to learn the relationship between entities and enhanced entity representations , we use entity co - occurrence data and KB S - P - O data to generate training samples : ‚Ä¢entity co - occurrence sequence{e1,e2,¬∑¬∑¬∑,en } , which are extracted from KB hyperlinks . 
‚Ä¢S - P - O triples from KB , which are extracted from the key - value block of Wikipedia and Baidu Encyclopedia . 
We obtained entity sequences as training sample , where each entity has an entity embedding . 
Then we updated the entity embedding representation with Skip - Gram Model to enhance the inter - entity relationships . 
Finally , we obtain as the Ô¨Ånal entity embedding . 
Entity embedding vector are input as feature representations of entities into an KGenhanced entity disambiguation network , as shown in Figure 2 . 
4 Experiments and Analysis In this section , we Ô¨Årst introduce the experimental dataset , and construction methods of the dataset we published with this paper . 
Then , we present evaluation metrics , the experiments conducted for both the English and the Chinese datasets with existing approaches , and we analyze the experimental results in detail . 
4.1 Datasets We have experimented on both Chinese and English datasets . 
For the English experiment , we use Wikipedia with a release time of 202003 as KB and apply the framework to NEEL and KORE50 datasets . 
For the Chinese experiment , due to the lack of large - scale short text entity disambiguation datasets , we constructed a dataset called DUEL and use it as the Chinese experiment dataset alongside the FUDAN dataset ( Xu et al . 
, 2017 ) . 
4.1.1 English Datasets Most of the existing datasets on NED are based on long text , which are not suitable for our task . 
Two English datasets could be found that were suitable for short text entity disambiguation . 
Because KORE50 only has test data , but no training data , we use the training samples of NEEL as the training samples of KORE50 as well , to compare their performances . 
‚Ä¢NEEL(Rizzo et al . 
, 2017 ): The training dataset consists of 6,025 tweets , the validation dataset consists of 100 tweets , and the testing dataset consists of 300 tweets.739 ‚Ä¢KORE50(Hoffart et al . 
, 2012 ): It contains 50 short sentences with highly ambiguous mentioned entities . 
It is considered to be among the most challenging for NED . 
Average sentence length ( after removing stop words ) is 6.88 words per sentence and each sentence has 2.96 mentioned entities on average . 
4.1.2 Chinese Datasets The typical size of existing Chinese NED datasets is about a few thousand annotated words ( Rizzo et al . 
, 2017 ; Hoffart et al . 
, 2012 ) . 
Because there is a lack of existing data sets for short text NED , we manually construct the largest available human annotated Chinese dataset , and we have released it to the global research community , please refer to this ( DUEL ) for more data details . 
4.1.3 Construction of Our Dataset Our dataset provides a high - precision manuallyannotated entity disambiguation dataset consistin of 100,000 short texts . 
The text corpus consists of queries and web page titles . 
The annotated entities are in the general domain , including instances ( e.g. Barack Obama ) and concepts ( e.g. Basketball player ) . 
Table 1 and 2 depict the statistical data of the KB and the annotated text . 
Table 1 : Statistics of knowledge base in our dataset . 
AvgNumOfEntityProperties is the average number of attributes for all entities . 
Statistic KB # Entities 398082 # SPO 3564565 # EntitiyDesc 361778 # AvgNumOfEntityProperties 9 Data Annotation Method : We annotated the entire short text in the dataset by crowd - sourcing . 
The same data was repeatedly labeled by three domain experts , then reviewed and released by additional experts . 
The average precision of annotating entities is about 95.2 % . 
The evaluation method of dataset is as follows : given an input of a short text q , the annotated entities is E / prime q = e / prime 1,e / prime 2,e / prime 3 , .... By comparing the outputs E / prime qwith additional expertsannotated set Eq = e1,e2,e3 , ... , precisionPis deÔ¨Åned as follows . 
P=/summationtext q‚ààQEq‚à©E / prime q / summationtext q‚ààQEq(11)Comparison with previous datasets : As summarized in Table 2 , FUDAN is a representative evaluation dataset for Chinese short text entity disambiguation , which consists of manually annotated short text . 
Both FUDAN and DUEL consist of entities in various domains ( including instances and concepts ) , such as persons , movies , and general concepts . 
However , DUEL is much larger . 
4.2 Results and Analysis 4.2.1 Evaluation Metrics We directly use the gold standard in mentioned entities - the NER results in the dataset , and choose standard micro F1 score as our performance metric for NED task ( aggregated over all mentions ) . 
4.2.2 Performance Comparison with Other Approaches In order to verify the enhancement of different methods used in NED , we compare the proposed method with several state - of - the - art approaches both for the Chinese and the English datasets . 
All of these methods are effective and comparable in the case of short text . 
Our method is called knowledge - enhanced NED ( K - NED ) . 
‚Ä¢FEL(Blanco et al . 
, 2015 ): A toolkit for training models to link entities to KB in documents and queries . 
And we use DSSM model to use this entity embedding for comparing . 
We experiment with default parameters . 
‚Ä¢NTEE ( Yamada et al . 
, 2017 ): A neural network model that learns embedding of texts and Wikipedia entities , and then use them in entity linking task . 
We experiment with default parameters . 
‚Ä¢Mulrel - nel ( Le and Titov . 
, 2018 ): A python implementation of multi - relational NED . 
We experiment with default parameters . 
‚Ä¢Fudan ( Xu et al . 
, 2017 ): Entity linking of Fudan University which is a Chinese entity linking service API . 
As summarized in Tables 3 , the experimental results indicate that our approach K - NED outperforms existing state - of - the - art methods such as FEL , NTEE , Mulrel - nil and Fudan on Chinese and English datasets except on KORE50 . 
In particular , our method disambiguate to all correct result of the examples in Figure 1 . 
We found that 72%740 Table 2 : Comparisons between DUEL and the FUDAN dataset . 
AvgLen is the average length of the annotated text . 
AvgNumEntity is the average number of entities in the annotated text . 
Statistic DUEL FUDAN NEEL KORE50 # Train 90000 - 6025 # Dev 10000 - 100 # Test 10000 1037 300 50 # AvgLen 21.73 23.38 16.5157 6.88 # AvgNumEntity 3.43 2.08 2.1 2.96 # Accuracy 95.2 % - - Table 3 : F1 scores on Chinese and English datasets . 
Method Datasets Chinese datasets DUEL FUDAN Fudan 0.861 0.945 Mulrel - nel 0.889 0.893 K - NED(Ours ) 0.897 0.947 English datasets NEEL KORE50 FEL 0.601 0.360 NTEE 0.748 0.618 Mulrel - nel 0.805 0.625 K - NED(Ours ) 0.811 0.544 of the types of annotated entities in the KORE50 dataset belong to the category ‚Äù Person ‚Äù , and so it is possible that this dataset distribution is biased . 
Compared to KORE50 , NEEL , FUDAN and DUEL datasets are more consistent with the entity type distribution of practical scenarios . 
NTEE and FEL use representational learning to improve performance , Mulrel - nel relied on supervised systems or heuristics to predict these relations and treat relations as latent variables in neural entity disambiguation model , and our approach uses knowledge enhancement to improve the performance without using other complex features . 
Data analyses demonstrate that each short text contains 3 mentioned entities on average , each of which includes 20 ambiguous entities to be linked , and the context is sparse . 
Experiments demonstrate that knowledge enhancement is helpful for short text entity disambiguation . 
4.2.3 Performance of Knowledge - Enhancement Components In order to gain a deeper understanding of the various components of our model , we compare the difference in performance after removing two com - ponents separately , where all models are trained using the same settings . 
Table 4 : F1 scores of each component on Chinese and English datasets . 
Feature DUEL NEEL K - NED 0.897 0.811 K - NED -DC 0.804 0.755 K - NED -ECE 0.874 0.779 K - NED -DC - ECE 0.759 0.577 As listed in Table 4 , K - NED is the result of our complete model . 
DC represents the Ô¨Åne - grained dynamic conceptualization component , and ECE represents entity context embedding components . 
We Ô¨Ånd that dynamic conceptualization exhibits a 10.36%improvement in performance , and entity context embedding exhibits a 2.56 % improvement in performance on our Chinese dataset : DUEL . 
By the analysis of examples in Figure 1 , we Ô¨Ånd that dynamic conceptualization can mark the concepts of ‚Äù Li Bai ‚Äù in ‚Äù Who is the singer of Li Bai ? ‚Äù , ‚Äù How to play the hero Li Bai ? ‚Äù and ‚Äù Which famous poems are written by Li Bai ? ‚Äù as ‚Äù songs ‚Äù , ‚Äù game characters ‚Äù and ‚Äù poets ‚Äù respectively . 
The correct conceptualization greatly facilitates the entity disambiguation . 
On the other hand , however , although we successfully mark the concept of ‚Äù Li Bai ‚Äù in ‚Äù Wedding photos of Li Bai and Sa Beining ‚Äù as ‚Äù person ‚Äù , it still disambiguates incorrectly without the help of entity context embedding , which indicates that dynamic conceptualization and entity context embedding can be complementary in NED . 
This result demonstrates that the conceptualization of entities is more direct and effective for the semantic disambiguation in short text entity disambiguation . 
In addition , we Ô¨Ånd that Ô¨Åne - grained conceptualization plays a signiÔ¨Åcant role in dynamic conceptualization.741 5 Related works Many efforts have been devoted to NED in recent years . 
Some methods ( Shen et al . 
, 2014 ; Ratinov et al . 
, 2011 ; Shen et al . 
, 2012b , a ; Han , 2015 ) exploit the Learning To Rank framework ( LTR ) ( Liu , 2009 ) to rank the candidate entities , taking advantage of the relationships between all candidates . 
Most commonly used ranking models are the pairwise framework ( Perceptron ( Shen and Joshi , 2005 ) , RankSvm(Chingpei Lee , 2014 ) ) and the listwise framework ( ListNet ( Cao et al . 
, 2007 ) ) . 
( BaoXing et al . 
, 2014 ) proposed a named entity linking method based on a probabilistic topic model(Blei , 2012 ) , which employs the conceptual topic model to map words and mentioned entities into the same topic space . 
( Nakashole et al . 
, 2013 ) used a graphbased collaborative entity linking model . 
( Bilenko et al . 
, 2003 ) proposed using random walks for entity linking . 
Some models choosed to rely solely on the context of the links to learn entity representations , such as ( Lazic et al . 
, 2015 ) , and some methods used a pipeline of existing annotators to Ô¨Ålter entity candidates such as ( Ling et al . 
, 2015 ) . 
Different from these conventional work , we use multiple sources of information and a deep structured semantic model to achieve better NED performance . 
Many efforts have been devoted to NED for queries , such as ( Hasibi et al . 
, 2015 ) and ( Hasibi et al . 
, 2017 ) . 
Some approaches try to solve NED by making extensive use of deep neural networks ( Globerson et al . 
, 2016 ) , or by adopting distributed representations of words or entities ( Yamada et al . 
, 2016 , 2017 ) . 
Other existing approaches take advantage of global context , which captures the coherence between mapped entities of the related keywords in a document ( Cucerzan , 2007 ; Han et al . 
, 2011 ) . 
In ( Globerson et al . 
, 2016 ) , the neural network model uses attention mechanism to focus on the contextual entities to be disambiguated . 
In ( Yamada et al . 
, 2016 , 2017 ) , the distributed representation of contexts models the relationships between words and entities or between documents and entities , where the distances between various vectors provides useful information for disambiguation . 
Different from these work where complicated techniques or features are used , we adopt external knowledge including factual and conceptual knowledge graphs for better NED performance . 
( Radhakrishnan and Varma , 2018 ) proposes a method to train entity embedding for entity similarity , but this method relies on a dense knowledge map , weuse the text and relationship information of entity to model the similarity between entity and context to improve the effect of NED . 
There are also previous work using concept or type information to improve NED performance . 
The models of ( Hua et al . 
, 2015 ; Wang et al . 
, 2015 ; Priya Radhakrishnan , 2018 ; Isaiah Onando Mulang , 2020 ) try to map short text to a concept space , and then generate comprehensive concept vectors to represent the short text . 
( Raiman and Raiman , 2018 ) constructs a type ontology and a type classiÔ¨Åer to map entities to a closed type ontology . 
( Chen and Xiao , 2018 ) proposes to modeling context explicitly by entity concept , but we use a complementary way of coarse - grained and Ô¨Åne - grained to dynamically predict the concept according to the context and improve the effect of disambiguation . 
( Derczynski et al . 
, 2015 ) studies named entity recognition ( NER ) and named entity linking ( NEL ) for tweets . 
Unlike these work , our method uses Ô¨Åne - grained entity concepts and predicts concepts more accurately by using an advanced knowledge feature network . 
6 Conclusion We propose a knowledge - enhanced approach to short text entity disambiguation . 
Through bridging and facilitating semantic understanding of the Ô¨Ånegrained concept associated to a mentioned entity and entity embedding , the performance of entity disambiguation can be signiÔ¨Åcantly improved . 
The experimental results demonstrate that our approach outperforms existing SOTA methods on English and Chinese datasets for this task . 
At the same time , we constructed a large - scale manual - annotated Chinese dataset for short text entity disambiguation , which has been released with the paper for use by researchers . 
As a future direction of research , we plan to explore better conceptualization and semantic understanding methods , and further improve the performance of the short text entity disambiguation task . 
We intend to continue to update our Chinese dataset . 
In the future , We will use more modern embedding(such as BERT ) or encoder(such as transformer ) to obtain better embedding , and we also plan to conduct experiments to verify the effectiveness of our methods in other tasks related to semantic understanding such as Q&A , Dialogue , etc.742 Abstract Relational facts are an important component of human knowledge , which are hidden in vast amounts of text . 
In order to extract these facts from text , people have been working on relation extraction ( RE ) for years . 
From early pattern matching to current neural networks , existing RE methods have achieved signiÔ¨Åcant progress . 
Yet with explosion of Web text and emergence of new relations , human knowledge is increasing drastically , and we thus require ‚Äú more ‚Äù from RE : a more powerful RE system that can robustly utilize more data , efÔ¨Åciently learn more relations , easily handle more complicated context , and Ô¨Çexibly generalize to more open domains . 
In this paper , we look back at existing RE methods , analyze key challenges we are facing nowadays , and show promising directions towards more powerful RE . 
We hope our view can advance this Ô¨Åeld and inspire more efforts in the community.1 1 Introduction Relational facts organize knowledge of the world in a triplet format . 
These structured facts act as an import role of human knowledge and are explicitly or implicitly hidden in the text . 
For example , ‚Äú Steve Jobs co - founded Apple ‚Äù indicates the fact ( Apple Inc. ,founded by , Steve Jobs ) , and we can also infer the fact ( USA , contains , New York ) from ‚Äú Hamilton made its debut in New York , USA ‚Äù . 
As these structured facts could beneÔ¨Åt downstream applications , e.g , knowledge graph completion ( Bordes et al . 
, 2013 ; Wang et al . 
, 2014 ) , search engine ( Xiong et al . 
, 2017 ; Schlichtkrull et al . 
, 2018 ) and question answering ( Bordes et al . 
, 2014 ; Dong et al . 
, 2015 ) , many efforts have been devoted ‚àóindicates equal contribution ‚Ä†Corresponding author e - mail : liuzy@tsinghua.edu.cn 1Most of the papers mentioned in this work are collected into the following paper list https://github . 
com / thunlp / NREPapers .to researching relation extraction ( RE ) , which aims at extracting relational facts from plain text . 
More speciÔ¨Åcally , after identifying entity mentions ( e.g. , USA andNew York ) in text , the main goal of RE is to classify relations ( e.g. , contains ) between these entity mentions from their context . 
The pioneering explorations of RE lie in statistical approaches , such as pattern mining ( Huffman , 1995 ; Califf and Mooney , 1997 ) , feature - based methods ( Kambhatla , 2004 ) and graphical models ( Roth and Yih , 2002 ) . 
Recently , with the development of deep learning , neural models have been widely adopted for RE ( Zeng et al . 
, 2014 ; Zhang et al . 
, 2015 ) and achieved superior results . 
These RE methods have bridged the gap between unstructured text and structured knowledge , and shown their effectiveness on several public benchmarks . 
Despite the success of existing RE methods , most of them still work in a simpliÔ¨Åed setting . 
These methods mainly focus on training models with large amounts ofhuman annotations to classify two given entities within one sentence into pre - deÔ¨Åned relations . 
However , the real world is much more complicated than this simple setting : ( 1 ) collecting high - quality human annotations is expensive and time - consuming , ( 2 ) many long - tail relations can not provide large amounts of training examples , ( 3 ) most facts are expressed by long context consisting of multiple sentences , and moreover ( 4 ) using a pre - deÔ¨Åned set to cover those relations with open - ended growth is difÔ¨Åcult . 
Hence , to build an effective and robust RE system for real - world deployment , there are still some more complex scenarios to be further investigated . 
In this paper , we review existing RE methods ( Section 2 ) as well as latest RE explorations ( Section 3 ) targeting more complex RE scenarios . 
Those feasible approaches leading to better RE abilities still require further efforts , and here we summarize them into four directions:745 ( 1)Utilizing More Data ( Section 3.1 ) . 
Supervised RE methods heavily rely on expensive human annotations , while distant supervision ( Mintz et al . 
, 2009 ) introduces more auto - labeled data to alleviate this issue . 
Yet distant methods bring noise examples and just utilize single sentences mentioning entity pairs , which signiÔ¨Åcantly weaken extraction performance . 
Designing schemas to obtain highquality and high - coverage data to train robust RE models still remains a problem to be explored . 
( 2)Performing More EfÔ¨Åcient Learning ( Section 3.2 ) . 
Lots of long - tail relations only contain a handful of training examples . 
However , it is hard for conventional RE methods to well generalize relation patterns from limited examples like humans . 
Therefore , developing efÔ¨Åcient learning schemas to make better use of limited or few - shot examples is a potential research direction . 
( 3)Handling More Complicated Context ( Section 3.3 ) . 
Many relational facts are expressed in complicated context ( e.g. multiple sentences or even documents ) , while most existing RE models focus on extracting intra - sentence relations . 
To cover those complex facts , it is valuable to investigate RE in more complicated context . 
( 4)Orienting More Open Domains ( Section 3.4 ) . 
New relations emerge every day from different domains in the real world , and thus it is hard to cover all of them by hand . 
However , conventional RE frameworks are generally designed for pre - deÔ¨Åned relations . 
Therefore , how to automatically detect undeÔ¨Åned relations in open domains remains an open problem . 
Besides the introduction of promising directions , we also point out two key challenges for existing methods : ( 1 ) learning from text or names ( Section 4.1 ) and ( 2 ) datasets towards special interests(Section 4.2 ) . 
We hope that all these contents could encourage the community to make further exploration and breakthrough towards better RE . 
2 Background and Existing Work Information extraction ( IE ) aims at extracting structural information from unstructured text , which is an important Ô¨Åeld in natural language processing ( NLP ) . 
Relation extraction ( RE ) , as an important task in IE , particularly focuses on extracting relations between entities . 
A complete relation extraction system consists of a named entity recognizer to identify named entities ( e.g. , people , organizations , locations ) from text , an entity linker to link entiTim Cook is Apple ‚Äôs current CEO.0.050.010.89 ... FounderPlace of BirthCEO Figure 1 : An example of RE . 
Given two entities and one sentence mentioning them , RE models classify the relation between them within a pre - deÔ¨Åned relation set . 
ties to existing knowledge graphs ( KGs , necessary when using relation extraction for knowledge graph completion ) , and a relational classiÔ¨Åer to determine relations between entities by given context . 
Among these steps , identifying the relation is the most crucial and difÔ¨Åcult task , since it requires models to well understand the semantics of the context . 
Hence , RE generally focuses on researching the classiÔ¨Åcation part , which is also known as relation classiÔ¨Åcation . 
As shown in Figure 1 , a typical RE setting is that given a sentence with two marked entities , models need to classify the sentence into one of the pre - deÔ¨Åned relations2 . 
In this section , we introduce the development of RE methods following the typical supervised setting , from early pattern - based methods , statistical approaches , to recent neural models . 
2.1 Pattern Extraction Models The pioneering methods use sentence analysis tools to identify syntactic elements in text , then automatically construct pattern rules from these elements ( Soderland et al . 
, 1995 ; Kim and Moldovan , 1995 ; Huffman , 1995 ; Califf and Mooney , 1997 ) . 
In order to extract patterns with better coverage and accuracy , later work involves larger corpora ( Carlson et al . 
, 2010 ) , more formats of patterns ( Nakashole et al . 
, 2012 ; Jiang et al . 
, 2017 ) , and more efÔ¨Åcient ways of extraction ( Zheng et al . 
, 2019 ) . 
As automatically constructed patterns may have mistakes , most of the above methods require further examinations from human experts , which is the main limitation of pattern - based models . 
2.2 Statistical Relation Extraction Models As compared to using pattern rules , statistical methods bring better coverage and require less human efforts . 
Thus statistical relation extraction ( SRE ) has been extensively studied . 
2Sometimes there is a special class in the relation set indicating that the sentence does not express any pre - speciÔ¨Åed relation ( usually named as N / A).746 One typical SRE approach is feature - based methods ( Kambhatla , 2004 ; Zhou et al . 
, 2005 ; Jiang and Zhai , 2007 ; Nguyen et al . 
, 2007 ) , which design lexical , syntactic and semantic features for entity pairs and their corresponding context , and then input these features into relation classiÔ¨Åers . 
Due to the wide use of support vector machines ( SVM ) , kernel - based methods have been widely explored , which design kernel functions for SVM to measure the similarities between relation representations and textual instances ( Culotta and Sorensen , 2004 ; Bunescu and Mooney , 2005 ; Zhao and Grishman , 2005 ; Mooney and Bunescu , 2006 ; Zhang et al . 
, 2006b , a ; Wang , 2008 ) . 
There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text . 
Graphical methods(Roth and Yih , 2002 , 2004 ; Sarawagi and Cohen , 2005 ; Yu and Lam , 2010 ) abstract the dependencies between entities , text and relations in the form of directed acyclic graphs , and then use inference models to identify the correct relations . 
Inspired by the success of embedding models in other NLP tasks ( Mikolov et al . 
, 2013a , b ) , there are also efforts in encoding text into low - dimensional semantic spaces and extracting relations from textual embeddings ( Weston et al . 
, 2013 ; Riedel et al . 
, 2013 ; Gormley et al . 
, 2015 ) . 
Furthermore , Bordes et al . 
( 2013),Wang et al . 
( 2014 ) and Lin et al . 
( 2015 ) utilize KG embeddings for RE . 
Although SRE has been widely studied , it still faces some challenges . 
Feature - based and kernelbased models require many efforts to design features or kernel functions . 
While graphical and embedding methods can predict relations without too much human intervention , they are still limited in model capacities . 
There are some surveys systematically introducing SRE models ( Zelenko et al . 
, 2003 ; Bach and Badaskar , 2007 ; Pawar et al . 
, 2017 ) . 
In this paper , we do not spend too much space for SRE and focus more on neural - based models . 
2.3 Neural Relation Extraction Models Neural relation extraction ( NRE ) models introduce neural networks to automatically extract semantic features from text . 
Compared with SRE models , NRE methods can effectively capture textual information and generalize to wider range of data . 
Studies in NRE mainly focus on designing and utilizing various network architectures to capture the relational semantics within text , such as recurBefore 2013 2013 2014 2015 2016 NowF1 Score ( % ) 77.6 ( SRE)82.4 82.784.386.389.5Figure 2 : The performance of state - of - the - art RE models in different years on widely - used dataset SemEval2010 Task 8 . 
The adoption of neural models ( since 2013 ) has brought great improvement in performance . 
sive neural networks ( Socher et al . 
, 2012 ; Miwa and Bansal , 2016 ) that learn compositional representations for sentences recursively , convolutional neural networks ( CNNs ) ( Liu et al . 
, 2013 ; Zeng et al . 
, 2014 ; Santos et al . 
, 2015 ; Nguyen and Grishman , 2015b ; Zeng et al . 
, 2015 ; Huang and Wang , 2017 ) that effectively model local textual patterns , recurrent neural networks ( RNNs ) ( Zhang and Wang , 2015 ; Nguyen and Grishman , 2015a ; Vu et al . 
, 2016 ; Zhang et al . 
, 2015 ) that can better handle long sequential data , graph neural networks ( GNNs ) ( Zhang et al . 
, 2018 ; Zhu et al . 
, 2019a ) that build word / entity graphs for reasoning , and attention - based neural networks ( Zhou et al . 
, 2016 ; Wang et al . 
, 2016 ; Xiao and Liu , 2016 ) that utilize attention mechanism to aggregate global relational information . 
Different from SRE models , NRE mainly utilizes word embeddings and position embeddings instead of hand - craft features as inputs . 
Word embeddings ( Turian et al . 
, 2010 ; Mikolov et al . 
, 2013b ) are the most used input representations in NLP , which encode the semantic meaning of words into vectors . 
In order to capture the entity information in text , position embeddings ( Zeng et al . 
, 2014 ) are introduced to specify the relative distances between words and entities . 
Except for word embeddings and position embeddings , there are also other works integrating syntactic information into NRE models . 
Xu et al . 
( 2015a ) and Xu et al . 
( 2015b ) adopt CNNs and RNNs over shortest dependency paths respectively . 
Liu et al . 
( 2015 ) propose a recursive neural network based on augmented dependency paths . 
Xu et al . 
( 2016 ) and Cai et al . 
( 2016 ) utilize deep RNNs to make further use of dependency paths . 
Besides , there are some efforts combining NRE with universal schemas ( Verga et al . 
, 2016 ; Verga and McCallum,747 CEOfounderproduct I looked up Apple Inc. on my iPhone.iPhone is designed by Apple Inc.iPhone is a iconic product of Apple . 
productApple Inc.iPhoneApple Inc. Steve JobsTim CookiPhoneFigure 3 : An example of distantly supervised relation extraction . 
With the fact ( Apple Inc. , product , iPhone ) , DS Ô¨Ånds all sentences mentioning the two entities and annotates them with the relation product , which inevitably brings noise labels . 
2016 ; Riedel et al . 
, 2013 ) . 
Recently , Transformers ( Vaswani et al . 
, 2017 ) and pre - trained language models ( Devlin et al . 
, 2019 ) have also been explored for NRE ( Du et al . 
, 2018 ; Verga et al . 
, 2018 ; Wu and He , 2019 ; Baldini Soares et al . 
, 2019 ) and have achieved new state - of - the - arts . 
By concisely reviewing the above techniques , we are able to track the development of RE from pattern and statistical methods to neural models . 
Comparing the performance of state - of - the - art RE models in years ( Figure 2 ) , we can see the vast increase since the emergence of NRE , which demonstrates the power of neural methods . 
3 ‚Äú More ‚Äù Directions for RE Although the above - mentioned NRE models have achieved superior results on benchmarks , they are still far from solving the problem of RE . 
Most of these models utilize abundant human annotations and just aim at extracting pre - deÔ¨Åned relations within single sentences . 
Hence , it is hard for them to work well in complex cases . 
In fact , there have been various works exploring feasible approaches that lead to better RE abilities on realworld scenarios . 
In this section , we summarize these exploratory efforts into four directions , and give our review and outlook about these directions . 
3.1 Utilizing More Data Supervised NRE models suffer from the lack of large - scale high - quality training data , since manually labeling data is time - consuming and humanintensive . 
To alleviate this issue , distant supervision ( DS ) assumption has been used to automatically label data by aligning existing KGs with plain text ( Mintz et al . 
, 2009 ; Nguyen and Moschitti , 2011 ; Min et al . 
, 2013 ) . 
As shown in Figure 3 , forDataset # Rel . 
# Fact # Inst . 
N / A NYT-10 53 377,980 694,491 79.43 % Wiki - Distant 454 605,877 1,108,288 47.61 % Table 1 : Statistics for NYT-10 and Wiki - Distant . 
Four columns stand for numbers of relations , facts and instances , and proportions of N / A instances respectively . 
Model NYT-10 Wiki - Distant PCNN - ONE 0.340 0.214 PCNN - ATT 0.349 0.222 BERT 0.458 0.361 Table 2 : Area under the curve ( AUC ) of PCNN - ONE ( Zeng et al . 
, 2015 ) , PCNN - ATT ( Lin et al . 
, 2016 ) and BERT ( Devlin et al . 
, 2019 ) on two datasets . 
any entity pair in KGs , sentences mentioning both the entities will be labeled with their corresponding relations in KGs . 
Large - scale training examples can be easily constructed by this heuristic scheme . 
Although DS provides a feasible approach to utilize more data , this automatic labeling mechanism is inevitably accompanied by the wrong labeling problem . 
The reason is that not all sentences mentioning the two entities express their relations in KGs exactly . 
For example , we may mistakenly label ‚Äú Bill Gates retired from Microsoft ‚Äù with the relation founder , if ( Bill Gates , founder , Microsoft ) is a relational fact in KGs . 
The existing methods to alleviate the noise problem can be divided into three major approaches : ( 1 ) Some methods adopt multi - instance learning by combining sentences with same entity pairs and then selecting informative instances from them . 
Riedel et al . 
( 2010 ) ; Hoffmann et al . 
( 2011 ) ; Surdeanu et al . 
( 2012 ) utilize graphical model to infer the informative sentences , while Zeng et al . 
( 2015 ) use a simple heuristic selection strategy . 
Later on , Lin et al . 
( 2016 ) ; Zhang et al . 
( 2017 ) ; Han et al . 
( 2018c ) ; Li et al . 
( 2020 ) ; Zhu et al . 
( 2019c ) ; Hu et al . 
( 2019 ) design attention mechanisms to highlight informative instances for RE . 
( 2)Incorporating extra context information to denoise DS data has also been explored , such as incorporating KGs as external information to guide instance selection ( Ji et al . 
, 2017 ; Han et al . 
, 2018b ; Zhang et al . 
, 2019a ; Qu et al . 
, 2019 ) and adopting multi - lingual corpora for the information consistency and complementarity ( Verga et al . 
, 2016 ; Lin et al . 
, 2017 ; Wang et al . 
, 2018 ) . 
( 3 ) Many methods tend to utilize sophisticated748 0 10 20 30 40 Relations100101102103104105Numbers of InstancesRelation Distribution on NYT-10 0 100 200 300 400 Relations100101102103104Numbers of InstancesRelation Distribution on Wiki - DistantFigure 4 : Relation distributions ( log - scale ) on the training part of DS datasets NYT-10 and Wiki - Distant , suggesting that real - world relation distributions suffer from the long - tail problem . 
mechanisms and training strategies to enhance distantly supervised NRE models . 
Vu et al . 
( 2016 ) ; Beltagy et al . 
( 2019 ) combine different architectures and training strategies to construct hybrid frameworks . 
Liu et al . 
( 2017 ) incorporate a softlabel scheme by changing unconÔ¨Ådent labels during training . 
Furthermore , reinforcement learning ( Feng et al . 
, 2018 ; Zeng et al . 
, 2018 ) and adversarial training ( Wu et al . 
, 2017 ; Wang et al . 
, 2018 ; Han et al . 
, 2018a ) have also been adopted in DS . 
The researchers have formed a consensus that utilizing more data is a potential way towards more powerful RE models , and there still remains some open problems worth exploring : ( 1 ) Existing DS methods focus on denoising auto - labeled instances and it is certainly meaningful to follow this research direction . 
Besides , current DS schemes are still similar to the original one in ( Mintz et al . 
, 2009 ) , which just covers the case that the entity pairs are mentioned in the same sentences . 
To achieve better coverage and less noise , exploring better DS schemes for autolabeling data is also valuable . 
( 2 ) Inspired by recent work in adopting pretrained language models ( Zhang et al . 
, 2019b ; Wu and He , 2019 ; Baldini Soares et al . 
, 2019 ) and active learning ( Zheng et al . 
, 2019 ) for RE , to perform unsupervised or semi - supervised learning for utilizing large - scale unlabeled data as well as using knowledge from KGs and introducing human experts in the loop is also promising . 
Besides addressing existing approaches and future directions , we also propose a new DS dataset to advance this Ô¨Åeld , which will be released once the paper is published . 
The most used benchmark for DS , NYT-10 ( Riedel et al . 
, 2010 ) , suffers from small amount of relations , limited relation domains and extreme long - tail relation performance . 
To Query InstancefounderproductiPhone is designed by Apple Inc. Steve Jobs is the co - founder of Apple Inc. Bill Gates founded Microsoft.founder?Tim Cook is Apple ‚Äôs current CEO.CEO Supporting SetFigure 5 : An example of few - shot RE . 
Give a few instances for new relation types , few - shot RE models classify query sentences into one of the given relations . 
alleviate these drawbacks , we utilize Wikipedia and Wikidata ( Vrande Àáci¬¥c and Kr ¬®otzsch , 2014 ) to construct Wiki - Distant in the same way as Riedel et al . 
( 2010 ) . 
As demonstrated in Table 1 , WikiDistant covers more relations and possesses more instances , with a more reasonable N / A proportion . 
Comparison results of state - of - the - art models on these two datasets3are shown in Table 2 , indicating that Wiki - Distant is more challenging and there is a long way to resolve distantly supervised RE . 
3.2 Performing More EfÔ¨Åcient Learning Real - world relation distributions are long - tail : Only the common relations obtain sufÔ¨Åcient training instances and most relations have very limited relational facts and corresponding sentences . 
We can see the long - tail relation distributions on two DS datasets from Figure 4 , where many relations even have less than 10 training instances . 
This phenomenon calls for models that can learn longtail relations more efÔ¨Åciently . 
Few - shot learning , which focuses on grasping tasks with only a few training examples , is a good Ô¨Åt for this need . 
To advance this Ô¨Åeld , Han et al . 
( 2018d ) Ô¨Årst built a large - scale few - shot relation extraction dataset ( FewRel ) . 
This benchmark takes the NwayK - shot setting , where models are given N random - sampled new relations , along with Ktraining examples for each relation . 
With limited information , RE models are required to classify query instances into given relations ( Figure 5 ) . 
The general idea of few - shot models is to train good representations of instances or learn ways of fast adaptation from existing large - scale data , and then transfer to new tasks . 
There are mainly two ways for handling few - shot learning : ( 1 ) Metric learning learns a semantic metric on existing 3Due to the large size , we do not use any denoise mechanism for BERT , which still achieves the best results.749 5 10 15 20 Numbers of Relations405060708090100Accuracy ( % ) Results with Increasing N BERT - PAIR Proto ( CNN ) 5 - 1 BERT PAIR5 - 1 Proto ( CNN)5 - 5 BERT PAIR5 - 5 Proto ( CNN)405060708090100Results on Similar Relations Random SimilarFigure 6 : Few - shot RE results with ( A ) increasing N and ( B ) similar relations . 
The left Ô¨Ågure shows the accuracy ( % ) of two models in N - way 1 - shot RE . 
In the right Ô¨Ågure , ‚Äú random ‚Äù stands for the standard few - shot setting and ‚Äú similar ‚Äù stands for evaluating with selected similar relations . 
data and classiÔ¨Åes queries by comparing them with training examples ( Koch et al . 
, 2015 ; Vinyals et al . 
, 2016 ; Snell et al . 
, 2017 ; Baldini Soares et al . 
, 2019 ) . 
While most metric learning models perform distance measurement on sentence - level representation , Ye and Ling ( 2019 ) ; Gao et al . 
( 2019 ) utilize token - level attention for Ô¨Åner - grained comparison . 
( 2 ) Meta - learning , also known as ‚Äú learning to learn ‚Äù , aims at grasping the way of parameter initialization and optimization through the experience gained on the meta - train data ( Ravi and Larochelle , 2017 ; Finn et al . 
, 2017 ; Mishra et al . 
, 2018 ) . 
Researchers have made great progress in fewshot RE . 
However , there remain many challenges that are important for its applications and have not yet been discussed . 
Gao et al . 
( 2019 ) propose two problems worth further investigation : ( 1)Few - shot domain adaptation studies how few - shot models can transfer across domains . 
It is argued that in the real - world application , the test domains are typically lacking annotations and could differ vastly from the training domains . 
Thus , it is crucial to evaluate the transferabilities of fewshot models across domains . 
( 2)Few - shot none - of - the - above detection is about detecting query instances that do not belong to any of the sampled Nrelations . 
In the N - way K - shot setting , it is assumed that all queries express one of the given relations . 
However , the real case is that most sentences are not related to the relations of our interest . 
Conventional few - shot models can not well handle this problem due to the difÔ¨Åculty to form a good representation for the none - of - the - above ( NOTA ) relation . 
Therefore , it is crucial to study how to identify NOTA instances . 
( 3 ) Besides the above challenges , it is also imporApple Inc. is a technology company founded by Steve Jobs , Steve Wozniak and Ronald Wayne . 
Its current CEO is Tim Cook . 
Apple is well known for its product iPhone . 
productSteve JobsTim CookiPhone co - founderCEOApple Inc. Ronald WayneSteve WozniakFigure 7 : An example of document - level RE . 
Given a paragraph with several sentences and multiple entities , models are required to extract all possible relations between these entities expressed in the document . 
tant to see that , the existing evaluation protocol may over - estimate the progress we made on fewshot RE . 
Unlike conventional RE tasks , few - shot RE randomly samples Nrelations for each evaluation episode ; in this setting , the number of relations is usually very small ( 5 or 10 ) and it is very likely to sample Ndistinct relations and thus reduce to a very easy classiÔ¨Åcation task . 
We carry out two simple experiments to show the problems ( Figure 6 ): ( A ) We evaluate few - shot models with increasing Nand the performance drops drastically with larger relation numbers . 
Considering that the real - world case contains much more relations , it shows that existing models are still far from being applied . 
( B ) Instead of randomly sampling Nrelations , we hand - pick 5relations similar in semantics and evaluate few - shot RE models on them . 
It is no surprise to observe a sharp decrease in the results , which suggests that existing few - shot models may overÔ¨Åt simple textual cues between relations instead of really understanding the semantics of the context . 
More details about the experiments are in Appendix A. 3.3 Handling More Complicated Context As shown in Figure 7 , one document generally mentions many entities exhibiting complex crosssentence relations . 
Most existing methods focus on intra - sentence RE and thus are inadequate for collectively identifying these relational facts expressed in a long paragraph . 
In fact , most relational facts can only be extracted from complicated context like documents rather than single sentences ( Yao et al . 
, 2019 ) , which should not be neglected . 
There are already some works proposed to extract relations across multiple sentences:750 ( 1)Syntactic methods ( Wick et al . 
, 2006 ; Gerber and Chai , 2010 ; Swampillai and Stevenson , 2011 ; Yoshikawa et al . 
, 2011 ; Quirk and Poon , 2017 ) rely on textual features extracted from various syntactic structures , such as coreference annotations , dependency parsing trees and discourse relations , to connect sentences in documents . 
( 2 ) Zeng et al . 
( 2017 ) ; Christopoulou et al . 
( 2018 ) build inter - sentence entity graphs , which can utilize multi - hop paths between entities for inferring the correct relations . 
( 3 ) Peng et al . 
( 2017 ) ; Song et al . 
( 2018 ) ; Zhu et al . 
( 2019b ) employ graph - structured neural networks to model cross - sentence dependencies for relation extraction , which bring in memory and reasoning abilities . 
To advance this Ô¨Åeld , some document - level RE datasets have been proposed . 
Quirk and Poon ( 2017 ) ; Peng et al . 
( 2017 ) build datasets by DS . 
Li et al . 
( 2016 ) ; Peng et al . 
( 2017 ) propose datasets for speciÔ¨Åc domains . 
Yao et al . 
( 2019 ) construct a general document - level RE dataset annotated by crowdsourcing workers , suitable for evaluating general - purpose document - level RE systems . 
Although there are some efforts investing into extracting relations from complicated context ( e.g. , documents ) , the current RE models for this challenge are still crude and straightforward . 
Followings are some directions worth further investigation : ( 1 ) Extracting relations from complicated context is a challenging task requiring reading , memorizing and reasoning for discovering relational facts across multiple sentences . 
Most of current RE models are still very weak in these abilities . 
( 2 ) Besides documents , more forms of context is also worth exploring , such as extracting relational facts across documents , or understanding relational information based on heterogeneous data . 
( 3 ) Inspired by Narasimhan et al . 
( 2016 ) , which utilizes search engines for acquiring external information , automatically searching and analysing context for RE may help RE models identify relational facts with more coverage and become practical for daily scenarios . 
3.4 Orienting More Open Domains Most RE systems work within pre - speciÔ¨Åed relation sets designed by human experts . 
However , our world undergoes open - ended growth of relations and it is not possible to handle all these emerging JeÔ¨Ä Bezos , an American entrepreneur , graduated from Princeton in 1986.graduated fromJeÔ¨Ä BezosPrincetonFigure 8 : An example of open information extraction , which extracts relation arguments ( entities ) and phrases without relying on any pre - deÔ¨Åned relation types . 
Relation BRelation ABill Gates founded Microsoft . 
Larry and Sergey founded Google . 
Steve Jobs is one of the co - founder of Apple . 
Tim Cook is Apple ‚Äôs current CEO.Satya Nadella became the CEO of Microsoft in 2014 . 
Figure 9 : An example of clustering - based relation discovery , which identifying potential relation types by clustering unlabeled relational instances . 
relation types only by humans . 
Thus , we need RE systems that do not rely on pre - deÔ¨Åned relation schemas and can work in open scenarios . 
There are already some explorations in handling open relations : ( 1 ) Open information extraction ( Open IE ) , as shown in Figure 8 , extracts relation phrases and arguments ( entities ) from text ( Banko et al . 
, 2007 ; Fader et al . 
, 2011 ; Mausam et al . 
, 2012 ; Del Corro and Gemulla , 2013 ; Angeli et al . 
, 2015 ; Stanovsky and Dagan , 2016 ; Mausam , 2016 ; Cui et al . 
, 2018 ) . 
Open IE does not rely on speciÔ¨Åc relation types and thus can handle all kinds of relational facts . 
( 2 ) Relation discovery , as shown in Figure 9 , aims at discovering unseen relation types from unsupervised data . 
Yao et al . 
( 2011 ) ; Marcheggiani and Titov ( 2016 ) propose to use generative models and treat these relations as latent variables , while Shinyama and Sekine ( 2006 ) ; Elsahar et al . 
( 2017 ) ; Wu et al . 
( 2019 ) cast relation discovery as a clustering task . 
Though relation extraction in open domains has been widely studied , there are still lots of unsolved research questions remained to be answered : ( 1)Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks ( Niklaus et al . 
, 2018 ) . 
If not canonicalized , the extracted relational facts could be redundant and ambiguous . 
For example , Open IE may extract two triples ( Barack Obama , was born in , Honolulu ) and ( Obama , place of birth , Honolulu ) indicating an identical fact . 
Thus , normalizing extracted results will largely beneÔ¨Åt the applications of Open IE . 
There are already some preliminary works in this area ( Gal ¬¥ arraga et al . 
, 2014;751 Vashishth et al . 
, 2018 ) and more efforts are needed . 
( 2 ) The not applicable ( N / A ) relation has been hardly addressed in relation discovery . 
In previous work , it is usually assumed that the sentence always expresses a relation between the two entities ( Marcheggiani and Titov , 2016 ) . 
However , in the real - world scenario , a large proportion of entity pairs appearing in a sentence do not have a relation , and ignoring them or using simple heuristics to get rid of them may lead to poor results . 
Thus , it would be of interest to study how to handle these N / A instances in relation discovery . 
4 Other Challenges In this section , we analyze two key challenges faced by RE models , address them with experiments and show their signiÔ¨Åcance in the research and development of RE systems4 . 
4.1 Learning from Text or Names In the process of RE , both entity names and their context provide useful information for classiÔ¨Åcation . 
Entity names provide typing information ( e.g. , we can easily tell JFK International Airport is an airport ) and help to narrow down the range of possible relations ; In the training process , entity embeddings may also be formed to help relation classiÔ¨Åcation ( like in the link prediction task of KG ) . 
On the other hand , relations can usually be extracted from the semantics of textaround entity pairs . 
In some cases , relations can only be inferred implicitly by reasoning over the context . 
Since there are two sources of information , it is interesting to study how much each of them contributes to the RE performance . 
Therefore , we design three different settings for the experiments : ( 1)normal setting , where both names and text are taken as inputs ; ( 2 ) masked - entity ( ME ) setting , where entity names are replaced with a special token ; ( 3 ) only - entity ( OE ) setting , where only names of the two entities are provided . 
Results from Table 3 show that compared to the normal setting , models suffer a huge performance drop in both the ME and OE settings . 
Besides , it is surprising to see that in some cases , only using entity names outperforms only using text with entities masked . 
It suggests that ( 1 ) both entity names and text provide crucial information for RE , and 4For more details about these experiments , please refer to our open - source toolkit https://github.com/ thunlp / OpenNRE .Benchmark Normal ME OE Wiki80 ( Acc ) 0.861 0.734 0.763 TACRED ( F-1 ) 0.666 0.554 0.412 NYT-10 ( AUC ) 0.349 0.216 0.185 Wiki - Distant ( AUC ) 0.222 0.145 0.173 Table 3 : Results of state - of - the - arts models on the normal setting , masked - entity ( ME ) setting and only - entity ( OE ) setting . 
We report accuracies of BERT on Wiki80 , F-1 scores of BERT on TACRED and AUC of PCNNATT on NYT-10 and Wiki - Distant . 
All models are from the OpenNRE package ( Han et al . 
, 2019 ) . 
( 2 ) for some existing state - of - the - art models and benchmarks , entity names contribute even more . 
The observation is contrary to human intuition : we classify the relations between given entities mainly from the text description , yet models learn more from their names . 
To make real progress in understanding how language expresses relational facts , this problem should be further investigated and more efforts are needed . 
4.2 RE Datasets towards Special Interests There are already many datasets that beneÔ¨Åt RE research : For supervised RE , there are MUC ( Grishman and Sundheim , 1996 ) , ACE-2005 ( Ntroduction , 2005 ) , SemEval-2010 Task 8 ( Hendrickx et al . 
, 2009 ) , KBP37 ( Zhang and Wang , 2015 ) and TACRED ( Zhang et al . 
, 2017 ) ; and we have NYT10 ( Riedel et al . 
, 2010 ) , FewRel ( Han et al . 
, 2018d ) and DocRED ( Yao et al . 
, 2019 ) for distant supervision , few - shot and document - level RE respectively . 
However , there are barely datasets targeting special problems of interest . 
For example , RE across sentences ( e.g. , two entities are mentioned in two different sentences ) is an important problem , yet there is no speciÔ¨Åc datasets that can help researchers study it . 
Though existing document - level RE datasets contain instances of this case , it is hard to analyze the exact performance gain towards this speciÔ¨Åc aspect . 
Usually , researchers ( 1 ) use handcrafted sub - sets of general datasets or ( 2 ) carry out case studies to show the effectiveness of their models in speciÔ¨Åc problems , which is lacking of convincing and quantitative analysis . 
Therefore , to further study these problems of great importance in the development of RE , it is necessary for the community to construct well - recognized , well - designed and Ô¨Åne - grained datasets towards special interests.752 5 Conclusion In this paper , we give a comprehensive and detailed review on the development of relation extraction models , generalize four promising directions leading to more powerful RE systems ( utilizing more data , performing more efÔ¨Åcient learning , handling more complicated context and orienting more open domains ) , and further investigate two key challenges faced by existing RE models . 
We thoroughly survey the previous RE literature as well as supporting our points with statistics and experiments . 
Through this paper , we hope to demonstrate the progress and problems in existing RE research and encourage more efforts in this area . 
Acknowledgments This work is supported by the Natural Science Foundation of China ( NSFC ) and the German Research Foundation ( DFG ) in Project Crossmodal Learning , NSFC 61621136008 / DFG TRR-169 , and Beijing Academy of ArtiÔ¨Åcial Intelligence ( BAAI ) . 
This work is also supported by the Pattern Recognition Center , WeChat AI , Tencent Inc. Gao is supported by 2019 Tencent Rhino - Bird Elite Training Program . 
Gao is also supported by Tsinghua University Initiative ScientiÔ¨Åc Research Program . 
Abstract It has been shown that word embeddings can exhibit gender bias , and various methods have been proposed to quantify this . 
However , the extent to which the methods are capturing social stereotypes inherited from the data has been debated . 
Bias is a complex concept and there exist multiple ways to deÔ¨Åne it . 
Previous work has leveraged gender word pairs to measure bias and extract biased analogies . 
We show that the reliance on these gendered pairs has strong limitations : bias measures based off of them are not robust and can not identify common types of real - world bias , whilst analogies utilising them are unsuitable indicators of bias . 
In particular , the well - known analogy ‚Äú man is to computer - programmer as woman is to homemaker ‚Äù is due to word similarity rather than societal bias . 
This has important implications for work on measuring bias in embeddings and related work debiasing embeddings . 
1 Introduction Word embeddings , distributed representations of words in a low - dimensional vector space , are used in many downstream NLP tasks ( Mikolov et al . 
, 2013a , b ; Pennington et al . 
, 2014 ; Peters et al . 
, 2018 ; Devlin et al . 
, 2019 ) . 
Recent work has shown they can contain harmful bias and proposed techniques to quantify it ( Bolukbasi et al . 
, 2016 ; Caliskan et al . 
, 2017 ; Ethayarajh et al . 
, 2019 ; Gonen and Goldberg , 2019 ) . 
These techniques leverage cosine similarity to a base pair of gender words , such as(man , woman ) . 
They include bias measures , which return a magnitude of bias for a given word , and analogies . 
A well - known example of the latter is ‚Äú Man is to computer programmer as woman is to homemaker ‚Äù ( Bolukbasi et al . 
, 2016 ) , which has been widely interpreted as demonstrating bias . 
There have also been related attempts to debias * denotes equal contribution.embeddings ( Bolukbasi et al . 
, 2016 ; Zhao et al . 
, 2018 ; Dev and Phillips , 2019 ; Kaneko and Bollegala , 2019 ; Manzini et al . 
, 2019 ) . 
However , to remove bias effectively , an accurate method of identifying it is Ô¨Årst required . 
This is a complex task , not least because the concept of ‚Äú bias ‚Äù has multiple interpretations : Mehrabi et al . 
( 2019 ) identify 23 types of bias that can occur in machine learning applications , including historic ( pre - existing in society ) , algorithmic ( introduced by the algorithm ) and evaluation ( occurs during model evaluation ) . 
In the case of word embeddings , it remains an open question if bias identifying techniques reÔ¨Çect social stereotypes in the training data , an artifact of the embedding process or noise . 
While it is often assumed the Ô¨Årst is true , and thus that bias in embeddings can perpetuate harmful stereotypes ( Bolukbasi et al . 
, 2016 ; Caliskan et al . 
, 2017 ) , this has not been conclusively established ( Gonen and Goldberg , 2019 ; Nissim et al . 
, 2019 ; Ethayarajh et al . 
, 2019 ) . 
To further complicate matters , multiple methods of quantifying bias have been proposed , often in response to one another ‚Äôs limitations ( see Section 2.1 ) . 
It is unclear how they compare and which are more reliable . 
This work shows that the use of gender base pairs in bias identifying techniques has serious limitations . 
We propose three criteria to evaluate the performance of gender bias measures using base pairs and systematically compare four popular measures , showing both that they not robust , and that they do not accurately reÔ¨Çect common types of societal bias . 
In addition , we demonstrate that the types of analogies proposed in Bolukbasi et al . 
( 2016 ) are unsuitable indicators of bias ; what is ascribed to social bias in analogies is actually an artifact of high cosine similarity in the base pair , which is arguably positive . 
Our argument is not that embeddings are free of bias ; rather it is that bias is a complex problem and current bias measures do759 not completely solve it . 
This has important implications for future work on bias in embeddings and debiasing techniques . 
The primary contributions of this work are to : ( 1 ) demonstrate the output of gender bias measures is heavily dependant on a chosen gendered base pair ( e.g. ( she , he ) ) and on the form of a word considered ( e.g. singular versus plural ) ; ( 2 ) show the measures can not accurately predict either the socially stereotyped gender of human traits or the correct gender of words when this is encoded linguistically ( e.g. lioness ) ; ( 3 ) show that analogies generated by gender base pairs ( e.g. ( she , he ) ) are Ô¨Çawed indicators of bias and the widely - known example ‚Äú Man is to computer programmer as woman is to homemaker ‚Äù is not due to gender bias and ( 4 ) highlight the complexities of identifying bias in word embeddings , and the limitations of these measures . 
2 Related Work 2.1 Bias Measures A variety of gender bias measures for word embeddings have been proposed in the literature . 
Each takes as input a word wand a gendered base pair ( such as ( she , he ) ) , and returns a numerical output . 
This output indicates both the magnitude of w ‚Äôs gender bias with respect to the base pair used , and the direction of w ‚Äôs bias ( male or female ) , which is determined by the sign of the score . 
Direct Bias ( DB ) ( Bolukbasi et al . 
, 2016 ) deÔ¨Ånes bias as a projection onto a gender subspace , which is constructed from a set of gender base pairs such as ( she , he ) . 
The DB of a word wis computed as wB=/summationtextk j=1(‚àí ‚Üíw¬∑bj)bj , where‚àí ‚Üíwis the embedding vector of w , the subspace Bis deÔ¨Åned by k orthogonal unit vectors b1 , ... , b kand vectors are normalised . 
In addition , the authors propsed a method of debiasing embeddings based off of DB . 
There is ambiguity in Bolukbasi et al . 
( 2016 ) about how many base pairs should be used with DB ; while experiments to identify bias use only one ( namely ( she , he ) ) , a set of ten is used for debiasing.1It is unclear why the particular ten pairs used were chosen , and the extent to which their choice matters . 
We follow recent work ( Gonen and Goldberg , 2019 ; Ethayarajh et al . 
, 2019 ) that evaluates DB and focus on the case of a single base 1The set of gender - deÔ¨Åning pairs used is { ( she , he),(her , his ) , ( woman , man ) , ( mary , john ) , ( herself , himself ) , ( daughter , son ) , ( mother , father ) , ( gal , guy ) , ( girl , boy ) , ( female , male ) } .pair , i.e. k= 1 . 
The DB of wwith respect to the gender base pair ( x , y)is then‚àí ‚Üíw¬∑(‚àí ‚Üíx‚àí‚àí ‚Üíy ) . 
Caliskan et al . 
( 2017 ) created an association test for word embeddings called WEAT to identify human - like biases . 
The Word Association ( WA ) , the key component of WEAT , measures the association of wwith two sets of attribute words , Xand Y. More formally , WA is computed as : mean x‚ààXcos ( ‚àí ‚Üíw , ‚àí ‚Üíx)‚àímean y‚ààYcos ( ‚àí ‚Üíw , ‚àí ‚Üíy ) To allow for a fair comparison with other methods being evaluated , we focus on the case where the attribute sets contain a single word , i.e. , X={x } andY={y } . 
Then WA and DB are equivalent as : cos ( ‚àí ‚Üíw , ‚àí ‚Üíx)‚àícos ( ‚àí ‚Üíw , ‚àí ‚Üíy ) = ‚àí ‚Üíw ||w||¬∑/parenleftbigg‚àí ‚Üíx ||x||‚àí‚àí ‚Üíy ||y||/parenrightbigg Since DB and WA assign a word the same score , we will use DB / WA to refer to both measures . 
Gonen and Goldberg ( 2019 ) argued that bias can not be directly observed , as assumed in methods such as DB , and that the debiasing method of Bolukbasi et al . 
( 2016 ) is ineffective . 
They proposed the Neighbourhood Bias Metric ( NBM ) , which measures the bias of a word was the percentage of socially female - biased words and malebiased words among its Knearest neighbours in a set of predeÔ¨Åned gender - neutral words . 
Setting K= 100 , the NBM bias of a target word wis measured as : |female ( w)|‚àí|male ( w)| 100 , where female ( w)andmale ( w)are sets of socially biased and male words in the neighborhood ofw . 
The bias direction of words in w ‚Äôs neighborhood is computed using the DB metric with a single base pair . 
Gonen and Goldberg ( 2019 ) use DB with base pair ( she , he ) ; our work considers a more general form with base pair ( x , y ) . 
Ethayarajh et al . 
( 2019 ) draw attention to the lack of theoretical guarantees surrounding previous work on bias and debiasing . 
They argue WEAT overestimates bias and is not robust to the choice of deÔ¨Åning sets . 
In addition , and in contrast Gonen and Goldberg ( 2019 ) , they argue that DB and the debiasing method based off it are effective , but state vectors used with DB should not be normalised . 
They propose Relational Inner Product Association ( RIPA ) and state that RIPA is most interpretable with a single base pair , a key advantage760 of it being that it ( unike WEAT ) does not depend on the base pair used . 
With a single base pair , the RIPA bias of wwith the base pair ( x , y)is : ‚àí ‚Üíw¬∑/parenleftbigg‚àí ‚Üíx‚àí‚àí ‚Üíy ||‚àí ‚Üíx‚àí‚àí ‚Üíy||/parenrightbigg . 
2.2 Analogies An alternative approach to identifying gender bias in embeddings is via word analogies . 
Unlike the gender bias measures dicussed in Section 2.1 , analogies do not measure the bias of a particular word . 
Instead , they identify pairs of words which are assumed to have a gendered relationship . 
Analogies in word embeddings are important because it has been observed that embedding vectors seem to possess unexpected linear properties : vectors associated with word pairs sharing the same analogical relationship can be identiÔ¨Åed using vector arithmetic ( Mikolov et al . 
, 2013a ; Levy and Goldberg , 2014 ; Ethayarajh et al . 
, 2018 ) . 
A notable example of this phenomena is‚àí‚àí‚Üíking -‚àí‚àí‚Üíman + ‚àí‚àí‚àí‚àí‚àí‚Üíwoman‚âà‚àí‚àí‚àí‚Üíqueen ( Mikolov et al . 
, 2013c ) . 
This relationship is frequently attributed to a gender difference vector between‚àí‚àí‚Üíman and‚àí‚àí‚àí‚àí‚àí‚Üíwoman , and between‚àí‚àí‚Üíking and‚àí‚àí‚àí‚Üíqueen ( Mikolov et al . 
, 2013c ; Ethayarajh et al . 
, 2018 ) . 
Analogies are considered a benchmark method of measuring the quality of embeddings , though their suitability has been debated ( Linzen , 2016 ; Drozd et al . 
, 2016 ; Gladkova et al . 
, 2016 ) . 
The standard approach to solving ‚Äò a is tobascis to ? , ‚Äù is to return : ‚àí ‚Üíd‚àó=argmax w‚ààV / primeCosSim ( ‚àí ‚Üíw , ‚àí ‚Üíb‚àí‚àí ‚Üía+‚àí ‚Üíc ) , where V / primeis the embedding vocabulary excluding { a , b , c}(Levy and Goldberg , 2014 ) . 
Bolukbasi et al . 
( 2016 ) proposed using analogies to quantify gender bias in embeddings and proposed a modiÔ¨Åed analogy task to produce analogies from the gender base pair ( she , he ) . 
The task identiÔ¨Åes word pairs ( x , y ) , such that ‚Äú heis toxas sheis toy ‚Äù , where||‚àí ‚Üíx‚àí‚àí ‚Üíy||= 1 . 
This method was expanded to mutli - class forms of bias such as racial bias by Mehrabi et al . 
( 2019 ) . 
However , the suitability of analogies as indicators of bias was questioned by Nissim et al . 
( 2019 ) , who highlighted the fact that the approach used by Bolukbasi et al . 
( 2016 ) did not allow analogies to return their input words , thus artiÔ¨Åcially increasing the perception of bias.3 Approach Our aim is to examine the extent to which bias identifying techniques are reliabily capturing societal gender bias . 
Bias is a highly complex concept , and although the four bias measures ( DB , WA , NBM and RIPA ) may detect certain kinds of bias , there is no theoretical guarantee they will detect all forms , that the ‚Äú bias ‚Äù they Ô¨Ånd will be accurate or that different choices of base pair will behave similarly . 
We therefore explore whether the bias measures are robust in detecting the bias they appear to detect and if there are forms of bias they are not sensitive to . 
We propose three conditions to test this : 1 ) Base pair stability : If bias measures captured real - world information in a reliable way , it would be expected that reasonable changes of the base pair , such as ( she , he ) to(woman , man ) or(she , he ) to(She , He ) , would not frequently cause a signiÔ¨Åcant change in bias . 
2 ) Word form stability : While different forms of a word , such as plurals , have different contexts and word vectors , their social bias will not signiÔ¨Åcantly change and they should have similar bias scores . 
3 ) Linguistic correspondence : We explore the extent to which the measures predict the expected gender of terms containing explicit gender information ( e.g. ‚Äú lioness ‚Äù ) or , based on some accounts , stereotypically ( e.g. ‚Äú compassionate ‚Äù ) . 
Of course , due to noise and the problem of implicit bias , these three conditions may not always be true . 
However , if they do not hold the majority of the time , it must be questioned if the measures are reliably identifying social bias . 
4 Data To allow for fair comparisons , we use the same datasets as previous work where possible : Embeddings : 300 - dimensional Google News word2vec ( Mikolov et al . 
, 2013a , b ) . 
Professions : A list of 320 professions ( Bolukbasi et al . 
, 2016 ) , often used to analyse bias measures . 
Base pairs : A standard list of 10 gender base pairs , including ( she , he ) ( Bolukbasi et al . 
, 2016 ) . 
Gender neutral : For NBM , we use the set of 26,145 gender neutral words deÔ¨Åned in ( Gonen and Goldberg , 2019 ) . 
In addition , we construct two new test sets , both listed in Appendix A : BSRI : To assess whether word embeddings contain undesirable gender stereotypes , we utilise the Bem Sex Role Inventory ( BSRI ) which developed761 a list of 20 traits for men and 20 for women that are considered to be socially desirable , such as ‚Äú assertive ‚Äù and ‚Äú compassionate ‚Äù respectively ( Bem , 1974).2Although derived in the 1970s , this work remains one of the most inÔ¨Çuential and widely accepted measures of socially constructed gender roles within the social sciences , e.g. ( Holt and Ellis , 1998 ; Dean and Tate , 2016 ; Starr and Zurbriggen , 2016 ; Matud et al . 
, 2019 ) . 
Of particular relevance to NLP applications , Gaucher et al . 
( 2011 ) use BSRI to identify gender - biased language in job advertisements and demonstrate this language can contribute to workplace gender inequality . 
BSRI traits not in the embedding vocabulary ( e.g. ‚Äú willing to take risks ‚Äù ) were removed . 
For each remaining trait , we queried Merriam Webster for other forms of that word ( for example , ‚Äú assertiveness ‚Äù is a form of ‚Äú assertive ‚Äù ) , resulting in a list of 58 characteristics ( 27 male and 31 female ) . 
Animals : Some words , including the names of certain animals , encode gender linguistically ( e.g. ‚Äú lioness ‚Äù ) . 
Wikipedia provides a table of male and female versions of animal names.3This table was downloaded , and duplicates , rare words and terms whose animal usage is uncommon ( for example , a ‚Äú cob ‚Äù is a male swan ) were removed . 
This resulted a set of 26 terms consisting 13 female - male pairs such as ( hen , rooster ) . 
5 Evaluation Evaluating gender bias measures is a complex task as there is no inherent ground truth interpretation of the measure ‚Äôs results . 
For example , it is unclear when a bias score is problematic . 
We choose to evaluate the four bias measures ( DB , WA , NBM andRIPA ) in two ways , Ô¨Årst by considering whether a word is assigned a male or female bias , and second what the magnitude of that score is . 
The bias direction ( male or female ) assigned by a measure to a word is determined by the sign of the score ( whether a positive score denotes male or female bias depends on the ordering of the base pair words ) . 
The assignment of bias direction is viewed an annotation task in which a bias measure ( with a speciÔ¨Åed base pair ) is considered an ‚Äú annotator ‚Äù making assignments . 
Consistency between annotators ( i.e. versions of bias measures ) can be 2Our use of BSRI should not be interpreted as an endorsement of these traits as either accurate or desirable ; rather we use them as a dataset of commonly held stereotypes . 
3https://en.wikipedia.org/wiki/List ofanimal namescomputed using Cohen ‚Äôs kappa to determine pairwise agreement ( Cohen , 1960 ) and Fleiss ‚Äô kappa ( Fleiss , 1971 ) for multiple annotators . 
We follow a widely used interpretation of kappa scores ( Landis and Koch , 1977 ) . 
The second method of evaluation is an analysis of the magnitude of bias assigned . 
Previous work in this area does not deÔ¨Åne what constitutes a ‚Äú signiÔ¨Åcant ‚Äù change of the magnitude of a bias score . 
Therefore , we estimate the mean bias in the embedding space as follows : The 50,000 most frequent words in the embedding vocabulary were selected and , following Bolukbasi et al . 
( 2016 ) , all words containing digits , punctuation or that were more than 20 characters long were removed . 
For each of the remaining 48,088 words , their bias score was calculated with respect to each of the 10 base pairs ( so for each measure , there are 480,880 scores ) . 
An examination of these scores revealed them to appear approximately normally distributed and so their mean and standard deviation are used as an approximation of the population mean and standard deviation ( see Table 1 ) . 
We consider a relevant change in magnitude to be a change of at least one standard deviation . 
DB / WA RIPA NBM Mean -0.001 0.024 -0.038 Standard Dev . 
0.053 0.239 0.431 Table 1 : Mean and standard deviation of bias scores for each measure . 
6 Results Base pair stability : The Ô¨Årst experiment explored the robustness of the four measures ( DB , WA , RIPA andNBM ) to changing the base pair . 
For example , Figure 1 illustrates the effects of changing the base pair on the bias score of the word ‚Äú professor . 
‚Äù More comprehensively , for each bias measure we computed the bias assigned to each profession for each base pair , and then calculated the agreement between the 10 base pairs via Fleiss ‚Äô kappa coefÔ¨Åcient . 
The changes in the bias magnitude of a word between base pairs were also computed . 
Results are shown in Table 2 . 
The level of agreement of bias direction between base pairs was fair ( 0.29 ) for NBM and moderate ( 0.42 and 0.45 ) for RIPA and DB / WA . 
This means that changing the base pair frequently caused a profession‚Äôs762 Figure 1 : Graphs demonstrating bias score variations . 
Each graph represents a measure , with the mean and standard deviation of that measure ( Section 5 ) denoted by dashed lines . 
Positive and negative scores indicate female and male bias respectively , while larger absolute values show higher levels of bias . 
The bias scores of the word ‚Äú professor ‚Äù and and its variations ( ‚Äú professors , ‚Äù Professor ‚Äù and ‚Äú PROFESSOR ‚Äù ) are shown , as calculated according to each base pair ( such as ( she , he ) and(her , his ) ) . 
The graphs demonstrate that the bias direction and magnitude of bias of each word depend heavily on which base pair is chosen . 
They also show that the different forms of the word exhibit different behaviour . 
Kappa Magnitude DB / WA 0.45 0.69 RIPA 0.42 0.66 NBM 0.29 0.71 Table 2 : For the 320 professions 1 ) the level of agreement kappa between bias directions assigned by each of the ten base pairs and 2 ) the mean proportion of signiÔ¨Åcant magnitude changes over the 10 base pairs . 
For 1 ) , higher is better , and for 2 ) , lower is better . 
bias direction to change . 
For example , the RIPA direction of ‚Äú surgeon ‚Äù is male for ( she , he ) but female for ( woman , man ) . 
For a given profession , only about a quarter of DB / WA and RIPA directions were the same for every base pair , and fewer than 15 % of NBM directions were . 
With regards to score magnitudes , on average over the professions , 66 % of base pair changes saw a relevant change in magnitude ( more than one standard deviation ) forRIPA , 69 % for DB / WA and 71 % for NBM . 
Next , to explore the robustness of the form of the base pairs chosen , we compared the bias direction assigned to each of the 320 professions by a base pair to the bias direction assigned by the capitalised form ( Ô¨Årst letter capitalised ) of that base pair ( for example , ( she , he ) versus ( She , He ) ) . 
The level of agreement of bias direction between each two base pair forms was calculated using Cohen ‚Äôs kappa coefÔ¨Åcient , results are shown in Table 3 . 
The mean of the level of agreement over each of the 10 base pairs ranged from 0.39 ( fair ) to 0.43 ( moderate ) , with many individual agreements below moderate level . 
In particular , an agreement level of only 0.03 ( very slight ) is found for the base pair ( gal , guy ) compared with ( Gal , Guy ) for DB / WA . 
Word form stability : The second experiment examined the measures ‚Äô robustness to changing the form of a word considered by comparing a word ‚Äôs plural , capitalised ( Ô¨Årst letter capitalised ) and up-763 percase ( all letters capitalised ) forms to its base form . 
For example , ‚Äú professors , ‚Äù ‚Äú Professor ‚Äù and ‚Äú PROFESSOR ‚Äù were compared to ‚Äú professor ‚Äù ( see Figure 1 ) . 
For this experiment , only the 230 words in the professions list whose plural , capitalised and uppercase forms are all included in the embedding vocabulary were used . 
For each measure and base pair , the direction of gender bias of each word form was computed , and the pairwise level of agreement ( Cohen ‚Äôs kappa ) between the original form of a word and each of its variants was calculated , see Table 4 . 
All four measures were found to give different versions of the same word ( plural , capital and uppercase forms ) different bias directions . 
For example , the DB / WA of ‚Äú surgeon ‚Äù is male but of ‚Äú surgeons ‚Äù is female ( base pair ( she , he ) ) . 
For each measure , the mean kappa coefÔ¨Åcients were moderate for the plural category and fair for the uppercase category . 
For the capital category , they were moderate for DB / WA and RIPA , and substantial for NBM . 
Since changing word form frequently changes bias direction , these results indicate the bias measures are not reliably reÔ¨Çecting any inherent social bias encoded into the word vectors , and that the gender bias direction assigned to a profession is not robust . 
Linguistic Correspondence : The Ô¨Ånal experiment examined the measures ‚Äô prediction for terms containing explicit or stereotypical gender information , in the form of social stereotypes ( BSRI ) and linguistic gender ( Animals ) . 
The predicted gender bias direction of the words in the Animals and BSRI lists was computed for each base pair and measure , and compared with the ground - truth gender of the words . 
Table 5 shows the pairwise agreement ( Cohen ‚Äôs kappa ) between prediction and ground - truth for each base pair , as well as the mean agreement over all 10 base pairs . 
The bias measures did not predict the groundtruth gender of either set of words with high accuracy ; mean agreement levels varied from 0.17 ( slight ) to 0.42 ( moderate ) . 
For example , the NBM gender prediction for ‚Äú bull , ‚Äù a male animal , was female and the direction of the feminine BSRI trait ‚Äú compassionate ‚Äù was male ( both for base pair(woman , man ) ) . 
As with the previous experiment , different forms of the BSRI words frequently were assigned opposite genders : unlike ‚Äú compassionate ‚Äù , ‚Äú compassionately ‚Äù had the correct NBM gender prediction , again with base pair ( woman , man ) . 
The BRSI results were overallpoorer than the Animal results , with some base pairs having negative kappa scores , indicating less agreement than random chance . 
This may be because the BSRI stereotypes are less likely to be mentioned in the context of base pair words like ‚Äú he ‚Äù and ‚Äú she . 
‚Äù Interestingly , the highest scoring BSRI base pair was ( mother , father ) . 
Some of the inaccurate predictions for the animal words may come from the fact that some terms can both refer to males and be gender neutral , e.g. ‚Äú lion . 
‚Äù 7 Discussion Lack of Robustness : The experiments in this work empirically showed that the four bias measures are not robust to changing either the base pair or the form of a word used ( such as singualar to plural ) . 
We hypothesise there are two primary reasons for this : sociolinguistic factors and mathematical properties of the bias measure formulae . 
It is highly likely that linguistic properties of the base pair chosen effect bias measure robustness.4 For example , ( she , he ) has quite different sociolinguistic connotations to the more casual ( gal , guy ) , and ‚Äú she ‚Äù and ‚Äú he ‚Äù are clearly linguistic opposites , unlike ‚Äú Mary ‚Äù and ‚Äú John . 
‚Äù Our results indicate that more neutral base pairs which are linguistic opposites , such as ( she , he ) or(man , woman ) are the most robust . 
However , even they exhibit variation and struggle particularly to pick up on social stereotypes ( the BSRI agreements for ( man , woman ) are all close to zero , indicating random chance ) . 
A further reason that the bias measures are not robust is their reliance on the direct output of a dot product , which is sensitive to the input vectors used . 
Given a base pair ( a , b ) , we will refer to‚àí ‚Üía‚àí‚àí ‚Üíbas its difference vector . 
The 10 base pairs have highly similar difference vectors : the mean over the 10 base pairs of cos(‚àí ‚Üía‚àí‚àí ‚Üíb , ‚àí ‚Üíc‚àí‚àí ‚Üíd ) , where ( a , b ) and(c , d)are base pairs is 0.5 . 
While this is very high for embedding vectors,5it does not guarantee ‚àí ‚Üíw¬∑(‚àí ‚Üíx‚àí‚àí ‚Üíy)and‚àí ‚Üíw¬∑(‚àí ‚Üía‚àí‚àí ‚Üíb)will have the same sign for all words w , resulting in opposite bias directions . 
The same sensitivity explains why words and their plurals can be assigned opposite bias directions , even if they have similar embeddings . 
Furthermore , similarity between base pair difference vectors is highly correlated with agree4Our choice of base pairs follows previous work . 
5We randomly sampled 100,000 sets of words { a , d , c , d } and computed cos(‚àí ‚Üía‚àí‚àí ‚Üíb , ‚àí ‚Üíc‚àí‚àí ‚Üíd ) ; the sample mean was 0.00 , with standard deviation 0.09.764 She Her Woman Mary Herself Dgtr Mother Gal Girl Female He His Man John Himself Son Father Guy Boy MaleMean DB / WA 0.65 0.53 0.56 0.32 0.60 0.28 0.40 0.03 0.49 0.38 0.42 RIPA 0.80 0.56 0.58 0.32 0.59 0.27 0.31 0.04 0.49 0.35 0.43 NBM 0.58 0.65 0.61 0.19 0.69 0.18 0.23 0.10 0.53 0.18 0.39 Table 3 : Results of the base pair stability experiments : Agreement between the bias directions assigned by a base pair and its capitalised form ( e.g. ( she , he ) and ( She , He ) ) for the 320 professions , and the mean over all base pairs . 
she her woman mary herself dgtr mother gal girl female he his man john himself son father guy boy maleMean PluralDB / WA 0.50 0.51 0.53 0.35 0.47 0.33 0.42 0.47 0.52 0.53 0.46 RIPA 0.57 0.58 0.63 0.39 0.53 0.46 0.44 0.53 0.53 0.50 0.52 NBM 0.69 0.57 0.72 0.38 0.65 0.32 0.50 0.59 0.60 0.62 0.56 CapitalDB / WA 0.61 0.66 0.59 0.42 0.67 0.79 0.61 0.50 0.50 0.44 0.58 RIPA 0.60 0.60 0.54 0.36 0.59 0.69 0.61 0.54 0.53 0.45 0.55 NBM 0.77 0.63 0.68 0.54 0.74 0.68 0.61 0.71 0.65 0.63 0.66 UpperDB / WA 0.19 0.35 0.43 0.17 0.29 0.48 0.18 0.20 0.34 0.30 0.29 RIPA 0.35 0.38 0.40 0.16 0.35 0.53 0.22 0.20 0.30 0.27 0.32 NBM 0.50 0.52 0.49 0.25 0.52 0.40 0.22 0.46 0.54 0.13 0.40 Table 4 : Results of the word form stability experiments : Agreement between the bias direction of a profession and its plural , capital and uppercase forms for each base pair , and the mean over all base pairs . 
she her woman mary herself dgtr mother gal girl female he his man john himself son father guy boy maleMean BSRIDB / WA 0.35 0.37 0.07 -0.03 0.14 0.03 0.45 0.39 -0.08 0.01 0.17 RIPA 0.44 0.40 0.09 -0.08 0.12 0.16 0.45 0.39 -0.08 0.01 0.19 NBM 0.27 0.32 -0.01 0.01 0.27 0.17 0.46 0.14 0.18 -0.04 0.18 AnimalDB / WA 0.54 0.38 0.54 0.54 0.54 0.31 0.23 0.46 0.54 0.08 0.42 RIPA 0.31 0.38 0.31 0.46 0.46 0.23 0.23 0.54 0.46 0.08 0.35 NBM 0.31 0.08 0.15 0.15 0.15 0.00 0.08 0.46 0.15 0.00 0.15 Table 5 : Results of the linguistic correspondence experiments : Agreement between the ground - truth and predicted gender for each base pair , and the mean over all 10 base pairs . 
she her woman mary herself dgtr mother gal girl female he his man john himself son father guy boy maleMean DB / WA & RIPA 0.69 0.86 0.64 0.90 0.82 0.79 0.92 0.85 0.89 0.96 0.83 DB / WA & NBM 0.54 0.37 0.62 0.44 0.55 0.54 0.46 0.34 0.48 0.47 0.48 RIPA & NBM 0.52 0.42 0.66 0.41 0.57 0.57 0.47 0.29 0.47 0.50 0.49 Table 6 : Comparing bias measures : Agreement between the bias direction assigned by each pair of bias measures ( with a Ô¨Åxed base pair ) for the 320 professions , and the mean over the 10 base pairs.765 she - he her - his woman - man mary - john herself - himself daughter - son mother - father gal - guy girl - boy female - male0.00.20.40.60.81.0Pearson Correlation CoefficientFigure 2 : Correlation between the cosine similarity of the base pair difference vectors and the corresponding pairwise kappa coefÔ¨Åcients for the DB / WA professions bias directions . 
ment between bias directions : For each base pair ( a , b ) , we computed cos(‚àí ‚Üía‚àí‚àí ‚Üíb , ‚àí ‚Üíc‚àí‚àí ‚Üíd ) , for each of the other 9 base pairs ( c , d ) , and compared these scores to the pairwise agreements between the corresponding DB / WA bias directions assigned to the professions . 
There was a high Pearson correlation ( max p - value 0.005 ) in each case , see Figure 2 . 
The lack of robustness of the gender bias measures means care should be taken is ascribing their output to historic bias in the training data or algorithmic bias in the embedding process . 
Rather , our analysis indicates that a signiÔ¨Åcant proportion of the ‚Äú bias ‚Äù found is an artifact of the evaluation method ( bias measures ) used . 
Comparing Bias Measures : A limitation of previous work is it unclear which of the proposed gender bias measures is best , even though they are often introduced as alternatives to one another . 
The results of our study are mixed and no one measure emerges as reliable . 
Despite NBM being designed as an alternative to DB , which takes into account the socially biased neighbours of a word , our experiments found it performs more poorly on the socially biased terms ( BSRI ) than DB with its recommended base pair ( she , he ) ( Table 5 ) . 
Conversely , it was less sensitive to different word - forms ( Table 4 ) . 
This is likely because different forms of wshare c ommon subsets of top K - neighbors with w. Furthermore , Ethayarajh et al . 
( 2019 ) claim RIPA is an improvement on WA because RIPA is robust to changing the base pair if the two corresponding difference vectors are ‚Äú roughly the same , ‚Äù and give ( man , woman ) and ( king , queen ) as an example . 
However , we Ô¨Ånd this claim does not hold : this change of base pair causes 28 % ( 91 ) of the Professions words to alter their RIPA bias direction . 
Finally , we compared agreement between the bias directions assigned to the professions by different pairs of measures ( Table 6 ) . 
The results show that on average , there is an almost perfect level of agreement ( 0.83 ) between RIPA and DB / WA , and moderate levels of agreement between NBM and the other measures . 
As RIPA and DB / WA have very similar formulae , the high level of agreement between them for each base pair indicates that the choice of base pair is highly inÔ¨Çuential and more important than the difference in their formulae . 
Figure 1 illustrates this point by showing that the measures tend to change in a similar manner from base pair to base pair for each word variant . 
Analogies do not indicate bias : Analogies are often used as evidence of bias in word embeddings ( Bolukbasi et al . 
, 2016 ; Manzini et al . 
, 2019 ) . 
This section argues they are unsuitable indicators of bias as they primarily reÔ¨Çect similarity , and not necessarily linguistic relationships like gender . 
More formally , given an analogy ‚Äú ais tobascis to ? , ‚Äù we show , using multi - dimensional vector - valued functions ( Larson and Edwards , 2016 ) , that if there is a high cosine similarity between aandc , the predicted answer will be a word similar to b. Suppose a function F : Rm‚ÜíRnhas component functions fi : Rm‚ÜíR , i‚àà{1 , . 
. 
. 
, n } , where F(‚àí ‚Üíx ) = ( fi(‚àí ‚Üíx))n i=1and‚àí ‚Üíx= ( xj)m j=1 . 
Then the limit of F , if it exists , can be found by taking the limit of each component function : lim‚àí ‚Üíx‚Üí‚àí ‚ÜíaF(‚àí ‚Üíx ) = /parenleftbigg lim‚àí ‚Üíx‚Üí‚àí ‚Üíafi(‚àí ‚Üíx)/parenrightbiggn i=1 . 
For Ô¨Åxed vectors‚àí ‚Üía , ‚àí ‚Üíb‚ààRn , letF : Rn‚ÜíRn , ‚àí ‚Üíx / mapsto‚Üí‚àí ‚Üíx‚àí‚àí ‚Üía+‚àí ‚Üíb . 
Fcan be expressed componentwise as F(‚àí ‚Üíx ) = ( fi(‚àí ‚Üíx))n i=1= ( xi‚àíai+bi)n i=1 . 
Then as each component function is continuous : lim‚àí ‚Üíx‚Üí‚àí ‚ÜíaF(‚àí ‚Üíx ) = /parenleftbigg lim‚àí ‚Üíx‚Üí‚àí ‚Üía(xi‚àíai+bi)/parenrightbiggn i=1 = ( ai‚àíai+bi)n i=1 = ‚àí ‚Üíb . 
Thus as‚àí ‚Üíxapproaches‚àí ‚Üía,‚àí ‚Üíx‚àí‚àí ‚Üía+‚àí ‚Üíbapproaches‚àí ‚Üíb . 
For embeddings , this means if‚àí ‚Üíais sufÔ¨Åciently similar to‚àí ‚Üíc , by Equation 2.2 , we expect the predicted answer d‚àóto the analogy ‚Äú ais tobascis to ? ‚Äù to be a word whose vector is similar to‚àí ‚Üíb . 
This was demonstrated empirically in ( Linzen , 2016 ) . 
Implications of the well - known analogy ‚Äú man is to computer programmer aswoman is to766 homemaker ‚Äù should be reinterpreted in light of this insight . 
Previous interpretations took this analogy to be evidence of systematic gender bias in the embedding space ( Bolukbasi et al . 
, 2016 ) . 
However , there is a very high cosine similarity between‚àí‚àí‚Üíman and‚àí‚àí‚àí‚àí‚àí‚Üíwoman ( 0.77)6 ; in fact , each is the most similar word to the other in the embedding space . 
The vectors for computer programmer andhomemaker are also highly similar ( 0.50 ) . 
The presence of homemaker can therefore be explained by its similarity to computer programmer rather than gender bias . 
Of course , embedding vector similarity does frequently indicate word relatedness ( e.g. ‚Äú king ‚Äù and ‚Äú queen ‚Äù ) . 
However , vector similarity may also be due to noise . 
As there is no obvious linguistic relationship between the words homemaker andcomputer programmer and neither are common words in the embedding vocabulary , we posit the latter is the case . 
This analogy has been taken as evidence of a gendered relationship between computer programmer and homemaker because it has been assumed that the principal relation between the vectors for man andwoman is gender , and that this relation carries over tocomputer programmer andhomemaker . 
This argument rests on the supposition that the difference vector‚àí‚àí‚Üíman‚àí‚àí‚àí‚àí‚àí‚àí‚Üíwoman encodes gender . 
However , embeddings were not designed to have such linear properties and their existence has been debated ( Linzen , 2016 ) . 
Furthermore , the top solution for ‚Äú man is to apple aswoman is to ? ‚Äù isapples , but the relationship between apple andapples is clearly pluralisation rather than gender . 
More generally , we took the commonly used Google Analogy Test Set ( Mikolov et al . 
, 2013a ) which contains 19,544 analogies ( 8,869 semantic and 10,675 syntactic ) split into 14 categories , such as countries and their capitals . 
This set contains 550 unique word pairs ( x , y ) ( such as ( apple , apples ) ) unrelated to gender.7In general , the two words in each of the 550 word pairs are highly similar to each other , with mean cosine similarity 0.62 and standard deviation 0.13 . 
We tested the analogy ‚Äú man is to xas woman is to ? ‚Äù using Equation 2.2 . 
This resulted 6By comparison , the mean cosine similarity for 100,000 pairs of words randomly sampled from the embedding space was 0.13 , with standard deviation 0.11 . 
7The category ‚Äú family ‚Äù was excluded as there are gender relationships between the word pairs.in 22 % being correctly solved ( i.e. returning y ) , including 76 % correct in the ‚Äú gram8 - plural ‚Äù category , which contains pluralised words ( note that the analogy not being solved correctly does not imply a dissimilar vector is being returned ) . 
This demonstrates that ‚Äú man is toxaswoman is to ? ‚Äù frequently solves analogies by returning words whose vectors are similar to‚àí ‚Üíx , without any need for a linguistically gendered relationship between xand the returned word . 
These observations have further implications for the biased analogy generating method of Bolukbasi et al . 
( 2016 ) , which was extended in ( Manzini et al . 
, 2019 ) . 
This method leveraged the base pair ( she , he ) to Ô¨Ånd word pairs ( x , y ) , such that ‚Äú heis toxassheis toy ‚Äù , where||‚àí ‚Üíx‚àí‚àí ‚Üíy||= 1 . 
However , the condition ||‚àí ‚Üíx‚àí‚àí ‚Üíy||= 1is equivalent to cos(‚àí ‚Üíx , ‚àí ‚Üíy ) = 1 2 . 
This forced similarity between xandycombined with the high similarity of she andhe(0.61 ) means this method is simply returning word pairs with a high similarity . 
Alternative choices of gender base pair such as ( woman , man ) would suffer from the same Ô¨Çaw . 
Consequently , analogies produced using this method should be treated with caution . 
8 Conclusions There has been a recent focus in the NLP community on identifying bias in word embeddings . 
While we strongly support the aim of such work , this paper highlights the complexity of trying to quantify bias in embeddings . 
We showed the reliance of popular gender bias measures on gender base pairs has strong limitations . 
None of the measures are robust enough to reliably capture social bias in embeddings , or to be leveraged in debiasing methods . 
In addition , we showed the use of gender base pairs to generate ‚Äú biased ‚Äù analogies is Ô¨Çawed . 
Our analysis can contribute to future work designing robust bias measures and effective debiasing methods . 
Although this paper focused on gender bias , it is relevant to work examining other forms of bias , such as racial stereotyping , in embeddings . 
Code to replicate our experiments can be found at : https://github.com / alisonsneyd / Gender _ bias_word_embeddings Acknowledgements This work was supported by the Institute of Coding which received funding from the OfÔ¨Åce for Students ( OfS ) in the United Kingdom.767 A Appendix BSRI Female Terms : affectionate , affectionately , cheerful , cheerfully , cheerfulness , childlike , compassionate , compassionately , feminine , femininely , gentle , gently , gullible , gullibility , gullibly , loyal , loyally , shy , shyly , shyness , sympathetic , sympathetically , tender , tenderly , tenderness , understanding , understandingly , warm , warmish , warmness , yielding BSRI Male Terms : aggressive , aggressively , aggressiveness , aggressivity , ambitious , ambitiously , ambitiousness , analytical , analytically , assertive , assertiveness , assertively , athletic , athleticism , athletically , competitive , competitiveness , competitively , dominant , dominantly , forceful , forcefulness , independent , independently , individualistic , masculine , selfsufÔ¨Åcient Female Animal Terms : bitch , cow , doe , duck , ewe , goose , hen , leopardess , lioness , mare , queen , sow , tigress Male Animal Terms : dog , bull , buck , drake , ram , gander , rooster , leopard , lion , stallion , drone , boar , tiger769 Proceedings of the 1st Conference of the Asia - PaciÔ¨Åc Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing , pages 770‚Äì780 December 4 - 7 , 2020 . 
¬© 2020 Association for Computational Linguistics ExpanRL : Hierarchical Reinforcement Learning for Course Concept Expansion in MOOCs Jifan Yu1 , Chenyu Wang1 , Gan Luo1 , Lei Hou1,2,3‚àó , Juanzi Li1,2,3 , Jie Tang1,2,3 , Minlie Huang1,2,3 , Zhiyuan Liu1,2,3 1Dept . 
of Computer SCi . 
& Tech . 
, Tsinghua University , China 100084 2KIRC , Institute for ArtiÔ¨Åcial Intelligence , Tsinghua University , China 100084 3Beijing National Research Center for Information Science and Technology , China 100084 { yujf18,luog18 } @mails.tsinghua.edu.cn { houlei , lijuanzi , jietang , aihuang , liuzy } @tsinghua.edu.cn Abstract Within the prosperity of Massive Open Online Courses ( MOOCs ) , the education applications that automatically provide extracurricular knowledge for MOOC users have become rising research topics . 
However , MOOC courses ‚Äô diversity and rapid updates make it more challenging to Ô¨Ånd suitable new knowledge for students . 
In this paper , we present ExpanRL , an end - to - end hierarchical reinforcement learning ( HRL ) model for concept expansion in MOOCs . 
Employing a two - level HRL mechanism of seed selection and concept expansion , ExpanRL is more feasible to adjust the expansion strategy to Ô¨Ånd new concepts based on the students ‚Äô feedback on expansion results . 
Our experiments on nine novel datasets from real MOOCs show that ExpanRL achieves signiÔ¨Åcant improvements over existing methods and maintain competitive performance under different settings . 
1 Introduction The cognitive - driven theory has been widely used in practical teaching since Ausubel Ô¨Årstly proposed it in ( Ausubel , 1968 ) , which suggests educators provide new knowledge for students to motivate their learning continuously . 
In fact , in addition to the concepts taught in course , many related concepts are also attractive and worthy of learning . 
As shown in Figure 1 , when a student studies the concept LSTM in ‚Äú Deep Learning ‚Äù course from Coursera1 , many related concepts , including its prerequisite concepts ( RNN ) , related scientists ( J¬®urgen Schmidhuber ) and its related applications ( Machine Translation ) can also beneÔ¨Åt his / her further study . 
In traditional classrooms , these concepts are often considerately introduced by teachers . 
‚àóCorresponding author . 
1https://www.coursera.org Figure 1 : An example of course - related concepts in the ‚Äú Deep Learning ‚Äù course from Coursera . 
However , in the era of Massive Open Online Courses ( MOOCs ) , thousands of courses are prerecorded for with millions of students with various backgrounds ( Shah , 2019 ) , which makes it infeasible to pick out these essential concepts manually . 
Therefore , there is a clear need to automatically discover course - related concepts so that they can easily acquire additional knowledge and achieve better educational outcomes . 
This task is formally deÔ¨Åned as Course Concept Expansion ( Yu et al . 
, 2019a ) , a special type of Concept Expansion orSet Expansion ( Wang and Cohen , 2007 ) , which refers to the task of expanding a small set of seed concepts into a complete set of concepts that belong to the same course or subject from external resources . 
Despite abundant efforts in related topics ( He and Xin , 2011 ; Shen et al . 
, 2017 ; Yan et al . 
, 2019 ) , existing methods still face three challenges when applied to MOOCs . 
First , distinct from the task of enriching a certain concept set , the purpose of course concept expansion is to beneÔ¨Åt students ‚Äô learning , making the context information insufÔ¨Åcient to detect whether a concept is appropriate to be an expansion result . 
How to properly introduce student feedback in the model ‚Äôs loop is a crucial challenge . 
Second , unlike the set expansion for a clear general category ( e.g. , countries ) , courses are often the770 combinations of multiple categories , especially in interdisciplinary courses like Mathematics for Computer Science2 . 
Therefore , it is n‚Äôt easy to model the course ‚Äôs semantic scope ( Curran et al . 
, 2007 ) when applying existing expansion methods . 
Third , MOOCs are updated continuously , and numerous new courses arise everyday ( Shah , 2019 ) , which requires a good generalization ability of the expansion model ; otherwise , the frequent model retraining will cause severe waste of resources . 
To address the above problems , we construct a novel interactive environment on real MOOCs , which collects students ‚Äô feedback on expansion results and provides new knowledge for MOOC students in an interesting way for better education . 
And based on the feedback , we propose ExpanRL , a hierarchical reinforcement learning framework for course concept expansion in MOOCs , which decomposes the concept expansion task into a hierarchy of two subtasks : high - level seed selection and low - level expansion . 
Boosted by user feedback on expansion results , ExpanRL jointly learns how to select seed concepts to model the semantic scope of the course better , and whether a concept is beneÔ¨Åcial for students . 
Moreover , the hierarchical reinforcement learning ( HRL ) structure enables ExpanRL to learn proper expansion strategies instead of the modeling of a particular course , making our model keep a high performance even in unobserved courses . 
The evaluation is conducted on 9datasets from real MOOC courses , compared with 5representative baseline methods . 
We further conduct an online evaluation to investigate whether students admit the expanded concepts . 
Our contributions include 1 ) an investigation on how to involve HRL framework into the task of concept expansion ; 2 ) a paradigm that connects the NLP concept expansion task with the educational application ; 3 ) an interactive MOOC environment , consisting of 9 novel datasets of different subjects , 6,553 extracted course concepts , and 495,324 user behaviors from a real MOOC website . 
2 Preliminaries 2.1 Problem Formulation Following ( Yu et al . 
, 2019a ) , Course Concept Expansion is formally deÔ¨Åned as : given the course corpusD , course conceptsM , and a knowledge 2A course from the University of London in Coursera.baseKBas an external source , the task is to return a ranked list of expanded concepts Ec . 
In this formulation , a course corpus is deÔ¨Åned asD={Cj}|n| j=1 , which is composed of ncourses ‚Äô video subtitles in the same subject area . 
Course concepts are the subjects taught in the course ( such asLSTM in Figure 1 ) , denoted as M={ci}|M| i=1 . 
( Pan et al . 
, 2017 ) . 
Knowledge base KB= ( E , R ) is consist of concepts Eand relations R , which is utilized as an external source to obtain expansion candidates . 
Though other source ( such as Web tables ) can also take on this role , we still employ aKBto search for expansion candidates like the prior work , i.e. , Ec‚äÇE. 2.2 Basic Model for Concept Expansion The general idea of concept expansion is Ô¨Årst to characterize the concept set according to its representative elements , then Ô¨Ånd new candidates and rank them to expand the set . 
Seed Selection Stage . 
A group of representative concepts are called seeds and formalized to K‚äÇ Ec(Wang and Cohen , 2007 ; Mamou et al . 
, 2018 ) . 
While the expansion process is often carried out iteratively , we also formalize the expansion set of roundttoEt c. Seed selection is to calculate the possibility that each concept in Et cbecomes a seed , i.e. ,P(ci‚ààKt‚äÇEt c|t ) , whereKtcontains the seeds oft - th round . 
Based on these seeds , we can extract features of the current set and search for candidate concepts for expansion from external sources . 
Expansion Stage . 
After Ô¨Ånding a new list of candidatesLt=/braceleftbig c1, ... ,ct / prime, ... ,c|Lt|/bracerightbig , expansion stage aims to calculate the likelihood of ct / primeto be a expanded concept . 
The top candidates ranked by ct / primeare selected as new expanded concepts , denoted asNtthe likelihood can be formalized as P(ct / prime‚ààNt‚äÇLt|Kt , t / prime ) . 
The expansion set is refreshed as Et+1 c = Et c‚à™ Ntuntil its size reaches the preset upper limit œÑor can not Ô¨Ånd new candidates ( He and Xin , 2011 ) . 
2.3 Interactive MOOC Environment The workÔ¨Çow above has been experimentally proven to be effective in many concept expansion tasks ( Shen et al . 
, 2018 ; Rastogi et al . 
, 2019 ) . 
However , such methods only consider the course concepts ‚Äô semantic information , which makes their expansion results hard to match real learning needs , especially when dealing with the multi - category771 MOOC courses . 
Meanwhile , since the models are trained before launching , how to maintain high performance on new arisen courses is challenging . 
Yu et al . 
( 2019a ) designs an online game in MOOCs to collect user feedback on the expansion result , thereby employing an active pipeline model to face the above problems , which provides an interactive MOOC environment for reinforcement learning models . 
However , the size of publicly published datasets ( 4 courses with 800 concepts in each course ) is still insufÔ¨Åcient to meet the need to train advanced deep learning models . 
Therefore , we extract 68 real MOOC courses of six subjects and build a large - scale MOOC interactive environment , which contains a gameÔ¨Åed interface for feedback collection and several course datasets : ‚Äú Mathematics ‚Äù , ‚Äú Chemistry ‚Äù , ‚Äú Architecture ‚Äù , ‚Äú Psychology ‚Äù , ‚Äú Material Science ‚Äù and ‚Äú Computer Science ‚Äù , covering diverse subjects of natural science , social science and engineering . 
The details of the datasets are presented in the experiment section . 
We construct the environment through three stages . 
First , for each subject , we select its most relevant courses from a real MOOC website3 . 
We use the method of Pan ( 2017 ) to extract the course concepts and manually select the high - quality ones as the course concepts M. Second , we take XLORE ( Jin et al . 
, 2019 ) as KBto search for candidate expansion concepts . 
Figure 2 : A demonstration of our interactive game in course Introduction to psychology . 
MOOC users can click irrelevant expansion candidates to get bonuses . 
The yellow concept on the left is from course , and the green concepts are expanded candidates . 
Finally , we set up a game to present the expansion candidates . 
As shown in Figure 2 , real MOOC users are drawn to pick out the course - unrelated ones to get bonuses . 
To ensure data quality , we set the game bonus depending on the group voting 3Anonymous for blind review.result . 
We also avoid their irresponsible operations by mixing some extracted course concepts among candidates to detect the spoilers . 
The operation records are employed to train our reinforcement learning model proposed in the next section . 
3 The Proposed Model In this section , we Ô¨Årst introduce our hierarchical reinforcement concept expansion framework , ExpanRL , then present our high - level seed selection model and low - level expansion model separately . 
Figure 3 : Framework of ExpanRL . 
3.1 Overview To obtain high - quality expanded course concepts for serving students in MOOCs , ExpanRL still needs to address three crucial problems . 
1 . 
How to properly utilize user feedback ? 2 . 
How to keep accurate modeling of the course during iterations ? 3 . 
How to keep a good generalization ability of the model when expanding in new MOOC courses ? Thanks to the interactive MOOC environment , we can deal with these issues by decomposing the basic concept expansion workÔ¨Çow into a hierarchical reinforcement learning framework . 
Figure 3 shows that the model can learn the complex connection between concepts and courses from user feedback instead of simple contextual information . 
The main idea of ExpanRL is to upgrade expanding strategies via such an end - to - end model , whose entire expansion process works as the basic concept expansion methods in Section 2.2 , which can be naturally formulated as a semi - Markov decision process ( Sutton et al . 
, 1999 ) like : 1 ) a high - level RL process that selects seeds fromEt cto search for a list of candidates Lt ; 2 ) a low - level RL process that detect the high - quality expansion results among candidates and obtain Ntto refresh the set772 toEt+1 c. This process iterate until the size of the expansion set reaches the preset limit , œÑ . 
Specially , before the whole process , we Ô¨Årst utilize the method in ( Pan et al . 
, 2017 ) to extract course conceptsMfrom the given course corpus Dand initialize E0 c = M. 3.2 Seed Selection with High - level RL The high - level RL policy ¬µaims to select kseeds from the existing set Ec , which can be regarded as a conventional RL over options . 
An option refers to a high - level action , and a low - level RL will be launched once the agent executes an option . 
The high - level time step tis the expansion round . 
Option : The option otis a vector consisting of 0and1 , which represents the i - th concepts from expansion set Et cis or is not a selected seed for the current expansion round . 
Thus the dimension of otis the same as the size of Et c. When a low - level RL process enters a Ô¨Ånal state , the agent ‚Äôs control will be taken over to the high - level RL process to execute the next options . 
State : The state sh t‚àà Shof the high level RL process at time step t , is represented by a k√óC matrix reshaped from the hidden state ht , where kis the size of seed set and Cis the size of a compressed word embedding . 
sh t = reshape ( ht ) ( 1 ) To obtain the hidden state ht , we introduce a set representation RepSet ( Skianis et al . 
, 2019 ) to encode the current expansion set Et c. RepSet is unsupervised , order independent and can encode an n√óVmatrix to aVdimension vector . 
Note that Et‚àí1 c‚äÇEt c , so the current state is effected by the last state ht‚àí1 . 
ht = RepSet ( Et c ) . 
( 2 ) Policy : The stochastic policy for seed selection ¬µ : S‚ÜíO which speciÔ¨Åes a probability distribution over options : ot‚àº¬µ(ot|sh t ) = Rt = softmax ( sh tW(Et c)T ) . 
( 3 ) where Wis a learnable parameter , which compresses aVlength word embedding to a Clength word embedding . 
Et cis the matrix which consists of all course concepts ‚Äô word vector . 
Rtis a matrix , while Rt j , iindicates the possibility of the i - thconcept inEt cto be thej - th seed : p(Kt j = ci , ci‚ààEt c|t ) = /braceleftbigg Rt j , i 0 ifciis selected before . 
( 4 ) And the possibility of the high - level RL to select Ktis shown below . 
Note that this possibility pis independent of i. ph(Kt ) = k / productdisplay j=1p(Kt j = ci , ci‚ààEt c|t ) ( 5 ) Reward : Then , the environment provides intermediate reward rh tto estimate the future return when executingot . 
The reward is given by the total reward of the last round of concept expansion . 
rh t=/summationdisplay rl t / prime(ot ) , ( 6 ) whererl t / prime(ot)is the low - level reward in time t / prime while the high - level option is ot . 
Candidate generation after high - level options : After the agent gives out an option ot , we link the seed concepts from KtintoKBand Ô¨Ånd their Ô¨Årstorder neighbor concepts as the candidate list Lt . 
Note thatLtis sorted using the pairwise similarity between newly found candidates and seeds . 
3.3 Concept Expansion with Low - level RL Once the high - level policy has selected the seed set and generated a candidate list Lt , the low - level policyœÄwill scan the list and select high - quality expansion concepts from it to update Ec . 
The lowlevel policy over actions is formulated very similarly as the high - level policy over options . 
The optionotandKtfrom the high - level RL is taken as additional input throughout the low - level expansion process . 
The time step t / primein low - level means thet / prime - th candidate inLtand the Ô¨Ånal expanded concepts in this round is Nt . 
Action : The action at each time step is to assign a tag to the current candidate concept . 
The action space , i.e. ,A={1,0 } , where 1represents the present concept is an expansion result of this set , 0represents that the concept is not an expansion result . 
State : The low - level intra - option state sl tis represented by the word embedding of current expansion candidatect / prime . 
sl t / prime = ct / prime ( 7 ) Moreover , we use a Bi - LSTM ( Huang et al . 
, 2015 ) to provide a hidden state of current candidate list773 hl tby encoding : 1 ) the selected seeds Kt , 2 ) a zero vector as a segmentation , 3 ) the candidate list Lt , thereby utilizing the information of high - level optionotto help low - level decisions . 
hl t = BiLSTM ( /bracketleftbig Kt;0;Lt / bracketrightbig ) ( 8) Policy : The stochastic policy for expansion œÄ : S‚ÜíA outputs an action distribution given intraoption state sl tand the high - level option ot / primethat launches the current subtask . 
Here ‚äôis the vector dot product . 
at / prime‚àºœÄ(at / prime|sl t;ot ) = pl(ct / prime ) = p(ct / prime‚ààNt|t / prime ) = sigmoid ( hl t‚äôst / prime ) , ( 9 ) Reward : As introduced in section of Preliminaries , we construct an interactive game on the MOOC website to collect feedback from users on the expanded concepts . 
Users can pick out the unrelated concepts of the course , and the picked times of each expansion result ciis recorded as œï(ci ) . 
Since such operations indicate the users ‚Äô disagreements of the result , the low - level reward is designed to be negatively correlated with œï(ci)as follows : rl t / prime=/braceleftbigg‚àíœï(ci)/maxcj‚ààLt(œï(cj)),at‚Äò=1 œï(ci)/maxcj‚ààLt(œï(cj)),at / prime=0(10 ) The count of user clicks determines the degree of relevance of each candidate to the course . 
It is worth noting that this degree is dynamic and depends on the concept that is mostly picked . 
This setting effectively controls the range of rewards . 
Set refreshment after low - level actions : After the agent gives out an action at / prime , we can Ô¨Ånally obtain the new expanded concepts Nt . 
The expansion set is updated as Et+1 c = Et c‚à™Ntand the process turn to another round . 
3.4 Hierarchical Policy Learning To optimize the high level policy , we aim to maximize the expected cumulative rewards from the main task at each step tas the agent samples trajectories following the high - level policy ¬µ , which can be computed as follows : J(Œ∏¬µ,t ) = Esh , o , rh‚àº¬µ(o|sh)[T / summationdisplay t=0logph(Kt)T / summationdisplay s = tŒ≥s‚àítrh s ] , ( 11 ) where¬µis parameterized by Œ∏¬µ,Œ≥is a discount factor in RL , and the whole sampling process ¬µ takes T time steps before it terminates . 
Algorithm 1 : Training Procedure of HRL 1Extract course concepts from Dand initiate E0 c = M ; 2Initiate state sh 0‚Üê0and time step t‚Üê0 ; 3while|Ec|<œÉdo 4 Calculate sh tby Eq.(1 ) ; 5 Sampleotfromsh tby Eq.(3 ) ; 6 Search for candidates from KBand generate a ranked candidate list L ; 7 forj‚Üê1to|L|do 8t / prime‚Üêt / prime+ 1 ; 9 Calculate sl t / primeby Eq.(7 ) ; 10 Sampleal t / primefromsl t / primeby Eq.(9 ) ; 11 Add the expansion result into game and get feedback ; 12 Obtain low - level reward rl t / primeby Eq.(10 ) ; 13 end 14t‚Üêt+ 1 , refreshEc ; 15 Obtain low - level Ô¨Ånal reward rl fin , high - level rewardrh t ; 16end 17Obtain high - level Ô¨Ånal reward rh finby Eq.(6 ) ; 18Optimize the model with Eq.(11 ) and Eq.(12 ) ; Similarly , we learn the low - level policy by maximizing the expected cumulative intra - option rewards from the sub task over option otwhen the agent samples along low - level policy œÄ(¬∑|ot)at time step t : J(Œ∏œÄ , t;ot / prime ) = Esl , a , rl‚àºœÄ(a|sl;ot / prime)[T / prime / summationdisplay t / prime=0logpl(ct / prime)T / prime / summationdisplay s = t / primeŒ≥s‚àítrl s ] , ( 12 ) if the subtask ends at time step T / prime . 
Then we use policy gradient methods ( Sutton et al . 
, 2000 ) with the REINFORCE ( Williams , 1992 ) algorithm to optimize both high - level and low - level policies . 
The entire training process is described at Algorithm 1 . 
4 Experiments 4.1 Experiment Setting 4.1.1 Datasets We construct an interactive MOOC environment as Section 2.3 to collect user feedback on expansion results . 
To build a solid evaluation , we randomly selected 5 % expanded concepts to be manually labeled benchmarks . 
For each concept , three annotators majoring in the corresponding domain are asked to label them as ‚Äú 0 : Not helpful ‚Äù or ‚Äú 1 : Helpful ‚Äù based on their knowledge . 
Thus , each dataset is triply annotated , and Pearson correlation coefÔ¨Åcient is computed to assess the inter - annotator agreement . 
A candidate is labeled as a related concept when more than two annotators give positive774 MAT CHEM PSY MS ARC CS MAT+CS CHEM+MS MS+ARC # courses 12 6 16 8 14 12 4 4 5 |M| 1,688 1,404 568 842 1,036 1,015 230 417 382 # operations 93,762 103,652 48,492 40,254 120,384 88,779 33,521 52,467 56,787 0 - Label 24,278 15,796 13,245 11,876 33,127 17,775 7,092 9,367 7,898 1 - Label 6,976 18,755 2,919 1,542 7,001 11,818 3,533 4,790 1,229 correlation 0.712 0.694 0.705 0.732 0.678 0.689 0.655 0.688 0.701 Table 1 : Statistics of datasets tags . 
Table 1 presents the detailed statistics , where # courses , |M|,1 - Label and0 - Label are the number of courses , course concepts , positive and negative labels . 
# operations are user click times which is obtained from the game . 
MAT , CHEM , PSY , MS , ARC andCScorrespond to Mathematics , Chemistry , Psychology , Material Science , Architecture and Computer Science . 
In particular , we select 13 interdisciplinary courses4and build three multi - category course datasets as MAT+CS , CHEM+MS andMS+ARC to further estimate the performance of ExpanRL on interdisciplinary courses . 
Note that these three datasets are subsets of the above six ‚Äôs . 
Dataset Usage . 
All the models are trained on the user operation data and evaluated on the expert annotated data . 
For the supervised learning baselines , we set the concepts with top 70 % click records as negative , and the rest as positive samples . 
4.1.2 Basic Settings All hyper - parameters are tuned on the validation set . 
The dimension of word vectors in Eq . 
( 2 ) is 768 . 
The dimension of the compressed word vector C in Eq . 
( 1 ) is 128 . 
The word vectors of all baseline methods are initialized using BERT ( Devlin et al . 
, 2019 ) . 
The learning rate is 1.0√ó10‚àí4for low - level RL , and 1.0√ó10‚àí5for high - level . 
The discount factorŒ≥is 0.99 . 
The seed size kis set to 10 and the upper limitœÑofEcis 20,000 . 
4.1.3 Baselines We compare our hierarchical RL model ( denoted as HRL ) with Ô¨Åve typical methods of set expansion . 
As these methods obtain expansion candidates from diverse resources , we mainly employ the different similarity metrics to rank the same expansion candidate list for evaluation . 
Especially to investigate the impact of seed selection strategies , we use a K - means clustering - based method and a pairwise similarity - based method to replace the high - level RL network , which are denoted as C - RL and P - RL . 
4Course list is shown in Appendix.‚Ä¢PR . 
Graph based method : We build the candidates and course concepts into a graph . 
When the similarity between two concepts exceeds a threshold5œÉPR , there is a link between them . 
The PageRank score of each candidate is Ô¨Ånally used for sorting . 
A most famous method employing graph based ranking is SEAL ( Wang and Cohen , 2007 ) ‚Ä¢SEISA . 
SEISA ( He and Xin , 2011 ) is an entity set expansion system developed by Microsoft after SEAL and outperforms traditional graph - based methods by an original unsupervised similarity metric . 
We implement its Dynamic Thresholding algorithm to sort expanded concepts . 
‚Ä¢EMB . 
Embedding based method mainly utilizes context information to examine the similarity between expanded concepts and seeds according to ( Mamou et al . 
, 2018 ) . 
For each expanded concept e , we calculate the sum of its cosine similarities with course concepts Min BERT ( Devlin et al . 
, 2019 ) and use the average as golden standard to rank the expanded concept list . 
‚Ä¢PUL . 
PU learning is a semi - supervised learning model regarding set expansion as a binary classiÔ¨Åcation task . 
We employ the same setting as ( Wang et al . 
, 2017 ) to classify and sort concepts . 
‚Ä¢PIP . 
It is a pipeline method for course concept expansion ( Yu et al . 
, 2019a ) , which Ô¨Årst uses an online clustering method during candidate generation and then classify them to obtain Ô¨Ånal expansion results . 
We follow the workÔ¨Çow of this work to sort expanded concepts . 
4.1.4 Evaluation Metrics Our objective is to generate a ranked list of expanded concepts . 
Thus , we use the Mean Average Precision ( MAP ) as our evaluation metric , which is the preferred metric in information retrieval for evaluating ranked lists . 
4.2 Overall Evaluation Table 2 summarizes the comparing results of different methods on all datasets . 
The evaluation is 5œÉPRis experimentally set to 0.5.775 MAT CHEM PSY MS ARC CS Avg MAT+CS CHEM+MS MS+ARC I - Avg PR 0.763 0.705 0.482 0.470 0.300 0.690 0.568 0.659 0.664 0.401 0.575 SEISA 0.805 0.711 0.473 0.524 0.570 0.713 0.632 0.797 0.691 0.377 0.622 EMB 0.747 0.687 0.474 0.533 0.442 0.812 0.616 0.710 0.655 0.377 0.581 PUL 0.878 0.811 0.845 0.745 0.757 0.850 0.822 0.880 0.782 0.646 0.769 PIP 0.848 0.782 0.803 0.772 0.775 0.821 0.800 0.893 0.835 0.851 0.865 C - RL 0.902 0.795 0.818 0.753 0.716 0.800 0.797 0.851 0.849 0.758 0.820 P - RL 0.892 0.768 0.606 0.749 0.821 0.767 0.835 0.871 0.852 0.662 0.795 HRL 0.903 0.857 0.901 0.806 0.828 0.878 0.862 0.909 0.903 0.886 0.898 Table 2 : MAP of different methods on datasets . 
( Seed set size = 10 ) divided into two parts . 
The six datasets on the left are the performance of the model on various subjects , and Avg represents the average of their MAPs . 
The three datasets on the right are from the selected interdisciplinary courses , and I - Avg is the average of the model performance on them . 
We also divide the methods into unsupervised , supervised , and reinforcement learning models for further analysis . 
Overall , our approach HRL maintains an impressive performance ( at 0.862 of Avg and 0.898 of I - Avg ) over the existing methods , and unsupervised methods ( such as SEISA , PR ) are not so competitive when compared with methods with supervised information . 
We lead a detailed investigation to detect the performance among different datasets and the impact of seed selection in the following aspects : For different datasets , our methods achieve robust results . 
It is worth noting that the range of the MAP of our method on these datasets does not exceed 0.097 , while other baselines suffering from severe oscillations ( SEISA of 0.428 , EMB of 0.435 , and PUL of 0.234 ) . 
And these supervised methods ( PUL , PIP ) that perform well on a certain dataset are further analyzed in subsequent experiments . 
For the performance on interdisciplinary courses . 
Most of the baselines meet a decline when turned to interdisciplinary courses . 
From this angle , PUL can not face this challenge . 
But PIP , C - RL , and HRL perform even better ( with a lift of 0.04 on average ) , most likely because they all have a clustering - like seed selection process . 
For different seed selection strategies . 
We also detect the impact of seed selection by replacing high - level RL . 
The comparison among three RL methods shows that : 1 ) P - RL performs better in one - category expansion tasks ( beat C - RL at 0.038 ) ; 2 ) C - RL deal with interdisciplinary courses better than P - RL ( as discussed above ) ; 3 ) HRL is stronger than these two methods in all datasets . 
The results ( a ) The MAP of different number of training sets . 
( b ) The MAP of seed sizes . 
Figure 4 : Performance of different settings . 
( a ) shows the average MAP when mask some of the datasets in training . 
( b ) shows the MAP of different seed size . 
exactly prove the superiority of HRL ‚Äôs seed selection over rule - based strategies . 
4.3 Result Analysis Generalization Ability . 
Expansion models in MOOCs need to face with plenty of new courses every day . 
Thus we lead strict experiments to estimate the generalization ability of the model by masking training datasets . 
For example , the bar of n= 5 in Figure 4(a ) indicates the average MAP when the models are trained on Ô¨Åve subject datasets and tested on the other one . 
Thus n= 6is the average MAP in Table 2 while n= 5 andn= 4 present the results of facing one or two kinds of new courses . 
Here we select HRL , PUL , and PIP for observation . 
Such an experiment shows that HRL still maintains an outstanding performance in new courses . 
Still , PIP and PUL suffer from a sharp decline in untrained new datasets ( even at the same level as unsupervised methods ) . 
The size of seed set k. For different settings of seed sizes , we compare the performance of ExpanRL with other RL based baselines . 
As shown in Figure 4(b ) , HRL keeps a high level of MAP among these settings ( all over 0.8 on average ) . 
Meanwhile , we Ô¨Ånd that all these RL - based methods perform776 Cr@10 Cr@20 Cr@50 PR 0.097 0.182 0.425 SEISA 0.097 0.204 0.459 EMB 0.071 0.150 0.359 PUL 0.041 0.091 0.349 PIP 0.069 0.126 0.342 HRL 0.036 0.082 0.258 Table 3 : Online Evaluation results . 
better in small or large seed size ( less than 10or larger than 40 ) , which requires future detection on this phenomenon . 
Discussion . 
Based on the above experimental results , we summarize the analysis as follows : 1 ) the performance of unsupervised methods on different datasets is not as stable as the supervised or RL methods ; 2 ) except for models that have a clustering - like seed selection process ( PIP , CRL , HRL ) , most models suffer from declines on interdisciplinary datasets ; 3 ) although supervised models ( PIP , PUL ) perform well in some cases , they drastically decline in untrained new courses ; 4 ) HRL , consisting of a feasible seed selection RL and expansion strategies from human efforts , keep a high performance under different settings . 
HRL deal with the challenges in MOOC expansion tasks , as claimed in the introduction . 
4.4 MOOC Online Evaluation Utilizing user feedback on the expansion results from our interactive MOOC environment , we also set up an online evaluation to detect whether users agree on the expansion results . 
Following the same evaluation metric in ( Yu et al . 
, 2019a ) , we denote Click Rate asCr@q , which means the click rate of topqexpanded concepts , i.e. , Cr@q = q / summationdisplay i=1œï(ci)/|Ec|/summationdisplay j=1œï(cj ) ) ( 13 ) A smallerCr@qindicates more users think the results are relevant to the course . 
We record the performance of each method in Table 3 . 
Results show that ExpanRL obtains the best feedback from MOOC users under all three settings . 
It ‚Äôs worth noting that the advantage of ExpanRL is evident while selecting larger - scale samples ( The overlap rises from 0.005 to 0.091 ) , which indicates that our model can provide more high - quality concepts . 
5 Related Work Our work follows the task of concept expansion in MOOCs ( Yu et al . 
, 2019a ) , a particular type of setexpansion problem , which takes several seeds as input and expands the entity set . 
Set expansion was born to serve knowledge acquisition applications on the Internet . 
Google Sets was a pioneer which leaded a series of early research , e.g. Bayesian Sets ( Ghahramani and Heller , 2006 ) , SEAL ( Wang and Cohen , 2007 ) , SEISA ( He and Xin , 2011 ) and others ( Sarmento et al . 
, 2007 ; Shi et al . 
, 2010 ; Wang et al . 
, 2015 ) . 
These efforts utilize web tables as a resource and mainly serves for search engines . 
Recently , more related research has turned its attention to other application Ô¨Åelds , such as news mining ( Redondo - Garc ¬¥ ƒ±a et al . 
, 2014 ) , knowledge graphs ( Zhang et al . 
, 2017 ) , education assistance ( Yu et al . 
, 2019a ) , etc . 
Meanwhile , corpus - based expansion methods snowball , and iterative bootstrapping became a common solution ( Shen et al . 
, 2017 ; Yu et al . 
, 2019b ; Yan et al . 
, 2019 ) , which expands the set in round and select high - quality results to extract feature iteratively . 
ExpanRL is inspired by this type of method and is designed to optimize the existing iterative process . 
ExpanRL also beneÔ¨Åts from hierarchical reinforcement learning ( HRL ) , which has been employed in many NLP tasks ( Zhang et al . 
, 2019 ; Takanobu et al . 
, 2019 ) and achieved impressive results . 
By decomposing complex tasks into multiple small tasks to reduce the complexity of decision making ( Barto and Mahadevan , 2003 ) , HRL naturally matches the iterative set expansion tasks . 
6 Conclusion and Future Work We investigate the task of course concept expansion , which utilizes the NLP approaches in improving MOOC education . 
After constructing a novel interactive MOOC environment to collect user feedback on expansion results , we design a paradigm , ExpanRL , which decomposes the concept expansion task into a hierarchy of two subtasks : highlevel seed selection and low - level concept expansion . 
Experiment results on nine datasets from real MOOCs prove that ExpanRL can better serve students by recognizing the helpful expanded results and maintaining good performance in interdisciplinary courses and even new courses . 
Promising future directions include detecting how to ensemble supervised learning and RL expansion models and applying the proposed model in related tasks . 
We also hope our design of interactive games can call for more fancy methods that utilize student feedback in NLP applications777 in Education . 
Acknowledgement This work is supported by the National Key Research and Development Program of China ( 2018YFB1004503 ) , NSFC Key Projects ( U1736204 , 61533018 ) , grants from Beijing Academy of ArtiÔ¨Åcial Intelligence ( BAAI2019ZD0502 ) , Institute for Guo Qiang , Tsinghua University ( 2019GQB0003 ) , and XuetangX. Abstract Question generation ( QG ) has recently attracted considerable attention . 
Most of the current neural models take as input only one or two sentences and perform poorly when multiple sentences or complete paragraphs are given as input . 
However , in real - world scenarios , it is very important to be able to generate high - quality questions from complete paragraphs . 
In this paper , we present a simple yet effective technique for answer - aware question generation from paragraphs . 
We augment a basic sequence - to - sequence QG model with dynamic , paragraph - speciÔ¨Åc dictionary and copy attention that is persistent across the corpus , without requiring features generated by sophisticated NLP pipelines or handcrafted rules . 
Our evaluation on SQuAD shows that our model signiÔ¨Åcantly outperforms current state - of - theart systems in question generation from paragraphs in both automatic and human evaluation . 
We achieve a 6 - point improvement over the best system on BLEU-4 , from 16.38 to 22.62 . 
1 Introduction and Related work Automatic question generation ( QG ) from text aims to generate meaningful , relevant , and answerable questions from a given textual input . 
Owing to its applicability in conversational systems such as Cortana , Siri , chatbots , and automated tutoring systems , QG has attracted considerable interest in both academia and industry . 
Recent neural network - based approaches ( Du et al . 
, 2017 ; Kumar et al . 
, 2018a , b ; Du and Cardie , 2018 ; Zhao et al . 
, 2018 ; Song et al . 
, 2018 ; Subramanian et al . 
, 2018 ; Tang et al . 
, 2017 ; Wang et al . 
, 2017 ) represent the state - of - the - art in question generation . 
Most of these techniques learn to generate questions from short text , i.e. , one or two sentences ( Du et al . 
, 2017 ; Kumar et al . 
, 2018a , b ; Du and Cardie , 2018 ) . 
On the other hand , the ability to generate high - quality questions from longer text such as from multiplesentences or from a paragraph in its entirety , is more useful in real - world settings . 
However , given that a paragraph contains a longer context and more information than a sentence , it is a signiÔ¨Åcantly more challenging problem to generate questions around a longer context . 
In Ô¨Ågure 1 we present one motivating example demonstrating why the model needs information more than just a single sentence for generating question a meaningful and relevant question . 
As we can see in Ô¨Ågure 1 , question 2 , question generated by our model use multiple sentences as context . 
Du et al . 
( 2017 ) recently observed that 20 % of the questions in the SQuAD dataset ( Rajpurkar et al . 
, 2016 ) require paragraphlevel information to answer them . 
For the same reason , it is intuitive to conclude that the ability to consider the complete context ; however long it may be , is critical for generating high - quality questions . 
Legislative power in Warsaw is vested in a unicameral Warsaw City Council ( Rada   Miasta ) , which comprises 06 members . 
Council members are elected directly every    four years . 
Like most legislative bodies , the City Council divides itself into   committees which have the oversight of various functions of the city government . 
  Bills passed by a simple majority are sent to the mayor ( the President of Warsaw ) ,   who may sign them into law . 
If the mayor vetoes a bill , the Council has 30 days to   override the veto by a two - thirds majority vote . 
  Human Generated : How many members are on the Warsaw City Counil ?   Our Model :   How many members are in the Warsaw City Council ?   Human Generated :   How often are elections for the counsel held ?   Our Model :   How often are the Rada Miasta elected ?   Human Generated : What does the City Council divide itself into ?   Our Model : The City Council divides itself into what ?   Human Generated :   How many days does the Council have to override the mayor 's veto ?   Our Model : How long does it take to override the veto ?   Figure 1 : Examples of ground - truth questions and questions generated by our model from the same paragraph . 
Each question and its corresponding answer are highlighted using the same color . 
Zhao et al . 
( 2018 ) very recently proposed a technique ( referred to MPGSN here ) for paragraph - level question generation using a max out pointer mechanism and a gated self - attention encoder . 
Their best model achieves BLEU-4 of 16.38 on SQuAD with paragraphs as input . 
Compared to ( Zhao et al . 
,781 2018 ) , our model has less number of parameters ( making it more computationally efÔ¨Åcient ) , is relatively easy to train and is somewhat deterministically biased toward the generation of important words in the input paragraph . 
In this paper , we propose a simple yet effective paragraph - level question generation technique . 
We augment the standard sequence - to - sequence model based on bidirectional LSTM with two components : ( 1 ) a dynamic , paragraph - speciÔ¨Åc dictionary and ( 2 ) a copy attention mechanism that is persistent across paragraphs . 
Our evaluation on SQuAD shows signiÔ¨Åcant improvement over MPGSN in automatic evaluation . 
We achieve a 6 - point increase with respect to BLEU-4 ( from 16.38 to 22.62 ) over MPGSN ‚Äôs best system . 
We perform the human evaluation of our model with and without copy attention , and we observe that we obtain 27 % more relevant questions when the copy attention is incorporated . 
For a given paragraph as input , we depict in Figure 1 , the ground - truth questions as well as the questions generated along with the answers highlighted in the paragraph . 
As can be seen from the example , while generating the second question(highlighted in green color ) , our model uses information not only from the sentence containing the answer , but also relevant context from the complete paragraph . 
... ... +     ùúéx x+ Attention   weights   Paragraph   Encoder   Word   Vectors   Answer vector Context   Vector   Question   Decoder Source Vocab   Distribution   Answer encoded paragraph Generated Question Final    Distribution   p(cs=0 )   p(cs=1 )   ... ùúé Figure 2 : Overall architecture of our paragraph - level question generation model . 
2 Problem Formulation & Approach Given a paragraph ‚Äò P ‚Äô and answer ‚Äò A ‚Äô , a question generation model iteratively samples question word qt‚ààVQat every time step ‚Äò t ‚Äô from the probability distribution given by : Pr(Q|P , A;Œ∏)=|Q|/productdisplay t=1Pr(qt|P , A;Œ∏ ) ( 1 ) WhereVQis the question vocabulary , Œ∏is the set of parameters , and Ais the answer . 
Our question generation model consists of a two - layer paragraph encoder and a one - layer question decoder , equipped with a dynamic dictionary and copy attention . 
In Figure 2 , we illustrate the overall architecture of our paragraph level question generation model . 
The dynamic dictionary allows every training instance ( paragraph ) to have its own vocabulary instead of relying on the preprocessed global vocabulary . 
Copy attention enables the model to predict question words from the extended vocabulary ( complete vocabulary + paragraph vocabulary ) . 
Copy attention operates over the union of words in vocabulary and paragraph words . 
2.1 Paragraph encoder We use a two - layer bidirectional long short - term memory ( Bi - LSTM ) network stack as the paragraph encoder . 
The paragraph encoder takes an answer - tagged paragraph as input and outputs a representation of the paragraph . 
Note that the Bi - LSTM network processes the input paragraph in both the forward and backward directions:‚àí ‚Üíht = LSTM ( et,‚àí‚àí‚Üíht‚àí1)and‚Üê ‚àíht = LSTM ( et,‚Üê‚àí‚àíht+1 ) , where‚àí ‚Üíht(resp.‚Üê ‚àíht ) is the forward ( resp . 
backward ) hidden state at time step tandetis the vector representation of current input xtat time stept . 
The Ô¨Ånal hidden state for the current word input is the concatenation of the forward and backward hidden state vectors : ht=[‚àí ‚Üíht,‚Üê ‚àíht ] . 
2.2 Dynamic , shared dictionary In the traditional approach , a new / unknown word is typically replaced with the ‚Äú < unk > ‚Äù token . 
The copy mechanism ( Gu et al . 
, 2016 ) then unfortunately learns to copy this ‚Äú < unk > ‚Äù token instead of the actual ( unknown ) word from the source paragraph . 
Instead , we use a separate dynamic dictionary unique to each source paragraph , which includes all and only words that occur in the paragraph . 
This allows our model to copy source words that may not be in the target dictionary into the target ( question ) . 
Using a dynamic dictionary consisting of the preprocessed vocabulary instead of a static one enables the copy mechanism to copy the exact words directly into the question , even if they are rare and unknown . 
Given a source paragraph p , we denote its dynamic vocabulary by Vp . 
Our copy attention mechanism takes into account Vpand the global vocabularyVto determine whether to copy a word from Vp or to predict a word from question vocabulary VQ . 
As our model ‚Äôs source as well as target are in782 Figure 3 : Visualizing attention weights for the second generated question in Fig . 
1 . 
the same language , we work with a shared source and target vocabulary , though we learn different language models for the paragraph and the question . 
Sharing source and target vocabulary also decreases the memory requirement resulting from matrix multiplication ( thus making faster training through larger batch size ) possible . 
It also enables efÔ¨Åcient question decoding , thus reducing the time for inference on the test data . 
2.3 Question decoder Our question decoder is another Bi - LSTM that takes as input the last hidden state and context representation from the encoder and generates question words sequentially based on the previously generated words . 
The decoder hidden state ( st= [ ‚àí ‚Üíst,‚Üê ‚àíst ] ) at time step tis the concatenation of the forward and backward hidden state representations : ‚àí ‚Üíst = LSTM ( ot,‚àí‚àí‚Üíst‚àí1)and‚Üê ‚àíst = LSTM ( ot,‚Üê‚àí‚àíst+1 ) , whereotis the vector representation of decoder input ( yt ) at time step t. During training time the vector representation of words from the ground - truth question is fed as decoder input , and during test time the vector representation of the vocabulary word with maximum probability is fed as input . 
We feed EOS symbol as input to decoder from both forward and backward dircetion at time t0 . 
Bidirectional decoder factorizes the conditional decoding probabilities in both directions ( left - to - right and right - to - left ) into summation as : P / parenleftBig yt|[ym]m / negationslash = t / parenrightBig = ‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚Üí logp / parenleftbig yt|Y[1 : t‚àí1]/parenrightbig + ‚Üê‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí logP / parenleftbig yt|Y[t+1 : Ty]/parenrightbig(2 ) The probability distribution over words in the vocabulary is calculated as : Pr(qt)=softmax ( WgœÉ(Ws[st , ht]+bs)+bg ) ( 3 ) where Wg , Ws , bsandbgare trainable model parameters . 
Probability distribution P(qt)uses the standard softmax over the question vocabulary VQ . 
This is used to sample word with maximum probability while decoding a question.2.4 Copy attention We know that a good question should be relevant to ( answerable from ) the paragraph . 
So we learn a probabilistic mixture model over the question vocabularyVQand the current paragraph vocabulary VP . 
The current paragraph vocabulary is generated by a dynamic dictionary module . 
Our copy attention calculates two values : cs : a binary - valued variable which acts a switch between copying a word from the paragraph ‚Äôs dynamic vocabulary VPor generating from the question vocabulary VQ Pr / parenleftbig .|VP / parenrightbig : probability of copying a particular word from paragraph vocabulary VP . 
Therefore , the Ô¨Ånal probability distribution from which a word will be sampled while generating a question is calculated over the extended vocabulary VQ‚à™Vp . 
Given a word from the extended vocabularyw‚ààVQ‚à™VP , its probability Pr(w)is computed as : Pr(w)=Pr(cs=1)Pr / parenleftbig w|VP / parenrightbig + Pr(cs=0)Pr / parenleftbig w|VQ / parenrightbig ( 4 ) The switch probability Pr(cs)is determined using the decoder hidden states as : Pr(cs=1)=œÉ(Wcsst+bcs ) ( 5 ) whereWcsandbcsare trainable model parameters . 
Pr / parenleftbig w|VQ / parenrightbig is the probability of predicting a word from complete vocabulary VQ . 
The copy attention weightatis computed as : et i = vTtanh ( Whhi+Wsst+battn ) ( 6 ) at = sparsemax ( et ) ( 7 ) Where v , Wh , Wsandbattnare trainable model parameters . 
The probability of copying a word from the paragraph vocabulary VPis estimated as : Pr / parenleftbig w|VP / parenrightbig = œÉ(Waat+ba ) ( 8) whereWaandbaare trainable model parameters.783 Model BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE - L MPGSN ( Zhao et al . 
, 2018 ) 45.07 29.58 21.60 16.38 20.25 44.48 L2A ( Du et al . 
, 2017 ) 42.54 25.33 16.98 11.86 16.28 39.37 NQG dd[w / o copy attention ] 55.32 32.39 20.12 12.86 17.00 42.77 NQG dd[with copy attention ] 61.84 41.73 30.19 22.62 21.93 48.60 Table 1 : Results on the test set on automatic evaluation metrics . 
Best results for each metric ( column ) are bolded . 
3 Experimental Setup We report the experimental result of our model ( referred to as NQG dd ) and compare it with the current state of the art MPGSN ( Zhao et al . 
, 2018 ) . 
We employ the widely - used metrics BLEU ( Papineni et al . 
, 2002 ) , ROUGE - L and METEOR for automatic evaluation . 
We use evaluation script provided by ( Chen et al . 
, 2015 ) . 
Similar to ( Kumar et al . 
, 2018a ) we also report qualitative assessment on the syntax , semantics and relevance of the questions generated by our model . 
All experiments are performed on the SQuAD dataset ( Rajpurkar et al . 
, 2016 ) , where complete paragraphs are taken as input instead of just one or two sentences . 
We reformat the SQuAD dataset such that during training time , each source instance is a ( paragraph , question ) pair annotated with the gold answers , and the target is a question . 
Following the exact setup from MPGSN ( Zhao et al . 
, 2018 ) , we split the SQuAD train set into train and validation set containing 77,526 and 9,995 instances respectively , and take the separate SQuAD dev set containing 10,556 instances as our test set . 
4 Results and Analysis Table 1 summarizes results of the automatic evaluation of the test set . 
As can be seen , our model signiÔ¨Åcantly outperforms the state - of - the - art MPGSN on all metrics . 
The improvements on BLEU are especially substantial , the BLEU-4 score of MPGSN is 16.38 , and ours ( with copy incorporated ) is 22.62 , an improvement of 6.24 , or 38 % . 
This large performance difference demonstrates the effectiveness of our dynamic dictionary . 
In Table 2 we present human evaluation results . 
We evaluate the quality of questions generated in terms on syntactic correctness , semantic correctness andrelevance to the paragraph . 
The evaluation is performed on a randomly selected subset of 100 sentences from the test set . 
Each of the three evaluators are presented the 100 paragraph - question pairs for two variants of our model ( with and without copy ) and asked for a binary responses for all threeparameters . 
We averaged responses received by all three evaluators to compute the Ô¨Ånal scores . 
As can be seen , the incorporation of the copy attention improves performance , especially on relevance . 
We also measure the inter - rater agreement using Randolph ‚Äôs free - marginal multirater kappa ( Randolph , 2005 ) . 
It can be observed that our quality metrics for both our models are rated as substantial agreement ( Viera et al . 
, 2005 ) . 
To explain how our model attends to different words in the source paragraph we visualize attention weights in Figure 3 , which shows attention weights between question 2 generated by our model and the corresponding paragraph in Figure 1 . 
We observe that the attention weight is high for words near the answer and the model attends to all relevant context rather that just the sentence containing the answer . 
ModelSyntax Semantics Relevance Score Kappa Score Kappa Score Kappa NQG dd[w / o copy ] 89 0.68 83 0.69 43 0.67 NQG dd[with copy ] 94 0.64 82 0.68 71 0.73 Table 2 : Human evaluation results ( columns ‚Äú Score ‚Äù ) as well as inter - rater agreement ( columns ‚Äú Kappa ‚Äù ) for each of our two models on 100 questions from the test set . 
The scores are between 0 ( worst ) and 100 ( best ) . 
Best results for each metric ( column ) are in bold . 
We also note that our training is faster atleast by a factor of 2 . 
We expected this since we replace a slightly expensive self - attention mechanism in the decoder of ( Zhao et al . 
, 2018 ) with a simpler dynamic dictionary and reusable copy attention . 
5 Conclusion Paragraph - level question generation ( QG ) is an important but challenging problem , mainly due to the challenge in effectively handling a longer context . 
We present a simple yet effective approach for automatic question generation from paragraphs . 
Besides using a standard global source dictionary , our RNN - based model incorporates a dynamic , paragraph - speciÔ¨Åc dictionary , and learns to switch between copying from the combined784 vocabulary and generating a new word . 
Through our experiments , we demonstrate how our model outperforms the current state - of - the - art model in paragraph - level QG by a wide margin , for example by 6.24 BLEU-4 points , a 38 % improvement . 
Abstract Adversarial attacks are label - preserving modiÔ¨Åcations to inputs of machine learning classiÔ¨Åers designed to fool machines but not humans . 
Natural Language Processing ( NLP ) has mostly focused on high - level attack scenarios such as paraphrasing input texts . 
We argue that these are less realistic in typical application scenarios such as in social media , and instead focus on low - level attacks on the character - level . 
Guided by human cognitive abilities and human robustness , we propose the Ô¨Årst large - scale catalogue and benchmark of low - level adversarial attacks , which we dub Z¬¥eroe , encompassing nine different attack modes including visual and phonetic adversaries . 
We show that RoBERTa , NLP ‚Äôs current workhorse , fails on our attacks . 
Our dataset provides a benchmark for testing robustness of future more human - like NLP models . 
1 Introduction Adversarial examples are label - preserving modiÔ¨Åcations to inputs of machine learning architectures . 
Their typical characteristic is that they cause little damage to humans but may maximally affect classiÔ¨Åer performance , exposing their weaknesses and outlining the differences between human and machine text processing ( Szegedy et al . 
, 2014 ; Goodfellow et al . 
, 2014 ; Eger et al . 
, 2019 ) . 
While in computer vision , pixel - level attacks , which go unnoticed by humans , may lead to catastrophic failure , attacks in NLP are more challenging . 
Some attacks in NLP replace individual words by synonyms or hyponyms ( Alzantot et al . 
, 2018 ) or paraphrase whole sentences ( Ribeiro et al . 
, 2018 ) . 
However , such high - level attacks are not only more difÔ¨Åcult to compute ( requiring available resources such as dictionaries or word embeddings ) but they are also implausible in real - world scenarios such as spamming or posting in social media , asusers would need to know the training data and/or the inner workings of the machine learning models in order to identify candidate substitutions ( or have unrestrained access to model predictions ) . 
In contrast , such users would typically use low - level attacks on characters , such as inserting placeholder symbols ( e.g. , underscores ) , mistyping words ( e.g. , Hilter forHitler ) , or using phonetically similar sounding words ( Tagg , 2011 ) to fool online detection models . 
To identify plausible such attack scenarios , human perceptual abilities play a decisive role . 
For instance , humans are guided by their senses , making them robust to , e.g. , visual and phonetic attacks . 
Other scenarios to which humans have been shown robust include the removal of vowels from words or the shufÔ¨Çing of characters while keeping the initial and Ô¨Ånal letters Ô¨Åxed ( see Section 2 ) . 
However , the varieties in which text can be perturbed is certainly far from inÔ¨Ånite , as ( ordinary ) humans , with all their cognitive constraints , still need to be able to decipher the text messages . 
In this work , we provide the Ô¨Årst large - scale catalogue for low - level ( orthographic ) attack scenarios . 
Our search is motivated by insights into human cognitive limitations and constraints and encompasses nine different attack modes ( some of which are overlapping ) ; cf . 
Table 1 . 
We then examine the robustness of RoBERTa ( Liu et al . 
, 2019 ) to our attacks , Ô¨Ånding that its performance can sometimes be severely decreased for our selection of attackers ( up to the random guessing baseline ) ; hence we call our benchmark Z¬¥eroe . 
The reason may be that our noises are not always natural , in the sense of having high support in large datasets such as CommonCrawl or Wikipedia , but they are still within the limits of cognitive abilities of ordinary humans . 
Finally , we show that under realistic conditions , standard adversarial training can restore786 Attacker Sentence inner - shufÔ¨Çe Aadrreavsil aacttks are hmarsels . 
full - shufÔ¨Çe idaAasvrler tstkaac are harmless . 
intrude A d v e r sar ial at : ta : ck : s are h}ar}m}less . 
disemvowel dvrsrl ttcks r hrmlss . 
truncate Adversaria attack are harmles . 
segment Adversarial attacksare harmless . 
typo Adverssrial attaxks are harmless . 
natural noise Adversarial attacs rae harmless . 
phonetic Advorcariel attaks are harmless . 
visual¬Ø√ÑÀôdU√ársar¬Ø ƒ±a √´at¬Øtack.s.¬Ø aRe h¬Ø√§r√Æ√èÀú es .. Table 1 : Ten different modiÔ¨Åcations of the sentence ‚Äú Adversarial attacks are harmless . 
‚Äù RoBERTa ‚Äôs performance only to a limited degree.1 2 Related Work We classify adversarial attacks into high- andlowlevel attacks.2 Attack Scenarios . 
There are a variety of works that introduce low - level orthographic attacks.3 Ebrahimi et al . 
( 2017 ) trick a character - level neural text classiÔ¨Åcation model by Ô¨Çipping the characters which cause most damage . 
Their approach is whitebox , i.e. , assumes access to the attack model ‚Äôs parameters . 
Eger et al . 
( 2019 ) exchange characters with similar looking ones and show that humans are robust to such visual perturbations , while machines may suffer severe performance drops . 
Belinkov and Bisk ( 2017 ) exchange adjacent letters on the keyboard with each other ( keyboard typos ) and introduce natural noise based on human typing errors extracted from different Wikipedia edit histories , as well as letter swaps . 
They use this natural and 1Code and data are provided at https://github . 
com / yannikbenz / zeroe . 
2As one reviewer points out , a conceptual difference between high- and low - level attacks is that low - level attacks ( as we deÔ¨Åne them ) oftentimes induce linguistically corrupt text which can still be understood by humans , while highlevel attacks operate in a noise - free environment to show the brittleness of systems even under ‚Äò normal ‚Äô circumstances . 
3Low - level adversarial attacks are in part examined by approaches to handle noisy user - generated text ( Baldwin et al . 
, 2015 ) , with one difference being that attacks are often malicious in nature and may thus come in different forms.synthetic noise to show the brittleness of machine translation ( MT ) systems , which contrasts with corresponding human robustness . 
Ebrahimi et al . 
( 2018 ) also fool MT systems with character - level modiÔ¨Åcations . 
Tan et al . 
( 2020 ) attack words by replacing them with morphological variants , which also mostly results in orthographic attacks ( in English ) . 
High - level attacks require a deeper understanding of the meaning and the syntactical structure of the sentence . 
Jin et al . 
( 2019 ) generate semantically similar and syntactically correct adversarial examples by replacing words with suitable synonyms . 
Hosseini et al . 
( 2017 ) and Rodriguez and RojasGaleano ( 2018 ) attack toxic detection systems by obfuscation , i.e. , misspelling of the abusive words ( a low - level attack ) , and via polarization , i.e. , inverting the meaning of the sentences by inserting the word ‚Äú not ‚Äù . 
Alzantot et al . 
( 2018 ) introduce an optimization - based algorithm to generate adversarial examples by replacing words in the input . 
Their generated words are semantically similar because they are nearest neighbors in the GloVe embedding space . 
They are also syntactically correct because they need to Ô¨Åt into the surrounding context with respect to the 1 billion words language model . 
Iyyer et al . 
( 2018 ) generate syntactically correct paraphrases for a sentence . 
Ribeiro et al . 
( 2018 ) use MT backtranslation to produce meaning - preserving adversaries . 
They generate adversarial examples for machine comprehension , sentiment analysis and visual question answering to show robustness issues in state - of - the - art models for each task . 
Jia and Liang ( 2017 ) insert semantically correct but irrelevant paragraphs into texts to fool neural reading comprehension models . 
Robustness . 
Adversarial training is a commonly used technique to address adversarial attacks ( Szegedy et al . 
, 2014 ) . 
The term may refer to calculating model gradients with respect to the input and inserting new training examples based on this gradient ( Goodfellow et al . 
, 2014 ) . 
Alternatively , adversaries obtained from the attacker are inserted at train time ( Belinkov and Bisk , 2017 ; Alzantot et al . 
, 2018 ; Eger et al . 
, 2019 ) . 
3 Catalogue of Attacks We propose a catalogue of ten different attacks . 
Our intention is to suggest a maximally inclusive list of potential attacks under the constraint that humans are robust to them.787 3.1 Attack protocol Our attack protocol is black - box andnon - targeted ( Xu et al . 
, 2019 ): we do not assume access to model parameters and our goal is to fool the system without any desired outcome in mind ‚Äî in contrast , a spammer would want spam emails to be misclassiÔ¨Åed as non - spam , but not necessarily the reverse . 
We parameterize attack levels by a perturbation probabilityp‚àà[0,1 ] . 
Withp , our goal is to attack p¬∑100 % of all tokens in each sample in our dataset . 
To do so , for each sample w= ( x1, ... ,xn ) , we randomly and without replacement draw a token indexito perturb . 
We independently Ô¨Çip a coin with tail probability pto determine whether the tokenxishould be attacked . 
We do so until either p¬∑100 % of all tokens in ware perturbed or else if there are no more indices left . 
3.2 Attacks Some of our attacks , each of which operates on the character - level of an attacked word , are parametrized by a character - level perturbation probabilityœÜ . 
For simplicity , we set œÜ = pthroughout , wherepis the above deÔ¨Åned word level perturbation probability . 
Inner ShufÔ¨Çe . 
This randomly shufÔ¨Çes all letters in a word except for the Ô¨Årst and last . 
This attacks builds on the human ability to still comprehend words if the Ô¨Årst and last letter remain intact ( Rayner et al . 
, 2006 ) . 
We only allow change in words with length‚â•3 . 
Full ShufÔ¨Çe . 
This is the extreme case of the inner - shufÔ¨Çe perturbation where the constraint relating to initial and Ô¨Ånal letters is dropped . 
We include this attack for completeness , even though we do not assume high degrees of human robustness to it . 
We apply this to all words with length ‚â•2 . 
Intruders . 
Inserting unobtrusive symbols ( Hosseini et al . 
, 2017 ) in words is a typical phenomenon in social media , e.g. , to avoid censorship . 
Depending on the symbols chosen , an attack may have little effect on humans . 
We choose the inserted symbol randomly but in case of multiple insertions into one word keep the symbol identical . 
We allow the following symbols to be inserted : ! ‚Äù # $ % & /prime()‚àó+,‚àí./:;<=>?@[\]ÀÜ‚Äò{| } , including whitespace . 
The perturbation probability œÜadditionally inÔ¨Çuences the number of insertions taking place . 
For each two characters , œÜindicates howlikely the insertion of a symbol between them is . 
We apply this attack to all words with length ‚â•3 . 
Disemvoweling . 
This removes all vowels ( a , e , i , o , u ) from a word . 
If a word only consists of vowels , it will be ignored to prevent it from being deleted . 
Words with length ‚â§3are skipped to maintain readability . 
Disemvoweling is a common feature of SMS language and on social media presumed to require little cognitive effort for humans ( Boyd et al . 
, 2010 ) . 
Truncating . 
This removes a Ô¨Åxed number of letters from the back of a word . 
We only cut the last letter from words of length ‚â•3to maintain readability . 
Predicting word endings from beginnings is considered an easy task for humans ( Elman , 1995 ) . 
Segmentation . 
This joins multiple words together into one word . 
Here , the perturbation level is the probability to merge the Ô¨Årst two adjacent words . 
Each following word gets a lower probability to get merged ( œÜ2, ... ,œÜn ) to prevent ‚Äò giant ‚Äô words . 
We do not apply this attack to sequence tagging tasks such as POS , because the joined words would have no proper tag , making evaluation more difÔ¨Åcult . 
The ability of humans to segment unsegmented input is already acquired during infancy ( Goldwater et al . 
, 2009 ) . 
Keyboard Typos . 
We adopt this attack from Belinkov and Bisk ( 2017 ) and adapt it to our workÔ¨Çow . 
Hereby , adjacent letters on the English keyboard are replaced by each other randomly . 
This simulates human typing errors . 
The higher the perturbation probability œÜ , the more characters are exchanged by adjacent letters . 
Natural Typos . 
Words are replaced by natural human errors from the Wikipedia edit history ( Belinkov and Bisk , 2017 ) which contains multiple sources of error : phonetic errors , omissions , morphological errors , key - swap errors and combinations of them . 
Phonetic . 
An ideal phonetic attack leaves the pronunication of a word intact but alters its spelling . 
Phonetic attacks are common especially in English with its irregular mapping of pronunciation and spelling . 
They do not only occur as mistakes but also as a form of creative language use ( Tagg , 2011 ) . 
Visual . 
Visual attacks are based on the idea that humans may easily recognize similar looking sym-788 bols ( Eger et al . 
, 2019 ) . 
We replace each character in the input sequence with one of its 20 visual nearest neighbors in the visual space deÔ¨Åned below . 
This attack is also parameterized by œÜ : we replace each letter in a word i.i.d . 
randomly with probabilityœÜ . 
We observe that our attacks are notdirectly comparable . 
For example , at some perturbation level p , truncate removes O(p¬∑n)characters , where n is sentence length . 
In contrast , intruders inserts O(p2¬∑n¬∑m)characters , where mis a bound on word length . 
3.3 Implementation of Visual and Phonetic Attacks We describe details of phonetic and visual attacks below , as they are more involved . 
Phonetic Embeddings and Attacks . 
In order to replace words by phonetically similar ones , we use two stages . 
First , we train two Seq2Seq models to translate a letter string into its phonetic representation and vice versa . 
We use the Combilex dataset to do so ( Richmond et al . 
, 2010 ) . 
In addition to that , we induce phonetic word representations , i.e. , a vector space where two words are close if they are pronounced alike . 
We use an InferSent - like architecture to do so ( Conneau et al . 
, 2017 ) . 
Details are given in the appendix . 
When a word x should be phonetically perturbed , we run the Ô¨Årst Seq2Seq model to obtain a phonemic representation and then convert this back to a letter string Àúx ( as in backtranslation in MT ) . 
We Ô¨Ånally keep Àúx when it is phonetically similar to x. We added the latter step because we observed that some resulting words Àúxhad very different pronunciation than x after the backtranslation . 
Visual embeddings . 
In order to generate visual character embeddings , we used an architecture introduced by Larsen et al . 
( 2016 ) as a combination of GAN and V AE , called V AEGAN . 
The model is able to learn embeddings which encode highlevel abstract features . 
This property is desirable in our case , because humans rely on abstract features ( Dehaene and Cohen , 2011 ) , i.e. , shape and spatial relation of the letter , instead of pixels while reading . 
The model is described in the appendix . 
To obtain visual character embeddings , we generate a grayscale image of size 24√ó24for each character in the Basic Multilingual Plane ( BMP ; 65k characters ) of the standard Unicode character set with Pillow . 
The V AEGAN is trained onthe full BMP dataset . 
After training , we compute 256 - dimensional visual letter embeddings by encoding the respective letter image with the encoder of the V AEGAN . 
The quality of the embeddings can be derived via the models ‚Äô ability to properly reconstruct an image from them , see Figure 7 in the appendix . 
4 Experimental Setup 4.1 Base model and datasets Our base architecture used in all experiments is RoBERTa ( Liu et al . 
, 2019 ) . 
RoBERTa is a robustly optimized extension of BERT that has been trained ( i ) for longer , ( ii ) on more data , and ( iii ) without the next sentence prediction task . 
RoBERTa has been shown to outperform BERT on a variety of benchmark tasks , including those contained in GLUE ( Wang et al . 
, 2018 ) . 
We study the performance of RoBERTa in our attack scenarios on three different NLP tasks . 
Dataset statistics are shown in Table 2 . 
POS tagging is a sequence tagging task where each token in the input needs to be labeled with its respective POS tag . 
We use the English universal dependency dataset with 17 different tags ( Nivre et al . 
, 2016 ) . 
NLI is a classiÔ¨Åcation task in which the relation of a sentence pair must be predicted . 
Relation labels are neutral , contradiction andentailment . 
We use SNLI ( Bowman et al . 
, 2015 ) . 
Toxic Comment ClassiÔ¨Åcation ( TC ) labels sentences ( typically from social media platforms ) with one or several toxicity classes . 
Possible labels are : toxic , obscene , threat , insult andidentity hate . 
For this task , we choose the jigsaw toxic comment challenge dataset from kaggle4 . 
The current best performance on the leaderboard has an AUCROC ( area under the receiver operations characteristic curve ) score of 98.8 % . 
4.2 Results We consider the cases of low(p= 0.2),mid(p= 0.5 ) and high ( p= 0.8 ) attack levels . 
In Figure 1 , we plot the performance of RoBERTa for the three tasks POS , NLI and TC individually as we perturb the test data using our attackers . 
Detailed numbers are reported in Table 6 4https://www.kaggle.com/c/jigsaw-toxic-commentclassiÔ¨Åcation-challenge789 Task Dataset Train Test Clean score POS Tagging Universal Dependencies ( part ) 13k 2k 96.95 NLI Stanford Natural Language Inference 550k 10k 90.41 Multilabel Toxic Comment 560k 234k 0.93 ClassiÔ¨Åcation Table 2 : Overview of the NLP tasks used in this work . 
Clean scores are scores from training and testing on clean data . 
none low mid high00.10.20.30.40.50.60.70.80.91 Attack Levels‚àó(p)POS none low mid high00.10.20.30.40.50.60.70.80.91 Attack Levels‚àó(p)NLI none low mid high0.50.60.70.80.91 Attack Levels‚àó(p)TCFull - ShuÔ¨Ñe Inner - ShuÔ¨Ñe Intrude Disemvowel Truncate Segment Keyboard - Typo Natural - Noise Phonetic Visual Figure 1 : Performance decreases of RoBERTa on the three downstream tasks : POS , SNLI an TC . 
Red lines indicate the random guessing baseline . 
in the appendix . 
We report scores relative to the model performances on the clean test set : s‚àó(p ) = s(p ) s(0 ) , p‚àà{0,0.2,0.5,0.8}(1 ) wheres(0)is the task speciÔ¨Åc performance on clean data listed in Table 2 and s(p)is the performance for attack level p. Scores are measured in accuracy for POS and NLI , and in AUCROC for TC classiÔ¨Åcation . 
Clean performance scores depend on the speciÔ¨Åc task and dataset . 
For example , NLI has a worst score of around 33 % accuracy ( majority label ) and POS has a corresponding worst score of around 16 % accuracy . 
The worst performance of TC is reached at AUCROC score of 50%‚Äîat this point , the model is no longer able to distinguish between the different classes . 
We mark these values relative to the tasks ‚Äô best performance ( s(0 ) ) in Figure 1 as red lines . 
Each task suffers performance decreases from each attacker . 
The higher the perturbation level , the lower the model performance . 
Thephonetic attack is the least effective for all tasks with maximally 10 percentage points ( pp ) performance decrease with the highest perturbation probability of 0.8 . 
The truncate attack yields higher performances decreases in all three tasks , being roughly twice as effective . 
The performance decreases by 10pp from none tolowand additional 10pp from lowtomid . 
Increasing the attack level beyond that does not cause further harm , especially for NLI and TC . 
Concerning the segmentation attack , for NLI , it leads to a similar performance decrease as the truncate attack for smallp , but becomes more successful as the perturbation level increases to midandhigh . 
For TC , the performance decrease is almost identical to the phonetic attack . 
We notice a linear decrease in performance for each task when increasing the perturbation level of the natural - noise attack . 
Especially POS and NLI suffer a strong performance deterioration of around 40pp and 50pp for the highest attack level . 
Both lose 15pp to 20pp performance per attack level increase . 
Full- andinner - shufÔ¨Çe randomize the order in an input word but humans are more robust to inner - shufÔ¨Çe . 
Full - shufÔ¨Çe also affects RoBERTa more than inner - shufÔ¨Çe . 
It tends to be one of the strongest attack scenarios , while inner - shufÔ¨Çe typically ranks in the midÔ¨Åeld . 
Thedisemvowel attack has different effects in different tasks . 
For POS , it is almost identical to the natural - noise attack with a slightly stronger impact of 5pp for midand 3pp for high and a maxi-790 mum on 50pp . 
NLI loses around 20pp performance onlowand it decreases an additional 20pp by increasing the level to mid , and reaches its greatest decrease by 55pp on high . 
In TC , model performances decrease linearly from none tolowandmid by 8pp each . 
The high attack level doubles to a total of 15pp performance loss . 
The keyboard - typo attacks have median impact throughout tasks and attack levels . 
Theintrude attack is among the most severe attacks across all three tasks . 
For TC , the lowand midattack levels have a relatively low impact compared to high which yields a performance loss of 30pp . 
It decreases model performance the most on the POS task by above 80pp . 
Especially for both sentence - based tasks NLI and TC , the visual attack decreases are also among the most severe , while RoBERTa is marginally more robust on the POS task . 
Even for the lowperturbation level , the NLI model suffers from more than 40pp performance decrease . 
The performance for high peven falls below the red line marked as our lower bound baseline . 
4.3 Defenses In the following , we report the performance increase from shielding the methods with adversarial training : ‚àÜœÑ(p ) : = œÉ(p ) s(0)‚àís‚àó(p ) ( 2 ) whereœÉ(p)is the score for each task with one of two defense methods œÑ : ‚Ä¢1 - 1 adversarial training ( Œ± , Œ≤ ): Here , we train on a mixture of low , mid , high attacked data ( each perturbation level is roughly equally likely to appear in the training data ) . 
We attack with some attacker Œ±and measure performance when the test data is attacked with attacker Œ≤ . 
‚Ä¢leave - one - out ( LOO ): Here , we train on a mix of all attackers except for the one with which the test data is attacked . 
The train data contains an equal mix of data from each attacker and attack level . 
4.3.1 Adversarial Training 1 - 1 ( Œ± , Œ±)In Figure 2 , we report the performance of our models each trained on perturbed data and evaluated against the same kind of perturbation . 
This gives an unrealistic upper bound since the defender would have to know how it is being attacked . 
For POS , the adversarially trained models lose a bit of their performance on clean data , but their performance on perturbed data improves , especially against intrude and truncate for the lowattack level . 
The robustness improvements for the remaining attackers are very similar and range from 3pp increase for the natural - noise attack to 8pp for the disemvowel attack . 
With one exception , the improvement at large perturbation levels pis highest , and obtains a maximum improvement of 40pp for inner - shufÔ¨Çe . 
For NLI , the models again tend perform worse on clean data . 
As the perturbation level increases , we see a smooth and steady increase of the values ‚àÜœÑ(p)across all attackers . 
Improvement is best for intrude which was also among the most damaging attacks . 
For TC , model performances increase also on clean data , which is likely due to the nature of the task . 
As the attack level increases , ‚àÜœÑ(p)gradually further increases across tasks . 
For high , largest increase is again observed for intrude as well as for visual , which also had largest impact in the non - shielded setting . 
1 - 1 ( Œ± , Œ≤)In Figure 3 , we show all 1 - 1 values for different combination of attackers on train ( Œ± ) and test data ( Œ≤ ) . 
We see that the diagonal ( Œ± = Œ≤ ) always proÔ¨Åts considerably , but the off - diagonal can be positive or negative , depending on the choice ofŒ±andŒ≤ . 
We clearly see that ( 1 ) truncate , disemvowel , keyboard - typo , natural noise , visual , and intruders are similar in the sense that training on them shields against their attacks at test time . 
( 2 ) Full - shufÔ¨Çe and inner - shufÔ¨Çe form a second group and ( 3 ) phonetic attacks a third group . 
This is to some degree a natural clustering , as ( 1 ) removes or replaces characters , ( 2 ) destroys the order of words , and ( 3 ) modiÔ¨Åes entire words using more complex operations . 
visual is an outlier in group ( 1 ) , since it improves no matter what attacks are added at train time . 
Leave - One - Out Figure 4 shows the performance of our models when trained on a mixture of all attackers except the one evaluated on . 
This is the most plausible scenario of model defense in the case of an unknown new attack scenario at test time . 
For POS , the performance against the phonetic attack remains mostly unchanged , while ‚àÜœÑ(p)increases as a function of pagainst natural - noise,791 none low mid high‚àí0.100.10.20.30.40.5 Attack Level‚àÜ1‚àí1s‚àó(p)POS none low mid high‚àí0.100.10.20.30.40.50.60.70.8 Attack Level‚àÜ1‚àí1s‚àó(p)NLI none low mid high‚àí0.100.10.20.30.40.5 Attack Level‚àÜ1‚àí1s‚àó(p)TCFull - ShuÔ¨Ñe Inner - ShuÔ¨Ñe Intrude Disemvowel Truncate Segment Keyboard - Typo Natural - Noise Phonetic VisualFigure 2 : Performance improvements of the models adversarial trained and evaluated individually on the attacker introduced in Section 3 for POS left , NLI mid and TC right . 
Performance measured in ‚àÜœÑ(p)deÔ¨Åned in Eq . 
2 . 
FS IS INT DIS TRUN KEY NAT PH VIS FS low 6.35 -0.46 -2.13 0.49 -0.29 -0.69 -1.21 0.02 0.96 high 17.59 -0.1 -4.1 4.03 1.81 -0.45 1.48 -1.23 2.24 mid 21.8 0.48 -4.36 7.59 4.37 0.11 3.93 -1.68 1.97 IS low 2.01 6.02 0.1 0.72 -0.31 1.13 -0.33 0.14 1.74 mid 3.7 16.71 0.89 4.24 2.4 3.53 0.41 0.08 4.08 high 3.77 18.29 1.25 5.25 2.52 3.89 0.59 -0.28 4.78 INT low -2.19 -1.04 11.3 0.4 0.63 5.46 0.85 -2.16 5.3 mid -7.79 -4.89 41.5 10.77 7.26 12.38 6.71 -2.54 13.93 high -7.69 -6.22 62.81 9.8 6.59 11.4 29.56 -2.27 4.3 DIS low -2.95 -2.79 -0.56 8.07 0.67 0.19 0.48 -0.97 0.35 mid -4.75 -1.42 1.44 27.73 6.05 5.18 5.68 -1.17 3.27 high -4.39 0.42 2.76 41.44 9.2 8.68 9.39 -1.82 4.62 TRUN low -0.4 -1.07 -0.23 0.4 6.01 0.88 1.46 0.06 0.23 mid -0.22 -1.38 0.93 2.18 15.87 3.18 4.93 -0.61 1.58 high -0.2 -1.27 1.17 2.79 17.88 3.53 5.73 -0.78 1.72 KEY low -1.36 -1.9 0.11 0.3 0.24 5.05 0.91 -0.95 1.93 mid -1.89 -1.42 1.4 2.82 1.66 11.04 4.65 -1.79 4.12 high -4.07 -2.49 3.96 3.26 4.65 22 6.72 -3.27 5.94 NAT low -1.42 -2.77 -0.13 -0.21 -0.6 0.49 3.18 -0.93 1.14 mid -0.31 -2.86 0.47 2.77 1.03 2.61 15.62 -0.77 2.15 high -2.23 -3.05 2.33 3.46 4.04 4.2 16.7 -1.27 3.19 PH low -1.8 -1.96 -1.73 -1.22 -1.41 -1.76 -0.81 4.27 -1.15 mid -2.41 -2.18 -1.97 -1.45 -1.73 -2.16 -1.02 5.3 -1.42 high -2.21 -2.16 -1.95 -1.44 -1.69 -2.1 -0.95 5.41 -1.39 VIS low 1.55 1.61 2.58 2.12 2.09 3.69 1.74 0.68 7.43 mid 5.01 6.65 7.91 9.13 7.41 9.92 5.09 1.18 18.44 high 1.46 3.62 -0.25 7.87 6.6 7.99 3.81 1.22 8.94 Figure 3 : 1 - 1 ( Œ± , Œ≤)adversarial training for POS . 
Column : train , row : test . 
Numbers give values ‚àÜœÑ(p ) , see Eq . 
( 2 ) . 
Red colors give performance decreases , relative to the results on clean data ; blue colors show increases . 
inner - shufÔ¨Çe , full - shufÔ¨Çe , truncate and keyboardtypo . 
The best defense is against natural - noise with 3pp for lowand 7pp for mid andhigh . 
Shielding against visual , intrude and disemvowel attacks yields lower values ‚àÜœÑ(p)on attack level high compared to mid . 
Overall , we see mild improvements compared to the unshielded situation , but expectedly , these are lower than for 1 - 1 shielding . 
For NLI , the performance against keyboard - typo , full - shufÔ¨Çe , inner - shufÔ¨Çe , natural - noise and truncate exhibits steady improvements with increasing attack level which range from 10pp to 20pp for attack level midandhigh . 
The performances against intrude and disemvowel also show steady improvements with the attack levels but are generally higher with up to 29pp . 
For attack level low , the perfor - mance improvement against the visual attacker is with 20pp more than twice the value of the others . 
This improvement diminishes in the midand high attack levels and even drops there below the improvements against most of the other attackers . 
In the TC task , the performance against visual improves even for lowlevel to 8pp , increases for midto 23pp and maximizes to 29pp total improvement for attack level high . 
The performance against the intrude attack is also very good : for lowattack level the improvement ( 11pp ) is even higher compared to visual ( 8pp ) . 
The performances against full - shufÔ¨Çe , inner - shufÔ¨Çe , disemvowel , segment , keyboard - typo , natural - noise and phonetic behave similar for attack level lowandmidwith 4pp to 7pp total improvement . 
Shielding against full - swap and792 none low mid high‚àí0.100.10.20.3 Attack Level‚àÜLOOs‚àó(p)POS none low mid high‚àí0.100.10.20.3 Attack Level‚àÜLOOs‚àó(p)NLI none low mid high‚àí0.100.10.20.3 Attack Level‚àÜLOOs‚àó(p)TCFull - ShuÔ¨Ñe Inner - ShuÔ¨Ñe Intrude Disemvowel Truncate Segment Keyboard - Typo Natural - Noise Phonetic VisualFigure 4 : Leave - one - out defense : Performance improvements of the models adversarial trained on all attackers introduced in Section 3 except the one they are evaluated on for POS left , NLI mid and TC right . 
Performance measured in ‚àÜœÑ(p)deÔ¨Åned in equation 2 . 
disemvowel is slightly better than the last group . 
There is no overall positive effect for truncate . 
4.4 Discussion Overall , the phonetic attack was least effective . 
We assume this is because few words were changed overall as a considerable amount of phonetic replacements were either identical to the input and some were even discarded . 
The truncate attack performed better than the phonetic attack in all three tasks but it still remained low overall , possibly as we truncated only by 1 character , leading to small changes in the appearance of a word . 
We attribute the low impact of the segmentation attack to RoBERTa ‚Äôs BPE encoding , which apparently allows it to partly de - segment unsegmented input . 
We observe that some attacks ( e.g. , segmentation , keyboard - typo , and natural - noise ) have less effect in TC compared to POS and NLI , possibly because of higher natural occurrences of these phenomena in the TC dataset . 
The intrude and visual attacks are among the strongest . 
This is not only because they are doubly parametrized unlike many others ‚Äî i.e. , for high attacks , not only the majority of words is attacked but also the majority of characters within a word ‚Äî since they are also effective at lowattack levels . 
We partly attribute their success to the fact that they cause a high out - of - vocabulary rate for RoBERTa and tend to increase the number of input tokens , as they cause RoBERTa to segment the input at unknown characters . 
This may lead to the number of input tokens exceeding RoBERTa ‚Äôs builti - in max token size , leading to cutting off the ending of the sentence . 
Rank POS NLI TC 1 1 - 1 ( Œ± , Œ± ) 16 1 - 1 20 1 - 1 12 2 LOO 4 LOO 10 LOO 9 3 1 - 1 ( Œ± , Œ≤ ) 1 1 - 1 3 1 - 1 7 Table 3 : Different defense approaches ranked by the average robustness improvement over all attackers . 
Improvement in percentage points ( pp ; rounded ) . 
In Table 5 ( appendix ) , attacks are ranked ( for high attack level ) by the performance degradation caused to the model for each individual task . 
In line with our previous discussion , the visual and the intrude attackers are always the both best performing , followed by full - shufÔ¨Çe ( which we deemed as unrealistic as it would also destroy human perception abilities ) . 
Figure 5 shows the relationship between the amount of text perturbed in a test dataset and the performance deterioration a model suffers . 
This shows a clear ( linear ) trend and indicates that a successful attacker most importantly needs to attack many characters of a text to be effective , despite all individual qualitative differences between the attackers discussed above . 
In Table 3 , a ranking of defense strategies is given . 
1 - 1 ( Œ± , Œ± ) performs best , but is unrealistic . 
LOO is a robust alternative for unknown new attacks . 
The effectiveness of LOO as defense is also a further justiÔ¨Åcation for designing multiple attack models . 
5 Conclusion We provided the Ô¨Årst large - scale catalogue for lowlevel adversarial attacks , providing a new simple benchmark for testing real - world robustness of future deep learning models . 
We further showed793 0 10 20 30 40 500.20.40.60.81 avg . 
levenshtein distances‚àó(p)Figure 5 : Relation between the amount of text perturbed ( measured in edit distance ) in a test data set and s‚àó(p ) , the performance decrease a model suffers . 
that one of the currently most successful deep learning paradigms , RoBERTa , is not robust to our benchmark , sometimes suffering catastrophic failure . 
While many of our errors could probably be addressed by placing a correction layer in front of RoBERTa ( Choudhury et al . 
, 2007 ; Pruthi et al . 
, 2019 ) , we believe that our Ô¨Åndings shed further light on the differences between human and machine text processing , which deep models eventually will have to innately overcome for true AI to become a viable prospect . 
Acknowledgments We thank the anonymous reviewers for their useful comments and suggestions . 
Steffen Eger has been funded by the HMWK ( Hessisches Ministerium f¬®ur Wissenschaft und Kunst ) as part of structural location promotion for TU Darmstadt in the context of the Hessian excellence cluster initiative ‚Äú Content Analytics for the Social Good ‚Äù ( CA - SG ) . 
A Appendices A.1 Phonetic and visual embeddings Phonetic Word Embeddings . 
To induce phonetic word embeddings , we adopt the Siamese network of InferSent ( Conneau et al . 
, 2017 ) . 
InferSent was originally designed to induce vector representations for two sentences from which their entailment relation was inferred . 
We adapt InferSent to encode two words so that their phonological similarity can be inferred : identical , very similar , similar anddifferent . 
We use the BiLSTM max - pooling approach from the original InferSent paper , where we set the induced phonetic embeddings size to 100 . 
We build our own dataset for phonetic similarity by leveraging data from different sources . 
Initially , we use Combilex ( Richmond et al . 
, 2010 ) , which gives phonetic representations for standard ( American ) English words . 
We calculate the normalized edit distance between the phonemes of each word pair to determine the phonetic similarity of two words : sim ph(œÄ1,œÄ2 ) = 1‚àíd(œÄ1,œÄ2 ) min(|œÄ1|,|œÄ2|)(3 ) whereœÄiare phonetic sequences for underlying words anddis the edit - distance . 
We then map the words into 4 different classes : identical ( sim ph= 0),very similar ( 0 < sim ph<0.1),similar ( 0.1 < sim ph<0.3 ) and different ( 0.3 < sim ph ) . 
To keep the training data for each class more balanced , we added handcrafted and crawled samples , e.g. , homophones . 
We also wanted to include ‚Äú internet slang ‚Äù style phonetic replacements like in Table 4 . 
We therefore crawled them and added them to the bins identical andvery similar based upon manual inspection . 
Overall , we compiled 5k examples for each of our four labels . 
The similaranddifferent bins consist only of data from Combilex , whereas the identical andvery similar bin contains 1.3k samples from Combilex and 3.7k crawled samples . 
References for crawled sites are given in A.2 . 
Visual Embeddings . 
The model reduces the dimension of input x , e.g. , an image , by applyingmultiple convolutional steps in the encoder to compute the latent representation zofx . 
Afterwards , it reconstructs the original input xin the decoder by applying multiple deconvolutional steps to z. This reconstructed version of xis called Àúx . 
Additionally , a second input zpsampled fromN(0,I)is inserted into the generator to obtainxp . 
Decoder andgenerator perform the same task on different inputs ; they can be considered as identical and therefore share their parameters . 
The discriminator takes x,Àúxandxpas inputs and discriminates which input is a real training sample and which is a fake . 
Figure 6 illustrates the working of the architecture . 
xz encode Àúxdecodezp xpgenerate discriminatortrue fake Figure 6 : Schematic representation of Variational Autoencoder Generative Adversarial Network ( V AEGAN ) taken and adapted from Larsen et al . 
( 2016 ) . 
zcan be decomposed as z=¬µ+œÉand is used to sample zp=¬µ+œÉ / epsilon1where / epsilon1is noise deÔ¨Åned as /epsilon1‚àº N ( 0,I ) Figure 7 : Reconstruction of images after being compressed to its latent representation and decompressed back to the original data distribution . 
Figure 8 gives an impression of the encoded visual similarity . 
A.2 Homophone resources List of used resources to gather homophones . 
‚Ä¢https://7esl.com / homonyms/ ‚Ä¢https://www.englishclub.com/ pronunciation / homophones - list.html ‚Ä¢https://www.thoughtco.com/ homonyms - homophones - and - homographs - a - b-1692660 ‚Ä¢http://www.singularis.ltd.uk/ bifroest / misc / homophones - list.html796 Ranking POS NLI TC 1 Intrude Visual & Intrude Visual 2 Visual - Intrude 3 Full - ShufÔ¨Çe Full - ShufÔ¨Çe Full - ShufÔ¨Çe 4 Keyboard - Typo Disemvowel Disemvowel 5 Disemvowel Keyboard - Typo Inner - ShufÔ¨Çe 6 Natural - Noise Inner - ShufÔ¨Çe Truncate 7 Inner - ShufÔ¨Çe Natural - Noise Keyboard - Typo 8 Truncate Segment Natural - Noise 9 Phonetic Truncate Segment 10 - Phonetic Phonetic Table 5 : Ranking on harmfulness of the attackers on POS , NLI , TC on attack level high . 
Figure 8 : tSNE plot of our character embedding space . 
As can be seen similar looking characters are clustered.797 ‚Ä¢https://web.archive.org / web/ 20160825095711/ ‚Ä¢http://people.sc.fsu.edu/ Àújburkardt/ fun / wordplay / multinyms.html ‚Ä¢http://homophonelist.com/ homophones - list/ ‚Ä¢https://web.archive.org / web/ 20160825095711/ ‚Ä¢http://homophonelist.com/ homophones - list/ ‚Ä¢https://www.webopedia.com / quick_ref/ textmessageabbreviations.asp ‚Ä¢https://www.smart - words.org/ abbreviations / text.html ‚Ä¢https://en.wiktionary.org/ wiki / Appendix : English _ dialect - independent_homophones ‚Ä¢https://en.wiktionary.org / wiki/ Appendix : English_dialect - dependent _ homophones A.3 Detailed Result Tables Hyperparameters of our models can be found in the github accompanying the publication ( https:// github.com/yannikbenz/zeroe ) . 
The following tables give detailed results of our experiments.798 Attack ModeAccuracy AUCROC POS NLI TC None - 96.65 90.41 0.93 Full - Swaplow 82.14 70.35 0.90 mid 58.14 45.70 0.83 high 40.47 38.35 0.74 Inner - Swaplow 85.96 67.70 0.90 mid 70.53 53.35 0.83 high 67.95 51.55 0.82 Intrudelow 81.42 75.97 0.91 mid 46.91 52.25 0.85 high 18.15 34.70 0.66 Disemvowellow 85.24 72.24 0.91 mid 61.50 51.62 0.86 high 44.69 41.00 0.79 Truncatelow 88.57 79.83 0.90 mid 77,40 72.87 0.84 high 75,11 72.02 0.83 Segmentlow - 86.08 0.93 mid - 77.53 0.92 high - 69.14 0.91 Keyboard - Typolow 85.06 76.93 0.92 mid 62.41 60.21 0.88 high 40.99 44.16 0.84 Natural Noiselow 85.34 78.43 0.92 mid 65.36 65.60 0.91 high 50.06 56.31 0.90 Phoneticlow 90.62 87.40 0.93 mid 89.09 84.75 0.92 high 88.95 82.80 0.91 Visuallow 80.52 53.07 0.86 mid 48.14 35.26 0.64 high 22.44 34.37 0.48 Table 6 : Attacks against unshielded model.799 TestTrainlevel FS IS INT DIS TRUN SEG KEY NAT PH VIS POS NLI TC FSnone         95.57 89.56 0.97 low 84.49 73.05 0.95 mid 63.48 57.54 0.90 high 45.72 51.73 0.86 ISnone        95.66 88.94 0.96 low 88.29 75.51 0.94 mid 75.90 69.07 0.91 high 73.68 68.54 0.90 INTnone         95.65 88.90 0.96 low 87.54 84.27 0.95 mid 57.58 74.92 0.93 high 19.44 61.07 0.84 DISnone         95.69 89.42 0.96 low 86.00 80.00 0.94 mid 64.39 70.98 0.91 high 48.65 66.60 0.89 TRUNnone         95.49 89.17 0.96 low 89.98 84.55 0.84 mid 81.23 81.97 0.83 high 79.38 81.62 0.82 SEGnone         - 89.02 0.96 low - 85.38 0.96 mid - 76.92 0.95 high - 62.83 0.95 KEYnone         95.61 88.80 0.96 low 87.71 80.47 0.95 mid 68.64 70.69 0.94 high 46.51 61.83 0.92 NATnone         95.72 88.67 0.96 low 88.17 81.27 0.96 mid 72.30 73.05 0.96 high 56.78 67.40 0.96 PHnone         95.30 88.95 0.96 low 89.74 87.54 0.96 mid 87.82 86.27 0.95 high 87.72 85.34 0.95 VISnone          95.72 89.02 0.96 low 85.18 70.77 0.93 mid 58.94 48.80 0.85 high 24.99 40.22 0.75 Table 7 : Adversarial training : leave - one - out.800 TestTrainlevel FS IS INT DIS TRUN KEY NAT PH VIS Clean - 95.17 95.18 95.04 95.44 95.48 95.40 95.40 96.06 95.66 FSlow 88.49 81.68 80.01 82.63 81.85 81.45 80.93 82.16 83.10 mid 75.73 58.04 54.04 62.17 59.95 57.69 59.62 56.91 60.38 high 62.27 40.95 36.11 48.06 44.84 40.58 44.40 38.79 42.44 ISlow 87.97 91.98 86.06 86.68 85.65 87.09 85.63 86.10 87.70 mid 74.23 87.24 71.42 74.77 72.93 74.06 70.94 70.61 74.61 high 71.72 86.24 69.20 73.20 70.47 71.84 68.54 67.67 72.73 INTlow 79.23 80.38 92.72 81.82 82.05 86.88 82.27 79.26 86.72 mid 39.12 42.02 88.41 57.68 54.17 59.29 53.62 44.37 60.84 high 10.46 11.93 80.96 27.95 24.74 29.55 47.71 15.88 22.45 DISlow 82.29 82.45 84.68 93.31 85.91 85.43 85.72 84.27 85.59 mid 56.75 60.08 62.94 89.23 67.55 66.68 67.18 60.33 64.77 high 40.30 45.11 47.45 86.13 53.89 53.37 54.08 42.87 49.31 TRUNlow 88.17 87.50 88.34 88.97 94.58 89.45 90.03 88.63 88.80 mid 77.18 76.02 78.33 79.58 93.27 80.58 82.33 76.79 78.98 high 74.91 73.84 76.28 77.90 92.99 78.64 80.84 74.33 76.83 KEYlow 83.70 83.16 85.17 85.36 85.30 90.11 85.97 84.11 86.99 mid 60.52 60.99 63.81 65.23 64.07 73.45 67.06 60.62 66.53 high 36.92 38.50 44.95 44.25 45.64 62.99 47.71 37.72 46.93 NATlow 83.92 82.57 85.21 85.13 84.74 85.83 88.52 84.41 86.48 mid 65.05 62.50 65.83 68.13 66.39 67.97 80.98 64.59 67.51 high 47.83 47.01 52.39 53.52 54.10 54.26 66.76 48.79 53.25 PHlow 88.82 88.66 88.89 89.40 89.21 88.86 89.81 94.89 89.47 mid 86.68 86.91 87.12 87.64 87.36 86.93 88.07 94.39 87.67 high 86.74 86.79 87.00 87.51 87.26 86.85 88.00 94.36 87.56 VISlow 82.07 82.13 83.10 82.64 82.61 84.21 82.26 81.20 87.95 mid 53.15 54.79 56.05 57.27 55.55 58.06 53.23 49.32 66.58 high 23.90 26.06 22.19 30.31 29.04 30.43 26.25 23.66 31.38 Table 8 : Part - of - Speech tagging adversarial training : 1 - 1.801 TestTrainlevel FS IS INT DIS TRUN SEG KEY NAT PH VIS Clean - 87.54 - 88.29 88.59 88.91 89.90 89.17 89.12 90.24 FSlow 83.04 - 64.65 63.28 60.38 62.67 69.46 68.46 66.87 mid 78.58 - 47.62 48.15 42.21 46.05 52.63 50.81 48.11 high 76.96 - 42.75 44.52 39.16 41.55 47.77 46.21 41.53 ISlow 81.31 - 71.32 66.40 59.81 63.26 72.23 72.25 66.40 mid 78.82 - 64.82 58.42 51.17 53.63 63.21 64.24 57.16 high 78.08 - 64.06 58.25 50.86 53.53 62.72 62.89 56.61 INTlow 76.22 - 85.83 72.49 72.97 72.78 83.73 80.83 74.83 mid 58.99 - 82.61 53.87 51.09 48.45 69.53 62.84 51.53 high 43.22 - 80.76 39.93 36.18 36.60 48.77 40.91 37.07 DISlow 79.14 - 76.27 86.56 67.51 72.52 78.96 77.77 72.65 mid 72.47 - 67.43 84.70 57.60 56.88 69.72 64.85 57.03 high 69.45 - 63.90 84.16 54.50 48.25 65.29 58.44 48.92 TRUNlow 81.63 - 84.31 79.66 88.15 80.02 86.35 84.79 80.46 mid 77.87 - 82.45 75.86 87.52 76.11 84.08 81.83 76.19 high 77.25 - 82.46 75.10 87.44 75.79 83.80 81.76 75.80 SEGlow 82.32 - 84.32 83.75 84.00 89.07 86.15 85.53 85.69 mid 68.85 - 76.41 75.84 77.33 87.54 80.28 79.37 78.14 high 50.94 - 64.98 68.11 71.36 86.42 73.39 73.19 71.88 KEYlow 74.23 - 76.90 71.38 69.95 73.10 86.63 81.04 74.46 mid 57.37 - 62.86 55.62 54.30 58.67 82.98 70.74 56.98 high 45.87 - 52.26 46.29 44.75 47.07 79.82 61.76 44.54 NATlow 77.87 - 78.32 73.62 73.50 75.51 82.39 87.67 76.47 mid 67.98 - 67.98 62.27 60.10 62.97 74.25 85.45 62.85 high 59.73 - 60.81 55.16 53.18 55.41 69.52 84.06 54.89 PHlow 85.68 - 85.98 84.23 85.36 86.53 87.50 87.80 89.93 mid 84.25 - 84.21 80.98 82.67 84.17 85.98 86.50 89.40 high 83.07 - 82.71 80.28 81.68 82.68 84.74 85.42 89.19 VISlow 59.79 - 72.33 55.74 56.09 56.91 70.65 66.32 55.34 mid 41.82 - 50.95 37.87 37.01 36.38 45.21 41.84 36.31 high 37.42 - 39.08 33.81 34.26 34.51 36.04 35.37 34.10 Table 9 : Natural language inference adversarial training : 1 - 1.802 TestTrainlevel FS IS INT DIS TRUN SEG KEY NAT PH VIS Clean - 0.96 0.96 0.96 0.97 0.97 0.98 0.97 0.96 0.97 0.96 FSlow 0.94 0.94 0.94 0.94 0.94 0.95 0.95 0.93 0.95 0.93 mid 0.92 0.90 0.87 0.88 0.87 0.87 0.89 0.87 0.87 0.88 high 0.90 0.86 0.80 0.81 0.79 0.79 0.83 0.80 0.77 0.83 ISlow 0.94 0.95 0.94 0.94 0.94 0.95 0.94 0.93 0.94 0.93 mid 0.92 0.94 0.89 0.90 0.89 0.89 0.91 0.88 0.89 0.89 high 0.91 0.94 0.88 0.90 0.88 0.88 0.90 0.87 0.88 0.89 INTlow 0.95 0.95 0.96 0.96 0.96 0.96 0.96 0.95 0.96 0.95 mid 0.92 0.92 0.95 0.92 0.92 0.91 0.94 0.90 0.91 0.91 high 0.81 0.80 0.91 0.80 0.76 0.75 0.81 0.76 0.72 0.83 DISlow 0.94 0.95 0.95 0.96 0.94 0.96 0.95 0.94 0.95 0.94 mid 0.91 0.91 0.91 0.96 0.90 0.90 0.92 0.90 0.89 0.90 high 0.88 0.88 0.88 0.95 0.86 0.83 0.89 0.86 0.83 0.88 TRUNlow 0.95 0.95 0.96 0.96 0.97 0.96 0.97 0.94 0.96 0.95 mid 0.94 0.94 0.95 0.94 0.97 0.95 0.96 0.93 0.95 0.94 high 0.94 0.94 0.95 0.94 0.97 0.95 0.96 0.93 0.95 0.94 SEGlow 0.96 0.96 0.96 0.96 0.97 0.97 0.97 0.96 0.97 0.95 mid 0.95 0.95 0.95 0.96 0.96 0.97 0.96 0.95 0.96 0.94 high 0.94 0.94 0.94 0.95 0.95 0.97 0.95 0.93 0.95 0.93 KEYlow 0.95 0.95 0.95 0.95 0.96 0.96 0.96 0.95 0.96 0.95 mid 0.92 0.92 0.93 0.93 0.93 0.94 0.95 0.92 0.93 0.92 high 0.88 0.88 0.91 0.89 0.90 0.89 0.95 0.89 0.87 0.90 NATlow 0.96 0.96 0.96 0.96 0.97 0.97 0.97 0.96 0.97 0.95 mid 0.95 0.95 0.96 0.96 0.96 0.97 0.97 0.96 0.96 0.95 high 0.95 0.95 0.95 0.95 0.96 0.96 0.96 0.96 0.96 0.94 PHlow 0.96 0.96 0.96 0.96 0.97 0.97 0.97 0.95 0.97 0.95 mid 0.95 0.95 0.95 0.96 0.96 0.97 0.96 0.95 0.97 0.95 high 0.95 0.95 0.95 0.95 0.96 0.96 0.96 0.94 0.97 0.94 VISlow 0.92 0.93 0.94 0.93 0.93 0.93 0.94 0.91 0.92 0.93 mid 0.82 0.81 0.86 0.80 0.78 0.77 0.83 0.76 0.71 0.90 high 0.70 0.69 0.75 0.65 0.62 0.62 0.66 0.64 0.55 0.85 Table 10 : Toxic comment adversarial training : 1 - 1.803 Proceedings of the 1st Conference of the Asia - PaciÔ¨Åc Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing , pages 804‚Äì810 December 4 - 7 , 2020 . 
¬© 2020 Association for Computational Linguistics Point - of - Interest Type Inference from Social Media Text Danae S ¬¥ anchez VillegasŒ±Daniel Preot ¬∏iuc - PietroŒ≤Nikolaos AletrasŒ± Œ±Computer Science Department , University of ShefÔ¨Åeld , UK Œ≤Bloomberg { dsanchezvillegas1 , n.aletras } @sheffield.ac.uk dpreotiucpie@bloomberg.net Abstract Physical places help shape how we perceive the experiences we have there . 
We study the relationship between social media text and the type of the place from where it was posted , whether a park , restaurant , or someplace else . 
To facilitate this , we introduce a novel data set of‚àº200,000 English tweets published from 2,761 different points - of - interest in the U.S. , enriched with place type information . 
We train classiÔ¨Åers to predict the type of the location a tweet was sent from that reach a macro F1 of 43.67 across eight classes and uncover the linguistic markers associated with each type of place . 
The ability to predict semantic place information from a tweet has applications in recommendation systems , personalization services and cultural geography.1 1 Introduction Social networks such as Twitter allow users to share information about different aspects of their lives including feelings and experiences from places that they visit , from local restaurants to sport stadiums and parks . 
Feelings and emotions triggered by performing an activity or living an experience in a Point - of - Interest ( POI ) can give a glimpse of the atmosphere in that place ( Tanasescu et al . 
, 2013 ) . 
In particular , the language used in posts from POIs is an important component that contributes toward the place ‚Äôs identity and has been extensively studied in the context of social and cultural geography ( Tuan , 1991 ; Scollon and Scollon , 2003 ; Benwell and Stokoe , 2006 ) . 
Social media posts from a particular location are usually focused on the person posting the content , rather than on providing explicit information about the place . 
Table 1 displays example Twitter posts from different POIs . 
Users express their feelings related to a certain 1Data is available here : https://archive.org/ details / poi - dataplace ( ‚Äò this places gives me war Ô¨Çashbacks ‚Äô ) , comments and thoughts associated with the place they are in ( ‚Äò few of us dressed appropriately ‚Äô ) or activities they are performing ( ‚Äò leaving the news station ‚Äô , ‚Äò on the way to the APCE Annual ‚Äô ) . 
In this paper , we aim to study the language that people on Twitter use to share information about a speciÔ¨Åc place they are visiting . 
Thus , we deÔ¨Åne the prediction of a POI type given a post ( i.e. tweet ) as a multi - class classiÔ¨Åcation task using only information available at posting time . 
Given the text from a user ‚Äôs post , our goal is to predict the correct type of the location it was posted , e.g. park , bar or shop . 
Inferring the type of place from a user ‚Äôs post using linguistic information , is useful for cultural geographers to study a place ‚Äôs identity ( Tuan , 1991 ) and has downstream geosocial applications such as POI visualisation ( McKenzie et al . 
, 2015 ) and recommendation ( Alazzawi et al . 
, 2012 ; Yuan et al . 
, 2013 ; Preo t ¬∏iuc - Pietro and Cohn , 2013 ; Gao et al . 
, 2015 ) . 
Predicting the type of a POI is inherently different to predicting the POI type from comments or reviews . 
The role of the latter is to provide opinions or descriptions of the places , rather than the activities and feelings of the user posting the text ( McKenzie et al . 
, 2015 ) , as illustrated in Table 1 . 
This is also different , albeit related , to the popular task of geolocation prediction ( Cheng et al . 
, 2010 ; Eisenstein et al . 
, 2010 ; Han et al . 
, 2012 ; Roller et al . 
, 2012 ; Rahimi et al . 
, 2015 ; Dredze et al . 
, 2016 ) , as this aims to infer the exact geographical location of a post using language variation and geographical cues rather than inferring the place ‚Äôs type . 
Our task aims to uncover the geographic agnostic features associated with POIs of different types . 
Our contributions are as follows : ( 1 ) We provide the Ô¨Årst study of POI type prediction in computational linguistics ; ( 2 ) A large data set made out of804 Category Sample Tweet Train Dev Test Tokens Arts & Entertainment i ‚Äôm back in central park . 
this place gives me war Ô¨Çashbacks now lol 40,417 4,755 5,284 14.41 College & University currently visiting my dream school   21,275 2,418 2,884 15.52 Food Some Breakfast , it ‚Äôs only right ! # LA 6,676 869 724 14.34 Great OutdoorsSorry Southport , Billy is dishing out donuts at # donutfest today . 
See you next weekend!27,763 4,173 3,653 13.49 Nightlife SpotChicago really needs to step up their Aloha shirt game . 
Only a few of us dressed ‚Äú appropriately ‚Äù tonight . 
:) 5,545 876 656 15.46 Professional & Other Places Leaving the news station after a long day 30,640 3,381 3,762 16.46 Shop & Service Came to get an old fashioned tape measures and a button for my coat 8,285 886 812 15.31 Travel & TransportShoutout to anyone currently on the way to the APCE Annual Event in Louisville , KY ! # APCE201816,428 2,201 1,872 14.88 Table 1 : Place categories with sample tweets and data set statistics . 
tweets linked to particular POI categories ; ( 3 ) Linguistic and temporal analyses related to the place the text was posted from ; ( 4 ) Predictive models using text and temporal information reaching up to 43.67 F1 across eight different POI types . 
2 Point - of - Interest Type Data We deÔ¨Åne the POI type prediction as a multi - class classiÔ¨Åcation task performed at the social media post level . 
Given a post T , deÔ¨Åned as a sequence of tokensT={t1, ... ,tn } , the goal is to label T as one of the MPOI categories . 
We create a novel data set for POI type prediction containing text and the location type it was posted from as , to the best of our knowledge , no such data set is available . 
We use Twitter as our data source because it contains a large variety of linguistic information such as expression of thoughts , opinions and emotions ( Java et al . 
, 2007 ; Kouloumpis et al . 
, 2011 ) . 
2.1 Types of POIs Foursquare is a location data platform that manages ‚Äò Places by Foursquare ‚Äô , a database of more than 105 million POIs worldwide . 
The place information includes veriÔ¨Åed metadata such as name , geo - coordinates and categories as well as other user - sourced metadata such as tags , comments or photos . 
POIs are organized into 9 top level primary categories with multiple subcategories . 
We only focus on 8 primary top - level POI categories since the category ‚Äò Residence ‚Äô has a considerably smaller number of tweets compared to the other categories ( 0.78 % tweets from the total ) . 
We leave Ô¨Åner - grained place category inference as well as using other metadata for future work since the scope of this work is to study the language of posts associated with semantic type places.2.2 Associating Tweets with POI Types Twitter users can tag their tweets to the locations they are posted from by linking to Foursquare places.2In this way , we collect tweets assigned to the POIs and associated metadata ( see Table 1 ) . 
We select a broad range of locations for our experiments . 
There is no public list of all Foursquare locations that can be used through Twitter and can be programmatically accessed . 
Hence , in order to discover Foursquare places that are actually used in tweets , we start with all places found in a 1 % sample of the Twitter feed between 31 July 2016 and 24 January 2017 leading us to a total of 9,125 different places . 
Then , we collect all tweets from these places between 17 August 2016 and 1 March 2018 using the Twitter Search API3 . 
We collect the place metadata from the public Foursquare Venues API . 
This resulted in a total data set of 1,648,963 tweets tagged to a Foursquare place . 
In order to extract metadata about each location , we crawled the Twitter website to identify the corresponding Foursquare Place ID of each Twitter place . 
Then , we used the public Foursquare Venues API4to download all the place metadata . 
2.3 Data Filtering To limit variation in our data , we Ô¨Ålter out all nonEnglish tweets and non - US places , as these were very limited in number . 
We keep POIs with at least 20 tweets and randomly subsample 100 tweets from POIs with more tweets to avoid skewing our data . 
Our Ô¨Ånal data set consists of 196,235 tweets from 2https://developer.foursquare.com/ places 3https://developer.twitter.com/ en / docs / tweets / search / guides/ tweets - by - place 4https://developer.foursquare.com/ overview / venues.html805 2,761 POIs . 
2.4 Data Split We create our data split at a location - level to ensure that our models are robust and generalize to locations held - out in training . 
We split the locations in train ( 80 % ) , development ( 10 % ) and test ( 10 % ) sets and assign tweets to one of the three splits based on the location they were posted from ( see Table 1 for detailed statistics ) . 
2.5 Text Processing We lower - case text and replace all URLs and mentions of users with placeholders . 
We preserve emoticons and punctuation and replace tokens that appear in less than Ô¨Åve tweets with an ‚Äò unknown ‚Äô token . 
We tokenize text using a Twitter - aware tokenizer ( Schwartz et al . 
, 2017 ) . 
3 Analysis We Ô¨Årst analyze our data set to understand the relationship between location type , language and posting time . 
3.1 Linguistic Analysis We analyze the linguistic features speciÔ¨Åc to each category by ranking unigrams that appear in at least 5 different locations , such that these are representative of the larger POI category rather than a few speciÔ¨Åc places . 
Features are normalized to sum up to unit for each tweet , then we compute the ( Pearson)œá2coefÔ¨Åcient independently between its distribution across posts and the binary category label of the post similar to the approach followed by Maronikolakis et al . 
( 2020 ) and Preo t ¬∏iuc - Pietro et al . 
( 2019 ) . 
Table 2 presents the top unigram features for each category . 
We note that most top unigrams speciÔ¨Åc of a category naturally refer to types of places ( e.g. ‚Äò campus ‚Äô , ‚Äò beach ‚Äô , ‚Äò mall ‚Äô , ‚Äò airport ‚Äô ) that are part of that category . 
All categories also contain words that refer to activities that the poster of the tweet is performing or observing while at a location ( e.g. ‚Äò camp ‚Äô and ‚Äò football ‚Äô for College , ‚Äò concert ‚Äô and ‚Äò show ‚Äô for Arts & Entertainment , ‚Äò party ‚Äô for Nightlife Spot , ‚Äò landed ‚Äô for Travel & Transport , ‚Äò hike ‚Äô for Greater Outdoors ) . 
Nightlife Spot and Food categories are represented by types of food or drinks that are typically consumed at these locations . 
Beyond these typical associations , we highlight that usernames are more likely mentioned inthe Arts & Entertainment category , usually indicating activities involving groups of users , emojis indicative of the user state ( e.g. happy emoji in Food places ) and adjectives indicative of the user ‚Äôs surroundings ( e.g. ‚Äò beautiful ‚Äô in Greater Outdoors places ) . 
Finally , we also uncover words indicative of the time the user is at a place , such as ‚Äò tonight ‚Äô for Arts & Entertainment , ‚Äò sunset ‚Äô for the Greater Outdoors and ‚Äò night ‚Äô for Nightlife Spots and Arts & Entertainment . 
3.2 Temporal Analysis We further examine the relationship between the time a tweet was posted and the POI type it was posted from . 
Figure 1 shows the percentage of tweets by day of week ( top ) and hour of day ( bottom ) . 
We observe that tweets posted from the ‚Äò Professional & Other Places ‚Äô , ‚Äò Travel & Transport ‚Äô and ‚Äò College & University ‚Äô categories are more prevalent on weekdays , peaking on Wednesday , while on weekends more tweets are posted from the ‚Äò Great Outdoors ‚Äô , ‚Äò Arts & Entertainment ‚Äô , ‚Äò Nightlife & Spot ‚Äô and ‚Äò Food ‚Äô categories when people focus less on professional activities and dedicate more time to leisure as expected . 
The hour of day pattern follows the daily human activity rhythm , but the differences between categories are less prominent , perhaps with the exception of the ‚Äò Arts & Entertainment ‚Äô category peaks around 8PM and ‚Äò Nightlife Spots ‚Äô that see a higher percent of tweets in the early hours of the day ( between 1 - 5am ) than other categories . 
4 Predicting POI Types of Tweets 4.1 Methods Logistic Regression We Ô¨Årst experiment with logistic regression using a standard bag of n - grams representation of the tweet ( LR - W ) , including unigrams to trigrams weighted using TF - IDF . 
We identiÔ¨Åed in the analysis section that temporal information about the tweet may be useful for classiÔ¨Åcation . 
Hence , to add temporal information extracted from a tweet , we create a 31 - dimensional vector encoding the hour of the day and the day of the week it was sent from . 
We experiment with only using the temporal features ( LR - T ) and in combination with the text features ( LR - W+T ) . 
We use L1 regularization ( Hoerl and Kennard , 1970 ) with hyperparameter Œ±=.01(selected based on dev set from{.001 , .01 , .1}).806 Arts College Food Outdoors Nightlife Professional Shop Travel Featureœá2Feature œá2Featureœá2Featureœá2Feature œá2Featureœá2Featureœá2Feature œá2 concert 167.20 campus 298.74 chicken 375.52 beach 591.81 # craftbeer 425.97 school 87.46 mall 462.03 airport 394.20 museum 152.14 college 266.63 # nola 340.64   239.00   311.68 students 79.93 store 403.00   343.30 show 134.39 university 155.65 lunch 255.98 hike 227.91 beer 203.57 grade 66.05 shopping 359.00 Ô¨Çight 292.94 night 104.48 class 112.23 fried 216.49 lake 193.58 bar 93.90 vote 65.80 shop 132.39 hotel 168.38 tonight 80.76 semester 103.19 dinner 203.65 park 165.92   67.00 our 63.12   126.07 conference 141.74 game 73.56 football 59.24   195.41 island 151.45   56.94 jv 60.64   95.32 landed 118.05 art 69.77 student 57.86 pizza 190.83 sunset 142.44 dj 56.56 church 52.97 apple 88.74 plane 88.42 USER 66.14 classes 57.37 shrimp 188.77 hiking 137.74 tonight 53.39 hs 50.63 market 76.60 bound 78.43 zoo 66.09 students 56.98   179.39 beautiful 109.45 ale 52.62 senior 50.05 auto 73.52 heading 62.09 baseball 62.90 camp 44.19   151.00 bridge 108.56 party 51.14 ss 44.46 stock 72.31 headed 57.12 Table 2 : Unigrams associated with each category , sorted by œá2value computed between the normalized frequency of each feature and the category label across all tweets in the training set ( p<0.001 ) . 
Figure 1 : Percentage of tweets by day of week ( top ) and by hour of day ( bottom ) . 
BiLSTM We train models based on bidirectional Long - Short Term Memory ( LSTM ) networks ( Hochreiter and Schmidhuber , 1997 ) , which are popular in text classiÔ¨Åcation tasks . 
Tokens in a tweet are mapped to embeddings and passed through the two LSTM networks , each processing the input in opposite directions . 
The outputs are concatenated and passed to the output layer using a softmax activation function ( BiLSTM ) . 
We extend the BiLSTM to encode temporal one - hot representation by : ( a ) concatenating the temporal vector to the tweet representation ( BiLSTM - TC ) ; and ( b ) projecting the time vector into a dense representation using a fully connected layer which is added to the tweet representation before passing it through the output layer using a softmax activation function ( BiLSTM - TS ) . 
We use 200dimensional GloVe embeddings ( Pennington et al . 
, 2014 ) pre - trained on Twitter data . 
The maximum sequence length is set to 26 , covering 95 % of thetweets in the training set . 
The LSTM size is h= 32 whereh‚àà{32,64,100,300}with dropout d = 0.5 where d‚àà{.2,.5 } . 
We use Adam ( Kingma and Ba , 2014 ) with default learning rate , minimizing cross - entropy using a batch size of 32 over 10 epochs with early stopping . 
BERT Bidirectional Encoder Representations from Transformers ( BERT ) is a pre - trained language model based on transformer networks ( Vaswani et al . 
, 2017 ; Devlin et al . 
, 2019 ) . 
BERT consists of multiple multi - head attention layers to learn bidirectional embeddings for input tokens . 
The model is trained on masked language modeling , where a fraction of the input tokens in a given sequence is replaced with a mask token , and the model attempts to predict the masked tokens based on the context provided by the non - masked tokens in the sequence . 
We Ô¨Åne - tune BERT for predicting the POI type of a tweet by adding a classiÔ¨Åcation layer with softmax activation function on top of the Transformer output for the ‚Äò classiÔ¨Åcation ‚Äô [ CLS]token ( BERT ) . 
Similarly to the previous models , we extend BERT to make use of the time vector in two ways , by concatenating ( BERT - TC ) , and by adding it ( BERTTS ) to the output of the Transformer before passing it to through the classiÔ¨Åcation layer with softmax activation function . 
We use the base model ( 12layer , 110 M parameters ) trained on lower - cased English text . 
We Ô¨Åne - tune it for 2 epochs with a learning rate l= 2e‚àí5,l‚àà{2e‚àí5,3e‚àí5,5e‚àí5 } and a batch size of 32 . 
4.2 Results Table 3 presents the results of POI type prediction measured using accuracy , macro F1 , precision and recall across three runs . 
In general , we observe that we can predict POI types of tweets with good accuracy , considering the classiÔ¨Åcation is across eight relatively well balanced classes.807 Model Acc F1 P R Major . 
Class 26.89 5.30 3.36 12.50 Random 13.63 12.64 13.63 15.68 LR - T 27.93 14.01 15.78 16.06 LR - W 43.04 37.33 37.06 38.03 LR - W+T 43.73 37.83 37.68 38.37 BiLSTM 44.38 35.77 45.29 33.78 BiLSTM - TC 44.01 38.07 41.51 36.46 BiLSTM - TS 44.72 38.26 42.91 36.30 BERT 48.89 43.67 48.44 41.33 BERT - TC 46.13 41.19 46.81 39.03 BERT - TS 49.17 43.47 48.40 41.26 Table 3 : Accuracy ( Acc ) , Macro - F1 Score ( F1 ) , Precision macro ( P ) , and Recall macro ( R ) for POI type prediction ( all std . 
dev < 0.01 ) . 
Best results are in bold . 
Best results are obtained using BERT - based models ( BERT , BERT - TC and BERT - TS ) , with the highest accuracy of 49.17 ( compared to 26.89 majority class ) and highest macro - F1 of 43.67 ( compared to 12.64 random ) . 
We observe that BERT models outperform both BiLSTM and linear methods across all metrics , with over 4 % improvement in accuracy and 5 points F1 . 
The BiLSTM models perform marginally better than the linear models . 
Temporal features alone are marginally useful when models are evaluated using accuracy ( +0.28 BERT , +0.34 for BiLSTMs , +0.69 for LR ) and perform similarly on F1 , with the notable exception of the BiLSTM models . 
We Ô¨Ånd that adding these features is more beneÔ¨Åcial than concatenating them , with concatenation hurting performance on accuracy for both BiLSTM and BERT . 
Figure 2 shows the confusion matrix of our best performing model , BERT , according to the macroF1 score . 
The confusion matrix is normalized over the actual values ( rows ) . 
The category ‚Äò Arts & Entertainment ‚Äò has the greatest percentage ( 62 % ) of correctly classiÔ¨Åed tweets , followed by the ‚Äò Great Outdoors ‚Äò category with 54 % , and the ‚Äò College & University ‚Äò category with 44 % . 
On the other hand , the categories ‚Äò Nightlife Spot ‚Äò and ‚Äò Shop & Service ‚Äò have the lowest results , where 30 % of the tweets predicted as each of these classes is correctly classiÔ¨Åed . 
Most common error is when the model classiÔ¨Åes tweets from the category ‚Äò College & University ‚Äô as ‚Äò Professional & Other Places ‚Äô , as tweets from these places contain similar terms such as ‚Äò students ‚Äô or ‚Äò class ‚Äô . 
5 Conclusion We presented the Ô¨Årst study on predicting the POI type a social media message was posted from Figure 2 : Confusion Matrix of the best performing model ( BERT ) . 
and developed a large - scale data set with tweets mapped to their POI category . 
We conducted an analysis to uncover features speciÔ¨Åc to place type and trained predictive models to infer the POI category using only tweet text and posting time with accuracy close to 50 % across eight categories . 
Future work will focus on using other modalities such as network ( Aletras and Chamberlain , 2018 ; Tsakalidis et al . 
, 2018 ) or image information ( Vempala and Preo t ¬∏iuc - Pietro , 2019 ; Alikhani et al . 
, 2019 ) and prediction at a more granular level of POI types . 
Acknowledgments DSV is supported by the Centre for Doctoral Training in Speech and Language Technologies ( SLT ) and their Applications funded by the UK Research and Innovation grant EP / S023062/1 . 
NA is supported by ESRC grant ES / T012714/1 . 
Abstract Event information is usually scattered across multiple sentences within a document . 
The local sentence - level event extractors often yield many noisy event role Ô¨Åller extractions in the absence of a broader view of the documentlevel context . 
Filtering spurious extractions and aggregating event information in a document remains a challenging problem . 
Following the observation that a document has several relevant event regions densely populated with event role Ô¨Ållers , we build graphs with candidate role Ô¨Åller extractions enriched by sentential embeddings as nodes , and use graph attention networks to identify event regions in a document and aggregate event information . 
We characterize edges between candidate extractions in a graph into rich vector representations to facilitate event region identiÔ¨Åcation . 
The experimental results on two datasets of two languages show that our approach yields new state - of - the - art performance for the challenging event extraction task . 
1 Introduction Event Extraction ( EE ) , a challenging task in Natural Language Processing , aims to extract key types of information ( aka event roles , e.g. , perpetrators andvictims of an attack event ) that can represent an event in texts and plays a critical role in downstream applications such as Question Answer ( Yang et al . 
, 2003 ) and Summarizing ( Filatova and Hatzivassiloglou , 2004 ) . 
Existing research on EE mostly focused on sentence - level , such as the evaluation in Automatic Content Extraction ( ACE ) 20051 . 
However , an event is usually described in ‚àóMost of the work was done when the Ô¨Årst author was a research engineer in the Institute of Automation , CAS . 
1http://projects.ldc.upenn.edu/ace/ Event Template Event Roles Role Fillers PerpI nd TERRORISTS , HOODED INDIVIDUALS PerpOrg SHINING PATH Victim DOLORES HINOSTROZA ,   HINOSTROZA Original Document S1 : That alleged   TERRORISTS today killed   DOLORES HINOSTROZA , the   mayor of   Mulqui district . 
  S2 :   HINOSTROZA , who was at home , was shot five times . 
S3 : H inostroza 's children told police that   four HOODED INDIVIDUALS   broke into the   HOUSE and shot their mother after having insulted her . 
  S4 :   And their   FATHER was on a business trip then . 
S5 :   DOLORES   HINOSTROZA   deceased   when the ambulance came . 
S6 :   She is the second woman mayor killed this week by alleged   commando groups of the Maoist   SHINING PATH . 
Region1 Region2Figure 1 : An example of document - level event extraction . 
We need to extract noun phrases from the document as role Ô¨Ållers for the event roles in the predeÔ¨Åned event template . 
The uppercased noun phrases in the document are role Ô¨Ållers extracted by the sentence - level extractor . 
Red phrases are correct while green phrases are noises compared to the standard in the template . 
There are two event regions in the sample document . 
multiple sentences in a document . 
As illustrated in Figure 1 , relevant event information ( noun phrases in green color ) is scattered across the whole document . 
To extract event information accurately and comprehensively at document - level , it is necessary to understand the wider context spanning over multiple sentences . 
The existing approaches for event extraction ( EE ) often decompose the document - level EE into sentence - level EE , and extract candidate event role Ô¨Ållers from individual sentences one by one . 
The event role Ô¨Åller extractors often use extraction patterns ( Riloff , 1996 ) or classiÔ¨Åers ( Boros et al . 
, 2014 ) to identify typical local contexts containing a certain type of event role Ô¨Ållers . 
However , local event role Ô¨Åller extractors often produce many false811 candidates , e.g. , the red noun phrases shown in the example document of Figure 1 . 
As shown in the example , one document often mentions a target event multiple times and each time it takes one or more sentences to articulate the event . 
The target event role Ô¨Ållers tend to be mentioned in several groups of adjacent sentences , and we deÔ¨Åne those adjacent relevant sentences as different event regions . 
For example , in Figure 1 , the document mentions the target event twice in two regions . 
The correct role Ô¨Ållers are crowding in the Ô¨Årst event region S1,S2,S3and the second one S5,S6respectively . 
Nevertheless , the sentence - level extractor will extract noise from both the event regions like HOUSE from S3and irrelevant sentence like FATHER inS4 , destroying the layout of the original regions . 
Many previous efforts try to avoid aggregating the noisy candidates by detecting such event regions . 
The popular approach is to apply sentential classiÔ¨Åcation to Ô¨Ålter the sentences and recognize role Ô¨Ållers from the chosen sentences ( Patwardhan and Riloff , 2009 ; Huang and Riloff , 2012 ) . 
However , these approaches only detect regions at single sentence - level and ignore the crowding of relevant sentences . 
Also , they also suffer from the accumulative error of sentential classiÔ¨Åcation . 
For example , they may identify S2as a relevant event region but S3as irrelevant because they fail to take into account the similarity of S2and S3 . 
Another solution proposed by Yang et al . 
( 2018 ) tries to detect the primary event description sentence and supplement the missing event roles with Ô¨Ållers from adjacent sentences . 
This method considers the multiple sentences in an event region but is limited to one region per document . 
For instance , it may detect S1as the primary sentence and supplement it with S2 , missing the valid items like SHINING PATH from region 2 . 
Moreover , it also suffers from the errors selecting primary sentence , and the supplementing strategy is coarse - grained and fails to take into account every candidate Ô¨Åller individually . 
We build a graph for each document to directly model the multiple event regions in a document , each region potentially consisting of multiple sentences . 
In each document graph , the nodes are candidate event role Ô¨Ållers and we insert an edge between two nodes based on either positional proximity ( in adjacent sentences or within the same sentence ) or the coreference relation between two candidate extractions . 
The document graphs capturesentence similarities and sophisticated discourse connections among the candidate event role Ô¨Ållers to reconstruct the original event regions , which can recognize false event role Ô¨Åller extractions from irrelevant sentences . 
For example , after identifying the differences between S4and adjacent sentences S3and S5 , our model will Ô¨Ålter the noisy candidate FATHER inS4 . 
Furthermore , constructing document graphs formed by candidate event role Ô¨Ållers and applying graph neural networks will enable recognizing false event role Ô¨Åller extractions within an event region . 
We employ attentional networks on the graphs to reinforce each candidate ‚Äôs representations by global contextual information and then classify the candidates in a Ô¨Åne - grained manner . 
SpeciÔ¨Åcally , we characterize the edges into vector representations with rich features to control the information Ô¨Çowing between any two nodes . 
For instance , this mechanism will be likely to recognize that it is a murder event based on the sentential contexts of sentences S2and S3 , and therefore determine that the candidate extraction HOUSE is a false extraction because the Targets of a murder are individuals most commonly , but not physical targets or buildings . 
We evaluate our approach on two documentlevel event extraction datasets : the MUC-4 dataset and a newly created dataset CFEED2 . 
Experimental results show that the proposed approach successfully reconstructs 70 % of the event regions and yields new state - of - the - art performance for event extraction on both datasets . 
In summary , the main contributions of this paper are as follows : ‚Ä¢We propose graphs directly modeling the multiple regions with multiple sentences , which successfully help to reconstruct event regions naturally avoid redundant extractions irrelevant sources . 
‚Ä¢We propose an edge - enriched graph attention algorithm that can blend both the local clues and global context to enforce semantic representations for each candidate and help to Ô¨Ålter noises in the event regions . 
‚Ä¢Experimental results show that our method outperforms the existing state - of - the - arts on two datasets with different languages , including a public English MUC-4 dataset and a large - scale Chinese CFEED dataset . 
2http://www.nlpr.ia.ac.cn/cip/ Àúliukang / dataset / documentevent1.html812 2 Related Work Sentence - level EE has achieved a lot of advancement in recent work ( Chen et al . 
, 2015 ; Nguyen et al . 
, 2016 ; Chen et al . 
, 2018 ) and can be classiÔ¨Åed into template - based approaches ( Jungermann and Morik , 2008 ; Bjorne et al . 
, 2010 ; Hogenboom et al . 
, 2016 ) and statistical approaches . 
Templatebased methods require human - crafted templates to match the events . 
Most of the statistical methods are supervised and either based on feature engineering ( Ahn , 2006 ; Ji and Grishman , 2008 ; Liao and Grishman , 2010 ; Reichart and Barzilay , 2012 ) or Neural network algorithm ( Chen et al . 
, 2015 ; Nguyen et al . 
, 2016 ; Chen et al . 
, 2018 ; Liu et al . 
, 2018 ; Sha et al . 
, 2018 ; Liu et al . 
, 2018 ) . 
However , these supervised methods rely on intensive manual annotations . 
To alleviate this problem , many weak supervised methods ( Chen et al . 
, 2017 ; Zeng et al . 
, 2018 ) have arisen and achieved good performance in ACE 2005 evaluation . 
However , most of the time , people care about the events discussed across a whole document . 
So research on document - level EE also prevails . 
Traditionally , pattern - based and classiÔ¨Åer - based methods are popular to solve this task . 
Systems like AutoSlog ( Riloff et al . 
, 1993 ) and AutoSlogTS ( Riloff , 1996 ) directly applied regular patterns to extract role Ô¨Ållers . 
Many works ( Patwardhan and Riloff , 2007 , 2009 ; Huang and Riloff , 2011 , 2012 ; Boros et al . 
, 2014 ) relied on feature - based classiÔ¨Åers to distinguish candidate role Ô¨Ållers from texts and achieved better performance . 
Until recent years , researchers ( Hsi , 2018 ; Yang et al . 
, 2018 ; Zheng et al . 
, 2019 ) began to utilize multiple neuralbased methods to solve the task . 
Notably , among the document - level EE research , some works ( Patwardhan and Riloff , 2009 ; Huang and Riloff , 2012 ; Yang et al . 
, 2018 ) have noticed the importance of identifying event regions to improve performance . 
Traditional neural networks such as Convolutional Neural Networks and Recursive Neural Networks are hard to deal with graphical data structures , so many graph - based neural networks ( GNNs ) emerge ( Gori et al . 
, 2005 ; Bruna et al . 
, 2013 ; Kipf and Welling , 2016 ) . 
In order to deal with graphs with different edge types , relational GNNs ( Schlichtkrull et al . 
, 2018 ; Marcheggiani and Titov , 2017 ; Vashishth et al . 
, 2019 ; Bastings et al . 
, 2017 ) try to use separate weights for different edges . 
However , one limitation of these GNNs is that the weights are Ô¨Åxed for allneighbors . 
So Veli Àáckovi ¬¥ c et al . 
( 2017 ) leveraged masked attentional layers ( GATs ) to learn adaptive weights for different neighbors . 
By now , some works ( Schlichtkrull et al . 
, 2018 ; Vashishth et al . 
, 2019 ) have successfully applied GNNs to model the document - level information within texts and achieved state - of - the - art performance . 
Our model is distinguishing because we not only utilize these recent advances but also turns the relational edges to feature - enriched nodes and extends GATs on such heterogeneous graphs . 
3 Fine - grained Filtering Framework 3.1 Overall Framework Our method for document - level Event Extraction follows three main procedures . 
Extracting role candidates by sentence - level event extractor ( SEE ) : Given a document , we disintegrate it into a series of sentences and apply sentence - level event extractors to identify candidate role Ô¨Ållers . 
Constructing graphs to model event regions : Based on the primitive results from the last step and the properties of event regions , we build graphs to capture both the local clues and global context among those candidates . 
Selecting role Ô¨Ållers via edge - enriched graph attention networks ( EE - GAT ) : We encode the different edges into vectors and then leverage the attention mechanism on the edge - enriched graphs to update the nodes ‚Äô representations . 
After that , we feed the candidates to classiÔ¨Åers for Ô¨Åltering . 
3.2 Extracting Role Candidates by Sentence - level Event Extractor Sentence - level Event Extractor aims at extracting event roles from each sentence in a document . 
We reproduce the SEE introduced by Yang et al . 
( 2018 ) and employ BiLSTM - CRF to identify candidates from each sentence . 
The model uses the word embedding as the input features , and this method is compatible with both the English and Chinese corpus . 
3.3 Constructing Graphs to Model Event Regions For each document , we want to utilize the observed event region information in our model . 
As discussed before , the original event region information of the candidates from the SEE is destroyed . 
So we make use of the properties of the original813 Candidate Role Fillers from SEE S1 : T hat alleged   [ c1 : TERRORISTS ]   PerpInd today   killed    [ c2 :   DOLORES HINOSTROZA   ] Victim , the   mayor of   Mulqui district . 
  S2 : [ c3 :   HINOSTROZA ] Victim , who was at home ,   was shot five times . 
S3 : ‚Ä¶   that four   [ c 4 : HOODED INDIVIDUALS ]   PerpInd broke into the   [ c 5 : HOUSE ]   Target and shot ‚Ä¶ S4 : ‚Ä¶   their   [ c6 : FATHER ] PerpInd was on ‚Ä¶ S5 : [ c7 : DOLORES   HINOSTROZA ] Victim   deceased   when the ambulance came . 
S6 : She   is the second woman mayor killed this   week by alleged commando groups of the   Maoist   [ c8 : SHINING PATH ] PerpOrg . 
c6 c8 c1 c7 c2 c5 + c2 ‚Äô Attention Classify Update c3 Ôºö Within regional Affinity ( Strong ) Ôºö Within regional   Affinity   ( Weak ) Ôºö Across regional   Coreference From   Region   2 c4 Region 1 Region 2 From   Region   1Figure 2 : The overall framework of Ô¨Åne - grained Ô¨Åltering framework . 
8 candidate role Ô¨Ållers ( c1‚àíc8 ) with sentential clues and speciÔ¨Åc role types are extracted by SEE as nodes . 
3 types of edges are deÔ¨Åned to connect those nodes : within - regional afÔ¨Ånity ( Strong ) , within - regional afÔ¨Ånity ( weak ) , across - regional coreference . 
Then we employ edge - enriched attention mechanism to update the representation of each candidate for classiÔ¨Åcation , like nodec2 / primefromc2 . 
Ideally , the framework will Ô¨Ålter noisy candidates c5,c6and reconstruct the original two event regions . 
event regions and , according to them , build a graph to link those candidates . 
SpeciÔ¨Åcally , we Ô¨Årst take each candidate role Ô¨Åller as the node in the graph . 
These nodes can easily take rich candidates ‚Äô rich features as initial representation , such as the entity embeddings and the local sentential information . 
For example , in Figure 2 , we extract 8 candidate role Ô¨Ållers with speciÔ¨Åc role type from a document using the aforementioned SEE . 
We mark them as c1‚àíc8and regard them as the nodes . 
As we know from the property of event regions , the correct role Ô¨Ållers tend to crowd within the same or adjacent sentences , such as c1,c2,c3and c4 in Figure 2 . 
Also , one event may be mentioned by multiple event regions , and there can be coreferential role Ô¨Åller across these regions , like c2and c7 . 
We employ such properties of event regions to construct the graphs so as to utilize regional information . 
In detail , we deÔ¨Åne the following 2 types of relations ( 3 types of edges ) in the graphs : Within - regional AfÔ¨Ånity When two candidates appear in the same or adjacent sentences , they have a within - regional afÔ¨Ånity . 
We use such afÔ¨Ånities to model the phenomenon that multiple event role Ô¨Ållers tend to crowd in an event region . 
When one candidate Ô¨Åller in the region has high conÔ¨Ådence to be a positive one , other candidates can share this conÔ¨Ådence and vice versa . 
Furthermore , we distinguish the same sentence afÔ¨Ånity from the adjacent sentences afÔ¨Ånity using different edges because we believe such afÔ¨Ånity is stronger within the same sentence . 
For instance , in Figure 2 , we assign c1 andc2with strong within - regional afÔ¨Ånity sincethey are both in S1 , and use a single solid line to represent this afÔ¨Ånity . 
And we assign c6andc7 with the weak within - regional afÔ¨Ånity because they occur in adjacent sentences S4and S5respectively . 
A single dotted line is used to illustrate it . 
The weak afÔ¨Ånity may have less conÔ¨Ådence sharing and help Ô¨Ålter nosy candidate c6while keeping c7 . 
Across - regional Coreference When two candidates are the same to each other lexically and also recognized as the same event role type , we assume that they have a coreference relationship . 
When these two coreferential candidates are not in the same or adjacent sentences ( they do not have within - regional afÔ¨Ånity ) , we assign them with across - regional coreference so as to bridge different regions . 
This is because a document usually mentions the target event in multiple event regions , and the same event role Ô¨Ållers may repeat in these regions . 
We connect these regions by utilizing such cross - region coreference relationships . 
Such connections will help exchange semantic information and share classiÔ¨Åcation conÔ¨Ådence among different regions . 
Here in Figure 2 , we assign c2andc7with across - regional coreference relationship and use a double solid line to represent corresponding edge in the graph . 
Although the constructed graphs do not precisely demonstrate the original event regions , the GNNs models will synthesize comprehensive context from such connections to enforce each candidate ‚Äôs representations , identify the noises , and reconstruct the original regions as a result.814 3.4 Selecting Role Fillers via Edge - enriched Graph Attention Networks After building graphs from the documents , we classify the nodes via supervised learning . 
We Ô¨Årst encode the nodes and edges into vectors and then apply the attention mechanism to update the representation of each node from its neighbors , and Ô¨Ånally feed the updated representation into classiÔ¨Åers for Ô¨Åltering . 
Encoding Each graph is represented by its nodes and edges , as G= ( C , E ) , whereCrepresents nodes andErepresents edges . 
We Ô¨Årst initialize all nodes with their feature representations and get C={c1,c2, ... ,c n},ci‚ààRF , wherecirepresents the features of node i , nis the number of nodes andFis the embedding size for each node . 
Each node is featured by 4 types of embeddings ci= [ wi , pi , ti , si ] , wherewiis the average word embedding of each candidate entity , piis the position embedding of the candidate with respect to the sentence , tiis the embedding of role type , and si is the sentence embedding by averaging all words in the sentence . 
For edges , the plain graph attention mechanism does not encode them into vectors . 
Such a mechanism equally treating the edges suffers from losing the information of distinguishing edges . 
A popular way to deal with this problem is to use different weights for different edges in the attention operation ( Relational GAT , R - GAT ) . 
However , R - GAT does not have edge representation nor controls the information Ô¨Çow equally for the same type edges . 
Our edge - enriched attention model characterizes the edges into vector representations , which can especially control the information between each candidate node pair . 
Initially , we regard each edge as a new type of node featuring its edge type and make a new set of nodes E / prime . 
For example in Figure 3 , we use the new node e1,2‚ààE / primeto represents the original within - regional afÔ¨Ånity edge between nodesc1andc2 . 
Here the same type of edges will share the same initial vector representation . 
c 1 c 2 c 1 c 2 e12 Figure 3 : Encoding of EdgesIn this way , we construct a new graph /tildewideG= ( /tildewideC,/tildewideE)in which all the new edges in the graph are the same , but we have two types of nodes /tildewideC= { C , E / prime } , which means the graph is heterogeneous now . 
To update all nodes in the same attention mechanism , we combine the feature spaces of both the original nodes and new edge - enriched nodes . 
In this way , any new node within the new graph will have 5 types of embedding : Àúci= [ wi , pi , ti , si , ei ] , where [ ei]is the edge type representation . 
We initializeeias zero vectors for original candidate nodes and the other 4 embeddings as zero vectors for the new edge nodes . 
Updating Then we update the edge - enriched graph based on GAT proposed by ( Veli Àáckovi ¬¥ c et al . 
, 2017 ) . 
GAT is in essence masked attention operation on graphs . 
For each layer of graph attention , it updates the representation of node Àúciby computing the linear combinations of its neighbors ‚Äô normalized attention scores and their corresponding transformed representations : Àúc / prime i = H /bardbl h=1œÉÔ£´ Ô£≠/summationdisplay j‚ààNiŒ±h ijWhÀúcjÔ£∂ Ô£∏ ( 1 ) Here we concatenate ( signiÔ¨Åed by /bardbl)Hheads of the attentions results . 
œÉrepresents the activation functions andNirepresents the neighbor nodes of Àúci , including itself . 
Transformation Whis shared for all nodes within each head . 
We obtain the attention scoreŒ±h ijin headhas followed : Œ±h ij = exp / parenleftbig LeakyReLU / parenleftbig aT / parenleftbig WhÀúci / bardblWhÀúcj / parenrightbig / parenrightbig / parenrightbig /summationtext k‚ààN(i)exp ( LeakyReLU ( aT(WhÀúci / bardblWhÀúck ) ) ) ( 2 ) Hereais a single - layer feedforward neural network . 
We apply two layers of the GAT to update on the graphs . 
The Ô¨Årst layer will exchange the information between candidate nodes and edge nodes , which will characterize the edge representation with the semantic context . 
Now each edge node will have unique vector representations . 
Then in the second layer , the candidate nodes will incorporate information from the updated edge nodes , indirectly blend in the features of adjacent candidate nodes in the original graph G. The enriched edges play the role to control the information Ô¨Çowing between neighbor candidate nodes uniquely . 
For comparison , the R - GAT model uses different weights for different edges as followed , where Ris the set of edge types . 
Here different edges control815 SystemsEvent Roles in MUC-4 Dataset PerpInd PerpOrg Target Victim Weapon Average ( Riloff , 1996 ) 33/49/40 53/33/41 54/59/56 49/54/51 38/44/41 45/48/46 ( Patwardhan and Riloff , 2009 ) 51/58/54 34/45/38 43/72/53 55/58/56 57/53/55 48/57/52 ( Huang and Riloff , 2011 ) 48/57/52 46/53/50 51/73/60 56/60/58 53/64/58 51/62/56 ( Huang and Riloff , 2012 ) 54/57/56 55/49/51 55/68/61 63/59/61 62/64/63 58/60/59 ( Boros et al . 
, 2014 ) 53/58/55 56/67/ 61 59/63/61 56/55/55 72/65/68 59/61/60 ( Yang et al . 
, 2018 ) 48/60/54 52/74/ 61 52/70/59 56/62/59 70/77/74 56/69/61 SEE 35/77/48 28/ 88/42 44/ 80/57 38/ 83/53 59/ 86/70 41/83/55 GAT 62/52/57 57/53/55 60/61/60 61/58/59 78/78/78 64/60/62 R - GAT 58/62/ 60 57/61/59 60/63/62 57/67/61 71/75/73 61/66/63 EE - GAT 60/59/ 60 58 /61/60 61/68/64 62/65/ 63 75/75/75 63/66/ 65 Table 1 : Evaluation on MUC-4 test set , P / R / F1 ( Precision / Recall / F1 - Score,% ) . 
Event Types SystemsEvent Roles in CFEED Dataset NAME NUM BEG END ORG Average Freeze(Boros et al . 
, 2014 ) 71/76/74 56/57/56 77/54/63 83/ 80/81 70/80/ 75 72/69/70 ( Yang et al . 
, 2018 ) 83/71/76 70 /49/58 75/67/71 85/65/74 71/67/69 77/64/70 EE - GAT 68/82/75 57/ 63/60 71/77/74 84/79/ 81 65/82/72 69/77/73 Pledge(Boros et al . 
, 2014 ) 74/95/83 60/46/52 68/ 81/74 74/ 30/42 83/ 92/87 72/69/70 ( Yang et al . 
, 2018 ) 84/87/86 76/54/63 81/72/76 85/28/42 88/82/85 83/64/72 EE - GAT 77/95/85 79/55/65 76/78/ 77 83/30/44 84/91/ 88 80/70/75 OW / UW(Boros et al . 
, 2014 ) 49/89/63 63/ 65/64 39/ 79/52 62/ 45/53 ‚Äî 54/70/61 ( Yang et al . 
, 2018 ) 77/70/73 79/54/64 66/68/67 74/39/51 ‚Äî 74/58/65 EE - GAT 66/82/ 73 80 /60/68 73 /79/76 77 /44/56 ‚Äî 74/66/70 Total(Boros et al . 
, 2014 ) 65/87/74 60/56/58 61/71/66 73/ 52/61 77/86/ 81 66/69/67 ( Yang et al . 
, 2018 ) 81/76/78 75 /52/61 74/69/71 81/44/57 80/75/77 78/62/69 EE - GAT 70/86/77 72/ 59/65 73/78/75 81 /51/63 75/87/81 74/71/72 Table 2 : Evaluation on the CFEED test set , P / R / F1 ( Precision / Recall / F1 - Score,% ) . 
the information exchange differently . 
However , this mechanism is not as effective as the enriched edges in our EE - GAT model . 
c / prime i = H /bardbl h=1œÉÔ£´ Ô£≠/summationdisplay r‚ààR / summationdisplay j‚ààNiŒ±h ijWr , hcjÔ£∂ Ô£∏ ( 3 ) ClassiÔ¨Åcation After updating the candidate nodes via the two layers multi - head attention mechanism , we need to classify each candidate node as either positive or negative . 
Now we average the vectors of multiple heads to get the Ô¨Ånal representation of each node and then project the results into a softmax classiÔ¨Åcation layer . 
As a result , we will get the probabilities of the node as either positive or negative . 
This process is illustrated in equation ( 4 ) , where yi‚àà{0,1}is the label of node i , Œ∏represents all the parameters , pis the probability of yiequals to 0 or 1 . 
p(yi|/tildewideG;Œ∏ ) = softmaxÔ£´ Ô£≠1 H / summationdisplay h=1 / summationdisplay j‚ààN(i)Œ±h ijWhÀúc / prime jÔ£∂ Ô£∏ ( 4 ) We train our model to minimize the crossentropy loss in the data and use the Adam optimization method proposed by Kingma and Ba ( 2014 ) toupdate the parameters Œ∏ . 
The loss function is as followed in equation ( 4 ) where ÀÜyi = p(yi= 1|G;Œ∏ ) is the predicted probability of node ias positive , N is the number of samples . 
L(Œ∏ ) = ‚àíN / summationdisplay i=1(yilog ÀÜyi+ ( 1‚àíyi ) log ( 1‚àíÀÜyi ) ) ( 5 ) 4 Experiments 4.1 MUC-4 MUC-4 dataset was released by Message Understanding Conferences in 1992 . 
It is about terrorism events and consists of 1700 documents as in Table 4 . 
We follow the same evaluation paradigm as previous work and evaluate the 5 kinds of event roles : PerpInd , ( individual perpetrator ) , PerpOrg ( organizational perpetrator ) , Target ( physical target ) , Victim ( human target name or description ) Datasets Event Types Train Dev Test Total MUC-4 Terrorism 1300 200 200 1700 CFEEDFreeze 589 150 300 1039 Pledge 3602 300 300 4202 OW / UW 1303 300 300 1903 Table 3 : Statistics of MUC-4 and CFEED816 andWeapon ( instrument i d or type ) . 
We use head noun matching ( e.g. HINOSTROZA is considered to match DOLORES HINOSTROZA ) as before too . 
Baselines For comparison , we choose the following 6 previous state - of - the - art systems as the baselines for MUC-4 . 
Riloff ( 1996 ) automatically produced many domain - speciÔ¨Åc extraction patterns for role Ô¨Ållers extraction . 
Patwardhan and Riloff ( 2009 ) incorporated both phrasal and sentential evidence to label role Ô¨Ållers . 
They Ô¨Årst used a sentential event recognizer to select sentences and then applied a plausible roleÔ¨Åller recognizer to extract role Ô¨Ållers . 
Huang and Riloff ( 2011 ) designed TIER system to better extract role Ô¨Ållers from Secondary Context , regardless of whether a relevant event is mentioned . 
Huang and Riloff ( 2012 ) deÔ¨Åned many features and used SVMs to extract local candidate role Ô¨Ållers and CRF to choose sentences for Ô¨Ånal results . 
Boros et al . 
( 2014 ) utilized domain - relevant word representations as the features of noun phrases and then applied randomized decision trees to identify role Ô¨Ållers . 
Here we adopt the same idea but use a different classiÔ¨Åer MLP . 
Besides , we use the same node features as in EE - GAT instead of just domain word vectors for comparison with our model . 
Yang et al . 
( 2018 ) proposed a document - level EE system following three steps . 
It Ô¨Årst extracted candidate role Ô¨Ållers from each sentence via sequence tagging model ; then it applied Convolutional Neural Networks to detect the primary sentence that mentions the target event ; Ô¨Ånally , it aggregated the candidate role Ô¨Ållers from the primary sentence and supplements the missing even roles from adjacent sentences . 
Experiments on MUC-4 For node representations , we randomly initialize pi , tias 50 - dim vectors and eias 200 - dim , and use the 100 - dim Glove3word embedding for wi , si . 
Each layer of the attention mechanism has 8 heads and the learning rate is set as 5e-4 . 
We train on MUC-4 training data for 100 epochs and choose the best model performed on the development set for testing . 
We report Precision / Recall / F1 - score of the test results for each event role individually and the macro - average over all Ô¨Åve roles . 
The test results 3https://nlp.stanford.edu/projects/ glove / are shown in Table 2 . 
From the table , we have the following observations : ( 1 ) In general , our EE - GAT framework achieves the best performance compared with previous state - of - the - art methods . 
It signiÔ¨Åcantly improves the previous best method by 4.0 % ( 65 % vs. 61 % ) on average F1 score and most of the improvement is contributed by the better precision 7.0 % ( 63 % vs. 56 % ) as opposed to Yang et al . 
( 2018 ) . 
( 2 ) The SEE results have high recall but very low precision because of the noisy candidates . 
Plain GAT Ô¨Ålters some noises and improves precision a lot . 
R - GAT and EE - GAT balance the trade - off between precision and recall and achieve a better overall F1 score . 
( 3 ) In detail , our method achieves the best performance nearly on most of the event roles . 
We signiÔ¨Åcantly improve the F1 score of 4.0 % ( 60 % vs. 56 % ) in PerInd and 3.0 % inTarget ( 64 % vs. 61 % ) compared to previous best in Huang and Riloff ( 2012 ) . 
4.2 CFEED CFEED C hinese Financial Event Extraction Dataset is a larger dataset in Chinese about the major events in the announcements of listed companies . 
We construct it by the same method proposed by Yang et al . 
( 2018 ) . 
We crawled the public announcements from sohu.com4and the event templates from eastmoney.com5 , and then align them . 
We assume that if the key role Ô¨Ållers in a template appear in an announcement , the announcement is describing the event in the template . 
As in Table 3 , it consists of a total of 7144 documents and 3 types of Ô¨Ånancial events : freezing shares ( freeze ) , pledging shares ( pledge ) and overweighting and underweighting shares ( OW&UW ) . 
We deÔ¨Åned 5 types of event role in these Ô¨Ånancial events : shareholder ‚Äôs name ( NAME ) , organization ( ORG ) , number of shares ( NUM ) , event starting date ( BEG ) , event ending date ( END ) . 
Note that the ORG is not included in OW&UW event . 
Baselines For comparison , we select the two methods mentioned above as the baselines for CFEED : Boros et al . 
( 2014 ) andYang et al . 
( 2018 ) . 
Experiments on CFEED We use the same settings as in MUC-4 to evaluate on the CFEED except that we use the character - level 100 - dim embeddings trained on Chinese wiki corpus6 . 
We sep4http://q.stock.sohu.com/index.shtml 5http://choice.eastmoney.com/ 6https://github.com/Embedding/ Chinese - Word - Vectors817 StatisticsMUC-4 CFEED Gold SEE EE - GAT Gold SEE EE - GAT Avg # Fillers /Doc 8.21 11.17 6.30 11.72 29.95 10.43 Avg # Regions /Doc 1.76 2.86 1.57 2.53 2.21 2.58 Avg # Fillers /Region 5.32 5.54 4.57 5.88 16.94 5.51 Eval for Regions ‚Äî 21/87/34 65/70/68 ‚Äî 16/96/27 68/77/72 Table 4 : Distributions of role Ô¨Ållers in the golden data and results of SEE and EE - GAT on the test set of MUC-4 and CFEED . 
The last row is the evaluation ( Precision / Recall / F1 - Score,% ) of the regions sentence by sentence . 
The statistics demonstrate the salient Event Regions in golden data and its reconstruction by EE - GAT . 
Settings MUC-4CFEED Freeze Pledge OW&UW Total ( Yang et al . 
, 2018 ) 56/69/61 77/64/70 83/64/72 74/58/65 78/62/69 EE - GAT w/ 1st Rel 63/59/61 71/72/71 77/68/72 64/68/66 71/69/70 EE - GAT w/ 1st & 2nd Rels 62/64/63 66/77/71 76/ 71/73 64/ 70/67 69/73/71 EE - GAT 63/66/65 69/77/73 80/70/ 75 74 /66/70 74/71/ 72 Table 5 : Effectiveness of the Regional Relations in EE - GAT ( Average P / R / F1 , Precision / Recall / F1 - Score,% ) . 
1st Relmeans strong within - regional afÔ¨Ånity and 2nd Rel means weak within - regional afÔ¨Ånity . 
arately evaluate the 3 types of events and the results are in Table 3 . 
We can observe that our EE - GAT can achieve the best performance on all the 3 types of events when compared with the baselines . 
The results verify the robustness of our method in Chinese corpus . 
Besides , compared with the method in Yang et al . 
( 2018 ) , the major improvement comes from recall rather than precision as on MUC-4 . 
This is because the Ô¨Ånancial announcement documents in CFEED usually have one main sentence describing the target event , so Yang ‚Äôs method can achieve high precision by detecting the primary event mention . 
However , MUC-4 dataset does not have such characteristics . 
4.3 Reconstructing Event Regions As in Table 4 about event regions , test if a sentence in the new regions appears in the golden regions and get the evaluation Precision , Recall , andF1scores . 
We can observe that in both of the datasets : ( 1 ) EE - GAT successfully reconstructs 70 % of the event regions during the evaluation , which improves about 40 % from the SEE results . 
The detection of the event regions contributes to most of the Ô¨Åltering process . 
( 2 ) SEE extracted too many noisy role Ô¨Ållers compared to the golden standard . 
EE - GAT Ô¨Ålters many noises and the counts of remaining Ô¨Ållers are similar to the golden standard . 
( 3 ) The distribution of role Ô¨Ållers and event regions are more close to the golden standard after EE - GAT Ô¨Åltering . 
In detail , on the gold test sets , there are about 1.76 regions in a document and 5.32 Ô¨Ållers in each region on MUC-4 , and 2.53 regions and 5.88 Ô¨Ållers per region on CFEED . 
However , the eventregion distribution diverges after SEE because of the noisy candidates , and we have about 2.86 regions in a document and 5.54 Ô¨Ållers in each region on MUC-4 , and 2.21 regions and 16.94 Ô¨Ållers per region on CFEED . 
Then these statistics recover back to normal after the Ô¨Åltering of EE - GAT , and there are about 1.57 regions in a document and 4.57 Ô¨Ållers in each region on MUC-4 , and 2.58 regions and 5.51 Ô¨Ållers per region on CFEED . 
4.4 Effectiveness of Regional Relations We set the following control experiments to demonstrate the effectiveness of the regional relations in Ô¨Åltering the noise . 
We add the three types of edges one by one and test the performance of EE - GAT . 
As in Table 5 , we can observe that the overall performance on all the datasets improves when more types of relations are used . 
( 1 ) Particularly , even the utilization of strong within - regional afÔ¨Ånity ( 1st Rel ) only in EE - GAT achieves slightly better performance compared to the previous state - of - theart ( Yang et al . 
, 2018 ) . 
( 2 ) Adding the weak withinregional afÔ¨Ånity ( 2nd Rel ) further improves the overall performance , especially the average 4.5pp improvement in recall score . 
( 3 ) And the complete EE - GAT model connecting the multiple event regions achieves even better overall performance . 
These results demonstrate that the event region relations can capture the global contextual information and help to Ô¨Ålter the noisy candidates . 
5 Conclusion We propose a Ô¨Åne - grained Ô¨Åltering framework to address the aggregating problem in document - level818 event extraction by reconstructing event regions . 
Our method can Ô¨Ålter those noise both in irrelevant sentences and in the event regions and achieve stateof - the - art performance on both the MUC-4 and CFEED datasets . 
Future work may consider using an end2end model to avoid error propagation from SEE . 
Acknowledgments This work is supported by the Natural Key RD Program of China ( No.2018YFB1005100 ) , the National Natural Science Foundation of China ( No.61922085 , No . 
U1936207 , No.61806201 ) and the Key Research Program of the Chinese Academy of Sciences ( Grant NO . 
ZDBS - SSW - JSC006 ) . 
This work is also supported by CCF - Tencent Open Research Fund , Beijing Academy of ArtiÔ¨Åcial Intelligence ( BAAI2019QN0301 ) and independent research project of National Laboratory of Pattern Recognition . 
Abstract We propose a newly annotated dataset for information extraction on recipes . 
Unlike previous approaches to machine comprehension of procedural texts , we avoid a priori pre - deÔ¨Åning domain - speciÔ¨Åc predicates to recognize ( e.g. , the primitive instructions in MILK ) and focus on basic understanding of the expressed semantics rather than directly reduce them to a simpliÔ¨Åed state representation ( e.g. , ProPara ) . 
We thus frame the semantic comprehension of procedural text such as recipes , as fairly generic NLP subtasks , covering ( i ) entity recognition ( ingredients , tools and actions ) , ( ii ) relation extraction ( what ingredients and tools are involved in the actions ) , and ( iii ) zero anaphora resolution ( link actions to implicit arguments , e.g. , results from previous recipe steps ) . 
Further , our Recipe Instruction Semantic Corpus ( RISeC ) dataset includes textual descriptions for the zero anaphora , to facilitate language generation thereof . 
Besides the dataset itself , we contribute a pipeline neural architecture that addresses entity and relation extraction as well as identiÔ¨Åcation of zero anaphora . 
These basic building blocks can facilitate more advanced downstream applications ( e.g. , question answering , conversational agents ) . 
1 Introduction Recently , several efforts have aimed at understanding recipe instructions ( see Section 2 ) . 
We consider such recipes as prototypical for procedural texts , for which processing is complex due to the need to ( i ) understand the ordering of steps ( not unlike , e.g. , event ordering in news ) , ( ii ) solve frequent ellipsis ( i.e. , zero anaphora ) and coreference resolution , and ( iii ) track the state changes they involve ( e.g. , ingredients processed / combined to new entities ) . 
Especially the latter distinguishes procedural text processingfrom more traditional information extraction ( e.g. , from news ) . 
Most existing works on recipes focus on recognizing pre - deÔ¨Åned predicates , typically in the form of a limited set of instruction types ( e.g. , to convert the recipe to robot instructions ) with predeÔ¨Åned argument slots to Ô¨Åll . 
Further , they often rely on an available starting list of ingredients ( which may not be available in other procedural text ) . 
Hence , current approaches towards recipe understanding make assumptions that are rather domain speciÔ¨Åc . 
In contrast , we aim for a more basic and generic structured representation of the procedural text , limiting domain - speciÔ¨Åc knowledge and building on more general semantic concepts . 
In particular , we build on semantic concepts as deÔ¨Åned in PropBank ( Kingsbury and Palmer , 2002 ) , which are not domain - speciÔ¨Åc . 
Note that our proposed form of structured representations not necessarily allows directly solving informational queries that require explicit reasoning and/or state tracking ( e.g. , ‚Äú Where are the tomatoes after step 5 ? ‚Äù ) . 
We however pose that properly detecting the various entities ( e.g. , ingredients and their derivations ) and the actions that are executed on them ( as described by verbs ) , with the appropriate coreference and zero anaphora resolution , would enable constructing a graph that facilitates such tracking . 
Thus , while our proposed representation based on the idea of joint entity and relation extraction ( Bekoulis et al . 
, 2018 ) , provides useful input for it , such explicit state tracking and representation ( e.g. , as in ProPara , Dalvi et al . 
, 2018 ) is left out of scope here . 
In summary , this paper reports on our work - inprogress and makes two main contributions . 
First , we present our newly annotated Recipe Instruction Semantic Corpus ( RISeC ) dataset ( Section 3 ) , following the frame - semantic representation of PropBank ( Kingsbury and Palmer , 2002 ) . 
Since821 PropBank is domain - agnostic , the approach should be largely generalizable1to other procedural text settings . 
Second , we introduce a baseline framework ( Section 4 ) to solve ( i ) entity recognition ( ingredients , tools and actions ) , ( ii ) relation extraction ( ingredients and tools linked to the actions ) , ( iii ) zero anaphora identiÔ¨Åcation . 
Experimental evaluation thereof on RISeC is provided ( Section 5 ) . 
2 Related work From the perspective of structured representation , Tasse and Smith ( 2008 ) deÔ¨Åne the Minimal Instruction Language for the Kitchen ( MILK ) , which is based on Ô¨Årst - order logic to describe the evolution of ingredients throughout a recipe , and use it for annotation in the CURD dataset . 
Building on this effort , Jermsurawong and Habash ( 2015 ) extend CURD toward ingredientinstruction dependency tree parsing in SIMMR : they present an ingredient - instruction dependency tree representation of the recipe , but do not model instruction semantics . 
This contrasts with Maeta et al . 
( 2015 ) , who propose a pipeline framework for information extraction on Japanese recipes from the the recipe Ô¨Çow graph ( r - FG ) dataset ( Mori et al . 
, 2014 ) . 
Maeta et al . 
use word segmentation , named entity recognition and syntactic analysis to extract predicate - argument structures and build a recipe Ô¨Çow graph that is conceptually similar to a SIMMR tree . 
Their work is conceptually closest to ours , in that they propose a chain of NLP subtasks ( but we do not need word boundary identiÔ¨Åcation in our English corpus ) . 
Yet , we build on a more elaborate and generic semantic relation scheme , PropBank ( Kingsbury and Palmer , 2002 ) . 
Further , methodologically we adopt neural network models as opposed to their logistic regression for NER and a maximum spanning tree ( MST ) parser for the relations ( i.e. , graph arcs ) . 
Tracking state changes is another key to understanding recipe language . 
Bosselut et al . 
( 2018 ) predict the dynamics of action and entity attributes in recipes by employing a recurrent memory network . 
Their work includes sentence generation , but does not address the zero anaphora problem ( see further ) directly . 
Besides recipes , other works focus on different procedural tasks . 
The ProPara2project aims at 1While some of our entity types are speciÔ¨Åc to the cooking domain ( e.g. , ‚Äú food ‚Äù , ‚Äú temperature ‚Äù ) , the relations that link action verbs to them are not ( cf . 
PropBank ) . 
2http://data.allenai.org/propara Preheat oven to 350 degrees F.In a casserole , combine soup mix , artichoke hearts , cheese and crab meat . 
Bake [ the crab mixture ] for 30 minutes ; then serve [ the baked crab ] immediately . 
ACTION TOOL TEMPERATUREArg_PPTArgM_MNR TOOL ACTION FOOD FOOD FOOD FOODArg_PPT ArgM_LOCArg_PPTArg_PPTArg_PPT ACTION DUR ACTIONArgM_TMP123 ZAV ZAVFigure 1 : An annotated recipe . 
The fragments between brackets are manually added anaphora descriptions . 
comprehending scientiÔ¨Åc processes and tracking the status of entities in them : Dalvi et al . 
( 2018 ) focus on tracking entity locations ( as well as their creation / destruction ) using a speciÔ¨Åc matrix state representation ( with a row per step , a column per entity ) . 
The proposed models however do not incorporate entity recognition and are speciÔ¨Åcally Ô¨Ålling the chosen state representation . 
In our work , we rather stick to a more ‚Äú basic ‚Äù understanding , which is broader in scope than location tracking . 
In terms of datasets beyond the recipe domain , the work of Mysore et al . 
( 2019 ) is noteworthy : it focuses on material synthesis and annotates domain - speciÔ¨Åc entities ( materials , operations , conditions , etc . 
) and relations . 
The latter in our case are rather domain - agnostic ( using PropBank ) . 
3 The RISeC Dataset The following paragraphs describe our dataset and the annotations underlying the presented extraction task3 . 
3.1 Dataset Collection Recipes in our RISeC dataset are those from the SIMMR dataset.4Unlike SIMMR , we only use the instruction text of each recipe , and rather detect ingredients ( as well as derived entities ) from the text itself . 
We annotate the dataset using BRAT ( Stenetorp et al . 
, 2012 ) , which eventually creates a directed acyclic graph where ( i ) vertices areentities ( text spans ) such as ingredients , tools , actions , intermediate products , and ( ii ) edges denote relations between entity spans . 
An example of our annotation is given in Fig . 
1 . 
Three expert annotators are involved in this task , who were are in close communication during the entire annotation process to maximize annotation consistency . 
3The annotated data is available for research at https:// github.com/YiweiJiang2015/RISeC 4https://camel.abudhabi.nyu.edu/simmr/822 3.2 Annotation Structure Entity Types Action : Most verbs , their present / past participles and verb phrases fall in this category . 
In addition to the Action label , speciÔ¨Åc verbs also carry a Zero Anaphora Verb ( ZA V ) label ( see further ) . 
Food : Ingredients , spices ( salt , sugar , etc . 
) , intermediate products ( e.g. , ‚Äú the meat mixture ‚Äù ) . 
If a sequence of ingredients is involved in an action , we label each of them individually , as in Fig . 
1 . 
Tool : Appliances ( e.g. , oven ) , recipients ( e.g. , bowl ) , utensils ( e.g. , fork ) used to perform an action involved in the cooking process . 
Duration : Time interval for which an action lasts ( e.g. , ‚Äò 20 minutes ‚Äô , ‚Äò half an hour ‚Äô ) . 
Temperature : E.g. , ‚Äú 400 degrees F ‚Äù . 
Other : This label is used for entities that can not be attributed to any entity label above . 
Further , we also annotate subclauses that provide information on certain actions as ‚Äú entities ‚Äù . 
Thus , we abuse entity labeling to indicate them and thus limit their annotation to shallow parsing : Condition Clause : Sub - clauses led by conjunctions like ‚Äú until ‚Äù , ‚Äú till ‚Äù , ‚Äú when ‚Äù , ‚Äú before ‚Äù , usually expressing timing . 
Purpose Clause : InÔ¨Ånitives and sub - clauses led by for example ‚Äú so that ‚Äù , ‚Äú to make sure that ‚Äù . 
Relation Types Following the methodology of PropBank , we deÔ¨Åne a set of relations for the semantic roles in recipe instructions . 
These relations have the verb as origin and link an action to its arguments ( Arg * ) or modiÔ¨Åers ( ArgM * ) . 
For details on their meanings , see PropBank ‚Äôs annotation guidelines ( Babko - Malaya , 2005 ) . 
However , to make the annotating schema self - consistent and adaptive to the cooking domain , we create ( or extend ) verb frames that are not ( yet ) included by PropBank . 
E.g. , for the verb phrase ‚Äú beat in ‚Äù , we borrow the argument structure from its main verb , i.e. , ‚Äú beat ‚Äù . 
ArgPPT : Participant , used for the argument which undergoes a change of state or is being affected by an action . 
ArgGOL : Goal , destination where an action ends . 
ArgDIR : Direction , the source where an action starts from . 
E.g. , ‚Äú Remove the pan from oven to a rack ‚Äù where ‚Äú oven ‚Äù is ArgDIR of the action ‚Äú remove ‚Äù . 
ArgPRD : Predicate , used for the end product of an action . 
E.g. , ‚Äú Roll the cool dough into 3 - inch ball ‚Äù where the dough is transformed into ‚Äú 3 - inchballs ‚Äù , ArgPRD of the action ‚Äú roll ‚Äù . 
ArgPAG : Agent , the subject that performs an action . 
ArgM MNR : Manner , describing how or in what condition we execute an action . 
E.g. , in‚ÄúPreheat the oven at 340 degrees ‚Äù , the relation ArgM MNR linksAction ‚Äú preheat ‚Äù to Temperature ‚Äú 340 degrees F ‚Äù . 
ArgM LOC : Location where an action takes place . 
This notion is not restricted to physical locations , but abstract locations are being marked asArgM LOC as well . 
E.g. , in ‚Äú Beat 2 eggs in the Ô¨Çour ‚Äù , ArgM LOC linksAction ‚Äú beat ‚Äù to Food ‚Äú the Ô¨Çour ‚Äù ArgM TMP : Temporal relation between action and timing nodes ( Duration , Condition clause ) . 
ArgM PRP : Purpose relation between action and purpose clause nodes . 
ArgM INT : Instrument , e.g. , the utensil to accomplish the action . 
ArgM SIM : Simultaneous , linking two actions performed at the same time . 
E.g. , in ‚Äú Broil the lamb , moving pan so entire surface browns evenly ‚Äù , ArgM SIM links ‚Äú broil ‚Äù to ‚Äú moving ‚Äù . 
Zero Anaphora Rephrasing Zero anaphora is the phenomenon of implicit , unmentioned references to earlier concepts . 
Figure 1 gives two examples where explicit anaphors are manually added inside the brackets . 
The last sentence in Fig . 
1 would be ungrammatical without the unmentioned ‚Äú the crab mixture ‚Äù and ‚Äú the baked crab ‚Äù . 
In our annotations , we annotated 1,526 Zero Anaphora Verbs with candidate expressions for the zero anaphora , providing at least two alternatives : a succinct noun , as well as a more detailed noun phrase . 
4 Model We focus on two tasks : ( 1 ) joint entity recognition , relation extraction and zero anaphora identiÔ¨Åcation , and ( 2 ) zero anaphora description generation . 
Next we present our models for each . 
4.1 Entity recognition , relation extraction & zero anaphora identiÔ¨Åcation We use a span - based model , taking the input sequence of words as input , and passing it through 4 components : ( i ) word representation , ( ii ) span representation , ( iii ) entity recognition , and ( iv ) relation identiÔ¨Åcation . 
Word Representation : We use a BiLSTM as the823 base encoder . 
The inputs are vector representations of the sentence tokens obtained by concatenating pre - trained GLoVe embeddings ( Pennington et al . 
, 2014 ) and character representations ( using a CNN , ReLU and max pooling , as proposed by dos Santos and Guimar Àúaes , 2015 ) . 
Further , we also experimented with pre - trained BERT models ( Devlin et al . 
, 2019 ) instead of Glove embeddings . 
Span Representation : We enumerate all possible word spans from the input sentence and concatenate the aforementioned BiLSTM ( hleft , hright ) encoder outputs at Ô¨Årst ( f ) and last ( l ) end - point tokens of each span , together with its length ( elen ) to obtain a span representation ( si= ( hleft , f , hright , f , hleft , l , hright , l , elen ) ) . 
Entity Recognition & Zero Anaphora Verb IdentiÔ¨Åcation : We pass the selected span representations sithrough a feed - forward neural network ( FFNN ) yielding per - class scores for predicting entity types as well as binary Zero Anaphora Verb labels ( with kentity classes , the FFNN thus has k+ 1outputs ) . 
Relation IdentiÔ¨Åcation : The concatenation of two span representations ( si , sj ) is passed through another FFNN to derive per - class relation scores . 
Since this is quadratic , we only pass the top 20 % highest scored spans to the Relation FFNN : every span pair ( si , sj)is Ô¨Årst passed through a pruning FFNN , and only its top - scored pairs are pushed through the Relation FFNN . 
Training : For each recipe instance , the objective is to optimize the weighted sum of the negative log likelihood of span representation , entity classiÔ¨Åcation and relation identiÔ¨Åcation . 
We use Adam to optimize the model with learning rate 0.001 . 
4.2 Zero anaphora description generation For the generation task , we build a baseline model corresponding to the sequence - to - sequence architecture used in Bahdanau et al . 
( 2015 ) . 
The input is the entire recipe , which we pass to an LSTM encoder taking the pre - trained GloVE embedding , concatenated with a binary label indicating whether it is a zero anaphora verb ( ZA V ) , and ( optionally ) an entity type embedding if the token is of a given type . 
Since usually the target description that the decoder needs to generate is much shorter than the full recipe , we adopt bilinear attention ( Luong et al . 
, 2015 ) . 
The model is trained to minimize the negative log likelihood ofGlove BertbaseBertlarge Entity 89.8 91.7 92.6 Zero Anaphora Verb 89.1 89.0 89.8 Relation 65.5 67.1 67.5 Table 1 : Micro - F1 scores of models with Glove , Bert base and Bert large on the test set . 
Full Test set Count Prec . 
Recall F1 Food 3,232 92.5 95.9 94.2 Action 3,061 96.6 97.4 97.0 Tool 1,138 92.9 86.8 89.8 Condition clause 487 93.0 71.1 80.5 Duration 411 85.7 87.4 86.5 Temperature 381 87.4 89.3 88.4 Other 270 54.2 34.7 41.9 Purpose clause 147 78.0 59.2 67.2 Table 2 : Entity counts in full dataset and extraction results with Bert large on test set . 
an emitted token given the full input and predicted tokens . 
5 Experiments and results We split our RISeC dataset into 50 % training , 20 % development and 30 % test sets , using the same splits as SIMMR ( Jermsurawong and Habash , 2015 ) . 
We tune hyperparameters on the development set . 
Reported performance metrics are obtained on the test set . 
In general , our span - based model shows good performance in the extraction task , as shown in Table 1 . 
We obtain micro - F1 scores for the joint entity , zero anaphora verbs and relation identiÔ¨Åcation tasks of respectively 89.8 , 89.1 and 65.5 when using Glove word embeddings . 
With Bert large word encodings , performance consistently improves by 2.8 , 0.7 and 2.0 percentage points respectively , indicating the applicability of the general linguistic knowledge from Bert on a cooking - domain task . 
Individual entity and relation type performance is reported in Tables 2‚Äì3 . 
As expected , Table 2 shows that entity F1 scores are positively correlated with the occurrence frequency , except forDuration andTemperature , of which the Ô¨Åxed pattern is easy to learn . 
The high precision and recall of important entities like Food andAction shows promising potential of our model for downstream applications like a question answering system in smart kitchen settings . 
The F1824 Full Test set Count Prec . 
Recall F1 Arg PPT 3,196 94.1 69.3 79.8 Argument Arg GOL 557 79.6 35.8 49.1 Relations Arg DIR 91 93.9 34.5 50.4 Arg PRD 74 77.8 27.4 40.0 Arg PAG 25 0.0 0.0 0.0 ArgM TMP 884 91.7 33.2 48.7 ArgM LOC 515 87.8 49.7 63.3 ModiÔ¨Åer ArgM MNR 432 86.7 35.6 50.1 Relations ArgM PRP 137 85.2 9.1 15.8 ArgM SIM 92 66.7 11.1 18.6 ArgM INT 73 77.4 20.3 31.8 Table 3 : Relation counts in full dataset and extraction results with Bert large on test set . 
scores of relation predictions in Table 3 show that the imbalanced distribution of relation types causes detection of several relations to be difÔ¨Åcult , e.g. , the low recall rates for ArgPAG andArgM PRP . 
Future work should address this , e.g. , using a larger dataset ( or pretraining on non - recipe corpora ) . 
While the detection of zero anaphora verbs ( ZA V ) performs well , our Seq2seq based description generation largely failed , with very low performance and oftentimes outputting the same descriptions ( e.g. , ‚Äú mixture ‚Äù or ‚Äú chicken ‚Äù ) . 
In hindsight , given the limited dataset size ( order of 1.5k ZA V occurences in the full dataset ) and the typically large training dataset needed for seq2seq models , this is not entirely unexpected . 
Further work on this task is clearly required . 
6 Conclusion and Future Work This paper introduced RISeC , a dataset for extracting structural information and resolving zero anaphora from unstructured recipes . 
The corpus consists of 260 recipes from SIMMR and provides semantic graph annotations of ( i ) recipe - related entities , ( ii ) generic verb relations ( from PropBank ) connecting these entities , ( iii ) zero anaphora verbs having implicit arguments , and ( iv ) textual descriptions of those implicit arguments . 
We reported on our work - in - progress with two baseline models using our corpus : ( i ) a neural span - based model extracting entities , zero anaphora verbs and relations , and ( ii ) a sequence - to - sequence attention model generating noun phrases for zero anaphora verbs . 
We plan to continue working in this direction , making the dataset larger and more Ô¨Ånegrained , and especially , to investigate how itcan be leveraged for human - machine interaction experiments . 
Acknowledgments The Ô¨Årst author was supported by China Scholarship Council ( 201806020194 ) . 
This research received funding from the Flemish Government under the ‚Äú Onderzoeksprogramma ArtiÔ¨Åci ¬®ele Intelligentie ( AI ) Vlaanderen ‚Äù programme . 
We would like to thank anonymous reviewers who helped to improve the draft . 
Abstract Studies on grammatical error correction ( GEC ) have reported the effectiveness of pretraining a Seq2Seq model with a large amount of pseudodata . 
However , this approach requires time - consuming pretraining for GEC because of the size of the pseudodata . 
In this study , we explore the utility of bidirectional and auto - regressive transformers ( BART ) as a generic pretrained encoder ‚Äì decoder model for GEC . 
With the use of this generic pretrained model for GEC , the time - consuming pretraining can be eliminated . 
We Ô¨Ånd that monolingual and multilingual BART models achieve high performance in GEC , with one of the results being comparable to the current strong results in English GEC . 
Our implementations are publicly available at GitHub1 . 
1 Introduction Grammatical error correction ( GEC ) is the automatic correction of grammatical and other language - related errors in text . 
Most works regard this task as a translation task and use encoder ‚Äì decoder ( Enc ‚Äì Dec ) architectures to convert ungrammatical sentences to grammatical ones . 
This Enc ‚Äì Dec approach often does not require linguistic knowledge of the target language . 
Strong Enc ‚Äì Dec models for GEC are pretrained with a large amount of artiÔ¨Åcially generated data , commonly referred to as ‚Äò pseudodata ‚Äô , that is created by introducing artiÔ¨Åcial error to a monolingual corpus . 
Hereafter , pretraining using pseudodata aimed at the GEC task is referred to as task - oriented pretraining ( Kiyono et al . 
, 2019 ; Grundkiewicz et al . 
, 2019 ; N ¬¥ aplava and Straka , 2019 ; Kaneko et al . 
, 2020 ) . 
For example , Kiyono et al . 
( 2019 ) generated a pseudo corpus using back - translation and ‚àóCurrently working at Retrieva , Inc. 1https://github.com/Katsumata420/generic-pretrainedGECachieved strong results for English GEC . 
N ¬¥ aplava and Straka ( 2019 ) generated a pseudo corpus by introducing artiÔ¨Åcial errors into monolingual corpora and achieved the best scores for GEC in several languages by adopting the methods proposed by Grundkiewicz et al . 
( 2019 ) . 
These task - oriented pretraining approaches require extensive use of a pseudo - parallel corpus . 
SpeciÔ¨Åcally , Grundkiewicz et al . 
( 2019 ) used 100 M ungrammatical and grammatical sentence pairs , while Kiyono et al . 
( 2019 ) and Kaneko et al . 
( 2020 ) used 70 M sentence pairs , which required time - consuming pretraining of GEC models using the pseudo corpus . 
In this study , we determined the effectiveness of publicly available pretrained Enc ‚Äì Dec models for GEC . 
SpeciÔ¨Åcally , we investigated pretrained models without the need for pseudodata . 
We explored a pretrained model proposed by Lewis et al . 
( 2020 ) called bidirectional and auto - regressive transformers ( BART ) . 
Liu et al . 
( 2020 ) also proposed multilingual BART . 
These models were pretrained by predicting the original sequence , given a masked and shufÔ¨Çed sentence . 
The motivation for using these models for GEC was that it achieved strong results for several text generation tasks , such as summarization ; we refer to it as a generic pretrained model . 
We used generic pretrained BART models to compare with GEC models using a pseudo - corpus approach ( Kiyono et al . 
, 2019 ; Kaneko et al . 
, 2020 ; N ¬¥ aplava and Straka , 2019 ) . 
We conducted GEC experiments for four languages : English , German , Czech , and Russian . 
The Enc ‚Äì Dec model based on BART achieved results comparable with those of current strong Enc ‚Äì Dec models for English GEC . 
The multilingual model also showed high performance in other languages , despite only requiring Ô¨Åne - tuning . 
These results suggest that BART can be used as a simple baseline827 for GEC . 
2 Previous Work The Enc ‚Äì Dec approach for GEC often uses the task - oriented pretraining strategy . 
For example , Zhao et al . 
( 2019 ) and Grundkiewicz et al . 
( 2019 ) reported that pretraining of the Enc ‚Äì Dec model using a pseudo corpus is effective for the GEC task . 
In particular , they introduced word- and character - level errors into a sentence in monolingual corpora . 
They developed a confusion set derived from a spellchecker and randomly replaced a word in a sentence . 
They also randomly deleted a word , inserted a random word , and swapped a word with an adjacent word . 
They performed these same operations , i.e. , replacing , deleting , inserting , and swapping , for characters . 
The pseudo corpus made by the above methods consisted of 100 M training samples . 
Our study aims to investigate whether the generic pretrained models are effective for GEC , because pretraining with such a large corpus is time - consuming . 
N¬¥aplava and Straka ( 2019 ) adopted Grundkiewicz et al . 
( 2019 ) ‚Äôs method for several languages , including German , Czech , and Russian . 
They trained a Transformer ( Vaswani et al . 
, 2017 ) with pseudo corpora ( 10 M sentence pairs ) , and achieved current state - of - the - art ( SOTA ) results for German , Czech , and Russian GEC . 
We compared their results with those of the generic pretrained model to conÔ¨Årm whether the model was effective for GEC in several languages . 
Kiyono et al . 
( 2019 ) explored the generation of a pseudo corpus by introducing random errors or using back - translation . 
They reported that a taskoriented pretraining with back - translation data and character errors is better than that with pseudodata based on random errors . 
Kaneko et al . 
( 2020 ) combined Kiyono et al . 
( 2019 ) ‚Äôs pretraining approach with BERT ( Devlin et al . 
, 2019 ) and improved Kiyono et al . 
( 2019 ) ‚Äôs results . 
SpeciÔ¨Åcally , Kaneko et al . 
( 2020 ) Ô¨Åne - tuned BERT with a grammatical error detection task . 
The Ô¨Åne - tuned BERT outputs for each token were combined with the original tokens as a GEC input . 
Their study is similar to our research in that both studies use publicly available generic pretrained models to perform GEC . 
The difference between these studies is that Kaneko et al . 
( 2020 ) used the architecture of the pretrained model as an encoder . 
Therefore , their method still requires pretraining with a largeamount of pseudodata . 
The current SOTA approach for English GEC uses the sequence tagging model proposed by Omelianchuk et al . 
( 2020 ) . 
They designed tokenlevel transformations to map input tokens to target corrections to produce training data . 
The sequence tagging model then predicts the transformation corresponding to the input token . 
We do not attempt to make a comparison with this approach , as the purpose of our study is to create a strong GEC model without using pseudodata or linguistic knowledge . 
3 Generic Pretrained Model BART ( Lewis et al . 
, 2020 ) is pretrained by predicting an original sequence , given a masked and shufÔ¨Çed sequence using a Transformer . 
They introduced masked tokens with various lengths based on the Poisson distribution , inspired by SpanBERT ( Joshi et al . 
, 2020 ) , at multiple positions . 
BART is pretrained with large monolingual corpora ( 160 GB ) , including news , books , stories , and web - text domains . 
This model achieved strong results in several generation tasks ; thus , it is regarded as a generic model . 
They released pretrained models using English monolingual corpora for several tasks , including summarization , which we used for English GEC . 
Liu et al . 
( 2020 ) proposed multilingual BART ( mBART ) for a machine translation task , which we used for GEC of several languages . 
The latter model was trained using monolingual corpora for 25 languages simultaneously . 
They used a special token for representing the language of a sentence . 
For example , they added < de_DE > and < ru_RU > into the initial token of the encoder and decoder for De ‚Äì Ru translation . 
To Ô¨Åne - tune mBART for German , Czech , and Russian GEC , we set the target language for the special token referring to that language . 
4 Experiment 4.1 Settings Common Settings . 
As presented in Table 1 , we used learner corpora , including BEA2(Bryant et al . 
, 2019 ; Granger , 1998 ; Mizumoto et al . 
, 2011 ; Tajiri et al . 
, 2012 ; Yannakoudakis et al . 
, 2011 ; Dahlmeier et al . 
, 2013 ) , JFLEG ( Napoles et al . 
, 2017 ) , and CoNLL-14 ( Ng et al . 
, 2014 ) data for 2BEA corpus is made of several corpora . 
Details can be found in Bryant et al . 
( 2019).828 lang Corpus Train Dev Test BEA 1,157,370 4,384 4,477 En JFLEG - - 747 CoNLL-2014 - - 1,312 De Falko+MERLIN 19,237 2,503 2,337 Cz AKCES - GEC 42,210 2,485 2,676 Ru RULEC - GEC 4,980 2,500 5,000 Table 1 : Data statistics . 
English ; Falko+MERLIN data ( Boyd et al . 
, 2014 ) for German ; AKCES - GEC ( N ¬¥ aplava and Straka , 2019 ) for Czech ; and RULEC - GEC ( Rozovskaya and Roth , 2019 ) for Russian . 
Our models were Ô¨Åne - tuned using a single GPU ( NVIDIA TITAN RTX ) , and our implementations were based on publicly available code3 . 
We used the hyperparameters provided in some previous works ( Lewis et al . 
, 2020 ; Liu et al . 
, 2020 ) , unless otherwise noted . 
The scores excluding the ensemble method were averaged in Ô¨Åve Ô¨Åne - tuned experiments with random seeds . 
English . 
Our setting for the English datasets was almost the same as that of Kiyono et al . 
( 2019 ) . 
We extracted the training data from BEA - train for English GEC . 
Similar to Kiyono et al . 
( 2019 ) , we did not use the unchanged sentences in the source and target sides ; thus , the training data consisted of 561,525 sentences . 
We used BEA - dev to determine the best model . 
We trained the BART - based models by using bart.large . 
This model was proposed for the summarization task , which required some constraints in inference to ensure appropriate outputs ; however , we did not impose any constraints because our task was different . 
We applied byte pair encoding ( BPE ) ( Sennrich et al . 
, 2016 ) to the training data for the BART - based model by using the BPE model of Lewis et al . 
( 2020 ) . 
We used the M2scorer ( Dahlmeier and Ng , 2012 ) and GLEU ( Napoles et al . 
, 2015 ) for CoNLL-14 and JFLEG , respectively , and used the ERRANT scorer ( Bryant et al . 
, 2017 ) for BEAtest . 
We compared these scores with strong results ( Kiyono et al . 
, 2019 ; Kaneko et al . 
, 2020 ) . 
German , Czech , and Russian . 
The dataset settings in this study were almost the same as those 3BART , mBART : https://github.com/pytorch/fairseqused by N ¬¥ aplava and Straka ( 2019 ) for each language . 
We used ofÔ¨Åcial training data and decided the best model by using the development data . 
In addition , we trained the mBART - based models for German , Czech , and Russian GEC . 
We usedmbart.cc25 for the mBART - based models . 
For the mBART - based model , we followed Liu et al . 
( 2020 ) ; we detokenized4the GEC training data for the mBART - based model and applied SentencePiece ( Kudo and Richardson , 2018 ) with the SentencePiece model shared by Liu et al . 
( 2020 ) . 
Using this preprocessing , the input sentence may not represent grammatical information , compared with the sentence tokenized using a morphological analysis tool and subword tokenizer . 
However , what preprocessing is appropriate for GEC is beyond this paper ‚Äôs scope and will be treated as future work . 
For evaluation , we tokenized the outputs after recovering the subwords . 
Then , we used a spaCy - based5tokenizer for German6and Russian7 , and the MorphoDiTa tokenizer8for Czech . 
Moreover , the M2scorer was used for each language . 
We compared these scores with the current SOTA results ( N ¬¥ aplava and Straka , 2019 ) . 
4.2 Results English . 
Table 2 presents the results of the English GEC task . 
When using a single model , the BART - based model is better than the model proposed by Kiyono et al . 
( 2019 ) , and the results are comparable to those reported by Kaneko et al . 
( 2020 ) in terms of CoNLL-14 and BEA - test . 
Kiyono et al . 
( 2019 ) and Kaneko et al . 
( 2020 ) incorporated several techniques to improve the accuracy of GEC . 
To compare these models , we experimented with an ensemble of Ô¨Åve models . 
Our ensemble model was slightly better than our single model , but worse than the ensemble models by Kiyono et al . 
( 2019 ) and Kaneko et al . 
( 2020 ) . 
The BART - based model along with the ensemble model achieved results comparable to current strong results despite only requiring Ô¨Åne - tuning of the BART model . 
We believe that the reason for the ineffectiveness of the ensemble method is that the Ô¨Åve models are not signiÔ¨Åcantly different as the 4We used detokenizer.perl in the Moses script ( Koehn et al . 
, 2007 ) . 
5https://spacy.io 6We used the built - in de model . 
7https://github.com/aatimofeev/spacy russian tokenizer 8https://github.com/ufal/morphodita829 CoNLL-14 ( M2 ) JFLEG BEA - test P R F0.5 GLEU P R F0.5 Kiyono et al . 
( 2019 ) 67.9/73.3 44.1/44.2 61.3/64.7 59.7/61.2 65.5/74.7 59.4/56.7 64.2/70.2 Kaneko et al . 
( 2020 ) 69.2/72.6 45.6/46.4 62.6/65.2 61.3/62.0 67.1/72.3 60.1/61.4 65.6/69.8 BART - based 69.3/69.9 45.0/45.1 62.6/63.0 57.3/57.2 68.3/68.8 57.1/57.1 65.6/66.1 Table 2 : English GEC results . 
Left and right scores represent single and ensemble model results , respectively . 
Bold scores represent the best score in the single models , and underlined scores represent the best overall score . 
P R F0.5 DeN¬¥aplava and Straka ( 2019 ) 78.21 59.94 73.31 mBART - based 73.97 53.98 68.86 CzN¬¥aplava and Straka ( 2019 ) 83.75 68.48 80.17 mBART - based 78.48 58.70 73.52 N¬¥aplava and Straka ( 2019 ) 63.26 27.50 50.20 Ru mBART - based 32.13 4.99 15.38 with pseudo corpus 53.50 26.35 44.36 Table 3 : German , Czech , and Russian GEC results . 
These models are not an ensemble of multiple models . 
initial weights are the same as those of the BART model , and seeds only affect minor changes , such as training data order , and so on . 
German , Czech , and Russian . 
Table 3 presents the results for German , Czech , and Russian GEC . 
In the German GEC task , the mBART - based model achieves 4.45 F0.5points lower than the model by N ¬¥ aplava and Straka ( 2019 ) . 
This may be because N ¬¥ aplava and Straka ( 2019 ) pretrains the GEC model with only the target language , whereas mBART is pretrained with 25 languages , resulting in the information of other languages being included as noise . 
In the Czech GEC task , the mBART - based model achieves 6.65 F0.5points lower than the model by N ¬¥ aplava and Straka ( 2019 ) . 
Similar to the case of the German GEC results , we suppose that mBART includes noisy information . 
Considering Russian GEC , the mBART - based model shows much lower scores than N ¬¥ aplava and Straka ( 2019 ) ‚Äôs model . 
This may be because the training data for Russian GEC are scarce compared to those of German or Czech . 
To investigate the effect of corpus size , we additionally trained the mBART model with a 10 M pseudo corpus , using the method proposed by Grundkiewicz et al . 
( 2019 ) , and Ô¨Åne - tuned it with the learner corpus to compensate for the low - resource scenario . 
The results presented in Table 3 support our hypothesis . 
Kaneko et al . 
( 2020 ) BART - based Error Type P R F0.5 P R F0.5 PUNCT 74.1 52.7 68.5 79.2 59.0 74.1 DET 73.7 72.9 73.5 76.3 71.1 75.2 PREP 73.4 69.1 72.5 71.2 64.8 69.9 ORTH 86.9 62.9 80.8 84.2 52.9 75.3 SPELL 83.1 79.5 82.3 84.7 55.2 76.5 Table 4 : BEA - test scores for the top Ô¨Åve error types , except for OTHER . 
Kaneko et al . 
( 2020 ) and BARTbased are ensemble models . 
Bold scores represent the best score for each error type . 
5 Discussion BART as a simple baseline model . 
According to the German and Czech GEC results , the mBART - based model , in which we only Ô¨Åne - tuned the pretrained mBART model , achieves comparable scores with SOTA models . 
In other words , mBART - based models are considered to show sufÔ¨Åciently high performance for several languages without using a pseudo corpus . 
These results indicate that the mBART - based model can be used as a simple GEC baseline for several languages . 
Performance comparison for each error type . 
We compare the BART - based model with Kaneko et al . 
( 2020 ) ‚Äôs model for common error types using a generic pretrained model . 
Table 4 presents the results for the top Ô¨Åve error types in BEA - test . 
According to these results , BART - based is superior to Kaneko et al . 
( 2020 ) in PUNCT and DET errors ; in particular , PUNCT is 5.6 F0.5points better . 
BART is pretrained to correct the shufÔ¨Çed and masked sequence , so that this model learns to place punctuation adequately . 
In contrast , Kaneko et al . 
( 2020 ) uses an encoder that is not pretrained with correcting shufÔ¨Çed sequences . 
Conversely , Kaneko et al . 
( 2020 ) report better results for other errors , except for DET . 
Regarding ORTH and SPELL , their model is more than 5F0.5points better than the BART - based one . 
It is difÔ¨Åcult for the BART - based model to cor-830 rect these errors because BART uses shufÔ¨Çed and masked sequences as noise in pretraining ; not using character - level errors . 
Kaneko et al . 
( 2020 ) introduce character errors into a pseudo corpus as task - oriented Enc ‚Äì Dec pretraining ; this is the reason why the BART - based model is inferior to Kaneko et al . 
( 2020 ) in these errors . 
6 Conclusion We introduced a generic pretrained Enc ‚Äì Dec model , BART , for GEC . 
The experimental results indicated that BART better initialized the Enc ‚Äì Dec model parameters . 
The Ô¨Åne - tuned BART achieved remarkable results , which were comparable to the current strong results in English GEC . 
Indeed , the monolingual BART seems to be more effective for GEC than the model with a multilingual setting . 
However , although it is not as good as SOTA , Ô¨Åne - tuned mBART exhibited high performance in other languages . 
This implies that BART is a simple baseline model for pretraining GEC methods because it only requires Ô¨Åne - tuning as training . 
Acknowledgements We thank the anonymous reviewers for their insightful comments . 
This work has been partly supported by the programs of the Grant - in - Aid for ScientiÔ¨Åc Research from the Japan Society for the Promotion of Science ( JSPS KAKENHI ) Grant Numbers 19K12099 and 19KK0286 . 
Abstract Mandarin Alphabetical Word ( MAW ) is one indispensable component of Modern Chinese that demonstrates unique code - mixing idiosyncrasies inÔ¨Çuenced by language exchanges . 
Yet , this interesting phenomenon has not been properly addressed and is mostly excluded from the Chinese language system . 
This paper addresses the core problem of MAW identiÔ¨Åcation and proposes to construct a large collection of MAWs from Sina Weibo ( SMAW ) using an automatic web - based technique which includes rule - based identiÔ¨Åcation , informaticsbased extraction , as well as Baidu search engine validation . 
A collection of 16,207 qualiÔ¨Åed SMAWs are obtained using this technique along with an annotated corpus of more than 200,000 sentences for linguistic research and applicable inquiries . 
1 Introduction Mandarin Alphabetic Words ( MAWs ) , also known as lettered words ( Liu , 1994 ) or code - mixing words ( Nguyen and Cornips , 2016 ) , are usually formed by Latin , Greek , Arabic alphabets in combination with Chinese characters , e.g. ‚Äú X- ÂÖâ / XÂ∞Ñ Á∫ø‚Äù,X - ray . 
Although pure alphabets ( e.g. ‚Äú NBA ‚Äù ) used in Chinese context have also been regarded as MAWs in some previous work ( Liu , 1994 ; Huang and Liu , 2017 ) , they are more like switching - codes that retain the orthography and linguistic behaviors of the original language , instead of showing typical Chinese lexical characteristics . 
It is noteworthy that MAWs shall be taken as a code - mixing phenomenon instead of code - switching as a MAW is still a Chinese word which is not switched into another language . 
Therefore , in this work , MAWS refer to the combined type which encodes both alphabet(s ) and Chinese character(s ) in one word , such as ‚Äú A Âûã‚Äù,A - type , ‚Äú PO‰∏ª‚Äù,post owner , and ‚Äú Œ≥Á∫ø‚Äù,Gamma Ray .It is linguistically - interesting and applicablysigniÔ¨Åcant to investigate MAWs due to two main reasons . 
First , A MAW maintains part of the Chinese characteristics in morphology , phonology and orthography ( e.g. ‚Äú PK Ëøá",player killed , past tense ) . 
Meanwhile , it also demonstrates some properties of the foreigner language ( e.g. ‚Äú Áª¥ÁîüÁ¥†ing " , supplementing Vitamin , progressive ) ) , providing a unique lexical resource for studying morphophonological idiosyncrasies of code - mixing words . 
Second , MAWs serve as an indispensable part of people ‚Äôs daily vocabulary , especially under the rapid development of social media communication . 
Yet , being out - liars of the Chinese lexicon , they can cause problems to existing word segmentation / new word extraction tools that are trained on traditional words ( Chen and Liu , 1992 ; Xue and Shen , 2003 ) . 
Consider the following example : E1 : PO‰∏ª ‰∏ª ‰∏ª‰πü‰∏çÁü•ÈÅìÈìæÊé•Ë¢´Âêû‰∫Ü ( The post owner did n‚Äôt know that the link has been hacked off ) Seg : PO / ‰∏ª ‰∏ª ‰∏ª / ‰πü / ‰∏ç / Áü•ÈÅì / ÈìæÊé• / Ë¢´ / Âêû / ‰∫Ü Golden Seg : PO‰∏ª ‰∏ª ‰∏ª / ‰πü / ‰∏ç / Áü•ÈÅì / ÈìæÊé• / Ë¢´ / Âêû / ‰∫Ü The sentence in E1 ( example 1 ) is segmented using Stanford Parser ( Manning et al . 
, 2014 ) which fails to identify the word ‚Äú PO ‰∏ª‚Äù,post owner and breaks it into two parts . 
The same type of error also occurs in other popular segmentation tools . 
Although Huang et al . 
( 2007 ) proposed a radical method of word segmentation to meet the challenge , using a concept of classifying a string of character - boundaries into either word - boundaries or non - word - boundaries , their work did not address the cases of code - mixing words , whose word boundaries can also fall on foreigner alphabets . 
Some other methods mainly rely on unsupervised methods ( Chang and Su , 1997 ) or simple statistical methods based on N - gram frequencies , with indices of collocation and co - occurrence ( Chang833 and Su , 1997 ; Chen and Ma , 2002 ; Dias , 2003 ) . 
However , these works are mainly designed for new words of pure Chinese characters , which are not applicable to MAWs . 
In this paper , we address the issue of MAW identiÔ¨Åcation and present the construction of theSina MA W lexicon ( SMA W ) ( available at https://github.com/Christainx/SMAW ) using a fully automatic information extraction technique . 
The quality of the MAWs ( accurateness and inter - rater agreement ) are rated by three experts for system evaluation . 
Compared to previous resources , this dataset provides an unprecedentedly large , balanced , and structured MAWs as well as a MAW annotated corpus . 
With the availability of a comprehensive MAWs as a valuable Chinese lexical resource as well as corpus resource , it shall beneÔ¨Åt many Chinese language processing tasks which need to deal with code - mixing , such as word segmentation and information extraction . 
2 Related Works The earliest MAW was probably ‚Äú X Â∞ÑÁ∫ø / XÂÖâ‚Äù,X - ray , which was ofÔ¨Åcially documented in 1903 ( Zhang , 2005 ) . 
For over 60 years , such words had been largely conÔ¨Åned to technical and medical domains with very few lexicalized and registered terms in dictionaries . 
The authoritative Xiandai Hanyu Cidian / XianHan ( ‚Äú Áé∞‰ª£Ê±âËØ≠ËØçÂÖ∏ ‚Äù ) , for instance , initiated a separate section to include 39 MAW entries in 1996 . 
This list has grown rapidly with each subsequent XianHan dictionary edition , reaching 239 entries by the 2012 edition . 
This in turn generated a Ô¨Çurry of related linguistic studies , which were mainly focused on lexicological and language policy issues ( Su and Wu , 2013 ; Zhang , 2013 ) . 
Some works have dealt with the emergence of MAWs in light of globalization , placing them in a socio - cultural context ( Kozha , 2012 ; Miao , 2005 ) , and a few are also interested in studying the morpho - lexical status of MAWs ( Lun , 2013 ; Riha and Baker , 2010 ; Riha , 2010 ) . 
In the age of Internet and social media , the scale of MAWs , their extraction methods , and resources of MAWs have changed drastically since the last decade . 
For example , Zheng et al . 
( 2005 ) extracted a small set of MAWs with manual validation from the corpus of People ‚Äôs Daily ( Year 2002 ) . 
Jiang and Dang ( 2007 ) extracted 93 MAWs ( out of 1,053 new domain - speciÔ¨Åc terms ) using a statistical approach with rule - based validation . 
Recently , Huangand Liu ( 2017 ) extracted over 1,157 MAWs from both the Sinica Corpus ( Chen et al . 
, 1996 ) and the Chinese Gigaword Corpus ( Huang , 2009 ) based on manually segmented MAWs in the corpora . 
Although they have extracted 60,000 tokens with alphabetical letters . 
However , the list mainly includes pure alphabets those are indeed switching codes of other languages . 
In our study , these pure code - switching words are excluded according to our deÔ¨Ånition . 
Their work has established a taxonomy of distributional patterns of alphabetical letters in MAWs and found that typical MAWs follow Chinese modiÔ¨Åer - modiÔ¨Åed ( head ) morphological rule and the most frequent and productive pattern is alphabetical letter+ mandarin character ( AC ) , such astype B in the form of ‚Äú B Âûã ‚Äù . 
Besides the above investigations , MAWs have not been identiÔ¨Åed in a systemic and automatic way . 
The problem of identifying MAWs can be generalized as an issue of new / unknown / out - ofvocabulary word extraction ( code - mixing Chinese words in particular ) ( Chen and Ma , 2002 ; Zhang et al . 
, 2010 ) . 
A commonly adopted way of identifying a new word usually rely on word segmentation at the Ô¨Årst step and then map the valid MAWs to an existing dictionary . 
Those not mapped in the dictionary will be identiÔ¨Åed as new words . 
This is actually problematic for identifying MAWs ( cf . 
example in Section 1 ) . 
In addition , previous studies mainly extract MAWs from manually segmented newspapers in pre-1990s ( Huang and Liu , 2017 ) . 
Hence , the resources are domain - constrained and usage - outdated . 
3 Construction of SMA W To address the bias in previous works , we propose to collect an MAW list using social - media text commonly commonly available on Sina Weibo platform ( Weibo for short , or micro - blogs ) , a near - natural context . 
Weibo is one of the most popular social media platform in China with over 400 million active users on monthly basis . 
This platform becomes the enabler for generating tons of online data , which can serve as a huge Web corpus . 
The raw dataset crawled from Weibo consists of over 226 million posts ( around 20 gigabytes data ) . 
On the other hand , as there are many debates among linguists about the deÔ¨Ånition of a MAW ( Ding et al . 
, 2017 ; Liu , 1994 ; Tan et al . 
, 2005 ; Xue , 2007 ; Liu , 2002 ) , this work uses a datadriven statistical approach as well as leveraging834 on search engine hits to exclude pseudo - MAWs of low - vitality . 
Details of the methodology are given in the next section . 
Figure 1 : The framework of SMAW construction Figure 1 depicts the framework of SMAW construction . 
Collecting the SMAW dataset is carried out through a two stage process : Candidate Extraction andCandidate Filtering . 
In our system , Candidate Extraction uses an alphabet - anchored brute - force extraction of N - grams tokens which contains both alphabets and Chinese . 
To eliminate as many false positive cases as possible , Candidate Filtering uses three methods to remove noisy candidates using ( 1 ) Rule - based ReÔ¨Ånement , ( 2 ) Informatics - based Elimination , as well as ( 3 ) Search Engine Validation . 
In rule - based reÔ¨Ånement , a number of rules are selected as preliminary reÔ¨Ånement for Candidate Filtering . 
These rules are easy implemented and fast in execution . 
Then , in informatics - based elimination , PMI ( Point - wise Mutual Information ) and entropy are calculated to select candidates of high co - occurrence rate and informative Ô¨Çexibility . 
Using informatics - based methods can greatly help narrow down the scope of MAW candidates and remove false positive cases . 
Lastly , search engine based validation is adopted to Ô¨Ålter out low - vitality terms based on user links . 
This intellectual agent provide use cases about a candidate word as extra evidence . 
Details of these steps are described in the following subsections.3.1 Rule - based ReÔ¨Ånement Brute - force based Candidate Extraction can ensure highest recall . 
Yet , it can create a substantial list of false positive candidates , such as the subcomponent of a positive case : ‚Äú Âï¶AÊ¢¶ ‚Äù , whose correct MAW should be ‚Äú ÂìÜÂï¶AÊ¢¶‚Äù,Doraemon ; and the under segmented token : ‚Äú A ËÇ° / ÂèçÂºπ‚Äù,rally of Shanghai SE Composite Index , although the correct MAW should be ‚Äú A ËÇ°‚Äù,Shanghai SE Composite Index , etc . 
Below is a typical example of a user post in this dataset which includes a number of web - speciÔ¨Åc linguistic usages . 
E2 : # BMWËµõËΩ¶Á∫™ÂΩïÁâá # # ‰∫öÊ¥≤ÂÖ¨Ë∑ØÊë©ÊâòÈî¶Ê†áËµõÁè†Êµ∑Á´ôÂÖ®ËÆ∞ÂΩï # @UNIQ - Áéã‰∏ÄÂçöhttp://t.cn / EPdahkI ( # BMW Racing Documentary#Records Zhuhai ( in Asian Highway Motorcycle Championship . 
@AX12FZ32 http://t.cn/EPdahkI ) As shown in E2 , among all alphabetical chunks , many candidates are URL links , tags related to topics ( surrounded by # ) , or user names ( introduced by the ‚Äú @ ‚Äù symbol ) . 
These alphabetical sequences is noise for MAWS and should be readily excluded from the Ô¨Ånal data using some simple rules . 
other false MAW candidates also demonstrate obvious patterns . 
For example , candidates of emoji ( e.g. ‚Äú QAQ ‚Äù , ‚Äú LOL ‚Äù , ‚Äú :P ‚Äù , ‚Äú T_T ‚Äù ) are transformed symbols that encode no lexical meanings and shall be eliminated from the MAW list . 
Using a set of 9 different pattern - based rules to Ô¨Ålter out these unambiguous noises can largely reduce noisy data without compromising the coverage of the MAW lexicon . 
Detailed description of these patterns shall be introduced in Section 4.1 . 
3.2 Informatics - based Elimination As will be shown in the evaluation that even after Rule - based ReÔ¨Ånement , the candidate list it is still too large to be correct even by common sense . 
Informatics - based elimination works on this set of candidates to further remove noise . 
Term - frequency ( TF ) is a commonly used metric to Ô¨Ålter out low - occurrence candidates . 
However , using TF alone is insufÔ¨Åcient to identify MAWs . 
For instance , both ‚Äú A ËÇ°‚Äù,Shanghai SE Composite Index and ‚Äú AËÇ° / ÂèçÂºπ‚Äù,rally of Shanghai SE Composite Index have high TF but only ‚Äú A ËÇ° ‚Äù is a valid MAW . 
In this work , informatics - based methods are used to automatically Ô¨Ålter the negative cases , including PMI for measuring the internal cohesion,835 and entropy for measuring the external uncertainty of the candidates . 
Point - wise mutual information ( PMI ) is proposed by Bouma ( 2009 ) to measure the cooccurrence probability of two variables . 
It is used to measure the internal ‚Äú Ô¨Åxedness ‚Äù of a word . 
Let wbe an MAW candidate that consists of two componentsc1,c2 . 
The PMI of wwith respect to c1and c2can be calculated via Formula 1 given below . 
PMI ( c1;c2 ) = ‚àílog(p(c1 , c2 ) p(c1)‚àóp(c2 ) ) ( 1 ) In practice , at least one component , denoted as camust contain alphabet character(s ) . 
If wconsists of more than three components , we use the combination coordinated by ca . 
For example , ‚Äú ÂìÜ Âï¶ / A / Ê¢¶‚ÄùDoraemon can be computed by using ‚Äú ÂìÜÂï¶A / Ê¢¶ ‚Äù and ‚Äú ÂìÜÂï¶ / AÊ¢¶ ‚Äù . 
Formula 1 can be extended to Formula 2 to handle three components . 
PMI ( w ) = min ( PMI ( c1;ca ) , PMI ( ca;c2 ) ) ( 2 ) The threshold of PMI is experimentally set . 
Another dimension for identifying word boundaries is to use information entropy of its collocation environment . 
As proposed by He and Jun - Fang ( 2006 ) , information entropy can be used to measure the uncertainty ( Ô¨Çexibility ) of a candidate ‚Äôs environment , the larger the more Ô¨Çexible , and the more likely the candidate being a word . 
Consider the negative case of ‚Äú Á¥†C ‚Äù which only occurs in the context of ‚Äú Áª¥ÁîüÁ¥†C‚Äù,Vitamin C ( entropy in this case is low ) . 
In contrast , the positive case ‚Äú Áª¥ÁîüÁ¥†C ‚Äù occur in many different contexts : ‚Äú Ë°•ÂÖÖ / Áª¥ÁîüÁ¥†C ‚Äù , Take Vitamin C , ‚Äú È´òÂâÇÈáè / Áª¥ÁîüÁ¥†C‚Äù,High - dosage Vitamin C , ‚Äú Áª¥ÁîüÁ¥†C / ÂØπ / ÊÑüÂÜí / ÊúâÊïà‚Äù,Vitamin C copes with colds , etc . 
( entropy in this case is high ) . 
Letchandctbe the respective head and tail components surrounding w. The head entropy of w , denoted byH(h ) , is deÔ¨Åned by Formula 3 . 
The tail entropyH(t)can be obtained similarly . 
Based on Formula 3 , the Ô¨Ånal entropy of wis obtained by min ( H(h),H(t ) ) . 
H(h ) = ‚àí/summationdisplay p(ch)i‚àólog(p(ch)i ) ( 3 ) 3.3 Search Engine Validation Search Engine Validation aims to further Ô¨Ålter out candidate MAWs which are either less frequently used or in proper word forms that are not necessarily meaningful as lexical terms . 
A search engine such as Google , Bing and Baidu provide access toa large knowledge base to validate the semantic information of a MAW candidate . 
Active MAW candidates with more links are more likely to carry proper semantic meanings . 
semantic information can help to exclude non - lexicon candidates . 
For instance , " UNIQ- Áéã‰∏ÄÂçö " , refers to Wang Yi Bo , a famous Chinese actor in the band " UNIQ " . 
The features of this false candidate can pass previous Ô¨Åltering methods perfectly . 
This indicates the need for a more intelligent validation scheme . 
As the data source in this work is Sina Weibo , it is more appropriate to use Baidu , the most popular search engine in China , as the knowledge agent for retrieving the validation evidence of the remaining candidates . 
Figure 2 is the Ô¨Çowchart of Search Engine Validation module . 
Figure 2 : Flowchart of Search Engine Validation Let us examine a user name as an example . 
‚Äú Êùé Ê¥ãÊ¥ã kelly ‚Äù , " Yangyang Li , Kelly " is a username combined with a Chinese name and an English nickname ) . 
The top Nlinks are Ô¨Årst collected as external evidence . 
The linked text is then cleaned and parsed to check whether this MAW candidate is meaningful . 
In the case of ‚Äú ÊùéÊ¥ãÊ¥ã kelly ‚Äù occurs only as ‚Äú @ ÊùéÊ¥ãÊ¥ã kelly ‚Äù . 
Thus , it is validated as a username , not a real MAW . 
In addition to username checking , stickers and in sufÔ¨Åcient occurrences are also used as indication of invalid MAWs . 
4 Results and Evaluation In our system , every Ô¨Åltering method is executed sequentially . 
Due to length limitation of this paper , we are giving the Ô¨Ånal selected parameters of our modules without showing the tuning process . 
The N - gram token window size of bruteforce method in Candidate Extraction is set to 5 because most new terms are not longer than 5 as a common practice . 
In Candidate Filtering , 836 LEN _ THRES andFREQ _ THRES ( detailed in Table 2 ) in rule - based reÔ¨Ånement are tuned to 15 and 3 , respectively . 
The upper bound of PMI and entropy in informatics - based elimination are experimentally set to -16.2 and 0.2 , respectively . 
In search engine validation , we use the top 10 links as external evidence . 
If the number of valid links is less than 5 , the corresponding MAW candidate is Ô¨Åltered out . 
4.1 Evaluation of SMA W This section gives an estimate on the quality of SMAW in terms of Accuracy , Candidate Size and Inter - rater agreement through evaluation by human raters . 
As MAWs demonstrate a dynamic role in the Chinese lexicon , it is infeasible to refer to a full reference set for calculating Recall and Precision . 
That is the reason accuracy is used to measure quality of SMAW . 
In the evaluation , three groups of SMAWs ( 100 each group , 300 in total ) are randomly sampled from each step for the participants to judge the acceptance of the candidates . 
Raters are asked to make judgements and give 1 if they think a candidate is a MAW , or 0 otherwise . 
Then , Accuracy ( Acc . 
) is calculated as the average of the three groups ‚Äô acceptance rates . 
Incrementally , the Candidate Size ( Size . 
) is also studied for each Ô¨Åltering method . 
Inter - rater agreement among the three raters is also measured using Cohen ‚Äôs Kappa CoefÔ¨Åcient ( K. ) ( Kraemer , 2014 ) . 
The evaluation results are given in Table 1 . 
Step Method Acc . 
K. Size . 
1 BF NA .56 25,594k 2 + Rule - based .22 .58 1,470k 3 + PMI .62 .65 592k 4 + Entropy .77 .70 32k 5 + Baidu .82 .78 16k B0 TF+Max . 
.15 .59 1,935k Table 1 : The Evaluation Results Staring from Brute - force , referred as BF , Table 1 summarizes the accumulative performances of using various metrics for candidate selection after each step . 
B0 is a baseline method that simply employs term frequency and the maximal sequence principle . 
For example , the maximal sequence principle will select ‚Äú ÂìÜÂï¶AÊ¢¶‚Äù,Doraemon over components ‚Äú Âï¶AÊ¢¶ ‚Äù or ‚Äú AÊ¢¶ ‚Äù . 
However , B0 ismore error - prone , For example , in ‚Äú ÂÆâÂÖ® / ‰ΩøÁî® / ÂÖç Ë¥π / WiFi ‚Äù , Safely use free wiÔ¨Å where ‚Äú ÂÖçË¥πWiFi ‚Äù , free wiÔ¨Å shall be a positive instance . 
In general , the accuracy increases when more Ô¨Åltering methods applied . 
It is worth mentioning that the accuracy shows a great boosting after using PMI and entropy , indicating the usefulness of informatics - based metrics for word identiÔ¨Åcation . 
In addition , the incremental K.of each phase suggests the increased agreement methods the three raters by adopting the several metrics , especially after the Baidu search engine validation . 
Compared with baseline method , our system makes use of a more reliable extraction approach that is obviously more effective for the identiÔ¨Åcation of alphabetical words ( Acc . 
= 0.82 , K.= 0.78 ) . 
The high accuracy score and agreement in the evaluation has proven the effectiveness of the extraction method , as well as demonstrating a good quality of the lexicon . 
As for the candidate size , it can be observed that the candidate size drastically decreases after Ô¨Åltering methods . 
The total number of tokens obtained after brute - force candidate extraction reaches 25,594 K , obviously too large and too noisy for direct use . 
After Rule - based ReÔ¨Ånement , a set of 1,470k potential MAW candidates is obtained , only 5.7 % of complete candidate collection . 
To provide more detail of rule - based reÔ¨Ånement , Table 2 shows the process of constructing SMAW list of patterns used and the information on the reduction in data sizes . 
By using PMI and entropy , 878k and 560k invalid MAW candidates are eliminated , respectively . 
The 97.8 % reduction further narrow down the candidate set , only 33k candidates remain in the list . 
After processing this list based on search engine validation , the Ô¨Ånal collection of SMAWS has 16,207 tokens . 
4.2 The Lexical Characteristics This section analyses the lexical properties of the SMAW lexicon . 
Comparisons between the SMAW list ( ‚Äú Web ‚Äù hereinafter ) and the MAWs in Huang and Liu ( 2017 ) ( ‚Äú Giga ‚Äù hereinafter ) will be made in terms of key vocabulary , length distribution , word formation types and lexical diversity so as to highlight the lexical differences of MAWs between social media and newspaper as well as the lexical development of alphabetical words in the recent two decades.837 Rule Description Quantity NONE brute force candidates collection 25,594k Topic remove candidates with ‚Äô # ‚Äô 165k Username remove candidates with ‚Äô @ ‚Äô 297k No Chinese remove candidates without Chinese character 1,302k Too Short Length remove candidates less than LEN_THRES characters 595k Rare Occurrence remove candidates which count less than FREQ_THRES 18,443k English Expression remove candidates contain two or more English words 1,421k Symbol remove candidates contain symbols such as ‚Äô & ‚Äô and ‚Äô * ‚Äô 419k Emoji remove candidates contain emoji such as " XDD " 193k POS tag remove candidates with invalid POS tag such as ‚Äô DET ‚Äô 1k ALL RULES Remains after using all rule - based reÔ¨Ånement 1,470k Table 2 : Noise Reduction Statistics by Rule - based ReÔ¨Ånement . 
4.2.1 Vocabulary Figure 3 visualizes the top 50 MAW vocabularies of the two lexicons . 
The sizes of the words reÔ¨Çect its usage frequency . 
It can be observed that the most frequent MAW in the Giga list is ‚Äú B Âûã ‚Äù ( B - type ) , while in the Web list , the most frequent MAW is ‚Äú HOLD ‰Ωè ‚Äù ( To endure ) , which is a typical Internet neology . 
Moreover , most MAWs in Giga are disyllabic , e.g. ‚Äú AÂûã ‚Äù ( A - type ) and ‚Äú A Á∫ß‚Äù(A - level ) , while SMAWs tend to be more lengthy , containing words of a wider range of syllables ( e.g. ‚Äú NBA ÂÖ®ÊòéÊòü ‚Äù ( NBA all - star ) ) . 
SpeciÔ¨Åcally , MAWs in Giga show a dominant ( rigid ) pattern of ‚Äú X Á±ª / Âûã ‚Äù ( Type - X ) . 
However , in Web , MAWs has more Part - of - Speech diversity , including verbs ( e.g. ‚Äú Hold ‰Ωè ‚Äù ) , nouns ( e.g. ‚Äú BB Èúú ‚Äù ( BB cream ) ) , or adjectives ( e.g. ‚Äú ÁâõX ‚Äù ( incredibly awesome ) ) , indicating the trend of MAWs accounting for different grammatical roles in the Chinese language . 
Lastly , the lexical senses of Giga MAWs are more concentrated to the " type / classiÔ¨Åcation " meaning , while MAWs in Web encode a wider range of meanings , including name entities , swear words , economics , entertainment , etc . 
The above keyword differences reÔ¨Çect a dramatic change of MAWs at syllabic , lexical , grammatical and semantic levels in recent decades . 
4.2.2 Length Distribution The box - plots in Figure 4 give an overview of the length distribution of MAWs in Giga ( Huang and Liu , 2017 ) and Web ( SMAW ) . 
As shown in Figure 4 , MAWs in Web are much longer and more scattered than that in Giga . 
The mean length of MAWs in Giga is 2 - 3 . 
But , the Figure 3 : Word clouds of MAWs in Web and Giga838 Figure 4 : Length distribution of MAWs in Giga and Web mean length in SNAW is around 5 . 
Overall , the MAWs in Web are distributed across a wider span . 
This may imply a tendency of code - mixing words being longer and richer in Modern Chinese . 
4.2.3 Word Formation In line with the work of Huang and Liu ( 2017 ) , word formation of MAWs is classiÔ¨Åed into four major types according to the positions of the A ( alphabet ) and C ( character ) , including AC ( e.g. ‚Äú x - ÂÖâ ‚Äù ) , CA ( e.g. ‚Äú Áâõb ‚Äù ) , CAC ( e.g. ‚Äú Á®ãIÈùí ‚Äù ( a Chinese Name ) ) and other types . 
The number of the four types of MAWs in Giga and Web is shown in Table 3 for comparison . 
AC CA CAC Other Total Giga 665 283 185 18 1151 ( pct ) 57.8 % 24.6 % 16.1 % 1.5 % 100.0 % Web 6971 6994 2242 0 16207 ( pct ) 43.0 % 43.2 % 13.8 % 0.0 % 100.0 % Table 3 : Word formation comparison As highlighted in Table 3 , the dominant type in Giga is AC , while CA is more prevalent in Web . 
Huang and Liu ( 2017 ) argued that the dominance of AC type with the modiÔ¨Åer - modiÔ¨Åed compound structure in Chinese is because heads of nouns are usually right positioned ( Sun , 2006 ) . 
However , MAWs in Web have wider grammatical roles and more verbs are found in SMAW . 
Contrary to nouns , verbs are left headed , such as in ‚Äú Êâìcall ‚Äù ( cheer up ) , where ‚Äú Êâì ‚Äù ( beat ) is the head . 
In addition , cases like ‚Äú Áª¥c ‚Äù ( Vitamin C ) , ‚Äú Âèåc ‚Äù ( double cores ) , and ‚Äú ÊúÄIn ‚Äù ( Most popular ) are headed on alphabetsinstead of the Chinese character , indicating that heads are not necessarily positioned at the Chinese characters . 
4.2.4 Lexical Diversity TTR ( type ‚Äì token ratio ) is used to measure the lexical diversity / richness of a language ( Dur√°n et al . 
, 2004 ) . 
This metric is adopted here with normalized data ( STTR ) , for measuring the lexical diversity of the MAWs in Giga and Web , as shown in Table 4 . 
Data STTR AC.STTR CA.STTR Web 14.53 16.9 12.3 Giga 8.77 7.6 15.2 Table 4 : Lexical Diversity Comparison Table 4 seems to suggest a reverse relation between the frequency of the MAW types and their lexical richness : the ‚Äú AC ‚Äù type is dominant in Giga , but it demonstrates a lower STTR ; similarly , the ‚Äú CA ‚Äù type is dominant in Web , and it also shows a lower STTR . 
Overall , the Web MAWs show a richer vocabulary compared to the newspaper MAWs ( Giga ) , indicating the higher productivity of social media language . 
4.3 The Corpus In addition to the SMAW lexicon , we have also retrieved more than 200,000 sentences ( around 2,000,000 tokens ) for the 16,207 SMAW ( each SMAW contains 10 or so sentences ) to construct a SMAW corpus which can support code - mixing words inquiries . 
‰∏ÄÂÆö(D)Ë¶Å(D ) HOLD‰Ωè ‰Ωè ‰Ωè(V A ) ! ÁñØÁãÇ(D)Â∫óÂ∫Ü ( V A ) 11Â§©(Nd)ÔºåËøò(D)ËÉΩ(D ) HOLD‰Ωè ‰Ωè ‰Ωè(V A)Âêó(T ) KITTYÊéß(Na)‰ª¨(Na)Ëøò(D ) HOLD‰Ωè ‰Ωè ‰Ωè(V A)Âêó(T ) ÂæÆÊó∂‰ª£(Na)ÔºåÂ§ß(A)Ë∂ãÂäø(Na)ÔºåÂèØÂæó(VH ) HOLD‰Ωè ‰Ωè ‰Ωè(V A ) ! ‰∫≤(I)ÔºÅ‰Ω†(Nh)Ë¶Å(D ) HOLD‰Ωè ‰Ωè ‰Ωè(V A)Âì¶(T ) Â§ßÂÆ∂(Nh ) HOLD‰Ωè ‰Ωè ‰Ωè(V A)Âì¶(T ) ÂêÑ‰Ωç(Nes)ÁúãÂÆò(Na)Ë¶Å(D ) HOLD‰Ωè ‰Ωè ‰Ωè(V A)‰∫Ü(Di ) Interface 1 : Corpus samples of ‚Äú HOLD ‰Ωè ‚Äù ( KWIC ) The characters in the sentences are all transferred into simpliÔ¨Åed Chinese for consistency . 
All sentences are automatically segmented using Stanford CoreNLP1(Manning et al . 
, 2014 ) . 
The automatic word segmentation is enabled as the alphabetical words are pre - identiÔ¨Åed in our SMAW lexicon . 
With conÔ¨Årmed boundaries of the alphabetical 1https://stanfordnlp.github.io/ CoreNLP/839 words , it becomes an ordinary task of segmenting the remaining Chinese characters . 
On the basis of the raw sentences , we are building a concordance engine for loading the content of the corpus following the Chinese Word Sketch schema ( Hong and Huang , 2006 ) , which can support users ‚Äô inquires of word and grammatical collocations of code - mixing words . 
Samples of the corpus are shown in Interface 1 . 
Figure 5 : POS distribution of MAWs in Giga and Web Besides , the corpus is undergoing a POS tagging process using the Academia Sinica segmentation and tagging system ( Chen et al . 
, 1996 ; Zhao et al . 
, 2006 ) in order to support grammatical inquiries of linguistic accounts . 
Tagging is conducted automatically with manual post - checking on the SMAWs . 
The precision accuracy is estimated to be over 85 % . 
Since tagging is still in progress , we provide the POS distribution2of the most frequent 50 SMAWs to show a general view of the grammatical distribution of popular SMAWs . 
Figure 5 shows the POS distribution of MAWs in Web and Giga for comparison purpose . 
2https://catalog.ldc.upenn.edu/LDC2009T14The POS distribution in Figure 5 shows that MAWs have developed a more salient role in the Chinese lexicon : from mainly nouns ( Na , Nb , Nd ) to verbs ( V A , VH ) , from modiÔ¨Åers ( A ) to core lexical components ( heads and arguments ) , and the graph demonstrates a more diversiÔ¨Åed lexical categories ( more divisions and colorful ) of new MAWs . 
5 Conclusion and Future Work This work uses social media platform ( Sina Weibo ) and search engine ( Baidu ) for collection and validation of code - mixing words to tackle the under - representation and identiÔ¨Åcation problems of MAWs . 
The evaluation of the new Sina MAW dataset ( SMAW ) , proves the high performance ( Acc . 
= 0.82 , K.= 0.78 ) of the proposed extraction method as well as the effectiveness our proposed candidate Ô¨Åltering techniques in terms of reducing number of noisy candidates . 
The contribution of this work is two - fold : it proposes an innovative method of leveraging the Web for MAW extraction without involvement of manual mediation , yet achieving promising performance in identifying out - of - vocabulary code - mixing words ; it provides a unique MAW dataset and corresponding corpus which are most updated , scaled , structured and comprehensive for supporting linguistic inquiries of code - mixing words , as well as for facilitating related NLP tasks . 
The preliminary analysis to the lexical and grammatical characteristics of SMAWs and the corpus imply the development of code - mixing words into being a more important and diversiÔ¨Åed component in the Chinese lexicon . 
Future work will continue the annotation of the lexicon and the corpus with information of domains , sources , active time , semantic classes , etc . 
, and conduct deeper linguistic analyses for uncovering the phonological and morpho - lexical characteristics of code - mixing words . 
Acknowledgments We acknowledge the research grants from Hong Kong Polytechnic University ( PolyU RTVU ) and GRF grant ( CERG PolyU 15211/14E , PolyU 152006/16E and PolyU 156086/18H ) . 
This work is also funded by the Post - doctoral project ( no . 
4ZZKE ) at the Hong Kong Polytechnic University.840 Abstract Although Indonesian is known to be the fourth most frequently used language over the internet , the research progress on this language in natural language processing ( NLP ) is slowmoving due to a lack of available resources . 
In response , we introduce the Ô¨Årst - ever vast resource for training , evaluation , and benchmarking on Indonesian natural language understanding ( IndoNLU ) tasks . 
IndoNLU includes twelve tasks , ranging from single sentence classiÔ¨Åcation to pair - sentences sequence labeling with different levels of complexity . 
The datasets for the tasks lie in different domains and styles to ensure task diversity . 
We also provide a set of Indonesian pre - trained models ( IndoBERT ) trained from a large and clean Indonesian dataset ( Indo4B ) collected from publicly available sources such as social media texts , blogs , news , and websites . 
We release baseline models for all twelve tasks , as well as the framework for benchmark evaluation , thus enabling everyone to benchmark their system performances . 
1 Introduction Following the notable success of contextual pretrained language methods ( Peters et al . 
, 2018 ; Devlin et al . 
, 2019 ) , several benchmarks to gauge the progress of general - purpose NLP research , such as GLUE ( Wang et al . 
, 2018 ) , SuperGLUE ( Wang et al . 
, 2019 ) , and CLUE ( Xu et al . 
, 2020 ) , have been proposed . 
These benchmarks cover a large range of tasks to measure how well pre - trained models achieve compared to humans . 
However , these metrics are limited to high - resource languages , such as English and Chinese , that already have existing datasets available and are accessible to the research community . 
Most languages , by contrast , suffer from limited data collection and low awareness of ‚àóThese authors contributed equally.published data for research . 
One of the languages which suffer from this resource scarcity problem is Indonesian . 
Indonesian is the fourth largest language used over the internet , with around 171 million users across the globe.1Despite a large amount of Indonesian data available over the internet , the advancement of NLP research in Indonesian is slowmoving . 
This problem occurs because available datasets are scattered , with a lack of documentation and minimal community engagement . 
Moreover , many existing studies in Indonesian NLP do not provide codes and test splits , making it impossible to reproduce results . 
To address the data scarcity problem , we propose the Ô¨Årst - ever Indonesian natural language understanding benchmark , IndoNLU , a collection of twelve diverse tasks . 
The tasks are mainly categorized based on the input , such as single - sentences and sentence - pairs , and objectives , such as sentence classiÔ¨Åcation tasks and sequence labeling tasks . 
The benchmark is designed to cater to a range of styles in both formal and colloquial Indonesian , which are highly diverse . 
We collect a range of datasets from existing works : an emotion classiÔ¨Åcation dataset ( Saputri et al . 
, 2018 ) , QA factoid dataset ( Purwarianti et al . 
, 2007 ) , sentiment analysis dataset ( Purwarianti and Crisdayanti , 2019 ) , aspect - based sentiment analysis dataset ( Ilmania et al . 
, 2018 ; Azhar et al . 
, 2019 ) , part - ofspeech ( POS ) tag dataset ( Dinakaramani et al . 
, 2014 ; Hoesen and Purwarianti , 2018 ) , named entity recognition ( NER ) dataset ( Hoesen and Purwarianti , 2018 ) , span extraction dataset ( Mahfuzh et al . 
, 2019 ; Septiandri and Sutiono , 2019 ; Fernando et al . 
, 2019 ) , and textual entailment dataset ( Setya and Mahendra , 2018 ) . 
It is difÔ¨Åcult to compare model performance since there is no ofÔ¨Åcial 1https://www.internetworldstats.com/stats3.htm843 split of information for existing datasets . 
Therefore we standardize the benchmark by resplitting the datasets on each task for reproducibility purposes . 
To expedite the modeling and evaluation processes for this benchmark , we present samples of the model pre - training code and a framework to evaluate models in all downstream tasks . 
We will publish the score of our benchmark on a publicly accessible leaderboard to provide better community engagement and benchmark transparency . 
To further advance Indonesian NLP research , we collect around four billion words from Indonesian preprocessed text data ( ‚âà23 GB ) , as a new standard dataset , called Indo4B , for self - supervised learning . 
The dataset comes from sources like online news , social media , Wikipedia , online articles , subtitles from video recordings , and parallel datasets . 
We then introduce an Indonesian BERTbased model , IndoBERT , which is trained on our Indo4B dataset . 
We also introduce another IndoBERT variant based on the ALBERT model ( Lan et al . 
, 2020 ) , called IndoBERT - lite . 
The two variants of IndoBERT are used as baseline models in theIndoNLU benchmark . 
In this work , we also extensively compare our IndoBERT models to different pre - trained word embeddings and existing multilingual pre - trained models , such as Multilingual BERT ( Devlin et al . 
, 2019 ) and XLM - R ( Conneau et al . 
, 2019 ) , to measure their effectiveness . 
Results show that our pre - trained models outperform most of the existing pre - trained models . 
2 Related Work Benchmarks GLUE ( Wang et al . 
, 2018 ) is a multi - task benchmark for natural language understanding ( NLU ) in the English language . 
It consists of nine tasks : single - sentence input , semantic similarity detection , and natural language inference ( NLI ) tasks . 
GLUE ‚Äôs harder counterpart SuperGLUE ( Wang et al . 
, 2019 ) covers question answering , NLI , co - reference resolution , and word sense disambiguation tasks . 
CLUE ( Xu et al . 
, 2020 ) is a Chinese NLU benchmark that includes a test set designed to probe a unique and speciÔ¨Åc linguistic phenomenon in the Chinese language . 
It consists of eight diverse tasks , including single - sentence , sentence - pair , and machine reading comprehension tasks . 
FLUE ( Le et al . 
, 2019 ) is an evaluation NLP benchmark for the French language which is divided into six different task categories : text classiÔ¨Åcation , paraphrasing , NLI , parsing , POS tagging , and word sense disambiguation . 
Contextual Language Models In recent years , contextual pre - trained language models have shown a major breakthrough in NLP , starting from ELMo ( Peters et al . 
, 2018 ) . 
With the emergence of the transformer model ( Vaswani et al . 
, 2017 ) , Devlin et al . 
( 2019 ) proposed BERT , a faster architecture to train a language model that eliminates recurrences by applying a multi - head attention layer . 
Liu et al . 
( 2019 ) later proposed RoBERTa , which improves the performance of BERT by applying dynamic masking , increasing the batch size , and removing the next - sentence prediction . 
Lan et al . 
( 2020 ) proposed ALBERT , which extends the BERT model by applying factorization and weight sharing to reduce the number of parameters and time . 
Many research studies have introduced contextual pre - trained language models on languages other than English . 
Cui et al . 
( 2019 ) introduced the Chinese BERT and RoBERTa models , while Martin et al . 
( 2019 ) and Le et al . 
( 2019 ) introduced CamemBERT and FLAUBert respectively , which are BERT - based models for the French language . 
Devlin et al . 
( 2019 ) introduced the Multilingual BERT model , a BERT model trained on monolingual Wikipedia data in many languages . 
Meanwhile , Lample and Conneau ( 2019 ) introduced XLM , a cross - lingual pre - trained language model that uses parallel data as a new translation masked loss to improve the cross - linguality . 
Finally , Conneau et al . 
( 2019 ) introduced XLM - R , a RoBERTabased XLM model . 
3IndoNLU Benchmark In this section , we describe our benchmark as four components . 
Firstly , we introduce the 12 tasks inIndoNLU for Indonesian natural language understanding . 
Secondly , we introduce a large - scale Indonesian dataset for self - supervised pre - training models . 
Thirdly , we explain the various kinds of baseline models used in our IndoNLU benchmark . 
Lastly , we describe the evaluation metric used to standardize the scoring over different models in our IndoNLU benchmark . 
3.1 Downstream Tasks TheIndoNLU downstream tasks covers 12 tasks divided into four categories : ( a ) single - sentence classiÔ¨Åcation , ( b ) single - sentence sequencetagging , ( c ) sentence - pair classiÔ¨Åcation , and ( d)844 Dataset|Train| |Valid| |Test|Task Description # Label # Class Domain Style Single - Sentence ClassiÔ¨Åcation Tasks EmoT‚Ä†3,521 440 442 emotion classiÔ¨Åcation 1 5 tweets colloquial SmSA 11,000 1,260 500 sentiment analysis 1 3 general colloquial CASA 810 90 180 aspect - based sentiment analysis 6 3 automobile colloquial HoASA‚Ä†2,283 285 286 aspect - based sentiment analysis 10 4 hotel colloquial Sentence - Pair ClassiÔ¨Åcation Tasks WReTE‚Ä†300 50 100 textual entailment 1 2 wiki formal Single - Sentence Sequence Labeling Tasks POSP‚Ä†6,720 840 840 part - of - speech tagging 1 26 news formal BaPOS 8,000 1,000 1,029 part - of - speech tagging 1 41 news formal TermA 3,000 1,000 1,000 span extraction 1 5 hotel colloquial KEPS 800 200 247 span extraction 1 3 banking colloquial NERGrit‚Ä†1,672 209 209 named entity recognition 1 7 wiki formal NERP‚Ä†6,720 840 840 named entity recognition 1 11 news formal Sentence - Pair Sequence Labeling Tasks FacQA 2,495 311 311 span extraction 1 3 news formal Table 1 : Task statistics and descriptions.‚Ä†We create new splits for the dataset . 
sentence - pair sequence labeling . 
The data samples for each task are shown in Appendix A. 3.1.1 Single - Sentence ClassiÔ¨Åcation Tasks EmoT An emotion classiÔ¨Åcation dataset collected from the social media platform Twitter ( Saputri et al . 
, 2018 ) . 
The dataset consists of around 4000 Indonesian colloquial language tweets , covering Ô¨Åve different emotion labels : anger , fear , happiness , love , and sadness . 
SmSA This sentence - level sentiment analysis dataset ( Purwarianti and Crisdayanti , 2019 ) is a collection of comments and reviews in Indonesian obtained from multiple online platforms . 
The text was crawled and then annotated by several Indonesian linguists to construct this dataset . 
There are three possible sentiments on the SmSA dataset : positive , negative , and neutral . 
CASA An aspect - based sentiment analysis dataset consisting of around a thousand car reviews collected from multiple Indonesian online automobile platforms ( Ilmania et al . 
, 2018 ) . 
The dataset covers six aspects of car quality . 
We deÔ¨Åne the task to be a multi - label classiÔ¨Åcation task , where each label represents a sentiment for a single aspect with three possible values : positive , negative , and neutral . 
HoASA An aspect - based sentiment analysis dataset consisting of hotel reviews collected from the hotel aggregator platform , AiryRooms ( Azharet al . 
, 2019).2The dataset covers ten different aspects of hotel quality . 
Similar to the CASA dataset , each review is labeled with a single sentiment label for each aspect . 
There are four possible sentiment classes for each sentiment label : positive , negative , neutral , and positive - negative . 
The positivenegative label is given to a review that contains multiple sentiments of the same aspect but for different objects ( e.g. , cleanliness of bed and toilet ) . 
3.1.2 Sentence - Pair ClassiÔ¨Åcation Task WReTE The Wiki Revision Edits Textual Entailment dataset ( Setya and Mahendra , 2018 ) consists of 450 sentence pairs constructed from Wikipedia revision history . 
The dataset contains pairs of sentences and binary semantic relations between the pairs . 
The data are labeled as entailed when the meaning of the second sentence can be derived from the Ô¨Årst one , and not entailed otherwise . 
3.1.3 Single - Sentence Sequence Labeling Tasks POSP This Indonesian part - of - speech tagging ( POS ) dataset ( Hoesen and Purwarianti , 2018 ) is collected from Indonesian news websites . 
The dataset consists of around 8000 sentences with 26 POS tags . 
The POS tag labels follow the Indonesian Association of Computational Linguistics ( INACL ) POS Tagging Convention.3 2https://github.com/annisanurulazhar/absa-playground 3http://inacl.id/inacl/wp-content/uploads/2017/06/INACLPOS-Tagging-Convention-26-Mei.pdf845 Model # Params # Layers # HeadsEmb . 
SizeHidden SizeFFN SizeLanguage TypePre - train Emb . 
Type Scratch 15.1 M 6 10 300 300 3072 Mono fastText - cc - id 15.1 M 6 10 300 300 3072 Mono Word Emb . 
fastText - indo4b 15.1 M 6 10 300 300 3072 Mono Word Emb . 
IndoBERT - lite BASE 11.7 M 12 12 128 768 3072 Mono Contextual IndoBERT BASE 124.5 M 12 12 768 768 3072 Mono Contextual IndoBERT - lite LARGE 17.7 M 24 16 128 1024 4096 Mono Contextual IndoBERT LARGE 335.2 M 24 16 1024 1024 4096 Mono Contextual mBERT 167.4 M 12 12 768 768 3072 Multi Contextual XLM - R BASE 278.7 M 12 12 768 768 3072 Multi Contextual XLM - R LARGE 561.0 M 24 16 1024 1024 4096 Multi Contextual XLM - MLM LARGE 573.2 M 16 16 1280 1280 5120 Multi Contextual Table 2 : The details of baseline models used in IndoNLU benchmark BaPOS This POS tagging dataset ( Dinakaramani et al . 
, 2014 ) contains about 1000 sentences , collected from the PAN Localization Project.4In this dataset , each word is tagged by one of 23 POS tag classes.5Data splitting used in this benchmark follows the experimental setting used by Kurniawan and Aji ( 2018 ) . 
TermA This span - extraction dataset is collected from the hotel aggregator platform , AiryRooms ( Septiandri and Sutiono , 2019 ; Fernando et al . 
, 2019).6The dataset consists of thousands of hotel reviews , which each contain a span label for aspect and sentiment words representing the opinion of the reviewer on the corresponding aspect . 
The labels use Inside - Outside - Beginning ( IOB ) tagging representation with two kinds of tags , aspect and sentiment . 
KEPS This keyphrase extraction dataset ( Mahfuzh et al . 
, 2019 ) consists of text from Twitter discussing banking products and services and is written in the Indonesian language . 
A phrase containing important information is considered a keyphrase . 
Text may contain one or more keyphrases since important phrases can be located at different positions . 
The dataset follows the IOB chunking format , which represents the position of the keyphrase . 
NERGrit This NER dataset is taken from the Grit - ID repository,7and the labels are spans in IOB chunking representation . 
The dataset consists of 4http://www.panl10n.net/ 5http://bahasa.cs.ui.ac.id/postag/downloads/Tagset.pdf 6https://github.com/jordhy97/Ô¨Ånal project 7https://github.com/grit-id/nergrit-corpusthree kinds of named entity tags , PERSON ( name of person ) , PLACE ( name of location ) , and ORGANIZATION ( name of organization ) . 
NERP This NER dataset ( Hoesen and Purwarianti , 2018 ) contains texts collected from several Indonesian news websites . 
There are Ô¨Åve labels available in this dataset , PER ( name of person ) , LOC ( name of location ) , IND ( name of product or brand ) , EVT ( name of the event ) , and FNB ( name of food and beverage ) . 
Similar to the TermA dataset , the NERP dataset uses the IOB chunking format . 
3.1.4 Sentence - Pair Sequence Labeling Task FacQA The goal of the FacQA dataset is to Ô¨Ånd the answer to a question from a provided short passage from a news article ( Purwarianti et al . 
, 2007 ) . 
Each row in the FacQA dataset consists of a question , a short passage , and a label phrase , which can be found inside the corresponding short passage . 
There are six categories of questions : date , location , name , organization , person , and quantitative . 
3.2Indo4B Dataset Indonesian NLP development has struggled with the availability of data . 
To cope with this issue , we provide a large - scale dataset called Indo4B for building a self - supervised pre - trained model . 
Our self - supervised dataset consists of around 4B words , with around 250 M sentences . 
The Indo4B dataset covers both formal and colloquial Indonesian sentences compiled from 12 datasets , of which two cover Indonesian colloquial language , eight cover formal Indonesian language , and the rest have a mixed style of both colloquial and formal . 
The statistics of our large - scale dataset can be846 Dataset # Words # Sentences Size Style Source OSCAR ( Ortiz Su ¬¥ arez et al . 
, 2019 ) 2,279,761,186 148,698,472 14.9 GB mixed OSCAR CoNLLu Common Crawl ( Ginter et al . 
, 2017 ) 905,920,488 77,715,412 6.1 GB mixed LINDAT / CLARIAH - CZ OpenSubtitles ( Lison and Tiedemann , 2016 ) 105,061,204 25,255,662 664.8 MB mixed OPUS OpenSubtitles Twitter Crawl2115,205,737 11,605,310 597.5 MB colloquial Twitter Wikipedia Dump176,263,857 4,768,444 528.1 MB formal Wikipedia Wikipedia CoNLLu ( Ginter et al . 
, 2017 ) 62,373,352 4,461,162 423.2 MB formal LINDAT / CLARIAH - CZ Twitter UI2(Saputri et al . 
, 2018 ) 16,637,641 1,423,212 88 MB colloquial Twitter OPUS JW300 ( Agi ¬¥ c and Vuli ¬¥ c , 2019 ) 8,002,490 586,911 52 MB formal OPUS Tempo35,899,252 391,591 40.8 MB formal ILSP Kompas33,671,715 220,555 25.5 MB formal ILSP TED 1,483,786 111,759 9.9 MB mixed TED BPPT 500,032 25,943 3.5 MB formal BPPT Parallel Corpus 510,396 35,174 3.4 MB formal PAN Localization TALPCo ( Nomoto et al . 
, 2018 ) 8,795 1,392 56.1 KB formal Tokyo University Frog Storytelling ( Moeljadi , 2012 ) 1,545 177 10.1 KB mixed Tokyo University TOTAL 3,581,301,476 275,301,176 23.43 GB Table 3 : Indo4B dataset statistics.1https://dumps.wikimedia.org/backup-index.html.2We crawl tweets from Twitter . 
The Twitter data will not be shared publicly due to restrictions of the Twitter Developer Policy and Agreement.3https://ilps.science.uva.nl/. found in Table 3 . 
We share the datasets that are listed in the table , except for those from Twitter due to restrictions of the Twitter Developer Policy and Agreement . 
The details of Indo4B dataset sources are shown in Appendix B. 3.3 Baselines In this section , we explain the baseline models and the Ô¨Åne - tuning settings that we use in the IndoNLU benchmark . 
3.3.1 Models We provide a diverse set of baseline models , from a non - pre - trained model ( scratch ) , to a wordembedding - based model , to contextualized language models . 
For the word - embeddings - based model , we use an existing fastText model trained on the Indonesian Common Crawl ( CC - ID ) dataset ( Joulin et al . 
, 2016 ; Grave et al . 
, 2018 ) . 
fastText We build a fastText model with our large - scale self - supervised dataset , Indo4B , for comparison with the CC - ID fastText model and contextualized language model . 
For the models above and the fastText model , we use the transformer architecture ( Vaswani et al . 
, 2017 ) . 
We experiment with different numbers of layers , 2 , 4 , and 6 , for the transformer encoder . 
For the fastText model , we Ô¨Årst pre - train the fastText embeddings with skipgram word representation and produce a 300 - dimensional embedding vector . 
We then generate all required embeddings for each downstream task from the pre - trained fastText embeddings andcover all words in the vocabulary . 
Contextualized Language Models We build our own Indonesian BERT and ALBERT models , named IndoBERT and IndoBERT - lite , respectively , in both base and large sizes . 
The details of our IndoBERT and IndoBERT - lite models are explained in Section 4 . 
Aside from a monolingual model , we also provide multilingual model baselines such as Multilingual BERT ( Devlin et al . 
, 2019 ) , XLM ( Lample and Conneau , 2019 ) , and XLM - R ( Conneau et al . 
, 2019 ) . 
The details of each model are shown in Table 2 . 
3.3.2 Fine - tuning Settings We Ô¨Åne - tune a pre - trained model for each task with initial learning with a range of learning rates [ 1e-5 , 4e-5 ] . 
We apply a decay rate of [ 0.8 , 0.9 ] for every epoch , and sample each batch with a size of 16 for all datasets except FacQA and POSP , for which we use a batch size of 8 . 
To establish a benchmark , we keep a Ô¨Åxed setting , and we use an early stop on the validation score to choose the best model . 
The details of the Ô¨Åne - tuning hyperparameter settings used are shown in Appendix D. 3.4 Evaluation Metrics We use the F1 score to measure the evaluation performance of all tasks . 
For the binary and multilabel classiÔ¨Åcation tasks , we measure the macroaveraged F1 score by taking the top-1 prediction from the model . 
For the sequence labeling task , we calculate word - level sequence labeling macro-847 ModelMaximum Sequence Length = 128 Maximum Sequence Length = 512 Batch Size Learning Rate Steps Duration ( Hr . 
) Batch Size Learning Rate Steps Duration ( Hr . 
) IndoBERT - lite BASE 4096 0.00176 112.5 K 38 1024 0.00088 50 K 23 IndoBERT BASE 256 0.00002 1 M 35 256 0.00002 68 K 9 IndoBERT - lite LARGE 1024 0.00044 500 K 134 256 0.00044 129 K 45 IndoBERT LARGE 256 0.0001 1 M 89 128 0.00008 120 K 32 Table 4 : Hyperparameters and training duration for IndoBERT model pre - training . 
averaged F1 - score for all models by following the sequence labeling evaluation method described in the CoNLL evaluation script . 
We calculate two mean F1 - scores separately for classiÔ¨Åcation and sequence labeling tasks to evaluate models on our IndoNLU benchmark . 
4 IndoBERT In this section , we describe the details of our Indonesian contextualized models , IndoBERT and IndoBERT - lite , which are trained using our Indo4B dataset . 
We elucidate the extensive details of the models ‚Äô development , Ô¨Årst the dataset preprocessing , followed by the pre - training setup . 
4.1 Preprocessing Dataset Preparation To get the most beneÔ¨Åcial next sentence prediction task training from the Indo4B dataset , we do either a paragraph separation or line separation if we notice document separator absence in the dataset . 
This document separation is crucial as it is used in the BERT architecture to extract long contiguous sequences ( Devlin et al . 
, 2019 ) . 
A separation between sentences with a new line is also required to differentiate each sentence . 
These are used by BERT to create input embeddings out of sentence pairs that are compacted into a single sequence . 
We specify the number of duplication factors for each of the datasets differently due to the various formats of the datasets that we collected . 
We create duplicates on datasets with the end of document separators with a higher duplication factor . 
The preprocessing method is applied in both the IndoBERT and IndoBERT - lite models . 
We keep the original form of a word to hold its contextual information since Indonesian words are built with rich morphological operations , such as compounding , afÔ¨Åxation , and reduplication ( Pisceldo et al . 
, 2008 ) . 
In addition , this setting is also suitable for contextual pre - training models that leverage inÔ¨Çections to improve the sentence - level representations.(Kutuzov and Kuzmenko , 2019)Twitter data contains speciÔ¨Åc details , such as usernames , hashtags , emails , and URL hyperlinks . 
To preserve privacy and also to reduce noise , this private information in the Twitter UI dataset ( Saputri et al . 
, 2018 ) is masked into generics tokens such as < username > , < hashtag > , < email > and < links > . 
On the other hand , this information is discarded in the larger Twitter Crawl dataset . 
Vocabulary For both the IndoBERT and the IndoBERT - lite models , we utilize SentencePiece ( Kudo and Richardson , 2018 ) with a byte pair encoding ( BPE ) tokenizer as the vocabulary generation method . 
We use a vocab size of 30.522 for the IndoBERT models and vocab size of 30.000 for the IndoBERT - lite models . 
4.2 Pre - training Setup All IndoBERT models are trained on TPUv38 in two phases . 
In the Ô¨Årst phase , we train the models with a maximum sequence length of 128 . 
The training takes around 35 , 89 , 38 and 134 hours on IndoBERT BASE , IndoBERT LARGE , IndoBERT - lite BASE , and IndoBERT - lite LARGE , respectively . 
In the second phase , we continue the training of the IndoBERT models with a maximum sequence length of 512 . 
It takes 9 , 32 , 23 and 45 hours on IndoBERT BASE , IndoBERT LARGE , IndoBERT - lite BASE , and IndoBERT - lite LARGE , respectively . 
The details of the pre - training hyperparameter settings are shown in Appendix D. IndoBERT We use a batch size of 256 and a learning rate of 2e-5 in both training phases for IndoBERT BASE , and we adjust the learning rate to 1e-4 for IndoBERT LARGE to stabilize the training . 
Due to memory limitation , we scale down the batch size to 128 and the learning rate to 8e-5 in the second phase of the training , with a number of training steps adapted accordingly . 
The base and large models are trained using the masked language modeling loss . 
We limit the maximum prediction per sequence into 20 tokens.848 ModelClassiÔ¨Åcation Sequence Labeling EmoT SmSA CASA HoASA WReTE A VG POSP BaPOS TermA KEPS NERGrit NERP FacQA A VG Scratch 57.31 67.35 67.15 76.28 64.35 66.49 86.78 70.24 70.36 39.40 5.80 30.66 5.00 44.03 fastText - cc - id 65.36 76.92 79.02 85.32 67.36 74.79 94.35 79.85 76.12 56.39 37.32 46.46 15.29 57.97 fastText - indo4b 69.23 82.13 82.20 85.88 60.42 75.97 94.94 81.77 74.43 56.70 38.69 46.79 14.65 58.28 mBERT 67.30 84.14 72.23 84.63 84.40 78.54 91.85 83.25 89.51 64.31 75.02 69.27 61.29 76.36 XLM - MLM 65.75 86.33 82.17 88.89 64.35 77.50 95.87 88.40 90.55 65.35 74.75 75.06 62.15 78.88 XLM - R BASE 71.15 91.39 91.71 91.57 79.95 85.15 95.16 84.64 90.99 68.82 79.09 75.03 64.58 79.76 XLM - R LARGE 78.51 92.35 92.40 94.27 83.82 88.27 92.73 87.03 91.45 70.88 78.26 78.52 74.61 81.92 IndoBERT - lite BASE‚Ä†73.88 90.85 89.68 88.07 82.17 84.93 91.40 75.10 89.29 69.02 66.62 46.58 54.99 70.43 + phase two 72.27 90.29 87.63 87.62 83.62 84.29 90.05 77.59 89.19 69.13 66.71 50.52 49.18 70.34 IndoBERT BASE‚Ä†75.48 87.73 93.23 92.07 78.55 85.41 95.26 87.09 90.73 70.36 69.87 75.52 53.45 77.47 + phase two 76.28 87.66 93.24 92.70 78.68 85.71 95.23 85.72 91.13 69.17 67.42 75.68 57.06 77.34 IndoBERT - lite LARGE 75.19 88.66 90.99 89.53 78.98 84.67 91.56 83.74 90.23 67.89 71.19 74.37 65.50 77.78 + phase two 70.80 88.61 88.13 91.05 85.41 84.80 94.53 84.91 90.72 68.55 73.07 74.89 62.87 78.51 IndoBERT LARGE 77.08 92.72 95.69 93.75 82.91 88.43 95.71 90.35 91.87 71.18 77.60 79.25 62.48 81.21 + phase two 79.47 92.03 94.94 93.38 80.30 88.02 95.34 87.36 92.14 71.27 76.63 77.99 68.09 81.26 Table 5 : Results of baseline models with best performing conÔ¨Åguration on the IndoNLU benchmark . 
Extensive experimental results are shown in Appendix E. Bold numbers are the best results among all.‚Ä†The IndoBERT models are trained using two training phases . 
IndoBERT - lite We follow the ALBERT pretraining hyperparameters setup ( Lan et al . 
, 2020 ) to pre - train the IndoBERT - lite models . 
We limit the maximum prediction per sequence into 20 tokens on the models , pre - training with whole word masked loss . 
We train the base model with a batch size of 4096 in the Ô¨Årst phase , and 1024 in the second phase . 
Since we have a limitation in computation power , we use a smaller batch size of 1024 in the Ô¨Årst phase and 256 in the second phase in training our large model . 
5 Results and Analysis In this section , we show the results of the IndoNLU benchmark and analyze the performance of our models in terms of downstream tasks score and performance - space trade - off . 
In addition , we show an analysis of the effectiveness of using our collected data compared to existing baselines . 
5.1 Benchmark Results Overall Performance As mentioned in Section 3 , we Ô¨Åne - tune all baseline models mentioned in Section 3.3 , and evaluate the model performance over all tasks , grouped into two categories , classiÔ¨Åcation and sequence labeling . 
We can see in Table 5 , that IndoBERT LARGE , XLM - R LARGE , and IndoBERT BASE achieve the top-3 best performance results on the classiÔ¨Åcation tasks , and XLM - R LARGE , IndoBERT LARGE , and XLM - R BASE achieve the top-3 best performance results on the sequence labeling tasks . 
The experimental results also suggest that larger models have a performance advantage over smaller models . 
It is also evidentthat all pre - trained models outperform the scratch model , which shows the effectiveness of model pretraining . 
Another interesting observation is that all contextualized pre - trained models outperform word embeddings - based models by signiÔ¨Åcant margins . 
This shows the superiority of the contextualized embeddings approach over the word embeddings approach . 
5.2 Performance - Space Trade - off Figure 1 shows the model performance with respect to the number of parameters . 
We can see two large clusters . 
On the bottom left , the scratch and fastText models appear , and they have the lowest F1 scores and the least Ô¨Çoating points in the inference time . 
On the top right , we can see that the pre - trained models achieve decent performance , but in the inference time , they incur a high computation cost . 
Interestingly , in the top - left region , we can see the IndoBERT - lite models , which achieve similar performance to the IndoBERT models , but with many fewer parameters and a slightly lower computation cost . 
5.3 Multilingual vs. Monolingual Models Based on Table 5 , we can conclude that contextualized monolingual models outperform contextualized multilingual models on the classiÔ¨Åcation tasks by a large margin , but on the sequence labeling tasks , multilingual models tend to perform better compared to monolingual models and even perform much better on the NERGrit and FacQA tasks . 
As shown in Appendix A , both the NERGrit and FacQA tasks contain many entity names which849 Figure 1 : Performance - space trade - off for all baseline models on classiÔ¨Åcation tasks ( left ) and sequence labeling tasks ( right ) . 
We take the best model for each model size . 
2L , 4L , and 6L denote the number of layers used in the model . 
The size of the dots represents the number of FLOPs of the model . 
We use python package thop taken from https://pypi.org/project/thop/ to calculate the number of FLOPs . 
come from other languages , especially English . 
These facts suggest that monolingual models capture the semantic meaning of a word better than multilingual models , but multilingual models identify foreign terms better than monolingual models . 
5.4 Effectiveness of Indo4B Dataset Tasks # Layer fastText - cc - id fastText - indo4b ClassiÔ¨Åcation2 72.00 74.17 4 74.79 75.97 6 74.80 76.00 Sequence Labeling2 56.26 55.55 4 57.97 58.28 6 56.82 57.42 Table 6 : Experiment results on fastText embeddings on IndoNLU tasks with different number of transformer layers According to Grave et al . 
( 2018 ) , Common Crawl is a corpus containing over 24 TB.8We estimate the size of the CC - ID dataset to be around ‚âà180 GB uncompressed . 
Although the Indo4B dataset size is much smaller ( ‚âà23 GB ) , Table 6 shows us that the fastText models trained on the Indo4B dataset ( fastText - indo4b ) consistently outperform fastText models trained on the CC - ID dataset ( fastText - cc - id ) in both classiÔ¨Åcation and sequence labeling tasks in all model settings . 
Based 8https://commoncrawl.github.io/cc-crawlstatistics/plots/languageson Table 5 , the fact that fastText - indo4b outperforms fastText - cc - id with a higher score on 10 out of 12 tasks suggests that a relatively smaller dataset ( ‚âà23 GB ) can signiÔ¨Åcantly outperform its larger counterpart ( ‚âà180 GB ) . 
We conclude that even though our Indo4B dataset is smaller , it covers more variety of the Indonesian language and has better text quality compared to the CC - ID dataset . 
5.5 Effectiveness of IndoBERT and IndoBERT - lite Table 5 shows that the IndoBERT models outperform the multilingual models on 8 out of 12 tasks . 
In general , the IndoBERT models achieve the highest average score on the classiÔ¨Åcation task . 
We conjecture that monolingual models learn better sentiment - level semantics on both colloquial and formal language styles than multilingual models , even though the IndoBERT models ‚Äô size is 40 % ‚Äì 60 % smaller . 
On sequence labeling tasks , the IndoBERT models can not perform as well as the multilingual models ( XLM - R ) in three sequence labeling tasks : POSP , NERGrit , and FacQA . 
One of the possible explanations is that these datasets have many borrowed words from English , and multilingual models have the advantage in transferring learning from English . 
Meanwhile , the IndoBERT - lite models achieve a decent performance on both classiÔ¨Åcation and sequence labeling tasks with the advantage of compact size . 
Interestingly , the IndoBERT - lite LARGE850 model performance is on par with that of XLM - R BASE while having 16x fewer parameters . 
We also observe that increasing the maximum sequence length to 512 in phase two improves the performance on the sequence labeling tasks . 
Moreover , training the model with longer input sequences enables it to learn temporal information from a given text input . 
6 Conclusion We introduce the Ô¨Årst Indonesian benchmark for natural language understanding , IndoNLU , which consists of 12 tasks , with different levels of difÔ¨Åculty , domains , and styles . 
To establish a strong baseline , we collect large clean Indonesian datasets into a dataset called Indo4B , which we use for training monolingual contextual pre - trained language models , called IndoBERT and IndoBERTlite . 
We demonstrate the effectiveness of our dataset and our pre - trained models in capturing sentence - level semantics , and apply them to the classiÔ¨Åcation and sequence labeling tasks . 
To help with the reproducibility of the benchmark , we release the pre - trained models , including the collected data and code . 
In order to accelerate the community engagement and benchmark transparency , we have set up a leaderboard website for the NLP community . 
We publish our leaderboard website at https://indobenchmark.com/ . 
Acknowledgments We want to thank Cahya Wirawan , Pallavi Jain , Irene Gianni , Martijn Wieriks , Ade Romadhony , and Andrea Madotto for insightful discussions about this project . 
We sincerely thank the three anonymous reviewers for their insightful comments on our paper . 
A Data Samples In this section , we show examples for downstream tasks in the IndoNLU benchmark . 
‚Ä¢The examples of SmSA task are shown in Table 7 . 
‚Ä¢The examples of EmoT task are shown in Table 8 . 
‚Ä¢The examples of KEPS task are shown in Table 9 . 
‚Ä¢The examples of HoASA task are shown in Table 10 . 
‚Ä¢The examples of CASA task are shown in Table 11 . 
‚Ä¢The examples of WReTE task are shown in Table 12 . 
‚Ä¢The examples of NERGrit task are shown in Table 13 . 
‚Ä¢The examples of NERP task are shown in Table 14 . 
‚Ä¢The examples of BaPOS task are shown in Table 15 . 
‚Ä¢The examples of POSP task are shown in Table 16 . 
‚Ä¢The examples of FacQA task are shown in Table 17 . 
‚Ä¢The examples of TermA task are shown in Table 18 . 
BIndo4B Data Sources In this section , we show the source of each dataset that we use to build our Indo4B dataset . 
The source of each corpus is shown in Table 19 . 
Sentence Sentiment pengecut dia itu , cuma bisa nantangin dari belakang saja neg wortel mengandung vitamin a yang bisa jaga kesehatan mata neut mocha Ô¨Çoat kfc itu minuman terenak yang pernah gue rasain pos Table 7 : Sample data on task SmSA Tweet Emotion Masalah ga akan pernah menjauh , hadapi Selasamu dengan penuh semangat ! happy Sayang seribu sayang namun tak ada satupun yg nyangkut sampai sekarang sadness cewek suka bola itu dimata cowok cantiknya nambah , biarpun matanya panda love Table 8 : Sample data on task EmoTC Pre - Training Hyperparameters In this section , we show all hyperparameters used in our IndoBERT and IndoBERT - lite training process . 
The hyperparameters is shown in Table 20 . 
D Fine - Tuning Hyperparameters In this section , we show all hyperparameters used in the Ô¨Åne - tuning process of each baseline model . 
The hyperparameter conÔ¨Åguration is shown in Table 21 . 
E Extensive Experiment Results on IndoNLU Benchmark In this section , we show all experiments conducted in the IndoNLU benchmark . 
We use a batch size of 16 for all datasets except FacQA and POSP , for which we use a batch size of 8 . 
The results of the full experiments are shown in Table 22 . 
Word Layanan BCA Mobile Banking Bermasalah Keyphrase O B I I B Word Tidak mengecewakan pakai BCA Mobile Keyphrase O O B B I Word nggak ada tandingannya e - channel BCA Keyphrase B I I B I Table 9 : Sample data on task KEPS854 SentenceAspect AC Air Panas Bau General Kebersihan Linen Service Sunrise Meal TV WiFi air panas kurang berfungsi dan handuk lembab . 
neut neg neut neut neut neg neut neut neut neut Shower zonk , resepsionis yang wanita judes neut neut neut neut neut neut neg neut neut neut Kamar kurang bersih , terutama kamar mandi . 
neut neut neut neut neg neut neut neut neut neut Table 10 : Sample data on task HoASA SentenceAspect Fuel Machine Others Part Price Service bodi plus tampilan nya Avanza baru mantap juragan neut neut neut pos neut neut udah gaya nya stylish ekonomis pula , beli calya deh neut neut neut pos pos neut Mobil kualitas jelek kayak wuling saja masuk Indonesia neut neut neg neut neut neut Table 11 : Sample data on task CASA Sentence A Sentence B Label Anak sebaiknya menjalani tirah baring Anak sebaiknya menjalani istirahat Entail or Paraphrase Kedua kata ini ditulis dengan huruf kanji yang sama Jepang disebut Nippon atau Nihon dalam bahasa Jepang Not Entail Elektron hanya menduduki 0,06 % massa total atom Elektron hanya mengambil 0,06 % massa total atom Entail or Paraphrase Table 12 : Sample data on task WReTE Word Produser David Heyman dan sutradara Mark Herman sedang mencari seseorang Entity O B - PER I - PER O O B - PER I - PERS O O O Word Pada tahun 1996 Williams pindah ke Sebastopol , California di Entity O O O B - PER O O B - PLA O B - PLA O Word bekerja untuk penerbitan perusahaan teknologi O , Reilly Media . 
Entity O O O O O B - ORG I - ORG I - ORG I - ORG O Table 13 : Sample data on task NERGrit . 
PER = PERSON , ORG = ORGANIZATION , PLA = PLACE Word kepala dinas tata kota manado amos kenda menyatakan tidak tahu Entity O O O O B - PLC B - PPL I - PPL O O O Word telah mendaftar untuk menjadi ofÔ¨Åcial merchant bandung great sale 2017 Entity O O O O O O B - EVT I - EVT I - EVT I - EVT Word sekitar timur dan barat arnhem , katherine dan daerah sekitar Entity O B - PLC O B - PLC I - PLC O B - PLC O O O Table 14 : Sample data on task NERP . 
PLC = PLACE , PPL = PEOPLE , EVT = EVENT Word Pemerintah kota Delhi mengerahkan monyet untuk mengusir monyet - monyet lain yang Tag B - NNP B - NNP B - NNP B - VB B - NN B - SC B - VB B - NN B - JJ B - SC Word Beberapa laporan menyebutkan setidaknya 10 monyet ditempatkan di luar arena Tag B - CD B - NN B - VB B - RB B - CD B - NN B - VB B - IN B - NN B - NN Word berencana mendatangkan 10 monyet sejenis dari negara bagian Rajasthan . 
Tag B - VB B - VB B - CD B - NN B - NN B - IN B - NNP I - NNP B - NNP B - Z Table 15 : Sample data on task BaPOS . 
POS tag labels follow Universitas Indonesia POS Tag Standard.9 Word kepala dinas tata kota manado amos kenda menyatakan tidak tahu Tag B - NNO B - VBP B - NNO B - NNO B - NNP B - NNP B - NNP B - VBT B - NEG B - VBI Word telah mendaftar untuk menjadi ofÔ¨Åcial merchant bandung great sale 2017 Tag B - ADK B - VBI B - PPO B - VBL B - NNO B - NNP B - NNP B - NNP B - NNP B - NUM Word sekitar timur dan barat arnhem , katherine dan daerah sekitar Tag B - PPO B - NNP B - CCN B - NNP B - NNP B - SYM B - NNP B - CCN B - NNO B - ADV Table 16 : Sample data on task POSP POS tag labels follow INACL POS Tagging Convention.10855 Question ‚Äù Siapakah penasihat utama Presiden AS George W Bush ? ‚Äù Passage Nasib Karl Rove Akan Segera Diputuskan Label O B I O O O Question ‚Äù Dimana terjadinya letusan gunung berapi dahsyat tahun 1883 ? ‚Äù Passage Di Kepulauan Krakatau Terdapat 400 Tanaman Label O B I O O O Question ‚Äù Perusahaan apakah yang sejak 1 Januari 2006 , menurunkan harga pertamax dan pertamax plus ? ‚Äù Passage Pesaing Semakin Banyak , Pertamina Berusaha Kompetitif Label O O O O B O O Table 17 : Sample data on task FacQA Word sayang wiÔ¨Å tidak bagus harus keluar kamar . 
fasilitas lengkap Entity O B - ASP B - SEN I - SEN O O O O B - ASP B - SEN Word pelayanan nya sangat bagus . 
kamar nya juga oke . 
Entity B - ASP I - ASP B - SEN I - SEN O B - ASP I - ASP O B - SEN O Word kamar cukup luas , interior menarik dan unik sekali , Entity B - ASP B - SEN I - SEN O B - ASP B - SEN O B - SEN I - SEN O Table 18 : Sample data on task TermA. SEN = SENTIMENT , ASP = ASPECT Corpus Name Source Public URL OSCAR OSCAR https://oscar-public.huma-num.fr/compressed/id dedup.txt.gz CoNLLu Common Crawl LINDAT / CLARIAH - CZ https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-1989/Indonesian-annotated-conll17.tar OpenSubtitles OPUS OpenSubtitles http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2016/mono/OpenSubtitles.raw.id.gz Wikipedia Dump Wikipedia https://dumps.wikimedia.org/idwiki/20200401/idwiki-20200401-pages-articles-multistream.xml.bz2 Wikipedia CoNLLu LINDAT / CLARIAH - CZ https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-1989/Indonesian-annotated-conll17.tar Twitter Crawl Twitter Not publicly available Twitter UI Twitter Not publicly available OPUS JW300 OPUS http://opus.nlpl.eu/JW300.php Tempo ILSP http://ilps.science.uva.nl/ilps/wp-content/uploads/sites/6/Ô¨Åles/bahasaindonesia/tempo.zip Kompas ILSP http://ilps.science.uva.nl/ilps/wp-content/uploads/sites/6/Ô¨Åles/bahasaindonesia/kompas.zip TED TED https://github.com/ajinkyakulkarni14/TED-Multilingual-Parallel-Corpus/tree/master/Monolingual data BPPT BPPT http://www.panl10n.net/english/outputs/Indonesia/BPPT/0902/BPPTIndToEngCorpusHalfM.zip Parallel Corpus PAN Localization http://panl10n.net/english/outputs/Indonesia/UI/0802/Parallel/%20Corpus.zip TALPCo Tokyo University https://github.com/matbahasa/TALPCo Frog Storytelling Tokyo University https://github.com/davidmoeljadi/corpus-frog-storytelling Table 19 : Indo4B Corpus Hyperparameter IndoBERT BASE IndoBERT LARGE IndoBERT - lite BASE IndoBERT - lite LARGE attention probs dropout prob 0.1 0.1 0 0 hidden act gelu gelu gelu gelu hidden dropout prob 0.1 0.1 0 0 embedding size 768 1024 128 128 hidden size 768 1024 768 1024 initializer range 0.02 0.02 0.02 0.02 intermediate size 3072 4096 3072 4096 max position embeddings 512 512 512 512 num attention heads 12 16 12 16 num hidden layers 12 24 12 24 type vocab size 2 2 2 2 vocab size 30522 30522 30000 30000 num hidden groups - - 1 1 netstructure type - - 0 0 gapsize - - 0 0 num memory blocks - - 0 0 inner group num - - 1 1 down scale factor - - 1 1 Table 20 : Hyperparameter conÔ¨Ågurations for IndoBERT and IndoBERT - lite pre - trained models.856 batch size n layers n epochs lr early stop gamma max norm seed Scratch [ 8,16 ] [ 2,4,6 ] 25 1e-4 12 0.9 10 42 fastText - cc - id [ 8,16 ] [ 2,4,6 ] 25 1e-4 12 0.9 10 42 fastText - indo4B [ 8,16 ] [ 2,4,6 ] 25 1e-4 12 0.9 10 42 mBERT [ 8,16 ] 12 25 1e-5 12 0.9 10 42 XLM - MLM [ 8,16 ] 16 25 1e-5 12 0.9 10 42 XLM - R BASE [ 8,16 ] 12 25 2e-5 12 0.9 10 42 XLM - R LARGE [ 8,16 ] 24 25 1e-5 12 0.9 10 42 IndoBERT - lite BASE [ 8,16 ] 12 25 1e-5 12 0.9 10 42 + phase 2 [ 8,16 ] 12 25 1e-5 12 0.9 10 42 IndoBERT - lite LARGE [ 8,16 ] 24 25 [ 1e-5,2e-5 ] 12 0.9 10 42 + phase 2 [ 8,16 ] 24 25 2e-5 12 0.9 10 42 IndoBERT BASE [ 8,16 ] 12 25 [ 1e-5,4e-5 ] 12 0.9 10 42 + phase 2 [ 8,16 ] 12 25 4e-5 12 0.9 10 42 IndoBERT LARGE [ 8,16 ] 24 25 4e-5 12 0.9 10 42 + phase 2 [ 8,16 ] 24 25 [ 3e-5,4e-5 ] 12 0.9 10 42 Table 21 : Hyperparameter conÔ¨Ågurations for Ô¨Åne - tuning in IndoNLU benchmark . 
We use a batch size of 8 for POSP and FacQA , and a batch size of 16 for EmoT , SmSA , CASA , HoASA , WReTE , BaPOS , TermA , KEPS , NERGrit , and NERP . 
Model LR # Layer ParamClassiÔ¨Åcation Sequence Labeling EmoT SmSA CASA HoASA WReTE A VG POSP BaPOS TermA KEPS NERGrit NERP FacQA A VG scratch 1e-4 2 38.6 M 58.51 64.22 65.58 78.31 59.54 65.23 85.69 66.30 69.67 47.71 4.62 31.14 4.08 44.17 scratch 1e-4 4 52.8 M 57.31 67.35 67.15 76.28 64.35 66.49 86.78 70.24 70.36 39.40 5.80 30.66 5.00 44.03 scratch 1e-4 6 67.0 M 52.84 67.07 69.88 76.83 58.06 64.94 86.16 68.18 70.64 45.65 5.14 27.88 5.21 44.12 fasttext - cc - id-300 - no - oov - uncased 1e-4 6 15.1 M 67.43 78.84 81.61 85.01 61.13 74.80 94.36 78.45 77.26 57.28 26.70 46.36 17.3 56.82 fasttext - cc - id-300 - no - oov - uncased 1e-4 4 10.7 M 65.36 76.92 79.02 85.32 67.36 74.79 94.35 79.85 76.12 56.39 37.32 46.46 15.29 57.97 fasttext - cc - id-300 - no - oov - uncased 1e-4 2 6.3 M 64.74 76.71 75.39 78.05 65.11 72.00 94.42 78.12 73.45 55.22 33.27 45.44 13.89 56.26 fasttext-4B - id-300 - no - oov - uncased 1e-4 6 15.1 M 68.47 83.07 81.96 86.20 60.33 76.00 95.15 80.61 75.26 44.71 40.83 47.02 18.39 57.42 fasttext-4B - id-300 - no - oov - uncased 1e-4 4 10.7 M 69.23 82.13 82.20 85.88 60.42 75.97 94.94 81.77 74.43 56.70 38.69 46.79 14.65 58.28 fasttext-4B - id-300 - no - oov - uncased 1e-4 2 6.3 M 70.97 83.63 78.97 80.16 57.11 74.17 94.93 80.11 71.92 56.67 31.46 45.08 8.65 55.55 indobert - lite - base-128 - 112.5k 1e-5 12 11.7 M 73.88 90.85 89.68 88.07 82.17 84.93 91.40 75.10 89.29 69.02 66.62 46.58 54.99 70.43 indobert - lite - base-128 - 191.5k 1e-5 12 11.7 M 71.95 89.87 84.71 87.57 80.30 82.88 87.27 67.33 89.15 65.84 67.67 49.32 51.76 68.33 indobert - lite - base-512 - 162.5k 1e-5 12 11.7 M 72.27 90.29 87.63 87.62 83.62 84.29 90.05 77.59 89.19 69.13 66.71 50.52 49.18 70.34 indobert - base-128 4e-5 12 124.5 M 75.48 87.73 93.23 92.07 78.55 85.41 95.26 87.09 90.73 70.36 69.87 75.52 53.45 77.47 indobert - base-512 1e-5 12 124.5 M 76.61 90.90 91.77 90.70 79.73 85.94 95.10 86.25 90.58 69.39 63.67 75.36 53.14 76.21 indobert - base-512 4e-5 12 124.5 M 76.28 87.66 93.24 92.70 78.68 85.71 95.23 85.72 91.13 69.17 67.42 75.68 57.06 77.34 indobert - lite - large-128 1e-5 24 17.7 M 75.19 88.66 90.99 89.53 78.98 84.67 91.56 83.74 90.23 67.89 71.19 74.37 65.50 77.78 indobert - lite - large-512 1e-5 24 17.7 M 71.67 90.13 88.88 88.80 81.19 84.13 91.53 83.51 90.07 67.36 73.27 74.34 69.47 78.51 indobert - lite - large-512 2e-5 24 17.7 M 70.80 88.61 88.13 91.05 85.41 84.80 94.53 84.91 90.72 68.55 73.07 74.89 62.87 78.51 indobert - large-128 - 1100k 4e-5 24 335.2 M 77.04 93.71 96.64 93.27 84.17 88.97 95.71 89.74 91.97 70.82 70.76 77.54 67.27 80.55 indobert - large-128 - 1000k 4e-5 24 335.2 M 77.08 92.72 95.69 93.75 82.91 88.43 95.71 90.35 91.87 71.18 77.60 79.25 62.48 81.21 indobert - large-512 - 1100k 4e-5 24 335.2 M 77.39 92.90 95.90 93.77 81.62 88.32 95.25 86.05 91.92 69.71 75.20 77.53 69.86 80.79 indobert - large-512 - 1100k 3e-5 24 335.2 M 79.47 92.03 94.94 93.38 80.30 88.02 95.34 87.36 92.14 71.27 76.63 77.99 68.09 81.26 bert - base - multilingual - uncased 1e-5 12 167.4 M 67.30 84.14 72.23 84.63 84.40 78.54 91.85 83.25 89.51 64.31 75.02 69.27 61.29 76.36 xlm - mlm-100 - 1280 1e-5 16 573.2 M 65.75 86.33 82.17 88.89 64.35 77.50 95.87 88.40 90.55 65.35 74.75 75.06 62.15 78.88 xlm - roberta - base 2e-5 12 278.7 M 71.15 91.39 91.71 91.57 79.95 85.15 95.16 84.64 90.99 68.82 79.09 75.03 64.58 79.76 xlm - roberta - large 1e-5 24 561.0 M 78.51 92.35 92.40 94.27 83.82 88.27 92.73 87.03 91.45 70.88 78.26 78.52 74.61 81.92 Table 22 : Results of all experiments conducted in IndoNLU benchmark . 
We sample each batch with a size of 16 for all datasets except FacQA and POSP , for which we use a batch size of 8.857 Proceedings of the 1st Conference of the Asia - PaciÔ¨Åc Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing , pages 858‚Äì872 December 4 - 7 , 2020 . 
¬© 2020 Association for Computational Linguistics Happy Are Those Who Grade without Seeing : A Multi - Task Learning Approach to Grade Essays Using Gaze Behaviour Sandeep Mathias ‚ô† , Rudra Murthy ‚ô† ,/uni2662 , Diptesh Kanojia ‚ô† , ‚ô£ , Abhijit Mishra / uni2662 , Pushpak Bhattacharyya ‚ô† ‚ô† Department of Computer Science , Indian Institute of Technology , Bombay /uni2662IBM Research , India ‚ô£ IITB - Monash Research Academy { sam , rudra , diptesh , pb } @cse.iitb.ac.in , abhijitmishra.530@gmail.com Abstract The gaze behaviour of a reader is helpful in solving several NLP tasks such as automatic essay grading . 
However , collecting gaze behaviour from readers is costly in terms of time and money . 
In this paper , we propose a way to improve automatic essay grading using gaze behaviour , which is learnt at run time using a multi - task learning framework . 
To demonstrate the efÔ¨Åcacy of this multi - task learning based approach to automatic essay grading , we collect gaze behaviour for 48 essays across 4 essay sets , and learn gaze behaviour for the rest of the essays , numbering over 7000 essays . 
Using the learnt gaze behaviour , we can achieve a statistically signiÔ¨Åcant improvement in performance over the state - of - the - art system for the essay sets where we have gaze data . 
We also achieve a statistically signiÔ¨Åcant improvement for 4 other essay sets , numbering about 6000 essays , where we have no gaze behaviour data available . 
Our approach establishes that learning gaze behaviour improves automatic essay grading . 
1 Introduction Collecting a reader ‚Äôs psychological input can be very beneÔ¨Åcial to a number of Natural Language Processing ( NLP ) tasks , like complexity ( Mishra et al . 
, 2017 ; Gonz ¬¥ alez - Gardu Àúno and S√∏gaard , 2017 ) , sentence simpliÔ¨Åcation ( Klerke et al . 
, 2016 ) , text understanding ( Mishra et al . 
, 2016 ) , text quality ( Mathias et al . 
, 2018 ) , parsing ( Hale et al . 
, 2018 ) , etc . 
This psychological information can be extracted using devices like eye - trackers , and electroencephalogram ( EEG ) machines . 
However , one of the challenges in using reader ‚Äôs information involves collecting the psycholinguistic data itself . 
In this paper , we choose the task of automatic essay grading and show how we can predict the score that a human rater would give using both text andlearnt gaze behaviour . 
An essay is a piece oftext , written in response to a topic , called a prompt . 
Automatic essay grading is assigning a score to the essay using a machine . 
An essay set is a set of essays written in response to the same prompt . 
Multi - task learning ( Caruana , 1998 ) is a machine learning paradigm where we utilize auxiliary tasks to aid in solving a primary task . 
This is done by exploiting similarities between the primary task and the auxiliary tasks . 
Scoring the essay is the primary task andlearning gaze behaviour is the auxiliary task . 
Using gaze behaviour for a very small number of essays ( less than 0.7 % of the essays in an essay set ) , we see an improvement in predicting the overall score of the essays . 
We also use our gaze behaviour dataset to run experiments on unseen essay sets - i.e. ,essay sets which have no gaze behaviour data - and observe improvements in the system ‚Äôs performance in automatically grading essays . 
Contributions The main contribution of our paper is describing how we use gaze behaviour information , in a multi - task learning framework , to automatically score essays outperforming the stateof - the - art systems . 
We will also release the gaze behaviour dataset1and code2- the Ô¨Årst of its kind , for automatic essay grading - to facilitate further research in using gaze behaviour for automatic essay grading and other similar NLP tasks . 
1.1 Gaze Behaviour Terminology AnInterest Area ( IA ) is an area of the screen that we are interested in . 
These areas are where some text is displayed , and not the white background on the left / right , as well as above / below the text . 
Each word is a separate and unique IA . 
1Gaze behaviour dataset : http://www.cfilt.iitb . 
ac.in/cognitive-nlp/ Essays : https://www.kaggle.com/c/asap-aes 2https://github.com/lwsam/ASAP-Gaze858 AFixation is an event when the reader ‚Äôs eye is focused on a part of the screen . 
For our experiments , we are concerned only with Ô¨Åxations that occur within the interest areas . 
Fixations that occur in the background are ignored . 
ASaccade is the path of the eye movement , as it goes from one Ô¨Åxation to the next . 
There are two types of saccades - Progressions and Regressions . 
Progressions are saccades where the reader moves from the current interest area to a later one . 
Regressions are saccades where the reader moves from the current interest area to an earlier one . 
The rest of the paper is organized as follows . 
Section 2 describes our motivation for using eyetracking and learning gaze behaviour from readers , over unseen texts . 
Section 3 describes some of the related work in the area of automatic essay grading , eye tracking and multi - task learning . 
Section 4 describes the gaze behaviour attributes used in our experiments , and the intuition behind them . 
We describe our dataset creation and experiment setup in Section 5 . 
In Section 6 , we report our results and present a detailed analysis . 
We present our conclusions and discuss possible future work in Section 7 . 
2 Motivation Mishra and Bhattacharyya ( 2018 ) , for instance , describe a lot of research in solving multiple problems in NLP using gaze behaviour of readers . 
However , most of their work involves collecting the gaze behaviour data Ô¨Årst , and then splitting the data into training and testing data , before performing their experiments . 
While their work did show signiÔ¨Åcant improvements over baseline approaches , across multiple NLP tasks , collecting the gaze behaviour data would be quite expensive , both in terms of time and money . 
Therefore , we ask ourselves : ‚Äú Can we learn gaze behaviour , using a small amount of seed data , to help solve an NLP task ? ‚Äù In order to use gaze behaviour on a large scale , we need to be able to learn it , since we can not ask a user to read texts every time we wish to use gaze behaviour data . 
Mathias et al . 
( 2018 ) describe using gaze behaviour to predict how a reader would rate a piece of text ( which is similar to our chosen application ) . 
Since they showed that gaze behaviour can help in predicting text quality , we use multi - task learning to simultaneously learn gaze behaviour information ( auxiliary task ) as well as score the essay ( theprimary task ) . 
However , they collect all their gaze behaviour data a priori , while we try to learn the gaze behaviour of a reader and use what we learn from our system , for grading the essays . 
Hence , while they showed that gaze behaviour could help in predicting how a reader would score a text , their approach requires a reader to read the text , while our approach does not do so , during testing / deployment . 
3 Related Work 3.1 Automatic Essay Grading ( AEG ) The very Ô¨Årst AEG system was proposed by Page ( 1966 ) . 
Since then , there have been a lot of other AEG systems ( see Shermis and Burstein ( 2013 ) for more details ) . 
In 2012 , the Hewlett Foundation released a dataset called the Automatic Student Assessment Prize ( ASAP ) AEG dataset . 
The dataset contains about 13,000 essays across eight different essay sets . 
We discuss more about that dataset later . 
With the availability of a large dataset , there has been a lot of research , especially using neural networks , in automatically grading essays - like using Long Short Term Memory ( LSTM ) Networks ( Taghipour and Ng , 2016 ; Tay et al . 
, 2018 ) , Convolutional Neural Networks ( CNNs ) ( Dong and Zhang , 2016 ) , or both ( Dong et al . 
, 2017 ) . 
Zhang and Litman ( 2018 ) improve on the results of Dong et al . 
( 2017 ) using co - attention between the source article and the essay for one of the types of essay sets . 
3.2 Eye - Tracking Capturing the gaze behaviour of readers has been found to be quite useful in improving the performance of NLP tasks ( Mishra and Bhattacharyya , 2018 ) . 
The main idea behind using gaze behaviour is the eye - mind hypothesis ( Just and Carpenter , 1980 ) , which states that whatever text the eye reads , that is what the mind processes . 
This hypothesis has led to a large body of work in psycholinguistic research that shows a relationship between text processing and gaze behaviour . 
Mishra and Bhattacharyya ( 2018 ) also describe some of the ways that eye - tracking can be used for multiple NLP tasks like translation complexity , sentiment analysis , etc . 
Research has been done on using gaze behaviour at run time to solve downstream NLP tasks like sentence simpliÔ¨Åcation ( Klerke et al . 
, 2016 ) , readability ( Gonz ¬¥ alez - Gardu Àúno and S√∏gaard , 2018 ; Singh859 et al . 
, 2016 ) , part - of - speech tagging ( Barrett et al . 
, 2016 ) , sentiment analysis ( Mishra et al . 
, 2018 ; Barrett et al . 
, 2018 ; Long et al . 
, 2019 ) , grammatical error detection ( Barrett et al . 
, 2018 ) , hate speech detection ( Barrett et al . 
, 2018 ) and named entity recognition ( Hollenstein and Zhang , 2019 ) . 
Different strategies have been adopted to alleviate the need for gaze behaviour at run time . 
Barrett et al . 
( 2016 ) use token level averages of gaze features at run time from the Dundee Corpus ( Kennedy et al . 
, 2003 ) , to alleviate the need for gaze behaviour at run time . 
Singh et al . 
( 2016 ) and Long et al . 
( 2019 ) predict gaze behaviour at the tokenlevel prior to using it at run time . 
Mishra et al . 
( 2018 ) , Gonz ¬¥ alez - Gardu Àúno and S√∏gaard ( 2018 ) , Barrett et al . 
( 2018 ) , and Klerke et al . 
( 2016 ) , use multi - task learning to learn gaze behaviour along with solving the primary NLP task . 
4 Gaze Behaviour Attributes In our experiments , we use only a subset of gaze behaviour attributes described by Mathias et al . 
( 2018 ) because most of the other attributes ( like Second Fixation Duration3 ) were mostly 0 , for most of the interest areas , and learning over them would not have yielded any meaningful results . 
Fixation Based Attributes In our experiments , we use the Dwell Time ( DT ) and First Fixation Duration ( FFD ) as Ô¨Åxation - based gaze behaviour attributes . 
Dwell Time is the total amount of time a user spends focusing on an interest area . 
First Fixation Duration is amount of time that a reader initially focuses on an interest area . 
Larger values for Ô¨Åxation durations ( for both DT and FFD ) usually indicate that a word could be wrong ( either a spelling mistake or grammar error ) . 
Errors would force a reader to pause , as they try to understand why the error was made ( For example , if the writer wrote ‚Äú short cat ‚Äù instead of ‚Äú short cut ‚Äù . 
Saccade Based Attribute In addition to the Fixation based attributes , we also look at a regressionbased attribute - IsRegression ( IR ) . 
This attribute is used to check whether or not a regression occurred from a given interest area . 
We do n‚Äôt focus on progression - based attributes , because the usual direction of reading is progressions . 
We are mainly concerned with regressions because they often occur when there is a mistake , or a need for disam3The duration of the Ô¨Åxation when the reader Ô¨Åxates on an interest area for the second time.biguation ( like trying to resolve the antecedent of an anaphora ) . 
Interest Area Based Attributes Lastly , we also use IA - based attributes , such as the Run Count ( RC ) and if the IA was Skipped ( Skip ) . 
The Run Count is the number of times a particular IA was Ô¨Åxated on , and Skip is whether or not the IA was skipped . 
A well - written text would be read more easily , meaning a lower RC , and higher Skip ( Mathias et al . 
, 2018 ) . 
5 Dataset and Experiment Setup 5.1 Essay Dataset Details We perform our experiments on the ASAP AEG dataset . 
The dataset has approximately 13,000 essays , across 8 essay sets . 
Table 1 reports the statistics of the dataset in terms of Number of Essays , Score Range , and Mean Word Count . 
The Ô¨Årst 4 rows in Table 1 are source - dependent response ( SDR ) essay sets , which we use to collect our gaze behaviour data . 
The other essays are used as unseen essay sets . 
SDRs are essays written in response to a question about a source article . 
For example , one of the essay sets that we use is based on an article called The Mooring Mast , by Marcia Amidon L ¬®usted4 . 
5.2 Evaluation Metric Essay Set Number of Essays Score Range Mean Word Count Prompt 3 1726 0 - 3 150 Prompt 4 1770 0 - 3 150 Prompt 5 1805 0 - 4 150 Prompt 6 1800 0 - 4 150 Prompt 1 1783 2 - 12 350 Prompt 2 1800 1 - 6 350 Prompt 7 1569 0 - 30 250 Prompt 8 723 0 - 60 650 Total 12976 0 - 60 250 Table 1 : Statistics of the 8 essay sets from the ASAP AEG dataset . 
We collect gaze behaviour data only for Prompts 3 - 6 , as explained in Section 5.3 . 
The other 4 prompts comprise our unseen essay sets . 
For measuring our system ‚Äôs performance , we use Cohen ‚Äôs Kappa with quadratic weights - Quadratic Weighted Kappa ( QWK ) ( Cohen , 1968 ) for the following reasons . 
Firstly , irrespective of whether we 4The prompt is ‚Äú Based on the excerpt , describe the obstacles the builders of the Empire State Building faced in attempting to allow dirigibles to dock there . 
Support your answer with relevant and speciÔ¨Åc information from the excerpt . 
‚Äù The original article is present in Appendix A.860 use regression , or ordinal classiÔ¨Åcation , the Ô¨Ånal scores that are predicted by the system should be discrete scores . 
Hence , using Pearson Correlation would not be appropriate for our system . 
Secondly , F - Score and accuracy do not consider chance agreements unlike Cohen ‚Äôs Kappa . 
If we were to give everyone an average grade , we would get a positive value for accuracy and F - Score , but a Kappa value of 0 . 
Thirdly , weighted Kappa takes into account the fact that the classes are ordered , i.e. 0<1<2 .... Using unweighted Kappa would penalize a 0graded as a 4 , as much as a 1 . 
We use quadratic weights , as opposed to linear weights , because quadratic weights reward agreements and penalize mismatches more than linear weights . 
5.3 Creation of the Gaze Behaviour Dataset In this subsection , we describe how we created our gaze behaviour dataset , how we chose our essays for eye - tracking , and how they were annotated . 
5.3.1 Details of Texts Essay Set 0 1 2 3 4 Total Prompt 3 2 4 5 1 N / A 12 Prompt 4 2 3 4 3 N / A 12 Prompt 5 2 1 3 5 1 12 Prompt 6 2 2 3 4 1 12 Total 8 10 15 13 2 48 Table 2 : Number of essays for each essay set which we collected gaze behaviour , scored between 0 to 3 ( or 4 ) . 
As mentioned earlier in Section 5 , we used only essays corresponding to prompts 3 to 6 of the ASAP AEG dataset . 
From each of the four essay sets , we selected 12 essays with a diverse vocabulary as well as all possible scores . 
We use a greedy algorithm to select essays i.e. , For each essay set , we pick 12 essays , covering all score points with maximum number of unique tokens , as well as being under 250 words . 
Table 2 reports the distribution of essays with each score , for each of the 4 essay sets that we use to create our gaze behaviour dataset . 
To display the essay text on the screen , we use a large font size , so that ( a ) the text is clear , and ( b ) the reader ‚Äôs gaze is captured on the words which they are currently reading . 
Although , this ensures the clarity in reading and recording the gaze pattern in a more accurate manner , it also imposes a limitation on the size of the essay which can be used forour experiment . 
This is why , the longest essay in our gaze behaviour dataset is about 250 words . 
The original essays have their named entities anonymized . 
Hence , before running the experiments , we replaced the required named entities with placeholders ( Eg . 
@NAME1 ‚Üí‚ÄúAl Smith ‚Äù , @PLACE1 ‚Üí‚ÄúNew Jersey ‚Äù , @MONTH1 ‚Üí‚ÄúMay ‚Äù , etc.)5 . 
5.3.2 Annotator Details We used a total of 8 annotators , aged between 18 and 31 , with an average age of 25 years . 
All of them were either in college , or had completed a Bachelor ‚Äôs degree . 
All but one of them also had experience as a teaching assistant . 
The annotators were Ô¨Çuent in English , and about half of them had participated earlier , in similar experiments . 
The annotators were adequately compensated for their work6 . 
To assess the quality of the individual annotators , we evaluated the scores they provided against the ground truth scores - i.e. ,the scores given by the original annotators . 
The QWK measures the agreement between the annotators and the ground truth score . 
Close is the number of times ( out of 48 ) in which the annotators either agreed with the ground truth scores , or differed from them by at most 1 score point . 
Correct is the number of times ( out of 48 ) in which the annotators agreed with the ground truth scores . 
The mean values for the 3 measures were 0.646 ( QWK ) , 42.75 ( Close ) and 22.25 ( Correct ) . 
5.4 System Details We conduct our experiments using well - established norms in eye - tracking research ( Holmqvist et al . 
, 2011 ) . 
The essays are displayed on a screen that is kept about 2 feet in front of the participant . 
The workÔ¨Çow of the experiment is as follows . 
First , the camera is calibrated . 
This is done by having the annotator look at 13 points on the screen , while the camera tracks their eyes . 
Next , the calibration is validated . 
In this step , the participant looks at the same points they saw earlier . 
If there is a big difference between the participant ‚Äôs Ô¨Åxation points tracked by the camera and the actual points , calibration is repeated . 
Then , the reader 5Another advantage of using source - dependent essays is that there is a source article which we can use to correctly replace the anonymized named entities 6We report details on individual annotators in Appendix B.861 performs a self - paced reading of the essay while we supervise the tracking of their eyes . 
After reading and scoring an essay , the participant takes a small break of about a minute , before continuing . 
Before the next essay is read , the camera has to again be calibrated and validated7 . 
The essay is displayed on the screen in Times New Roman typeface with a font size of 23 . 
Finally , the reader scores the essay andprovides a justiÔ¨Åcation for their score8 . 
This entire process is done using an SR Research Eye Link 1000 eye - tracker ( monocular stabilized head mode , with a sampling rate of 500Hz ) . 
The machine collects all the gaze details that we need for our experiments . 
An interest area report is generated for gaze behaviour using the SR Research Data Viewer software . 
5.5 Experiment Details We use Ô¨Åve - fold cross - validation to evaluate our system . 
For each fold , 60 % is used as training , 20 % for validation , and 20 % for testing . 
The folds are the same as those used by Taghipour and Ng ( 2016 ) . 
Prior to running our experiments , we convert the scores from their original score range ( given in Table 1 ) to the range of [ 0,1]as described by Taghipour and Ng ( 2016 ) . 
In order to normalize idiosyncratic reading patterns across different readers , we perform binning for each of the features for each of the readers . 
For IR and Skip we use only two bins - 0 and 1 - corresponding to their values . 
For the run count , we use six bins ( from 0 to 5 ) , where each bin is the run count ( up to 4 ) , and bin 5 contains run counts more than 4 . 
For the Ô¨Åxation attributes - DT and FFD - we use the same binning scheme as described in Klerke et al . 
( 2016 ) . 
The binning scheme for Ô¨Åxation attributes is as follows : 0ifFV=0 , 1ifFV>0andFV‚â§¬µ‚àíœÉ , 2ifFV>¬µ‚àíœÉandFV‚â§¬µ‚àí0.5√óœÉ , 3ifFV>¬µ‚àí0.5√óœÉandFV‚â§¬µ+0.5√óœÉ , 4ifFV>¬µ+0.5√óœÉandFV‚â§¬µ+œÉ , 5ifFV>¬µ+œÉ , whereFV is the value of the given Ô¨Åxation attribute,¬µis the average Ô¨Åxation attribute value for 7The average time for the participants was about 2 hours , with the fastest completing the task in slightly under one and a half hours . 
8As part of our data release , we will release the scores given by each annotator , as well as their justiÔ¨Åcations for their scorethe reader and œÉis the standard deviation . 
5.6 Network Architecture Figure 1 ( b ) shows the architecture of our proposed system , based on the co - attention based architecture described by Zhang and Litman ( 2018 ) . 
Given an essay , we split the essay into sentences . 
For each sentence , we look - up the word embeddings for all words in the Word Embedding layer . 
The 4000 most frequent words are used as the vocabulary , with all other words mapped to a special unknown token . 
This sequence of word embeddings is then sent through a Time - Delay Neural Network ( TDNN ) , or 1 - d Convolutional Neural Network ( CNN ) , of Ô¨Ålter width k. The output from CNN is pooled using an attention layer - the Word Level Attention Pooling Layer - which results in a representation for every sentence . 
These sentence representations are then sent through a Sentence Level LSTM Layer and their output pooled in the Sentence Level Attention Pooling Layer to obtain the sentence representation for the essay . 
A similar procedure is repeated for the source article . 
We then perform co - attention between the sentence representations of the essay andthe source article .Co - attention is performed to learn similarities between the sentences in the essay and the source article . 
This is done as a way to ensure that the writer sticks to answering the prompt , rather than drifting off topic . 
We now represent every sentence in the essay as a weighted combination of the sentence representation between the essay and the source article ( Essay2Article ) . 
The weights are obtained from the output of the co - attention layer . 
The weights represent how each sentence in the essay are similar to the sentences in the source article . 
If a sentence in the essay has low weights this indicates that the sentence would be off topic . 
A similar procedure is repeated to get a weighted representation of sentences in the source article with respect to the essay ( Article2Essay ) . 
Finally , we send the sentence representation of the essay and article , through a dense layer ( i.e. the Modeling Layer ) to predict the Ô¨Ånal essay score , with a sigmoid activation function . 
As the essay scores are in the range [ 0,1 ] , we use sigmoid activation at the output layer . 
During prediction , we map the output scores from the sigmoid layer back to the original score range , minimizing the mean squared error ( MSE ) loss .862 Figure 1 : Architecture of the proposed gaze behaviour and essay scoring multi - task learning systems , namely ( a ) theSelf - Attention multi - task learning system , for an essay of nsentences - and ( b ) - the Co - Attention system for an essay ofnsentences and a source article of msentences . 
For essay sets without a source article , we use theSelf - Attention model proposed by Dong et al . 
( 2017 ) . 
This is a simpler model which does not consider the source article , and uses only the essay text . 
This is applicable whenever a source article is not present . 
Figure 1 ( a ) shows the architecture of the model . 
Like the earlier system , we get the sentence representation of the essay from the Sentence Level LSTM Layer and send it through the Dense Layer with a sigmoid activation function . 
Gaze behaviour is learnt at the Word - Level Convolutional Layer in both the models because the gaze attributes are deÔ¨Åned at the word - level , while the essay is scored at the document - level . 
The output from the CNN layer is sent through a linear layer followed by sigmoid activation for a particular gaze behaviour . 
For learning multiple gaze attributes simultaneously , we have multiple linear layers for each of the gaze attributes . 
In the multitask setting , we also minimize the mean squared error of the learnt gaze behaviour and the actual gaze behaviour attribute value . 
We assign weights to each of the gaze behaviour loss functions to control the importance given to individual gaze behaviour learning tasks . 
5.7 Network Hyperparameters Table 3 gives the different hyperparameters which we used in our experiment . 
We use the 50 dimension GloVe pre - trained word embeddings ( Pennington et al . 
, 2014 ) trained on the Wikipedia 2014 + Gigawords 5 Corpus ( 6B tokens , 4 K vocabulary , uncased ) . 
We run our experiments over a batch size of 100 , for 100 epochs , and set the learningLayer Hyperparameter Value Embedding layer Pre - trained embeddings GloVe Embeddings dimensions 50 Word - level CNN Kernel size 5 Filters 100 Sentence - level LSTM Hidden units 100 Network - wide Batch size 100 Epochs 100 Learning rate 0.001 Dropout rate 0.5 Momentum 0.9 Table 3 : Hyperparameters for our experiment . 
rate as 0.001 , and a dropout rate of 0.5 . 
The Wordlevel CNN layer has a kernel size of 5 , with 100 Ô¨Ålters . 
The Sentence - level LSTM layer and modeling layer both have 100 hidden units . 
We use the RMSProp Optimizer ( Dauphin et al . 
, 2015 ) with a 0.001 initial learning rate and momentum of 0.9 . 
Gaze Feature Gaze Feature Weight Dwell Time 0.05 First Fixation Duration 0.05 IsRegression 0.01 Run Count 0.01 Skip 0.1 Table 4 : This table shows the best weights assigned to the different gaze features from our grid search . 
In addition to the network hyper - parameters , we also weigh the loss functions of the different gaze863 behaviours differently , with weight levels of 0.5 , 0.1,0.05,0.01 and0.001 . 
We use grid search and pick the weight giving the lowest mean - squared error on the development set . 
The best weights from grid search are 0.05 for DT and FFD , 0.01 for IR and RC , and 0.1for Skip . 
5.8 Experiment ConÔ¨Ågurations To test our system on essay sets which we collected gaze behaviour , we run experiments using the following conÔ¨Ågurations . 
( a ) Self - Attention - This is the implementation of Dong et al . 
( 2017 ) ‚Äôs system in TensorÔ¨Çow by Zhang and Litman ( 2018 ) . 
( b ) Co - Attention . 
This is Zhang and Litman ( 2018 ) ‚Äôs system9 . 
( c ) Co - Attention+Gaze . 
This is our system , which uses gaze behaviour . 
In addition to this , we also run experiments on theunseen essay sets using the following trainingconÔ¨Ågurations . 
( a ) Only Prompt - This uses our self - attention model , with the training data being only the essays from that essay set . 
We use this model , because there are no source articles for these essay sets . 
( b ) Extra Essays - Here , we augment the training data of ( a ) with the 48 essays for which we collect gaze behaviour data . 
( c ) Essays+Gaze - Here , we augment the training data of ( a ) with the 48 essays which we collect gaze behaviour data , and their corresponding gaze data . 
We also compare our results with a string kernel based system proposed by Cozma et al . 
( 2018 ) . 
6 Results and Analysis Table 5 reports the results of our experiments on the essay sets for which we collect the gaze behaviour data . 
The table is divided into 3 parts . 
The Ô¨Årst part ( i.e. ,Ô¨Årst 3 rows ) are the reported results previously available deep - learning systems , namely Taghipour and Ng ( 2016 ) , Dong and Zhang ( 2016 ) , and Tay et al . 
( 2018 ) . 
The next 2 rows feature results using the self - attention ( Dong et al . 
, 2017 ) and co - attention ( Zhang and Litman , 2018 ) . 
The last row reports results using gaze behaviour on top of co - attention , i.e. ,Co - Attention+Gaze . 
The Ô¨Årst column is the different systems . 
The next 4 columns report the QWK results of each system for each of the 4 essay sets . 
The last column reports the Mean QWK value across all 4 essay sets . 
Our system is able to outperform the CoAttention system ( Zhang and Litman , 2018 ) in all 9The implementation of both systems can be downloaded from here.the essay sets . 
Overall , it is also the best system achieving the highest QWK results among all the systems in 3 out of the 4 essay sets ( and the secondbest in the other essay set ) . 
To test our hypothesis that the model trained by learning gaze behaviour helps in automatic essay grading - we run the Paired T - Test . 
Our null hypothesis is : ‚Äú Learning gaze behaviour to score an essay does not help any more than the self - attention and co - attention systems and whatever improvements we see are due to chance . 
‚Äù We choose a signiÔ¨Åcance level of p<0.05 , and observe that the improvements of our system are found to be statistically signiÔ¨Åcant - rejecting the null hypothesis . 
6.1 Results for Unseen Essay Sets In order to run our experiments on unseen essay sets , we augment the training data with the gaze behaviour data collected . 
Since none of these essays have source articles , we use the self - attention model of Dong et al . 
( 2017 ) as the baseline system . 
We now augment the gaze behaviour learning task as the auxiliary task and report the results in Table 6 . 
The Ô¨Årst column in the table is the different systems . 
The next 4 columns are the results for each of the unseen essay sets , and the last column is the mean QWK . 
From Table 6 , we observe that our system which uses both the extra 48 essays and their gaze behaviour outperforms the other 2 conÔ¨Ågurations ( Only Prompt andExtra Essays ) across all 4 unseen essay sets . 
The improvement when learning gaze behaviour for unseen essay sets is statistically signiÔ¨Åcant for p<0.05 . 
6.2 Comparison with String Kernel System Since Cozma et al . 
( 2018 ) have n‚Äôt released their data splits ( train / test / dev ) , we ran their system with our data splits . 
We observed a mean QWK of 0.750 with the string kernel - based system on the essay sets where we have gaze behaviour data , and 0.685 on the unseen essay sets . 
One possible reason for this could be that while they used cross - validation , they may have used only a training - testing split ( as compared to a train / test / dev split ) . 
6.3 Analysis of Gaze Attributes In order to see which of the gaze attributes are the most important , we ran ablation tests , where we ablate each gaze attribute . 
We found that the most important gaze behaviour attribute across all the essay sets is the Dwell Time , followed closely by the First Fixation Duration . 
One of the reasons864 System Prompt 3 Prompt 4 Prompt 5 Prompt 6 Mean QWK Taghipour and Ng ( 2016 ) 0.683 0.795 0.818 0.813 0.777 Dong and Zhang ( 2016 ) 0.662 0.778 0.800 0.809 0.762 Tay et al . 
( 2018 ) 0.695 0.788 0.815 0.810 0.777 Self - Attention ( Dong et al . 
, 2017 ) 0.677 0.807 0.806 0.809 0.775 Co - Attention ( Zhang and Litman , 2018 ) 0.689‚Ä† 0.809‚Ä† 0.812‚Ä† 0.813‚Ä† 0.780‚Ä† Co - Attention+Gaze 0.698 * 0.818 * 0.815 * 0.821 * 0.788 * Table 5 : Results of our experiments in scoring the essays ( QWK values ) from the essay sets where we collected gaze behaviour . 
The Ô¨Årst 3 rows are results reported from other state - of - the - art deep learning systems . 
The next 2 rows are the results we obtained on existing systems - self - attention and co - attention - without gaze behaviour . 
The last row is the results from our system using gaze behaviour data ( Co - Attention+Gaze ) . 
‚Ä† denotes the baseline system performance , and * denotes a statistically signiÔ¨Åcant result of p<0.05for the gaze behaviour system . 
System Prompt 1 Prompt 2 Prompt 7 Prompt 8 Mean QWK Taghipour and Ng ( 2016 ) 0.775 0.687 0.805 0.594 0.715 Dong and Zhang ( 2016 ) 0.805 0.613 0.758 0.644 0.705 Tay et al . 
( 2018 ) 0.832 0.684 0.800 0.697 0.753 Only Prompt ( Dong et al . 
( 2017 ) ) 0.816 0.667 0.792 0.678 0.738 Extra Essays 0.828‚Ä† 0.672‚Ä† 0.802‚Ä† 0.685‚Ä† 0.747‚Ä† Extra Essays + Gaze 0.833 0.681 0.806 * 0.699 * 0.754 * Table 6 : Results of our experiments on the unseen essay sets our dataset . 
The Ô¨Årst 3 rows are results reported from other state - of - the - art deep learning systems . 
The next 2 rows are the results obtained without using gaze behaviour ( without and with the extra essays ) . 
The last row is the results from our system . 
‚Ä† denotes the baseline system without gaze behaviour , and * denotes a statistically signiÔ¨Åcant result of p<0.05for the gaze behaviour system . 
Gaze Feature Diff . 
in QWK Dwell Time 0.0137 First Fixation Duration 0.0136 IsRegression 0.0090 Run Count 0.0110 Skip 0.0091 Table 7 : Results of ablation tests for each gaze behaviour attribute across all the essay sets . 
The reported numbers are the difference in QWK before and after ablating the given gaze attribute . 
The number in bold denotes the best gaze attribute . 
for this is the fact that both DT and FFD were very useful in detecting errors made by the essay writers . 
From Figure 210 , we observe that most of the longest dwell times have come at / around spelling mistakes ( tock instead of took ) , or outof - context words ( bayinstead of by ) , or incorrect phrases ( short cat , instead of short cut ) . 
These errors force the reader to spend more time Ô¨Åxating on the word which we also mentioned earlier . 
10We have given more examples in Appendix C.The normalized MSE of each of the gaze features learnt by our system was between 0.125 to 0.128 for all the gaze behaviour attributes . 
6.4 Analysis Using Only a Native English Speaker System No Native All Prompt 1 0.816 0.824 0.833 Prompt 2 0.667 0.679 0.681 Prompt 3 0.677 0.679 0.698 Prompt 4 0.807 0.812 0.818 Prompt 5 0.806 0.810 0.815 Prompt 6 0.809 0.815 0.821 Prompt 7 0.792 0.809 0.806 Prompt 8 0.678 0.679 0.699 Mean QWK 0.757 0.764 0.771 Table 8 : Result using only gaze behaviour of the native speaker ( Native ) , compared using no gaze behaviour ( No ) and gaze behaviour of all the readers ( All ) . 
We also ran our experiments using only the gaze behaviour of an annotator who was a native En-865 Figure 2 : Dwell Time of one of the readers for one of the essays . 
The darker the background , the larger the bin . 
glish speaker ( as opposed to the rest of our annotators who were just Ô¨Çuent English speakers ) . 
Table 8 shows the results of those experiments . 
We observed a mean QWK of 0.779 for the seen essay sets , and a mean QWK of 0.748 for the essays sets where we have no gaze data . 
The difference in performance between both our systems ( i.e. with only native speaker and with all annotators ) were found to be statistically signiÔ¨Åcant with p=0.024511 . 
Similarly , the improvement in performance using the native English speaker , compared to not using any gaze behaviour was also found to be statistically signiÔ¨Åcant for p=0.0084 . 
7 Conclusion and Future Work In this paper , we describe how learning gaze behaviour can help AEG in a multi - task learning setup . 
We explained how we created a resource by collecting gaze behaviour data , and using multitask learning we are able to achieve better results over a state - of - the - art system developed by Zhang and Litman ( 2018 ) for the essay sets which we collected gaze behaviour data from . 
We also analyze the transferability of gaze behaviour patterns across essay sets by training a multi - task learning model onunseen essay sets ( i.e. essay sets where we have no gaze behaviour data ) , thereby establishing that learning gaze behaviour improves automatic essay grading . 
In the future , we would like to look at using gaze behaviour to help in cross - domain AEG . 
This is done mainly when we do n‚Äôt have enough training examples in our essay set . 
We would also like to explore the possibility of generating textual feedback ( rather than just a number , denoting the score of the essay ) based on the justiÔ¨Åcations that the annotators gave for their grades . 
11The p - values for the different experiments are in Appendix D.References Maria Barrett , Joachim Bingel , Nora Hollenstein , Marek Rei , and Anders S√∏gaard . 
2018 . 
Sequence classiÔ¨Åcation with human attention . 
In Proceedings of the 22nd Conference on Computational Natural Language Learning , pages 302‚Äì312 , Brussels , Belgium . 
Association for Computational Linguistics . 
Maria Barrett , Joachim Bingel , Frank Keller , and Anders S√∏gaard . 
2016 . 
Weakly supervised part - ofspeech tagging using eye - tracking data . 
In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 2 : Short Papers ) , pages 579‚Äì584 , Berlin , Germany . 
Association for Computational Linguistics . 
Rich Caruana . 
1998 . 
Multitask Learning , pages 95 ‚Äì 133 . 
Springer US , Boston , MA . 
Jacob Cohen . 
1968 . 
Weighted kappa : Nominal scale agreement provision for scaled disagreement or partial credit . 
Psychological bulletin , 70(4):213 . 
MÀòadÀòalina Cozma , Andrei Butnaru , and Radu Tudor Ionescu . 
2018 . 
Automated essay scoring with string kernels and word embeddings . 
In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 2 : Short Papers ) , pages 503‚Äì509 , Melbourne , Australia . 
Association for Computational Linguistics . 
Yann Dauphin , Harm De Vries , and Yoshua Bengio . 
2015 . 
Equilibrated adaptive learning rates for nonconvex optimization . 
In Advances in neural information processing systems , pages 1504‚Äì1512 . 
Fei Dong and Yue Zhang . 
2016 . 
Automatic features for essay scoring ‚Äì an empirical study . 
In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 1072‚Äì1077 , Austin , Texas . 
Association for Computational Linguistics . 
Fei Dong , Yue Zhang , and Jie Yang . 
2017 . 
Attentionbased recurrent convolutional neural network for automatic essay scoring . 
In Proceedings of the 21st Conference on Computational Natural Language Learning ( CoNLL 2017 ) , pages 153‚Äì162 , Vancouver , Canada . 
Association for Computational Linguistics . 
Ana V Gonz ¬¥ alez - Gardu Àúno and Anders S√∏gaard . 
2018 . 
Learning to predict readability using eye - movement data from natives and learners . 
In Thirty - Second AAAI Conference on ArtiÔ¨Åcial Intelligence .866 Ana Valeria Gonz ¬¥ alez - Gardu Àúno and Anders S√∏gaard . 
2017 . 
Using gaze to predict text readability . 
In Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications , pages 438‚Äì443 , Copenhagen , Denmark . 
Association for Computational Linguistics . 
John Hale , Chris Dyer , Adhiguna Kuncoro , and Jonathan Brennan . 
2018 . 
Finding syntax in human encephalography with beam search . 
In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 2727‚Äì2736 , Melbourne , Australia . 
Association for Computational Linguistics . 
Nora Hollenstein and Ce Zhang . 
2019 . 
Entity recognition at Ô¨Årst sight : Improving NER with eye movement information . 
In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 1‚Äì10 , Minneapolis , Minnesota . 
Association for Computational Linguistics . 
Kenneth Holmqvist , Marcus Nystr ¬®om , Richard Andersson , Richard Dewhurst , Halszka Jarodzka , and Joost Van de Weijer . 
2011 . 
Eye tracking : A comprehensive guide to methods and measures . 
OUP Oxford . 
Marcel A Just and Patricia A Carpenter . 
1980 . 
A theory of reading : From eye Ô¨Åxations to comprehension . 
Psychological review , 87(4):329 . 
Alan Kennedy , Robin Hill , and Jo ¬®el Pynte . 
2003 . 
The dundee corpus . 
In Proceedings of the 12th European conference on eye movement . 
Sigrid Klerke , Yoav Goldberg , and Anders S√∏gaard . 
2016 . 
Improving sentence compression by learning to predict gaze . 
In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 1528‚Äì1533 , San Diego , California . 
Association for Computational Linguistics . 
Yunfei Long , Rong Xiang , Qin Lu , Chu - Ren Huang , and Minglei Li . 
2019 . 
Improving attention model based on cognition grounded data for sentiment analysis . 
IEEE Transactions on Affective Computing . 
Sandeep Mathias , Diptesh Kanojia , Kevin Patel , Samarth Agrawal , Abhijit Mishra , and Pushpak Bhattacharyya . 
2018 . 
Eyes are the windows to the soul : Predicting the rating of text quality using gaze behaviour . 
In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 2352‚Äì2362 , Melbourne , Australia . 
Association for Computational Linguistics . 
Abhijit Mishra and Pushpak Bhattacharyya . 
2018 . 
Cognitively Inspired Natural Language Processing : An Investigation Based on Eye - tracking . 
Springer . 
Abhijit Mishra , Diptesh Kanojia , and Pushpak Bhattacharyya . 
2016 . 
Predicting readers ‚Äô sarcasm understandability by modeling gaze behavior . 
Abhijit Mishra , Diptesh Kanojia , Seema Nagar , Kuntal Dey , and Pushpak Bhattacharyya . 
2017 . 
Scanpath complexity : Modeling reading effort using gaze information . 
Abhijit Mishra , Srikanth Tamilselvam , Riddhiman Dasgupta , Seema Nagar , and Kuntal Dey . 
2018 . 
Cognition - cognizant sentiment analysis with multitask subjectivity summarization based on annotators ‚Äô gaze behavior . 
In Thirty - Second AAAI Conference on ArtiÔ¨Åcial Intelligence . 
Ellis B Page . 
1966 . 
The imminence of ... grading essays by computer . 
The Phi Delta Kappan , 47(5):238 ‚Äì 243 . 
Jeffrey Pennington , Richard Socher , and Christopher Manning . 
2014 . 
Glove : Global vectors for word representation . 
In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1532‚Äì1543 , Doha , Qatar . 
Association for Computational Linguistics . 
Mark D Shermis and Jill Burstein . 
2013 . 
Handbook of automated essay evaluation : Current applications and new directions . 
Routledge . 
Abhinav Deep Singh , Poojan Mehta , Samar Husain , and Rajkumar Rajakrishnan . 
2016 . 
Quantifying sentence complexity based on eye - tracking measures . 
In Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity ( CL4LC ) , pages 202‚Äì212 , Osaka , Japan . 
The COLING 2016 Organizing Committee . 
Kaveh Taghipour and Hwee Tou Ng . 
2016 . 
A neural approach to automated essay scoring . 
In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 1882‚Äì1891 , Austin , Texas . 
Association for Computational Linguistics . 
Yi Tay , Minh Phan , Luu Anh Tuan , and Siu Cheung Hui . 
2018 . 
SkipÔ¨Çow : Incorporating neural coherence features for end - to - end automatic text scoring . 
Haoran Zhang and Diane Litman . 
2018 . 
Co - attention based neural network for source - dependent essay scoring . 
In Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications , pages 399‚Äì409 , New Orleans , Louisiana . 
Association for Computational Linguistics . 
A Source Article ( Prompt 6 ) The Mooring Mast , by Marcia Amidon L ¬®usted When the Empire State Building was conceived , it was planned as the world ‚Äôs tallest building , taller even than the new Chrysler Building that was being constructed at Forty - second Street and Lexington867 Avenue in New York . 
At seventy - seven stories , it was the tallest building before the Empire State began construction , and Al Smith was determined to outstrip it in height . 
The architect building the Chrysler Building , however , had a trick up his sleeve . 
He secretly constructed a 185 - foot spire inside the building , and then shocked the public and the media by hoisting it up to the top of the Chrysler Building , bringing it to a height of 1,046 feet , 46 feet taller than the originally announced height of the Empire State Building . 
Al Smith realized that he was close to losing the title of world ‚Äôs tallest building , and on December 11 , 1929 , he announced that the Empire State would now reach the height of 1,250 feet . 
He would add a top or a hat to the building that would be even more distinctive than any other building in the city . 
John Tauranac describes the plan : ‚Äú [ The top of the Empire State Building ] would be more than ornamental , more than a spire or dome or a pyramid put there to add a desired few feet to the height of the building or to mask something as mundane as a water tank . 
Their top , they said , would serve a higher calling . 
The Empire State Building would be equipped for an age of transportation that was then only the dream of aviation pioneers . 
‚Äù This dream of the aviation pioneers was travel by dirigible , or zeppelin , and the Empire State Building was going to have a mooring mast at its top for docking these new airships , which would accommodate passengers on already existing transatlantic routes and new routes that were yet to come . 
A.1 The Age of Dirigibles By the 1920s , dirigibles were being hailed as the transportation of the future . 
Also known today as blimps , dirigibles were actually enormous steelframed balloons , with envelopes of cotton fabric Ô¨Ålled with hydrogen and helium to make them lighter than air . 
Unlike a balloon , a dirigible could be maneuvered by the use of propellers and rudders , and passengers could ride in the gondola , or enclosed compartment , under the balloon . 
Dirigibles had a top speed of eighty miles per hour , and they could cruise at seventy miles per hour for thousands of miles without needing refueling . 
Some were as long as one thousand feet , the same length as four blocks in New York City . 
The one obstacle to their expanded use in NewYork City was the lack of a suitable landing area . 
Al Smith saw an opportunity for his Empire State Building : A mooring mast added to the top of the building would allow dirigibles to anchor there for several hours for refueling or service , and to let passengers off and on . 
Dirigibles were docked by means of an electric winch , which hauled in a line from the front of the ship and then tied it to a mast . 
The body of the dirigible could swing in the breeze , and yet passengers could safely get on and off the dirigible by walking down a gangplank to an open observation platform . 
The architects and engineers of the Empire State Building consulted with experts , taking tours of the equipment and mooring operations at the U.S. Naval Air Station in Lakehurst , New Jersey . 
The navy was the leader in the research and development of dirigibles in the United States . 
The navy even offered its dirigible , the Los Angeles , to be used in testing the mast . 
The architects also met with the president of a recently formed airship transport company that planned to offer dirigible service across the PaciÔ¨Åc Ocean . 
When asked about the mooring mast , Al Smith commented : ‚Äú [ It ‚Äôs ] on the level , all right . 
No kidding . 
We ‚Äôre working on the thing now . 
One set of engineers here in New York is trying to dope out a practical , workable arrangement and the Government people in Washington are Ô¨Åguring on some safe way of mooring airships to this mast . 
‚Äù A.2 Designing the Mast The architects could not simply drop a mooring mast on top of the Empire State Building ‚Äôs Ô¨Çat roof . 
A thousand - foot dirigible moored at the top of the building , held by a single cable tether , would add stress to the building ‚Äôs frame . 
The stress of the dirigible ‚Äôs load and the wind pressure would have to be transmitted all the way to the building ‚Äôs foundation , which was nearly eleven hundred feet below . 
The steel frame of the Empire State Building would have to be modiÔ¨Åed and strengthened to accommodate this new situation . 
Over sixty thousand dollars ‚Äô worth of modiÔ¨Åcations had to be made to the building ‚Äôs framework . 
Rather than building a utilitarian mast without any ornamentation , the architects designed a shiny glass and chrome - nickel stainless steel tower that would be illuminated from inside , with a steppedback design that imitated the overall shape of the868 building itself . 
The rocket - shaped mast would have four wings at its corners , of shiny aluminum , and would rise to a conical roof that would house the mooring arm . 
The winches and control machinery for the dirigible mooring would be housed in the base of the shaft itself , which also housed elevators and stairs to bring passengers down to the eightysixth Ô¨Çoor , where baggage and ticket areas would be located . 
The building would now be 102 Ô¨Çoors , with a glassed - in observation area on the 101st Ô¨Çoor and an open observation platform on the 102nd Ô¨Çoor . 
This observation area was to double as the boarding area for dirigible passengers . 
Once the architects had designed the mooring mast and made changes to the existing plans for the building ‚Äôs skeleton , construction proceeded as planned . 
When the building had been framed to the 85th Ô¨Çoor , the roof had to be completed before the framing for the mooring mast could take place . 
The mast also had a skeleton of steel and was clad in stainless steel with glass windows . 
Two months after the workers celebrated framing the entire building , they were back to raise an American Ô¨Çag again ‚Äî this time at the top of the frame for the mooring mast . 
A.3 The Fate of the Mast The mooring mast of the Empire State Building was destined to never fulÔ¨Åll its purpose , for reasons that should have been apparent before it was ever constructed . 
The greatest reason was one of safety : Most dirigibles from outside of the United States used hydrogen rather than helium , and hydrogen is highly Ô¨Çammable . 
When the German dirigible Hindenburg was destroyed by Ô¨Åre in Lakehurst , New Jersey , on May 6 , 1937 , the owners of the Empire State Building realized how much worse that accident could have been if it had taken place above a densely populated area such as downtown New York . 
The greatest obstacle to the successful use of the mooring mast was nature itself . 
The winds on top of the building were constantly shifting due to violent air currents . 
Even if the dirigible were tethered to the mooring mast , the back of the ship would swivel around and around the mooring mast . 
Dirigibles moored in open landing Ô¨Åelds could be weighted down in the back with lead weights , but using these at the Empire State Building , where they would be dangling high above pedestrians onthe street , was neither practical nor safe . 
The other practical reason why dirigibles could not moor at the Empire State Building was an existing law against airships Ô¨Çying too low over urban areas . 
This law would make it illegal for a ship to ever tie up to the building or even approach the area , although two dirigibles did attempt to reach the building before the entire idea was dropped . 
In December 1930 , the U.S. Navy dirigible Los Angeles approached the mooring mast but could not get close enough to tie up because of forceful winds . 
Fearing that the wind would blow the dirigible onto the sharp spires of other buildings in the area , which would puncture the dirigible ‚Äôs shell , the captain could not even take his hands off the control levers . 
Two weeks later , another dirigible , the Goodyear blimp Columbia , attempted a publicity stunt where it would tie up and deliver a bundle of newspapers to the Empire State Building . 
Because the complete dirigible mooring equipment had never been installed , a worker atop the mooring mast would have to catch the bundle of papers on a rope dangling from the blimp . 
The papers were delivered in this fashion , but after this stunt the idea of using the mooring mast was shelved . 
In February 1931 , Irving Clavan of the building ‚Äôs architectural ofÔ¨Åce said , ‚Äú The as yet unsolved problems of mooring air ships to a Ô¨Åxed mast at such a height made it desirable to postpone to a later date the Ô¨Ånal installation of the landing gear . 
‚Äù By the late 1930s , the idea of using the mooring mast for dirigibles and their passengers had quietly disappeared . 
Dirigibles , instead of becoming the transportation of the future , had given way to airplanes . 
The rooms in the Empire State Building that had been set aside for the ticketing and baggage of dirigible passengers were made over into the world ‚Äôs highest soda fountain and tea garden for use by the sightseers who Ô¨Çocked to the observation decks . 
The highest open observation deck , intended for disembarking passengers , has never been open to the public . 
B Annotator ProÔ¨Åles Table 9 summarizes the proÔ¨Åles of the different annotators . 
It details each of the 8 annotators , their sex , age , occupations , L1 / native languages , their performance in a high school Examination in English and whether or not they have had experience as a TA . 
The last 3 columns are their performance869 ID Sex Age Occupation TA ? L1 Language English Score QWK Correct Close Annotator 1 Male 23 Masters student Yes Hindi 94 % 0.611 19 41 Annotator 2 Male 18 Undergraduate Yes Marathi 95 % 0.587 24 41 Annotator 3 Male 31 Research scholar Yes Marathi 85 % 0.659 21 43 Annotator 4 Male 28 Software engineer Yes English 96 % 0.659 26 44 Annotator 5 Male 30 Research scholar Yes Gujarati 92 % 0.600 19 42 Annotator 6 Female 22 Masters student Yes Marathi 95 % 0.548 19 40 Annotator 7 Male 19 Undergraduate Yes Marathi 93 % 0.732 21 46 Annotator 8 Male 28 Masters student Yes Gujarati 94 % 0.768 29 45 Table 9 : ProÔ¨Åle of the annotators on the annotation grading task , where QWK is their agreement with the ground truth scores , Correct is the number of times ( out of 48 ) where their essay scores matched with the ground truth scores , and Close is the number of times ( out of 48 ) where they disagreed with the ground truth score by at most 1 grade point . 
C Heat Map Examples C.1 Different Gaze Features Here , we show examples of heat maps for different gaze behaviour attributes of one of our readers . 
1 . 
Figure 3 shows the dwell time of the reader . 
2.Figure 4 shows the heat map of the Ô¨Årst Ô¨Åxation duration of a reader . 
3.Figure 5 shows the heat map of the IsRegression feature - i.e. whether or not the reader regressed from a particular word . 
4.Figure 6 shows the heat map of the Run Count of the reader . 
5.Figure 7 shows the words that the reader read ( highlighted ) and skipped ( unhighlighted ) . 
C.2 Dwell Times of Good and Bad Essays Figures 8 and 9 show the dwell time heat maps of a reader as he reads a good essay and a bad essay respectively . 
For the bad essay , notice the amount of a lot more darker blues compared to the good essay . 
D P - Values In this section , we report the p - values and other results for our experiments . 
D.1 Source - Dependent Essay Set ‚Äôs p - values The results shown here in Table 10 are the p - values for the different essay sets with and without gaze from Table 5.Essay Set p - value Prompt 3 0.0042 Prompt 4 0.0109 Prompt 5 0.0133 Prompt 6 0.0003 Table 10 : Source - Dependent essay set ‚Äôs p - values D.2 Unseen Essay Set ‚Äôs p - values The results shown here in Table 10 are the p - values for the different essay sets with and without gaze from Table 6 . 
Essay Set p - value Prompt 1 0.0887 Prompt 2 0.1380 Prompt 7 0.0393 Prompt 8 0.0315 Table 11 : Unseen Essay ‚Äôs p - values D.3 Native Gaze vs. No Gaze & All Gaze p - values The results shown in Table 12 are the p - values for the essay sets using the gaze behaviour of a native English speaker compared to not using gaze behaviour , and using gaze behaviour of all readers . 
Essay Set No vs. Native Native vs. All Prompt 1 0.1407 0.0471 Prompt 2 0.0161 0.9161 Prompt 3 0.3239 0.0239 Prompt 4 0.0810 0.0805 Prompt 5 0.4971 0.4010 Prompt 6 0.2462 0.2961 Prompt 7 0.0189 0.0098 Prompt 8 0.8768 0.0068 Table 12 : No gaze vs. native gaze and native gaze vs. all gaze p - values.870 Figure 3 : Sample heat map of the dwell of a reader for the text . 
The darker the blue , the larger the bin , and the longer the dwell time . 
Figure 4 : Sample heat map of the Ô¨Årst Ô¨Åxation duration of a reader for the text . 
The darker the blue , the larger the bin , and the longer the Ô¨Årst Ô¨Åxation duration . 
Figure 5 : Sample heat map of the Is Regression feature of a reader for the text . 
The highlighted words denote words that the reader regressed from . 
Figure 6 : Sample heat map of the run count of a reader for the text . 
The darker the blue , the larger the bin , and the higher the run count . 
Figure 7 : Sample heat map of the Skip feature of a reader for the text . 
The unhighlighted words denote words that the reader skipped.871 Figure 8 : Dwell Time for a reader for an essay which he scored well . 
Figure 9 : Dwell Time for a reader for an essay which he scored badly.872 Proceedings of the 1st Conference of the Asia - PaciÔ¨Åc Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing , pages 873‚Äì883 December 4 - 7 , 2020 . 
¬© 2020 Association for Computational Linguistics Multi - Source Attention for Unsupervised Domain Adaptation Xia Cui University of Liverpool Xia . 
Cui@liverpool.ac.ukDanushka Bollegala‚àó University of Liverpool , Amazon danushka@liverpool.ac.uk Abstract We model source - selection in multi - source Unsupervised Domain Adaptation ( UDA ) as an attention - learning problem , where we learn attention over the sources per given target instance . 
We Ô¨Årst independently learn sourcespeciÔ¨Åc classiÔ¨Åcation models , and a relatedness map between sources and target domains using pseudo - labelled target domain instances . 
Next , we learn domain - attention scores over the sources for aggregating the predictions of the source - speciÔ¨Åc models . 
Experimental results on two cross - domain sentiment classiÔ¨Åcation datasets show that the proposed method reports consistently good performance across domains , and at times outperforming more complex prior proposals . 
Moreover , the computed domain - attention scores enable us to Ô¨Ånd explanations for the predictions made by the proposed method.1 1 Introduction Domain adaptation ( DA ) considers the problem of generalising a model learnt using the data from a particular source domain to a different target domain ( Zhang et al . 
, 2015 ) . 
Although most DA methods consider adapting to a target domain from a single source domain ( Blitzer et al . 
, 2006 , 2007 ; Ganin et al . 
, 2016 ) , often it is difÔ¨Åcult to Ô¨Ånd a suitable single source to adapt from , and one must consider multiple sources . 
For example , in sentiment classiÔ¨Åcation , each product category is considered as a domain ( Blitzer et al . 
, 2006 ) , resulting in a multi - domain adaptation setting . 
Unsupervised DA ( UDA ) is a special case of DA where labelled instances are not available for ‚àóDanushka Bollegala holds concurrent appointments as a Professor at University of Liverpool and as an Amazon Scholar . 
This paper describes work performed at the University of Liverpool and is not associated with Amazon . 
1Source code available at https://github.com/ LivNLP / multi - source - attentionthe target domain . 
Existing approaches for UDA can be categorised into pivot - based and instancebased methods . 
Pivots refer to the features common to both source and target domains ( Blitzer et al . 
, 2006 ) . 
Pivot - based single - source domain adaptation methods , such as Structural Correspondence Learning ( SCL ; Blitzer et al . 
, 2006 , 2007 ) and Spectral Feature Alignment ( SFA ; Pan et al . 
, 2010 ) , Ô¨Årst select a set of pivots and then project the source and target domain documents into a shared space . 
Next , a prediction model is learnt in this shared space . 
However , these methods fail in multi - source settings because it is challenging to Ô¨Ånd pivots across all sources such that a single shared projection can be learnt . 
Similarly , instance - based methods , such as Stacked Denoising Autoencoders ( SDA ; Glorot et al . 
, 2011 ) and marginalised SDA ( mSDA ; Chen et al . 
, 2012 ) minimise the loss between the original inputs and their reconstructions . 
Not all of the source domains are appropriate for learning transferable projections for a particular target domain . 
Adapting from an unrelated source can result in poor performance on the given target , which is known as negative transfer ( Rosenstein et al . 
, 2005 ; Pan and Yang , 2010 ; Guo et al . 
, 2018 ) . 
Prior proposals for multi - source UDA can be broadly classiÔ¨Åed into methods that : ( a ) Ô¨Årst select a source domain and then select instances from that source domain to adapt to a given target domain test instance ( Ganin et al . 
, 2016 ; Kim et al . 
, 2017 ; Zhao et al . 
, 2018 ; Guo et al . 
, 2018 ) ; ( b ) pool all source domain instances together and from this pool select instances to adapt to a given target domain test instance ( Chattopadhyay et al . 
, 2012 ) ; ( c ) pick a source domain and use all instances in that source ( source domain selection ) ( Schultz et al . 
, 2018 ) ; and ( d ) pick all source domains and use all instances ( utilising all instances ) ( Aue and Gamon , 2005 ; Bollegala et al . 
, 2011 ; Wu and Huang , 2016 ) . 
In contrast , we propose a multi - source UDA873 method that systematically addresses the various challenges in multi - source UDA . 
‚Ä¢Although in UDA we have labelled instances in each source domain , its number is signiÔ¨Åcantly smaller than that of the unlabelled instances in the same domain . 
For example , in the Amazon product review dataset released by Blitzer et al . 
( 2007 ) there are 73679 unlabelled instances in total across the four domains , whereas there are only 4800 labelled instances . 
To increase the labelled instances in a source domain , we induce pseudo - labels for the unlabelled instances in each source domain using self - training as in ¬ß 3.1 . 
‚Ä¢In UDA , we have no labelled data for the target domain . 
To address this challenge , we infer pseudo - labels for the target domain ‚Äôs unlabelled training instances by majority voting over the classiÔ¨Åers trained from each source domain , using both labelled and pseudolabelled instances as in ¬ß 3.1 . 
‚Ä¢Given that the pseudo - labels inferred for the target domain instances are inherently more noisier compared to the manually labelled source domain instances , we propose a method to identify a subset of prototypical target domain instances for DA using document embedding similarities as described in ¬ß 3.2 . 
‚Ä¢The accuracy of UDA is upper - bounded by theH - divergence between a source and a target domain ( Kifer et al . 
, 2004 ; Ben - David et al . 
, 2006 , 2009 ) . 
Therefore , when predicting the label of a target domain test instance , we must select only the relevant labelled instances from a source domain . 
We propose a method to learn such a relatedness map between source and target domains in ¬ß 3.3 . 
‚Ä¢To reduce negative transfer , for each target domain test instance we dynamically compute a domain - attention score that expresses the relevance of a source domain . 
For this purpose , we represent each domain by a domain embedding , which we learn in an end - to - end fashion using the target domain ‚Äôs pseudo - labelled instances as detailed in ¬ß 3.4 . 
We evaluate the proposed method on two standard cross - domain sentiment classiÔ¨Åcation benchmarks for UDA . 
We Ô¨Ånd that both pseudo - labels and domain - attention scores contribute toward improving the classiÔ¨Åcation accuracy for a target domain . 
The proposed method reports consistently goodperformance in both datasets and across multiple domains . 
Although the proposed method does not outperform more complex UDA methods in some cases , using the domain - attention scores , we are able to retrieve justiÔ¨Åcations for the predicted labels . 
2 Related Work In¬ß1 we already mentioned prior proposals for single - source DA and this section discusses multisource DA , which is the main focus of this paper . 
Bollegala et al . 
( 2011 ) created a sentiment sensitive thesaurus ( SST ) using the data from the union of multiple source domains to train a cross - domain sentiment classiÔ¨Åer . 
The SST is used to expand feature spaces during train and test times . 
The performance of SST depends heavily on the selection of pivots ( Cui et al . 
, 2017 ; Li et al . 
, 2017 ) . 
Wu and Huang ( 2016 ) proposed a sentiment DA method from multiple sources ( SDAMS ) by introducing two components : a sentiment graph and a domain similarity measure . 
The sentiment graph is extracted from unlabelled data . 
Similar to SST , SDAMS uses data from multiple sources to maximise the available labelled data . 
Guo et al . 
( 2020 ) proposed a mixture of distance measures and used a multi - arm bandit to dynamically select a single source during training . 
However , in our proposed method all domains are selected and contributing differently as speciÔ¨Åed by their domain - attention weights for each train and test instance . 
Moreover , we use only one distance measure and is easier to implement . 
Recently , Adversarial NNs have become popular in DA ( Ganin et al . 
, 2016 ; Zhao et al . 
, 2018 ; Guo et al . 
, 2018 ) . 
Adversarial training is used to reduce the discrepancy between source and target domains ( Ding et al . 
, 2019 ) . 
Domain - Adversarial Neural Networks ( DANN ; Ganin et al . 
, 2016 ) use a gradient reversal layer to learn domain independent features for a given task . 
Multiple Source Domain Adaptation with Adversarial Learning ( MDAN ; Zhao et al . 
, 2018 ) generalises DANN and aims to learn domain independent features while being relevant to the target task . 
Li et al . 
( 2017 ) proposed End - to - End Adversarial Memory Network ( AMN ) , inspired by memory networks ( Sukhbaatar et al . 
, 2015 ) , and automatically capture pivots using an attention mechanism . 
Guo et al . 
( 2018 ) proposed an UDA method using a mixture of experts for each domain . 
They model the domain relations874 using a point - to - set distance metric to the encoded training matrix for source domains . 
Next , they perform joint training over all domain - pairs to update the parameters in the model by meta - training . 
However , they ignore the available unlabelled instances for the source domain . 
Adversarial training methods have shown to be sensitive to the hyper parameter values and require problem - speciÔ¨Åc techniques ( Mukherjee et al . 
, 2018 ) . 
Kim et al . 
( 2017 ) modeled domain relations using exampleto - domain based on an attention mechanism . 
However , the attention weights are learnt using source domain training data in a supervised manner . 
Following a self - training approach , Chattopadhyay et al . 
( 2012 ) proposed a two - stage weighting framework for multi - source DA that Ô¨Årst computes the weights for features from different source domains using Maximum Mean Discrepancy ( MMD ; Borgwardt et al . 
, 2006 ) . 
Next , they generate pseudo labels for the target unlabelled instances using a classiÔ¨Åer learnt from the multiple source domains . 
Finally , a classiÔ¨Åer is trained on the pseudo - labelled instances for the target domain . 
Their method requires labelled data for the target domain , which is asupervised DA setting , different from the UDA setting we consider in this paper . 
Our proposed method uses self - training to assign pseudo - labels for the unlabelled target instances , and learn an embedding for each domain using an attention mechanism . 
3 Multi - Source Domain Attention Let us assume that we are given Nsource domains , S1,S2, ... ,S N , and required to adapt to a target domainT. Moreover , let us denote the labelled instances in SibySL iand unlabelled instances by SU i. ForTwe have only unlabelled instances TU in UDA . 
Our goal is to learn a binary classiÔ¨Åer2 to predict labels ( ‚àà{0,1 } ) for the target domain instances usingSL=‚à™N i=1SL i , SU=‚à™N i=1SU iand TU . 
We denote labelled and unlabelled instances in Siby respectively xL iandxU i , whereas instances in Tare denoted by xT. To simplify the notation , we drop the superscripts LandUwhen it is clear from the context whether the instance is respectively labelled or not . 
The steps of our proposed method can be summarised as follows : ( a ) use labelled and unlabelled 2Although we consider binary sentiment classiÔ¨Åcation as an evaluation task in this paper , the proposed method can be easily extended to multi - class classiÔ¨Åcation settings by making 1 - vs - rest prediction tasks ( Rifkin and Klautau , 2004).instances from each of the source domains to learn classiÔ¨Åers that can predict the label for a given instance . 
Next , develop a majority voter and use it to predict the pseudo - labels for the target domain unlabelled instances TU(¬ß3.1 ) ; ( b ) compute a relatedness map between the target domain ‚Äôs pseudolabelled instances , TL‚àó , and source domains ‚Äô labelled instancesSL(¬ß3.3 ) ; ( c ) compute domainattention weights for each source domain ( ¬ß 3.4 ) ; ( d ) jointly learn a model based on the relatedness map and the domain - attention weights for predicting labels for the target domain ‚Äôs test instances ( ¬ß 3.5 ) . 
3.1 Pseudo - Label Generation In UDA , we have only unlabelled data for the target domain . 
Therefore , we Ô¨Årst infer pseudolabels for the target domain instances TUby selftraining ( Abney , 2007 ) following Algorithm 1 . 
SpeciÔ¨Åcally , we Ô¨Årst train a predictor fifor the i - th source domain using only its labelled instances SL iusing a base learner Œì(Line 1 - 2 ) . 
Any classiÔ¨Åcation algorithm that can learn a predictor fithat can compute the probability , fi(x , y ) , of a given instancexbelonging to the class ycan be used asŒì. In our experiments , we use logistic regression for its simplicity and popularity in prior UDA work ( Bollegala et al . 
, 2011 ; Bollegala et al . 
, 2013 ) . 
Next , for each unlabelled instance in the selected source domain , we compute the probability of it belonging to each class and Ô¨Ånd the most probable class label . 
If the probability of the most likely class is greater than the given conÔ¨Ådence thresholdœÑ‚àà[0,1 ] , we will append that instance to the current labelled training set . 
This enables us to increase the labelled instances for the source domains , which is important for learning accurate classiÔ¨Åers when the amount of labelled instances available is small . 
After processing all unlabelled instances inSi , we train the Ô¨Ånal classiÔ¨Åer fiforSi using both original and pseudo - labelled instances . 
Finally , we predict a pseudo - label for a target domain instance as the majority vote , f‚àó‚àà{0,1 } , over the predictions made by the individual classiÔ¨Åersfi . 
3.2 Prototype Selection Selecting the highest conÔ¨Ådent pseudo - labelled instances for training a classiÔ¨Åer for the target domain as done in prior work ( Zhou and Li , 2005 ; Abney , 2007 ; S√∏gaard , 2010 ; Ruder and Plank , 2018 ) does not guarantee that those instances will be the most875 Algorithm 1 Multi - Source Self - Training Input : source domains ‚Äô labelled instances SL 1, ... ,SL N , source domains ‚Äô unlabelled instances SU 1, ... ,SU Nand target domain ‚Äôs unlabelled instancesTU , target classesY , base learner Œìand the classiÔ¨Åcation conÔ¨Ådence threshold œÑ . 
Output : multi - source self - training classiÔ¨Åer f‚àó 1 : fori= 1toNdo 2 : Li‚ÜêSL i 3 : fi‚ÜêŒì(Li ) 4 : forx‚ààSU ido 5 : ÀÜy= arg max y‚ààYfi(x , y ) 6 : iffi(x,ÀÜy)>œÑthen 7 : Li‚ÜêL i‚à™{(x,ÀÜy ) } 8 : end if 9 : end for 10 : fi‚ÜêŒì(Li ) 11 : end for 12 : return majority voter f‚àóoverf1, ... ,f N. suitable ones for adapting to the target domain , which was not considered during the self - training stage . 
For example , some target instances might not be good prototypical examples of the target domain and we would not want to use the pseudolabels induced for those instances when training a classiÔ¨Åer for the target domain . 
To identify instances in the target domain that are better prototypes , we Ô¨Årst encode each target instance by a vector and select the instances that are closest to the centroid , cT , of the target domain instances given by ( 1 ) . 
cT=1 |TU|/summationdisplay x‚ààTUx ( 1 ) In the case of text documents x , their embeddings , x , can be computed using numerous approaches such as using bi - directional LSTMs ( Melamud et al . 
, 2016 ) or transformers ( Reimers and Gurevych , 2019 ) . 
In our experiments , we use the Smoothed Inversed Frequency ( SIF ; Arora et al . 
, 2017 ) , which computes document embeddings as the weighted - average of the pre - trained word embeddings for the words in a document . 
Despite being unsupervised , SIF has shown strong performance in numerous semantic textual similarity benchmarks ( Agirre et al . 
, 2015 ) . 
Using the centroid computed in ( 1 ) , similarity for target instance to the centroid is computed usingthe cosine similarity given in ( 2 ) . 
sim(x , cT ) = x / latticetopcT ||x||||cT||(2 ) Other distance measures such as the Euclidean distance can also be used . 
We use cosine similarity here for its simplicity . 
We predict the labels for the target domain unlabelled instances , TU , using f‚àó , and select the instances with the top- khighest similarities to the target domain according to ( 2)as the target domain ‚Äôs pseudo - labelled instances TL‚àó. 3.3 Relatedness Map Learning Not all of the source domain instances are relevant to a given target domain instance and the performance of a classiÔ¨Åer under domain shift can be upper bounded by the H - divergence between a source and a target domain ( Kifer et al . 
, 2004 ; Ben - David et al . 
, 2006 , 2009 ) . 
To model the relatedness between a target domain instance and each instance from theNsource domains , we use the pseudolabelled target domain instances TL‚àóand source domains ‚Äô labelled instances SL ito learn a relatedness map , œài , between a target domain instance xT(‚ààTL‚àó ) and a source domain labelled instance xL i(‚ààSL i ) as given by ( 3 ) . 
œài(xT , xL i ) = exp(xT / latticetopxL i)/summationtext x / prime‚ààSL iexp(xT / latticetopx / prime)(3 ) Usingœài , we can determine how well each instance in a source domain contributes to the prediction of the label of a target domain ‚Äôs instance . 
3.4 Instance - based Domain - Attention To avoid negative transfer , we dynamically select the source domain(s ) to use when predicting the label for a given target domain instance . 
Specifically , we learn domain - attention , Œ∏(xT , Si ) , for each source domain , Si , conditioned on xTas given by ( 4 ) . 
Œ∏(xT , Si ) = exp(xT / latticetopœÜi)/summationtextN j=1exp(xT / latticetopœÜj)(4 ) œÜican be considered as a domain embedding for Siand has the same dimensionality as the instance embeddings . 
During training we initialise œÜiusing Xavier initialisation ( Glorot and Bengio , 2010 ) and normalise such that ‚àÄxT,/summationtextN i=1Œ∏(xT , Si ) = 1 .876 3.5 Training We combine the relatedness map ( ¬ß 3.3 ) and domain - attention ( ¬ß 3.4 ) and predict the label , ÀÜy(xT ) , of a target domain instance xTusing ( 5 ) . 
ÀÜy(xT ) = œÉÔ£´ Ô£≠N / summationdisplay i=1 / summationdisplay xL i‚ààSL iy(xL i)œài(xT , xL i)Œ∏(xT , Si)Ô£∂ Ô£∏ ( 5 ) Here , œÉ(z ) = 1/(1 + exp(‚àíz))is the logistic sigmoid function and y(xL i)is the label of the source domain labelled instance xL i. First , we use the target instances , x‚ààTL‚àó , with inferred labels y‚àó(x ) ( computed using f‚àófrom Algorithm 1 ) as the training instances and predict their labels , ÀÜy(x ) , by(5 ) . 
The cross entropy error , E(ÀÜy(x),y‚àó(x))for this prediction is given by ( 6 ): E(ÀÜy(x),y‚àó(x ) ) = ‚àíŒª(x)(1‚àíy‚àó(x ) ) log(1‚àíÀÜy(x ) ) ‚àíŒª(x)y‚àó(x ) log(ÀÜy(x ) ) ( 6 ) Here , Œª(x)is a rescaling factor computed using the normalised similarity score as in ( 7 ): Œª(x ) = sim(x , cT)/summationtext x / prime‚ààTL‚àósim(x / prime , cT)(7 ) We minimise ( 6)using ADAM ( Kingma and Ba , 2015 ) for learning the domain - embeddings , œÜi . 
The initial learning rate is set to 10‚àí3using a subset ofTL‚àóheld - out as a validation dataset . 
4 Experiments To evaluate the proposed method , we use the multi - domain Amazon product review dataset compiled by Blitzer et al . 
( 2007 ) . 
This dataset contains product reviews from four domains : Books ( B ) , DVD ( D ) , Electronics ( E ) and Kitchen Appliances ( K ) . 
Following Guo et al . 
( 2018 ) , we conduct experiments under two different splits of this dataset as originally proposed by Blitzer et al . 
( 2007 ) ( Blitzer2007 ) and by Chen et al . 
( 2012 ) ( Chen2012 ) . 
Table 1 shows the number of instances in each dataset . 
By using these two versions of the Amazon review dataset , we can directly compare the proposed method against relevant prior work . 
Next , we describe how the proposed method was trained on each dataset . 
ForBlitzer2007 , we use the ofÔ¨Åcial train and test splits where each domain contains 1600 labelled training instances ( 800positive and 800negative ) , and400target test instances ( 200positive and 200negative ) . 
In addition , each domain also contains 6K-35 K unlabelled instances . 
We use 300 dimensional pre - trained GloVe embeddings ( Pennington et al . 
, 2014 ) following prior work ( Bollegala et al . 
, 2011 ; Wu and Huang , 2016 ) with SIF to create document embeddings for the reviews . 
InChen2012 , each domain contains 2000 labelled training instances ( 1000 positive and 1000 negative ) , and 2000 target test instances ( 1000 positive and 1000 negative ) . 
The remainder of the instances are used as unlabelled instances ( ca . 
4K-6 K for each domain ) . 
We use the publicly available3 5000 dimensional tf - idf vectors produced by Zhao et al . 
( 2018 ) . 
We use a multilayer perceptron ( MLP ) with an input layer of 5000 dimensions and 3hidden layers with 500dimensions . 
We use Ô¨Ånal output layer with 500dimensions as the representation of an instance . 
For each setting , we follow the standard input representation methods as used in prior work . 
It also shows the Ô¨Çexibility of the proposed method to use different ( embedding vs. BoW ) text representation methods . 
We conduct experiments for cross - domain sentiment classiÔ¨Åcation with multiple sources by selecting one domain as the target and the remaining three as sources . 
The statistics for the two settings are shown in Table 1 . 
4.1 Effect of Self - Training As described in ¬ß 3.1 , our proposed method uses self - training to generate pseudo - labels for the target domain unlabelled instances . 
In Table 2 , we compare self - training against alternative pseudo - labelling methods on Chen2012 : SelfTraining ( Self ; Abney , 2007 ; Chattopadhyay et al . 
, 2012 ) , Union Self - Training ( uni - Self ; Aue and Gamon , 2005 ) , Tri - Training ( Tri ; Zhou and Li , 2005 ) and Tri - Training with Disagreement ( TriD ; S√∏gaard , 2010 ) . 
We observe that all semisupervised learning methods improve only slightly over uni - MS , the baseline model trained on the union of all sources and tested directly on a target domain without any DA , which has been identiÔ¨Åed as a strong baseline for multi - source DA ( Aue and Gamon , 2005 ; Zhao et al . 
, 2018 ; Guo et al . 
, 2018 ) . 
Therefore , pseudo - labelling step alone is insufÔ¨Åcient for DA . 
Moreover , we observe that all semi - supervised methods perform comparably . 
3https://github.com/KeiraZhao/MDAN/877 Target Source Train Test Unlabel Train Test Unlabel Blitzer2007 ( Blitzer et al . 
, 2006 ) Chen2012 ( Chen et al . 
, 2012 ) B D , E , K 1600√ó3400 6000 2000√ó32000 4465 D B , E , K 1600√ó3400 34741 2000√ó32000 5586 E B , D , K 1600√ó3400 13153 2000√ó32000 5681 K B , D , E 1600√ó3400 16785 2000√ó32000 5945 Table 1 : Number of train , test and unlabelled instances for the two Amazon product review datasets . 
( a ) prob sorted in ascending order   ( b ) prob sorted in descending order Figure 1 : The number of selected pseudo - labelled instances konBlitzer2007 is shown on the x - axis . 
prob denotes prediction conÔ¨Ådence from the pseudo classiÔ¨Åer trained on the source domains , sim denotes the similarity to the target domain , asc and dsc respectively denote sorted in ascending and descending order ( only applied to prob related selection methods , sim is always sorted in dsc ) . 
prob only denotes using only prediction conÔ¨Ådence , simonly denotes using only target similarity . 
prob sim indicates selecting by prob Ô¨Årst and then sim ( likewise for sim prob ) . 
prob√ósim denotes using the product of prob and sim , and prob+sim denotes using their sum . 
The marker for the best result of each method is Ô¨Ålled . 
Example ( 1 ) Why anybody everest feet would want reading this ? ... pure pleasure why 29028 feet account this ? ... It ‚Äôs a pleasure to read . 
( a )   ( b )   ( c ) Figure 2 : A positively labelled a target test instance in B(top ) and resulted Œ∏ , œàiand the product of œàiand Œ∏(bottom ) . 
Here , the x - axis represents the instances and the y - axis represents the prediction scores . 
Instance speciÔ¨Åc values in ( a ) and ( c ) are shown as > 0for positive labelled instances and otherwise < 0 . 
Source instances from D , EandKare shown in blue , green and red respectively . 
The contributions from top- 150instances from three source domains are shown . 
4.2 Pseudo - labelled Instances Selection When selecting the pseudo - labelled instances from the target domain for training a classiÔ¨Åer for the target domain , we have two complementary strategies : ( a ) select the most conÔ¨Ådent instances according tof‚àó(denoted by prob ) or ( b ) select the most sim - ilar instances to the target domain ‚Äôs centroid ( denoted by sim ) . 
To evaluate the effect of these two strategies and their combinations ( i.e prob+sim and prob√ósim ) , in Figure 1 , we select target instances with each strategy and measure the accuracy on the target domain Bfor increasing numbers of in-878 Example ( 2 ) Her relationship limited own pass her own analysis , there ‚Äôre issues mainly focus in turn for codependency . 
Disappointing , dysfunctional . 
Mother‚Äôll book her daughter ‚Äôs turn the pass , message turn the message issues analysis of very disappointing information . 
( a )   ( b )   ( c ) Figure 3 : A negatively labelled target test instance in B. DM L Score Evidences ( Reviews ) E - 0.16943 Serious problems . 
E - 0.02823 Sound great but lacking isolation in other areas . 
E + 0.02801 Cases for the cats walking years , no around and knocking ... walking on similar cases of cats . 
E + 0.02233 Cord supposed to no problems , this extension extension not worked as cord did ... whatever expected just worked Ô¨Åne . 
E - 0.02209 Buy this like characters not used names ... be aware of many commonly used characters before you accept Ô¨Åle like drive . 
Table 3 : The top- 5evidences for Example ( 2 ) selected from the source domains . 
DM denotes the domain of the instance . 
L denotes the label for the instance . 
Score is œài(x)Œ∏(x ) . 
T uni - MS Self uni - Self Tri Tri - D B 79.46 79.60 79.46 79.61 79.51 D 82.32 82.49 82.35 82.35 82.35 E 84.93 84.97 84.93 84.99 84.93 K 87.17 87.18 87.17 87.15 87.23 Table 2 : ClassiÔ¨Åcation accuracies ( % ) for semisupervised methods on Chen2012 . 
stanceskin the descending ( dsc ) and ascending ( asc ) order of the selection scores . 
From Figure 1b we observe that selecting the highest conÔ¨Ådent instances does not produce the best UDA accuracies . 
In fact , merely selecting instances based on conÔ¨Ådence scores only ( corresponds to prob only ) reports the worst performance . 
On the other hand , instances that are highly similar to the target domain ‚Äôs centroid are more effective for DA . 
We observe that with only k= 1000 instances , sim only reaches almost its optimal accuracy . 
Using validation data , we estimated that k= 2000 to be sufÔ¨Åcient for all domains to reach the peak performance regardless of the selection strategy . 
Therefore , we selected 2000 pseudolabelled instances for the attention step . 
In ourexperiments , we used sim only to select pseudolabelled instances because it steadily improves the classiÔ¨Åcation accuracy with kfor all target domains , and is competitive against other methods . 
4.3 Effect of the Relatedness Map In Table 4 we report the classiÔ¨Åcation accuracy on the test instances in the target domain over the different steps : uni - MS ( no adapt baseline ) , Self(selftraining ) , PL(pseudo - labelling ) and Att(attention ) . 
We use the self - training method described in Algorithm 1 . 
The results clearly demonstrate a consistent improvement over all the steps in the proposed method . 
For Selfstep , the proposed method improves the accuracy only slightly without any information from the target domain . 
In the PLstep , we report the results of a predictor trained on target pseudo - labelled instances . 
We report the evaluation results for the trained attention model in Att . 
InAttstep , we use the relatedness map œàito express the similarity between a target instance and each of source domain instances , and the domain attention score Œ∏to express the relation between a target instance and each of the source domain instances . 
Two example test instances ( one positive and one negative ) from the target domain B879 T uni - MS Self PL Att B 79.46 79.60 79.57 79.68 D 82.32 82.49 82.71 82.96 E 84.93 84.97 85.30 85.30 K 87.17 87.18 87.30 87.48 Table 4 : ClassiÔ¨Åcation accuracies ( % ) across different steps of the proposed method , evaluated on Chen2012 . 
are shown in Figures 2 and 3 . 
We observe that different source instances contribute to the predicted labels in different ways . 
As expected , in Figure 2a more positive source instances are selected using the relatedness map for a positive target instance , and Figure 3a more negative source instances are selected for a negative target instance . 
After training , we Ô¨Ånd that the proposed method identiÔ¨Åes the level of importance of different source domains . 
Example ( 1 ) is closer to D , whereas Example ( 2 ) is closer to Ewith a very high value of Œ∏ . 
Figures 2c and 3c show that the instance speciÔ¨Åc contribution to the target instance . 
The proposed method also identiÔ¨Åes the level of importance within the most relevant source domain . 
Figure 3 shows the actual reviews as the top- 5evidences from the source domains in Example ( 2 ) . 
Negative labelled source training instance from E:‚ÄúSerious problem . 
‚Äù is the most important instance with the highest contribution ofœài(x)Œ∏(x)to the decision . 
4.4 Comparisons against Prior Work Table 5 compares the proposed method against the following methods on Blitzer2007 dataset . 
SCL : Structural Correspondence Learning ( Blitzer et al . 
, 2006 , 2007 ) is a single - source DA method , trained on the union of all source domains and tested on the target domain . 
We report the published results from Wu and Huang ( 2016 ) . 
SFA : Spectral Feature Alignment ( Pan et al . 
, 2010 ) is a single - source DA method , trained on the union of all source domains , and tested on the target domain . 
We report the published results from Wu and Huang ( 2016 ) . 
SST : Sensitive Sentiment Thesaurus ( Bollegala et al . 
, 2011 ; Bollegala et al . 
, 2013 ) is the SoTA multi - source DA method on Blitzer2007 . 
We report the published results from Bollegala et al . 
( 2011 ) . 
SDAMS : Sentiment Domain Adaptation with Multiple Sources proposed by Wu and Huang ( 2016 ) . 
We report the results from the original paper . 
T uni - MS SCL SFA SST SDAMS AMN Proposed B 80.00 74.57 75.98 76.32 78.29 79.75 83.50 D 76.00 76.30 78.48 78.77 79.13 79.83 80.50 E 74.75 78.93 78.08 83.63 * 84.18 * * 80.92 * 80.00 * K 85.25 82.07 82.10 85.18 86.29 85.00 86.00 Table 5 : ClassiÔ¨Åcation accuracies ( % ) for the proposed method and prior work on Blitzer2007 . 
Statistically signiÔ¨Åcant improvements over uni - MS according to the Binomial exact test are shown by ‚Äú * ‚Äù and ‚Äú * * ‚Äù respectively at p= 0.01andp= 0.001levels . 
T uni - MS mSDA DANN MDAN MoE Proposed B 79.46 76.98 76.50 78.63 79.42 79.68 D 82.32 78.61 77.32 80.65 83.35 82.96 E 84.93 81.98 83.81 85.34 86.62 85.30 K 86.71 84.26 84.33 86.26 87.96 87.48 Table 6 : ClassiÔ¨Åcation accuracies ( % ) for the proposed method and prior work on Chen2012 . 
AMN : End - to - End Adversarial Memory Network ( Li et al . 
, 2017 ) is a single - source DA method , trained on the union of all source domains , and tested on the target domain . 
We report the published results from Ding et al . 
( 2019 ) . 
In Table 6 , we compare our proposed method against the following methods on Chen2012 . 
mSDA : Marginalized Stacked Denoising Autoencoders proposed by Chen et al . 
( 2012 ) . 
We report the published results from Guo et al . 
( 2018 ) . 
DANN : Domain - Adversarial Neural Networks proposed by Ganin et al . 
( 2016 ) . 
We report the published results from Zhao et al . 
( 2018 ) . 
MDAN : Multiple Source Domain Adaptation with Adversarial Learning proposed by Zhao et al . 
( 2018 ) . 
We report the published results from the original paper . 
MoE : Mixture of Experts proposed by Guo et al . 
( 2018 ) . 
We report the published results from the original paper . 
From Tables 5 and 6 , we observe that the proposed method obtains the best classiÔ¨Åcation accuracy on Books domain ( B ) in both settings , which is the domain with the smallest number of unlabelled instances . 
In particular , when the amount of training instances are small , pseudo - labelling and domain - attention in our proposed method play a vital role in multi - source UDA . 
Although SDAMS ( inBlitzer2007 ) and MoE ( inChen2012 ) outperform the proposed method , the simplicity and the ability to provide explanations are attractive properties for a UDA method when applying in an industrial setting involving a massive number of880 source domains such as sentiment classiÔ¨Åcation in E - commerce reviews . 
5 Conclusions We propose a multi - source UDA method that combines self - training with an attention module . 
In contrast to prior works that select pseudo - labelled instances based on prediction conÔ¨Ådence of a predictor learnt from source domains , our proposed method uses similarity to the target domain during adaptation . 
Our proposed method reports competitive performance against previously proposed multi - source UDA methods on two splits on a standard benchmark dataset . 
Abstract Large pre - trained language models reach stateof - the - art results on many different NLP tasks when Ô¨Åne - tuned individually ; They also come with a signiÔ¨Åcant memory and computational requirements , calling for methods to reduce model sizes ( green AI ) . 
We propose a twostage model - compression method to reduce a model ‚Äôs inference time cost . 
We Ô¨Årst decompose the matrices in the model into smaller matrices and then perform feature distillation on the internal representation to recover from the decomposition . 
This approach has the beneÔ¨Åt of reducing the number of parameters while preserving much of the information within the model . 
We experimented on BERTbase model with the GLUE benchmark dataset and show that we can reduce the number of parameters by a factor of 0.4x , and increase inference speed by a factor of 1.45x , while maintaining a minimal loss in metric performance . 
1 Introduction Deep learning models have been demonstrated to achieve state - of - the - art results , but require large parameter storage and computation . 
It ‚Äôs estimated that training a Transformer model with a neural architecture search has a CO 2emissions equivalent to nearly Ô¨Åve times the lifetime emissions of the average U.S. car , including its manufacturing ( Strubell et al . 
, 2019 ) . 
Alongside the increase in deep learning models complexity , in the NLP domain , there has been a shift in the NLP modeling paradigm from training a randomly initialized model to Ô¨Åne - tuning a large and computational heavy pre - trained language model ( Howard and Ruder , 2018 ; Peters et al . 
, 2018 ; Devlin et al . 
, 2018 ; Radford , 2018 ; Radford et al . 
, 2019 ; Dai et al . 
, 2019 ; Yang et al . 
, 2019 ; Lample and Conneau , 2019 ; Liu et al . 
, 2019b ; Raffel et al . 
, 2019 ; Lan et al . 
, 2019 ; Lewis et al . 
, 2019).While re - using pre - trained models offsets the training costs , inference time costs of the Ô¨Ånetuned models remain signiÔ¨Åcant , and are showstoppers in many applications . 
The main challenge with pre - trained models is how can we reduce their size while saving the information contained within them . 
Recent work , approached this by keeping some of the layers while removing others ( Sanh et al . 
, 2019 ; Sun et al . 
, 2019 ; Xu et al . 
, 2020 ) . 
A main drawback of such approach is in its coarse - grained nature : removing entire layers might discard important information contained within the model , and working at the granularity of layers makes the trade - off between compression and accuracy of a model hard to control . 
Motivated by this , in this work we suggest a more Ô¨Ånegrained approach which decomposes each matrix to two smaller matrices and then perform feature distillation on the internal representation to recover from the decomposition . 
This approach has the beneÔ¨Åt of preserving much of the information while reducing the number of parameters . 
Alongside the advantage of preserving the information within each layer , there is also a memory Ô¨Çexibility advantage compared to removing entire layers ; As a result of decomposing each matrix to two smaller matrices , we can store each of the two matrices in two different memory blocks . 
This has the beneÔ¨Åt of distributing the model matrices in many small memory blocks , which is useful when working in shared CPU - based environments . 
We evaluated our approach on the General Language Understanding Evaluation ( GLUE ) benchmark dataset ( Wang et al . 
, 2018 ) and show that our approach is superior or competitive in the different GLUE tasks to previous approaches which remove entire layers . 
Furthermore , we study the effects of different base models to decompose and show the superiority of decomposing a Ô¨Åne - tuned model compared to a pre - trained model or a ran-884 domly initialized model . 
Finally , we demonstrate the trade - off between compression and accuracy of a model . 
2 Related Work In the past year , there have been many attempts to compress transformer models involving pruning ( McCarley , 2019 ; Guo et al . 
, 2019 ; Wang et al . 
, 2019 ; Michel et al . 
, 2019 ; V oita et al . 
, 2019 ; Gordon et al . 
, 2020 ) , quantization ( Zafrir et al . 
, 2019 ; Shen et al . 
, 2019 ) and distillation ( Sanh et al . 
, 2019 ; Zhao et al . 
, 2019 ; Tang et al . 
, 2019 ; Mukherjee and Awadallah , 2019 ; Sun et al . 
, 2019 ; Liu et al . 
, 2019a ; Jiao et al . 
, 2019 ; Izsak et al . 
, 2019 ) . 
SpeciÔ¨Åcally , works on compressing pretrained transformer language models focused on pruning layers . 
Sun et al . 
( 2019 ) suggested to prune layers while distilling information from the unpruned model layers . 
Xu et al . 
( 2020 ) proposed to gradually remove layers during training . 
We also note that very recently a work similar to ours was uploaded to arxiv ( Mao et al . 
, 2020 ) . 
There are a few differences from their work to ours . 
Firstly , we distill different parts of the model ( see Section 3 for details ) . 
Secondly , we focus on training the decomposed model and do not prune the model parameters . 
Thirdly , our base model , which is used for decomposition and as a teacher , is a Ô¨Åne - tuned model ; This has the beneÔ¨Åt of task - speciÔ¨Åc information as we show in our experiments in Section 4.2 . 
3 Method Our goal is to decompose each matrix W‚ààRn√ód as two smaller matrices , obtaining an approximated matrix W / prime = AB , A‚ààRn√ór , B‚ààRr√ód , wherer < nd n+d . 
We seek a decomposition s.t . 
W / primeis close toWin the sense that d(Wx , W / primex ) is small for all x , wheredis a distance metric between vectors . 
In practice , we require the condition to hold not for all x , but for vectors seen in a Ô¨Ånite relevant sample ( in our case , the training data ) . 
While one could start with random matrices and optimize the objective using gradient descent , we show that a two - staged approach performs better : we Ô¨Årst decompose the matrices using SVD , obtaining A / prime , B / primes.t.||A / primeB / prime‚àíW||2 2 is small ( SVD is guaranteed to produce the best rank - r approximation to W , ( Stewart , 1991 ) ) . 
We then use these matrices as initialization and optimized(Wx , W / primex)(feature distillation ) , whilealso optimizing for task loss . 
We show that this process works substantially better in practice . 
Our loss function is thus composed of three different objectives : Cross Entropy Loss The cross entropy loss over an example xwith labelyis deÔ¨Åned likewise : LCE=‚àílogps(y|x ) , wherepsis the probability for label ygiven by the decomposed student model . 
Knowledge Distillation Loss The goal of knowledge distillation is to imitate the output layer of a teacher model by a student model . 
The Knowledge Distillation Loss is deÔ¨Åned likewise : LKD=/vextenddouble / vextenddoublezs‚àízt T / vextenddouble / vextenddouble 2 , where zsandztare the logits of the decomposed and original models respectively and T is a temperature hyper - parameter . 
Feature Distillation Loss The goal of feature distillation is to imitate the intermediate layers of a teacher model by a student model . 
we use the following intermediate representations to distill the knowledge from1 : ‚Ä¢Query , Key and Value Layers - The dot product of a matrix of concatenated tokens representation vectors Xby the query , key and value parameter matrices , Zq = X¬∑WQ , Zk = X¬∑WK , Zv = X¬∑WV ‚Ä¢Attention Matrix - The attention matrix probabilities . 
Zatt = softmax ( Zq¬∑ZT k ) ‚Ä¢Attention Heads - The output of the attention heads . 
ZH = Zatt¬∑Zv ‚Ä¢The Multihead Attention Layer Output - The dot product of the attention heads by the matrixWO.ZMH = ZH¬∑WO ‚Ä¢The Ô¨Årst feed forward layer - The dot product of the multihead attention layer by the Ô¨Årst feed forward layer . 
Zf1 = ZMH¬∑W1 ‚Ä¢The second feed forward layer - The dot product of the Ô¨Årst feed forward layer by the second feed forward layer . 
Zf2 = Zf1¬∑W2 We denoteSi zandTi zas the intermediate representations which were described above of layer ifor 1We follow the notations of Vaswani et al . 
( 2017 ) for the transformer parameters and omit biases for notation convenience.885 the decomposed student and original teacher models respectively . 
Our loss function then is deÔ¨Åned by : LFD=/summationtext iTi z , Si z / summationtext Tz , Sz / bardblTz‚àíSz / bardbl2 Full Objective Our loss function is then deÔ¨Åned by a weighted combination of these three loss functions likewise : L = Œ±LCE+ ( 1‚àí Œ±)LKD+LFDwhereŒ±‚àà[0,1]is a chosen hyperparameter . 
4 Experiments We compare various variants of our compression method , corresponding to different subsets of our loss . 
All variants decompose the matrices using SVD , but differ in their objective functions . 
These correspond to the four last lines in Table 1 . 
Low Rank BERT Fine - tuning ( LRBF ) corresponds toL = LCE . 
LRBF+KD corresponds to L = Œ±LCE+ ( 1‚àíŒ±)LKD . 
LRBF+FD corresponds toL = LCE+LFD , while LRBF+FD+KD corresponds to the complete objective . 
The other lines in the table correspond to uncompressed model ( Ô¨Årst line ) and to baselines which prune layers and distill . 
Fine - tuning Ô¨Ånetunes a six layered BERT model . 
Vanilla KD trains a six - layered BERT model with L= Œ±LCE+(1‚àíŒ±)LKD . 
BERT - PKD trains a six layered BERT model with L = Œ±LCE+(1‚àíŒ±)LKD while also adding an LFDobjective , but on the hidden states between every consecutive layer . 
BERT - of - Theseus Ô¨Åne - tunes BERT model while gradually pruning half of the layers . 
We chose this baselines for several reasons : like our method they result in a practical reduction of parameters;2 , they are task - speciÔ¨Åc;3and they do not require the pretraining stage , which is expensive and not practical for most practitioners . 
Datasets We evaluate our proposed approach on the General Language Understanding Evaluation ( GLUE ) benchmark ( Wang et al . 
, 2018 ) , a collection of diverse NLP tasks . 
Training Details We Ô¨Åne - tune a pre - trained BERT model ( Devlin et al . 
, 2018 ) for each task with a batch size of 8and a learning rate of 2e‚àí5 for3epochs with an early stop mechanism according to the validation set . 
We perform the matrix 2Unlike , e.g. , pruning , which sets parameters to zero and requires specialized hardware to fully take advantage of . 
3Unlike , e.g. , DistillBERT which is meant to be run before Ô¨Åne-tuning.decomposition on every parametric weight matrix of the encoder ( excluding the embedding matrix ) in a Ô¨Åne - tuned model and train the decomposed model as the student model and the original Ô¨Åne - tuned model as the teacher . 
For each task we train for 3epochs with an early stopping mechanism according to the task validation set , the maximum sequence length is 128 and we perform a grid search over the learning rates { 2e‚àí6,5e‚àí6,2e‚àí5,5e‚àí5,2e‚àí4,5e‚àí4}and5different seeds and choose the best model according to the validation set of each task.4For knowledge distillation hyper - parameters we used a temperature hyper - parameter T= 10 andŒ±= 0.7.5 4.1 Main Results Table 1 compares the results for validation and test of other compression approaches which prune layers , along with low rank models which were Ô¨Ånetuned and trained with one or more of the distillation objectives described in Section 3 . 
As can be seen , Low Rank BERT Feature Distillation + KD and Low Rank BERT Feature Distillation surpass all of results of all methods in both validation and test sets except BERT - of - Theseus method in the test set , in which Low Rank BERT Feature Distillation + KD surpasses the results in 5 of the tasks and reach comparable results in 2of the tasks . 
Also , as can be seen knowledge distillation alone is not sufÔ¨Åcient to compensate for the decomposition , but it slightly improves the results when incorporating feature distillation alone . 
4.2 Further Analysis Effect of Base Model and Decomposition In this experiment we test the importance of the base model we use to decompose and use as a teacher . 
We compared between three types of distillation sources : Ô¨Åne - tuned teacher , pre - trained teacher and no teacher . 
Furthermore , we compared between three types of model initializations : a decomposed Ô¨Åne - tuned model , a decomposed pretrained model and a randomly initialized model with the same architecture as the decomposed models . 
The results are shown in Table 2 , on all tasks when training with no teacher distilla4We detailed the changes we made to the original Ô¨Ånetuning procedure , every other hyper - parameters which were not mentioned , is set as described in ( Devlin et al . 
, 2018 ) . 
5We chose those hyper - parameters from a grid search over T={5,10,20}andŒ±={0.2,0.5,0.7}on the MRPC validation set.886 MethodCoLA MNLI MRPC QNLI QQP RTE SST-2 STS - B Macro Score Dev Test Dev Test Dev Test Dev Test Dev Test Dev Test Dev Test Dev Test Dev Test Uncompressed Models - 110 M Parameters BERT - base ( uncompressed ) 59.9 53.9 84.6 83.9 89.0 85.6 91.6 90.9 88.1 80.2 71.5 67.2 93.5 93.6 89.8 84.8 83.5 80.0 6 Layers Transformer Models - 66 M Parameters Fine - tuning 43.4 41.5 80.1 80.1 86.0 83.1 86.9 86.7 87.8 78.7 62.1 63.6 89.6 90.7 81.9 81.1 77.2 75.7 Vanilla KD ( Hinton et al . 
, 2015 ) 45.1 42.9 80.1 80.0 86.2 83.4 88.0 88.3 88.1 79.5 64.9 64.7 90.5 91.5 84.9 81.2 78.5 76.4 BERT - PKD ( Sun et al . 
, 2019 ) 45.5 43.5 81.3 81.3 85.7 82.5 88.4 89.0 88.4 79.8 66.5 65.5 91.3 92.0 86.2 82.5 79.2 77.0 BERT - of - Theseus ( Xu et al . 
, 2020 ) 51.1 47.8 82.3 82.3 89.0 85.4 89.5 89.6 89.6 80.5 68.2 66.2 91.5 92.2 88.7 84.9 81.2 78.6 Low Rank Approximated Models - 65.2 M Parameters ( This Work ) Low Rank BERT Fine - tuning 41.0 40.5 82.9 82.3 82.4 79.8 89.4 88.8 89.0 79.5 65.0 60.4 91.3 92.0 87.0 81.2 78.5 75.6 Low Rank BERT + KD 44.7 34.0 83.1 82.4 83.4 80.4 89.1 88.7 89.0 79.9 64.3 60.6 91.3 91.5 86.6 80.9 78.9 74.8 Low Rank BERT Feature Distillation 51.2 43.4 84.9 83.8 89.4 86.1 91.4 90.7 89.8 80.5 70.8 66.0 92.2 92.9 89.3 84.2 82.4 78.4 Low Rank BERT Feature Distillation + KD 53.0 42.9 84.8 83.7 90.4 86.2 91.4 90.8 89.7 80.5 71.1 67.8 92.4 92.9 89.4 84.6 82.8 78.7 Table 1 : Results on GLUE dev and test sets . 
Metrics are Accuracy ( MNLI ( average of MNLI match and MNLI mis - match ) , QNLI , RTE , SST-2 ) , Avg of Accuracy and F1 ( MRPC , QQP ) , Matthew ‚Äôs correlation ( CoLA ) , Avg of Pearson and Spearman correlations ( STS - B ) . 
BERT - base ( Teacher ) is our Ô¨Åne - tuned BERT model . 
The numbers for the 6 layered models are taken from ( Xu et al . 
, 2020 ) , Best results are indicated in Bold . 
CoLA MRPC SST-2 Base Model / Teacher Model Fine - tuned Pre - trained None Fine - tuned Pre - trained None Fine - tuned Pre - trained None Fine - tuned 48.7¬±2.4 47.5¬±0.7 40.1¬±0.6 88.5¬±0.5 85.8¬±0.5 81.6¬±1.3 91.8¬±0.4 91.3¬±0.5 90.9¬±0.4 Pre - trained 49.4¬±1.7 44.8¬±2.1 10.8¬±2.6 89.2¬±0.4 86.3¬±1.0 77.1¬±0.6 91.7¬±0.2 91.2¬±0.4 89.6¬±1.1 Random 3.6¬±5.1 0.0¬±0.0 0.6¬±0.6 75.9¬±1.1 75.3¬±0.7 75.0¬±0.4 88.2¬±0.5 87.2¬±0.7 81.2¬±0.5 Table 2 : Results on the dev set of CoLA , MRPC and SST-2 tasks with different initializations and different teachers . 
The results are averages and standard deviations of Ô¨Åve runs with different seeds . 
tion , the results are best when decomposing a Ô¨Ånetuned model and decomposing a pre - trained model is better than randomly initializing a model ; This indicates that the decomposition saves the information within the model and when decomposing a Ô¨Åne - tuned model it saves some of the more task speciÔ¨Åc information . 
Furthermore , on all tasks and all initialization the best results are when using a Ô¨Åne - tuned model as a teacher . 
Rank ( Parameter Count ) CoLA MRPC SST-2 Full Rank ( 110 M ) 58.4¬±1.2 88.3¬±0.7 92.8¬±0.5 350 ( 82.6 M ) 57.7¬±0.9 88.9¬±0.7 92.0¬±0.5 245 ( 65.2 M ) 48.7¬±2.4 88.5¬±0.5 91.8¬±0.4 150 ( 49.4 M ) 38.7¬±1.6 87.8¬±0.6 91.3¬±0.4 Table 3 : Results on the dev set of CoLA , MRPC and SST-2 tasks with different ranks . 
The results are averages and standard deviations of Ô¨Åve runs with different seeds . 
Compression vs. Performance Trade - off Our method requires to determine a rank for the compression . 
But can we achieve better results when choosing a higher rank ? Can we choose a lower rank for smaller models and still achieve satisfactory results ? To determine this we experimented on three different ranks . 
As shown in Table 3 , higher ranks achieve better results , while lower ranks achieve satisfactory results while compromising metric performance . 
Figure 1 : Average time in milliseconds to run a batch of samples from all of the GLUE tasks , when running on a Intel(R ) Xeon(R ) Platinum 8180 CPU @ 2.50GHz and on a single TITAN V 12 GB GPU . 
Run - time Savings In this experiment we measured the average time in milliseconds it takes for BERT - base compared to its decomposed and six - layered counterparts to output predictions for a batch of samples with varying batch sizes . 
As shown in Figure 1 , we still gain a signiÔ¨Åcant time performance improvement when running on both CPU and GPU architectures over a BERT - base887 model . 
Models that are decomposed to a rank r= 245are about 1.45faster than their uncompressed counterpart for batches larger than one when running on a GPU and around 1.2‚àí1.55faster for batches 8,16,32,64when running on a CPU . 
Furthermore , higher ranks still beneÔ¨Åt running time and lower ranks improve the running time further . 
Also , we note that although a six - layered BERT does achieve faster inference time , due to the coarse - grained compression , it losses more information contained within it and thus achieves inferior results ; As shown in the results in Table 1 , a six - layered model trained with distillation ( e.g. BERT - PKD ( Sun et al . 
, 2019 ) ) achieves signiÔ¨Åcantly lower results and the BERT - of - Theseus model , which does improve upon BERT - PKD , requires many training iterations to achieve this to overcome the loss of information when gradually removing entire layers , which result in higher training times . 
5 Conclusions We presented a way to compress pre - trained large language models Ô¨Åne - tuned for speciÔ¨Åc tasks , while preserving much of the information contained within them , by using matrix decomposition to two small matrices . 
For future work it might be interesting to combine this approach with another approach such as pruning or quantization to achieve smaller models . 
Acknowledgements This project has received funding from the Europoean Research Council ( ERC ) under the Europoean Union ‚Äôs Horizon 2020 research and innovation programme , grant agreement No . 
802774 ( iEXTRACT ) and was sponsored in part by an Intel AI grant to the Bar - Ilan University NLP lab . 
Abstract Explainable recommendation is a good way to improve user satisfaction . 
However , explainable recommendation in dialogue is challenging since it has to handle natural language as both input and output . 
To tackle the challenge , this paper proposes a novel and practical task to explain evidences in recommending hotels given vague requests expressed freely in natural language . 
We decompose the process into two subtasks on hotel reviews : evidence identiÔ¨Åcation andevidence explanation . 
The former predicts whether or not a sentence contains evidence that expresses why a given request is satisÔ¨Åed . 
The latter generates a recommendation sentence given a request and an evidence sentence . 
In order to address these subtasks , we build an Evidence - based Explanation dataset , which is the largest dataset for explaining evidences in recommending hotels for vague requests . 
The experimental results demonstrate that the BERT model can Ô¨Ånd evidence sentences with respect to various vague requests and that the LSTM - based model can generate recommendation sentences . 
1 Introduction Recently , dialog systems using Natural Language Processing technology have been adopted in interactive services such as call centers ( Zumstein and Hundertmark , 2017 ) . 
One challenging issue in a real - world scenario is vague requests1from users . 
For example , in a hotel booking service , users often ask operators for ‚Äú a child - friendly hotel ‚Äù or ‚Äú a convenient inn . 
‚Äù To respond to such vague requests , human operators need to explain the reason why the given request 1In this study , a vague request means one that does not specify a speciÔ¨Åc product , experience or service.is satisÔ¨Åed . 
An example response would be , ‚Äú This hotel has a large kids ‚Äô space , so I recommend it for families with children like you . 
‚Äù Responding to vague requests with evidences is effective because it not only strengthens the recommendation , but also urges users to make more concrete requests such as ‚Äú I do n‚Äôt need a kids ‚Äô space but want a baby stroller rental service . 
‚Äù Several studies have addressed explainable recommendations that produce natural language sentences ( Zhao et al . 
, 2014 ; Zhang et al . 
, 2014 ; Wang et al . 
, 2018 ; Zhao et al . 
, 2019 ) . 
One major approach is feature - based explanations . 
Zhang et al . 
( 2014 ) generated explanation sentences using templates with slots , for example , ‚Äú You might be interested in [ feature ] , on which this product performs well . 
‚Äù However , by handling only predeÔ¨Åned and limited features , this study can not explain detailed evidences for each hotel such as ‚Äú a view of Mount Fuji and Lake Kawaguchi . 
‚Äù Furthermore , this study does not accept natural language requests as inputs , which is a major bottleneck for building dialog - based interactive systems . 
In this study , we propose a novel and practical task to identify and explain evidences that satisfy a given vague request expressed freely in natural language . 
SpeciÔ¨Åcally , assuming a practical situation of recommendation , we address a hotel booking service . 
When choosing a hotel on an interactive service , users make a wide range of vague requests , which differ from predeÔ¨Åned aspects ( Wang et al . 
, 2010 ) , emotional expressions ( Chen et al . 
, 2010 ) and questions ( Rajani et al . 
, 2019 ) . 
In order to satisfy vague requests by recommending hotels with evidences , the system must understand a given request , associate the request to a hotel with spe-890 Figure 1 : Pipeline for building the Evidence - based Explanation dataset ciÔ¨Åc evidence , and generate an explanation ( recommendation sentence ) for the evidence . 
To address these challenges , we decompose the process into two subtasks : Evidence IdentiÔ¨ÅcationandEvidence Explanation . 
The former predicts whether a sentence contains evidence that expresses why a given request is satisÔ¨Åed . 
The latter generates a recommendation sentence given the evidence sentence . 
In order to focus on evidence explanations for requests , we assume that recommending hotels are given in advance in this study . 
For these subtasks , we present an Evidencebased Explanation dataset , which is the largest dataset for explaining evidences in recommending hotels for vague requests . 
Assuming that titles of hotel reviews often correspond to vague requests , the dataset includes 37,280 hotel reviews with annotations for vague requests , evidence sentences for the requests , recommendation sentences based on the evidence sentences . 
The key feature of the dataset is the variety of requests : it includes 15,767 unique types of requests written in natural language . 
This dataset is publicly available2 . 
We report experiments for the two subtasks in Section 3 . 
We build a BERT ( Devlin et al . 
, 2019 ) model for the Ô¨Årst subtask , which predicts whether a sentence contains evidence for a request . 
Experimental results show that the model can detect evidence sentence for various requests with a high ( 79.94 ) F1 - score , and that the score does not drop so much even for requests unseen in the training data . 
We present encoderdecoder models for the second subtask , which rewrite an evidence sentence into a recommendation sentence . 
The experiments demonstrate that an LSTM ( Luong et al . 
, 2015 ) based model achieves the BLEU score ( Papineni et al . 
, 2002 ) of 56.09 with a gold evidence sentence given and that of 45.38 without a gold sentence ( only a re2https://github.com/megagonlabs/ebe-datasetview and a request is given ) . 
We also report experiments when the two subtasks are combined to generate a recommendation sentence for a given review . 
The contributions of this paper are as follows : 1.We propose a novel and practical task to explain evidences given vague requests expressed freely in natural language . 
2.We create a new dataset by annotating review sentences with evidences and rewriting each evidence into a recommendation sentence . 
This is the largest dataset for explaining evidences in recommending hotels for vague requests . 
3.Experiments show that our dataset enables to train models that can effectively Ô¨Ånd evidences to various vague requests and generate recommendation sentences . 
2 Dataset Creation In this section , we describe the procedure to create the Evidence - based Explanation dataset . 
The dataset is expected to include ( i ) vague requests from users , ( ii ) items ( in this study , hotel candidates ) , ( iii ) evidence where an item satisÔ¨Åes an request , ( iv ) and a recommendation sentence based on each evidence . 
As a corpus that meets these requirements , we use review data on Jalan3 , which is a major hotel booking service in Japan . 
On jalan , users can enter reviews after their stay at the hotel . 
In addition to review texts , Jalan accepts ratings for some speciÔ¨Åc aspects ( e.g. , ‚Äò Service ‚Äô and ‚Äò Cleanliness ‚Äô ) , similarly to other booking services ( Wang et al . 
, 2010 ) . 
Although some aspects are similar to vague requests(e.g . 
, ‚Äú good service ‚Äù or ‚Äú cheap hotel ‚Äù ) , the number of such pre3https://www.jalan.net/891 CategoryExamples of requests # of collection # of annotations With ‚Äú inn ‚Äù or ‚Äú hotel ‚Äù Additional titles ‚Äú inn ‚Äù Additional ( Types ) Clean Clean hotel Clean 15k 71k 3.6k ( 0.8k ) Relax Relaxing inn Grate place to relax 8k 80k 3.6k ( 1.1k ) Service Helpful hotel staff were very helpful 10k 143k 3.4k ( 2.0k ) Useful Useful inn Useful for sightseeing 4k 113k 2.7k ( 1.1k ) Child friendly Child friendly hotel Child friendly 3k 81k 2.6k ( 1.3k ) Good view Good view hotel Good view 1k 34k 2.3k ( 0.7k ) Delicious Hotel with delicious food Delicious dinner 1k 145k 2.3k ( 1.0k ) Cost Good low cost hotel Low cost but very good 5k 89k 2.2k ( 1.3k ) Good Perfect hotel Perfect 33k 278k 2.6k ( 1.4k ) Others Historic hotel Historic atmosphere 19k 297k 11.8k ( 5.2k ) Total ‚Äî ‚Äî 99k 1.3 M 37.3k ( 15.8k ) Table 1 : Examples of collected vague requests and the number of collections , uses , and types deÔ¨Åned aspects is very limited and can not cover diverse requests , such as ‚Äú dog - friendly hotel . 
‚Äù Consequently , we created a new dataset using review titles and review texts . 
In the review texts , users describe their impressions on the service of the hotel based on their real experiences . 
Additionally , the review titles often summarize the most salient point of the experiences and often include similar expressions to vague requests such as ‚Äú dog - friendly hotel . 
‚Äù Hence , assuming that some review titles express vague requests and that the corresponding review texts contain evidence , we extracted vague requests from review titles and annotated evidence sentences for requests in review texts . 
Finally , we rewrote the evidence sentences into recommendation sentences . 
Figure 1illustrates the overall pipeline to construct the dataset . 
It consists of three steps . 
1.Collect vague requests from review titles : Use rules to Ô¨Ånd review titles that correspond to vague requests . 
2.Identify evidence : Ask crowdworkers to identify whether each review sentence contains evidence for the request corresponding to the review title . 
3.Explain evidence : Ask crowdworkers to write recommendation sentences based on the evidence sentences . 
2.1 Collecting Vague Requests Based on the fact that some titles have similar expressions to vague requests , we collected vague requests by selecting review titles . 
Some review titles are inappropriate as requests , for example , ‚Äú Thanks ‚Äù or ‚Äú Stayed for the Ô¨Årst time . 
‚Äù Therefore , to comprehensively collect vague requests for hotels with less noise , we Ô¨Årst extracted review titlesthat included words representing accommodations such as ‚Äú inn ‚Äù or ‚Äú hotel . 
‚Äù In addition , we applied Ô¨Åltering rules to remove other unuseful titles4 . 
Considering the possibility of data imbalance , we performed a categorical analysis . 
First , we applied morphological analysis of the collected requests using SudachiPy ( Takaoka et al . 
, 2018 ) to normalize surface variations in the requests . 
We manually checked and categorized all Ô¨Åltered titles appearing more than twenty times in the corpus , which resulted in ten categories of vague requests . 
The distribution of categories in the dataset was skewed ; the numbers of instances for some categories were small . 
For example , ‚Äú Good hotel ‚Äù is common but not ‚Äú Hotel with delicious food . 
‚Äù This is because a small percentage of requests appear with the expression ‚Äú inn ‚Äù or ‚Äú hotel . 
‚Äù Titles such as ‚Äú Delicious dinner ‚Äù are more frequent than ‚Äú Hotel with delicious food . 
‚Äù Therefore , we extracted additional titles that contained the same content words as the extracted titles , excluding the accommodation expressions such as ‚Äú inn ‚Äù or ‚Äú hotel . 
‚Äù For example , ‚Äú Hotel with delicious food ‚Äù ‚Üí‚Äúdelicious ‚Äù ( excluding hotel and extracting a content word ) ‚Üí‚ÄúDelicious dinner ‚Äù ( additional titles ) . 
Table 1shows examples of vague requests collected from review titles . 
We extracted about 1.4 million reviews ( 99k + 1.3 M ) that have the collected requests in titles ( # of collection ) . 
For annotation in the next subsection , we selected 37,280 reviews ( # of annotations ) . 
By expanding the collection rules , the number of requests increased greatly , and the data imbalance problem reduced . 
Furthermore , it also increased the variation of the request expressions . 
Overall , we collected 15,767 unique kinds of titles in 37,280 reviews . 
4The rules include , for example , titles must not contain proper nouns and must contain one or more content words.892 Clean Relax Service Useful Child View Delicious Cost Good Others All Ratio of Relevant [ % ] 82.1 69.7 85.3 83.8 71.9 82.6 91.6 69.6 72.9 68.6 75.6 Ratio of Evidence [ % ] 48.3 55.4 71.4 74.1 58.8 60.1 68.6 44.3 56.0 46.1 55.3 Table 2 : Ratio of relevant and evidence sentences included in the review text for a request Amount of evidence sentences # of reviews No evidence 16,654 ( 44.7 % ) 1 evidence sentence 16,456 ( 44.1 % ) 2 evidence sentences 3,382 ( 9.1 % ) ‚â•3 evidence sentences 788 ( 2.1 % ) Table 3 : Amount of evidence sentences in each review 2.2 Evidence IdentiÔ¨Åcation Dataset We used Yahoo Crowdsourcing5to annotate review data with evidence for requests . 
Workers were shown a review title and a single sentence of the review text . 
Then they were asked , ‚Äú Is the following sentence relevant to the title , and does it contain evidence for the title ? ‚Äù There were three options for the answer : Evidence , Relevant ( not as Evidence ) , and Irrelevant . 
Relevant ( not as Evidence ) means that the sentence contains the same expression as the request or its synonymous expression , but it does not present an evidence to support the request ( title ) . 
Although the evidence may make sense by combining two or more sentences , we annotated each sentence of the review independently to simplify the annotation work . 
We annotated 37,280 reviews in total ( ‚Äú # of annotations ‚Äù in Table 1 ) . 
For a higher quality , each task was annotated by Ô¨Åve people . 
We also prepared check questions for each task . 
Table 2reports the ratios of the Evidence and Relevant instances by category . 
In the ‚Äò Useful ‚Äô category , 74 % of the reviews contained evidence in the text , while only 44 % of the reviews in the ‚Äò Cost ‚Äô category did . 
This is because users apt to explain the reason for an ‚Äò useful ‚Äô hotel in a review , but because the necessity of explaining the reason for ‚Äò cheap ‚Äô hotel is relatively low . 
Table 3shows the number of evidence sentences for each review request . 
Approximately half of the reviews contained evidence . 
Requests that have a lot of evidence per review were an unique feature of this dataset . 
For example , requests that express 5It is a microtask crowdsourcing service in Japan . 
We mixed some check questions in the tasks and receive annotated data from only workers who answered the check questions correctly . 
We did not set gender or attribute limits of workers in all our tasks . 
https://crowdsourcing.yahoo.co.jp/general goodness such as ‚Äú good hotel ‚Äù have lots of evidence . 
In this case , the task of labeling evidence sentences was similar to annotation efforts for sentiment analysis . 
2.3 Evidence Explanation Dataset Using crowdsourcing , we rewrote evidence sentences into recommendation sentences . 
First , we showed workers a review title and an evidence sentence . 
Then we asked them to write a recommendation sentence so that the sentence can be used to explain the evidence in recommending the hotel to a user . 
We annotated 25,804 sentences that at least three of the Ô¨Åve workers judged to contain evidence in Section 2.2 . 
We asked workers to report the following two cases . 
( 1 ) The request is a negative expression such as ‚Äú bad view . 
‚Äù ( 2 ) There is no evidence in a given sentence6 . 
To ensure the quality of the annotation , each sentence was annotated by Ô¨Åve workers , and we prepared check questions for each task . 
In the check questions , we prepared negative expressions for requests , and conÔ¨Årmed that the workers followed the instructions properly . 
Table 4shows the number of the exact matches of Ô¨Åve workers for the created recommendation sentence . 
When only extracting a phrase from a review is sufÔ¨Åcient as a recommendation sentence , the Ô¨Åve workers tended to produce an identical result . 
On the other hand , when a certain part in a review had to be rewritten , recommendation sentences from the Ô¨Åve workers tended to differ . 
3 Experiments Using the annotated dataset , we conducted two experiments . 
( 1 ) Evidence IdentiÔ¨Åcation and ( 2 ) Evidence Explanation . 
The former predicts whether a sentence contains evidence for a request , whereas the latter generates a recommendation sentence . 
6We targeted sentences where at least three people judged to contain evidence . 
However , it was sometimes difÔ¨Åcult to write recommendation sentences when two out of Ô¨Åve workers judged that the sentence has no evidence.893 # of sameanswers # of sent . 
Examples Title Evidence sentence Recommendation sentence ‚â•2 matches 13,100 PetfriendlyThis hotel is tolerant of dog lovers because you can sleep in a bed with your dog.(We recommend this ) because you can sleep in a bed with your dog . 
All different 9,889 Nice openair bathThe temperature of the bath was just right , and we spent a long time in the open - air bath watching the stars.(We recommend this ) because you can take a long open - air bath while gazing at the stars . 
Negative req . 
( ‚â•3)1,651 The scenery ... We booked a Bay Bridge view , but it was only visible from the edge of the window. î No Evidence 1,164 Mountain side viewIt was an ocean view hotel but we stayed on the mountain side. î Table 4 : Examples of recommendation sentences rewritten by workers and matching rate of rewriting Reviews Sentences Positive ( % ) Train 29,826 148,671 20,709 ( 13.9 ) Dev 3,726 18,549 2,606 ( 14.0 ) Test 3,728 18,823 2,489 ( 13.2 ) Total 37,280 186,043 25,804 ( 13.9 ) Table 5 : Evaluation data for evidence identiÔ¨Åcation 3.1 Evidence IdentiÔ¨Åcation Task Task Description The task is to predict whether or not a sentence contains an evidence for a request . 
This is a binary classiÔ¨Åcation problem . 
A positive example is a sentence to which at least three out of the Ô¨Åve workers labeled evidence . 
All other sentences are treated as negative examples . 
We randomly divided the data by review into training , development , and test set ( see Table 5 ) . 
We used the same data split in all experiments . 
Experimental Settings We explored logistic regression7and BERT ( Devlin et al . 
, 2019 ) as classiÔ¨Åcation models . 
For the tokenization , we used juman++8(Tolmachev et al . 
, 2018 ) and Byte pair encoding ( BPE ) ( Sennrich et al . 
, 2016 ) with the vocabulary size of 8k . 
We pre - trained word2vec ( Mikolov et al . 
, 2013 ) CBOW model , and the BERT model on two million review sentences in Jalan . 
For the logistic regression , we calculated the TF - IDF9(Jones , 1972 ) vector and the average vector of word2vec for requests and sentences respectively . 
We used the request vector , the sentence vector , and the difference between the two vectors as features . 
The input to the BERT model was in the following order : request sentence , [ SEP ] , and evidence sentence . 
Hyperparameters of each model were tuned by the F1score on the development set . 
7Implemented in : https://scikit-learn.org 8https://github.com/ku-nlp/jumanpp 9We used the word frequency in the sentence as TF , and the word frequency of the review text as DF.Results and Analysis Table 6reports F1 - score of both models for each category and all categories . 
The F1 - score of BERT for all the data was 79.94 , which is 33.15 points higher than the logistic regression . 
Results in each category show that BERT had the highest F1 - score for ‚Äò Useful ‚Äô and the lowest for ‚Äò Good ‚Äô . 
We analyze the results of the evidence identiÔ¨Åcation by the BERT model from different perspectives in the following paragraphs . 
Evidence IdentiÔ¨Åcation without Requests The F1 - score of the BERT model was relatively high , considering the nature of this task , i.e. , associating evidences to requests . 
However , we need to make sure whether the BERT model considers a request when identifying an evidence . 
Thus , we trained another BERT model without a request ( only a sentence is given ) as an input . 
The model trained without a request resulted in the F1 - score of 43.22 , which is 37 points lower than that with a request . 
This huge gap indicates that evidences in our dataset depend on requests and that the BERT model pays attention to requests properly . 
For example , a model trained with a request predicts that the sentence , ‚Äú It was pleasant in the room with a view of the sea ‚Äù is evidence for a request ‚Äú good view ‚Äù but not for ‚Äú good food ‚Äù . 
In contrast , a model trained without a request predicts that the both are evidence sentences . 
Unseen Requests Since the dataset contains a wide range of requests , 30 % of the requests in the test set are unseen , not appearing in the training set . 
Thus , we divided the test set in terms whether a request is unseen or not , and computed the F1score in Table 7 . 
Although the F1 - score for unseen requests drops by 6.44 points , it is still high compared to the score trained without a request ( described in the previous paragraph ) . 
This indicates that the model makes a successful prediction for894 Model Clean Relax Service Useful Child View Delicious Cost Good Others All Logistic regression 44.39 46.03 51.21 61.30 49.39 61.02 54.34 30.24 34.76 37.15 46.79 BERT 79.52 82.89 84.98 89.48 85.04 81.23 82.54 73.59 68.85 73.89 79.94 Table 6 : F1 - score for evidence identiÔ¨Åcation for each category Figure 2 : Characteristics of evidence sentencesQuadrant F1 A 87.11 B 82.64 C 79.01 D 75.87 A+B 83.12 C+D 76.30 A+C 82.82 B+D 79.54 All 79.94 Table 8 : F1 - score for each quadrant F1 # of instances Unseen requests 75.40 5,857 Seen requests 81.84 12,966 Table 7 : F1 - score for unseen / seen requests majority of the unknown requests . 
Examining successful predictions for unseen requests , we found that the same expression to the request often appears in the evidence sentence , For example , in response to a request for ‚Äú a good location to watch a football game , ‚Äù the evidence sentence includes , ‚Äú It ‚Äôs located in front of Tosu Station in Saga , and it ‚Äôs a good location to watch the Tosu football game . 
‚Äù The expression in italic is considered to be a clue for predicting the evidence label for the sentence . 
The analysis of whether the request is included in the evidence sentence is discussed in detail in the next paragraph . 
In contrast , we observed difÔ¨Åcult instances as well . 
For example , the request ( review title ) is , ‚Äú You can fully enjoy an extraordinary experience , ‚Äù and the evidence sentence is , ‚Äú I was refreshed by soaking in a hot spring while listening to the chirping of birds and the sound of insects . 
‚Äù The BERT model could not infer that the experience ( hot spring , chirping birds ) is extraordinary and that the sentence is an evidence for the request . 
Characteristics of Evidence Sentences There are various ways to express an evidence sentence , for example , with and without a use of conjunctions . 
Figure 2illustrates four categories ( decomposed into two axes ) of how a sentence presents an evidence for a request . 
The y - axis is whether a request expression appears in an evidence sentence . 
The x - axis is whether there is an explicit conjunction ( e.g. , ‚Äò because ‚Äô ) expressing the discourse relation between a request and evidence . 
We have automatically divided these categories by rules . 
The top - left quadrant A includes a request expression and an explicit conjunction in the sentence . 
Although 60 % contain evidence for a request , quadrant A has the smallest volume . 
On the other hand , the lower - right quadrant D has the largest volume , but has the smallest ratio of including evidence for the request ( only 7 % ) . 
The evidence for quadrant A can be collected by a simple rule , but it is comprised of only about 6 % of the total evidence . 
Our dataset successfully extracts other evidence expressions using the relationship between the review title and the text . 
Table 8shows the F1 - score of the BERT model for each quadrant . 
The F1 - score of quadrant A , which contains an explicit conjunction and request words , was highest ( 87.11 ) . 
It was 7.17 points higher than the average F1 - score of all test data . 
On the other hand , the F1 - score of quadrant D , which does not contain an explicit conjunction nor any request words , was lowest ( 75.87 ) . 
It was 4.07 points lower than the average F1 - score of all test data . 
In addition , the F1 - score of quadrant A+B was 6.82 points higher than the F1 - score of quadrant C+D , indicating that the presence of the request expression in the evidence sentence significantly impacts on the performance of predicting evidence . 
We examined successful cases in quadrant D , which is the most difÔ¨Åcult of all . 
In these cases , we found that expressions similar to the requests often appear in the evidence sentence . 
For exam-895 ple , in response to the request ‚Äú I am soothed by a meal , ‚Äù the evidence sentence is ‚Äú I was impressed by the deliciousness of the freshly made egg rolls forbreakfast . 
‚Äù In the example , the word ‚Äò meal ‚Äô in the request is related to the word ‚Äò breakfast . 
‚Äô However , the model could not recognize that the sentence , ‚Äú We have a foot washing place next to the entrance , gum roller and wet tissue , it was very thorough , ‚Äù contains an evidence for the request , ‚Äú An inn where I can stay with my pet dog . 
‚Äù This may be due to the lack of similar expressions for the request in the sentence , and the failure to associate dog and dog amenities . 
3.2 Evidence Explanation Task Task Description The task generates a recommendation sentence given request and evidence sentences . 
We used only the data that three or more workers rewrote into recommendation sentences in Section 2.3 . 
Each evidence sentence had multiple recommendation sentences rewritten by the workers , and we use all of them as training data . 
We use BLEU ( Papineni et al . 
, 2002 ) to evaluate generated sentences . 
Experiment Settings We compared three models : a rule - based model and two neural network models . 
The rule - based model rewrites an evidence sentence into a recommendation sentence by focusing on the root node in the parse tree of the evidence sentence . 
The rules include : if the root node is a verb , adjective , or auxiliary verb , add ‚Äú because ‚Äù at the beginning ; if the root node is a noun , add ‚Äú because of ‚Äù at the beginning ; and if the root node in an adverb , add ‚Äú because you can do ‚Äù at the beginning . 
For neural network models , we employed an LSTM model with attention ( Luong et al . 
, 2015 ) and a Transformer model ( Vaswani et al . 
, 2017 ) , assuming that the task is translation from an evidence sentence into a recommendation sentence . 
We used the FAIRSEQ ( Ott et al . 
, 2019 ) to implement the models . 
We tokenized it using Juman++ and BPE . 
The input to the model was in the following order : request sentence , [ SEP ] , and evidence sentence . 
Hyper - parameters of the models were tuned by the BLEU score on the development set . 
For the evaluation , we used the BLEU score on sentences tokenized by Juman++ ( not by BPE ) . 
Since the number of references for each evidence sentence was not constant , we randomly selected one . 
Method BLEU No - rewrite 47.17 Rule - based 50.26 LSTM 56.09 Transformer 55.79 Table 9 : BLEU score to generate recommendation given evidence and a request Method BLEU F1 Pipeline ( BERT ‚ÜíLSTM ) 45.38 63.30 End - to - end ( LSTM ) 16.27 49.13 Table 10 : BLEU score to generate recommendation given review text and a request Results and Analysis Table 9shows BLEU scores of generated recommendation sentences . 
‚Äò No - rewrite ‚Äô is the baseline where the evidence sentence is treated as the recommendation sentence without a rewrite . 
Compared with this baseline ( 47.17 BLEU ) , all generation methods obtained higher BLEU scores . 
The score of the LSTM - based model ( 56.09 ) was 0.30 points higher than that of the Transformer - based model ( 55.79 ) . 
However , the BLEU score of the rulebased model was only 5.83 point lower than the LSTM - based model . 
This implies that this task requires fewer rewrites than we expected . 
There are some differences between the outputs of the rule - based model and the LSTM - based model . 
The rule - based model tends to produce longer sentences because it can not generate a sentence from scratch . 
In addition , the rule - based model fails when an evidence sentence includes unnecessary information , for example , ‚Äú it ‚Äôs close to the station and it ‚Äôs convenient , so we ‚Äôd like to use it again . 
‚Äù The LSTM - based model could successfully generate ‚Äú ( We recommend this hotel ) Because it ‚Äôs also close to the station and it ‚Äôs convenient , ‚Äù although the rule - base model kept ‚Äú so we ‚Äôd like to use it again ‚Äù and generated , ‚Äú Because it ‚Äôs close to the station and it ‚Äôs convenient , so we ‚Äôd like to use it again . 
‚Äù 3.3 End - to - end Experiment In this section , we present an experiment to generate a recommendation sentence given review data ( a request and review sentences ) as an input . 
Combining the subtasks 1 and 2 , this end - to - end experiment converts a hotel review into a recommendation sentence . 
When a given review does not have evidence , a system is expected to yield a special896 Review title : The view was great ! Review text : On the Ô¨Årst day , I purchased a premium sightseeing ticket before check - in . 
Then I went to the Higashiyama Zoo to meet a handsome gorilla , Shabani . 
On the next day , I went to the Maglev and Railway Museum , and enjoyed it . 
In the room , where Nagoya Castle can be seen beautifully , the children were delighted . 
... Reference : ( I recommend this hotel ) Because the room has a beautiful view of Nagoya Castle Pipeline : Because the children were overjoyed because they had a beautiful view of Nagoya Castle . 
End - to - end : Because you have a beautiful view of Nagoya Castle from your room . 
Review title : Happy with my doggy Review text : The couple , father and two dogs used it . 
... The rooms were spacious , and the large windows overlooked the sea . 
I was very satisÔ¨Åed . 
... While many pet - friendly hotels are disappointing , this one is recommended . 
... Reference : While many pet - friendly hotels are disappointing , this one is recommended Pipeline : Because pet - friendly hotels are very convenient End - to - end : Because the rooms were spacious the large windows overlooked the sea , and I was very satisÔ¨Åed Review title : A hotel recommended for families with children Review text : Because I have a 16 - month old child , I was drawn to the plan that included room service and a private hot spring ... The mattress was thin because of its age , so it would have been better if it were thicker . 
The hot springs and customer service were good , and it was good that the staff treated my children kindly . 
Reference : Because the hot springs and customer service were good , and it was good for children . 
Pipeline : Because the hot springs and customer service were good , and it was good for children . 
End - to - end : Because the pool and customer service were good , and it was good for children . 
Table 11 : Examples of generating recommendation sentences given the review data token [ no - evidence ] . 
We explored two approaches , pipeline and endto - end . 
The pipeline method is simply a combination of the models from Sections 3.1and3.2 . 
The method Ô¨Årst predicts whether a sentence in a review present an evidence for a request by using the BERT model . 
It then generates a recommendation sentence by using the LSTM - based model for the request and the predicted evidence sentence with the highest score assigned by the BERT model only when the review includes evidence sentences . 
If the BERT model predicts no sentence in the review as evidence , the method generates [ no - evidence ] . 
The end - to - end method is an encoder - decoder LSTM model that directly generates a recommendation sentence given a review title and text . 
An input to the model is request and [ SEP ] , followed by multiple sentences of the review . 
When a review did not contain an evidence for the request , the model is trained to generate [ no - evidence ] . 
Table 10shows the BLEU scores and the macro - average F1 - scores of the methods . 
The macro - average F1 - score is deÔ¨Åned similarly to the evaluation conducted by Rajpurkar et al . 
( 2016 ) 10 . 
The pipeline method outperformed the end - to - end method , achiving a BLEU score of 45.38 , 29.11 points higher than the end2end model . 
This is probably because the pipeline model could utilize 10The metric measures matches of bag - of - tokens in the reference and generated sentences . 
For reviews without an evidence , we regard that the system output is correct if the generated output is no-evidence.the pre - trained BERT model and because training the end - to - end method was difÔ¨Åcult with very long sequences of tokens given as inputs . 
In addition , the end - to - end method tends to output too many [ no - evidence ] and the total number of output words is low , so the BLEU score is also low due to brevity penalty . 
Table 11presents examples of the generated sentences . 
In the Ô¨Årst example , the both models successfully generated appropriate recommendation sentences . 
Although the end - to - end method generated the natural sentence in the second example , the recommendation is nothing to do with the request , ‚Äú happy with my doggy . 
‚Äù In the third example , the end - to - end method generated the word ‚Äú pool ‚Äù , which was actually false because the the review text only refers to ‚Äú hot spring . 
‚Äù We observed these incorrect generations from the end - toend method more than from the pipeline method . 
4 Related Work Several studies addressed explainable recommendations ( Sarwar et al . 
, 2001 ; Diao et al . 
, 2014 ; Zhao et al . 
, 2014 ; Zhang et al . 
, 2014 ; Wang et al . 
, 2018 ; Zhang et al . 
, 2020 ; Zhao et al . 
, 2019 ) . 
In feature - based explanations , Zhang et al . 
( 2014 ) generated textual sentences as explanations using templates such as ‚Äú You might be interested in [ feature ] , on which this product performs well . 
‚Äù In aspect - based explanations , Wang et al . 
( 2010 ) discovered latent ratings on each aspect , and selected sentences related to each aspect to help users better understand the opinions given a set of review897 texts with the overall ratings . 
Zhao et al . 
( 2019 ) formulated a problem called personalized reason generation and generated a recommendation sentence given a song name , author , and user tag as input . 
The inputs of those studies were user vectors created from the user ‚Äôs action history or limited aspects . 
However , our study deals with a wide range of natural language requests for a dialog system in the hotel booking domain . 
In the Ô¨Åeld of sentiment analysis , research that extracts evidence based on sentiment expressions has attracted attention ( Chen et al . 
, 2010 ; Gui et al . 
, 2016 ; Kim and Klinger , 2018 ) . 
Chen et al . 
( 2010 ) extracted the cause of a target emotional expression based on a rule . 
Gui et al . 
( 2016 ) annotated an emotional expression and its cause . 
These studies aimed to gather useful information to extract emotional expressions and provide evidence simultaneously by examining the reputations for speciÔ¨Åc products . 
Although our study also aims to collect useful information , the requests are not limited to emotional expressions . 
In addition , we generate recommendation sentences . 
Our study can be viewed as a special application of argument mining in the domain of hotel review . 
Liu et al . 
( 2017 ) used manually annotated arguments of evidence - conclusion discourse relations in 110 hotel reviews . 
The study showed the effectiveness of several combinations of argumentbased features . 
In Japanese , Murakami et al . 
( 2009 ) proposed a method to collect consents and dissents for queries that can be answered with Yes or No . 
As part of that , they extracted evidence using rules . 
Our dataset is useful as training data to extract evidence in argument mining . 
5 Conclusion We proposed a novel task of predicting an evidence to satisfy a request and generating a recommendation sentence . 
We built an Evidence - based Explanation dataset for the task . 
The experimental results demonstrated that the BERT model could Ô¨Ånd evidence sentences with respect to various vague requests and that the LSTM - based model could generate recommendation sentences . 
Future directions of this study include choosing the best evidence sentence from multiple candidate sentences for a vague request from a user and developing a concierge service that can recommend a hotel with evidence . 
Acknowledgments We would like to thank the anonymous reviewers for their valuable feedback . 
Abstract In this paper , we propose an effective deep learningframeworkformultilingualandcodemixed visual question answering . 
The proposed model is capable of predicting answers from the questions in Hindi , English or Codemixed ( Hinglish : Hindi - English ) languages . 
ThemajorityoftheexistingtechniquesonVisualQuestionAnswering(VQA)focusonEnglishquestionsonly . 
However , manyapplicationssuchasmedicalimaging , tourism , visual assistants require a multilinguality - enabled module for their widespread usages . 
As there is no available dataset in English - Hindi VQA , we firstly create Hindi and Code - mixed VQA datasetsbyexploitingthelinguisticproperties oftheselanguages . 
Weproposearobusttechniquecapableofhandlingthemultilingualand code - mixed question to provide the answer against the visual information ( image ) . 
To betterencodethemultilingualandcode - mixed questions , we introduce a hierarchy of shared layers . 
We control the behaviour of these shared layers by an attention - based soft layer sharing mechanism , which learns how shared layersareappliedindifferentwaysforthedifferent languages of the question . 
Further , our model uses bi - linear attention with a residual connectiontofusethelanguageandimagefeatures . 
We perform extensive evaluation and ablation studies for English , Hindi and Codemixed VQA . 
The evaluation shows that the proposedmultilingualmodelachievesstate - ofthe - artperformanceinallthesesettings . 
1 Introduction VisualQuestionAnswering(VQA)isachallengingproblemthatrequirescomplexreasoningover visualelementstoprovideanaccurateanswertoa naturallanguagequestion . 
AnefficientVQAsystemcanbeusedtobuildanArtificialIntelligence ( AI)agentwhichtakesanaturallanguagequestion ‚àóWorkcarriedoutduringtheinternshipatIITPatnaandpredictsthedecisionbyanalyzingthecomplex scene(s ) . 
VQA requires language understanding , fine - grainedvisualprocessingandmultiplesteps of reasoning to produce the correct answer . 
As theexistingresearchonVQAaremainlyfocused on natural language questions written in English ( Antol et al . 
, 2015;Hu et al . 
,2017;Fukui et al . 
, 2016;Andersonetal . 
, 2018;Lietal . 
,2018;Xuand Saenko,2016;Shihetal . 
,2016),theirapplications areoftenlimited . 
QE : What color are the trees ? QH : ‡§™‡•á‡§°‡§º ‡§ï‡•á ‡§ï‡•ç‡§Ø‡§æ ‡§∞‡§Ç‡§ó ‡§π‡•à‡§Ç ? ( Trans : What color are the trees ? ) QCM : Trees ke kya color hain ? ( Trans : What color are the trees ? ) Answer(English ): Green Answer(Hindi ): ‡§π‡§∞‡§æ QE : Where is this picture ? QH : ‡§Ø‡§π ‡§§‡§∏‡•ç‡§µ‡•Ä‡§∞ ‡§ï‡§π‡§æ‡§Ç ‡§ï‡•Ä ‡§π‡•à ? ( Trans : Where is this picture ? ) QCM : Yahpicture kahan ki hai ? ( Trans : Where is this picture ? ) Answer : Market Answer(Hindi ): ‡§¨‡§æ‡§ú‡§æ‡§∞ Figure 1 : Examples of questions ( English , Hindi and Code - mixed ) with their corresponding images and answers Multilingual speakers often switch back and forthbetweentheirnativeandforeign(popular)languagestoexpressthemselves . 
Thisphenomenon ofembeddingthemorphemes , words , phrases , etc . 
, ofonelanguageintoanotherispopularlyknownas code - mixing ( Myers - Scotton , 1997,2002 ) . 
Codemixing phenomena is common in chats , conversations , and messages posted over social media , especiallyinbilingual / multilingualcountrieslike India , China , Singapore , andmostoftheotherEuropeancountries . 
Sectorsliketourism , food , education , marketing , etc . 
haverecentlystartedusing code - mixed languages in their advertisements to attract their consumer base . 
In order to build an AIagent whichcan serve multilingual end users,900 aVQAsystemshouldbeputinplacethatwould belanguageagnosticandtailoredtodealwiththe code - mixed and multilingual environment . 
It is worthstudyingtheVQAsysteminthesesettings whichwouldbeimmenselyusefultoaverylarge numberofpopulationwhospeak / writeinmorethan onelanguage . 
Arecentstudy ( Parshadetal . 
, 2016 ) alsoshowsthepopularityofcode - mixedEnglishHindilanguageandthedynamicsoflanguageshift inIndia . 
Ourcurrentworkfocusesondevelopinga languageagnosticVQAsystemforHindi , English andcode - mixedEnglish - Hindilanguages . 
LetusconsidertheexamplesshowninFig 1 . 
The majorityoftheVQAmodels ( Andersonetal . 
, 2018 ; Lietal . 
,2018;Yuetal . 
,2018)arecapableenough to provide correct answers for English questions QE , butourevaluationshowsthatthesamemodel could not predict correct answers for Hindi QH andCode - mixedquestion QCM . 
Thequestions QH andQCMcorrespondtothesamequestion QE , but areformulatedintwodifferentlanguages . 
Inthis paper , weinvestigatetheissueofmultilingualand code - mixedVQA.Weassumethatthereareseveral techniquesavailableformonolingual(especially , English)VQAsuchthatastrongVQAmodelcan bebuilt . 
However , weareinterestedinbuildinga systemthatcananswerthequestionsfromdifferent languages(multilingual)andthelanguageformed bymixingupofmultiplelanguages(code - mixed ) . 
Weshowthatinacross - lingualscenarioduetolanguagemismatch , applyingdirectlyalearnedsystem fromonelanguagetoanotherlanguageresultsin poorperformance . 
Thus , weproposeatechnique for multilingual and code - mixed VQA . 
Our proposedmethodmainlyconsistsofthreecomponents . 
The first component is the multilingual question encodingwhichtransformsagivenquestiontoits feature representation . 
This component handles themultilingualityandcode - mixinginquestions . 
Weusemultilingualembeddingcoupledwithahierarchy of shared layers to encode the questions . 
Todoso , weemployanattentionmechanismon thesharedlayerstolearnlanguagespecificquestion representation . 
Furthermore , we utilize the self - attentiontoobtainanimprovedquestionrepresentationbyconsideringtheotherwordsinthe question . 
Thesecondcomponent ( imagefeatures ) obtainstheeffectiveimagerepresentationfromobject level and pixel level features . 
The last componentis multimodalfusion whichisaccountable toencodethequestion - imagepairrepresentationbyensuingthatthelearnedrepresentationistightly coupledwithboththequestion(language)andimage(vision)feature . 
ItistobenotedthatdesigningaVQAsystemfor eachlanguageseparatelyiscomputationallyvery expensive ( both time and cost ) , especially when multiple languages are involved . 
Hence , an endto - end model that integrates multilinguality and code - mixinginitscomponentsisextremelyuseful . 
Wesummarizeourcontributionasfollows : 1.We create linguistically - driven Hindi and English - Hindicode - mixedVQAdatasets . 
To thebestofourknowledge , thisistheveryfirst attempttowardsthisdirection . 
2.Weproposeaunifiedneuralmodelformultilingualandcode - mixedVQA , whichcanpredictanswerofamultilingualorcode - mixed question . 
3.Toeffectivelyansweraquestion , weenhance thevisionunderstandingbycombininglocal image grid and object - level visual features . 
We propose a simple , yet powerful mechanismbasedonsoft - sharingofsharedlayersto betterencodethemultilingualandcode - mixed questions . 
ThisbridgesthegapbetweenVQA andmultilinguality . 
4.Weperformextensiveevaluationandablation studies for English , Hindi and Code - mixed VQA.Theevaluationshowsthatourproposed multilingual model achieves state - of - the - art performanceinallthesesettings . 
2 Related Work Multilingual and Code - Mixing : Recently , researchers have started investigating methods for creating tools and resources for various Natural Language Processing ( NLP ) applications involvingmultilingual ( GarciaandGamallo , 2015;Gupta et al . 
,2019;Agerri et al . 
, 2014 ) and code - mixed languages ( Gupta et al . 
, 2018a;Bali et al . 
,2014 ; Guptaetal . 
, 2016;Rudraetal . 
, 2016;Guptaetal . 
, 2014 ) . 
DevelopingaVQAsysteminacode - mixed scenariois , itself , verynovelinthesensethatthere hasnotbeenanypriorresearchtowardsthisdirection . 
VQA Datasets : Quite a few VQA datasets ( Gaoetal . 
,2015;Antoletal . 
, 2015;Goyaletal . 
, 2017;Johnson et al . 
, 2017;Shimizu et al . 
, 2018 ; Hasanetal . 
, 2018;Wangetal . 
, 2018)havebeen createdtoencouragemulti - disciplinaryresearchinvolvingNaturalLanguageProcessing(NLP)and901 Computer Vision . 
In majority of these datasets , the images are taken from the large - scale image databaseMSCOCO ( Linetal . 
,2014)orartificially constructed ( Antoletal . 
, 2015;Andreasetal . 
, 2016 ; Johnsonetal . 
, 2017 ) . 
Thereareafewdatasets ( Gao etal . 
,2015;Shimizuetal . 
, 2018)formultilingual VQA , but these are limited only to some chosen languages , andunlikeourdatasettheydonotoffer anycode - mixedchallenges . 
VQA Models : The popular frameworks for VQAintheliteraturearebuilttolearnthejointrepresentationofimageandquestionusingtheattentionmechanism ( Kimetal . 
,2018;Luetal . 
,2016 ; Yuetal . 
,2017;KafleandKanan , 2017;Zhaoetal . 
, 2017).Hu et al.(2018 ) proposed a technique to separatelylearntheanswerembeddingwithbest parameterssuchthatthecorrectanswerhashigher likelihoodamongallpossibleanswers . 
Thereare someworks ( Chaoetal . 
,2018;Liuetal . 
,2018;Wu etal . 
,2018)whichexploittheadversariallearning strategyinVQA.VQAhasalsobeenexploredin medicaldomains ( Zhouetal . 
, 2018;Guptaetal . 
, 2021;Abachaetal . 
, 2018;BenAbachaetal . 
, 2019 ) . 
Theselearnedrepresentationsarepassedtoamultilabelclassifierwhoselabelsarethemostfrequent answersinthedataset . 
Ouranalysis(c.f . 
Section 5.5)revealsthatthesemodelsperformverypoorly inacross - lingualsetting . 
3MCVQADataset DatasetCreation : ThepopularVQAdatasetreleasedbyAntoletal . 
( 2015)containsimages , with theircorrespondingquestions(inEnglish)andanswers ( in English ) . 
This is a challenging large scaledatasetfortheVQAtask . 
Tocreateacomparable version of this English VQA dataset in Hindi and code - mixed Hinglish , we introduce a newVQAdatasetnamed ‚Äú Multilingualand CodemixedVisualQuestionAnswering ‚Äù ( MCVQA)which comprisesofquestionsinHindiandHinglish . 
Our dataset1 , inadditiontotheoriginalEnglishquestions , also presents the questions in Hindi and Hinglishlanguages . 
Thismakesour MCVQAdataset suitable for multilingual and code - mixed VQA tasks . 
Asampleofquestion - answerpairsandimagesfromourdatasetareshowninFig 2 . 
Wedonotconstructtheanswerincode - mixed language because a recent study ( Gupta et al . 
, 2018b)hasshownthatcode - mixedsentencesand 1Thedatasetcanbefoundhere : http://www.iitp.ac . 
in/~ai - nlp - ml / resources.htmltheir corresponding English sentences share the samenouns(commonnouns , propernouns , spatiotemporal nouns ) , adjectives , etc . 
For example , givenanEnglishanditscorrespondingcode - mixed question : QE : Where is the treein thispicture ? QCM : Ispicturemetreekahan hai ? Itcanbeobservedthatboth QEandQCMsharethe same noun { picture , tree } . 
The majority of answersintheVQAv1.0datasetareoftype ‚Äò yes / no ‚Äô , ‚Äò numbers‚Äô,‚Äònouns‚Äô,‚Äòverbs‚Äôand‚Äòadjectives ‚Äô . 
Therefore , wekeepthesameanswerinbothEnglishand Code - mixedVQAdataset . 
Wefollowthetechniquessimilarto Guptaetal . 
( 2018b ) for our code - mixed question generation , which takes a Hindi sentence as input and generates the corresponding Hinglish sentence as the output . 
We translate original English questions andanswersusingtheGoogleTranslate2thathas shownremarkableperformanceintranslatingshort sentences ( Wuetal . 
,2016 ) . 
Weusethisserviceas ouroriginalquestionsandanswersinEnglishare veryshort . 
Forthecode - mixedquestiongeneration , wefirstobtainthePart - of - Speech3(PoS)and NamedEntity4(NE)tagsofeachquestion . 
Thereafter , wereplacetheHindiwordshavingthePoS tags(commonnoun , propernoun , spatio - temporal noun , adjective)withtheirbestlexicaltranslation . 
SamestrategyisalsofollowedforthewordshavingtheNEtagsas LOCATION andORGANIZATION . 
The remaining Hindi words are replaced withtheirRomantransliteration . 
Inordertoobtain thebestlexicaltranslation , wefollowtheiterative disambiguationalgorithm ( MonzandDorr , 2005 ) . 
Wegeneratethelexicaltranslationbytrainingthe StatisticalMachineTranslation(SMT)modelon thepubliclyavailableEnglish - Hindi(EN - HI)parallel corpus ( Bojar et al . 
, 2014 ) . 
Please refer to theAppendix forthecomparisonwithotherVQA datasets . 
DatasetAnalysis : TheMCVQAdatasetconsistsof 248,349trainingquestionsand 121,512validation questionsforrealimagesinHindiandCode - mixed . 
ForeachHindiquestion , wealsoprovideits 10correspondinganswersinHindi . 
Inordertoanalyze thecomplexityofthegeneratedcode - mixedquestions , wecomputetheCode - mixingIndex(CMI ) ( Gamb√§ckandDas , 2014)andComplexityFactor 2https://cloud.google.com/translate 3https://bit.ly/2rpNBJR 4https://bit.ly/2Qljan5902 3200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299NAACL¬≠HLT 2019 Submission * * * . 
ConÔ¨Ådential Review Copy . 
DO NOT DISTRIBUTE . 
Metrics Manually AnnotatedAutomatically Generated Training Validation CMI Score 0.2946 0.3223 0.3228 CF2 14.765 16.094 16.114 CF3 13.122 14.0708 14.096 Table 1 : Performance comparison between manually annotated code - mixed questions and automatically generated code - mixed questions . 
Language BLEU-1 BLEU-2 BLEU-3 ROUGE-1 ROUGE-2 ROUGE - L TER Hindi 92.18 84.19 80.22 93.14 83.16 92.20 9.63 Table 2 : Performance comparison between manually annotated Hindi questions and automatically generated Hindi questions . 
Dataset Images used Created by Multilingual Code - Mixed DAQUAR ( Malinowski and Fritz , 2014 ) NYU Depth V2In - house participants , Automatically generated7 7 FM - IQA ( Gao et al . 
, 2015 ) MSCOCO Crowd workers ( Baidu ) " 7 VQA v1.0 ( Antol et al . 
, 2015 ) MSCOCO Crowd workers ( AMT ) 7 7 Visual7W ( Zhu et al . 
, 2016 ) MSCOCO Crowd workers ( AMT ) 7 7 CLEVR ( Johnson et al . 
, 2017 ) Synthetic Shapes Automatically generated 7 7 KB - VQA ( Wang et al . 
, 2017 ) MSCOCO In - house participants 7 7 FVQA ( Wang et al . 
, 2018 ) MSCOCO In - house participants 7 7 Japanese VQA ( Shimizu et al . 
, 2018 ) MSCOCO Crowd workers ( Yahoo ) " 7 MCVQA ( Ours ) MSCOCO Automatically generated " " Table 3 : Comparison of VQA datasets with our MCVQA dataset QE : Is this a salad ? QH:‘π‡§æ ‡§Ø‡§π ‡§è‡§ï ‡§∏‡§≤‡§æ‡§¶ ‡§π ‡•à ? ( Trans : Is this a salad ? ) QCM : Kya yah ek salad hai ? ( Trans : Is this a salad ? ) Answer(E ): no Answer(H ): ‡§®‡§π“∞‡§Ç QE : What time is it ? QH:‘π‡§æ ‡§∏‡§Æ‡§Ø ‡§π ‡•Å‡§Ü ‡§π ‡•à ? ( Trans : What time is it ? ) QCM : Kya time hua hai ? ( Trans : What time is it ? ) Answer(E ): 1:08 Answer(H ): 1:08 QE : What is in the window ? QH : ‡§ø‡§ñ‡•ú‡§ï“¥ ‡§Æ”í ‘π‡§æ ‡§π ‡•à ? ? ( Trans : What is in the window ? ) QCM : Window me kya hai ? ( Trans : What is in the window ? ) Answer(E ): cat Answer(H ): —ü‡§¨’©‡•Ä QE : What sport is this ? QH : ‡§Ø‡§π ‡§ï‡•å‡§® ‡§∏‡§æ ‡§ñ ‡•á‡§≤ ‡§π‡•à ? ( Trans : What sport is this ? ) QCM : Yah kaun sa sport hai ? ( Trans : What sport is this ? ) Answer(E ): baseball Answer(H ): ‡§¨‡•á‡§∏‡§¨‡•â‡§≤ QE : Where is the tree ? QH : ‡§™‡•á‡•ú ‡§ï‡§π‡§æ ‡§Ç‡§π‡•à ? ( Trans : Where is the tree ? ) QCM : Tree kahan hai ? ( Trans : Where is the tree ? ) Answer(E ): wall Answer(H ): ‡§¶“∞‡§µ‡§æ‡§∞ where S is the number of code - switches and W is the number of words in the sentences or block of text . 
MF = W‚Ä≤ maxfwg W‚Ä≤ , if W ‚Äô > 0 MF = 0 , if W ‚Äô = 0 where W ‚Äô is the number of words in distinct languages , i.e. , the number of words except the undefined ones , max{w } is the maximum number of words belonging to the most frequent language in the sentence . 
LF = W Nwhere W is the number of words and N is the number of distinct languages in the sentence . 
A Appendices Thedetailedcomparisonofautomaticallycreated andmanuallycode - mixedquestionsw.r.ttheCodemixingIndex(CMI)score , ComplexityFactor(CF2 and CF3 ) are shown in Table 4 . 
We also show thecomparisonofourMCVQAdatasetwithother VQAdatasetsinTable 5 . 
TheanalysisofMCVQA datasetareillustratedinFig 5and6.912 Figure5 : Analysisofquestiondistributionw.r.tthequestionlengthbetweenVQAv1.0Englishandcode - mixed , trainandtestdataset . 
( a )   ( b )   ( c ) Figure 6 : Analysis of code - mixed VQA dataset on various code - mixing metrics : ( a),(b)and(c)show the distribution of code - mixed questions from training and test set w.r.t the Code - mixing Index ( CMI ) score , Complexity Factor(CF2andCF3),respectively . 
Metrics ManuallyAnnotatedAutomaticallyGenerated Training Testing CMIScore 0.2946 0.3223 0.3228 CF2 14.765 16.094 16.114 CF3 13.122 14.0708 14.096 Table4 : Comparisonbetweenmanuallyannotatedcode - mixedquestionsandautomaticallygeneratedcode - mixed questionsw.r.ttheCMIscore , CF2,andCF3 . 
Dataset Images used Created by Multilingual Code - Mixed DAQUAR ( MalinowskiandFritz , 2014)NYUDepthV2In - houseparticipants , Automaticallygenerated  FM - IQA(Gaoetal . 
,2015 ) MSCOCO Crowdworkers(Baidu ) /enc-34  VQAv1.0 ( Antoletal . 
, 2015 ) MSCOCO Crowdworkers(AMT )   Visual7W ( Zhuetal . 
,2016 ) MSCOCO Crowdworkers(AMT )   CLEVR(Johnsonetal . 
, 2017)SyntheticShapes Automaticallygenerated   KB - VQA ( Wangetal . 
, 2017 ) MSCOCO In - houseparticipants   FVQA(Wangetal . 
, 2018 ) MSCOCO In - houseparticipants   JapaneseVQA ( Shimizuetal . 
, 2018)MSCOCO Crowdworkers(Yahoo ) /enc-34  MCVQA(Ours ) MSCOCO Automatically generated /enc-34 /enc-34 Table5 : ComparisonofVQAdatasetswithour MCVQAdataset . 
Theimagesusedare : MSCOCO ( Linetal . 
,2014 ) andNYUDepthv2 ( NathanSilbermanandFergus , 2012 ) .913 Proceedings of the 1st Conference of the Asia - PaciÔ¨Åc Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing , pages 914‚Äì924 December 4 - 7 , 2020 . 
¬© 2020 Association for Computational Linguistics Toxic Language Detection in Social Media for Brazilian Portuguese : New Dataset and Multilingual Analysis JoÀúao A. Leite , Diego F. Silva Departamento de Computac ¬∏ Àúao Federal University of S Àúao Carlos SÀúao Carlos , Brazil joaoaugustobr@hotmail.com diegofs@ufscar.brKalina Bontcheva , Carolina Scarton Department of Computer Science University of ShefÔ¨Åeld ShefÔ¨Åeld , UK k.bontcheva@sheffield.ac.uk c.scarton@sheffield.ac.uk Abstract Hate speech and toxic comments are a common concern of social media platform users . 
Although these comments are , fortunately , the minority in these platforms , they are still capable of causing harm . 
Therefore , identifying these comments is an important task for studying and preventing the proliferation of toxicity in social media . 
Previous work in automatically detecting toxic comments focus mainly in English , with very few work in languages like Brazilian Portuguese . 
In this paper , we propose a new large - scale dataset for Brazilian Portuguese with tweets annotated as either toxic or non - toxic or in different types of toxicity . 
We present our dataset collection and annotation process , where we aimed to select candidates covering multiple demographic groups . 
State - of - the - art BERT models were able to achieve 76 % macro- F1score using monolingual data in the binary case . 
We also show that large - scale monolingual data is still needed to create more accurate models , despite recent advances in multilingual approaches . 
An error analysis and experiments with multi - label classiÔ¨Åcation show the difÔ¨Åculty of classifying certain types of toxic comments that appear less frequently in our data and highlights the need to develop models that are aware of different categories of toxicity . 
1 Introduction Social media can be a powerful tool that enables virtual human interactions , connecting people and enhancing businesses ‚Äô presence . 
On the other hand , since users feel somehow protected under their virtual identities , social media has also become a platform for hate speech and use of toxic language . 
Although hate speech is a crime in most countries , identifying cases in social media is not an easy task , given the massive amounts of data posted every day . 
Therefore , automatic approaches for detecting online hate speech have received signiÔ¨Åcant attentionin recent years ( Waseem and Hovy , 2016 ; Davidson et al . 
, 2017 ; Zampieri et al . 
, 2019b ) . 
In this paper , we focus on the analysis and automatic detection oftoxic comments . 
Our deÔ¨Ånition of toxic is similar to the one used by the Jigsaw competition,1 where comments containing insults and obscene language are also considered , besides hate speech.2 Systems capable of automatically identifying toxic comments are useful for platform ‚Äôs moderators and to select content for speciÔ¨Åc users ( e.g. children ) . 
Nevertheless , there are multiple challenges speciÔ¨Åc to process toxic comments automatically , e.g. ( i ) toxic language may not be explicit , i.e. may not contain explicit toxic terms ; ( ii ) there is a large spectrum of types of toxicity ( e.g. sexism , racism , insult ) ; ( iii ) toxic comments correspond to a minority of comments , which is fortunate , but means that automatic data - driven approaches need to deal with highly unbalanced data . 
Although there is some work on this topic for other languages ‚Äì e.g. Arabic ( Mubarak et al . 
, 2017 ) and German ( Wiegand et al . 
, 2018 ) ‚Äì , most of the resources and studies available are for English ( Davidson et al . 
, 2017 ; Wulczyn et al . 
, 2017 ; Founta et al . 
, 2018 ; Mandl et al . 
, 2019 ; Zampieri et al . 
, 2019b).3For Portuguese , only two previous works are available ( Fortuna et al . 
, 2019 ; de Pelle and Moreira , 2017 ) and their datasets are considerably small , mainly when compared to resources available for English . 
We present ToLD - Br ( Toxic Language Dataset for Brazilian Portuguese ) , a new dataset with Twitter posts in the Brazilian Portuguese language.4 1https://www.kaggle.com/c/ jigsaw - toxic - comment - classification - challenge/ overview 2This is also similar to the usage of offensive comments in OffensEval ( Zampieri et al . 
, 2019b , 2020 ) . 
3A large list of resources is available at http:// hatespeechdata.com . 
4It is important to distinguish the language variant , since914 A total of 21 K tweets were manually annotated into seven categories : non - toxic , LGBTQ+phobia , obscene , insult , racism , misogyny andxenophobia . 
Each tweet has three annotations that were made by volunteers from a university in Brazil . 
V olunteers were selected taking into account demographic information , aiming to create a dataset as balanced as possible in regarding to demographic group biases . 
This is then the largest dataset available for toxic data analysis in social media for the Portuguese language and the Ô¨Årst dataset with demographic information about annotators.5 We experiment with Brazilian Portuguese ( Souza et al . 
, 2019 ) and Multilingual ( Wolf et al . 
, 2019 ) BERT models ( Devlin et al . 
, 2019 ) for the binary task of automatically classifying toxic comments , since similar models achieve state - of - the - art results for the same task in other languages ( Zampieri et al . 
, 2019b ) . 
Models Ô¨Åne - tuned on monolingual data achieve up to 76 % of macro - F1 , improving 3points over a baseline . 
Besides , BERT - based approaches with multilingual pre - trained models enable transfer learning and zero - shot learning . 
The OffensEval 2019 OLID dataset ( Zampieri et al . 
, 2019a ) is then used to experiment with ( i ) transfer - learning : where both OLID and ToLD - Br are used to Ô¨Åne - tune BERT ; and , ( ii ) zero - shot learning : where BERT is Ô¨Åne - tuned using only OLID . 
Results highlight the importance of language - speciÔ¨Åc datasets , since transfer learning does not improve over monolingual models and zero - shot learning achieves only a macro- F1of56 % . 
An error analysis is performed using our best model , where the worst - case scenario , i.e. , classifying toxic comments as non - toxic , is further investigated , taking into account the Ô¨Åne - grained categories . 
Results show that categories with fewer examples in the dataset ( racism andxenophobia ) are more likely to be mislabelled than other classes , with the best performance being achieved by majority classes ( insult andobscene ) . 
We also analyse theamount of data needed in order to achieve the best performance in binary classiÔ¨Åcation . 
Models trained with few examples are only accurate in predicting the majority class ( non - toxic ) . 
As the number of instances grow , the performance on the minority class ( toxic ) improves signiÔ¨Åcantly . 
there are multiple differences between Brazilian Portuguese lexicon and other variants of Portuguese . 
5ToLD - Br is available at : https://github.com/ JAugusto97 / ToLD - BrFinally , we experiment with multi - label classiÔ¨Åcation , where each different type of toxicity is automatically classiÔ¨Åed . 
This is a considerably harder problem than binary classiÔ¨Åcation , where BERTbased models do not outperform the baseline . 
Section 2 presents an overview of relevant previous work . 
Section 3 shows details about the ToLDBr dataset . 
Material and methods are presented in Section 4 , whilst results are discussed in Section 5 . 
Finally , Section 6 shows a Ô¨Ånal discussion and future work . 
2 Related Work Although multiple researchers have addressed the topic of hate speech ( e.g. Waseem and Hovy ( 2016 ) , Chung et al . 
( 2019 ) , Basile et al . 
( 2019 ) ) , we focus the literature review on previous work related to toxic comments detection , the topic of our paper . 
Due to space constraints , we only describe papers that create and use Twitter - based datasets and/or focus on the Brazilian Portuguese language . 
English Davidson et al . 
( 2017 ) present a dataset with around 25 K tweets annotated by crowdworkers as containing hate , offensive language , orneither . 
They build a feature - based classiÔ¨Åer with TF - IDF transformation over n - grams , part - ofspeech information , sentiment analysis , network information ( e.g. , number of replies ) , among other features . 
Their best model , trained using logistic regression , achieves a macro- F1of90 . 
Founta et al . 
( 2018 ) also rely on crowd - workers to annotate 80 K tweets into eight categories : offensive , abusive , hateful speech , aggressive , cyberbullying , spam , andnormal . 
They perform an exploratory approach to identify the categories that cause most confusion to crowd - workers . 
Their Ô¨Ånal , large - scale annotation is done using four categories : abusive , hateful , normal , orspam . 
OffensEval is a series of shared tasks focusing on offensive comments detection ( Zampieri et al . 
, 2019b , 2020 ) . 
The OLID dataset ( used in the 2019 edition ) has around 14 K tweets in English manually annotated as offensive ornonoffensive . 
The best model for the relevant task A ( offensive versus non - offensive ) uses a BERT - based classiÔ¨Åer and achieves 82.9of macro - F1 . 
German A shared task ( organized as part of GermEval 2018 ) aimed to classify tweets in German categorized into offensive ornon - offensive ( Wiegand et al . 
, 2018 ) . 
They make available a manually annotated dataset with approximately 8.5 K tweets.915 The best system achieved 76.77ofF1 - score and was a feature - based ensemble approach . 
Arabic Mubarak et al . 
( 2017 ) present a dataset with 1.1 K manually annotated tweets into obscene , offensive , orclean . 
They experiment with lexicalbased approaches that achieve a maximum of 60 F1 - score . 
Mulki et al . 
( 2019 ) create a dataset with tweets in the Levantine dialect of Arabic manually annotated into normal , abusive , orhate ( with approximately 5 K tweets ) . 
The authors use featurebased approaches to induce models for ternary and binary scenarios , with best systems achieving 74.4 and89.6ofF1 - score , respectively . 
Spanish Carmona et al . 
( 2018 ) present a shared task aiming to detect aggressive tweets in Mexican Spanish . 
They manually annotate 11 K tweets into aggressive ornon - aggressive . 
The best system is a feature - based approach with macro- F1of62 . 
Hindi Mathur et al . 
( 2018 ) present a dataset of around 3.6 K tweets in Hinglish ( spoken Hindi written using the Roman script ) . 
The dataset was annotated into three classes not offensive , abusive and hate - inducing by ten NLP researchers . 
A Convolutional Neural Network ( CNN ) architecture with transfer learning is used , where the model is trained with both Hinglish and English data ( from ( Davidson et al . 
, 2017 ) ) , achieving 71.4 % ofF1 - score . 
Portuguese de Pelle and Moreira ( 2017 ) make available a dataset with 1,250comments , extracted from comment sessions of g1.globo.com website , and annotated them into categories of offensive or non - offensive . 
The offensive class was also subdivided into racism , sexism , LGBTQ+phobia , xenophobia , religious in - tolerance , orcursing . 
They experiment with binary classiÔ¨Åcation , using n - grams as features to SVM and NaiveBayes models . 
Best results are achieved with SVM reaching a weighted F1 score between 77and82 , depending on different label interpretations . 
Fortuna et al . 
( 2019 ) describe a dataset with 5,668tweets classiÔ¨Åed as hate vs. non - hate , with the hate class further classiÔ¨Åed following a Ô¨Åne - grained hierarchy . 
Experiments with binary classiÔ¨Åcation show a F1score of 78using an LSTM - based architecture . 
Multilingual HASOC was a shared task aiming to classify hate speech and offensive comments in English , German , and Hindi ( Mandl et al . 
, 2019 ) . 
Their dataset contains around 7 K tweets and Facebook posts manually annotated . 
Sub - task A sep - arates posts into hate speech oroffensive versus neither ; and , sub - task B separates posts containinghate speech oroffence into three categories : hate speech , offensive orprofane . 
Best performing systems in all languages used deep learning approaches . 
For OffensEval 2020 ( Zampieri et al . 
, 2020 ) , a more extensive training data is available for English ( over 9 M tweets ) , although the annotation was made semi - automatically . 
Arabic , Danish , Greek , and Turkish datasets are also available with manually annotated labels . 
For all languages , best models are achieved using some variation of BERT . 
Our work is different from previous approaches because we ( i ) release a large - scale dataset for a language other than English , that was created with the aim to reduce demographic biases ; ( ii ) experiment with multilingual approaches , including transfer learning and zero - shot - learning ; ( iii ) perform an analysis of the amount of data needed to train reliable models ; and , ( iv ) experiment with multilabel classiÔ¨Åcation , providing Ô¨Årst insights into this challenge task . 
3 Dataset In this section , we describe the procedure adopted to create ToLD - Br and present its main features . 
3.1 Data collection We used the GATE Cloud ‚Äôs Twitter Collector6to collect posts on the Twitter platform from July to August 2019 . 
We used two different strategies to select tweets for ToLD - Br , aiming to increase the probability of obtaining posts with toxic content , given that the volume of toxic tweets is signiÔ¨Åcantly smaller than data without offensive language . 
Our Ô¨Årst strategy searches for tweets that mention predeÔ¨Åned hashtags or keywords . 
We chose predeÔ¨Åned terms highly likely to belong to a toxic tweet in Brazilian Twitter , such as gay(‚ÄúGay tem que apanhar ‚Äù ‚Äì ‚Äú Gay should be beaten up ‚Äù ) , mulherzinha ( ‚Äú Mulherzinha , vai lavar lou c ¬∏a ‚Äù ‚Äì ‚Äú Sissy , go wash dishes ‚Äù ) , and nordestino ( ‚Äú Nordestino pregui c ¬∏oso ‚Äù ‚Äì ‚Äú Lazy Northeastern ‚Äù ) . 
However , using this strategy alone may hinder learning a model capable of generalising the concept of toxicity beyond the scope of keywords . 
Consequently , another strategy was adopted : we scraped tweets that mention inÔ¨Çuential users like Brazil ‚Äôs president Jair Bolsonaro and soccer player Neymar Jr , 6https://cloud.gate.ac.uk/shopfront/ displayItem / twitter - collector916 prone to receive abuse ( around 50inÔ¨Çuential users were monitored ) . 
Tweets collected through this method have no restrictions in terms of keywords and should broaden the scope of the data . 
We collected more than 10 M unique tweets and randomly selected 21 K examples to compose the annotated corpus . 
We note that 12,600of these posts ( 60 % ) comes from the Ô¨Årst strategy ‚Äì predeÔ¨Åned keywords ‚Äì and the remaining are tweets from threads of predeÔ¨Åned users . 
The data was pseudoanonymised before being sent for annotation , with all @mentions replaced by @user . 
3.2 Corpus annotation The annotation process started by choosing volunteers to perform the task of assigning labels for each example . 
For this , we made a public consultation at the Federal University of S Àúao Carlos ( Brazil ) to Ô¨Ånd candidate annotators ( 129volunteers registered for the task ) . 
From these candidates , 42 were selected based on their demographic information , aiming to balance annotation bias as the interpretation of toxicity may vary . 
Each annotator labelled 1,500tweets , selecting one of the following categories : LGBTQ+phobia , obscene , insult , racism , misogyny and/or xenophobia ( or leaving it blank for none ) . 
Each tweet was annotated by three different annotators . 
To evaluate the diversity among the annotators , we explore their proÔ¨Åle . 
We emphasise that the identity of all annotators has been preserved . 
At this stage , we only survey general aspects of the volunteers who joined the labelling process . 
Table 1 presents the distribution of annotators regarding sex , sexual orientation , and ethnicity . 
To deÔ¨Åne these categories , we use the same values as the Brazilian Institute of Geography and Statistics,7 in addition to giving the candidate the option of not declaring a value for each characteristic . 
Although we tried to keep the demographic aspects as balanced as possible when selecting the annotators , our pool of volunteers was still biased towards people identiÔ¨Åed as white andheterosexual ( sexis a more balanced aspect than the others ) . 
The age of the annotators varies between 18and37years , with most of them in the range between 19and23 . 
Figure 1 illustrates the age distribution . 
We perform different data analysis over the dataset to better understand its properties . 
Inter7https://www.ibge.gov.br/en/home-eng . 
htmlCategories # annotators SexMale 18 Female 24 Heterosexual 22 Sexual Bisexual 12 orientation Homosexual 5 Pansexual 3 EthnicityWhite 25 Brown 9 Black 5 Asian 2 Non - Declared 1 Table 1 : Annotators demographic information . 
1819202122232425262729303537 age02468count Figure 1 : Annotators age distribution . 
Œ± LGBTQ+phobia 0.68 Insult 0.56 Xenophobia 0.57 Misogyny 0.52 Obscene 0.49 Racism 0.48 Mean 0.55 Table 2 : Krippendorff ‚Äô sŒ±for each label . 
annotator agreement is calculated in terms of Krippendorf ‚Äô sŒ±(Table 2 ) , since Œ±is robust to multiple annotators , different degrees of disagreement and , missing values ( Artstein and Poesio , 2008 ) . 
The LGBTQ+phobia class shows the highest agreement , which may indicate that comments in this class have a more distinctive lexicon than other classes . 
The lowest agreement is seem in obscene andracism classes . 
Besides , we observed in the annotations many cases in which some examples were labelled as separate classes , although they intend917 Ann 1 Ann 2 Ann 3 o fdp do Ô¨Ålho dela nao parava de tocar auto pra c*****o [ ... ] Insult None Obscene her sob son did not stop to play loud as f**k [ ... ] [ ... ] VAI SE F***R IRM ÀúAO VC N ÀúAO¬¥E FELIZ PQ NAO QUER Obscene Insult Insult [ ... ] f**k you brother you are not happy because you do not want to be ‚Äú Aonde tem um monte que fala mal , mas ningu ¬¥ em vai embora do morro . 
‚Äù acha que algu ¬¥ em mora aqui por que quer , c*****o ! ? Que i d ¬¥ eia . 
[ ... ] Obscene Obscene Insult ‚Äú Where there are loads saying bad things , but nobody leaves the slum . 
‚Äù who thinks that someone lives here because they want , f**k ! ? What an idea . 
[ ... ] Table 3 : Example of annotation divergence . 
LGBTQ+phobia Obscene Insult Racism Misogyny Xenophobia viado ( 59 ) porra ( 332 ) puta ( 221 ) nego ( 6 ) putinha ( 38 ) sulista ( 12 ) boiola ( 15 ) caralho ( 317 ) caralho ( 150 ) branco ( 6 ) puta ( 22 ) carioca ( 7 ) viadinho ( 13 ) puta ( 268 ) cara ( 135 ) preto ( 4 ) piranha ( 19 ) fala ( 4 ) sapat Àúao ( 12 ) tomar ( 136 ) porra ( 122 ) nada ( 4 ) mulher ( 11 ) paulista ( 4 ) caralho ( 11 ) fuder ( 98 ) lixo ( 101 ) neg Àúao ( 3 ) vagabunda ( 11 ) gente ( 3 ) cara ( 10 ) cara ( 94 ) Ô¨Ålho ( 92 ) cara ( 3 ) quer ( 8) nordestino ( 3 ) quer ( 9 ) merda ( 90 ) burro ( 87 ) falando ( 3 ) vaca ( 8) todo ( 3 ) homem ( 9 ) mano ( 87 ) tomar ( 86 ) vida ( 3 ) Ô¨Åca ( 6 ) ainda ( 3 ) todo ( 9 ) toma ( 85 ) merda ( 78 ) segue ( 2 ) onde ( 5 ) sendo ( 2 ) bicha ( 9 ) fazer ( 77 ) idiota ( 76 ) p ¬¥ agina ( 2 ) tudo ( 5 ) danc ¬∏a ( 2 ) Table 4 : The most common words of each class and the number of sentences they occur ( within parentheses ) . 
to point the same concept . 
Classes like obscene andinsult seem to have confused the annotators , which may indicate an intersection in these concepts . 
Table 3 shows examples of disagreements in the classiÔ¨Åcation of obscene andinsult . 
Table 4 presents the ten most frequent words for each class , after removing stopwords . 
It conÔ¨Årms the intersection between classes obscene andinsult , with six out of ten words in common . 
For a quantitative analysis , Table 5 presents the Jaccard distance between the 100most frequent words for each class . 
Obscene andinsult show a considerably lower distance than other pairs ( 0.57 ) , indicating that they have more words in common . 
3.3 Dataset characteristics For the purpose of training models for automatically classifying toxic comments , we must create aggregated annotations to provide only one binary label for each class . 
Different rules can be employed to aggregate the annotations , with different semantics . 
When we set an example as positive for toxicity only when all the annotators consider it to have the same category of offence , we insert bias toa b c d e f a0.00 0.73 0.78 0.90 0.80 0.94 b - 0.00 0.57 0.84 0.77 0.90 c - - 0.00 0.86 0.75 0.92 d - - - 0.00 0.87 0.95 e - - - - 0.00 0.94 Table 5 : Jaccard distance between all pair of classes . 
( a ) LGBTQ+phobia ; ( b ) Obscene ; ( c ) Insult ; ( d ) Racism ; ( e ) Misogyny ; ( f ) Xenophobia . 
the model to not accuse a comment as toxic unless the offence is evident . 
Since this is very restrictive , we can also use the majority rule , but there must still be a consensus among the annotators . 
A last option is to consider that only a positive annotation is sufÔ¨Åcient to label the example as positive . 
This procedure acknowledges that annotators may have divergent views about what was said . 
It is a risky rule if we intend to create rigid systems that classify the tweets and take corrective or prohibitive actions . 
However , it is beneÔ¨Åcial for training a model that ‚Äú raises a Ô¨Çag ‚Äù to help moderators to assess the com-918 LGBTQ+phobia Insult Xenophobia Misogyny Obscene Racism Toxic At least one annotator 0 20656 16615 20849 20537 14348 20862 11745 1 344 4385 151 463 6652 138 9255 At least two annotators 0 20824 19131 20958 20867 18597 20967 16566 1 176 1869 42 133 2403 33 4424 Three annotators 0 20926 20483 20985 20971 20388 20994 19510 1 74 517 15 29 612 6 1490 Table 6 : Dataset distribution considering different types of label aggregation . 
ments . 
Table 6 shows the data distribution for each label and each aggregation strategy . 
For the sake of reproducibility and further usage , ToLD - Br is split into default training ( 80 % ) , development ( 10 % ) and test ( 10 % ) sets using a stratiÔ¨Åed strategy . 
Besides , the corpus is released with all the annotations . 
Thus , future users of ToLD - Br will be able to use it with all the labels and with varying levels of agreement between the annotators . 
In this paper , we consider the least restrictive case , where if at least one annotator marked any offence category in an example , the example is positive for toxicity . 
Likewise , if a tweet was not tagged in any of these categories , it is considered non - toxic . 
We believe that it is essential that if any person feels uncomfortable with a post , it should be Ô¨Çagged as having a certain degree of toxicity . 
Therefore , a model built with this data must be able to identify offensive posts , even for a speciÔ¨Åc group of people . 
4 Materials and Methods In this section , we describe the techniques , tools , and other materials used in our experimental evaluation . 
As mentioned before , we restrict our experiments on the dataset labelled as positive when at least one annotator considers the example as toxic . 
We then investigate the effects of the number of instances in the training data , different algorithms to train a classiÔ¨Åcation model , various scenarios considering single- and multilingual models , and perform an initial experiment with multi - label classiÔ¨Åcation . 
We use Bag - of - Words ( BoW ) to represent the examples and an AutoML model to build the baseline model ( BoW+AutoML ) . 
For this , weuse the auto - sklearn8library ( Feurer et al . 
, 2019 ) . 
For our BERT - based models , we use thesimpletransformers9library , that allows easy training and evaluation . 
We use default arguments for parameter tuning and deÔ¨Åne a seed to allow for reproducibility . 
Two versions of pretrained BERT language models are applied : Brazilian Portuguese BERT10(Souza et al . 
, 2019 ) , and Multilingual BERT11(Wolf et al . 
, 2019 ) . 
ToLD - Br is used to Ô¨Åne - tune BERT - based models for our monolingual experiments , with monolingual BERT ( BR - BERT ) and multilingual BERT ( M - BERT - BR ) . 
Although M - BERT - BR refers to the multilingual version of BERT , we refer to these two models as ‚Äú monolingual models , ‚Äù as we trained using the dataset with Brazilian Portuguese sentences alone . 
Using the multilingual model , we also carry out experiments in which we add data in English to train the models either through transfer learning or zero - shot learning . 
For these experiments we use the OLID data , concatenating the training and test splits into a single dataset . 
For transfer learning , we merged OLID and ToLD - Br to obtain a model with both languages as input , aiming to assess whether extra data in English helps in building better models ( M - BERT(transfer ) ) . 
For zero - shot learning , OLID is used alone at training time , building a model that did not have access to any data in Brazilian Portuguese ( M - BERT(zero - shot ) ) . 
8https://automl.github.io/auto-sklearn 9github.com/ThilinaRajapakse/ simpletransformers 10huggingface.co/neuralmind/ bert - base - portuguese - cased 11huggingface.co/bert-base-multilingual-cased919 Through these experiments , we can assess the advantages of monolingual models , whether data from another language can directly beneÔ¨Åt the classiÔ¨Åcation , and whether a speciÔ¨Åc monolingual dataset is necessary or not . 
We experiment with different sizes of the training set to assess the inÔ¨Çuence of the volume of data on the classiÔ¨Åcation . 
For that , we evaluate the results on random subsets of the data . 
The size of each partition varies in a range between 10 % and 100 % adding 10 % of the data at each iteration . 
For each step , we repeat the classiÔ¨Åcation three times to minimise the probability of reporting results obtained by chance . 
Our best model ( M - BERT - BR ) is used for this experiment ( c.f . 
Section 5 ) . 
Evaluation for binary classiÔ¨Åcation is done in terms of precision , recall and , F1 - score per class and macro - F1 . 
We also analyse the confusion matrices of our systems in order to better visualise the performance of our models in each class , mainly focusing on an analysis of false negatives . 
Although we mainly focus on binary classiÔ¨Åcation , an initial approach for multi - label classiÔ¨Åcation is also presented . 
We use the adaptation for the multi - label classiÔ¨Åcation scenario available in simpletransformers . 
In this case , the transformer ‚Äôs output consists of six neurons , each representing one of the labels . 
These neurons are considered independent in the training and prediction process . 
Thus , when an output neuron is activated , we set the label represented by this neuron to positive . 
Besides , we evaluate the performance of a baseline based on BoW+AutoML , where we train an AutoML model for multilabel classiÔ¨Åcation . 
Evaluation is done in terms of Hamming loss and average precision ( Tsoumakas et al . 
, 2009 ) . 
5 Results and Discussion This section shows the results of our experiments in classifying toxic comments using ToLD - Br . 
5.1 Binary ClassiÔ¨Åcation For evaluating our models , we are particularly interested in models with high performance in the positive class ( classiÔ¨Åcation of toxic comments ) . 
The worst case scenario are false negatives , i.e. toxic comments classiÔ¨Åed as non - toxic . 
Tables 7 through 11 summarises the results for each model . 
BoW+AutoML is already a competitive model , achieving 74 % of macro- F1 , as shown in Table 7 and Figure 2a . 
Precision Recall F1 - score 0 0.76 0.75 0.75 1 0.71 0.73 0.72 Macro Avg 0.74 0.74 0.74 Weighted Avg 0.74 0.74 0.74 Table 7 : BoW + AutoML Precision Recall F1 - score 0 0.77 0.80 0.79 1 0.76 0.73 0.74 Macro Avg 0.76 0.76 0.76 Weighted Avg 0.76 0.77 0.76 Table 8 : BR - BERT Precision Recall F1 - score 0 0.81 0.69 0.75 1 0.69 0.82 0.75 Macro Avg 0.75 0.75 0.75 Weighted Avg 0.76 0.75 0.75 Table 9 : M - BERT - BR Precision Recall F1 - score 0 0.80 0.74 0.77 1 0.72 0.79 0.75 Macro Avg 0.76 0.76 0.76 Weighted Avg 0.77 0.76 0.76 Table 10 : M - BERT(transfer ) Precision Recall F1 - score 0 0.59 0.83 0.69 1 0.63 0.32 0.43 Macro Avg 0.61 0.58 0.56 Weighted Avg 0.61 0.60 0.57 Table 11 : M - BERT(zero - shot ) The monolingual models BR - BERT and M - BERT - BR ( Tables 8 and 9 , respectively ) show very similar performances in all metrics , withBR - BERT being slightly better in terms of macro - F1 . 
However , M - BERT - BR is better in terms ofF1 - score for the positive class and shows fewer false negatives than BR - BERT ( Figure 2b forBR - BERT and Figure 2c for M - BERT - BR ) . 
M - BERT(transfer ) ( Table 10 ) does not out-920 0 1 Predicted0 1True843 285 263 709 0.20.40.60.81.0 ( a ) 0 1 Predicted0 1True902 226 267 705 0.20.40.60.81.0   ( b ) 0 1 Predicted0 1True778 350 179 793 0.20.40.60.81.0   ( c ) 0 1 Predicted0 1True837 291 207 765 0.20.40.60.81.0   ( d ) 0 1 Predicted0 1True940 188 657 315 0.20.40.60.81.0   ( e ) Figure 2 : Confusion matrices for each model ( a ) BoW+AutoML ( Baseline ) ; ( b ) BR - BERT ; ( c ) M - BERT - BR ; ( d ) M - BERT(transfer ) ; ( e ) M - BERT(zero - shot ) 2500 5000 7500 10000 12500 15000 17500 20000 examples0.20.30.40.50.60.70.8scoreRecall Precision ( a ) 2500 5000 7500 10000 12500 15000 17500 20000 examples0.550.600.650.700.750.800.850.900.95scoreRecall Precision ( b ) Figure 3 : Precision and recall for different sizes of the training dataset for the ( a ) positive and ( b ) negative classes . 
perform the monolingual models and it also shows more false negatives than M - BERT - BR ( Figure 2e ) . 
On the other hand , the number of false negatives in BR - BERT ( 267 ) is slightly higher than the number of false negatives in M - BERT(transfer ) ( 207 ) . 
Finally , M - BERT(zero - shot ) ( Table 11 ) is the worst model , as expected . 
It performs particularly bad when classifying the positive class , achieving only 43 % ofF1 - score for this class , mainly caused by its high number of false negatives ( Figure 2d ) . 
In summary , transfer learning does not seem to improve over the overall performance of monolingual models . 
Based on the analysis of false negatives , M - BERT - BR appears as our best model . 
Zero - shot learning shows a very low performance , being particularly bad in the positive class . 
Error Analysis We also analyse the performance of our best model ( M - BERT - BR ) in each Ô¨Ånegrained class . 
The idea is to identify which toxic classes are most difÔ¨Åcult to be classiÔ¨Åed as toxic by our binary classiÔ¨Åer . 
As false negatives are a critical type of error in our application , Table 12 shows the false negative rate ( false negatives / expected positives ) for each toxic class . 
The ratio of false negatives is inversely proportional to the number of examples for a speciÔ¨Åc class . 
Insult and obscene , the largest classes , show the lowest falsenegative rate , whilst the highest rates are shown by classes with less examples ( racism andxenophobia ) . 
Therefore , in order to improve classiÔ¨Åcation models , these aspects of the imbalanced data need to be taken into account and further studied . 
False negative rate LGBTQ+phobia 7/35 ( 0.2 ) Insult 67/448 ( 0.15 ) Xenophobia 13/19 ( 0.68 ) Misogyny 7/45 ( 0.15 ) Obscene 117/701 ( 0.17 ) Racism 8/17 ( 0.47 ) Table 12 : Error analysis for each label . 
5.2 Importance of Large Datasets In this experiment , we highlight the importance of collecting a considerable amount of examples , as toxicity can be expressed in many different ways . 
We separated the training data into 10random splits from 10 % to 100 % of the data , increasing 10 % of data at each step , and trained M - BERT - BR with three random samples for each step . 
Figure 3 shows the mean recall , precision and F1 - score for the positive and negative classes , respectively , for each921 data split . 
With few training examples , the model only performs well on the majority class , but as the number of instances grows , recall for the negative class starts decreasing while recall for the positive class increases , and precision rises for both classes . 
At least 6 K examples seems to be necessary to achieve reliable results , while previous work for Portuguese reports the largest dataset with only 5,668examples . 
This highlights the importance of ToLD - Br , as a large - scale dataset . 
5.3 Multi - Label ClassiÔ¨Åcation We experiment with multi - label classiÔ¨Åcation , building a model using the Multilingual BERT ( similar to M - BERT - BR ) . 
Our baseline is a set of BoW+AutoML models trained using Binary Relevance ( Tsoumakas et al . 
, 2009 ) for multi - label classiÔ¨Åcation . 
The BERT - based models adopt a score threshold of 0.5 in the output neuron to deal with multi - label . 
If the activation for a label in the output layer is higher than the threshold , we consider it positive . 
The baseline model obtained 0.08and0.20of Hamming loss and average precision , respectively , while M - BERT - BR resulted in 0.07and0.19for these measures , respectively . 
Figure 4 displays the confusion matrices obtained by M - BERT - BR . 
0 1 Predicted0 1True2072 2 25 1 0.20.40.60.81.0 ( a ) 0 1 Predicted0 1True1430 38 427 205 0.20.40.60.81.0   ( b ) 0 1 Predicted0 1True1635 41 290 134 0.20.40.60.81.0 ( c ) 0 1 Predicted0 1True2089 0 11 0 0.20.40.60.81.0   ( d ) 0 1 Predicted0 1True2057 0 39 4 0.20.40.60.81.0 ( e ) 0 1 Predicted0 1True2081 0 19 0 0.20.40.60.81.0   ( f ) Figure 4 : Confusion matrices for each label ( a ) LGBTQ+phobia ; ( b ) Obscene ; ( c ) Insult ; ( d ) Racism ; ( e ) Misogyny ; ( f ) Xenophobia . 
This scenario is considerably more challenging than binary classiÔ¨Åcation . 
The positive class of each label corresponds to a subset of the examples labelled as toxic . 
Thus , it is likely that the number of instances for these classes will be insufÔ¨Åcient for the model to learn . 
Besides , the problem of unbalanced classes becomes evident ( c.f . 
Table 6 ) . 
As a consequence , it is clear that labels with a small number of positive examples , like racism , misogyny , xenophobia , and LGBTQ+phobia were almost entirely classiÔ¨Åed as negative . 
In contrast , for obscene andinsult , labels with a considerable amount of positive examples , the model was capable of classifying some examples correctly . 
In all cases , besides insult , the baseline performs slightly better for the positive class ( which justify the higher Hamming loss ) . 
This setback is likely due to the difÔ¨Åculty of the neural model to learn with few examples . 
6 Concluding Remarks In this paper , we present ToLD - Br : a dataset for the classiÔ¨Åcation of toxic comments on Twitter in Brazilian Portuguese . 
Through a wide and comprehensive analysis , we demonstrated the need for this dataset for studies on automatic classiÔ¨Åcation of toxic comments . 
We highlight that monolingual approaches for this task still outperform multilingual experiments and that large - scale datasets are needed for building reliable models . 
Also , we show that there are still challenges to be overcome , such as the naturally signiÔ¨Åcant class imbalance when dealing with multi - label classiÔ¨Åcation . 
As future work , in addition to deal with class imbalance , we intend to evaluate if aggregating classes with high divergences between annotators can build more reliable models . 
Besides , we intend to assess the beneÔ¨Åts of adding unlabelled data to ToLD - Br to use semi - supervised techniques . 
7 Acknowledgements We thank the volunteers from UFSCar that made this research possible . 
The MIDAS group12from the Federal University of S Àúao Carlos ( UFSCar ) , Brazil , funded the annotation process . 
The SoBigData TransNational Access program ( EU H2020 , grant agreement : 654024 ) funded Diego Silva and JoÀúao Leite ‚Äôs visits to the University of ShefÔ¨Åeld . 
12midas.ufscar.br922 Abstract Stance classiÔ¨Åcation can be a powerful tool for understanding whether and which users believe in online rumours . 
The task aims to automatically predict the stance of replies towards a given rumour , namely support , deny , question , or comment . 
Numerous methods have been proposed and their performance compared in the RumourEval shared tasks in 2017 and 2019 . 
Results demonstrated that this is a challenging problem since naturally occurring rumour stance data is highly imbalanced . 
This paper speciÔ¨Åcally questions the evaluation metrics used in these shared tasks . 
We reevaluate the systems submitted to the two RumourEval tasks and show that the two widely adopted metrics ‚Äì accuracy and macro- F1 ‚Äì are not robust for the four - class imbalanced task of rumour stance classiÔ¨Åcation , as they wrongly favour systems with highly skewed accuracy towards the majority class . 
To overcome this problem , we propose new evaluation metrics for rumour stance detection . 
These are not only robust to imbalanced data but also score higher systems that are capable of recognising the two most informative minority classes ( support anddeny ) . 
1 Introduction The automatic analysis of online rumours has emerged as an important and challenging Natural Language Processing ( NLP ) task . 
Rumours in social media can be deÔ¨Åned as claims that can not be veriÔ¨Åed as true or false at the time of posting ( Zubiaga et al . 
, 2018 ) . 
Prior research ( Mendoza et al . 
, 2010 ; Kumar and Carley , 2019 ) has shown that the stances of user replies are often a useful predictor of a rumour ‚Äôs likely veracity , specially in the case of false rumours that tend to receive a higher number of replies denying them ( Zubiaga et al . 
, 2016 ) . 
However , their automatic classiÔ¨Åcation is far from trivial as demonstrated by theresults of two shared tasks ‚Äì RumourEval 2017 and 2019 ( Derczynski et al . 
, 2017 ; Gorrell et al . 
, 2019 ) . 
More speciÔ¨Åcally , sub - task A models rumour stance classiÔ¨Åcation ( RSC ) as a four - class problem , where replies can : ‚Ä¢support /agree with the rumour ; ‚Ä¢deny the veracity of the rumour ; ‚Ä¢query /ask for additional evidence ; ‚Ä¢comment without clear contribution to assessing the veracity of the rumour . 
Figure 1 shows an example of a reply denying a post on Twitter . 
Figure 1 : Example of a deny stance . 
In RumourEval 2017 the training data contains 297 rumourous threads about eight events . 
The test set has 28 threads , with 20 threads about the same events as the training data and eight threads about unseen events . 
In 2019 , the 2017 training data is augmented with 40 Reddit threads . 
The new 2019 test set has 56 threads about natural disasters from Twitter and a set of Reddit data ( 25 threads ) . 
These datasets for RSC are highly imbalanced : thecomment class is considerably larger than the other classes . 
Table 1 shows the distribution of stances per class in both 2017 and 2019 datasets , where 66 % and 72 % of the data ( respectively ) correspond to comments .Comments arguably are the925 2017 2019 support 1,004 ( 18 % ) 1,184 ( 14 % ) deny 415 ( 7 % ) 606 ( 7 % ) query 464 ( 8 % ) 608 ( 7 % ) comment 3,685 ( 66 % ) 6,176 ( 72 % ) total 5,568 8,574 Table 1 : Distribution of stances per class ‚Äì with percentages between parenthesis . 
least useful when it comes to assessing overall rumour veracity , unlike support anddeny which have been shown to help with rumour veriÔ¨Åcation ( Mendoza et al . 
, 2010 ) . 
Therefore , RSC is not only an imbalanced , multi - class problem , but it also has classes with different importance . 
This is different from standard stance classiÔ¨Åcation tasks ( e.g. SemEval 2016 task 6 ( Mohammad et al . 
, 2016 ) ) , where classes have arguably the same importance . 
It also differs from the veracity task ( RumourEval sub - task B ) , where the problem is binary and it is not as an imbalanced problem as RSC.1 RumourEval 2017 evaluated systems based on accuracy ( ACC ) , which is not sufÔ¨Åciently robust on imbalanced datasets ( Huang and Ling , 2005 ) . 
This prompted the adoption of macro- F1 in the 2019 evaluation . 
Kumar and Carley ( 2019 ) also argue that macro- F1is a more reliable evaluation metric for RSC . 
Previous work on RSC also adopted these metrics ( Li et al . 
, 2019b ; Kochkina et al . 
, 2018 ; Dungs et al . 
, 2018 ) . 
This paper re - evaluates the sub - task A results of RumourEval 2017 and 2019.2It analyses the performance of the participating systems according to different evaluation metrics and shows that even macro - F1 , that is robust for evaluating binary classiÔ¨Åcation on imbalanced datasets , fails to reliably evaluate the performance on RSC . 
This is particularly critical in RumourEval where not only is data imbalanced , but also two minority classes ( deny andsupport ) are the most important to classify well . 
Based on prior research on imbalanced datasets in areas other that NLP ( e.g. Yijing et al . 
( 2016 ) and Elrahman and Abraham ( 2013 ) ) , we propose four alternative metrics for evaluating RSC . 
These metrics change the systems ranking for RSC in RumourEval 2017 and 2019 , rewarding systems with high performance on the minority classes . 
1Other NLP tasks , like sentiment analysis are also not comparable , since these tasks are either binary classiÔ¨Åcation ( which is then solved by using macro- F1 ) or do not have a clear priority over classes . 
2We thank the organisers for making the data available.2 Evaluation metrics for classiÔ¨Åcation We deÔ¨ÅneTP = true positives , TN = true negatives , FP = false positives and FN = false negatives , where TPc(FPc ) is equivalent to the true ( false ) positives and TNc(FNc ) is equivalent to the true ( false ) negatives for a given class c. Accuracy ( ACC ) is the ratio between the number of correct predictions and the total number of predictions ( N):ACC = /summationtextC c=1TPc N , whereCis the number of classes . 
ACC only considers the values that were classiÔ¨Åed correctly , disregarding the mistakes . 
This is inadequate for imbalanced problems like RSC where , as shown in Table 1 , most of the data is classiÔ¨Åed as comments . 
As shown in Section 3 , most systems will fail to classify the deny class and still achieve high scores in terms of ACC . 
In fact , the best system for 2017 according toACC ( Turing ) fails to classify all denies . 
Precision ( Pc ) and Recall ( Rc)Pcis the ratio between the number of correctly predicted instances and all the predicted values for c : Pc= TPc TPc+FPc . 
Rcis the ratio between correctly predicted instances and the number of instances that actually belongs to the class c : Rc = TPc TPc+FNc . 
macro - FŒ≤FŒ≤cscore is deÔ¨Åned as the harmonic mean of precision and recall , where the per - class score can be deÔ¨Åned as : FŒ≤c= ( 1 + Œ≤2)Pc¬∑Rc Œ≤2Pc+Rc . 
IfŒ≤= 1,FŒ≤is theF1score . 
IfŒ≤ > 1,Ris given a higher weight and if Œ≤ < 1,Pis given a higher weight . 
The macro- FŒ≤is the arithmetic mean between the FŒ≤scores for each class : macroFŒ≤c=/summationtextC c=1FŒ≤c C. Although macro- F1is expected to perform better than ACC for imbalanced binary problems , its beneÔ¨Åts in the scenario of multi - class classiÔ¨Åcation are not clear . 
SpeciÔ¨Åcally , as it relies on the arithmetic mean over the classes , it may hide the poor performance of a model in one of the classes if it performs well on the majority class ( i.e. comments in this case ) . 
For instance , as shown in Table 2 , according to macro- F1the best performing system would be ECNU , which still fails to classify correctly almost all deny instances . 
Geometric mean Metrics like the geometric mean ofR : GMR = C / radicaltp / radicalvertex / radicalvertex / radicalbtC / productdisplay c=1Rc.926 ACC macro - F1GMRwAUC wF1wF2 Turing a 0.784 ( 1 ) 0.434 ( 5 ) 0.000 ( 8) 0.583 ( 7 ) 0.274 ( 6 ) 0.230 ( 7 ) UWaterloo ( Bahuleyan and Vechtomova , 2017 ) 0.780 ( 2 ) 0.455 ( 2 ) 0.237 ( 5 ) 0.595 ( 5 ) 0.300 ( 2 ) 0.255 ( 6 ) ECNU ( Wang et al . 
, 2017 ) 0.778 ( 3 ) 0.467 ( 1 ) 0.214 ( 7 ) 0.599 ( 4 ) 0.289 ( 4 ) 0.263 ( 4 ) Mama Edha ( Garc ¬¥ ƒ±a Lozano et al . 
, 2017 ) 0.749 ( 4 ) 0.453 ( 3 ) 0.220 ( 6 ) 0.607 ( 1 ) 0.299 ( 3 ) 0.283 ( 3 ) NileTMRG ( Enayet and El - Beltagy , 2017 ) 0.709 ( 5 ) 0.452 ( 4 ) 0.363 ( 1 ) 0.606 ( 2 ) 0.306 ( 1 ) 0.296 ( 1 ) IKM ( Chen et al . 
, 2017 ) 0.701 ( 6 ) 0.408 ( 7 ) 0.272 ( 4 ) 0.570 ( 8) 0.241 ( 7 ) 0.226 ( 8) IITP ( Singh et al . 
, 2017 ) 0.641 ( 7 ) 0.403 ( 8) 0.345 ( 2 ) 0.602 ( 3 ) 0.276 ( 5 ) 0.294 ( 2 ) DFKI DKT ( Srivastava et al . 
, 2017 ) 0.635 ( 8) 0.409 ( 6 ) 0.316 ( 3 ) 0.589 ( 6 ) 0.234 ( 8) 0.256 ( 5 ) majority class 0.742 0.213 0.000 0.500 0.043 0.047 all denies 0.068 0.032 0.000 0.500 0.051 0.107 all support 0.090 0.041 0.000 0.500 0.066 0.132 Table 2 : Evaluation of RumourEval 2017 submissions . 
Values between parenthesis are the ranking of the system according to the metric . 
The ofÔ¨Åcial evaluation metric column ( ACC ) is highlighted in bold . 
ACC macro - F1GMR wAUC wF1wF2 BLCU NLP ( Yang et al . 
, 2019 ) 0.841 ( 2 ) 0.619 ( 1 ) 0.571 ( 2 ) 0.722 ( 2 ) 0.520 ( 1 ) 0.500 ( 2 ) BUT - FIT ( Fajcik et al . 
, 2019 ) 0.852 ( 1 ) 0.607 ( 2 ) 0.519 ( 3 ) 0.689 ( 3 ) 0.492 ( 3 ) 0.441 ( 3 ) eventAI ( Li et al . 
, 2019a ) 0.735 ( 11 ) 0.578 ( 3 ) 0.726 ( 1 ) 0.807 ( 1 ) 0.502 ( 2 ) 0.602 ( 1 ) UPV ( Ghanem et al . 
, 2019 ) 0.832 ( 4 ) 0.490 ( 4 ) 0.333 ( 5 ) 0.614 ( 5 ) 0.340 ( 4 ) 0.292 ( 5 ) GWU ( Hamidian and Diab , 2019 ) 0.797 ( 9 ) 0.435 ( 5 ) 0.000 ( 7 ) 0.604 ( 6 ) 0.284 ( 5 ) 0.265 ( 6 ) SINAI - DL ( Garc ¬¥ ƒ±a - Cumbreras et al . 
, 2019 ) 0.830 ( 5 ) 0.430 ( 6 ) 0.000 ( 8) 0.577 ( 7 ) 0.255 ( 7 ) 0.215 ( 7 ) wshuyi 0.538 ( 13 ) 0.370 ( 7 ) 0.467 ( 4 ) 0.627 ( 4 ) 0.261 ( 6 ) 0.325 ( 4 ) Columbia ( Liu et al . 
, 2019 ) 0.789 ( 10 ) 0.363 ( 8) 0.000 ( 9 ) 0.562 ( 10 ) 0.221 ( 10 ) 0.191 ( 9 ) jurebb 0.806 ( 8) 0.354 ( 9 ) 0.122 ( 6 ) 0.567 ( 9 ) 0.229 ( 8) 0.120 ( 12 ) mukundyr 0.837 ( 3 ) 0.340 ( 10 ) 0.000 ( 10 ) 0.570 ( 8) 0.224 ( 9 ) 0.198 ( 8) nx1 0.828 ( 7 ) 0.327 ( 11 ) 0.000 ( 11 ) 0.557 ( 11 ) 0.206 ( 11 ) 0.173 ( 10 ) WeST ( Baris et al . 
, 2019 ) 0.829 ( 6 ) 0.321 ( 12 ) 0.000 ( 12 ) 0.551 ( 12 ) 0.197 ( 12 ) 0.161 ( 11 ) Xinthl 0.725 ( 12 ) 0.230 ( 13 ) 0.000 ( 13 ) 0.493 ( 13 ) 0.072 ( 13 ) 0.071 ( 13 ) majority class 0.808 0.223 0.000 0.500 0.045 0.048 all denies 0.055 0.026 0.000 0.500 0.042 0.091 all support 0.086 0.040 0.000 0.500 0.063 0.128 Table 3 : Evaluation of RumourEval 2019 submissions . 
Values between parenthesis are the ranking of the system according to the metric . 
The ofÔ¨Åcial evaluation metric column ( macro- F1 ) is highlighted in bold . 
are proposed for evaluating speciÔ¨Åc types of errors . 
AsFNs may be more relevant than FPs for imbalanced data , assessing models using Ris an option to measure this speciÔ¨Åc type of error . 
Moreover , applyingGMR for each class severely penalises a model that achieves a low score for a given class . 
Area under the ROC curve Receiver operating characteristic ( ROC ) ( Fawcett , 2006 ) assesses the performance of classiÔ¨Åers considering the relation between Rcand the false positive rate , deÔ¨Åned as ( per class ): FPRc = FPc TNc+FPc . 
Since RSC consists of discrete classiÔ¨Åcations , ROC charts for each ccontain only one point regarding the coordinate ( FPRc , Rc ) . 
Area under the ROC curve ( AUC ) measures the area of the curve produced by the points in an ROC space . 
In the discrete case , it measures the area of the polygon drawn by the segments connecting the vertices ( ( 0,0),(FPRc , Rc),(1,1),(0,1 ) ) . 
High AUC scores are achieved when R(probability of detection ) is maximised , while FPR ( probability of false alarm ) is minimised . 
We experiment with a weighted variation of AUC : wAUC = C / summationdisplay c=1wc¬∑AUCc . 
Weighted macro- FŒ≤ a variation of macro- FŒ≤ , where each class also receives different weights , is also considered : wFŒ≤ = C / summationdisplay c=1wc¬∑FŒ≤c , We useŒ≤= 1(PandRhave the same importance ) andŒ≤= 2(Ris more important ) . 
Arguably , misclassifying denies andsupports ( FNDandFNS , respectively ) is equivalent to ignore relevant information for debunking a rumour . 
Since FNs negatively impact R , we hypothesise that Œ≤= 2is more robust for the RSC case . 
wAUC andwFŒ≤ are inspired by empirical evidence that different classes have different importance for RSC.3Weights should be manually deÔ¨Åned , since they can not be automatically learnt . 
3Similarly , previous work proposes metrics ( Elkan , 2001 ) and learning algorithms ( Chawla et al . 
, 2008 ) based on classspeciÔ¨Åc mis - classiÔ¨Åcation costs.927 Figure 2 : Confusion matrix for systems from RumourEval 2017 . 
Figure 3 : Confusion matrix for selected systems from RumourEval 2019 . 
All other systems failed to classify correctly either all or the vast majority of deny instances . 
We follow the hypothesis that support anddeny classes are more informative than others.4 3 Re - evaluating RumourEval task A Tables 2 and 3 report the different evaluation scores per metric for each of the RumourEval 2017 and 2019 systems.5ACC and macro - F1are reported in the second and third columns respectively , followed by a column for each of the four proposed metrics . 
Besides evaluating the participating systems , we also computed scores for three baselines : majority class ( all stances are considered comments ) , all denies andall support ( all replies are classed as deny /support ) . 
Our results show that the choice of evaluation metric has a signiÔ¨Åcant impact on system ranking . 
In RumourEval 2017 , the winning system based onACC wasTuring . 
However , Figure 2 shows that this system classiÔ¨Åed all denies in4wsupport = wdeny = 0.40,wquery = 0.15and wcomment = 0.05 . 
5The systems HLT(HITSZ ) , LECS , magc , UI - AI , shaheyu andNimbusTwoThousand are omitted because they do not provide the same number of inputs as the test set.correctly , favouring the majority class ( comment ) . 
When looking at the macro- F1score , Turing is classiÔ¨Åed as Ô¨Åfth , whilst the winner is ECNU , followed by UWaterloo . 
Both systems also perform very poorly on denies , classifying only 1 % and 3 % of them correctly . 
On the other hand , the four proposed metrics penalise these systems for these errors and rank higher those that perform better on classes other than the majority one . 
For example , the winner according to GMR , wF1 andwF2isNileTMRG that , according to Figure 2 , shows higher accuracy on the deny , support andquery classes , without considerably degraded performance on the majority class . 
wAUC still favours the Mama Edha system which has very limited performance on the important deny class . 
As is evident from Figure 2 , NileTMRG is arguably the best system in predicting all classes : it has the highest accuracy for denies , and a sufÔ¨Åciently high accuracy for support , queries and comments . 
Using the same criteria , the second best system should be IITP . 
The only two metrics that reÔ¨Çect this ranking are GMR andwF2 . 
In the case ofwF1 , the second system is UWaterloo , 928 which has a very low accuracy on the deny class . 
For RumourEval 2019 , the best system according to macro- F1(the ofÔ¨Åcial metric ) is BLCU NLP , followed by BUT - FIT . 
However , after analysing the confusion matrices in Figure 3 , we can conclude that eventAI is a more suitable model due to its high accuracy on support anddeny . 
MetricsGMR , wAUC andwF2show eventAI as the best system . 
Finally , wshuyi is ranked as fourth according to GMR , wAUC andwF2 , while it ranked seventh in terms of macro- F1 , behind systems like GWU andSINAI - DL that fail to classify all deny instances . 
Although wshuyi is clearly worse than eventAI , BLCU NLP and BUT - FIT , it is arguably more reliable than systems that misclassify the large majority of denies .6 Our analyses suggest that GMR andwF2are the most reliable for evaluating RSC tasks . 
4 Weight selection In Section 3 , wAUC , wF1andwF2have been obtained using empirically deÔ¨Åned weights ( wsupport = wdeny = 0.40,wquery = 0.15and wcomment = 0.05 ) . 
These values reÔ¨Çect the key importance of the support anddeny classes . 
Although query is less important than the Ô¨Årst two , it is nevertheless more informative than comment . 
Previous work tried to adjust the learning weights in order to minimise the effect of the imbalanced data . 
Garc ¬¥ ƒ±a Lozano et al . 
( 2017 ) ( Mama Edha ) , change the weights of their Convolutional Neural Network ( CNN ) architecture , giving higher importance to support , deny andquery classes , to better reÔ¨Çect their class distribution.7Ghanem et al . 
( 2019 ) ( UPV ) also change the weights in their Logistic Regression model in accordance with the data distribution criterion.8Nevertheless , these systems misclassify almost all deny instances . 
Table 4 shows the RumourEval 2017 systems ranked according to wF2using the Mama Edha andUPV weights . 
In these cases , wF2beneÔ¨Åts DFKI DKT , ranking it Ô¨Årst , since queries receive a higher weight than support . 
However , this system only correctly classiÔ¨Åes 6 % of support instances , which makes it less suitable for our task thanNileTMRG for instance . 
ECNU is also ranked 6Confusion matrices for all systems of RumourEval 2019 are presented in Appendix A. 7wsupport = 0.157 , wdeny = 0.396,wquery = 0.399 andwcomment = 0.048 8wsupport = 0.2 , wdeny = 0.35,wquery = 0.35and wcomment = 0.1better than Mama Edha andIITP , likely due to its higher performance on query instances . 
wF2wF2 Mama Edha UPV Turing 0.246 ( 8) 0.289 ( 8) UWaterloo 0.283 ( 7 ) 0.322 ( 5 ) ECNU 0.334 ( 3 ) 0.364 ( 3 ) Mama Edha 0.312 ( 4 ) 0.349 ( 4 ) NileTMRG 0.350 ( 2 ) 0.374 ( 2 ) IKM 0.293 ( 5 ) 0.318 ( 7 ) IITP 0.289 ( 6 ) 0.321 ( 6 ) DFKI DKT 0.399 ( 1 ) 0.398 ( 1 ) Table 4 : RumourEval 2017 evaluated using wF2with weights from Mama Edha andUPV . 
Arguably , deÔ¨Åning weights based purely on data distribution is not sufÔ¨Åcient for RSC . 
Thus our empirically deÔ¨Åned weights seem to be more suitable than those derived from data distribution alone , as the former accurately reÔ¨Çect that support anddeny are the most important , albeit minority distributed classes . 
Further research is required in order to identify the most suitable weights for this task . 
5 Discussion This paper re - evaluated the systems that participated in the two editions of RumourEval task A ( stance classiÔ¨Åcation ) . 
We showed that the choice of evaluation metric for assessing the task has a signiÔ¨Åcant impact on system rankings . 
The metrics proposed here are better suited to evaluating tasks with imbalanced data , since they do not favour the majority class . 
We also suggest variations of AUC and macro - FŒ≤that give different weights for each class , which is desirable for scenarios where some classes are more important than others . 
The main lesson from this paper is that evaluation is an important aspect of NLP tasks and it needs to be done accordingly , after a careful consideration of the problem and the data available . 
In particular , we recommend that future work on RSC usesGMR and / orwFŒ≤ ( preferablyŒ≤= 2 ) as evaluation metrics . 
Best practices on evaluation rely on several metrics that can assess different aspects of quality . 
Therefore , relying on several metrics is likely the best approach for RSC evaluation . 
Acknowledgments This work was funded by the WeVerify project ( EU H2020 , grant agreement : 825297 ) . 
The SoBigData TransNational Access program ( EU H2020 , grant agreement : 654024 ) funded Diego Silva ‚Äôs visit to the University of ShefÔ¨Åeld.929 