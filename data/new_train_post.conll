In O
this O
paper O
, O
we O
consider O
approaches O
to O
finetuning O
and O
interpolation O
that O
are O
novel O
in O
that O
they O
leverage O
data O
from O
similar O
users O
to O
boost O
personalized O
LM O
performance O
. O

We O
consider O
the O
case O
of O
users O
with O
a O
small O
number O
of O
available O
tokens O
and O
propose O
ways O
to O
( O
1 O
) O
find O
similar O
users O
in O
our O
corpus O
and O
( O
2 O
) O
leverage O
data O
from O
similar O
users O
to O
build O
a O
personalized O
LM O
for O
a O
new O
user O
. O

We O
explore O
the O
trade O
- O
offs O
between O
the O
amount O
of O
available O
data O
from O
existing O
users O
, O
the O
number O
of O
existing O
users O
and O
new O
users O
, O
and O
how O
our O
similarity O
metrics O
and O
methods O
scale O
. O

We O
then O
show O
an O
analysis O
to O
explore O
what O
types O
of O
words O
our O
method O
predicts O
more O
accurately O
and O
are O
thus O
more O
important O
to O
consider O
in O
personalization O
methods O
. O

Personalized B-TaskName
Language I-TaskName
Modeling I-TaskName
. O

King O
and O
Cook O
( O
2020 O
) O
examined O
methods O
for O
creating O
personalized O
LMs O
and O
their O
work O
is O
most O
similar O
to O
ours O
. O

They O
consider O
interpolating O
, O
fine O
- O
tuning O
, O
and O
priming O
LMs O
as O
methods O
of O
personalization O
, O
though O
they O
use O
these O
methods O
with O
a O
large O
generic O
model O
. O

In O
contrast O
, O
our O
work O
shows O
that O
performance O
can O
be O
improved O
by O
leveraging O
data O
from O
similar O
users O
. O

They O
also O
analyzed O
model O
adaptation O
for O
models O
trained O
on O
users O
with O
similar O
demographics O
, O
inspired O
by O
Lynn O
et O
al O
. O
( O
2017 O
) O
, O
who O
showed O
that O
these O
demographic O
factors O
could O
help O
model O
a O
variety O
of O
tasks O
, O
and O
found O
that O
personalized O
models O
perform O
better O
than O
those O
adapted O
from O
similar O
demographics O
. O

Shao O
et O
al O
. O
( O
2020 O
) O
have O
also O
explored O
models O
for O
personalization O
but O
focused O
on O
handling O
OOV O
tokens O
. O

Wu O
et O
al O
. O
( O
2020 O
) O
proposed O
a O
framework O
to O
learn O
user O
embeddings O
from O
Reddit O
posts O
. O

Their O
user O
embeddings O
were O
built O
on O
the O
sentence O
embeddings O
generated O
by O
a O
BERT B-MethodName
model O
. O

By O
using O
the O
learned O
user O
embeddings O
to O
predict O
gender O
, O
detect O
depression O
and O
classify O
MBTI O
personality O
, O
they O
concluded O
that O
their O
embeddings O
incorporate O
intrinsic O
attributes O
of O
users O
. O

In O
our O
work O
, O
user O
embeddings O
are O
learned O
in O
a O
different O
approach O
, O
and O
we O
focus O
on O
how O
to O
use O
similarity O
calculated O
from O
user O
embeddings O
to O
build O
better O
LMs O
. O

We O
study O
personalization O
in O
language B-TaskName
modeling I-TaskName
, O
a O
core O
task O
in O
NLP O
. O

Direct O
applications O
of O
language O
models O
( O
LM O
) O
include O
predictive O
text O
, O
authorship O
attribution O
, O
and O
dialog O
systems O
used O
to O
model O
the O
style O
of O
an O
individual O
or O
profession O
( O
e.g. O
, O
therapist O
, O
counselor O
) O
. O

LMs O
are O
increasingly O
used O
as O
the O
backbone O
of O
models O
for O
a O
range O
of O
tasks O
in O
NLP O
, O
increasing O
the O
potential O
impact O
of O
personalization O
even O
further O
( O
Brown O
et O
al O
. O
, O
2020 O
) O
. O

Recent O
work O
has O
suggested O
that O
there O
are O
several O
benefits O
to O
personalized O
models O
in O
natural O
language O
processing O
( O
NLP O
) O
over O
one O
- O
size O
- O
fits O
- O
all O
solutions O
: O
they O
are O
more O
accurate O
for O
individual O
users O
; O
they O
help O
us O
understand O
communities O
better O
; O
and O
they O
focus O
the O
attention O
of O
our O
evaluations O
on O
the O
enduser O
( O
Flek O
, O
2020 O
) O
. O

Generation B-TaskName
tasks O
in O
particular O
benefit O
from O
a O
personalized O
approach O
, O
for O
example O
, O
Dudy O
et O
al O
. O
( O
2021 O
) O
argue O
that O
user O
intention O
is O
more O
often O
difficult O
to O
recover O
from O
the O
context O
alone O
. O

In O
FSS B-MethodName
- I-MethodName
Net I-MethodName
, O
the O
visual O
features O
output O
from O
convolutional O
layers O
are O
passed O
through O
a O
1layer B-HyperparameterValue
Bi B-HyperparameterValue
- I-HyperparameterValue
LSTM I-HyperparameterValue
with O
256 B-HyperparameterValue
hidden B-HyperparameterName
units I-HyperparameterName
per O
direction O
to O
capture O
temporal O
information O
. O

To O
generate O
proposals O
, O
we O
first O
transform O
the O
feature O
sequence O
via O
a O
1D O
- O
CNN O
with O
the O
following O
architecture O
: O
conv O
layer O
( O
512 B-HyperparameterValue
output B-HyperparameterName
dimension I-HyperparameterName
, O
kernel B-HyperparameterName
width I-HyperparameterName
8) B-HyperparameterValue
, O
max O
pooling O
( O
kernel B-HyperparameterName
width I-HyperparameterName
8 B-HyperparameterValue
, O
stride B-HyperparameterName
4 B-HyperparameterValue
) O
, O
conv O
layer O
( O
256 B-HyperparameterValue
output B-HyperparameterName
dimension I-HyperparameterName
, O
kernel B-HyperparameterName
width I-HyperparameterName
3 B-HyperparameterValue
) O
and O
conv O
layer O
( O
256 B-HyperparameterValue
output B-HyperparameterName
dimension I-HyperparameterName
, O
kernel B-HyperparameterName
width I-HyperparameterName
3 B-HyperparameterValue
) O
. O

The O
scale B-HyperparameterName
of I-HyperparameterName
anchors I-HyperparameterName
is O
chosen O
from O
the O
range O
: O
{ O
1 B-HyperparameterValue
, O
2 B-HyperparameterValue
, O
3 B-HyperparameterValue
, O
4 B-HyperparameterValue
, O
5 B-HyperparameterValue
, O
6 B-HyperparameterValue
, O
7 B-HyperparameterValue
, O
8 B-HyperparameterValue
, O
9 B-HyperparameterValue
, O
10 B-HyperparameterValue
, O
12 B-HyperparameterValue
, O
14 B-HyperparameterValue
, O
18 B-HyperparameterValue
, O
20 B-HyperparameterValue
, O
24 B-HyperparameterValue
, B-HyperparameterValue
32 I-HyperparameterValue
, O
40 B-HyperparameterValue
, O
60 B-HyperparameterValue
, O
75 B-HyperparameterValue
} O
, O
according O
to O
the O
typical O
fingerspelling O
lengths O
in O
the O
two O
datasets O
. O

The O
positve B-HyperparameterName
/ I-HyperparameterName
negative I-HyperparameterName
threshold I-HyperparameterName
of O
the O
anchors O
are O
0.6/0.3 B-HyperparameterValue
respectively O
. O

δ B-HyperparameterName
IoU I-HyperparameterName
/δ B-HyperparameterName
IS I-HyperparameterName
are O
1.0/0.8 B-HyperparameterValue
( O
chosen O
from O
{ O
0.4 B-HyperparameterValue
, O
0.6 B-HyperparameterValue
, O
0.8 B-HyperparameterValue
, O
1.0 B-HyperparameterValue
} O
) O
. O

The O
FS O
- O
encoder O
and O
text O
encoder O
are O
3 B-HyperparameterValue
- O
layer/1 B-HyperparameterName
- O
layer B-HyperparameterName
BiLSTM B-HyperparameterValue
with O
256 B-HyperparameterValue
hidden B-HyperparameterName
units I-HyperparameterName
respectively O
. O

The O
margin B-HyperparameterName
m B-HyperparameterName
, B-HyperparameterName
number I-HyperparameterName
of I-HyperparameterName
negative I-HyperparameterName
samples I-HyperparameterName
in O
N O
v O
and O
N O
w O
are O
tuned O
to O
be O
0.45 B-HyperparameterValue
, O
5 B-HyperparameterValue
and O
5 B-HyperparameterValue
. O

The O
model O
is O
trained O
for O
25 B-HyperparameterValue
epochs B-HyperparameterName
with B-HyperparameterValue
Adam I-HyperparameterValue
( O
Kingma O
and O
Ba O
, O
2015 O
) O
at O
initial B-HyperparameterName
learning I-HyperparameterName
rate I-HyperparameterName
of O
0.001 B-HyperparameterValue
and O
batch B-HyperparameterName
size I-HyperparameterName
of O
32 B-HyperparameterValue
. O

The O
learning B-HyperparameterName
rate I-HyperparameterName
is O
halved O
if O
the O
mean B-MetricName
average I-MetricName
precision I-MetricName
on O
the O
dev O
set O
does O
not O
improve O
for O
3 B-HyperparameterValue
epochs O
. O

λ B-HyperparameterName
det I-HyperparameterName
in O
equation O
4 O
is O
0.1 B-HyperparameterValue
( O
chosen O
from O
{ O
0.1 B-HyperparameterValue
, O
0.5 B-HyperparameterValue
, O
1.0 B-HyperparameterValue
} O
) O
. O

At O
test O
time O
, O
we O
generate O
M B-HyperparameterName
= O
50 B-HyperparameterValue
proposals O
after O
NMS O
with O
IoU B-HyperparameterName
threshold I-HyperparameterName
of O
0.7 B-HyperparameterValue
. O

β B-HyperparameterName
is O
tuned O
to O
1 B-HyperparameterValue
( O
chosen O
from O
{ O
0.5 B-HyperparameterValue
, O
1 B-HyperparameterValue
, O
2 B-HyperparameterValue
, O
3 B-HyperparameterValue
} O
) O
. O

Model O
implementation O
The O
backbone O
convolutional O
layers O
are O
taken O
from O
VGG-19 O
( O
Simonyan O
and O
Zisserman O
, O
2015 O
) O
. O

We O
pre O
- O
train O
the O
convolutional O
layers O
with O
a O
fingerspelling O
recognition O
task O
using O
the O
video O
- O
text O
pairs O
from O
the O
corresponding O
dataset O
. O

In O
pre O
- O
training O
, O
the O
VGG-19 O
layers O
are O
first O
pre O
- O
trained O
on O
ImageNet B-DatasetName
( O
Deng O
et O
al O
. O
, O
2009 O
) O
and O
the O
image O
features O
further O
go O
through O
a O
1- B-HyperparameterValue
layer B-HyperparameterName
Bi B-HyperparameterValue
- I-HyperparameterValue
LSTM I-HyperparameterValue
with O
512 B-HyperparameterValue
hidden B-HyperparameterName
units I-HyperparameterName
per O
direction O
. O

The O
model O
is O
trained O
with O
CTC B-HyperparameterValue
loss I-HyperparameterValue
( O
Graves O
et O
al O
. O
, O
2006 O
) O
. O

The O
output O
labels O
include O
the O
English O
alphabet O
plus O
the O
few O
special O
symbols O
, O
<space> O
, O
' O
, O
& O
, O
. O

, O
@ O
, O
as O
well O
as O
the O
blank O
symbol O
for O
CTC O
. O

The O
model O
is O
trained O
with O
SGD B-HyperparameterValue
with O
batch B-HyperparameterName
size I-HyperparameterName
1 B-HyperparameterValue
at O
the O
initial B-HyperparameterName
learning I-HyperparameterName
rate I-HyperparameterName
of O
0.01 B-HyperparameterValue
. O

The O
model O
is O
trained O
for O
30 B-HyperparameterValue
epochs B-HyperparameterName
and O
the B-HyperparameterName
learning I-HyperparameterName
rate I-HyperparameterName
is O
decayed O
to O
0.001 B-HyperparameterValue
after O
20 B-HyperparameterValue
epochs B-HyperparameterName
. O

The O
recognizer B-HyperparameterName
achieves O
52.5%/64.4 B-MetricValue
% B-MetricValue
lettter B-MetricName
accuracy I-MetricName
on O
ChicagoFSWild B-DatasetName
/ O
ChicagoFSWild+ B-DatasetName
test O
sets O
. O

The O
VGG-19 O
convolutional O
layers O
are O
frozen O
in O
FSS B-MethodName
- I-MethodName
Net I-MethodName
training O
. O

The O
ASL O
fingerspelling O
alphabet O
, O
from O
( O
Keane O
, O
2014 O
) O
Table O
6 O
shows O
the O
number O
of O
video O
clips O
in O
the O
two O
datasets O
. O

Figure O
6 O
shows O
the O
distribution O
of O
fingerspelling O
sequence O
length O
in O
the O
two O
datasets O
. O

Figure O
7 O
shows O
image O
examples O
from O
the O
following O
three O
data O
sources O
: O
YouTube O
, O
DeafVIDEO O
, O
misc O
. O

Pre O
- O
processing O
The O
raw O
images O
in O
ChicagoF B-DatasetName
- I-DatasetName
SWild I-DatasetName
and O
ChicagoFSWild+ B-DatasetName
datasets O
contain O
diverse O
visual O
scenes O
which O
can O
involve O
multiple O
persons O
. O

We O
adapt O
the O
heuristic O
approach O
used O
in O
( O
Shi O
et O
al O
. O
, O
2019 O
) O
to O
select O
the O
target O
signer O
. O

Specifically O
, O
we O
use O
an O
off O
- O
the O
- O
shelf O
face O
detector O
to O
detect O
all O
the O
faces O
in O
the O
image O
. O

We O
extend O
each O
face O
bounding O
box O
by O
1.5 O
times O
size O
of O
the O
bounding O
box O
in O
4 O
directions O
and O
select O
the O
largest O
one O
with O
highest O
average O
magnitude O
of O
optical O
flow O
( O
Farnebäck O
, O
2003 O
) O
. O

We O
further O
use O
the O
bounding O
box O
averaged O
over O
the O
whole O
sequence O
to O
crop O
the O
ROI O
area O
, O
which O
roughly O
denotes O
the O
signing O
region O
of O
a O
signer O
. O

Each O
image O
is O
resized O
to O
160 O
× O
160 O
before O
feeding O
into O
the O
model O
. O

The O
datasets O
we O
use O
are O
collected O
from O
multiple O
sources O
, O
and O
the O
video O
quality O
varies O
between O
them O
. O

To O
quantify O
the O
effect O
of O
visual O
quality O
on O
search O
/ O
retrieval O
performance O
, O
we O
categorize O
the O
ASL O
videos O
into O
three O
categories O
according O
to O
their O
source O
: O
YouTube O
, O
DeafVIDEO O
, O
and O
other O
miscellaneous O
sources O
( O
misc O
) O
. O

YouTube O
videos O
are O
mostly O
ASL O
lectures O
with O
high O
resolution O
. O

DeafVIDEO O
videos O
are O
vlogs O
from O
deaf O
users O
of O
the O
social O
media O
site O
deafvideo.tv O
, O
where O
the O
style O
, O
camera O
angle O
, O
and O
image O
quality O
vary O
greatly O
. O

The O
visual O
quality O
of O
videos O
in O
the O
miscellaneous O
category O
tends O
to O
fall O
between O
the O
other O
two O
categories O
. O

Typical O
image O
examples O
from O
the O
three O
categories O
can O
be O
found O
in O
the O
Appendix O
( O
figure O
7 O
) O
. O

The O
FWS B-TaskName
performance O
of O
our O
model O
on O
videos O
in O
YouTube O
, O
Deaf O
- O
VIDEO O
, O
and O
misc O
are O
0.684 B-MetricValue
, O
0.584 B-MetricValue
, O
0.629 B-MetricValue
( O
mAP B-MetricName
) O
respectively O
. O

The O
results O
are O
overall O
consistent O
with O
the O
perceived O
relative O
visual O
qualities O
of O
these O
categories O
. O

6 O
Results O
and O
analysisTable O
1 O
shows O
the O
performance O
of O
the O
above O
approaches O
on O
the O
two O
datasets O
. O

First O
, O
we O
notice O
that O
embedding O
- O
based O
approaches O
consistently O
outperform O
the O
recognizer O
baseline O
in O
the O
larger O
data O
setting O
( O
ChicagoFSWild+ B-DatasetName
) O
but O
not O
the O
smaller O
data O
setting O
( O
ChicagoFSWild B-DatasetName
) O
, O
which O
suggests O
that O
embedding O
- O
based O
models O
generally O
require O
more O
training O
data O
. O

The O
inferior O
performance O
of O
recognizer O
also O
shows O
that O
explicit O
fingerspelling O
recognition O
is O
not O
necessary O
for O
the O
search O
tasks O
. O

In O
addition O
, O
explicit O
fingerspelling O
detection O
( O
Ext B-MethodName
- I-MethodName
Det I-MethodName
, O
FSS B-MethodName
- I-MethodName
Net I-MethodName
) O
improves O
performance O
over O
implicit O
fin- O
Of O
the O
models O
that O
do O
n't O
use O
such O
supervision O
, O
Attn B-MethodName
- I-MethodName
KWS I-MethodName
is O
the O
best O
performer O
given O
enough O
data O
, O
but O
is O
still O
far O
behind O
FSS B-MethodName
- I-MethodName
Net I-MethodName
. O

Our O
model O
outperforms O
all O
of O
the O
alternatives O
. O

The O
relative O
performance O
of O
different O
models O
remains O
consistent O
across O
the O
various O
metrics O
and O
the O
two O
search O
tasks O
. O

For O
completeness O
, O
we O
also O
measure O
the O
performance O
of O
different O
models O
in O
terms O
of O
rankingbased O
metrics O
( O
e.g. O
, O
Precision@N B-MetricName
, O
Recall@N B-MetricName
) O
, O
as O
in O
prior O
work O
on O
video B-TaskName
- I-TaskName
text I-TaskName
retrieval I-TaskName
( O
Ging O
et O
al O
. O
, O
2020;Ranjay O
et O
al O
. O
, O
2017 O
) O
( O
see O
full O
results O
in O
the O
Appendix O
) O
. O

The O
relative O
performance O
of O
different O
models O
remains O
consistent O
on O
these O
metrics O
. O

The O
analysis O
below O
is O
done O
on O
ChicagoFSWild B-DatasetName
for O
simplicity O
. O

The O
conclusions O
also O
hold O
for O
ChicagoF B-DatasetName
- I-DatasetName
SWild+.In I-DatasetName
the O
previous O
section O
we O
have O
seen O
that O
models O
that O
explicitly O
detect O
and O
localize O
fingerspelling O
outperform O
ones O
that O
do O
not O
. O

Next O
we O
look O
more O
closely O
at O
how O
well O
several O
models O
- O
Ext B-MethodName
- I-MethodName
Det I-MethodName
, O
Attn B-MethodName
- I-MethodName
KWS I-MethodName
and O
FSS B-MethodName
- I-MethodName
Net I-MethodName
- O
perform O
on O
the O
task O
of O
localizing O
fingerspelling O
, O
which O
is O
a O
byproduct O
of O
these O
models O
' O
output O
. O

We O
measure O
performance O
via O
AP@IoU B-MetricName
, O
a O
commonly O
used O
evaluation O
metric O
for O
action B-TaskName
detection I-TaskName
( O
Idrees O
et O
al O
. O
, O
2016;Heilbron O
et O
al O
. O
, O
2015 O
) O
that O
has O
also O
been O
used O
for O
fingerspelling B-TaskName
detection I-TaskName
( O
Shi O
et O
al O
. O
, O
2021 O
) O
. O

AP@IoU B-MetricName
measures O
the O
average B-MetricName
precision I-MetricName
of O
a O
detector O
under O
the O
constraint O
that O
the O
overlap O
of O
its O
predicted O
segments O
with O
the O
ground O
truth O
is O
above O
some O
threshold O
Intersectionover B-MetricName
- I-MetricName
Union I-MetricName
( O
IoU B-MetricName
) O
value O
. O

For O
Attn B-MethodName
- I-MethodName
KWS I-MethodName
, O
the O
model O
outputs O
an O
attention O
vector O
, O
which O
we O
convert O
to O
segments O
as O
in O
( O
Shi O
et O
al O
. O
, O
2021 O
) O
. O

In O
general O
, O
the O
models O
with O
more O
accurate O
localization O
also O
have O
higher O
search O
and O
retrieval O
performance O
, O
as O
seen O
by O
comparing O
Table O
2 O
with O
Table O
1 O
. O

However O
, O
differences O
in O
AP@IoU B-MetricName
do O
not O
directly O
translate O
to O
differences O
in O
search O
performance O
. O

For O
example O
, O
the O
AP@IoU B-MetricName
of O
Ext B-MethodName
- I-MethodName
Det I-MethodName
( O
0.344 B-MetricValue
) O
is O
an O
order O
of O
magnitude O
higher O
than O
that O
of O
Attn B-MethodName
- I-MethodName
KWS I-MethodName
( O
0.035 B-MetricValue
) O
while O
their O
FVS B-TaskName
mAP B-MetricName
results O
are O
much O
closer O
( O
0.593 B-MetricValue
vs. O
0.573 B-MetricValue
) O
. O

Attention B-MethodName
- I-MethodName
based I-MethodName
keyword I-MethodName
search I-MethodName
( O
Attn B-MethodName
- I-MethodName
KWS I-MethodName
) O
This O
model O
is O
adapted O
from O
( O
Tamer O
and O
Saraçlar O
, O
2020b O
) O
's O
approach O
for O
keyword O
search O
in O
sign O
language O
. O

The O
model O
employs O
an O
attention O
mechanism O
to O
match O
a O
text O
query O
with O
a O
video O
clip O
, O
where O
each O
frame O
is O
weighted O
based O
on O
the O
query O
embedding O
. O

The O
attention O
mechanism O
enables O
the O
model O
to O
implicitly O
localize O
frames O
relevant O
to O
the O
text O
. O

The O
model O
of O
( O
Tamer O
and O
Saraçlar O
, O
2020b O
) O
is O
designed O
for O
lexical O
signs O
rather O
than O
fingerspelling O
. O

To O
adapt O
the O
model O
to O
our O
open O
- O
vocabulary O
fingerspelling O
setting O
, O
we O
use O
the O
same O
text O
encoder O
as O
in O
FSS B-MethodName
- I-MethodName
Net I-MethodName
to O
map O
words O
into O
embeddings O
instead O
of O
using O
a O
word O
embedding O
matrix O
as O
in O
( O
Tamer O
and O
Saraçlar O
, O
2020b O
) O
. O

Fingerspelling O
boundary O
information O
is O
again O
not O
used O
in O
training O
this O
model O
, O
which O
arguably O
puts O
it O
at O
a O
disadvantage O
compared O
to O
FSS B-MethodName
- I-MethodName
Net I-MethodName
. O

More O
details O
on O
the O
formulation O
of O
the O
model O
can O
be O
found O
in O
the O
Appendix O
. O

For O
FWS B-TaskName
, O
we O
use O
all O
words O
in O
the O
test O
set O
as O
the O
test O
vocabulary O
w O
1 O
: O
n O
. O

For O
FVS B-TaskName
, O
all O
video O
clips O
in O
the O
test O
are O
used O
as O
candidates O
and O
the O
text O
queries O
are O
again O
the O
entire O
test O
vocabulary O
. O

We O
report O
the O
results O
in O
terms O
of O
standard O
metrics O
from O
the O
video O
- O
text O
retrieval O
literature O
Tamer O
and O
Saraçlar O
, O
2020a O
): O
mean B-MetricName
Average I-MetricName
Precision I-MetricName
( B-MetricName
mAP I-MetricName
) O
and O
mean B-MetricName
F1 I-MetricName
score O
( O
mF1 B-MetricName
) O
, O
where O
the O
averages O
are O
over O
words O
for O
FVS B-TaskName
and O
over O
videos O
for O
FWS B-TaskName
. O

Hyperparameters O
are O
chosen O
to O
maximize O
the O
mAP B-MetricName
on O
the O
dev O
set O
, O
independently O
for O
the O
two O
tasks O
( O
though O
ultimately O
, O
the O
best O
hyperparameter O
values O
in O
our O
search O
are O
identical O
for O
both O
tasks O
) O
. O

Additional O
details O
on O
data O
, O
preprocessing O
, O
model O
implementation O
, O
and O
hyperparameters O
can O
be O
found O
in O
the O
Appendix O
. O

External B-MethodName
detector I-MethodName
( O
Ext B-MethodName
- I-MethodName
Det I-MethodName
) O
This O
baseline O
uses O
the O
off O
- O
the O
- O
shelf O
fingerspelling O
detectors O
of O
( O
Shi O
et O
al O
. O
, O
2021 O
) O
to O
generate O
fingerspelling O
proposals O
, O
instead O
of O
our O
proposal O
generator O
, O
and O
is O
otherwise O
identical O
to O
FSS B-MethodName
- I-MethodName
Net I-MethodName
. O

For O
each O
dataset O
( O
ChicagoF B-DatasetName
- I-DatasetName
SWild I-DatasetName
, O
ChicagoFSWild+ B-DatasetName
) O
, O
we O
use O
the O
detector O
trained O
on O
the O
training O
subset O
of O
that O
dataset O
. O

This O
baseline O
uses O
ground O
- O
truth O
fingerspelling O
boundaries O
for O
the O
detector O
training O
. O

where O
d O
is O
the O
cosine O
distance O
as O
in O
FSS B-MethodName
- I-MethodName
Net I-MethodName
. O

Fingerspelling O
boundary O
information O
is O
again O
not O
used O
in O
this O
baseline O
. O

Whole B-MethodName
- I-MethodName
clip I-MethodName
The O
whole B-MethodName
- I-MethodName
clip I-MethodName
baseline O
encodes O
the O
whole O
video O
clip O
I O
1 O
: O
T O
into O
a O
visual O
embedding O
e O
I O
v O
, O
which O
is O
matched O
to O
the O
textual O
embedding O
e O
w O
x O
of O
the O
query O
w. O
The O
model O
is O
trained O
with O
contrastive O
loss O
as O
in O
equation O
2 O
. O

At O
test O
time O
, O
the O
score O
for O
video O
clip O
I O
1 O
: O
T O
and O
word O
w O
is O
: O

Recognizer B-MethodName
In O
this O
approach O
, O
we O
train O
a O
recognizer O
that O
transcribes O
the O
video O
clip O
into O
text O
. O

Specifically O
, O
we O
train O
a O
recognizer B-MethodName
to O
output O
a O
sequence O
of O
symbols O
consisting O
of O
either O
fingerspelled O
letters O
or O
a O
special O
non O
- O
fingerspelling O
symbol O
< O
x O
> O
. O

We O
train O
the O
recognizer B-MethodName
with O
a O
connectionist O
temporal O
classification O
( O
CTC O
) O
loss O
( O
Graves O
et O
al O
. O
, O
2006 O
) O
, O
which O
is O
commonly O
used O
for O
speech B-TaskName
recognition I-TaskName
. O

At O
test O
time O
, O
we O
use O
beam O
search O
to O
generate O
a O
list O
of O
hypothesesŵ O
1 O
: O
M O
for O
the O
target O
video O
clip O
I O
1 O
: O
T O
. O

Each O
hypothesisŵ O
m O
is O
split O
into O
a O
list O
of O
words O
{ O
ŵ O
n O
m O
} O
1≤n≤N O
separated O
by O
< O
x O
> O
. O

The O
matching O
score O
between O
video O
I O
1 O
: O
T O
and O
w O
is O
defined O
as O
: O

where O
the O
letter O
error O
rate O
LER O
is O
the O
Levenshtein O
edit O
distance O
. O

This O
approach O
is O
adapted O
from O
( O
Saraçlar O
and O
Sproat O
, O
2004 O
) O
for O
spoken B-TaskName
utterance I-TaskName
retrieval I-TaskName
. O

Fingerspelling O
boundary O
information O
is O
not O
used O
in O
training O
this O
baseline O
model O
. O

5 O
Experimental O
SetupWe O
conduct O
experiments O
on O
ChicagoFSWild B-DatasetName
( O
Shi O
et O
al O
. O
, O
2018 O
) O
and O
ChicagoFSWild+ B-DatasetName
( O
Shi O
et O
al O
. O
, O
2019 O
) O
, O
two O
large O
- O
scale O
publicly O
available O
fingerspelling O
datasets O
containing O
7,304 O
and O
55,272 O
fingerspelling O
sequences O
respectively O
. O

The O
ASL O
videos O
in O
the O
two O
datasets O
are O
collected O
from O
online O
resources O
and O
include O
a O
variety O
of O
viewpoints O
and O
styles O
, O
such O
as O
webcam O
videos O
and O
lectures O
. O

We O
follow O
the O
setup O
of O
( O
Shi O
et O
al O
. O
, O
2021 O
) O
and O
split O
the O
raw O
ASL O
videos O
into O
300 O
- O
frame O
clips O
with O
a O
75frame O
overlap O
between O
neighboring O
chunks O
and O
remove O
clips O
without O
fingerspelling O
. O

The O
numbers O
of O
clips O
in O
the O
various O
splits O
can O
be O
found O
in O
the O
Appendix O
. O

On O
average O
, O
each O
clip O
contains O
1.9/1.8 O
fingerspelling O
segments O
in O
the O
ChicagoFSWild B-DatasetName
and O
ChicagoFSWild+ B-DatasetName
datasets O
respectively O
. O

We O
compare O
the O
proposed O
model O
, O
FSS B-MethodName
- I-MethodName
Net I-MethodName
, O
to O
the O
following O
baselines O
adapted O
from O
common O
approaches O
for O
search O
and O
retrieval O
in O
related O
fields O
. O

To O
facilitate O
comparison O
, O
the O
network O
architecture O
for O
the O
visual O
and O
text O
encoding O
in O
all O
baselines O
is O
the O
same O
as O
in O
FSS B-MethodName
- I-MethodName
Net I-MethodName
. O

In O
addition O
to O
introducing O
the O
task O
, O
we O
address O
the O
research O
question O
of O
whether O
the O
explicit O
temporal O
localization O
of O
fingerspelling O
can O
help O
its O
search O
and O
retrieval O
, O
and O
how O
best O
to O
localize O
it O
. O

As O
fingerspelling O
occurs O
sparsely O
in O
the O
signing O
stream O
, O
explicit O
detection O
of O
fingerspelling O
could O
potentially O
improve O
search O
performance O
by O
removing O
unrelated O
signs O
. O

To O
this O
end O
, O
we O
propose O
an O
end O
- O
to O
- O
end O
model O
, O
FSS B-MethodName
- I-MethodName
Net I-MethodName
, O
which O
jointly O
detects O
fingerspelling O
from O
unconstrained O
signing O
video O
and O
matches O
it O
to O
text O
queries O
. O

Our O
approach O
consistently O
outperforms O
a O
series O
of O
baselines O
without O
explicit O
detection O
and O
a O
baseline O
with O
an O
off O
- O
theshelf O
fingerspelling O
detector O
by O
a O
large O
margin O
. O

In O
existing O
work O
on O
sign B-TaskName
language I-TaskName
video I-TaskName
processing I-TaskName
, O
search O
and O
retrieval O
tasks O
have O
been O
studied O
much O
less O
than O
sign B-TaskName
language I-TaskName
recognition I-TaskName
( O
mapping O
from O
sign O
language O
video O
to O
gloss O
labels O
) O
( O
Koller O
et O
al O
. O
, O
2017;Forster O
et O
al O
. O
, O
2016 O
) O
and O
translation B-TaskName
( O
mapping O
from O
sign O
language O
video O
to O
text O
in O
another O
language O
) O
( O
Yin O
and O
Read O
, O
2020;Camgöz O
et O
al O
. O
, O
2018 O
) O
. O

Work O
thus O
far O
on O
sign O
language O
search O
has O
been O
framed O
mainly O
as O
the O
retrieval O
of O
lexical O
signs O
rather O
than O
fingerspelling O
. O

Pfister O
et O
al O
. O
( O
2013 O
) O
; O
employ O
mouthing O
to O
detect O
keywords O
in O
sign O
- O
interpreted O
TV O
programs O
with O
coarsely O
aligned O
subtitles O
. O

Tamer O
and O
Saraçlar O
( O
2020a O
, O
b O
) O
utilize O
whole O
- O
body O
pose O
estimation O
to O
search O
for O
sign O
language O
keywords O
( O
gloss O
or O
translated O
word O
) O
in O
a O
German B-TaskName
Sign I-TaskName
Language I-TaskName
translation I-TaskName
dataset O
PHOENIX-2014 B-DatasetName
T I-DatasetName
( O
Camgöz O
et O
al O
. O
, O
2018 O
) O
. O

All O
prior O
work O
on O
keyword O
search O
for O
sign O
language O
has O
been O
done O
in O
a O
closed O
- O
vocabulary O
setting O
, O
which O
assumes O
that O
only O
words O
from O
a O
pre O
- O
determined O
set O
will O
be O
queried O
. O

Searching O
in O
an O
open O
- O
vocabulary O
setting O
, O
including O
proper O
nouns O
, O
typically O
requires O
searching O
for O
fingerspelling O
. O

Some O
related O
tasks O
in O
the O
speech O
processing O
literature O
are O
spoken B-TaskName
term I-TaskName
detection I-TaskName
( O
STD O
) O
and O
queryby B-TaskName
- I-TaskName
example I-TaskName
search I-TaskName
, O
which O
are O
the O
tasks O
of O
automatically O
retrieving O
speech O
segments O
from O
a O
database O
that O
match O
a O
given O
text O
or O
audio O
query O
( O
Knill O
et O
al O
. O
, O
2013;Mamou O
et O
al O
. O
, O
2007;Chen O
et O
al O
. O
, O
2015 O
) O
. O

In O
terms O
of O
methodology O
, O
our O
model O
also O
shares O
some O
aspects O
with O
prior O
work O
on O
moment O
retrieval O
( O
Gao O
et O
al O
. O
, O
2017;Xu O
et O
al O
. O
, O
2019;Zhang O
et O
al O
. O
, O
2020 O
) O
, O
which O
also O
combines O
candidate O
generation O
and O
matching O
components O
. O

However O
, O
we O
incorporate O
additional O
task O
- O
specific O
elements O
that O
consistently O
improve O
performance O
. O

We O
consider O
two O
tasks O
: O
Fingerspelled B-TaskName
Word I-TaskName
Search I-TaskName
( O
FWS B-TaskName
) O
and O
Fingerspelling B-TaskName
- I-TaskName
based I-TaskName
Video I-TaskName
Search I-TaskName
( O
FVS B-TaskName
) O
. O

FWS B-TaskName
and O
FVS B-TaskName
respectively O
consist O
of O
detecting O
fingerspelled O
words O
within O
a O
given O
raw O
ASL O
video O
stream O
and O
detecting O
video O
clips O
of O
interest O
containing O
a O
given O
fingerspelled O
word O
. O

2 O
Given O
a O
query O
video O
clip O
v O
and O
a O
list O
of O
n O
words O
w O
1 O
: O
n O
, O
FWS B-TaskName
is O
the O
task O
of O
finding O
which O
( O
if O
any O
) O
of O
w O
1 O
: O
n O
are O
present O
in O
v. O
Conversely O
, O
in O
FVS B-TaskName
the O
input O
is O
a O
query O
word O
w O
and O
n O
video O
clips O
v O
1 O
: O
n O
, O
and O
the O
task O
consists O
of O
finding O
all O
videos O
containing O
the O
fingerspelled O
word O
w. O
We O
consider O
an O
openvocabulary O
setting O
where O
the O
word O
w O
is O
not O
constrained O
to O
a O
pre O
- O
determined O
set O
. O

The O
two O
tasks O
correspond O
to O
two O
directions O
of O
search O
( O
video−→text O
and O
text−→video O
) O
, O
as O
is O
standard O
practice O
in O
other O
retrieval O
work O
such O
as O
video O
- O
text O
search O
( O
Zhang O
et O
al O
. O
, O
2018;Ranjay O
et O
al O
. O
, O
2017;Ging O
et O
al O
. O
, O
2020).We O
propose O
a O
single O
model O
, O
FSS B-MethodName
- I-MethodName
Net I-MethodName
( O
for O
" O
Finger B-MethodName
- I-MethodName
Spelling I-MethodName
Search I-MethodName
Network I-MethodName
" O
) O
, O
summarized O
in O
Fig- O
Image O
encoding O
The O
input O
image O
frames O
are O
encoded O
into O
a O
sequence O
of O
feature O
vectors O
via O
an O
image O
encoder O
, O
which O
consists O
of O
the O
VGG-19 O
( O
Simonyan O
and O
Zisserman O
, O
2015 O
) O
convolutional O
layers O
followed O
by O
a O
Bi O
- O
LSTM O
. O

3 O
We O
use O
raw O
RGB O
images O
as O
input O
, O
instead O
of O
signer O
pose O
as O
used O
in O
some O
prior O
work O
( O
Tamer O
and O
Saraçlar O
, O
2020b O
, O
a O
) O
on O
sign O
language O
search O
, O
as O
estimating O
pose O
for O
hands O
is O
particularly O
hard O
for O
signing O
videos O
in O
the O
wild O
( O
see O
Section O
6 O
for O
details O
) O
. O

Automatic B-TaskName
sign I-TaskName
language I-TaskName
processing I-TaskName
has O
recently O
received O
growing O
interest O
in O
the O
computer O
vision O
( O
CV O
) O
and O
natural O
language O
processing O
( O
NLP O
) O
communities O
. O

Yin O
et O
al O
. O
( O
2021 O
) O
make O
several O
recommendations O
for O
the O
study O
of O
sign O
languages O
in O
NLP O
research O
, O
including O
greater O
emphasis O
on O
real O
- O
world O
data O
. O

Most O
studies O
on O
sign O
language O
are O
based O
on O
data O
collected O
in O
a O
controlled O
environment O
, O
either O
1 O
From O
https://wfdeaf.org/our-work/ O
in O
a O
studio O
setting O
( O
Martínez O
et O
al O
. O
, O
2002;Kim O
et O
al O
. O
, O
2017 O
) O
or O
in O
a O
specific O
domain O
( O
Forster O
et O
al O
. O
, O
2016 O
) O
. O

The O
challenges O
involved O
in O
real O
- O
world O
signing O
videos O
, O
including O
various O
visual O
conditions O
and O
different O
levels O
of O
fluency O
in O
signing O
, O
are O
not O
fully O
reflected O
in O
such O
datasets O
. O

Automatic O
processing O
of O
sign O
language O
videos O
" O
in O
the O
wild O
" O
has O
not O
been O
addressed O
until O
recently O
, O
and O
is O
still O
restricted O
to O
tasks O
like O
isolated B-TaskName
sign I-TaskName
recognition I-TaskName
Joze O
and O
Koller O
, O
2019;Li O
et O
al O
. O
, O
2020 O
) O
and O
fingerspelling B-TaskName
recognition I-TaskName
( O
Shi O
et O
al O
. O
, O
2018(Shi O
et O
al O
. O
, O
, O
2019 O
. O

In O
this O
work O
we O
take O
a O
step O
further O
and O
study O
search O
and O
retrieval O
of O
arbitrary O
fingerspelled O
content O
in O
real O
- O
world O
American O
Sign O
Language O
( O
ASL O
) O
video O
( O
see O
Figure O
1 O
) O
. O

fingerspelling O
video O
search O
( O
FVS O
) O
for O
searching O
for O
sign O
language O
videos O
that O
include O
a O
fingerspelled O
query O
word O
/ O
phrase O
. O

The O
sign O
language O
videos O
are O
untrimmed O
, O
i.e. O
they O
include O
regular O
signs O
in O
addition O
to O
fingerspelling O
, O
and O
are O
downsampled O
here O
for O
visualization O
. O

In O
activation O
boundary O
distillation O
, O
we O
perform O
a O
grid O
search O
to O
determine O
the O
number B-HyperparameterName
of I-HyperparameterName
epochs I-HyperparameterName
( O
e B-HyperparameterName
1 I-HyperparameterName
) O
and O
learning B-HyperparameterName
rate I-HyperparameterName
( O
lr B-HyperparameterName
1 I-HyperparameterName
) O
for O
initial O
student O
finetuning O
. O

Then O
, O
we O
conduct O
another O
grid O
search O
of O
the O
learning B-HyperparameterName
rate I-HyperparameterName
( O
lr B-HyperparameterName
2 I-HyperparameterName
) O
, O
number B-HyperparameterName
of I-HyperparameterName
epochs I-HyperparameterName
( B-HyperparameterName
e I-HyperparameterName
2 I-HyperparameterName
) O
, O
weight B-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
confidence I-HyperparameterName
penalty I-HyperparameterName
( O
β B-HyperparameterName
2 I-HyperparameterName
) O
, O
and O
loss B-HyperparameterName
switch I-HyperparameterName
rate I-HyperparameterName
( O
γ B-HyperparameterName
) O
for O
the O
distillation O
and O
refinement O
steps O
. O

Both O
searches O
are O
performed O
on O
the O
development O
set O
. O

The O
actual O
values O
of O
the O
hyperparameters O
for O
the O
ALBERT B-MethodName
student O
are O
summarized O
in O
Table O
A2 O
. O

For O
the O
RoBERTa B-MethodName
model O
as O
a O
student O
, O
we O
use O
the O
same O
teacher O
with O
ALBERT B-MethodName
. O

The O
hyperparameters O
of O
the O
activation O
boundary O
distillation O
for O
the O
RoBERTa B-MethodName
student O
are O
searched O
in O
the O
same O
manner O
with O
the O
ALBERT B-MethodName
and O
summarized O
in O
Table O
A3.In O
this O
section O
, O
we O
report O
on O
the O
details O
of O
two O
financial O
downstream O
task O
datasets O
, O
the O
experimental O
details O
, O
and O
hyperparameters O
of O
the O
financial O
task O
experiments O
. O

The O
actual O
values O
of O
the O
hyperparameters O
for O
the O
calibrated O
teacher O
training O
are O
summarized O
in O
Table O
A1 O
. O

In O
calibrated O
teacher O
training O
, O
we O
first O
select O
the O
number B-HyperparameterName
of I-HyperparameterName
epochs I-HyperparameterName
and O
the O
learning B-HyperparameterName
rate I-HyperparameterName
as O
the O
default O
values O
of O
the O
BioBERT B-MethodName
code O
and O
slightly O
change O
the O
number B-HyperparameterName
of I-HyperparameterName
epochs I-HyperparameterName
( O
e B-HyperparameterName
) O
for O
the O
unreported O
tasks O
from O
BioBERT B-MethodName
. O

Then O
, O
we O
select O
the O
strength O
of O
the O
confidence B-HyperparameterName
regularization I-HyperparameterName
( O
β B-HyperparameterName
1 I-HyperparameterName
) O
by O
a O
grid O
search O
in O
terms O
of O
the O
F1 B-MetricName
score O
and O
expected B-MetricName
calibration I-MetricName
error I-MetricName
( O
ECE B-MetricName
) O
on O
the O
development O
set O
. O

The O
formula O
for O
calculating O
ECE B-MetricName
is O
as O
follows O
: O

As O
shown O
in O
Table O
7 O
, O
ALBERT B-MethodName
- I-MethodName
DoKTRa I-MethodName
and O
RoBERTa B-MethodName
- I-MethodName
DoKTRa I-MethodName
outperformed O
the O
FinBERT B-MethodName
- I-MethodName
ft I-MethodName
teacher O
on O
financial O
downstream O
tasks O
. O

Note O
that O
we O
used O
the O
RoBERTa B-MethodName
- I-MethodName
base I-MethodName
model O
in O
this O
section O
because O
of O
the O
training O
stability O
. O

This O
result O
suggests O
that O
DoKTra B-MethodName
can O
be O
applied O
regardless O
of O
the O
domain O
and O
can O
be O
an O
efficient O
alternative O
to O
in O
- O
domain O
pre O
- O
training O
. O

In O
this O
study O
, O
we O
proposed O
the O
DoKTra B-MethodName
framework O
as O
a O
domain O
knowledge O
transfer O
method O
for O
PLMs O
. O

The O
experimental O
results O
from O
the O
biomedical O
, O
clinical O
, O
and O
financial O
domain O
downstream O
tasks O
demonstrated O
that O
our O
proposed O
framework O
could O
transfer O
domain O
- O
specific O
knowledge O
into O
a O
PLM O
, O
while O
preserving O
its O
own O
expressive O
advantages O
without O
any O
further O
pre O
- O
training O
with O
additional O
in O
- O
domain O
data O
. O

We O
employed O
advanced O
models O
as O
the O
student O
model O
and O
verified O
the O
future O
applicability O
of O
our O
framework O
to O
emerging O
language O
models O
by O
achieving O
even O
higher O
performances O
than O
the O
teacher O
model O
. O

However O
, O
the O
limitations O
of O
our O
approach O
are O
that O
it O
is O
task O
- O
specific O
and O
was O
evaluated O
only O
in O
classification O
tasks O
. O

Our O
future O
studies O
would O
focus O
on O
developing O
the O
proposed O
framework O
as O
a O
task O
- O
agnostic O
method O
and O
evaluating O
it O
on O
various O
tasks O
. O

In O
this O
section O
, O
we O
report O
the O
searching O
scheme O
and O
actual O
values O
of O
the O
hyperparameters O
used O
by O
us O
. O

In O
all O
cases O
, O
we O
set O
the O
batch B-HyperparameterName
size I-HyperparameterName
to O
the O
maximum O
that O
a O
single O
GPU O
can O
process O
, O
with O
128 B-HyperparameterValue
being O
the O
maximum B-HyperparameterName
sequence I-HyperparameterName
length I-HyperparameterName
. O

In O
this O
study O
, O
we O
selected O
the O
FinBERT B-MethodName
( O
Yang O
et O
al O
. O
, O
2020 O
) O
model O
as O
a O
teacher O
in O
the O
DoKTra B-MethodName
framework O
and O
evaluated O
our O
approach O
on O
two O
tasks O
, O
the O
Financial B-DatasetName
PhraseBank I-DatasetName
( O
FPB B-DatasetName
) O
and O
Fin B-DatasetName
- I-DatasetName
TextSen I-DatasetName
( O
FTS B-DatasetName
) O
. O

The O
Financial B-DatasetName
PhraseBank I-DatasetName
( O
FPB B-DatasetName
) O
( O
Malo O
et O
al O
. O
, O
2014 O
) O
contains O
sentences O
from O
financial O
news O
annotated O
for O
positive O
, O
neutral O
, O
and O
negative O
sentiments O
. O

The O
FinTextSen B-DatasetName
( O
FTS B-DatasetName
) O
( O
Cortis O
et O
al O
. O
, O
2017 O
) O
consists O
of O
financial O
tweets O
from O
Twitter O
and O
StockTwits O
with O
real O
- O
valued O
sentiment O
scores O
. O

To O
transform O
it O
into O
a O
classification O
task O
, O
we O
clustered O
the O
sentiment O
score O
into O
a O
3 O
- O
class O
label O
, O
following O
Daudert O
et O
al O
. O
( O
2018 O
) O
. O

The O
Financial B-DatasetName
PhraseBank I-DatasetName
dataset O
contains O
4,846 O
sentences O
, O
and O
we O
set O
10 O
% O
of O
the O
examples O
as O
the O
test O
set O
while O
preserving O
the O
label O
distribution O
. O

The O
FinTextSen B-DatasetName
originally O
includes O
2,488 O
tweets O
, O
but O
only O
1,700 O
tweets O
are O
available O
now O
. O

We O
set O
10 O
% O
of O
the O
entire O
data O
as O
the O
test O
set O
, O
which O
is O
similar O
to O
FPB B-DatasetName
. O

Table O
5 O
shows O
the O
experimental O
results O
on O
four O
relation O
extraction O
tasks O
with O
ALBERT B-MethodName
students O
. O

As O
shown O
in O
Table O
5 O
, O
the O
application O
of O
the O
calibrated O
teacher O
training O
reduces O
the O
L O
AT O
and O
improves O
the O
classification O
performance O
. O

In O
other O
words O
, O
calibration O
on O
the O
teacher O
training O
clearly O
aids O
the O
supervision O
of O
the O
teacher O
in O
activation O
boundary O
distillation O
, O
even O
though O
the O
output O
probability O
information O
is O
not O
directly O
used O
in O
distillation O
. O

To O
observe O
how O
each O
component O
contributed O
to O
the O
proposed O
framework O
, O
we O
conducted O
an O
ablation O
study O
. O

We O
ablated O
two O
major O
components O
: O
calibrated O
teacher O
training O
( O
CTT O
) O
and O
activation O
boundary O
distillation O
( O
ABD O
) O
. O

The O
experiments O
were O
performed O
on O
the O
ChemProt B-DatasetName
dataset O
, O
using O
the O
ALBERT B-MethodName
- I-MethodName
xlarge I-MethodName
model O
as O
the O
student O
architecture O
. O

To O
ablate O
the O
calibrated O
teacher O
training O
, O
we O
trained O
the O
teacher O
model O
using O
only O
L O
CE O
. O

We O
compared O
the O
activation O
boundary O
distillation O
with O
KL O
- O
divergence O
based O
distillation O
( O
KLD O
) O
, O
which O
penalizes O
the O
difference O
between O
the O
output O
probability O
distributions O
of O
the O
two O
models O
. O

Table O
6 O
presents O
the O
results O
of O
the O
ablation O
study O
. O

As O
we O
proposed O
, O
applying O
both O
calibrated O
teacher O
training O
and O
activation O
boundary O
distillation O
resulted O
in O
a O
superior O
performance O
. O

In O
particular O
, O
the O
calibrated O
teacher O
model O
was O
able O
to O
distil O
its O
activation O
boundary O
to O
the O
student O
model O
much O
more O
effectively O
, O
thus O
improving O
the O
performance O
of O
the O
student O
model O
, O
as O
we O
hypothesized O
in O
the O
previous O
section O
. O

Applying O
KL O
- O
divergence O
- O
based O
distillation O
yielded O
positive O
results O
in O
terms O
of O
classification O
performance O
. O

Notably O
, O
calibrated O
teacher O
training O
also O
improved O
the O
KL O
- O
divergence O
- O
based O
distillation O
because O
it O
enabled O
the O
distillation O
of O
a O
considerably O
more O
reliable O
output O
probability O
, O
as O
reported O
in O
Menon O
et O
al O
. O
( O
2021 O
) O
. O

Note O
that O
applying O
the O
confidence O
regularizer O
to O
the O
fine O
- O
tuning O
of O
the O
student O
model O
only O
slightly O
improved O
the O
performance O
, O
suggesting O
that O
the O
observed O
gains O
in O
our O
model O
are O
only O
partially O
because O
of O
the O
calibration O
regularizer O
. O

To O
verify O
the O
general O
applicability O
of O
our O
approach O
, O
we O
conducted O
experiments O
on O
financial B-TaskName
sentiment I-TaskName
classification I-TaskName
tasks O
. O

( O
Araci O
, O
2019;Yang O
et O
al O
. O
, O
2020;Liu O
et O
al O
. O
, O
2021 O
) O
to O
fill O
the O
gap O
between O
the O
general O
and O
financial O
domains O
. O

Irrespective O
of O
the O
use O
of O
an O
alternative O
version O
( O
Equation O
5 O
) O
during O
the O
training O
, O
the O
extent O
to O
which O
the O
activation O
pattern O
is O
distilled O
can O
be O
intuitively O
observed O
by O
calculating O
the O
original O
" O
activation O
transfer O
loss O
" O
( O
Equation O
4 O
) O
. O

The O
value O
of O
Equation O
4 O
directly O
refers O
to O
the O
number O
of O
neurons O
activated O
differently O
than O
the O
teacher O
model O
. O

For O
instance O
, O
if O
L O
AT O
= O
500 O
for O
an O
ALBERT B-MethodName
model O
( O
H=2,048 B-HyperparameterName
) O
, O
it O
indicates O
that O
500 O
of O
the O
2,048 O
elements O
in O
the O
hidden O
representation O
vector O
exhibited O
signs O
different O
to O
those O
of O
the O
teacher O
. O

The O
comparison O
results O
are O
shown O
in O
Table O
4 O
. O

Note O
that O
the O
performance O
on O
GAD B-DatasetName
in O
Table O
4 O
was O
evaluated O
with O
the O
first O
split O
of O
a O
10 B-HyperparameterValue
- O
fold O
crossvalidation O
, O
while O
the O
main O
result O
in O
Table O
3 O
was O
evaluated O
with O
all O
splits O
. O

As O
revealed O
in O
the O
results O
, O
even O
though O
TAPT B-MethodName
showed O
improved O
results O
in O
the O
original O
study O
with O
Google O
Cloud O
TPU O
, O
it O
was O
unstable O
with O
the O
small O
batch B-HyperparameterName
size I-HyperparameterName
and O
sequence B-HyperparameterName
length I-HyperparameterName
; O
the O
performances O
were O
even O
degraded O
in O
the O
general O
GPU O
environment O
. O

Although O
the O
TAPT B-MethodName
performance O
improved O
when O
the O
batch B-HyperparameterName
size I-HyperparameterName
increased O
through O
distributed O
training O
, O
the O
improvement O
was O
inadequate O
. O

This O
may O
be O
because O
of O
the O
batch B-HyperparameterName
size I-HyperparameterName
being O
smaller O
than O
that O
in O
the O
TPU O
environment O
. O

Moreover O
, O
DoKTra B-MethodName
required O
less O
training O
time O
than O
TAPT B-MethodName
while O
both O
methods O
were O
task O
- O
specific O
. O

For O
instance O
, O
TAPT B-MethodName
required O
a O
total O
of O
seven O
hours O
of O
training O
, O
while O
DoKTRa B-MethodName
was O
completed O
in O
only O
1.1 O
hours O
for O
the O
ChemProt B-DatasetName
task O
. O

This O
is O
because O
DoKTra B-MethodName
leverages O
the O
knowledge O
of O
an O
existing O
indomain O
PLM O
, O
thus O
requiring O
only O
a O
few O
fine O
- O
tuning O
and O
distillation O
steps O
. O

The O
comparison O
of O
TAPT B-MethodName
and O
DoKTra B-MethodName
using O
more O
advanced O
computing O
resources O
is O
left O
as O
a O
future O
work O
. O

We O
conducted O
an O
experiment O
to O
verify O
the O
positive O
effect O
of O
combining O
calibrated O
teacher O
training O
and O
activation O
boundary O
distillation O
. O

Because O
the O
entropy O
regularizer O
in O
calibrated O
teacher O
training O
issues O
penalties O
based O
on O
the O
output O
probability O
distribution O
, O
it O
is O
difficult O
to O
intuitively O
understand O
how O
it O
positively O
affects O
activation O
boundary O
distillation O
, O
which O
uses O
hidden O
representation O
. O

Thus O
, O
we O
ablate O
the O
calibrated O
teacher O
training O
steps O
in O
our O
framework O
and O
compare O
the O
final O
performances O
and O
loss O
values O
. O

Since O
the O
results O
of O
the O
RoBERTa B-MethodName
- I-MethodName
large I-MethodName
model O
with O
a O
small O
batch B-HyperparameterName
size I-HyperparameterName
were O
unstable O
, O
we O
also O
performed O
a O
distributed O
training O
with O
three O
GPUs O
, O
resulting O
in O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
108 B-HyperparameterValue
. O

We O
also O
compared O
our O
framework O
with O
taskadaptive B-MethodName
pre I-MethodName
- I-MethodName
training I-MethodName
( O
TAPT B-MethodName
) O
( O
Gururangan O
et O
al O
. O
, O
2020 O
) O
, O
an O
additional O
pre O
- O
training O
method O
for O
PLMs O
. O

The O
TAPT B-MethodName
approach O
additionally O
pre O
- O
trains O
an O
existing O
PLM O
before O
fine O
- O
tuning O
it O
with O
the O
training O
samples O
of O
each O
task O
. O

As O
both O
TAPT B-MethodName
and O
DoK B-MethodName
- I-MethodName
Tra I-MethodName
only O
utilize O
the O
task O
- O
specific O
training O
data O
, O
they O
can O
be O
fairly O
compared O
in O
terms O
of O
performance O
and O
training O
resources O
. O

For O
TAPT B-MethodName
, O
we O
additionally O
pre O
- O
trained O
the O
RoBERTa B-MethodName
- I-MethodName
large I-MethodName
model O
with O
each O
pre O
- O
processed O
downstream O
task O
's O
training O
data O
. O

We O
followed O
the O
hyperparameters O
used O
in O
TAPT B-MethodName
except O
for O
batch B-HyperparameterName
size I-HyperparameterName
and O
the O
maximum B-HyperparameterName
sequence I-HyperparameterName
length I-HyperparameterName
because O
we O
used O
the O
same O
computing O
resource O
as O
DoKTra B-MethodName
for O
a O
fair O
comparison O
. O

The O
possible O
maximum O
pre O
- O
training O
batch B-HyperparameterName
size I-HyperparameterName
with O
the O
given O
computing O
resource O
for O
the O
RoBERTa B-MethodName
- I-MethodName
large I-MethodName
model O
was O
36 B-HyperparameterValue
. O

Table O
3 O
shows O
the O
classification O
performance O
of O
BioBERT B-MethodName
, O
RoBERTa B-MethodName
- I-MethodName
PM I-MethodName
, O
and O
our O
approach O
in O
five O
biomedical O
and O
clinical O
tasks O
. O

As O
mentioned O
before O
, O
our O
best O
model O
outperformed O
the O
BioBERT B-MethodName
( O
teacher O
) O
model O
on O
four O
of O
the O
five O
tasks O
. O

Notably O
, O
our O
approach O
even O
outperformed O
RoBERTa B-MethodName
- I-MethodName
PM I-MethodName
on O
two O
tasks O
and O
demonstrated O
comparable O
performances O
on O
the O
others O
. O

These O
results O
are O
remarkable O
since O
our O
approach O
spent O
only O
a O
few O
hours O
on O
each O
task O
, O
whereas O
RoBERTa B-MethodName
- I-MethodName
PM I-MethodName
may O
require O
several O
days O
and O
billions O
of O
words O
to O
be O
pre O
- O
trained O
. O

Note O
that O
RoBERTa B-MethodName
- I-MethodName
PM I-MethodName
has O
an O
advantage O
in O
the O
i2b2 B-DatasetName
task O
since O
its O
pre O
- O
training O
data O
contains O
MIMIC B-DatasetName
- I-DatasetName
III I-DatasetName
clinical O
text O
data O
, O
while O
our O
teacher O
model O
was O
pretrained O
with O
only O
biomedical O
texts O
. O

In O
other O
words O
, O
this O
implies O
our O
approach O
has O
a O
room O
for O
further O
improvement O
when O
a O
better O
in O
- O
domain O
model O
is O
set O
as O
a O
teacher O
. O

The O
RoBERTa B-MethodName
model O
that O
was O
applied O
to O
the O
proposed O
framework O
outperformed O
the O
teacher O
model O
on O
an O
average O
, O
specifically O
in O
four O
of O
five O
downstream O
tasks O
( O
ChemProt B-DatasetName
, O
DDI B-DatasetName
, O
i2b2 B-DatasetName
, O
and O
HoC B-DatasetName
) O
. O

RoBERTa B-MethodName
's O
performance O
was O
already O
similar O
to O
the O
teacher O
model O
in O
the O
initial O
fine O
- O
tuning O
stage O
because O
it O
was O
pre O
- O
trained O
with O
more O
data O
than O
BERT B-MethodName
and O
exhibited O
a O
greater O
robustness O
. O

The O
results O
on O
RoBERTa B-MethodName
imply O
that O
our O
proposed O
framework O
can O
be O
effectively O
applied O
to O
emerging O
and O
advanced O
pre O
- O
trained O
language O
models O
. O

In O
other O
words O
, O
domain O
- O
specific O
knowledge O
can O
be O
transferred O
into O
advanced O
models O
without O
a O
time O
- O
consuming O
pre- O
training O
and O
perturbing O
the O
model O
's O
efficacy O
in O
the O
general O
domain O
. O

To O
compare O
our O
approach O
with O
the O
in O
- O
domain O
pre O
- O
training O
method O
, O
we O
used O
RoBERTa B-MethodName
- I-MethodName
PM I-MethodName
- I-MethodName
large I-MethodName
( O
Lewis O
et O
al O
. O
, O
2020 O
) O
, O
which O
is O
a O
RoBERTa B-MethodName
- I-MethodName
large I-MethodName
model O
additionally O
pre O
- O
trained O
with O
a O
large O
biomedical O
and O
clinical O
corpus O
consisting O
of O
14 O
billion O
words O
. O

We O
fine O
- O
tuned O
the O
RoBERTa B-MethodName
- I-MethodName
PM I-MethodName
for O
each O
task O
. O

By O
applying O
the O
DoKTra B-MethodName
framework O
, O
the O
ALBERT B-MethodName
- I-MethodName
xlarge I-MethodName
student O
model O
was O
able O
to O
retain O
99.72 B-MetricValue
% I-MetricValue
of O
the O
teacher O
model O
performance O
on O
an O
average O
. O

ALBERT B-MethodName
has O
two O
advantages O
: O
a O
small O
number O
of O
parameters O
and O
high O
performance O
( O
Lan O
et O
al O
. O
, O
2019 O
) O
. O

Applying O
our O
framework O
to O
ALBERT B-MethodName
allowed O
us O
to O
obtain O
a O
student O
model O
with O
performance O
comparable O
to O
that O
of O
the O
teacher O
with O
half O
the O
parameters O
. O

In O
other O
words O
, O
we O
successfully O
transferred O
domain O
- O
specific O
knowledge O
to O
AL B-MethodName
- I-MethodName
BERT I-MethodName
while O
maintaining O
its O
existing O
advantages O
. O

Consequently O
, O
the O
distilled O
ALBERT B-MethodName
achieved O
a O
higher O
performance O
than O
the O
teacher O
model O
on O
ChemProt B-DatasetName
and O
DDI B-DatasetName
. O

The O
experiments O
were O
run O
on O
a O
single O
RTX O
3090 O
24 O
GB O
GPU O
, O
and O
the O
training O
codes O
were O
implemented O
in O
PyTorch O
. O

All O
experiments O
were O
repeated O
three B-HyperparameterValue
times O
with O
different O
random O
seeds O
, O
and O
the O
average O
performances O
and O
standard O
deviations O
have O
been O
reported O
. O

Table O
2 O
shows O
the O
overall O
experimental O
F1 B-MetricName
score O
results O
of O
the O
DoKTra B-MethodName
framework O
on O
five O
biomedical B-TaskName
and O
clinical B-TaskName
classification I-TaskName
tasks O
. O

The O
initially O
fine O
- O
tuned O
student O
models O
are O
in O
the O
second O
and O
fourth O
rows O
and O
the O
DoKTra B-MethodName
framework O
is O
applied O
to O
both O
, O
as O
shown O
in O
the O
third O
and O
fifth O
rows O
. O

We O
pre O
- O
process O
every O
classification O
dataset O
except O
for O
GAD B-DatasetName
in O
the O
same O
manner O
as O
the O
BLUE B-MethodName
( O
Peng O
et O
al O
. O
, O
2019 O
) O
benchmark O
. O

In O
particular O
, O
entity O
anonymization O
is O
applied O
to O
all O
relation O
extraction O
datasets O
, O
which O
replace O
the O
entity O
mentions O
with O
anonymous O
tokens O
( O
e.g. O
, O
@GENE$ O
, O
@DISEASE$ O
) O
to O
avoid O
confusion O
in O
using O
complex O
entity O
names O
. O

We O
use O
a O
pre O
- O
processed O
version O
of O
the O
GAD B-DatasetName
dataset O
provided O
by O
BioBERT B-MethodName
, O
which O
is O
split O
for O
10 O
- O
fold O
cross O
- O
validation O
. O

The O
statistics O
of O
the O
pre O
- O
processed O
downstream O
task O
datasets O
are O
listed O
in O
For O
the O
experiments O
, O
we O
used O
the O
pre O
- O
trained O
BioBERT B-MethodName
- I-MethodName
base I-MethodName
model O
( O
L=12 B-HyperparameterName
, O
H=768 B-HyperparameterName
, O
A=12 B-HyperparameterName
) O
as O
the O
initial O
teacher O
model O
. O

We O
used O
two O
pre O
- O
trained O
models O
as O
the O
initial O
student O
model O
: O
ALBERT B-MethodName
- I-MethodName
xlarge I-MethodName
( O
L=24 B-HyperparameterName
, O
H=2048 B-HyperparameterName
, O
A=32 B-HyperparameterName
) O
and O
RoBERTa B-MethodName
- I-MethodName
large I-MethodName
( O
L=24 B-HyperparameterName
, O
H=1024 B-HyperparameterName
, O
A=16 B-HyperparameterName
) O
. O

In O
the O
previous O
description O
, O
we O
have O
assumed O
that O
the O
embedding B-HyperparameterName
dimensions I-HyperparameterName
of O
teachers O
and O
students O
are O
identical O
. O

However O
, O
because O
the O
hidden B-HyperparameterName
embedding I-HyperparameterName
dimensions I-HyperparameterName
of O
teachers O
and O
students O
are O
different O
in O
our O
setting O
, O
we O
applied O
a O
linear O
transformation O
to O
the O
teacher O
's O
classification O
embedding O
to O
match O
the O
dimension O
with O
the O
student O
model O
. O

In O
calibrated O
teacher O
training O
, O
we O
trained O
for O
3 B-HyperparameterValue
- O
10 B-HyperparameterValue
epochs B-HyperparameterName
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
2e-5 B-HyperparameterValue
. O

The O
hyperparameter O
β B-HyperparameterName
1 I-HyperparameterName
, O
the O
strength B-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
confidence I-HyperparameterName
penalty I-HyperparameterName
in O
teacher O
training O
, O
was O
chosen O
from O
{ O
0 B-HyperparameterValue
, O
0.3 B-HyperparameterValue
, O
0.5 B-HyperparameterValue
, O
0.7 B-HyperparameterValue
} O
. O

For O
activation O
boundary O
distillation O
, O
we O
first O
fine O
- O
tuned O
the O
initial O
student O
model O
for O
5 B-HyperparameterValue
- O
10 B-HyperparameterValue
epochs B-HyperparameterName
with O
learning B-HyperparameterName
rates I-HyperparameterName
of O
{ O
6e-6 B-HyperparameterValue
, O
8e-6 B-HyperparameterValue
, O
1e-5 B-HyperparameterValue
} O
. O

Then O
, O
we O
distilled O
for O
10 B-HyperparameterValue
epochs B-HyperparameterName
with B-HyperparameterName
learning I-HyperparameterName
rates I-HyperparameterName
of O
{ O
6e-6 B-HyperparameterValue
, O
8e-6 B-HyperparameterValue
, O
1e-5 B-HyperparameterValue
} O
. O

The O
confidence B-HyperparameterName
penalty I-HyperparameterName
strength I-HyperparameterName
β B-HyperparameterName
2 I-HyperparameterName
in O
the O
refinement O
step O
and O
loss B-HyperparameterName
switch I-HyperparameterName
rate I-HyperparameterName
γ B-HyperparameterName
were O
chosen O
from O
{ O
0 B-HyperparameterValue
, O
0.3 B-HyperparameterValue
, B-HyperparameterValue
0.5 I-HyperparameterValue
, O
0.7 B-HyperparameterValue
} O
and O
{ O
0.6 B-HyperparameterValue
, O
0.7 B-HyperparameterValue
, O
0.8 B-HyperparameterValue
, O
0.9 B-HyperparameterValue
} O
, O
respectively O
. O

The O
margin B-HyperparameterName
µ B-HyperparameterName
of O
the O
activation O
transfer O
loss O
was O
set O
to O
1.0 B-HyperparameterValue
. O

Every O
hyperparameter O
was O
tuned O
on O
the O
development O
set O
. O

The O
selected O
hyperparameters O
are O
shown O
in O
the O
Appendix O
. O

The O
relation B-TaskName
extraction I-TaskName
task O
aims O
to O
classify O
the O
relationship O
between O
two O
entities O
( O
e.g. O
, O
gene O
, O
chemical O
, O
and O
disease O
) O
that O
are O
already O
annotated O
. O

The O
ChemProt B-DatasetName
( O
Krallinger O
et O
al O
. O
, O
2017 O
) O
dataset O
contains O
PubMed O
abstracts O
with O
10 O
types O
of O
chemicalprotein O
interaction O
annotations O
and O
only O
five O
of O
the O
types O
are O
used O
for O
evaluation O
. O

The O
GAD B-DatasetName
dataset O
( O
Bravo O
et O
al O
. O
, O
2015 O
) O
consists O
of O
gene O
- O
disease O
binary O
relation O
annotations O
. O

The O
DDI B-DatasetName
( O
Herrero O
- O
Zazo O
et O
al O
. O
, O
2013 O
) O
dataset O
consists O
of O
text O
from O
the O
DrugBank O
database O
and O
Medline O
abstracts O
, O
with O
four O
types O
of O
drug O
- O
drug O
interaction O
annotations O
. O

In O
the O
clinical O
domain O
, O
the O
i2b2 B-DatasetName
dataset O
( O
Uzuner O
et O
al O
. O
, O
2011 O
) O
contains O
texts O
from O
clinical O
documents O
, O
and O
eight O
types O
of O
relations O
between O
medical O
problems O
and O
treatments O
have O
been O
annotated O
. O

The O
HoC B-DatasetName
( O
Baker O
et O
al O
. O
, O
2016 O
) O
corpus O
consists O
of O
PubMed O
abstracts O
with O
ten O
types O
of O
hallmarks O
of O
cancer O
annotation O
. O

Note O
that O
the O
HoC B-DatasetName
dataset O
is O
a O
multi O
- O
label O
document B-MethodName
classification I-MethodName
task O
predicting O
the O
combination O
of O
labels O
from O
an O
input O
text O
. O

Specifically O
, O
we O
select O
two O
PLMs O
as O
the O
initial O
student O
model O
: O
ALBERT B-MethodName
- I-MethodName
xlarge I-MethodName
( O
Lan O
et O
al O
. O
, O
2019 O
) O
, O
which O
has O
a O
smaller O
number O
of O
parameters O
but O
performs O
better O
than O
BERT B-MethodName
, O
and O
RoBERTa B-MethodName
- I-MethodName
large I-MethodName
, O
which O
has O
a O
larger O
number O
of O
parameters O
and O
is O
known O
to O
outperform O
BERT B-MethodName
significantly O
for O
most O
of O
the O
tasks O
. O

To O
distil O
the O
knowledge O
from O
a O
teacher O
model O
, O
we O
first O
fine O
- O
tune O
the O
student O
model O
to O
provide O
initial O
knowledge O
about O
the O
task O
. O

Then O
the O
student O
model O
is O
trained O
with O
L O
AT O
. O

We O
also O
add O
a O
few O
refinement O
steps O
to O
refine O
the O
classification O
layer O
of O
the O
student O
model O
. O

Because O
the O
student O
model O
is O
already O
fine O
- O
tuned O
before O
the O
distillation O
step O
, O
this O
additional O
refinement O
may O
cause O
overconfidence O
. O

Thus O
, O
we O
apply O
a O
confidence O
penalty O
regularization O
in O
the O
refinement O
step O
. O

Namely O
, O
the O
student O
is O
refined O
with O
L O
cls O
after O
the O
distillation O
steps O
. O

We O
add O
a O
hyperparameter B-HyperparameterName
γ I-HyperparameterName
∈ O
[ O
0 B-HyperparameterValue
, O
1 B-HyperparameterValue
] O
, O
which O
determines O
when O
the O
training O
loss O
is O
switched O
from O
distillation O
to O
refinement O
. O

The O
procedure O
of O
the O
DoKTra B-MethodName
framework O
is O
summarized O
in O
Algorithm O
1.Input O
: O
Downstream O
task O
data O
D O
= O
{ O
x O
k O
, O
y O
k O
} O
N O
k=1 O
, O
hyperparameter O
β B-HyperparameterName
1 I-HyperparameterName
, O
β B-HyperparameterName
2 I-HyperparameterName
, O
γ B-HyperparameterName
1 I-HyperparameterName
: O

We O
apply O
our O
framework O
to O
the O
biomedical O
domain O
and O
verify O
its O
effectiveness O
by O
conducting O
experiments O
on O
several O
biomedical O
and O
clinical O
downstream O
tasks O
. O

Consequent O
to O
applying O
our O
framework O
to O
ALBERT B-MethodName
and O
RoBERTa B-MethodName
student O
models O
, O
we O
were O
able O
to O
obtain O
models O
that O
retained O
most O
of O
the O
teacher O
model O
's O
performance O
with O
fewer O
model O
parameters O
( O
ALBERT B-MethodName
) O
, O
and O
models O
with O
a O
higher O
performance O
than O
both O
students O
and O
teachers O
( O
RoBERTa B-MethodName
) O
. O

We O
also O
investigate O
the O
general O
applicability O
of O
our O
framework O
by O
applying O
it O
to O
a O
financial O
domain O
PLM O
and O
downstream O
tasks O
. O

The O
contributions O
of O
this O
study O
can O
be O
summarized O
as O
follows O
: O

However O
, O
these O
models O
must O
be O
further O
improved O
for O
tasks O
requiring O
domain O
knowledge O
, O
such O
as O
those O
in O
the O
biomedical O
or O
financial O
domains O
, O
as O
the O
pre O
- O
training O
data O
usually O
consist O
of O
general O
domain O
text O
( O
e.g. O
, O
Wikipedia O
) O
. O

Additional O
pre O
- O
training O
with O
in O
- O
domain O
text O
has O
been O
proposed O
to O
provide O
the O
PLMs O
with O
domain O
- O
specific O
knowledge O
. O

For O
example O
, O
in O
the O
biomedical O
domain O
, O
several O
domainspecific O
PLMs O
trained O
with O
large O
biomedical O
texts O
, O
such O
as O
BioBERT B-MethodName
, O
PubMedBERT B-MethodName
( O
Gu O
et O
al O
. O
, O
2020 O
) O
and O
BlueBERT B-MethodName
( O
Peng O
et O
al O
. O
, O
2019 O
) O
, O
have O
been O
successfully O
used O
as O
strong O
baselines O
for O
several O
downstream O
tasks O
. O

Nevertheless O
, O
additional O
pre O
- O
training O
has O
several O
limitations O
, O
such O
as O
the O
need O
for O
sufficient O
training O
data O
and O
resources O
, O
and O
a O
longer O
training O
time O
. O

Furthermore O
, O
whenever O
a O
new O
PLM O
emerges O
, O
it O
must O
be O
re O
- O
trained O
to O
create O
more O
advanced O
domain O
- O
specific O
models O
. O

Recently O
, O
transformer O
( O
Vaswani O
et O
al O
. O
, O
2017)-based O
language O
models O
have O
been O
successfully O
applied O
in O
the O
field O
of O
natural O
language O
processing O
( O
NLP O
) O
. O

In O
particular O
, O
the O
two O
- O
stage O
approach O
of O
" O
pre O
- O
training O
and O
fine O
- O
tuning O
, O
" O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
has O
become O
the O
standard O
for O
NLP O
applications O
. O

Generally O
, O
a O
transformer O
- O
based O
model O
is O
pretrained O
with O
a O
large O
amount O
of O
text O
data O
in O
an O
unsu- O
* O
Hyunju O
Lee O
is O
the O
corresponding O
author O
. O

pervised O
manner O
, O
and O
then O
fine O
- O
tuned O
with O
a O
small O
dataset O
for O
several O
downstream O
tasks O
. O

Further O
, O
advanced O
pre O
- O
trained O
language O
models O
( O
PLMs O
) O
with O
improved O
architectures O
or O
training O
methods O
continue O
to O
emerge O
, O
including B-MethodName
ALBERT I-MethodName
( O
Lan O
et O
al O
. O
, O
2019 O
) O
or O
RoBERTa B-MethodName
. O

For O
γ B-HyperparameterName
analysis O
, O
we O
consider O
the O
consistency O
of O
recognition O
results O
and O
similarity O
score O
by O
teachers O
. O

The O
F1 B-MetricName
- O
score O
and O
similarity O
score O
of O
teachers O
are O
all O
higher O
in O
the O
higher O
γ B-HyperparameterName
intervals O
, O
as O
shown O
in O
Figure O
6c O
. O

The O
student O
model O
learns O
less O
from O
unreasonable O
results O
, O
and O
it O
can O
make O
more O
accurate O
entity B-TaskName
recognition I-TaskName
for O
the O
target O
language O
. O

In O
this O
paper O
, O
we O
propose O
an O
unsupervised O
multipletask B-MethodName
and I-MethodName
multiple I-MethodName
- I-MethodName
teacher I-MethodName
model I-MethodName
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
. O

The O
student O
model O
learns O
two O
source O
language O
patterns O
of O
entity B-TaskName
recognition I-TaskName
and O
entity B-TaskName
similarity I-TaskName
evaluation I-TaskName
. O

Moreover O
, O
to O
guarantee O
the O
student O
learning O
performance O
, O
we O
also O
propose O
a O
weighting O
strategy O
to O
take O
into O
consideration O
the O
reliability O
of O
the O
teachers O
. O

Our O
experimental O
results O
show O
that O
the O
proposed O
model O
yields O
significant O
improvements O
on O
six O
target O
language O
datasets O
and O
outperforms O
the O
existing O
state O
- O
of O
- O
the O
- O
art O
approaches O
. O

This O
work O
is O
supported O
partly O
by O
the O
Fundamental O
Research O
Funds O
for O
the O
Central O
Universities O
and O
by O
the O
State O
Key O
Laboratory O
of O
Software O
Development O
Environment O
. O

We O
propose O
a O
novel O
data O
- O
augmentation O
technique O
for O
neural B-TaskName
machine I-TaskName
translation I-TaskName
based O
on O
ROT O
- O
k O
ciphertexts O
. O

ROT O
- O
k O
is O
a O
simple O
letter O
substitution O
cipher O
that O
replaces O
a O
letter O
in O
the O
plaintext O
with O
the O
kth O
letter O
after O
it O
in O
the O
alphabet O
. O

We O
first O
generate O
multiple O
ROT O
- O
k O
ciphertexts O
using O
different O
values O
of O
k O
for O
the O
plaintext O
which O
is O
the O
source O
side O
of O
the O
parallel O
data O
. O

We O
then O
leverage O
this O
enciphered O
training O
data O
along O
with O
the O
original O
parallel O
data O
via O
multi O
- O
source O
training O
to O
improve O
neural B-TaskName
machine I-TaskName
translation I-TaskName
. O

Our O
method O
, O
CipherDAug B-MethodName
, O
uses O
a O
co O
- O
regularization O
- O
inspired O
training O
procedure O
, O
requires O
no O
external O
data O
sources O
other O
than O
the O
original O
training O
data O
, O
and O
uses O
a O
standard O
Transformer B-HyperparameterName
to O
outperform O
strong O
data O
augmentation O
techniques O
on O
several O
datasets O
by O
a O
significant O
margin O
. O

This O
technique O
combines O
easily O
with O
existing O
approaches O
to O
data O
augmentation O
, O
and O
yields O
particularly O
strong O
results O
in O
low O
- O
resource O
settings O
. O

1 O

For O
β B-HyperparameterName
analysis O
, O
we O
observe O
that O
F1 B-MetricName
- O
score O
are O
increasing O
with O
the O
entity O
similarity O
score O
from O
0.5 O
to O
both O
sides O
0 O
and O
1 O
in O
Figure O
6b O
. O

The O
encoder O
of O
the O
student O
model O
obtains O
the O
clustering O
information O
of O
the O
target O
language O
with O
the O
help O
of O
β B-HyperparameterName
. O

For O
α B-HyperparameterName
analysis O
, O
we O
calculate O
the O
F1 B-MetricName
- O
score O
in O
different O
probability B-HyperparameterName
intervals I-HyperparameterName
of O
entity O
recognizer O
teacher O
, O
we O
find O
that O
the O
recognizer O
teacher O
tends O
to O
predict O
more O
correct O
in O
higher O
probability B-HyperparameterName
interval I-HyperparameterName
, O
as O
illustrated O
in O
Figure O
6a O
. O

Therefore O
, O
the O
student O
model O
is O
better O
suited O
to O
the O
target O
language O
with O
learning O
fewer O
low O
- O
confidence O
misrecognitions O
for O
the O
target O
language O
. O

( O
3 O
) O
MTMT B-MethodName
w/o O
similarity O
, O
which O
removes O
the O
similarity O
teacher O
model O
. O

In O
this O
case O
, O
our O
approach O
degrades O
into O
the O
single O
teacherstudent O
learning O
model O
as O
in O
TSL O
( O
Wu O
et O
al O
. O
, O
2020a O
) O
. O

Without O
the O
similarity O
knowledge O
fed O
into O
the O
student O
model O
, O
the O
performance O
drops O
significantly O
. O

We O
give O
a O
case O
study O
to O
show O
that O
the O
failed O
cases O
of O
baseline O
models O
can O
be O
corrected O
by O
our O
model O
. O

We O
try O
to O
bring O
up O
insights O
on O
why O
the O
proposed O
multiple B-MethodName
- I-MethodName
task I-MethodName
and I-MethodName
multiple I-MethodName
- I-MethodName
teacher I-MethodName
model I-MethodName
works O
. O

( O
2 O
) O
MTMT B-MethodName
w/o O
weighting O
, O
which O
set O
the O
α B-HyperparameterName
( I-HyperparameterName
• I-HyperparameterName
) I-HyperparameterName
, O
β B-HyperparameterName
and O
γ B-HyperparameterName
all O
to O
be O
1 B-HyperparameterValue
in O
the O
loss O
of O
student O
learning O
. O

It O
can O
be O
seen O
that O
the O
performance O
decrease O
in O
terms O
of O
F1 B-MetricName
- O
score O
ranges O
from O
0.45 B-MetricValue
for O
Dutch(nl O
) O
to O
0.98 B-MetricValue
for O
Spanish(es O
) O
, O
which O
validates O
that O
weighting O
loss O
can O
bring O
more O
confident O
knowledge O
to O
the O
student O
model O
. O

( O
1 O
) O
MTST B-MethodName
, O
which O
combines O
the O
multiple O
- O
teacher O
to O
single O
- O
teacher O
. O

That O
is O
, O
the O
teacher O
model O
has O
the O
same O
as O
the O
neural O
network O
structure O
of O
the O
student O
model O
. O

This O
causes O
a O
performance O
drop O
across O
all O
languages O
due O
to O
two O
single O
teachers O
can O
not O
make O
a O
difference O
with O
the O
combination O
. O

Note O
that O
BERT B-MethodName
- I-MethodName
f I-MethodName
performs O
better O
than O
our O
model O
on O
the O
Chinese O
dataset O
due O
to O
their O
re O
- O
tokenization O
of O
the O
dataset O
. O

Moreover O
, O
compared O
with O
the O
latest O
model O
TOF B-MethodName
, O
RIKD B-MethodName
, O
Unitrans B-MethodName
, O
our O
model O
requires O
much O
lower O
computational O
costs O
for O
both O
translation O
and O
iterative O
knowledge O
distillation O
, O
meanwhile O
reaching O
superior O
performance O
. O

For O
a O
fair O
comparison O
, O
we O
compare O
our O
model O
against O
the O
version O
of O
TOF B-MethodName
w/o O
continual O
learning O
( O
Zhang O
et O
2021 O
) O
, O
RIKD B-MethodName
w/o O
IKD O
( O
Liang O
et O
al O
. O
, O
2021 O
) O
and O
Unitrans B-MethodName
w/o O
translation O
( O
Wu O
et O
al O
. O
, O
2020b O
) O
as O
reported O
in O
their O
paper O
. O

To O
demonstrate O
the O
effectiveness O
of O
our O
approach O
, O
we O
designed O
the O
following O
ablation O
studies O
. O

Table O
5 O
presents O
the O
results O
. O

It O
can O
be O
seen O
that O
our O
model O
outperforms O
the O
state O
- O
of O
- O
the O
- O
arts O
. O

Specifically O
, O
compared O
with O
the O
remarkable O
RIKD B-MethodName
, O
AdvPicker B-MethodName
, O
and O
Unitrans B-MethodName
, O
which O
also O
use O
knowledge O
distillation O
but O
ignore O
the O
entity O
similarity O
knowledge O
, O
our O
model O
obtains O
significant O
and O
consistent O
improvements O
in O
F1 B-MetricName
- O
score O
ranging O
from O
0.23 B-MetricValue
for O
German O
[ O
de O
] O
to O
6.81 B-MetricValue
for O
Arabic O
[ O
ar O
] O
. O

That O
demonstrates O
the O
benefits O
of O
our O
proposed O
MTMT B-MethodName
model O
, O
compared O
to O
direct O
model O
transfer O
( O
Wu O
and O
Dredze O
, O
2019 O
) O
. O

TOF B-MethodName
( O
Zhang O
et O
al O
. O
, O
2021 O
) O
transfers O
knowledge O
from O
three O
aspects O
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
. O

RIKD B-MethodName
( O
Liang O
et O
al O
. O
, O
2021 O
) O
develops O
a O
reinforced O
iterative O
knowledge O
distillation O
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
. O

AdvPicker B-MethodName
proposes O
a O
adversarial O
discriminator O
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
. O

We O
set O
our O
hyperparameters O
empirically O
following O
( O
Wu O
et O
al O
. O
, O
2020c O
) O
with O
some O
modifications O
. O

We O
do O
not O
freeze O
any O
layers O
and O
we O
use O
the O
output O
of O
the O
last O
layer O
as O
our O
hidden O
feature O
vector O
. O

We O
set O
the B-HyperparameterName
batch I-HyperparameterName
size I-HyperparameterName
to O
be O
32 B-HyperparameterValue
, O
maximum B-HyperparameterName
sequence I-HyperparameterName
length I-HyperparameterName
to O
be O
128 B-HyperparameterValue
, O
dropout B-HyperparameterName
rate I-HyperparameterName
to O
be O
0.2 B-HyperparameterValue
, O
and O
we O
use O
Adam B-HyperparameterValue
as O
optimizer B-HyperparameterName
( O
Kingma O
and O
Ba O
, O
2014 O
) O
. O

For O
the O
training O
of O
recognition O
teacher O
model O
and O
similarity O
teacher O
model O
, O
we O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
be O
1e-5 B-HyperparameterValue
and O
5e-6 B-HyperparameterValue
separately O
. O

For O
knowledge O
distillation O
, O
we O
use O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1e-6 B-HyperparameterValue
for O
the O
student O
models O
training O
. O

Note O
that O
if O
a O
word O
is O
divided O
into O
several O
subwords O
after O
tokenization O
, O
then O
only O
the O
first O
subword O
is O
considered O
in O
the O
loss O
function O
. O

Following O
( O
Tjong O
Kim O
Sang O
, O
2002 O
) O
, O
we O
use O
the O
entity B-MetricName
level I-MetricName
F1 I-MetricName
- O
score O
as O
the O
evaluation O
metric O
. O

Moreover O
, O
we O
conduct O
each O
experiment O
5 B-HyperparameterValue
times O
and O
report O
the O
mean O
F1 B-MetricName
- O
score O
. O

( O
Tsai O
et O
al O
. O
, O
2016 O
) O
48.12 B-MetricValue
60.55 B-MetricValue
61.56 B-MetricValue
WS B-MethodName
( O
Ni O
et O
al O
. O
, O
2017 O
) O
58.50 B-MetricValue
65.10 B-MetricValue
65.40 B-MetricValue
TMP B-MethodName
( O
Jain O
et O
al O
. O
, O
2019 O
) O
61.50 B-MetricValue
73.50 B-MetricValue
69.9 B-MetricValue
BERT B-MethodName
- I-MethodName
f I-MethodName
( O
Wu O
and O
Dredze O
, O
2019 O
) O
69.56 B-MetricValue
74.96 B-MetricValue
77.57 B-MetricValue
AdvCE B-MethodName
( O
Keung O
et O
al O
. O
, O
2019 O
) O
71.90 B-MetricValue
74.3 B-MetricValue
77.6 B-MetricValue
TSL B-MethodName
( O
Wu O
et O
al O
. O
, O
2020a O
) O
73.16 B-MetricValue
76.75 B-MetricValue
80.44 B-MetricValue
Unitrans B-MethodName
( O
Wu O
et O
al O
. O
, O
2020b O
) O
74 O
TSL B-MethodName
( O
Wu O
et O
al O
. O
, O
2020c O
) O
proposes O
a O
teacher O
- O
student O
learning O
model O
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER.Unitrans I-TaskName
( O
Wu O
et O
al O
. O
, O
2020b O
) O
unifies O
a O
data O
transfer O
and O
model O
transfer O
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
. O

α O
( O
• O
) O
= O
( O
max(ŷ O
T O
i O
) O
) O
2 O
β O
= O
( O
2 O
t O
T O
( O
x O
T O
, O
x O
T O
, O
i O
, O
j O
) O
− O
1 O
) O
2 O
γ O
= O
1 O
− O
|σ(cos(ŷ O
T O
i O
, O
ŷ O
T O
j O
) O
) O
−t O
T O
( O
x O
T O
, O
x O
T O
, O
i O
, O
j)|In O
this O
section O
, O
we O
evaluate O
our O
multiple O
- O
task O
and O
multiple O
- O
teacher O
model O
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
and O
compare O
our O
model O
with O
a O
series O
of O
state O
- O
of O
- O
the O
- O
art O
models O
. O

We O
conducted O
experiments O
on O
three O
benchmark O
datasets O
: O
CoNLL2002 B-DatasetName
( O
Tjong O
Kim O
Sang O
, O
2002 O
) O
, O
CoNLL2003 B-DatasetName
( O
Tjong O
Kim O
Sang O
and O
De O
Meulder O
, O
2003 O
) O
and O
WikiAnn B-DatasetName
( O
Pan O
et O
al O
. O
, O
2017 O
) O
. O

CoNLL2002 B-DatasetName
includes O
Spanish O
and O
Dutch O
, O
CoNLL2003 B-DatasetName
includes O
English O
and O
German O
, O
and O
WikiAnn B-DatasetName
includes O
English O
and O
three O
non O
- O
western O
languages O
: O
Arabic O
, O
Hindi O
, O
and O
Chinese O
. O

Each O
language O
is O
divided O
into O
a O
training O
set O
, O
a O
development O
set O
and O
a O
test O
set O
. O

All O
datasets O
were O
annotated O
with O
four O
entity O
types O
: O
LOC O
, O
MISC O
, O
ORG O
, O
and O
PER O
. O

Following O
( O
Wu O
and O
Dredze O
, O
2019 O
) O
, O
all O
datasets O
are O
annotated O
using O
the O
BIO O
entity O
labelling O
scheme O
. O

To O
imitate O
the O
zero B-TaskName
- I-TaskName
resource I-TaskName
cross I-TaskName
lingual I-TaskName
NER I-TaskName
case O
, O
following O
( O
Wu O
and O
Dredze O
, O
2019 O
) O
, O
we O
used O
English O
as O
the O
source O
language O
and O
other O
languages O
as O
the O
target O
language O
. O

In O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
, O
the O
training O
set O
without O
entity O
label O
of O
the O
target O
language O
is O
also O
available O
when O
training O
the O
model O
. O

We O
trained O
the O
model O
with O
the O
labeled O
training O
set O
of O
the O
source O
language O
and O
evaluated O
the O
model O
on O
the O
test O
set O
of O
each O
target O
language O
. O

Table O
1 O
and O
2 O
shows O
the O
statistics O
of O
all O
datasets O
. O

We O
use O
PyTorch O
1.7.1 O
to O
implement O
our O
model O
. O

All O
of O
the O
feature O
encoders O
mentioned O
in O
this O
paper O
use O
pre O
- O
trained O
mBERT B-MethodName
model O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
in O
HuggingFace O
Transformer O
1 O
, O
which O
has O
12 B-HyperparameterValue
Transformer B-HyperparameterName
blocks I-HyperparameterName
, O
12 B-HyperparameterValue
attention B-HyperparameterName
heads I-HyperparameterName
, O
and O
768 B-HyperparameterValue
hidden B-HyperparameterName
units I-HyperparameterName
. O

where O
α B-HyperparameterName
1 I-HyperparameterName
, O
α B-HyperparameterName
2 I-HyperparameterName
, O
β B-HyperparameterName
, O
and O
γ B-HyperparameterName
are O
weights O
in O
loss O
function O
which O
are O
set O
to O
make O
the O
student O
model O
learns O
less O
noisy O
knowledge O
from O
teachers O
. O

The O
weights O
are O
set O
as O
follows O
: O
α B-HyperparameterName
1 I-HyperparameterName
( O
α B-HyperparameterName
2 I-HyperparameterName
) O
is O
an O
increasing O
function O
concerning O
the O
output O
of O
the O
entity O
recognizer O
teacher O
as O
shown O
in O
Figure O
4 O
. O

And O
β B-HyperparameterName
is O
set O
such O
that O
it O
is O
high O
when O
the O
output O
of O
the O
entity O
similarity O
teacher O
is O
close O
to O
0 O
or O
1 O
, O
and O
it O
is O
low O
when O
the O
output O
is O
close O
to O
0.5 O
. O

γ B-HyperparameterName
indicates O
consistency B-HyperparameterName
level I-HyperparameterName
between O
the O
outputs O
from O
two O
teacher O
models O
, O
e.g. O
for O
two O
input O
tokens O
, O
if O
the O
output O
from O
entity O
similarity O
teacher O
is O
high O
, O
and O
the O
similarity O
level O
computed O
from O
the O
outputs O
of O
the O
entity O
recognizer O
teacher O
is O
low O
, O
then O
their O
consistency B-HyperparameterName
level I-HyperparameterName
is O
low O
. O

We O
want O
the O
student O
model O
to O
learn O
from O
the O
two O
teachers O
as O
follows O
: O
the O
higher O
the O
prediction O
of O
the O
entity O
recognizer O
teacher O
is O
( O
the O
further O
away O
from O
0.5 O
the O
prediction O
of O
the O
entity O
similarity O
teacher O
is O
, O
the O
higher O
the O
consistency B-HyperparameterName
level I-HyperparameterName
is O
) O
, O
the O
more O
accurate O
the O
prediction O
is O
, O
thus O
the O
more O
attention O
the O
student O
model O
pays O
attention O
to O
the O
input O
tokens O
, O
and O
vice O
versa O
. O

Therefore O
, O
we O
heuristically O
devises O
the O
three O
weights O
scheduling O
as O
functions O
of O
the O
inputs O
, O

Siamese B-MethodName
Network I-MethodName
is O
originally O
introduced O
by O
( O
Bromley O
et O
al O
. O
, O
1994 O
) O
to O
treat O
signature B-TaskName
verification I-TaskName
as O
a O
matching O
problem O
. O

It O
has O
been O
successfully O
applied O
to O
transfer O
learning O
such O
as O
one B-TaskName
- I-TaskName
shot I-TaskName
image I-TaskName
recognition I-TaskName
( O
Koch O
et O
al O
. O
, O
2015 O
) O
, O
text B-TaskName
similarity I-TaskName
( O
Neculoiu O
et O
al O
. O
, O
2016 O
) O
. O

However O
, O
there O
is O
a O
dilemma O
to O
adapt O
the O
siamese B-MethodName
network I-MethodName
to O
tokenlevel B-TaskName
recognition I-TaskName
tasks O
such O
as O
NER B-TaskName
. O

Siamese B-MethodName
network I-MethodName
assumes O
the O
input O
is O
a O
pair O
, O
and O
the O
output O
is O
a O
similarity O
score O
. O

To O
handle O
this O
issue O
, O
we O
reconstruct O
the O
data O
to O
pair O
format O
. O

To O
the O
best O
of O
our O
knowledge O
, O
we O
are O
the O
first O
to O
learn O
the O
entity O
similarity O
by O
siamese B-MethodName
network I-MethodName
. O

In O
this O
section O
, O
we O
introduce O
our O
framework O
and O
its O
detailed O
implementation O
. O

Our O
framework O
is O
consist O
of O
two O
models O
: O
teacher O
training O
model O
learned O
from O
the O
source O
language O
and O
teacher O
- O
student O
distillation O
learning O
model O
learned O
from O
the O
target O
language O
. O

In O
the O
teacher O
training O
model O
, O
there O
are O
two O
sub O
- O
models O
, O
i.e. O
an O
entity O
recognizer O
teacher O
and O
a O
similarity O
evaluator O
teacher O
. O

These O
two O
models O
are O
two O
parallel O
tasks O
, O
wherein O
the O
entity O
recognition O
teacher O
focuses O
on O
identifying O
the O
named O
entities O
and O
the O
similarity O
evaluator O
teacher O
is O
to O
decide O
if O
two O
tokens O
are O
in O
the O
same O
type O
. O

Shared O
feature O
space O
based O
models O
generally O
train O
a O
language O
- O
independent O
encoder O
using O
source O
and O
target O
language O
data O
( O
Tsai O
et O
al O
. O
, O
2016 O
) O
. O

Recently O
, O
the O
pre O
- O
trained O
multilingual O
language O
model O
is O
effective O
to O
address O
the O
challenge O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

Moreover O
, O
some O
research O
introduces O
new O
components O
on O
top O
of O
the O
mBERT B-MethodName
by O
directly O
transferring O
the O
model O
learned O
from O
the O
labeled O
source O
language O
to O
that O
of O
target O
languages O
( O
Keung O
et O
al O
. O
, O
2019 O
) O
. O

The O
performance O
is O
still O
weak O
due O
to O
the O
lack O
of O
annotations O
of O
target O
languages O
. O

Our O
main O
contributions O
are O
as O
follows O
: O

We O
validate O
the O
model O
performance O
on O
the O
three O
commonly O
- O
used O
datasets O
across O
7 O
languages O
and O
the O
experimental O
results O
show O
the O
superiority O
of O
our O
presented O
MTMT B-MethodName
model O
. O

To O
leverage O
the O
similarity O
between O
the O
tokens O
of O
the O
source O
languages O
, O
we O
design O
an O
multiple B-MethodName
- I-MethodName
task I-MethodName
and I-MethodName
multiple I-MethodName
- I-MethodName
teacher I-MethodName
model O
( O
short O
as O
MTMT B-MethodName
, O
as O
shown O
in O
Figure O
1 O
) O
, O
which O
helps O
the O
NER B-TaskName
learning O
process O
on O
the O
target O
languages O
. O

Specifically O
, O
we O
first O
introduce O
the O
knowledge O
distillation O
to O
build O
entity O
recognizer O
and O
similarity O
evaluator O
teachers O
in O
the O
source O
language O
and O
transfer O
the O
learned O
patterns O
to O
the O
student O
in O
the O
target O
language O
. O

In O
the O
student O
model O
, O
we O
then O
borrow O
the O
idea O
of O
multitask O
learning O
to O
incorporate O
a O
similarity B-TaskName
evaluation I-TaskName
task O
as O
an O
auxiliary O
task O
into O
the O
entity B-TaskName
recognition I-TaskName
classifier O
. O

During O
the O
student O
learning O
process O
, O
we O
input O
unlabelled O
samples O
from O
the O
target O
languages O
into O
the O
entity O
recognizer O
and O
evaluator O
, O
and O
take O
output O
pesudo O
labels O
as O
supervisory O
signals O
for O
these O
two O
tasks O
in O
the O
student O
model O
. O

Note O
that O
a O
weighting O
strategy O
is O
also O
provide O
therein O
to O
take O
into O
consideration O
of O
the O
reliability O
of O
the O
teachers O
. O

Although O
the O
above O
- O
mentioned O
models O
solve O
the O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
problem O
to O
some O
extent O
, O
the O
auxiliary O
tasks O
, O
as O
in O
multi O
- O
task O
learning O
, O
have O
not O
been O
studied O
in O
this O
problem O
. O

Due O
to O
the O
distributed O
representation O
of O
natural O
languages O
, O
the O
relatedness O
among O
the O
embedding O
of O
target O
languages O
, O
which O
is O
measured O
by O
the O
similarity O
, O
can O
be O
utilized O
to O
further O
boost O
the O
learned O
encoder O
and O
improve O
the O
final O
NER B-TaskName
performance O
on O
target O
languages O
. O

Many O
studies O
have O
been O
done O
to O
solve O
this O
crosslingual B-TaskName
NER I-TaskName
problem O
. O

Existing O
models O
can O
be O
separated O
into O
three O
categories O
, O
shared O
feature O
space O
based O
, O
translation O
based O
and O
knowledge O
distillation O
based O
. O

Shared O
feature O
space O
based O
models O
exploit O
language O
- O
independent O
features O
, O
which O
lacks O
the O
domain O
- O
specific O
features O
for O
the O
target O
language O
( O
Tsai O
et O
al O
. O
, O
2016;Wu O
and O
Dredze O
, O
2019;Keung O
et O
al O
. O
, O
2019 O
) O
. O

Translation O
based O
models O
generate O
pseudo O
labeled O
target O
language O
data O
to O
train O
the O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
model O
, O
but O
the O
noise O
from O
translation O
process O
restrains O
its O
performance O
. O

( O
Mayhew O
et O
al O
. O
, O
2017;Xie O
et O
al O
. O
, O
2018;Wu O
et O
al O
. O
, O
2020b O
) O
. O

Knowledge O
distillation O
based O
models O
train O
a O
student O
model O
using O
soft O
labels O
of O
the O
target O
language O
( O
Wu O
et O
al O
. O
, O
2020a O
, O
b;Liang O
et O
al O
. O
, O
2021 O
) O
. O

Named B-TaskName
entity I-TaskName
recognition I-TaskName
, O
NER B-TaskName
in O
short O
, O
refers O
to O
identifying O
entity O
types O
, O
i.e. O
location O
, O
person O
, O
organization O
, O
etc O
. O

, O
in O
a O
given O
sentence O
. O

The O
exploiting O
of O
deep O
neural O
networks O
, O
such O
as O
Bi B-MethodName
- I-MethodName
LSTM I-MethodName
- I-MethodName
CRF I-MethodName
( O
Lample O
et O
al O
. O
, O
2016 O
) O
, O
Bi B-MethodName
- I-MethodName
LSTM- I-MethodName
CNN I-MethodName
( O
Chiu O
and O
Nichols O
, O
2016 O
) O
makes O
this O
task O
achieve O
significant O
performances O
. O

However O
, O
since O
deep O
neural O
networks O
highly O
rely O
on O
a O
large O
amount O
of O
labelled O
training O
data O
, O
the O
annotation O
acquiring O
process O
is O
expensive O
and O
time O
consuming O
. O

This O
situation O
is O
more O
severe O
for O
zero O
- O
resource O
languages O
. O

With O
the O
help O
of O
transfer O
learning O
( O
Ruder O
et O
al O
. O
, O
2019 O
) O
and O
multilingual B-MethodName
BERT I-MethodName
( O
short O
as O
mBERT O
) O
( O
Devlin O
et O
al O
. O
, O
* O
NER O
/ O
NER O
tea O
: O
learned O
NER B-TaskName
model O
for O
source O
language O
; O
NER O
stu O
: O
learned O
NER B-TaskName
model O
for O
target O
language O
; O
SIM O
tea O
learned O
similarity O
model O
for O
source O
language O
; O
{ O
X O
, O
Y O
} O
src O
: O
labeled O
data O
in O
source O
language O
; O
{ O
X O
} O
tgt O
: O
unlabeled O
data O
in O
target O
language O
; O
{ O
X O
, O
P O
} O
tgt O
: O
labeled O
data O
in O
target O
language O
with O
probability O
; O
{ O
X O
, O
S O
} O
tgt O
: O
labeled O
data O
in O
target O
language O
with O
entity O
similarity O
score O
. O

2019 O
) O
, O
it O
is O
possible O
to O
transfer O
the O
annotated O
training O
samples O
or O
trained O
models O
from O
a O
rich O
- O
resource O
domain O
to O
a O
zero O
- O
resource O
domain O
. O

As O
can O
be O
seen O
from O
the O
table O
, O
teachers O
with O
smaller O
length B-HyperparameterName
penalty I-HyperparameterName
( O
i.e. O
, O
1.0 B-HyperparameterValue
or O
0.5 B-HyperparameterValue
) O
can O
not O
teach O
better O
students O
than O
the O
Regular O
pseudo O
- O
labeling O
or O
our O
method O
. O

We O
present O
the O
detailed O
content O
of O
the O
example O
in O
Section O
1 O
in O
table O
12.We O
present O
more O
examples O
of O
student O
models O
' O
outputs O
and O
cross O
attention O
visualization O
here O
. O

The O
student O
models O
are O
with O
the O
BART B-MethodName
12 I-MethodName
- I-MethodName
6 I-MethodName
setting O
and O
are O
trained O
on O
CNNDM B-DatasetName
and O
the O
following O
examples O
are O
from O
the O
validation O
set O
of O
CNNDM B-DatasetName
. O

It O
's O
a O
more O
direct O
idea O
to O
change O
the O
softmax B-HyperparameterName
temperature I-HyperparameterName
in O
the O
final O
decoder O
layer O
rather O
than O
attention B-HyperparameterName
temperatures I-HyperparameterName
, O
namely O
changing O
the O
T B-HyperparameterName
in O
equation O
5 O
to O
some O
other O
values O
rather O
than O
the O
default O
value O
1.0 B-HyperparameterValue
. O

We O
apply O
our O
method O
on O
the O
WMT16 B-DatasetName
En B-TaskName
- I-TaskName
De I-TaskName
translation I-TaskName
task O
. O

We O
use O
Transformer B-MethodName
- I-MethodName
Big I-MethodName
model O
as O
the O
teacher O
and O
Transformer B-MethodName
- I-MethodName
Base I-MethodName
as O
the O
student O
. O

Our O
results O
on O
newstest2014 B-DatasetName
are O
shown O
in O
Table O
8 O
. O

The O
student O
models O
with O
our O
method O
( O
λ B-HyperparameterName
= O
1.5 B-HyperparameterValue
and O
λ B-HyperparameterName
= O
2.0 B-HyperparameterValue
) O
slightly O
outperform O
the O
student O
with O
regular O
pseudo O
- O
labeling O
method O
( O
λ B-HyperparameterName
= O
1.0 B-HyperparameterValue
) O
. O

Note O
that O
the O
improvement O
is O
not O
as O
significant O
as O
in O
summarization B-TaskName
tasks O
. O

We O
speculate O
the O
reason O
may O
be O
that O
, O
unlike O
summarization B-TaskName
, O
outputs O
of O
the O
machine B-TaskName
translation I-TaskName
task O
are O
relatively O
fixed O
. O

The O
strength O
of O
our O
methodconciseness O
and O
abstractiveness O
are O
good O
properties O
for O
summarization B-TaskName
but O
seem O
not O
very O
beneficial O
to O
the O
translation B-TaskName
task O
. O

Besides O
the O
λ B-HyperparameterName
values O
of O
1.5 B-HyperparameterValue
and O
2.0 B-HyperparameterValue
, O
we O
also O
try O
more O
values O
in O
a O
broader O
range O
. O

Table O
9 O
shows O
the O
distillation O
performance O
of O
BART B-MethodName
12 I-MethodName
- I-MethodName
6 I-MethodName
student O
models O
with O
more O
values O
of O
λ B-HyperparameterName
we O
try O
on O
CNNDM B-DatasetName
dataset O
( O
we O
also O
include O
the O
values O
of O
1.0 B-HyperparameterValue
, O
1.5 B-HyperparameterValue
, O
and O
2.0 B-HyperparameterValue
in O
table O
for O
convenient O
comparison O
) O
. O

As O
can O
be O
seen O
, O
both O
lower O
and O
larger O
λ B-HyperparameterName
values O
are O
not O
helpful O
to O
the O
distillation O
. O

Though O
the O
suitable O
λ B-HyperparameterName
values O
may O
vary O
across O
datasets O
, O
we O
recommend O
considering O
the B-HyperparameterName
λ I-HyperparameterName
value O
1.5 B-HyperparameterValue
or O
2.0 B-HyperparameterValue
firstly O
in O
most O
cases O
. O

Temperature B-HyperparameterName
in O
the O
Final O
Decoder O
Layer O

To O
sum O
up O
, O
teachers O
with O
higher O
attention B-HyperparameterName
temperatures I-HyperparameterName
can O
generate O
more O
concise O
and O
abstractive O
pseudo O
summaries O
, O
which O
makes O
the O
teacher O
provide O
more O
summary O
- O
like O
pseudo O
labels O
to O
students O
. O

High O
- O
temperature B-HyperparameterName
teachers O
can O
alleviate O
the O
leading O
bias O
problems O
by O
providing O
pseudo O
labels O
with O
better O
coverage O
of O
source O
documents O
to O
students O
. O

Attention O
We O
have O
shown O
earlier O
in O
Figure O
1 O
that O
with O
higher O
attention B-HyperparameterName
temperature I-HyperparameterName
, O
cross O
- O
attention O
modules O
of O
a O
teacher O
can O
attend O
to O
later O
parts O
in O
documents O
. O

We O
observe O
that O
students O
behave O
similarly O
, O
and O
we O
put O
more O
cross O
attention O
visualization O
of O
students O
in O
Appendix O
F. O
To O
obtain O
corpus O
- O
level O
statistics O
, O
we O
further O
calculate O
the O
evident B-MetricName
crossattention I-MetricName
weight I-MetricName
distributions I-MetricName
of O
the O
teacher O
when O
generating O
pseudo O
labels O
on O
the O
training O
set O
of O
CN B-DatasetName
- I-DatasetName
NDM I-DatasetName
. O

Note O
that O
an O
attention O
weight O
is O
evident O
if O
it O
is O
greater O
than O
0.15 O
, O
and O
these O
evident O
attention O
weights O
account O
for O
around O
15 B-MetricValue
% I-MetricValue
of O
all O
attention O
weights O
. O

Specifically O
, O
we O
normalize O
the O
token O
positions O
of O
each O
document O
to O
( O
0.0 O
, O
1.0 O
] O
and O
divide O
the O
normalized O
positions O
into O
five O
bins O
. O

The O
mean O
proportions O
of O
evident O
attentions O
for O
all O
bins O
are O
shown O
in O
Figure O
2 O
. O

Compared O
to O
the O
teacher O
with O
normal O
attention B-HyperparameterName
temperature I-HyperparameterName
( O
pink O
bar O
) O
, O
teachers O
with O
higher O
attention B-HyperparameterName
temperatures I-HyperparameterName
( O
blue O
and O
green O
bars O
) O
attend O
less O
on O
the O
heading O
parts O
of O
documents O
while O
more O
on O
the O
tail O
parts O
of O
documents O
. O

Length B-MetricName
and O
novel B-MetricName
n I-MetricName
- I-MetricName
grams I-MetricName
We O
first O
analyze O
the O
pseudo O
summaries O
generated O
by O
the O
teacher O
models O
. O

We O
calculate O
novel B-MetricName
n I-MetricName
- I-MetricName
grams I-MetricName
and B-MetricName
lengths I-MetricName
of O
generated O
summaries O
. O

Note O
that O
if O
an O
n O
- O
gram O
appears O
in O
the O
summary O
, O
but O
not O
in O
the O
original O
document O
, O
we O
call O
it O
a O
novel B-MetricName
n I-MetricName
- I-MetricName
gram I-MetricName
. O

Proportions O
of O
novel B-MetricName
n I-MetricName
- I-MetricName
grams I-MetricName
are O
used O
to O
measure O
the O
abstractiveness O
of O
summaries O
( O
See O
et O
al O
. O
, O
2017;Liu O
and O
Lapata O
, O
2019 O
) O
. O

As O
shown O
in O
Table O
6 O
, O
when O
using O
a O
larger O
λ B-HyperparameterName
, O
pseudo O
summaries O
are O
shorter O
6 O
and O
contain O
a O
larger O
portion O
of O
novel B-MetricName
n I-MetricName
- I-MetricName
grams I-MetricName
. O

It O
indicates O
that O
the O
teachers O
can O
produce O
more O
concise O
and O
abstractive O
summaries O
, O
which O
matches O
the O
goal O
of B-TaskName
abstractive I-TaskName
summarization I-TaskName
. O

Are O
these O
pseudo O
summaries O
of O
good O
quality O
? O
set O
is O
shown O
in O
Table O
7 O
. O

Their O
results O
are O
all O
decent O
and O
close O
to O
each O
other O
( O
at O
least O
for O
ROUGE-1 B-MetricName
and O
ROUGE B-MetricName
- I-MetricName
L I-MetricName
) O
. O

Interestingly O
, O
compared O
with O
λ B-HyperparameterName
= O
1.0 B-HyperparameterValue
, O
the O
performance O
of O
the O
teacher O
with B-HyperparameterName
λ I-HyperparameterName
= O
2.0 B-HyperparameterValue
is O
worse O
, O
but O
the O
resulting O
student O
is O
much O
better O
( O
see O
Table O
2 O
) O
. O

Perhaps O
not O
surprisingly O
, O
the O
styles O
of O
summaries O
from O
students O
are O
similar O
with O
these O
from O
their O
teachers O
. O

Concise O
and O
abstractive O
teachers O
lead O
to O
concise O
and O
abstractive O
students O
( O
see O
Table O
6 O
) O
. O

Conciseness O
and O
abstractiveness O
are O
good O
properties O
for O
summarization B-TaskName
, O
which O
however O
may O
not O
be O
the O
case O
for O
other O
generation O
tasks O
such O
as O
machine B-TaskName
translation I-TaskName
. O

We O
apply O
PLATE B-MethodName
to O
the O
WMT16 B-DatasetName
( O
Bojar O
et O
al O
. O
, O
2016 O
) O
English B-TaskName
- I-TaskName
German I-TaskName
translation I-TaskName
task O
and O
use O
Transformer B-MethodName
- I-MethodName
big I-MethodName
as O
the O
teacher O
and O
Transformer B-MethodName
- I-MethodName
base I-MethodName
as O
the O
student O
. O

With O
λ B-HyperparameterName
= O
1.5 B-HyperparameterValue
, O
we O
obtain O
a O
BLEU B-MetricName
of O
27.90 B-MetricValue
, O
while O
the O
result O
of O
the O
regular O
pseudo O
- O
labeling O
is O
27.79 B-MetricValue
( O
more O
details O
are O
in O
Appendix O
A O
) O
. O

Comparison O
with O
sampling O
and O
tuning O
output O
layer O
temperature O
Sampling O
based O
methods O
can O
produce O
more O
diverse O
and O
richer O
outputs O
than O
its O
beam O
search O
based O
counterpart O
and O
has O
been O
proven O
useful O
in O
back B-TaskName
translation I-TaskName
( O
Edunov O
et O
al O
. O
, O
2018 O
) O
. O

We O
implement O
the O
sampling O
method O
in O
Edunov O
et O
al O
. O
( O
2018 O
) O
and O
Nucleus O
Sampling O
( O
Holtzman O
et O
al O
. O
, O
2019 O
) O
, O
a O
more O
advanced O
sampling O
method O
, O
to O
generate O
pseudo O
labels O
for O
distillation O
. O

We O
use O
the O
BART B-MethodName
12 I-MethodName
- I-MethodName
6 I-MethodName
as O
the O
student O
model O
, O
and O
the O
distillation O
results O
on O
CNNDM B-DatasetName
are O
in O
Table O
5 O
. O

As O
can O
be O
seen O
, O
both O
of O
the O
sampling O
based O
methods O
above O
perform O
worse O
than O
the O
regular O
beam O
search O
based O
pseudolabeling O
method O
( O
Regular O
) O
, O
let O
alone O
ours O
. O

Besides O
the O
attention B-HyperparameterName
temperatures I-HyperparameterName
, O
we O
can O
also O
tune O
the O
temperature B-HyperparameterName
T B-HyperparameterName
in O
the O
decoder O
output O
softmax O
layer O
. O

With O
a O
proper O
T B-HyperparameterName
( O
i.e. O
, O
T B-HyperparameterName
= O
0.5 B-HyperparameterValue
) O
during O
pseudo O
label O
generation O
, O
the O
resulting O
student O
model O
slightly O
outperforms O
the O
baseline O
student O
model O
with O
regular O
pseudo O
labeling O
method O
on O
ROUGE-2 B-MetricName
/ O
L B-MetricName
( O
see O
Table O
5 O
) O
, O
but O
worse O
than O
PLATE B-MethodName
λ=2.0 I-MethodName
. O

More O
results O
with O
different O
T B-HyperparameterName
s O
are O
in O
Appendix O
C.Why O
does O
our O
distillation O
method O
work O
? O
To O
answer O
this O
question O
, O
we O
first O
try O
to O
analyze O
the O
reasons O
from O
both O
the O
external O
characteristics O
of O
the O
summaries O
generated O
by O
the O
teacher O
model O
and O
the O
internal O
characteristics O
of O
the O
teacher O
's O
attention O
mechanism O
. O

Then O
, O
we O
will O
give O
an O
in O
- O
depth O
explanation O
. O

Ablation O
study O
In O
a O
Transformer O
, O
there O
are O
three O
types O
of O
attention O
modules O
( O
i.e. O
, O
encoder O
selfattention O
, O
decoder O
self O
- O
attention O
and O
decoder O
crossattention O
) O
, O
and O
we O
can O
scale O
attention B-HyperparameterName
temperatures I-HyperparameterName
for O
all O
of O
them O
or O
some O
of O
them O
. O

Let O
λ B-HyperparameterName
enc I-HyperparameterName
, O
λ B-HyperparameterName
cross I-HyperparameterName
, O
and O
λ B-HyperparameterName
dec I-HyperparameterName
denote O
the O
attention B-HyperparameterName
temperature I-HyperparameterName
coefficient I-HyperparameterName
of O
the O
encoder O
self O
- O
attention O
module O
, O
the O
decoder O
cross O
- O
attention O
module O
, O
and O
the O
decoder O
self O
- O
attention O
module O
, O
respectively O
. O

As O
shown O
in O
Table O
4 O
, O
using O
large O
attention B-HyperparameterName
temperature I-HyperparameterName
coefficients I-HyperparameterName
( O
2.0 B-HyperparameterValue
) O
for O
all O
three O
types O
of O
attention O
modules O
leads O
to O
the O
best O
result O
. O

When O
setting O
the O
coefficient B-HyperparameterName
of O
the O
cross O
attention O
module O
to B-HyperparameterName
λ I-HyperparameterName
cross I-HyperparameterName
= O
1.0 B-HyperparameterValue
, O
the O
ROUGE B-MetricName
scores O
drop O
most O
. O

Perhaps O
this O
is O
not O
surprising O
, O
since O
cross O
attentions O
are O
directly O
related O
to O
the O
selection O
of O
document O
contents O
for O
summarization B-TaskName
. O

Besides O
, O
the O
attention B-HyperparameterName
temperature I-HyperparameterName
of O
the O
decoder O
self O
- O
attention O
is O
also O
crucial O
but O
not O
as O
important O
as O
the O
cross O
- O
attention O
( O
see O
the O
fourth O
row O
) O
. O

Results O
with O
the O
Transformer B-MethodName
student O
( O
the O
sixth O
block O
) O
follow O
a O
similar O
trend O
, O
although O
the O
improvements O
are O
smaller O
. O

It O
may O
because O
the O
model- O

In O
the O
fifth O
block O
, O
we O
additionally O
conduct O
selfdistillation O
experiments O
, O
which O
is O
not O
the O
focus O
of O
this O
work O
. O

Our O
method O
improves O
the O
teacher O
model O
on O
CNNDM B-DatasetName
; O
ROUGE-2 B-MetricName
/ O
L B-MetricName
scores O
are O
improved O
on O
XSum B-DatasetName
; O
while O
on O
NYT B-DatasetName
, O
there O
are O
improvements O
on O
ROUGE-1 B-MetricName
/ O
L. B-MetricName

Results O
of O
our O
BART B-MethodName
12 I-MethodName
- I-MethodName
3 I-MethodName
and O
BART B-MethodName
12 I-MethodName
- I-MethodName
6 I-MethodName
student O
models O
are O
in O
the O
third O
and O
fourth O
block O
. O

We O
present O
results O
of O
students O
trained O
with O
gold O
labels O
( O
Gold O
) O
and O
regular O
pseudo O
labels O
( O
Regular O
) O
as O
well O
as O
pseudo O
labels O
with O
higher O
and O
random O
attention O
temperatures O
( O
PLATE B-MethodName
B12 I-MethodName
- I-MethodName
3 I-MethodName
λ=1.5 I-MethodName
, O
PLATE B-MethodName
B12 I-MethodName
- I-MethodName
3 I-MethodName
λ=2.0 I-MethodName
and O
PLATE B-MethodName
B12 I-MethodName
- I-MethodName
3 I-MethodName
rnd I-MethodName
) O
. O

PLATE B-MethodName
B12 I-MethodName
- I-MethodName
3 I-MethodName
λ=1.5 I-MethodName
means O
that O
the O
student O
uses O
attention B-HyperparameterName
temperature I-HyperparameterName
coefficient I-HyperparameterName
λ B-HyperparameterName
= O
1.5 B-HyperparameterValue
with O
architecture O
setting O
BART B-MethodName
12 I-MethodName
- I-MethodName
3 I-MethodName
. O

PLATE B-MethodName
B12 I-MethodName
- I-MethodName
3 I-MethodName
rnd I-MethodName
means O
that O
we O
use O
random O
attention B-HyperparameterName
temperature I-HyperparameterName
of O
λ B-HyperparameterName
∼ O
U O
[ O
1.0 B-HyperparameterValue
, O
2.0 B-HyperparameterValue
] O
. O

We O
observe O
that O
using O
pseudo O
- O
labeling O
methods O
with O
higher O
attention B-HyperparameterName
temperatures I-HyperparameterName
consistently O
improves O
over O
its O
counterpart O
with O
normal O
attention B-HyperparameterName
temperatures I-HyperparameterName
( O
Regular O
) O
across O
all O
three O
datasets O
, O
and O
the O
differences O
between O
them O
are O
almost O
always O
significant O
measured O
with O
the O
ROUGE B-MetricName
script O
5 O
( O
see O
details O
in O
Table O
2 O
) O
. O

Interestingly O
, O
our O
student O
models O
PLATE B-MethodName
B12 I-MethodName
- I-MethodName
3 I-MethodName
λ=2.0 I-MethodName
and O
PLATE B-MethodName
B12 I-MethodName
- I-MethodName
6 I-MethodName
λ=2.0 I-MethodName
outperform O
all O
models O
in O
comparison O
( O
including O
student O
models O
and O
even O
the O
teacher O
model O
) O
on O
CNNDM B-DatasetName
. O

Our O
best O
performing O
student O
model O
PLATE B-MethodName
B12 I-MethodName
- I-MethodName
3 I-MethodName
λ=1.5 I-MethodName
outperforms O
BART B-MethodName
- I-MethodName
PL I-MethodName
, O
BART B-MethodName
- I-MethodName
SFT I-MethodName
, O
and O
BART B-MethodName
- I-MethodName
KD I-MethodName
on O
XSum B-DatasetName
. O

Meanwhile O
, O
our O
method O
is O
conceptually O
simpler O
and O
can O
further O
be O
combined O
with O
their O
methods O
with O
additional O
train- O
ing O
objectives O
. O

In O
Section O
3.3 O
, O
we O
also O
propose O
a O
variant O
of O
our O
method O
, O
which O
employs O
random O
attention B-HyperparameterName
temperatures I-HyperparameterName
( O
PLATE B-MethodName
rnd I-MethodName
in O
Table O
2 O
) O
. O

We O
can O
see O
that O
though O
random O
temperature B-HyperparameterName
based O
method O
is O
not O
as O
good O
as O
our O
best O
fixed O
- O
temperature B-HyperparameterName
method O
, O
it O
in O
general O
produces O
decent O
results O
. O

Therefore O
, O
we O
recommend O
using O
this O
method O
when O
the O
computing O
budget O
is O
limited O
. O

Note O
that O
we O
also O
tried O
more O
extreme O
λ B-HyperparameterName
values O
as O
shown O
in O
Appendix O
B O
, O
and O
we O
find O
the O
value O
of O
1.5 B-HyperparameterValue
or O
2.0 B-HyperparameterValue
works O
better O
than O
others O
. O

The O
second O
block O
presents O
results O
of O
student O
models O
. O

Shleifer O
and O
Rush O
( O
2020 O
) O
compare O
pseudolabeling O
( O
BART B-MethodName
- I-MethodName
PL I-MethodName
) O
, O
knowledge O
distillation O
using O
both O
output O
and O
intermediate O
layers O
( O
BART B-MethodName
- I-MethodName
KD I-MethodName
) O
as O
well O
as O
shrink O
and O
fine O
- O
tuning O
( O
BART B-MethodName
- I-MethodName
SFT I-MethodName
) O
methods O
. O

They O
also O
use O
BART B-MethodName
as O
teacher O
models O
. O

Note O
their O
settings O
of O
student O
models O
are O
BART B-MethodName
12 I-MethodName
- I-MethodName
6 I-MethodName
on O
CNNDM B-DatasetName
and O
BART B-MethodName
12 I-MethodName
- I-MethodName
3 I-MethodName
on O
XSum B-DatasetName
. O

Summaries O
generated O
by O
abstractive O
models O
may O
be O
ungrammatical O
or O
unfaithful O
to O
the O
original O
document O
. O

Additionally O
, O
we O
also O
measure O
the O
quality O
of O
generated O
summaries O
by O
eliciting O
human O
judgements O
. O

We O
randomly O
sample O
50 O
documents O
from O
the O
test O
set O
of O
CNNDM B-DatasetName
. O

12 O
annotators O
are O
invited O
( O
they O
are O
either O
native O
English O
speakers O
or O
graduate O
students O
with O
IELTS O
test O
score O
over O
6.5 O
) O
. O

In O
the O
evaluation O
, O
participants O
are O
presented O
with O
a O
document O
and O
a O
list O
of O
outputs O
by O
different O
models O
. O

First O
, O
they O
are O
asked O
to O
evaluate O
the O
summaries O
on O
three O
dimensions O
: O
fluency O
( O
is O
the O
summary O
grammatically O
correct O
? O
) O
, O
faithfulness O
( O
is O
the O
summary O
faithful O
to O
the O
original O
document O
? O
) O
, O
and O
coverage O
( O
does O
the O
summary O
coverage O
important O
information O
of O
the O
document O
? O
) O
. O

Then O
, O
they O
are O
asked O
to O
rank O
the O
summaries O
from O
best O
to O
worst O
as O
a O
way O
of O
determining O
the O
overall O
quality O
of O
summaries O
. O

Each O
document O
is O
ensured O
to O
be O
annotated O
by O
3 O
different O
subjects O
. O

Our O
main O
results O
are O
shown O
in O
Table O
2 O
. O

The O
first O
block O
includes O
several O
recent O
abstractive O
summarization O
models O
based O
on O
large O
pre O
- O
trained O
Transformers O
. O

BERTSUM B-MethodName
( O
Liu O
and O
Lapata O
, O
2019 O
) O
employs O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
as O
its O
encoder O
and O
uses O
randomly O
initialized O
decoder O
. O

T5 B-MethodName
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
, O
PEGASUS B-MethodName
( O
Zhang O
et O
al O
. O
, O
2020 O
) O
and O
BART B-MethodName
( O
Lewis O
et O
al O
. O
, O
2020 O
) O
are O
three O
popular O
large O
Seq2Seq O
Transformer O
models O
with O
different O
pretraining O
objectives O
. O

Our O
own O
fine O
- O
tuning O
version O
of O
BART B-MethodName
( O
BART B-MethodName
( I-MethodName
ours I-MethodName
) I-MethodName
) O
is O
comparable O
or O
slightly O
better O
than O
the O
original O
reported O
BART B-MethodName
results O
, O
and O
we O
use O
it O
as O
the O
teacher O
model O
on O
the O
three O
datasets O
. O

During O
inference O
, O
as O
common O
wisdom O
, O
we O
apply O
beam O
search O
. O

The O
beam B-HyperparameterName
size I-HyperparameterName
, O
length B-HyperparameterName
penalty I-HyperparameterName
, O
and O
minimal B-HyperparameterName
length I-HyperparameterName
are O
4 B-HyperparameterValue
, O
2.0 B-HyperparameterValue
, O
and O
55 B-HyperparameterValue
on O
CNNDM B-DatasetName
; O
6 B-HyperparameterValue
, O
0.1 B-HyperparameterValue
, O
and O
1 B-HyperparameterValue
on O
XSum B-DatasetName
; O
and O
4 B-HyperparameterValue
, O
0.7 B-HyperparameterValue
, O
and O
80 B-HyperparameterValue
on B-DatasetName
NYT I-DatasetName
, O
respectively O
. O

All O
our O
models O
are O
trained O
on O
8 O
NVIDIA O
V100 O
GPUs O
. O

The O
training O
is O
fairly O
fast O
. O

Training O
on O
CNNDM B-DatasetName
with O
the O
teacher O
model O
( O
i.e. O
, O
BART B-MethodName
) O
is O
most O
time O
- O
consuming O
. O

It O
takes O
about O
45 O
minutes O
for O
one O
epoch O
, O
and O
we O
need O
6 B-HyperparameterValue
epochs O
in O
total O
. O

We O
evaluate O
the O
quality O
of O
different O
summarization O
systems O
using O
ROUGE B-MetricName
. O

On O
CNNDM B-DatasetName
and O
XSum B-DatasetName
datasets O
, O
we O
report O
full B-MetricName
- I-MetricName
length I-MetricName
F1 I-MetricName
based O
ROUGE-1 B-MetricName
( O
R1 B-MetricName
) O
, O
ROUGE-2 B-MetricName
( O
R2 B-MetricName
) O
, O
and O
ROUGE B-MetricName
- I-MetricName
L I-MetricName
( O
RL B-MetricName
) O
scores O
. O

Following O
Durrett O
et O
al O
. O
( O
2016 O
) O
; O
Liu O
and O
Lapata O
( O
2019 O
) O
, O
we O
report O
limited O
- O
length O
recall O
based O
ROUGE-1 B-MetricName
, O
ROUGE-2 B-MetricName
, O
and O
ROUGE B-MetricName
- I-MetricName
L I-MetricName
, O
where O
generated O
summaries O
are O
truncated O
to O
the O
lengths O
of O
gold O
summaries O
. O

All O
ROUGE B-MetricName
scores O
are O
computed O
using O
the O
ROUGE-1.5.5.pl O
script O
4 O
. O

Training O
and O
inference O
Hyper O
- O
parameters O
for O
BART B-MethodName
, O
BART B-MethodName
12 I-MethodName
- I-MethodName
6 I-MethodName
, O
BART B-MethodName
12 I-MethodName
- I-MethodName
3 I-MethodName
, O
and O
BART B-MethodName
12 I-MethodName
- I-MethodName
12 I-MethodName
are O
similar O
. O

Specifically O
, O
all O
models O
are O
optimized O
using O
Adam O
( O
Kingma O
and O
with O
β B-HyperparameterName
1 I-HyperparameterName
= O
0.9 B-HyperparameterValue
, O
β B-HyperparameterName
2 I-HyperparameterName
= O
0.999 B-HyperparameterValue
. O

Learning B-HyperparameterName
rates I-HyperparameterName
are O
tuned O
on O
validation O
sets O
( O
choose O
from O
1e-5 B-HyperparameterValue
, O
3e-5 B-HyperparameterValue
, O
5e-5 B-HyperparameterValue
, O
7e-5 B-HyperparameterValue
) O
. O

We O
truncate O
all O
documents O
and O
summaries O
to O
1024 B-HyperparameterValue
sub O
- O
word O
tokens O
. O

We O
use O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
around O
80 B-HyperparameterValue
documents O
( O
we O
limit O
the O
max B-HyperparameterName
number I-HyperparameterName
of I-HyperparameterName
tokens I-HyperparameterName
on O
each O
GPU O
to O
2048 B-HyperparameterValue
) O
and O
train O
our O
models O
for O
20,000/15,000/6,000 B-HyperparameterValue
steps O
with O
500 B-HyperparameterValue
warmup O
steps O
for O
CNNDM B-DatasetName
, O
XSum B-DatasetName
, O
and O
NYT B-DatasetName
, O
respectively O
. O

We O
also O
employ O
a O
weight B-HyperparameterName
decay I-HyperparameterName
of O
0.01 B-HyperparameterValue
. O

For O
Transformer B-MethodName
, O
the O
hyperparameters O
of O
the O
Adam O
optimizer O
is O
a O
bit O
different O
, O
and O
we O
use O
β B-HyperparameterName
1 I-HyperparameterName
= O
0.9 B-HyperparameterValue
, O
β B-HyperparameterName
2 I-HyperparameterName
= O
0.98 B-HyperparameterValue
. O

Learning B-HyperparameterName
rates I-HyperparameterName
are O
picked O
from O
1e-4 B-HyperparameterValue
, O
3e-4 B-HyperparameterValue
, O
5e-4 B-HyperparameterValue
, O
7e-4 B-HyperparameterValue
accord O
- O
ing O
to O
validation O
sets O
. O

The O
weight B-HyperparameterName
decay I-HyperparameterName
is O
set O
to O
0.0001 B-HyperparameterValue
. O

The O
warmup B-HyperparameterName
step I-HyperparameterName
we O
use O
is O
4000 B-HyperparameterValue
. O

We O
train B-MethodName
Transformer I-MethodName
for O
100 B-HyperparameterValue
epochs O
and O
select O
the O
best O
model O
w.r.t O
. O

their O
ROUGE B-MetricName
scores O
on O
validation O
sets O
. O

For O
all O
models O
above O
we O
apply O
a O
label B-HyperparameterName
smoothing I-HyperparameterName
of O
0.1 B-HyperparameterValue
to O
prevent O
overfitting O
( O
Pereyra O
et O
al O
. O
, O
2017 O
) O
. O

The O
hidden B-HyperparameterName
size I-HyperparameterName
of O
each O
layer O
is O
1024 B-HyperparameterValue
, O
and O
each O
layer O
contains O
16 B-HyperparameterValue
attention O
heads O
with O
a O
hidden B-HyperparameterName
size I-HyperparameterName
of O
64 B-HyperparameterValue
. O

We O
have O
four O
kinds O
of O
student O
models O
. O

The O
first O
three O
student O
models O
are O
initialized O
from O
BART B-MethodName
weights O
( O
therefore O
, O
their O
hidden O
sizes O
are O
the O
same O
as O
that O
of O
BART B-MethodName
) O
. O

All O
the O
three O
students O
have O
the O
12 B-HyperparameterValue
layers O
of O
BART B-MethodName
encoder O
and O
differ O
in O
the O
number O
of O
decoder O
layers O
. O

They O
are O
denoted O
by O
BART B-MethodName
12 I-MethodName
- I-MethodName
6 I-MethodName
, O
BART B-MethodName
12 I-MethodName
- I-MethodName
3 I-MethodName
, O
and O
BART B-MethodName
12 I-MethodName
- I-MethodName
12 I-MethodName
with O
6 B-HyperparameterValue
, O
3 B-HyperparameterValue
, O
and O
12 B-HyperparameterValue
decoder O
layers O
, O
respectively O
. O

For O
BART B-MethodName
12 I-MethodName
- I-MethodName
6 I-MethodName
( O
or O
BART B-MethodName
12 I-MethodName
- I-MethodName
3 I-MethodName
) O
, O
the O
decoder O
is O
initialized O
from O
the O
first O
6 B-HyperparameterValue
( O
or O
3 B-HyperparameterValue
) O
layers O
or O
the O
maximally O
spaced O
6 B-HyperparameterValue
( O
or O
3 B-HyperparameterValue
) O
layers O
of O
BART B-MethodName
decoder O
. O

The O
fourth O
student O
is O
the O
Transformer B-MethodName
base O
model O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
which O
has O
6 O
layers O
in O
each O
of O
the O
encoder O
and O
decoder O
. O

Each O
layer O
has O
a O
hidden B-HyperparameterName
size I-HyperparameterName
of O
512 B-HyperparameterValue
and O
8 O
attention O
heads O
. O

This O
student O
is O
randomly O
initialized O
and O
denoted O
by O
Transformer O
. O

The O
latency O
statistics O
( O
Milliseconds O
) O
and O
numbers O
of O
parameters O
of O
above O
four O
models O
are O
in O
Table O
1 O
. O

NYT B-DatasetName
The O
New B-DatasetName
York I-DatasetName
Times I-DatasetName
dataset O
( O
NYT B-DatasetName
; O
Sandhaus O
, O
2008 O
) O
is O
composed O
of O
articles O
published O
by O
New O
York O
Times O
, O
and O
the O
summaries O
are O
written O
by O
library O
scientists O
. O

After O
applying O
the O
pre O
- O
processing O
procedures O
described O
in O
Durrett O
et O
al O
. O
( O
2016 O
) O
; O
Liu O
and O
Lapata O
( O
2019 O
) O
, O
we O
first O
obtain O
110,540 O
articles O
with O
abstractive O
summaries O
. O

The O
test O
set O
is O
constructed O
by O
including O
the O
9,076 O
articles O
published O
after O
January O
1 O
, O
2007 O
. O

The O
remaining O
100,834 O
articles O
are O
further O
split O
into O
training O
and O
validation O
sets O
. O

After O
removing O
articles O
with O
summaries O
less O
than O
50 O
words O
, O
we O
obtain O
the O
final O
dataset O
with O
38,264 O
articles O
for O
training O
; O
4,002 O
articles O
for O
validation O
; O
and O
3,421 O
articles O
for O
test O
. O

Teacher O
/ O
Student O
model O
settings O
We O
use O
BART B-MethodName
Large I-MethodName
( O
Lewis O
et O
al O
. O
, O
2020 O
) O
as O
our O
teacher O
model O
, O
which O
has O
12 B-HyperparameterValue
layers O
in O
the O
encoder O
and O
decoder O
. O

We O
find O
that O
λ B-HyperparameterName
= O
1.5 B-HyperparameterValue
or O
λ B-HyperparameterName
= O
2.0 B-HyperparameterValue
usually O
works O
well O
in O
practice O
. O

To O
encourage O
teacher O
models O
to O
generate O
pseudo O
labels O
with O
more O
diversity O
, O
we O
further O
propose O
to O
use O
a O
random O
λ B-HyperparameterName
for O
each O
input O
document O
( O
λ B-HyperparameterName
∼ O
U O
[ O
a B-HyperparameterName
, O
b B-HyperparameterName
] O
) O
. O

Note O
that O
U O
[ O
a B-HyperparameterName
, O
b B-HyperparameterName
] O
is O
a O
uniform O
distribution O
and O
we O
typically O
set O
a B-HyperparameterName
= O
1.0 B-HyperparameterValue
and O
b B-HyperparameterName
= O
2.0.We B-HyperparameterValue
conduct O
our O
experiments O
on O
three O
popular O
document O
summarization O
datasets O
: O
CNN B-DatasetName
/ I-DatasetName
DailyMail I-DatasetName
( O
Hermann O
et O
al O
. O
, O
2015 O
) O
, O
XSum B-DatasetName
( O
Narayan O
et O
al O
. O
, O
2018 O
) O
, O
and O
New B-DatasetName
York I-DatasetName
Times I-DatasetName
( O
Sandhaus O
, O
2008 O
) O
. O

All O
datasets O
are O
tokenized O
with O
the O
GPT-2 B-MethodName
tokenizer O
( O
Radford O
et O
al O
. O
, O
2019 O
) O
, O
which O
is O
based O
on O
UTF-8 B-MethodName
BPE I-MethodName
( O
Sennrich O
et O
al O
. O
, O
2016).CNN B-DatasetName
/ I-DatasetName
DailyMail I-DatasetName
dataset O
( O
CNNDM B-DatasetName
; O
Hermann O
et O
al O
. O
, O
2015 O
) O
contains O
online O
news O
articles O
from O
the O
CNN O
and O
DailyMail O
websites O
paired O
with O
their O
associated O
highlights O
as O
reference O
summaries O
. O

We O
follow O
the O
standard O
pre O
- O
processing O
steps O
described O
in O
See O
et O
al O
. O
( O
2017 O
) O
; O
Liu O
and O
Lapata O
( O
2019 O
) O
. O

3 O
The O
resulting O
numbers O
of O
document O
- O
summary O
pairs O
for O
training O
, O
validation O
, O
and O
test O
are O
287,227 O
, O
13,368 O
, O
and O
11,490 O
, O
respectively O
. O

XSum B-DatasetName
The O
XSum B-DatasetName
dataset O
is O
collected O
by O
harvesting O
online O
articles O
from O
the O
BBC O
with O
single O
sentence O
summaries O
, O
which O
is O
professionally O
written O
. O

The O
summaries O
are O
extremely O
abstractive O
. O

We O
use O
the O
official O
splits O
of O
Narayan O
et O
al O
. O
( O
2018 O
) O
. O

There O
are O
204,045 O
articles O
for O
training O
; O
11,332 O
articles O
for O
validation O
; O
and O
11,334 O
articles O
for O
test O
. O

Our O
distillation O
method O
PLATE O
works O
as O
follows O
. O

Assume O
we O
have O
a O
teacher O
model O
trained O
with O
τ O
= O
√ O
d. O
When O
the O
teacher O
generates O
pseudo O
labels O
with O
beam O
search O
, O
we O
use O
a O
higher O
attention O
temperature O
and O
set O
τ O
= O
√ O
λ B-HyperparameterName
d O
where O
λ B-HyperparameterName
> O
1 B-HyperparameterValue
( O
λ B-HyperparameterName
is O
the O
attention B-HyperparameterName
temperature I-HyperparameterName
coefficient I-HyperparameterName
) O
. O

Note O
that O
we O
only O
change O
the O
teacher O
's O
attention O
temperature O
during O
inference O
time O
. O

When O
we O
train O
our O
student O
model O
with O
pseudo O
labels O
, O
we O
still O
use O
a O
normal O
temperature O
( O
i.e. O
, O
τ O
= O
√ O
d O
) O
. O

We O
find O
that O
adjusting O
the O
student O
's O
attention O
temperature O
does O
not O
work O
. O

Probably O
because O
the O
student O
can O
easily O
adapt O
to O
the O
scaled O
attention O
temperature O
during O
training O
. O

There O
is O
an O
interesting O
line O
of O
work O
called O
selfdistillation O
or O
self O
- O
training O
( O
Furlanello O
et O
al O
. O
, O
2018;Xie O
et O
al O
. O
, O
2020;Deng O
et O
al O
. O
, O
2009;He O
et O
al O
. O
, O
2019 O
) O
, O
where O
the O
size O
of O
the O
student O
model O
is O
identical O
to O
the O
size O
of O
the O
teacher O
model O
. O

Our O
method O
can O
also O
be O
applied O
in O
selfdistillation O
and O
can O
potentially O
be O
combined O
with O
the O
self O
- O
distillation O
methods O
above O
. O

Abstractive B-TaskName
summarization I-TaskName
aims O
to O
rewrite O
a O
document O
into O
its O
shorter O
form O
( O
i.e. O
, O
summary O
) O
, O
which O
is O
a O
typical O
Seq2Seq O
learning O
problem O
. O

We O
adopt O
the O
Seq2Seq O
Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
model O
. O

Given O
a O
document O
X O
= O
( O
x O
1 O
, O
x O
2 O
, O
. O

. O

. O

, O
x O
|X| O
) O
and O
its O
gold O
summary O
Y O
= O
( O
y O
1 O
, O
y O
2 O
, O
. O

. O

. O

, O
y O
|Y O
| O
) O
, O
we O
estimate O
the O
following O
conditional O
probability O
: O

In O
Seq2Seq O
learning O
tasks O
such O
as O
summarization B-TaskName
, O
we O
can O
apply O
distillation O
methods O
above O
to O
each O
step O
of O
sequence O
model O
predictions O
. O

However O
, O
the O
sequence O
- O
level O
knowledge O
of O
teacher O
mod O
- O
els O
is O
not O
well O
utilized O
. O

Therefore O
, O
Kim O
and O
Rush O
( O
2016 O
) O
introduce O
a O
sequence O
- O
level O
knowledge O
distillation O
method O
( O
i.e. O
, O
pseudo O
- O
labeling O
) O
, O
where O
a O
student O
model O
is O
trained O
with O
pseudo O
labels O
generated O
by O
the O
teacher O
model O
using O
beam O
search O
decoding O
. O

Kim O
and O
Rush O
( O
2016 O
) O
and O
later O
work O
( O
Kasai O
et O
al O
. O
, O
2020;Gu O
et O
al O
. O
, O
2017;Denkowski O
and O
Neubig O
, O
2017 O
) O
show O
pseudo O
- O
labeling O
achieves O
competitive O
performance O
for O
Seq2Seq O
tasks O
such O
as O
machine O
translation O
. O

Shleifer O
and O
Rush O
( O
2020 O
) O
propose O
the O
shrink O
and O
fine O
- O
tune O
( O
SFT O
) O
approach O
for O
pre O
- O
trained O
summarization O
distillation O
, O
which O
re O
- O
finetunes O
a O
teacher O
model O
with O
some O
layers O
removed O
, O
and O
they O
show O
SFT O
outperforms O
pseudo O
- O
labeling O
and O
a O
modification O
of O
direct O
knowledge O
distillation O
( O
Jiao O
et O
al O
. O
, O
2020 O
) O
on O
one O
of O
their O
datasets O
, O
but O
not O
others O
. O

Our O
method O
, O
which O
builds O
on O
top O
of O
pseudo O
- O
labeling O
, O
is O
conceptually O
simple O
and O
improves O
pseudo O
- O
labeling O
across O
different O
summarization B-TaskName
datasets O
. O

Experiments O
on B-DatasetName
CNN I-DatasetName
/ I-DatasetName
DailyMail I-DatasetName
, O
XSum B-DatasetName
, O
and O
New B-DatasetName
York I-DatasetName
Times I-DatasetName
datasets O
with O
student O
models O
of O
different O
sizes O
show O
PLATE B-MethodName
consistently O
outperforms O
vanilla O
pseudo O
- O
labeling O
methods O
. O

Further O
empirical O
analysis O
shows O
that O
, O
with O
PLATE B-MethodName
, O
both O
pseudo O
summaries O
generated O
by O
teacher O
models O
and O
summaries O
generated O
by O
student O
models O
are O
shorter O
and O
more O
abstractive O
, O
which O
matches O
the O
goal O
of O
abstractive O
summarization O
. O

Large O
pre O
- O
trained O
Seq2Seq O
Transformer O
models O
largely O
improve O
results O
of O
generation O
tasks O
including O
text B-TaskName
summarization I-TaskName
( O
Song O
et O
al O
. O
, O
2019;Lewis O
et O
al O
. O
, O
2020;Raffel O
et O
al O
. O
, O
2020 O
; O
Token O
index O
in O
summary O
Zhang O
et O
al O
. O
, O
2020 O
) O
. O

These O
models O
are O
pre O
- O
trained O
using O
unsupervised O
text O
- O
to O
- O
text O
objectives O
. O

For O
example O
, O
T5 B-MethodName
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
is O
pre O
- O
trained O
by O
predicting O
corrupted O
text O
spans O
. O

BART B-MethodName
( O
Lewis O
et O
al O
. O
, O
2020 O
) O
employs O
denoising O
auto O
- O
encoding O
objectives O
such O
as O
text O
infilling O
and O
sentence O
permutation O
during O
its O
pre O
- O
training O
. O

The O
pre O
- O
training O
objective O
of O
PEGASUS B-MethodName
( O
Zhang O
et O
al O
. O
, O
2020 O
) O
is O
tailored O
for O
the O
summarization O
task O
, O
which O
predicts O
the O
most O
" O
summary O
worthy O
" O
sentences O
in O
a O
document O
. O

Our O
method O
aims O
to O
make O
these O
large O
models O
faster O
. O

In O
knowledge O
distillation O
, O
besides O
learning O
from O
gold O
labels O
in O
the O
training O
set O
, O
student O
models O
can O
learn O
from O
soft O
targets O
( O
Ba O
and O
Caruana O
, O
2014;Hinton O
et O
al O
. O
, O
2015 O
) O
, O
intermediate O
hidden O
states O
( O
Romero O
et O
al O
. O
, O
2014 O
) O
, O
attentions O
( O
Zagoruyko O
and O
Komodakis O
, O
2017 O
; O
, O
and O
target O
output O
derivatives O
( O
Czarnecki O
et O
al O
. O
, O
2017 O
) O
of O
teacher O
models O
. O

Recent O
work O
for O
distillation O
of O
pre O
- O
trained O
Transformers O
( O
e.g. O
, O
DistilBERT B-MethodName
( O
Sanh O
et O
al O
. O
, O
2019 O
) O
, O
TinyBERT B-MethodName
( O
Jiao O
et O
al O
. O
, O
2020 O
) O
, O
Mobile B-MethodName
- I-MethodName
BERT I-MethodName
( O
Sun O
et O
al O
. O
, O
2020 O
) O
, O
BERT B-MethodName
- I-MethodName
of I-MethodName
- I-MethodName
Theseus I-MethodName
( O
Xu O
et O
al O
. O
, O
2020a O
) O
, O
MINILM B-MethodName
) O
focuses O
on O
natural O
language O
understanding O
tasks O
such O
as O
GLUE B-DatasetName
( O
Wang O
et O
al O
. O
, O
2018 O
) O
or O
SQuAD B-DatasetName
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
benchmarks O
. O

Most O
methods O
above O
are O
designed O
for O
classification O
models O
. O

Based O
on O
the O
observations O
above O
, O
we O
propose O
a O
simple O
method O
called B-MethodName
PLATE I-MethodName
( O
as O
shorthand O
for O
Pseudo B-MethodName
- I-MethodName
labeling I-MethodName
with I-MethodName
Larger I-MethodName
Attention I-MethodName
TEmperature I-MethodName
) O
to O
smooth O
attention O
distributions O
of O
teacher O
models O
. O

Specifically O
, O
we O
re O
- O
scale O
attention O
weights O
in O
all O
attention O
modules O
with O
a O
higher O
temperature O
, O
which O
leads O
to O
softer O
attention O
distributions O
. O

Figure O
1 O
intuitively O
shows O
the O
effect O
of O
using O
higher O
attention O
temperatures O
. O

Compared O
with O
the O
left O
graph O
, O
the O
right O
graph O
with O
higher O
attention O
temperature O
has O
shorter O
lines O
( O
less O
copy O
bias O
) O
with O
high O
attention O
weights O
, O
and O
positions O
of O
high O
attention O
weights O
extend O
to O
the O
first O
450 O
words O
( O
less O
leading O
bias O
) O
. O

Less O
copy O
bias O
in O
pseudo O
summaries O
encourages O
student O
models O
to O
be O
more O
abstractive O
, O
while O
less O
leading O
bias O
in O
pseudo O
summaries O
encourages O
student O
models O
to O
take O
advantage O
of O
longer O
context O
in O
documents O
. O

In O
this O
paper O
, O
we O
argue O
that O
attention O
distributions O
of O
a O
Seq2Seq O
teacher O
model O
might O
be O
too O
sharp O
. O

As O
a O
result O
, O
pseudo O
labels O
generated O
from O
it O
are O
sub O
- O
optimal O
for O
student O
models O
. O

In O
the O
summarization B-TaskName
task O
, O
we O
observe O
that O
1 O
) O
pseudo O
summaries O
generated O
from O
our O
teacher O
model O
copy O
more O
continuous O
text O
spans O
from O
original O
documents O
than O
reference O
summaries O
( O
56 B-MetricValue
% I-MetricValue
4 B-MetricName
- I-MetricName
grams I-MetricName
in O
pseudo O
summaries O
and O
15 B-MetricValue
% I-MetricValue
4 B-MetricName
- I-MetricName
grams I-MetricName
in O
reference O
summaries O
are O
copied O
from O
their O
original O
documents O
on O
CNN B-DatasetName
/ I-DatasetName
DailyMail I-DatasetName
dataset O
) O
; O
2 O
) O
pseudo O
summaries O
tend O
to O
summarize O
the O
leading O
part O
of O
a O
document O
( O
measured O
on O
CNN B-DatasetName
/ I-DatasetName
DailyMail I-DatasetName
, O
74 O
% O
of O
sentences O
in O
pseudo O
summaries O
and O
64 O
% O
of O
sentences O
in O
reference O
summaries O
are O
from O
the O
leading O
40 O
% O
sentences O
in O
original O
documents O
) O
. O

We O
obtain O
the O
two O
numbers O
above O
by O
matching O
each O
sentence O
in O
a O
summary O
with O
the O
sentence O
in O
its O
original O
document O
that O
can O
produce O
maximum O
ROUGE B-MetricName
( O
Lin O
, O
2004 O
) O
score O
between O
them O
. O

We O
call O
the O
two O
biases O
above O
the O
copy O
bias O
and O
the O
leading O
bias O
. O

In O
order O
to O
have O
an O
intuitive O
feeling O
, O
we O
select O
a O
rep O
- O
resentative O
example O
1 O
and O
visualize O
its O
cross O
attention O
weights O
2 O
( O
see O
the O
left O
graph O
in O
Figure O
1 O
) O
. O

We O
observe O
that O
attention O
weights O
form O
three O
" O
lines O
" O
, O
which O
indicates O
very O
time O
the O
decoder O
predicts O
the O
next O
word O
, O
its O
attention O
points O
to O
the O
next O
word O
in O
the O
input O
document O
. O

That O
may O
be O
the O
reason O
why O
multiple O
continuous O
spans O
of O
text O
are O
copied O
. O

Another O
phenomenon O
we O
observe O
is O
that O
all O
high O
- O
value O
attention O
weights O
( O
in O
deeper O
color O
) O
concentrate O
on O
the O
first O
200 O
words O
in O
the O
input O
document O
, O
which O
reflects O
the O
leading O
bias O
. O

In O
either O
case O
, O
the O
attention O
distribution O
is O
too O
sharp O
( O
i.e. O
, O
attention O
weights O
of O
the O
next O
word O
position O
or O
the O
leading O
part O
is O
much O
larger O
than O
other O
positions O
) O
, O
which O
means O
our O
teacher O
model O
is O
over O
- O
confident O
. O

Knowledge O
distillation O
is O
a O
class O
of O
methods O
that O
leverage O
the O
output O
of O
a O
( O
large O
) O
teacher O
model O
to O
guide O
the O
training O
of O
a O
( O
small O
) O
student O
model O
. O

In O
classification O
tasks O
, O
it O
is O
typically O
done O
by O
minimizing O
the O
distance O
between O
the O
teacher O
and O
student O
predictions O
( O
Hinton O
et O
al O
. O
, O
2015 O
) O
. O

As O
to O
Seq2Seq O
models O
, O
an O
effective O
distillation O
method O
is O
called O
pseudo O
- O
labeling O
( O
Kim O
and O
Rush O
, O
2016 O
) O
, O
where O
the O
teacher O
model O
generates O
pseudo O
summaries O
for O
all O
documents O
in O
the O
training O
set O
and O
the O
resulting O
document O
- O
pseudo O
- O
summary O
pairs O
are O
used O
to O
train O
the O
student O
model O
. O

Automatic B-TaskName
document I-TaskName
summarization I-TaskName
is O
the O
task O
of O
rewriting O
a O
long O
document O
into O
its O
shorter O
form O
while O
still O
retaining O
its O
most O
important O
content O
. O

In O
the O
literature O
, O
there O
are O
mainly O
two O
kinds O
of O
methods O
for O
summarization B-TaskName
: O
extractive B-TaskName
summarization I-TaskName
and O
abstractive B-TaskName
summarization I-TaskName
( O
Nenkova O
and O
McKeown O
, O
2011 O
) O
. O

In O
this O
work O
, O
we O
focus O
on O
abstractive B-TaskName
summarization I-TaskName
, O
which O
is O
viewed O
as O
a O
sequence O
- O
tosequence O
( O
Seq2Seq O
) O
learning O
problem O
, O
since O
recent O
abstractive O
models O
outperform O
their O
extractive O
counterparts O
and O
can O
produce O
more O
concise O
summaries O
( O
Raffel O
et O
al O
. O
, O
2020;Lewis O
et O
al O
. O
, O
2020;Zhang O
et O
al O
. O
, O
2020;Liu O
and O
Lapata O
, O
2019 O
) O
. O

Recent O
progress O
of O
abstractive B-TaskName
summarization I-TaskName
largely O
relies O
on O
large O
pre O
- O
trained O
Transformer O
models O
( O
Raffel O
et O
al O
. O
, O
2020;Lewis O
et O
al O
. O
, O
2020;Zhang O
et O
al O
. O
, O
2020;Liu O
and O
Lapata O
, O
2019 O
; O
. O

With O
these O
extremely O
large O
models O
, O
we O
can O
obtain O
state O
- O
of O
- O
theart O
summarization O
results O
, O
but O
they O
are O
slow O
for O
online O
inference O
, O
which O
makes O
them O
difficult O
to O
be O
used O
in O
the O
production O
environment O
even O
with O
cutting O
- O
edge O
hardware O
. O

This O
paper O
aims O
to O
distill O
these O
large O
Transformer O
summarization O
models O
into O
smaller O
ones O
with O
minimal O
loss O
in O
performance O
. O

LAVA B-MethodName
( O
Lubis O
et O
al O
. O
, O
2020 O
) O
, O
reduces O
the O
action O
space O
of O
policy O
in O
end O
- O
to O
- O
end O
ToD B-TaskName
, O
by O
using O
the O
latent O
space O
of O
a O
variational O
model O
with O
an O
informed O
prior O
. O

The O
work O
use O
variable O
distribution O
: O
via O
pretraining O
, O
to O
obtain O
an O
informed O
prior O
, O
and O
uses O
autoencoding O
as O
the O
auxiliary O
task O
, O
to O
capture O
generative O
factors O
of O
dialogue O
responses O
. O

MinTL B-MethodName
- I-MethodName
BART I-MethodName
( O
Lin O
et O
al O
. O
, O
2020 O
) O
, O
introduced O
Levenshtein O
belief O
spans O
framework O
that O
predicts O
only O
the O
incremental O
change O
in O
dialogue O
state O
per O
turn O
. O

It O
leverages O
the O
pretrained O
T5 B-MethodName
and O
BART B-MethodName
( O
Lewis O
et O
al O
. O
, O
2019 O
) O
as O
backbone O
for O
model O
architecture O
. O

Greedy O
agent O
: O
In O
certain O
domains O
, O
the O
agents O
has O
a O
tendency O
to O
book O
a O
service O
before O
it O
has O
gathered O
all O
the O
required O
information O
or O
before O
the O
user O
requested O
or O
agreed O
for O
booking O
a O
service O
. O

The O
first O
example O
in O
Fig O
: O
9 O
demonstrate O
this O
behaviour O
. O

Here O
the O
user O
has O
requested O
for O
a O
taxi O
, O
before O
enough O
information O
such O
as O
destination O
or O
time O
of O
departure O
are O
gathered O
, O
the O
agent O
books O
the O
taxi O
. O

This O
happens O
because O
there O
are O
gaps O
in O
automatic O
evaluation O
metrics O
. O

A O
low O
BLEU B-MetricName
score O
and O
relatively O
high O
inform B-MetricName
and O
success B-MetricName
rate I-MetricName
might O
indicate O
greedy O
agent O
behaviour O
. O

Other O
reasons O
for O
low O
BLEU B-MetricName
score O
includes O
: O
lack O
of O
diversity O
in O
the O
responses O
or O
malformation O
of O
response O
. O

1 O
) O
Appropriateness B-MetricName
: O
Are O
the O
generated O
responses O
appropriate O
for O
the O
given O
context O
in O
the O
dialogue O
turn O
? O
2 O
) O
Fluency B-MetricName
: O
Are O
the O
generated O
responses O
coherent O
and O
comprehensible O
? O

This O
is O
very O
similar O
to O
combined O
score O
used O
in O
evaluation O
and O
both O
are O
equivalent O
when O
λ B-HyperparameterName
= O
2 B-HyperparameterValue
. O

We O
introduced O
hyperparamter O
λ B-HyperparameterName
to O
normalize O
the O
achievable O
scale O
of O
BLEU B-MetricName
. O

We O
observe O
that O
success B-MetricName
rate I-MetricName
, O
if O
used O
as O
is O
, O
will O
result O
in O
non O
- O
markovian O
and O
stochastic O
per O
turn O
reward O
function O
. O

This O
is O
because O
the O
reward O
of O
current O
state O
will O
depend O
on O
the O
performance O
of O
future O
states O
. O

Hence O
, O
we O
also O
use O
a O
soft O
version O
of O
the O
metric O
M B-MetricName
sof I-MetricName
t I-MetricName
, O
where O
the O
success B-MetricName
rate I-MetricName
measures O
a O
fraction O
of O
requested O
information O
provided O
in O
a O
dialogue O
. O

We O
refer O
the O
original O
metric O
that O
uses O
the O
discrete O
variant O
of O
success B-MetricName
rate I-MetricName
as O
M B-MetricName
hard I-MetricName
. O

The O
choice O
of O
action O
in O
reward O
function O
R(s O
t O
, O
a O
t O
, O
g O
) O
can O
either O
be O
dialogue B-TaskName
act I-TaskName
or O
generate B-TaskName
response I-TaskName
, O
we O
refer O
corresponding O
variants O
of O
metrics O
as O
M B-MetricName
( I-MetricName
act I-MetricName
) I-MetricName
and O
M B-MetricName
( I-MetricName
resp I-MetricName
) I-MetricName
. O

To O
demonstrate O
the O
versatility O
of O
our O
method O
to O
adapt O
to O
different O
metrics O
, O
we O
use O
all O
the O
discussed O
variants O
of O
the O
metric O
. O

We O
compare O
both O
adaptation O
of O
our O
methods O
CASPI(DAMD B-MethodName
) I-MethodName
and O
CASPI(MinTL B-MethodName
) I-MethodName
on O
the O
endto O
- O
end O
dialogue O
tasks O
defined O
by O
MultiWoz2.0 B-DatasetName
( O
Budzianowski O
et O
al O
. O
, O
2018a O
) O
. O

The O
results O
are O
tabulated O
at O
Table O
:1 O
. O

CASPI(DAMD B-MethodName
) I-MethodName
with O
its O
light O
weight O
model O
architecture O
and O
no O
pretraining O
on O
any O
external O
corpus O
, O
except O
for O
( O
Lubis O
et O
al O
. O
, O
2020 O
) O
, O
out O
perform O
all O
other O
previous O
methods O
, O
these O
includes O
methods O
that O
use O
large O
pretrained O
language O
models O
such O
as O
Hosseini O
- O
Asl O
et O
al O
. O
( O
2020 O
) O
, O
Peng O
et O
al O
. O
( O
2020 O
) O
and O
Lin O
et O
al O
. O
( O
2020 O
) O
. O

This O
show O
using O
CASPI B-MethodName
to O
shepard O
the O
gradient O
update O
process O
as O
sample O
weights O
for O
each O
dialogue O
turn O
leads O
to O
a O
model O
that O
's O
well O
aligned O
with O
true O
objective O
of O
the O
task O
. O

CASPI(MinTL B-MethodName
) I-MethodName
with O
its O
robust O
pretrained O
model O
out O
performs O
CASPI(DAMD B-MethodName
) I-MethodName
and O
LAVA B-MethodName
( O
Lubis O
et O
al O
. O
, O
2020 O
) O
by O
a O
large O
margin O
. O

This O
demonstrates O
the O
ease O
of O
adaptation O
of O
existing O
methods O
with O
CASPI.Inverse B-MethodName
reinforcement O
learning O
, O
coupled O
with O
offpolicy O
policy O
learning O
and O
evaluation O
are O
proven O
to O
be O
sample O
efficient O
( O
Thomas O
and O
Brunskill O
, O
2016 O
) O
. O

We O
argue O
CASPI B-MethodName
is O
competitive O
with O
other O
sample O
efficiency O
techniques O
, O
such O
as O
data B-MethodName
augmentation I-MethodName
and O
transfer B-MethodName
learning I-MethodName
as O
performed O
by O
Zhang O
et O
al O
. O
( O
2019 O
) O
and O
Lin O
et O
al O
. O
( O
2020 O
) O
respectively O
. O

To O
demonstrate O
the O
hypothesis O
, O
we O
test O
our O
method O
against O
baseline O
in O
a O
low O
sample O
complexity O
regime O
. O

For O
experimental O
setup O
, O
we O
adopt O
the O
low O
resource O
testing O
strategy O
from O
Lin O
et O
al O
. O
( O
2020 O
) O
. O

We O
train O
our O
model O
on O
5 O
% O
, O
10 O
% O
, O
and O
20 O
% O
of O
the O
training O
data O
and O
compared O
with O
other O
baselines O
on O
end O
- O
to O
- O
end O
dialogue O
task O
, O
Table O
2 O
list O
the O
results O
. O

CASPI(MinTL B-MethodName
) I-MethodName
trained O
only O
on O
20 O
% O
of O
data O
was O
able O
to O
out O
perform O
previous O
state O
of O
the O
art O
method O
, O
LAVA B-MethodName
( O
Lubis O
et O
al O
. O
, O
2020 O
) O
and O
MINTL B-MethodName
( O
Lin O
et O
al O
. O
, O
2020 O
) O
trained O
on O
100 O
% O
data O
on O
two O
of O
the O
three O
performance O
metrics O
. O

This O
goes O
to O
show O
that O
having O
the O
right O
reward O
function O
to O
guide O
the O
budget O
of O
the O
gradient O
update O
process O
to O
reach O
the O
true O
objective O
is O
important O
in O
extremely O
low O
resource O
setting O
. O

Automatic O
evaluation O
metrics O
have O
their O
own O
biases O
. O

True O
objective O
of O
ToD B-TaskName
is O
human B-MetricName
experience I-MetricName
while O
interacting O
with O
the O
dialogue O
systems O
, O
which O
automatic O
evaluation O
metrics O
might O
fall O
short O
to O
capture O
. O

To O
this O
end O
we O
conduct O
human O
evaluation O
on O
the O
quality O
of O
the O
generated O
response O
. O

We O
define O
quality O
by O
the O
following O
criterias O
: O

For O
the O
pairwise O
casual O
reward O
learning O
network O
, O
we O
use O
three B-HyperparameterValue
single O
bi O
- O
LSTM O
layers O
, O
one O
each O
to O
encode O
goal O
, O
belief O
state O
and O
either O
dialogue O
act O
or O
response O
sequences O
at O
each O
dialogue O
turn O
on O
each O
of O
the O
sampled O
roll O
- O
outs O
pairs O
, O
τ O
1 O
and O
τ O
2 O
. O

The O
three O
encoded O
representations O
are O
concatenate O
and O
are O
fed O
through O
a O
couple O
of O
feed O
- O
forward O
layers O
before O
making O
a O
bounded O
reward O
prediction O
R(s O
t O
, O
a O
t O
, O
g O
) O
∈ O
[ O
0 O
, O
1 O
] O
for O
each O
turn O
using O
a O
sigmoid O
function O
. O

The O
per O
turn O
rewards O
are O
summed O
to O
form O
a O
global O
reward O
R(τ O
) O
for O
the O
roll O
- O
out O
τ O
. O

Using O
a O
pair O
of O
dialogue O
rewards O
R(τ O
1 O
) O
and O
R(τ O
2 O
) O
, O
we O
compute O
the O
probabilistic O
preference O
between O
the O
roll O
- O
outs O
P O
[ O
τ O
1 O
≻ O
τ O
2 O
] O
either O
by O
standard O
normalization O
or O
a O
softmax O
function O
. O

The O
output O
of O
this O
optimized O
using O
binary O
crossentopy O
loss O
described O
in O
Eqn:4 O
. O

The O
above O
described O
architecture O
is O
illustrated O
in O
Fig O
: O
10 O
.To O
evaluate O
our O
proposed O
method O
on O
Multi B-DatasetName
- I-DatasetName
domain I-DatasetName
Wizard I-DatasetName
- I-DatasetName
of I-DatasetName
- I-DatasetName
Oz I-DatasetName
( O
MultiWoz B-DatasetName
) O
( O
Budzianowski O
et O
al O
. O
, O
2018a O
) O
dataset O
. O

It O
is O
a O
large O
scale O
multidomain O
, O
task O
oriented O
dataset O
generated O
by O
human O
- O
to O
- O
human O
conversation O
, O
where O
one O
participant O
plays O
the O
role O
of O
a O
user O
while O
the O
other O
plays O
the O
agent O
. O

The O
conversations O
are O
between O
a O
tourist O
and O
a O
clerk O
at O
an O
information O
center O
. O

The O
conversations O
span O
across O
7 O
domains O
including O
attraction O
, O
hospital O
, O
hotel O
, O
police O
, O
restaurant O
, O
taxi O
and O
train O
. O

Each O
dialogue O
is O
generated O
by O
users O
with O
a O
defined O
goal O
which O
may O
cover O
1 O
- O
5 O
domains O
with O
a O
maximum O
of O
13 O
turns O
in O
a O
conversation O
. O

The O
dataset O
has O
10438 O
dialogues O
split O
into O
8438 O
dialogues O
for O
training O
set O
and O
1000 O
dialogues O
each O
for O
validation O
and O
test O
set O
. O

We O
represent O
DB O
results O
as O
one O
- O
hot O
vectors O
as O
proposed O
by O
Budzianowski O
et O
al O
. O
( O
2018b O
) O
. O

To O
reduce O
surface O
- O
level O
variability O
in O
the O
responses O
, O
we O
use O
domain O
- O
adaptive O
delexicalization O
preprocess O
- O
ing O
proposed O
in O
Wen O
et O
al O
. O
( O
2016 O
) O
. O

As O
proposed O
in O
Zhang O
et O
al O
. O
( O
2019 O
) O
, O
We O
generate O
delexicalized O
responses O
with O
placeholders O
for O
specific O
values O
which O
can O
be O
filled O
with O
information O
in O
DST O
and O
database O
. O

We O
evaluate O
performance O
of O
our O
method O
on O
end O
- O
to O
- O
end O
dialogue O
modeling O
task O
of O
Multi B-DatasetName
- I-DatasetName
woz2.0 I-DatasetName
( O
Budzianowski O
et O
al O
. O
, O
2018a O
) O
. O

We O
uses O
three O
evaluations O
metrics O
proposed O
by O
( O
Budzianowski O
et O
al O
. O
, O
2018a O
) O
. O

These O
include O
: O
1 O
) O
inform B-MetricName
ratemeasures I-MetricName
the O
fraction O
of O
dialogue O
, O
the O
system O
has O
provided O
the O
correct O
entity O
, O
2 O
) O
success B-MetricName
rate I-MetricName
-fraction O
of O
dialogues O
, O
the O
system O
has O
answered O
all O
the O
requested O
information O
and O
3 O
) O
BLEU B-MetricName
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
-measures O
the O
fluency O
of O
the O
generated O
response O
. O

We O
also O
report O
the O
combined O
score O
( O
Inf B-MetricName
orm I-MetricName
+ O
Success B-MetricName
) O
× O
0.5 O
+ O
BLEU B-MetricName
proposed O
by O
Mehri O
et O
al O
. O
( O
2019 O
) O
. O

All O
the O
numbers O
of O
CASPI B-MethodName
reported O
in O
this O
work O
are O
median O
of O
5 B-HyperparameterValue
runs O
with O
different O
seeds O
. O

For O
the O
metric O
M B-MetricName
used O
in O
pairwise O
causal O
reward O
learning O
, O
we O
use O
the O
following O
: O

MinTL B-MethodName
does O
n't O
explicitly O
predict O
dialogue B-TaskName
act I-TaskName
. O

Hence O
we O
only O
use O
the O
deterministic O
loss O
, O
L O
det O
directly O
on O
the O
generated O
response O
and O
for O
DST B-TaskName
we O
retain O
the O
loss O
as O
is O
from O
MintTL B-MethodName
( O
Lin O
et O
al O
. O
, O
2020).For O
k B-HyperparameterName
- O
model O
training O
of O
pairwise O
casual O
reward O
learning O
illustrated O
in O
Fig O
: O
3 O
, O
we O
chose O
DAMD B-MethodName
( O
Zhang O
et O
al O
. O
, O
2019 O
) O
model O
for O
it O
's O
light O
weight O
model O
architecture O
. O

In O
all O
our O
experiments O
, O
we O
use B-HyperparameterName
K I-HyperparameterName
= O
10 B-HyperparameterValue
. O

θ O
: O
= O
θ O
− O
R(s O
t O
, O
a O
t O
, O
g)∇π O
blackbox O
( O
a O
t O
|s O
t O
; O
θ O
) O
( O
6 O
) O
Hence O
we O
believe O
our O
pairwise O
casual O
reward O
learning O
and O
associated O
improvement O
in O
sample O
efficiency O
are O
independent O
of O
model O
architecture O
. O

To O
this O
end O
we O
choose O
two O
ToD B-TaskName
methods O
that O
are O
at O
the O
extremes O
of O
model O
architecture O
spectrum O
1 O
) O
One O
uses O
a O
light O
weight O
custom O
model O
and O
2 O
) O
Other O
uses O
a O
large O
standard O
pre O
- O
trained O
out O
- O
of O
- O
the O
box O
universal O
language O
model O
. O

In O
this O
setting O
, O
we O
use O
the O
neural O
model O
proposed O
by O
Zhang O
et O
al O
. O
( O
2019 O
) O
. O

DAMD B-MethodName
is O
composed O
of O
three O
seq2seq O
generative O
model O
using O
GRUs O
. O

The O
three O
seq2seq O
models O
are O
one O
each O
for O
belief O
state O
, O
dialogue O
act O
and O
response O
generation O
modules O
. O

An O
attention O
layers O
is O
used O
to O
attend O
the O
outputs O
of O
the O
seq2seq O
models O
with O
the O
context O
vector O
of O
previous O
turn O
for O
copy O
over O
mechanism O
. O

The O
outputs O
of O
these O
attention O
layer O
are O
used O
as O
representation O
for O
predicting O
series O
of O
tokens O
for O
their O
respective O
modules O
. O

For O
more O
details O
on O
the O
model O
architecture O
and O
parameter O
setting O
refer O
Zhang O
et O
al O
. O
( O
2019 O
) O
. O

In O
this O
setting O
we O
use O
both O
stochastic O
, O
L O
sto O
and O
deterministic O
, O
L O
det O
loss O
functions O
on O
dialogue B-TaskName
act I-TaskName
. O

For O
DST B-TaskName
and O
response B-TaskName
generation I-TaskName
, O
we O
retain O
the O
cross O
entropy O
loss O
as O
is O
from O
DAMD B-MethodName
( O
Zhang O
et O
al O
. O
, O
2019).On O
the O
other O
extreme O
of O
model O
complexity O
, O
we O
use O
the O
Task O
oriented O
Dialogue O
model O
, O
MinTL B-MethodName
( O
Lin O
et O
al O
. O
, O
2020 O
) O
. O

MinTL B-MethodName
uses O
a O
large O
pretrained O
language O
model O
BART B-MethodName
( O
Lewis O
et O
al O
. O
, O
2019 O
) O
. O

BART B-MethodName
use O
as O
a O
standard O
encoder O
decoder O
transformer O
architecture O
with O
a O
bidirectional O
encoder O
and O
an O
autoregressive O
decoder O
. O

It O
is O
pre O
- O
trained O
on O
the O
task O
of O
denoising O
corrupt O
documents O
. O

BART B-MethodName
is O
trained O
using O
cross O
- O
entropy O
loss O
between O
the O
decoder O
output O
and O
the O
original O
document O
. O

For O
more O
details O
of O
the O
model O
architecture O
and O
parameter O
setting O
, O
we O
suggest O
referring O
to O
( O
Lin O
et O
al O
. O
, O
2020 O
) O
( O
Lewis O
et O
al O
. O
, O
2019 O
) O
. O

2.We O
propose O
a O
safe O
policy O
improvement O
method O
for O
task B-TaskName
oriented I-TaskName
dialogue I-TaskName
setting O
that O
guarantees O
performance O
against O
a O
baseline O
. O

By O
use O
of O
these O
two O
methods O
, O
we O
demonstrate O
performance O
and O
sample O
efficiency O
. O

With O
the O
release O
of O
multi O
- O
domain O
, O
multi O
- O
turn O
Multi B-DatasetName
- I-DatasetName
Woz2.0 I-DatasetName
dataset O
( O
Budzianowski O
et O
al O
. O
, O
2018a O
) O
, O
there O
has O
been O
flurry O
of O
recent O
works O
, O
of O
which O
Zhang O
et O
al O
. O
( O
2019 O
) O
uses O
data O
augmentation O
. O

Rastogi O
et O
al O
. O
( O
2019 O
) O
and O
Hosseini O
- O
Asl O
et O
al O
. O
( O
2020 O
) O
frame O
dialogue O
policy O
learning O
as O
language B-TaskName
modeling I-TaskName
task O
. O

Among O
the O
works O
that O
uses O
reinforcement O
learning O
. O

Mehri O
et O
al O
. O
( O
2019 O
) O
uses O
supervised O
learning O
to O
bootstrap O
followed O
by O
RL O
fine O
tuning O
, O
whereas O
Zhao O
et O
al O
. O
( O
2019 O
) O
uses O
policy O
gradient O
on O
latent O
action O
space O
as O
against O
handcrafted O
ones O
. O

Jaques O
et O
al O
. O
( O
2019 O
) O
and O
Wang O
et O
al O
. O
( O
2020 O
) O
uses O
Batch O
- O
RL O
for O
dialogue O
policy O
learning O
. O

( O
Wang O
et O
al O
. O
, O
2020 O
) O
is O
first O
to O
argue O
the O
use O
of O
automated O
evaluation O
metrics O
directly O
as O
reward O
is O
under O
- O
specified O
for O
ToD B-TaskName
policy O
learning O
. O

Recently O
there O
's O
has O
been O
proliferation O
in O
use O
of O
large O
pretrained O
language O
model O
based O
systems O
like O
Hosseini O
- O
Asl O
et O
al O
. O
( O
2020 O
) O
, O
Lin O
et O
al O
. O
( O
2020 O
) O
, O
etc O
. O

More O
details O
on O
contrasting O
the O
merits O
and O
limitations O
of O
these O
methods O
can O
be O
found O
in O
Sec O
: O
A.1 O

1.We O
introduce O
pairwise O
causal O
reward O
learning O
to O
learn O
fine O
grained O
per O
turn O
reward O
that O
reason O
the O
intention O
of O
human O
utterance O
. O

We O
address O
aforementioned O
shortcomings O
with O
following O
key O
contributions O
: O

Offline O
task B-TaskName
- I-TaskName
oriented I-TaskName
dialogue I-TaskName
( O
ToD B-TaskName
) O
systems O
involves O
solving O
disparate O
tasks O
of O
belief O
states O
tracking O
, O
dialogue O
policy O
management O
, O
and O
response O
generation O
. O

Of O
these O
tasks O
, O
in O
this O
work O
we O
focus O
on O
dialogue O
policy O
management O
to O
improve O
the O
endto O
- O
end O
performance O
of O
ToD. O
The O
need O
for O
sample O
efficiency O
is O
key O
for O
learning O
offline O
task O
- O
oriented O
dialogue O
system O
, O
as O
access O
to O
data O
are O
finite O
and O
expensive O
. O

Recent O
advancements O
in O
off O
- O
policy O
reinforcement O
learning O
methods O
that O
uses O
offline O
data O
as O
against O
a O
simulator O
has O
proven O
to O
be O
sample O
efficient O
( O
Thomas O
and O
Brunskill O
, O
2016 O
) O
. O

The O
effective O
use O
of O
these O
techniques O
are O
hindered O
by O
the O
nature O
of O
ToD. B-TaskName
For O
instance O
, O
bias O
correction O
in O
off O
- O
policy O
based O
methods O
usually O
requires O
estimation O
of O
behaviour O
policy O
for O
a O
given O
state O
of O
Markov O
Decision O
Process O
( O
MDP O
) O
. O

In O
ToD B-TaskName
, O
per O
- O
turn O
annotated O
belief O
- O
state O
does O
not O
capture O
the O
true O
state O
of O
the O
MDP O
. O

Example O
of O
such O
annotated O
belief O
- O
state O
are O
shown O
in O
Fig O
: O
1 O
. O

Latent O
state O
information O
such O
as O
prosody O
, O
richness O
of O
natural O
language O
and O
among O
others O
induces O
stochasticity O
in O
the O
agents O
response O
. O

In O
addition O
to O
these O
short O
comings O
, O
the O
direct O
use O
of O
automatic O
evaluation O
metric O
as O
reward O
for O
policy O
learning O
is O
not O
desirable O
, O
since O
these O
automatic O
evaluation O
metrics O
are O
often O
for O
the O
entire O
dialogue O
and O
not O
per O
turn O
. O

Hence O
such O
rewards O
are O
sparse O
and O
under O
- O
specified O
( O
Wang O
et O
al O
. O
, O
2020 O
) O
. O

Use O
of O
under O
- O
specified O
reward O
will O
often O
lead O
to O
policy O
that O
suffers O
from O
high O
variance O
( O
Agarwal O
et O
al O
. O
, O
2019 O
) O
. O

Alternatively O
use O
of O
imitation O
learning O
based O
methods O
falls O
short O
of O
reasoning O
on O
the O
outcome O
. O

This O
is O
demonstrated O
in O
Fig O
: O
1 O
. O

Turns#3 O
and O
# O
2 O
are O
rich O
in O
semantic O
information O
and O
Turn#3 O
is O
key O
to O
success O
of O
the O
booking O
process O
. O

While O
Turn#4 O
contributes O
least O
to O
successful O
outcome O
. O

Though O
the O
turns O
have O
varying O
levels O
of O
importance O
, O
each O
of O
the O
turns O
are O
treated O
equally O
in O
imitation O
learning O
. O

In O
worst O
case O
, O
turns O
like O
Turn#4 O
will O
appear O
more O
often O
than O
turns O
Turn#2 O
and O
# O
3 O
in O
a O
ToD B-TaskName
dataset O
, O
there O
by O
taking O
greater O
share O
of O
the O
gradient O
budget O
. O

We O
also O
conduct O
experiments O
on O
fixing O
the O
parameter O
of O
PLM O
during O
training O
on O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
as O
initialized O
by O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
. O

The O
performance O
drops O
to O
EM B-MetricName
48.5 B-MetricValue
and O
F B-MetricName
1 I-MetricName
49.0 B-MetricValue
. O

Fixing O
the O
parameter O
of O
PLM O
largely O
impedes O
the O
performance O
of O
TAGOP B-MethodName
- I-MethodName
L2I I-MethodName
on O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
, O
showing O
that O
encoding O
factual O
and O
hypothetical O
questions O
requires O
different O
mechanisms O
. O

To O
further O
investigate O
the O
difference O
in O
answering O
factual O
and O
hypothetical O
questions O
, O
we O
test O
TAGOP B-MethodName
- I-MethodName
L2I I-MethodName
on O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
. O

The O
result O
in O
Figure O
4 O
shows O
that O
training O
on O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
causes O
a O
performance O
drop O
in O
counting O
, O
span O
and O
multi O
- O
span O
groups O
of O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
, O
and O
performs O
similar O
on O
the O
in O
arithmetic O
group O
. O

We O
conjecture O
the O
performance O
drop O
in O
the O
first O
three O
groups O
is O
because O
the O
question B-TaskName
- I-TaskName
answering I-TaskName
label O
in O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
under O
the O
same O
c O
and O
q O
is O
different O
from O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
. O

However O
, O
for O
arithmetic O
questions O
, O
the O
question B-TaskName
- I-TaskName
answering I-TaskName
label O
for O
one O
pair O
of O
c O
and O
q O
remains O
the O
same O
between O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
and O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
, O
and O
the O
intervention O
is O
achieved O
explicitly O
by O
deriving O
operators O
and O
tagging O
head O
. O

Figure O
3(a O
) O
shows O
the O
validation O
result O
of O
TAGOP B-MethodName
- I-MethodName
L2I I-MethodName
as O
increasing O
the O
matching B-HyperparameterName
block I-HyperparameterName
from O
1 B-HyperparameterValue
to O
4 B-HyperparameterValue
layers O
. O

We O
can O
observe O
that O
: O
1 O
) O
Stacking O
more O
layers O
does O
not O
always O
bring O
performance O
gain O
. O

2 O
) O
In O
particular O
, O
three B-HyperparameterValue
layers O
of O
matching O
block O
achieve O
the O
best O
performance O
on O
TAGOP B-MethodName
- I-MethodName
L2I. I-MethodName
The O
result O
indicates O
that O
three B-HyperparameterValue
layers O
should O
be O
sufficient O
to O
capture O
the O
semantic O
connection O
across O
the O
context O
, O
question O
and O
assumption O
. O

This O
is O
reasonable O
since O
the O
average O
length O
of O
both O
assumption O
and O
question O
are O
only O
around O
10 O
words O
( O
cf O
. O

Table O
2 O
) O
. O

As O
to O
operator O
types O
( O
the O
right O
half O
) O
, O
we O
observe O
that O
: O
1 O
) O
TAGOP B-MethodName
- I-MethodName
L2I I-MethodName
achieves O
imagination O
on O
the O
majority O
of O
operator O
types O
with O
better O
performance O
than O
TAGOP B-MethodName
, O
yet O
TAGOP B-MethodName
can O
only O
achieve O
imagination O
on O
a O
few O
operator O
types O
. O

The O
better O
performance O
of O
TAGOP B-MethodName
- I-MethodName
L2I I-MethodName
is O
attributed O
to O
modeling O
the O
deriving O
operations O
as O
specific O
operators O
. O

We O
thus O
believe O
that O
TAGOP B-MethodName
- I-MethodName
L2I I-MethodName
can O
generalize O
well O
to O
more O
deriving O
operations O
by O
simply O
incorporating O
the O
operators O
, O
as O
long O
as O
the O
corresponding O
training O
questions O
are O
not O
rare O
. O

This O
result O
thus O
reflects O
the O
advantage O
of O
the O
unified O
operator O
framework O
adopted O
by O
the O
L2I B-MethodName
module O
, O
which O
is O
consistent O
with O
previous O
work O
( O
Andor O
et O
al O
. O
, O
2019 O
) O
. O

2 O
) O
Across O
the O
groups O
, O
TAGOP B-MethodName
achieves O
relatively O
good O
performance O
on O
the O
SWAP O
group O
, O
which O
replaces O
the O
target O
fact O
with O
a O
number O
in O
the O
assumption O
. O

It O
corresponds O
to O
the O
simplest O
imagination O
since O
the O
assumed O
value O
( O
i.e. O
, O
c O
i O
) O
is O
explicitly O
mentioned O
in O
the O
assumption O
. O

Therefore O
, O
the O
result O
shows O
that O
the O
NDR B-TaskName
model O
can O
achieve O
simple O
counterfactual O
thinking O
by O
learning O
to O
answer O
hypothetical O
questions O
. O

However O
, O
such O
indirect O
guidance O
on O
imagination O
fails O
on O
the O
groups O
requiring O
more O
complex O
imagination O
, O
e.g. O
, O
requiring O
add O
or O
minus O
. O

3 O
) O
TAGOP B-MethodName
- I-MethodName
L2I I-MethodName
achieves O
the O
worst O
performance O
on O
SWAP O
MIN O
NUM O
, O
which O
is O
merely O
comparable O
to O
TAGOP B-MethodName
. O

We O
suspect O
the O
reason O
is O
that O
the O
operation O
of O
SWAP O
MIN O
NUM O
is O
very O
close O
to O
SWAP O
, O
which O
may O
confuse O
the O
deriving O
head O
when O
making O
classification O
over O
the O
operators O
. O

To O
address O
this O
issue O
, O
it O
is O
worth O
considering O
the O
operator O
relation O
in O
the O
deriving O
head O
in O
the O
future O
. O

Study O
on B-MethodName
L2I I-MethodName
module O
design O
. O

We O
then O
explore O
the O
influence O
of O
network O
architecture O
on O
the O
effectiveness O
of O
the O
L2I B-MethodName
module O
from O
three O
perspectives O
: O
1 O
) O
module B-HyperparameterName
depth I-HyperparameterName
; O
2 O
) O
configuration O
of O
the O
matching B-HyperparameterName
block I-HyperparameterName
; O
and O
3 O
) O
the O
setting O
of O
PLM O
. O

3 O
) O
The O
performance O
of O
TAGOP B-MethodName
on O
arithmetic O
has O
a O
large O
gap O
with O
other O
types O
, O
showing O
that O
arithmetic O
questions O
are O
more O
difficult O
to O
conduct O
imagination O
and O
reasoning O
even O
though O
arithmetic O
makes O
up O
the O
majority O
of O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
data O
. O

As O
to O
TAGOP B-MethodName
- I-MethodName
L2I I-MethodName
, O
the O
gap O
between O
arithmetic O
question O
and O
other O
types O
of O
question O
largely O
reduces O
, O
validating O
the O
effectiveness O
of O
learning O
intervention O
with O
discrete O
operators O
and O
neural O
network O
modules O
. O

Detailed O
performance O
. O
To O
further O
investigate O
the O
effectiveness O
of O
the O
proposed O
L2I B-MethodName
module O
, O
we O
perform O
a O
detailed O
comparison O
between O
TAGOP B-MethodName
- I-MethodName
L2I I-MethodName
and O
TAGOP B-MethodName
w.r.t O
. O

the O
discrete O
operation O
required O
in O
answering O
the O
question O
or O
counterfactual O
thinking O
. O

We O
group O
the O
questions O
according O
to O
1 O
) O
the O
answer O
type O
and O
2 O
) O
the O
operator O
to O
derive O
the O
intervention O
. O

Table O
5 O
shows O
the O
group O
- O
wise O
It O
should O
be O
noted O
that O
the O
separation O
also O
facilitates O
the O
generalization O
to O
new O
operations O
since O
the O
modules O
can O
be O
separately O
updated O
. O

Multi O
- O
iteration O
derivation O
. O

In O
causal O
inference O
, O
a O
rigorous O
derivation O
of O
an O
intervention O
considers O
the O
successors O
of O
the O
target O
variable O
, O
e.g. O
, O
finished O
goods O
in O
2019 O
affects O
total O
inventories O
in O
2019 O
. O

Currently O
, O
we O
omit O
the O
following O
iterations O
in O
Step O
2 O
of O
L2I B-MethodName
( O
cf O
. O

Eq O
1 O
) O
. O

This O
is O
because O
not O
all O
successors O
are O
necessary O
for O
answering O
the O
question O
. O

For O
instance O
, O
answering O
the O
question O
in O
Figure O
1 O
does O
not O
require O
the O
post O
- O
intervention O
value O
of O
total O
inventories O
in O
2019 O
. O

In O
conventional O
causal O
inference O
, O
such O
successors O
will O
also O
be O
omitted O
according O
to O
the O
local O
surgery O
principle O
( O
Pearl O
, O
2009 O
) O
. O

Moreover O
, O
we O
believe O
that O
the O
following O
iterations O
can O
be O
achieved O
by O
the O
current O
L2I B-MethodName
module O
in O
an O
iterative O
manner O
. O

Assume O
that O
NDR B-TaskName
model O
equipped O
with O
L2I B-MethodName
can O
answer O
the O
hypothetical O
questions O
requiring O
one O
- O
iteration O
derivation O
( O
i.e. O
, O
c O
i O
→ O
c O
i O
) O
. O

We O
can O
thus O
derive O
the O
value O
of O
successors O
( O
e.g. O
, O
c O
i O
→ O
c O
j O
) O
by O
forming O
a O
simple O
hypothetical O
question O
: O
" O
What O
c O
j O
would O
be O
if O
c O
i O
is O
c O
i O
? O
" O
and O
answering O
it O
with O
the O
NDR B-TaskName
model O
. O

We O
conduct O
experiments O
on O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
dataset O
to O
answer O
the O
following O
questions O
: O
RQ1 O
: O
How O
does O
L2I B-MethodName
perform O
on O
HQA B-TaskName
? O
RQ2 O
: O
What O
factors O
influ- O
Compared O
methods O
. O

To O
validate O
the O
effectiveness O
of O
our O
proposed O
L2I B-MethodName
module O
, O
we O
apply O
it O
to O
TAGOP B-MethodName
, O
obtaining O
an O
NDR B-TaskName
model O
for O
HQA B-TaskName
, O
named O
TAGOP B-MethodName
- I-MethodName
L2I. I-MethodName
In O
addition O
to O
the O
vanilla O
TAGOP B-MethodName
, O
we O
compare O
our O
method O
against O
representative O
methods O
of O
traditional B-TaskName
QA I-TaskName
, O
numerical B-TaskName
QA I-TaskName
, B-TaskName
tabular I-TaskName
QA I-TaskName
, O
and O
hybrid B-TaskName
QA I-TaskName
. O

Besides O
, O
we O
want O
to O
select O
baselines O
that O
are O
effective O
for O
learning O
counterfactual O
samples O
. O

The O
baselines O
are O
: O
BERT B-MethodName
- I-MethodName
RC I-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
a O
traditional B-TaskName
QA I-TaskName
method O
that O
selects O
answer O
spans O
from O
the O
context O
. O

NumNet+ B-MethodName
V2 I-MethodName
( O
Ran O
et O
al O
. O
, O
2019 O
) O
, O
a O
numerical B-TaskName
QA I-TaskName
method O
with O
numerically O
- O
aware O
graph O
neural O
network O
. O

TAPAS B-MethodName
- I-MethodName
WTQ I-MethodName
( O
Herzig O
et O
al O
. O
, O
2020 O
) O
, O
a O
tabular B-TaskName
QA I-TaskName
method O
that O
focuses O
on O
parsing O
and O
understanding O
tables O
, O
pre O
- O
trained O
over O
tables O
collected O
from O
Wikipedia O
before O
training O
on O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
. B-MethodName
HyBrider I-MethodName
( O
Chen O
et O
al O
. O
, O
2020c O
) O
, O
a O
hybrid B-TaskName
QA I-TaskName
method O
that O
considers O
the O
connection O
between O
the O
table O
and O
text O
. O

TAGOP B-MethodName
, O
a O
hybrid B-TaskName
QA I-TaskName
method O
that O
performs O
discrete O
reasoning O
over O
both O
the O
tabular O
and O
textual O
contexts O
. O

It O
is O
the O
state O
- O
of O
- O
the O
- O
art O
method O
on O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
dataset O
. O

TAGOP B-MethodName
- I-MethodName
CLO I-MethodName
, O
incorporating O
the O
Contrastive O
Learning O
Objective O
( O
CLO O
) O
into O
the O
training O
objective O
of O
TAGOP B-MethodName
, O
which O
is O
shown O
to O
be O
effective O
in O
learning O
the O
relationship O
between O
factual O
and O
counterfactual O
samples O
( O
Liang O
et O
al O
. O
, O
2020 O
) O
. O

3 O
) O
As O
to O
the O
remaining O
methods O
, O
their O
performance O
has O
a O
clear O
gap O
between O
TAGOP B-MethodName
, O
which O
is O
consistent O
with O
the O
result O
on O
the O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
dataset O
( O
Zhu O
et O
al O
. O
, O
2021 O
) O
. O

This O
is O
because O
both O
datasets O
have O
textual O
and O
tabular O
texts O
, O
where O
the O
ability O
of O
TAGOP B-MethodName
to O
perform O
discrete O
reasoning O
across O
hybrid O
contexts O
brings O
significant O
advantages O
. O

4 O
) O
The O
performance O
achieved O
is O
still O
low O
w.r.t O
. O

the O
two O
metrics O
( O
e.g. O
, O
54.4→100 B-MetricValue
) O
, O
showing O
a O
large O
space O
for O
future O
exploration O
on O
the O
challenging O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
dataset O
. O

Parameter O
settings O
. O
We O
implement O
TAGOP B-MethodName
- I-MethodName
L2I I-MethodName
based O
on O
TAGOP B-MethodName
4 O
. O

We O
set O
the O
number B-HyperparameterName
of I-HyperparameterName
crossattention I-HyperparameterName
layers I-HyperparameterName
to O
3 B-HyperparameterValue
, O
and O
fine O
tune O
from O
TAGOP B-MethodName
trained O
on O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
5e-5 B-HyperparameterValue
, O
batch B-HyperparameterName
size I-HyperparameterName
of O
32 B-HyperparameterValue
, O
and O
gradient B-HyperparameterName
accumulation I-HyperparameterName
step I-HyperparameterName
of O
4 B-HyperparameterValue
. O

All O
compared O
methods O
are O
initialized O
with O
the O
model O
trained O
on O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
and O
then O
fine O
- O
tuned O
on O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
. O

For O
TAGOP B-MethodName
- I-MethodName
CLO I-MethodName
, O
we O
conduct O
max O
pooling O
for O
H O
and O
adopt O
cosine B-MetricName
similarity I-MetricName
as O
the O
distance O
metric O
. O

We O
select O
the O
corresponding O
factual O
question O
as O
the O
positive O
sample O
and O
a O
randomly O
selected O
factual O
question O
as O
the O
negative O
sample O
. O

The O
weight B-HyperparameterName
for I-HyperparameterName
the I-HyperparameterName
contrastive I-HyperparameterName
loss I-HyperparameterName
is O
0.1.Overall B-HyperparameterValue
performance O
. O

Table O
4 O
shows O
the O
performance O
of O
the O
compared O
methods O
on O
the O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
dataset O
. O

We O
can O
observe O
that O
: O
1 O
) O
TAGOP B-MethodName
- I-MethodName
L2I I-MethodName
achieves O
the O
best O
performance O
among O
all O
the O
compared O
methods O
. O

In O
particular O
, O
it O
outperforms O
the O
best O
baselines O
by O
19.8 B-MetricValue
% I-MetricValue
and O
19.7 B-MetricValue
% I-MetricValue
on O
EM B-MetricName
and O
F B-MetricName
1 I-MetricName
, O
respectively O
. O

Such O
significant O
performance O
gain O
validates O
the O
effectiveness O
of O
the O
L2I B-MethodName
module O
and O
reveal O
the O
rationality O
of O
modeling O
counterfactual O
thinking O
as O
a O
neural O
network O
module O
. O

2 O
) O
TAGOP B-MethodName
- I-MethodName
CLO I-MethodName
outperforms O
TAGOP B-MethodName
by O
10.5 B-MetricValue
% I-MetricValue
and O
10.4 B-MetricValue
% I-MetricValue
on O
EM B-MetricName
and O
F B-MetricName
1 I-MetricName
. O

The O
only O
difference O
between O
these O
two O
methods O
is O
that O
TAGOP B-MethodName
- I-MethodName
CLO I-MethodName
incorporates O
an O
extra O
CLO O
. O

The O
improvement O
indicates O
that O
learning O
the O
relationship O
between O
the O
factual O
and O
counterfactual O
samples O
with O
CLO O
provides O
some O
clue O
for O
counterfactual O
imagination O
, O
yet O
it O
is O
still O
worse O
than O
directly O
learning O
to O
imagine O
with O
neural O
network O
modules O
. O

wherep O
j O
∈ O
{ O
0 O
, O
1 O
} O
denotes O
the O
label O
of O
the O
target O
fact O
( O
token O
j O
in O
context O
) O
or O
the O
premise O
( O
token O
j O
in O
assumption O
) O
; O
andō O
∈ O
R O
O O
is O
the O
label O
of O
the O
deriving O
operator O
( O
see O
Appendix O
C O
for O
the O
details O
of O
label O
construction).Readers O
might O
have O
raised O
the O
following O
two O
concerns O
for O
L2I B-MethodName
: O
1 O
) O
the O
operators O
defined O
are O
limited O
, O
and O
2 O
) O
the O
operators O
are O
tailored O
to O
one O
step O
of O
derivation O
on O
one O
target O
fact O
. O

Actually O
, O
it O
is O
a O
common O
approach O
for O
current O
state O
- O
of O
- O
the O
- O
art O
NDR B-TaskName
models O
to O
apply O
a O
set O
of O
defined O
operators O
( O
Ran O
et O
al O
. O
, O
2019;Chen O
et O
al O
. O
, O
2020a;Zhu O
et O
al O
. O
, O
2021 O
) O
. O

• O
Deriving O
head O
. O

It O
derives O
the O
intervention O
result O
for O
the O
target O
fact O
. O

To O
calculate O
the O
intervention O
result O
, O
we O
select O
a O
set O
of O
commonly O
used O
discrete O
operators O
such O
as O
SWAP O
, O
ADD O
, O
and O
MINUS O
( O
cf O
. O

Appendix O
B O
) O
. O

Then O
, O
we O
model O
the O
derivation O
as O
making O
a O
choice O
across O
the O
operators O
and O
tagging O
the O
premise O
for O
executing O
the O
operator O
. O

In O
particular O
, O
we O
adopt O
a O
tagging O
head O
to O
identify O
the O
premise O
and O
a O
multi O
- O
way O
classifier O
for O
choosing O
operators O
, O
which O
is O
formulated O
as O
: O
o O
= O
sof O
tmax(MLP(h O
CLS O
) O
) O
. O

o O
∈ O
R B-HyperparameterName
O I-HyperparameterName
is O
a O
distribution O
over O
the O
operators O
where O
O B-HyperparameterName
denotes O
the O
number B-HyperparameterName
of I-HyperparameterName
operators I-HyperparameterName
. O

h O
CLS O
corresponds O
to O
the O
CLS O
token O
in O
H.Most O
recent O
NDR B-TaskName
models O
( O
Ran O
et O
al O
. O
, O
2019;Andor O
et O
al O
. O
, O
2019;Chen O
et O
al O
. O
, O
2020a;Herzig O
et O
al O
. O
, O
2020;Zhu O
et O
al O
. O
, O
2021 O
) O
( O
Dua O
et O
al O
. O
, O
2019 O
) O
. O

Suppose O
we O
have O
a O
set O
of O
labeled O
questions O
D O
= O
{ O
< O
ȳ O
, O
( O
q O
, O
c O
, O
a O
) O
> O
} O
, O
the O
training O
objective O
can O
be O
abstracted O
as O
min O
θ O
D O
QA O
( O
ȳ O
, O
f O
( O
q O
, O
c O
, O
a O
) O
) O
where O
θ O
denotes O
model O
parameters O
. O

Note O
that O
QA(• O
) O
measures O
the O
discrepancy O
between O
the O
ground O
- O
truth O
and O
the O
predicted O
answers O
which O
can O
have O
different O
formats O
. O

For O
instance O
, O
it O
can O
be O
a O
combination O
of O
the O
cross B-MetricName
- I-MetricName
entropy I-MetricName
( O
CE B-MetricName
) O
loss O
over O
the O
operand O
look O
- O
up O
and O
the O
CE B-MetricName
loss O
over O
the O
choice O
of O
discrete O
operation O
( O
Herzig O
et O
al O
. O
, O
2020;Zhu O
et O
al O
. O
, O
2021 O
) O
. O

When O
applying O
L2I B-MethodName
to O
an O
existing O
NDR B-TaskName
method O
, O
we O
keep O
its O
question O
- O
answering O
objective O
unchanged O
. O

To O
optimize O
the O
L2I B-MethodName
module O
, O
we O
incorporate O
supervision O
on O
the O
classifiers O
in O
the O
tagging O
head O
and O
deriving O
head O
. O

Formally O
, O

Module O
Design O
. O
Based O
on O
the O
two O
- O
step O
formulation O
, O
we O
then O
design O
the O
L2I B-MethodName
module O
as O
neural O
network O
operations O
. O

We O
have O
two O
considerations O
for O
the O
module O
design O
: O
1 O
) O
the O
module O
should O
recognize O
the O
semantic O
connection O
between O
the O
assumption O
and O
the O
context O
, O
and O
2 O
) O
the O
module O
should O
uniformly O
support O
various O
discrete O
operations O
to O
enable O
accurate O
derivation O
. O

To O
this O
end O
, O
we O
devise O
four O
key O
building O
blocks O
for O
the O
L2I B-MethodName
module O
: O
• O
Encoder O
. O

It O
projects O
the O
raw O
content O
into O
latent O
representation O
. O

Inspired O
by O
the O
recent O
research O
on O
NDR B-TaskName
, O
we O
employ O
a O
pre O
- O
trained O
language O
model O
( O
PLM O
) O
, O
i.e. O
, O
RoBERTa B-MethodName
, O
as O
the O
encoder O
to O
learn O
an O
overall O
representation O
of O
the O
context O
, O
question O
, O
and O
assumption O
; O

Two O
- O
step O
Formulation O
. O

To O
this O
end O
, O
we O
propose O
a O
two O
- O
step O
formulation O
of O
counterfactual O
thinking O
for O
HQA B-TaskName
to O
perform O
the O
identification O
and O
derivation O
. O

Formally O
, O

We O
conduct O
a O
pilot O
study O
on O
the O
generalization O
ability O
of O
existing O
NDR B-TaskName
models O
on O
hypothetical O
questions O
. O

In O
particular O
, O
we O
evaluate O
TAGOP B-MethodName
( O
Zhu O
et O
al O
. O
, O
2021 O
) O
, O
which O
is O
the O
state O
- O
of O
- O
the O
- O
art O
model O
on O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
( O
see O
detailed O
settings O
in O
Section O
4.1 O
) O
by O
training O
on O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
and O
testing O
on B-DatasetName
TAT I-DatasetName
- I-DatasetName
HQA I-DatasetName
. O

In O
Table O
3 O
, O
the O
huge O
performance O
drop O
shows O
that O
even O
the O
state O
- O
of O
- O
the O
- O
art O
NDR B-TaskName
model O
lacks O
counterfactual O
thinking O
ability O
. O

We O
aim O
to O
empower O
NDR B-TaskName
models O
with O
counterfactual O
thinking O
ability O
. O

Firstly O
, O
we O
decide O
to O
choose O
the O
approach O
of O
explicitly O
modeling O
discrete O
operations O
, O
since O
existing O
NDR B-TaskName
solutions O
have O
demonstrated O
its O
superiority O
( O
Dua O
et O
al O
. O
, O
2019;Ran O
et O
al O
. O
, O
2019;Herzig O
et O
al O
. O
, O
2020;Zhu O
et O
al O
. O
, O
2021 O
) O
. O

We O
devise O
a O
Learning O
to O
Imagine O
module O
to O
model O
counterfactual O
thinking O
( O
Section O
3.1 O
) O
, O
and O
then O
incorporate O
the O
L2I B-MethodName
module O
( O
Section O
3.2 O
) O
into O
existing O
NRD B-TaskName
methods O
, O
followed O
by O
a O
discussion O
about O
potential O
extensions O
( O
Section O
3.3).Functionally O
speaking O
, O
the O
L2I B-MethodName
module O
aims O
to O
construct O
a O
counterfactual O
context O
based O
on O
the O
factual O
context O
and O
the O
assumption O
. O

We O
formulate O
it O
as O
: O
c O
= O
g(c O
, O
a O
) O
, O
where O
the O
counterfactual O
context O
c O
is O
the O
status O
of O
the O
context O
c O
after O
the O
assumption O
a O
is O
executed O
. O

Resorting O
to O
the O
language O
of O
causality O
, O
it O
can O
be O
expressed O
as O
the O
do O
- O
operation O
that O
intervenes O
a O
variable O
to O
execute O
the O
assumption O
and O
the O
action O
to O
derive O
the O
outcome O
of O
the O
intervention O
3 O
( O
Pearl O
, O
2009 O
) O
. O

The O
key O
to O
achieving O
counterfactual O
thinking O
in O
NDR B-TaskName
lies O
in O
: O
1 O
) O
parsing O
the O
assumption O
to O
identify O
the O
target O
fact O
to O
intervene O
; O
and O
2 O
) O
deriving O
the O
assumed O
value O
to O
construct O
the O
counterfactual O
context O
. O

Taking O
the O
hypothetical O
question O
in O
Figure O
1 O
as O
an O
example O
, O
an O
ideal O
L2I B-MethodName
should O
recognize O
the O
target O
variable O
( O
finished O
goods O
in O
2019 O
) O
, O
identify O
the O
corresponding O
fact O
( O
$ O
133,682 O
) O
, O
and O
replace O
the O
fact O
with O
the O
assumed O
value O
( O
$ O
132,935 O
) O
. O

To O
facilitate O
the O
evaluation O
of B-TaskName
HQA I-TaskName
and O
diagnose O
counterfactual O
thinking O
, O
we O
construct O
an O
HQA B-TaskName
dataset O
based O
on O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
( O
Zhu O
et O
al O
. O
, O
2021 O
) O
, O
which O
is O
a O
QA B-TaskName
dataset O
with O
a O
mix O
of O
tabular O
and O
textual O
context O
extracted O
from O
financial O
reports O
. O

Inspired O
by O
previous O
work O
on O
constructing O
counterfactual O
samples O
( O
Kaushik O
et O
al O
. O
, O
2019 O
) O
, O
we O
recruit O
college O
students O
with O
finance O
- O
related O
majors O
to O
imagine O
an O
intervention O
based O
on O
the O
factual O
question O
and O
context O
from O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
which O
involves O
numerical O
thinking O
, O
e.g. O
, O
a O
change O
of O
number O
. O

Then O
they O
phrase O
the O
intervention O
into O
an O
assumption O
, O
forming O
a O
" O
what O
if O
" O
type O
of O
question O
, O
and O
calculate O
the O
answer O
( O
see O
an O
example O
in O
Figure O
1 O
) O
. O

To O
ensure O
the O
diversity O
of O
the O
phrasing O
, O
annotators O
are O
free O
to O
generate O
various O
phrasing O
of O
the O
assumption O
, O
and O
there O
is O
no O
restriction O
on O
the O
position O
of O
the O
assumption O
. O

Usually O
, O
the O
assumption O
appears O
either O
before O
of O
after O
the O
factual O
question O
. O

Each O
hypothetical O
question O
is O
related O
to O
one O
factual O
question O
from O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
, O
but O
each O
factual O
question O
in O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
is O
not O
guaranteed O
to O
have O
one O
hypothetical O
question O
. O

We O
follow O
the O
quality O
control O
approaches O
of O
annotator O
training O
and O
two O
- O
round O
validation O
in O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
to O
guarantee O
the O
quality O
of O
the O
hypothetical O
questions O
. O

Following O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
, O
the O
hypothetical O
questions O
are O
also O
labeled O
with O
four O
answer O
types O
: O
arithmetic O
, O
span O
, O
count O
, O
and O
multi O
- O
span O
, O
three O
types O
of O
answer O
sources O
: O
table O
, O
text O
and O
table O
- O
text O
, O
and O
a O
derivation O
on O
how O
the O
answer O
is O
derived O
from O
the O
context O
. O

In O
total O
, O
we O
obtain O
8,283 O
hypothetical O
questions O
, O
naming O
it O
as O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
. O

The O
statistics O
of O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
are O
shown O
in O
Table O
1 O
. O

We O
follow O
the O
split O
of O
training O
, O
testing O
and O
validation O
set O
of O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
as O
shown O
in O
Table O
2 O
. O

• O
We O
construct O
a O
challenging O
HQA B-TaskName
dataset O
and O
conduct O
extensive O
experiments O
on O
the O
dataset O
, O
where O
the O
performance O
validates O
the O
rationality O
and O
effectiveness O
of O
the O
proposed B-MethodName
L2I.In I-MethodName
the O
general O
setting O
of O
machine O
reading O
comprehension O
, O
the O
task O
is O
to O
answer O
a O
question O
according O
to O
the O
facts O
in O
a O
given O
context O
. O

Formally O
, O
it O
is O
to O
learn O
a O
function O
y O
= O
f O
( O
q O
, O
c O
) O
, O
where O
y O
, O
q O
, O
and O
c O
are O
the O
word O
list O
representing O
the O
answer O
, O
the O
question O
, O
and O
the O
context O
2 O
respectively O
. O

This O
work O
studies O
a O
new O
and O
more O
challenging O
task O
that O
focuses O
on O
hypothetical O
question O
. O

As O
shown O
in O
Figure O
1 O
, O
a O
hypothetical O
question O
includes O
an O
assumption O
, O
e.g. O
, O
" O
if O
the O
amount O
in O
2019 O
was O
$ O
132,935 O
thousand O
instead O
" O
. O

The O
target O
of O
HQA B-TaskName
is O
to O
learn O
y O
= O
f O
( O
q O
, O
c O
, O
a O
) O
where O
a O
denotes O
the O
assumption O
. O

The O
existence O
of O
an O
assumption O
calls O
for O
the O
imagination O
of O
a O
counterfactual O
context O
before O
inferring O
the O
answer O
, O
pushing O
the O
NDR B-TaskName
model O
to O
grasp O
both O
semantic O
understanding O
and O
counterfactual O
thinking O
. O

• O
We O
devise O
the O
L2I B-MethodName
module O
, O
which O
is O
designed O
as O
neural O
network O
operations O
and O
can O
be O
seamlessly O
incorporated O
into O
the O
NDR B-TaskName
model O
for O
answering O
hypothetical O
questions O
. O

In O
this O
light O
, O
we O
consider O
modeling O
counterfactual O
thinking O
as O
neural O
network O
modules O
that O
can O
be O
seamlessly O
incorporated O
into O
existing O
NDR B-TaskName
models O
. O

One O
straightforward O
solution O
is O
to O
model O
counterfactual O
thinking O
as O
a O
generation O
procedure O
with O
the O
fact O
and O
assumption O
as O
inputs O
by O
using O
a O
generation O
model O
such O
as O
GPT B-MethodName
( O
Brown O
et O
al O
. O
, O
2020 O
) O
. O

However O
, O
such O
uncontrollable O
model O
( O
Zou O
et O
al O
. O
, O
2021 O
) O
can O
hardly O
generate O
high O
- O
quality O
context O
for O
two O
reasons O
: O
1 O
) O
the O
context O
is O
more O
complex O
than O
plain O
text O
, O
which O
can O
include O
a O
table O
( O
Figure O
1 O
) O
; O
and O
2 O
) O
NDR B-TaskName
requires O
a O
precise O
context O
with O
the O
correct O
numbers O
( O
Figure O
1 O
, O
$ O
132,935 O
for O
the O
finished O
goods O
in O
2019 O
) O
. O

Therefore O
, O
we O
resort O
to O
an O
alternative O
approach O
: O
constructing O
the O
counterfactual O
( O
Pearl O
, O
2009 O
) O
where O
the O
target O
variable O
is O
intervened O
according O
to O
the O
hypothetical O
condition O
to O
infer O
a O
counterfactual O
. O

We O
propose O
Learning O
to O
Imagine O
, O
where O
the O
counterfactual O
thinking O
is O
implemented O
with O
two O
intervening O
steps O
: O
1 O
) O
identifying O
the O
facts O
to O
intervene O
, O
and O
2 O
) O
deriving O
the O
result O
of O
intervention O
. O

To O
pursue O
accurate O
context O
, O
we O
derive O
the O
intervention O
with O
a O
set O
of O
discrete O
operators O
such O
as O
SWAP O
and O
ADD O
for O
imagination O
. O

To O
evaluate O
the O
counterfactual O
thinking O
ability O
, O
we O
recruit O
volunteers O
with O
domain O
expertise O
to O
construct O
an O
HQA B-TaskName
dataset O
based O
on O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
( O
Zhu O
et O
al O
. O
, O
2021 O
) O
by O
posting O
an O
assumption O
for O
each O
original O
question O
, O
named O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
. O

We O
apply O
L2I B-MethodName
to O
TAGOP B-MethodName
( O
Zhu O
et O
al O
. O
, O
2021 O
) O
, O
and O
obtain O
a O
promising O
solution O
for O
HQA B-TaskName
. O

In O
summary O
, O
the O
main O
contributions O
are O
as O
follows O
: O

• O
We O
highlight O
the O
importance O
of O
counterfactual O
thinking O
in O
NDR B-TaskName
and O
formulate O
counterfactual O
thinking O
as O
an O
intervening O
procedure O
to O
achieve O
precise O
imagination O
. O

In O
this O
work O
, O
we O
extend O
NDR B-TaskName
to O
hypothetical B-TaskName
question I-TaskName
answering I-TaskName
( O
HQA B-TaskName
) O
, O
where O
the O
question O
consists O
of O
an O
assumption O
beyond O
the O
context O
( O
Figure O
1 O
) O
. O

The O
ability O
of O
HQA B-TaskName
will O
undoubtedly O
enhance O
the O
practical O
use O
of O
NDR B-TaskName
due O
to O
the O
universality O
of O
hypothetical O
questions O
. O

However O
, O
current B-TaskName
NDR I-TaskName
models O
face O
severe O
generalization O
failure O
on O
hypothetical O
questions O
. O

An O
empirical O
evidence O
on O
such O
vulnerability O
is O
that O
the O
state O
- O
of O
- O
the O
- O
art O
model O
( O
Zhu O
et O
al O
. O
, O
2021 O
) O
encounters O
a O
sharp O
performance O
drop O
( O
F1 O
score O
drops O
from O
68.6 O
% O
to O
3.8 O
% O
) O
on O
the O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
dataset O
when O
changing O
the O
questions O
to O
be O
hypothetical O
by O
adding O
a O
related O
assumption O
( O
see O
details O
in O
Section O
2 O
, O
Table O
3 O
) O
. O

We O
postulate O
that O
the O
failure O
is O
due O
to O
unable O
of O
imagining O
the O
counterfactual O
context O
according O
to O
the O
assumption O
( O
Figure O
1 O
) O
. O

To O
pursue O
such B-TaskName
reasoning I-TaskName
ability O
, O
we O
resort O
to O
the O
concept O
of O
counterfactual O
thinking O
( O
Pearl O
, O
2019 O
) O
from O
the O
theory O
of O
causality O
, O
which O
is O
the O
ability O
to O
imagine O
and O
reason O
over O
unseen O
cases O
based O
on O
the O
seen O
facts O
and O
counterfactual O
assumptions O
. O

Neural B-TaskName
discrete I-TaskName
reasoning I-TaskName
( O
Dua O
et O
al O
. O
, O
2019 O
) O
is O
an O
emerging O
technique O
for O
machine O
reading O
comprehension O
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
which O
aims O
at O
answering O
numerical O
questions O
from O
textual O
( O
Dua O
et O
al O
. O
, O
2019 O
) O
or O
hybrid O
( O
Zhu O
et O
al O
. O
, O
2021 O
) O
context O
1 O
. O

NDR B-TaskName
combines O
deep O
neural O
network O
with O
discrete B-TaskName
and I-TaskName
symbolic I-TaskName
reasoning I-TaskName
( O
e.g. O
, O
addition B-TaskName
, O
sorting B-TaskName
, O
or O
counting B-TaskName
) O
( O
Dua O
et O
al O
. O
, O
2019 O
) O
and O
enables O
the O
comprehension O
of O
complex O
contexts O
and O
compositional O
questions O
, O
which O
is O
critical O
for O
many O
practical O
applications O
such O
as O
automatic O
diagnosis O
( O
Wei O
et O
al O
. O
, O
2018 O
) O
and O
robo O
- O
advisor O
( O
Fisch O
et O
al O
. O
, O
2019 O
) O
. O

Existing O
state O
- O
of O
- O
the O
- O
art B-TaskName
NDR I-TaskName
models O
implement O
the O
nu B-TaskName
- I-TaskName
merical I-TaskName
reasoning I-TaskName
process O
as O
neural O
network O
modules O
( O
Ran O
et O
al O
. O
, O
2019;Herzig O
et O
al O
. O
, O
2020;Zhu O
et O
al O
. O
, O
2021 O
) O
, O
e.g. O
, O
a O
graph O
neural O
network O
for O
sorting O
( O
Ran O
et O
al O
. O
, O
2019;Chen O
et O
al O
. O
, O
2020a O
) O
. O

In O
this O
section O
, O
we O
describe O
quantified B-TaskName
reproducibility I-TaskName
assessment I-TaskName
( O
QRA B-TaskName
) O
, O
an O
approach O
that O
is O
directly O
derived O
from O
the O
concepts O
and O
definitions O
of O
metrology O
, O
adopting O
the O
latter O
exactly O
as O
they O
are O
, O
and O
yields O
assessments O
of O
the O
degree O
of O
similarity O
between O
numerical O
results O
and O
between O
the O
studies O
that O
produced O
them O
. O

We O
start O
below O
with O
the O
concepts O
and O
definitions O
that O
QRA B-TaskName
is O
based O
on O
, O
followed O
by O
an O
overview O
of O
the O
framework O
( O
Section O
3.2 O
) O
and O
steps O
in O
applying O
it O
in O
practice O
( O
Section O
3.3).The O
International O
Vocabulary O
of O
Metrology O
( O
VIM O
) O
( O
JCGM O
, O
2012 O
) O
defines O
repeatability O
and O
reproducibility O
as O
follows O
( O
defined O
terms O
in O
bold O
, O
see O
VIM O
for O
subsidiary O
defined O
terms O
): O

According O
to O
the O
ACM O
's O
definitions O
( O
Association O
for O
Computing O
Machinery O
, O
2020 O
) O
, O
results O
have O
been O
reproduced O
if O
obtained O
in O
a O
different O
study O
by O
a O
different O
team O
using O
artifacts O
supplied O
in O
part O
by O
the O
original O
authors O
, O
and O
replicated O
if O
obtained O
in O
a O
different O
study O
by O
a O
different O
team O
using O
artifacts O
not O
supplied O
by O
the O
original O
authors O
. O

The O
ACM O
originally O
had O
these O
definitions O
the O
other O
way O
around O
until O
asked O
by O
ISO O
to O
bring O
them O
in O
line O
with O
the O
scientific O
standard O
( O
ibid O
. O

) O
. O

Despite O
this O
growing O
body O
of O
research O
, O
no O
consensus O
has O
emerged O
about O
standards O
, O
terminology O
and O
definitions O
. O

Particularly O
for O
the O
two O
most O
frequently O
used O
terms O
, O
reproducibility O
and O
replicability O
, O
multiple O
divergent O
definitions O
are O
in O
use O
, O
variously O
conditioned O
on O
same O
vs. O
different O
teams O
, O
methods O
, O
artifacts O
, O
code O
, O
and O
data O
. O

For O
example O
, O
for O
Rougier O
et O
al O
. O
( O
2017 O
) O
, O
reproducing O
a O
result O
means O
running O
the O
same O
code O
on O
the O
same O
data O
and O
obtaining O
the O
same O
result O
, O
while O
replicating O
the O
result O
is O
writing O
and O
running O
new O
code O
based O
on O
the O
information O
provided O
by O
the O
original O
publication O
. O

For O
Wieling O
et O
al O
. O
( O
2018 O
) O
, O
reproducibility O
is O
achieving O
the O
same O
results O
using O
the O
same O
data O
and O
methods O
. O

Reproducibility O
more O
generally O
is O
becoming O
more O
of O
a O
research O
focus O
. O

There O
have O
been O
several O
workshops O
and O
initiatives O
on O
reproducibility O
, O
including O
workshops O
at O
ICML O
2017 O
and O
2018 O
, O
the O
reproducibility O
challenge O
at O
ICLR O
2018 O
and O
2019 O
, O
and O
at O
NeurIPS O
2019 O
and O
2020 O
, O
the O
RE O
- O
PROLANG O
( O
Branco O
et O
al O
. O
, O
2020 O
) O
initiative O
at O
LREC O
2020 O
, O
and O
the O
ReproGen O
shared O
task O
on O
reproducibility O
in O
NLG O
( O
Belz O
et O
al O
. O
, O
2021b O
) O
. O

In O
this O
paper O
, O
we O
( O
i O
) O
describe O
a O
method O
for O
quantified B-TaskName
reproducibility I-TaskName
assessment I-TaskName
( O
QRA B-TaskName
) O
directly O
derived O
from O
standard O
concepts O
and O
definitions O
from O
metrology O
which O
addresses O
the O
above O
issues O
, O
and O
( O
ii O
) O
test O
it O
on O
diverse O
sets O
of O
NLP O
results O
. O

Following O
a O
review O
of O
related O
research O
( O
Section O
2 O
) O
, O
we O
present O
the O
method O
( O
Section O
3 O
) O
, O
tests O
and O
results O
( O
Section O
4 O
) O
, O
discuss O
method O
and O
results O
( O
Section O
5 O
) O
, O
and O
finish O
with O
some O
conclusions O
( O
Section O
6).The O
situation O
memorably O
caricatured O
by O
Pedersen O
( O
2008 O
) O
still O
happens O
all O
the O
time O
: O
you O
download O
some O
code O
you O
read O
about O
in O
a O
paper O
and O
liked O
the O
sound O
of O
, O
you O
run O
it O
on O
the O
data O
provided O
, O
only O
to O
find O
that O
the O
results O
are O
not O
the O
same O
as O
reported O
in O
the O
paper O
, O
in O
fact O
they O
are O
likely O
to O
be O
worse O
( O
Belz O
et O
al O
. O
, O
2021a O
) O
. O

When O
both O
data O
and O
code O
are O
provided O
, O
the O
number O
of O
potential O
causes O
of O
such O
differences O
is O
limited O
, O
and O
the O
NLP O
field O
has O
shared O
increasingly O
detailed O
information O
about O
system O
, O
dependencies O
and O
evaluation O
to O
chase O
down O
sources O
of O
differences O
. O

Sharing O
code O
and O
data O
together O
with O
detailed O
information O
about O
them O
is O
now O
expected O
as O
standard O
, O
and O
checklists O
and O
datasheets O
have O
been O
proposed O
to O
standardise O
information O
sharing O
( O
Pineau O
, O
2020;Shimorina O
and O
Belz O
, O
2021 O
) O
. O

Being O
able O
to O
assess O
reproducibility O
of O
results O
objectively O
and O
comparably O
is O
important O
not O
only O
to O
establish O
that O
results O
are O
valid O
, O
but O
to O
provide O
evidence O
about O
which O
methods O
have O
better O
/ O
worse O
reproducibility O
and O
what O
may O
need O
to O
be O
changed O
to O
improve O
reproducibility O
. O

To O
do O
this O
, O
assessment O
has O
to O
be O
done O
in O
a O
way O
that O
is O
also O
comparable O
across O
reproduction O
studies O
of O
different O
original O
studies O
, O
e.g. O
to O
develop O
common O
expectations O
of O
how O
similar O
original O
and O
reproduction B-TaskName
results O
should O
be O
for O
different O
types O
of O
system O
, O
task O
and O
evaluation O
. O

There O
is O
no O
standard O
way O
of O
going O
about O
a O
reproduction B-TaskName
study O
in O
NLP O
, O
and O
different O
reproduction B-TaskName
studies O
of O
the O
same O
original O
set O
of O
results O
can O
differ O
substantially O
in O
terms O
of O
their O
similarity O
in O
system O
and/or O
evaluation O
design O
( O
as O
is O
the O
case O
with O
the O
Vajjala O
and O
Rama O
( O
2018 O
) O
reproductions O
, O
see O
Section O
4 O
for O
details O
) O
. O

Other O
things O
being O
equal O
, O
a O
more O
similar O
reproduction O
can O
be O
expected O
to O
produce O
more O
similar O
results O
, O
and O
such O
( O
dis)similarities O
should O
be O
factored O
into O
reproduction O
analysis O
and O
conclusions O
, O
but O
NLP O
lacks O
a O
method O
for O
doing O
so O
. O

This O
framing O
, O
whether O
the O
same O
conclusions O
can O
be O
drawn O
, O
involves O
subjective O
judgments O
and O
different O
researchers O
can O
come O
to O
contradictory O
con O
- O
clusions O
: O
e.g. O
the O
four O
papers O
( O
Arhiliuc O
et O
al O
. O
, O
2020;Bestgen O
, O
2020;Caines O
and O
Buttery O
, O
2020;Huber O
and O
Çöltekin O
, O
2020 O
) O
reproducing O
Vajjala O
and O
Rama O
( O
2018 O
) O
in O
REPROLANG O
all O
report O
similarly O
large O
differences O
, O
but O
only O
Arhiliuc O
et O
al O
. O
conclude O
that O
reproduction O
was O
unsuccessful O
. O

To O
answer O
this O
question O
for O
a O
given O
specific O
system O
, O
typically O
( O
Wieling O
et O
al O
. O
, O
2018;Arhiliuc O
et O
al O
. O
, O
2020;Popović O
and O
Belz O
, O
2021 O
) O
an O
original O
study O
is O
selected O
and O
repeated O
more O
or O
less O
closely O
, O
before O
comparing O
the O
results O
obtained O
in O
the O
original O
study O
with O
those O
obtained O
in O
the O
repeat O
, O
and O
deciding O
whether O
the O
two O
sets O
of O
results O
are O
similar O
enough O
to O
support O
the O
same O
conclusions O
. O

Reproduction B-TaskName
studies O
are O
becoming O
more O
common O
in O
Natural O
Language O
Processing O
( O
NLP O
) O
, O
with O
the O
first O
shared O
tasks O
being O
organised O
, O
including O
RE B-MethodName
- I-MethodName
PROLANG I-MethodName
( O
Branco O
et O
al O
. O
, O
2020 O
) O
and O
ReproGen B-MethodName
( O
Belz O
et O
al O
. O
, O
2021b O
) O
. O

In O
NLP O
, O
reproduction O
studies O
generally O
address O
the O
following O
question O
: O
if O
we O
create O
and/or O
evaluate O
this O
system O
multiple O
times O
, O
will O
we O
obtain O
the O
same O
results O
? O

This O
paper O
describes O
and O
tests O
a O
method O
for O
carrying O
out O
quantified B-TaskName
reproducibility I-TaskName
assessment I-TaskName
( O
QRA B-TaskName
) O
that O
is O
based O
on O
concepts O
and O
definitions O
from O
metrology O
. O

QRA B-TaskName
produces O
a O
single O
score O
estimating O
the O
degree O
of O
reproducibility O
of O
a O
given O
system O
and O
evaluation O
measure O
, O
on O
the O
basis O
of O
the O
scores O
from O
, O
and O
differences O
between O
, O
different O
reproductions O
. O

We O
test O
QRA B-TaskName
on O
18 O
system O
and O
evaluation O
measure O
combinations O
( O
involving O
diverse O
NLP O
tasks O
and O
types O
of O
evaluation O
) O
, O
for O
each O
of O
which O
we O
have O
the O
original O
results O
and O
one O
to O
seven O
reproduction O
results O
. O

The O
proposed O
QRA B-TaskName
method O
produces O
degree B-MetricName
- I-MetricName
of I-MetricName
- I-MetricName
reproducibility I-MetricName
scores O
that O
are O
comparable O
across O
multiple O
reproductions O
not O
only O
of O
the O
same O
, O
but O
of O
different O
original O
studies O
. O

We O
find O
that O
the O
proposed O
method O
facilitates O
insights O
into O
causes O
of O
variation O
between O
reproductions O
, O
and O
allows O
conclusions O
to O
be O
drawn O
about O
what O
changes O
to O
system O
and/or O
evaluation O
design O
might O
lead O
to O
improved O
reproducibility O
. O

Speech B-TaskName
translation I-TaskName
( O
ST B-TaskName
) O
aims O
at O
translating O
from O
source O
language O
speech O
into O
target O
language O
text O
, O
which O
is O
widely O
helpful O
in O
various O
scenarios O
such O
as O
conference O
speeches O
, O
business O
meetings O
, O
crossborder O
customer O
service O
, O
and O
overseas O
travel O
. O

There O
are O
two O
kinds O
of O
application O
scenarios O
, O
including O
the O
non O
- O
streaming O
translation O
and O
the O
streaming O
one O
. O

The O
non O
- O
streaming O
models O
can O
listen O
to O
the O
complete O
utterances O
at O
one O
time O
and O
then O
generate O
the O
translation O
afterward O
. O

While O
, O
the O
streaming O
models O
need O
to O
balance O
the O
latency O
and O
quality O
and O
generate O
translations O
based O
on O
the O
partial O
utterance O
, O
as O
shown O
in O
Figure O
1 O
. O

( O
1 O
) O
Source O
How O
to O
find O
proper O
moments O
to O
generate O
partial O
sentence O
translation O
given O
a O
streaming O
speech O
input O
? O
Existing O
approaches O
waitingand O
- O
translating O
for O
a O
fixed O
duration O
often O
break O
the O
acoustic O
units O
in O
speech O
, O
since O
the O
boundaries O
between O
acoustic O
units O
in O
speech O
are O
not O
even O
. O

In O
this O
paper O
, O
we O
propose O
MoSST B-MethodName
, O
a O
simple O
yet O
effective O
method O
for O
translating B-TaskName
streaming I-TaskName
speech I-TaskName
content I-TaskName
. O

Given O
a O
usually O
long O
speech O
sequence O
, O
we O
develop O
an O
efficient O
monotonic O
segmentation O
module O
inside O
an O
encoder O
- O
decoder O
model O
to O
accumulate O
acoustic O
information O
incrementally O
and O
detect O
proper O
speech O
unit O
boundaries O
for O
the O
input O
in O
speech B-TaskName
translation I-TaskName
task O
. O

Experiments O
on O
multiple O
translation O
directions O
of O
the O
MuST B-DatasetName
- I-DatasetName
C I-DatasetName
dataset O
show O
that O
MoSST B-MethodName
outperforms O
existing O
methods O
and O
achieves O
the O
best O
trade O
- O
off O
between O
translation O
quality O
( O
BLEU B-MetricName
) O
and O
latency O
. O

Our O
code O
is O
available O
at O
https://github O
. O

com O
/ O
dqqcasia O
/ O
mosst B-MethodName
. O

* O
Equal O
contribution O
. O

† O
Work O
is O
done O
while O
at O
ByteDance O
. O

EN O
: O
Do O
you O
know O
that O
one O
of O
the O
intense O
pleasures O
of O
travel O
and O
one O
of O
the O
delights O
of O
ethnographic O
research O
is O
the O
opportunity O
to O
live O
amongst O
those O
who O
have O
not O
forgotten O
the O
old O
ways O
, O
who O
still O
feel O
their O
past O
in O
the O
wind O
, O
touch O
it O
in O
stones O
polished O
by O
rain O
, O
taste O
it O
in O
the O
bitter O
leaves O
of O
plants O
. O

Paraphrasing B-TaskName
, O
Transliteration B-TaskName
, O
and O
Title B-TaskName
Generation I-TaskName
Output O
. O

Tables O
D.3 O
, O
D.4 O
, O
and O
D.5 O
each O
shows O
two O
output O
samples O
from O
our O
paraphrasing B-TaskName
, O
transliteration B-TaskName
, O
and O
title B-TaskName
generation I-TaskName
models O
, O
respectively O
. O

In O
each O
case O
, O
the O
samples O
are O
high O
- O
quality O
, O
informative O
, O
and O
fluent O
. O

Our O
paraphrase O
samples O
also O
tightly O
capture O
the O
meaning O
of O
the O
source O
sentences O
. O

( O
1 O
) O
Source O
: O
: O
MSA O
Target O
: O

( O
3 O
) O
We O
run O
the O
multi O
- O
lingual O
semantic O
similarity O
model O
from O
Yang O
et O
al O
. O
( O
2019 O
) O
on O
the O
Arabic O
machine O
translated O
sentences O
and O
the O
human O
translation O
( O
i.e. O
, O
original O
Arabic O
sentences O
from O
OPUS B-DatasetName
) O
, O
keeping O
only O
sentences O
with O
an O
arbitrary O
semantic B-HyperparameterName
similarity I-HyperparameterName
score I-HyperparameterName
between O
0.70 B-HyperparameterValue
and O
0.99 B-HyperparameterValue
. O

This O
allows O
us O
to O
filter O
out O
identical O
sentence O
pairs O
( O
i.e. O
, O
similarity B-HyperparameterName
score I-HyperparameterName
= O
1 B-HyperparameterValue
) O
and O
those O
that O
are O
not O
good O
translations O
( O
i.e. O
, O
those O
with O
a O
semantic B-HyperparameterName
similarity I-HyperparameterName
score I-HyperparameterName
< O
0.70 B-HyperparameterValue
) O
. O

( O
4 O
) O
In O
order O
to O
maximize O
syntactic O
and O
lexical O
diversity O
of O
the O
pairs O
of O
paraphrased O
sentences O
, O
we O
perform O
an O
analysis O
based O
on O
word O
overlap O
between O
the O
semantically O
similar O
pair O
sentences O
( O
i.e. O
, O
the O
output O
of O
the O
previous O
step O
) O
. O

We O
then O
perform O
a O
manual O
analysis O
of O
the O
data O
, O
identifying O
sentences B-HyperparameterName
with I-HyperparameterName
unigram I-HyperparameterName
token I-HyperparameterName
overlap I-HyperparameterName
between O
35 B-HyperparameterValue
% I-HyperparameterValue
and O
70 B-HyperparameterValue
% I-HyperparameterValue
as O
sufficiently O
distinct O
paraphrase O
pairs O
. O

This O
gives O
us O
122 B-HyperparameterValue
K I-HyperparameterValue
paraphrase B-HyperparameterName
pairs I-HyperparameterName
. O

We O
split O
these O
sentence B-HyperparameterName
pairs I-HyperparameterName
into O
116 B-HyperparameterValue
K I-HyperparameterValue
for O
training O
and O
6 B-HyperparameterValue
K I-HyperparameterValue
for O
validation O
. O

In O
this O
section O
we O
describe O
the O
ARGEN B-DatasetName
MT I-DatasetName
datasets O
splits O
and O
report O
the O
evaluation O
results O
in O
validation O
datasets O
. O

Jordanian O
dialect O
translated O
into O
English O
. O

In O
both O
cases O
, O
our O
models O
not O
only O
handle O
the O
dialects O
but O
also O
their O
use O
in O
code O
- O
switched O
contexts O
better O
than O
mT5 B-MethodName
. O

( O
2 O
) O
We O
translate O
the O
English O
sentences O
using O
a O
high O
- O
quality O
in O
- O
house O
English→Arabic O
MT B-TaskName
model O
. O

We O
also O
run O
an O
in O
- O
house O
MSA O
- O
dialect O
classifier O
on O
the O
same O
1 B-HyperparameterValue
M I-HyperparameterValue
data B-HyperparameterName
sample I-HyperparameterName
. O

The O
classifier O
predicts O
an O
overriding O
majority O
of O
the O
data O
( O
99.83 O
% O
) O
as O
MSA O
. O

We O
again O
manually O
inspect O
∼ O
100 B-MetricName
samples B-HyperparameterName
from O
the O
small O
fraction O
predicted O
as O
dialects O
( O
i.e. O
, O
0.17 O
% O
) O
. O

While O
we O
find O
some O
of O
these O
to O
be O
actual O
dialectal O
text O
( O
usually O
short O
belonging O
to O
either O
Egyptian O
or O
Saudi O
dialects O
) O
from O
web O
fora O
, O
in O
the O
majority O
of O
cases O
the O
text O
is O
simply O
names O
of O
soap O
operas O
or O
advertisements O
. O

Our O
own O
pre O
- O
training O
data O
in O
the O
case O
of O
Twitter O
, O
in O
comparison O
, O
involve O
much O
more O
dialectal O
content O
( O
28.39 O
% O
as O
listed O
in O
§ O
2.1 O
) O
. O

Baselines O
. O

For O
comparison O
, O
we O
fine O
- O
tune O
a O
number O
of O
models O
on O
the O
same O
training O
data O
as O
our O
new O
models O
. O

These O
include O
the O
multilingual O
sequenceto O
- O
sequence O
model O
mT5 B-MethodName
( O
Xue O
et O
al O
. O
, O
2020 O
) O
, O
and O
the O
powerful O
Arabic O
- O
specific O
BERT O
- O
based O
model O
MARBERT B-MethodName
( O
Abdul O
- O
Mageed O
et O
al O
. O
, O
2021 O
) O
. O

We O
note O
that O
MARBERT B-MethodName
achieves O
the O
SOTA O
22 O
across O
the O
majority O
of O
6 O
cluster O
tasks O
of O
ARLUE B-DatasetName
, O
with O
the O
highest O
ARLUE B-DatasetName
score O
. O

Settings O
and O
Evaluation O
. O

We O
evaluate O
our O
models O
on O
the O
language O
understanding O
benchmark O
, O
AR B-DatasetName
- I-DatasetName
LUE I-DatasetName
, O
under O
two O
settings O
: O
( O
i O
) O
single O
task O
learning O
and O
( O
ii O
) O
multi O
- O
task O
learning O
. O

We O
present O
results O
on O
all O
the O
task O
clusters O
included O
in O
ARLUE B-DatasetName
except O
for O
NER B-TaskName
which O
is O
a O
token O
- O
level O
task O
that O
is O
not O
straightforward O
with O
the O
text O
- O
to O
- O
text O
set O
up O
we O
adopt O
. O

Table O
B.2 O
shows O
our O
evaluation O
results O
using O
the O
relevant O
metric O
for O
each O
task O
. O

Abdul O
- O
Mageed O
et O
al O
. O
( O
2021 O
) O
introduced O
ARLUE B-DatasetName
score O
, O
a O
metric O
used O
to O
score O
pre O
- O
trained O
language O
model O
performance O
on O
multiple O
datasets O
. O

AR B-DatasetName
- I-DatasetName
LUE I-DatasetName
score O
is O
a O
simply O
macro O
- O
average O
of O
the O
different O
scores O
across O
all O
task O
clusters O
, O
where O
each O
task O
is O
weighted O
equally O
following O
( O
Wang O
et O
al O
. O
, O
2018 O
) O
. O

We O
compute O
the O
ARLUE B-DatasetName
score O
( O
i.e. O
, O
overall O
macro O
- O
average O
) O
for O
each O
of O
our O
three O
models O
( O
i.e. O
, O
AraT5 B-MethodName
MSA I-MethodName
, O
AraT5 B-MethodName
Tw I-MethodName
, O
and O
AraT5 B-MethodName
) O
and O
the O
baseline O
( O
mT5 B-MethodName
) O
. O

Single O
Task O
. O

We O
fine O
- O
tune O
our O
three O
models O
and O
22 O
MARBERT B-MethodName
outperform O
both O
multilingual O
encoder O
- O
only O
Transformers O
mBERT B-MethodName
, O
XLM B-MethodName
- I-MethodName
RBase I-MethodName
, O
XLM B-MethodName
- I-MethodName
RLarge I-MethodName
, O
and O
Arabicspecific O
BERT O
- O
based O
AraBERT B-MethodName
( O
Antoun O
et O
al O
. O
, O
2020 O
) O
, O
AR B-MethodName
- I-MethodName
BERT I-MethodName
( O
Abdul O
- O
Mageed O
et O
al O
. O
, O
2021 O
) O
. O

mT5 B-MethodName
individually O
on O
each O
of O
the O
six O
tasks O
of O
AR B-DatasetName
- I-DatasetName
LUE I-DatasetName
. O

We O
typically O
( O
i.e. O
, O
in O
all O
our O
experiments O
) O
identify O
the O
best O
checkpoint O
for O
each O
model O
on O
the O
development O
set O
, O
and O
report O
its O
performance O
on O
both O
development O
and O
test O
data O
. O

As O
Table O
B.2 O
shows O
, O
our O
AraT5 B-MethodName
model O
achieves O
the O
highest O
AR B-DatasetName
- I-DatasetName
LUE I-DatasetName
score O
( O
77.52 B-MetricValue
) O
, O
followed O
by O
AraT5 B-MethodName
MSA I-MethodName
( O
77.50 B-MetricValue
) O
and O
AraT5 B-MethodName
TW I-MethodName
( O
75.33 B-MetricValue
) O
. O

We O
note O
that O
all O
our O
models O
outperform O
mT5 B-MethodName
and O
the O
MARBERT B-MethodName
( O
SOTA O
) O
by O
∼ O
+2.74 B-MetricValue
and O
∼ O
+1 B-MetricValue
ARLUE B-MetricName
score I-MetricName
points I-MetricName
, O
respectively O
. O

Multitask O
. O

We O
also O
investigate O
multitask O
learning O
( O
Caruana O
, O
1997;Ruder O
, O
2017 O
) O
with O
our O
AraT5 B-MethodName
models O
. O

This O
approach O
consists O
of O
training O
the O
model O
on O
multiple O
tasks O
simultaneously O
( O
i.e. O
, O
the O
model O
and O
its O
parameters O
are O
shared O
across O
all O
tasks O
) O
in O
order O
to O
eventually O
improve O
performance O
on O
each O
individual O
task O
. O

In O
our O
case O
, O
we O
fine O
- O
tune O
our O
models O
on O
many O
tasks O
at O
the O
same O
time O
using O
: O
( O
i O
) O
The O
three O
dialect O
datasets O
: O
ARLUE B-DatasetName
Dia I-DatasetName
- I-DatasetName
B I-DatasetName
, O
ARLUE B-DatasetName
Dia I-DatasetName
- I-DatasetName
R I-DatasetName
, O
and O
ARLUE B-DatasetName
Dia I-DatasetName
- I-DatasetName
C I-DatasetName
and O
( O
ii O
) O
the O
social O
meaning O
datasets O
of O
ARLUE B-DatasetName
SM I-DatasetName
. O

Table O
B O
.3 O
and O
Table O
B.4 O
show O
the O
results O
of O
multi O
- O
task O
experiments O
for O
dialect O
settings O
and O
social O
meaning O
, O
respectively O
. O

Our O
results O
show O
that O
multi O
- O
task O
training O
outperforms O
single O
task O
models O
in O
the O
majority O
of O
the O
dialects O
experiments O
( O
n=7 O
out O
of O
9 O
experiments O
, O
77.78 O
% O
of O
the O
tasks O
) O
and O
half O
of O
the O
social O
meaning O
tasks O
( O
n=18 O
out O
of O
36 O
experiments O
, O
50 O
% O
of O
the O
tasks O
) O
. O

These O
results O
are O
promising O
, O
and O
hence O
we O
plan O
to O
further O
investigate O
multi O
- O
task O
learning O
with O
our O
new O
models O
in O
the O
future O
. O

AraPara B-MethodName
. O

is O
a O
new O
multi O
- O
domain O
Arabic O
paraphrasing O
dataset O
we O
create O
using O
English O
- O
Arabic O
parallel O
OPUS B-DatasetName
data O
( O
Tiedemann O
, O
2012 O
) O
. O

To O
ensure O
highquality O
, O
we O
follow O
four O
careful O
steps O
: O
( O
1 O
) O
We O
pick O
1 O
million O
English O
- O
Arabic O
parallel O
sentences O
from O
OPUS B-DatasetName
( O
Tiedemann O
, O
2012 O
) O
covering O
the O
different O
domains O
. O

AraT5 B-MethodName
Models O
Release O
. O

All O
our O
pre O
- O
trained O
models O
are O
publicly O
available O
for O
non O
- O
malicious O
use O
. O

We O
acknowledge O
our O
models O
may O
still O
be O
misused O
in O
real O
world O
. O

However O
, O
we O
hope O
the O
models O
will O
be O
deployed O
in O
domains O
such O
as O
education O
, O
disaster O
management O
, O
health O
, O
recreation O
, O
travel O
, O
etc O
. O

in O
socially O
beneficial O
ways O
. O

These O
meaningful O
potential O
use O
cases O
are O
behind O
our O
decision O
to O
release O
the O
models O
. O

A O
A O
Study O
of O
Arabic O
mC4 B-DatasetName
Data O
Quality O
Xue O
et O
al O
. O
( O
2020 O
) O
train O
mT5 B-MethodName
on O
the O
mC4 B-DatasetName
dataset O
. O

They O
report O
57B B-HyperparameterValue
Arabic B-HyperparameterName
tokens I-HyperparameterName
( O
almost O
double O
our O
token O
size O
) O
from O
53 B-HyperparameterValue
M I-HyperparameterValue
webpages B-HyperparameterName
, O
making O
1.66 B-HyperparameterValue
% I-HyperparameterValue
of O
all O
mT5 B-MethodName
data O
. O

For O
our O
analysis O
, O
we O
randomly O
sample O
1 B-HyperparameterValue
M I-HyperparameterValue
paragraphs B-HyperparameterName
from O
the O
Arabic O
part O
of O
mC4 B-DatasetName
. O

We O
use O
paragraphs O
rather O
than O
whole O
documents O
for O
a O
more O
fine O
- O
grained O
analysis O
that O
is O
more O
comparable O
to O
our O
own O
data O
( O
especially O
in O
the O
case O
of O
Twitter O
) O
. O

We O
first O
perform O
language O
identification O
using O
CLD3 O
( O
McCandless O
, O
2010 O
) O
on O
the O
data O
. O

We O
find O
a O
sizable O
amount B-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
data I-HyperparameterName
( O
i.e. O
, O
13.59 B-HyperparameterValue
% I-HyperparameterValue
) O
to O
be O
non O
- O
Arabic O
( O
mostly O
English O
or O
French O
) O
. O

We O
manually O
inspect O
∼ O
100 O
random O
samples O
of O
the O
data O
predicted O
as O
non O
- O
Arabic O
. O

We O
find O
these O
are O
mostly O
either O
non O
- O
linguistic O
content O
( O
e.g. O
, O
java O
- O
script O
or O
HTML O
code O
) O
or O
non O
- O
Arabic O
text O
. O

The O
non O
- O
Arabic O
text O
is O
sometimes O
foreign O
language O
advertising O
or O
even O
full O
translation O
of O
the O
Arabic O
text O
in O
some O
cases O
. O

In O
many O
cases O
, O
non O
- O
Arabic O
is O
also O
boilerplate O
text O
such O
as O
that O
in O
web O
fora O
. O

Also O
, O
no O
samples O
of O
the O
non O
- O
Arabic O
included O
real O
code O
- O
switching O
. O

Table O
7 O
: O
CS O
sentences O
with O
their O
English O
/ O
French O
translations O
using O
our O
Models O
and O
mT5 B-MethodName
. O

Data O
samples O
are O
extracted O
from O
the O
Dev O
datasets O
. O

Green O
refers O
to O
good O
translation O
. O

Red O
refers O
to O
problematic O
translation O
. O

We O
also O
perform O
qualitative O
analyses O
of O
the O
outputs O
of O
several O
of O
our O
models O
, O
including O
as O
to O
length B-HyperparameterName
of I-HyperparameterName
MT I-HyperparameterName
source I-HyperparameterName
data I-HyperparameterName
( O
Appendix O
D O
) O
. O

In O
particular O
, O
our O
analyses O
are O
for O
the O
following O
tasks O
: O
machine B-TaskName
translation I-TaskName
, O
code B-TaskName
- I-TaskName
switched I-TaskName
translation I-TaskName
, O
paraphrasing B-TaskName
, O
transliteration B-TaskName
, O
and O
news B-TaskName
title I-TaskName
generation I-TaskName
. O

MT B-TaskName
Model O
. O

Paraphrasing B-TaskName
, O
Transliteration B-TaskName
, O
and O
Title B-TaskName
Generation I-TaskName
. O

Each O
of O
Tables O
D.3 O
, O
D.4 O
, O
and O
D.5 O
( O
Appendix O
D O
) O
shows O
two O
output O
samples O
from O
our O
paraphrasing B-TaskName
, O
transliteration B-TaskName
, O
and O
title B-TaskName
generation I-TaskName
models O
, O
respectively O
. O

In O
each O
case O
, O
the O
samples O
are O
high O
- O
quality O
, O
informative O
, O
and O
fluent O
. O

Our O
paraphrase O
samples O
also O
tightly O
capture O
the O
meaning O
of O
the O
source O
sentences O
. O

Multilingual O
LMs O
. O

mBERT B-MethodName
is O
the O
multilingual O
version O
of O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
which O
is O
an O
encoder O
model O
with O
bidirectional O
representations O
from O
Transformers O
trained O
with O
a O
denoising O
objective O
. O

mBERT B-MethodName
is O
trained O
on O
Wikipedia B-DatasetName
for O
104 B-HyperparameterValue
languages B-HyperparameterName
, O
including O
Arabic O
. O

XLM B-MethodName
- I-MethodName
R I-MethodName
( O
Conneau O
et O
al O
. O
, O
2020 O
) O
is O
also O
a O
Transformer O
- O
based O
multilingual O
masked O
language O
model O
pre O
- O
trained O
on O
more O
than O
2 O
TB O
of O
CommonCrawl B-DatasetName
( O
CC B-DatasetName
) O
data O
in O
100 B-HyperparameterValue
languages B-HyperparameterName
, O
including O
Arabic O
( O
2.9B O
tokens O
) O
. O

XLM B-MethodName
- I-MethodName
R I-MethodName
model O
uses O
the O
same O
masking O
objective O
as O
BERT B-MethodName
, O
but O
not O
the O
next O
sentence O
prediction O
. O

mT5 B-MethodName
( O
Xue O
et O
al O
. O
, O
2020 O
) O
is O
the O
multilingual O
version O
of O
Textto B-MethodName
- I-MethodName
Text I-MethodName
Transfer I-MethodName
Transformer I-MethodName
model I-MethodName
( O
T5 B-MethodName
) O
( O
Raffel O
et O
al O
. O
, O
2019 O
As O
we O
have O
demonstrated O
, O
our O
resulting O
models O
are O
better O
equipped O
to O
power O
applications O
involving O
several O
varieties O
of O
Arabic O
as O
well O
as O
code O
- O
switched O
language O
use O
involving O
Arabic O
. O

From O
this O
perspective O
, O
we O
hope O
they O
add O
to O
ongoing O
efforts O
in O
the O
community O
to O
design O
models O
that O
are O
fairer O
and O
more O
representative O
. O

ARGEN B-DatasetName
Benchmark O
Release O
. O

We O
design O
AR B-DatasetName
- I-DatasetName
GEN I-DatasetName
using O
both O
existing O
datasets O
and O
new O
datasets O
that O
we O
create O
for O
this O
work O
. O

In O
our O
accompanying O
GitHub O
repository O
, O
we O
link O
to O
all O
existing O
publicly O
available O
components O
of O
the O
benchmark O
with O
standard O
splits O
from O
source O
as O
well O
as O
components O
that O
can O
be O
acquired O
from O
data O
organizations O
. O

In O
addition O
, O
we O
released O
all O
the O
new O
datasets O
we O
have O
developed O
. O

While O
we O
have O
prioritized O
standardizing O
evaluation O
on O
as O
many O
unified O
and O
consolidated O
datasets O
and O
tasks O
as O
possible O
, O
we O
also O
report O
performance O
on O
individual O
test O
sets O
so O
as O
to O
enable O
the O
community O
to O
replicate O
our O
work O
even O
on O
particular O
parts O
or O
tasks O
of O
ARGEN B-DatasetName
if O
they O
so O
wish O
. O

Code O
- O
Switching O
. O

We O
also O
study O
code O
- O
switching O
in O
both O
our O
Twitter B-DatasetName
dataset O
and O
the O
Arabic O
part O
of O
mC4 B-DatasetName
. O

We O
find O
that O
while O
our O
Twitter B-DatasetName
data O
involves O
natural O
code O
- O
switching O
( O
∼ O
4 B-HyperparameterValue
% I-HyperparameterValue
of O
sequences B-HyperparameterName
) O
, O
codeswitching O
in O
Arabic O
mC4 B-DatasetName
is O
very O
rare O
. O

This O
explains O
the O
strong O
performance O
of O
our O
AraT5 B-MethodName
Tw I-MethodName
model O
on O
the O
natural O
code B-TaskName
- I-TaskName
switched I-TaskName
translation I-TaskName
data O
on O
French O
. O

We O
conjecture O
that O
mT5 B-MethodName
good O
performance O
on O
English O
code O
- O
switched O
data O
is O
due O
to O
it O
being O
pre O
- O
trained O
on O
very O
large O
amounts O
of O
English O
rather O
than O
natural O
code O
- O
switching O
. O

We O
were O
inquisitive O
how O
MT B-TaskName
models O
fine O
- O
tuning O
our O
pre O
- O
trained O
language O
models O
compare O
to O
mT5 B-MethodName
under O
different O
length O
conditions O
. O

For O
this O
, O
we O
( O
1 O
) O
Source O
: O
J'aime O
une O
vidéo O
Episode O
1 O
-4 O
: O
ALG O
- O
FR O
Target O
: O
FR O
: O
J O
' O
aime O
une O
vidéo O
Episode O
1 O
-ma O
chère O
belle O
- O
mère O
4 O
mT5 O
J O
' O
aime O
une O
v O
- O
Chère O
nièce O
4.J'aime O
une O
vidéo O
Episode O
1 O
-ma O
chère O
tante O
4.J'aime O
une O
vidéo O
1 O
-Ma O
chère O
soeur O
4.J'aime O
une O
vidéo O
1 O
-Ma O
chère O
bébé O

As O
pointed O
out O
earlier O
, O
Kreutzer O
et O
al O
. O
( O
2021 O
) O
find O
systematic O
issues O
with O
data O
representing O
several O
languages O
( O
including O
Arabic O
) O
in O
the O
mC4 B-DatasetName
dataset O
on O
which O
mT5 B-MethodName
is O
pre O
- O
trained O
. O

We O
perform O
a O
data O
quality O
study O
confirming O
the O
findings O
of O
Kreutzer O
et O
al O
. O
( O
2021 O
) O
. O

We O
also O
find O
Arabic O
mC4 B-DatasetName
data O
to O
be O
less O
geographically O
diverse O
than O
our O
Twitter B-DatasetName
pretraining O
data O
( O
described O
in O
§ O
2.1 O
) O
. O

Our O
mC4 B-DatasetName
data O
study O
is O
in O
Appendix O
A. O

Our O
results O
confirm O
the O
utility O
of O
dedicated O
language O
models O
as O
compared O
to O
multilingual O
models O
such O
as O
mT5 B-MethodName
( O
101 O
+ O
languages O
) O
. O

Our O
AraT5 B-MethodName
model O
outperforms O
mT5 B-MethodName
, O
even O
though O
it O
is O
pre O
- O
trained O
with O
49 B-HyperparameterValue
% I-HyperparameterValue
less O
data B-HyperparameterName
( O
see O
§ O
2.1 O
) O
. O

One O
reason O
might O
be O
that O
massively O
multilingual O
models O
are O
more O
prone O
to O
suffering O
from O
capacity O
issues O
. O

Data O
quality O
is O
another O
challenge O
for O
multilingual O
models O
. O

We O
report O
results O
in O
For O
the O
two O
ARGEN B-DatasetName
ST I-DatasetName
datasets O
, O
we O
fine O
- O
tune O
and O
identify O
the O
best O
model O
on O
the O
Train O
and O
Dev O
splits O
of O
WikiLingua B-DatasetName
( O
Faisal O
Ladhak O
and O
McKeown O
, O
2020 O
) O
and O
test O
on O
all O
EASC B-DatasetName
and O
the O
Test O
of O
Wik B-DatasetName
- I-DatasetName
iLingua I-DatasetName
. O

We O
report O
different O
ROUGE B-MetricName
scores O
( O
Lin O
, O
2004 O
) O
in O
Table O
5 O
. O

As O
the O
Table O
shows O
, O
AraT5 B-MethodName
Tw I-MethodName
acquires O
best O
results O
on O
WikiLingua B-DatasetName
data O
, O
while O
mT5 B-MethodName
outperforms O
us O
on O
EASC B-DatasetName
( O
we O
hypothesize O
since O
EASC B-DatasetName
is O
older O
data O
that O
is O
likely O
part O
of O
the O
mC4 B-DatasetName
on O
which O
mT5 B-MethodName
was O
pre O
- O
trained O
) O
. O

On O
both O
datasets O
, O
we O
establish O
new O
SOTA O
( O
both O
with O
our O
pre O
- O
trained O
models O
and O
mT5).For B-MethodName
both O
tasks O
, O
we O
fine O
- O
tune O
all O
our O
models O
on O
the O
Train O
splits O
of O
ARGEN B-DatasetName
NTG I-DatasetName
and O
ARGEN B-DatasetName
QG I-DatasetName
, O
respectively O
. O

As O
Table O
6 O
shows O
, O
all O
our O
models O
outperform O
mT5 B-MethodName
on O
each O
of O
the O
two O
tasks O
. O

AraT5 B-MethodName
MSA I-MethodName
excels O
with O
20.61 B-MetricValue
% I-MetricValue
BLEU B-MetricName
on O
ARGEN B-DatasetName
NTG I-DatasetName
and O
AraT5 B-MethodName
is O
at O
16.99 B-MetricValue
% I-MetricValue
on O
ARGEN B-DatasetName
QG I-DatasetName
.For O
the O
paraphrasing B-TaskName
task O
, O
we O
fine O
- O
tune O
and O
validate O
on O
our O
new O
AraPra B-DatasetName
dataset O
and O
blind O
- O
test O
on O
both O
APB B-DatasetName
and O
ASEP B-DatasetName
datasets O
( O
described O
in O
§ O
3.6 O
) O
. O

5.1 O
Multilingual O
vs. O
Dedicated O
Models O
. O

In O
addition O
, O
our O
AraT5 B-MethodName
model O
outperforms O
even O
the O
S2S O
model O
trained O
with O
5X O
more O
data O
. O

For O
completeness O
, O
we O
also O
provide O
the O
current O
SOTA O
on O
each O
of O
our O
datasets O
. O

We O
do O
not O
compare O
our O
results O
to O
SOTA O
since O
these O
are O
acquired O
by O
models O
fine O
- O
tuned O
on O
much O
larger O
datasets O
than O
ours O
. O

For O
example O
, O
Sajjad O
et O
al O
. O
( O
2020 O
) O
exploit O
∼ O
42 B-HyperparameterValue
M I-HyperparameterValue
parralel B-HyperparameterName
sentences I-HyperparameterName
to O
train O
their O
models O
. O

To O
limit O
GPU O
needs O
during O
our O
experiments O
, O
especially O
given O
the O
time O
- O
consuming O
fine O
- O
tuning O
process O
typical O
of O
T5 B-MethodName
models O
, O
we O
do O
not O
fine O
- O
tune O
the O
models O
on O
the O
full O
amounts O
of O
available O
parallel O
data O
. O

However O
, O
in O
the O
future O
we O
plan O
to O
compare O
our O
models O
under O
the O
full O
data O
setting O
. O

X O
→ O
Arabic O
. O

Our O
language O
models O
are O
not O
pretrained O
on O
foreign O
data O
, O
but O
we O
include O
vocabulary O
from O
11 B-HyperparameterValue
foreign B-HyperparameterName
languages I-HyperparameterName
. O

Our O
X O
→ O
Arabic O
experiments O
here O
are O
hence O
zero O
- O
shot O
( O
from O
the O
perspective O
of O
pre O
- O
training O
) O
. O

Table O
4.2 O
shows O
the O
results O
of O
AraT5 B-MethodName
MSA I-MethodName
and O
mT5 B-MethodName
on O
OPUS B-DatasetName
- I-DatasetName
X I-DatasetName
- I-DatasetName
Ara I-DatasetName
. O

16 O
We O
observe O
that O
our O
model O
outperforms O
mT5 B-MethodName
in O
the O
four O
X O
→ O
Arabic O
sub O
- O
tasks O
with O
an O
average O
of O
+1.12 B-MetricValue
and O
+0.86 B-MetricValue
BLEU B-MetricName
points O
on O
Dev O
and O
Test O
, O
respectively O
. O

For O
this O
task O
, O
we O
test O
on O
the O
two O
natural O
codeswitched B-TaskName
translation I-TaskName
( O
CST B-TaskName
) O
test O
sets O
that O
we O
manually O
created O
, O
ALG O
- O
FR→FR O
and O
JOR O
- O
EN→EN O
. O

We O
also O
evaluate O
on O
our O
two O
synthetic O
CST O
datasets O
, O
MSA B-DatasetName
- I-DatasetName
EN I-DatasetName
and O
MSA B-DatasetName
- I-DatasetName
FR I-DatasetName
, O
one O
time O
with O
EN O
/ O
FR O
as O
target O
( O
e.g. O
, O
MSA O
- O
EN→EN O
) O
and O
another O
with O
MSA O
as O
target O
( O
e.g. O
, O
MSA O
- O
EN→MSA O
) O
. O

We O
fine O
- O
tune O
our O
three O
pre O
- O
trained O
models O
as O
well O
as O
mT5 B-MethodName
on O
the O
OPUS B-DatasetName
- I-DatasetName
X I-DatasetName
- I-DatasetName
Ara I-DatasetName
segments O
involving O
English O
and O
French O
( O
each O
with O
1 B-HyperparameterValue
M I-HyperparameterValue
parallel B-HyperparameterName
sentences I-HyperparameterName
, O
described O
in O
§ O
3.1.2 O
) O
, O
in O
both O
directions O
. O

Since O
these O
MT B-TaskName
models O
are O
only O
fine O
- O
tuned O
on O
parallel O
monolingual O
data O
, O
we O
refer O
to O
these O
experiments O
as O
zeroshot O
. O

We O
test O
these O
models O
on O
both O
our O
natural O
and O
synthetic O
code O
- O
switched O
data O
( O
described O
in O
§ O
3.2 O
) O
. O

Fine O
- O
tuned O
mT5 B-MethodName
is O
our O
second O
baseline O
baseline O
II O
. O

Arabic O
→ O
English O
. O

Results O
of O
ARGEN B-DatasetName
MT I-DatasetName
are O
reported O
in O
Table O
2 O
. O

Results O
show O
that O
our O
models O
achieve O
best O
BLEU B-MetricName
score O
in O
37 O
out O
of O
the O
42 O
tests O
splits O
. O

AraT5 B-MethodName
MSA I-MethodName
acquires O
best O
results O
in O
32 O
of O
these O
test O
splits O
, O
outperforming O
all O
the O
baselines O
( O
S2S B-MethodName
2 I-MethodName
M I-MethodName
) O
, O
( O
S2S B-MethodName
10 I-MethodName
M I-MethodName
) O
, O
and O
mT5 B-MethodName
with O
+5.25 B-MetricValue
, O
+4.99 B-MetricValue
, O
and O
+0.45 B-MetricValue
BLEU B-MetricName
points O
. O

These O
results O
are O
striking O
since O
our O
language O
models O
are O
pre O
- O
trained O
on O
Arabic O
data O
only O
( O
although O
they O
include O
English O
vocabulary O
and O
marginal O
amounts O
of O
code O
- O
switching O
; O
see O
§ O
2.1 O
) O
. O

In O
other O
words O
, O
even O
under O
this O
arguably O
zero O
- O
shot O
setting O
, O
15 O
the O
models O
perform O
very O
well O
. O

These O
are O
MSR B-DatasetName
- I-DatasetName
Paraphrase I-DatasetName
( O
510 B-HyperparameterValue
pairs B-HyperparameterName
) O
, O
MSR B-DatasetName
- I-DatasetName
Video I-DatasetName
( O
368 B-HyperparameterValue
pairs B-HyperparameterName
) O
, O
and O
SMTeuroparl B-DatasetName
( O
203 B-HyperparameterValue
pairs B-HyperparameterName
) O
. O

The O
pairs O
are O
labeled O
with O
a O
similarity O
score O
on O
a O
scale O
from O
0 O
to O
5 O
. O

For O
our O
purpose O
, O
we O
only O
keep O
sentence O
pairs O
with O
a O
semantic O
similarity O
score O
≥ O
3.5 O
which O
gives O
us O
603 B-HyperparameterValue
pairs B-HyperparameterName
. O

We O
merge O
and O
shuffle O
all O
three O
ASEP B-DatasetName
datasets O
for O
our O
use O
. O

Arabic B-DatasetName
Paraphrasing I-DatasetName
Benchmark I-DatasetName
( O
APB B-DatasetName
) O
. O

APB B-DatasetName
is O
created O
by O
Alian O
et O
al O
. O
( O
2019 O
) O
. O

It O
consists O
of O
1 B-HyperparameterValue
, I-HyperparameterValue
010 I-HyperparameterValue
Arabic B-HyperparameterName
sentence I-HyperparameterName
pairs I-HyperparameterName
that O
are O
collected O
from O
different O
Arabic O
books O
. O

Paraphrasing B-TaskName
was O
performed O
manually O
using O
six O
transformation O
procedures O
( O
i.e. O
, O
addition O
, O
deletion O
, O
expansion O
, O
permutation O
, O
reduction O
, O
and O
replacement).Transliteration B-TaskName
involves O
mapping O
a O
text O
written O
with O
orthographic O
symbols O
in O
a O
given O
script O
into O
another O
( O
Beesley O
, O
1998 O
) O
. O

We O
use O
the O
BOLT B-DatasetName
Egyptian I-DatasetName
Arabic I-DatasetName
SMS I-DatasetName
/ I-DatasetName
Chat I-DatasetName
and I-DatasetName
Transliteration I-DatasetName
dataset O
( O
Song O
et O
al O
. O
, O
2014 O
) O
, O
13 O
a O
collection O
of O
naturally O
- O
occurring O
chat O
and O
short O
messages O
( O
SMS O
) O
from O
Egyptian O
native O
speakers O
. O

The O
messages O
( O
sources O
) O
were O
natively O
written O
in O
either O
romanized O
Arabizi O
or O
Egyptian O
Arabic O
orthography O
. O

The O
target O
is O
the O
Egyptian O
transliteration O
of O
these O
message O
. O

14 O
For O
experiments O
, O
we O
use O
the O
same O
split O
proposed O
by O
Shazal O
et O
al O
. O
( O
2020 O
) O
( O
58.9 O
K O
for O
Train O
and O
5.4 O
K O
for O
Dev O
and O
Test O
each O
) O
. O

We O
refer O
to O
this O
dataset O
as O
ARGEN B-DatasetName
TR I-DatasetName
.Baselines O
and O
Procedure O
. O

For O
all O
tasks O
, O
we O
compare O
our O
models O
to O
models O
fine O
- O
tuned O
with O
mT5 B-MethodName
using O
the O
same O
training O
data O
. O

In O
addition O
, O
for O
MT B-TaskName
, O
we O
compare O
to O
a O
vanilla O
sequence O
- O
to O
- O
sequence O
( O
S2S O
) O
Transformer B-MethodName
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
trained O
from O
scratch O
as O
implemented O
in O
Fairseq O
( O
Ott O
et O
al O
. O
, O
2019 O
) O
. O

For O
all O
models O
and O
baselines O
, O
across O
all O
tasks O
, O
we O
identify O
the O
best O
model O
on O
the O
respective O
Dev O
data O
and O
blind O
- O
test O
it O
on O
Test O
data O
. O

As O
a O
rule O
, O
we O
report O
on O
both O
Dev O
and O
Test O
sets O
. O

All O
our O
Dev O
results O
are O
in O
Section O
C.2 O
in O
the O
Appendix O
. O

We O
train O
two O
S2S B-MethodName
Transformers I-MethodName
models O
on O
2 B-HyperparameterValue
M I-HyperparameterValue
( O
S2S B-MethodName
2 I-MethodName
M I-MethodName
) O
and O
10 B-HyperparameterValue
M I-HyperparameterValue
( O
S2S B-MethodName
10 I-MethodName
M I-MethodName
) O
MSA B-HyperparameterName
- I-HyperparameterName
English I-HyperparameterName
parallel I-HyperparameterName
sentences I-HyperparameterName
extracted O
from O
OPUS B-DatasetName
. O

We O
take O
these O
two O
models O
as O
our O
baseline O
I. O
We O
also O
fine O
- O
tune O
our O
three O
models O
as O
well O
as O
mT5 B-MethodName
on O
the O
same O
OPUS B-DatasetName
2 B-HyperparameterValue
M I-HyperparameterValue
MSA B-HyperparameterName
- I-HyperparameterName
English I-HyperparameterName
parallel I-HyperparameterName
sentences I-HyperparameterName
used O
for O
baseline O
I. O

An O
abstractive O
summarization O
dataset O
in O
18 B-HyperparameterValue
languages B-HyperparameterName
, O
including O
Arabic O
( O
Faisal O
Ladhak O
and O
McKeown O
, O
2020 O
) O
. O

It O
contains O
articles O
and O
their O
summaries O
from O
WikiHow O
. O

11 O
The O
Arabic O
part O
includes O
summaries O
for O
29.2 B-HyperparameterValue
K I-HyperparameterValue
articles B-HyperparameterName
, O
which O
we O
split O
into O
80 B-HyperparameterValue
% I-HyperparameterValue
Train B-HyperparameterName
( O
23.4 B-HyperparameterValue
K I-HyperparameterValue
) O
, O
10 B-HyperparameterValue
% I-HyperparameterValue
Dev B-HyperparameterName
( O
2.9 B-HyperparameterValue
K I-HyperparameterValue
) O
, O
and O
10 B-HyperparameterValue
% I-HyperparameterValue
Test B-HyperparameterName
( O
2.9K).The B-HyperparameterValue
purpose O
of O
the O
news B-TaskName
title I-TaskName
generation I-TaskName
( O
NTG B-TaskName
) O
task O
is O
to O
produce O
proper O
news O
article O
titles O
( O
Liang O
et O
al O
. O
, O
2020 O
) O
. O

We O
introduce O
NTG B-TaskName
as O
a O
new O
task O
for O
Arabic B-TaskName
language I-TaskName
generation I-TaskName
. O

Given O
an O
article O
, O
a O
title O
generation O
model O
needs O
to O
output O
a O
short O
grammatical O
sequence O
of O
words O
suited O
to O
the O
article O
content O
. O

For O
this O
, O
we O
introduce O
ARGEN B-DatasetName
NTG I-DatasetName
, O
a O
novel O
NTG O
dataset O
exploiting O
120 B-HyperparameterValue
K I-HyperparameterValue
articles B-HyperparameterName
along O
with O
their O
titles O
extracted O
from O
AraNews O
( O
Nagoudi O
et O
al O
. O
, O
2020 O
) O
. O

12 O
We O
only O
include O
titles O
with O
at O
least O
three O
words O
in O
this O
dataset O
. O

We O
split O
ARGEN B-DatasetName
NTG I-DatasetName
data O
into O
80 B-HyperparameterValue
% I-HyperparameterValue
Train B-HyperparameterName
( O
93.3 B-HyperparameterValue
K I-HyperparameterValue
) O
, O
10 B-HyperparameterValue
% I-HyperparameterValue
Dev B-HyperparameterName
( O
11.7 B-HyperparameterValue
K I-HyperparameterValue
) O
, O
and O
10 B-HyperparameterValue
% I-HyperparameterValue
Test B-HyperparameterName
( O
11.7 B-HyperparameterValue
K I-HyperparameterValue
) O
. O

Details O
about O
ARGEN B-DatasetName
NTG I-DatasetName
are O
in O
Table O
C.1 O
( O
Appendix O
) O
. O

A O
sample O
of O
a O
news O
article O
from O
our O
Test O
split O
and O
example O
titles O
generated O
by O
our O
models O
are O
in O
Table O
D.5 O
( O
Appendix).In O
the O
question B-TaskName
generation I-TaskName
( O
QG B-TaskName
) O
task O
, O
a O
question O
is O
produced O
for O
a O
passage O
( O
Gehrmann O
et O
al O
. O
, O
2021 O
) O
. O

Given O
the O
absence O
of O
an O
Arabic O
QG O
dataset O
, O
we O
create O
a O
new O
Arabic O
QG O
dataset O
( O
ARGEN B-DatasetName
QG I-DatasetName
) O
using O
a O
publicly O
available O
Arabic O
question B-TaskName
answering I-TaskName
( O
QA B-TaskName
) O
resource O
. O

We O
follow O
Kriangchaivech O
and O
Wangperawong O
( O
2019 O
) O
who O
train O
a O
model O
to O
generate O
simple O
questions O
relevant O
to O
passages O
and O
answers O
extracted O
from O
SQuAD B-DatasetName
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
. O

In O
our O
case O
, O
we O
build O
ARGEN B-DatasetName
QG I-DatasetName
by O
extracting O
96 B-HyperparameterValue
K I-HyperparameterValue
( B-HyperparameterName
passage I-HyperparameterName
, I-HyperparameterName
answer I-HyperparameterName
, I-HyperparameterName
and I-HyperparameterName
question I-HyperparameterName
) I-HyperparameterName
triplets I-HyperparameterName
from O
( O
1 O
) O
The O
Arabic O
QA O
dataset O
ARCD B-DatasetName
( O
Mozannar O
et O
al O
. O
, O
2019 O
) O
, O
and O
( O
2 O
) O
three O
multi O
- O
lingual O
QA O
datasets O
: O
XTREME B-DatasetName
benchmark O
( O
Hu O
et O
al O
. O
, O
2020 O
) O
, O
MLQA B-DatasetName
( O
Lewis O
et O
al O
. O
, O
2019 O
) O
, O
XQuAD B-DatasetName
( O
Artetxe O
et O
al O
. O
, O
2020 O
) O
, O
and O
TyDi B-DatasetName
QA I-DatasetName
( O
Artetxe O
et O
al O
. O
, O
2020).The O
main O
goal O
of O
this O
task O
is O
to O
produce O
for O
a O
given O
Arabic O
sentence O
a O
paraphrase O
with O
the O
same O
meaning O
. O

In O
order O
to O
build O
our O
paraphrasing B-TaskName
benchmark O
component O
( O
ARGEN B-DatasetName
PPH I-DatasetName
) O
, O
we O
use O
the O
following O
three O
datasets O
: O
AraPara B-DatasetName
. O

We O
introduce O
AraPara B-DatasetName
, O
a O
new O
multidomain O
Arabic O
paraphrasing O
dataset O
we O
create O
using O
English O
- O
Arabic O
parallel O
OPUS B-DatasetName
data O
( O
Tiedemann O
, O
2012 O
) O
. O

AraPara B-DatasetName
covers O
several O
domains O
such O
as O
news O
, O
religion O
, O
politics O
, O
movies O
, O
and O
technology O
. O

To O
create O
a O
high O
quality O
machine O
generated O
paraphrase O
dataset O
, O
we O
follow O
four O
careful O
steps O
involving O
human O
validation O
( O
more O
details O
are O
offered O
in O
Appendix O
C.1 O
) O
. O

AraPara B-DatasetName
consists O
of O
122 B-HyperparameterValue
K I-HyperparameterValue
paraphrase B-HyperparameterName
pairs I-HyperparameterName
. O

We O
only O
use O
AraPara B-DatasetName
for O
model O
development O
, O
and O
hence O
we O
split O
it O
into O
116 B-HyperparameterValue
K I-HyperparameterValue
Train B-HyperparameterName
and O
6 B-HyperparameterValue
K I-HyperparameterValue
Dev B-HyperparameterName
. O

Arabic B-DatasetName
SemEval I-DatasetName
Paraphrasing I-DatasetName
( O
ASEP B-DatasetName
) O
. O

We O
also O
create O
a O
new O
Arabic O
paraphrasing O
dataset O
using O
three O
existing O
Arabic O
semantic O
similarity O
datasets O
released O
during O
SemEval O
2017 O
( O
Cer O
et O
al O
. O
, O
2017 O
) O
. O

( O
2 O
) O
JOR B-DatasetName
- I-DatasetName
CST I-DatasetName
. O

This O
is O
collected O
from O
Jordanian O
Twitter O
and O
consists O
of O
code O
- O
switched O
Arabic O
- O
English O
posts O
, O
which O
we O
manually O
translate O
into O
monolingual O
English O
. O

Each O
of O
ALG O
- O
CST O
and O
JOR B-DatasetName
- I-DatasetName
CST I-DatasetName
comprises O
300 B-HyperparameterValue
tweets B-HyperparameterName
( O
total=600 B-HyperparameterValue
) O
. O

Human O
translation O
is O
performed O
by O
one O
native O
speaker O
from O
each O
dialect O
with O
seminative O
English O
/ O
French O
fluency O
. O

Synthetic B-DatasetName
Code I-DatasetName
- I-DatasetName
Switched I-DatasetName
Data O
. O

We O
use O
the O
multilingual O
sequence O
- O
to O
- O
sequence O
model O
mBART B-DatasetName
( O
Liu O
et O
al O
. O
, O
2020 O
) O
to O
create O
synthetic O
code O
- O
switched O
data O
following O
Jawahar O
et O
al O
. O
( O
2021 O
) O
. O

We O
exploit O
the O
UN O
multi O
- O
parallel O
data O
( O
Ziemski O
et O
al O
. O
, O
2016 O
) O
using O
the O
Arabic O
- O
English O
and O
Arabic O
- O
French O
test O
splits O
( O
4 O
, O
000 O
sentences O
each O
, O
described O
in O
§ O
3.1 O
) O
to O
generate O
our O
two O
code O
- O
switched O
test O
sets O
( O
3 O
) O
MSA B-DatasetName
- I-DatasetName
EN I-DatasetName
and O
( O
4 O
) O
MSA B-DatasetName
- I-DatasetName
FR I-DatasetName
. O

In O
each O
case O
, O
we O
use O
mBART B-MethodName
to O
translate O
∼ O
30 B-HyperparameterValue
% I-HyperparameterValue
random O
Arabic B-HyperparameterName
n I-HyperparameterName
- I-HyperparameterName
grams I-HyperparameterName
into O
the O
target O
language O
( O
i.e. O
, O
English O
or O
French).To O
WikiLingua B-DatasetName
. O

( O
2 O
) O
IWSLT B-DatasetName
Corpus I-DatasetName
. O

Several O
Arabic O
- O
to O
- O
English O
parallel O
datasets O
were O
released O
during O
IWSLT B-DatasetName
evaluation O
campaigns O
( O
Federico O
et O
al O
. O
, O
2012;Cettolo O
et O
al O
. O
, O
2013Cettolo O
et O
al O
. O
, O
, O
2014Cettolo O
et O
al O
. O
, O
, O
2016 O
. O

The O
datasets O
are O
mainly O
extracted O
from O
transcriptions O
of O
TED O
talks O
between O
2010 O
and O
2016 O
, O
and O
the O
QCRI O
Educational O
Domain O
Corpus O
( O
QED O
2016 O
) O
( O
Abdelali O
et O
al O
. O
, O
2014 O
) O
. O

AraBench B-DatasetName
Datasets O
. O

Sajjad O
et O
al O
. O
( O
2020 O
) O
introduce O
AraBench O
, O
an O
evaluation O
suite O
for O
MSA O
and O
dialectal O
Arabic O
to O
English O
MT O
consisting O
of O
five O
publicly O
available O
datasets O
: O
( O
3 O
) O
ADPT B-DatasetName
: O
Arabic B-DatasetName
- I-DatasetName
Dialect I-DatasetName
/ I-DatasetName
English I-DatasetName
Parallel I-DatasetName
Text I-DatasetName
( O
Zbib O
et O
al O
. O
, O
2012 O
) O
, O
( O
4 O
) O
MADAR B-DatasetName
: O
Multi B-DatasetName
- I-DatasetName
Arabic I-DatasetName
Dialect I-DatasetName
Applications I-DatasetName
and I-DatasetName
Resources I-DatasetName
dataset O
( O
Bouamor O
et O
al O
. O
, O
2018 O
) O
, O
( O
5 O
) O
QAraC B-DatasetName
: O
Qatari B-DatasetName
- I-DatasetName
English I-DatasetName
speech I-DatasetName
corpus I-DatasetName
( O
Elmahdy O
et O
al O
. O
, O
2014 O
) O
, O
and O
( O
6 O
) O
Bible B-DatasetName
: O
The O
English O
Bible O
translated O
into O
MSA O
, O
Moroccan O
, O
and O
Tunisian O
Arabic O
dialects O
. O

9 O
For O
all O
these O
datasets O
, O
we O
use O
the O
same O
splits O
as O
Sajjad O
et O
al O
. O
( O
2020 O
) O
in O
our O
experiments O
. O

To O
investigate O
ability O
of O
our O
models O
to O
generate O
Arabic O
starting O
from O
foreign O
languages O
in O
our O
vocabulary O
, O
we O
create O
an O
X→Arabic O
benchmark O
of O
four B-HyperparameterValue
languages B-HyperparameterName
( O
English O
, O
French O
, O
German O
, O
and O
Russian O
) O
by O
extracting O
parallel O
data O
from O
OPUS B-DatasetName
( O
Tiedemann O
, O
2012 O
) O
. O

For O
each O
language O
, O
we O
pick O
1 B-HyperparameterValue
M I-HyperparameterValue
sentences B-HyperparameterName
for O
training O
and O
5 B-HyperparameterValue
K I-HyperparameterValue
sentences B-HyperparameterName
for O
each O
of O
development O
and O
test O
splits O
. O

This O
gives O
us O
our O
seventh O
ARGEN B-DatasetName
MT I-DatasetName
dataset O
, O
which O
we O
call O
( O
7 O
) O
OPUS B-DatasetName
- I-DatasetName
X I-DatasetName
- I-DatasetName
Ara I-DatasetName
. O

There O
is O
rising O
interest O
in O
translating O
code O
- O
switched O
data O
( O
Nagoudi O
et O
al O
. O
, O
2021 O
) O
. O

Our O
purpose O
here O
is O
to O
translate O
Arabic O
text O
involving O
code O
- O
switching O
from O
a O
foreign O
language O
into O
( O
i O
) O
that O
foreign O
language O
as O
well O
as O
into O
( O
ii O
) O
MSA O
. O

Hence O
we O
create O
ARGEN B-DatasetName
CST I-DatasetName
, O
our O
code O
- O
switched O
translation O
benchmark O
component O
, O
using O
four B-HyperparameterValue
sub B-HyperparameterName
- I-HyperparameterName
test I-HyperparameterName
sets I-HyperparameterName
. O

Two B-HyperparameterValue
of O
these O
are O
natural B-HyperparameterName
and O
two B-HyperparameterValue
are O
synthetic B-HyperparameterName
, O
as O
follows O
: O
Natural B-DatasetName
Code I-DatasetName
- I-DatasetName
Switched I-DatasetName
Data O
. O

We O
create O
two O
human O
written O
( O
natural O
) O
code O
- O
switched O
parallel O
datasets O
: O
( O
1 O
) O
ALG B-DatasetName
- I-DatasetName
CST I-DatasetName
. O

This O
is O
collected O
from O
Algerian O
Twitter O
and O
consists O
of O
code O
- O
switched O
Arabic O
- O
French O
posts O
. O

We O
translate O
these O
manually O
into O
monolingual O
French O
. O

It O
is O
also O
linguistically O
diverse O
as O
it O
covers O
both O
MSA O
and O
various O
Arabic O
dialects O
, O
in O
addition O
to O
Arabizi O
( O
romanized O
Arabic O
in O
the O
TS B-TaskName
task O
) O
and O
codeswitching O
( O
in O
the O
CST B-TaskName
task O
) O
. O

We O
now O
describe O
each O
component O
of O
ARGEN.To B-DatasetName
design O
the O
MT B-TaskName
component O
of O
ARGEN B-DatasetName
, O
ARGEN B-DatasetName
MT I-DatasetName
, O
we O
consolidate O
7 B-HyperparameterValue
unique O
datasets B-HyperparameterName
with O
46 B-HyperparameterValue
different O
test B-HyperparameterName
splits I-HyperparameterName
. O

The O
datasets O
come O
from O
both O
MSA O
and O
Arabic O
dialects O
, O
and O
range O
between O
600 B-HyperparameterValue
- O
138 B-HyperparameterValue
K I-HyperparameterValue
sentences B-HyperparameterName
( O
details O
in O
Table O
C.2 O
in O
Appendix O
) O
. O

We O
introduce O
each O
dataset O
briefly O
here.(1 O
) O
United B-DatasetName
Nations I-DatasetName
Parallel I-DatasetName
Corpus I-DatasetName
. O

Ziemski O
et O
al O
. O
( O
2016 O
) O
introduce O
this O
parallel O
corpus O
of O
man O
- O
ually O
translated O
UN O
documents O
covering O
the O
six O
official O
UN O
languages O
( O
i.e. O
, O
Arabic O
, O
Chinese O
, O
English O
, O
French O
, O
Russian O
, O
and O
Spanish O
) O
. O

The O
corpus O
consists O
of O
development O
and O
test O
sets O
only O
, O
each O
of O
which O
comprise O
4 B-HyperparameterValue
, I-HyperparameterValue
000 I-HyperparameterValue
sentences B-HyperparameterName
that O
are O
one O
- O
toone O
alignments O
across O
all O
official O
languages O
. O

Our O
combined O
MSA O
and O
Twitter O
data O
make O
up O
29B B-HyperparameterValue
tokens B-HyperparameterName
, O
and O
hence O
is O
∼ O
49 B-HyperparameterValue
% I-HyperparameterValue
less O
than O
Arabic B-HyperparameterName
tokens I-HyperparameterName
on O
which O
mT5 B-MethodName
is O
pre O
- O
trained O
( O
57B B-HyperparameterValue
Arabic B-HyperparameterName
tokens I-HyperparameterName
) O
. O

More O
information O
about O
our O
pre O
- O
training O
data O
is O
in O
Table O
1 O
. O

MSA O
Vs O
. O

Dialect O
Distribution O
. O

In O
order O
to O
analyze O
MSA O
- O
dialect O
distribution O
in O
our O
Twitter O
data O
, O
we O
run O
the O
binary O
( O
MSA O
- O
dialect O
) O
classifier O
introduced O
in O
Abdul O
- O
Mageed O
et O
al O
. O
( O
2020b O
) O
on O
a O
random O
sample O
of O
100 B-HyperparameterValue
M I-HyperparameterValue
tweets B-HyperparameterName
. O

We O
find O
the O
data O
to O
involve O
28.39 B-HyperparameterValue
% I-HyperparameterValue
predicted O
dialect B-HyperparameterName
tweets O
and O
71.61 B-HyperparameterValue
% I-HyperparameterValue
predicted O
MSA B-HyperparameterName
. O

We O
also O
acquire O
country O
- O
level O
dialect O
labels O
using O
an O
in O
- O
house O
strong O
classifier O
on O
the O
dialectal O
portion O
of O
the O
data O
( O
i.e. O
, O
∼ O
28.39 B-HyperparameterValue
millions I-HyperparameterValue
tweets B-HyperparameterName
) O
, O
finding O
dialectal O
tweets O
to O
be O
truly O
geographically O
diverse O
as O
shown O
in O
Figure O
2 O
. O

Naturally O
- O
Occurring O
Code O
- O
Switching O
. O

Using O
1 B-HyperparameterValue
M I-HyperparameterValue
random O
tweets B-HyperparameterName
from O
our O
data O
, O
we O
perform O
an O
analysis O
of O
code O
- O
switching O
. O

For O
this O
, O
we O
employ O
simple O
string O
matching O
to O
identify O
Arabic O
and O
run O
the O
CLD3 O
language O
ID O
tool O
4 O
on O
the O
non O
- O
Arabic O
string O
sequences O
. O

We O
find O
the O
data O
to O
have O
4.14 B-HyperparameterValue
% I-HyperparameterValue
non B-HyperparameterName
- I-HyperparameterName
Arabic I-HyperparameterName
. O

These O
turn O
out O
to O
be O
almost O
always O
natural O
code O
- O
switching O
involving O
many O
foreign O
languages O
( O
e.g. O
, O
English O
, O
French O
, O
Korean O
, O
etc O
. O

) O
. O

We O
remove O
diacritics O
and O
replace O
URLs O
and O
user O
mentions O
with O
< O
URL O
> O
and O
< O
USER O
> O
. O

We O
also O
clean O
the O
data O
by O
removing O
HTML O
tags O
, O
elongation O
, O
and O
the O
hash O
signs O
. O

Further O
, O
we O
reduce O
repetitive O
characters O
, O
emojis O
, O
and O
emoticons O
to O
one O
. O

To O
create O
our O
language O
model O
vocabulary O
, O
we O
use O
Sentence B-HyperparameterValue
- I-HyperparameterValue
Piece I-HyperparameterValue
( O
Kudo O
, O
2018 O
) O
to O
encode O
text O
as O
WordPiece O
tokens O
( O
Sennrich O
et O
al O
. O
, O
2016 O
) O
with O
110 B-HyperparameterValue
K I-HyperparameterValue
Word B-HyperparameterName
- I-HyperparameterName
Pieces I-HyperparameterName
. O

To O
allow O
for O
further O
pre O
- O
training O
( O
and/or O
fine O
- O
tuning O
) O
on O
additional O
languages O
, O
we O
extract O
our O
vocabulary O
as O
follows O
: O
70 B-HyperparameterValue
M I-HyperparameterValue
MSA B-HyperparameterName
sentences I-HyperparameterName
, O
200 B-HyperparameterValue
M I-HyperparameterValue
Arabic B-HyperparameterName
twitter I-HyperparameterName
data I-HyperparameterName
, O
15 B-HyperparameterValue
M I-HyperparameterValue
sentences B-HyperparameterName
from I-HyperparameterName
Wikipedia I-HyperparameterName
English I-HyperparameterName
, O
and O
5 B-HyperparameterValue
M I-HyperparameterValue
sentences B-HyperparameterName
from I-HyperparameterName
the I-HyperparameterName
Wikipedia I-HyperparameterName
of O
10 O
other O
languages O
( O
Bulgarian O
, O
French O
, O
German O
, O
Greek O
, O
Italian O
, O
Portuguese O
, O
Russian O
, O
Spanish O
, O
Turkish O
, O
Czech O
) O
. O

5 O
In O
§ O
3.1.2 O
, O
we O
describe O
parallel O
data O
from O
four O
of O
these O
languages O
on O
which O
we O
fine O
- O
tune O
our O
models O
for O
X→Arabic O
MT O
. O

Our O
respective O
results O
( O
reported O
in O
Table O
4.2 O
) O
demonstrate O
the O
utility O
of O
including O
foreign O
vocabulary O
in O
our O
models O
. O

Model O
Architecture O
. O

We O
leverage O
our O
unlabeled O
MSA B-DatasetName
and O
Twitter B-DatasetName
data O
described O
in O
§ O
2.1 O
to O
pretrain O
three O
models O
: O
AraT5 B-MethodName
MSA I-MethodName
on O
MSA B-DatasetName
data O
, O
AraT5 B-MethodName
TW I-MethodName
on O
twitter B-DatasetName
data O
, O
and O
AraT5 B-MethodName
on O
both O
MSA B-DatasetName
and O
twitter B-DatasetName
data O
using O
the O
T5 B-MethodName
Base O
encoderdecoder O
architecture O
( O
Raffel O
et O
al O
. O
, O
2019 O
) O
. O

Each O
of O
the O
encoder O
and O
decoder O
components O
is O
similar O
in O
size O
and O
configuration O
to O
BERT B-MethodName
Base I-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
with O
12 B-HyperparameterValue
layers B-HyperparameterName
each O
with O
12 B-HyperparameterValue
attention B-HyperparameterName
heads I-HyperparameterName
, O
and O
768 B-HyperparameterValue
hidden B-HyperparameterName
units I-HyperparameterName
. O

In O
total O
, O
this O
results O
in O
a O
model O
with O
∼ O
220 O
million O
parameters O
. O

6 O
Objective O
. O

Raffel O
et O
al O
. O
( O
2019 O
) O
pre O
- O
train O
T5 B-MethodName
Base I-MethodName
using O
a O
self O
- O
supervised O
( O
denoising O
) O
objective O
. O

The O
main O
idea O
is O
to O
feed O
the O
model O
with O
masked O
( O
corrupted O
) O
versions O
of O
the O
original O
sentence O
, O
and O
train O
it O
to O
reconstruct O
the O
original O
sequence O
. O

Inspired O
by O
BERT B-MethodName
's O
objective O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
the O
denoising O
objective O
( O
Raffel O
et O
al O
. O
, O
2019 O
) O
works O
by O
randomly O
sampling O
and O
dropping O
out O
15 B-HyperparameterValue
% I-HyperparameterValue
of O
tokens O
in O
the O
input O
sequence O
. O

All O
consecutive O
spans O
of O
dropped O
- O
out O
tokens O
are O
then O
replaced O
by O
a O
single O
sentinel O
token O
. O

Pre O
- O
Training O
. O

For O
all O
three O
of O
our O
pre O
- O
trained O
models O
, O
we O
use O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
0.01 B-HyperparameterValue
, O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
128 B-HyperparameterValue
sequences O
, O
and O
a O
maximum B-HyperparameterName
sequence I-HyperparameterName
length I-HyperparameterName
of O
512 B-HyperparameterValue
, O
except O
for O
AraT5 B-MethodName
TW I-MethodName
where O
the O
maximum B-HyperparameterName
sequence I-HyperparameterName
is O
128 B-HyperparameterValue
. O

7 O
We O
pre O
- O
train O
each O
model O
for O
1 B-HyperparameterValue
M I-HyperparameterValue
steps B-HyperparameterName
. O

Pre O
- O
training O
of O
each O
model O
took O
∼ O
80 O
days O
on O
one O
Google O
Cloud O
TPU B-HyperparameterName
with O
8 O
cores O
( O
v3.8 B-HyperparameterValue
) O
from O
TensorFlow O
Research O
Cloud O
( O
TFRC O
) O
. O

8 O
We O
now O
introduce O
our O
language B-TaskName
generation I-TaskName
and O
understating B-TaskName
benchmarks O
. O

In O
order O
to O
evaluate O
our O
pre O
- O
trained O
language O
models O
, O
we O
introduce O
our O
new O
benchmark O
for O
Arabic B-TaskName
language I-TaskName
generation I-TaskName
evaluation O
ARGEN B-DatasetName
. O

It O
includes O
19 B-HyperparameterValue
different O
datasets B-HyperparameterName
with O
59 B-HyperparameterValue
test B-HyperparameterName
splits I-HyperparameterName
and O
covers O
seven B-HyperparameterValue
tasks B-HyperparameterName
: O
machine B-TaskName
translation I-TaskName
( O
MT B-TaskName
) O
, O
codeswitched B-TaskName
translation I-TaskName
( O
CST B-TaskName
) O
, O
text B-TaskName
summarization I-TaskName
( O
TS B-TaskName
) O
, O
news B-TaskName
title I-TaskName
generation I-TaskName
( O
NGT B-TaskName
) O
, O
question B-TaskName
generation I-TaskName
( O
QG B-TaskName
) O
, O
transliteration B-TaskName
( O
TR B-TaskName
) O
, O
and O
paraphrasing B-TaskName
( O
PPH B-TaskName
) O
. O

As O
such O
, O
ARGEN B-DatasetName
has O
wide O
- O
coverage O
both O
in O
terms O
of O
the O
number B-HyperparameterName
of I-HyperparameterName
tasks I-HyperparameterName
and O
datasets B-HyperparameterName
. O

AraNews B-DatasetName
( O
Nagoudi O
et O
al O
. O
, O
2020 O
) O
, O
El B-DatasetName
- I-DatasetName
Khair I-DatasetName
El I-DatasetName
- I-DatasetName
Khair I-DatasetName
( O
2016 O
) O
, O
Gigaword O
, O
2 O
, O
OSCAR O
( O
Suárez O
et O
al O
. O
, O
2019 O
) O
, O
OSIAN B-DatasetName
( O
Zeroual O
et O
al O
. O
, O
2019 O
) O
, O
Wikipedia B-DatasetName
Arabic I-DatasetName
, O
and O
Hindawi B-DatasetName
Books I-DatasetName
. O

3 O
Twitter B-DatasetName
Data I-DatasetName
. O

We O
randomly O
sample O
1.5B B-HyperparameterValue
Arabic O
tweets B-HyperparameterName
( O
178 B-HyperparameterValue
GB I-HyperparameterValue
) O
from O
a O
large O
in O
- O
house O
dataset O
of O
∼ O
10B B-HyperparameterValue
tweets B-HyperparameterName
. O

We O
use O
string O
matching O
to O
only O
include O
tweets O
with O
at O
least O
3 B-HyperparameterValue
Arabic B-HyperparameterName
words I-HyperparameterName
, O
regardless O
whether O
the O
tweet O
has O
non O
- O
Arabic O
string O
or O
not O
. O

( O
1 O
) O
We O
introduce O
three O
powerful O
variants O
of O
the O
text O
- O
to O
- O
text O
transformer O
( O
T5 B-MethodName
) O
model O
dedicated O
to O
Modern O
Standard O
Arabic O
( O
MSA O
) O
and O
a O
diverse O
set O
of O
Arabic O
dialects O
. O

We O
include O
in O
our O
vocabulary O
11 B-HyperparameterValue
languages B-HyperparameterName
other I-HyperparameterName
than I-HyperparameterName
Arabic I-HyperparameterName
( O
e.g. O
, O
English O
, O
French O
, O
German O
, O
Russian O
) O
, O
which O
also O
allows O
us O
to O
evaluate O
our O
models O
under O
zero O
- O
shot O
pre O
- O
training O
conditions O
involving O
these O
languages O
. O

The O
rest O
of O
the O
paper O
is O
organized O
as O
follows O
: O
Section O
2 O
describes O
our O
Arabic O
pre O
- O
tained O
models O
. O

In O
Section O
3 O
, O
we O
introduce O
ARGEN B-DatasetName
, O
our O
new O
natural B-TaskName
language I-TaskName
generation I-TaskName
benchmark O
. O

We O
evaluate O
our O
models O
on O
ARGEN B-DatasetName
in O
Section O
4 O
. O

Section O
5 O
is O
an O
analysis O
and O
discussion O
of O
our O
results O
. O

In O
Section O
6 O
, O
we O
provide O
an O
overview O
of O
related O
work O
. O

We O
conclude O
in O
Section O
7 O
. O

We O
now O
introduce O
our O
new O
pre O
- O
trained O
models O
. O

We O
use O
70 B-HyperparameterValue
GB I-HyperparameterValue
of O
MSA B-DatasetName
text I-DatasetName
( O
7.1B B-HyperparameterValue
tokens B-HyperparameterName
) O
from O
the O
following O
sources O
: O

( O
2 O
) O
We O
propose O
a O
novel O
unified O
benchmark O
for O
ARabic B-TaskName
natural I-TaskName
language I-TaskName
GEeneration I-TaskName
( O
ARGEN B-DatasetName
) O
composed O
of O
seven O
tasks O
: O
machine B-TaskName
translation I-TaskName
, O
code B-TaskName
- I-TaskName
switched I-TaskName
text I-TaskName
translation I-TaskName
, O
summarization B-TaskName
, O
news B-TaskName
title I-TaskName
generation I-TaskName
, O
question B-TaskName
generation I-TaskName
, O
paraphrasing B-TaskName
, O
and O
transliteration B-TaskName
. O

ARGEN B-DatasetName
is O
collected O
from O
a O
total O
of O
19 O
datasets O
, O
including O
9 O
new O
datasets O
proposed O
in O
this O
work O
. O

( O
3 O
) O
To O
show O
the O
utility O
of O
our O
new O
models O
, O
we O
evaluate O
them O
on O
ARGEN B-DatasetName
under O
both O
full O
and O
zero O
- O
shot O
pre O
- O
training O
conditions O
. O

Our O
models O
set O
new O
SOTA O
on O
the O
majority O
of O
datasets O
in O
all O
seven O
tasks O
. O

( O
4 O
) O
Although O
the O
main O
focus O
of O
our O
work O
is O
language B-TaskName
generation I-TaskName
, O
we O
also O
show O
the O
effectiveness O
of O
our O
models O
on O
Arabic B-TaskName
language I-TaskName
understanding I-TaskName
by O
fine O
- O
tuning O
our O
new O
models O
on O
a O
large O
, O
recently O
proposed O
Arabic O
language O
understanding O
benchmark O
. O

Again O
, O
our O
models O
establish O
new O
SOTA O
on O
the O
majority O
of O
language B-TaskName
understanding I-TaskName
tasks O
. O

We O
also O
investigate O
that O
memory O
imitation O
improves O
the O
learning O
of O
model O
initialization O
via O
another O
criterion O
I(θ O
; O
[ O
D O
q O
, O
M]|D O
q O
) O
> O
0 O
following O
Yao O
et O
al O
. O
( O
2021 O
) O
. O

This O
criterion O
guarantees O
that O
the O
additional O
memory O
knowledge O
contributes O
to O
updating O
the O
initialization O
in O
the O
outer O
loop O
. O

SinceTransfer O
learning O
with O
a O
unified O
Transformer O
framework O
( O
T5 B-MethodName
) O
that O
converts O
all O
language O
problems O
into O
a O
text O
- O
to O
- O
text O
format O
was O
recently O
proposed O
as O
a O
simple O
and O
effective O
transfer O
learning O
approach O
. O

Although O
a O
multilingual O
version O
of O
the O
T5 B-MethodName
model O
( O
mT5 B-MethodName
) O
was O
also O
introduced O
, O
it O
is O
not O
clear O
how O
well O
it O
can O
fare O
on O
non O
- O
English O
tasks O
involving O
diverse O
data O
. O

To O
investigate O
this O
question O
, O
we O
apply O
mT5 B-MethodName
on O
a O
language O
with O
a O
wide O
variety O
of O
dialects O
- O
Arabic O
. O

For O
evaluation O
, O
we O
introduce O
a O
novel O
benchmark O
for O
ARabic B-TaskName
language I-TaskName
GENeration I-TaskName
( O
ARGEN B-DatasetName
) O
, O
covering O
seven O
important O
tasks O
. O

For O
model O
comparison O
, O
we O
pre O
- O
train O
three O
powerful O
Arabic O
T5 O
- O
style O
models O
and O
evaluate O
them O
on O
ARGEN B-DatasetName
. O

Although O
pre O
- O
trained O
with O
∼ O
49 O
% O
less O
data O
, O
our O
new O
models O
perform O
significantly O
better O
than O
mT5 B-MethodName
on O
all O
ARGEN B-DatasetName
tasks O
( O
in O
52 O
out O
of O
59 O
test O
sets O
) O
and O
set O
several O
new O
SOTAs O
. O

Our O
models O
also O
establish O
new O
SOTA O
on O
the O
recently O
- O
proposed O
, O
large O
Arabic B-DatasetName
language I-DatasetName
understanding I-DatasetName
evaluation I-DatasetName
benchmark O
ARLUE B-DatasetName
( O
Abdul O
- O
Mageed O
et O
al O
. O
, O
2021 O
) O
. O

Our O
models O
are O
publicly O
available O
. O

We O
also O
link O
to O
individual O
ARGEN B-TaskName
datasets O
through O
our O
public O
repository O
. O

1 O

Our O
main O
contributions O
are O
as O
follows O
: O

In O
this O
work O
, O
we O
offer O
the O
first O
comparison O
of O
the O
mT5 B-MethodName
model O
to O
similar O
encoder O
- O
decoder O
models O
dedicated O
to O
Arabic O
. O

We O
choose O
Arabic O
as O
our O
context O
due O
to O
its O
large O
set O
of O
diverse O
varieties O
as O
well O
as O
its O
wide O
use O
on O
social O
media O
. O

Our O
work O
aims O
at O
uncovering O
the O
extent O
to O
which O
mT5 B-MethodName
can O
serve O
Arabic O
's O
different O
varieties O
. O

Our O
work O
also O
meets O
an O
existing O
need O
for O
pre O
- O
trained O
Transformer O
- O
based O
sequenceto O
- O
sequence O
models O
. O

In O
other O
words O
, O
while O
several O
BERT O
- O
based O
models O
have O
been O
pre O
- O
trained O
for O
Arabic O
( O
Antoun O
et O
al O
. O
, O
2020;Abdul O
- O
Mageed O
et O
al O
. O
, O
2021;Inoue O
et O
al O
. O
, O
2021 O
) O
, O
no O
such O
attempts O
have O
been O
made O
to O
create O
sequence O
- O
to O
- O
sequence O
models O
that O
we O
know O
of O
. O

Another O
motivation O
for O
our O
work O
is O
absence O
of O
an O
evaluation O
benchmark O
for O
Arabic B-TaskName
language I-TaskName
generation I-TaskName
tasks O
. O

Apart O
from O
machine B-TaskName
translation I-TaskName
where O
researchers O
are O
starting O
to O
propose O
benchmarks O
such O
as O
AraBench B-DatasetName
( O
Sajjad O
et O
al O
. O
, O
2020 O
) O
, O
there O
are O
no O
benchmarks O
that O
can O
be O
used O
to O
methodically O
measure O
Arabic B-TaskName
natural I-TaskName
language I-TaskName
generation I-TaskName
performance O
. O

Due O
to O
their O
remarkable O
ability O
to O
transfer O
knowledge O
from O
unlabeled O
data O
to O
downstream O
tasks O
, O
pre O
- O
trained O
Transformer O
- O
based O
language O
models O
have O
emerged O
as O
important O
components O
of O
modern O
natural O
language O
processing O
( O
NLP O
) O
systems O
. O

In O
particular O
, O
the O
unified O
framework O
that O
converts O
all O
text O
- O
based O
language O
problems O
into O
a O
text O
- O
to O
- O
text O
format O
presented O
through O
the O
T5 B-MethodName
model O
( O
Raffel O
et O
al O
. O
, O
2019 O
) O
is O
attractive O
. O

In O
addition O
to O
its O
simplicity O
, O
this O
approach O
is O
effective O
since O
it O
allows O
knowledge O
transfer O
from O
high O
- O
resource O
to O
low O
- O
resource O
tasks O
without O
the O
need O
for O
changing O
model O
architecture O
. O

Unlike O
models O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
which O
are O
based O
on O
encoders O
only O
, O
the O
T5 B-MethodName
model O
is O
an O
encoder O
- O
decoder O
that O
can O
naturally O
be O
employed O
for O
natural B-TaskName
language I-TaskName
generation I-TaskName
. O

Although O
the O
T5 B-MethodName
model O
, O
originally O
pre O
- O
trained O
for O
English O
, O
was O
recently O
extended O
to O
the O
multilingual O
setting O
as O
mT5 B-MethodName
( O
Xue O
et O
al O
. O
, O
2020 O
) O
, O
it O
is O
not O
clear O
how O
suited O
it O
is O
to O
individual O
languages O
( O
and O
varieties O
of O
these O
languages O
) O
. O

In O
addition O
, O
systematic O
issues O
have O
been O
discovered O
in O
multilingual O
corpora O
on O
which O
language O
models O
have O
been O
trained O
( O
Kreutzer O
et O
al O
. O
, O
2021 O
) O
. O

In O
absence O
of O
comparisons O
with O
monolingual O
pre O
- O
trained O
language O
models O
that O
serve O
different O
non O
- O
English O
contexts O
, O
it O
remains O
unknown O
how O
multilingual O
models O
really O
fare O
against O
languagespecific O
models O
. O

where O
the O
last O
inequality O
holds O
due O
toŶ O
q O
is O
dependent O
on O
M. O

Then O
the O
equation O
( O
7 O
) O
will O
become O
to O

since O
p(Ŷ O
q O
, O
Z O
) O
does O
not O
rely O
on O
the O
variable O
M. O
Hence O
, O
we O
can O
just O
write O
EŶ O
q O
, O
Z O
, O
M O
as O
E O
for O
short O
. O

Note O
that O
trivially O
, O
we O
have O

For O
short O
, O
we O
use O
notation O
Z O
= O
( O
X O
q O
, O
X O
s O
, O
Y O
s O
, O
θ O
) O
to O
denote O
a O
set O
of O
variables O
. O

Then O
we O
can O
rewrite O
( O
7 O
) O
as O

Human B-MetricName
Evaluation I-MetricName
We O
conduct O
human O
evaluation O
following O
Song O
et O
al O
. O
( O
2020 O
) O
considering O
two O
aspects O
Quality O
and O
Consistency O
where O
five O
welleducated O
volunteers O
annotate O
250 B-HyperparameterValue
generated B-HyperparameterName
responses I-HyperparameterName
for O
each O
model O
. O

The O
annotators O
score O
each O
response O
from O
two O
aspects O
: O
Quality B-MetricName
and O
Consistency B-MetricName
in O
a O
3 O
- O
point O
scale O
: O
2 B-MetricValue
for O
good O
, O
1 B-MetricValue
for O
fair O
, O
and O
0 B-MetricValue
for O
bad O
. O

Quality B-MetricName
measures O
coherence O
, O
fluency O
, O
and O
informativeness O
. O

Consistency B-MetricName
measures O
the O
task O
consistency O
between O
the O
generated O
responses O
and O
the O
person O
's O
persona O
description O
. O

Experimental O
Setup O
. O

We O
utilize O
a O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
as O
the O
encoder O
. O

We O
fine O
- O
tune O
the O
off O
- O
the O
- O
shelf O
pre O
- O
trained O
BERT B-MethodName
on O
the O
masked O
language O
modeling O
task O
following O
( O
Dopierre O
et O
al O
. O
, O
2021 O
) O
as O
it O
greatly O
improves O
embeddings O
' O
quality O
. O

The O
fine O
- O
tuned O
BERT B-MethodName
is O
then O
used O
as O
the O
initialization O
for O
all O
few O
- O
shot O
models O
. O

We O
use O
Adam B-HyperparameterValue
( O
Kingma O
and O
Ba O
, O
2015 O
) O
optimizer B-HyperparameterName
for O
both O
inner O
and O
outer O
loop O
update O
with O
learning B-HyperparameterName
rate I-HyperparameterName
2e B-HyperparameterValue
−5 I-HyperparameterValue
and O
1e B-HyperparameterValue
−5 I-HyperparameterValue
respectively O
, O
and O
we O
set O
β B-HyperparameterName
= O
0.2 B-HyperparameterValue
in O
Eqn O
. O

5 O
, O
the O
number B-HyperparameterName
of I-HyperparameterName
neighbors I-HyperparameterName
N B-HyperparameterName
= O
20 B-HyperparameterValue
and O
the O
number B-HyperparameterName
of I-HyperparameterName
local I-HyperparameterName
adaptation I-HyperparameterName
steps I-HyperparameterName
L B-HyperparameterName
= O
5.To B-HyperparameterValue
measure O
whether O
MemIML B-MethodName
improves O
the O
learned O
model O
initialization O
, O
we O
add O
an O
experiment O
that O
does O
not O
incorporate O
the O
memory O
module O
during O
meta O
- O
testing O
( O
i.e. O
, O
β B-HyperparameterName
= O
1 B-HyperparameterValue
in O
Eq O
. O

5 O
) O
for O
the O
multi O
- O
domain O
sentiment B-TaskName
classification I-TaskName
task O
. O

Research O
on O
this O
paper O
was O
supported O
by O
Hong O
Kong O
Research O
Grants O
Council O
( O
Grant O
No O
. O

16204920 O
) O
and O
National O
Natural O
Science O
Foundation O
of O
China O
( O
Grant O
No O
. O

62106275).Proof O
of O
inequality O
in O
Eqn O
. O

6 O
. O

We O
check O
the O
validity O
of O
memory O
imitation O
by O
examining O
whether O
the O
criterion O
in O
Section O
4.4 O
is O
met O
. O

We O
check O
the O
increase O
of O
mutual B-MetricName
information I-MetricName
between O
predictions O
of O
query O
sets O
with O
the O
provided O
support O
- O
set O
information O
after O
augmented O
with O
the O
memory O
information O
M. O

Proof O
. O
Experimental O
Setup O
. O

We O
implement O
our O
model O
based O
on O
the O
transformer O
( O
Dehghani O
et O
al O
. O
, O
2018;Vaswani O
et O
al O
. O
, O
2017 O
) O
with O
pre O
- O
trained O
Glove B-HyperparameterValue
embedding B-HyperparameterName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
following O
( O
Madotto O
et O
al O
. O
, O
2019 O
) O
. O

The O
hidden B-HyperparameterName
dimensions I-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
LSTM I-HyperparameterName
unit I-HyperparameterName
are O
set O
to O
1024 B-HyperparameterValue
. O

We O
set O
the O
number B-HyperparameterName
of I-HyperparameterName
neighbors I-HyperparameterName
N B-HyperparameterName
= O
10 B-HyperparameterValue
and O
the O
number B-HyperparameterName
of I-HyperparameterName
local I-HyperparameterName
adaptation I-HyperparameterName
steps I-HyperparameterName
L B-HyperparameterName
= O
20 B-HyperparameterValue
. O

We O
follow O
all O
other O
hyperparameter O
settings O
in O
Madotto O
et O
al O
. O
( O
2019 O
): O
we O
use O
SGD B-HyperparameterValue
for O
the O
inner O
loop O
training O
and O
Adam B-HyperparameterValue
for O
the O
outer O
loop O
update O
with O
learning B-HyperparameterName
rates I-HyperparameterName
0.01 B-HyperparameterValue
and O
0.0003 B-HyperparameterValue
, O
respectively O
. O

We O
set O
batch B-HyperparameterName
size I-HyperparameterName
as O
16 B-HyperparameterValue
and O
use O
beam O
search O
with O
beam B-HyperparameterName
size I-HyperparameterName
5 B-HyperparameterValue
. O

all O
the O
meta O
- O
training O
tasks O
satisfy O
this O
criterion O
, O
the O
generalization O
ability O
of O
the O
model O
initialization O
improves O
. O

Removing O
the O
value O
predictor O
means O
directly O
using O
the O
memory O
output O
without O
a O
learnable O
network O
. O

Its O
results O
are O
not O
too O
bad O
, O
indicating O
that O
the O
memory O
module O
helps O
to O
mitigate O
the O
memorization O
overfitting O
problem O
. O

However O
, O
this O
usage O
simply O
aggre O
- O
gates O
the O
support O
set O
information O
into O
the O
query O
set O
, O
which O
is O
not O
as O
precise O
as O
learning O
the O
information O
required O
by O
the O
query O
set O
itself O
. O

Therefore O
, O
it O
is O
still O
inferior O
to O
our O
model O
. O

Removing O
Local O
adaptation O
means O
we O
only O
use O
the O
global O
value O
predictor O
to O
estimate O
the O
memory O
output O
. O

It O
is O
crucial O
to O
the O
value O
predictor O
since O
removing O
it O
from O
the O
value O
predictor O
results O
in O
an O
even O
worse O
performance O
than O
removing O
the O
value O
predictor O
. O

Besides O
, O
the O
significant O
drop O
in O
task O
consistency O
( O
C B-MetricName
- I-MetricName
score I-MetricName
) O
shows O
that O
local O
adaptation O
contributes O
a O
lot O
to O
making O
the O
model O
adaptive O
to O
specific O
tasks O
, O
as O
it O
learns O
to O
adapt O
to O
each O
query O
- O
set O
sample O
. O

Memory B-HyperparameterName
Size I-HyperparameterName
. O

In O
Table O
4 O
and O
5 O
, O
we O
investigate O
the O
variants O
of O
our O
task O
- O
specific O
memory O
module O
of O
different O
sizes O
. O

We O
control O
the O
memory O
size O
through O
|M O
| O
= O
store O
ratio O
× O
|D O
s O
| O
. O

The O
results O
demonstrate O
that O
our O
model O
is O
able O
to O
maintain O
high O
performance O
even O
with O
only O
a O
20 B-HyperparameterValue
% I-HyperparameterValue
memory B-HyperparameterName
size I-HyperparameterName
by O
storing O
diverse O
and O
representative O
samples O
of O
support O
sets O
. O

Besides O
, O
as O
the O
ratio O
of O
stored O
samples O
increases O
, O
the O
model O
's O
performance O
is O
improved O
since O
it O
provides O
more O
information O
for O
the O
inference O
of O
query O
samples O
and O
the O
optimization O
of O
the O
model O
initialization O
. O

Storing O
all O
the O
encountered O
samples O
( O
i.e. O
, O
with O
store O
ratio O
100 O
% O
) O
in O
the O
memory O
instead O
introduces O
some O
noise O
that O
damages O
the O
model O
performance O
. O

Number B-HyperparameterName
of I-HyperparameterName
Neighbors I-HyperparameterName
. O

We O
also O
investigate O
the O
effects O
of O
different O
numbers B-HyperparameterName
of I-HyperparameterName
neighbors I-HyperparameterName
for O
the O
model O
performance O
in O
Table O
4 O
and O
Table O
5 O
. O

In O
both O
datasets O
, O
the O
model O
performs O
better O
with O
a O
larger O
number O
of O
neighbors O
. O

However O
, O
when O
the O
number O
of O
neighbors B-HyperparameterName
is I-HyperparameterName
too I-HyperparameterName
large I-HyperparameterName
, O
the O
model O
retrieves O
some O
dissimilar O
slots O
from O
the O
memory O
module O
. O

These O
dissimilar O
slots O
bring O
much O
noise O
, O
which O
makes O
the O
predictions O
of O
query O
samples O
inaccurate O
. O

We O
present O
two O
generated O
cases O
in O
personalized O
dialog O
in O
In O
this O
paper O
, O
we O
tackle O
the O
memorization O
overfitting O
problem O
of O
meta O
- O
learning O
for O
text B-TaskName
classification I-TaskName
and O
generation B-TaskName
applications O
. O

We O
propose O
MemIML B-MethodName
to O
enhance O
the O
dependence O
of O
the O
model O
on O
the O
support O
sets O
for O
task O
adaptation O
. O

MemIML B-MethodName
introduces O
a O
memory O
module O
storing O
the O
information O
of O
support O
sets O
, O
and O
propose O
an O
imitation O
module O
to O
better O
leverage O
the O
support O
set O
information O
by O
imitating O
the O
behaviors O
of O
the O
memory O
. O

Both O
empirical O
and O
theoretical O
results O
demonstrate O
that O
our O
method O
MemIML B-MethodName
effectively O
alleviates O
the O
memorization O
overfitting O
problem O
. O

The O
dialogues O
may O
lead O
to O
the O
leakage O
of O
personal O
privacy O
information O
. O

In O
this O
work O
, O
the O
data O
source O
we O
use O
is O
from O
a O
published O
dataset O
and O
does O
not O
involve O
privacy O
issues O
for O
the O
data O
collection O
. O

Our O
proposed O
method O
does O
not O
include O
inference O
or O
judgments O
about O
individuals O
and O
does O
not O
generate O
any O
discriminatory O
, O
insulting O
responses O
. O

Our O
work O
validates O
the O
proposed O
method O
and O
baseline O
models O
on O
human B-MetricName
evaluation I-MetricName
which O
involves O
manual O
labor O
. O

We O
hire O
five O
annotators O
to O
score O
750 B-HyperparameterValue
generated B-HyperparameterName
sentences I-HyperparameterName
in O
total O
( O
250 B-HyperparameterValue
sentences B-HyperparameterName
for O
each O
model O
we O
evaluate O
) O
. O

The O
hourly O
pay O
is O
set O
to O
15 O
US$ O
per O
person O
, O
which O
is O
higher O
than O
the O
local O
statutory O
minimum O
wage O
. O

Overall O
Performance O
. O
Table O
2 O
shows O
the O
performance O
measured O
by O
the O
mean O
accuracy O
of O
meta O
- O
testing O
tasks O
. O

Our O
model O
, O
MemIML B-MethodName
outperforms O
all O
competing O
approaches O
including O
nonmeta O
- O
learning O
, O
metric O
- O
based O
meta O
- O
learning O
, O
and O
optimization O
- O
based O
meta O
- O
learning O
methods O
. O

Particularly O
, O
our O
model O
surpasses O
the O
current O
solutions O
to O
the O
memorization O
overfitting O
problem O
( O
MR B-MethodName
- I-MethodName
MAML I-MethodName
, O
Meta B-MethodName
- I-MethodName
Aug I-MethodName
, O
MetaMix B-MethodName
) O
, O
indicating O
that O
our O
method O
is O
more O
effective O
compared O
to O
regularization O
and O
textual O
augmentation O
. O

In O
Figure O
2 O
, O
the O
gaps O
of O
the O
losses O
on O
query O
sets O
between O
pre O
- O
update O
θ O
( O
before O
training O
on O
support O
sets O
) O
In O
Figure O
2 O
( O
c O
) O
, O
MemIML B-MethodName
has O
large O
gaps O
between O
θ O
and O
θ O
i O
, O
implying O
that O
θ O
i O
better O
leverages O
support O
sets O
when O
adapting O
to O
new O
tasks O
and O
thus O
alleviates O
the O
memorization O
overfitting O
issue O
. O

In O
Table O
3 O
, O
we O
conduct O
ablation O
studies O
to O
verify O
the O
effectiveness O
of O
each O
component O
. O

Removing O
Similarity O
- O
Search O
means O
the O
memory O
reading O
operation O
randomly O
outputs O
memory O
slots O
instead O
of O
searching O
for O
similar O
memory O
slots O
. O

This O
variant O
underperforms O
MemIML B-MethodName
, O
indicating O
that O
similar O
samples O
stored O
in O
the O
memory O
provide O
more O
useful O
information O
to O
improve O
the O
model O
performance O
. O

Baselines O
. O
We O
compare O
our O
methods O
with O
the O
following O
baselines O
: O
Fine O
- O
tune O
: O
We O
fine O
- O
tune O
a O
pre O
- O
trained O
BERT B-MethodName
on O
the O
support O
set O
of O
metatesting O
tasks O
( O
non O
- O
meta O
- O
learning O
method O
) O
as O
in O
Appendix O
. O

B.2 O
. O

We O
choose O
five O
metric O
- O
based O
meta O
- O
learning O
baselines O
: O
Matching B-MethodName
Net I-MethodName
( O
Vinyals O
et O
al O
. O
, O
2016 O
) O
, O
Prototypical B-MethodName
Net I-MethodName
( O
Snell O
et O
al O
. O
, O
2017 O
) O
, O
Proto O
+ O
+ O
, O
( O
Ren O
et O
al O
. O
, O
2018 O
) O
, O
Relation B-MethodName
Net I-MethodName
( O
Sung O
et O
al O
. O
, O
2018 O
) O
, O
and O
Induction B-MethodName
Net I-MethodName
( O
Geng O
et O
al O
. O
, O
2019 O
) O
. O

We O
apply O
an O
optimization O
- O
based O
baseline O
( O
MAML B-MethodName
) O
( O
Finn O
et O
al O
. O
, O
2017 O
) O
to O
the O
base O
model O
, O
and O
implement O
some O
approaches O
tackling O
the O
memorization O
overfitting O
problem O
based O
on O
MAML B-MethodName
: O
MR B-MethodName
- I-MethodName
MAML I-MethodName
( O
Yin O
et O
al O
. O
, O
2020 O
) O
, O
MetaMix B-MethodName
, O
( O
Yao O
et O
al O
. O
, O
2021 O
) O
and O
Meta B-MethodName
- I-MethodName
Aug I-MethodName
( O
Rajendran O
et O
al O
. O
, O
2020 O
) O
. O

We O
conduct O
experiments O
on O
the O
ARSC B-DatasetName
( O
Yu O
et O
al O
. O
, O
2018 O
) O
. O

It O
contains O
English O
reviews O
of O
23 B-HyperparameterValue
types B-HyperparameterName
of I-HyperparameterName
Amazon I-HyperparameterName
products I-HyperparameterName
, O
where O
each O
product O
consists O
of O
three B-HyperparameterValue
different B-HyperparameterName
binary I-HyperparameterName
classification I-HyperparameterName
tasks I-HyperparameterName
. O

Following O
Geng O
et O
al O
. O
( O
2019 O
) O
, O
we O
select O
12 B-HyperparameterValue
tasks B-HyperparameterName
from O
4 B-HyperparameterValue
domains B-HyperparameterName
( O
Books O
, O
DVD O
, O
Electronics O
, O
Kitchen O
) O
for O
meta O
- O
testing O
tasks O
, O
and O
the O
support O
sets O
of O
these O
tasks O
are O
fixed O
( O
Yu O
et O
al O
. O
, O
2018 O
) O
. O

• O
Quality O
: O
BLEU B-MetricName
- I-MetricName
n I-MetricName
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
, O
CIDEr B-MetricName
( O
Vedantam O
et O
al O
. O
, O
2015 O
, O
and O
ROUGE B-MetricName
( O
Lin O
, O
2004 O
) O
measures O
the O
n O
- O
gram O
matching O
between O
the O
generated O
response O
and O
ground O
truth O
. O

PPL B-MetricName
( O
perplexity B-MetricName
) O
measures O
the O
sentence O
fluency O
. O

• O
Diversity O
. O

Dist B-MetricName
- I-MetricName
n I-MetricName
( O
Li O
et O
al O
. O
, O
2016 O
) O
evaluates O
the O
response O
diversity O
by O
counting O
unique O
n O
- O
grams O
. O

• O
Consistency O
: O
C B-MetricName
score I-MetricName
( O
Madotto O
et O
al O
. O
, O
2019 O
) O
measures O
the O
consistency O
between O
the O
generated O
responses O
and O
persona O
descriptions O
through O
a O
pretrained O
natural O
language O
inference O
model O
. O

Overall O
Performance O
. O

As O
shown O
in O
Table O
1.Fine O
- O
tune O
outperforms O
Base O
Model O
in O
all O
metrics O
, O
which O
verifies O
that O
the O
task O
- O
specific O
data O
is O
helpful O
to O
its O
performance O
on O
specific O
tasks O
. O

Compared O
to O
Fine O
- O
tune O
, O
MAML B-MethodName
behaves O
better O
on O
diversity O
and O
consistency O
but O
behaves O
worse O
on O
quality O
. O

Pretraining O
the O
base O
model O
achieves O
the O
best O
perplexity B-MetricName
( O
lowest O
PPL B-MetricName
) O
as O
shown O
by O
Base O
Model O
and O
Fine O
- O
tune O
. O

We O
analyze O
that O
it O
's O
because O
pretraining O
leads O
to O
a O
considerable O
degree O
of O
fluency O
in O
their O
generated O
utterances O
and O
is O
careless O
about O
each O
task O
's O
specific O
information O
, O
resulting O
in O
low O
consistency O
with O
tasks O
. O

Our O
model O
, O
MemIML B-MethodName
, O
performs O
the O
best O
in O
most O
aspects O
, O
including O
quality O
, O
diversity O
, O
and O
task O
consistency O
. O

In O
particular O
, O
MemIML B-MethodName
significantly O
improves O
MR B-MethodName
- I-MethodName
MAML I-MethodName
in O
alleviating O
the O
memorization O
overfitting O
issue O
, O
suggesting O
that O
memory O
imitation O
is O
more O
effective O
than O
only O
regularizing O
model O
initialization O
. O

Dataset O
. O

Amazon B-DatasetName
Review I-DatasetName
sentiment I-DatasetName
classification I-DatasetName
dataset O
( O
ARSC B-DatasetName
) O
( O
Yu O
et O
al O
. O
, O
2018 O
) O
contains O
69 B-HyperparameterValue
tasks B-HyperparameterName
in O
total O
. O

Following O
( O
Geng O
et O
al O
. O
, O
2019 O
) O
, O
we O
build O
a O
2 B-HyperparameterValue
- I-HyperparameterValue
way I-HyperparameterValue
5 I-HyperparameterValue
- I-HyperparameterValue
shot I-HyperparameterValue
meta O
- O
learning O
with O
57 B-HyperparameterValue
tasks B-HyperparameterName
for O
meta O
- O
training O
and O
12 B-HyperparameterValue
tasks B-HyperparameterName
for O
meta O
- O
testing O
. O

The O
procedure O
of O
meta O
- O
training O
and O
meta O
- O
testing O
are O
almost O
the O
same O
except O
that O
meta O
- O
testing O
does O
not O
optimize O
the O
learned O
model O
initialization O
θ O
and O
the O
initial O
parameter O
ω O
of O
the O
value O
predictor O
. O

For O
each O
task O
T O
t O
in O
the O
meta O
- O
testing O
phase O
, O
MemIML B-MethodName
also O
adapts O
θ O
to O
task O
- O
specific O
parameters O
θ O
i O
in O
the O
inner O
- O
loop O
and O
constructs O
the O
task O
- O
specific O
memory O
. O

In O
the O
outer O
- O
loop O
, O
MemIML B-MethodName
retrieves O
key O
- O
value O
pairs O
from O
the O
memory O
to O
conduct O
local O
adaptation O
based O
on O
the O
initial O
parameter O
ω O
. O

The O
estimated O
valueV O
q O
t O
from O
local O
adaptation O
helps O
the O
base O
model O
to O
infer O
the O
final O
outputŶ O
q O
t O
.Experiments O
on O
personalized B-TaskName
dialogue I-TaskName
generation I-TaskName
and O
multi O
- O
domain O
sentiment B-TaskName
classification I-TaskName
verify O
our O
model O
on O
text B-TaskName
generation I-TaskName
and I-TaskName
classification I-TaskName
, O
respectively O
, O
where O
we O
use O
Persona B-DatasetName
- I-DatasetName
Chat I-DatasetName
and O
ARSC B-DatasetName
datasets O
. O

Dataset O
. O

Following O
, O
we O
use O
Persona B-DatasetName
- I-DatasetName
chat I-DatasetName
( O
Madotto O
et O
al O
. O
, O
2019 O
) O
Metrics O
. O

Automatic O
evaluation O
has O
three O
aspects O
, O

I(Ŷ O
q O
i O
; O
[ O
D O
s O
i O
, O
M O
i O
] O
| O
θ O
, O
X O
q O
i O
) O
> O
I(Ŷ O
q O
i O
; O
D O
s O
i O
| O
θ O
, O
X O
q O
i O
) O
, O
( O
In O
the O
meta O
- O
training O
phase O
( O
shown O
in O
Alg O
. O

1 O
) O
, O
MemIML B-MethodName
first O
constructs O
an O
empty O
memory O
for O
each O
task O
and O
then O
follows O
the O
bi O
- O
level O
optimization O
process O
of O
MAML B-MethodName
. O

In O
the O
inner O
loop O
, O
MemIML B-MethodName
adapts O
the O
base O
model O
initialization O
θ O
to O
taskspecific O
parameters O
via O
training O
on O
the O
support O
set O
. O

At O
the O
same O
time O
, O
from O
each O
support O
- O
set O
sample O
, O
MemIML B-MethodName
obtains O
a O
key O
- O
value O
pair O
and O
determines O
whether O
to O
write O
it O
into O
the O
memory O
or O
not O
. O

Then O
, O
MemIML B-MethodName
conducts O
the O
global O
optimization O
of O
the O
value O
predictor O
over O
these O
key O
- O
value O
pairs O
. O

In O
the O
outer O
loop O
, O
each O
sample O
of O
the O
query O
set O
reads O
the O
memory O
to O
retrieve O
the O
most O
similar O
memory O
slots O
. O

Local O
adaptation O
fine O
- O
tunes O
the O
value O
predictor O
on O
those O
retrieved O
slots O
. O

Next O
, O
the O
adapted O
value O
predictor O
estimates O
the O
value O
of O
each O
query O
sample O
and O
uses O
it O
to O
augment O
the O
learning O
of O
the O
model O
initialization O
. O

The O
total B-HyperparameterName
loss I-HyperparameterName
function I-HyperparameterName
in O
the O
inner O
loop O
is O
L B-HyperparameterName
total I-HyperparameterName
= O
L B-HyperparameterValue
base I-HyperparameterValue
+ I-HyperparameterValue
L I-HyperparameterValue
rec I-HyperparameterValue
, O
where O

L B-HyperparameterName
base I-HyperparameterName
= O
L(f O
( O
X O
s O
) O
, O
Y O
s O
) O
is O
the O
cross B-HyperparameterValue
- I-HyperparameterValue
entropy I-HyperparameterValue
loss B-HyperparameterName
. O

We O
propose O
a O
criterion O
similar O
to O
( O
Yao O
et O
al O
. O
, O
2021 O
) O
to O
measure O
the O
validity O
of O
our O
method O
for O
tackling O
this O
problem O
. O

For O
a O
task O
T O
i O
= O
{ O
D O
s O
i O
, O
D O
q O
i O
} O
, O
the O
criterion O
aims O
to O
mitigate O
the O
memorization O
overfitting O
by O
enhancing O
the O
model O
's O
dependence O
on O
the O
support O
set O
D O
s O
i O
, O
i.e. O
increasing O
the O
mutual B-MetricName
information I-MetricName
between O
support O
set O
andŶ O
q O
i O
as O
follows O
: O
6 O
) O
where O
M O
i O
means O
additional O
memory O
information O
we O
provide O
, O
which O
contains O
support O
sets O
information O
to O
augment O
the O
inference O
of O
the O
sample O
X O
q O
i O
in O
D O
q O
i O
. O

We O
demonstrate O
our O
method O
MemIML B-MethodName
meets O
the O
above O
criterion O
( O
See O
details O
in O
Appendix O
. O

A. O
) O
. O

Following O
Yin O
et O
al O
. O
( O
2020 O
) O
, O
we O
use O
mutual B-MetricName
information I-MetricName
I(Ŷ O
q O
i O
; O
D O
s O
i O
|θ O
, O
X O
q O
i O
) O
to O
measure O
the O
level O
of O
the O
memorization O
overfitting O
. O

When O
the O
learned O
model O
ignores O
support O
sets O
to O
predict O
query O
sets O
, O
I(Ŷ O
q O
i O
; O
D O
s O
i O
) O
|θ O
, O
X O
q O
i O
) O
= O
0 O
occurs O
, O
which O
indicates O
the O
complete O
memorization O
overfitting O
in O
metalearning O
( O
Yin O
et O
al O
. O
, O
2020 O
) O
. O

Hence O
, O
lower O
mutual B-MetricName
information I-MetricName
means O
more O
serious O
memorization O
overfitting O
issues O
. O

Update O
θ O
← O
θ O
− O
α4∇ B-HyperparameterName
θ O
T O
i O
∼p(T O
) O
L O
base O
T O
i O
, O
θ O
i O
( O
Ŷ O
q O
, O
Y O
q O
) O
We O
theoretically O
investigate O
how O
our O
method O
helps O
to O
alleviate O
the O
memorization O
overfitting O
problem O
. O

ω O
q O
j O
← O
ω O
− O
α3∇ωL B-HyperparameterName
loc O
# O
Local O
adaptation O
14 O
: O
V O
q O
j O
= O
g O
ω O
q O
j O
( O
K O
q O
j O
) O
# O
Predict O

Retrieve O
N B-HyperparameterName
nearest O
neighbors O
of O
K O
q O
j O
from O
Mi O
. O

Obtain O
the O
keys O
K O
q O
j O
for O
each O
sample O
X O
q O
j O
12 O
: O

where O
β B-HyperparameterName
balancesỸ O
q O
j O
andV O
q O
j O
. O

Notice O
that O
the O
interpolation O
not O
only O
works O
on O
the O
prediction O
output O
but O
also O
guides O
the O
training O
via O
gradient O
descent O
based O
on O
the O
interpolated O
output O
. O

We O
verify O
the O
effectiveness O
of O
the O
interpolation O
in O
Appendix O
. O

C. O
Mi O
← O
{ O
< O
K O
s O
l O
, O
V O
s O
l O
> O
} O
N O
s O
l=1 O
# O
Write O
memory O
8 O
: O
ω O
← O
ω O
− O
α1∇ωL B-HyperparameterName
rec O
# O
Global O
optimization O
9 O
: O
θ O
i O
← O
θ O
− O
α2∇ B-HyperparameterName
θ O
L O
base O
# O
Learn O
θ O
i O
in O
Eq O
. O

2 O
10 O
: O
for O
( O
X O
q O
j O
, O
Y O
q O
j O
) O
in O
D O
q O
i O
do O
11 O
: O

Y O
q O
j O
= O
βỸ B-HyperparameterName
q O
j O
+ O
( O
1 O
− O
β)V B-HyperparameterName
q O
j O
( O
5 O

Multi O
- O
domain O
Sentiment B-TaskName
Classification I-TaskName
. O

The O
base O
model O
is O
a O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
followed O
by O
a O
fully O
- O
connected O
network O
. O

Each O
sample O
consists O
of O
an O
input O
sentence O
and O
a O
sentiment O
label O
( O
ground O
truth O
) O
, O
so O
the O
memory O
value O
V O
s O
l O
is O
the O
sentiment O
label O
. O

To O
leverageV O
q O
j O
, O
we O
interpolate O
it O
with O
the O
original O
output O
of O
the O
base O
modelỸ O
q O
j O
aŝ O

Personalized B-TaskName
Dialogue I-TaskName
Generation I-TaskName
. O

The O
base O
model O
is O
the O
transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
consisting O
of O
an O
encoder O
and O
a O
decoder O
. O

In O
this O
task O
, O
each O
sample O
consists O
of O
an O
input O
utterance O
and O
a O
ground O
truth O
utterance O
, O
so O
the O
value O
V O
s O
l O
stored O
in O
the O
memory O
is O
obtained O
from O
the O
ground O
truth O
utterance O
Y O
s O
l O
of O
a O
support O
- O
set O
sample O
, O
which O
is O
embedded O
by O
the O
key O
network O
followed O
by O
an O
LSTM O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
. O

This O
LSTM O
is O
optimized O
with O
the O
base O
model O
. O

Thê O
V O
q O
j O
, O
concatenated O
with O
the O
encoder O
outputs O
, O
serves O
as O
a O
new O
input O
for O
the O
decoder O
. O

Hence O
, O
we O
acquire O
the O
prediction O
of O
a O
query O
- O
set O
sample O
viâ O
Y O
q O
j O
= O
Decoder([V O
q O
j O
; O
Encoder(X O
q O
j O
) O
] O
) O
. O

where O
the O
adapted O
parameters O
ω O
q O
j O
are O
discarded O
thereafter O
, O
and O
the O
model O
does O
not O
back O
- O
propagate O
throughV O
q O
j O
. O

In O
this O
sense O
, O
besides O
the O
task O
- O
specific O
parameter O
θ O
i O
provided O
by O
MAML B-MethodName
, O
there O
will O
also O
be O
ω O
q O
j O
learned O
from O
support O
sets O
specific O
to O
each O
query O
- O
set O
sample O
. O

This O
guarantees O
that O
the O
model O
relies O
more O
on O
support O
sets O
for O
task O
adaptation O
. O

Fig O
. O

1 O
( O
right O
part O
) O
illustrates O
the O
mechanism O
of O
local O
adaptation O
. O

In O
this O
part O
, O
we O
will O
elaborate O
on O
two O
few O
- O
shot O
applications O
in O
NLP O
( O
i.e. O
, O
text B-TaskName
generation I-TaskName
and O
text B-TaskName
classification I-TaskName
) O
to O
solve O
the O
memorization O
overfitting O
problem O
of O
MAML B-MethodName
. O

The O
model O
structures O
of O
these O
applications O
are O
basically O
the O
same O
, O
except O
for O
the O
following O
three O
points O
: O
the O
base O
model O
, O
the O
way O
to O
get O
the O
value O
V O
s O
l O
stored O
in O
the O
memory O
module O
, O
and O
the O
way O
to O
leverage O
the O
outputV O
q O
j O
of O
Sec O
. O

4.2 O
. O

V O
q O
j O
= O
g O
ω O
q O
j O
( O
K O
q O
j O
) O
, O
( O
4 O
) O

is O
the O
memory O
reading O
output O
of O
the O
query O
- O
set O
sample O
, O
and O
the O
factor O
γ B-HyperparameterName
restricts O
the O
distance O
between O
ω O
q O
j O
and O
ω O
. O

Minimizing O
the O
second O
term O
encourages O
g O
ω O
q O
j O
to O
better O
estimate O
the O
retrieved O
memory O
values O
{ O
V O
s O
l O
} O
N O
l=1 O
. O

Then O
we O
can O
acquire O
the O
locally O
adapted O
value O
prediction O
network O
g O
ω O
q O
j O
with O
parameters O
ω O
q O
j O
= O
arg O
miñ O
ω O
L O
loc O
( O
ω O
) O
. O

Given O
a O
query O
- O
sample O
key O
K O
q O
j O
, O
we O
can O
thus O
predict O
its O
associated O
value O
aŝ O

L O
loc O
= O
γ O
ω O
− O
ω O
2 O
2 O
+ O
1 O
N O
N O
l=1 O
L O
rec O
ω O
( O
V O
s O
l O
, O
V O
s O
l O
) O
( O
3 O
) O
Here O
, O
V O
s O
l O
= O
gω(K O
s O
l O
) O
, O
{ O
K O
s O
l O
, O
V O
s O
l O
} O
N O
l=1 O

Local O
Adaptation O
. O
To O
make O
the O
value O
predictor O
adaptive O
to O
each O
query O
- O
set O
sample O
X O
q O
j O
, O
inspired O
by O
( O
Sprechmann O
et O
al O
. O
, O
2018 O
) O
, O
we O
propose O
local O
adaptation O
that O
fine O
- O
tunes O
the O
global O
value O
predictor O
g O
ω O
to O
get O
an O
adapted O
one O
with O
parameters O
ω O
q O
j O
. O

The O
local O
adaptation O
only O
works O
when O
predicting O
X O
q O
j O
. O

Based O
on O
the O
initial O
parameters O
ω O
from O
the O
global O
optimization O
, O
we O
perform O
several O
gradient O
descent O
steps O
to O
minimize O
the O
loss O
L O
loc O
, O
which O
is O
: O

Global O
Optimization O
. O
To O
obtain O
the O
taskindependent O
global O
parameters O
ω O
, O
we O
train O
the O
value O
predictor O
over O
constructed O
keys O
( O
i.e. O
, O
as O
inputs O
) O
and O
values O
( O
i.e. O
, O
as O
outputs O
) O
from O
support O
- O
set O
samples O
of O
all O
tasks O
. O

The O
global O
optimization O
keeps O
updating O
in O
the O
whole O
meta O
- O
training O
phase O
. O

The O
training O
procedure O
includes O
the O
global O
optimization O
shared O
across O
tasks O
and O
the O
local O
adaptation O
for O
each O
specific O
task O
. O

Specifically O
, O
we O
first O
train O
the O
value O
predictor O
with O
samples O
from O
support O
sets O
of O
all O
tasks O
. O

After O
feeding O
the O
memory O
reading O
output O
of O
a O
query O
- O
set O
sample O
to O
this O
network O
, O
we O
perform O
local O
adaptation O
and O
employ O
the O
adapted O
network O
to O
estimate O
the O
value O
for O
the O
query O
sample O
. O

Specifically O
, O
we O
use O
a O
two B-HyperparameterValue
- I-HyperparameterValue
layer I-HyperparameterValue
fully B-HyperparameterName
- I-HyperparameterName
connected I-HyperparameterName
network I-HyperparameterName
g B-HyperparameterName
ω I-HyperparameterName
with O
parameters O
ω O
to O
build O
the O
mapping O
. O

The O
value O
predictor O
is O
learned O
over O
constructed O
keyvalue O
pairs O
of O
support O
sets O
across O
all O
tasks O
. O

Given O
the O
key O
K O
q O
j O
of O
a O
query O
- O
set O
sample O
input O
X O
q O
j O
, O
we O
can O
then O
estimate O
its O
associated O
value O
asV O
q O
j O
.To O
train O
the O
value O
predictor O
, O
we O
minimize O
the O
reconstruction B-HyperparameterName
loss I-HyperparameterName
L B-HyperparameterName
rec I-HyperparameterName
ω I-HyperparameterName
( O
V O
, O
V O
) O
to O
make O
the O
predicted O
values O
as O
close O
as O
possible O
to O
values O
constructed O
from O
the O
ground O
truths O
of O
support O
- O
set O
samples O
, O
where O
L B-HyperparameterName
rec I-HyperparameterName
ω I-HyperparameterName
is O
the O
cross B-HyperparameterValue
- I-HyperparameterValue
entropy I-HyperparameterValue
loss B-HyperparameterName
if O
the O
value O
V O
is O
a O
label O
and O
is O
the O
mean O
square O
loss O
if O
V O
is O
a O
vector O
. O

In O
this O
way O
, O
the O
proposed O
imitation O
module O
is O
customized O
for O
each O
query O
- O
set O
sample O
, O
which O
facilitates O
better O
capture O
of O
specific O
task O
information O
than O
directly O
using O
the O
memory O
reading O
output O
, O
especially O
when O
tasks O
are O
versatile O
. O

The O
reason O
is O
that O
the O
similarity O
measurement O
of O
previous O
memory O
reading O
operations O
is O
based O
on O
the O
fixed O
BERT B-MethodName
representations O
, O
which O
ignores O
the O
task O
- O
specific O
information O
. O

In O
MemIML B-MethodName
, O
the O
proposed O
value O
predictor O
aims O
to O
build O
a O
mapping O
from O
keys O
to O
values O
of O
the O
memory O
module O
mentioned O
in O
Sec O
. O

4.1 O
. O

The O
input O
of O
the O
value O
predictor O
is O
a O
key O
obtained O
from O
the O
key O
network O
, O
and O
the O
output O
is O
the O
associated O
value O
. O

Memory O
Reading O
obtains O
information O
from O
memory O
to O
enhance O
the O
meta O
- O
learning O
. O

The O
input O
is O
the O
sentence O
representation O
of O
the O
sample O
in O
query O
sets O
encoded O
by O
the O
key O
network O
, O
and O
the O
output O
is O
the O
memory O
slots O
similar O
to O
the O
query O
sample O
. O

Specifically O
, O
given O
the O
key O
representation O
K O
q O
j O
of O
a O
sample O
X O
q O
j O
∈ O
D O
q O
i O
, O
we O
retrieve O
the O
top O
N B-HyperparameterName
most O
similar O
slots O
from O
its O
task O
- O
specific O
memory O
M O
i O
. O

The O
similarity B-HyperparameterName
is O
measured O
based O
on O
the O
Euclidean B-HyperparameterValue
distance I-HyperparameterValue
between O
K O
q O
j O
and O
each O
key O
K O
s O
l O
in O
the O
memory O
slots O
. O

The O
retrieved O
key O
- O
value O
pairs O
{ O
K O
s O
l O
, O
V O
s O
l O
} O
N O
l=1 O
act O
as O
the O
output O
of O
memory O
reading O
. O

In O
order O
to O
better O
leverage O
the O
retrieved O
memory O
and O
enhance O
the O
dependence O
of O
our O
model O
on O
support O
sets O
, O
we O
propose O
an O
imitation O
module O
to O
encourage O
the O
imitation O
of O
support O
sets O
behaviors O
when O
making O
predictions O
on O
query O
sets O
. O

For O
each O
sample O
X O
q O
j O
in O
the O
query O
set O
, O
the O
inputs O
of O
the O
imitation O
module O
are O
the O
key O
K O
q O
j O
and O
its O
retrieved O
N O
memory O
slots O
, O
and O
the O
output O
is O
the O
predicted O
valueV O
q O
j O
for O
X O
q O
j O
. O

To O
achieve O
the O
imitation O
, O
we O
construct O
a O
value O
predictor O
that O
can O
model O
the O
behaviors O
of O
supportset O
samples O
( O
i.e. O
key O
- O
value O
matching O
) O
stored O
in O
the O
memory O
. O

For O
estimating O
the O
value O
of O
each O
query O
- O
set O
sample O
, O
we O
conduct O
local O
adaptation O
on O
the O
value O
predictor O
to O
adapt O
the O
matching O
. O

For O
each O
task O
- O
specific O
memory O
module O
M O
i O
, O
we O
adopt O
the O
diversity O
score O
as O
S(M O
i O
) O
on O
the O
stored O
keys O
. O

Here O
, O
a O
more O
diverse O
memory O
gets O
a O
higher O
diversity O
score O
. O

When O
the O
memory O
is O
not O
full O
, O
we O
directly O
write O
support O
- O
set O
samples O
without O
selection O
; O
otherwise O
, O
we O
compute O
the O
diversity O
score O
of O
the O
current O
memory O
and O
scores O
after O
every O
old O
key O
- O
value O
pair O
is O
replaced O
with O
a O
new O
key O
- O
value O
pair O
. O

Then O
we O
replace O
the O
old O
pair O
with O
the O
new O
one O
where O
the O
replacement O
can O
maximize O
the O
diversity O
score O
. O

In O
this O
way O
, O
the O
memory O
we O
build O
can O
carry O
more O
distinguishable O
and O
representative O
information O
and O
efficiently O
utilize O
the O
storage O
space O
. O

) O
. O
To O
build O
these O
memory O
slots O
, O
we O
select O
samples O
from O
support O
sets O
and O
write O
their O
information O
into O
the O
memory O
. O

The O
sample O
selection O
is O
according O
to O
a O
diversity B-HyperparameterValue
- I-HyperparameterValue
based I-HyperparameterValue
selection B-HyperparameterName
criterion I-HyperparameterName
( O
Xie O
et O
al O
. O
, O
2015 O
) O
to O
ensure O
the O
diversity O
and O
representativeness O
of O
the O
memory O
content O
. O

The O
detailed O
description O
of O
this O
criterion O
is O
in O
Appendix O
. O

D. O

{ O
K O
s O
l O
, O
V O
s O
l O
} O
N O
i O
l=1 O

Key B-HyperparameterName
Network I-HyperparameterName
represents O
a O
sample O
with O
a O
vector O
. O

Specifically O
, O
we O
use O
a O
frozen B-HyperparameterValue
pre I-HyperparameterValue
- I-HyperparameterValue
trained I-HyperparameterValue
BERT I-HyperparameterValue
model I-HyperparameterValue
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
as O
the O
key B-HyperparameterName
network I-HyperparameterName
. O

The O
input O
of O
the O
key B-HyperparameterName
network I-HyperparameterName
is O
the O
sample O
input O
sentence O
X O
s O
j O
∈ O
D O
s O
i O
( O
X O
q O
j O
∈ O
D O
q O
i O
) O
, O
and O
the O
output O
is O
the O
encoded O
representation O
of O
the O
first O
token O
( O
i.e. O
[ O
CLS O
] O
token O
) O
of O
the O
sentence O
. O

The O
acquired O
representation O
is O
regarded O
as O
the O
key O
K O
s O
j O
for O
X O
s O
j O
( O
K O
q O
j O
for O
X O
q O
j O
) O
. O

Memory O
Writing O
constructs O
the O
memory O
using O
the O
information O
of O
samples O
in O
the O
support O
set O
D O
s O
i O
. O

For O
each O
task O
T O
i O
, O
the O
task O
- O
specific O
memory O
M O
i O
consists O
of O
N O
i O
memory O
slots O
( O
i.e. O
key O
- O
value O
pairs O

where O
α B-HyperparameterName
is O
the O
inner B-HyperparameterName
loop I-HyperparameterName
learning I-HyperparameterName
rate I-HyperparameterName
. O

During O
the O
meta O
- O
testing O
stage O
, O
the O
learned O
initialization O
θ O
* O
is O
fine O
- O
tuned O
on O
the O
support O
set O
D O
s O
t O
for O
task O
T O
t O
, O
and O
the O
resulting O
model O
is O
evaluated O
on O
the O
query O
set O
D O
q O
t O
with O
the O
post O
- O
update O
parameters O
θ O
t O
.To O
alleviate O
the O
memorization O
overfitting O
issue O
in O
meta O
- O
learning O
, O
we O
propose O
MemIML B-MethodName
, O
which O
includes O
a O
memory O
module O
and O
an O
imitation O
module O
on O
the O
grounds O
of O
a O
base O
model O
. O

The O
memory O
module O
is O
task O
- O
specific O
, O
recording O
the O
mapping O
behaviors O
between O
inputs O
and O
outputs O
of O
support O
sets O
for O
each O
task O
. O

The O
imitation O
module O
is O
shared O
across O
tasks O
and O
predicts O
values O
for O
each O
query O
- O
set O
sample O
by O
dynamically O
imitating O
the O
memory O
construction O
. O

The O
acquired O
support O
set O
information O
leveraged O
by O
the O
imitation O
module O
augments O
the O
model O
initialization O
learning O
, O
enhancing O
the O
dependence O
of O
the O
model O
's O
task O
adaptation O
on O
support O
sets O
. O

Fig O
. O

1 O
shows O
our O
model O
architecture O
. O

We O
design O
a O
memory O
module O
M O
i O
for O
each O
task O
T O
i O
and O
incorporate O
it O
in O
the O
MAML B-MethodName
framework O
. O

In O
order O
to O
fully O
leverage O
information O
from O
support O
sets O
, O
we O
construct O
key O
- O
value O
pairs O
from O
support O
- O
set O
samples O
and O
store O
them O
in O
the O
memory O
module O
. O

The O
key O
is O
the O
sentence O
representation O
of O
a O
sample O
input O
from O
support O
sets O
obtained O
from O
an O
introduced O
key O
network O
. O

The O
corresponding O
value O
is O
constructed O
to O
store O
the O
information O
of O
the O
sample O
output O
( O
ground O
truth O
) O
as O
in O
Sec O
. O

4.3 O
: O
in O
NLG B-TaskName
tasks O
, O
the O
value O
is O
the O
sentence O
embedding O
of O
the O
output O
sentence O
; O
in O
NLU B-TaskName
tasks O
, O
the O
value O
is O
the O
one O
hot O
embedding O
of O
the O
class O
label O
( O
a O
scalar O
) O
of O
the O
sample O
. O

Our O
memory O
has O
two O
operations O
: O
memory O
writing O
that O
constructs O
the O
memory O
and O
memory O
reading O
that O
acquires O
information O
from O
memory O
. O

In O
the O
following O
, O
we O
elaborate O
on O
these O
contents O
in O
detail O
. O

θ O
* O
= O
min O
θ O
E O
T O
i O
∼p(T O
) O
L O
f O
θ O
i O
( O
X O
q O
i O
) O
, O
Y O
q O
i O
( O
1 O
) O
s.t O
. O

θ O
i O
= O
θ O
− O
α∇ B-HyperparameterName
θ O
L O
( O
f O
θ O
( O
X O
s O
i O
) O
, O
Y O
s O
i O
) O
( O
2 O
) O

, O
where O
X O
and O
Y O
denote O
the O
input O
and O
ground O
truth O
of O
a O
sample O
, O
respectively O
. O

During O
the O
meta O
- O
training O
stage O
, O
a O
taskspecific O
( O
a.k.a O
. O

, O
post O
- O
update O
) O
model O
f O
θ O
i O
is O
first O
obtained O
for O
each O
task O
T O
i O
via O
gradient O
descent O
over O
its O
support O
set O
D O
s O
i O
. O

Then O
MAML B-MethodName
updates O
its O
initialization O
( O
a.k.a O
. O

, O
pre O
- O
update O
) O
θ O
according O
to O
the O
performance O
of O
f O
θ O
i O
on O
the O
query O
set O
D O
q O
i O
as O
in O
Eq.1 O
: O

D O
s O
i O
= O
{ O
( O
X O
s O
j O
, O
Y O
s O
j O
) O
} O
N O
s O
j=1 O
and O
a O
query O
set O
D O
q O
i O
= O
{ O
( O
X O
q O
j O
, O
Y O
q O
j O
) O
} O
N O
q O
j=1 O

Memory O
mechanism O
has O
proven O
to O
be O
powerful O
for O
few O
- O
shot O
learning O
( O
Geng O
et O
al O
. O
, O
2019;Santoro O
et O
al O
. O
, O
2016;Munkhdalai O
et O
al O
. O
, O
2019 O
) O
. O

Current O
methods O
either O
refine O
representations O
stored O
in O
the O
memory O
( O
Ramalho O
and O
Garnelo O
, O
2018 O
) O
or O
refining O
parameters O
using O
the O
memory O
( O
Munkhdalai O
and O
Yu O
, O
2017;Cai O
et O
al O
. O
, O
2018 O
; O
. O

In O
the O
NLP O
domain O
, O
some O
methods O
store O
encoded O
contextual O
information O
into O
a O
memory O
Holla O
et O
al O
. O
, O
2020;Zheng O
et O
al O
. O
, O
2019 O
) O
. O

Geng O
et O
al O
. O
( O
2019 O
) O
propose O
a O
memory O
induction O
module O
with O
a O
dynamic O
routing O
algorithm O
for O
few O
- O
shot O
text B-TaskName
classification I-TaskName
tasks O
. O

Munkhdalai O
et O
al O
. O
( O
2019 O
) O
augment O
the O
model O
with O
an O
external O
memory O
by O
learning O
a O
neural O
memory O
. O

Wang O
et O
al O
. O
( O
2021 O
) O
reuse O
learned O
features O
stored O
in O
the O
memory O
on O
the O
few O
- O
shot O
slot B-TaskName
tagging I-TaskName
. O

We O
first O
formulate O
model B-MethodName
- I-MethodName
agnostic I-MethodName
meta I-MethodName
- I-MethodName
learning I-MethodName
( O
MAML B-MethodName
) O
( O
Finn O
et O
al O
. O
, O
2017 O
) O
. O

Specifically O
, O
denote O
the O
base O
model O
used O
in O
MAML B-MethodName
as O
f O
θ O
and O
assume O
each O
task O
T O
i O
sampled O
from O
a O
task O
distribution O
p(T O
) O
associates O
with O
a O
dataset O
D O
i O
. O

Each O
dataset O
D O
i O
consists O
of O
a O
support O
set O

External O
Memory O
for O
Few O
- O
shot O
Learning O
. O

Memorization O
overfitting O
of O
Meta O
- O
learning O
. O

Meta O
- O
learning O
algorithms O
suffer O
from O
memorization O
overfitting O
. O

Yin O
et O
al O
. O
( O
2020 O
) O
build O
an O
information O
bottleneck O
to O
the O
model O
, O
while O
this O
approach O
decreases O
the O
model O
performance O
with O
this O
passive O
regularization O
. O

Rajendran O
et O
al O
. O
( O
2020 O
) O
inject O
random O
noise O
to O
the O
ground O
truth O
of O
both O
support O
and O
query O
sets O
, O
while O
little O
extra O
knowledge O
is O
introduced O
to O
learn O
a O
good O
initialization O
. O

Yao O
et O
al O
. O
( O
2021 O
) O
address O
overfitting O
issues O
by O
augmenting O
meta O
- O
training O
tasks O
through O
mixing O
up O
support O
and O
query O
sets O
. O

However O
, O
such O
augmentation O
for O
text O
needs O
to O
be O
based O
on O
the O
assumption O
of O
keeping O
the O
label O
and O
the O
data O
distribution O
unchanged O
, O
which O
is O
often O
not O
true O
in O
practice O
( O
Chen O
et O
al O
. O
, O
2021 O
) O
. O

Instead O
of O
regularization O
and O
data O
augmentation O
, O
we O
leverage O
the O
support O
sets O
information O
stored O
in O
the O
memory O
to O
augment O
the O
meta O
- O
learning O
. O

The O
contributions O
of O
this O
work O
are O
: O
Meta O
- O
Learning O
. O

Meta O
- O
Learning O
aims O
to O
improve O
the O
learning O
algorithm O
itself O
based O
on O
the O
previously O
learned O
experience O
( O
Thrun O
and O
Pratt O
, O
1998;Hospedales O
et O
al O
. O
, O
2021 O
) O
. O

In O
general O
, O
there O
are O
three O
categories O
of O
meta O
- O
learning O
methods O
: O
model O
- O
based O
methods O
, O
( O
Santoro O
et O
al O
. O
, O
2016;Obamuyide O
et O
al O
. O
, O
2019 O
) O
which O
depend O
on O
the O
particular O
model O
design O
to O
facilitate O
fast O
learning O
; O
metric O
- O
based O
methods O
, O
( O
Vinyals O
et O
al O
. O
, O
2016;Snell O
et O
al O
. O
, O
2017;Geng O
et O
al O
. O
, O
2019 O
) O
which O
encode O
samples O
into O
an O
embedding O
space O
and O
classify O
them O
based O
on O
the O
learned O
distance O
metric O
; O
optimization O
- O
based O
methods O
( O
Finn O
et O
al O
. O
, O
2017;Mi O
et O
al O
. O
, O
2019 O
) O
that O
learn O
a O
wellgeneralized O
model O
initialization O
which O
allows O
for O
fast O
adaptation O
to O
new O
tasks O
. O

For O
low O
- O
resource O
scenarios O
in O
NLP O
, O
optimization O
- O
based O
meta O
- O
learning O
methods O
achieved O
promising O
results O
on O
tasks O
such O
as O
personalized O
dialog B-TaskName
generation I-TaskName
( O
Madotto O
et O
al O
. O
, O
2019;Song O
et O
al O
. O
, O
2020 O
; O
, O
lowresource O
machine B-TaskName
translation I-TaskName
( O
Gu O
et O
al O
. O
, O
2018;Sharaf O
et O
al O
. O
, O
2020 O
) O
and O
question B-TaskName
answering I-TaskName
, O
few O
- O
shot O
slot B-TaskName
tagging I-TaskName
( O
Wang O
et O
al O
. O
, O
2021 O
) O
, O
and O
so O
on O
. O

Specifically O
, O
we O
propose O
a O
Memory B-MethodName
- I-MethodName
Imitation I-MethodName
Meta I-MethodName
- I-MethodName
Learning I-MethodName
( O
MemIML B-MethodName
) O
method O
that O
forces O
query O
set O
predictions O
to O
depend O
on O
their O
corresponding O
support O
sets O
by O
dynamically O
imitating O
behaviors O
of O
the O
latter O
. O

We O
therefore O
, O
introduce O
a O
memory O
module O
and O
an O
imitation O
module O
to O
enhance O
such O
dependence O
. O

The O
memory O
module O
is O
task O
- O
specific O
, O
storing O
representative O
information O
of O
support O
sets O
. O

The O
imitation O
module O
assists O
in O
predicting O
samples O
of O
query O
sets O
by O
dynamically O
imitating O
the O
memory O
construction O
. O

In O
this O
way O
, O
the O
model O
has O
to O
access O
the O
support O
set O
by O
memory O
imitation O
each O
time O
it O
makes O
a O
prediction O
on O
a O
query O
- O
set O
sample O
, O
hence O
it O
's O
no O
longer O
feasible O
for O
the O
model O
to O
memorize O
all O
meta O
tasks O
. O

In O
this O
paper O
, O
we O
address O
the O
memorization O
overfitting O
issue O
by O
enhancing O
the O
model O
's O
dependence O
on O
support O
sets O
when O
learning O
the O
model O
initialization O
, O
which O
forces O
the O
model O
to O
better O
leverage O
information O
from O
support O
sets O
. O

As O
an O
analogy O
, O
consider O
a O
young O
investor O
who O
has O
the O
ability O
to O
adapt O
to O
new O
circumstances O
rapidly O
but O
little O
memory O
of O
learned O
experiences O
, O
and O
an O
old O
investor O
who O
is O
experienced O
but O
refuses O
to O
be O
flexible O
. O

Our O
idea O
is O
to O
make O
the O
young O
investor O
adaptive O
to O
the O
various O
situations O
when O
he O
assesses O
his O
benefits O
so O
that O
he O
can O
not O
only O
take O
advantage O
of O
the O
old O
one O
's O
experience O
but O
also O
learn O
from O
the O
old O
investor O
how O
to O
leverage O
the O
learned O
experience O
. O

In O
this O
paper O
, O
the O
young O
investor O
stands O
for O
a O
standard O
meta O
- O
learning O
algorithm O
( O
e.g. O
, O
MAML B-MethodName
) O
, O
which O
is O
prone O
to O
memorization O
overfitting O
, O
and O
the O
old O
investor O
is O
a O
memory O
module O
we O
integrate O
into O
the O
method O
, O
carrying O
information O
of O
support O
sets O
. O

Several O
works O
have O
been O
proposed O
to O
tackle O
the O
memorization O
overfitting O
issue O
for O
regression O
and O
image O
classification O
tasks O
. O

Some O
studies O
try O
to O
explicitly O
regularize O
the O
model O
parameters O
( O
Yin O
et O
al O
. O
, O
2020;Rajendran O
et O
al O
. O
, O
2020 O
) O
, O
but O
this O
restricts O
the O
complexity O
of O
model O
initialization O
and O
reduces O
the O
model O
capacity O
. O

Another O
line O
of O
research O
integrates O
samples O
from O
support O
sets O
into O
the O
corresponding O
query O
sets O
via O
data O
augmentation O
( O
Yao O
et O
al O
. O
, O
2021 O
) O
. O

However O
, O
data O
augmentation O
on O
textual O
data O
may O
result O
in O
noisy O
labels O
or O
distribution O
shifts O
, O
which O
impairs O
the O
model O
performance O
( O
Chen O
et O
al O
. O
, O
2021 O
) O
. O

Despite O
its O
effectiveness O
, O
optimization O
- O
based O
meta O
- O
learning O
algorithms O
usually O
suffer O
from O
the O
memorization O
overfitting O
issue O
1 O
( O
Yin O
et O
al O
. O
, O
2020;Rajendran O
et O
al O
. O
, O
2020 O
) O
, O
where O
the O
learned O
model O
tends O
to O
solve O
all O
the O
meta O
- O
training O
tasks O
by O
memorization O
, O
rather O
than O
learning O
how O
to O
quickly O
adapt O
from O
one O
task O
to O
another O
via O
support O
sets O
. O

This O
is O
acceptable O
for O
training O
process O
, O
but O
results O
in O
poor O
generalization O
on O
the O
meta O
- O
testing O
sets O
, O
because O
the O
memorized O
model O
does O
not O
have O
knowledge O
of O
those O
tasks O
and O
does O
not O
know O
how O
to O
utilize O
the O
base O
learner O
to O
learn O
new O
tasks O
. O

Hence O
, O
this O
issue O
hinders O
the O
model O
from O
capturing O
task O
- O
specific O
characteristics O
from O
support O
sets O
and O
thus O
prevents O
the O
model O
from O
adapting O
to O
distinct O
new O
tasks O
( O
Rajendran O
et O
al O
. O
, O
2020 O
) O
. O

For O
instance O
, O
in O
personalized B-TaskName
dialogue I-TaskName
generation I-TaskName
, O
this O
implies O
that O
the O
dialog O
model O
can O
not O
adapt O
to O
individual O
users O
based O
on O
short O
conversation O
histories O
and O
hence O
fails O
to O
generate O
personalized O
responses O
. O

Among O
different O
meta O
- O
learning O
approaches O
( O
Hospedales O
et O
al O
. O
, O
2021 O
) O
, O
optimization O
- O
based O
ap O
- O
proaches O
have O
been O
widely O
used O
in O
various O
lowresource O
NLP O
scenarios O
( O
Madotto O
et O
al O
. O
, O
2019;Qian O
and O
Yu O
, O
2019;Li O
et O
al O
. O
, O
2020;Mi O
et O
al O
. O
, O
2019 O
) O
because O
they O
are O
model O
- O
agnostic O
and O
easily O
applicable O
. O

Concretely O
, O
optimization O
- O
based O
meta O
- O
learning O
algorithms O
aim O
to O
learn O
a O
well O
- O
generalized O
global O
model O
initialization O
θ O
that O
can O
quickly O
adapt O
to O
new O
tasks O
within O
a O
few O
steps O
of O
gradient O
updates O
. O

In O
the O
meta O
- O
training O
process O
, O
we O
first O
train O
θ O
on O
a O
support O
set O
( O
i.e. O
, O
a O
few O
training O
samples O
of O
a O
new O
task O
i O
) O
to O
obtain O
task O
- O
specific O
parameters O
θ O
i O
. O

Then O
, O
we O
optimize O
θ O
based O
on O
the O
performance O
of O
θ O
i O
on O
a O
query O
set O
( O
i.e. O
, O
another O
set O
of O
samples O
in O
task O
i O
) O
. O

Figure O
20 O
: O
The O
performance O
of O
our O
model O
with O
varying O
amounts B-HyperparameterName
of I-HyperparameterName
pre I-HyperparameterName
- I-HyperparameterName
training I-HyperparameterName
data I-HyperparameterName
( O
the O
full O
result O
of O
Fig O
. O

7).This O
work O
is O
supported O
in O
part O
by O
ARC O
DP21010347 O
, O
ARC O
DP180100966 O
and O
Facebook O
Research O
. O

Joey O
Tianyi O
Zhou O
is O
supported O
by O
A*STAR O
SERC O
Central O
Research O
Fund O
( O
Useinspired O
Basic O
Research O
) O
. O

We O
thank O
the O
anonymous O
reviewers O
for O
their O
constructive O
suggestions O
. O

We O
thank O
Smashicons O
and O
Trazobanana O
for O
providing O
the O
icons O
in O
Fig O
. O

1.The O
appendix O
is O
organized O
as O
follows O
: O
Sec O
. O

A O
details O
the O
environment O
. O

Sec O
. O

B O
illustrates O
the O
process O
for O
constructing O
the O
pre O
- O
training O
datasets O
. O

Sec O
. O

C O
demonstrates O
the O
baselines O
' O
architecture O
and O
training O
details O
. O

Sec O
. O

D O
provides O
more O
experimental O
results O
. O

In O
the O
cooking B-TaskName
game I-TaskName
, O
the O
player O
is O
located O
in O
a O
house O
, O
which O
contains O
multiple O
rooms O
and O
interactable O
objects O
( O
food O
, O
tools O
, O
etc O
. O

) O
. O

Her O
/ O
his O
task O
is O
to O
follow O
the O
recipe O
to O
prepare O
the O
meal O
. O

Each O
game O
instance O
has O
a O
unique O
recipe O
, O
including O
different O
numbers O
of O
ingredients O
( O
food O
objects O
that O
are O
necessary O
for O
preparing O
the O
meal O
) O
and O
their O
corresponding O
preparation O
requirements O
( O
e.g. O
, O
" O
slice O
" O
, O
" O
fry O
" O
) O
. O

Besides O
the O
textual O
observation O
, O
the O
KG O
- O
based O
observation O
can O
also O
be O
directly O
obtained O
from O
the O
environment O
. O

The O
game O
sets O
used O
in O
our O
work O
contains O
a O
task O
set O
T O
of O
268 B-HyperparameterValue
subtasks B-HyperparameterName
, O
and O
an O
action O
set O
A O
of O
1304 B-HyperparameterValue
actions B-HyperparameterName
. O

Following O
GATA B-MethodName
's O
experiment O
setting O
( O
Adhikari O
et O
al O
. O
, O
2020 O
) O
, O
we O
simplify O
the O
game O
environment O
by O
making O
the O
action O
set O
changeable O
over O
time O
, O
which O
can O
be O
provided O
by O
the O
TextWorld O
platform O
. O

Note O
that O
although O
the O
action O
space O
is O
reduced O
, O
it O
still O
remains O
challenging O
as O
the O
agent O
may O
encounter O
unseen O
action O
candidates O
( O
Chandak O
et O
al O
. O
, O
2019(Chandak O
et O
al O
. O
, O
, O
2020 O
. O

We O
then O
use O
a O
similar O
way O
to O
obtain O
a O
changeable O
task O
set O
, O
which O
is O
a O
combination O
of O
the O
verb O
set O
{ O
chop O
, O
dice O
, O
slice O
, O
fry O
, O
make O
, O
get O
, O
grill O
, O
roast O
} O
and O
the O
ingredient O
set O
, O
where O
the O
construction O
details O
are O
provided O
in O
Appendix O
B. O
Table O
4 O
and O
Table O
5 O
show O
the O
KG O
- O
based O
observations O
o O
t O
, O
corresponding O
subtask O
candidates O
T O
and O
action O
candidates O
A. O
Table O
6 O
and O
Table O
7 O
show O
more O
examples O
of O
subtasks O
and O
actions O
, O
respectively O
. O

The O
underlined O
subtask O
candidates O
denote O
the O
available O
subtask O
set O
T O
t O
. O

The O
underlined O
action O
candidates O
in O
Table O
7 O
denote O
the O
refined O
action O
set O
A O
t O
after O
selecting O
the O
subtask O
" O
roast O
carrot O
" O
. O

We O
still O
denote O
the O
subtask O
candidate O
set O
( O
action O
candidate O
set O
) O
as O
T O
( O
A O
) O
to O
distinguish O
it O
from O
the O
available O
subtask O
set O
T O
t O
( O
refined O
action O
set O
A O
t O
) O
.We O
build O
separate O
datasets O
for O
each O
pre O
- O
training O
task O
( O
task O
decomposition O
, O
action O
pruning O
, O
and O
imitation O
learning O
) O
. O

We O
first O
let O
the O
player O
to O
go O
through O
each O
simple O
game O
, O
then O
construct O
the O
datasets O
upon O
the O
interaction O
data O
. O

For O
each O
time O
step O
, O
the O
game O
environment O
provides O
the O
player O
with O
the O
action O
set O
A O
and O
the O
KG O
- O
based O
observation O
o O
t O
, O
which O
is O
represented O
as O
a O
set O
of O
triplets O
. O

We O
use O
a O
simple O
method O
to O
build O
the O
subtask O
set O
T O
from O
o O
t O
: O
As O
shown O
in O
Fig O
. O

8 O
, O
we O
first O
obtain O
the O
ingredients O
by O
extracting O
the O
nodes O
having O
the O
relation O
" O
part_of O
" O
with O
the O
node O
" O
cookbook O
" O
. O

Then O
we O
build O
T O
as O
the O
Cartesian O
product O
of O
the O
ingredients O
and O
the O
verbs O
{ O
chop O
, O
dice O
, O
slice O
, O
fry O
, O
get O
, O
grill O
, O
roast O
} O
plus O
two O
special O
subtasks O
" O
get O
knife O
" O
and O
" O
make O
meal O
" O
. O

The O
player O
is O
required O
to O
select O
a O
subtask O
T O
t O
∈ O
T O
, O
and O
select O
an O
action O
a O
t O
∈ O
A. O
After O
executing O
a O
t O
, O
the O
environment O
will O
transit O
to O
next O
state O
s O
t+1 O
, O
and O
the O
player O
will O
receive O
o O
t+1 O
and O
r O
t+1 O
to O
form O
a O
transition O
{ O
o O
t O
, O
T O
, O
T O
t O
, O
A O
, O
a O
t O
, O
o O
t+1 O
, O
r O
t+1 O
} O
, O
where O
{ O
o O
t O
, O
T O
, O
T O
t O
, O
A O
, O
a O
t O
} O
will O
be O
used O
for O
imitation O
learning O
. O

Fig O
. O

8 O
shows O
the O
construction O
process O
of O
the O
pre O
- O
training O
dataset O
for O
task O
decomposition O
. O

Each O
subtask O
candidate O
T O
∈ O
T O
will O
formulate O
a O
question O
" O
Is O
T O
available O
? O
" O
, O
whose O
answer O
is O
1 O
( O
yes O
) O
if O
T O
is O
an O
available O
subtask O
for O
o O
t O
, O
otherwise O
0 O
( O
no O
) O
. O

Fig O
. O

9 O
shows O
the O
construction O
process O
of O
the O
pre O
- O
training O
dataset O
for O
action O
pruning O
. O

The O
action O
selector O
is O
made O
invariant O
of O
o O
t O
, O
that O
we O
consider O
every O
subtask O
candidate O
T O
∈ O
T O
during O
pre O
- O
training O
, O
regardless O
of O
whether O
T O
is O
a O
currently O
- O
available O
subtask O
. O

Each O
action O
candidate O
a O
∈ O
A O
will O
be O
paired O
with O
T O
to O
formulate O
a O
question O
" O
Is O
a O
relevant O
to O
T O
" O
, O
whose O
answer O
is O
1 O
if O
a O
is O
relevant O
to O
T O
, O
otherwise O
0 O
. O

Building O
models O
of O
natural O
language O
processing O
( O
NLP O
) O
is O
challenging O
in O
low O
- O
resource O
scenarios O
where O
only O
limited O
data O
are O
available O
. O

Optimization O
- O
based O
meta O
- O
learning O
algorithms O
achieve O
promising O
results O
in O
low O
- O
resource O
scenarios O
by O
adapting O
a O
well O
- O
generalized O
model O
initialization O
to O
handle O
new O
tasks O
. O

Nonetheless O
, O
these O
approaches O
suffer O
from O
the O
memorization O
overfitting O
issue O
, O
where O
the O
model O
tends O
to O
memorize O
the O
meta O
- O
training O
tasks O
while O
ignoring O
support O
sets O
when O
adapting O
to O
new O
tasks O
. O

To O
address O
this O
issue O
, O
we O
propose O
a O
memory B-MethodName
imitation I-MethodName
meta I-MethodName
- I-MethodName
learning I-MethodName
( O
MemIML B-MethodName
) O
method O
that O
enhances O
the O
model O
's O
reliance O
on O
support O
sets O
for O
task O
adaptation O
. O

Specifically O
, O
we O
introduce O
a O
task O
- O
specific O
memory O
module O
to O
store O
support O
set O
information O
and O
construct O
an O
imitation O
module O
to O
force O
query O
sets O
to O
imitate O
the O
behaviors O
of O
some O
representative O
supportset O
samples O
stored O
in O
the O
memory O
. O

A O
theoretical O
analysis O
is O
provided O
to O
prove O
the O
effectiveness O
of O
our O
method O
, O
and O
empirical O
results O
also O
demonstrate O
that O
our O
method O
outperforms O
competitive O
baselines O
on O
both O
text B-TaskName
classification I-TaskName
and O
generation B-TaskName
tasks O
. O

Figure O
19 O
: O
The O
performance O
of O
our O
model O
and O
the O
variants O
with O
expert O
modules O
( O
the O
full O
result O
of O
Fig O
. O

6 O
) O
. O

Figure O
18 O
: O
The O
RL O
performance O
of O
our O
model O
and O
the O
variant O
without O
time O
- O
awareness O
( O
the O
full O
result O
of O
Fig O
. O

5 O
) O
. O

We O
train O
the O
GATA B-MethodName
through O
reinforcement O
learning O
, O
the O
experiment O
setting O
is O
same O
with O
Sec O
. O

5.3 O
. O

Instead O
of O
initializing O
the O
word O
embedding O
, O
node O
embedding O
and O
edge O
embedding O
with O
fastText O
word O
vectors O
( O
Mikolov O
et O
al O
. O
, O
2017 O
) O
, O
we O
found O
that O
the O
action B-HyperparameterValue
prediction I-HyperparameterValue
task I-HyperparameterValue
( O
AP B-HyperparameterValue
) O
, O
which O
is O
also O
included O
in O
GATA B-MethodName
's O
work O
( O
Adhikari O
et O
al O
. O
, O
2020 O
) O
, O
could O
provide O
better O
initialization B-HyperparameterName
. O

In O
light O
of O
this O
, O
we O
could O
like O
to O
conduct O
such O
task O
, O
and O
apply O
the O
AP B-HyperparameterValue
initialization B-HyperparameterName
to O
all O
encoders O
( O
observation O
encoder O
, O
task O
encoder O
, O
action O
encoder O
) O
. O

Fig O
. O

11 O
shows O
the O
action O
predicting O
process O
. O

Given O
the O
transition O
data O
, O
the O
task O
is O
to O
predict O
the O
action O
a O
t O
∈ O
A O
given O
the O
current O
observation O
o O
t O
, O
and O
the O
next O
observation O
o O
t+1 O
after O
executing O
a O
t O
. O

The O
transition O
data O
for O
AP O
task O
is O
collected O
from O
the O
FTWP B-DatasetName
game I-DatasetName
set I-DatasetName
and O
is O
provided O
by O
GATA B-MethodName
's O
released O
code O
. O

Fig O
. O

12 O
shows O
the O
IL B-MethodName
baseline O
. O

We O
follow O
to O
conduct O
a O
two O
- O
phase O
training O
process O
: O
imitation O
pre O
- O
training O
and O
reinforcement O
fine O
- O
tuning O
. O

In O
the O
imitation O
pre O
- O
training O
phase O
, O
we O
use O
the O
transition O
data O
to O
train O
both O
the O
task O
selector O
( O
f O
( O
o O
t O
, O
T O
) O
→ O
T O
t O
) O
and O
the O
action O
selector O
( O
f O
( O
o O
t O
, O
T O
t O
, O
A O
) O
→ O
a O
t O
) O
through O
supervised O
learning O
. O

The O
modules O
are O
optimized O
via O
cross B-HyperparameterValue
entropy I-HyperparameterValue
loss B-HyperparameterName
and O
Adam B-HyperparameterValue
optimizer B-HyperparameterName
with O
learning B-HyperparameterName
rate I-HyperparameterName
0.001 B-HyperparameterValue
. O

We O
train O
the O
modules O
with O
batch B-HyperparameterName
size I-HyperparameterName
128 B-HyperparameterValue
for O
up O
to O
50 B-HyperparameterValue
epochs B-HyperparameterName
. O

Then O
in O
the O
reinforcement O
fine O
- O
tuning O
phase O
, O
we O
freeze O
the O
task O
selector O
and O
fine O
- O
tune O
the O
action O
selector O
through O
reinforcement O
learning O
, O
where O
the O
experiment O
setting O
is O
same O
with O
QWA B-MethodName
and O
GATA B-MethodName
. O

In O
the O
pre O
- O
training O
phase O
, O
we O
conduct O
rough O
hyper O
- O
parameter O
tuning O
by O
varying O
batch B-HyperparameterName
sizes I-HyperparameterName
. O

Fig O
. O

13 O
and O
Fig O
. O

14 O
show O
the O
pre O
- O
training O
performance O
of O
QWA B-MethodName
's O
task O
selector O
and O
action O
validator O
, O
respectively O
. O

Fig O
. O

15 O
shows O
the O
pre O
- O
training O
performance O
of O
IL O
baseline O
. O

Figure O
17 O
: O
The O
RL O
performance O
of O
models O
with O
respect O
to O
training O
episodes O
( O
the O
full O
result O
of O
Fig O
. O

4 O
) O
. O

Building O
natural O
language O
processing O
( O
NLP O
) O
models O
in O
low O
- O
resource O
scenarios O
is O
of O
great O
importance O
in O
practical O
applications O
because O
labeled O
data O
are O
scarce O
. O

Meta O
- O
learning O
- O
based O
methods O
( O
Thrun O
and O
Pratt O
, O
2012 O
) O
have O
been O
commonly O
used O
in O
such O
scenarios O
owing O
to O
their O
fast O
adaptation O
ability O
. O

Notable O
successes O
have O
been O
achieved O
by O
metalearning O
on O
low O
- O
resource O
NLP O
tasks O
, O
such O
as O
multidomain B-TaskName
sentiment I-TaskName
classification I-TaskName
( O
Yu O
et O
al O
. O
, O
2018;Geng O
et O
al O
. O
, O
2019 O
) O
and O
personalized B-TaskName
dialogue I-TaskName
generation I-TaskName
( O
Madotto O
et O
al O
. O
, O
2019;Song O
et O
al O
. O
, O
2020;Zheng O
et O
al O
. O
, O
2020 O
) O
. O

A O
possible O
reason O
is O
that O
there O
exists O
large O
domain O
gap O
between O
simple O
and O
medium O
( O
hard O
) O
games O
, O
and O
our O
model O
is O
more O
robust O
against O
such O
domain O
shifts O
. O

For O
example O
, O
our O
world O
- O
perceiving O
task O
selector O
performs O
better O
than O
IL O
- O
based O
task O
selector O
in O
handling O
more O
complex O
observations O
( O
according O
to O
Table O
1 O
, O
the O
observations O
in O
medium O
/ O
hard O
games O
contain O
more O
triplets O
, O
rooms O
and O
objects O
) O
, O
facilitating O
the O
training O
of O
the O
action O
selector O
. O

Besides O
the O
domain O
gap O
in O
terms O
of O
the O
observation O
space O
, O
there O
is O
also O
a O
gap O
between O
domains O
in O
terms O
of O
the O
number O
of O
available O
subtasks O
− O
while O
there O
's O
always O
one O
available O
subtask O
per O
time O
step O
in O
simple O
games O
, O
the O
model O
will O
face O
more O
available O
subtasks O
in O
the O
medium O
/ O
hard O
games O
. O

Different O
from O
our O
task O
selector O
, O
which O
is O
trained O
to O
check O
the O
availability O
of O
every O
subtask O
candidate O
, O
the O
IL O
pre O
- O
trained O
task O
selector O
can O
not O
adapt O
well O
in O
this O
situation O
, O
as O
it O
is O
trained O
to O
find O
the O
unique O
subtask O
and O
ignore O
the O
other O
subtask O
candidates O
despite O
whether O
they O
are O
also O
available O
. O

We O
further O
investigate O
the O
generalization O
performance O
of O
our O
model O
on O
simple O
games O
, O
considering O
that O
simple O
games O
are O
not O
engaged O
in O
our O
RL O
training O
. O

To O
conduct O
the O
experiment O
, O
after O
RL O
training O
, O
we O
deploy O
all O
models O
on O
a O
set O
of O
140 B-HyperparameterValue
held B-HyperparameterName
- I-HyperparameterName
out I-HyperparameterName
sim- O
ple O
games O
for O
RL O
interaction O
. O

Table O
3 O
shows O
the O
results O
, O
where O
" O
Medium O
100 O
% O
" O
( O
" O
Hard O
100 O
% O
" O
) O
denotes O
that O
the O
model O
is O
trained O
on O
medium O
( O
hard O
) O
games O
for O
the O
whole O
RL O
phase O
. O

The O
generalizability O
of O
GATA B-MethodName
, O
which O
is O
trained O
purely O
with O
medium O
and O
hard O
games O
, O
is O
significantly O
low O
and O
can O
not O
perform O
well O
on O
simple O
games O
. O

In O
contrast O
, O
our O
model O
performs O
very O
well O
and O
achieves O
over O
80 B-MetricValue
% I-MetricValue
of O
the O
scores B-MetricName
. O

The O
world O
- O
perceiving O
modules O
, O
which O
are O
pre O
- O
trained O
with O
simple O
games O
, O
help O
to O
train O
a O
decision O
module O
that O
adapts O
well O
on O
unseen O
games O
. O

It O
is O
not O
surprising O
that O
the O
variant O
" O
IL B-MethodName
w/o I-MethodName
FT I-MethodName
" O
also O
performs O
well O
on O
simple O
games O
, O
since O
they O
are O
only O
pre O
- O
trained O
with O
simple O
games O
. O

However O
, O
as O
indicated O
by O
the O
performance O
of O
" O
IL B-MethodName
" O
, O
after O
fine O
- O
tuning O
on O
medium O
/ O
hard O
games O
( O
recalling O
Sec O
. O

6.1 O
) O
, O
the O
action O
scorer O
" O
forgets O
" O
the O
experience O
/ O
skills O
dealing O
with O
simple O
games O
and O
the O
model O
fails O
to O
generalize O
on O
unseen O
simple O
games O
. O

In O
summary O
, O
the O
best O
performance O
achieved O
by O
QWA B-MethodName
demonstrates O
that O
our O
model O
can O
generalize O
well O
on O
games O
with O
different O
complexities O
. O

We O
study O
the O
contribution O
of O
the O
subtask O
timeawareness O
by O
comparing O
our O
full O
model O
with O
the O
variant O
without O
this O
technique O
. O

Fig O
. O

5 O
shows O
the O
result O
. O

Although O
the O
models O
perform O
similarly O
in O
the O
medium O
games O
, O
the O
full O
model O
shows O
better O
performance O
in O
the O
hard O
games O
, O
where O
there O
may O
exist O
more O
difficult O
subtasks O
( O
we O
regard O
a O
subtask O
more O
difficult O
if O
it O
requires O
more O
actions O
to O
be O
completed O
) O
. O

Assigning O
each O
subtask O
a O
time O
limit O
prevents O
the O
agent O
from O
pursuing O
a O
too O
difficult O
subtask O
, O
and O
improves O
subtask O
diversity O
by O
encouraging O
the O
agent O
to O
try O
different O
subtasks O
. O

Besides O
, O
it O
prevents O
the O
agent O
from O
being O
stuck O
in O
a O
wrong O
subtask O
, O
making O
the O
agent O
more O
robust O
to O
the O
compound O
error O
. O

We O
then O
investigate O
the O
performance O
upper O
bound O
of O
our O
method O
by O
comparing O
our O
model O
to O
variants O
with O
oracle O
world O
- O
perceiving O
modules O
. O

Fig O
. O

6 O
shows O
the O
results O
, O
where O
" O
+ B-HyperparameterValue
expTS I-HyperparameterValue
" O
( O
" O
+ B-HyperparameterValue
expAV I-HyperparameterValue
" O
) O
denotes O
that O
the O
model O
uses O
an O
expert O
task B-HyperparameterName
selector I-HyperparameterName
( O
action B-HyperparameterName
validator I-HyperparameterName
) O
. O

There O
's O
still O
space O
to O
improve O
the O
pre O
- O
trained O
modules O
. O

The O
variant O
" O
QWA B-MethodName
+ I-MethodName
expTS I-MethodName
+ I-MethodName
expAV I-MethodName
" O
solves O
all O
the O
medium O
games O
and O
achieves O
nearly O
80 B-MetricValue
% I-MetricValue
of O
the O
scores B-MetricName
in O
hard O
games O
, O
showing O
the O
potential O
of O
introducing O
world O
- O
perceiving O
modules O
in O
facilitating O
RL O
. O

We O
also O
find O
that O
assigning O
either O
the O
expert B-HyperparameterValue
task I-HyperparameterValue
selector I-HyperparameterValue
or O
the O
expert B-HyperparameterValue
action I-HyperparameterValue
validator I-HyperparameterValue
helps O
to O
improve O
the O
performance O
. O

In O
light O
of O
these O
findings O
, O
we O
will O
consider O
more O
powerful O
pre O
- O
training O
methods O
as O
a O
future O
direction O
. O

Although O
we O
only O
collect O
labeled O
data O
from O
the O
simple O
games O
, O
it O
is O
still O
burdensome O
for O
human O
players O
to O
go O
through O
the O
games O
and O
answer O
the O
questions O
. O

We O
are O
thus O
interested O
in O
investigating O
how O
the O
performance O
of O
our O
QWA B-MethodName
( O
or O
world O
- O
perceiving O
modules O
) O
varies O
with O
respect O
to O
a O
reduced B-HyperparameterName
amount I-HyperparameterName
of I-HyperparameterName
pre I-HyperparameterName
- I-HyperparameterName
training I-HyperparameterName
data I-HyperparameterName
. O

Fig O
. O

7 O
shows O
the O
results O
, O
where O
the O
pre O
- O
training O
dataset O
has O
been O
reduced O
to O
75 B-HyperparameterValue
% I-HyperparameterValue
, O
50 B-HyperparameterValue
% I-HyperparameterValue
and O
25 B-HyperparameterValue
% I-HyperparameterValue
, O
respectively O
. O

Our O
model O
still O
performs O
well O
when O
the O
pre O
- O
training O
data O
is O
reduced O
to O
75 B-HyperparameterValue
% I-HyperparameterValue
and O
50 B-HyperparameterValue
% I-HyperparameterValue
. O

When O
we O
only O
use O
25 B-HyperparameterValue
% I-HyperparameterValue
of O
the O
pre O
- O
training O
data O
, O
the O
model O
exhibits O
instability O
during O
the O
learning O
of O
hard O
games O
. O

Being O
pre O
- O
trained O
on O
a O
largely O
- O
reduced O
dataset O
, O
the O
world O
- O
perceiving O
modules O
might O
be O
more O
likely O
to O
make O
wrong O
predictions O
with O
the O
progress O
of O
RL O
training O
, O
leading O
to O
the O
performance O
fluctuation O
. O

However O
, O
the O
fi- O
nal O
performance O
of O
this O
variant O
is O
still O
comparable O
. O

To O
summarize O
, O
our O
model O
is O
robust O
to O
limited O
pretraining O
data O
and O
largely O
alleviates O
the O
burden O
of O
human O
annotations O
. O

In O
this O
paper O
, O
we O
addressed O
the O
challenges O
of O
low O
sample O
efficiency O
and O
large O
action O
space O
for O
deep O
reinforcement O
learning O
in O
solving O
text O
- O
based O
games O
. O

We O
introduced O
the O
world O
- O
perceiving O
modules O
, O
which O
are O
capable O
of O
automatic O
task O
decomposition O
and O
action O
pruning O
through O
answering O
questions O
about O
the O
environment O
. O

We O
proposed O
a O
twophase O
training O
framework O
, O
which O
decouples O
the O
language O
learning O
from O
the O
reinforcement O
learning O
. O

Experimental O
results O
show O
that O
our O
method O
achieves O
improved O
performance O
with O
high O
sample O
efficiency O
. O

Besides O
, O
it O
shows O
robustness O
against O
compound O
error O
and O
limited O
pre O
- O
training O
data O
. O

Regarding O
the O
future O
work O
, O
we O
would O
like O
to O
further O
improve O
the O
pre O
- O
training O
performance O
by O
introducing O
contrastive O
learning O
objective O
( O
You O
et O
al O
. O
, O
2020 O
) O
and O
KG O
- O
based O
data O
augmentation O
( O
Zhao O
et O
al O
. O
, O
2021 O
) O
. O

[ O
" O
block O
of O
cheese O
" O
, O
" O
cookbook O
" O
, O
" O
part_of O
" O
] O
, O
[ O
" O
block O
of O
cheese O
" O
, O
" O
fried O
" O
, O
" O
needs O
" O
] O
, O
[ O
" O
block O
of O
cheese O
" O
, O
" O
player O
" O
, O
" O
in O
" O
] O
, O
[ O
" O
block O
of O
cheese O
" O
, O
" O
raw O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
block O
of O
cheese O
" O
, O
" O
sliced O
" O
, O
" O
needs O
" O
] O
, O
[ O
" O
block O
of O
cheese O
" O
, O
" O
uncut O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
cookbook O
" O
, O
" O
counter O
" O
, O
" O
on O
" O
] O
, O
[ O
" O
counter O
" O
, O
" O
kitchen O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
fridge O
" O
, O
" O
kitchen O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
fridge O
" O
, O
" O
open O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
knife O
" O
, O
" O
counter O
" O
, O
" O
on O
" O
] O
, O
[ O
" O
oven O
" O
, O
" O
kitchen O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
player O
" O
, O
" O
kitchen O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
stove O
" O
, O
" O
kitchen O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
table O
" O
, O
" O
kitchen O
" O
, O
" O
at O
" O
] O
" O
fry O
block O
of O
cheese O
" O
, O
" O
get O
knife O
" O
, O
" O
chop O
block O
of O
cheese O
" O
, O
" O
dice O
block O
of O
cheese O
" O
, O
" O
get O
block O
of O
cheese O
" O
, O
" O
grill O
block O
of O
cheese O
" O
, O
" O
make O
meal O
" O
, O
" O
roast O
block O
of O
cheese O
" O
, O
" O
slice O
block O
of O
cheese O
" O
" O
close O
fridge O
" O
, O
" O
cook O
block O
of O
cheese O
with O
oven O
" O
, O
" O
cook O
block O
of O
cheese O
with O
stove O
" O
, O
" O
drop O
block O
of O
cheese O
" O
, O
" O
eat O
block O
of O
cheese O
" O
, O
" O
insert O
block O
of O
cheese O
into O
fridge O
" O
, O
" O
prepare O
meal O
" O
, O
" O
put O
block O
of O
cheese O
on O
counter O
" O
, O
" O
put O
block O
of O
cheese O
on O
stove O
" O
, O
" O
put O
block O
of O
cheese O
on O
table O
" O
, O
" O
take O
cookbook O
from O
counter O
" O
, O
" O
take O
knife O
from O
counter O
" O
Medium O
[ O
" O
bathroom O
" O
, O
" O
corridor O
" O
, O
" O
south_of O
" O
] O
, O
[ O
" O
bed O
" O
, O
" O
bedroom O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
bedroom O
" O
, O
" O
livingroom O
" O
, O
" O
north_of O
" O
] O
, O
[ O
" O
block O
of O
cheese O
" O
, O
" O
cookbook O
" O
, O
" O
part_of O
" O
] O
, O
[ O
" O
block O
of O
cheese O
" O
, O
" O
diced O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
block O
of O
cheese O
" O
, O
" O
diced O
" O
, O
" O
needs O
" O
] O
, O
[ O
" O
block O
of O
cheese O
" O
, O
" O
fridge O
" O
, O
" O
in O
" O
] O
, O
[ O
" O
block O
of O
cheese O
" O
, O
" O
fried O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
block O
of O
cheese O
" O
, O
" O
fried O
" O
, O
" O
needs O
" O
] O
, O
[ O
" O
carrot O
" O
, O
" O
fridge O
" O
, O
" O
in O
" O
] O
, O
[ O
" O
carrot O
" O
, O
" O
raw O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
carrot O
" O
, O
" O
uncut O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
cookbook O
" O
, O
" O
counter O
" O
, O
" O
on O
" O
] O
, O
[ O
" O
corridor O
" O
, O
" O
bathroom O
" O
, O
" O
north_of O
" O
] O
, O
[ O
" O
corridor O
" O
, O
" O
kitchen O
" O
, O
" O
east_of O
" O
] O
, O
[ O
" O
corridor O
" O
, O
" O
livingroom O
" O
, O
" O
south_of O
" O
] O
, O
[ O
" O
counter O
" O
, O
" O
kitchen O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
flour O
" O
, O
" O
cookbook O
" O
, O
" O
part_of O
" O
] O
, O
[ O
" O
flour O
" O
, O
" O
shelf O
" O
, O
" O
on O
" O
] O
, O
[ O
" O
fridge O
" O
, O
" O
closed O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
fridge O
" O
, O
" O
kitchen O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
frosted O
- O
glass O
door O
" O
, O
" O
closed O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
frosted O
- O
glass O
door O
" O
, O
" O
kitchen O
" O
, O
" O
west_of O
" O
] O
, O
[ O
" O
frosted O
- O
glass O
door O
" O
, O
" O
pantry O
" O
, O
" O
east_of O
" O
] O
, O
[ O
" O
kitchen O
" O
, O
" O
corridor O
" O
, O
" O
west_of O
" O
] O
, O
[ O
" O
knife O
" O
, O
" O
counter O
" O
, O
" O
on O
" O
] O
, O
[ O
" O
livingroom O
" O
, O
" O
bedroom O
" O
, O
" O
south_of O
" O
] O
, O
[ O
" O
livingroom O
" O
, O
" O
corridor O
" O
, O
" O
north_of O
" O
] O
, O
[ O
" O
oven O
" O
, O
" O
kitchen O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
parsley O
" O
, O
" O
fridge O
" O
, O
" O
in O
" O
] O
, O
[ O
" O
parsley O
" O
, O
" O
uncut O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
player O
" O
, O
" O
kitchen O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
pork O
chop O
" O
, O
" O
chopped O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
pork O
chop O
" O
, O
" O
chopped O
" O
, O
" O
needs O
" O
] O
, O
[ O
" O
pork O
chop O
" O
, O
" O
cookbook O
" O
, O
" O
part_of O
" O
] O
, O
[ O
" O
pork O
chop O
" O
, O
" O
fridge O
" O
, O
" O
in O
" O
] O
, O
[ O
" O
pork O
chop O
" O
, O
" O
fried O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
pork O
chop O
" O
, O
" O
fried O
" O
, O
" O
needs O
" O
] O
, O
[ O
" O
purple O
potato O
" O
, O
" O
counter O
" O
, O
" O
on O
" O
] O
, O
[ O
" O
purple O
potato O
" O
, O
" O
uncut O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
red O
apple O
" O
, O
" O
counter O
" O
, O
" O
on O
" O
] O
, O
[ O
" O
red O
apple O
" O
, O
" O
raw O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
red O
apple O
" O
, O
" O
uncut O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
red O
onion O
" O
, O
" O
fridge O
" O
, O
" O
in O
" O
] O
, O
[ O
" O
red O
onion O
" O
, O
" O
raw O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
red O
onion O
" O
, O
" O
uncut O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
red O
potato O
" O
, O
" O
counter O
" O
, O
" O
on O
" O
] O
, O
[ O
" O
red O
potato O
" O
, O
" O
uncut O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
shelf O
" O
, O
" O
pantry O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
sofa O
" O
, O
" O
livingroom O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
stove O
" O
, O
" O
kitchen O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
table O
" O
, O
" O
kitchen O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
toilet O
" O
, O
" O
bathroom O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
white O
onion O
" O
, O
" O
fridge O
" O
, O
" O
in O
" O
] O
, O
[ O
" O
white O
onion O
" O
, O
" O
raw O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
white O
onion O
" O
, O
" O
uncut O
" O
, O
" O
is O
" O
] O
" O
get O
block O
of O
cheese O
" O
, O
" O
get O
flour O
" O
, O
" O
get O
pork O
chop O
" O
, O
" O
chop O
block O
of O
cheese O
" O
, O
" O
chop O
flour O
" O
, O
" O
chop O
pork O
chop O
" O
, O
" O
dice O
block O
of O
cheese O
" O
, O
" O
dice O
flour O
" O
, O
" O
dice O
pork O
chop O
" O
, O
" O
fry O
block O
of O
cheese O
" O
, O
" O
fry O
flour O
" O
, O
" O
fry O
pork O
chop O
" O
, O
" O
get O
knife O
" O
, O
" O
grill O
block O
of O
cheese O
" O
, O
" O
grill O
flour O
" O
, O
" O
grill O
pork O
chop O
" O
, O
" O
make O
meal O
" O
, O
" O
roast O
block O
of O
cheese O
" O
, O
" O
roast O
flour O
" O
, O
" O
roast O
pork O
chop O
" O
, O
" O
slice O
block O
of O
cheese O
" O
, O
" O
slice O
flour O
" O
, O
" O
slice O
pork O
chop O
" O
" O
go O
east O
" O
, O
" O
open O
fridge O
" O
, O
" O
open O
frosted O
- O
glass O
door O
" O
, O
" O
take O
cookbook O
from O
counter O
" O
, O
" O
take O
knife O
from O
counter O
" O
, O
" O
take O
purple O
potato O
from O
counter O
" O
, O
" O
take O
red O
apple O
from O
counter O
" O
, O
" O
take O
red O
potato O
from O
counter O
" O
Table O
5 O
: O
The O
observations O
o O
t O
, O
subtask O
candidates O
T O
and O
action O
candidates O
A O
of O
a O
hard O
game O
. O

The O
underlined O
subtask O
candidates O
denote O
the O
available O
subtask O
set O
T O
t O
. O

The O
underlined O
action O
candidates O
denote O
the O
refined O
action O
set O
A O
t O
after O
selecting O
the O
subtask O
" O
roast O
carrot" O
. O

Subtask O
candidates O
Action O
candidates O
Hard O
[ O
" O
backyard O
" O
, O
" O
garden O
" O
, O
" O
west_of O
" O
] O
, O
[ O
" O
barn O
door O
" O
, O
" O
backyard O
" O
, O
" O
west_of O
" O
] O
, O
[ O
" O
barn O
door O
" O
, O
" O
closed O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
barn O
door O
" O
, O
" O
shed O
" O
, O
" O
east_of O
" O
] O
, O
[ O
" O
bathroom O
" O
, O
" O
corridor O
" O
, O
" O
east_of O
" O
] O
, O
[ O
" O
bbq O
" O
, O
" O
backyard O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
bed O
" O
, O
" O
bedroom O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
bedroom O
" O
, O
" O
corridor O
" O
, O
" O
north_of O
" O
] O
, O
[ O
" O
bedroom O
" O
, O
" O
livingroom O
" O
, O
" O
south_of O
" O
] O
, O
[ O
" O
carrot O
" O
, O
" O
cookbook O
" O
, O
" O
part_of O
" O
] O
, O
[ O
" O
carrot O
" O
, O
" O
player O
" O
, O
" O
in O
" O
] O
, O
[ O
" O
carrot O
" O
, O
" O
raw O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
carrot O
" O
, O
" O
roasted O
" O
, O
" O
needs O
" O
] O
, O
[ O
" O
carrot O
" O
, O
" O
sliced O
" O
, O
" O
needs"],["carrot O
" O
, O
" O
uncut O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
commercial O
glass O
door O
" O
, O
" O
closed O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
commercial O
glass O
door O
" O
, O
" O
street O
" O
, O
" O
east_of O
" O
] O
, O
[ O
" O
commercial O
glass O
door O
" O
, O
" O
supermarket O
" O
, O
" O
west_of O
" O
] O
, O
[ O
" O
cookbook O
" O
, O
" O
table O
" O
, O
" O
on O
" O
] O
, O
[ O
" O
corridor O
" O
, O
" O
bathroom O
" O
, O
" O
west_of O
" O
] O
, O
[ O
" O
corridor O
" O
, O
" O
bedroom O
" O
, O
" O
south_of O
" O
] O
, O
[ O
" O
counter O
" O
, O
" O
kitchen O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
driveway O
" O
, O
" O
street O
" O
, O
" O
north_of O
" O
] O
, O
[ O
" O
fridge O
" O
, O
" O
closed O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
fridge O
" O
, O
" O
kitchen O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
front O
door O
" O
, O
" O
closed O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
front O
door O
" O
, O
" O
driveway O
" O
, O
" O
west_of O
" O
] O
, O
[ O
" O
front O
door O
" O
, O
" O
livingroom O
" O
, O
" O
east_of O
" O
] O
, O
[ O
" O
frosted O
- O
glass O
door O
" O
, O
" O
closed O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
frostedglass O
door O
" O
, O
" O
kitchen O
" O
, O
" O
south_of O
" O
] O
, O
[ O
" O
frosted O
- O
glass O
door O
" O
, O
" O
pantry O
" O
, O
" O
north_of O
" O
] O
, O
[ O
" O
garden O
" O
, O
" O
backyard O
" O
, O
" O
east_of O
" O
] O
, O
[ O
" O
kitchen O
" O
, O
" O
livingroom O
" O
, O
" O
west_of O
" O
] O
, O
[ O
" O
knife O
" O
, O
" O
counter O
" O
, O
" O
on O
" O
] O
, O
[ O
" O
livingroom O
" O
, O
" O
bedroom O
" O
, O
" O
north_of O
" O
] O
, O
[ O
" O
livingroom O
" O
, O
" O
kitchen O
" O
, O
" O
east_of O
" O
] O
, O
[ O
" O
oven O
" O
, O
" O
kitchen O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
patio O
chair O
" O
, O
" O
backyard O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
patio O
door O
" O
, O
" O
backyard O
" O
, O
" O
north_of O
" O
] O
, O
[ O
" O
patio O
door O
" O
, O
" O
corridor O
" O
, O
" O
south_of O
" O
] O
, O
[ O
" O
patio O
door O
" O
, O
" O
open O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
patio O
table O
" O
, O
" O
backyard O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
player O
" O
, O
" O
backyard O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
red O
apple O
" O
, O
" O
counter O
" O
, O
" O
on O
" O
] O
, O
[ O
" O
red O
apple O
" O
, O
" O
raw O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
red O
apple O
" O
, O
" O
uncut O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
red O
hot O
pepper O
" O
, O
" O
cookbook O
" O
, O
" O
part_of O
" O
] O
, O
[ O
" O
red O
hot O
pepper O
" O
, O
" O
player O
" O
, O
" O
in O
" O
] O
, O
[ O
" O
red O
hot O
pepper O
" O
, O
" O
raw O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
red O
hot O
pepper O
" O
, O
" O
roasted O
" O
, O
" O
needs O
" O
] O
, O
[ O
" O
red O
hot O
pepper O
" O
, O
" O
sliced O
" O
, O
" O
needs O
" O
] O
, O
[ O
" O
red O
hot O
pepper O
" O
, O
" O
uncut O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
red O
onion O
" O
, O
" O
garden O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
red O
onion O
" O
, O
" O
raw O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
red O
onion O
" O
, O
" O
uncut O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
shelf O
" O
, O
" O
pantry O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
showcase O
" O
, O
" O
supermarket O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
sofa O
" O
, O
" O
livingroom O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
stove O
" O
, O
" O
kitchen O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
street O
" O
, O
" O
driveway O
" O
, O
" O
south_of O
" O
] O
, O
[ O
" O
table O
" O
, O
" O
kitchen O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
toilet O
" O
, O
" O
bathroom O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
toolbox O
" O
, O
" O
closed O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
toolbox O
" O
, O
" O
shed O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
white O
onion O
" O
, O
" O
chopped O
" O
, O
" O
needs O
" O
] O
, O
[ O
" O
white O
onion O
" O
, O
" O
cookbook O
" O
, O
" O
part_of O
" O
] O
, O
[ O
" O
white O
onion O
" O
, O
" O
grilled O
" O
, O
" O
needs O
" O
] O
, O
[ O
" O
white O
onion O
" O
, O
" O
player O
" O
, O
" O
in O
" O
] O
, O
[ O
" O
white O
onion O
" O
, O
" O
raw O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
white O
onion O
" O
, O
" O
uncut O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
workbench O
" O
, O
" O
shed O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
yellow O
bell O
pepper O
" O
, O
" O
garden O
" O
, O
" O
at O
" O
] O
, O
[ O
" O
yellow O
bell O
pepper O
" O
, O
" O
raw O
" O
, O
" O
is O
" O
] O
, O
[ O
" O
yellow O
bell O
pepper O
" O
, O
" O
uncut O
" O
, O
" O
is O
" O
] O
" O
roast O
carrot O
" O
, O
" O
roast O
red O
hot O
pepper O
" O
, O
" O
grill O
white O
onion O
" O
, O
" O
get O
knife O
" O
, O
" O
chop O
carrot O
" O
, O
" O
chop O
red O
hot O
pepper O
" O
, O
" O
chop O
white O
onion O
" O
, O
" O
dice O
carrot O
" O
, O
" O
dice O
red O
hot O
pepper O
" O
, O
" O
dice O
white O
onion O
" O
, O
" O
fry O
carrot O
" O
, O
" O
fry O
red O
hot O
pepper O
" O
, O
" O
fry O
white O
onion O
" O
, O
" O
get O
carrot O
" O
, O
" O
get O
red O
hot O
pepper O
" O
, O
" O
get O
white O
onion O
" O
, O
" O
grill O
carrot O
" O
, O
" O
grill O
red O
hot O
pepper O
" O
, O
" O
make O
meal O
" O
, O
" O
roast O
white O
onion O
" O
, O
" O
slice O
carrot O
" O
, O
" O
slice O
red O
hot O
pepper O
" O
, O
" O
slice O
white O
onion O
" O
" O
go O
east O
" O
, O
" O
go O
north O
" O
, O
" O
open O
barn O
door O
" O
, O
" O
open O
patio O
door O
" O
, O
" O
close O
patio O
door O
" O
, O
" O
cook O
carrot O
with O
bbq O
" O
, O
" O
cook O
red O
hot O
pepper O
with O
bbq O
" O
, O
" O
cook O
white O
onion O
with O
bbq O
" O
, O
" O
drop O
carrot O
" O
, O
" O
drop O
red O
hot O
pepper O
" O
, O
" O
drop O
white O
onion O
" O
, O
" O
eat O
carrot O
" O
, O
" O
eat O
red O
hot O
pepper O
" O
, O
" O
eat O
white O
onion O
" O
, O
" O
put O
carrot O
on O
patio O
chair O
" O
, O
" O
put O
carrot O
on O
patio O
table O
" O
, O
" O
put O
red O
hot O
pepper O
on O
patio O
chair O
" O
, O
" O
put O
red O
hot O
pepper O
on O
patio O
table O
" O
, O
" O
put O
white O
onion O
on O
patio O
chair O
" O
, O
" O
put O
white O
onion O
on O
patio O
table O
" O
C O
Baseline O
details O
C.1 O
GATA O
Fig O
. O

10 O
shows O
our O
backbone O
model O
GATA B-MethodName
, O
which O
consists O
of O
an O
observation O
encoder O
, O
an O
action O
encoder O
and O
an O
action O
scorer O
. O

The O
observation O
encoder O
is O
a O
graph O
encoder O
for O
encoding O
the O
KG O
- O
based O
observation O
o O
t O
, O
and O
the O
action O
encoder O
is O
a O
text O
encoder O
to O
encode O
the O
action O
set O
A O
as O
a O
stack O
of O
action O
candidate O
representations O
. O

The O
observation O
representation O
will O
be O
paired O
with O
each O
action O
candidate O
, O
and O
then O
fed O
into O
the O
action O
scorer O
, O
which O
consists O
of O
linear O
layers O
. O

6 O
Results O
and O
discussionsFig O
. O

4 O
shows O
the O
RL O
testing O
performance O
with O
respect O
to O
the O
training O
episodes O
. O

saving O
80 O
% O
of O
the O
online O
interaction O
data O
in O
complex O
games O
. O

The O
effectiveness O
of O
pre O
- O
training O
can O
also O
be O
observed O
from O
the O
variant O
" O
IL B-MethodName
w/o I-MethodName
FT I-MethodName
" O
: O
even O
though O
it O
requires O
no O
further O
training O
on O
the O
medium O
/ O
hard O
games O
, O
it O
achieves O
comparable O
performance O
to O
our O
model O
. O

However O
, O
the O
performance O
of O
QWA B-MethodName
can O
be O
further O
improved O
through O
RL O
, O
while O
it O
does O
not O
work O
for O
the O
IL B-MethodName
- O
based O
model O
, O
as O
we O
can O
observe O
the O
performance O
of O
" O
IL B-MethodName
" O
becomes O
unstable O
and O
drops O
significantly O
during O
the O
RL O
fine O
- O
tuning O
. O

) O
could O
be O
found O
at O
Appendix O
B. O
We O
train O
the O
task O
selector O
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
256 B-HyperparameterValue
, O
and O
the O
action O
( O
Adhikari O
et O
al O
. O
, O
2020 O
) O
to O
conduct O
reinforcement O
learning O
. O

We O
set O
the O
step B-HyperparameterName
limit I-HyperparameterName
of I-HyperparameterName
an I-HyperparameterName
episode I-HyperparameterName
as O
50 B-HyperparameterValue
for O
training O
and O
100 B-HyperparameterValue
for O
validation O
/ O
testing O
. O

We O
set O
the O
subtask B-HyperparameterName
time I-HyperparameterName
limit I-HyperparameterName
ξ B-HyperparameterName
= O
5 B-HyperparameterValue
. O

For O
each O
episode O
, O
we O
sample O
a O
game O
from O
the O
training O
set O
to O
interact O
with O
. O

We O
train O
the O
models O
for O
100,000 B-HyperparameterValue
episodes B-HyperparameterName
. O

The O
models O
are O
optimized O
via O
Double B-HyperparameterValue
DQN I-HyperparameterValue
( O
epsilon B-HyperparameterName
decays O
from O
1.0 B-HyperparameterValue
to O
0.1 B-HyperparameterValue
in O
20,000 B-HyperparameterValue
episodes B-HyperparameterName
, O
Adam B-HyperparameterValue
optimizer B-HyperparameterName
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
0.001 B-HyperparameterValue
) O
with O
Pritorized O
Experience O
Replay O
( O
replay B-HyperparameterName
buffer I-HyperparameterName
size I-HyperparameterName
500,000 B-HyperparameterValue
) O
. O

For O
every O
1,000 O
training O
episodes O
, O
we O
validate O
the O
model O
and O
report O
the O
testing O
performance O
. O

We O
measure O
the O
models O
through O
their O
RL O
testing O
performance O
. O

We O
denote O
a O
game B-MetricName
's I-MetricName
score I-MetricName
as O
the O
episodic O
sum O
of O
rewards O
without O
discount O
. O

As O
different O
games O
may O
have O
different O
maximum O
available O
scores O
, O
we O
report O
the O
normalized B-MetricName
score I-MetricName
, O
which O
is O
defined O
as O
the O
collected O
score O
normalized O
by O
the O
maximum O
score O
for O
a O
game O
. O

• O
QWA B-MethodName
: O
the O
proposed O
model O
with O
worldperceiving O
modules O
. O

Model O
architecture O
All O
models O
are O
implemented O
based O
on O
GATA B-MethodName
's O
released O
code O
¶ O
. O

In O
particular O
, O
we O
use O
the O
version O
GATA B-MethodName
- I-MethodName
GTF I-MethodName
, O
which O
takes O
only O
the O
KG O
- O
based O
observation O
, O
and O
denote O
it O
as O
GATA B-MethodName
for O
simplicity O
. O

The O
observation B-HyperparameterName
encoder I-HyperparameterName
is O
implemented O
based O
on O
the O
Relational B-HyperparameterValue
Graph I-HyperparameterValue
Convolutional I-HyperparameterValue
Networks I-HyperparameterValue
( O
R B-HyperparameterValue
- I-HyperparameterValue
GCNs I-HyperparameterValue
) O
( O
Schlichtkrull O
et O
al O
. O
, O
2018 O
) O
by O
taking O
into O
account O
both O
nodes O
and O
edges O
. O

Both O
the O
task O
encoder O
and O
the O
action O
encoder O
are O
implemented O
based O
on O
a O
single O
transformer O
block O
with O
single O
head O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
to O
encode O
short O
texts O
. O

The O
binary B-HyperparameterName
classifier I-HyperparameterName
, O
the O
task B-HyperparameterName
scorer I-HyperparameterName
and O
the O
action B-HyperparameterName
scorer I-HyperparameterName
are O
linear B-HyperparameterValue
layers I-HyperparameterValue
. O

The O
GATA B-MethodName
and O
IL B-MethodName
models O
are O
equipped O
with O
similar O
modules O
. O

Please O
refer O
to O
Appendix O
C O
for O
details O
. O

We O
train O
the O
task O
selector O
and O
the O
action O
validator O
separately O
, O
as O
they O
use O
different O
types O
of O
QAs O
. O

We O
ask O
human O
players O
to O
play O
the O
simple O
games O
, O
and O
answer O
the O
yes O
- O
or O
- O
no O
questions O
based O
on O
the O
observations O
. O

The O
details O
of O
the O
dataset O
construction O
( O
interaction O
data O
collection O
, O
question O
generation O
, O
answer O
annotation O
, O
etc O
. O

• O
IL B-MethodName
w/o I-MethodName
FT I-MethodName
: O
a O
variant O
of O
the O
IL B-MethodName
baseline O
, O
where O
only O
the O
imitation O
pre O
- O
training O
phase O
is O
conducted O
, O
and O
there O
's O
no O
RL O
fine O
- O
tuning O
. O

• O
IL B-MethodName
: O
a O
hierarchical O
agent O
which O
also O
uses O
two O
training O
phases O
. O

In O
the O
first O
phase O
, O
both O
the O
task O
selector O
and O
the O
action O
selector O
are O
pre O
- O
trained O
through O
imitation O
learning O
. O

Then O
in O
the O
second O
phase O
, O
the O
action O
selector O
is O
fine O
- O
tuned O
through O
reinforcement O
learning O
. O

One O
issue O
we O
are O
concerned O
about O
is O
the O
compound O
error O
− O
the O
prediction O
error O
from O
imperfect O
pre O
- O
trained O
modules O
will O
adversely O
affect O
RL O
training O
( O
Talvitie O
, O
2014;Racanière O
et O
al O
. O
, O
2017 O
) O
. O

For O
example O
, O
the O
false O
predictions O
made O
by O
the O
binary O
classifier O
in O
the O
task O
selector O
may O
lead O
to O
a O
wrong O
T O
t O
, O
which O
affects O
A O
t O
and O
a O
t O
in O
turn O
. O

To O
alleviate O
the O
influence O
of O
the O
compound O
error O
, O
we O
assign O
time O
- O
awareness O
to O
subtasks O
. O

A O
subtask O
is O
bounded O
by O
a O
time B-HyperparameterName
limit I-HyperparameterName
[ O
0 O
, O
ξ B-HyperparameterName
] O
. O

If O
the O
current O
subtask O
T O
is O
not O
finished O
within O
its O
time O
limit O
, O
we O
force O
the O
agent O
to O
re O
- O
select O
a O
new O
subtask O
T O
t O
∈ O
T O
t O
\ O
{ O
T O
} O
, O
regardless O
whether O
T O
is O
still O
available O
. O

Besides O
making O
the O
agent O
robust O
against O
errors O
, O
another O
benefit O
by O
introducing O
time O
- O
awareness O
to O
subtasks O
is O
that O
it O
improves O
the O
subtask O
selection O
diversity O
, O
which O
helps O
the O
agent O
to O
avoid O
getting O
stuck O
in O
local O
minima O
( O
Pong O
et O
al O
. O
, O
2020;Campero O
et O
al O
. O
, O
2020).We O
conduct O
experiments O
on O
cooking B-TaskName
games I-TaskName
provided O
by O
the O
rl.0.2 B-DatasetName
game I-DatasetName
set I-DatasetName
† O
and O
the O
FTWP B-DatasetName
game I-DatasetName
set I-DatasetName
‡ O
, O
which O
share O
the O
vocabulary O
set O
. O

Based O
on O
the O
number B-HyperparameterName
of I-HyperparameterName
subtasks I-HyperparameterName
, O
which O
is O
highly O
correlated O
to O
the O
number O
of O
ingredients O
& O
preparing O
requirements O
, O
we O
design O
three B-HyperparameterValue
game B-HyperparameterName
sets I-HyperparameterName
with O
varying O
complexities O
: O
3488 B-HyperparameterValue
simple B-HyperparameterName
games I-HyperparameterName
, O
280 B-HyperparameterValue
medium B-HyperparameterName
games I-HyperparameterName
and O
420 B-HyperparameterValue
hard B-HyperparameterName
games I-HyperparameterName
. O

Note O
that O
there O
is O
no O
overlapping O
games O
between O
the O
simple O
set O
and O
the O
medium O
/ O
hard O
game O
sets O
. O

Table O
1 O
shows O
the O
game O
statistics O
. O

Besides O
" O
Traj O
. O

Length O
" O
, O
which O
denotes O
the O
average O
length O
of O
the O
expert O
demonstrations O
per O
game O
§ O
, O
other O
statistic O
metrics O
are O
averaged O
per O
time O
step O
per O
game O
( O
e.g. O
, O
" O
# O
Subtasks O
" O
and O
" O
# O
Avail O
. O

Subtasks O
" O
denote O
the O
average O
number O
of O
subtask O
candidates O
T O
, O
and O
the O
average O
number O
of O
available O
subtasks O
T O
t O
, O
respectively O
) O
. O

We O
will O
collect O
human O
interaction O
data O
from O
the O
simple O
games O
for O
pre O
- O
training O
. O

We O
regard O
both O
medium O
& O
hard O
games O
as O
complex O
, O
and O
will O
conduct O
reinforcement O
learning O
on O
these O
two O
game O
sets O
without O
labeled O
data O
. O

We O
consider O
the O
following O
four O
models O
, O
and O
compare O
with O
more O
variants O
in O
ablation O
studies O
: O
KG O
- O
based O
RL O
agent O
, O
which O
is O
the O
benchmark O
model O
for O
cooking B-TaskName
games I-TaskName
. O

• O
GATA B-MethodName
( O
Adhikari O

Given O
the O
set O
of O
available O
subtasks O
T O
t O
, O
arbitrary O
strategies O
can O
be O
used O
to O
select O
a O
subtask O
T O
t O
from O
it O
. O

For O
example O
, O
we O
can O
employ O
a O
non O
- O
learnable O
task O
scorer O
to O
obtain O
T O
t O
by O
random O
sampling O
, O
since O
each O
subtask O
T O
∈ O
T O
t O
is O
essential O
for O
accomplishing O
the O
task O
. O

We O
can O
also O
train O
a O
task O
scorer O
via O
a O
metapolicy O
for O
adaptive O
task O
selection O
( O
Xu O
et O
al O
. O
, O
2021).After O
obtaining O
the O
subtask O
T O
t O
, O
we O
conduct O
action O
pruning O
conditioned O
on O
it O
( O
or O
on O
both O
T O
t O
and O
o O
t O
) O
to O
reduce O
the O
action O
space O
, O
tackling O
the O
challenge O
of O
large O
action O
space O
. O

Similar O
to O
the O
task O
selector O
, O
we O
formulate O
action O
pruning O
as O
|A| O
binary O
classifica O
- O
tion O
problems O
, O
and O
devise O
another O
world O
- O
perceiving O
module O
: O
the O
action O
validator O
. O

The O
action O
validator O
is O
designed O
to O
check O
the O
relevance O
of O
each O
action O
candidate O
a O
∈ O
A O
with O
respect O
to O
T O
t O
by O
answering O
questions O
like O
" O
Is O
the O
action O
candidate O
' O
take O
beef O
' O
relevant O
to O
the O
subtask O
' O
fry O
chicken O
' O
? O
" O
, O
so O
as O
to O
obtain O
a O
subset O
of O
actions O
A O
t O
⊆ O
A O
with O
irrelevant O
actions O
filtered O
. O

Fig O
. O

3 O
shows O
the O
module O
architecture O
. O

Similar O
to O
the O
task O
selector O
, O
we O
pre O
- O
train O
this O
module O
through O
question O
answering O
. O

Sample O
QAs O
have O
been O
shown O
in O
Fig O
. O

1 O
( O
b).After O
pre O
- O
training O
, O
we O
deploy O
the O
agent O
in O
the O
complex O
games O
, O
and O
train O
the O
action O
selector O
through O
RL O
. O

We O
freeze O
the O
pre O
- O
trained O
modules O
, O
as O
no O
human O
labeled O
data O
will O
be O
obtained O
in O
this O
phase O
. O

At O
each O
time O
step O
, O
we O
use O
the O
task O
selector O
and O
the O
action O
validator O
to O
produce O
T O
t O
and O
A O
t O
, O
respectively O
. O

We O
keep O
using O
the O
same O
subtask O
T O
over O
time O
until O
it O
is O
not O
included O
in O
T O
t O
, O
as O
we O
do O
not O
want O
the O
agent O
to O
switch O
subtasks O
too O
frequently O
. O

The O
agent O
can O
simply O
treat O
T O
t O
as O
the O
additional O
observation O
of O
o O
t O
. O

If O
we O
do O
not O
limit O
the O
use O
of O
human O
knowledge O
in O
this O
phase O
, O
we O
can O
also O
treat O
T O
t O
as O
a O
goal O
with O
either O
hand O
- O
crafted O
( O
Jiang O
et O
al O
. O
, O
2019 O
) O
or O
learnt O
reward O
function O
( O
Colas O
et O
al O
. O
, O
2020 O
) O
. O

Arbitrary O
methods O
can O
be O
used O
for O
optimizing O
Adhikari O
et O
al O
. O
, O
2020 O
) O
. O

We O
formulate O
the O
mapping O
f O
( O
o O
t O
, O
T O
) O
→ O
T O
t O
as O
a O
multi O
- O
label O
learning O
problem O
( O
Zhang O
and O
Zhou O
, O
2013 O
) O
. O

For O
simplicity O
, O
we O
assume O
that O
the O
subtask O
candidates O
are O
independent O
with O
each O
other O
. O

Thus O
, O
the O
multi O
- O
label O
learning O
problem O
can O
be O
decomposed O
as O
|T O
| O
binary O
classification O
problems O
. O

Inspired O
by O
the O
recent O
progress O
of O
questionconditional O
probing O
( O
Das O
et O
al O
. O
, O
2020 O
) O
, O
language O
grounding O
( O
Hill O
et O
al O
. O
, O
2021 O
) O
, O
and O
QA O
- O
based O
graph O
construction O
, O
we O
cast O
these O
binary O
classification O
problems O
as O
yes O
- O
orno O
questions O
, O
making O
the O
task O
selector O
a O
worldperceiving O
module O
. O

For O
example O
, O
the O
corresponding O
question O
for O
the O
subtask O
candidate O
" O
get O
apple O
" O
could O
be O
" O
Whether O
' O
get O
apple O
' O
is O
an O
available O
subtask O
? O
" O
. O

This O
module O
can O
guide O
the O
agent O
to O
under- O
stand O
the O
environment O
conditions O
through O
answering O
questions O
, O
but O
will O
not O
directly O
lead O
the O
agent O
to O
a O
specific O
decision O
. O

We O
can O
obtain O
this O
module O
through O
supervised O
pre O
- O
training O
, O
and O
decouple O
it O
from O
reinforcement O
learning O
to O
yield O
better O
sample O
efficiency O
. O

Fig O
. O

1 O
( O
b O
) O
shows O
some O
sample O
QAs O
, O
where O
a O
human O
answerer O
can O
be O
replaced O
by O
a O
pretrained O
task O
selector O
. O

Some O
previous O
work O
also O
considered O
task O
decomposition O
Hu O
et O
al O
. O
, O
2019 O
) O
, O
but O
the O
related O
module O
is O
obtained O
through O
imitating O
human O
demonstrations O
, O
which O
is O
directly O
related O
to O
decision O
making O
instead O
of O
world O
perceiving O
. O

Compared O
with O
these O
work O
, O
our O
method O
has O
two O
folds O
of O
benefits O
. O

First O
, O
there O
may O
exist O
multiple O
available O
subtasks O
at O
a O
timestep O
. O

Imitating O
human O
demonstrations O
will O
specify O
only O
one O
of O
them O
, O
which O
may O
be O
insufficient O
and O
lead O
to O
information O
loss O
. O

Second O
, O
we O
do O
not O
require O
expert O
demonstrations O
which O
guarantee O
to O
solve O
the O
game O
. O

Instead O
, O
we O
can O
ask O
humans O
to O
annotate O
either O
imperfect O
demonstrations O
, O
or O
even O
demonstrations O
from O
a O
random O
agent O
. O

We O
will O
treat O
the O
IL O
- O
based O
method O
as O
a O
baseline O
and O
conduct O
comparisons O
in O
the O
experiments O
. O

In O
the O
reinforcement O
learning O
phase O
, O
we O
freeze O
the O
pre O
- O
trained O
modules O
, O
and O
train O
the O
action O
selector O
in O
the O
complex O
games O
through O
reinforcement O
learning O
. O

Depending O
on O
the O
experiment O
settings O
, O
T B-HyperparameterName
and O
A B-HyperparameterName
can O
be O
either O
fixed O
vocabulary O
sets O
( O
parser B-HyperparameterValue
- I-HyperparameterValue
based I-HyperparameterValue
) O
, O
or O
changing O
over O
time O
( O
choice B-HyperparameterValue
- I-HyperparameterValue
based I-HyperparameterValue
) O
. O

We O
regard O
a O
subtask O
available O
if O
it O
is O
essential O
for O
solving O
the O
" O
global O
" O
task O
, O
and O
there O
's O
no O
prerequisite O
subtask O
. O

For O
example O
, O
the O
subtask O
" O
get O
apple O
" O
in O
Fig O
. O

1 O
, O
as O
the O
object O
" O
apple O
" O
is O
an O
ingredient O
which O
has O
not O
been O
collected O
. O

Although O
another O
subtask O
" O
dice O
apple O
" O
is O
also O
essential O
for O
making O
the O
meal O
, O
it O
is O
not O
available O
since O
there O
exists O
a O
prerequisite O
subtask O
( O
i.e. O
, O
you O
should O
collect O
the O
apple O
before O
dicing O
it O
) O
. O

The O
aim O
of O
the O
task O
selector O
is O
to O
identify O
a O
subset O
of O
available O
subtasks O
T O
t O
⊆ O
T O
, O
and O
then O
select O
one O
subtask O
T O
t O
∈ O
T O
t O
. O

The O
training O
of O
the O
world O
- O
perceiving O
modules O
can O
be O
regarded O
as O
the O
language O
learning O
regime O
, O
while O
the O
training O
of O
the O
action O
selector O
can O
be O
regarded O
as O
the O
decision O
making O
regime O
. O

We O
consider O
a O
two O
- O
phase O
training O
strategy O
to O
decouple O
these O
two O
regimes O
to O
further O
improve O
the O
sample O
efficiency O
( O
Hill O
et O
al O
. O
, O
2021 O
) O
. O

In O
the O
pre O
- O
training O
phase O
, O
we O
collect O
human O
interaction O
data O
from O
the O
simple O
games O
, O
and O
design O
QA B-TaskName
datasets O
to O
train O
the O
worldperceiving O
modules O
through O
supervised O
learning O
. O

Problem O
setting O
We O
aim O
to O
design O
an O
RL O
- O
based O
agent O
that O
is O
able O
to O
conduct O
automatic O
task O
decomposition O
and O
action O
pruning O
in O
solving O
text B-TaskName
- I-TaskName
based I-TaskName
games I-TaskName
. O

We O
consider O
games O
sharing O
similar O
themes O
and O
tasks O
, O
but O
varying O
in O
their O
complexities O
( O
Adhikari O
et O
al O
. O
, O
2020 O
; O
. O

Taking O
the O
cooking B-TaskName
games I-TaskName
as O
an O
example O
, O
the O
task O
is O
always O
" O
make O
the O
meal O
" O
. O

To O
accomplish O
this O
task O
, O
the O
agent O
has O
to O
explore O
different O
rooms O
to O
collect O
all O
ingredients O
, O
prepare O
them O
in O
right O
ways O
, O
and O
make O
the O
meal O
. O

A O
game O
's O
complexity O
depends O
on O
the O
number O
of O
rooms O
, O
ingredients O
, O
and O
the O
required O
preparation O
steps O
. O

We O
define O
a O
subtask O
as O
a O
milestone O
towards O
completing O
the O
task O
( O
e.g. O
, O
" O
get O
apple O
" O
if O
" O
apple O
" O
is O
included O
in O
the O
recipe O
) O
, O
and O
a O
subtask O
requires O
a O
sequence O
of O
actions O
to O
accomplish O
( O
e.g. O
, O
the O
agent O
has O
to O
explore O
the O
house O
to O
find O
the O
apple O
) O
. O

A O
game O
is O
considered O
simple O
, O
if O
it O
consists O
of O
only O
a O
few O
subtasks O
, O
and O
complex O
if O
it O
consists O
of O
more O
subtasks O
. O

Fig O
. O

2 O
gives O
examples O
of O
simple O
games O
and O
complex O
games O
. O

While O
being O
closer O
to O
real O
world O
applications O
, O
complex O
games O
are O
hard O
to O
solve O
by O
RL O
agents O
because O
: O
1 O
) O
it O
's O
expensive O
to O
collect O
sufficient O
human O
labeled O
data O
for O
pre O
- O
training O
; O
2 O
) O
it O
's O
unrealistic O
to O
train O
an O
RL O
agent O
from O
scratch O
. O

We O
therefore O
focus O
on O
agent O
's O
sample O
efficiency O
and O
performance O
on O
complex O
games O
. O

Our O
objective O
is O
to O
leverage O
the O
labeled O
data O
collected O
from O
simple O
games O
to O
speed O
up O
RL O
training O
in O
complex O
games O
, O
thus O
obtaining O
an O
agent O
capable O
of O
complex O
games O
. O

For O
more O
details O
and O
statistics O
of O
the O
simple O
/ O
complex O
games O
used O
in O
our O
work O
, O
please O
refer O
to O
Sec O
. O

5.1.Fig O
. O

3 O
shows O
the O
overview O
of O
our O
QWA B-MethodName
agent O
. O

We O
consider O
two O
world O
- O
perceiving O
modules O
: O
a O
task O
selector O
and O
an O
action O
validator O
. O

Given O
the O
observation O
o O
t O
and O
the O
task O
candidate O
set O
T O
, O
we O
use O
the O
task O
selector O
to O
first O
obtain O
a O
subset O
of O
currently O
available O
subtasks O
T O
t O
⊆ O
T O
, O
then O
select O
a O
subtask O
T O
t O
∈ O
T O
t O
. O

Given O
T O
t O
and O
the O
action O
candidate O
set O
A O
, O
we O
use O
the O
action O
validator O
to O
get O
an O
action O
subset O
A O
t O
⊆ O
A O
, O
which O
contains O
only O
those O
relevant O
to O
the O
subtask O
T O
t O
. O

Finally O
, O
given O
o O
t O
and O
T O
t O
, O
we O
use O
an O
action O
selector O
to O
score O
each O
action O
a O
∈ O
A O
t O
, O
and O
the O
action O
with O
the O
highest O
score O
will O
be O
selected O
as O
a O
t O
. O

Observation O
form O
In O
text B-TaskName
- I-TaskName
based I-TaskName
games I-TaskName
, O
the O
observation O
can O
be O
in O
the O
form O
of O
text O
, O
knowledge O
graph O
, O
or O
hybrid O
. O

Fig O
. O

1 O
( O
a O
) O
shows O
an O
example O
of O
the O
textual O
observation O
and O
the O
corresponding O
KG O
- O
based O
observation O
. O

We O
do O
not O
make O
assumptions O
about O
the O
observation O
form O
and O
our O
method O
is O
compatible O
with O
any O
of O
those O
forms O
. O

R O
t O
= O
E O
[ O
∞ O
t=0 O
γ B-HyperparameterName
k O
r O
t O
] O
. O

3 O
Background O
POMDP O
Text B-TaskName
- I-TaskName
based I-TaskName
games I-TaskName
can O
be O
formulated O
as O
a O
Partially O
Observable O
Markov O
Decision O
Processes O
( O
POMDPs O
) O
. O

A O
POMDP O
can O
be O
described O
by O
a O
tuple O
G O
= O
⟨S O
, O
A O
, O
P O
, O
r O
, O
Ω O
, O
O O
, O
γ⟩ O
, O
with O
S O
representing O
the O
state O
set O
, O
A O
the O
action O
set O
, O
P O
( O
s O
′ O
|s O
, O
a O
) O
: O
S O
× O
A O
× O
S O
→ O
R O
+ O
the O
state O
transition O
probabilities O
, O
r(s O
, O
a O
) O
: O
S O
× O
A O
→ O
R O
the O
reward O
function O
, O
Ω O
the O
observation O
set O
, O
O O
the O
conditional O
observation O
probabilities O
, O
and O
γ B-HyperparameterName
∈ O
( O
0 B-HyperparameterValue
, O
1 B-HyperparameterValue
] O
the O
discount B-HyperparameterName
factor I-HyperparameterName
. O

At O
each O
time O
step O
, O
the O
agent O
receives O
an O
observation O
o O
t O
∈ O
Ω O
based O
on O
the O
probability O
O(o O
t O
|s O
t O
, O
a O
t−1 O
) O
, O
and O
select O
an O
action O
a O
t O
∈ O
A. O
The O
environment O
will O
transit O
into O
a O
new O
state O
based O
on O
the O
probability O
T O
( O
s O
t+1 O
|s O
t O
, O
a O
t O
) O
, O
and O
return O
a O
scalar O
reward O
r O
t+1 O
. O

The O
goal O
of O
the O
agent O
is O
to O
select O
the O
action O
to O
maximize O
the O
expected O
cumulative O
discounted O
rewards O
: O

In O
the O
domain O
of O
text B-TaskName
- I-TaskName
based I-TaskName
games I-TaskName
, O
some O
prior O
works O
have O
involved O
pre O
- O
training O
tasks O
such O
as O
state O
representation O
learning O
( O
Ammanabrolu O
et O
al O
. O
, O
2021;Singh O
et O
al O
. O
, O
2021 O
) O
, O
knowledge O
graph O
constructing O
( O
Murugesan O
et O
al O
. O
, O
2021 O
) O
and O
action O
pruning O
( O
Hausknecht O
et O
al O
. O
, O
2019;Yao O
et O
al O
. O
, O
2020 O
) O
. O

For O
example O
, O
designed O
a O
module O
to O
extract O
triplets O
from O
the O
textual O
observation O
by O
answering O
questions O
, O
and O
use O
these O
triplets O
to O
update O
the O
knowledge O
graph O
. O

As O
far O
as O
we O
know O
, O
we O
are O
the O
first O
to O
incorporate O
pre O
- O
training O
based O
task O
decompositon O
in O
this O
domain O
. O

Besides O
, O
instead O
of O
directly O
pruning O
the O
actions O
based O
on O
the O
observation O
, O
we O
introduce O
subtask O
- O
conditioned O
action O
pruning O
to O
further O
reduce O
the O
action O
space O
. O

Our O
contributions O
are O
summarized O
as O
follows O
: O
Firstly O
, O
we O
develop O
an O
RL O
agent O
featured O
with O
question O
- O
guided O
task O
decomposition O
and O
action O
space O
reduction O
. O

Secondly O
, O
we O
design O
a O
two O
- O
phase O
framework O
to O
efficiently O
train O
the O
agent O
with O
limited O
data O
. O

Thirdly O
, O
we O
empirically O
validate O
our O
method O
's O
effectiveness O
and O
robustness O
in O
complex O
games O
. O

The O
RL O
agents O
for O
text B-TaskName
- I-TaskName
based I-TaskName
games I-TaskName
can O
be O
divided O
as O
text O
- O
based O
agents O
and O
KG O
- O
based O
agents O
based O
on O
the O
form O
of O
observations O
. O

Compared O
with O
the O
textbased O
agents O
( O
Narasimhan O
et O
al O
. O
, O
2015;Adolphs O
and O
Hofmann O
, O
2020;Jain O
et O
al O
. O
, O
2020;Yin O
and O
May O
, O
2019;Xu O
et O
al O
. O
, O
2020a;Guo O
et O
al O
. O
, O
2020 O
) O
, O
which O
take O
the O
raw O
textual O
observations O
as O
input O
to O
build O
state O
representations O
, O
the O
KGbased O
agents O
construct O
the O
knowledge O
graph O
and O
leverage O
it O
as O
the O
additional O
input O
( O
Ammanabrolu O
and O
Riedl O
, O
2019;Xu O
et O
al O
. O
, O
2020b O
) O
. O

By O
providing O
structural O
and O
historical O
information O
, O
the O
knowledge O
graph O
helps O
the O
agent O
to O
handle O
partial O
observability O
, O
reduce O
action O
space O
, O
and O
improve O
generalizability O
across O
games O
. O

Based O
on O
how O
actions O
are O
selected O
, O
the O
RL O
agents O
can O
also O
be O
divided O
as O
parser O
- O
based O
agents O
, O
choice O
- O
based O
agents O
, O
and O
template O
- O
based O
agents O
. O

The O
parser O
- O
based O
agents O
generate O
actions O
word O
by O
word O
, O
leading O
to O
a O
huge O
combinatorial O
action O
space O
( O
Kohita O
et O
al O
. O
, O
2021 O
) O
. O

The O
choice O
- O
based O
agents O
circumvent O
this O
challenge O
by O
assuming O
the O
access O
to O
a O
set O
of O
admissible O
actions O
at O
each O
game O
state O
( O
He O
et O
al O
. O
, O
2016 O
) O
. O

The O
template O
- O
based O
agents O
achieve O
a O
trade O
- O
off O
between O
the O
huge O
action O
space O
and O
the O
assumption O
of O
admissible O
action O
set O
by O
introducing O
the O
template O
- O
based O
action O
space O
, O
where O
the O
agent O
selects O
first O
a O
template O
, O
and O
then O
a O
verb O
- O
object O
pair O
either O
individually O
or O
conditioned O
on O
the O
selected O
template O
. O

In O
this O
work O
, O
we O
aim O
to O
improve O
the O
sam O
- O
ple O
efficiency O
and O
reduce O
the O
action O
space O
through O
pre O
- O
training O
. O

Being O
agnostic O
about O
the O
form O
of O
observations O
and O
the O
action O
selecting O
methods O
, O
our O
work O
complements O
the O
existing O
RL O
agents O
. O

Our O
work O
is O
closely O
related O
to O
task O
decomposition O
( O
Oh O
et O
al O
. O
, O
2017;Shiarlis O
et O
al O
. O
, O
2018;Sohn O
et O
al O
. O
, O
2018 O
) O
and O
hierarchical O
reinforcement O
learning O
( O
Dayan O
and O
Hinton O
, O
1992;Kulkarni O
et O
al O
. O
, O
2016;Vezhnevets O
et O
al O
. O
, O
2017 O
) O
. O

Similar O
to O
our O
efforts O
, O
Jiang O
et O
al O
. O
( O
2019 O
) O
and O
Xu O
et O
al O
. O
( O
2021 O
) O
designed O
a O
meta O
- O
policy O
for O
task O
decomposition O
and O
subtask O
selection O
, O
and O
a O
sub O
- O
policy O
for O
goal O
- O
conditioned O
decision O
making O
. O

Typically O
, O
these O
works O
either O
assume O
the O
access O
to O
a O
set O
of O
available O
subtasks O
, O
or O
decompose O
a O
task O
through O
pre O
- O
defined O
rules O
, O
while O
we O
aim O
to O
achieve O
automatic O
task O
decomposition O
through O
pre O
- O
training O
, O
and O
remove O
the O
requirement O
for O
expert O
knowledge O
during O
reinforcement O
learning O
. O

Besides O
, O
existing O
work O
assumes O
that O
unlimited O
interaction O
data O
can O
be O
obtained O
to O
train O
the O
whole O
model O
through O
RL O
. O

In O
contrast O
, O
we O
consider O
the O
more O
practical O
situation O
where O
the O
interaction O
data O
is O
limited O
, O
and O
focus O
on O
improving O
the O
RL O
agent O
's O
data O
efficiency O
. O

Regarding O
the O
sub O
- O
policy O
, O
we O
do O
not O
assume O
the O
access O
to O
the O
termination O
states O
of O
the O
subtasks O
. O

We O
also O
do O
not O
require O
additional O
handcrafted O
operations O
in O
reward O
shaping O
( O
Bahdanau O
et O
al O
. O
, O
2019).There O
have O
been O
a O
wide O
range O
of O
work O
studying O
pre O
- O
training O
methods O
or O
incorporating O
pre O
- O
trained O
modules O
to O
facilitate O
reinforcement O
learning O
( O
Eysenbach O
et O
al O
. O
, O
2018;Hansen O
et O
al O
. O
, O
2019;Sharma O
et O
al O
. O
, O
2019;Gehring O
et O
al O
. O
, O
2021;Liu O
et O
al O
. O
, O
2021;Schwarzer O
et O
al O
. O
, O
2021 O
) O
. O

One O
major O
branch O
among O
them O
is O
Imitation O
Learning O
( O
IL O
) O
, O
where O
the O
agent O
is O
trained O
to O
imitate O
human O
demonstrations O
before O
being O
deployed O
in O
RL O
( O
Hester O
et O
al O
. O
, O
2018;Zhu O
et O
al O
. O
, O
2018;Reddy O
et O
al O
. O
, O
2019 O
) O
. O

Although O
we O
also O
collect O
human O
labeled O
data O
for O
pre O
- O
training O
, O
we O
leverage O
the O
data O
to O
help O
the O
agent O
to O
perceive O
the O
environment O
instead O
of O
learning O
the O
solving O
strategies O
. O

Therefore O
, O
we O
do O
not O
require O
the O
demonstrations O
to O
be O
perfect O
to O
solve O
the O
game O
. O

Besides O
, O
our O
method O
prevails O
when O
pre O
- O
trained O
on O
simple O
tasks O
rather O
than O
complicated O
ones O
, O
making O
it O
more O
feasible O
for O
human O
to O
interact O
and O
annotate O
( O
Arumugam O
et O
al O
. O
, O
2017;Mirchandani O
et O
al O
. O
, O
2021 O
) O
. O

Further O
discussions O
to O
compare O
our O
method O
with O
IL O
are O
provided O
in O
subsequent O
sections O
. O

We O
conduct O
experiments O
on O
a O
series O
of O
cooking O
games O
. O

We O
divide O
the O
games O
as O
simple O
games O
and O
complex O
games O
, O
and O
construct O
the O
pre O
- O
training O
dataset O
from O
simple O
games O
only O
. O

The O
experimental O
results O
show O
that O
QWA B-MethodName
achieves O
high O
sample O
efficiency O
in O
solving O
complex O
games O
. O

We O
also O
show O
that O
our O
method O
enjoys O
robustness O
against O
compound O
error O
and O
limited O
pre O
- O
training O
data O
. O

ings O
can O
understand O
the O
environment O
conditions O
through O
question O
answering O
( O
Das O
et O
al O
. O
, O
2020 O
; O
, O
we O
design O
worldperceiving O
modules O
to O
realize O
the O
aforementioned O
functionalities O
( O
i.e. O
, O
task O
decomposition O
and O
action O
pruning O
) O
and O
name O
our O
method O
as O
Question B-MethodName
- I-MethodName
guided I-MethodName
World I-MethodName
- I-MethodName
perceiving I-MethodName
Agent I-MethodName
( O
QWA B-MethodName
) O
* O
. O

Fig O
. O

1 O
( O
b O
) O
shows O
an O
example O
of O
our O
decision O
making O
process O
. O

Being O
guided O
by O
some O
questions O
, O
the O
agent O
first O
decomposes O
the O
task O
to O
obtain O
a O
set O
of O
available O
subtasks O
, O
and O
selects O
one O
from O
them O
. O

Next O
, O
conditioned O
on O
the O
selected O
subtask O
, O
the O
agent O
conducts O
action O
pruning O
to O
obtain O
a O
refined O
set O
of O
actions O
. O

In O
order O
to O
decouple O
language O
learning O
from O
decision O
making O
, O
which O
further O
improves O
the O
sample O
efficiency O
, O
we O
propose O
to O
acquire O
the O
world O
- O
perceiving O
modules O
through O
supervised O
pre O
- O
training O
. O

We O
design O
a O
two O
- O
phase O
framework O
to O
train O
our O
agent O
. O

In O
the O
first O
phase O
, O
a O
dataset O
is O
built O
for O
the O
training O
of O
the O
world O
- O
perceiving O
modules O
. O

In O
the O
second O
phase O
, O
we O
deploy O
the O
agent O
in O
games O
with O
the O
pretrained O
modules O
frozen O
, O
and O
train O
the O
agent O
through O
reinforcement O
learning O
. O

Inspired O
by O
the O
observation O
that O
human O
be- O
The O
decision O
making O
process O
. O

Through O
question O
answering O
, O
the O
agent O
is O
guided O
to O
first O
decompose O
the O
task O
as O
subtasks O
, O
then O
reduce O
the O
action O
space O
conditioned O
on O
the O
subtask O
. O

In O
this O
paper O
, O
we O
aim O
to O
address O
these O
two O
challenges O
for O
reinforcement O
learning O
in O
solving O
textbased B-TaskName
games I-TaskName
. O

Since O
it O
is O
inefficient O
to O
train O
an O
agent O
to O
solve O
complicated O
tasks O
( O
games O
) O
from O
scratch O
, O
we O
consider O
decomposing O
a O
task O
into O
a O
sequence O
of O
subtasks O
as O
inspired O
by O
( O
Andreas O
et O
al O
. O
, O
2017 O
) O
. O

We O
design O
an O
RL O
agent O
that O
is O
capable O
of O
automatic O
task O
decomposition O
and O
subtask O
- O
conditioned O
action O
pruning O
, O
which O
brings O
two O
branches O
of O
benefits O
. O

First O
, O
the O
subtasks O
are O
easier O
to O
solve O
, O
as O
the O
involved O
temporal O
dependencies O
are O
usually O
shortterm O
. O

Second O
, O
by O
acquiring O
the O
skills O
to O
solve O
subtasks O
, O
the O
agent O
will O
be O
able O
to O
learn O
to O
solve O
a O
new O
task O
more O
quickly O
by O
reusing O
the O
learnt O
skills O
( O
Barreto O
et O
al O
. O
, O
2020 O
) O
. O

The O
challenge O
of O
large O
action O
space O
can O
also O
be O
alleviated O
, O
if O
we O
can O
filter O
out O
the O
actions O
that O
are O
irrelevant O
to O
the O
current O
subtask O
. O

Despite O
the O
effectiveness O
, O
there O
are O
two O
major O
challenges O
for O
RL O
- O
based O
agents O
, O
preventing O
them O
from O
being O
deployed O
in O
real O
world O
applications O
: O
the O
low O
sample O
efficiency O
, O
and O
the O
large O
action O
space O
( O
Dulac O
- O
Arnold O
et O
al O
. O
, O
2021 O
) O
. O

The O
low O
sample O
efficiency O
is O
a O
crucial O
limitation O
of O
RL O
which O
refers O
to O
the O
fact O
that O
it O
typically O
requires O
a O
huge O
amount O
of O
data O
to O
train O
an O
agent O
to O
achieve O
human O
- O
level O
performance O
( O
Tsividis O
et O
al O
. O
, O
2017 O
) O
. O

This O
is O
because O
human O
beings O
are O
usually O
armed O
with O
prior O
knowledge O
so O
that O
they O
do O
n't O
have O
to O
learn O
from O
scratch O
( O
Dubey O
et O
al O
. O
, O
2018 O
) O
. O

In O
a O
language O
- O
informed O
RL O
system O
, O
in O
contrast O
, O
the O
agent O
is O
required O
to O
conduct O
both O
language O
learning O
and O
decision O
making O
regimes O
, O
where O
the O
former O
can O
be O
considered O
as O
prior O
knowledge O
and O
is O
much O
slower O
than O
the O
later O
( O
Hill O
et O
al O
. O
, O
2021 O
) O
. O

The O
sample O
efficiency O
could O
be O
improved O
through O
pre O
- O
training O
methods O
, O
which O
decouple O
the O
language O
learning O
from O
decision O
making O
( O
Su O
et O
al O
. O
, O
2017 O
) O
. O

The O
selection O
of O
pre O
- O
training O
methods O
thus O
plays O
an O
important O
role O
: O
if O
the O
pre O
- O
trained O
modules O
perform O
poorly O
on O
unseen O
data O
during O
RL O
training O
, O
the O
incurred O
compound O
error O
will O
severely O
affect O
the O
decision O
making O
process O
. O

Another O
challenge O
is O
the O
large O
discrete O
action O
space O
: O
the O
agent O
may O
waste O
both O
time O
and O
training O
data O
if O
attempting O
irrelevant O
or O
inferior O
actions O
( O
Dulac O
- O
Arnold O
et O
al O
. O
, O
2015;Zahavy O
et O
al O
. O
, O
2018 O
) O
. O

Text B-TaskName
- I-TaskName
based I-TaskName
games I-TaskName
are O
simulated O
environments O
where O
the O
player O
observes O
textual O
descriptions O
, O
and O
acts O
using O
text O
commands O
Urbanek O
et O
al O
. O
, O
2019 O
) O
. O

These O
games O
provide O
a O
safe O
and O
interactive O
way O
to O
study O
natural B-TaskName
language I-TaskName
understanding I-TaskName
, O
commonsense B-TaskName
reasoning I-TaskName
, O
and O
dialogue B-TaskName
systems I-TaskName
. O

Besides O
language O
processing O
techniques O
, O
Reinforcement O
Learning O
has O
become O
a O
quintessential O
methodology O
for O
solving O
text B-TaskName
- I-TaskName
based I-TaskName
games I-TaskName
. O

Some O
RL O
- O
based O
game O
agents O
have O
been O
developed O
recently O
and O
proven O
to O
be O
effective O
in O
handling O
challenges O
such O
as O
language O
representation O
learning O
and O
partial O
observability O
( O
Narasimhan O
et O
al O
. O
, O
2015;Fang O
et O
al O
. O
, O
2017;Ammanabrolu O
and O
Riedl O
, O
2019 O
) O
. O

and O
help O
the O
model O
make O
better O
predictions O
. O

We O
also O
find O
that O
some O
questions O
have O
a O
higher O
probability O
on O
a O
very O
small O
number O
of O
irrelevant O
labels O
. O

However O
, O
as O
the O
number O
of O
labels O
in O
KBs O
is O
large O
, O
having O
a O
high O
probability O
of O
only O
a O
few O
irrelevant O
labels O
will O
not O
greatly O
affect O
the O
results O
of O
the O
entire O
model O
. O

Ground O
- O
truth O
Labels O
The O
present O
research O
was O
supported O
by O
Zhejiang O
Lab O
( O
No O
. O

2022KH0AB01 O
) O
and O
Huawei O
( O
No O
. O

TC20210528011 O
) O
. O

We O
would O
like O
to O
thank O
the O
anonymous O
reviewers O
for O
their O
insightful O
comments O
. O

We O
also O
want O
to O
thank O
MindSpore O
1 O
for O
the O
partial O
suppoort O
of O
this O
work O
, O
which O
is O
a O
new O
deep O
learning O
computing O
framework O
. O

The O
few O
- O
shot O
natural B-TaskName
language I-TaskName
understanding I-TaskName
( O
NLU B-TaskName
) O
task O
has O
attracted O
much O
recent O
attention O
. O

However O
, O
prior O
methods O
have O
been O
evaluated O
under O
a O
disparate O
set O
of O
protocols O
, O
which O
hinders O
fair O
comparison O
and O
measuring O
progress O
of O
the O
field O
. O

To O
address O
this O
issue O
, O
we O
introduce O
an O
evaluation O
framework O
that O
improves O
previous O
evaluation O
procedures O
in O
three O
key O
aspects O
, O
i.e. O
, O
test O
performance O
, O
dev O
- O
test O
correlation O
, O
and O
stability O
. O

Under O
this O
new O
evaluation O
framework O
, O
we O
re O
- O
evaluate O
several O
stateof O
- O
the O
- O
art O
few O
- O
shot O
methods O
for O
NLU B-TaskName
tasks O
. O

Our O
framework O
reveals O
new O
insights O
: O
( O
1 O
) O
both O
the O
absolute O
performance O
and O
relative O
gap O
of O
the O
methods O
were O
not O
accurately O
estimated O
in O
prior O
literature O
; O
( O
2 O
) O
no O
single O
method O
dominates O
most O
tasks O
with O
consistent O
performance O
; O
( O
3 O
) O
improvements O
of O
some O
methods O
diminish O
with O
a O
larger O
pretrained O
model O
; O
and O
( O
4 O
) O
gains O
from O
different O
methods O
are O
often O
complementary O
and O
the O
best O
combined O
model O
performs O
close O
to O
a O
strong O
fully O
- O
supervised O
baseline O
. O

We O
open O
- O
source O
our O
toolkit O
, O
FewNLU B-DatasetName
, O
that O
implements O
our O
evaluation O
framework O
along O
with O
a O
number O
of O
state O
- O
of O
- O
the O
- O
art O
methods O
. O

1 O
2 O

Few O
- O
shot O
learning O
for O
natural B-TaskName
language I-TaskName
understanding I-TaskName
( O
NLU B-TaskName
) O
has O
been O
significantly O
advanced O
by O
pretrained O
language O
models O
( O
PLMs O
; O
Brown O
et O
al O
. O
, O
2020;Schick O
and O
Schütze O
, O
2021a O
, O
b O
) O
. O

With O
the O
goal O
of O
learning O
a O
new O
task O
with O
very O
few O
( O
usually O
less O
than O
a O
hundred O
) O
samples O
, O
few O
- O
shot O
learning O
benefits O
from O
the O
prior O
knowledge O
stored O
in O
PLMs O
. O

Various O
few O
- O
shot O
methods O
based O
on O
PLMs O
and O
prompting O
have O
been O
proposed O
( O
Liu O
et O
al O
. O
, O
2021b;Menon O
et O
al O
. O
, O
2021;Gao O
et O
al O
. O
, O
2020 O
) O
. O

3 B-MetricValue
-Talks O
about O
topics O
with O
the O
correct O
orientation O
. O

For O
example O
, O
if O
the O
target O
style O
is O
democratic O
and O
talks O
about O
progressive O
issues O
like O
liberty O
, O
free O
speech O
, O
Elizabeth O
Warren O
, O
Joe O
Biden O
, O
gay O
rights O
etc O
. O

Hallucinates O
Sen O
Booker O
which O
appears O
frequently O
in O
the O
dataset O
Target O
by O
far O
, O
the O
best O
spot O
for O
ramen O
. O

simple O
menu O
. O

fast O
service O
. O

silky O
, O
creamy O
chicken O
broth O
. O

by O
far O
the O
best O
breakfast O
tacos O
in O
the O
area O
. O

friendly O
staff O
. O

great O
food O
. O

ask O
for O
the O
spicy O
chicken O
, O
and O
they O
have O
a O
great O
selection O
. O

try O
sushi O
boat O
. O

it O
's O
totally O
amazing O
. O

they O
offer O
good O
food O
and O
high O
quality O
. O

good O
sake O
is O
ready O
. O

thank O
you O
for O
good O
place O
. O

love O
it O
. O

good O
food O
. O

they O
have O
good O
margaritas O
and O
good O
food O
. O

good O
prices O
. O

there O
's O
a O
good O
amount O
of O
food O
for O
you O
. O

best O
thai O
in O
austin O
. O

we O
love O
the O
atmosphere O
, O
the O
service O
and O
obviously O
the O
food O
. O

they O
did O
a O
great O
job O
last O
time O
we O
were O
there O
since O
our O
party O
had O
specific O
requirements O
like O
< O
unk O
> O
free O
and O
< O
unk O
> O
. O

best O
mexican O
food O
in O
the O
area O
. O

the O
service O
was O
great O
and O
the O
food O
was O
so O
good O
. O

we O
had O
a O
party O
of O
10 O
and O
they O
were O
very O
accommodating O
to O
our O
group O
of O
us O
. O

we O
were O
there O
last O
night O
and O
the O
food O
was O
good O
fabulous O
, O
delicious O
, O
authentic O
. O

at O
lunch O
on O
a O
saturday O
the O
place O
was O
packed O
! O
20 O
minute O
wait O
for O
a O
table O
. O

i O
was O
one O
of O
two O
customers O
who O
was O
not O
chinese O
. O

i O
'll O
be O
back O
frequently O
. O

awesome O
mexican O
food O
, O
a O
little O
on O
the O
corner O
of O
a O
< O
unk O
> O
. O

i O
was O
here O
on O
a O
saturday O
night O
. O

they O
were O
busy O
, O
but O
we O
were O
able O
to O
get O
a O
table O
. O

i O
will O
definitely O
be O
back O
! O
this O
place O
is O
great O
! O
i O
grew O
up O
going O
to O
china O
inn O
in O
chamblee O
plaza O
and O
it O
's O
the O
same O
owner O
! O
lunch O
service O
is O
fast O
and O
delicious O
! O
give O
it O
a O
shot O
, O
you O
wo O
n't O
be O
disappointed O
! O
this O
place O
is O
awesome O
! O
! O
i O
've O
been O
coming O
to O
this O
location O
for O
years O
and O
it O
's O
always O
clean O
and O
the O
service O
is O
fast O
and O
friendly O
. O

it O
's O
a O
great O
mexican O
restaurant O
, O
you O
ca O
n't O
go O
wrong O
with O
the O
food O
! O
awful O
. O

i O
'm O
writing O
this O
as O
i O
eat O
it O
now O
. O

worst O
poke O
bowl O
i O
've O
ever O
had O
. O

the O
smallest O
portion O
of O
poke O
possible O
, O
< O
unk O
> O
overcooked O
rice O
, O
and O
barely O
got O
any O
ponzu O
. O

most O
standard O
toppings O
cost O
extra O
too O
. O

awful O
! O
i O
've O
never O
had O
a O
bad O
meal O
here O
. O

i O
only O
ordered O
two O
of O
them O
. O

the O
only O
thing O
i O
did O
n't O
like O
was O
the O
< O
unk O
> O
. O

it O
's O
not O
much O
flavor O
, O
but O
the O
meat O
is O
dry O
. O

worst O
chinese O
food O
experience O
i O
ever O
had O
. O

told O
the O
manager O
about O
my O
allergies O
and O
that O
all O
i O
wanted O
was O
vegetable O
fried O
rice O
no O
soy O
sauce O
they O
could O
n't O
even O
handle O
that O
! O
! O
! O
amateur O
hour O
here O
do O
n't O
waste O
your O
time O
. O

go O
to O
china O
blossom O
worst O
experience O
ever O
. O

i O
ordered O
the O
< O
unk O
> O
and O
they O
were O
all O
wrong O
with O
that O
i O
could O
n't O
eat O
the O
food O
. O

that O
's O
how O
i O
do O
n't O
care O
about O
how O
they O
charge O
you O
for O
the O
fajitas O
. O

no O
one O
ever O
came O
to O
eat O
here O
. O

the O
food O
was O
terrible O
. O

it O
definitely O
was O
not O
fresh O
. O

the O
broccoli O
was O
over O
cooked O
on O
my O
beef O
broccoli O
. O

my O
chicken O
chow O
mean O
fried O
rice O
just O
looked O
and O
tasted O
like O
last O
weeks O
rice O
. O

there O
was O
one O
chunk O
of O
chicken O
and O
< O
unk O
> O
pieces O
of O
egg O
in O
the O
food O
was O
just O
ok O
. O

the O
chicken O
was O
dry O
. O

it O
was O
very O
dry O
. O

i O
ordered O
the O
chicken O
chimichanga O
and O
it O
was O
just O
plain O
gross O
. O

the O
only O
thing O
that O
was O
< O
unk O
> O
was O
the O
chicken O
burrito O
. O

there O
was O
only O
one O
other O
person O
in O
the O
< O
unk O
> O
We O
would O
like O
to O
thank O
the O
anonymous O
reviewers O
for O
their O
useful O
suggestions O
. O

We O
would O
also O
like O
to O
acknowledge O
the O
support O
of O
the O
NExT O
research O
grant O
funds O
, O
supported O
by O
the O
National O
Research O
Foundation O
, O
Prime O
Ministers O
Office O
, O
Singapore O
under O
its O
IRC@ O
SG O
Funding O
Initiative O
, O
and O
to O
gratefully O
acknowledge O
the O
support O
of O
NVIDIA O
Corporation O
with O
the O
donation O
of O
the O
GeForce O
GTX O
Titan O
XGPU O
used O
in O
this O
research O
. O

The O
work O
is O
also O
supported O
by O
the O
project O
no O
. O

T2MOE2008 O
titled O
CSK O
- O
NLP O
: O
Leveraging O
Commonsense O
Knowledge O
for O
NLP O
awarded O
by O
Singapore O
's O
Ministry O
of O
Education O
under O
its O
Tier-2 O
grant O
scheme O
. O

In O
this O
paper O
, O
we O
study O
two O
issues O
of O
semantic O
parsing O
approaches O
to O
conversational B-TaskName
question I-TaskName
answering I-TaskName
over O
a O
large O
- O
scale O
knowledge O
base O
: O
( O
1 O
) O
The O
actions O
defined O
in O
grammar O
are O
not O
sufficient O
to O
handle O
uncertain O
reasoning O
common O
in O
real O
- O
world O
scenarios O
. O

( O
2 O
) O
Knowledge O
base O
information O
is O
not O
well O
exploited O
and O
incorporated O
into O
semantic O
parsing O
. O

To O
mitigate O
the O
two O
issues O
, O
we O
propose O
a O
knowledge B-MethodName
- I-MethodName
aware I-MethodName
fuzzy I-MethodName
semantic I-MethodName
parsing I-MethodName
framework O
( O
KaFSP B-MethodName
) O
. O

It O
defines O
fuzzy O
comparison O
operations O
in O
the O
grammar O
system O
for O
uncertain O
reasoning O
based O
on O
the O
fuzzy O
set O
theory O
. O

In O
order O
to O
enhance O
the O
interaction O
between O
semantic O
parsing O
and O
knowledge O
base O
, O
we O
incorporate O
entity O
triples O
from O
the O
knowledge O
base O
into O
a O
knowledgeaware O
entity O
disambiguation O
module O
. O

Additionally O
, O
we O
propose O
a O
multi O
- O
label O
classification O
framework O
to O
not O
only O
capture O
correlations O
between O
entity O
types O
and O
relations O
but O
also O
detect O
knowledge O
base O
information O
relevant O
to O
the O
current O
utterance O
. O

Both O
enhancements O
are O
based O
on O
pre O
- O
trained O
language O
models O
. O

Experiments O
on O
a O
large O
- O
scale O
conversational B-TaskName
question I-TaskName
answering I-TaskName
benchmark O
demonstrate O
that O
the O
proposed O
KaFSP B-MethodName
achieves O
significant O
improvements O
over O
previous O
state O
- O
of O
- O
the O
- O
art O
models O
, O
setting O
new O
SOTA O
results O
on O
8 O
out O
of O
10 O
question O
types O
, O
gaining O
improvements O
of O
over O
10 B-MetricValue
% I-MetricValue
F1 B-MetricName
or O
accuracy B-MetricName
on O
3 O
question O
types O
, O
and O
improving O
overall O
F1 B-MetricName
from O
83.01 B-MetricValue
% I-MetricValue
to O
85.33 B-MetricValue
% I-MetricValue
. O

The O
source O
code O
of O
KaFSP B-MethodName
is O
available O
at O
https O
: O
//github.com O
/ O
tjunlp O
- O
lab O
/ O
KaFSP B-MethodName
. O

If O
the O
sentence O
itself O
has O
no O
sentiment O
then O
chose O
2 B-MetricValue
Political O
Orientation O
1 O
-Talks O
about O
topics O
with O
the O
other O
orientation O
. O

For O
example O
, O
if O
the O
target O
style O
is O
democratic O
and O
the O
target O
sentence O
talks O
about O
conservative O
issues O
like O
abortion O
, O
gun O
control O
2 B-MetricValue
-Neutral O
. O

2 B-MetricValue
-Fluent O
but O
with O
some O
mistakes O
-Fluent O
but O
with O
some O
grammatical O
errors O
3 B-MetricValue
-Entirely O
fluent O
. O

-A O
good O
English O
Sentence O
Similarity O
: O
Indicate O
how O
semantically O
similar O
the O
target O
sentence O
is O
. O

1 B-MetricValue
-Does O
not O
share O
any O
words O
/ O
phrases O
with O
the O
source O
sentence O
and/or O
is O
not O
semantically O
similar O
( O
does O
not O
share O
high O
level O
topics O
of O
the O
sentence O
) O
2 B-MetricValue
-Shares O
some O
words O
/ O
phrases O
with O
the O
source O
sentence O
and/or O
has O
moderate O
level O
of O
semantic O
similarity O
( O
talks O
about O
similar O
high O
level O
topics O
) O
3 B-MetricValue
-Shares O
appropriate O
words O
/ O
phrases O
with O
the O
source O
sentence O
and O
is O
highly O
semantically O
similar O
Accuracy B-MetricName
: O
Indicate O
whether O
the O
target O
sentence O
is O
accurately O
transferred O
to O
the O
target O
domain O
Sentiment O
Transfer O
1 B-MetricValue
-The O
target O
sentiment O
is O
not O
evident O
in O
the O
target O
sentence O
at O
all O
. O

Has O
words O
expressing O
opposite O
sentiment O
2 B-MetricValue
-Neutral O
Sentiment O
. O

Choose O
this O
option O
, O
if O
it O
has O
both O
positive O
and O
negative O
sentiment O
-The O
target O
sentiment O
is O
evident O
in O
the O
target O
sentiment O
. O

Has O
appropriate O
sentiment O
bearing O
words O
. O

1 B-MetricValue
-Not O
fluent O
at O
all O
-Does O
not O
look O
like O
an O
English O
sentence O
. O

Fluency B-MetricName
: O
Indicate O
how O
fluent O
the O
target O
sentence O
is O
( O
regardless O
of O
whether O
the O
sentence O
is O
appropriately O
transferred O
to O
the O
target O
sentence O
) O

• O
Target O
sentence O
: O
The O
transferred O
sentence O
produced O
by O
one O
of O
the O
systems O
For O
every O
target O
sentence O
you O
will O
be O
asked O
to O
rate O
it O
according O
to O
three O
measures O
described O
below O
. O

Information O
about O
participants O
: O
We O
hire O
three O
graduate O
researchers O
in O
NLP O
( O
average O
age O
25 O
) O
for O
the O
annotation O
task O
who O
are O
well O
versed O
in O
English O
. O

We O
obtained O
permission O
for O
their O
participation O
and O
compensated O
them O
appropriately O
according O
to O
hourly O
wages O
in O
the O
country O
. O

The O
specific O
instruction O
given O
to O
them O
for O
the O
evaluation O
are O
as O
follows O
. O

Consider O
two O
sentences O
• O
Source O
sentence O
: O
Sentence O
from O
the O
source O
domain O

In O
recent O
times O
, O
text O
style O
transfer O
models O
are O
moving O
away O
from O
disentanglement O
approaches O
( O
Subramanian O
et O
al O
. O
, O
2018 O
) O
. O

Recent O
works O
that O
use O
transformers O
for O
style B-TaskName
transfer I-TaskName
also O
have O
adopted O
this O
( O
Dai O
et O
al O
. O
, O
2019;Krishna O
et O
al O
. O
, O
2020 O
) O
. O

How O
- O
ever O
, O
these O
methods O
do O
not O
explicitly O
maintain O
the O
constraints O
between O
the O
two O
styles O
which O
is O
the O
main O
aim O
of O
our O
work O
. O

Text B-TaskName
style I-TaskName
transfer I-TaskName
works O
focuses O
on O
retaining O
content O
and O
changing O
the O
style O
of O
sentences O
but O
does O
not O
maintain O
other O
desirable O
constraints O
. O

We O
address O
this O
by O
introducing O
two O
cooperative B-HyperparameterValue
losses I-HyperparameterValue
to O
the O
GAN O
- O
inspired O
Adversarially B-MethodName
Regularized I-MethodName
Autoencoder I-MethodName
( O
ARAE B-MethodName
) O
that O
further O
regularizes O
the O
latent O
space O
. O

While O
satisfying O
the O
constraints O
our O
methods O
brings O
significant O
improvements O
in O
overall O
score O
. O

While O
we O
focus O
on O
simple O
constraints O
at O
the O
sentence O
- O
and O
word O
- O
level O
, O
future O
work O
can O
add O
phrase O
- O
level O
and O
more O
fine O
- O
grained O
constraints O
. O

Potential O
future O
work O
may O
explore O
reinforcement O
learning O
losses O
to O
directly O
optimize O
the O
constraints O
. O

More O
transfer O
results O
are O
mention O
in O
Table O
8 O
. O

Examples O
where O
our O
system O
fails O
with O
plausible O
explanation O
are O
given O
in O
Table O
9 O
. O

Examples O
of O
translation O
from O
the O
multi O
- O
attribute O
dataset O
is O
shown O
in O
Table O
10.For O
FL B-MetricName
, O
0 B-MetricValue
indicates O
not O
fluent O
at O
all O
, O
1 B-MetricValue
indicates O
somewhat O
fluent O
and O
2 B-MetricValue
is O
a O
completely O
fluent O
sentence O
. O

We O
explicitly O
ask O
the O
annotators O
to O
consider O
semantic B-MetricName
similarity I-MetricName
for O
SIM B-MetricName
, O
irrespective O
of O
whether O
the O
target O
sentence O
shares O
some O
phrases O
with O
the O
source O
sentence O
, O
with O
1 O
indicating O
no O
semantic O
similarity O
and O
3 O
indicating O
complete O
semantic O
similarity O
. O

For O
ACC B-MetricName
, O
1 B-MetricValue
indicates O
that O
the O
target O
sentence O
has O
only O
the O
source O
sentence O
style O
while O
2 B-MetricValue
indicates O
good O
transfer O
to O
the O
target O
style O
. O

We O
calculate O
the O
Krippendorff O
's O
alpha O
to O
assess O
the O
inter O
annotator O
agreement O
. O

Table O
7 O
shows O
the O
inter O
- O
annotator O
agreement O
. O

An O
α B-MetricName
of O
0.4 B-MetricValue
is O
considered O
good O
agreeement O
( O
Hedayatnia O
et O
al O
. O
, O
2020 O
) O
. O

We O
have O
moderate O
to O
good O
agreements O
on O
all O
the O
datasets O
for O
different O
measures O
. O

On O
more O
inspection O
we O
found O
that O
the O
disagreements O
in O
fluency B-MetricName
mostly O
arrives O
for O
small O
phrases O
like O
" O
my O
fav O
" O
although O
is O
an O
accepted O
phrase O
in O
social O
media O
text O
is O
considered O
2 O
by O
one O
annotator O
and O
3 O
by O
another O
. O

We O
also O
further O
note O
that O
, O
smaller O
sentences O
were O
easier O
to O
judge O
and O
had O
better O
agreement O
rates O
on O
SIM B-MetricName
compared O
to O
longer O
sentences O
. O

Disentanglement O
approaches O
are O
the O
prevalent O
approach O
to O
tackle O
unsupervised B-TaskName
attribute I-TaskName
transfer I-TaskName
: O
attributes O
and O
content O
are O
separated O
in O
latent O
dimension O
. O

To O
disentangle O
the O
attributes O
adversarial O
methods O
maximize O
the O
loss O
of O
a O
pre O
- O
trained O
attribute O
classifier O
Fu O
et O
al O
. O
, O
2018;Zhao O
et O
al O
. O
, O
2018a;John O
et O
al O
. O
, O
2019 O
) O
. O

However O
, O
the O
literature O
has O
paid O
little O
attention O
in O
defining O
and O
preserving O
content O
. O

Cycle O
consistency O
losses O
-imposing O
that O
reconstruction O
from O
the O
target O
style O
sentence O
should O
resemble O
the O
source O
sentence O
-is O
the O
most O
prevalent O
( O
Prabhumoye O
et O
al O
. O
, O
2018;Logeswaran O
et O
al O
. O
, O
2018;Dai O
et O
al O
. O
, O
2019;Huang O
et O
al O
. O
, O
2020;Yi O
et O
al O
. O
, O
2020 O
) O
. O

However O
, O
this O
is O
expensive O
and O
nondifferentiable O
, O
thus O
requiring O
reinforcement O
learning O
techniques O
to O
enforce O
it O
. O

Our O
work O
defines O
the O
different O
constraints O
that O
should O
be O
preserved O
and O
adds O
simple O
differentiable O
contrastive O
learning O
losses O
to O
preserve O
them O
. O

Next O
, O
we O
train O
the O
best O
variant O
of O
ARAE B-MethodName
seq2seq I-MethodName
to O
transfer O
a O
separate O
set O
DVD O
reviews O
to O
ELECTRON B-DatasetName
- I-DatasetName
ICS I-DatasetName
reviews I-DatasetName
and O
use O
them O
as O
adversarial O
examples O
to O
test O
the O
DANN B-MethodName
model O
6 O
. O

We O
find O
that O
the O
accuracy B-MetricName
of O
DANN B-MethodName
on O
the O
ELECTRONICS B-DatasetName
domain O
reduces O
by O
∼3 B-MetricValue
points O
. O

This O
shows O
the O
potential O
application O
of O
domain O
transferred O
sentences O
as O
adversarial O
examples O
. O

Similar O
ideas O
have O
been O
tried O
for O
image B-TaskName
style I-TaskName
transfer I-TaskName
, O
but O
needs O
more O
investigation O
in O
NLP.Text B-TaskName
attribute I-TaskName
transfer I-TaskName
has O
a O
vast O
literature O
( O
Jin O
et O
al O
. O
, O
2020a O
) O
with O
deep O
learning O
methods O
becoming O
popular O
. O

The O
methods O
are O
either O
supervised O
( O
requiring O
parallel O
data O
) O
or O
unsupervised O
. O

Supervised O
methods O
re O
- O
purpose O
Sequence O
to O
Sequence O
models O
used O
in O
machine B-TaskName
translation I-TaskName
to O
achieve O
the O
goals O
( O
Rao O
and O
Tetreault O
, O
2018 O
) O
. O

However O
, O
obtaining O
parallel O
data O
is O
cumbersome O
and O
thus O
unsupervised O
methods O
that O
consider O
pseudo O
- O
parallel O
data O
have O
become O
popular O
. O

Transferred O
sentences O
as O
Adversarial O
Examples O
: O
We O
demonstrate O
an O
important O
application O
of O
our O
proposed O
constrained O
transfer O
by O
considering O
them O
as O
adversarial O
examples O
for O
domain O
adaptation O
. O

Domain B-MethodName
Adversarial I-MethodName
Neural I-MethodName
Network I-MethodName
( O
DANN B-MethodName
) O
( O
Ganin O
et O
al O
. O
, O
2017 O
) O
is O
an O
unsupervised O
domain O
adaptation O
method O
that O
improves O
performance O
of O
an O
end O
- O
task O
( O
e.g O
, O
sentiment B-TaskName
analysis I-TaskName
) O
on O
a O
target O
domain O
considering O
only O
supervised O
data O
from O
source O
domain O
. O

We O
train O
DANN B-MethodName
for O
sentiment B-TaskName
analysis I-TaskName
on O
amazon B-DatasetName
reviews I-DatasetName
dataset O
( O
He O
and O
McAuley O
, O
2016 O
) O
with O
DVD O
as O
source O
and O
ELECTRONICS O
as O
the O
tar O
- O
get O
domain O
-achieving O
an O
accuracy O
of O
83.75 B-MetricName
% I-MetricName
on O
ELECTRONICS B-DatasetName
. O

Using O
Transformers O
: O
We O
also O
replace O
our O
LSTM B-HyperparameterValue
auto B-HyperparameterName
- I-HyperparameterName
encoders I-HyperparameterName
with O
both O
pre O
- O
trained O
and O
randomly O
initialized O
transformer B-HyperparameterValue
encoder O
- O
decoders O
( O
Rothe O
et O
al O
. O
, O
2020 O
) O
. O

Although O
we O
found O
an O
increase O
in O
the O
AGG B-MetricName
, O
it O
was O
mostly O
because O
of O
very O
high O
SIM B-MetricName
and O
very O
low O
ACC B-MetricName
. O

Reducing O
the O
number B-HyperparameterName
of I-HyperparameterName
layers I-HyperparameterName
, O
attention O
heads O
would O
still O
result O
in O
a O
large O
model O
that O
is O
still O
prone O
to O
copying O
text O
. O

This O
reveals O
the O
potential O
limitations O
of O
our O
method O
and O
training O
using O
transformers O
is O
a O
future O
work O
. O

ARAE B-MethodName
john O
abraham O
had O
one O
of O
my O
favorite O
roles O
.Source O
( O
IMDB B-DatasetName
) O
chris O
klein O
's O
character O
was O
unlikable O
from O
the O
start O
and O
never O
made O
an O
improvement O
Ours O
robert O
de O
niro O
was O
very O
good O
as O
the O
man O
and O
she O
's O
never O
been O
ARAE B-MethodName
both O
of O
his O
character O
was O
made O
and O
had O
a O
huge O
smile O
on O
me O
Qualitative O
Examples O
: O
Table O
6 O
shows O
examples O
of O
our O
model O
maintaining O
constraints O
compared O
to O
ARAE B-MethodName
. O

Sometimes O
, O
ARAE B-MethodName
hallucinates O
and O
adds O
personal O
pronouns O
like O
" O
my O
" O
to O
the O
text O
even O
when O
there O
are O
no O
personal O
pronouns O
( O
row O
1 O
) O
. O

Also O
, O
our O
model O
produces O
sentences O
where O
the O
number O
of O
proper O
nouns O
are O
retained O
( O
Chris O
Klein O
vs. O
Robert O
De O
Niro O
) O
, O
whereas O
ARAE B-MethodName
does O
not O
. O

Cycle O
Consistency O
Loss O
: O
a O
) O
In O
Latent O
Spaces O
-Cycle O
consistency O
in O
latent O
spaces O
has O
been O
shown O
to O
improve O
word O
- O
level O
tasks O
, O
such O
as O
cross B-TaskName
- I-TaskName
lingual I-TaskName
dictionary I-TaskName
construction I-TaskName
( O
Mohiuddin O
and O
Joty O
, O
2019 O
) O
and O
topic B-TaskName
modeling I-TaskName
. O

A O
recent O
work O
from O
( O
Huang O
et O
al O
. O
, O
2020 O
) O
claims O
to O
improve O
unsupervised B-TaskName
style I-TaskName
transfer I-TaskName
using O
such O
losses B-HyperparameterName
. O

In O
our O
experiments O
, O
however O
, O
it O
did O
not O
result O
in O
any O
noticeable O
performance O
improvement O
5 O
. O

Given O
this O
, O
we O
hypothesize O
that O
cycle O
consistency O
might O
be O
too O
restrictive O
for O
sentence O
- O
level O
tasks O
. O

b O
) O
Using O
Back O
- O
Translation O
- O
Back O
- O
translation O
is O
another O
alternative O
to O
ensure O
semantic O
consistency O
between O
source O
and O
the O
target O
sentence O
( O
Prabhumoye O
et O
al O
. O
, O
2018;Artetxe O
et O
al O
. O
, O
2018;Lample O
et O
al O
. O
, O
2017 O
) O
. O

However O
, O
in O
our O
case O
, O
since O
we O
are O
training O
an O
ARAE B-MethodName
, O
it O
would O
involve O
an O
additional O
inference O
and O
auto O
- O
encoder O
training O
step O
which O
is O
expensive O
and O
we O
defer O
exploring O
this O
. O

Ours O
michael O
keaton O
was O
also O
great O
in O
his O
role O
. O

Table O
5 O
: O
Example O
outputs O
generated O
by O
the O
best O
system O
according O
to O
AGG B-MetricName
score O
. O

Source O
( O
IMDB B-DatasetName
) O
jean O
seberg O
had O
not O
one O
iota O
of O
acting O
talent O
. O

POLITICAL B-DatasetName
i O
wish O
u O
would O
bring O
change O
and O
i O
wish O
you O
would O
help O
bring O
democracy O
and O
i O
' O
m O
not O
sure O
mr.trump O
. O

this O
is O
a O
film O
that O
has O
been O
a O
lot O
of O
times O
and O
it O
's O
really O
good O
. O

this O
movie O
is O
a O
very O
good O
example O
of O
a O
film O
that O
will O
never O
be O
forgotten O
. O

Multiple O
Attribute O
Datasets O
: O
To O
test O
whether O
our O
model O
can O
satisfy O
constraints O
across O
domains O
where O
multiple O
attributes O
change O
, O
we O
use O
the O
multiattribute O
dataset O
released O
by O
( O
Lample O
et O
al O
. O
, O
2019 O
) O
. O

this O
movie O
is O
a O
very O
poor O
attempt O
to O
make O
money O
using O
a O
classical O
theme O
. O

A O
seemingly O
easy O
to O
maintain O
constraint O
is O
the O
length B-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
sentence I-HyperparameterName
. O

However O
, O
seq2seq O
systems O
have O
a O
difficulty O
of O
maintaining O
appropriate O
lengths O
( O
Murray O
and O
Chiang O
, O
2018 O
) O
. O

With O
no O
additional O
regularization O
ARAE B-MethodName
does O
not O
maintain O
the O
length O
as O
well O
as O
ARAE B-MethodName
seq2seq I-MethodName
+ I-MethodName
CLF I-MethodName
. O

On O
the O
other O
hand O
, O
compared O
to O
the O
lexical O
constraints O
, O
syntactic O
attributes O
like O
descriptiveness O
, O
tree B-HyperparameterName
height I-HyperparameterName
and O
domain O
specific O
constraints O
present O
challenges O
, O
with O
significantly O
lower O
F B-MetricName
scores O
. O

ARAE B-MethodName
seq2seq I-MethodName
+ I-MethodName
CLF I-MethodName
produces O
significantly O
better O
results O
in O
maintaining O
them O
. O

This O
shows O
that O
obtaining O
improvements O
on O
the O
overall O
AGG B-MetricName
does O
not O
necessarily O
translate O
to O
producing O
outputs O
that O
satisfy O
constraints O
. O

DRG B-MethodName
maintains O
the O
proper O
noun O
for O
IMDB B-DatasetName
effectively O
, O
because O
it O
contains O
a O
wide O
variety O
of O
actor O
and O
movie O
names O
. O

They O
are O
retained O
verbatim O
after O
the O
delete O
operation O
. O

Human O
Evaluation O
: O
We O
average O
the O
results O
and O
present O
it O
in O
Table O
4 O
. O

DRG B-MethodName
produces O
marginally O
better O
semantically O
similar O
sentences O
. O

Compared O
to O
ARAE B-MethodName
, O
our O
model O
performs O
well O
except O
for O
in O
YELP B-DatasetName
. O

This O
may O
be O
because O
we O
use O
nucleus B-HyperparameterName
sampling I-HyperparameterName
with O
0.9 B-HyperparameterValue
which O
optimizes O
for O
diversity O
rather O
than O
similarity O
. O

On O
other O
metrics O
we O
perform O
on O
par O
or O
better O
than O
our O
competing O
systems O
. O

( O
See O
Appendix O
B O
) O
Qualitative O
Examples O
: O
Table O
5 O
shows O
examples O
of O
the O
quality O
of O
transferred O
examples O
( O
see O
Appendix O
A O
for O
more O
) O
. O

Mistakes O
made O
by O
the O
model O
can O
be O
attributed O
to O
poor O
understanding O
of O
the O
original O
semantics O
, O
lack O
of O
diversity O
, O
and O
not O
producing O
attribute O
- O
specific O
words O
. O

Figure O
3 O
shows O
that O
introducing O
the O
cooperative O
losses O
significantly O
outperform O
DRG B-MethodName
and O
ARAE B-MethodName
in O
maintaining O
constraints O
. O

Specifically O
the O
ARAE B-MethodName
seq2seq I-MethodName
+ I-MethodName
CLF I-MethodName
model O
performs O
better O
than O
ARAE B-MethodName
seq2seq I-MethodName
+ I-MethodName
CONTRA I-MethodName
. O

One O
reason O
could O
be O
that O
, O
finding O
the O
appropriate O
positives O
and O
strong O
negatives O
can O
be O
problematic O
for O
contrastive O
learning O
. O

On O
the O
other O
hand O
, O
the O
classifier O
's O
objective O
is O
simpler O
and O
forces O
the O
encoder O
to O
produce O
representations O
that O
satisfy O
the O
different O
constraints O
effectively O
. O

Cooperative B-HyperparameterValue
Losses I-HyperparameterValue
are O
Important O
on O
Both O
the O
Generator O
and O
Critic O
: O
Table O
3 O
shows O
the O
importance O
of O
adding O
the O
cooperative B-HyperparameterValue
losses I-HyperparameterValue
on O
the O
generator O
and O
critic O
. O

First O
, O
we O
see O
that O
adding O
the O
cooperative B-HyperparameterValue
losses I-HyperparameterValue
on O
both O
the O
generator O
and O
the O
critic O
is O
crucial O
for O
the O
overall O
performance O
. O

While O
adding O
the O
cooperative B-HyperparameterValue
contrastive I-HyperparameterValue
loss I-HyperparameterValue
to O
both O
the O
generator O
and O
critic O
increases O
FL B-MetricName
and O
ACC B-MetricName
while O
maintaining O
similar O
levels O
of O
SIM B-MetricName
, O
adding O
the O
cooperative B-HyperparameterValue
classification I-HyperparameterValue
loss I-HyperparameterValue
improves O
SIM B-MetricName
which O
shows O
the O
complementary O
nature O
of O
the O
losses O
. O

Nucleus O
Sampling O
: O
Our O
system O
achieves O
the O
highest O
AGG B-MetricName
score O
with O
greedy B-HyperparameterValue
decoding I-HyperparameterValue
. O

We O
also O
experiment O
with O
nucleus O
sampling O
( O
Holtzman O
et O
al O
. O
, O
2019 O
) O
with O
different O
p B-HyperparameterName
values O
. O

We O
report O
results O
for O
only O
p=0.6 B-HyperparameterName
in O
With O
p=0.6 B-HyperparameterName
, O
the O
results O
are O
more O
diverse O
, O
increasing O
ACC B-MetricName
as O
expected O
. O

However O
we O
find O
that O
with O
higher O
values O
of O
p B-HyperparameterValue
, O
there O
is O
a O
trade O
- O
off O
with O
SIM B-MetricName
resulting O
in O
a O
lower O
AGG B-MetricName
score O
overall O
-similar O
to O
Krishna O
et O
al O
. O
( O
2020).The O
number B-HyperparameterName
of I-HyperparameterName
positive I-HyperparameterName
and I-HyperparameterName
negative I-HyperparameterName
samples I-HyperparameterName
used O
for O
contrastive O
learning O
( O
Eq O
. O

5 O
) O
have O
a O
significant O
effect O
on O
the O
overall O
performance O
( O
Khosla O
et O
al O
. O
, O
2020;Henaff O
, O
2020 O
) O
. O

Table O
3 O
( O
rows O
|P O
| O
∈ O
{ O
1 O
, O
2 O
, O
5 O
, O
10 O
} O
) O
shows O
the O
AGG B-MetricName
scores O
on O
IMDB B-DatasetName
( O
for O
one O
of O
the O
runs O
) O
, O
for O
different O
number B-HyperparameterName
of I-HyperparameterName
positives I-HyperparameterName
. O

We O
find O
that O
AGG B-MetricName
is O
the O
highest O
with O
2 B-HyperparameterValue
positives B-HyperparameterName
per I-HyperparameterName
sample I-HyperparameterName
as O
also O
used O
by O
Khosla O
et O
al O
. O
( O
2020 O
) O
. O

Although O
increasing O
the O
number B-HyperparameterName
of I-HyperparameterName
negatives I-HyperparameterName
is O
beneficial O
for O
contrastive O
learning O
, O
when O
more O
than O
one O
positive O
example O
is O
available O
, O
using O
them O
brings O
further O
improvements O
( O
Khosla O
et O
al O
. O
, O
2020 O
) O
. O

Fluency B-MetricName
( O
FL B-MetricName
) O
also O
improves O
over O
all O
datasets O
. O

We O
hypothesize O
that O
reducing O
cooperative O
losses O
regularizes O
the O
latent O
space O
bringing O
fluent O
sentences O
closer O
together O
, O
enabling O
the O
decoder O
to O
produce O
semantically O
similar O
and O
linguistically O
acceptable O
sentences O
. O

The O
improvement O
for O
POLITICAL B-DatasetName
is O
less O
; O
we O
find O
these O
source O
sentences O
themselves O
are O
less O
fluent O
and O
contain O
many O
U.S. O
political O
acronyms O
, O
and O
that O
our O
system O
produces O
many O
outof O
- O
vocabulary O
words O
affecting O
fluency O
. O

Human O
Evaluation O
: O
We O
also O
perform O
an O
indicative O
human O
evaluation O
where O
we O
randomly O
sample O
100 B-HyperparameterValue
samples O
from O
each O
of O
the O
three O
datasets O
and O
hire O
three O
researchers O
to O
rate O
every O
sentence O
for O
FL B-MetricName
, O
SIM B-MetricName
and O
ACC B-MetricName
on O
a O
3 O
- O
point O
scale O
( O
Krishna O
et O
al O
. O
, O
2020 O
) O
. O

We O
compare O
ARAE B-MethodName
seq2seq I-MethodName
with O
the O
following O
baselines O
: O
a O
) O
DRG B-MethodName
: O
The O
Delete O
, O
Retrieve O
, O
Generate O
method O
that O
deletes O
domain O
specific O
attributes O
, O
retrieves O
a O
template O
and O
generates O
the O
target O
domain O
text O
( O
Li O
et O
al O
. O
, O
2018 O
) O
. O

We O
use O
the O
stronger O
, O
entire O
system O
rather O
than O
the O
weaker O
DELETEONLY O
and O
RETRIEVEONLY O
baselines O
; O
b O
) O
ARAE B-MethodName
: O
Adversarially O
regularized O
autoencoders O
our O
system O
is O
based O
on O
( O
Zhao O
et O
al O
. O
, O
2018b O
) O
Although O
DRG B-MethodName
produces O
sentences O
with O
high O
SIM B-MetricName
as O
most O
of O
the O
text O
from O
the O
original O
sentence O
is O
retained O
after O
the O
delete O
step O
, O
there O
is O
a O
large O
tradeoff O
with O
ACC B-MetricName
resulting O
in O
low O
AGG B-MetricName
scores O
. O

Also O
, O
compared O
to O
ARAE B-MethodName
, O
adding O
cooperative B-HyperparameterValue
losses I-HyperparameterValue
significantly O
increases O
the O
SIM B-MetricName
, O
with O
the O
highest O
increase O
observed O
for O
POLITICAL B-DatasetName
. O

The O
reasons O
for O
this O
could O
be O
two O
- O
fold O
: O
i O
) O
since O
we O
mine O
positive O
sentences O
from O
a O
corpus O
that O
is O
grounded O
in O
real O
world O
events O
, O
most O
lexically O
- O
similar O
sentences O
may O
also O
be O
semantically O
similar O
( O
Guu O
et O
al O
. O
, O
2018 O
) O
, O
and O
ii O
) O
since O
we O
tie O
the O
encoders O
from O
the O
source O
and O
target O
domain O
, O
we O
extract O
domain O
- O
agnostic O
information O
before O
generation O
, O
which O
retains O
content O
. O

AGG B-MetricName
= O
1 O
|S| O
s∈S O
ACC B-MetricName
( O
s O
) O
• O
SIM B-MetricName
( O
s O
) O
• O
FL B-MetricName
( O
s O
) O

Inference O
Hyper O
- O
parameters O
: O
We O
used O
nucleus B-HyperparameterName
sampling I-HyperparameterName
with O
p B-HyperparameterName
∈ O
[ O
0.6 B-HyperparameterValue
, O
0.9 B-HyperparameterValue
] O
. O

We O
tried O
different O
temperatures B-HyperparameterName
of O
scaling O
the O
softmax O
( O
Guo O
et O
al O
. O
, O
2017 O
) O
-0.4 B-HyperparameterValue
, O
0.5 B-HyperparameterValue
, O
0.6 B-HyperparameterValue
, O
0.7 B-HyperparameterValue
and O
chose O
the O
one O
that O
produced O
the O
best O
result O
on O
the O
dev O
set O
. O

Automatic O
Evaluation O
: O
Our O
automatic O
evaluation O
considers O
the O
following O
three O
prominent O
criteria O
: O
i O
) O
Semantic B-MetricName
Similarity I-MetricName
( O
SIM B-MetricName
): O
Measured O
between O
source O
and O
translated O
target O
sentences O
using O
encoders O
( O
Wieting O
et O
al O
. O
, O
2019 O
) O
, O
instead O
of O
ngram O
metrics O
like O
BLEU B-MetricName
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
which O
have O
weak O
correlations O
with O
human O
judgments O
. O

ii O
) O
Transfer B-MetricName
Accuracy I-MetricName
( O
ACC B-MetricName
): O
The O
transferred O
sentence O
should O
belong O
to O
the O
target O
domain O
and O
a O
classifier O
is O
trained O
to O
distinguish O
between O
the O
source O
and O
the O
target O
sentence O
. O

We O
use O
fastText O
classifiers O
( O
Joulin O
et O
al O
. O
, O
2017 O
) O
for O
every O
dataset O
. O

We O
achieve O
accuracy B-MetricName
of O
97.9 B-MetricValue
for O
YELP B-DatasetName
, O
96.9 B-MetricValue
for O
IMDB B-DatasetName
and O
97.1 B-MetricValue
for O
POLITICAL B-DatasetName
. O

iii O
) O
Fluency B-MetricName
( O
FL B-MetricName
): O
A O
transferred O
sentence O
should O
be O
grammatically O
correct O
. O

We O
fine O
- O
tune O
a O
RoBERTa B-MethodName
- I-MethodName
large I-MethodName
model O
on O
the O
COLA B-DatasetName
( O
Warstadt O
et O
al O
. O
, O
2018 O
) O
dataset O
to O
indicate O
whether O
a O
sentence O
is O
linguistically O
acceptable O
. O

Finally O
, O
we O
combine O
the O
three O
scores O
into O
an O
aggregate O
, O
following O
the O
criteria O
suggested O
by O
Krishna O
et O
al O
. O
( O
2020 O
): O

Training O
Hyper O
- O
parameters O
: O
For O
all O
our O
experiments O
we O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
of O
the O
auto O
- O
encoder O
( O
lr B-HyperparameterName
ae I-HyperparameterName
) O
to O
1e-3 B-HyperparameterValue
and O
( O
lr B-HyperparameterName
disc I-HyperparameterName
) O
to O
1e-4 B-HyperparameterValue
. O

The O
number B-HyperparameterName
of I-HyperparameterName
discriminator I-HyperparameterName
steps I-HyperparameterName
( O
n B-HyperparameterName
dis I-HyperparameterName
) O
is O
set O
to O
5 B-HyperparameterValue
. O

The O
Adam B-HyperparameterValue
optimizer B-HyperparameterName
parameters O
β B-HyperparameterName
1 I-HyperparameterName
= O
0.5 B-HyperparameterValue
and O
β B-HyperparameterName
2 I-HyperparameterName
= O
0.9 B-HyperparameterValue
, O
which O
ensures O
a O
more O
conservative O
optimization O
and O
is O
known O
to O
improve O
stability O
. O

We O
also O
add O
a O
gradient O
penalty O
to O
the O
loss O
function O
of O
the O
discriminator O
that O
stabilizes O
training O
. O

All O
the O
suggestions O
for O
stabilizing O
training O
are O
mostly O
obtained O
from O
( O
Arjovsky O
and O
Bottou O
, O
2017 O
) O
. O

We O
provide O
a O
summary O
of O
the O
dataset O
statistics O
in O
Table O
1 O
. O

We O
include O
datasets O
of O
varied O
length O
and O
complexity O
. O

Apart O
from O
having O
different O
topics O
, O
the O
IMDB B-DatasetName
dataset O
is O
more O
formal O
compared O
to O
the O
more O
colloquial O
YELP B-DatasetName
. O

We O
fix O
the O
maximum B-HyperparameterName
vocabulary I-HyperparameterName
size I-HyperparameterName
for O
YELP B-DatasetName
, O
IMDB B-DatasetName
and O
POLITICAL B-DatasetName
at O
30 B-HyperparameterValue
K I-HyperparameterValue
which O
is O
also O
the O
default O
maximum B-HyperparameterName
vocab I-HyperparameterName
size I-HyperparameterName
used O
in O
( O
Zhao O
et O
al O
. O
, O
2018b O
7 O
63.4 O
36.7 O
20.2 O
96.0 O
73.6 O
35.4 O
26.2 O
98.6 O
55.0 O
44.4 O
25.5 O
nucleus(p O
= O
0.6 O
) O
85.6 O
63.0 O
36.6 O
20.0 O
95.8 O
72.8 O
35.3 O
25.7 O
98.6 O
54.4 O
44.2 O
25 O
= O
0.6 O
) O
89.4 O
68.6 O
32.8 O
20.4 O
97.1 O
82.6 O
33.6 O
27.4 O
99.0 O
56.0 O
41.6 O
24.4 O
Table O
2 O
: O
Evaluation O
of O
ARAE B-MethodName
seq2seq I-MethodName
with O
ACC B-MetricName
( O
transfer B-MetricName
accuracy I-MetricName
) O
, O
FL B-MetricName
( O
fluency B-MetricName
) O
and O
SIM B-MetricName
( O
semantic B-MetricName
similarity I-MetricName
) O
, O
AGG B-MetricName
( O
aggregate B-MetricName
metric I-MetricName
) O
. O

Cooperatively O
reducing O
the O
contrastive O
or O
the O
classification O
loss O
is O
better O
than O
ARAE B-MethodName
. O

We O
report O
the O
mean O
of O
five O
runs O
for O
our O
experiments O
. O

The O
bolded O
measures O
are O
the O
best O
results O
between O
the O
two O
domains O
: O
i O
) O
Lexical O
: O
Sentence B-HyperparameterName
length I-HyperparameterName
-The O
transferred O
sentence O
should O
maintain O
a O
length O
similar O
to O
the O
original O
sentence O
( O
binarized O
to O
long O
sentences O
with O
10 B-HyperparameterValue
or O
or O
more O
words O
or O
short O
otherwise O
) O
. O

ii O
) O
Syntactic O
: O
Presence O
of O
personal O
pronouns O
( O
binarized O
to O
indicate O
the O
presence O
of O
a O
personal O
pronoun O
) O
; O
number B-HyperparameterName
of I-HyperparameterName
adjectives I-HyperparameterName
( O
categorical O
up O
to O
5 B-HyperparameterValue
) O
; O
number B-HyperparameterName
of I-HyperparameterName
proper I-HyperparameterName
nouns I-HyperparameterName
( O
categorical O
up O
to O
3 B-HyperparameterValue
) O
; O
syntactic B-HyperparameterName
tree I-HyperparameterName
height I-HyperparameterName
( O
categorical O
up O
to O
10 B-HyperparameterValue
) O
. O

iii O
) O
Domain O
specific O
-number B-HyperparameterName
of I-HyperparameterName
domain I-HyperparameterName
- I-HyperparameterName
specific I-HyperparameterName
attributes I-HyperparameterName
( O
Li O
et O
al O
. O
, O
2018 O
) O
( O
categorical O
up O
to O
5 B-HyperparameterValue
) O
. O

Further O
, O
we O
label O
the O
sentence O
with O
a O
constraintspecific O
, O
catch O
- O
all O
label O
if O
the O
bounds O
are O
beyond O
what O
we O
mention O
above O
. O

Since O
the O
distribution O
of O
the O
labels O
may O
be O
different O
, O
we O
report O
the O
F1 B-MetricName
score O
on O
our O
constraints O
. O

For O
the O
encoders O
, O
we O
use O
a O
one B-HyperparameterValue
- I-HyperparameterValue
layer I-HyperparameterValue
LSTM B-HyperparameterName
network I-HyperparameterName
with O
300 B-HyperparameterValue
hidden B-HyperparameterName
dimensions I-HyperparameterName
for O
all O
the O
datasets O
. O

For O
the O
critics O
and O
classification O
loss O
, O
we O
use O
a O
two B-HyperparameterValue
- I-HyperparameterValue
layer I-HyperparameterValue
multi B-HyperparameterName
- I-HyperparameterName
layer I-HyperparameterName
perceptron I-HyperparameterName
with O
100 B-HyperparameterValue
hidden B-HyperparameterName
units I-HyperparameterName
. O

where O
|C| B-HyperparameterName
is O
the O
number B-HyperparameterName
of I-HyperparameterName
constraints I-HyperparameterName
per I-HyperparameterName
sentence I-HyperparameterName
, O
σ B-HyperparameterName
is O
the O
sigmoid B-HyperparameterValue
function I-HyperparameterValue
and O
l O
c O
are O
the O
logits O
produced O
by O
the O
classifier O
for O
z O
i O
. O

As O
in O
contrastive O
loss O
, O
the O
z O
i O
can O
be O
produced O
by O
encoders O
of O
S O
, O
T O
or O
from O
the O
hidden O
layers O
of O
the O
critic O
. O

Xent O
loss O
, O
we O
use O
one O
that O
is O
amenable O
to O
multiple O
positive O
instances O
( O
Khosla O
et O
al O
. O
, O
2020 O
) O
. O

Given O
a O
sentence O
s O
i O
∈ O
S O
in O
a O
minibatch B-HyperparameterName
of I-HyperparameterName
size I-HyperparameterName
B I-HyperparameterName
, O
we O
mine O
P O
positive O
sentences O
each O
from O
S O
and O
T O
that O
share O
the O
same O
constraints O
with O
s O
i O
. O

This O
contrastive O
loss O
is O
given O
by O
: O

To O
alleviate O
the O
issue O
, O
we O
propose O
to O
learn O
a O
structured O
latent O
space O
which O
embodies O
notions O
of O
our O
constraints O
in O
its O
embedded O
latent O
codes O
. O

This O
ensure O
that O
instances O
with O
similar O
constraints O
are O
closer O
in O
the O
latent O
space O
. O

In O
particular O
, O
we O
propose O
two O
types O
of O
optimization B-HyperparameterName
-self O
- O
supervised O
and O
discriminative O
-to O
maintain O
the O
constraints O
better O
. O

We O
use O
contrastive O
representation O
learning O
to O
regularize O
the O
latent O
space O
, O
such O
that O
encoders O
bring O
two O
sentences O
sharing O
similar O
constraints O
closer O
together O
( O
positive O
pairs O
) O
, O
and O
force O
dissimilar O
ones O
away O
( O
negative O
pairs O
) O
. O

For O
example O
, O
sentences O
of O
similar O
lengths O
( O
irrespective O
of O
their O
domains O
) O
should O
be O
closer O
together O
. O

Among O
many O
self B-HyperparameterName
- I-HyperparameterName
supervised I-HyperparameterName
metric I-HyperparameterName
losses I-HyperparameterName
such O
as O
Triplet B-HyperparameterValue
Loss I-HyperparameterValue
( O
Hoffer O
and O
Ailon O
, O
2015 O
) O
and O
NT- B-MethodName
2 O
) O
Train O
the O
Critic O
: O

In O
addition O
, O
we O
tie O
the O
weights O
of O
the O
encoders O
from O
both O
domains O
, O
so O
that O
the O
encoders O
learn O
to O
encode O
domain O
- O
agnostic O
information O
. O

Tying O
encoder O
weights O
has O
also O
been O
used O
by O
unsupervised O
machine B-TaskName
translation I-TaskName
( O
Artetxe O
et O
al O
. O
, O
2018;Lample O
et O
al O
. O
, O
2017 O
) O
and O
multiple O
other O
works O
( O
Mai O
et O
al O
. O
, O
2020;Huang O
et O
al O
. O
, O
2020;Artetxe O
et O
al O
. O
, O
2018 O
) O
3 O
.While O
the O
latent O
space O
in O
ARAE B-MethodName
seq2seq I-MethodName
learns O
to O
match O
S O
and O
T O
sentences O
, O
there O
is O
no O
guarantee O
on O
translations O
maintaining O
the O
" O
content O
" O
. O

This O
issue O
is O
particularly O
pronounced O
in O
unsupervised O
attribute B-TaskName
transfer I-TaskName
due O
to O
lack O
of O
parallel O
sentences O
between O
S O
and O
T O
. O

In O
the O
above O
process O
, O
instead O
of O
sampling O
s O
from O
a O
noise O
distribution O
like O
N O
( O
0 O
, O
I O
) O
and O
passing O
it O
through O
a O
generator O
enc O
ψ O
, O
we O
feed O
it O
text O
from O
the O
target O
domain O
T O
and O
a O
decoder O
dec O
η O
that O
decodes O
text O
in O
T O
. O

This O
is O
inspired O
from O
Cycle B-MethodName
- I-MethodName
GAN I-MethodName
( O
Zhu O
et O
al O
. O
, O
2017 O
) O
, O
where O
instead O
of O
matching O
the O
noise O
distribution O
N O
, O
we O
match O
the O
distribution O
of O
T O
. O

Next O
, O
to O
generate O
sentences O
, O
we O
consider O
two O
decodersx O
src O
∼ O
p O
φ O
( O
x|z O
) O
andx O
tgt O
∼ O
p O
η O
( O
x|z O
) O
. O

Here O
, O
z O
can O
be O
either O
z O
s O
or O
z O
t O
based O
on O
whether O
we O
autoencode O
( O
e.g. O
, O
p O
φ O
( O
x|z O
s O
= O
enc O
θ O
( O
x O
src O
) O
) O
) O
or O
translate O
( O
e.g. O
, O
p O
φ O
x|z O
t O
= O
enc O
ψ O
( O
x O
tgt O
) O
) O
. O

Unlike O
ARAE B-MethodName
's O
single B-HyperparameterValue
decoder O
, O
we O
incorporate O
two B-HyperparameterValue
decoders O
to O
enable O
bi O
- O
directional O
translation O
. O

To O
achieve O
this O
, O
we O
utilize O
enc O
θ O
to O
encode O
x O
src O
and O
repurpose O
enc O
ψ O
to O
encode O
x O
tgt O
. O

We O
obtain O
their O
latent O
codes O
( O
z O
, O
z O
) O
which O
we O
name O
as O
( O
z O
s O
, O
z O
t O
) O
, O
i.e. O
, O
z O
s O
= O
enc O
θ O
( O
x O
src O
) O
and O
z O
t O
= O
enc O
ψ O
( O
x O
tgt O
) O
. O

While O
ARAE B-MethodName
is O
an O
auto O
- O
encoder O
that O
recreates O
input O
x O
→x O
, O
our O
requirement O
is O
to O
translate O
sentences O
from O
one O
domain O
to O
another O
. O

Given O
this O
, O
we O
modify O
the O
ARAE B-MethodName
to O
a O
seq2seq O
variant O
such O
that O
we O
can O
translate O
input O
sentences O
between O
source O
and O
target O
domains O
; O
i.e. O
, O
x O
src O
→x O
tgt O
and O
x O
tgt O
→x O
src O
. O

Our O
approach O
, O
while O
simple O
and O
aimed O
at O
maintaining O
constraints O
, O
improves O
the O
overall O
performance O
of O
the O
generation O
. O

We O
demonstrate O
these O
gains O
over O
three O
datasets O
: O
YELP B-DatasetName
( O
Zhao O
et O
al O
. O
, O
2018b O
) O
, O
IMDB B-DatasetName
( O
Dai O
et O
al O
. O
, O
2019 O
) O
and O
POLITICAL B-DatasetName
( O
Prabhumoye O
et O
al O
. O
, O
2018 O
) O
, O
generating O
six O
constraints O
including O
lexical O
, O
syntactic O
and O
domainspecific O
constraints O
. O

The O
introduced O
cooperative B-HyperparameterValue
losses I-HyperparameterValue
satisfy O
the O
constraints O
more O
effectively O
compared O
against O
strong O
baselines O
. O

Since O
multiple O
attributes O
can O
change O
between O
two O
domains O
( O
Subramanian O
et O
al O
. O
, O
2018 O
) O
, O
we O
test O
our O
method O
on O
one O
such O
dataset O
and O
show O
that O
the O
constraints O
of O
identity O
are O
maintained O
more O
effectively O
( O
§ O
4.4.2 O
) O
. O

To O
the O
best O
of O
our O
knowledge O
, O
our O
approach O
is O
the O
first O
to O
introduce O
cooperative B-HyperparameterValue
losses I-HyperparameterValue
in O
a O
GAN O
- O
like O
setup O
for O
NLG.Task B-TaskName
Setup O
: O
We O
consider O
two O
sets O
of O
sentences O
( O
or O
corpora O
) O
S= O
{ O
x O
1 O
src O
, O
x O
2 O
src O
, O
. O

. O

. O

x O
m O
src O
} O
and O
T O
= O
{ O
x O
1 O
trg O
, O
x O
2 O
trg O
, O
. O

. O

. O

x O
n O
trg O
} O
, O
as O
the O
source O
and O
target O
domains O
, O
respectively O
. O

Each O
corpus O
-which O
we O
interpret O
as O
domains O
-contain O
discernable O
attributes O
, O
ranging O
from O
sentiment O
( O
e.g. O
, O
positive O
vs. O
negative O
) O
, O
topics O
, O
political O
slant O
( O
e.g. O
, O
democratic O
vs. O
republican O
) O
, O
or O
some O
combination O
( O
Li O
et O
al O
. O
, O
2018;Lample O
et O
al O
. O
, O
2019 O
) O
. O

The O
overall O
task O
is O
to O
rewrite O
a O
piece O
of O
text O
s O
i O
∈ O
S O
to O
t O
i O
∈ O
T O
, O
such O
that O
the O
translation O
changes O
the O
attributes O
varying O
across O
the O
two O
domains O
but O
retains O
the O
remaining O
content O
. O

While O
content O
retention O
is O
not O
explicitly O
defined O
in O
the O
literature O
, O
we O
design O
this O
new O
task O
of O
constrained O
unsupervised O
attribute B-TaskName
transfer I-TaskName
that O
assigns O
explicit O
constraints O
C O
= O
{ O
c O
1 O
, O
c O
2 O
, O
. O

. O

. O

, O
c O
|C| O
} O
, O
to O
be O
retained O
. O

These O
constraints O
can O
be O
defined O
at O
various O
levels O
of O
a O
sentence O
: O
lexical O
, O
syntactic O
and O
domain O
- O
specific O
. O

Regularized B-MethodName
Autoencoder I-MethodName
( O
ARAE B-MethodName
): O
To O
perform O
unsupervised O
attribute B-TaskName
transfer I-TaskName
, O
we O
consider O
seq2seq O
models O
that O
encode O
source O
sentences O
to O
a O
latent O
space O
and O
then O
decodes O
them O
to O
the O
target O
sentences O
. O

ARAEs O
( O
Zhao O
et O
al O
. O
, O
2018b O
) O
are O
the O
auto O
- O
encoder O
variants O
of O
the O
Generative O
Adversarial O
Network O
( O
GAN O
) O
( O
Goodfellow O
et O
al O
. O
, O
2014 O
) O
framework O
. O

They O
learn O
smooth O
latent O
spaces O
( O
by O
imposing O
implicit O
priors O
) O
to O
ease O
the O
sampling O
of O
latent O
sentences O
. O

ARAEs O
have O
been O
widely O
adopted O
in O
tasks O
like O
unsupervised O
text B-TaskName
generation I-TaskName
( O
Huang O
et O
al O
. O
, O
2020 O
) O
, O
topic B-TaskName
modeling I-TaskName
, O
among O
others O
, O
and O
form O
the O
backbone O
of O
our O
proposed O
model O
. O

ARAE B-MethodName
consists O
of O
an O
auto O
- O
encoder O
with O
a O
deterministic O
encoder O
enc O
θ O
: O
X O
→ O
Z O
that O
encodes O
sentences O
into O
a O
latent O
space O
; O
i.e. O
, O
z O
= O
enc O
θ O
( O
x O
) O
∼ O
P O
z O
, O
and O
a O
conditional O
decoder O
p O
φ O
( O
x|z O
) O
that O
generates O
a O
sentence O
given O
a O
latent O
code O
. O

ARAE B-MethodName
regularizes O
this O
latent O
space O
utilizing O
a O
GAN O
- O
like O
setup O
that O
includes O
an O
implicit O
prior O
obtained O
from O
a O
parameterized O
generator O
network O
enc O
ψ O
: O
N O
( O
0 O
, O
I O
) O
→ O
Z. O
Here O
, O
enc O
ψ O
maps O
a O
noise O
sample O
s O
∼ O
N O
( O
0 O
, O
I O
) O
to O
the O
corresponding O
prior O
latent O
codez O
= O
enc O
ψ O
( O
s O
) O
∼ O
Pz O
. O

In O
this O
paper O
, O
we O
improve O
unsupervised O
attribute B-TaskName
transfer I-TaskName
by O
enforcing O
invariances O
via O
explicit O
constraints O
. O

Current O
methods O
in O
text O
attribute B-TaskName
transfer I-TaskName
lack O
mechanisms O
to O
explicitly O
enforce O
such O
constraints O
between O
the O
source O
and O
the O
transferred O
sentence O
. O

To O
this O
end O
, O
we O
build O
upon O
unsupervised O
text O
style O
transfer O
work O
by O
introducing O
an O
additional O
explicit O
regularization O
component O
in O
the O
latent O
space O
of O
a O
GAN O
- O
based O
seq2seq O
network O
through O
two O
complementary O
losses O
( O
§ O
3 O
) O
. O

Unlike O
the O
adversarial O
losses O
in O
the O
GAN O
framework O
, O
our O
proposed O
losses O
cooperatively O
reduce O
the O
same O
objective O
. O

The O
first O
loss B-HyperparameterName
is O
a O
contrastive B-HyperparameterValue
loss I-HyperparameterValue
( O
Le O
- O
Khac O
et O
al O
. O
, O
2020 O
) O
that O
brings O
sentences O
that O
have O
similar O
constraints O
closer O
and O
pushes O
sentences O
that O
are O
dissimilar O
farther O
away O
. O

The O
second O
loss B-HyperparameterName
is O
a O
classification B-HyperparameterValue
loss I-HyperparameterValue
that O
helps O
maintain O
the O
sentence O
identity O
via O
constraints O
from O
the O
latent O
vectors O
( O
Odena O
et O
al O
. O
, O
2017 O
) O
. O

Text O
style B-TaskName
transfer I-TaskName
, O
a O
popular O
form O
of O
attribute B-TaskName
transfer I-TaskName
, O
regards O
" O
style O
" O
as O
any O
attribute O
that O
changes O
between O
datasets O
( O
Jin O
et O
al O
. O
, O
2020a O
) O
. O

Building O
on O
the O
progress O
of O
supervised O
transfer O
models O
, O
recent O
works O
have O
focused O
on O
unsupervised O
style B-TaskName
transfer I-TaskName
that O
avoids O
costly O
annotation O
of O
parallel O
sentences O
. O

However O
, O
models O
built O
using O
unsupervised O
methods O
perform O
poorly O
when O
compared O
to O
supervised O
( O
parallel O
) O
training O
( O
Artetxe O
et O
al O
. O
, O
2020 O
) O
. O

These O
methods O
, O
while O
capable O
of O
achieving O
the O
target O
domain O
characteristics O
, O
often O
fail O
to O
maintain O
the O
invariant O
content O
. O

Figure O
1 O
illustrates O
one O
such O
example O
, O
where O
a O
sentence O
from O
the O
BOOKS O
domain O
is O
translated O
to O
the O
MOVIE O
domain O
. O

While O
the O
translated O
sentence O
" O
Loved O
the O
movie O
" O
has O
correctly O
transferred O
the O
attribute O
( O
style O
) O
, O
it O
does O
not O
have O
the O
same O
length O
, O
does O
not O
retain O
the O
personal O
noun O
( O
" O
I O
" O
) O
, O
nor O
use O
a O
domain O
- O
appropriate O
proper O
noun O
. O

Comparatively O
, O
the O
higher O
- O
fidelity O
transfer O
" O
I O
absolutely O
enjoyed O
Spielberg O
's O
direction O
" O
, O
maintains O
such O
constraints O
of O
identity O
, O
in O
addition O
to O
being O
apt O
. O

This O
problem O
setting O
is O
an O
important O
application O
of O
text B-TaskName
transfer I-TaskName
, O
as O
enforcing O
constraints O
of O
identity O
can O
help O
maintain O
the O
brand O
identity O
when O
the O
product O
descriptions O
are O
mapped O
from O
one O
commercial O
product O
to O
another O
. O

They O
can O
also O
help O
in O
data O
augmentation O
for O
downstream O
domain O
adaptation O
NLP O
applications O
( O
§ O
5 O
) O
. O

Constraints O
of O
identity O
are O
explored O
extensively O
in O
the O
computer O
vision O
task O
of O
cross O
- O
domain O
image O
generation O
. O

( O
Taigman O
et O
al O
. O
, O
2017 O
) O
, O
but O
these O
issues O
- O
to O
the O
best O
of O
our O
knowledge O
- O
are O
unexplored O
in O
NLP O
. O

where O
M O
q O
, O
M O
k O
are O
a O
row O
- O
wise O
concatenated O
question O
words O
and O
knowledge O
entities O
, O
W O
[ O
• O
] O
is O
learn O
- O
Automatic O
transfer O
of O
text O
between O
domains O
has O
become O
popular O
in O
recent O
times O
. O

One O
of O
its O
aims O
is O
to O
preserve O
the O
semantic O
content O
of O
text O
being O
translated O
from O
source O
to O
target O
domain O
. O

However O
, O
it O
does O
not O
explicitly O
maintain O
other O
attributes O
between O
the O
source O
and O
translated O
text O
, O
for O
e.g. O
, O
text O
length O
and O
descriptiveness O
. O

Maintaining O
constraints O
in O
transfer O
has O
several O
downstream O
applications O
, O
including O
data B-TaskName
augmentation I-TaskName
and O
de B-TaskName
- I-TaskName
biasing I-TaskName
. O

We O
introduce O
a O
method O
for O
such O
constrained O
unsupervised O
text O
style O
transfer O
by O
introducing O
two O
complementary O
losses O
to O
the O
generative O
adversarial O
network O
( O
GAN O
) O
family O
of O
models O
. O

Unlike O
the O
competing O
losses O
used O
in O
GANs O
, O
we O
introduce O
cooperative B-HyperparameterValue
losses I-HyperparameterValue
where O
the O
discriminator O
and O
the O
generator O
cooperate O
and O
reduce O
the O
same O
loss O
. O

The O
first O
is O
a O
contrastive B-HyperparameterValue
loss I-HyperparameterValue
and O
the O
second O
is O
a O
classification B-HyperparameterValue
loss I-HyperparameterValue
-aiming O
to O
regularize O
the O
latent O
space O
further O
and O
bring O
similar O
sentences O
across O
domains O
closer O
together O
. O

We O
demonstrate O
that O
such O
training O
retains O
lexical O
, O
syntactic O
, O
and O
domain O
- O
specific O
constraints O
between O
domains O
for O
multiple O
benchmark O
datasets O
, O
including O
ones O
where O
more O
than O
one O
attribute O
change O
. O

We O
show O
that O
the O
complementary B-HyperparameterValue
cooperative I-HyperparameterValue
losses I-HyperparameterValue
improve O
text O
quality O
, O
according O
to O
both O
automated O
and O
human O
evaluation O
measures O
. O

1 O

) O
where O
i O
and O
j O
are O
a B-HyperparameterValue
single I-HyperparameterValue
layer I-HyperparameterValue
feed B-HyperparameterName
- I-HyperparameterName
forward I-HyperparameterName
layer I-HyperparameterName
, O
respectively O
. O

Then O
, O
the O
two O
aggregated O
graph O
representations O
are O
concatenated O
and O
fed O
into O
another O
single B-HyperparameterValue
layer I-HyperparameterValue
feed B-HyperparameterName
- I-HyperparameterName
forward I-HyperparameterName
layer I-HyperparameterName
to O
get O
joint O
representation O
of O
question O
and O
knowledge O
graph O
. O

We O
reproduce O
end O
- O
to O
- O
end O
memory O
networks O
( O
Sukhbaatar O
et O
al O
. O
, O
2015 O
) O
proposed O
as O
a O
baseline O
model O
in O
. O

First O
, O
we O
use O
Bag O
- O
of O
- O
words O
( O
BoW O
) O
representation O
for O
knowledge O
facts O
and O
a O
question O
. O

The O
soft O
attention O
over O
the O
knowledge O
facts O
and O
the O
given O
question O
is O
computed O
as O
follows O
: O
p O
ij O
= O
softmax(q O
T O
i−1 O
m O
ij O
) O
where O
m O
is O
the O
embeddings O
of O
knowledge O
facts O
, O
i O
is O
a O
number O
of O
layer O
and O
j O
is O
an O
index O
of O
knowledge O
facts O
. O

The O
output O
representation O
of O
i O
- O
th O
layer O
is O
O O
i O
= O
j O
p O
ij O
o O
ij O
where O
o O
is O
the O
another O
embeddings O
of O
knowledge O
facts O
different O
from O
m. O
The O
updated O
question O
representation O
is O
q O
k+1 O
= O
O O
k+1 O
+ O
q O
k O
, O
and O
based O
on O
the O
output O
representation O
and O
question O
representation O
, O
answer O
is O
predicted O
as O
follows O
: O
a O
= O
softmax(f O
( O
O O
K O
+ O
q O
K−1 O
) O
) O
where O
f O
is O
a B-HyperparameterValue
single I-HyperparameterValue
layer I-HyperparameterValue
feed B-HyperparameterName
- I-HyperparameterName
forward I-HyperparameterName
layer I-HyperparameterName
. O

Here O
, O
we O
set O
up O
the O
model O
as O
three B-HyperparameterValue
layers O
with O
adjacent O
and O
layer O
- O
wise O
weight O
tying O
. O

Bilinear B-MethodName
attention I-MethodName
networks I-MethodName
exploit O
a O
multi O
- O
head O
co O
- O
attention O
mechanism O
between O
knowledge O
and O
question O
. O

BAN B-MethodName
calculates O
soft O
attention O
scores O
between O
knowledge O
entities O
and O
question O
words O
as O
follows O
: O

) O
. O
After O
the O
propagation O
phase O
, O
the O
nodes O
in O
the O
graph O
are O
aggregated O
to O
a O
graph O
- O
level O
representation O
as O
h O
G O
= O
tanh B-HyperparameterValue
( O
v∈V O
σ(i(h O

) O
where O
σ B-HyperparameterName
is O
a O
logistic B-HyperparameterValue
sigmoid I-HyperparameterValue
function I-HyperparameterValue
, O
and O
W O
[ O
• O
] O
and O
U O
[ O
• O
] O
are O
learnable O
parameters O
. O

Finally O
, O
the O
hidden O
states O
of O
nodes O
in O
the O
given O
graph O
are O
updates O
as O
h O

] O
T O
+ O
b O
where O
the O
matrix O
A O
determines O
how O
nodes O
in O
the O
graph O
communicate O
each O
other O
and O
b O
is O
a O
bias O
vector O
. O

Then O
, O
the O
update O
gate O
and O
reset O
gate O
are O
computed O
as O
follows O
: O

T O
where O
x O
v O
is O
the O
v O
- O
th O
word O
embedding O
of O
each O
en O
- O
tity O
in O
the O
knowledge O
and O
question O
graph O
, O
a O

where O
the O
subscript O
i O
denotes O
the O
i O
- O
th O
index O
of O
column O
vectors O
in O
each O
matrix O
. O

For O
multi O
- O
head O
attention O
, O
the O
attended O
outputs O
with O
different O
heads O
are O
concatenated O
and O
fed O
into O
a B-HyperparameterValue
single I-HyperparameterValue
layer I-HyperparameterValue
feedforward B-HyperparameterName
layer I-HyperparameterName
to O
make O
a O
final O
representation O
. O

Here O
, O
we O
use O
four O
attention O
heads O
as O
multi O
- O
head O
. O

The O
model O
architecture O
and O
detailed O
operation O
of O
hypergraph B-MethodName
attention I-MethodName
networks I-MethodName
are O
similar O
to O
that O
of O
BAN B-MethodName
. O

The O
difference O
between O
BAN B-MethodName
and O
HAN B-MethodName
is O
the O
abstraction B-HyperparameterName
level I-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
input I-HyperparameterName
. O

For O
HAN B-MethodName
, O
the O
hyperedges O
sampled O
by O
stochastic O
graph O
walk O
are O
fed O
into O
the O
co O
- O
attention O
mechanism O
. O

What O
HAN B-MethodName
and O
our O
model O
have O
in O
common O
is O
introducing O
a O
hypergraph O
to O
consider O
high O
- O
order O
relationships O
in O
question O
graph O
and O
knowledge O
graph O
. O

Both O
models O
share O
the O
similar O
motivation O
, O
but O
the O
core O
operations O
are O
quite O
different O
. O

Especially O
, O
HAN B-MethodName
employs O
stochastic O
graph O
walk O
to O
construct O
question O
and O
knowledge O
hypergraph O
. O

Due O
to O
the O
randomness O
of O
the O
stochasticity O
, O
misinformed O
or O
incomplete O
hyperedges O
can O
be O
extracted O
. O

The O
model O
architectures O
of O
Transformer B-MethodName
( I-MethodName
SA I-MethodName
) I-MethodName
and O
Transformer B-MethodName
( I-MethodName
SA+GA I-MethodName
) I-MethodName
presented O
in O
this O
paper O
are O
the O
same O
as O
Hypergraph B-MethodName
Transformer I-MethodName
. O

The O
only O
difference O
is O
the O
abstraction O
level O
of O
input O
. O

The O
Transformer B-MethodName
( I-MethodName
SA I-MethodName
) I-MethodName
and O
Transformer B-MethodName
( I-MethodName
SA+GA I-MethodName
) I-MethodName
take O
single B-HyperparameterValue
- I-HyperparameterValue
word I-HyperparameterValue
- I-HyperparameterValue
unit I-HyperparameterValue
as O
input B-HyperparameterName
tokens I-HyperparameterName
, O
and O
Hypergraph B-MethodName
Transformer I-MethodName
takes O
hyperedges O
as O
input O
tokens O
. O

Following O
( O
Vaswani O
et O
al O
. O
, O
2017;Tsai O
et O
al O
. O
, O
2019 O
) O
, O
we O
apply O
positional O
embeddings O
to O
the O
input O
sequence O
of O
both O
models O
. O

We O
stack O
two B-HyperparameterValue
guided B-HyperparameterName
- I-HyperparameterName
attention I-HyperparameterName
blocks I-HyperparameterName
and O
three B-HyperparameterValue
self B-HyperparameterName
- I-HyperparameterName
attention I-HyperparameterName
blocks I-HyperparameterName
, O
respectively O
. O

Each O
attention O
block O
has O
multi O
- O
head O
attention O
with O
four B-HyperparameterValue
attention B-HyperparameterName
heads I-HyperparameterName
followed O
by O
layer O
normalization O
, O
residual O
connections O
and O
a O
single O
multi O
- O
layer O
perceptron O
. O

We O
set O
the O
dropout B-HyperparameterName
applied O
on O
the O
token O
embedding O
weights O
, O
query O
and O
key O
- O
value O
embedding O
weights O
, O
attention O
weights O
and O
residual O
connections O
from O
0.05 B-HyperparameterValue
to O
0.2 B-HyperparameterValue
. O

We O
minimize O
negative O
log O
- O
likelihood O
using O
Adam B-HyperparameterValue
optimizer B-HyperparameterName
( O
Kingma O
and O
Ba O
, O
2015 O
) O
with O
an O
initial B-HyperparameterName
learning I-HyperparameterName
rate I-HyperparameterName
from O
1e B-HyperparameterValue
− I-HyperparameterValue
4 I-HyperparameterValue
to O
1e B-HyperparameterValue
− I-HyperparameterValue
5 I-HyperparameterValue
with O
batch B-HyperparameterName
size I-HyperparameterName
from O
128 B-HyperparameterValue
to O
256 B-HyperparameterValue
. O

All O
transformer O
variant O
models O
described O
in O
this O
paper O
have O
the O
same O
fixed O
- O
number B-HyperparameterName
of I-HyperparameterName
sequence I-HyperparameterName
length I-HyperparameterName
as O
follows O
: O
300 B-HyperparameterValue
for O
1 O
- O
hop O
, O
1,000 B-HyperparameterValue
for O
2 O
- O
hop O
and O
1,800 B-HyperparameterValue
for O
3 O
- O
hop O
graphs O
. O

We O
would O
like O
to O
thank O
Woo O
Young O
Kang O
, O
Kyoung O
- O
Woon O
On O
, O
Seonil O
Son O
, O
Gi O
- O
Cheon O
Kang O
, O
Christina O
Baek O
, O
Junseok O
Park O
, O
Min O
Whoo O
Lee O
, O
Hwiyeol O
Jo O
and O
Sang O
- O
Woo O
Lee O
for O
their O
helpful O
comments O
and O
discussion O
. O

This O
work O
was O
partly O
supported O
by O
the O
IITP O
( O
2015 O
- O
0 O
- O
00310 O
- O
SW.StarLab/20 O
% O
, O
2017 O
- O
0 O
- O
01772 O
- O
VTT/20 O
% O
, O
2019 O
- O
0 O
- O
01371 O
- O
BabyMind/10 O
% O
, O
2021 O
- O
0 O
- O
02068 O
- O
AIHub/10 O
% O
, O
2021 O
- O
0 O
- O
01343 O
- O
GSAI/10 O
% O
, O
2020 O
- O
0 O
- O
01373/10 O
% O
) O
grants O
, O
the O
KIAT O
( O
P0006720 O
- O
ILIAS/10 O
% O
) O
grant O
funded O
by O
the O
Korean O
government O
, O
and O
the O
Hanyang O
University O
( O
HY-202100000003160/10%).As O
the O
same O
as O
graph B-MethodName
convolutional I-MethodName
networks I-MethodName
, O
the O
knowledge O
and O
question O
graph O
are O
encoded O
separately O
by O
two B-MethodName
gated I-MethodName
graph I-MethodName
neural I-MethodName
networks I-MethodName
( O
GGNN B-MethodName
) O
. O

Each O
GGNN B-MethodName
model O
consists O
of O
three B-HyperparameterValue
gated B-HyperparameterName
recurrent I-HyperparameterName
propagation I-HyperparameterName
layers I-HyperparameterName
and O
a O
graphlevel O
aggregator O
. O

Motivated O
by O
Gated O
Recurrent O
Units O
( O
Cho O
et O
al O
. O
, O
2014 O
) O
, O
GGNN B-MethodName
adopts O
a O
update O
gate O
and O
a O
reset O
gate O
to O
renew O
each O
node O
's O
hidden O
state O
. O

The O
detailed O
equation O
of O
gated O
recurrent O
propagation O
is O
as O
follows O
: O
h O

i O
= O
( O
M O
q O
W O
q O
) O
i O
⊤ O
A(M O
k O
W O
k O
) O
i O

Here O
, O
H O
( O
0 O
) O
is O
the O
word O
embeddings O
of O
each O
entity O
in O
the O
knowledge O
and O
question O
graph O
. O

After O
propagation O
and O
aggregation O
phase O
, O
the O
knowledge O
and O
question O
graph O
representations O
are O
obtained O
. O

Then O
, O
the O
two O
graph O
representations O
are O
concatenated O
and O
fed O
into O
a B-HyperparameterValue
single I-HyperparameterValue
layer I-HyperparameterValue
feed B-HyperparameterName
- I-HyperparameterName
forward I-HyperparameterName
layer I-HyperparameterName
to O
get O
joint O
representation O
. O

able O
matrices O
, O
and O
• O
is O
element O
- O
wise O
multiplication O
. O

Based O
on O
the O
attention O
map O
A O
, O
the O
joint O
feature O
is O
obtained O
as O
follows O
: O
z O

Appendix O
. O
This O
supplementary O
material O
provides O
additional O
information O
not O
described O
in O
the O
main O
text O
due O
to O
the O
page O
limit O
. O

The O
contents O
of O
this O
appendix O
are O
as O
follows O
: O
In O
Section O
A O
, O
we O
show O
the O
detailed O
statistics O
for O
the O
diverse O
splits O
of O
four O
benchmark O
datasets O
, O
i.e. O
, O
KVQA B-DatasetName
, O
FVQA B-DatasetName
, O
PQ B-DatasetName
and O
PQL B-DatasetName
. O

In O
Section O
B O
and O
C O
, O
we O
present O
the O
additional O
quantitative O
and O
qualitative O
analyses O
on O
KVQA B-DatasetName
and O
PQ B-DatasetName
datasets O
, O
respectively O
. O

In O
Section O
D O
, O
we O
describe O
the O
experimental O
details O
for O
each O
dataset O
. O

In O
Section O
E O
, O
we O
depict O
the O
implementation O
details O
of O
comparative O
models O
for O
KVQA.The B-DatasetName
diverse O
split B-HyperparameterName
statistics I-HyperparameterName
for O
four O
benchmark O
datasets O
, O
KVQA B-DatasetName
, O
FVQA B-DatasetName
( O
Wang O
et O
al O
. O
, O
2018 O
) O
, O
PQ B-DatasetName
and O
PQL B-DatasetName
( O
Zhou O
et O
al O
. O
, O
2018 O
) O
, O
are O
shown O
in O
Table O
4 O
. O

Here O
, O
we O
highlight O
four O
aspects O
as O
follows O
: O
1 O
) O
KVQA B-DatasetName
dataset O
covers O
the O
large O
number O
of O
entities O
( O
at O
least O
5 O
times O
more O
) O
and O
knowledge O
facts O
( O
at O
least O
17 O
times O
more O
) O
than O
FVQA B-DatasetName
, O
PQ B-DatasetName
and O
PQL B-DatasetName
. O

2 O
) O
PQ B-DatasetName
and O
PQL B-DatasetName
datasets O
have O
annotations O
of O
a O
ground O
- O
truth O
reasoning O
path O
to O
answer O
a O
given O
question O
. O

2H B-HyperparameterValue
and O
3H B-HyperparameterValue
denote O
the O
number B-HyperparameterName
of I-HyperparameterName
hops I-HyperparameterName
( O
i.e. O
, O
2 B-HyperparameterValue
- I-HyperparameterValue
hop I-HyperparameterValue
and O
3 B-HyperparameterValue
- I-HyperparameterValue
hop I-HyperparameterValue
) O
in O
ground O
- O
truth O
reasoning O
paths O
. O

Also O
, O
M O
denotes O
a O
mixture O
of O
the O
2H O
and O
3H O
questions O
. O

3 O
) O
PQL B-DatasetName
covers O
more O
knowledge O
facts O
including O
a O
large O
number O
of O
entities O
and O
relations O
than O
PQ B-DatasetName
, O
but O
has O
fewer O
QA O
pairs O
. O

4 O
) O
PQL-3H B-DatasetName
has O
a O
quite O
limited O
number O
of O
QA O
pairs O
( O
1,031 O
) O
. O

PQL-3H B-DatasetName
- I-DatasetName
More I-DatasetName
has O
twice O
more O
QA O
pairs O
( O
2,062 O
) O
with O
the O
same O
number O
of O
entities O
, O
relations O
, O
knowledge O
facts O
and O
answers O
as O
PQL-3H.Here B-DatasetName
, O
we O
analyze O
more O
in O
- O
depth O
on O
KVQA B-DatasetName
dataset O
concerning O
i O
) O
categories O
of O
question O
, O
and O
ii O
) O
types O
of O
answer O
selector O
. O

All O
models O
are O
under O
the O
same O
setting O
of O
ORG+3 B-HyperparameterValue
- I-HyperparameterValue
hop I-HyperparameterValue
reported O
in O
Table O
1.We O
analyze O
QA B-TaskName
performances O
over O
different O
question O
categories O
in O
Table O
5 O
. O

Hypergraph B-MethodName
Transformer I-MethodName
achieves O
the O
best O
accuracy B-MetricName
in O
all O
categories O
except O
Multi O
- O
hop O
( O
slightly O
low O
at O
second O
- O
best O
) O
. O

Our O
model O
shows O
notable O
strengths O
especially O
on O
complex O
problems O
such O
as O
Comparison O
, O
Multi O
- O
entity O
or O
Subtraction O
. O

To O
draw O
inferences O
for O
these O
question O
categories O
, O
the O
model O
needs O
to O
attend O
to O
multiple O
knowledge O
facts O
related O
to O
a O
given O
question O
, O
and O
conducts O
multi O
- O
hop O
reasoning O
based O
on O
the O
facts O
. O

Also O
, O
our O
model O
shows O
significant O
improvement O
in O
spatial O
question O
compared O
to O
other O
models O
. O

Whereas O
spatial O
question O
is O
quite O
simple O
, O
it O
is O
required O
to O
understand O
a O
correct O
spatial O
relationship O
between O
multiple O
entities O
in O
a O
given O
image O
. O

Examples O
of O
QA B-TaskName
on O
diverse O
question O
categories O
are O
depicted O
in O
Figure O
4 O
. O

Answers O
, O
inferred O
by O
five O
comparative O
models O
and O
the O
proposed O
model O
, O
are O
presented O
with O
corresponding O
image O
and O
question O
. O

The O
qualitative O
results O
indicate O
that O
our O
model O
draws O
reasonable O
inferences O
across O
diverse O
question O
categories O
. O

To O
validate O
the O
impact O
of O
similarity O
- O
based O
answer O
selector O
, O
we O
replace O
the O
similarity O
- O
based O
answer O
selector O
( O
SIM O
) O
with O
a O
multi O
- O
layer O
perceptron O
( O
MLP O
) O
. O

We O
first O
note O
that O
KVQA B-DatasetName
dataset O
includes O
a O
large O
number O
of O
unique O
answers O
( O
19,360 O
) O
, O
and O
contains O
a O
lot O
of O
zero O
- O
shot O
and O
few O
- O
shot O
answers O
in O
test O
phase O
. O

As O
shown O
in O
Table O
6 O
, O
the O
MLP O
fails O
to O
infer O
zeroshot O
answers O
which O
are O
not O
appeared O
in O
the O
training O
phase O
at O
all O
. O

Besides O
, O
the O
performance O
difference O
between O
SIM O
and O
MLP O
in O
one O
- O
shot O
answer O
( O
appeared O
in O
the O
only O
one O
time O
in O
training O
phase O
) O
is O
more O
than O
18 O
% O
. O

The O
MLP O
uses O
17 O
% O
more O
parameters O
than O
SIM O
because O
KVQA O
has O
a O
large O
number O
of O
answer O
candidates O
( O
19,360 O
) O
. O

When O
the O
number O
of O
candidate O
answers O
increases O
, O
the O
MLP O
needs O
more O
parameters O
, O
but O
SIM O
does O
not O
. O

To O
sum O
up O
, O
the O
similarity O
- O
based O
answer O
selector O
( O
SIM O
) O
contributes O
to O
infer O
few O
- O
shot O
and O
zero O
- O
shot O
answers O
in O
parameter O
- O
efficient O
manner O
. O

is O
wrong O
even O
though O
it O
attends O
correctly O
to O
the O
first O
knowledge O
hyperedge O
{ O
Wallace O
Reid O
⪯ O
spouse O
⪯ O
Dorothy O
Davenport O
⪯ O
parents O
⪯ O
Harry O
Davenport O
⪯ O
cause O
of O
death O
⪯ O
Myocardial O
Infarction O
} O
. O

However O
, O
Transformer B-MethodName
( I-MethodName
SA+GA I-MethodName
) I-MethodName
attends O
to O
only O
the O
second O
and O
seventh O
word O
( O
Dorothy O
Davenport O
) O
and O
the O
fourth O
and O
ninth O
word O
( O
Harry O
Davenport O
) O
in O
knowledge O
with O
high O
attention O
score O
, O
not O
the O
answer O
entity O
, O
Myocardial O
Infarction O
. O

We O
consider O
that O
the O
reason O
why O
Hypergraph B-MethodName
Transformer I-MethodName
failed O
to O
infer O
the O
correct O
answer O
despite O
focusing O
on O
the O
exact O
knowledge O
fact O
is O
that O
the O
correct O
answer O
word O
( O
Myocardial O
Infarction O
) O
appears O
rarely O
in O
QA O
pairs O
. O

We O
follow O
the O
experimental O
settings O
suggested O
in O
. O

For O
entity O
linking O
, O
we O
apply O
well O
- O
known O
pre O
- O
trained O
models O
for O
face B-TaskName
identification I-TaskName
: O
RetinaFace B-MethodName
( O
Deng O
et O
al O
. O
, O
2020 O
) O
for O
face B-TaskName
detection I-TaskName
and O
ArcFace B-MethodName
( O
Deng O
et O
al O
. O
, O
2019 O
) O
for O
face B-TaskName
feature I-TaskName
extraction I-TaskName
. O

We O
first O
assign O
a O
name O
of O
the O
detected O
faces O
with O
the O
label O
of O
the O
closest O
distance O
compared O
to O
all O
of O
the O
face O
embeddings O
of O
18,880 O
named O
entities O
. O

In O
addition O
, O
we O
refine O
a O
list O
of O
de O
- O
tected O
named O
entities O
by O
matching O
the O
associated O
image O
caption O
( O
i.e. O
, O
Wikipedia O
caption O
) O
. O

By O
doing O
so O
, O
we O
obtain O
the O
result O
of O
entity O
linking O
with O
top-1 O
precision B-MetricName
65.0 B-MetricValue
% I-MetricValue
and O
top-1 O
recall B-MetricName
72.8 B-MetricValue
% I-MetricValue
. O

QA B-TaskName
performances O
in O
the O
entity O
linking O
setting O
on O
KVQA B-DatasetName
are O
shown O
in O
Table O
7 O
. O

Here O
, O
we O
note O
that O
BLSTM B-MethodName
and O
MemNN B-MethodName
of O
the O
first O
section O
in O
the O
table O
are O
based O
on O
the O
different O
entity O
linking O
modules O
with O
top-1 O
precision B-MetricName
81.1 B-MetricValue
% I-MetricValue
and O
top-1 O
recall B-MetricName
82.2 B-MetricValue
% I-MetricValue
1 O
. O

It O
is O
more O
accurate O
than O
ours O
around O
9.4 B-MetricValue
% I-MetricValue
in O
the O
recall B-MetricName
metric O
. O

We O
follow O
the O
experimental O
settings O
suggested O
in O
( O
Wang O
et O
al O
. O
, O
2018 O
) O
. O

Following O
the O
paper O
, O
the O
dataset O
provides O
five O
splits O
of O
train O
and O
test O
data O
. O

We O
report O
the O
average B-MetricName
accuracy I-MetricName
of O
five O
repeated O
runs O
on O
different O
data O
split O
: O
76.55 B-MetricValue
as O
top-1 O
accuracy O
( O
average O
of O
76.93 B-MetricValue
, O
75.92 B-MetricValue
, O
76.24 B-MetricValue
, O
76.16 B-MetricValue
, O
and O
77.50 B-MetricValue
) O
and O
82.20 B-MetricValue
as O
top-3 O
accuracy B-MetricName
( O
average O
of O
82.90 B-MetricValue
, O
81.45 B-MetricValue
, O
81.70 B-MetricValue
, O
81.74 B-MetricValue
and O
83.20 B-MetricValue
) O
. O

The O
experimental O
results O
are O
shown O
in O
Table O
8 O
. O

We O
follow O
the O
same O
experimental O
settings O
suggested O
in O
( O
Zhou O
et O
al O
. O
, O
2018 O
) O
. O

Following O
the O
paper O
, O
we O
split B-HyperparameterName
the O
dataset O
into O
train O
, O
validation O
, O
and O
test O
sets O
with O
a O
proportion O
of O
8:1:1 B-HyperparameterValue
, O
and O
report O
the O
average B-MetricName
accuracy I-MetricName
of O
five O
repeated O
runs O
on O
different O
data O
split O
. O

( O
Kim O
et O
al O
. O
, O
2018 O
) O
and O
hypergraph B-MethodName
attention I-MethodName
networks I-MethodName
( O
HAN B-MethodName
) O
( O
Kim O
et O
al O
. O
, O
2020 O
) O
consider O
interactions O
between O
knowledge O
and O
question O
based O
on O
co O
- O
attention O
mechanism O
. O

BAN B-MethodName
calculates O
soft O
attention O
scores O
between O
knowledge O
entities O
and O
question O
words O
. O

Meanwhile O
, O
HAN B-MethodName
employs O
stochastic O
graph O
walk O
in O
a O
knowledge O
and O
question O
graph O
to O
encode O
high O
- O
order O
semantics O
( O
e.g. O
, O
knowledge O
facts O
and O
question O
phrases O
) O
, O
and O
considers O
attention O
scores O
between O
knowledge O
facts O
and O
question O
phrases O
. O

Joint O
representation O
is O
obtained O
based O
on O
the O
attention O
as O
well O
. O

The O
more O
implementation O
details O
of O
the O
above O
comparative O
models O
is O
described O
as O
follows O
. O

The O
knowledge O
and O
question O
graph O
are O
encoded O
separately O
by O
two O
graph B-MethodName
convolutional I-MethodName
networks I-MethodName
( O
GCN B-MethodName
) O
( O
Kipf O
and O
Welling O
, O
2017 O
) O
. O

Each O
GCN B-MethodName
model O
consists O
of O
two B-HyperparameterValue
propagation B-HyperparameterName
layers I-HyperparameterName
and O
a B-HyperparameterValue
sum B-HyperparameterName
pooling I-HyperparameterName
layer I-HyperparameterName
across O
the O
nodes O
in O
the O
graph O
. O

The O
operation O
of O
the O
propagation B-HyperparameterName
layer I-HyperparameterName
is O
as O
follows O
: O
f O
( O
H O
( O
l O
) O
, O
A O
) O
= O
σ(D O
− O
1 O
2ÂD O
− O
1 O
2 O
H O
( O
l O
) O
W O
( O
l O
) O
) O
wherê O
A O
= O
A O
+ O
I O
, O
A O
is O
an O
adjacency O
matrix O
of O
the O
graph O
, O
I O
is O
an O
identity O
matrix O
, O
D O
is O
a O
degree O
matrix O
of O
A O
, O
W O
( O
l O
) O
is O
the O
model O
parameters O
of O
l O
- O
th O
layer O
, O
and O
H O
( O
l O
) O
is O
the O
representations O
of O
the O
graph O
in O
the O
l O
- O
th O
layer O
. O

When O
hypergraph B-HyperparameterValue
- I-HyperparameterValue
based I-HyperparameterValue
representations B-HyperparameterName
are O
used O
for O
both O
knowledge O
and O
question O
, O
the O
results O
show O
the O
best O
performance O
across O
all O
settings O
over O
question O
types O
( O
ORG O
and O
PRP O
) O
and O
a O
number O
of O
graph B-HyperparameterName
walk I-HyperparameterName
( O
1 B-HyperparameterValue
- I-HyperparameterValue
hop I-HyperparameterValue
, O
2 B-HyperparameterValue
- I-HyperparameterValue
hop I-HyperparameterValue
, O
and O
3 B-HyperparameterValue
- I-HyperparameterValue
hop I-HyperparameterValue
) O
. O

As O
shown O
in O
Table O
3 O
, O
the O
mean B-MetricName
accuracy I-MetricName
of O
QA B-TaskName
achieves O
89.7 B-MetricValue
% I-MetricValue
when O
both O
are O
encoded O
using O
hyperedges O
, O
while O
using O
single B-HyperparameterValue
- I-HyperparameterValue
word I-HyperparameterValue
- I-HyperparameterValue
unit I-HyperparameterValue
- I-HyperparameterValue
based I-HyperparameterValue
representation B-HyperparameterName
causes O
performance O
to O
drop O
to O
81.6 B-MetricValue
% I-MetricValue
. O

Especially O
, O
when O
we O
convert O
the O
one O
of O
both O
hyperedge B-HyperparameterValue
- I-HyperparameterValue
level I-HyperparameterValue
representation B-HyperparameterName
to O
single B-HyperparameterValue
- I-HyperparameterValue
word I-HyperparameterValue
- I-HyperparameterValue
unit I-HyperparameterValue
- I-HyperparameterValue
based I-HyperparameterValue
representation B-HyperparameterName
, O
the O
mean B-MetricName
accuracy I-MetricName
of O
QA B-TaskName
is O
82.7 B-MetricValue
% I-MetricValue
and O
88.7 B-MetricValue
% I-MetricValue
, O
respectively O
. O

These O
results O
validate O
that O
it O
is O
meaningful O
to O
consider O
not O
only O
knowledge O
but O
also O
question O
as O
hypergraphs O
. O

Effect O
of O
multi O
- O
hop O
graph O
walk O
We O
compare O
the O
performances O
with O
different O
number B-HyperparameterName
of I-HyperparameterName
graph I-HyperparameterName
walks I-HyperparameterName
used O
to O
construct O
a O
knowledge O
hypergraph O
( O
i.e. O
, O
1 B-HyperparameterValue
- I-HyperparameterValue
hop I-HyperparameterValue
, O
2 B-HyperparameterValue
- I-HyperparameterValue
hop I-HyperparameterValue
, O
and O
3 B-HyperparameterValue
- I-HyperparameterValue
hop I-HyperparameterValue
) O
. O

All O
models O
except O
ours O
show O
slightly O
lower O
performance O
on O
the O
3 B-HyperparameterValue
- I-HyperparameterValue
hop I-HyperparameterValue
graph O
than O
on O
the O
2 B-HyperparameterValue
- I-HyperparameterValue
hop I-HyperparameterValue
graph O
. O

We O
observe O
that O
the O
number B-HyperparameterName
of I-HyperparameterName
extracted I-HyperparameterName
knowledge I-HyperparameterName
facts O
increases O
when O
the O
number B-HyperparameterName
of I-HyperparameterName
graph I-HyperparameterName
walk I-HyperparameterName
increases O
, O
and O
unnecessary O
facts O
for O
answering O
a O
given O
question O
are O
usually O
included O
. O

Nonetheless O
, O
our O
model O
shows O
robust O
reasoning O
performance O
when O
a O
large O
and O
noisy O
knowledge O
facts O
are O
given O
. O

To O
investigate O
the O
impacts O
of O
each O
attention O
block O
( O
i.e. O
, O
GA O
and O
SA O
) O
, O
ablation O
studies O
are O
shown O
in O
Table O
3(e O
- O
g O
) O
. O

The O
scores O
across O
all O
settings O
drop O
when O
GA O
or O
SA O
is O
removed O
. O

Particularly O
, O
the O
mean B-MetricName
accuracy I-MetricName
of O
QA B-TaskName
is O
decreased O
by O
6.0 B-MetricValue
% I-MetricValue
( O
89.7 B-MetricValue
% I-MetricValue
→ O
83.7 B-MetricValue
% I-MetricValue
) O
, O
2.6 B-MetricValue
% I-MetricValue
( O
89.7 B-MetricValue
% I-MetricValue
→ O
87.1 B-MetricValue
% I-MetricValue
) O
for O
cutting O
out O
the O
GA O
and O
the O
SA O
block O
, O
respectively O
. O

Based O
on O
the O
two O
experiments O
, O
we O
identify O
that O
not O
only O
the O
guided O
- O
attention O
which O
captures O
inter O
- O
relationships O
between O
question O
and O
knowledge O
but O
also O
the O
selfattention O
which O
learns O
intra O
- O
relationship O
in O
them O
are O
crucial O
to O
the O
complex B-TaskName
QA I-TaskName
. O

To O
sum O
up O
, O
Hypergraph B-MethodName
Transformer I-MethodName
takes O
graph O
- O
level O
inputs O
, O
i.e. O
, O
hyperedge O
, O
and O
conducts O
semantic O
matching O
between O
hyperedges O
by O
the O
attention O
mechanism O
. O

Due O
to O
the O
two O
characteristics O
, O
the O
model O
shows O
better O
reasoning O
performance O
focusing O
on O
the O
evidences O
necessary O
for O
reasoning O
under O
weak O
supervision O
. O

Figure O
3 O
provides O
the O
qualitative O
analysis O
on O
effectiveness O
of O
using O
a O
hypergraph O
as O
an O
input O
format O
to O
Transformer O
architecture O
. O

We O
present O
the O
attention O
map O
from O
the O
guided O
- O
attention O
block O
, O
and O
visualize O
top O
- O
k O
attended O
knowledge O
facts O
or O
entities O
with O
the O
attention O
scores O
. O

In O
the O
first O
example O
, O
both O
model O
, O
Hypergraph B-MethodName
Transformer I-MethodName
and O
Transformer B-MethodName
( I-MethodName
SA+GA I-MethodName
) I-MethodName
, O
infer O
the O
correct O
answer O
, O
Q5075293 O
. O

Our O
model O
responds O
by O
focusing O
on O
{ O
second O
⪯ O
from O
⪯ O
left O
} O
phrase O
of O
the O
question O
and O
four O
facts O
having O
a O
left O
relation O
among O
86 O
knowledge O
hyperedges O
. O

In O
comparison O
, O
Transformer B-MethodName
( I-MethodName
SA+GA I-MethodName
) I-MethodName
strongly O
attends O
to O
the O
knowledge O
entities O
which O
appear O
repetitive O
in O
the O
knowledge O
facts O
. O

Especially O
, O
the O
model O
attends O
to O
Q3476753 O
, O
Q290666 O
and O
Ireland O
with O
the O
high O
attention O
score O
0.237 O
, O
0.221 O
, O
and O
0.202 O
. O

In O
the O
second O
example O
, O
our O
model O
attends O
to O
the O
correct O
knowledge O
hyperedges O
considering O
the O
multi O
- O
hop O
facts O
about O
place O
of O
birth O
of O
the O
people O
shown O
in O
the O
given O
image O
, O
and O
infers O
the O
correct O
answer O
. O

On O
the O
other O
hand O
, O
Transformer B-MethodName
( I-MethodName
SA+GA I-MethodName
) I-MethodName
strongly O
attends O
to O
the O
knowledge O
entity O
of O
person O
( O
Q2439789 O
) O
presented O
in O
the O
image O
with O
undesired O
attention O
score O
0.788 O
. O

The O
second O
and O
third O
attended O
knowledge O
entities O
are O
the O
other O
person O
( O
Q7141361 O
) O
and O
Iran O
. O

Transformer B-MethodName
( I-MethodName
SA+GA I-MethodName
) I-MethodName
fails O
to O
focus O
on O
the O
multi O
- O
hop O
facts O
required O
to O
answer O
the O
given O
question O
and O
predicts O
the O
answer O
with O
the O
wrong O
number O
at O
the O
end O
. O

In O
this O
paper O
, O
we O
proposed O
Hypergraph B-MethodName
Transformer I-MethodName
for O
multi O
- O
hop O
reasoning O
over O
knowledge O
graph O
under O
weak O
supervision O
. O

Hypergraph B-MethodName
Transformer I-MethodName
adopts O
hypergraph O
- O
based O
representation O
to O
encode O
high O
- O
order O
semantics O
of O
knowledge O
and O
questions O
and O
considers O
associations O
between O
a O
knowledge O
hypergraph O
and O
a O
question O
hypergraph O
. O

Here O
, O
each O
node O
representation O
in O
the O
hypergraphs O
is O
updated O
by O
inter O
- O
and O
intra O
- O
attention O
mechanisms O
in O
two O
hypergraphs O
, O
rather O
than O
by O
iterative O
message O
passing O
scheme O
. O

Thus O
, O
Hypergraph B-MethodName
Transformer I-MethodName
can O
mitigate O
the O
well O
- O
known O
over O
- O
smoothing O
problem O
in O
the O
previous O
graph O
- O
based O
methods O
exploiting O
the O
message O
passing O
scheme O
. O

Extensive O
experiments O
on O
various O
datasets O
, O
KVQA B-DatasetName
, O
FVQA B-DatasetName
, O
PQ B-DatasetName
, O
and O
PQL B-DatasetName
validated O
that O
Hypergraph B-MethodName
Transformer I-MethodName
conducts O
accurate O
inference O
by O
focusing O
on O
knowledge O
evidences O
necessary O
for O
question O
from O
a O
large O
knowledge O
graph O
. O

Although O
not O
covered O
in O
this O
paper O
, O
an O
interesting O
future O
work O
is O
to O
construct O
heterogeneous O
knowledge O
graph O
that O
includes O
more O
diverse O
knowledge O
sources O
( O
e.g. O
documents O
on O
web O
) O
. O

( O
b O
- O
e O
) O
. O

Our O
model O
shows O
comparable O
performances O
on O
PQ-{2H B-DatasetName
, I-DatasetName
3H I-DatasetName
, I-DatasetName
M I-DatasetName
} I-DatasetName
to O
the O
state O
- O
of O
- O
the O
- O
art O
weaklysupervised O
model O
, O
SRN O
. O

Especially O
, O
Hypergraph B-MethodName
Transformer I-MethodName
shows O
significant O
performance O
improvement O
( O
78.6 B-MetricValue
% I-MetricValue
→ O
90.5 B-MetricValue
% I-MetricValue
for O
PQL-2H B-DatasetName
, O
78.3 B-MetricValue
% I-MetricValue
→ O
94.5 B-MetricValue
% I-MetricValue
for O
PQL B-DatasetName
- I-DatasetName
M I-DatasetName
) O
on O
PQL B-DatasetName
. O

We O
highlight O
that O
PQL B-DatasetName
is O
more O
challenging O
dataset O
than O
PQ B-DatasetName
in O
that O
PQL B-DatasetName
not O
only O
covers O
more O
knowledge O
facts O
but O
also O
has O
fewer O
QA O
instances O
. O

We O
observe O
that O
the O
accuracy B-MetricName
on O
PQL-3H B-DatasetName
is O
relatively O
lower O
than O
the O
other O
splits O
. O

This O
is O
due O
to O
the O
insufficient O
number O
of O
training O
QA O
pairs O
in O
PQL-3H. B-DatasetName
When O
we O
use O
PQL-3H B-DatasetName
- I-DatasetName
More I-DatasetName
which O
has O
twice O
more O
QA O
pairs O
( O
1031 O
→ O
2062 O
) O
on O
the O
same O
knowledge O
base O
as O
PQL-3H B-DatasetName
, O
our O
model O
achieves O
95.4 B-MetricValue
% I-MetricValue
accuracy B-MetricName
. O

We O
verify O
the O
effectiveness O
of O
each O
module O
in O
Hypergraph B-MethodName
Transformer I-MethodName
. O

To O
analyze O
the O
performances O
of O
the O
variants O
in O
our O
model O
, O
we O
use O
KVQA B-DatasetName
which O
is O
a O
representative O
and O
large O
- O
scale O
dataset O
for O
knowledge B-TaskName
- I-TaskName
based I-TaskName
VQA I-TaskName
. O

Here O
, O
we O
mainly O
focus O
on O
two O
aspects O
: O
i O
) O
effect O
of O
hypergraph O
and O
ii O
) O
effect O
of O
attention O
mechanism O
. O

To O
evaluate O
a O
pure O
reasoning O
ability O
of O
the O
models O
, O
we O
conduct O
experiments O
in O
the O
oracle O
setting O
. O

To O
analyze O
the O
effectiveness O
of O
hypergraph O
- O
based O
input O
representation O
, O
we O
conduct O
comparative O
experiments O
on O
the O
different O
types O
of O
input O
formats O
for O
Transformer O
architecture O
. O

Here O
, O
we O
consider O
the O
two O
types O
of O
input B-HyperparameterName
format I-HyperparameterName
, O
which O
are O
single B-HyperparameterValue
- I-HyperparameterValue
wordunit I-HyperparameterValue
and O
hyperedge B-HyperparameterValue
- I-HyperparameterValue
based I-HyperparameterValue
representations B-HyperparameterName
. O

Compared O
to O
hyperedge B-HyperparameterValue
- I-HyperparameterValue
based I-HyperparameterValue
inputs O
considering O
multiple O
relational O
facts O
as O
a O
input O
token O
, O
single B-HyperparameterValue
- I-HyperparameterValue
wordunit I-HyperparameterValue
takes O
every O
entity O
and O
relation O
tokens O
as O
separate O
input O
tokens O
. O

We O
note O
that O
using O
single B-HyperparameterValue
- I-HyperparameterValue
wordunit I-HyperparameterValue
- I-HyperparameterValue
based I-HyperparameterValue
input O
format O
for O
both O
knowledge O
and O
question O
is O
the O
standard O
settings O
for O
the O
Transformer O
network O
and O
using O
hyperedge B-HyperparameterValue
- I-HyperparameterValue
based I-HyperparameterValue
input O
format O
for O
both O
is O
the O
proposed O
model O
, O
Hypergraph B-MethodName
Transformer I-MethodName
. O

We O
set O
the O
Transformer B-MethodName
( I-MethodName
SA+GA I-MethodName
) I-MethodName
as O
a O
backbone O
model O
, O
and O
present O
the O
results O
in O
Table O
3 O

The O
experimental O
results O
on O
diverse O
split B-HyperparameterName
of O
PQ B-DatasetName
and O
PQL B-DatasetName
datasets O
are O
provided O
in O
Table O
2 O
. O

The O
first O
section O
in O
the O
table O
includes O
fully O
- O
supervised O
models O
which O
require O
a O
ground O
- O
truth O
path O
annotation O
as O
an O
additional O
supervision O
. O

The O
second O
section O
contains O
weakly O
- O
supervised O
models O
learning O
to O
infer O
the O
multi O
- O
hop O
reasoning O
paths O
without O
the O
groundtruth O
path O
annotation O
. O

Hypergraph B-MethodName
Transformer I-MethodName
is O
involved O
in O
the O
weakly O
- O
supervised O
models O
because O
it O
only O
exploits O
an O
answer O
as O
a O
supervision O
. O

We O
confirm O
that O
our O
model O
works O
effectively O
as O
a O
general O
reasoning O
framework O
without O
considering O
characteristics O
of O
different O
knowledge O
sources O
( O
i.e. O
, O
Wikidata B-DatasetName
for O
KVQA B-TaskName
, O
DBpedia B-DatasetName
, O
ConceptNet B-DatasetName
, O
WebChild B-DatasetName
for O
FVQA).To B-TaskName
required O
to O
answer O
a O
given O
question O
is O
unknown O
. O

Here O
, O
we O
assume O
that O
the O
performance O
of O
entity O
linking O
is O
perfect O
, O
and O
evaluate O
the O
pure O
reasoning O
ability O
of O
our O
model O
. O

As O
shown O
in O
Table O
8 O
of O
Appendix O
D O
, O
Hypergraph B-MethodName
Transformer I-MethodName
shows O
comparable O
performance O
in O
both O
top-1 O
and O
top-3 O
accuracy B-MetricName
in O
comparison O
with O
the O
state O
- O
of O
- O
the O
- O
art O
methods O
. O

Entity O
linking O
setting O
We O
also O
present O
the O
experimental O
results O
on O
the O
entity O
linking O
setting O
where O
the O
named O
entities O
are O
not O
provided O
as O
the O
oracle O
setting O
, O
but O
detected O
by O
the O
module O
as O
described O
in O
Section O
3.2 O
. O

As O
shown O
in O
Table O
7 O
of O
Appendix O
E O
, O
our O
model O
shows O
the O
best O
performances O
for O
both O
original O
and O
paraphrased O
questions O
. O

For O
all O
comparative O
models O
, O
we O
use O
the O
same O
knowledge O
hypergraph O
extracted O
by O
the O
3 B-HyperparameterValue
- I-HyperparameterValue
hop I-HyperparameterValue
graph B-HyperparameterName
walk I-HyperparameterName
. O

In O
entity O
linking O
setting O
, O
the O
constructed O
knowledge O
hypergraph O
can O
be O
incomplete O
and O
quite O
noisy O
due O
to O
the O
undetected O
entities O
or O
misclassified O
entity O
labels O
. O

However O
, O
Hypergraph B-MethodName
Transformer I-MethodName
shows O
robust O
reasoning O
capacity O
over O
the O
noisy O
inputs O
. O

Here O
, O
we O
remark O
that O
the O
upper O
bound O
of O
QA B-TaskName
performance O
is O
72.8 B-MetricValue
% I-MetricValue
due O
to O
the O
error O
rate O
of O
entity O
linking O
module O
. O

We O
expect O
that O
the O
performance O
will O
be O
improved O
when O
the O
entity O
linking O
module O
is O
enhanced O
. O

We O
conduct O
experiments O
on O
Fact B-DatasetName
- I-DatasetName
based I-DatasetName
Visual I-DatasetName
Question I-DatasetName
Answering I-DatasetName
( O
FVQA B-DatasetName
) O
as O
an O
additional O
benchmark O
dataset O
for O
knowledge B-TaskName
- I-TaskName
based I-TaskName
VQA I-TaskName
. O

Different O
from O
KVQA B-DatasetName
focusing O
on O
world O
knowledge O
for O
named O
entities O
, O
FVQA B-DatasetName
considers O
commonsense O
knowledge O
about O
common O
nouns O
in O
a O
given O
image O
. O

5 O
Quantitative O
ResultsWe O
all O
settings O
. O

From O
the O
results O
, O
we O
find O
that O
the O
attention O
mechanism O
between O
question O
and O
knowledge O
is O
crucial O
for O
complex O
QA B-TaskName
. O

Since O
GCN B-MethodName
( O
Kipf O
and O
Welling O
, O
2017 O
) O
and O
GGNN B-MethodName
encode O
question O
and O
knowledge O
graph O
separately O
, O
they O
do O
not O
learn O
interactions O
between O
question O
and O
knowledge O
. O

Thus O
, O
GCN B-MethodName
and O
GGNN B-MethodName
show O
quite O
low O
performance O
under O
74 B-MetricValue
% I-MetricValue
mean O
accuracy B-MetricName
. O

On O
the O
other O
hand O
, O
MemNN B-MethodName
† I-MethodName
, O
HAN B-MethodName
( O
Kim O
et O
al O
. O
, O
2020 O
) O
, O
and O
BAN B-MethodName
( O
Kim O
et O
al O
. O
, O
2018 O
) O
achieve O
comparatively O
high O
performance O
because O
MemNN B-MethodName
† I-MethodName
adopts O
question O
- O
guided O
soft O
attention O
over O
knowledge O
memories O
. O

HAN B-MethodName
and O
BAN B-MethodName
utilize O
multi O
- O
head O
co O
- O
attention O
between O
question O
and O
knowledge O
. O

For O
entity O
linking O
for O
KVQA B-DatasetName
, O
we O
apply O
the O
wellknown O
pre O
- O
trained O
models O
for O
face O
identification O
: O
RetinaFace B-MethodName
( O
Deng O
et O
al O
. O
, O
2020 O
) O
for O
face B-TaskName
detection I-TaskName
and O
ArcFace B-MethodName
( O
Deng O
et O
al O
. O
, O
2019 O
) O
for O
face B-TaskName
feature I-TaskName
extraction I-TaskName
. O

For O
all O
datasets O
, O
we O
follow O
the O
experimental O
settings O
as O
in O
previous O
works O
. O

We O
use O
the O
similarity O
- O
based O
answer O
predictor O
for O
KVQA B-DatasetName
, O
and O
MLP O
for O
the O
others O
. O

We O
adopt O
Adam B-HyperparameterValue
( O
Kingma O
and O
Ba O
, O
2015 O
) O
to O
optimize O
all O
learnable O
parameters O
in O
the O
model O
. O

We O
describe O
details O
of O
the O
experimental O
settings O
and O
the O
tuned O
hyperparameters O
for O
each O
dataset O
in O
Appendix O
D. O

The O
most O
similar O
answer O
to O
the O
joint O
representation O
is O
selected O
as O
an O
answer O
among O
the O
answer O
candidates O
. O

For O
training O
, O
we O
use O
only O
supervision O
from O
QA O
pairs O
without O
annotations O
for O
ground O
- O
truth O
reasoning O
paths O
. O

To O
this O
end O
, O
cross B-HyperparameterValue
- I-HyperparameterValue
entropy I-HyperparameterValue
between O
prediction O
p O
and O
ground O
- O
truth O
t O
is O
utilized O
as O
a O
loss B-HyperparameterName
function I-HyperparameterName
. O

( O
Auer O
et O
al O
. O
, O
2007 O
) O
, O
ConceptNet B-DatasetName
( O
Liu O
and O
Singh O
, O
2004 O
) O
, O
and O
WebChild B-DatasetName
( O
Tandon O
et O
al O
. O
, O
2014 O
) O
. O

The O
last O
two O
datasets O
, O
PQ B-DatasetName
and O
PQL B-DatasetName
, O
focus O
on O
evaluating O
multi O
- O
hop O
reasoning O
ability O
in O
the O
knowledgebased B-TaskName
textual I-TaskName
QA I-TaskName
task I-TaskName
. O

PQ B-DatasetName
and O
PQL B-DatasetName
contain O
7,106 O
and O
2,625 O
QA O
pairs O
on O
4,050 O
and O
9,844 O
knowledge O
facts O
from O
the O
subset O
of O
Freebase B-DatasetName
( O
Bollacker O
et O
al O
. O
, O
2008 O
) O
, O
respectively O
. O

The O
detailed O
statistics O
of O
the O
datasets O
are O
shown O
in O
Appendix O
A.Each O
node O
in O
the O
knowledge O
hypergraph O
and O
the O
question O
hypergraph O
is O
represented O
as O
a O
300dimensional B-HyperparameterValue
vector B-HyperparameterName
( O
i.e. O
, O
w B-HyperparameterName
= O
300 B-HyperparameterValue
) O
initialized O
using O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
. O

Random O
initialization O
is O
applied O
when O
a O
word O
for O
a O
node O
does O
not O
exist O
in O
the O
vocabulary O
of O
GloVe B-MethodName
. O

Mean O
pooling O
is O
applied O
when O
a O
node O
consists O
of O
multiple O
words O
. O

Following O
the O
standard O
structure O
of O
the O
transformer O
, O
we O
build O
up O
guided O
- O
attention O
block O
and O
selfattention O
block O
where O
each O
block O
consists O
of O
each O
attention O
operation O
with O
layer O
normalization O
, O
residual O
connection O
, O
and O
a O
single B-HyperparameterValue
feed B-HyperparameterName
- I-HyperparameterName
forward I-HyperparameterName
layer I-HyperparameterName
. O

By O
passing O
the O
guided O
- O
attention O
blocks O
and O
selfattention O
blocks O
sequentially O
, O
representations O
of O
knowledge O
hyperedges O
and O
question O
hyperedges O
are O
updated O
and O
finally O
aggregated O
to O
single O
vector O
representation O
as O
z O
k O
∈ O
R O
dv B-HyperparameterName
and O
z O
q O
∈ O
R O
dv B-HyperparameterName
, O
respectively O
. O

To O
predict O
an O
answer O
, O
we O
first O
concatenate O
the O
representation O
z O
k O
and O
z O
q O
obtained O
from O
the O
attention O
blocks O
and O
feed O
into O
a O
single B-HyperparameterValue
feed B-HyperparameterName
- I-HyperparameterName
forward I-HyperparameterName
layer I-HyperparameterName
( O
i.e. O
, O
R O
2dv O
→ O
R O
w O
) O
to O
make O
a O
joint O
representation O
z. O
We O
then O
consider O
two O
types O
of O
answer O
predictor O
: O
multi O
- O
layer O
perceptron O
and O
similarity O
- O
based O
answer O
predictor O
. O

Multi O
- O
layer O
perceptron O
as O
an O
answer O
classifier O
p O
= O
ψ(z O
) O
is O
a O
prevalent O
for O
visual B-TaskName
question I-TaskName
answering I-TaskName
problems O
. O

For O
similarity O
- O
based O
answer O
, O
we O
calculate O
a O
dot O
product O
similarity O
p O
= O
zC O
T O
between O
z O
and O
answer O
candidate O
set O
C O
∈ O
R O
|A|×w O
where O
|A| O
is O
a O
number O
of O
candidate O
answers O
and O
w O
is O
a O
dimension O
of O
representation O
for O
each O
answer O
. O

Attention(Q O
k O
, O
K O
k O
, O
V O
k O
) O
. O

For O
question O
hyperedges O
E O
q O
, O
self O
- O
attention O
is O
performed O
in O
a O
similar O
manner O
: O
Attention(Q O
q O
, O
K O
q O
, O
V O
q O
) O
. O

Self O
- O
attention O
The O
only O
difference O
between O
guided O
- O
attention O
and O
self O
- O
attention O
is O
that O
the O
same O
input O
is O
used O
for O
both O
query O
and O
key O
- O
value O
within O
self O
- O
attention O
. O

For O
example O
, O
we O
set O
query O
, O
key O
, O
and O
value O
based O
on O
the O
knowledge O
hyperedges O
E O
k O
, O
and O
the O
self O
- O
attention O
for O
knowledge O
hyperedges O
is O
conducted O
by O

Attention(Q O
q O
, O
K O
k O
, O
V O
k O
) O
. O

where O
d B-HyperparameterName
v I-HyperparameterName
is O
the O
dimension B-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
query I-HyperparameterName
and O
the O
key O
vector O
. O

In O
addition O
, O
the O
guided O
- O
attention O
which O
uses O
the O
question O
hyperedges O
as O
query O
and O
the O
knowledge O
hyperedges O
as O
key O
- O
value O
pairs O
is O
performed O
in O
a O
similar O
manner O
: O

as O
Attention(Q O
k O
, O
K O
q O
, O
V O
q O
) O
= O
softmax B-HyperparameterValue
( O
Q O
k O
K O
T O
q O
√ O
dv O
) O
V O
q O

where O
all O
projection O
matrices O
W O
[ O
• O
] O
∈ O
R O
d×dv O
are O
learnable O
parameters O
. O

Then O
, O
scaled O
dot O
product O
at O
- O
tention O
using O
the O
query O
, O
key O
, O
and O
value O
is O
calculated O

Q O
k O
= O
E O
k O
W O
Q O
k O
, O
a O
key O
K O
q O
= O
E O
q O
W O
Kq O
, O
and O
a O
value O
V O
q O
= O
E O
q O
W O
Vq O
, O

e O
k O
= O
ϕ O
k O
• O
f O
k O
( O
h O
k O
) O
∈ O
R O
d O
, O
e O
q O
= O
ϕ O
q O
• O
f O
q O
( O
h O
q O
) O
∈ O
R O
d O
where O
h O
[ O
• O
] O
is O
a O
hyperedge O
in O
E O
[ O
• O
] O
. O

Here O
, O
f O
[ O
• O
] O

Guided O
- O
attention O
To O
learn O
inter O
- O
association O
between O
two O
hypergraphs O
, O
we O
first O
embed O
a O
knowledge O
hyperedge O
and O
a O
question O
hyperedge O
as O
follows O
: O
We O
define O
the O
knowledge O
hyperedges O
E O
k O
and O
the O
question O
hyperedges O
E O
q O
as O
a O
query O
and O
key O
- O
value O
pairs O
, O
respectively O
. O

We O
set O
a O
query O

Question O
hypergraph O
We O
transform O
a O
question O
sentence O
into O
a O
question O
hypergraph O
H O
q O
consisting O
of O
a O
node O
set O
V O
q O
and O
a O
hyperedge O
set O
E O
q O
. O

We O
assume O
that O
each O
word O
unit O
( O
a O
word O
or O
named O
entity O
) O
of O
the O
question O
is O
defined O
as O
a O
node O
, O
and O
has O
edges O
to O
adjacent O
nodes O
. O

For O
question O
hypergraph O
, O
each O
word O
unit O
is O
used O
as O
a O
start O
node O
of O
a O
graph O
walk O
. O

The O
multi O
- O
hop O
graph O
walk O
is O
conducted O
in O
the O
same O
manner O
as O
the O
knowledge O
hypergraph O
. O

A O
n O
- O
gram O
phrase O
is O
considered O
as O
a O
hyperedge O
in O
the O
question O
hypergraph O
( O
see O
Figure O
2(b)).To O
consider O
high O
- O
order O
associations O
between O
knowledge O
and O
question O
, O
we O
devise O
structural O
semantic O
matching O
between O
the O
query O
- O
aware O
knowledge O
hypergraph O
and O
the O
question O
hypergraph O
. O

We O
introduce O
an O
attention O
mechanism O
over O
two O
hypergraphs O
based O
on O
guided O
- O
attention O
( O
Tsai O
et O
al O
. O
, O
2019 O
) O
and O
self O
- O
attention O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O

As O
shown O
in O
Figure O
2(c O
) O
, O
the O
guided O
- O
attention O
blocks O
are O
introduced O
to O
learn O
correlations O
between O
knowledge O
hyperedges O
and O
question O
hyperedges O
by O
interattention O
mechanism O
, O
and O
then O
intra O
- O
relationships O
of O
in O
knowledge O
or O
question O
hyperedges O
are O
trained O
with O
the O
following O
self O
- O
attention O
blocks O
. O

The O
details O
of O
two O
modules O
, O
guided O
- O
attention O
blocks O
and O
selfattention O
blocks O
, O
are O
described O
as O
below O
. O

Note O
that O
we O
use O
Q O
, O
K O
, O
and O
V O
for O
query O
, O
key O
, O
value O
, O
and O
q O
, O
k O
as O
subscripts O
to O
represent O
question O
and O
knowledge O
, O
respectively O
. O

We O
consider O
a O
huge O
number O
of O
knowledge O
facts O
in O
the O
KB O
as O
a O
huge O
knowledge O
graph O
, O
and O
construct O
a O
hypergraph O
by O
traversing O
the O
knowledge O
graph O
. O

Such O
traversal O
, O
called O
graph O
walk O
, O
starts O
from O
the O
node O
linked O
from O
the O
previous O
module O
( O
see O
section O
3.2 O
) O
and O
considers O
all O
entity O
nodes O
associated O
with O
the O
start O
node O
. O

We O
define O
a O
triplet O
as O
a O
basic O
unit O
of O
graph O
walk O
to O
preserve O
high O
- O
order O
semantics O
inherent O
in O
knowledge O
graph O
, O
i.e. O
, O
every O
single O
graph O
walk O
contains O
three B-HyperparameterValue
nodes B-HyperparameterName
{ B-HyperparameterValue
head I-HyperparameterValue
, I-HyperparameterValue
predicate I-HyperparameterValue
, I-HyperparameterValue
tail I-HyperparameterValue
} I-HyperparameterValue
, O
rather O
than O
having O
only O
one O
of O
these O
three O
nodes O
. O

In O
addition O
to O
the O
triplet O
- O
based O
graph O
walks O
, O
a O
multihop O
graph O
walk O
is O
proposed O
to O
encode O
multiple O
relational O
facts O
that O
are O
interconnected O
. O

Multi O
- O
hop O
graph O
walk O
connects O
multiple O
facts O
by O
setting O
the O
arrival O
node O
( O
tail O
) O
of O
the O
preceding O
walk O
as O
the O
starting O
( O
head O
) O
node O
of O
the O
next O
walk O
, O
thus O
, O
n O
- O
hop O
graph O
walk O
combines O
n O
facts O
as O
a O
hyperedge O
. O

V O
′k O
⊂ O
V O
k O
. O

A O
hyperedge O
is O
flexible O
to O
encode O
different O
kinds O
of O
semantics O
in O
the O
underlying O
graph O
without O
the O
constraint O
of O
length O
. O

As O
shown O
in O
Figure O
2(a O
) O
, O
entity O
linking O
module O
first O
links O
concepts O
from O
query O
( O
a O
given O
image O
- O
question O
pair O
) O
to O
knowledge O
base O
. O

We O
detect O
visual O
concepts O
( O
e.g. O
, O
objects O
, O
attributes O
, O
person O
names O
) O
in O
a O
given O
image O
and O
named O
entities O
in O
a O
given O
question O
. O

The O
semantic O
labels O
of O
visual O
concepts O
or O
named O
entities O
are O
then O
linked O
with O
knowledge O
entities O
in O
the O
knowledge O
base O
using O
exact O
keyword O
matching O
. O

Query O
- O
aware O
knowledge O
hypergraph O
A O
knowledge O
base O
( O
KB O
) O
, O
a O
vast O
amount O
of O
general O
knowledge O
facts O
, O
contains O
not O
only O
knowledge O
facts O
required O
to O
answer O
a O
given O
question O
but O
also O
unnecessary O
knowledge O
facts O
. O

Thus O
, O
we O
construct O
a O
queryaware O
knowledge O
hypergraph O
H O
k O
= O
{ O
V O
k O
, O
E O
k O
} O
to O
extract O
related O
information O
for O
answering O
a O
given O
question O
. O

It O
consists O
of O
a O
node O
set O
V O
k O
and O
hyperedge O
set O
E O
k O
, O
which O
represent O
a O
set O
of O
entities O
in O
knowledge O
facts O
and O
a O
set O
of O
hyperedges O
, O
respectively O
. O

Each O
hyperedge O
connects O
the O
subset O
of O
vertices O

i O
= O
{ O
v O
′ O
1 O
⪯ O
... O
⪯ O
v O
′ O
l O
} O
where O
V O
′ O
= O
{ O
v O
′ O
1 O
, O
... O
, O
v O
′ O
l O
} O
is O
a O
subset O
of O
V O
and O
⪯ O
is O
a O
binary O
relation O
which O
denotes O
an O
element O
( O
v O
′ O
i O
) O
precedes O
the O
other O
( O
v O
′ O
j O
) O
in O
the O
ordering O
when O
v O
′ O
i O
⪯ O
v O
′ O
j O
. O

Multi O
- O
hop O
knowledge O
graph O
reasoning O
is O
a O
process O
of O
sequential O
reasoning O
based O
on O
multiple O
evidences O
of O
a O
knowledge O
graph O
, O
and O
has O
been O
broadly O
used O
in O
various O
downstream O
tasks O
such O
as O
question B-TaskName
answering I-TaskName
( O
Lin O
et O
al O
. O
, O
2019;Saxena O
et O
al O
. O
, O
2020;Han O
et O
al O
. O
, O
2020b O
, O
a;Yadati O
et O
al O
. O
, O
2021 O
) O
, O
or O
knowledge B-TaskName
- I-TaskName
enhanced I-TaskName
text I-TaskName
generation I-TaskName
( O
Liu O
et O
al O
. O
, O
2019;Moon O
et O
al O
. O
, O
2019;Ji O
et O
al O
. O
, O
2020 O
) O
. O

Recent O
researches O
have O
introduced O
the O
concept O
of O
hypergraph O
for O
multi O
- O
hop O
graph O
reasoning O
( O
Kim O
et O
al O
. O
, O
2020;Han O
et O
al O
. O
, O
2020b O
, O
a;Yadati O
et O
al O
. O
, O
, O
2021 O
. O

These O
models O
have O
a O
similar O
motivation O
to O
the O
Hypergraph B-MethodName
Transformer I-MethodName
proposed O
in O
this O
paper O
, O
but O
core O
operations O
are O
vastly O
different O
. O

These O
models O
mainly O
update O
node O
representations O
in O
the O
hypergraph O
through O
a O
message O
passing O
process O
using O
graph O
convolution O
operation O
. O

On O
the O
contrary O
, O
our O
method O
update O
node O
representations O
via O
hyperedge O
matching O
of O
hypergraphs O
instead O
of O
message O
passing O
scheme O
. O

We O
argue O
that O
this O
update O
process O
effectively O
learns O
the O
high O
- O
order O
semantics O
inherent O
in O
each O
hypergraph O
and O
the O
high O
- O
order O
associations O
between O
two O
hypergraphs O
. O

To O
capture O
high O
- O
order O
semantics O
inherent O
in O
the O
knowledge O
sources O
, O
we O
adopt O
the O
concept O
of O
hypergraph O
. O

Formally O
, O
directed O
hypergraph O
H O
= O
{ O
V O
, O
E O
} O
is O
defined O
by O
a O
set O
of O
nodes O
V O
= O
{ O
v O
1 O
, O
... O
, O
v O
|V| O
} O
and O
a O
set O
of O
hyperedges O
E O
= O
{ O
h O
1 O
, O
... O
, O
h O
|E| O
} O
. O

Each O
node O
is O
represented O
as O
a O
w B-HyperparameterValue
- I-HyperparameterValue
dimensional I-HyperparameterValue
embedding B-HyperparameterName
vector I-HyperparameterName
, O
i.e. O
, O
v O
i O
∈ O
R O
w O
. O

Each O
hyperedge O
connects O
an O
arbitrary O
number O
of O
nodes O
and O
has O
partial O
order O
itself O
, O
i.e. O
, O
h O

The O
main O
contributions O
of O
this O
paper O
can O
be O
summarized O
as O
follows O
. O

i O
) O
We O
propose O
Hypergraph B-MethodName
Transformer I-MethodName
which O
enhances O
multi O
- O
hop O
reasoning O
ability O
by O
encoding O
high O
- O
order O
semantics O
in O
the O
form O
of O
a O
hypergraph O
and O
learning O
inter O
- O
and O
intrahigh O
- O
order O
associations O
in O
hypergraphs O
using O
the O
attention O
mechanism O
. O

ii O
) O
We O
conduct O
extensive O
experiments O
on O
two O
knowledge B-TaskName
- I-TaskName
based I-TaskName
VQA I-TaskName
datasets O
( O
KVQA B-DatasetName
and O
FVQA B-DatasetName
) O
and O
two O
knowledge B-TaskName
- I-TaskName
based I-TaskName
textual I-TaskName
QA I-TaskName
datasets O
( O
PQ B-DatasetName
and O
PQL B-DatasetName
) O
and O
show O
superior O
performances O
on O
all O
datasets O
, O
especially O
multi O
- O
hop O
reasoning O
problem O
. O

iii O
) O
We O
qualitatively O
observe O
that O
Hypergraph B-MethodName
Transformer I-MethodName
performs O
robust O
in O
- O
ference O
by O
focusing O
on O
correct O
reasoning O
evidences O
under O
weak O
supervision O
. O

Knowledge B-DatasetName
- I-DatasetName
based I-DatasetName
visual I-DatasetName
question I-DatasetName
answering I-DatasetName
( O
Wang O
et O
al O
. O
, O
2017(Wang O
et O
al O
. O
, O
, O
2018Marino O
et O
al O
. O
, O
2019;Sampat O
et O
al O
. O
, O
2020 O
) O
proposed O
benchmark O
datasets O
for O
knowledge B-TaskName
- I-TaskName
based I-TaskName
visual I-TaskName
question I-TaskName
answering I-TaskName
that O
requires O
reasoning O
about O
an O
image O
on O
the O
basis O
of O
facts O
from O
a O
large O
- O
scale O
knowledge O
base O
( O
KB O
) O
such O
as O
Freebase B-DatasetName
( O
Bollacker O
et O
al O
. O
, O
2008 O
) O
or O
DBPedia B-DatasetName
( O
Auer O
et O
al O
. O
, O
2007 O
) O
. O

To O
solve O
the O
task O
, O
two O
pioneering O
studies O
( O
Wang O
et O
al O
. O
, O
2017(Wang O
et O
al O
. O
, O
, O
2018 O
suggested O
logical O
parsing O
- O
based O
methods O
which O
convert O
a O
question O
to O
a O
KB O
logic O
query O
using O
predefined O
query O
templates O
and O
execute O
the O
generated O
query O
on O
KB O
for O
searching O
an O
answer O
. O

Since O
then O
information O
retrieval O
- O
based O
methods O
which O
retrieve O
knowledge O
facts O
associated O
with O
a O
question O
and O
conduct O
semantic O
matching O
between O
the O
facts O
and O
the O
question O
are O
introduced O
. O

( O
Narasimhan O
and O
Schwing O
, O
2018 O
; O
proposed O
memory O
- O
based O
methods O
that O
represent O
knowledge O
facts O
in O
the O
form O
of O
memory O
and O
calculate O
soft O
attention O
scores O
of O
the O
memory O
with O
a O
question O
. O

( O
Narasimhan O
et O
al O
. O
, O
2018 O
; O
represented O
the O
retrieved O
facts O
as O
a O
graph O
and O
performed O
graph O
reasoning O
through O
message O
passing O
scheme O
utilizing O
graph O
convolution O
. O

However O
, O
these O
methods O
are O
complicated O
to O
encode O
inherent O
high O
- O
order O
semantics O
and O
multi O
- O
hop O
relationships O
present O
in O
the O
knowledge O
graph O
. O

Therefore O
, O
we O
introduce O
a O
concept O
of O
hypergraph O
and O
propose O
transformer O
- O
based O
attention O
mechanism O
over O
hypergraphs O
. O

To O
address O
the O
above O
limitation O
, O
we O
propose O
a O
novel O
method O
, O
Hypergraph B-MethodName
Transformer I-MethodName
, O
which O
exploits O
hypergraph O
structure O
to O
encode O
multi O
- O
hop O
relationships O
and O
transformer O
- O
based O
attention O
mechanism O
to O
learn O
to O
pay O
attention O
to O
important O
knowledge O
evidences O
for O
a O
question O
. O

We O
construct O
a O
question O
hypergraph O
and O
a O
knowledge O
hypergraph O
to O
explicitly O
encode O
high O
- O
order O
semantics O
present O
in O
the O
question O
and O
each O
knowledge O
fact O
, O
and O
capture O
multi O
- O
hop O
relational O
knowledge O
facts O
effectively O
. O

Then O
, O
we O
perform O
hyperedge B-HyperparameterValue
matching I-HyperparameterValue
between O
the O
two O
hypergraphs O
by O
leveraging O
transformer O
- O
based O
attention O
mechanism O
. O

We O
argue O
that O
introducing O
the O
concept O
of O
hypergraph O
is O
powerful O
for O
multi O
- O
hop O
reasoning O
problem O
in O
that O
it O
can O
encode O
high O
- O
order O
semantics O
without O
the O
constraint O
of O
length O
and O
learn O
cross O
- O
modal O
high O
- O
order O
associations O
. O

Under O
weak O
supervision O
, O
previous O
studies O
proposed O
memory O
- O
based O
methods O
( O
Narasimhan O
and O
Schwing O
, O
2018 O
; O
and O
graph O
- O
based O
methods O
( O
Narasimhan O
et O
al O
. O
, O
2018 O
; O
to O
learn O
to O
selectively O
focus O
on O
necessary O
pieces O
of O
knowledge O
. O

The O
memory O
- O
based O
methods O
represent O
knowledge O
facts O
in O
a O
form O
of O
memory O
and O
calculate O
soft O
attention O
scores O
of O
each O
memory O
with O
respect O
to O
a O
question O
. O

Then O
, O
it O
infers O
an O
answer O
by O
attending O
to O
knowledge O
evidence O
with O
high O
attention O
scores O
. O

On O
the O
other O
hand O
, O
to O
explicitly O
consider O
relational O
structure O
between O
knowledge O
facts O
, O
graph O
- O
based O
methods O
construct O
a O
query O
- O
aware O
knowledge O
graph O
by O
retrieving O
facts O
from O
KB O
and O
perform O
graph O
reasoning O
for O
a O
question O
. O

These O
methods O
mainly O
adopt O
an O
iterative O
message O
passing O
process O
to O
propagate O
information O
between O
adjacent O
nodes O
in O
the O
graph O
. O

However O
, O
it O
is O
difficult O
to O
capture O
multi O
- O
hop O
relationships O
containing O
long O
- O
distance O
nodes O
from O
the O
graph O
due O
to O
the O
well O
- O
known O
over O
- O
smoothing O
problem O
, O
where O
repetitive O
message O
passing O
process O
to O
propagate O
information O
across O
long O
distance O
makes O
features O
of O
connected O
nodes O
too O
similar O
and O
undiscriminating O
( O
Li O
et O
al O
. O
, O
2018 O
; O
. O

That O
is O
, O
the O
model O
should O
learn O
which O
knowledge O
facts O
to O
be O
attended O
to O
and O
how O
to O
combine O
them O
to O
infer O
the O
correct O
answer O
on O
its O
own O
. O

Following O
the O
previous O
work O
( O
Zhou O
et O
al O
. O
, O
2018 O
) O
, O
we O
call O
this O
setting O
under O
weak O
supervision O
. O

where O
a O
massive O
number O
of O
knowledge O
facts O
from O
a O
general O
knowledge O
base O
( O
KB O
) O
is O
given O
with O
an O
image O
- O
question O
pair O
. O

To O
answer O
the O
given O
question O
as O
shown O
in O
Figure O
1 O
, O
a O
model O
should O
understand O
the O
semantics O
of O
the O
given O
question O
, O
link O
visual O
entities O
appearing O
in O
the O
given O
image O
to O
the O
KB O
, O
extract O
a O
number O
of O
evidences O
from O
the O
KB O
and O
predict O
an O
answer O
by O
aggregating O
semantics O
of O
both O
the O
question O
and O
the O
extracted O
evidences O
. O

Following O
these O
, O
there O
are O
two O
fundamental O
challenges O
in O
this O
task O
. O

i O
) O
To O
answer O
a O
complex O
question O
, O
multi O
- O
hop O
reasoning O
over O
multiple O
knowledge O
evidences O
is O
necessary O
. O

ii O
) O
Learning O
a O
complex O
reasoning O
process O
is O
difficult O
especially O
in O
a O
condition O
where O
only O
QA B-TaskName
is O
provided O
without O
extra O
supervision O
on O
how O
to O
capture O
any O
evidence O
from O
the O
KB O
and O
infer O
based O
on O
them O
. O

In O
this O
paper O
, O
we O
focus O
on O
the O
task O
which O
is O
called O
knowledge B-TaskName
- I-TaskName
based I-TaskName
visual I-TaskName
question I-TaskName
answering I-TaskName
, O
To O
answer O
the O
given O
question O
, O
the O
multiple O
reasoning O
evidences O
( O
marked O
as O
orange O
) O
are O
required O
. O

Visual B-TaskName
question I-TaskName
answering I-TaskName
( O
VQA B-TaskName
) O
is O
a O
semantic B-TaskName
reasoning I-TaskName
task O
that O
aims O
to O
answer O
questions O
about O
visual O
content O
depicted O
in O
images O
( O
Antol O
et O
al O
. O
, O
2015;Zhu O
et O
al O
. O
, O
2016;Hudson O
and O
Manning O
, O
2019 O
) O
, O
and O
has O
become O
one O
of O
the O
most O
active O
areas O
of O
research O
with O
advances O
in O
natural O
language O
processing O
and O
computer O
vision O
. O

Recently O
, O
researches O
for O
VQA B-TaskName
have O
advanced O
, O
from O
inferring O
visual O
properties O
on O
entities O
in O
a O
given O
image O
, O
to O
inferring O
commonsense O
or O
world O
knowledge O
about O
those O
entities O
( O
Wang O
et O
al O
. O
, O
2017(Wang O
et O
al O
. O
, O
, O
2018Marino O
et O
al O
. O
, O
2019;Zellers O
et O
al O
. O
, O
2019 O
) O
. O

GLM B-MethodName
: O
He O
was O
a O
voice O
actor O
for O
the O
" O
X O
- O
Men O
" O
cartoon O
series O
. O

He O
was O
also O
a O
voice O
actor O
for O
" O
the O
Simpsons O
" O
as O
well O
as O
" O
the O
marvelous O
misadventures O
of O
superman O
. O

He O
also O
has O
voiced O
characters O
in O
" O
the O
legend O
of O
korra O
" O
He O
has O
appeared O
on O
several O
television O
series O
, O
including O
" O
the O
simpsons O
" O
, O
" O
the O
x O
- O
files O
" O
, O
" O
heroes O
" O
and O
" O
the O
simpsons O
movie O
" O
as O
the O
character O
captain O
billy O
higgledy O
- O
pig O
, O
and O
the O
web O
series O
" O
krusty O
mysteries O
" O
as O
the O
character O
Colonel O
Trungus O
. O

He O
wrote O
for O
" O
the O
Guardian O
" O
newspaper O
. O

Jonathan O
Terry O
was O
born O
in O
London O
. O

Terry O
grew O
up O
in O
Surrey O
, O
England O
and O
attended O
the O
University O
of O
Sussex O
in O
the O
United O
Kingdom O
, O
graduating O
with O
a O
degree O
in O
english O
literature O
. O

He O
was O
a O
guest O
lecturer O
at O
King O
's O
College O
London O
, O
and O
then O
took O
two O
years O
of O
acting O
courses O
at O
the O
brit O
school O
of O
acting O
to O
prepare O
for O
his O
future O
career O
in O
the O
entertainment O
industry O
. O

Terry O
first O
appeared O
in O
the O
TV O
series O
" O
theKnowledge O
- O
based O
visual O
question B-TaskName
answering I-TaskName
( O
QA B-TaskName
) O
aims O
to O
answer O
a O
question O
which O
requires O
visually O
- O
grounded O
external O
knowledge O
beyond O
image O
content O
itself O
. O

Answering O
complex O
questions O
that O
require O
multi O
- O
hop O
reasoning O
under O
weak O
supervision O
is O
considered O
as O
a O
challenging O
problem O
since O
i O
) O
no O
supervision O
is O
given O
to O
the O
reasoning O
process O
and O
ii O
) O
highorder O
semantics O
of O
multi O
- O
hop O
knowledge O
facts O
need O
to O
be O
captured O
. O

In O
this O
paper O
, O
we O
introduce O
a O
concept O
of O
hypergraph O
to O
encode O
highlevel O
semantics O
of O
a O
question O
and O
a O
knowledge O
base O
, O
and O
to O
learn O
high O
- O
order O
associations O
between O
them O
. O

The O
proposed O
model O
, O
Hypergraph B-MethodName
Transformer I-MethodName
, O
constructs O
a O
question O
hypergraph O
and O
a O
query O
- O
aware O
knowledge O
hypergraph O
, O
and O
infers O
an O
answer O
by O
encoding O
inter O
- O
associations O
between O
two O
hypergraphs O
and O
intra O
- O
associations O
in O
both O
hypergraph O
itself O
. O

Extensive O
experiments O
on O
two O
knowledgebased B-TaskName
visual I-TaskName
QA I-TaskName
and O
two O
knowledge B-TaskName
- I-TaskName
based I-TaskName
textual I-TaskName
QA I-TaskName
demonstrate O
the O
effectiveness O
of O
our O
method O
, O
especially O
for O
multi O
- O
hop O
reasoning O
problem O
. O

Our O
source O
code O
is O
available O
at O
https://github.com/yujungheo/ O
kbvqa O
- O
public O
. O

Example O
D.2 O
. O
Jonathan O
Terry O
is O
a O
television O
and O
film O
actor O
. O

Perplexity B-MetricName
is O
an O
evaluation O
criterion O
that O
has O
been O
changes O
to O
the O
senate O
. O

The O
most O
recent O
is O
the O
creation O
of O
a O
six O
- O
seat O
district O
that O
includes O
all O
or O
part O
of O
the O
following O
: O
In O
the O
2009 O
elections O
, O
the O
state O
senate O
members O
were O
elected O
to O
six O
- O
year O
terms O
. O

The O
current O
state O
house O
members O
are O
: O
The O
Wyoming O
Constitution O
assigns O
certain O
powers O
to O
the O
governor O
. O

Most O
notably O
, O
the O
governor O
is O
president O
of O
the O
senate O
and O
governor O
. O

However O
, O
if O
the O
governor O
desires O
to O
appoint O
a O
member O
to O
the O
Wyoming O
state O
senate O
, O
a O
law O
authorizes O
the O
governor O
to O
do O
so O
. O

The O
governor O
of O
Wyoming O
holds O
no O
legislative O
power O
but O
has O
the O
power O
to O
veto O
lawmakers O
, O
which O
is O
not O
limited O
to O
the O
veto O
of O
laws O
. O

Under O
the O
wyoming O
state O
constitution O
, O
the O
governor O
can O
veto O
the O
actions O
of O
the O
other O
members O
of O
the O
wyoming O
house O
of O
representatives O
. O

The O
governor O
can O
also O
appoint O
members O
of O
the O
wyoming O
senate O
. O

In O
addition O
, O
the O
governor O
can O
appoint O
members O
of O
the O
Wyoming O
house O
of O
representatives O
. O

Wyoming O
's O
constitution O
provides O
that O
the O
governor O
can O
appoint O
a O
member O
of O
the O
wyoming O
state O
senate O
to O
the O
wyoming O
supreme O
court O
, O
and O
the O
chairman O
of O
the O
wyoming O
senate O
. O

For O
comparison O
with O
previous O
work O
, O
we O
use O
the O
same O
test O
set O
constructed O
by O
( O
Shen O
et O
al O
. O
, O
2020 O
) O
. O

The O
evaluation O
metric O
is O
the O
BLEU B-MetricName
score O
of O
the O
infilled O
text O
against O
the O
original O
document O
. O

We O
compare O
with O
two O
baselines O
: O
( O
1 O
) O
BERT B-MethodName
, O
which O
learns O
a O
left O
- O
to O
- O
right O
language O
model O
to O
generate O
the O
masked O
tokens O
on O
top O
of O
the O
blank O
representation O
, O
and O
( O
2 O
) O
BLM B-MethodName
proposed O
by O
( O
Shen O
et O
al O
. O
, O
2020 O
) O
, O
which O
can O
fill O
in O
the O
blank O
with O
arbitrary O
trajectories O
. O

We O
evaluate O
the O
model O
's O
ability O
of O
language B-TaskName
modeling I-TaskName
with O
perplexity B-MetricName
on O
BookWiki B-DatasetName
and O
accuracy B-MetricName
on O
the O
LAMBDA B-DatasetName
dataset O
( O
Paperno O
et O
al O
. O
, O
2016 O
) O
. O

To O
train O
GLM B-MethodName
RoBERTa I-MethodName
, O
we O
follow O
most O
of O
the O
hyperparameters O
of O
RoBERTa B-MethodName
. O

The O
main O
difference O
baselines O
on O
seq2seq B-TaskName
tasks O
are O
obtained O
from O
the O
corresponding O
papers O
. O

We O
follow O
( O
Shen O
et O
al O
. O
, O
2020 O
) O
and O
evaluate O
text B-TaskName
infilling I-TaskName
performance O
on O
the O
Yahoo B-DatasetName
Answers I-DatasetName
dataset O
( O
Yang O
et O
al O
. O
, O
2017 O
) O
, O
which O
contains O
100K/10K/10 B-HyperparameterValue
K I-HyperparameterValue
documents I-HyperparameterValue
for O
train B-HyperparameterName
/ I-HyperparameterName
valid I-HyperparameterName
/ I-HyperparameterName
test I-HyperparameterName
respectively O
. O

The O
average O
document O
length O
is O
78 O
words O
. O

To O
construct O
the O
text B-TaskName
infilling I-TaskName
task O
, O
we O
randomly O
mask O
a O
given O
ratio B-HyperparameterName
r O
∈ O
{ O
10 B-HyperparameterValue
% I-HyperparameterValue
• O
• O
• O
50 B-HyperparameterValue
% I-HyperparameterValue
} O
of O
each O
document O
's O
tokens O
and O
the O
contiguous O
masked O
tokens O
are O
collapsed O
into O
a O
single O
blank O
. O

We O
finetune O
GLM B-MethodName
Large I-MethodName
on O
the O
training O
set O
for O
5 B-HyperparameterValue
epochs B-HyperparameterName
with O
dynamic O
masking O
, O
i.e. O
the O
blanks O
are O
randomly O
generated O
at O
training O
time O
. O

Similar O
to O
the O
sequence O
- O
to O
- O
sequence O
experiments O
, O
we O
use O
an O
AdamW B-HyperparameterValue
optimizer B-HyperparameterName
with O
a O
peak B-HyperparameterName
learning I-HyperparameterName
rate I-HyperparameterName
1e-5 B-HyperparameterValue
and O
6 B-HyperparameterValue
% I-HyperparameterValue
warm B-HyperparameterName
- I-HyperparameterName
up I-HyperparameterName
linear I-HyperparameterName
scheduler I-HyperparameterName
. O

To O
train O
GLM B-MethodName
RoBERTa I-MethodName
, O
we O
follow O
the O
pretraining O
datasets O
of O
RoBERTa B-MethodName
, O
which O
consist O
of O
BookCorups B-DatasetName
( O
Zhu O
et O
al O
. O
, O
2015),Wikipedia B-DatasetName
( O
16 O
GB O
) O
, O
CC B-DatasetName
- I-DatasetName
News I-DatasetName
( O
the O
English B-DatasetName
portion I-DatasetName
of I-DatasetName
the I-DatasetName
Com I-DatasetName
- I-DatasetName
monCrawl I-DatasetName
News I-DatasetName
dataset O
3 O
76 O
GB O
) O
, O
OpenWebText B-DatasetName
( O
web O
content O
extracted O
from O
URLs O
shared O
on O
Reddit O
with O
at O
least O
three O
upvotes O
( O
Gokaslan O
and O
Cohen O
, O
2019 O
) O
, O
38 O
GB O
) O
and O
Stories B-DatasetName
( O
subset O
of O
Common B-DatasetName
- I-DatasetName
Crawl I-DatasetName
data O
filtered O
to O
match O
the O
story O
- O
like O
style O
of O
Winograd B-DatasetName
schemas I-DatasetName
( O
Trinh O
and O
Le O
, O
2019 O
) O
, O
31 O
GB O
) O
. O

The O
Stories B-DatasetName
dataset O
is O
no O
longer O
publicly O
available O
4 O
. O

Therefore O
, O
we O
remove O
the O
Stories B-DatasetName
dataset O
and O
replace O
OpenWebText B-DatasetName
with O
OpenWebText2 B-DatasetName
5 I-DatasetName
( O
66 O
GB O
) O
. O

The O
CC B-DatasetName
- I-DatasetName
News I-DatasetName
dataset O
is O
not O
publicly O
available O
and O
we O
use O
the O
CC B-DatasetName
- I-DatasetName
News I-DatasetName
- I-DatasetName
en I-DatasetName
published O
by O
( O
Mackenzie O
et O
al O
. O
, O
2020 O
) O
. O

All O
the O
datasets O
used O
total O
158 O
GB O
of O
uncompressed O
texts O
, O
close O
in O
size O
to O
RoBERTa B-MethodName
's O
160 O
GB O
datasets O
. O

The O
hyperparameters O
for O
GLM B-MethodName
Base I-MethodName
and O
GLM B-MethodName
Large I-MethodName
are O
similar O
to O
those O
used O
by O
BERT B-MethodName
. O

For O
trade O
- O
off O
of O
training O
speed O
and O
fair O
comparison O
with O
BERT B-MethodName
( O
batch B-HyperparameterName
size I-HyperparameterName
256 B-HyperparameterValue
and O
1,000,000 B-HyperparameterValue
training B-HyperparameterName
steps I-HyperparameterName
) O
, O
we O
use O
batch B-HyperparameterName
size I-HyperparameterName
of O
1024 B-HyperparameterValue
and O
200,000 B-HyperparameterValue
training B-HyperparameterName
steps I-HyperparameterName
for O
GLM B-MethodName
Large I-MethodName
. O

Since O
GLM B-MethodName
Base I-MethodName
is O
smaller O
, O
we O
reduce O
the O
number B-HyperparameterName
of I-HyperparameterName
training I-HyperparameterName
steps I-HyperparameterName
to O
120,000 B-HyperparameterValue
to O
speed O
up O
pre O
- O
training O
. O

The O
hyperparameters O
for O
GLM B-MethodName
Doc I-MethodName
and O
GLM B-MethodName
Sent I-MethodName
are O
the O
same O
as O
those O
of O
GLM B-MethodName
Large I-MethodName
. O

The O
hyperparameters O
except O
Transformer O
architecture O
for O
GLM B-MethodName
410 I-MethodName
M I-MethodName
and O
GLM B-MethodName
515 I-MethodName
M I-MethodName
are O
the O
same O
as O
those O
of O
GLM B-MethodName
Large I-MethodName
. O

The O
models O
are O
trained O
on O
64 B-HyperparameterValue
V100 I-HyperparameterValue
GPUs B-HyperparameterName
for O
200 B-HyperparameterValue
K I-HyperparameterValue
steps B-HyperparameterName
with O
batch B-HyperparameterName
size I-HyperparameterName
of O
1024 B-HyperparameterValue
and O
maximum B-HyperparameterName
sequence I-HyperparameterName
length I-HyperparameterName
of O
512 B-HyperparameterValue
, O
which O
takes O
about O
2.5 O
days O
for O
GLM B-MethodName
Large I-MethodName
. O

GLM B-MethodName
: O
In O
his O
four O
- O
year O
NFL O
career O
, O
he O
played O
in O
33 O
games O
and O
started O
14 O
, O
registering O
62 O
career O
interceptions O
. O

He O
completed O
his O
NFL O
career O
with O
five O
interceptions O
. O

Smith O
was O
born O
in O
La O
Canada O
Flintridge O
, O
Michigan O
, O
in O
1938 O
. O

He O
attended O
Bishop O
O O
' O
Dowd O
high O
school O
in O
Flintridge O
. O

Smith O
' O
s O
father O
was O
a O
pharmacist O
. O

He O
graduated O
from O
Michigan O
State O
University O
in O
1958 O
with O
a O
degree O
in O
business O
administration O
. O

Smith O
played O
for O
the O
Michigan O
Wolverines O
football O
team O
from O
1959 O
to O
1963 O
. O

In O
his O
two O
years O
as O
a O
reserve O
cornerback O
, O
he O
led O
the O
conference O
in O
interceptions O
with O
five O
. O

In O
1962 O
, O
he O
set O
the O
Wolverines O
' O
all O
- O
time O
interception O
record O
with O
13 O
, O
and O
was O
second O
overall O
in O
the O
1962 O
season O
's O
Heisman O
Trophy O
voting O
. O

Smith O
also O
won O
the O
Dick O
Butkus O
award O
as O
the O
nation O
's O
outstanding O
linebacker O
. O

In O
1961 O
, O
the O
" O
Los O
Angeles O
Times O
" O
wrote O
that O
Smith O
" O
is O
an O
outstanding O
pass O
rusher O
, O
with O
an O
average O
of O
almost O
100 O
yards O
per O
punt O
return O
. O

" O
Smith O
was O
inducted O
into O
the O
university O
of O
Michigan O
athletic O
hall O
of O
honor O
in O
1989 O
and O
the O
national O
football O
foundation O
hall O
of O
fame O
in O
1991 O
. O

He O
was O
elected O
to O
the O
Michigan O
sports O
hall O
of O
fame O
in O
1995 O
. O

Smith O
earned O
the O
honor O
because O
of O
his O
accomplishments O
prior O
to O
his O
NFL O
career O
. O

He O
was O
one O
of O
four O
Michigan O
players O
honored O
as O
first O
- O
overall O
selections O
in O
the O
1964 O
NFL O
draft O
. O

The O
others O
were O
Joe O
Namath O
, O
Bill O
Nelsen O
, O
and O
Jerry O
Kramer O
. O

In O
1966 O
, O
the O
NFL O
gave O
players O
$ O
300,000 O
a O
season O
to O
play O
football O
. O

After O
his O
rookie O
season O
, O
he O
was O
not O
selected O
to O
play O
in O
the O
1966 O
pro O
bowl O
. O

On O
January O
13 O
, O
1966 O
, O
the O
Rams O
traded O
smith O
to O
the O
Detroit O
Lions O
for O
Paul O
Hornung O
, O
and O
later O
that O
year O
he O
was O
traded O
to O
the O
Lions O
for O
Ray O
" O
the O
Lion O
" O
Jones O
in O
exchange O
for O
Linebacker O
Jim O
" O
the O
Hawk O
" O
Johnson O
. O

On O
September O
10 O
, O
1968 O
, O
he O
was O
traded O
back O
to O
Los O
Angeles O
for O
a O
second O
round O
pick O
in O
the O
1970 O
draft O
. O

He O
was O
also O
traded O
to O
the O
St. O
Louis O
Cardinals O
for O
a O
second O
round O
pick O
in O
the O
1970 O
draft O
. O

On O
June O
2 O
, O
1970 O
he O
was O
cut O
by O
the O
Cardinals O
. O

On O
November O
15 O
, O
1970 O
, O
the O
Los O
Angeles O
Rams O
acquired O
Smith O
from O
the O
Lions O
in O
exchange O
for O
Linebacker O
Tony O
Harris O
. O

The O
Rams O
waived O
Smith O
during O
the O
September O
1 O
, O
1972 O
offseason O
. O

Smith O
's O
number O
at O
Michigan O
State O
was O
# O
7 O
in O
1969.The O
work O
is O
supported O
by O
the O
NSFC O
for O
Distinguished O
Young O
Scholar(61825602 O
) O
, O
and O
Beijing O
Academy O
of O
Artificial O
Intelligence O
( O
BAAI).To O
train O
GLM B-MethodName
Base I-MethodName
and O
GLM B-MethodName
Large I-MethodName
, O
we O
use O
Book B-DatasetName
- I-DatasetName
Corpus I-DatasetName
( O
Zhu O
et O
al O
. O
, O
2015 O
) O
and O
Wikipedia B-DatasetName
used O
by O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

Example O
D.4 O
. O
Robert O
Lee O
Smith O
( O
born O
July O
5 O
, O
1938 O
) O
is O
a O
former O
American O
football O
cornerback O
in O
the O
national O
football O
league O
. O

He O
played O
for O
the O
Los O
Angeles O
Rams O
( O
1962)(1963)(1964)(1965 O
) O
and O
the O
Detroit O
Lions O
( O
1965)(1966 O
) O
. O

He O
was O
also O
a O
kick O
and O
punt O
returner O
. O

Example O
D.3 O
. O
Corona O
was O
a O
station O
along O
the O
port O
Washington O
branch O
of O
the O
long O
island O
rail O
road O
in O
the O
Corona O
section O
of O
queens O
, O
New O
York O
City O
. O

It O
was O
one O
of O
two O
stations O
built O
by O
the O
flushing O
railroad O
in O
Corona O
, O
this O
one O
having O
been O
at O
Grand O
Avenue O
( O
later O
called O
National O
Avenue O
, O
now O
National O
Street O
) O
and O
45th O
Avenue O
. O

GLM O
: O
It O
had O
a O
brick O
entrance O
building O
next O
to O
the O
tracks O
, O
and O
one O
platform O
with O
2 O
side O
platforms O
. O

The O
station O
burned O
down O
and O
was O
rebuilt O
in O
1908 O
. O

The O
station O
closed O
on O
september O
15 O
, O
1927 O
, O
with O
the O
train O
service O
transferred O
from O
Grand O
Avenue O
to O
45th O
Avenue O
. O

There O
were O
plans O
to O
build O
a O
subway O
extension O
to O
Corona O
, O
but O
it O
was O
never O
built O
. O

In O
the O
1980s O
, O
two O
stations O
were O
constructed O
on O
the O
line O
, O
Corona O
Road O
and O
Corona O
Park O
. O

The O
new O
Corona O
station O
opened O
in O
1988 O
, O
and O
the O
original O
Corona O
station O
was O
demolished O
. O

He O
appeared O
in O
the O
first O
few O
episodes O
of O
" O
" O
as O
the O
character O
major O
Jack O
Ryan O
. O

He O
has O
also O
appeared O
as O
part O
of O
the O
supporting O
cast O
of O
several O
episodes O
of O
" O
the O
secret O
life O
of O
pets O
" O
. O

He O
has O
also O
worked O
on O
" O
the O
simpsons O
" O
TV O
show O
since O
" O
the O
simpsons O
movie O
" O
, O
most O
notably O
playing O
the O
roles O
of O
Captain O
Skeletor O
and O
the O
ghost O
of O
the O
same O
name O
. O

He O
plays O
characters O
in O
several O
films O
, O
including O
" O
" O
, O
" O
" O
, O
" O
" O
and O
" O
" O
. O

He O
has O
appeared O
in O
music O
videos O
for O
the O
killers O
in O
1993 O
, O
the O
pretenders O
in O
1995 O
, O
and O
in O
the O
TV O
shows O
" O
the O
royal O
" O
and O
" O
the O
bill O
" O
. O

The O
results O
on O
GLUE B-DatasetName
and O
SQuAD B-DatasetName
are O
shown O
in O
Tables O
9 O
and O
10 O
. O

On O
the O
two O
benchmarks O
, O
GLM B-MethodName
can O
still O
outperform O
BERT B-MethodName
with O
the O
same O
amount O
of O
parameters O
, O
but O
with O
a O
smaller O
margin O
. O

We O
show O
texts O
generated O
by O
GLM B-MethodName
Doc I-MethodName
given O
unseen O
contexts O
randomly O
sampled O
from O
the O
test O
set O
. O

We O
use O
top B-HyperparameterName
- I-HyperparameterName
k I-HyperparameterName
random I-HyperparameterName
sampling I-HyperparameterName
with O
k B-HyperparameterName
= O
40 B-HyperparameterValue
for O
generation O
and O
set O
maximum B-HyperparameterName
sequence I-HyperparameterName
length I-HyperparameterName
to I-HyperparameterName
512 I-HyperparameterName
. O

Some O
of O
the O
texts O
are O
cut O
short O
. O

Simpsons O
" O
as O
the O
character O
captain O
Billy O
Higgledypig O
, O
but O
his O
character O
was O
only O
a O
one O
- O
time O
recurring O
character O
in O
the O
series O
' O
first O
six O
seasons O
. O

He O
later O
appeared O
as O
a O
regular O
for O
the O
show O
's O
final O
six O
seasons O
, O
and O
has O
been O
a O
frequent O
guest O
in O
the O
show O
since O
. O

Results O
of O
T5 B-MethodName
Large I-MethodName
on O
XSum B-DatasetName
are O
obtained O
by O
running O
the O
summarization O
script O
provided O
by O
Huggingface B-HyperparameterValue
transformers I-HyperparameterValue
6 I-HyperparameterValue
. O

All O
the O
other O
results O
of O
well O
studied O
for O
language O
modeling O
. O

Perplexity B-MetricName
is O
the O
exponentiation O
of O
the O
average O
cross O
entropy O
of O
a O
corpus O
. O

LAMBDA B-DatasetName
is O
a O
cloze O
- O
style O
dataset O
to O
test O
the O
ability O
of O
long O
- O
range O
dependency O
modeling O
. O

Each O
example O
is O
a O
passage O
consisting O
of O
4 O
- O
5 O
sentences O
with O
the O
last O
word O
missing O
and O
the O
model O
is O
required O
to O
predict O
the O
last O
word O
of O
the O
passage O
. O

Since O
we O
use O
WordPiece B-HyperparameterValue
tokenization B-HyperparameterName
, O
a O
word O
can O
be O
split O
into O
several O
subword O
units O
. O

We O
use O
teacher B-HyperparameterValue
forcing I-HyperparameterValue
and O
consider O
the O
prediction O
correct O
only O
when O
all O
the O
predicted O
tokens O
are O
correct O
. O

GLUE B-DatasetName
( O
Wang O
et O
al O
. O
, O
2018 O
) O
is O
another O
widely O
- O
used O
NLU B-TaskName
benchmark O
, O
including O
single O
sentence O
tasks O
( O
e.g. O
sentiment B-TaskName
analysis I-TaskName
( O
Socher O
et O
al O
. O
, O
2013 O
) O
) O
and O
sentence O
pair O
tasks O
( O
e.g. O
text B-TaskName
similarity I-TaskName
( O
Cer O
et O
al O
. O
, O
2017 O
) O
and O
natural B-TaskName
language I-TaskName
inference I-TaskName
( O
Williams O
et O
al O
. O
, O
2018;Dagan O
et O
al O
. O
, O
2005 O
) O
) O
. O

The O
benchmark O
is O
usually O
considered O
as O
less O
challenging O
than O
Super B-DatasetName
- I-DatasetName
GLUE I-DatasetName
. O

SQuAD B-DatasetName
( O
Rajpurkar O
et O
al O
. O
, O
2016(Rajpurkar O
et O
al O
. O
, O
, O
2018 O
) O
is O
an O
extractive B-TaskName
question I-TaskName
answering I-TaskName
benchmark O
. O

We O
further O
compare O
GLM B-MethodName
with O
BERT B-MethodName
on O
the O
two O
benchmarks O
. O

For O
the O
question B-TaskName
generation I-TaskName
task O
, O
we O
use O
the O
SQuAD B-DatasetName
1.1 I-DatasetName
dataset O
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
and O
follow O
the O
dataset B-HyperparameterName
split I-HyperparameterName
of O
( O
Du O
et O
al O
. O
, O
2017 O
) O
. O

The O
optimizer B-HyperparameterName
hyperparameters I-HyperparameterName
are O
the O
same O
as O
those O
of O
abstractive B-TaskName
summarization I-TaskName
. O

The O
maximum B-HyperparameterName
passage I-HyperparameterName
length I-HyperparameterName
is O
464 B-HyperparameterValue
and O
the O
maximum B-HyperparameterName
question I-HyperparameterName
length I-HyperparameterName
is O
48 B-HyperparameterValue
. O

During O
decoding O
, O
we O
use O
beam B-HyperparameterValue
search I-HyperparameterValue
with O
beam B-HyperparameterName
size I-HyperparameterName
5 B-HyperparameterValue
and O
tweak O
the O
value O
of O
length O
penalty O
on O
the O
development O
set O
. O

The O
evaluation O
metrics O
are O
the O
scores O
of O
BLEU-1 B-MetricName
, O
BLEU-2 B-MetricName
, O
BLEU-3 B-MetricName
, O
BLEU-4 B-MetricName
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
, O
METEOR B-MetricName
( O
Denkowski O
and O
Lavie O
, O
2014 O
) O
and O
Rouge B-MetricName
- I-MetricName
L I-MetricName
( O
Lin O
, O
2004 O
) O
. O

The O
learning B-HyperparameterName
rate I-HyperparameterName
has O
a O
peak O
value O
of O
3e-5 B-HyperparameterValue
, O
warmup B-HyperparameterName
over O
the O
6 B-HyperparameterValue
% I-HyperparameterValue
training I-HyperparameterValue
steps I-HyperparameterValue
and O
a O
linear B-HyperparameterValue
decay I-HyperparameterValue
. O

We O
also O
use O
label B-HyperparameterName
smoothing I-HyperparameterName
with O
rate O
0.1 B-HyperparameterValue
( O
Pereyra O
et O
al O
. O
, O
2017 O
) O
. O

The O
maximum B-HyperparameterName
document I-HyperparameterName
length I-HyperparameterName
is O
192 B-HyperparameterValue
and O
the O
maximum B-HyperparameterName
summary I-HyperparameterName
length I-HyperparameterName
is O
32 B-HyperparameterValue
. O

During O
decoding O
, O
we O
use O
beam B-HyperparameterValue
search I-HyperparameterValue
with O
beam B-HyperparameterName
size I-HyperparameterName
of O
5 B-HyperparameterValue
and O
remove O
repeated O
trigrams O
. O

We O
tweak O
the O
value O
of O
length O
penalty O
on O
the O
development O
set O
. O

The O
evaluation O
metrics O
are O
the O
F1 B-MetricName
scores O
of O
Rouge-1 B-MetricName
, O
Rouge-2 B-MetricName
, O
and O
Rouge B-MetricName
- I-MetricName
L I-MetricName
( O
Lin O
, O
2004 O
) O
on O
the O
test O
set O
. O

For O
the O
baseline O
classifiers O
, O
we O
follow O
the O
standard O
practice O
to O
concatenate O
the O
input O
parts O
of O
each O
task O
( O
such O
as O
the O
premise O
and O
hypothesis O
for O
textual B-TaskName
entailment I-TaskName
, O
or O
the O
passage O
, O
question B-TaskName
and I-TaskName
answer I-TaskName
for O
ReCORD O
and O
MultiRC O
) O
and O
add O
a O
classification B-HyperparameterName
layer I-HyperparameterName
on O
top O
of O
the O
[ O
CLS O
] O
token O
representation O
. O

We O
also O
implemented O
cloze O
- O
style O
finetuning O
for O
the O
other O
pre O
- O
trained O
models O
, O
but O
the O
performance O
was O
usually O
similar O
to O
the O
standard O
classifier O
, O
as O
we O
shown O
in O
the O
ablation O
study O
. O

Models O
with O
blank O
- O
infilling O
objectives O
, O
such O
as O
T5 B-MethodName
and O
our O
GLM B-MethodName
, O
benefits O
more O
from O
converting O
the O
NLU B-TaskName
tasks O
into O
cloze B-TaskName
questions I-TaskName
. O

Thus O
for O
T5 B-MethodName
and O
GLM B-MethodName
, O
we O
report O
the O
performance O
after O
such O
conversion O
in O
our O
main O
results O
. O

Fot O
the O
text O
summarization O
task O
, O
we O
use O
the O
dataset O
Gigaword B-DatasetName
( O
Rush O
et O
al O
. O
, O
2015 O
) O
for O
model O
fine O
- O
tuning O
and O
evaluation O
. O

We O
finetune O
GLM B-MethodName
LARGE I-MethodName
on O
the O
training O
set O
for O
4 B-HyperparameterValue
epochs B-HyperparameterName
with O
AdamW B-HyperparameterValue
optimizer B-HyperparameterName
. O

When O
finetuning O
GLM B-MethodName
on O
the O
SuperGLUE B-DatasetName
tasks O
, O
we O
construct O
the O
input O
using O
the O
cloze O
questions O
in O
Table O
8 O
and O
replace O
the O
blank O
with O
a O
[ O
MASK O
] O
token O
. O

Then O
we O
compute O
the O
score O
of O
generating O
each O
answer O
candidate O
. O

For O
the O
5 O
single O
- O
token O
tasks O
, O
the O
score O
is O
defined O
to O
be O
the O
logit O
of O
the O
verbalizer O
token O
. O

For O
the O
3 O
multi O
- O
token O
tasks O
, O
we O
use O
the O
sum O
of O
the O
log O
- O
probabilities O
of O
the O
verbalizer O
tokens O
. O

Thanks O
to O
the O
autoregressive O
blank O
infilling O
mechanism O
we O
proposed O
, O
we O
can O
obtain O
all O
the O
log O
- O
probabilities O
in O
one O
pass O
. O

Then O
we O
compute O
the O
cross B-HyperparameterValue
entropy I-HyperparameterValue
loss B-HyperparameterName
using O
the O
groundtruth O
label O
and O
update O
the O
model O
parameters O
. O

Blank B-MethodName
Language I-MethodName
Modeling I-MethodName
. O

Donahue O
et O
al O
. O
( O
2020 O
) O
and O
Shen O
et O
al O
. O
( O
2020 O
) O
also O
study O
blanking O
infilling O
models O
. O

Different O
from O
their O
work O
, O
we O
pre O
- O
train O
language O
models O
with O
blank O
infilling O
objectives O
and O
evaluate O
their O
performance O
in O
downstream O
NLU B-TaskName
and O
generation B-TaskName
tasks O
. O

GLM B-MethodName
is O
a O
general O
pretraining O
framework O
for O
natural B-TaskName
language I-TaskName
understanding I-TaskName
and I-TaskName
generation I-TaskName
. O

We O
show O
that O
the O
NLU B-TaskName
tasks O
can O
be O
formulated O
as O
conditional B-TaskName
generation I-TaskName
tasks I-TaskName
, O
and O
therefore O
solvable O
by O
autoregressive O
models O
. O

GLM B-MethodName
unifies O
the O
pretraining O
objectives O
for O
different O
tasks O
as O
autoregressive O
blank O
infilling O
, O
with O
mixed O
attention O
masks O
and O
the O
novel O
2D B-HyperparameterValue
position I-HyperparameterValue
encodings I-HyperparameterValue
. O

Empirically O
we O
show O
that O
GLM B-MethodName
outperforms O
previous O
methods O
for O
NLU B-TaskName
tasks O
and O
can O
effectively O
share O
parameters O
for O
different O
tasks O
. O

The O
hyperparameters O
for O
all O
the O
pre O
- O
training O
settings O
are O
summarized O
in O
Table O
7.Our O
pretraining O
implementation O
is O
based O
on O
Megatron B-MethodName
- I-MethodName
LM I-MethodName
( O
Shoeybi O
et O
al O
. O
, O
2019 O
) O
and O
Deep B-MethodName
- I-MethodName
Speed I-MethodName
( O
Rasley O
et O
al O
. O
, O
2020 O
) O
. O

We O
include O
our O
code O
in O
the O
supplementary O
material O
. O

Due O
to O
the O
size O
limit O
of O
supplementary O
material O
, O
we O
can O
not O
include O
the O
pretrained O
models O
, O
but O
will O
make O
them O
public O
available O
in O
the O
future O
. O

The O
SuperGLUE B-DatasetName
benchmark O
consists O
of O
8 O
NLU O
tasks O
. O

We O
formulate O
them O
as O
blank B-TaskName
infilling I-TaskName
tasks O
, O
following O
( O
Schick O
and O
Schütze O
, O
2020b O
) O
. O

Table O
8 O
shows O
the O
cloze O
questions O
and O
verbalizers O
we O
used O
in O
our O
experiments O
. O

For O
3 O
tasks O
( O
ReCoRD B-DatasetName
, O
COPA B-DatasetName
, O
and O
WSC B-DatasetName
) O
, O
the O
answer O
may O
consist O
of O
multiple O
tokens O
, O
and O
for O
the O
other O
5 O
tasks O
, O
the O
answer O
is O
always O
a O
single O
token O
. O

NLU B-TaskName
as O
Generation O
. O

Previously O
, O
pretrained O
language O
models O
complete O
classification B-TaskName
tasks O
for O
NLU B-TaskName
with O
linear O
classifiers O
on O
the O
learned O
representations O
. O

GPT-2 B-MethodName
( O
Radford O
et O
al O
. O
, O
2018b O
) O
and O
GPT-3 B-MethodName
( O
Brown O
et O
al O
. O
, O
2020 O
) O
show O
that O
generative O
language O
models O
can O
complete O
NLU B-TaskName
tasks O
such O
as O
question B-TaskName
answering I-TaskName
by O
directly O
predicting O
the O
correct O
answers O
without O
finetuning O
, O
given O
task O
instructions O
or O
a O
few O
labeled O
examples O
. O

However O
, O
generative O
models O
require O
much O
more O
parameters O
to O
work O
due O
to O
the O
limit O
of O
unidirectional O
attention O
. O

Recently O
, O
PET O
( O
Schick O
and O
Schütze O
, O
2020a O
, O
b O
) O
proposes O
to O
reformulate O
input O
examples O
as O
cloze B-TaskName
questions I-TaskName
with O
patterns O
similar O
to O
the O
pretraining O
corpus O
in O
the O
few O
- O
shot O
setting O
. O

It O
has O
been O
shown O
that O
combined O
with O
gradient O
- O
based O
finetuning O
, O
PET O
can O
achieve O
better O
performance O
in O
the O
few O
- O
shot O
setting O
than O
GPT-3 B-MethodName
while O
requiring O
only O
0.1 O
% O
of O
its O
parameters O
. O

Similarly O
, O
Athiwaratkun O
et O
al O
. O
( O
2020 O
) O
and O
Paolini O
et O
al O
. O
( O
2020 O
) O
convert O
structured B-TaskName
prediction I-TaskName
tasks O
, O
such O
as O
sequence B-TaskName
tagging I-TaskName
and O
relation B-TaskName
extraction I-TaskName
, O
to O
sequence B-TaskName
generation I-TaskName
tasks O
. O

Among O
encoder O
- O
decoder O
models O
, O
BART B-MethodName
conducts O
NLU B-TaskName
tasks O
by O
feeding O
the O
same O
input O
into O
the O
encoder O
and O
decoder O
, O
and O
taking O
the O
final O
hidden O
states O
of O
the O
decoder O
. O

Instead O
, O
T5 B-MethodName
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
formulates O
most O
language O
tasks O
in O
the O
text O
- O
to O
- O
text O
framework O
. O

However O
, O
both O
models O
require O
more O
parameters O
to O
outperform O
autoencoding O
models O
such O
as O
RoBERTa B-MethodName
. O

UniLM B-MethodName
( O
Dong O
et O
al O
. O
, O
2019;Bao O
et O
al O
. O
, O
2020 O
) O
unifies O
three O
pretraining O
models O
under O
the O
masked O
language O
modeling O
objective O
with O
different O
attention O
masks O
. O

( O
1 O
) O
GLM B-MethodName
consists O
of O
a O
single B-HyperparameterValue
encoder B-HyperparameterName
, O
( O
2 O
) O
GLM B-MethodName
shuffles O
the O
masked O
spans O
, O
and O
( O
3 O
) O
GLM B-MethodName
uses O
a O
single O
[ O
MASK O
] O
instead O
of O
multiple O
sentinel O
tokens O
. O

While O
we O
can O
not O
directly O
compare O
GLM B-MethodName
with O
T5 B-MethodName
due O
to O
the O
differences O
in O
training O
data O
and O
the O
number B-HyperparameterName
of I-HyperparameterName
parameters I-HyperparameterName
, O
the O
results O
in O
Tables O
1 O
and O
6 O
have O
demonstrated O
the O
advantage O
of O
GLM.Pretrained B-MethodName
Language O
Models O
. O

Pretraining O
largescale O
language O
models O
significantly O
improves O
the O
performance O
of O
downstream O
tasks O
. O

There O
are O
three O
types O
of O
pretrained O
models O
. O

First O
, O
autoencoding O
models O
learn O
a O
bidirectional O
contextualized O
encoder O
for O
natural O
language O
understanding O
via O
denoising O
objectives O
( O
Devlin O
et O
al O
. O
, O
2019;Joshi O
et O
al O
. O
, O
2020;Yang O
et O
al O
. O
, O
2019;Lan O
et O
al O
. O
, O
2020;Clark O
et O
al O
. O
, O
2020 O
) O
. O

Second O
, O
autoregressive O
models O
are O
trained O
with O
a O
left O
- O
to O
- O
right O
language O
modeling O
objective O
( O
Radford O
et O
al O
. O
, O
2018a O
, O
b;Brown O
et O
al O
. O
, O
2020 O
) O
. O

Third O
, O
encoder O
- O
decoder O
models O
are O
pretrained O
for O
sequence O
- O
to O
- O
sequence O
tasks O
( O
Song O
et O
al O
. O
, O
2019;Bi O
et O
al O
. O
, O
2020;Zhang O
et O
al O
. O
, O
2020 O
) O
. O

We O
note O
that O
T5 B-MethodName
is O
pretrained O
with O
a O
similar O
blank O
infilling O
objective O
. O

GLM B-MethodName
differs O
in O
three O
aspects O
: O

The O
results O
are O
shown O
in O
Figure O
4 O
. O

All O
the O
models O
are O
evaluated O
in O
the O
zero O
- O
shot O
setting O
. O

Since O
GLM B-MethodName
learns O
the O
bidirectional O
attention O
, O
we O
also O
evaluate O
GLM B-MethodName
under O
the O
setting O
in O
which O
the O
contexts O
are O
encoded O
with O
bidirectional O
attention O
. O

Without O
generative O
objective O
during O
pretraining O
, O
GLM B-MethodName
Large I-MethodName
can O
not O
complete O
the O
language B-TaskName
modeling I-TaskName
tasks O
, O
with O
perplexity B-MetricName
larger O
than O
100 B-MetricValue
. O

With O
the O
same O
amount O
of O
parameters O
, O
GLM B-MethodName
Doc I-MethodName
performs O
worse O
than O
GPT B-MethodName
Large I-MethodName
. O

This O
is O
expected O
since O
GLM B-MethodName
Doc I-MethodName
also O
optimizes O
the O
blank O
infilling O
objective O
. O

Increasing O
the O
model O
's O
parameters B-HyperparameterName
to O
410 B-HyperparameterValue
M I-HyperparameterValue
( O
1.25× O
of O
GPT O
Large O
) O
leads O
to O
a O
performance O
close O
to O
GPT B-MethodName
Large I-MethodName
. O

GLM B-MethodName
515 I-MethodName
M I-MethodName
( O
1.5× O
of O
GPT O
Large O
) O
can O
further O
outperform O
GPT B-MethodName
Large I-MethodName
. O

With O
the O
same O
amount O
of O
parameters O
, O
encoding O
the O
context O
with O
bidirectional O
attention O
can O
improve O
the O
performance O
of O
language O
modeling O
. O

Under O
this O
setting O
, O
GLM B-MethodName
410 I-MethodName
M I-MethodName
outperforms O
GPT B-MethodName
Large I-MethodName
. O

This O
is O
the O
advantage O
of O
GLM B-MethodName
over O
unidirectional B-MethodName
GPT I-MethodName
. O

We O
also O
study O
the O
contribution O
of O
2D B-HyperparameterValue
positional I-HyperparameterValue
encoding I-HyperparameterValue
to O
long B-TaskName
text I-TaskName
generation I-TaskName
. O

We O
find O
that O
removing O
the O
2D B-HyperparameterValue
positional I-HyperparameterValue
encoding I-HyperparameterValue
leads O
to O
lower O
accuracy B-MetricName
and O
higher O
perplexity B-MetricName
in O
language O
modeling O
. O

Summary O
. O

Above O
all O
, O
we O
conclude O
that O
GLM B-MethodName
effectively O
shares O
model O
parameters O
across O
natural B-TaskName
language I-TaskName
understanding I-TaskName
and O
generation B-TaskName
tasks O
, O
achieving O
better O
performance O
than O
a O
standalone B-MethodName
BERT I-MethodName
, O
encoder B-MethodName
- I-MethodName
decoder I-MethodName
, O
or O
GPT B-MethodName
model I-MethodName
. O

Table O
6 O
shows O
our O
ablation O
analysis O
for O
GLM B-MethodName
. O

First O
, O
to O
provide O
an O
apple O
- O
to O
- O
apple O
comparison O
with O
BERT B-MethodName
, O
we O
train O
a O
BERT B-MethodName
Large I-MethodName
model O
with O
our O
implementation O
, O
data O
, O
and O
hyperparameters O
( O
row O
2 O
) O
. O

The O
performance O
is O
slightly O
worse O
than O
the O
official O
BERT B-MethodName
Large I-MethodName
and O
significantly O
worse O
than O
GLM B-MethodName
Large I-MethodName
. O

It O
confirms O
the O
superiority O
of O
GLM B-MethodName
over O
Masked O
LM O
pretraining O
on O
NLU O
tasks O
. O

Second O
, O
we O
show O
the O
SuperGLUE B-DatasetName
performance O
of O
GLM B-MethodName
finetuned O
as O
sequence O
classifiers O
( O
row O
5 O
) O
and O
BERT B-MethodName
with O
clozestyle O
finetuning O
( O
row O
3 O
) O
. O

Compared O
to O
BERT B-MethodName
with O
cloze O
- O
style O
finetuning O
, O
GLM B-MethodName
benefits O
from O
the O
autoregressive O
pretraining O
. O

Especially O
on O
ReCoRD B-DatasetName
and O
WSC B-DatasetName
, O
where O
the O
verbalizer O
consists O
of O
multiple O
tokens O
, O
GLM B-MethodName
consistently O
outperforms O
BERT B-MethodName
. O

This O
demonstrates O
GLM B-MethodName
's O
advantage O
in O
handling O
variable O
- O
length O
blank O
. O

Another O
observation O
is O
that O
the O
cloze O
formulation O
is O
critical O
for O
GLM B-MethodName
's O
performance O
on O
NLU B-TaskName
tasks O
. O

For O
the O
large O
model O
, O
clozestyle O
finetuning O
can O
improve O
the O
performance O
by O
7 B-MetricValue
points I-MetricValue
. O

Finally O
, O
we O
compare O
GLM B-MethodName
variants O
with O
different O
pretraining B-HyperparameterName
designs I-HyperparameterName
to O
understand O
their O
importance O
. O

Row O
6 O
shows O
that O
removing O
the O
span O
shuffling O
( O
always O
predicting O
the O
masked O
spans O
from O
left O
to O
right O
) O
leads O
to O
a O
severe O
performance O
drop O
on O
SuperGLUE B-DatasetName
. O

Row O
7 O
uses O
different O
sentinel O
tokens O
instead O
of O
a O
single O
[ O
MASK O
] O
token O
to O
represent O
different O
masked O
spans O
. O

The O
model O
performs O
worse O
than O
the O
standard B-MethodName
GLM I-MethodName
. O

We O
hypothesize O
that O
it O
wastes O
some O
modeling O
capacity O
to O
learn O
the O
different O
sentinel O
tokens O
which O
are O
not O
used O
in O
downstream O
tasks O
with O
only O
one O
blank O
. O

In O
Figure O
4 O
, O
we O
show O
that O
removing O
the O
second O
dimension O
of O
2D O
positional O
encoding O
hurts O
the O
performance O
of O
long B-TaskName
text I-TaskName
generation I-TaskName
. O

Text B-TaskName
Infilling I-TaskName
. O
Text B-TaskName
infilling I-TaskName
is O
the O
task O
of O
predicting O
missing O
spans O
of O
text O
which O
are O
consistent O
with O
the O
surrounding O
context O
( O
Zhu O
et O
al O
. O
, O
2019;Donahue O
et O
al O
. O
, O
2020;Shen O
et O
al O
. O
, O
2020 O
) O
. O

GLM B-MethodName
is O
trained O
with O
an O
autoregressive O
blank O
infilling O
objective O
, O
thus O
can O
straightforwardly O
solve O
this O
task O
. O

We O
evaluate O
GLM B-MethodName
on O
the O
Yahoo B-DatasetName
Answers I-DatasetName
dataset I-DatasetName
( O
Yang O
et O
al O
. O
, O
2017 O
) O
and O
compare O
it O
with O
Blank B-MethodName
Language I-MethodName
Model I-MethodName
( O
BLM B-MethodName
) O
( O
Shen O
et O
al O
. O
, O
2020 O
) O
, O
which O
is O
a O
specifically O
designed O
model O
for O
text B-TaskName
infilling I-TaskName
. O

From O
the O
results O
in O
Table O
5 O
, O
GLM B-MethodName
outperforms O
previous O
methods O
by O
large O
margins O
( O
1.3 B-MetricValue
to O
3.9 B-MetricValue
BLEU B-MetricName
) O
and O
achieves O
the O
state O
- O
of O
- O
the O
- O
art O
result O
on O
this O
dataset O
. O

We O
notice O
that O
GLM B-MethodName
Doc I-MethodName
slightly O
underperforms O
GLM B-MethodName
Large I-MethodName
, O
which O
is O
consistent O
with O
our O
observations O
in O
the O
seq2seq O
experiments O
. O

Language B-TaskName
Modeling I-TaskName
. O

Most O
language B-TaskName
modeling I-TaskName
datasets O
such O
as O
WikiText103 B-DatasetName
are O
constructed O
from O
Wikipedia O
documents O
, O
which O
our O
pretraining O
dataset O
already O
contains O
. O

Therefore O
, O
we O
evaluate O
the O
language B-TaskName
modeling I-TaskName
perplexity B-MetricName
on O
a O
held O
- O
out O
test O
set O
of O
our O
pretraining O
dataset O
, O
which O
contains O
about O
20 B-HyperparameterValue
M I-HyperparameterValue
tokens I-HyperparameterValue
, O
denoted O
as O
BookWiki B-DatasetName
. O

We O
also O
evaluate O
GLM B-MethodName
on O
the O
LAMBADA B-DatasetName
dataset O
( O
Paperno O
The O
task O
is O
to O
predict O
the O
final O
word O
of O
a O
passage O
. O

As O
the O
baseline O
, O
we O
train O
a O
GPT B-MethodName
Large I-MethodName
model O
( O
Radford O
et O
al O
. O
, O
2018b;Brown O
et O
al O
. O
, O
2020 O
) O
with O
the O
same O
data O
and O
tokenization O
as O
GLM B-MethodName
Large I-MethodName
. O

The O
results O
for O
models O
trained O
on O
BookCorpus B-DatasetName
and O
Wikipedia B-DatasetName
are O
shown O
in O
Tables O
3 O
and O
4 O
. O

We O
observe O
that O
GLM B-MethodName
Large I-MethodName
can O
achieve O
performance O
matching O
the O
other O
pretraining O
models O
on O
the O
two O
generation O
tasks O
. O

GLM B-MethodName
Sent I-MethodName
can O
perform O
better O
than O
GLM B-MethodName
Large I-MethodName
, O
while O
GLM B-MethodName
Doc I-MethodName
performs O
slightly O
worse O
than O
GLM B-MethodName
Large I-MethodName
. O

This O
indicates O
that O
the O
documentlevel O
objective O
, O
which O
teaches O
the O
model O
to O
extend O
the O
given O
contexts O
, O
is O
less O
helpful O
to O
conditional B-TaskName
generation I-TaskName
, O
which O
aims O
to O
extract O
useful O
information O
from O
the O
context O
. O

Increasing B-MethodName
GLM I-MethodName
Doc I-MethodName
's O
parameters O
to O
410 O
M O
leads O
to O
the O
best O
performance O
on O
both O
tasks O
. O

The O
results O
for O
models O
trained O
on O
larger O
corpora O
are O
shown O
in O
Table O
2 O
. O

GLM B-MethodName
RoBERTa I-MethodName
can O
achieve O
performance O
matching O
the O
seq2seq B-MethodName
BART I-MethodName
model O
, O
and O
outperform O
T5 B-MethodName
and O
UniLMv2 B-MethodName
. O

Considering O
the O
available O
baseline O
results O
, O
we O
use O
the O
Gigaword B-DatasetName
dataset O
( O
Rush O
et O
al O
. O
, O
2015 O
) O
for O
abstractive B-TaskName
summarization I-TaskName
and O
the O
SQuAD B-DatasetName
1.1 I-DatasetName
dataset O
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
for O
question B-TaskName
generation I-TaskName
( O
Du O
et O
al O
. O
, O
2017 O
) O
as O
the O
benchmarks O
for O
models O
pretrained O
on O
BookCorpus B-DatasetName
and O
Wikipedia B-DatasetName
. O

Additionally O
, O
we O
use O
the O
CNN B-DatasetName
/ I-DatasetName
DailyMail I-DatasetName
( O
See O
et O
al O
. O
, O
2017 O
) O
and O
XSum B-DatasetName
( O
Narayan O
et O
al O
. O
, O
2018 O
) O
datasets O
for O
abstractive B-TaskName
summarization I-TaskName
as O
the O
benchmarks O
for O
models O
pretrained O
on O
larger O
corpora O
. O

Table O
1 O
shows O
the O
results O
. O

With O
the O
same O
amount O
of O
training O
data O
, O
GLM B-MethodName
consistently O
outperforms O
BERT B-MethodName
on O
most O
tasks O
with O
either O
base O
or O
large O
architecture O
. O

The O
only O
exception O
is O
WiC B-TaskName
( O
word B-TaskName
sense I-TaskName
disambiguation I-TaskName
) O
. O

On O
average O
, O
GLM B-MethodName
Base I-MethodName
scores O
4.6 B-MetricValue
% I-MetricValue
higher O
than O
BERT B-MethodName
Base I-MethodName
, O
and O
GLM B-MethodName
Large I-MethodName
scores O
5.0 B-MetricValue
% I-MetricValue
higher O
than O
BERT B-MethodName
Large I-MethodName
. O

It O
clearly O
demonstrates O
the O
advantage O
of O
our O
method O
in O
NLU B-TaskName
tasks O
. O

In O
the O
setting O
of O
RoBERTa B-MethodName
Large I-MethodName
, O
GLM B-MethodName
RoBERTa I-MethodName
can O
still O
achieve O
improvements O
over O
the O
baselines O
, O
but O
with O
a O
smaller O
margin O
. O

Specifically O
, O
GLM B-MethodName
RoBERTa I-MethodName
outperforms O
T5 B-MethodName
Large I-MethodName
but O
is O
only O
half O
its O
size O
. O

We O
also O
find O
that O
BART B-MethodName
does O
not O
perform O
well O
on O
the O
challenging O
SuperGLUE B-DatasetName
benchmark O
. O

We O
conjecture O
this O
can O
be O
attributed O
to O
the O
low O
parameter O
efficiency O
of O
the O
encoder O
- O
decoder O
architecture O
and O
the O
denoising O
sequence O
- O
to O
- O
sequence O
objective O
. O

Then O
we O
evaluate O
the O
GLM B-MethodName
's O
performance O
in O
a O
multi O
- O
task O
setting O
( O
Section O
2.1 O
) O
. O

Within O
one O
training O
batch O
, O
we O
sample O
short O
spans O
and O
longer O
spans O
( O
document O
- O
level O
or O
sentence O
- O
level O
) O
with O
equal O
chances O
. O

We O
evaluate O
the O
multi O
- O
task O
model O
for O
NLU B-TaskName
, O
seq2seq B-TaskName
, O
blank B-TaskName
infilling I-TaskName
, O
and O
zero B-TaskName
- I-TaskName
shot I-TaskName
language O
modeling O
. O

SuperGLUE B-DatasetName
. O

For O
NLU B-TaskName
tasks O
, O
we O
evaluate O
models O
on O
the O
SuperGLUE B-DatasetName
benchmark O
. O

The O
results O
Sequence O
- O
to O
- O
Sequence O
. O

For O
a O
fair O
comparison O
with O
GLM B-MethodName
Base I-MethodName
and O
GLM B-MethodName
Large I-MethodName
, O
we O
choose O
BERT B-MethodName
Base I-MethodName
and O
BERT B-MethodName
Large I-MethodName
as O
our O
baselines O
, O
which O
are O
pretrained O
on O
the O
same O
corpus O
and O
for O
a O
similar O
amount O
of O
time O
. O

We O
report O
the O
performance O
of O
standard O
finetuning O
( O
i.e. O
classification O
on O
the O
[ O
CLS O
] O
token O
representation O
) O
. O

The O
performance O
of O
BERT B-MethodName
with O
cloze B-TaskName
questions I-TaskName
is O
reported O
in O
Section O
3.4 O
. O

To O
compare O
with O
GLM B-MethodName
RoBERTa B-MethodName
, O
we O
choose O
T5 B-MethodName
, O
BART B-MethodName
Large I-MethodName
, O
and O
RoBERTa B-MethodName
Large I-MethodName
as O
our O
baselines O
. O

T5 B-MethodName
has O
no O
direct O
match O
in O
the O
number O
of O
parameters O
for O
BERT B-MethodName
Large I-MethodName
, O
so O
we O
present O
the O
results O
of O
both O
T5 B-MethodName
Base I-MethodName
( O
220 O
M O
parameters O
) O
and O
T5 B-MethodName
Large I-MethodName
( O
770 O
M O
parameters O
) O
. O

All O
the O
other O
baselines O
are O
of O
similar O
size O
to O
BERT B-MethodName
Large I-MethodName
. O

To O
compare O
with O
SOTA O
models O
, O
we O
also O
train O
a O
Large O
- O
sized O
model O
with O
the O
same O
data O
, O
tokenization O
, O
and O
hyperparameters O
as O
RoBERTa B-MethodName
, O
denoted O
as O
GLM B-MethodName
RoBERTa I-MethodName
. O

Due O
to O
resource O
limitations O
, O
we O
only O
pretrain O
the O
model O
for O
250,000 B-HyperparameterValue
steps I-HyperparameterValue
, O
which O
are O
half O
of O
RoBERTa B-MethodName
and O
BART B-MethodName
's O
training O
steps O
and O
close O
to O
T5 B-MethodName
in O
the O
number B-HyperparameterName
of I-HyperparameterName
trained I-HyperparameterName
tokens I-HyperparameterName
. O

More O
experiment O
details O
can O
be O
found O
in O
Appendix O
A.To O
evaluate O
our O
pretrained O
GLM B-MethodName
models O
, O
we O
conduct O
experiments O
on O
the O
SuperGLUE B-DatasetName
bench O
- O
mark O
and O
report O
the O
standard B-MetricName
metrics I-MetricName
. O

SuperGLUE B-DatasetName
consists O
of O
8 O
challenging O
NLU B-TaskName
tasks O
. O

We O
reformulate O
the O
classification B-TaskName
tasks O
as O
blank B-TaskName
infilling I-TaskName
with O
human O
- O
crafted O
cloze B-TaskName
questions I-TaskName
, O
following O
PET O
( O
Schick O
and O
Schütze O
, O
2020b O
) O
. O

Then O
we O
finetune O
the O
pretrained O
GLM B-MethodName
models O
on O
each O
task O
as O
described O
in O
Section O
2.3 O
. O

The O
cloze B-TaskName
questions I-TaskName
and O
other O
details O
can O
be O
found O
in O
Appendix O
B.1 O
. O

For O
multi O
- O
task O
pretraining O
, O
we O
train O
two O
Largesized O
models O
with O
a O
mixture O
of O
the O
blank O
infilling O
objective O
and O
the O
document O
- O
level O
or O
sentencelevel O
objective O
, O
denoted O
as O
GLM B-MethodName
Doc I-MethodName
and O
GLM B-MethodName
Sent I-MethodName
. O

Additionally O
, O
we O
train O
two O
larger O
GLM B-MethodName
models O
of O
410 O
M O
( O
30 B-HyperparameterValue
layers B-HyperparameterName
, O
hidden B-HyperparameterName
size I-HyperparameterName
1024 B-HyperparameterValue
, O
and O
16 B-HyperparameterValue
attention B-HyperparameterName
heads I-HyperparameterName
) O
and O
515 O
M O
( O
30 B-HyperparameterValue
layers B-HyperparameterName
, O
hidden B-HyperparameterName
size I-HyperparameterName
1152 B-HyperparameterValue
, O
and O
18 B-HyperparameterValue
attention B-HyperparameterName
heads I-HyperparameterName
) O
parameters O
with O
documentlevel O
multi O
- O
task O
pretraining O
, O
denoted O
as O
GLM B-MethodName
410 I-MethodName
M I-MethodName
and O
GLM B-MethodName
515 I-MethodName
M I-MethodName
. O

Comparison O
with O
UniLM B-MethodName
( O
Dong O
et O
al O
. O
, O
2019 O
) O
. O

UniLM B-MethodName
combines O
different O
pretraining O
objectives O
under O
the O
autoencoding O
framework O
by O
changing O
the O
attention O
mask O
among O
bidirectional O
, O
unidirectional O
, O
and O
cross O
attention O
. O

However O
, O
UniLM B-MethodName
always O
replaces O
masked O
spans O
with O
[ O
MASK O
] O
tokens O
, O
which O
limits O
its O
ability O
to O
model O
the O
dependencies O
between O
the O
masked O
spans O
and O
their O
context O
. O

GLM B-MethodName
feeds O
in O
the O
previous O
token O
and O
autoregressively O
generates O
the O
next O
token O
. O

Finetuning O
UniLM B-MethodName
on O
downstream O
generation O
tasks O
also O
relies O
on O
masked O
language O
modeling O
, O
which O
is O
less O
efficient O
. O

UniLMv2 B-MethodName
( O
Bao O
et O
al O
. O
, O
2020 O
) O
adopts O
partially O
autoregressive O
modeling O
for O
generation O
tasks O
, O
along O
with O
the O
autoencoding O
objective O
for O
NLU B-TaskName
tasks O
. O

Instead O
, O
GLM B-MethodName
unifies O
NLU B-TaskName
and O
generation O
tasks O
with O
autoregressive O
pretraining O
. O

We O
now O
describe O
our O
pretraining O
setup O
and O
the O
evaluation O
of O
downstream O
tasks O
. O

For O
a O
fair O
comparison O
with O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
we O
use O
BooksCorpus B-DatasetName
( O
Zhu O
et O
al O
. O
, O
2015 O
) O
and O
English B-DatasetName
Wikipedia I-DatasetName
as O
our O
pretraining O
data O
. O

We O
use O
the O
uncased O
wordpiece O
tokenizer O
of O
BERT B-MethodName
with O
30k O
vocabulary O
. O

We O
train O
GLM B-MethodName
Base I-MethodName
and O
GLM B-MethodName
Large I-MethodName
with O
the O
same O
architectures O
as O
BERT B-MethodName
Base I-MethodName
and O
BERT B-MethodName
Large I-MethodName
, O
containing O
110 O
M O
and O
340 O
M O
parameters O
respectively O
. O

Comparison O
with O
T5 B-MethodName
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
. O

T5 B-MethodName
proposes O
a O
similar O
blank O
infilling O
objective O
to O
pretrain O
an O
encoder O
- O
decoder O
Transformer O
. O

T5 B-MethodName
uses O
independent O
positional O
encodings O
for O
the O
encoder O
and O
decoder O
, O
and O
relies O
on O
multiple O
sentinel O
tokens O
to O
differentiate O
the O
masked O
spans O
. O

In O
downstream O
tasks O
, O
only O
one O
of O
the O
sentinel O
tokens O
is O
used O
, O
leading O
to O
a O
waste O
of O
model O
capacity O
and O
inconsistency O
between O
pretraining O
and O
finetuning O
. O

Moreover O
, O
T5 B-MethodName
always O
predicts O
spans O
in O
a O
fixed O
left O
- O
to O
- O
right O
order O
. O

As O
a O
result O
, O
GLM B-MethodName
can O
significantly O
outperform O
T5 B-MethodName
on O
NLU B-TaskName
and O
seq2seq B-TaskName
tasks I-TaskName
with O
fewer O
parameters O
and O
data O
, O
as O
stated O
in O
Sections O
3.2 O
and O
3.3 O
. O

Comparison O
with O
XLNet B-MethodName
( O
Yang O
et O
al O
. O
, O
2019 O
) O
. O

Both O
GLM B-MethodName
and O
XLNet B-MethodName
are O
pretrained O
with O
autoregressive O
objectives O
, O
but O
there O
are O
two O
differences O
between O
them O
. O

First O
, O
XLNet B-MethodName
uses O
the O
original O
position O
encodings O
before O
corruption O
. O

During O
inference O
, O
we O
need O
to O
either O
know O
or O
enumerate O
the O
length O
of O
the O
answer O
, O
the O
same O
problem O
as O
BERT B-MethodName
. O

Second O
, O
XLNet B-MethodName
uses O
a O
two O
- O
stream O
self O
- O
attention O
mechanism O
, O
instead O
of O
the O
right O
- O
shift O
, O
to O
avoid O
the O
information O
leak O
within O
Transformer O
. O

It O
doubles O
the O
time O
cost O
of O
pretraining O
. O

Comparison O
with O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

As O
pointed O
out O
by O
( O
Yang O
et O
al O
. O
, O
2019 O
) O
, O
BERT B-MethodName
fails O
to O
capture O
the O
interdependencies O
of O
masked O
tokens O
due O
to O
the O
independence O
assumption O
of O
MLM O
. O

Another O
disadvantage O
of O
BERT B-MethodName
is O
that O
it O
can O
not O
fill O
in O
the O
blanks O
of O
multiple O
tokens O
properly O
. O

To O
infer O
the O
probability O
of O
an O
answer O
of O
length O
l O
, O
BERT B-MethodName
needs O
to O
perform O
l O
consecutive O
predictions O
. O

If O
the O
length O
l O
is O
unknown O
, O
we O
may O
need O
to O
enumerate O
all O
possible O
lengths O
, O
since O
BERT B-MethodName
needs O
to O
change O
the O
number O
of O
[ O
MASK O
] O
tokens O
according O
to O
the O
length O
. O

For O
text B-TaskName
generation I-TaskName
tasks O
, O
the O
given O
context O
constitutes O
the O
Part O
A O
of O
the O
input O
, O
with O
a O
mask O
token O
appended O
at O
the O
end O
. O

The O
model O
generates O
the O
text O
of O
Part O
B O
autoregressively O
. O

We O
can O
directly O
apply O
the O
pretrained O
GLM B-MethodName
for O
unconditional B-TaskName
generation I-TaskName
, O
or O
finetune O
it O
on O
downstream O
conditional B-TaskName
generation I-TaskName
tasks O
. O

In O
this O
section O
, O
we O
discuss O
the O
differences O
between O
GLM B-MethodName
and O
other O
pretraining O
models O
. O

We O
are O
mainly O
concerned O
with O
how O
they O
can O
be O
adapted O
to O
downstream O
blank O
infilling O
tasks O
. O

Then O
we O
finetune O
GLM B-MethodName
with O
a O
cross B-HyperparameterValue
- I-HyperparameterValue
entropy I-HyperparameterValue
loss B-HyperparameterName
( O
see O
Figure O
3 O
) O
. O

where O
Y O
is O
the O
label O
set O
. O

Therefore O
the O
probability O
of O
the O
sentence O
being O
positive O
or O
negative O
is O
proportional O
to O
predicting O
" O
good O
" O
or O
" O
bad O
" O
in O
the O
blank O
. O

) O

p(y|x O
) O
= O
p(v(y)|c(x O
) O
) O
y O
∈Y O
p(v(y O
) O
|c(x O
) O
) O
( O
3 O

Instead O
, O
we O
reformulate O
NLU B-TaskName
classification O
tasks O
as O
generation O
tasks O
of O
blank O
infilling O
, O
following O
PET O
( O
Schick O
and O
Schütze O
, O
2020a O
) O
. O

Specifically O
, O
given O
a O
labeled O
example O
( O
x O
, O
y O
) O
, O
we O
convert O
the O
input O
text O
x O
to O
a O
cloze O
question O
c(x O
) O
via O
a O
pattern O
containing O
a O
single O
mask O
token O
. O

The O
pattern O
is O
written O
in O
natural O
language O
to O
represent O
the O
semantics O
of O
the O
task O
. O

For O
example O
, O
a O
sentiment B-TaskName
classification I-TaskName
task O
can O
be O
formulated O
as O
" O
{ O
SENTENCE O
} O
. O

It O
's O
really O
[ O
MASK O
] O
" O
. O

The O
candidate O
labels O
y O
∈ O
Y O
are O
also O
mapped O
to O
answers O
to O
the O
cloze O
, O
called O
verbalizer O
v(y O
) O
. O

In O
sentiment B-TaskName
classification I-TaskName
, O
the O
labels O
" O
positive O
" O
and O
" O
negative O
" O
are O
mapped O
to O
the O
words O
" O
good O
" O
and O
" O
bad O
" O
. O

The O
conditional O
probability O
of O
predicting O
y O
given O
x O
is O

+ O
A O
Q O
J O
o O
0 O
q O
7 O
7 O
r O
c O
1 O
N O
7 O
+ O
w O
u O
L O
R O
c O
W O
L O
F O
X O
1 O
9 O
Y O
3 O
N O
o O
t O
b O
2 O
3 O
U O
l O
U O
o O
l O
J O
D O
Q O
s O
m O
Z O
D O
N O
A O
i O
j O
D O
K O
S O
U O
1 O
T O
z O
U O
g O
z O
k O
Q O
T O
F O
A O
S O
O O
N O
o O
H O
+ O
R O
+ O
4 O
1 O
b O
I O
h O
U O
V O
/ O
F O
o O
P O
E O
u O
L O
H O
q O
M O
t O
p O
R O
D O
H O
S O
R O
r O
p O
p O
B O
4 O
K O
F O
a O
h O
C O
b O
K O
7 O
s O
f O
d O
o O
o O
l O
t O
+ O
y O
O O
4 O
M O
w O
S O
b O
0 O
J O
K O
Z O
x O
/ O
2 O
a O
f O
L O
y O
Z O
V O
c O
7 O
x O
c O
9 O
2 O
K O
H O
A O
a O
E O
6 O
4 O
x O
Q O
0 O
q O
1 O
P O
D O
f O
R O
f O
o O
a O
k O
p O
p O
i O
R O
o O
d O
1 O
O O
F O
U O
k O
Q O
7 O
q O
M O
u O
a O
R O
n O
K O
U O
U O
y O
U O
n O
4 O
1 O
S O
D O
5 O
1 O
9 O
o O
4 O
R O
O O
J O
K O
Q O
5 O
X O
D O
s O
j O
9 O
f O
d O
G O
h O
m O
K O
V O
R O
z O
O O
T O
M O
d O
I O
9 O
N O
e O
3 O
l O
4 O
n O
9 O
e O
K O
9 O
X O
R O
i O
Z O
9 O
R O
n O
q O
S O
a O
c O
D O
x O
+ O
K O
E O
q O
Z O
o O
4 O
W O
T O
V O
+ O
C O
E O
V O
B O
K O
s O
2 O
c O
A O
Q O
h O
C O
U O
1 O
W O
R O
3 O
c O
Q O
x O
J O
h O
b O
Y O
q O
y O
T O
Q O
n O
e O
9 O
J O
d O
n O
S O
f O
2 O
w O
7 O
B O
2 O
V O
3 O
S O
u O
3 O
V O
D O
m O
H O
M O
Q O
q O
w O
C O
3 O
t O
w O
A O
B O
4 O
c O
Q O
w O
U O
u O
o O
Q O
o O
1 O
w O
C O
D O
h O
A O
Z O
7 O
g O
2 O
b O
q O
z O
HTypically O
, O
for O
downstream O
NLU B-TaskName
tasks O
, O
a O
linear O
classifier O
takes O
the O
representations O
of O
sequences O
or O
tokens O
produced O
by O
pretrained O
models O
as O
input O
and O
predicts O
the O
correct O
labels O
. O

The O
practices O
are O
different O
from O
the O
generative O
pretraining O
task O
, O
leading O
to O
inconsistency O
between O
pretraining O
and O
finetuning O
. O

3 O
r O
+ O
R O
G O
E O
y O
3 O
g O
K O
C O
U O
a O
U O
f O
2 O
i O
3 O
S O
j O
J O
Q O
= O
" O
> O
A O
A O
A O
B O
6 O
H O
i O
c O
b O
V O
D O
J O
S O
g O
N O
B O
E O
K O
2 O
J O
W O
4 O
x O
b O
1 O
K O
M O
i O
j O
U O
H O
w O
F O
G O
Y O
q O
M O
e O
g O
F O
4 O
8 O
J O
m O
A O
W O
S O
I O
f O
R O
0 O
a O
p O
I O
2 O
P O
Q O
v O
d O
P O
c O
I O
w O
5 O
O O
j O
J O
i O
w O
d O
F O
v O
P O
o O
V O
+ O
Q O
5 O
v O
f O
o O
M O
/ O
Y O
W O
c O
5 O
a O
P O
R O
B O
w O
e O
O O
9 O
K O
q O
r O
q O
e O
b O
H O
g O
S O
t O
v O
2 O
p O
5 O
V O
b O
W O
l O
5 O
Z O
X O
c O
u O
v O
F O
z O
Y O
2 O
t O
7 O
Z O
3 O
i O
r O
t O
7 O
D O
R O
U O
l O
k O
m O
G O
d O
R O
S O
K O
S O
L O
Y O
8 O
q O
F O
D O
z O
E O
u O
u O
Z O
a O
Y O
C O
u O
W O
S O
A O
N O
P O
Y O
N O
M O
b O
X O
k O
/ O
8 O
5 O
j O
1 O
K O
x O
a O
P O
w O
V O
q O
c O
x O
u O
g O
H O
t O
h O
9 O
z O
n O
j O
G O
o O
j O
1 O
d O
J O
u O
s O
W O
S O
X O
7 O
S O
n O
I O
X O
+ O
L O
M O
S O
a O
l O
y O
O O
K O
5 O
9 O
P O
R O
y O
N O
q O
9 O
3 O
i O
R O
6 O
c O
X O
s O
S O
T O
A O
U O
D O
N O
B O
l O
W O
o O
7 O
d O
q O
z O
d O
j O
E O
r O
N O
m O
c O
B O
R O
o O
Z O
M O
o O
j O
C O
k O
b O
0 O
j O
6 O
2 O
D O
Q O
1 O
p O
g O
M O
r O
N O
p O
o O
e O
O O
y O
I O
l O
R O
e O
s O
S O
P O
p O
K O
l O
Q O
k O
6 O
n O
6 O
c O
y O
K O
j O
g O
V O
J O
p O
4 O
J O
n O
O O
g O
O O
q O
B O
W O
v O
Q O
m O
4 O
n O
9 O
e O
O O
9 O
H O
+ O
p O
Z O
v O
x O
M O
E O
4 O
0 O
h O
m O
y O
2 O
y O
E O
8 O
E O
0 O
R O
G O
Z O
f O
E O
1 O
6 O
X O
C O
L O
T O
I O
j O
W O
E O
M O
s O
n O
N O
r O
Y O
Q O
N O
q O
K O
R O
M O
m O
2 O
w O
K O
J O
g O
R O
n O
8 O
e O
W O
/ O
p O
H O
F O
W O
d O
s O
7 O
L O
d O
s O
2 O
k O
c O
Q O
U O
z O
5 O
O O
E O
A O
j O
u O
E O
U O
H O
L O
i O
A O
C O
t O
x O
A O
F O
e O
r O
A O
A O
O O
E O
R O
n O
u O
H O
F O
u O
r O
O O
e O
r O
F O
f O
r O
b O
d O
a O
a O
s O
+ O
Y O
z O
+ O
/ O
A O
L O
1 O
v O
s O
3 O
1 O
m O
K O
Q O
q O
Q O
= O
= O
< O
/ O
l O
a O
t O
e O
x O
i O
t O
> O
y O
good O
< O
lA O
A O
A O
B O
6 O
3 O
i O
c O
b O
V O
B O
N O
S O
w O
M O
x O
E O
J O
2 O
t O
X O
7 O
V O
+ O
V O
T O
1 O
6 O
C O
S O
1 O
C O
R O
S O
i O
7 O
H O
t O
R O
j O
0 O
Y O
v O
H O
C O
v O
Y O
D O
2 O
q O
V O
k O
0 O
7 O
Q O
N O
T O
b O
J O
L O
k O
i O
0 O
s O
S O
/ O
+ O
C O
F O
w O
V O
F O
v O
P O
q O
H O
v O
P O
X O
f O
m O
G O
1 O
7 O
0 O
N O
Y O
H O
A O
4 O
/ O
3 O
Z O
p O
i O
Z O
F O
0 O
S O
c O
a O
e O
O O
6 O
M O
y O
e O
3 O
s O
b O
m O
1 O
v O
Z O
P O
f O
L O
e O
z O
t O
H O
x O
w O
e O
F O
Y O
9 O
P O
m O
j O
q O
M O
F O
a O
E O
N O
E O
v O
J O
Q O
t O
Q O
O O
s O
K O
W O
e O
S O
N O
g O
w O
z O
n O
L O
Y O
j O
R O
b O
E O
I O
O O
G O
0 O
F O
4 O
/ O
v O
M O
b O
0 O
2 O
o O
0 O
i O
y O
U O
T O
y O
a O
J O
q O
C O
/ O
w O
U O
L O
I O
B O
I O
9 O
h O
k O
0 O
q O
S O
S O
X O
P O
S O
K O
Z O
b O
f O
q O
z O
o O
H O
W O
i O
b O
c O
k O
5 O
V O
q O
p O
e O
/ O
k O
6 O
q O
y O
X O
1 O
X O
v O
G O
7 O
2 O
w O
9 O
J O
L O
K O
g O
0 O
h O
G O
O O
t O
O O
5 O
4 O
b O
G O
T O
/ O
F O
y O
j O
D O
C O
6 O
b O
T O
Q O
j O
T O
W O
N O
M O
B O
n O
j O
I O
e O
1 O
Y O
K O
r O
G O
g O
2 O
k O
/ O
n O
t O
0 O
7 O
R O
u O
V O
X O
6 O
a O
B O
A O
q O
W O
9 O
K O
g O
u O
f O
p O
7 O
I O
s O
V O
C O
6 O
0 O
Q O
E O
t O
l O
N O
g O
M O
9 O
K O
r O
X O
i O
b O
+ O
5 O
3 O
V O
i O
M O
7 O
j O
1 O
U O
y O
a O
j O
2 O
F O
B O
J O
F O
o O
s O
G O
M O
U O
c O
m O
R O
N O
n O
j O
q O
M O
8 O
U O
J O
Y O
Y O
n O
l O
m O
C O
i O
m O
L O
0 O
V O
k O
R O
F O
W O
m O
B O
g O
b O
T O
8 O
G O
G O
4 O
K O
2 O
+ O
v O
E O
6 O
a O
V O
1 O
X O
v O
u O
u O
o O
+ O
2 O
j O
T O
u O
Y O
I O
E O
8 O
n O
E O
E O
J O
K O
u O
D O
B O
D O
d O
T O
g O
A O
e O
r O
Q O
A O
A O
I O
j O
e O
I O
Y O
3 O
e O
H O
e O
E O
8 O
+ O
J O
8 O
O O
J O
+ O
L O
1 O
p O
y O
z O
n O
D O
m O
F O
P O
3 O
C O
+ O
f O
g O
C O
i O
o O
5 O
D O
y O
< O
/ O
l O
a O
t O
e O
x O
i O
t O
> O
v(y O
) O
GLM B-MethodName
< O
l O
a O
t O
e O
x O
i O
t O
s O
h O
a O
1 O
_ O
b O
a O
s O
e O
6 O
4 O
= O
" O
c O
I O
l O
X O
H O
K O
T O
M O
H O
L O
8 O
y O
9 O
4 O
G O
I O
+ O
K O
Z O
X O
n O
l O
T O
1 O
K O
7 O
g O
= O
" O
> O
A O
A O
A O
B O
7 O
X O
i O
c O
b O
V O
D O
L O
S O
g O
N O
B O
E O
O O
y O
N O
r O
x O
h O
f O
U O
Y O
9 O
e O
h O
g O
Q O
h O
I O
o O
R O
d O
D O
+ O
o O
x O
6 O
M O
V O
j O
B O
P O
O O
A O
Z O
A O
m O
z O
k O
9 O
l O
k O
z O
O O
z O
M O
M O
j O
M O
r O
L O
j O
H O
/ O
4 O
E O
E O
P O
i O
n O
j O
1 O
f O
7 O
z O
l O
b O
5 O
w O
8 O
D O
p O
p O
Y O
0 O
F O
B O
U O
d O
d O
P O
d O
F O
c O
S O
c O
a O
e O
O O
6 O
Y O
y O
e O
z O
s O
r O
q O
2 O
v O
p O
H O
d O
z O
G O
1 O
t O
7 O
+ O
z O
u O
5 O
f O
c O
P O
6 O
l O
o O
m O
i O
t O
A O
a O
k O
V O
y O
q O
Z O
o O
A O
1 O
5 O
U O
z O
Q O
m O
m O
G O
G O
0 O
2 O
a O
s O
K O
I O
4 O
C O
T O
h O
v O
B O
4 O
H O
r O
i O
N O
x O
6 O
o O
0 O
k O
y O
K O
O O
5 O
P O
G O
1 O
I O
9 O
w O
T O
7 O
C O
Q O
E O
W O
y O
s O
V O
I O
9 O
L O
6 O
d O
P O
j O
S O
S O
d O
f O
d O
M O
v O
u O
F O
G O
i O
Z O
e O
H O
N O
S O
r O
B O
T O
a O
p O
y O
/ O
j O
S O
l O
r O
t O
5 O
L O
/ O
b O
X O
U O
m O
S O
i O
A O
p O
D O
O O
N O
a O
6 O
5 O
b O
m O
x O
8 O
Y O
d O
Y O
G O
U O
Y O
4 O
H O
e O
X O
a O
i O
a O
Y O
x O
J O
g O
P O
c O
o O
y O
1 O
L O
B O
Y O
6 O
o O
9 O
o O
f O
T O
a O
0 O
f O
o O
2 O
C O
p O
d O
F O
E O
p O
l O
S O
x O
g O
0 O
V O
X O
9 O
P O
D O
H O
G O
k O
d O
R O
o O
F O
t O
j O
P O
C O
p O
q O
8 O
X O
v O
Y O
n O
4 O
n O
9 O
d O
K O
T O
H O
j O
p O
D O
5 O
m O
I O
E O
0 O
M O
F O
m O
S O
0 O
K O
E O
4 O
6 O
M O
R O
J O
P O
X O
U O
Z O
c O
p O
S O
g O
x O
P O
L O
c O
F O
E O
M O
X O
s O
r O
I O
n O
2 O
s O
M O
D O
E O
2 O
o O
J O
w O
N O
w O
V O
t O
8 O
e O
Z O
n O
U O
z O
8 O
r O
e O
e O
d O
m O
9 O
t O
W O
l O
c O
w O
Q O
x O
Z O
O O
I O
I O
C O
l O
M O
C O
D O
C O
6 O
j O
A O
D O
V O
S O
h O
B O
g O
T O
u O
4 O
R O
n O
e O
4 O
N O
2 O
R O
z O
q O
v O
z O
4 O
X O
z O
O O
W O
j O
P O
O O
f O
O O
Y O
Q O
/ O
s O
D O
5 O
+ O
g O
F O
Y O
z O
5 O
H O
0 O
< O
/ O
l O
a O
t O
e O
x O
i O
t O
> O
p(y|x O
) O
< O
l O
a O
t O
e O
x O
i O
t O
s O
h O
a O
1 O
_ O
b O
a O
s O
e O
6 O
4 O
= O
" O
X O
k O
n O
P O
s O
X O
X O
F O
T O
3 O
s O
4 O
6 O
5 O
1 O
7 O
5 O
q O
1 O
X O
6 O
2 O
0 O
8 O
reconstructing O
them O
. O

It O
is O
an O
important O
difference O
as O
compared O
to O
other O
models O
. O

For O
example O
, O
XL B-MethodName
- I-MethodName
Net I-MethodName
( O
Yang O
et O
al O
. O
, O
2019 O
) O
encodes O
the O
original O
position O
so O
that O
it O
can O
perceive O
the O
number O
of O
missing O
tokens O
, O
and O
SpanBERT B-MethodName
( O
Joshi O
et O
al O
. O
, O
2020 O
) O
replaces O
the O
span O
with O
multiple O
[ O
MASK O
] O
tokens O
and O
keeps O
the O
length O
unchanged O
. O

Our O
design O
fits O
downstream O
tasks O
as O
usually O
the O
length O
of O
the O
generated O
text O
is O
unknown O
beforehand O
. O

Our O
encoding O
method O
ensures O
that O
the O
model O
is O
not O
aware O
of O
the O
length B-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
masked I-HyperparameterName
span I-HyperparameterName
when O
Coronet O
has O
the O
best O
lines O
of O
all O
day O
cruisers O
. O

Positive O
< O
l O
a O
t O
e O
x O
i O
t O
s O
h O
a O
1 O
_ O
b O
a O
s O
e O
6 O
4 O
= O
" O
c O
b O
5 O
S O
9 O

( O
3 O
) O
we O
replace O
ReLU B-HyperparameterValue
activation B-HyperparameterName
functions I-HyperparameterName
with O
GeLUs B-HyperparameterValue
( O
Hendrycks O
and O
Gimpel O
, O
2016).One O
of O
the O
challenges O
of O
the O
autoregressive O
blank O
infilling O
task O
is O
how O
to O
encode O
the O
positional O
information O
. O

Transformers O
rely O
on O
positional B-HyperparameterName
encodings I-HyperparameterName
to O
inject O
the O
absolute O
and O
relative O
positions O
of O
the O
tokens O
. O

We O
propose O
2D B-HyperparameterValue
positional I-HyperparameterValue
encodings I-HyperparameterValue
to O
address O
the O
challenge O
. O

Specifically O
, O
each O
token O
is O
encoded O
with O
two O
positional O
ids O
. O

The O
first O
positional O
i O
d O
represents O
the O
position O
in O
the O
corrupted O
text O
x O
corrupt O
. O

For O
the O
masked O
spans O
, O
it O
is O
the O
position O
of O
the O
corresponding O
[ O
MASK O
] O
token O
. O

The O
second O
positional O
i O
d O
represents O
the O
intra O
- O
span O
position O
. O

For O
tokens O
in O
Part O
A O
, O
their O
second O
positional O
ids O
are O
0 O
. O

For O
tokens O
in O
Part O
B O
, O
they O
range O
from O
1 O
to O
the O
length O
of O
the O
span O
. O

The O
two O
positional O
ids O
are O
projected O
into O
two O
vectors O
via O
learnable O
embedding O
tables O
, O
which O
are O
both O
added O
to O
the O
input O
token O
embeddings O
. O

Both O
new O
objectives O
are O
defined O
in O
the O
same O
way O
as O
the O
original O
objective O
, O
i.e. O
Eq O
. O

1 O
. O

The O
only O
difference O
is O
the O
number B-HyperparameterName
of I-HyperparameterName
spans I-HyperparameterName
and O
the O
span B-HyperparameterName
lengths I-HyperparameterName
. O

GLM B-MethodName
uses O
a O
single O
Transformer B-HyperparameterName
with O
several O
modifications O
to O
the O
architecture O
: O
( O
1 O
) O
we O
rearrange O
the O
order O
of O
layer O
normalization O
and O
the O
residual O
connection O
, O
which O
has O
been O
shown O
critical O
for O
large O
- O
scale O
language O
models O
to O
avoid O
numerical O
errors O
( O
Shoeybi O
et O
al O
. O
, O
2019 O
) O
; O
( O
2 O
) O
we O
use O
a O
single O
linear B-HyperparameterName
layer I-HyperparameterName
for O
the O
output O
token O
prediction O
; O

• O
Sentence O
- O
level O
. O

We O
restrict O
that O
the O
masked O
spans O
must O
be O
full O
sentences O
. O

Multiple B-HyperparameterName
spans I-HyperparameterName
( O
sentences O
) O
are O
sampled O
to O
cover O
15 B-HyperparameterValue
% I-HyperparameterValue
of I-HyperparameterValue
the I-HyperparameterValue
original I-HyperparameterValue
tokens I-HyperparameterValue
. O

This O
objective O
aims O
for O
seq2seq O
tasks O
whose O
predictions O
are O
often O
complete O
sentences O
or O
paragraphs O
. O

• O
Document O
- O
level O
. O

We O
sample O
a O
single O
span O
whose O
length O
is O
sampled O
from O
a O
uniform O
distribution O
over O
50%-100 B-HyperparameterValue
% I-HyperparameterValue
of I-HyperparameterValue
the I-HyperparameterValue
original I-HyperparameterValue
length I-HyperparameterValue
. O

The O
objective O
aims O
for O
long O
text O
generation O
. O

Position O
1 O
1 O
2 O
3 O
4 O
5 O
5 O
5 O
5 O
3 O
3 O
Position O
2 O
0 O
0 O
0 O
0 O
0 O
1 O
2 O
3 O
1 O
2 O
output O
respectively O
. O

In O
this O
way O
, O
our O
model O
automatically O
learns O
a O
bidirectional O
encoder O
( O
for O
Part O
A O
) O
and O
a O
unidirectional O
decoder O
( O
for O
Part O
B O
) O
in O
a O
unified O
model O
. O

The O
implementation O
of O
GLM B-MethodName
is O
illustrated O
in O
Figure O
2 O
. O

We O
randomly O
sample O
spans O
of O
length O
drawn O
from O
a O
Poisson O
distribution O
with O
λ B-HyperparameterName
= O
3 B-HyperparameterValue
. O

We O
repeatedly O
sample O
new O
spans O
until O
at O
least O
15 B-HyperparameterValue
% I-HyperparameterValue
of O
the O
original B-HyperparameterName
tokens I-HyperparameterName
are I-HyperparameterName
masked I-HyperparameterName
. O

Empirically O
, O
we O
have O
found O
that O
the O
15 B-HyperparameterValue
% I-HyperparameterValue
ratio O
is O
critical O
for O
good O
performance O
on O
downstream O
NLU B-TaskName
tasks O
. O

In O
the O
previous O
section O
, O
GLM B-MethodName
masks O
short O
spans O
and O
is O
suited O
for O
NLU B-TaskName
tasks O
. O

However O
, O
we O
are O
interested O
in O
pretraining O
a O
single O
model O
that O
can O
handle O
both O
NLU B-TaskName
and O
text B-TaskName
generation I-TaskName
. O

We O
then O
study O
a O
multi O
- O
task O
pretraining O
setup O
, O
in O
which O
a O
second O
objective O
of O
generating O
longer O
text O
is O
jointly O
optimized O
with O
the O
blank O
infilling O
objective O
. O

We O
consider O
the O
following O
two O
objectives O
: O

We O
implement O
the O
autoregressive O
blank O
infilling O
objective O
with O
the O
following O
techniques O
. O

The O
input O
x O
is O
divided O
into O
two O
parts O
: O
Part O
A O
is O
the O
corrupted O
text O
x O
corrupt O
, O
and O
Part O
B O
consists O
of O
the O
masked O
spans O
. O

Part O
A O
tokens O
can O
attend O
to O
each O
other O
, O
but O
can O
not O
attend O
to O
any O
tokens O
in O
B. O
Part O
B O
tokens O
can O
attend O
to O
Part O
A O
and O
antecedents O
in O
B O
, O
but O
can O
not O
attend O
to O
any O
subsequent O
tokens O
in O
B. O
To O
enable O
autoregressive O
generation O
, O
each O
span O
is O
padded O
with O
special O
tokens O
[ O
START O
] O
and O
[ O
END O
] O
, O
for O
input O
and O
Token O

p O
θ O
( O
s O
i O
|x O
corrupt O
, O
s O
z O
< O
i O
) O
= O
l O
i O
j=1 O
p(s O
i O
, O
j O
|x O
corrupt O
, O
s O
z O
< O
i O
, O
s O
i,<j O
) O
( O
2 O
) O

We O
always O
generate O
the O
tokens O
in O
each O
blank O
following O
a O
left O
- O
to O
- O
right O
order O
, O
i.e. O
the O
probability O
of O
generating O
the O
span O
s O
i O
is O
factorized O
as O
: O

max O
θ O
E O
z∼Zm O
m O
i=1 O
log O
p O
θ O
( O
s O
z O
i O
|x O
corrupt O
, O
s O
z O
< O
i O
) O
( O
1 O
) O

Each O
span O
is O
replaced O
with O
a O
single O
[ O
MASK O
] O
token O
, O
forming O
a O
corrupted O
text O
x O
corrupt O
. O

The O
model O
predicts O
the O
missing O
tokens O
in O
the O
spans O
from O
the O
corrupted O
text O
in O
an O
autoregressive O
manner O
, O
which O
means O
when O
predicting O
the O
missing O
tokens O
in O
a O
span O
, O
the O
model O
has O
access O
to O
the O
corrupted O
text O
and O
the O
previously O
predicted O
spans O
. O

To O
fully O
capture O
the O
interdependencies O
between O
different O
spans O
, O
we O
randomly O
permute O
the O
order O
of O
the O
spans O
, O
similar O
to O
the O
permutation O
language O
model O
( O
Yang O
et O
al O
. O
, O
2019 O
) O
. O

Formally O
, O
let O
Z O
m O
be O
the O
set O
of O
all O
possible O
permutations O
of O
the O
length O
- O
m O
index O
sequence O
[ O
1 O
, O
2 O
, O
• O
• O
• O
, O
m O
] O
, O
and O
s O
z O
< O
i O
be O
[ O
s O
z O
1 O
, O
• O
• O
• O
, O
s O
z O
i−1 O
] O
, O
we O
define O
the O
pretraining O
objective O
as O

x O
= O
[ O
x O
1 O
, O
• O
• O
• O
, O
x O
n O
] O
, O
multiple O
text O
spans O
{ O
s O
1 O
, O
• O
• O
• O
, O
s O
m O
} O
are O
sampled O
, O
where O
each O
span O
s O
i O
corresponds O
to O
a O
series O
of O
consecutive O
tokens O
[ O
s O
i,1 O
, O
• O
• O
• O
, O
s O
i O
, O
l O
i O
] O
in O
x. O

Furthermore O
, O
we O
show O
that O
by O
varying O
the B-HyperparameterName
number I-HyperparameterName
and I-HyperparameterName
lengths I-HyperparameterName
of I-HyperparameterName
missing I-HyperparameterName
spans I-HyperparameterName
, O
the O
autoregressive O
blank O
filling O
objective O
can O
pretrain O
language O
models O
for O
conditional B-TaskName
and I-TaskName
unconditional I-TaskName
generation I-TaskName
. O

Through O
multi O
- O
task O
learning O
of O
different O
pretraining O
objectives O
, O
a O
single O
GLM B-MethodName
can O
excel O
in O
both O
NLU B-TaskName
and O
( O
conditional O
and O
unconditional O
) O
text B-TaskName
generation I-TaskName
. O

Empirically O
, O
compared O
with O
standalone O
baselines O
, O
GLM B-MethodName
with O
multi O
- O
task O
pretraining O
achieves O
improvements O
in O
NLU B-TaskName
, O
conditional B-TaskName
text I-TaskName
generation I-TaskName
, O
and O
language B-TaskName
modeling I-TaskName
tasks O
altogether O
by O
sharing O
the O
parameters O
. O

We O
propose O
a O
general O
pretraining O
framework O
GLM B-MethodName
based O
on O
a O
novel O
autoregressive O
blank O
infilling O
objective O
. O

GLM B-MethodName
formulates O
NLU B-TaskName
tasks O
as O
cloze B-TaskName
questions I-TaskName
that I-TaskName
contain I-TaskName
task I-TaskName
descriptions I-TaskName
, O
which O
can O
be O
answered O
by O
autoregressive O
generation O
. O

GLM B-MethodName
is O
trained O
by O
optimizing O
an O
autoregressive O
blank O
infilling O
objective O
. O

Given O
an O
input O
text O

Inspired O
by O
Pattern O
- O
Exploiting O
Training O
( O
PET O
) O
( O
Schick O
and O
Schütze O
, O
2020a O
) O
, O
we O
reformulate O
NLU B-TaskName
tasks O
as O
manually B-TaskName
- I-TaskName
crafted I-TaskName
cloze I-TaskName
questions I-TaskName
that O
mimic O
human O
language O
. O

Different O
from O
the O
BERTbased O
models O
used O
by O
PET O
, O
GLM B-MethodName
can O
naturally O
handle O
multi O
- O
token O
answers O
to O
the O
cloze O
question O
via O
autoregressive O
blank O
filling O
. O

In O
this O
paper O
, O
we O
propose O
a O
pretraining O
framework O
named O
GLM B-MethodName
( O
General B-MethodName
Language I-MethodName
Model I-MethodName
) O
, O
based O
on O
autoregressive O
blank O
infilling O
. O

We O
randomly O
blank O
out O
continuous O
spans O
of O
tokens O
from O
the O
input O
text O
, O
following O
the O
idea O
of O
autoencoding O
, O
and O
train O
the O
model O
to O
sequentially O
reconstruct O
the O
spans O
, O
following O
the O
idea O
of O
autoregressive O
pretraining O
( O
see O
Figure O
1 O
) O
. O

While O
blanking O
filling O
has O
been O
used O
in O
T5 B-MethodName
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
for O
text O
- O
to O
- O
text O
pretraining O
, O
we O
propose O
two O
improvements O
, O
namely O
span O
shuffling O
and O
2D O
positional O
encoding O
. O

Empirically O
, O
we O
show O
that O
with O
the O
same O
amount O
of O
parameters O
and O
computational O
cost O
, O
GLM B-MethodName
significantly O
outperforms O
BERT B-MethodName
on O
the O
SuperGLUE B-DatasetName
benchmark O
by O
a O
large O
margin O
of O
4.6 O
% O
-5.0 O
% O
and O
outperforms O
RoBERTa B-MethodName
and O
BART B-MethodName
when O
pretrained O
on O
a O
corpus O
of O
similar O
size O
( O
158 O
GB O
) O
. O

GLM B-MethodName
also O
significantly O
outperforms O
T5 B-MethodName
on O
NLU B-TaskName
and O
generation B-TaskName
tasks O
with O
fewer O
parameters O
and O
data O
. O

None O
of O
these O
pretraining O
frameworks O
is O
flexible O
enough O
to O
perform O
competitively O
across O
all O
NLP O
tasks O
. O

Previous O
works O
have O
tried O
to O
unify O
different O
frameworks O
by O
combining O
their O
objectives O
via O
multi O
- O
task O
learning O
( O
Dong O
et O
al O
. O
, O
2019;Bao O
et O
al O
. O
, O
2020 O
) O
. O

However O
, O
since O
the O
autoencoding O
and O
autoregressive O
objectives O
differ O
by O
nature O
, O
a O
simple O
unification O
can O
not O
fully O
inherit O
the O
advantages O
of O
both O
frameworks O
. O

† O
Corresponding O
authors O
. O

1 O
The O
code O
and O
pre O
- O
trained O
models O
are O
available O
at O
https O
: O
//github.com O
/ O
THUDM O
/ O
GLM O
In O
general O
, O
existing O
pretraining O
frameworks O
can O
be O
categorized O
into O
three O
families O
: O
autoregressive O
, O
autoencoding O
, O
and O
encoder O
- O
decoder O
models O
. O

Autoregressive O
models O
, O
such O
as O
GPT B-MethodName
( O
Radford O
et O
al O
. O
, O
2018a O
) O
, O
learn O
left O
- O
to O
- O
right O
language O
models O
. O

While O
they O
succeed O
in O
long B-TaskName
- I-TaskName
text I-TaskName
generation I-TaskName
and O
show O
fewshot O
learning O
ability O
when O
scaled O
to O
billions O
of O
parameters O
( O
Radford O
et O
al O
. O
, O
2018b;Brown O
et O
al O
. O
, O
2020 O
) O
, O
the O
inherent O
disadvantage O
is O
the O
unidirectional O
attention O
mechanism O
, O
which O
can O
not O
fully O
capture O
the O
dependencies O
between O
the O
context O
words O
in O
NLU B-TaskName
tasks O
. O

Autoencoding O
models O
, O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
learn O
bidirectional O
context O
encoders O
via O
denoising O
objectives O
, O
e.g. O
Masked O
Language O
Model O
( O
MLM O
) O
. O

The O
encoders O
produce O
contextualized O
representations O
that O
suit O
natural O
language O
understanding O
tasks O
, O
but O
could O
not O
be O
directly O
applied O
for O
text B-TaskName
generation I-TaskName
. O

Encoder O
- O
decoder O
models O
adopt O
bidirectional O
attention O
for O
the O
encoder O
, O
unidirectional O
attention O
for O
the O
decoder O
, O
and O
cross O
attention O
between O
them O
( O
Song O
et O
al O
. O
, O
2019;Bi O
et O
al O
. O
, O
2020 O
; O
. O

They O
are O
typically O
deployed O
in O
conditional O
generation O
tasks O
, O
such O
as O
text B-TaskName
summarization I-TaskName
and O
response B-TaskName
generation I-TaskName
. O

2 O
. O

T5 B-MethodName
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
unifies O
NLU B-TaskName
and O
conditional B-TaskName
generation I-TaskName
via O
encoder O
- O
decoder O
models O
but O
requires O
more O
parameters O
to O
match O
the O
performance O
of O
BRET B-MethodName
- O
based O
models O
such O
as O
RoBERTa B-MethodName
and O
DeBERTa B-MethodName
( O
He O
et O
al O
. O
, O
2021 O
) O
. O

Language O
models O
pretrained O
on O
unlabeled O
texts O
have O
substantially O
advanced O
the O
state O
of O
the O
art O
in O
various O
NLP O
tasks O
, O
ranging O
from O
natural B-TaskName
language I-TaskName
understanding I-TaskName
( B-TaskName
NLU I-TaskName
) O
to O
text B-TaskName
generation I-TaskName
( O
Radford O
et O
al O
. O
, O
2018a;Devlin O
et O
al O
. O
, O
2019;Yang O
et O
al O
. O
, O
2019;Radford O
et O
al O
. O
, O
2018b;Raffel O
et O
al O
. O
, O
2020;Brown O
et O
al O
. O
, O
2020 O
) O
. O

Downstream O
task O
performance O
as O
well O
as O
the O
scale O
of O
the O
parameters O
have O
also O
constantly O
increased O
in O
the O
past O
few O
years O
. O

* O
The O
first O
two O
authors O
contributed O
equally O
. O

