Abstract O
We O
propose O
a O
touch O
- O
based O
editing O
method O
for O
translation O
, O
which O
is O
more O
Ô¨Çexible O
than O
traditional O
keyboard O
- O
mouse O
- O
based O
translation O
postediting O
. O
This O
approach O
relies O
on O
touch O
actions O
that O
users O
perform O
to O
indicate O
translation O
errors O
. O
We O
present O
a O
dual O
- O
encoder O
model O
to O
handle O
the O
actions O
and O
generate O
reÔ¨Åned O
translations O
. O
To O
mimic O
the O
user O
feedback O
, O
we O
adopt O
the O
TER O
algorithm O
comparing O
between O
draft O
translations O
and O
references O
to O
automatically O
extract O
the O
simulated O
actions O
for O
training O
data O
construction O
. O
Experiments O
on O
translation O
datasets O
with O
simulated O
editing O
actions O
show O
that O
our O
method O
signiÔ¨Åcantly O
improves O
original O
translation O
of O
Transformer O
( O
up O
to O
25.31 O
BLEU O
) O
and O
outperforms O
existing O
interactive O
translation O
methods O
( O
up O
to O
16.64 O
BLEU O
) O
. O
We O
also O
conduct O
experiments O
on O
post O
- O
editing O
dataset O
to O
further O
prove O
the O
robustness O
and O
effectiveness O
of O
our O
method O
. O
1 O
Introduction O
Neural O
machine O
translation O
( O
NMT O
) O
has O
made O
great O
success O
during O
the O
past O
few O
years O
( O
Sutskever O
et O
al O
. O
, O
2014 O
; O
Bahdanau O
et O
al O
. O
, O
2014 O
; O
Wu O
et O
al O
. O
, O
2016 O
; O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
but O
automatic O
machine O
translation O
is O
still O
far O
from O
perfect O
and O
can O
not O
meet O
the O
strict O
requirements O
of O
users O
in O
real O
applications O
( O
Petrushkov O
et O
al O
. O
, O
2018 O
) O
. O
Many O
notable O
humanmachine O
interaction O
approaches O
have O
been O
proposed O
for O
allowing O
professional O
translators O
to O
improve O
machine O
translation O
results O
( O
Wuebker O
et O
al O
. O
, O
2016 O
; O
Knowles O
and O
Koehn O
, O
2016 O
; O
Hokamp O
and O
Liu O
, O
2017 O
) O
. O
As O
an O
instance O
of O
such O
approaches O
, O
post O
- O
editing O
directly O
requires O
translators O
to O
modify O
outputs O
from O
machine O
translation O
( O
Simard O
et O
al O
. O
, O
2007 O
) O
. O
However O
, O
traditional O
post O
- O
editing O
requires O
intensive O
keyboard O
interaction O
, O
which O
is O
inconvenient O
on O
mobile O
devices O
. O
Grangier O
and O
Auli O
( O
2018 O
) O
suggest O
a O
one O
- O
time O
interaction O
approach O
with O
lightweight O
editing O
ef O
- O
forts O
, O
QuickEdit O
, O
in O
which O
users O
are O
asked O
to O
simply O
mark O
incorrect O
words O
in O
a O
translation O
hypothesis O
for O
one O
time O
in O
the O
hope O
that O
the O
system O
will O
change O
them O
. O
QuickEdit O
delivers O
appealing O
improvements O
on O
draft O
hypotheses O
while O
maintaining O
the O
Ô¨Çexibility O
of O
human O
- O
machine O
interaction O
. O
Unfortunately O
, O
only O
marking O
incorrect O
words O
is O
far O
from O
adequate O
: O
for O
example O
, O
it O
does O
not O
indicate O
the O
missing O
information O
beyond O
the O
original O
hypothesis O
, O
which O
is O
a O
typical O
issue O
called O
under O
- O
translation O
in O
machine O
translation O
( O
Tu O
et O
al O
. O
, O
2016 O
) O
. O
In O
this O
paper O
, O
we O
propose O
a O
novel O
one O
- O
time O
interaction O
method O
called O
Touch O
Editing O
, O
which O
is O
Ô¨Çexible O
for O
users O
and O
more O
adequate O
for O
a O
system O
to O
generate O
better O
translations O
. O
Inspired O
by O
human O
editing O
process O
, O
the O
proposed O
method O
relies O
on O
a O
series O
of O
touch O
- O
based O
actions O
including O
SUBSTITU O
TION O
, O
DELETION O
, O
INSERTION O
and O
REORDERING O
. O
These O
actions O
do O
not O
include O
lexical O
information O
and O
thus O
can O
be O
Ô¨Çexibly O
provided O
by O
users O
through O
various O
of O
gestures O
on O
touch O
screen O
devices O
. O
By O
using O
these O
actions O
, O
our O
method O
is O
able O
to O
capture O
the O
editing O
intention O
from O
users O
to O
generate O
better O
translations O
: O
for O
instance O
, O
INSERTION O
indicates O
a O
word O
is O
missing O
at O
a O
particular O
position O
, O
and O
our O
method O
is O
expected O
to O
insert O
the O
correct O
word O
. O
To O
this O
end O
, O
we O
present O
a O
neural O
network O
model O
by O
augmenting O
Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
with O
an O
extra O
encoder O
for O
a O
hypothesis O
and O
its O
actions O
. O
Since O
it O
is O
impractical O
to O
manually O
annotate O
large O
- O
scale O
action O
dataset O
to O
train O
the O
model O
, O
we O
thereby O
adopt O
the O
algorithm O
of O
TER O
( O
Snover O
et O
al O
. O
, O
2006 O
) O
to O
automatically O
extract O
actions O
from O
a O
draft O
hypothesis O
and O
its O
reference O
. O
To O
evaluate O
our O
method O
, O
we O
conduct O
simulated O
experiments O
on O
translation O
datasets O
the O
same O
as O
in O
other O
works O
( O
Denkowski O
et O
al O
. O
, O
2014 O
; O
Grangier O
and O
Auli O
, O
2018 O
) O
, O
The O
results O
demonstrate O
that O
our O
method O
can O
address O
the O
well O
- O
known O
challenging O
issues O
in O
machine O
translation O
including O
over-1QuickEditHypothesis O
  O
y‚Äôtravel O
   O
far O
   O
does O
   O
not O
    O
necessary O
   O
to O
   O
proctor O
   O
for O
    O
food O
    O
supply O
  O
.Resulttravel O
   O
far O
   O
does O
   O
not O
      O
require O
     O
to O
   O
proctor O
            O
food O
   O
supplies O
  O
.Source O
  O
xweite O
wegem√ºsse O
proctorf√ºr O
die O
nahrungsmittelbeschaffungnicht O
gehen O
. O
Reference O
  O
yproctor O
   O
does O
   O
not O
   O
have O
   O
to O
   O
travel O
   O
far O
   O
to O
   O
buy O
   O
food O
   O
. O
Touch O
EditingHypothesis O
  O
y‚Äôtravel O
   O
fardoes O
   O
not O
    O
necessary O
   O
to O
   O
proctorfor O
        O
food O
    O
supply O
  O
. O
Modified O
  O
ùëöùëö(y O
‚Äô O
) O
proctordoes O
   O
not O
   O
necessary O
   O
to O
   O
travel O
   O
farfor O
   O
< O
INS O
> O
food O
   O
supply O
  O
. O
Action O
Sequence O
  O
a O
--- O
S O
           O
---S O
        O
I O
            O
-D O
      O
- O
Resultproctor O
   O
does O
   O
not O
       O
have O
      O
to O
   O
travel O
   O
far O
     O
to O
     O
buy O
      O
food O
                O
.Figure O
1 O
: O
Example O
of O
interaction O
methods O
. O
QuickEdit O
allows O
users O
to O
mark O
incorrect O
words O
. O
Our O
method O
introduces O
more O
Ô¨Çexible O
actions O
. O
m(y O
/ O
prime)is O
modiÔ¨Åed O
from O
y O
/ O
primeby O
applying O
reordering O
actions O
and O
inserting O
a O
special O
token O
/angbracketleftINS O
/ O
angbracketrightto O
keep O
alignment O
with O
the O
action O
sequence O
awhich O
contains O
actions O
like O
SUBSTITUTION O
, O
INSERTION O
and O
DELETION O
. O
‚Äú O
- O
‚Äù O
denotes O
the O
word O
in O
that O
position O
is O
unmarked O
. O
Our O
method O
then O
generates O
a O
reÔ¨Åned O
translation O
based O
on O
the O
modiÔ¨Åed O
hypothesis O
m(y O
/ O
prime)and O
the O
action O
sequence O
a. O
translation O
, O
under O
- O
translation O
and O
mis O
- O
ordering O
, O
and O
thus O
it O
outperforms O
Transformer O
and O
QuickEdit O
by O
a O
margin O
up O
to O
25.31 O
and O
16.64 O
BLEU O
points O
respectively O
. O
In O
addition O
, O
experiments O
on O
post O
- O
editing O
dataset O
further O
prove O
the O
effectiveness O
and O
robustness O
of O
our O
method O
. O
Finally O
, O
we O
implement O
a O
real O
application O
on O
mobile O
phones O
to O
discuss O
the O
usability O
in O
real O
senarios O
. O
2 O
Touch O
Editing O
Approach O
2.1 O
Actions O
QuickEdit O
allows O
translators O
to O
mark O
incorrect O
words O
which O
they O
expect O
the O
system O
to O
change O
( O
Grangier O
and O
Auli O
, O
2018 O
) O
. O
However O
, O
as O
shown O
in O
Figure O
1 O
, O
the O
information O
is O
inadequate O
for O
a O
system O
to O
correct O
a O
translation O
hypothesis O
, O
especially O
when O
it O
comes O
to O
under O
- O
translation O
, O
in O
which O
the O
system O
is O
hardly O
to O
predict O
missing O
words O
into O
hypotheses O
. O
To O
achieve O
better O
adequacy O
, O
we O
take O
human O
editing O
habits O
into O
consideration O
. O
As O
shown O
in O
Figure O
1 O
, O
a O
human O
translator O
may O
insert O
, O
delete O
, O
substitute O
or O
reorder O
some O
words O
to O
correct O
errors O
of O
undertranslation O
, O
over O
- O
translation O
, O
mis O
- O
translation O
and O
mis O
- O
ordering O
in O
an O
original O
translation O
hypothesis O
. O
Based O
on O
human O
editing O
process O
, O
we O
deÔ¨Åne O
a O
set O
of O
actions O
to O
represent O
human O
editing O
intentions O
: O
‚Ä¢INSERTION O
: O
a O
new O
word O
should O
be O
inserted O
into O
a O
given O
position O
. O
‚Ä¢DELETION O
: O
a O
word O
at O
a O
speciÔ¨Åc O
position O
should O
be O
deleted.‚Ä¢SUBSTITUTION O
: O
a O
word O
should O
be O
substituted O
by O
another O
word O
. O
‚Ä¢REORDERING O
: O
a O
segment O
of O
words O
should O
be O
moved O
to O
another O
position O
. O
In O
Touch O
Editing O
, O
these O
actions O
can O
be O
performed O
by O
human O
translators O
on O
a O
given O
machine O
hypothesis O
to O
indicate O
translation O
errors O
. O
To O
keep O
the O
Ô¨Çexibility O
of O
interactions O
, O
for O
SUBSTITUTION O
and O
INSERTION O
actions O
, O
our O
method O
allows O
users O
to O
only O
indicate O
which O
word O
should O
be O
substitute O
or O
in O
which O
position O
a O
word O
should O
be O
inserted O
. O
The O
light O
- O
weight O
interaction O
in O
Touch O
Editing O
is O
nonlexical O
, O
i.e. O
, O
it O
does O
not O
require O
any O
keyboard O
inputs O
, O
and O
thus O
can O
be O
adopted O
to O
mobile O
devices O
with O
touch O
screens O
. O
2.2 O
Model O
Our O
model O
seeks O
to O
correct O
translation O
errors O
of O
an O
original O
hypothesis O
y O
/ O
primebased O
on O
actionsAprovided O
by O
human O
translator O
. O
To O
make O
full O
use O
of O
the O
actions O
, O
we O
Ô¨Årstly O
modify O
the O
original O
hypothesis O
by O
applying O
Aony O
/ O
primeto O
obtainA(y O
/ O
prime O
): O
A(y O
/ O
prime O
) O
= O
/angbracketleftm(y O
/ O
prime),a O
/ O
angbracketright O
. O
( O
1 O
) O
SpeciÔ¨Åcally O
, O
as O
shown O
in O
Figure O
1 O
, O
m(y O
/ O
prime)is O
modiÔ¨Åed O
from O
y O
/ O
primeby O
reordering O
the O
segment O
in O
gray O
color O
and O
inserting O
a O
token O
/angbracketleftINS O
/ O
angbracketright O
, O
and O
thus O
theREORDERING O
actions O
is O
implicitly O
included O
in2Target O
EmbeddingLinear O
& O
SoftmaxOutput O
Probabilities O
TargetTransformerDecoder O
Source O
EmbeddingTransformer O
Encoder(source O
) O
SourceModified O
Hypothesis O
Action O
SequenceAction O
PositionalEmbedding O
Target O
EmbeddingTransformerEncoder(hypothesis O
) O
Figure O
2 O
: O
Model O
architecture O
. O
We O
add O
a O
hypothesis O
encoder O
( O
the O
right O
part O
) O
into O
Transformer O
which O
differs O
from O
source O
encoder O
( O
the O
left O
part O
) O
in O
positional O
embedding O
. O
We O
use O
learned O
action O
positional O
embedding O
instead O
of O
the O
sinusoids O
. O
INSERTION O
Positional O
Embedding012 O
‚Ä¶ O
ùë≥ùë≥ O
DELETION O
Positional O
Embedding012 O
‚Ä¶ O
ùë≥ùë≥ O
SUBSTITUTION O
Positional O
Embedding012 O
‚Ä¶ O
ùë≥ùë≥ O
None O
ActionPositional O
Embedding012 O
‚Ä¶ O
ùë≥ùë≥ O
Action O
SequenceI O
     O
D O
    O
S O
     O
-Figure O
3 O
: O
Action O
positional O
embedding O
. O
The O
model O
Ô¨Årstly O
chooses O
an O
embedding O
matrix O
according O
to O
the O
action O
at O
position O
i O
, O
then O
lookups O
the O
ith O
row O
of O
the O
matrix O
as O
the O
positional O
embedding O
of O
position O
i. O
Lis O
the O
maximum O
length O
of O
sentences O
. O
m(y O
/ O
prime O
) O
. O
The O
action O
sequence O
abelowm(y O
/ O
prime)contains O
SUBSTITUTION O
, O
INSERTION O
andDELETION O
at O
the O
corresponding O
position O
. O
We O
then O
use O
a O
neural O
network O
model O
to O
generate O
a O
translation O
yfor O
the O
source O
sentence O
x O
, O
the O
hypothesis O
y O
/ O
primeand O
the O
actionsA O
: O
P(y|x O
, O
y O
/ O
prime O
, O
A;Œ∏ O
) O
= O
N O
/ O
productdisplay O
n=1P(yn|y O
< O
n O
, O
x O
, O
m(y O
/ O
prime),a;Œ∏).(2 O
) O
As O
shown O
in O
Figure O
2 O
, O
the O
neural O
network O
model O
we O
developed O
is O
a O
dual O
encoder O
model O
based O
on O
Transformer O
similar O
to O
Tebbifakhr O
et O
al O
. O
( O
2018 O
) O
. O
SpeciÔ¨Åcally O
, O
besides O
encoding O
the O
source O
sentence O
xwith O
source O
encoder O
( O
the O
left O
part O
of O
Figure O
2 O
) O
, O
our O
model O
additionally O
encodes O
A(y O
/ O
prime)with O
an O
extrahypothesis O
encoder O
( O
the O
right O
part O
of O
Figure O
2 O
) O
and O
integrates O
the O
encoded O
representations O
into O
decoding O
network O
using O
dual O
multi O
- O
head O
attention O
. O
EncodingA(y O
/ O
prime)As O
shown O
in O
the O
right O
part O
of O
Figure O
2 O
, O
the O
hypothesis O
encoder O
Ô¨Årstly O
embeds O
m(y O
/ O
prime)with O
lengthlin O
distributed O
space O
using O
the O
same O
word O
embedding O
as O
in O
decoder O
, O
which O
is O
denoted O
as O
w={w1,¬∑¬∑¬∑,wl O
} O
. O
Then O
it O
encodes O
a={a1,¬∑¬∑¬∑,al}with O
learned O
positional O
embedding O
according O
to O
the O
speciÔ¨Åc O
actions O
. O
As O
shown O
in O
Figure O
3 O
, O
the O
action O
positional O
embedding O
includes O
four O
embedding O
matrixes O
corresponding O
to O
three O
action O
types O
and O
a O
none O
action O
for O
positions O
without O
any O
action O
. O
For O
the O
ith O
position O
of O
a O
, O
the O
encoder O
chooses O
an O
embedding O
matrix O
based O
on O
the O
action O
type O
ofaiand O
selects O
the O
ith O
row O
of O
the O
matrix O
as O
the O
positional O
embedding O
vector O
, O
which O
is O
denoted O
aspi O
: O
pi=Ô£± O
Ô£¥Ô£¥Ô£≤ O
Ô£¥Ô£¥Ô£≥PE O
INSERTION O
( O
i O
) O
ifai O
= O
I O
PE O
DELETION O
( O
i O
) O
ifai O
=D O
PE O
SUBSTITUTION O
( O
i)ifai O
= O
S O
PE O
None(i O
) O
ifai=-(3 O
) O
WherePE‚àódenote O
the O
action O
positional O
embedding O
matrixes O
in O
Figure O
3 O
. O
The O
learned O
action O
positional O
embedding O
is O
used O
in O
hypothesis O
encoder O
to O
replace O
the O
Ô¨Åxed O
sinusoids O
positional O
encoding O
in O
Transformer O
encoder O
. O
Next O
, O
the O
encoder O
adds O
the O
word O
embedding O
wand O
the O
action O
positional O
embedding O
pto O
obtain O
input O
embedding O
e={w1+p1,¬∑¬∑¬∑,wl+pl O
} O
. O
The O
following O
part O
ofhypothesis O
encoder O
lies O
the O
same O
as O
Transformer O
encoder O
. O
Decoding O
The O
output O
of O
hypothesis O
encoder O
, O
together O
with O
the O
output O
of O
source O
encoder O
, O
are O
fed O
into O
the O
decoder O
. O
To O
combine O
both O
of O
the O
encoders O
‚Äô O
outputs O
, O
we O
apply O
dual O
multi O
- O
head O
attention O
in O
each O
layer O
of O
decoder O
: O
the O
attention O
sub O
- O
layer O
attends O
to O
both O
encoders O
‚Äô O
outputs O
by O
performing O
multi O
- O
head O
attention O
respectively O
: O
Asrc O
= O
MultiHead O
( O
Qtgt O
, O
Ksrc O
, O
Vsrc O
) O
Ahyp O
= O
MultiHead O
( O
Qtgt O
, O
Khyp O
, O
Vhyp)(4 O
) O
WhereQtgtis O
coming O
from O
previous O
layer O
of O
the O
decoder O
, O
KsrcandVsrcmatrixes O
are O
Ô¨Ånal O
representations O
of O
the O
source O
encoder O
whileKhypandVhyp O
matrixes O
are O
Ô¨Ånal O
representations O
of O
the O
hypothesis O
encoder O
. O
The O
two O
attention O
vectors O
AsrcandAhyp O
are O
then O
averaged O
to O
replace O
encoder O
- O
decoder O
attention O
in O
Transformer O
, O
resulting O
in O
the O
input O
of O
next O
layer.3Training O
The O
overall O
model O
, O
which O
includes O
a O
source O
encoder O
, O
ahypothesis O
encoder O
with O
action O
positional O
embedding O
, O
and O
a O
decoder O
, O
is O
jointly O
trained O
. O
We O
maximize O
the O
log O
- O
likelihood O
of O
the O
reference O
sentence O
ygiven O
the O
source O
sentence O
x O
, O
the O
initial O
hypothesis O
y O
/ O
prime O
, O
and O
the O
corresponding O
actions O
A. O
By O
applyingAony O
/ O
prime O
, O
the O
training O
objective O
becomes O
: O
ÀÜŒ∏= O
arg O
max O
Œ∏ O
/ O
braceleftBigg O
/ O
summationdisplay O
DlogP(y|x O
, O
m(y O
/ O
prime),a;Œ∏)/bracerightBigg O
. O
( O
5 O
) O
whereDis O
the O
training O
dataset O
consists O
of O
quadruplets O
like O
( O
source O
x O
, O
modiÔ¨Åed O
hypothesis O
m(y O
/ O
prime O
) O
, O
action O
sequence O
a O
, O
target O
y O
) O
. O
We O
use O
Adam O
optimizer O
( O
Kingma O
and O
Ba O
, O
2014 O
) O
, O
an O
extension O
of O
stochastic O
gradient O
descent O
( O
Bottou O
, O
1991 O
) O
, O
to O
train O
the O
model O
. O
After O
training O
, O
the O
model O
with O
parameter O
ÀÜŒ∏is O
then O
used O
in O
inference O
phase O
to O
generate O
reÔ¨Åned O
translations O
for O
test O
data O
, O
which O
consists O
of O
triplets O
like O
( O
source O
x O
, O
modiÔ¨Åed O
hypothesis O
m(y O
/ O
prime O
) O
, O
action O
sequence O
a O
) O
. O
3 O
Automatic O
Data O
Annotation O
The O
actions O
we O
deÔ¨Åned O
in O
Section O
2.1 O
can O
be O
provided O
by O
human O
translators O
in O
real O
applications O
. O
However O
, O
it O
is O
impractical O
to O
manually O
collect O
a O
large O
scale O
annotated O
dataset O
for O
training O
our O
model O
. O
Thus O
we O
resort O
to O
propose O
an O
approach O
to O
automatically O
extract O
editing O
actions O
from O
a O
machine O
translation O
hypothesis O
and O
its O
corresponding O
reference O
. O
To O
make O
our O
method O
powerful O
, O
the O
number O
of O
editing O
actions O
which O
convert O
a O
hypothesis O
to O
its O
Algorithm O
1 O
Extracting O
actions O
with O
TER O
Input O
: O
hypothesis O
y O
/ O
prime O
, O
reference O
y O
m(y O
/ O
prime)‚Üêy O
/ O
prime O
a‚ÜêEmpty O
action O
sequence O
repeat O
Find O
reordering O
rthat O
most O
reduces O
min O
- O
editdistance(m(y O
/ O
prime),y O
) O
ifrreduces O
edit O
distance O
then O
m(y O
/ O
prime)‚Üêapplyingrtom(y O
/ O
prime O
) O
end O
if O
until O
no O
beneÔ¨Åcial O
reordering O
remains O
a‚Üêmin O
- O
edit(m(y O
/ O
prime),y O
) O
m(y O
/ O
prime)‚Üêinsert O
/ O
angbracketleftINS O
/ O
angbracketrightintom(y O
/ O
prime)based O
on O
a O
Output O
: O
m(y O
/ O
prime),areference O
is O
minimal O
as O
presented O
in O
Section O
2.1 O
. O
Snover O
et O
al O
. O
( O
2006 O
) O
study O
this O
problem O
and O
point O
out O
that O
its O
optimal O
solution O
is O
NP O
- O
hard O
( O
Lopresti O
and O
Tomkins O
, O
1997 O
; O
Shapira O
and O
Storer O
, O
2002 O
) O
. O
To O
optimize O
the O
number O
of O
editing O
actions O
, O
they O
instead O
propose O
an O
approximate O
algorithm O
based O
on O
minimal O
edit O
distance O
. O
The O
basic O
idea O
of O
their O
algorithm O
can O
be O
explained O
as O
follows O
. O
It O
repeatedly O
modiÔ¨Åes O
the O
intermediate O
string O
by O
applying O
reordering O
actions O
which O
is O
greedily O
found O
to O
mostly O
reduce O
the O
edit O
distance O
between O
the O
intermediate O
string O
and O
the O
reference O
, O
until O
no O
more O
beneÔ¨Åcial O
reordering O
remains O
. O
In O
this O
paper O
, O
we O
adopt O
the O
basic O
idea O
of O
Snover O
et O
al O
. O
( O
2006 O
) O
to O
automatically O
extract O
actions O
. O
As O
shown O
in O
Algorithm O
1 O
, O
given O
a O
reference O
and O
a O
hypothesis O
, O
the O
algorithm O
repeatedly O
reorders O
words O
to O
reduce O
the O
word O
- O
level O
minimal O
edit O
distance O
between O
reference O
yand O
modiÔ¨Åed O
hypothesis O
m(y O
/ O
prime O
) O
until O
no O
beneÔ¨Åcial O
reordering O
remains O
. O
With O
the O
modiÔ¨Åed O
hypothesis O
m(y O
/ O
prime O
) O
, O
the O
algorithm O
then O
calculates O
the O
editing O
action O
sequence O
athat O
minimize O
the O
word O
- O
level O
edit O
distance O
between O
m(y O
/ O
prime)andy O
( O
see O
Action O
Sequence O
ain O
Figure O
1 O
) O
. O
It O
Ô¨Ånally O
inserts O
special O
token O
/ O
angbracketleftINS O
/ O
angbracketrightto O
keep O
alignment O
between O
the O
modiÔ¨Åed O
hypothesis O
and O
the O
action O
sequence O
( O
see O
ModiÔ¨Åed O
m(y O
/ O
prime)in O
Figure O
1 O
) O
. O
The O
output O
of O
the O
algorithm O
, O
which O
is O
a O
tuple O
of O
modiÔ¨Åed O
hypothesis O
and O
action O
sequence O
, O
together O
with O
the O
source O
sentence O
and O
its O
reference O
, O
are O
used O
to O
train O
our O
model O
as O
described O
in O
Section O
2.2 O
. O
4 O
Experiment O
We O
conduct O
simulated O
experiment O
on O
translation O
datasets O
. O
SpeciÔ¨Åcally O
, O
we O
translate O
the O
source O
sentences O
in O
translation O
datasets O
with O
a O
pre O
- O
trained O
Transformer O
model O
and O
build O
the O
training O
data O
with O
simulated O
human O
feedback O
using O
algorithm O
described O
in O
Section O
3 O
. O
4.1 O
Dataset O
and O
Settings O
The O
experiment O
is O
conducted O
on O
three O
translation O
datasets O
: O
the O
IWSLT‚Äô14 O
English O
- O
German O
dataset O
( O
Cettolo O
et O
al O
. O
, O
2014 O
) O
, O
the O
WMT‚Äô14 O
EnglishGerman O
dataset O
( O
Bojar O
et O
al O
. O
, O
2014 O
) O
and O
the O
WMT‚Äô17 O
Chinese O
- O
English O
dataset O
( O
Ondrej O
et O
al O
. O
, O
2017 O
) O
. O
The O
IWSLT‚Äô14 O
English O
- O
German O
dataset O
consists O
of O
170k O
sentence O
pairs O
from O
TED O
talk O
subtitles O
. O
We O
use O
dev2010 O
as O
validation O
set O
which O
contains O
887 O
sentent O
pairs O
, O
and O
a O
concatenation O
of O
tst2010 O
, O
tst2011 O
andtst2012 O
as O
test O
set O
which O
con-4ModelIWSLT‚Äô14 O
WMT‚Äô14 O
WMT‚Äô17 O
EN O
- O
DE O
DE O
- O
EN O
EN O
- O
DE O
DE O
- O
EN O
EN O
- O
ZH O
ZH O
- O
EN O
BLEU O
TER O
BLEU O
TER O
BLEU O
TER O
BLEU O
TER O
BLEU O
TER O
BLEU O
TER O
ConvS2S‚Ä†24.20 O
- O
27.40 O
- O
25.20 O
- O
29.70 O
- O
- O
- O
- O
QuickEdit‚Ä†30.80 O
- O
34.60 O
- O
36.60 O
- O
41.30 O
- O
- O
- O
- O
Transformer O
27.40 O
0.52 O
33.17 O
0.45 O
26.69 O
0.56 O
31.73 O
0.48 O
32.53 O
0.55 O
21.89 O
0.61 O
QuickEdit‚Ä°34.33 O
0.43 O
40.13 O
0.39 O
37.00 O
0.43 O
41.48 O
0.39 O
41.20 O
0.43 O
29.78 O
0.51 O
Touch O
Baseline O
34.48 O
0.42 O
40.09 O
0.35 O
33.92 O
0.43 O
39.47 O
0.37 O
38.96 O
0.42 O
29.17 O
0.51 O
Touch O
Editing O
44.25 O
0.32 O
50.39 O
0.29 O
50.49 O
0.28 O
56.47 O
0.24 O
57.84 O
0.28 O
45.67 O
0.33 O
Table O
1 O
: O
Results O
of O
different O
systems O
measured O
in O
BLEU O
and O
TER.‚Ä†denotes O
the O
results O
from O
Quick O
Edit O
. O
QuickEdit‚Ä°is O
our O
reimplementation O
based O
on O
Transformer O
. O
Touch O
baseline O
is O
the O
result O
modiÔ¨Åed O
from O
initial O
hypothesis O
by O
deleting O
and O
reordering O
words O
. O
Touch O
Editing O
is O
our O
model O
trained O
with O
all O
actions O
described O
in O
Section O
2.1 O
. O
tains O
4698 O
sentence O
pairs O
. O
For O
WMT‚Äô14 O
EnglishGerman O
dataset O
, O
we O
use O
the O
same O
data O
and O
preprocessing O
as O
( O
Luong O
et O
al O
. O
, O
2015 O
) O
. O
The O
dataset O
consists O
of O
4.5 O
M O
sentence O
pairs O
for O
training1 O
. O
We O
take O
newstest2013 O
for O
validation O
and O
newstest2014 O
for O
testing O
. O
For O
Chinese O
to O
English O
dataset O
, O
we O
use O
CWMT O
portion O
which O
is O
a O
subset O
of O
WMT‚Äô17 O
training O
data O
containing O
9 O
M O
sentence O
pairs O
. O
We O
validate O
onnewsdev2017 O
and O
test O
on O
newstest2017 O
. O
As O
for O
vocabulary O
, O
the O
English O
and O
German O
datasets O
are O
encoded O
using O
byte O
- O
pair O
encoding O
( O
Sennrich O
et O
al O
. O
, O
2015 O
) O
with O
a O
shared O
vocabulary O
of O
8k O
tokens O
for O
IWSLT‚Äô14 O
and O
32k O
tokens O
for O
WMT‚Äô14 O
. O
For O
Chinese O
to O
English O
dataset O
, O
the O
English O
vocabulary O
is O
set O
to O
30k O
subwords O
, O
while O
the O
Chinese O
data O
is O
tokenized O
into O
character O
level O
and O
the O
vocabulary O
is O
set O
to O
10k O
characters O
. O
Note O
that O
even O
with O
subword O
units O
or O
character O
units O
, O
the O
actions O
are O
marked O
in O
word O
level O
, O
i.e. O
all O
units O
from O
a O
given O
word O
share O
the O
same O
actions O
. O
We O
train O
the O
models O
with O
two O
settings O
. O
For O
the O
larger O
WMT O
English O
- O
German O
and O
English O
- O
Chinese O
dataset O
, O
we O
borrow O
the O
Transformer O
base O
parameter O
set O
of O
Vaswani O
et O
al O
. O
( O
2017 O
) O
, O
which O
contains O
6 O
layers O
for O
encoders O
and O
decoder O
respectively O
. O
The O
multi O
- O
head O
attention O
of O
each O
layer O
contains O
8 O
heads O
. O
The O
word O
embedding O
size O
is O
set O
to O
512 O
and O
the O
feedforward O
layer O
dimension O
is O
2048 O
. O
For O
the O
smaller O
IWSLT O
dataset O
, O
we O
use O
3 O
layers O
for O
each O
component O
and O
multi O
- O
head O
attention O
with O
4 O
heads O
in O
each O
layer O
. O
The O
word O
embedding O
size O
is O
256 O
and O
the O
feedforward O
layers O
‚Äô O
hidden O
size O
is O
1024 O
. O
We O
also O
apply O
label O
smoothing O
œÉls= O
0.1and O
dropout O
pdropout O
= O
0.1during O
training O
. O
All O
models O
are O
1We O
use O
the O
pre O
- O
processed O
data O
from O
https://nlp O
. O
stanford.edu/projects/nmt/trained O
from O
scratch O
with O
corresponding O
training O
data O
, O
e.g. O
, O
parallel O
data O
for O
Transformer O
baseline O
model O
and O
annotated O
data O
for O
Touch O
Editing O
. O
4.2 O
Main O
Results O
We O
report O
the O
results O
of O
different O
systems O
including O
Transformer O
and O
QuickEdit O
. O
The O
Transformer O
model O
is O
tested O
on O
bitext O
data O
, O
i.e. O
, O
the O
model O
directly O
generates O
translations O
based O
on O
source O
sentences O
. O
As O
for O
the O
QuickEdit O
, O
we O
followed O
the O
settings O
of O
Grangier O
and O
Auli O
( O
2018 O
) O
, O
in O
which O
they O
mark O
all O
words O
in O
initial O
translation O
results O
that O
do O
not O
appear O
in O
the O
references O
as O
incorrect O
, O
and O
use O
the O
QuickEdit O
model O
to O
generate O
reÔ¨Åned O
translations O
. O
In O
Touch O
Baseline O
setting O
, O
we O
use O
the O
algorithm O
described O
in O
Section O
3 O
to O
obtain O
the O
actions O
respect O
to O
initial O
translations O
and O
references O
, O
and O
then O
apply O
reordering O
and O
deletion O
actions O
to O
obtain O
reÔ¨Åned O
translations O
. O
The O
Touch O
Edit O
setting O
accesses O
the O
same O
information O
as O
Touch O
Baseline O
but O
uses O
the O
neural O
model O
described O
in O
Section O
2.2 O
to O
handle O
the O
actions O
. O
Note O
that O
the O
original O
QuickEdit O
model O
is O
based O
on O
ConvS2S O
, O
and O
thus O
we O
reimplement O
it O
based O
on O
Transformer O
to O
keep O
the O
fairness O
of O
comparison2 O
. O
As O
shown O
in O
Table O
1 O
, O
our O
model O
strongly O
outperforms O
other O
systems O
. O
As O
for O
BLEU O
score O
, O
our O
model O
achieves O
up O
to O
+25.31 O
than O
Transformer O
and O
+16.64 O
than O
QuickEdit O
. O
Our O
model O
also O
signiÔ¨Åcantly O
reduces O
TER O
by O
-0.28 O
and O
-0.18 O
comparing O
to O
Transformer O
and O
QuickEdit O
. O
We O
also O
notice O
that O
the O
improvement O
on O
the O
smaller O
IWSLT‚Äô14 O
dataset O
( O
up O
to O
17.22 O
) O
is O
not O
as O
2In O
fact O
, O
the O
comparison O
is O
still O
unfair O
because O
QuickEdit O
and O
our O
mothod O
access O
more O
supervised O
information O
than O
Transformer O
form O
simulated O
human O
feedback O
, O
which O
is O
the O
nature O
of O
interaction O
settings.5Reordering O
RIBES O
Transformer O
4672 O
79.97 O
QuickEdit O
4799 O
84.33 O
Touch O
Editing O
650 O
90.50 O
Table O
2 O
: O
Word O
reordering O
quality O
, O
measured O
in O
number O
of O
word O
reorderings O
required O
to O
align O
to O
references O
, O
and O
RIBES O
score O
. O
signiÔ¨Åcant O
as O
that O
on O
the O
larger O
WMT‚Äô14 O
dataset O
( O
up O
to O
24.74 O
) O
and O
WMT‚Äô17 O
dataset O
( O
up O
to O
25.31 O
) O
. O
This O
observation O
is O
in O
consistent O
with O
QuickEdit O
, O
which O
also O
gains O
lower O
improvement O
on O
the O
smaller O
dataset O
. O
The O
reason O
, O
as O
described O
in O
Grangier O
and O
Auli O
( O
2018 O
) O
, O
is O
that O
the O
underlying O
machine O
translation O
model O
is O
overÔ¨Åtted O
on O
the O
smaller O
170k O
dataset O
. O
Thus O
the O
translation O
output O
requires O
less O
edits O
on O
which O
we O
build O
simulated O
editing O
action O
dataset O
. O
The O
limited O
supervised O
data O
further O
impacts O
the O
model O
quality O
and O
Ô¨Ånal O
results O
. O
4.3 O
Analysis O
To O
further O
investigate O
the O
model O
capacity O
, O
we O
conduct O
four O
experiments O
on O
WMT‚Äô14 O
English O
to O
German O
dataset O
. O
We O
analyze O
the O
factors O
that O
bring O
the O
remarkable O
improvement O
by O
modeling O
coverage O
, O
reordering O
quality O
and O
accuracy O
of O
each O
action O
type O
. O
We O
also O
test O
our O
model O
with O
limited O
number O
of O
actions O
to O
evaluate O
the O
model O
usability O
with O
partial O
feedback O
. O
Reordering O
We O
evaluate O
the O
word O
reordering O
quality O
of O
our O
model O
, O
compared O
with O
Transformer O
and O
QuickEdit O
. O
We O
adopt O
two O
automatic O
evaluation O
metrics O
. O
One O
metric O
is O
based O
on O
monolingual O
alignment O
. O
We O
Ô¨Årstly O
align O
model O
hypotheses O
and O
references O
with O
TER O
, O
and O
then O
count O
the O
number O
of O
words O
that O
should O
be O
reordered O
. O
As O
shown O
by O
Reordering O
in O
Table O
2 O
, O
the O
output O
of O
our O
model O
requires O
less O
word O
reorderings O
to O
align O
with O
reference O
. O
The O
other O
metric O
is O
RIBES O
( O
Isozaki O
et O
al O
. O
, O
2010 O
) O
, O
which O
is O
based O
on O
rank O
correlation O
. O
As O
shown O
in O
Table O
2 O
, O
our O
method O
outperforms O
the O
other O
two O
systems O
with O
90.50versus O
79.97for O
Transformer O
and84.33for O
QuickEdit O
. O
Accuracy O
As O
described O
in O
Section O
2.1 O
, O
the O
actions O
of O
our O
method O
represent O
human O
editing O
intentions O
, O
i.e. O
, O
they O
indicate O
errors O
in O
original O
hypothesis O
and O
our O
model O
is O
expected O
to O
correct O
these O
errors O
based O
on O
editing O
actions O
. O
To O
evaluate O
the O
accuracyTotal O
Correct O
Accuracy O
Quick O
EditDeletion O
6438 O
4440 O
68.97 O
% O
Insertion O
4430 O
681 O
15.37 O
% O
Substitution O
20858 O
5030 O
24.12 O
% O
Touch O
EditingDeletion O
6438 O
6383 O
99.15 O
% O
Insertion O
4430 O
1609 O
36.32 O
% O
Substitution O
20858 O
6645 O
31.86 O
% O
Table O
3 O
: O
Accuracy O
of O
actions O
. O
Total O
means O
number O
of O
actions O
to O
transform O
the O
draft O
machine O
translations O
into O
references O
. O
Correct O
means O
how O
many O
words O
are O
corrected O
( O
or O
deleted O
) O
by O
the O
model O
. O
ofINSERTION O
, O
DELETION O
and O
SUBSTITUTION O
, O
we O
Ô¨Årst O
use O
TER O
to O
align O
machine O
translation O
hypotheses O
and O
references O
, O
as O
well O
as O
our O
model O
‚Äôs O
outputs O
and O
references O
. O
With O
the O
references O
as O
intermediates O
, O
we O
then O
align O
our O
model O
‚Äôs O
outputs O
and O
original O
machine O
translations O
. O
With O
the O
alignment O
result O
, O
we O
directly O
check O
whether O
the O
words O
with O
actions O
are O
corrected O
or O
not O
to O
calculate O
the O
accuracy O
of O
the O
three O
actions O
. O
To O
make O
a O
complete O
comparison O
, O
we O
also O
analyze O
the O
results O
of O
QuickEdit O
and O
calculate O
the O
accuracy O
. O
As O
shown O
in O
Table O
3 O
, O
our O
model O
achieves O
the O
accuracy O
of O
99.15 O
% O
for O
deletion3,36.32 O
% O
for O
insertion O
and O
31.86 O
% O
for O
substitution O
. O
The O
high O
deletion O
accuracy O
shows O
that O
our O
model O
indeed O
learns O
to O
delete O
over O
- O
translated O
words O
. O
For O
insertion O
and O
substitution O
, O
the O
actions O
only O
indicate O
where O
to O
insert O
or O
substitute O
, O
and O
do O
not O
provide O
any O
ground O
truth O
. O
Since O
the O
self O
- O
attention O
mechanism O
in O
Transformer O
is O
good O
at O
word O
sense O
disambiguation O
( O
Tang O
et O
al O
. O
, O
2018a O
, O
b O
) O
, O
our O
model O
is O
able O
to O
select O
correct O
words O
to O
insert O
or O
substitute O
. O
Partial O
Feedback O
The O
model O
we O
train O
and O
test O
is O
based O
on O
all O
actions O
, O
i.e. O
, O
all O
translation O
errors O
of O
the O
initial O
hypotheses O
are O
marked O
out O
. O
However O
, O
a O
human O
translator O
may O
not O
provide O
all O
marks O
. O
In O
fact O
, O
the O
feedback O
of O
human O
translators O
is O
hard O
to O
predict O
, O
and O
vary O
with O
different O
translators O
. O
In O
this O
case O
, O
we O
test O
our O
model O
with O
simulated O
partial O
feedback O
. O
We O
train O
our O
model O
with O
all O
actions O
and O
randomly O
select O
0%,5%, O
... O
100 O
% O
of O
actions O
in O
test O
set O
to O
simulate O
human O
behavior O
. O
To O
further O
investigate O
the O
effect O
of O
partial O
feedback O
3We O
do O
not O
explicitly O
remove O
words O
that O
marked O
as O
DELE O
TION O
and O
the O
neural O
model O
is O
responsible O
for O
making O
Ô¨Ånal O
decision O
whether O
these O
words O
should O
be O
deleted O
. O
It O
might O
slightly O
hurt O
BLEU O
and O
accuracy O
but O
potentially O
generates O
more O
Ô¨Çuent O
translations.6253035404550550%20%40%60%80%100%BLEU O
Percentage O
of O
Actionsalltypeswithout O
REORDERINGSUBSTITUTIONDELETIONINSERTIONFigure O
4 O
: O
Results O
of O
partial O
feedback O
measured O
in O
BLEU O
score O
. O
We O
train O
Ô¨Åve O
models O
to O
investigate O
the O
effects O
of O
partial O
feedback O
on O
different O
actions O
. O
on O
different O
actions O
, O
we O
train O
three O
extra O
models O
with O
speciÔ¨Åc O
kinds O
of O
actions O
: O
INSERT O
, O
DELETE O
and O
SUBSTITUTE O
. O
We O
then O
randomly O
select O
part O
of O
each O
kind O
of O
actions O
to O
test O
the O
model O
. O
Note O
that O
theREORDERING O
actions O
are O
always O
enabled O
since O
they O
are O
operated O
on O
a O
segment O
of O
words O
and O
can O
not O
be O
partially O
disabled O
. O
To O
investigate O
the O
effect O
of O
REORDERING O
actions O
, O
we O
also O
train O
a O
model O
without O
reordering O
and O
partially O
select O
three O
kinds O
of O
actions O
to O
test O
the O
model O
. O
As O
shown O
in O
Figure O
4 O
, O
for O
the O
model O
trained O
with O
all O
actions O
, O
the O
BLEU O
scores O
increases O
from O
29.43 O
( O
with O
reordering O
only O
) O
to O
50.49 O
( O
with O
all O
actions O
) O
as O
more O
actions O
are O
provided O
. O
For O
the O
models O
trained O
with O
speciÔ¨Åc O
kinds O
of O
actions O
and O
the O
model O
trained O
without O
reordering O
, O
the O
observation O
is O
similar O
. O
4.4 O
Experiments O
on O
Post O
- O
Editing O
Data O
In O
previous O
sections O
, O
our O
model O
is O
tested O
and O
analyzed O
on O
automatic O
machine O
translation O
datasets O
. O
However O
, O
in O
post O
- O
editing O
scenarios O
, O
our O
model O
faces O
three O
major O
challenges O
: O
action O
inconsistency O
, O
data O
inconsistency O
and O
model O
inconsistency O
. O
For O
action O
inconsistency O
, O
the O
editing O
actions O
to O
train O
our O
model O
are O
extracted O
from O
machine O
predictions O
and O
references O
. O
The O
references O
in O
our O
training O
data O
are O
written O
by O
human O
from O
scratch O
, O
while O
in O
post O
- O
editing O
the O
references O
( O
human O
post O
- O
edited O
results O
) O
are O
revisions O
of O
machine O
translations O
, O
and O
thus O
the O
editing O
actions O
might O
be O
different O
. O
For O
data O
inconsistency O
, O
our O
model O
is O
trained O
on O
dataset O
of O
News O
domain O
( O
WMT O
) O
or O
TED O
talks O
( O
IWSLT O
) O
. O
However O
in O
real O
world O
, O
data O
may O
be O
from O
any O
other O
domains O
. O
For O
model O
inconsistency O
, O
we O
use O
Transformer O
to O
build O
our O
training O
data O
while O
the O
translation O
model O
usedWMT O
16 O
WMT O
17 O
BLEU O
TER O
BLEU O
TER O
MT O
62.48 O
0.24 O
62.83 O
0.24 O
QuickEdit O
67.14 O
0.19 O
69.22 O
0.18 O
Touch O
Editing O
82.05 O
0.09 O
82.88 O
0.09 O
Table O
4 O
: O
Results O
on O
post O
- O
editing O
dataset O
in O
terms O
of O
BLEU O
and O
TER O
. O
in O
real O
applications O
may O
be O
different O
. O
To O
investigate O
the O
performance O
facing O
the O
three O
challenges O
, O
we O
test O
our O
model O
on O
WMT O
EnglishGerman O
Automatic O
Post O
- O
Editing O
( O
APE O
) O
dataset O
in O
IT O
domain O
using O
data O
from O
WMT‚Äô16 O
( O
Bojar O
et O
al O
. O
, O
2016 O
) O
and O
WMT‚Äô17 O
( O
Ondrej O
et O
al O
. O
, O
2017 O
) O
. O
The O
test O
data O
consists O
of O
triplets O
like O
( O
source O
, O
machine O
translation O
, O
human O
post O
- O
edit O
) O
, O
in O
which O
the O
machine O
translation O
is O
generated O
with O
a O
PBSMT O
system O
. O
We O
use O
the O
algorithm O
of O
Section O
3 O
to O
extract O
actions O
from O
machine O
translations O
and O
human O
post O
- O
edited O
sentences O
. O
With O
the O
actions O
and O
original O
machine O
translations O
, O
we O
use O
the O
model O
trained O
on O
WMT‚Äô14 O
English O
- O
German O
dataset O
in O
Section O
4 O
to O
generate O
reÔ¨Åned O
translations O
. O
To O
make O
a O
comparison O
, O
we O
also O
evaluate O
QuickEdit O
with O
the O
same O
setting O
. O
Table O
4 O
summarizes O
the O
results O
on O
post O
- O
editing O
dataset O
. O
It O
is O
clear O
to O
see O
that O
even O
with O
the O
three O
kinds O
of O
inconsistency O
, O
our O
model O
still O
gains O
signiÔ¨Åcant O
improvements O
of O
up O
to O
20.05 O
BLEU O
than O
the O
raw O
machine O
translation O
system O
( O
PBSMT O
) O
. O
As O
for O
QuickEdit O
, O
the O
improvement O
on O
post O
- O
editing O
dataset O
( O
about O
4 O
- O
7 O
BLEU O
) O
is O
smaller O
than O
that O
on O
translation O
dataset O
( O
about O
11 O
BLEU O
) O
. O
We O
conjecture O
that O
the O
stable O
improvement O
of O
our O
method O
is O
due O
to O
more O
Ô¨Çexible O
action O
types O
. O
With O
the O
detailed O
editing O
actions O
, O
the O
model O
is O
competent O
to O
correct O
various O
of O
errors O
in O
draft O
machine O
translations O
, O
and O
thus O
leads O
to O
the O
robustness O
and O
effectiveness O
of O
our O
method O
. O
4.5 O
Discussion O
on O
Real O
Scenarios O
So O
far O
, O
the O
experiments O
we O
conducted O
are O
based O
on O
simulated O
human O
feedbacks O
, O
in O
which O
the O
actions O
are O
extracted O
from O
initial O
machine O
translation O
results O
and O
their O
corresponding O
references O
to O
simulate O
human O
editing O
actions O
. O
Thus O
in O
our O
simulated O
setting O
, O
the O
references O
are O
used O
in O
inference O
phase O
to O
simulate O
human O
behavior O
, O
as O
in O
other O
interaction O
methods O
( O
Denkowski O
et O
al O
. O
, O
2014 O
; O
Marie O
and O
Max O
, O
2015 O
; O
Grangier O
and O
Auli O
, O
2018 O
) O
. O
These O
experiments O
show O
that O
our O
method O
can O
signiÔ¨Åcantly7improve O
the O
initial O
translation O
with O
similated O
actions O
. O
However O
, O
whether O
the O
actions O
are O
convenient O
to O
perform O
is O
a O
key O
point O
in O
real O
applications O
. O
To O
investigate O
the O
usability O
and O
applicable O
scenarios O
of O
our O
method O
, O
we O
implement O
a O
real O
mobile O
application O
on O
iPhone O
, O
in O
which O
the O
actions O
can O
be O
performed O
on O
multi O
- O
touch O
screens O
. O
For O
a O
given O
source O
sentence O
, O
the O
application O
provides O
an O
initial O
machine O
translation O
. O
The O
text O
area O
of O
translation O
can O
response O
to O
several O
gestures4 O
: O
Tap O
indicated O
a O
missing O
word O
should O
be O
inserted O
into O
the O
nearest O
space O
between O
two O
words O
; O
Swipe O
on O
a O
word O
indicated O
that O
the O
word O
should O
be O
deleted O
; O
LongPress O
a O
word O
means O
the O
word O
should O
be O
substituted O
with O
other O
word O
; O
Pan O
can O
drag O
a O
word O
to O
another O
position O
. O
We O
conduct O
a O
free O
- O
use O
study O
with O
four O
participants O
, O
in O
which O
the O
participants O
are O
asked O
to O
translate O
20 O
sentences O
randomly O
selected O
from O
LDC O
Chinese O
- O
English O
test O
set O
with O
( O
1 O
) O
Touch O
Editing O
or O
( O
2 O
) O
keyboard O
input O
after O
5 O
minutes O
to O
get O
familiar O
with O
the O
application O
. O
We O
observe O
that O
the O
users O
with O
Touch O
Editing O
tends O
to O
correct O
an O
error O
for O
multiple O
times O
when O
the O
system O
can O
not O
predict O
a O
word O
they O
want O
, O
while O
the O
users O
with O
keyboard O
input O
tends O
to O
modify O
more O
content O
of O
initial O
translation O
and O
spend O
more O
time O
on O
choosing O
words O
. O
We O
then O
conduct O
an O
unstructured O
interview O
on O
the O
usability O
of O
our O
method O
. O
The O
result O
of O
the O
interview O
shows O
that O
Touch O
Editing O
is O
convenient O
and O
intuitive O
but O
lack O
of O
ability O
of O
generating O
Ô¨Ånal O
accurate O
translation O
. O
It O
can O
be O
treated O
as O
a O
light O
- O
weight O
proofreading O
method O
, O
and O
suitable O
for O
Pre O
- O
Post O
- O
Editing O
( O
Marie O
and O
Max O
, O
2015 O
) O
. O
5 O
Related O
Work O
Post O
- O
editing O
is O
a O
pragmatic O
method O
that O
allows O
human O
translators O
to O
directly O
correct O
errors O
in O
draft O
machine O
translations O
( O
Simard O
et O
al O
. O
, O
2007 O
) O
. O
Comparing O
to O
purely O
manual O
translation O
, O
it O
achieves O
higher O
productivity O
while O
maintaining O
the O
human O
translation O
quality O
( O
Plitt O
and O
Masselot O
, O
2010 O
; O
Federico O
et O
al O
. O
, O
2012 O
) O
. O
Many O
notable O
works O
introduce O
different O
levels O
of O
human O
- O
machine O
interactions O
in O
post O
- O
editing O
. O
Barrachina O
et O
al O
. O
( O
2009 O
) O
propose O
a O
preÔ¨Åx O
- O
based O
interactive O
method O
which O
enable O
users O
to O
correct O
the O
Ô¨Årst O
translation O
error O
from O
left O
to O
right O
in O
each O
iteration O
. O
4These O
gestures O
are O
explicit O
and O
directly O
supported O
by O
Apple O
iOS O
devices O
: O
https://developer.apple.com/ O
documentation O
/ O
uikit O
/ O
uigesturerecognizerGreen O
et O
al O
. O
( O
2014 O
) O
implement O
a O
preÔ¨Åx O
- O
based O
interactive O
translation O
system O
and O
Huang O
et O
al O
. O
( O
2015 O
) O
adopt O
the O
preÔ¨Åx O
constrained O
translation O
candidates O
into O
a O
novel O
input O
method O
for O
translators O
. O
Peris O
et O
al O
. O
( O
2017 O
) O
further O
extend O
this O
idea O
to O
neural O
machine O
translation O
. O
The O
preÔ¨Åx O
- O
based O
protocol O
is O
inÔ¨Çexible O
since O
users O
have O
to O
follow O
the O
left O
- O
to O
- O
right O
order O
. O
To O
overcome O
the O
weakness O
of O
preÔ¨Åx O
- O
based O
approach O
, O
Gonz O
¬¥ O
alez O
- O
Rubio O
et O
al O
. O
( O
2016 O
) O
; O
Cheng O
et O
al O
. O
( O
2016 O
) O
introduce O
interaction O
methods O
that O
allow O
users O
to O
correct O
errors O
at O
arbitrary O
position O
in O
a O
machine O
hypothesis O
, O
while O
Weng O
et O
al O
. O
( O
2019 O
) O
also O
preventing O
repeat O
mistakes O
by O
memorizing O
revision O
actions O
. O
Hokamp O
and O
Liu O
( O
2017 O
) O
propose O
grid O
beam O
search O
to O
incorporate O
lexical O
constraints O
like O
words O
and O
phrases O
provided O
by O
human O
translators O
and O
force O
the O
constraints O
to O
appear O
in O
hypothesis O
. O
Recently O
, O
some O
researchers O
resort O
to O
more O
Ô¨Çexible O
interactions O
, O
which O
only O
require O
mouse O
click O
or O
touch O
actions O
. O
For O
example O
, O
Marie O
and O
Max O
( O
2015 O
) O
; O
Domingo O
et O
al O
. O
( O
2016 O
) O
propose O
interactive O
translation O
methods O
which O
ask O
user O
to O
select O
correct O
or O
incorrect O
segments O
of O
a O
translation O
with O
mouse O
only O
. O
Similar O
to O
our O
work O
, O
Grangier O
and O
Auli O
( O
2018 O
) O
propose O
a O
mouse O
based O
interactive O
method O
which O
allows O
users O
to O
simply O
mark O
the O
incorrect O
words O
in O
draft O
machine O
hypotheses O
and O
expect O
the O
system O
to O
generate O
reÔ¨Åned O
translations O
. O
Herbig O
et O
al O
. O
( O
2019 O
, O
2020 O
) O
propose O
a O
multi O
- O
modal O
interface O
for O
post O
- O
editors O
which O
takes O
pen O
, O
touch O
, O
and O
speech O
modalities O
into O
consideration O
. O
The O
protocol O
that O
given O
an O
initial O
translation O
to O
generate O
a O
reÔ¨Åned O
translation O
, O
is O
also O
used O
in O
polishing O
mechanism O
in O
machine O
translation O
( O
Xia O
et O
al O
. O
, O
2017 O
; O
Geng O
et O
al O
. O
, O
2018 O
) O
and O
automatic O
post O
- O
editing O
( O
APE O
) O
task O
( O
Lagarda O
et O
al O
. O
, O
2009 O
; O
Pal O
et O
al O
. O
, O
2016 O
) O
. O
The O
idea O
of O
multi O
- O
source O
encoder O
is O
also O
widely O
used O
in O
the O
Ô¨Åeld O
of O
APE O
research O
( O
Chatterjee O
et O
al O
. O
, O
2018 O
, O
2019 O
) O
. O
In O
human O
- O
machine O
interaction O
scenarios O
, O
the O
human O
feedback O
is O
used O
as O
extra O
information O
in O
polishing O
process O
. O
6 O
Conclusion O
In O
this O
paper O
, O
we O
propose O
Touch O
Editing O
, O
a O
Ô¨Çexible O
and O
effective O
interaction O
approach O
which O
allows O
human O
translators O
to O
revise O
machine O
translation O
results O
via O
touch O
actions O
. O
The O
actions O
we O
introduce O
can O
be O
provided O
with O
gestures O
like O
tapping O
, O
panning O
, O
swiping O
or O
long O
pressing O
on O
touch O
screens O
to O
represent O
human O
editing O
intentions O
. O
We O
present O
a8simulated O
action O
extraction O
method O
for O
constructing O
training O
data O
and O
a O
dual O
- O
encoder O
model O
to O
handle O
the O
actions O
to O
generate O
reÔ¨Åned O
translations O
. O
We O
prove O
the O
effectiveness O
of O
the O
proposed O
interaction O
approach O
and O
discuss O
the O
applicable O
scenarios O
with O
a O
free O
- O
use O
study O
. O
For O
future O
works O
, O
we O
plan O
to O
conduct O
large O
scale O
real O
world O
experiments O
to O
evaluate O
the O
productivity O
of O
different O
interactive O
machine O
translation O
methods O
. O
Abstract O
Multilingual O
pretrained O
language O
models O
( O
such O
as O
multilingual O
BERT O
) O
have O
achieved O
impressive O
results O
for O
cross O
- O
lingual O
transfer O
. O
However O
, O
due O
to O
the O
constant O
model O
capacity O
, O
multilingual O
pre O
- O
training O
usually O
lags O
behind O
the O
monolingual O
competitors O
. O
In O
this O
work O
, O
we O
present O
two O
approaches O
to O
improve O
zero O
- O
shot O
cross O
- O
lingual O
classiÔ¨Åcation O
, O
by O
transferring O
the O
knowledge O
from O
monolingual O
pretrained O
models O
to O
multilingual O
ones O
. O
Experimental O
results O
on O
two O
cross O
- O
lingual O
classiÔ¨Åcation O
benchmarks O
show O
that O
our O
methods O
outperform O
vanilla O
multilingual O
Ô¨Åne O
- O
tuning O
. O
1 O
Introduction O
Supervised O
text O
classiÔ¨Åcation O
heavily O
relies O
on O
manually O
annotated O
training O
data O
, O
while O
the O
data O
are O
usually O
only O
available O
in O
rich O
- O
resource O
languages O
, O
such O
as O
English O
. O
It O
requires O
great O
effort O
to O
make O
the O
resources O
available O
in O
other O
languages O
. O
Various O
methods O
have O
been O
proposed O
to O
build O
cross O
- O
lingual O
classiÔ¨Åcation O
models O
by O
exploiting O
machine O
translation O
systems O
( O
Xu O
and O
Yang O
, O
2017 O
; O
Chen O
et O
al O
. O
, O
2018 O
; O
Conneau O
et O
al O
. O
, O
2018 O
) O
, O
and O
learning O
multilingual O
embeddings O
( O
Conneau O
et O
al O
. O
, O
2018 O
; O
Yu O
et O
al O
. O
, O
2018 O
; O
Artetxe O
and O
Schwenk O
, O
2019 O
; O
Eisenschlos O
et O
al O
. O
, O
2019 O
) O
. O
Recently O
, O
multilingual O
pretrained O
language O
models O
have O
shown O
surprising O
cross O
- O
lingual O
effectiveness O
on O
a O
wide O
range O
of O
downstream O
tasks O
( O
Devlin O
et O
al O
. O
, O
2019 O
; O
Conneau O
and O
Lample O
, O
2019 O
; O
Conneau O
et O
al O
. O
, O
2020 O
; O
Chi O
et O
al O
. O
, O
2020a O
, O
b O
) O
. O
Even O
without O
using O
any O
parallel O
corpora O
, O
the O
pretrained O
models O
can O
still O
perform O
zero O
- O
shot O
cross O
- O
lingual O
classiÔ¨Åcation O
( O
Pires O
et O
al O
. O
, O
2019 O
; O
Wu O
and O
Dredze O
, O
2019 O
; O
Keung O
et O
al O
. O
, O
2019 O
) O
. O
That O
is O
, O
these O
models O
can O
be O
Ô¨Åne O
- O
tuned O
in O
a O
source O
language O
, O
and O
then O
directly O
evaluated O
in O
other O
target O
languages O
. O
Despite O
‚àóContribution O
during O
internship O
at O
Microsoft O
Research.the O
effectiveness O
of O
cross O
- O
lingual O
transfer O
, O
the O
multilingual O
pretrained O
language O
models O
have O
their O
own O
drawbacks O
. O
Due O
to O
the O
constant O
number O
of O
model O
parameters O
, O
the O
model O
capacity O
of O
the O
richresource O
languages O
decreases O
if O
we O
adds O
languages O
for O
pre O
- O
training O
. O
The O
curse O
of O
multilinguality O
results O
in O
that O
the O
multilingual O
models O
usually O
perform O
worse O
than O
their O
monolingual O
competitors O
on O
downstream O
tasks O
( O
Arivazhagan O
et O
al O
. O
, O
2019 O
; O
Conneau O
et O
al O
. O
, O
2020 O
) O
. O
The O
observations O
motivate O
us O
to O
leverage O
monolingual O
pretrained O
models O
to O
improve O
multilingual O
models O
for O
cross O
- O
lingual O
classiÔ¨Åcation O
. O
In O
this O
paper O
, O
we O
propose O
a O
multilingual O
Ô¨Ånetuning O
method O
( O
M O
ONOX O
) O
based O
on O
the O
teacherstudent O
framework O
, O
where O
a O
multilingual O
student O
model O
learns O
end O
task O
skills O
from O
a O
monolingual O
teacher O
. O
Intuitively O
, O
monolingual O
pretrained O
models O
are O
used O
to O
provide O
supervision O
of O
downstream O
tasks O
, O
while O
multilingual O
models O
are O
employed O
for O
knowledge O
transfer O
across O
languages O
. O
We O
conduct O
experiments O
on O
two O
widely O
used O
cross O
- O
lingual O
classiÔ¨Åcation O
datasets O
, O
where O
our O
methods O
outperform O
baseline O
models O
on O
zero O
- O
shot O
cross O
- O
lingual O
classiÔ¨Åcation O
. O
Moreover O
, O
we O
show O
that O
the O
monolingual O
teacher O
model O
can O
help O
the O
student O
multilingual O
model O
for O
both O
the O
source O
language O
and O
target O
languages O
, O
even O
though O
the O
student O
model O
is O
only O
trained O
in O
the O
source O
language O
. O
2 O
Background O
: O
Multilingual O
Fine O
- O
Tuning O
We O
use O
multilingual O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
for O
multilingual O
pretrained O
language O
models O
. O
The O
pretrained O
model O
uses O
the O
BERT O
- O
style O
Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
architecture O
, O
and O
follows O
the O
similar O
Ô¨Åne O
- O
tuning O
procedure O
as O
BERT O
for O
text O
classiÔ¨Åcation O
, O
which O
is O
illustrated O
in O
Figure O
1(a O
) O
. O
To O
be O
speciÔ¨Åc O
, O
the O
Ô¨Årst O
input O
token O
of O
the O
models O
is O
always O
a O
special O
classiÔ¨Åcation O
token12Figure O
1 O
: O
Illustration O
of O
multilingual O
LM O
Ô¨Åne O
- O
tuning O
. O
( O
a O
) O
The O
original O
multilingual O
LM O
Ô¨Åne O
- O
tuning O
procedure O
for O
cross O
- O
lingual O
classiÔ¨Åcation O
. O
( O
b O
) O
The O
Ô¨Åne O
- O
tuning O
procedure O
of O
our O
proposed O
M O
ONOX O
via O
knowledge O
distillation O
( O
M O
ONOX O
- O
K O
D O
) O
. O
Notice O
that O
M O
ONOX O
does O
not O
use O
any O
target O
language O
data O
during O
Ô¨Åne O
- O
tuning O
. O
[ O
CLS O
] O
. O
During O
Ô¨Åne O
- O
tuning O
, O
the O
Ô¨Ånal O
hidden O
state O
of O
the O
special O
token O
is O
used O
as O
the O
sentence O
representation O
. O
In O
order O
to O
output O
predictions O
, O
an O
additional O
softmax O
classiÔ¨Åer O
is O
built O
on O
top O
of O
the O
sentence O
representation O
. O
Denoting O
Das O
the O
training O
data O
in O
the O
source O
language O
, O
the O
pretrained O
models O
are O
Ô¨Åne O
- O
tuned O
with O
standard O
cross O
- O
entropy O
loss O
: O
LCE(Œ∏;D O
) O
= O
‚àí/summationdisplay O
( O
x O
, O
y)‚ààDlogp(y|x;Œ∏ O
) O
whereŒ∏represents O
model O
parameters O
. O
Then O
the O
model O
is O
directly O
evaluated O
on O
other O
languages O
for O
cross O
- O
lingual O
classiÔ¨Åcation O
. O
3 O
Methods O
As O
shown O
in O
Figure O
1(b O
) O
, O
we O
Ô¨Årst O
Ô¨Åne O
- O
tune O
the O
monolingual O
pretrained O
model O
in O
the O
source O
language O
. O
Then O
we O
transfer O
task O
knowledge O
to O
the O
multilingual O
pretrained O
model O
by O
soft O
( O
Section O
3.1 O
) O
or O
hard O
( O
Section O
3.2 O
) O
labels O
. O
We O
describe O
two O
variants O
of O
our O
proposed O
method O
( O
M O
ONOX O
) O
as O
follows O
. O
3.1 O
Knowledge O
Distillation O
In O
order O
to O
transfer O
task O
- O
speciÔ¨Åc O
knowledge O
from O
monolingual O
model O
to O
multilingual O
model O
, O
we O
propose O
to O
use O
knowledge O
distillation O
( O
Hinton O
et O
al O
. O
,2015 O
) O
under O
our O
M O
ONOX O
framework O
, O
where O
a O
student O
modelsis O
trained O
with O
soft O
labels O
generated O
by O
a O
better O
- O
learned O
teacher O
model O
t. O
The O
loss O
function O
of O
the O
student O
model O
is O
: O
LKD(Œ∏s;D O
, O
Œ∏t O
) O
= O
‚àí/summationdisplay O
( O
x O
, O
y)‚ààDK O
/ O
summationdisplay O
k=1q(y O
= O
k|x;Œ∏t O
) O
logp(y O
= O
k|x;Œ∏s O
) O
wherep(¬∑)andq(¬∑)represent O
the O
probability O
distribution O
over O
Kcategories O
, O
predicted O
by O
the O
studentsand O
the O
teacher O
t O
, O
respectively O
. O
Notice O
that O
only O
the O
student O
model O
parameters O
Œ∏sare O
updated O
during O
knowledge O
distillation O
. O
As O
shown O
in O
Figure O
1(b O
) O
, O
we O
Ô¨Årst O
use O
the O
Ô¨Åne O
- O
tuned O
monolingual O
pretrained O
model O
as O
a O
teacher O
, O
which O
is O
learned O
by O
minimizing O
LCE(Œ∏t;D O
) O
. O
Then O
we O
perform O
knowledge O
distillation O
for O
the O
student O
model O
withLKD(Œ∏s;DC O
, O
Œ∏t)as O
the O
loss O
function O
, O
where O
DCis O
the O
concatenation O
of O
training O
dataset O
and O
the O
unlabeled O
dataset O
in O
the O
source O
language O
. O
We O
denote O
this O
implementation O
as O
M O
ONOX O
- O
K O
D. O
3.2 O
Pseudo O
- O
Label O
In O
addition O
to O
knowledge O
distillation O
, O
we O
also O
consider O
implementing O
M O
ONOX O
by O
training O
the O
student O
multilingual O
model O
with O
pseudo O
- O
label O
( O
Lee O
, O
2013 O
) O
. O
SpeciÔ¨Åcally O
, O
after O
Ô¨Åne O
- O
tuning O
the O
monolingual O
pretrained O
model O
on O
the O
training O
data O
as O
teacher O
, O
we O
apply O
the O
teacher O
model O
on O
the O
unlabeled O
data O
in O
the O
source O
language O
to O
generate O
pseudo O
labels O
. O
Next O
, O
we O
Ô¨Ålter O
the O
pseudo O
labels O
by O
a O
prediction O
conÔ¨Ådence O
threshold O
, O
and O
only O
keep O
the O
examples O
with O
higher O
conÔ¨Ådence O
scores O
. O
Notice O
that O
the O
pseudo O
training O
data O
are O
assigned O
with O
hard O
labels O
. O
Finally O
, O
we O
concatenate O
the O
original O
training O
data O
and O
the O
pseudo O
data O
as O
the O
Ô¨Ånal O
training O
set O
for O
the O
student O
model O
. O
We O
denote O
this O
implementation O
as O
M O
ONOX O
- O
P O
L. O
4 O
Experiments O
4.1 O
Experimental O
Setup O
In O
the O
following O
experiments O
, O
we O
consider O
the O
zero O
- O
shot O
cross O
- O
lingual O
setting O
, O
where O
models O
are O
trained O
with O
English O
data O
and O
directly O
evaluated O
on O
all O
target O
languages O
. O
Datasets O
We O
conduct O
experiments O
on O
two O
widely O
used O
datasets O
for O
cross O
- O
lingual O
evaluation O
: O
( O
1 O
) O
Cross O
- O
Lingual O
Sentiment O
( O
CLS O
) O
dataset O
( O
Prettenhofer O
and O
Stein O
, O
2010 O
) O
, O
containing O
Amazon13reviews O
in O
three O
domains O
and O
four O
languages O
; O
( O
2 O
) O
Cross O
- O
Lingual O
NLI O
( O
XNLI O
) O
dataset O
( O
Conneau O
et O
al O
. O
, O
2018 O
) O
, O
containing O
development O
and O
test O
sets O
in O
15 O
languages O
and O
a O
training O
set O
in O
English O
for O
the O
natural O
language O
inference O
task O
. O
Pretrained O
Language O
Models O
We O
use O
multilingual O
BERT O
BASE1for O
cross O
- O
lingual O
transfer O
. O
For O
monolingual O
pretrained O
language O
model O
, O
the O
English O
- O
version O
RoBERTa O
LARGE2is O
employed O
. O
All O
the O
pretrained O
models O
used O
in O
our O
experiments O
are O
cased O
models O
. O
Baselines O
We O
compare O
our O
methods O
( O
M O
ONOXKD O
, O
and O
M O
ONOX O
- O
P O
L O
) O
with O
the O
following O
models O
: O
‚Ä¢MBERT O
: O
directly O
Ô¨Åne O
- O
tuning O
the O
multilingual O
BERT O
BASE O
with O
English O
training O
data O
. O
‚Ä¢MBERT O
- O
ST O
: O
Ô¨Åne O
- O
tuning O
the O
multilingual O
BERT O
BASE O
by O
self O
- O
training O
, O
i.e. O
, O
alternately O
Ô¨Åne O
- O
tuning O
mBERT O
and O
updating O
the O
training O
data O
by O
labeling O
English O
unlabeled O
examples O
. O
4.2 O
ConÔ¨Åguration O
For O
the O
CLS O
dataset O
, O
we O
randomly O
select O
20 O
% O
examples O
from O
training O
data O
as O
the O
development O
set O
and O
use O
the O
remaining O
examples O
as O
the O
training O
set O
. O
For O
XNLI O
, O
we O
randomly O
sample O
20 O
% O
examples O
from O
training O
data O
as O
the O
training O
set O
, O
and O
regard O
the O
other O
examples O
as O
the O
unlabeled O
set O
. O
We O
use O
the O
vocabularies O
provided O
by O
the O
pretrained O
models O
, O
which O
are O
extracted O
by O
Byte O
- O
Pair O
Encoding O
( O
Sennrich O
et O
al O
. O
, O
2016 O
) O
. O
The O
input O
sentences O
are O
truncated O
to O
256 O
tokens O
. O
For O
both O
datasets O
, O
we O
use O
Adam O
optimizer O
with O
a O
learning O
rate O
of O
5√ó10‚àí6 O
, O
and O
a O
batch O
size O
of O
8 O
. O
We O
train O
models O
with O
epoch O
size O
of O
200 O
and O
2,500 O
steps O
for O
CLS O
and O
XNLI O
, O
respectively O
. O
For O
M O
ONOX O
- O
K O
D O
, O
the O
softmax O
temperature O
of O
knowledge O
distillation O
is O
set O
to O
0.1 O
. O
For O
M O
ONOX O
- O
P O
L O
, O
the O
conÔ¨Ådence O
threshold O
is O
set O
to O
zero O
, O
which O
means O
all O
of O
the O
generated O
pseudo O
labels O
are O
used O
as O
training O
data O
. O
4.3 O
Results O
and O
Discussion O
Preliminary O
Experiments O
To O
see O
how O
much O
monolingual O
pretrained O
models O
is O
better O
than O
multilingual O
pretrained O
models O
, O
we O
Ô¨Ånetune O
several O
different O
pretrained O
language O
models O
on O
the O
two O
datasets O
under O
the O
aforementioned O
conÔ¨Åguration O
, O
1https://github.com/google-research/ O
bert O
/ O
blob O
/ O
master O
/ O
multilingual.md O
2https://github.com/pytorch/fairseq/ O
tree O
/ O
master O
/ O
examples O
/ O
robertaParameters O
CLS O
XNLI O
Multilingual O
Pretrained O
Models O
MBERT O
110 O
M O
86.37 O
77.07 O
Monolingual O
Pretrained O
Models O
BERT O
BASE O
110 O
M O
90.10 O
80.46 O
RoBERTa O
BASE O
125 O
M O
93.82 O
85.09 O
RoBERTa O
LARGE O
355 O
M O
95.77 O
89.24 O
Table O
1 O
: O
Preliminary O
experiments O
results O
. O
Models O
are O
Ô¨Ånetuned O
with O
English O
training O
data O
of O
CLS O
and O
XNLI O
under O
the O
conÔ¨Åguration O
( O
see O
Section O
3.2 O
) O
, O
and O
only O
evaluated O
in O
English O
. O
The O
results O
on O
CLS O
are O
averaged O
over O
three O
domains O
. O
and O
only O
evaluate O
them O
in O
English O
. O
As O
shown O
in O
Table O
1 O
, O
the O
gap O
between O
multilingual O
and O
monolingual O
pretrained O
models O
is O
large O
, O
even O
when O
using O
the O
same O
size O
of O
parameters O
. O
It O
is O
not O
hard O
to O
explain O
because O
MBERT O
is O
trained O
in O
104 O
languages O
, O
where O
different O
languages O
tend O
to O
confuse O
each O
other O
. O
Sentiment O
ClassiÔ¨Åcation O
We O
evaluate O
our O
method O
on O
the O
zero O
- O
shot O
cross O
- O
lingual O
sentiment O
classiÔ¨Åcation O
task O
. O
The O
goal O
of O
sentiment O
classiÔ¨Åcation O
is O
to O
classify O
input O
sentences O
to O
positive O
or O
negative O
sentiments O
. O
In O
Table O
2 O
we O
compare O
the O
results O
of O
our O
methods O
with O
baselines O
on O
CLS O
. O
It O
can O
be O
observed O
that O
our O
M O
ONOX O
method O
outperforms O
baselines O
in O
all O
evaluated O
languages O
and O
domains O
, O
providing O
4.91 O
% O
improvement O
of O
averaged O
accuracy O
to O
the O
original O
multilingual O
BERT O
Ô¨Åne O
- O
tuning O
method O
. O
Notice O
that O
MBERTSTis O
trained O
under O
the O
same O
condition O
with O
our O
method O
, O
i.e. O
, O
using O
the O
same O
labeled O
and O
unlabeled O
data O
as O
ours O
. O
However O
, O
we O
only O
observe O
a O
slight O
improvement O
over O
MBERT O
, O
which O
demonstrates O
that O
the O
performance O
improvement O
of O
M O
ONOX O
mainly O
beneÔ¨Åts O
from O
its O
end O
task O
knowledge O
transfer O
rather O
than O
the O
unlabeled O
data O
. O
Natural O
Language O
Inference O
We O
also O
evaluate O
our O
method O
on O
the O
zero O
- O
shot O
cross O
- O
lingual O
NLI O
task O
, O
which O
is O
more O
challenging O
than O
sentiment O
classiÔ¨Åcation O
. O
The O
goal O
of O
NLI O
is O
to O
identify O
the O
relationship O
of O
a O
pair O
of O
input O
sentences O
, O
including O
a O
premise O
and O
a O
hypothesis O
with O
an O
entailment O
, O
contradiction O
, O
orneutral O
relationship O
between O
them O
. O
As O
shown O
in O
Table O
3 O
, O
we O
present O
the O
evaluation O
results O
on O
XNLI O
. O
Unsurprisingly O
, O
both O
M O
ONOXPLand O
M O
ONOX O
- O
K O
Dperform O
better O
than O
baseline O
methods O
, O
showing O
that O
our O
method O
success-14en O
de O
fr O
ja O
Books O
DVD O
Music O
Books O
DVD O
Music O
Books O
DVD O
Music O
Books O
DVD O
Music O
avg O
MBERT O
87.75 O
86.60 O
84.75 O
79.55 O
75.90 O
77.05 O
81.45 O
80.35 O
80.35 O
75.15 O
76.90 O
75.90 O
80.14 O
MBERT O
- O
ST O
88.20 O
85.50 O
88.00 O
79.65 O
76.70 O
80.00 O
84.85 O
83.25 O
80.55 O
74.60 O
75.80 O
76.90 O
81.17 O
MONOX O
- O
P O
L94.00 O
92.75 O
91.80 O
83.20 O
79.25 O
82.95 O
86.00 O
84.95 O
84.55 O
78.85 O
80.00 O
79.35 O
84.80 O
MONOX O
- O
K O
D93.90 O
91.40 O
92.25 O
84.20 O
81.50 O
83.65 O
85.40 O
85.90 O
83.95 O
78.95 O
79.15 O
80.30 O
85.05 O
Table O
2 O
: O
Evaluation O
results O
of O
zero O
- O
shot O
cross O
- O
lingual O
sentiment O
classiÔ¨Åcation O
on O
the O
CLS O
dataset O
. O
ar O
bg O
de O
el O
en O
es O
fr O
hi O
ru O
sw O
th O
tr O
ur O
vi O
zh O
avg O
MBERT O
61.2 O
67.4 O
65.8 O
61.6 O
77.1 O
70.7 O
68.6 O
53.4 O
67.0 O
50.6 O
44.6 O
56.3 O
57.8 O
43.6 O
67.8 O
60.9 O
MBERT O
- O
ST O
60.9 O
67.6 O
65.4 O
61.0 O
77.6 O
70.4 O
68.9 O
53.1 O
65.9 O
50.6 O
41.8 O
55.2 O
56.8 O
43.6 O
67.9 O
60.5 O
MONOX O
- O
P O
L63.5 O
70.1 O
69.8 O
61.7 O
80.9 O
74.1 O
72.1 O
52.5 O
68.4 O
51.2 O
42.3 O
57.9 O
58.0 O
44.0 O
70.2 O
62.5 O
MONOX O
- O
K O
D62.2 O
69.3 O
69.3 O
62.1 O
79.6 O
72.9 O
72.0 O
52.8 O
68.6 O
52.3 O
41.7 O
57.9 O
58.5 O
45.9 O
70.8 O
62.4 O
Table O
3 O
: O
Evaluation O
results O
of O
zero O
- O
shot O
cross O
- O
lingual O
NLI O
on O
the O
XNLI O
dataset O
. O
Note O
that O
20 O
% O
of O
the O
original O
training O
data O
are O
used O
as O
training O
set O
, O
and O
the O
other O
80 O
% O
are O
used O
as O
unlabeled O
set O
. O
101102103104105 O
Training O
Data O
Size354045505560Averaged O
Accuracy O
mBert O
MonoX O
- O
PL O
MonoX O
- O
KD O
Figure O
2 O
: O
Averaged O
accuracy O
scores O
on O
zero O
- O
shot O
XNLI O
with O
different O
training O
data O
sizes O
. O
( O
20 O
% O
and O
80 O
% O
of O
the O
training O
data O
are O
regraded O
training O
and O
unlabeled O
set O
. O
) O
fully O
helps O
the O
multilingual O
pretrained O
model O
gain O
end O
task O
knowledge O
from O
the O
monolingual O
pretrained O
model O
for O
cross O
- O
lingual O
classiÔ¨Åcation O
. O
It O
is O
also O
worth O
mentioning O
that O
the O
performance O
of O
MBERT O
- O
STis O
similar O
to O
MBERT O
. O
We O
believe O
the O
reason O
is O
that O
XNLI O
has O
more O
training O
data O
than O
CLS O
, O
which O
wakens O
the O
impact O
of O
self O
- O
training O
. O
Effects O
of O
Training O
Data O
Size O
We O
conduct O
a O
study O
on O
how O
much O
multilingual O
pretrained O
model O
can O
learn O
from O
monolingual O
pretrained O
model O
for O
different O
training O
data O
size O
. O
We O
cut O
the O
training O
data O
to O
10 O
, O
100 O
, O
1 O
K O
, O
10 O
K O
and O
78 O
K O
( O
full O
training O
data O
in O
our O
setting O
) O
examples O
, O
and O
keep O
other O
hyper O
- O
parameters O
Ô¨Åxed O
. O
In O
Figure O
2 O
, O
we O
show O
the O
averaged O
accuracy O
scores O
for O
zero O
- O
shot O
XNLI O
with O
different O
training O
data O
sizes O
. O
We O
observe O
that O
MONOX O
outperforms O
MBERTon O
all O
data O
sizes O
except O
the O
10 O
- O
example O
setting O
. O
When O
the O
training O
data O
is O
relatively O
small O
( O
‚â§104 O
) O
, O
our O
method O
shows O
103 O
102 O
101 O
100101102 O
Distillation O
Temperature61.562.062.5Averaged O
Accuracy O
Figure O
3 O
: O
Averaged O
accuracy O
scores O
on O
the O
development O
set O
for O
zero O
- O
shot O
XNLI O
with O
different O
softmax O
temperatures O
of O
M O
ONOX O
- O
K O
D. O
a O
great O
improvement O
. O
Effects O
of O
Distillation O
Temperature O
Figure O
3 O
presents O
XNLI O
averaged O
accuracy O
scores O
of O
MONOX O
- O
K O
Dwith O
different O
softmax O
temperatures O
in O
knowledge O
distillation O
. O
Even O
though O
the O
temperature O
varies O
from O
10‚àí3to102 O
, O
all O
of O
the O
results O
are O
higher O
than O
baseline O
scores O
, O
which O
indicates O
MONOX O
- O
K O
Dis O
nonsensitive O
to O
the O
temperature O
. O
When O
the O
temperature O
is O
set O
to O
10‚àí1 O
, O
we O
observe O
the O
best O
results O
on O
the O
development O
set O
. O
Therefore O
we O
set O
temperature O
as O
0.1 O
in O
other O
experiments O
. O
5 O
Conclusion O
In O
this O
work O
, O
we O
investigated O
whether O
a O
monolingual O
pretrained O
model O
can O
help O
cross O
- O
lingual O
classiÔ¨Åcation O
. O
Our O
results O
have O
shown O
that O
, O
with O
a O
RoBERTa O
model O
pretrained O
in O
English O
, O
we O
can O
boost O
the O
classiÔ¨Åcation O
performance O
of O
a O
pretrained O
multilingual O
BERT O
in O
other O
languages O
. O
For O
future O
work O
, O
we O
will O
explore O
whether O
mono-15lingual O
pretrained O
models O
can O
help O
other O
crosslingual O
NLP O
tasks O
, O
such O
as O
natural O
language O
generation O
( O
Chi O
et O
al O
. O
, O
2020a O
) O
. O
Acknowledgements O
Prof. O
Heyan O
Huang O
is O
the O
corresponding O
author O
. O
The O
work O
is O
supported O
by O
National O
Key O
R&D O
Plan O
( O
No O
. O
2016QY03D0602 O
) O
, O
NSFC O
( O
No O
. O
U19B2020 O
, O
61772076 O
, O
61751201 O
and O
61602197 O
) O
and O
NSFB O
( O
No O
. O
Z181100008918002 O
) O
. O
Abstract O
Aspect O
- O
level O
sentiment O
analysis(ASC O
) O
predictseach O
speciÔ¨Åc O
aspect O
term O
‚Äôs O
sentiment O
polar O
- O
ity O
in O
a O
given O
text O
or O
review O
. O
Recent O
stud O
- O
ies O
used O
attention O
- O
based O
methods O
that O
can O
ef O
- O
fectively O
improve O
the O
performance O
of O
aspect O
- O
level O
sentiment O
analysis O
. O
These O
methods O
ig O
- O
nored O
the O
syntactic O
relationship O
between O
the O
as O
- O
pect O
and O
its O
corresponding O
context O
words O
, O
lead O
- O
ing O
the O
model O
to O
focus O
on O
syntactically O
unre O
- O
lated O
words O
mistakenly O
. O
One O
proposed O
solu O
- O
tion O
, O
the O
graph O
convolutional O
network O
( O
GCN),cannot O
completely O
avoid O
the O
problem O
. O
While O
itdoes O
incorporate O
useful O
information O
about O
syn O
- O
tax O
, O
it O
assigns O
equal O
weight O
to O
all O
the O
edgesbetween O
connected O
words O
. O
It O
may O
still O
incor O
- O
rectly O
associate O
unrelated O
words O
to O
the O
targetaspect O
through O
the O
iterations O
of O
graph O
convo O
- O
lutional O
propagation O
. O
In O
this O
study O
, O
a O
graphattention O
network O
with O
memory O
fusion O
is O
pro O
- O
posed O
to O
extend O
GCN O
‚Äôs O
idea O
by O
assigning O
dif O
- O
ferent O
weights O
to O
edges O
. O
Syntactic O
constraintscan O
be O
imposed O
to O
block O
the O
graph O
convolu O
- O
tional O
propagation O
of O
unrelated O
words O
. O
A O
con O
- O
volutional O
layer O
and O
a O
memory O
fusion O
were O
ap O
- O
plied O
to O
learn O
and O
exploit O
multiword O
relationsand O
draw O
different O
weights O
of O
words O
to O
im O
- O
prove O
performance O
further O
. O
Experimental O
re O
- O
sults O
on O
Ô¨Åve O
datasets O
show O
that O
the O
proposedmethod O
yields O
better O
performance O
than O
exist O
- O
ing O
methods O
. O
The O
code O
of O
this O
paper O
is O
avail O
- O
abled O
at O
https://github.com/YuanLi95/GATT-For-Aspect O
. O
1 O
Introduction O
Aspect O
- O
level O
sentiment O
classiÔ¨Åcation O
is O
a O
Ô¨Ånegrained O
subtask O
in O
sentiment O
analysis O
( O
Wang O
et O
al O
. O
, O
2019;Peng O
et O
al O
. O
,2020 O
) O
. O
Given O
a O
sentence O
andan O
aspect O
that O
appears O
in O
the O
sentence O
, O
ASC O
aimsto O
determine O
the O
sentiment O
polarity O
of O
that O
aspect(e.g O
. O
, O
negative O
, O
neutral O
, O
or O
positive O
) O
. O
For O
example O
, O
a O
review O
of O
a O
restaurant O
‚Äú O
The O
price O
is O
reasonable O
although O
the O
service O
is O
poor O
. O
‚Äù O
expresses O
apositive O
sentiment O
for O
thepriceaspect O
, O
but O
also O
conveys O
anegativesentiment O
for O
theserviceaspect O
, O
as O
shown O
in O
Figure1 O
. O
Such O
a O
technique O
is O
widely O
used O
toanalyze O
online O
posts O
reviews O
, O
mainly O
from O
Ama O
- O
zon O
reviews O
or O
Twitter O
, O
to O
help O
raise O
the O
ability O
to O
understand O
consumer O
needs O
or O
experiences O
with O
a O
product O
, O
guiding O
a O
manufacturer O
towards O
productimprovement O
. O
Aspect O
- O
level O
sentiment O
classiÔ¨Åcation O
is O
much O
more O
complicated O
than O
sentence O
- O
level O
sentiment O
classiÔ¨Åcation O
. O
ASC O
task O
is O
necessary O
to O
identify O
the O
parts O
of O
the O
sentence O
that O
describe O
the O
correspondence O
between O
multiple O
aspects O
. O
Traditional O
methods O
mostly O
use O
shallow O
machine O
learning O
models O
with O
hand O
- O
crafted O
features O
to O
build O
sentiment O
classiÔ¨Åers O
for O
the O
ASC O
task O
( O
Jiang O
et O
al O
. O
,2011;Wagner O
et O
al O
. O
,2014).However O
, O
the O
process O
for O
manual O
feature O
engineering O
is O
time O
- O
consuming O
and O
labor O
- O
intensive O
as O
well O
as O
limited O
in O
classiÔ¨Åcation O
performance O
Recently O
, O
with O
the O
development O
of O
deep O
learning O
techniques O
, O
various O
attention O
- O
based O
neural O
models O
have O
achieved O
remarkable O
success O
in O
ASC O
. O
( O
Wang O
et O
al O
. O
,2016;Ma O
et O
al O
. O
,2017;Chen O
et O
al O
. O
,2017;Gu O
et O
al O
. O
,2018;Tang O
et O
al O
. O
,2019 O
) O
. O
However O
, O
these O
methods O
ignored O
the O
syntactic O
dependence O
betweencontext O
words O
and O
aspects O
in O
a O
sentence O
. O
As O
a O
result O
, O
the O
current O
attention O
model O
may O
inappropriately O
focus O
on O
syntactically O
unrelated O
context O
words O
. O
As O
shown O
in O
Figure1 O
, O
when O
predicting O
the O
emotional O
polarity O
ofprice O
, O
the O
attention O
mechanism O
may O
focus O
on O
the O
wordpoor O
, O
which O
is O
not O
related O
to O
its O
syntax O
. O
To O
address O
this O
issue O
, O
Zhang O
et O
al.(2019 O
) O
built O
a O
graph O
convolutional O
network O
( O
GCN O
) O
over O
a O
dependency O
tree O
to O
exploit O
syntactical O
information O
andword O
dependencies O
. O
However O
, O
the O
model O
assigns O
equal O
weight O
to O
the O
edges O
connected O
between O
words O
so O
that O
words O
may O
mistakenly O
associate O
syntactically O
unrelated O
words O
to O
the O
target O
aspect O
through O
iterations O
of O
graph O
convolutional O
propagation O
. O
As O
indicated O
in O
Figure1 O
, O
after O
three O
iterations O
, O
bothreasonable(yellow O
lines O
) O
andpoor(red O
lines O
) O
may O
be27Theprice O
is O
resonable O
although O
' O
( O
71281 O
$ O
8 O
; O
$ O
' O
- O
6&21- O
' O
( O
7 O
1281the O
serviceispoorDFRPSGHW O
QVXEM O
DFRPSDGYFO O
$ O
8 O
; O
$ O
' O
Figure O
1 O
: O
Grammatical O
Relational O
Examples O
. O
identiÔ¨Åed O
as O
descriptors O
of O
the O
aspectprice O
, O
which O
is O
incorrect O
. O
As O
a O
result O
, O
the O
model O
will O
falsely O
classify O
the O
aspectpriceas O
a O
negative O
sentiment O
. O
In O
this O
paper O
, O
a O
graph O
attention O
model O
with O
memory O
fusion O
was O
proposed O
. O
This O
model O
extends O
theidea O
of O
graph O
convolutional O
networks O
in O
two O
as O
- O
pects O
. O
First O
, O
the O
graph O
attention O
mechanism O
is O
applied O
to O
assign O
different O
weights O
to O
the O
edge O
, O
so O
the O
syntactical O
constraints O
can O
be O
imposed O
to O
block O
thepropagation O
of O
syntactically O
unrelated O
words O
to O
the O
target O
aspect O
. O
Second O
, O
a O
convolutional O
operation O
is O
applied O
to O
extract O
local O
information O
to O
exploit O
multiword O
relations O
, O
such O
asnot O
goodandfar O
from O
perfect O
, O
which O
can O
further O
improve O
the O
performance O
. O
To O
integrate O
all O
features O
, O
a O
memory O
fusion O
layer O
, O
which O
is O
similar O
to O
a O
memory O
network O
, O
is O
appliedto O
draw O
different O
weights O
for O
words O
according O
to O
their O
contribution O
to O
the O
Ô¨Ånal O
classiÔ¨Åcation O
. O
Experiments O
are O
conducted O
on O
Ô¨Åve O
datasets O
demonstrate O
how O
the O
proposed O
model O
outperforms O
baselines O
for O
aspect O
- O
level O
sentiment O
analysis O
. O
The O
remainder O
of O
this O
paper O
is O
organized O
asfollows O
. O
Section O
2 O
brieÔ¨Çy O
reviews O
the O
existing O
works O
for O
aspect O
- O
level O
sentiment O
analysis O
. O
Section O
3 O
presents O
a O
detailed O
description O
of O
the O
proposedgraph O
attention O
model O
with O
memory O
fusion O
. O
Section O
4 O
summarizes O
the O
implementation O
details O
and O
experimental O
results O
. O
The O
conclusions O
of O
this O
study O
are O
Ô¨Ånally O
drawn O
in O
Section O
5 O
. O
2 O
Related O
Works O
Aspect O
- O
level O
sentiment O
classiÔ¨Åcation O
is O
an O
impor O
- O
tant O
branch O
of O
sentiment O
classiÔ¨Åcation O
, O
aiming O
toidentify O
the O
sentiment O
polarity O
of O
an O
aspect O
targetin O
a O
sentence O
. O
ASC O
methods O
can O
be O
divided O
into O
traditional O
and O
deep O
learning O
methods O
. O
Traditional O
methods O
usually O
used O
feature O
- O
based O
machine O
learning O
algorithms O
, O
such O
as O
a O
feature O
- O
based O
supportvector O
machine O
( O
SVM O
) O
( O
Kiritchenko O
et O
al O
. O
,2014).Due O
to O
the O
inefÔ¨Åciency O
of O
manually O
constructed O
features O
, O
several O
neural O
network O
methods O
have O
been O
proposed O
for O
aspect O
- O
level O
sentiment O
analysis O
( O
Jiang O
et O
al O
. O
,2011 O
) O
, O
which O
are O
mainly O
based O
on O
long O
shortterm O
memory O
( O
LSTM O
) O
( O
Tang O
et O
al O
. O
,2016a;Wanget O
al O
. O
,2020).Tang O
et O
al.(2016b O
) O
indicated O
thatthe O
ASC O
task O
‚Äôs O
challenge O
is O
to O
identify O
better O
thesemantic O
correlation O
between O
context O
words O
andaspect O
words O
so O
that O
several O
recent O
works O
widely O
applied O
an O
attention O
mechanism O
and O
achieved O
good O
performance O
. O
Ma O
et O
al.(2017 O
) O
used O
an O
interac O
- O
tive O
attention O
network O
to O
obtain O
a O
two O
- O
way O
atten O
- O
tion O
representation O
of O
context O
words O
and O
aspect O
words O
. O
Huang O
et O
al.(2018 O
) O
proposed O
a O
joint O
model O
based O
on O
an O
attention O
mechanism O
to O
model O
aspects O
and O
sentences O
. O
Tang O
et O
al.(2019 O
) O
proposed O
a O
self O
- O
supervised O
attention O
model O
that O
can O
dynamically O
update O
attention O
weights O
. O
Yao O
et O
al.(2019 O
) O
introduced O
the O
graph O
convo O
- O
lutional O
network O
into O
the O
sentiment O
classiÔ¨Åcation O
task O
and O
achieved O
good O
performance O
. O
Subsequently O
, O
Zhang O
et O
al.(2019 O
) O
proposed O
to O
use O
GCN O
on O
the O
dependency O
tree O
of O
a O
sentence O
to O
exploit O
the O
longrange O
syntactic O
information O
for O
the O
ASC O
task O
. O
3 O
Graph O
Attention O
Network O
withMemory O
Fusion O
The O
proposed O
graph O
attention O
network O
with O
memory O
fusion O
is O
mainly O
composed O
of O
the O
followingfour O
parts O
: O
a O
context O
encoder O
, O
a O
graph O
attentionlayer O
, O
a O
convolutional O
layer O
and O
a O
memory O
fusionlayer O
, O
as O
shown O
in O
Figure2.The O
context O
encoderemploys O
a O
vanilla O
bidirectional O
LSTM O
to O
capture O
the O
textual O
features O
. O
It O
contains O
a O
word O
embedding O
layer O
and O
a O
BiLSTM O
layer O
to O
produce O
a O
hidden O
representation O
of O
the O
text O
. O
Taking O
the O
hidden O
representation O
as O
input O
, O
the O
graph O
attention O
layer O
( O
G O
- O
ATT)28ƒÇ O
Word O
EmbeddingsƒÇ O
Context O
Encoder O
ƒÇ O
copyƒÇ O
copyƒÇ O
L O
- O
Layer O
G O
- O
ATTƒÇ O
Aspect- O
specific O
  O
Masking O
ƒÇ O
ƒÇ O
  O
2 O
- O
Layer O
ConvMemory O
FusionConcatenateSoftmax O
Aspect O
- O
specific O
Masking O
Attention O
Vector O
Hidden O
Representation O
Hidden O
Representation O
Figure O
2 O
: O
The O
overall O
architecture O
of O
the O
proposed O
graph O
attention O
network O
with O
memory O
fusion O
. O
is O
trained O
on O
the O
dependency O
tree O
to O
mine O
explicit O
structural O
information O
between O
words O
. O
The O
convolutional O
layer O
was O
used O
to O
extract O
the O
local O
informa O
- O
tion O
around O
the O
sentiment O
word O
, O
which O
can O
dynami O
- O
cally O
deal O
with O
non O
- O
single O
word O
aspects O
such O
as O
not O
goodandfar O
from O
perfect O
, O
instead O
of O
only O
takingthe O
average O
of O
its O
vectors O
. O
To O
merge O
all O
features O
, O
we O
adopt O
a O
memory O
fusion O
layer O
similar O
to O
a O
memory O
network O
( O
Tang O
et O
al O
. O
,2016b O
) O
, O
which O
can O
assign O
different O
weights O
to O
the O
context O
words O
according O
to O
their O
contribution O
to O
the O
Ô¨Ånal O
classiÔ¨Åcation O
. O
The O
detailed O
description O
is O
presented O
as O
follows.3.1 O
Context O
Encoder O
Given O
a O
sentence O
x=[x O
1,x2,¬∑¬∑¬∑,xœÑ+1,¬∑¬∑¬∑,xœÑ+m O
, O
¬∑ O
¬∑ O
¬∑ O
xn]containingnwords O
, O
the O
target O
aspect O
starts O
from O
the O
( O
œÑ+1 O
) O
-th O
word O
with O
a O
length O
ofm O
. O
ABiLSTM O
was O
applied O
as O
context O
encoder O
, O
which O
can O
capture O
long O
- O
distance O
dependencies O
within O
the O
sentence O
. O
We O
average O
the O
hidden O
representation O
ofboth O
the O
forward O
direction O
and O
backward O
direction O
to O
obtain O
the O
contextual O
representation O
, O
deÔ¨Åned O
as,(/vectorh O
Ei,/vectorcEi)=LSTM(x O
i,/vectorhEi‚àí1,/vectorcEi‚àí1)(1 O
) O
( O
‚Üê O
hEi,‚ÜêcEi)=LSTM(x O
i,‚Üê O
hEi+1,‚ÜêcEi+1)(2)h O
i=(‚àí‚Üíhi‚äï‚Üê‚àíhi)/2(3 O
) O
where‚äïis O
an O
element O
- O
wise O
addition O
operator;‚àí‚Üíh O
i‚ààRdh,‚Üê‚àíh O
i‚ààRdhandhi‚ààRdharethe O
forward O
, O
backward O
and O
output O
representa O
- O
tion O
, O
respectively O
; O
anddhis O
the O
dimension O
ofhidden O
state O
. O
Thus O
, O
the O
Ô¨Ånal O
representation O
ofthe O
context O
encoder O
can O
be O
denoted O
as O
HE=[h O
E1,hE2,¬∑¬∑¬∑,hEœÑ+1,¬∑¬∑¬∑,hEœÑ+m,¬∑¬∑¬∑,hEœÑ+m].3.2 O
Graph O
Attention O
Layer O
The O
graph O
attention O
( O
G O
- O
ATT O
) O
layer O
learns O
syntac O
- O
tically O
relevant O
words O
to O
the O
target O
aspect O
on O
the O
dependency O
tree1 O
, O
which O
is O
widely O
used O
in O
several O
NLP O
tasks O
to O
effectively O
identify O
the O
relationships O
and O
roles O
of O
words O
. O
After O
parsing O
the O
given O
sen O
- O
tence O
as O
a O
dependency O
tree O
, O
the O
adjacency O
matrixwas O
built O
from O
the O
tree O
topology O
. O
It O
is O
worth O
not O
- O
ing O
that O
the O
dependency O
tree O
is O
a O
directed O
graph O
. O
Therefore O
, O
the O
graph O
attention O
mechanism O
was O
applied O
with O
consideration O
of O
the O
direction O
, O
but O
themechanism O
could O
be O
adapted O
to O
the O
undirection O
- O
aware O
scenario O
. O
Therefore O
, O
we O
propose O
a O
variant O
on O
dependency O
graphs O
that O
are O
undirectional O
. O
The O
obtained O
hidden O
state O
HE‚ààRn√ódhwas O
fed O
into O
a O
stacked O
G O
- O
ATT O
model O
, O
which O
was O
performed O
in O
a O
multilayer O
fashion O
with O
anLgraph O
attention O
layer O
. O
In O
practice O
, O
the O
representation O
in O
the O
l O
- O
the O
layer O
was O
not O
immediately O
fed O
into O
the O
G O
- O
ATT O
layer O
. O
To O
enhance O
the O
relevance O
of O
the O
context O
words O
to O
the O
corresponding O
aspect O
, O
we O
adopted O
a O
position O
weight O
function O
to O
the O
representation O
of O
word O
iin O
layerl O
, O
1We O
use O
spaCy O
toolkit O
: O
https://spacy.io/.29which O
is O
widely O
used O
in O
previous O
works O
( O
Li O
et O
al O
. O
, O
2018;Zhang O
et O
al O
. O
,2019 O
) O
, O
deÔ¨Åned O
as O
, O
q O
i=‚éß‚é®‚é©1‚àíœÑ+1‚àíi O
n1‚â§i O
< O
œÑ+10œÑ+1‚â§i‚â§œÑ+m1‚àí O
i‚àíœÑ‚àím O
nœÑ+m O
< O
i‚â§n(4)ÀÜh O
li O
= O
qihli(5)whereq O
i‚ààRis O
the O
position O
weight O
to O
wordi O
. O
In O
each O
layer O
, O
an O
attention O
coefÔ¨Åcient O
Œ±lijwas O
applied O
to O
measure O
the O
importance O
between O
word O
iand O
wordj O
, O
deÔ¨Åned O
as O
, O
Œ±li O
, O
j O
= O
exp O
/ O
parenleftBigLeakyReLU(aT[WlŒ±ÀÜhli||WlŒ±ÀÜhlj])/parenrightBig O
/summationtext O
k‚ààN O
iexp O
/ O
parenleftBigLeakyReLU(aT[WlŒ±ÀÜhli||WlŒ±ÀÜhlj])/parenrightBig(6 O
) O
whereNiis O
the O
set O
of O
the O
neighbor O
of O
wordiand O
WlŒ±‚ààRdh√ódhis O
a O
shared O
weight O
matrix O
applied O
toperform O
linear O
transformation O
to O
each O
word O
in O
order O
to O
obtain O
sufÔ¨Åcient O
express O
ability O
of O
high O
- O
levelrepresentation O
. O
||is O
the O
concatenation O
operator O
, O
a‚ààR2dhis O
a O
weight O
vector O
, O
and O
the O
leaky O
rectiÔ¨Åed O
linear O
unit O
( O
LeakyReLU O
) O
is O
the O
non O
- O
linearity O
. O
To O
stabilize O
the O
learning O
process O
of O
the O
graph O
‚Äôs O
attention O
, O
we O
implement O
Kdifferent O
attention O
with O
the O
same O
parameter O
settings O
, O
which O
is O
similar O
tothe O
multi O
- O
head O
attention O
mechanism O
proposed O
by O
Vaswani O
et O
al.(2017 O
) O
. O
Thus O
, O
the O
Ô¨Ånal O
representation O
hl+1iof O
wordiin O
layerl+1 O
can O
be O
obtained O
as O
, O
h O
l+1i= O
ReLU(1KK O
/ O
summationdisplay O
k=1 O
/ O
summationdisplay O
j‚ààNiŒ±l O
, O
ki O
, O
jWlkÀÜhlj)(7 O
) O
whereŒ±l O
, O
ki O
, O
jis O
thek O
- O
th O
attention O
coefÔ¨Åcientscomputed O
by O
Eq O
. O
( O
6 O
) O
, O
Wlkis O
the O
correspondingweight O
matrix O
ofk O
- O
th O
attention O
in O
l O
- O
th O
GAT O
layer O
, O
and O
the O
nonlinear O
function O
is O
ReLU O
. O
The O
Ô¨Ånalrepresentation O
of O
the O
L O
- O
layer O
G O
- O
ATT O
is O
denotedas O
HL=[hL1,hL2,¬∑¬∑¬∑,hLœÑ+1¬∑¬∑¬∑,hLœÑ+m,¬∑¬∑¬∑,hLn O
] O
, O
hLi‚ààRdh.3.3 O
Convolutional O
Layer O
The O
convolutional O
layer O
( O
Conv O
) O
was O
applied O
to O
extract O
local O
n O
- O
gram O
information O
which O
are O
composed O
of O
multiple O
sentiment O
words O
( O
e.g O
, O
not O
goodand O
far O
from O
perfect O
) O
, O
in O
order O
to O
improve O
the O
learning O
ability O
of O
then O
- O
gram O
features O
. O
The O
hidden O
repre O
- O
sentation O
of O
context O
encoder O
HEis O
fed O
into O
two O
convolutional O
layers O
. O
In O
each O
layer O
, O
we O
use O
Fconvolution O
Ô¨Ålters O
to O
learn O
local O
n O
- O
gram O
features O
. O
In O
awindow O
of O
œâwordshi O
: O
i+œâ‚àí1 O
, O
the O
Ô¨Ålter O
f O
- O
th O
generates O
the O
feature O
mapcfias O
follows O
, O
c O
fi= O
ReLU(Wf O
‚ó¶ O
hEi O
: O
i+œâ‚àí1+bf)(8 O
) O
where O
‚ó¶ O
is O
a O
convolutional O
operator O
, O
Wf‚ààRœâ√ódh O
andbf‚ààRdhrespectively O
denote O
the O
weight O
ma O
- O
trix O
and O
bias O
, O
œâis O
the O
length O
of O
the O
Ô¨Ålter O
, O
and O
thenon O
- O
linearity O
is O
ReLU O
. O
By O
concatenating O
all O
fea O
- O
ture O
maps O
, O
the O
representation O
for O
word O
iwill O
be O
hci=[c1i O
, O
c2i,¬∑¬∑¬∑,cfi,¬∑¬∑¬∑,cFi O
] O
. O
To O
ensure O
that O
the O
shape O
of O
the O
output O
is O
consistent O
with O
the O
shape O
of O
the O
input O
in O
the O
convolutional O
layer O
, O
we O
set O
Fto O
dhand O
pad O
each O
sentence O
with O
zero O
vectors O
to O
the O
maximum O
input O
length O
in O
the O
corpora O
. O
Then O
, O
we O
send O
the O
feature O
maps O
to O
the O
second O
convolutional O
layer O
, O
which O
has O
a O
similar O
structure O
, O
to O
obtain O
theÔ¨Ånal O
representation O
of O
convolutional O
layer O
HC=[h O
C1,hC2,¬∑¬∑¬∑,hCœÑ+1,¬∑¬∑¬∑,hCœÑ+m,¬∑¬∑¬∑hCn],hCi‚ààdh O
. O
3.4 O
Aspect O
- O
SpeciÔ¨Åc O
Masking O
The O
aspect O
- O
speciÔ¨Åc O
masking O
layer O
aims O
to O
learn O
aspect O
- O
speciÔ¨Åc O
content O
for O
memory O
fusion O
and O
the O
Ô¨Ånal O
classiÔ¨Åcation O
. O
Therefore O
, O
we O
mask O
out O
thehidden O
state O
vectors O
of O
the O
input O
from O
the O
G O
- O
ATTand O
Conv O
layer O
, O
i.e. O
, O
HLandHC O
. O
Formally O
, O
we O
set O
all O
the O
vectors O
of O
non O
- O
aspect O
words O
to O
zero O
and O
leave O
the O
vectors O
of O
the O
aspect O
words O
unchanged O
, O
deÔ¨Åned O
as O
, O
h O
i=/braceleftbigg01‚â§i O
< O
œÑ+1,œÑ+m O
< O
i‚â§nh O
iœÑ+1‚â§i‚â§œÑ+m(9 O
) O
The O
output O
vector O
of O
the O
G O
- O
ATT O
layer O
af O
- O
ter O
the O
mask O
operation O
is O
HLmasked O
=[ O
0,¬∑¬∑¬∑,h O
LœÑ+1,¬∑¬∑¬∑,hLœÑ+m,¬∑¬∑¬∑,0 O
] O
, O
which O
hasperceived O
contexts O
around O
the O
aspect O
so O
bothsyntactical O
dependencies O
and O
the O
long O
- O
rangemultiword O
relations O
can O
be O
considered O
. O
Sim O
- O
ilarly O
, O
the O
output O
representation O
of O
the O
con O
- O
volutional O
layer O
after O
the O
mask O
operation O
is O
HCmask= O
[ O
0,¬∑¬∑¬∑,hCœÑ+1,¬∑¬∑¬∑,hCœÑ+m,¬∑¬∑¬∑,0].3.5 O
Memory O
Fusion O
Memory O
fusion O
aims O
to O
learn O
the O
Ô¨Ånal O
representa O
- O
tion O
related O
to O
the O
meaning O
of O
aspect O
words O
. O
Theidea O
is O
to O
retrieve O
signiÔ¨Åcant O
features O
that O
are O
se O
- O
mantically O
relevant O
to O
the O
aspect O
words O
from O
thehidden O
representation O
by O
aligning O
the O
vectors O
of O
both O
G O
- O
ATT O
and O
Conv O
to O
the O
hidden O
vectors O
. O
Formally O
, O
we O
calculate O
the O
attention O
score O
for O
the O
i O
- O
th O
word O
inHEandj O
- O
th O
word O
inHL O
, O
deÔ¨Åned O
as,30Dataset O
Positive O
Neutral O
Negative O
Total O
Max O
Length O
Mean O
Length O
TwitterTrain O
1561 O
3127 O
1560 O
6248 O
43 O
19 O
Test O
173 O
346 O
173 O
692 O
39 O
19 O
Lap14Train O
994 O
464 O
870 O
2328 O
81 O
21 O
Test O
341 O
169 O
128 O
638 O
70 O
17 O
Rest14Train O
2164 O
637 O
807 O
3608 O
77 O
18 O
Test O
728 O
196 O
196 O
1120 O
68 O
17 O
Rest15Train O
912 O
36 O
256 O
1204 O
72 O
15 O
Test O
326 O
34 O
182 O
542 O
61 O
17 O
Rest16Train O
1240 O
69 O
439 O
1748 O
72 O
16 O
Test O
469 O
30 O
117 O
616 O
77 O
18 O
Table O
1 O
: O
The O
summary O
of O
datasets O
ei O
= O
n O
/ O
summationdisplay O
j=1hLiTWlhEj O
= O
œÑ+m O
/ O
summationdisplay O
j O
= O
œÑ+1hLiTWlhEj(10 O
) O
where O
Wl‚ààRdh√ódhis O
a O
bilinear O
term O
that O
inter O
- O
acts O
with O
these O
two O
vectors O
and O
captures O
the O
speciÔ¨Åc O
semantic O
relations O
. O
According O
toSocher O
et O
al O
. O
( O
2013 O
) O
, O
such O
a O
tensor O
operator O
can O
be O
used O
to O
model O
complicated O
compositions O
between O
those O
vectors O
. O
Therefore O
, O
the O
attention O
score O
weight O
and O
Ô¨Ånal O
representation O
of O
G O
- O
ATT O
are O
computed O
as O
, O
Œ± O
i O
= O
exp(ei)/summationtextnk=1exp(ek)(11)s O
g O
= O
n O
/ O
summationdisplay O
i=1Œ±ihEi(12 O
) O
Accordingly O
, O
the O
Ô¨Ånal O
representation O
of O
the O
Conv O
layer O
is O
computed O
as O
, O
r O
i O
= O
n O
/ O
summationdisplay O
j=1hCiTWchEj O
= O
œÑ+m O
/ O
summationdisplay O
j O
= O
œÑ+1hCiTWchEj(13)Œ≤ O
i O
= O
exp(ri)/summationtextnk=1exp(rk)(14)s O
c O
= O
n O
/ O
summationdisplay O
i=1Œ≤ihEi(15 O
) O
3.6 O
Sentiment O
ClassiÔ¨Åcation O
After O
obtaining O
representation O
sgandsc O
, O
they O
are O
fed O
into O
a O
fully O
connected O
layer O
and O
then O
a O
softmax O
layer O
to O
generate O
a O
probability O
distribution O
over O
the O
classes,ÀÜy= O
softmax(W O
s[sg||sc]+bs)(16 O
) O
where O
Wsandbsrespectively O
denote O
the O
weights O
and O
bias O
in O
the O
output O
layer O
. O
Thus O
, O
given O
a O
trainingset O
/ O
braceleftbigx(t),y(t)/bracerightbigTt=1=1 O
, O
where O
x(t)is O
a O
training O
sample O
, O
y(t)is O
the O
corresponding O
actual O
sentiment O
label O
, O
and O
Tis O
the O
number O
of O
training O
samples O
inthe O
corpus O
. O
The O
training O
goal O
is O
to O
minimize O
the O
cross O
- O
entropyL O
cls(Œ∏)deÔ¨Åned O
as O
, O
Lcls(Œ∏)=‚àí1TT O
/ O
summationdisplay O
t=1logp(ÀÜy(t)|x(t);Œ∏)+Œª O
/ O
bardblŒ∏ O
/ O
bardbl22(17 O
) O
whereŒ∏denotes O
all O
trainable O
parameters O
. O
To O
avoid O
overÔ¨Åtting O
, O
an O
L2 O
- O
regularization O
Œª O
/ O
bardblŒ∏ O
/ O
bardbl22is O
also O
introduced O
to O
the O
loss O
function O
in O
the O
training O
phase O
, O
whereŒªis O
the O
decay O
factor O
. O
4 O
Experimental O
Results O
This O
section O
conducts O
comparative O
experiments O
on O
Ô¨Åve O
corpora O
against O
several O
previously O
proposedmethods O
for O
aspect O
- O
level O
sentiment O
analysis O
. O
The O
experimental O
setting O
and O
empirical O
results O
are O
then O
presented O
in O
detail.4.1 O
Dataset O
To O
compare O
the O
proposed O
model O
with O
other O
aspectlevel O
sentiment O
analysis O
models O
, O
we O
conduct O
ex O
- O
periments O
on O
the O
following O
Ô¨Åve O
commonly O
used O
datasets O
: O
Twitter O
was O
originally O
proposed O
byDong O
et O
al.(2014 O
) O
and O
contains O
several O
Twitter O
posts O
, O
while O
the O
other O
four O
corpora O
( O
Lap14 O
, O
Rest14 O
, O
Rest15 O
, O
Rest16 O
) O
were O
respectively O
retrieved O
from O
SemEval O
2014 O
task O
4 O
( O
Pontiki O
et O
al O
. O
,2014 O
) O
, O
Se O
- O
mEval O
2015 O
task O
12 O
( O
Pontiki O
et O
al O
. O
,2015 O
) O
and O
Se O
- O
mEval O
2016 O
Task O
5 O
( O
Pontiki O
et O
al O
. O
,2016 O
) O
, O
whichinclude O
two O
types O
of O
data O
, O
i.e. O
, O
reviews O
of O
laptops O
and O
restaurants O
. O
The O
statistical O
descriptions O
of O
these O
corpora O
are O
shown O
in O
Table1 O
. O
We O
use O
accuracy O
and O
Macro O
- O
average O
F1 O
- O
score O
as O
evaluation O
metrics O
; O
these O
are O
commonly O
used O
in O
ASC O
task O
( O
Huang O
and O
Carley,2019;Zhang O
et O
al O
. O
,2019 O
) O
. O
A O
higher O
accuracy O
orF1 O
- O
score O
indicates O
better O
prediction O
performance31ModelTwitter O
Lap14 O
Rest14 O
Rest15 O
Rest16 O
Acc O
F1 O
Acc O
F1 O
Acc O
F1 O
Acc O
F1 O
Acc O
F1 O
LSTM O
69.56 O
67.70 O
69.29 O
63.09 O
78.13 O
67.47 O
77.37 O
55.17 O
86.80 O
63.88 O
TD O
- O
LSTM O
70.81 O
69.11 O
70.45 O
64.78 O
79.47 O
69.01 O
78.23 O
57.25 O
87.17 O
64.89 O
MemNet O
71.48 O
69.90 O
70.64 O
65.17 O
79.61 O
69.64 O
77.31 O
58.28 O
85.44 O
65.99 O
IAN O
72.50 O
70.81 O
72.05 O
67.38 O
79.26 O
70.09 O
78.54 O
52.65 O
84.74 O
55.21 O
RAM O
69.36 O
67.30 O
74.4971.35 O
80.23 O
70.80 O
78.8561.97 O
88.92 O
68.23 O
AOA O
72.30 O
70.20 O
72.62 O
67.52 O
79.97 O
70.42 O
78.17 O
57.02 O
87.50 O
66.21 O
TNet O
- O
LF O
72.98 O
71.43 O
74.61 O
70.14 O
80.42 O
71.03 O
78.47 O
59.47 O
89.07 O
70.43 O
ASGCN O
72.15 O
70.40 O
75.5571.05 O
80.77 O
72.02 O
79.8961.89 O
88.99 O
67.48 O
G O
- O
ATT O
- O
U O
73.6072.12 O
76.18 O
72.23 O
81.59 O
72.65 O
81.18 O
64.07 O
89.0671.97 O
G O
- O
ATT O
- O
D O
73.8971.82 O
75.75 O
71.52 O
80.89 O
71.68 O
80.93 O
64.03 O
88.8172.36 O
Table O
2 O
: O
Model O
comparison O
results O
( O
% O
) O
. O
In O
the O
case O
of O
random O
initialization O
, O
the O
average O
accuracy O
of O
the O
3 O
runsand O
the O
macroF O
1 O
- O
score O
. O
The O
best O
results O
of O
its O
baseline O
model O
and O
our O
model O
are O
shown O
in O
bold O
. O
ModelTwitter O
Lap14 O
Rest14 O
Rest15 O
Rest16 O
Acc O
F1 O
Acc O
F1 O
Acc O
F1 O
Acc O
F1 O
Acc O
F1 O
ASGCN O
- O
DG O
72.15 O
70.40 O
75.55 O
71.05 O
80.77 O
72.02 O
79.89 O
61.89 O
88.99 O
67.48 O
G O
- O
ATT O
- O
U O
73.6072.12 O
76.18 O
72.23 O
81.59 O
72.65 O
81.18 O
64.07 O
89.06 O
71.97 O
G O
- O
ATT O
- O
U O
w/o O
Pos O
73.74 O
72.00 O
75.13 O
71.26 O
81.82 O
73.91 O
80.07 O
62.42 O
88.6969.54 O
G O
- O
ATT O
- O
U O
w/o O
Mask O
73.36 O
71.47 O
75.2470.70 O
80.15 O
70.49 O
79.8962.78 O
88.5370.34 O
G O
- O
ATT O
- O
U O
w/o O
GAT O
73.03 O
71.04 O
74.56 O
71.23 O
80.21 O
71.16 O
80.38 O
61.31 O
87.66 O
68.27 O
G O
- O
ATT O
- O
U O
w/o O
Conv O
73.23 O
71.22 O
74.8271.35 O
80.86 O
71.77 O
80.5462.02 O
87.39 O
69.22 O
Table O
3 O
: O
Ablation O
study O
results O
( O
% O
) O
. O
Accuracy O
and O
macro O
F1 O
- O
scores O
are O
the O
average O
value O
over O
3 O
runs O
with O
randominitialization O
. O
4.2 O
Implementation O
Details O
To O
comprehensively O
evaluate O
the O
proposed O
model O
, O
we O
selected O
the O
following O
baseline O
methods O
, O
which O
are O
introduced O
as O
follows:‚Ä¢LSTM O
( O
Tang O
et O
al O
. O
,2016a O
) O
uses O
the O
standard O
LSTM O
model O
to O
send O
the O
state O
of O
the O
last O
layer O
to O
thesoftmaxlayer O
to O
obtain O
the O
output O
of O
sentiment O
probability.‚Ä¢TD O
- O
LSTM O
( O
Tang O
et O
al O
. O
,2016a O
) O
connects O
as O
- O
pect O
word O
embedding O
and O
context O
word O
em O
- O
bedding O
to O
obtain O
the O
Ô¨Ånal O
word O
embedding O
representation O
, O
and O
the O
two O
sides O
of O
the O
aspect O
word O
are O
respectively O
modeled O
by O
LSTM O
to O
obtain O
the O
hidden O
layer O
representation.‚Ä¢MemNet O
( O
Tang O
et O
al O
. O
,2016b O
) O
consists O
of O
a O
multilevel O
memory O
network O
, O
which O
effectively O
retains O
context O
and O
aspect O
information.‚Ä¢IAN O
( O
Ma O
et O
al O
. O
,2017 O
) O
exchanges O
information O
between O
context O
and O
aspect O
as O
an O
interactive O
attention O
model.‚Ä¢RAM O
( O
Chen O
et O
al O
. O
,2017 O
) O
learns O
sentencerepresentation O
by O
layers O
consisting O
of O
an O
attention O
- O
based O
aggregation O
of O
word O
features O
and O
a O
GRU O
cell O
with O
multilayer O
architecture.‚Ä¢AOA(Huang O
et O
al O
. O
,2018 O
) O
captures O
the O
inter O
- O
action O
between O
context O
and O
aspect O
words O
by O
jointly O
modeling O
aspects O
and O
sentences.‚Ä¢TNet O
- O
LFT O
( O
Li O
et O
al O
. O
,2018 O
) O
increases O
the O
retention O
of O
context O
information O
through O
a O
context O
retention O
conversion O
mechanism.‚Ä¢ASGCN O
( O
Zhang O
et O
al O
. O
,2019 O
) O
uses O
externalgrammatical O
information O
through O
the O
graph O
convolution O
neural O
network O
, O
while O
aspect O
obtains O
syntax O
- O
related O
context O
information.‚Ä¢G O
- O
ATT O
uses O
either O
undirectional O
( O
G O
- O
ATT O
- O
U O
) O
or O
directional O
( O
G O
- O
ATT O
- O
D O
) O
graphs O
to O
represent O
the O
parsed O
tree O
- O
structure O
as O
the O
proposed O
model O
. O
For O
all O
the O
models O
, O
the O
300 O
- O
dimensional O
GloVe O
vector O
( O
Pennington O
et O
al O
. O
,2014 O
) O
pretrained O
on O
840B O
Common O
Crawl O
was O
used O
as O
the O
initial O
word O
embedding O
. O
Words O
that O
do O
not O
appear O
in O
GloVe O
were O
initialized O
with O
a O
uniform O
distribution O
of O
U(-0.25 O
, O
0.25 O
) O
. O
The O
hidden O
layer O
vectors O
‚Äô O
dimensions O
are O
all O
300 O
, O
and O
all O
model O
weights O
are O
initialized O
with O
the O
Xaviernormalization O
( O
Glorot O
and O
Bengio,2010 O
) O
. O
RMSprop O
was O
used O
as O
the O
optimizer O
with O
a O
learning O
rate O
of O
0.001 O
to O
train O
all O
the O
models O
. O
We O
set O
the O
L2regularization O
decay O
factor O
to O
1e-4 O
and O
the O
batch O
size O
to O
40 O
. O
The O
negative O
input O
slope O
of O
LeakyReLUin O
the O
G O
- O
ATT O
layer O
is O
set O
to O
0.2 O
. O
All O
aforementioned32Aspect O
Model O
Attention O
Visualization O
Prediction O
Label O
OSASGCN O
neutral O
positive O
Convpositive O
positive O
G O
- O
ATT O
CajunshrimpASGCN O
negative O
positive O
Convpositive O
positive O
G O
- O
ATT O
PlaceASGCN O
neutral O
negative O
Convnegative O
negative O
G O
- O
ATT O
Table O
4 O
: O
Visualization O
of O
the O
proposed O
model O
. O
hyperparameters O
are O
selected O
using O
a O
grid O
- O
search O
strategy O
. O
The O
epoch O
was O
set O
depending O
on O
an O
early O
stop O
strategy O
. O
The O
training O
processing O
stops O
after O
Ô¨Åve O
epochs O
if O
there O
is O
no O
improvement O
. O
The O
experimental O
results O
are O
obtained O
by O
averaging O
the O
results O
of O
three O
random O
initialization O
runs.4.3 O
Comparative O
Results O
Table2shows O
the O
comparative O
results O
of O
G O
- O
ATT O
- O
D O
and O
G O
- O
ATT O
- O
U O
against O
several O
baselines O
. O
As O
indicated O
, O
G O
- O
ATT O
- O
U O
outperformed O
all O
baseline O
models O
by O
usingF1 O
- O
score O
as O
a O
criterion O
. O
In O
terms O
of O
accuracy O
, O
except O
results O
slightly O
lower O
than O
the O
TNet O
- O
LF O
model O
on O
Rest16 O
, O
both O
G O
- O
ATT O
- O
D O
and O
G O
- O
ATT O
- O
Uachieved O
better O
performance O
. O
The O
rational O
reasonis O
that O
the O
proposed O
model O
can O
capture O
both O
syntactic O
and O
local O
information O
, O
thus O
improving O
performance O
. O
In O
addition O
, O
the O
improvement O
of O
the O
F1 O
- O
score O
of O
the O
proposed O
model O
on O
Rest15 O
and O
Rest16 O
is O
huge O
compared O
to O
the O
baselines O
, O
which O
is O
2.1 O
% O
and O
1.5%,respectively O
. O
The O
possible O
reason O
is O
that O
the O
syntactical O
structure O
of O
the O
texts O
in O
Rest15 O
and O
Rest16 O
is O
more O
complicated O
than O
those O
in O
Twitter O
, O
Lap14 O
and O
Rest14 O
. O
The O
performance O
of O
directional O
ver O
- O
sion O
( O
G O
- O
ATT O
- O
D O
) O
is O
slightly O
higher O
than O
the O
undi O
- O
rectional O
version O
( O
G O
- O
ATT O
- O
U O
) O
on O
Twitter O
, O
Rest15 O
and O
Rest16 O
, O
while O
performance O
is O
slightly O
lower O
on O
Lap14 O
andRest14 O
, O
indicating O
an O
undirectional O
syntax O
relationship O
that O
is O
more O
appropriate O
on O
those O
datasets.4.4 O
Ablation O
Experiment O
Table3shows O
the O
ablation O
experiments O
to O
investigate O
further O
how O
the O
models O
can O
beneÔ¨Åt O
from O
each O
component O
. O
As O
indicated O
, O
removing O
the O
position O
weight O
( O
i.e. O
, O
G O
- O
ATT O
- O
U O
w/o O
Pos O
) O
causes O
the O
performance O
on O
Lap14 O
, O
Rest15 O
and O
Rest16 O
to O
decrease O
. O
However O
, O
the O
performance O
of O
G O
- O
ATT O
- O
U O
w/o O
Posincreases O
F1 O
- O
score O
by O
1.26 O
% O
when O
used O
on O
Twit- O
ter O
and O
Rest14 O
since O
the O
local O
information O
is O
lessimportant O
than O
syntactic O
. O
Removing O
the O
mask O
op-33(a)Twitter O
( O
b)Lap14 O
( O
c)Rest14 O
( O
d)Rest15 O
( O
e)Rest16 O
Figure O
3 O
: O
Effect O
of O
the O
number O
of O
G O
- O
ATT O
Layers O
eration O
( O
i.e. O
, O
G O
- O
ATT O
- O
U O
w/o O
mask O
) O
reduces O
the O
performance O
, O
which O
shows O
that O
the O
mask O
operation O
prevents O
the O
noise O
word O
from O
entering O
the O
Ô¨Ånal O
representation O
. O
Further O
, O
Twitter O
, O
Lap14 O
, O
and O
Rest14 O
are O
less O
syntactical O
, O
so O
the O
integration O
of O
position O
weight O
does O
not O
beneÔ¨Åt O
or O
can O
even O
negatively O
beneÔ¨Åt O
the O
results O
. O
Besides O
, O
it O
is O
observed O
that O
G O
- O
ATT O
- O
U O
w/o O
Conv O
is O
generally O
better O
than O
G O
- O
ATT O
- O
U O
w/o O
G O
- O
ATT O
, O
which O
shows O
that O
the O
GAT O
layer O
beneÔ¨Åts O
for O
the O
model O
are O
greater O
than O
the O
Conv O
layer O
, O
indicating O
that O
the O
contextual O
syntax O
- O
related O
information O
is O
more O
important O
than O
local O
information O
. O
Compared O
withASGCN O
- O
DG O
, O
the O
proposed O
G O
- O
ATT O
- O
U O
w/o O
Conv O
achieved O
better O
performance O
, O
especially O
on O
Twitter O
and O
Rest16 O
, O
withF1 O
- O
score O
improvements O
of O
0.64 O
% O
and O
1.74 O
% O
, O
respectively O
. O
This O
result O
shows O
that O
GATT O
- O
U O
w/o O
Conv O
outperformed O
the O
ASGCN O
modelin O
most O
cases O
, O
indicating O
that O
graph O
attention O
layers O
with O
different O
edge O
weights O
are O
more O
effective O
than O
graph O
convolution O
layers O
with O
equal O
edge O
weights.4.5 O
Visualization O
Memory O
fusion O
can O
capture O
both O
syntax O
- O
relatedand O
local O
information O
with O
the O
attention O
mechanism O
. O
For O
visualization O
, O
we O
selected O
three O
examples O
from O
Lap14 O
and O
Rest16 O
that O
are O
signiÔ¨Åcantly O
improved O
by O
the O
proposed O
G O
- O
ATT O
model O
against O
the O
ASGCN O
- O
DG O
model O
. O
We O
conducted O
a O
visualization O
experiment O
using O
a O
heat O
map O
to O
show O
the O
attention O
score O
offered O
by O
parameters O
Œ±andŒ≤in O
Eq.(11 O
) O
and O
Eq.(14 O
) O
, O
respectively O
, O
as O
shown O
in O
Table4 O
. O
Thecolor O
density O
is O
the O
attention O
score O
of O
each O
token O
. O
Adeeper O
color O
indicates O
that O
more O
weight O
is O
assigned O
to O
the O
token O
according O
to O
its O
contribution O
to O
the O
Ô¨Ånal O
classiÔ¨Åcation O
. O
As O
indicated O
, O
ASGCN O
allows O
the O
syntactically O
unrelated O
words O
to O
be O
associated O
with O
the O
target O
aspect O
by O
assigning O
equal O
weight O
to O
the O
edge O
, O
such O
asgreatfor O
OS O
, O
goodforCajun O
shrimp O
andnot O
invitingforplace O
. O
Conversely O
, O
G O
- O
ATT O
- O
U O
tends O
to O
block O
graph O
convolution O
propagation O
from O
unrelated O
words O
to O
the O
target O
aspect O
by O
assignedattention O
weights O
to O
the O
edges O
. O
The O
convolution O
operation O
can O
also O
exploit O
some O
explicit O
structure O
, O
such O
asnot O
greatandnot O
inviting O
. O
Such O
phrases O
are O
expressive O
and O
task O
- O
speciÔ¨Åc O
, O
thus O
improve O
performance.4.6 O
Number O
of O
GAT O
layers O
Since O
G O
- O
ATT O
involves O
Llayers O
of O
graph O
attention O
, O
we O
investigate O
whether O
the O
number O
of O
layers O
can O
determine O
the O
proposed O
model O
‚Äôs O
performance O
. O
As O
indicated O
, O
the O
best O
performance O
can O
be O
achieved O
whenLi O
s2o O
n O
Twitter O
, O
7o O
n O
Lap14 O
, O
3o O
n O
Rest14 O
a O
n O
d6o O
n O
Rest15 O
and O
Rest16 O
. O
WhenLis O
greaterthan O
7 O
, O
a O
decreasing O
trend O
in O
both O
metrics O
is O
pre O
- O
sented O
. O
As O
Lreaches O
10 O
, O
the O
model O
contains O
toomany O
parameters O
and O
becomes O
more O
difÔ¨Åcult O
to O
train O
. O
5 O
Conclusions O
In O
this O
study O
, O
a O
graph O
attention O
network O
with O
memory O
fusion O
is O
proposed O
for O
aspect O
- O
level O
sentiment O
analysis O
. O
A O
graph O
attention O
layer O
was O
implemented34to O
capture O
a O
context O
word O
‚Äôs O
syntactic O
relationshipto O
the O
target O
aspect O
by O
learning O
different O
weights O
for O
edges O
to O
block O
the O
propagation O
from O
unrelated O
words O
. O
Moreover O
, O
a O
convolutional O
layer O
and O
a O
memory O
fusion O
were O
used O
to O
learn O
the O
local O
information O
and O
draw O
different O
weights O
for O
context O
words O
. O
Experimental O
results O
show O
that O
the O
G O
- O
ATT O
model O
yields O
better O
performance O
than O
the O
existing O
methods O
for O
aspect O
- O
based O
sentiment O
analysis O
. O
Besides O
, O
ablation O
studies O
and O
case O
studies O
are O
provided O
to O
prove O
the O
effectiveness O
of O
the O
proposed O
model O
further O
. O
Future O
works O
will O
improve O
the O
graph O
attention O
layerand O
dynamic O
to O
learn O
the O
attention O
score O
, O
so O
the O
proposed O
model O
can O
better O
integrate O
syntax O
- O
related O
context O
information O
. O
Acknowledgments O
This O
work O
was O
supported O
by O
the O
National O
Natural O
Science O
Foundation O
of O
China O
( O
NSFC O
) O
under O
Grant O
No O
. O
61966038 O
, O
61702443 O
and O
61762091 O
, O
and O
inpart O
by O
the O
Ministry O
of O
Science O
and O
Technology O
, O
Taiwan O
, O
ROC O
, O
under O
Grant O
No O
. O
MOST107 O
- O
2628 O
- O
E-155 O
- O
002 O
- O
MY3 O
. O
The O
authors O
would O
like O
to O
thank O
the O
anonymous O
reviewers O
for O
their O
constructive O
comments O
. O
Abstract O
Unlike O
non O
- O
conversation O
scenes O
, O
emotion O
recognition O
in O
dialogues O
( O
ERD O
) O
poses O
more O
complicated O
challenges O
due O
to O
its O
interactive O
nature O
and O
intricate O
contextual O
information O
. O
All O
present O
methods O
model O
historical O
utterances O
without O
considering O
the O
content O
of O
the O
target O
utterance O
. O
However O
, O
different O
parts O
of O
a O
historical O
utterance O
may O
contribute O
differently O
to O
emotion O
inference O
of O
different O
target O
utterances O
. O
Therefore O
we O
propose O
Fine O
- O
grained O
Extraction O
and O
Reasoning O
Network O
( O
FERNet O
) O
to O
generate O
target O
- O
speciÔ¨Åc O
historical O
utterance O
representations O
. O
The O
reasoning O
module O
effectively O
handles O
both O
local O
and O
global O
sequential O
dependencies O
to O
reason O
over O
context O
, O
and O
updates O
target O
utterance O
representations O
to O
more O
informed O
vectors O
. O
Experiments O
on O
two O
benchmarks O
show O
that O
our O
method O
achieves O
competitive O
performance O
compared O
with O
previous O
methods O
. O
1 O
Introduction O
With O
the O
development O
of O
human O
- O
machine O
interaction O
( O
HMI O
) O
applications O
, O
textual O
dialogue O
scenes O
appear O
more O
frequently O
. O
These O
scenes O
request O
effective O
and O
high O
- O
performance O
emotion O
recognition O
systems O
helping O
in O
building O
empathetic O
machines O
( O
Young O
et O
al O
. O
, O
2018 O
) O
. O
Therefore O
, O
emotion O
recognition O
in O
dialogues O
( O
ERD O
) O
is O
getting O
growing O
attention O
from O
both O
academic O
and O
business O
community O
. O
Different O
from O
non O
- O
conversation O
scenes O
, O
the O
ERD O
task O
poses O
a O
more O
complicated O
challenge O
of O
modeling O
context O
- O
sensitive O
dependencies O
. O
Most O
of O
existing O
approaches O
adopt O
Convolution O
Neural O
Network O
( O
CNN O
) O
( O
Krizhevsky O
et O
al O
. O
, O
2012 O
) O
, O
followed O
by O
a O
max O
- O
pooling O
layer O
to O
obtain O
utterance O
representations O
( O
Kim O
, O
2014 O
; O
Torres O
, O
2018 O
; O
Hazarika O
et O
al O
. O
, O
2018a O
, O
b O
; O
Majumder O
et O
al O
. O
, O
2019 O
; O
Ghosal O
et O
al O
. O
, O
2019 O
) O
. O
The O
process O
proceeds O
without O
the O
guidance O
of O
the O
target O
utterance O
, O
thus O
generated O
historicalutterance O
representations O
are O
indistinguishable O
toward O
different O
target O
utterances O
. O
Emotion O
recognition O
may O
fail O
in O
cases O
where O
historical O
utterances O
express O
various O
emotions O
toward O
various O
targets O
, O
which O
may O
confuse O
the O
emotion O
recognition O
of O
target O
utterances O
. O
As O
Figure O
1 O
shows O
, O
for O
different O
target O
utterances O
B1andB2 O
, O
the O
model O
should O
attend O
the O
words O
‚Äú O
good O
service O
‚Äù O
and O
‚Äú O
bad O
food O
‚Äù O
in O
A1 O
, O
separately O
. O
In O
a O
word O
, O
it O
is O
desired O
to O
pay O
different O
attention O
to O
different O
words O
of O
a O
certain O
historical O
utterance O
to O
generate O
the O
target O
- O
speciÔ¨Åc O
historical O
utterance O
representation O
. O
Therestaurantwherewehaddinnerprovided[goodservice]but[badfood O
] O
. O
üòê O
Whataboutthefood?Ican‚Äôtagreewithyoumore O
. O
üòê O
üòû O
üòÑ O
Yeah O
, O
Iwasimpressedbytheservice.ùê¥"ùê¥#ùêµ"ùêµ O
# O
Figure O
1 O
: O
A O
dialogue O
shows O
that O
modeling O
intricate O
contextual O
information O
is O
crucial O
for O
emotion O
recognition O
. O
In O
this O
paper O
, O
we O
propose O
Fine O
- O
grained O
Extraction O
and O
Reasoning O
Network O
( O
FERNet O
) O
to O
generate O
target O
- O
speciÔ¨Åc O
historical O
utterance O
representations O
conditioned O
on O
the O
content O
of O
target O
utterances O
by O
using O
the O
multi O
- O
head O
attention O
mechanism O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
extracting O
more O
Ô¨Åne O
- O
grained O
, O
relevant O
and O
contributing O
information O
for O
emotion O
recognition O
. O
Besides O
, O
we O
devise O
the O
reasoning O
module O
, O
which O
employs O
historical O
utterances O
as O
a O
sequence O
of O
triggers O
, O
and O
updates O
the O
representation O
of O
the O
target O
utterance O
to O
a O
more O
informed O
vector O
as O
it O
observes O
historical O
utterances O
through O
time O
. O
In O
the O
reasoning O
process O
, O
the O
module O
models O
both O
short O
- O
term O
and O
long O
- O
term O
sequential O
dependencies O
effectively O
. O
We O
demonstrate O
the O
effectiveness O
of O
our O
method O
on O
two O
benchmarks O
. O
Experimental O
results O
show O
that O
our O
method O
achieves O
competitive O
performance O
compared O
with O
previous O
methods.372 O
Related O
Work O
Primitive O
approaches O
deal O
with O
the O
ERD O
task O
as O
simple O
solely O
- O
sentence O
emotion O
recognition O
task O
with O
no O
consideration O
of O
the O
historical O
information O
( O
Joulin O
et O
al O
. O
, O
2016 O
; O
Chen O
et O
al O
. O
, O
2016 O
; O
Yang O
et O
al O
. O
, O
2016 O
; O
Chatterjee O
et O
al O
. O
, O
2019 O
) O
. O
To O
exploit O
contextual O
information O
, O
Poria O
et O
al O
. O
( O
2017 O
) O
; O
Huang O
et O
al O
. O
( O
2019 O
) O
; O
Jiao O
et O
al O
. O
( O
2019 O
) O
; O
Hazarika O
et O
al O
. O
( O
2018a O
, O
b O
) O
; O
Torres O
( O
2018 O
) O
use O
RNN O
architecture O
, O
Hazarika O
et O
al O
. O
( O
2018b O
, O
a O
) O
use O
conversational O
memory O
networks O
( O
Sukhbaatar O
et O
al O
. O
, O
2015 O
) O
, O
Torres O
( O
2018 O
) O
; O
Jiao O
et O
al O
. O
( O
2019 O
) O
use O
attention O
mechanism O
and O
Ghosal O
et O
al O
. O
( O
2019 O
) O
uses O
graph O
neural O
network O
. O
Besides O
, O
Majumder O
et O
al O
. O
( O
2019 O
) O
; O
Hazarika O
et O
al O
. O
( O
2018a O
) O
propose O
to O
keep O
track O
of O
states O
of O
individual O
speakers O
throughout O
the O
dialogue O
and O
Ghosal O
et O
al O
. O
( O
2019 O
) O
incorporates O
speaker O
information O
into O
edge O
types O
. O
Some O
of O
these O
works O
consider O
the O
context O
following O
the O
target O
utterance O
such O
as O
Luo O
et O
al O
. O
( O
2018 O
) O
; O
Saxena O
et O
al O
. O
( O
2018 O
) O
; O
Ghosal O
et O
al O
. O
( O
2019 O
) O
and O
some O
variants O
of O
Majumder O
et O
al O
. O
( O
2019 O
) O
. O
However O
, O
this O
condition O
is O
quite O
incompatible O
with O
some O
practical O
situations O
like O
real O
- O
time O
dialogue O
systems O
in O
which O
we O
possess O
no O
future O
utterances O
while O
handling O
the O
target O
utterance O
. O
So O
in O
our O
paper O
, O
we O
only O
focus O
on O
the O
setting O
that O
only O
historical O
utterances O
can O
be O
utilized O
. O
3 O
Proposed O
Model O
Each O
dialogue O
Dconsists O
of O
two O
parts O
denoted O
as O
D={(U O
, O
S O
) O
} O
, O
whereU= O
[ O
u1,u2, O
... O
,u O
n]is O
a O
sequence O
of O
utterances O
ordered O
based O
on O
their O
temporal O
occurrence O
. O
S= O
[ O
s1,s2, O
... O
,s O
n]denotes O
corresponding O
speakers O
and O
nis O
the O
number O
of O
utterances O
in O
the O
dialogue O
. O
The O
ERD O
task O
aims O
to O
predict O
Y= O
[ O
y1,y2, O
... O
,y O
n O
] O
, O
whereyi‚ààC(1‚â§i‚â§n O
) O
denotes O
the O
underlying O
emotion O
of O
the O
utterance O
ui O
. O
Cis O
the O
set O
of O
candidate O
emotion O
categories O
. O
The O
FERNet O
consists O
of O
four O
successive O
modules O
: O
feature O
extraction O
module O
, O
attention O
module O
, O
reasoning O
module O
and O
output O
module O
. O
Figure O
2 O
presents O
the O
overall O
architecture O
of O
the O
proposed O
model O
. O
3.1 O
Feature O
Extraction O
Module O
We O
use O
two O
multi O
- O
layer O
bidirectional O
Gated O
Recurrent O
Unit O
( O
bi O
- O
GRU O
) O
Networks O
( O
Tang O
et O
al O
. O
, O
2015 O
) O
to O
accumulate O
contextual O
information O
from O
two O
directions O
for O
each O
word O
of O
target O
utterances O
and O
historical O
utterances O
, O
separately O
. O
The O
inputs O
consist O
of O
Multi O
- O
layerBi O
- O
GRUMulti O
- O
layerBi O
- O
GRUMulti O
- O
headAttentionReasoningModuleùë¢ O
" O
ùë¢#ùêæùëâùëÑùë•"ùêª"ùëÖùëÖ""ùëÖ""Figure O
2 O
: O
The O
overall O
architecture O
of O
the O
model O
. O
300 O
dimensional O
pre O
- O
trained O
GloVe O
vectors O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
. O
The O
k‚àíth O
contextual O
word O
representation O
hl O
k= O
[ O
‚àí O
‚Üí O
hl O
k;‚Üê O
‚àí O
hl O
k]is O
generated O
by O
concatenating O
the O
hidden O
states O
of O
the O
k‚àíth O
time O
steps O
of O
forward O
and O
backward O
GRU O
, O
where O
lis O
the O
number O
of O
layers O
. O
3.2 O
Attention O
Module O
We O
utilize O
multi O
- O
head O
attention O
mechanism O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
to O
focus O
on O
more O
relevant O
parts O
of O
each O
historical O
utterance O
according O
to O
the O
target O
utterance O
. O
We O
also O
employ O
residual O
connection O
( O
He O
et O
al O
. O
, O
2016 O
) O
followed O
by O
layer O
normalization O
( O
Ba O
et O
al O
. O
, O
2016 O
) O
to O
make O
model O
training O
easier O
. O
The O
target O
- O
speciÔ¨Åc O
representations O
of O
historical O
utterances O
are O
obtained O
by O
: O
X O
= O
Concat O
( O
head O
1, O
... O
head O
t)WO(1 O
) O
head O
i O
= O
Attention O
( O
QWQ O
i O
, O
KWK O
i O
, O
VWV O
i)(2 O
) O
Attention O
( O
Q O
, O
K O
, O
V O
) O
= O
softmax O
( O
QKT O
‚àödk)V(3 O
) O
where O
queries O
Q O
= O
Rare O
representations O
of O
target O
utterances O
, O
keys O
Kand O
valuesVare O
contextual O
word O
representations O
for O
words O
of O
historical O
utterances O
. O
WQ O
i‚ààRd√ódk O
, O
WK O
i‚ààRd√ódk O
, O
WV O
i‚ààRd√ódv O
andWO‚ààRtdv√ódare O
parameter O
matrices O
, O
where O
dkis O
the O
dimension O
of O
queries O
and O
keys O
, O
dvis O
the O
dimension O
of O
values O
, O
dis O
the O
dimension O
of O
the O
output O
of O
feature O
extraction O
module O
and O
tis O
the O
number O
of O
heads O
. O
X= O
[ O
x1,x2, O
... O
,x O
n‚àí1]are O
target O
- O
speciÔ¨Åc O
representations O
of O
historical O
utterances O
. O
3.3 O
Reasoning O
Module O
The O
reasoning O
module O
takes O
target O
- O
speciÔ¨Åc O
historical O
utterance O
representations O
[ O
x1,x2, O
... O
x O
n‚àí1]and O
target O
utterance O
representations O
Ras O
inputs O
. O
Target O
utterance O
representations O
are O
updated O
through O
time O
and O
layers O
. O
Each O
unit O
in O
this O
module O
takes O
two O
inputs O
: O
R O
andxi(1‚â§i‚â§n‚àí1 O
) O
. O
Thet‚àíth O
unit O
updates O
R O
according O
to O
xtby O
: O
zt O
= O
Œ±(xt O
, O
R O
) O
= O
œÉ(Wz(xt O
‚ó¶ O
R O
) O
+ O
bz O
) O
( O
4)38rt O
= O
Œ≤(xt O
, O
R O
) O
= O
œÉ(Wr(xt O
‚ó¶ O
R O
) O
+ O
br)(5 O
) O
ÀúRt O
= O
œÅ(xt O
, O
R O
) O
= O
tanh(Wh[xt;R O
] O
+ O
bh)(6 O
) O
Rt O
= O
ztrtÀúRt+ O
( O
1‚àízt)Rt‚àí1 O
( O
7 O
) O
whereztis O
the O
update O
gate O
, O
rtis O
a O
reset O
function O
, O
ÀúRtis O
the O
candidate O
of O
updated O
representation O
of O
target O
utterance O
and O
Rtis O
the O
updated O
representation O
after O
observing O
the O
t‚àíth O
historical O
utterance O
. O
œÉis O
sigmoid O
activation O
, O
tanh O
is O
hyperbolic O
tangent O
activation, O
‚ó¶ O
is O
element O
- O
wise O
vector O
multiplication O
, O
and O
[ O
; O
] O
is O
vector O
concatenation O
along O
the O
last O
dimension O
. O
Wz‚ààRd√ód O
, O
Wr‚ààRd√ód O
, O
Wh‚ààRd√ó2dare O
weight O
matrices O
, O
bz‚ààRd O
, O
br‚ààRd O
, O
bh‚ààRdare O
bias O
terms O
. O
SpeciÔ¨Åcally O
, O
ztmeasures O
the O
relevance O
between O
the O
target O
utterance O
representation O
and O
the O
tth O
historical O
utterance O
representation O
for O
Ô¨Åne O
- O
controlled O
gating O
. O
Compared O
with O
global O
attention O
computed O
over O
all O
historical O
utterances O
, O
the O
gate O
can O
be O
considered O
as O
local O
attention O
which O
models O
short O
- O
term O
sequential O
dependency O
. O
rtis O
a O
reset O
function O
to O
determine O
how O
much O
previous O
information O
should O
be O
ignored O
by O
resetting O
the O
candidate O
of O
updated O
representation O
of O
target O
utterance O
. O
As O
shown O
in O
Sukhbaatar O
et O
al O
. O
( O
2015 O
) O
, O
multi O
- O
hop O
can O
perform O
reasoning O
over O
multiple O
facts O
more O
effectively O
. O
So O
we O
stack O
several O
layers O
with O
outputs O
of O
the O
current O
layer O
used O
as O
inputs O
to O
the O
next O
layer O
. O
Besides O
, O
to O
model O
more O
abundant O
information O
, O
we O
compute‚àí O
‚Üí O
Rl O
tand‚Üê O
‚àí O
Rl O
tin O
both O
forward O
and O
backward O
directions O
and O
add O
them O
together O
to O
get O
Rl O
tas O
the O
updated O
representation O
of O
the O
t‚àíth O
unit O
inl‚àíth O
layer O
: O
Rl O
t=‚àí O
‚ÜíRl O
t+‚Üê O
‚àíRl O
t O
( O
8) O
Finally O
, O
we O
get O
the O
updated O
representation O
of O
target O
utterance O
Rupdate O
= O
RL O
n‚àí1 O
, O
wheren‚àí1and O
Lare O
the O
number O
of O
units O
and O
the O
number O
of O
layers O
in O
the O
reasoning O
module O
, O
respectively O
. O
3.4 O
Output O
Module O
After O
the O
feature O
extraction O
and O
reasoning O
modules O
, O
we O
obtain O
the O
updated O
representation O
of O
target O
utterance O
. O
To O
preserve O
original O
semantic O
content O
, O
we O
concatenate O
the O
updated O
representation O
and O
the O
original O
representation O
together O
: O
Rfinal O
= O
[ O
Rupdate;R O
] O
( O
9 O
) O
We O
use O
a O
fully O
connected O
layer O
with O
softmax O
as O
activation O
to O
calculate O
emotion O
- O
class O
probabilities O
: O
P O
= O
softmax O
( O
WfRfinal O
+ O
bf O
) O
( O
10)whereWf‚ààRdclass√ó2dis O
a O
weight O
matrix O
, O
bf‚àà O
Rdclass O
is O
a O
bias O
term O
and O
P‚ààRdclass O
are O
emotionclass O
probabilities O
. O
4 O
Experiment O
Datasets O
We O
perform O
experiments O
on O
two O
benchmarks O
: O
IEMOCAP O
( O
Busso O
et O
al O
. O
, O
2008 O
) O
and O
A O
VEC O
( O
Schuller O
et O
al O
. O
, O
2012 O
) O
. O
They O
are O
multimodal O
datasets O
involved O
in O
two O
- O
way O
dynamic O
conversations O
. O
In O
this O
paper O
, O
we O
only O
focus O
on O
using O
textual O
modality O
to O
recognize O
the O
emotion O
. O
The O
data O
distribution O
is O
shown O
in O
Appendices O
. O
Evaluation O
Metrics O
We O
use O
accuracy O
( O
Acc O
. O
) O
, O
F1score O
( O
F1 O
) O
and O
weighted O
average O
F1 O
- O
socre O
( O
Average O
) O
as O
evaluation O
metrics O
for O
IEMOCAP O
dataset O
. O
Mean O
Absolute O
Error O
( O
MAE O
) O
and O
Pearson O
correlation O
coefÔ¨Åcient O
( O
r O
) O
are O
used O
as O
metrics O
for O
A O
VEC O
dataset O
. O
Baselines O
We O
compare O
the O
FERNet O
with O
following O
existing O
approaches O
: O
CNN O
( O
Kim O
, O
2014 O
) O
, O
c O
- O
LSTM O
( O
Poria O
et O
al O
. O
, O
2017 O
) O
, O
c O
- O
LSTM+Attention O
( O
Poria O
et O
al O
. O
, O
2017 O
) O
, O
Memnet O
( O
Ba O
et O
al O
. O
, O
2016 O
) O
, O
CMN O
( O
Hazarika O
et O
al O
. O
, O
2018b O
) O
, O
DialogueRNN O
( O
Majumder O
et O
al O
. O
, O
2019 O
) O
. O
Training O
Details O
The O
training O
details O
such O
as O
hyper O
- O
parameters O
and O
settings O
we O
used O
are O
shown O
in O
Appendices O
. O
4.1 O
Results O
The O
overall O
results O
of O
experiments O
are O
shown O
in O
Table O
1 O
. O
We O
can O
see O
that O
our O
model O
outperforms O
baselines O
signiÔ¨Åcantly O
on O
all O
evaluation O
metrics O
of O
both O
datasets O
. O
SpeciÔ¨Åcally O
, O
our O
model O
surpasses O
DialogueRNN O
by O
1.69 O
% O
on O
weighted O
average O
F1score O
. O
For O
A O
VEC O
dataset O
, O
our O
model O
lower O
mean O
absolute O
error O
by O
0.03 O
, O
0.027 O
, O
0.009 O
and O
0.31 O
for O
valence O
, O
arousal O
, O
expectancy O
and O
power O
, O
separately O
. O
We O
attribute O
the O
enhancement O
to O
the O
fundamental O
improvement O
of O
FERNet O
, O
which O
are O
generating O
target O
- O
speciÔ¨Åc O
representations O
of O
historical O
utterances O
and O
handling O
both O
short O
- O
term O
and O
long O
- O
term O
sequential O
dependencies O
. O
4.2 O
Discussion O
and O
Analysis O
Parameters O
We O
conduct O
experiments O
with O
different O
values O
of O
the O
number O
of O
historical O
utterances O
( O
N O
) O
and O
the O
number O
of O
layers O
of O
reasoning O
module O
( O
L O
) O
on O
the O
IEMOCAP O
dataset O
. O
Results O
are O
shown O
in O
Figure O
4 O
. O
We O
observe O
that O
as O
Nincreases O
, O
the O
performance O
of O
the O
model O
tends O
to O
be O
improved O
. O
This O
trend O
shows O
that O
adequate O
historical O
information39methodsIEMOCAP O
A O
VEC O
Happy O
Sad O
Neutral O
Angry O
Excited O
Frustrated O
Average O
Valence O
Arousal O
Expectancy O
Power O
Acc O
. O
F1 O
Acc O
. O
F1 O
Acc O
. O
F1 O
Acc O
. O
F1 O
Acc O
. O
F1 O
Acc O
. O
F1 O
Acc O
. O
F1 O
MAE O
r O
MAE O
r O
MAE O
r O
MAE O
r O
CNN O
27.22 O
29.86 O
57.14 O
53.83 O
34.33 O
40.14 O
61.17 O
52.44 O
46.15 O
50.09 O
62.99 O
55.75 O
48.92 O
48.18 O
0.545 O
-0.01 O
0.542 O
0.01 O
0.605 O
-0.01 O
8.71 O
0.19 O
c O
- O
LSTM O
29.17 O
34.43 O
57.14 O
60.87 O
54.17 O
51.81 O
57.06 O
56.73 O
51.17 O
57.95 O
67.19 O
58.92 O
55.21 O
54.95 O
0.194 O
0.14 O
0.212 O
0.23 O
0.201 O
0.25 O
8.90 O
-0.04 O
c O
- O
LSTM+Attention O
30.56 O
35.63 O
56.73 O
62.90 O
57.55 O
53.00 O
59.41 O
59.24 O
52.84 O
58.85 O
65.88 O
59.41 O
56.32 O
56.19 O
0.189 O
0.16 O
0.213 O
0.25 O
0.190 O
0.24 O
8.67 O
0.10 O
Memnet O
25.72 O
33.53 O
55.53 O
61.77 O
58.12 O
52.84 O
59.32 O
55.39 O
51.50 O
58.30 O
67.2 O
59.00 O
55.72 O
55.10 O
0.202 O
0.16 O
0.211 O
0.24 O
0.216 O
0.23 O
8.97 O
0.05 O
CMN O
25.00 O
30.38 O
55.92 O
62.41 O
52.86 O
52.39 O
61.76 O
59.83 O
55.52 O
60.25 O
71.13 O
60.69 O
56.56 O
56.13 O
0.192 O
0.23 O
0.213 O
0.29 O
0.195 O
0.26 O
8.74 O
-0.02 O
DialogueRNN‚àó31.25 O
33.83 O
66.12 O
69.83 O
63.02 O
57.76 O
61.76 O
62.50 O
61.54 O
64.45 O
59.58 O
59.46 O
59.33 O
59.89 O
0.188 O
0.28 O
0.201 O
0.36 O
0.188 O
0.32 O
8.19 O
0.31 O
FERNet O
38.89 O
40.14 O
72.65 O
70.22 O
67.19 O
61.50 O
66.47 O
62.43 O
68.90 O
68.21 O
50.39 O
58.63 O
61.80 O
61.58 O
0.158 O
0.44 O
0.174 O
0.43 O
0.179 O
0.37 O
7.88 O
0.36 O
Table O
1 O
: O
Performance O
of O
FERNet O
compared O
with O
baselines O
on O
the O
IEMOCAP O
dataset O
and O
A O
VEC O
dataset O
. O
Bold O
font O
denotes O
the O
best O
performances.‚àópresents O
the O
state O
- O
of O
- O
the O
- O
art O
method O
in O
the O
setting O
that O
only O
historical O
utterances O
can O
be O
utilized O
. O
I‚Äômgettingmarried.[excited]Noway.[excited O
] O
. O
                     O
[ O
1]Noway O
, O
when?When O
, O
when O
, O
whendidithappen?[excited][1]Justacoupledaysago.[excited]Ican‚Äôtbelieveit.[2]Inever O
thought O
you O
would O
get O
married.[excited][2]Iknowmeneither.[excited O
] O
Noway O
, O
when?When O
, O
when O
, O
whendidithappen?<EOS O
> O
Figure O
3 O
: O
Average O
attention O
vectors O
across O
all O
attention O
heads O
for O
words O
of O
a O
historical O
utterance O
with O
regard O
to O
different O
target O
utterances O
. O
[ O
1 O
] O
shows O
the O
attention O
vector O
for O
the O
sentence O
‚Äù O
Just O
a O
couple O
days O
ago O
‚Äù O
; O
[ O
2 O
] O
shows O
the O
attention O
vector O
for O
the O
sentence O
‚Äù O
I O
know O
me O
either O
‚Äù O
. O
contributes O
to O
the O
performance O
of O
emotion O
recognition O
. O
However O
, O
a O
further O
increase O
of O
Ndegrades O
the O
performance O
of O
the O
model O
. O
It O
is O
mainly O
due O
to O
that O
there O
is O
too O
much O
- O
unrelated O
information O
confusing O
the O
model O
. O
As O
for O
L O
, O
the O
trend O
is O
similar O
to O
the O
parameterN. O
Models O
with O
hops O
in O
the O
range O
of O
2 O
- O
8 O
outperform O
the O
single O
layer O
variant O
. O
However O
, O
with O
Lincreasing O
, O
the O
reasoning O
module O
deepens O
and O
may O
cause O
the O
gradient O
vanishing O
problem O
which O
damages O
the O
performance O
of O
the O
model O
. O
55.2455.5356.9558.0458.8858.3959.4260.2961.5860.8558.915052545658606264 O
234510152025303540The O
number O
of O
historical O
utterances(N)57.4659.4260.7358.658.458.8157.8858.2856.4356.3350525456586062 O
12345678910The O
number O
of O
hops(L)(ùëé)(ùëè)ùëäùëíùëñùëî‚Ñéùë°ùëíùëë O
	 O
ùêπ1‚àíùë†ùëêùëúùëüùëíùëäùëíùëñùëî‚Ñéùë°ùëíùëë O
		 O
ùêπ1‚àíùë†ùëêùëúùëüùëí O
Figure O
4 O
: O
Performance O
of O
FERNet O
with O
different O
values O
ofNandL. O
In O
( O
a),L= O
2and O
in O
( O
b),N= O
20 O
. O
Ablation O
Study O
In O
order O
to O
demonstrate O
the O
effect O
of O
each O
module O
, O
we O
perform O
ablation O
studies O
. O
We O
compare O
the O
attention O
- O
based O
model O
with O
the O
attention O
- O
free O
model O
and O
replace O
the O
reasoning O
module O
with O
a O
memory O
network O
. O
As O
shown O
in O
Table O
2 O
, O
attention O
module O
and O
reasoning O
module O
both O
have O
a O
positive O
impact O
on O
model O
performance O
. O
Case O
Study O
and O
Error O
Analysis O
We O
analyze O
the O
predicted O
results O
and O
Ô¨Ånd O
that O
misclassiÔ¨Åcation O
often O
occurs O
when O
utterances O
are O
short O
. O
For O
example O
, O
our O
model O
classiÔ¨Åes O
‚Äú O
what O
? O
‚Äù O
as O
‚Äú O
neutral O
‚Äù O
, O
but O
the O
label O
is O
‚Äú O
excited O
‚Äù O
. O
We O
think O
it O
is O
due O
to O
the O
lack O
of O
visual O
and O
audio O
modality O
. O
In O
this O
utterance O
, O
methods O
Acc O
. O
F1 O
FERNet O
without O
attention O
58.84 O
58.58 O
FERNet O
with O
memory O
network O
59.77 O
59.33 O
FERNet O
61.80 O
61.58 O
Table O
2 O
: O
Performance O
of O
variants O
of O
FERNet O
on O
the O
IEMOCAP O
dataset O
. O
Bold O
font O
denotes O
the O
best O
performances O
. O
high O
pitched O
audio O
can O
provide O
vital O
information O
for O
recognizing O
the O
emotion O
. O
Besides O
, O
we O
Ô¨Ånd O
our O
model O
misclassiÔ¨Åes O
several O
‚Äú O
excited O
‚Äù O
utterances O
as O
‚Äú O
happy O
‚Äù O
utterances O
, O
several O
‚Äú O
sad O
‚Äù O
utterances O
as O
‚Äú O
frustrated O
‚Äù O
utterances O
, O
and O
vice O
versa O
. O
The O
reason O
is O
that O
it O
is O
hard O
for O
the O
model O
to O
distinguish O
the O
subtle O
difference O
between O
these O
similar O
emotions O
. O
Besides O
, O
we O
perform O
qualitative O
visualization O
of O
the O
attention O
module O
. O
The O
dialogue O
in O
Figure O
3 O
shows O
that O
for O
different O
target O
utterances O
, O
the O
model O
allocates O
different O
attention O
to O
words O
of O
a O
historical O
utterance O
. O
It O
demonstrates O
the O
effectiveness O
of O
the O
attention O
module O
. O
5 O
Conclusion O
In O
this O
paper O
, O
we O
propose O
FERNet O
to O
solve O
the O
ERD O
task O
. O
The O
model O
generates O
target O
- O
speciÔ¨Åc O
historical O
utterances O
according O
to O
the O
content O
of O
the O
target O
utterance O
using O
attention O
mechanism O
. O
The O
reasoning O
module O
effectively O
handles O
both O
local O
and O
global O
sequential O
dependencies O
to O
update O
the O
original O
representation O
of O
the O
target O
utterance O
to O
a O
more O
informed O
vector O
. O
Our O
model O
achieves O
competitive O
performance O
on O
two O
benchmarks.40References O
Jimmy O
Lei O
Ba O
, O
Jamie O
Ryan O
Kiros O
, O
and O
Geoffrey O
E O
Hinton O
. O
2016 O
. O
Layer O
normalization O
. O
arXiv O
preprint O
arXiv:1607.06450 O
. O
Carlos O
Busso O
, O
Murtaza O
Bulut O
, O
Chi O
- O
Chun O
Lee O
, O
Abe O
Kazemzadeh O
, O
Emily O
Mower O
, O
Samuel O
Kim O
, O
Jeannette O
N O
Chang O
, O
Sungbok O
Lee O
, O
and O
Shrikanth O
S O
Narayanan O
. O
2008 O
. O
Iemocap O
: O
Interactive O
emotional O
dyadic O
motion O
capture O
database O
. O
Language O
resources O
and O
evaluation O
, O
42(4):335 O
. O
Ankush O
Chatterjee O
, O
Umang O
Gupta O
, O
Manoj O
Kumar O
Chinnakotla O
, O
Radhakrishnan O
Srikanth O
, O
Michel O
Galley O
, O
and O
Puneet O
Agrawal O
. O
2019 O
. O
Understanding O
emotions O
in O
text O
using O
deep O
learning O
and O
big O
data O
. O
Computers O
in O
Human O
Behavior O
, O
93:309‚Äì317 O
. O
Huimin O
Chen O
, O
Maosong O
Sun O
, O
Cunchao O
Tu O
, O
Yankai O
Lin O
, O
and O
Zhiyuan O
Liu O
. O
2016 O
. O
Neural O
sentiment O
classiÔ¨Åcation O
with O
user O
and O
product O
attention O
. O
In O
Proceedings O
of O
the O
2016 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
1650‚Äì1659 O
. O
Deepanway O
Ghosal O
, O
Navonil O
Majumder O
, O
Soujanya O
Poria O
, O
Niyati O
Chhaya O
, O
and O
Alexander O
Gelbukh O
. O
2019 O
. O
Dialoguegcn O
: O
A O
graph O
convolutional O
neural O
network O
for O
emotion O
recognition O
in O
conversation O
. O
arXiv O
preprint O
arXiv:1908.11540 O
. O
Devamanyu O
Hazarika O
, O
Soujanya O
Poria O
, O
Rada O
Mihalcea O
, O
Erik O
Cambria O
, O
and O
Roger O
Zimmermann O
. O
2018a O
. O
Icon O
: O
Interactive O
conversational O
memory O
network O
for O
multimodal O
emotion O
detection O
. O
In O
Proceedings O
of O
the O
2018 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
2594‚Äì2604 O
. O
Devamanyu O
Hazarika O
, O
Soujanya O
Poria O
, O
Amir O
Zadeh O
, O
Erik O
Cambria O
, O
Louis O
- O
Philippe O
Morency O
, O
and O
Roger O
Zimmermann O
. O
2018b O
. O
Conversational O
memory O
network O
for O
emotion O
recognition O
in O
dyadic O
dialogue O
videos O
. O
In O
Proceedings O
of O
the O
2018 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
Papers O
) O
, O
pages O
2122 O
‚Äì O
2132 O
. O
Kaiming O
He O
, O
Xiangyu O
Zhang O
, O
Shaoqing O
Ren O
, O
and O
Jian O
Sun O
. O
2016 O
. O
Deep O
residual O
learning O
for O
image O
recognition O
. O
In O
Proceedings O
of O
the O
IEEE O
conference O
on O
computer O
vision O
and O
pattern O
recognition O
, O
pages O
770 O
‚Äì O
778 O
. O
Chenyang O
Huang O
, O
Amine O
Trabelsi O
, O
and O
Osmar O
R O
Za¬®ƒ±ane O
. O
2019 O
. O
Ana O
at O
semeval-2019 O
task O
3 O
: O
Contextual O
emotion O
detection O
in O
conversations O
through O
hierarchical O
lstms O
and O
bert O
. O
arXiv O
preprint O
arXiv:1904.00132 O
. O
Wenxiang O
Jiao O
, O
Haiqin O
Yang O
, O
Irwin O
King O
, O
and O
Michael O
R O
Lyu O
. O
2019 O
. O
Higru O
: O
Hierarchical O
gated O
recurrent O
units O
for O
utterance O
- O
level O
emotion O
recognition O
. O
arXiv O
preprint O
arXiv:1904.04446 O
.Armand O
Joulin O
, O
Edouard O
Grave O
, O
Piotr O
Bojanowski O
, O
and O
Tomas O
Mikolov O
. O
2016 O
. O
Bag O
of O
tricks O
for O
efÔ¨Åcient O
text O
classiÔ¨Åcation O
. O
arXiv O
preprint O
arXiv:1607.01759 O
. O
Yoon O
Kim O
. O
2014 O
. O
Convolutional O
neural O
networks O
for O
sentence O
classiÔ¨Åcation O
. O
arXiv O
preprint O
arXiv:1408.5882 O
. O
Alex O
Krizhevsky O
, O
Ilya O
Sutskever O
, O
and O
Geoffrey O
E O
Hinton O
. O
2012 O
. O
Imagenet O
classiÔ¨Åcation O
with O
deep O
convolutional O
neural O
networks O
. O
In O
Advances O
in O
neural O
information O
processing O
systems O
, O
pages O
1097‚Äì1105 O
. O
Linkai O
Luo O
, O
Haiqing O
Yang O
, O
and O
Francis O
YL O
Chin O
. O
2018 O
. O
Emotionx O
- O
dlc O
: O
self O
- O
attentive O
bilstm O
for O
detecting O
sequential O
emotions O
in O
dialogue O
. O
arXiv O
preprint O
arXiv:1806.07039 O
. O
Navonil O
Majumder O
, O
Soujanya O
Poria O
, O
Devamanyu O
Hazarika O
, O
Rada O
Mihalcea O
, O
Alexander O
Gelbukh O
, O
and O
Erik O
Cambria O
. O
2019 O
. O
Dialoguernn O
: O
An O
attentive O
rnn O
for O
emotion O
detection O
in O
conversations O
. O
In O
Proceedings O
of O
the O
AAAI O
Conference O
on O
ArtiÔ¨Åcial O
Intelligence O
, O
volume O
33 O
, O
pages O
6818‚Äì6825 O
. O
Jeffrey O
Pennington O
, O
Richard O
Socher O
, O
and O
Christopher O
Manning O
. O
2014 O
. O
Glove O
: O
Global O
vectors O
for O
word O
representation O
. O
In O
Proceedings O
of O
the O
2014 O
conference O
on O
empirical O
methods O
in O
natural O
language O
processing O
( O
EMNLP O
) O
, O
pages O
1532‚Äì1543 O
. O
Soujanya O
Poria O
, O
Erik O
Cambria O
, O
Devamanyu O
Hazarika O
, O
Navonil O
Majumder O
, O
Amir O
Zadeh O
, O
and O
Louis O
- O
Philippe O
Morency O
. O
2017 O
. O
Context O
- O
dependent O
sentiment O
analysis O
in O
user O
- O
generated O
videos O
. O
In O
Proceedings O
of O
the O
55th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
873‚Äì883 O
. O
Rohit O
Saxena O
, O
Savita O
Bhat O
, O
and O
Niranjan O
Pedanekar O
. O
2018 O
. O
Emotionx O
- O
area66 O
: O
Predicting O
emotions O
in O
dialogues O
using O
hierarchical O
attention O
network O
with O
sequence O
labeling O
. O
In O
Proceedings O
of O
the O
Sixth O
International O
Workshop O
on O
Natural O
Language O
Processing O
for O
Social O
Media O
, O
pages O
50‚Äì55 O
. O
Bj¬®orn O
Schuller O
, O
Michel O
Valster O
, O
Florian O
Eyben O
, O
Roddy O
Cowie O
, O
and O
Maja O
Pantic O
. O
2012 O
. O
Avec O
2012 O
: O
the O
continuous O
audio O
/ O
visual O
emotion O
challenge O
. O
In O
Proceedings O
of O
the O
14th O
ACM O
international O
conference O
on O
Multimodal O
interaction O
, O
pages O
449‚Äì456 O
. O
ACM O
. O
Sainbayar O
Sukhbaatar O
, O
Jason O
Weston O
, O
Rob O
Fergus O
, O
et O
al O
. O
2015 O
. O
End O
- O
to O
- O
end O
memory O
networks O
. O
In O
Advances O
in O
neural O
information O
processing O
systems O
, O
pages O
2440‚Äì2448 O
. O
Duyu O
Tang O
, O
Bing O
Qin O
, O
and O
Ting O
Liu O
. O
2015 O
. O
Document O
modeling O
with O
gated O
recurrent O
neural O
network O
for O
sentiment O
classiÔ¨Åcation O
. O
In O
Proceedings O
of O
the O
2015 O
conference O
on O
empirical O
methods O
in O
natural O
language O
processing O
, O
pages O
1422‚Äì1432 O
. O
Johnny O
Torres O
. O
2018 O
. O
Emotionx O
- O
jtml O
: O
Detecting O
emotions O
with O
attention O
. O
In O
Proceedings O
of O
the O
Sixth O
International O
Workshop O
on O
Natural O
Language O
Processing O
for O
Social O
Media O
, O
pages O
56‚Äì60.41Ashish O
Vaswani O
, O
Noam O
Shazeer O
, O
Niki O
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion O
Jones O
, O
Aidan O
N O
Gomez O
, O
≈Åukasz O
Kaiser O
, O
and O
Illia O
Polosukhin O
. O
2017 O
. O
Attention O
is O
all O
you O
need O
. O
In O
Advances O
in O
neural O
information O
processing O
systems O
, O
pages O
5998‚Äì6008 O
. O
Zichao O
Yang O
, O
Diyi O
Yang O
, O
Chris O
Dyer O
, O
Xiaodong O
He O
, O
Alex O
Smola O
, O
and O
Eduard O
Hovy O
. O
2016 O
. O
Hierarchical O
attention O
networks O
for O
document O
classiÔ¨Åcation O
. O
InProceedings O
of O
the O
2016 O
conference O
of O
the O
North O
American O
chapter O
of O
the O
association O
for O
computational O
linguistics O
: O
human O
language O
technologies O
, O
pages O
1480‚Äì1489 O
. O
Tom O
Young O
, O
Erik O
Cambria O
, O
Iti O
Chaturvedi O
, O
Hao O
Zhou O
, O
Subham O
Biswas O
, O
and O
Minlie O
Huang O
. O
2018 O
. O
Augmenting O
end O
- O
to O
- O
end O
dialogue O
systems O
with O
commonsense O
knowledge O
. O
In O
Thirty O
- O
Second O
AAAI O
Conference O
on O
ArtiÔ¨Åcial O
Intelligence O
.42A O
Appendices O
Dataset O
Partition O
# O
of O
utterances O
# O
of O
dialogues O
IEMOCAPtrain O
5810 O
120 O
test O
1623 O
31 O
A O
VECtrain O
4368 O
63 O
test O
1430 O
32 O
Table O
3 O
: O
Data O
distribution O
of O
IEMOCAP O
and O
A O
VEC O
datasets O
. O
Training O
Details O
We O
use O
10 O
% O
of O
the O
training O
set O
as O
the O
validation O
set O
for O
hyper O
- O
parameters O
tuning O
. O
All O
tokens O
are O
lowercased O
with O
removal O
of O
stop O
words O
, O
symbols O
and O
digits O
, O
and O
sentences O
are O
zero O
- O
padded O
to O
the O
length O
of O
the O
longest O
sentence O
in O
the O
dataset O
. O
We O
alter O
the O
weight O
that O
each O
training O
instance O
carries O
when O
computing O
the O
loss O
to O
mitigate O
the O
inÔ¨Çuence O
of O
data O
imbalance O
. O
The O
weights O
are O
speciÔ¨Åc O
factors O
depending O
on O
corresponding O
emotions O
. O
Hyper O
- O
parameters O
IEMOCAP O
A O
VEC O
Optimizer O
Adam O
Adam O
Learning O
rate O
0.001 O
0.001 O
Batch O
size O
16 O
16 O
Bi O
- O
GRU O
layer O
2 O
2 O
Reasoning O
module O
layer O
2 O
2 O
Historical O
utterance O
30 O
20 O
GRU O
hidden O
size O
150 O
150 O
Attention O
head O
4 O
2 O
Attention O
hidden O
size O
256 O
256 O
Table O
4 O
: O
Hyper O
- O
parameters O
and O
settings O
used O
for O
the O
two O
datasets.43Proceedings O
of O
the O
1st O
Conference O
of O
the O
Asia O
- O
PaciÔ¨Åc O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
10th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
44‚Äì53 O
December O
4 O
- O
7 O
, O
2020 O
. O
¬© O
2020 O
Association O
for O
Computational O
Linguistics O
SentiRec O
: O
Sentiment O
Diversity O
- O
aware O
Neural O
News O
Recommendation O
Chuhan O
Wu‚Ä†Fangzhao O
Wu‚Ä°Tao O
Qi‚Ä†Yongfeng O
Huang‚Ä† O
‚Ä†Department O
of O
Electronic O
Engineering O
& O
BNRist O
, O
Tsinghua O
University O
, O
Beijing O
100084 O
, O
China O
‚Ä°Microsoft O
Research O
Asia O
, O
Beijing O
100080 O
, O
China O
{ O
wuchuhan15,wufangzhao O
, O
taoqi.qt O
} O
@gmail.com O
yfhuang@tsinghua.edu.cn O
Abstract O
Personalized O
news O
recommendation O
is O
important O
for O
online O
news O
services O
. O
Many O
news O
recommendation O
methods O
recommend O
news O
based O
on O
their O
relevance O
to O
users O
‚Äô O
historical O
browsed O
news O
, O
and O
the O
recommended O
news O
usually O
have O
similar O
sentiment O
with O
browsed O
news O
. O
However O
, O
if O
browsed O
news O
is O
dominated O
by O
certain O
kinds O
of O
sentiment O
, O
the O
model O
may O
intensively O
recommend O
news O
with O
the O
same O
sentiment O
orientation O
, O
making O
it O
difÔ¨Åcult O
for O
users O
to O
receive O
diverse O
opinions O
and O
news O
events O
. O
In O
this O
paper O
, O
we O
propose O
a O
sentiment O
diversity O
- O
aware O
neural O
news O
recommendation O
approach O
, O
which O
can O
recommend O
news O
with O
more O
diverse O
sentiment O
. O
In O
our O
approach O
, O
we O
propose O
a O
sentiment O
- O
aware O
news O
encoder O
, O
which O
is O
jointly O
trained O
with O
an O
auxiliary O
sentiment O
prediction O
task O
, O
to O
learn O
sentiment O
- O
aware O
news O
representations O
. O
We O
learn O
user O
representations O
from O
browsed O
news O
representations O
, O
and O
compute O
click O
scores O
based O
on O
user O
and O
candidate O
news O
representations O
. O
In O
addition O
, O
we O
propose O
a O
sentiment O
diversity O
regularization O
method O
to O
penalize O
the O
model O
by O
combining O
the O
overall O
sentiment O
orientation O
of O
browsed O
news O
as O
well O
as O
the O
click O
and O
sentiment O
scores O
of O
candidate O
news O
. O
Extensive O
experiments O
on O
real O
- O
world O
dataset O
show O
that O
our O
approach O
can O
effectively O
improve O
the O
sentiment O
diversity O
in O
news O
recommendation O
without O
performance O
sacriÔ¨Åce O
. O
1 O
Introduction O
Online O
news O
websites O
such O
as O
Google O
news1have O
gained O
huge O
popularity O
for O
consuming O
digital O
news O
( O
Das O
et O
al O
. O
, O
2007 O
) O
. O
However O
, O
it O
is O
difÔ¨Åcult O
for O
users O
to O
Ô¨Ånd O
their O
interested O
news O
information O
due O
to O
the O
huge O
volume O
of O
news O
emerging O
every O
day O
( O
Okura O
et O
al O
. O
, O
2017 O
) O
. O
Thus O
, O
personalized O
news O
recommendation O
is O
important O
for O
news O
websites O
to O
1https://news.google.com/ O
Driving O
the O
Highway O
1 O
  O
North O
of O
San O
Francisco O
' O
Snack O
Man O
' O
Helps O
Feed O
  O
The O
Homeless O
3 O
dead O
, O
3 O
injured O
in O
plane O
  O
crash O
west O
of O
Michigan O
  O
airport1 O
killed O
in O
shooting O
at O
  O
Washington O
state O
  O
apartment O
building4 O
Dead O
In O
Horrific O
Wrong O
Way O
Crash O
On O
Highway O
  O
101 O
In O
San O
Francisco O
Clicked O
News O
Candidate O
News O
Recommend O
‚Ä¶ O
‚Ä¶ O
Early O
morning O
Christmas O
  O
fire O
displaces O
250 O
at O
  O
shelter O
for O
homelessFigure O
1 O
: O
Several O
news O
browsed O
by O
two O
users O
of O
MSN O
News O
and O
the O
candidate O
news O
recommended O
to O
them O
. O
target O
user O
interest O
and O
alleviate O
information O
overload O
( O
Wu O
et O
al O
. O
, O
2019a O
) O
. O
Many O
existing O
news O
recommendation O
methods O
rank O
candidate O
news O
based O
on O
their O
relevance O
to O
the O
interests O
of O
users O
inferred O
from O
their O
historical O
browsed O
news O
( O
Okura O
et O
al O
. O
, O
2017 O
; O
Wu O
et O
al O
. O
, O
2019c O
) O
. O
For O
example O
, O
Okura O
et O
al O
. O
( O
2017 O
) O
proposed O
to O
learn O
news O
representations O
from O
news O
texts O
via O
autoencoders O
, O
and O
learn O
user O
representations O
from O
browsed O
news O
using O
a O
gated O
recurrent O
unit O
( O
GRU O
) O
network O
. O
They O
ranked O
candidate O
news O
based O
on O
the O
inner O
product O
of O
the O
user O
representation O
and O
candidate O
news O
representation O
. O
Wu O
et O
al O
. O
( O
2019c O
) O
proposed O
to O
learn O
news O
and O
user O
representations O
using O
multi O
- O
head O
self O
- O
attention O
networks O
. O
They O
ranked O
news O
based O
on O
the O
click O
scores O
computed O
by O
the O
dot O
product O
between O
news O
and O
user O
representations O
. O
The O
news O
articles O
recommended O
by O
these O
methods O
are O
usually O
similar O
to O
those O
previously O
browsed O
by O
a O
user O
in O
many O
aspects O
, O
such O
as O
content O
and O
sentiment O
. O
For O
example O
, O
in O
Fig O
. O
1 O
the O
two O
candidate O
news O
articles O
are O
recommended O
to O
both O
users O
. O
The O
Ô¨Årst O
user O
browses O
a O
news O
about O
the O
highway O
in O
San O
Francisco O
and O
a O
news O
about O
a O
person O
helping O
the O
homeless O
, O
which O
has O
inherent O
relatedness O
with O
the O
content O
of O
the O
candidate O
news O
. O
The O
second O
user O
browses O
several O
news O
about O
deadly O
accidents O
and44crime O
, O
which O
has O
the O
same O
sentiment O
orientation O
as O
the O
candidate O
news O
. O
However O
, O
like O
the O
recommendations O
for O
the O
second O
user O
in O
Fig O
. O
1 O
, O
if O
a O
user O
mainly O
browses O
news O
articles O
that O
have O
a O
certain O
kind O
of O
sentiment O
( O
e.g. O
, O
negative O
sentiment O
) O
, O
many O
existing O
methods O
may O
intensively O
recommend O
news O
with O
the O
same O
sentiment O
orientation O
, O
which O
is O
not O
beneÔ¨Åcial O
for O
this O
user O
to O
receive O
diverse O
opinions O
and O
news O
events O
that O
convey O
other O
sentiments O
. O
In O
this O
paper O
, O
we O
propose O
a O
sentiment O
diversityaware O
news O
recommendation O
approach O
named O
SentiRec O
, O
which O
can O
improve O
the O
sentiment O
diversity O
of O
news O
recommendation O
by O
considering O
the O
sentiment O
orientation O
of O
candidate O
and O
browsed O
news O
. O
In O
our O
approach O
, O
we O
propose O
a O
sentiment O
- O
aware O
news O
encoder O
, O
which O
is O
jointly O
trained O
with O
an O
auxiliary O
news O
sentiment O
prediction O
task O
, O
to O
incorporate O
sentiment O
information O
into O
news O
modeling O
and O
generate O
sentiment O
- O
aware O
news O
representations O
. O
We O
learn O
user O
representations O
from O
the O
representations O
of O
browsed O
news O
, O
and O
compute O
the O
click O
scores O
of O
candidate O
news O
based O
on O
their O
relevance O
to O
the O
user O
representations O
. O
In O
addition O
, O
to O
enhance O
the O
sentiment O
diversity O
of O
news O
recommendation O
, O
we O
propose O
a O
sentiment O
diversity O
regularization O
method O
to O
penalize O
our O
model O
during O
model O
training O
, O
which O
is O
based O
on O
the O
overall O
sentiment O
orientation O
of O
browsed O
news O
as O
well O
as O
the O
sentiment O
scores O
and O
click O
scores O
of O
candidate O
news O
. O
We O
conduct O
extensive O
experiments O
on O
a O
real O
- O
world O
benchmark O
dataset O
, O
and O
the O
results O
show O
that O
our O
approach O
can O
achieve O
better O
sentiment O
diversity O
and O
recommendation O
accuracy O
than O
many O
baseline O
methods O
. O
The O
contributions O
of O
this O
paper O
are O
summarized O
as O
follows O
: O
‚Ä¢To O
the O
best O
of O
our O
knowledge O
, O
this O
is O
the O
Ô¨Årst O
work O
that O
explores O
to O
improve O
the O
sentiment O
diversity O
of O
news O
recommendation O
. O
‚Ä¢We O
propose O
a O
sentiment O
- O
aware O
news O
encoder O
that O
incorporates O
an O
auxiliary O
news O
sentiment O
prediction O
task O
to O
encode O
sentiment O
- O
aware O
news O
representations O
. O
‚Ä¢We O
propose O
a O
sentiment O
diversity O
regularization O
method O
to O
encourage O
the O
model O
to O
recommend O
news O
with O
diverse O
sentiment O
from O
the O
browsed O
news O
. O
‚Ä¢Extensive O
experiments O
on O
real O
- O
world O
benchmark O
dataset O
verify O
that O
our O
approach O
can O
recommend O
news O
with O
diverse O
sentiment O
without O
performance O
loss.2 O
Related O
Work O
News O
recommendation O
is O
an O
important O
technique O
for O
online O
news O
websites O
to O
provide O
personalized O
news O
reading O
services O
( O
Zheng O
et O
al O
. O
, O
2018 O
) O
. O
A O
core O
problem O
in O
news O
recommendation O
is O
building O
accurate O
representations O
of O
news O
and O
users O
and O
further O
ranking O
candidate O
news O
according O
to O
news O
and O
user O
representations O
( O
Okura O
et O
al O
. O
, O
2017 O
) O
. O
In O
many O
news O
recommendation O
methods O
, O
news O
ranking O
is O
based O
on O
the O
representations O
of O
news O
and O
users O
built O
by O
manual O
feature O
engineering O
( O
Liu O
et O
al O
. O
, O
2010 O
; O
Capelle O
et O
al O
. O
, O
2012 O
; O
Son O
et O
al O
. O
, O
2013 O
; O
Karkali O
et O
al O
. O
, O
2013 O
; O
Garcin O
et O
al O
. O
, O
2013 O
; O
Bansal O
et O
al O
. O
, O
2015 O
; O
Ren O
et O
al O
. O
, O
2015 O
; O
Chen O
et O
al O
. O
, O
2017 O
; O
Zihayat O
et O
al O
. O
, O
2019 O
) O
. O
For O
example O
, O
Liu O
et O
al O
. O
( O
2010 O
) O
proposed O
to O
use O
topic O
categories O
and O
interest O
features O
generated O
by O
a O
Bayesian O
model O
to O
build O
news O
and O
user O
representations O
. O
They O
ranked O
candidate O
news O
based O
on O
the O
product O
of O
a O
content O
- O
based O
score O
computed O
from O
news O
representations O
and O
a O
Ô¨Ålter O
- O
based O
score O
computed O
by O
collaborative O
Ô¨Åltering O
. O
Son O
et O
al O
. O
( O
2013 O
) O
proposed O
an O
Explicit O
Localized O
Semantic O
Analysis O
( O
ELSA O
) O
model O
for O
location O
- O
based O
news O
recommendation O
. O
They O
proposed O
to O
represent O
news O
and O
users O
by O
extracting O
topic O
and O
location O
features O
from O
Wikipedia O
pages O
, O
and O
ranked O
news O
based O
on O
the O
cosine O
distance O
between O
the O
representations O
of O
news O
and O
user O
. O
Lian O
et O
al O
. O
( O
2018 O
) O
proposed O
to O
use O
various O
handcrafted O
features O
to O
represent O
news O
and O
users O
, O
such O
as O
title O
length O
, O
news O
categories O
, O
user O
proÔ¨Åles O
and O
features O
extracted O
from O
user O
behavior O
histories O
. O
They O
ranked O
candidate O
news O
based O
on O
the O
click O
scores O
computed O
by O
a O
neural O
factorization O
machine O
. O
However O
, O
these O
methods O
rely O
on O
manual O
feature O
engineering O
to O
build O
news O
and O
user O
representations O
, O
which O
usually O
necessitate O
massive O
expertise O
. O
In O
addition O
, O
handcrafted O
features O
may O
not O
be O
optimal O
in O
representing O
news O
content O
and O
user O
interest O
. O
In O
recent O
years O
, O
several O
news O
recommendation O
methods O
based O
on O
deep O
learning O
techniques O
are O
proposed O
( O
Okura O
et O
al O
. O
, O
2017 O
; O
Khattar O
et O
al O
. O
, O
2018 O
; O
Wang O
et O
al O
. O
, O
2018 O
; O
Wu O
et O
al O
. O
, O
2019a O
; O
An O
et O
al O
. O
, O
2019 O
; O
Wu O
et O
al O
. O
, O
2019b O
, O
c O
; O
Ge O
et O
al O
. O
, O
2020 O
) O
. O
For O
example O
, O
Okura O
et O
al O
. O
( O
2017 O
) O
proposed O
to O
learn O
Ô¨Årst O
news O
representations O
from O
news O
bodies O
using O
autoencoders O
, O
and O
then O
learn O
representations O
of O
users O
from O
their O
clicked O
news O
with O
a O
GRU O
network O
. O
Candidate O
news O
are O
ranked O
based O
on O
the O
click O
scores O
computed O
by O
the O
dot O
products O
between O
news O
and O
user O
representations O
. O
Wang O
et O
al O
. O
( O
2018 O
) O
proposed O
to O
learn O
news O
representations O
from O
news O
titles O
and45their O
entities O
via O
a O
knowledge O
- O
aware O
CNN O
network O
, O
and O
learn O
user O
representations O
from O
clicked O
news O
with O
a O
candidate O
- O
aware O
attention O
network O
. O
They O
ranked O
candidate O
news O
based O
on O
the O
click O
scores O
computed O
from O
the O
concatenation O
of O
news O
and O
user O
representations O
via O
a O
feed O
- O
forward O
neural O
network O
. O
Wu O
et O
al O
. O
( O
2019c O
) O
proposed O
to O
learn O
news O
and O
user O
representations O
with O
a O
combination O
of O
multi O
- O
head O
self O
- O
attention O
and O
additive O
attention O
networks O
. O
They O
also O
used O
dot O
product O
to O
compute O
click O
scores O
for O
news O
ranking O
. O
These O
methods O
tend O
to O
recommend O
news O
articles O
which O
are O
similar O
with O
the O
news O
users O
previously O
browsed O
( O
Lin O
et O
al O
. O
, O
2014 O
) O
. O
Thus O
, O
these O
methods O
may O
recommend O
news O
with O
similar O
sentiment O
orientation O
with O
those O
previously O
browsed O
by O
users O
, O
which O
is O
not O
beneÔ¨Åcial O
for O
users O
to O
receive O
diverse O
news O
information O
. O
Different O
from O
these O
methods O
, O
our O
approach O
can O
effectively O
recommend O
news O
with O
diverse O
sentiment O
to O
users O
by O
incorporating O
sentiment O
information O
into O
news O
modeling O
via O
a O
sentiment O
- O
aware O
news O
encoder O
and O
regularizing O
the O
model O
based O
on O
the O
sentiment O
orientation O
of O
browsed O
and O
candidate O
news O
. O
3 O
Our O
Approach O
In O
this O
section O
, O
we O
Ô¨Årst O
present O
the O
formal O
deÔ¨Ånitions O
of O
the O
problem O
explored O
in O
this O
paper O
, O
then O
introduce O
the O
details O
of O
our O
sentiment O
diversity O
- O
aware O
news O
recommendation O
( O
SentiRec O
) O
approach O
. O
3.1 O
Problem O
DeÔ¨Ånition O
The O
problem O
studied O
in O
this O
paper O
is O
deÔ¨Åned O
as O
follows O
. O
Given O
a O
user O
uwith O
her O
news O
browsing O
historyH= O
[ O
D1,D2, O
... O
,D O
N]and O
a O
set O
of O
candidate O
news2C= O
[ O
Dc O
1,Dc O
2, O
... O
,Dc O
P](NandPrespectively O
denote O
the O
number O
of O
browsed O
news O
and O
candidate O
news O
) O
, O
the O
goal O
of O
the O
news O
recommendation O
model O
is O
to O
predict O
the O
personalized O
click O
scores O
[ O
ÀÜy1,ÀÜy2, O
... O
,ÀÜyP]of O
these O
candidate O
news O
, O
which O
are O
further O
used O
for O
ranking O
and O
display O
. O
We O
denote O
the O
sentiment O
labels O
of O
the O
browsed O
news O
and O
candidate O
news O
as O
[ O
s1,s2, O
... O
,s O
N]and[sc O
1,sc O
2, O
... O
,sc O
P O
] O
, O
respectively O
. O
In O
this O
paper O
we O
assume O
the O
sentiment O
labels O
are O
real O
values O
from O
-1 O
to O
1 O
, O
which O
indicate O
the O
sentiment O
polarity O
of O
news O
articles O
. O
We O
denote O
the O
overall O
sentiment O
orientation O
of O
browsed O
news O
ass O
. O
The O
sentiment O
diversity O
is O
deÔ¨Åned O
as O
the O
differences O
between O
the O
sentiment O
orientation O
of O
recommended O
news O
and O
the O
overall O
sentiment O
2The O
candidate O
news O
set O
is O
usually O
recalled O
from O
the O
entire O
news O
pool.of O
browsed O
news.3The O
sentiment O
diversity O
of O
the O
news O
ranking O
results O
C O
/ O
primefor O
the O
useruis O
measured O
by O
a O
function O
d O
= O
f(C O
/ O
prime O
, O
s O
) O
. O
The O
recommendation O
diversity O
is O
better O
if O
more O
top O
ranked O
news O
in O
C O
/ O
prime O
have O
the O
different O
sentiment O
orientation O
with O
s. O
3.2 O
News O
Recommendation O
Framework O
In O
this O
section O
, O
we O
introduce O
the O
general O
news O
recommendation O
framework O
of O
our O
SentiRec O
approach O
, O
as O
shown O
in O
Fig O
. O
2 O
. O
There O
are O
three O
core O
components O
in O
this O
framework O
for O
news O
recommendation O
, O
i.e. O
, O
sentiment O
- O
aware O
( O
SA O
) O
news O
encoder O
, O
user O
encoder O
, O
and O
click O
predictor O
. O
The O
sentimentaware O
news O
encoder O
aims O
to O
learn O
representations O
of O
news O
articles O
from O
their O
texts O
, O
where O
their O
sentiments O
are O
taken O
into O
consideration O
. O
We O
apply O
the O
sentiment O
- O
aware O
news O
encoder O
to O
the O
browsed O
news O
[ O
D1,D2, O
... O
,D O
N]and O
the O
candidate O
news O
Dcto O
encode O
their O
sentiment O
- O
aware O
representations O
, O
which O
are O
respectively O
denoted O
as O
[ O
r1,r2, O
... O
,rN]andrc O
. O
The O
user O
encoder O
aims O
to O
learn O
representations O
of O
users O
from O
the O
sentiment O
- O
aware O
representations O
of O
their O
browsed O
news O
. O
Motivated O
by O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
we O
use O
Transformer O
to O
capture O
the O
relatedness O
between O
browsed O
news O
and O
learn O
a O
uniÔ¨Åed O
representation O
ufor O
each O
user O
. O
The O
click O
predictor O
aims O
to O
compute O
the O
personalized O
click O
scores O
of O
candidate O
news O
by O
measuring O
the O
relevance O
between O
user O
and O
candidate O
news O
representations O
. O
Following O
many O
previous O
works O
( O
Okura O
et O
al O
. O
, O
2017 O
; O
Wu O
et O
al O
. O
, O
2019b O
) O
, O
we O
use O
dot O
product O
to O
implement O
the O
click O
predictor O
, O
and O
the O
click O
score O
ÀÜyis O
predicted O
by O
ÀÜy O
= O
u O
/ O
latticetoprc O
. O
3.3 O
Sentiment O
- O
Aware O
News O
Encoder O
In O
this O
section O
, O
we O
introduce O
the O
details O
of O
the O
sentiment O
- O
aware O
news O
encoders O
in O
our O
SentiRec O
approach O
. O
Its O
architecture O
is O
shown O
in O
Fig O
. O
3 O
. O
Motivated O
by O
the O
news O
encoder O
in O
( O
Wu O
et O
al O
. O
, O
2019c O
) O
, O
we O
Ô¨Årst O
use O
a O
word O
embedding O
layer O
to O
convert O
the O
sequence O
of O
words O
in O
a O
news O
title O
into O
a O
sequence O
of O
semantic O
vectors O
, O
and O
then O
use O
a O
Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
to O
capture O
the O
contexts O
of O
words O
and O
build O
a O
uniÔ¨Åed O
rrepresentation O
of O
news O
texts O
. O
However O
, O
the O
news O
representations O
directly O
learned O
by O
the O
Transformer O
are O
usually O
not O
sentiment O
- O
bearing O
. O
In O
fact O
, O
the O
sentiment O
information O
of O
news O
is O
very O
important O
for O
understanding O
3We O
do O
not O
strictly O
require O
the O
recommendation O
results O
in O
an O
impression O
to O
be O
diverse O
in O
sentiment O
. O
We O
expect O
the O
sentiment O
of O
recommended O
news O
in O
a O
long O
term O
( O
e.g. O
, O
multiple O
impressions O
in O
months O
) O
is O
diverse.46ùê∑1 O
Browsed O
Newsùê∑2 O
ùê∑ùëêùíñClick O
Predictorùëù O
Click O
  O
Score O
Candidate O
Newsùíìùüè O
SA O
News O
  O
Encoderùíìùëµ O
SANews O
  O
EncoderùíìùüêUser O
Encoder O
SA O
News O
  O
Encoderùíìùëê O
ùë†ùëê O
ùë†1 O
ùë†2Sentiment O
  O
Monitor‡∑ùùíöSentiment O
Diversity O
Score O
Sentiment O
  O
AnalyzerSentiment O
  O
AnalyzerSentiment O
  O
Analyzer“ßùë† O
ùê∑ùëÅSANews O
  O
Encoderùë†ùëÅ O
Sentiment O
  O
Analyzer O
Figure O
2 O
: O
The O
framework O
of O
our O
SentiRec O
approach O
. O
Transformer O
Word O
EmbeddingùíÜ1ùíÜ2 O
ùíÜùëÄùíì O
Sentiment O
  O
Predictor∆∏ùë†Sentiment O
ScoreNews O
Representation O
Driving O
the O
Francisco O
Figure O
3 O
: O
The O
architecture O
of O
the O
sentiment O
- O
aware O
news O
encoder O
. O
the O
content O
of O
news O
. O
For O
example O
, O
in O
Fig O
. O
1 O
, O
although O
the O
news O
‚Äú O
Early O
morning O
... O
‚Äù O
and O
‚Äú O
Snack O
Man O
... O
‚Äù O
are O
both O
related O
to O
the O
homeless O
, O
they O
have O
opposite O
sentiment O
polarity O
, O
and O
modeling O
the O
sentiment O
of O
them O
can O
help O
understand O
their O
content O
better O
. O
In O
addition O
, O
the O
sentiment O
of O
news O
can O
also O
provide O
useful O
clues O
for O
user O
modeling O
and O
news O
ranking O
. O
For O
example O
, O
if O
a O
user O
frequently O
clicks O
negative O
news O
as O
the O
second O
user O
in O
Fig O
. O
1 O
, O
it O
may O
be O
more O
appropriate O
to O
recommend O
several O
positive O
news O
to O
this O
user O
rather O
than O
continuously O
recommending O
similar O
negative O
news O
. O
Thus O
, O
modeling O
news O
sentiment O
has O
the O
potential O
to O
enhance O
news O
recommendation O
. O
However O
, O
since O
the O
sentiment O
scores O
of O
news O
are O
numerical O
variables O
, O
simply O
re O
- O
garding O
them O
as O
model O
input O
may O
be O
not O
optimal O
. O
Thus O
, O
we O
propose O
an O
auxiliary O
sentiment O
prediction O
task O
, O
and O
we O
jointly O
train O
the O
news O
encoder O
with O
this O
task O
to O
encourage O
it O
to O
learn O
sentiment O
- O
aware O
news O
representations O
. O
The O
real O
- O
valued O
sentiment O
score O
ÀÜsis O
predicted O
as O
follows O
: O
ÀÜs O
= O
Vs√ór+vs O
, O
( O
1 O
) O
where O
Vsandvsare O
parameters O
. O
The O
loss O
function O
of O
sentiment O
prediction O
we O
use O
is O
the O
mean O
absolute O
error O
( O
MAE O
) O
, O
which O
is O
formulated O
as O
follows O
: O
Lsenti=1 O
SS O
/ O
summationdisplay O
i=1|ÀÜsi‚àísi| O
, O
( O
2 O
) O
where O
ÀÜsiandsirespectively O
stand O
for O
the O
predicted O
sentiment O
score O
and O
sentiment O
label O
of O
the O
i O
- O
th O
news O
, O
andSdenotes O
the O
number O
of O
news O
. O
The O
sentiment O
labels O
are O
obtained O
by O
the O
sentiment O
analyzer O
modules O
in O
Fig O
. O
2 O
, O
which O
can O
be O
implemented O
by O
many O
sentiment O
analysis O
methods O
. O
3.4 O
Sentiment O
Diversity O
Regularization O
To O
further O
improve O
the O
sentiment O
diversity O
of O
news O
recommendation O
, O
we O
propose O
a O
sentiment O
diversity O
regularization O
method O
to O
penalize O
the O
recommendation O
model O
according O
to O
the O
overall O
sentiment O
score O
of O
browsed O
news O
, O
the O
sentiment O
score O
of O
candidate O
news O
, O
and O
its O
predicted O
click O
score O
. O
As O
shown O
in O
Fig O
. O
2 O
, O
we O
Ô¨Årst O
use O
the O
sentiment O
analyzer O
to O
obtain O
the O
sentiment O
scores O
of O
the O
candidate47news O
( O
denoted O
as O
sc O
) O
and O
browsed O
news O
( O
denoted O
as[s1,s2, O
... O
,s O
N O
] O
) O
. O
We O
then O
compute O
an O
overall O
sentiment O
score4of O
browsed O
news O
to O
indicate O
the O
historical O
sentiment O
preference O
of O
a O
user O
as O
follows O
: O
¬Øs=1 O
NN O
/ O
summationdisplay O
i=1si O
. O
( O
3 O
) O
A O
positive O
¬Øsindicates O
that O
the O
user O
has O
read O
news O
with O
more O
positive O
sentiment O
and O
a O
negative O
¬Øsindicates O
the O
negative O
sentiment O
is O
dominant O
in O
the O
browsed O
news O
. O
If O
the O
news O
recommender O
intensively O
recommends O
news O
with O
the O
same O
sentiment O
polarity O
with O
the O
overall O
sentiment O
sof O
a O
user O
‚Äôs O
browsed O
news O
, O
it O
is O
difÔ¨Åcult O
for O
this O
user O
to O
receive O
diverse O
news O
information O
. O
Thus O
, O
it O
is O
important O
to O
recommend O
news O
with O
diverse O
sentiment O
to O
users O
. O
To O
solve O
this O
problem O
, O
we O
propose O
a O
sentiment O
diversity O
regularization O
method O
. O
We O
Ô¨Årst O
propose O
to O
compute O
a O
sentiment O
diversity O
score O
pwith O
a O
sentiment O
monitor O
, O
which O
is O
formulated O
as O
follows O
: O
p O
= O
max(0,¬ØsscÀÜy O
) O
, O
( O
4 O
) O
where O
a O
larger O
score O
of O
pindicates O
less O
sentiment O
diversity O
. O
In O
this O
formula O
, O
for O
a O
candidate O
news O
that O
shares O
the O
same O
sentiment O
polarity O
with O
s O
, O
the O
score O
pis O
larger O
if O
the O
model O
assigns O
it O
a O
higher O
click O
score O
or O
its O
sentiment O
and O
the O
overall O
browsed O
news O
sentiment O
are O
more O
intense O
, O
which O
indicate O
that O
the O
recommendation O
is O
less O
diverse O
in O
sentiment O
. O
Then O
, O
we O
propose O
a O
sentiment O
diversity O
loss O
function O
to O
regularize O
our O
model O
as O
follows O
: O
Ldiv=1 O
|S|/summationdisplay O
i‚ààSpi O
, O
( O
5 O
) O
whereSis O
the O
data O
set O
for O
model O
training O
, O
and O
pi O
denotes O
the O
sentiment O
diversity O
score O
of O
the O
i O
- O
th O
sample O
inS. O
3.5 O
Model O
Training O
In O
this O
section O
, O
we O
introduce O
how O
to O
train O
the O
models O
in O
our O
SentiRec O
approach O
. O
Following O
( O
Huang O
et O
al O
. O
, O
2013 O
; O
Wu O
et O
al O
. O
, O
2019c O
) O
, O
we O
use O
negative O
sampling O
techniques O
to O
construct O
labeled O
data O
for O
the O
news O
recommendation O
task O
from O
the O
user O
impression O
logs O
. O
More O
speciÔ¨Åcally O
, O
for O
each O
news O
clicked O
by O
a O
user O
, O
we O
randomly O
sample O
Knews O
displayed O
in O
the O
same O
impression O
which O
are O
not O
clicked O
by O
4We O
do O
not O
incorporate O
the O
numbers O
of O
positive O
and O
negative O
news O
because O
they O
can O
not O
take O
the O
sentiment O
intensity O
into O
consideration.this O
user O
. O
We O
denote O
the O
click O
scores O
of O
the O
i O
- O
th O
clicked O
news O
as O
ÀÜy+ O
iand O
the O
associated O
Knon O
- O
click O
news O
as O
[ O
ÀÜy‚àí O
i,1,ÀÜy‚àí O
i,2, O
... O
,ÀÜy‚àí O
i O
, O
K O
] O
. O
we O
use O
the O
click O
predictor O
to O
jointly O
predict O
these O
scores O
, O
and O
normalize O
these O
scores O
via O
the O
softmax O
function O
to O
compute O
the O
click O
probability O
scores O
. O
The O
news O
recommendation O
loss O
we O
used O
is O
the O
negative O
log O
- O
likelihood O
of O
the O
clicked O
news O
samples O
, O
which O
is O
computed O
as O
: O
Lrec=/summationdisplay O
i‚ààSlog(exp(ÀÜy+ O
i O
) O
exp(ÀÜy+ O
i O
) O
+ O
/summationtextK O
j=1exp(ÀÜy‚àí O
i O
, O
j)),(6 O
) O
whereSis O
the O
data O
set O
for O
model O
training O
. O
We O
jointly O
train O
the O
news O
recommendation O
model O
with O
the O
auxiliary O
sentiment O
prediction O
task O
and O
meanwhile O
regularize O
it O
using O
the O
sentiment O
diversity O
loss O
. O
The O
Ô¨Ånal O
uniÔ¨Åed O
loss O
function O
of O
our O
approach O
is O
a O
weighted O
summation O
of O
the O
three O
loss O
functions O
, O
which O
is O
formulated O
as O
follows O
: O
L O
= O
Lrec+ŒªLsenti+¬µLdiv O
, O
( O
7 O
) O
whereŒªand¬µare O
coefÔ¨Åcients O
to O
control O
the O
relative O
importance O
of O
the O
sentiment O
prediction O
loss O
and O
sentiment O
diversity O
regularization O
loss O
. O
4 O
Experiments O
4.1 O
Datasets O
and O
Experimental O
Settings O
Our O
experiments O
were O
conducted O
on O
a O
real O
- O
world O
news O
recommendation O
dataset O
provided O
by O
( O
Wu O
et O
al O
. O
, O
2019b O
) O
, O
which O
is O
constructed O
from O
MSN O
News5logs O
from O
Oct. O
31 O
, O
2018 O
to O
Jan. O
29 O
, O
2019 O
. O
We O
use O
the O
logs O
in O
the O
last O
week O
as O
the O
test O
set O
and O
the O
rest O
are O
used O
for O
training O
and O
validation O
, O
where O
the O
split O
ratio O
is O
9:1.6To O
obtain O
the O
sentiment O
labels O
of O
the O
news O
in O
this O
dataset O
, O
we O
use O
the O
V O
ADER O
algorithm O
( O
Hutto O
and O
Gilbert O
, O
2014 O
) O
as O
the O
sentiment O
analyzer O
in O
our O
approach.7It O
is O
a O
famous O
sentiment O
analysis O
method O
based O
on O
a O
set O
of O
sentiment O
lexicons O
such O
as O
LIWC O
( O
Pennebaker O
et O
al O
. O
, O
2001 O
) O
, O
ANEW O
( O
Nielsen O
, O
2011 O
) O
and O
GI O
( O
Stone O
et O
al O
. O
, O
1966 O
) O
. O
We O
use O
V O
ADER O
to O
compute O
an O
overall O
sentiment O
orientation O
score O
of O
each O
news O
as O
the O
gold O
label O
, O
and O
these O
scores O
are O
ranged O
in O
[ O
-1 O
, O
1 O
] O
. O
The O
detailed O
statistics O
of O
the O
news O
recommendation O
dataset O
are O
shown O
in O
Table O
1 O
. O
We O
also O
plot O
the O
distribution O
of O
news O
sentiment O
scores O
and O
the O
overall O
sentiment O
orientation O
of O
users O
‚Äô O
browsed O
news O
5https://www.msn.com/en-us/news O
6The O
numbers O
of O
constructed O
samples O
for O
training O
and O
validation O
are O
277,811 O
and O
30,868 O
, O
respectively O
. O
The O
number O
of O
samples O
for O
test O
is O
1,707,588 O
. O
7We O
choose O
this O
algorithm O
because O
it O
can O
compute O
the O
real O
- O
valued O
sentiment O
scores O
rather O
than O
polarity O
only.48 O
# O
users O
10,000 O
avg O
. O
# O
words O
per O
title O
11.29 O
# O
news O
42,255 O
# O
click O
samples O
489,644 O
# O
impressions O
445,230 O
# O
non O
- O
click O
samples O
6,651,940 O
# O
samples O
7,141,584 O
avg O
. O
sentiment O
score O
0.0314 O
Table O
1 O
: O
Statistics O
of O
the O
dataset O
. O
-1.0 O
-0.6 O
-0.2 O
0.2 O
0.6 O
1.0Sentiment O
Polarity O
Score012345Density O
( O
a O
) O
News O
sentiment O
polarity O
scores O
. O
-1.0 O
-0.6 O
-0.2 O
0.2 O
0.6 O
1.0Overall O
Sentiment O
Polarity O
Score012345Density(b O
) O
Overall O
sentiment O
orientation O
of O
users O
‚Äô O
browsed O
news O
. O
Figure O
4 O
: O
Distributions O
of O
the O
news O
sentiment O
polarity O
scores O
and O
the O
overall O
sentiment O
orientation O
of O
users O
‚Äô O
browsed O
news O
in O
our O
dataset O
. O
in O
Figs O
. O
4(a O
) O
and O
4(b O
) O
, O
respectively O
. O
We O
Ô¨Ånd O
that O
although O
positive O
and O
negative O
news O
are O
almost O
balanced O
, O
the O
overall O
sentiment O
orientation O
of O
users O
‚Äô O
browsed O
news O
is O
negative O
. O
In O
addition O
, O
we O
show O
the O
average O
click O
- O
through O
rate O
( O
CTR O
) O
of O
news O
with O
different O
ranges O
of O
sentiment O
scores O
in O
Fig O
. O
5 O
. O
As O
the O
saying O
goes O
, O
‚Äú O
evil O
news O
rides O
fast O
, O
while O
good O
news O
baits O
later O
‚Äù O
. O
We O
Ô¨Ånd O
it O
is O
interesting O
that O
more O
negative O
news O
have O
higher O
CTRs O
, O
which O
indicates O
that O
negative O
news O
has O
stronger O
ability O
in O
attracting O
news O
clicks O
. O
Thus O
, O
it O
is O
important O
to O
recommend O
news O
with O
diverse O
sentiment O
to O
avoid O
overwhelming O
users O
with O
too O
much O
negative O
news O
information O
. O
Following O
Wu O
et O
al O
. O
( O
2019b O
) O
, O
in O
our O
experiments O
the O
word O
embeddings O
were O
initialized O
by O
the O
300dimensional O
Glove O
embeddings O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
. O
The O
negative O
sampling O
ratio O
Kwas O
set O
to O
4 O
. O
Adam O
( O
Kingma O
and O
Ba O
, O
2014 O
) O
was O
chosen O
as O
the O
optimizer O
and O
the O
size O
of O
a O
minibatch O
was O
30 O
. O
In O
addition O
, O
the O
loss O
weights O
Œªand¬µwere O
respectively O
set O
to O
0.4 O
and O
10 O
. O
These O
hyperparameters O
were O
tuned O
on O
the O
validation O
set O
. O
To O
evaluate O
the O
performance O
of O
news O
recommendation O
, O
we O
use O
metrics O
including O
AUC O
, O
MRR O
, O
nDCG@5 O
and O
nDCG@108 O
. O
Since O
there O
is O
no O
off O
- O
the O
- O
shelf O
metric O
to O
evaluate O
the O
sentiment O
diversity O
of O
news O
recommendation O
, O
motivated O
by O
the O
MRR O
and O
hit O
ratio O
metrics O
, O
we O
propose O
three O
metrics O
named O
Senti O
MRR O
, O
Senti O
@5 O
andSenti O
@10 O
to O
quantitatively O
measure O
sentiment O
8The O
relevance O
grade O
is O
binary O
, O
i.e. O
, O
0 O
for O
non O
- O
clicked O
news O
and O
1 O
for O
clicked O
news O
. O
[ O
-1.0,-0.6 O
) O
[ O
-0.6,-0.2 O
) O
[ O
-0.2,0.2 O
) O
[ O
0.2,0.6 O
) O
[ O
0.6,1.0 O
] O
Sentiment O
Score0.000.010.020.030.040.050.06CTRFigure O
5 O
: O
Click O
- O
through O
rates O
of O
news O
with O
different O
sentiment O
polarity O
scores O
. O
diversity O
. O
They O
are O
computed O
as O
follows O
: O
Senti O
MRR O
= O
max(0,¬ØsC O
/ O
summationdisplay O
i=1sc O
i O
i O
) O
, O
Senti O
@5 O
= O
max(0,¬Øs5 O
/ O
summationdisplay O
i=1sc O
i O
) O
, O
Senti O
@10 O
= O
max(0,¬Øs10 O
/ O
summationdisplay O
i=1sc O
i),(8 O
) O
whereCis O
the O
number O
of O
candidate O
news O
in O
an O
impression O
, O
sc O
idenotes O
the O
sentiment O
score O
of O
the O
candidate O
news O
with O
the O
i O
- O
th O
highest O
click O
score O
. O
In O
these O
metrics O
, O
higher O
scores O
indicate O
that O
the O
recommendation O
results O
are O
less O
diverse O
from O
the O
browsed O
news O
in O
their O
sentiment.9We O
repeated O
each O
experiment O
10 O
times O
and O
reported O
the O
average O
results O
over O
all O
impressions O
in O
terms O
of O
the O
recommendation O
performance O
and O
sentiment O
diversity O
. O
4.2 O
Performance O
Evaluation O
We O
evaluate O
the O
recommendation O
performance O
and O
sentiment O
diversity O
of O
our O
approach O
by O
comparing O
it O
with O
several O
baseline O
methods O
, O
including O
: O
( O
1)LibFM O
( O
Rendle O
, O
2012 O
) O
, O
a O
feature O
- O
based O
recommendation O
method O
based O
on O
factorization O
machine O
. O
TF O
- O
IDF O
features O
are O
used O
to O
represent O
the O
textual O
content O
of O
news O
. O
( O
2 O
) O
EBNR O
( O
Okura O
et O
al O
. O
, O
2017 O
) O
, O
an O
embedding O
- O
based O
neural O
news O
recommendation O
method O
. O
It O
uses O
denoising O
autoencoders O
to O
learn O
news O
representations O
and O
a O
GRU O
network O
to O
encode O
user O
representations O
. O
( O
3 O
) O
DKN O
( O
Wang O
et O
al O
. O
, O
2018 O
) O
, O
a O
knowledge O
- O
aware O
news O
recommendation O
method O
, O
which O
learns O
news O
representations O
via O
knowledgeaware O
CNN O
networks O
and O
learns O
user O
representations O
with O
a O
candidate O
- O
aware O
attention O
network O
. O
9The O
scores O
are O
positive O
if O
the O
top O
ranked O
news O
have O
the O
same O
sentiment O
orientation O
with O
the O
overall O
sentiment O
, O
and O
are O
higher O
if O
these O
sentiments O
are O
more O
intensive.49Methods O
AUC O
MRR O
nDCG@5 O
nDCG@10 O
LibFM O
0.5661 O
0.2414 O
0.2689 O
0.3552 O
EBNR O
0.6102 O
0.2811 O
0.3035 O
0.3952 O
DKN O
0.6032 O
0.2744 O
0.2967 O
0.3873 O
Conv3D O
0.6051 O
0.2765 O
0.2987 O
0.3904 O
DAN O
0.6154 O
0.2860 O
0.3093 O
0.3996 O
NPA O
0.6240 O
0.2952 O
0.3185 O
0.4094 O
NAML O
0.6205 O
0.2902 O
0.3144 O
0.4060 O
NRMS O
0.6275 O
0.2985 O
0.3217 O
0.4139 O
SentiRec O
0.6294 O
0.3013 O
0.3237 O
0.4165 O
SentiRec O
- O
same O
0.6299 O
0.3017 O
0.3240 O
0.4171 O
Table O
2 O
: O
Results O
of O
recommendation O
performance O
. O
Higher O
scores O
indicate O
better O
performance O
. O
( O
4)Conv3D O
( O
Khattar O
et O
al O
. O
, O
2018 O
) O
, O
a O
neural O
news O
recommendation O
method O
which O
learns O
news O
representations O
using O
2 O
- O
D O
CNN O
models O
and O
learns O
user O
representations O
using O
a O
3 O
- O
D O
CNN O
model O
. O
( O
5 O
) O
DAN O
( O
Zhu O
et O
al O
. O
, O
2019 O
) O
, O
a O
neural O
news O
recommendation O
method O
which O
learns O
news O
representations O
from O
title O
and O
entities O
with O
two O
independent O
CNN O
models O
and O
learns O
user O
representations O
using O
attentive O
LSTM O
network O
. O
( O
6 O
) O
NPA O
( O
Wu O
et O
al O
. O
, O
2019b O
) O
, O
a O
neural O
news O
recommendation O
method O
which O
learns O
news O
and O
user O
representations O
via O
personalized O
attention O
mechanism O
. O
( O
7 O
) O
NAML O
( O
Wu O
et O
al O
. O
, O
2019a O
) O
, O
a O
neural O
news O
recommendation O
method O
which O
learns O
news O
representations O
with O
CNN O
models O
and O
learns O
user O
representations O
using O
attention O
networks O
. O
( O
8) O
NRMS O
( O
Wu O
et O
al O
. O
, O
2019c O
) O
, O
a O
neural O
news O
recommendation O
method O
which O
learns O
news O
and O
representations O
using O
multi O
- O
head O
self O
- O
attention O
and O
additive O
attention O
networks O
. O
For O
fair O
comparison O
, O
in O
all O
methods O
we O
used O
news O
titles O
to O
learn O
news O
representations O
. O
In O
addition O
, O
we O
compare O
the O
sentiment O
diversity O
of O
random O
news O
ranking O
, O
which O
aims O
to O
show O
the O
benchmark O
sentiment O
diversity O
without O
news O
and O
user O
modeling O
. O
Besides O
, O
we O
also O
compare O
a O
variant O
of O
our O
method O
( O
denoted O
as O
SentiRec O
- O
same O
) O
which O
only O
recommends O
the O
news O
with O
the O
same O
sentiment O
polarity O
with O
the O
browsed O
news O
( O
Ô¨Ålter O
the O
candidate O
news O
with O
different O
sentiment O
polarity O
) O
, O
which O
aims O
to O
show O
the O
scores O
of O
an O
extreme O
case O
with O
minimal O
sentiment O
diversity O
. O
The O
results O
of O
recommendation O
performance O
and O
sentiment O
diversity O
are O
summarized O
in O
Tables O
2 O
and O
3 O
. O
From O
these O
results O
, O
we O
Ô¨Ånd O
that O
neural O
news O
recommendation O
approaches O
achieve O
better O
recommendation O
performance O
than O
LibFM O
. O
This O
is O
probably O
because O
neural O
networks O
can O
learn O
more O
informative O
news O
and O
user O
representations O
than O
traditional O
matrix O
factorization O
methods O
. O
However O
, O
compared O
with O
random O
ranking O
, O
we O
Ô¨Ånd O
that O
the O
diver O
- O
Methods O
Senti O
MRRSenti O
@5Senti O
@10 O
Random O
0.0262 O
0.0442 O
0.0687 O
LibFM O
0.0843 O
0.1192 O
0.2579 O
EBNR O
0.0989 O
0.1476 O
0.2868 O
DKN O
0.0954 O
0.1389 O
0.2810 O
Conv3D O
0.0973 O
0.1431 O
0.2830 O
DAN O
0.1005 O
0.1520 O
0.2897 O
NPA O
0.1044 O
0.1583 O
0.3015 O
NAML O
0.1030 O
0.1569 O
0.2967 O
NRMS O
0.1066 O
0.1592 O
0.3034 O
SentiRec O
0.0046 O
0.0083 O
0.0115 O
SentiRec O
- O
same O
0.3271 O
0.4963 O
0.9373 O
Table O
3 O
: O
Results O
of O
sentiment O
diversity O
. O
Lower O
scores O
indicate O
better O
sentiment O
diversity O
. O
sity O
scores O
of O
all O
baseline O
methods O
are O
much O
larger O
, O
especially O
those O
based O
on O
neural O
networks O
. O
This O
is O
probably O
because O
the O
compared O
baseline O
methods O
mainly O
recommend O
news O
based O
on O
the O
relevance O
between O
candidate O
news O
and O
browsed O
news O
, O
and O
will O
tend O
to O
recommend O
news O
with O
similar O
sentiment O
orientation O
with O
browsed O
news O
, O
which O
is O
harmful O
for O
users O
to O
receive O
diverse O
news O
information O
. O
Different O
from O
baseline O
methods O
, O
our O
SentiRec O
approach O
can O
achieve O
much O
better O
sentiment O
diversity O
even O
than O
random O
ranking O
. O
These O
results O
show O
that O
our O
approach O
can O
actively O
recommend O
news O
with O
diverse O
sentiment O
from O
browsed O
news O
. O
In O
addition O
, O
our O
approach O
can O
also O
achieve O
better O
recommendation O
performance O
than O
baseline O
methods O
. O
These O
results O
validate O
that O
our O
approach O
can O
achieve O
the O
goal O
of O
improving O
sentiment O
diversity O
in O
news O
recommendation O
without O
hurting O
the O
recommendation O
performance O
. O
Besides O
, O
by O
comparing O
SentiRec O
and O
its O
variant O
SentiRec O
- O
same O
, O
although O
the O
recommendation O
performance O
of O
SentiRec O
- O
same O
is O
slightly O
better O
, O
the O
sentiment O
of O
its O
recommendation O
results O
are O
minimally O
diverse O
from O
browsed O
news O
, O
which O
may O
amplify O
the O
problem O
of O
Ô¨Ålter O
bubble O
and O
hurt O
user O
experience O
. O
4.3 O
Ablation O
Study O
In O
this O
section O
, O
we O
conduct O
ablation O
studies O
to O
verify O
the O
inÔ¨Çuence O
of O
the O
auxiliary O
sentiment O
prediction O
task O
in O
the O
sentiment O
- O
aware O
news O
encoder O
and O
the O
sentiment O
diversity O
regularization O
method O
on O
the O
recommendation O
performance O
and O
sentiment O
diversity O
. O
The O
results O
are O
shown O
in O
Fig O
. O
6 O
. O
From O
Fig O
. O
6 O
, O
we O
Ô¨Ånd O
that O
the O
sentiment O
prediction O
task O
can O
improve O
both O
sentiment O
diversity O
and O
recommendation O
performance O
. O
This O
may O
be O
because O
this O
auxiliary O
task O
can O
encourage O
the O
news O
encoder O
to50AUC61.562.062.563.063.5AUCNone O
+ O
Sentiment O
Prediction O
+ O
Diversity O
Regularization O
SentiRecFigure O
6 O
: O
InÔ¨Çuence O
of O
the O
sentiment O
prediction O
task O
and O
sentiment O
diversity O
regularization O
method O
. O
0.00.10.20.30.40.50.662.562.662.862.963.0AUC O
AUC O
Figure O
7 O
: O
InÔ¨Çuence O
of O
the O
hyperparameter O
Œª O
. O
exploit O
the O
sentiment O
information O
in O
news O
texts O
to O
encode O
sentiment O
- O
aware O
news O
representations O
, O
which O
is O
beneÔ¨Åcial O
for O
predicting O
news O
clicks O
more O
accurately O
and O
further O
improving O
sentiment O
diversity O
by O
modeling O
users O
‚Äô O
dynamic O
preferences O
on O
news O
sentiment O
. O
In O
addition O
, O
the O
sentiment O
diversity O
regularization O
can O
also O
effectively O
improve O
the O
sentiment O
diversity O
of O
news O
recommendation O
and O
meanwhile O
keep O
the O
recommendation O
performance O
. O
This O
is O
because O
this O
regularization O
method O
can O
enforce O
the O
model O
to O
recommend O
news O
with O
different O
sentiment O
orientations O
with O
the O
browsed O
news O
. O
Moreover O
, O
combining O
both O
techniques O
can O
further O
improve O
sentiment O
diversity O
, O
which O
veriÔ¨Åes O
the O
effectiveness O
of O
our O
SentiRec O
method O
. O
4.4 O
InÔ¨Çuence O
of O
Hyperparameters O
In O
this O
section O
, O
we O
will O
explore O
the O
inÔ¨Çuence O
of O
two O
important O
hyperparameters O
on O
our O
approach O
, O
i.e. O
, O
the O
loss O
coefÔ¨Åcients O
Œªand¬µin O
Eq O
. O
( O
7 O
) O
on O
the O
performance O
and O
sentiment O
diversity O
of O
our O
approach O
. O
Since O
there O
are O
two O
hyperparameters O
, O
we O
Ô¨Årst O
vary O
the O
value O
of O
Œªto O
Ô¨Ånd O
the O
optimal O
one O
to O
learn O
sentiment O
- O
aware O
news O
representations O
in O
our O
approach O
. O
The O
results O
are O
illustrated O
in O
Fig O
. O
7 O
. O
According O
to O
Fig.7 O
, O
we O
Ô¨Ånd O
both O
sentiment O
diversity O
and O
recommendation O
performance O
of O
our O
approach O
0 O
5 O
10 O
15 O
2062.062.262.462.662.863.0AUC O
AUCFigure O
8 O
: O
InÔ¨Çuence O
of O
¬µunderŒª= O
0.4 O
. O
improves O
when O
Œªincreases O
from O
0 O
. O
This O
is O
probably O
because O
when O
Œªis O
too O
small O
, O
the O
useful O
sentiment O
information O
in O
news O
can O
not O
be O
fully O
exploited O
. O
However O
, O
the O
performance O
of O
our O
approach O
starts O
to O
decline O
when O
Œªis O
too O
large O
. O
This O
may O
be O
because O
when O
Œªis O
too O
large O
, O
the O
auxiliary O
sentiment O
prediction O
task O
is O
over O
- O
emphasized O
and O
the O
news O
recommendation O
task O
is O
not O
fully O
respected O
. O
Thus O
, O
a O
moderateŒª(e.g O
. O
, O
0.4 O
) O
is O
more O
appropriate O
for O
our O
approach O
to O
make O
a O
tradeoff O
between O
recommendation O
performance O
and O
sentiment O
diversity O
. O
Then O
, O
we O
vary O
the O
value O
of O
¬µunderŒª= O
0.4 O
to O
evaluate O
the O
recommendation O
performance O
and O
sentiment O
diversity O
of O
our O
approach.10The O
results O
are O
illustrated O
in O
Fig O
. O
8 O
. O
According O
to O
the O
results O
, O
we O
Ô¨Ånd O
the O
sentiment O
diversity O
can O
be O
consistently O
improved O
when O
¬µincreases O
. O
This O
is O
probably O
because O
when O
¬µis O
larger O
, O
the O
model O
is O
regularized O
more O
intensively O
and O
may O
tend O
to O
recommend O
more O
news O
with O
diverse O
sentiment O
from O
browsed O
news O
. O
However O
, O
when O
¬µgoes O
too O
large O
, O
the O
performance O
in O
terms O
of O
AUC O
declines O
signiÔ¨Åcantly O
, O
which O
may O
hurt O
user O
experience O
. O
Thus O
, O
a O
moderate O
selection O
on O
¬µ(e.g O
. O
, O
10 O
) O
is O
appropriate O
to O
achieve O
the O
goal O
of O
recommending O
news O
with O
diverse O
sentiment O
and O
meanwhile O
keep O
good O
recommendation O
performance O
. O
4.5 O
Case O
Study O
In O
this O
section O
, O
we O
present O
several O
case O
studies O
to O
better O
demonstrate O
the O
effectiveness O
of O
our O
approach O
in O
improving O
sentiment O
diversity O
of O
news O
recommendation O
. O
The O
clicked O
news O
of O
a O
randomly O
selected O
user O
as O
well O
as O
the O
top O
ranked O
candidate O
news O
recommended O
by O
a O
state O
- O
of O
- O
the O
- O
art O
method O
NRMS O
and O
our O
SentiRec O
approach O
are O
shown O
in O
Table O
4 O
. O
We O
can O
see O
that O
the O
historical O
browsed O
news O
of O
this O
user O
are O
mainly O
about O
negative O
topics O
such O
10We O
Ô¨Ånd O
that O
the O
scale O
of O
the O
regularization O
loss O
is O
relatively O
small O
and O
the O
magnitude O
of O
¬µneeds O
to O
be O
larger.51Browsed O
NewsTop O
Ranked O
Candidate O
News O
NRMS O
SentiRec O
Woman O
Arrested O
for O
Alleged O
California O
WildÔ¨Åre O
ScamSheriff O
: O
California O
ofÔ¨Åcer O
‚Äôs O
killer O
is O
in O
the O
US O
illegallyEight O
2018 O
Fashion O
Trends O
We O
‚Äôre O
Ready O
to O
Move O
On O
From O
Guns O
are O
the O
second O
leading O
killer O
of O
kids O
, O
after O
carsProfessional O
golfer O
and O
his O
caddie O
arrested O
for O
poaching O
at O
a O
tiger O
reserveJosh O
Duhamel O
Wants O
to O
Date O
Someone O
Young O
Enough O
to O
Have O
Kids O
From O
international O
fashion O
model O
to O
suspect O
in O
racist O
attack O
on O
Kansas O
toddlerTrump O
threatens O
years O
- O
long O
shutdown O
for O
his O
wall O
as O
GOP O
support O
begins O
to O
fracture58 O
Amazing O
After O
- O
Christmas O
Deals O
Happening O
Right O
Now O
Table O
4 O
: O
The O
browsed O
news O
of O
a O
user O
and O
top O
ranked O
candidate O
news O
provided O
by O
different O
methods O
. O
as O
crime O
, O
which O
usually O
convey O
negative O
sentiment O
. O
However O
, O
the O
NRMS O
method O
still O
intensively O
recommends O
news O
with O
negative O
sentiment O
such O
as O
‚Äú O
Sheriff O
: O
California O
ofÔ¨Åcer O
‚Äôs O
killer O
... O
‚Äù O
. O
It O
indicates O
thatNRMS O
tends O
to O
recommend O
news O
with O
similar O
sentiment O
to O
the O
browsed O
news O
, O
which O
is O
not O
suitable O
for O
users O
to O
acquire O
diverse O
news O
information O
. O
Different O
from O
NRMS O
, O
our O
approach O
can O
effectively O
recommend O
news O
with O
diverse O
sentiment O
from O
browsed O
news O
, O
and O
the O
recommended O
news O
also O
has O
some O
inherent O
relatedness O
with O
browsed O
news O
in O
their O
content O
( O
e.g. O
, O
both O
the O
Ô¨Årst O
candidate O
news O
and O
the O
third O
browsed O
news O
mention O
‚Äú O
fashion O
‚Äù O
) O
. O
It O
shows O
that O
our O
approach O
can O
improve O
the O
sentiment O
diversity O
of O
news O
recommendation O
and O
meanwhile O
keep O
recommendation O
accuracy O
. O
5 O
Conclusion O
and O
Future O
Work O
In O
this O
paper O
, O
we O
propose O
a O
sentiment O
diversityaware O
neural O
news O
recommendation O
approach O
which O
can O
effectively O
recommend O
news O
with O
diverse O
sentiment O
from O
browsed O
news O
. O
We O
propose O
a O
sentiment O
- O
aware O
news O
encoder O
to O
learn O
sentimentaware O
news O
representations O
by O
jointly O
training O
it O
with O
an O
auxiliary O
sentiment O
prediction O
task O
. O
We O
learn O
user O
representations O
from O
representations O
of O
browsed O
news O
, O
and O
compute O
click O
scores O
based O
on O
user O
and O
candidate O
news O
representations O
. O
In O
addition O
, O
we O
propose O
a O
sentiment O
diversity O
regularization O
method O
to O
regularize O
the O
model O
according O
to O
the O
overall O
sentiment O
orientation O
of O
browsed O
news O
as O
well O
as O
the O
click O
scores O
and O
sentiment O
scores O
of O
candidate O
news O
. O
Extensive O
experiments O
on O
realworld O
benchmark O
dataset O
validate O
that O
our O
approach O
can O
effectively O
enhance O
the O
sentiment O
diversity O
of O
news O
recommendation O
without O
hurting O
the O
recommendation O
performance O
. O
In O
our O
future O
work O
, O
we O
plan O
to O
analyze O
the O
sentiment O
on O
the O
entities O
in O
news O
and O
explore O
to O
improve O
the O
entity O
- O
level O
sentiment O
diversity O
of O
news O
recommendation O
. O
In O
addition O
, O
we O
plan O
to O
extend O
sentiment O
polarities O
to O
more O
kinds O
of O
emotions O
, O
such O
asangry O
, O
happiness O
, O
sad O
and O
surprise O
, O
to O
enhance O
the O
emotion O
diversity O
of O
news O
recommendation O
. O
Acknowledgments O
This O
work O
was O
supported O
by O
the O
National O
Key O
Research O
and O
Development O
Program O
of O
China O
under O
Grant O
number O
2018YFB2101501 O
, O
and O
the O
National O
Natural O
Science O
Foundation O
of O
China O
under O
Grant O
numbers O
U1936208 O
and O
U1936216 O
. O
Abstract O
Similarity O
search O
is O
to O
Ô¨Ånd O
the O
most O
similar O
items O
for O
a O
certain O
target O
item O
. O
The O
ability O
of O
similarity O
search O
at O
large O
scale O
plays O
a O
significant O
role O
in O
many O
information O
retrieval O
applications O
and O
has O
received O
much O
attention O
. O
Text O
hashing O
is O
a O
promising O
strategy O
, O
which O
utilizes O
binary O
encoding O
to O
represent O
documents O
, O
and O
is O
able O
to O
obtain O
attractive O
performance O
. O
This O
paper O
makes O
the O
Ô¨Årst O
attempt O
to O
utilize O
Bayesian O
Clustering O
for O
TextHashing O
, O
dubbed O
as O
BCTH O
. O
SpeciÔ¨Åcally O
, O
BCTH O
can O
map O
documents O
to O
binary O
codes O
by O
utilizing O
multiple O
Bayesian O
Clusterings O
in O
parallel O
, O
where O
each O
Bayesian O
Clustering O
is O
responsible O
for O
one O
bit O
. O
Our O
approach O
employs O
the O
bit O
- O
balanced O
constraint O
to O
maximize O
the O
amount O
of O
information O
in O
each O
bit O
. O
Meanwhile O
, O
the O
bit O
- O
uncorrelated O
constraint O
is O
adopted O
to O
keep O
independence O
among O
all O
bits O
. O
The O
time O
complexity O
of O
BCTH O
is O
linear O
, O
where O
the O
hash O
codes O
and O
hash O
functions O
are O
jointly O
learned O
. O
Based O
on O
four O
widely O
- O
used O
datasets O
, O
the O
experimental O
results O
demonstrate O
that O
BCTH O
is O
competitive O
compared O
with O
currently O
competitive O
baselines O
from O
the O
perspective O
of O
both O
precision O
and O
training O
speed O
. O
1 O
Introduction O
The O
task O
of O
similarity O
search O
, O
also O
called O
nearest O
neighbor O
search O
, O
aims O
to O
Ô¨Ånd O
the O
most O
similar O
objects O
for O
a O
given O
query O
item O
( O
Gionis O
et O
al O
. O
, O
1999 O
; O
Andoni O
and O
Indyk O
, O
2006 O
) O
. O
It O
plays O
a O
signiÔ¨Åcant O
role O
in O
many O
information O
retrieval O
applications O
, O
such O
as O
document O
clustering O
, O
content O
- O
based O
retrieval O
, O
collaborative O
Ô¨Åltering O
( O
Wang O
et O
al O
. O
, O
2016 O
) O
, O
etc O
. O
With O
the O
development O
of O
many O
intelligent O
terminals O
, O
massive O
textual O
data O
has O
been O
produced O
over O
the O
past O
several O
decades O
. O
Huge O
challenges O
exist O
in O
applying O
text O
similarity O
algorithms O
( O
Conneau O
et O
al O
. O
, O
2017 O
; O
Le O
et O
al O
. O
, O
2018 O
) O
to O
large O
- O
scale O
corpora O
, O
since O
these O
‚á§ O
means O
the O
corresponding O
author.methods O
require O
complicated O
numerical O
computation O
. O
Text O
hashing O
( O
Severyn O
and O
Moschitti O
, O
2015 O
) O
is O
a O
promising O
strategy O
and O
has O
obtained O
much O
attention O
. O
It O
maps O
semantically O
similar O
documents O
to O
hash O
codes O
with O
similar O
semantics O
through O
designing O
binary O
codes O
in O
a O
low O
- O
dimensional O
space O
. O
A O
hashing O
representation O
of O
each O
document O
usually O
needs O
only O
a O
few O
bits O
to O
be O
stored O
. O
The O
calculation O
of O
the O
similarity O
between O
two O
hash O
codes O
can O
be O
executed O
by O
a O
bit O
- O
wise O
XOR O
operation O
. O
Therefore O
, O
text O
hashing O
is O
an O
effective O
strategy O
to O
accelerate O
similarity O
queries O
and O
reduce O
data O
storage O
. O
Most O
of O
the O
traditional O
text O
hashing O
methods O
consist O
of O
two O
stages O
( O
Zhang O
et O
al O
. O
, O
2010 O
; O
Lin O
et O
al O
. O
, O
2014b O
; O
Severyn O
and O
Moschitti O
, O
2015 O
) O
. O
The O
Ô¨Årst O
step O
is O
to O
learn O
hash O
code O
, O
preserving O
similarity O
among O
neighbors O
. O
Then O
the O
hash O
function O
is O
trained O
through O
the O
self O
- O
taught O
method O
, O
with O
the O
text O
features O
and O
hash O
codes O
as O
the O
input O
( O
Wang O
et O
al O
. O
, O
2013b O
) O
. O
However O
, O
for O
mdocuments O
, O
O(m2)training O
time O
complexity O
is O
needed O
to O
generate O
the O
pairwise O
similarity O
matrix O
used O
to O
preserve O
the O
similarity O
information O
. O
On O
the O
other O
hand O
, O
due O
to O
the O
success O
of O
deep O
learning O
, O
researchers O
have O
attempted O
to O
study O
text O
hashing O
through O
deep O
neural O
networks O
( O
Xu O
et O
al O
. O
, O
2015 O
) O
. O
Some O
of O
the O
most O
representative O
works O
include O
VDSH O
( O
Chaidaroon O
and O
Fang O
, O
2017 O
) O
and O
NASH O
( O
Kalchbrenner O
et O
al O
. O
, O
2014 O
) O
. O
The O
NASH O
model O
studies O
text O
hashing O
through O
an O
endto O
- O
end O
Neural O
Architecture O
, O
which O
treats O
the O
hash O
codes O
as O
the O
latent O
factor O
. O
The O
VDSH O
model O
introduces O
a O
latent O
factor O
for O
documents O
to O
capture O
the O
semantic O
information O
. O
Even O
though O
these O
methods O
have O
achieved O
attractive O
performance O
, O
the O
training O
time O
is O
unsatisfactory O
, O
making O
them O
unscalable O
to O
large O
- O
scale O
datasets O
. O
Motivated O
by O
the O
above O
observations O
, O
this O
paper O
attempts O
to O
utilize O
Bayesian O
Clustering O
for O
Text O
Hashing O
, O
dubbed O
as O
BCTH O
. O
SpeciÔ¨Åcally O
, O
BCTH54can O
map O
documents O
to O
binary O
codes O
by O
using O
multiple O
Bayesian O
Clusterings O
in O
parallel O
, O
where O
each O
Bayesian O
Clustering O
is O
responsible O
for O
one O
bit O
. O
Our O
approach O
employs O
the O
bit O
- O
balanced O
constraint O
to O
maximize O
the O
amount O
of O
information O
in O
each O
bit O
. O
Meanwhile O
, O
the O
bit O
- O
uncorrelated O
constraint O
is O
adopted O
to O
keep O
independence O
among O
all O
bits O
. O
Experimental O
results O
prove O
that O
our O
approach O
is O
competitive O
in O
the O
perspective O
of O
both O
precision O
and O
training O
speed O
. O
Our O
contributions O
are O
summarized O
as O
follows O
: O
‚Ä¢We O
propose O
a O
novel O
Text O
Hashing O
based O
on O
the O
Bayesian O
Clustering O
framework O
, O
dubbed O
as O
BCTH O
, O
for O
learning O
effective O
hash O
codes O
from O
documents O
. O
To O
the O
best O
of O
our O
knowledge O
, O
this O
is O
the O
Ô¨Årst O
work O
that O
utilizes O
Bayesian O
Clustering O
in O
text O
hashing O
. O
‚Ä¢The O
time O
complexity O
of O
our O
method O
is O
linear O
, O
where O
the O
hash O
codes O
and O
hash O
function O
are O
jointly O
learned O
. O
What O
‚Äôs O
more O
, O
we O
visualize O
the O
hash O
codes O
and O
prove O
that O
BCTH O
can O
obtain O
effective O
semantics O
from O
the O
original O
documents O
. O
‚Ä¢We O
conduct O
extensive O
experiments O
on O
four O
public O
text O
datasets O
. O
Based O
on O
four O
widelyused O
datasets O
, O
the O
experimental O
results O
demonstrate O
that O
BCTH O
is O
competitive O
compared O
with O
currently O
competitive O
baselines O
from O
the O
perspective O
of O
both O
precision O
and O
training O
speed O
. O
2 O
Model O
The O
approach O
of O
our O
proposed O
BCTH O
is O
introduced O
in O
this O
section O
. O
As O
is O
shown O
in O
Fig O
. O
1 O
, O
BCTH O
is O
a O
general O
learning O
idea O
, O
which O
utilizes O
Bayesian O
Clustering O
that O
is O
based O
on O
the O
latent O
factor O
framework O
in O
Text O
Hashing O
. O
BCTH O
can O
map O
documents O
to O
binary O
codes O
by O
using O
multiple O
Bayesian O
Clusterings O
in O
parallel O
, O
where O
each O
Bayesian O
Clustering O
is O
responsible O
for O
one O
bit O
. O
During O
this O
process O
, O
the O
bit O
- O
balanced O
constraint O
is O
to O
maximize O
the O
amount O
of O
information O
in O
each O
bit O
. O
Meanwhile O
, O
the O
bituncorrelated O
constraint O
is O
adopted O
to O
keep O
independence O
among O
all O
bits O
. O
2.1 O
Preliminaries O
Given O
a O
set O
of O
mdocuments O
X={x(i)}m O
i=1 O
, O
where O
x(i)is O
the O
feature O
representation O
of O
the O
i O
- O
th O
document O
. O
The O
binary O
code O
for O
the O
i O
- O
th O
document O
isexpressed O
as O
b(i)={b(i O
) O
k O
, O
b(i O
) O
k2 O
{ O
 1,1}}r O
k=1 O
, O
and O
ris O
the O
length O
of O
the O
hash O
codes O
. O
Unlike O
the O
existing O
approaches O
( O
Liu O
et O
al O
. O
, O
2011;Zhang O
et O
al O
. O
, O
2010 O
; O
Xu O
et O
al O
. O
, O
2015 O
) O
that O
aim O
to O
preserve O
the O
pair O
- O
wise O
similarity O
among O
all O
the O
documents O
, O
we O
use O
Naive O
Bayes O
to O
extract O
the O
semantic O
information O
of O
the O
i O
- O
th O
document O
as O
: O
P O
‚á£ O
b(i O
) O
k O
= O
ck|x(i O
) O
‚åò O
= O
P O
‚á£ O
x(i)|b(i O
) O
k O
= O
ck O
‚åò O
P O
‚á£ O
b(i O
) O
k O
= O
ck O
‚åò O
P(x(i O
) O
) O
( O
1 O
) O
The O
Naive O
Bayes O
method O
assumes O
the O
conditional O
independence O
for O
the O
conditional O
probability O
distribution O
, O
and O
therefore O
, O
we O
obtain O
the O
following O
equation O
: O
P O
‚á£ O
x(i)|b(i O
) O
k O
= O
ck O
‚åò O
= O
nY O
j=1P(wj O
= O
l(i O
) O
j|b(i O
) O
k O
= O
ck)(2 O
) O
where O
ckrepresents O
the O
k O
- O
th O
bit O
‚Äôs O
value O
of O
the O
hash O
codes O
of O
the O
i O
- O
th O
document O
, O
ck2 O
{ O
 1,1 O
} O
, O
andnis O
the O
size O
of O
the O
vocabulary O
. O
The O
l(i O
) O
jdenotes O
whether O
the O
j O
- O
th O
word O
of O
the O
vocabulary O
appears O
in O
thei O
- O
th O
document O
, O
and O
l(i O
) O
j2{0,1 O
} O
. O
The O
previous O
formula O
adopts O
the O
cumulative O
multiplication O
of O
all O
words O
‚Äô O
probabilities O
to O
calculate O
the O
likelihood O
of O
a O
particular O
document O
. O
However O
, O
since O
many O
words O
will O
not O
appear O
in O
a O
speciÔ¨Åc O
document O
, O
to O
avoid O
redundant O
calculation O
, O
we O
consider O
using O
the O
cumulative O
multiplication O
of O
the O
probabilities O
of O
words O
that O
appear O
in O
that O
particular O
document O
to O
calculate O
the O
probability O
of O
that O
document O
. O
P O
‚á£ O
x(i)|b(i O
) O
k O
= O
ck O
‚åò O
= O
Y O
j2 (i)P(wj=1|b(i O
) O
k O
= O
ck)(3 O
) O
In O
the O
above O
equation O
, O
 (i)is O
a O
set O
of O
words O
that O
appear O
in O
the O
i O
- O
th O
document O
. O
Each O
hash O
code O
can O
be O
learned O
through O
an O
unsupervised O
iteration O
process O
. O
By O
utilizing O
multiple O
Bayesian O
Clusterings O
to O
calculate O
all O
hash O
codes O
of O
documents O
in O
parallel O
, O
we O
obtain O
the O
following O
objective O
function O
: O
P(B|X)=P(X|B)P(B O
) O
P(X)(4 O
) O
where O
hash O
codes O
of O
documents O
are O
expressed O
as O
B={b(i O
) O
k O
, O
k=1,2 O
.. O
r O
, O
i O
= O
1,2 O
.. O
, O
m O
} O
. O
In O
order O
to O
obtain O
high O
- O
quality O
hash O
codes O
, O
the O
bit O
- O
balanced O
and O
the O
bit O
- O
uncorrelated O
constraints O
are O
introduced O
. O
In O
addition O
, O
we O
transform O
the O
probability O
from O
the O
interval O
[ O
0,1]to O
the O
interval O
[ O
 1,1]551 O
0 O
Input O
documents O
ùëëùëúùëêùë¢ùëöùëíùëõùë°m O
1 O
0 O
1 O
0 O
1 O
0 O
0 O
0 O
0 O
1 O
1 O
0 O
1 O
1 O
‚Ä¶ O
‚Ä¶ O
ùëëùëúùëêùë¢ùëöùëíùëõùë°1Bayesian O
ClusteringHash O
codesFigure O
1 O
: O
Illustration O
of O
how O
to O
learn O
the O
hash O
codes O
through O
multiple O
Bayesian O
Clusterings O
jointly O
from O
mdocuments O
. O
The O
size O
of O
hash O
codes O
in O
the O
illustration O
is O
r=4 O
. O
by O
the O
function O
f(P)= O
2P 1 O
. O
Therefore O
, O
we O
obtain O
the O
following O
loss O
function O
: O
minrX O
k=1mX O
i=1   b(i O
) O
k p(i O
) O
k   2 O
( O
) O
kB Pk O
s.t O
. O
B2 O
{ O
 1,1}r O
‚á• O
m O
B1=0,BBT O
= O
mIr O
‚á• O
r(5 O
) O
where O
p(i O
) O
k O
= O
f O
‚á£ O
P(b(i O
) O
k O
= O
ck|x(i O
) O
) O
‚åò O
andP= O
{ O
p(i O
) O
k O
, O
k=1,2 O
.. O
r O
, O
i O
= O
1,2 O
.. O
, O
m O
} O
. O
The O
1denotes O
a O
vector O
with O
all O
of O
its O
elements O
equal O
to O
1 O
. O
The O
equalityB1=0denotes O
the O
bit O
- O
balanced O
constraint O
, O
which O
aims O
to O
maximize O
the O
amount O
of O
information O
in O
each O
bit O
. O
The O
equality O
BBT O
= O
mIr O
‚á• O
rdenotes O
the O
bit O
- O
uncorrelated O
constraint O
, O
aiming O
to O
keep O
the O
independence O
among O
all O
bits O
. O
However O
, O
the O
Eq O
. O
( O
5 O
) O
is O
difÔ¨Åcult O
to O
solve O
directly O
. O
Following O
the O
prior O
work O
in O
discrete O
graph O
hashing O
( O
Liu O
et O
al O
. O
, O
2014 O
) O
, O
let O
us O
deÔ¨Åne O
the O
constraint O
space O
as O
‚å¶ O
=  O
Y2Rr O
‚á• O
m|Y1=0,YYT O
= O
mIr O
‚á• O
r O
  O
. O
Then O
we O
formulate O
a O
more O
general O
framework O
which O
softens O
the O
two O
hard O
constraints O
in O
Eq O
. O
( O
5 O
) O
as O
: O
minkB Pk2+ kB Yk2 O
s.t O
. O
B2 O
{ O
 1,1}r O
‚á• O
m O
Y1=0,YYT O
= O
mIr O
‚á• O
r(6 O
) O
where O
  0is O
a O
hyper O
parameter O
and O
Y O
is O
relaxation O
factor O
. O
If O
problem O
( O
5 O
) O
is O
feasible O
, O
we O
can O
enforce O
B1=0,BBT O
= O
mIr O
‚á• O
rin O
Eq.(5 O
) O
by O
setting O
an O
extremely O
large O
value O
to O
  O
, O
thereby O
converting O
problem O
( O
6 O
) O
into O
problem O
( O
5 O
) O
. O
2.2 O
Learning O
The O
learning O
process O
aims O
to O
Ô¨Ånd O
the O
desirable O
hash O
codes O
that O
can O
optimize O
the O
Eq O
. O
( O
6 O
) O
. O
Similar O
to O
( O
Liuet O
al O
. O
,2014 O
) O
, O
we O
utilize O
a O
tractable O
alternating O
minimization O
algorithm O
, O
which O
is O
an O
unsupervised O
iteration O
process O
, O
including O
alternately O
solving O
three O
sub O
- O
problems O
. O
W O
- O
subproblem O
: O
Let O
us O
initialize O
the O
hash O
codes O
B O
randomly O
, O
and O
the O
parameter O
W={p(wj= O
1|bk O
= O
ck),p(bk O
= O
ck)},j2{1,2 O
, O
. O
. O
. O
, O
n O
} O
, O
k2 O
{ O
1,2 O
, O
. O
. O
. O
, O
r O
} O
can O
be O
calculated O
by O
Naive O
Bayes O
. O
The O
document O
is O
represented O
by O
the O
one O
- O
hot O
method O
. O
The O
variable O
Pis O
calculated O
in O
the O
following O
way O
, O
speciÔ¨Åcally O
, O
through O
the O
conditional O
probability O
and O
prior O
probability O
. O
The O
formula O
is O
as O
follow O
: O
p(bk O
= O
ck)=Pm O
i=1I(b(i O
) O
k O
= O
ck O
) O
m O
, O
k2{1,2 O
, O
. O
. O
. O
, O
r O
} O
( O
7 O
) O
where O
the O
p(bk O
= O
ck)is O
the O
ratio O
of O
the O
number O
of O
documents O
with O
the O
k O
- O
th O
hash O
code O
equal O
to O
ck O
to O
the O
total O
number O
of O
documents O
. O
Iis O
the O
indicator O
function O
. O
If O
the O
input O
value O
is O
true O
, O
it O
returns O
1 O
, O
else O
returns O
0 O
. O
The O
calculation O
process O
of O
the O
conditional O
probability O
P(wj=1|bk O
= O
ck O
) O
, O
which O
includes O
the O
strategy O
of O
Laplace O
smoothing O
, O
is O
as O
follow O
: O
P(wj=1|bk O
= O
ck)= O
Pm O
i=1I(w(i O
) O
j=1\b(i O
) O
k O
= O
ck)+1 O
Pn O
j=1Pm O
i=1I(w(i O
) O
j=1\b(i O
) O
k O
= O
ck)+n(8 O
) O
wherePm O
i=1I(w(i O
) O
j=1\b(i O
) O
k O
= O
ck)is O
the O
number O
of O
documents O
whose O
k O
- O
th O
hash O
code O
value O
is O
ck O
, O
which O
contains O
the O
word O
wj O
. O
The O
‚Äù O
\‚Äùsymbol O
means O
‚Äù O
and O
‚Äù O
. O
Y O
- O
subproblem O
: O
Given O
the O
value O
of O
B O
, O
the O
continuous O
variable O
Ycan O
be O
calculated O
by O
Eq O
. O
( O
10),56the O
details O
are O
as O
follows O
: O
min O
YkB Yk2()min O
Y2(m tr(BTY O
) O
) O
s.t O
. O
Y1=0,YYT O
= O
mIr O
‚á• O
r O
( O
9 O
) O
Where O
tris O
solving O
the O
trace O
of O
a O
matrix O
and O
Ir O
‚á• O
ris O
an O
identity O
matrix O
. O
Minimizing O
2(m tr(BTY))is O
equivalent O
to O
maximizing O
the O
trace O
of O
the O
BTY O
, O
and O
it O
can O
be O
solved O
by O
performing O
singular O
value O
decomposition O
( O
SVD O
) O
operation O
on O
the O
matrix O
Bwhere O
every O
element O
is O
calculated O
by O
: O
b(i O
) O
k O
= O
b(i O
) O
k 1 O
mPm O
i=1b(i O
) O
k. O
TheUbandVb O
, O
therein O
satisfying O
[ O
Vb1]TbVb=0 O
, O
are O
stacked O
by O
the O
left O
and O
right O
singular O
vectors O
respectively O
from O
the O
result O
of O
SVD O
. O
After O
performing O
Gram O
- O
Schmidt O
process O
on O
UbandVb O
, O
we O
obtainUbandVb O
. O
Finally O
, O
according O
to O
( O
Zhang O
et O
al O
. O
, O
2016 O
) O
, O
the O
Yis O
updated O
by O
: O
Y O
= O
pmh O
UbbUbih O
VbbVbiT(10 O
) O
B O
- O
subproblem O
: O
Given O
the O
value O
of O
Pand O
the O
continuous O
variable O
Y O
, O
the O
value O
of O
Bcan O
be O
calculated O
by O
minimizing O
Eq O
. O
( O
12 O
) O
, O
and O
the O
details O
are O
as O
follows O
: O
min O
BkB Pk2+ kB Yk2 O
s.t O
. O
B2 O
{ O
 1,1}r O
‚á• O
m(11 O
) O
Since O
Eq O
. O
( O
12 O
) O
is O
a O
simple O
binary O
optimization O
process O
, O
we O
can O
update O
Bby O
updating O
each O
element O
of O
it O
in O
parallel O
according O
to O
: O
b(i O
) O
k= O
argmin O
b(i O
) O
k2{ 1,1}   b(i O
) O
k p(i O
) O
k   2 O
+ O
    b(i O
) O
k y(i O
) O
k   2 O
( O
12 O
) O
The O
whole O
algorithm O
implementation O
process O
is O
shown O
in O
algorithm O
1 O
. O
2.3 O
Complexity O
Analysis O
In O
this O
section O
, O
we O
analyze O
the O
space O
and O
time O
complexity O
of O
BCTH O
. O
The O
learning O
algorithm O
of O
BCTH O
is O
shown O
in O
Algorithm O
1 O
. O
For O
space O
complexity O
, O
Algorithm O
1 O
requires O
O(mn+mr+nr)to O
store O
the O
training O
datasets O
, O
hash O
codes O
, O
and O
parameters O
. O
Asris O
usually O
less O
than O
1024 O
, O
we O
can O
easily O
store O
the O
above O
variables O
at O
large O
- O
scale O
in O
memory O
. O
For O
time O
complexity O
, O
we O
Ô¨Årst O
analyze O
each O
of O
the O
sub O
- O
problems O
. O
For O
W O
- O
subproblem O
, O
it O
takes O
O(mnr O
) O
to O
calculate O
parameter O
Wand O
updateAlgorithm O
1 O
: O
Learning O
algorithm O
of O
BCTH O
Input O
: O
Training O
data O
: O
X2Rm O
‚á• O
n O
code O
length O
: O
r O
hyperparameter O
: O
  O
; O
Output O
: O
W={p(wj=1|bk= O
ck),p(bk O
= O
ck)},j2 O
{ O
1,2 O
, O
. O
. O
. O
, O
n O
} O
, O
k2{1,2 O
, O
. O
. O
. O
, O
r O
} O
; O
1Initialize O
: O
Bby O
randomization O
; O
2repeat O
3 O
W O
- O
step O
: O
4 O
Solve O
WandPin O
W O
- O
subproblem O
5 O
Y O
- O
step O
: O
6 O
Solve O
Yin O
Y O
- O
subproblem O
7 O
B O
- O
step O
: O
8 O
Solve O
Bby O
B O
- O
subproblem O
9until O
convergence O
; O
10return O
W O
, O
B O
; O
probability O
P. O
For O
Y O
- O
subproblem O
, O
it O
requires O
O(r2n)to O
perform O
the O
SVD O
, O
Gram O
- O
Schmidt O
orthogonalization O
, O
and O
matrix O
multiplication O
. O
For O
B O
- O
subproblem O
, O
it O
requires O
O(mn)to O
update O
each O
b(i O
) O
kofB. O
The O
time O
complexity O
of O
the O
whole O
Algorithm O
1 O
is O
O(t(mnr O
+ O
r2n+mn O
) O
) O
, O
where O
tis O
the O
number O
of O
iterations O
needed O
for O
convergence O
. O
In O
our O
experiments O
, O
tis O
set O
to O
10by O
default O
( O
See O
section O
3.8 O
) O
. O
It O
can O
be O
seen O
that O
the O
time O
complexity O
of O
BCTH O
is O
linear O
. O
3 O
Experiments O
3.1 O
Datasets O
Following O
prior O
works O
( O
Chaidaroon O
and O
Fang O
, O
2017 O
) O
, O
we O
experiment O
on O
four O
public O
text O
datasets O
. O
‚Ä¢Reuters O
Corpus O
Volume O
I O
( O
RCV1 O
): O
The O
RCV1 O
is O
a O
large O
collection O
of O
manually O
labeled O
800,000 O
newswire O
stories O
provided O
by O
Reuters O
. O
The O
full O
- O
topics O
version O
is O
available O
at O
the O
LIBSVM O
website1 O
. O
‚Ä¢Reuters21578 O
( O
Reuters)2 O
: O
This O
dataset O
is O
a O
widely O
- O
used O
text O
corpus O
for O
text O
classiÔ¨Åcation O
. O
This O
collection O
contains O
10,788 O
documents O
with O
90 O
categories O
and O
7,164 O
unique O
words O
. O
‚Ä¢TMC3 O
: O
This O
dataset O
has O
22 O
labels O
, O
21,519 O
1https://www.csie.ntu.edu.tw/ O
cjlin O
/ O
libsvmtools O
/ O
datasets/ O
multilabel.html O
2http://www.nltk.org/book/ch02.html O
3https://catalog.data.gov/dataset/siam-2007-text-miningcompetition-dataset57training O
set O
, O
3,498 O
test O
set O
, O
and O
3,498 O
documents O
for O
the O
validation O
set O
. O
This O
dataset O
is O
used O
as O
part O
of O
the O
SIAM O
text O
mining O
competition O
and O
contains O
the O
air O
traffc O
reports O
provided O
by O
NASA O
. O
‚Ä¢20Newsgroups4 O
: O
The O
20 O
Newsgroups O
dataset O
is O
a O
collection O
of O
18828 O
newsgroup O
documents O
. O
It O
is O
divided O
into O
different O
newsgroups O
, O
each O
corresponding O
to O
a O
speciÔ¨Åc O
topic O
. O
3.2 O
Baselines O
and O
Evaluation O
Metrics O
We O
compare O
BCTH O
with O
the O
following O
competitive O
unsupervised O
methods O
since O
BCTH O
also O
belongs O
to O
unsupervised O
methods O
. O
‚Ä¢LSH O
: O
This O
approach O
applies O
( O
Datar O
et O
al O
. O
, O
2004 O
) O
random O
projections O
as O
the O
hash O
function O
to O
transform O
the O
data O
points O
from O
its O
original O
space O
to O
the O
binary O
hash O
space O
. O
More O
hash O
bits O
are O
needed O
to O
guarantee O
the O
precision O
on O
account O
of O
the O
randomness O
of O
the O
hash O
function O
. O
‚Ä¢SH O
: O
This O
baseline O
( O
Weiss O
et O
al O
. O
, O
2008 O
) O
calculates O
the O
bits O
through O
thresholding O
a O
subset O
of O
eigenvectors O
of O
the O
Laplacian O
of O
the O
similarity O
graph O
. O
‚Ä¢STH O
: O
STH O
( O
Zhang O
et O
al O
. O
, O
2010 O
) O
aims O
to O
Ô¨Ånd O
the O
best O
l O
- O
bit O
binary O
codes O
for O
all O
documents O
in O
the O
corpus O
via O
unsupervised O
learning O
. O
‚Ä¢AGH O
: O
This O
method O
( O
Liu O
et O
al O
. O
, O
2011 O
) O
discovers O
the O
neighborhood O
structure O
hidden O
in O
the O
data O
to O
learn O
proper O
compact O
codes O
. O
To O
make O
the O
method O
computationally O
feasible O
, O
it O
utilizes O
Anchor O
Graphs O
to O
gain O
tractable O
lowrank O
adjacency O
matrices O
. O
‚Ä¢VDSH O
: O
( O
Chaidaroon O
and O
Fang O
, O
2017 O
) O
presents O
a O
series O
of O
deep O
learning O
models O
for O
text O
hashing O
, O
including O
VDSH O
, O
VDSH O
- O
S O
, O
and O
VDSH O
- O
SP O
. O
The O
VDSH O
- O
S O
and O
VDSH O
- O
SP O
models O
are O
supervised O
by O
utilizing O
document O
labels O
/ O
tags O
for O
the O
hashing O
process O
. O
For O
the O
comparison O
‚Äôs O
fairness O
, O
the O
VDSH O
is O
adopted O
as O
the O
baseline O
since O
our O
method O
is O
also O
unsupervised O
. O
To O
better O
evaluate O
the O
effectiveness O
of O
hash O
codes O
used O
in O
the O
Ô¨Åeld O
of O
similarity O
search O
, O
every O
document O
in O
the O
test O
set O
is O
adopted O
as O
the O
query O
document O
. O
The O
similarity O
between O
the O
query O
document O
4http://ana.cachopo.org/datasets-for-single-label-textcategorizationand O
each O
target O
similar O
document O
, O
which O
is O
utilized O
to O
retrieve O
relevant O
documents O
, O
is O
calculated O
by O
the O
Hamming O
distance O
of O
their O
hash O
codes O
respectively O
. O
The O
performance O
is O
measured O
by O
Precision O
, O
which O
is O
the O
ratio O
of O
the O
number O
of O
the O
similar O
documents O
to O
the O
number O
of O
total O
retrieved O
documents O
. O
The O
retrieved O
document O
that O
shares O
any O
common O
test O
label O
with O
the O
query O
document O
is O
denoted O
as O
a O
relevant O
document O
. O
Similar O
to O
previous O
works O
( O
Chaidaroon O
and O
Fang O
, O
2017 O
) O
, O
the O
precision O
for O
the O
top O
100 O
( O
pre@100 O
) O
is O
employed O
as O
the O
main O
criterion O
. O
The O
Ô¨Ånal O
results O
are O
averaged O
over O
all O
the O
test O
documents O
. O
3.3 O
Experimental O
Setup O
We O
randomly O
split O
each O
dataset O
into O
two O
subsets O
for O
training O
and O
testing O
, O
which O
account O
for O
90 O
% O
and O
10 O
% O
, O
respectively O
. O
The O
training O
data O
is O
used O
to O
learn O
the O
mapping O
from O
the O
document O
to O
the O
hash O
code O
. O
Each O
document O
in O
the O
test O
set O
is O
used O
to O
retrieve O
similar O
documents O
based O
on O
the O
mapping O
, O
and O
the O
results O
are O
evaluated O
. O
The O
similar O
as O
( O
Chaidaroon O
and O
Fang O
, O
2017 O
) O
, O
we O
use O
one O
- O
hot O
encoding O
as O
the O
default O
representation O
of O
the O
raw O
document O
. O
The O
hyper O
- O
parameter O
 = O
[ O
0,0.001 O
, O
0.01,0.1 O
] O
, O
where O
the O
number O
in O
bold O
denotes O
the O
default O
setting O
( O
see O
Section O
3.7 O
) O
. O
The O
number O
of O
iterations O
is O
set O
to O
10 O
( O
see O
Section O
3.8 O
) O
. O
Our O
codes O
are O
available O
at O
the O
open O
- O
source O
code O
repository5 O
. O
In O
addition O
, O
the O
settings O
of O
the O
SH6 O
, O
AGH7 O
, O
STH8 O
and O
VDSH9remain O
unchanged O
with O
original O
paper O
. O
We O
run O
Ô¨Åve O
trials O
for O
each O
methods O
and O
an O
average O
of O
Ô¨Åve O
trials O
is O
reported O
to O
avoid O
bias O
introduced O
by O
randomness O
. O
All O
of O
the O
methods O
are O
run O
on O
Windows O
with O
1 O
Intel O
i7 O
- O
7500 O
CPU O
and O
1 O
GeForce O
GTX O
1050Ti O
GPU O
. O
3.4 O
Comparison O
Results O
To O
examine O
the O
competitiveness O
of O
BCTH O
, O
we O
compared O
our O
method O
with O
competitive O
baselines O
, O
including O
traditional O
techniques O
and O
deep O
learning O
models O
from O
the O
perspective O
of O
both O
precision O
and O
training O
speed O
. O
Table O
2 O
reports O
the O
training O
time O
on O
the O
20Newsgroups O
dataset O
. O
From O
the O
table O
, O
we O
can O
derive O
the O
following O
interesting O
conclusions O
: O
( O
1 O
) O
Compared O
5https://github.com/myazi/SemHash O
6https://github.com/superhans/SpectralHashing O
7https://github.com/ColumbiaDVMM/Anchor-GraphHashing O
8http://www.dcs.bbk.ac.uk/ O
dell O
/ O
publications O
/ O
dellzhang O
sigir2010/ O
sthv1.zip O
9https://github.com/unsuthee/VariationalDeepSemanticHashing58Table O
1 O
: O
Precision O
of O
the O
top O
100 O
retrieved O
documents O
on O
four O
datasets O
with O
different O
numbers O
of O
hashing O
bits O
. O
The O
bold O
font O
denotes O
the O
best O
result O
at O
that O
number O
of O
bits O
. O
MethodsRCV1 O
Reuters O
8bits O
16bits O
32bits O
64bits O
128bits O
8bits O
16bits O
32bits O
64bits O
128bits O
LSH O
0.4180 O
0.4352 O
0.4716 O
0.5214 O
0.5877 O
0.2802 O
0.3215 O
0.3862 O
0.4667 O
0.5194 O
SH O
0.5321 O
0.5658 O
0.6786 O
0.7337 O
0.7064 O
0.4016 O
0.4201 O
0.4631 O
0.4590 O
0.4622 O
STH O
0.6992 O
0.7688 O
0.8016 O
0.8098 O
0.8037 O
0.6955 O
0.7239 O
0.7576 O
0.7486 O
0.7240 O
AGH O
0.4257 O
0.4976 O
0.5457 O
0.5698 O
0.5799 O
0.6552 O
0.7046 O
0.7313 O
0.7189 O
0.7043 O
VDSH O
0.7285 O
0.7718 O
0.8165 O
0.7720 O
0.6630 O
0.6642 O
0.7118 O
0.7335 O
0.7083 O
0.7079 O
BCTH O
0.7339 O
0.7989 O
0.8389 O
0.8641 O
0.8690 O
0.6827 O
0.7307 O
0.7584 O
0.7669 O
0.7889 O
Methods20Newsgroups O
TMC O
8bits O
16bits O
32bits O
64bits O
128bits O
8bits O
16bits O
32bits O
64bits O
128bits O
LSH O
0.0578 O
0.0597 O
0.0666 O
0.0770 O
0.0949 O
0.4388 O
0.4393 O
0.4514 O
0.4553 O
0.4773 O
SH O
0.0699 O
0.1096 O
0.2010 O
0.2732 O
0.2632 O
0.5999 O
0.6206 O
0.6108 O
0.5813 O
0.5612 O
STH O
0.2035 O
0.3481 O
0.4581 O
0.5129 O
0.5247 O
0.7278 O
0.7520 O
0.7633 O
0.7569 O
0.7411 O
AGH O
0.2435 O
0.3531 O
0.3861 O
0.3796 O
0.3579 O
0.6000 O
0.6334 O
0.6443 O
0.6423 O
0.6273 O
VDSH O
0.3514 O
0.3848 O
0.4667 O
0.2219 O
0.0651 O
0.6503 O
0.6640 O
0.7062 O
0.6567 O
0.5868 O
BCTH O
0.3089 O
0.4497 O
0.5216 O
0.5534 O
0.5830 O
0.7076 O
0.7351 O
0.7651 O
0.7804 O
0.7926 O
Methods O
8bits O
16bits O
32bits O
64bits O
128bits O
SH O
28.1 O
28.9 O
32.2 O
37.8 O
47.1 O
STH O
16.3 O
16.5 O
17.9 O
20.3 O
28.3 O
AGH O
10.3 O
10.8 O
11.4 O
12.8 O
15.5 O
VDSH O
100 O
+ O
100 O
+ O
100 O
+ O
100 O
+ O
100 O
+ O
BCTH O
0.5 O
0.9 O
2.0 O
5.0 O
10.8 O
Table O
2 O
: O
Training O
time O
( O
second O
) O
of O
different O
methods O
on20Newsgroups O
dataset O
. O
with O
these O
methods O
, O
BCTH O
costs O
less O
training O
time O
among O
all O
different O
hash O
bits O
. O
The O
reason O
can O
be O
attributed O
to O
the O
joint O
learning O
of O
hash O
codes O
and O
hash O
function O
, O
without O
needing O
to O
build O
the O
pairwise O
similarity O
matrix O
and O
the O
linear O
time O
complexity O
of O
BTCH O
. O
( O
2 O
) O
It O
consumes O
extremely O
more O
time O
to O
train O
the O
VDSH O
model O
than O
train O
a O
traditional O
model O
. O
It O
shows O
that O
deep O
learning O
methods O
with O
sophisticated O
network O
architecture O
bring O
many O
parameters O
, O
thus O
requiring O
much O
more O
time O
to O
complete O
the O
training O
process O
. O
( O
3 O
) O
The O
SH O
, O
STH O
, O
and O
AGH O
spend O
less O
time O
on O
the O
training O
process O
, O
which O
indicates O
that O
the O
traditional O
methods O
has O
its O
advantage O
over O
the O
deep O
learning O
method O
in O
training O
time O
. O
Apart O
from O
comparing O
the O
20Newsgroups O
dataset O
, O
we O
also O
compare O
over O
four O
datasets O
from O
the O
perspective O
of O
precision O
. O
Table O
1 O
reports O
the O
comparison O
results O
with O
various O
methods O
over O
different O
numbers O
of O
bits O
. O
From O
this O
table O
, O
we O
can O
derive O
the O
following O
interesting O
conclusions O
: O
( O
1 O
) O
Our O
proposed O
BCTH O
outperforms O
nearly O
all O
baselines O
among O
all O
different O
hash O
bits O
on O
four O
datasets O
. O
It O
demonstrates O
that O
BCTH O
, O
which O
introduces O
the O
bit O
- O
balanced O
and O
the O
bit O
- O
uncorrelated O
constraints O
, O
can O
learn O
effectively O
hash O
code O
from O
documents O
. O
( O
2 O
) O
The O
VDSH O
model O
outperforms O
traditional O
methods O
in O
almost O
all O
situations O
. O
It O
denotes O
that O
deep O
learning O
techniques O
can O
capture O
inherent O
hidden O
text O
semantics O
, O
which O
are O
beneÔ¨Åcial O
to O
generate O
the O
text O
hash O
codes O
. O
Although O
our O
method O
does O
not O
get O
the O
best O
results O
in O
some O
datasets O
under O
the O
circumstance O
of O
short O
hashing O
bit O
, O
it O
is O
approximating O
the O
best O
ones O
. O
Since O
our O
method O
utilizes O
the O
bit O
- O
balanced O
and O
the O
bit O
- O
uncorrelated O
constraints O
to O
make O
each O
bit O
capture O
independent O
semantics O
for O
documents O
, O
it O
is O
worthwhile O
to O
study O
the O
relationship O
between O
the O
length O
of O
the O
hash O
codes O
and O
the O
effect O
of O
our O
method O
. O
3.5 O
Impact O
of O
the O
Length O
of O
Hash O
Codes O
Previous O
works O
usually O
limit O
the O
length O
of O
the O
hash O
code O
to O
128 O
bits O
on O
account O
of O
data O
storage O
. O
To O
study O
the O
effectiveness O
of O
the O
hash O
codes O
‚Äô O
size O
, O
we O
conduct O
experiments O
on O
hash O
codes O
ranging O
from O
8 O
bits O
to O
128 O
bits O
and O
extend O
hash O
codes O
to O
1024 O
bits O
in O
this O
section O
. O
Figure O
2 O
reports O
the O
compared O
results O
on O
four O
datasets O
. O
From O
this O
Ô¨Ågure O
, O
we O
can O
Ô¨Ånd O
the O
following O
phenomena O
: O
( O
1 O
) O
when O
the O
length O
of O
the O
hash O
codes O
is O
equal O
to O
or O
over O
128 O
bits O
, O
the O
effect O
of O
most O
other O
methods O
starts O
to O
decline O
. O
( O
2 O
) O
the O
performance O
of O
our O
method O
always O
increases O
with O
the O
length O
of O
the O
hash O
codes O
increasing O
over O
all O
datasets O
. O
The5923242526272829210code O
length0.40.50.60.70.80.91Pre@100RCV1 O
23242526272829210code O
length0.20.30.40.50.60.70.80.9Reuters O
23242526272829210code O
length00.10.20.30.40.50.60.720Newsgroups O
23242526272829210code O
length0.40.450.50.550.60.650.70.750.80.85TMCLSHSHSTHAGHVDSHBCTH O
Figure O
2 O
: O
Precision@100 O
curve O
on O
four O
datasets O
with O
hash O
codes O
length O
from O
8 O
to O
1024 O
. O
reason O
is O
that O
our O
approach O
, O
which O
introduces O
the O
bit O
- O
balanced O
and O
the O
bit O
- O
uncorrelated O
constraints O
, O
can O
better O
keep O
independent O
semantic O
for O
all O
bits O
. O
3.6 O
Qualitative O
Analysis O
Figure O
3 O
: O
Visualization O
of O
the O
1024 O
- O
dimensional O
document O
latent O
semantic O
vectors O
by O
BCTH O
on O
the O
20Newsgroup O
dataset O
using O
t O
- O
SNE O
. O
To O
evaluate O
whether O
our O
presented O
BCTH O
model O
can O
preserve O
the O
original O
documents O
‚Äô O
semantics O
, O
we O
visualize O
the O
documents O
‚Äô O
low O
- O
dimensional O
representations O
on O
the O
20Newsgroups O
dataset O
in O
this O
section O
. O
In O
particular O
, O
the O
hash O
codes O
, O
obtained O
by O
BCTH O
, O
can O
be O
regarded O
as O
the O
latent O
semantic O
vectors O
of O
documents O
. O
We O
use O
t O
- O
SNE10tool O
to O
generate O
the O
scatter O
plots O
through O
1024 O
- O
bit O
hash O
codes O
. O
Figure O
3 O
shows O
the O
results O
. O
Different O
colors O
represent O
different O
categories O
based O
on O
the O
ground O
truth O
. O
As O
we O
can O
see O
from O
Ô¨Ågure O
3 O
, O
BCTH O
generates O
wellseparated O
clusters O
with O
each O
corresponding O
to O
a O
true O
category O
. O
It O
shows O
that O
our O
method O
can O
effectively O
learn O
low O
- O
dimensional O
representations O
for O
documents O
. O
3.7 O
Impact O
of O
Parameters O
Our O
method O
is O
involved O
with O
a O
critical O
parameter O
  O
, O
which O
is O
used O
to O
control O
the O
bit O
- O
balanced O
and O
the O
bit10https://lvdmaaten.github.io/tsne/Datasets O
 =0 O
 =0.001 O
 =0.01 O
 =0.1 O
RCV1 O
0.8476 O
0.8641 O
0.8526 O
0.8269 O
Reuters O
0.7577 O
0.7669 O
0.7668 O
0.7532 O
20Newsgroups O
0.5528 O
0.5534 O
0.5464 O
0.5502 O
TMC O
0.7073 O
0.7172 O
0.7070 O
0.7025 O
Table O
3 O
: O
The O
effect O
of O
 on O
four O
datasets O
with O
64hashing O
bits O
. O
uncorrelated O
constraints O
. O
We O
here O
study O
the O
impact O
of O
hyper O
- O
parameter O
 in O
this O
section O
. O
Table O
3 O
shows O
the O
results O
, O
which O
are O
obtained O
by O
using O
64 O
hash O
bits O
. O
From O
this O
table O
, O
we O
can O
Ô¨Ånd O
that O
: O
( O
1 O
) O
With O
 varying O
from O
0 O
to O
0.1 O
, O
BCTH O
is O
able O
to O
achieve O
relatively O
desirable O
results O
over O
all O
four O
datasets O
, O
which O
means O
that O
 is O
universally O
applicable O
; O
( O
2 O
) O
With O
 set O
to O
0.001 O
, O
BCTH O
obtains O
the O
optimal O
result O
, O
and O
therefore O
, O
0.001 O
is O
set O
as O
the O
default O
value O
for O
our O
method O
. O
3.8 O
Convergence O
Speed O
1 O
3 O
5 O
7 O
9 O
11 O
13 O
15Iteration0400800120016002000Loss O
/ O
bitsBCTHvariant8bits16bits32bits64bits128bits1 O
3 O
5 O
7 O
9 O
11 O
13 O
15Iteration0400800120016002000Loss O
/ O
bitsBCTH8bits16bits32bits64bits128bitsFigure O
4 O
: O
Convergence O
curve O
of O
the O
loss O
on O
the O
20Newsgroups O
. O
In O
order O
to O
evaluate O
the O
convergence O
performance O
of O
our O
proposed O
BCTH O
algorithm O
, O
we O
performed O
convergence O
experiments O
on O
the O
20Newsgroups O
dataset O
. O
Considering O
the O
different O
loss O
scales O
produced O
under O
different O
hash O
bits O
, O
we O
consider O
the O
ratio O
of O
the O
loss O
to O
the O
hash O
length O
to O
make60it O
comparable O
at O
the O
same O
scale O
for O
different O
iterations O
. O
Note O
that O
, O
in O
this O
set O
of O
experiments O
, O
we O
also O
test O
one O
variant O
of O
BCTH O
methods O
, O
which O
is O
known O
asBCTH O
variant O
, O
which O
calculates O
the O
conditional O
probability O
through O
Eq O
. O
( O
2 O
) O
. O
The O
result O
is O
reported O
in O
Ô¨Ågure O
4 O
, O
and O
we O
can O
Ô¨Ånd O
that O
: O
( O
1 O
) O
both O
the O
BCTH O
and O
the O
BCTH O
variant O
converge O
after O
approximately O
10iterations O
, O
and O
therefore O
, O
10is O
set O
as O
the O
default O
value O
for O
our O
method O
; O
( O
2 O
) O
the O
convergence O
effect O
of O
BCTH O
is O
better O
than O
BCTH O
variant O
, O
which O
has O
a O
lower O
loss O
. O
This O
demonstrates O
that O
BTCH O
is O
effective O
. O
4 O
Related O
Work O
Hashing O
methods O
can O
be O
divided O
into O
dataindependent O
methods O
and O
data O
- O
dependent O
methods O
( O
Chang O
et O
al O
. O
, O
2012 O
) O
. O
The O
well O
- O
known O
dataindependent O
methods O
include O
locality O
sensitive O
hashing O
( O
LSH O
) O
( O
Datar O
et O
al O
. O
, O
2004 O
) O
and O
its O
variants O
. O
Data O
- O
dependent O
hashing O
methods O
are O
also O
known O
as O
learning O
to O
hash O
( O
L2H O
) O
methods O
by O
learning O
a O
hash O
function O
from O
data O
( O
Li O
et O
al O
. O
, O
2016 O
) O
. O
At O
present O
, O
the O
main O
L2H O
methods O
( O
Wang O
et O
al O
. O
, O
2018 O
) O
can O
be O
divided O
into O
three O
categories O
: O
pairwise O
similarity O
preserving O
, O
multiwise O
similarity O
preserving O
, O
and O
implicit O
similarity O
preserving O
. O
The O
pairwise O
similarity O
- O
preserving O
methods O
aim O
to O
build O
a O
pairwise O
similarity O
matrix O
between O
two O
points O
, O
such O
as O
spectral O
hashing O
( O
SH O
) O
( O
Weiss O
et O
al O
. O
, O
2008 O
) O
, O
hashing O
with O
graphs O
( O
AGH O
) O
( O
Liu O
et O
al O
. O
, O
2011 O
) O
, O
discrete O
graph O
hashing O
( O
DGH O
) O
( O
Liu O
et O
al O
. O
, O
2014 O
) O
, O
fast O
supervised O
hashing O
( O
FastH O
) O
( O
Lin O
et O
al O
. O
, O
2014a O
) O
and O
column O
- O
sampling O
- O
based O
discrete O
supervised O
hashing O
( O
COSDISH O
) O
( O
Kang O
et O
al O
. O
, O
2016 O
) O
. O
The O
multiwise O
similarity O
- O
preserving O
is O
similar O
to O
pairwise O
similarity O
, O
which O
uses O
three O
or O
more O
samples O
as O
a O
group O
to O
deÔ¨Åne O
generalized O
similarity O
measures O
( O
Norouzi O
et O
al O
. O
, O
2012 O
; O
Wang O
et O
al O
. O
, O
2013a O
) O
. O
The O
implicit O
similarity O
- O
preserving O
methods O
maintain O
the O
similarity O
in O
an O
equivalent O
manner O
that O
adopts O
the O
idea O
of O
preserving O
the O
similarity O
of O
local O
neighbors O
( O
Irie O
et O
al O
. O
,2014 O
) O
. O
Compared O
with O
this O
line O
of O
works O
, O
although O
our O
work O
also O
focuses O
on O
the O
nearest O
neighbor O
search O
, O
our O
work O
is O
different O
from O
theirs O
since O
( O
1 O
) O
most O
of O
these O
works O
focus O
on O
images O
data O
, O
and O
( O
2 O
) O
Bayesian O
Clustering O
is O
not O
covered O
in O
these O
works O
. O
Another O
line O
of O
works O
discuss O
text O
hashing O
, O
is O
related O
to O
our O
work O
since O
our O
work O
also O
aims O
to O
learn O
binary O
code O
from O
documents O
effectively O
. O
For O
example O
, O
( O
Zhang O
et O
al O
. O
, O
2010 O
) O
presented O
the O
Self O
- O
TaughtHashing O
( O
STH O
) O
method O
for O
efÔ¨Åciently O
learning O
semantic O
hashing O
. O
( O
Zhang O
et O
al O
. O
, O
2010 O
) O
incorporated O
both O
the O
tag O
information O
and O
the O
similarity O
information O
from O
probabilistic O
topic O
modeling O
. O
However O
, O
many O
of O
these O
models O
rely O
on O
pairwise O
similaritypreserving O
technique O
, O
which O
the O
time O
complexity O
is O
unavoidable O
O(m2)where O
m O
is O
the O
number O
of O
documents O
. O
On O
the O
other O
hand O
, O
researchers O
have O
attempted O
to O
study O
text O
hashing O
( O
Xu O
et O
al O
. O
, O
2015 O
) O
via O
deep O
neural O
networks O
owing O
to O
the O
success O
of O
deep O
learning O
. O
For O
example O
, O
( O
Chaidaroon O
and O
Fang O
, O
2017 O
) O
introduces O
a O
latent O
factor O
for O
documents O
to O
capture O
the O
semantic O
information O
. O
( O
Kalchbrenner O
et O
al O
. O
,2014 O
) O
proposed O
an O
end O
- O
to O
- O
end O
Neural O
Architecture O
for O
Semantic O
Hashing O
( O
NASH O
) O
, O
which O
treats O
the O
hashing O
codes O
as O
latent O
variables O
. O
Compared O
to O
this O
line O
of O
works O
, O
our O
work O
shares O
several O
common O
features O
: O
( O
1 O
) O
our O
work O
also O
learns O
hashing O
by O
introducing O
latent O
factor O
, O
and O
( O
2 O
) O
our O
work O
also O
aims O
to O
the O
issues O
related O
to O
text O
hashing O
. O
Nevertheless O
, O
our O
work O
differs O
from O
theirs O
in O
several O
features O
: O
( O
1 O
) O
most O
of O
these O
works O
are O
based O
on O
complex O
nonlinear O
functions O
like O
convolutional O
neural O
networks O
, O
and O
training O
time O
complexity O
is O
enormous O
, O
and O
( O
2 O
) O
Bayesian O
Clustering O
is O
not O
covered O
in O
these O
works O
. O
In O
this O
paper O
, O
we O
make O
the O
Ô¨Årst O
attempts O
to O
utilize O
Bayesian O
Clustering O
for O
text O
hashing O
and O
gain O
training O
time O
‚Äôs O
linear O
complexity O
. O
5 O
Conclusion O
This O
paper O
presents O
a O
general O
learning O
framework O
that O
utilizes O
multiple O
Bayesian O
Clusterings O
jointly O
for O
text O
hashing O
. O
We O
introduce O
two O
constraints O
to O
make O
the O
hash O
code O
effectively O
. O
SpeciÔ¨Åcally O
, O
the O
bit O
- O
balanced O
constraint O
is O
employed O
to O
maximize O
the O
amount O
of O
information O
in O
each O
bit O
, O
and O
the O
bituncorrelated O
constraint O
is O
adopted O
to O
keep O
the O
independence O
among O
all O
bits O
. O
The O
time O
complexity O
of O
our O
method O
is O
linear O
. O
Based O
on O
four O
widelyused O
datasets O
, O
the O
experiment O
results O
demonstrate O
that O
BCTH O
is O
competitive O
compared O
with O
current O
competitive O
baselines O
from O
the O
perspective O
of O
both O
precision O
and O
training O
speed O
. O
Abstract O
Recently O
, O
BERT O
has O
become O
an O
essential O
ingredient O
of O
various O
NLP O
deep O
models O
due O
to O
its O
effectiveness O
and O
universal O
- O
usability O
. O
However O
, O
the O
online O
deployment O
of O
BERT O
is O
often O
blocked O
by O
its O
large O
- O
scale O
parameters O
and O
high O
computational O
cost O
. O
There O
are O
plenty O
of O
studies O
showing O
that O
the O
knowledge O
distillation O
is O
efÔ¨Åcient O
in O
transferring O
the O
knowledge O
from O
BERT O
into O
the O
model O
with O
a O
smaller O
size O
of O
parameters O
. O
Nevertheless O
, O
current O
BERT O
distillation O
approaches O
mainly O
focus O
on O
taskspeciÔ¨Åed O
distillation O
, O
such O
methodologies O
lead O
to O
the O
loss O
of O
the O
general O
semantic O
knowledge O
of O
BERT O
for O
universal O
- O
usability O
. O
In O
this O
paper O
, O
we O
propose O
a O
sentence O
representation O
approximating O
oriented O
distillation O
framework O
that O
can O
distill O
the O
pre O
- O
trained O
BERT O
into O
a O
simple O
LSTM O
based O
model O
without O
specifying O
tasks O
. O
Consistent O
with O
BERT O
, O
our O
distilled O
model O
is O
able O
to O
perform O
transfer O
learning O
via O
Ô¨Åne O
- O
tuning O
to O
adapt O
to O
any O
sentencelevel O
downstream O
task O
. O
Besides O
, O
our O
model O
can O
further O
cooperate O
with O
task O
- O
speciÔ¨Åc O
distillation O
procedures O
. O
The O
experimental O
results O
on O
multiple O
NLP O
tasks O
from O
the O
GLUE O
benchmark O
show O
that O
our O
approach O
outperforms O
other O
taskspeciÔ¨Åc O
distillation O
methods O
or O
even O
much O
larger O
models O
, O
i.e. O
, O
ELMO O
, O
with O
efÔ¨Åciency O
well O
- O
improved O
. O
1 O
Introduction O
As O
one O
of O
the O
most O
important O
progress O
in O
the O
Natural O
Language O
Processing O
Ô¨Åeld O
recently O
, O
the O
Bidirectional O
Encoder O
Representation O
from O
Transformers O
( O
BERT O
) O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
has O
been O
proved O
to O
be O
effective O
in O
improving O
the O
performances O
of O
various O
NLP O
tasks O
by O
providing O
a O
powerful O
pre O
- O
trained O
language O
model O
based O
on O
large O
- O
scale O
unlabeled O
corpora O
. O
Recent O
studies O
have O
shown O
that O
BERT O
‚Äôs O
capability O
can O
be O
further O
enhanced O
by O
utilizing O
deeper O
architectures O
or O
performing O
the O
pre O
- O
training O
on O
larger O
‚àó O
* O
Equal O
contribution O
during O
the O
internship O
at O
Tencent.corpora O
with O
appropriate O
guidance O
( O
Radford O
et O
al O
. O
, O
2019 O
; O
Yang O
et O
al O
. O
, O
2019 O
; O
Liu O
et O
al O
. O
, O
2019b O
) O
. O
Despite O
its O
strength O
in O
building O
distributed O
semantic O
representations O
of O
sentences O
and O
supporting O
various O
NLP O
tasks O
, O
BERT O
holds O
a O
huge O
amount O
of O
parameters O
that O
raises O
the O
difÔ¨Åculty O
of O
conducting O
online O
deployment O
due O
to O
its O
unsatisfying O
computational O
efÔ¨Åciency O
. O
To O
address O
this O
issue O
, O
various O
studies O
have O
been O
done O
to O
utilize O
the O
knowledge O
distillation O
( O
Hinton O
et O
al O
. O
, O
2015 O
) O
for O
compressing O
BERT O
and O
meanwhile O
keep O
its O
semantic O
modeling O
capability O
as O
much O
as O
possible O
( O
Chia O
et O
al O
. O
, O
2019 O
; O
Tsai O
et O
al O
. O
, O
2019 O
) O
. O
The O
distilling O
methodologies O
include O
simulating O
BERT O
with O
a O
much O
smaller O
model O
( O
e.g. O
, O
LSTM O
) O
( O
Tang O
et O
al O
. O
, O
2019b O
) O
and O
reducing O
some O
of O
the O
components O
, O
such O
as O
transformers O
, O
attentions O
to O
obtain O
the O
smaller O
BERT O
based O
model O
( O
Sun O
et O
al O
. O
, O
2019 O
; O
Barkan O
et O
al O
. O
, O
2019 O
) O
. O
Nevertheless O
, O
the O
current O
methods O
highly O
rely O
on O
a O
labeled O
dataset O
upon O
a O
speciÔ¨Åed O
task O
. O
Firstly O
, O
BERT O
is O
Ô¨Åne O
- O
tuned O
on O
the O
speciÔ¨Åed O
task O
to O
get O
the O
teaching O
signal O
for O
distillation O
, O
and O
the O
student O
model O
with O
simpler O
architectures O
attempts O
to O
Ô¨Åt O
the O
task O
- O
speciÔ¨Åed O
Ô¨Åne O
- O
tuned O
BERT O
afterward O
. O
Such O
methodologies O
can O
achieve O
satisfying O
results O
by O
capturing O
the O
task O
- O
speciÔ¨Åed O
biases O
( O
McCallum O
and O
Nigam O
, O
1999 O
; O
Godbole O
et O
al O
. O
, O
2018 O
; O
Min O
et O
al O
. O
, O
2019 O
) O
, O
which O
are O
inherited O
by O
the O
tuned O
BERT O
( O
Niven O
and O
Kao O
, O
2019 O
; O
McCoy O
et O
al O
. O
, O
2019 O
) O
. O
Unfortunately O
, O
the O
powerful O
generalization O
nature O
of O
BERT O
tends O
to O
be O
lost O
. O
Apparently O
, O
distilling O
BERT O
‚Äôs O
original O
motivation O
is O
to O
obtain O
a O
lightweight O
substitution O
of O
BERT O
for O
online O
implementations O
, O
and O
BERT O
‚Äôs O
general O
semantic O
knowledge O
, O
which O
plays O
a O
signiÔ¨Åcant O
role O
in O
some O
NLP O
tasks O
like O
sentence O
similarity O
quantiÔ¨Åcation O
, O
is O
expected O
to O
be O
maintained O
accordingly O
. O
Meanwhile O
, O
for O
many O
NLP O
tasks O
, O
manual O
labeling O
is O
quite O
a O
high O
- O
cost O
work O
, O
and O
large O
amounts O
of O
annotated O
data O
can O
not O
be O
guaranteed O
to O
obtain O
. O
Thus O
, O
it70is O
of O
great O
necessity O
to O
compress O
BERT O
with O
the O
non O
- O
task O
- O
speciÔ¨Åc O
training O
procedure O
on O
unlabeled O
datasets O
. O
For O
achieving O
the O
Non O
- O
task O
- O
speciÔ¨Åc O
Distillation O
from O
BERT O
, O
this O
paper O
proposes O
a O
distillation O
loss O
function O
to O
approximate O
sentence O
representations O
by O
minimizing O
the O
cosine O
distance O
between O
the O
sentence O
representation O
given O
by O
the O
student O
network O
and O
the O
one O
from O
BERT O
. O
As O
a O
result O
, O
a O
student O
network O
with O
a O
much O
smaller O
scale O
of O
parameters O
is O
produced O
. O
Since O
the O
distilling O
strategy O
purely O
focuses O
on O
the O
simulation O
of O
sentence O
embeddings O
from O
BERT O
, O
which O
is O
not O
directly O
related O
to O
any O
speciÔ¨Åc O
NLP O
task O
, O
the O
whole O
training O
procedure O
takes O
only O
a O
large O
amount O
of O
sentences O
without O
any O
manual O
labeling O
work O
. O
Similar O
to O
BERT O
, O
the O
smaller O
student O
network O
can O
also O
perform O
transfer O
learning O
to O
any O
sentence O
- O
level O
downstream O
tasks O
, O
such O
as O
text O
classiÔ¨Åcation O
and O
sentence O
matching O
. O
The O
proposed O
methodology O
is O
evaluated O
on O
the O
open O
platform O
of O
General O
Language O
Understanding O
Evaluation O
( O
GLUE O
) O
( O
Wang O
et O
al O
. O
, O
2019 O
) O
, O
including O
the O
Single O
Sentence O
( O
SST-2 O
) O
, O
Similarity O
and O
Paraphrase O
( O
QQP O
and O
MRPC O
) O
, O
and O
Natural O
Language O
Inference O
( O
MNLI O
) O
tasks O
. O
The O
experimental O
results O
show O
that O
our O
proposed O
model O
outperforms O
the O
models O
distilled O
from O
a O
BERT O
Ô¨Åne O
- O
tuned O
on O
a O
speciÔ¨Åc O
task O
. O
Moreover O
, O
our O
model O
inferences O
more O
efÔ¨Åciently O
than O
other O
transformer O
- O
based O
distilled O
models O
. O
2 O
Related O
Works O
With O
the O
propose O
of O
ELMo O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
, O
various O
studies O
take O
the O
representation O
given O
by O
pretrained O
language O
models O
as O
additional O
features O
to O
improve O
the O
performances O
. O
Howard O
and O
Ruder O
( O
2018 O
) O
propose O
Universal O
Language O
Model O
Finetuning O
( O
ULMFiT O
) O
, O
an O
effective O
transfer O
learning O
method O
that O
can O
be O
applied O
to O
any O
NLP O
task O
and O
accordingly O
, O
using O
pre O
- O
trained O
language O
models O
in O
downstream O
tasks O
became O
one O
of O
the O
most O
exciting O
directions O
. O
On O
this O
basis O
, O
developing O
with O
deeper O
network O
design O
and O
more O
effective O
training O
methods O
, O
pre O
- O
trained O
models O
‚Äô O
performances O
improved O
continuously O
( O
Devlin O
et O
al O
. O
, O
2019 O
; O
Radford O
et O
al O
. O
, O
2019 O
; O
Yang O
et O
al O
. O
, O
2019 O
; O
Liu O
et O
al O
. O
, O
2019b O
) O
. O
Since O
the O
release O
of O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
the O
stateof O
- O
the O
- O
art O
( O
SOTA O
) O
results O
on O
11 O
NLP O
tasks O
have O
been O
produced O
consequently O
. O
With O
the O
improvement O
in O
performances O
, O
the O
computing O
cost O
increases O
, O
and O
the O
inference O
procedure O
becomes O
slower O
accordingly O
. O
Thus O
, O
various O
stud O
- O
ies O
focused O
on O
the O
model O
compression O
upon O
BERT O
. O
Among O
the O
most O
common O
model O
compression O
techniques O
, O
the O
knowledge O
distillation O
( O
Hinton O
et O
al O
. O
, O
2015 O
) O
has O
been O
proven O
to O
be O
efÔ¨Åcient O
in O
transferring O
the O
knowledge O
from O
large O
- O
scaled O
pre O
- O
trained O
language O
models O
into O
another O
one O
( O
Liu O
et O
al O
. O
, O
2019a O
; O
Wang O
et O
al O
. O
, O
2020 O
; O
Jiao O
et O
al O
. O
, O
2019 O
; O
Sun O
et O
al O
. O
, O
2020 O
) O
. O
With O
the O
help O
of O
proposed O
distillation O
loss O
, O
Sun O
et O
al O
. O
( O
2019 O
) O
compressed O
BERT O
into O
fewer O
layers O
by O
shortening O
the O
distance O
of O
internal O
representations O
between O
student O
and O
teacher O
BERTs O
. O
For O
the O
sentence O
- O
pair O
modeling O
, O
Barkan O
et O
al O
. O
( O
2019 O
) O
found O
the O
cross O
- O
attention O
function O
across O
sentences O
is O
consuming O
and O
tried O
to O
remove O
it O
with O
distillation O
on O
sentence O
- O
pair O
tasks O
. O
Different O
from O
these O
studies O
distilling O
BERT O
into O
transformer O
- O
based O
models O
, O
Chia O
et O
al O
. O
( O
2019 O
) O
proposed O
convolutional O
student O
architecture O
to O
distill O
GPT O
for O
efÔ¨Åcient O
text O
classiÔ¨Åcation O
. O
Moreover O
, O
focusing O
on O
the O
sequence O
labeling O
tasks O
, O
( O
Tsai O
et O
al O
. O
, O
2019 O
) O
derived O
a O
BiLSTM O
or O
MiniBERT O
from O
BERT O
via O
standard O
distillation O
procedure O
to O
simulate O
the O
prediction O
on O
each O
token O
. O
Besides O
, O
Tang O
et O
al O
. O
( O
2019a O
, O
b O
) O
proposed O
to O
distill O
BERT O
into O
a O
BiLSTM O
based O
model O
with O
penalizing O
the O
mean O
square O
error O
between O
the O
student O
‚Äôs O
logits O
and O
the O
ones O
given O
by O
BERT O
as O
the O
objective O
on O
speciÔ¨Åc O
tasks O
, O
and O
introduced O
various O
data O
augmentation O
methods O
during O
distillation O
. O
3 O
Method O
As O
introduced O
in O
Section O
1 O
, O
our O
proposed O
method O
consists O
of O
two O
procedures O
. O
Firstly O
, O
we O
distill O
BERT O
into O
a O
smaller O
student O
model O
via O
approximating O
the O
representation O
of O
sentences O
given O
by O
BERT O
. O
Afterward O
, O
similar O
to O
BERT O
, O
the O
student O
model O
can O
be O
Ô¨Åne O
- O
tuned O
on O
any O
sentence O
- O
level O
task O
, O
such O
as O
text O
classiÔ¨Åcation O
and O
sentence O
matching O
. O
3.1 O
Distillation O
Procedure O
Suppose O
x={w1 O
, O
w2 O
, O
¬∑ O
¬∑ O
¬∑ O
, O
wi,¬∑¬∑¬∑wn|i‚àà[1 O
, O
n O
] O
} O
stands O
for O
a O
sentence O
containing O
ntokens O
( O
wiis O
the O
i O
- O
th O
token O
of O
x O
) O
, O
and O
let O
T O
: O
x‚ÜíTx‚ààRdbe O
the O
teacher O
model O
which O
encodes O
xintod O
- O
dimensional O
sentence O
embedding O
Tx O
, O
the O
goal O
of O
the O
sentence O
approximation O
oriented O
distillation O
is O
to O
train O
a O
student O
model O
S O
: O
x‚ÜíSx‚ààRdgenerating O
Sxas O
the O
approximation O
of O
Tx O
. O
In O
our O
proposed O
distillation O
architecture O
, O
as O
shown O
in O
Figure O
1a O
, O
we O
take O
the O
BERT O
as O
the O
teacher O
model O
T O
, O
and O
the O
hidden O
representation O
Cis O
extracted O
from O
the O
top O
transformer O
layer O
upon71ECLS O
EN O
‚ãØE2 O
[ O
CLS O
] O
Tok O
1 O
Tok O
2 O
Tok O
N‚ãØC O
T1 O
TN O
‚ãØT2 O
BERTloss=1 O
2(1 O
-C‚ãÖH O
CH O
) O
E1 O
Single O
Sentence‚ãØ O
‚ãØE1 O
Tok O
1E2 O
Tok O
2EN O
Tok O
NLSTM O
LSTM O
‚ãØ O
LSTMLSTM O
LSTM O
‚ãØ O
LSTM O
Single O
SentenceFC O
layerH O
ùêÅùê¢ùêãùêíùêìùêåùêíùêëùêÄ(a O
) O
Distilling O
BERT O
based O
on O
Representation O
Approximation O
. O
FC O
layerClass O
Label O
H O
‚ãØ O
Tok O
1 O
Tok O
2 O
Tok O
N O
Single O
SentenceùêÅùê¢ùêãùêíùêìùêåùêíùêëùêÄ O
( O
b O
) O
Tuning O
( O
sentence O
classiÔ¨Åcation O
) O
. O
Matching O
FC O
layer O
‡∑©HH O
H‚àí‡∑©HH‚äô‡∑©H O
H O
‚ãØ O
Tok O
1 O
Tok O
2 O
Tok O
N O
Sentence O
1ùêÅùê¢ùêãùêíùêìùêåùêíùêëùêÄ‡∑©H O
‚ãØ O
Tok O
1 O
Tok O
2 O
Tok O
M O
Sentence O
2ùêÅùê¢ùêãùêíùêìùêåùêíùêëùêÄ(c O
) O
Tuning O
( O
sentence O
- O
pair O
oriented O
) O
. O
Figure O
1 O
: O
The O
illustration O
of O
the O
proposed O
BERT O
distillation O
architecture O
including O
the O
distilling O
and O
tuning O
procedures O
. O
Sub-Ô¨Ågure O
( O
a O
) O
demonstrates O
the O
distillation O
procedure O
taking O
BERT O
as O
the O
teacher O
model O
and O
BiLSTM O
as O
the O
student O
model O
, O
with O
the O
objective O
of O
approximating O
the O
sentence O
representations O
given O
by O
BERT O
. O
( O
b O
) O
and O
( O
c O
) O
show O
two O
types O
of O
Ô¨Åne O
- O
tuning O
frameworks O
, O
in O
which O
( O
b O
) O
addresses O
the O
sentence O
classiÔ¨Åcation O
task O
with O
the O
single O
sentence O
as O
the O
input O
, O
and O
( O
c O
) O
goes O
for O
the O
sentence O
- O
pair O
- O
oriented O
tasks O
, O
i.e. O
, O
sentence O
similarity O
quantiÔ¨Åcation O
, O
natural O
language O
inference O
. O
the O
[ O
CLS]1token O
as O
Tx O
. O
For O
the O
student O
model O
, O
a O
standard O
bidirectional O
LSTM O
( O
BiLSTM O
) O
is O
Ô¨Årst O
employed O
to O
encode O
the O
sentence O
into O
a O
Ô¨Åxed O
- O
size O
vectorH. O
After O
that O
, O
a O
fully O
connected O
layer O
without O
bias O
terms O
is O
built O
upon O
the O
BiLSTM O
layer O
to O
map O
Hinto O
a O
d O
- O
dimensional O
representation O
, O
followed O
by O
atanh O
activation O
that O
normalizes O
the O
values O
of O
previous O
representation O
between O
-1 O
and O
1 O
as O
the O
Ô¨ÅnalSx O
. O
As O
our O
non O
- O
task O
- O
speciÔ¨Åc O
distillation O
task O
has O
no O
labeling O
data O
, O
and O
the O
signal O
given O
by O
the O
teacher O
is O
a O
real O
value O
vector O
, O
it O
is O
not O
feasible O
to O
minimize O
the O
cross O
- O
entropy O
loss O
over O
the O
soft O
labels O
and O
ground O
truth O
labels O
( O
Sun O
et O
al O
. O
, O
2019 O
; O
Barkan O
et O
al O
. O
, O
2019 O
; O
Tang O
et O
al O
. O
, O
2019b O
) O
. O
On O
this O
basis O
, O
we O
propose O
an O
adjusted O
cosine O
similarity O
between O
the O
two O
real O
value O
vectors O
TxandSxto O
perform O
the O
sentence O
representation O
approximation O
. O
Our O
distillation O
objective O
is O
computed O
as O
follows O
: O
Ldistill O
= O
1 O
2(1‚àíTx¬∑Sx O
/bardblTx O
/ O
bardbl O
/ O
bardblSx O
/ O
bardbl O
) O
( O
1 O
) O
Heretanh O
is O
chosen O
as O
the O
activation O
function O
since O
most O
values O
( O
more O
than O
98 O
% O
according O
to O
our O
statics O
) O
in O
Txobtained O
from O
BERT O
are O
within O
range O
of O
tanh O
( O
-1 O
to O
1 O
) O
. O
The O
choice O
of O
using O
cosine O
similarity O
based O
loss O
is O
mainly O
based O
on O
the O
following O
two O
considerations O
. O
Firstly O
, O
since O
2 O
% O
values O
in O
Txare O
outside O
the O
range O
of O
[ O
-1 O
, O
1 O
] O
, O
it O
is O
more O
reasonable O
1[CLS O
] O
is O
a O
special O
symbol O
added O
in O
front O
of O
other O
tokens O
in O
BERT O
, O
and O
the O
Ô¨Ånal O
hidden O
state O
corresponding O
to O
this O
token O
is O
usually O
used O
as O
the O
aggregate O
sequence O
representation.to O
use O
a O
scalable O
measurement O
, O
such O
as O
cosine O
similarity O
, O
to O
deal O
with O
these O
deviations O
. O
Secondly O
, O
it O
is O
meaningful O
to O
compute O
the O
cosine O
similarity O
between O
sentence O
embeddings O
given O
by O
BERT O
( O
Xiao O
, O
2018 O
) O
. O
Overall O
, O
after O
the O
distillation O
procedure O
, O
we O
obtained O
a O
BiLSTM O
based O
‚Äú O
BERT O
‚Äù O
, O
which O
is O
smaller O
in O
parameter O
scale O
and O
more O
efÔ¨Åcient O
in O
generating O
a O
sentence O
‚Äôs O
semantic O
representation O
. O
Distilling O
data O
As O
our O
distillation O
procedure O
needs O
no O
dependency O
on O
sentence O
type O
or O
labeling O
resources O
but O
only O
standard O
sentences O
available O
everywhere O
, O
the O
distillation O
data O
selection O
follows O
the O
existing O
literature O
on O
language O
model O
pre O
- O
training O
as O
well O
as O
BERT O
. O
We O
use O
the O
English O
Wikipedia O
to O
perform O
the O
distillation O
. O
Furthermore O
, O
as O
the O
proposed O
method O
focus O
on O
the O
sentence O
representation O
approximation O
, O
the O
document O
is O
segmented O
into O
sentences O
using O
spacy O
( O
Honnibal O
and O
Montani O
, O
2017 O
) O
. O
3.2 O
Fine O
- O
tuning O
the O
Student O
Model O
The O
Ô¨Åne O
- O
tuning O
on O
sentence O
- O
level O
tasks O
is O
straightforward O
. O
The O
downstream O
tasks O
discussed O
in O
this O
paper O
can O
be O
summarized O
as O
type O
judgment O
on O
a O
single O
sentence O
and O
predicting O
the O
relationship O
between O
two O
sentences O
( O
same O
as O
all O
GLUE O
tasks O
) O
. O
Figure O
1b O
illustrates O
the O
model O
architecture O
for O
single O
sentence O
classiÔ¨Åcation O
tasks O
. O
The O
student O
model O
S O
is O
utilized O
to O
provide O
sentence O
representation O
. O
After O
that O
, O
a O
multilayer O
perceptron O
( O
MLP O
) O
based O
classiÔ¨Åer72using O
Relu O
as O
activation O
of O
hidden O
layers O
is O
applied O
for O
the O
speciÔ¨Åc O
task O
. O
For O
the O
sentence O
pair O
tasks O
, O
as O
shown O
in O
Figure O
1c O
, O
the O
representations O
Hand O
ÀúHfor O
the O
sentence O
pair O
are O
obtained O
by O
transforming O
two O
sentences O
into O
two O
BiLSTM O
based O
student O
models O
with O
shared O
weights O
respectively O
. O
Then O
, O
following O
the O
baseline O
BiLSTM O
model O
reported O
by O
GLUE O
( O
Wang O
et O
al O
. O
, O
2019 O
) O
, O
we O
apply O
a O
standard O
concatenate O
- O
compare O
operation O
between O
the O
two O
sentence O
embeddings O
and O
get O
an O
interactive O
vector O
as[H,ÀúH,|H‚àíÀúH| O
, O
H‚äôÀúH O
] O
, O
where O
the‚äôdemotes O
for O
the O
element O
- O
wise O
multiplication O
. O
Then O
, O
same O
as O
the O
single O
sentence O
task O
, O
an O
MLP O
based O
classiÔ¨Åer O
is O
built O
upon O
the O
interactive O
representation O
. O
For O
both O
types O
of O
tasks O
, O
MLP O
layers O
are O
initialized O
randomly O
, O
and O
the O
rest O
parameters O
are O
inherited O
from O
the O
distilled O
student O
model O
. O
Meanwhile O
, O
all O
parameters O
are O
optimized O
through O
the O
training O
procedure O
for O
the O
speciÔ¨Åc O
task O
. O
4 O
Experimental O
Setups O
4.1 O
Datasets O
& O
Evaluation O
Tasks O
To O
evaluate O
the O
performance O
of O
our O
proposed O
non O
- O
task O
- O
speciÔ¨Åc O
distilling O
method O
, O
we O
conduct O
experiments O
on O
three O
types O
of O
sentence O
- O
level O
tasks O
: O
sentiment O
classiÔ¨Åcation O
( O
SST-2 O
) O
, O
similarity O
( O
QQP O
, O
MRPC O
) O
, O
and O
natural O
language O
inference O
( O
MNLI O
) O
. O
All O
the O
tasks O
come O
from O
the O
GLUE O
benchmark O
( O
Wang O
et O
al O
. O
, O
2019 O
) O
. O
SST-2 O
Based O
on O
the O
Stanford O
Sentiment O
Treebank O
dataset O
( O
Socher O
et O
al O
. O
, O
2013 O
) O
, O
the O
SST-2 O
task O
is O
to O
predict O
the O
binary O
sentiment O
of O
a O
given O
single O
sentence O
. O
The O
dataset O
contains O
64k O
sentences O
for O
training O
and O
remains O
1k O
for O
testing O
. O
QQP O
The O
Quora O
Question O
Pairs2dataset O
consists O
of O
pairs O
of O
questions O
, O
and O
the O
corresponding O
task O
is O
to O
determine O
whether O
each O
pair O
is O
semantically O
equivalent O
. O
MNLI O
The O
Multi O
- O
Genre O
Language O
Inference O
Corpus O
( O
Williams O
et O
al O
. O
, O
2018 O
) O
is O
a O
crowdsourced O
collection O
of O
sentence O
pairs O
with O
textual O
entailment O
annotations O
. O
There O
are O
two O
sections O
of O
the O
test O
dataset O
: O
matched O
( O
in O
- O
domain O
, O
noted O
as O
MNLI O
- O
m O
) O
and O
mismatched O
( O
cross O
- O
domain O
, O
noted O
as O
MNLImm O
) O
. O
MRPC O
The O
Microsoft O
Research O
Paraphrase O
Corpus O
( O
Dolan O
and O
Brockett O
, O
2005 O
) O
is O
similar O
to O
the O
2https://www.quora.com/q/quoradata/ O
First O
- O
Quora O
- O
Dataset O
- O
Release O
- O
Question O
- O
PairsQQP O
dataset O
. O
This O
dataset O
consists O
of O
sentence O
pairs O
with O
binary O
labels O
denoting O
their O
semantic O
equivalence O
. O
4.2 O
Model O
Variations O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
with O
two O
variants O
: O
BERT O
BASE O
and O
BERT O
LARGE O
, O
containing O
12 O
and O
24 O
layers O
of O
Transformer O
respectively O
. O
ELMO O
Baseline O
( O
Wang O
et O
al O
. O
, O
2019 O
) O
is O
a O
BiLSTM O
based O
model O
, O
taking O
ELMo O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
embeddings O
in O
place O
of O
word O
embeddings O
. O
BERT O
- O
PKD O
( O
Sun O
et O
al O
. O
, O
2019 O
) O
proposes O
a O
patient O
knowledge O
distillation O
approach O
to O
compress O
BERT O
into O
a O
BERT O
with O
fewer O
layers O
. O
BERT O
3 O
- O
PKD O
and O
BERT O
6 O
- O
PKD O
stand O
for O
the O
student O
models O
consisting O
of O
3 O
and O
6 O
layers O
of O
Transformer O
, O
respectively O
. O
DSE O
( O
Barkan O
et O
al O
. O
, O
2019 O
) O
is O
a O
sentence O
embedding O
model O
based O
on O
knowledge O
distillation O
from O
cross O
- O
attentive O
models O
. O
For O
each O
single O
sentence O
modeling O
, O
the O
24 O
- O
layers O
BERT O
is O
employed O
. O
BiLSTM O
KD(Tang O
et O
al O
. O
, O
2019b O
) O
introduces O
a O
new O
distillation O
objective O
to O
distill O
a O
BiLSTM O
based O
model O
from O
BERT O
for O
a O
speciÔ¨Åc O
task O
. O
BiLSTM O
KD+TS O
( O
Tang O
et O
al O
. O
, O
2019a O
) O
donates O
the O
distilling O
procedure O
performed O
with O
the O
proposed O
data O
augmentation O
strategies O
. O
BiLSTM O
SRA O
stands O
for O
the O
Sentence O
Representation O
Approximation O
based O
distillation O
model O
proposed O
in O
this O
paper O
. O
BiLSTM O
SRA O
+ O
KD O
donates O
performing O
knowledge O
distillation O
method O
proposed O
by O
Tang O
et O
al O
. O
( O
2019b O
) O
during O
Ô¨Åne O
- O
tuning O
on O
a O
speciÔ¨Åc O
task O
, O
and O
BiLSTM O
SRA O
+ O
KD O
+ O
TS O
demonstrates O
using O
the O
same O
augmented O
dataset O
to O
perform O
the O
distillation O
. O
4.3 O
Hyperparameters O
For O
the O
student O
model O
in O
our O
proposed O
distilling O
method O
, O
we O
employ O
the O
300 O
- O
dimension O
GloVe O
( O
840B O
Common O
Crawl O
version O
; O
Pennington O
et O
al O
. O
, O
2014 O
) O
to O
initialize O
the O
word O
embeddings O
. O
The O
number O
of O
hidden O
units O
for O
the O
bi O
- O
directional O
LSTM O
is O
set O
to O
512 O
, O
and O
the O
size O
of O
the O
task O
- O
speciÔ¨Åc O
layers O
is O
set O
to O
256 O
. O
All O
the O
models O
are O
optimized O
using O
Adam O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
. O
In O
the O
distilling O
procedure O
, O
we O
choose O
the O
learning O
rate O
as O
1√ó10‚àí3with O
the O
batch O
size=1024 O
. O
During O
Ô¨Ånetuning O
, O
the O
best O
learning O
rate O
on O
the O
validation O
set O
is O
picked O
from{2,3,5,10}√ó10‚àí4 O
. O
For O
the O
data73 O
# O
ModelsSST-2 O
QQP O
MNLI O
- O
m O
/ O
mm O
MRPC O
Acc O
F O
1 O
/ O
Acc O
Acc O
F O
1 O
/ O
Acc O
1 O
BiLSTM O
( O
report O
by O
GLUE O
) O
85.9 O
61.4 O
/ O
81.7 O
70.3 O
/ O
70.8 O
79.4 O
/ O
69.3 O
2 O
BiLSTM O
( O
report O
by O
Tang O
et O
al O
. O
( O
2019b O
) O
) O
86.7 O
63.7 O
/ O
86.2 O
68.7 O
/ O
68.3 O
80.9 O
/ O
69.4 O
3 O
BiLSTM O
( O
our O
implementation O
) O
84.5 O
60.3 O
/ O
81.6 O
70.8 O
/ O
69.4 O
80.2 O
/ O
69.7 O
4 O
ELMO O
Baseline O
( O
Wang O
et O
al O
. O
, O
2019 O
) O
90.2 O
65.6 O
/ O
85.7 O
72.9 O
/ O
73.4 O
84.9 O
/ O
78.0 O
5 O
BERT O
BASE O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
93.5 O
71.2 O
/ O
89.2 O
84.6 O
/ O
83.4 O
88.9 O
/ O
84.8 O
6 O
BERT O
LARGE O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
94.9 O
72.1 O
/ O
89.3 O
86.7 O
/ O
85.9 O
89.3 O
/ O
85.4 O
7 O
DSE O
( O
Barkan O
et O
al O
. O
, O
2019 O
) O
- O
68.5 O
/ O
86.9 O
80.9 O
/ O
80.4 O
86.7 O
/ O
80.7 O
8 O
BERT O
6 O
- O
PKD O
( O
Sun O
et O
al O
. O
, O
2019 O
) O
92.0 O
70.7 O
/ O
88.9 O
81.5 O
/ O
81.0 O
85.0 O
/ O
79.9 O
9 O
BERT O
3 O
- O
PKD O
( O
Sun O
et O
al O
. O
, O
2019 O
) O
87.5 O
68.1 O
/ O
87.8 O
76.7 O
/ O
76.3 O
80.7 O
/ O
72.5 O
10 O
BiLSTM O
KD(Tang O
et O
al O
. O
, O
2019a O
) O
88.4 O
- O
/ O
- O
- O
/ O
- O
78.0 O
/ O
69.7 O
11 O
BiLSTM O
SRA(Ours O
) O
90.0 O
64.4 O
/ O
86.2 O
72.6 O
/72.5 O
83.1 O
/75.1 O
12 O
BiLSTM O
SRA O
+ O
KD O
90.2 O
67.7 O
/87.8 O
72.3 O
/ O
72.0 O
80.2 O
/ O
72.8 O
13 O
BiLSTM O
KD+TS O
( O
Tang O
et O
al O
. O
, O
2019b O
) O
90.7 O
68.2 O
/ O
88.1 O
73.0 O
/ O
72.6 O
82.4 O
/ O
76.1 O
14 O
BiLSTM O
SRA O
+ O
KD O
+ O
TS O
91.1 O
68.4 O
/88.6 O
73.0 O
/72.9 O
83.8 O
/76.2 O
Improvements O
obtained O
by O
performing O
different O
knowledge O
distillations O
15 O
PKD O
( O
Sun O
et O
al O
. O
, O
2019 O
) O
+1.1 O
+2.3 O
/ O
+0.9 O
+1.9 O
/ O
+2.0 O
+0.2 O
/ O
-0.1 O
16 O
KD O
( O
Tang O
et O
al O
. O
, O
2019a O
) O
+1.7 O
- O
/ O
- O
- O
/ O
- O
-2.9 O
/ O
+0.3 O
17 O
SRA O
( O
Ours O
) O
+5.5 O
+4.1 O
/ O
+4.6 O
+1.8 O
/ O
+3.1 O
+2.9 O
/+5.4 O
18 O
SRA O
( O
Ours O
) O
+ O
KD O
+5.7 O
+7.4 O
/+6.2 O
+1.5 O
/ O
+2.6 O
0 O
. O
/ O
+3.1 O
19 O
KD+TS O
( O
Tang O
et O
al O
. O
, O
2019a O
) O
+4.0 O
+4.5 O
/ O
+1.9 O
+4.3 O
/+4.2 O
+1.5 O
/ O
+6.7 O
20 O
SRA O
( O
Ours O
) O
+ O
KD+TS O
+6.6 O
+8.1 O
/+7.0 O
+2.2 O
/ O
+3.5 O
+3.6 O
/ O
+6.5 O
Table O
1 O
: O
Evaluation O
results O
with O
scores O
given O
by O
the O
ofÔ¨Åcial O
evaluation O
server3 O
. O
augmentation O
, O
we O
use O
the O
rule O
- O
based O
method O
originally O
suggested O
by O
Tang O
et O
al O
. O
( O
2019b O
) O
. O
Notably O
, O
on O
the O
SST-2 O
and O
MRPC O
dataset O
, O
we O
stop O
data O
augmenting O
when O
the O
transfer O
set O
achieves O
800 O
K O
samples O
following O
the O
setting O
of O
their O
follow O
- O
up O
research O
( O
Tang O
et O
al O
. O
, O
2019a O
) O
. O
Besides O
, O
inspired O
by O
the O
comparisons O
in O
the O
research O
of O
Sun O
et O
al O
. O
( O
2019 O
) O
, O
we O
Ô¨Ånd O
BERT O
BASE O
can O
provide O
more O
instructive O
representations O
than O
BERT O
LARGE O
. O
So O
that O
, O
we O
chose O
BERT O
BASE O
as O
our O
teacher O
model O
to O
train O
the O
non O
- O
task O
- O
speciÔ¨Åed O
BiLSTM O
SRA O
. O
5 O
Results O
and O
Analysis O
5.1 O
Model O
Performance O
Analysis O
For O
a O
comprehensive O
experiment O
analysis O
, O
we O
collect O
data O
and O
implement O
comparative O
experiments O
on O
various O
published O
BERT O
and O
BERT O
- O
distillation O
methods O
. O
Table O
1 O
shows O
the O
results O
of O
our O
proposed O
BiLSTM O
SRAand O
the O
baselines O
on O
the O
four O
datasets O
. O
All O
models O
in O
the O
Ô¨Årst O
block O
( O
row O
1 O
- O
6 O
) O
belong O
to O
base O
methods O
without O
implementing O
distillation O
, O
the O
second O
( O
row O
7 O
- O
9 O
) O
and O
third O
( O
row O
10 O
- O
12 O
) O
blocks O
3https://gluebenchmark.com/leaderboardshow O
the O
performances O
of O
distillation O
models O
using O
BERT O
and O
BiLSTM O
structures O
, O
respectively O
. O
Moreover O
, O
the O
fourth O
block O
( O
row O
13 O
- O
14 O
) O
displays O
the O
inÔ¨Çuences O
of O
textual O
data O
augmentation O
approach O
on O
our O
BiLSTM O
SRAand O
BiLSTM O
KDdistillation O
baseline O
. O
The O
last O
two O
blocks O
contain O
the O
results O
of O
pure O
improvements O
obtained O
by O
different O
distillation O
methods O
. O
To O
analyze O
the O
effectiveness O
of O
BiLSTM O
SRAthoroughly O
, O
we O
break O
down O
the O
analyses O
into O
the O
following O
two O
perspectives O
. O
5.1.1 O
Comparison O
Between O
Models O
Taking O
those O
non O
- O
distillation O
methods O
in O
the O
Ô¨Årst O
block O
as O
references O
, O
BiLSTM O
SRAperforms O
on O
par O
with O
ELMO O
on O
all O
tasks O
. O
Especially O
, O
BiLSTM O
SRA O
+ O
KD O
+ O
TS O
outperforms O
the O
ELMO O
baseline O
by O
approximately O
3 O
% O
on O
QQP O
and O
1 O
% O
on O
SST2 O
( O
row O
14 O
vs O
4 O
) O
. O
Such O
fact O
shows O
our O
compressed O
‚Äú O
BERT O
‚Äù O
can O
provide O
as O
good O
pre O
- O
trained O
representations O
as O
ELMO O
on O
the O
sentence O
- O
level O
tasks O
. O
For O
those O
distillation O
methods O
, O
both O
our O
model O
and O
BiLSTM O
KDdistill O
knowledge O
from O
BERT O
into O
a O
simple O
BiLSTM O
based O
model O
, O
while O
BERT O
- O
PKD O
focuses O
on O
distilling O
with O
the O
BERT O
of O
fewer O
lay-74ers O
. O
Despite O
the O
powerful O
BERT O
based O
student O
model O
and O
large O
- O
scale O
parameters O
used O
by O
BERTPKD O
, O
our O
proposed O
BiLSTM O
SRAstill O
outperforms O
BERT O
3 O
- O
PKD O
on O
SST-2 O
and O
MRPC O
dataset O
( O
row O
12 O
vs. O
9 O
) O
. O
For O
BiLSTM O
KD O
, O
it O
proposes O
a O
rule O
- O
based O
textual O
data O
augmentation O
approach O
( O
noted O
as O
TS O
) O
to O
construct O
transfer O
sets O
for O
the O
task O
- O
speciÔ¨Åc O
knowledge O
distillation O
. O
We O
also O
employ O
such O
method O
upon O
BiLSTM O
SRA O
+ O
KD O
. O
With O
and O
without O
the O
data O
augmentation O
, O
BiLSTM O
SRA O
consistently O
outperforms O
BiLSTM O
KDon O
all O
tasks O
( O
row O
12 O
vs O
10 O
; O
row O
14 O
vs O
13 O
) O
. O
Coworking O
with O
the O
standard O
knowledge O
distillation O
and O
data O
augmentation O
methods O
, O
our O
proposed O
model O
is O
sufÔ¨Åcient O
to O
distill O
semantic O
representation O
modeled O
from O
pre O
- O
training O
tasks O
as O
well O
as O
the O
task O
- O
speciÔ¨Åc O
knowledge O
included O
in O
a O
Ô¨Åne O
- O
tuned O
BERT O
. O
Besides O
, O
DSE O
‚Äôs O
overall O
architecture O
is O
similar O
to O
our O
method O
for O
modeling O
the O
sentence O
matching O
task O
, O
except O
DSE O
does O
not O
reduce O
the O
parameter O
size O
because O
it O
employs O
the O
pretrained O
BERT O
LARGE O
to O
give O
sentence O
representations O
. O
Thus O
, O
on O
the O
sentence O
- O
pair O
level O
tasks O
, O
DSE O
somehow O
is O
an O
upper O
bound O
of O
the O
distilled O
models O
without O
utilizing O
any O
cross O
attention O
to O
model O
the O
two O
sentences O
‚Äô O
interaction O
. O
Comparing O
with O
DSE O
achieved O
an O
averaged O
80.7 O
score O
on O
all O
sentencepair O
level O
tasks O
, O
BiLSTM O
SRA O
+ O
KD O
+ O
TS O
can O
also O
obtain O
77.2 O
that O
only O
3.5 O
points O
lower O
( O
row O
7 O
vs. O
14 O
) O
. O
Analyzing O
from O
this O
fact O
, O
our O
proposed O
model O
has O
distilled O
a O
much O
smaller O
‚Äú O
BERT O
‚Äù O
with O
acceptable O
performances O
. O
5.1.2 O
Distillation O
Effectiveness O
Because O
in O
each O
paper O
, O
the O
performances O
of O
student O
models O
used O
for O
distillation O
vary O
from O
each O
other O
. O
To O
further O
evaluate O
the O
distillation O
effectiveness O
, O
we O
also O
report O
each O
distillation O
method O
‚Äôs O
improvement O
upon O
the O
corresponding O
student O
directly O
trained O
without O
distillation O
( O
in O
row O
15 O
- O
20 O
) O
. O
It O
can O
be O
observed O
that O
SRA O
improves O
the O
scores O
by O
over O
3.9 O
% O
on O
average O
, O
while O
PKD O
and O
KD O
only O
provide O
less O
than O
1.2 O
% O
increase O
( O
row O
17/16 O
vs. O
15 O
) O
. O
Since O
our O
distillation O
method O
is O
unrelated O
to O
speciÔ¨Åc O
tasks O
, O
KD O
can O
also O
be O
performed O
upon O
BiLSTM O
SRAduring O
Ô¨Åne O
- O
tuning O
on O
a O
given O
dataset O
. O
This O
operation O
provides O
a O
notable O
boost O
on O
the O
QQP O
task O
, O
but O
damages O
the O
performance O
on O
both O
MNLI O
and O
MRPC O
datasets O
( O
row O
17 O
vs. O
18 O
) O
. O
We O
attribute O
these O
differences O
to O
the O
following O
aspects O
: O
a O
) O
the O
QQP O
dataset O
has O
more O
obvious O
task O
- O
speciÔ¨Åed O
bi O
- O
Models O
# O
of O
Par O
. O
Inference O
Time O
BERT O
LARGE O
309 O
( O
64x O
) O
1461.9 O
( O
54.4x O
) O
BERT O
BASE O
87 O
( O
18x O
) O
479.7 O
( O
17.7x O
) O
ELMO O
93 O
( O
19x O
) O
- O
( O
23.7x O
) O
BERT O
3 O
- O
PKD O
21 O
( O
4x O
) O
- O
( O
4.8x O
) O
BERT O
6 O
- O
PKD O
42 O
( O
9x O
) O
- O
( O
9.2x O
) O
DSE O
309 O
( O
64x O
) O
- O
( O
109.1x O
) O
BiLSTM O
KD O
2.4 O
( O
0.5x O
) O
31.9 O
( O
1.2x O
) O
BiLSTM O
SRA O
4.8 O
( O
1x O
) O
26.8 O
( O
1x O
) O
Table O
2 O
: O
Comparisons O
of O
model O
size O
and O
inference O
speed O
. O
# O
of O
Par O
. O
denotes O
the O
number O
of O
millions O
of O
parameters O
, O
and O
the O
inference O
time O
is O
in O
seconds O
. O
The O
factors O
inside O
the O
brackets O
are O
computed O
comparing O
to O
our O
proposed O
model O
. O
ases O
during O
the O
sampling O
process4 O
. O
A O
pre O
- O
trained O
BERT O
can O
not O
learn O
such O
biases O
; O
b O
) O
a O
Ô¨Åne O
- O
tuned O
BERT O
on O
the O
MNLI O
can O
not O
further O
provide O
more O
easy O
- O
to O
- O
use O
information O
to O
guide O
the O
student O
training O
after O
performing O
SRA O
; O
c O
) O
MRPC O
does O
not O
include O
enough O
data O
to O
complete O
KD O
, O
which O
is O
also O
indicated O
by O
the O
decreased O
F1 O
score O
shown O
in O
row O
16 O
in O
Table O
1 O
. O
These O
phenomena O
reÔ¨Çect O
that O
the O
pre O
- O
distillation O
without O
paying O
attention O
to O
a O
speciÔ¨Åc O
task O
can O
help O
to O
learn O
more O
useful O
semantic O
information O
from O
the O
teacher O
model O
. O
Different O
from O
obtaining O
the O
best O
results O
on O
the O
MNLI O
dataset O
, O
SRA+KD+TS O
brings O
few O
improvements O
compared O
to O
KS+TS O
( O
row O
19 O
vs. O
20 O
) O
. O
We O
attribute O
this O
to O
the O
difference O
in O
the O
results O
of O
pure O
student O
BiLSTM O
between O
our O
implementation O
and O
the O
one O
of O
Tang O
et O
al O
. O
( O
2019b O
) O
, O
though O
our O
scores O
are O
more O
constant O
with O
the O
baselines O
given O
by O
the O
GLUE O
benchmark O
( O
Wang O
et O
al O
. O
, O
2019 O
) O
. O
5.2 O
Model O
EfÔ¨Åciency O
Analysis O
To O
compare O
the O
inference O
speeds O
of O
different O
models O
, O
we O
also O
implement O
experiments O
on O
100k O
samples O
from O
the O
QQP O
dataset O
. O
The O
results O
are O
shown O
in O
Table O
2 O
. O
All O
the O
inference O
procedures O
are O
performed O
on O
a O
single O
P40 O
GPU O
with O
a O
batch O
size O
of O
1024 O
, O
respectively O
. O
As O
the O
inference O
time O
is O
affected O
by O
the O
test O
machine O
‚Äôs O
computing O
power O
, O
for O
fair O
comparisons O
with O
ELMO O
, O
BERT O
3 O
- O
PKD O
, O
BERT O
6 O
- O
PKD O
, O
and O
DSE O
, O
we O
inherit O
the O
speed O
- O
up O
factors O
from O
previous O
papers O
. O
Besides O
, O
the O
numbers O
of O
parameters O
reported O
in O
Table O
2 O
exclude O
those O
4https://www.kaggle.com/c/ O
quora O
- O
question O
- O
pairs O
/ O
discussion/32819 O
# O
latest-18949375Models O
20 O
% O
30 O
% O
50 O
% O
100 O
% O
BERT O
LARGE O
91.9 O
92.5 O
93.5 O
93.7 O
BiLSTM O
80.7 O
81.0 O
83.6 O
84.5 O
BiLSTM O
KD O
81.9 O
83.2 O
84.8 O
86.3 O
BiLSTM O
SRA O
85.9 O
87.3 O
88.1 O
89.2 O
Table O
3 O
: O
The O
accuracy O
scores O
evaluated O
on O
the O
SST-2 O
validation O
set O
. O
The O
models O
are O
trained O
with O
different O
proportions O
of O
the O
training O
data O
. O
from O
the O
embedding O
layers O
, O
since O
such O
components O
do O
not O
affect O
the O
inference O
speed O
and O
are O
positively O
related O
to O
the O
vocabulary O
sizes O
, O
i.e. O
, O
usually O
few O
words O
appeared O
for O
a O
speciÔ¨Åc O
task O
. O
From O
the O
results O
shown O
in O
Table O
2 O
, O
it O
can O
be O
observed O
that O
the O
BiLSTM O
based O
distilled O
models O
have O
fewer O
parameters O
than O
BERT O
, O
ELMO O
, O
as O
well O
as O
the O
other O
transformer O
- O
based O
models O
. O
Compared O
to O
the O
lightest O
model O
, O
both O
the O
BERT O
BASE O
and O
ELMO O
are O
around O
20 O
times O
larger O
in O
parameter O
size O
and O
20 O
times O
slower O
in O
inference O
speed O
. O
Even O
the O
smallest O
transformer O
based O
model O
BERT O
3PKD O
is O
also O
four O
times O
larger O
than O
our O
proposed O
BiLSTM O
SRA O
. O
Comparing O
with O
BiLSTM O
KD O
, O
although O
our O
proposed O
BiLSTM O
SRAis O
larger O
in O
parameter O
size O
due O
to O
the O
restriction O
of O
the O
sentence O
embedding O
‚Äôs O
dimension O
given O
by O
the O
teacher O
BERT O
, O
it O
stills O
inferences O
more O
efÔ¨Åciently O
. O
This O
is O
mainly O
due O
to O
the O
fact O
that O
the O
more O
hidden O
units O
in O
BiLSTM O
SRAare O
more O
accessible O
to O
calculated O
in O
parallel O
by O
the O
GPU O
core O
, O
while O
the O
larger O
word O
embedding O
size O
in O
BiLSTM O
KDslows O
down O
its O
inference O
efÔ¨Åciency O
. O
In O
conclusion O
, O
the O
cost O
and O
production O
per O
second O
of O
BiLSTM O
KDand O
BiLSTM O
SRA O
are O
within O
the O
same O
scale O
, O
but O
our O
method O
achieves O
better O
results O
on O
GLUE O
tasks O
according O
to O
the O
comparison O
shown O
in O
Table O
1 O
. O
5.3 O
InÔ¨Çuence O
of O
Task O
- O
speciÔ¨Åc O
Data O
Size O
Since O
pre O
- O
trained O
language O
models O
have O
wellinitialized O
parameters O
and O
only O
learn O
a O
few O
parameters O
from O
scratch O
, O
these O
models O
usually O
converge O
faster O
and O
are O
less O
dependent O
on O
large O
- O
scale O
annotations O
. O
Correspondingly O
, O
the O
non O
- O
task O
- O
speciÔ¨Åc O
distillation O
method O
proposed O
in O
this O
paper O
also O
aims O
to O
obtain O
a O
compressed O
pre O
- O
trained O
BERT O
and O
keep O
these O
desirable O
properties O
. O
To O
evaluate O
it O
, O
in O
this O
section O
, O
we O
discuss O
the O
inÔ¨Çuence O
of O
the O
task O
- O
speciÔ¨Åc O
training O
data O
and O
learning O
iterations O
on O
the O
performance O
of O
our O
model O
and O
the O
others O
. O
As O
illustrated O
in O
Table O
3 O
, O
we O
experiment O
in O
train O
0.7 O
0.75 O
0.8 O
0.85 O
0.9 O
0.95 O
  O
1 O
  O
3 O
  O
5 O
  O
7 O
  O
9Validation O
Accuracy O
Iteration O
Number O
BiLSTM O
+ O
30 O
% O
dataBiLSTM O
+ O
100 O
% O
data O
BiLSTM O
SRA O
  O
+ O
30 O
% O
dataBiLSTM O
SRA O
  O
+ O
100 O
% O
dataFigure O
2 O
: O
Learning O
curve O
on O
the O
QQP O
dataset O
. O
ing O
the O
models O
using O
different O
proportions O
of O
the O
dataset O
. O
BERT O
LARGE O
trained O
on O
the O
corresponding O
data O
stands O
for O
the O
teacher O
model O
of O
each O
BiLSTM O
KD O
. O
No O
doubt O
, O
all O
the O
models O
can O
achieve O
better O
results O
using O
more O
training O
data O
, O
while O
BERT O
performs O
the O
best O
. O
BERT O
even O
successfully O
predicts O
91.9 O
% O
of O
validation O
samples O
under O
only O
20 O
% O
training O
data O
. O
Comparing O
with O
the O
pure O
BiLSTM O
models O
, O
the O
BiLSTM O
KDmodels O
slightly O
improve O
the O
performances O
by O
1 O
% O
‚àº2 O
% O
, O
whereas O
BiLSTM O
SRA O
outperforms O
the O
best O
BiLSTM O
model O
as O
well O
as O
the O
BiLSTM O
KDtrained O
with O
20 O
% O
and O
30 O
% O
percent O
data O
respectively O
. O
Besides O
, O
similar O
to O
BERT O
, O
the O
difference O
of O
accuracy O
between O
BiLSTM O
SRAtrained O
with O
20 O
% O
and O
the O
one O
using O
100 O
% O
corpus O
is O
relatively O
small O
. O
This O
phenomenon O
indicates O
that O
our O
model O
converges O
faster O
and O
is O
less O
dependent O
on O
the O
amount O
of O
training O
data O
for O
downstream O
tasks O
. O
Such O
conclusions O
are O
also O
reÔ¨Çected O
in O
the O
comparison O
in O
Figure O
2 O
of O
the O
models O
‚Äô O
learning O
curves O
on O
QQP O
. O
Even O
though O
QQP O
is O
a O
large O
dataset O
to O
train O
a O
good O
BiLSTM O
model O
, O
it O
can O
be O
observed O
that O
BiLSTM O
SRAtrained O
with O
30 O
% O
data O
performs O
equivalent O
to O
BiLSTM O
using O
the O
whole O
corpus O
. O
Moreover O
, O
using O
100 O
% O
training O
data O
, O
BiLSTM O
SRAeven O
outperforms O
the O
converged O
BiLSTM O
after O
the O
Ô¨Årst O
epoch O
. O
Besides O
, O
all O
the O
BiLSTM O
SRAmodels O
converge O
in O
much O
fewer O
epochs O
. O
5.4 O
InÔ¨Çuence O
of O
Distilling O
Data O
Size O
Despite O
the O
task O
- O
speciÔ¨Åed O
data O
, O
Wikipedia O
corpus O
is O
used O
in O
the O
distillation O
procedure O
of O
our O
proposed O
method O
. O
We O
also O
pre O
- O
train O
different O
BiLSTM O
SRA O
base O
models O
using{1 O
, O
2 O
, O
4}million O
Wikipedia O
data O
, O
and O
the O
corresponding O
Ô¨Åne O
- O
tuning O
performances O
on O
SST-2 O
and O
MNLI O
are O
reported O
in O
Table O
4 O
. O
It O
can O
be O
observed O
that O
both O
the O
performances O
of76SizeDistillation O
SST-2 O
MNLI O
- O
m O
Loss O
Acc O
Acc O
0 O
M O
- O
84.5 O
70.23 O
1 O
M O
0.0288 O
88.9 O
( O
+4.4 O
) O
72.01 O
( O
+1.78 O
) O
2 O
M O
0.0257 O
89.3 O
( O
+4.8 O
) O
72.09 O
( O
+1.86 O
) O
4 O
M O
0.0241 O
89.4 O
( O
+4.9 O
) O
72.45 O
( O
+2.22 O
) O
Table O
4 O
: O
The O
distillation O
losses O
on O
the O
Wikipedia O
validation O
set O
and O
the O
accuracy O
scores O
of O
the O
downstream O
tasks O
various O
with O
the O
distillation O
data O
sizes O
. O
BiLSTM O
SRAon O
SST-2 O
and O
MNLI O
are O
proportional O
to O
the O
distillation O
loss O
. O
This O
observation O
indicates O
the O
effectiveness O
of O
our O
proposed O
distillation O
process O
and O
objective O
. O
Besides O
, O
distilling O
with O
adequate O
data O
is O
sufÔ¨Åcient O
to O
produce O
more O
BERT O
- O
like O
sentence O
representations O
as O
well O
as O
achieve O
better O
performance O
on O
the O
downstream O
tasks O
. O
Nevertheless O
, O
different O
from O
the O
fact O
that O
more O
training O
data O
has O
a O
signiÔ¨Åcant O
beneÔ¨Åt O
in O
a O
particular O
task O
, O
four O
times O
the O
distilling O
data O
can O
only O
improve O
around O
0.5 O
points O
on O
both O
SST-2 O
and O
MNLI O
- O
m O
tasks O
. O
Thus O
, O
our O
method O
does O
not O
require O
a O
vast O
amount O
of O
training O
data O
and O
a O
long O
training O
time O
to O
obtain O
good O
sentence O
representations O
. O
Furthermore O
, O
the O
second O
column O
‚Äôs O
loss O
scores O
suggest O
BiLSTM O
SRAcan O
generate O
more O
than O
95 O
% O
similar O
sentence O
embeddings O
with O
the O
ones O
given O
by O
BERT O
under O
the O
measure O
of O
the O
cosine O
similarity O
. O
5.5 O
Analysis O
on O
the O
Untuned O
Sentence O
Representations O
A O
notable O
characteristic O
of O
the O
pre O
- O
trained O
language O
models O
, O
such O
as O
ELMO O
, O
BERT O
, O
and O
certainly O
the O
non O
- O
task O
oriented O
distillation O
models O
, O
lies O
in O
the O
capability O
of O
providing O
sentence O
representations O
for O
quantifying O
similarities O
of O
sentences O
, O
without O
any O
tuning O
operation O
based O
on O
speciÔ¨Åc O
tasks O
. O
In O
this O
subsection O
, O
we O
conduct O
the O
comparisons O
among O
models O
by O
directly O
extracting O
their O
sentence O
embeddings O
without O
Ô¨Åne O
- O
tuning O
upon O
sentence O
similarity O
oriented O
tasks O
. O
Table O
5 O
lists O
the O
results O
of O
models O
on O
the O
QQP O
dataset O
. O
It O
should O
be O
noted O
that O
, O
in O
this O
table O
, O
ELMO O
, O
BERT O
BASE O
( O
CLS O
) O
and O
BERT O
BASE O
( O
averaged O
) O
are O
introduced O
as O
the O
comparison O
basis O
, O
since O
they O
can O
give O
the O
SOTA O
untuned O
sentence O
representations O
for O
the O
similarity O
measurement O
. O
The O
comparison O
mainly O
focuses O
on O
the O
performances O
ofModels O
Acc O
F O
1 O
ELMO O
65.1 O
64.4 O
BERT O
BASE O
( O
CLS O
) O
63.9 O
61.0 O
BERT O
BASE O
( O
averaged O
) O
66.4 O
64.1 O
BiLSTM O
KD O
56.3 O
56.6 O
BiLSTM O
SRA O
62.9 O
61.0 O
Table O
5 O
: O
Results O
of O
untuned O
sentence O
representing O
models O
on O
QQP O
dataset O
. O
our O
proposed O
BiLSTM O
SRAand O
BiLSTM O
KD O
. O
For O
a O
thorough O
comparison O
, O
we O
deÔ¨Åne O
the O
training O
objective O
of O
BiLSTM O
KDas O
Ô¨Åtting O
the O
cosine O
similarity O
score O
of O
the O
sentence O
pair O
directly O
given O
by O
the O
pretrained O
BERT O
BASE O
, O
which O
means O
both O
the O
teacher O
BERT O
and O
distilled O
models O
do O
not O
utilize O
the O
labels O
of O
QQP O
dataset O
. O
Even O
though O
the O
training O
goal O
of O
BiLSTM O
KDis O
more O
direct O
than O
BiLSTM O
SRA O
, O
it O
can O
be O
seen O
that O
our O
BiLSTM O
SRAoutperforms O
the O
former O
on O
the O
metrics O
. O
Furthermore O
, O
it O
achieves O
scores O
closed O
to O
those O
of O
BERT O
BASE O
. O
Besides O
, O
we O
can O
also O
observe O
that O
, O
for O
sentence O
similarity O
quantiÔ¨Åcation O
, O
averaging O
the O
context O
word O
embeddings O
as O
the O
sentence O
representation O
( O
ELMO O
and O
BERT O
BASE O
( O
averaged O
) O
) O
works O
better O
than O
taking O
the O
Ô¨Ånal O
hidden O
state O
corresponding O
to O
the O
[ O
CLS O
] O
token O
( O
BERT O
BASE O
( O
CLS O
) O
) O
. O
6 O
Conclusions O
In O
this O
paper O
, O
we O
have O
presented O
a O
sentence O
representation O
approximating O
oriented O
method O
for O
distilling O
the O
pre O
- O
trained O
BERT O
model O
into O
a O
much O
smaller O
BiLSTM O
without O
specifying O
tasks O
, O
so O
as O
to O
inherit O
the O
general O
semantic O
knowledge O
of O
BERT O
for O
better O
generalization O
and O
universal O
- O
usability O
. O
The O
experiments O
conducted O
based O
on O
the O
GLUE O
benchmark O
have O
shown O
that O
our O
proposed O
nontask O
- O
speciÔ¨Åc O
distillation O
methodology O
can O
improve O
the O
performances O
on O
multiple O
sentence O
- O
level O
downstream O
tasks O
. O
From O
the O
experimental O
results O
, O
the O
following O
conclusions O
can O
be O
drawn O
: O
1 O
) O
for O
a O
speciÔ¨Åed O
task O
, O
our O
proposed O
distillation O
method O
can O
bring O
the O
5 O
% O
improvement O
to O
the O
pure O
BiLSTM O
model O
on O
average O
; O
2 O
) O
the O
proposed O
model O
can O
outperform O
the O
state O
- O
of O
- O
the O
- O
art O
BiLSTM O
based O
pre O
- O
trained O
language O
model O
, O
which O
contains O
much O
more O
parameters O
; O
3 O
) O
compared O
to O
the O
task O
- O
speciÔ¨Åc O
distillation O
, O
our O
distilled O
model O
is O
less O
dependent O
on O
the O
corpus O
size O
of O
the O
downstream O
task O
with O
satisfying O
performances O
guaranteed.77References O
Oren O
Barkan O
, O
Noam O
Razin O
, O
Itzik O
Malkiel O
, O
Ori O
Katz O
, O
Avi O
Caciularu O
, O
and O
Noam O
Koenigstein O
. O
2019 O
. O
Scalable O
attentive O
sentence O
- O
pair O
modeling O
via O
distilled O
sentence O
embedding O
. O
arXiv O
preprint O
arXiv:1908.05161 O
. O
Yew O
Ken O
Chia O
, O
Sam O
Witteveen O
, O
and O
Martin O
Andrews O
. O
2019 O
. O
Transformer O
to O
cnn O
: O
Label O
- O
scarce O
distillation O
for O
efÔ¨Åcient O
text O
classiÔ¨Åcation O
. O
arXiv O
preprint O
arXiv:1909.03508 O
. O
Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O
Bert O
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
understanding O
. O
In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
4171‚Äì4186 O
. O
William O
B O
Dolan O
and O
Chris O
Brockett O
. O
2005 O
. O
Automatically O
constructing O
a O
corpus O
of O
sentential O
paraphrases O
. O
InProceedings O
of O
the O
Third O
International O
Workshop O
on O
Paraphrasing O
( O
IWP2005 O
) O
. O
Ameya O
Godbole O
, O
Aman O
Dalmia O
, O
and O
Sunil O
Kumar O
Sahu O
. O
2018 O
. O
Siamese O
neural O
networks O
with O
random O
forest O
for O
detecting O
duplicate O
question O
pairs O
. O
arXiv O
preprint O
arXiv:1801.07288 O
. O
Geoffrey O
Hinton O
, O
Oriol O
Vinyals O
, O
and O
Jeff O
Dean O
. O
2015 O
. O
Distilling O
the O
knowledge O
in O
a O
neural O
network O
. O
arXiv O
preprint O
arXiv:1503.02531 O
. O
Matthew O
Honnibal O
and O
Ines O
Montani O
. O
2017 O
. O
spacy O
2 O
: O
Natural O
language O
understanding O
with O
bloom O
embeddings O
, O
convolutional O
neural O
networks O
and O
incremental O
parsing O
. O
To O
appear O
, O
7 O
. O
Jeremy O
Howard O
and O
Sebastian O
Ruder O
. O
2018 O
. O
Universal O
language O
model O
Ô¨Åne O
- O
tuning O
for O
text O
classiÔ¨Åcation O
. O
In O
Proceedings O
of O
the O
56th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
328‚Äì339 O
. O
Xiaoqi O
Jiao O
, O
Yichun O
Yin O
, O
Lifeng O
Shang O
, O
Xin O
Jiang O
, O
Xiao O
Chen O
, O
Linlin O
Li O
, O
Fang O
Wang O
, O
and O
Qun O
Liu O
. O
2019 O
. O
Tinybert O
: O
Distilling O
BERT O
for O
natural O
language O
understanding O
. O
CoRR O
, O
abs/1909.10351 O
. O
Diederik O
P. O
Kingma O
and O
Jimmy O
Ba O
. O
2015 O
. O
Adam O
: O
A O
method O
for O
stochastic O
optimization O
. O
In O
3rd O
International O
Conference O
on O
Learning O
Representations O
, O
ICLR O
2015 O
, O
San O
Diego O
, O
CA O
, O
USA O
, O
May O
7 O
- O
9 O
, O
2015 O
, O
Conference O
Track O
Proceedings O
. O
Xiaodong O
Liu O
, O
Pengcheng O
He O
, O
Weizhu O
Chen O
, O
and O
Jianfeng O
Gao O
. O
2019a O
. O
Improving O
multi O
- O
task O
deep O
neural O
networks O
via O
knowledge O
distillation O
for O
natural O
language O
understanding O
. O
arXiv O
preprint O
arXiv:1904.09482 O
. O
Yinhan O
Liu O
, O
Myle O
Ott O
, O
Naman O
Goyal O
, O
Jingfei O
Du O
, O
Mandar O
Joshi O
, O
Danqi O
Chen O
, O
Omer O
Levy O
, O
Mike O
Lewis O
, O
Luke O
Zettlemoyer O
, O
and O
Veselin O
Stoyanov O
. O
2019b O
. O
Roberta O
: O
A O
robustly O
optimized O
bert O
pretraining O
approach O
. O
arXiv O
preprint O
arXiv:1907.11692 O
. O
Andrew O
McCallum O
and O
Kamal O
Nigam O
. O
1999 O
. O
Text O
classiÔ¨Åcation O
by O
bootstrapping O
with O
keywords O
, O
em O
and O
shrinkage O
. O
In O
Unsupervised O
Learning O
in O
Natural O
Language O
Processing O
. O
Tom O
McCoy O
, O
Ellie O
Pavlick O
, O
and O
Tal O
Linzen O
. O
2019 O
. O
Right O
for O
the O
wrong O
reasons O
: O
Diagnosing O
syntactic O
heuristics O
in O
natural O
language O
inference O
. O
In O
Proceedings O
of O
the O
57th O
Conference O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
ACL O
2019 O
, O
Florence O
, O
Italy O
, O
July O
28- O
August O
2 O
, O
2019 O
, O
Volume O
1 O
: O
Long O
Papers O
, O
pages O
3428‚Äì3448 O
. O
Sewon O
Min O
, O
Eric O
Wallace O
, O
Sameer O
Singh O
, O
Matt O
Gardner O
, O
Hannaneh O
Hajishirzi O
, O
and O
Luke O
Zettlemoyer O
. O
2019 O
. O
Compositional O
questions O
do O
not O
necessitate O
multi O
- O
hop O
reasoning O
. O
arXiv O
preprint O
arXiv:1906.02900 O
. O
Timothy O
Niven O
and O
Hung O
- O
Yu O
Kao O
. O
2019 O
. O
Probing O
neural O
network O
comprehension O
of O
natural O
language O
arguments O
. O
In O
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
4658‚Äì4664 O
. O
Jeffrey O
Pennington O
, O
Richard O
Socher O
, O
and O
Christopher O
Manning O
. O
2014 O
. O
Glove O
: O
Global O
vectors O
for O
word O
representation O
. O
In O
Proceedings O
of O
the O
2014 O
conference O
on O
empirical O
methods O
in O
natural O
language O
processing O
( O
EMNLP O
) O
, O
pages O
1532‚Äì1543 O
. O
Matthew O
E O
Peters O
, O
Mark O
Neumann O
, O
Mohit O
Iyyer O
, O
Matt O
Gardner O
, O
Christopher O
Clark O
, O
Kenton O
Lee O
, O
and O
Luke O
Zettlemoyer O
. O
2018 O
. O
Deep O
contextualized O
word O
representations O
. O
In O
Proceedings O
of O
NAACL O
- O
HLT O
, O
pages O
2227‚Äì2237 O
. O
Alec O
Radford O
, O
Jeffrey O
Wu O
, O
Rewon O
Child O
, O
David O
Luan O
, O
Dario O
Amodei O
, O
and O
Ilya O
Sutskever O
. O
2019 O
. O
Language O
models O
are O
unsupervised O
multitask O
learners O
. O
OpenAI O
Blog O
, O
1(8 O
) O
. O
Richard O
Socher O
, O
Alex O
Perelygin O
, O
Jean O
Wu O
, O
Jason O
Chuang O
, O
Christopher O
D O
Manning O
, O
Andrew O
Ng O
, O
and O
Christopher O
Potts O
. O
2013 O
. O
Recursive O
deep O
models O
for O
semantic O
compositionality O
over O
a O
sentiment O
treebank O
. O
In O
Proceedings O
of O
the O
2013 O
conference O
on O
empirical O
methods O
in O
natural O
language O
processing O
, O
pages O
1631‚Äì1642 O
. O
Siqi O
Sun O
, O
Yu O
Cheng O
, O
Zhe O
Gan O
, O
and O
Jingjing O
Liu O
. O
2019 O
. O
Patient O
knowledge O
distillation O
for O
bert O
model O
compression O
. O
In O
Proceedings O
of O
the O
2019 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
and O
the O
9th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
( O
EMNLP O
- O
IJCNLP O
) O
, O
pages O
4314‚Äì4323 O
. O
Zhiqing O
Sun O
, O
Hongkun O
Yu O
, O
Xiaodan O
Song O
, O
Renjie O
Liu O
, O
Yiming O
Yang O
, O
and O
Denny O
Zhou O
. O
2020 O
. O
Mobilebert O
: O
a O
compact O
task O
- O
agnostic O
BERT O
for O
resource O
- O
limited O
devices O
. O
In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics,78ACL O
2020 O
, O
Online O
, O
July O
5 O
- O
10 O
, O
2020 O
, O
pages O
2158 O
‚Äì O
2170 O
. O
Association O
for O
Computational O
Linguistics O
. O
Raphael O
Tang O
, O
Yao O
Lu O
, O
and O
Lin O
Jimmy O
. O
2019a O
. O
Natural O
language O
generation O
for O
effective O
knowledge O
distillation O
. O
In O
Proceedings O
of O
the O
Workshop O
on O
Deep O
Learning O
Approaches O
for O
Low O
- O
Resource O
NLP O
. O
Raphael O
Tang O
, O
Yao O
Lu O
, O
Linqing O
Liu O
, O
Lili O
Mou O
, O
Olga O
Vechtomova O
, O
and O
Jimmy O
Lin O
. O
2019b O
. O
Distilling O
taskspeciÔ¨Åc O
knowledge O
from O
bert O
into O
simple O
neural O
networks O
. O
arXiv O
preprint O
arXiv:1903.12136 O
. O
Henry O
Tsai O
, O
Jason O
Riesa O
, O
Melvin O
Johnson O
, O
Naveen O
Arivazhagan O
, O
Xin O
Li O
, O
and O
Amelia O
Archer O
. O
2019 O
. O
Small O
and O
practical O
bert O
models O
for O
sequence O
labeling O
. O
In O
Proceedings O
of O
the O
2019 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
and O
the O
9th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
( O
EMNLP O
- O
IJCNLP O
) O
, O
pages O
3623 O
‚Äì O
3627 O
. O
Alex O
Wang O
, O
Amanpreet O
Singh O
, O
Julian O
Michael O
, O
Felix O
Hill O
, O
Omer O
Levy O
, O
and O
Samuel O
R. O
Bowman O
. O
2019 O
. O
GLUE O
: O
A O
multi O
- O
task O
benchmark O
and O
analysis O
platform O
for O
natural O
language O
understanding O
. O
In O
7th O
International O
Conference O
on O
Learning O
Representations O
, O
ICLR O
2019 O
, O
New O
Orleans O
, O
LA O
, O
USA O
, O
May O
6 O
- O
9 O
, O
2019 O
. O
Wenhui O
Wang O
, O
Furu O
Wei O
, O
Li O
Dong O
, O
Hangbo O
Bao O
, O
Nan O
Yang O
, O
and O
Ming O
Zhou O
. O
2020 O
. O
Minilm O
: O
Deep O
selfattention O
distillation O
for O
task O
- O
agnostic O
compression O
of O
pre O
- O
trained O
transformers O
. O
arXiv O
: O
Computation O
and O
Language O
. O
Adina O
Williams O
, O
Nikita O
Nangia O
, O
and O
Samuel O
Bowman O
. O
2018 O
. O
A O
broad O
- O
coverage O
challenge O
corpus O
for O
sentence O
understanding O
through O
inference O
. O
In O
Proceedings O
of O
the O
2018 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
Papers O
) O
, O
pages O
1112‚Äì1122 O
. O
Han O
Xiao O
. O
2018 O
. O
bert O
- O
as O
- O
service O
. O
https://github O
. O
com O
/ O
hanxiao O
/ O
bert O
- O
as O
- O
service O
. O
Zhilin O
Yang O
, O
Zihang O
Dai O
, O
Yiming O
Yang O
, O
Jaime O
Carbonell O
, O
Ruslan O
Salakhutdinov O
, O
and O
Quoc O
V O
Le O
. O
2019 O
. O
Xlnet O
: O
Generalized O
autoregressive O
pretraining O
for O
language O
understanding O
. O
arXiv O
preprint O
arXiv:1906.08237 O
.79Proceedings O
of O
the O
1st O
Conference O
of O
the O
Asia O
- O
PaciÔ¨Åc O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
10th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
80‚Äì86 O
December O
4 O
- O
7 O
, O
2020 O
. O
¬© O
2020 O
Association O
for O
Computational O
Linguistics O
A O
Simple O
and O
Effective O
Usage O
of O
Word O
Clusters O
for O
CBOW O
Model O
Yukun O
Feng1 O
, O
Chenlong O
Hu1 O
, O
Hidetaka O
Kamigaito1 O
, O
Hiroya O
Takamura1,2and O
Manabu O
Okumura1 O
1Tokyo O
Institute O
of O
Technology O
2National O
Institute O
of O
Advanced O
Industrial O
Science O
and O
Technology O
( O
AIST O
) O
{ O
yukun O
, O
huchenlong O
, O
kamigaito O
, O
oku O
} O
@lr.pi.titech.ac.jp O
takamura@pi.titech.ac.jp O
Abstract O
We O
propose O
a O
simple O
and O
effective O
method O
for O
incorporating O
word O
clusters O
into O
the O
Continuous O
Bag O
- O
of O
- O
Words O
( O
CBOW O
) O
model O
. O
SpeciÔ¨Åcally O
, O
we O
propose O
to O
replace O
infrequent O
input O
and O
output O
words O
in O
CBOW O
model O
with O
their O
clusters O
. O
The O
resulting O
cluster O
- O
incorporated O
CBOW O
model O
produces O
embeddings O
of O
frequent O
words O
and O
a O
small O
amount O
of O
cluster O
embeddings O
, O
which O
will O
be O
Ô¨Åne O
- O
tuned O
in O
downstream O
tasks O
. O
We O
empirically O
show O
our O
replacing O
method O
works O
well O
on O
several O
downstream O
tasks O
. O
Through O
our O
analysis O
, O
we O
show O
that O
our O
method O
might O
be O
also O
useful O
for O
other O
similar O
models O
which O
produce O
word O
embeddings O
. O
1 O
Introduction O
Word O
embeddings O
have O
been O
widely O
applied O
to O
various O
natural O
language O
processing O
( O
NLP O
) O
tasks O
. O
These O
embeddings O
can O
be O
pretrained O
on O
a O
large O
corpus O
and O
carry O
useful O
semantic O
information O
. O
One O
of O
the O
most O
well O
- O
known O
methods O
for O
obtaining O
word O
embeddings O
is O
based O
on O
Continuous O
Bag O
- O
of O
- O
Words O
( O
CBOW O
) O
( O
Mikolov O
et O
al O
. O
, O
2013a O
) O
and O
there O
have O
been O
many O
research O
efforts O
to O
extend O
it O
. O
In O
this O
paper O
, O
we O
focus O
on O
incorporating O
word O
clusters O
into O
CBOW O
model O
. O
Each O
word O
cluster O
consists O
of O
words O
that O
function O
similarly O
. O
By O
aggregating O
such O
words O
, O
we O
can O
alleviate O
data O
sparsity O
, O
even O
though O
each O
of O
those O
words O
is O
infrequent O
. O
In O
the O
past O
few O
years O
, O
word O
clusters O
have O
been O
applied O
to O
various O
tasks O
, O
such O
as O
named O
- O
entity O
recognition O
( O
Ritter O
et O
al O
. O
, O
2011 O
) O
, O
machine O
translation O
( O
Wuebker O
et O
al O
. O
, O
2013 O
) O
and O
parsing O
( O
Kong O
et O
al O
. O
, O
2014 O
) O
. O
Many O
word O
clustering O
algorithms O
can O
be O
applied O
to O
a O
raw O
corpus O
with O
different O
languages O
and O
help O
us O
obtain O
word O
clusters O
easily O
without O
additional O
language O
resources O
. O
In O
our O
method O
, O
we O
keep O
only O
very O
frequent O
words O
and O
replace O
the O
other O
words O
with O
their O
clusters O
for O
both O
input O
and O
output O
words O
in O
the O
CBOW O
model O
. O
This O
is O
motivated O
by O
the O
fact O
that O
word O
clusters O
are O
more O
reliable O
than O
infrequent O
words O
. O
Thus O
, O
only O
very O
frequent O
word O
embeddings O
and O
a O
small O
amount O
of O
cluster O
embeddings O
are O
produced O
as O
the O
output O
. O
When O
Ô¨Åne O
- O
tuning O
the O
trained O
embeddings O
on O
downstream O
tasks O
, O
the O
embeddings O
of O
infrequent O
words O
within O
one O
cluster O
are O
initialized O
by O
the O
embedding O
of O
their O
cluster O
to O
increase O
the O
coverage O
of O
pretrained O
word O
embeddings O
. O
Since O
word O
embeddings O
are O
usually O
trained O
on O
the O
large O
- O
scale O
dataset O
. O
For O
making O
clusters O
on O
the O
large O
- O
scale O
dataset O
, O
we O
choose O
bidirectional O
, O
interpolated O
, O
reÔ¨Åning O
, O
and O
alternating O
( O
BIRA O
) O
predictive O
exchange O
algorithm O
( O
Dehdari O
et O
al O
. O
, O
2016)1as O
our O
clustering O
method O
. O
Because O
BIRA O
was O
reported O
to O
be O
faster O
than O
many O
other O
methods O
. O
Notably O
, O
it O
can O
produce O
800 O
clusters O
on O
1 O
billion O
English O
tokens O
in O
1.4 O
hours O
. O
We O
evaluate O
our O
cluster O
- O
incorporated O
word O
embeddings2on O
downstream O
tasks O
, O
in O
which O
Ô¨Ånetuning O
of O
word O
embeddings O
is O
involved O
. O
The O
evaluation O
for O
frequent O
words O
, O
for O
which O
our O
method O
also O
works O
well O
, O
on O
word O
similarity O
tasks O
can O
be O
found O
in O
appendix O
A. O
For O
the O
downstream O
tasks O
, O
we O
choose O
language O
modeling O
( O
LM O
) O
tasks O
, O
which O
are O
a O
fundamental O
task O
in O
NLP O
, O
as O
well O
as O
two O
machine O
translation O
( O
MT O
) O
tasks O
. O
To O
verify O
the O
effect O
of O
word O
clusters O
across O
different O
languages O
, O
8 O
typologically O
diverse O
languages O
are O
further O
selected O
for O
the O
LM O
task O
. O
Finally O
, O
an O
analysis O
is O
provided O
for O
our O
method O
. O
In O
summary O
, O
our O
replacing O
method O
can O
be O
used O
to O
improve O
the O
embeddings O
of O
frequent O
and O
infrequent O
words O
, O
to O
reduce O
the O
number O
of O
word O
embeddings O
and O
to O
make O
training O
more O
effective O
. O
1We O
used O
ClusterCat O
( O
https://github.com/jonsafari/clustercat O
) O
as O
the O
implementation O
. O
2https://github.com/yukunfeng/cluster-cbow802 O
Related O
Work O
A O
number O
of O
related O
research O
efforts O
have O
been O
done O
to O
help O
to O
learn O
better O
word O
embeddings O
aiming O
at O
different O
aspects O
. O
For O
example O
, O
Neelakantan O
et O
al O
. O
( O
2014 O
) O
proposed O
an O
extension O
that O
learns O
multiple O
embeddings O
per O
word O
type O
. O
Ammar O
et O
al O
. O
( O
2016 O
) O
proposed O
methods O
for O
estimating O
embeddings O
for O
different O
languages O
in O
a O
single O
shared O
embedding O
space O
. O
There O
is O
also O
a O
lot O
of O
work O
that O
incorporates O
internal O
information O
of O
words O
, O
such O
as O
character O
- O
level O
information O
( O
Chen O
et O
al O
. O
, O
2015 O
; O
Bojanowski O
et O
al O
. O
, O
2017 O
) O
and O
morpheme O
information O
( O
Luong O
et O
al O
. O
, O
2013 O
; O
Qiu O
et O
al O
. O
, O
2014 O
) O
. O
Our O
research O
aims O
at O
another O
aspect O
and O
focuses O
on O
incorporating O
word O
clusters O
into O
the O
CBOW O
model O
, O
which O
has O
not O
been O
studied O
before O
. O
There O
have O
also O
been O
some O
previous O
researches O
that O
utilized O
word O
clusters O
for O
reducing O
the O
number O
of O
word O
embeddings O
. O
Botha O
et O
al O
. O
( O
2017 O
) O
used O
word O
clusters O
to O
reduce O
the O
network O
size O
for O
the O
part O
- O
of O
- O
speech O
tagging O
task O
. O
Shu O
and O
Nakayama O
( O
2018 O
) O
attempted O
to O
compress O
word O
embeddings O
without O
losing O
performance O
by O
constructing O
the O
embeddings O
with O
a O
few O
basic O
vectors O
. O
Our O
goal O
is O
different O
from O
the O
previous O
work O
in O
that O
we O
attempt O
to O
learn O
better O
word O
embeddings O
and O
do O
not O
aim O
at O
reducing O
the O
parameters O
when O
our O
embeddings O
are O
Ô¨Åne O
- O
tuned O
in O
downstream O
tasks O
. O
Nonetheless O
, O
the O
reduction O
of O
the O
number O
of O
word O
embeddings O
from O
the O
CBOW O
model O
before O
Ô¨Åne O
- O
tuning O
is O
still O
one O
of O
our O
goals O
as O
we O
can O
save O
space O
to O
store O
these O
embeddings O
and O
save O
time O
to O
download O
them O
. O
For O
example O
, O
Google O
News O
Vectors O
have O
around O
3 O
million O
words O
, O
and O
we O
need O
only O
2 O
% O
of O
the O
number O
of O
the O
word O
embeddings O
if O
we O
choose O
100 O
K O
most O
frequent O
words O
and O
10 O
K O
word O
clusters O
in O
our O
method O
. O
3 O
Our O
Method O
3.1 O
CBOW O
Model O
Letwtdenote O
thet O
- O
th O
word O
in O
a O
given O
text O
. O
We O
adopt O
the O
basic O
CBOW O
model O
architecture O
for O
learning O
word O
embeddings O
. O
The O
CBOW O
model O
predicts O
the O
output O
word O
wtgiven O
the O
input O
words O
in O
the O
window O
which O
precede O
or O
follow O
the O
output O
word O
. O
When O
the O
window O
size O
is O
2 O
, O
as O
an O
example O
, O
the O
input O
words O
are O
wt‚àí2,wt‚àí1,wt+1,wt+2 O
. O
We O
denote O
the O
input O
and O
output O
embeddings O
of O
word O
wirespectively O
as O
/ O
vector O
xiand O
/ O
vector O
oi O
. O
The O
CBOW O
model O
computesthe O
hidden O
representation O
as O
follows O
: O
/vectorh=1 O
2cc O
/ O
summationdisplay O
i=‚àíc O
, O
i O
/ O
negationslash=0 O
/ O
vector O
xt+i O
, O
( O
1 O
) O
wherecis O
the O
window O
size O
. O
We O
use O
negative O
sampling O
( O
Mikolov O
et O
al O
. O
, O
2013b O
) O
to O
train O
the O
CBOW O
model O
by O
maximizing O
the O
following O
objective O
function O
: O
logœÉ(/vectorhT O
/ O
vector O
ot O
) O
+ O
k O
/ O
summationdisplay O
j=1logœÉ(‚àí/vectorhT O
/ O
vector O
oj O
) O
, O
( O
2 O
) O
wherekis O
the O
size O
of O
the O
negative O
sample O
, O
/vector O
ojis O
thej O
- O
th O
noise O
word O
embedding O
and O
œÉis O
the O
sigmoid O
function O
. O
Each O
word O
in O
the O
negative O
sample O
is O
drawn O
from O
the O
unigram O
distribution O
. O
AYeragereplaceZordembeddingsclXsWerembeddings O
replaceWargeWnoiseinpXW O
ZordsoXWpXW O
Zords O
Figure O
1 O
: O
CBOW O
architecture O
with O
our O
replacing O
method O
for O
input O
and O
output O
words O
trained O
with O
negative O
sampling O
. O
Suppose O
that O
wt‚àí2,wt+1,o1ando3are O
infrequent O
words O
. O
3.2 O
Replacing O
Methods O
As O
a O
method O
for O
incorporating O
word O
clusters O
, O
we O
propose O
to O
replace O
infrequent O
words O
with O
their O
clusters O
for O
the O
input O
and O
output O
. O
The O
architecture O
is O
shown O
in O
Figure O
1 O
. O
This O
is O
motivated O
by O
the O
intuition O
that O
the O
embeddings O
of O
clusters O
should O
be O
more O
reliable O
than O
those O
of O
infrequent O
words O
. O
We O
denote O
the O
embedding O
of O
the O
cluster O
for O
word O
wt+i O
as O
/ O
vectordt+i O
. O
We O
present O
the O
following O
two O
replacing O
methods O
: O
‚Ä¢ReIn O
: O
In O
the O
input O
, O
/vector O
xt+iin O
Eq O
. O
( O
1 O
) O
will O
be O
replaced O
with O
/vectordt+iif O
the O
frequency O
of O
wt+iis O
less O
than O
threshold O
fin O
. O
‚Ä¢ReOut O
: O
In O
the O
output O
, O
output O
words O
whose O
frequency O
is O
less O
than O
foutare O
replaced O
with O
their O
clusters O
. O
Thus O
, O
in O
negative O
sampling O
, O
a O
noise O
word O
will O
be O
sampled O
from O
clusters O
and O
frequent O
words.81As O
with O
the O
standard O
CBOW O
model O
, O
we O
use O
the O
input O
word O
embeddings O
and O
input O
cluster O
embeddings O
for O
downstream O
tasks O
. O
Thresholds O
finand O
foutare O
set O
to O
100 O
in O
all O
experiments O
. O
Due O
to O
this O
large O
value O
, O
each O
cluster O
contains O
many O
infrequent O
words O
, O
which O
share O
the O
same O
embedding O
. O
We O
use O
two O
methods O
together O
, O
which O
is O
referred O
to O
as O
ReIn+ReOut O
in O
the O
following O
experiments O
. O
3.2.1 O
Motivation O
of O
ReIn O
and O
ReOut O
Since O
the O
embeddings O
of O
clusters O
are O
learned O
by O
aggregating O
many O
infrequent O
words O
, O
they O
are O
more O
robust O
than O
the O
embeddings O
of O
the O
infrequent O
words O
. O
During O
the O
Ô¨Åne O
- O
tuning O
process O
for O
a O
downstream O
task O
, O
the O
embeddings O
of O
infrequent O
words O
are O
Ô¨Årst O
initialized O
with O
the O
embeddings O
of O
their O
clusters O
. O
As O
most O
of O
these O
infrequent O
words O
appear O
only O
a O
few O
times O
, O
these O
embeddings O
will O
not O
be O
updated O
far O
away O
from O
each O
other O
within O
one O
cluster O
. O
The O
visualization O
of O
these O
embeddings O
before O
and O
after O
Ô¨Åne O
- O
tuning O
can O
be O
found O
in O
the O
appendix O
B. O
As O
a O
result O
, O
these O
embeddings O
for O
infrequent O
words O
become O
more O
reliable O
since O
originally O
most O
infrequent O
word O
embeddings O
are O
updated O
only O
several O
times O
and O
are O
not O
far O
away O
from O
where O
they O
were O
randomly O
initialized O
. O
Since O
the O
context O
of O
frequent O
words O
becomes O
less O
noisy O
by O
replacing O
all O
the O
infrequent O
words O
with O
their O
clusters O
, O
the O
learned O
frequent O
word O
embeddings O
are O
also O
better O
, O
as O
shown O
later O
in O
our O
experiments O
. O
The O
standard O
CBOW O
model O
is O
usually O
trained O
with O
negative O
sampling O
, O
which O
is O
designed O
for O
speeding O
up O
the O
training O
process O
. O
By O
using O
ReOut O
, O
infrequent O
noise O
words O
will O
be O
replaced O
with O
their O
clusters O
, O
which O
contain O
more O
noise O
words O
than O
the O
original O
CBOW O
model O
. O
As O
a O
result O
, O
ReOut O
makes O
the O
training O
of O
the O
CBOW O
model O
more O
effective O
, O
as O
shown O
later O
in O
our O
experiments O
. O
4 O
Experiments O
on O
LM O
and O
MT O
We O
applied O
our O
embeddings O
to O
downstream O
tasks O
: O
language O
modeling O
( O
LM O
) O
and O
low O
- O
resource O
machine O
translation O
( O
MT O
) O
. O
When O
applying O
to O
the O
downstream O
tasks O
, O
we O
only O
used O
the O
training O
data O
of O
the O
speciÔ¨Åc O
task O
to O
obtain O
word O
clusters O
and O
embeddings O
without O
any O
extra O
data O
. O
We O
then O
used O
the O
learned O
embeddings O
to O
initialize O
the O
lookup O
table O
of O
word O
embeddings O
for O
the O
task O
. O
In O
this O
paper O
, O
we O
limit O
the O
applications O
of O
our O
model O
to O
relatively O
small O
datasets O
to O
demonstrate O
the O
usefulness O
of O
our O
method O
. O
We O
plan O
to O
conduct O
larger O
- O
scale O
experiments O
on O
more O
downstream O
tasks O
in O
future O
work O
. O
Inthe O
following O
tables O
, O
CBOW O
and O
ReIn+ReOut O
indicate O
that O
they O
are O
initialization O
methods O
for O
speciÔ¨Åc O
downstream O
tasks O
. O
4.1 O
Hyper O
- O
parameter O
Settings O
In O
this O
section O
, O
we O
describe O
the O
hyper O
- O
parameters O
for O
producing O
word O
clusters O
and O
word O
embeddings O
. O
As O
we O
mentioned O
before O
, O
we O
obtained O
word O
clusters O
through O
the O
ClusterCat O
software O
. O
For O
most O
hyperparameters O
, O
we O
used O
its O
default O
values O
. O
We O
set O
the O
number O
of O
clusters O
to O
600 O
in O
all O
our O
experiments O
. O
Since O
our O
work O
involves O
many O
tasks O
in O
total O
, O
it O
is O
hard O
to O
choose O
the O
optimal O
number O
of O
word O
clusters O
for O
each O
task O
. O
We O
experimented O
with O
several O
values O
( O
600 O
, O
800 O
and O
1000 O
) O
and O
observed O
the O
same O
trend O
. O
Thus O
, O
we O
simply O
chose O
600 O
, O
for O
convenience O
, O
for O
all O
tasks O
. O
For O
producing O
word O
embeddings O
, O
our O
implementation O
was O
based O
on O
the O
fasttext3 O
. O
Our O
cluster O
- O
incorporated O
CBOW O
model O
and O
the O
standard O
CBOW O
model O
were O
trained O
under O
the O
same O
hyper O
- O
parameters O
. O
We O
set O
most O
hyper O
- O
parameters O
as O
its O
default O
values O
. O
Namely O
, O
we O
set O
the O
training O
epoch O
to O
5 O
, O
the O
number O
of O
negative O
examples O
to O
5 O
, O
the O
window O
size O
to O
5 O
, O
and O
the O
minimum O
count O
of O
word O
occurrence O
to O
54 O
. O
4.2 O
LM O
on O
Standard O
English O
Datasets O
We O
test O
ReIn+ReOut O
based O
on O
the O
recent O
state O
- O
ofthe O
- O
art O
awd O
- O
lstm O
- O
lm O
codebase5(Merity O
et O
al O
. O
, O
2018 O
) O
using O
two O
standard O
language O
modeling O
datasets O
: O
Penn O
Treebank O
( O
PTB O
) O
and O
WikiText-2 O
( O
Wiki2 O
) O
. O
We O
followed O
exactly O
the O
same O
setting O
in O
the O
source O
code O
. O
The O
results O
are O
shown O
in O
Table O
1 O
, O
and O
we O
found O
that O
our O
ReIn+ReOut O
is O
effective O
even O
with O
the O
strong O
baseline O
. O
PTB O
Wiki2 O
AWD O
- O
LSTM O
w/o O
Ô¨Åne O
- O
tuning O
( O
Merity O
et O
al O
. O
, O
2018)58.80 O
66.00 O
CBOW O
58.39 O
65.48 O
ReIn+ReOut O
57.85 O
63.93 O
Table O
1 O
: O
Perplexity O
results O
on O
PTB O
and O
Wiki2 O
. O
4.3 O
Low O
- O
resource O
NMT O
We O
applied O
our O
method O
to O
the O
standard O
long O
- O
short O
term O
memory O
networks O
( O
LSTMs O
) O
based O
sequenceto O
- O
sequence O
( O
seq2seq O
) O
model O
on O
two O
datasets O
: O
German O
- O
English O
( O
de O
- O
en O
) O
with O
153 O
K O
sentence O
pairs O
3https://github.com/facebookresearch/fastText O
4When O
we O
set O
the O
minimum O
count O
of O
word O
occurrence O
to O
1 O
, O
the O
standard O
CBOW O
does O
not O
perform O
well O
. O
5https://github.com/salesforce/awd-lstm-lm82from O
IWSLT O
2014 O
( O
Cettolo O
et O
al O
. O
, O
2014 O
) O
, O
EnglishVietnamese O
( O
en O
- O
vi O
) O
with O
133 O
K O
sentence O
pairs O
from O
IWSLT O
2015 O
( O
Cettolo O
et O
al O
. O
, O
2012 O
) O
. O
The O
detailed O
data O
statistics O
of O
two O
low O
- O
resource O
NMT O
datasets O
is O
in O
Table O
2 O
. O
We O
used O
the O
opennmt O
- O
py O
toolkit6with O
a O
2 O
- O
layer O
bidirectional O
LSTM O
with O
hidden O
size O
of O
500 O
and O
set O
the O
training O
epoch O
to O
30 O
. O
The O
word O
embedding O
size O
is O
set O
to O
500 O
and O
the O
batch O
size O
is O
64 O
. O
We O
trained O
the O
seq2seq O
models O
by O
the O
SGD O
optimizer O
with O
start O
learning O
rate O
being O
1.0 O
, O
which O
will O
be O
decayed O
by O
0.5 O
if O
perplexity O
does O
not O
decrease O
on O
the O
validation O
set O
. O
Other O
hyper O
- O
parameters O
were O
kept O
default O
. O
We O
also O
include O
some O
published O
results O
based O
on O
LSTM O
- O
based O
seq2seq O
models O
to O
gauge O
the O
result O
of O
our O
baseline O
. O
As O
shown O
in O
Table O
3 O
, O
without O
any O
extra O
language O
pair O
resources O
, O
the O
ReIn+ReOut O
initialization O
improves O
the O
BLEU O
score O
over O
the O
baseline O
by O
1.29 O
and O
0.51 O
points O
on O
de O
- O
en O
, O
en O
- O
vi O
respectively O
. O
de O
- O
en O
en O
- O
vi O
# O
Training O
pairs O
153,348 O
133,317 O
# O
Test O
pairs O
6,750 O
1,268 O
# O
Valid O
pairs O
6,970 O
1,553 O
Train O
V O
ocab O
( O
source O
) O
103,796 O
54,169 O
Train O
V O
ocab O
( O
target O
) O
50,045 O
25,615 O
Table O
2 O
: O
Data O
statistics O
of O
two O
low O
- O
resource O
NMT O
datasets O
. O
de O
- O
en O
en O
- O
vi O
seq2seq O
with O
attention O
( O
Luong O
and O
Manning O
, O
2015 O
) O
- O
23.3 O
AC+LL O
( O
Bahdanau O
et O
al O
. O
, O
2017 O
) O
28.53 O
NPMT O
( O
Huang O
et O
al O
. O
, O
2018 O
) O
29.92 O
27.69 O
Our O
seq2seq O
with O
attention O
28.95 O
28.16 O
CBOW O
29.25 O
28.24 O
Our O
ReIn+ReOut O
30.24 O
28.67 O
Table O
3 O
: O
BLEU O
scores O
on O
two O
low O
- O
resource O
MT O
datasets O
. O
NPMT O
in O
Huang O
et O
al O
. O
( O
2018 O
) O
used O
a O
neural O
phrase O
- O
based O
machine O
translation O
model O
and O
AC+LL O
in O
Bahdanau O
et O
al O
. O
( O
2017 O
) O
used O
a O
one O
- O
layer O
GRU O
encoder O
and O
decoder O
with O
attention O
. O
4.4 O
LM O
in O
Diverse O
Languages O
To O
verify O
the O
effect O
of O
word O
clusters O
on O
different O
languages O
, O
we O
selected O
8 O
datasets O
containing O
typologically O
diverse O
languages O
from O
LM O
datasets O
released O
by O
Gerz O
et O
al O
. O
( O
2018 O
) O
. O
The O
data O
statistics O
of O
8 O
LM O
datasets O
is O
in O
Table O
5 O
. O
We O
basically O
used O
standard O
LSTMs O
instead O
of O
AWD O
- O
LSTM O
- O
LM O
to O
save O
time O
. O
We O
chose O
the O
available O
standard O
LSTM O
- O
LM O
code7 O
. O
Hyper O
- O
parameters O
of O
our O
standard O
LSTM O
model O
on O
6https://github.com/OpenNMT/OpenNMT-py O
7https://github.com/pytorch/examples/tree/master/ O
word O
language O
modellanguage O
modeling O
tasks O
is O
in O
Table O
4 O
. O
The O
results O
are O
shown O
in O
Table O
6 O
. O
Our O
LSTM O
- O
LM O
obtained O
better O
results O
than O
the O
one O
from O
Gerz O
et O
al O
. O
( O
2018 O
) O
on O
all O
datasets O
. O
As O
we O
see O
, O
ReIn+ReOut O
is O
effective O
for O
typologically O
diverse O
languages O
and O
also O
requires O
a O
smaller O
input O
vocabulary O
. O
For O
example O
, O
the O
input O
vocabulary O
of O
ReIn+ReOut O
for O
en O
dataset O
contains O
1.3 O
K O
words O
while O
the O
full O
vocabulary O
50K. O
Embedding O
size O
200 O
Epochs O
40 O
LSTM O
layers O
2 O
Optimizer O
SGD O
LSTM O
sequence O
length O
35 O
Learning O
rate O
20 O
LSTM O
hidden O
unit O
200 O
Learning O
rate O
decay O
4 O
Param O
. O
init O
: O
rand O
uniform O
[ O
-0.1,0.1 O
] O
Gradient O
clipping O
0.25 O
Dropout O
0.2 O
Batch O
size O
20 O
Table O
4 O
: O
Hyper O
- O
parameters O
of O
our O
standard O
LSTM O
model O
on O
language O
modeling O
task O
. O
5 O
Analysis O
In O
this O
section O
, O
we O
analyse O
ReIn+ReOut O
on O
the O
basis O
of O
LM O
experiments O
with O
en O
and O
de O
datasets O
. O
5.1 O
Targeted O
Perplexity O
Results O
To O
show O
the O
gain O
for O
frequent O
and O
infrequent O
words O
, O
we O
measured O
the O
perplexity O
for O
frequent O
and O
infrequent O
words O
in O
the O
test O
data O
separately O
. O
SpeciÔ¨Åcally O
, O
we O
calculated O
the O
perplexity O
of O
the O
next O
word O
, O
when O
an O
infrequent O
word O
is O
given O
as O
the O
current O
word O
. O
A O
similar O
analysis O
on O
language O
models O
can O
be O
found O
in O
Vania O
and O
Lopez O
( O
2017 O
) O
. O
Our O
analysis O
do O
not O
contain O
new O
words O
in O
the O
test O
dataset O
. O
The O
results O
are O
shown O
in O
Table O
7 O
. O
As O
we O
see O
, O
ReIn+ReOut O
is O
more O
effective O
than O
CBOW O
in O
learning O
both O
the O
embeddings O
of O
frequent O
and O
infrequent O
words O
, O
as O
we O
explained O
in O
Sec O
. O
3.2.1 O
. O
5.2 O
Ablation O
Study O
The O
results O
of O
ablation O
study O
are O
in O
Table O
8 O
. O
Comparing O
the O
methods O
ReIn O
and O
CBOW O
, O
we O
found O
replacing O
only O
input O
infrequent O
words O
in O
CBOW O
also O
works O
better O
than O
the O
original O
CBOW O
. O
We O
can O
also O
conclude O
that O
replacing O
only O
output O
infrequent O
words O
in O
CBOW O
works O
better O
than O
the O
original O
CBOW O
, O
by O
comparing O
ReOut O
and O
CBOW O
. O
Both O
ReIn O
and O
ReOut O
work O
well O
even O
when O
they O
are O
used O
alone O
. O
As O
mentioned O
in O
the O
motivation O
of O
ReOut O
, O
it O
makes O
the O
training O
more O
effective O
. O
To O
verify83TypologyTrain O
vocab#Train O
tokens#Test O
tokens#Valid O
tokens#Input O
vocab O
of O
ReIn+ReOut O
zh O
( O
Chinese O
) O
Isolating O
43674 O
746 O
K O
56.8 O
K O
56.9 O
K O
1661 O
vi O
( O
Vietnamese O
) O
Isolating O
32065 O
754 O
K O
61.9 O
K O
64.8 O
K O
1716 O
de O
( O
German O
) O
Fusional O
80743 O
682 O
K O
51.3 O
K O
52.6 O
K O
1163 O
en O
( O
English O
) O
Fusiona O
55522 O
783 O
K O
59.5 O
K O
57.3 O
K O
1381 O
ar O
( O
Arabic O
) O
IntroÔ¨Çexive O
89091 O
723 O
K O
54.7 O
K O
55.2 O
K O
1431 O
he O
( O
Hebrew O
) O
IntroÔ¨Çexive O
83223 O
719 O
K O
54.7 O
K O
52.9 O
K O
1345 O
et O
( O
Estonian O
) O
Agglutinative O
94184 O
556 O
K O
38.6 O
K O
40.0 O
K O
1285 O
tr O
( O
Turkish O
) O
Agglutinative O
90847 O
627 O
K O
45.2 O
K O
47.4 O
K O
1241 O
Table O
5 O
: O
Data O
statistics O
of O
8 O
language O
modeling O
datasets O
and O
size O
of O
input O
vocabulary O
of O
our O
ReIn+ReOut O
. O
Dataset O
Random O
CBOW O
ReIn+ReOut O
zh O
555 O
527 O
494 O
vi O
153 O
145 O
138 O
de O
609 O
542 O
484 O
en O
365 O
317 O
289 O
ar O
1647 O
1447 O
1305 O
he O
1482 O
1236 O
1175 O
et O
1451 O
1157 O
1004 O
tr O
1379 O
1220 O
1148 O
Table O
6 O
: O
Perplexity O
results O
of O
standard O
LSTM O
LM O
on O
8 O
datasets O
with O
different O
initialization O
methods O
. O
Freq O
. O
Infreq O
. O
All O
enCBOW O
340 O
198 O
283 O
ReIn+ReOut O
316 O
184 O
264 O
deCBOW O
591 O
352 O
489 O
ReIn+ReOut O
564 O
318 O
458 O
Table O
7 O
: O
Targeted O
perplexity O
results O
of O
standard O
LSTM O
LM O
with O
different O
initializations O
. O
this O
, O
we O
increased O
the O
number O
of O
negative O
samples O
for O
ReIn O
and O
CBOW O
. O
The O
training O
will O
be O
more O
effective O
if O
we O
increase O
the O
number O
of O
negative O
samples O
, O
while O
training O
the O
model O
will O
also O
take O
longer O
time O
. O
As O
we O
increased O
the O
size O
of O
negative O
samples O
, O
we O
obtained O
better O
results O
for O
both O
ReIn O
and O
CBOW O
. O
We O
increased O
it O
only O
to O
30 O
because O
we O
did O
not O
observe O
improvements O
when O
we O
made O
it O
further O
larger O
. O
This O
result O
indicates O
that O
we O
can O
use O
word O
clusters O
to O
obtain O
better O
results O
with O
a O
small O
amount O
of O
negative O
samples O
. O
In O
reality O
, O
we O
can O
also O
use O
off O
- O
the O
- O
shelf O
word O
clusters O
to O
avoid O
spending O
time O
for O
producing O
word O
clusters O
. O
en O
de O
en O
de O
ReIn O
300 O
528 O
CBOW O
317 O
542 O
ReIn O
neg+10 O
293 O
499 O
CBOW O
neg+10 O
309 O
523 O
ReIn O
neg+30 O
300 O
494 O
CBOW O
neg+30 O
312 O
554 O
ReIn+ReOut O
289 O
484 O
ReOut O
312 O
515 O
Table O
8 O
: O
Perplexity O
results O
of O
LSTM O
LM O
by O
changing O
the O
number O
of O
negative O
samples O
. O
‚Äô O
+ O
neg O
‚Äô O
represents O
the O
number O
of O
negative O
samples O
, O
which O
is O
5 O
at O
default.5.3 O
LM O
Results O
on O
Off O
- O
the O
- O
shelf O
Vectors O
To O
gauge O
the O
improvements O
, O
we O
used O
off O
- O
the O
- O
shelf O
pretrained O
word O
vectors O
in O
English O
: O
GloVe O
vectors O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
and O
Google O
News O
Vectors8 O
. O
We O
obtained O
258 O
, O
290 O
and O
289 O
perplexity O
scores O
on O
en O
with O
Google O
News O
Vectors O
, O
Glove O
vectors O
and O
ReIn+ReOut O
respectively O
. O
Although O
ReIn+ReOut O
underperforms O
Google O
News O
Vectors O
, O
which O
were O
trained O
on O
100 O
billion O
tokens O
, O
it O
obtained O
the O
results O
comparable O
to O
Glove O
Vectors O
, O
trained O
on O
6 O
billion O
tokens O
. O
This O
indicates O
that O
our O
ReIn+ReOut O
is O
effective O
even O
without O
extra O
training O
data O
( O
only O
783 O
K O
training O
tokens O
in O
en O
) O
. O
en O
Google O
News O
Vectors O
258 O
GloVe O
Vectors O
290 O
ReIn+ReOut O
289 O
Table O
9 O
: O
Perplexity O
results O
of O
standard O
LSTM O
compared O
with O
off O
- O
the O
- O
shelf O
vectors O
. O
6 O
Conclusion O
We O
proposed O
a O
simple O
and O
effective O
method O
to O
incorporate O
word O
clusters O
into O
the O
CBOW O
model O
. O
Our O
method O
is O
effective O
on O
several O
downstream O
tasks O
. O
For O
future O
work O
, O
we O
will O
test O
our O
methods O
on O
larger O
corpora O
and O
also O
add O
more O
downstream O
tasks O
. O
We O
will O
also O
study O
how O
to O
combine O
word O
clusters O
and O
subword O
information O
. O
Acknowledgments O
We O
would O
like O
to O
thank O
anonymous O
reviewers O
for O
their O
constructive O
comments O
and O
Hu O
also O
thanks O
his O
support O
from O
China O
Scholarship O
Council O
. O
Abstract O
The O
recently O
introduced O
pre O
- O
trained O
language O
model O
BERT O
advances O
the O
state O
- O
of O
- O
the O
- O
art O
on O
many O
NLP O
tasks O
through O
the O
Ô¨Åne O
- O
tuning O
approach O
, O
but O
few O
studies O
investigate O
how O
the O
Ô¨Åne O
- O
tuning O
process O
improves O
the O
model O
performance O
on O
downstream O
tasks O
. O
In O
this O
paper O
, O
we O
inspect O
the O
learning O
dynamics O
of O
BERT O
Ô¨Åne O
- O
tuning O
with O
two O
indicators O
. O
We O
use O
JS O
divergence O
to O
detect O
the O
change O
of O
the O
attention O
mode O
and O
use O
SVCCA O
distance O
to O
examine O
the O
change O
to O
the O
feature O
extraction O
mode O
during O
BERT O
Ô¨Åne O
- O
tuning O
. O
We O
conclude O
that O
BERT O
Ô¨Åne O
- O
tuning O
mainly O
changes O
the O
attention O
mode O
of O
the O
last O
layers O
and O
modiÔ¨Åes O
the O
feature O
extraction O
mode O
of O
the O
intermediate O
and O
last O
layers O
. O
Moreover O
, O
we O
analyze O
the O
consistency O
of O
BERT O
Ô¨Åne O
- O
tuning O
between O
different O
random O
seeds O
and O
different O
datasets O
. O
In O
summary O
, O
we O
provide O
a O
distinctive O
understanding O
of O
the O
learning O
dynamics O
of O
BERT O
Ô¨Ånetuning O
, O
which O
sheds O
some O
light O
on O
improving O
the O
Ô¨Åne O
- O
tuning O
results O
. O
1 O
Introduction O
BERT O
( O
Bidirectional O
Encoder O
Representations O
from O
Transformers O
; O
Devlin O
et O
al O
. O
2019 O
) O
is O
a O
large O
pre O
- O
trained O
language O
model O
. O
It O
obtains O
state O
- O
of O
- O
theart O
results O
on O
a O
wide O
array O
of O
Natural O
Language O
Processing O
( O
NLP O
) O
tasks O
. O
Unlike O
other O
previous O
pretrained O
language O
models O
( O
Peters O
et O
al O
. O
, O
2018a O
; O
Radford O
et O
al O
. O
, O
2018 O
) O
, O
BERT O
employs O
the O
multi O
- O
layer O
bidirectional O
Transformer O
encoder O
as O
the O
model O
architecture O
and O
proposes O
two O
novel O
pre O
- O
training O
tasks O
: O
the O
masked O
language O
modeling O
and O
the O
next O
sentence O
prediction O
. O
There O
are O
two O
approaches O
to O
adapt O
the O
pretrained O
language O
representations O
to O
the O
downstream O
tasks O
. O
One O
is O
the O
feature O
- O
based O
approach O
, O
where O
the O
parameters O
of O
the O
original O
pre O
- O
trained O
‚àóContribution O
during O
internship O
at O
Microsoft O
Research.model O
are O
frozen O
when O
applied O
on O
the O
downstream O
tasks O
( O
Mikolov O
et O
al O
. O
, O
2013 O
; O
Pennington O
et O
al O
. O
, O
2014 O
; O
Peters O
et O
al O
. O
, O
2018a O
) O
. O
Another O
one O
is O
the O
Ô¨Åne O
- O
tuning O
approach O
, O
where O
the O
pre O
- O
trained O
model O
and O
the O
taskspeciÔ¨Åc O
model O
are O
trained O
together O
( O
Dai O
and O
Le O
, O
2015 O
; O
Howard O
and O
Ruder O
, O
2018 O
; O
Radford O
et O
al O
. O
, O
2018 O
) O
. O
Take O
the O
classiÔ¨Åcation O
task O
as O
an O
example O
, O
the O
new O
parameter O
added O
for O
BERT O
Ô¨Åne O
- O
tuning O
is O
a O
task O
- O
speciÔ¨Åc O
fully O
- O
connected O
layer O
, O
then O
all O
parameters O
of O
BERT O
and O
the O
classiÔ¨Åcation O
layer O
are O
trained O
together O
to O
minimize O
the O
loss O
function O
. O
Peters O
et O
al O
. O
( O
2019 O
) O
demonstrate O
that O
the O
Ô¨Ånetuning O
approach O
of O
BERT O
generally O
outperforms O
the O
feature O
- O
based O
approach O
. O
We O
know O
that O
BERT O
encodes O
task O
- O
speciÔ¨Åc O
representations O
during O
Ô¨Ånetuning O
, O
but O
it O
is O
unclear O
about O
the O
learning O
dynamics O
of O
BERT O
Ô¨Åne O
- O
tuning O
, O
i.e. O
, O
how O
Ô¨Åne O
- O
tuning O
helps O
BERT O
to O
improve O
performance O
on O
downstream O
tasks O
. O
We O
investigate O
the O
learning O
dynamics O
of O
BERT O
Ô¨Åne O
- O
tuning O
with O
two O
indicators O
. O
First O
, O
we O
use O
Jensen O
- O
Shannon O
divergence O
to O
measure O
the O
change O
of O
the O
attention O
mode O
during O
BERT O
Ô¨Åne O
- O
tuning O
. O
Second O
, O
we O
use O
Singular O
Vector O
Canonical O
Correlation O
Analysis O
( O
SVCCA O
; O
Raghu O
et O
al O
. O
( O
2017 O
) O
) O
distance O
to O
measure O
the O
change O
of O
the O
feature O
extraction O
mode O
. O
We O
conclude O
that O
during O
the O
Ô¨Åne O
- O
tuning O
procedure O
, O
BERT O
mainly O
changes O
the O
attention O
mode O
of O
the O
last O
layers O
, O
and O
modiÔ¨Åes O
the O
feature O
extraction O
mode O
of O
intermediate O
and O
last O
layers O
. O
At O
the O
same O
time O
, O
BERT O
has O
the O
ability O
to O
avoid O
catastrophic O
forgetting O
of O
knowledge O
in O
low O
layers O
. O
Moreover O
, O
we O
also O
analyze O
the O
consistency O
of O
the O
Ô¨Åne O
- O
tuning O
procedure O
. O
Across O
different O
random O
seeds O
and O
different O
datasets O
, O
we O
observe O
that O
the O
changes O
of O
low O
layers O
( O
0 O
- O
9th O
layer O
) O
are O
generally O
consistent O
, O
which O
indicates O
that O
BERT O
has O
learned O
some O
common O
transferable O
language O
knowledge O
in O
low O
layers O
during O
the O
pre O
- O
training O
process O
, O
while O
the O
task O
- O
speciÔ¨Åc87information O
is O
mostly O
encoded O
in O
intermediate O
and O
last O
layers O
. O
2 O
Experimental O
Setup O
We O
employ O
the O
BERT O
- O
large O
model1on O
a O
diverse O
set O
of O
NLP O
tasks O
: O
natural O
language O
inference O
( O
NLI O
) O
, O
sentiment O
analysis O
( O
SA O
) O
and O
paraphrase O
detection O
( O
PD O
) O
. O
For O
NLI O
, O
we O
use O
both O
the O
Multi O
- O
Genre O
Natural O
Language O
Inference O
dataset O
( O
MNLI O
; O
Williams O
et O
al O
. O
2018 O
) O
and O
the O
Recognizing O
Textual O
Entailment O
dataset O
( O
RTE O
; O
aggregated O
from O
Dagan O
et O
al O
. O
2006 O
, O
Haim O
et O
al O
. O
2006 O
, O
Giampiccolo O
et O
al O
. O
2007 O
, O
Bentivogli O
et O
al O
. O
2009 O
) O
. O
For O
SA O
, O
we O
use O
the O
binary O
version O
of O
the O
Stanford O
Sentiment O
Treebank O
dataset O
( O
SST-2 O
; O
Socher O
et O
al O
. O
2013 O
) O
. O
For O
PD O
, O
we O
use O
the O
Microsoft O
Research O
Paraphrase O
Corpus O
dataset O
( O
MRPC O
; O
Dolan O
and O
Brockett O
2005 O
) O
. O
Dataset O
LR O
BS O
NE O
MNLI O
3e-5 O
64 O
3 O
RTE O
1e-5 O
32 O
5 O
SST-2 O
3e-5 O
64 O
4 O
MRPC O
1e-5 O
16 O
5 O
Table O
1 O
: O
Hyperparameter O
conÔ¨Åguration O
for O
BERT O
Ô¨Ånetuning O
. O
LR O
: O
learning O
rate O
, O
BS O
: O
batch O
size O
, O
NE O
: O
number O
of O
epochs O
. O
The O
hyperparameter O
choice O
for O
Ô¨Åne O
- O
tuning O
is O
task O
- O
speciÔ¨Åc O
. O
We O
choose O
relatively O
optimal O
parameters O
for O
every O
dataset O
as O
suggested O
in O
Devlin O
et O
al O
. O
( O
2019 O
) O
. O
The O
detailed O
hyperparameter O
conÔ¨Åguration O
is O
shown O
in O
Table O
1 O
. O
Moreover O
, O
we O
use O
Adam O
optimizer O
with O
the O
slanted O
triangular O
learning O
rate O
schedule O
( O
Howard O
and O
Ruder O
, O
2018 O
) O
and O
keep O
the O
dropout O
probability O
at O
0.1 O
. O
3 O
Fine O
- O
tuning O
changes O
the O
attention O
mode O
of O
the O
last O
layers O
The O
model O
architecture O
of O
BERT O
is O
essentially O
based O
on O
the O
multi O
- O
layer O
bidirectional O
Transformer O
, O
the O
core O
function O
of O
which O
is O
the O
self O
- O
attention O
mechanism O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O
We O
use O
JensenShannon O
divergence O
between O
two O
attention O
scores O
to O
detect O
changes O
of O
the O
attention O
mode O
in O
different O
layers O
during O
Ô¨Åne O
- O
tuning O
. O
Jensen O
- O
Shannon O
divergence O
JS O
divergence O
is O
a O
method O
of O
measuring O
the O
distance O
between O
two O
1github.com/google-research/bertprobability O
distributions O
, O
it O
is O
deÔ¨Åned O
as O
: O
DJS(P||Q O
) O
= O
1 O
2DKL(P||R O
) O
+1 O
2DKL(Q||R O
) O
wherePandQare O
two O
different O
probability O
distributions O
, O
R O
= O
P+Q O
2is O
the O
average O
probability O
distribution O
of O
them O
and O
DKLrepresents O
the O
Kullback O
- O
Leibler O
divergence O
. O
For O
every O
layer O
of O
BERT O
, O
there O
are O
16 O
attention O
heads O
, O
each O
head O
produces O
an O
attention O
score O
of O
the O
input O
sequence O
. O
Each O
attention O
score O
is O
a O
probability O
distribution O
about O
how O
much O
attention O
a O
target O
word O
pays O
to O
other O
words O
. O
We O
compute O
JS O
divergence O
of O
attention O
scores O
between O
the O
original O
BERT O
model O
M0and O
the O
Ô¨Åne O
- O
tuned O
model O
Mton O
the O
development O
set O
, O
by O
calculating O
the O
average O
of O
the O
sum O
of O
JS O
divergence O
at O
each O
word O
and O
each O
attention O
head O
for O
every O
layer O
, O
the O
speciÔ¨Åc O
calculation O
formula O
is O
as O
follows O
: O
DJS(Mt||M0 O
) O
= O
1 O
N1 O
HN O
/ O
summationdisplay O
n=1H O
/ O
summationdisplay O
h=11 O
WW O
/ O
summationdisplay O
i=1 O
DJS(Ah O
t(word O
i)||Ah O
0(word O
i O
) O
) O
whereNdenotes O
the O
number O
of O
development O
examples O
, O
Hdenotes O
the O
number O
of O
attention O
heads O
, O
Wdenotes O
the O
number O
of O
tokens O
in O
a O
sequence O
andAh O
t(word O
i)denotes O
the O
attention O
score O
of O
the O
attention O
head O
hatword O
iin O
modelMt O
. O
1357911131517192123 O
Layer O
Index0.00.10.20.3JS O
Divergence O
SST-2 O
MRPC O
MNLI O
Figure O
1 O
: O
JS O
divergence O
of O
attention O
scores O
of O
every O
layer O
between O
the O
original O
BERT O
model O
and O
the O
Ô¨Ånetuned O
model O
. O
We O
present O
JS O
divergence O
results O
in O
Figure O
1 O
, O
from O
which O
we O
observe O
the O
attention O
mode O
in O
low O
layers O
and O
intermediate O
layers O
do O
not O
change O
seriously O
, O
while O
the O
attention O
mode O
of O
last O
layers O
changes O
drastically O
. O
It O
indicates O
that O
the O
Ô¨Åne O
- O
tuning O
procedure O
has O
the O
ability O
to O
keep O
the O
attention O
mode O
of O
low O
layers O
consistent O
with O
the O
original O
BERT O
model O
, O
and O
changes O
the O
attention O
mode O
of O
the O
last O
layers O
to O
adapt O
BERT O
on O
speciÔ¨Åc O
tasks.884 O
Fine O
- O
tuning O
modiÔ¨Åes O
the O
feature O
extraction O
mode O
of O
the O
intermediate O
and O
the O
last O
layers O
While O
the O
attention O
score O
implies O
the O
inherent O
dependencies O
between O
different O
words O
, O
the O
output O
representation O
of O
every O
layer O
is O
the O
practical O
feature O
that O
the O
model O
extracts O
. O
We O
use O
SVCCA O
distance O
( O
Raghu O
et O
al O
. O
, O
2017 O
) O
to O
quantify O
the O
change O
of O
these O
output O
representations O
during O
Ô¨Åne O
- O
tuning O
, O
which O
indicates O
the O
change O
of O
the O
feature O
extraction O
mode O
of O
BERT O
. O
Singular O
Vector O
Canonical O
Correlation O
Analysis O
. O
SVCCA O
distance O
is O
used O
as O
a O
metric O
to O
measure O
the O
differences O
of O
hidden O
representations O
between O
the O
original O
BERT O
model O
M0and O
the O
Ô¨Ånetuned O
model O
Mtat O
a O
target O
layer O
. O
It O
is O
calculated O
by O
: O
DSV O
CCA O
( O
Mt||M0 O
) O
= O
1‚àí1 O
cc O
/ O
summationdisplay O
i=1œÅ(i O
) O
wherecdenotes O
the O
hidden O
size O
of O
BERT O
, O
œÅis O
the O
Canonical O
Correlation O
Analysis O
( O
CCA O
) O
resulting O
in O
a O
value O
between O
0 O
and O
1 O
, O
which O
indicates O
how O
well O
correlated O
the O
two O
representations O
derived O
by O
two O
models O
are O
. O
For O
a O
detailed O
explanation O
of O
SVCCA O
, O
please O
see O
Raghu O
et O
al O
. O
( O
2017 O
) O
. O
1357911131517192123 O
Layer O
Index0.00.20.4SVCCA O
Distance O
SST-2 O
MRPC O
MNLI O
Figure O
2 O
: O
SVCCA O
distance O
of O
individual O
layers O
between O
the O
original O
BERT O
model O
and O
the O
Ô¨Åne O
- O
tuned O
model O
. O
From O
Figure O
2 O
, O
we O
observe O
that O
changes O
in O
SVCCA O
distance O
in O
higher O
layers O
are O
more O
distinct O
than O
lower O
layers O
. O
This O
phenomenon O
is O
reasonable O
because O
the O
output O
representation O
of O
higher O
layers O
undergoes O
more O
transformations O
, O
so O
the O
change O
of O
SVCCA O
distance O
in O
higher O
layers O
is O
more O
dramatic O
. O
As O
the O
output O
representation O
of O
the O
last O
layer O
is O
directly O
used O
for O
classiÔ¨Åcation O
, O
we O
aim O
to O
compare O
the O
effect O
of O
each O
layer O
on O
the O
Ô¨Ånal O
output O
representation O
respectively O
. O
We O
replace O
the O
parameters O
of O
/uni00000013 O
/ O
uni00000015 O
/ O
uni00000017 O
/ O
uni00000019 O
/ O
uni0000001b O
/ O
uni00000014 O
/ O
uni00000013 O
/ O
uni00000014 O
/ O
uni00000015 O
/ O
uni00000014 O
/ O
uni00000017 O
/ O
uni00000014 O
/ O
uni00000019 O
/ O
uni00000014 O
/ O
uni0000001b O
/ O
uni00000015 O
/ O
uni00000013 O
/ O
uni00000015 O
/ O
uni00000015 O
/uni0000002f O
/ O
uni00000044 O
/ O
uni0000005c O
/ O
uni00000048 O
/ O
uni00000055 O
/ O
uni00000003 O
/ O
uni0000002c O
/ O
uni00000051 O
/ O
uni00000047 O
/ O
uni00000048 O
/ O
uni0000005b O
/ O
uni00000036 O
/ O
uni00000036 O
/ O
uni00000037 O
/ O
uni00000010 O
/ O
uni00000015 O
/uni00000030 O
/ O
uni00000035 O
/ O
uni00000033 O
/ O
uni00000026 O
/uni00000030 O
/ O
uni00000031 O
/ O
uni0000002f O
/ O
uni0000002c O
/ O
uni00000013 O
/ O
uni00000011 O
/ O
uni00000013 O
/ O
uni00000015 O
/uni00000013 O
/ O
uni00000011 O
/ O
uni00000013 O
/ O
uni00000017 O
/uni00000013 O
/ O
uni00000011 O
/ O
uni00000013 O
/ O
uni00000019 O
/uni00000013 O
/ O
uni00000011 O
/ O
uni00000013 O
/ O
uni0000001b O
/uni00000013 O
/ O
uni00000011 O
/ O
uni00000014 O
/ O
uni00000013 O
Figure O
3 O
: O
SVCCA O
distance O
of O
the O
last O
layer O
between O
the O
original O
Ô¨Åne O
- O
tuned O
model O
and O
the O
Ô¨Åne O
- O
tuned O
model O
with O
parameters O
of O
a O
target O
layer O
replaced O
with O
their O
pretrained O
values O
. O
every O
layer O
in O
the O
Ô¨Åne O
- O
tuned O
model O
with O
their O
original O
values O
in O
the O
BERT O
model O
before O
Ô¨Åne O
- O
tuning O
and O
compute O
the O
SVCCA O
distance O
of O
the O
last O
layer O
output O
representation O
. O
The O
results O
are O
shown O
in O
Figure O
3 O
, O
we O
observe O
that O
whether O
the O
low O
layers O
( O
0 O
- O
10 O
) O
are O
replaced O
with O
their O
original O
values O
or O
not O
, O
it O
has O
little O
effect O
on O
the O
Ô¨Ånal O
output O
representation O
. O
Moreover O
, O
the O
change O
in O
the O
intermediate O
and O
last O
layers O
will O
increase O
the O
SVCCA O
distance O
, O
which O
reÔ¨Çects O
that O
Ô¨Åne O
- O
tuning O
mainly O
changes O
the O
feature O
extraction O
mode O
of O
intermediate O
and O
last O
layers O
. O
5 O
Consistency O
of O
Fine O
- O
tuning O
In O
this O
section O
, O
we O
investigate O
the O
consistency O
of O
different O
Ô¨Åne O
- O
tuning O
procedures O
, O
including O
the O
consistency O
between O
different O
random O
seeds O
and O
the O
consistency O
between O
different O
datasets O
. O
5.1 O
Consistency O
between O
different O
random O
seeds O
We O
Ô¨Åne O
- O
tune O
two O
models O
on O
every O
dataset O
with O
the O
same O
hyperparameters O
but O
different O
random O
seeds O
. O
We O
compute O
the O
pairwise O
JS O
divergence O
and O
SVCCA O
distance O
of O
each O
layer O
between O
the O
two O
models O
with O
different O
random O
seeds O
. O
As O
shown O
in O
Figure O
4 O
, O
for O
large O
dataset O
MNLI O
and O
SST-2 O
, O
the O
attention O
mode O
of O
low O
and O
intermediate O
layers O
is O
basically O
consistent O
between O
two O
different O
random O
seeds O
, O
whereas O
the O
attention O
mode O
of O
last O
layers O
is O
relatively O
divergent O
. O
For O
MRPC O
, O
the O
attention O
mode O
appears O
to O
be O
divergent O
at O
the O
9th O
layer O
. O
Figure O
5 O
illustrates O
SVCCA O
distance O
between O
different O
random O
seeds O
, O
we O
observe O
that O
the O
SVCCA O
distance O
gradually O
increases O
in O
all O
layers O
. O
For O
MNLI O
and O
SST-2 O
, O
the O
increase O
of O
last O
layers O
is891357911131517192123 O
Layer O
Index0.00.10.20.3JS O
Divergence O
SST-2 O
MRPC O
MNLIFigure O
4 O
: O
JS O
divergence O
between O
two O
models O
with O
different O
random O
seeds O
. O
1357911131517192123 O
Layer O
Index0.00.20.4SVCCA O
Distance O
SST-2 O
MRPC O
MNLIFigure O
5 O
: O
SVCCA O
distance O
between O
two O
models O
with O
different O
random O
seeds O
. O
1357911131517192123 O
Layer O
Index0.00.10.20.30.40.5JS O
Divergence O
MNLI&RTE O
MRPC&RTE O
MRPC&SST-2 O
Figure O
6 O
: O
JS O
divergence O
between O
different O
datasets O
. O
1357911131517192123 O
Layer O
Index0.00.20.4SVCCA O
Distance O
MNLI&RTE O
MRPC&RTE O
MRPC&SST-2Figure O
7 O
: O
SVCCA O
distance O
between O
different O
datasets O
. O
more O
obvious O
, O
and O
for O
MRPC O
, O
the O
increase O
appears O
to O
be O
obvious O
from O
the O
13th O
layer O
. O
5.2 O
Consistency O
between O
different O
datasets O
Besides O
the O
consistency O
between O
different O
random O
seeds O
, O
we O
also O
aim O
to O
investigate O
the O
consistency O
between O
different O
datasets O
. O
We O
Ô¨Åne O
- O
tune O
two O
models O
on O
two O
different O
datasets O
then O
evaluate O
on O
a O
combined O
dataset O
containing O
200 O
examples O
respectively O
from O
both O
two O
datasets O
. O
For O
different O
datasets O
of O
the O
same O
domain O
, O
we O
use O
two O
models O
Ô¨Åne O
- O
tuned O
on O
RTE O
and O
MNLI O
dataset O
. O
For O
different O
domains O
, O
we O
examine O
the O
consistency O
between O
MRPC O
and O
RTE O
, O
which O
both O
have O
pairwise O
input O
sequences O
, O
and O
the O
consistency O
between O
MRPC O
and O
SST-2 O
, O
which O
have O
different O
patterns O
of O
input O
sequences O
. O
The O
JS O
divergence O
results O
and O
SVCCA O
distance O
results O
between O
different O
datasets O
are O
shown O
in O
Figure O
6 O
and O
Figure O
7 O
. O
Figure O
6 O
and O
Figure O
7 O
demonstrate O
that O
no O
matter O
two O
datasets O
are O
from O
the O
same O
domain O
or O
the O
different O
domain O
, O
the O
attention O
mode O
and O
the O
feature O
extraction O
mode O
of O
low O
layers O
( O
0 O
- O
7 O
layer O
) O
are O
consistent O
, O
which O
indicates O
BERT O
studies O
some O
common O
language O
knowledge O
during O
the O
pre O
- O
training O
procedure O
and O
low O
layers O
are O
stable O
to O
change O
their O
original O
modes O
. O
JS O
divergence O
of O
the O
attention O
scoresand O
SVCCA O
distance O
of O
the O
output O
representations O
in O
intermediate O
and O
last O
layers O
between O
two O
models O
are O
more O
distinct O
when O
the O
difference O
between O
two O
training O
datasets O
increases O
. O
The O
consistency O
between O
datasets O
from O
similar O
tasks O
like O
RTE O
and O
MNLI O
is O
still O
relatively O
strong O
in O
last O
layers O
compared O
to O
the O
consistency O
between O
datasets O
from O
the O
different O
domain O
. O
And O
when O
the O
input O
sequence O
pattern O
and O
the O
domain O
of O
two O
datasets O
are O
different O
, O
the O
consistency O
of O
intermediate O
and O
last O
layers O
is O
weak O
as O
expected O
. O
6 O
Related O
Work O
Pre O
- O
trained O
language O
models O
( O
Radford O
et O
al O
. O
, O
2018 O
; O
Devlin O
et O
al O
. O
, O
2019 O
; O
Liu O
et O
al O
. O
, O
2019 O
; O
Dong O
et O
al O
. O
, O
2019 O
; O
Yang O
et O
al O
. O
, O
2019 O
; O
Clark O
et O
al O
. O
, O
2020 O
; O
Bao O
et O
al O
. O
, O
2020 O
) O
stimulate O
the O
research O
interest O
on O
the O
interpretation O
of O
these O
black O
- O
box O
models O
. O
Peters O
et O
al O
. O
( O
2018b O
) O
show O
that O
the O
biLM O
- O
based O
models O
learn O
representations O
that O
vary O
with O
network O
depth O
, O
the O
lower O
layers O
specialize O
in O
local O
syntactic O
relationships O
and O
the O
higher O
layers O
model O
longer O
range O
relationships O
. O
Kovaleva O
et O
al O
. O
( O
2019 O
) O
propose O
a O
methodology O
and O
offer O
the O
analysis O
of O
BERTs O
capacity O
to O
capture O
different O
kinds O
of O
linguistic O
information O
by O
encoding O
it O
in O
its O
self O
- O
attention O
weights O
. O
Hao O
et O
al O
. O
( O
2019 O
) O
visualize O
the O
loss O
landscapes O
and90optimization O
trajectories O
of O
the O
BERT O
Ô¨Åne O
- O
tuning O
procedure O
and O
Ô¨Ånd O
that O
low O
layers O
of O
the O
BERT O
model O
are O
more O
invariant O
and O
transferable O
across O
tasks O
. O
Merchant O
et O
al O
. O
( O
2020 O
) O
Ô¨Ånd O
that O
Ô¨Åne O
- O
tuning O
primarily O
affects O
the O
top O
layers O
of O
BERT O
, O
but O
with O
noteworthy O
variation O
across O
tasks O
. O
Hao O
et O
al O
. O
( O
2020 O
) O
propose O
a O
self O
- O
attention O
attribution O
method O
to O
interpret O
information O
Ô¨Çow O
within O
Transformer O
. O
7 O
Discussions O
We O
use O
JS O
divergence O
to O
detect O
the O
change O
of O
the O
attention O
mode O
in O
different O
layers O
during O
BERT O
Ô¨Åne O
- O
tuning O
and O
use O
SVCCA O
distance O
to O
detect O
the O
change O
of O
the O
feature O
extraction O
mode O
. O
We O
observe O
that O
BERT O
Ô¨Åne O
- O
tuning O
mainly O
changes O
the O
attention O
mode O
of O
last O
layers O
and O
modiÔ¨Åes O
the O
feature O
extraction O
mode O
of O
intermediate O
and O
last O
layers O
. O
We O
also O
demonstrate O
that O
the O
changes O
of O
low O
layers O
are O
consistent O
between O
different O
random O
seeds O
and O
different O
datasets O
, O
which O
indicates O
that O
BERT O
learns O
common O
transferable O
language O
knowledge O
in O
low O
layers O
. O
In O
future O
research O
, O
we O
would O
like O
to O
explore O
learning O
dynamics O
for O
cross O
- O
lingual O
pretrained O
models O
( O
Conneau O
and O
Lample O
, O
2019 O
; O
Conneau O
et O
al O
. O
, O
2020 O
; O
Chi O
et O
al O
. O
, O
2020 O
) O
. O
Acknowledgements O
The O
work O
was O
partially O
supported O
by O
National O
Natural O
Science O
Foundation O
of O
China O
( O
NSFC O
) O
[ O
Grant O
No O
. O
61421003 O
] O
. O
Abstract O
In O
this O
paper O
, O
we O
propose O
second O
- O
order O
graphbased O
neural O
dependency O
parsing O
using O
message O
passing O
and O
end O
- O
to O
- O
end O
neural O
networks O
. O
We O
empirically O
show O
that O
our O
approaches O
match O
the O
accuracy O
of O
very O
recent O
state O
- O
ofthe O
- O
art O
second O
- O
order O
graph O
- O
based O
neural O
dependency O
parsers O
and O
have O
signiÔ¨Åcantly O
faster O
speed O
in O
both O
training O
and O
testing O
. O
We O
also O
empirically O
show O
the O
advantage O
of O
second O
- O
order O
parsing O
over O
Ô¨Årst O
- O
order O
parsing O
and O
observe O
that O
the O
usefulness O
of O
the O
head O
- O
selection O
structured O
constraint O
vanishes O
when O
using O
BERT O
embedding O
. O
1 O
Introduction O
Graph O
- O
based O
dependency O
parsing O
is O
a O
popular O
approach O
to O
dependency O
parsing O
that O
scores O
parse O
components O
of O
a O
sentence O
and O
then O
Ô¨Ånds O
the O
highest O
scoring O
tree O
through O
inference O
. O
First O
- O
order O
graphbased O
dependency O
parsing O
takes O
individual O
dependency O
edges O
as O
the O
components O
of O
a O
parse O
tree O
, O
while O
higher O
- O
order O
dependency O
parsing O
considers O
more O
complex O
components O
consisting O
of O
multiple O
edges O
. O
There O
exist O
both O
exact O
inference O
algorithms O
( O
Carreras O
, O
2007 O
; O
Koo O
and O
Collins O
, O
2010 O
; O
Ma O
and O
Zhao O
, O
2012 O
) O
and O
approximate O
inference O
algorithms O
( O
McDonald O
and O
Pereira O
, O
2006 O
; O
Smith O
and O
Eisner O
, O
2008 O
; O
Gormley O
et O
al O
. O
, O
2015 O
) O
to O
Ô¨Ånd O
the O
best O
parse O
tree O
. O
Recent O
work O
focused O
on O
neural O
network O
based O
graph O
dependency O
parsers O
( O
Kiperwasser O
and O
Goldberg O
, O
2016 O
; O
Wang O
and O
Chang O
, O
2016 O
; O
Cheng O
et O
al O
. O
, O
2016 O
; O
Kuncoro O
et O
al O
. O
, O
2016 O
; O
Ma O
and O
Hovy O
, O
2017 O
; O
Dozat O
and O
Manning O
, O
2017 O
) O
. O
Dozat O
and O
Manning O
( O
2017 O
) O
proposed O
a O
Ô¨Årst O
- O
order O
graph O
- O
based O
neural O
dependency O
parsing O
approach O
with O
a O
simple O
headselection O
training O
objective O
. O
It O
uses O
a O
biafÔ¨Åne O
function O
to O
score O
dependency O
edges O
and O
has O
high O
efÔ¨Åciency O
and O
good O
performance O
. O
Subsequent O
work O
‚àóKewei O
Tu O
is O
the O
corresponding O
author.introduced O
second O
- O
order O
inference O
into O
their O
parser O
. O
Ji O
et O
al O
. O
( O
2019 O
) O
proposed O
a O
graph O
neural O
network O
that O
captures O
second O
- O
order O
information O
in O
token O
representations O
, O
which O
are O
then O
used O
for O
Ô¨Årst O
- O
order O
parsing O
. O
Very O
recently O
, O
Zhang O
et O
al O
. O
( O
2020 O
) O
proposed O
an O
efÔ¨Åcient O
second O
- O
order O
tree O
CRF O
model O
for O
dependency O
parsing O
and O
achieved O
state O
- O
of O
- O
the O
- O
art O
performance O
. O
In O
this O
paper O
, O
we O
Ô¨Årst O
show O
how O
a O
previously O
proposed O
second O
- O
order O
semantic O
dependency O
parser O
( O
Wang O
et O
al O
. O
, O
2019 O
) O
can O
be O
applied O
to O
syntactic O
dependency O
parsing O
with O
simple O
modiÔ¨Åcations O
. O
The O
parser O
is O
an O
end O
- O
to O
- O
end O
neural O
network O
derived O
from O
message O
passing O
inference O
on O
a O
conditional O
random O
Ô¨Åeld O
that O
encodes O
the O
second O
- O
order O
parsing O
problem O
. O
We O
then O
propose O
an O
alternative O
conditional O
random O
Ô¨Åeld O
that O
incorporates O
the O
head O
- O
selection O
constraint O
of O
syntactic O
dependency O
parsing O
, O
and O
derive O
a O
novel O
second O
- O
order O
dependency O
parser O
. O
We O
empirically O
compare O
the O
two O
second O
- O
order O
approaches O
and O
the O
Ô¨Årst O
- O
order O
baselines O
on O
English O
Penn O
Tree O
Bank O
3.0 O
( O
PTB O
) O
, O
Chinese O
Penn O
Tree O
Bank O
5.1 O
( O
CTB O
) O
and O
datasets O
of O
12 O
languages O
in O
Universal O
Dependencies O
( O
UD O
) O
. O
We O
show O
that O
our O
approaches O
achieve O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
both O
PTB O
and O
CTB O
and O
our O
approaches O
are O
signiÔ¨Åcantly O
faster O
than O
recently O
proposed O
second O
- O
order O
parsers O
. O
We O
also O
make O
two O
interesting O
observations O
from O
our O
empirical O
study O
. O
First O
, O
it O
is O
a O
common O
belief O
that O
contextual O
word O
embeddings O
such O
as O
ELMo O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
and O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
already O
conveys O
sufÔ¨Åcient O
high O
- O
order O
information O
that O
renders O
high O
- O
order O
parsing O
less O
useful O
, O
but O
we O
Ô¨Ånd O
that O
second O
- O
order O
decoding O
is O
still O
helpful O
even O
with O
strong O
contextual O
embeddings O
like O
BERT O
. O
Second O
, O
while O
Zhang O
et O
al O
. O
( O
2019 O
) O
previously O
found O
that O
incoperating O
the O
head O
- O
selection O
constraint O
is O
helpful O
in O
Ô¨Årst O
- O
order O
parsing O
, O
we O
Ô¨Ånd O
that O
with O
a O
better O
loss O
function O
design O
and O
hyper O
- O
parameter O
tun-93ing O
both O
Ô¨Årst- O
and O
second O
- O
order O
parsers O
without O
the O
head O
- O
selection O
constraint O
can O
match O
the O
accuracy O
of O
parsers O
with O
the O
head O
- O
selection O
constraint O
and O
can O
even O
outperform O
the O
latter O
when O
using O
BERT O
embedding O
. O
Our O
approaches O
are O
closely O
related O
to O
the O
work O
of O
Gormley O
et O
al O
. O
( O
2015 O
) O
, O
which O
proposed O
a O
nonneural O
second O
- O
order O
parser O
based O
on O
Loopy O
Belief O
Propagation O
( O
LBP O
) O
. O
Our O
work O
differs O
from O
theirs O
in O
that O
: O
1 O
) O
we O
use O
Mean O
Field O
Variational O
Inference O
( O
MFVI O
) O
instead O
of O
LBP O
, O
which O
Wang O
et O
al O
. O
( O
2019 O
) O
found O
is O
faster O
and O
equally O
accurate O
in O
practice O
; O
2 O
) O
we O
add O
the O
head O
- O
selection O
constraint O
and O
do O
not O
include O
the O
global O
tree O
constraint O
that O
is O
shown O
to O
produce O
only O
slight O
improvement O
( O
Zhang O
et O
al O
. O
, O
2019 O
) O
but O
would O
complicate O
our O
neural O
network O
design O
and O
implementation O
; O
3 O
) O
we O
employ O
modern O
neural O
encoders O
and O
achieve O
much O
better O
parsing O
accuracy O
. O
Our O
approaches O
are O
also O
closely O
related O
to O
the O
very O
recent O
work O
of O
Fonseca O
and O
Martins O
( O
2020 O
) O
. O
The O
main O
difference O
is O
that O
we O
use O
MFVI O
while O
they O
use O
the O
dual O
decomposition O
algorithm O
AD3(Martins O
et O
al O
. O
, O
2011 O
, O
2013 O
) O
for O
approximate O
inference O
. O
2 O
Approach O
Zhang O
et O
al O
. O
( O
2019 O
) O
categorized O
different O
kinds O
of O
graph O
- O
based O
dependency O
parsers O
based O
on O
their O
structured O
output O
constraints O
according O
to O
the O
normalization O
for O
output O
scores O
. O
A O
Local O
approach O
views O
dependency O
parsing O
as O
a O
head O
- O
selection O
problem O
, O
in O
which O
each O
word O
selects O
exactly O
one O
dependency O
head O
. O
A O
Single O
approach O
places O
no O
structured O
constraint O
, O
viewing O
the O
existence O
of O
each O
possible O
dependency O
edge O
as O
an O
independent O
binary O
classiÔ¨Åcation O
problem O
. O
The O
second O
- O
order O
semantic O
dependency O
parser O
of O
Wang O
et O
al O
. O
( O
2019 O
) O
is O
an O
end O
- O
to O
- O
end O
neural O
network O
derived O
from O
message O
passing O
inference O
on O
a O
conditional O
random O
Ô¨Åeld O
that O
encodes O
the O
second O
- O
order O
parsing O
problem O
. O
It O
is O
clearly O
a O
Single O
approach O
because O
of O
the O
lack O
of O
structured O
constraints O
in O
semantic O
dependency O
parsing O
. O
We O
can O
apply O
this O
approach O
to O
syntactic O
dependency O
parsing O
with O
two O
minor O
modiÔ¨Åcations O
. O
First O
, O
co O
- O
parents O
, O
one O
of O
the O
three O
types O
of O
second O
- O
order O
parts O
, O
become O
invalid O
and O
hence O
are O
removed O
. O
Second O
, O
for O
the O
approach O
to O
output O
valid O
parse O
trees O
during O
testing O
, O
we O
run O
maximum O
spanning O
tree O
( O
MST O
) O
( O
McDonald O
et O
al O
. O
, O
2005 O
) O
based O
on O
the O
posterior O
edge O
probabilities O
predicted O
by O
the O
approach O
. O
Inspired O
by O
Wang O
et O
al O
. O
( O
2019 O
) O
, O
below O
we O
propose O
a O
Local O
second O
- O
order O
parsing O
approach O
. O
While O
the O
Single O
approach O
uses O
Boolean O
random O
variables O
to O
represent O
existence O
of O
possible O
dependency O
edges O
, O
our O
Local O
approach O
deÔ¨Ånes O
a O
discrete O
random O
variable O
for O
each O
word O
specifying O
its O
dependency O
head O
, O
thus O
enforcing O
the O
head O
- O
selection O
constraint O
and O
leading O
to O
different O
formulation O
of O
the O
message O
passing O
inference O
steps O
. O
2.1 O
Scoring O
Following O
Dozat O
and O
Manning O
( O
2017 O
) O
, O
we O
predict O
edge O
existence O
and O
edge O
labels O
separately O
. O
Suppose O
the O
input O
sentence O
is O
w= O
[ O
w0,w1,w2, O
... O
,w O
n O
] O
wherew0is O
a O
dummy O
root O
. O
We O
feed O
word O
representations O
outputted O
by O
the O
BiLSTM O
encoder O
into O
a O
biafÔ¨Åne O
function O
to O
assign O
score O
s(edge O
) O
ij O
to O
edge O
wi‚Üíwj O
. O
We O
use O
a O
Trilinear O
function O
to O
assign O
scores(sib O
) O
ij O
, O
ikto O
the O
siblings O
part O
consisting O
of O
edges O
wi‚Üíwjandwi‚Üíwk O
, O
and O
another O
Trilinear O
function O
to O
assign O
score O
s(gp O
) O
ij O
, O
jkto O
the O
grandparent O
part O
consisting O
of O
edges O
wi‚Üíwjandwj‚Üíwk O
. O
For O
edge O
labels O
, O
we O
use O
a O
biafÔ¨Åne O
function O
to O
predict O
label O
scores O
of O
each O
potential O
edge O
and O
use O
a O
softmax O
function O
to O
compute O
the O
label O
distribution O
P(y(label O
) O
ij|w O
) O
, O
wherey(label O
) O
ijrepresents O
the O
possible O
label O
for O
edge O
wi‚Üíwj O
. O
2.2 O
Message O
Passing O
The O
head O
- O
selection O
structured O
constraint O
requires O
that O
each O
word O
except O
the O
root O
has O
exactly O
one O
head O
. O
We O
deÔ¨Åne O
variable O
Xj‚àà{0,1,2, O
... O
,n}to O
indicate O
the O
head O
of O
word O
wj O
. O
We O
then O
deÔ¨Åne O
a O
conditional O
random O
Ô¨Åeld O
( O
CRF O
) O
over O
[ O
X1, O
... O
,X O
n O
] O
. O
For O
each O
variable O
Xj O
, O
the O
unary O
potential O
is O
deÔ¨Åned O
by O
: O
œÜu(Xj O
= O
i O
) O
= O
exp(s(edge O
) O
ij O
) O
Given O
two O
variables O
XjandXl O
, O
the O
binary O
potential O
is O
deÔ¨Åned O
by O
: O
œÜp(Xj O
= O
i O
, O
Xl O
= O
k O
) O
= O
Ô£± O
Ô£¥Ô£≤ O
Ô£¥Ô£≥exp(s(sib O
) O
ij O
, O
kl)k O
= O
i O
exp(s(gp O
) O
ij O
, O
kl)k O
= O
j O
1 O
Otherwise O
We O
use O
MFVI O
for O
approximate O
inference O
on O
this O
CRF O
. O
The O
algorithm O
updates O
the O
factorized O
poste-94rior O
distribution O
Qj(Xj)of O
each O
word O
iteratively O
. O
M(t‚àí1 O
) O
j(i O
) O
= O
/summationdisplay O
k O
/ O
negationslash O
= O
i O
, O
jQ(t‚àí1 O
) O
k(i)s(sib O
) O
ij O
, O
ik O
+ O
Q(t‚àí1 O
) O
k(j)s(gp O
) O
ij O
, O
jk+Q(t‚àí1 O
) O
i(k)s(gp O
) O
ki O
, O
ij O
Q(t O
) O
j(i O
) O
= O
exp{s(edge O
) O
ij O
+ O
M(t‚àí1 O
) O
j(i O
) O
} O
n O
/ O
summationtext O
k=0exp{s(edge O
) O
kj+M(t‚àí1 O
) O
j(k O
) O
} O
Att= O
0,Q(t O
) O
j(Xj)is O
initialized O
by O
normalizing O
the O
unary O
potential O
. O
The O
iterative O
update O
steps O
can O
be O
unfolded O
as O
recurrent O
neural O
network O
layers O
parameterized O
by O
part O
scores O
, O
thus O
forming O
an O
end O
- O
to O
- O
end O
neural O
network O
. O
Compared O
with O
the O
update O
formula O
in O
the O
Single O
approach O
, O
here O
the O
posterior O
distributions O
are O
deÔ¨Åned O
over O
head O
- O
selections O
and O
are O
normalized O
over O
all O
possible O
heads O
. O
The O
computational O
complexity O
remains O
the O
same O
. O
2.3 O
Learning O
We O
deÔ¨Åne O
the O
cross O
entropy O
losses O
by O
: O
L(edge)=‚àí/summationdisplay O
ilog[Qi(y‚àó(edge O
) O
i|w O
) O
] O
L(label)=‚àí/summationdisplay O
i O
, O
j1(y‚àó(edge O
) O
j O
= O
i O
) O
log(P(y‚àó(label O
) O
ij|w O
) O
) O
L O
= O
ŒªL(label)+ O
( O
1‚àíŒª)L(edge O
) O
wherey‚àó(edge O
) O
i O
is O
the O
head O
of O
word O
wiandy‚àó(label O
) O
ij O
is O
the O
label O
of O
edge O
wi‚Üíwjin O
the O
golden O
parse O
tree O
, O
Œªis O
a O
hyper O
- O
parameter O
and O
1(x)is O
an O
indicator O
function O
that O
returns O
1whenxis O
true O
and O
0 O
otherwise O
. O
3 O
Experiments O
3.1 O
Setups O
Following O
previous O
work O
( O
Dozat O
and O
Manning O
, O
2017 O
; O
Ma O
et O
al O
. O
, O
2018 O
) O
, O
we O
use O
PTB O
3.0 O
( O
Marcus O
et O
al O
. O
, O
1993 O
) O
, O
CTB O
5.1 O
( O
Xue O
et O
al O
. O
, O
2002 O
) O
and O
12 O
languages O
in O
Universal O
Dependencies O
( O
Nivre O
et O
al O
. O
, O
2018 O
) O
( O
UD O
) O
2.2 O
to O
evaluate O
our O
parser O
. O
Punctuation O
is O
ignored O
in O
all O
the O
evaluations O
. O
We O
use O
the O
same O
treebanks O
and O
preprocessing O
as O
Ma O
et O
al O
. O
( O
2018 O
) O
for O
PTB O
, O
CTB O
, O
and O
UD O
. O
For O
all O
the O
datasets O
, O
we O
remove O
sentences O
longer O
than O
90 O
words O
in O
training O
sets O
for O
faster O
computation O
. O
We O
use O
GNN O
, O
Local1O O
, O
Single1O O
, O
Local2O O
andSingle2O O
to O
represent O
the O
approaches O
of O
Ji O
et O
al O
. O
( O
2019 O
) O
, O
Dozat O
and O
Manning O
( O
2017 O
) O
, O
DozatHidden O
Layer O
Hidden O
Sizes O
Word O
/ O
GloVe O
/ O
Char O
100 O
POS O
50 O
GloVe O
Linear O
125 O
BERT O
Linear O
125 O
BiLSTM O
3 O
* O
600 O
Char O
LSTM O
1 O
* O
400 O
Unary O
Arc O
( O
UD O
) O
500 O
Local1O O
/Local2O O
Unary O
Arc O
( O
Others O
) O
450 O
Single1O O
/Single2O O
Unary O
Arc O
( O
Others O
) O
550 O
Label O
150 O
Binary O
Arc O
150 O
Dropouts O
Dropout O
Prob O
. O
Word O
/ O
GloVe O
/ O
POS O
20 O
% O
Char O
LSTM O
( O
FF O
/ O
recur O
) O
33 O
% O
Char O
Linear O
33 O
% O
BiLSTM O
( O
FF O
/ O
recur O
) O
45%/25 O
% O
Unary O
Arc O
/ O
Label O
25%/33 O
% O
Binary O
Arc O
25 O
% O
Optimizer O
& O
Loss O
Value O
Local1O O
/Local2O O
Interpolation O
( O
Œª O
) O
0.40 O
Single1O O
/Single2O O
Interpolation O
( O
Œª O
) O
0.07 O
AdamŒ≤1 O
0 O
AdamŒ≤2 O
0.95 O
Decay O
Rate O
0.85 O
Decay O
Step O
( O
without O
devimprovement O
) O
500 O
Weight O
Initialization O
Mean O
/ O
Stddev O
Unary O
weight O
0.0/1.0 O
Binary O
weight O
0.0/0.25 O
Table O
1 O
: O
Hyper O
- O
parameter O
for O
Local1O O
, O
Single2O O
and O
Local2O O
in O
our O
experiment O
. O
and O
Manning O
( O
2018 O
) O
, O
and O
our O
two O
second O
- O
order O
approaches O
respectively O
. O
For O
all O
the O
approaches O
, O
we O
use O
the O
MST O
algorithm O
to O
guarantee O
treestructured O
output O
in O
testing O
. O
We O
use O
the O
concatenation O
of O
word O
embeddings O
, O
character O
- O
level O
embeddings O
and O
part O
- O
of O
- O
speech O
( O
POS O
) O
tag O
embeddings O
to O
represent O
words O
and O
additionally O
concatenate O
BERT O
embeddings O
for O
experiments O
with O
BERT O
. O
For O
a O
fair O
comparison O
with O
previous O
work O
, O
we O
use O
GloVe O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
and O
BERTLarge O
- O
Uncased O
model O
for O
PTB O
, O
and O
structuredskipgram O
( O
Ling O
et O
al O
. O
, O
2015 O
) O
and O
BERT O
- O
BaseChinese O
model O
for O
CTB O
. O
For O
UD O
, O
we O
use O
fastText O
embeddings O
( O
Bojanowski O
et O
al O
. O
, O
2017 O
) O
and O
BERTBase O
- O
Multilingual O
- O
Cased O
model O
for O
different O
languages O
. O
We O
set O
the O
default O
iteration O
number O
for O
our O
approaches O
to O
3 O
because O
we O
Ô¨Ånd O
no O
improvement O
on O
more O
or O
less O
iterations O
. O
ForGNN1 O
, O
we O
rerun O
the O
code O
based O
on O
the O
ofÔ¨Åcial O
release O
of O
Ji O
et O
al O
. O
( O
2019 O
) O
. O
For O
Single1O O
, O
Local1O2,Single2O3 O
, O
we O
implement O
these O
ap1https://github.com/AntNLP/ O
gnn O
- O
dep O
- O
parsing O
2https://github.com/tdozat/Parser-v3 O
3https://github.com/wangxinyu0922/ O
Second_Order_SDP95PTB O
CTB O
UAS O
LAS O
UAS O
LAS O
Dozat O
and O
Manning O
( O
2017 O
) O
95.74 O
94.08 O
89.30 O
88.23 O
Ma O
et O
al O
. O
( O
2018) O
‚ô† O
95.87 O
94.19 O
90.59 O
89.29 O
F&G O
( O
2019) O
‚ô† O
96.04 O
94.43 O
- O
GNN O
95.87 O
94.15 O
90.78 O
89.50 O
Single1O O
95.75 O
94.04 O
90.53 O
89.28 O
Local1O O
95.83 O
94.23 O
90.59 O
89.28 O
Single2O O
95.86 O
94.19 O
90.75 O
89.55 O
Local2O O
95.98 O
94.34 O
90.81 O
89.57 O
Ji O
et O
al O
. O
( O
2019)‚Ä†95.97 O
94.31 O
- O
Zhang O
et O
al O
. O
( O
2020)‚Ä†‚Ä°96.14 O
94.49 O
- O
Local2O‚Ä†‚Ä°96.12 O
94.47 O
- O
+ O
BERT O
Zhou O
and O
Zhao O
( O
2019) O
‚ô£ O
97.20 O
95.72 O
Clark O
et O
al O
. O
( O
2018)/diamondmath96.60 O
95.00 O
- O
Single1O O
96.82 O
95.20 O
92.73 O
91.64 O
Local1O O
96.86 O
95.32 O
92.47 O
91.30 O
Single2O O
96.86 O
95.31 O
92.78 O
91.69 O
Local2O O
96.91 O
95.34 O
92.55 O
91.38 O
Table O
2 O
: O
Comparison O
of O
our O
approaches O
and O
the O
previous O
state O
- O
of O
- O
the O
- O
art O
approaches O
on O
PTB O
and O
CTB O
. O
We O
report O
our O
results O
averaged O
over O
5 O
runs.‚Ä† O
: O
These O
approaches O
perform O
model O
selection O
based O
on O
the O
score O
on O
the O
development O
set.‚Ä° O
: O
These O
approaches O
do O
not O
use O
POS O
tags O
as O
input./diamondmath O
: O
Clark O
et O
al O
. O
( O
2018 O
) O
uses O
semisupervised O
multi O
- O
task O
learning O
with O
ELMo O
embeddings O
. O
‚ô† O
: O
These O
approaches O
use O
structured O
- O
skipgram O
embeddings O
instead O
of O
GloVe O
embeddings O
for O
PTB O
. O
‚ô£ O
: O
For O
reference O
, O
Zhou O
and O
Zhao O
( O
2019 O
) O
utilized O
both O
dependency O
and O
constituency O
information O
in O
their O
approach O
. O
Therefore O
, O
the O
results O
are O
not O
comparable O
to O
our O
results O
. O
proaches O
based O
on O
the O
ofÔ¨Åcial O
release O
code O
of O
Wang O
et O
al O
. O
( O
2019 O
) O
and O
we O
implement O
Local2O O
based O
on O
this O
code O
. O
In O
speed O
comparison O
, O
we O
implement O
the O
second O
- O
order O
approaches O
based O
on O
an O
PyTorch O
implementation O
biafÔ¨Åne O
parser4implemented O
by O
Zhang O
et O
al O
. O
( O
2020 O
) O
for O
a O
fair O
speed O
comparison O
with O
their O
approach5 O
. O
Since O
we O
Ô¨Ånd O
that O
the O
accuracy O
of O
our O
approaches O
based O
on O
PyTorch O
implementation O
on O
PTB O
does O
not O
change O
, O
we O
only O
report O
scores O
based O
on O
Wang O
et O
al O
. O
( O
2019 O
) O
. O
3.2 O
Hyper O
- O
parameters O
The O
hyper O
- O
parameters O
we O
used O
in O
our O
experiments O
is O
shown O
in O
Table O
1 O
. O
We O
tune O
the O
the O
hidden O
size O
for O
calculating O
s(edge O
) O
ij(Unary O
Arc O
in O
the O
table O
) O
separately O
for O
PTB O
and O
CTB O
. O
Following O
Qi O
et O
al O
. O
( O
2018 O
) O
, O
we O
switch O
to O
AMSGrad O
( O
Reddi O
et O
al O
. O
, O
2018 O
) O
after O
5,000 O
iterations O
without O
improvement O
. O
We O
train O
models O
for O
75,000 O
iterations O
with O
batch O
sizes O
of O
4https://github.com/yzhangcs/parser O
5At O
the O
time O
we O
Ô¨Ånished O
the O
paper O
, O
the O
ofÔ¨Åcial O
code O
for O
the O
second O
- O
order O
tree O
CRF O
parser O
have O
not O
release O
yet O
. O
We O
believe O
it O
is O
a O
fair O
comparison O
since O
we O
use O
the O
same O
settings O
and O
GPU O
as O
Zhang O
et O
al O
. O
( O
2020).6000 O
tokens O
and O
stopped O
the O
training O
early O
after O
10,000 O
iterations O
without O
improvements O
on O
development O
sets O
. O
Different O
from O
previous O
approaches O
such O
as O
Dozat O
and O
Manning O
( O
2017 O
) O
and O
Ji O
et O
al O
. O
( O
2019 O
) O
, O
we O
use O
Adam O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
with O
a O
learning O
rate O
of O
0.01 O
and O
anneal O
the O
learning O
rate O
by O
0.85 O
for O
every O
500 O
iterations O
without O
improvement O
on O
the O
development O
set O
for O
optimization O
. O
For O
GNN O
, O
we O
train O
the O
models O
with O
the O
same O
setting O
as O
in O
Ji O
et O
al O
. O
( O
2019 O
) O
. O
We O
do O
not O
use O
character O
embeddings O
and O
our O
optimization O
settings O
for O
GNN O
because O
we O
Ô¨Ånd O
they O
do O
not O
improve O
the O
accuracy O
. O
For O
the O
edge O
loss O
of O
Single O
approaches O
, O
Zhang O
et O
al O
. O
( O
2019 O
) O
proposed O
to O
sample O
a O
subset O
of O
the O
negative O
edges O
to O
balance O
positive O
and O
negative O
examples O
, O
but O
we O
Ô¨Ånd O
that O
using O
a O
relatively O
small O
interpolation O
Œª(shown O
in O
Table O
1 O
) O
on O
label O
loss O
can O
improve O
the O
accuracy O
and O
the O
sampling O
does O
not O
help O
further O
improve O
the O
accuracy O
. O
3.3 O
Results O
Table O
2 O
shows O
the O
Unlabeled O
Attachment O
Score O
( O
UAS O
) O
and O
Labeled O
Attachment O
Score O
( O
LAS O
) O
of O
all O
the O
approaches O
as O
well O
as O
the O
reported O
scores O
of O
previous O
state O
- O
of O
- O
the O
- O
art O
approaches O
on O
PTB O
and O
CTB O
. O
It O
can O
be O
seen O
that O
without O
BERT O
, O
our O
Local2O O
achieves O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
CTB O
and O
has O
almost O
the O
same O
accuracy O
as O
the O
very O
recent O
work O
of O
Zhang O
et O
al O
. O
( O
2020 O
) O
on O
PTB O
. O
With O
BERT O
embeddings O
, O
Local2O O
performs O
the O
best O
on O
PTB O
while O
Single2O O
has O
the O
best O
accuracy O
on O
CTB O
. O
Table O
3 O
shows O
the O
results O
of O
the O
Ô¨Åve O
approaches O
on O
UD O
in O
addition O
to O
PTB O
and O
CTB O
. O
We O
make O
the O
following O
observations O
. O
First O
, O
our O
second O
- O
order O
approaches O
outperform O
GNN O
and O
the O
Ô¨Årst O
- O
order O
approaches O
both O
with O
and O
without O
BERT O
embeddings O
, O
showing O
that O
second O
- O
order O
decoders O
are O
still O
helpful O
in O
neural O
parsing O
even O
with O
strong O
contextual O
embeddings O
. O
Second O
, O
without O
BERT O
, O
Local O
slightly O
outperforms O
Single O
, O
although O
the O
difference O
between O
the O
two O
is O
quite O
small6 O
; O
when O
BERT O
is O
used O
, O
however O
, O
Single O
clearly O
outperforms O
Local O
, O
which O
is O
quite O
interesting O
and O
warrants O
further O
investigation O
in O
the O
future O
. O
Third O
, O
the O
relative O
strength O
ofLocal O
andSingle O
approaches O
varies O
over O
treebanks O
, O
suggesting O
varying O
importance O
of O
the O
headselection O
constraint O
. O
6Note O
that O
Zhang O
et O
al O
. O
( O
2019 O
) O
reports O
higher O
difference O
in O
accuracy O
between O
Ô¨Årst O
- O
order O
Local O
andSingle O
approaches O
. O
The O
discrepancy O
is O
most O
likely O
caused O
by O
our O
better O
designed O
loss O
function O
and O
tuned O
hyper O
- O
parameters.96PTB O
CTB O
bg O
ca O
cs O
de O
en O
es O
fr O
it O
nl O
no O
ro O
ru O
Avg O
. O
GNN O
94.15 O
89.50‚Ä†90.33 O
92.39 O
90.95 O
79.73 O
88.43 O
91.56 O
87.23 O
92.44 O
88.57 O
89.38 O
85.26 O
91.20 O
89.37 O
Single1O O
94.04 O
89.28 O
90.05 O
92.72‚Ä†92.07 O
81.73 O
89.55 O
92.10 O
88.27 O
92.64 O
89.57 O
91.81 O
85.39 O
92.60 O
90.13 O
Local1O O
94.23 O
89.28 O
90.30 O
92.56 O
92.15 O
81.42 O
89.43 O
91.99 O
88.26 O
92.49 O
89.76 O
91.91 O
85.27 O
92.72 O
90.13 O
Single2O O
94.19 O
89.55‚Ä†90.24 O
92.82‚Ä†92.13 O
81.99‚Ä†89.64‚Ä†92.17‚Ä†88.69 O
92.83‚Ä†89.97‚Ä†91.90 O
85.53‚Ä†92.58 O
90.30‚Ä† O
Local2O O
94.34‚Ä†‚Ä°89.57‚Ä†90.53‚Ä†92.83‚Ä†92.12 O
81.73 O
89.72‚Ä†92.07 O
88.53 O
92.78 O
90.19‚Ä†91.88 O
85.88‚Ä†‚Ä°92.67 O
90.35‚Ä† O
+ O
BERT O
Single1O O
95.20 O
91.64‚Ä†90.87 O
93.55‚Ä†92.01 O
81.95‚Ä†90.44‚Ä†92.56‚Ä†89.35 O
93.44‚Ä†90.89 O
91.78 O
86.13‚Ä†92.51 O
90.88‚Ä† O
Local1O O
95.32 O
91.30 O
91.03 O
93.17 O
91.93 O
81.66 O
90.09 O
92.32 O
89.26 O
93.05 O
90.93 O
91.62 O
85.67 O
92.51 O
90.70 O
Single2O O
95.31 O
91.69‚Ä†‚Ä°91.30‚Ä†93.60‚Ä†‚Ä°92.09‚Ä†82.00‚Ä†‚Ä°90.75‚Ä†‚Ä°92.62‚Ä†‚Ä°89.32 O
93.66‚Ä†91.21 O
91.74 O
86.40‚Ä†92.61 O
91.02‚Ä†‚Ä° O
Local2O O
95.34 O
91.38 O
91.13 O
93.34‚Ä†92.07‚Ä†81.67 O
90.43‚Ä†92.45‚Ä†89.26 O
93.50‚Ä†90.99 O
91.66 O
86.09‚Ä†92.66 O
90.86‚Ä† O
Table O
3 O
: O
LAS O
and O
standard O
deviations O
on O
test O
sets O
. O
We O
report O
results O
averaged O
over O
5 O
runs O
. O
We O
use O
ISO O
639 O
- O
1 O
codes O
to O
represent O
languages O
from O
UD O
. O
‚Ä†means O
that O
the O
model O
is O
statistically O
signiÔ¨Åcantly O
better O
than O
the O
Local1O O
model O
by O
Wilcoxon O
rank O
- O
sum O
test O
with O
a O
signiÔ¨Åcance O
level O
of O
p O
< O
0.05 O
. O
We O
use‚Ä°to O
represent O
winner O
of O
the O
signiÔ¨Åcant O
test O
between O
the O
Single2O O
andLocal2O O
models O
. O
System O
Train O
Test O
Time O
Complexity O
GNN O
392 O
464 O
O(n2d O
) O
Zhang O
et O
al O
. O
( O
2020 O
) O
200 O
400 O
O(n3 O
) O
Single1O O
616 O
1123 O
O(n2 O
) O
Local1O O
625 O
1150 O
O(n2 O
) O
Single2O O
481 O
966 O
O(n3 O
) O
Local2O O
486 O
1006 O
O(n3 O
) O
Table O
4 O
: O
Comparison O
of O
training O
and O
testing O
speed O
( O
sentences O
per O
second O
) O
and O
the O
time O
complexity O
of O
the O
decoders O
of O
different O
approaches O
on O
PTB O
. O
3.4 O
Speed O
Comparison O
We O
evaluate O
the O
speed O
of O
different O
approaches O
on O
a O
single O
GeForce O
GTX O
1080 O
Ti O
GPU O
following O
the O
setting O
of O
Zhang O
et O
al O
. O
( O
2020 O
) O
. O
As O
shown O
in O
Table O
4 O
, O
our O
Local O
approach O
and O
Single O
approach O
have O
almost O
the O
same O
speed O
. O
Our O
second O
- O
order O
approaches O
only O
slow O
down O
the O
training O
and O
testing O
speed O
in O
comparison O
with O
the O
Ô¨Årst O
- O
order O
approaches O
by O
23 O
% O
and O
12 O
% O
respectively O
. O
They O
are O
also O
signiÔ¨Åcantly O
faster O
than O
previous O
state O
- O
of O
- O
theart O
approaches O
. O
Our O
Local O
approach O
is O
1.2 O
and O
2.3 O
times O
faster O
than O
GNN O
in O
training O
and O
testing O
respectively O
and O
is O
2.4 O
and O
2.9 O
times O
faster O
than O
the O
second O
- O
order O
tree O
CRF O
approach O
of O
Zhang O
et O
al O
. O
( O
2020 O
) O
. O
In O
terms O
of O
time O
complexity O
, O
our O
second O
- O
order O
decoders O
have O
a O
time O
complexity O
of O
O(n3)7 O
; O
while O
the O
time O
complexity O
of O
GNN O
isO(n2d O
) O
, O
the O
hidden O
sized(500 O
by O
default O
) O
is O
typically O
much O
larger O
than O
sentence O
length O
n O
; O
and O
the O
decoder O
of O
Zhang O
et O
al O
. O
( O
2020 O
) O
has O
a O
time O
complexity O
of O
O(n3)as O
well O
, O
but O
it O
requires O
sequential O
computation O
over O
the O
input O
sentence O
while O
our O
decoders O
can O
be O
parallelized O
7The O
MST O
algorithm O
has O
a O
time O
complexity O
of O
O(n2)and O
we O
follow O
Dozat O
et O
al O
. O
( O
2017 O
) O
only O
using O
the O
MST O
algorithm O
when O
the O
argmax O
predictions O
of O
structured O
output O
are O
not O
trees.over O
words O
of O
the O
input O
sentence O
. O
4 O
Conclusion O
We O
propose O
second O
- O
order O
graph O
- O
based O
dependency O
parsing O
based O
on O
message O
passing O
and O
end O
- O
toend O
neural O
networks O
. O
We O
modify O
a O
previous O
approach O
that O
predicts O
dependency O
edges O
independently O
and O
also O
design O
a O
new O
approach O
that O
incorporates O
the O
head O
- O
selection O
structured O
constraint O
. O
Our O
experiments O
show O
that O
our O
second O
- O
order O
approaches O
have O
better O
overall O
performance O
than O
the O
Ô¨Årst O
- O
order O
baselines O
; O
they O
achieve O
competitive O
accuracy O
with O
very O
recent O
start O
- O
of O
- O
the O
- O
art O
second O
- O
order O
graph O
- O
based O
parsers O
and O
are O
signiÔ¨Åcantly O
faster O
. O
Our O
empirical O
comparisons O
also O
show O
that O
secondorder O
decoders O
still O
outperform O
Ô¨Årst O
- O
order O
decoders O
even O
with O
BERT O
embeddings O
, O
and O
that O
the O
usefulness O
of O
the O
head O
- O
selection O
constraint O
is O
limited O
, O
especially O
when O
using O
BERT O
embeddings O
. O
Our O
code O
is O
publicly O
avilable O
at O
https://github.com/ O
wangxinyu0922 O
/ O
Second_Order_Parsing O
. O
Acknowledgements O
This O
work O
was O
supported O
by O
the O
National O
Natural O
Science O
Foundation O
of O
China O
( O
61976139 O
) O
. O
Abstract O
Current O
end O
- O
to O
- O
end O
semantic O
role O
labeling O
is O
mostly O
accomplished O
via O
graph O
- O
based O
neural O
models O
. O
However O
, O
these O
all O
are O
Ô¨Årst O
- O
order O
models O
, O
where O
each O
decision O
for O
detecting O
any O
predicate O
- O
argument O
pair O
is O
made O
in O
isolation O
with O
local O
features O
. O
In O
this O
paper O
, O
we O
present O
a O
high O
- O
order O
reÔ¨Åning O
mechanism O
to O
perform O
interaction O
between O
all O
predicate O
- O
argument O
pairs O
. O
Based O
on O
the O
baseline O
graph O
model O
, O
our O
highorder O
reÔ¨Åning O
module O
learns O
higher O
- O
order O
features O
between O
all O
candidate O
pairs O
via O
attention O
calculation O
, O
which O
are O
later O
used O
to O
update O
the O
original O
token O
representations O
. O
After O
several O
iterations O
of O
reÔ¨Ånement O
, O
the O
underlying O
token O
representations O
can O
be O
enriched O
with O
globally O
interacted O
features O
. O
Our O
high O
- O
order O
model O
achieves O
state O
- O
of O
- O
the O
- O
art O
results O
on O
Chinese O
SRL O
data O
, O
including O
CoNLL09 O
and O
Universal O
Proposition O
Bank O
, O
meanwhile O
relieving O
the O
long O
- O
range O
dependency O
issues O
. O
1 O
Introduction O
Semantic O
role O
labeling O
( O
SRL O
) O
, O
as O
the O
shallow O
semantic O
parsing O
aiming O
to O
detect O
the O
semantic O
predicates O
and O
their O
argument O
roles O
in O
texts O
, O
plays O
a O
core O
role O
in O
natural O
language O
processing O
( O
NLP O
) O
community O
( O
Pradhan O
et O
al O
. O
, O
2005 O
; O
Zhao O
et O
al O
. O
, O
2009 O
; O
Lei O
et O
al O
. O
, O
2015 O
; O
Xia O
et O
al O
. O
, O
2019b O
) O
. O
SRL O
is O
traditionally O
handled O
by O
two O
pipeline O
steps O
: O
predicate O
identiÔ¨Åcation O
( O
Scheible O
, O
2010 O
) O
and O
argument O
role O
labeling O
( O
Pradhan O
et O
al O
. O
, O
2005 O
) O
. O
More O
recently O
, O
growing O
interests O
are O
paid O
for O
developing O
end O
- O
to O
- O
end O
SRL O
, O
achieving O
both O
two O
subtasks O
, O
i.e. O
, O
recognizing O
all O
possible O
predicates O
together O
with O
their O
arguments O
jointly O
, O
via O
one O
single O
model O
( O
He O
et O
al O
. O
, O
2018a O
) O
. O
The O
end O
- O
to O
- O
end O
joint O
architecture O
can O
greatly O
alleviate O
the O
error O
propagation O
problem O
, O
thus O
helping O
to O
achieve O
better O
task O
performance O
. O
Currently O
, O
the O
end O
- O
to O
- O
end O
SRL O
methods O
largely O
are O
graph O
- O
based O
‚àóCorresponding O
author.neural O
models O
, O
enumerating O
all O
possible O
predicates O
and O
their O
arguments O
exhaustively O
( O
He O
et O
al O
. O
, O
2018a O
; O
Cai O
et O
al O
. O
, O
2018 O
; O
Li O
et O
al O
. O
, O
2019 O
) O
. O
However O
, O
these O
Ô¨Årst O
- O
order O
models O
that O
only O
consider O
one O
predicateargument O
pair O
at O
a O
time O
can O
be O
limited O
to O
short O
- O
term O
features O
and O
local O
decisions O
, O
thus O
being O
subjective O
to O
long O
- O
range O
dependency O
issues O
existing O
at O
large O
surface O
distances O
between O
arguments O
( O
Chen O
et O
al O
. O
, O
2019 O
; O
Lyu O
et O
al O
. O
, O
2019 O
) O
. O
This O
makes O
it O
imperative O
to O
capture O
the O
global O
interactions O
between O
multiple O
predicates O
and O
arguments O
. O
In O
this O
paper O
, O
based O
on O
the O
graph O
- O
based O
model O
architecture O
, O
we O
propose O
to O
further O
learn O
the O
higherorder O
interaction O
between O
all O
predicate O
- O
argument O
pairs O
by O
performing O
iterative O
reÔ¨Åning O
for O
the O
underlying O
token O
representations O
. O
Figure O
1 O
illustrates O
the O
overall O
framework O
of O
our O
method O
. O
The O
BiLSTM O
encoder O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
Ô¨Årst O
encodes O
the O
inputs O
into O
the O
initial O
token O
representations O
for O
producing O
predicate O
and O
argument O
representations O
, O
respectively O
. O
The O
biafÔ¨Åne O
attention O
then O
exhaustively O
calculates O
the O
score O
representations O
for O
all O
the O
candidate O
predicate O
- O
argument O
pairs O
. O
Based O
on O
all O
these O
score O
representations O
, O
our O
highorder O
reÔ¨Åning O
module O
generates O
high O
- O
order O
feature O
for O
each O
corresponding O
token O
via O
an O
attention O
mechanism O
, O
which O
is O
then O
used O
for O
upgrading O
the O
raw O
token O
representation O
. O
After O
total O
Niterations O
of O
the O
above O
reÔ¨Åning O
procedure O
, O
the O
information O
between O
the O
predicates O
and O
the O
associated O
arguments O
can O
be O
fully O
interacted O
, O
and O
thus O
results O
in O
global O
consistency O
for O
SRL O
. O
On O
the O
other O
hand O
, O
most O
of O
the O
existing O
SRL O
studies O
focus O
on O
the O
English O
language O
, O
while O
there O
is O
little O
work O
in O
Chinese O
, O
mainly O
due O
to O
the O
limited O
amount O
of O
annotated O
data O
. O
In O
this O
study O
, O
we O
focus O
on O
the O
Chinese O
SRL O
. O
We O
show O
that O
our O
proposed O
high O
- O
order O
reÔ¨Åning O
mechanism O
can O
be O
especially O
beneÔ¨Åcial O
for O
such O
lower O
- O
resource O
language O
. O
Meanwhile O
, O
our O
proposed O
reÔ¨Åning O
process O
is O
fully100FFNs O
..... O
+ O
.+‚äïw1 O
w2 O
b. O
. O
.++ O
... O
.... O
. O
. O
... O
... O
BiLSTM O
encoderBiaffine O
attention O
Input O
  O
representationToken O
representationArgument O
  O
representationToken O
representationHigh O
-order O
feature O
representationRefined O
t O
oken O
representation O
Score O
  O
representation O
i O
- O
th O
refining O
  O
iteration O
Predicate O
  O
representationFigure O
1 O
: O
The O
overview O
of O
the O
graph O
- O
based O
high O
- O
order O
model O
for O
end O
- O
to O
- O
end O
SRL O
. O
The O
dotted O
- O
line O
green O
box O
is O
our O
proposed O
high O
- O
order O
reÔ¨Åning O
module O
. O
parallel O
and O
differentiable O
. O
We O
conduct O
experiments O
on O
the O
dependencybased O
Chinese O
SRL O
datasets O
, O
including O
CoNLL09 O
( O
Haji O
Àác O
et O
al O
. O
, O
2009 O
) O
, O
and O
Universal O
Proposition O
Bank O
( O
Akbik O
et O
al O
. O
, O
2015 O
; O
Akbik O
and O
Li O
, O
2016 O
) O
. O
Results O
show O
that O
the O
graph O
- O
based O
end O
- O
to O
- O
end O
model O
with O
our O
proposed O
high O
- O
order O
reÔ¨Åning O
consistently O
brings O
task O
improvements O
, O
compared O
with O
baselines O
, O
achieving O
state O
- O
of O
- O
the O
- O
art O
results O
for O
Chinese O
end O
- O
to O
- O
end O
SRL O
. O
2 O
Related O
Work O
Gildea O
and O
Jurafsky O
( O
2000 O
) O
pioneer O
the O
task O
of O
semantic O
role O
labeling O
, O
as O
a O
shallow O
semantic O
parsing O
. O
Earlier O
efforts O
are O
paid O
for O
designing O
hand O
- O
crafted O
discrete O
features O
with O
machine O
learning O
classiÔ¨Åers O
( O
Pradhan O
et O
al O
. O
, O
2005 O
; O
Punyakanok O
et O
al O
. O
, O
2008 O
; O
Zhao O
et O
al O
. O
, O
2009 O
) O
. O
Later O
, O
a O
great O
deal O
of O
work O
takes O
advantages O
of O
neural O
networks O
with O
distributed O
features O
( O
FitzGerald O
et O
al O
. O
, O
2015 O
; O
Roth O
and O
Lapata O
, O
2016 O
; O
Marcheggiani O
and O
Titov O
, O
2017 O
; O
Strubell O
et O
al O
. O
, O
2018 O
) O
. O
On O
the O
other O
hand O
, O
many O
previous O
work O
shows O
that O
integrating O
syntactic O
tree O
structure O
can O
greatly O
facilitate O
SRL O
( O
Marcheggiani O
et O
al O
. O
, O
2017 O
; O
He O
et O
al O
. O
, O
2018b O
; O
Zhang O
et O
al O
. O
, O
2019 O
; O
Fei O
et O
al O
. O
, O
2020b O
) O
. O
Prior O
studies O
traditionally O
separate O
SRL O
into O
two O
individual O
subtasks O
, O
i.e. O
, O
predicate O
disambiguation O
and O
argument O
role O
labeling O
, O
mostly O
conducting O
only O
the O
argument O
role O
labeling O
based O
on O
the O
pre O
- O
identiÔ¨Åed O
predicate O
( O
Pradhan O
et O
al O
. O
, O
2005 O
; O
Zhao O
et O
al O
. O
, O
2009 O
; O
FitzGerald O
et O
al O
. O
, O
2015 O
; O
He O
et O
al O
. O
, O
2018b O
; O
Fei O
et O
al O
. O
, O
2020a O
) O
. O
More O
recently O
, O
several O
researches O
consider O
the O
end O
- O
to O
- O
end O
solution O
that O
handles O
both O
two O
subtasks O
by O
one O
single O
model O
. O
All O
of O
them O
employs O
graph O
- O
based O
neural O
model O
, O
exhaustively O
enumerating O
all O
the O
possible O
predicate O
and O
argument O
mentions O
, O
as O
well O
as O
their O
relations O
( O
He O
et O
al O
. O
, O
2018a O
; O
Cai O
et O
al O
. O
, O
2018 O
; O
Li O
et O
al O
. O
, O
2019 O
; O
Xia O
et O
al O
. O
, O
2019a O
) O
. O
Most O
of O
these O
end O
- O
to O
- O
end O
models O
, O
however O
, O
are O
Ô¨Årst O
- O
order O
, O
considering O
merely O
one O
predicate O
- O
argument O
pair O
at O
a O
time O
. O
In O
this O
work O
, O
we O
propose O
a O
high O
- O
order O
reÔ¨Åning O
mechanism O
to O
reinforce O
the O
graph O
- O
based O
end O
- O
to O
- O
end O
method O
. O
Note O
that O
most O
of O
the O
existing O
SRL O
work O
focuses O
on O
the O
English O
language O
, O
with O
less O
for O
Chinese O
, O
mainly O
due O
to O
the O
limited O
amount O
of O
annotated O
data O
( O
Xia O
et O
al O
. O
, O
2019a O
) O
. O
In O
this O
paper O
, O
we O
aim O
to O
improve O
the O
Chinese O
SRL O
and O
make O
compensation O
of O
the O
data O
scarcity O
by O
our O
proposed O
high O
- O
order O
model O
. O
3 O
Framework O
Task O
formulation O
. O
Following O
prior O
end O
- O
to O
- O
end O
SRL O
work O
( O
He O
et O
al O
. O
, O
2018a O
; O
Li O
et O
al O
. O
, O
2019 O
) O
, O
we O
treat O
the O
task O
as O
predicate O
- O
argument O
- O
role O
triplets O
prediction O
. O
Given O
an O
input O
sentence O
S= O
{ O
w1,¬∑¬∑¬∑,wn O
} O
, O
the O
system O
is O
expected O
to O
output O
a O
set O
of O
tripletsY O
‚àà O
P√óA√óR O
, O
whereP= O
{ O
p1,¬∑¬∑¬∑,pm}are O
all O
possible O
predicate O
tokens O
, O
A={a1,¬∑¬∑¬∑,al}are O
all O
associated O
argument O
tokens O
, O
andRare O
the O
corresponding O
role O
labels O
for O
eachai O
, O
including O
a O
null O
label O
/epsilon1indicating O
no O
relation O
between O
a O
pair O
of O
predicate O
argument O
. O
3.1 O
Baseline O
Graph O
- O
based O
SRL O
Model O
Our O
baseline O
SRL O
model O
is O
mostly O
from O
He O
et O
al O
. O
( O
2018a O
) O
. O
First O
, O
we O
obtain O
the O
vector O
representation O
xw O
tof O
each O
word O
wtfrom O
pre O
- O
trained O
embeddings O
. O
We O
then O
make O
use O
of O
the O
part O
- O
of O
- O
speech O
( O
POS O
) O
tag O
for O
each O
word O
, O
and O
use O
its O
embedding O
xpos O
t. O
A O
convolutional O
neural O
networks O
( O
CNNs O
) O
is O
used O
to O
encode O
Chinese O
characters O
inside O
a O
word O
xc O
t. O
We O
concatenate O
them O
as O
input O
representations O
: O
xt= O
[ O
xw O
t;xpos O
t;xc O
t O
] O
. O
Thereafter O
, O
a O
multi O
- O
layer O
bidirectional O
LSTM O
( O
BiLSTM O
) O
is O
used O
to O
encode O
the O
input O
representations O
into O
contextualized O
token O
representations O
: O
h1,¬∑¬∑¬∑,hn O
= O
BiLSTM O
( O
x1,¬∑¬∑¬∑,xn O
) O
. O
Based O
on O
the O
token O
representations O
, O
we O
further O
generate O
the O
separate O
predicate O
representations O
and O
argument O
representations O
: O
vp O
t O
= O
FFN(ht),va O
t O
= O
FFN(ht).101Then O
, O
a O
biafÔ¨Åne O
attention O
( O
Dozat O
and O
Manning O
, O
2016 O
) O
is O
used O
for O
scoring O
the O
semantic O
relationships O
exhaustively O
over O
all O
the O
predicate O
- O
argument O
pairs O
: O
vs(pi O
, O
aj O
) O
= O
vp O
i¬∑W1¬∑va O
j+W2¬∑[vp O
i;va O
j]+b,(1 O
) O
where O
W1,W2andbare O
parameters O
. O
Decoding O
and O
learning O
. O
Once O
a O
predicateargument O
pair O
( O
pi O
, O
aj)(i.e O
. O
, O
the O
role O
label O
r O
/ O
negationslash=/epsilon1 O
) O
is O
determined O
by O
a O
softmax O
classiÔ¨Åer O
, O
based O
on O
the O
score O
representation O
vs(pi O
, O
aj O
) O
, O
the O
model O
outputs O
this O
tuple O
( O
p O
, O
a O
, O
r O
) O
. O
During O
training O
, O
we O
optimize O
the O
probability O
PŒ∏(ÀÜy|S)of O
the O
tupley(pi O
, O
aj O
, O
r)over O
a O
sentence O
S O
: O
PŒ∏(y|S O
) O
= O
/productdisplay O
p‚ààP O
, O
a‚ààA O
, O
r‚ààRPŒ∏(y(p O
, O
a O
, O
r)|S O
) O
= O
/productdisplay O
p‚ààP O
, O
a‚ààA O
, O
r‚ààRœÜ(p O
, O
a O
, O
r O
) O
/summationtext O
ÀÜr‚ààRœÜ(p O
, O
a,ÀÜr),(2 O
) O
whereŒ∏is O
the O
parameters O
of O
the O
model O
and O
œÜ(p O
, O
a O
, O
r O
) O
represents O
the O
total O
unary O
score O
from O
: O
œÜ(p O
, O
a O
, O
r O
) O
= O
WpReLU O
( O
vp O
) O
+ O
WaReLU O
( O
va O
) O
+ O
WsReLU O
( O
vs(p O
, O
a O
) O
) O
. O
( O
3 O
) O
The O
Ô¨Ånal O
objective O
is O
to O
minimize O
the O
negative O
loglikelihood O
of O
the O
golden O
structure O
: O
L=‚àílogPŒ∏(y|S O
) O
. O
( O
4 O
) O
3.2 O
Higher O
- O
order O
ReÔ¨Åning O
The O
baseline O
graph O
model O
is O
a O
Ô¨Årst O
- O
order O
model O
, O
since O
it O
only O
considers O
one O
predicate O
- O
argument O
pair O
( O
as O
in O
Eq O
. O
3 O
) O
at O
a O
time O
. O
This O
makes O
it O
limited O
to O
short O
- O
term O
and O
local O
decisions O
, O
and O
thus O
subjective O
to O
long O
- O
distance O
dependency O
problem O
wherever O
there O
are O
larger O
surface O
distances O
between O
arguments O
. O
We O
here O
propose O
a O
higher O
- O
order O
reÔ¨Åning O
mechanism O
for O
allowing O
a O
deep O
interactions O
between O
all O
predicate O
- O
argument O
pairs O
. O
Our O
high O
- O
order O
model O
is O
shown O
in O
Figure O
1 O
. O
Compared O
with O
the O
baseline O
model O
, O
the O
main O
difference O
lies O
in O
the O
high O
- O
order O
reÔ¨Åning O
module O
. O
Our O
motivation O
is O
to O
inform O
each O
predicate O
- O
argument O
pair O
with O
the O
information O
of O
the O
other O
rest O
of O
pairs O
from O
the O
global O
viewpoint O
. O
We O
reach O
this O
by O
reÔ¨Åning O
the O
underlying O
token O
representations O
htwith O
reÔ¨Åned O
ones O
which O
carry O
high O
- O
order O
interacted O
features O
. O
Concretely O
, O
we O
take O
the O
baseline O
as O
the O
initiation O
, O
performing O
reÔ¨Ånement O
iteratively O
. O
At O
the O
i O
- O
th O
reÔ¨Åning O
iteration O
, O
we O
can O
collect O
the O
score O
representations O
Vi O
, O
s={vi O
, O
s O
1,¬∑¬∑¬∑,vi O
, O
s O
K}of O
all O
candidatepredicate O
- O
argument O
pairs O
, O
where O
K(i.e O
. O
,/parenleftbign O
2 O
/ O
parenrightbig O
) O
are O
the O
total O
combination O
number O
of O
these O
pairs O
. O
Based O
onVi O
, O
s O
, O
we O
then O
generate O
the O
high O
- O
order O
feature O
vector O
oi O
tby O
using O
an O
attention O
mechanism O
guided O
by O
the O
current O
token O
representation O
hi‚àí1 O
tfor O
word O
wtat O
last O
turn O
, O
i.e. O
, O
the O
( O
i-1)-th O
iteration O
: O
ui O
k O
= O
tanh(W3hi‚àí1 O
t+W4vi O
, O
s O
k O
) O
, O
Œ±i O
k O
= O
softmax O
( O
ui O
k O
) O
, O
oi O
t=/summationtextK O
k=1Œ±i O
kvi O
, O
s O
k,(5 O
) O
where O
W3andW4are O
parameters O
. O
We O
then O
concatenate O
the O
raw O
token O
representation O
and O
highorder O
feature O
representation O
together O
, O
and O
obtain O
the O
reÔ¨Åned O
token O
representation O
after O
a O
non O
- O
linear O
projection O
ÀÜhi O
t O
= O
FFN([oi O
t;hi‚àí1 O
t O
] O
) O
. O
Finally O
, O
we O
use O
ÀÜhi O
t O
to O
update O
the O
old O
one O
hi‚àí1 O
t. O
After O
totalNiterations O
of O
high O
- O
order O
reÔ¨Ånement O
, O
we O
expect O
the O
model O
to O
capture O
more O
informative O
features O
at O
global O
scope O
and O
achieve O
the O
global O
consistency O
. O
4 O
Experiments O
4.1 O
Settings O
Our O
method O
is O
evaluated O
on O
the O
Chinese O
SRL O
benchmarks O
, O
including O
CoNLL091and O
Universal O
Proposition O
Bank O
( O
UPB)2 O
. O
Each O
dataset O
comes O
with O
its O
own O
training O
, O
development O
and O
test O
sets O
. O
Precision O
, O
recall O
and O
F1 O
score O
are O
used O
as O
the O
metrics O
. O
We O
use O
the O
pre O
- O
trained O
Chinese O
fasttext O
embeddings3 O
. O
The O
BiLSTM O
has O
hidden O
size O
of O
350 O
, O
with O
three O
layers O
. O
The O
kernel O
sizes O
of O
CNN O
are O
[ O
3,4,5 O
] O
. O
We O
adopt O
the O
Adam O
optimizer O
with O
initial O
learning O
rate O
of O
1e-5 O
. O
We O
train O
the O
model O
by O
mini O
- O
batch O
size O
in O
[ O
16,32 O
] O
with O
early O
- O
stop O
strategy O
. O
We O
also O
use O
the O
contextualized O
Chinese O
word O
representations O
, O
i.e. O
, O
ELMo4and O
BERT O
( O
Chinese O
- O
base O
- O
version)5 O
. O
4.2 O
Main O
Results O
We O
mainly O
make O
comparisons O
with O
the O
recent O
endto O
- O
end O
SRL O
models O
, O
as O
well O
as O
the O
pipeline O
methods O
on O
standalone O
argument O
role O
labeling O
given O
the O
gold O
predicates O
. O
Table O
1 O
shows O
the O
results O
on O
the O
Chinese O
CoNLL09 O
. O
We O
Ô¨Årst O
Ô¨Ånd O
that O
the O
joint O
detection O
for O
predicates O
and O
arguments O
can O
be O
more O
beneÔ¨Åcial O
1https://catalog.ldc.upenn.edu/ O
LDC2012T03 O
2https://github.com/System-T/ O
UniversalPropositions O
3https://fasttext.cc/ O
4https://github.com/HIT-SCIR/ O
ELMoForManyLangs O
5https://github.com/google-research/ O
bert102Arg O
. O
Prd O
. O
P O
R O
F1 O
F1 O
‚Ä¢Pipeline O
method O
Zhao O
et O
al O
. O
( O
2009 O
) O
80.4 O
75.2 O
77.7 O
Bj¬®orkelund O
et O
al O
. O
( O
2009 O
) O
82.4 O
75.1 O
78.6 O
Roth O
and O
Lapata O
( O
2016 O
) O
83.2 O
75.9 O
79.4 O
Marcheggiani O
and O
Titov O
( O
2017 O
) O
84.6 O
80.4 O
82.5 O
He O
et O
al O
. O
( O
2018b O
) O
84.2 O
81.5 O
82.8 O
Cai O
and O
Lapata O
( O
2019)‚Ä°85.4 O
84.6 O
85.0 O
‚Ä¢End O
- O
to O
- O
end O
method O
He O
et O
al O
. O
( O
2018a O
) O
82.6 O
83.6 O
83.0 O
85.7 O
Cai O
et O
al O
. O
( O
2018 O
) O
84.7 O
84.0 O
84.3 O
86.0 O
Li O
et O
al O
. O
( O
2019 O
) O
84.9 O
84.6 O
84.8 O
86.9 O
Xia O
et O
al O
. O
( O
2019a O
) O
84.6 O
85.7 O
85.1 O
87.2 O
+ O
BERT O
88.0 O
89.1 O
88.5 O
89.6 O
Ours O
85.7 O
86.2 O
85.9 O
88.6 O
+ O
ELMo O
86.4 O
87.6 O
87.1 O
88.9 O
+ O
BERT O
87.4 O
89.3 O
88.8 O
90.3 O
Table O
1 O
: O
Performances O
on O
CoNLL09 O
. O
Results O
with‚Ä° O
indicates O
the O
additional O
resources O
are O
used O
. O
P O
R O
F1 O
He O
et O
al O
. O
( O
2018a O
) O
64.8 O
65.3 O
64.9 O
Cai O
et O
al O
. O
( O
2018 O
) O
65.0 O
66.4 O
65.8 O
Li O
et O
al O
. O
( O
2019 O
) O
65.4 O
67.2 O
66.0 O
Xia O
et O
al O
. O
( O
2019a O
) O
65.2 O
67.6 O
66.1 O
Ours O
67.5 O
68.8 O
67.9 O
+ O
ELMo O
68.0 O
70.6 O
68.8 O
+ O
BERT O
70.0 O
73.0 O
72.4 O
Table O
2 O
: O
Performances O
by O
end O
- O
to O
- O
end O
models O
for O
the O
argument O
role O
labeling O
on O
UPB O
. O
than O
the O
pipeline O
detection O
of O
SRL O
, O
notably O
with O
85.1 O
% O
F1 O
score O
on O
argument O
detection O
by O
Xia O
et O
al O
. O
( O
2019a O
) O
. O
Most O
importantly O
, O
our O
high O
- O
order O
end O
- O
toend O
model O
outperforms O
all O
these O
baselines O
on O
both O
two O
subtasks O
, O
with O
85.9 O
% O
F1 O
score O
for O
argument O
role O
labeling O
and O
88.6 O
% O
F1 O
score O
for O
predicate O
detection O
. O
When O
the O
contextualized O
word O
embeddings O
are O
available O
, O
we O
Ô¨Ånd O
that O
our O
model O
can O
achieve O
further O
improvements O
, O
i.e. O
, O
88.8 O
% O
and O
90.3 O
% O
F1 O
scores O
for O
two O
subtasks O
, O
respectively O
. O
Table O
2 O
shows O
the O
performances O
on O
UPB O
. O
Overall O
, O
the O
similar O
trends O
are O
kept O
as O
that O
on O
CoNLL09 O
. O
Our O
high O
- O
order O
model O
still O
performs O
the O
best O
, O
yielding O
67.9 O
% O
F1 O
score O
on O
argument O
role O
labeling O
, O
verifying O
its O
prominent O
capability O
for O
the O
SRL O
task O
. O
Also O
with O
BERT O
embeddings O
, O
our O
model O
further O
wins O
a O
great O
advance O
of O
performances O
. O
4.3 O
Analysis O
High O
- O
order O
reÔ¨Ånement O
. O
We O
take O
a O
further O
step O
, O
looking O
into O
our O
proposed O
high O
- O
order O
reÔ¨Åning O
1 O
2 O
3 O
4 O
583858789 O
IterationsF1(%)Arg.(Ours O
) O
Prd.(Ours O
) O
Arg.(He O
et O
al O
. O
( O
2018 O
) O
) O
Prd.(He O
et O
al O
. O
( O
2018))Figure O
2 O
: O
Performances O
by O
varying O
reÔ¨Åning O
iterations O
. O
1 O
2 O
3 O
- O
4 O
5 O
- O
6 O
7 O
- O
8 O
‚â•9607590 O
Distance O
from O
predicate O
to O
argument O
( O
words)F1(%)Ours O
He O
et O
al O
. O
( O
2018 O
) O
Li O
et O
al O
. O
( O
2019 O
) O
Figure O
3 O
: O
Argument O
recognition O
under O
varying O
surface O
distance O
between O
predicates O
and O
arguments O
. O
mechanism O
. O
We O
examine O
the O
performances O
under O
varying O
reÔ¨Åning O
iterations O
in O
Figure O
2 O
. O
Compared O
with O
the O
Ô¨Årst O
- O
order O
baseline O
model O
by O
He O
et O
al O
. O
( O
2018a O
) O
, O
our O
high O
- O
order O
model O
can O
achieve O
better O
performances O
for O
both O
two O
subtasks O
. O
We O
Ô¨Ånd O
that O
our O
model O
can O
reach O
the O
peak O
for O
predicate O
detection O
with O
total O
2 O
iterations O
of O
reÔ¨Ånement O
, O
while O
the O
best O
iteration O
number O
is O
4 O
for O
argument O
labeling O
. O
Long O
- O
distance O
dependencies O
. O
Figure O
3 O
shows O
the O
performances O
of O
argument O
recognition O
by O
different O
surface O
distance O
between O
predicates O
and O
arguments O
. O
The O
overall O
results O
decrease O
when O
arguments O
are O
farther O
away O
from O
the O
predicates O
. O
Nevertheless O
, O
our O
high O
- O
order O
model O
can O
beat O
against O
such O
drop O
signiÔ¨Åcantly O
. O
Especially O
when O
the O
distance O
grows O
larger O
, O
e.g. O
, O
distance O
‚â•7 O
, O
the O
winning O
score O
by O
our O
model O
even O
becomes O
more O
notable O
. O
5 O
Conclusion O
We O
proposed O
a O
high O
- O
order O
end O
- O
to O
- O
end O
model O
for O
Chinese O
SRL O
. O
Based O
on O
the O
baseline O
graph O
- O
based O
model O
, O
our O
high O
- O
order O
reÔ¨Åning O
module O
performed O
interactive O
learning O
between O
all O
predicate O
- O
argument O
pairs O
via O
attention O
calculation O
. O
The O
generated O
higher O
- O
order O
featured O
token O
representations O
then O
were O
used O
to O
update O
the O
original O
ones O
. O
After O
total O
Niterations O
of O
reÔ¨Ånement O
, O
we O
enriched O
the O
underlying O
token O
representations O
with O
global O
interactions O
, O
and O
made O
the O
learnt O
features O
more O
informative O
. O
Our103high O
- O
order O
model O
brought O
state O
- O
of O
- O
the O
- O
art O
results O
on O
Chinese O
SRL O
data O
, O
i.e. O
, O
CoNLL09 O
and O
Universal O
Proposition O
Bank O
, O
meanwhile O
relieving O
the O
longrange O
dependency O
issues O
. O
Acknowledgments O
We O
thank O
the O
anonymous O
reviewers O
for O
their O
valuable O
and O
detailed O
comments O
. O
This O
work O
is O
supported O
by O
the O
National O
Natural O
Science O
Foundation O
of O
China O
( O
No O
. O
61772378 O
, O
No O
. O
61702121 O
) O
, O
the O
National O
Key O
Research O
and O
Development O
Program O
of O
China O
( O
No O
. O
2017YFC1200500 O
) O
, O
the O
Research O
Foundation O
of O
Ministry O
of O
Education O
of O
China O
( O
No O
. O
18JZD015 O
) O
, O
the O
Major O
Projects O
of O
the O
National O
Social O
Science O
Foundation O
of O
China O
( O
No O
. O
11&ZD189 O
) O
, O
the O
Key O
Project O
of O
State O
Language O
Commission O
of O
China O
( O
No O
. O
ZDI135 O
- O
112 O
) O
and O
Guangdong O
Basic O
and O
Applied O
Basic O
Research O
Foundation O
of O
China O
( O
No O
. O
2020A151501705 O
) O
. O
Abstract O
Answer O
selection O
( O
AS O
) O
is O
an O
important O
subtask O
of O
document O
- O
based O
question O
answering O
( O
DQA O
) O
. O
In O
this O
task O
, O
the O
candidate O
answers O
come O
from O
the O
same O
document O
, O
and O
each O
answer O
sentence O
is O
semantically O
related O
to O
the O
given O
question O
, O
which O
makes O
it O
more O
challenging O
to O
select O
the O
true O
answer O
. O
WordNet O
provides O
powerful O
knowledge O
about O
concepts O
and O
their O
semantic O
relations O
, O
so O
we O
employ O
WordNet O
to O
enrich O
the O
abilities O
of O
paraphrasing O
and O
reasoning O
of O
the O
network O
- O
based O
question O
answering O
model O
. O
SpeciÔ¨Åcally O
, O
we O
exploit O
the O
synset O
and O
hypernym O
concepts O
to O
enrich O
the O
word O
representation O
and O
incorporate O
the O
similarity O
scores O
of O
two O
concepts O
that O
share O
the O
synset O
or O
hypernym O
relations O
into O
the O
attention O
mechanism O
. O
The O
proposed O
WordNet O
- O
enhanced O
hierarchical O
model O
( O
WEHM O
) O
consists O
of O
four O
modules O
, O
including O
WordNet O
- O
enhanced O
word O
representation O
, O
sentence O
encoding O
, O
WordNetenhanced O
attention O
mechanism O
, O
and O
hierarchical O
document O
encoding O
. O
Extensive O
experiments O
on O
the O
public O
WikiQA O
and O
SelQA O
datasets O
demonstrate O
that O
our O
proposed O
model O
signiÔ¨Åcantly O
improves O
the O
baseline O
system O
and O
outperforms O
all O
existing O
state O
- O
of O
- O
the O
- O
art O
methods O
by O
a O
large O
margin O
. O
1 O
Introduction O
Answer O
selection O
( O
AS O
) O
is O
a O
challenging O
subtask O
of O
document O
- O
based O
question O
answering O
( O
DQA O
) O
in O
natural O
language O
processing O
( O
NLP O
) O
. O
The O
AS O
task O
is O
to O
select O
a O
whole O
answer O
sentence O
from O
the O
document O
and O
can O
be O
regarded O
as O
a O
ranking O
problem O
, O
which O
is O
different O
from O
the O
machine O
reading O
comprehension O
( O
MRC O
) O
task O
on O
the O
SQuAD O
and O
MS O
- O
MARCO O
datasets O
. O
Compared O
with O
a O
single O
word O
or O
phrase O
, O
returning O
the O
full O
sentence O
often O
adds O
more O
value O
as O
the O
user O
can O
easily O
verify O
the O
correctness O
without O
reading O
a O
lengthy O
document O
( O
Yih O
et O
al O
. O
, O
2013 O
) O
. O
In O
‚àóCorresponding O
author.this O
paper O
, O
we O
focus O
on O
the O
AS O
task O
of O
DQA O
. O
Table O
1 O
gives O
a O
real O
example O
of O
this O
task O
. O
Lots O
of O
fruits O
on O
answer O
selection O
have O
been O
achieved O
via O
deep O
learning O
models O
, O
including O
convolutional O
neural O
network O
( O
CNN O
) O
( O
Yang O
et O
al O
. O
, O
2015 O
) O
, O
recurrent O
neural O
network O
( O
RNN O
) O
( O
Tan O
et O
al O
. O
, O
2015 O
) O
, O
attention O
- O
way O
( O
Wang O
et O
al O
. O
, O
2016 O
) O
and O
generative O
adversarial O
networks O
( O
GAN O
) O
( O
Wang O
et O
al O
. O
, O
2017a O
) O
. O
Recently O
proposed O
models O
often O
consist O
of O
an O
embedding O
layer O
, O
an O
encoding O
layer O
, O
an O
interaction O
layer O
, O
and O
an O
answer O
layer O
( O
Weissenborn O
et O
al O
. O
, O
2017 O
; O
Wang O
et O
al O
. O
, O
2017b O
; O
Hewlett O
et O
al O
. O
, O
2017 O
) O
. O
Different O
from O
other O
question O
answering O
like O
community O
- O
based O
question O
answering O
, O
the O
candidate O
answers O
of O
DQA O
come O
from O
the O
same O
document O
, O
and O
each O
candidate O
answer O
is O
semantically O
related O
to O
the O
question O
. O
From O
the O
example O
in O
Table O
1 O
, O
we O
can O
see O
that O
almost O
every O
candidate O
answer O
contains O
the O
information O
related O
to O
the O
word O
‚Äú O
food O
‚Äù O
and O
‚Äú O
afghan O
‚Äù O
in O
the O
given O
question O
. O
As O
a O
result O
, O
it O
is O
difÔ¨Åcult O
for O
the O
existing O
network O
- O
based O
models O
to O
choose O
the O
right O
answer O
, O
since O
the O
power O
generation O
ability O
of O
the O
networks O
may O
have O
transformed O
the O
sentences O
into O
similar O
meanings O
in O
the O
latent O
space O
. O
To O
tackle O
this O
challenge O
, O
we O
propose O
to O
leverage O
WordNet O
knowledge O
base O
into O
the O
neural O
network O
model O
. O
Our O
hypothesis O
is O
that O
the O
ability O
of O
paraphrase O
and O
reasoning O
is O
essential O
to O
the O
questionanswering O
task O
. O
WordNet O
is O
a O
semantic O
network O
( O
Fellbaum O
, O
1998 O
) O
, O
where O
the O
words O
that O
are O
related O
in O
meanings O
are O
interlinked O
by O
means O
of O
pointers O
, O
which O
stand O
for O
different O
semantic O
relations O
. O
It O
organizes O
concepts O
mainly O
with O
the O
is O
- O
a O
relation O
, O
where O
a O
concept O
is O
a O
set O
of O
word O
senses O
( O
synset O
) O
. O
On O
the O
one O
hand O
, O
we O
apply O
the O
synset O
information O
to O
enrich O
the O
sentence O
‚Äôs O
paraphrase O
representation O
, O
which O
could O
distinguish O
the O
candidate O
answers O
in O
the O
latent O
semantic O
space O
to O
some O
degree O
. O
On O
the O
other O
hand O
, O
we O
apply O
the O
hypernym O
information O
to O
capture O
reasoning O
knowledge O
. O
The O
real O
case106Question O
: O
what O
food O
is O
in O
afghan O
? O
Document O
: O
[ O
1]A O
table O
setting O
of O
Afghan O
food O
in O
Kabul O
. O
[ O
2]Afghan O
cuisine O
is O
largely O
based O
upon O
the O
nation O
‚Äôs O
chief O
crops O
; O
cereals O
like O
wheat O
, O
maize O
, O
barley O
and O
rice O
. O
[ O
3 O
] O
...... O
[ O
4]Afghanistan O
‚Äôs O
culinary O
specialties O
reÔ¨Çect O
its O
ethnic O
and O
geographic O
diversity O
. O
[ O
5]Though O
it O
has O
similarities O
with O
neighboring O
countries O
, O
Afghan O
cuisine O
is O
undeniably O
unique O
. O
[ O
6 O
] O
...... O
Reference O
Answer O
: O
Afghan O
cuisine O
is O
largely O
based O
upon O
the O
nation O
‚Äôs O
chief O
crops O
; O
cereals O
like O
wheat O
, O
maize O
, O
barley O
and O
rice O
. O
Table O
1 O
: O
An O
example O
from O
the O
WikiQA O
data O
. O
The O
text O
is O
shown O
in O
its O
original O
form O
, O
which O
may O
contain O
errors O
in O
typing O
. O
from O
the O
WikiQA O
dataset O
in O
table O
1 O
shows O
that O
if O
our O
model O
has O
the O
ability O
of O
reasoning O
on O
common O
sense O
, O
like O
‚Äú O
wheat O
is O
a O
kind O
of O
food O
‚Äù O
, O
‚Äú O
maize O
is O
a O
kind O
of O
food O
‚Äù O
and O
so O
on O
, O
it O
would O
be O
of O
great O
help O
for O
choosing O
the O
right O
answer O
with O
respect O
to O
the O
question O
‚Äú O
what O
food O
is O
in O
afghan O
? O
‚Äù O
. O
The O
overall O
framework O
of O
our O
proposed O
model O
is O
shown O
in O
Figure O
1 O
, O
which O
mainly O
consists O
of O
four O
modules O
. O
First O
, O
we O
apply O
the O
synset O
and O
hypernym O
information O
to O
enrich O
the O
word O
representation O
. O
Second O
, O
we O
use O
an O
RNN O
module O
to O
encode O
the O
WordNet O
- O
enhanced O
word O
representation O
. O
Third O
, O
we O
propose O
to O
use O
the O
synset O
‚Äôs O
and O
hypernym O
‚Äôs O
relation O
score O
based O
on O
two O
senses O
‚Äô O
path O
in O
the O
WordNet O
to O
enrich O
the O
attention O
mechanism O
. O
SpeciÔ¨Åcally O
, O
the O
attention O
similarity O
matrix O
is O
not O
only O
measured O
by O
a O
similarity O
score O
over O
hidden O
vectors O
produced O
by O
CNN O
or O
RNN O
networks O
but O
also O
measured O
based O
on O
the O
synset O
and O
hypernym O
relation O
scores O
of O
two O
concepts O
in O
Wordnet O
. O
And O
then O
following O
the O
compareaggregate O
framework O
( O
Wang O
and O
Jiang O
, O
2016 O
) O
, O
we O
combine O
the O
original O
representation O
with O
the O
attention O
representation O
. O
Finally O
, O
considering O
the O
strong O
relations O
among O
context O
sentences O
, O
we O
employ O
a O
hierarchical O
neural O
network O
for O
answer O
sentence O
selection O
. O
We O
conduct O
extensive O
experiments O
on O
the O
public O
WikiQA O
and O
SelQA O
datasets O
. O
The O
results O
show O
that O
our O
proposed O
WordNet O
- O
enhanced O
hierarchical O
model O
outperforms O
the O
baseline O
models O
by O
a O
large O
margin O
and O
achieves O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
both O
datasets O
. O
On O
the O
WikiQA O
data O
, O
it O
obtains O
a O
MAP O
of O
77.02 O
, O
which O
beats O
the O
existing O
best O
result O
by O
1.62 O
points O
; O
on O
the O
SelQA O
data O
, O
it O
achieves O
a O
MAP O
of O
91.71 O
, O
which O
outperforms O
the O
previous O
best O
result O
by O
2.57 O
points O
. O
2 O
Model O
Description O
Given O
a O
question O
qand O
the O
sentences O
ai O
, O
i= O
1,2, O
... O
,S O
in O
a O
document O
d O
, O
our O
model O
aimsto O
select O
the O
best O
sentence O
which O
could O
answer O
the O
question O
. O
2.1 O
WordNet O
- O
Enhanced O
Word O
Representation O
Firstly O
, O
we O
map O
each O
word O
into O
the O
vector O
space O
. O
Different O
from O
directly O
using O
word O
embedding O
or O
the O
concatenation O
of O
word O
embedding O
and O
sum O
of O
its O
character O
embeddings O
, O
we O
propose O
to O
exploit O
the O
word O
‚Äôs O
hypernym O
and O
synset O
in O
the O
WorNet O
to O
enrich O
the O
word O
representation O
. O
Suppose O
wjis O
the O
jthword O
in O
a O
sequence O
, O
ksjandkhjrepresent O
the O
hypernym O
and O
synset O
in O
the O
WordNet O
with O
respect O
to O
the O
wordwj O
. O
The O
WordNet O
- O
enhanced O
word O
embedding O
is O
computed O
as O
follows O
: O
kj= O
[ O
wj;ksj;khj O
] O
( O
1 O
) O
ksj=1 O
|S|/summationdisplay|S| O
i=1wksj O
i O
( O
2 O
) O
khj=1 O
|H|/summationdisplay|H| O
i=1wkhj O
i O
( O
3 O
) O
wherewksj O
iandwkhj O
irepresent O
word O
embeddings O
in O
the O
synset O
and O
hypernym O
concepts O
respectively O
; O
|S|and|H|denote O
the O
number O
of O
concepts O
in O
the O
synset O
and O
hypernym O
respectively O
. O
And O
; O
means O
the O
concatenation O
operation O
. O
We O
usekq O
jandkai O
jto O
represent O
the O
jthword O
‚Äôs O
WordNet O
- O
enhanced O
embedding O
of O
the O
question O
and O
theithcandidate O
answer O
sentence O
respectively O
. O
2.2 O
Sentcene O
Encoding O
We O
encode O
the O
question O
and O
each O
sentence O
in O
the O
document O
into O
latent O
vectors O
using O
a O
Bi O
- O
directional O
Gated O
Recurrent O
Unit O
( O
Bi O
- O
GRU O
) O
network O
. O
The O
formulas O
of O
a O
GRU O
( O
Cho O
et O
al O
. O
, O
2014 O
) O
are O
as O
follows O
: O
rj O
= O
œÉ(Wrkj+Urhj‚àí1+br O
) O
( O
4 O
) O
zj O
= O
œÉ(Wzkj+Uzhj‚àí1+bz O
) O
( O
5)107!"#$%&"#$soorr%'"#$%"#$!(#$%&(#$%'(#$%(#$!)#$%&)#$%')#$%)#$ O
‚Ä¶ O
... O
‚Ä¶ O
... O
‚Ä¶ O
... O
‚Ä¶ O
... O
!"*%&"*%'"*%"*!(*%&(*%'(*%(*!+*%&+*%'+*%+* O
‚Ä¶ O
... O
‚Ä¶ O
... O
‚Ä¶ O
... O
‚Ä¶ O
... O
WordNet O
- O
EnhancedWordRepresentationWordNet O
- O
EnhancedAttentionMechanismSentenceEncoding!,#$%&,#$%',#$%,#$-."#$-.(#$-.)#$ O
‚Ä¶ O
... O
-.,#$'+/0+/1HierarchicalDocumentEncoding2#32#$2#4softmax O
wordnetAfghancuisineis O
‚Ä¶ O
‚Ä¶ O
riceQuestionCandidateAnswerWhatfood O
‚Ä¶ O
‚Ä¶ O
afghanFigure O
1 O
: O
Framework O
of O
our O
proposed O
WordNet O
- O
enhanced O
hierarchical O
model O
( O
WEHM O
) O
. O
/tildewidehj= O
tanh(Whkj+Uh(rj‚äôhj‚àí1 O
) O
+ O
bh)(6 O
) O
hj= O
( O
1‚àízj)‚äôhj‚àí1+zj‚äô/tildewidehj O
( O
7 O
) O
where‚äôis O
element O
- O
wise O
multiplication O
. O
rjandzj O
are O
the O
reset O
and O
update O
gates O
respectively O
. O
And O
Wr O
, O
Wz O
, O
Wh‚ààRH√óE O
, O
Ur O
, O
Uz O
, O
Uh‚ààRH√óHand O
br O
, O
bz O
, O
bh‚ààRH√ó1are O
parameters O
to O
be O
learned O
. O
A O
Bi O
- O
GRU O
processes O
the O
sequence O
in O
both O
forward O
and O
backward O
directions O
to O
produce O
two O
sequences O
[ O
hf O
1,hf O
2 O
, O
... O
, O
hf O
S O
] O
and O
[ O
hb O
1,hb O
2 O
, O
... O
, O
hb O
S O
] O
. O
The O
Ô¨Ånal O
output O
ofhjis O
the O
concatenation O
of O
hf O
jandhb O
j. O
We O
usehq O
jandhai O
jto O
represent O
jthword O
‚Äôs O
hidden O
vector O
produced O
by O
sentence O
encoding O
in O
the O
question O
and O
in O
the O
ithcandidate O
answer O
sentence O
respectively O
. O
2.3 O
WordNet O
- O
Enhanced O
Attention O
Mechanism O
Different O
from O
the O
vanilla O
attention O
mechanism O
, O
where O
the O
attention O
score O
is O
only O
measured O
by O
hidden O
vectors O
, O
we O
propose O
to O
employ O
the O
synset O
and O
hypernym O
relation O
scores O
of O
two O
concepts O
in O
WordNet O
to O
enhance O
the O
attention O
mechanism O
, O
which O
can O
capture O
more O
rich O
interaction O
information O
between O
two O
sequences O
. O
The O
sketch O
of O
our O
proposed O
WordNet O
- O
enhanced O
attention O
mechanism O
is O
shown O
in O
Figure O
2 O
, O
which O
consists O
of O
three O
parts O
: O
the O
standard O
attention O
score O
, O
the O
synset O
relation O
score O
, O
and O
the O
hypernym O
relation O
score O
. O
As O
for O
the O
standard O
attention O
mechanism O
, O
we O
adopt O
the O
Luong O
attention O
( O
also O
known O
as O
bilinear O
function O
attention O
mechanism O
) O
( O
Luong O
et O
al O
. O
, O
2015 O
) O
, O
which O
is O
widely O
used O
in O
NLP O
. O
In O
our O
model O
, O
Mh O
|ai|,|q| O
represents O
the O
attention O
score O
between O
the O
question O
and O
one O
of O
its O
candidate O
answers O
. O
The O
formulas O
of O
computing O
each O
element O
are O
as O
follows O
: O
Mh O
n O
, O
m O
= O
hainWhq O
mT(8 O
) O
Mh O
n O
, O
m= O
exp(Mh O
n O
, O
m)//summationdisplay|q| O
k=1exp(Mh O
n O
, O
k)(9 O
) O
wherehainandhq O
mrepresent O
the O
nthandmthword O
hidden O
vector O
in O
the O
candidate O
answer O
and O
the O
question O
respectively O
, O
|ai|and|q|are O
the O
candidate O
answer O
‚Äôs O
length O
and O
the O
question O
‚Äôs O
length O
respectively O
. O
Besides O
the O
standard O
attention O
, O
we O
employ O
two O
kinds O
of O
WordNet O
- O
enhanced O
mechanism O
to O
measure O
the O
attention O
score O
. O
Lots O
of O
studies O
have O
been O
done O
on O
computing O
lexical O
similarity O
based O
on O
WordNet O
( O
Pedersen O
et O
al O
. O
, O
2004 O
) O
. O
Wu O
- O
Palmer O
Similarity O
( O
Wu O
and O
Palmer O
, O
1994 O
) O
denotes O
how O
similar O
two O
words O
senses O
are O
, O
based O
on O
the O
depth O
of O
the O
two O
senses O
in O
the O
taxonomy O
and O
that O
of O
their O
Least O
Common O
Subsumer O
. O
Leacock O
- O
Chodorow O
Similarity O
( O
Leacock O
and O
Chodorow O
, O
1998 O
) O
denotes O
how O
similar O
twoword O
senses O
are O
, O
based O
on O
the O
shortest O
path O
that O
connects O
the O
senses O
in O
the O
is O
- O
a O
( O
hypernym O
/ O
hyponym O
) O
taxonomy.108QueryAttentionValue!"#$%&'()*"$!"#$%+!"#$,'()*"-'()*". O
‚Ä¶ O
‚Ä¶ O
!"#-%&!"#-%+!"#-,!"#.%&!"#.%+!"#.,Figure O
2 O
: O
Sketch O
of O
our O
proposed O
WordNet O
- O
enhanced O
attention O
mechanism O
. O
Keyh O
jmeans O
the O
attention O
score O
derived O
by O
two O
hidden O
vectors O
. O
Keyks O
jandKeykh O
jrepresent O
the O
attention O
score O
derived O
by O
synset O
relation O
and O
hypernym O
relation O
respectively O
. O
Vlaue O
jmeans O
the O
hidden O
vector O
of O
question O
, O
and O
Query O
means O
the O
candidate O
answer O
. O
We O
use O
Wu O
- O
Palmer O
Similarity O
to O
compute O
the O
attention O
score O
with O
the O
synset O
relation O
. O
Mks O
|ai||q| O
represents O
the O
attention O
matrix O
between O
the O
question O
and O
one O
of O
its O
candidate O
answers O
, O
where O
each O
elementMksn O
, O
mis O
computed O
as O
: O
Mksn O
, O
m= O
2‚àóNc/(Nain+Nqm+ O
2‚àóNc)(10 O
) O
Mksn O
, O
m= O
exp(Mksn O
, O
m)//summationdisplay|q| O
k=1exp(Mks O
n O
, O
k)(11 O
) O
whereai O
nandqmrepresent O
the O
corresponding O
concepts O
of O
the O
nth O
word O
of O
the O
ith O
candidate O
answer O
and O
themth O
word O
of O
the O
question O
respectively O
, O
cis O
the O
least O
common O
superconcept O
of O
ai O
nandqm O
, O
Nain O
is O
the O
number O
of O
nodes O
on O
the O
path O
from O
ai O
ntoc O
, O
Nqmis O
the O
number O
of O
nodes O
on O
the O
path O
from O
Nqm O
toc O
, O
Ncis O
the O
number O
of O
nodes O
on O
the O
path O
from O
c O
to O
root O
. O
We O
use O
Leacock O
- O
Chodorow O
Similarity O
to O
measure O
the O
attention O
score O
with O
hypernym O
relation O
. O
LetMkh O
|ai||q|denote O
the O
attention O
matrix O
between O
the O
question O
and O
one O
of O
its O
candidate O
answers O
, O
where O
each O
element O
Mkhn O
, O
mcan O
be O
computed O
as O
: O
Mkhn O
, O
m=‚àílog(path(ai O
n O
, O
qm)/2L O
) O
( O
12 O
) O
Mkhn O
, O
m= O
exp(Mkhn O
, O
m)//summationdisplay|q| O
k=1exp(Mkhn O
, O
m)(13 O
) O
wherepath(ai O
n O
, O
qm)is O
the O
shortest O
path O
length O
connecting O
two O
concepts O
and O
Lis O
the O
whole O
taxonomy O
depth O
. O
Finally O
, O
we O
combine O
all O
the O
three O
similarity O
matrixes O
. O
The O
formulas O
are O
as O
follows O
: O
Mn O
, O
m O
= O
Mh O
n O
, O
m+Mksn O
, O
m+Mkhn O
, O
m O
( O
14 O
) O
Mn O
, O
m= O
exp(Mn O
, O
m)//summationdisplay|q| O
k=1exp(Mn O
, O
k)(15)Equipped O
with O
the O
WordNet O
- O
enhanced O
similarity O
matrixM O
, O
we O
apply O
the O
attention O
mechanism O
between O
the O
question O
encoding O
hqand O
the O
sentence O
encodinghaito O
obtain O
a O
new O
sentence O
representationvai O
, O
which O
is O
a O
weighted O
sum O
of O
hidden O
vectors O
of O
the O
question O
. O
We O
then O
aggregate O
the O
vectors O
of O
haiandvai O
. O
Formulas O
are O
as O
follows O
: O
vai O
= O
M¬∑hq(16 O
) O
ÀÜvai= O
[ O
vai;hai;vai‚äôhai;vai+hai;vai‚àíhai O
] O
( O
17 O
) O
where O
; O
is O
the O
concatenation O
operation O
, O
+ O
is O
element O
- O
wise O
addition O
, O
‚àíis O
element O
- O
wise O
subtraction O
and‚äôis O
element O
- O
wise O
multiplication O
. O
2.4 O
Hierarchical O
Document O
Encoding O
Inspired O
by O
the O
work O
( O
Bian O
et O
al O
. O
, O
2017 O
) O
, O
we O
also O
adopt O
a O
list O
- O
wise O
method O
to O
model O
the O
answer O
selection O
task O
. O
But O
different O
from O
their O
model O
, O
we O
employ O
a O
hierarchical O
Bi O
- O
GRU O
architecture O
to O
compare O
candidate O
sentences O
by O
ranking O
them O
with O
respect O
to O
a O
given O
question O
. O
Considering O
that O
candidate O
answers O
all O
come O
from O
a O
whole O
document O
, O
the O
hierarchical O
Bi O
- O
GRU O
architecture O
can O
capture O
contextual O
features O
among O
sentences O
and O
make O
the O
understanding O
of O
a O
document O
more O
coherent O
. O
We O
Ô¨Årst O
encode O
each O
candidate O
answer O
ÀÜvaiand O
then O
extract O
features O
among O
sentences O
‚Äô O
hidden O
vectors O
. O
Then O
we O
again O
encode O
the O
document O
based O
on O
each O
candidate O
answer O
‚Äôs O
extracted O
features O
. O
The O
BiGRU O
is O
the O
same O
to O
that O
mentioned O
in O
our O
sentence O
encoding O
section O
. O
uai O
j O
= O
BiGRU O
/ O
parenleftBig O
uai O
j‚àí1,ÀÜvai O
j O
/ O
parenrightBig O
( O
18 O
) O
uaiavg=1 O
|ai|/summationdisplay|ai| O
j=1uai O
j O
, O
uaimax=|ai|max O
j=1uai O
j(19 O
) O
fai=/bracketleftBig O
uaiavg;uaimax O
/ O
bracketrightBig O
( O
20 O
) O
ÀÜud O
i O
= O
BiGRU O
/ O
parenleftBig O
ÀÜud O
i‚àí1,fai O
/ O
parenrightBig O
( O
21 O
) O
wherejis O
thejthword O
in O
the O
ithsentence O
in O
the O
candidate O
answers O
, O
faiis O
theithsentence O
extracted O
features O
and O
ÀÜud O
iis O
theithsentence O
‚Äôs O
hidden O
vector O
after O
the O
document O
encoding O
phase O
. O
At O
last O
, O
we O
use O
a O
softmax O
layer O
to O
choose O
the O
right O
answer O
among O
every O
step O
‚Äôs O
output O
of O
the O
document O
‚Äôs O
RNN O
layer O
. O
The O
model O
is O
trained O
to O
minimize O
the O
cross O
- O
entropy O
loss O
function O
: O
Àúai O
= O
œÉ(FC(ÀÜud O
i O
) O
) O
( O
22)109Dataset O
Split O
# O
Questions O
# O
Pairs O
WikiQATRAIN O
873 O
8672 O
DEV O
126 O
1130 O
TEST O
243 O
2351 O
SelQATRAIN O
5529 O
66438 O
DEV O
785 O
9377 O
TEST O
1590 O
19435 O
Table O
2 O
: O
Statistical O
distribution O
of O
two O
benchmark O
datasets O
. O
C=‚àí1 O
|d|/summationdisplay O
i‚àà|d|[ailog O
Àúai+ O
( O
1‚àíai O
) O
log O
( O
1‚àíÀúai O
) O
] O
( O
23 O
) O
whereFC O
is O
a O
feed O
- O
forward O
neural O
network O
, O
i O
means O
the O
sentence O
index O
in O
the O
document O
, O
|d|is O
the O
document O
‚Äôs O
length O
in O
terms O
of O
sentences O
, O
aiis O
the O
true O
label O
( O
0 O
or O
1 O
) O
from O
the O
training O
data O
and O
Àúaiis O
the O
predicted O
probability O
score O
by O
our O
model O
. O
The O
sentence O
with O
the O
highest O
probability O
score O
is O
regarded O
as O
the O
right O
answer O
. O
3 O
Experiments O
3.1 O
Datasets O
and O
Baselines O
We O
use O
two O
different O
datasets O
to O
conduct O
our O
answer O
selection O
experiments O
: O
WikiQA O
( O
Yang O
et O
al O
. O
, O
2015 O
) O
and O
SelQA O
( O
Jurczyk O
et O
al O
. O
, O
2016 O
) O
. O
Both O
datasets O
contain O
open O
- O
domain O
questions O
whose O
answers O
were O
extracted O
from O
Wikipedia O
articles O
. O
In O
the O
AS O
task O
, O
it O
is O
assumed O
that O
there O
is O
at O
least O
one O
correct O
answer O
for O
a O
question O
. O
In O
the O
WikiQA O
, O
there O
are O
some O
questions O
which O
have O
no O
answer O
, O
we O
removed O
these O
questions O
, O
just O
like O
other O
researches O
do O
. O
Table O
2 O
shows O
the O
statistical O
distribution O
of O
the O
two O
datasets O
. O
As O
for O
the O
WikiQA O
dataset O
, O
it O
has O
been O
well O
studied O
by O
lots O
of O
literature O
. O
Baselines O
adopted O
are O
as O
follows O
: O
‚Ä¢CNN O
- O
Cnt O
: O
this O
model O
combines O
sentence O
representations O
produced O
by O
a O
convolutional O
neural O
network O
with O
the O
logistic O
regression O
( O
Yang O
et O
al O
. O
, O
2015 O
) O
. O
‚Ä¢ABCNN O
: O
this O
model O
is O
an O
attention O
- O
based O
convolutional O
neural O
network O
( O
Yin O
et O
al O
. O
, O
2015 O
) O
. O
‚Ä¢IARNN O
- O
Occam O
: O
this O
model O
adds O
regularization O
on O
the O
attention O
weights O
( O
Wang O
et O
al O
. O
, O
2016).‚Ä¢IARNN O
- O
Gate O
: O
this O
model O
uses O
the O
question O
representation O
to O
build O
GRU O
gates O
for O
each O
candidate O
answer O
( O
Wang O
et O
al O
. O
, O
2016 O
) O
. O
‚Ä¢CubeCNN O
: O
this O
model O
builds O
a O
CNN O
on O
all O
pairs O
of O
word O
similarities O
( O
He O
and O
Lin O
, O
2016 O
) O
. O
‚Ä¢CA O
- O
Network O
: O
this O
model O
applies O
a O
compareaggregate O
neural O
network O
to O
model O
question O
answering O
problem O
( O
Wang O
and O
Jiang O
, O
2016 O
) O
. O
‚Ä¢IWAN O
- O
Skip O
: O
this O
model O
measures O
the O
similarity O
of O
sentence O
pairs O
by O
focusing O
on O
the O
interaction O
information O
( O
Shen O
et O
al O
. O
, O
2017b O
) O
. O
‚Ä¢Dynamic O
- O
Clip O
: O
this O
model O
proposes O
a O
novel O
attention O
mechanism O
named O
DynamicClip O
Attention O
, O
which O
is O
then O
directly O
integrated O
into O
the O
Compare O
- O
Aggregate O
framework O
. O
( O
Bian O
et O
al O
. O
, O
2017 O
) O
. O
As O
for O
the O
SelQA O
dataset O
, O
besides O
the O
above O
mentioned O
CNN O
- O
Cnt O
model O
, O
Jurczyk O
et O
al O
. O
( O
2016 O
) O
also O
re O
- O
implement O
CNN O
- O
Tree O
and O
two O
attention O
RNN O
models O
. O
Other O
baselines O
are O
as O
follows O
: O
‚Ä¢CNN O
- O
hinge O
: O
this O
is O
a O
re O
- O
implemented O
CNNbased O
model O
with O
hinge O
loss O
function O
( O
dos O
Santos O
et O
al O
. O
, O
2017 O
) O
. O
‚Ä¢CNN O
- O
DAN O
: O
dos O
Santos O
et O
al O
. O
( O
2017 O
) O
propose O
a O
CNN O
- O
based O
model O
trained O
with O
a O
DAN O
framework O
, O
which O
is O
to O
learn O
loss O
functions O
for O
predictors O
and O
also O
implements O
semisupervised O
learning O
. O
‚Ä¢AdaQA O
: O
Shen O
et O
al O
. O
( O
2017a O
) O
propose O
an O
adaptive O
question O
answering O
( O
AdaQA O
) O
model O
, O
which O
consists O
of O
a O
novel O
two O
- O
way O
feature O
abstraction O
mechanism O
to O
encapsulate O
codependent O
sentence O
representations O
. O
The O
answer O
selection O
task O
can O
be O
considered O
as O
a O
ranking O
problem O
, O
and O
so O
two O
evaluation O
metrics O
are O
used O
: O
mean O
average O
precision O
( O
MAP O
) O
and O
mean O
reciprocal O
rank O
( O
MRR O
) O
. O
3.2 O
Experiment O
Setup O
The O
proposed O
models O
are O
implemented O
with O
TensorFlow O
. O
The O
dimension O
of O
word O
embeddings O
is O
set O
to O
300 O
. O
The O
word O
embeddings O
are O
initialized O
by O
300D O
GloVe O
840B O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
, O
and O
out O
- O
of O
- O
vocabulary O
words O
are O
initialized O
randomly O
. O
We O
Ô¨Åx O
the O
embeddings O
during O
training O
. O
We O
train O
the O
model O
with O
the O
Adam O
optimization O
algorithm O
with110Model O
MAP O
MRR O
CNN O
- O
Cnt O
( O
Yang O
et O
al O
. O
, O
2015 O
) O
65.20 O
66.52 O
ABCNN O
( O
Yin O
et O
al O
. O
, O
2015 O
) O
69.21 O
71.08 O
CubeCNN O
( O
He O
and O
Lin O
, O
2016 O
) O
70.90 O
72.34 O
IARNN O
- O
Gate O
( O
Wang O
et O
al O
. O
, O
2016 O
) O
72.58 O
73.94 O
IARNN O
- O
Occam O
( O
Wang O
et O
al O
. O
, O
2016 O
) O
73.41 O
74.18 O
CA O
- O
Network O
( O
Wang O
and O
Jiang O
, O
2016 O
) O
74.33 O
75.45 O
IWAN O
- O
Skip O
( O
Shen O
et O
al O
. O
, O
2017b O
) O
73.30 O
75.00 O
Dynamic O
- O
Clip O
( O
Bian O
et O
al O
. O
, O
2017 O
) O
75.40 O
76.40 O
WEHM O
( O
Proposed O
) O
77.02 O
78.82 O
Table O
3 O
: O
Experimental O
results O
on O
the O
WikiQA O
dataset O
Model O
MAP O
MRR O
CNN O
- O
Cnt O
( O
Jurczyk O
et O
al O
. O
, O
2016 O
) O
84.00 O
84.94 O
CNN O
- O
Tree O
( O
Jurczyk O
et O
al O
. O
, O
2016 O
) O
84.66 O
85.68 O
RNN O
: O
one O
- O
way O
( O
Jurczyk O
et O
al O
. O
, O
2016 O
) O
82.06 O
83.18 O
RNN O
: O
attn O
- O
pool O
( O
Jurczyk O
et O
al O
. O
, O
2016 O
) O
86.43 O
87.59 O
CNN O
- O
DAN O
( O
dos O
Santos O
et O
al O
. O
, O
2017 O
) O
86.55 O
87.30 O
CNN O
- O
hinge O
( O
dos O
Santos O
et O
al O
. O
, O
2017 O
) O
87.58 O
88.12 O
AdaQA O
( O
Shen O
et O
al O
. O
, O
2017a O
) O
89.14 O
89.83 O
WEHM O
( O
Proposed O
) O
91.71 O
92.22 O
Table O
4 O
: O
Experimental O
results O
on O
the O
SelQA O
dataset O
a O
learning O
rate O
of O
0.001 O
. O
Our O
models O
are O
trained O
in O
mini O
- O
batches O
( O
with O
a O
batch O
size O
of O
10 O
) O
. O
We O
Ô¨Åx O
the O
length O
of O
the O
question O
and O
each O
sentence O
in O
the O
document O
according O
to O
their O
sentence O
‚Äôs O
max O
length O
in O
each O
mini O
- O
batch O
, O
and O
any O
sentences O
not O
enough O
to O
this O
range O
are O
padded O
. O
The O
hidden O
vector O
size O
is O
set O
to O
150 O
for O
a O
single O
RNN O
. O
We O
conduct O
word O
sense O
disambiguation O
for O
ambiguous O
words O
via O
the O
nltk O
tool O
. O
3.3 O
Results O
and O
Analysis O
3.3.1 O
Performance O
We O
compare O
our O
model O
with O
state O
- O
of O
- O
the O
- O
art O
methods O
on O
the O
WikiQA O
and O
SelQA O
dataset O
in O
Table O
3 O
and O
Table O
4 O
, O
respectively O
. O
Our O
proposed O
model O
not O
only O
obtains O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
two O
datasets O
but O
also O
makes O
a O
signiÔ¨Åcant O
improvement O
. O
Compared O
with O
the O
existing O
best O
method O
DynamicClip O
, O
our O
model O
yields O
nearly O
1.6 O
% O
improvement O
in O
MAP O
and O
2.4 O
% O
in O
MRR O
on O
the O
WikiQA O
dataset O
. O
On O
the O
SelQA O
dataset O
, O
our O
model O
improves O
2.6 O
% O
in O
MAP O
and O
2.4 O
% O
in O
MRR O
, O
compared O
with O
the O
previous O
best O
method O
AdaQA O
. O
It O
is O
a O
challenging O
task O
for O
answer O
selection O
, O
especially O
for O
the O
WikiQA O
dataset O
. O
As O
is O
shown O
in O
Table O
3 O
, O
the O
notable O
CA O
- O
network O
outperforms O
the O
IARNN O
- O
Occam O
approach O
only O
by O
0.92 O
MAP O
points O
, O
and O
our O
best O
result O
( O
77.02 O
) O
achieves O
a O
performance O
gain O
of O
1.6 O
MAP O
points O
over O
the O
Dynamic O
- O
Clip O
. O
In O
this O
sense O
, O
the O
improvement O
of O
our O
model O
is O
valuable O
. O
Model O
MAP O
‚àÜ/% O
without O
WordNet O
knowledge O
84.87 O
( O
1 O
) O
only O
hypernym O
token O
85.35 O
0.48 O
( O
2 O
) O
only O
synset O
token O
85.17 O
0.30 O
( O
3 O
) O
only O
hypernym&synset O
token O
86.32 O
1.45 O
( O
4 O
) O
only O
hypernym O
attention O
90.21 O
5.34 O
( O
5 O
) O
only O
synset O
attention O
89.99 O
5.12 O
( O
6 O
) O
only O
hypernym&synset O
attention O
90.49 O
5.62 O
WEHM O
91.71 O
6.84 O
Table O
5 O
: O
Ablation O
study O
on O
the O
SelQA O
dataset O
3.3.2 O
Ablation O
Study O
We O
further O
conduct O
an O
ablation O
study O
to O
explore O
different O
WordNet O
- O
enhanced O
components O
in O
our O
model O
, O
including O
WordNet O
- O
enhanced O
word O
embedding O
and O
WordNet O
- O
enhanced O
Attention O
Mechanism O
. O
Table O
5 O
reports O
the O
experimental O
results O
. O
We O
Ô¨Årst O
remove O
all O
knowledge O
components O
from O
our O
model O
, O
denoted O
as O
without O
WordNet O
knowledge O
, O
which O
can O
be O
regarded O
as O
the O
baseline O
model O
. O
In O
the O
baseline O
model O
, O
we O
only O
use O
the O
original O
word O
embeddings O
and O
the O
conventional O
Luong O
attention O
mechanism O
. O
Then O
we O
evaluate O
the O
WordNetenhanced O
word O
embedding O
by O
adding O
the O
hypernym O
, O
synset O
, O
and O
the O
combination O
of O
both O
to O
the O
word O
embeddings O
, O
shown O
in O
( O
1)-(3 O
) O
of O
Table O
5 O
. O
To O
evaluate O
the O
WordNet O
- O
enhanced O
attention O
mechanism O
, O
we O
also O
add O
the O
synset O
relation O
score O
, O
the O
hypernym O
score O
or O
its O
combination O
to O
the O
original O
hidden O
vectors O
‚Äô O
score O
based O
on O
the O
baseline O
model O
, O
shown O
in O
( O
4)-(6 O
) O
of O
Table O
5 O
. O
Compared O
with O
the O
baseline O
model O
, O
the O
WordNet O
knowledge O
brings O
consistent O
performance O
gain O
both O
for O
the O
WorNet O
- O
enhanced O
word O
embedding O
and O
WordNet O
- O
enhanced O
attention O
mechanism O
. O
As O
for O
the O
Knowledge O
- O
enhanced O
word O
embedding O
, O
the O
hypernym O
and O
synset O
improve O
0.48 O
% O
and O
0.30 O
% O
in O
MAP O
, O
respectively O
, O
and O
the O
combination O
of O
them O
improves O
1.45 O
% O
in O
MAP O
. O
As O
for O
the O
Knowledgeenhanced O
attention O
mechanism O
, O
the O
hypernym O
and O
synset O
improve O
5.34 O
% O
and O
5.12 O
% O
in O
MAP O
respectively O
, O
and O
the O
combination O
of O
them O
improves O
5.62 O
% O
in O
MAP O
. O
At O
the O
result O
, O
our O
full O
proposed O
model O
WEHM O
yields O
a O
signiÔ¨Åcant O
performance O
gain O
of O
6.84 O
MAP O
points O
. O
We O
could O
Ô¨Ånd O
that O
the O
knowledge O
- O
enhanced O
attention O
mechanism O
is O
more O
effective O
than O
the O
simple O
knowledge O
- O
enriched O
word O
embedding O
, O
perhaps O
because O
computing O
the O
similarity O
scores O
of O
two O
concepts O
takes O
into O
account O
much O
information O
, O
like O
the O
shortest O
path O
between O
them O
and O
the O
depth O
of O
the O
concept O
in O
the O
taxonomy O
. O
Moreover O
, O
the O
combina-111(a)Mvector O
  O
( O
b)Mwordnet O
Figure O
3 O
: O
Attention O
score O
matrixes O
Mvector O
andMwordnet O
of O
a O
real O
case O
on O
the O
WikiQA O
dataset O
. O
The O
matrix O
Mwordnet O
not O
only O
captures O
the O
paraphrase O
information O
like O
‚Äù O
food O
‚Äù O
and O
‚Äô O
cuisine O
‚Äô O
, O
but O
also O
enhances O
relations O
between O
the O
question O
‚Äôs O
word O
‚Äú O
food O
‚Äù O
and O
some O
of O
the O
sentence O
‚Äôs O
words O
, O
like O
‚Äú O
crops O
‚Äù O
, O
‚Äú O
cereals O
‚Äù O
, O
‚Äú O
wheat O
‚Äù O
and O
‚Äú O
rice O
‚Äù O
. O
tion O
of O
hypernym O
and O
synset O
is O
better O
than O
the O
single O
hypernym O
or O
synset O
information O
in O
both O
knowledge O
components O
because O
it O
captures O
more O
diverse O
information O
. O
Interestingly O
, O
the O
hypernym O
information O
is O
more O
effective O
than O
the O
synset O
information O
in O
the O
question O
- O
answering O
task O
. O
3.3.3 O
Case O
Study O
To O
make O
a O
detailed O
analysis O
of O
the O
effectiveness O
of O
our O
proposed O
model O
, O
we O
give O
a O
case O
study O
to O
visualize O
the O
different O
attention O
score O
matrix O
Mvector O
andMwordnet O
, O
by O
a O
heatmap O
in O
Figure O
3 O
. O
Mvector O
is O
only O
computed O
by O
hidden O
vectors O
, O
and O
Mwordnet O
is O
calculated O
by O
our O
proposed O
model O
. O
When O
answering O
the O
question O
, O
our O
proposed O
model O
not O
only O
captures O
the O
information O
of O
‚Äù O
food O
‚Äù O
and O
‚Äù O
afghan O
‚Äù O
, O
but O
also O
pays O
more O
attention O
to O
the O
related O
meaning O
of O
‚Äù O
wheat O
- O
food O
‚Äù O
, O
‚Äù O
rice O
- O
food O
‚Äù O
and O
so O
on O
, O
which O
brings O
vital O
information O
to O
the O
prediction O
, O
while O
the O
baseline O
method O
performs O
weakly O
on O
capturing O
this O
information O
. O
3.3.4 O
Error O
Analysis O
We O
further O
make O
an O
error O
analysis O
of O
our O
model O
for O
further O
improvements O
. O
Table O
6 O
is O
a O
wrong O
prediction O
produced O
by O
our O
proposed O
model O
( O
WEHM O
) O
. O
‚Äù O
Cardiovascular O
disease O
‚Äù O
is O
another O
name O
for O
‚Äù O
heart O
disease O
‚Äù O
. O
However O
, O
‚Äù O
Cardiovascular O
disease O
‚Äù O
is O
n‚Äôt O
mentioned O
in O
the O
given O
question O
. O
Although O
we O
have O
enriched O
the O
model O
with O
WordNet O
knowledge O
, O
it O
is O
still O
hard O
for O
the O
model O
to O
capture O
the O
lexical O
gap O
between O
these O
two O
words O
, O
for O
that O
their O
concepts O
are O
not O
the O
same O
in O
WordNet O
. O
From O
this O
analysis O
, O
we O
‚Äôd O
like O
to O
employ O
more O
Ô¨Åne O
- O
grained O
knowledge O
, O
like O
the O
clariÔ¨Åcation O
for O
proper O
nouns O
. O
3.3.5 O
Comparison O
with O
other O
knowledge O
- O
enhanced O
models O
To O
the O
best O
of O
our O
knowledge O
, O
we O
are O
the O
Ô¨Årst O
to O
explore O
the O
WordNet O
knowledge O
to O
enhance O
theQuestion O
: O
what O
causes O
heart O
disease O
? O
Document O
: O
[ O
1]Cardiovascular O
disease O
( O
also O
called O
heart O
disease O
) O
is O
a O
class O
of O
diseases O
that O
involve O
the O
heart O
or O
blood O
vessels O
( O
arteries O
, O
capillaries O
, O
and O
veins O
) O
. O
[ O
2 O
] O
...... O
[ O
3]The O
causes O
of O
cardiovascular O
disease O
are O
diverse O
but O
atherosclerosis O
and O
hypertension O
are O
the O
most O
common O
. O
[ O
4 O
] O
...... O
Reference O
Answer O
: O
The O
causes O
of O
cardiovascular O
disease O
are O
diverse O
but O
atherosclerosis O
and O
hypertension O
are O
the O
most O
common O
. O
Table O
6 O
: O
The O
error O
prediction O
of O
our O
proposed O
model O
. O
The O
text O
is O
shown O
in O
its O
original O
form O
, O
which O
may O
contain O
errors O
in O
typing O
. O
Our O
proposed O
model O
predict O
the O
Ô¨Årst O
sentence O
is O
the O
right O
answer O
, O
however O
it O
is O
wrong O
. O
neural O
network O
model O
for O
the O
DQA O
problem O
. O
There O
are O
also O
some O
other O
knowledge O
- O
enhanced O
models O
designed O
for O
speciÔ¨Åc O
tasks O
, O
in O
which O
the O
natural O
language O
inference O
( O
NLI O
) O
task O
is O
somewhat O
similar O
to O
the O
QA O
task O
. O
In O
order O
to O
compare O
with O
our O
proposed O
WEHN O
model O
, O
we O
re O
- O
run O
the O
KEM O
model O
on O
the O
WikiQA O
dataset O
by O
using O
its O
public O
codes O
, O
which O
is O
designed O
for O
NLI O
task O
by O
Chen O
et O
al O
. O
( O
2018 O
) O
. O
ESIM O
( O
Chen O
et O
al O
. O
, O
2017 O
) O
is O
the O
basic O
model O
of O
KEM O
without O
knowledge O
. O
KEM O
uses O
feature O
vectors O
of O
speciÔ¨Åc O
dimensions O
in O
WordNet O
, O
while O
our O
WEHM O
model O
directly O
employs O
synset O
and O
hypernym O
relation O
scores O
to O
enrich O
the O
attention O
score O
and O
also O
use O
their O
concepts O
to O
enrich O
the O
word O
representation O
. O
Table O
7 O
shows O
the O
results O
of O
the O
WikiQA O
dataset O
. O
We O
could O
see O
that O
our O
proposed O
model O
outperforms O
the O
KEM O
model O
by O
a O
large O
margin O
. O
Besides O
, O
when O
comparing O
the O
improvements O
produced O
by O
the O
enriched O
knowledge O
, O
our O
proposed O
model O
is O
still O
better O
than O
KEM O
, O
with O
nearly O
4 O
% O
gain O
versus O
about O
3 O
% O
gain O
in O
MAP.112Model O
MAP O
MRR O
ESIM O
( O
Lan O
and O
Xu O
, O
2018 O
) O
65.20 O
66.40 O
KEM O
( O
Chen O
et O
al O
. O
, O
2018 O
) O
68.03 O
69.58 O
WEHM O
( O
without O
knowledge O
) O
73.17 O
74.63 O
WEHM O
( O
Proposed O
) O
77.02 O
78.82 O
Table O
7 O
: O
Experimental O
results O
on O
the O
WikiQA O
dataset O
. O
We O
list O
the O
reported O
results O
of O
ESIM O
in O
the O
paper O
( O
Lan O
and O
Xu O
, O
2018 O
) O
, O
and O
re O
- O
run O
the O
public O
code O
of O
KEM O
proposed O
in O
the O
paper O
( O
Chen O
et O
al O
. O
, O
2018 O
) O
to O
produce O
its O
results O
. O
4 O
Related O
Work O
In O
the O
NLP O
Ô¨Åeld O
, O
many O
problems O
involve O
matching O
two O
or O
more O
sequences O
to O
make O
a O
decision O
. O
For O
the O
DQA O
task O
, O
most O
of O
the O
studies O
also O
consider O
this O
problem O
as O
text O
matching O
, O
and O
they O
compute O
the O
semantic O
similarity O
between O
the O
question O
and O
candidate O
answers O
to O
decide O
whether O
a O
sentence O
in O
the O
document O
could O
answer O
the O
question O
. O
There O
have O
been O
various O
deep O
neural O
network O
models O
proposed O
to O
tackle O
sentence O
pairs O
matching O
. O
Two O
kinds O
of O
matching O
strategies O
have O
been O
considered O
: O
the O
Ô¨Årst O
is O
to O
convert O
the O
whole O
source O
and O
target O
sentences O
into O
embedding O
vectors O
in O
the O
latent O
spaces O
respectively O
, O
and O
then O
calculate O
the O
similarity O
score O
between O
them O
; O
the O
second O
is O
to O
calculate O
the O
similarities O
among O
all O
possible O
local O
positions O
of O
the O
source O
and O
target O
sentences O
and O
then O
summarize O
the O
local O
scores O
into O
the O
Ô¨Ånal O
similarity O
value O
. O
As O
for O
works O
using O
the O
Ô¨Årst O
strategy O
, O
Qiu O
and O
Huang O
( O
2015 O
) O
apply O
a O
tensor O
transformation O
layer O
on O
CNN O
- O
based O
embeddings O
to O
capture O
the O
interactions O
between O
the O
question O
and O
answer O
. O
Tan O
et O
al O
. O
( O
2015 O
) O
employ O
the O
long O
short O
- O
term O
memory O
( O
LSTM O
) O
network O
to O
address O
this O
problem O
. O
In O
the O
second O
strategy O
, O
Pang O
et O
al O
. O
( O
2016 O
) O
build O
hierarchical O
convolution O
layers O
on O
the O
word O
similarity O
matrix O
between O
sentences O
, O
and O
Yin O
and O
Sch O
¬®utze O
( O
2015 O
) O
propose O
MultiGranCNN O
to O
integrate O
multiple O
granularity O
levels O
of O
matching O
models O
. O
For O
the O
DQA O
task O
, O
the O
notable O
work O
is O
the O
compare O
- O
aggregate O
structure O
, O
which O
is O
Ô¨Årst O
proposed O
by O
Wang O
and O
Jiang O
( O
2016 O
) O
. O
Following O
this O
structure O
, O
Bian O
et O
al O
. O
( O
2017 O
) O
propose O
the O
dynamicclip O
way O
to O
compute O
the O
attention O
score O
. O
Our O
basic O
model O
also O
adopts O
this O
structure O
, O
but O
with O
a O
different O
implementation O
. O
What O
‚Äôs O
more O
, O
we O
employ O
a O
hierarchical O
module O
to O
capture O
inter O
- O
sentence O
relations O
. O
Exploiting O
the O
background O
knowledge O
and O
common O
sense O
to O
improve O
NLP O
tasks O
‚Äô O
performancehas O
long O
been O
a O
heated O
research O
topic O
. O
To O
facilitate O
NLP O
tasks O
, O
various O
semantic O
knowledge O
bases O
( O
KBs O
) O
have O
been O
constructed O
, O
ranging O
from O
manually O
annotated O
semantic O
networks O
like O
WordNet O
( O
Fellbaum O
, O
1998 O
) O
to O
semi O
- O
automatically O
or O
automatically O
constructed O
knowledge O
graphs O
like O
Freebase O
( O
Bollacker O
et O
al O
. O
, O
2008 O
) O
. O
Recently O
, O
several O
approaches O
have O
been O
proposed O
to O
leverage O
the O
prior O
knowledge O
in O
neural O
networks O
on O
different O
tasks O
( O
Yang O
and O
Mitchell O
, O
2017 O
; O
Chen O
et O
al O
. O
, O
2018 O
; O
Wu O
et O
al O
. O
, O
2018 O
; O
Wang O
et O
al O
. O
, O
2019 O
) O
. O
Wu O
et O
al O
. O
( O
2018 O
) O
fuse O
the O
prior O
knowledge O
into O
word O
representations O
with O
a O
knowledge O
gate O
by O
using O
question O
categories O
for O
the O
QA O
task O
and O
topics O
for O
the O
conversation O
task O
. O
Yang O
and O
Mitchell O
( O
2017 O
) O
propose O
a O
KBLSTM O
network O
architecture O
, O
which O
incorporates O
the O
background O
knowledge O
into O
LSTM O
to O
improve O
machine O
reading O
. O
Unlike O
the O
two O
approaches O
, O
our O
model O
directly O
employs O
the O
synset O
and O
hypernym O
concepts O
information O
to O
enrich O
the O
word O
representation O
. O
Chen O
et O
al O
. O
( O
2018 O
) O
use O
WordNet O
to O
measure O
the O
semantic O
relatedness O
of O
word O
pairs O
for O
the O
natural O
language O
inference O
task O
, O
including O
synonym O
, O
antonym O
, O
hypernym O
, O
and O
same O
hypernym O
. O
Each O
of O
these O
features O
is O
denoted O
as O
a O
real O
number O
and O
is O
incorporated O
into O
the O
neural O
networks O
. O
Compared O
to O
the O
feature O
vectors O
derived O
from O
the O
WordNet O
, O
our O
model O
directly O
employ O
the O
synset O
and O
hypernym O
relation O
scores O
to O
enrich O
the O
attention O
mechanism O
. O
Wang O
et O
al O
. O
( O
2019 O
) O
present O
an O
entailment O
model O
for O
solving O
the O
Natural O
Language O
Inference O
( O
NLI O
) O
problem O
that O
utilizes O
ConceptNet O
as O
an O
external O
knowledge O
source O
, O
while O
our O
method O
mainly O
focus O
on O
the O
WordNet O
. O
5 O
Conclusion O
In O
this O
paper O
, O
we O
exploit O
a O
WordNet O
- O
enhanced O
hierarchical O
model O
to O
address O
the O
answer O
selection O
problem O
. O
Based O
on O
WordNet O
‚Äôs O
prior O
knowledge O
, O
the O
proposed O
model O
applies O
the O
synset O
and O
hypernym O
concepts O
to O
enrich O
word O
representations O
and O
uses O
synset O
and O
hypernym O
relation O
scores O
between O
two O
concepts O
to O
enhance O
the O
traditional O
attention O
score O
. O
Extensive O
experiments O
conducted O
on O
two O
benchmark O
datasets O
demonstrate O
that O
our O
method O
signiÔ¨Åcantly O
improves O
the O
baseline O
model O
and O
outperforms O
state O
- O
of O
- O
the O
- O
art O
results O
by O
a O
large O
margin O
. O
Our O
approach O
obtains O
1.62 O
% O
improvement O
and O
2.57 O
% O
improvement O
in O
MAP O
on O
the O
WikiQA O
and O
SelQA O
datasets O
, O
respectively O
, O
compared O
to O
the O
stateof O
- O
the O
- O
art O
results O
. O
In O
the O
future O
, O
we O
would O
like O
to113explore O
more O
knowledge O
in O
the O
neural O
networks O
to O
deal O
with O
different O
NLP O
tasks O
. O
Acknowledgments O
This O
work O
is O
supported O
by O
the O
National O
Natural O
Science O
Foundation O
of O
China O
( O
61773026 O
) O
and O
the O
Key O
Project O
of O
Natural O
Science O
Foundation O
of O
China O
( O
61936012 O
) O
. O
Abstract O
In O
this O
paper O
, O
we O
propose O
a O
simple O
method O
to O
predict O
salient O
locations O
from O
news O
article O
text O
using O
a O
knowledge O
base O
( O
KB O
) O
. O
The O
proposed O
method O
uses O
a O
dictionary O
of O
locations O
created O
from O
the O
KB O
to O
identify O
occurrences O
of O
locations O
in O
the O
text O
and O
uses O
the O
hierarchical O
information O
between O
entities O
in O
the O
KB O
for O
assigning O
appropriate O
saliency O
scores O
to O
regions O
. O
It O
allows O
prediction O
at O
arbitrary O
region O
units O
and O
has O
only O
a O
few O
hyperparameters O
that O
need O
to O
be O
tuned O
. O
We O
show O
using O
manually O
annotated O
news O
articles O
that O
the O
proposed O
method O
improves O
the O
f O
- O
measure O
by O
> O
0.12compared O
to O
multiple O
baselines O
. O
1 O
Introduction O
Predicting O
relevant O
locations O
from O
news O
articles O
can O
result O
in O
numerous O
useful O
applications O
. O
For O
example O
, O
it O
enables O
the O
delivery O
of O
news O
related O
to O
a O
speciÔ¨Åc O
city O
that O
is O
of O
user O
interest O
, O
or O
facilitates O
the O
prediction O
of O
a O
disease O
outbreak O
in O
a O
speciÔ¨Åc O
region O
when O
used O
with O
event O
detection O
techniques O
. O
In O
this O
paper O
, O
we O
focus O
on O
predicting O
relevant O
locations O
from O
news O
articles O
. O
The O
goal O
of O
this O
task O
is O
to O
identify O
locations O
that O
are O
salient O
to O
the O
article O
, O
not O
those O
that O
simply O
appeared O
in O
the O
article O
. O
For O
example O
, O
consider O
the O
following O
excerpt O
: O
‚Äú O
The O
Aoi O
Festival O
is O
one O
of O
the O
three O
major O
festivals O
in O
Kyoto O
. O
It O
originated O
as O
a O
series O
of O
rites O
to O
calm O
down O
angry O
gods O
. O
A O
visitor O
from O
Australia O
said O
... O
‚Äù O
In O
this O
example O
, O
the O
phrase O
‚Äú O
Kyoto O
‚Äù O
is O
highly O
relevant O
to O
the O
article O
, O
but O
‚Äú O
Australia O
‚Äù O
is O
not O
. O
Traditional O
methods O
to O
predict O
locations O
require O
speciÔ¨Åc O
data O
, O
such O
as O
a O
training O
dataset O
or O
phrase O
distribution O
, O
that O
match O
the O
application O
domain O
and O
the O
granularity O
of O
the O
prediction O
. O
However O
, O
it O
is O
costly O
to O
prepare O
such O
data O
for O
individual O
applications O
. O
We O
propose O
a O
knowledge O
base O
( O
KB)-based O
method O
that O
only O
requires O
a O
general O
- O
purpose O
KBinstead O
of O
a O
labeled O
dataset O
for O
training O
. O
It O
propagates O
phrase O
- O
level O
importance O
to O
region O
entities O
following O
their O
relationship O
in O
the O
KB O
. O
It O
can O
theoretically O
be O
applied O
to O
predictions O
at O
an O
arbitrary O
level O
of O
granularity O
( O
e.g. O
countries O
, O
prefectures O
, O
cities O
) O
without O
dedicated O
training O
data O
. O
In O
this O
study O
, O
we O
focus O
on O
Japanese O
news O
articles O
and O
report O
the O
performance O
of O
predictions O
at O
the O
Japanese O
prefecture O
- O
level O
. O
We O
provide O
practical O
tips O
to O
tackle O
un O
- O
tokenized O
language O
like O
Japanese O
. O
2 O
Related O
Works O
Depending O
on O
the O
objective O
, O
geolocation O
prediction O
tasks O
from O
texts O
are O
roughly O
divided O
into O
two O
types O
. O
One O
type O
is O
for O
detecting O
and O
identifying O
mentions O
of O
points O
- O
of O
- O
interest O
( O
POIs O
) O
in O
the O
text O
, O
well O
known O
by O
entity O
linking O
( O
Nadeau O
and O
Sekine O
, O
2007 O
; O
Shen O
et O
al O
. O
, O
2015 O
) O
. O
This O
task O
focuses O
on O
extracting O
all O
mentions O
regardless O
of O
their O
saliency O
. O
The O
other O
type O
is O
for O
estimating O
the O
author O
‚Äôs O
current O
location O
or O
home O
town O
from O
his O
or O
her O
posts O
( O
Huang O
and O
Carley O
, O
2019 O
) O
. O
It O
is O
mainly O
performed O
to O
complement O
user O
proÔ¨Åles O
in O
services O
that O
deal O
with O
user O
- O
generated O
content O
( O
e.g. O
SNS O
) O
. O
In O
this O
case O
, O
in O
addition O
to O
text O
, O
various O
user O
metadata O
such O
as O
the O
relationship O
between O
authors O
is O
available O
( O
Backstrom O
et O
al O
. O
, O
2010 O
) O
. O
Our O
work O
is O
similar O
to O
the O
former O
type O
in O
terms O
of O
purpose O
, O
but O
we O
focus O
on O
identifying O
salient O
regions O
rather O
than O
extracting O
all O
of O
the O
individual O
POIs O
. O
There O
are O
two O
main O
approaches O
to O
predicting O
location O
. O
One O
is O
the O
dictionary O
- O
based O
approach O
( O
Berggren O
et O
al O
. O
, O
2015 O
; O
Han O
et O
al O
. O
, O
2012 O
; O
Li O
et O
al O
. O
, O
2014 O
) O
, O
where O
dictionaries O
of O
location O
indicative O
words O
are O
created O
in O
advance O
and O
used O
for O
the O
prediction O
. O
In O
addition O
to O
explicit O
region O
names O
, O
the O
choice O
of O
which O
words O
to O
add O
to O
the O
dictionary O
is O
a O
hot O
topic O
of O
discussion O
( O
Han O
et O
al O
. O
, O
2014 O
) O
. O
The O
other O
is O
the O
machine O
learning(ML)-based O
ap-116proach O
( O
Zhou O
and O
Luo O
, O
2012 O
; O
Miyazaki O
et O
al O
. O
, O
2018 O
) O
. O
Methods O
based O
on O
this O
approach O
usually O
perform O
well O
if O
sufÔ¨Åcient O
training O
data O
is O
available O
. O
However O
, O
in O
practice O
, O
it O
is O
difÔ¨Åcult O
to O
prepare O
data O
whose O
granularity O
matches O
the O
requirements O
of O
the O
application O
. O
In O
particular O
, O
estimating O
regions O
that O
are O
rarely O
found O
in O
the O
training O
data O
is O
one O
of O
the O
weaknesses O
of O
machine O
learning O
. O
3 O
Task O
Setting O
and O
Baseline O
3.1 O
Task O
LetAbe O
the O
set O
of O
articles O
and O
Rbe O
the O
set O
of O
candidate O
regions O
, O
e.g. O
, O
Japanese O
prefectures O
. O
Our O
goal O
is O
to O
construct O
a O
function O
F O
: O
A‚ÜíP(R O
) O
such O
thatr‚àà O
F(a)if O
and O
only O
if O
there O
are O
mentions O
of O
region O
rin O
the O
article O
aand O
regionris O
salient O
to O
the O
text O
, O
where O
P(R)denotes O
the O
power O
set O
ofR. O
Note O
that O
even O
when O
there O
are O
mentions O
ofrina O
, O
ifris O
not O
a O
main O
topic O
in O
a O
, O
F(a)does O
not O
containr O
. O
Similarly O
, O
when O
ais O
not O
a O
locationaware O
article O
, O
then O
F(a)is‚àÖ. O
In O
this O
paper O
, O
we O
assumeAto O
be O
a O
set O
of O
Japanese O
news O
articles O
and O
Rto O
be O
a O
set O
of O
Japanese O
prefectures O
. O
3.2 O
Gazetteer O
baseline O
The O
baseline O
method O
we O
adopted O
is O
similar O
to O
the O
baseline O
used O
in O
( O
Berggren O
et O
al O
. O
, O
2015 O
) O
and O
consists O
of O
the O
following O
three O
steps O
: O
1 O
) O
Create O
a O
gazetteer O
from O
an O
external O
data O
source O
. O
2 O
) O
Identify O
the O
strings O
contained O
in O
the O
gazetteer O
from O
the O
given O
text O
. O
3 O
) O
Aggregate O
the O
results O
and O
return O
the O
relevant O
locations O
. O
In O
practice O
, O
there O
are O
several O
choices O
regarding O
step O
3 O
) O
. O
For O
example O
, O
we O
could O
return O
all O
the O
regions O
mentioned O
in O
the O
text O
, O
return O
the O
region O
mentioned O
most O
frequently O
in O
the O
text O
, O
or O
return O
only O
the O
region O
that O
appears O
earliest O
in O
the O
text O
. O
We O
decided O
to O
return O
locations O
that O
appear O
in O
the O
Ô¨Årst O
20 O
% O
of O
the O
text O
. O
4 O
KB O
- O
based O
Methods O
4.1 O
Knowledge O
base O
A O
KB O
consists O
of O
information O
about O
entities O
expressed O
in O
a O
structured O
, O
machine O
- O
readable O
graph O
format O
. O
YAGO O
and O
DBpedia O
are O
examples O
of O
KBs O
( O
Hoffart O
et O
al O
. O
, O
2011 O
; O
Lehmann O
et O
al O
. O
, O
2015 O
) O
. O
Entities O
are O
assigned O
class(es O
) O
that O
represent O
what O
kind O
of O
entity O
they O
are O
. O
Examples O
of O
possible O
entity O
classes O
include O
person O
, O
place O
, O
and O
company O
. O
Mount O
Fuji O
is O
an O
example O
of O
a O
place O
entity O
. O
A O
KB O
can O
be O
regarded O
as O
an O
edge O
- O
labeled O
directed O
graph O
. O
Entities O
correspond O
to O
nodes O
in O
the O
graph O
, O
and O
the O
relationships O
between O
entities O
correspond O
to O
edges O
in O
the O
graph O
. O
These O
relationships O
are O
given O
relation O
- O
type O
labels O
called O
predicates O
. O
We O
focus O
on O
the O
subgraph O
of O
KB O
that O
is O
useful O
in O
terms O
of O
location O
prediction O
. O
Let O
us O
reduce O
the O
graph O
by O
keeping O
only O
Place O
class O
entities O
, O
and O
vertexes O
connected O
to O
such O
entities O
with O
inclusive O
relations O
such O
as O
containedBy O
predicates O
and O
notation O
relations O
such O
as O
name O
, O
alsoKnownAs O
predicates O
, O
and O
name O
the O
resulting O
graph O
G= O
( O
V O
, O
E O
) O
. O
The O
notation O
relations O
in O
the O
graph O
will O
be O
used O
in O
¬ß O
4.3to O
create O
a O
gazetteer O
, O
while O
the O
inclusive O
relations O
will O
be O
used O
in O
¬ß O
4.5and¬ß4.6to O
determine O
the O
set O
of O
corresponding O
entities O
for O
each O
mention O
and O
calculate O
the O
corresponding O
score O
for O
different O
candidate O
regions O
, O
respectively O
. O
4.2 O
Overview O
of O
the O
proposed O
method O
The O
overview O
of O
our O
proposed O
method O
is O
shown O
in O
Fig O
. O
1 O
. O
It O
consists O
of O
four O
steps O
, O
three O
of O
which O
correspond O
to O
those O
in O
¬ß O
3.2and O
the O
rest O
is O
the O
entity O
linking O
step O
performed O
between O
2 O
) O
and O
3 O
) O
. O
As O
shown O
in O
the O
following O
sections O
, O
we O
add O
the O
efÔ¨Åcient O
use O
of O
KB O
information O
at O
each O
step O
. O
4.3 O
Create O
gazetteer O
Create O
a O
dictionary O
Dwith O
location O
names O
as O
keys O
and O
corresponding O
entities O
as O
values O
using O
the O
notation O
relations O
in O
the O
KB O
. O
Note O
that O
multiple O
entities O
may O
have O
the O
same O
name O
, O
so O
D(m)is O
a O
set O
of O
entities O
that O
belong O
to O
Vfor O
each O
key O
m. O
However O
, O
in O
practice O
, O
if O
we O
use O
all O
of O
the O
notation O
relations O
for O
D O
, O
it O
may O
adversely O
affect O
the O
prediction O
. O
For O
example O
, O
‚Äú O
the O
park O
‚Äù O
can O
be O
an O
alias O
for O
all O
parks O
in O
the O
world O
, O
but O
due O
to O
some O
inconsistencies O
in O
KB O
entries O
, O
some O
parks O
have O
such O
an O
alias O
in O
the O
KB O
and O
others O
do O
not O
. O
Therefore O
, O
by O
using O
inclusive O
relations O
between O
entities O
in O
the O
KB O
, O
we O
systematically O
extract O
phrases O
that O
appear O
as O
the O
preÔ¨Åx O
/ O
postÔ¨Åx O
of O
entity O
names O
in O
wildly O
distant O
multiple O
regions O
and O
create O
a O
blacklist O
of O
phrases O
by O
manually O
reviewing O
them O
. O
In O
addition O
, O
we O
manually O
added O
the O
names O
of O
central O
ministries O
to O
the O
blacklist O
, O
since O
occurrences O
of O
such O
names O
rarely O
indicate O
the O
locality O
of O
the O
news O
. O
The O
blacklist O
currently O
consists O
of O
151 O
words O
. O
4.4 O
Phrase O
identiÔ¨Åcation O
When O
given O
an O
article O
a O
, O
identify O
phrases O
that O
serve O
as O
clues O
by O
the O
following O
three O
steps:117l‡¢Åﬂ¥‡¨úz O
l‹∞“™‡≠îzl‡Øï‡ª∫›ùz O
1.0 O
l‡¥®‡¢≥Õ∑‡≠åz O
# O
region O
name O
, O
entity O
i O
d O
‡±¶⁄ò‡±é O
 O
‹∞“™‡≠î O
 O
‡¢Åﬂ¥‡¨ú O
 O
 O
 O
 ú‡¢Åﬂ¥‡¨úÕ∞Õ∏ O
 O
‡Øï‡ª∫›ùÕ∑‹∞“™ O
‡≠î‡ØäÕ∑ Æ‡¥®‡¢≥ O
Õ∑‡≠å ØÕ∞Õ∏…∫ O
‡¢Åﬂ¥‡¨úÕ∞Õ∏ O
 O
‡Øï‡ª∫›ùÕ∑‹∞“™ O
‡≠î‡ØäÕ∑ Æ‡¥®‡¢≥ O
Õ∑‡≠å ØÕ∞Õ∏…∫ O
Shiraito O
Falls O
ID:617 O
Shiraito O
Falls O
ID:660Takayama O
VLG O
ID:134Takayama O
VLG O
ID:312 O
Karuizawa O
Town O
ID:811 O
Fujinomiya O
City O
ID:950Gunma O
Pref O
. O
ID O
: O
200 O
Nagano O
Pref O
. O
ID:300 O
Shizuoka O
Pref O
. O
ID:400Extract O
only O
hierarchical O
relations O
  O
between O
entities O
0.9 O
0.5 O
0.30.30.9 O
0.82.7 O
KB O
1 O
. O
Create O
  O
gazetteer O
2 O
. O
Phrase O
identification3 O
. O
Entity O
linking O
( O
in O
red O
) O
4 O
. O
Scoring O
& O
  O
propagation O
Figure O
1 O
: O
Overview O
of O
the O
proposed O
method O
. O
1 O
. O
Tokenize O
text O
of O
the O
article O
into O
morphemes O
. O
2 O
. O
Chunk O
the O
list O
of O
morphemes O
so O
that O
it O
results O
in O
as O
long O
and O
as O
many O
matches O
for O
keys O
in O
D. O
3 O
. O
Perform O
named O
entity O
recognition O
( O
NER O
) O
and O
only O
keep O
phrases O
that O
at O
least O
partially O
overlap O
with O
named O
entities O
whose O
IREX1category O
is O
LOCATION O
, O
ARTIFACT O
, O
orORGANIZATION O
. O
The O
additional O
NER O
step O
is O
necessary O
to O
avoid O
confusion O
between O
the O
names O
of O
persons O
and O
places O
. O
There O
are O
many O
family O
names O
in O
Japanese O
that O
have O
similar O
characters O
to O
region O
names O
. O
The O
reason O
we O
do O
not O
limit O
the O
phrases O
to O
those O
categorized O
as O
LOCATION O
is O
that O
some O
ORGANIZATION O
orARTIFACT O
entities O
may O
contain O
region O
names O
in O
their O
names O
, O
e.g. O
, O
small O
local O
businesses O
. O
The O
sets O
of O
phrases O
identiÔ¨Åed O
from O
article O
aiin O
this O
step O
are O
represented O
by O
{ O
mi}hereafter O
. O
4.5 O
Entity O
linking O
Entity O
linking O
is O
the O
task O
of O
mapping O
entity O
mentions O
to O
the O
corresponding O
entities O
in O
a O
KB O
. O
The O
purpose O
of O
this O
step O
is O
to O
reduce O
the O
candidate O
entities O
formifromD(mi)using O
the O
contexts O
of O
articlea O
. O
Da(mi)denotes O
the O
candidates O
remaining O
after O
the O
following O
steps O
. O
1 O
. O
If|D(mi)|= O
1 O
, O
we O
have O
nothing O
to O
do O
. O
2 O
. O
IfD(mi)containses.t.‚àÉmjina O
, O
D(mj O
) O
= O
{ O
e O
} O
, O
we O
remove O
other O
candidates O
for O
mi O
. O
3 O
. O
IfD(mi)doesn‚Äôt O
satisfy O
the O
above O
but O
containses.t.‚àÉmjina O
, O
D(mj O
) O
= O
{ O
e‚Ä≤},eande‚Ä≤ O
1https://nlp.cs.nyu.edu/irex/NE/df990214.txtare O
both O
contained O
by O
the O
same O
region O
r‚ààR O
, O
we O
remove O
other O
candidates O
for O
mi O
. O
In O
short O
, O
we O
give O
preference O
to O
entities O
when O
there O
is O
relevant O
evidence O
elsewhere O
in O
the O
article O
. O
Unlike O
in O
traditional O
entity O
linking O
tasks O
, O
for O
our O
purposes O
, O
if O
the O
procedure O
fails O
to O
resolve O
the O
phrase O
to O
a O
single O
entity O
, O
but O
Ô¨Ånds O
a O
list O
of O
candidates O
, O
it O
is O
still O
quite O
useful O
in O
terms O
of O
location O
prediction O
. O
As O
discussed O
later O
in O
the O
paper O
, O
such O
phrases O
and O
corresponding O
candidate O
entities O
will O
be O
taken O
properly O
into O
account O
in O
the O
later O
steps O
. O
4.6 O
Scoring O
and O
propagation O
In O
this O
step O
, O
we O
score O
each O
phrase O
occurrence O
mi O
and O
propagate O
the O
score O
to O
corresponding O
entities O
. O
First O
, O
we O
deÔ¨Åne O
phrase O
scores O
œÜaas O
: O
œÜa(mi O
) O
= O
length(mi O
) O
log(pos(mi O
) O
+ O
C O
) O
, O
where O
pos(mi)means O
the O
number O
of O
words O
that O
precedemiin O
the O
article O
and O
Cis O
a O
positive O
constant O
. O
Next O
, O
we O
calculate O
entity O
scores O
œàaas O
: O
œàa(ei O
) O
= O
‚àë O
( O
ej O
, O
ei)‚ààEœàa(ej O
) O
|{(ej,¬∑)‚ààE}|+‚àë O
ei‚ààDa(mj)œÜa(mj O
) O
|Da(mj)| O
, O
whereEhas O
a O
DAG O
structure O
because O
it O
is O
composed O
of O
inclusive O
relations O
and O
the O
calculation O
order O
is O
naturally O
determined O
. O
Finally O
, O
return O
regions O
that O
satisfy O
certain O
criteria O
as O
F(a O
) O
. O
There O
are O
several O
possibilities O
for O
the O
actual O
criteria O
to O
determine O
which O
regions O
to O
return O
, O
such O
as O
F(a O
) O
= O
{ O
r‚ààR|œàa(r)>T}(absolute O
) O
, O
F(a O
) O
= O
{ O
r‚ààR|œàa(r O
) O
max O
r‚Ä≤(œàa(r‚Ä≤))>Œ±}(relative O
) O
, O
F(a O
) O
= O
{ O
r‚ààR|rank(œàa(r))‚â§N}(rank).118#of O
articles O
1,711 O
# O
of O
candidate O
prefs O
47 O
# O
of O
salient O
prefs O
/ O
article O
1.29 O
articles O
with O
no O
salient O
prefs O
28.4 O
% O
Table O
1 O
: O
Statistics O
on O
dataset O
. O
After O
experiments O
, O
we O
decided O
to O
adopt O
the O
intersection O
of O
the O
above O
three O
criteria O
with O
parameters O
T= O
0.5,Œ±= O
0.7 O
, O
andN= O
2 O
. O
5 O
Experiments O
5.1 O
Knowledge O
base O
resource O
We O
implemented O
the O
method O
proposed O
in O
¬ß O
4using O
an O
in O
- O
house O
KB O
of O
Yahoo O
Japan O
Corporation O
( O
Yamazaki O
et O
al O
. O
, O
2019 O
) O
and O
in O
- O
house O
morphological O
analysis O
/ O
NER O
tools O
for O
the O
following O
experiments O
. O
In O
short O
, O
the O
KB O
consists O
of O
data O
integrated O
from O
various O
open O
data O
, O
data O
purchased O
from O
our O
suppliers O
, O
and O
information O
extracted O
from O
web O
crawling O
. O
When O
open O
data O
is O
available O
in O
multiple O
languages O
( O
e.g. O
, O
Wikipedia O
) O
, O
a O
Japanese O
data O
dump O
is O
used O
to O
construct O
the O
KB O
. O
For O
historical O
reasons O
, O
the O
entities O
that O
correspond O
to O
regions O
in O
the O
KB O
were O
little O
used O
, O
and O
there O
were O
problems O
regarding O
the O
quality O
of O
data O
in O
this O
domain O
. O
Therefore O
, O
we O
incorporated O
various O
ofÔ¨Åcial O
data O
sources O
containing O
lists O
of O
regions O
, O
regional O
codes O
, O
and O
zip O
codes O
into O
the O
KB O
, O
as O
the O
accuracy O
/ O
completeness O
of O
regions O
and O
the O
inclusive O
relations O
between O
them O
play O
a O
crucial O
role O
in O
location O
prediction O
. O
5.2 O
Dataset O
Since O
there O
is O
no O
publicly O
available O
Japanese O
corpus O
of O
salient O
locations O
, O
we O
asked O
a O
team O
of O
professional O
annotators O
to O
label O
a O
total O
of O
1,711 O
news O
articles O
with O
relevant O
prefectures O
. O
There O
are O
47 O
prefectures O
in O
Japan O
. O
The O
details O
of O
this O
dataset O
are O
shown O
in O
Table O
1 O
. O
The O
team O
consists O
of O
Ô¨Åve O
annotators O
independent O
of O
us O
. O
Although O
each O
article O
is O
labeled O
by O
one O
annotator O
, O
the O
annotation O
team O
created O
an O
annotation O
guideline O
in O
an O
iterative O
way O
as O
follows O
to O
ensure O
consistency O
of O
the O
annotation O
: O
First O
, O
create O
a O
temporary O
annotation O
guideline O
and O
annotate O
a O
relatively O
small O
number O
of O
articles O
. O
Then O
, O
share O
the O
annotated O
results O
and O
discuss O
whether O
an O
annotation O
guideline O
needs O
to O
be O
updated O
. O
This O
iteration O
was O
repeated O
until O
a O
reasonable O
annotation O
guideline O
is O
Ô¨Åxed O
. O
Note O
that O
the O
annotation O
guideline O
was O
Ô¨Ånalized O
before O
the O
development O
of O
the O
proposed O
method O
started O
. O
Although O
our O
method O
enables O
prediction O
with O
Ô¨Åner O
granularity O
, O
we O
evaluate O
only O
at O
the O
prefecture O
- O
level O
in O
this O
Ô¨Årst O
research O
due O
to O
the O
cost O
of O
annotation O
. O
5.3 O
Metrics O
We O
evaluate O
the O
prediction O
performance O
by O
microaveraged O
precision O
( O
pm O
) O
, O
micro O
- O
averaged O
recall O
( O
rm O
) O
, O
and O
article O
- O
averaged O
f O
- O
measure O
( O
fA O
) O
calculated O
as O
: O
pm=‚àë O
a‚ààA|Ra‚à© O
F(a)|‚àë O
a‚ààA|F(a)|,fA=1 O
|A|‚àë O
a‚ààA2para O
pa+ra O
, O
rm=‚àë O
a‚ààA|Ra‚à© O
F(a)|‚àë O
a‚ààA|Ra| O
, O
whereRais O
the O
set O
of O
salient O
prefectures O
for O
articleain O
the O
ground O
truth O
and O
pa=|Ra‚à© O
F(a)|/|F(a)|,ra=|Ra‚à© O
F(a)|/|Ra|are O
articlelevel O
precision O
and O
recall O
, O
respectively O
. O
We O
considerpa= O
1.0ifF(a O
) O
= O
‚àÖ O
, O
andra= O
1.0if O
Ra=‚àÖ. O
Hence O
, O
when O
ais O
not O
a O
location O
- O
aware O
article O
, O
the O
harmonic O
average O
of O
the O
two O
is O
1.0 O
if O
and O
only O
if O
the O
method O
returns O
an O
empty O
set O
, O
and O
0.0 O
otherwise O
. O
5.4 O
Baseline O
methods O
We O
adopted O
two O
different O
baseline O
methods O
to O
demonstrate O
the O
validity O
of O
the O
proposed O
method O
, O
the O
baseline O
method O
that O
relies O
on O
gazetteer O
described O
in O
3.2and O
ML O
- O
based O
method O
. O
The O
second O
ML O
- O
based O
baseline O
method O
treats O
location O
prediction O
as O
a O
multi O
- O
label O
classiÔ¨Åcation O
problem O
( O
i.e. O
, O
an O
article O
can O
have O
multiple O
subject O
regions O
) O
. O
In O
this O
setting O
, O
the O
classiÔ¨Åer O
assigns O
different O
labels O
that O
correspond O
to O
Japanese O
prefectures O
to O
each O
article O
. O
We O
used O
fastText O
( O
Joulin O
et O
al O
. O
, O
2017 O
) O
library O
for O
this O
task O
and O
tokenized O
the O
text O
for O
each O
article O
using O
the O
same O
in O
- O
house O
morphological O
tool O
described O
in O
5.1 O
. O
5.5 O
Comparison O
with O
baselines O
The O
results O
for O
the O
proposed O
and O
baseline O
methods O
are O
listed O
in O
Table O
2 O
. O
As O
shown O
, O
the O
proposed O
method O
outperformed O
the O
baseline O
methods O
in O
all O
performance O
metrics O
. O
Note O
that O
the O
evaluation O
metrics O
for O
fastText O
baseline O
are O
calculated O
in O
a O
slightly O
different O
way O
from O
other O
methods O
and O
are O
meant O
as O
approximate O
reference O
values O
. O
It O
was O
calculated O
by O
taking O
an O
average O
of O
models O
obtained119methods O
pmrmfA O
gazetteer O
baseline O
0.501 O
0.476 O
0.708 O
fastText O
baseline O
0.660 O
* O
0.420 O
* O
0.430 O
* O
proposed O
0.824 O
0.515 O
0.830 O
proposed O
+ O
BL O
0.856 O
0.515 O
0.852 O
Table O
2 O
: O
Results O
of O
proposed O
and O
baseline O
methods O
. O
* O
The O
average O
over O
nested O
4 O
- O
fold O
cross O
- O
validation O
. O
by O
nested O
4 O
- O
fold O
cross O
- O
validation O
over O
the O
evaluation O
dataset O
. O
We O
tuned O
the O
hyperparameters O
to O
optimize O
article O
- O
averaged O
f O
- O
measure O
( O
fA O
) O
in O
each O
inner O
loop O
of O
cross O
- O
validation O
. O
For O
the O
proposed O
method O
and O
the O
gazetteer O
baseline O
that O
require O
no O
dataset O
for O
training O
, O
the O
evaluation O
metrics O
are O
calculated O
using O
the O
entire O
dataset O
. O
The O
gazetteer O
baseline O
suffers O
from O
low O
precision O
. O
We O
give O
the O
following O
example O
to O
demonstrate O
how O
the O
proposed O
method O
‚Äôs O
output O
improved O
over O
that O
of O
the O
gazetteer O
baseline O
. O
Yokohama O
most O
often O
represented O
the O
well O
- O
known O
city O
in O
Kanagawa O
Pref O
. O
but O
on O
rare O
occasions O
represented O
the O
small O
town O
with O
a O
similar O
name O
in O
Aomori O
Pref O
. O
The O
gazetteer O
baseline O
is O
not O
able O
to O
prioritize O
between O
them O
and O
returns O
both O
locations O
. O
The O
proposed O
method O
considered O
the O
other O
entity O
mentions O
to O
resolve O
Yokohama O
into O
the O
correct O
region O
. O
The O
fastText O
baseline O
performs O
differently O
for O
different O
kinds O
of O
articles O
. O
While O
most O
of O
the O
articles O
in O
the O
evaluation O
dataset O
are O
labeled O
one O
or O
two O
prefectures O
, O
some O
articles O
contain O
phrases O
that O
collectively O
refer O
to O
multiple O
Japanese O
prefectures2 O
and O
are O
labeled O
a O
large O
number O
of O
prefectures O
. O
Since O
such O
phrases O
have O
only O
a O
limited O
number O
of O
variations O
and O
appear O
in O
the O
dataset O
repeatedly O
, O
it O
is O
relatively O
easy O
for O
the O
ML O
- O
based O
approach O
to O
learn O
such O
expressions O
. O
Therefore O
it O
performs O
relatively O
well O
in O
terms O
of O
micro O
- O
averaged O
metrics O
heavily O
weighted O
to O
articles O
with O
a O
high O
number O
of O
relevant O
prefectures O
. O
However O
, O
the O
article O
- O
averaged O
metric O
fAis O
incredibly O
low O
compared O
to O
other O
methods O
. O
This O
is O
because O
the O
knowledge O
of O
names O
of O
individual O
prefectures O
or O
cities O
is O
essential O
in O
order O
to O
make O
correct O
predictions O
for O
the O
rest O
of O
the O
articles O
. O
We O
found O
that O
fastText O
classiÔ¨Åer O
often O
fails O
to O
predict O
locations O
for O
such O
articles O
even O
when O
names O
of O
prefectures O
are O
explicitly O
stated O
in O
the O
article O
. O
We O
conclude O
that O
it O
is O
practically O
impossible O
to O
learn O
all O
the O
necessary O
region O
names O
from O
a O
few O
thou2Examples O
include O
‚Äú O
T O
¬Øohoku O
region O
‚Äù O
that O
refers O
to O
6 O
prefectures O
and O
‚Äú O
Western O
Japan O
‚Äù O
that O
refers O
to O
> O
20 O
prefectures.0.50.60.70.80.9 O
0.3 O
0.5 O
0.7 O
0.9pm O
, O
r O
m O
, O
f O
A O
Œ±pmrmfA O
0.50.60.70.80.9 O
0.3 O
0.5 O
0.7 O
TpmrmfA O
Figure O
2 O
: O
Effect O
of O
varying O
Œ±(left O
) O
andT(right O
) O
. O
sand O
articles O
and O
that O
utilizing O
external O
resources O
such O
as O
the O
KB O
is O
the O
critical O
element O
in O
achieving O
good O
performance O
. O
As O
another O
example O
, O
there O
is O
‚Äú O
Tokyo O
Disneyland O
‚Äù O
that O
is O
actually O
located O
in O
Chiba O
Pref O
. O
, O
not O
in O
Tokyo O
Pref O
. O
It O
is O
crucial O
to O
treat O
it O
as O
an O
entity O
and O
not O
be O
confused O
by O
their O
apparent O
region O
names O
. O
When O
we O
added O
the O
blacklist O
created O
in O
¬ß O
4.3to O
the O
proposed O
method O
, O
there O
was O
a O
huge O
improvement O
in O
precision O
. O
This O
highlights O
the O
incompleteness O
of O
the O
aliases O
in O
the O
KB O
and O
indicates O
that O
care O
must O
be O
taken O
when O
applying O
entries O
in O
KB O
to O
a O
real O
service O
. O
5.6 O
Impact O
of O
hyperparameters O
The O
hyperparameters O
that O
govern O
the O
performance O
of O
our O
proposed O
method O
are O
T O
, O
Œ±andN(introduced O
in O
¬ß O
4.6 O
) O
. O
We O
can O
see O
the O
effect O
of O
varying O
these O
hyperparameters O
in O
Fig O
. O
2 O
. O
These O
results O
demonstrate O
that O
the O
precision O
/ O
recall O
tradeoff O
can O
be O
adjusted O
by O
varying O
hyperparameters O
. O
6 O
Conclusion O
In O
this O
paper O
, O
we O
presented O
a O
simple O
KB O
- O
based O
method O
to O
predict O
relevant O
locations O
from O
articles O
. O
The O
proposed O
method O
requires O
no O
training O
data O
or O
maintenance O
of O
a O
dictionary O
thanks O
to O
a O
freshly O
generated O
KB O
, O
and O
it O
can O
be O
used O
to O
make O
predictions O
at O
an O
arbitrary O
level O
of O
granularity O
, O
as O
long O
as O
the O
corresponding O
data O
is O
present O
in O
the O
KB O
. O
We O
demonstrated O
the O
effectiveness O
of O
this O
method O
at O
predicting O
salient O
Japanese O
prefectures O
using O
manually O
annotated O
articles O
. O
In O
future O
work O
, O
we O
plan O
to O
make O
location O
predictions O
at O
the O
city O
- O
level O
and O
evaluate O
its O
performance O
. O
Acknowledgments O
We O
thank O
our O
teammates O
for O
help O
in O
developing O
and O
deploying O
our O
location O
prediction O
system O
. O
We O
are O
grateful O
to O
the O
annotation O
team O
for O
providing O
us O
with O
an O
evaluation O
dataset O
. O
We O
wish O
to O
thank O
the O
anonymous O
referees O
for O
helpful O
comments.120References O
Lars O
Backstrom O
, O
Eric O
Sun O
, O
and O
Cameron O
Marlow O
. O
2010 O
. O
Find O
me O
if O
you O
can O
: O
improving O
geographical O
prediction O
with O
social O
and O
spatial O
proximity O
. O
InProceedings O
of O
the O
19th O
International O
Conference O
on O
World O
Wide O
Web O
, O
pages O
61‚Äì70 O
. O
Max O
Berggren O
, O
Jussi O
Karlgren O
, O
Robert O
¬®Ostling O
, O
and O
Mikael O
Parkvall O
. O
2015 O
. O
Inferring O
the O
location O
of O
authors O
from O
words O
in O
their O
texts O
. O
In O
Proceedings O
of O
the O
20th O
Nordic O
Conference O
of O
Computational O
Linguistics O
, O
pages O
211‚Äì218 O
. O
Bo O
Han O
, O
Paul O
Cook O
, O
and O
Timothy O
Baldwin O
. O
2012 O
. O
Geolocation O
prediction O
in O
social O
media O
data O
by O
Ô¨Ånding O
location O
indicative O
words O
. O
InProceedings O
of O
COLING O
2012 O
, O
pages O
1045‚Äì1062 O
. O
Bo O
Han O
, O
Paul O
Cook O
, O
and O
Timothy O
Baldwin O
. O
2014 O
. O
Textbased O
twitter O
user O
geolocation O
prediction O
.Journal O
of O
ArtiÔ¨Åcial O
Intelligence O
Research O
, O
49(1):451‚Äì500 O
. O
Johannes O
Hoffart O
, O
Fabian O
M O
Suchanek O
, O
Klaus O
Berberich O
, O
Edwin O
Lewis O
- O
Kelham O
, O
Gerard O
De O
Melo O
, O
and O
Gerhard O
Weikum O
. O
2011 O
. O
Yago2 O
: O
exploring O
and O
querying O
world O
knowledge O
in O
time O
, O
space O
, O
context O
, O
and O
many O
languages O
. O
In O
Proceedings O
of O
the O
20th O
International O
Conference O
Companion O
on O
World O
Wide O
Web O
, O
pages O
229‚Äì232 O
. O
Binxuan O
Huang O
and O
Kathleen O
Carley O
. O
2019 O
. O
A O
hierarchical O
location O
prediction O
neural O
network O
for O
twitter O
user O
geolocation O
. O
InProceedings O
of O
the O
2019 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
and O
the O
9th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
4732 O
‚Äì O
4742 O
. O
Armand O
Joulin O
, O
Edouard O
Grave O
, O
Piotr O
Bojanowski O
, O
and O
Tomas O
Mikolov O
. O
2017 O
. O
Bag O
of O
tricks O
for O
efÔ¨Åcient O
text O
classiÔ¨Åcation O
. O
InProceedings O
of O
the O
15th O
Conference O
of O
the O
European O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Volume O
2 O
, O
Short O
Papers O
, O
pages O
427‚Äì431 O
. O
Jens O
Lehmann O
, O
Robert O
Isele O
, O
Max O
Jakob O
, O
Anja O
Jentzsch O
, O
Dimitris O
Kontokostas O
, O
Pablo O
N O
Mendes O
, O
Sebastian O
Hellmann O
, O
Mohamed O
Morsey O
, O
Patrick O
Van O
Kleef O
, O
S O
¬®oren O
Auer O
, O
et O
al O
. O
2015 O
. O
Dbpedia O
‚Äì O
a O
large O
- O
scale O
, O
multilingual O
knowledge O
base O
extracted O
from O
wikipedia O
.Semantic O
Web O
, O
6(2):167‚Äì195 O
. O
Guoliang O
Li O
, O
Jun O
Hu O
, O
Jianhua O
Feng O
, O
and O
Kian O
- O
lee O
Tan O
. O
2014 O
. O
Effective O
location O
identiÔ¨Åcation O
from O
microblogs O
. O
In2014 O
IEEE O
30th O
International O
Conference O
on O
Data O
Engineering O
, O
pages O
880‚Äì891 O
. O
Taro O
Miyazaki O
, O
Afshin O
Rahimi O
, O
Trevor O
Cohn O
, O
and O
Timothy O
Baldwin O
. O
2018 O
. O
Twitter O
geolocation O
using O
knowledge O
- O
based O
methods O
. O
In O
Proceedings O
of O
the O
2018 O
EMNLP O
Workshop O
W O
- O
NUT O
: O
The O
4th O
Workshop O
on O
Noisy O
User O
- O
generated O
Text O
, O
pages O
7‚Äì16 O
. O
David O
Nadeau O
and O
Satoshi O
Sekine O
. O
2007 O
. O
A O
survey O
of O
named O
entity O
recognition O
and O
classiÔ¨Åcation O
. O
Lingvisticae O
Investigationes O
, O
30(1):3‚Äì26.Wei O
Shen O
, O
Jianyong O
Wang O
, O
and O
Jiawei O
Han O
. O
2015 O
. O
Entity O
linking O
with O
a O
knowledge O
base O
: O
Issues O
, O
techniques O
, O
and O
solutions O
.IEEE O
Transactions O
on O
Knowledge O
and O
Data O
Engineering O
, O
27(2):443‚Äì460 O
. O
Tomoya O
Yamazaki O
, O
Kentaro O
Nishi O
, O
Takuya O
Makabe O
, O
Mei O
Sasaki O
, O
Chihiro O
Nishimoto O
, O
Hiroki O
Iwasawa O
, O
Masaki O
Noguchi O
, O
and O
Yukihiro O
Tagami O
. O
2019 O
. O
A O
scalable O
and O
plug O
- O
in O
based O
system O
to O
construct O
a O
production O
- O
level O
knowledge O
base O
. O
In O
DI2KG@ O
KDD O
. O
Youjie O
Zhou O
and O
Jiebo O
Luo O
. O
2012 O
. O
Geo O
- O
location O
inference O
on O
news O
articles O
via O
multimodal O
plsa O
. O
InProceedings O
of O
the O
20th O
ACM O
International O
Conference O
on O
Multimedia O
, O
pages O
741‚Äì744.121Proceedings O
of O
the O
1st O
Conference O
of O
the O
Asia O
- O
PaciÔ¨Åc O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
10th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
122‚Äì132 O
December O
4 O
- O
7 O
, O
2020 O
. O
¬© O
2020 O
Association O
for O
Computational O
Linguistics O
Learning O
Goal O
- O
oriented O
Dialogue O
Policy O
with O
Opposite O
Agent O
Awareness O
Zheng O
Zhang‚Ä† O
, O
Lizi O
Liao‚Ä° O
, O
Xiaoyan O
Zhu‚Ä† O
, O
Tat O
- O
Seng O
Chua‚Ä° O
, O
Zitao O
Liu¬ß O
, O
Yan O
Huang¬ß O
, O
Minlie O
Huang‚Ä†‚àó O
‚Ä†Department O
of O
Computer O
Science O
and O
Technology O
, O
Institute O
for O
ArtiÔ¨Åcial O
Intelligence O
, O
State O
Key O
Lab O
of O
Intelligent O
Technology O
and O
Systems O
, O
Beijing O
National O
Research O
Center O
for O
Information O
Science O
and O
Technology O
, O
Tsinghua O
University O
‚Ä°School O
of O
Computing O
, O
National O
University O
of O
Singapore O
, O
Singpore¬ßTAL O
Education O
Group O
‚Ä†zhangz.goal@gmail.com O
{ O
zxy O
- O
dcs O
, O
aihuang O
} O
@tsinghua.edu.cn O
‚Ä°liaolizi.llz@gmail.com O
chuats@comp.nus.edu.sg¬ß{liuzitao O
, O
galehuang O
} O
@100tal.com O
Abstract O
Most O
existing O
approaches O
for O
goal O
- O
oriented O
dialogue O
policy O
learning O
used O
reinforcement O
learning O
, O
which O
focuses O
on O
the O
target O
agent O
policy O
and O
simply O
treats O
the O
opposite O
agent O
policy O
as O
part O
of O
the O
environment O
. O
While O
in O
realworld O
scenarios O
, O
the O
behavior O
of O
an O
opposite O
agent O
often O
exhibits O
certain O
patterns O
or O
underlies O
hidden O
policies O
, O
which O
can O
be O
inferred O
and O
utilized O
by O
the O
target O
agent O
to O
facilitate O
its O
own O
decision O
making O
. O
This O
strategy O
is O
common O
in O
human O
mental O
simulation O
by O
Ô¨Årst O
imaging O
a O
speciÔ¨Åc O
action O
and O
the O
probable O
results O
before O
really O
acting O
it O
. O
We O
therefore O
propose O
an O
opposite O
behavior O
aware O
framework O
for O
policy O
learning O
in O
goal O
- O
oriented O
dialogues O
. O
We O
estimate O
the O
opposite O
agent O
‚Äôs O
policy O
from O
its O
behavior O
and O
use O
this O
estimation O
to O
improve O
the O
target O
agent O
by O
regarding O
it O
as O
part O
of O
the O
target O
policy O
. O
We O
evaluate O
our O
model O
on O
both O
cooperative O
and O
competitive O
dialogue O
tasks O
, O
showing O
superior O
performance O
over O
state O
- O
of O
- O
the O
- O
art O
baselines O
. O
1 O
Introduction O
In O
goal O
- O
oriented O
dialogue O
systems O
, O
dialogue O
policy O
plays O
a O
crucial O
role O
by O
deciding O
the O
next O
action O
to O
take O
conditioned O
on O
the O
dialogue O
state O
. O
This O
problem O
is O
often O
formulated O
using O
reinforcement O
learning O
( O
RL O
) O
in O
which O
the O
user O
serves O
as O
the O
environment O
( O
Levin O
et O
al O
. O
, O
1997 O
; O
Rieser O
and O
Lemon O
, O
2011 O
; O
Lemon O
and O
Pietquin O
, O
2012 O
; O
Young O
et O
al O
. O
, O
2013 O
; O
Fatemi O
et O
al O
. O
, O
2016 O
; O
Zhao O
and O
Eskenazi O
, O
2016 O
; O
Dhingra O
et O
al O
. O
, O
2016 O
; O
Su O
et O
al O
. O
, O
2016 O
; O
Li O
et O
al O
. O
, O
2017 O
; O
Williams O
et O
al O
. O
, O
2017 O
; O
Liu O
and O
Lane O
, O
2017 O
; O
Lipton O
et O
al O
. O
, O
2018 O
; O
Liu O
et O
al O
. O
, O
2018 O
; O
Gao O
et O
al O
. O
, O
2019 O
; O
Takanobu O
et O
al O
. O
, O
2019 O
, O
2020 O
; O
Jhunjhunwala O
et O
al O
. O
, O
2020 O
) O
. O
However O
, O
different O
from O
symbolic O
- O
based O
and O
simulation O
- O
based O
RL O
tasks O
, O
such O
as O
chess O
( O
Silver O
et O
al O
. O
, O
2016 O
) O
and O
video O
games O
( O
Mnih O
et O
al O
. O
, O
‚àóCorresponding O
author.2015 O
) O
, O
which O
can O
get O
vast O
amounts O
of O
training O
interactions O
in O
low O
cost O
, O
dialogue O
systems O
require O
to O
learn O
directly O
from O
real O
users O
, O
which O
is O
too O
expensive O
. O
Therefore O
, O
there O
are O
some O
efforts O
using O
simulation O
methods O
to O
provide O
an O
affordable O
training O
environment O
. O
One O
principle O
direction O
for O
mitigating O
this O
problem O
is O
to O
leverage O
human O
conversation O
data O
to O
build O
a O
user O
simulator O
, O
and O
then O
to O
learn O
the O
dialogue O
policy O
by O
making O
simulated O
interactions O
with O
the O
simulator O
( O
Schatzmann O
et O
al O
. O
, O
2006 O
; O
Li O
et O
al O
. O
, O
2016 O
; O
G O
¬®ur O
et O
al O
. O
, O
2018 O
) O
. O
However O
, O
there O
always O
exist O
discrepancies O
between O
simulated O
users O
and O
real O
users O
due O
to O
the O
inductive O
biases O
of O
the O
simulation O
model O
, O
which O
can O
lead O
to O
a O
sub O
- O
optimal O
dialogue O
policy O
( O
Dhingra O
et O
al O
. O
, O
2016 O
) O
. O
Another O
direction O
is O
to O
learn O
the O
dynamics O
of O
dialogue O
environment O
during O
interacting O
with O
real O
user O
, O
and O
concurrently O
use O
the O
learned O
dynamics O
for O
RL O
planning O
( O
Peng O
et O
al O
. O
, O
2018 O
; O
Su O
et O
al O
. O
, O
2018 O
; O
Wu O
et O
al O
. O
, O
2018 O
; O
Zhang O
et O
al O
. O
, O
2019b O
) O
. O
Most O
of O
these O
works O
are O
based O
on O
Deep O
Dyna O
- O
Q O
( O
DDQ O
) O
framework O
( O
Sutton O
, O
1990 O
) O
, O
where O
a O
world O
model O
is O
introduced O
to O
learn O
the O
dynamics O
( O
which O
is O
much O
like O
a O
simulated O
user O
) O
from O
real O
experiences O
. O
The O
target O
agent O
‚Äôs O
policy O
is O
trained O
using O
both O
real O
experiences O
via O
direct O
RL O
and O
simulated O
experiences O
via O
a O
world O
- O
model O
. O
In O
the O
above O
methods O
, O
both O
the O
simulated O
user O
and O
world O
model O
facilitate O
target O
policy O
learning O
by O
providing O
more O
simulated O
experiences O
and O
remain O
a O
black O
box O
for O
the O
target O
agent O
. O
That O
is O
, O
the O
target O
agent O
‚Äôs O
knowledge O
about O
the O
simulated O
agents O
is O
still O
passively O
obtained O
through O
interaction O
and O
implicitly O
learned O
by O
the O
policy O
model O
updating O
as O
indirect O
try O
- O
and O
- O
error O
with O
real O
user O
. O
However O
, O
we O
argue O
that O
from O
the O
angle O
of O
a O
target O
agent O
, O
actively O
exploring O
the O
world O
with O
proper O
estimation O
would O
not O
only O
make O
user O
simulation122Policy O
ModelUserEstimatorUserHumanConversational O
Data O
RealExperienceSupervisedLearningImitationLearningDirectRLActingùíÇùíï#ùíÇùíï$ùüèùíê O
	 O
( O
HumanConversational O
DataSupervisedLearningImitationLearningPolicy O
ModelUserRealExperienceWorld O
ModelActingDirect O
RLWorld O
ModelLearningPlanningActingDirect O
RLUser O
  O
/ O
SimulatorHumanConversational O
DataPolicy O
ModelDirect O
RLDDQOPPAFigure O
1 O
: O
A O
comparison O
of O
dialogue O
policy O
learning O
a O
) O
with O
real O
/ O
simulated O
user O
, O
b O
) O
with O
real O
user O
via O
DDQ O
and O
c O
) O
with O
real O
user O
guided O
by O
active O
user O
estimation O
. O
more O
efÔ¨Åcient O
but O
also O
improve O
the O
target O
agent O
‚Äôs O
performance O
. O
In O
agreement O
with O
the O
Ô¨Åndings O
from O
cognitive O
science O
, O
humans O
often O
maintain O
models O
of O
other O
people O
they O
interact O
with O
to O
capture O
their O
goals O
( O
Harper O
, O
2014 O
; O
Premack O
and O
Woodruff O
, O
1978 O
) O
. O
And O
humans O
manage O
to O
use O
their O
mental O
process O
to O
simulate O
others O
‚Äô O
behavior O
( O
Gordon O
, O
1986 O
; O
Gallese O
and O
Goldman O
, O
1998 O
) O
. O
Therefore O
, O
to O
carefully O
treat O
and O
model O
the O
behaviors O
of O
other O
agents O
would O
be O
full O
of O
potential O
. O
For O
example O
, O
in O
competitive O
tasks O
such O
as O
chess O
, O
the O
player O
often O
sees O
a O
number O
of O
moves O
ahead O
by O
considering O
the O
possible O
reaction O
of O
the O
other O
player O
. O
In O
goal O
- O
oriented O
dialogues O
for O
a O
hotel O
booking O
task O
, O
the O
agent O
can O
reduce O
interaction O
numbers O
and O
improve O
user O
experience O
by O
modeling O
users O
as O
business O
travellers O
with O
strict O
time O
limit O
or O
backpackers O
seeking O
adventure O
. O
In O
this O
paper O
, O
we O
propose O
a O
new O
dialogue O
policy O
learning O
method O
with O
OPPosite O
agent O
Awareness O
( O
OPPA O
) O
, O
where O
the O
agent O
maintains O
explicit O
modeling O
of O
the O
opposite O
agent O
or O
user O
for O
facilitating O
its O
own O
policy O
learning O
. O
Different O
from O
DDQ O
, O
the O
estimated O
user O
model O
is O
not O
utilized O
as O
a O
simulator O
to O
produce O
simulated O
experiences O
, O
but O
as O
an O
auxiliary O
component O
of O
the O
target O
agent O
‚Äôs O
policy O
to O
guide O
the O
next O
action O
. O
Figure O
1(c O
) O
shows O
the O
framework O
of O
our O
model O
. O
SpeciÔ¨Åcally O
, O
whenever O
the O
system O
needs O
to O
output O
an O
action O
, O
it O
foresees O
a O
candidate O
action O
ÀÜatand O
consequently O
estimates O
the O
user O
‚Äôs O
response O
behaviorao O
t+1 O
/ O
prime O
. O
On O
top O
of O
this O
estimation O
, O
as O
well O
as O
the O
dialogue O
context O
, O
it O
makes O
better O
decisions O
with O
a O
dynamic O
estimation O
of O
the O
user O
‚Äôs O
strategy O
. O
To O
further O
regulate O
the O
behavior O
of O
the O
system O
agent O
, O
we O
mitigate O
the O
difference O
between O
the O
real O
system O
actionatand O
the O
sampled O
action O
ÀÜatwith O
decay O
for O
better O
robustness O
and O
consistency O
. O
Without O
any O
constraint O
on O
the O
type O
of O
agents O
( O
either O
competitive O
or O
cooperative O
) O
, O
the O
proposed O
OPPA O
method O
can O
be O
applied O
to O
both O
cooperative O
and O
non O
- O
cooperative O
goal O
- O
oriented O
dialogues O
. O
To O
summarize O
, O
our O
contributions O
are O
three O
- O
fold O
: O
‚Ä¢We O
propose O
a O
new O
dialogue O
policy O
learning O
setting O
where O
the O
agent O
shifts O
from O
passively O
learning O
to O
actively O
estimating O
the O
opposite O
agent O
or O
user O
for O
more O
efÔ¨Åcient O
simulations O
, O
thereby O
obtaining O
better O
performance O
. O
‚Ä¢We O
mitigate O
the O
difference O
between O
real O
system O
agent O
action O
and O
the O
sampled O
action O
with O
decay O
to O
further O
enhance O
estimated O
system O
agent O
behaviors O
. O
‚Ä¢Extensive O
experiments O
on O
both O
cooperative O
and O
competitive O
goal O
- O
oriented O
dialogues O
indicate O
that O
the O
proposed O
model O
can O
achieve O
better O
dialogue O
policies O
than O
baselines O
. O
2 O
Related O
Work O
2.1 O
RL O
- O
based O
Dialogue O
Policy O
Learning O
Policy O
learning O
plays O
a O
central O
role O
in O
building O
goaloriented O
dialogue O
systems O
by O
deciding O
the O
next O
action O
, O
which O
is O
often O
formulated O
using O
the O
RL O
framework O
. O
Early O
methods O
used O
probabilistic O
graph O
model O
, O
such O
as O
partially O
observable O
Markov O
decision O
process O
( O
POMDP O
) O
, O
to O
learn O
dialogue O
policy O
by O
modeling O
the O
conditional O
dependences O
between O
observation O
, O
belief O
states O
and O
actions O
( O
Williams O
and O
Young O
, O
2007 O
) O
. O
However O
, O
these O
methods O
require O
manual O
work O
to O
deÔ¨Åne O
features O
and O
state O
representation O
, O
which O
leads O
to O
poor O
domain O
adaptation O
. O
More O
recently O
, O
deep O
learning O
methods O
are O
applied O
in O
dialogue O
policy O
learning O
, O
including O
DQN O
( O
Mnih O
et O
al O
. O
, O
2015 O
) O
and O
Policy O
Gradient O
( O
Sutton O
et O
al O
. O
, O
2000 O
) O
methods O
, O
which O
mitigate O
the O
problem O
of O
domain O
adaptation O
through O
function O
approximation O
and O
representation O
learning O
( O
Zhao O
and O
Eskenazi O
, O
2016 O
) O
. O
Recently O
, O
there O
are O
some O
efforts O
focused O
on O
multi O
- O
domain O
dialogue O
policy O
. O
An O
intuitive O
way O
is O
to O
learn O
independent O
policies O
for O
each O
speciÔ¨Åc O
domain O
and O
aggregate O
them O
( O
Wang O
et O
al O
. O
, O
2014 O
; O
Ga O
Àási¬¥c123et O
al O
. O
, O
2015 O
; O
Cuay O
¬¥ O
ahuitl O
et O
al O
. O
, O
2016 O
) O
. O
There O
are O
also O
some O
works O
using O
hierarchical O
RL O
, O
which O
decomposes O
the O
complex O
task O
into O
several O
sub O
- O
tasks O
( O
Peng O
et O
al O
. O
, O
2017 O
; O
Casanueva O
et O
al O
. O
, O
2018 O
) O
according O
to O
pre O
- O
deÔ¨Åned O
domain O
structure O
and O
cross O
- O
domain O
constraints O
. O
Nevertheless O
, O
most O
of O
the O
above O
works O
regard O
the O
opposite O
agent O
as O
part O
of O
the O
environment O
without O
explicitly O
modeling O
its O
behavior O
. O
Planning O
based O
RL O
methods O
are O
also O
introduced O
to O
make O
a O
trade O
- O
off O
between O
reducing O
human O
interaction O
cost O
and O
learning O
a O
more O
realistic O
simulator O
. O
Peng O
et O
al O
. O
( O
2018 O
) O
proposed O
to O
use O
Deep O
Dynamic O
Q O
- O
network O
, O
in O
which O
a O
world O
model O
is O
co O
- O
trained O
with O
the O
target O
policy O
model O
. O
By O
training O
the O
world O
model O
with O
the O
real O
system O
- O
human O
interaction O
data O
, O
it O
consistently O
approaches O
the O
performance O
of O
real O
users O
, O
which O
provides O
better O
simulated O
experience O
for O
planning O
. O
Adversarial O
methods O
are O
applied O
to O
dynamically O
control O
the O
proportion O
of O
simulated O
and O
real O
experience O
during O
different O
stages O
of O
training O
( O
Su O
et O
al O
. O
, O
2018 O
; O
Wu O
et O
al O
. O
, O
2018 O
) O
. O
Still O
, O
these O
methods O
work O
from O
the O
opposite O
agents O
‚Äô O
angle O
. O
2.2 O
Dialogue O
User O
Simulation O
In O
RL O
- O
based O
dialogue O
policy O
learning O
methods O
, O
a O
user O
simulator O
is O
often O
required O
to O
provide O
affordable O
training O
environments O
due O
to O
the O
high O
cost O
of O
collecting O
real O
human O
corpus O
. O
Agenda O
- O
based O
simulation O
( O
Schatzmann O
et O
al O
. O
, O
2007 O
; O
Li O
et O
al O
. O
, O
2016 O
) O
is O
a O
widely O
applied O
rule O
- O
based O
method O
, O
which O
starts O
with O
a O
randomly O
generated O
user O
goal O
that O
is O
unknown O
to O
the O
system O
. O
During O
a O
dialogue O
session O
, O
it O
remains O
a O
stack O
data O
structure O
known O
as O
user O
agenda O
, O
which O
holds O
some O
pending O
user O
intentions O
to O
achieve O
. O
In O
the O
stack O
update O
process O
, O
machine O
learning O
or O
expert O
- O
deÔ¨Åned O
methods O
can O
be O
applied O
. O
There O
are O
also O
some O
model O
- O
based O
methods O
that O
learn O
a O
simulator O
from O
real O
conversation O
data O
. O
The O
seq2seq O
framework O
has O
recently O
been O
introduced O
by O
encoding O
dialogue O
history O
and O
generates O
the O
next O
response O
or O
dialogue O
action O
( O
Asri O
et O
al O
. O
, O
2016 O
; O
Kreyssig O
et O
al O
. O
, O
2018 O
) O
. O
By O
incorporating O
a O
variational O
step O
to O
the O
seq2seq O
network O
, O
it O
can O
introduce O
meaningful O
diversity O
into O
the O
simulator O
( O
G O
¬®ur O
et O
al O
. O
, O
2018 O
) O
. O
Our O
work O
tackles O
the O
problem O
from O
a O
different O
point O
of O
view O
. O
We O
let O
the O
target O
agent O
approximate O
an O
opposite O
agent O
model O
to O
save O
user O
simulation O
efforts O
. O
3 O
Model O
In O
this O
section O
, O
we O
introduce O
our O
proposed O
OPPA O
model O
. O
There O
are O
two O
agents O
in O
our O
framework O
, O
oneis O
the O
system O
agent O
we O
want O
to O
optimize O
, O
and O
the O
other O
is O
the O
user O
agent O
. O
We O
refer O
to O
these O
two O
agents O
astarget O
andopposite O
agents O
in O
the O
following O
sections O
. O
Note O
that O
the O
proposed O
model O
works O
at O
dialog O
act O
level O
, O
and O
it O
can O
also O
work O
at O
natural O
language O
level O
when O
equipped O
with O
natural O
language O
understanding O
( O
NLU O
) O
and O
natural O
language O
generation O
( O
NLG O
) O
modules O
. O
3.1 O
Overview O
As O
shown O
in O
Figure O
2 O
, O
the O
proposed O
model O
consists O
of O
two O
key O
components O
: O
a O
target O
agent O
QfunctionQ(s O
, O
a)and O
an O
opposite O
agent O
policy O
estimatorœÄo(s O
, O
a O
) O
. O
SpeciÔ¨Åcally O
, O
each O
time O
before O
the O
target O
agent O
needs O
to O
take O
an O
action O
, O
the O
model O
samples O
a O
candidate O
action O
ÀÜat O
. O
Then O
the O
opposite O
estimatorœÄoestimates O
the O
opposite O
agent O
‚Äôs O
response O
behaviorao O
t+1 O
/ O
prime O
, O
which O
is O
then O
aggregated O
with O
the O
original O
dialog O
state O
stto O
generate O
a O
new O
state O
ÀÜst O
. O
On O
top O
of O
this O
new O
state O
, O
the O
target O
policy O
Q(s O
, O
a O
) O
gets O
the O
next O
target O
action O
. O
In O
more O
detail O
, O
a O
brief O
script O
of O
our O
proposed O
OPPA O
model O
is O
shown O
in O
Algorithm O
1 O
. O
3.2 O
Opposite O
Action O
Estimation O
One O
essential O
target O
of O
the O
opposite O
estimator O
is O
to O
measure O
how O
the O
opposite O
agent O
reacts O
given O
its O
preceding O
target O
agent O
action O
and O
state O
. O
In O
OPPA O
, O
we O
implement O
the O
opposite O
estimation O
model O
using O
a O
two O
- O
layer O
feed O
- O
forward O
neural O
network O
followed O
by O
a O
softmax O
layer O
. O
It O
takes O
as O
input O
the O
current O
statest O
, O
a O
sampled O
target O
action O
ÀÜat O
, O
and O
predicts O
an O
opposite O
action O
ao O
t+1 O
/ O
primeas O
below O
: O
ao O
t+1 O
/ O
prime O
= O
œÄo(st,ÀÜat O
) O
. O
( O
1 O
) O
Note O
that O
we O
regard O
the O
opposite O
action O
estimation O
task O
as O
a O
classiÔ¨Åcation O
problem O
, O
and O
ao O
t+1 O
/ O
primeis O
an O
action O
label O
. O
It O
has O
been O
shown O
effective O
in O
other O
studies O
like O
Su O
et O
al O
. O
( O
2018 O
) O
. O
We O
also O
carried O
out O
preliminary O
experiments O
on O
other O
more O
complicated O
designs O
such O
as O
Weber O
et O
al O
. O
( O
2017 O
) O
. O
However O
, O
results O
have O
shown O
MLP O
‚Äôs O
superior O
performance O
in O
our O
dialogue O
policy O
learning O
task O
. O
3.3 O
Opposite O
Aware O
Q O
- O
Learning O
After O
obtaining O
the O
estimated O
opposite O
reaction O
ao O
t+1 O
/ O
prime O
, O
it O
serves O
as O
an O
extra O
input O
to O
the O
DQN O
- O
based O
policy O
component O
besides O
the O
original O
dialogue O
state O
representation O
st O
. O
Therefore O
, O
we O
form O
a O
new O
state O
representation O
ÀÜstas O
below O
: O
ÀÜst= O
[ O
st O
, O
Eoao O
t+1 O
/ O
prime O
] O
, O
( O
2)124Natural O
LanguageUnderstanding O
Natural O
LanguageGenerationDialogue O
StateTrackingDialogue O
Stateùë†"Sample O
Target O
Action O
ùëé"$Opposite O
Estimatorùúã&(ùë†"$,ùëé"$)DQN O
Policy O
a+=argmùëéùë•1ùëÑ(ùë†"$,ùëé)Estimated O
Opposite O
Action O
ùëé"34 O
& O
	 O
6Target O
Action O
ùëé"UserDialogue O
Stateùë† O
" O
ùë•47ùë•87ùë•9:7Dialogue O
ContextsFF O
Layer O
+ O
Attention‚Ñé7ùëõoutputs O
‚Ä¶ O
ùëú4ùëú9Reward O
Function O
HumanRewardorFigure O
2 O
: O
The O
proposed O
OPPA O
model O
and O
the O
reward O
function O
. O
Note O
that O
the O
reward O
for O
policy O
model O
can O
be O
either O
from O
real O
user O
or O
the O
reward O
function O
depending O
on O
whether O
real O
reward O
is O
available O
. O
in O
whichEoao O
t+1 O
/ O
primeintroduces O
the O
knowledge O
of O
opposite O
agent O
into O
our O
policy O
learning O
. O
Eois O
the O
opposite O
action O
embedding O
matrix O
which O
maps O
the O
action O
into O
speciÔ¨Åc O
vector O
representation O
. O
Given O
the O
outputatofargmaxa O
/ O
primeQ O
( O
ÀÜst O
, O
a O
/ O
prime O
) O
, O
the O
agent O
chooses O
an O
action O
to O
execute O
using O
an O
/epsilon1 O
- O
greedy O
policy O
that O
selects O
a O
random O
action O
with O
probability O
/epsilon1or O
otherwise O
follows O
the O
output O
at O
. O
We O
update O
the O
Qfunction O
by O
minimizing O
the O
mean O
- O
squared O
loss O
function O
, O
which O
is O
deÔ¨Åned O
as O
L1(Œ∏ O
) O
= O
E(s O
, O
a O
, O
r O
, O
s O
/ O
prime)‚àºDL[(yi‚àíQ(s O
, O
a))2],(3 O
) O
yi O
= O
r+Œ≥max O
a O
/ O
primeQ O
/ O
prime(s O
/ O
prime O
, O
a O
/ O
prime O
) O
, O
( O
4 O
) O
whereŒ≥‚àà[0,1]is O
a O
discount O
factor O
, O
DLis O
the O
replay O
buffer O
and O
yirepresents O
the O
expected O
reward O
computed O
based O
on O
the O
transition O
. O
3.4 O
Target O
Action O
Sampling O
In O
this O
subsection O
, O
we O
explain O
how O
the O
action O
ÀÜatis O
sampled O
utilizing O
the O
above O
modules O
. O
For O
generating O
the O
true O
target O
action O
at O
, O
we O
predict O
it O
using O
a O
deep O
Q O
- O
network O
which O
takes O
as O
input O
an O
estimated O
opposite O
action O
ao O
t+1 O
/ O
primeand O
the O
dialogue O
state O
he O
t. O
However O
, O
we O
can O
not O
get O
ao O
t+1 O
/ O
primewithout O
ÀÜat O
. O
Therefore O
, O
we O
further O
leverage O
this O
Q O
- O
network O
at O
hand O
. O
Specifcally O
, O
we O
feed O
an O
constant O
opposite O
action O
placeholder O
aoto O
the O
Q O
- O
function O
: O
ÀÜat= O
argmax O
a O
/ O
primeQ([he O
t O
, O
Eoao],a O
/ O
prime O
) O
( O
5 O
) O
whereaoserves O
as O
a O
constant O
opposite O
action O
. O
In O
our O
experiment O
, O
aocorresponds O
to O
the O
general O
actions O
which O
do O
not O
inÔ¨Çuence O
business O
logic O
, O
such O
asHello O
andThanks O
. O
3.5 O
Action O
Regularization O
with O
Decay O
In O
our O
method O
, O
ÀÜatis O
sampled O
from O
a O
distribution O
. O
At O
the O
very O
beginning O
of O
training O
, O
since O
the O
model O
isnot O
well O
trained O
, O
the O
sampled O
ÀÜatmay O
perform O
badly O
, O
which O
would O
lead O
to O
slow O
convergence O
. O
Therefore O
, O
we O
apply O
action O
regularization O
to O
mitigate O
the O
difference O
between O
ÀÜatand O
realat O
. O
As O
the O
training O
progress O
goes O
on O
, O
such O
guidance O
becomes O
less O
effective O
, O
and O
we O
hope O
to O
encourage O
the O
model O
to O
explore O
more O
in O
the O
action O
space O
. O
Therefore O
, O
we O
adopt O
a O
decay O
mechanism O
inspired O
by O
( O
Zhang O
et O
al O
. O
, O
2019a O
) O
. O
The O
regularization O
term O
is O
deÔ¨Åned O
as O
the O
cross O
entropy O
of O
ÀÜatand O
real O
action O
: O
L2(Œ∏ O
) O
= O
‚àíŒ≤ O
/ O
summationdisplay O
tatlog O
( O
ÀÜat O
) O
, O
( O
6 O
) O
whereŒ≤is O
the O
decay O
coefÔ¨Åcient O
. O
The O
value O
of O
Œ≤ O
decreases O
along O
with O
time O
by O
applying O
a O
discount O
factorŒ≥in O
each O
epoch O
. O
As O
a O
consequence O
, O
a O
strict O
constraint O
on O
the O
sampled O
action O
is O
applied O
to O
avoid O
large O
action O
sampling O
performance O
drop O
at O
the O
beginning O
stage O
. O
After O
that O
, O
the O
constraint O
is O
continuously O
relaxed O
so O
that O
the O
model O
can O
explore O
more O
actions O
for O
better O
strategy O
. O
To O
sum O
up O
, O
the O
Ô¨Ånal O
loss O
function O
for O
training O
our O
OPPA O
model O
is O
the O
weighted O
sum O
of O
the O
DQN O
loss O
and O
action O
regularization O
loss O
: O
L(Œ∏ O
) O
= O
L1(Œ∏ O
) O
+ O
ŒªL2(Œ∏ O
) O
. O
( O
7 O
) O
whereŒªis O
an O
adjustable O
hyperparameter O
. O
3.6 O
Reward O
Function O
When O
a O
dialogue O
session O
is O
completed O
, O
what O
we O
get O
are O
several O
dialogue O
acts O
or O
natural O
language O
utterances O
( O
when O
paired O
with O
NLU O
and O
NLG O
) O
. O
For O
most O
goal O
- O
oriented O
dialogues O
, O
the O
reward O
signal O
can O
be O
obtained O
from O
the O
user O
simulator O
or O
real O
user O
ratings O
. O
However O
, O
when O
that O
reward O
is O
not O
available O
, O
an O
output O
prediction O
model O
is O
required O
which O
takes O
as O
input O
the O
whole O
dialogue O
session125Algorithm O
1 O
OPPA O
for O
Dialogue O
Policy O
Learning O
Require:/epsilon1,C O
1 O
: O
initializeœÄo(s O
, O
a;Œ∏œÄ)andQ(s O
, O
a;Œ∏Q)by O
supervised O
and O
imitation O
learning O
2 O
: O
initializeQ O
/ O
prime(s O
, O
a O
, O
Œ∏Q O
/ O
prime)withŒ∏Q O
/ O
prime O
= O
Œ∏Q O
3 O
: O
initialize O
replay O
buffer O
D O
4 O
: O
for O
each O
iteration O
do O
5 O
: O
user O
actsau O
6 O
: O
initialize O
state O
s O
7 O
: O
while O
notdone O
do O
8 O
: O
e O
= O
random O
( O
0,1 O
) O
9 O
: O
ife</epsilon1 O
then O
10 O
: O
select O
a O
random O
action O
a O
11 O
: O
else O
12 O
: O
sample O
ÀÜat O
13 O
: O
est O
. O
user O
action O
ao O
t+1 O
/ O
prime O
= O
œÄo(s,ÀÜat O
) O
14 O
: O
ÀÜs= O
[ O
s O
, O
Eoao O
t+1 O
/ O
prime O
] O
15 O
: O
a O
= O
argmaxa O
/ O
primeQ(ÀÜs O
, O
a O
/ O
prime;Œ∏Q O
) O
16 O
: O
end O
if O
17 O
: O
executea O
18 O
: O
get O
user O
response O
aoand O
rewardr O
19 O
: O
state O
updated O
to O
s O
/ O
prime O
20 O
: O
store O
( O
s O
, O
a O
, O
r O
, O
s O
/ O
prime)toD O
21 O
: O
end O
while O
22 O
: O
sample O
minibatches O
of O
( O
s O
, O
a O
, O
r O
, O
s O
/ O
prime)fromD O
23 O
: O
updateŒ∏Qaccording O
to O
Equation O
4 O
24 O
: O
each O
everyCiterations O
set O
Œ∏Q O
/ O
prime O
= O
Œ∏Q O
25 O
: O
end O
for O
X={xs O
1,xs O
2, O
... O
,xs O
n}whereXis O
a O
sequence O
of O
tokens O
, O
and O
outputs O
structured O
result O
to O
calculate O
the O
reward O
. O
We O
use O
a O
bi O
- O
directional O
GRU O
model O
with O
an O
attention O
mechanism O
to O
learn O
a O
summarization O
hsof O
the O
whole O
session O
: O
ho O
j= O
BiGRU(ho O
j‚àí1,[Exs O
j O
, O
hj O
] O
) O
, O
( O
8) O
ha O
j O
= O
Wa[tanh(Whho O
j O
) O
] O
, O
( O
9 O
) O
Œ±j O
= O
exp(w¬∑ha O
j)/summationtext O
t O
/ O
primeexp(w¬∑ha O
j O
/ O
prime O
) O
, O
( O
10 O
) O
hs= O
tanh(Ws[hg,/summationdisplay O
jŒ±jhj O
] O
) O
. O
( O
11 O
) O
Note O
that O
in O
this O
process O
, O
we O
concatenated O
all O
the O
utterances O
by O
time O
order O
, O
and O
the O
subscript O
j O
indicates O
the O
index O
of O
word O
in O
the O
concatenated O
sequence O
. O
In O
addition O
, O
there O
may O
be O
multiple O
aspects O
of O
the O
output O
. O
For O
example O
, O
in O
a O
negotiation O
goaloriented O
dialogue O
with O
multiple O
issues O
( O
we O
denote O
the O
book O
or O
hat O
items O
to O
negotiate O
on O
as O
issues O
) O
, O
weneed O
to O
get O
the O
output O
of O
each O
issue O
to O
calculate O
the O
total O
reward O
. O
Therefore O
, O
for O
each O
issue O
oi O
, O
a O
speciÔ¨Åc O
softmax O
classiÔ¨Åer O
is O
applied O
: O
pŒ∏(oi|x0 O
... O
T O
, O
g O
) O
= O
softmax O
( O
Woihs O
) O
. O
( O
12 O
) O
After O
the O
structured O
output O
is O
predicted O
, O
we O
can O
obtain O
the O
Ô¨Ånal O
reward O
by O
applying O
the O
task O
- O
speciÔ¨Åc O
reward O
function O
on O
the O
output O
. O
r O
= O
fR(o1,o2, O
... O
,oNo O
) O
, O
( O
13 O
) O
whereNois O
the O
number O
of O
output O
aspects O
and O
fR O
is O
the O
reward O
function O
which O
is O
often O
manually O
deÔ¨Åned O
according O
to O
the O
task O
. O
4 O
Experiment O
Depending O
on O
the O
task O
, O
dialogues O
can O
be O
divided O
into O
cooperative O
and O
competitive O
ones O
. O
In O
a O
cooperative O
task O
, O
the O
aim O
can O
be O
reducing O
unnecessary O
interactions O
by O
inferring O
the O
opposite O
person O
‚Äôs O
intention O
. O
While O
in O
competitive O
tasks O
, O
the O
aim O
is O
usually O
to O
maximize O
their O
own O
interests O
by O
considering O
the O
opposite O
agents O
‚Äô O
possible O
reactions O
. O
To O
test O
our O
method O
‚Äôs O
wide O
suitability O
, O
we O
evaluated O
it O
on O
both O
cooperative O
and O
competitive O
tasks O
. O
4.1 O
Dataset O
For O
the O
cooperative O
task O
, O
we O
used O
MultiWOZ O
( O
Budzianowski O
et O
al O
. O
, O
2018 O
) O
, O
a O
large O
- O
scale O
linguistically O
rich O
multi O
- O
domain O
goal O
- O
oriented O
dialogue O
dataset O
, O
which O
contains O
7 O
domains O
, O
13 O
intents O
and O
25 O
slot O
types O
. O
There O
are O
10,483 O
sessions O
and O
71,544 O
turns O
, O
which O
is O
at O
least O
one O
order O
of O
magnitude O
larger O
than O
previous O
annotated O
task O
- O
oriented O
dialogue O
dataset O
. O
Among O
all O
the O
dialogue O
sessions O
, O
we O
used O
1,000 O
each O
for O
validation O
and O
test O
. O
SpeciÔ¨Åcally O
, O
in O
the O
data O
collection O
stage O
, O
the O
user O
follows O
a O
speciÔ¨Åc O
goal O
to O
converse O
with O
the O
agent O
but O
is O
encouraged O
to O
change O
his O
/ O
her O
goal O
dynamically O
during O
the O
session O
, O
which O
makes O
the O
dataset O
more O
challenging O
. O
For O
the O
competitive O
task O
, O
we O
used O
a O
bilateral O
negotiation O
dataset O
( O
Lewis O
et O
al O
. O
, O
2017 O
) O
, O
where O
there O
are O
5,808 O
dialogues O
from O
2,236 O
scenarios O
. O
In O
each O
session O
, O
there O
are O
two O
people O
negotiating O
to O
divide O
some O
items O
, O
such O
as O
books O
, O
hats O
and O
balls O
. O
Each O
kind O
of O
item O
is O
of O
different O
value O
to O
each O
person O
, O
thus O
they O
can O
give O
priority O
to O
valuable O
items O
in O
the O
negotiation O
. O
For O
example O
, O
a O
hat O
may O
worth O
5 O
for O
personAand3for O
personB O
, O
soBcan O
give O
up O
some O
hat O
in O
order O
to O
get O
other O
valuable O
items O
. O
To O
conduct O
our O
experiment O
, O
we O
further O
labeled O
the O
dataset O
with O
system O
dialogue O
actions.1264.2 O
Experimental O
Settings O
We O
implemented O
the O
model O
using O
PyTorch O
( O
Paszke O
et O
al O
. O
, O
2017 O
) O
. O
The O
hyper O
- O
parameters O
were O
decided O
using O
validation O
set O
. O
The O
dimension O
of O
GRUohidden O
state O
is O
256 O
, O
and O
the O
hidden O
state O
size O
of O
GRUg O
andGRUware O
64 O
and O
128 O
respectively O
. O
The O
size O
ofhsis O
256 O
. O
As O
for O
the O
Q O
- O
function O
, O
the O
size O
of O
st O
is O
256./epsilon1 O
- O
greedy O
is O
applied O
for O
exploration O
. O
The O
buffer O
size O
of O
Dis O
set O
to O
500 O
and O
the O
update O
step O
C O
is O
1 O
. O
Note O
that O
due O
to O
the O
complexity O
of O
MultiWOZ O
, O
the O
error O
propagation O
problem O
caused O
by O
NLU O
and O
NLG O
is O
serious O
. O
Therefore O
, O
the O
cooperative O
experiment O
is O
conducted O
on O
the O
dialogue O
act O
level O
. O
In O
the O
experiment O
, O
our O
proposed O
model O
interacts O
with O
a O
robust O
rule O
- O
based O
user O
simulator O
, O
which O
appends O
an O
agenda O
- O
based O
model O
( O
Schatzmann O
et O
al O
. O
, O
2007 O
) O
with O
extensive O
manual O
rules O
. O
The O
simulator O
gives O
user O
response O
, O
termination O
signal O
and O
goal O
- O
completion O
feedback O
during O
training O
. O
For O
the O
competitive O
task O
, O
the O
experiment O
is O
on O
natural O
language O
level O
. O
Following O
( O
Lewis O
et O
al O
. O
, O
2017 O
) O
, O
we O
built O
a O
seq2seq O
language O
model O
for O
the O
NLU O
and O
NLG O
module O
, O
which O
is O
pre O
- O
trained O
on O
the O
negotiation O
corpus O
. O
Our O
proposed O
model O
was O
Ô¨Årst O
pre O
- O
trained O
with O
supervised O
learning O
( O
SL O
) O
. O
SpeciÔ¨Åcally O
, O
we O
pretrained O
the O
opposite O
estimator O
œÄoand O
the O
QfunctionQ(s O
, O
a)via O
supervised O
learning O
and O
imitation O
learning O
. O
We O
then O
Ô¨Åne O
- O
tuned O
the O
model O
using O
reinforcement O
learning O
( O
RL O
) O
. O
The O
reward O
of O
the O
MultiWOZ O
experiment O
consists O
of O
two O
parts O
: O
a O
) O
a O
small O
negative O
value O
in O
each O
turn O
to O
encourage O
shorter O
sessions O
and O
b O
) O
a O
large O
positive O
reward O
when O
the O
session O
ends O
successfully O
. O
Note O
that O
the O
task O
completion O
signal O
is O
obtained O
from O
the O
user O
. O
For O
the O
negotiation O
experiment O
, O
the O
reward O
is O
the O
total O
value O
of O
item O
items O
that O
the O
agent O
Ô¨Ånally O
got O
. O
In O
the O
negotiation O
dataset O
, O
the O
reward O
is O
given O
by O
the O
proposed O
output O
model O
described O
in O
the O
Reward O
Function O
section O
. O
4.3 O
Baselines O
To O
demonstrate O
the O
effectiveness O
of O
our O
proposed O
model O
, O
we O
compared O
it O
with O
the O
following O
baselines O
. O
For O
the O
MultiWOZ O
task O
, O
we O
compared O
with O
: O
‚Ä¢DQN O
: O
The O
conventional O
DQN O
( O
Mnih O
et O
al O
. O
, O
2015 O
) O
algorithm O
with O
a O
2 O
- O
layer O
fully O
- O
connected O
network O
for O
Q O
- O
function O
. O
‚Ä¢REINFORCE O
: O
The O
REINFORCE O
algorithm O
( O
Williams O
, O
1992 O
) O
with O
a O
2 O
- O
layer O
fully O
- O
connected O
policy O
network.‚Ä¢PPO O
: O
Proximal O
Policy O
Optimization O
( O
Schulman O
et O
al O
. O
, O
2017 O
) O
, O
a O
policy O
- O
based O
RL O
algorithm O
using O
a O
constant O
clipping O
mechanism O
. O
‚Ä¢DDQ O
: O
The O
Deep O
Dyna O
- O
Q O
( O
Peng O
et O
al O
. O
, O
2018 O
) O
algorithm O
which O
introduced O
a O
world O
- O
model O
for O
RL O
planning O
. O
Note O
that O
the O
DQN O
can O
be O
seen O
as O
our O
proposed O
model O
without O
opposite O
estimator O
( O
OPPA O
w/o O
OBE O
) O
. O
For O
the O
negotiation O
task O
, O
we O
compared O
with O
: O
‚Ä¢SL O
RNN O
: O
A O
supervised O
learning O
method O
that O
is O
based O
on O
an O
RNN O
language O
generation O
model O
. O
‚Ä¢RL O
RNN O
: O
The O
reinforcement O
learning O
extension O
of O
SL O
RNN O
by O
reÔ¨Åning O
the O
model O
parameters O
after O
SL O
pretraining O
. O
‚Ä¢ROL O
: O
SL O
RNN O
with O
goal O
- O
based O
decoding O
in O
which O
the O
model O
Ô¨Årst O
generates O
several O
candidate O
utterances O
and O
chooses O
the O
one O
with O
the O
highest O
expected O
overall O
reward O
after O
rolling O
out O
several O
sessions O
. O
‚Ä¢RL O
ROL O
: O
The O
extension O
of O
RL O
RNN O
with O
rollout O
decoding O
. O
‚Ä¢HTG O
: O
A O
hierarchical O
text O
generation O
model O
with O
planning O
( O
Yarats O
and O
Lewis O
, O
2018 O
) O
, O
which O
learns O
explicit O
turn O
- O
level O
representation O
before O
generating O
a O
natural O
language O
response O
. O
Note O
that O
the O
rollout O
mechanism O
used O
in O
ROL O
and O
RL O
ROL O
also O
endows O
them O
with O
the O
ability O
of O
‚Äú O
seeing O
ahead O
‚Äù O
in O
which O
the O
candidate O
actions O
‚Äô O
rewards O
are O
predicted O
using O
a O
random O
search O
algorithm O
, O
while O
our O
OPPA O
explicitly O
models O
the O
opposite O
‚Äôs O
behavior O
. O
RL O
RNN O
, O
RL O
ROL O
and O
HTG O
used O
the O
REINFORCE O
( O
Williams O
, O
1992 O
) O
algorithm O
for O
reinforcement O
learning O
on O
both O
strategy O
and O
language O
level O
, O
while O
in O
OPPA O
we O
used O
the O
DQN O
( O
Mnih O
et O
al O
. O
, O
2015 O
) O
algorithm O
only O
on O
strategy O
level O
. O
To O
further O
examine O
the O
effectiveness O
of O
our O
proposed O
action O
regularization O
with O
decay O
, O
we O
did O
an O
ablation O
study O
by O
removing O
the O
regularization O
with O
decay O
part O
in O
Equation O
6 O
( O
OPPA O
w/o O
A O
) O
. O
4.4 O
Evaluation O
Metric O
For O
the O
evaluation O
of O
experiments O
on O
MultiWOZ O
, O
we O
used O
the O
number O
of O
turns O
, O
inform O
F1 O
score O
, O
match O
rate O
and O
success O
rate O
. O
The O
Number O
of O
turns O
is O
the O
averaged O
number O
on O
all O
sessions O
, O
and O
less O
turns O
in O
cooperative O
goal O
- O
oriented O
task O
can O
promote O
user O
satisfaction O
. O
Inform O
F1 O
evaluates O
whether O
all O
the O
slots O
of O
an O
entity O
requested O
by O
the O
user O
have O
been O
successfully O
informed O
. O
We O
use O
F1 O
score O
because O
it O
considers O
both O
the O
precision O
and O
recall O
so127that O
a O
policy O
which O
greedily O
informs O
all O
slot O
information O
of O
an O
entity O
wo O
n‚Äôt O
get O
a O
high O
score O
. O
Match O
rate O
evaluates O
whether O
the O
booked O
entities O
match O
the O
goals O
in O
all O
domains O
. O
The O
score O
of O
a O
domain O
is O
1 O
only O
when O
its O
entity O
is O
successfully O
booked O
. O
Finally O
, O
a O
session O
is O
considered O
successful O
only O
if O
all O
the O
requested O
slots O
are O
informed O
( O
recall O
= O
1 O
) O
and O
all O
entities O
are O
correctly O
booked O
. O
For O
the O
negotiation O
task O
, O
we O
used O
the O
averaged O
scores O
( O
total O
values O
of O
items O
) O
of O
all O
the O
sessions O
and O
those O
with O
an O
agreement O
as O
the O
primary O
evaluation O
metrics O
following O
Lewis O
et O
al O
. O
( O
2017 O
) O
. O
The O
percentage O
of O
agreed O
and O
Pareto O
optimal‚àósessions O
are O
also O
reported O
. O
Method O
# O
Turn O
Inform O
F1 O
Match O
Success O
DQN O
10.50 O
78.23 O
60.31 O
51.7 O
REINFORCE O
9.49 O
81.73 O
67.41 O
58.1 O
PPO O
9.83 O
83.34 O
69.09 O
59.0 O
DDQ O
9.31 O
81.49 O
63.10 O
62.7 O
OPPA O
w/o O
A O
8.19 O
88.45 O
77.18 O
75.2 O
OPPA O
8.47 O
91.68 O
79.62 O
81.6 O
Human O
7.37 O
66.89 O
95.29 O
75.0 O
Table O
1 O
: O
The O
results O
on O
MultiWoZ O
dataset O
, O
a O
large O
scale O
multi O
- O
domain O
task O
- O
oriented O
dialog O
dataset O
. O
We O
used O
a O
rule O
- O
based O
method O
for O
DST O
and O
Agenda O
- O
based O
user O
simulator O
. O
The O
DQN O
method O
can O
be O
regard O
as O
OPPA O
w/o O
OBE O
. O
Human O
- O
human O
performance O
from O
the O
test O
set O
serves O
as O
the O
upper O
bound O
. O
4.5 O
Cooperative O
Dialogue O
Analysis O
The O
results O
on O
MultiWOZ O
dataset O
are O
shown O
in O
Table O
1 O
. O
OPPA O
shows O
superior O
performance O
on O
task O
success O
rate O
than O
other O
baseline O
methods O
due O
to O
the O
considerable O
improvement O
in O
Inform O
F1 O
and O
Match O
rate O
. O
By O
Ô¨Årst O
infer O
the O
next O
action O
of O
the O
opposite O
agent O
, O
the O
target O
agent O
policy O
can O
make O
better O
choices O
to O
match O
the O
reward O
signal O
during O
training O
. O
When O
compared O
with O
human O
performance O
, O
OPPA O
even O
achieves O
a O
higher O
success O
rate O
, O
although O
the O
number O
of O
turns O
is O
still O
higher O
. O
This O
might O
be O
due O
to O
the O
fact O
that O
the O
user O
is O
sensitive O
to O
the O
dialogue O
length O
. O
When O
a O
dialogue O
becomes O
intolerably O
long O
, O
many O
user O
will O
leave O
without O
completing O
the O
dialogue O
. O
By O
taking O
actions O
in O
account O
of O
the O
inferred O
opposite O
action O
, O
the O
target O
agent O
can O
also O
make O
the O
dialogue O
more O
efÔ¨Åciently O
by O
avoiding O
some O
lengthy O
interactions O
, O
which O
is O
extremely O
important O
in O
applications O
where O
the O
user O
is O
sensitive O
to O
dialogue O
length O
. O
Meanwhile O
, O
DDQ O
achieves O
higher O
task O
success O
rate O
than O
other O
baseline O
models O
since O
it O
also O
mod‚àóA O
dialogue O
is O
Pareto O
optimal O
if O
neither O
agent O
‚Äôs O
score O
can O
be O
improved O
without O
lowering O
the O
other O
‚Äôs O
score.els O
the O
behavior O
of O
opposite O
agent O
through O
world O
model O
. O
However O
, O
it O
makes O
use O
of O
the O
learned O
world O
model O
by O
providing O
more O
simulated O
experiences O
, O
which O
does O
not O
give O
a O
direct O
hint O
on O
how O
to O
act O
in O
the O
middle O
of O
a O
session O
. O
Therefore O
, O
in O
its O
experiments O
, O
it O
still O
gets O
longer O
dialogue O
sessions O
and O
a O
lower O
success O
rate O
than O
OPPA O
. O
If O
we O
remove O
the O
action O
regularization O
mechanism O
, O
we O
can O
see O
an O
obvious O
decline O
on O
performance O
, O
which O
is O
as O
expected O
. O
The O
action O
regularization O
is O
introduced O
to O
mitigate O
the O
difference O
between O
sampled O
ÀÜatand O
realat O
, O
so O
there O
can O
be O
a O
large O
discrepancy O
between O
the O
sampled O
and O
real O
actions O
if O
we O
remove O
it O
at O
the O
early O
training O
stage O
. O
When O
the O
ÀÜatis O
not O
reliable O
, O
the O
consequent O
estimated O
opposite O
action O
a O
/ O
prime O
t+1also O
becomes O
noisy O
, O
which O
leads O
to O
performance O
drop O
. O
4.6 O
Competitive O
Dialogue O
Analysis O
Table O
2 O
shows O
the O
scores O
for O
all O
sessions O
and O
for O
only O
agreed O
ones O
. O
When O
comparing O
with O
the O
seq2seq O
models O
, O
OPPA O
achieves O
signiÔ¨Åcantly O
better O
results O
. O
This O
can O
be O
attributed O
to O
the O
hierarchical O
structure O
of O
OPPA O
. O
The O
sequence O
models O
only O
take O
as O
input O
( O
and O
outputs O
) O
the O
word O
- O
level O
natural O
language O
utterances O
, O
without O
explicitly O
modeling O
turn O
- O
level O
dialogue O
actions O
. O
In O
this O
way O
, O
the O
parameters O
for O
linguistic O
and O
strategy O
functions O
are O
tangled O
together O
, O
and O
the O
back O
- O
propagation O
errors O
can O
inÔ¨Çuence O
both O
sides O
. O
As O
for O
the O
two O
ROL O
models O
, O
although O
they O
can O
predict O
the O
value O
of O
a O
candidate O
action O
in O
advance O
, O
they O
still O
can O
not O
beat O
OPPA O
. O
The O
reason O
is O
that O
the O
rollout O
method O
did O
not O
explicitly O
maintain O
an O
estimation O
of O
the O
opposite O
agent O
as O
our O
OPPA O
did O
. O
Instead O
, O
it O
just O
estimates O
the O
candidate O
acitons O
‚Äô O
rewards O
based O
on O
Monte O
Carlo O
search O
by O
using O
its O
own O
model O
for O
predicting O
future O
movements O
. O
Therefore O
, O
when O
the O
opposite O
model O
‚Äôs O
behavior O
is O
not O
very O
familiar O
to O
the O
target O
agent O
, O
the O
estimated O
reward O
becomes O
unreliable O
. O
The O
HTG O
model O
also O
used O
a O
hierarchical O
framework O
by O
learning O
an O
explicit O
turn O
- O
level O
latent O
representation O
. O
By O
doing O
this O
, O
it O
obtains O
higher O
scores O
than O
the O
seq2seq O
models O
. O
However O
, O
it O
does O
not O
make O
any O
assumptions O
about O
the O
opposite O
agent O
. O
Therefore O
, O
its O
scores O
are O
still O
lower O
than O
OPPA O
, O
although O
the O
discrepancy O
narrows O
down O
. O
By O
removing O
the O
opposite O
estimator O
, O
we O
Ô¨Ånd O
that O
the O
performance O
of O
OPPA O
w/o O
OBE O
drops O
signiÔ¨Åcantly O
compared O
to O
that O
of O
OPPA O
. O
This O
ablation O
study O
directly O
veriÔ¨Åes O
the O
effectiveness O
of O
our O
proposed O
opposite O
behavior O
estimator O
. O
There O
fore,128Methodvs O
. O
SL O
RNN O
vs. O
RL O
RNN O
vs. O
ROL O
vs. O
RL O
ROL O
All O
Agreed O
All O
Agreed O
All O
Agreed O
All O
Agreed O
SL O
RNN O
5.4 O
vs. O
5.5 O
6.2 O
vs. O
6.2 O
- O
- O
- O
- O
- O
RL O
RNN O
7.1 O
vs. O
4.2 O
7.9 O
vs. O
4.7 O
5.5 O
vs. O
5.6 O
5.9 O
vs. O
5.8 O
- O
- O
- O
ROL O
7.3 O
vs. O
5.1 O
7.9 O
vs. O
5.5 O
5.7 O
vs. O
5.2 O
6.2 O
vs. O
5.6 O
5.5 O
vs. O
5.4 O
5.8 O
vs. O
5.9 O
- O
RL O
ROL O
8.3 O
vs. O
4.2 O
8.8 O
vs. O
4.5 O
5.8 O
vs. O
5.0 O
6.5 O
vs. O
5.5 O
6.2 O
vs. O
4.9 O
7.0 O
vs. O
5.4 O
5.9 O
vs. O
5.8 O
6.4 O
vs. O
6.3 O
HTG O
8.7 O
vs. O
4.4 O
8.8 O
vs O
4.5 O
6.0 O
vs. O
5.1 O
6.9 O
vs. O
5.5 O
6.5 O
vs. O
5.0 O
6.9 O
vs. O
5.3 O
6.5 O
vs. O
5.6 O
7.0 O
vs. O
6.3 O
OPPA O
w/o O
OE O
8.2 O
vs. O
4.2 O
8.8 O
vs. O
4.7 O
6.1 O
vs. O
5.2 O
6.8 O
vs. O
5.6 O
6.5 O
v.s. O
4.8 O
7.0 O
v.s. O
5.3 O
5.7 O
v.s. O
5.8 O
6.5 O
v.s. O
6.4 O
OPPA O
w/o O
A O
8.7 O
vs. O
4.1 O
8.9 O
vs. O
4.3 O
6.3 O
vs. O
5.0 O
7.2 O
vs. O
5.4 O
6.5 O
vs. O
4.8 O
7.2 O
vs. O
5.4 O
6.5 O
vs. O
6.1 O
7.1 O
vs. O
6.8 O
OPPA O
8.8 O
vs. O
3.9 O
9.0 O
vs. O
4.1 O
6.7 O
vs. O
4.6 O
7.3 O
vs. O
5.2 O
6.8 O
vs. O
4.2 O
7.4 O
vs. O
5.1 O
6.7 O
vs. O
6.0 O
7.2 O
vs. O
6.6 O
Table O
2 O
: O
The O
results O
of O
our O
proposed O
OPPA O
and O
the O
baselines O
on O
the O
negotiation O
dataset O
. O
AllandAgreed O
indicates O
averaged O
scores O
for O
all O
sessions O
and O
only O
the O
agreed O
sessions O
respectively O
. O
Methodvs O
. O
SL O
RNN O
vs. O
RL O
RNN O
vs. O
ROL O
vs. O
RL O
ROL O
Agreed(% O
) O
PO(% O
) O
Agreed(% O
) O
PO(% O
) O
Agreed(% O
) O
PO(% O
) O
Agreed(% O
) O
PO(% O
) O
SL O
RNN O
87.9 O
49.6 O
- O
- O
- O
- O
- O
RL O
RNN O
89.9 O
58.6 O
81.5 O
60.3 O
- O
- O
- O
ROL O
92.9 O
63.7 O
87.4 O
65.0 O
85.1 O
67.3 O
- O
RL O
ROL O
94.4 O
74.8 O
85.7 O
74.6 O
71.2 O
76.4 O
67.5 O
77.2 O
HTG O
94.8 O
75.1 O
88.3 O
75.4 O
83.2 O
77.8 O
66.1 O
73.2 O
OPPA O
w/o O
OBE O
94.6 O
74.6 O
87.9 O
75.2 O
79.3 O
78.2 O
73.7 O
77.9 O
OPPA O
w/o O
A O
95.6 O
77.9 O
91.9 O
77.4 O
82.4 O
78.8 O
78.0 O
79.5 O
OPPA O
95.7 O
77.7 O
91.4 O
77.2 O
82.3 O
79.1 O
78.2 O
79.7 O
Table O
3 O
: O
The O
proportion O
of O
agreed O
and O
Pareto O
optimal O
( O
PO O
) O
sessions O
for O
our O
proposed O
OPPA O
and O
the O
baselines O
on O
the O
negotiation O
dataset O
. O
modeling O
the O
opposite O
policy O
in O
one O
‚Äôs O
mind O
is O
a O
crucial O
source O
to O
achieve O
better O
results O
in O
competitive O
dialogue O
policy O
learning O
. O
When O
comparing O
with O
OPPA O
w/o O
A O
which O
removed O
action O
regularization O
, O
we O
can O
see O
that O
the O
OPPA O
model O
gets O
better O
results O
. O
This O
veriÔ¨Åes O
the O
importance O
of O
regularizing O
the O
action O
sampling O
. O
By O
controlling O
the O
difference O
between O
real O
and O
model O
generated O
actions O
, O
we O
can O
keep O
the O
opposite O
model O
consistent O
with O
the O
real O
opposite O
agent O
at O
the O
early O
training O
stage O
. O
The O
percentage O
of O
agreed O
and O
Pareto O
optimal O
session O
are O
shown O
in O
Table O
3 O
. O
As O
we O
can O
see O
, O
the O
percentage O
of O
Pareto O
optimal O
increases O
in O
our O
method O
, O
showing O
that O
the O
OPPA O
model O
can O
explore O
the O
solution O
space O
more O
effectively O
. O
However O
, O
the O
agreement O
rate O
decreases O
when O
the O
opposite O
model O
gets O
stronger O
. O
This O
phenomenon O
is O
also O
found O
in O
Lewis O
et O
al O
. O
( O
2017 O
) O
when O
they O
change O
the O
opposite O
agent O
from O
SL O
RNN O
to O
real O
human O
. O
This O
can O
be O
attributed O
to O
the O
aggressiveness O
of O
the O
agent O
: O
when O
both O
agents O
act O
aggressively O
, O
they O
are O
less O
likely O
to O
reach O
an O
agreement O
. O
The O
SL O
RNN O
model O
simply O
imitates O
the O
behavior O
in O
the O
dialogue O
corpus O
, O
while O
the O
ROL O
and O
RL O
mechanisms O
both O
help O
the O
agent O
to O
explore O
more O
spaces O
, O
which O
makes O
them O
more O
aggressive O
on O
action O
selection.4.7 O
Human O
Evaluation O
To O
better O
validate O
our O
propositions O
, O
we O
further O
conducted O
human O
evaluation O
by O
making O
our O
model O
conversing O
with O
real O
user O
. O
We O
only O
conducted O
human O
evaluation O
on O
the O
negotiation O
task O
since O
the O
MultiWOZ O
model O
is O
implemented O
on O
the O
dialogue O
act O
level O
. O
We O
tested O
the O
models O
on O
a O
total O
of O
1,000 O
dialogue O
sessions O
. O
In O
the O
evaluation O
, O
the O
users O
conversed O
with O
the O
agent O
, O
and O
the O
total O
item O
values O
are O
used O
as O
the O
evaluation O
metrics O
. O
The O
results O
are O
shown O
in O
Table O
4 O
. O
We O
can O
see O
that O
our O
proposed O
OPPA O
outperforms O
the O
baseline O
models O
. O
The O
system O
score O
are O
lower O
than O
that O
in O
Table O
2 O
, O
and O
the O
discrepancy O
between O
AllandAgreed O
results O
is O
large O
. O
This O
can O
be O
due O
to O
the O
high O
intelligence O
and O
aggressiveness O
of O
real O
humans O
who O
want O
to O
get O
as O
more O
values O
as O
possible O
and O
do O
not O
make O
compromises O
easily O
. O
Due O
to O
this O
reason O
, O
the O
sessions O
become O
considerably O
lengthy O
, O
and O
the O
target O
agent O
exceeds O
our O
length O
limit O
before O
reaching O
an O
agreement O
. O
Method O
All O
Agreed O
RL O
ROL O
4.5 O
vs. O
5.2 O
7.8 O
vs. O
7.1 O
HTG O
4.8 O
vs. O
4.7 O
8.0 O
vs. O
7.2 O
OPPA O
w/o O
A O
4.7 O
vs. O
4.9 O
8.4 O
vs. O
6.7 O
OPPA O
5.2 O
vs. O
5.1 O
8.2 O
vs. O
6.5 O
Table O
4 O
: O
The O
rewards O
of O
each O
model O
vs. O
human O
user O
. O
5 O
Conclusion O
In O
this O
work O
, O
we O
present O
an O
opposite O
agent O
- O
aware O
dialogue O
policy O
model O
which O
actively O
estimates O
the129opposite O
agent O
instead O
of O
doing O
passive O
learning O
from O
experiences O
. O
We O
have O
shown O
that O
it O
is O
possible O
to O
harvest O
a O
reliable O
model O
of O
the O
opposite O
agent O
through O
more O
efÔ¨Åcient O
dialogue O
interactions O
. O
By O
incorporating O
the O
estimated O
model O
output O
as O
part O
of O
dialogue O
state O
, O
the O
target O
agent O
shows O
signiÔ¨Åcant O
improvement O
on O
both O
cooperative O
and O
competitive O
goal O
- O
oriented O
tasks O
. O
As O
future O
work O
, O
we O
will O
explore O
multi O
- O
party O
dialogue O
modeling O
in O
which O
multi O
- O
agent O
learning O
techniques O
can O
be O
applied O
. O
Acknowledgments O
This O
work O
was O
jointly O
supported O
by O
the O
NSFC O
projects O
( O
key O
project O
with O
No O
. O
61936010 O
and O
regular O
project O
with O
No O
. O
61876096 O
) O
, O
and O
the O
Guoqiang O
Institute O
of O
Tsinghua O
University O
with O
Grant O
No O
. O
2019GQG1 O
. O
This O
work O
was O
also O
supported O
by O
Beijing O
Academy O
of O
ArtiÔ¨Åcial O
Intelligence O
, O
BAAI O
and O
Beijing O
Nova O
Program O
( O
Z201100006820068 O
) O
from O
Beijing O
Municipal O
Science O
& O
Technology O
Commission O
. O
We O
thank O
THUNUS O
NExT O
Joint O
- O
Lab O
for O
the O
support O
. O
Abstract O
Typically O
, O
tokenization O
is O
the O
very O
Ô¨Årst O
step O
in O
most O
text O
processing O
works O
. O
As O
a O
token O
serves O
as O
an O
atomic O
unit O
that O
embeds O
the O
contextual O
information O
of O
text O
, O
how O
to O
deÔ¨Åne O
a O
token O
plays O
a O
decisive O
role O
in O
the O
performance O
of O
a O
model O
. O
Even O
though O
Byte O
Pair O
Encoding O
( O
BPE O
) O
has O
been O
considered O
the O
de O
facto O
standard O
tokenization O
method O
due O
to O
its O
simplicity O
and O
universality O
, O
it O
still O
remains O
unclear O
whether O
BPE O
works O
best O
across O
all O
languages O
and O
tasks O
. O
In O
this O
paper O
, O
we O
test O
several O
tokenization O
strategies O
in O
order O
to O
answer O
our O
primary O
research O
question O
, O
that O
is O
, O
‚Äú O
What O
is O
the O
best O
tokenization O
strategy O
for O
Korean O
NLP O
tasks O
? O
‚Äù O
Experimental O
results O
demonstrate O
that O
a O
hybrid O
approach O
of O
morphological O
segmentation O
followed O
by O
BPE O
works O
best O
in O
Korean O
to O
/ O
from O
English O
machine O
translation O
and O
natural O
language O
understanding O
tasks O
such O
as O
KorNLI O
, O
KorSTS O
, O
NSMC O
, O
and O
PAWS O
- O
X. O
As O
an O
exception O
, O
for O
KorQuAD O
, O
the O
Korean O
extension O
of O
SQuAD O
, O
BPE O
segmentation O
turns O
out O
to O
be O
the O
most O
effective O
. O
Our O
code O
and O
pre O
- O
trained O
models O
are O
publicly O
available O
at O
https://github.com/ O
kakaobrain O
/ O
kortok O
. O
1 O
Introduction O
Tokenization O
is O
the O
very O
Ô¨Årst O
step O
in O
most O
text O
processing O
works O
. O
Not O
surprisingly O
, O
tremendous O
academic O
efforts O
have O
been O
made O
to O
Ô¨Ånd O
the O
best O
tokenization O
method O
for O
various O
NLP O
tasks O
. O
For O
the O
past O
few O
years O
, O
Byte O
Pair O
Encoding O
( O
BPE O
) O
( O
Gage O
, O
1994 O
) O
has O
been O
considered O
the O
de O
facto O
standard O
tokenization O
technique O
since O
it O
was O
reintroduced O
by O
Sennrich O
et O
al O
. O
( O
2016a O
) O
. O
Besides O
the O
fact O
that O
BPE O
turns O
out O
to O
be O
very O
effective O
in O
the O
machine O
translation O
task O
, O
another O
important O
reason O
BPE O
has O
gained O
‚àó*Equal O
contribution.such O
popularity O
is O
that O
BPE O
is O
a O
data O
- O
driven O
statistical O
algorithm O
so O
it O
is O
independent O
of O
language O
. O
However O
, O
it O
is O
still O
not O
clear O
whether O
BPE O
works O
best O
across O
all O
languages O
, O
irrespective O
of O
tasks O
. O
In O
this O
paper O
we O
study O
various O
tokenization O
strategies O
for O
Korean O
, O
a O
language O
which O
is O
morphologically O
by O
far O
richer O
than O
English O
. O
Concretely O
, O
we O
empirically O
examine O
what O
is O
the O
best O
tokenization O
strategy O
for O
Korean O
to O
English O
/ O
English O
to O
Korean O
machine O
translation O
tasks O
, O
and O
natural O
language O
understanding O
( O
NLU O
) O
tasks O
‚Äî O
machine O
reading O
comprehension O
( O
MRC O
) O
, O
natural O
language O
inference O
( O
NLI O
) O
, O
semantic O
textual O
similarity O
( O
STS O
) O
, O
sentiment O
analysis O
, O
and O
paraphrase O
identiÔ¨Åcation O
. O
We O
are O
particularly O
interested O
in O
how O
complementary O
BPE O
and O
linguistically O
motivated O
segmentation O
are O
. O
2 O
Background O
2.1 O
MeCab O
- O
ko O
: O
A O
Korean O
Morphological O
Analyzer O
MeCab O
( O
Kudo O
, O
2006 O
) O
is O
an O
open O
- O
source O
morphological O
analyzer O
based O
on O
Conditional O
Random O
Fields O
( O
CRFs O
) O
. O
It O
is O
originally O
designed O
for O
Japanese O
, O
but O
also O
serves O
generic O
purposes O
so O
it O
can O
be O
applied O
to O
other O
languages O
. O
MeCab O
- O
ko1 O
, O
a O
Korean O
extension O
of O
MeCab O
, O
started O
from O
the O
idea O
that O
MeCab O
can O
be O
easily O
extended O
to O
the O
Korean O
language O
due O
to O
the O
close O
similarity O
between O
Japanese O
and O
Korean O
in O
terms O
of O
morphology O
or O
syntax O
. O
MeCab O
- O
ko O
trained O
its O
model O
on O
the O
Sejong O
Corpus O
( O
Kang O
and O
Kim O
, O
2001 O
) O
, O
arguably O
the O
largest O
Korean O
corpus O
morphologically O
annotated O
by O
many O
experts O
, O
using O
MeCab O
. O
Ever O
since O
released O
in O
2013 O
, O
MeCab O
- O
ko O
has O
been O
widely O
used O
for O
many O
Korean O
NLP O
tasks O
due O
to O
its O
high O
accuracy O
and O
good O
usability O
. O
For O
example O
, O
the O
Workshop O
on O
Asian O
Transla1https://bitbucket.org/eunjeon/ O
mecab O
- O
ko133Tokenization O
Tokenized O
Sequence O
Raw O
Text O
/uni1102 O
/uni1161 O
/ O
uni1105.1 O
/ O
uni1161.10 O
/ O
uni11BC.10 O
/ O
uni1109.27 O
/uni116D.15 O
/ O
uni1111.36 O
/ O
uni1175.5 O
/uni11BC.2 O
/ O
uni1112 O
/uni1161 O
/ O
uni110C O
/uni1161 O
. O
CV O
( O
4.1 O
) O
/uni3134//uni314F//uni3139//uni314F//uni3147/‚ãÜ//uni3145//uni315B//uni314D//uni3163//uni3147//uni314E//uni314F//uni3148//uni314F/. O
Syllable O
( O
4.2 O
) O
/uni1102 O
/uni1161//uni1105.1 O
/ O
uni1161.10 O
/ O
uni11BC.10/‚ãÜ//uni1109.27 O
/uni116D.15//uni1111.36 O
/ O
uni1175.5 O
/uni11BC.2//uni1112 O
/uni1161//uni110C O
/uni1161/. O
Morpheme O
( O
4.3 O
) O
/uni1102 O
/uni1161//uni1105.1 O
/ O
uni1161.10 O
/ O
uni11BC.10/‚ãÜ//uni1109.27 O
/uni116D.15 O
/ O
uni1111.36 O
/ O
uni1175.5 O
/uni11BC.2//uni1112 O
/uni1161//uni110C O
/uni1161/. O
Subword O
( O
4.4 O
) O
/uni1102 O
/uni1161 O
/ O
uni1105.1 O
/ O
uni1161.10 O
/ O
uni11BC.10//uni1109.27 O
/uni116D.15//uni1111.36 O
/ O
uni1175.5 O
/uni11BC.2 O
/ O
uni1112 O
/uni1161//uni110C O
/uni1161/. O
Morpheme O
- O
aware O
Subword O
( O
4.5 O
) O
/uni1102 O
/uni1161//uni1105.1 O
/ O
uni1161.10 O
/ O
uni11BC.10/‚ãÜ//uni1109.27 O
/uni116D.15//uni1111.36 O
/ O
uni1175.5 O
/uni11BC.2//uni1112 O
/uni1161//uni110C O
/uni1161/. O
Word O
( O
4.6 O
) O
/uni1102 O
/uni1161 O
/ O
uni1105.1 O
/ O
uni1161.10 O
/ O
uni11BC.10//uni1109.27 O
/uni116D.15 O
/ O
uni1111.36 O
/ O
uni1175.5 O
/uni11BC.2 O
/ O
uni1112 O
/uni1161 O
/ O
uni110C O
/uni1161 O
/. O
Table O
1 O
: O
An O
input O
sentence O
/uni1102 O
/uni1161 O
/ O
uni1105.1 O
/ O
uni1161.10 O
/ O
uni11BC.10 O
/ O
uni1109.27 O
/uni116D.15 O
/ O
uni1111.36 O
/ O
uni1175.5 O
/uni11BC.2 O
/ O
uni1112 O
/uni1161 O
/ O
uni110C O
/uni1161 O
. O
‚Äò O
Let O
‚Äôs O
go O
shopping O
with O
me O
. O
‚Äô O
is O
differently O
tokenized O
depending O
on O
the O
various O
tokenization O
strategies O
. O
Slashes O
( O
/ O
) O
are O
token O
separators O
. O
tion O
( O
WAT O
) O
has O
adopted O
it O
as O
the O
ofÔ¨Åcial O
segmentation O
tool O
for O
evaluating O
Korean O
machine O
translation O
results O
since O
2015 O
. O
( O
Nakazawa O
et O
al O
. O
, O
2015 O
, O
2016 O
, O
2017 O
, O
2018 O
, O
2019 O
) O
. O
2.2 O
Byte O
Pair O
Encoding O
Byte O
Pair O
Encoding O
( O
BPE O
) O
is O
a O
simple O
data O
compression O
technique O
that O
iteratively O
replaces O
the O
most O
frequent O
pair O
of O
bytes O
in O
text O
with O
a O
single O
, O
unused O
byte O
( O
Gage O
, O
1994 O
) O
. O
Since O
Sennrich O
et O
al O
. O
( O
2016b O
) O
successfully O
applied O
it O
to O
neural O
machine O
translation O
models O
, O
it O
has O
been O
regarded O
as O
the O
standard O
tokenization O
method O
across O
languages O
. O
Korean O
is O
not O
an O
exception O
; O
Park O
et O
al O
. O
( O
2019b O
) O
applied O
BPE O
to O
the O
Korean O
text O
in O
the O
Korean O
to O
Japanese O
task O
of O
WAT O
2019 O
and O
ranked O
Ô¨Årst O
. O
In O
addition O
, O
most O
recent O
Korean O
neural O
language O
models O
( O
e.g. O
, O
KoBERT2 O
) O
used O
BPE O
to O
tokenize O
the O
training O
text O
. O
3 O
Related O
Work O
There O
have O
been O
extensive O
studies O
about O
tokenization O
techniques O
for O
machine O
translation O
. O
Several O
papers O
claimed O
that O
a O
hybrid O
of O
linguistically O
informed O
segmentation O
and O
a O
data O
- O
driven O
method O
like O
BPE O
or O
unigram O
language O
modeling O
performs O
the O
best O
for O
non O
- O
English O
languages O
. O
Banerjee O
and O
Bhattacharyya O
( O
2018 O
) O
combined O
an O
off O
- O
the O
- O
shelf O
morphological O
segmenter O
and O
BPE O
in O
Hindi O
and O
Bengali O
translations O
against O
English O
. O
TawÔ¨Åk O
et O
al O
. O
( O
2019 O
) O
used O
a O
retrained O
version O
of O
linguistically O
motivated O
segmentation O
model O
along O
with O
statistical O
segmentation O
methods O
for O
Arabic O
. O
Pinnis O
et O
al O
. O
( O
2017 O
) O
adopted O
linguistic O
guidance O
to O
BPE O
for O
English O
- O
Latvian O
translation O
. O
Particularly O
( O
Park O
et O
al O
. O
, O
2019a O
) O
is O
close O
to O
ours O
, O
but O
their O
main O
focus O
is O
on O
preprocessing O
techniques O
for O
neural O
machine O
2https://github.com/SKTBrain/KoBERTtranslation O
like O
parallel O
corpus O
Ô¨Åltering O
rather O
than O
on O
tokenization O
strategies O
per O
se O
. O
Compared O
with O
the O
tokenization O
studies O
for O
machine O
translation O
, O
those O
for O
NLU O
tasks O
have O
gained O
less O
attention O
. O
Among O
them O
is O
Bostrom O
and O
Durrett O
( O
2020 O
) O
, O
which O
compared O
the O
Ô¨Åne O
- O
tuning O
task O
performance O
of O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
pre O
- O
trained O
with O
BPE O
and O
unigram O
language O
modeling O
. O
Moon O
and O
Okazaki O
( O
2020 O
) O
proposed O
a O
novel O
encoding O
method O
for O
Korean O
and O
showed O
its O
efÔ¨Åciency O
in O
vocabulary O
compression O
with O
a O
few O
Korean O
NLU O
datasets O
. O
4 O
Tokenization O
Strategies O
We O
introduce O
assorted O
Korean O
tokenization O
strategies O
arranged O
from O
the O
smallest O
to O
the O
largest O
unit O
. O
Each O
of O
them O
induces O
different O
tokenization O
results O
, O
as O
illustrated O
in O
Table O
1 O
. O
4.1 O
Consonant O
and O
Vowel O
( O
CV O
) O
In O
Hangul O
, O
the O
standard O
Korean O
writing O
system O
, O
consonants O
and O
vowels O
, O
called O
Jamo O
in O
Korean O
, O
corresponding O
to O
Latin O
letters O
are O
assembled O
to O
form O
a O
syllable O
character O
. O
For O
example O
, O
a O
Hangul O
consonant O
/ O
uni314E O
/ O
h/ O
( O
U+314E O
) O
is O
combined O
with O
a O
vowel O
/uni314F O
/a/ O
( O
U+314F O
) O
to O
make O
a O
syllable O
character O
/uni1112 O
/uni1161 O
/ O
ha/ O
( O
U+558 O
) O
. O
Readers O
who O
are O
not O
familiar O
with O
such O
a O
mechanism O
can O
think O
of O
Jamo O
and O
syllables O
as O
atoms O
and O
molecules O
respectively O
. O
As O
a O
molecule O
H2Ocan O
be O
decomposed O
into O
two O
Hatoms O
and O
an O
Oatom O
, O
a O
syllable O
/ O
uni1112 O
/uni1161 O
/ O
ha/ O
can O
be O
decomposed O
into O
its O
constituent O
consonant O
/uni314E O
/ O
h/ O
and O
vowel O
/ O
uni314F O
/ O
a/. O
The O
Ô¨Årst O
syllable O
/ O
uni1102 O
/uni1161 O
/ O
na/ O
of O
the O
raw O
text O
in O
Table O
1 O
is O
tokenized O
into O
/ O
uni3134 O
/ O
n/ O
and O
/ O
uni314F O
/ O
a/ O
, O
and O
the O
second O
syllable O
/ O
uni1105.1 O
/ O
uni1161.10 O
/ O
uni11BC.10 O
/ O
lang/ O
is O
tokenized O
into O
/uni3139 O
/ O
l/,/uni314F O
/ O
a/ O
, O
and O
/ O
uni3147 O
/ O
ng/ O
, O
and O
so O
on O
. O
A O
whitespace O
is O
replaced O
by O
a O
special O
symbol O
‚ãÜ.1344.2 O
Syllable O
We O
can O
tokenize O
a O
sentence O
at O
the O
syllable O
level O
. O
A O
whitespace O
is O
replaced O
by O
the O
special O
symbol O
‚ãÜ. O
4.3 O
Morpheme O
MeCab O
- O
ko O
provides O
a O
convenient O
tokenization O
option O
in O
the O
command O
line O
interface3 O
. O
For O
example O
, O
it O
returns O
A O
, O
B O
, O
and O
Cgiven O
an O
input O
text O
AB O
C O
, O
where O
A O
- O
Crepresent O
morphemes O
. O
Note O
that O
the O
original O
space O
between O
ABandCis O
missing O
in O
the O
output O
token O
list O
. O
Accordingly O
, O
it O
is O
NOT O
possible O
to O
recover O
the O
original O
text O
from O
the O
tokenized O
result O
. O
This O
can O
be O
problematic O
in O
some O
tasks O
that O
require O
us O
to O
restore O
the O
input O
text O
such O
as O
machine O
translation O
whose O
target O
language O
is O
Korean O
, O
or O
machine O
reading O
comprehension O
where O
we O
are O
expected O
to O
suggest O
a O
certain O
phrase O
in O
the O
given O
text O
as O
the O
answer O
. O
For O
this O
reason O
, O
we O
insert O
a O
special O
token O
‚ãÜ O
( O
U+2B51 O
) O
to O
the O
original O
whitespace O
position O
. O
As O
a O
result O
, O
in O
the O
above O
example O
, O
the O
tokenized O
sequence O
looks O
like O
A O
, O
B,‚ãÜ O
, O
andD. O
4.4 O
Subword O
We O
learn O
and O
apply O
BPE O
using O
the O
SentencePiece O
( O
Kudo O
and O
Richardson O
, O
2018 O
) O
library O
. O
It O
prepends O
‚Äò O
‚Äô O
( O
U+2581 O
) O
to O
every O
word O
to O
mark O
the O
original O
whitespace O
, O
then O
tokenizes O
text O
into O
subword O
pieces O
. O
As O
seen O
in O
Table O
1 O
, O
/uni1102 O
/uni1161 O
/ O
uni1105.1 O
/ O
uni1161.10 O
/ O
uni11BC.10 O
/ O
uni1109.27 O
/uni116D.15 O
/ O
uni1111.36 O
/ O
uni1175.5 O
/uni11BC.2 O
/ O
uni1112 O
/uni1161 O
/ O
uni110C O
/uni1161 O
. O
can O
be O
split O
into O
/ O
uni1102 O
/uni1161 O
/ O
uni1105.1 O
/ O
uni1161.10 O
/ O
uni11BC.10,/uni1109.27 O
/uni116D.15,/uni1111.36 O
/ O
uni1175.5 O
/uni11BC.2 O
/ O
uni1112 O
/uni1161,/uni110C O
/uni1161 O
, O
and O
. O
( O
period O
) O
. O
4.5 O
Morpheme O
- O
aware O
Subword O
Motivated O
by O
the O
combined O
methods O
of O
dataand O
linguistically O
- O
driven O
approaches O
( O
Banerjee O
and O
Bhattacharyya O
, O
2018 O
; O
Park O
et O
al O
. O
, O
2019a O
; O
Pinnis O
et O
al O
. O
, O
2017 O
; O
TawÔ¨Åk O
et O
al O
. O
, O
2019 O
) O
, O
we O
apply O
MeCabko O
and O
BPE O
in O
sequence O
to O
make O
morpheme O
- O
aware O
subwords O
. O
According O
to O
this O
strategy O
, O
since O
BPE O
is O
applied O
after O
the O
original O
text O
is O
split O
into O
morphemes O
, O
tokens O
spanning O
multiple O
morphemes O
( O
e.g. O
, O
/uni1111.36 O
/ O
uni1175.5 O
/uni11BC.2 O
/ O
uni1112 O
/uni1161 O
in O
the O
Section O
4.4 O
) O
are O
not O
generated O
. O
Instead O
, O
the O
BPE O
algorithm O
further O
segments O
morphemes O
into O
frequent O
pieces O
. O
4.6 O
Word O
We O
can O
simply O
split O
text O
by O
whitespaces O
. O
Note O
that O
punctuation O
marks O
are O
split O
into O
separate O
tokens O
. O
Check O
that O
/ O
uni1102 O
/uni1161 O
/ O
uni1105.1 O
/ O
uni1161.10 O
/ O
uni11BC.10 O
/ O
uni1109.27 O
/uni116D.15 O
/ O
uni1111.36 O
/ O
uni1175.5 O
/uni11BC.2 O
/ O
uni1112 O
/uni1161 O
/ O
uni110C O
/uni1161 O
. O
is O
tokenized O
into O
/ O
uni1102 O
/uni1161 O
/ O
uni1105.1 O
/ O
uni1161.10 O
/ O
uni11BC.10 O
, O
/uni1109.27 O
/uni116D.15 O
/ O
uni1111.36 O
/ O
uni1175.5 O
/uni11BC.2 O
/ O
uni1112 O
/uni1161 O
/ O
uni110C O
/uni1161 O
and O
. O
( O
period O
) O
in O
Table O
1 O
. O
3 O
% O
mecab O
-O O
wakatiLang O
PairVocab O
SizeKorean O
BPE O
Training O
DataDev O
Test O
Ko O
- O
En O
32KAI O
Hub O
( O
130 O
MB O
) O
35.79 O
36.06 O
Wiki O
( O
613 O
MB O
) O
39.05 O
38.69 O
En O
- O
Ko O
32KAI O
Hub O
( O
130 O
MB O
) O
37.19 O
36.98 O
Wiki O
( O
613 O
MB O
) O
37.11 O
36.98 O
Table O
2 O
: O
BLEU O
scores O
of O
Korean O
to O
English O
( O
Ko O
- O
En O
) O
and O
English O
to O
Korean O
( O
En O
- O
Ko O
) O
translation O
models O
with O
different O
BPE O
training O
data O
. O
Note O
that O
the O
English O
sentences O
are O
tokenized O
using O
a O
32 O
K O
BPE O
model O
trained O
on O
the O
English O
Wiki O
. O
5 O
Experiments O
5.1 O
Korean O
to O
/ O
from O
English O
Machine O
Translation O
5.1.1 O
Dataset O
To O
date O
, O
there O
have O
yet O
been O
few O
open O
source O
benchmark O
datasets O
for O
Korean O
- O
English O
machine O
translation O
, O
not O
to O
mention O
that O
Korean O
is O
not O
in O
the O
language O
list O
of O
WMT4or O
IWSLT5 O
. O
Park O
et O
al O
. O
( O
2019a O
) O
used O
OpenSubtitles O
( O
Lison O
and O
Tiedemann O
, O
2016 O
) O
, O
a O
collection O
of O
crowd O
- O
sourced O
movie O
subtitles O
across O
65 O
different O
languages O
, O
for O
English O
to O
Korean O
translation O
, O
but O
they O
are O
too O
noisy O
to O
serve O
as O
a O
translation O
benchmark O
dataset.6 O
Recently O
, O
a O
Korean O
- O
English O
parallel O
corpus O
was O
publicly O
released O
by O
AI O
Hub7 O
, O
which O
was O
gathered O
from O
various O
sources O
such O
as O
news O
, O
government O
web O
sites O
, O
legal O
documents O
, O
etc O
. O
We O
download O
the O
news O
data O
, O
which O
amount O
to O
800 O
K O
sentence O
pairs O
, O
and O
randomly O
split O
them O
into O
784 O
K O
( O
train O
) O
, O
8 O
K O
( O
dev O
) O
, O
and O
8 O
K O
( O
test O
) O
. O
5.1.2 O
BPE O
Modeling O
Prior O
to O
training O
, O
we O
do O
simple O
preliminary O
experiments O
to O
decide O
which O
dataset O
to O
use O
for O
learning O
BPE O
. O
There O
are O
two O
choices O
: O
AI O
Hub O
news O
training O
data O
and O
open O
source O
large O
text O
such O
as O
Wiki O
. O
AI O
Hub O
training O
data O
is O
relatively O
small O
in O
size O
( O
130 O
MB O
) O
, O
but O
can O
be O
optimal O
as O
its O
lexical O
distribution O
will O
be O
close O
to O
that O
of O
the O
test O
data O
, O
considering O
both O
of O
them O
are O
from O
the O
same O
source O
. O
On O
the O
other O
hand O
, O
Wiki O
is O
larger O
, O
but O
it O
is O
not O
news O
per O
se O
, O
so O
can O
be O
not O
as O
appropriate O
as O
AI O
Hub O
data O
for O
4https://www.aclweb.org/anthology/ O
venues O
/ O
wmt O
5http://iwslt.org/doku.php?id=start O
6Park O
et O
al O
. O
( O
2019a O
) O
reported O
BLEU O
scores O
of O
7 O
- O
12 O
. O
7http://www.aihub.or.kr/aidata/87135Tokenization O
Vocab O
SizeKo O
- O
En O
En O
- O
KoOOV O
Rate O
( O
% O
) O
Avg O
. O
LengthDev O
Test O
Dev O
Test O
CV O
166 O
39.11 O
38.56 O
36.52 O
36.45 O
0.02 O
142.75 O
Syllable O
2 O
K O
39.30 O
38.75 O
38.64 O
38.45 O
0.06 O
69.20 O
Morpheme8 O
K O
31.59 O
31.24 O
32.44 O
32.19 O
7.51 O
49.19 O
16 O
K O
34.38 O
33.80 O
35.74 O
35.52 O
4.67 O
49.19 O
32 O
K O
36.19 O
35.74 O
36.51 O
36.12 O
2.72 O
49.19 O
64 O
K O
37.88 O
37.37 O
37.51 O
37.03 O
1.40 O
49.19 O
Subword4 O
K O
39.18 O
38.75 O
38.31 O
38.18 O
0.07 O
48.02 O
8 O
K O
39.16 O
38.75 O
38.09 O
37.94 O
0.08 O
38.44 O
16 O
K O
39.22 O
38.77 O
37.64 O
37.34 O
0.10 O
33.69 O
32 O
K O
39.05 O
38.69 O
37.11 O
36.98 O
0.11 O
30.21 O
64 O
K O
37.02 O
36.46 O
35.77 O
35.64 O
0.12 O
27.50 O
Morpheme O
- O
aware O
Subword4 O
K O
39.41 O
38.95 O
39.29 O
39.13 O
0.06 O
65.17 O
8 O
K O
39.42 O
39.06 O
39.78 O
39.61 O
0.06 O
56.79 O
16 O
K O
39.84 O
39.41 O
40.23 O
40.04 O
0.07 O
53.30 O
32 O
K O
41.00 O
40.34 O
40.43 O
40.41 O
0.07 O
51.38 O
64 O
K O
39.62 O
39.34 O
38.63 O
38.42 O
0.07 O
50.27 O
Word O
64 O
K O
7.04 O
7.07 O
18.68 O
18.42 O
26.20 O
18.96 O
Table O
3 O
: O
BLEU O
scores O
of O
Korean O
to O
English O
( O
Ko O
- O
En O
) O
and O
English O
to O
Korean O
( O
En O
- O
Ko O
) O
translation O
models O
of O
various O
tokenization O
strategies O
. O
Note O
that O
we O
use O
an O
32 O
K O
Subword O
model O
for O
English O
for O
all O
of O
them O
. O
The O
OOV O
rate O
values O
in O
the O
table O
are O
obtained O
from O
the O
test O
set O
, O
but O
there O
is O
no O
meaningful O
difference O
between O
the O
test O
and O
the O
dev O
set O
in O
terms O
of O
the O
OOV O
rate O
. O
The O
best O
BLEU O
scores O
in O
each O
column O
( O
global O
) O
and O
group O
( O
local O
) O
are O
bold O
- O
faced O
and O
underlined O
, O
respectively O
. O
BPE O
modeling O
. O
To O
investigate O
this O
, O
Ô¨Årst O
we O
train O
a O
32 O
K O
Korean O
BPE O
model O
( O
A O
) O
using O
SentencePiece O
with O
the O
Korean O
sentences O
in O
the O
AI O
Hub O
training O
data O
. O
Then O
we O
download O
the O
latest O
Wikipedia O
Korean8 O
/ O
English9dumps O
, O
and O
extract O
plain O
texts O
using O
WikiExtractor10 O
. O
Next O
, O
we O
make O
32 O
K O
BPE O
models O
for O
Korean O
( O
B O
) O
and O
English O
( O
C O
) O
with O
them O
. O
Finally O
, O
we O
train O
Korean O
to O
English O
( O
Ko O
- O
En O
) O
and O
English O
to O
Korean O
( O
En O
- O
Ko O
) O
translation O
models O
on O
the O
AI O
Hub O
training O
data O
with O
the O
two O
different O
Korean O
BPE O
models O
( O
A O
, O
B O
) O
. O
The O
training O
details O
are O
explained O
in O
Section O
5.1.3 O
. O
For O
comparison O
, O
we O
use O
the O
same O
English O
BPE O
model O
( O
C O
) O
for O
both O
. O
The O
results O
are O
shown O
in O
Table O
2 O
. O
For O
Ko O
- O
En O
translation O
, O
the O
Wiki O
- O
based O
BPE O
model O
performs O
better O
in O
both O
dev O
and O
test O
sets O
by O
2 O
- O
3 O
points O
. O
For O
En O
- O
Ko O
translation O
, O
there O
is O
no O
practical O
difference O
in O
performance O
between O
the O
Wiki O
and O
AI O
Hub O
- O
based O
models O
. O
It O
is O
also O
worth O
considering O
the O
BPE O
models O
are O
used O
for O
NLU O
tasks O
as O
well O
as O
machine O
translation O
. O
All O
things O
taken O
together O
, O
we O
opt O
for O
8https://dumps.wikimedia.org/kowiki O
9https://dumps.wikimedia.org/enwiki O
10https://github.com/attardi/ O
wikiextractorthe O
Wiki O
- O
based O
BPE O
model O
. O
5.1.3 O
Training O
We O
test O
the O
tokenization O
strategies O
in O
Section O
4 O
with O
various O
vocabulary O
sizes O
on O
the O
AI O
Hub O
news O
dataset O
. O
We O
use O
the O
Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
the O
state O
- O
of O
- O
the O
- O
art O
model O
for O
neural O
machine O
translation O
. O
We O
mostly O
follow O
the O
base O
model O
conÔ¨Åguration O
: O
6 O
blocks O
of O
512 O
- O
2048 O
units O
with O
8 O
attention O
heads O
. O
We O
run O
all O
of O
our O
experiments O
using O
FAIRSEQ11(Ott O
et O
al O
. O
, O
2019 O
) O
, O
a O
PyTorch O
based O
deep O
learning O
library O
for O
sequence O
to O
sequence O
models O
. O
Each O
model O
is O
trained O
using O
a O
Tesla O
V100 O
GPU O
with O
batch O
size O
128 O
, O
dropout O
rate O
0.3 O
, O
label O
smoothing O
0.1 O
, O
and O
the O
Adam O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
optimizer O
. O
We O
set O
the O
learning O
rate O
to O
5e-4 O
with O
the O
inverse O
square O
- O
root O
schedule O
. O
We O
train O
all O
models O
for O
50 O
epochs O
and O
save O
the O
checkpoint O
Ô¨Åles O
at O
every O
epoch O
. O
5.1.4 O
Results O
After O
all O
training O
stages O
are O
Ô¨Ånished O
, O
we O
evaluate O
the O
saved O
checkpoint O
Ô¨Åles O
of O
each O
model O
on O
11https://github.com/pytorch/fairseq136Vocab O
Size O
# O
Tokens O
# O
Tokens O
Spanning O
Morpheme O
Boundaries O
4 O
K O
387,088 O
25,458 O
( O
6.58 O
% O
) O
8 O
K O
309,360 O
50,029 O
( O
16.17 O
% O
) O
16 O
K O
271,334 O
62,861 O
( O
23.17 O
% O
) O
32 O
K O
242,736 O
73,609 O
( O
30.26 O
% O
) O
64 O
K O
221,530 O
82,324 O
( O
37.16 O
% O
) O
Table O
4 O
: O
Number O
of O
tokens O
spanning O
morpheme O
boundaries O
in O
Subword O
models O
. O
the O
dev O
set O
to O
Ô¨Ånd O
the O
best O
one O
, O
which O
is O
subsequently O
used O
for O
the O
Ô¨Ånal O
test O
. O
In O
Table O
3 O
we O
report O
BLEU O
scores O
on O
both O
the O
dev O
and O
test O
sets O
using O
the O
Moses12multi-bleu.perl O
script O
. O
Following O
WAT O
2019 O
( O
Nakazawa O
et O
al O
. O
, O
2019 O
) O
, O
Moses O
tokenizer O
and O
MeCab O
- O
ko O
are O
used O
for O
tokenizing O
the O
evaluation O
data O
. O
For O
both O
Ko O
- O
En O
and O
En O
- O
Ko O
, O
overall O
, O
the O
Subword O
models O
( O
35.64 O
- O
39.22 O
) O
and O
the O
Syllable O
models O
( O
38.45 O
- O
39.30 O
) O
are O
superior O
to O
the O
Morpheme O
models O
( O
31.59 O
- O
37.37 O
) O
or O
the O
Word O
models O
( O
7.04 O
- O
18.42 O
) O
in O
performance O
. O
It O
is O
highly O
likely O
to O
come O
from O
the O
lower O
OOV O
rates O
of O
the O
Subword O
models O
( O
0.070.12 O
) O
and O
the O
Syllable O
models O
( O
0.06 O
) O
compared O
to O
those O
of O
the O
Morpheme O
models O
( O
1.40 O
- O
7.51 O
) O
and O
the O
Word O
models O
( O
26.20 O
) O
. O
While O
BPE O
tends O
to O
split O
rare O
words O
into O
subword O
pieces O
, O
MeCab O
- O
ko O
is O
ignorant O
of O
statistics O
so O
it O
splits O
words O
into O
morphemes O
by O
linguistic O
knowledge O
instead O
. O
That O
the O
Morpheme O
and O
Word O
models O
generate O
many O
OOVs O
suggests O
Korean O
has O
so O
large O
types O
of O
morphemes O
or O
word O
forms O
that O
even O
64 O
K O
vocabulary O
is O
not O
enough O
to O
cover O
them O
all O
. O
CV O
models O
are O
tiny O
in O
vocabulary O
size O
( O
166 O
) O
so O
they O
show O
the O
lowest O
OOV O
rate O
( O
0.02 O
) O
. O
However O
, O
their O
performance O
is O
not O
as O
good O
as O
the O
Syllable O
or O
Subword O
models O
. O
We O
speculate O
this O
is O
because O
a O
single O
consonant O
or O
vowel O
must O
bear O
too O
much O
contextual O
information O
in O
the O
CV O
models O
. O
Morpheme O
- O
aware O
Subword O
32 O
K O
models O
achieve O
the O
best O
BLEU O
scores O
. O
Each O
Subword O
model O
, O
as O
shown O
in O
Table O
4 O
, O
contains O
6 O
- O
37 O
% O
of O
tokens O
spanning O
morpheme O
boundaries O
in O
the O
test O
set O
, O
which O
implies O
that O
subword O
segmentation O
by O
BPE O
is O
not O
optimal O
and O
morpheme O
boundaries O
are O
meaningful O
in O
tokenization O
. O
To O
sum O
up O
, O
morpheme O
- O
aware O
subword O
tokenization O
that O
makes O
the O
best O
use O
of O
linguistic O
knowledge O
and O
statistical O
information O
is O
the O
best O
for O
Korean O
machine O
translation O
. O
12http://www.statmt.org/mosesHyperparamKorQuAD O
KorNLI O
KorSTS O
NSMC O
PA O
WS O
Epoch O
5 O
3 O
5 O
3 O
5 O
Batch O
16 O
64 O
64 O
64 O
64 O
Œ∑ O
5e-5 O
1e-4 O
5e-5 O
5e-5 O
1e-4 O
Dropout O
0.1 O
0.1 O
0.1 O
0.1 O
0.1 O
Warm O
- O
up O
0.1 O
0.1 O
0.1 O
0.1 O
0.1 O
Max O
Seq.‚Ä†128 O
128 O
128 O
128 O
128 O
Table O
5 O
: O
Fine O
- O
tuning O
hyper O
- O
parameters O
for O
NLU O
tasks O
. O
Œ∑ O
: O
learning O
rate.‚Ä† O
: O
Max O
sequence O
length O
is O
256 O
for O
CV O
models O
in O
all O
tasks O
. O
5.2 O
Korean O
Natural O
Language O
Understanding O
Tasks O
Large O
pre O
- O
trained O
language O
models O
have O
proven O
their O
effectiveness O
in O
many O
downstream O
tasks O
( O
Peters O
et O
al O
. O
, O
2018 O
; O
Devlin O
et O
al O
. O
, O
2019 O
; O
Liu O
et O
al O
. O
, O
2019 O
) O
. O
We O
pre O
- O
train O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
models O
with O
various O
tokenization O
strategies O
, O
and O
Ô¨Åne O
- O
tune O
them O
on O
Ô¨Åve O
different O
Korean O
NLU O
tasks O
. O
5.2.1 O
Machine O
Reading O
Comprehension O
: O
KorQuAD O
1.0 O
Dataset O
The O
KorQuAD O
1.0 O
dataset O
( O
Lim O
et O
al O
. O
, O
2019 O
) O
is O
a O
Korean O
adaptation O
of O
SQuAD O
1.0 O
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
, O
a O
popular O
reading O
comprehension O
dataset O
. O
KorQuAD O
1.0 O
consists O
of O
10,645 O
passages O
and O
their O
paired O
66,181 O
questions O
( O
60,407 O
for O
training O
+ O
5,774 O
for O
development13 O
) O
. O
Like O
SQuAD O
1.0 O
, O
KorQuAD O
1.0 O
involves O
answering O
a O
question O
given O
a O
passage O
. O
The O
answer O
must O
be O
a O
phrase O
within O
the O
passage O
. O
5.2.2 O
Natural O
Language O
Inference O
: O
KorNLI O
Dataset O
The O
KorNLI O
Dataset O
( O
Ham O
et O
al O
. O
, O
2020 O
) O
is O
a O
Korean O
NLI O
dataset O
sourced O
from O
three O
different O
NLI O
datasets O
: O
SNLI O
( O
Bowman O
et O
al O
. O
, O
2015 O
) O
, O
MNLI O
( O
Williams O
et O
al O
. O
, O
2018 O
) O
, O
and O
XNLI O
( O
Conneau O
et O
al O
. O
, O
2018 O
) O
. O
It O
is O
composed O
of O
950,354 O
sentence O
pairs O
: O
942,854 O
for O
training O
, O
2,490 O
for O
development O
, O
and O
5,010 O
for O
test O
. O
A O
model O
receives O
a O
pair O
of O
sentences O
‚Äî O
a O
premise O
and O
a O
hypothesis O
‚Äî O
and O
classiÔ¨Åes O
their O
relationship O
into O
one O
out O
of O
three O
categories O
: O
entailment O
, O
contradiction O
, O
and O
neutral O
. O
5.2.3 O
Semantic O
Textual O
Similarity O
: O
KorSTS O
Dataset O
The O
KorSTS O
Dataset O
( O
Ham O
et O
al O
. O
, O
2020 O
) O
is O
a O
Korean O
STS O
dataset O
translated O
from O
the O
STS O
- O
B O
dataset O
( O
Cer O
et O
al O
. O
, O
2017 O
) O
. O
It O
comprises O
8,628 O
sentence O
13The O
test O
dataset O
is O
not O
included.137TokenizationVocab O
SizeKorQuAD O
KorNLI O
KorSTS O
NSMC O
PA O
WS O
- O
X O
Dev O
( O
EM O
/ O
F1 O
) O
Dev O
Test O
Dev O
Test O
Dev O
Test O
Dev O
Test O
CV O
166 O
59.66 O
/ O
73.91 O
70.60 O
71.20 O
77.22 O
71.47 O
87.97 O
87.89 O
58.00 O
55.20 O
Syllable O
2 O
K O
69.10 O
/ O
83.29 O
73.98 O
73.47 O
82.70 O
75.86 O
88.94 O
89.07 O
68.65 O
67.20 O
Morpheme32 O
K O
68.05 O
/ O
83.82 O
74.86 O
74.37 O
82.37 O
76.83 O
87.87 O
88.04 O
69.30 O
67.20 O
64 O
K O
70.68 O
/ O
85.25 O
75.06 O
75.69 O
83.21 O
77.38 O
88.72 O
88.88 O
73.40 O
68.65 O
Subword4 O
K O
71.48 O
/ O
83.11 O
74.38 O
74.03 O
83.37 O
76.80 O
89.08 O
89.30 O
72.00 O
69.60 O
8 O
K O
72.91 O
/ O
85.11 O
74.18 O
74.65 O
83.23 O
76.42 O
89.08 O
89.19 O
73.45 O
69.00 O
16 O
K O
73.42 O
/ O
85.75 O
74.46 O
75.15 O
83.30 O
76.41 O
88.89 O
88.88 O
73.40 O
70.70 O
32 O
K O
74.04 O
/ O
86.30 O
74.74 O
74.29 O
83.02 O
77.01 O
89.39 O
89.38 O
74.05 O
70.95 O
64 O
K O
74.04 O
/86.66 O
73.73 O
74.55 O
83.52 O
77.47 O
88.80 O
89.19 O
75.85 O
72.10 O
Morpheme O
- O
aware O
Subword4 O
K O
67.53 O
/ O
81.93 O
73.53 O
73.45 O
83.34 O
76.03 O
88.93 O
89.32 O
69.75 O
67.45 O
8 O
K O
70.90 O
/ O
84.57 O
74.14 O
73.95 O
83.71 O
76.07 O
89.37 O
89.29 O
73.40 O
71.30 O
16 O
K O
69.47 O
/ O
83.36 O
75.02 O
74.99 O
83.22 O
76.59 O
89.33 O
89.41 O
75.05 O
71.70 O
32 O
K O
72.65 O
/ O
86.35 O
74.10 O
75.13 O
83.65 O
78.11 O
89.53 O
89.65 O
74.60 O
71.60 O
64 O
K O
69.48 O
/ O
83.73 O
76.39 O
76.61 O
84.29 O
76.78 O
89.82 O
89.66 O
76.15 O
74.00 O
Word O
64 O
K O
1.54 O
/ O
8.86 O
64.06 O
65.83 O
69.00 O
60.41 O
70.10 O
70.58 O
58.25 O
55.30 O
Table O
6 O
: O
Performance O
of O
various O
models O
on O
several O
Korean O
natural O
language O
understanding O
tasks O
. O
The O
evaluation O
metrics O
are O
as O
follows O
: O
KorQuAD O
: O
Exact O
Match O
/ O
F1 O
, O
KorNLI O
: O
accuracy O
( O
% O
) O
, O
KorSTS O
: O
100 O
√óSpearman O
correlation O
, O
NSMC O
: O
accuracy O
( O
% O
) O
, O
PAWS O
- O
X O
: O
accuracy O
( O
% O
) O
. O
The O
best O
scores O
in O
each O
column O
( O
global O
) O
and O
group O
( O
local O
) O
are O
bold O
- O
faced O
and O
underlined O
, O
respectively O
. O
pairs‚Äî5,749 O
for O
training O
, O
1,500 O
for O
development O
, O
and O
1,379 O
for O
test O
. O
The O
task O
assesses O
the O
gradations O
of O
semantic O
similarity O
between O
two O
sentences O
with O
a O
scale O
from O
0 O
to O
5 O
. O
5.2.4 O
Sentiment O
Analysis O
: O
NSMC O
Dataset O
NSMC14is O
a O
movie O
review O
dataset O
scraped O
from O
Naver O
MoviesTM O
. O
It O
consists O
of O
200 O
K O
samples O
of O
which O
150 O
K O
are O
the O
training O
set O
and O
the O
rest O
50 O
K O
are O
the O
test O
set O
. O
Each O
sample O
is O
labeled O
with O
0 O
( O
negative O
) O
or O
1 O
( O
positive O
) O
. O
We O
hold O
out O
10 O
percent O
of O
the O
training O
data O
for O
development O
. O
5.2.5 O
Paraphrase O
IdentiÔ¨Åcation O
: O
PA O
WS O
- O
X O
Dataset O
The O
PAWS O
- O
X O
dataset O
( O
Yang O
et O
al O
. O
, O
2019 O
) O
is O
a O
challenging O
paraphrase O
identiÔ¨Åcation O
dataset O
in O
six O
languages O
including O
Korean O
. O
The O
Korean O
portion O
amounts O
to O
53,338 O
sentence O
pairs O
( O
49,410 O
for O
training O
, O
1,965 O
for O
development O
, O
and O
1,972 O
for O
test O
) O
. O
Like O
the O
NSMC O
dataset O
, O
each O
sentence O
pair O
is O
annotated O
with O
either O
0 O
( O
negative O
) O
or O
1 O
( O
positive O
) O
. O
For O
each O
tokenization O
strategy O
, O
we O
pre O
- O
train O
a O
BERT O
- O
Base O
model O
on O
a O
large O
corpus O
and O
Ô¨Åne O
- O
tune O
it O
on O
the O
training O
sets O
of O
the O
Ô¨Åve O
NLU O
tasks O
independently O
. O
Pre O
- O
training O
. O
Because O
the O
Korean O
Wiki O
corpus O
is O
not O
enough O
in O
volume O
, O
640 O
MB O
, O
for O
the O
pre14https://github.com/e9t/nsmctraining O
purpose O
, O
we O
additionally O
download O
the O
recent O
dump O
of O
Namuwiki15 O
, O
a O
Korean O
Wiki O
, O
and O
extract O
plain O
texts O
using O
Namu O
Wiki O
Extractor16 O
. O
On O
the O
resulting O
Namuwiki O
corpus O
( O
5.5 O
GB O
) O
along O
with O
the O
Wiki O
corpus O
( O
640 O
MB O
) O
, O
pre O
- O
training O
is O
performed O
with O
a O
Cloud O
TPU O
v3 O
- O
8 O
for O
1 O
M O
steps O
using O
the O
ofÔ¨Åcial O
BERT O
training O
code17 O
, O
which O
is O
based O
on O
TensorFlow O
. O
We O
set O
the O
training O
hyperparameters O
of O
all O
models O
as O
follows O
: O
batch O
size O
= O
1024 O
, O
max O
sequence O
length O
= O
128 O
, O
optimizer O
= O
AdamW O
( O
Loshchilov O
and O
Hutter O
, O
2019 O
) O
, O
learning O
rate O
= O
5e-5 O
, O
warm O
- O
up O
steps O
= O
10K. O
Fine O
- O
tuning O
. O
After O
converting O
each O
of O
the O
pretrained O
models O
in O
TensorFlow O
into O
PyTorch O
, O
we O
Ô¨Åne O
- O
tune O
it O
using O
HuggingFace O
Transformers18 O
( O
Wolf O
et O
al O
. O
, O
2019 O
) O
. O
The O
hyper O
- O
parameters O
for O
each O
task O
are O
shown O
in O
Table O
5 O
. O
5.2.6 O
Results O
In O
Table O
6 O
we O
report O
the O
evaluation O
results O
of O
the O
various O
models O
on O
the O
dev O
and O
test O
sets O
. O
Since O
KorQuAD O
lacks O
the O
test O
set O
, O
we O
report O
the O
results O
on O
the O
dev O
set O
only O
. O
15http://dump.thewiki.kr O
16https://github.com/jonghwanhyeon/ O
namu O
- O
wiki O
- O
extractor O
17https://github.com/google-research/ O
bert O
18https://github.com/huggingface/ O
transformers1380.00 O
0.25 O
0.50 O
0.75 O
1.00 O
1.25 O
1.50 O
1.75 O
2.00 O
Avg O
. O
# O
syllables O
per O
token3234363840BLEU O
scores O
on O
test O
set O
8k16k32k64k4k O
8k O
16k O
32k O
64k4k8k16k32k O
64k O
2k166 O
CV O
Syllable O
Morpheme O
Subword O
Morpheme O
- O
aware O
Subword(a O
) O
Ko O
- O
En O
0.00 O
0.25 O
0.50 O
0.75 O
1.00 O
1.25 O
1.50 O
1.75 O
2.00 O
Avg O
. O
# O
syllables O
per O
token3234363840BLEU O
scores O
on O
test O
set O
8k16k32k64k4k O
8k O
16k O
32k O
64k4k8k16k32k O
64k O
2k O
166 O
CV O
Syllable O
Morpheme O
Subword O
Morpheme O
- O
aware O
Subword O
( O
b O
) O
En O
- O
Ko O
Figure O
1 O
: O
Translation O
performance O
over O
the O
average O
number O
of O
syllables O
per O
token O
As O
for O
KorQuAD O
, O
Subword O
64 O
K O
models O
achieve O
the O
highest O
Exact O
Match O
( O
EM O
) O
and O
F1 O
scores O
. O
The O
scores O
in O
the O
Subword O
and O
Morpheme O
models O
increase O
monotonically O
as O
the O
vocabulary O
size O
grows O
. O
On O
the O
other O
hand O
, O
the O
32 O
K O
models O
outperform O
the O
others O
in O
the O
Morpheme O
- O
aware O
Subword O
models O
; O
no O
clear O
correlation O
is O
found O
between O
performance O
and O
vocabulary O
sizes O
in O
them O
. O
For O
all O
the O
other O
four O
tasks O
, O
Morpheme O
- O
aware O
Subword O
64 O
K O
models O
show O
the O
best O
scores O
. O
One O
noteworthy O
phenomenon O
is O
that O
the O
scores O
tend O
to O
increase O
as O
the O
vocabulary O
size O
grows O
across O
the O
tokenization O
groups O
. O
This O
is O
discordant O
with O
the O
machine O
translation O
results O
in O
Section O
5.1.4 O
, O
where O
a O
larger O
vocabulary O
size O
does O
not O
guarantee O
better O
performance O
for O
the O
Subword O
and O
Morpheme O
- O
aware O
Subword O
models O
. O
6 O
Discussion O
We O
further O
examine O
which O
factors O
with O
respect O
to O
tokenization O
affect O
the O
Ko O
- O
En O
and O
En O
- O
Ko O
translation O
performance O
. O
6.1 O
Token O
Length O
Because O
tokenization O
involves O
splitting O
a O
text O
into O
shorter O
segments O
, O
we O
Ô¨Ånd O
it O
important O
to O
Ô¨Ågure O
out O
how O
much O
information O
each O
segment O
bears O
. O
To O
this O
end O
, O
based O
on O
the O
assumption O
that O
the O
longer O
a O
text O
is O
, O
the O
more O
information O
it O
is O
likely O
to O
have O
, O
we O
plot O
the O
BLEU O
scores O
by O
the O
average O
number O
of O
syllables O
per O
Korean O
token O
in O
the O
translation O
testsets O
in O
Figure O
1 O
. O
The O
BLEU O
scores O
of O
the O
subword O
models O
‚Äî O
Syllable O
, O
Morpheme O
, O
Subword O
, O
and O
Morphemeaware O
Subword O
‚Äî O
are O
mostly O
higher O
than O
those O
of O
the O
CV O
models O
, O
which O
are O
plotted O
as O
dotted O
lines O
. O
In O
particular O
, O
the O
Syllable O
, O
Subword O
, O
and O
Morphemeaware O
Subword O
models O
between O
1.00 O
and O
1.50 O
show O
the O
best O
scores O
both O
in O
Ko O
- O
En O
and O
in O
En O
- O
Ko O
. O
When O
a O
token O
has O
more O
than O
1.5 O
syllables O
on O
average O
, O
the O
scores O
begin O
to O
decrease O
, O
and O
the O
Word O
models O
which O
has O
more O
than O
2.5 O
syllables O
in O
a O
token O
performs O
the O
worst O
( O
7.07 O
for O
Ko O
- O
En O
and O
18.42 O
for O
En O
- O
Ko O
) O
. O
Note O
that O
they O
are O
not O
in O
the O
Ô¨Ågures O
due O
to O
space O
constraints O
. O
6.2 O
Linguistic O
Awareness O
Obviously O
token O
length O
is O
not O
the O
only O
key O
factor O
in O
tokenization O
strategies O
. O
Let O
us O
compare O
the O
Morpheme O
- O
aware O
Subword O
16 O
K O
models O
( O
green O
markers O
) O
and O
Subword O
8 O
K O
models O
( O
red O
markers O
) O
in O
the O
shaded O
regions O
in O
Figure O
1 O
. O
Although O
they O
have O
the O
same O
average O
token O
length O
around O
1.4 O
, O
the O
Morpheme O
- O
aware O
Subword O
models O
outperform O
the O
Subword O
models O
. O
We O
believe O
this O
is O
evidence O
to O
support O
that O
linguistic O
awareness O
is O
another O
important O
factor O
in O
Korean O
tokenization O
strategies O
for O
machine O
translation O
. O
6.3 O
Under O
- O
trained O
Tokens O
In O
section O
5.1.4 O
, O
we O
pointed O
out O
high O
OOV O
rates O
are O
highly O
likely O
to O
degrade O
the O
performance O
of139n=1 O
n=20 O
n=40 O
n=60 O
n=80 O
n=1000.00.51.01.52.02.53.03.5Percentage O
of O
tokens O
that O
appear O
in O
training O
set O
less O
than O
n O
times O
( O
% O
) O
CV O
166 O
Syllable O
2 O
K O
Morpheme O
32 O
K O
Subword O
32 O
K O
Morpheme O
- O
aware O
Subword O
32KFigure O
2 O
: O
Percentage O
of O
under O
- O
trained O
tokens O
in O
various O
tokenization O
strategies O
Morpheme O
models O
. O
It O
is O
also O
worth O
noting O
that O
in O
Figure O
1 O
as O
most O
of O
the O
orange O
markers O
denoting O
Morpheme O
models O
are O
below O
the O
dotted O
lines O
. O
OOVs O
are O
the O
tokens O
that O
appear O
only O
in O
the O
test O
set O
. O
They O
are O
an O
extreme O
case O
of O
under O
- O
trained O
tokens O
‚Äî O
test O
set O
‚Äôs O
tokens O
that O
appear O
in O
the O
training O
set O
for O
the O
limited O
number O
of O
times O
. O
Figure O
2 O
shows O
how O
much O
under O
- O
trained O
tokens O
account O
for O
in O
each O
model O
, O
ranging O
from O
n= O
1ton= O
100 O
, O
wheren O
is O
the O
frequency O
of O
the O
under O
- O
trained O
tokens O
in O
the O
training O
set O
. O
Clearly O
, O
the O
curve O
of O
the O
Morpheme O
32 O
K O
model O
is O
far O
above O
that O
of O
the O
others O
, O
indicating O
that O
it O
suffers O
from O
the O
problem O
of O
under O
- O
trained O
tokens O
the O
most O
. O
7 O
Conclusion O
We O
explored O
various O
Korean O
tokenization O
strategies O
on O
machine O
translation O
and O
Ô¨Åve O
NLU O
tasks O
. O
In O
machine O
translation O
Morpheme O
- O
aware O
Subword O
models O
with O
a O
vocabulary O
size O
worked O
best O
for O
both O
Korean O
to O
English O
and O
English O
to O
Korean O
settings O
. O
By O
contrast O
, O
there O
was O
no O
single O
best O
tokenization O
strategy O
for O
the O
NLU O
tasks O
. O
Instead O
, O
Subword O
64 O
K O
models O
showed O
the O
best O
performance O
on O
KorQuAD O
, O
whereas O
Morpheme O
- O
aware O
Subword O
64 O
K O
models O
turned O
out O
to O
be O
optimal O
for O
the O
other O
KorNLI O
, O
KorSTS O
, O
NSMC O
, O
and O
PAWS O
- O
X O
tasks O
. O
Acknowledgments O
We O
are O
grateful O
to O
the O
anonymous O
reviewers O
for O
their O
valuable O
comments O
. O
For O
pre O
- O
training O
models O
, O
we O
used O
Cloud O
TPUs O
provided O
by O
TensorFlow O
Research O
Cloud O
program O
. O
References O
Tamali O
Banerjee O
and O
Pushpak O
Bhattacharyya O
. O
2018 O
. O
Meaningless O
yet O
meaningful O
: O
Morphology O
grounded O
subword O
- O
level O
NMT O
. O
In O
Proceedings O
of O
the O
Second O
Workshop O
on O
Subword O
/ O
Character O
LEvel O
Models O
, O
pages O
55‚Äì60 O
, O
New O
Orleans O
. O
Association O
for O
Computational O
Linguistics O
. O
Kaj O
Bostrom O
and O
Greg O
Durrett O
. O
2020 O
. O
Byte O
pair O
encoding O
is O
suboptimal O
for O
language O
model O
pretraining O
. O
arXiv O
preprint O
arXiv:2004.03720 O
. O
Samuel O
R. O
Bowman O
, O
Gabor O
Angeli O
, O
Christopher O
Potts O
, O
and O
Christopher O
D. O
Manning O
. O
2015 O
. O
A O
large O
annotated O
corpus O
for O
learning O
natural O
language O
inference O
. O
InProceedings O
of O
the O
2015 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
632‚Äì642 O
, O
Lisbon O
, O
Portugal O
. O
Association O
for O
Computational O
Linguistics O
. O
Daniel O
Cer O
, O
Mona O
Diab O
, O
Eneko O
Agirre O
, O
I O
Àúnigo O
LopezGazpio O
, O
and O
Lucia O
Specia O
. O
2017 O
. O
SemEval-2017 O
task O
1 O
: O
Semantic O
textual O
similarity O
multilingual O
and O
crosslingual O
focused O
evaluation O
. O
In O
Proceedings O
of O
the O
11th O
International O
Workshop O
on O
Semantic O
Evaluation O
( O
SemEval-2017 O
) O
, O
pages O
1‚Äì14 O
, O
Vancouver O
, O
Canada O
. O
Association O
for O
Computational O
Linguistics O
. O
Alexis O
Conneau O
, O
Ruty O
Rinott O
, O
Guillaume O
Lample O
, O
Adina O
Williams O
, O
Samuel O
Bowman O
, O
Holger O
Schwenk O
, O
and O
Veselin O
Stoyanov O
. O
2018 O
. O
XNLI O
: O
Evaluating O
cross O
- O
lingual O
sentence O
representations O
. O
In O
Proceedings O
of O
the O
2018 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
2475‚Äì2485 O
, O
Brussels O
, O
Belgium O
. O
Association O
for O
Computational O
Linguistics O
. O
Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O
BERT O
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
understanding O
. O
In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
4171‚Äì4186 O
, O
Minneapolis O
, O
Minnesota O
. O
Association O
for O
Computational O
Linguistics O
. O
Philip O
Gage O
. O
1994 O
. O
A O
new O
algorithm O
for O
data O
compression O
. O
C O
Users O
J. O
, O
12(2):23‚Äì38 O
. O
Jiyeon O
Ham O
, O
Yo O
Joong O
Choe O
, O
Kyubyong O
Park O
, O
Ilji O
Choi O
, O
and O
Hyungjoon O
Soh O
. O
2020 O
. O
KorNLI O
and O
KorSTS O
: O
New O
benchmark O
datasets O
for O
korean O
natural O
language O
understanding O
. O
arXiv O
preprint O
arXiv:2004.03289 O
. O
Beom O
- O
mo O
Kang O
and O
Hung O
- O
gyu O
Kim O
. O
2001 O
. O
21st O
century O
sejong O
project O
- O
compiling O
korean O
corpora O
. O
In O
Proceedings O
of O
the O
19th O
International O
Conference O
on O
Computer O
Processing O
of O
Oriental O
Languages O
( O
ICCPOL O
2001 O
) O
, O
pages O
180‚Äì183 O
. O
Diederik O
P O
Kingma O
and O
Jimmy O
Ba O
. O
2015 O
. O
Adam O
: O
A O
method O
for O
stochastic O
optimization O
. O
3rd O
International O
Conference O
on O
Learning O
Representations O
, O
ICLR O
2015 O
.140Taku O
Kudo O
. O
2006 O
. O
Mecab O
: O
Yet O
another O
part O
- O
ofspeech O
and O
morphological O
analyzer O
. O
https:// O
sourceforge.net/projects/mecab/ O
. O
Taku O
Kudo O
and O
John O
Richardson O
. O
2018 O
. O
SentencePiece O
: O
A O
simple O
and O
language O
independent O
subword O
tokenizer O
and O
detokenizer O
for O
neural O
text O
processing O
. O
In O
Proceedings O
of O
the O
2018 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
: O
System O
Demonstrations O
, O
pages O
66‚Äì71 O
, O
Brussels O
, O
Belgium O
. O
Association O
for O
Computational O
Linguistics O
. O
Seungyoung O
Lim O
, O
Myungji O
Kim O
, O
and O
Jooyoul O
Lee O
. O
2019 O
. O
KorQuAD O
1.0 O
: O
Korean O
qa O
dataset O
for O
machine O
reading O
comprehension O
. O
arXiv O
preprint O
arXiv:1909.07005 O
. O
Pierre O
Lison O
and O
J O
¬®org O
Tiedemann O
. O
2016 O
. O
OpenSubtitles2016 O
: O
Extracting O
large O
parallel O
corpora O
from O
movie O
and O
TV O
subtitles O
. O
In O
Proceedings O
of O
the O
Tenth O
International O
Conference O
on O
Language O
Resources O
and O
Evaluation O
( O
LREC‚Äô16 O
) O
, O
pages O
923‚Äì929 O
, O
PortoroÀáz O
, O
Slovenia O
. O
European O
Language O
Resources O
Association O
( O
ELRA O
) O
. O
Yinhan O
Liu O
, O
Myle O
Ott O
, O
Naman O
Goyal O
, O
Jingfei O
Du O
, O
Mandar O
Joshi O
, O
Danqi O
Chen O
, O
Omer O
Levy O
, O
Mike O
Lewis O
, O
Luke O
Zettlemoyer O
, O
and O
Veselin O
Stoyanov O
. O
2019 O
. O
Roberta O
: O
A O
robustly O
optimized O
bert O
pretraining O
approach O
. O
arXiv O
preprint O
arXiv:1907.11692 O
. O
Ilya O
Loshchilov O
and O
Frank O
Hutter O
. O
2019 O
. O
Decoupled O
weight O
decay O
regularization O
. O
7th O
International O
Conference O
on O
Learning O
Representations O
, O
ICLR O
2019 O
. O
Sangwhan O
Moon O
and O
Naoaki O
Okazaki O
. O
2020 O
. O
Jamo O
pair O
encoding O
: O
Subcharacter O
representation O
- O
based O
extreme O
Korean O
vocabulary O
compression O
for O
efÔ¨Åcient O
subword O
tokenization O
. O
In O
Proceedings O
of O
The O
12th O
Language O
Resources O
and O
Evaluation O
Conference O
, O
pages O
3490‚Äì3497 O
, O
Marseille O
, O
France O
. O
European O
Language O
Resources O
Association O
. O
Toshiaki O
Nakazawa O
, O
Chenchen O
Ding O
, O
Hideya O
Mino O
, O
Isao O
Goto O
, O
Graham O
Neubig O
, O
and O
Sadao O
Kurohashi O
. O
2016 O
. O
Overview O
of O
the O
3rd O
workshop O
on O
Asian O
translation O
. O
In O
Proceedings O
of O
the O
3rd O
Workshop O
on O
Asian O
Translation O
( O
WAT2016 O
) O
, O
pages O
1‚Äì46 O
, O
Osaka O
, O
Japan O
. O
The O
COLING O
2016 O
Organizing O
Committee O
. O
Toshiaki O
Nakazawa O
, O
Nobushige O
Doi O
, O
Shohei O
Higashiyama O
, O
Chenchen O
Ding O
, O
Raj O
Dabre O
, O
Hideya O
Mino O
, O
Isao O
Goto O
, O
Win O
Pa O
Pa O
, O
Anoop O
Kunchukuttan O
, O
Shantipriya O
Parida O
, O
Ond O
Àárej O
Bojar O
, O
and O
Sadao O
Kurohashi O
. O
2019 O
. O
Overview O
of O
the O
6th O
workshop O
on O
Asian O
translation O
. O
In O
Proceedings O
of O
the O
6th O
Workshop O
on O
Asian O
Translation O
, O
pages O
1‚Äì35 O
, O
Hong O
Kong O
, O
China O
. O
Association O
for O
Computational O
Linguistics O
. O
Toshiaki O
Nakazawa O
, O
Shohei O
Higashiyama O
, O
Chenchen O
Ding O
, O
Hideya O
Mino O
, O
Isao O
Goto O
, O
Hideto O
Kazawa O
, O
Yusuke O
Oda O
, O
Graham O
Neubig O
, O
and O
Sadao O
Kurohashi O
. O
2017 O
. O
Overview O
of O
the O
4th O
workshop O
on O
Asian O
translation O
. O
In O
Proceedings O
of O
the O
4th O
Workshop O
on O
Asian O
Translation O
( O
WAT2017 O
) O
, O
pages O
1‚Äì54 O
, O
Taipei O
, O
Taiwan O
. O
Asian O
Federation O
of O
Natural O
Language O
Processing O
. O
Toshiaki O
Nakazawa O
, O
Hideya O
Mino O
, O
Isao O
Goto O
, O
Graham O
Neubig O
, O
Sadao O
Kurohashi O
, O
and O
Eiichiro O
Sumita O
. O
2015 O
. O
Overview O
of O
the O
2nd O
workshop O
on O
Asian O
translation O
. O
In O
Proceedings O
of O
the O
2nd O
Workshop O
on O
Asian O
Translation O
( O
WAT2015 O
) O
, O
pages O
1‚Äì28 O
, O
Kyoto O
, O
Japan O
. O
Workshop O
on O
Asian O
Translation O
. O
Toshiaki O
Nakazawa O
, O
Katsuhito O
Sudoh O
, O
Shohei O
Higashiyama O
, O
Chenchen O
Ding O
, O
Raj O
Dabre O
, O
Hideya O
Mino O
, O
Isao O
Goto O
, O
Win O
Pa O
Pa O
, O
Anoop O
Kunchukuttan O
, O
and O
Sadao O
Kurohashi O
. O
2018 O
. O
Overview O
of O
the O
5th O
workshop O
on O
Asian O
translation O
. O
In O
Proceedings O
of O
the O
32nd O
PaciÔ¨Åc O
Asia O
Conference O
on O
Language O
, O
Information O
and O
Computation O
: O
5th O
Workshop O
on O
Asian O
Translation O
: O
5th O
Workshop O
on O
Asian O
Translation O
, O
Hong O
Kong O
. O
Association O
for O
Computational O
Linguistics O
. O
Myle O
Ott O
, O
Sergey O
Edunov O
, O
Alexei O
Baevski O
, O
Angela O
Fan O
, O
Sam O
Gross O
, O
Nathan O
Ng O
, O
David O
Grangier O
, O
and O
Michael O
Auli O
. O
2019 O
. O
fairseq O
: O
A O
fast O
, O
extensible O
toolkit O
for O
sequence O
modeling O
. O
In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Demonstrations O
) O
, O
pages O
48‚Äì53 O
, O
Minneapolis O
, O
Minnesota O
. O
Association O
for O
Computational O
Linguistics O
. O
Chanjun O
Park O
, O
Gyeongmin O
kim O
, O
and O
Heuiseok O
Lim O
. O
2019a O
. O
Parallel O
corpus O
Ô¨Åltering O
and O
koreanoptimized O
subword O
tokenization O
for O
machine O
translation O
. O
In O
Proceedings O
of O
the O
31st O
Annual O
Conference O
on O
Human O
& O
Cognitive O
Language O
Technology O
. O
Cheoneum O
Park O
, O
Young O
- O
Jun O
Jung O
, O
Kihoon O
Kim O
, O
Geonyeong O
Kim O
, O
Jae O
- O
Won O
Jeon O
, O
Seongmin O
Lee O
, O
Junseok O
Kim O
, O
and O
Changki O
Lee O
. O
2019b O
. O
KNUHYUNDAI O
‚Äôs O
NMT O
system O
for O
scientiÔ¨Åc O
paper O
and O
patent O
tasks O
on O
WAT O
2019 O
. O
In O
Proceedings O
of O
the O
6th O
Workshop O
on O
Asian O
Translation O
, O
pages O
81‚Äì89 O
, O
Hong O
Kong O
, O
China O
. O
Association O
for O
Computational O
Linguistics O
. O
Matthew O
Peters O
, O
Mark O
Neumann O
, O
Mohit O
Iyyer O
, O
Matt O
Gardner O
, O
Christopher O
Clark O
, O
Kenton O
Lee O
, O
and O
Luke O
Zettlemoyer O
. O
2018 O
. O
Deep O
contextualized O
word O
representations O
. O
In O
Proceedings O
of O
the O
2018 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
Papers O
) O
, O
pages O
2227‚Äì2237 O
, O
New O
Orleans O
, O
Louisiana O
. O
Association O
for O
Computational O
Linguistics O
. O
M¬Øarcis O
Pinnis O
, O
Rihards O
Kri O
Àáslauks O
, O
Daiga O
Deksne O
, O
and O
Toms O
Miks O
. O
2017 O
. O
Neural O
machine O
translation O
for O
morphologically O
rich O
languages O
with O
improved O
subword O
units O
and O
synthetic O
data O
. O
In O
International O
Conference O
on O
Text O
, O
Speech O
, O
and O
Dialogue O
, O
pages O
237 O
‚Äì O
245 O
. O
Springer O
. O
Pranav O
Rajpurkar O
, O
Jian O
Zhang O
, O
Konstantin O
Lopyrev O
, O
and O
Percy O
Liang O
. O
2016 O
. O
SQuAD O
: O
100,000 O
+ O
questions O
for O
machine O
comprehension O
of O
text O
. O
In O
Proceedings O
of O
the O
2016 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
2383‚Äì2392 O
, O
Austin O
, O
Texas O
. O
Association O
for O
Computational O
Linguistics.141Rico O
Sennrich O
, O
Barry O
Haddow O
, O
and O
Alexandra O
Birch O
. O
2016a O
. O
Improving O
neural O
machine O
translation O
models O
with O
monolingual O
data O
. O
In O
Proceedings O
of O
the O
54th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
86‚Äì96 O
, O
Berlin O
, O
Germany O
. O
Association O
for O
Computational O
Linguistics O
. O
Rico O
Sennrich O
, O
Barry O
Haddow O
, O
and O
Alexandra O
Birch O
. O
2016b O
. O
Neural O
machine O
translation O
of O
rare O
words O
with O
subword O
units O
. O
In O
Proceedings O
of O
the O
54th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
1715 O
‚Äì O
1725 O
, O
Berlin O
, O
Germany O
. O
Association O
for O
Computational O
Linguistics O
. O
Ahmed O
TawÔ¨Åk O
, O
Mahitab O
Emam O
, O
Khaled O
Essam O
, O
Robert O
Nabil O
, O
and O
Hany O
Hassan O
. O
2019 O
. O
Morphology O
- O
aware O
word O
- O
segmentation O
in O
dialectal O
Arabic O
adaptation O
of O
neural O
machine O
translation O
. O
In O
Proceedings O
of O
the O
Fourth O
Arabic O
Natural O
Language O
Processing O
Workshop O
, O
pages O
11‚Äì17 O
, O
Florence O
, O
Italy O
. O
Association O
for O
Computational O
Linguistics O
. O
Ashish O
Vaswani O
, O
Noam O
Shazeer O
, O
Niki O
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion O
Jones O
, O
Aidan O
N O
Gomez O
, O
≈Å O
ukasz O
Kaiser O
, O
and O
Illia O
Polosukhin O
. O
2017 O
. O
Attention O
is O
all O
you O
need O
. O
In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
30 O
, O
pages O
5998‚Äì6008 O
. O
Curran O
Associates O
, O
Inc. O
Adina O
Williams O
, O
Nikita O
Nangia O
, O
and O
Samuel O
Bowman O
. O
2018 O
. O
A O
broad O
- O
coverage O
challenge O
corpus O
for O
sentence O
understanding O
through O
inference O
. O
In O
Proceedings O
of O
the O
2018 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
Papers O
) O
, O
pages O
1112‚Äì1122 O
. O
Association O
for O
Computational O
Linguistics O
. O
Thomas O
Wolf O
, O
Lysandre O
Debut O
, O
Victor O
Sanh O
, O
Julien O
Chaumond O
, O
Clement O
Delangue O
, O
Anthony O
Moi O
, O
Pierric O
Cistac O
, O
Tim O
Rault O
, O
R‚Äôemi O
Louf O
, O
Morgan O
Funtowicz O
, O
and O
Jamie O
Brew O
. O
2019 O
. O
Huggingface O
‚Äôs O
transformers O
: O
State O
- O
of O
- O
the O
- O
art O
natural O
language O
processing O
. O
ArXiv O
, O
abs/1910.03771 O
. O
Yinfei O
Yang O
, O
Yuan O
Zhang O
, O
Chris O
Tar O
, O
and O
Jason O
Baldridge O
. O
2019 O
. O
PAWS O
- O
x O
: O
A O
cross O
- O
lingual O
adversarial O
dataset O
for O
paraphrase O
identiÔ¨Åcation O
. O
In O
Proceedings O
of O
the O
2019 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
and O
the O
9th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
( O
EMNLP O
- O
IJCNLP O
) O
, O
pages O
3687‚Äì3692 O
, O
Hong O
Kong O
, O
China O
. O
Association O
for O
Computational O
Linguistics.142Proceedings O
of O
the O
1st O
Conference O
of O
the O
Asia O
- O
PaciÔ¨Åc O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
10th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
143‚Äì153 O
December O
4 O
- O
7 O
, O
2020 O
. O
¬© O
2020 O
Association O
for O
Computational O
Linguistics O
BERT O
- O
Based O
Neural O
Collaborative O
Filtering O
and O
Fixed O
- O
Length O
Contiguous O
Tokens O
Explanation O
Reinald O
Adrian O
Pugoy1,2and O
Hung O
- O
Yu O
Kao1 O
1National O
Cheng O
Kung O
University O
, O
Tainan O
City O
, O
Taiwan O
2University O
of O
the O
Philippines O
Open O
University O
, O
Los O
Ba O
Àúnos O
, O
Philippines O
rdpugoy@up.edu.ph O
, O
hykao@mail.ncku.edu.tw O
Abstract O
We O
propose O
a O
novel O
, O
accurate O
, O
and O
explainable O
recommender O
model O
( O
BENEFICT O
) O
that O
addresses O
two O
drawbacks O
that O
most O
reviewbased O
recommender O
systems O
face O
. O
First O
is O
their O
utilization O
of O
traditional O
word O
embeddings O
that O
could O
inÔ¨Çuence O
prediction O
performance O
due O
to O
their O
inability O
to O
model O
the O
word O
semantics O
‚Äô O
dynamic O
characteristic O
. O
Second O
is O
their O
black O
- O
box O
nature O
that O
makes O
the O
explanations O
behind O
every O
prediction O
obscure O
. O
Our O
model O
uniquely O
integrates O
three O
key O
elements O
: O
BERT O
, O
multilayer O
perceptron O
, O
and O
maximum O
subarray O
problem O
to O
derive O
contextualized O
review O
features O
, O
model O
user O
- O
item O
interactions O
, O
and O
generate O
explanations O
, O
respectively O
. O
Our O
experiments O
show O
that O
BENEFICT O
consistently O
outperforms O
other O
state O
- O
of O
- O
the O
- O
art O
models O
by O
an O
average O
improvement O
gain O
of O
nearly O
7 O
% O
. O
Based O
on O
the O
human O
judges O
‚Äô O
assessment O
, O
the O
BENEFICT O
- O
produced O
explanations O
can O
capture O
the O
essence O
of O
the O
customer O
‚Äôs O
preference O
and O
help O
future O
customers O
make O
purchasing O
decisions O
. O
To O
the O
best O
of O
our O
knowledge O
, O
our O
model O
is O
one O
of O
the O
Ô¨Årst O
recommender O
models O
to O
utilize O
BERT O
for O
neural O
collaborative O
Ô¨Åltering O
. O
1 O
Introduction O
In O
recommender O
systems O
research O
, O
collaborative O
Ô¨Åltering O
( O
CF O
) O
is O
the O
dominant O
state O
- O
of O
- O
the O
- O
art O
recommendation O
model O
, O
which O
primarily O
focuses O
on O
learning O
accurate O
representations O
of O
users O
( O
user O
preferences O
) O
and O
items O
( O
item O
characteristics O
) O
( O
Chen O
et O
al O
. O
, O
2018 O
; O
Tay O
et O
al O
. O
, O
2018 O
) O
. O
The O
earliest O
recommender O
models O
learned O
these O
representations O
based O
on O
user O
- O
given O
numeric O
ratings O
that O
each O
item O
received O
( O
Mnih O
and O
Salakhutdinov O
, O
2008 O
; O
Koren O
et O
al O
. O
, O
2009 O
) O
. O
However O
, O
ratings O
, O
which O
are O
values O
on O
a O
single O
discrete O
scale O
, O
oversimplify O
user O
preferences O
and O
item O
characteristics O
( O
Musto O
et O
al O
. O
, O
2017 O
) O
. O
The O
large O
amount O
of O
users O
and O
items O
in O
a O
typical O
online O
platform O
consequently O
results O
in O
a O
highlysparse O
rating O
matrix O
, O
making O
it O
hard O
to O
learn O
accurate O
representations O
( O
Zheng O
et O
al O
. O
, O
2017 O
) O
. O
To O
alleviate O
these O
issues O
, O
review O
texts O
have O
instead O
been O
utilized O
to O
model O
such O
representations O
for O
subsequent O
recommendation O
and O
rating O
prediction O
, O
and O
this O
approach O
has O
attracted O
growing O
attention O
in O
research O
( O
Catherine O
and O
Cohen O
, O
2017 O
; O
Zheng O
et O
al O
. O
, O
2017 O
) O
. O
The O
main O
advantage O
of O
reviews O
as O
the O
source O
of O
features O
is O
that O
they O
can O
cover O
user O
opinions O
‚Äô O
multi O
- O
faceted O
substance O
. O
Because O
users O
can O
explain O
their O
reasons O
underlying O
their O
given O
ratings O
, O
reviews O
contain O
a O
large O
amount O
of O
latent O
information O
that O
is O
both O
rich O
and O
valuable O
, O
and O
that O
can O
not O
be O
otherwise O
obtained O
from O
ratings O
alone O
( O
Chen O
et O
al O
. O
, O
2018 O
; O
Wang O
et O
al O
. O
, O
2019 O
) O
. O
Recently O
, O
models O
that O
incorporate O
user O
reviews O
have O
yielded O
state O
- O
of O
- O
the O
- O
art O
performances O
( O
Zheng O
et O
al O
. O
, O
2017 O
; O
Chen O
et O
al O
. O
, O
2018 O
) O
. O
These O
approaches O
learn O
user O
and O
item O
representations O
by O
using O
traditional O
word O
embeddings O
( O
e.g. O
, O
word2vec O
, O
GloVe O
) O
to O
map O
each O
word O
in O
the O
review O
into O
its O
corresponding O
vector O
. O
The O
review O
is O
transformed O
into O
an O
embedded O
matrix O
before O
being O
fed O
to O
a O
convolutional O
neural O
network O
( O
CNN O
) O
( O
Chen O
et O
al O
. O
, O
2018 O
) O
. O
CNNs O
have O
been O
shown O
to O
effectively O
model O
reviews O
and O
have O
illustrated O
outstanding O
results O
in O
numerous O
natural O
language O
processing O
tasks O
( O
Wang O
et O
al O
. O
, O
2018a O
) O
. O
Nevertheless O
, O
there O
are O
drawbacks O
that O
most O
review O
- O
based O
recommender O
models O
experience O
. O
First O
is O
the O
utilization O
of O
traditional O
or O
mainstream O
word O
embeddings O
to O
learn O
review O
features O
. O
Their O
static O
nature O
is O
a O
hindrance O
, O
as O
each O
word O
sense O
is O
associated O
with O
the O
same O
embedding O
regardless O
of O
the O
context O
. O
In O
other O
words O
, O
such O
embeddings O
can O
not O
identify O
the O
dynamic O
nature O
of O
each O
word O
‚Äôs O
semantics O
. O
For O
review O
- O
based O
recommenders O
, O
this O
could O
be O
an O
issue O
in O
modeling O
users O
and O
items O
, O
which O
could O
, O
in O
turn O
, O
affect O
recommendation O
performance O
( O
Pilehvar O
and O
Camacho O
- O
Collados O
, O
2019 O
) O
. O
Also O
, O
once O
a O
CNN O
is O
fed O
with O
the O
matrix O
of O
word O
embeddings O
, O
the O
word O
frequency O
information O
of O
contextual O
fea-143tures O
, O
said O
to O
be O
crucial O
for O
modeling O
reviews O
, O
will O
be O
lost O
( O
Wang O
et O
al O
. O
, O
2018a O
) O
. O
Another O
drawback O
is O
the O
inherent O
black O
- O
box O
nature O
of O
deep O
learning O
- O
based O
models O
that O
makes O
the O
explanations O
behind O
every O
prediction O
obscure O
( O
Ribeiro O
et O
al O
. O
, O
2016 O
; O
Wang O
et O
al O
. O
, O
2018b O
) O
. O
The O
complex O
architecture O
of O
hidden O
layers O
has O
opaqued O
the O
models O
‚Äô O
internal O
decision O
- O
making O
processes O
( O
Peake O
and O
Wang O
, O
2018 O
) O
. O
Providing O
explanations O
could O
help O
persuade O
users O
to O
make O
decisions O
and O
develop O
trust O
in O
a O
recommender O
system O
( O
Zhang O
et O
al O
. O
, O
2014 O
; O
Ribeiro O
et O
al O
. O
, O
2016 O
; O
Costa O
et O
al O
. O
, O
2018 O
; O
Peake O
and O
Wang O
, O
2018 O
) O
. O
However O
, O
this O
leads O
us O
to O
a O
dilemma O
, O
i.e. O
, O
a O
trade O
- O
off O
between O
accuracy O
and O
explainability O
. O
Usually O
, O
the O
most O
accurate O
models O
are O
inherently O
complicated O
, O
non O
- O
transparent O
, O
and O
unexplainable O
( O
Zhang O
and O
Chen O
, O
2018 O
) O
. O
The O
same O
is O
also O
true O
for O
explainable O
and O
straightforward O
methods O
that O
sacriÔ¨Åce O
accuracy O
. O
Formulating O
models O
that O
are O
both O
explainable O
and O
accurate O
is O
a O
challenging O
yet O
critical O
research O
agenda O
for O
the O
machine O
learning O
community O
to O
ensure O
that O
we O
derive O
beneÔ¨Åts O
from O
machine O
learning O
fairly O
and O
responsibly O
( O
Peake O
and O
Wang O
, O
2018 O
) O
. O
In O
this O
paper O
, O
we O
propose O
a O
unique O
model O
: O
BERT O
- O
Based O
Neural O
Collaborative O
Filtering O
and O
Fixed O
- O
Length O
Contiguous O
Tokens O
Explanation O
( O
BENEFICT O
) O
. O
Our O
model O
learns O
user O
and O
item O
representations O
simultaneously O
using O
two O
parallel O
networks O
. O
To O
address O
the O
Ô¨Årst O
drawback O
, O
we O
incorporate O
BERT O
as O
a O
key O
component O
in O
each O
parallel O
network O
. O
BERT O
affords O
us O
to O
extract O
more O
meaningful O
, O
contextualized O
features O
adaptable O
to O
arbitrary O
contexts O
; O
such O
features O
can O
not O
be O
derived O
from O
mainstream O
word O
embeddings O
( O
Pilehvar O
and O
CamachoCollados O
, O
2019 O
; O
Zakbik O
et O
al O
. O
, O
2019 O
) O
. O
BERT O
can O
also O
retain O
the O
word O
frequency O
information O
that O
makes O
CNN O
an O
unnecessary O
component O
of O
our O
model O
. O
Once O
user O
and O
item O
representations O
are O
learned O
, O
they O
are O
concatenated O
together O
in O
a O
shared O
hidden O
space O
before O
being O
Ô¨Ånally O
fed O
to O
an O
optimal O
stack O
of O
multilayer O
perceptron O
( O
MLP O
) O
layers O
that O
serve O
as O
BENEFICT O
‚Äôs O
interaction O
function O
. O
To O
address O
the O
second O
drawback O
, O
we O
introduce O
a O
novel O
component O
in O
our O
model O
that O
integrates O
BERT O
‚Äôs O
self O
- O
attention O
and O
an O
implementation O
of O
the O
Ô¨Åxed O
- O
length O
maximum O
subarray O
problem O
( O
MSP O
) O
, O
which O
is O
considered O
to O
be O
a O
classic O
computer O
science O
problem O
. O
BERT O
applies O
self O
- O
attention O
in O
each O
encoder O
layer O
that O
consequently O
produces O
selfattention O
weights O
for O
each O
token O
. O
These O
are O
passedto O
the O
successive O
encoder O
layers O
through O
feedforward O
networks O
. O
We O
argue O
that O
these O
self O
- O
attention O
weights O
can O
be O
the O
basis O
for O
explaining O
rating O
predictions O
. O
Based O
on O
this O
premise O
, O
MSP O
then O
selects O
a O
segment O
or O
subarray O
of O
consecutive O
tokens O
that O
has O
the O
maximum O
possible O
sum O
of O
self O
- O
attention O
weights O
. O
1.1 O
Contributions O
Our O
work O
aims O
to O
Ô¨Åll O
the O
research O
gap O
by O
implementing O
a O
solution O
that O
is O
both O
accurate O
and O
explainable O
. O
We O
propose O
a O
novel O
model O
that O
uniquely O
integrates O
three O
vital O
elements O
, O
i.e. O
, O
BERT O
, O
MLP O
, O
and O
MSP O
, O
to O
derive O
review O
features O
, O
model O
useritem O
interactions O
, O
and O
produce O
possible O
explanations O
. O
To O
the O
best O
of O
our O
knowledge O
, O
BENEFICT O
is O
one O
of O
the O
Ô¨Årst O
review O
- O
based O
recommender O
models O
to O
utilize O
BERT O
for O
neural O
CF O
. O
Also O
, O
to O
the O
extent O
of O
our O
knowledge O
, O
BENEFICT O
is O
one O
of O
the O
Ô¨Årst O
models O
to O
repurpose O
a O
portion O
of O
the O
Neural O
Collaborative O
Filtering O
( O
NCF O
) O
framework O
( O
He O
et O
al O
. O
, O
2017 O
) O
as O
the O
user O
- O
item O
interaction O
function O
for O
review O
- O
based O
, O
explicit O
CF O
. O
Moreover O
, O
our O
experiments O
have O
demonstrated O
that O
our O
model O
achieves O
better O
rating O
prediction O
results O
than O
the O
other O
stateof O
- O
the O
- O
art O
recommender O
models O
. O
2 O
Related O
Work O
and O
Concepts O
Designing O
a O
CF O
model O
involves O
two O
crucial O
steps O
: O
learning O
user O
and O
item O
representations O
and O
modeling O
user O
- O
item O
interactions O
based O
on O
those O
representations O
( O
He O
et O
al O
. O
, O
2018 O
) O
. O
Before O
the O
advancements O
provided O
by O
neural O
networks O
, O
matrix O
factorization O
( O
MF O
) O
was O
the O
dominant O
model O
representing O
users O
and O
items O
as O
vectors O
of O
latent O
factors O
( O
called O
embeddings O
) O
and O
models O
user O
- O
item O
interactions O
using O
the O
inner O
product O
operation O
. O
The O
said O
operation O
leads O
to O
poor O
performance O
because O
it O
is O
sub O
- O
optimal O
for O
learning O
rich O
yet O
complicated O
patterns O
from O
realworld O
data O
( O
He O
et O
al O
. O
, O
2018 O
) O
. O
To O
address O
this O
scenario O
, O
neural O
networks O
( O
NN O
) O
have O
been O
integrated O
into O
recommender O
architectures O
. O
One O
of O
the O
initial O
works O
that O
have O
laid O
the O
foundation O
in O
employing O
NN O
for O
CF O
is O
NCF O
( O
He O
et O
al O
. O
, O
2017 O
) O
. O
Their O
framework O
, O
originally O
implemented O
for O
rating O
- O
based O
, O
implicit O
CF O
, O
learns O
non O
- O
linear O
interactions O
between O
users O
and O
items O
by O
employing O
MLP O
layers O
as O
their O
interaction O
function O
, O
granting O
it O
a O
high O
degree O
of O
non O
- O
linearity O
and O
Ô¨Çexibility O
to O
learn O
meaningful O
interactions O
. O
Two O
common O
designs O
have O
emerged O
when O
it O
comes O
to O
leveraging O
MLP O
layers O
: O
placing144an O
MLP O
above O
either O
the O
concatenated O
user O
- O
item O
embeddings O
( O
He O
et O
al O
. O
, O
2017 O
; O
Bai O
et O
al O
. O
, O
2017 O
) O
or O
the O
element O
- O
wise O
product O
of O
user O
and O
item O
embeddings O
( O
Zhang O
et O
al O
. O
, O
2017 O
; O
Wang O
et O
al O
. O
, O
2017 O
) O
. O
As O
far O
as O
rating O
prediction O
is O
concerned O
, O
two O
notable O
recommender O
models O
have O
yielded O
signiÔ¨Åcant O
state O
- O
of O
- O
the O
- O
art O
prediction O
performances O
. O
DeepCoNN O
is O
the O
Ô¨Årst O
deep O
model O
that O
represents O
users O
and O
items O
from O
reviews O
jointly O
( O
Zheng O
et O
al O
. O
, O
2017 O
) O
. O
It O
consists O
of O
two O
parallel O
, O
CNN O
- O
powered O
networks O
. O
One O
network O
learns O
user O
behavior O
by O
examining O
all O
reviews O
that O
he O
has O
written O
, O
and O
the O
other O
network O
models O
item O
properties O
by O
exploring O
all O
reviews O
that O
it O
has O
received O
. O
A O
shared O
layer O
connects O
these O
two O
networks O
, O
and O
factorization O
machines O
capture O
user O
- O
item O
interactions O
. O
The O
second O
model O
is O
NARRE O
, O
which O
shares O
certain O
similarities O
with O
DeepCoNN O
. O
NARRE O
is O
also O
composed O
of O
two O
parallel O
networks O
for O
user O
and O
item O
modeling O
with O
respective O
CNNs O
to O
process O
reviews O
( O
Chen O
et O
al O
. O
, O
2018 O
) O
. O
Rather O
than O
concatenating O
reviews O
to O
one O
long O
sequence O
the O
same O
way O
that O
DeepCoNN O
does O
, O
their O
model O
introduces O
an O
attention O
mechanism O
that O
learns O
review O
- O
level O
usefulness O
in O
the O
form O
of O
attention O
weights O
. O
These O
weights O
are O
integrated O
into O
user O
and O
item O
representations O
to O
enhance O
the O
embedding O
quality O
and O
the O
subsequent O
prediction O
accuracy O
. O
Both O
DeepCoNN O
and O
NARRE O
employ O
traditional O
word O
embeddings O
. O
Other O
relevant O
studies O
have O
claimed O
to O
provide O
explanations O
for O
recommendations O
such O
as O
EFM O
( O
Zhang O
et O
al O
. O
, O
2014 O
) O
, O
sCVR O
( O
Ren O
et O
al O
. O
, O
2017 O
) O
, O
and O
TriRank O
( O
He O
et O
al O
. O
, O
2015 O
) O
. O
These O
models O
initially O
extract O
aspects O
and O
opinions O
by O
performing O
phraselevel O
sentiment O
analysis O
on O
reviews O
. O
Afterward O
, O
they O
generate O
feature O
- O
level O
explanations O
according O
to O
product O
features O
that O
correspond O
to O
user O
interests O
( O
Chen O
et O
al O
. O
, O
2018 O
) O
. O
However O
, O
these O
models O
have O
some O
limitations O
; O
manual O
preprocessing O
is O
required O
for O
sentiment O
analysis O
and O
feature O
extraction O
, O
and O
the O
explanations O
are O
simple O
extraction O
of O
words O
or O
phrases O
from O
the O
review O
text O
( O
Zhang O
et O
al O
. O
, O
2014 O
; O
Ren O
et O
al O
. O
, O
2017 O
) O
. O
This O
also O
has O
the O
unintended O
effect O
of O
distorting O
the O
reviews O
‚Äô O
original O
meaning O
( O
Ribeiro O
et O
al O
. O
, O
2016 O
; O
Chen O
et O
al O
. O
, O
2018 O
) O
. O
Another O
limitation O
is O
that O
textual O
similarity O
is O
solely O
based O
on O
lexical O
similarity O
; O
this O
implies O
that O
semantic O
meaning O
is O
ignored O
( O
Zheng O
et O
al O
. O
, O
2017 O
; O
Chen O
et O
al O
. O
, O
2018).3 O
Methodology O
BENEFICT O
, O
as O
illustrated O
in O
Figure O
1 O
, O
has O
two O
parallel O
networks O
to O
model O
user O
and O
item O
embeddings O
that O
both O
utilize O
BERT O
. O
Hereafter O
, O
we O
will O
only O
illustrate O
the O
user O
modeling O
process O
because O
the O
same O
is O
also O
observed O
for O
item O
modeling O
, O
with O
their O
inputs O
as O
the O
only O
difference O
. O
3.1 O
Input O
Layer O
and O
BERT O
Encoding O
Given O
an O
input O
set O
of O
user O
- O
written O
reviews O
Vu= O
{ O
Vu1,Vu2, O
... O
,V O
uj}wherejis O
the O
total O
number O
of O
reviews O
from O
user O
u O
, O
Vuis O
fed O
to O
a O
pre O
- O
trained O
BERT O
BASE O
model O
to O
encode O
the O
reviews O
and O
obtain O
their O
respective O
contextualized O
representations O
. O
BERT O
BASE O
consists O
of O
12 O
encoder O
layers O
and O
12 O
self O
- O
attention O
heads O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
. O
It O
also O
has O
a O
hidden O
size O
of O
768 O
, O
which O
we O
will O
directly O
utilize O
later O
as O
the O
Ô¨Åxed O
embedding O
dimension O
. O
Furthermore O
, O
BERT O
requires O
every O
review O
to O
follow O
a O
particular O
format O
. O
For O
this O
purpose O
, O
the O
model O
applies O
WordPiece O
tokenization O
to O
the O
review O
‚Äôs O
input O
sequence O
( O
Wu O
et O
al O
. O
, O
2016 O
) O
. O
The O
format O
is O
comprised O
of O
token O
embeddings O
, O
segment O
embeddings O
, O
position O
embeddings O
, O
and O
padding O
masks O
. O
Because O
rating O
prediction O
is O
not O
a O
sentence O
pairing O
task O
, O
BERT O
takes O
each O
review O
as O
a O
single O
segment O
of O
contiguous O
text O
. O
Typically O
, O
BERT O
supports O
a O
maximum O
sequence O
length O
of O
512 O
tokens O
. O
In O
this O
study O
, O
we O
use O
a O
shorter O
length O
of O
256 O
tokens O
to O
save O
substantial O
memory O
. O
As O
such O
, O
each O
input O
sequence O
is O
truncated O
or O
padded O
accordingly O
. O
The O
newly O
- O
formatted O
input O
sequence O
then O
passes O
through O
a O
stack O
of O
Transformer O
encoders O
to O
obtain O
the O
contextualized O
representations O
of O
reviews O
: O
h[CLS O
] O
, O
u={h[CLS O
] O
, O
u1,h[CLS O
] O
, O
u2, O
... O
,h O
[ O
CLS O
] O
, O
uj O
} O
, O
whereh[CLS O
] O
, O
u‚ààRj√ó768 O
. O
We O
utilize O
the O
hidden O
state O
of O
the O
special O
[ O
CLS O
] O
token O
to O
serve O
as O
the O
review O
‚Äôs O
aggregate O
sequence O
representation O
or O
pooled O
contextualized O
embedding O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
. O
In O
theory O
, O
any O
encoder O
layer O
may O
be O
selected O
to O
provide O
the O
hidden O
state O
of O
[ O
CLS O
] O
as O
the O
review O
‚Äôs O
representation O
. O
We O
select O
the O
twelfth O
layer O
for O
our O
approach O
; O
prior O
studies O
have O
illustrated O
that O
its O
predictive O
capability O
is O
the O
best O
among O
the O
other O
layers O
( O
Sun O
et O
al O
. O
, O
2019 O
) O
. O
3.2 O
Embedding O
Generation O
, O
Multilayer O
Perceptron O
, O
and O
Prediction O
The O
user O
embedding O
( O
user O
feature O
vector O
) O
Pu‚àà O
R1√ó768is O
obtained O
by O
calculating O
the O
average O
of O
the O
[ O
CLS O
] O
representations O
of O
the O
reviews O
written O
by145useru O
, O
given O
by O
the O
formula O
below O
. O
Similarly O
, O
the O
item O
embedding O
( O
item O
feature O
vector O
) O
Qi‚ààR1√ó768 O
can O
be O
generated O
from O
the O
item O
modeling O
network O
. O
Pu=1 O
jj O
/ O
summationdisplay O
t=1h[CLS O
] O
, O
ut O
( O
1 O
) O
Furthermore O
, O
the O
purpose O
of O
incorporating O
an O
MLP O
is O
to O
learn O
the O
interactions O
between O
user O
and O
item O
representations O
and O
to O
model O
the O
CF O
effect O
, O
which O
will O
not O
be O
properly O
covered O
by O
solely O
using O
vector O
concatenation O
or O
element O
- O
wise O
product O
( O
He O
et O
al O
. O
, O
2017 O
) O
. O
Adding O
a O
certain O
number O
of O
hidden O
layers O
on O
top O
of O
the O
concatenated O
user O
- O
item O
embedding O
provides O
further O
Ô¨Çexibility O
and O
non O
- O
linearity O
. O
Formally O
, O
the O
MLP O
component O
of O
BENEFICT O
is O
deÔ¨Åned O
as O
follows O
: O
h0=/bracketleftbig O
Pu O
, O
Qi O
/ O
bracketrightbigT O
h1 O
= O
ReLU O
( O
W1h0+b1 O
) O
hL O
= O
ReLU O
( O
WLhL‚àí1+bL O
) O
ÀÜRui O
= O
WL+1hL+bL+1(2 O
) O
whereh0‚ààR1√ó1536is O
the O
concatenated O
user O
- O
item O
embedding O
in O
the O
shared O
hidden O
space O
; O
hLrepresents O
theL O
- O
th O
MLP O
layer O
; O
WLandbLpertain O
to O
the O
weight O
matrix O
and O
bias O
vector O
of O
the O
L O
- O
th O
layer O
, O
respectively O
; O
and O
ÀÜRuidenotes O
the O
predicted O
rating O
that O
userugives O
to O
item O
i. O
For O
the O
activation O
function O
of O
the O
MLP O
layers O
, O
we O
choose O
the O
rectiÔ¨Åed O
linear O
unit O
( O
ReLU O
) O
, O
which O
generally O
yields O
better O
performance O
than O
other O
activation O
functions O
such O
as O
tanh O
and O
sigmoid O
( O
Glorot O
et O
al O
. O
, O
2011 O
; O
He O
et O
al O
. O
, O
2016 O
, O
2017 O
) O
. O
Concerning O
the O
structure O
, O
our O
model O
‚Äôs O
MLP O
component O
follows O
a O
tower O
pattern O
where O
the O
bottom O
layer O
is O
the O
widest O
, O
and O
every O
subsequent O
top O
layer O
has O
a O
smaller O
number O
of O
neurons O
. O
The O
rationale O
behind O
this O
is O
that O
the O
MLP O
can O
learn O
more O
abstractive O
data O
features O
by O
utilizing O
fewer O
hidden O
units O
for O
the O
top O
layers O
( O
He O
et O
al O
. O
, O
2016 O
) O
. O
In O
our O
implementation O
for O
a O
three O
- O
layered O
MLP O
, O
the O
number O
of O
neurons O
from O
the O
bottom O
layer O
to O
the O
top O
layer O
follows O
this O
pattern O
: O
1536 O
( O
concatenated O
embedding O
) O
‚Üí768 O
( O
MLP O
layer O
1)‚Üí384 O
( O
MLP O
layer O
2 O
) O
‚Üí192 O
( O
MLP O
layer O
3)‚Üí1 O
( O
prediction O
layer O
) O
3.3 O
Learning O
In O
training O
the O
model O
, O
the O
loss O
function O
is O
the O
mean O
squared O
error O
( O
MSE O
) O
given O
by O
this O
formula O
: O
MSE O
= O
1 O
|Tr|/summationdisplay O
u O
, O
i‚ààTr(Rui‚àíÀÜRui)2(3)whereTrrefers O
to O
the O
training O
samples O
or O
instances O
, O
andRuiis O
the O
ground O
- O
truth O
rating O
given O
by O
user O
u O
to O
itemi O
. O
Moreover O
, O
we O
employ O
the O
Adaptive O
Moment O
Estimation O
with O
weight O
decay O
or O
AdamW O
( O
Loshchilov O
and O
Hutter O
, O
2018 O
) O
to O
optimize O
the O
loss O
function O
. O
Based O
on O
the O
original O
Adam O
optimizer O
, O
AdamW O
also O
leverages O
the O
power O
of O
adaptive O
learning O
rates O
during O
training O
. O
This O
makes O
the O
selection O
of O
a O
proper O
learning O
rate O
less O
cumbersome O
that O
consequently O
leads O
to O
faster O
convergence O
( O
Chen O
et O
al O
. O
, O
2018 O
) O
. O
Unlike O
Adam O
, O
AdamW O
implements O
a O
weight O
decay O
Ô¨Åx O
, O
a O
regularization O
technique O
that O
prevents O
weights O
from O
growing O
too O
huge O
and O
is O
proven O
to O
yield O
better O
training O
loss O
and O
generalization O
error O
( O
Loshchilov O
and O
Hutter O
, O
2018 O
) O
. O
3.4 O
Explanation O
Generation O
The O
stack O
of O
BERT O
‚Äôs O
Transformer O
encoders O
also O
provides O
sets O
of O
self O
- O
attention O
weights O
that O
a O
token O
gives O
to O
every O
token O
found O
in O
the O
review O
text O
. O
We O
are O
particularly O
interested O
in O
the O
attention O
that O
[ O
CLS O
] O
gives O
to O
each O
review O
token O
using O
the O
twelfth O
layer O
‚Äôs O
multiple O
attention O
heads O
. O
Given O
an O
input O
sequence O
of O
tokens O
Fujproduced O
by O
WordPiece O
tokenization O
from O
review O
Vuj O
, O
a O
set O
of O
attention O
weights O
is O
represented O
as O
: O
Œ±[CLS O
] O
, O
uj={Œ±k O
1(Fuj),Œ±k O
2(Fuj), O
... O
,Œ±k O
g(Fuj O
) O
} O
( O
4 O
) O
wherekis O
the O
speciÔ¨Åc O
attention O
head O
in O
a O
particular O
encoder O
layer O
, O
and O
Œ±k O
gis O
the O
attention O
that O
[ O
CLS O
] O
gives O
to O
the O
g O
- O
th O
WordPiece O
token O
over O
the O
input O
sequenceFuj O
. O
There O
are O
12 O
attention O
heads O
in O
an O
encoder O
layer O
which O
translate O
to O
12 O
different O
attention O
weights O
that O
each O
token O
receives O
from O
the O
[ O
CLS O
] O
token O
. O
For O
a O
given O
token O
g O
, O
the O
following O
formula O
is O
applied O
to O
compress O
the O
weights O
into O
a O
single O
value O
: O
ComAtt O
g=12 O
/ O
summationdisplay O
k=1Œ±k O
g(Fuj O
) O
( O
5 O
) O
We O
then O
reformulate O
the O
task O
of O
generating O
explanations O
as O
a O
Ô¨Åxed O
- O
length O
MSP O
. O
In O
its O
vanilla O
sense O
, O
MSP O
selects O
a O
segment O
of O
consecutive O
array O
elements O
( O
i.e. O
, O
a O
contiguous O
subarray O
of O
tokens O
) O
that O
has O
the O
maximum O
possible O
sum O
over O
all O
other O
segments O
( O
Bae O
, O
2007 O
) O
. O
In O
this O
paper O
, O
we O
introduce O
constraintNto O
the O
MSP O
; O
Nis O
a O
Ô¨Åxed O
value O
that O
pertains O
to O
the O
length O
of O
the O
explanation O
. O
Formally O
, O
the O
set O
of O
compressed O
attention O
weights O
per O
review O
is O
given O
by O
the O
following O
array O
: O
Auj= O
[ O
ComAtt O
1,ComAtt O
2, O
... O
,ComAtt O
g](6)146MLP O
Layer O
1MLP O
Layer O
2MLP O
Layer O
L O
ReLUReLUMSE O
Loss O
... O
Concatenation O
... O
BERT O
... O
BERTUser O
/ O
Item O
Embedding O
Generation O
BERT O
¬†  O
Encoding O
Input O
Layer O
User O
Modeling O
Item O
ModelingCompressed O
AttentionMSP O
Compressed O
AttentionMSPExplanation O
Generation O
Explanation O
¬†  O
GenerationMultilayer O
PerceptronFigure O
1 O
: O
The O
proposed O
BENEFICT O
architecture O
. O
Dataset O
# O
Reviews O
# O
Users O
# O
Items O
Toys O
and O
Games O
167,597 O
19,412 O
11,924 O
Digital O
Music O
64,706 O
5,541 O
3,568 O
Yelp O
- O
Dense O
159,114 O
8,919 O
7,122 O
Yelp O
- O
Sparse O
229,907 O
45,981 O
11,537 O
Table O
1 O
: O
Statistics O
summary O
of O
the O
datasets O
. O
The O
goal O
is O
to O
Ô¨Ånd O
token O
indices O
xandythat O
maximize O
: O
y O
/ O
summationdisplay O
t O
= O
xAuj[t O
] O
( O
7 O
) O
This O
is O
subject O
to O
the O
requirements O
that O
1‚â§x O
< O
y‚â§256 O
and(y‚àíx O
) O
+ O
1 O
= O
N. O
Finally O
, O
the O
generated O
explanation O
for O
review O
Vujis O
represented O
as O
: O
EXP O
uj O
= O
Concat O
( O
Fuj O
, O
x O
, O
Fuj O
, O
x O
+1, O
... O
,F O
uj O
, O
y O
) O
( O
8) O
4 O
Experiments O
In O
this O
section O
, O
we O
perform O
relevant O
experiments O
intending O
to O
answer O
the O
following O
research O
questions O
: O
RQ1 O
: O
Does O
BENEFICT O
outperform O
other O
stateof O
- O
the O
- O
art O
recommender O
models?RQ2 O
: O
What O
is O
the O
optimal O
conÔ¨Åguration O
for O
learning O
user O
- O
item O
interactions O
? O
RQ3 O
: O
Can O
our O
model O
produce O
explanations O
acceptable O
to O
humans O
? O
4.1 O
Datasets O
and O
Experimental O
Settings O
Table O
1 O
summarizes O
the O
four O
public O
datasets O
from O
different O
domains O
used O
in O
our O
study O
. O
Two O
of O
these O
datasets O
are O
Amazon O
5 O
- O
core1 O
: O
Toys O
and O
Games O
, O
which O
consists O
of O
nearly O
168 O
thousand O
reviews O
, O
and O
Digital O
Music O
, O
which O
contains O
about O
65 O
thousand O
reviews O
( O
McAuley O
et O
al O
. O
, O
2015 O
) O
. O
These O
datasets O
are O
said O
to O
be O
5 O
- O
core O
wherein O
users O
and O
items O
have O
Ô¨Åve O
reviews O
each O
. O
We O
also O
utilize O
Yelp2 O
, O
a O
large O
- O
scale O
dataset O
for O
restaurant O
feedback O
and O
ratings O
. O
We O
both O
employ O
its O
original O
, O
sparse O
version O
and O
its O
5core O
, O
dense O
version O
with O
about O
160 O
thousand O
and O
230 O
thousand O
reviews O
, O
respectively O
. O
The O
ratings O
in O
all O
datasets O
are O
in O
the O
range O
of O
[ O
1 O
, O
5 O
] O
. O
We O
randomly O
split O
each O
dataset O
of O
user O
- O
item O
pairs O
into O
training O
( O
80 O
% O
) O
, O
validation O
( O
10 O
% O
) O
, O
and O
test O
( O
10 O
% O
) O
sets O
. O
In O
our O
experiments O
, O
we O
perform O
an O
exhaustive O
grid O
search O
over O
the O
following O
hyperparameters O
: O
number O
of O
epochs O
[ O
1 O
, O
20 O
] O
and O
number O
of O
MLP O
layers O
[ O
0 O
, O
3 O
] O
. O
The O
learning O
rate O
and O
weight O
decay O
are O
both O
set O
to O
1http://jmcauley.ucsd.edu/data/amazon/ O
2https://github.com/danielfrg/kaggle-yelp-recruitingcompetition147Model O
Toys O
and O
GamesDigital O
MusicYelpDenseYelpSparseAverage O
DeepCoNN O
0.8971 O
0.8972 O
1.0311 O
1.2006 O
1.0065 O
NARRE O
0.8840 O
0.8997 O
1.0312 O
1.1770 O
0.9979 O
BENEFICT O
0.8348 O
0.8750 O
0.9963 O
0.9764 O
0.9206 O
‚àÜBENEFICT O
5.57 O
% O
2.47 O
% O
3.38 O
% O
17.04 O
% O
7.11 O
% O
Table O
2 O
: O
RMSE O
comparison O
of O
the O
recommender O
models O
. O
The O
best O
RMSE O
values O
are O
highlighted O
in O
bold O
. O
The O
last O
row O
shows O
the O
improvement O
gained O
by O
BENEFICT O
against O
the O
better O
performing O
baseline O
. O
0.001 O
. O
Due O
to O
memory O
limitations O
, O
the O
batch O
size O
is O
Ô¨Åxed O
at O
32 O
. O
We O
select O
the O
model O
conÔ¨Åguration O
( O
i.e. O
, O
a O
grid O
point O
) O
with O
the O
best O
root O
mean O
square O
error O
( O
RMSE O
) O
on O
the O
validation O
set O
. O
We O
use O
the O
test O
set O
for O
evaluating O
the O
model O
‚Äôs O
Ô¨Ånal O
performance O
. O
4.2 O
Baselines O
and O
Evaluation O
Metric O
To O
validate O
the O
effectiveness O
of O
BENEFICT O
, O
we O
select O
two O
other O
state O
- O
of O
- O
the O
- O
art O
models O
as O
baselines O
: O
‚Ä¢DeepCoNN O
( O
Zheng O
et O
al O
. O
, O
2017 O
): O
It O
is O
a O
deep O
collaborative O
neural O
network O
model O
based O
on O
two O
parallel O
CNNs O
to O
learn O
user O
and O
item O
feature O
vectors O
in O
a O
joint O
manner O
. O
‚Ä¢NARRE O
( O
Chen O
et O
al O
. O
, O
2018 O
): O
Similar O
to O
DeepCoNN O
, O
it O
is O
a O
neural O
attentional O
regression O
model O
that O
integrates O
two O
parallel O
CNNs O
and O
an O
attention O
mechanism O
to O
model O
latent O
features O
. O
Afterward O
, O
we O
calculate O
the O
RMSE O
, O
a O
widely O
used O
metric O
for O
rating O
prediction O
, O
to O
evaluate O
the O
models O
‚Äô O
respective O
performances O
. O
RMSE O
= O
/radicalBigg O
1 O
|Ts|/summationdisplay O
u O
, O
i‚ààTs(Rui‚àíÀÜRui)2(9 O
) O
In O
the O
formula O
, O
Tsdenotes O
the O
test O
samples O
or O
instances O
of O
user O
- O
item O
pairs O
. O
4.3 O
Prediction O
Results O
and O
Discussion O
Table O
2 O
reports O
the O
RMSE O
values O
of O
BENEFICT O
and O
the O
two O
baselines O
, O
with O
the O
last O
row O
( O
represented O
by O
‚àÜBENEFICT O
) O
indicating O
the O
improvement O
gained O
by O
our O
model O
compared O
with O
the O
better O
baseline O
. O
The O
results O
show O
that O
BENEFICT O
consistently O
outperforms O
the O
baselines O
across O
all O
datasets O
; O
our O
model O
has O
an O
average O
RMSE O
score O
of O
0.9206 O
, O
as O
opposed O
to O
1.0065 O
and O
0.9979 O
for O
DeepCoNN O
and O
NARRE O
, O
respectively O
. O
On O
average O
, O
this O
has O
Figure O
2 O
: O
RMSE O
comparison O
of O
BENEFICT O
variants O
using O
different O
user O
- O
item O
interaction O
functions O
. O
The O
solid O
lines O
pertain O
to O
the O
concatenation O
- O
MLP O
interaction O
function O
. O
On O
the O
other O
hand O
, O
the O
broken O
lines O
refer O
to O
the O
interaction O
function O
based O
on O
the O
element O
- O
wise O
product O
( O
EWP O
) O
and O
MLP O
. O
resulted O
in O
the O
improvement O
gained O
by O
BENEFICT O
of O
nearly O
7 O
% O
. O
These O
results O
validate O
our O
hypothesis O
that O
using O
BERT O
- O
derived O
embeddings O
and O
representations O
, O
considered O
to O
be O
more O
semantically O
meaningful O
than O
their O
traditional O
counterparts O
, O
can O
signiÔ¨Åcantly O
improve O
rating O
prediction O
accuracy O
and O
that O
BERT O
can O
likewise O
offset O
the O
limitations O
of O
mainstream O
word O
embeddings O
and O
CNN O
. O
Moreover O
, O
the O
rationale O
of O
employing O
two O
versions O
of O
Yelp O
is O
to O
compare O
the O
recommender O
models O
‚Äô O
performances O
on O
both O
dense O
and O
sparse O
datasets O
. O
As O
illustrated O
in O
the O
fourth O
and O
Ô¨Åfth O
columns O
of O
Table O
2 O
, O
both O
the O
RMSE O
values O
of O
DeepCoNN O
and O
NARRE O
worsen O
when O
they O
attempt O
to O
perform O
predictions O
on O
the O
original O
, O
sparse O
Yelp O
. O
For O
DeepCoNN O
, O
from O
the O
dense O
version O
‚Äôs O
RMSE O
of O
1.0311 O
, O
it O
increases O
to O
1.2006 O
. O
The O
same O
is148Figure O
3 O
: O
Distribution O
of O
the O
judges O
‚Äô O
given O
usefulness O
scores O
based O
on O
US1 O
. O
also O
true O
for O
NARRE O
, O
whose O
RMSE O
increases O
to O
1.1770 O
from O
1.0312 O
. O
Interestingly O
, O
BENEFICT O
produces O
an O
entirely O
different O
observation O
; O
its O
RMSE O
decreases O
to O
0.9764 O
from O
0.9963 O
. O
Our O
model O
‚Äôs O
improvement O
is O
17.04 O
% O
, O
greater O
than O
‚àÜBENEFICT O
for O
the O
three O
other O
datasets O
. O
We O
attribute O
these O
Ô¨Åndings O
to O
the O
greater O
amount O
of O
information O
in O
Yelp O
- O
Sparse O
that O
can O
be O
successfully O
utilized O
by O
BENEFICT O
for O
modeling O
reviews O
. O
It O
should O
be O
noted O
that O
Yelp O
- O
Sparse O
has O
nearly O
230 O
thousand O
reviews O
, O
while O
Yelp O
- O
Dense O
has O
almost O
160 O
thousand O
. O
In O
conclusion O
, O
these O
results O
provide O
evidence O
that O
our O
model O
is O
best O
equipped O
and O
capable O
of O
performing O
predictions O
regardless O
of O
a O
dataset O
‚Äôs O
inherent O
sparsity O
or O
density O
. O
4.3.1 O
Optimal O
Interaction O
Function O
BENEFICT O
employs O
an O
MLP O
above O
the O
concatenated O
user O
- O
item O
embeddings O
in O
the O
shared O
hidden O
space O
. O
We O
compare O
it O
against O
another O
variant O
of O
our O
model O
, O
which O
utilizes O
an O
MLP O
on O
top O
of O
the O
element O
- O
wise O
product O
of O
user O
and O
item O
representations O
. O
We O
examine O
their O
performances O
using O
a O
different O
number O
of O
hidden O
layers O
[ O
0 O
, O
3 O
] O
. O
It O
should O
be O
noted O
that O
an O
MLP O
with O
zero O
layers O
pertains O
to O
the O
shared O
hidden O
space O
‚Äôs O
direct O
projection O
to O
the O
prediction O
layer O
. O
Figure O
2 O
demonstrates O
that O
BENEFICT O
‚Äôs O
utilization O
of O
concatenation O
exceeds O
the O
element O
- O
wise O
product O
by O
a O
signiÔ¨Åcant O
margin O
across O
all O
MLP O
layers O
and O
datasets O
. O
This O
result O
veriÔ¨Åes O
the O
positive O
effect O
of O
feeding O
the O
concatenated O
features O
Figure O
4 O
: O
Distribution O
of O
the O
judges O
‚Äô O
given O
usefulness O
scores O
based O
on O
US2 O
. O
to O
the O
MLP O
to O
learn O
user O
- O
item O
interactions O
. O
Furthermore O
, O
consistent O
with O
the O
Ô¨Åndings O
of O
He O
et O
al O
. O
( O
2017 O
) O
, O
stacking O
more O
layers O
is O
indeed O
beneÔ¨Åcial O
and O
effective O
for O
neural O
explicit O
collaborative O
Ô¨Åltering O
as O
well O
. O
There O
appears O
to O
be O
a O
trend O
: O
increasing O
the O
hidden O
layers O
implies O
decreasing O
( O
and O
better O
) O
RMSE O
values O
. O
Simply O
projecting O
the O
shared O
hidden O
space O
to O
the O
prediction O
layer O
is O
insufÔ¨Åcient O
and O
weak O
, O
as O
evidenced O
by O
its O
relatively O
high O
RMSE O
scores O
. O
On O
the O
contrary O
, O
using O
three O
MLP O
layers O
has O
generally O
resulted O
in O
the O
lowest O
RMSE O
scores O
. O
The O
only O
exception O
is O
with O
the O
Digital O
Music O
dataset O
wherein O
utilizing O
two O
layers O
produces O
the O
best O
RMSE O
value O
. O
Furthermore O
, O
even O
though O
the O
element O
- O
wise O
product O
is O
more O
inferior O
than O
concatenation O
, O
the O
former O
also O
beneÔ¨Åts O
from O
increasing O
the O
MLP O
layers O
. O
In O
summary O
, O
all O
these O
Ô¨Åndings O
validate O
the O
necessity O
of O
incorporating O
the O
MLP O
as O
an O
integral O
part O
of O
the O
whole O
BENEFICT O
model O
. O
5 O
Explainability O
Study O
5.1 O
Human O
Assessment O
of O
Explanations O
To O
validate O
the O
helpfulness O
of O
BENEFICTproduced O
explanations O
in O
real O
life O
, O
we O
also O
generate O
possible O
explanations O
using O
TF O
- O
IDF O
andTextRank O
. O
Applying O
TF O
- O
IDF O
determines O
which O
words O
are O
more O
favorable O
or O
relevant O
in O
a O
corpus O
of O
documents O
( O
Rajaraman O
and O
Ullman O
, O
2011 O
) O
. O
To O
make O
the O
assessment O
fair O
, O
we O
only O
select O
words O
with O
the O
topNTF O
- O
IDF O
scores O
, O
where O
the O
value O
of O
Nis O
the O
same O
as O
the O
constraint O
introduced O
in O
BENEFICT‚Äôs149Explanation O
US O
Scores O
TF O
- O
IDF O
: O
Some O
ofthe O
tracks O
were O
really O
quite O
... O
dare O
I O
say O
it O
, O
catchy O
. O
And O
there O
was O
even O
a O
Top O
30 O
- O
friendly O
single O
on O
the O
album O
( O
‚Äô O
Only O
Time O
will O
tell O
‚Äô O
) O
. O
But O
was O
n‚Äôt O
this O
Carl O
Palmer O
‚Äì O
he O
ofthe70striple O
album O
and O
seriousdevo O
teeofclassical O
percussionist O
James O
Blades O
? O
And O
was O
n‚Äôt O
this O
also O
Steve O
Hose O
‚Äì O
he O
ofanother O
70striple O
album O
and O
several O
serious O
solo O
albums O
. O
And O
had O
n‚Äôt O
John O
Wetton O
starred O
on O
theseriously O
serious O
‚Äô O
Red O
‚Äô O
in O
74 O
? O
How O
could O
the O
three O
come O
together O
yet O
produce O
this O
Adult O
- O
Oriented O
stadium O
Rock?Let O
‚Äôs O
not O
forget O
Palmer O
‚Äôs O
beginnings O
in O
the O
Crazy O
World O
of O
Arthur O
Brown O
and O
Atomic O
Rooster O
. O
Or O
Wetton‚Äôsbizarre O
phase O
with O
Uriah O
Heep O
. O
And O
Geoff O
Downes O
was O
nominally O
half O
of O
‚Äô O
Buggles O
‚Äô O
, O
whose O
minimal O
output O
was O
unashamed O
pop O
. O
The O
style O
of O
this O
, O
Asia O
‚Äôs O
debut O
album O
was O
n‚Äôt O
a O
million O
miles O
from O
UK O
‚Äôs O
eponymous O
LP O
of O
1978 O
, O
although O
it O
was O
distinctly O
more O
mainstream O
. O
I O
like O
this O
album O
, O
the O
best O
of O
all O
theAsia O
output O
that O
I O
‚Äôve O
heard O
. O
I O
would O
have O
preferred O
the O
music O
to O
be O
a O
little O
more O
ambitious O
; O
there O
‚Äôs O
a O
sense O
in O
which O
it O
‚Äôs O
all O
been O
concocted O
to O
maximise O
the O
commercial O
return O
, O
which O
you O
could O
n‚Äôt O
say O
ofUK O
. O
But O
it O
‚Äôs O
a O
good O
, O
undemand O
ing O
listen O
. O
US1 O
: O
1.5 O
US2 O
: O
1.5 O
TextRank O
: O
Some O
of O
the O
tracks O
were O
really O
quite O
... O
dare O
I O
say O
it O
, O
catchy O
. O
And O
there O
was O
even O
a O
Top O
30 O
- O
friendly O
single O
on O
the O
album O
( O
‚Äô O
Only O
Time O
will O
tell O
‚Äô O
) O
. O
Butwasn‚Äôt O
thisCarl O
Palmer O
‚Äì O
heofthe70striple O
album O
andseriousdevo O
teeofclassicalpercussionistJames O
Blades O
? O
And O
was O
n‚Äôt O
this O
also O
Steve O
Hose O
.... O
US1 O
: O
2 O
US2 O
: O
2 O
BENEFICT O
: O
..... O
The O
style O
of O
this O
, O
Asia O
‚Äôs O
debut O
album O
was O
n‚Äôt O
a O
million O
miles O
from O
UK O
‚Äôs O
eponymous O
LP O
of O
1978 O
, O
although O
it O
was O
distinctly O
more O
mainstream O
. O
I O
likethisalbum O
, O
thebestofalltheAsia O
outputthatI‚Äôve O
heard O
. O
Iwould O
have O
preferred O
the O
music O
to O
be O
a O
little O
more O
ambitious O
; O
there O
‚Äôs O
a O
sense O
in O
which O
it O
‚Äôs O
all O
been O
concocted O
to O
maximise O
the O
commercial O
return O
, O
which O
you O
could O
n‚Äôt O
say O
of O
UK O
..... O
US1 O
: O
4 O
US2 O
: O
4 O
Table O
3 O
: O
Sample O
explanations O
( O
highlighted O
in O
yellow O
) O
generated O
by O
TF O
- O
IDF O
, O
TextRank O
, O
and O
BENEFICT O
from O
a O
speciÔ¨Åc O
user O
review O
. O
The O
second O
column O
includes O
the O
average O
judge O
- O
given O
US1 O
and O
US2 O
scores O
. O
explanation O
generation O
module O
. O
On O
the O
other O
hand O
, O
TextRank O
is O
a O
fully O
unsupervised O
, O
graph O
- O
based O
extractive O
summarization O
algorithm O
( O
Mihalcea O
and O
Tarau O
, O
2004 O
) O
. O
Its O
goal O
is O
to O
rank O
entire O
sentences O
that O
comprise O
a O
given O
review O
text O
. O
Also O
, O
to O
make O
the O
assessment O
consistent O
, O
we O
only O
take O
the O
top O
sentence O
with O
a O
length O
of O
less O
than O
or O
equal O
to O
Nfor O
each O
review O
. O
We O
then O
ask O
two O
human O
judges O
to O
evaluate O
a O
total O
of O
90 O
explanations O
, O
30 O
explanations O
each O
for O
TF O
- O
IDF O
, O
TextRank O
, O
and O
BENEFICT O
, O
with O
N= O
20 O
. O
We O
instruct O
them O
to O
score O
each O
explanation O
based O
on O
the O
following O
usefulness O
statements O
( O
US O
) O
on O
a O
Ô¨Åve O
- O
point O
Likert O
scale O
, O
ranging O
from O
1 O
( O
strongly O
disagree O
) O
to O
5 O
( O
strongly O
agree O
) O
. O
US1 O
: O
The O
explanation O
captures O
the O
essence O
of O
the O
customer O
‚Äôs O
preference O
( O
like O
or O
dislike O
) O
in O
the O
review O
. O
US2 O
: O
The O
explanation O
is O
helpful O
for O
you O
or O
any O
customer O
to O
decide O
to O
purchase O
that O
particular O
item O
in O
the O
future O
. O
We O
further O
examine O
the O
human O
assessment O
results O
by O
determining O
the O
strength O
of O
agreement O
between O
the O
two O
judges O
. O
This O
is O
done O
by O
calculatingthe O
Quadratic O
Weighted O
Kappa O
( O
QWK O
) O
statistic O
. O
It O
measures O
inter O
- O
rater O
agreement O
and O
is O
suitable O
for O
ordinal O
or O
ranked O
variables O
. O
The O
Kappa O
metric O
lies O
on O
a O
scale O
of O
-1 O
to O
1 O
, O
where O
1 O
implies O
perfect O
agreement O
, O
0 O
indicates O
random O
agreement O
, O
and O
negative O
values O
mean O
that O
the O
agreement O
is O
less O
than O
chance O
, O
such O
as O
disagreement O
. O
SpeciÔ¨Åcally O
, O
a O
coefÔ¨Åcient O
of O
0.01 O
- O
0.20 O
indicates O
slight O
agreement O
, O
0.21 O
- O
0.40 O
implies O
fair O
agreement O
, O
0.41 O
- O
0.60 O
refers O
to O
moderate O
agreement O
, O
0.61 O
- O
0.80 O
pertains O
to O
substantial O
agreement O
, O
and O
0.81 O
- O
0.99 O
denotes O
nearly O
perfect O
agreement O
( O
Borromeo O
and O
Toyama O
, O
2015 O
) O
. O
5.2 O
Explainability O
Results O
and O
Discussion O
5.2.1 O
Overall O
Assessment O
Figure O
3 O
summarizes O
the O
judges O
‚Äô O
given O
scores O
on O
their O
assessment O
of O
explanations O
based O
on O
US1 O
. O
They O
Ô¨Ånd O
that O
nearly O
58 O
% O
of O
BENEFICT O
- O
derived O
explanations O
capture O
the O
essence O
of O
the O
customer O
‚Äôs O
preference O
( O
i.e. O
, O
those O
with O
usefulness O
scores O
of O
either O
four O
or O
Ô¨Åve O
) O
. O
It O
is O
followed O
by O
TextRank O
, O
with O
almost O
52 O
% O
of O
its O
produced O
explanations O
, O
and O
TFIDF O
, O
with O
only O
1.67 O
% O
of O
its O
generated O
explanations O
. O
With O
respect O
to O
the O
inter O
- O
rater O
agreement O
on O
US1150 O
in O
Table O
5 O
, O
the O
judges O
express O
fair O
agreement O
on O
BENEFICT O
( O
having O
a O
Kappa O
value O
of O
0.2019 O
) O
. O
On O
the O
other O
hand O
, O
they O
slightly O
agree O
with O
each O
other O
on O
both O
TF O
- O
IDF O
and O
TextRank O
, O
with O
QWK O
values O
of O
0.1924 O
and O
0.0625 O
, O
respectively O
. O
As O
Table O
4 O
indicates O
, O
our O
model O
has O
a O
mean O
usefulness O
score O
of O
3.45 O
, O
better O
than O
TextRank O
( O
3.26 O
) O
and O
TF O
- O
IDF O
( O
2.05 O
) O
. O
Figure O
4 O
shows O
the O
judges O
‚Äô O
assessment O
scores O
based O
on O
US2 O
. O
Interestingly O
, O
the O
judges O
express O
that O
nearly O
63 O
% O
of O
the O
explanations O
generated O
by O
BENEFICT O
and O
TextRank O
are O
helpful O
for O
any O
future O
customers O
. O
Upon O
including O
the O
low O
- O
scoring O
explanations O
, O
BENEFICT O
is O
still O
better O
than O
TextRank O
; O
the O
former O
has O
a O
mean O
usefulness O
score O
of O
3.61 O
against O
the O
latter O
‚Äôs O
3.40 O
. O
Furthermore O
, O
the O
judges O
moderately O
agree O
as O
far O
as O
our O
model O
‚Äôs O
generated O
explanation O
is O
concerned O
( O
with O
a O
Kappa O
value O
of O
0.4705 O
) O
. O
At O
the O
same O
time O
, O
they O
express O
less O
than O
chance O
agreement O
for O
TextRank O
( O
obtaining O
a O
Kappa O
value O
of O
-0.0073 O
) O
. O
This O
statement O
means O
that O
the O
large O
majority O
of O
TextRank O
‚Äôs O
high O
assessment O
scores O
come O
from O
one O
judge O
alone O
. O
Lastly O
, O
the O
judges O
observe O
that O
only O
8.33 O
% O
of O
the O
explanations O
from O
TF O
- O
IDF O
are O
helpful O
, O
with O
a O
mean O
usefulness O
score O
of O
2.18 O
and O
a O
QWK O
value O
of O
0.1921 O
, O
which O
implies O
their O
slight O
agreement O
. O
These O
results O
indicate O
that O
BENEFICT O
‚Äôs O
explanation O
generation O
module O
can O
effectively O
provide O
useful O
explanations O
that O
capture O
the O
essence O
of O
the O
customer O
‚Äôs O
preference O
and O
help O
future O
customers O
make O
purchasing O
decisions O
. O
5.2.2 O
SpeciÔ¨Åc O
Example O
Comparison O
Given O
an O
example O
, O
we O
highlight O
words O
that O
serve O
as O
the O
explanations O
in O
Table O
3 O
. O
The O
explanation O
produced O
by O
TF O
- O
IDF O
can O
capture O
a O
few O
important O
words O
, O
such O
as O
unashamed O
andundemanding O
. O
However O
, O
due O
to O
its O
bag O
- O
of O
- O
words O
property O
, O
it O
includes O
several O
other O
unnecessary O
words O
that O
may O
not O
contribute O
to O
the O
explanation O
. O
Therefore O
, O
the O
judges O
do O
not O
Ô¨Ånd O
it O
to O
be O
helpful O
. O
Next O
, O
the O
TextRank O
- O
generated O
explanation O
also O
does O
not O
appear O
to O
capture O
the O
essence O
of O
the O
user O
‚Äôs O
like O
or O
dislike O
. O
It O
does O
not O
seem O
useful O
for O
customers O
to O
decide O
whether O
to O
purchase O
that O
item O
in O
the O
future O
. O
Still O
, O
the O
judges O
give O
TextRank O
higher O
usefulness O
scores O
than O
TF O
- O
IDF O
, O
even O
though O
the O
latter O
captures O
more O
adjectives O
and O
important O
words O
. O
We O
attribute O
this O
to O
human O
‚Äôs O
natural O
bias O
toward O
less O
noisy O
sentences O
that O
express O
complete O
thoughts O
. O
Lastly O
, O
the O
BENEFICT O
- O
produced O
explanation O
con O
- O
Method O
US1 O
Mean O
US2 O
Mean O
TF O
- O
IDF O
2.05 O
2.18 O
TextRank O
3.26 O
3.40 O
BENEFICT O
3.45 O
3.61 O
Table O
4 O
: O
Mean O
usefulness O
scores O
of O
explanations O
assessed O
by O
the O
judges O
, O
based O
on O
US1 O
and O
US2 O
. O
Method O
US1 O
QWK O
US2 O
QWK O
TF O
- O
IDF O
0.1924 O
0.1921 O
TextRank O
0.0625 O
-0.0073 O
BENEFICT O
0.2019 O
0.4705 O
Table O
5 O
: O
The O
strength O
of O
inter O
- O
judge O
agreement O
for O
both O
US1 O
and O
US2 O
given O
by O
the O
QWK O
values O
. O
veys O
a O
near O
- O
complete O
thought O
; O
take O
note O
that O
it O
is O
not O
a O
sentence O
but O
a O
segment O
of O
contiguous O
tokens O
that O
maximize O
the O
sum O
of O
attention O
weights O
. O
This O
enables O
BENEFICT O
to O
capture O
important O
phrases O
such O
as O
like O
this O
album O
andthe O
best O
of O
all O
. O
Hence O
, O
the O
judges O
agree O
that O
it O
captures O
the O
essence O
of O
the O
customer O
‚Äôs O
preference O
and O
helps O
customers O
make O
purchasing O
decisions O
in O
the O
future O
. O
6 O
Conclusion O
and O
Future O
Work O
We O
have O
successfully O
implemented O
a O
novel O
recommender O
model O
that O
uniquely O
integrates O
BERT O
, O
MLP O
, O
and O
MSP O
. O
BENEFICT O
‚Äôs O
predictive O
capability O
is O
validated O
by O
experiments O
performed O
on O
Amazon O
and O
Yelp O
datasets O
, O
consistently O
outperforming O
other O
state O
- O
of O
- O
the O
- O
art O
models O
. O
Moreover O
, O
its O
explanation O
generation O
capability O
is O
veriÔ¨Åed O
by O
human O
judges O
. O
We O
argue O
that O
our O
work O
offers O
an O
avenue O
to O
help O
bridge O
the O
research O
gap O
between O
accuracy O
and O
explainability O
. O
In O
the O
future O
, O
we O
will O
consider O
incorporating O
other O
neural O
components O
, O
such O
as O
attention O
mechanisms O
, O
in O
improving O
the O
user O
- O
item O
modeling O
process O
. O
We O
also O
intend O
to O
enhance O
the O
expressiveness O
and O
the O
overall O
quality O
of O
the O
generated O
explanations O
. O
Acknowledgment O
First O
and O
foremost O
, O
we O
extend O
our O
gratitude O
to O
the O
hardworking O
anonymous O
reviewers O
for O
their O
valuable O
insights O
and O
suggestions O
. O
Likewise O
, O
we O
sincerely O
thank O
the O
judges O
in O
our O
explainability O
study O
, O
Dr. O
Ria O
Mae O
Borromeo O
and O
Ms. O
Verna O
Banasihan O
, O
for O
their O
time O
and O
participation.151References O
Sung O
Eun O
Bae O
. O
2007 O
. O
Sequential O
and O
parallel O
algorithms O
for O
the O
generalized O
maximum O
subarray O
problem O
. O
Ting O
Bai O
, O
Ji O
- O
Rong O
Wen O
, O
Jun O
Zhang O
, O
and O
Wayne O
Xin O
Zhao O
. O
2017 O
. O
A O
neural O
collaborative O
Ô¨Åltering O
model O
with O
interaction O
- O
based O
neighborhood O
. O
In O
Proceedings O
of O
the O
2017 O
ACM O
on O
Conference O
on O
Information O
and O
Knowledge O
Management O
, O
pages O
1979‚Äì1982 O
. O
Ria O
Mae O
Borromeo O
and O
Motomichi O
Toyama O
. O
2015 O
. O
Automatic O
vs. O
crowdsourced O
sentiment O
analysis O
. O
In O
Proceedings O
of O
the O
19th O
International O
Database O
Engineering O
& O
Applications O
Symposium O
, O
pages O
90‚Äì95 O
. O
Rose O
Catherine O
and O
William O
Cohen O
. O
2017 O
. O
Transnets O
: O
Learning O
to O
transform O
for O
recommendation O
. O
In O
Proceedings O
of O
the O
eleventh O
ACM O
conference O
on O
recommender O
systems O
, O
pages O
288‚Äì296 O
. O
Chong O
Chen O
, O
Min O
Zhang O
, O
Yiqun O
Liu O
, O
and O
Shaoping O
Ma O
. O
2018 O
. O
Neural O
attentional O
rating O
regression O
with O
review O
- O
level O
explanations O
. O
In O
Proceedings O
of O
the O
2018 O
World O
Wide O
Web O
Conference O
, O
pages O
1583 O
‚Äì O
1592 O
. O
International O
World O
Wide O
Web O
Conferences O
Steering O
Committee O
. O
Felipe O
Costa O
, O
Sixun O
Ouyang O
, O
Peter O
Dolog O
, O
and O
Aonghus O
Lawlor O
. O
2018 O
. O
Automatic O
generation O
of O
natural O
language O
explanations O
. O
In O
Proceedings O
of O
the O
23rd O
International O
Conference O
on O
Intelligent O
User O
Interfaces O
Companion O
, O
page O
57 O
. O
ACM O
. O
Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2018 O
. O
Bert O
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
understanding O
. O
arXiv O
preprint O
arXiv:1810.04805 O
. O
Xavier O
Glorot O
, O
Antoine O
Bordes O
, O
and O
Yoshua O
Bengio O
. O
2011 O
. O
Deep O
sparse O
rectiÔ¨Åer O
neural O
networks O
. O
In O
Proceedings O
of O
the O
fourteenth O
international O
conference O
on O
artiÔ¨Åcial O
intelligence O
and O
statistics O
, O
pages O
315 O
‚Äì O
323 O
. O
Kaiming O
He O
, O
Xiangyu O
Zhang O
, O
Shaoqing O
Ren O
, O
and O
Jian O
Sun O
. O
2016 O
. O
Deep O
residual O
learning O
for O
image O
recognition O
. O
In O
Proceedings O
of O
the O
IEEE O
conference O
on O
computer O
vision O
and O
pattern O
recognition O
, O
pages O
770 O
‚Äì O
778 O
. O
Xiangnan O
He O
, O
Tao O
Chen O
, O
Min O
- O
Yen O
Kan O
, O
and O
Xiao O
Chen O
. O
2015 O
. O
Trirank O
: O
Review O
- O
aware O
explainable O
recommendation O
by O
modeling O
aspects O
. O
In O
Proceedings O
of O
the O
24th O
ACM O
International O
on O
Conference O
on O
Information O
and O
Knowledge O
Management O
, O
pages O
1661 O
‚Äì O
1670 O
. O
ACM O
. O
Xiangnan O
He O
, O
Xiaoyu O
Du O
, O
Xiang O
Wang O
, O
Feng O
Tian O
, O
Jinhui O
Tang O
, O
and O
Tat O
- O
Seng O
Chua O
. O
2018 O
. O
Outer O
productbased O
neural O
collaborative O
Ô¨Åltering O
. O
arXiv O
preprint O
arXiv:1808.03912 O
.Xiangnan O
He O
, O
Lizi O
Liao O
, O
Hanwang O
Zhang O
, O
Liqiang O
Nie O
, O
Xia O
Hu O
, O
and O
Tat O
- O
Seng O
Chua O
. O
2017 O
. O
Neural O
collaborative O
Ô¨Åltering O
. O
In O
Proceedings O
of O
the O
26th O
international O
conference O
on O
world O
wide O
web O
, O
pages O
173 O
‚Äì O
182 O
. O
Yehuda O
Koren O
, O
Robert O
Bell O
, O
and O
Chris O
V O
olinsky O
. O
2009 O
. O
Matrix O
factorization O
techniques O
for O
recommender O
systems O
. O
Computer O
, O
42(8):30‚Äì37 O
. O
Ilya O
Loshchilov O
and O
Frank O
Hutter O
. O
2018 O
. O
Fixing O
weight O
decay O
regularization O
in O
adam O
. O
Julian O
McAuley O
, O
Christopher O
Targett O
, O
Qinfeng O
Shi O
, O
and O
Anton O
Van O
Den O
Hengel O
. O
2015 O
. O
Image O
- O
based O
recommendations O
on O
styles O
and O
substitutes O
. O
In O
Proceedings O
of O
the O
38th O
international O
ACM O
SIGIR O
conference O
on O
research O
and O
development O
in O
information O
retrieval O
, O
pages O
43‚Äì52 O
. O
Rada O
Mihalcea O
and O
Paul O
Tarau O
. O
2004 O
. O
Textrank O
: O
Bringing O
order O
into O
text O
. O
In O
Proceedings O
of O
the O
2004 O
conference O
on O
empirical O
methods O
in O
natural O
language O
processing O
, O
pages O
404‚Äì411 O
. O
Andriy O
Mnih O
and O
Russ O
R O
Salakhutdinov O
. O
2008 O
. O
Probabilistic O
matrix O
factorization O
. O
In O
Advances O
in O
neural O
information O
processing O
systems O
, O
pages O
1257‚Äì1264 O
. O
Cataldo O
Musto O
, O
Marco O
de O
Gemmis O
, O
Giovanni O
Semeraro O
, O
and O
Pasquale O
Lops O
. O
2017 O
. O
A O
multi O
- O
criteria O
recommender O
system O
exploiting O
aspect O
- O
based O
sentiment O
analysis O
of O
users O
‚Äô O
reviews O
. O
In O
Proceedings O
of O
the O
eleventh O
ACM O
conference O
on O
recommender O
systems O
, O
pages O
321‚Äì325 O
. O
ACM O
. O
Georgina O
Peake O
and O
Jun O
Wang O
. O
2018 O
. O
Explanation O
mining O
: O
Post O
hoc O
interpretability O
of O
latent O
factor O
models O
for O
recommendation O
systems O
. O
In O
Proceedings O
of O
the O
24th O
ACM O
SIGKDD O
International O
Conference O
on O
Knowledge O
Discovery O
& O
Data O
Mining O
, O
pages O
2060 O
‚Äì O
2069 O
. O
ACM O
. O
Mohammad O
Taher O
Pilehvar O
and O
Jose O
CamachoCollados O
. O
2019 O
. O
Wic O
: O
the O
word O
- O
in O
- O
context O
dataset O
for O
evaluating O
context O
- O
sensitive O
meaning O
representations O
. O
In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
1267‚Äì1273 O
. O
Anand O
Rajaraman O
and O
Jeffrey O
David O
Ullman O
. O
2011 O
. O
Mining O
of O
massive O
datasets O
. O
Cambridge O
University O
Press O
. O
Zhaochun O
Ren O
, O
Shangsong O
Liang O
, O
Piji O
Li O
, O
Shuaiqiang O
Wang O
, O
and O
Maarten O
de O
Rijke O
. O
2017 O
. O
Social O
collaborative O
viewpoint O
regression O
with O
explainable O
recommendations O
. O
In O
Proceedings O
of O
the O
tenth O
ACM O
international O
conference O
on O
web O
search O
and O
data O
mining O
, O
pages O
485‚Äì494 O
. O
ACM.152Marco O
Tulio O
Ribeiro O
, O
Sameer O
Singh O
, O
and O
Carlos O
Guestrin O
. O
2016 O
. O
Why O
should O
i O
trust O
you O
? O
: O
Explaining O
the O
predictions O
of O
any O
classiÔ¨Åer O
. O
In O
Proceedings O
of O
the O
22nd O
ACM O
SIGKDD O
international O
conference O
on O
knowledge O
discovery O
and O
data O
mining O
, O
pages O
1135‚Äì1144 O
. O
ACM O
. O
Chi O
Sun O
, O
Xipeng O
Qiu O
, O
Yige O
Xu O
, O
and O
Xuanjing O
Huang O
. O
2019 O
. O
How O
to O
Ô¨Åne O
- O
tune O
bert O
for O
text O
classiÔ¨Åcation O
? O
arXiv O
preprint O
arXiv:1905.05583 O
. O
Yi O
Tay O
, O
Anh O
Tuan O
Luu O
, O
and O
Siu O
Cheung O
Hui O
. O
2018 O
. O
Multi O
- O
pointer O
co O
- O
attention O
networks O
for O
recommendation O
. O
In O
Proceedings O
of O
the O
24th O
ACM O
SIGKDD O
International O
Conference O
on O
Knowledge O
Discovery O
& O
Data O
Mining O
, O
pages O
2309‚Äì2318 O
. O
Qianqian O
Wang O
, O
Si O
Li O
, O
and O
Guang O
Chen O
. O
2018a O
. O
Worddriven O
and O
context O
- O
aware O
review O
modeling O
for O
recommendation O
. O
In O
Proceedings O
of O
the O
27th O
ACM O
International O
Conference O
on O
Information O
and O
Knowledge O
Management O
, O
pages O
1859‚Äì1862 O
. O
Xianchen O
Wang O
, O
Hongtao O
Liu O
, O
Peiyi O
Wang O
, O
Fangzhao O
Wu O
, O
Hongyan O
Xu O
, O
Wenjun O
Wang O
, O
and O
Xing O
Xie O
. O
2019 O
. O
Neural O
review O
rating O
prediction O
with O
hierarchical O
attentions O
and O
latent O
factors O
. O
In O
International O
Conference O
on O
Database O
Systems O
for O
Advanced O
Applications O
, O
pages O
363‚Äì367 O
. O
Springer O
. O
Xiang O
Wang O
, O
Xiangnan O
He O
, O
Fuli O
Feng O
, O
Liqiang O
Nie O
, O
and O
Tat O
- O
Seng O
Chua O
. O
2018b O
. O
Tem O
: O
Tree O
- O
enhanced O
embedding O
model O
for O
explainable O
recommendation O
. O
InProceedings O
of O
the O
2018 O
World O
Wide O
Web O
Conference O
, O
pages O
1543‚Äì1552 O
. O
International O
World O
Wide O
Web O
Conferences O
Steering O
Committee O
. O
Xiang O
Wang O
, O
Xiangnan O
He O
, O
Liqiang O
Nie O
, O
and O
Tat O
- O
Seng O
Chua O
. O
2017 O
. O
Item O
silk O
road O
: O
Recommending O
items O
from O
information O
domains O
to O
social O
users O
. O
In O
Proceedings O
of O
the O
40th O
International O
ACM O
SIGIR O
conference O
on O
Research O
and O
Development O
in O
Information O
Retrieval O
, O
pages O
185‚Äì194 O
. O
Yonghui O
Wu O
, O
Mike O
Schuster O
, O
Zhifeng O
Chen O
, O
Quoc O
V O
Le O
, O
Mohammad O
Norouzi O
, O
Wolfgang O
Macherey O
, O
Maxim O
Krikun O
, O
Yuan O
Cao O
, O
Qin O
Gao O
, O
Klaus O
Macherey O
, O
et O
al O
. O
2016 O
. O
Google O
‚Äôs O
neural O
machine O
translation O
system O
: O
Bridging O
the O
gap O
between O
human O
and O
machine O
translation O
. O
arXiv O
preprint O
arXiv:1609.08144 O
. O
Alan O
Zakbik O
, O
Tanja O
Bergmann O
, O
and O
Roland O
V O
ollgraf O
. O
2019 O
. O
Pooled O
contextualized O
embeddings O
for O
named O
entity O
recognition O
. O
In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
724‚Äì728 O
. O
Yongfeng O
Zhang O
, O
Qingyao O
Ai O
, O
Xu O
Chen O
, O
and O
W O
Bruce O
Croft O
. O
2017 O
. O
Joint O
representation O
learning O
for O
topn O
recommendation O
with O
heterogeneous O
information O
sources O
. O
In O
Proceedings O
of O
the O
2017 O
ACM O
on O
Conference O
on O
Information O
and O
Knowledge O
Management O
, O
pages O
1449‚Äì1458.Yongfeng O
Zhang O
and O
Xu O
Chen O
. O
2018 O
. O
Explainable O
recommendation O
: O
A O
survey O
and O
new O
perspectives O
. O
arXiv O
preprint O
arXiv:1804.11192 O
. O
Yongfeng O
Zhang O
, O
Guokun O
Lai O
, O
Min O
Zhang O
, O
Yi O
Zhang O
, O
Yiqun O
Liu O
, O
and O
Shaoping O
Ma O
. O
2014 O
. O
Explicit O
factor O
models O
for O
explainable O
recommendation O
based O
on O
phrase O
- O
level O
sentiment O
analysis O
. O
In O
Proceedings O
of O
the O
37th O
international O
ACM O
SIGIR O
conference O
on O
Research O
& O
development O
in O
information O
retrieval O
, O
pages O
83‚Äì92 O
. O
ACM O
. O
Lei O
Zheng O
, O
Vahid O
Noroozi O
, O
and O
Philip O
S O
Yu O
. O
2017 O
. O
Joint O
deep O
modeling O
of O
users O
and O
items O
using O
reviews O
for O
recommendation O
. O
In O
Proceedings O
of O
the O
Tenth O
ACM O
International O
Conference O
on O
Web O
Search O
and O
Data O
Mining O
, O
pages O
425‚Äì434 O
. O
ACM.153Proceedings O
of O
the O
1st O
Conference O
of O
the O
Asia O
- O
PaciÔ¨Åc O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
10th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
154‚Äì162 O
December O
4 O
- O
7 O
, O
2020 O
. O
¬© O
2020 O
Association O
for O
Computational O
Linguistics O
Transformer O
- O
based O
Approach O
for O
Predicting O
Chemical O
Compound O
Structures O
Yutaro O
Omote O
Ehime O
University O
omote@ai.cs.ehime O
- O
u.ac.jpKyoumoto O
Matsushita O
Fujitsu O
Laboratories O
, O
Ltd. O
m.kyoumoto@fujitsu.comTomoya O
Iwakura O
Fujitsu O
Laboratories O
, O
Ltd. O
iwakura.tomoya@fujitsu.com O
Akihiro O
Tamura O
Doshisha O
University O
aktamura@mail.doshisha.ac.jpTakashi O
Ninomiya O
Ehime O
University O
ninomiya@cs.ehime-u.ac.jp O
Abstract O
By O
predicting O
chemical O
compound O
structures O
from O
their O
names O
, O
we O
can O
better O
comprehend O
chemical O
compounds O
written O
in O
text O
and O
identify O
the O
same O
chemical O
compound O
given O
different O
notations O
for O
database O
creation O
. O
Previous O
methods O
have O
predicted O
the O
chemical O
compound O
structures O
from O
their O
names O
and O
represented O
them O
by O
SimpliÔ¨Åed O
Molecular O
Input O
Line O
Entry O
System O
( O
SMILES O
) O
strings O
. O
However O
, O
these O
methods O
mainly O
apply O
handcrafted O
rules O
, O
and O
can O
not O
predict O
the O
structures O
of O
chemical O
compound O
names O
not O
covered O
by O
the O
rules O
. O
Instead O
of O
handcrafted O
rules O
, O
we O
propose O
Transformer O
- O
based O
models O
that O
predict O
SMILES O
strings O
from O
chemical O
compound O
names O
. O
We O
improve O
the O
conventional O
Transformer O
- O
based O
model O
by O
introducing O
two O
features O
: O
( O
1 O
) O
a O
loss O
function O
that O
constrains O
the O
number O
of O
atoms O
of O
each O
element O
in O
the O
structure O
, O
and O
( O
2 O
) O
a O
multi O
- O
task O
learning O
approach O
that O
predicts O
both O
SMILES O
strings O
and O
InChI O
strings O
( O
another O
string O
representation O
of O
chemical O
compound O
structures O
) O
. O
In O
evaluation O
experiments O
, O
our O
methods O
achieved O
higher O
Fmeasures O
than O
previous O
rule O
- O
based O
approaches O
( O
Open O
Parser O
for O
Systematic O
IUPAC O
Nomenclature O
and O
two O
commercially O
used O
products O
) O
, O
and O
the O
conventional O
Transformer O
- O
based O
model O
. O
We O
release O
the O
dataset O
used O
in O
this O
paper O
as O
a O
benchmark O
for O
the O
future O
research1 O
. O
1 O
Introduction O
Knowledge O
of O
chemical O
substances O
is O
necessary O
for O
developing O
new O
materials O
and O
drugs O
, O
and O
for O
synthesizing O
products O
from O
new O
materials O
. O
To O
utilize O
such O
knowledge O
, O
researchers O
have O
created O
databases O
containing O
the O
physical O
property O
values O
of O
chemical O
substances O
and O
the O
interrelationships O
among O
chemical O
substances O
. O
It O
is O
thought O
that O
several O
billions O
of O
chemical O
compounds O
exist O
( O
Lahana O
, O
1999 O
; O
Hoffmann O
and O
1http://aiweb.cs.ehime-u.ac.jp/ O
pred O
- O
chem O
- O
structGastreich O
, O
2019 O
) O
, O
but O
only O
a O
portion O
of O
these O
are O
entered O
into O
chemical O
databases O
. O
Even O
PubChem2 O
, O
one O
of O
the O
largest O
databases O
of O
chemical O
compounds O
, O
includes O
the O
information O
of O
only O
approximately O
100 O
million O
chemical O
compounds O
. O
Moreover O
, O
databases O
for O
chemical O
domains O
are O
manually O
maintained O
, O
which O
consumes O
much O
time O
and O
cost O
. O
One O
of O
the O
time O
consuming O
processes O
is O
the O
integration O
of O
the O
same O
chemical O
compounds O
with O
different O
notations O
. O
For O
instance O
, O
a O
chemical O
structure O
can O
be O
derived O
from O
partial O
structures O
which O
are O
given O
notational O
variants O
, O
or O
the O
notation O
can O
Ô¨Çuctuate O
for O
a O
given O
chemical O
compound O
( O
Watanabe O
et O
al O
. O
, O
2019 O
) O
. O
Therefore O
, O
a O
system O
that O
automatically O
predicts O
a O
chemical O
compound O
structure O
from O
its O
chemical O
compound O
names O
would O
improve O
the O
database O
creation O
procedure O
. O
Structures O
are O
most O
commonly O
predicted O
from O
their O
notations O
by O
rule O
- O
based O
conversion O
methods O
( O
Lowe O
et O
al O
. O
, O
2011 O
) O
. O
Although O
rule O
- O
based O
conversion O
can O
accurately O
predict O
the O
structures O
of O
chemical O
compounds O
based O
on O
systematic O
nomenclatures O
such O
as O
the O
International O
Union O
of O
Pure O
and O
Applied O
Chemistry O
( O
IUPAC)3nomenclature O
, O
it O
often O
fails O
the O
structure O
prediction O
of O
chemical O
compound O
names O
that O
violate O
these O
nomenclatures O
( O
e.g. O
, O
Synonyms4 O
) O
. O
To O
improve O
the O
low O
prediction O
performance O
of O
compounds O
with O
non O
- O
IUPAC O
names O
, O
we O
propose O
neural O
network O
- O
based O
models O
that O
predict O
chemical O
compound O
structures O
represented O
as O
SimpliÔ¨Åed O
Molecular O
Input O
Line O
Entry O
System O
( O
SMILES O
) O
( O
Weininger O
, O
1988 O
) O
strings O
from O
chemical O
compound O
names O
categorized O
as O
Synonyms5 O
. O
In O
this O
work O
, O
we O
use O
the O
Transformer O
- O
based O
sequence2https://pubchem.ncbi.nlm.nih.gov/ O
3https://iupac.org O
4PubChem O
‚Äôs O
deÔ¨Ånition O
of O
chemical O
compound O
names O
other O
than O
IUPAC O
names O
5Our O
Synonyms O
excludes O
DATABASE O
IDs O
from O
the O
original O
deÔ¨Ånition O
of O
Synonyms O
because O
DATABASE O
IDs O
can O
be O
efÔ¨Åciently O
recognized O
by O
rules.154Name O
Type O
Name O
IUPAC O
2 O
- O
acetyloxybenzoic O
acid O
DATABASE O
ID O
( O
CAS O
registry O
number O
) O
50 O
- O
78 O
- O
2 O
ABBREVIATION O
ASA O
COMMON O
aspirin O
Table O
1 O
: O
Examples O
of O
‚Äú O
aspirin O
‚Äù O
representations O
. O
In O
this O
table O
, O
ABBREVIATION O
and O
COMMON O
are O
Synonyms O
. O
to O
- O
sequence O
neural O
network O
model O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
for O
machine O
translation O
, O
which O
achieves O
a O
state O
- O
of O
- O
the O
- O
art O
performance O
in O
various O
tasks O
among O
the O
sequence O
- O
to O
- O
sequence O
neural O
network O
models O
such O
as O
recurrent O
neural O
network O
- O
based O
models O
. O
To O
improve O
the O
conventional O
Transformer O
- O
based O
model O
, O
we O
introduce O
the O
following O
two O
chemicalstructure O
oriented O
features O
: O
1.A O
loss O
function O
considering O
the O
constraints O
on O
the O
number O
of O
atoms O
of O
each O
element O
in O
the O
chemical O
structure O
. O
2.A O
multi O
- O
task O
learning O
for O
predicting O
both O
SMILES O
strings O
and O
IUPAC O
International O
Chemical O
IdentiÔ¨Åer O
( O
InChI O
) O
( O
Heller O
et O
al O
. O
, O
2015 O
) O
strings O
, O
which O
are O
representations O
for O
denoting O
chemical O
compound O
structures O
as O
strings O
. O
For O
our O
experiments O
, O
we O
created O
a O
dataset O
from O
PubChem O
for O
predicting O
chemical O
compound O
structures O
represented O
by O
SMILES O
strings O
from O
Synonyms O
. O
The O
experimental O
results O
demonstrate O
the O
Transformer O
- O
based O
conversion O
methods O
achieve O
higher O
F O
- O
measures O
than O
the O
existing O
rule O
- O
based O
methods O
. O
In O
addition O
, O
our O
two O
proposals O
( O
i.e. O
, O
constraining O
the O
number O
of O
atoms O
of O
each O
element O
and O
multi O
- O
task O
learning O
of O
both O
SMILES O
strings O
and O
InChI O
strings O
) O
improve O
the O
performance O
of O
the O
conventional O
Transformer O
- O
based O
method O
. O
2 O
Preliminary O
2.1 O
Chemical O
Compound O
Names O
In O
PubChem O
, O
the O
text O
names O
of O
chemical O
compounds O
are O
represented O
by O
three O
main O
types O
of O
notational O
categories O
: O
IUPAC O
, O
DATABASE O
ID O
, O
and O
Synonyms O
. O
IUPAC O
is O
a O
systematic O
nomenclature O
for O
chemical O
compound O
names O
. O
DATABASE O
ID O
is O
the O
unique O
identiÔ¨Åer O
of O
a O
chemical O
compound O
in O
a O
database O
. O
An O
example O
is O
the O
Chemical O
Abstracts O
Service O
( O
CAS)6registry O
number O
. O
The O
Synonyms O
6https://www.cas.org/ O
O O
OH O
NH2HO[Chemical O
Structure O
] O
[ O
SMILES O
] O
[ O
InChI]N[C@@H](Cc1ccc(O)cc1)C(=O)O O
InChI=1S O
/ O
C9H1 O
1NO3 O
/ O
c10 O
- O
8(9(12)13)5 O
- O
6 O
- O
1 O
- O
3 O
- O
7(1 O
1)4 O
- O
2 O
- O
6 O
/h1 O
- O
4,8,11H,5,10H2,(H,12,13)/t8-/m0 O
/ O
s1Figure O
1 O
: O
Chemical O
structure O
of O
L O
- O
tyrosine O
( O
top O
) O
, O
and O
its O
SMILES O
( O
middle O
) O
and O
InChI O
( O
bottom O
) O
representations O
naming O
category O
in O
PubChem O
includes O
ABBREVIATION O
and O
COMMON O
. O
As O
an O
example O
, O
Table O
1 O
shows O
various O
‚Äú O
aspirin O
‚Äù O
representations O
. O
The O
IUPAC O
nomenclature O
provides O
a O
systematic O
naming O
under O
standardized O
rules O
, O
which O
are O
easily O
and O
accurately O
converted O
by O
rule O
- O
based O
conversion O
methods O
( O
Lowe O
et O
al O
. O
, O
2011 O
) O
; O
( O
Heller O
et O
al O
. O
, O
2015 O
) O
. O
Provided O
they O
are O
registered O
in O
the O
database O
, O
DATABASE O
IDs O
are O
easily O
converted O
to O
their O
corresponding O
chemical O
compounds O
using O
dictionary O
- O
lookup O
methods O
. O
However O
, O
neither O
rulebased O
nor O
dictionary O
- O
based O
approach O
can O
convert O
chemical O
compound O
names O
that O
are O
not O
covered O
by O
the O
rules O
or O
dictionaries O
. O
Unlike O
IUPAC O
and O
DATABASE O
ID O
notations O
, O
the O
naming O
patterns O
of O
Synonyms O
are O
complex O
and O
widely O
variable O
. O
In O
many O
cases O
, O
the O
chemical O
compound O
names O
appearing O
in O
documents O
can O
not O
be O
converted O
by O
rule O
- O
based O
or O
dictionary O
- O
based O
approaches O
. O
Consequently O
, O
the O
prediction O
performance O
of O
chemical O
compound O
names O
is O
worse O
in O
Synonyms O
than O
in O
IUPAC O
, O
as O
shown O
in O
section O
6.1 O
. O
In O
our O
preliminary O
experiments O
, O
the O
highest O
F O
- O
measure O
obtained O
with O
an O
existing O
tool O
exceeded O
0.96 O
on O
IUPAC O
data O
, O
but O
was O
reduced O
to O
0.75 O
on O
Synonyms O
data.155BPE O
Tokenizer O
amino O
gall O
ane O
cesium O
picrate O
hex O
dec O
en O
olid O
non O
an O
al O
oxime O
¬∑ O
¬∑ O
¬∑ O
( O
a O
) O
Learning O
TokenizationCompound O
Name O
EncoderSMILES O
DecoderC O
= O
C O
O O
C O
C O
O O
C O
C O
Cl O
< O
s O
> O
2 O
- O
( O
2 O
- O
C O
h O
l O
o O
ro O
eth O
ox O
y O
) O
ethyl O
v O
inyl O
ether O
< O
s O
> O
C O
= O
C O
O O
C O
C O
O O
C O
C O
Cl O
BPE O
TokenizerElement O
- O
wise O
& O
Character O
- O
wise O
Tokenizer O
2-(2 O
- O
Chloroethoxy)ethylvinyl O
ether O
C O
= O
COCCOCCCl O
( O
b O
) O
Training O
ModelCompound O
Name O
EncoderSMILES O
DecoderC O
= O
C O
O O
C O
C O
O O
C O
C O
Cl O
< O
s O
> O
2 O
- O
( O
2 O
- O
C O
h O
l O
o O
ro O
eth O
ox O
y O
) O
ethyl O
v O
inyl O
ether O
< O
s O
> O
BPE O
Tokenizer O
2-(2 O
- O
Chloroethoxy)ethylvinyl O
ether O
( O
c O
) O
Inference O
Figure O
2 O
: O
Overview O
of O
Transformer O
- O
based O
prediction O
of O
SMILES O
strings O
from O
chemical O
compound O
names O
2.2 O
Representation O
of O
Chemical O
Compound O
Structures O
For O
multi O
- O
task O
learning O
, O
we O
represented O
chemical O
compound O
structures O
as O
SMILES O
strings O
and O
InChI O
strings O
. O
These O
two O
representations O
are O
major O
notations O
of O
chemical O
compound O
structures O
. O
We O
use O
SMILES O
strings O
as O
the O
target O
representation O
because O
they O
are O
simpler O
than O
InChI O
strings O
but O
were O
sufÔ¨Åciently O
representative O
for O
our O
purpose O
( O
i.e. O
, O
creating O
a O
chemical O
compound O
database O
) O
. O
The O
SMILES O
( O
Weininger O
, O
1988 O
) O
notation O
system O
was O
designed O
for O
modern O
chemical O
information O
processing O
. O
Based O
on O
the O
principles O
of O
molecular O
graph O
theory O
, O
SMILES O
allows O
rigorous O
structure O
speciÔ¨Åcation O
using O
a O
very O
small O
and O
natural O
grammar O
. O
SMILES O
strings O
are O
composed O
of O
atoms O
and O
symbols O
representing O
their O
bonds O
, O
branches O
, O
rings O
, O
and O
other O
structural O
features O
, O
assembled O
into O
a O
linear O
expression O
of O
the O
two O
- O
dimensional O
structure O
of O
a O
molecule O
. O
An O
example O
of O
a O
SMILES O
string O
is O
shown O
in O
Figure O
1 O
. O
In O
this O
work O
, O
we O
used O
Canonical O
SMILES O
because O
it O
uniquely O
determines O
the O
correspondence O
between O
chemical O
structures O
and O
SMILES O
strings O
. O
In O
the O
InChI O
( O
Heller O
et O
al O
. O
, O
2015 O
) O
representation O
, O
the O
information O
of O
a O
chemical O
compound O
structure O
is O
represented O
by O
Ô¨Åve O
layers O
. O
In O
Figure O
1 O
, O
the O
layers O
are O
separated O
by O
‚Äú O
/ O
‚Äù O
symbols O
. O
Each O
layer O
adds O
detailed O
information O
to O
the O
following O
layer O
. O
Because O
these O
layers O
are O
interrelated O
, O
InChI O
strings O
are O
more O
complex O
than O
SMILES O
strings O
. O
3 O
Proposed O
Methods O
This O
section O
presents O
our O
proposed O
methods O
, O
namely O
, O
our O
tokenizer O
training O
method O
and O
sequence O
- O
to O
- O
sequence O
models O
. O
Let O
XandTbe O
a O
set O
of O
chemical O
compound O
names O
and O
a O
setof O
SMILES O
strings O
, O
respectively O
. O
We O
deÔ¨Åne O
a O
training O
dataset O
consisting O
of O
nsamples O
asD= O
/angbracketleft(X1,T1), O
... O
,(Xn O
, O
Tn)/angbracketright O
, O
whereXi‚ààX O
is O
a O
chemical O
compound O
name O
and O
Ti‚ààT O
is O
the O
SMILES O
string O
ofXifor1‚â§i‚â§n O
. O
Our O
objective O
is O
to O
learn O
a O
mapping O
function O
fthat O
realizes O
f(Xi)= O
TifromD. O
Figure O
2 O
overviews O
the O
Transformer O
- O
based O
prediction O
of O
SMILES O
strings O
from O
chemical O
compound O
names O
, O
where O
< O
s O
> O
is O
a O
special O
symbol O
denoting O
the O
start O
and O
end O
of O
a O
sequence O
. O
Chemical O
compound O
names O
, O
SMILES O
, O
and O
InChI O
are O
long O
strings O
without O
explicit O
boundaries O
( O
such O
as O
white O
spaces O
in O
English O
text O
) O
. O
Therefore O
, O
to O
convert O
chemical O
compound O
names O
to O
SMILES O
strings O
, O
we O
propose O
( O
a O
) O
training O
of O
a O
tokenizer O
and O
( O
b O
) O
a O
Transformer O
- O
based O
approach O
. O
3.1 O
Tokenizer O
Chemical O
compound O
names O
can O
be O
tokenized O
by O
the O
Open O
Parser O
for O
Systematic O
IUPAC O
Nomenclature O
( O
OPSIN O
) O
( O
Lowe O
et O
al O
. O
, O
2011 O
) O
tokenizer O
, O
a O
rule O
- O
based O
parser O
that O
generates O
SMILES O
and O
InChI O
strings O
from O
chemical O
compound O
names O
( O
mainly O
, O
from O
IUPAC O
names O
) O
. O
However O
, O
some O
chemical O
compound O
names O
, O
especially O
Synonyms O
, O
can O
not O
be O
tokenized O
by O
rule O
- O
based O
tokenizers O
such O
as O
OPSIN O
. O
In O
particular O
, O
the O
OPSIN O
tokenizer O
is O
limited O
to O
chemical O
compound O
names O
covered O
by O
its O
dictionary O
and O
rules O
; O
meanwhile O
( O
as O
mentioned O
above O
) O
chemical O
compound O
names O
lack O
explicit O
word O
- O
boundary O
markers O
. O
To O
overcome O
these O
restrictions O
, O
we O
propose O
a O
method O
that O
trains O
tokenizers O
for O
Synonyms O
, O
SMILES O
, O
and O
InChI O
representations O
. O
Note O
that O
InChI O
is O
used O
in O
a O
multi O
- O
task O
learning O
. O
To O
eliminate O
the O
unknown O
tokens O
, O
our O
tokenizer O
learning O
method O
is O
unsupervised O
and O
covers O
a O
large156set O
of O
chemical O
compound O
names O
. O
The O
tokenization O
is O
performed O
by O
byte O
pair O
encoding O
( O
BPE O
) O
( O
Sennrich O
et O
al O
. O
, O
2016)7 O
. O
The O
BPE O
- O
based O
tokenizer O
was O
learned O
by O
fastBPE8 O
. O
First O
, O
the O
chemical O
compound O
names O
obtained O
by O
the O
OPSIN O
tokenizer O
were O
segmented O
because O
fastBPE O
requires O
segmented O
input O
text O
. O
By O
virtue O
of O
the O
newly O
obtained O
BPE O
dictionary O
, O
the O
BPE O
- O
based O
tokenizer O
can O
tokenize O
chemical O
compound O
names O
that O
can O
not O
be O
handled O
by O
the O
OPSIN O
tokenizer O
. O
When O
tokenizing O
the O
SMILES O
strings O
, O
each O
element O
( O
e.g. O
, O
‚Äú O
C O
‚Äù O
, O
‚Äú O
O O
‚Äù O
, O
‚Äú O
Cl O
‚Äù O
) O
identiÔ¨Åed O
by O
regular O
expressions O
was O
regarded O
as O
one O
token O
. O
The O
remaining O
symbols O
not O
covered O
by O
regular O
expressions O
were O
divided O
into O
single O
characters O
, O
each O
regarded O
as O
one O
token O
. O
For O
tokenizing O
InChI O
strings O
, O
the O
model O
was O
learned O
on O
SentencePiece O
( O
Kudo O
and O
Richardson O
, O
2018 O
) O
, O
a O
unigram O
- O
based O
unsupervised O
training O
method O
for O
word O
segmentation O
. O
Note O
that O
InChI O
strings O
can O
not O
be O
tokenized O
by O
BPE O
because O
the O
segmentations O
of O
InChI O
strings O
are O
not O
preliminarily O
given O
. O
3.2 O
Transformer O
- O
based O
Prediction O
of O
SMILES O
Strings O
from O
Chemical O
Compound O
Names O
The O
Transformer O
model O
consists O
of O
stacked O
encoder O
and O
decoder O
layers O
. O
Based O
on O
self O
- O
attention O
, O
it O
attends O
to O
tokens O
in O
the O
same O
sequence O
, O
i.e. O
, O
a O
single O
input O
sequence O
or O
a O
single O
output O
sequence O
. O
The O
encoder O
maps O
an O
input O
sequence O
to O
a O
sequence O
of O
vector O
representations O
. O
From O
this O
vector O
representations O
, O
the O
decoder O
generates O
an O
output O
sequence O
. O
The O
Transformer O
- O
based O
model O
predicts O
SMILES O
strings O
from O
chemical O
compound O
names O
, O
so O
its O
input O
is O
a O
chemical O
compound O
name O
and O
its O
output O
is O
a O
SMILES O
string O
. O
During O
the O
learning O
process O
, O
the O
following O
objective O
function O
is O
minimized O
: O
Lsmiles O
= O
‚àílogP(T|X;Œ∏enc O
, O
Œ∏smiles O
) O
, O
( O
1 O
) O
whereŒ∏encandŒ∏smiles O
are O
the O
parameter O
sets O
of O
the O
compound O
name O
encoder O
and O
SMILES O
decoder O
, O
respectively O
, O
and O
X=/angbracketleftx1,x2, O
... O
,x O
n O
/ O
angbracketrightis O
the O
word O
sequence O
of O
a O
chemical O
compound O
name O
segmented O
by O
the O
BPE O
model O
. O
T=/angbracketleftt1,t2, O
... O
,t O
m O
/ O
angbracketrightis O
the O
7In O
preliminary O
experiments O
, O
BPE O
achieved O
a O
higher O
Fmeasure O
than O
SentencePiece O
( O
Kudo O
and O
Richardson O
, O
2018 O
) O
. O
Therefore O
, O
it O
was O
used O
for O
tokenizing O
the O
chemical O
compound O
names O
. O
8https://github.com/glample/fastBPE O
ùìõùíÇùíïùíêùíé=1 O
ùê¥ùëÅ"C"‚àíùë¶1ùëùùëüùëíùëë2+ùëÅ"O"‚àíùë¶3ùëùùëüùëíùëë2+‚ãØ O
= O
1 O
ùê¥2‚àí2.272 O
+ O
1‚àí0.842+‚ãØLoss O
for O
‚Äú O
C O
‚Äù O
Loss O
for O
‚Äú O
O‚Äù012 O
C O
= O
O O
ùíöùüè O
ùíöùüê O
ùíöùüë01 O
C O
= O
O01 O
C O
= O
O O
ùíöùíëùíìùíÜùíÖ= O
+ O
+ O
ùíöùüí+01 O
C O
= O
O01 O
C O
= O
Oùë¶1ùëùùëüùëíùëëùë¶3ùëùùëüùëíùëë O
ùê¥=ùëé|"C","O",‚ãØ O
, O
" O
= O
" O
‚àâùê¥ùëÅ"C":2,ùëÅ"O":1Figure O
3 O
: O
Calculating O
the O
constraints O
on O
the O
number O
of O
atoms O
of O
each O
element O
sequence O
of O
elements O
and O
symbols O
in O
the O
correct O
SMILES O
string O
of O
X. O
3.3 O
Training O
with O
a O
Constraint O
on O
the O
Number O
of O
Atoms O
To O
correctly O
predict O
the O
chemical O
structure O
from O
a O
chemical O
compound O
name O
, O
the O
number O
of O
atoms O
of O
each O
element O
included O
in O
the O
chemical O
structure O
must O
be O
Ô¨Åxed O
. O
In O
this O
subsection O
, O
we O
propose O
a O
softmax O
- O
based O
loss O
function O
that O
constrains O
the O
number O
of O
atoms O
of O
each O
element O
, O
that O
is O
, O
we O
minimize O
the O
difference O
between O
the O
numbers O
of O
atoms O
of O
each O
element O
in O
the O
predicted O
and O
correct O
SMILES O
strings O
. O
The O
differences O
are O
measured O
by O
their O
squared O
errors O
. O
The O
squared O
errors O
are O
computed O
using O
the O
Gumbel O
softmax O
( O
Jang O
et O
al O
. O
, O
2016 O
) O
function O
, O
which O
obtains O
the O
probability O
distribution O
of O
the O
number O
of O
atoms O
of O
each O
element O
in O
a O
predicted O
SMILES O
string O
. O
Let O
œÄi= O
( O
œÄi1,œÄi2, O
... O
,œÄ O
i|V| O
) O
be O
the O
probability O
distribution O
of O
the O
i O
- O
th O
output O
token O
from O
the O
Transformer O
model O
. O
Then O
, O
yi= O
( O
yi1,yi2, O
... O
,y O
i|V|)for O
thei O
- O
th O
output O
token O
with O
Gumbel O
softmax O
is O
calculated O
as O
follows O
: O
yij O
= O
exp O
( O
( O
log(œÄij O
) O
+ O
gij)/œÑ O
) O
/summationtext|V| O
k=1exp O
( O
( O
log(œÄik O
) O
+ O
gik)/œÑ),(2 O
) O
gij=‚àílog(‚àílog(uij O
) O
) O
, O
uij‚àºUniform(0,1 O
) O
, O
whereVrepresents O
the O
vocabulary O
set O
of O
SMILES O
, O
andœÑis O
a O
hyperparameter O
of O
Gumbel O
softmax O
. O
The O
distributionyiapproximates O
an O
one O
- O
hot O
vector O
as O
œÑ O
decreases O
, O
and O
a O
uniform O
distribution O
as O
œÑincreases O
. O
In O
this O
work O
, O
œÑwas O
set O
to O
0.1 O
. O
Using O
Equation O
2 O
, O
the O
loss O
function O
under O
the157proposed O
constraints O
is O
given O
by O
Latom O
= O
1 O
|A|/summationdisplay O
a‚ààA(Na(T)‚àíypred O
idx(a))2,(3 O
) O
ypred O
= O
y1+y2+¬∑¬∑¬∑+ym O
= O
( O
ypred O
1,ypred O
2, O
... O
,ypred O
|V| O
) O
, O
whereAis O
a O
set O
of O
elements O
, O
Na(T)is O
a O
function O
that O
returns O
the O
number O
of O
atoms O
of O
element O
ain O
SMILES O
string O
T O
, O
andidx(a)is O
a O
function O
that O
returns O
the O
index O
of O
element O
ainV. O
Note O
that O
Acontains O
only O
elemental O
symbols O
, O
and O
the O
other O
features O
such O
as O
symbols O
representing O
bonds O
are O
absent O
. O
More O
formally O
, O
‚Äú O
C O
‚Äù O
, O
‚Äú O
O O
‚Äù O
‚ààA O
, O
‚Äú O
= O
‚Äù O
, O
‚Äú O
# O
‚Äù O
/‚ààA O
, O
andV‚äÉA. O
Each O
dimension O
of O
ypredis O
an O
estimation O
of O
the O
frequency O
of O
the O
corresponding O
token O
of O
the O
vocabulary O
Vin O
the O
predicted O
SMILES O
. O
The O
proposed O
constraint O
calculation O
uses O
only O
the O
estimation O
of O
the O
elements O
in O
V. O
The O
frequencies O
of O
elements O
not O
included O
in O
the O
correct O
SMILES O
are O
set O
to O
0 O
. O
As O
an O
example O
, O
Figure O
3 O
shows O
how O
the O
number O
of O
atoms O
of O
each O
element O
is O
constrained O
when O
the O
correct O
SMILES O
string O
is O
‚Äú O
CC O
= O
O O
‚Äù O
. O
As O
‚Äú O
C O
‚Äù O
and O
‚Äú O
O O
‚Äù O
are O
elements O
and O
‚Äú O
= O
‚Äù O
is O
a O
subsidiary O
symbol O
representing O
a O
double O
bond O
, O
the O
proposed O
constraint O
function O
treats O
the O
number O
of O
atoms O
of O
each O
element O
( O
‚Äú O
C O
‚Äù O
and O
‚Äú O
O O
‚Äù O
) O
as O
the O
error O
to O
be O
minimized O
, O
and O
disregards O
the O
‚Äú O
= O
‚Äù O
symbol O
. O
The O
objective O
function O
under O
the O
proposed O
constraints O
is O
deÔ¨Åned O
as O
follows O
: O
Lsmiles O
+ O
ŒªatomLatom O
, O
( O
4 O
) O
whereŒªatom O
is O
a O
hyperparameter O
that O
controls O
the O
degree O
of O
considering O
Latom O
. O
3.4 O
Multi O
- O
task O
Learning O
for O
Predicting O
both O
SMILES O
Strings O
and O
InChI O
Strings O
The O
same O
chemical O
structure O
is O
differently O
represented O
in O
a O
SMILES O
string O
and O
an O
InChI O
string O
. O
Assuming O
that O
the O
models O
for O
predicting O
SMILES O
and O
InChI O
strings O
compensate O
each O
other O
, O
we O
propose O
a O
multi O
- O
task O
learning O
method O
that O
shares O
the O
encoder O
of O
the O
name O
- O
to O
- O
SMILES O
and O
name O
- O
to O
- O
InChI O
conversion O
models O
, O
and O
trains O
both O
models O
at O
the O
same O
time O
. O
LetIbe O
the O
set O
of O
InChI O
strings O
. O
We O
deÔ¨Åne O
a O
training O
dataset O
consisting O
of O
nsamples O
as O
ÀúD= O
/angbracketleft(X1,T1,I1 O
) O
, O
... O
, O
, O
( O
Xn O
, O
Tn O
, O
In)/angbracketright O
, O
whereXi‚ààX O
, O
Ti‚ààT O
, O
andIi‚ààIfor1‚â§i‚â§n O
. O
The O
objectiveCompound O
Name O
EncoderInChI O
DecoderSMILES O
DecoderC O
= O
C O
O O
C O
C O
O O
C O
C O
Cl O
< O
s>1 O
S O
/ O
C O
6 O
H O
1 O
1 O
Cl O
O O
2/ O
c O
1 O
- O
2- O
8- O
56- O
9 O
- O
4- O
3 O
- O
7 O
/ O
h O
2 O
H O
, O
1 O
, O
3 O
- O
6 O
H O
2 O
< O
s O
> O
2 O
- O
( O
2 O
- O
C O
h O
l O
o O
ro O
eth O
ox O
y O
) O
ethyl O
v O
inyl O
ether O
< O
s O
> O
C O
= O
C O
O O
C O
C O
O O
C O
C O
Cl O
< O
s>1 O
S O
/ O
C O
6 O
H O
1 O
1 O
Cl O
O O
2/ O
c O
1 O
- O
28- O
5 O
- O
6- O
9 O
- O
4- O
3 O
- O
7 O
/ O
h O
2 O
H O
, O
1 O
, O
3 O
- O
6 O
H O
2 O
BPE O
TokenizerElement O
- O
wise O
& O
Character O
- O
wise O
TokenizerUnigram O
Model O
Tokenizer O
2-(2 O
- O
Chloroethoxy)ethylvinyl O
etherC O
= O
COCCOCCCl O
1S O
/ O
C6H11ClO2 O
/ O
c1 O
- O
2 O
- O
85 O
- O
6 O
- O
9 O
- O
4 O
- O
3 O
- O
7 O
/ O
h2H,1,3 O
- O
6H2 O
Figure O
4 O
: O
Overview O
of O
multi O
- O
task O
learning O
for O
predicting O
both O
SMILES O
strings O
and O
InChI O
strings O
Split O
Size O
Training O
5,000,000 O
Development O
1,113 O
Test O
11,194 O
Table O
2 O
: O
Sizes O
of O
the O
training O
, O
development O
, O
and O
test O
datasets O
is O
to O
learn O
a O
function O
Àúffrom O
ÀúD.Àúf(Xi)predicts O
bothTiandIi O
. O
SpeciÔ¨Åcally O
, O
the O
proposed O
multi O
- O
task O
learning O
minimizes O
the O
following O
objective O
function O
: O
Lsmiles O
+ O
ŒªinchiLinchi O
, O
( O
5 O
) O
Linchi O
= O
‚àílogP(I|X;Œ∏enc O
, O
Œ∏inchi O
) O
, O
whereŒ∏inchi O
andŒ∏encare O
parameter O
sets O
for O
the O
InChI O
decoder O
and O
shared O
encoder O
, O
respectively O
, O
andŒªinchi O
is O
a O
hyperparameter O
that O
controls O
the O
degree O
of O
considering O
Linchi O
. O
Lsmiles O
is O
calculated O
by O
Eq O
. O
1 O
. O
The O
method O
is O
overviewed O
in O
Figure O
4 O
. O
4 O
Experimental O
Settings O
4.1 O
Data O
Set O
In O
all O
experiments O
, O
the O
data O
comprised O
a O
chemical O
compound O
name O
and O
a O
correct O
SMILES O
string O
. O
Using O
the O
dump O
data O
of O
PubChem9(97 O
M O
compound O
records O
) O
, O
the O
chemical O
compound O
names O
were O
converted O
to O
Synonyms O
associated O
with O
each O
CID10 O
, O
and O
the O
correct O
SMILES O
strings O
were O
converted O
from O
isomeric O
SMILES O
strings11to O
canon9ftp://ftp.ncbi.nlm.nih.gov/pubchem/ O
10PubChem O
‚Äôs O
compound O
identiÔ¨Åer O
for O
a O
unique O
chemical O
structure O
11SMILES O
strings O
written O
with O
isotopic O
and O
chiral O
speciÔ¨Åcations158method O
recall O
precision O
F O
- O
measure O
Rule O
- O
based O
OPSIN O
0.693 O
0.836 O
0.758 O
tool O
A O
0.711 O
0.797 O
0.752 O
tool O
B O
0.653 O
0.800 O
0.719 O
Transformer O
- O
based O
transformer O
0.793 O
0.806 O
0.799 O
( O
BPE O
) O
atomnum O
0.798 O
0.808 O
0.803 O
inchigen O
0.810 O
0.819 O
0.814 O
Transformer O
- O
based O
transformer O
0.763 O
0.873 O
0.814 O
( O
OPSIN O
- O
TK O
+ O
BPE O
) O
atomnum O
0.768 O
0.876 O
0.818 O
inchigen O
0.779 O
0.886 O
0.829 O
Transformer O
- O
based O
transformer O
0.755 O
0.868 O
0.808 O
( O
OPSIN O
- O
TK O
) O
atomnum O
0.757 O
0.867 O
0.808 O
inchigen O
0.754 O
0.869 O
0.807 O
Table O
3 O
: O
Evaluation O
results O
of O
each O
converter O
for O
Synonyms O
. O
Transformer O
- O
based O
ones O
are O
our O
proposed O
methods O
. O
We O
evaluated O
the O
Transformer O
- O
based O
ones O
with O
different O
three O
tokenizers O
, O
BPE O
, O
OPSIN O
- O
TK+BPE O
, O
and O
OPSIN O
- O
TK O
. O
ical O
SMILES O
strings O
using O
RDKit12 O
. O
Note O
that O
in O
PubChem O
, O
the O
Synonyms O
includes O
the O
IUPAC O
names O
, O
common O
names O
, O
and O
IDs O
of O
the O
compounds O
in O
chemical O
compound O
databases O
. O
Here O
, O
we O
used O
the O
isomeric O
SMILES O
strings O
because O
they O
least O
overlap O
with O
their O
corresponding O
CIDs O
. O
In O
the O
multi O
- O
task O
learning O
, O
the O
InChI O
strings O
are O
also O
associated O
with O
CIDs O
. O
From O
the O
dump O
data O
, O
10,000 O
CIDs O
and O
100,000 O
CIDs O
were O
randomly O
selected O
as O
the O
development O
and O
test O
datasets O
, O
respectively O
, O
and O
only O
the O
two O
chemical O
compound O
names O
with O
the O
longest O
edit O
distance O
were O
assigned O
to O
each O
CID O
. O
To O
create O
Synonyms O
in O
the O
development O
and O
test O
data O
, O
chemical O
compound O
names O
like O
IDs O
in O
the O
chemical O
compound O
databases O
were O
removed O
using O
manually O
created O
regular O
expressions O
. O
In O
the O
development O
and O
test O
datasets O
, O
duplicate O
chemical O
compound O
names O
with O
different O
CIDs O
were O
removed13 O
. O
From O
the O
development O
and O
test O
datasets O
, O
we O
removed O
820 O
and O
8,241 O
duplicates O
, O
respectively O
. O
As O
the O
training O
dataset O
, O
we O
selected O
chemical O
compound O
names O
that O
were O
categorized O
as O
Synonyms O
that O
could O
be O
tokenized O
by O
the O
OPSIN O
tokenizer O
. O
The O
size O
of O
each O
dataset O
is O
listed O
in O
Table O
2 O
. O
4.2 O
Parameter O
Settings O
The O
hyperparameters O
of O
the O
Transformer O
model O
were O
set O
as O
follows O
: O
number O
of O
stacks O
in O
the O
encoder O
and O
decoder O
layers O
= O
6 O
, O
number O
of O
heads O
12https://github.com/rdkit/rdkit O
13The O
same O
chemical O
compound O
name O
may O
have O
more O
than O
one O
CID.= O
8 O
, O
embedding O
dimension O
= O
512 O
, O
and O
dropout O
probability O
= O
0.1 O
. O
The O
loss O
functions O
Lsmiles O
and O
Linchi O
were O
computed O
using O
a O
label O
- O
smoothing O
cross O
entropy O
with O
the O
smoothing O
parameter O
/epsilon1set O
to O
0.1 O
. O
The O
learning O
rate O
was O
linearly O
increased O
to O
0.0005 O
over O
the O
Ô¨Årst O
4,000 O
steps O
. O
In O
later O
steps O
, O
it O
was O
decreased O
proportionally O
to O
the O
inverse O
square O
root O
of O
the O
step O
number O
. O
The O
optimizer O
was O
an O
Adam O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
optimizer O
with O
Œ≤1= O
0.9 O
, O
Œ≤2= O
0.98,and O
/ O
epsilon1= O
10‚àí8 O
. O
The O
model O
parameters O
were O
updated O
300,000 O
times O
. O
The O
hyperparameters O
Œªatom O
andŒªinchi O
for O
controlling O
the O
degree O
of O
constraint O
consideration O
were O
set O
to O
0.7 O
and0.3 O
, O
respectively O
. O
The O
number O
of O
merge O
operations O
for O
the O
BPE O
- O
based O
tokenizer O
of O
chemical O
compound O
names O
was O
set O
to O
500 O
. O
The O
vocabulary O
size O
for O
the O
tokenizer O
of O
InChI O
strings O
was O
set O
to O
1,000 O
. O
We O
tuned O
the O
hyperparameters O
for O
our O
constraints O
and O
subword O
on O
the O
development O
data O
. O
To O
present O
the O
results O
of O
our O
Transformer O
- O
based O
models O
, O
we O
averaged O
the O
last O
10 O
checkpoints O
( O
saved O
at O
1,000 O
- O
step O
intervals O
) O
of O
the O
Transformer O
models O
. O
We O
used O
beam O
search O
with O
a O
beam O
size O
of O
4 O
and O
length O
penalty O
Œ±= O
0.6(Vaswani O
et O
al O
. O
, O
2017 O
) O
. O
The O
maximum O
output O
length O
of O
an O
inference O
was O
set O
to O
200 O
. O
5 O
Experimental O
Results O
5.1 O
Prediction O
Performance O
The O
results O
are O
shown O
in O
Table O
3 O
. O
Here O
, O
tool O
A O
and O
tool O
B O
are O
two O
commercially O
available O
tools O
, O
atomnum O
indicates O
the O
method O
based O
on O
the O
number O
of O
atoms O
described O
in O
section O
3.3 O
, O
and O
inchigen O
denotes O
the O
multitask O
learning O
method159Figure O
5 O
: O
Histogram O
of O
Jaccard O
similarities O
between O
incorrect O
structures O
generated O
by O
inchigen O
with O
BPE O
and O
their O
correct O
structures O
described O
in O
section O
3.4 O
. O
The O
notations O
BPE O
and O
OPSIN O
- O
TK O
indicate O
the O
use O
of O
the O
BPE O
- O
based O
and O
OPSIN O
tokenizers O
, O
respectively O
. O
As O
conÔ¨Årmed O
in O
Table O
3 O
, O
the O
proposed O
methods O
attained O
higher O
prediction O
performance O
the O
existing O
rule O
- O
based O
methods O
and O
the O
conventional O
Transformer O
- O
based O
model O
. O
inchigen O
with O
BPE O
showed O
0.056 O
, O
0.062 O
, O
and O
0.095 O
points O
higher O
Fmeasure O
than O
OPSIN O
, O
tool O
A O
, O
and O
tool O
B O
, O
respectively O
. O
The O
F O
- O
measure O
was O
further O
improved O
by O
combining O
the O
two O
tokenizers O
( O
see O
the O
results O
of O
OPSINTK+BPE O
in O
Table O
3 O
) O
. O
In O
the O
OPSIN O
- O
TK+BPE O
method O
, O
the O
Transformer O
- O
based O
method O
with O
BPE O
predicted O
the O
structures O
from O
chemical O
compound O
names O
that O
could O
be O
tokenized O
by O
the O
OPSIN O
tokenizer O
. O
The O
highest O
F O
- O
measure O
and O
precision O
( O
0.829 O
and O
0.886 O
, O
respectively O
) O
were O
achieved O
by O
inchigen O
with O
OPSIN O
- O
TK+BPE O
. O
In O
the O
Transformer O
- O
based O
models O
, O
the O
OPSIN O
tokenizer O
obtained O
higher O
precision O
than O
the O
BPEbased O
tokenizers O
because O
approximately O
11.5 O
% O
( O
1,293/11,194 O
) O
of O
the O
chemical O
compounds O
in O
the O
test O
set O
could O
not O
be O
tokenized O
by O
OPSIN O
. O
Consequently O
, O
the O
precision O
was O
improved O
by O
the O
reduced O
number O
of O
outputs O
. O
In O
contrast O
, O
the O
recall O
was O
lower O
than O
in O
the O
BPE O
- O
based O
tokenizers O
. O
These O
results O
clarify O
the O
impact O
of O
tokenizer O
outputs O
on O
the O
recall O
, O
precision O
, O
and O
F O
- O
measure O
scores O
. O
5.2 O
Error O
Analysis O
Most O
of O
the O
predictions O
in O
the O
Transformer O
- O
based O
approach O
were O
grammatically O
correct O
SMILES O
strings O
. O
In O
this O
context O
, O
‚Äú O
grammatically O
correct‚Äùmeans O
that O
the O
chemical O
structure O
can O
be O
visualized O
from O
the O
predicted O
SMILES O
string O
using O
RDKit O
, O
and O
does O
not O
require O
the O
correct O
SMILES O
string O
of O
a O
chemical O
compound O
name O
. O
In O
particular O
, O
inchigen O
with O
BPE O
achieved O
grammatically O
correct O
predictions O
for O
99 O
% O
of O
the O
test O
data O
, O
10.6 O
‚Äì O
17.4 O
% O
higher O
than O
OPSIN O
, O
tool O
A O
, O
and O
tool O
B. O
To O
evaluate O
the O
usefulness O
of O
the O
Transformer O
- O
based O
approach O
, O
we O
also O
analyzed O
the O
proportion O
of O
incorrect O
structure O
predictions O
that O
were O
grammatically O
correct O
SMILES O
strings O
but O
did O
not O
match O
the O
correct O
SMILES O
strings O
. O
To O
this O
end O
, O
we O
measured O
the O
Jaccard O
similarity O
( O
Tanimoto O
similarity)14between O
each O
structure O
that O
was O
incorrectly O
predicted O
by O
inchigen O
with O
BPE O
and O
the O
correct O
structure O
. O
The O
Jaccard O
similarity O
, O
a O
common O
technique O
for O
measuring O
chemical O
compound O
similarities O
, O
is O
deÔ¨Åned O
as O
follows O
: O
J(X O
, O
Y O
) O
= O
vX¬∑vY O
|vX+vY|‚àívX¬∑vY O
, O
where O
thevXandvYare O
binary O
chemical O
Ô¨Ångerprints O
of O
chemical O
compounds O
X O
and O
Y O
, O
respectively O
, O
represented O
by O
binary O
vectors O
. O
|v|is O
the O
L1 O
norm O
ofv O
, O
andvX¬∑vYis O
the O
inner O
product O
of O
vX O
andvY. O
Here O
, O
a O
chemical O
Ô¨Ångerprint O
expresses O
a O
chemical O
compound O
structure O
as O
a O
calculable O
vector O
. O
A O
famous O
type O
of O
Ô¨Ångerprint O
is O
a O
series O
of O
binary O
digits O
( O
bits O
) O
that O
represent O
the O
presence O
or O
absence O
of O
particular O
partial O
structures O
in O
the O
chemical O
compound O
. O
For O
example O
, O
the O
Molecular O
Access O
System O
key O
( O
Durant O
et O
al O
. O
, O
2002 O
) O
, O
which O
is O
used O
as O
the O
Ô¨Ångerprints O
in O
the O
present O
evaluation O
, O
comprises O
166 O
partial O
structures O
of O
chemical O
compounds O
. O
Figure O
5 O
is O
a O
histogram O
of O
the O
Jaccard O
similarity O
scores O
obtained O
in O
this O
analysis O
. O
We O
Ô¨Ånd O
that O
most O
of O
the O
incorrect O
SMILES O
strings O
generated O
by O
inchigen O
with O
BPE O
possessed O
high O
Jaccard O
similarities O
to O
the O
correct O
SMILES O
strings O
. O
The O
average O
Jaccard O
similarity O
was O
0.753 O
. O
An O
incorrect O
structure O
generated O
by O
inchigen O
with O
BPE O
is O
compared O
with O
its O
correct O
structure O
in O
Figure O
6 O
. O
The O
two O
structures O
differed O
only O
by O
whether O
ethylsulfanylbutane O
or O
methanethiol O
was O
bonded O
in O
the O
partial O
structures O
enclosed O
by O
the O
red O
ellipses O
. O
In O
other O
words O
, O
the O
two O
structures O
are O
very O
similar O
( O
Jaccard O
similarity O
= O
0.76 O
) O
. O
From O
this O
result O
, O
we O
observe O
that O
even O
when O
the O
proposed O
method O
generates O
an O
incorrect O
structure O
, O
14Jaccard O
similarity O
, O
also O
called O
the O
Tanimoto O
similarity O
, O
measures O
the O
similarities O
between O
pairs O
of O
chemical O
compounds.160ONH O
HSO O
HOO O
( O
a O
) O
Predicted O
by O
inchigen O
with O
BPE O
S O
NH O
OO O
OHO O
  O
( O
b O
) O
Correct O
Chemical O
Structure O
Figure O
6 O
: O
Example O
of O
a O
chemical O
structure O
mistakenly O
for O
‚Äú O
fmoc O
- O
l O
- O
buthionine O
‚Äù O
. O
The O
red O
- O
edged O
ellipses O
enclose O
the O
partial O
structures O
that O
differ O
between O
the O
two O
chemical O
structures O
. O
the O
outcome O
does O
not O
deviate O
greatly O
from O
the O
correct O
structure O
. O
6 O
Related O
Work O
6.1 O
Predicting O
SMILES O
Strings O
from O
Chemical O
Compound O
Names O
OPSIN O
( O
Lowe O
et O
al O
. O
, O
2011 O
) O
is O
a O
rule O
- O
based O
parser O
that O
generates O
SMILES O
strings O
and O
InChI O
strings O
from O
chemical O
compound O
names O
( O
mainly O
from O
IUPAC O
names O
) O
. O
The O
OPSIN O
tokenization O
approach O
is O
based O
on O
regular O
grammar O
. O
From O
a O
tokenized O
chemical O
name O
, O
an O
XML O
parse O
tree O
is O
constructed O
. O
Stepwise O
operations O
on O
this O
tree O
are O
continued O
until O
the O
structure O
has O
been O
reconstructed O
from O
the O
name O
. O
The O
construction O
is O
performed O
on O
substructures O
associated O
with O
the O
terms O
. O
As O
mentioned O
earlier O
, O
many O
of O
chemical O
compound O
names O
described O
in O
papers O
and O
patents O
do O
not O
comply O
with O
IUPAC O
names O
or O
other O
systematic O
nomenclatures O
, O
so O
are O
difÔ¨Åcult O
to O
reconstruct O
using O
rule O
- O
based O
methods O
. O
In O
our O
preliminary O
experiments O
using O
OPSIN O
and O
commercially O
available O
tools O
, O
the O
F O
- O
measures O
of O
predicting O
the O
IUPAC O
names O
in O
the O
dataset O
ranged O
from O
0.878 O
to O
0.960 O
. O
However O
, O
on O
the O
Synonyms O
dataset O
, O
the O
F O
- O
measures O
fell O
to O
0.719 O
- O
0.758 O
. O
6.2 O
Deep O
Learning O
methods O
using O
SMILES O
Recently O
, O
SMILES O
strings O
have O
been O
applied O
to O
chemical O
reaction O
prediction O
( O
Nam O
and O
Kim O
, O
2016 O
; O
Schwaller O
et O
al O
. O
, O
2019 O
) O
. O
The O
method O
of O
Nam O
and O
Kim O
( O
2016 O
) O
predicts O
SMILES O
strings O
representing O
products O
from O
SMILES O
strings O
representing O
reactants O
and O
reagents O
. O
This O
method O
employs O
a O
sequence O
- O
to O
- O
sequence O
model O
with O
an O
attention O
mechanism O
based O
on O
a O
recurrent O
neural O
network O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
) O
. O
Schwaller O
et O
al O
. O
( O
2019 O
) O
achieved O
higher O
accuracy O
than O
Nam O
andKim O
( O
2016 O
) O
‚Äôs O
model O
by O
applying O
the O
conventional O
Transformer O
model O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O
Similarly O
to O
our O
study O
, O
their O
models O
adapt O
SMILES O
strings O
to O
sequence O
- O
to O
- O
sequence O
models O
, O
but O
our O
target O
task O
( O
predicting O
chemical O
structures O
from O
their O
chemical O
compound O
names O
) O
differs O
from O
theirs O
. O
To O
improve O
the O
accuracy O
of O
our O
target O
task O
, O
we O
will O
improve O
the O
update O
speed O
and O
quality O
of O
our O
chemical O
compounds O
databases O
. O
We O
also O
intend O
to O
solve O
other O
chemistry O
problems O
, O
including O
chemical O
reactions O
, O
by O
predictive O
machine O
learning O
. O
7 O
Conclusions O
This O
paper O
introduced O
our O
Transformer O
- O
based O
prediction O
methods O
, O
which O
convert O
chemical O
compound O
names O
to O
SMILES O
strings O
trained O
with O
the O
constraint O
of O
the O
number O
of O
atoms O
of O
each O
element O
in O
the O
SMILES O
string O
. O
We O
also O
proposed O
a O
multitask O
learning O
approach O
that O
simultaneously O
learns O
the O
conversions O
to O
SMILES O
strings O
and O
InChI O
strings O
. O
In O
an O
experimental O
comparison O
evaluation O
, O
our O
proposed O
method O
achieved O
higher O
F O
- O
measures O
than O
the O
existing O
methods O
. O
In O
future O
work O
, O
we O
intend O
to O
explore O
various O
tokenization O
methods O
, O
and O
further O
improve O
the O
prediction O
performance O
. O
We O
also O
hope O
to O
apply O
the O
proposed O
loss O
function O
to O
multi O
- O
task O
learning O
. O
Acknowledgments O
The O
research O
results O
were O
achieved O
by O
the O
RIKEN O
AIP O
- O
FUJITSU O
Collaboration O
Center O
, O
Japan O
. O
Abstract O
In O
recent O
years O
, O
pre O
- O
trained O
models O
have O
been O
extensively O
studied O
, O
and O
several O
downstream O
tasks O
have O
beneÔ¨Åted O
from O
their O
utilization O
. O
In O
this O
study O
, O
we O
verify O
the O
effectiveness O
of O
two O
methods O
that O
incorporate O
a O
BERT O
- O
based O
pre O
- O
trained O
model O
developed O
by O
Cui O
et O
al O
. O
( O
2020 O
) O
into O
an O
encoder O
- O
decoder O
model O
on O
Chinese O
grammatical O
error O
correction O
tasks O
. O
We O
also O
analyze O
the O
error O
type O
and O
conclude O
that O
sentence O
- O
level O
errors O
are O
yet O
to O
be O
addressed O
. O
1 O
Introduction O
Grammatical O
error O
correction O
( O
GEC O
) O
can O
be O
regarded O
as O
a O
sequence O
- O
to O
- O
sequence O
task O
. O
GEC O
systems O
receive O
an O
erroneous O
sentence O
written O
by O
a O
language O
learner O
and O
output O
the O
corrected O
sentence O
. O
In O
previous O
studies O
that O
adopted O
neural O
models O
for O
Chinese O
GEC O
( O
Ren O
et O
al O
. O
, O
2018 O
; O
Zhou O
et O
al O
. O
, O
2018 O
) O
, O
the O
performance O
was O
improved O
by O
initializing O
the O
models O
with O
a O
distributed O
word O
representation O
, O
such O
as O
Word2Vec O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
. O
However O
, O
in O
these O
methods O
, O
only O
the O
embedding O
layer O
of O
a O
pretrained O
model O
was O
used O
to O
initialize O
the O
models O
. O
In O
recent O
years O
, O
pre O
- O
trained O
models O
based O
on O
Bidirectional O
Encoder O
Representations O
from O
Transformers O
( O
BERT O
) O
have O
been O
studied O
extensively O
( O
Devlin O
et O
al O
. O
, O
2019 O
; O
Liu O
et O
al O
. O
, O
2019 O
) O
, O
and O
the O
performance O
of O
many O
downstream O
Natural O
Language O
Processing O
( O
NLP O
) O
tasks O
has O
been O
dramatically O
improved O
by O
utilizing O
these O
pre O
- O
trained O
models O
. O
To O
learn O
existing O
knowledge O
of O
a O
language O
, O
a O
BERTbased O
pre O
- O
trained O
model O
is O
trained O
on O
a O
large O
- O
scale O
corpus O
using O
the O
encoder O
of O
Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O
Subsequently O
, O
for O
a O
downstream O
task O
, O
a O
neural O
network O
model O
is O
initialized O
with O
the O
weights O
learned O
by O
a O
pre O
- O
trained O
model O
that O
has O
the O
same O
structure O
and O
is O
Ô¨Åne O
- O
tuned O
on O
training O
data O
of O
‚àóCurrently O
at O
Nomura O
Research O
Institute O
, O
Ltd. O
‚Ä†Currently O
at O
Retrieva O
, O
Inc. O
Figure O
1 O
: O
Two O
methods O
for O
incorporating O
a O
pre O
- O
trained O
model O
into O
the O
GEC O
model O
. O
the O
downstream O
task O
. O
Using O
this O
two O
- O
stage O
method O
, O
the O
performance O
is O
expected O
to O
improve O
because O
downstream O
tasks O
are O
informed O
by O
the O
knowledge O
learned O
by O
the O
pre O
- O
trained O
model O
. O
Recent O
works O
( O
Kaneko O
et O
al O
. O
, O
2020 O
; O
Kantor O
et O
al O
. O
, O
2019 O
) O
show O
that O
BERT O
helps O
improve O
the O
performance O
on O
the O
English O
GEC O
task O
. O
As O
the O
Chinese O
pre O
- O
trained O
models O
are O
developed O
and O
released O
continuously O
( O
Cui O
et O
al O
. O
, O
2020 O
; O
Zhang O
et O
al O
. O
, O
2019 O
) O
, O
the O
Chinese O
GEC O
task O
may O
also O
beneÔ¨Åt O
from O
using O
those O
pre O
- O
trained O
models O
. O
In O
this O
study O
, O
as O
shown O
in O
Figure O
1 O
, O
we O
develop O
a O
Chinese O
GEC O
model O
based O
on O
Transformer O
with O
a O
pre O
- O
trained O
model O
using O
two O
methods O
: O
Ô¨Årst O
, O
by O
initializing O
the O
encoder O
with O
the O
pre O
- O
trained O
model O
( O
BERT O
- O
encoder O
) O
; O
second O
, O
by O
utilizing O
the O
technique O
proposed O
by O
Zhu O
et O
al O
. O
( O
2020 O
) O
, O
which O
uses O
the O
pre O
- O
trained O
model O
for O
additional O
features O
( O
BERTfused O
) O
; O
on O
the O
Natural O
Language O
Processing O
and O
Chinese O
Computing O
( O
NLPCC O
) O
2018 O
Grammatical O
Error O
Correction O
shared O
task O
test O
dataset O
( O
Zhao O
et O
al O
. O
, O
2018 O
) O
, O
our O
single O
models O
obtain O
F0.5scores O
of O
29.76 O
and O
29.94 O
respectively O
, O
which O
is O
similar O
to O
the O
performance O
of O
ensemble O
models O
developed163by O
the O
top O
team O
of O
the O
shared O
task O
. O
Moreover O
, O
using O
a O
4 O
- O
ensemble O
model O
, O
we O
obtain O
an O
F0.5score O
of O
35.51 O
, O
which O
outperforms O
the O
results O
from O
the O
top O
team O
by O
a O
large O
margin O
. O
We O
annotate O
the O
error O
types O
of O
the O
development O
data O
; O
the O
results O
show O
that O
word O
- O
level O
errors O
dominate O
all O
error O
types O
and O
that O
sentence O
- O
level O
errors O
remain O
challenging O
and O
require O
a O
stronger O
approach O
. O
2 O
Related O
Work O
Given O
the O
success O
of O
the O
shared O
tasks O
on O
English O
GEC O
at O
the O
Conference O
on O
Natural O
Language O
Learning O
( O
CoNLL O
) O
( O
Ng O
et O
al O
. O
, O
2013 O
, O
2014 O
) O
, O
a O
Chinese O
GEC O
shared O
task O
was O
performed O
at O
the O
NLPCC O
2018 O
. O
In O
this O
task O
, O
approximately O
one O
million O
sentences O
from O
the O
language O
learning O
website O
Lang81were O
used O
as O
training O
data O
and O
two O
thousand O
sentences O
from O
the O
PKU O
Chinese O
Learner O
Corpus O
( O
Zhao O
et O
al O
. O
, O
2018 O
) O
were O
used O
as O
test O
data O
. O
Here O
, O
we O
brieÔ¨Çy O
describe O
the O
three O
methods O
with O
the O
highest O
performance O
. O
First O
, O
Fu O
et O
al O
. O
( O
2018 O
) O
combined O
a O
5 O
- O
gram O
language O
model O
- O
based O
spell O
checker O
with O
subwordlevel O
and O
character O
- O
level O
encoder O
- O
decoder O
models O
using O
Transformer O
to O
obtain O
Ô¨Åve O
types O
of O
outputs O
. O
Then O
, O
they O
re O
- O
ranked O
these O
outputs O
using O
the O
language O
model O
. O
Although O
they O
reported O
a O
high O
performance O
, O
several O
models O
were O
required O
, O
and O
the O
combination O
method O
was O
complex O
. O
Second O
, O
Ren O
et O
al O
. O
( O
2018 O
) O
utilized O
a O
convolutional O
neural O
network O
( O
CNN O
) O
, O
such O
as O
in O
Chollampatt O
and O
Ng O
( O
2018 O
) O
. O
However O
, O
because O
the O
structure O
of O
the O
CNN O
is O
different O
from O
that O
of O
BERT O
, O
it O
can O
not O
be O
initialized O
with O
the O
weights O
learned O
by O
the O
BERT O
. O
Last O
, O
Zhao O
and O
Wang O
( O
2020 O
) O
proposed O
a O
dynamic O
masking O
method O
that O
replaces O
the O
tokens O
in O
the O
source O
sentences O
of O
the O
training O
data O
with O
other O
tokens O
( O
e.g. O
[ O
PAD O
] O
token O
) O
. O
They O
achieved O
stateof O
- O
the O
- O
art O
results O
on O
the O
NLPCC O
2018 O
Grammar O
Error O
Correction O
shared O
task O
without O
using O
any O
extra O
knowledge O
. O
This O
is O
a O
data O
augmentation O
method O
that O
can O
be O
a O
supplement O
for O
our O
study O
. O
3 O
Methods O
In O
the O
proposed O
method O
, O
we O
construct O
a O
correction O
model O
using O
Transformer O
, O
and O
incorporate O
a O
Chinese O
pre O
- O
trained O
model O
developed O
by O
Cui O
et O
al O
. O
( O
2020 O
) O
in O
two O
ways O
as O
described O
in O
the O
following O
sections O
. O
1https://lang-8.com/3.1 O
Chinese O
Pre O
- O
trained O
Model O
We O
use O
a O
BERT O
- O
based O
model O
as O
our O
pre O
- O
trained O
model O
. O
BERT O
is O
mainly O
trained O
with O
a O
task O
called O
Masked O
Language O
Model O
. O
In O
the O
Masked O
Language O
Model O
task O
, O
some O
tokens O
in O
a O
sentence O
are O
replaced O
with O
masked O
tokens O
( O
[ O
MASK O
] O
) O
and O
the O
model O
has O
to O
predict O
the O
replaced O
tokens O
. O
In O
this O
study O
, O
we O
use O
the O
Chinese O
- O
RoBERTawwm O
- O
ext O
model O
developed O
by O
Cui O
et O
al O
. O
( O
2020 O
) O
. O
The O
main O
difference O
between O
Chinese O
- O
RoBERTawwm O
- O
ext O
and O
the O
original O
BERT O
is O
that O
the O
latter O
uses O
whole O
word O
masking O
( O
WWM O
) O
to O
train O
the O
model O
. O
In O
WWM O
, O
when O
a O
Chinese O
character O
is O
masked O
, O
other O
Chinese O
characters O
that O
belong O
to O
the O
same O
word O
should O
also O
be O
masked O
. O
3.2 O
Grammatical O
Error O
Correction O
Model O
In O
this O
study O
, O
we O
use O
Transformer O
as O
the O
correction O
model O
. O
Transformer O
has O
shown O
excellent O
performance O
in O
sequence O
- O
to O
- O
sequence O
tasks O
, O
such O
as O
machine O
translation O
, O
and O
has O
been O
widely O
adopted O
in O
recent O
studies O
on O
English O
GEC O
( O
Kiyono O
et O
al O
. O
, O
2019 O
; O
Junczys O
- O
Dowmunt O
et O
al O
. O
, O
2018 O
) O
. O
However O
, O
a O
BERT O
- O
based O
pre O
- O
trained O
model O
only O
uses O
the O
encoder O
of O
Transformer O
; O
therefore O
, O
it O
can O
not O
be O
directly O
applied O
to O
sequence O
- O
to O
- O
sequence O
tasks O
that O
require O
both O
an O
encoder O
and O
a O
decoder O
, O
such O
as O
GEC O
. O
Hence O
, O
we O
incorporate O
the O
encoderdecoder O
model O
with O
the O
pre O
- O
trained O
model O
in O
two O
ways O
as O
described O
in O
the O
following O
subsections O
. O
BERT O
- O
encoder O
We O
initialize O
the O
encoder O
of O
Transformer O
with O
the O
parameters O
learned O
by O
Chinese O
- O
RoBERTa O
- O
wwm O
- O
ext O
; O
the O
decoder O
is O
initialized O
randomly O
. O
Finally O
, O
we O
Ô¨Åne O
- O
tune O
the O
initialized O
model O
on O
Chinese O
GEC O
data O
. O
BERT O
- O
fused O
Zhu O
et O
al O
. O
( O
2020 O
) O
proposed O
a O
method O
that O
uses O
a O
pre O
- O
trained O
model O
as O
the O
additional O
features O
. O
In O
this O
method O
, O
input O
sentences O
are O
fed O
into O
the O
pre O
- O
trained O
model O
and O
representations O
from O
the O
last O
layer O
of O
the O
pre O
- O
trained O
model O
are O
acquired O
Ô¨Årst O
. O
Then O
, O
the O
representations O
will O
interact O
with O
the O
encoder O
and O
decoder O
by O
using O
attention O
mechanism O
. O
Kaneko O
et O
al O
. O
( O
2020 O
) O
veriÔ¨Åed O
the O
effectiveness O
of O
this O
method O
on O
English O
GEC O
tasks O
. O
4 O
Experiments O
4.1 O
Experimental O
Settings O
Data O
In O
this O
study O
, O
we O
use O
the O
data O
provided O
by O
the O
NLPCC O
2018 O
Grammatical O
Error O
Correction164shared O
task O
. O
We O
Ô¨Årst O
segment O
all O
sentences O
into O
characters O
because O
the O
Chinese O
pre O
- O
trained O
model O
we O
used O
is O
character O
- O
based O
. O
In O
the O
GEC O
task O
, O
source O
and O
target O
sentences O
do O
not O
tend O
to O
change O
signiÔ¨Åcantly O
. O
Considering O
this O
, O
we O
Ô¨Ålter O
the O
training O
data O
by O
excluding O
sentence O
pairs O
that O
meet O
the O
following O
criteria O
: O
i O
) O
the O
source O
sentence O
is O
identical O
to O
the O
target O
sentence O
; O
ii O
) O
the O
edit O
distance O
between O
the O
source O
sentence O
and O
the O
target O
sentence O
is O
greater O
than O
15 O
; O
iii O
) O
the O
number O
of O
characters O
of O
the O
source O
sentence O
or O
the O
target O
sentence O
exceeds O
64 O
. O
Once O
the O
training O
data O
were O
Ô¨Åltered O
, O
we O
obtained O
971,318 O
sentence O
pairs O
. O
Because O
the O
NLPCC O
2018 O
Grammatical O
Error O
Correction O
shared O
task O
did O
not O
provide O
development O
data O
, O
we O
opted O
to O
randomly O
extract O
5,000 O
sentences O
from O
the O
training O
data O
as O
the O
development O
data O
following O
Ren O
et O
al O
. O
( O
2018 O
) O
. O
The O
test O
data O
consist O
of O
2,000 O
sentences O
extracted O
from O
the O
PKU O
Chinese O
Learner O
Corpus O
. O
According O
to O
Zhao O
et O
al O
. O
( O
2018 O
) O
, O
the O
annotation O
guidelines O
follow O
the O
minimum O
edit O
distance O
principle O
( O
Nagata O
and O
Sakaguchi O
, O
2016 O
) O
, O
which O
selects O
the O
edit O
operation O
that O
minimizes O
the O
edit O
distance O
from O
the O
original O
sentence O
. O
Model O
We O
implement O
the O
Transformer O
model O
using O
fairseq O
0.8.0.2and O
load O
the O
pre O
- O
trained O
model O
using O
pytorch O
transformer O
2.2.0.3 O
We O
then O
train O
the O
following O
models O
based O
on O
Transformer O
. O
Baseline O
: O
a O
plain O
Transformer O
model O
that O
is O
initialized O
randomly O
without O
using O
a O
pre O
- O
trained O
model O
. O
BERT O
- O
encoder O
: O
the O
correction O
model O
introduced O
in O
Section O
3.2 O
. O
BERT O
- O
fused O
: O
the O
correction O
model O
introduced O
in O
Section O
3.2 O
. O
We O
use O
the O
implementation O
provided O
by O
Zhu O
et O
al O
. O
( O
2020).4 O
Finally O
, O
we O
train O
a O
4 O
- O
ensemble O
BERT O
- O
encoder O
model O
and O
a O
4 O
- O
ensemble O
BERT O
- O
fused O
model O
. O
More O
details O
on O
the O
training O
are O
provided O
in O
the O
appendix O
A. O
Evaluation O
As O
the O
evaluation O
is O
performed O
on O
word O
- O
unit O
, O
we O
strip O
all O
delimiters O
from O
the O
system O
output O
sentences O
and O
segment O
the O
sentences O
using O
2https://github.com/pytorch/fairseq O
3https://github.com/huggingface/ O
transformers O
4https://github.com/bert-nmt/bert-nmt[Our O
models O
] O
P O
R O
F0.5 O
Baseline O
25.14 O
14.34 O
21.85 O
BERT O
- O
encoder O
32.67 O
22.19 O
29.76 O
BERT O
- O
fused O
32.11 O
23.57 O
29.94 O
BERT O
- O
encoder O
( O
4 O
- O
ensemble O
) O
41.94 O
22.02 O
35.51 O
BERT O
- O
fused O
( O
4 O
- O
ensemble O
) O
32.20 O
23.16 O
29.87 O
[ O
SOTA O
Result O
] O
Zhao O
and O
Wang O
( O
2020 O
) O
44.36 O
22.18 O
36.97 O
[ O
NLPCC O
2018 O
] O
Fu O
et O
al O
. O
( O
2018 O
) O
35.24 O
18.64 O
29.91 O
Ren O
et O
al O
. O
( O
2018 O
) O
41.73 O
13.08 O
29.02 O
Ren O
et O
al O
. O
( O
2018 O
) O
( O
4 O
- O
ensemble O
) O
47.63 O
12.56 O
30.57 O
Table O
1 O
: O
Experimental O
results O
on O
the O
NLPCC O
2018 O
Grammatical O
Error O
Correction O
shared O
task O
. O
the O
pkunlp5provided O
in O
the O
NLPCC O
2018 O
Grammatical O
Error O
Correction O
shared O
task O
. O
Based O
on O
the O
setup O
of O
the O
NLPCC O
2018 O
Grammatical O
Error O
Correction O
shared O
task O
, O
the O
evaluation O
is O
conducted O
using O
MaxMatch O
( O
M2).6 O
4.2 O
Evaluation O
Results O
Table O
1 O
summarizes O
the O
experimental O
results O
of O
our O
models O
. O
We O
run O
the O
single O
models O
four O
times O
, O
and O
report O
the O
average O
score O
. O
For O
comparison O
, O
we O
also O
cite O
the O
result O
of O
the O
state O
- O
of O
- O
the O
- O
art O
model O
( O
Zhao O
and O
Wang O
, O
2020 O
) O
and O
the O
results O
of O
the O
models O
developed O
by O
two O
teams O
in O
the O
NLPCC O
2018 O
Grammatical O
Error O
Correction O
shared O
task O
. O
The O
performances O
of O
BERT O
- O
encoder O
and O
BERTfused O
are O
signiÔ¨Åcantly O
superior O
to O
that O
of O
the O
baseline O
model O
and O
are O
comparable O
to O
those O
achieved O
by O
the O
two O
teams O
in O
the O
NLPCC O
2018 O
Grammatical O
Error O
Correction O
shared O
task O
, O
indicating O
the O
effectiveness O
of O
adopting O
the O
pre O
- O
trained O
model O
. O
The O
BERT O
- O
encoder O
( O
4 O
- O
ensemble O
) O
model O
yields O
anF0.5score O
nearly O
5 O
points O
higher O
than O
the O
highest O
- O
performance O
model O
in O
the O
NLPCC O
2018 O
Grammatical O
Error O
Correction O
shared O
task O
. O
However O
, O
there O
is O
no O
improvement O
for O
the O
BERT O
- O
fused O
( O
4 O
- O
ensemble O
) O
model O
compared O
with O
the O
single O
BERT O
- O
fused O
model O
. O
We O
Ô¨Ånd O
that O
the O
performance O
of O
the O
BERT O
- O
fused O
model O
depends O
on O
the O
warm O
- O
up O
model O
. O
Compared O
with O
Kaneko O
et O
al O
. O
( O
2020 O
) O
using O
a O
state O
- O
of O
- O
the O
- O
art O
model O
to O
warm O
- O
up O
their O
BERTfused O
model O
, O
we O
did O
not O
use O
a O
warm O
- O
up O
model O
in O
this O
work O
. O
The O
performance O
noticeably O
drops O
when O
we O
try O
to O
warm O
- O
up O
the O
BERT O
- O
fused O
model O
from O
a O
weak O
baseline O
model O
, O
therefore O
, O
the O
BERT O
- O
fused O
model O
may O
perform O
better O
when O
warmed O
- O
up O
from O
a O
5http://59.108.48.12/lcwm/pkunlp/ O
downloads O
/ O
libgrass O
- O
ui.tar.gz O
6https://github.com/nusnlp/m2scorer165src O
ÊåÅ O
ÊåÅ O
ÊåÅÂà´ O
Âà´ O
Âà´ÊòØÂåó‰∫¨ÔºåÊ≤°Êúâ‚ÄúËá™ÁÑ∂‚ÄùÁöÑÊÑüËßâ O
„ÄÇ O
‰∫∫‰ª¨Âú®‰∏ÄËæàÂ≠êÁªè O
Áªè O
ÁªèÈ™å O
È™å O
È™åÂæàÂ§ö‰∫ãÊÉÖ O
„ÄÇ O
gold O
Áâπ O
Áâπ O
ÁâπÂà´ O
Âà´ O
Âà´ÊòØÂåó‰∫¨ÔºåÊ≤°Êúâ‚ÄúËá™ÁÑ∂‚ÄùÁöÑÊÑüËßâ O
„ÄÇ O
‰∫∫‰ª¨Âú®‰∏ÄËæàÂ≠êÁªè O
Áªè O
ÁªèÂéÜ O
ÂéÜ O
ÂéÜÂæàÂ§ö‰∫ãÊÉÖ O
„ÄÇ O
baseline O
ÊåÅ O
ÊåÅ O
ÊåÅÂà´ O
Âà´ O
Âà´ÊòØÂåó‰∫¨ÔºåÊ≤°Êúâ‚ÄúËá™ÁÑ∂‚ÄùÁöÑÊÑüËßâ O
„ÄÇ O
‰∫∫‰ª¨Âú®‰∏ÄËæàÂ≠êÁªè O
Áªè O
ÁªèÂéÜ O
ÂéÜ O
ÂéÜ‰∫ÜÂæàÂ§ö‰∫ãÊÉÖ O
„ÄÇ O
BERT O
- O
encoder O
Áâπ O
Áâπ O
ÁâπÂà´ O
Âà´ O
Âà´ÊòØÂåó‰∫¨ÔºåÊ≤°Êúâ‚ÄúËá™ÁÑ∂‚ÄùÁöÑÊÑüËßâ O
„ÄÇ O
‰∫∫‰ª¨ O
‰∏ÄËæàÂ≠ê‰ºö O
‰ºö O
‰ºöÁªè O
Áªè O
ÁªèÂéÜ O
ÂéÜ O
ÂéÜÂæàÂ§ö‰∫ãÊÉÖ O
„ÄÇ O
Translation O
Especially O
in O
Beijing O
, O
there O
is O
no O
natural O
feeling O
. O
People O
experience O
many O
things O
in O
their O
lifetime O
. O
Table O
2 O
: O
Source O
sentence O
, O
gold O
edit O
, O
and O
output O
of O
our O
models O
. O
Error O
TypeNumber O
of O
errorsExamples O
B O
9ÊúÄÂêéÔºåË¶ÅÂÖ≥‰∏ª{ÂÖ≥ O
ÂÖ≥ O
ÂÖ≥Ê≥® O
Ê≥® O
Ê≥®}‰∏Ä‰∫õÂÖ≥‰∫éÂ§©Ê∞îÈ¢ÑÊä•ÁöÑÊñ∞Èóª O
„ÄÇ O
( O
Finally O
, O
pay O
attention O
to O
some O
weather O
forecast O
news O
. O
) O
CC O
35Êúâ‰∏ÄÂ§©Êôö‰∏ä O
‰ªñ O
‰∏ã O
‰∫Ü O
ÂÜ≥ÂÆö{ÂÜ≥ O
ÂÜ≥ O
ÂÜ≥ÂøÉ O
ÂøÉ O
ÂøÉ}ÂêëÂØå‰∏ΩÂ†ÇÁöá O
ÁöÑÂÆ´ÊÆøÈáåËµ∞ÔºåÂÅ∑ÂÅ∑ÁöÑ{Âú∞ O
Âú∞ O
Âú∞}ËøõÂÖ• O
ÂÆ´ÂÜÖ„ÄÇ(One O
night O
he O
decided O
to O
walk O
to O
the O
magniÔ¨Åcent O
palace O
, O
and O
sneaked O
in O
it O
secretly O
. O
) O
CQ O
30Âú®‰∏äÊµ∑ÊàëÊÄªÊòØ‰ΩèNONE{Âú® O
Âú® O
Âú®}‰∏ÄÂÆ∂ÁâπÂÆöNONE{ÁöÑ O
ÁöÑ O
ÁöÑ}ÈÖíÂ∫ó O
„ÄÇ O
( O
I O
always O
stay O
in O
the O
same O
hotel O
in O
Shanghai O
. O
) O
CD O
21 O
ÊàëÂæàÂñúÊ¨¢Âøµ{NONE}ËØªÂ∞èËØ¥ O
. O
( O
I O
like O
to O
read O
novels O
. O
) O
CJ O
35 O
. O
. O
. O
. O
. O
.‰ΩÜÊòØÂêåÊó∂‰πüÂØπÁéØÂ¢ÉÈóÆÈ¢ò{NONE}Êó•Áõä‰∏•ÈáçÈÄ†Êàê‰∫Ü{ÈÄ† O
ÈÄ† O
ÈÄ†Êàê O
Êàê O
Êàê‰∫Ü O
‰∫Ü O
‰∫ÜÊó• O
Êó• O
Êó•Áõä O
Áõä O
Áõä‰∏• O
‰∏• O
‰∏•Èáç O
Èáç O
ÈáçÁöÑ O
ÁöÑ O
ÁöÑ O
} O
Á©∫Ê∞î O
Ê±°ÊüìÈóÆÈ¢ò„ÄÇ(But O
on O
the O
meanwhile O
, O
it O
also O
aggravated O
the O
problem O
of O
air O
pollution O
. O
) O
Table O
3 O
: O
Examples O
of O
each O
error O
type O
. O
The O
underlined O
tokens O
are O
detected O
errors O
that O
should O
be O
replaced O
with O
the O
tokens O
in O
braces O
. O
TypeDetection O
Correction O
P O
RF0.5 O
P O
RF0.5 O
BERT O
- O
encoder O
B O
80.0 O
55.6 O
73.5 O
80.0 O
55.6 O
73.5 O
CC O
62.5 O
31.4 O
52.2 O
43.8 O
20.0 O
35.4 O
CQ O
65.0 O
43.3 O
59.1 O
45.0 O
30.0 O
40.9 O
CD O
58.3 O
28.6 O
48.3 O
50.0 O
28.6 O
43.5 O
CJ O
56.5 O
42.9 O
53.1 O
4.3 O
2.9 O
3.9 O
BERT O
- O
fused O
B O
80.0 O
44.4 O
69.0 O
80.0 O
44.4 O
69.0 O
CC O
61.9 O
42.9 O
56.9 O
38.1 O
22.9 O
33.6 O
CQ O
69.0 O
63.3 O
67.8 O
44.8 O
46.7 O
45.2 O
CD O
71.4 O
42.9 O
63.0 O
57.1 O
38.1 O
51.9 O
CJ O
63.2 O
34.3 O
54.1 O
15.8 O
8.6 O
13.5 O
Table O
4 O
: O
Detection O
and O
correction O
performance O
of O
BERT O
- O
encoder O
and O
BERT O
- O
fused O
models O
on O
each O
type O
of O
error O
. O
stronger O
model O
( O
e.g. O
, O
the O
model O
proposed O
by O
Zhao O
and O
Wang O
( O
2020 O
) O
) O
. O
For O
the O
state O
- O
of O
- O
the O
- O
art O
result O
achieved O
by O
Zhao O
and O
Wang O
( O
2020 O
) O
, O
both O
the O
precision O
and O
the O
recall O
are O
comparatively O
high O
, O
and O
they O
therefore O
obtain O
the O
best O
F0.5score O
. O
Additionally O
, O
the O
precision O
of O
the O
models O
that O
used O
a O
pre O
- O
trained O
model O
is O
lower O
than O
that O
of O
the O
models O
proposed O
by O
the O
two O
teams O
; O
conversely O
, O
the O
recall O
is O
signiÔ¨Åcantly O
higher O
. O
5 O
Discussion O
Case O
Analysis O
Table O
2 O
shows O
the O
sample O
outputs O
. O
In O
the O
Ô¨Årst O
example O
, O
the O
spelling O
error O
ÊåÅÂà´is O
accurately O
corrected O
to O
ÁâπÂà´(which O
means O
especially O
) O
by O
the O
proposed O
model O
, O
whereas O
it O
is O
not O
corrected O
by O
the O
baseline O
model O
. O
Hence O
, O
it O
appearsthat O
the O
proposed O
model O
captures O
context O
more O
efÔ¨Åciently O
by O
using O
the O
pre O
- O
trained O
model O
through O
the O
WWM O
strategy O
. O
In O
the O
second O
example O
, O
the O
output O
of O
the O
proposed O
model O
is O
more O
Ô¨Çuent O
, O
although O
the O
correction O
made O
by O
the O
proposed O
model O
is O
different O
from O
the O
gold O
edit O
. O
The O
proposed O
model O
not O
only O
changed O
the O
wrong O
word O
ÁªèÈ™å(which O
usually O
means O
the O
noun O
experience O
) O
toÁªèÂéÜ(which O
usually O
means O
the O
verb O
experience O
) O
, O
but O
also O
added O
a O
new O
word O
‰ºö O
( O
would O
, O
could O
) O
; O
this O
addition O
makes O
the O
sentence O
more O
Ô¨Çuent O
. O
It O
appears O
that O
the O
proposed O
model O
can O
implement O
additional O
changes O
to O
the O
source O
sentence O
because O
the O
pre O
- O
trained O
model O
is O
trained O
with O
a O
large O
- O
scale O
corpus O
. O
However O
, O
this O
type O
of O
change O
may O
affect O
the O
precision O
because O
the O
gold O
edit O
in O
this O
dataset O
followed O
the O
principle O
of O
minimum O
edit O
distance O
( O
Zhao O
et O
al O
. O
, O
2018 O
) O
. O
Error O
Type O
Analysis O
To O
understand O
the O
error O
distribution O
of O
Chinese O
GEC O
, O
we O
annotate O
100 O
sentences O
of O
development O
data O
and O
obtain O
130 O
errors O
( O
one O
sentence O
may O
contain O
more O
than O
one O
error O
) O
. O
We O
refer O
to O
the O
annotation O
of O
the O
HSK O
learner O
corpus7and O
adopt O
Ô¨Åve O
categories O
of O
error O
: O
B O
, O
CC O
, O
CQ O
, O
CD O
, O
and O
CJ O
. O
B O
denotes O
character O
- O
level O
errors O
, O
which O
are O
mainly O
spelling O
and O
punctuation O
errors O
. O
CC O
, O
CQ O
, O
and O
CD O
are O
word O
- O
level O
errors O
, O
which O
are O
word O
selection O
, O
missed O
word O
, O
and O
redundant O
word O
errors O
, O
respectively O
. O
CJ O
denotes O
sentence O
- O
level O
errors O
which O
contain O
several O
complex O
errors O
, O
such O
as O
word O
order O
and O
lack O
of O
subject O
errors O
. O
Several O
7http://hsk.blcu.edu.cn/166examples O
are O
presented O
in O
Table O
3 O
. O
Based O
on O
the O
number O
of O
errors O
, O
it O
is O
evident O
that O
word O
- O
level O
errors O
( O
CC O
, O
CQ O
, O
and O
CD O
) O
are O
the O
most O
frequent O
. O
Table O
4 O
lists O
the O
detection O
and O
correction O
results O
of O
the O
BERT O
- O
encoder O
and O
BERT O
- O
fused O
models O
for O
each O
error O
type O
. O
The O
two O
models O
perform O
poorly O
on O
sentence O
- O
level O
errors O
( O
CJ O
) O
, O
which O
often O
involve O
sentence O
reconstructions O
, O
demonstrating O
that O
this O
is O
a O
difÔ¨Åcult O
task O
. O
For O
character O
- O
level O
errors O
( O
B O
) O
, O
the O
models O
achieve O
better O
performance O
than O
for O
other O
error O
types O
. O
Compared O
with O
the O
correction O
performance O
, O
the O
systems O
indicate O
moderate O
detection O
performance O
, O
demonstrating O
that O
the O
systems O
address O
error O
positions O
appropriately O
. O
With O
respect O
to O
the O
difference O
in O
performance O
of O
the O
two O
systems O
on O
each O
error O
type O
, O
we O
can O
conclude O
that O
BERTencoder O
performs O
better O
on O
character O
- O
level O
errors O
( O
B O
) O
, O
and O
BERT O
- O
fused O
performs O
better O
on O
other O
error O
types O
. O
6 O
Conclusion O
In O
this O
study O
, O
we O
incorporated O
a O
pre O
- O
trained O
model O
into O
an O
encoder O
- O
decoder O
model O
using O
two O
methods O
on O
Chinese O
GEC O
tasks O
. O
The O
experimental O
results O
demonstrate O
the O
usefulness O
of O
the O
BERT O
- O
based O
pretrained O
model O
in O
the O
Chinese O
GEC O
task O
. O
Additionally O
, O
our O
error O
type O
analysis O
showed O
that O
sentencelevel O
errors O
remain O
to O
be O
addressed O
. O
Acknowledgments O
This O
work O
has O
been O
partly O
supported O
by O
the O
programs O
of O
the O
Grant O
- O
in O
- O
Aid O
for O
ScientiÔ¨Åc O
Research O
from O
the O
Japan O
Society O
for O
the O
Promotion O
of O
Science O
( O
JSPS O
KAKENHI O
) O
Grant O
Numbers O
19K12099 O
and O
19KK0286 O
. O
A O
Appendices O
Table O
5 O
shows O
the O
training O
details O
for O
each O
model O
. O
Baseline O
Architecture O
Encoder O
( O
12 O
- O
layer O
) O
, O
Decoder O
( O
12 O
- O
layer O
) O
Learning O
rate O
1√ó10‚àí5 O
Batch O
size O
32 O
Optimizer O
Adam O
( O
Œ≤1= O
0.9,Œ≤2= O
0.999,/epsilon1= O
1√ó10‚àí8 O
) O
Max O
epochs O
20 O
Loss O
function O
cross O
- O
entropy O
Dropout O
0.1 O
BERT O
- O
encoder O
Architecture O
Encoder O
( O
12 O
- O
layer O
) O
, O
Decoder O
( O
12 O
- O
layer O
) O
Learning O
rate O
3√ó10‚àí5 O
Batch O
size O
32 O
Optimizer O
Adam O
( O
Œ≤1= O
0.9,Œ≤2= O
0.999,/epsilon1= O
1√ó10‚àí8 O
) O
Max O
epochs O
20 O
Loss O
function O
cross O
- O
entropy O
Dropout O
0.1 O
BERT O
- O
fused O
Architecture O
Transformer O
( O
big O
) O
Learning O
rate O
3√ó10‚àí5 O
Batch O
size O
32 O
Optimizer O
Adam O
( O
Œ≤1= O
0.9,Œ≤2= O
0.98,/epsilon1= O
1√ó10‚àí8 O
) O
Max O
epochs O
20 O
Loss O
function O
label O
smoothed O
cross O
- O
entropy O
( O
/epsilon1ls= O
0.1 O
) O
Dropout O
0.3 O
Table O
5 O
: O
Training O
details O
for O
each O
model.168Proceedings O
of O
the O
1st O
Conference O
of O
the O
Asia O
- O
PaciÔ¨Åc O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
10th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
169‚Äì180 O
December O
4 O
- O
7 O
, O
2020 O
. O
¬© O
2020 O
Association O
for O
Computational O
Linguistics O
Neural O
Gibbs O
Sampling O
for O
Joint O
Event O
Argument O
Extraction O
Xiaozhi O
Wang1‚àó O
, O
Shengyu O
Jia3‚àó O
, O
Xu O
Han1 O
, O
Zhiyuan O
Liu1,2‚Ä† O
, O
Juanzi O
Li1,2,Peng O
Li4,Jie O
Zhou4 O
1Department O
of O
Computer O
Science O
and O
Technology O
, O
BNRist O
; O
2KIRC O
, O
Institute O
for O
ArtiÔ¨Åcial O
Intelligence O
; O
3Department O
of O
Electrical O
Engineering O
, O
Tsinghua O
University O
, O
Beijing O
, O
100084 O
, O
China O
4Pattern O
Recognition O
Center O
, O
WeChat O
AI O
, O
Tencent O
Inc O
, O
China O
{ O
wangxz20 O
, O
jsy20 O
, O
hanxu17 O
} O
@mails.tsinghua.edu.cn O
Abstract O
Event O
Argument O
Extraction O
( O
EAE O
) O
aims O
at O
predicting O
event O
argument O
roles O
of O
entities O
in O
text O
, O
which O
is O
a O
crucial O
subtask O
and O
bottleneck O
of O
event O
extraction O
. O
Existing O
EAE O
methods O
either O
extract O
each O
event O
argument O
roles O
independently O
or O
sequentially O
, O
which O
can O
not O
adequately O
model O
the O
joint O
probability O
distribution O
among O
event O
arguments O
and O
their O
roles O
. O
In O
this O
paper O
, O
we O
propose O
a O
Bayesian O
model O
named O
Neural O
Gibbs O
Sampling O
( O
NGS O
) O
to O
jointly O
extract O
event O
arguments O
. O
SpeciÔ¨Åcally O
, O
we O
train O
two O
neural O
networks O
to O
model O
the O
prior O
distribution O
and O
conditional O
distribution O
over O
event O
arguments O
respectively O
and O
then O
use O
Gibbs O
sampling O
to O
approximate O
the O
joint O
distribution O
with O
the O
learned O
distributions O
. O
For O
overcoming O
the O
shortcoming O
of O
the O
high O
complexity O
of O
the O
original O
Gibbs O
sampling O
algorithm O
, O
we O
further O
apply O
simulated O
annealing O
to O
efÔ¨Åciently O
estimate O
the O
joint O
probability O
distribution O
over O
event O
arguments O
and O
make O
predictions O
. O
We O
conduct O
experiments O
on O
the O
two O
widely O
- O
used O
benchmark O
datasets O
ACE O
2005 O
and O
TAC O
KBP O
2016 O
. O
The O
Experimental O
results O
show O
that O
our O
NGS O
model O
can O
achieve O
comparable O
results O
to O
existing O
state O
- O
of O
- O
the O
- O
art O
EAE O
methods O
. O
The O
source O
code O
can O
be O
obtained O
from O
https:// O
github.com/THU-KEG/NGS O
. O
1 O
Introduction O
Event O
argument O
extraction O
( O
EAE O
) O
is O
a O
crucial O
subtask O
of O
Event O
Extraction O
, O
which O
aims O
at O
predicting O
entities O
and O
their O
event O
argument O
roles O
in O
event O
mentions O
. O
For O
instance O
, O
given O
the O
sentence O
‚Äú O
Fox O
‚Äôs O
stock O
price O
rises O
after O
the O
acquisition O
of O
its O
entertainment O
businesses O
by O
Disney O
‚Äù O
, O
the O
event O
detection O
( O
ED O
) O
model O
will O
Ô¨Årst O
identify O
the O
trigger O
word O
‚Äú O
acquisition O
‚Äù O
triggering O
a O
Transfer O
- O
Ownership O
event O
. O
Then O
, O
with O
the O
trigger O
word O
and O
event O
type O
, O
‚àóindicates O
equal O
contribution O
‚Ä†Corresponding O
author O
: O
Z.Liu O
( O
liuzy@tsinghua.edu.cn O
) O
        O
's O
stock O
price O
rises O
after O
the O
acquisition O
of O
its O
                                           O
by O
            O
.Transfer O
- O
OwnershipFoxentertainment O
businessesDisneyBuyerSellerArtifactTextEventArgument O
RoleEvent O
TypeacquisitionFoxTrigger O
WordArgumentFigure O
1 O
: O
An O
example O
of O
event O
extraction O
, O
including O
event O
detection O
and O
event O
argument O
extraction O
. O
the O
EAE O
model O
is O
required O
to O
identify O
that O
‚Äú O
Fox O
‚Äù O
and O
‚Äú O
Disney O
‚Äù O
are O
event O
arguments O
whose O
roles O
are O
‚Äú O
Seller O
‚Äù O
and O
‚Äú O
Buyer O
‚Äù O
respectively O
. O
As O
ED O
is O
well O
- O
studied O
in O
recent O
years O
( O
Liu O
et O
al O
. O
, O
2018a O
; O
Nguyen O
and O
Grishman O
, O
2018 O
; O
Zhao O
et O
al O
. O
, O
2018 O
; O
Wang O
et O
al O
. O
, O
2019a O
) O
, O
EAE O
becomes O
the O
bottleneck O
and O
has O
drawn O
growing O
attention O
. O
As O
EAE O
is O
the O
bottleneck O
of O
event O
extraction O
, O
especially O
is O
also O
important O
for O
various O
NLP O
applications O
( O
Yang O
et O
al O
. O
, O
2003 O
; O
Basile O
et O
al O
. O
, O
2014 O
; O
Cheng O
and O
Erk O
, O
2018 O
) O
, O
intensive O
efforts O
have O
already O
been O
devoted O
to O
designing O
effective O
EAE O
systems O
. O
The O
early O
feature O
- O
based O
methods O
( O
Patwardhan O
and O
Riloff O
, O
2009 O
; O
Gupta O
and O
Ji O
, O
2009 O
) O
manually O
design O
sophisticated O
features O
and O
heuristic O
rules O
to O
extract O
event O
arguments O
. O
As O
the O
development O
of O
neural O
networks O
, O
various O
neural O
methods O
adopt O
convolutional O
( O
Chen O
et O
al O
. O
, O
2015 O
) O
or O
recurrent O
( O
Nguyen O
et O
al O
. O
, O
2016 O
) O
neural O
networks O
to O
automatically O
represent O
sentence O
semantics O
with O
lowdimensional O
vectors O
, O
and O
independently O
determine O
argument O
roles O
with O
the O
vectors O
. O
Recently O
, O
some O
advanced O
techniques O
have O
also O
been O
adopted O
to O
further O
enhance O
the O
performance O
of O
EAE O
models O
, O
such O
as O
zero O
- O
shot O
learning O
( O
Huang O
et O
al O
. O
, O
2018 O
) O
, O
multimodal O
integration O
( O
Zhang O
et O
al O
. O
, O
2017 O
) O
and O
weak O
supervision O
( O
Chen O
et O
al O
. O
, O
2017 O
) O
. O
However O
, O
above O
- O
mentioned O
methods O
do O
not O
model O
the O
correlation O
among O
event O
arguments O
in169event O
mentions O
. O
As O
shown O
in O
Figure O
1 O
, O
all O
event O
arguments O
are O
correlated O
with O
each O
other O
. O
It O
is O
more O
likely O
to O
see O
a O
‚Äú O
Seller O
‚Äù O
when O
you O
have O
seen O
a O
‚Äú O
Buyer O
‚Äù O
and O
an O
‚Äú O
Artifact O
‚Äù O
in O
event O
mentions O
, O
and O
vice O
versa O
. O
Formally O
, O
with O
xidenoting O
the O
random O
variable O
of O
the O
i O
- O
th O
event O
argument O
candidate O
, O
the O
required O
probability O
distribution O
for O
EAE O
isP(x1,x2, O
... O
,x O
n|o O
) O
, O
whereois O
the O
observation O
from O
sentence O
semantics O
of O
event O
mentions O
. O
The O
existing O
methods O
which O
independently O
extract O
event O
arguments O
solely O
model O
P(xi|o O
) O
, O
totally O
ignoring O
the O
correlation O
among O
event O
arguments O
, O
which O
may O
lead O
models O
to O
trapping O
in O
a O
local O
optimum O
. O
Recently O
, O
some O
proactive O
works O
view O
EAE O
as O
a O
sequence O
labeling O
problem O
( O
Yang O
and O
Mitchell O
, O
2016 O
; O
Nguyen O
et O
al O
. O
, O
2016 O
; O
Zeng O
et O
al O
. O
, O
2018 O
) O
and O
adopt O
conditional O
random O
Ô¨Åeld O
( O
CRF O
) O
with O
the O
Viterbi O
algorithm O
( O
Rabiner O
, O
1989 O
) O
to O
solve O
the O
problem O
. O
These O
explorations O
consider O
the O
correlation O
of O
event O
arguments O
unintentionally O
. O
Yet O
limited O
by O
the O
Markov O
property O
, O
their O
linear O
- O
chain O
CRF O
only O
considers O
the O
correlation O
between O
two O
adjacent O
event O
arguments O
in O
the O
sequence O
and O
Ô¨Ånds O
a O
maximum O
likelihood O
path O
to O
model O
the O
joint O
distribution O
, O
i.e O
, O
these O
sequence O
models O
can O
not O
adequately O
handle O
the O
complex O
situation O
that O
each O
event O
argument O
is O
correlated O
with O
each O
other O
in O
event O
mentions O
, O
just O
like O
the O
example O
shown O
in O
Figure O
1 O
. O
To O
adequately O
model O
the O
genuine O
joint O
distributionP(x1,x2, O
... O
,x O
n|o)rather O
than O
/ O
producttextn O
iP(xi|o O
) O
for O
EAE O
, O
we O
propose O
a O
Bayesian O
method O
named O
Neural O
Gibbs O
Sampling O
( O
NGS O
) O
inspired O
by O
previous O
work O
( O
Finkel O
et O
al O
. O
, O
2005 O
; O
Sun O
et O
al O
. O
, O
2014 O
) O
. O
Gibbs O
sampling O
( O
Geman O
and O
Geman O
, O
1987 O
) O
is O
a O
Markov O
Chain O
Monte O
Carlo O
( O
MCMC O
) O
algorithm O
, O
which O
deÔ¨Ånes O
a O
Markov O
chain O
in O
the O
space O
of O
possible O
variable O
assignments O
whose O
stationary O
distribution O
is O
the O
desired O
joint O
distribution O
. O
Then O
, O
a O
Monte O
Carlo O
method O
is O
adopted O
to O
sample O
a O
sequence O
of O
observations O
, O
and O
the O
sampled O
sequence O
can O
be O
used O
to O
approximate O
the O
joint O
distribution O
. O
More O
speciÔ¨Åcally O
, O
for O
NGS O
, O
we O
Ô¨Årst O
adopt O
a O
neural O
network O
to O
model O
the O
prior O
distribution O
Pp(xi|o)and O
independently O
predict O
an O
argument O
role O
for O
each O
event O
argument O
candidate O
to O
get O
an O
initial O
state O
for O
the O
random O
variable O
sequence O
x1,x2, O
... O
,x O
n O
, O
which O
is O
similar O
to O
the O
previous O
methods O
. O
Then O
, O
we O
train O
a O
special O
neural O
network O
to O
model O
the O
conditional O
probability O
distribution O
Pc(xi|x1,x2, O
... O
,x O
i‚àí1,xi+1, O
... O
,x O
n O
, O
o)and O
iteratively O
change O
the O
sequence O
state O
by O
this O
conditionaldistribution O
. O
Intuitively O
, O
the O
network O
modeling O
the O
conditional O
probability O
distribution O
aims O
to O
predict O
unknown O
argument O
roles O
based O
on O
both O
sentence O
semantics O
and O
some O
known O
argument O
roles O
. O
After O
enough O
steps O
, O
the O
state O
of O
the O
sequence O
will O
accurately O
follow O
the O
posterior O
joint O
distribution O
P(x1,x2, O
... O
,x O
n|o O
) O
, O
and O
the O
most O
frequent O
state O
in O
history O
will O
be O
the O
best O
result O
of O
EAE O
. O
Considering O
that O
it O
will O
take O
many O
steps O
to O
accurately O
estimate O
the O
shape O
of O
the O
joint O
distribution O
and O
each O
step O
uses O
neural O
networks O
for O
inference O
, O
it O
is O
time O
- O
consuming O
and O
impractical O
. O
Due O
to O
what O
we O
want O
for O
EAE O
is O
the O
max O
- O
likelihood O
state O
of O
the O
argument O
roles O
, O
we O
follow O
Geman O
and O
Geman O
( O
1987 O
) O
and O
adopt O
simulated O
annealing O
( O
Kirkpatrick O
et O
al O
. O
, O
1983 O
) O
to O
efÔ¨Åciently O
Ô¨Ånd O
the O
max O
- O
likelihood O
state O
based O
on O
the O
Gibbs O
sampling O
. O
To O
conclude O
, O
our O
main O
contributions O
can O
be O
summarized O
as O
follows O
: O
( O
1 O
) O
Our O
NGS O
method O
combines O
both O
the O
advantages O
of O
neural O
networks O
and O
the O
Gibbs O
sampling O
method O
. O
The O
neural O
networks O
have O
shown O
their O
strong O
ability O
to O
Ô¨Åt O
a O
distribution O
from O
data O
. O
Gibbs O
sampling O
has O
remarkable O
advantages O
in O
performing O
Bayesian O
inference O
and O
modeling O
the O
complex O
correlation O
among O
event O
arguments O
. O
( O
2 O
) O
Considering O
the O
shortcoming O
of O
high O
complexity O
of O
the O
original O
Gibbs O
sampling O
algorithm O
, O
we O
further O
apply O
simulated O
annealing O
to O
efÔ¨Åciently O
estimate O
the O
joint O
probability O
distribution O
and O
Ô¨Ånd O
the O
max O
- O
likelihood O
state O
for O
NGS O
. O
( O
3 O
) O
Experimental O
results O
on O
the O
widely O
- O
used O
benchmark O
datasets O
ACE O
2005 O
and O
TAC O
KBP O
2016 O
show O
that O
our O
NGS O
works O
well O
to O
consider O
the O
correlation O
among O
event O
arguments O
and O
achieves O
the O
state O
- O
of O
- O
the O
- O
art O
results O
. O
The O
experiments O
also O
show O
that O
the O
simulated O
annealing O
method O
can O
signiÔ¨Åcantly O
improve O
the O
convergence O
speed O
and O
the O
stability O
of O
Gibbs O
sampling O
, O
which O
demonstrate O
that O
our O
NGS O
is O
both O
effective O
and O
efÔ¨Åcient O
. O
2 O
Related O
Work O
Event O
Extraction O
( O
EE O
) O
aims O
to O
extract O
structured O
information O
from O
plain O
text O
, O
which O
is O
a O
challenging O
task O
in O
the O
Ô¨Åeld O
of O
information O
extraction O
. O
EE O
consists O
of O
two O
subtasks O
, O
one O
is O
event O
detection O
( O
ED O
) O
to O
detect O
words O
triggering O
events O
and O
identify O
event O
types O
, O
the O
other O
is O
event O
argument O
extraction O
( O
EAE O
) O
to O
extract O
argument O
entities O
in O
event O
mentions O
and O
identify O
event O
argument O
roles O
. O
As O
EE O
is O
important O
and O
beneÔ¨Åcial O
for O
various O
downstream170Gibbs O
SamplingPrior O
Neural O
ModelPrior O
Distribution O
‚Ä¶ O
Pp(xi|o)Prior O
StateInitialization O
x2xn O
‚Ä¶ O
x1 O
ConditionalNeural O
ModelConditionalDistribution O
‚Ä¶ O
SamplingProcess O
.x(t)1x(t)2x(t)nx(t)i.t O
step O
statet+1 O
step O
statex(t+1)i.x(t)1x(t)2x(t)n O
. O
Pc(x(t+1)i|X(t) i O
, O
o)i O
‚á† O
max(Pc(x(t+1)i|X(t) i O
, O
o))1 O
/ O
cPnj=1max(Pc(x(t+1)j|X(t) j O
, O
o))1 O
/ O
cSimulated O
Annealingdecreasec O
       O
's O
stock O
price O
rises O
after O
the O
acquisition O
of O
its O
entertainment O
businesses O
by O
            O
.Transfer O
- O
OwnershipFoxDisneyBuyerSeller O
ArtifactFox O
's O
stock O
price O
rises O
after O
the O
acquisition O
of O
its O
entertainment O
businesses O
by O
Disney O
. O
TextResult O
ConvergeInitializationFigure O
2 O
: O
Overall O
framework O
of O
our O
Neural O
Gibbs O
Sampling O
model O
. O
NLP O
tasks O
, O
e.g. O
, O
question O
answering O
( O
Yang O
et O
al O
. O
, O
2003 O
) O
, O
information O
retrieval O
( O
Basile O
et O
al O
. O
, O
2014 O
) O
, O
and O
reading O
comprehension O
( O
Cheng O
and O
Erk O
, O
2018 O
) O
, O
it O
has O
attracted O
wide O
attentions O
recently O
. O
ED O
has O
been O
well O
- O
studied O
by O
the O
previous O
works O
due O
to O
its O
simple O
and O
clear O
deÔ¨Ånition O
, O
including O
feature O
- O
based O
and O
rule O
- O
based O
methods O
( O
Ahn O
, O
2006 O
; O
Ji O
and O
Grishman O
, O
2008 O
; O
Gupta O
and O
Ji O
, O
2009 O
; O
Riedel O
et O
al O
. O
, O
2010 O
; O
Hong O
et O
al O
. O
, O
2011 O
; O
McClosky O
et O
al O
. O
, O
2011 O
; O
Huang O
and O
Riloff O
, O
2012a O
, O
b O
; O
Araki O
and O
Mitamura O
, O
2015 O
; O
Li O
et O
al O
. O
, O
2013 O
; O
Yang O
and O
Mitchell O
, O
2016 O
; O
Liu O
et O
al O
. O
, O
2016b O
) O
, O
neural O
methods O
( O
Chen O
et O
al O
. O
, O
2015 O
; O
Nguyen O
and O
Grishman O
, O
2015 O
; O
Nguyen O
et O
al O
. O
, O
2016 O
; O
Duan O
et O
al O
. O
, O
2017 O
; O
Nguyen O
et O
al O
. O
, O
2016 O
; O
Ghaeini O
et O
al O
. O
, O
2016 O
; O
Lin O
et O
al O
. O
, O
2018 O
) O
, O
the O
methods O
with O
external O
heterogeneous O
knowledge O
( O
Liu O
et O
al O
. O
, O
2016a O
, O
2017 O
; O
Zhang O
et O
al O
. O
, O
2017 O
; O
Duan O
et O
al O
. O
, O
2017 O
; O
Zhao O
et O
al O
. O
, O
2018 O
; O
Liu O
et O
al O
. O
, O
2018b O
) O
. O
Some O
advanced O
architectures O
, O
such O
as O
graph O
convolutional O
networks O
( O
Nguyen O
and O
Grishman O
, O
2018 O
) O
and O
adversarial O
training O
( O
Hong O
et O
al O
. O
, O
2018 O
; O
Wang O
et O
al O
. O
, O
2019a O
) O
, O
have O
also O
been O
applied O
recently O
. O
As O
ED O
models O
has O
achieved O
relatively O
promising O
results O
, O
the O
more O
difÔ¨Åcult O
EAE O
becomes O
the O
bottleneck O
of O
EE O
, O
and O
have O
drawn O
growing O
research O
interests O
. O
The O
early O
works O
( O
Patwardhan O
and O
Riloff O
, O
2009 O
; O
Gupta O
and O
Ji O
, O
2009 O
; O
Liao O
and O
Grishman O
, O
2010b O
, O
a O
; O
Huang O
and O
Riloff O
, O
2012b O
; O
Li O
et O
al O
. O
, O
2013 O
) O
focus O
on O
designing O
hand O
- O
crafted O
features O
and O
heuristic O
rules O
to O
extract O
event O
arguments O
, O
which O
suffer O
from O
the O
problem O
of O
both O
implementation O
complexity O
and O
low O
recall O
. O
As O
the O
rapid O
develop O
- O
ment O
of O
neural O
networks O
, O
various O
neural O
methods O
have O
been O
proposed O
, O
such O
as O
utilizing O
convolutional O
models O
( O
Chen O
et O
al O
. O
, O
2015 O
) O
, O
utilizing O
recurrent O
models O
( O
Nguyen O
et O
al O
. O
, O
2016 O
; O
Sha O
et O
al O
. O
, O
2018 O
) O
, O
and O
Ô¨Ånetuning O
pre O
- O
trained O
language O
model O
BERT O
( O
Wang O
et O
al O
. O
, O
2019b O
) O
. O
As O
compared O
with O
the O
early O
featurebased O
and O
rule O
- O
based O
methods O
, O
neural O
methods O
automatically O
represent O
sentence O
semantics O
with O
lowdimensional O
vectors O
, O
and O
independently O
determine O
argument O
roles O
with O
the O
vectors O
, O
leading O
to O
getting O
rid O
of O
designing O
sophisticated O
features O
and O
rules O
. O
Recently O
, O
some O
works O
adopt O
some O
advanced O
techniques O
to O
further O
improve O
EAE O
models O
in O
different O
scenarios O
, O
including O
zero O
- O
shot O
learning O
( O
Huang O
et O
al O
. O
, O
2018 O
) O
, O
multi O
- O
modal O
integration O
( O
Zhang O
et O
al O
. O
, O
2017 O
) O
, O
cross O
- O
lingual O
( O
Subburathinam O
et O
al O
. O
, O
2019 O
) O
, O
end O
- O
to O
- O
end O
( O
Wadden O
et O
al O
. O
, O
2019 O
) O
, O
and O
weak O
supervision O
( O
Chen O
et O
al O
. O
, O
2017 O
; O
Zeng O
et O
al O
. O
, O
2018 O
) O
. O
The O
current O
methods O
for O
EAE O
have O
achieved O
some O
promising O
results O
. O
However O
, O
they O
focus O
on O
independently O
handling O
each O
argument O
entity O
to O
predict O
its O
role O
. O
Because O
of O
ignoring O
to O
capture O
rich O
correlated O
knowledge O
among O
event O
arguments O
, O
the O
above O
- O
mentioned O
methods O
are O
easy O
to O
trap O
in O
a O
local O
optimum O
and O
make O
some O
inexplicable O
mistakes O
. O
Inspired O
by O
some O
methods O
in O
named O
entity O
recognition O
( O
Huang O
et O
al O
. O
, O
2015 O
) O
and O
relation O
extraction O
( O
Miwa O
and O
Bansal O
, O
2016 O
) O
, O
some O
recent O
proactive O
works O
view O
EAE O
as O
a O
sequence O
labeling O
problem O
. O
Following O
the O
methods O
for O
sequence O
labeling O
problem O
( O
Ma O
and O
Hovy O
, O
2016 O
) O
, O
these O
sequential O
EAE O
models O
( O
Yang O
and O
Mitchell O
, O
2016 O
; O
Zeng O
et O
al O
. O
,1712018 O
) O
adopt O
conditional O
random O
Ô¨Åeld O
( O
CRF O
) O
with O
the O
Viterbi O
algorithm O
( O
Rabiner O
, O
1989 O
) O
, O
and O
unintentionally O
consider O
the O
correlation O
of O
event O
arguments O
. O
Limited O
by O
the O
Markov O
property O
, O
the O
linear O
- O
chain O
CRF O
sequentially O
considers O
the O
correlation O
between O
two O
adjacent O
event O
arguments O
, O
which O
can O
not O
adequately O
handle O
the O
complex O
situation O
in O
EAE O
that O
each O
argument O
and O
any O
other O
arguments O
may O
be O
correlated O
. O
To O
this O
end O
and O
inspired O
by O
some O
proactive O
works O
( O
Finkel O
et O
al O
. O
, O
2005 O
; O
Sun O
et O
al O
. O
, O
2014 O
) O
, O
we O
adapt O
Gibbs O
sampling O
( O
Geman O
and O
Geman O
, O
1987 O
) O
for O
EAE O
to O
perform O
approximate O
inference O
from O
the O
joint O
distribution O
. O
Moreover O
, O
we O
incorporate O
simulated O
annealing O
( O
Kirkpatrick O
et O
al O
. O
, O
1983 O
) O
to O
accelerate O
the O
sampling O
process O
, O
leading O
to O
an O
effective O
and O
efÔ¨Åcient O
method O
. O
3 O
Methodology O
3.1 O
Framework O
For O
convenience O
, O
we O
denote O
X={x1, O
... O
,x O
n O
} O
andX‚àíi={x1, O
... O
,x O
i‚àí1,xi+1, O
... O
,x O
n O
} O
. O
Figure O
2 O
shows O
the O
overall O
framework O
of O
our O
Neural O
Gibbs O
Sampling O
( O
NGS O
) O
method O
, O
consisting O
of O
the O
following O
modules O
: O
The O
neural O
models O
, O
including O
a O
prior O
neural O
model O
to O
model O
the O
prior O
distribution O
Pp(xi|o O
) O
, O
and O
a O
conditional O
neural O
model O
to O
model O
the O
conditional O
distributionPc(xi|X‚àíi O
, O
o O
) O
. O
The O
prior O
neural O
model O
is O
similar O
with O
existing O
EAE O
methods O
, O
which O
takes O
the O
event O
mention O
text O
as O
input O
and O
outputs O
the O
labels O
of O
event O
argument O
candidates O
. O
The O
labels O
will O
serve O
as O
the O
prior O
state O
for O
the O
Gibbs O
sampling O
module O
. O
The O
conditional O
neural O
model O
takes O
the O
text O
and O
the O
results O
of O
the O
last O
step O
as O
input O
and O
outputs O
the O
probability O
distribution O
over O
labels O
for O
each O
event O
argument O
candidate O
. O
The O
Gibbs O
sampling O
module O
to O
sample O
variable O
assignments O
XwithPp(xi|o)and O
Pc(xi|X‚àíi O
, O
o O
) O
, O
which O
gradually O
match O
the O
implicit O
posterior O
joint O
distribution O
. O
The O
simulated O
annealing O
method O
to O
efÔ¨Åciently O
Ô¨Ånd O
the O
optimal O
state O
in O
the O
Markov O
chain O
of O
Gibbs O
sampling O
. O
It O
uses O
a O
‚Äú O
temperature O
‚Äù O
parameter O
to O
control O
the O
sharpness O
of O
the O
transition O
distribution O
. O
With O
the O
‚Äú O
temperature O
‚Äù O
decreasing O
, O
the O
algorithm O
will O
more O
and O
more O
tend O
to O
choose O
the O
max O
- O
likelihood O
state O
as O
the O
next O
state O
. O
3.2 O
Neural O
Models O
The O
Prior O
Neural O
Model O
is O
to O
model O
the O
prior O
distribution O
Pp(xi|o O
) O
. O
In O
this O
paper O
, O
we O
use O
DM O
- O
CNN O
( O
Chen O
et O
al O
. O
, O
2015 O
) O
and O
DMBERT O
as O
the O
prior O
neural O
models O
. O
Given O
a O
sentence O
consisting O
of O
several O
words{w1, O
... O
,t, O
... O
,w O
i, O
... O
,w O
n O
} O
, O
wheret O
andwidenote O
the O
trigger O
word O
and O
the O
candidate O
argument O
entity O
respectively O
. O
DMCNN O
transfers O
each O
word O
in O
the O
word O
sequence O
into O
an O
input O
embedding O
ei O
, O
which O
consists O
of O
word O
embedding O
, O
event O
type O
embedding O
, O
and O
position O
embedding O
. O
Then O
, O
DMCNN O
feeds O
the O
input O
embeddings O
into O
a O
convolutional O
encoding O
layer O
to O
automatically O
learn O
the O
features O
and O
a O
dynamic O
multi O
- O
pooling O
layer O
to O
aggregate O
the O
features O
into O
a O
uniÔ¨Åed O
sentence O
observation O
embedding O
to O
predict O
an O
argument O
role O
xiforwi O
. O
DMBERT O
is O
a O
variation O
of O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
proposed O
by O
Wang O
et O
al O
. O
( O
2019b O
) O
. O
It O
adopts O
a O
pre O
- O
trained O
BERT O
to O
represent O
the O
word O
sequence O
as O
feature O
vectors O
and O
also O
uses O
a O
dynamic O
multipooling O
mechanism O
like O
DMCNN O
to O
aggregate O
the O
features O
into O
an O
instance O
embedding O
for O
prediction O
. O
It O
inserts O
special O
tokens O
around O
the O
event O
argument O
candidates O
to O
indicate O
their O
positions O
. O
We O
sample O
an O
argument O
role O
following O
Pp(xi|o O
) O
for O
each O
argument O
candidate O
and O
Ô¨Ånally O
predict O
an O
initial O
argument O
role O
state O
X(0)={x(0 O
) O
1, O
... O
,x(0 O
) O
n O
} O
as O
the O
start O
point O
of O
Gibbs O
sampling O
. O
Note O
that O
, O
our O
NGS O
method O
does O
not O
have O
any O
special O
requirements O
for O
the O
prior O
neural O
model O
, O
any O
other O
neural O
networks O
can O
also O
be O
used O
. O
Conditional O
Neural O
Model O
is O
to O
model O
the O
conditional O
distribution O
Pc(xi|X‚àíi O
, O
o)for O
the O
state O
transition O
in O
Gibbs O
sampling O
. O
Considering O
that O
it O
requires O
to O
integrate O
the O
argument O
role O
information O
of O
X‚àíito O
computePc(xi|X‚àíi O
, O
o O
) O
, O
we O
set O
an O
argument O
role O
embedding O
aifor O
each O
word O
wito O
represent O
whether O
it O
is O
an O
event O
argument O
and O
which O
role O
it O
is O
of O
. O
Then O
, O
we O
modify O
the O
input O
layer O
of O
DMCNN O
and O
DMBERT O
to O
feed O
the O
argument O
role O
embeddings O
in O
. O
More O
speciÔ¨Åcally O
, O
DMCNN O
concatenates O
the O
original O
input O
embedding O
eiwith O
the O
argument O
role O
embedding O
aias O
new O
inputs O
. O
DMBERT O
utilizes O
the O
pre O
- O
trained O
parameters O
and O
adds O
aiinto O
the O
input O
embedding O
. O
3.3 O
Gibbs O
Sampling O
Module O
The O
Gibbs O
sampling O
module O
aims O
at O
sampling O
from O
the O
implicit O
joint O
distribution O
P(X|o O
) O
. O
As O
Algorithm O
1 O
shows O
, O
we O
use O
the O
prior O
neural O
model O
to O
initialize O
an O
initial O
state O
X(0 O
) O
. O
In O
stept O
, O
for O
each O
random O
variable O
xi O
, O
we O
input O
the O
other O
random O
variables O
‚Äô O
states O
X(t‚àí1 O
) O
‚àíiinto O
the O
conditional O
neu-172Algorithm O
1 O
Neural O
Gibbs O
sampling O
Input O
: O
Initial O
state O
X(0)={x(0 O
) O
1, O
... O
,x(0 O
) O
n}predicted O
by O
the O
prior O
neural O
network O
Result O
: O
Nsamples O
matching O
the O
joint O
distribution O
P(X|o O
) O
Train O
the O
conditional O
neural O
model O
to O
Ô¨Åt O
Pc(xi|X‚àíi O
, O
o O
) O
fort‚Üê1toNdo O
// O
iteratively O
change O
the O
state O
fori‚Üê1tondo O
x(t O
) O
i‚Üêsample O
/ O
parenleftBig O
Pc O
/ O
parenleftbig O
x(t O
) O
i|X(t‚àí1 O
) O
‚àíi O
, O
o O
/ O
parenrightbig O
/ O
parenrightBig O
end O
X(t)‚Üê{x(t O
) O
1, O
... O
,x(t O
) O
n O
} O
end O
ReturnX(1), O
... O
,X(N O
) O
ral O
model O
to O
get O
the O
distribution O
Pc O
/ O
parenleftbig O
x(t O
) O
i|X(t‚àí1 O
) O
‚àíi O
, O
o O
/ O
parenrightbig O
. O
Then O
we O
sample O
x(t O
) O
ifrom O
the O
distribution O
, O
and O
Ô¨Ånally O
get O
the O
new O
state O
X(t O
) O
. O
We O
can O
approximately O
sampleNsamplesX(1), O
... O
,X(N)with O
the O
Gibbs O
sampling O
module O
. O
Our O
Appendix O
gives O
the O
proof O
that O
the O
samples O
will O
accurately O
follow O
the O
joint O
distribution O
after O
enough O
steps O
. O
Geman O
and O
Geman O
( O
1987 O
) O
have O
shown O
that O
the O
samples O
from O
the O
beginning O
of O
the O
Markov O
chain O
( O
the O
burn O
- O
in O
period O
) O
may O
not O
accurately O
follow O
the O
desired O
distribution O
, O
hence O
we O
choose O
the O
most O
frequent O
state O
from O
X(N O
2), O
... O
,X(N)as O
the O
result O
. O
3.4 O
Simulated O
Annealing O
Method O
The O
Gibbs O
sampling O
module O
is O
to O
accurately O
estimate O
the O
shape O
of O
P(X|o O
) O
, O
which O
will O
take O
many O
steps O
to O
reach O
the O
convergence O
. O
As O
what O
we O
want O
for O
EAE O
is O
only O
the O
max O
- O
likelihood O
state O
, O
we O
adopt O
a O
simulated O
annealing O
method O
to O
efÔ¨Åciently O
Ô¨Ånd O
the O
optimal O
state O
following O
Geman O
and O
Geman O
( O
1987 O
) O
. O
As O
shown O
in O
Algorithm O
2 O
, O
in O
step O
t O
, O
the O
simulated O
annealing O
method O
randomly O
sample O
an O
ifrom O
the O
distributionmax O
/ O
parenleftbig O
Pc O
/ O
parenleftbig O
x(t O
) O
i|X(t‚àí1 O
) O
‚àíi O
, O
o O
/ O
parenrightbig O
/ O
parenrightbig1 O
/ O
c O
/summationtextn O
j=1max O
/ O
parenleftbig O
Pc O
/ O
parenleftbig O
x(t O
) O
j|X(t‚àí1 O
) O
‚àíj O
, O
o O
/ O
parenrightbig O
/ O
parenrightbig1 O
/ O
c. O
The O
probability O
of O
ibeing O
chosen O
has O
positive O
correlation O
with O
the O
probability O
of O
the O
max O
- O
likelihood O
state O
in O
the O
conditional O
distribution O
of O
xi O
. O
Then O
we O
only O
need O
to O
update O
xiwith O
its O
max O
- O
likelihood O
state O
in O
conditional O
distribution O
Pc O
/ O
parenleftbig O
x(t O
) O
i|X(t‚àí1 O
) O
‚àíi O
/ O
parenrightbig O
modeled O
by O
the O
conditional O
neural O
model O
to O
get O
the O
next O
stateX(t O
) O
, O
which O
is O
more O
efÔ¨Åcient O
than O
the O
original O
Gibbs O
sampling O
method O
. O
The O
simulated O
annealing O
method O
adopts O
a O
time O
- O
varying O
parameter O
cAlgorithm O
2 O
NGS O
+ O
simulated O
annealing O
Input O
: O
Initial O
state O
X(0)={x(0 O
) O
1, O
... O
,x(0 O
) O
n}predicted O
by O
the O
prior O
neural O
network O
Result O
: O
The O
max O
- O
likelihood O
state O
X(N O
) O
Train O
the O
conditional O
neural O
model O
to O
Ô¨Åt O
Pc(xi|X‚àíi O
, O
o O
) O
c= O
1 O
fort‚Üê1toNdo O
// O
randomly O
choose O
ito O
transit O
i‚Üêsample O
/ O
parenleftBigg O
max O
/ O
parenleftBig O
Pc O
/ O
parenleftBig O
x(t O
) O
i|X(t‚àí1 O
) O
‚àíi O
, O
o O
/ O
parenrightBig O
/ O
parenrightBig1 O
/ O
c O
/summationtextn O
j=1max O
/ O
parenleftBig O
Pc O
/ O
parenleftBig O
x(t O
) O
j|X(t‚àí1 O
) O
‚àíj O
, O
o O
/ O
parenrightBig O
/ O
parenrightBig1 O
/ O
c O
/ O
parenrightBigg O
x(t O
) O
i‚Üêarg O
max O
/ O
parenleftBig O
Pc O
/ O
parenleftbig O
x(t O
) O
i|X(t‚àí1 O
) O
‚àíi O
, O
o O
/ O
parenrightbig O
/ O
parenrightBig O
X(t)‚ÜêX(t‚àí1 O
) O
‚àíi‚à™{x(t O
) O
i O
} O
decreasec O
end O
ReturnX(N O
) O
to O
control O
the O
sharpness O
of O
the O
distribution O
. O
With O
c O
gradually O
decreasing O
, O
the O
algorithm O
more O
and O
more O
tends O
to O
transit O
in O
the O
max O
- O
likelihood O
way O
and O
will O
quickly O
reach O
the O
max O
- O
likelihood O
state O
. O
When O
cis O
large O
, O
it O
performs O
like O
the O
original O
Gibbs O
sampling O
, O
so O
that O
can O
avoid O
falling O
into O
suboptimal O
results O
. O
4 O
Experiments O
4.1 O
Datasets O
and O
Evaluation O
Metrics O
We O
evaluate O
the O
proposed O
models O
on O
two O
real O
- O
world O
datasets O
: O
the O
most O
widely O
- O
used O
ACE O
2005 O
( O
Walker O
et O
al O
. O
, O
2006 O
) O
and O
the O
newly O
- O
developed O
TAC O
KBP O
2016 O
( O
Ellis O
et O
al O
. O
, O
2015 O
) O
. O
They O
are O
both O
often O
used O
as O
the O
benchmark O
in O
the O
previous O
works O
. O
ACE O
20051is O
the O
most O
widely O
- O
used O
dataset O
in O
EE O
, O
consisting O
of O
599 O
documents O
, O
8 O
event O
types O
, O
33 O
event O
subtypes O
, O
and O
35 O
argument O
roles O
. O
We O
evaluate O
our O
models O
by O
the O
performance O
of O
argument O
classiÔ¨Åcation O
. O
When O
testing O
models O
, O
an O
argument O
is O
correctly O
classiÔ¨Åed O
only O
if O
its O
event O
subtype O
, O
offsets O
and O
argument O
role O
match O
the O
annotation O
results O
. O
For O
fair O
comparison O
with O
the O
previous O
works O
( O
Liao O
and O
Grishman O
, O
2010b O
; O
Chen O
et O
al O
. O
, O
2015 O
) O
, O
we O
follow O
them O
to O
use O
the O
same O
test O
set O
containing O
40 O
newswire O
documents O
, O
the O
similar O
development O
set O
with O
30 O
randomly O
selected O
documents O
and O
training O
set O
with O
the O
remaining O
529 O
documents O
. O
TAC O
KBP O
20162indicates O
the O
data O
of O
the O
TAC O
KBP O
2016 O
Event O
Argument O
Extraction O
track O
, O
which O
is O
the O
latest O
benchmark O
dataset O
in O
EE O
. O
Different O
1https://catalog.ldc.upenn.edu/LDC2006T06 O
2https://tac.nist.gov//2016/KBP/173from O
ACE O
2005 O
, O
this O
competition O
only O
annotates O
difÔ¨Åcult O
test O
data O
but O
no O
training O
data O
. O
Accordingly O
, O
they O
encourage O
participants O
to O
construct O
training O
data O
from O
any O
other O
sources O
by O
themselves O
. O
Considering O
the O
argument O
roles O
of O
TAC O
KBP O
2016 O
are O
almost O
the O
same O
with O
ACE O
2005 O
expect O
TAC O
KBP O
2016 O
merges O
all O
the O
time O
- O
related O
roles O
in O
ACE O
2005 O
. O
We O
use O
the O
ACE O
2005 O
dataset O
as O
our O
training O
data O
, O
which O
is O
also O
provided O
to O
the O
participants O
of O
the O
competition O
. O
Hence O
we O
can O
have O
a O
fair O
comparison O
with O
the O
baselines O
. O
For O
fair O
comparison O
with O
the O
baselines O
, O
we O
use O
the O
same O
evaluation O
metrics O
with O
previous O
works O
: O
( O
1)Precision O
( O
P O
) O
, O
which O
is O
deÔ¨Åned O
as O
the O
number O
of O
correct O
argument O
predictions O
divided O
by O
the O
number O
of O
all O
argument O
predictions O
returned O
by O
the O
model O
. O
( O
2 O
) O
Recall O
( O
R O
) O
, O
which O
deÔ¨Åned O
as O
the O
number O
of O
correct O
argument O
predictions O
divided O
by O
the O
number O
of O
all O
correct O
golden O
results O
in O
the O
test O
set O
. O
( O
3 O
) O
F1 O
score O
( O
F1 O
) O
, O
which O
is O
deÔ¨Åned O
as O
the O
harmonic O
mean O
of O
the O
precision O
and O
recall O
. O
F1 O
score O
is O
the O
most O
important O
metric O
to O
evaluate O
EAE O
performance O
. O
4.2 O
Baselines O
To O
directly O
show O
the O
improvement O
of O
our O
method O
from O
the O
comparisons O
, O
we O
reproduce O
DMCNN O
and O
DMBERT O
as O
baselines O
on O
both O
of O
the O
two O
datasets O
. O
In O
addition O
, O
we O
also O
select O
some O
state O
- O
of O
- O
the O
- O
art O
baselines O
on O
the O
two O
datasets O
respectively O
. O
OnACE O
2005 O
, O
we O
compare O
our O
models O
with O
various O
state O
- O
of O
- O
the O
- O
art O
baselines O
, O
including O
: O
( O
1 O
) O
Feature O
- O
based O
methods O
. O
Li O
‚Äôs O
joint O
( O
Li O
et O
al O
. O
, O
2013 O
) O
adopts O
structure O
prediction O
to O
extract O
events O
, O
which O
is O
the O
best O
traditional O
feature O
- O
based O
method O
. O
RBPB O
( O
Sha O
et O
al O
. O
, O
2016 O
) O
adopts O
a O
regularization O
- O
based O
method O
to O
balance O
the O
effect O
of O
features O
and O
patterns O
, O
and O
also O
consider O
the O
relationship O
between O
argument O
candidates O
. O
( O
2 O
) O
Vanilla O
neural O
network O
methods O
. O
JRNN O
( O
Nguyen O
et O
al O
. O
, O
2016 O
) O
jointly O
conducts O
event O
detection O
and O
event O
argument O
extraction O
with O
bidirectional O
recurrent O
neural O
networks O
. O
( O
3 O
) O
Advanced O
neural O
network O
method O
with O
external O
information O
. O
The O
dbRNN O
( O
Sha O
et O
al O
. O
, O
2018 O
) O
utilizes O
a O
recurrent O
neural O
network O
with O
dependency O
bridges O
to O
carry O
syntactically O
related O
information O
between O
words O
, O
which O
considers O
not O
only O
sequence O
structures O
but O
also O
tree O
structures O
of O
the O
sentences O
. O
TheHMEAE O
( O
Wang O
et O
al O
. O
, O
2019b O
) O
leverages O
the O
latent O
concept O
hierarchy O
among O
argument O
roles O
with O
neural O
module O
networks O
, O
which O
considers O
the O
labelLearning O
Rate O
10‚àí3 O
Batch O
Size O
60 O
Dropout O
Probability O
0.5 O
Hidden O
Layer O
Dimension O
300 O
Kernel O
Size O
3 O
Word O
Embedding O
Dimension O
100 O
Position O
Embedding O
Dimension O
5 O
Event O
Type O
Embedding O
Dimension O
5 O
Argument O
Role O
Embedding O
Dimension O
5 O
Table O
1 O
: O
Hyperparameter O
settings O
for O
CNN O
models O
. O
Learning O
Rate O
6√ó10‚àí5 O
Batch O
Size O
50 O
Warmup O
Rate O
for O
the O
Prior O
Neural O
Model O
0.1 O
Warmup O
Rate O
for O
the O
Conditional O
Nueral O
Model O
0.05 O
Argument O
Role O
Embedding O
Dimension O
768 O
Table O
2 O
: O
Hyperparameter O
settings O
for O
BERT O
models O
. O
dependency O
but O
still O
classify O
each O
event O
argument O
independently O
. O
OnTAC O
KBP O
2016 O
, O
we O
compare O
our O
models O
with O
the O
top O
systems O
of O
the O
competition O
, O
including O
: O
DISCERN O
- O
R O
( O
Dubbin O
et O
al O
. O
, O
2016 O
) O
, O
CMU O
CS O
Event1 O
( O
Hsi O
et O
al O
. O
, O
2016 O
) O
, O
Washington1 O
and O
Washington4 O
( O
Ferguson O
et O
al O
. O
, O
2016 O
) O
. O
4.3 O
Hyperparameter O
Settings O
Our O
methods O
with O
DMCNN O
and O
DMBERT O
as O
the O
prior O
and O
conditional O
neural O
networks O
are O
named O
as O
NGS O
( O
CNN O
) O
andNGS O
( O
BERT O
) O
respectively O
. O
They O
both O
transit O
for O
200 O
steps O
and O
the O
clinearly O
decrease O
from O
1to0 O
. O
As O
our O
work O
focuses O
on O
extracting O
event O
arguments O
and O
their O
roles O
and O
our O
methods O
do O
not O
involve O
the O
event O
detection O
stage O
( O
to O
identify O
the O
trigger O
and O
determine O
the O
event O
type O
) O
, O
we O
conduct O
EAE O
based O
on O
the O
event O
detection O
models O
in O
( O
Chen O
et O
al O
. O
, O
2015 O
) O
and O
( O
Wang O
et O
al O
. O
, O
2019a O
) O
for O
the O
CNN O
and O
BERT O
models O
respectively O
. O
ForNGS O
( O
CNN O
) O
, O
the O
hyperparameters O
of O
the O
prior O
and O
conditional O
neural O
networks O
are O
set O
as O
the O
same O
as O
in O
the O
original O
DMCNN O
( O
Chen O
et O
al O
. O
, O
2015 O
) O
. O
We O
also O
use O
the O
pre O
- O
trained O
word O
embeddings O
learned O
by O
Skip O
- O
Gram O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
as O
the O
initial O
word O
embeddings O
. O
The O
detailed O
hyperparameters O
are O
shown O
in O
Table O
1 O
. O
ForNGS O
( O
BERT O
) O
, O
the O
two O
BERT O
models O
for O
the O
prior O
and O
conditional O
probability O
distributions O
are O
both O
based O
on O
the O
BERT O
BASE O
model O
in O
Devlin O
et O
al O
. O
( O
2019 O
) O
. O
We O
apply O
the O
pre O
- O
trained O
model3to O
initialize O
the O
parameters O
. O
To O
utilize O
the O
event O
type O
information O
in O
our O
model O
, O
we O
append O
a O
special O
token O
into O
each O
input O
sequence O
for O
BERT O
to O
indicate O
3github.com/google-research/bert174MethodTrigger O
ClassiÔ¨ÅcationArgument O
Role O
ClassiÔ¨Åcation O
P O
R O
F1 O
P O
R O
F1 O
Li O
‚Äôs O
Joint O
73.7 O
62.3 O
67.5 O
64.7 O
44.4 O
52.7 O
DMCNN O
75.6 O
63.6 O
69.1 O
62.2 O
46.9 O
53.5 O
RBPB O
70.3 O
67.5 O
68.9 O
54.1 O
53.5 O
53.8 O
JRNN O
66.0 O
73.0 O
69.3 O
54.2 O
56.7 O
55.4 O
HMEAE O
( O
CNN O
) O
75.6 O
63.6 O
69.1 O
57.3 O
54.2 O
55.7 O
DMBERT O
77.6 O
71.8 O
74.6 O
58.8 O
55.8 O
57.2 O
dbRNN O
74.1 O
69.8 O
71.9 O
66.2 O
52.8 O
58.7 O
HMEAE O
( O
BERT O
) O
77.6 O
71.8 O
74.6 O
62.2 O
56.6 O
59.3 O
NGS O
( O
CNN O
) O
75.6 O
63.6 O
69.1 O
61.3 O
51.3 O
55.9 O
NGS O
( O
BERT O
) O
77.6 O
71.8 O
74.6 O
59.9 O
59.1 O
59.5 O
Table O
3 O
: O
The O
overall O
EAE O
results O
( O
% O
) O
of O
various O
baselines O
and O
NGS O
on O
ACE O
2005 O
. O
EAE O
performances O
are O
inÔ¨Çuenced O
by O
the O
trigger O
quality O
, O
hence O
we O
also O
provide O
the O
trigger O
classiÔ¨Åcation O
( O
event O
detection O
) O
results O
. O
Note O
that O
as O
our O
work O
does O
not O
involve O
the O
event O
detection O
stage O
, O
the O
NGS O
( O
CNN O
) O
and O
NGS O
( O
BERT O
) O
use O
the O
triggers O
predicted O
by O
DMCNN O
and O
DMBERT O
respectively O
. O
MethodArgument O
Role O
ClassiÔ¨Åcation O
P O
R O
F1 O
DISCERN O
- O
R O
( O
Dubbin O
et O
al O
. O
, O
2016 O
) O
7.9 O
7.4 O
7.7 O
Washington4 O
( O
Ferguson O
et O
al O
. O
, O
2016 O
) O
32.1 O
5.0 O
8.7 O
CMU O
CS O
Event1 O
( O
Hsi O
et O
al O
. O
, O
2016 O
) O
31.2 O
4.9 O
8.4 O
Washington1 O
( O
Ferguson O
et O
al O
. O
, O
2016 O
) O
26.5 O
6.8 O
10.8 O
DMCNN O
( O
Chen O
et O
al O
. O
, O
2015 O
) O
17.9 O
16.0 O
16.9 O
HMEAE O
( O
CNN O
) O
( O
Wang O
et O
al O
. O
, O
2019b O
) O
15.3 O
22.5 O
18.2 O
DMBERT O
( O
Wang O
et O
al O
. O
, O
2019b O
) O
22.6 O
24.7 O
23.6 O
HMEAE O
( O
BERT O
) O
( O
Wang O
et O
al O
. O
, O
2019b O
) O
24.8 O
25.4 O
25.1 O
NGS O
( O
CNN O
) O
21.5 O
16.2 O
18.5 O
NGS O
( O
BERT O
) O
25.5 O
25.1 O
25.3 O
Table O
4 O
: O
The O
overall O
EAE O
results O
( O
% O
) O
of O
various O
baseline O
methods O
and O
our O
NGS O
on O
TAC O
KBP O
2016 O
Event O
Argument O
Task O
. O
All O
the O
models O
use O
golden O
triggers O
. O
the O
event O
type O
. O
Additional O
hyperparameters O
used O
in O
our O
experiments O
are O
shown O
in O
Table O
2 O
. O
4.4 O
Overall O
Evaluation O
Results O
The O
overall O
results O
of O
various O
baseline O
methods O
and O
NGS O
on O
ACE O
2005 O
are O
shown O
in O
Table O
3 O
. O
And O
the O
results O
on O
TAC O
KBP O
2016 O
are O
shown O
in O
Table O
4 O
. O
From O
the O
results O
, O
we O
observe O
that O
: O
( O
1 O
) O
NGS O
( O
CNN O
) O
and O
NGS O
( O
BERT O
) O
achieve O
signiÔ¨Åcant O
improvements O
as O
compared O
with O
DMCNN O
and O
DMBERT O
respectively O
. O
Meanwhile O
, O
our O
models O
still O
outperform O
other O
baseline O
methods O
, O
which O
are O
either O
the O
typical O
EAE O
models O
or O
the O
recent O
state O
- O
of O
- O
the O
- O
art O
models O
. O
It O
indicates O
that O
our O
Gibbs O
sampling O
with O
simulated O
annealing O
works O
well O
to O
improve O
EAE O
with O
the O
help O
of O
adequately O
model O
/ O
uni00000013 O
/uni00000015 O
/ O
uni00000013 O
/ O
uni00000013 O
/uni00000017 O
/ O
uni00000013 O
/ O
uni00000013 O
/uni00000019 O
/ O
uni00000013 O
/ O
uni00000013 O
/uni0000001b O
/ O
uni00000013 O
/ O
uni00000013 O
/uni00000014 O
/ O
uni00000013 O
/ O
uni00000013 O
/ O
uni00000013 O
/uni00000014 O
/ O
uni00000015 O
/ O
uni00000013 O
/ O
uni00000013 O
/uni00000036 O
/ O
uni00000057 O
/ O
uni00000048 O
/ O
uni00000053 O
/ O
uni00000013 O
/ O
uni00000011 O
/ O
uni00000018 O
/ O
uni00000014 O
/ O
uni00000013 O
/ O
uni00000011 O
/ O
uni00000018 O
/ O
uni00000015 O
/ O
uni00000013 O
/ O
uni00000011 O
/ O
uni00000018 O
/ O
uni00000016 O
/ O
uni00000013 O
/ O
uni00000011 O
/ O
uni00000018 O
/ O
uni00000017 O
/ O
uni00000013 O
/ O
uni00000011 O
/ O
uni00000018 O
/ O
uni00000018 O
/ O
uni00000013 O
/ O
uni00000011 O
/ O
uni00000018 O
/ O
uni00000019 O
/ O
uni00000029 O
/ O
uni00000014 O
/uni0000000e O
/ O
uni00000036 O
/ O
uni0000004c O
/ O
uni00000050 O
/ O
uni00000058 O
/ O
uni0000004f O
/ O
uni00000044 O
/ O
uni00000057 O
/ O
uni00000048 O
/ O
uni00000047 O
/ O
uni00000003 O
/ O
uni00000024 O
/ O
uni00000051 O
/ O
uni00000051 O
/ O
uni00000048 O
/ O
uni00000044 O
/ O
uni0000004f O
/ O
uni0000004c O
/ O
uni00000051 O
/ O
uni0000004a O
/uni0000002a O
/ O
uni0000004c O
/ O
uni00000045 O
/ O
uni00000045 O
/ O
uni00000056 O
/ O
uni00000003 O
/ O
uni00000036 O
/ O
uni00000044 O
/ O
uni00000050 O
/ O
uni00000053 O
/ O
uni0000004f O
/ O
uni0000004c O
/ O
uni00000051 O
/ O
uni0000004a O
/uni00000013 O
/uni00000015 O
/ O
uni00000013 O
/ O
uni00000013 O
/uni00000017 O
/ O
uni00000013 O
/ O
uni00000013 O
/uni00000019 O
/ O
uni00000013 O
/ O
uni00000013 O
/uni0000001b O
/ O
uni00000013 O
/ O
uni00000013 O
/uni00000014 O
/ O
uni00000013 O
/ O
uni00000013 O
/ O
uni00000013 O
/uni00000014 O
/ O
uni00000015 O
/ O
uni00000013 O
/ O
uni00000013 O
/uni00000036 O
/ O
uni00000057 O
/ O
uni00000048 O
/ O
uni00000053 O
/ O
uni00000013 O
/ O
uni00000011 O
/ O
uni00000014 O
/ O
uni0000001a O
/ O
uni00000013 O
/ O
uni00000013 O
/ O
uni00000011 O
/ O
uni00000014 O
/ O
uni0000001a O
/ O
uni00000018 O
/ O
uni00000013 O
/ O
uni00000011 O
/ O
uni00000014 O
/ O
uni0000001b O
/ O
uni00000013 O
/ O
uni00000013 O
/ O
uni00000011 O
/ O
uni00000014 O
/ O
uni0000001b O
/ O
uni00000018 O
/ O
uni00000029 O
/ O
uni00000014 O
/uni0000000e O
/ O
uni00000036 O
/ O
uni0000004c O
/ O
uni00000050 O
/ O
uni00000058 O
/ O
uni0000004f O
/ O
uni00000044 O
/ O
uni00000057 O
/ O
uni00000048 O
/ O
uni00000047 O
/ O
uni00000003 O
/ O
uni00000024 O
/ O
uni00000051 O
/ O
uni00000051 O
/ O
uni00000048 O
/ O
uni00000044 O
/ O
uni0000004f O
/ O
uni0000004c O
/ O
uni00000051 O
/ O
uni0000004a O
/uni0000002a O
/ O
uni0000004c O
/ O
uni00000045 O
/ O
uni00000045 O
/ O
uni00000056 O
/ O
uni00000003 O
/ O
uni00000036 O
/ O
uni00000044 O
/ O
uni00000050 O
/ O
uni00000053 O
/ O
uni0000004f O
/ O
uni0000004c O
/ O
uni00000051 O
/ O
uni0000004aFigure O
3 O
: O
F1 O
- O
step O
curves O
of O
NGS O
( O
CNN O
) O
with O
the O
simulated O
annealing O
method O
and O
the O
original O
Gibbs O
sampling O
on O
ACE O
2005 O
( O
left O
) O
and O
TAC O
KBP O
2016 O
( O
right O
) O
. O
ing O
the O
correlation O
between O
event O
arguments O
. O
This O
demonstrates O
that O
our O
method O
is O
effective O
. O
( O
2 O
) O
As O
NGS O
enhances O
both O
CNN O
models O
and O
BERT O
models O
on O
different O
datasets O
, O
it O
shows O
that O
our O
Gibbs O
sampling O
with O
simulated O
annealing O
is O
independent O
of O
EAE O
models O
. O
In O
other O
words O
, O
our O
method O
can O
be O
easily O
adapted O
for O
other O
EAE O
models O
to O
enhance O
their O
extraction O
performances O
. O
( O
3 O
) O
From O
the O
experimental O
results O
on O
both O
ACE O
2005 O
and O
TAC O
KBP O
2016 O
, O
we O
can O
Ô¨Ånd O
that O
the O
recall O
scores O
and O
F1 O
scores O
of O
our O
models O
are O
much O
better O
than O
the O
baseline O
models O
. O
The O
precision O
scores O
of O
our O
models O
do O
not O
achieve O
such O
obvious O
improvements O
. O
This O
is O
consistent O
with O
what O
we O
mention O
in O
the O
previous O
sections O
. O
We O
argue O
that O
the O
baseline O
models O
focusing O
on O
independently O
handling O
each O
event O
argument O
candidates O
may O
sever O
the O
constraints O
among O
argument O
roles O
, O
and O
may O
trap O
in O
a O
local O
optimum O
or O
over-Ô¨Åt O
the O
training O
set O
. O
The O
models O
without O
considering O
argument O
correlations O
may O
predict O
various O
argument O
roles O
with O
high O
conÔ¨Ådence O
, O
even O
make O
some O
inexplicable O
mistakes O
. O
Hence O
the O
precision O
scores O
of O
these O
models O
may O
increase O
, O
but O
their O
recall O
scores O
and O
F1 O
scores O
may O
decrease O
. O
Our O
models O
adopt O
Gibbs O
sampling O
for O
EAE O
to O
perform O
approximate O
inference O
from O
the O
joint O
distribution O
, O
and O
make O
the O
most O
of O
the O
corrleation O
and O
constraints O
among O
argument O
roles O
. O
Accordingly O
, O
our O
models O
can O
avoid O
these O
issues O
and O
achieve O
the O
state O
- O
of O
- O
the O
- O
art O
results O
. O
4.5 O
Ablation O
Study O
In O
order O
to O
verify O
the O
effectiveness O
of O
our O
method O
, O
especially O
for O
the O
simulated O
annealing O
method O
and O
the O
prior O
neural O
network O
, O
we O
conduct O
ablation O
studies O
on O
ACE O
2005 O
and O
TAC O
KBP O
2016 O
. O
Effectiveness O
of O
the O
Simulated O
Annealing O
To O
demonstrate O
the O
effectiveness O
of O
the O
simulated O
annealing O
method O
, O
we O
show O
the O
F1 O
- O
step O
curves O
of175Type O
: O
Justice O
Subtype O
: O
Appeal O
Text O
: O
Malaysia O
‚Äôs O
second O
highest O
court O
onFriday O
rejected O
an O
appeal O
by O
... O
Anwar O
Ibrahim O
against O
his O
conviction O
and O
nine O
- O
year O
prison O
sentence O
for O
sodomy O
. O
Event O
Argument O
Candidate O
Malaysia O
court O
Friday O
Anwar O
Ibrahim O
sodomy O
DMCNN O
Place O
/ O
check O
Adjudicator O
/check O
Time O
- O
Within O
/check O
Plaintiff O
/check O
N O
/ O
A√ó O
NGS O
( O
CNN O
) O
Place O
/ O
check O
Adjudicator O
/check O
Time O
- O
Within O
/check O
Plaintiff O
/check O
Crime O
/ O
check O
Table O
5 O
: O
Top O
: O
An O
example O
sentence O
highlighting O
the O
event O
argument O
candidates O
, O
which O
is O
sampled O
from O
ACE O
2005 O
. O
Bottom O
: O
EAE O
results O
of O
DMCNN O
and O
NGS O
( O
CNN O
) O
. O
NGS O
( O
CNN O
) O
correctly O
classiÔ¨Åes O
‚Äú O
sodomy O
‚Äù O
into O
Crime O
with O
the O
help O
of O
correlations O
among O
event O
arguments O
. O
/uni00000013 O
/uni00000015 O
/ O
uni00000018 O
/uni00000018 O
/ O
uni00000013 O
/uni0000001a O
/ O
uni00000018 O
/uni00000014 O
/ O
uni00000013 O
/ O
uni00000013 O
/uni00000014 O
/ O
uni00000015 O
/ O
uni00000018 O
/uni00000014 O
/ O
uni00000018 O
/ O
uni00000013 O
/uni00000036 O
/ O
uni00000057 O
/ O
uni00000048 O
/ O
uni00000053 O
/ O
uni00000013 O
/ O
uni00000011 O
/ O
uni00000015 O
/ O
uni00000013 O
/ O
uni00000013 O
/ O
uni00000011 O
/ O
uni00000015 O
/ O
uni00000018 O
/ O
uni00000013 O
/ O
uni00000011 O
/ O
uni00000016 O
/ O
uni00000013 O
/ O
uni00000013 O
/ O
uni00000011 O
/ O
uni00000016 O
/ O
uni00000018 O
/ O
uni00000013 O
/ O
uni00000011 O
/ O
uni00000017 O
/ O
uni00000013 O
/ O
uni00000013 O
/ O
uni00000011 O
/ O
uni00000017 O
/ O
uni00000018 O
/ O
uni00000013 O
/ O
uni00000011 O
/ O
uni00000018 O
/ O
uni00000013 O
/ O
uni00000013 O
/ O
uni00000011 O
/ O
uni00000018 O
/ O
uni00000018 O
/ O
uni00000013 O
/ O
uni00000011 O
/ O
uni00000019 O
/ O
uni00000013 O
/ O
uni00000029 O
/ O
uni00000014 O
/uni00000033 O
/ O
uni00000055 O
/ O
uni0000004c O
/ O
uni00000052 O
/ O
uni00000055 O
/ O
uni00000003 O
/ O
uni00000031 O
/ O
uni00000048 O
/ O
uni00000058 O
/ O
uni00000055 O
/ O
uni00000044 O
/ O
uni0000004f O
/ O
uni00000003 O
/ O
uni00000030 O
/ O
uni00000052 O
/ O
uni00000047 O
/ O
uni00000048 O
/ O
uni0000004f O
/ O
uni00000003 O
/ O
uni0000002c O
/ O
uni00000051 O
/ O
uni0000004c O
/ O
uni00000057 O
/ O
uni0000004c O
/ O
uni00000044 O
/ O
uni0000004f O
/ O
uni0000004c O
/ O
uni0000005d O
/ O
uni00000044 O
/ O
uni00000057 O
/ O
uni0000004c O
/ O
uni00000052 O
/ O
uni00000051 O
/uni00000035 O
/ O
uni00000044 O
/ O
uni00000051 O
/ O
uni00000047 O
/ O
uni00000052 O
/ O
uni00000050 O
/ O
uni00000003 O
/ O
uni0000002c O
/ O
uni00000051 O
/ O
uni0000004c O
/ O
uni00000057 O
/ O
uni0000004c O
/ O
uni00000044 O
/ O
uni0000004f O
/ O
uni0000004c O
/ O
uni0000005d O
/ O
uni00000044 O
/ O
uni00000057 O
/ O
uni0000004c O
/ O
uni00000052 O
/ O
uni00000051 O
/uni00000013 O
/uni00000015 O
/ O
uni00000013 O
/uni00000017 O
/ O
uni00000013 O
/uni00000019 O
/ O
uni00000013 O
/uni0000001b O
/ O
uni00000013 O
/uni00000014 O
/ O
uni00000013 O
/ O
uni00000013 O
/uni00000014 O
/ O
uni00000015 O
/ O
uni00000013 O
/uni00000036 O
/ O
uni00000057 O
/ O
uni00000048 O
/ O
uni00000053 O
/ O
uni00000013 O
/ O
uni00000011 O
/ O
uni00000014 O
/ O
uni00000018 O
/ O
uni00000013 O
/ O
uni00000011 O
/ O
uni00000014 O
/ O
uni00000019 O
/ O
uni00000013 O
/ O
uni00000011 O
/ O
uni00000014 O
/ O
uni0000001a O
/ O
uni00000013 O
/ O
uni00000011 O
/ O
uni00000014 O
/ O
uni0000001b O
/ O
uni00000029 O
/ O
uni00000014 O
/uni00000033 O
/ O
uni00000055 O
/ O
uni0000004c O
/ O
uni00000052 O
/ O
uni00000055 O
/ O
uni00000003 O
/ O
uni00000031 O
/ O
uni00000048 O
/ O
uni00000058 O
/ O
uni00000055 O
/ O
uni00000044 O
/ O
uni0000004f O
/ O
uni00000003 O
/ O
uni00000030 O
/ O
uni00000052 O
/ O
uni00000047 O
/ O
uni00000048 O
/ O
uni0000004f O
/ O
uni00000003 O
/ O
uni0000002c O
/ O
uni00000051 O
/ O
uni0000004c O
/ O
uni00000057 O
/ O
uni0000004c O
/ O
uni00000044 O
/ O
uni0000004f O
/ O
uni0000004c O
/ O
uni0000005d O
/ O
uni00000044 O
/ O
uni00000057 O
/ O
uni0000004c O
/ O
uni00000052 O
/ O
uni00000051 O
/uni00000035 O
/ O
uni00000044 O
/ O
uni00000051 O
/ O
uni00000047 O
/ O
uni00000052 O
/ O
uni00000050 O
/ O
uni00000003 O
/ O
uni0000002c O
/ O
uni00000051 O
/ O
uni0000004c O
/ O
uni00000057 O
/ O
uni0000004c O
/ O
uni00000044 O
/ O
uni0000004f O
/ O
uni0000004c O
/ O
uni0000005d O
/ O
uni00000044 O
/ O
uni00000057 O
/ O
uni0000004c O
/ O
uni00000052 O
/ O
uni00000051 O
Figure O
4 O
: O
F1 O
- O
step O
curves O
of O
NGS O
( O
CNN O
) O
with O
prior O
neural O
network O
initialization O
and O
random O
initialization O
on O
ACE O
2005 O
( O
left O
) O
and O
TAC O
KBP O
2016 O
( O
right O
) O
. O
Gibbs O
sampling O
with O
and O
without O
the O
simulated O
annealing O
in O
Figure O
3 O
. O
We O
can O
observe O
that O
: O
( O
1 O
) O
The O
simulated O
annealing O
method O
can O
signiÔ¨Åcantly O
improve O
the O
convergence O
speed O
and O
the O
stability O
. O
Our O
methods O
just O
require O
quarter O
to O
half O
of O
the O
steps O
to O
reach O
the O
convergence O
. O
( O
2 O
) O
The O
simulated O
annealing O
method O
does O
not O
weaken O
the O
performance O
of O
our O
models O
. O
Although O
the O
methods O
with O
the O
simulated O
annealing O
are O
much O
more O
efÔ¨Åcient O
than O
those O
without O
the O
simulated O
annealing O
, O
their O
results O
are O
comparable O
. O
Effectiveness O
of O
the O
Prior O
Neural O
Network O
As O
the O
mathematical O
proof O
in O
the O
Appendix O
shows O
, O
a O
prior O
distribution O
is O
not O
necessary O
for O
Gibbs O
sampling O
. O
To O
demonstrate O
the O
effectiveness O
of O
the O
prior O
neural O
model O
, O
we O
show O
the O
F1 O
- O
step O
curves O
of O
the O
prior O
neural O
model O
initialization O
and O
a O
random O
initialization O
for O
our O
NGS O
method O
( O
with O
simulated O
annealing O
) O
in O
Figure O
4 O
. O
As O
it O
shows O
in O
Ô¨Ågures O
, O
our O
NGS O
models O
with O
the O
prior O
neural O
network O
initialization O
take O
much O
fewer O
steps O
to O
reach O
the O
convergence O
than O
those O
models O
with O
random O
initialization O
, O
which O
is O
important O
and O
meaningful O
for O
the O
application O
. O
Combining O
the O
prior O
neural O
network O
initialization O
and O
the O
simulated O
annealing O
for O
our O
NGS O
will O
lead O
to O
a O
more O
efÔ¨Åcient O
model.#arguments O
1 O
- O
2 O
3 O
- O
4 O
> O
5 O
DMCNN O
55.3 O
54.1 O
61.8 O
NGS O
( O
CNN O
) O
56.7 O
( O
+1.4 O
) O
57.9 O
( O
+3.8 O
) O
69.5 O
( O
+7.7 O
) O
Table O
6 O
: O
F1 O
scores O
( O
% O
) O
of O
DMCNN O
and O
NGS O
( O
CNN O
) O
on O
different O
parts O
of O
ACE O
2005 O
dev O
set O
with O
different O
event O
argument O
numbers O
per O
sentence O
. O
4.6 O
Analysis O
on O
Modeling O
Event O
Argument O
Correlations O
To O
analyze O
whether O
NGS O
can O
successfully O
capture O
the O
event O
argument O
correlations O
and O
further O
improve O
EAE O
performance O
, O
we O
conduct O
a O
case O
study O
in O
Table O
5 O
and O
a O
quantitative O
analysis O
in O
Table O
6 O
. O
The O
sentence O
in O
Table O
5 O
is O
a O
real O
sentence O
containing O
an O
Appeal O
event O
, O
which O
is O
sampled O
from O
the O
test O
set O
of O
ACE O
2005 O
. O
From O
the O
EAE O
results O
, O
we O
can O
see O
that O
the O
vanilla O
DMCNN O
correctly O
classiÔ¨Åes O
most O
of O
the O
event O
argument O
candidates O
. O
But O
because O
‚Äú O
sodomy O
‚Äù O
is O
a O
rare O
word O
, O
it O
misclassiÔ¨Åed O
‚Äú O
sodomy O
‚Äù O
into O
‚Äú O
N O
/ O
A O
‚Äù O
( O
not O
an O
event O
argument O
) O
. O
With O
the O
help O
of O
our O
NGS O
method O
‚Äôs O
ability O
to O
model O
the O
joint O
distribution O
among O
event O
arguments O
, O
NGS O
( O
CNN O
) O
can O
infer O
that O
‚Äú O
sodomy O
‚Äù O
is O
a O
crime O
from O
the O
event O
argument O
correlations O
as O
it O
has O
known O
there O
are O
some O
crime O
- O
related O
arguments O
( O
adjudicator O
and O
plaintiff O
) O
in O
the O
sentence O
. O
On O
the O
other O
side O
, O
we O
show O
the O
comparisons O
between O
the O
basic O
model O
DMCNN O
and O
NGS O
( O
CNN O
) O
on O
data O
with O
different O
numbers O
of O
event O
arguments O
in O
Table O
6 O
. O
With O
the O
increase O
of O
event O
argument O
number O
, O
our O
improvements O
signiÔ¨Åcantly O
rise O
, O
which O
demonstrates O
our O
improvements O
come O
from O
modeling O
the O
correlations O
among O
event O
arguments O
. O
Note O
that O
the O
F1 O
scores O
are O
higher O
than O
the O
overall O
F1 O
scores O
, O
which O
is O
due O
to O
we O
Ô¨Ålter O
out O
the O
negative O
instances O
without O
event O
arguments.1765 O
Conclusion O
and O
Future O
Work O
In O
this O
paper O
, O
we O
propose O
a O
novel O
Neural O
Gibbs O
Sampling O
( O
NGS O
) O
method O
to O
adequately O
model O
the O
correlation O
between O
event O
arguments O
and O
argument O
roles O
, O
which O
combines O
the O
advantages O
of O
the O
Gibbs O
sampling O
method O
to O
model O
the O
joint O
distribution O
among O
random O
variables O
and O
the O
neural O
network O
models O
to O
automatically O
learn O
the O
effective O
representations O
. O
Considering O
the O
shortcoming O
of O
high O
complexity O
of O
Gibbs O
sampling O
algorithm O
, O
we O
further O
apply O
simulated O
annealing O
to O
accelerate O
the O
whole O
estimation O
process O
, O
which O
lead O
our O
method O
to O
being O
both O
effective O
and O
efÔ¨Åcient O
. O
The O
experimental O
results O
on O
two O
widely O
- O
used O
real O
- O
world O
datasets O
show O
that O
NGS O
can O
achieve O
comparable O
results O
to O
existing O
state O
- O
of O
- O
the O
- O
art O
EAE O
methods O
. O
The O
empirical O
analyses O
and O
ablation O
studies O
further O
verify O
the O
effectiveness O
and O
efÔ¨Åciency O
of O
our O
method O
. O
In O
the O
future O
: O
( O
1 O
) O
We O
will O
try O
to O
extend O
NGS O
to O
other O
tasks O
and O
scenarios O
to O
evaluate O
its O
general O
effectiveness O
of O
modeling O
the O
latent O
correlations O
. O
( O
2 O
) O
We O
will O
also O
explore O
more O
effective O
and O
simple O
methods O
to O
consider O
the O
correlations O
. O
Acknowledgement O
We O
thank O
Hedong O
( O
Ben O
) O
Hou O
for O
his O
help O
in O
the O
mathematical O
proof O
. O
This O
work O
is O
supported O
by O
the O
Key O
- O
Area O
Research O
and O
Development O
Program O
of O
Guangdong O
Province O
( O
2019B010153002 O
) O
, O
NSFC O
Key O
Projects O
( O
U1736204 O
, O
61533018 O
) O
, O
a O
grant O
from O
Institute O
for O
Guo O
Qiang O
, O
Tsinghua O
University O
( O
2019GQB0003 O
) O
and O
THUNUS O
NExT O
CoLab O
. O
This O
work O
is O
also O
supported O
by O
the O
Pattern O
Recognition O
Center O
, O
WeChat O
AI O
, O
Tencent O
Inc. O
Xiaozhi O
Wang O
is O
supported O
by O
Tsinghua O
University O
Initiative O
ScientiÔ¨Åc O
Research O
Program O
. O
Abstract O
Despite O
the O
success O
of O
neural O
machine O
translation O
( O
NMT O
) O
, O
simultaneous O
neural O
machine O
translation O
( O
SNMT O
) O
, O
the O
task O
of O
translating O
in O
real O
time O
before O
a O
full O
sentence O
has O
been O
observed O
, O
remains O
challenging O
due O
to O
the O
syntactic O
structure O
difference O
and O
simultaneity O
requirements O
. O
In O
this O
paper O
, O
we O
propose O
a O
general O
framework O
for O
adapting O
neural O
machine O
translation O
to O
translate O
simultaneously O
. O
Our O
framework O
contains O
two O
parts O
: O
preÔ¨Åx O
translation O
that O
utilizes O
a O
consecutive O
NMT O
model O
to O
translate O
source O
preÔ¨Åxes O
and O
a O
stopping O
criterion O
that O
determines O
when O
to O
stop O
the O
preÔ¨Åx O
translation O
. O
Experiments O
on O
three O
translation O
corpora O
and O
two O
language O
pairs O
show O
the O
efÔ¨Åcacy O
of O
the O
proposed O
framework O
on O
balancing O
the O
quality O
and O
latency O
in O
adapting O
NMT O
to O
perform O
simultaneous O
translation O
. O
1 O
Introduction O
Simultaneous O
translation O
( O
F O
¬®ugen O
et O
al O
. O
, O
2007 O
; O
Oda O
et O
al O
. O
, O
2014 O
; O
Grissom O
et O
al O
. O
, O
2014 O
; O
Niehues O
et O
al O
. O
, O
2016 O
; O
Cho O
and O
Esipova O
, O
2016 O
; O
Gu O
et O
al O
. O
, O
2017 O
; O
Ma O
et O
al O
. O
, O
2018 O
) O
, O
the O
task O
of O
producing O
a O
partial O
translation O
of O
a O
sentence O
before O
the O
whole O
input O
sentence O
ends O
, O
is O
useful O
in O
many O
scenarios O
including O
outbound O
tourism O
, O
international O
summit O
and O
multilateral O
negotiations O
. O
Different O
from O
the O
consecutive O
translation O
in O
which O
translation O
quality O
alone O
matters O
, O
simultaneous O
translation O
trades O
off O
between O
translation O
quality O
and O
latency O
. O
The O
syntactic O
structure O
difference O
between O
the O
source O
and O
target O
language O
makes O
simultaneous O
translation O
more O
challenging O
. O
For O
example O
, O
when O
translating O
from O
a O
verb-Ô¨Ånal O
( O
SOV O
) O
language O
( O
e.g. O
, O
Japanese O
) O
to O
a O
verb O
- O
media O
( O
SVO O
) O
language O
( O
e.g. O
, O
English O
) O
, O
the O
verb O
appears O
much O
later O
in O
the O
source O
sequence O
‚àóPart O
of O
the O
work O
was O
done O
when O
Yun O
is O
working O
at O
Huawei O
Noah O
‚Äôs O
Ark O
Lab.than O
in O
the O
target O
language O
. O
Some O
premature O
translations O
can O
lead O
to O
signiÔ¨Åcant O
loss O
in O
quality O
( O
Ma O
et O
al O
. O
, O
2018 O
) O
. O
Recently O
, O
a O
number O
of O
researchers O
have O
endeavored O
to O
explore O
methods O
for O
simultaneous O
translation O
in O
the O
context O
of O
NMT O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
; O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O
Some O
of O
them O
propose O
sophisticated O
training O
frameworks O
explicitly O
designed O
for O
simultaneous O
translation O
( O
Ma O
et O
al O
. O
, O
2018 O
; O
Arivazhagan O
et O
al O
. O
, O
2019 O
) O
. O
These O
approaches O
are O
either O
memory O
inefÔ¨Åcient O
during O
training O
( O
Ma O
et O
al O
. O
, O
2018 O
) O
or O
with O
hyper O
- O
parameters O
hard O
to O
tune O
( O
Arivazhagan O
et O
al O
. O
, O
2019 O
) O
. O
Others O
utilize O
a O
full O
- O
sentence O
base O
model O
to O
perform O
simultaneous O
translation O
by O
modiÔ¨Åcations O
to O
the O
encoder O
and O
the O
decoding O
process O
. O
To O
match O
the O
incremental O
source O
context O
, O
they O
replace O
the O
bidirectional O
encoder O
with O
a O
left O
- O
to O
- O
right O
encoder O
( O
Cho O
and O
Esipova O
, O
2016 O
; O
Satija O
and O
Pineau O
, O
2016 O
; O
Gu O
et O
al O
. O
, O
2017 O
; O
Alinejad O
et O
al O
. O
, O
2018 O
) O
or O
recompute O
the O
encoder O
hidden O
states O
( O
Zheng O
et O
al O
. O
, O
2019 O
) O
. O
On O
top O
of O
that O
, O
heuristic O
algorithms O
( O
Cho O
and O
Esipova O
, O
2016 O
; O
Dalvi O
et O
al O
. O
, O
2018 O
) O
or O
a O
READ O
/ O
WRITE O
model O
trained O
with O
reinforcement O
learning O
( O
Satija O
and O
Pineau O
, O
2016 O
; O
Gu O
et O
al O
. O
, O
2017 O
; O
Alinejad O
et O
al O
. O
, O
2018 O
) O
or O
supervised O
learning O
( O
Zheng O
et O
al O
. O
, O
2019 O
) O
are O
used O
to O
decide O
, O
at O
every O
step O
, O
whether O
to O
wait O
for O
the O
next O
source O
token O
or O
output O
a O
target O
token O
. O
However O
, O
these O
models O
either O
can O
not O
directly O
use O
a O
pretrained O
consecutive O
neural O
machine O
translation O
( O
CNMT O
) O
model O
with O
bidirectional O
encoder O
as O
the O
base O
model O
or O
work O
in O
a O
sub O
- O
optimal O
way O
in O
the O
decoding O
stage O
. O
In O
this O
paper O
, O
we O
study O
the O
problem O
of O
adapting O
neural O
machine O
translation O
to O
translate O
simultaneously O
. O
We O
formulate O
simultaneous O
translation O
as O
two O
nested O
loops O
: O
an O
outer O
loop O
that O
updates O
input O
buffer O
with O
newly O
observed O
source O
tokens O
and O
an O
inner O
loop O
that O
translates O
source O
tokens O
in O
the O
buffer O
updated O
at O
each O
outer O
step O
. O
For O
the O
outer O
loop O
, O
the O
input O
buffer O
can O
be O
updated O
by O
an O
ASR O
system O
with191an O
arbitrary O
update O
schedule O
. O
For O
the O
inner O
loop O
, O
we O
translate O
using O
the O
pretrained O
CNMT O
model O
and O
stop O
translation O
with O
a O
stopping O
controller O
. O
Such O
formulation O
is O
different O
from O
previous O
work O
( O
Satija O
and O
Pineau O
, O
2016 O
; O
Gu O
et O
al O
. O
, O
2017 O
; O
Alinejad O
et O
al O
. O
, O
2018 O
; O
Zheng O
et O
al O
. O
, O
2019 O
) O
which O
deÔ¨Åne O
simultaneous O
translation O
as O
sequentially O
making O
interleaved O
READ O
or O
WRITE O
decisions O
. O
We O
argue O
that O
our O
formulation O
is O
better O
than O
the O
previous O
one O
in O
two O
aspects O
: O
( O
i O
) O
Our O
formulation O
can O
better O
utilize O
the O
available O
source O
tokens O
. O
Under O
previous O
formulation O
, O
the O
number O
of O
source O
tokens O
observed O
by O
the O
CNMT O
model O
is O
determined O
by O
the O
number O
of O
READ O
actions O
that O
has O
been O
produced O
by O
the O
policy O
network O
. O
It O
is O
likely O
that O
the O
CNMT O
model O
does O
not O
observe O
all O
the O
available O
source O
tokens O
produced O
by O
the O
ASR O
system O
. O
In O
contrast O
, O
the O
CNMT O
model O
observes O
all O
the O
available O
source O
tokens O
when O
performing O
inner O
loop O
translation O
in O
our O
framework O
. O
( O
ii O
) O
Previous O
formulation O
makes O
TŒ∑+TœÑREAD O
or O
WRITE O
decisions O
regardless O
of O
the O
ASR O
update O
schedule O
, O
where O
TŒ∑andTœÑare O
source O
sentence O
and O
translation O
length O
, O
respectively O
. O
For O
an O
ASR O
system O
that O
outputs O
multiple O
tokens O
at O
a O
time O
, O
this O
is O
computational O
costly O
. O
Consider O
an O
extreme O
case O
where O
the O
ASR O
system O
outputs O
a O
full O
source O
sentence O
at O
a O
time O
. O
Previous O
work O
translates O
with O
a O
sequence O
of O
TŒ∑+TœÑactions O
, O
while O
we O
translate O
with O
a O
sequence O
ofTœÑdecisions O
( O
TœÑ‚àí1CONTINUE O
and O
1 O
STOP O
) O
. O
Under O
our O
proposed O
framework O
, O
we O
present O
two O
schedules O
for O
simultaneous O
translation O
: O
one O
stops O
the O
inner O
loop O
translation O
with O
heuristic O
and O
one O
with O
a O
stopping O
controller O
learned O
in O
a O
reinforcement O
learning O
framework O
to O
balance O
translation O
quality O
and O
latency O
. O
We O
evaluate O
our O
method O
on O
IWSLT16 O
German O
- O
English O
( O
DE O
- O
EN O
) O
translation O
in O
both O
directions O
, O
WMT15 O
English O
- O
German O
( O
EN O
- O
DE O
) O
translation O
in O
both O
directions O
, O
and O
NIST O
Chineseto O
- O
English O
( O
ZH‚ÜíEN O
) O
translation O
. O
The O
results O
show O
our O
method O
with O
reinforced O
stopping O
controller O
consistently O
improves O
over O
the O
de O
- O
facto O
baselines O
, O
and O
achieves O
low O
latency O
and O
reasonable O
BLEU O
scores O
. O
2 O
Background O
Given O
a O
set O
of O
source O
‚Äì O
target O
sentence O
pairs O
/angbracketleftxm O
, O
y‚àó O
m O
/ O
angbracketrightM O
m=1 O
, O
a O
consecutive O
NMT O
model O
can O
be O
trained O
by O
maximizing O
the O
log O
- O
likelihood O
of O
the O
target O
sentence O
from O
its O
entire O
source O
side O
context O
: O
ÀÜœÜ= O
argmax O
œÜ O
/ O
braceleftbiggM O
/ O
summationdisplay O
m=1logp(y‚àó O
m|xm;œÜ)/bracerightbigg O
, O
( O
1)whereœÜis O
a O
set O
of O
model O
parameters O
. O
At O
inference O
time O
, O
the O
NMT O
model O
Ô¨Årst O
encodes O
a O
source O
language O
sentence O
x={x1, O
... O
,xTŒ∑}with O
its O
encoder O
and O
passes O
the O
encoded O
representations O
h={h1, O
... O
,hTŒ∑}to O
a O
greedy O
decoder O
. O
Then O
the O
greedy O
decoder O
generates O
a O
translated O
sentence O
in O
target O
language O
by O
sequentially O
choosing O
the O
most O
likely O
token O
at O
each O
step O
t O
: O
yt= O
argmaxyp(y|y O
< O
t O
, O
x O
) O
. O
( O
2 O
) O
The O
distribution O
of O
next O
target O
word O
is O
deÔ¨Åned O
as O
: O
p(y|y O
< O
t O
, O
x)‚àùexp O
[ O
œÜOUT(zt O
) O
] O
zt O
= O
œÜDEC(yt‚àí1,z O
< O
t O
, O
h O
) O
, O
( O
3 O
) O
whereztis O
the O
decoder O
hidden O
state O
at O
position O
t. O
In O
consecutive O
NMT O
, O
once O
obtained O
, O
the O
encoder O
hidden O
states O
hand O
the O
decoder O
hidden O
state O
ztare O
not O
updated O
anymore O
and O
will O
be O
reused O
during O
the O
entire O
decoding O
process O
. O
3 O
Simultaneous O
NMT O
In O
SNMT O
, O
we O
receive O
streaming O
input O
tokens O
, O
and O
learn O
to O
translate O
them O
in O
real O
- O
time O
. O
We O
formulate O
simultaneous O
translation O
as O
two O
nested O
loops O
: O
the O
outer O
loop O
that O
updates O
an O
input O
buffer O
with O
newly O
observed O
source O
tokens O
and O
the O
inner O
loop O
that O
translates O
source O
tokens O
in O
the O
buffer O
updated O
at O
each O
outer O
step O
. O
More O
precisely O
, O
suppose O
at O
the O
end O
of O
outer O
step O
s‚àí1 O
, O
the O
input O
buffer O
is O
xs‚àí1={x1, O
... O
,xŒ∑[s‚àí1 O
] O
} O
, O
and O
the O
output O
buffer O
is O
ys‚àí1={y1, O
... O
,yœÑ[s‚àí1 O
] O
} O
. O
Then O
at O
outer O
step O
s O
, O
the O
system O
translates O
with O
the O
following O
steps O
: O
1The O
system O
observes O
cs>0new O
source O
tokens O
and O
updates O
the O
input O
buffer O
to O
be O
xs= O
{ O
x1, O
... O
,xŒ∑[s]}whereŒ∑[s O
] O
= O
Œ∑[s‚àí1 O
] O
+ O
cs O
. O
2Then O
, O
the O
system O
starts O
inner O
loop O
translation O
and O
writes O
ws>= O
0 O
target O
tokens O
to O
the O
output O
buffer O
. O
The O
output O
buffer O
is O
updated O
to O
be O
ys={y1, O
... O
,yœÑ[s]}whereœÑ[s O
] O
= O
œÑ[s‚àí1 O
] O
+ O
ws O
. O
The O
simultaneous O
decoding O
process O
continues O
until O
no O
more O
source O
tokens O
are O
added O
in O
the O
outer O
loop O
. O
We O
deÔ¨Åne O
the O
last O
outer O
step O
as O
the O
terminal O
outer O
stepS O
, O
and O
other O
outer O
steps O
as O
non O
- O
terminal O
outer O
steps O
. O
For O
the O
outer O
loop O
, O
we O
make O
no O
assumption O
about O
the O
value O
of O
cs O
, O
while O
all O
previous O
work O
assumes192cs= O
1 O
. O
This O
setting O
is O
more O
realistic O
because O
( O
i O
) O
increasing O
cscan O
reduce O
the O
number O
of O
outer O
steps O
, O
thus O
reducing O
computation O
cost O
; O
( O
ii O
) O
in O
a O
real O
speech O
translation O
application O
, O
an O
ASR O
system O
may O
generate O
multiple O
tokens O
at O
a O
time O
. O
For O
the O
inner O
loop O
, O
we O
adapt O
a O
pretrained O
vanilla O
CNMT O
model O
to O
perform O
partial O
translation O
with O
two O
important O
concerns O
: O
1.PreÔ¨Åx O
translation O
: O
given O
a O
source O
preÔ¨Åx O
xs= O
{ O
x1, O
... O
,xŒ∑[s]}and O
a O
target O
preÔ¨Åx O
ys O
œÑ[s‚àí1]= O
{ O
y1, O
... O
,yœÑ[s‚àí1 O
] O
} O
, O
how O
to O
predict O
the O
remaining O
target O
tokens O
? O
2.Stopping O
criterion O
: O
since O
the O
NMT O
model O
is O
trained O
with O
full O
sentences O
, O
how O
to O
design O
the O
stopping O
criterion O
for O
it O
when O
translating O
partial O
source O
sentcnes O
? O
3.1 O
PreÔ¨Åx O
Translation O
At O
an O
outer O
step O
s O
, O
given O
encoder O
hidden O
states O
hs O
for O
source O
preÔ¨Åx O
xs={x1, O
... O
,xŒ∑[s]}and O
decoder O
hidden O
states O
zs O
œÑ[s‚àí1]for O
target O
preÔ¨Åx O
ys O
œÑ[s‚àí1]= O
{ O
y1, O
... O
,yœÑ[s‚àí1 O
] O
} O
, O
we O
perform O
preÔ¨Åx O
translation O
sequentially O
with O
a O
greedy O
decoder O
: O
zs O
t O
= O
œÜDEC(yt‚àí1,zs O
< O
t O
, O
hs O
) O
p(y|y O
< O
t O
, O
xs)‚àùexp O
[ O
œÜOUT(zs O
t O
) O
] O
yt= O
argmaxyp(y|y O
< O
t O
, O
xs O
) O
, O
( O
4 O
) O
wheretstarts O
fromt O
= O
œÑ[s‚àí1 O
] O
+ O
1 O
. O
The O
preÔ¨Åx O
translation O
terminates O
when O
a O
stopping O
criterion O
meets O
, O
yielding O
a O
translation O
ys={y1, O
... O
,yœÑ[s O
] O
} O
. O
However O
, O
a O
major O
problem O
comes O
from O
the O
above O
translation O
method O
: O
how O
can O
we O
obtain O
the O
encoder O
hidden O
states O
hsand O
decoder O
hidden O
states O
zs O
œÑ[s‚àí1 O
] O
at O
the O
beginning O
of O
preÔ¨Åx O
translation O
? O
We O
propose O
to O
rebuild O
all O
encoder O
and O
decoder O
hidden O
states O
with O
hs O
= O
œÜENC O
/ O
parenleftbig O
xs O
/ O
parenrightbig O
, O
( O
5 O
) O
zs O
œÑ[s‚àí1]=œÜDEC O
/ O
parenleftbig O
ys O
œÑ[s‚àí1],hs O
/ O
parenrightbig O
. O
( O
6 O
) O
During O
full O
sentence O
training O
, O
all O
the O
decoder O
hidden O
states O
are O
computed O
conditional O
on O
the O
same O
source O
tokens O
. O
By O
rebuilding O
encoder O
and O
decoder O
hidden O
states O
, O
we O
also O
ensure O
that O
the O
decoder O
hidden O
states O
are O
computed O
conditional O
on O
the O
same O
source O
. O
This O
strategy O
is O
different O
from O
previous O
work O
that O
reuse O
previous O
encoder O
( O
Cho O
and O
Esipova O
, O
2016 O
; O
Gu O
et O
al O
. O
, O
2017 O
; O
Dalvi O
et O
al O
. O
, O
2018 O
; O
Alinejad O
et O
al O
. O
, O
2018 O
) O
or O
decoder O
( O
Cho O
and O
Esipova O
, O
2016 O
; O
Gu O
et O
al O
. O
, O
2017 O
; O
Dalvi O
et O
al O
. O
, O
2018 O
; O
Ma O
et O
al O
. O
, O
2018 O
) O
1ÊôìËéπ‚Üíxiaoying2ÊôìËéπ‰Ω†‚Üí O
xiaoyingyou3ÊôìËéπ O
‰Ω†Â•Ω‚Üí O
xiaoying O
you O
are O
good4ÊôìËéπ O
‰Ω† O
Â•Ω„ÄÇ‚Üí O
xiaoying O
you O
are O
good.src O
transFigure O
1 O
: O
Failure O
case O
when O
using O
EOS O
alone O
as O
the O
stopping O
criterion O
. O
hidden O
states O
. O
We O
carefully O
compare O
the O
effect O
of O
rebuilding O
hidden O
states O
in O
Section O
4.2 O
and O
experiment O
results O
show O
that O
rebuilding O
all O
hidden O
states O
beneÔ¨Åts O
translation O
. O
3.2 O
Stopping O
Criterion O
In O
consecutive O
NMT O
, O
the O
decoding O
algorithm O
such O
as O
greedy O
decoding O
or O
beam O
search O
terminates O
when O
the O
translator O
predicts O
an O
EOS O
token O
or O
the O
length O
of O
the O
translation O
meets O
a O
predeÔ¨Åned O
threshold O
( O
e.g. O
200 O
) O
. O
The O
decoding O
for O
most O
source O
sentences O
terminates O
when O
the O
translator O
predicts O
the O
EOS O
token.1In O
simultaneous O
decoding O
, O
since O
we O
use O
a O
NMT O
model O
pretrained O
on O
full O
sentences O
to O
translate O
partial O
source O
sentences O
, O
it O
tends O
to O
predict O
EOS O
when O
the O
source O
context O
has O
been O
fully O
translated O
. O
However O
, O
such O
strategy O
could O
be O
too O
aggressive O
for O
simultaneous O
translation O
. O
Fig O
. O
1 O
shows O
such O
an O
example O
. O
At O
outer O
step O
2 O
, O
the O
translator O
predicts O
‚Äú O
you O
EOS O
‚Äù O
, O
emiting O
target O
token O
‚Äú O
you O
‚Äù O
. O
However O
, O
‚Äú O
you O
‚Äù O
is O
not O
the O
expected O
translation O
for O
‚Äú O
‰Ω† O
‚Äù O
in O
the O
context O
of O
‚Äú O
‰Ω†Â•Ω O
„ÄÇ O
‚Äù O
. O
Therefore O
, O
we O
hope O
preÔ¨Åx O
translation O
at O
outer O
step O
2can O
terminate O
without O
emitting O
any O
words O
. O
To O
alleviate O
such O
problems O
and O
do O
better O
simultaneous O
translation O
with O
pretrained O
CNMT O
model O
, O
we O
propose O
two O
novel O
stopping O
criteria O
for O
preÔ¨Åx O
translation O
. O
3.2.1 O
Length O
and O
EOS O
Control O
In O
consecutive O
translation O
, O
the O
decoding O
process O
stops O
mainly O
when O
predicting O
EOS O
. O
In O
contrast O
, O
for O
preÔ¨Åx O
translation O
at O
non O
- O
terminal O
outer O
step O
, O
we O
stop O
the O
translation O
process O
when O
translation O
length O
isdtokens O
behind O
source O
sentence O
length O
: O
œÑ[s O
] O
= O
Œ∑[s]‚àíd O
. O
SpeciÔ¨Åcally O
, O
at O
the O
beginning O
of O
outer O
steps O
, O
we O
have O
source O
preÔ¨Åx O
xs={x1, O
... O
,xŒ∑[s O
] O
} O
and O
target O
preÔ¨Åx O
ys O
œÑ[s‚àí1]={y1, O
... O
,yœÑ[s‚àí1 O
] O
} O
. O
PreÔ¨Åx O
translation O
terminates O
at O
inner O
step O
wswhen O
1We O
conduct O
greedy O
decoding O
on O
the O
validation O
set O
of O
WMT15 O
EN O
‚ÜíDE O
translation O
with O
fairseq O
- O
py O
, O
and O
Ô¨Ånd O
that O
100 O
% O
translation O
terminates O
with O
EOS O
predicted.193Figure O
2 O
: O
Framework O
of O
our O
proposed O
model O
with O
the O
TN O
controller O
. O
predicting O
an O
EOS O
token O
or O
satisfying O
: O
ws=/braceleftBigmax(0,Œ∑[s]‚àíœÑ[s‚àí1]‚àíd)s O
< O
S O
200‚àíœÑ[s‚àí1]s O
= O
S O
( O
7 O
) O
wheredis O
a O
non O
- O
negative O
integer O
that O
determines O
the O
translation O
latency O
of O
the O
system O
. O
We O
call O
this O
stopping O
criterion O
as O
Length O
and O
EOS O
( O
LE O
) O
stopping O
controller O
. O
3.2.2 O
Learning O
When O
to O
Stop O
Although O
simple O
and O
easy O
to O
implement O
, O
LE O
controller O
lacks O
the O
capability O
to O
learn O
the O
optimal O
timing O
with O
which O
to O
stop O
preÔ¨Åx O
translation O
. O
Therefore O
, O
we O
design O
a O
small O
trainable O
network O
called O
trainable O
( O
TN O
) O
stopping O
controller O
to O
learn O
when O
to O
stop O
preÔ¨Åx O
translation O
for O
non O
- O
terminal O
outer O
step O
. O
Fig O
. O
2 O
shows O
the O
illustration O
. O
At O
each O
inner O
decoding O
step O
kfor O
non O
- O
terminal O
outer O
steps O
, O
the O
TN O
controller O
utilizes O
a O
stochastic O
policyœÄŒ∏parameterized O
by O
a O
neural O
network O
to O
make O
the O
binary O
decision O
on O
whether O
to O
stop O
translation O
at O
current O
step O
: O
œÄŒ∏(aœÑ[s‚àí1]+k|zs O
œÑ[s‚àí1]+k O
) O
= O
fŒ∏(zs O
œÑ[s‚àí1]+k),(8 O
) O
wherezs O
œÑ[s‚àí1]+kis O
the O
current O
decoder O
hidden O
state O
. O
We O
implement O
fŒ∏with O
a O
feedforward O
network O
with O
two O
hidden O
layers O
, O
followed O
by O
a O
softmax O
layer O
. O
The O
preÔ¨Åx O
translation O
stops O
if O
the O
TN O
controller O
predictsaœÑ[s‚àí1]+k= O
1 O
. O
Our O
TN O
controller O
is O
much O
simpler O
than O
previous O
work O
( O
Gu O
et O
al O
. O
, O
2017 O
) O
which O
implements O
the O
READ O
/ O
WRITE O
policy O
network O
using O
a O
recurrent O
neural O
network O
whose O
input O
is O
the O
combination O
of O
the O
current O
context O
vector O
, O
the O
current O
decoder O
state O
and O
the O
embedding O
vector O
of O
the O
candidate O
word O
. O
To O
train O
the O
TN O
controller O
, O
we O
freeze O
the O
NMT O
model O
with O
pretrained O
parameters O
, O
and O
optimizethe O
TN O
network O
with O
policy O
gradient O
for O
reward O
maximizationJ O
= O
EœÄŒ∏(/summationtextTœÑ O
t=1rt O
) O
. O
With O
a O
trained O
TN O
controller O
, O
preÔ¨Åx O
translation O
stops O
at O
inner O
decoding O
step O
wswhen O
predicting O
an O
EOS O
token O
or O
satisfying O
: O
/braceleftBigaœÑ[s‚àí1]+ws= O
1s O
< O
S O
ws= O
200‚àíœÑ[s‚àí1]s‚â§S.(9 O
) O
In O
the O
following O
, O
we O
talk O
about O
the O
details O
of O
the O
reward O
function O
and O
the O
training O
with O
policy O
gradient O
. O
Reward O
To O
trade O
- O
off O
between O
translation O
quality O
and O
latency O
, O
we O
deÔ¨Åne O
the O
reward O
function O
at O
inner O
decoding O
step O
kof O
outer O
step O
sas O
: O
rt O
= O
rQ O
t+Œ±¬∑rD O
t O
, O
( O
10 O
) O
wheret O
= O
œÑ[s‚àí1]+k O
, O
andrQ O
tandrD O
tare O
rewards O
related O
to O
quality O
and O
delay O
, O
respectively O
. O
Œ±‚â•0 O
is O
a O
hyper O
- O
parameter O
that O
we O
adjust O
to O
balance O
the O
trade O
- O
off O
between O
translation O
quality O
and O
delay O
. O
Similar O
to O
Gu O
et O
al O
. O
( O
2017 O
) O
, O
we O
utilize O
sentencelevel O
BLEU O
( O
Papineni O
et O
al O
. O
, O
2002 O
; O
Lin O
and O
Och O
, O
2004 O
) O
with O
reward O
shaping O
( O
Ng O
et O
al O
. O
, O
1999 O
) O
as O
the O
reward O
for O
quality O
: O
rQ O
t=/braceleftBig‚àÜBLEU O
( O
y‚àó,y O
, O
t)k O
/ O
negationslash O
= O
wsors O
/ O
negationslash O
= O
S O
BLEU O
( O
y‚àó,y)k O
= O
wsands O
= O
S O
( O
11 O
) O
where O
‚àÜBLEU O
( O
y‚àó,y O
, O
t O
) O
= O
BLEU O
( O
y‚àó,yt)‚àíBLEU O
( O
y‚àó,yt‚àí1)(12 O
) O
is O
the O
intermediate O
reward O
. O
Note O
that O
the O
higher O
the O
values O
of O
BLEU O
are O
, O
the O
more O
rewards O
the O
TN O
controller O
receives O
. O
Following O
Ma O
et O
al O
. O
( O
2018 O
) O
, O
we O
use O
average O
lagging O
( O
AL O
) O
as O
the O
reward O
for O
latency O
: O
rD O
t=Ô£± O
Ô£≤ O
Ô£≥0 O
k O
/ O
negationslash O
= O
wsors O
/ O
negationslash O
= O
S O
‚àí‚åäd(x O
, O
y)‚àíd‚àó‚åã+k O
= O
wsands O
= O
S O
( O
13 O
) O
where O
d(x O
, O
y O
) O
= O
1 O
teœÑe O
/ O
summationdisplay O
t=1l(t)‚àít‚àí1 O
Œª O
. O
( O
14 O
) O
l(t)is O
the O
number O
of O
observed O
source O
tokens O
when O
generating O
the O
t O
- O
th O
target O
token O
, O
te=194Dataset O
Train O
Validation O
Test O
IWSLT16 O
193,591 O
993 O
1,305 O
WMT15 O
3,745,796 O
3,003 O
2,169 O
NIST O
1,252,977 O
878 O
4,103 O
Table O
1 O
: O
# O
sentences O
in O
each O
dataset O
. O
argmint(l(t O
) O
= O
|x|)denotes O
the O
earliest O
point O
when O
the O
system O
observes O
the O
full O
source O
sentence O
, O
Œª=|y| O
|x|represents O
the O
target O
- O
to O
- O
source O
length O
ratio O
andd‚àó‚â•0is O
a O
hyper O
- O
parameter O
called O
target O
delay O
that O
indicates O
the O
desired O
system O
latency O
. O
Note O
that O
the O
lower O
the O
values O
of O
AL O
are O
, O
the O
more O
rewards O
the O
TN O
controller O
receives O
. O
Policy O
Gradient O
We O
train O
the O
TN O
controller O
with O
policy O
gradient(Sutton O
et O
al O
. O
, O
1999 O
) O
, O
and O
the O
gradients O
are O
: O
‚àáŒ∏J O
= O
EœÄŒ∏ O
/ O
bracketleftBiggTœÑ O
/ O
summationdisplay O
t=1Rt‚àáŒ∏logœÄŒ∏(at|¬∑)/bracketrightBigg O
, O
( O
15 O
) O
whereRt=/summationtextTœÑ O
i O
= O
triis O
the O
cumulative O
future O
rewards O
for O
the O
current O
decision O
. O
We O
can O
adopt O
any O
sampling O
approach O
( O
Chen O
et O
al O
. O
, O
2017 O
, O
2018 O
; O
Shen O
et O
al O
. O
, O
2018 O
) O
to O
estimate O
the O
expected O
gradient O
. O
In O
our O
experiments O
, O
we O
randomly O
sample O
multiple O
action O
trajectories O
from O
the O
current O
policy O
œÄŒ∏and O
estimate O
the O
gradient O
with O
the O
collected O
accumulated O
reward O
. O
We O
try O
the O
variance O
reduction O
techniques O
by O
subtracting O
a O
baseline O
average O
reward O
estimated O
by O
a O
linear O
regression O
model O
from O
Rt O
and O
Ô¨Ånd O
that O
it O
does O
not O
help O
to O
improve O
the O
performance O
. O
Therefore O
, O
we O
just O
normalize O
the O
reward O
in O
each O
mini O
- O
batch O
without O
using O
baseline O
reward O
for O
simplicity O
. O
4 O
Experiments O
4.1 O
Settings O
Dataset O
We O
compare O
our O
approach O
with O
the O
baselines O
on O
WMT15 O
German O
- O
English2(DE O
- O
EN O
) O
translation O
in O
both O
directions O
. O
This O
is O
also O
the O
most O
widely O
used O
dataset O
to O
evaluate O
SNMT O
‚Äôs O
performance O
( O
Cho O
and O
Esipova O
, O
2016 O
; O
Gu O
et O
al O
. O
, O
2017 O
; O
Ma O
et O
al O
. O
, O
2018 O
; O
Arivazhagan O
et O
al O
. O
, O
2019 O
; O
Zheng O
et O
al O
. O
, O
2019 O
) O
. O
To O
further O
evaluate O
our O
approach O
‚Äôs O
efÔ¨Åcacy O
in O
trading O
off O
translation O
quality O
and O
latency O
on O
other O
language O
pair O
and O
spoken O
language O
, O
we O
also O
2http://www.statmt.org/wmt15/conduct O
experiments O
with O
the O
proposed O
LE O
and O
TN O
methods O
on O
NIST O
Chinese O
- O
to O
- O
English3(ZH‚ÜíEN O
) O
translation O
and O
IWSLT16 O
German O
- O
English4(DEEN O
) O
translation O
in O
both O
directions O
. O
For O
WMT15 O
, O
we O
use O
newstest2014 O
for O
validation O
and O
newstest2015 O
for O
test O
. O
For O
NIST O
, O
we O
use O
MT02 O
for O
validation O
, O
and O
MT05 O
, O
MT06 O
, O
MT08 O
for O
test O
. O
For O
IWSLT16 O
, O
we O
use O
tst13 O
for O
validation O
and O
tst14 O
for O
test O
. O
All O
the O
data O
is O
tokenized O
and O
segmented O
into O
subword O
symbols O
using O
byte O
- O
pair O
encoding O
( O
Sennrich O
et O
al O
. O
, O
2016 O
) O
to O
restrict O
the O
size O
of O
the O
vocabulary O
. O
We O
use O
40,000 O
joint O
merge O
operations O
on O
WMT15 O
, O
and O
24,000 O
on O
IWSLT16 O
. O
For O
NIST O
, O
we O
use O
30,000 O
merge O
operations O
for O
source O
and O
target O
side O
separately O
. O
Without O
explicitly O
mention O
, O
we O
simulate O
simultaneous O
translation O
scenario O
at O
inference O
time O
with O
these O
datasets O
by O
assuming O
that O
the O
system O
observes O
one O
new O
source O
token O
at O
each O
outer O
step O
, O
i.e. O
,cs= O
1 O
. O
Table O
1 O
shows O
the O
data O
statistics O
. O
Pretrained O
NMT O
Model O
We O
use O
Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
trained O
with O
maximum O
likelihood O
estimation O
as O
the O
pretrained O
CNMT O
model O
and O
implement O
our O
method O
based O
on O
fairseq O
- O
py.5We O
follow O
the O
setting O
in O
transformer O
iwslt O
deen O
for O
IWSLT16 O
dataset O
, O
and O
transformer O
wmtende O
for O
WMT15 O
and O
NIST O
dataset O
. O
Fairseq O
- O
py O
adds O
an O
EOS O
token O
for O
all O
source O
sentences O
during O
training O
and O
inference O
. O
Therefore O
, O
to O
be O
consistent O
with O
the O
CNMT O
model O
implemented O
with O
fairseq O
- O
py O
, O
we O
also O
add O
an O
EOS O
token O
at O
the O
end O
of O
the O
source O
preÔ¨Åx O
for O
preÔ¨Åx O
translation O
and O
Ô¨Ånd O
that O
the O
EOS O
helps O
translation O
. O
TN O
Controller O
To O
train O
the O
TN O
controller O
, O
we O
use O
a O
mini O
- O
batch O
size O
of O
8,16,16 O
and O
sample O
5,10,10 O
trajectories O
for O
each O
sentence O
pair O
in O
a O
batch O
for O
IWSLT16 O
, O
WMT15 O
and O
NIST O
, O
respectively O
. O
We O
set O
the O
number O
of O
newly O
observed O
source O
tokens O
at O
each O
outer O
step O
to O
be O
1during O
the O
training O
for O
simplicity O
. O
We O
set O
Œ±to O
be O
0.04 O
, O
andd‚àóto O
be O
2,5,8 O
. O
All O
our O
TN O
controllers O
are O
trained O
with O
policy O
gradient O
using O
Adam O
optimizer O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
with O
30,000 O
updates O
. O
We O
select O
the O
last O
model O
as O
our O
Ô¨Ånal O
TN O
controller O
. O
Baseline O
We O
compare O
our O
model O
against O
three O
baselines O
that O
utilize O
a O
pretrained O
CNMT O
model O
to O
3These O
sentence O
pairs O
are O
mainly O
extracted O
from O
LDC2002E18 O
, O
LDC2003E07 O
, O
LDC2003E14 O
, O
Hansards O
portion O
of O
LDC2004T07 O
, O
LDC2004T08 O
and O
LDC2005T06 O
4https://workshop2016.iwslt.org/ O
5https://github.com/pytorch/fairseq195Figure O
3 O
: O
Comparison O
with O
the O
baselines O
on O
the O
test O
set O
of O
WMT15 O
EN O
‚ÜíDE O
and O
WMT15 O
DE O
‚ÜíEN O
translations O
. O
The O
shown O
points O
from O
left O
to O
right O
on O
the O
same O
line O
are O
the O
results O
of O
simultaneous O
greedy O
decoding O
with O
d‚àó‚àà O
{ O
2,5,8}for O
TN O
, O
d‚àà{0,2,4,6,8}for O
LE O
, O
œÅ‚àà{0.65,0.6,0.55,0.5,0.45,0.4}for O
SL O
, O
k‚àà{1,3,5,7,9}for O
testtime O
waitk O
andCW‚àà{2,5,8}for O
RWAgent O
. O
The O
scores O
of O
Greedy O
decoding O
: O
BLEU= O
25.16 O
, O
AL= O
28.10for O
WMT15 O
EN‚ÜíDE O
translation O
and O
BLEU= O
26.17 O
, O
AL= O
31.20for O
WMT15 O
DE‚ÜíEN O
translation O
. O
Figure O
4 O
: O
Performance O
on O
the O
test O
set O
of O
IWSLT16 O
EN O
‚ÜíDE O
translation O
, O
IWSLT16 O
DE O
‚ÜíEN O
translation O
and O
NIST O
ZH‚ÜíEN O
translation O
. O
The O
shown O
points O
from O
left O
to O
right O
on O
the O
same O
line O
are O
the O
results O
of O
d‚àó‚àà{2,5,8}for O
TN O
andd‚àà{0,2,4,6,7}for O
LE O
. O
# O
$ O
: O
full O
- O
sentence O
( O
greedy O
and O
beam O
- O
search O
) O
. O
perform O
simultaneous O
translation O
: O
‚Ä¢testtime O
waitk O
( O
Ma O
et O
al O
. O
, O
2018 O
): O
the O
method O
that O
decodes O
with O
a O
waitk O
policy O
with O
a O
CNMT O
model O
. O
We O
report O
the O
results O
when O
k‚àà O
{ O
1,3,5,7,9 O
} O
. O
‚Ä¢SL(Zheng O
et O
al O
. O
, O
2019 O
): O
the O
method O
that O
adapts O
CNMT O
to O
SNMNT O
by O
learning O
an O
adaptive O
READ O
/ O
WRITE O
policy O
from O
oracle O
READ O
/ O
WRITE O
sequences O
generated O
with O
heuristics O
. O
We O
report O
the O
results O
with O
thresholdœÅ‚àà{0.65,0.6,0.55,0.5,0.45,0.4 O
} O
. O
‚Ä¢RWAgent O
( O
Gu O
et O
al O
. O
, O
2017 O
): O
the O
adaptation O
of O
Gu O
et O
al O
. O
( O
2017 O
) O
‚Äôs O
full O
- O
sentence O
model O
and O
reinforced O
READ O
/ O
WRITE O
policy O
network O
to O
Transformer O
by O
Ma O
et O
al O
. O
( O
2018 O
) O
. O
We O
report O
the O
results O
when O
using O
CW‚àà{2,5,8}as O
the O
target O
delay O
. O
We O
report O
the O
result O
with O
d‚àà{0,2,4,6,8}for O
our O
proposed O
LE O
method O
and O
d‚àó‚àà{2,5,8}for O
our O
proposed O
TN O
method O
. O
For O
all O
baselines O
, O
we O
cite O
the O
results O
reported O
in O
Zheng O
et O
al O
. O
( O
2019).6 O
4.2 O
Results O
We O
compare O
our O
methods O
with O
the O
baselines O
on O
the O
test O
set O
of O
WMT15 O
EN O
‚ÜíDE O
and O
DE‚ÜíEN O
translation O
tasks O
, O
as O
shown O
in O
Fig O
. O
3 O
. O
The O
points O
closer O
to O
the O
upper O
left O
corner O
indicate O
better O
overall O
performance O
, O
namely O
low O
latency O
and O
high O
quality O
. O
We O
observe O
that O
as O
latency O
increases O
, O
all O
methods O
improve O
in O
quality O
. O
the O
TN O
method O
signiÔ¨Åcantly O
outperforms O
all O
the O
baselines O
in O
both O
translation O
tasks O
, O
6Since O
Zheng O
et O
al O
. O
( O
2019 O
) O
did O
not O
mention O
the O
details O
of O
data O
preprocessing O
, O
we O
can O
not O
compare O
the O
BLEU O
and O
AL O
scores O
directly O
with O
theirs O
. O
Therefore O
, O
we O
normalize O
the O
BLEU O
and O
AL O
scores O
with O
its O
corresponding O
upper O
bound O
, O
i.e. O
the O
BLEU O
and O
AL O
scores O
obtained O
when O
the O
pretrained O
Transformer O
performs O
standard O
greedy O
decoding O
( O
Greedy O
) O
.196Figure O
5 O
: O
Comparison O
of O
whether O
to O
reuse O
previous O
encoder O
or O
decoder O
hidden O
states O
on O
WMT15 O
EN O
‚ÜíDE O
test O
set O
with O
the O
LE O
controller O
. O
The O
left O
Y O
axis O
is O
the O
BLEU O
score O
and O
the O
right O
Y O
axis O
is O
the O
length O
ratio O
: O
the O
translation O
length O
divided O
by O
the O
reference O
length O
. O
The O
points O
on O
the O
same O
line O
are O
the O
results O
of O
d‚àà{0,2,4,6,8}.none O
: O
rebuild O
all O
encoder O
/ O
decoder O
hidden O
states O
; O
decoder O
: O
reuse O
decoder O
hidden O
states O
and O
rebuild O
all O
encoder O
hidden O
states O
; O
encoder O
: O
reuse O
previous O
encoder O
hidden O
states O
and O
rebuild O
all O
decoder O
hidden O
states O
. O
demonstrating O
that O
it O
indeed O
learns O
the O
appropriate O
timing O
to O
stop O
preÔ¨Åx O
translation O
. O
LE O
outperforms O
the O
baselines O
on O
WMT15 O
EN O
‚ÜíDE O
translation O
at O
high O
latency O
region O
and O
performs O
similarly O
or O
worse O
on O
other O
cases O
. O
We O
show O
the O
methods O
‚Äô O
efÔ¨Åcacy O
in O
trading O
off O
quality O
and O
latency O
on O
other O
language O
pair O
and O
spoken O
language O
in O
Fig O
. O
4 O
. O
TN O
outperforms O
LE O
on O
all O
translation O
tasks O
, O
especially O
at O
the O
low O
latency O
region O
. O
It O
obtains O
promising O
translation O
quality O
with O
acceptable O
latency O
: O
with O
a O
lag O
of O
< O
7tokens O
, O
TN O
obtains O
96.95 O
% O
, O
97.20 O
% O
and O
94.03 O
% O
BLEU O
with O
respect O
to O
consecutive O
greedy O
decoding O
for O
IWSLT16 O
EN‚ÜíDE O
, O
IWSLT16 O
DE‚ÜíEN O
and O
NIST O
ZH‚ÜíEN O
translations O
, O
respectively O
. O
4.3 O
Analyze O
We O
analyze O
the O
effect O
of O
different O
ways O
to O
obtain O
the O
encoder O
and O
decoder O
hidden O
states O
at O
the O
beginning O
of O
preÔ¨Åx O
translation O
with O
the O
LE O
controller O
. O
Fig O
. O
5 O
shows O
the O
result O
. O
We O
try O
three O
variants O
: O
a O
) O
dynamically O
rebuild O
all O
encoder O
/ O
decoder O
hidden O
states O
( O
none O
) O
; O
b O
) O
reuse O
decoder O
hidden O
states O
and O
rebuild O
all O
encoder O
hidden O
states O
( O
decoder O
) O
; O
c O
) O
reuse O
previous O
encoder O
hidden O
states O
and O
rebuild O
all O
decoder O
hidden O
states O
( O
encoder O
) O
. O
The O
left O
Y O
axis O
and O
X O
axis O
show O
BLEU O
- O
vs O
- O
AL O
curve O
. O
We O
observe O
that O
if O
reusing O
previous O
encoder O
hidden O
states O
( O
encoder O
) O
, O
the O
translation O
fails O
. O
We O
ascribe O
this O
to O
the O
discrepancy O
between O
training O
and O
decoding O
for O
the O
encoder O
. O
We O
also O
observe O
that O
when O
d‚àà0,2 O
, O
reusing O
decoder O
hidden O
states O
( O
decoder O
) O
obtain O
negative O
AL.To O
analyze O
this O
, O
we O
plot O
the O
translation O
to O
reference O
length O
ratio O
versus O
AL O
curve O
with O
the O
right O
Y O
axis O
and O
X O
axis O
. O
It O
shows O
that O
with O
decoder O
, O
the O
decoding O
process O
stops O
too O
early O
and O
generates O
too O
short O
translations O
. O
Therefore O
, O
to O
avoid O
such O
problem O
and O
to O
be O
consistent O
with O
the O
training O
process O
of O
the O
CNMT O
model O
, O
it O
is O
important O
to O
dynamically O
rebuild O
all O
encoder O
/ O
decoder O
hidden O
states O
for O
preÔ¨Åx O
translation O
. O
Since O
we O
make O
no O
assumption O
about O
the O
cs O
, O
i.e. O
, O
the O
number O
of O
newly O
observed O
source O
tokens O
at O
each O
outer O
step O
, O
we O
also O
test O
the O
effect O
of O
differentcs O
. O
Fig O
. O
6 O
shows O
the O
result O
with O
the O
LE O
and O
TN O
controllers O
on O
the O
test O
set O
of O
WMT15 O
EN O
‚ÜíDE O
translation O
. O
We O
observe O
that O
as O
csincreases O
, O
both O
LE O
and O
TN O
trend O
to O
improve O
in O
quality O
and O
worsen O
in O
latency O
. O
When O
cs= O
1 O
, O
LE O
controller O
obtains O
the O
best O
balance O
between O
quality O
and O
latency O
. O
In O
contrast O
, O
TN O
controller O
obtains O
similar O
quality O
and O
latency O
balance O
with O
different O
cs O
, O
demonstrating O
that O
TN O
controller O
successfully O
learns O
the O
right O
timing O
to O
stop O
regardless O
of O
the O
input O
update O
schedule O
. O
We O
also O
analyze O
the O
TN O
controller O
‚Äôs O
adaptability O
by O
monitoring O
the O
initial O
delay O
, O
i.e. O
, O
the O
number O
of O
observed O
source O
tokens O
before O
emitting O
the O
Ô¨Årst O
target O
token O
, O
on O
the O
test O
set O
of O
WMT15 O
EN O
‚ÜíDE O
translation O
, O
as O
shown O
in O
Fig O
. O
7 O
. O
d‚àóis O
the O
target O
delay O
measured O
with O
AL O
( O
used O
in O
Eq O
. O
13 O
) O
. O
It O
demonstrates O
that O
the O
TN O
controller O
has O
a O
lot O
of O
variance O
in O
it O
‚Äôs O
initial O
delay O
. O
The O
distribution O
of O
initial O
delay O
changes O
with O
different O
target O
delay O
: O
with O
higher O
target O
delay O
, O
the O
average O
initial O
delay O
is O
larger O
. O
For O
most O
sentences O
, O
the O
initial O
delay O
is O
within O
1‚àí7 O
. O
In O
speech O
translation O
, O
listeners O
are O
also O
concerned O
with O
long O
silences O
during O
which O
no O
translation O
occurs O
. O
Following O
Gu O
et O
al O
. O
( O
2017 O
) O
; O
Ma O
et O
al O
. O
( O
2018 O
) O
, O
we O
use O
Consecutive O
Wait O
( O
CW O
) O
to O
measure O
this O
: O
CW(x O
, O
y O
) O
= O
/summationtextS O
s=1cs O
/ O
summationtextS O
s=1 O
1ws>0 O
. O
( O
16 O
) O
Fig O
. O
8 O
shows O
the O
BLEU O
- O
vs O
- O
CW O
plots O
for O
our O
proposed O
two O
methods O
. O
The O
TN O
controller O
has O
higher O
CW O
than O
the O
LE O
controller O
. O
This O
is O
because O
TN O
controller O
prefers O
consecutive O
updating O
output O
buffer O
( O
e.g. O
, O
it O
often O
produces O
wsas O
0 O
0 O
0 O
0 O
3 O
0 O
0 O
0 O
0 O
0 O
5 O
0 O
0 O
0 O
0 O
4 O
... O
) O
while O
the O
LE O
controller O
often O
updates O
its O
output O
buffer O
following O
the O
input O
buffer O
( O
e.g. O
, O
it O
often O
produces O
wsas O
0 O
0 O
0 O
0 O
1 O
1 O
1 O
1 O
1 O
1 O
... O
whend= O
4 O
) O
. O
Although O
larger O
than O
LE O
, O
the O
CW O
for O
TN O
( O
< O
6 O
) O
is O
acceptable O
for O
most O
speech O
translation O
scenarios.197Figure O
6 O
: O
Performance O
on O
the O
test O
set O
of O
WMT15 O
EN O
‚ÜíDE O
translation O
with O
different O
input O
buffer O
update O
schedule O
. O
Points O
on O
the O
same O
line O
are O
obtained O
by O
increasing O
d‚àà0,2,4,6,8for O
( O
a O
) O
andd‚àó‚àà2,5,8for O
( O
b O
) O
. O
Figure O
7 O
: O
Number O
of O
observed O
source O
tokens O
before O
emitting O
the O
Ô¨Årst O
target O
token O
for O
the O
TN O
controller O
on O
the O
test O
set O
of O
WMT15 O
EN‚ÜíDE O
translation O
. O
Figure O
8 O
: O
Average O
consecutive O
write O
length O
on O
the O
test O
set O
of O
WMT15 O
EN‚ÜíDE O
translation O
. O
4.4 O
Translation O
Examples O
Fig O
. O
9 O
shows O
two O
translation O
examples O
with O
the O
LE O
and O
TN O
controllers O
on O
the O
test O
set O
of O
NIST O
ZH O
‚ÜíEN O
and O
WMT15 O
EN‚ÜíDE O
translation O
. O
In O
manual O
inspection O
of O
these O
examples O
and O
others O
, O
we O
Ô¨Ånd O
that O
the O
TN O
controller O
learns O
a O
conservative O
timing O
for O
stopping O
preÔ¨Åx O
translation O
. O
For O
example O
, O
in O
example O
1 O
, O
TN O
outputs O
translation O
‚Äú O
wu O
bangguo O
attended O
the O
signing O
ceremony O
‚Äù O
when O
observing O
‚Äú O
Âê¥ÈÇ¶ÂõΩÂá∫Â∏≠Á≠æÂ≠ó‰ª™ÂºèÂπ∂ O
‚Äù O
, O
instead O
of O
a O
more O
radical O
translation O
‚Äú O
wu O
bangguo O
attended O
the O
signing O
ceremony O
and O
‚Äù O
. O
Such O
strategy O
helps O
to O
alleviate O
the O
problem O
of O
premature O
translation O
, O
i.e. O
, O
translating O
before O
observing O
enough O
future O
context.5 O
Related O
Work O
A O
number O
of O
works O
in O
simultaneous O
translation O
divide O
the O
translation O
process O
into O
two O
stages O
. O
A O
segmentation O
component O
Ô¨Årst O
divides O
the O
incoming O
text O
into O
segments O
, O
and O
then O
each O
segment O
is O
translated O
by O
a O
translator O
independently O
or O
with O
previous O
context O
. O
The O
segmentation O
boundaries O
can O
be O
predicted O
by O
prosodic O
pauses O
detected O
in O
speech O
( O
F¬®ugen O
et O
al O
. O
, O
2007 O
; O
Bangalore O
et O
al O
. O
, O
2012 O
) O
, O
linguistic O
cues O
( O
Sridhar O
et O
al O
. O
, O
2013 O
; O
Matusov O
et O
al O
. O
, O
2007 O
) O
, O
or O
a O
classiÔ¨Åer O
based O
on O
alignment O
information O
( O
Siahbani O
et O
al O
. O
, O
2014 O
; O
Yarmohammadi O
et O
al O
. O
, O
2013 O
) O
and O
translation O
accuracy O
( O
Oda O
et O
al O
. O
, O
2014 O
; O
Grissom O
et O
al O
. O
, O
2014 O
; O
Siahbani O
et O
al O
. O
, O
2018 O
) O
. O
Some O
authors O
have O
recently O
endeavored O
to O
perform O
simultaneous O
translation O
in O
the O
context O
of O
NMT O
. O
Niehues O
et O
al O
. O
( O
2018 O
) O
; O
Arivazhagan O
et O
al O
. O
( O
2020 O
) O
adopt O
a O
re O
- O
translation O
approach O
where O
the O
source O
is O
repeatedly O
translated O
from O
scratch O
as O
it O
grows O
and O
propose O
methods O
to O
improve O
translation O
stability O
. O
Cho O
and O
Esipova O
( O
2016 O
) O
; O
Dalvi O
et O
al O
. O
( O
2018 O
) O
; O
Ma O
et O
al O
. O
( O
2018 O
) O
introduce O
a O
manually O
designed O
criterion O
to O
control O
when O
to O
translate O
. O
Satija O
and O
Pineau O
( O
2016 O
) O
; O
Gu O
et O
al O
. O
( O
2017 O
) O
; O
Alinejad O
et O
al O
. O
( O
2018 O
) O
extend O
the O
criterion O
into O
a O
trainable O
agent1981 O
2 O
3 O
4 O
5 O
6 O
7 O
8 O
9 O
Âê¥ÈÇ¶ÂõΩÂá∫Â∏≠Á≠æÂ≠ó‰ª™ÂºèÂπ∂ O
Âú®ÂçèËÆÆ‰∏ä O
Á≠æÂ≠ó O
LE O
wu O
bangguo O
attended O
the O
signing O
ceremony O
and O
signed O
the O
agreement O
TN O
wu O
bangguo O
attended O
the O
signing O
ceremony O
and O
signed O
the O
agreement O
Greedy O
wu O
bangguo O
attended O
the O
signing O
ceremony O
and O
signed O
the O
agreement O
Ref O
wu O
bangguo O
attends O
signing O
ceremony O
and O
signs O
agreement O
NATO O
does O
not O
want O
to O
break O
agreements O
with O
Russia O
LE O
Die O
NATO O
m O
¬®ochte O
keine O
Abkommen O
mit O
Russland O
brechen O
TN O
Die O
NATO O
will O
keine O
Abkommen O
mit O
Russland O
brechen O
Greedy O
Die O
NATO O
m O
¬®ochte O
keine O
Abkommen O
mit O
Russland O
brechen O
Ref O
NATO O
will O
Vereinbarungen O
mit O
Russland O
nicht O
brechen O
Figure O
9 O
: O
Translation O
examples O
from O
the O
test O
set O
of O
NIST O
ZH O
‚ÜíEN O
( O
example O
1 O
) O
and O
WMT15 O
EN O
‚ÜíDE O
translation O
( O
example O
2 O
) O
. O
We O
compare O
LE O
with O
d= O
4 O
and O
TN O
with O
d‚àó= O
5 O
because O
these O
two O
models O
achieve O
similar O
latency O
. O
Greedy O
and O
Ref O
represent O
the O
greedy O
decoding O
result O
from O
consecutive O
translation O
and O
the O
reference O
, O
respectively O
. O
in O
a O
reinforcement O
learning O
framework O
. O
However O
, O
these O
work O
either O
develop O
sophisticated O
training O
frameworks O
explicitly O
designed O
for O
simultaneous O
translation O
( O
Ma O
et O
al O
. O
, O
2018 O
) O
or O
fail O
to O
use O
a O
pretrained O
consecutive O
NMT O
model O
in O
an O
optimal O
way O
( O
Cho O
and O
Esipova O
, O
2016 O
; O
Dalvi O
et O
al O
. O
, O
2018 O
; O
Satija O
and O
Pineau O
, O
2016 O
; O
Gu O
et O
al O
. O
, O
2017 O
; O
Alinejad O
et O
al O
. O
, O
2018 O
; O
Zheng O
et O
al O
. O
, O
2019 O
) O
. O
In O
contrast O
, O
our O
work O
is O
signiÔ¨Åcantly O
different O
from O
theirs O
in O
the O
way O
of O
using O
pretrained O
consecutive O
NMT O
model O
to O
perform O
simultaneous O
translation O
and O
the O
design O
of O
the O
two O
stopping O
criteria O
. O
6 O
Conclusion O
We O
have O
presented O
a O
novel O
framework O
for O
improving O
simultaneous O
translation O
with O
a O
pretrained O
consecutive O
NMT O
model O
. O
The O
basic O
idea O
is O
to O
translate O
partial O
source O
sentence O
with O
the O
consecutive O
NMT O
model O
and O
stops O
the O
translation O
with O
two O
novel O
stopping O
criteria O
. O
Extensive O
experiments O
demonstrate O
that O
our O
method O
with O
trainable O
stopping O
controller O
outperforms O
the O
state O
- O
of O
- O
the O
- O
art O
baselines O
in O
balancing O
between O
translation O
quality O
and O
latency O
. O
Acknowledgments O
We O
thank O
the O
anonymous O
reviewers O
for O
their O
insightful O
feedback O
on O
this O
work O
. O
Yun O
Chen O
is O
partially O
supported O
by O
the O
Fundamental O
Research O
Funds O
for O
the O
Central O
Universities O
and O
the O
funds O
of O
Beijing O
Advanced O
Innovation O
Center O
for O
Language O
Resources O
( O
No O
. O
TYZ19005 O
) O
. O
Abstract O
Chinese O
and O
Japanese O
share O
many O
characters O
with O
similar O
surface O
morphology O
. O
To O
better O
utilize O
the O
shared O
knowledge O
across O
the O
languages O
, O
we O
propose O
UnihanLM O
, O
a O
self O
- O
supervised O
Chinese O
- O
Japanese O
pretrained O
masked O
language O
model O
( O
MLM O
) O
with O
a O
novel O
two O
- O
stage O
coarse O
- O
to-Ô¨Åne O
training O
approach O
. O
We O
exploit O
Unihan O
, O
a O
ready O
- O
made O
database O
constructed O
by O
linguistic O
experts O
to O
Ô¨Årst O
merge O
morphologically O
similar O
characters O
into O
clusters O
. O
The O
resulting O
clusters O
are O
used O
to O
replace O
the O
original O
characters O
in O
sentences O
for O
the O
coarse O
- O
grained O
pretraining O
of O
the O
MLM O
. O
Then O
, O
we O
restore O
the O
clusters O
back O
to O
the O
original O
characters O
in O
sentences O
for O
the O
Ô¨Ånegrained O
pretraining O
to O
learn O
the O
representation O
of O
the O
speciÔ¨Åc O
characters O
. O
We O
conduct O
extensive O
experiments O
on O
a O
variety O
of O
Chinese O
and O
Japanese O
NLP O
benchmarks O
, O
showing O
that O
our O
proposed O
UnihanLM O
is O
effective O
on O
both O
mono- O
and O
cross O
- O
lingual O
Chinese O
and O
Japanese O
tasks O
, O
shedding O
light O
on O
a O
new O
path O
to O
exploit O
the O
homology O
of O
languages.1 O
1 O
Introduction O
Recently O
, O
Pretrained O
Language O
Models O
have O
shown O
promising O
performance O
on O
many O
NLP O
tasks O
( O
Peters O
et O
al O
. O
, O
2018 O
; O
Devlin O
et O
al O
. O
, O
2019 O
; O
Liu O
et O
al O
. O
, O
2019 O
; O
Yang O
et O
al O
. O
, O
2019c O
; O
Lan O
et O
al O
. O
, O
2020 O
) O
. O
Many O
attempts O
have O
been O
made O
to O
train O
a O
model O
that O
supports O
multiple O
languages O
. O
Among O
them O
, O
Multilingual O
BERT O
( O
mBERT O
) O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
is O
released O
as O
a O
part O
of O
BERT O
. O
It O
directly O
adopts O
the O
same O
model O
architecture O
and O
training O
objective O
, O
and O
is O
trained O
on O
Wikipedia O
in O
different O
languages O
. O
XLM O
( O
Lample O
and O
Conneau O
, O
2019 O
) O
is O
proposed O
with O
an O
additional O
language O
embedding O
and O
a O
new O
training O
‚àóThis O
work O
was O
done O
during O
Canwen O
‚Äôs O
internship O
at O
Microsoft O
Research O
Asia O
. O
1The O
code O
and O
pretrained O
weights O
are O
available O
at O
https O
: O
//github.com O
/ O
JetRunner O
/ O
unihan O
- O
lm O
.JA O
/ O
uni53F01 O
/ O
uni98A82 O
/ O
uni306F O
/ O
uni71B13 O
/ O
uni5E2F4 O
/ O
uni4F4E O
/ O
uni6C175 O
/ O
uni57276 O
/ O
uni306E O
/ O
uni4E00 O
/ O
uni7A2E7 O
/ O
uni3067 O
/ O
uni3059 O
/ O
uni3002 O
T O
- O
ZHÈ¢±1È¢®2ÊòØÁÜ±3Â∏∂4Ê∞£5ÊóãÁöÑ‰∏ÄÁ®Æ7 O
„ÄÇ O
S O
- O
ZHÂè∞1È£é2ÊòØÁÉ≠3Â∏¶4‰ΩéÊ∞î5Âéã6ÁöÑ‰∏ÄÁßç7 O
„ÄÇ O
EN O
Typhoon O
is O
a O
type O
of O
tropical O
depression O
. O
Table O
1 O
: O
A O
sentence O
example O
in O
Japanese O
( O
JA O
) O
, O
Traditional O
Chinese O
( O
T O
- O
ZH O
) O
and O
SimpliÔ¨Åed O
Chinese O
( O
S O
- O
ZH O
) O
with O
its O
English O
translation O
( O
EN O
) O
. O
The O
characters O
that O
already O
share O
the O
same O
Unicode O
are O
marked O
with O
an O
underline O
. O
In O
this O
work O
, O
we O
further O
match O
characters O
with O
identical O
meanings O
but O
different O
Unicode O
, O
then O
merge O
them O
. O
Characters O
eligible O
to O
be O
merged O
together O
are O
marked O
with O
the O
same O
superscript O
. O
objective O
( O
translation O
language O
modeling O
, O
TLM O
) O
. O
XLM O
- O
R O
( O
Conneau O
et O
al O
. O
, O
2019 O
) O
has O
a O
larger O
size O
and O
is O
trained O
with O
more O
data O
. O
Based O
on O
XLM O
, O
Unicoder O
( O
Huang O
et O
al O
. O
, O
2019 O
) O
collects O
more O
data O
and O
uses O
multi O
- O
task O
learning O
to O
train O
on O
three O
supervised O
tasks O
. O
The O
census O
of O
cross O
- O
lingual O
approaches O
is O
to O
allow O
lexical O
information O
to O
be O
shared O
between O
languages O
. O
XLM O
and O
mBERT O
exploit O
shared O
lexical O
information O
by O
Byte O
Pair O
Encoding O
( O
BPE O
) O
( O
Sennrich O
et O
al O
. O
, O
2016 O
) O
and O
WordPiece O
( O
Wu O
et O
al O
. O
, O
2016 O
) O
, O
respectively O
. O
However O
, O
these O
automatically O
learned O
shared O
representations O
have O
been O
criticized O
by O
recent O
work O
( O
K O
et O
al O
. O
, O
2020 O
) O
, O
which O
reveals O
their O
limitations O
in O
sharing O
meaningful O
semantics O
across O
languages O
. O
Also O
, O
words O
in O
both O
Chinese O
and O
Japanese O
are O
short O
, O
which O
prohibits O
an O
effective O
learning O
of O
sub O
- O
word O
representations O
. O
Different O
from O
European O
languages O
, O
Chinese O
and O
Japanese O
naturally O
share O
Chinese O
characters O
as O
a O
subword O
component O
. O
Early O
work O
( O
Chu O
et O
al O
. O
, O
2013 O
) O
shows O
that O
shared O
characters O
in O
these O
two O
languages O
can O
beneÔ¨Åt O
Examplebased O
Machine O
Translation O
( O
EBMT O
) O
with O
a O
statistical O
based O
phrase O
extraction O
and O
alignment O
. O
For O
Neural O
Machine O
Translation O
( O
NMT O
) O
, O
( O
Zhang O
and O
Ko-201machi O
, O
2019 O
) O
exploited O
such O
information O
by O
learning O
a O
BPE O
representation O
over O
sub O
- O
character O
( O
i.e. O
, O
ideograph O
and O
stroke O
) O
sequence O
. O
They O
applied O
this O
technique O
to O
unsupervised O
Chinese O
- O
Japanese O
machine O
translation O
and O
achieved O
state O
- O
of O
- O
the O
- O
art O
performance O
. O
However O
, O
this O
approach O
greatly O
relies O
on O
unreliable O
automatic O
BPE O
learning O
and O
may O
suffer O
from O
the O
noise O
brought O
by O
various O
variants O
. O
To O
facilitate O
lexical O
sharing O
, O
we O
propose O
Unihan O
Language O
Model O
( O
UnihanLM O
) O
, O
a O
cross O
- O
lingual O
pretrained O
masked O
language O
model O
for O
Chinese O
and O
Japanese O
. O
We O
propose O
a O
two O
- O
stage O
coarse O
- O
to-Ô¨Åne O
pretraining O
procedure O
to O
empower O
better O
generalization O
and O
take O
advantages O
of O
shared O
characters O
in O
Japanese O
, O
Traditional O
and O
SimpliÔ¨Åed O
Chinese O
. O
First O
, O
we O
let O
the O
model O
exploit O
maximum O
possible O
shared O
lexical O
information O
. O
Instead O
of O
learning O
a O
shared O
sub O
- O
word O
vocabulary O
like O
the O
prior O
work O
, O
we O
leverage O
Unihan O
database O
( O
Jenkins O
et O
al O
. O
, O
2019 O
) O
, O
a O
ready O
- O
made O
constituent O
of O
the O
Unicode O
standard O
, O
to O
extract O
the O
shared O
lexical O
information O
across O
the O
languages O
. O
By O
exploiting O
this O
database O
, O
we O
can O
effectively O
merge O
characters O
with O
the O
similar O
surface O
morphology O
but O
independent O
Unicodes O
, O
as O
shown O
in O
Table O
1 O
into O
thousands O
of O
clusters O
. O
The O
clusters O
will O
be O
used O
to O
replace O
the O
characters O
in O
sentences O
during O
the O
Ô¨Årst O
- O
stage O
coarse O
- O
grained O
pretraining O
. O
After O
the O
coarse O
- O
grained O
pretraining O
Ô¨Ånishes O
, O
we O
restore O
the O
clusters O
back O
to O
the O
original O
characters O
and O
initialize O
their O
representation O
with O
their O
corresponding O
cluster O
‚Äôs O
representation O
and O
then O
learn O
their O
speciÔ¨Åc O
representation O
during O
the O
second O
- O
stage O
Ô¨Ånegrained O
pretraining O
. O
In O
this O
way O
, O
our O
model O
can O
make O
full O
use O
of O
shared O
characters O
while O
maintaining O
a O
good O
sense O
for O
nuances O
of O
similar O
characters O
. O
To O
verify O
the O
effectiveness O
of O
our O
approach O
, O
we O
evaluate O
on O
both O
lexical O
and O
semantic O
tasks O
in O
Chinese O
and O
Japanese O
. O
On O
word O
segmentation O
, O
our O
model O
outperforms O
monolingual O
and O
multilingual O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
and O
shows O
a O
much O
higher O
performance O
on O
cross O
- O
lingual O
zeroshot O
transfer O
. O
Also O
, O
our O
model O
achieves O
state O
- O
of O
- O
theart O
performance O
on O
unsupervised O
Chinese O
- O
Japanese O
machine O
translation O
, O
and O
is O
even O
comparable O
to O
the O
supervised O
baseline O
on O
Chinese O
- O
to O
- O
Japanese O
translation O
. O
On O
classiÔ¨Åcation O
tasks O
, O
our O
model O
achieves O
a O
comparable O
performance O
with O
monolingual O
BERT O
and O
other O
cross O
- O
lingual O
models O
trained O
with O
the O
same O
scale O
of O
data O
. O
To O
summarize O
, O
our O
contributions O
are O
three O
- O
fold O
: O
( O
1 O
) O
We O
propose O
UnihanLM O
, O
a O
cross O
- O
lingual O
pre O
- O
trained O
language O
model O
for O
Chinese O
and O
Japanese O
NLP O
tasks O
. O
( O
2 O
) O
We O
pioneer O
to O
apply O
the O
language O
resource O
‚Äì O
the O
Unihan O
Database O
to O
help O
model O
pretraining O
, O
allowing O
more O
lexical O
information O
to O
be O
shared O
between O
the O
two O
languages O
. O
( O
3 O
) O
We O
devise O
a O
novel O
coarse O
- O
to-Ô¨Åne O
two O
- O
stage O
pretraining O
strategy O
with O
different O
granularity O
for O
Chinese O
- O
Japanese O
language O
modeling O
. O
2 O
Preliminaries O
2.1 O
Chinese O
Character O
Chinese O
character O
is O
a O
pictograph O
used O
in O
Chinese O
and O
Japanese O
. O
These O
characters O
often O
share O
the O
same O
background O
and O
origin O
. O
However O
, O
due O
to O
historic O
reasons O
, O
Chinese O
characters O
have O
developed O
into O
different O
writing O
systems O
, O
including O
Japanese O
Kanji O
, O
Traditional O
Chinese O
and O
SimpliÔ¨Åed O
Chinese O
. O
Also O
, O
even O
in O
a O
single O
text O
, O
multiple O
variants O
of O
the O
same O
characters O
can O
be O
used O
interchangeably O
( O
e.g. O
, O
‚Äú O
Âè∞ÁÅ£ O
‚Äù O
and O
‚Äú O
Ëá∫ÁÅ£ O
‚Äù O
for O
‚Äú O
Taiwan O
‚Äù O
, O
in O
Traditional O
Chinese O
) O
. O
These O
characters O
have O
identical O
or O
overlapping O
meanings O
. O
Thus O
, O
it O
is O
critical O
to O
better O
exploit O
such O
information O
for O
modeling O
both O
cross O
- O
lingual O
( O
i.e. O
, O
between O
Chinese O
and O
Japanese O
) O
, O
cross O
- O
system O
( O
i.e. O
, O
between O
Traditional O
and O
SimpliÔ¨Åed O
Chinese O
) O
and O
cross O
- O
variant O
semantics O
. O
Both O
Chinese O
and O
Japanese O
have O
no O
delimiter O
( O
e.g. O
, O
white O
space O
) O
to O
mark O
the O
boundaries O
of O
words O
. O
There O
have O
always O
been O
debates O
over O
whether O
word O
segmentation O
is O
necessary O
for O
Chinese O
NLP O
. O
Recent O
work O
( O
Li O
et O
al O
. O
, O
2019 O
) O
concludes O
that O
it O
is O
not O
necessary O
for O
various O
NLP O
tasks O
in O
Chinese O
. O
Previous O
cross O
- O
lingual O
language O
models O
use O
different O
methods O
for O
tokenization O
. O
mBERT O
adds O
white O
spaces O
around O
Chinese O
characters O
and O
lefts O
Katakana O
/ O
Hiragana O
Japanese O
( O
also O
known O
as O
kanas O
) O
unprocessed O
. O
Different O
from O
mBERT O
, O
XLM O
uses O
Stanford O
Tokenizer2and O
KyTea3to O
segment O
Chinese O
and O
Japanese O
sentences O
, O
respectively O
. O
After O
tokenization O
, O
mBERT O
and O
XLM O
use O
WordPiece O
( O
Wu O
et O
al O
. O
, O
2016 O
) O
and O
Byte O
Pair O
Encoding O
( O
Sennrich O
et O
al O
. O
, O
2016 O
) O
for O
sub O
- O
word O
encoding O
, O
respectively O
. O
Nevertheless O
, O
both O
approaches O
suffer O
from O
obvious O
drawbacks O
. O
For O
mBERT O
, O
the O
kanas O
and O
Chinese O
characters O
are O
treated O
differently O
, O
which O
causes O
a O
mismatch O
for O
labeling O
tasks O
. O
Also O
, O
leaving O
kanas O
untokenized O
may O
cause O
the O
data O
sparsity O
problem O
. O
For O
XLM O
, O
as O
pointed O
out O
in O
( O
Li O
et O
al O
. O
, O
2019 O
) O
, O
an O
2https://nlp.stanford.edu/software/ O
tokenizer.html O
3http://www.phontron.com/kytea/202Variant O
Description O
Example O
Traditional O
Variant O
The O
traditional O
versions O
of O
a O
simpliÔ¨Åed O
Chinese O
character O
. O
Âèë‚ÜíÈ´Æ(hair),Áôº(to O
burgeon O
) O
SimpliÔ¨Åed O
Variant O
The O
simpliÔ¨Åed O
version O
of O
a O
traditional O
Chinese O
character O
. O
Âúò‚ÜíÂõ¢(group O
) O
Z O
- O
Variant O
Same O
character O
with O
different O
unicodes O
only O
for O
compatibility O
. O
Ë™™‚Üî/uni8AAC(say O
) O
Semantic O
Variant O
Characters O
with O
identical O
meaning O
. O
/uni514E‚ÜîÂÖî(rabbit O
) O
Specialized O
Semantic O
Variant O
Characters O
with O
overlapping O
meaning O
. O
/uni4E3C(rice O
bowl O
, O
well)‚Üî‰∫ï(well O
) O
Table O
2 O
: O
The O
Ô¨Åve O
types O
of O
variants O
in O
the O
Unihan O
database O
. O
external O
word O
segmenter O
would O
introduce O
extra O
segmentation O
errors O
and O
compromise O
the O
performance O
of O
the O
model O
. O
Also O
, O
as O
a O
word O
- O
based O
model O
, O
it O
is O
difÔ¨Åcult O
to O
share O
cross O
- O
lingual O
characters O
unless O
the O
segmented O
words O
in O
both O
Chinese O
and O
Japanese O
are O
exactly O
matched O
. O
Furthermore O
, O
both O
approaches O
would O
enlarge O
the O
vocabulary O
size O
and O
thus O
introduce O
more O
parameters O
. O
2.2 O
Unihan O
Database O
Chinese O
, O
Japanese O
and O
Korean O
( O
CJK O
) O
characters O
share O
a O
common O
origin O
from O
the O
ancient O
Chinese O
characters O
. O
However O
, O
with O
the O
development O
of O
each O
language O
, O
both O
the O
shape O
and O
semantics O
of O
characters O
drastically O
change O
. O
When O
exchanging O
information O
, O
different O
codings O
of O
the O
same O
character O
hinders O
the O
text O
processing O
. O
Thus O
, O
as O
the O
result O
of O
Han O
uniÔ¨Åcation4 O
, O
the O
database O
of O
CJK O
UniÔ¨Åed O
Ideographs O
, O
Unihan O
( O
Jenkins O
et O
al O
. O
, O
2019 O
) O
, O
is O
constructed O
by O
human O
experts O
tracing O
the O
sources O
of O
each O
character O
. O
As O
part O
of O
the O
Unicode O
Standard O
, O
Unihan O
merges O
the O
Unicode O
for O
some O
characters O
from O
different O
languages O
and O
provides O
extra O
variant O
information O
between O
different O
characters O
. O
In O
previous O
studies O
( O
Zhang O
and O
Komachi O
, O
2019 O
; O
Lample O
and O
Conneau O
, O
2019 O
; O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
Unicode O
is O
used O
by O
default O
. O
However O
, O
due O
to O
the O
‚Äú O
Source O
Separation O
Rule O
‚Äù O
of O
Unicode O
, O
to O
remain O
the O
compatibility O
with O
prior O
encoding O
systems O
, O
a O
single O
character O
can O
have O
multiple O
Unicodes O
with O
different O
glyphs O
. O
For O
example O
, O
for O
the O
character O
‚Äú O
Êà∂ O
‚Äù O
, O
there O
are O
three O
unicodes O
: O
U+6236 O
, O
U+6237 O
and O
U+6238 O
. O
This O
feature O
could O
be O
useful O
for O
message O
exchange O
but O
is O
undoubtedly O
undesirable O
for O
NLP O
and O
may O
bring O
the O
problems O
of O
data O
sparsity O
and O
prevent O
the O
alignment O
of O
a O
crosslingual O
language O
model O
. O
Fortunately O
, O
Unihan O
database O
also O
provides O
12,373 O
entries O
of O
variant O
information O
in O
Ô¨Åve O
types O
, O
as O
listed O
in O
Table O
2 O
. O
Note O
that O
one O
character O
may O
have O
multiple O
types O
of O
variants O
and O
each O
type O
may O
4https://en.wikipedia.org/wiki/Han O
_ O
unificationTokenization O
Scheme O
Result O
BERT O
( O
2019 O
) O
/uni53F0 O
/ O
uni98A8//uni306F O
/ O
uni3072 O
/ O
uni3069 O
/ O
uni3044 O
XLM O
( O
2019 O
) O
/uni53F0 O
/ O
uni98A8//uni306F//uni3072 O
/ O
uni3069 O
/ O
uni3044 O
UnihanLM O
/uni53F0//uni98A8//uni306F//uni3072//uni3069//uni3044 O
Table O
3 O
: O
Different O
tokenization O
schemes O
used O
in O
recent O
work O
and O
ours O
. O
Note O
that O
the O
tokenized O
results O
of O
both O
BERT O
and O
XLM O
in O
this O
table O
are O
before O
WordPiece O
/ O
BPE O
applied O
. O
WordPiece O
/ O
BPE O
may O
further O
split O
a O
token O
. O
have O
multiple O
variant O
characters O
( O
e.g. O
, O
the O
traditional O
variants O
of O
‚Äú O
Âèë O
‚Äù O
in O
Table O
2 O
) O
. O
Such O
information O
forms O
a O
complex O
graph O
structure O
. O
3 O
UnihanLM O
In O
this O
section O
, O
we O
introduce O
the O
tokenization O
, O
character O
merging O
and O
training O
procedure O
for O
our O
proposed O
UnihanLM O
. O
3.1 O
Tokenization O
As O
analyzed O
in O
Section O
2.1 O
, O
the O
tokenization O
scheme O
is O
tricky O
and O
critical O
for O
East O
Asian O
languages O
. O
Although O
recent O
work O
( O
Li O
et O
al O
. O
, O
2019 O
) O
reveals O
that O
tokenization O
is O
unnecessary O
for O
most O
highlevel O
NLU O
and O
NLG O
tasks O
, O
many O
downstream O
labeling O
tasks O
( O
e.g. O
, O
Part O
- O
of O
- O
speech O
Tagging O
, O
Named O
Entity O
Recognition O
) O
still O
require O
an O
implicit O
or O
explicit O
segmentation O
. O
To O
enable O
all O
NLP O
tasks O
, O
we O
tokenize O
the O
sentences O
by O
treating O
every O
character O
( O
including O
Japanese O
Kana O
) O
as O
a O
token O
. O
Thus O
, O
our O
model O
is O
capable O
of O
processing O
all O
tasks O
, O
from O
the O
lowest O
- O
level O
Chinese O
and O
Japanese O
word O
segmentation O
to O
high O
- O
level O
NLU O
tasks O
. O
We O
summarize O
the O
different O
tokenization O
schemes O
used O
in O
recent O
work O
and O
ours O
in O
Table O
3 O
. O
We O
do O
not O
further O
apply O
BPE O
to O
our O
tokenized O
sentences O
for O
two O
reasons O
. O
First O
, O
a O
character O
is O
the O
atomic O
element O
in O
both O
Chinese O
and O
Japanese O
grammars O
which O
should O
not O
be O
further O
split O
. O
Second O
, O
character O
itself O
is O
naturally O
a O
sub O
- O
word O
semantic O
element O
, O
e.g. O
, O
‚Äú O
Ëá™ O
‚Äù O
( O
self O
) O
+ O
‚Äú O
‰ø°‚Äù(belief O
) O
= O
‚Äú O
Ëá™‰ø°‚Äù203U+53F0U+6AAFU+81FAU+98B1TraditionalSimpliÔ¨ÅedTraditionalTraditionalTraditionalSemanticU+5113SemanticU+67B1SemanticSimpliÔ¨ÅedSimpliÔ¨Åed O
SemanticFigure O
1 O
: O
A O
connected O
subgraph O
of O
Unihan O
database O
. O
For O
example O
, O
for O
the O
word O
‚Äú O
typhoon O
‚Äù O
, O
‚Äú O
Âè∞ O
‚Äù O
is O
used O
in O
Japanese O
and O
SimpliÔ¨Åed O
Chinese O
while O
‚Äú O
È¢± O
‚Äù O
is O
used O
in O
Traditional O
Chinese O
. O
( O
conÔ¨Ådence O
) O
; O
‚Äú O
Ëá™ O
‚Äù O
( O
self O
) O
+ O
‚Äú O
Â∞ä‚Äù(respect O
) O
= O
‚Äú O
Ëá™Â∞ä O
‚Äù O
( O
self O
- O
esteem O
) O
. O
3.2 O
Character O
Merging O
To O
reduce O
the O
vocabulary O
size O
and O
align O
the O
Chinese O
characters O
in O
Traditional O
Chinese O
, O
SimpliÔ¨Åed O
Chinese O
and O
Japanese O
to O
the O
greatest O
extent O
, O
it O
is O
important O
to O
merge O
as O
many O
characters O
as O
possible O
while O
ensuring O
only O
merging O
characters O
with O
the O
identical O
or O
overlapping O
meanings O
. O
Thus O
, O
we O
use O
Unihan O
database O
, O
which O
includes O
character O
variant O
information O
collected O
and O
approved O
by O
human O
experts O
. O
We O
use O
four O
types O
of O
variants O
including O
Traditional O
Variant O
, O
SimpliÔ¨Åed O
Variant O
, O
Z O
- O
Variant O
and O
Semantic O
Variant O
. O
Note O
that O
we O
exclude O
Specialized O
Semantic O
Variant O
which O
may O
raise O
ambiguity O
problem O
since O
it O
is O
not O
very O
common O
and O
the O
semantics O
of O
the O
two O
characters O
are O
merely O
overlapping O
, O
not O
identical O
. O
However O
, O
merging O
characters O
is O
still O
challenging O
since O
the O
variant O
information O
in O
Unihan O
database O
is O
a O
complex O
graph O
, O
as O
illustrated O
in O
Figure O
1 O
. O
To O
merge O
the O
characters O
as O
much O
as O
possible O
, O
we O
convert O
Unihan O
database O
to O
a O
large O
undirected O
graph O
and O
use O
Union O
Find O
Algorithm O
( O
Galler O
and O
Fischer O
, O
1964 O
) O
to O
Ô¨Ånd O
all O
maximal O
connected O
subgraph O
. O
For O
example O
, O
the O
whole O
Figure O
1 O
is O
a O
subgraph O
in O
the O
Unihan O
graph O
found O
by O
the O
algorithm O
. O
We O
call O
all O
characters O
in O
a O
maximal O
connected O
subgraph O
belong O
to O
a O
‚Äú O
cluster O
‚Äù O
. O
After O
this O
merging O
procedure O
, O
the O
12,373 O
variant O
entries O
yield O
a O
total O
of O
4,001 O
clusters O
. O
3.3 O
Training O
Procedure O
As O
illustrated O
in O
Figure O
2 O
, O
the O
model O
is O
a O
Transformer O
based O
model O
with O
three O
embeddings O
as O
inTransformerPositionEmbedding>V@>0$6.@ŒÖ>0$6.@ÕøÕö#1Œà O
-$-$-$-$-$-$-$++++++++++++++LanguageEmbedding O
CharacterEmbeddingClusterEmbedding>V@-$++ O
TransformerPositionEmbedding>V@›£>0$6.@ŒÖŒà>0$6.@ÕöÊ∞±Õø O
-$-$-$-$-$-$-$++++++++++++++LanguageEmbedding>V@-$++ O
 O
 O
 O
&OXVWHUOHYHO3UHWUDLQLQJ O
 O
 O
 O
&KDUDFWHUOHYHO3UHWUDLQLQJCharacter O
EmbeddingInitialization#1›£Figure O
2 O
: O
The O
model O
architecture O
of O
UnihanLM O
. O
( O
1 O
) O
We O
merge O
characters O
to O
clusters O
and O
use O
cluster O
indices O
when O
doing O
cluster O
- O
level O
pretraining O
. O
In O
the O
Ô¨Ågure O
, O
‚Äú O
# O
1 O
‚Äù O
and O
‚Äú O
# O
2 O
‚Äù O
indicate O
indices O
of O
the O
clusters O
which O
‚Äú O
Âè∞ O
‚Äù O
and O
‚Äú O
È¢® O
‚Äù O
belong O
to O
, O
respectively O
. O
( O
2 O
) O
We O
initialize O
the O
embedding O
of O
each O
character O
in O
a O
cluster O
with O
the O
cluster O
embedding O
and O
do O
character O
- O
level O
pretraining O
to O
predict O
each O
character O
. O
put O
and O
the O
training O
procedure O
is O
composed O
of O
two O
phases O
. O
3.3.1 O
Model O
Our O
model O
is O
a O
Transformer O
- O
based O
Masked O
Language O
Model O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
which O
learns O
to O
predict O
the O
randomly O
masked O
words O
with O
the O
context O
. O
Also O
, O
following O
( O
Lample O
and O
Conneau O
, O
2019 O
) O
, O
we O
add O
language O
embedding O
to O
help O
the O
model O
distinguish O
between O
Chinese O
and O
Japanese O
, O
especially O
when O
we O
share O
the O
characters O
between O
these O
two O
languages O
. O
The O
detailed O
hyperparameter O
settings O
are O
described O
in O
Section O
4.1 O
. O
3.3.2 O
Coarse O
- O
grained O
Cluster O
- O
level O
Pretraining O
To O
maximize O
the O
shared O
lexicon O
and O
force O
them O
to O
share O
a O
representation O
, O
we O
leverage O
clusters O
to O
pretrain O
our O
models O
on O
a O
coarse O
- O
grained O
cluster O
level.204We O
Ô¨Årst O
append O
the O
cluster O
indices O
to O
the O
vocabulary O
. O
During O
cluster O
- O
level O
pretraining O
, O
we O
substitute O
the O
character O
index O
with O
its O
corresponding O
cluster O
index O
if O
the O
character O
is O
in O
the O
Unihan O
database O
. O
For O
Japanese O
kanas O
, O
punctuation O
, O
number O
and O
other O
characters O
not O
in O
Unihan O
database O
, O
we O
keep O
its O
original O
token O
index O
. O
In O
this O
way O
, O
we O
employ O
human O
prior O
knowledge O
to O
the O
pretraining O
procedure O
and O
allow O
the O
model O
to O
roughly O
model O
the O
semantic O
knowledge O
. O
3.3.3 O
Fine O
- O
grained O
Character O
- O
level O
Pretraining O
Although O
the O
clusters O
training O
is O
effective O
, O
there O
are O
two O
problems O
remaining O
unsolved O
. O
First O
, O
Traditional O
Variant O
could O
be O
ambiguous O
. O
As O
shown O
in O
Table O
2 O
, O
a O
character O
( O
most O
likely O
one O
used O
in O
SimpliÔ¨Åed O
Chinese O
) O
may O
have O
multiple O
Traditional O
Variants O
. O
Although O
it O
should O
not O
have O
a O
signiÔ¨Åcant O
negative O
effect O
for O
understanding O
the O
language O
( O
since O
a O
SimpliÔ¨Åed O
Chinese O
user O
can O
disambiguate O
between O
different O
meanings O
of O
a O
character O
based O
on O
its O
context O
) O
, O
it O
still O
makes O
sense O
to O
improve O
the O
overall O
performance O
by O
distinguish O
the O
characters O
explicitly O
( O
Navigli O
et O
al O
. O
, O
2017 O
) O
. O
Also O
, O
in O
tasks O
involving O
decoding O
( O
e.g. O
, O
machine O
translation O
) O
, O
they O
must O
be O
processed O
independently O
. O
Thus O
, O
character O
disambiguation O
can O
be O
naturally O
used O
as O
a O
selfsupervised O
task O
. O
Second O
, O
when O
using O
the O
trained O
model O
for O
translation O
, O
it O
would O
be O
important O
for O
the O
model O
to O
decode O
the O
right O
character O
for O
different O
languages O
and O
writing O
systems O
. O
For O
example O
, O
for O
the O
word O
meaning O
‚Äú O
typhoon O
‚Äù O
, O
‚Äú O
/uni53F0 O
/ O
uni98A8 O
‚Äù O
, O
‚Äú O
È¢±È¢® O
‚Äù O
, O
‚Äú O
Âè∞ O
È£é O
‚Äù O
should O
be O
used O
in O
Japanese O
, O
Traditional O
Chinese O
and O
SimpliÔ¨Åed O
Chinese O
, O
respectively O
. O
Consequently O
, O
we O
leave O
these O
nuances O
of O
characters O
to O
a O
Ô¨Åne O
- O
grained O
character O
- O
level O
pretraining O
. O
Since O
during O
the O
cluster O
- O
level O
pretraining O
, O
all O
characters O
in O
Unihan O
database O
are O
preserved O
in O
the O
vocabulary O
but O
their O
embedding O
is O
untrained O
, O
we O
initialize O
their O
embedding O
with O
their O
corresponding O
cluster O
embedding O
trained O
in O
cluster O
- O
level O
pretraining O
stage O
. O
In O
the O
character O
- O
level O
pretraining O
stage O
, O
we O
discard O
the O
clusters O
in O
the O
vocabulary O
and O
do O
not O
substitute O
any O
character O
since O
then O
. O
In O
this O
way O
, O
the O
model O
can O
handle O
each O
character O
case O
by O
case O
, O
with O
a O
Ô¨Åne O
granularity O
. O
We O
restart O
the O
training O
with O
a O
smaller O
learning O
rate O
to O
allow O
the O
model O
to O
learn O
to O
disambiguate O
. O
Model O
# O
Layer O
# O
Param O
. O
BERT O
- O
Mono O
- O
ZH O
( O
2019 O
) O
12 O
110 O
M O
mBERT O
( O
2019 O
) O
12 O
179 O
M O
XLM O
( O
2019 O
) O
16 O
571 O
M O
UnihanLM O
12 O
176 O
M O
Table O
4 O
: O
The O
numbers O
of O
layers O
and O
parameters O
for O
different O
models O
. O
4 O
Experiments O
In O
this O
section O
, O
we O
compare O
UnihanLM O
with O
other O
self O
- O
supervised O
pretrained O
language O
models O
. O
All O
of O
our O
baselines O
( O
monolingual O
BERT O
, O
mBERT O
and O
XLM O
) O
use O
Wikipedia O
for O
self O
- O
supervised O
pretraining O
. O
Note O
that O
we O
do O
not O
compare O
our O
model O
to O
XLM O
- O
R O
( O
Conneau O
et O
al O
. O
, O
2019 O
) O
and O
Unicoder O
( O
Huang O
et O
al O
. O
, O
2019 O
) O
since O
they O
are O
trained O
with O
much O
more O
data O
and O
even O
on O
supervised O
tasks O
. O
4.1 O
Training O
Details O
We O
use O
the O
mixture O
of O
Chinese O
and O
Japanese O
Wikipedia5as O
the O
unparalleled O
pretraining O
corpus O
. O
We O
sample O
5,000sentences O
as O
validation O
set O
for O
model O
selection O
and O
use O
the O
rest O
for O
training O
. O
Our O
model O
uses O
12 O
layers O
of O
Transformer O
blocks O
with O
16 O
attention O
heads O
. O
The O
hidden O
size O
is O
set O
to O
1,024 O
. O
The O
vocabulary O
size O
is O
24,044 O
. O
Shown O
in O
Table O
4 O
, O
our O
model O
has O
a O
similar O
size O
to O
mBERT O
. O
We O
train O
our O
model O
on O
8 O
Nvidia O
V100 O
32 O
GB O
GPUs O
to O
optimize O
Masked O
Language O
Model O
( O
MLM O
) O
objective O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
with O
an O
Adam O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
optimizer O
. O
The O
masking O
probability O
is O
set O
to O
15 O
% O
. O
We O
add O
a O
L2 O
regularization O
of O
0.01 O
. O
We O
warm O
up O
the O
Ô¨Årst O
30,000 O
steps O
for O
each O
stage O
of O
pretraining O
by O
an O
inverse O
square O
root O
function O
. O
The O
batch O
size O
is O
set O
to O
64 O
per O
GPU O
. O
The O
maximum O
sequence O
length O
is O
limited O
to O
256tokens O
. O
We O
add O
dropout O
( O
Srivastava O
et O
al O
. O
, O
2014 O
) O
for O
both O
feedforward O
network O
and O
attention O
with O
a O
drop O
rate O
of O
0.1 O
. O
The O
learning O
rate O
for O
cluster O
- O
level O
pretraining O
is O
set O
to O
1√ó10‚àí4 O
. O
After O
264 O
hours O
of O
clusterlevel O
pretraining O
until O
convergence O
, O
we O
perform O
character O
- O
level O
pretraining O
with O
a O
smaller O
learning O
rate O
of O
5√ó10‚àí5for O
another O
43hours O
. O
We O
choose O
the O
best O
model O
according O
to O
its O
perplexity O
on O
validation O
set O
. O
For O
downstream O
tasks O
( O
to O
be O
detailed O
shortly O
) O
, O
we O
Ô¨Åne O
- O
tune O
UnihanLM O
with O
a O
learning O
rate O
of O
5√ó10‚àí7,1√ó10‚àí4,2.5√ó10‚àí5and O
a O
batch O
5https://dumps.wikimedia.org/205Method O
PKU O
( O
ZH O
) O
KWDLC O
( O
JA O
) O
Standard O
training O
mBERT O
( O
2019 O
) O
95.0 O
96.3 O
BERT O
- O
Mono O
- O
ZH O
( O
2019 O
) O
96.5 O
UnihanLM O
96.6 O
98.2 O
Cross O
- O
lingual O
zero O
- O
shot O
transfer O
mBERT O
( O
2019 O
) O
82.0 O
63.1 O
UnihanLM O
85.7 O
74.1 O
Table O
5 O
: O
F1 O
scores O
on O
Chinese O
Word O
Segmentation O
( O
CWS O
) O
and O
Japanese O
Word O
Segmentation O
( O
JWS O
) O
tasks O
. O
‚Äú O
Cross O
- O
lingual O
zero O
- O
shot O
transfer O
‚Äù O
indicates O
that O
the O
model O
is O
trained O
on O
CWS O
and O
zero O
- O
shot O
tested O
on O
JWS O
, O
vice O
versa O
. O
size O
of O
20,24,16for O
word O
segmentation O
, O
unsupervised O
machine O
translation O
and O
classiÔ¨Åcation O
tasks O
, O
respectively O
. O
4.2 O
Word O
Segmentation O
Word O
segmentation O
is O
a O
fundamental O
task O
in O
both O
Chinese O
and O
Japanese O
NLP O
. O
It O
is O
often O
recognized O
as O
the O
Ô¨Årst O
step O
for O
further O
processing O
in O
many O
systems O
. O
Thus O
, O
we O
evaluate O
Chinese O
Word O
Segmentation O
( O
CWS O
) O
and O
Japanese O
Word O
Segmentation O
( O
JWS O
) O
on O
PKU O
dataset O
( O
Emerson O
, O
2005 O
) O
and O
KWDLC O
( O
Kawahara O
et O
al O
. O
, O
2014 O
) O
. O
We O
use O
Multilingual O
BERT O
and O
monolingual O
Chinese O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
as O
baselines O
. O
We O
use O
pretrained O
checkpoints O
provided O
by O
Google6 O
. O
Following O
previous O
work O
, O
we O
treat O
the O
word O
segmentation O
task O
as O
a O
sequence O
labeling O
task O
. O
Note O
that O
XLM O
( O
Lample O
and O
Conneau O
, O
2019 O
) O
uses O
pre O
- O
segmented O
sentences O
as O
input O
, O
making O
it O
inapplicable O
for O
this O
task O
. O
As O
shown O
in O
Table O
5 O
, O
our O
proposed O
UnihanLM O
outperforms O
mBERT O
and O
monolingual O
BERT O
by O
1.6and0.1 O
in O
terms O
of O
F1 O
score O
on O
CWS O
, O
respectively O
. O
On O
JWS O
, O
our O
model O
outperforms O
mBERT O
by O
1.9on O
F1 O
. O
Additionally O
, O
we O
conduct O
zero O
- O
shot O
transfer O
experiments O
to O
determine O
how O
much O
lexical O
knowledge O
is O
shared O
within O
Chinese O
and O
Japanese O
for O
each O
model O
. O
We O
use O
the O
weights O
trained O
on O
CWS O
and O
JWS O
for O
zero O
- O
shot O
transferring O
on O
the O
other O
language O
. O
Our O
model O
drastically O
outperforms O
mBERT O
on O
this O
task O
by O
3.7and O
11.0on O
CWS O
and O
JWS O
, O
respectively O
. O
This O
proves O
that O
our O
model O
can O
better O
capture O
the O
lexical O
knowledge O
shared O
between O
Chinese O
and O
Japanese O
. O
Also O
, O
it O
is O
notable O
that O
zero O
- O
shot O
JWS O
has O
a O
prominently O
poorer O
performance O
than O
zero O
- O
shot O
CWS O
. O
As O
we O
6https://github.com/google-research/ O
bertMethod O
ZH‚ÜíJA O
JA‚ÜíZH O
Supervised O
baseline O
OpenNMT O
( O
Klein O
et O
al O
. O
, O
2017 O
) O
42.12 O
40.63 O
Fine O
- O
tuned O
on O
Wikipedia O
XLM O
( O
Lample O
and O
Conneau O
, O
2019 O
) O
14.58 O
15.06 O
UnihanLM O
33.53 O
28.70 O
Fine O
- O
tuned O
on O
shufÔ¨Çed O
ASPEC O
- O
JC O
training O
set O
Stroke O
( O
Zhang O
and O
Komachi O
, O
2019 O
) O
33.81 O
31.66 O
UnihanLM O
44.59 O
40.58 O
Table O
6 O
: O
BLEU O
scores O
of O
Chinese O
- O
Japanese O
unsupervised O
translation O
on O
ASPEC O
- O
JC O
dataset O
. O
analyze O
, O
the O
criterion O
for O
segmenting O
Chinese O
characters O
can O
be O
learned O
with O
a O
Japanese O
corpus O
and O
then O
transferred O
to O
CWS O
. O
However O
, O
since O
no O
kana O
is O
present O
in O
CWS O
, O
the O
model O
can O
not O
successfully O
segment O
kanas O
, O
when O
performing O
zero O
- O
shot O
inference O
on O
JWS O
. O
4.3 O
Unsupervised O
Machine O
Translation O
A O
Chinese O
speaker O
who O
never O
learned O
Japanese O
can O
roughly O
understand O
a O
Japanese O
text O
( O
and O
vice O
versa O
) O
, O
due O
to O
the O
similarity O
between O
the O
writing O
systems O
of O
these O
two O
languages O
. O
On O
the O
other O
hand O
, O
only O
a O
few O
parallel O
corpora O
between O
Chinese O
and O
Japanese O
are O
publicly O
available O
, O
and O
they O
are O
usually O
small O
in O
size O
. O
Thus O
, O
Unsupervised O
Machine O
Translation O
( O
UMT O
) O
is O
very O
promising O
and O
meaningful O
on O
the O
Chinese O
- O
Japanese O
translation O
task O
. O
We O
evaluate O
on O
Asian O
ScientiÔ¨Åc O
Paper O
Excerpt O
Corpus O
Japanese O
- O
Chinese O
( O
ASPEC O
- O
JC)7 O
, O
the O
most O
widely O
- O
used O
Chinese O
- O
Japanese O
Machine O
Translation O
dataset O
. O
We O
perform O
our O
experiments O
under O
two O
settings O
: O
( O
1 O
) O
Chinese O
and O
Japanese O
Wikipedia O
is O
used O
as O
the O
monolingual O
corpora O
, O
following O
the O
setting O
of O
( O
Lample O
and O
Conneau O
, O
2019 O
) O
. O
( O
2 O
) O
ShufÔ¨Çed O
unparalleled O
ASPEC O
- O
JC O
training O
set O
is O
used O
as O
the O
monolingual O
corpora O
, O
following O
the O
settings O
in O
( O
Zhang O
and O
Komachi O
, O
2019 O
) O
. O
Except O
for O
XLM O
, O
we O
choose O
( O
Zhang O
and O
Komachi O
, O
2019 O
) O
, O
the O
current O
state O
- O
of O
- O
the O
- O
art O
ChineseJapanese O
UMT O
model O
as O
a O
strong O
baseline O
. O
They O
decomposed O
a O
Chinese O
character O
in O
both O
Chinese O
and O
Japanese O
into O
strokes O
and O
then O
learn O
a O
shared O
token O
in O
the O
stroke O
sequence O
to O
increase O
the O
shared O
tokens O
in O
the O
vocabulary O
. O
However O
, O
this O
method O
relies O
on O
an O
unsupervised O
BPE O
( O
Sennrich O
et O
al O
. O
, O
2016 O
) O
to O
learn O
shared O
stroke O
tokens O
from O
a O
long O
noisy O
7http://orchid.kuee.kyoto-u.ac.jp/ O
ASPEC/206MethodPAWS O
- O
X O
ZH O
JA O
BOW O
54.5 O
55.1 O
ESIM O
( O
Chen O
et O
al O
. O
, O
2017a O
) O
60.3 O
59.6 O
mBERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
82.3 O
79.2 O
XLM O
( O
Lample O
and O
Conneau O
, O
2019 O
) O
82.5 O
79.5 O
UnihanLM O
82.7 O
80.5 O
Table O
7 O
: O
Accuracy O
scores O
on O
PAWS O
- O
X O
dataset O
. O
stroke O
sequence O
, O
which O
is O
rather O
unreliable O
compared O
to O
our O
solution O
. O
For O
example O
, O
‚Äú O
‰∏ë O
‚Äù O
( O
ugly O
) O
and O
‚Äú O
‰∫î O
‚Äù O
( O
Ô¨Åve O
) O
have O
a O
very O
similar O
stroke O
sequence O
but O
completely O
different O
meanings O
. O
Following O
( O
Lample O
and O
Conneau O
, O
2019 O
) O
, O
we O
use O
our O
pretrained O
weights O
to O
initialize O
the O
translation O
model O
and O
train O
the O
model O
with O
denoising O
auto O
- O
encoding O
loss O
and O
online O
back O
- O
translation O
loss O
. O
Note O
that O
both O
baselines O
use O
Wikipedia O
as O
the O
unsupervised O
data O
and O
are O
based O
on O
the O
same O
UMT O
method O
( O
Lample O
et O
al O
. O
, O
2018c O
) O
. O
We O
use O
character O
- O
level O
BLEU O
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
as O
the O
evaluation O
metric O
. O
We O
demonstrate O
the O
results O
in O
Table O
6 O
. O
As O
we O
analyzed O
, O
XLM O
suffers O
from O
a O
severe O
out O
- O
ofvocabulary O
( O
OOV O
) O
problem O
on O
AESPEC O
- O
JC O
, O
a O
dataset O
composed O
of O
scientiÔ¨Åc O
papers O
, O
containing O
many O
new O
terminologies O
which O
do O
not O
show O
up O
in O
the O
pretraining O
corpus O
of O
XLM O
. O
As O
a O
word O
- O
based O
model O
, O
XLM O
is O
not O
able O
to O
handle O
these O
new O
words O
and O
thus O
yields O
a O
rather O
poor O
result O
. O
When O
Ô¨Ånetuned O
on O
unparalleled O
training O
set O
of O
ASPEC O
- O
JC O
, O
our O
model O
outperforms O
the O
previous O
state O
- O
of O
- O
the O
- O
art O
model O
( O
Zhang O
and O
Komachi O
, O
2019 O
) O
by O
a O
large O
margin O
of O
10.78and8.92 O
in O
terms O
of O
BLEU O
. O
Also O
notably O
, O
UnihanLM O
even O
outperforms O
the O
supervised O
baseline O
on O
Chinese O
- O
to O
- O
Japanese O
translation O
and O
has O
a O
performance O
in O
close O
proximity O
on O
Japaneseto O
- O
Chinese O
task O
, O
compared O
to O
an O
early O
supervised O
machine O
translation O
model O
, O
OpenNMT O
( O
Klein O
et O
al O
. O
, O
2017 O
) O
, O
trained O
on O
the O
paired O
training O
set O
of O
ASPECJC O
. O
4.4 O
Text O
ClassiÔ¨Åcation O
To O
further O
evaluate O
our O
model O
, O
we O
perform O
our O
experiments O
on O
Cross O
- O
lingual O
Paraphrase O
Aversaries O
from O
Word O
Scrambling O
( O
PAWS O
- O
X O
) O
( O
Yang O
et O
al O
. O
, O
2019b O
) O
, O
a O
newly O
proposed O
cross O
- O
lingual O
text O
classiÔ¨Åcation O
dataset O
supporting O
seven O
languages O
including O
Chinese O
and O
Japanese O
. O
This O
dataset O
consists O
of O
challenging O
English O
paraphrase O
identiÔ¨Åcation O
pairs O
from O
Wikipedia O
and O
Quora O
. O
Then O
the O
human O
trans O
- O
lators O
translate O
the O
text O
into O
the O
other O
six O
languages O
. O
We O
test O
under O
the O
setting O
of O
TRANSLATE O
- O
TRAIN O
( O
i.e. O
, O
we O
use O
the O
provided O
translation O
of O
the O
training O
set O
for O
both O
Chinese O
and O
Japanese O
and O
test O
in O
the O
same O
language O
) O
. O
Shown O
in O
Table O
7 O
, O
UnihanLM O
outperforms O
all O
baselines O
in O
( O
Yang O
et O
al O
. O
, O
2019b O
) O
, O
including O
mBERT O
. O
4.5 O
Ablation O
Study O
To O
verify O
the O
effectiveness O
of O
our O
two O
- O
stage O
pretraining O
procedure O
, O
we O
conduct O
an O
ablation O
study O
. O
A O
character O
- O
level O
model O
is O
trained O
from O
scratch O
without O
the O
cluster O
- O
level O
pretraining O
and O
marked O
as O
‚Äú O
‚àícluster O
‚Äù O
. O
On O
the O
other O
hand O
, O
we O
use O
the O
model O
trained O
in O
cluster O
- O
level O
stage O
for O
downstream O
tasks O
and O
mark O
it O
as O
‚Äú O
‚àícharacter O
‚Äù O
. O
Note O
that O
since O
the O
objective O
for O
cluster O
- O
level O
stage O
is O
to O
predict O
the O
masked O
cluster O
, O
it O
can O
not O
be O
used O
for O
unsupervised O
translation O
. O
Shown O
in O
Figure O
8 O
, O
both O
cluster O
- O
level O
and O
character O
- O
level O
pretraining O
play O
an O
essential O
role O
on O
classiÔ¨Åcation O
tasks O
. O
On O
translation O
task O
, O
cluster O
- O
level O
pretraining O
is O
more O
important O
when O
Ô¨Åne O
- O
tuned O
on O
Wikipedia O
but O
has O
a O
relatively O
smaller O
impact O
when O
using O
shufÔ¨Çed O
ASPEC O
- O
JC O
training O
set O
. O
To O
analyze O
the O
success O
of O
our O
two O
- O
stage O
training O
strategy O
, O
we O
would O
like O
to O
emphasize O
two O
strengths O
. O
First O
, O
as O
mentioned O
before O
, O
our O
easy O
- O
to O
- O
hard O
training O
procedure O
matches O
the O
core O
idea O
of O
Curriculum O
Learning O
( O
Bengio O
et O
al O
. O
, O
2009 O
) O
, O
which O
smooths O
the O
training O
and O
help O
the O
model O
generalize O
better O
. O
Second O
, O
the O
two O
- O
stage O
procedure O
inherently O
introduces O
a O
new O
self O
- O
supervised O
task O
, O
which O
could O
take O
the O
advantage O
of O
Multitask O
Learning O
( O
Caruana O
, O
1993 O
) O
. O
5 O
Related O
Work O
Multilingual O
Representation O
Learning O
Learning O
cross O
- O
lingual O
representations O
are O
useful O
for O
downstream O
tasks O
such O
as O
cross O
- O
lingual O
classiÔ¨Åcation O
( O
Conneau O
et O
al O
. O
, O
2018 O
; O
Yang O
et O
al O
. O
, O
2019b O
) O
, O
cross O
- O
lingual O
retrieval O
( O
Zweigenbaum O
et O
al O
. O
, O
2017 O
; O
Artetxe O
and O
Schwenk O
, O
2019 O
) O
and O
cross O
- O
lingual O
QA O
( O
Artetxe O
et O
al O
. O
, O
2019 O
; O
Lewis O
et O
al O
. O
, O
2019 O
; O
Clark O
et O
al O
. O
, O
2020 O
) O
. O
Earlier O
work O
on O
multilingual O
representations O
exploiting O
parallel O
corpora O
( O
Luong O
et O
al O
. O
, O
2015 O
; O
Gouws O
et O
al O
. O
, O
2015 O
) O
or O
a O
bilingual O
dictionary O
to O
learn O
a O
linear O
mapping O
( O
Mikolov O
et O
al O
. O
, O
2013 O
; O
Faruqui O
and O
Dyer O
, O
2014 O
) O
. O
Subsequent O
methods O
explored O
self O
- O
training O
( O
Artetxe O
et O
al O
. O
, O
2017 O
) O
and O
unsupervised O
learning O
( O
Zhang O
et O
al O
. O
, O
2017 O
; O
Artetxe O
et O
al O
. O
, O
2018 O
; O
Lample O
et O
al O
. O
, O
2018b O
) O
. O
Recently O
, O
multilingual O
pretrained O
encoders O
have O
shown O
its O
effec-207MethodPAWS O
- O
XASPEC O
- O
JC O
Wiki O
ShufÔ¨Çed O
- O
train O
ZH O
JA O
ZH‚ÜíJA O
JA‚ÜíZH O
ZH‚ÜíJA O
JA‚ÜíZH O
UnihanLM O
82.7 O
80.5 O
33.53 O
28.70 O
44.59 O
40.58 O
‚àícluster O
81.5 O
79.2 O
29.33 O
20.93 O
42.34 O
39.24 O
‚àícharacter O
82.0 O
80.1 O
- O
- O
- O
Table O
8 O
: O
The O
results O
of O
ablation O
study O
on O
text O
classiÔ¨Åcation O
and O
UMT O
. O
‚Äú O
-cluster O
‚Äù O
and O
‚Äú O
-character O
‚Äù O
indicate O
the O
model O
trained O
without O
the O
cluster O
- O
level O
pretraining O
and O
character O
- O
level O
pretraining O
, O
respectively O
. O
The O
metrics O
for O
PAWS O
- O
X O
and O
ASPEC O
- O
JC O
are O
accuracy O
and O
BLEU O
, O
respectively O
. O
tiveness O
for O
learning O
deep O
cross O
- O
lingual O
representations O
( O
Eriguchi O
et O
al O
. O
, O
2018 O
; O
Pires O
et O
al O
. O
, O
2019 O
; O
Wu O
and O
Dredze O
, O
2019 O
; O
Lample O
and O
Conneau O
, O
2019 O
; O
Conneau O
et O
al O
. O
, O
2019 O
; O
Huang O
et O
al O
. O
, O
2019 O
) O
. O
Word O
Segmentation O
Word O
segmentation O
is O
often O
formalized O
as O
a O
sequence O
tagging O
task O
. O
It O
requires O
lexical O
knowledge O
to O
split O
a O
character O
sequence O
into O
a O
word O
list O
that O
can O
be O
used O
for O
downstream O
tasks O
. O
This O
step O
is O
necessary O
for O
many O
earlier O
NLP O
systems O
for O
Chinese O
and O
Japanese O
. O
Recent O
work O
on O
Chinese O
Word O
Segmentation O
( O
Wang O
and O
Xu O
, O
2017 O
; O
Zhou O
et O
al O
. O
, O
2017 O
; O
Yang O
et O
al O
. O
, O
2017 O
; O
Cai O
et O
al O
. O
, O
2017 O
; O
Chen O
et O
al O
. O
, O
2017b O
; O
Yang O
et O
al O
. O
, O
2019a O
) O
and O
Japanese O
Word O
Segmentation O
( O
Kaji O
and O
Kitsuregawa O
, O
2014 O
; O
Fujinuma O
and O
II O
, O
2017 O
; O
Kitagawa O
and O
Komachi O
, O
2018 O
) O
exploit O
deep O
neural O
networks O
and O
focus O
on O
building O
end O
- O
to O
- O
end O
sequence O
tagging O
models O
. O
Unsupervised O
Machine O
Translation O
Recently O
, O
machine O
translation O
systems O
have O
demonstrated O
near O
human O
- O
level O
performance O
on O
some O
languages O
. O
However O
, O
it O
depends O
on O
the O
availability O
of O
large O
amounts O
of O
parallel O
sentences O
. O
Unsupervised O
Machine O
Translation O
addresses O
this O
problem O
by O
exploiting O
monolingual O
corpora O
which O
can O
be O
easily O
constructed O
. O
Lample O
et O
al O
. O
( O
2018a O
) O
proposed O
a O
UMT O
model O
by O
learning O
to O
reconstruct O
in O
both O
languages O
from O
a O
shared O
feature O
space O
. O
Lample O
et O
al O
. O
( O
2018c O
) O
exploited O
language O
modeling O
and O
back O
- O
translation O
and O
thus O
proposed O
a O
neural O
unsupervised O
translation O
model O
and O
a O
phase O
- O
based O
translation O
model O
. O
Different O
from O
European O
languages O
( O
e.g. O
, O
English O
) O
, O
Chinese O
and O
Japanese O
naturally O
share O
Chinese O
characters O
. O
Zhang O
and O
Komachi O
( O
2019 O
) O
exploited O
such O
information O
by O
learning O
a O
BPE O
representation O
over O
sub O
- O
character O
( O
i.e. O
, O
ideograph O
and O
stroke O
) O
sequence O
. O
They O
applied O
this O
technique O
to O
unsupervised O
Chinese O
- O
Japanese O
machine O
translation O
and O
achieved O
state O
- O
of O
- O
the O
- O
art O
performance O
. O
This O
information O
is O
also O
shown O
to O
beeffective O
by O
( O
Xu O
et O
al O
. O
, O
2019 O
) O
. O
6 O
Discussion O
and O
Future O
Work O
There O
is O
still O
space O
to O
improve O
for O
our O
method O
. O
First O
, O
as O
we O
analyze O
, O
except O
for O
Chinese O
characters O
, O
English O
words O
often O
appear O
in O
both O
Chinese O
and O
Japanese O
texts O
. O
In O
our O
current O
model O
, O
they O
are O
treated O
as O
normal O
characters O
without O
any O
special O
processing O
. O
However O
, O
such O
a O
rough O
processing O
may O
harm O
the O
performance O
of O
the O
model O
on O
some O
tasks O
. O
For O
example O
, O
in O
PAWS O
- O
X O
, O
many O
entities O
remain O
untranslated O
and O
this O
may O
have O
a O
negative O
effect O
on O
the O
performance O
of O
our O
model O
. O
Also O
, O
loan O
words O
( O
i.e. O
, O
Gairaigo O
) O
, O
especially O
from O
English O
, O
constitute O
a O
large O
part O
of O
nouns O
in O
modern O
Japanese O
( O
Miller O
, O
1998 O
) O
. O
These O
words O
are O
written O
with O
kanas O
, O
instead O
of O
Chinese O
characters O
which O
makes O
it O
inapplicable O
to O
be O
shared O
with O
our O
approach O
. O
Thus O
, O
it O
may O
be O
reasonable O
to O
involve O
English O
in O
cross O
- O
lingual O
modeling O
of O
Asian O
languages O
, O
as O
well O
. O
Similarly O
, O
Chinese O
characters O
exist O
in O
Korean O
and O
Vietnamese O
but O
are O
now O
written O
in O
Hangul O
( O
Korean O
alphabet O
) O
and O
Vietnamese O
alphabet O
, O
respectively O
. O
Our O
future O
work O
will O
explore O
the O
possibility O
to O
generalize O
the O
idea O
to O
more O
Asian O
languages O
including O
Korean O
and O
Vietnamese O
. O
7 O
Conclusion O
In O
this O
paper O
, O
we O
exploit O
the O
ready O
- O
made O
Unihan O
database O
constructed O
by O
linguistic O
experts O
and O
propose O
a O
novel O
Chinese O
- O
Japanese O
cross O
- O
lingual O
language O
model O
trained O
by O
a O
two O
- O
stage O
coarse O
- O
to-Ô¨Åne O
procedure O
. O
Our O
extensive O
experiments O
on O
word O
segmentation O
, O
unsupervised O
machine O
translation O
and O
text O
classiÔ¨Åcation O
verify O
the O
effectiveness O
of O
our O
model O
. O
Our O
approach O
sheds O
some O
light O
on O
the O
linguistic O
features O
that O
receive O
insufÔ¨Åcient O
attention O
recently O
and O
showcases O
a O
novel O
way O
to O
fuse O
human O
linguistic O
knowledge O
and O
exploit O
the O
similarity O
between O
two O
languages.208Acknowledgments O
We O
are O
grateful O
for O
the O
insightful O
comments O
from O
the O
anonymous O
reviewers O
. O
We O
would O
like O
to O
thank O
Longtu O
Zhang O
and O
Mamoru O
Komachi O
from O
Tokyo O
Metropolitan O
University O
for O
their O
help O
with O
the O
MT O
baseline O
. O
Abstract O
In O
order O
to O
combat O
overÔ¨Åtting O
and O
in O
pursuit O
of O
better O
generalization O
, O
label O
smoothing O
is O
widely O
applied O
in O
modern O
neural O
machine O
translation O
systems O
. O
The O
core O
idea O
is O
to O
penalize O
over O
- O
conÔ¨Ådent O
outputs O
and O
regularize O
the O
model O
so O
that O
its O
outputs O
do O
not O
diverge O
too O
much O
from O
some O
prior O
distribution O
. O
While O
training O
perplexity O
generally O
gets O
worse O
, O
label O
smoothing O
is O
found O
to O
consistently O
improve O
test O
performance O
. O
In O
this O
work O
, O
we O
aim O
to O
better O
understand O
label O
smoothing O
in O
the O
context O
of O
neural O
machine O
translation O
. O
Theoretically O
, O
we O
derive O
and O
explain O
exactly O
what O
label O
smoothing O
is O
optimizing O
for O
. O
Practically O
, O
we O
conduct O
extensive O
experiments O
by O
varying O
which O
tokens O
to O
smooth O
, O
tuning O
the O
probability O
mass O
to O
be O
deducted O
from O
the O
true O
targets O
and O
considering O
different O
prior O
distributions O
. O
We O
show O
that O
label O
smoothing O
is O
theoretically O
wellmotivated O
, O
and O
by O
carefully O
choosing O
hyperparameters O
, O
the O
practical O
performance O
of O
strong O
neural O
machine O
translation O
systems O
can O
be O
further O
improved O
. O
1 O
Introduction O
In O
recent O
years O
, O
Neural O
Network O
( O
NN O
) O
models O
bring O
steady O
and O
concrete O
improvements O
on O
the O
task O
of O
Machine O
Translation O
( O
MT O
) O
. O
From O
the O
introduction O
of O
sequence O
- O
to O
- O
sequence O
models O
( O
Cho O
et O
al O
. O
, O
2014 O
; O
Sutskever O
et O
al O
. O
, O
2014a O
) O
, O
to O
the O
invention O
of O
the O
attention O
mechanism O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
; O
Luong O
et O
al O
. O
, O
2015 O
) O
, O
end O
- O
to O
- O
end O
sequence O
learning O
with O
attention O
becomes O
the O
dominant O
design O
choice O
for O
Neural O
Machine O
Translation O
( O
NMT O
) O
models O
. O
From O
the O
study O
of O
convolutional O
sequence O
to O
sequence O
learning O
( O
Gehring O
et O
al O
. O
, O
2017a O
, O
b O
) O
, O
to O
the O
prosperity O
of O
self O
- O
attention O
networks O
( O
Vaswani O
et O
al O
. O
, O
2017 O
; O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
modern O
NMT O
systems O
, O
especially O
Transformer O
- O
based O
ones O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
often O
deliver O
state O
- O
of O
- O
the O
- O
art O
performances(Bojar O
et O
al O
. O
, O
2018 O
; O
Barrault O
et O
al O
. O
, O
2019 O
) O
, O
even O
under O
the O
condition O
of O
large O
- O
scale O
corpora O
( O
Ott O
et O
al O
. O
, O
2018 O
; O
Edunov O
et O
al O
. O
, O
2018 O
) O
. O
In O
Transformer O
- O
based O
models O
, O
label O
smoothing O
is O
a O
widely O
applied O
method O
to O
improve O
model O
performance O
. O
Szegedy O
et O
al O
. O
( O
2016 O
) O
initially O
introduce O
the O
method O
when O
making O
reÔ¨Ånements O
to O
the O
Inception O
( O
Szegedy O
et O
al O
. O
, O
2015 O
) O
model O
, O
with O
the O
motivation O
to O
combat O
overÔ¨Åtting O
and O
improve O
adaptability O
. O
In O
principle O
, O
label O
smoothing O
discounts O
a O
certain O
probability O
mass O
from O
the O
true O
label O
and O
redistributes O
it O
uniformly O
across O
all O
the O
class O
labels O
. O
This O
lowers O
the O
difference O
between O
the O
largest O
probability O
output O
and O
the O
others O
, O
effectively O
discouraging O
the O
model O
to O
generate O
overly O
conÔ¨Ådent O
predictions O
. O
Since O
information O
entropy O
( O
Shannon O
, O
1948 O
) O
can O
be O
thought O
of O
as O
a O
conÔ¨Ådence O
measure O
of O
a O
probability O
distribution O
, O
Pereyra O
et O
al O
. O
( O
2017 O
) O
add O
a O
negative O
entropy O
regularization O
term O
to O
the O
conventional O
cross O
entropy O
training O
criterion O
and O
compare O
it O
with O
uniform O
smoothing O
and O
unigram O
smoothing O
. O
M O
¬®uller O
et O
al O
. O
( O
2019 O
) O
deliver O
further O
insightful O
discussions O
about O
label O
smoothing O
, O
empirically O
investigating O
it O
in O
terms O
of O
model O
calibration O
, O
knowledge O
distillation O
and O
representation O
learning O
. O
Label O
smoothing O
itself O
is O
an O
interesting O
topic O
that O
brings O
insights O
about O
the O
general O
learnability O
of O
a O
neural O
model O
. O
While O
existing O
methods O
are O
rather O
heuristical O
in O
their O
nature O
, O
the O
fact O
that O
simply O
discounting O
some O
probability O
mass O
from O
the O
true O
label O
and O
redistributing O
it O
with O
some O
prior O
distribution O
( O
see O
Figure O
1 O
for O
an O
illustration O
) O
works O
in O
practice O
, O
is O
worth O
to O
be O
better O
understood O
. O
In O
this O
paper O
, O
we O
raise O
two O
high O
- O
level O
research O
questions O
to O
outline O
our O
work O
: O
1.Theoretically O
, O
what O
is O
label O
smoothing O
( O
or O
the O
related O
conÔ¨Ådence O
penalty O
) O
optimizing O
for O
? O
2.Practically O
, O
what O
is O
a O
good O
recipe O
in O
order O
to O
apply O
label O
smoothing O
successfully O
in O
NMT?212V O
v01 O
Vm(a O
) O
uniform O
distribution O
V O
v0AB O
( O
b O
) O
conÔ¨Ådence O
penalty O
V O
v0rvm O
( O
c O
) O
arbitrary O
distribution O
Figure O
1 O
: O
An O
illustration O
of O
label O
smoothing O
with O
various O
prior O
distributions O
. O
mandBare O
discounted O
probabiltiy O
masses O
. O
Vis O
the O
vocabulary O
size O
and O
v0is O
the O
correct O
target O
word.1 O
V O
, O
Aandrvare O
prior O
distributions O
. O
Smoothing O
with O
( O
a),mis O
equally O
redistributed O
across O
the O
vocabulary O
. O
Smoothing O
with O
( O
b O
) O
, O
Ais O
implicitly1 O
Veverywhere O
as O
well O
, O
and O
the O
exact O
value O
of O
Bcan O
be O
obtained O
( O
Section O
3.2 O
) O
. O
Smoothing O
with O
( O
c O
) O
, O
mgoes O
to O
each O
class O
in O
proportion O
to O
an O
arbitrary O
smoothing O
prior O
rv(Section O
4.3 O
) O
. O
The O
presentation O
of O
our O
results O
is O
organized O
into O
three O
major O
sections O
: O
‚Ä¢First O
, O
we O
introduce O
a O
generalized O
formula O
for O
label O
smoothing O
and O
derive O
the O
theoretical O
solution O
to O
the O
training O
problem O
. O
‚Ä¢Second O
, O
we O
investigate O
various O
aspects O
that O
affect O
the O
training O
process O
and O
show O
an O
empirically O
good O
recipe O
to O
apply O
label O
smoothing O
. O
‚Ä¢Finally O
, O
we O
examine O
the O
implications O
in O
search O
and O
scoring O
and O
motivate O
further O
research O
into O
the O
mismatch O
between O
training O
and O
testing O
. O
2 O
Related O
Work O
The O
extensive O
use O
of O
NNs O
in O
MT O
( O
Bojar O
et O
al O
. O
, O
2016 O
, O
2017 O
, O
2018 O
; O
Barrault O
et O
al O
. O
, O
2019 O
) O
is O
a O
result O
of O
many O
pioneering O
and O
inspiring O
works O
. O
Continuousvalued O
word O
vectors O
lay O
the O
foundation O
of O
modern O
Natural O
Language O
Processing O
( O
NLP O
) O
NNs O
, O
capturing O
semantic O
and O
syntactic O
relations O
and O
providing O
numerical O
ways O
to O
calculate O
meaningful O
distances O
among O
words O
( O
Bengio O
et O
al O
. O
, O
2001 O
; O
Schwenk O
et O
al O
. O
, O
2006 O
; O
Schwenk O
, O
2007 O
; O
Sundermeyer O
et O
al O
. O
, O
2012 O
; O
Mikolov O
et O
al O
. O
, O
2013a O
, O
b O
) O
. O
The O
investigations O
of O
sequence O
- O
to O
- O
sequence O
learning O
( O
Cho O
et O
al O
. O
, O
2014 O
; O
Sutskever O
et O
al O
. O
, O
2014b O
) O
, O
the O
studies O
of O
attention O
mechanism O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
; O
Luong O
et O
al O
. O
, O
2015 O
) O
and O
the O
explorations O
into O
convolutional O
and O
self O
- O
attention O
NNs O
( O
Gehring O
et O
al O
. O
, O
2017a O
, O
b O
; O
Vaswani O
et O
al O
. O
, O
2017 O
) O
mark O
steady O
and O
important O
steps O
in O
the O
Ô¨Åeld O
of O
NMT O
. O
Since O
the O
introduction O
of O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
the O
Transformer O
model O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
becomes O
the O
de O
facto O
architectural O
choice O
for O
many O
competitive O
NLP O
systems O
. O
Among O
the O
numerous O
ingredients O
that O
make O
Transformer O
networks O
successful O
, O
label O
smoothing O
is O
one O
that O
must O
not O
be O
overlooked O
and O
shall O
be O
the O
focus O
of O
this O
work O
. O
The O
idea O
of O
smoothing O
is O
not O
new O
in O
itself O
. O
For O
instance O
, O
many O
smoothing O
heuristics O
and O
functions O
are O
investigated O
in O
the O
context O
of O
count O
- O
based O
language O
modeling O
( O
Jelinek O
and O
Mercer O
, O
1980 O
; O
Katz O
, O
1987 O
; O
Church O
and O
Gale O
, O
1991 O
; O
Kneser O
and O
Ney O
, O
1995 O
; O
Chen O
and O
Goodman O
, O
1996 O
) O
. O
Interestingly O
, O
when O
training O
NNs O
, O
the O
idea O
of O
smoothing O
comes O
in O
a O
new O
form O
and O
is O
applied O
on O
the O
empirical O
one O
- O
hot O
target O
distributions O
. O
Proposed O
to O
counteract O
overÔ¨Åtting O
and O
pursue O
better O
generalization O
, O
label O
smoothing O
( O
Szegedy O
et O
al O
. O
, O
2016 O
) O
Ô¨Ånds O
its O
Ô¨Årst O
applications O
in O
NNs O
in O
the O
Ô¨Åeld O
of O
computer O
vision O
. O
Later O
, O
the O
method O
is O
shown O
to O
be O
effective O
in O
MT O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O
Furthermore O
, O
it O
is O
also O
helpful O
when O
applied O
in O
other O
scenarios O
, O
e.g. O
Generative O
Adversarial O
Networks O
( O
GANs O
) O
( O
Salimans O
et O
al O
. O
, O
2016 O
) O
, O
automatic O
speech O
recognition O
( O
Chiu O
et O
al O
. O
, O
2018 O
) O
, O
and O
person O
re O
- O
identiÔ¨Åcation O
( O
Ainam O
et O
al O
. O
, O
2019 O
) O
. O
Since O
the O
method O
centralizes O
on O
the O
idea O
of O
avoiding O
over O
- O
conÔ¨Ådent O
model O
outputs O
on O
training O
data O
, O
it O
is O
reanalyzed O
in O
Pereyra O
et O
al O
. O
( O
2017 O
) O
. O
The O
authors O
include O
an O
additional O
conÔ¨Ådence O
penalty O
regularization O
term O
in O
the O
training O
loss O
, O
and O
compare O
it O
to O
standard O
label O
smoothing O
with O
uniform O
or O
unigram O
prior O
. O
While O
label O
smoothing O
boosts O
performance O
signiÔ¨Åcantly O
compared O
to O
using O
hard O
target O
labels O
, O
the O
difference O
in O
performance O
gains O
when O
comparing O
different O
smoothing O
methods O
is O
relatively O
small O
. O
M O
¬®uller O
et O
al O
. O
( O
2019 O
) O
bring O
recent O
advancements O
towards O
better O
intuitive O
understandings O
of O
label O
smoothing O
. O
They O
observe O
a O
clustering O
effect O
of O
learned O
features O
and O
argue O
that O
label O
smoothing O
improves O
model O
calibration O
, O
yet O
hurting O
knowledge O
distillation O
when O
the O
model O
is O
used O
as O
a O
teacher O
for O
another O
student O
network O
. O
As O
a O
regularization O
technique O
in O
training O
, O
label O
smoothing O
can O
be O
compared O
against O
other O
methods O
such O
as O
dropout O
( O
Srivastava O
et O
al O
. O
, O
2014 O
) O
and O
Dis-213turbLabel O
( O
Xie O
et O
al O
. O
, O
2016 O
) O
. O
Intuitively O
, O
dropout O
can O
be O
viewed O
as O
ensembling O
different O
model O
architectures O
on O
the O
same O
data O
and O
DisturbLabel O
can O
be O
viewed O
as O
ensembling O
the O
same O
model O
architecture O
on O
different O
data O
, O
as O
pointed O
out O
in O
Xie O
et O
al O
. O
( O
2016 O
) O
. O
Interestingly O
, O
label O
smoothing O
can O
also O
be O
understood O
as O
estimating O
the O
marginalized O
label O
dropout O
during O
training O
( O
Pereyra O
et O
al O
. O
, O
2017 O
) O
. O
In O
this O
paper O
, O
we O
propose O
two O
straightforward O
extensions O
to O
label O
smoothing O
, O
examining O
token O
selection O
and O
prior O
distribution O
. O
Salimans O
et O
al O
. O
( O
2016 O
) O
and O
Zhou O
et O
al O
. O
( O
2017 O
) O
investigate O
a O
similar O
issue O
to O
the O
former O
. O
In O
the O
context O
of O
GANs O
, O
they O
select O
only O
those O
positive O
examples O
to O
smooth O
while O
we O
consider O
the O
task O
of O
MT O
, O
discussing O
how O
many O
tokens O
to O
smooth O
and O
how O
they O
should O
be O
selected O
. O
Pereyra O
et O
al O
. O
( O
2017 O
) O
and O
Gao O
et O
al O
. O
( O
2019 O
) O
talk O
about O
ideas O
similar O
to O
the O
latter O
. O
In O
their O
respective O
contexts O
, O
one O
experiments O
with O
unigram O
probabilities O
for O
label O
smoothing O
and O
the O
other O
uses O
Language O
Model O
( O
LM O
) O
posteriors O
to O
softly O
augment O
the O
source O
and O
target O
side O
of O
MT O
training O
data O
. O
3 O
Solving O
the O
Training O
Problem O
The O
standard O
label O
smoothing O
( O
STN O
) O
loss O
, O
as O
used O
by O
Vaswani O
et O
al O
. O
( O
2017 O
) O
, O
can O
be O
expressed O
as O
: O
LSTN=‚àíN O
/ O
summationdisplay O
n=1V O
/ O
summationdisplay O
v=1 O
/ O
parenleftbigg O
( O
1‚àím)pv+m1 O
V O
/ O
parenrightbigg O
logqv O
( O
1 O
) O
whereLSTNdenotes O
the O
cross O
entropy O
with O
standard O
label O
smoothing O
, O
nis O
a O
running O
index O
in O
the O
total O
number O
of O
training O
tokens O
N O
, O
vis O
a O
running O
index O
in O
the O
target O
vocabulary O
V O
, O
mis O
the O
hyperparameter O
that O
controls O
the O
amount O
of O
probability O
mass O
to O
discount O
, O
pvis O
the O
one O
- O
hot O
true O
target O
distribution O
and O
qvis O
the O
output O
distribution O
of O
the O
model O
. O
The O
conÔ¨Ådence O
penalty O
( O
CFD O
) O
loss O
, O
as O
used O
by O
Pereyra O
et O
al O
. O
( O
2017 O
) O
, O
can O
be O
expressed O
as O
: O
LCFD=‚àíN O
/ O
summationdisplay O
n=1V O
/ O
summationdisplay O
v=1 O
/ O
parenleftbig O
pv‚àím O
/ O
primeqv O
/ O
parenrightbig O
logqv O
( O
2 O
) O
whereLCFDdenotes O
the O
conÔ¨Ådence O
- O
penalized O
cross O
entropy O
, O
m O
/ O
primein O
this O
case O
is O
the O
hyperparameter O
that O
controls O
the O
strength O
of O
the O
conÔ¨Ådence O
penalty O
and O
thus O
differs O
from O
min O
Equation O
1 O
. O
In O
both O
cases O
, O
the O
outer O
summation O
is O
over O
all O
of O
the O
training O
tokens O
N O
, O
implicating O
that O
all O
of O
the O
target O
token O
probabilities O
are O
smoothed O
. O
Thedependencies O
of O
qvandpvonnare O
omitted O
for O
simplicity O
. O
Additionally O
for O
Equation O
1 O
, O
authors O
of O
both O
papers O
( O
Vaswani O
et O
al O
. O
, O
2017 O
; O
Pereyra O
et O
al O
. O
, O
2017 O
) O
point O
out O
that O
the O
uniform O
prior O
can O
be O
replaced O
with O
alternative O
distributions O
over O
the O
target O
vocabulary O
. O
One O
more O
thing O
to O
notice O
is O
the O
negative O
sign O
in O
front O
of O
the O
non O
- O
negative O
term O
m O
/ O
primein O
Equation O
2 O
, O
which O
means O
that O
pv‚àím O
/ O
primeqvis O
not O
a O
probability O
distribution O
anymore O
. O
One O
can O
nonetheless O
apply O
tricks O
to O
normalize O
the O
term O
inside O
the O
parentheses O
so O
that O
it O
becomes O
a O
probability O
distribution O
, O
e.g. O
: O
LCFD O
normalized O
1=‚àíN O
/ O
summationdisplay O
n=1V O
/ O
summationdisplay O
v=1logqv O
¬∑ O
( O
pv‚àím O
/ O
primeqv)‚àímin(pv‚àím O
/ O
primeqv)/summationtextV O
v O
/ O
prime=1(pv O
/ O
prime‚àím O
/ O
primeqv O
/ O
prime)‚àímin(pv O
/ O
prime‚àím O
/ O
primeqv O
/ O
prime)(3 O
) O
or O
LCFD O
normalized O
2=‚àíN O
/ O
summationdisplay O
n=1V O
/ O
summationdisplay O
v=1logqv O
¬∑ O
exp(pv‚àím O
/ O
primeqv)/summationtextV O
v O
/ O
prime=1exp(pv O
/ O
prime‚àím O
/ O
primeqv O
/ O
prime)(4 O
) O
and O
implement O
it O
as O
an O
additional O
layer O
of O
activation O
during O
training O
, O
where O
v O
/ O
primeis O
an O
alternative O
running O
index O
in O
the O
vocabulary O
. O
In O
any O
case O
, O
the O
integration O
of O
Equation O
2 O
into O
the O
form O
of O
Equation O
1 O
can O
not O
be O
done O
without O
signiÔ¨Åcantly O
modifying O
the O
original O
conÔ¨Ådence O
penalty O
, O
and O
we O
leave O
it O
for O
future O
work O
. O
3.1 O
Generalized O
Formula O
In O
an O
effort O
to O
obtain O
a O
uniÔ¨Åed O
view O
, O
we O
propose O
a O
simple O
generalized O
formula O
and O
make O
two O
major O
changes O
. O
First O
, O
we O
separate O
the O
outer O
summation O
over O
the O
tokens O
and O
divide O
it O
into O
two O
summations O
, O
namely O
‚Äú O
not O
to O
smooth O
‚Äù O
and O
‚Äú O
to O
smooth O
‚Äù O
. O
Second O
, O
we O
modify O
the O
prior O
distribution O
to O
allow O
it O
to O
depend O
on O
the O
position O
, O
current O
token O
and O
model O
output O
. O
In O
this O
case O
, O
rcould O
be O
the O
posterior O
from O
some O
helper O
model O
( O
e.g. O
an O
LM O
) O
, O
and O
during O
training O
, O
obtaining O
it O
on O
- O
the-Ô¨Çy O
is O
not O
expensive O
, O
as O
previously O
shown O
( O
Bi O
et O
al O
. O
, O
2019 O
; O
Wang O
et O
al O
. O
, O
2019 O
) O
. O
The O
generalized O
label O
smoothing O
( O
GNR O
) O
loss O
can O
be O
expressed O
as O
: O
LGNR=‚àí/summationdisplay O
n‚ààAV O
/ O
summationdisplay O
v=1pvlogqv‚àí O
/summationdisplay O
n‚ààBV O
/ O
summationdisplay O
v=1((1‚àím)pv+mrv O
, O
qv O
) O
logqv O
( O
5)214whereLGNRdenotes O
the O
generalized O
cross O
entropy O
, O
Ais O
the O
set O
of O
tokens O
not O
to O
smooth O
, O
Bis O
the O
set O
of O
tokens O
to O
smooth O
, O
rv O
, O
qvis O
an O
arbitrary O
prior O
distribution O
for O
smoothing O
and O
again O
we O
drop O
the O
dependencies O
ofpv O
, O
qvandrv O
, O
qvonnfor O
simplicity O
. O
A O
natural O
question O
when O
explicitly O
writing O
out O
A O
andB O
, O
s.t O
. O
A‚à©B O
= O
‚àÖand|A‚à™B| O
= O
N O
, O
is O
which O
tokens O
to O
include O
in O
B. O
Here O
, O
we O
consider O
two O
simple O
ideas O
: O
uniform O
random O
sampling O
( O
RND O
) O
and O
an O
entropy O
- O
based O
uncertainty O
heuristic O
( O
ENT O
) O
. O
The O
former O
chooses O
a O
certain O
percentage O
of O
tokens O
to O
smooth O
by O
sampling O
tokens O
uniformly O
at O
random O
. O
The O
latter O
prioritizes O
those O
tokens O
whose O
prior O
distributions O
have O
higher O
entropy O
. O
The O
logic O
behind O
the O
ENT O
formulation O
is O
that O
when O
the O
prior O
distribution O
is O
Ô¨Çattened O
out O
, O
yielding O
a O
higher O
entropy O
, O
the O
helper O
model O
is O
uncertain O
about O
the O
current O
position O
, O
and O
the O
model O
output O
should O
thus O
be O
smoothed O
. O
Formally O
, O
the O
two O
heuristics O
can O
be O
expressed O
as O
: O
BRND={n;œÅn‚àºU(0,1),œÅn‚â§œÄ O
} O
( O
6 O
) O
BENT={b1,b2, O
... O
,b O
‚åàœÄN‚åâ O
} O
( O
7 O
) O
whereœÅnis O
a O
sample O
from O
the O
uniform O
distribution O
Uin[0,1],œÄis O
a O
hyperparameter O
controlling O
the O
percentage O
of O
tokens O
to O
smooth O
and O
{ O
b1,b2, O
... O
,bN O
} O
is O
a O
permutation O
of O
data O
indices O
{ O
1,2, O
... O
N}in O
descending O
order O
of O
the O
entropy O
of O
prior O
r O
, O
i.e.‚àÄ1‚â§ O
i‚â§j‚â§N,‚àí/summationtext O
Vrbilogrbi‚â•‚àí/summationtext O
Vrbjlogrbj O
. O
The O
hyperparameter O
min O
Equation O
5 O
deserves O
some O
further O
notice O
. O
This O
is O
essentially O
the O
parameter O
that O
controls O
the O
strength O
of O
the O
label O
smoothing O
procedure O
. O
When O
it O
is O
zero O
, O
no O
smoothing O
is O
done O
. O
When O
it O
is O
one O
and O
|B|=N O
, O
the O
model O
is O
optimized O
to O
output O
the O
prior O
distribution O
r. O
One O
can O
obviously O
further O
generalize O
it O
so O
that O
mdepends O
also O
onn O
, O
vandqv O
. O
However O
in O
this O
work O
, O
we O
focus O
on O
the O
outer O
summation O
in O
Nand O
alternative O
priors O
r O
, O
and O
leave O
the O
exploration O
of O
adaptive O
smoothing O
strengthmn O
, O
r O
, O
qvfor O
future O
work O
. O
3.2 O
Theoretical O
Solution O
When O
it O
comes O
to O
the O
analysis O
of O
label O
smoothing O
, O
previous O
works O
focus O
primarily O
on O
intuitive O
understandings O
. O
Pereyra O
et O
al O
. O
( O
2017 O
) O
observe O
that O
both O
label O
smoothing O
and O
conÔ¨Ådence O
penalty O
lead O
to O
smaller O
gradient O
norms O
during O
training O
. O
M O
¬®uller O
et O
al O
. O
( O
2019 O
) O
argue O
that O
label O
smoothing O
helps O
beamsearch O
by O
improving O
model O
calibration O
. O
They O
further O
visualize O
the O
learned O
features O
and O
show O
a O
clustering O
effect O
of O
features O
from O
the O
same O
class O
. O
In O
this O
work O
, O
we O
concentrate O
on O
Ô¨Ånding O
a O
theoreticalsolution O
to O
the O
training O
problem O
, O
and O
show O
exactly O
what O
label O
smoothing O
and O
conÔ¨Ådence O
penalty O
are O
optimizing O
for O
. O
Consider O
the O
optimization O
problem O
when O
training O
with O
Equation O
1 O
: O
min O
q1,q2, O
... O
,qVLSTN O
n O
, O
s.t O
. O
V O
/ O
summationdisplay O
v=1qv= O
1 O
( O
8) O
While O
in O
practice O
we O
use O
gradient O
optimizers O
to O
obtain O
a O
good O
set O
of O
parameters O
of O
the O
NN O
, O
the O
optimization O
problem O
actually O
has O
well O
- O
deÔ¨Åned O
analytical O
solutions O
locally O
: O
ÀúqSTN O
v= O
( O
1‚àím)pv+m1 O
V(9 O
) O
which O
is O
simply O
a O
linear O
interpolation O
between O
the O
one O
- O
hot O
target O
distribution O
pvand O
the O
smoothing O
prior1 O
V O
, O
withm‚àà[0,1]being O
the O
interpolation O
weight O
. O
One O
can O
use O
either O
the O
divergence O
inequality O
or O
the O
Lagrange O
multiplier O
method O
to O
obtain O
this O
result O
( O
see O
Appendix O
A O
) O
. O
Consider O
the O
optimization O
problem O
when O
training O
with O
Equation O
2 O
: O
min O
q1,q2, O
... O
,qVLCFD O
n O
, O
s.t O
. O
V O
/ O
summationdisplay O
v=1qv= O
1 O
( O
10 O
) O
The O
problem O
becomes O
harder O
because O
now O
the O
regularization O
term O
also O
depends O
on O
qv O
. O
Introducing O
the O
Lagrange O
multiplier O
Œªand O
solving O
for O
optima O
will O
result O
in O
a O
transcendental O
equation O
. O
Making O
use O
of O
the O
Lambert O
Wfunction O
( O
Corless O
et O
al O
. O
, O
1996 O
) O
, O
the O
solution O
can O
be O
expressed O
as O
( O
see O
Appendix O
A O
for O
detailed O
derivation O
): O
ÀúqCFD O
v O
= O
pv O
m O
/ O
primeW0 O
/ O
parenleftBig O
pv O
m O
/ O
primee1+Œª O
m O
/ O
prime O
/ O
parenrightBig O
( O
11 O
) O
whereW0is O
the O
principal O
branch O
of O
the O
Lambert O
W O
function O
and O
Œªis O
the O
Lagrange O
multiplier O
, O
which O
is O
numerically O
solvable1when O
non O
- O
negative O
m O
/ O
primeand O
probability O
distribution O
pvare O
given O
. O
Equation O
11 O
essentially O
gives O
a O
non O
- O
linear O
relationship O
between O
ÀúqCFD O
v O
andpv O
, O
controlled O
by O
the O
hyperparameter O
m O
/ O
prime O
. O
Now O
that O
theoretical O
solutions O
are O
presented O
in O
Equation O
9 O
and O
11 O
, O
it O
is O
possible O
to O
plot O
the O
graphs O
of O
optimal O
Àúqv O
, O
with O
respect O
to O
mandm O
/ O
prime O
. O
Shown O
in O
Figure O
2 O
, O
as O
expected O
for O
both O
STN O
and O
CFD O
, O
the O
overall O
effect O
is O
to O
decrease O
qvwhenpv= O
1and O
increaseqvwhenpv= O
0 O
. O
Whenmorm O
/ O
primegets O
large O
1One O
can O
use O
limm O
/ O
prime‚Üí0ÀúqCFD O
v O
to O
avoid O
division O
by O
zero.21510‚àí210‚àí1100101102 O
morm‚Ä≤00.20.40.60.81ÀúqvÀúqCFD O
vvs.m‚Ä≤ O
ÀúqSTD O
vvs.m(a)pv= O
1 O
10‚àí210‚àí1100101102 O
morm‚Ä≤00.71.42.12.83.5Àúqv√ó10‚àí5 O
1 O
V O
ÀúqCFD O
vvs.m‚Ä≤ O
ÀúqSTD O
vvs.m O
( O
b)pv= O
0 O
Figure O
2 O
: O
Graphs O
of O
optimal O
Àúqvw.r.t.morm/prime O
. O
Note O
the O
logarithmic O
scale O
in O
horizontal O
axes O
, O
with O
m‚àà[0,1 O
] O
andm O
/ O
prime‚â•0 O
. O
In O
order O
to O
obtain O
numerical O
solutions O
for O
ÀúqSTN O
v O
andÀúqCFD O
v O
, O
we O
setV= O
32000 O
, O
which O
is O
a O
common O
vocabulary O
size O
when O
operating O
on O
sub O
- O
word O
levels O
. O
enough O
, O
the O
total O
probability O
mass O
is O
discounted O
and1 O
Vis O
redistributed O
to O
each O
token O
in O
the O
vocabulary O
. O
The O
graph O
of O
GRN2is O
similar O
to O
STD O
, O
only O
changing O
the O
limit O
from1 O
Vtorvasmapproaches O
one O
, O
and O
not O
included O
here O
for O
brevity O
. O
One O
last O
thing O
to O
notice O
is O
that O
the O
outer O
summation O
over O
the O
tokens O
is O
ignored O
. O
If O
it O
is O
taken O
into O
consideration O
, O
Àúq O
is O
dragged O
towards O
the O
empirical O
distribution O
given O
by O
the O
corpus3 O
. O
4 O
Finding O
a O
Good O
Recipe O
In O
this O
section O
, O
we O
describe O
our O
results O
and O
insights O
towards O
a O
good O
recipe O
to O
successfully O
apply O
label O
smoothing O
. O
We O
experiment O
with O
six O
IWSLT2014 O
datasets O
: O
German O
( O
de O
) O
, O
Spanish O
( O
es O
) O
, O
Italian O
( O
it O
) O
, O
Dutch O
( O
nl O
) O
, O
Romanian O
( O
ro O
) O
, O
Russian O
( O
ru O
) O
to O
English O
( O
en O
) O
, O
and O
one O
WMT2014 O
dataset O
: O
English O
to O
German O
. O
The O
statistics O
of O
these O
datasets O
are O
summarized O
in O
Table O
1 O
. O
To O
prepare O
the O
subword O
tokens O
, O
we O
adopt O
joint O
byte O
pair O
encoding O
( O
Sennrich O
et O
al O
. O
, O
2016 O
) O
, O
and O
use O
10 O
K O
and O
32 O
K O
merge O
operations O
on O
IWSLT O
and O
WMT O
, O
respectively O
. O
When O
preprocessing O
IWSLT O
, O
we O
remove O
sentences O
longer O
than O
175 O
words O
, O
lowercase O
both O
source O
and O
target O
sides O
, O
randomly O
subsample O
roughly O
4.35 O
% O
of O
the O
training O
sentence O
pairs O
as O
development O
data O
and O
concatenate O
all O
previously O
available O
development O
and O
test O
sets O
as O
test O
data O
, O
similar O
to O
Gehring O
et O
al O
. O
( O
2017a O
) O
. O
As O
for O
the O
preprocessing O
of O
WMT O
, O
we O
follow O
the O
setup O
in O
Ott O
et O
al O
. O
( O
2018 O
) O
. O
Using O
the O
Transformer O
architec2Assuming O
ronly O
depends O
on O
n O
, O
vand O
not O
qv O
. O
In O
the O
latter O
case O
, O
one O
needs O
to O
solve O
the O
optimization O
problem O
ignoring O
the O
outer O
summation O
and O
reusing O
the O
Lagrange O
multiplier O
. O
3For O
an O
intuitive O
understanding O
, O
consider O
the O
case O
when O
two O
sentence O
pairs O
have O
the O
exact O
same O
context O
up O
to O
a O
certain O
target O
position O
but O
the O
next O
tokens O
are O
different O
( O
e.g. O
‚Äú O
Danke O
. O
‚Äù O
in O
German O
being O
translated O
to O
‚Äú O
Thank O
you O
. O
‚Äù O
and O
‚Äú O
Thank O
you O
very O
much O
. O
‚Äù O
in O
English O
, O
the O
period O
in O
the O
Ô¨Årst O
translation O
and O
‚Äú O
very O
‚Äù O
in O
the O
second O
translation O
have O
the O
same O
context.)ture O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
we O
apply O
the O
base O
setup O
for O
IWSLT O
and O
the O
big O
setup O
for O
WMT O
. O
For O
all O
language O
pairs O
, O
we O
share O
all O
three O
embedding O
matrices O
. O
All O
helper O
models O
are O
also O
Transformer O
- O
based O
. O
We O
conduct O
all O
experiments O
using O
fairseq O
( O
Ott O
et O
al O
. O
, O
2019 O
) O
, O
monitor O
development O
set O
perplexity O
during O
training O
, O
and O
report O
BLEU O
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
scores O
on O
test O
sets O
after O
beam O
search O
. O
4.1 O
Token O
Selection O
The O
Ô¨Årst O
thing O
to O
determine O
is O
how O
to O
select O
tokens O
for O
smoothing O
and O
how O
many O
tokens O
to O
smooth O
. O
0.0 O
0.1 O
0.2 O
0.3 O
0.4 O
0.5 O
0.6 O
0.7 O
0.8 O
0.9 O
1.0 O
œÄ-0.5 O
+ O
0.0 O
+ O
0.5 O
+ O
1.0BLEURND O
ENT O
Figure O
3 O
: O
Smoothing O
with O
RND O
versus O
ENT O
on O
de O
- O
en O
. O
mis O
set O
to O
0.1 O
. O
The O
development O
and O
test O
perplexities O
of O
the O
helper O
LM O
are O
53.8 O
and O
46.5 O
. O
For O
this O
purpose O
, O
we O
begin O
by O
considering O
models O
smoothed O
with O
an O
LM O
helper O
. O
The O
helper O
LM O
is O
trained O
on O
target O
sentences O
from O
the O
corresponding O
parallel O
data O
till O
convergence O
. O
Figure O
3 O
shows O
a O
comparison O
between O
RND O
and O
ENT O
, O
varying O
the O
percentage O
of O
smoothed O
tokens O
œÄand O
using O
the O
absolute O
performance O
improvements O
in O
BLEU O
as O
the O
vertical O
axis O
. O
Since O
the O
two O
methods O
only O
affect O
the O
order O
in O
which O
tokens O
are O
selected O
, O
they O
should O
yield O
the O
exact O
same O
results O
when O
all O
tokens O
are O
selected O
. O
This O
can O
be O
clearly O
seen O
from O
the O
Ô¨Ågure O
and O
serves O
as O
a O
sanity O
check O
for O
the O
correctness O
of216dataset O
IWSLT O
WMT O
language O
pair O
de O
- O
en O
es O
- O
en O
it O
- O
en O
nl O
- O
en O
ro O
- O
en O
ru O
- O
en O
en O
- O
de O
number O
of O
sentence O
pairstrain O
160 O
K O
169 O
K O
167 O
K O
154 O
K O
168 O
K O
153 O
K O
4.50 O
M O
valid O
7.3 O
K O
7.7 O
K O
7.6 O
K O
7.0 O
K O
7.6 O
K O
7.0 O
K O
3.0 O
K O
test O
6.8 O
K O
5.6 O
K O
6.6 O
K O
5.4 O
K O
5.6 O
K O
5.5 O
K O
3.0 O
K O
Table O
1 O
: O
Data O
statistics O
of O
IWSLT O
and O
WMT O
datasets O
. O
the O
implementation O
. O
The O
RND O
and O
ENT O
curves O
follow O
a O
similar O
trend O
, O
increasing O
with O
the O
number O
of O
smoothed O
tokens O
. O
From O
the O
curves O
, O
neither O
selection O
method O
is O
consistently O
better O
than O
the O
other O
, O
indicating O
that O
the O
entropy O
- O
based O
selection O
heuristics O
is O
probably O
an O
oversimpliÔ¨Åcation O
considering O
the O
stochasticity O
introduced O
when O
altering O
the O
number O
of O
smoothed O
tokens O
. O
We O
continue O
to O
examine O
the O
uphill O
trend O
seen O
in O
Figure O
3 O
in O
other O
cases O
. O
0.0 O
0.1 O
0.2 O
0.3 O
0.4 O
0.5 O
0.6 O
0.7 O
0.8 O
0.9 O
1.0 O
œÄ+0.0 O
+ O
0.5 O
+ O
1.0 O
+ O
1.5 O
+ O
2.0BLEUde O
- O
en O
es O
- O
en O
it O
- O
en O
nl O
- O
en O
ro O
- O
en O
ru O
- O
en O
( O
a O
) O
uniform O
as O
rv O
, O
m= O
0.1 O
0.0 O
0.1 O
0.2 O
0.3 O
0.4 O
0.5 O
0.6 O
0.7 O
0.8 O
0.9 O
1.0 O
œÄ+0.0 O
+ O
0.5 O
+ O
1.0 O
+ O
1.5 O
+ O
2.0BLEUde O
- O
en O
es O
- O
en O
it O
- O
en O
nl O
- O
en O
ro O
- O
en O
ru O
- O
en O
( O
b O
) O
unigram O
as O
rv O
, O
m= O
0.1 O
Figure O
4 O
: O
Smoothing O
different O
percentages O
of O
tokens O
. O
Figure O
4 O
reveals O
the O
relationship O
between O
absolute O
BLEU O
improvements O
and O
œÄ O
, O
when O
smoothing O
with O
uniform O
or O
unigram O
( O
RND O
) O
distributions O
. O
While O
for O
each O
language O
pair O
the O
actual O
changes O
in O
BLEU O
differ O
, O
it O
is O
clear O
to O
conclude O
that O
, O
the O
more O
tokens O
smoothed O
, O
the O
better O
the O
performance O
. O
This O
conclusion O
is O
rather O
universal O
and O
holds O
true O
for O
the O
majority O
of O
our O
experiment O
settings O
( O
varying O
m O
andr O
) O
. O
From O
here O
on O
, O
we O
smooth O
all O
tokens O
, O
i.e. O
|B|=N O
, O
by O
default.4.2 O
Probability O
Mass O
Our O
next O
goal O
is O
to O
Ô¨Ånd O
good O
values O
of O
m. O
0 O
0.2 O
0.4 O
0.6 O
0.8 O
1 O
m363738394041BLEU O
es O
- O
en O
nl O
- O
en O
( O
a O
) O
uniform O
as O
rv O
0.0 O
0.1 O
0.2 O
0.3 O
m+0.0 O
+ O
0.5 O
+ O
1.0 O
+ O
1.5 O
+ O
2.0BLEUde O
- O
en O
es O
- O
en O
it O
- O
en O
nl O
- O
en O
ro O
- O
en O
ru O
- O
en O
( O
b O
) O
unigram O
as O
rv O
Figure O
5 O
: O
Discounting O
different O
probability O
masses O
. O
The O
discounted O
probability O
mass O
mis O
a O
tunable O
hyperparameter O
that O
is O
set O
to O
0.1 O
in O
the O
original O
Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
paper O
. O
We O
vary O
this O
parameter O
in O
the O
case O
of O
uniform O
smoothing O
and O
unigram O
smoothing O
, O
and O
plot O
the O
results O
in O
Figure O
5 O
. O
As O
shown O
in O
Figure O
5a O
, O
the O
BLEU O
score O
immediately O
improves O
at O
m= O
0.1 O
, O
then O
plateaus O
when O
m‚àà[0.3,0.6 O
] O
, O
slowly O
decreases O
whenm‚àà[0.7,0.9]and O
quickly O
drops O
to O
zero O
whenmapproaches O
one O
. O
When O
m= O
1 O
, O
the O
model O
is O
optimized O
towards O
a O
uniform O
distribution O
and O
completely O
ignores O
the O
training O
data O
. O
Because O
perplexity O
can O
be O
thought O
of O
as O
the O
effective O
vocabulary O
size O
of O
a O
model O
, O
we O
examine O
the O
perplexities O
when O
m= O
1for O
both O
language O
pairs O
. O
As O
expected O
, O
the O
development O
perplexities O
are O
around O
10 O
K O
, O
which O
is O
in O
the O
same O
order O
of O
magnitude O
as O
the O
corresponding O
vocabulary O
sizes O
. O
Another O
interesting O
observation O
is O
that O
the O
BLEU O
scores O
only O
drop O
when217mgets O
close O
to O
one O
and O
the O
model O
produces O
acceptable O
translations O
elsewhere O
. O
This O
indicates O
that O
NN O
models O
trained O
with O
gradient O
optimizers O
are O
very O
good O
at O
picking O
out O
the O
effective O
training O
signals O
even O
when O
they O
are O
buried O
in O
much O
stronger O
noise O
signals O
( O
the O
uniform O
smoothing O
priors O
in O
the O
case O
of O
Figure O
5a O
) O
. O
This O
could O
be O
further O
related O
to O
multi O
- O
task O
learning O
( O
Ruder O
, O
2017 O
) O
, O
where O
the O
system O
performances O
are O
also O
related O
to O
the O
regularization O
weights O
of O
the O
auxiliary O
losses O
. O
For O
unigram O
, O
we O
varymin{0.1 O
, O
0.2 O
, O
0.3 O
} O
. O
As O
seen O
in O
Figure O
5b O
, O
while O
smoothing O
with O
m= O
0.1gives O
a O
large O
improvement O
over O
no O
smoothing O
, O
setting O
m= O
0.3 O
further O
boosts O
the O
performance O
, O
consistently O
for O
all O
six O
IWSLT O
language O
pairs O
. O
4.3 O
Prior O
Distribution O
Furthermore O
, O
we O
explore O
the O
use O
of O
LM O
and O
MT O
posteriors O
as O
prior O
distributions O
for O
smoothing O
. O
1.0 O
31.0 O
52.9 O
100.2 O
195.6 O
helper O
model O
perplexity36.53737.53838.53939.5BLEULM O
cheating O
LM O
normal O
no O
smoothing O
uniform O
unigram O
( O
a)ro O
- O
en O
, O
LM O
posterior O
as O
rv O
, O
m= O
0.1 O
4.62 O
4.72 O
4.85 O
4.93 O
5.02 O
5.15 O
5.29 O
helper O
model O
perplexity33.53434.535BLEUMT O
no O
smoothing O
uniform O
unigram O
( O
b)de O
- O
en O
, O
MT O
posterior O
as O
rv O
, O
m= O
0.1 O
Figure O
6 O
: O
Smoothing O
with O
LM O
and O
MT O
posteriors O
. O
We O
train O
systems O
using O
Transformer O
LMs O
and O
MT O
models O
of O
different O
qualities O
for O
label O
smoothing O
, O
as O
in O
Figure O
6 O
. O
To O
obtain O
very O
good O
LMs O
, O
we O
train O
them O
with O
test O
data O
and O
mark O
the O
cheating O
LMs O
in O
Figure O
6a O
. O
We O
additionally O
plot O
the O
BLEU O
scores O
of O
models O
with O
no O
smoothing O
, O
smoothed O
with O
uniform O
and O
unigram O
, O
as O
horizontal O
lines O
to O
compare O
the O
absolute O
performances O
. O
Intuitively O
, O
the O
curve O
should O
follow O
a O
downhill O
trend O
, O
meaning O
that O
the O
worse O
the O
helper O
model O
performs O
, O
the O
worse O
themodel O
smoothed O
with O
it O
performs O
. O
This O
is O
loosely O
the O
case O
for O
LM O
, O
with O
cheating O
LMs O
giving O
better O
performances O
than O
uniform O
and O
unigram O
, O
and O
normal O
LMs O
lacking O
behind O
. O
As O
for O
MT O
, O
improvement O
over O
the O
no O
smoothing O
case O
is O
seen O
in O
Figure O
6b O
. O
However O
, O
neither O
the O
downhill O
trend O
nor O
the O
competence O
over O
other O
priors O
in O
terms O
of O
BLEU O
, O
is O
seen O
. O
This O
suggests O
that O
the O
model O
is O
probably O
not O
utilizing O
the O
information O
in O
the O
soft O
distribution O
effectively O
. O
Related O
to O
knowledge O
distillation O
( O
Hinton O
et O
al O
. O
, O
2015 O
; O
Kim O
and O
Rush O
, O
2016 O
) O
, O
a O
trainable O
teacher O
( O
the O
helper O
model O
in O
our O
case O
) O
might O
be O
further O
beneÔ¨Åcial O
( O
Bi O
et O
al O
. O
, O
2019 O
; O
Wang O
et O
al O
. O
, O
2018 O
) O
. O
One O
important O
thing O
to O
mention O
is O
that O
, O
while O
neither O
LM O
nor O
MT O
outperforms O
uniform O
or O
unigram O
in O
terms O
of O
test O
BLEU O
score O
in O
our O
experiments O
, O
we O
see O
signiÔ¨Åcant O
drops O
in O
development O
set O
perplexities O
when O
smoothing O
with O
LM O
or O
MT O
. O
This O
signals O
a O
mismatch O
between O
training O
and O
testing O
, O
and O
suggests O
that O
smoothing O
with O
LM O
or O
MT O
indeed O
works O
well O
for O
the O
optimization O
criterion O
, O
but O
not O
as O
much O
for O
the O
Ô¨Ånal O
metric O
, O
the O
calculation O
of O
which O
involves O
beam O
search O
and O
scoring O
of O
the O
discrete O
tokens O
. O
4.4 O
Final O
Results O
Finally O
, O
we O
report O
BLEU O
scores O
of O
our O
best O
systems O
across O
all O
language O
pairs O
in O
Table O
2 O
. O
While O
applying O
uniform O
label O
smoothing O
signiÔ¨Åcantly O
improves O
over O
the O
baselines O
, O
by O
using O
a O
good O
recipe O
, O
an O
additional O
improvement O
of O
around O
+0.5 O
BLEU O
is O
obtained O
across O
all O
language O
pairs O
. O
For O
the O
hyperparameters O
, O
we O
Ô¨Ånd O
that O
smoothing O
all O
tokens O
by O
m=0.3with O
a O
unigram O
prior O
is O
a O
good O
recipe O
, O
consistently O
giving O
one O
of O
the O
best O
BLEU O
scores O
. O
5 O
Analyzing O
the O
Mismatch O
As O
discussed O
in O
Section O
4.3 O
, O
models O
smoothed O
with O
LMs O
or O
MT O
model O
posteriors O
yield O
very O
good O
development O
set O
perplexities O
but O
no O
big O
improvements O
in O
terms O
of O
test O
BLEU O
scores O
. O
Here O
, O
we O
further O
investigate O
this O
phenomenon O
in O
terms O
of O
search O
and O
scoring O
. O
5.1 O
Search O
We O
Ô¨Årst O
plot O
the O
test O
BLEU O
scores O
with O
respect O
to O
the O
beam O
size O
used O
during O
search O
. O
In O
Figure O
7 O
, O
we O
see O
that O
the O
dashed O
curves O
for O
‚Äú O
no O
smoothing O
‚Äù O
, O
‚Äú O
uniform O
‚Äù O
and O
‚Äú O
unigram O
‚Äù O
initially O
increase O
and O
then O
plateau O
, O
which O
is O
an O
expected O
shape O
( O
see O
Figure O
8218dataset O
IWSLT O
WMT O
language O
pair O
de O
- O
en O
es O
- O
en O
it O
- O
en O
nl O
- O
en O
ro O
- O
en O
ru O
- O
en O
en O
- O
de O
no O
label O
smoothing O
33.6 O
39.3 O
31.2 O
36.5 O
37.0 O
22.3 O
28.0 O
Vaswani O
et O
al O
. O
( O
2017 O
) O
34.4 O
40.8 O
32.4 O
37.5 O
38.5 O
23.4 O
28.4 O
our O
best O
recipe O
35.0 O
41.5 O
32.8 O
38.0 O
39.0 O
23.9 O
29.0 O
Table O
2 O
: O
BLEU O
scores O
can O
be O
signiÔ¨Åcantly O
improved O
with O
good O
label O
smoothing O
recipes O
. O
The O
Ô¨Årst O
row O
of O
numbers O
corresponds O
to O
using O
only O
the O
cross O
entropy O
criterion O
for O
training O
. O
The O
second O
row O
of O
numbers O
corresponds O
to O
the O
Transformer O
baselines O
. O
The O
last O
row O
contains O
scores O
obtained O
with O
our O
best O
hyperparameters O
. O
1 O
2 O
3 O
4 O
5 O
6 O
7 O
8 O
9 O
10 O
beam O
size32333435BLEU O
no O
smoothing O
uniform O
, O
m= O
0.1 O
unigram O
, O
m= O
0.3 O
LM O
, O
m= O
0.3 O
Figure O
7 O
: O
BLEU O
versus O
beam O
size O
on O
de O
- O
en O
. O
in O
Zhou O
et O
al O
. O
( O
2019 O
) O
) O
. O
However O
, O
the O
solid O
curve O
for O
LM O
drops O
quickly O
as O
beam O
size O
increases O
( O
see O
Stahlberg O
and O
Byrne O
( O
2019 O
) O
for O
more O
insight O
) O
. O
A O
possible O
explanation O
is O
that O
models O
smoothed O
with O
LMs O
generate O
search O
spaces O
that O
are O
richer O
in O
probability O
variations O
and O
more O
diversiÔ¨Åed O
, O
compared O
to O
e.g. O
uniform O
label O
smoothing O
. O
As O
search O
becomes O
stronger O
, O
hypotheses O
that O
have O
higher O
probabilities O
, O
but O
not O
necessarily O
closer O
to O
the O
true O
targets O
, O
are O
found O
. O
This O
suggests O
that O
the O
mismatch O
in O
development O
set O
perplexity O
and O
test O
BLEU O
is O
a O
complex O
phenomenon O
and O
calls O
for O
more O
analysis O
. O
5.2 O
Scoring O
We O
further O
examine O
test O
BLEU O
with O
respect O
to O
development O
( O
dev O
) O
BLEU O
and O
dev O
perplexity O
. O
As O
shown O
in O
Figure O
8a O
, O
test O
BLEU O
is O
nicely O
correlated O
with O
dev O
BLEU O
, O
indicating O
that O
there O
is O
no O
mismatch O
between O
dev O
and O
test O
in O
the O
dataset O
itself O
. O
However O
, O
as O
in O
Figure O
8b O
, O
although O
test O
BLEU O
increases O
with O
a O
decreasing O
dev O
perplexity O
, O
in O
regions O
of O
low O
dev O
perplexities O
, O
there O
exist O
many O
systems O
with O
very O
different O
test O
performances O
ranging O
from O
39.3 O
BLEU O
to O
41.5 O
BLEU O
. O
Despite O
perplexity O
being O
directly O
related O
to O
the O
cross O
entropy O
training O
criterion O
, O
this O
is O
an O
example O
where O
it O
fails O
to O
be O
a O
good O
proxy O
for O
the O
Ô¨Ånal O
BLEU O
metric O
. O
Against O
this O
mismatch O
between O
training O
and O
testing O
, O
either O
a O
more O
BLEU O
- O
related O
dev O
score O
or O
a O
more O
perplexityrelated O
test O
metric O
needs O
to O
be O
considered O
. O
41 O
41.5 O
42 O
42.5 O
43 O
43.5 O
dev O
BLEU3939.54040.54141.5test O
BLEU O
data O
Ô¨Åtted O
curve(a O
) O
Dev O
BLEU O
is O
a O
good O
proxy O
for O
test O
BLEU O
. O
0 O
5 O
10 O
15 O
20 O
25 O
30 O
dev O
perplexity3939.54040.54141.5test O
BLEUdata O
Ô¨Åtted O
curve O
( O
b O
) O
Dev O
perplexity O
is O
a O
bad O
proxy O
for O
test O
BLEU O
. O
Figure O
8 O
: O
Relationships O
between O
test O
BLEU O
and O
dev O
metrics O
. O
79 O
converged O
es O
- O
enmodels O
with O
different O
label O
smoothing O
hyperparameters O
are O
scattered O
. O
6 O
Conclusion O
In O
this O
work O
, O
we O
investigate O
label O
smoothing O
in O
neural O
machine O
translation O
. O
Considering O
important O
aspects O
in O
label O
smoothing O
: O
token O
selection O
, O
probability O
mass O
and O
prior O
distribution O
, O
we O
introduce O
a O
generalized O
formula O
and O
derive O
theoretical O
solutions O
to O
the O
training O
problem O
. O
Examining O
the O
effect O
of O
various O
hyperparameter O
choices O
, O
practically O
we O
show O
that O
with O
a O
good O
label O
smoothing O
recipe O
, O
one O
can O
obtain O
consistent O
improvements O
over O
strong O
baselines O
. O
Delving O
into O
search O
and O
scoring O
, O
we O
Ô¨Ånally O
emphasize O
the O
mismatch O
between O
training O
and O
testing O
, O
and O
motivate O
future O
research O
. O
Reassuring O
that O
label O
smoothing O
brings O
concrete O
improvements O
and O
considering O
that O
it O
only O
operates O
at O
the O
output O
side O
of O
the O
model O
, O
our O
next O
step O
is O
to O
explore O
similar O
smoothing O
ideas O
at O
the O
input O
side.219Acknowledgements O
This O
work O
has O
received O
funding O
from O
the O
European O
Research O
Council O
( O
ERC O
) O
( O
under O
the O
European O
Union O
‚Äôs O
Horizon O
2020 O
research O
and O
innovation O
programme O
, O
grant O
agreement O
No O
694537 O
, O
project O
‚Äú O
SEQCLAS O
‚Äù O
) O
and O
the O
Deutsche O
Forschungsgemeinschaft O
( O
DFG O
; O
grant O
agreement O
NE O
572/8 O
- O
1 O
, O
project O
‚Äú O
CoreTec O
‚Äù O
) O
. O
The O
GPU O
computing O
cluster O
was O
supported O
by O
DFG O
( O
Deutsche O
Forschungsgemeinschaft O
) O
under O
grant O
INST O
222/1168 O
- O
1 O
FUGG O
. O
Abstract O
In O
linguistics O
and O
cognitive O
science O
, O
Logical O
metonymies O
are O
deÔ¨Åned O
as O
type O
clashes O
between O
an O
event O
- O
selecting O
verb O
and O
an O
entitydenoting O
noun O
( O
e.g. O
The O
editor O
Ô¨Ånished O
the O
article O
) O
, O
which O
are O
typically O
interpreted O
by O
inferring O
a O
hidden O
event O
( O
e.g. O
reading O
) O
on O
the O
basis O
of O
contextual O
cues O
. O
This O
paper O
tackles O
the O
problem O
of O
logical O
metonymy O
interpretation O
, O
that O
is O
, O
the O
retrieval O
of O
the O
covert O
event O
via O
computational O
methods O
. O
We O
compare O
different O
types O
of O
models O
, O
including O
the O
probabilistic O
and O
the O
distributional O
ones O
previously O
introduced O
in O
the O
literature O
on O
the O
topic O
. O
For O
the O
Ô¨Årst O
time O
, O
we O
also O
tested O
on O
this O
task O
some O
of O
the O
recent O
Transformer O
- O
based O
models O
, O
such O
as O
BERT O
, O
RoBERTa O
, O
XLNet O
, O
and O
GPT-2 O
. O
Our O
results O
show O
a O
complex O
scenario O
, O
in O
which O
the O
best O
Transformer O
- O
based O
models O
and O
some O
traditional O
distributional O
models O
perform O
very O
similarly O
. O
However O
, O
the O
low O
performance O
on O
some O
of O
the O
testing O
datasets O
suggests O
that O
logical O
metonymy O
is O
still O
a O
challenging O
phenomenon O
for O
computational O
modeling O
. O
1 O
Introduction O
The O
phenomenon O
of O
logical O
metonymy O
is O
deÔ¨Åned O
as O
a O
type O
clash O
between O
an O
event O
- O
selecting O
metonymic O
verb O
( O
e.g. O
, O
begin O
) O
and O
an O
entity O
- O
denoting O
nominal O
object O
( O
e.g. O
, O
the O
book O
) O
, O
which O
triggers O
the O
recovery O
of O
a O
hidden O
event O
( O
e.g. O
, O
reading O
) O
. O
Logical O
metonymies O
have O
been O
widely O
studied O
, O
on O
the O
one O
hand O
, O
in O
theoretical O
linguistics O
as O
they O
represent O
a O
challenge O
to O
traditional O
theories O
of O
compositionality O
( O
Asher O
, O
2015 O
; O
Pustejovsky O
and O
Batiukova O
, O
2019 O
) O
. O
On O
the O
other O
hand O
, O
they O
received O
extensive O
attention O
in O
cognitive O
research O
on O
human O
sentence O
processing O
as O
they O
determine O
extra O
processing O
costs O
during O
online O
sentence O
comprehension O
( O
McElree O
et O
al O
. O
, O
2001 O
; O
Traxler O
et O
al O
. O
, O
2002 O
) O
, O
apparently O
relatedto O
‚Äú O
the O
deployment O
of O
operations O
to O
construct O
a O
semantic O
representation O
of O
the O
event O
‚Äù O
( O
Frisson O
and O
McElree O
, O
2008).1 O
Logical O
metonymy O
has O
also O
been O
explained O
in O
terms O
of O
the O
words O
- O
as O
- O
cues O
hypothesis O
proposed O
by O
Jeffrey O
Elman O
( O
Elman O
, O
2009 O
, O
2014 O
) O
. O
This O
hypothesis O
relies O
on O
the O
experimental O
evidence O
that O
human O
semantic O
memory O
stores O
knowledge O
about O
events O
and O
their O
typical O
participants O
( O
see O
McRae O
and O
Matsuki O
( O
2009 O
) O
for O
an O
overview O
) O
and O
claims O
that O
words O
act O
like O
cues O
to O
access O
event O
knowledge O
, O
incrementally O
modulating O
sentence O
comprehension O
. O
The O
results O
obtained O
in O
a O
probe O
recognition O
experiment O
by O
Zarcone O
et O
al O
. O
( O
2014 O
) O
, O
in O
line O
with O
this O
explanation O
, O
suggest O
that O
speakers O
interpret O
logical O
metonymies O
by O
inferring O
the O
most O
likely O
event O
the O
sentences O
could O
refer O
to O
, O
given O
the O
contextual O
cues O
. O
Previous O
research O
in O
NLP O
on O
logical O
metonymy O
has O
often O
been O
inÔ¨Çuenced O
by O
such O
theoretical O
explanation O
( O
Zarcone O
and O
Pad O
¬¥ O
o O
, O
2011 O
; O
Zarcone O
et O
al O
. O
, O
2012 O
; O
Chersoni O
et O
al O
. O
, O
2017 O
) O
. O
In O
our O
contribution O
, O
we O
propose O
a O
general O
comparison O
of O
different O
classes O
of O
computational O
models O
for O
logical O
metonymy O
. O
To O
begin O
with O
, O
we O
tested O
two O
approaches O
that O
have O
been O
previously O
introduced O
in O
the O
literature O
on O
the O
topic O
: O
probabilistic O
and O
distributional O
models O
( O
Zarcone O
et O
al O
. O
, O
2012 O
) O
. O
We O
also O
examined O
the O
Structured O
Distributional O
Model O
( O
SDM O
) O
by O
Chersoni O
et O
al O
. O
( O
2019 O
) O
, O
which O
represents O
sentence O
meaning O
with O
a O
combination O
of O
formal O
structures O
and O
distributional O
embeddings O
to O
dynamically O
integrate O
knowledge O
about O
events O
and O
their O
typical O
participants O
, O
as O
they O
are O
activated O
by O
lexical O
items O
. O
Finally O
, O
to O
the O
best O
of O
our O
knowledge O
, O
we O
are O
the O
Ô¨Årst O
ones O
to O
include O
the O
recent O
Transformer O
language O
models O
into O
a O
contrastive O
study O
on O
1Notice O
however O
that O
the O
evidence O
is O
not O
uncontroversial O
: O
Delogu O
et O
al O
. O
( O
2017 O
) O
report O
that O
coercion O
costs O
largely O
reÔ¨Çect O
word O
surprisal O
, O
without O
any O
speciÔ¨Åc O
effect O
of O
type O
shift O
in O
the O
early O
processing O
measures.224logical O
metonymy O
. O
Transformers O
( O
Vaswani O
et O
al O
. O
, O
2017 O
; O
Devlin O
et O
al O
. O
, O
2019 O
) O
are O
the O
dominant O
class O
of O
NLP O
systems O
in O
the O
last O
few O
years O
, O
since O
they O
are O
able O
to O
generate O
‚Äú O
dynamic O
‚Äù O
representations O
for O
a O
target O
word O
depending O
on O
the O
sentence O
context O
. O
As O
the O
interpretation O
of O
logical O
metonymy O
is O
highly O
sensitive O
to O
context O
, O
we O
deem O
that O
the O
contextual O
representations O
built O
by O
Transformers O
might O
be O
able O
to O
integrate O
the O
covert O
event O
that O
is O
missing O
in O
the O
surface O
form O
of O
the O
sentence O
. O
All O
models O
are O
evaluated O
on O
their O
capability O
ofassigning O
the O
correct O
interpretation O
to O
a O
metonymic O
sentence O
, O
that O
is O
, O
recovering O
the O
verb O
that O
refers O
to O
the O
correct O
interpretation O
. O
This O
task O
is O
hard O
for O
computational O
models O
, O
as O
they O
must O
exploit O
contextual O
cues O
to O
distinguish O
covert O
events O
with O
a O
high O
typicality O
( O
e.g. O
, O
The O
pianist O
begins O
the O
symphony‚Üíplaying O
) O
from O
plausible O
but O
less O
typical O
ones O
( O
‚Üícomposing O
) O
. O
2 O
Related O
Work O
2.1 O
Computational O
Models O
of O
Logical O
Metonymy O
According O
to O
Zarcone O
et O
al O
. O
( O
2013 O
) O
, O
the O
phenomenon O
of O
logical O
metonymy O
can O
be O
explained O
in O
terms O
of O
the O
thematic O
Ô¨Åt O
, O
that O
is O
, O
the O
degree O
of O
compatibility O
between O
the O
verb O
and O
one O
of O
its O
arguments O
( O
the O
direct O
object O
, O
in O
this O
case O
) O
. O
On O
the O
one O
hand O
, O
a O
low O
thematic O
Ô¨Åt O
between O
an O
event O
- O
selecting O
verb O
and O
an O
entity O
- O
denoting O
argument O
triggers O
the O
recovery O
of O
a O
covert O
event O
, O
while O
on O
the O
other O
hand O
, O
the O
recovered O
event O
is O
often O
the O
best O
Ô¨Åtting O
one O
, O
given O
the O
information O
available O
in O
the O
sentence O
. O
Research O
in O
NLP O
on O
logical O
metonymy O
initially O
focused O
on O
the O
problem O
of O
covert O
event O
retrieval O
, O
which O
was O
tackled O
by O
means O
of O
probabilistic O
models O
( O
Lapata O
and O
Lascarides O
, O
2003 O
; O
Shutova O
, O
2009 O
) O
, O
or O
by O
using O
Distributional O
Semantic O
Models O
( O
DSMs O
) O
that O
identify O
the O
candidate O
covert O
event O
with O
the O
one O
that O
has O
the O
highest O
thematic O
Ô¨Åt O
with O
the O
arguments O
in O
the O
sentence O
( O
Zarcone O
et O
al O
. O
, O
2012 O
) O
. O
Following O
the O
psycholinguistic O
works O
by O
McElree O
et O
al O
. O
( O
2001 O
) O
and O
Traxler O
et O
al O
. O
( O
2002 O
) O
, O
which O
reported O
increased O
reading O
times O
and O
longer O
Ô¨Åxations O
in O
eye O
- O
tracking O
for O
the O
metonymic O
sentences O
, O
Zarcone O
et O
al O
. O
( O
2013 O
) O
proposed O
a O
distributional O
model O
of O
the O
thematic O
Ô¨Åt O
between O
verb O
and O
object O
, O
and O
showed O
that O
it O
accurately O
reproduces O
the O
differences O
between O
the O
experimental O
conditions O
in O
the O
data O
from O
the O
two O
original O
studies O
. O
A O
general O
distributional O
model O
for O
sentence O
com O
- O
prehension O
was O
used O
by O
Chersoni O
et O
al O
. O
( O
2017 O
) O
to O
simultaneously O
tackle O
both O
these O
two O
aspects O
of O
logical O
metonymy O
( O
covert O
event O
retrieval O
and O
increased O
processing O
times O
) O
, O
although O
at O
the O
cost O
of O
a O
highly O
- O
elaborated O
compositional O
model O
. O
The O
authors O
recently O
introduced O
a O
more O
up O
- O
to O
- O
date O
and O
reÔ¨Åned O
version O
of O
their O
sentence O
comprehension O
model O
( O
Chersoni O
et O
al O
. O
, O
2019 O
) O
, O
but O
it O
has O
not O
been O
tested O
on O
the O
logical O
metonymy O
task O
so O
far O
. O
2.2 O
Transformer O
Models O
in O
NLP O
The O
traditional O
approach O
in O
Distributional O
Semantics O
has O
been O
the O
building O
of O
a O
single O
, O
stable O
vector O
representation O
for O
each O
word O
type O
in O
the O
corpus O
( O
Turney O
and O
Pantel O
, O
2010 O
; O
Lenci O
, O
2018 O
) O
. O
Lately O
, O
a O
new O
generation O
of O
embeddings O
has O
emerged O
, O
in O
which O
each O
occurrence O
of O
a O
word O
in O
a O
speciÔ¨Åc O
sentence O
context O
gets O
a O
unique O
representation O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
. O
The O
most O
recent O
systems O
typically O
rely O
on O
an O
LSTM O
or O
a O
Transformer O
architecture O
for O
getting O
word O
representations O
: O
they O
are O
trained O
on O
large O
amounts O
of O
textual O
data O
and O
the O
word O
vectors O
are O
learned O
as O
a O
function O
of O
the O
internal O
states O
of O
the O
encoder O
, O
such O
that O
a O
word O
in O
different O
sentence O
contexts O
determines O
different O
activation O
states O
and O
is O
represented O
by O
a O
different O
vector O
. O
Thus O
, O
embeddings O
generated O
by O
these O
new O
models O
are O
said O
to O
be O
contextualized O
, O
as O
opposed O
to O
the O
static O
vectors O
generated O
by O
the O
earlier O
frameworks O
, O
and O
they O
aim O
at O
modeling O
the O
speciÔ¨Åc O
sense O
assumed O
by O
the O
word O
in O
context O
. O
One O
of O
the O
most O
popular O
and O
successful O
contextualized O
model O
is O
probably O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
whose O
key O
technical O
innovation O
is O
applying O
the O
bidirectional O
training O
of O
Transformer O
, O
a O
popular O
attention O
model O
, O
to O
language O
modelling O
. O
This O
is O
in O
contrast O
to O
previous O
efforts O
which O
looked O
at O
a O
text O
sequence O
either O
from O
left O
to O
right O
or O
combined O
left O
- O
to O
- O
right O
and O
right O
- O
toleft O
training O
. O
The O
results O
of O
the O
paper O
show O
that O
a O
language O
model O
with O
bidirectional O
training O
can O
have O
a O
deeper O
sense O
of O
language O
context O
and O
structure O
than O
single O
- O
direction O
language O
models O
. O
An O
interesting O
aspect O
of O
Transformer O
models O
like O
BERT O
is O
that O
they O
are O
trained O
via O
masked O
language O
modeling O
, O
that O
is O
, O
they O
have O
to O
retrieve O
a O
word O
that O
has O
been O
masked O
in O
a O
given O
input O
sentence O
. O
Since O
interpreting O
logical O
metonymy O
implies O
the O
retrieval O
of O
an O
event O
that O
is O
not O
overtly O
expressed O
and O
that O
humans O
retrieve O
integrating O
the O
lexical O
cues O
in O
the O
sentence O
, O
these O
models O
are O
potentially O
a O
very O
good O
Ô¨Åt O
for O
this O
task O
. O
To O
draw O
an O
analogy O
, O
we O
could225imagine O
that O
the O
covert O
event O
is O
a O
verb O
that O
has O
been O
‚Äô O
masked O
‚Äô O
in O
the O
linguistic O
input O
and O
that O
we O
ask O
BERT O
- O
like O
models O
to O
make O
a O
guess O
. O
It O
is O
important O
to O
point O
out O
that O
not O
all O
Transformers O
are O
used O
for O
masked O
language O
modeling O
: O
among O
those O
tested O
for O
this O
study O
, O
BERT O
and O
RoBERTa O
are O
directly O
trained O
with O
this O
objective O
, O
XLNet O
is O
trained O
with O
permutation O
language O
modeling O
, O
but O
can O
still O
retrieve O
a O
hidden O
word O
given O
a O
bidirectional O
context O
, O
and O
GPT-2 O
works O
similarly O
to O
a O
traditional O
, O
unidirectional O
language O
model O
. O
3 O
Experimental O
Settings O
3.1 O
Task O
Our O
research O
question O
focuses O
on O
how O
computational O
models O
can O
interpret O
metonymic O
sentences O
. O
To O
explore O
this O
issue O
, O
we O
deÔ¨Åne O
the O
task O
of O
logical O
metonymy O
interpretation O
as O
a O
covert O
event O
recoverytask O
. O
More O
speciÔ¨Åcally O
, O
given O
a O
sentence O
like O
The O
architect O
Ô¨Ånished O
the O
house O
, O
the O
computational O
model O
has O
to O
return O
the O
most O
likely O
hidden O
verb O
for O
the O
sentence O
, O
i.e. O
the O
covert O
event O
representing O
its O
interpretation O
. O
Despite O
the O
architectural O
differences O
, O
all O
tested O
models O
compute O
a O
plausibility O
score O
of O
a O
verb O
as O
expressing O
the O
covert O
event O
associated O
with O
a O
< O
subject O
, O
metonymic O
verb O
, O
object O
> O
triple O
. O
We O
evaluate O
the O
scores O
returned O
by O
a O
model O
against O
human O
judgments O
using O
the O
standard O
measures O
of O
accuracy O
and O
correlation O
depending O
if O
the O
dataset O
contains O
categorical O
or O
continuous O
variables O
. O
3.2 O
Datasets O
In O
our O
experiments O
, O
we O
use O
three O
datasets O
designed O
for O
previous O
psycholinguistic O
studies O
, O
and O
a O
newly O
created O
one O
by O
means O
of O
an O
elicitation O
task O
. O
TheMcElree O
dataset O
( O
MC O
) O
comprises O
the O
stimuli O
from O
the O
sentences O
of O
the O
self O
- O
paced O
reading O
experiment O
of O
McElree O
et O
al O
. O
( O
2001 O
) O
and O
includes O
30 O
pairs O
of O
tuples O
. O
Each O
pair O
has O
the O
same O
subject O
, O
metonymic O
verb O
, O
object O
, O
just O
the O
covert O
verb O
varies O
. O
As O
in O
the O
conditions O
of O
the O
original O
experiment O
, O
the O
hidden O
verb O
could O
be O
either O
highly O
plausible O
, O
or O
plausible O
but O
less O
typical O
, O
given O
the O
subject O
and O
the O
object O
of O
the O
tuple O
. O
The O
Traxler O
dataset O
( O
TR O
) O
results O
from O
the O
sentences O
of O
the O
eye O
- O
tracking O
experiment O
of O
Traxler O
et O
al O
. O
( O
2002 O
) O
and O
includes O
36 O
pairs O
of O
tuples O
. O
The O
format O
is O
the O
same O
as O
the O
McElree O
dataset O
. O
On O
these O
two O
datasets O
, O
the O
models O
have O
to O
perform O
a O
binary O
classiÔ¨Åcation O
task O
, O
with O
the O
goal O
of O
assigning O
a O
higher O
score O
to O
the O
covert O
event O
in O
the O
typical O
condition O
. O
TheLapata O
- O
Lascarides O
dataset O
( O
L&L O
) O
( O
Lapata O
and O
Lascarides O
, O
2003 O
) O
includes O
174 O
tuples O
, O
each O
composed O
by O
a O
metonymic O
verb O
, O
an O
object O
and O
a O
potential O
covert O
verb O
. O
The O
authors O
collected O
plausibility O
ratings O
for O
each O
metonymy O
by O
turning O
the O
tuples O
into O
sentences O
and O
used O
the O
Magnitude O
Estimation O
Paradigm O
( O
Stevens O
, O
1957 O
) O
to O
ask O
human O
subjects O
to O
rate O
the O
plausibility O
of O
the O
interpretation O
of O
the O
metonymic O
verb O
. O
Finally O
, O
the O
mean O
ratings O
have O
been O
normalized O
and O
log O
- O
transformed O
. O
A O
further O
dataset O
of O
recovered O
covert O
events O
( O
CE O
) O
was O
collected O
by O
the O
authors O
. O
The O
metonymic O
sentences O
used O
in O
the O
McElree O
and O
Traxler O
experiments O
were O
turned O
into O
69templates O
with O
an O
empty O
slot O
corresponding O
to O
the O
covert O
event O
( O
e.g. O
, O
The O
student O
began O
the O
book O
late O
in O
the O
semester O
) O
. O
Thirty O
subjects O
recruited O
with O
crowdsourcing O
were O
asked O
to O
produce O
two O
verbs O
that O
provided O
the O
most O
likely O
Ô¨Ållers O
for O
the O
event O
slot O
. O
Out O
of O
the O
4,084 O
collected O
verbs O
, O
we O
selected O
those O
with O
a O
production O
frequency‚â•3for O
a O
given O
stimulus O
. O
The O
Ô¨Ånal O
dataset O
comprises O
285items O
each O
consisting O
of O
a O
subject O
‚Äì O
metonymic O
verb O
‚Äì O
object O
tuple O
t O
and O
a O
covert O
event O
eassociated O
with O
a O
salience O
score O
corresponding O
to O
the O
event O
conditional O
probability O
given O
the O
tuple O
P(e|t)(i.e O
. O
, O
the O
production O
frequency O
of O
enormalized O
by O
the O
total O
events O
produced O
for O
t O
) O
. O
In O
the O
case O
of O
the O
latter O
two O
datasets O
, O
for O
each O
model O
we O
compute O
the O
Spearman O
‚Äôs O
correlation O
between O
the O
probabilities O
generated O
by O
the O
model O
and O
the O
human O
judgements O
. O
Examples O
from O
these O
datasets O
are O
provided O
in O
Table O
1 O
. O
While O
collecting O
the O
data O
for O
CE O
, O
we O
also O
run O
a O
statistical O
comparison O
between O
the O
production O
frequencies O
of O
the O
verbs O
in O
the O
typical O
and O
in O
the O
atypical O
condition O
that O
appear O
in O
the O
binary O
classiÔ¨Åcation O
datasets O
, O
to O
ensure O
that O
humans O
genuinely O
agree O
on O
the O
higher O
typicality O
of O
the O
former O
. O
The O
result O
conÔ¨Årmed O
this O
assumption O
: O
according O
to O
the O
Wilcoxon O
signed O
rank O
test O
with O
continuity O
correction O
, O
the O
frequencies O
of O
production O
of O
the O
typical O
verbs O
for O
the O
MC O
dataset O
were O
signiÔ¨Åcantly O
higher O
( O
W= O
424 O
, O
p O
< O
0.001 O
) O
, O
and O
the O
same O
holds O
for O
the O
typical O
verbs O
in O
the O
TRdataset O
( O
W= O
526 O
.5 O
, O
p O
< O
0.001 O
) O
. O
3.3 O
Models O
In O
the O
following O
section O
, O
we O
describe O
the O
general O
aspects O
of O
the O
computational O
models O
that O
we O
tested O
on O
logical O
metonymy O
interpretation.226Dataset O
Subject O
- O
verb O
- O
object O
Covert O
event O
Condition O
/ O
Score O
Size O
MC O
chef O
start O
dinnerprepare O
HIGH O
TYP30 O
( O
pairs)eat O
LOW O
TYP O
TR O
dieter O
resist O
cakeeat O
HIGH O
TYP36 O
( O
pairs)taste O
LOW O
TYP O
L&L O
‚Äî O
start O
experimentimplement O
0.1744174study O
0.0184 O
CE O
architect O
start O
housedraw O
0.348258build O
0.087 O
Table O
1 O
: O
Examples O
of O
stimuli O
from O
each O
dataset O
. O
3.3.1 O
Probabilistic O
Model O
As O
a O
baseline O
model O
, O
we O
adopt O
the O
simple O
probabilistic O
approach O
proposed O
by O
Lapata O
and O
Lascarides O
( O
2003 O
) O
and O
replicated O
by O
Zarcone O
et O
al O
. O
( O
2012 O
) O
as O
the O
SOpmodel O
, O
which O
was O
reported O
as O
the O
best O
performing O
probabilistic O
model O
on O
the O
task O
. O
The O
interpretation O
of O
a O
logical O
metonymy O
( O
e.g. O
, O
The O
pianist O
began O
the O
symphony O
) O
is O
modelled O
as O
the O
joint O
distribution O
P(s O
, O
v O
, O
o O
, O
e O
) O
of O
the O
variables O
s(the O
subject O
, O
pianist O
) O
, O
v(the O
metonymic O
verb O
, O
began O
) O
, O
o O
( O
the O
object O
, O
symphony O
) O
, O
and O
the O
covert O
event O
e(e.g O
. O
, O
play O
) O
. O
We O
compute O
that O
probability O
considering O
the O
metonymic O
verb O
constant O
: O
P(s O
, O
v O
, O
o O
, O
e O
) O
‚âàP(e)P(o|e)P(s|e O
) O
The O
verb O
Erepresenting O
the O
preferred O
interpretation O
of O
the O
metonymy O
is O
the O
verb O
emaximizing O
the O
following O
equation O
: O
E O
= O
argmax O
eP(e)P(o|e)P(s|e O
) O
We O
computed O
the O
statistics O
from O
a O
2018 O
dump O
of O
the O
English O
Wikipedia O
, O
parsed O
with O
the O
Stanford O
CoreNLP O
toolkit O
( O
Manning O
et O
al O
. O
, O
2014 O
) O
. O
Dataset O
Coverage O
MC O
19/30 O
( O
pairs O
) O
TR O
21/36 O
( O
pairs O
) O
L&L O
151/174 O
( O
items O
) O
CE O
195/285 O
( O
items O
) O
Table O
2 O
: O
Coverage O
for O
the O
probabilistic O
model O
. O
3.3.2 O
Logical O
Metonymy O
as O
Thematic O
Fit O
Distributional O
models O
of O
logical O
metonymy O
assume O
that O
the O
event O
recovery O
task O
can O
be O
seen O
as O
a O
thematic O
Ô¨Åt O
task O
: O
recovering O
the O
covert O
event O
means O
identifying O
the O
verb O
with O
the O
highest O
thematic O
Ô¨Åt O
with O
the O
metonymic O
sentence O
. O
We O
reimplement O
thedistributional O
model O
by O
Zarcone O
et O
al O
. O
( O
2012 O
) O
with O
the O
following O
procedure O
: O
‚Ä¢we O
retrieve O
the O
n(= O
500 O
) O
2most O
strongly O
associated O
verbs O
for O
the O
subject O
and O
the O
object O
respectively O
, O
and O
we O
take O
the O
intersection O
of O
the O
two O
lists O
; O
‚Ä¢we O
update O
their O
association O
scores O
using O
either O
the O
sum O
( O
add O
) O
or O
the O
product O
( O
prod O
) O
function O
; O
‚Ä¢we O
select O
the O
embeddings O
corresponding O
to O
the O
Ô¨Årst O
m(= O
20 O
) O
verbs O
in O
this O
list O
and O
we O
add O
them O
together O
to O
create O
the O
prototype O
vector O
of O
the O
verb O
given O
the O
subject O
and O
the O
object O
; O
‚Ä¢the O
thematic O
Ô¨Åt O
of O
the O
covert O
event O
ewith O
respect O
to O
the O
nominal O
entities O
is O
computed O
as O
the O
similarity O
score O
of O
its O
corresponding O
lexical O
vector O
/vector O
ewith O
the O
prototype O
vector O
. O
As O
we O
did O
the O
probabilistic O
model O
, O
we O
discard O
the O
metonymic O
verb O
from O
this O
computation.3 O
We O
test O
two O
variations O
of O
this O
model O
, O
TF O
- O
add O
andTF O
- O
prod O
, O
which O
differ O
for O
the O
Ô¨Åller O
selection O
update O
function O
. O
Statistics O
were O
extracted O
from O
Wikipedia O
2018 O
, O
and O
the O
vectors O
were O
the O
publiclyavailable O
Wikipedia O
embeddings4trained O
with O
the O
FastText O
model O
( O
Bojanowski O
et O
al O
. O
, O
2017 O
) O
. O
The O
verb-Ô¨Åller O
association O
score O
is O
the O
Local O
Mutual O
Information O
( O
Evert O
, O
2008 O
) O
. O
Similarly O
, O
the O
scores O
for O
the O
subject O
Ô¨Ållers O
are O
deÔ¨Åned O
as O
: O
LMI O
( O
s O
, O
e O
) O
= O
f(esbj‚Üê‚àí‚àís)log2p(s|e O
) O
p(s)p(e O
) O
2We O
set O
a O
high O
value O
for O
this O
parameter O
in O
order O
to O
maximize O
the O
coverage O
. O
3Zarcone O
et O
al O
. O
( O
2012 O
) O
show O
that O
, O
for O
both O
the O
probabilistic O
and O
the O
distributional O
model O
, O
including O
the O
metonymic O
verb O
does O
not O
help O
too O
much O
in O
terms O
of O
performance O
and O
leads O
to O
coverage O
issues O
. O
4https://fasttext.cc/docs/en/ O
english O
- O
vectors.html227where O
sis O
the O
subject O
, O
ethe O
covert O
event O
, O
and O
f(esbj‚Üê‚àí‚àís)indicates O
the O
frequency O
of O
ewith O
the O
subject O
. O
The O
scores O
for O
the O
object O
position O
are O
computed O
with O
the O
following O
formula O
: O
LMI O
( O
o O
, O
e O
) O
= O
f(eobj‚Üê‚àí‚àío)log2p(o|e O
) O
p(o)p(e O
) O
where O
ois O
the O
object O
and O
f(eobj‚Üê‚àí‚àío)represents O
the O
joint O
frequency O
of O
ewith O
the O
object O
. O
3.3.3 O
Structured O
Distributional O
Model O
The O
Structured O
Distributional O
Model O
( O
SDM O
) O
proposed O
by O
Chersoni O
et O
al O
. O
( O
2019 O
) O
consists O
of O
two O
components O
: O
a O
Distributional O
Event O
Graph O
( O
henceforth O
, O
DEG O
) O
, O
and O
a O
meaning O
composition O
function O
. O
DEG O
represents O
event O
knowledge O
as O
a O
graph O
automatically O
built O
from O
parsed O
corpora O
, O
where O
the O
nodes O
are O
words O
associated O
to O
a O
numeric O
vector O
, O
and O
the O
edges O
are O
labeled O
with O
syntactic O
relations O
and O
weighted O
using O
statistic O
association O
measures O
. O
Each O
event O
is O
represented O
as O
a O
path O
in O
DEG O
, O
that O
is O
, O
a O
sequence O
of O
edges O
( O
relations O
) O
which O
joins O
a O
sequence O
of O
vertices O
( O
words O
) O
. O
Thus O
, O
given O
a O
lexical O
cue O
w O
, O
it O
is O
possible O
to O
identify O
the O
associated O
events O
and O
to O
generate O
expectations O
about O
incoming O
inputs O
on O
both O
the O
paradigmatic O
and O
the O
syntagmatic O
axis O
. O
The O
composition O
function O
makes O
use O
of O
two O
semantic O
structures O
( O
inspired O
by O
DRT O
( O
Kamp O
, O
2013 O
) O
): O
the O
linguistic O
condition O
( O
LC O
) O
, O
a O
contextindependent O
tier O
of O
meaning O
, O
and O
the O
active O
context O
( O
AC O
) O
, O
which O
accumulates O
contextual O
information O
available O
during O
sentence O
processing O
or O
activated O
by O
lexical O
items O
. O
The O
crucial O
aspect O
is O
that O
the O
model O
associates O
a O
vectorial O
representation O
to O
these O
formal O
structures O
: O
/vectorLCis O
the O
sum O
of O
the O
embeddings O
of O
the O
lexical O
items O
of O
a O
sentence O
; O
/vectorAC O
, O
for O
each O
syntactic O
slot O
, O
is O
represented O
as O
the O
centroid O
vector O
built O
out O
of O
the O
role O
vectors O
/vector O
r1 O
, O
... O
, O
/vector O
r O
navailable O
in O
AC O
, O
i.e. O
the O
syntactic O
associates O
of O
the O
lexical O
items O
that O
have O
been O
already O
processed O
. O
In O
our O
implementation O
of O
SDM O
, O
the O
DEG O
is O
constructed O
by O
extracting O
syntactic O
relations O
from O
the O
same O
dump O
of O
Wikipedia O
adopted O
in O
the O
previous O
models O
, O
and O
we O
chose O
as O
lexical O
embeddings O
the O
same O
FastText O
Wikipedia O
vectors O
. O
Following O
the O
same O
assumption O
of O
the O
previous O
experiment O
, O
we O
model O
the O
covert O
event O
recovery O
task O
as O
a O
thematic O
Ô¨Åt O
task O
: O
the O
goal O
is O
to O
predict O
the O
hidden O
verb O
on O
the O
basis O
of O
the O
subject O
and O
the O
object O
, O
treating O
the O
metonymic O
verb O
as O
a O
constant O
. O
SpeciÔ¨Åcally O
, O
the O
model O
builds O
a O
semantic O
representation O
for O
eachModel O
settings O
Data O
size O
L O
H O
A O
P O
BERT O
large O
- O
cased24 O
1024 O
16 O
340 O
M O
16 O
GB O
RoBERTa O
large24 O
1024 O
16 O
355 O
M O
160 O
GB O
XLNet O
large O
- O
cased24 O
1024 O
16 O
340 O
M O
113 O
GB O
GPT-2 O
extra O
- O
large48 O
1600 O
25 O
1542 O
M O
40 O
GB O
Table O
3 O
: O
Comparison O
between O
transformer O
models O
. O
Model O
details O
: O
L O
: O
number O
of O
layers O
, O
H O
: O
dimension O
of O
hidden O
states O
, O
A O
: O
attention O
head O
numbers O
, O
and O
P O
: O
total O
parameter O
size O
. O
tuple O
in O
the O
dataset O
. O
The O
linguistic O
condition O
vector O
/vectorLCcontains O
the O
sum O
of O
the O
subject O
and O
object O
embeddings O
. O
At O
the O
same O
time O
, O
the O
event O
knowledge O
vector O
/vectorACcontains O
the O
prototypical O
embedding O
for O
the O
main O
verb O
, O
using O
DEG O
to O
retrieve O
the O
most O
associated O
verbs O
for O
the O
subject O
and O
the O
object O
, O
as O
in O
Chersoni O
et O
al O
. O
( O
2019 O
) O
. O
The O
scoring O
function O
has O
been O
adapted O
to O
the O
event O
recovery O
task O
as O
follows O
: O
cos(/vector O
e,/vectorLC(sent O
) O
) O
+ O
cos(/vector O
e,/vectorAC(sent O
) O
) O
where O
sentrefers O
to O
the O
metonymic O
test O
tuple O
. O
In O
other O
words O
, O
we O
quantify O
the O
typicality O
of O
a O
verb O
for O
a O
tuple O
subject O
- O
object O
as O
the O
sum O
of O
i. O
) O
the O
cosine O
similarity O
between O
the O
event O
embedding O
and O
the O
additive O
combination O
of O
the O
other O
argument O
vectors O
( O
/vectorLC O
) O
and O
ii O
. O
) O
the O
cosine O
similarity O
between O
the O
event O
embedding O
and O
the O
prototype O
vector O
representing O
the O
active O
context O
( O
/vectorAC O
) O
. O
3.3.4 O
Transformer O
- O
based O
Models O
We O
experiment O
with O
four O
Transformer O
models O
which O
have O
been O
shown O
to O
obtain O
state O
- O
of O
- O
the O
- O
art O
performances O
on O
several O
NLP O
benchmarks O
. O
The O
popular O
BERT O
model O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
was O
the O
Ô¨Årst O
to O
adopt O
the O
bidirectional O
training O
of O
Transformer O
for O
a O
language O
modeling O
task O
. O
To O
make O
this O
kind O
of O
training O
possible O
, O
BERT O
introduced O
a O
masked O
language O
modeling O
objective O
function O
: O
random O
words O
in O
the O
input O
sentences O
are O
replaced O
by O
a O
[ O
MASK O
] O
token O
and O
the O
model O
attempts O
to O
predict O
the O
masked O
token O
based O
on O
the O
surrounding O
context O
. O
Simultaneously O
, O
BERT O
is O
optimized O
on O
a O
next O
sentence O
prediction O
task O
, O
as O
the O
model O
receives O
sentence O
pairs O
in O
input O
and O
has O
to O
predict O
whether O
the O
second O
sentence O
is O
subsequent O
to O
the228Ô¨Årst O
one O
in O
the O
training O
data.5BERT O
has O
been O
trained O
on O
a O
concatenation O
of O
the O
BookCorpus O
and O
the O
English O
Wikipedia O
, O
for O
a O
total O
of O
3300 O
M O
tokens O
ca O
. O
In O
our O
experiments O
, O
we O
used O
the O
larger O
pre O
- O
trained O
version O
, O
called O
BERT O
- O
large O
- O
cased O
. O
RoBERTa O
( O
Liu O
et O
al O
. O
, O
2019 O
) O
has O
the O
same O
architecture O
as O
BERT O
, O
but O
it O
introduces O
several O
parameter O
optimization O
choices O
: O
it O
makes O
use O
of O
dynamic O
masking O
( O
compared O
to O
the O
static O
masking O
of O
the O
original O
model O
) O
, O
of O
a O
larger O
batch O
- O
size O
and O
a O
larger O
vocabulary O
size O
. O
Moreover O
, O
the O
input O
consists O
of O
complete O
sentences O
randomly O
extracted O
from O
one O
or O
multiple O
documents O
, O
and O
the O
next O
sentence O
prediction O
objective O
is O
removed O
. O
Besides O
the O
optimized O
design O
choice O
, O
another O
key O
difference O
of O
RoBERTa O
with O
the O
other O
models O
is O
the O
larger O
training O
corpus O
, O
which O
consists O
of O
a O
concatenation O
of O
the O
BookCorpus O
, O
CCNEWS O
, O
OpenWebText O
, O
and O
STORIES O
. O
With O
a O
total O
160 O
GB O
of O
text O
, O
RoBERTa O
has O
access O
to O
more O
potential O
knowledge O
than O
the O
other O
models O
. O
For O
our O
tests O
, O
we O
used O
the O
large O
pre O
- O
trained O
model O
. O
XLNet O
( O
Yang O
et O
al O
. O
, O
2019 O
) O
is O
a O
generalized O
autoregressive O
( O
AR O
) O
pretraining O
method O
which O
uses O
the O
context O
words O
to O
predict O
the O
next O
word O
. O
The O
AR O
architecture O
is O
constrained O
to O
a O
single O
direction O
( O
either O
forward O
or O
backwards O
) O
, O
that O
is O
, O
context O
representation O
takes O
in O
consideration O
only O
the O
tokens O
to O
the O
left O
or O
to O
the O
right O
of O
the O
i O
- O
th O
position O
, O
while O
BERT O
representation O
has O
access O
to O
the O
contextual O
information O
on O
both O
sides O
. O
To O
capture O
bidirectional O
contexts O
, O
XLNet O
is O
trained O
with O
a O
permutation O
method O
as O
language O
modeling O
objective O
, O
where O
all O
tokens O
are O
predicted O
but O
in O
random O
order O
. O
XLNet O
‚Äôs O
training O
corpora O
were O
the O
same O
as O
BERT O
plus O
Giga5 O
, O
ClueWeb O
2012 O
- O
B O
and O
Common O
Crawl O
, O
for O
a O
total O
of O
32.89B O
subword O
piece O
. O
Also O
in O
this O
case O
, O
we O
used O
the O
large O
pre O
- O
trained O
model O
. O
GPT-2 O
( O
Radford O
et O
al O
. O
, O
2019 O
) O
, O
a O
variation O
of O
GPT O
, O
is O
a O
uni O
- O
directional O
transformer O
language O
model O
, O
which O
means O
that O
the O
training O
objective O
is O
to O
predict O
the O
next O
word O
, O
given O
all O
of O
the O
previous O
words O
. O
Compared O
with O
GPT O
, O
GPT-2 O
optimizes O
the O
layer O
normalization O
, O
expands O
the O
vocabulary O
size O
to O
50,257 O
, O
increases O
the O
context O
size O
from O
512 O
to O
1024 O
tokens O
, O
and O
optimizes O
with O
a O
larger O
batch O
size O
of O
512 O
. O
In O
addition O
, O
GPT-2 O
is O
pre O
- O
trained O
on O
WebText O
, O
which O
was O
created O
by O
scraping O
web O
pages O
, O
for O
a O
total O
of O
8 O
million O
documents O
of O
data O
( O
40 O
GB O
) O
. O
We O
5Notice O
that O
the O
usefulness O
of O
this O
secondary O
objective O
function O
was O
questioned O
, O
and O
it O
was O
indeed O
removed O
in O
more O
recent O
models O
( O
Yang O
et O
al O
. O
, O
2019 O
; O
Liu O
et O
al O
. O
, O
2019 O
; O
Joshi O
et O
al O
. O
, O
2020).used O
the O
XL O
version O
of O
GPT-2 O
for O
our O
experiments O
. O
The O
parameters O
of O
the O
Transformer O
models O
are O
reported O
in O
Table O
3 O
. O
BERT O
, O
RoBERTa O
and O
XLNet O
are O
used O
to O
perform O
a O
word O
prediction O
task O
: O
given O
a O
sentence O
and O
a O
masked O
word O
in O
position O
k O
, O
they O
compute O
the O
probability O
of O
a O
word O
wkgiven O
the O
context O
k O
: O
P(wi|context O
k O
) O
. O
For O
our O
experiments O
, O
the O
context O
is O
the O
entire O
sentence O
Swith O
the O
k O
- O
th O
word O
( O
the O
covert O
event O
) O
being O
replaced O
by O
a O
special O
token O
‚Äò O
[ O
MASK O
] O
‚Äô O
. O
Therefore O
, O
we O
turned O
the O
test O
tuples O
into O
full O
sentences O
, O
masking O
the O
verb O
as O
in O
the O
example O
below O
: O
The O
architect O
Ô¨Ånishes O
[ O
MASK O
] O
house.6We O
then O
compute O
the O
probability O
of O
a O
hidden O
verb O
to O
occur O
in O
that O
position O
, O
and O
we O
expect O
the O
preferred O
verb O
to O
get O
a O
high O
value O
. O
We O
performed O
this O
task O
using O
the O
packages O
of O
the O
HappyTransformer O
library.7 O
As O
GPT-2 O
works O
as O
a O
traditional O
language O
model O
, O
we O
adopted O
this O
model O
to O
calculate O
the O
probability O
of O
the O
entire O
sentence O
( O
instead O
of O
the O
probability O
of O
the O
hidden O
verb O
given O
the O
context O
) O
. O
In O
this O
case O
, O
we O
expect O
that O
sentences O
evoking O
more O
typical O
events O
get O
higher O
values O
. O
We O
adopted O
the O
lm O
- O
scorer O
package O
to O
compute O
sentence O
probabilities.8 O
4 O
Evaluation O
Results O
Table O
5 O
and O
4 O
report O
the O
Ô¨Ånal O
evaluation O
scores O
. O
The O
performance O
of O
the O
probabilistic O
model O
is O
in O
line O
with O
previous O
studies O
, O
and O
it O
outperforms O
distributional O
models O
in O
some O
cases O
, O
proving O
that O
it O
is O
indeed O
a O
hard O
baseline O
to O
beat O
. O
However O
, O
accuracy O
and O
correlation O
are O
computed O
only O
on O
a O
subgroup O
of O
the O
test O
items O
: O
actually O
, O
the O
model O
covers O
about O
60 O
% O
of O
the O
datasets O
‚Äô O
tuples O
( O
86.8 O
% O
for O
L&L O
) O
, O
as O
we O
reported O
in O
Table O
2 O
. O
Coverage O
is O
the O
main O
issue O
probabilistic O
models O
have O
to O
face O
( O
Zarcone O
et O
al O
. O
, O
2013 O
) O
, O
while O
distributional O
models O
do O
not O
experience O
such O
limitation O
. O
Regarding O
the O
thematic O
Ô¨Åt O
models O
, O
we O
observe O
that O
there O
is O
no O
difference O
between O
the O
TF O
- O
add O
and O
TF O
- O
prod O
models O
, O
as O
they O
obtain O
similar O
scores O
. O
6One O
of O
the O
anonymous O
reviewers O
argues O
that O
the O
performance O
of O
the O
Transformer O
- O
based O
models O
might O
be O
inÔ¨Çuenced O
by O
the O
prompt O
sentence O
and O
suggest O
more O
variations O
of O
the O
input O
sentences O
. O
We O
indeed O
tested O
several O
manipulations O
of O
the O
inputs O
before O
feeding O
them O
to O
the O
transformers O
, O
changing O
1 O
) O
the O
tense O
of O
the O
metonymic O
verb O
( O
using O
the O
past O
tense O
) O
and O
2 O
) O
the O
number O
of O
the O
direct O
object O
( O
we O
used O
the O
plurals O
of O
the O
dataset O
nouns O
) O
. O
However O
, O
the O
results O
did O
not O
show O
any O
consistent O
trend O
. O
7https://github.com/EricFillion/ O
happy O
- O
transformer O
8https://pypi.org/project/lm-scorer/229Probabilistic O
Distributional O
Transformer O
- O
based O
SOp O
TF O
- O
add O
TF O
- O
prod O
SDM O
BERT O
RoBERTa O
XLNet O
GPT-2 O
MC O
0.68 O
0.70 O
0.73 O
0.77 O
0.70 O
0.80 O
0.40 O
0.87 O
TR O
0.48 O
0.53 O
0.53 O
0.72 O
0.47 O
0.72 O
0.39 O
0.69 O
O. O
P. O
0.58 O
0.62 O
0.63 O
0.75 O
0.59 O
0.76 O
0.40 O
0.78 O
Table O
4 O
: O
Results O
for O
binary O
classiÔ¨Åcation O
task O
. O
Probabilistic O
Distributional O
Transformer O
- O
based O
SOp O
TF O
- O
add O
TF O
- O
prod O
SDM O
BERT O
RoBERTa O
XLNet O
GPT-2 O
L&L O
0.53 O
0.41 O
0.41 O
0.53 O
0.61 O
0.73 O
0.04 O
0.43 O
CE O
0.36 O
0.26 O
0.22 O
0.40 O
0.27 O
0.39 O
0.18 O
0.31 O
O. O
P. O
0.45 O
0.34 O
0.32 O
0.47 O
0.44 O
0.56 O
0.11 O
0.37 O
Table O
5 O
: O
Results O
for O
correlation O
task O
. O
However O
, O
we O
need O
to O
point O
out O
that O
, O
when O
the O
system O
computes O
the O
intersection O
of O
the O
two O
lists O
of O
the O
top O
verbs O
for O
subjects O
and O
objects O
, O
sometimes O
the O
number O
of O
retrieved O
items O
is O
less O
than O
20(the O
model O
parameter O
for O
the O
verb O
embedding O
selection O
, O
cf O
. O
Section O
3.3.2 O
) O
. O
Therefore O
, O
independently O
of O
the O
selected O
function O
, O
the O
verbs O
used O
to O
compute O
the O
prototypical O
vector O
are O
eventually O
all O
those O
belonging O
to O
the O
intersection O
. O
Moreover O
, O
TF O
- O
models O
are O
often O
close O
to O
, O
and O
never O
signiÔ¨Åcantly O
outperform O
the O
probabilistic O
baseline O
. O
Among O
the O
distributional O
models O
, O
SDM O
is O
the O
one O
that O
obtains O
a O
considerable O
performance O
across O
all O
the O
datasets O
. O
This O
model O
performs O
close O
to O
RoBERTa O
both O
in O
the O
Traxler O
and O
in O
the O
CE O
dataset O
. O
This O
result O
is O
surprising O
, O
considering O
that O
SDM O
is O
trained O
just O
on O
a O
dump O
of O
Wikipedia O
, O
while O
RoBERTa O
is O
trained O
on O
160 O
GB O
of O
text O
and O
implements O
advanced O
deep O
learning O
techniques O
. O
This O
outcome O
conÔ¨Årms O
that O
SDM O
, O
which O
has O
been O
designed O
to O
represent O
event O
knowledge O
and O
the O
dynamic O
construction O
of O
sentence O
meaning O
, O
is O
able O
to O
adequately O
model O
the O
typicality O
of O
events O
. O
This O
aspect O
has O
been O
suggested O
to O
be O
one O
of O
the O
core O
components O
of O
the O
language O
processing O
system O
( O
Baggio O
and O
Hagoort O
, O
2011 O
; O
Baggio O
et O
al O
. O
, O
2012 O
; O
Chersoni O
et O
al O
. O
, O
2019 O
) O
. O
On O
the O
other O
hand O
, O
Transformers O
also O
provided O
interesting O
results O
. O
RoBERTa O
achieves O
the O
best O
score O
for O
the O
L&L O
dataset O
, O
reaching O
a O
statistical O
signiÔ¨Åcance O
of O
the O
improvement O
over O
SDM O
( O
p O
< O
0.01).9More O
importantly O
, O
it O
is O
the O
only O
Transformer O
that O
consistently O
obtains O
good O
results O
across O
all O
datasets O
, O
while O
the O
scores O
from O
other O
9Thep O
- O
value O
is O
computed O
with O
Fisher O
‚Äôs O
r O
- O
to O
- O
z O
transformation O
, O
one O
- O
tailed O
test O
. O
Transformer O
models O
are O
highly O
Ô¨Çuctuating O
. O
We O
believe O
that O
the O
gigantic O
size O
of O
the O
training O
corpus O
is O
a O
factor O
that O
positively O
affects O
its O
performance O
. O
At O
the O
same O
time O
, O
GPT-2 O
achieves O
the O
highest O
score O
for O
MC O
dataset O
( O
0.87 O
) O
( O
but O
the O
improvement O
over O
RoBERTa O
and O
SDM O
does O
not O
reach O
statistical O
signiÔ¨Åcance O
) O
, O
although O
it O
performs O
signiÔ¨Åcantly O
lower O
on O
the O
other O
benchmarks10 O
. O
For O
the O
sake O
of O
completeness O
, O
we O
also O
report O
the O
overall O
performance O
of O
each O
model O
over O
the O
two O
tasks O
. O
Results O
identify O
RoBERTa O
and O
GPT-2 O
as O
the O
best O
models O
for O
the O
correlation O
and O
classiÔ¨Åcation O
tasks O
, O
respectively O
. O
However O
, O
we O
wonder O
if O
the O
average O
score O
is O
a O
valid O
measure O
to O
identify O
the O
best O
model O
. O
These O
two O
models O
tend O
to O
have O
a O
wavering O
behavior O
, O
which O
results O
in O
large O
differences O
between O
the O
two O
datasets O
scores O
. O
SpeciÔ¨Åcally O
, O
Roberta O
achieves O
0.75 O
for O
the O
L&L O
dataset O
, O
but O
only O
0.39 O
for O
the O
CE O
one O
, O
with O
0.36 O
points O
of O
difference O
. O
Similarly O
, O
GPT-2 O
reaches O
0.89 O
scores O
for O
the O
MC O
dataset O
, O
but O
its O
performance O
goes O
down O
by O
0.16 O
. O
On O
the O
contrary O
, O
SDM O
behavior O
is O
more O
stable O
, O
with O
a O
smaller O
gap O
between O
the O
two O
datasets O
‚Äô O
scores O
( O
0.13 O
point O
difference O
for O
the O
correlation O
task O
and O
just O
0.05 O
for O
the O
accuracy O
task O
) O
. O
4.1 O
Error O
analysis O
Binary O
classiÔ¨Åcation O
task O
For O
the O
MC O
and O
TR O
datasets O
, O
we O
evaluate O
the O
models O
for O
their O
capability O
of O
assigning O
a O
higher O
probability O
to O
the O
verb O
in O
the O
typical O
condition O
. O
It O
is O
important O
to O
empha10We O
determine O
the O
signiÔ¨Åcance O
of O
differences O
between O
models O
for O
MC O
and O
TR O
datasets O
with O
a O
McNemar O
‚Äôs O
Chi O
- O
Square O
Test O
, O
applied O
to O
a O
2x2 O
contingency O
matrix O
containing O
the O
number O
of O
correct O
and O
incorrect O
answers O
( O
replicating O
the O
approach O
of O
Zarcone O
et O
al O
. O
( O
2012)).230size O
that O
both O
verbs O
are O
plausible O
in O
the O
context O
, O
but O
one O
describes O
a O
more O
likely O
event O
given O
the O
subject O
and O
the O
object O
. O
This O
remark O
is O
essential O
, O
because O
it O
explains O
the O
performance O
of O
all O
models O
, O
distributional O
and O
Transformer O
ones O
. O
To O
identify O
which O
tuples O
are O
the O
most O
difÔ¨Åcult O
ones O
, O
we O
built O
a O
heat O
map O
visualizing O
the O
correctlypredicted O
ones O
in O
blue O
, O
and O
the O
wrong O
ones O
in O
yellow O
( O
see O
Figures O
1 O
and O
2 O
) O
. O
We O
do O
not O
consider O
the O
accuracy O
values O
obtained O
by O
the O
probabilistic O
model O
for O
its O
partial O
coverage O
. O
Figure O
1 O
: O
Heat O
map O
for O
error O
analysis O
over O
MC O
dataset O
. O
Figure O
2 O
: O
Heat O
map O
for O
error O
analysis O
over O
TR O
dataset O
. O
This O
visualization O
technique O
reveals O
that O
some O
pairs O
are O
never O
predicted O
correctly O
, O
corresponding O
to O
the O
fully O
vertical O
yellow O
lines O
in O
the O
Ô¨Ågures O
. O
In O
what O
follow O
we O
report O
the O
tuples O
that O
are O
consistently O
mistaken O
for O
MC O
( O
1 O
) O
and O
TR O
( O
2 O
) O
datasets O
. O
( O
1 O
) O
a. O
The O
teenager O
starts O
the O
novel O
. O
b. O
The O
worker O
begins O
the O
memo.(2 O
) O
a. O
The O
editor O
Ô¨Ånishes O
the O
newspaper O
. O
b. O
The O
director O
starts O
the O
script O
. O
c. O
The O
teenager O
begins O
the O
novel O
. O
In O
all O
the O
above O
cases O
, O
a O
model O
must O
discriminate O
between O
the O
verb O
read O
( O
HIGH O
TYP O
) O
and O
write O
( O
LOW O
TYP).11It O
is O
interesting O
to O
notice O
that O
, O
for O
many O
of O
the O
read O
- O
write O
pairs O
in O
the O
binary O
classiÔ¨Åcation O
data O
, O
the O
production O
frequencies O
of O
typical O
and O
atypical O
verb O
are O
much O
closer O
than O
on O
average O
, O
suggesting O
that O
the O
interpretation O
requires O
understanding O
of O
subtle O
nuances O
of O
context O
- O
sensitive O
typicality O
, O
which O
might O
not O
be O
trivial O
even O
for O
humans O
. O
Furthermore O
, O
in O
Figure O
2 O
we O
observe O
that O
for O
two O
TR O
‚Äôs O
pairs O
, O
SDM O
is O
the O
only O
one O
picking O
the O
right O
choice O
: O
The O
stylist O
starts O
the O
braid O
andThe O
auditor O
begins O
the O
taxes O
. O
It O
seems O
that O
models O
regularly O
tend O
to O
prefer O
a O
verb O
with O
a O
more O
generic O
and O
undetermined O
meaning O
( O
make O
anddo O
, O
respectively O
) O
, O
while O
only O
SDM O
correctly O
assigns O
the O
HIGH O
TYP O
class O
to O
the O
verbs O
that O
indicate O
more O
precisely O
the O
manner O
of O
doing O
something O
( O
braid O
andaudit O
) O
. O
On O
the O
other O
hand O
, O
GPT-2 O
and O
RoBERTa O
managed O
to O
pick O
the O
right O
choice O
for O
a O
few O
of O
the O
readwrite O
items O
on O
which O
SDM O
is O
mistaken O
. O
Correlation O
task O
Correlation O
is O
a O
more O
complex O
task O
compared O
to O
classiÔ¨Åcation O
, O
as O
the O
lower O
scores O
also O
reveal O
. O
To O
better O
understand O
our O
results O
, O
we O
select O
the O
best O
model O
for O
the O
CE O
( O
i.e. O
, O
SDM O
) O
and O
L&L O
( O
i.e. O
, O
RoBERTa O
) O
datasets O
, O
and O
we O
plot O
the O
linear O
relationship O
between O
the O
human O
ratings O
and O
the O
model O
- O
derived O
probabilities.12For O
CE O
, O
Figure O
3 O
reveals O
1 O
) O
a O
small O
positive O
correlation O
between O
the O
two O
variables O
, O
2 O
) O
a O
large O
amount O
of O
variance O
, O
and O
3 O
) O
a O
few O
outliers O
. O
As O
for O
L&L O
in O
Figure O
4 O
, O
the O
majority O
of O
the O
points O
follow O
a O
roughly O
linear O
relationship O
, O
and O
there O
is O
a O
small O
variation O
around O
the O
trend O
. O
Nevertheless O
, O
this O
result O
could O
be O
inÔ¨Çuenced O
by O
the O
form O
of O
the O
input O
sentences O
. O
For O
all O
the O
other O
datasets O
, O
we O
masked O
the O
token O
between O
the O
verb O
and O
the O
object O
, O
and O
the O
corresponding O
hidden O
verb O
had O
to O
be O
in O
the O
progressive O
form O
( O
The O
chef O
starts O
[ O
cooking O
] O
dinner O
) O
. O
For O
L&L O
, O
instead O
, O
we O
chose O
to O
insert O
the O
prepositiontoafter O
the O
verb O
since O
lots O
of O
the O
metonymic O
verbs O
( O
want O
, O
try O
, O
etc O
. O
) O
require O
to O
be O
followed O
by O
the O
inÔ¨Ånitive O
verb O
. O
Thus O
, O
the O
context O
gives O
a O
higher O
11Except O
for O
the O
sentence O
in O
2.a O
, O
where O
the O
typical O
verb O
is O
edit O
. O
12We O
apply O
the O
logarithmic O
transformation O
of O
data O
for O
visualization O
purposes.231probability O
to O
verbs O
as O
masked O
tokens O
, O
while O
different O
parts O
of O
speech O
could O
be O
equally O
plausible O
for O
the O
other O
conditions O
. O
Figure O
3 O
: O
SDM O
correlation O
for O
CE O
. O
Figure O
4 O
: O
RoBERTa O
correlation O
for O
L&L. O
5 O
Discussion O
and O
Conclusions O
In O
this O
paper O
, O
we O
have O
presented O
a O
comparative O
evaluation O
of O
several O
computational O
models O
on O
the O
task O
of O
logical O
metonymy O
interpretation O
. O
We O
frame O
this O
problem O
as O
the O
retrieval O
of O
an O
event O
that O
is O
not O
overtly O
expressed O
in O
the O
surface O
form O
of O
the O
sentence O
. O
According O
to O
Elman O
‚Äôs O
Words O
- O
as O
- O
Cues O
framework O
, O
human O
subjects O
can O
infer O
the O
covert O
event O
in O
logical O
metonymy O
thanks O
to O
the O
generalized O
knowledge O
about O
events O
and O
participants O
stored O
in O
their O
semantic O
memory O
. O
Hence O
, O
during O
sentence O
processing O
, O
words O
in O
the O
sentence O
create O
a O
network O
of O
mutual O
expectations O
that O
triggers O
the O
retrieval O
of O
typical O
events O
associated O
with O
lexical O
items O
and O
gen O
- O
erates O
expectations O
about O
the O
upcoming O
words O
( O
Elman O
, O
2014 O
) O
. O
To O
tackle O
the O
task O
of O
logical O
metonymy O
interpretation O
, O
computational O
models O
must O
be O
able O
to O
recover O
unexpressed O
relationships O
between O
the O
words O
, O
using O
a O
context O
- O
sensitive O
representation O
of O
meaning O
that O
captures O
this O
event O
knowledge O
. O
The O
most O
compelling O
outcome O
of O
the O
reported O
experiments O
is O
probably O
the O
performance O
of O
SDM O
, O
which O
achieves O
the O
best O
score O
for O
the O
TR O
and O
the O
CE O
datasets O
. O
These O
results O
demonstrate O
the O
signiÔ¨Åcance O
of O
encoding O
event O
structures O
outside O
the O
embeddings O
( O
which O
are O
treated O
as O
nodes O
in O
a O
distributional O
graph O
) O
, O
and O
the O
ability O
of O
the O
SDM O
compositional O
function O
to O
dynamically O
update O
the O
semantic O
representation O
for O
a O
sentence O
. O
However O
, O
the O
evaluation O
scores O
are O
not O
very O
high O
, O
especially O
in O
the O
correlation O
task O
. O
Results O
reveal O
that O
the O
contextualized O
information O
used O
by O
computational O
models O
is O
useful O
to O
recall O
plausible O
events O
connected O
to O
the O
arguments O
, O
but O
this O
is O
still O
not O
sufÔ¨Åcient O
. O
Even O
Transformer O
models O
, O
which O
currently O
report O
state O
- O
of O
- O
theart O
performances O
on O
several O
NLP O
benchmarks O
, O
are O
not O
performing O
signiÔ¨Åcantly O
better O
than O
the O
SDM O
model O
, O
which O
is O
trained O
on O
a O
smaller O
corpus O
and O
without O
any O
advanced O
deep O
learning O
technique O
. O
Error O
analysis O
highlights O
that O
they O
are O
able O
to O
identify O
the O
plausible O
scenarios O
in O
which O
the O
participants O
could O
occur O
, O
but O
they O
still O
struggle O
in O
perceiving O
different O
nuances O
of O
typicality O
. O
Our O
experiments O
show O
how O
the O
logical O
metonymy O
task O
can O
be O
seen O
as O
a O
testing O
ground O
to O
check O
whether O
computational O
models O
encode O
common O
- O
sense O
event O
knowledge O
. O
Future O
work O
might O
follow O
two O
directions O
. O
On O
the O
one O
hand O
, O
expanding O
the O
coverage O
of O
the O
graph O
could O
favourably O
increase O
the O
performance O
of O
SDM O
. O
On O
the O
other O
hand O
, O
Transformer O
models O
could O
be O
tested O
with O
new O
experimental O
settings O
, O
such O
as O
the O
Ô¨Åne O
- O
tuning O
of O
the O
pre O
- O
trained O
weights O
on O
thematic O
Ô¨Åt O
- O
related O
( O
Lenci O
, O
2011 O
; O
Sayeed O
et O
al O
. O
, O
2016 O
; O
Santus O
et O
al O
. O
, O
2017 O
) O
or O
semantic O
role O
classiÔ¨Åcation O
tasks O
( O
Collobert O
et O
al O
. O
, O
2011 O
; O
Zapirain O
et O
al O
. O
, O
2013 O
; O
Roth O
and O
Lapata O
, O
2015 O
) O
. O
6 O
Acknowledgements O
This O
work O
, O
carried O
out O
within O
the O
Institut O
Convergence O
ILCB O
( O
ANR-16 O
- O
CONV-0002 O
) O
, O
has O
beneÔ¨Åted O
from O
support O
from O
the O
French O
government O
, O
managed O
by O
the O
French O
National O
Agency O
for O
Research O
( O
ANR O
) O
and O
the O
Excellence O
Initiative O
of O
AixMarseille O
University O
( O
A*MIDEX O
) O
. O
We O
thank O
the O
anonymous O
reviewers O
for O
their O
insightful O
feedback.232References O
Nicholas O
Asher O
. O
2015 O
. O
Types O
, O
Meanings O
and O
Coercions O
in O
Lexical O
Semantics O
. O
Lingua O
, O
157:66‚Äì82 O
. O
Giosu O
` O
e O
Baggio O
and O
Peter O
Hagoort O
. O
2011 O
. O
The O
Balance O
between O
Memory O
and O
UniÔ¨Åcation O
in O
Semantics O
: O
A O
Dynamic O
Account O
of O
the O
N400 O
. O
Language O
and O
Cognitive O
Processes O
, O
26(9):1338‚Äì1367 O
. O
Giosu O
` O
e O
Baggio O
, O
Michiel O
Van O
Lambalgen O
, O
and O
Peter O
Hagoort O
. O
2012 O
. O
The O
Processing O
Consequences O
of O
Compositionality O
. O
The O
Oxford O
Handbook O
of O
Compositionality O
. O
Oxford O
, O
pages O
657‚Äì674 O
. O
Piotr O
Bojanowski O
, O
Edouard O
Grave O
, O
Armand O
Joulin O
, O
and O
Tomas O
Mikolov O
. O
2017 O
. O
Enriching O
Word O
Vectors O
with O
Subword O
Information O
. O
Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
5:135‚Äì146 O
. O
Emmanuele O
Chersoni O
, O
Alessandro O
Lenci O
, O
and O
Philippe O
Blache O
. O
2017 O
. O
Logical O
Metonymy O
in O
a O
Distributional O
Model O
of O
Sentence O
Comprehension O
. O
In O
Proceedings O
of O
* O
SEM O
. O
Emmanuele O
Chersoni O
, O
Enrico O
Santus O
, O
Ludovica O
Pannitto O
, O
Alessandro O
Lenci O
, O
Philippe O
Blache O
, O
and O
C O
- O
R O
Huang O
. O
2019 O
. O
A O
Structured O
Distributional O
Model O
of O
Sentence O
Meaning O
and O
Processing O
. O
Natural O
Language O
Engineering O
, O
25(4):483‚Äì502 O
. O
Ronan O
Collobert O
, O
Jason O
Weston O
, O
L O
¬¥ O
eon O
Bottou O
, O
Michael O
Karlen O
, O
Koray O
Kavukcuoglu O
, O
and O
Pavel O
Kuksa O
. O
2011 O
. O
Natural O
Language O
Processing O
( O
Almost O
) O
from O
Scratch O
. O
Journal O
of O
Machine O
Learning O
Research O
, O
12(Aug):2493‚Äì2537 O
. O
Francesca O
Delogu O
, O
Matthew O
W O
Crocker O
, O
and O
Heiner O
Drenhaus O
. O
2017 O
. O
Teasing O
Apart O
Coercion O
and O
Surprisal O
: O
Evidence O
from O
Eye O
- O
Movements O
and O
ERPs O
. O
Cognition O
, O
161:46‚Äì59 O
. O
Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O
BERT O
: O
Pre O
- O
training O
of O
Deep O
Bidirectional O
Transformers O
for O
Language O
Understanding O
. O
In O
Proceedings O
of O
NAACL O
- O
HLT O
2019 O
, O
Minneapolis O
, O
MN O
. O
Jeffrey O
L O
Elman O
. O
2009 O
. O
On O
the O
Meaning O
of O
Words O
and O
Dinosaur O
Bones O
: O
Lexical O
Knowledge O
without O
a O
Lexicon O
. O
Cognitive O
Science O
, O
33(4):547‚Äì582 O
. O
Jeffrey O
L O
Elman O
. O
2014 O
. O
Systematicity O
in O
the O
Lexicon O
: O
On O
Having O
your O
Cake O
and O
Eating O
It O
Too O
. O
In O
Paco O
Calvo O
and O
John O
Symons O
, O
editors O
, O
The O
Architecture O
of O
Cognition O
: O
Rethinking O
Fodor O
and O
Pylyshyn O
‚Äôs O
Systematicity O
Challenge O
. O
The O
MIT O
Press O
, O
Cambridge O
, O
MA O
. O
Stefan O
Evert O
. O
2008 O
. O
Corpora O
and O
collocations O
. O
Corpus O
linguistics O
. O
An O
international O
handbook O
, O
2:1212 O
‚Äì O
1248 O
. O
Steven O
Frisson O
and O
Brian O
McElree O
. O
2008 O
. O
Complement O
Coercion O
is O
not O
Modulated O
by O
Competition O
: O
Evidence O
from O
Eye O
Movements O
. O
Journal O
of O
Experimental O
Psychology O
: O
Learning O
, O
Memory O
, O
and O
Cognition O
, O
34(1):1‚Äì11.Mandar O
Joshi O
, O
Danqi O
Chen O
, O
Yinhan O
Liu O
, O
Daniel O
S O
Weld O
, O
Luke O
Zettlemoyer O
, O
and O
Omer O
Levy O
. O
2020 O
. O
Spanbert O
: O
Improving O
Pre O
- O
training O
by O
Representing O
and O
Predicting O
Spans O
. O
Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
8:64‚Äì77 O
. O
Hans O
Kamp O
. O
2013 O
. O
Meaning O
and O
the O
Dynamics O
of O
Interpretation O
: O
Selected O
Papers O
by O
Hans O
Kamp O
. O
Brill O
, O
Leiden O
- O
Boston O
. O
Mirella O
Lapata O
and O
Alex O
Lascarides O
. O
2003 O
. O
A O
Probabilistic O
Account O
of O
Logical O
Metonymy O
. O
Computational O
Linguistics O
, O
29(2):261‚Äì315 O
. O
Alessandro O
Lenci O
. O
2011 O
. O
Composing O
and O
Updating O
Verb O
Argument O
Expectations O
: O
A O
Distributional O
Semantic O
Model O
. O
In O
Proceedings O
of O
the O
ACL O
Workshop O
on O
Cognitive O
Modeling O
and O
Computational O
Linguistics O
. O
Alessandro O
Lenci O
. O
2018 O
. O
Distributional O
Models O
of O
Word O
Meaning O
. O
Annual O
Review O
of O
Linguistics O
, O
4:151‚Äì171 O
. O
Yinhan O
Liu O
, O
Myle O
Ott O
, O
Naman O
Goyal O
, O
Jingfei O
Du O
, O
Mandar O
Joshi O
, O
Danqi O
Chen O
, O
Omer O
Levy O
, O
Mike O
Lewis O
, O
Luke O
Zettlemoyer O
, O
and O
Veselin O
Stoyanov O
. O
2019 O
. O
Roberta O
: O
A O
Robustly O
Optimized O
BERT O
Pretraining O
Approach O
. O
arXiv O
preprint O
arXiv:1907.11692 O
. O
Christopher O
Manning O
, O
Mihai O
Surdeanu O
, O
John O
Bauer O
, O
Jenny O
Finkel O
, O
Steven O
Bethard O
, O
and O
David O
McClosky O
. O
2014 O
. O
The O
Stanford O
CoreNLP O
natural O
language O
processing O
toolkit O
. O
In O
Proceedings O
of O
52nd O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
System O
Demonstrations O
, O
pages O
55‚Äì60 O
, O
Baltimore O
, O
Maryland O
. O
Association O
for O
Computational O
Linguistics O
. O
Brian O
McElree O
, O
Matthew O
J O
Traxler O
, O
Martin O
J O
Pickering O
, O
Rachel O
E O
Seely O
, O
and O
Ray O
Jackendoff O
. O
2001 O
. O
Reading O
Time O
Evidence O
for O
Enriched O
Composition O
. O
Cognition O
, O
78 O
: O
B17 O
‚Äì O
B25 O
. O
Ken O
McRae O
and O
Kazunaga O
Matsuki O
. O
2009 O
. O
People O
Use O
their O
Knowledge O
of O
Common O
Events O
to O
Understand O
Language O
, O
and O
Do O
So O
as O
Quickly O
as O
Possible O
. O
Language O
and O
Linguistics O
Compass O
, O
3(6):1417‚Äì1429 O
. O
Matthew O
E O
Peters O
, O
Mark O
Neumann O
, O
Mohit O
Iyyer O
, O
Matt O
Gardner O
, O
Christopher O
Clark O
, O
Kenton O
Lee O
, O
and O
Luke O
Zettlemoyer O
. O
2018 O
. O
Deep O
Contextualized O
Word O
Representations O
. O
In O
Proceedings O
of O
NAACL O
. O
James O
Pustejovsky O
and O
Olga O
Batiukova O
. O
2019 O
. O
The O
Lexicon O
. O
Cambridge O
University O
Press O
. O
Alec O
Radford O
, O
Jeffrey O
Wu O
, O
Rewon O
Child O
, O
David O
Luan O
, O
Dario O
Amodei O
, O
and O
Ilya O
Sutskever O
. O
2019 O
. O
Language O
Models O
Are O
Unsupervised O
Multitask O
Learners O
. O
OpenAI O
Blog O
, O
1(8):9 O
. O
Michael O
Roth O
and O
Mirella O
Lapata O
. O
2015 O
. O
ContextAware O
Frame O
- O
Semantic O
Role O
Labeling O
. O
Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
3:449‚Äì460.233Enrico O
Santus O
, O
Emmanuele O
Chersoni O
, O
Alessandro O
Lenci O
, O
and O
Philippe O
Blache O
. O
2017 O
. O
Measuring O
Thematic O
Fit O
with O
Distributional O
Feature O
Overlap O
. O
In O
Proceedings O
of O
EMNLP O
. O
Asad O
Sayeed O
, O
Clayton O
Greenberg O
, O
and O
Vera O
Demberg O
. O
2016 O
. O
Thematic O
Fit O
Evaluation O
: O
An O
Aspect O
of O
Selectional O
Preferences O
. O
In O
Proceedings O
of O
the O
ACL O
Workshop O
on O
Evaluating O
Vector O
Space O
Representations O
for O
NLP O
. O
Ekaterina O
Shutova O
. O
2009 O
. O
Sense O
- O
Based O
Interpretation O
of O
Logical O
Metonymy O
Using O
a O
Statistical O
Method O
. O
In O
Proceedings O
of O
the O
ACL O
- O
IJCNLP O
2009 O
Student O
Research O
Workshop O
, O
pages O
1‚Äì9 O
. O
Stanley O
S O
Stevens O
. O
1957 O
. O
On O
the O
Psychophysical O
Law O
. O
Psychological O
review O
, O
64(3):153 O
. O
Matthew O
J O
Traxler O
, O
Martin O
J O
Pickering O
, O
and O
Brian O
McElree O
. O
2002 O
. O
Coercion O
in O
Sentence O
Processing O
: O
Evidence O
from O
Eye O
- O
Movements O
and O
Self O
- O
Paced O
Reading O
. O
Journal O
of O
Memory O
and O
Language O
, O
47(4):530 O
‚Äì O
547 O
. O
Peter O
D O
Turney O
and O
Patrick O
Pantel O
. O
2010 O
. O
From O
Frequency O
to O
Meaning O
: O
Vector O
Space O
Models O
of O
Semantics O
. O
Journal O
of O
ArtiÔ¨Åcial O
Intelligence O
Research O
, O
37:141‚Äì188 O
. O
Ashish O
Vaswani O
, O
Noam O
Shazeer O
, O
Niki O
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion O
Jones O
, O
Aidan O
N O
Gomez O
, O
≈Åukasz O
Kaiser O
, O
and O
Illia O
Polosukhin O
. O
2017 O
. O
Attention O
Is O
All O
You O
Need O
. O
In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
, O
pages O
5998‚Äì6008 O
. O
Zhilin O
Yang O
, O
Zihang O
Dai O
, O
Yiming O
Yang O
, O
Jaime O
Carbonell O
, O
Ruslan O
Salakhutdinov O
, O
and O
Quoc O
V O
Le O
. O
2019 O
. O
XLNet O
: O
Generalized O
Autoregressive O
Pretraining O
for O
Language O
Understanding O
. O
arXiv O
preprint O
arXiv:1906.08237 O
. O
Benat O
Zapirain O
, O
Eneko O
Agirre O
, O
Lluis O
Marquez O
, O
and O
Mihai O
Surdeanu O
. O
2013 O
. O
Selectional O
Preferences O
for O
Semantic O
Role O
ClassiÔ¨Åcation O
. O
Computational O
Linguistics O
, O
39(3):631‚Äì663 O
. O
Alessandra O
Zarcone O
, O
Alessandro O
Lenci O
, O
Sebastian O
Pad¬¥o O
, O
and O
Jason O
Utt O
. O
2013 O
. O
Fitting O
, O
not O
Clashing O
! O
A O
Distributional O
Semantic O
Model O
of O
Logical O
Metonymy O
. O
In O
Proceedings O
of O
IWCS O
. O
Alessandra O
Zarcone O
and O
Sebastian O
Pad O
¬¥ O
o. O
2011 O
. O
Generalized O
Event O
Knowledge O
in O
Logical O
Metonymy O
Resolution O
. O
In O
Proceedings O
of O
CogSci O
. O
Alessandra O
Zarcone O
, O
Sebastian O
Pad O
¬¥ O
o O
, O
and O
Alessandro O
Lenci O
. O
2014 O
. O
Logical O
Metonymy O
Resolution O
in O
a O
Words O
- O
as O
- O
Cues O
Framework O
: O
Evidence O
from O
Selfpaced O
Reading O
and O
Probe O
Recognition O
. O
Cognitive O
Science O
, O
38(5):973‚Äì996 O
. O
Alessandra O
Zarcone O
, O
Jason O
Utt O
, O
and O
Sebastian O
Pad O
¬¥ O
o. O
2012 O
. O
Modeling O
Covert O
Event O
Retrieval O
in O
Logical O
Metonymy O
: O
Probabilistic O
and O
Distributional O
Accounts O
. O
In O
Proceedings O
of O
the O
NAACL O
Workshop O
on O
Cognitive O
Modeling O
and O
Computational O
Linguistics O
.234Proceedings O
of O
the O
1st O
Conference O
of O
the O
Asia O
- O
PaciÔ¨Åc O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
10th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
235‚Äì247 O
December O
4 O
- O
7 O
, O
2020 O
. O
¬© O
2020 O
Association O
for O
Computational O
Linguistics O
AMR O
Quality O
Rating O
with O
a O
Lightweight O
CNN O
Juri O
Opitz O
Dept O
. O
of O
Computational O
Linguistics O
Heidelberg O
University O
69120 O
Heidelberg O
opitz@cl.uni-heidelberg.de O
Abstract O
Structured O
semantic O
sentence O
representations O
such O
as O
Abstract O
Meaning O
Representations O
( O
AMRs O
) O
are O
potentially O
useful O
in O
various O
NLP O
tasks O
. O
However O
, O
the O
quality O
of O
automatic O
parses O
can O
vary O
greatly O
and O
jeopardizes O
their O
usefulness O
. O
This O
can O
be O
mitigated O
by O
models O
that O
can O
accurately O
rate O
AMR O
quality O
in O
the O
absence O
of O
costly O
gold O
data O
, O
allowing O
us O
to O
inform O
downstream O
systems O
about O
an O
incorporated O
parse O
‚Äôs O
trustworthiness O
or O
select O
among O
different O
candidate O
parses O
. O
In O
this O
work O
, O
we O
propose O
to O
transfer O
the O
AMR O
graph O
to O
the O
domain O
of O
images O
. O
This O
allows O
us O
to O
create O
a O
simple O
convolutional O
neural O
network O
( O
CNN O
) O
that O
imitates O
a O
human O
judge O
tasked O
with O
rating O
graph O
quality O
. O
Our O
experiments O
show O
that O
the O
method O
can O
rate O
quality O
more O
accurately O
than O
strong O
baselines O
, O
in O
several O
quality O
dimensions O
. O
Moreover O
, O
the O
method O
proves O
to O
be O
efÔ¨Åcient O
and O
reduces O
the O
incurred O
energy O
consumption O
. O
1 O
Introduction O
The O
goal O
of O
sentence O
meaning O
representations O
is O
to O
capture O
the O
meaning O
of O
sentences O
in O
a O
well O
- O
deÔ¨Åned O
format O
. O
One O
of O
the O
most O
prominent O
frameworks O
for O
achieving O
this O
is O
Abstract O
Meaning O
Representation O
( O
AMR O
) O
( O
Banarescu O
et O
al O
. O
, O
2013 O
) O
. O
In O
AMR O
, O
sentences O
are O
represented O
as O
directed O
acyclic O
and O
rooted O
graphs O
. O
An O
example O
is O
displayed O
in O
Figure O
1 O
, O
where O
we O
see O
three O
equivalent O
displays O
of O
an O
AMR O
that O
represents O
the O
meaning O
of O
the O
sentence O
‚Äú O
The O
baby O
is O
sleeping O
well O
‚Äù O
. O
In O
AMR O
, O
nodes O
are O
variables O
or O
concepts O
, O
while O
( O
labeled O
) O
edges O
express O
their O
relations O
. O
Among O
other O
phenomena O
, O
this O
allows O
AMR O
to O
capture O
coreference O
( O
via O
re O
- O
entrant O
structures O
) O
and O
semantic O
roles O
( O
via O
: O
argnrelation O
) O
. O
Furthermore O
, O
AMR O
links O
sentences O
to O
KBs O
: O
e.g. O
, O
predicates O
are O
mapped O
to O
PropBank O
( O
Palmer O
et O
al O
. O
, O
2005 O
; O
Kingsbury O
and O
Palmer O
, O
2002 O
) O
, O
while O
named O
ws O
bsleep-1 O
baby O
arg0mod O
   O
( O
s O
/ O
sleep-01 O
     O
: O
arg0 O
( O
b O
/ O
baby O
) O
     O
: O
mod O
( O
w O
/ O
well O
) O
) O
Penman O
notation O
well O
{ O
< O
s O
, O
instance O
, O
sleep O
> O
, O
    O
< O
s O
, O
arg0 O
, O
b O
> O
, O
   O
< O
b O
, O
instance O
, O
baby O
> O
, O
   O
< O
w O
, O
instance O
, O
well O
> O
, O
   O
< O
s O
, O
mod O
, O
w O
> O
} O
Triples O
Figure O
1 O
: O
Equivalent O
representations O
of O
the O
AMR O
for O
‚Äú O
The O
baby O
is O
sleeping O
well O
‚Äù O
. O
( O
p4 O
¬† O
/ O
¬† O
possible-01 O
: O
arg1 O
¬† O
( O
d5 O
¬† O
/ O
¬† O
destabilize-01 O
¬†¬†¬†¬†¬†¬†¬†¬† O
: O
arg0 O
[: O
arg1 O
] O
¬† O
( O
c3 O
¬† O
/ O
¬† O
country O
¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†       O
: O
quant O
¬† O
( O
w2 O
¬† O
/ O
¬† O
whole O
) O
) O
) O
¬†¬†¬†¬† O
: O
condition O
¬† O
( O
e1 O
¬† O
/ O
¬† O
economy O
   O
[: O
poss O
c3 O
] O
 ¬†¬†¬†¬†¬†¬†¬†¬† O
: O
arg0 O
- O
of O
¬† O
( O
f0 O
¬† O
/ O
¬† O
function-01 O
   O
[: O
pol O
- O
] O
) O
) O
) O
Figure O
2 O
: O
Parse O
of O
Without O
a O
functioning O
economy O
, O
the O
whole O
country O
may O
destabilize O
with O
errors O
outlined O
. O
entities O
are O
linked O
to O
Wikipedia O
. O
From O
a O
logical O
perspective O
, O
AMR O
is O
closely O
related O
to O
Ô¨Årst O
- O
order O
logic O
( O
FOL O
, O
see O
Bos O
( O
2016 O
, O
2019 O
) O
for O
translation O
mechanisms O
) O
. O
Currently O
, O
AMRs O
are O
leveraged O
to O
enhance O
a O
variety O
of O
natural O
language O
understanding O
tasks O
. O
E.g. O
, O
they O
have O
enhanced O
commonsense O
reasoning O
and O
question O
answering O
( O
Mitra O
and O
Baral O
, O
2016 O
) O
, O
machine O
translation O
( O
Song O
et O
al O
. O
, O
2019 O
) O
, O
text O
summarization O
( O
Liao O
et O
al O
. O
, O
2018 O
; O
Dohare O
et O
al O
. O
, O
2017 O
) O
and O
paraphrasing O
( O
Issa O
et O
al O
. O
, O
2018 O
) O
. O
However O
, O
there O
is O
a O
critical O
issue O
with O
automatically O
generated O
AMRs O
( O
parses O
): O
they O
are O
often O
deÔ¨Åcient O
. O
These O
deÔ¨Åciencies O
can O
be O
quite O
severe O
, O
even O
when O
high O
- O
performance O
parsers O
are O
used O
. O
For O
example O
, O
in O
Figure O
2 O
, O
a O
neural O
parser O
( O
Lyu O
and O
Titov O
, O
2018 O
) O
conducts O
several O
errors O
when O
parsing O
Without O
a O
functioning O
economy O
the O
whole O
country O
may O
destabilize O
. O
E.g. O
, O
it O
misses O
a O
negative O
polarity O
and O
classiÔ¨Åes O
a O
patient O
argument O
as O
the O
agent O
by O
failing235graph O
representation O
computer O
processing O
human O
understanding O
well O
- O
deÔ¨Åned O
triples O
(e.g O
. O
, O
GNN O
) O
 O
 O
graph O
visualization O
 O
 O
( O
short O
sentences O
) O
 O
PENMAN O
, O
linearized O
string O
(e.g O
. O
, O
LSTM O
) O
 O
 O
PENMAN O
, O
indents O
(this O
work O
) O
 O
 O
Table O
1 O
: O
Four O
( O
equivalent O
) O
AMR O
representations O
and O
their O
accessibility O
with O
respect O
to O
human O
or O
computer O
( O
 O
: O
‚Äò O
okay O
‚Äô O
, O
 O
: O
‚Äò O
perhaps O
possible O
, O
but O
difÔ¨Åcult O
‚Äô O
) O
. O
to O
see O
that O
destabilize O
here O
functions O
as O
an O
ergative O
verb O
( O
parser O
: O
the O
country O
is O
the O
causer O
of O
destabilize O
; O
correct O
: O
the O
country O
is O
the O
object O
that O
is O
destabilized O
) O
. O
In O
sum O
, O
the O
parse O
has O
misrepresented O
the O
sentence O
‚Äôs O
meaning.1However O
, O
assessing O
such O
deÔ¨Åciencies O
via O
comparison O
against O
a O
gold O
reference O
( O
as O
in O
classical O
parser O
evaluation O
) O
is O
often O
infeasible O
in O
practice O
: O
it O
takes O
a O
trained O
annotator O
and O
appr O
. O
10 O
minutes O
to O
manually O
create O
one O
AMR O
( O
Banarescu O
et O
al O
. O
, O
2013 O
) O
. O
To O
mitigate O
these O
issues O
, O
we O
would O
like O
to O
automatically O
rate O
the O
quality O
of O
AMRs O
without O
the O
costly O
gold O
graphs O
. O
This O
would O
allow O
us O
to O
signal O
downstream O
task O
systems O
the O
incorporated O
graphs O
‚Äô O
trustworthiness O
or O
select O
among O
different O
candidate O
graphs O
from O
different O
parsing O
systems O
. O
To O
achieve O
this O
, O
we O
propose O
a O
method O
that O
imitates O
a O
human O
rater O
, O
who O
is O
inspecting O
the O
graphs O
. O
We O
show O
that O
the O
method O
can O
efÔ¨Åciently O
rate O
the O
quality O
of O
the O
AMRs O
in O
the O
absence O
of O
gold O
graphs O
. O
The O
remainder O
of O
the O
paper O
is O
structured O
as O
follows O
: O
in O
Section O
2 O
, O
we O
outline O
our O
idea O
to O
exploit O
the O
textual O
multi O
- O
line O
string O
representation O
of O
AMRs O
, O
allowing O
for O
efÔ¨Åcient O
and O
simple O
AMR O
processing O
while O
preserving O
vital O
graph O
structure O
. O
In O
Section O
2.2 O
, O
we O
instantiate O
this O
idea O
in O
a O
lightweight O
CNN O
that O
predicts O
the O
quality O
of O
AMR O
graphs O
along O
multiple O
dimensions O
of O
interest O
. O
In O
our O
experiments O
( O
Section O
3 O
) O
, O
we O
show O
that O
this O
framework O
is O
efÔ¨Åcient O
and O
performs O
better O
than O
strong O
baselines O
. O
Our O
code O
is O
available O
at O
https://github O
. O
com O
/ O
flipz357 O
/ O
amr O
- O
quality O
- O
rater O
. O
2 O
AMR O
as O
image O
with O
latent O
channels O
In O
this O
section O
, O
we O
Ô¨Årst O
motivate O
to O
treat O
AMRs O
as O
images O
with O
latent O
channels O
in O
order O
to O
rate O
them O
efÔ¨Åciently O
. O
Second O
, O
we O
brieÔ¨Çy O
describe O
the O
task O
at O
hand O
: O
Rating O
the O
quality O
of O
AMR O
graphs O
in O
the O
absence O
of O
gold O
graphs O
. O
Finally O
, O
to O
solve O
this O
task O
, O
we O
create O
a O
lightweight O
CNN O
that O
evaluates O
AMR O
quality O
in O
multiple O
dimensions O
of O
interest O
. O
1?With O
a O
functioning O
economy O
, O
the O
whole O
country O
may O
cause O
something O
to O
destabilize O
. O
( O
p4 O
¬† O
/ O
¬† O
possible-01 O
     O
: O
pol O
     O
: O
arg1 O
¬† O
( O
d5 O
¬† O
/ O
¬† O
destabilize-01 O
¬†¬†¬†¬†¬†¬†¬†  O
: O
arg1 O
¬† O
( O
c3 O
¬† O
/ O
¬† O
country O
¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† O
: O
quant O
¬† O
( O
w2 O
¬† O
/ O
¬† O
whole O
) O
) O
) O
¬†¬†¬†¬† O
: O
condition O
¬† O
( O
e1 O
¬† O
/ O
¬† O
economy O
         O
: O
poss O
c3 O
¬†¬†¬†¬†¬†¬†¬†¬† O
: O
arg0 O
- O
of O
¬† O
( O
f0 O
¬† O
/ O
¬† O
function-01)))possible O
p4- O
  O
pol O
d5 O
c3w2e1 O
arg1 O
                    O
condition O
poss O
       O
arg1 O
           O
quant O
               O
f0 O
                   O
arg0 O
- O
of O
      O
destabilize-01 O
countrywholeeconomy O
function-01 O
Figure O
3 O
: O
Different O
displays O
for O
an O
AMR O
structure O
of O
a O
sentence O
that O
has O
medium O
length O
( O
left O
: O
P O
ENMAN O
notation O
, O
right O
: O
graphical O
visualization O
) O
The O
P O
ENMAN O
notation O
and O
its O
( O
hidden O
) O
advantages O
The O
native O
AMR O
notation O
is O
called O
PENMAN O
-notation O
or O
Sentence O
Plan O
Language O
( O
Kasper O
, O
1989 O
; O
Mann O
, O
1983 O
) O
. O
Provably O
, O
an O
advantage O
of O
this O
notation O
is O
that O
it O
allows O
for O
secure O
AMR O
storage O
in O
text O
Ô¨Åles O
. O
However O
, O
we O
argue O
that O
it O
has O
more O
advantages O
. O
For O
example O
, O
due O
to O
its O
clear O
structure O
, O
it O
allows O
humans O
a O
fairly O
quick O
understanding O
even O
of O
medium O
- O
sized O
to O
large O
AMR O
structures O
( O
Figure O
3 O
, O
left O
) O
. O
On O
the O
other O
hand O
, O
we O
argue O
that O
a O
graphical O
visualization O
of O
such O
medium O
- O
sized O
to O
large O
AMRs O
( O
Figure O
3 O
, O
right O
) O
could O
hamper O
intuitive O
understanding O
, O
since O
the O
abundant O
visual O
signals O
( O
circles O
, O
arrows O
, O
etc O
. O
) O
may O
more O
easily O
overwhelm O
humans O
. O
Moreover O
, O
in O
every O
display O
, O
one O
would O
depend O
on O
an O
algorithm O
that O
needs O
to O
determine O
a O
suitable O
( O
and O
spacious O
) O
arrangement O
of O
the O
nodes O
, O
edges O
and O
edge O
labels O
. O
It O
may O
be O
for O
these O
reasons O
, O
that O
in O
the O
AMR O
annotation O
tool2 O
, O
the O
graph O
that O
is O
under O
construction O
is O
always O
shown O
in O
PENMAN O
notation O
to O
the O
human O
user O
. O
In O
sum O
, O
we O
Ô¨Ånd O
that O
the O
indented O
multi O
- O
line O
PENMAN O
form O
possesses O
three O
key O
advantages O
( O
Table O
1 O
): O
( O
i O
) O
it O
enables O
fairly O
easy O
human O
understanding O
, O
( O
ii O
) O
, O
it O
is O
well O
- O
deÔ¨Åned O
and O
( O
iii O
) O
, O
which O
is O
what O
we O
will O
show O
next O
, O
it O
can O
be O
computationally O
exploited O
to O
better O
rate O
AMR O
quality O
. O
AMR O
as O
image O
to O
preserve O
graph O
structure O
Figure O
4 O
describes O
our O
proposed O
sentence O
representation O
treatment O
. O
After O
non O
- O
degenerate O
AMR O
graph O
simpliÔ¨Åcation O
( O
more O
details O
in O
Preprocessing O
, O
3.1 O
) O
, O
we O
Ô¨Årst O
project O
the O
PENMAN O
representation O
onto O
a O
small O
grid O
( O
‚Äò O
image O
‚Äô O
) O
. O
Each O
AMR O
token O
( O
e.g. O
, O
a O
node O
or O
an O
edge O
) O
is O
represented O
as O
a O
‚Äò O
categorical O
pixel O
‚Äô O
. O
Second O
, O
Œ¶adds O
latent O
‚Äò O
channels O
‚Äô O
to O
the O
categorical O
pixels O
, O
which O
can O
be O
learned O
incrementally O
in O
an O
application O
. O
In O
other O
words O
, O
every O
AMR O
token O
is O
represented O
by O
a O
Ô¨Åxed O
- O
sized O
vector O
of O
real O
2https://www.isi.edu/cgi-bin/div3/mt/ O
amr O
- O
editor O
/ O
login O
- O
gen O
- O
v1.7.cgi236ws O
bsleep-1 O
baby O
arg0mod O
( O
s O
/ O
sleep-01 O
       O
: O
arg0 O
( O
b O
/ O
baby O
) O
       O
: O
mod O
( O
w O
/ O
well O
) O
) O
( O
sleep-01 O
     O
: O
arg0 O
baby O
     O
: O
mod O
well O
) O
wellsleep-01 O
babyarg0modsleep-01 O
: O
arg0 O
: O
modbaby O
wellsleep-01 O
: O
arg0 O
: O
modbaby O
well O
AMR O
as O
imageAMR O
as O
image O
With O
latent O
channelswell O
Figure O
4 O
: O
We O
transform O
the O
( O
simpliÔ¨Åed O
) O
P O
ENMAN O
representation O
to O
an O
image O
and O
use O
Œ¶to O
add O
latent O
channels O
. O
numbers O
. O
These O
vectors O
are O
arranged O
such O
that O
the O
original O
graph O
structure O
is O
fully O
preserved O
. O
2.1 O
Task O
: O
Rating O
the O
quality O
of O
AMR O
graphs O
We O
aim O
at O
rating O
the O
quality O
of O
AMR O
graphs O
( O
‚Äò O
parses O
‚Äô O
) O
in O
the O
absence O
of O
gold O
graphs O
. O
This O
boils O
down O
to O
answering O
the O
following O
question O
: O
how O
well O
does O
a O
candidate O
AMR O
graph O
capture O
a O
given O
natural O
language O
sentence O
? O
Therefore O
, O
the O
exact O
goal O
in O
this O
task O
is O
to O
learn O
a O
mapping O
f O
: O
S√óG‚Üí O
Rd O
, O
( O
1 O
) O
that O
maps O
a O
sentence O
s‚ààStogether O
with O
a O
candidate O
AMR O
graph O
g‚ààG O
ontodscores O
, O
which O
describe O
the O
AMR O
with O
regard O
to O
dquality O
dimensions O
of O
interest O
. O
A O
successful O
mapping O
function O
should O
strongly O
correlate O
with O
the O
gold O
scores O
as O
they O
would O
emerge O
from O
evaluation O
against O
gold O
graphs O
. O
We O
proceed O
by O
describing O
the O
targeted O
dimensions O
in O
more O
detail O
. O
Main O
AMR O
quality O
dimensions O
The O
main O
quality O
dimensions O
that O
we O
desire O
our O
model O
to O
predict O
are O
estimated O
Smatch O
F1 O
/recall O
/precision O
. O
Smatch O
is O
the O
canonical O
AMR O
metric O
, O
assessing O
the O
triple O
overlap O
between O
two O
graphs O
, O
after O
an O
alignment O
step O
( O
Cai O
and O
Knight O
, O
2013 O
) O
. O
AMR O
sub O
- O
task O
quality O
dimensions O
However O
, O
we O
predict O
also O
other O
quality O
dimensions O
to O
assess O
various O
AMR O
aspects O
( O
Damonte O
et O
al O
. O
, O
2017 O
) O
. O
In O
this O
place O
, O
we O
can O
merely O
provide O
a O
brief O
overview O
: O
( O
i)Unlabeled O
: O
Smatch O
F1 O
when O
disregarding O
edgelabels O
. O
( O
ii O
) O
No O
WSD O
: O
Smatch O
F1 O
when O
ignoring O
ProbBank O
senses O
. O
( O
iii O
) O
Frames O
: O
PropBank O
frame O
identiÔ¨Åcation O
F1 O
( O
iii O
) O
WikiÔ¨Åcation O
: O
KB O
linking O
F1 O
score O
on O
: O
wiki O
relations O
. O
( O
iv O
) O
Negations O
: O
negation O
detection O
F1 O
. O
( O
v O
) O
NamedEnt O
: O
NER O
F1 O
. O
( O
vi O
) O
NS O
frames O
: O
F1 O
score O
for O
ProbBank O
frame O
identiÔ¨Åcation O
when O
disregarding O
the O
sense O
. O
( O
vii O
) O
Concepts O
F O
score O
for O
concept O
identiÔ¨Åcation O
( O
viii O
) O
SRL O
: O
Smatch O
F1 O
computed O
on O
arg O
- O
i O
roles O
only O
. O
( O
ix O
) O
Reentrancy O
: O
Smatch O
F1 O
computed O
on O
re O
- O
entrant O
edges O
only O
. O
( O
x)IgnoreVars O
: O
F1 O
when O
variable O
nodes O
are O
ignored O
. O
( O
xi)Concepts O
: O
F1 O
for O
concept O
detection O
. O
2.2 O
A O
lightweight O
CNN O
to O
rate O
AMR O
quality O
We O
want O
to O
model O
f(Eq O
. O
1 O
) O
in O
order O
to O
estimate O
a O
suite O
of O
quality O
scores O
y‚ààRdfor O
any O
automatically O
generated O
AMR O
graph O
, O
given O
only O
the O
graph O
and O
the O
sentence O
from O
whence O
it O
is O
derived O
. O
Following O
Opitz O
and O
Frank O
( O
2019 O
) O
, O
we O
will O
contrast O
the O
AMR O
against O
the O
sentence O
‚Äôs O
dependency O
parse O
, O
exploiting O
observed O
structural O
similarities O
between O
these O
two O
types O
of O
information O
( O
Wang O
et O
al O
. O
, O
2015 O
) O
. O
Our O
proposed O
method O
allows O
this O
in O
a O
simple O
way O
by O
processing O
dependency O
and O
AMR O
graphs O
in O
parallel O
. O
The O
architecture O
is O
outlined O
in O
Figure O
5 O
. O
Symbol O
embedding O
The O
latent O
channels O
of O
AMR O
and O
dependency O
‚Äò O
pixels O
‚Äô O
represent O
the O
embeddings O
of O
the O
‚Äò O
tokens O
‚Äô O
or O
‚Äò O
symbols O
‚Äô O
contained O
in O
the O
AMR O
and O
dependency O
vocabulary O
. O
These O
symbols O
represent O
nodes O
or O
edges O
. O
We O
use O
two O
special O
tokens O
: O
the O
< O
tab O
> O
token O
, O
which O
represents O
the O
indention O
level O
, O
and O
the O
< O
pad O
> O
token O
, O
which O
Ô¨Ålls O
the O
remaining O
empty O
‚Äò O
pixels O
‚Äô O
. O
By O
embedding O
lookup O
, O
we O
obtain O
AMR O
and O
dependency O
images O
with O
128 O
latent O
channels O
and O
45x15 O
‚Äò O
pixels O
‚Äô O
( O
Œ¶in O
Figure O
5 O
; O
the O
amount O
of O
pixels O
is O
chosen O
such O
that O
more O
than O
95 O
% O
of O
training O
AMRs O
can O
be O
fully O
captured O
) O
. O
Encoding O
local O
graph O
regions O
Given O
AMR O
and O
dependency O
images O
with O
128 O
latent O
channels O
and O
45x15 O
pixels O
, O
we O
apply O
to O
each O
of O
the O
two O
images O
256 O
Ô¨Ålters O
of O
size O
3x3 O
, O
which O
is O
a O
standard O
type O
of O
kernel O
in O
CNNs O
. O
This O
converts O
both O
graphs O
to O
256 O
feature O
maps O
each O
‚ààR45√ó15(samepadding O
) O
, O
obtaining O
two O
three O
- O
dimensional O
tensors O
L1 O
amr O
, O
L1 O
dep‚ààR45√ó15√ó256 O
. O
From O
here O
, O
we O
construct O
our O
Ô¨Årst O
joint O
representation O
, O
which O
matches O
local O
dependency O
regions O
with O
local O
AMR O
regions O
: O
jres O
= O
GPF O
( O
L1 O
amr‚äóL1 O
dep O
) O
, O
( O
2 O
) O
wherex‚äóy= O
[ O
x‚äôy;x O
/ O
circleminusy]denotes O
the O
concatenation O
of O
element O
- O
wise O
multiplication O
and O
element O
- O
wise O
subtraction O
. O
GPF O
is O
an O
operation O
that O
performs O
global O
pooling O
and O
vectorization O
( O
‚Äò O
Ô¨Çattening O
‚Äô O
) O
of O
any O
input O
tensor O
. O
This O
means O
that O
jres‚ààR512is O
a O
joint O
representation O
of O
the O
locally O
matched O
dependency O
and O
AMR O
graph O
regions O
. O
This O
intermediate O
process O
is O
outlined O
in O
Figure O
5 O
by O
‚äó O
( O
left O
) O
and O
GPF O
. O
Finally O
, O
we O
reduce O
the O
dimensions O
of O
the O
two O
intermediate O
three O
- O
dimensional O
representationsL1 O
amrandL1 O
depwith O
3x3 O
max O
- O
pooling O
and237Figure O
5 O
: O
Our O
proposed O
architecture O
for O
efÔ¨Åcient O
AMR O
quality O
assessment O
. O
obtainL2 O
amrandL2 O
dep‚ààR15√ó5√ó256 O
Encoding O
global O
graph O
regions O
For O
a O
moment O
, O
we O
put O
the O
joint O
residual O
( O
jres O
) O
aside O
and O
proceed O
by O
processing O
the O
locally O
convolved O
feature O
maps O
with O
larger O
Ô¨Ålters O
. O
While O
the O
Ô¨Årst O
convolutions O
allowed O
us O
to O
obtain O
abstract O
local O
graph O
regions O
L2 O
amrand O
L2 O
dep O
, O
we O
now O
aim O
at O
matching O
more O
global O
regions O
. O
More O
precisely O
, O
we O
use O
128 O
2D O
Ô¨Ålters O
of O
shape O
10x5 O
, O
followed O
by O
a O
5x5 O
max O
- O
pooling O
operations O
on O
L2 O
amrandL2 O
dep O
. O
Thus O
, O
we O
have O
obtained O
vectorized O
abstract O
global O
graph O
representations O
gamr O
, O
gdep‚àà O
R384 O
. O
Then O
, O
we O
construct O
a O
joint O
representation O
( O
right‚äó O
, O
Figure O
5 O
): O
jglob O
= O
gamr‚äógdep O
. O
( O
3 O
) O
At O
this O
point O
, O
together O
with O
the O
joint O
residual O
representation O
from O
the O
local O
region O
matching O
, O
we O
have O
arrived O
at O
two O
joint O
vector O
representations O
jgloband O
jres O
. O
We O
concatenate O
them O
( O
[ O
¬∑ O
; O
¬∑ O
] O
in O
Figure O
5 O
) O
to O
form O
one O
joint O
representation O
j‚ààR1280 O
: O
j= O
[ O
jres;jglob O
] O
( O
4 O
) O
Quality O
prediction O
The O
shared O
representation O
j O
is O
further O
processed O
by O
a O
feed O
- O
forward O
layer O
with O
ReLU O
activation O
functions O
( O
FF O
+ O
ReLU O
, O
Figure O
5 O
) O
and O
a O
consecutive O
feed O
- O
forward O
layer O
with O
sigmoid O
activation O
functions O
( O
FF O
+ O
sigm O
, O
Figure O
5 O
): O
y O
= O
sigm O
( O
ReLU O
( O
jTA)B O
) O
, O
( O
5 O
) O
whereA‚ààR1280√óh O
, O
B‚ààRh√ódim O
( O
out)are O
parameters O
of O
the O
model O
and O
sigm O
( O
x O
) O
= O
( O
1 O
1+e‚àíx1, O
... O
,1 O
1+e‚àíxdim O
( O
out))projectsxonto[0,1]dim O
( O
out O
) O
. O
When O
estimating O
the O
main O
AMR O
metric O
scores O
we O
instantiate O
three O
output O
neurons O
( O
dim(out O
) O
= O
3 O
) O
that O
represent O
estimated O
Smatch O
precision O
, O
Smatch O
recall O
and O
Smatch O
F1 O
. O
In O
the O
case O
where O
we O
are O
interested O
in O
a O
more O
Ô¨Åne O
- O
grained O
assessment O
of O
AMR O
quality O
( O
e.g. O
, O
knowledge O
- O
base O
linking O
quality O
) O
, O
we O
have O
33 O
output O
neurons O
representing O
expected O
scores O
for O
various O
semantic O
aspects O
involved O
in O
AMR O
parsing O
( O
we O
predict O
precision O
, O
recall O
and O
F1 O
of O
11 O
aspects O
, O
as O
outlined O
in¬ß2.1 O
) O
. O
To O
summarize O
, O
the O
residual O
joint O
representation O
should O
capture O
local O
similarities O
. O
On O
the O
other O
hand O
, O
the O
second O
joint O
representation O
aims O
to O
capture O
the O
more O
global O
and O
structural O
properties O
of O
the O
two O
graphs O
. O
Both O
types O
of O
information O
inform O
the O
Ô¨Ånal O
quality O
assessment O
of O
our O
model O
in O
the O
last O
layer O
. O
3 O
Experiments O
In O
this O
section O
, O
we O
Ô¨Årst O
describe O
the O
data O
, O
changes O
to O
the O
data O
that O
target O
the O
reduction O
of O
biases O
, O
and O
the O
baseline O
. O
After O
discussing O
our O
main O
results O
, O
we O
conduct O
further O
analyses O
. O
( O
i O
) O
, O
we O
study O
the O
effects O
of O
our O
data O
- O
debiasing O
steps O
. O
( O
ii O
) O
, O
we O
assess O
the O
performance O
of O
our O
model O
in O
a O
classiÔ¨Åcation O
task O
( O
distinguishing O
good O
from O
bad O
parses O
) O
. O
( O
iii O
) O
, O
we O
assess O
the O
model O
performance O
when O
we O
only O
provide O
the O
candidate O
AMR O
and O
the O
sentence O
( O
dependency O
tree O
ablation O
) O
. O
( O
iv O
) O
, O
we O
provide O
detailed O
measurements O
of O
the O
method O
‚Äôs O
computational O
cost O
. O
3.1 O
Experimental O
setup O
Data O
We O
use O
the O
data O
from O
Opitz O
and O
Frank O
( O
2019 O
) O
. O
The O
data O
set O
consists O
of O
more O
than O
15,000238sentences O
with O
more O
than O
60,000 O
corresponding O
parses O
, O
by O
three O
different O
automatic O
parsing O
systems O
and O
a O
human O
. O
More O
precisely O
, O
the O
data O
set O
D={(si O
, O
gi O
, O
yi)}N O
i=1consists O
of O
tuples O
( O
si O
, O
gi O
, O
yi O
) O
, O
wheresi‚àà O
S O
is O
a O
natural O
language O
sentence O
, O
gi‚ààGis O
a O
‚Äò O
candidate O
‚Äô O
AMR O
graph O
and O
yi‚ààRdis O
a O
36 O
- O
dimensional O
vector O
containing O
scores O
which O
represent O
the O
quality O
of O
the O
AMR O
graph O
in O
terms O
of O
precision O
, O
recall O
and O
F1 O
with O
respect O
to O
12 O
different O
tasks O
captured O
by O
AMR O
( O
as O
outlined O
in O
¬ß O
2.1 O
) O
. O
Debiasing O
of O
the O
data O
We O
observe O
three O
biases O
in O
the O
data O
. O
First O
, O
the O
graphs O
in O
the O
training O
section O
of O
our O
data O
are O
less O
deÔ¨Åcient O
than O
in O
the O
development O
and O
testing O
data O
, O
because O
the O
parsers O
were O
trained O
on O
( O
sentence O
, O
gold O
graph O
) O
pairs O
from O
the O
training O
section O
. O
For O
our O
task O
, O
this O
means O
that O
the O
training O
section O
‚Äôs O
target O
scores O
are O
higher O
, O
on O
average O
, O
than O
the O
target O
scores O
in O
the O
other O
data O
partitions O
. O
To O
achieve O
more O
balance O
in O
this O
regard O
, O
we O
re O
- O
split O
the O
data O
randomly O
on O
the O
sentence O
- O
id O
level O
( O
such O
that O
a O
sentence O
does O
not O
appear O
in O
more O
than O
one O
partition O
with O
different O
parses O
) O
. O
Second O
, O
we O
observe O
that O
the O
data O
contains O
some O
superÔ¨Åcial O
hidden O
clues O
that O
could O
give O
away O
the O
parse O
‚Äôs O
source O
. O
This O
bears O
the O
danger O
that O
a O
model O
does O
not O
learn O
to O
assess O
the O
parse O
quality O
, O
but O
to O
assess O
the O
source O
of O
the O
parse O
. O
And O
since O
some O
parsers O
are O
better O
or O
worse O
than O
others O
, O
the O
model O
could O
exploit O
this O
bias O
. O
For O
example O
, O
consider O
that O
one O
parser O
prefers O
to O
write O
( O
r O
/ O
run-01 O
: O
arg1 O
( O
c O
/ O
cat O
) O
: O
polarity O
- O
) O
, O
while O
the O
other O
parser O
prefers O
to O
write O
( O
r O
/ O
run-01 O
: O
polarity O
- O
: O
arg1 O
( O
c O
/ O
cat O
) O
) O
. O
These O
two O
structures O
are O
semantically O
equivalent O
but O
differ O
on O
the O
surface O
. O
Hence O
, O
the O
arrangement O
of O
the O
output O
may O
provide O
unwanted O
clues O
on O
the O
source O
of O
the O
parse O
. O
To O
alleviate O
this O
issue O
, O
we O
randomly O
re O
- O
arrange O
all O
parses O
on O
the O
surface O
, O
keeping O
their O
semantics.34 O
A O
third O
bias O
stems O
from O
a O
design O
choice O
in O
the O
metric O
scripts O
used O
to O
calculate O
the O
target O
scores O
. O
More O
precisely O
, O
the O
extended O
Smatch O
metric O
script O
, O
per O
default O
, O
assigns O
a O
parse O
that O
does O
not O
contain O
a O
certain O
edge O
- O
type O
( O
e.g. O
, O
: O
arg O
n O
) O
the O
score O
0 O
with O
respect O
to O
the O
speciÔ¨Åc O
quality O
dimension O
( O
in O
this O
case O
, O
SRL O
: O
0.00 O
Precision O
/ O
Recall O
/ O
F1 O
) O
. O
However O
, O
if O
the O
gold O
parse O
also O
does O
not O
contain O
an O
3Technically O
, O
this O
is O
achieved O
by O
reformatting O
the O
parses O
such O
that O
in O
the O
depth-Ô¨Årst O
writing O
- O
traversal O
at O
node O
nthe O
out O
- O
going O
edges O
of O
nwill O
be O
traversed O
in O
random O
order O
. O
4Different O
variable O
names O
, O
e.g. O
, O
( O
r O
/ O
run-01 O
) O
and O
( O
x O
/ O
run-01 O
) O
are O
not O
an O
issue O
in O
this O
work O
since O
the O
variables O
are O
handled O
via O
( O
van O
Noord O
and O
Bos O
, O
2017a O
) O
. O
See O
also O
Preprocessing O
, O
¬ß O
3.1edge O
of O
this O
type O
( O
i.e. O
, O
: O
arg O
n O
) O
, O
then O
we O
believe O
that O
the O
correct O
default O
score O
should O
be O
1 O
, O
since O
the O
parse O
is O
, O
in O
the O
speciÔ¨Åc O
dimension O
, O
in O
perfect O
agreement O
with O
the O
gold O
( O
i.e. O
, O
SRL O
: O
1.00 O
Precision O
/ O
Recall O
/ O
F1 O
) O
. O
Therefore O
, O
we O
set O
all O
sub O
- O
task O
scores O
, O
where O
the O
predicted O
graph O
agrees O
with O
the O
gold O
graph O
in O
the O
absence O
of O
a O
feature O
, O
from O
0 O
to O
1 O
. O
Preprocessing O
Same O
as O
prior O
work O
, O
we O
dependency O
- O
parse O
and O
tokenize O
the O
sentences O
with O
spacy O
( O
Honnibal O
and O
Montani O
, O
2017 O
) O
and O
replace O
variables O
with O
corresponding O
concepts O
( O
e.g. O
, O
( O
j O
/ O
jump-01 O
: O
arg0 O
( O
g O
/ O
girl O
) O
) O
is O
translated O
to O
( O
jump-01 O
: O
arg0 O
( O
girl O
) O
) O
. O
Re O
- O
entrancies O
are O
handled O
with O
pointers O
according O
to O
van O
Noord O
and O
Bos O
( O
2017a O
) O
, O
which O
ensures O
non O
- O
degenerate O
AMR O
simpliÔ¨Åcation.5Furthermore O
, O
we O
lower O
- O
case O
all O
tokens O
, O
remove O
quotation O
marks O
and O
join O
sub O
- O
structures O
that O
represent O
names.6The O
vocabulary O
encompasses O
all O
tokens O
of O
frequency O
‚â•5 O
, O
remaining O
ones O
are O
set O
to O
< O
unk O
> O
. O
Training O
All O
parameters O
are O
initialized O
randomly O
. O
We O
train O
for O
5 O
epochs O
and O
select O
the O
parameters O
Œ∏from O
the O
epoch O
where O
maximum O
development O
scores O
were O
achieved O
( O
with O
respect O
to O
average O
Pearson‚ÄôsœÅover O
the O
quality O
dimensions O
) O
. O
In O
training O
, O
we O
reduce O
the O
squared O
error O
with O
gradient O
descent O
( O
Adam O
rule O
( O
Kingma O
and O
Ba O
, O
2019 O
) O
, O
learning O
rate O
= O
0.001 O
, O
mini O
batch O
size O
= O
64 O
): O
Œ∏‚àó= O
arg O
min O
Œ∏|D|/summationdisplay O
i=1|M|/summationdisplay O
j=1(yi O
, O
j‚àífŒ∏(si O
, O
gi)j)2,(6 O
) O
whereMis O
the O
set O
of O
target O
metrics O
. O
Baseline O
Our O
main O
baseline O
is O
the O
model O
of O
previous O
work O
, O
henceforth O
denoted O
by O
LG O
- O
LSTM O
. O
The O
method O
works O
in O
the O
following O
steps O
: O
Ô¨Årst O
, O
it O
uses O
a O
depth-Ô¨Årst O
graph O
traversal O
to O
linearize O
the O
automatic O
AMR O
graph O
and O
the O
corresponding O
dependency O
tree O
of O
the O
sentence O
. O
Second O
, O
it O
constructs O
a O
joint O
representation O
and O
predicts O
the O
score O
estimations O
. O
To O
further O
improve O
its O
performance O
, O
the O
baseline O
uses O
some O
extra O
- O
features O
( O
e.g. O
, O
a O
shallow O
alignment O
from O
5For O
example O
, O
consider O
the O
sentence O
The O
cat O
scratches O
itself O
and O
its O
graph O
( O
x O
/ O
scratch-01 O
: O
arg0 O
( O
y O
/ O
cat O
) O
: O
arg1 O
y O
) O
) O
. O
Replacing O
the O
variables O
with O
concepts O
would O
come O
at O
the O
cost O
of O
an O
information O
loss O
w.r.t O
. O
to O
coreference O
: O
( O
scratch-01 O
: O
arg0 O
cat O
: O
arg1 O
cat O
) O
‚Äî O
does O
the O
cat O
scratch O
itself O
or O
another O
cat O
? O
Hence O
, O
pointers O
are O
used O
to O
translate O
the O
graph O
into O
( O
scratch-01 O
: O
arg0 O
* O
0 O
* O
cat O
: O
arg1 O
* O
0 O
* O
) O
) O
. O
6E.g O
. O
, O
: O
name O
( O
name O
: O
op1 O
‚Äò O
Barack O
‚Äô O
: O
op2 O
‚Äò O
Obama O
‚Äô O
) O
is O
translated O
to O
: O
name O
barack O
obama O
.239Smatch O
Ridge O
GNN O
LG O
- O
LSTM O
ours O
change O
% O
P‚ÄôsœÅF1 O
0.428 O
0.659 O
0.662¬±0.000.696¬±0.00 O
+ O
5.14‚Ä†‚Ä° O
Precision O
0.348 O
0.601 O
0.600¬±0.000.623¬±0.01 O
+ O
3.83‚Ä† O
Recall O
0.463 O
0.667 O
0.676¬±0.000.719¬±0.00 O
+ O
6.36‚Ä†‚Ä°RMSEF1 O
0.155 O
0.132 O
0.130¬±0.000.128¬±0.00 O
- O
1.54 O
Precision O
0.146 O
0.127 O
0.126¬±0.000.126¬±0.00 O
+ O
-0.0 O
Recall O
0.169 O
0.141 O
0.142¬±0.000.136¬±0.00 O
- O
4.23 O
Table O
2 O
: O
Main O
results O
. O
Pearson O
‚Äôs O
corr O
. O
coefÔ¨Åcient O
( O
row O
1 O
- O
3 O
) O
is O
better O
if O
higher O
; O
root O
mean O
square O
error O
( O
RMSE O
, O
row O
4 O
- O
6 O
) O
is O
better O
if O
lower O
. O
The O
quality O
dimensions O
are O
explained O
in¬ß2.1.‚Ä†(‚Ä° O
): O
p<0.05 O
( O
p<0.005 O
) O
, O
significant O
difference O
in O
the O
correlations O
with O
two O
- O
tailed O
test O
using O
Fisher O
œÅto O
z O
transformation O
( O
Fisher O
, O
1915 O
) O
. O
dependency O
tokens O
to O
AMR O
tokens).7Generally O
speaking O
, O
the O
baseline O
is O
a O
model O
that O
works O
based O
on O
graph O
linearizations O
. O
Such O
type O
of O
model O
, O
despite O
its O
apparent O
simplicity O
, O
has O
proven O
to O
be O
an O
effective O
baseline O
or O
state O
- O
of O
- O
the O
- O
art O
method O
in O
various O
works O
about O
converting O
texts O
into O
graphs O
( O
Konstas O
et O
al O
. O
, O
2017 O
; O
van O
Noord O
and O
Bos O
, O
2017b O
) O
, O
or O
converting O
graphs O
into O
texts O
( O
Bastings O
et O
al O
. O
, O
2017 O
; O
Beck O
et O
al O
. O
, O
2018 O
; O
Song O
, O
2019 O
; O
Pourdamghani O
et O
al O
. O
, O
2016 O
; O
Song O
et O
al O
. O
, O
2018 O
; O
Vinyals O
et O
al O
. O
, O
2015 O
; O
Mager O
et O
al O
. O
, O
2020 O
) O
, O
or O
performing O
mathematically O
complex O
tasks O
modeled O
as O
graph O
- O
to O
- O
graph O
problems O
, O
such O
as O
symbolic O
integration O
( O
Lample O
and O
Charton O
, O
2020 O
) O
. O
However O
, O
in O
our O
main O
results O
, O
we O
also O
display O
the O
results O
of O
two O
additional O
baselines O
: O
GNN O
( O
Song O
et O
al O
. O
, O
2018 O
) O
, O
where O
we O
encode O
the O
dependency O
tree O
and O
the O
AMR O
with O
a O
graph O
- O
recurrent O
encoder O
and O
perform O
regression O
on O
the O
joint O
averaged O
node O
embedding O
vectors.8And O
Ridge O
, O
an O
l2 O
- O
regularized O
linear O
regression O
that O
is O
based O
on O
shallow O
graph O
statistics.9 O
3.2 O
Results O
Main O
AMR O
quality O
dimensions O
The O
main O
quality O
of O
an O
AMR O
graph O
is O
estimated O
in O
expected O
triple O
match O
ratios O
( O
Smatch O
F1 O
, O
Precision O
and O
Recall O
) O
. O
The O
results O
, O
averaged O
over O
10 O
runs O
, O
are O
displayed O
in O
Table O
2 O
. O
With O
regard O
to O
estimated O
Smatch O
F1 O
, O
we O
7Furthermore O
, O
the O
baseline O
uses O
auxiliary O
losses O
to O
achieve O
a O
slight O
performance O
gain O
in O
predicting O
the O
Smatch O
metrics O
. O
For O
the O
sake O
of O
simplicity O
, O
we O
do O
not O
use O
these O
auxiliary O
losses O
, O
except O
in O
one O
experiment O
, O
where O
we O
show O
that O
our O
method O
achieves O
a O
similar O
small O
gain O
with O
the O
auxiliary O
losses O
. O
8 O
/ O
bracketleftbigg O
1 O
|VA|/summationtext O
v‚ààVAemb(v)/bracketrightbigg O
‚äó/bracketleftbigg O
1 O
|VD|/summationtext O
v‚ààVDemb(v)/bracketrightbigg O
. O
9For O
the O
dependency O
graph O
( O
D O
) O
and O
the O
AMR O
graph O
( O
A O
) O
we O
both O
compute O
œÜ(A|D)= O
[ O
density O
, O
avg O
. O
node O
degree O
, O
node O
count O
, O
edge O
count O
, O
( O
arg0 O
|subj O
) O
count O
, O
( O
arg1|obj O
) O
count O
] O
, O
the O
Ô¨Ånal O
feature O
vector O
then O
is O
deÔ¨Åned O
as O
Œ¶(x)= O
[ O
œÜ(A O
) O
-œÜ(D O
) O
; O
œÜ(D);œÜ(A);|lemmas(D)‚à©concepts(A)| O
|lemmas(D)‚à™concepts(A)|]Quality O
Dim O
. O
LG O
- O
LSTM O
ours O
change O
% O
F1 O
Pearson O
‚Äôs O
œÅConcepts O
0.508¬±0.010.545¬±0.01 O
+ O
7.28‚Ä† O
Frames O
0.420¬±0.010.488¬±0.01 O
+ O
16.19‚Ä†‚Ä† O
IgnoreVars O
0.627¬±0.010.665¬±0.00 O
+ O
6.06‚Ä†‚Ä† O
NamedEnt O
. O
0.429¬±0.020.460¬±0.01 O
+ O
7.23‚Ä† O
Negations O
0.685¬±0.020.746¬±0.01 O
+ O
8.91‚Ä†‚Ä† O
NoWSD O
0.640¬±0.010.680¬±0.00 O
+ O
6.25‚Ä†‚Ä† O
NS O
- O
frames O
0.419¬±0.020.505¬±0.01 O
+ O
20.53‚Ä†‚Ä† O
Reentrancies O
0.508¬±0.010.602¬±0.00 O
+ O
18.50‚Ä†‚Ä† O
SRL O
0.519¬±0.010.581¬±0.01 O
+ O
11.95‚Ä†‚Ä† O
Unlabeled O
0.628¬±0.010.663¬±0.00 O
+ O
5.57‚Ä†‚Ä† O
WikiÔ¨Åcation O
0.901¬±0.000.904¬±0.00 O
+ O
0.33F1 O
RMSEConcepts O
0.117¬±0.000.114¬±0.00 O
- O
2.56 O
Frames O
0.186¬±0.000.182¬±0.00 O
- O
2.15 O
IgnoreVars O
0.195¬±0.000.186¬±0.00 O
- O
4.62 O
NamedEnt O
. O
0.159¬±0.000.156¬±0.00 O
- O
1.89 O
Negations O
0.197¬±0.000.180¬±0.00 O
- O
8.63 O
NoWSD O
0.132¬±0.000.126¬±0.00 O
- O
4.55 O
NS O
- O
frames O
0.157¬±0.000.155¬±0.00 O
- O
1.27 O
Reentrancies O
0.285¬±0.000.265¬±0.00 O
- O
7.02 O
SRL O
0.189¬±0.000.181¬±0.00 O
- O
4.23 O
Unlabeled O
0.124¬±0.000.121¬±0.00 O
- O
2.42 O
WikiÔ¨Åcation O
0.165¬±0.000.162¬±0.00 O
- O
1.82 O
Table O
3 O
: O
Results O
for O
AMR O
quality O
rating O
w.r.t O
. O
various O
sub O
- O
tasks.‚Ä†(‚Ä° O
): O
signiÔ¨Åcance O
( O
c.f O
. O
caption O
Table O
2 O
) O
. O
achieve O
a O
correlation O
with O
the O
gold O
scores O
of O
0.695 O
Pearson‚ÄôsœÅ O
. O
This O
constitutes O
a O
signiÔ¨Åcant O
improvement O
of O
appr O
. O
5 O
% O
over O
LG O
- O
LSTM O
. O
Similarly O
, O
recall O
and O
precision O
correlations O
improve O
by O
6.36 O
% O
and O
3.83 O
% O
( O
from O
0.676 O
to O
0.719 O
and O
0.600 O
to O
0.623 O
) O
. O
While O
the O
improvement O
in O
predicted O
recall O
is O
significant O
at O
p<0.05 O
and O
p<0.005 O
, O
the O
improvement O
in O
predicted O
precision O
is O
signiÔ¨Åcant O
at O
p O
< O
0.05 O
. O
When O
we O
consider O
the O
root O
mean O
square O
error O
( O
RMSE O
) O
, O
we O
Ô¨Ånd O
that O
the O
method O
improves O
over O
the O
best O
baseline O
by O
-1.54 O
% O
in O
estimated O
Smatch O
F1 O
and O
-4.23 O
% O
in O
estimated O
Smatch O
recall O
. O
On O
the O
other O
hand O
, O
the O
RMS O
error O
in O
estimated O
precision O
remains O
unchanged O
. O
AMR O
subtask O
quality O
Our O
model O
can O
also O
rate O
the O
quality O
of O
an O
AMR O
graph O
in O
a O
more O
Ô¨Åne O
- O
grained O
way O
. O
The O
results O
are O
displayed O
in O
Table O
3 O
. O
Over O
almost O
every O
dimension O
we O
see O
considerable O
improvements O
. O
For O
instance O
, O
a O
considerable O
improvement O
in O
Pearson O
‚Äôs O
œÅis O
achieved O
for O
assessment O
of O
frame O
prediction O
quality O
( O
‚Äò O
NSFrames O
‚Äô O
in O
Table O
3 O
, O
+20.5%œÅ O
) O
and O
coreference O
quality O
( O
‚Äò O
Reentrancies O
‚Äô O
in O
Table O
3 O
, O
+18.5 O
% O
) O
. O
A O
substantial O
error O
reduction O
is O
achieved O
in O
polarity O
( O
‚Äò O
Negations O
‚Äô O
, O
Table O
3 O
) O
, O
where O
we O
reduce O
the O
RMSE O
of O
the O
estimated O
F1 O
score O
by O
-8.6 O
% O
. O
When O
rating O
the O
SRL O
- O
quality O
of O
an O
AMR O
parse O
, O
our O
model O
reduces O
the O
RMSE O
by O
appr O
. O
4 O
% O
. O
In O
general,240Pearson‚ÄôsœÅ O
error O
data O
method O
P O
R O
F1 O
RMSE O
( O
F1 O
) O
0 O
2LG O
- O
LSTM O
0.72 O
0.78 O
0.77 O
0.138 O
LG O
- O
LSTM O
+ O
aux O
0.74 O
0.79 O
0.78 O
0.137 O
ours O
0.75 O
0.80 O
0.79 O
0.133 O
ours O
+ O
aux O
0.76 O
0.81 O
0.80 O
0.132 O
1 O
2LG O
- O
LSTM O
0.67 O
0.73 O
0.72 O
0.120 O
ours O
0.68 O
0.75 O
0.74 O
0.117 O
2 O
2LG O
- O
LSTM O
0.60 O
0.68 O
0.66 O
0.130 O
ours O
0.62 O
0.72 O
0.70 O
0.128 O
Table O
4 O
: O
Performance O
- O
effects O
of O
data O
debiasing O
steps O
. O
+ O
auxindicates O
a O
model O
variant O
that O
is O
trained O
using O
auxiliary O
losses O
that O
incorporate O
information O
about O
the O
other O
AMR O
aspects O
in O
the O
training O
process O
( O
see O
Fn.7 O
) O
. O
improvements O
are O
obtained O
over O
almost O
all O
tested O
quality O
dimensions O
, O
both O
in O
RMSE O
reduction O
and O
increased O
correlation O
with O
the O
gold O
scores O
. O
3.3 O
Analysis O
Effect O
of O
data O
debiasing O
We O
want O
to O
study O
the O
effect O
of O
the O
data O
set O
cleaning O
steps O
by O
analyzing O
the O
performance O
of O
our O
method O
and O
the O
baseline O
on O
three O
different O
versions O
of O
the O
data O
, O
with O
respect O
to O
estimated O
Smatch O
scores O
. O
The O
three O
versions O
are O
( O
i O
) O
0 O
2= O
A O
MRQUALITY O
, O
which O
is O
the O
original O
data O
; O
( O
ii O
) O
1 O
2 O
, O
which O
is O
the O
data O
after O
the O
random O
re O
- O
split O
and O
score O
correction O
; O
( O
iii)2 O
2= O
A O
MRQUALITY O
CLEAN O
which O
is O
our O
main O
data O
after O
the O
Ô¨Ånal O
debiasing O
step O
( O
shallow O
structure O
debiasing O
) O
has O
been O
applied O
. O
The O
results O
are O
shown O
in O
Table O
4 O
. O
We O
can O
make O
three O
main O
observations O
: O
( O
i O
) O
from O
the O
Ô¨Årst O
to O
the O
second O
debiasing O
step O
, O
the O
baseline O
and O
our O
model O
have O
in O
common O
that O
Pearson O
‚Äôs O
œÅand O
the O
error O
decrease O
. O
While O
we O
can O
not O
exactly O
explain O
why O
œÅ O
decreases O
, O
it O
is O
somewhat O
in O
line O
with O
recent O
research O
that O
observed O
performance O
drops O
when O
data O
was O
re O
- O
split O
( O
Gorman O
and O
Bedrick O
, O
2019 O
) O
. O
On O
the O
other O
hand O
, O
the O
error O
decrease O
can O
be O
explained O
by O
the O
random O
re O
- O
split O
that O
balances O
the O
target O
scores O
. O
( O
ii O
) O
The O
second O
debiasing O
step O
leads O
to O
a O
decrease O
in O
œÅand O
an O
increase O
in O
error O
, O
for O
both O
models O
. O
This O
indicates O
that O
we O
have O
successfully O
removed O
shallow O
biases O
from O
the O
data O
that O
can O
give O
away O
the O
parse O
‚Äôs O
source O
. O
( O
iii O
) O
On O
all O
considered O
versions O
of O
the O
data O
, O
the O
method O
performs O
better O
than O
the O
baseline O
. O
AMRs O
: O
telling O
the O
good O
from O
the O
bad O
In O
this O
experiment O
, O
we O
want O
to O
see O
how O
well O
the O
model O
can O
discriminate O
between O
good O
and O
bad O
graphs O
. O
To O
this O
aim O
, O
we O
create O
a O
Ô¨Åve O
- O
way O
classiÔ¨Åcation O
task O
: O
graphs O
are O
assigned O
the O
label O
‚Äò O
very O
bad O
‚Äô O
( O
Smatch O
F1 O
< O
0.25 O
) O
, O
‚Äò O
bad O
‚Äô O
( O
0.25‚â•Smatch O
F1<0.5 O
) O
, O
‚Äò O
good‚Äômajority O
random O
LG O
- O
LSTM O
ours O
avg O
. O
F1 O
0.13 O
0.20 O
0.40 O
0.44‚Ä†‚Ä° O
quadr O
. O
kappa O
0.0 O
0.03 O
0.53 O
0.60‚Ä†‚Ä° O
Table O
5 O
: O
Graph O
quality O
classiÔ¨Åcation O
task O
. O
‚Ä†(‚Ä° O
) O
significance O
with O
paired O
t O
- O
test O
at O
p O
< O
0.05 O
( O
p<0.005 O
) O
over O
10 O
random O
inititalizations O
. O
Quality O
Dim O
. O
LG O
- O
LSTM O
ours O
ours O
( O
no O
dep.)P‚ÄôsœÅSmatch O
F1 O
0.662¬±0.000.696¬±0.000.682¬±0.01 O
Smatch O
precision O
0.600¬±0.000.623¬±0.010.614¬±0.01 O
Smatch O
recall O
0.676¬±0.000.719¬±0.000.702¬±0.01RMSESmatch O
F1 O
0.130¬±0.000.128¬±0.000.128¬±0.00 O
Smatch O
precision O
0.126¬±0.000.126¬±0.000.129¬±0.00 O
Smatch O
recall O
0.142¬±0.000.136¬±0.000.139¬±0.00 O
Table O
6 O
: O
Right O
column O
: O
results O
of O
our O
system O
when O
we O
abstain O
from O
feeding O
the O
dependency O
tree O
, O
and O
only O
show O
the O
sentence O
together O
with O
the O
candidate O
AMR O
. O
( O
0.5‚â•Smatch O
F1<0.75 O
) O
, O
‚Äò O
very O
good O
‚Äô O
( O
0.75 O
‚â• O
Smatch O
F1 O
< O
0.95 O
) O
and O
‚Äò O
excellent O
‚Äô O
( O
Smatch O
F1 O
‚â•0.95 O
) O
. O
Here O
, O
we O
do O
not O
retrain O
the O
models O
with O
a O
classiÔ¨Åcation O
objective O
but O
convert O
the O
estimated O
Smatch O
F1 O
to O
the O
corresponding O
label O
. O
Since O
the O
classes O
are O
situated O
on O
a O
nominal O
scale O
, O
and O
ordinary O
classiÔ¨Åcation O
metrics O
would O
not O
fully O
reÔ¨Çect O
the O
performance O
, O
we O
also O
use O
quadratic O
weighted O
kappa O
( O
Cohen O
, O
1968 O
) O
for O
evaluation O
. O
The O
results O
are O
shown O
in O
Table O
5 O
. O
All O
baselines O
, O
including O
LG O
- O
LSTM O
, O
are O
signiÔ¨Åcantly O
outperformed O
by O
our O
approach O
, O
both O
in O
terms O
of O
macro O
F1 O
( O
+4 O
points O
, O
10 O
% O
improvement O
) O
and O
quadratic O
kappa O
( O
+7 O
points O
, O
13 O
% O
improvement O
) O
. O
How O
important O
is O
the O
dependency O
information O
? O
To O
investigate O
this O
question O
, O
instead O
of O
feeding O
the O
dependency O
tree O
of O
the O
sentence O
, O
we O
only O
feed O
the O
sentence O
itself O
. O
To O
achieve O
this O
, O
we O
simply O
insert O
the O
tokens O
in O
the O
Ô¨Årst O
row O
of O
the O
former O
dependency O
input O
image O
, O
and O
pad O
all O
remaining O
empty O
‚Äò O
pixels O
‚Äô O
. O
In O
this O
mode O
, O
the O
sentence O
encoding O
is O
similar O
to O
standard O
convolutional O
sentence O
encoders O
as O
they O
are O
typically O
used O
in O
many O
tasks O
( O
Kim O
, O
2014 O
) O
. O
The O
results O
are O
shown O
in O
the O
right O
column O
of O
Table O
6 O
. O
The O
performance O
drops O
are O
small O
but O
consistent O
across O
all O
analyzed O
dimensions O
, O
both O
in O
terms O
of O
error O
( O
0 O
to O
2.2 O
% O
increase O
) O
and O
Pearson O
‚Äôs O
œÅ(1.4 O
to O
2.4 O
% O
decrease O
) O
. O
This O
indicates O
that O
the O
dependency O
trees O
contain O
information O
that O
can O
be O
exploited O
by O
our O
model O
to O
better O
judge O
the O
AMR O
quality O
. O
We O
hypothesize O
that O
this O
is O
due O
to O
similarities O
between O
relations O
such O
as O
subj O
/ O
obj(syntactic O
) O
or O
arg0 O
/ O
arg1 O
( O
semantic O
) O
, O
etc O
. O
Yet O
, O
we O
see O
that O
this O
simpler O
model,241GPU O
type O
GTX O
Titan O
GTX O
1080 O
method O
LG O
- O
LSTM O
ours O
LG O
- O
LSTM O
ours O
avg O
. O
ep O
. O
time O
722s O
59s O
1582s O
64s O
avg O
. O
W O
105 O
166 O
45 O
128 O
kWh O
per O
epoch O
0.021 O
0.003 O
0.020 O
0.002 O
Table O
7 O
: O
EfÔ¨Åciency O
analysis O
of O
two O
approaches O
. O
time O
/ O
epoch O
( O
hours O
) O
kWh O
/ O
epoch0.00.10.20.30.40.50.6cost O
LG O
- O
LSTM O
ours O
Figure O
6 O
: O
Training O
cost O
diagram O
of O
two O
approaches O
. O
which O
does O
not O
see O
the O
dependency O
tree O
, O
still O
outperforms O
the O
baseline O
, O
except O
in O
estimated O
precision O
, O
where O
the O
error O
is O
increased O
by O
2.4 O
% O
. O
EfÔ¨Åciency O
analysis O
Recently O
, O
in O
many O
countries O
, O
there O
have O
been O
efforts O
to O
reduce O
energy O
consumption O
and O
carbon O
emission O
. O
Since O
deep O
learning O
typically O
requires O
intensive O
GPU O
computing O
, O
this O
aspect O
is O
of O
increasing O
importance O
to O
researchers O
and O
applicants O
( O
Strubell O
et O
al O
. O
, O
2019 O
; O
Tang O
et O
al O
. O
, O
2019 O
; O
Ganguly O
et O
al O
. O
, O
2019 O
) O
. O
To O
investigate O
energy O
consumption O
of O
our O
method O
and O
previous O
work O
, O
we O
monitor O
their O
GPU O
usage O
during O
training O
, O
assessing O
the O
following O
quantities O
: O
( O
i O
) O
avg O
. O
time O
per O
epoch O
, O
( O
ii O
) O
avg O
. O
watts O
GPU O
usage O
, O
( O
iii O
) O
kilowatts O
per O
epoch O
( O
in O
kWh O
) O
. O
The O
results O
of O
this O
analysis O
are O
displayed O
in O
Table O
7 O
and O
outlined O
in O
Figure O
6 O
. O
Our O
method O
consumes O
approximately O
6.6 O
times O
less O
total O
kWh O
on O
a O
GTX O
Titan O
( O
10 O
times O
less O
on O
a O
GTX O
1080 O
) O
. O
Directly O
related O
, O
it O
also O
reduces O
the O
training O
time O
: O
prior O
work O
requires O
appr O
. O
1500s O
training O
time O
per O
epoch O
( O
GTX O
1080 O
) O
, O
while O
our O
method O
requires O
appr O
. O
60s O
per O
epoch O
( O
GTX O
1080 O
) O
. O
The O
main O
reason O
for O
this O
is O
that O
our O
model O
does O
not O
depend O
on O
recurrent O
operations O
and O
proÔ¨Åts O
more O
from O
parallelism O
. O
4 O
Related O
work O
Quality O
measurement O
of O
structured O
predictions O
Since O
evaluating O
structured O
representations O
against O
human O
annotations O
is O
costly O
, O
systems O
have O
been O
developed O
that O
attempt O
an O
automatic O
quality O
assessment O
of O
these O
structures O
. O
Due O
to O
its O
popularity O
, O
much O
work O
has O
been O
conducted O
in O
machine O
translation O
( O
MT O
) O
under O
the O
umbrella O
of O
quality O
estimation O
( O
QE O
) O
. O
QE O
can O
take O
place O
either O
on O
a O
wordlevel O
( O
Martins O
et O
al O
. O
, O
2017 O
) O
, O
sentence O
- O
level O
( O
Spe O
- O
cia O
et O
al O
. O
, O
2009 O
) O
, O
or O
document O
- O
level O
( O
Scarton O
et O
al O
. O
, O
2015 O
) O
. O
The O
conference O
on O
Machine O
Translation O
( O
WMT O
) O
has O
a O
long O
- O
standing O
workshop O
and O
shared O
task O
- O
series O
on O
MT O
quality O
assessment O
( O
Bojar O
et O
al O
. O
, O
2013 O
, O
2014 O
, O
2015 O
, O
2016 O
, O
2017 O
; O
Specia O
et O
al O
. O
, O
2018 O
; O
Fonseca O
et O
al O
. O
, O
2019 O
) O
. O
Quality O
estimation O
for O
neural O
language O
generation O
has O
been O
investigated O
, O
i.a O
. O
, O
by O
Scarton O
et O
al O
. O
( O
2016 O
) O
, O
and O
recently O
by O
Du O
Àásek O
et O
al O
. O
( O
2019 O
) O
, O
who O
design O
a O
model O
that O
jointly O
learns O
to O
rate O
and O
rank O
generations O
, O
or O
by O
Zopf O
( O
2018 O
) O
, O
who O
predicts O
pair O
- O
wise O
preferences O
for O
generated O
summaries O
. O
Furthermore O
, O
automatic O
techniques O
for O
the O
quality O
assessment O
of O
syntactic O
parses O
have O
been O
proposed O
. O
For O
instance O
, O
Ravi O
et O
al O
. O
( O
2008 O
) O
formulate O
the O
task O
as O
a O
single O
- O
variable O
regression O
problem O
to O
assess O
the O
quality O
of O
constituency O
trees O
. O
A O
major O
difference O
to O
our O
work O
is O
that O
they O
try O
to O
assess O
the O
performance O
of O
a O
single O
parser O
, O
while O
we O
aim O
at O
a O
parser O
- O
agnostic O
setting O
where O
candidate O
parses O
stem O
from O
different O
parsers O
. O
Similarly O
, O
Kawahara O
and O
Uchimoto O
( O
2008 O
) O
predict O
a O
binary O
label O
that O
reÔ¨Çects O
whether O
the O
tree O
- O
quality O
lies O
above O
a O
certain O
threshold O
( O
or O
not O
) O
. O
When O
multiple O
candidate O
parses O
are O
available O
, O
tree O
ranking O
methods O
( O
Zhu O
et O
al O
. O
, O
2015 O
; O
Zhou O
et O
al O
. O
, O
2016 O
) O
may O
also O
be O
interpreted O
as O
some O
form O
of O
parse O
quality O
assessment O
( O
see O
Do O
and O
Rehbein O
( O
2020 O
) O
for O
a O
recent O
overview O
) O
. O
Compared O
with O
assessing O
the O
quality O
of O
( O
abstract O
) O
meaning O
representations O
, O
judging O
about O
syntactic O
trees O
perhaps O
is O
a O
conceptually O
slightly O
simpler O
task O
, O
since O
the O
syntactic O
graphs O
are O
more O
directly O
grounded O
in O
the O
sentence10 O
, O
and O
therefore O
it O
may O
be O
easier O
to O
judge O
whether O
graph O
components O
are O
correct O
, O
redundant O
, O
missing O
, O
or O
false O
. O
In O
comparison O
to O
MT O
, O
automatic O
quality O
assessment O
of O
meaning O
representations O
is O
insufÔ¨Åciently O
researched O
. O
Opitz O
and O
Frank O
( O
2019 O
) O
propose O
an O
LSTM O
based O
model O
that O
performs O
a O
multi O
- O
variate O
quality O
analysis O
of O
AMRs O
( O
constituting O
the O
baseline O
which O
we O
compared O
against O
) O
. O
We O
believe O
that O
quality O
estimation O
approaches O
may O
also O
prove O
valuable O
for O
other O
meaning O
representation O
formalisms O
( O
MRs O
) O
, O
such O
as O
, O
e.g. O
, O
discourse O
representations O
( O
Kamp O
and O
Reyle O
, O
1993 O
; O
Kamp O
, O
2008 O
; O
Abzianidze O
et O
al O
. O
, O
2019 O
) O
or O
universal O
semantic O
dependencies O
( O
Reisinger O
et O
al O
. O
, O
2015 O
; O
Stengel O
- O
Eskin O
et O
al O
. O
, O
2020 O
) O
. O
For O
example O
, O
since O
the O
manual O
creation O
of O
MRs O
10In O
dependency O
trees O
, O
nodes O
are O
words O
; O
in O
constituency O
trees O
, O
nodes O
are O
( O
labeled O
) O
phrases O
; O
in O
meaning O
representations O
, O
words O
or O
phrases O
may O
be O
projected O
to O
abstract O
semantic O
nodes O
, O
or O
they O
may O
be O
omitted.242is O
a O
notoriously O
laborious O
task O
, O
automatic O
quality O
assessment O
tools O
could O
assist O
humans O
in O
the O
annotation O
process O
( O
e.g. O
, O
by O
serving O
as O
a O
cheap O
annotation O
quality O
check O
or O
by O
Ô¨Åltering O
automatic O
parses O
in O
active O
learning O
) O
. O
AMR O
metrics O
When O
a O
gold O
graph O
is O
available O
, O
it O
can O
be O
used O
to O
compute O
the O
canonical O
AMR O
metric O
Smatch O
( O
Cai O
and O
Knight O
, O
2013 O
) O
that O
assesses O
matching O
triples O
. O
Furthermore O
, O
Damonte O
et O
al O
. O
( O
2017 O
) O
have O
extended O
Smatch O
to O
inspect O
various O
aspects O
of O
AMR O
. O
In O
this O
work O
, O
we O
have O
shown O
that O
our O
model O
can O
predict O
the O
expected O
outcomes O
of O
these O
metrics O
in O
the O
absence O
of O
the O
gold O
graph O
. O
Recently O
, O
more O
AMR O
metrics O
have O
been O
proposed O
, O
for O
example O
the O
Bleu O
- O
based O
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
SemBleu O
metric O
( O
Song O
and O
Gildea O
, O
2019 O
) O
, O
Sema O
( O
Anchi O
ÀÜeta O
et O
al O
. O
, O
2019 O
) O
or O
S2match O
( O
Opitz O
et O
al O
. O
, O
2020 O
) O
, O
a O
variant O
of O
Smatch O
. O
We O
plan O
to O
extend O
our O
model O
such O
that O
it O
also O
predicts O
these O
metrics O
. O
AMR O
parsing O
Recent O
advances O
in O
AMR O
parsing O
have O
been O
achieved O
by O
parsers O
that O
either O
predict O
latent O
alignments O
jointly O
with O
nodes O
( O
Lyu O
and O
Titov O
, O
2018 O
) O
, O
or O
by O
transducing O
a O
graph O
from O
a O
sequence O
with O
a O
minimum O
spanning O
tree O
( O
MST O
) O
decoding O
algorithm O
( O
Zhang O
et O
al O
. O
, O
2019 O
) O
, O
or O
by O
focusing O
on O
core O
semantics O
in O
a O
top O
- O
down O
fashion O
( O
Cai O
and O
Lam O
, O
2019 O
) O
, O
or O
by O
performing O
auto O
- O
regressive O
decoding O
with O
a O
graph O
encoder O
( O
Cai O
and O
Lam O
, O
2020 O
) O
. O
Other O
approaches O
apply O
statistical O
machine O
translation O
( O
Pust O
et O
al O
. O
, O
2015 O
) O
or O
sequence O
- O
to O
- O
sequence O
models O
, O
which O
tend O
to O
suffer O
from O
data O
scarcity O
issues O
and O
need O
considerable O
amounts O
of O
silver O
data O
to O
improve O
results O
( O
van O
Noord O
and O
Bos O
, O
2017c O
; O
Konstas O
et O
al O
. O
, O
2017 O
) O
. O
Previously O
, O
alignment O
- O
based O
pipeline O
models O
have O
proved O
effective O
( O
Flanigan O
et O
al O
. O
, O
2014 O
) O
or O
transition O
- O
based O
approaches O
that O
convert O
dependency O
trees O
step O
- O
by O
- O
step O
to O
AMR O
graphs O
( O
Wang O
et O
al O
. O
, O
2015 O
, O
2016 O
; O
Lindemann O
et O
al O
. O
, O
2020 O
) O
. O
5 O
Conclusion O
In O
this O
work O
, O
we O
have O
developed O
an O
approach O
to O
rate O
the O
quality O
of O
AMR O
graphs O
in O
the O
absence O
of O
costly O
gold O
data O
. O
Our O
model O
imitates O
a O
human O
judge O
that O
is O
confronted O
, O
‚Äò O
on O
paper O
‚Äô O
, O
with O
the O
AMR O
in O
its O
native O
multi O
- O
line O
Penman O
format O
. O
We O
saw O
how O
this O
setup O
allowed O
efÔ¨Åcient O
AMR O
processing O
with O
convolutions O
. O
Our O
experiments O
indicate O
that O
the O
method O
rates O
AMR O
quality O
more O
accurately O
and O
more O
efÔ¨Åciently O
than O
previous O
work O
. O
Acknowledgments O
I O
am O
grateful O
to O
the O
anonymous O
reviewers O
for O
their O
valuable O
thoughts O
and O
comments O
. O
Moreover O
, O
I O
am O
grateful O
to O
Anette O
Frank O
for O
her O
thoughtful O
feedback O
on O
an O
earlier O
draft O
of O
this O
paper O
and O
her O
general O
guidance O
throughout O
my O
studies O
. O
Abstract O
Commonsense O
explanation O
generation O
aims O
to O
empower O
the O
machine O
‚Äôs O
sense O
- O
making O
capability O
by O
generating O
plausible O
explanations O
to O
statements O
against O
commonsense O
. O
While O
this O
task O
is O
easy O
to O
human O
, O
the O
machine O
still O
struggles O
to O
generate O
reasonable O
and O
informative O
explanations O
. O
In O
this O
work O
, O
we O
propose O
a O
method O
that O
Ô¨Årst O
extracts O
the O
underlying O
concepts O
which O
are O
served O
as O
bridges O
in O
the O
reasoning O
chain O
and O
then O
integrates O
these O
concepts O
to O
generate O
the O
Ô¨Ånal O
explanation O
. O
To O
facilitate O
the O
reasoning O
process O
, O
we O
utilize O
external O
commonsense O
knowledge O
to O
build O
the O
connection O
between O
a O
statement O
and O
the O
bridge O
concepts O
by O
extracting O
and O
pruning O
multi O
- O
hop O
paths O
to O
build O
a O
subgraph O
. O
We O
design O
a O
bridge O
concept O
extraction O
model O
that O
Ô¨Årst O
scores O
the O
triples O
, O
routes O
the O
paths O
in O
the O
subgraph O
, O
and O
further O
selects O
bridge O
concepts O
with O
weak O
supervision O
at O
both O
the O
triple O
level O
and O
the O
concept O
level O
. O
We O
conduct O
experiments O
on O
the O
commonsense O
explanation O
generation O
task O
and O
our O
model O
outperforms O
the O
state O
- O
of O
- O
the O
- O
art O
baselines O
in O
both O
automatic O
and O
human O
evaluation.1 O
1 O
Introduction O
Machine O
commonsense O
reasoning O
has O
been O
widely O
acknowledged O
as O
a O
crucial O
component O
of O
artiÔ¨Åcial O
intelligence O
and O
a O
considerable O
amount O
of O
work O
has O
been O
dedicated O
to O
evaluate O
this O
ability O
from O
various O
aspects O
in O
natural O
language O
processing O
( O
Levesque O
et O
al O
. O
, O
2011 O
; O
Talmor O
et O
al O
. O
, O
2018 O
; O
Sap O
et O
al O
. O
, O
2019 O
) O
. O
A O
large O
proportion O
of O
existing O
tasks O
frame O
commonsense O
reasoning O
as O
multi O
- O
choice O
reading O
comprehension O
problems O
, O
which O
lack O
direct O
assessment O
to O
machine O
commonsense O
( O
Wang O
et O
al O
. O
, O
2019 O
) O
and O
impede O
its O
practicability O
to O
realistic O
scenarios O
( O
Lin O
‚àóCorresponding O
author O
1The O
source O
code O
is O
available O
at O
https://github O
. O
com O
/ O
cdjhz O
/ O
CommExpGen O
.Statement O
: O
Theschool O
was O
open O
for O
summer O
. O
Explanation O
: O
Summertime O
is O
typically O
vacation O
time O
forschool O
. O
Figure O
1 O
: O
Generating O
a O
reasonable O
and O
informative O
explanation O
involves O
generating O
bridge O
concepts O
likevacation O
by O
identifying O
the O
relation O
to O
the O
source O
concepts O
, O
i.e. O
school O
andsummer O
in O
the O
statement O
. O
et O
al O
. O
, O
2019b O
) O
. O
Recently O
, O
Wang O
et O
al O
. O
( O
2019 O
) O
proposed O
a O
commonsense O
explanation O
generation O
challenge O
that O
directly O
tests O
machine O
‚Äôs O
sense O
- O
making O
capability O
via O
commonsense O
reasoning O
. O
In O
this O
paper O
, O
we O
focus O
on O
the O
challenging O
explanation O
generation O
task O
where O
the O
goal O
is O
to O
generate O
a O
sentence O
to O
explain O
the O
reasons O
why O
the O
input O
statement O
is O
against O
commonsense O
, O
as O
shown O
in O
Figure O
1 O
. O
Generating O
a O
reasonable O
explanation O
for O
a O
statement O
faces O
two O
main O
challenges O
: O
1 O
) O
Trivial O
and O
uninformative O
explanations O
. O
As O
this O
task O
can O
be O
formulated O
as O
a O
sequence O
- O
to O
- O
sequence O
generation O
task O
, O
existing O
neural O
language O
generation O
models O
tend O
to O
generate O
trivial O
and O
uninformative O
explanations O
. O
For O
example O
, O
one O
of O
the O
existing O
neural O
models O
generates O
an O
explanation O
‚Äú O
The O
school O
was O
n‚Äôt O
open O
for O
summer O
‚Äù O
to O
the O
statement O
in O
Figure O
1 O
. O
Although O
it O
is O
sometimes O
reasonable O
, O
simple O
modiÔ¨Åcation O
of O
the O
statement O
to O
the O
negation O
form O
with O
no O
additional O
information O
can O
not O
explain O
the O
reasons O
why O
the O
statement O
conÔ¨Çicts O
with O
commonsense O
. O
2 O
) O
Noisy O
commonsense O
knowledge O
grounding O
. O
It O
‚Äôs O
still O
challenging O
for O
most O
existing O
language O
generation O
models O
to O
generate O
explanations O
that O
are O
faithful O
to O
commonsense O
( O
Lin O
et O
al O
. O
, O
2019b O
) O
. O
Thus O
, O
explicitly O
incorporating O
external O
knowledge O
sources O
is O
necessary O
for O
this O
task O
. O
Since O
the O
nature O
of O
the O
explanation O
generation O
task O
involves O
using O
underlying O
commonsense O
knowledge O
to O
explain O
, O
locating O
useful O
commonsense O
knowledge O
from O
large O
- O
scale O
knowledge O
graph O
is O
not O
trivial O
and O
generally O
requires O
multi O
- O
hop O
reasoning.248To O
address O
the O
above O
challenges O
, O
we O
propose O
a O
two O
- O
stage O
generation O
framework O
that O
Ô¨Årst O
extracts O
the O
critical O
concepts O
served O
as O
bridges O
between O
the O
statement O
and O
the O
explanation O
from O
an O
external O
commonsense O
knowledge O
graph O
, O
and O
then O
generates O
plausible O
explanations O
with O
these O
concepts O
. O
We O
Ô¨Årst O
retrieve O
multi O
- O
hop O
reasoning O
paths O
from O
ConceptNet O
( O
Speer O
et O
al O
. O
, O
2017 O
) O
and O
heuristically O
prune O
the O
paths O
to O
maintain O
the O
coverage O
to O
plausible O
concepts O
while O
keeping O
the O
scale O
of O
the O
subgraph O
tractable O
. O
Before O
the O
extraction O
stage O
, O
we O
initialize O
the O
representation O
of O
each O
node O
on O
the O
subgraph O
by O
fusing O
both O
the O
contextual O
and O
graph O
information O
. O
Then O
, O
we O
design O
a O
bridge O
concept O
extraction O
model O
that O
scores O
triples O
, O
propagates O
the O
probabilities O
along O
multi O
- O
hop O
paths O
to O
the O
connected O
concepts O
and O
further O
extracts O
plausible O
concepts O
. O
In O
the O
second O
stage O
, O
we O
use O
a O
pre O
- O
trained O
language O
model O
( O
Radford O
et O
al O
. O
, O
2019 O
) O
to O
generate O
the O
explanation O
by O
integrating O
both O
the O
statement O
and O
the O
extracted O
concept O
representations O
. O
Experimental O
results O
show O
that O
our O
framework O
outperforms O
knowledge O
- O
aware O
text O
generation O
baselines O
and O
GPT-2 O
( O
Radford O
et O
al O
. O
, O
2019 O
) O
in O
both O
automatic O
and O
human O
evaluation O
. O
Particularly O
, O
our O
model O
generates O
explanations O
with O
more O
informative O
content O
and O
provides O
reasoning O
paths O
on O
the O
knowledge O
graph O
for O
concept O
extraction O
. O
To O
summarize O
, O
our O
contributions O
are O
two O
- O
fold O
: O
‚Ä¢We O
analyze O
the O
under O
- O
explored O
commonsense O
explanation O
generation O
task O
and O
investigate O
the O
challenges O
in O
incorporating O
external O
knowledge O
graph O
to O
aid O
the O
generation O
problem O
. O
To O
the O
best O
of O
our O
knowledge O
, O
this O
is O
the O
Ô¨Årst O
work O
on O
generating O
explanations O
for O
counter O
- O
commonsense O
statements O
. O
‚Ä¢We O
propose O
a O
two O
- O
stage O
generation O
method O
that O
Ô¨Årst O
extracts O
the O
bridge O
concepts O
from O
reasoning O
paths O
and O
then O
generates O
the O
explanation O
based O
on O
these O
concepts O
. O
Our O
model O
outperforms O
state O
- O
of O
- O
the O
- O
art O
baselines O
on O
the O
commonsense O
explanation O
generation O
task O
in O
both O
automatic O
and O
human O
evaluation O
. O
2 O
Related O
Work O
2.1 O
Machine O
Commonsense O
Reasoning O
Previous O
work O
on O
machine O
commonsense O
reasoning O
mainly O
focuses O
on O
the O
tasks O
of O
inference O
( O
Levesque O
et O
al O
. O
, O
2011 O
) O
, O
question O
answering O
( O
Talmor O
et O
al O
. O
, O
2018 O
; O
Sap O
et O
al O
. O
, O
2019 O
) O
andknowledge O
base O
completion O
( O
Bosselut O
et O
al O
. O
, O
2019 O
) O
. O
While O
the O
ultimate O
goals O
of O
these O
tasks O
are O
different O
from O
ours O
, O
we O
argue O
that O
performing O
explicit O
commonsense O
reasoning O
is O
also O
critical O
to O
generation O
. O
A O
line O
of O
work O
( O
Bauer O
et O
al O
. O
, O
2018 O
; O
Lin O
et O
al O
. O
, O
2019a O
) O
resorts O
to O
structured O
commonsense O
knowledge O
and O
builds O
graph O
- O
aware O
representations O
along O
with O
the O
contextualized O
word O
embeddings O
to O
tackle O
the O
commonsense O
question O
answering O
problem O
. O
In O
our O
work O
, O
we O
focus O
on O
reasoning O
over O
structured O
knowledge O
to O
explicitly O
infer O
discrete O
bridge O
concepts O
that O
are O
further O
used O
for O
text O
generation O
. O
Another O
line O
of O
work O
( O
Rajani O
et O
al O
. O
, O
2019 O
; O
Khot O
et O
al O
. O
, O
2019 O
) O
identiÔ¨Åes O
the O
knowledge O
gap O
critical O
for O
the O
complete O
reasoning O
chain O
and O
Ô¨Ålls O
the O
gap O
by O
writing O
general O
explanation O
or O
acquiring O
Ô¨Åne O
- O
grained O
annotations O
with O
human O
effort O
. O
While O
sharing O
a O
similar O
motivation O
, O
our O
method O
differs O
from O
theirs O
in O
the O
sense O
that O
we O
acquire O
distant O
supervisions O
for O
the O
bridge O
concepts O
to O
extract O
reasoning O
paths O
and O
generate O
plausible O
explanations O
without O
the O
need O
of O
additional O
human O
annotation O
. O
2.2 O
Knowledge O
- O
Grounded O
Text O
Generation O
Existing O
work O
that O
utilizes O
structured O
knowledge O
graphs O
to O
generate O
texts O
mainly O
lies O
in O
conversation O
generation O
( O
Zhou O
et O
al O
. O
, O
2018 O
; O
Tuan O
et O
al O
. O
, O
2019 O
; O
Moon O
et O
al O
. O
, O
2019 O
) O
, O
story O
generation O
( O
Guan O
et O
al O
. O
, O
2019 O
) O
and O
language O
modeling O
( O
Ahn O
et O
al O
. O
, O
2016 O
; O
Logan O
et O
al O
. O
, O
2019 O
; O
Hayashi O
et O
al O
. O
, O
2019 O
) O
. O
Zhou O
et O
al O
. O
( O
2018 O
) O
and O
Guan O
et O
al O
. O
( O
2019 O
) O
propose O
to O
use O
graph O
attention O
that O
incorporates O
the O
information O
of O
neighbouring O
concepts O
into O
context O
representations O
to O
help O
generate O
the O
target O
sentence O
. O
Yang O
et O
al O
. O
( O
2019 O
) O
resort O
to O
a O
dynamic O
concept O
memory O
that O
updates O
during O
essay O
generation O
. O
Guan O
et O
al O
. O
( O
2020 O
) O
conduct O
post O
- O
training O
on O
knowledge O
triples O
to O
enhance O
the O
GPT-2 O
with O
commonsense O
knowledge O
. O
Since O
one O
- O
hop O
graphs O
of O
concepts O
in O
the O
statement O
have O
low O
coverage O
to O
the O
concepts O
in O
the O
explanation O
, O
merely O
leveraging O
information O
of O
individual O
concepts O
or O
triples O
is O
not O
suitable O
for O
this O
task O
. O
Another O
direction O
that O
utilizes O
more O
complex O
graph O
is O
to O
model O
multi O
- O
hop O
reasoning O
by O
performing O
random O
walk O
( O
Moon O
et O
al O
. O
, O
2019 O
) O
on O
the O
knowledge O
graph O
or O
simulating O
a O
Markov O
process O
on O
the O
pre O
- O
extracted O
knowledge O
paths O
( O
Tuan O
et O
al O
. O
, O
2019 O
) O
. O
While O
in O
our O
task O
, O
we O
do O
n‚Äôt O
have O
access O
to O
a O
parallel O
grounded O
knowledge O
source O
nor O
the O
bridge O
concepts O
, O
which O
makes O
the O
problem O
even O
more O
challenging.249TheZaVforschoolopensXmmerReaVRQiQg O
PaWh O
ReWUieYaOCRQceSWNeWVchRROSRXUce O
CRQceSWVVchRROVXPPeUVchRROVXPPeUThe O
...... O
VchoolVXmmerYacaWionbreakTranVformerSXPPeUWiPe O
iV O
W\SicaOO\ O
YacaWiRn¬¨WiPe O
... O
TUiSOe O
ScRUiQgPaWh O
RRXWiQgCRQceSW O
SeOecWiRQ O
... O
TRS O
- O
UaQNed O
CRQceSWVYacaWiRQbUeakWimehXmaQ O
VXPPeUSWaWePeQWFigure O
2 O
: O
The O
inference O
process O
of O
our O
model O
. O
In O
the O
reasoning O
path O
retrieval O
stage O
( O
¬ß O
3.3 O
) O
, O
a O
subgraph O
is O
Ô¨Årstly O
retrieved O
from O
the O
ConceptNet O
given O
the O
source O
concepts O
( O
Cx O
) O
, O
where O
each O
node O
representation O
is O
fused O
with O
both O
textual O
and O
graph O
- O
aware O
representations O
( O
¬ß O
3.4 O
) O
. O
Then O
the O
model O
scores O
each O
triple O
on O
the O
subgraph O
, O
routes O
the O
path O
by O
propagating O
the O
probabilities O
along O
paths O
to O
the O
connected O
nodes O
, O
and O
selects O
concepts O
from O
activated O
nodes O
( O
¬ß O
3.5 O
) O
. O
Finally O
, O
the O
model O
generates O
the O
explanation O
by O
integrating O
the O
token O
embeddings O
of O
both O
the O
statement O
and O
the O
top O
- O
ranked O
concepts O
( O
¬ß O
3.6 O
) O
. O
3 O
Methodology O
3.1 O
Task O
DeÔ¨Ånition O
The O
commonsense O
explanation O
generation O
task O
is O
deÔ¨Åned O
as O
generating O
an O
explanation O
given O
a O
statement O
against O
commonsense O
. O
Let O
x O
= O
x1¬∑¬∑¬∑xN O
be O
the O
input O
statement O
with O
Nwords O
and O
y= O
y1¬∑¬∑¬∑yMbe O
the O
explanation O
with O
Mwords O
. O
A O
simple O
sequence O
- O
to O
- O
sequence O
formulation O
which O
learns O
a O
mapping O
from O
xtoycan O
be O
adopted O
in O
this O
task O
: O
P(y|x O
) O
= O
M O
/ O
productdisplay O
t=1P(yt|y O
< O
t O
, O
x O
) O
. O
( O
1 O
) O
3.2 O
Model O
Overview O
Formally O
, O
our O
model O
generates O
the O
explanation O
by O
Ô¨Årstly O
extracting O
the O
critical O
bridge O
concepts O
con O
a O
retrieved O
knowledge O
graph O
Gxgiven O
the O
statement O
xand O
then O
integrating O
the O
bridge O
concepts O
and O
the O
statement O
to O
generate O
a O
proper O
explanation O
y O
, O
which O
can O
be O
formulated O
as O
follows O
: O
P(y O
, O
c|x O
) O
= O
P(c|x)P(y|x O
, O
c O
) O
( O
2 O
) O
where O
the O
bridge O
concepts O
care O
deÔ¨Åned O
as O
the O
unique O
concepts O
delivered O
in O
the O
explanation O
but O
not O
mentioned O
in O
the O
statement O
. O
Figure O
2 O
presents O
the O
overview O
of O
our O
model O
framework O
. O
Firstly O
, O
we O
retrieve O
multi O
- O
hop O
reasoning O
paths O
from O
the O
ConceptNet O
based O
on O
the O
statement O
, O
and O
heuristically O
prune O
the O
noisy O
connections O
to O
obtain O
a O
subgraph O
for O
further O
concept O
extraction O
( O
¬ß O
3.3 O
) O
. O
To O
score O
the O
paths O
and O
concepts O
, O
we O
obtain O
the O
fused O
concept O
representation O
for O
each O
node O
on O
the O
subgraph O
by O
considering O
both O
the O
contextual O
and O
graph O
information O
( O
¬ß O
3.4 O
) O
. O
Secondly O
, O
we O
design O
a O
path O
routing O
algorithm O
to O
propagate O
the O
triple O
probabilities O
alongmulti O
- O
hop O
paths O
to O
the O
connected O
concepts O
and O
further O
extract O
plausible O
concepts O
( O
¬ß O
3.5 O
) O
. O
Finally O
, O
our O
model O
generates O
the O
explanation O
by O
integrating O
the O
statement O
representation O
and O
the O
selected O
concept O
representation O
as O
inputs O
( O
¬ß O
3.6 O
) O
. O
3.3 O
Reasoning O
Path O
Retrieval O
In O
this O
section O
, O
we O
demonstrate O
how O
we O
retrieve O
and O
prune O
the O
reasoning O
paths O
to O
form O
a O
subgraph O
. O
We O
also O
acquire O
distant O
supervision O
for O
uncovering O
the O
bridge O
concepts O
in O
the O
subgraph O
to O
supervise O
the O
concept O
extraction O
in O
the O
next O
stage O
. O
Given O
an O
external O
commonsense O
knowledge O
graphG= O
( O
V O
, O
E O
) O
, O
for O
each O
statement O
x O
, O
we O
extract O
source O
conceptsCx={ci O
x}fromxby O
aligning O
the O
surface O
texts O
in O
xto O
the O
concepts O
in O
V. O
We O
also O
use O
the O
stem O
form O
of O
the O
surface O
texts O
to O
enable O
soft O
alignment O
and O
Ô¨Ålter O
out O
stop O
words O
. O
At O
the O
training O
phase O
, O
we O
extract O
the O
target O
concepts O
Cy={cj O
y O
} O
from O
the O
explanation O
ywith O
a O
similar O
procedure O
. O
Starting O
with O
the O
source O
concepts O
, O
we O
then O
retrieve O
reasoning O
paths O
from O
the O
knowledge O
graph O
to O
form O
a O
subgraph O
that O
has O
relatively O
high O
coverage O
to O
the O
bridge O
concepts O
with O
a O
tractable O
scale O
. O
We O
Ô¨Årst O
examine O
the O
minimum O
length O
of O
paths O
that O
connect O
source O
concepts O
Cxwith O
each O
concept O
in O
the O
explanation O
set O
Cy‚àíCx O
. O
As O
shown O
in O
Figure O
3 O
, O
over O
80 O
% O
of O
the O
examples O
require O
two O
or O
three O
hops O
of O
connection O
from O
the O
source O
concepts O
to O
the O
concepts O
that O
are O
merely O
mentioned O
in O
the O
explanation O
, O
which O
indicates O
the O
necessity O
for O
multi O
- O
hop O
reasoning O
. O
We O
then O
count O
the O
number O
of O
concepts O
covered O
by O
subgraphs O
with O
different O
numbers O
of O
hops O
starting O
from O
the O
source O
concepts O
( O
We O
only O
consider O
concepts O
in O
the O
training O
data O
) O
. O
As O
Figure O
3 O
shows O
, O
the O
average O
number O
of O
nodes O
covered O
by O
3 O
- O
hop O
sub-25002000400060008000#Nodes1234#Hops020406080Proportion O
( O
% O
) O
Figure O
3 O
: O
The O
left O
axis O
presents O
the O
distribution O
of O
the O
minimum O
required O
number O
of O
hops O
to O
reach O
the O
concepts O
in O
the O
explanation O
set O
Cy‚àíCxfrom O
the O
source O
concepts O
inCx O
. O
The O
right O
axis O
shows O
the O
number O
of O
nodes O
in O
the O
subgraph O
with O
different O
number O
of O
hops O
. O
graph O
exceeds O
6,000 O
, O
indicating O
the O
need O
of O
path O
pruning O
to O
keep O
the O
scale O
tractable O
. O
Therefore O
, O
we O
design O
a O
heuristic O
algorithm O
to O
retrieve O
a O
subgraph O
Gx={Vx O
, O
Ex}from O
the O
ConceptNet O
by O
expanding O
the O
source O
concepts O
with O
3 O
hops O
to O
cover O
most O
bridge O
concepts O
. O
To O
keep O
the O
scale O
of O
the O
subgraph O
tractable O
, O
at O
each O
iterating O
step O
, O
we O
enlarge O
VxwithBneighbour O
concepts O
most O
commonly O
visited O
by O
concepts O
in O
Vx O
. O
Intuitively O
, O
the O
salient O
bridge O
concepts O
should O
be O
in O
a O
reasonable O
distance O
from O
the O
source O
concepts O
on O
the O
graph O
to O
maintain O
the O
semantic O
relation O
and O
should O
be O
commonly O
visited O
nodes O
that O
support O
the O
information O
Ô¨Çow O
on O
the O
graph O
. O
We O
distantly O
label O
the O
bridge O
concepts O
as O
the O
unique O
concepts O
in O
the O
explanation O
that O
could O
be O
covered O
by O
the O
subgraph O
: O
Bx‚Üíy={c|c‚ààCy‚àíCx O
, O
c‚ààVx O
} O
( O
3 O
) O
3.4 O
Fused O
Concept O
Representation O
We O
initialize O
each O
node O
on O
the O
subgraph O
with O
a O
fused O
concept O
representation O
hcby O
considering O
both O
the O
contextual O
feature O
of O
the O
concept O
and O
the O
graph O
- O
aware O
information O
. O
We O
Ô¨Årst O
obtain O
the O
contextualized O
statement O
representation O
Hx‚ààRN√ód1 O
using O
a O
multi O
- O
layer O
bi O
- O
directional O
Transformer O
encoder O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O
H0 O
x O
= O
onehot(x)¬∑We+Wp O
( O
4 O
) O
Hl O
x O
= O
trmblock O
( O
Hl‚àí1 O
x O
) O
, O
l= O
1, O
... O
,L O
( O
5 O
) O
where O
Weis O
the O
token O
embedding O
matrix O
, O
Wpis O
the O
position O
embedding O
matrix O
, O
trmblock O
( O
¬∑ O
) O
is O
the O
transformer O
block O
with O
bi O
- O
directional O
attention O
andLis O
the O
number O
of O
Transformer O
blocks O
. O
We O
typically O
choose O
the O
output O
of O
the O
last O
layer O
HL O
xas O
the O
statement O
representation O
Hx O
. O
Then O
we O
consider O
the O
following O
embeddings:‚Ä¢Context O
- O
aware O
token O
embedding O
. O
In O
order O
to O
enhance O
the O
contextual O
dependency O
of O
the O
conceptcto O
the O
statement O
x O
, O
we O
utilize O
a O
biattention O
network O
( O
Seo O
et O
al O
. O
, O
2016 O
) O
that O
models O
the O
cross O
interaction O
between O
the O
concept O
and O
the O
statement O
. O
Htok O
c O
= O
onehot(c)¬∑We O
( O
6 O
) O
Hcon O
c O
= O
bi O
- O
attention O
( O
Htok O
c O
, O
Hx)(7 O
) O
Then O
we O
integrate O
Htok O
candHcon O
cby O
max O
pooling O
and O
linear O
transformation O
to O
obtain O
a O
Ô¨Åxed O
- O
length O
representation O
that O
encodes O
the O
textual O
information O
of O
the O
concept O
c O
: O
htext O
c O
= O
mlp O
/ O
parenleftBig O
max O
/ O
parenleftbig O
[ O
Htok O
c;Hcon O
c]/parenrightbig O
/ O
parenrightBig O
( O
8) O
‚Ä¢Concept O
distance O
embedding O
. O
To O
encode O
the O
graph O
- O
aware O
structure O
information O
into O
the O
node O
representation O
, O
we O
design O
a O
concept O
distance O
embedding O
hdist O
c‚ààRd1that O
encodes O
the O
relative O
distance O
from O
concept O
cto O
the O
source O
conceptsCxon O
the O
subgraph O
. O
SpeciÔ¨Åcally O
, O
the O
concept O
distance O
for O
concept O
cis O
deÔ¨Åned O
as O
the O
minimum O
length O
of O
the O
path O
that O
can O
be O
reached O
from O
one O
source O
concept O
in O
Cx O
: O
dc= O
min O
cx‚ààCxDist(cx O
, O
c O
) O
( O
9 O
) O
The O
concept O
distance O
is O
then O
used O
as O
an O
index O
to O
look O
up O
a O
trainable O
matrix O
Wdand O
obtain O
thehdist O
c‚ààRd1 O
. O
Finally O
, O
the O
fused O
concept O
representation O
hcis O
obtained O
by O
concatenating O
the O
context O
- O
aware O
token O
embedding O
and O
the O
concept O
distance O
embedding O
. O
hc= O
[ O
htext O
c;hdist O
c O
] O
( O
10 O
) O
3.5 O
Bridge O
Concept O
Extraction O
We O
describe O
the O
core O
component O
of O
our O
method O
in O
this O
section O
, O
which O
extracts O
the O
bridge O
concepts O
for O
further O
explanation O
generation O
. O
It O
Ô¨Årst O
scores O
triples O
on O
the O
subgraph O
to O
downweight O
the O
noisy O
paths O
. O
Then O
it O
aggregates O
the O
path O
scores O
to O
each O
connected O
concepts O
by O
a O
path O
routing O
process O
and O
deactivates O
the O
nodes O
with O
low O
routing O
scores O
. O
Finally O
it O
selects O
top O
- O
ranked O
bridge O
concepts O
from O
the O
activated O
nodes.2513.5.1 O
Triple O
Scoring O
Firstly O
, O
we O
calculate O
the O
triple O
scores O
according O
to O
the O
representation O
of O
triples O
and O
the O
input O
statement O
. O
For O
each O
triple O
e= O
( O
ce O
, O
head O
, O
re O
, O
ce O
, O
tail O
) O
wherece O
, O
head O
/ce O
, O
tail O
indicates O
the O
head O
/ O
tail O
concept O
andredenotes O
the O
relation O
, O
we O
can O
obtain O
its O
representation O
by O
concatenating O
the O
representations O
of O
the O
head O
concept O
, O
the O
relation O
and O
the O
tail O
concept O
: O
he= O
[ O
hce O
, O
head;hre;hce O
, O
tail O
] O
( O
11 O
) O
Both O
the O
head O
and O
the O
tail O
representations O
are O
calculated O
by O
Equation O
( O
10 O
) O
and O
the O
relation O
representation O
is O
acquired O
by O
indexing O
a O
trainable O
relation O
embedding O
matrix O
Wr O
. O
Then O
we O
use O
the O
statement O
representation O
to O
query O
each O
triple O
representation O
by O
taking O
the O
bilinear O
dot O
- O
product O
attention O
and O
calculate O
the O
selection O
probability O
for O
each O
triple O
: O
hx O
= O
max O
- O
pooling O
( O
Hx)‚ààRd1(12 O
) O
P(e|x O
) O
= O
œÉ(heW2hT O
x O
) O
( O
13 O
) O
We O
adopt O
weak O
supervision O
to O
supervise O
the O
triple O
scoring O
process O
. O
For O
each O
concept O
c‚ààBx‚Üíy O
, O
we O
obtain O
the O
set O
of O
the O
shortest O
paths O
Px‚Üícusing O
the O
breadth-Ô¨Årst O
search O
from O
each O
concept O
ofCxtoc O
. O
We O
consider O
all O
these O
shortest O
paths O
Px‚Üíy=/uniontext O
c‚ààBx‚ÜíyPx‚Üícas O
the O
supervision O
of O
our O
triple O
scoring O
process O
as O
they O
connect O
the O
reasoning O
chain O
from O
the O
statement O
to O
the O
explanation O
with O
minimum O
distractive O
information O
. O
Accordingly O
, O
other O
triples O
in O
Gxwhich O
do O
n‚Äôt O
belong O
to O
Px‚Üíyare O
regarded O
as O
negative O
samples O
. O
The O
loss O
function O
of O
triple O
scoring O
is O
devised O
as O
follows O
: O
Ltriple O
= O
‚àí/summationdisplay O
e‚ààGxI(e‚ààPx‚Üíy O
) O
logP(e|x O
) O
+ O
[ O
1‚àíI(e‚ààPx‚Üíy O
) O
] O
log[1‚àíP(e|x O
) O
] O
( O
14 O
) O
where O
I(e‚ààPx‚Üíy)is O
an O
indicator O
function O
that O
takes O
the O
value O
1 O
iff O
e‚ààPx‚Üíy O
, O
and O
0 O
otherwise O
. O
3.5.2 O
Path O
Routing O
Next O
, O
we O
describe O
the O
path O
routing O
process O
which O
involves O
propagating O
the O
scores O
along O
the O
paths O
to O
each O
concept O
on O
the O
subgraph O
from O
the O
source O
concepts O
. O
For O
each O
path O
pretrieved O
from O
the O
subgraph O
Gx O
, O
we O
calculate O
a O
path O
score O
s(p)by O
aggregating O
the O
triple O
score O
P(e|x)along O
the O
path O
: O
s(p O
) O
= O
1 O
|p|/summationdisplay O
e‚ààpP(e|x O
) O
( O
15)For O
each O
concept O
c O
, O
we O
consider O
all O
the O
shortest O
paths O
Px‚Üícthat O
starts O
with O
the O
source O
concepts O
and O
ends O
with O
cmonotonically O
, O
i.e. O
, O
the O
concept O
distance O
of O
each O
node O
on O
the O
path O
increases O
monotonically O
along O
the O
path O
. O
Then O
we O
calculate O
the O
routing O
score O
for O
the O
concept O
cby O
averaging O
the O
path O
scores O
of O
Px‚Üíc O
. O
s(c O
) O
= O
1 O
|Px‚Üíc|/summationdisplay O
p‚ààPx‚Üícs(p O
) O
( O
16 O
) O
Intuitively O
, O
this O
process O
disseminates O
the O
triple O
scores O
and O
aggregates O
them O
to O
the O
connected O
concepts O
. O
Then O
we O
deactivate O
some O
paths O
based O
on O
the O
path O
routing O
results O
and O
obtain O
Vx‚Üíyby O
preserving O
concepts O
with O
the O
top- O
K1routing O
scores O
. O
3.5.3 O
Concept O
Selection O
Finally O
, O
we O
conduct O
concept O
selection O
based O
on O
the O
concept O
representation O
and O
the O
statement O
representation O
. O
For O
each O
concept O
in O
Vx‚Üíy O
, O
we O
calculate O
the O
selection O
probability O
for O
it O
by O
taking O
the O
dotproduct O
attention O
and O
adopt O
a O
similar O
cross O
- O
entropy O
loss O
with O
supervision O
from O
bridge O
concepts O
Bx‚Üíy O
: O
P(c|x O
) O
= O
œÉ(hcW3hT O
x O
) O
( O
17 O
) O
Lconcept O
= O
‚àí/summationdisplay O
c‚ààVx‚ÜíyI(c‚ààBx‚Üíy O
) O
logP(c|x O
) O
+ O
[ O
1‚àíI(c‚ààBx‚Üíy O
) O
] O
log[1‚àíP(c|x O
) O
] O
( O
18 O
) O
where O
the O
indicator O
function O
is O
similar O
to O
that O
of O
Equation O
( O
14 O
) O
. O
Finally O
, O
the O
bridge O
concepts O
with O
top- O
K2probabilityP(c|x)are O
selected O
as O
the O
additional O
input O
to O
the O
generation O
model O
. O
3.6 O
Explanation O
Generation O
We O
utilize O
a O
pre O
- O
trained O
Transformer O
decoder O
( O
Radford O
et O
al O
. O
, O
2019 O
) O
as O
our O
generation O
model O
which O
shares O
the O
parameter O
with O
the O
Transformer O
encoder O
. O
Essentially O
, O
it O
takes O
the O
statement O
xand O
the O
conceptscas O
input O
and O
auto O
- O
regressively O
generates O
the O
explanation O
y O
: O
P(y|x O
, O
c O
) O
= O
P(y|x O
, O
c1,¬∑¬∑¬∑,cK2 O
) O
= O
M O
/ O
productdisplay O
t=1P(yt|x O
, O
c1,¬∑¬∑¬∑,cK2,y O
< O
t O
) O
( O
19 O
) O
Lgeneration O
= O
‚àílogP(y|x O
, O
c1,¬∑¬∑¬∑,cK2)(20)252As O
shown O
in O
Figure O
2 O
, O
the O
input O
to O
the O
Transformer O
decoder O
is O
the O
token O
embeddings O
of O
both O
the O
statement O
and O
the O
selected O
concepts O
concatenated O
along O
the O
sequence O
length O
dimension O
. O
To O
model O
bi O
- O
directional O
attention O
on O
the O
input O
side O
while O
preserving O
the O
causal O
dependency O
of O
the O
generated O
sequence O
, O
we O
adopt O
a O
hybrid O
attention O
mask O
where O
each O
token O
on O
the O
input O
side O
could O
attend O
to O
all O
the O
tokens O
in O
the O
input O
sequence O
while O
the O
generated O
token O
at O
each O
time O
step O
only O
attends O
to O
the O
input O
sequence O
and O
the O
previously O
generated O
tokens O
. O
3.7 O
Training O
and O
Inference O
To O
train O
the O
model O
, O
we O
optimize O
the O
Ô¨Ånal O
loss O
function O
which O
is O
the O
sum O
of O
the O
three O
loss O
functions O
: O
Lfinal O
= O
Lgeneration O
+ O
Œª1Ltriple O
+ O
Œª2Lconcept O
( O
21 O
) O
As O
for O
the O
inference O
process O
, O
Figure O
2 O
demonstrates O
how O
our O
model O
retrieves O
reasoning O
paths O
given O
the O
statement O
, O
extracts O
bridge O
concepts O
and O
Ô¨Ånally O
generates O
the O
explanation O
. O
4 O
Experiment O
4.1 O
Dataset O
and O
Experimental O
Setup O
4.1.1 O
Commonsense O
Explanation O
Dataset O
We O
adopt O
the O
dataset O
from O
the O
Commonsense O
Validation O
and O
Explanation O
Challenge2which O
consists O
of O
three O
subtasks O
, O
i.e. O
, O
commonsense O
validation O
, O
commonsense O
explanation O
selection O
and O
commonsense O
explanation O
generation O
. O
We O
focus O
on O
the O
explanation O
generation O
subtask O
in O
this O
paper O
. O
The O
commonsense O
explanation O
generation O
subtask O
contains O
10,000statements O
that O
are O
against O
commonsense O
. O
For O
each O
statement O
, O
three O
human O
- O
written O
explanations O
are O
provided O
. O
To O
evaluate O
our O
proposed O
model O
and O
other O
baselines O
, O
we O
randomly O
split O
10 O
% O
data O
as O
the O
test O
set O
, O
5 O
% O
as O
the O
development O
set O
and O
the O
latter O
as O
the O
training O
set O
. O
Note O
that O
we O
further O
split O
each O
example O
in O
the O
training O
set O
into O
three O
statement O
- O
explanation O
pairs O
, O
while O
for O
the O
development O
set O
and O
the O
test O
set O
we O
use O
the O
three O
corresponding O
explanations O
as O
references O
for O
each O
statement O
. O
This O
results O
in O
our O
Ô¨Ånal O
data O
split O
( O
25,596 O
/ O
476 O
/ O
992 O
) O
denoted O
as O
( O
train O
/ O
dev O
/ O
test O
) O
. O
2https://competitions.codalab.org/ O
competitions/210804.1.2 O
Commonsense O
Knowledge O
Graph O
We O
use O
the O
English O
version O
ConceptNet O
as O
our O
external O
commonsense O
knowledge O
graph O
. O
It O
contains O
triples O
in O
the O
form O
of O
( O
h O
, O
r O
, O
t O
) O
wherehandtrepresent O
head O
and O
tail O
concepts O
and O
ris O
the O
relation O
type O
. O
We O
follow O
Lin O
et O
al O
. O
( O
2019a O
) O
to O
merge O
the O
original O
42 O
relation O
types O
into O
17 O
types O
. O
We O
additionally O
deÔ¨Åne O
17 O
reverse O
types O
corresponding O
to O
the O
original O
17 O
relation O
types O
to O
distinguish O
the O
direction O
of O
the O
triples O
on O
the O
graph O
. O
4.2 O
Automatic O
Evaluation O
Metrics O
To O
automatically O
evaluate O
the O
performance O
of O
the O
generation O
models O
, O
we O
use O
the O
BLEU-3/4 O
( O
Papineni O
et O
al O
. O
, O
2001 O
) O
, O
ROUGE-2 O
/ O
L O
( O
Lin O
, O
2004 O
) O
, O
METEOR O
( O
Banerjee O
and O
Lavie O
, O
2005 O
) O
as O
our O
main O
metrics O
. O
We O
also O
propose O
Concept O
F1 O
to O
evaluate O
the O
accuracy O
of O
the O
unique O
concepts O
in O
the O
generated O
explanation O
that O
do O
not O
occur O
in O
the O
statement O
. O
SpeciÔ¨Åcally O
, O
given O
the O
generated O
explanation O
ÀÜy O
and O
the O
reference O
explanation O
y O
, O
we O
extract O
a O
set O
of O
conceptsCÀÜyandCyfrom O
the O
generated O
explanation O
and O
the O
reference O
explanation O
respectively O
using O
the O
method O
in¬ß3.3 O
. O
We O
denote O
the O
sets O
of O
unique O
concepts O
in O
the O
explanation O
as O
Uy O
= O
Cy‚àíCxand O
UÀÜy O
= O
CÀÜy‚àíCx O
. O
Then O
we O
can O
compute O
the O
Concept O
F1 O
as O
the O
harmonic O
mean O
of O
recall O
andprecision O
. O
recall O
= O
|UÀÜy‚à©Uy| O
|Uy| O
, O
precision O
= O
|UÀÜy‚à©Uy| O
|UÀÜy| O
( O
22 O
) O
4.3 O
Implementation O
Details O
For O
the O
reasoning O
path O
retrieval O
process O
, O
we O
set O
the O
maximum O
number O
of O
neighbours O
B= O
300 O
at O
each O
hop O
. O
For O
each O
example O
, O
we O
restrict O
the O
concepts O
of O
the O
subgraph O
to O
those O
only O
appeared O
in O
the O
training O
and O
development O
set O
. O
We O
use O
a O
pre O
- O
trained O
Transformer O
language O
model O
GPT-2 O
( O
Radford O
et O
al O
. O
, O
2019 O
) O
as O
the O
initialization O
of O
the O
Transformer O
model O
. O
We O
set O
the O
hidden O
dimensiond1= O
768 O
identical O
to O
the O
hidden O
size O
of O
the O
Transformer O
. O
We O
empirically O
set O
the O
following O
hyperparameters O
by O
tuning O
the O
model O
on O
the O
development O
set O
: O
selection O
threshold O
K1= O
30,K2= O
3 O
, O
loss O
coefÔ¨Åcients O
Œª1= O
1,Œª2= O
1 O
, O
number O
of O
epochs O
= O
3 O
, O
batch O
size O
= O
4 O
, O
learning O
rate O
= O
4√ó10‚àí5and O
use O
the O
Adam O
optimizer O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
with O
10 O
% O
warmup O
steps O
. O
We O
select O
the O
model O
with O
the O
highest O
BLEU-4 O
score O
on O
the O
development O
set O
and O
evaluate O
it O
on O
the O
test O
set O
. O
At O
the O
decoding253Model O
B-3/4 O
R-2 O
/ O
L O
M O
Concept O
F1 O
Seq2Seq O
10.7/6.1 O
9.9/25.8 O
11.4 O
11.1 O
MemNet O
10.2/5.7 O
8.8/25.7 O
11.0 O
11.5 O
Transformer O
10.0/5.8 O
9.6/26.0 O
12.0 O
11.7 O
GPT-2 O
- O
FT O
23.4/15.7 O
18.9/36.5 O
17.7 O
17.4 O
Ours O
24.7/17.1 O
20.2/37.9 O
18.3 O
20.1 O
Table O
1 O
: O
Automatic O
evaluation O
of O
explanation O
generation O
in O
terms O
of O
BLEU O
( O
B O
) O
, O
ROUGE O
( O
R O
) O
, O
METEOR O
( O
M O
) O
and O
Concept O
F1 O
. O
Setting O
BLEU-4 O
Concept O
F1 O
Ours O
17.1 O
20.1 O
w/o O
Context O
Emb O
. O
16.0 O
18.6 O
w/o O
Distance O
Emb O
. O
16.4 O
18.5 O
w/o O
Path O
Routing O
16.5 O
19.2 O
# O
Hop O
= O
2 O
16.2 O
18.3 O
# O
Hop O
= O
1 O
15.9 O
17.3 O
Table O
2 O
: O
Ablation O
study O
of O
our O
framework O
on O
the O
test O
set O
. O
We O
present O
the O
model O
ablation O
results O
in O
the O
upper O
block O
and O
the O
data O
ablation O
results O
in O
the O
lower O
block O
. O
phase O
, O
we O
use O
beam O
search O
with O
a O
beam O
size O
of O
3 O
for O
all O
models O
. O
4.4 O
Baseline O
Models O
We O
compare O
with O
the O
following O
baseline O
models O
: O
‚Ä¢Seq2Seq O
: O
a O
sequence O
- O
to O
- O
sequence O
model O
based O
on O
gated O
recurrent O
unit O
( O
GRU O
) O
( O
Cho O
et O
al O
. O
, O
2014 O
) O
and O
attention O
mechanism O
, O
which O
is O
widely O
used O
in O
text O
generation O
tasks O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
) O
. O
‚Ä¢MemNet O
: O
a O
knowledge O
- O
grounded O
sequenceto O
- O
sequence O
model O
( O
Ghazvininejad O
et O
al O
. O
, O
2018 O
) O
. O
In O
our O
experimental O
setting O
, O
we O
regard O
all O
the O
concepts O
which O
are O
connected O
with O
those O
in O
the O
statements O
as O
knowledge O
facts O
. O
‚Ä¢Transformer O
: O
an O
encoder O
- O
decoder O
framework O
commonly O
used O
in O
machine O
translation O
tasks O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O
‚Ä¢GPT-2 O
: O
a O
multi O
- O
layer O
Transformer O
decoder O
pre O
- O
trained O
on O
WebText O
( O
Radford O
et O
al O
. O
, O
2019 O
) O
which O
is O
then O
directly O
Ô¨Åne O
- O
tuned O
on O
our O
dataset O
. O
4.5 O
Experimental O
Results O
As O
shown O
in O
Table O
1 O
, O
our O
model O
achieves O
the O
best O
performance O
in O
terms O
of O
all O
the O
automatic O
evaluation O
metrics O
, O
which O
demonstrates O
that O
our O
model O
12345Selection O
thresold O
k1015202530P O
/ O
R@Nprecisionrecall O
12345Selection O
thresold O
k1718192021Concept O
F1K2 O
K2Figure O
4 O
: O
P O
/ O
R@N O
measures O
the O
precision O
/ O
recall O
of O
the O
top O
- O
Nselected O
bridge O
concepts O
. O
Concept O
F1 O
measures O
the O
F1 O
- O
score O
of O
concepts O
in O
the O
generated O
explanations O
. O
can O
generate O
high O
quality O
explanations O
. O
SpeciÔ¨Åcally O
, O
our O
model O
achieves O
a O
2.7 O
% O
gain O
on O
Concept O
F1 O
compared O
with O
GPT-2 O
which O
indicates O
that O
explicitly O
extracting O
bridge O
concepts O
enhances O
the O
informativeness O
of O
the O
generated O
explanation O
. O
To O
evaluate O
the O
effects O
of O
different O
modules O
in O
our O
method O
, O
we O
conduct O
ablation O
studies O
on O
both O
the O
model O
components O
and O
the O
external O
knowledge O
base O
. O
For O
the O
model O
components O
, O
we O
test O
the O
following O
variants O
: O
( O
1 O
) O
without O
the O
context O
- O
aware O
token O
embeddings O
( O
w/o O
Context O
Emb O
. O
) O
; O
( O
2 O
) O
without O
the O
concept O
distance O
embeddings O
( O
w/o O
Distance O
Emb O
. O
) O
; O
( O
3 O
) O
without O
the O
path O
routing O
process O
( O
w/o O
Path O
Routing O
) O
. O
As O
for O
the O
data O
ablation O
, O
we O
sample O
subgraphs O
by O
restricting O
the O
maximum O
number O
of O
hops O
to O
2 O
( O
# O
Hop=2 O
) O
and O
1 O
( O
# O
Hop=1 O
) O
. O
As O
shown O
in O
Table O
2 O
, O
each O
module O
contributes O
to O
the O
Ô¨Ånal O
results O
. O
Particularly O
, O
discarding O
the O
context O
- O
aware O
embeddings O
leads O
to O
the O
most O
remarkable O
performance O
drop O
, O
which O
indicates O
the O
signiÔ¨Åcance O
for O
context O
modeling O
in O
multi O
- O
hop O
reasoning O
. O
Besides O
, O
the O
data O
ablation O
results O
demonstrate O
that O
as O
the O
subgraph O
has O
less O
coverage O
, O
the O
generation O
model O
will O
suffer O
from O
the O
noisy O
concepts O
and O
thus O
deteriorate O
the O
generation O
results O
. O
We O
additionally O
present O
the O
results O
of O
the O
selected O
and O
generated O
concepts O
with O
different O
concepts O
selection O
threshold O
K2 O
. O
As O
shown O
in O
the O
upper O
part O
of O
the O
Figure O
4 O
, O
as O
the O
number O
of O
selected O
concepts O
increases O
, O
more O
true O
positives O
are O
selected O
, O
resulting O
in O
the O
increase O
of O
the O
recall O
( O
Recall@N O
) O
while O
the O
inclusion O
of O
more O
false O
positives O
leads O
to254Error O
Type O
Ratio O
( O
% O
) O
Input O
Output O
Repetition O
7.7 O
She O
begins O
working O
for O
relaxation O
. O
People O
work O
to O
relax O
, O
not O
relax O
. O
Overstatement O
19.2 O
Less O
people O
seek O
knowledge O
. O
People O
do O
n‚Äôt O
seek O
knowledge O
. O
Unrelated O
26.9 O
The O
simplest O
carbohydrates O
are O
amino O
acid O
. O
Alkaloids O
are O
not O
found O
in O
bread O
. O
Chaotic O
11.5 O
Giving O
assistance O
is O
for O
revenge O
. O
If O
you O
help O
someone O
, O
you O
are O
grateful O
. O
Table O
3 O
: O
Distribution O
and O
typical O
cases O
of O
different O
error O
types O
of O
the O
explanations O
generated O
by O
our O
model O
. O
Underlined O
texts O
denote O
the O
error O
types O
including O
repetition O
, O
overstatement O
, O
unrelated O
words O
and O
chaotic O
expression O
. O
ModelFluency O
Reasonability O
Informativeness O
Win O
Lose O
Win O
Lose O
Win O
Lose O
vs. O
Seq2seq O
0.41 O
0.02 O
0.86 O
0.04 O
0.84 O
0.05 O
vs. O
MemNet O
0.48 O
0.00 O
0.84 O
0.03 O
0.87 O
0.03 O
vs. O
Transformer O
0.33 O
0.01 O
0.71 O
0.03 O
0.72 O
0.03 O
vs. O
GPT-2 O
0.20 O
0.10 O
0.40 O
0.27 O
0.34 O
0.15 O
Table O
4 O
: O
Human O
evaluation O
results O
. O
The O
scores O
are O
the O
percentages O
of O
win O
andlose O
of O
our O
model O
in O
pair O
- O
wise O
comparison O
( O
tiecan O
be O
calculated O
by O
1‚àíwin‚àílose O
) O
. O
Our O
model O
is O
signiÔ¨Åcantly O
better O
( O
sign O
test O
, O
p O
- O
value O
< O
0.005 O
) O
than O
all O
the O
baseline O
models O
on O
all O
three O
criteria O
. O
the O
decrease O
of O
the O
precision O
( O
Precision@N O
) O
. O
The O
Concept O
F1 O
reaches O
maximum O
when O
K2= O
3(see O
the O
lower O
part O
) O
, O
which O
demonstrates O
that O
the O
model O
learns O
to O
extract O
critical O
concepts O
for O
explanation O
generation O
while O
keeping O
out O
most O
noisy O
candidates O
with O
an O
appropriate O
selection O
threshold O
. O
4.6 O
Human O
Evaluation O
To O
further O
evaluate O
the O
quality O
of O
the O
generated O
explanations O
, O
we O
conduct O
the O
human O
evaluation O
and O
recruit O
Ô¨Åve O
annotators O
to O
perform O
pair O
- O
wise O
comparisons O
. O
Each O
annotator O
is O
given O
100 O
paired O
explanations O
( O
one O
generated O
by O
our O
model O
and O
the O
other O
by O
a O
baseline O
model O
, O
along O
with O
the O
statement O
) O
and O
is O
required O
to O
give O
a O
preference O
among O
‚Äú O
win O
‚Äù O
, O
‚Äú O
tie O
‚Äù O
, O
and O
‚Äú O
lose O
‚Äù O
according O
to O
three O
criteria O
: O
( O
1 O
) O
Fluency O
which O
measures O
the O
grammatical O
correctness O
and O
the O
readability O
of O
the O
explanation O
. O
( O
2 O
) O
Reasonabilitywhich O
measures O
whether O
the O
explanation O
is O
reasonable O
and O
accords O
with O
the O
commonsense O
. O
( O
3 O
) O
Informativeness O
which O
measures O
the O
amount O
of O
new O
information O
delivered O
in O
the O
explanation O
that O
helps O
explain O
the O
statement O
. O
The O
results O
are O
shown O
in O
Table O
4 O
, O
our O
model O
outperforms O
all O
the O
baseline O
models O
signiÔ¨Åcantly O
on O
all O
three O
criteria O
( O
sign O
test O
, O
p O
- O
value<0.005 O
) O
. O
SpeciÔ¨Åcally O
, O
our O
model O
wins O
GPT-2 O
substantially O
in O
terms O
of O
reasonability O
and O
informativeness O
. O
To O
evaluate O
the O
inter O
- O
rater O
agreement O
for O
each O
criterion O
, O
we O
calculate O
the O
Fleiss O
‚Äô O
kappa O
( O
Fleiss O
, O
1971 O
) O
. O
For O
Reasonability O
/Informativeness O
, O
the O
kappa O
is O
0.429 O
/ O
0.433 O
respectively O
indicating O
aStatement O
1 O
: O
I O
buy O
popcorn O
and O
knife O
when O
I O
go O
to O
the O
cinema O
. O
Seq2Seq O
: O
A O
person O
can O
not O
buy O
a O
person O
to O
watch O
a O
movie O
. O
MemNet O
: O
A O
toothbrush O
is O
not O
a O
place O
to O
play O
a O
movie O
. O
Transformer O
: O
A O
fridge O
is O
not O
a O
place O
to O
store O
groceries O
. O
GPT-2 O
: O
You O
do O
n‚Äôt O
buy O
popcorn O
and O
knife O
at O
the O
cinema O
. O
Ours O
: O
Knives O
are O
not O
sold O
at O
the O
cinema O
. O
Top-3 O
reasoning O
paths O
: O
( O
buy‚Üíantonym O
‚Üísell O
) O
, O
( O
popcorn O
‚Üírelated O
to O
‚Üífood O
) O
, O
( O
cinema O
‚Üírelated O
to O
‚Üí O
movie O
) O
Selected O
concepts O
: O
sell O
, O
place O
, O
movie O
Statement O
2 O
: O
He O
eats O
his O
chips O
with O
toothpaste O
. O
Seq2Seq O
: O
Chopsticks O
are O
not O
edible O
. O
MemNet O
: O
A O
potato O
is O
too O
soft O
to O
eat O
juice O
with O
your O
teeth O
. O
Transformer O
: O
You O
do O
not O
eat O
sand O
with O
a O
cup O
. O
GPT-2 O
: O
Toothpaste O
is O
not O
edible O
. O
Ours O
: O
Toothpaste O
is O
used O
to O
clean O
teeth O
. O
Top-3 O
reasoning O
paths O
: O
( O
eat‚Üírelated O
to O
‚Üítooth O
) O
, O
( O
toothpaste O
‚Üírelated O
to O
‚Üípaste‚Üírelated O
to O
‚Üíuse O
) O
, O
( O
eat O
‚Üíhas O
subevent O
‚Üíwork‚Üírelated O
to O
‚Üíuse O
) O
Selected O
concepts O
: O
use O
, O
tooth O
, O
food O
Table O
5 O
: O
Examples O
of O
generated O
explanations O
. O
Irrelevant O
contents O
are O
in O
red O
and O
critical O
concepts O
for O
explanation O
are O
in O
green O
. O
moderate O
agreement O
among O
annotators O
. O
In O
terms O
of O
Fluency O
, O
annotators O
show O
diverse O
preferences O
( O
Œ∫= O
0.245 O
) O
since O
GPT-2 O
has O
strong O
ability O
in O
generating O
Ô¨Çuent O
texts O
. O
4.7 O
Case O
Study O
Table O
5 O
presents O
the O
generated O
explanations O
. O
Our O
model O
is O
capable O
to O
generate O
reasonable O
and O
informative O
explanations O
by O
utilizing O
the O
extracted O
bridge O
concepts O
. O
SpeciÔ¨Åcally O
, O
in O
the O
Ô¨Årst O
case O
our O
model O
extracts O
bridge O
concepts O
‚Äú O
sell O
‚Äù O
and O
identiÔ¨Åes O
the O
incompatibility O
between O
‚Äú O
knives O
‚Äù O
and O
‚Äú O
cinema O
‚Äù O
. O
In O
the O
second O
case O
, O
our O
model O
clariÔ¨Åes O
the O
function O
of O
the O
‚Äú O
toothpaste O
‚Äù O
by O
extracting O
‚Äú O
use O
‚Äù O
from O
two O
reasoning O
paths O
and O
provides O
more O
information O
rather O
than O
simply O
negative O
phrasing.2554.8 O
Error O
Analysis O
To O
analyze O
the O
error O
types O
of O
the O
explanations O
generated O
by O
our O
model O
, O
we O
manually O
check O
all O
the O
failed O
cases3 O
in O
the O
pair O
- O
wise O
comparison O
between O
our O
model O
and O
the O
strong O
baseline O
GPT-2 O
. O
The O
number O
of O
these O
cases O
is O
26 O
in O
all O
100 O
explanations O
. O
We O
manually O
annotated O
four O
types O
of O
errors O
from O
the O
failed O
explanations O
: O
repetition O
( O
words O
repeating O
) O
, O
overstatement O
( O
overstate O
the O
points O
) O
, O
unrelated O
concepts O
towards O
the O
statement O
( O
the O
explanation O
itself O
may O
be O
reasonable O
) O
, O
chaotic O
sentences O
( O
difÔ¨Åcult O
to O
understand O
) O
. O
As O
shown O
in O
Table O
3 O
, O
it O
is O
still O
challenging O
for O
the O
model O
to O
generate O
explanations O
highly O
related O
to O
the O
statement O
with O
accurate O
wording O
. O
5 O
Conclusion O
In O
this O
paper O
, O
we O
analyze O
the O
challenges O
in O
incorporating O
external O
knowledge O
graph O
to O
aid O
the O
commonsense O
generation O
problem O
and O
propose O
a O
twostage O
method O
that O
Ô¨Årst O
extracts O
bridge O
concepts O
from O
a O
retrieved O
subgraph O
and O
then O
generates O
the O
explanation O
by O
integrating O
the O
extracted O
concepts O
. O
Experimental O
results O
show O
that O
our O
model O
outperforms O
baselines O
including O
the O
strong O
pre O
- O
trained O
language O
model O
GPT-2 O
in O
both O
automatic O
and O
manual O
evaluation O
. O
Acknowledgments O
This O
work O
was O
jointly O
supported O
by O
the O
NSFC O
projects O
( O
key O
project O
with O
No O
. O
61936010 O
and O
regular O
project O
with O
No O
. O
61876096 O
) O
, O
and O
the O
Guoqiang O
Institute O
of O
Tsinghua O
University O
with O
Grant O
No O
. O
2019GQG1 O
. O
We O
thank O
THUNUS O
NExT O
Joint O
- O
Lab O
for O
the O
support O
. O
Abstract O
The O
KB O
- O
to O
- O
text O
task O
aims O
at O
generating O
texts O
based O
on O
the O
given O
KB O
triples O
. O
Traditional O
methods O
usually O
map O
KB O
triples O
to O
sentences O
via O
a O
supervised O
seq O
- O
to O
- O
seq O
model O
. O
However O
, O
existing O
annotated O
datasets O
are O
very O
limited O
and O
human O
labeling O
is O
very O
expensive O
. O
In O
this O
paper O
, O
we O
propose O
a O
method O
which O
trains O
the O
generation O
model O
in O
a O
completely O
unsupervised O
way O
with O
unaligned O
raw O
text O
data O
and O
KB O
triples O
. O
Our O
method O
exploits O
a O
novel O
dual O
training O
framework O
which O
leverages O
the O
inverse O
relationship O
between O
the O
KB O
- O
to O
- O
text O
generation O
task O
and O
an O
auxiliary O
triple O
extraction O
task O
. O
In O
our O
architecture O
, O
we O
reconstruct O
KB O
triples O
or O
texts O
via O
a O
closed O
- O
loop O
framework O
via O
linking O
a O
generator O
and O
an O
extractor O
. O
Therefore O
the O
loss O
function O
that O
accounts O
for O
the O
reconstruction O
error O
of O
KB O
triples O
and O
texts O
can O
be O
used O
to O
train O
the O
generator O
and O
extractor O
. O
To O
resolve O
the O
cold O
start O
problem O
in O
training O
, O
we O
propose O
a O
method O
using O
a O
pseudo O
data O
generator O
which O
generates O
pseudo O
texts O
and O
KB O
triples O
for O
learning O
an O
initial O
model O
. O
To O
resolve O
the O
multiple O
- O
triple O
problem O
, O
we O
design O
an O
allocated O
reinforcement O
learning O
component O
to O
optimize O
the O
reconstruction O
loss O
. O
The O
experimental O
results O
demonstrate O
that O
our O
model O
can O
outperform O
other O
unsupervised O
generation O
methods O
and O
close O
to O
the O
bound O
of O
supervised O
methods O
. O
1 O
Introduction O
Knowledge O
Base O
( O
KB)-to O
- O
text O
task O
focuses O
on O
generating O
plain O
text O
descriptions O
from O
given O
knowledge O
bases O
( O
KB O
) O
triples O
which O
makes O
them O
accessible O
to O
users O
. O
For O
instance O
, O
given O
a O
KB O
triple O
< O
101 O
Helena O
, O
discoverer O
, O
James O
Craig O
Watson O
> O
, O
it O
is O
expected O
to O
generate O
a O
description O
sentence O
such O
as O
‚àóThe O
work O
described O
in O
this O
paper O
is O
substantially O
supported O
by O
a O
grant O
from O
the O
Research O
Grant O
Council O
of O
the O
Hong O
Kong O
Special O
Administrative O
Region O
, O
China O
( O
Project O
Codes O
: O
14204418).‚Äú101 O
Helena O
is O
discovered O
by O
James O
Craig O
Watson O
. O
‚Äù O
. O
Recently O
, O
many O
research O
works O
have O
been O
proposed O
for O
this O
task O
. O
For O
example O
, O
Gardent O
et O
al O
. O
( O
2017a O
, O
b O
) O
create O
the O
WebNLG O
dataset O
to O
generate O
description O
for O
triples O
sampled O
from O
DBPedia O
( O
Auer O
et O
al O
. O
, O
2007 O
) O
. O
Lebret O
et O
al O
. O
‚Äôs O
( O
2016 O
) O
method O
generates O
people O
‚Äôs O
biographies O
from O
extracted O
Wikipedia O
infobox O
. O
Novikova O
et O
al O
. O
( O
2017 O
) O
propose O
to O
generate O
restaurant O
reviews O
by O
some O
given O
attributes O
and O
Fu O
et O
al O
. O
( O
2020a O
) O
create O
the O
WikiEvent O
dataset O
to O
generate O
text O
based O
on O
an O
event O
chain O
. O
However O
, O
the O
works O
mentioned O
above O
usually O
map O
structured O
triples O
to O
text O
via O
a O
supervised O
seq O
- O
to O
- O
seq O
( O
Sutskever O
et O
al O
. O
, O
2014 O
) O
model O
, O
in O
which O
large O
amounts O
of O
annotated O
data O
is O
necessary O
and O
the O
annotation O
is O
very O
expensive O
and O
time O
- O
consuming O
. O
We O
aim O
to O
tackle O
the O
problem O
of O
completely O
unsupervised O
KB O
- O
to O
- O
text O
generation O
which O
only O
requires O
a O
text O
corpus O
and O
a O
KB O
corpus O
and O
does O
not O
assume O
any O
alignment O
between O
them O
. O
We O
propose O
a O
dual O
learning O
framework O
based O
on O
the O
inverse O
relationship O
between O
the O
KB O
- O
to O
- O
text O
generation O
task O
and O
the O
triple O
extraction O
task O
. O
SpeciÔ¨Åcally O
, O
the O
KBto O
- O
text O
task O
generates O
sentences O
from O
structured O
triples O
while O
the O
task O
of O
triple O
extraction O
extracts O
multiple O
triples O
from O
plain O
texts O
. O
Such O
a O
relationship O
enables O
the O
design O
of O
a O
closed O
- O
loop O
learning O
framework O
in O
which O
we O
link O
KB O
- O
to O
- O
text O
generation O
and O
its O
dual O
task O
of O
triple O
extraction O
so O
as O
to O
reconstruct O
the O
unaligned O
KB O
triples O
and O
texts O
. O
The O
non O
- O
differentiability O
issue O
of O
picking O
words O
from O
our O
neural O
model O
before O
reconstruction O
makes O
it O
hard O
to O
train O
the O
extractor O
or O
generator O
effectively O
using O
backpropagation O
. O
To O
solve O
this O
issue O
, O
we O
apply O
Reinforcement O
Learning O
( O
RL O
) O
based O
on O
policy O
gradients O
into O
our O
dual O
learning O
framework O
to O
optimize O
our O
extractor O
or O
generator O
according O
to O
the O
rewards O
. O
Some O
semi O
- O
supervised O
works O
( O
He O
et O
al O
. O
, O
2016 O
; O
Cao O
et O
al O
. O
, O
2019 O
) O
have O
been O
proposed O
to O
generate258101 O
Helena O
is O
discovered O
by O
James O
Craig O
Watson O
who O
was O
born O
in O
Canada O
. O
< O
101 O
Helena O
, O
discoverer O
, O
James O
Craig O
Watson>“Ö O
< O
James O
Craig O
Watson O
, O
nationality O
, O
Canada O
> O
, O
< O
James O
Craig O
Watson O
, O
profession O
, O
Writer O
> O
, O
< O
James O
Craig O
Watson O
, O
deathPlace O
, O
Australia O
> O
101 O
Helena O
is O
discovered O
by O
James O
Craig O
Watson O
who O
was O
born O
in O
Canada O
and O
died O
in O
Australia O
. O
EG101 O
Helena O
is O
discovered O
by O
James O
Craig O
Watson O
. O
  O
James O
Craig O
Watson O
was O
born O
in O
Canada O
. O
James O
Craig O
Watson O
is O
a O
writer O
. O
James O
Craig O
Watson O
died O
in O
Australia O
. O
1.0 O
1.0 O
0.5 O
0.50.9GTraditional O
RLOur O
Proposed O
ARLFigure O
1 O
: O
Illustration O
of O
the O
multiple O
- O
triple O
problem O
, O
in O
whichEandGare O
extractor O
and O
generator O
respectively O
. O
The O
left O
part O
is O
the O
traditional O
RL O
methods O
and O
the O
right O
is O
our O
proposed O
ARL O
method O
. O
Four O
triples O
are O
extracted O
by O
the O
extractor O
. O
The O
top O
two O
triples O
are O
right O
and O
the O
others O
are O
wrong O
. O
Traditional O
RL O
methods O
give O
a O
single O
reward O
( O
0.9 O
) O
for O
all O
the O
four O
triples O
while O
our O
proposed O
ARL O
gives O
each O
triple O
a O
different O
reward O
. O
Then O
the O
right O
triples O
and O
the O
wrong O
triples O
will O
be O
distinguished O
and O
optimized O
differently O
. O
plain O
texts O
from O
data O
of O
certain O
forms O
in O
other O
domains O
( O
e.g. O
, O
translation O
, O
semantic O
parsing O
) O
with O
limited O
annotated O
resources O
. O
These O
models O
contain O
two O
major O
steps O
. O
Firstly O
, O
they O
pre O
- O
train O
a O
weak O
model O
based O
on O
the O
labeled O
data O
. O
Secondly O
, O
they O
use O
an O
iterative O
model O
whose O
aim O
is O
to O
improve O
the O
weak O
model O
using O
the O
unlabeled O
data O
. O
In O
each O
iteration O
, O
the O
input O
sequence O
of O
the O
original O
data O
form O
is O
transformed O
into O
another O
form O
by O
the O
original O
model O
. O
Then O
, O
it O
is O
transformed O
back O
to O
the O
original O
data O
form O
by O
an O
inverse O
model O
. O
However O
, O
there O
are O
still O
some O
challenges O
applying O
the O
existing O
methods O
into O
KB O
- O
to O
- O
text O
directly O
: O
( O
1 O
) O
Cold O
start O
problem O
. O
Existing O
approaches O
pre O
- O
train O
the O
model O
with O
labeled O
data O
and O
then O
Ô¨Åne O
- O
tune O
their O
models O
via O
unlabelled O
data O
. O
Such O
a O
mechanism O
still O
needs O
annotated O
data O
which O
is O
more O
difÔ¨Åcult O
and O
expensive O
to O
obtain O
in O
KB O
- O
to O
- O
text O
task O
. O
( O
2 O
) O
Multiple O
- O
triple O
problem O
. O
As O
shown O
in O
Fig O
. O
1 O
, O
multiple O
triples O
might O
be O
extracted O
from O
a O
text O
example O
, O
and O
inevitably O
, O
the O
neural O
extractor O
could O
extract O
some O
wrong O
triples O
. O
The O
traditional O
dual O
learning O
approaches O
( O
He O
et O
al O
. O
, O
2016 O
; O
Cao O
et O
al O
. O
, O
2019 O
) O
, O
if O
directly O
applied O
, O
will O
regard O
all O
these O
triples O
as O
one O
unit O
and O
calculate O
a O
single O
reward O
for O
all O
the O
triples O
regardless O
of O
whether O
they O
are O
correct O
or O
not O
. O
It O
not O
only O
results O
in O
the O
slow O
convergence O
of O
RL O
, O
but O
also O
leads O
to O
unsatisfactory O
model O
performance O
. O
We O
propose O
a O
novel O
Extractor- O
Generator O
Dual O
( O
EGD O
) O
framework O
which O
exploits O
the O
inverse O
relationship O
between O
KB O
- O
to O
- O
text O
generation O
and O
auxiliary O
triple O
extraction O
. O
Our O
model O
can O
resolve O
the O
KB O
- O
to O
- O
text O
task O
in O
a O
totally O
unsupervised O
way O
. O
To O
cope O
with O
the O
cold O
start O
problem O
, O
we O
propose O
apseudo O
data O
generator O
( O
PDG O
) O
which O
can O
generate O
pseudo O
text O
and O
pseudo O
KB O
triples O
based O
on O
the O
given O
unaligned O
KB O
triples O
and O
text O
respectively O
with O
prior O
knowledge O
. O
The O
extractor O
and O
the O
generator O
are O
then O
pre O
- O
trained O
with O
the O
generated O
pseudo O
data O
. O
To O
resolve O
the O
multiple O
- O
triple O
problem O
, O
we O
propose O
a O
novel O
Allocated O
Reinforcement O
Learning O
( O
ARL O
) O
component O
. O
Different O
from O
traditional O
RL O
methods O
in O
which O
one O
reward O
is O
calculated O
for O
the O
whole O
sequence O
, O
ARL O
allocates O
different O
rewards O
to O
different O
sub O
- O
parts O
of O
the O
sequence O
( O
Fig O
. O
1 O
right O
) O
. O
Therefore O
, O
our O
model O
can O
distinguish O
the O
quality O
of O
each O
triple O
and O
optimize O
the O
extractor O
and O
the O
generator O
more O
accurately O
. O
We O
compare O
our O
framework O
with O
existing O
dual O
learning O
methods O
and O
the O
experimental O
results O
demonstrate O
that O
our O
model O
can O
outperform O
other O
unsupervised O
generation O
methods O
and O
close O
to O
the O
bound O
of O
supervised O
methods O
. O
2 O
Related O
Works O
Recently O
many O
tasks O
and O
methods O
have O
been O
proposed O
to O
transform O
existing O
data O
into O
humanreadable O
text O
. O
WebNLG O
( O
Gardent O
et O
al O
. O
, O
2017a O
, O
b O
) O
is O
proposed O
to O
describe O
a O
list O
of O
triples O
sampled O
from O
DBPedia O
( O
Auer O
et O
al O
. O
, O
2007 O
) O
. O
Except O
for O
the O
KB O
triples O
, O
many O
other O
types O
of O
data O
have O
also O
been O
investigated O
for O
how O
to O
generate O
text O
from O
them O
. O
For O
example O
, O
E2E O
( O
Novikova O
et O
al O
. O
, O
2017 O
) O
aims O
at O
generating O
text O
from O
some O
restaurants O
‚Äô O
attributes O
. O
Wikibio O
( O
Lebret O
et O
al O
. O
, O
2016 O
) O
proposes O
to O
generate O
biographies O
for O
the O
Wikipedia O
infobox O
while O
WikiEvent O
( O
Fu O
et O
al O
. O
, O
2020a O
) O
proposes O
to O
generate O
text O
based O
on O
an O
event O
chain O
. O
Besides O
, O
Chen O
and O
Mooney O
( O
2008 O
) O
; O
Wiseman O
et O
al O
. O
( O
2017 O
) O
propose O
to O
generate O
a O
summarization O
of O
a O
match O
based O
on O
the O
scores O
and O
Liang O
et O
al O
. O
( O
2009 O
) O
propose O
to O
generate O
weather O
reports O
based O
on O
the O
records O
. O
All O
these O
tasks O
require O
an O
elaborately O
annotated O
dataset O
which O
is O
very O
expensive O
to O
prepare O
. O
Many O
methods O
have O
been O
proposed O
to O
tackle O
the O
dataset O
insufÔ¨Åciency O
problem O
in O
other O
tasks O
. O
Fu O
et O
al O
. O
( O
2020c O
) O
propose O
to O
directly O
train O
the O
model O
on O
partially O
- O
aligned O
data O
in O
which O
the O
data O
and O
the O
text O
are O
not O
necessarily O
exactly O
math O
, O
and O
it O
can O
be O
built O
automatically O
. O
He O
et O
al O
. O
( O
2016 O
) O
; O
Sennrich O
et O
al O
. O
( O
2016 O
) O
; O
Yi O
et O
al O
. O
( O
2017 O
) O
propose O
dual O
learning O
frameworks O
. O
They O
pre O
- O
train O
a O
weak O
model O
with O
parallel O
data O
and O
reÔ¨Åne O
the O
model O
with O
monolingual O
data O
. O
This O
strategy O
has O
been O
applied O
in O
many O
related O
tasks O
including O
semantic O
parsing O
( O
Cao O
et O
al O
. O
, O
2019 O
) O
, O
summarization O
( O
Baziotis O
et O
al O
. O
, O
2019 O
) O
and259KB1KB2KBn O
‚Ä¶ O
TextTextKBGEŒ∏PDGGœÜEEŒ∏EGœÜPDGEŒ∏GGœÜpre O
- O
train O
kb2kbARLGtxt2txtARLETextText1Text2Textn O
‚Ä¶ O
KBKB1KB2KBn O
‚Ä¶ O
KBsplitKBKB1KB2KBn O
‚Ä¶ O
TextText1Text2Textn O
‚Ä¶ O
KBsplitLE O
< O
latexit O
sha1_base64="Y8Oe2xpmNErGDbs8P67Jf4pRbrs=">AAAB+XicbVDLSsNAFJ3UV62vVFfSzWARXJWkRZuCi4IILly0YGuhCWEynbZDJw9mJoUS8iduXCji1v9w4U6 O
/ O
xknbhVYPDBzOuZd75ngRo0IaxqeWW1vf2NzKbxd2dvf2D O
/ O
TiYVeEMcekg0MW8p6HBGE0IB1JJSO9iBPke4zce5OrzL+fEi5oGNzJWUQcH40COqQYSSW5um77SI4xYslt6ibXacHVy0alYRm1RhUaFcu6qNbPFTHmgOaSlJtFu O
/ O
TVPn5vufqHPQhx7JNAYoaE6JtGJJ0EcUkxI2nBjgWJEJ6gEekrGiCfCCeZJ0 O
/ O
hqVIGcBhy9QIJ5+rPjQT5Qsx8T01mOcWql4n O
/ O
ef1YDi0noUEUSxLgxaFhzKAMYVYDHFBOsGQzRRDmVGWFeIw4wlKVlZVgrn75L+lWK2atUm2b5eYlWCAPSuAEnAET1EET3IAW6AAMpuABPIFnLdEetRftdTGa05Y7R+AXtLdvgOyWgw==</latexit O
> O
LARLG O
< O
latexit O
sha1_base64="kU43c7Im1Fe8lkxpEhyB0d0rxeA=">AAAB O
/ O
HicdVBNSwJBGJ61L7OvNU O
/ O
hZUiCTsuuZiZ0MDrUwYNGmqCyzI6zOjj7wcxsIIv9lS4diujaz+jQrX5Ns1pQUQ8M78Pzvi O
/ O
vM48TMiqkab5pqYXFpeWV9GpmbX1jc0vPbrdFEHFMWjhgAe84SBBGfdKSVDLSCTlBnsPIlTM+TfpX14QLGviXchKSvoeGPnUpRlJJtp7reUiOMGJxfWrHJxf1s2nG1gumUS1ViuUyNI2kHlQVOVTFLELLMGco1LK9 O
/ O
Htz56Vh66 O
+ O
9QYAjj O
/ O
gSMyRE1zJD2Y8RlxQzMs30IkFChMdoSLqK+sgjoh O
/ O
PzE O
/ O
hnlIG0A24er6EM O
/ O
X7Row8ISaeoyYTq+J3LxH/6nUj6R71Y+qHkSQ+nh9yIwZlAJMk4IBygiWbKIIwp8orxCPEEZYqrySEr5 O
/ O
C/0m7aFglo9i0CrVjMEca5MEu2AcWqIAaOAcN0AIYTMAtuAcP2o12pz1qT O
/ O
PRlPa5kwM O
/ O
oD1 O
/ O
AHTjl5Y=</latexit O
> O
LG O
< O
latexit O
sha1_base64="y O
/ O
LVboRZvPiT5759n9zigPQZrw8=">AAAB+XicdVDLSsNAFJ34rPWV6kq6CRbBVUjqoy5cFFzowkUL9gFNCJPppB06mYSZSaGE O
/ O
IkbF4q49T9cuNOvcdJWsKIHLhzOuZd7OH5MiZCW9aEtLa+srq0XNoqbW9s7u3ppry2ihCPcQhGNeNeHAlPCcEsSSXE35hiGPsUdf3SV+50x5oJE7E5OYuyGcMBIQBCUSvJ03QmhHCJI09vMS6+zoqdXbNOawrDMs9r5ac1WZK58W5V6ySl O
/ O
Ng O
/ O
eGp7 O
+ O
7vQjlISYSUShED3biqWbQi4JojgrOonAMUQjOMA9RRkMsXDTafLMOFJK3wgiroZJY6r+vEhhKMQk9NVmnlP89nLxL6+XyODCTQmLE4kZmj0KEmrIyMhrMPqEYyTpRBGIOFFZDTSEHCKpyloo4X O
/ O
Srpr2iVlt2pX6JZihAMrgEBwDG9RAHdyABmgBBMbgHjyCJy3VHrRn7WW2uqTNb O
/ O
bBArTXL07GlmE=</latexit O
> O
LARLE O
< O
latexit O
sha1_base64="pL0RyuW8v2vDVjBk9FKEysQokYo=">AAAB O
/ O
HicdVDLSgMxFM3UV62vqV1JN8EiuBoyLfQBLioiuOiiFfuAtpRMmrahmQdJRihD O
/ O
RU3LhRx62e4cKdfY6ZVUNEDgcM593JPjhNwJhVCb0ZiZXVtfSO5mdra3tndM9P7LemHgtAm8bkvOg6WlDOPNhVTnHYCQbHrcNp2pmex376mQjLfu1KzgPZdPPbYiBGstDQwMz0XqwnBPKrNB9HpZe18nhqYOWQVUQyILFQpo0JFk0KxXMyXoG0tHJSrpnvZ98bBS31gvvaGPgld6inCsZRdGwWqH2GhGOF0nuqFkgaYTPGYdjX1sEtlP1qEn8MjrQzhyBf6eQou1O8bEXalnLmOnoyjyt9eLP7ldUM1Kvcj5gWhoh5ZHhqFHCofxk3AIROUKD7TBBPBdFZIJlhgonRfcQlfP4X O
/ O
k1besgtWvmHnqidgiSTIgkNwDGxQAlVwAeqgCQiYgVtwDx6MG+POeDSelqMJ43MnA37AeP4AU8SXgA==</latexit O
> O
Data O
from O
corpusE O
/ O
GExtractor O
/ O
generator O
modelEŒ∏ O
/ O
GœÜModel O
with O
updatable O
parametersCalculate O
with O
gradient O
  O
back O
- O
propagationCalculate O
without O
gradientFigure O
2 O
: O
The O
extractor O
- O
generator O
dual O
( O
EGD O
) O
framework O
. O
It O
contains O
three O
processes O
namely O
a O
pre O
- O
train O
process O
, O
a O
kb2 O
kb O
process O
and O
a O
txt2txt O
process O
. O
information O
narration O
( O
Sun O
et O
al O
. O
, O
2018 O
) O
. O
However O
, O
as O
indicated O
in O
Hoang O
et O
al O
. O
( O
2018 O
) O
, O
the O
dual O
learning O
approach O
is O
not O
easy O
to O
train O
. O
Moreover O
, O
these O
methods O
still O
need O
some O
aligned O
data O
to O
pre O
- O
train O
the O
weak O
model O
. O
Another O
line O
of O
research O
proposes O
to O
use O
some O
extra O
annotations O
instead O
of O
using O
aligned O
data O
. O
Lample O
et O
al O
. O
( O
2018a O
, O
b O
) O
propose O
to O
train O
an O
unsupervised O
NMT O
system O
based O
on O
few O
annotated O
word O
pairs O
( O
Conneau O
et O
al O
. O
, O
2018 O
) O
. O
Luo O
et O
al O
. O
( O
2019 O
) O
propose O
to O
generate O
pseudo O
data O
with O
a O
rule O
- O
based O
template O
( O
Li O
et O
al O
. O
, O
2018 O
) O
. O
However O
, O
these O
models O
can O
not O
be O
directly O
applied O
in O
our O
scenario O
since O
our O
dataset O
is O
too O
complicated O
to O
make O
these O
annotations O
. O
Fu O
et O
al O
. O
( O
2020b O
) O
propose O
to O
utilize O
topic O
information O
from O
a O
dynamic O
topic O
tracker O
to O
solve O
the O
dataset O
insufÔ¨Åciency O
problem O
. O
Cheng O
et O
al O
. O
( O
2020 O
) O
propose O
to O
generate O
better O
text O
description O
for O
a O
few O
entities O
by O
exploring O
the O
knowledge O
from O
KB O
and O
distill O
the O
useful O
part O
. O
In O
the O
Ô¨Åeld O
of O
computer O
vision O
, O
Zhu O
et O
al O
. O
( O
2017 O
) O
propose O
cycleGAN O
which O
uses O
a O
cycled O
training O
method O
that O
transforms O
the O
input O
into O
another O
data O
form O
and O
then O
transforms O
it O
back O
, O
minimizing O
the O
recover O
loss O
. O
The O
method O
works O
well O
in O
the O
image O
domain O
but O
has O
some O
problems O
in O
text O
generation O
considering O
the O
non O
- O
differentiable O
discrete O
layer O
. O
We O
follow O
the O
ideas O
of O
cycleGAN O
to O
train O
the O
whole O
model O
without O
supervised O
data O
and O
adopt O
the O
RL O
method O
proposed O
in O
dual O
learning O
methods O
. O
Reinforcement O
Learning O
( O
RL O
) O
has O
been O
utilized O
to O
solve O
the O
infeasibility O
of O
backpropagation O
through O
discrete O
tokens O
layer O
. O
Li O
et O
al O
. O
( O
2016 O
) O
propose O
to O
use O
RL O
to O
focus O
on O
the O
long O
term O
target O
and O
thus O
improve O
the O
performance O
. O
Yu O
et O
al O
. O
( O
2017 O
) O
propose O
to O
use O
the O
RL O
in O
generative O
adversarial O
networks O
to O
solve O
the O
discrete O
tokens O
problem O
. O
He O
et O
al O
. O
( O
2016 O
) O
; O
Sun O
et O
al O
. O
( O
2018 O
) O
propose O
to O
use O
RL O
in O
dual O
training O
. O
As O
far O
as O
we O
know O
, O
no O
studies O
of O
RL O
have O
been O
conducted O
for O
KB O
triples O
in O
whichthe O
reward O
is O
different O
for O
each O
triple O
considering O
multiple O
- O
triple O
problem O
. O
3 O
Method O
3.1 O
Problem O
DeÔ¨Ånition O
Formally O
, O
we O
denote O
the O
KB O
corpus O
as O
K= O
{ O
Ki|‚àÄi}in O
whichKi= O
[ O
k(i O
) O
1,k(i O
) O
2,¬∑¬∑¬∑,k(i O
) O
ni]is O
the O
ith O
KB O
triple O
list O
containing O
nitriples.k(i O
) O
j= O
( O
h(i O
) O
j O
, O
r(i O
) O
j O
, O
t(i O
) O
j)represents O
the O
jth O
KB O
triple O
in O
Ki O
containing O
the O
head O
, O
relation O
and O
tail O
entity O
respectively O
. O
We O
denote O
the O
texts O
corpus O
as O
T={Ti|‚àÄi O
} O
in O
whichTi= O
[ O
t(i O
) O
1,t(i O
) O
2,¬∑¬∑¬∑,t(i O
) O
ni]is O
theith O
sentence O
andt(i O
) O
jis O
thejth O
word O
in O
the O
sentence O
. O
In O
our O
problem O
, O
we O
are O
only O
given O
a O
collection O
of O
KB O
triplesKt‚äÇK O
and O
a O
collection O
of O
text O
Tt‚äÇT O
without O
any O
alignment O
information O
between O
them O
. O
The O
ultimate O
goal O
is O
to O
train O
a O
model O
that O
generates O
the O
corresponding O
text O
in O
Tdescribing O
the O
given O
triple O
list O
fromK. O
3.2 O
Extractor O
- O
Generator O
Dual O
Framework O
Our O
proposed O
Extractor O
- O
Generator O
Dual O
( O
EGD O
) O
framework O
is O
composed O
of O
a O
generator O
Gand O
an O
extractorEthat O
translate O
data O
in O
one O
form O
to O
another O
. O
We O
denote O
all O
trainable O
parameters O
in O
E O
andGasŒ∏andœÜ O
, O
respectively O
. O
The O
generator O
generates O
text O
representation O
for O
each O
KB O
triple O
asT O
/ O
prime O
= O
G(K),K‚àà O
K O
, O
T O
/ O
prime‚àà O
T O
while O
the O
extractor O
extracts O
KB O
triples O
from O
raw O
text O
as O
K O
/ O
prime= O
E(T),T‚ààT O
, O
K O
/ O
prime‚ààK. O
Our O
EGD O
framework O
is O
trained O
in O
an O
unsupervised O
manner O
and O
it O
contains O
three O
processes O
, O
as O
shown O
in O
Fig O
. O
2 O
. O
The O
Ô¨Årst O
process O
is O
a O
pre O
- O
train O
process O
in O
which O
both O
EandG O
are O
trained O
with O
the O
pseudo O
data O
generated O
by O
the O
pseudo O
generator O
. O
The O
second O
process O
is O
the O
kb2 O
kb O
process O
which O
generates O
description O
text O
based O
on O
the O
given O
KB O
triples O
with O
Gand O
then O
recovers O
the O
KB O
triples O
from O
the O
generated O
text O
with O
E. O
The260third O
process O
is O
called O
txt2txt O
which O
extracts O
KB O
triples O
from O
the O
given O
text O
with O
Eand O
then O
recovers O
the O
text O
from O
the O
generated O
KB O
triples O
with O
G. O
In O
order O
to O
overcome O
the O
multiple O
- O
mapping O
problem O
, O
we O
propose O
a O
novel O
allocated O
reinforcement O
learning O
component O
in O
kb2 O
kb O
and O
txt2txt O
, O
respectively O
. O
The O
EGD O
framework O
Ô¨Årstly O
pre O
- O
trains O
the O
extractor O
and O
generator O
with O
the O
data O
generated O
by O
the O
pseudo O
data O
generator O
( O
PDG O
) O
. O
For O
the O
text O
corpusTt O
, O
we O
generate O
corresponding O
pseudo O
KB O
triples O
asK O
/ O
prime O
t={K O
= O
PK(T)|‚àÄT‚àà O
Tt O
} O
, O
in O
whichPKis O
the O
pseudo O
KB O
generator O
. O
We O
pretrain O
the O
generator O
Gto O
transform O
K‚àà O
K O
/ O
prime O
tto O
T‚àà O
Tt O
. O
Similarly O
, O
we O
generate O
pseudo O
text O
as O
T O
/ O
prime O
t={T O
= O
PT(K)|‚àÄK‚ààKt O
} O
, O
in O
whichPTis O
the O
pseudo O
text O
generator O
. O
Then O
, O
we O
train O
the O
extractor O
to O
transform O
T‚ààT O
/ O
prime O
ttoK‚ààKt O
. O
AfterG O
andEhave O
been O
pre O
- O
trained O
, O
the O
kb2 O
kb O
process O
and O
the O
txt2txt O
process O
are O
conducted O
alternately O
to O
further O
improve O
the O
performance O
. O
In O
the O
kb2 O
kb O
process O
, O
the O
input O
KB O
triples O
are O
Ô¨Årstly O
Ô¨Çattened O
and O
concatenated O
one O
by O
one O
as O
K= O
[ O
k1,k2,¬∑¬∑¬∑,knk O
] O
= O
[ O
w1,w2,¬∑¬∑¬∑,wnw]in O
whichkiis O
theith O
triple O
inKwhilewidenotes O
theith O
words O
in O
the O
concatenated O
word O
list O
. O
nk O
is O
the O
number O
of O
triples O
while O
nwis O
the O
number O
of O
the O
words O
. O
Kis O
then O
sent O
into O
the O
generator O
Gto O
get O
a O
text O
description O
Tm= O
[ O
t1,t2,¬∑¬∑¬∑,tnt O
] O
, O
wheretiis O
theith O
word O
in O
the O
sentence O
Tmand O
ntis O
the O
length O
of O
Tm O
. O
Afterwards O
, O
The O
extractor O
takes O
the O
sentences O
Tmas O
input O
and O
outputs O
the O
triple O
sequence O
K O
/ O
prime= O
[ O
w O
/ O
prime O
1,w O
/ O
prime O
2,¬∑¬∑¬∑,w O
/ O
prime O
n O
/ O
primew O
] O
, O
in O
which O
w O
/ O
prime O
iis O
theith O
word O
inK O
/ O
primewhilen O
/ O
prime O
wis O
the O
length O
ofK O
/ O
prime O
. O
The O
target O
is O
to O
make O
K O
/ O
primeas O
close O
toKas O
possible O
. O
Therefore O
, O
in O
the O
training O
step O
, O
the O
loss O
function O
for O
the O
extractor O
is O
deÔ¨Åned O
as O
the O
negative O
log O
probability O
of O
each O
word O
in O
K O
: O
LE=‚àínw O
/ O
summationdisplay O
i=1logpŒ∏(w O
/ O
prime O
i O
= O
wi|Tm O
, O
w1,¬∑¬∑¬∑,wi‚àí1 O
) O
. O
We O
can O
also O
use O
the O
output O
to O
improve O
the O
generator O
. O
SinceTmis O
discrete O
, O
the O
gradient O
can O
not O
be O
passed O
to O
the O
generator O
as O
the O
cycleGAN O
( O
Zhu O
et O
al O
. O
, O
2017 O
) O
does O
. O
To O
tackle O
this O
problem O
, O
we O
propose O
an O
Allocated O
Reinforcement O
Learning O
for O
Generator O
( O
ARLG O
) O
component O
to O
utilize O
the O
extractor O
‚Äôs O
result O
to O
optimize O
the O
generator O
. O
Different O
rewards O
are O
allocated O
to O
different O
parts O
of O
the O
generator O
output O
. O
The O
gradient O
for O
the O
generator O
is O
denoted O
as O
‚àáœÜLARLG O
which O
will O
be O
introduced O
in O
the O
later O
section O
. O
In O
the O
txt2txt O
process O
, O
the O
input O
text O
T= O
[ O
t1,t2,¬∑¬∑¬∑,tnt]is O
transformed O
into O
its O
KB O
representationKm= O
[ O
k1,k2,¬∑¬∑¬∑,knm]by O
the O
extractor O
E. O
Kmis O
then O
transformed O
to O
T O
/ O
prime= O
[ O
t O
/ O
prime O
1,t O
/ O
prime O
2,¬∑¬∑¬∑,t O
/ O
prime O
nt O
] O
by O
the O
generator O
and O
the O
loss O
is O
deÔ¨Åned O
as O
: O
LG=‚àínt O
/ O
summationdisplay O
i=1logpœÜ(t O
/ O
prime O
i O
= O
ti|Km O
, O
t1,¬∑¬∑¬∑,ti‚àí1 O
) O
. O
Similarly O
, O
we O
also O
propose O
an O
Allocated O
Reinforcement O
Learning O
for O
Extractor O
( O
ARLE O
) O
to O
utilize O
the O
generator O
‚Äôs O
result O
to O
optimize O
the O
extractor O
. O
Different O
rewards O
are O
allocated O
to O
different O
parts O
of O
the O
extractor O
output O
. O
Let O
the O
gradient O
for O
the O
extractor O
be O
denoted O
as O
‚àáŒ∏LARLE O
. O
The O
Ô¨Ånal O
gradient O
for O
extractor O
‚Äôs O
parameters O
Œ∏is O
formulated O
as O
‚àáŒ∏LE+‚àáŒ∏LARLE O
while O
the O
gradient O
for O
generator O
‚Äôs O
parameters O
œÜis‚àáœÜLG+‚àáœÜLARLG O
. O
We O
use O
the O
Adam O
( O
Kingma O
and O
Ba O
, O
2014 O
) O
as O
the O
optimizer O
to O
optimize O
all O
the O
parameters O
. O
3.3 O
Background O
of O
Transformer O
The O
extractor O
and O
the O
generator O
are O
both O
backboned O
by O
the O
prevalent O
Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
model O
, O
which O
is O
a O
variant O
of O
the O
seq O
- O
to O
- O
seq O
model O
. O
It O
takes O
a O
sequence O
as O
input O
and O
generates O
another O
sequence O
as O
output O
. O
The O
Transformer O
model O
contains O
two O
parts O
, O
namely O
an O
encoder O
and O
a O
decoder O
. O
Both O
of O
them O
are O
built O
with O
several O
attention O
layers O
. O
We O
refer O
readers O
to O
the O
original O
paper O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
for O
more O
details O
. O
3.4 O
Pseudo O
Data O
Generator O
To O
handle O
the O
cold O
start O
problem O
, O
we O
propose O
a O
novel O
pseudo O
data O
generator O
( O
PDG O
) O
to O
generate O
pseudo O
data O
. O
It O
contains O
two O
components O
, O
namely O
a O
pseudo O
text O
generator O
and O
a O
pseudo O
KB O
generator O
. O
Pseudo O
Text O
Generator O
generates O
pseudo O
text O
for O
each O
KB O
and O
forms O
a O
pseudo O
supervised O
training O
data O
for O
pre O
- O
training O
the O
extractor O
and O
thus O
solving O
the O
cold O
start O
problem O
. O
We O
compute O
a O
statistics O
of O
the O
word O
count O
in O
the O
training O
set O
Ttand O
calculate O
the O
empirical O
distribution O
for O
each O
word O
as O
: O
p(w O
) O
= O
# O
w O
/ O
summationtext O
w O
/ O
prime‚ààTt#w O
/ O
prime O
, O
where O
# O
w O
stands O
for O
the O
total O
word O
count O
for O
winTt O
. O
For O
a O
list O
of O
KB O
triples O
K O
= O
[ O
k1,k2,¬∑¬∑¬∑,knk O
] O
= O
[ O
h1,r1,t1,h2,r2,t2,¬∑¬∑¬∑,hnk O
, O
rnk O
, O
tnk O
] O
, O
we O
Ô¨Årstly O
sample O
head O
entities O
and O
tail O
entities O
asKs= O
[ O
h1,t1,h2,t2,¬∑¬∑¬∑;hn O
, O
tn O
] O
. O
The O
Ô¨Ånal261sequence O
is O
generated O
by O
sampling O
from O
both O
Ksandp(w O
) O
. O
When O
generating O
each O
word O
ÀúTi O
, O
a O
random O
number O
generator O
is O
used O
to O
generate O
a O
random O
number O
riuniformly O
. O
riis O
used O
to O
compare O
with O
a O
threshold O
parameter O
Œ± O
. O
Ifri O
> O
Œ± O
, O
ÀúTiis O
sampled O
with O
the O
word O
distribution O
p(w O
) O
, O
otherwise O
, O
it O
is O
sampled O
form O
the O
next O
token O
in O
Ks O
. O
This O
process O
can O
be O
expressed O
mathematically O
as O
: O
ÀúTi=Ô£± O
Ô£¥Ô£¥Ô£≤ O
Ô£¥Ô£¥Ô£≥w‚àºp(w O
) O
ri O
> O
Œ± O
Ks[1 O
+ O
i‚àí1 O
/ O
summationdisplay O
j=11(ÀúTj‚ààKs)]otherwise O
, O
in O
which O
1(C O
) O
= O
1 O
if O
condition O
Cis O
true O
and O
0 O
otherwise O
. O
ÀúTj‚ààKsindicates O
whether O
the O
word O
ÀúTj O
is O
sampled O
from O
Ks O
. O
This O
pseudo O
text O
data O
is O
used O
to O
solve O
the O
cold O
start O
problem O
when O
training O
the O
extractor O
. O
Pseudo O
KB O
Generator O
generates O
pseudo O
KB O
triples O
for O
each O
text O
and O
form O
a O
pseudo O
supervised O
training O
data O
. O
This O
data O
is O
used O
to O
solve O
the O
cold O
start O
problem O
when O
pre O
- O
training O
the O
generator O
. O
Similar O
with O
the O
work O
of O
Freitag O
and O
Roy O
( O
2018 O
) O
, O
for O
an O
input O
sequence O
Twe O
randomly O
remove O
words O
in O
the O
input O
text O
with O
a O
probability O
Œ≤1and O
sample O
new O
words O
by O
sampling O
words O
from O
a O
distribution O
with O
a O
probability O
Œ≤2 O
. O
The O
generated O
sequence O
ÀúK O
is O
the O
pseudo O
KB O
sequence O
for O
each O
text O
. O
Similar O
to O
the O
Pseudo O
Text O
Generator O
, O
we O
randomly O
add O
some O
words O
by O
sampling O
from O
the O
distribution O
p(w O
) O
. O
We O
do O
not O
use O
the O
probability O
calculated O
from O
Ktsince O
it O
may O
sample O
some O
wrong O
relations O
or O
wrong O
entity O
names O
which O
undermines O
the O
performance O
. O
Mathematically O
, O
it O
can O
be O
expressed O
as O
: O
ÀúKi=Ô£± O
Ô£¥Ô£¥Ô£≤ O
Ô£¥Ô£¥Ô£≥w‚àºp(w O
) O
ri O
< O
Œ≤ O
2 O
Ts[1 O
+ O
i‚àí1 O
/ O
summationdisplay O
j=11(ÀúKj‚ààTs)]otherwise O
, O
in O
whichTs O
= O
s(T)ands(¬∑)is O
a O
sample O
function O
deÔ¨Åned O
as O
: O
s(T O
) O
= O
Ô£± O
Ô£¥Ô£≤ O
Ô£¥Ô£≥T O
/bardblT O
/ O
bardbl= O
0 O
[ O
T1;s(T2:/bardblT O
/ O
bardbl)]r O
< O
Œ≤ O
1,/bardblT O
/ O
bardbl O
/ O
negationslash= O
0 O
s(T2:/bardblT O
/ O
bardbl O
) O
otherwise O
, O
where O
/ O
bardblT O
/ O
bardbldenotes O
the O
length O
of O
the O
sequence O
T O
whileT2:/bardblT O
/ O
bardblstands O
for O
the O
sub O
- O
sequence O
from O
the O
second O
to O
the O
last O
of O
T.3.5 O
Allocated O
Reinforcement O
Learning O
Traditional O
reinforcement O
learning O
for O
sequence O
generation O
calculates O
a O
reward O
for O
the O
whole O
sequence O
( O
He O
et O
al O
. O
, O
2016 O
; O
Hoang O
et O
al O
. O
, O
2018 O
; O
Keneshloo O
et O
al O
. O
, O
2018 O
) O
and O
uses O
the O
policy O
gradient O
( O
Sutton O
et O
al O
. O
, O
2000 O
) O
algorithm O
to O
optimize O
the O
parameters O
. O
It O
suffers O
from O
the O
multiple O
- O
triple O
problem O
as O
discussed O
above O
. O
We O
propose O
an O
allocated O
reinforcement O
learning O
method O
to O
allocate O
different O
rewards O
for O
different O
KB O
triples O
and O
thus O
alleviate O
this O
problem O
. O
In O
the O
kb2 O
kb O
process O
, O
the O
RL O
model O
is O
called O
the O
Allocated O
Reinforcement O
Learning O
for O
Generator O
( O
ARLG O
) O
since O
it O
optimizes O
the O
parameters O
in O
the O
generator O
while O
in O
the O
txt2txt O
process O
, O
it O
is O
called O
Allocated O
Reinforcement O
Learning O
for O
Extractor O
( O
ARLE O
) O
accordingly O
. O
ARLE O
is O
shown O
in O
Fig O
. O
2 O
. O
The O
main O
idea O
is O
to O
recover O
and O
evaluate O
the O
KB O
triples O
separately O
which O
inherently O
has O
the O
following O
beneÔ¨Åts O
: O
1 O
) O
Each O
triple O
is O
given O
a O
distinct O
reward O
as O
discussed O
above O
; O
2 O
) O
Traditional O
RL O
is O
more O
likely O
to O
ignore O
some O
triples O
( O
e.g. O
, O
3rd O
triple O
in O
Fig O
. O
1 O
) O
since O
it O
handles O
several O
triples O
at O
once O
while O
our O
method O
alleviates O
such O
problem O
by O
handling O
triples O
one O
by O
one O
. O
It O
Ô¨Årstly O
sends O
the O
input O
text O
T= O
[ O
t1,t2,¬∑¬∑¬∑,tnt O
] O
into O
the O
extractor O
and O
get O
the O
extracted O
triples O
: O
Km O
= O
E(T O
) O
= O
[ O
k(1 O
) O
m O
, O
k(2 O
) O
m,¬∑¬∑¬∑,k(nk O
) O
m].The O
corresponding O
probability O
for O
each O
token O
is O
denoted O
asp(i O
) O
j O
, O
in O
whichidenotes O
the O
ith O
triple O
and O
j O
denotes O
the O
jth O
word O
in O
the O
triple O
. O
Afterwards O
, O
the O
generator O
is O
applied O
on O
each O
triple O
in O
Km O
to O
recover O
the O
corresponding O
text O
, O
which O
denotes O
as O
: O
T O
/ O
prime= O
[ O
G(k(1 O
) O
m),G(k(2 O
) O
m),¬∑¬∑¬∑,G(k(nk O
) O
m O
) O
] O
= O
[ O
t O
/ O
prime O
1,t O
/ O
prime O
2,¬∑¬∑¬∑,t O
/ O
prime O
nk O
] O
. O
We O
calculate O
the O
reward O
for O
each O
k(i O
) O
mas O
the O
recall O
for O
each O
corresponding O
t O
/ O
prime O
ireferring O
toT O
: O
R(k(i O
) O
m O
) O
= O
/summationtext O
/ O
bardblt O
/ O
prime O
i O
/ O
bardbl O
j=1 O
1(t O
/ O
prime(j O
) O
i‚ààT O
) O
/bardblt O
/ O
prime O
i O
/ O
bardbl O
, O
in O
which O
/ O
bardblt O
/ O
prime O
i O
/ O
bardbldenotes O
the O
length O
of O
t O
/ O
prime O
iand O
t O
/ O
prime(j O
) O
iis O
thejth O
word O
in O
t O
/ O
prime O
i. O
The O
reward O
for O
each O
sentence O
in O
Kmis O
denoted O
as O
: O
Re= O
[ O
R(k(1 O
) O
m),R(k(2 O
) O
m),¬∑¬∑¬∑,R(k(nk O
) O
m)].Different O
from O
the O
traditional O
policy O
gradient O
algorithm O
( O
Sutton O
et O
al O
. O
, O
2000 O
) O
, O
our O
RL O
uses O
a O
different O
reward O
for O
each O
generated O
triple O
. O
The O
gradient O
is O
calculated O
as O
: O
‚àáŒ∏LARLE O
= O
‚àíE[nk O
/ O
summationdisplay O
i=1R(k(i O
) O
m)/bardblk O
/ O
prime O
i O
/ O
bardbl O
/ O
summationdisplay O
j=1‚àáŒ∏logp(i O
) O
j].262Since O
the O
RL O
model O
only O
guides O
the O
model O
with O
some O
reward O
scores O
which O
is O
only O
one O
aspect O
of O
the O
result O
. O
It O
misleads O
the O
model O
into O
generating O
some O
sequences O
which O
have O
a O
high O
reward O
while O
actually O
perform O
worse O
. O
To O
prevent O
this O
, O
we O
propose O
to O
conduct O
the O
gradient O
descent O
together O
with O
the O
kb2 O
kb O
process O
simultaneously O
in O
which O
the O
extractor O
is O
trained O
with O
a O
supervised O
sequence O
. O
ARLG O
is O
applied O
in O
the O
kb2 O
kb O
process O
. O
The O
input O
KB O
triples O
is O
Ô¨Årstly O
splitted O
into O
nktriplesK= O
[ O
k1,k2,¬∑¬∑¬∑,knk]which O
is O
then O
sent O
into O
the O
generator O
separately O
and O
get O
the O
corresponding O
description O
sentences O
: O
Tm= O
[ O
G(k1),G(k2),¬∑¬∑¬∑,G(knk O
) O
] O
= O
[ O
t(1 O
) O
m O
, O
t(2 O
) O
m,¬∑¬∑¬∑,t(nk O
) O
m].The O
corresponding O
probability O
for O
thejth O
word O
in O
the O
ith O
sentence O
is O
denoted O
asp(i O
) O
j. O
Afterwards O
, O
the O
text O
is O
sent O
into O
the O
extractor O
to O
recover O
the O
input O
KB O
triple O
for O
each O
t(i O
) O
m O
: O
K O
/ O
prime= O
[ O
E(t(1 O
) O
m),E(t(2 O
) O
m),¬∑¬∑¬∑,E(t(nk O
) O
m O
) O
] O
= O
[ O
k O
/ O
prime O
1,k O
/ O
prime O
2,¬∑¬∑¬∑,k O
/ O
prime O
nk].We O
calculate O
the O
reward O
for O
each O
t(i O
) O
mas O
the O
precision O
for O
each O
corresponding O
k O
/ O
prime O
ireferring O
tokiinK O
: O
P(t(i O
) O
m O
) O
= O
/summationtext O
/ O
bardblk O
/ O
prime O
i O
/ O
bardbl O
j=1 O
1(k O
/ O
prime(j O
) O
i‚ààki O
) O
/bardblk O
/ O
prime O
i O
/ O
bardbl O
, O
in O
which O
/ O
bardblk O
/ O
prime O
i O
/ O
bardbldenotes O
the O
total O
word O
number O
count O
ofk O
/ O
prime O
i. O
The O
reward O
for O
each O
sentence O
in O
Tmis O
denoted O
as O
: O
Rg= O
[ O
P(t(1 O
) O
m),P(t(2 O
) O
m),¬∑¬∑¬∑,P(t(nk O
) O
m O
) O
] O
. O
We O
use O
RL O
to O
maximize O
the O
expected O
reward O
for O
each O
KB O
triple O
t(i O
) O
mwith O
corresponding O
reward O
. O
The O
gradient O
is O
: O
‚àáœÜLARLG O
= O
‚àíE[nk O
/ O
summationdisplay O
i=1P(t(i O
) O
m)/bardblt O
/ O
prime O
i O
/ O
bardbl O
/ O
summationdisplay O
j=1‚àáŒ∏logp(i O
) O
j O
] O
. O
Similar O
to O
ARLE O
, O
we O
also O
train O
the O
model O
with O
the O
txt2txt O
process O
to O
give O
a O
targeted O
sequence O
to O
guide O
the O
training O
together O
with O
the O
reward O
score O
. O
4 O
Experiments O
4.1 O
Dataset O
We O
adopt O
the O
WebNLG O
v2 O
dataset O
( O
Gardent O
et O
al O
. O
, O
2017a)1 O
. O
It O
samples O
KB O
triples O
from O
DBpedia O
and O
annotates O
corresponding O
texts O
by O
crowdsourcing O
. O
In O
order O
to O
show O
that O
our O
model O
can O
work O
under O
the O
unsupervised O
setting O
, O
we O
split O
the O
original O
dataset O
into O
two O
parts O
, O
namely O
the O
KB O
part O
and O
the O
text O
part O
. O
We O
do O
not O
assume O
any O
alignment O
between O
1https://gitlab.com/shimorina/webnlg-dataset#triples O
1 O
2 O
3 O
4 O
5 O
6 O
7 O
Total O
train O
7,429 O
6,717 O
7,377 O
6,888 O
4,982 O
488 O
471 O
34,352 O
dev O
924 O
842 O
919 O
877 O
632 O
64 O
58 O
4,316 O
test O
931 O
831 O
903 O
838 O
608 O
58 O
55 O
4,224 O
Table O
1 O
: O
Statistics O
for O
the O
dataset O
. O
The O
number O
of O
instances O
with O
different O
number O
of O
triples O
are O
listed O
. O
KB O
and O
text O
. O
Table O
1 O
shows O
the O
statistics O
of O
instances O
with O
different O
number O
of O
triples O
. O
In O
this O
dataset O
, O
one O
sentence O
can O
be O
mapped O
to O
at O
most O
seven O
triples O
. O
We O
use O
the O
same O
dev O
and O
test O
set O
as O
the O
original O
WebNLG O
. O
The O
training O
set O
has O
34,352 O
samples O
in O
total O
while O
the O
dev O
set O
and O
the O
test O
set O
have O
4,316 O
and O
4,224 O
samples O
respectively O
. O
It O
can O
be O
observed O
that O
there O
are O
78.2 O
% O
sentences O
mapped O
with O
multiple O
- O
triple O
. O
4.2 O
Comparison O
Models O
We O
compare O
our O
model O
against O
the O
following O
baseline O
methods O
: O
PDG O
uses O
the O
Pseudo O
Data O
Generator O
to O
generate O
the O
pseudo O
data O
for O
pre O
- O
training O
both O
extractor O
and O
generator O
. O
PDG O
does O
not O
conduct O
the O
subsequent O
dual O
learning O
process O
and O
thus O
illustrates O
the O
capability O
of O
PDG O
. O
DLuses O
the O
dual O
learning O
process O
proposed O
in O
He O
et O
al O
. O
( O
2016 O
) O
; O
Zhu O
et O
al O
. O
( O
2017 O
) O
. O
It O
is O
Ô¨Åne O
- O
tuned O
on O
the O
PDG O
model O
and O
iterates O
alternatively O
between O
txt2txt O
and O
kb2 O
kb O
processes O
. O
Here O
, O
we O
do O
not O
use O
any O
reinforcement O
learning O
component O
. O
DL O
- O
RL1 O
uses O
the O
dual O
learning O
process O
together O
with O
an O
RL O
component O
. O
It O
is O
similar O
to O
the O
dual O
learning O
method O
proposed O
in O
He O
et O
al O
. O
( O
2016 O
) O
; O
Zhu O
et O
al O
. O
( O
2017 O
) O
. O
We O
use O
the O
PDG O
‚Äôs O
data O
to O
train O
the O
weak O
model O
. O
It O
uses O
the O
log O
- O
likelihood O
of O
the O
recover O
process O
‚Äôs O
output O
sequence O
as O
the O
reward O
. O
DL O
- O
RL2 O
follows O
the O
settings O
of O
Sun O
et O
al O
. O
( O
2018 O
) O
. O
Different O
from O
DL O
- O
RL1 O
, O
this O
model O
uses O
the O
ROUGEL(Lin O
, O
2004 O
) O
score O
of O
the O
recovered O
sequence O
instead O
of O
using O
the O
log O
- O
likelihood O
as O
the O
reward O
. O
SEG O
is O
a O
Supervised O
Extractor O
- O
Generator O
using O
the O
original O
setting O
of O
WebNLG O
for O
both O
generator O
and O
extractor O
. O
It O
utilizes O
all O
the O
alignment O
information O
between O
KB O
and O
text O
and O
thus O
provides O
an O
upper O
bound O
for O
our O
experiment O
. O
4.3 O
Experimental O
settings O
We O
evaluate O
the O
performances O
of O
the O
generator O
and O
the O
extractor O
with O
several O
metrics O
including O
BLEU O
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
, O
NIST O
( O
Dodding-263Generator O
Extractor O
BLEU O
NIST O
METEOR O
ROUGE O
LCIDEr O
BLEU O
NIST O
METEOR O
ROUGE O
LCIDEr O
Precision O
Recall O
F1 O
PDG O
0.322 O
7.06 O
0.349 O
0.505 O
2.63 O
0.489 O
6.01 O
0.351 O
0.618 O
3.97 O
0.635 O
0.465 O
0.510 O
DL O
0.352 O
7.71 O
0.347 O
0.528 O
2.96 O
0.735 O
10.4 O
0.502 O
0.743 O
5.67 O
0.644 O
0.691 O
0.646 O
DL O
- O
RL1 O
0.356 O
7.73 O
0.350 O
0.532 O
3.00 O
0.760 O
10.8 O
0.501 O
0.755 O
5.92 O
0.670 O
0.687 O
0.658 O
DL O
- O
RL2 O
0.356 O
7.75 O
0.350 O
0.533 O
2.99 O
0.757 O
10.7 O
0.503 O
0.755 O
5.90 O
0.668 O
0.691 O
0.659 O
EGD O
0.369 O
7.77 O
0.364 O
0.541 O
3.13 O
0.775 O
11.1 O
0.503 O
0.772 O
6.25 O
0.704 O
0.691 O
0.680 O
EGD O
w/o O
ARLE O
0.351 O
7.72 O
0.347 O
0.529 O
2.97 O
0.770 O
10.9 O
0.501 O
0.764 O
6.11 O
0.683 O
0.682 O
0.665 O
EGD O
w/o O
ARLG O
0.353 O
7.77 O
0.348 O
0.531 O
2.99 O
0.729 O
10.4 O
0.505 O
0.746 O
5.61 O
0.639 O
0.695 O
0.645 O
EGD O
w/o O
PDG O
0.010 O
0.82 O
0.037 O
0.119 O
0.02 O
0.020 O
0.42 O
0.026 O
0.042 O
0.08 O
0.011 O
0.008 O
0.007 O
SEG O
0.406 O
8.31 O
0.385 O
0.585 O
3.66 O
0.848 O
11.8 O
0.595 O
0.867 O
7.43 O
0.783 O
0.830 O
0.796 O
Table O
2 O
: O
Results O
for O
generator O
( O
left O
) O
and O
extractor O
( O
right O
) O
, O
which O
are O
evaluated O
with O
generation O
metrics O
. O
For O
the O
extractor O
, O
precision O
, O
recall O
, O
and O
F1 O
scores O
are O
also O
calculated O
at O
triple O
‚Äôs O
level O
. O
The O
performances O
of O
our O
EGD O
method O
without O
different O
components O
and O
the O
supervised O
method O
SEG O
are O
shown O
in O
the O
bottom O
. O
RatioGenerator O
Extractor O
BLEU O
ROUGE O
L O
BLEU O
ROUGE O
L O
0.10 O
0.235 O
0.439 O
0.335 O
0.557 O
0.15 O
0.281 O
0.49 O
0.655 O
0.708 O
0.20 O
0.308 O
0.506 O
0.746 O
0.757 O
0.25 O
0.347 O
0.524 O
0.71 O
0.764 O
PDG O
0.322 O
0.505 O
0.489 O
0.618 O
Table O
3 O
: O
Compare O
our O
PDG O
framework O
with O
semisupervised O
models O
at O
different O
labeling O
ratios O
. O
ton O
, O
2002 O
) O
, O
METEOR O
( O
Banerjee O
and O
Lavie O
, O
2005 O
) O
, O
ROUGEL(Lin O
, O
2004 O
) O
, O
and O
CIDEr O
( O
Vedantam O
et O
al O
. O
, O
2015 O
) O
. O
These O
metrics O
are O
calculated O
with O
the O
evaluation O
code O
provided O
in O
Novikova O
et O
al O
. O
( O
2017 O
) O
. O
Moreover O
, O
we O
also O
evaluate O
the O
performance O
of O
the O
extractor O
with O
precision O
, O
recall O
, O
and O
F1 O
scores O
( O
Manning O
et O
al O
. O
, O
2010 O
) O
. O
In O
PDG O
, O
we O
set O
Œ±= O
0.8,Œ≤1= O
0.2,Œ≤2= O
0.6 O
. O
We O
Ô¨Årstly O
pre O
- O
train O
the O
extractor O
and O
the O
generator O
in O
the O
PDG O
model O
with O
the O
data O
generated O
by O
PDG O
until O
convergence O
. O
All O
other O
models O
are O
Ô¨Åne O
- O
tuned O
on O
the O
PDG O
model O
. O
For O
the O
DL O
model O
, O
we O
train O
the O
generator O
for O
5 O
steps O
with O
the O
txt2txt O
process O
and O
train O
the O
extractor O
with O
the O
kb2 O
kb O
process O
for O
another O
5 O
steps O
with O
the O
new O
generator O
. O
We O
iterate O
this O
process O
10 O
times O
. O
For O
all O
transformers O
, O
we O
set O
clip O
norm O
to O
1.0 O
, O
label O
smoothing O
to O
0.1 O
, O
and O
dropout O
to O
0.3 O
. O
We O
use O
Adam O
( O
Kingma O
and O
Ba O
, O
2014 O
) O
as O
our O
optimizer O
and O
set O
the O
learning O
rate O
for O
the O
extractor O
to O
2e-4 O
and O
generator O
to O
5e-4 O
. O
All O
hyper O
- O
parameters O
are O
tuned O
on O
the O
dev O
dataset O
with O
grid O
search O
. O
4.4 O
Experimental O
Results O
The O
performances O
of O
our O
KB O
- O
to O
- O
text O
generator O
and O
triple O
extractor O
are O
shown O
in O
the O
left O
and O
right O
of O
Table O
2 O
respectively O
. O
Both O
generator O
and O
extractor O
of O
our O
model O
outperform O
all O
baseline O
models O
signiÔ¨Åcantly O
and O
consistently O
. O
The O
comparison O
between O
our O
EGD O
model O
and O
the O
supervised O
SEG O
model O
indicates O
that O
our O
unsupervised O
EGD O
model O
2 O
4 O
60.40.60.8Extractor O
BLEU O
2 O
4 O
60.40.6 O
ROUGEL O
2 O
4 O
60.30.40.5Generator O
2 O
4 O
60.350.400.450.50 O
PDG O
DL O
EGD O
SEGFigure O
3 O
: O
The O
inÔ¨Çuence O
of O
KB O
triples O
count O
. O
The O
xaxis O
represents O
the O
KB O
triples O
count O
while O
the O
y O
- O
axis O
represents O
the O
scores O
. O
is O
close O
to O
the O
bound O
of O
the O
supervised O
methods O
. O
Compared O
with O
the O
PDG O
model O
, O
our O
EGD O
model O
has O
a O
much O
better O
performance O
with O
the O
dual O
learning O
framework O
and O
the O
ARL O
component O
. O
Moreover O
, O
Our O
EGD O
model O
outperforms O
the O
DL O
- O
RL1 O
and O
DLRL2 O
model O
, O
which O
indicates O
that O
our O
proposed O
ARL O
component O
can O
handle O
the O
multiple O
- O
triple O
problem O
between O
triples O
and O
texts O
. O
In O
the O
traditional O
RL O
models O
, O
the O
reward O
is O
the O
same O
for O
a O
whole O
sequence O
including O
all O
the O
triples O
while O
in O
our O
ARL O
model O
, O
the O
reward O
is O
calculated O
for O
several O
subparts O
of O
the O
sequence O
, O
which O
is O
more O
accurate O
and O
effective O
. O
By O
comparing O
PDG O
with O
SEG O
, O
we O
found O
that O
the O
model O
trained O
with O
our O
proposed O
pseudo O
data O
generator O
( O
PDG O
) O
‚Äôs O
output O
achieves O
acceptable O
results O
. O
It O
indicates O
that O
using O
the O
PDG O
‚Äôs O
output O
is O
a O
feasible O
alternative O
to O
initialize O
the O
model O
and O
can O
handle O
the O
cold O
start O
problem O
. O
Ablation O
Study O
. O
We O
also O
conduct O
some O
ablation O
studies O
to O
show O
that O
each O
component O
contributes264Extractor O
Generator O
Gold(1634 O
: O
The O
Ram O
Rebellion O
, O
mediaType O
, O
E O
- O
book O
) O
( O
1634 O
: O
The O
Ram O
Rebellion O
, O
author O
, O
Virginia O
DeMarce)Virginia O
DeMarce O
is O
the O
author O
of O
1634 O
: O
The O
Ram O
Rebellion O
, O
which O
can O
be O
found O
as O
an O
e O
- O
book O
. O
SEG(1634 O
: O
The O
Ram O
Rebellion O
, O
mediaType O
, O
E O
- O
book O
) O
( O
1634 O
: O
The O
Ram O
Rebellion O
, O
author O
, O
Virginia O
DeMarce)1634 O
: O
The O
Ram O
Rebellion O
was O
written O
by O
Virginia O
DeMarce O
and O
has O
the O
ISBN O
number O
1 O
- O
4165 O
- O
2060 O
- O
0 O
. O
PDG O
( O
1634 O
: O
The O
Ram O
Rebellion O
, O
mediaType O
, O
E O
- O
book)1634 O
: O
The O
Ram O
Rebellion O
was O
followed O
by O
1634 O
: O
The O
Galileo O
Affair O
and O
its O
author O
is O
Virginia O
DeMarce O
. O
DL(1634 O
: O
The O
Ram O
Rebellion O
, O
mediaType O
, O
E O
- O
book O
) O
( O
1634 O
: O
The O
Ram O
Rebellion O
, O
author O
, O
Virginia O
DeMarce O
) O
( O
1634 O
: O
The O
Ram O
Rebellion O
, O
ISBN O
number O
, O
1 O
- O
4165 O
- O
2060 O
- O
0)1634 O
: O
The O
Ram O
Rebellion O
is O
available O
as O
an O
E O
- O
Book O
. O
EGD(1634 O
: O
The O
Ram O
Rebellion O
, O
mediaType O
, O
E O
- O
book O
) O
( O
1634 O
: O
The O
Ram O
Rebellion O
, O
author O
, O
Virginia O
DeMarce)Virginia O
DeMarce O
is O
the O
author O
of O
1634 O
: O
The O
Ram O
Rebellion O
, O
currently O
in O
print O
. O
Table O
4 O
: O
Case O
study O
. O
The O
input O
KB O
and O
text O
are O
listed O
in O
the O
Ô¨Årst O
row O
. O
to O
the O
Ô¨Ånal O
performance O
. O
The O
results O
are O
shown O
at O
the O
bottom O
part O
of O
Table O
2 O
. O
By O
comparing O
the O
model O
EGD O
w/o O
ARLE O
and O
EGD O
w/o O
ARLG O
with O
the O
EGD O
model O
, O
we O
can O
see O
that O
both O
the O
ARLE O
and O
ARLG O
components O
are O
effective O
to O
handle O
the O
multiple O
- O
triple O
problem O
and O
help O
improve O
the O
performance O
. O
It O
is O
interesting O
to O
see O
that O
the O
result O
of O
EGD O
w/o O
PDG O
is O
extremely O
poor O
showing O
the O
importance O
of O
our O
PDG O
component O
. O
The O
EGD O
w/o O
PDG O
removes O
the O
pre O
- O
train O
stage O
with O
the O
pseudo O
data O
generator O
and O
conducts O
the O
iterations O
between O
txt2txt O
and O
kb2 O
kb O
directly O
. O
Without O
PDG O
, O
we O
observe O
that O
the O
models O
trend O
to O
learn O
some O
‚Äú O
own O
language O
‚Äù O
without O
a O
good O
initialization O
which O
is O
incomprehensible O
to O
human O
. O
The O
InÔ¨Çuence O
of O
the O
KB O
triples O
Number O
. O
We O
analyze O
the O
inÔ¨Çuence O
of O
the O
KB O
triples O
‚Äô O
number O
on O
the O
performance O
. O
The O
results O
are O
shown O
in O
Fig O
. O
3 O
. O
As O
expected O
, O
the O
SEG O
model O
performs O
the O
best O
over O
all O
numbers O
since O
it O
is O
fully O
supervised O
. O
The O
PDG O
model O
performs O
the O
worst O
since O
it O
only O
uses O
pseudo O
data O
to O
train O
. O
The O
DL O
model O
improves O
signiÔ¨Åcantly O
comparing O
with O
the O
PDG O
model O
over O
all O
numbers O
, O
especially O
in O
the O
extractor O
model O
. O
It O
shows O
that O
using O
dual O
learning O
‚Äôs O
iteration O
approach O
does O
improve O
the O
model O
of O
training O
solely O
based O
on O
PDG O
‚Äôs O
data O
. O
Our O
proposed O
EGD O
model O
outperforms O
the O
DL O
model O
and O
the O
PDG O
model O
. O
This O
shows O
that O
the O
ARL O
model O
does O
help O
to O
give O
more O
information O
to O
train O
the O
model O
. O
Nearly O
all O
generators O
‚Äô O
scores O
decrease O
as O
the O
number O
increases O
. O
This O
is O
because O
if O
the O
sequence O
is O
long O
, O
it O
has O
more O
ways O
to O
express O
those O
triples O
which O
may O
be O
different O
from O
the O
gold O
standard O
sentence O
. O
However O
, O
when O
extracting O
triples O
from O
the O
text O
, O
it O
only O
has O
one O
correct O
way O
and O
thus O
the O
extractor O
‚Äôs O
scores O
are O
similar O
in O
all O
lengths O
. O
Error O
Analysis O
. O
We O
conduct O
an O
error O
analysis O
experiment O
for O
the O
top O
20 O
mentioned O
relations O
in O
Figure O
4 O
: O
Error O
analysis O
for O
top O
20 O
mentioned O
relations O
. O
the O
extractor O
which O
is O
shown O
in O
Fig O
. O
4 O
. O
We O
focus O
on O
two O
kinds O
of O
errors O
. O
The O
Ô¨Årst O
kind O
of O
error O
is O
called O
‚Äú O
false O
negative O
‚Äù O
which O
means O
when O
extracting O
, O
some O
correct O
triples O
are O
ignored O
. O
The O
second O
kind O
of O
error O
is O
called O
‚Äú O
false O
positive O
‚Äù O
which O
means O
that O
the O
extractor O
generates O
some O
incorrect O
triples O
that O
the O
text O
does O
not O
mention O
. O
It O
can O
be O
observed O
that O
the O
‚Äú O
false O
negative O
‚Äù O
problem O
is O
much O
more O
severe O
than O
the O
‚Äú O
false O
positive O
‚Äù O
problem O
for O
the O
PDG O
model O
, O
while O
the O
DL O
model O
and O
the O
EGD O
model O
alleviate O
this O
problem O
a O
lot O
. O
This O
reason O
is O
that O
the O
pseudo O
text O
data O
is O
made O
by O
sampling O
entities O
in O
KB O
ignoring O
relation O
information O
. O
Iterating O
alternately O
between O
txt2txt O
and O
kb2 O
solves O
the O
problem O
since O
the O
missing O
information O
is O
supplemented O
. O
It O
can O
also O
be O
observed O
that O
when O
comparing O
with O
the O
DL O
model O
, O
our O
EGD O
model O
mainly O
solves O
the O
‚Äú O
false O
positive O
‚Äù O
problem O
. O
The O
reason O
is O
that O
the O
RL O
can O
penalize O
the O
wrong O
generated O
triples O
but O
can O
not O
give O
speciÔ¨Åc O
guidance O
on O
which O
missing O
triples O
the O
model O
should O
generate O
. O
Comparison O
with O
Semi O
- O
Supervised O
Learning O
. O
To O
measure O
the O
quality O
of O
the O
initialization O
via O
PDG O
, O
we O
compare O
our O
PDG O
method O
against O
the O
semisupervised O
learning O
method O
. O
We O
sample O
labeled O
data O
from O
the O
original O
dataset O
with O
different O
ratios O
to O
train O
the O
models O
and O
compare O
the O
results O
with O
the O
PDG O
model O
. O
The O
result O
is O
shown O
in O
Table O
3 O
. O
It O
can O
be O
concluded O
from O
the O
result O
that O
training O
the265extractor O
with O
the O
PDG O
‚Äôs O
data O
outperforms O
training O
with O
10 O
% O
aligned O
data O
and O
it O
also O
outperforms O
20 O
% O
aligned O
data O
for O
the O
generator O
. O
It O
shows O
that O
our O
PDG O
component O
does O
provide O
usable O
data O
and O
it O
can O
be O
boosted O
a O
lot O
in O
the O
subsequent O
dual O
iteration O
process O
. O
Case O
Study O
. O
Table O
4 O
shows O
a O
case O
study O
for O
4 O
models O
. O
For O
the O
extractor O
, O
the O
input O
is O
‚Äú O
Virginia O
DeMarce O
is O
the O
author O
of O
1634 O
: O
The O
Ram O
Rebellion O
, O
which O
can O
be O
found O
as O
an O
e O
- O
book O
. O
‚Äù O
. O
For O
the O
generator O
, O
the O
input O
is O
‚Äú O
( O
1634 O
: O
The O
Ram O
Rebellion O
, O
mediaType O
, O
E O
- O
book O
) O
( O
1634 O
: O
The O
Ram O
Rebellion O
, O
author O
, O
Virginia O
DeMarce O
) O
‚Äù O
. O
It O
can O
be O
observed O
that O
for O
the O
PDG O
model O
, O
it O
omits O
the O
second O
triple O
. O
It O
also O
shows O
that O
the O
PDG O
model O
has O
a O
severe O
false O
negative O
problem O
which O
has O
been O
mentioned O
in O
the O
error O
analysis O
sub O
- O
section O
. O
The O
DL O
model O
alleviates O
this O
problem O
but O
it O
introduces O
more O
triples O
causing O
the O
false O
positive O
problem O
. O
Our O
EGD O
model O
solves O
the O
false O
positive O
problem O
by O
the O
RL O
component O
. O
All O
models O
make O
some O
mistakes O
in O
the O
generation O
process O
including O
the O
supervised O
SEG O
model O
. O
The O
result O
of O
the O
generator O
shows O
that O
it O
is O
more O
difÔ¨Åcult O
to O
generate O
a O
sequence O
than O
extracting O
triples O
. O
5 O
Conclusions O
We O
propose O
a O
new O
challenging O
task O
, O
namely O
, O
unsupervised O
KB O
- O
to O
- O
text O
generation O
. O
To O
solve O
this O
task O
, O
we O
propose O
an O
extractor O
- O
generator O
dual O
framework O
which O
exploits O
the O
inverse O
relationship O
between O
the O
KB O
- O
to O
- O
text O
generation O
task O
and O
the O
auxiliary O
triple O
extraction O
task O
. O
To O
handle O
the O
cold O
start O
problem O
and O
the O
multiple O
- O
triple O
problem O
respectively O
, O
we O
propose O
a O
novel O
pseudo O
data O
generator O
and O
an O
allocated O
reinforcement O
learning O
component O
. O
Experimental O
results O
show O
that O
our O
proposed O
method O
successfully O
resolves O
the O
observed O
problems O
and O
outperforms O
all O
the O
baseline O
models O
. O
Abstract O
Despite O
the O
recent O
achievements O
made O
in O
the O
multi O
- O
modal O
emotion O
recognition O
task O
, O
two O
problems O
still O
exist O
and O
have O
not O
been O
well O
investigated O
: O
1 O
) O
the O
relationship O
between O
different O
emotion O
categories O
are O
not O
utilized O
, O
which O
leads O
to O
sub O
- O
optimal O
performance O
; O
and O
2 O
) O
current O
models O
fail O
to O
cope O
well O
with O
low O
- O
resource O
emotions O
, O
especially O
for O
unseen O
emotions O
. O
In O
this O
paper O
, O
we O
propose O
a O
modality O
- O
transferable O
model O
with O
emotion O
embeddings O
to O
tackle O
the O
aforementioned O
issues O
. O
We O
use O
pre O
- O
trained O
word O
embeddings O
to O
represent O
emotion O
categories O
for O
textual O
data O
. O
Then O
, O
two O
mapping O
functions O
are O
learned O
to O
transfer O
these O
embeddings O
into O
visual O
and O
acoustic O
spaces O
. O
For O
each O
modality O
, O
the O
model O
calculates O
the O
representation O
distance O
between O
the O
input O
sequence O
and O
target O
emotions O
and O
makes O
predictions O
based O
on O
the O
distances O
. O
By O
doing O
so O
, O
our O
model O
can O
directly O
adapt O
to O
the O
unseen O
emotions O
in O
any O
modality O
since O
we O
have O
their O
pre O
- O
trained O
embeddings O
and O
modality O
mapping O
functions O
. O
Experiments O
show O
that O
our O
model O
achieves O
stateof O
- O
the O
- O
art O
performance O
on O
most O
of O
the O
emotion O
categories O
. O
Besides O
, O
our O
model O
also O
outperforms O
existing O
baselines O
in O
the O
zero O
- O
shot O
and O
few O
- O
shot O
scenarios O
for O
unseen O
emotions1 O
. O
1 O
Introduction O
Multi O
- O
modal O
emotion O
recognition O
is O
an O
increasingly O
popular O
but O
challenging O
task O
. O
One O
main O
challenge O
is O
that O
labelled O
data O
is O
difÔ¨Åcult O
to O
come O
by O
as O
humans O
Ô¨Ånd O
it O
time O
- O
consuming O
to O
discern O
emotion O
categories O
from O
either O
speech O
or O
video O
. O
Indeed O
we O
humans O
express O
emotions O
through O
a O
combination O
of O
modalities O
, O
including O
the O
way O
we O
speak O
, O
the O
words O
we O
use O
, O
facial O
expressions O
and O
sometimes O
gestures O
. O
It O
is O
also O
much O
more O
comfortable O
for O
humans O
to O
understand O
each O
other O
‚Äôs O
emotions O
when O
they O
can O
both O
1Code O
is O
available O
at O
https://github.com/ O
wenliangdai O
/ O
Modality O
- O
Transferable O
- O
MER O
happysurprised O
Acoustic O
	 O
Spacehappy O
surprised O
T O
extual O
	 O
Spacehappy O
surprised O
V O
isual O
	 O
Space O
He O
	 O
pr O
oposed O
. O
		 O
W O
ell O
, O
	 O
and O
	 O
I O
	 O
said O
	 O
yes O
, O
	 O
of O
	 O
course O
. O
Textual O
Visual O
Acoustic O
I O
'm O
	 O
going O
	 O
to O
	 O
miss O
	 O
you O
. O
Textual O
Visual O
Acoustic O
. O
	 O
. O
	 O
. O
sad O
sad O
sad O
( O
happy O
	 O
and O
	  O
surprised O
) O
( O
sad)Seen O
	 O
Emotion O
	 O
Example O
Unseen O
	 O
Emotion O
	 O
Example O
. O
	 O
. O
	 O
. O
Figure O
1 O
: O
An O
intuitive O
example O
of O
our O
method O
. O
In O
the O
upper O
image O
, O
the O
relative O
positions O
of O
GloVe O
emotion O
embeddings O
( O
happy O
, O
surprised O
) O
are O
shown O
in O
the O
textual O
space O
, O
which O
are O
then O
projected O
to O
acoustic O
and O
visual O
spaces O
by O
two O
mapping O
functions O
( O
ft‚Üíaandft‚Üív O
) O
. O
Our O
model O
learns O
to O
group O
the O
representations O
of O
input O
sentences O
( O
  O
, O
) O
based O
on O
their O
corresponding O
emotion O
embeddings O
. O
Examples O
are O
shown O
in O
the O
lower O
image O
. O
When O
a O
sample O
has O
both O
happy O
andsurprised O
emotions O
, O
its O
representation O
gets O
close O
to O
these O
two O
emotion O
embeddings O
in O
all O
three O
spaces O
. O
If O
an O
unseen O
emotion O
sad O
( O
) O
comes O
, O
the O
model O
processes O
it O
with O
the O
same O
Ô¨Çow O
and O
recognizes O
corresponding O
data O
samples O
. O
hear O
and O
see O
the O
other O
person O
. O
It O
follows O
that O
multimodal O
emotion O
recognition O
can O
, O
therefore O
, O
yield O
more O
reliable O
results O
than O
restricting O
machines O
to O
a O
single O
modality O
. O
In O
the O
past O
few O
years O
, O
much O
research O
has O
been O
done O
to O
better O
understand O
intra O
- O
modality O
and O
intermodality O
dynamics O
, O
and O
modality O
fusion O
is O
a O
widely O
studied O
approach O
. O
For O
example O
, O
Zadeh O
et O
al O
. O
( O
2017 O
) O
proposed O
a O
tensor O
fusion O
network O
that O
combines O
three O
modalities O
from O
vectors O
to O
a O
tensor O
using O
the O
Cartesian O
product O
. O
In O
addition O
, O
the O
attention269mechanism O
is O
commonly O
used O
to O
do O
modality O
fusion O
( O
Zadeh O
et O
al O
. O
, O
2018a O
; O
Wang O
et O
al O
. O
, O
2018 O
; O
Liang O
et O
al O
. O
, O
2018 O
; O
Hazarika O
et O
al O
. O
, O
2018 O
; O
Pham O
et O
al O
. O
, O
2018 O
; O
Tsai O
et O
al O
. O
, O
2019a O
) O
. O
Although O
signiÔ¨Åcant O
improvements O
have O
been O
made O
on O
the O
multi O
- O
modal O
emotion O
recognition O
task O
, O
however O
, O
the O
relationship O
between O
emotions O
has O
not O
been O
well O
modelled O
, O
which O
can O
lead O
to O
sub O
- O
optimal O
performance O
. O
Also O
, O
the O
problem O
of O
low O
- O
resource O
multi O
- O
modal O
emotion O
recognition O
is O
not O
adequately O
studied O
. O
Multi O
- O
modal O
emotion O
recognition O
data O
is O
hard O
to O
collect O
and O
annotate O
, O
especially O
for O
low O
- O
resource O
emotions O
( O
e.g. O
, O
surprise O
) O
that O
are O
rarely O
seen O
in O
daily O
life O
, O
which O
motivates O
us O
to O
investigate O
this O
problem O
. O
In O
this O
paper O
, O
we O
propose O
a O
modality O
- O
transferable O
network O
with O
cross O
- O
modality O
emotion O
embeddings O
to O
model O
the O
relationship O
between O
emotions O
. O
Given O
that O
emotion O
embeddings O
contain O
semantic O
information O
and O
emotion O
relations O
in O
the O
vector O
space O
, O
we O
use O
them O
to O
represent O
emotion O
categories O
and O
measure O
the O
similarity O
of O
the O
representations O
between O
the O
input O
sentence O
and O
target O
emotions O
to O
make O
predictions O
. O
Concretely O
, O
for O
the O
textual O
modality O
, O
we O
use O
the O
pre O
- O
trained O
GloVe O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
embeddings O
of O
emotion O
words O
as O
the O
emotion O
embeddings O
. O
As O
there O
are O
no O
pre O
- O
trained O
emotion O
embeddings O
for O
the O
visual O
and O
acoustic O
modalities O
, O
the O
model O
learns O
two O
mapping O
functions O
, O
ft‚Üívand O
ft‚Üía O
, O
to O
transfer O
the O
emotion O
embeddings O
from O
the O
textual O
space O
to O
the O
visual O
and O
acoustic O
spaces O
( O
Figure O
1 O
) O
. O
Therefore O
, O
for O
each O
modality O
, O
there O
will O
be O
a O
dedicated O
set O
of O
emotion O
embeddings O
. O
The O
distances O
computed O
in O
all O
modalities O
will O
be O
Ô¨Ånally O
fused O
, O
and O
the O
model O
will O
make O
predictions O
based O
on O
that O
. O
BeneÔ¨Åting O
from O
this O
prediction O
mechanism O
, O
our O
model O
can O
easily O
carry O
out O
zero O
- O
shot O
learning O
( O
ZSL O
) O
to O
identify O
unseen O
emotion O
categories O
using O
the O
embeddings O
from O
unseen O
emotions O
. O
The O
intuition O
behind O
it O
is O
that O
the O
pre O
- O
trained O
and O
projected O
emotion O
embeddings O
form O
a O
semantic O
knowledge O
space O
, O
which O
is O
shared O
by O
both O
the O
seen O
and O
unseen O
classes O
. O
Furthermore O
, O
with O
the O
help O
of O
embedding O
mapping O
functions O
, O
the O
model O
can O
also O
perform O
ZSL O
on O
a O
single O
modality O
during O
inference O
time O
. O
When O
a O
few O
samples O
from O
unseen O
emotions O
are O
available O
, O
our O
model O
can O
adapt O
to O
new O
emotions O
without O
forgetting O
the O
previous O
emotions O
by O
using O
joint O
training O
and O
continual O
learning O
( O
Lopez O
- O
Paz O
and O
Ranzato O
, O
2017 O
) O
. O
Our O
contributions O
in O
this O
work O
are O
three O
- O
fold:‚Ä¢We O
introduce O
a O
simple O
but O
effective O
end O
- O
toend O
model O
for O
the O
multi O
- O
modal O
emotion O
recognition O
task O
. O
It O
learns O
the O
relationship O
of O
different O
emotion O
categories O
using O
emotion O
embeddings O
. O
‚Ä¢To O
the O
best O
of O
our O
knowledge O
, O
this O
paper O
is O
the O
Ô¨Årst O
to O
investigate O
multi O
- O
modal O
emotion O
recognition O
in O
the O
low O
- O
resource O
scenario O
. O
Our O
model O
can O
directly O
adapt O
to O
an O
unseen O
emotion O
, O
even O
if O
only O
one O
modality O
is O
available O
. O
‚Ä¢Experimental O
results O
show O
that O
our O
model O
achieves O
state O
- O
of O
- O
the O
- O
art O
results O
on O
most O
emotion O
categories O
. O
We O
also O
provide O
a O
thorough O
analysis O
of O
zero O
- O
shot O
and O
few O
- O
shot O
learning O
. O
2 O
Related O
Works O
2.1 O
Multi O
- O
modal O
Emotion O
Recognition O
Since O
the O
early O
2010s O
, O
multi O
- O
modal O
emotion O
recognition O
has O
drawn O
more O
and O
more O
attention O
with O
the O
rise O
of O
deep O
learning O
and O
its O
advances O
in O
computer O
vision O
and O
natural O
language O
processing O
( O
BaltruÀásaitis O
et O
al O
. O
, O
2018 O
) O
. O
Schuller O
et O
al O
. O
( O
2011 O
) O
proposed O
the O
Ô¨Årst O
Audio O
- O
Visual O
Emotion O
Challenge O
and O
Workshop O
( O
A O
VEC O
) O
, O
which O
focused O
on O
multimodal O
emotion O
analysis O
for O
health O
. O
In O
recent O
years O
, O
most O
achievements O
in O
this O
area O
aimed O
to O
Ô¨Ånd O
a O
better O
modality O
fusion O
method O
. O
Zadeh O
et O
al O
. O
( O
2017 O
) O
introduced O
a O
tensor O
fusion O
network O
that O
combined O
data O
representation O
from O
each O
modality O
to O
a O
tensor O
by O
performing O
the O
Cartesian O
product O
. O
In O
addition O
, O
the O
attention O
mechanism O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
) O
has O
been O
widely O
applied O
to O
do O
modality O
fusion O
and O
emphasis O
( O
Zadeh O
et O
al O
. O
, O
2018a O
; O
Pham O
et O
al O
. O
, O
2018 O
; O
Tsai O
et O
al O
. O
, O
2019a O
) O
. O
Furthermore O
, O
Liu O
et O
al O
. O
( O
2018 O
) O
proposed O
a O
low O
- O
rank O
architecture O
to O
decrease O
the O
problem O
complexity O
, O
and O
Tsai O
et O
al O
. O
( O
2019b O
) O
introduced O
a O
modality O
re O
- O
construction O
method O
to O
generate O
occasional O
missing O
data O
in O
a O
modality O
. O
Although O
prior O
works O
have O
made O
progress O
on O
this O
task O
, O
the O
relationship O
between O
emotion O
categories O
has O
not O
been O
well O
modelled O
in O
previous O
works O
, O
except O
by O
Xu O
et O
al O
. O
( O
2020 O
) O
, O
who O
captured O
emotion O
correlations O
using O
graph O
networks O
for O
emotion O
recognition O
. O
However O
, O
the O
model O
is O
only O
based O
on O
a O
single O
textual O
modality O
. O
Additionally O
, O
the O
previous O
studies O
have O
not O
put O
much O
effort O
toward O
unseen O
and O
lowresource O
emotion O
categories O
, O
which O
is O
a O
problem O
of O
multi O
- O
modal O
emotion O
data O
by O
nature.270LSTMV O
LSTMV O
LSTMV O
LSTMV O
LSTMAIntra O
- O
Modality O
	 O
Encoder O
	 O
Networks O
Thank O
	   O
excited O
you O
I O
V O
isual O
	 O
emotion O
	 O
embeddings O
Acoustic O
	 O
emotion O
	 O
embeddingsT O
extual O
	 O
emotion O
	 O
embeddingsFusionAttention O
Attention O
Attention O
... O
		  O
... O
	 O
... O
Modality O
	 O
Mapping O
Modality O
	 O
Fusion O
Loss O
... O
	  O
LSTM O
T O
LSTM O
T O
LSTM O
T O
LSTM O
T O
LSTMA O
LSTMA O
LSTMAFigure O
2 O
: O
The O
architecture O
of O
our O
proposed O
multi O
- O
modal O
emotion O
recognition O
model O
. O
It O
consists O
of O
three O
LSTM O
networks O
, O
one O
emotion O
embedding O
mapping O
module O
, O
and O
one O
modality O
fusion O
module O
. O
For O
each O
modality O
, O
the O
input O
is O
a O
sequence O
of O
length O
T. O
Each O
modality O
has O
a O
set O
of O
emotion O
embeddings O
by O
mapping O
the O
GloVe O
textual O
emotion O
embeddings O
to O
the O
other O
modalities O
using O
ft‚Üívandft‚Üía O
. O
The O
whole O
architecture O
is O
optimized O
end O
- O
to O
- O
end O
. O
2.2 O
Zero O
/ O
Few O
- O
Shot O
and O
Continual O
Learning O
Zero O
- O
shot O
and O
few O
- O
shot O
learning O
methods O
, O
which O
address O
the O
data O
scarcity O
scenario O
, O
have O
been O
applied O
to O
many O
popular O
machine O
learning O
tasks O
where O
zero O
or O
only O
a O
few O
training O
samples O
are O
available O
for O
the O
target O
tasks O
or O
domains O
, O
such O
as O
machine O
translation O
( O
Johnson O
et O
al O
. O
, O
2017 O
; O
Gu O
et O
al O
. O
, O
2018 O
) O
, O
dialogue O
generation O
( O
Zhao O
and O
Eskenazi O
, O
2018 O
; O
Madotto O
et O
al O
. O
, O
2019 O
) O
, O
dialogue O
state O
tracking O
( O
Liu O
et O
al O
. O
, O
2019c O
; O
Wu O
et O
al O
. O
, O
2019 O
) O
, O
slot O
Ô¨Ålling O
( O
Bapna O
et O
al O
. O
, O
2017 O
; O
Liu O
et O
al O
. O
, O
2019b O
, O
2020 O
) O
, O
and O
accented O
speech O
recognition O
( O
Winata O
et O
al O
. O
, O
2020 O
) O
. O
They O
have O
also O
been O
adopted O
in O
multiple O
cross O
- O
lingual O
tasks O
, O
such O
as O
named O
entity O
recognition O
( O
Xie O
et O
al O
. O
, O
2018 O
; O
Ni O
et O
al O
. O
, O
2017 O
) O
, O
part O
- O
ofspeech O
tagging O
( O
Wisniewski O
et O
al O
. O
, O
2014 O
; O
Huck O
et O
al O
. O
, O
2019 O
) O
, O
and O
question O
answering O
( O
Liu O
et O
al O
. O
, O
2019a O
; O
Lewis O
et O
al O
. O
, O
2019 O
) O
. O
Recently O
, O
several O
methods O
have O
been O
proposed O
for O
continual O
learning O
( O
Rusu O
et O
al O
. O
, O
2016 O
; O
Kirkpatrick O
et O
al O
. O
, O
2017 O
; O
Lopez O
- O
Paz O
and O
Ranzato O
, O
2017 O
; O
Fernando O
et O
al O
. O
, O
2017 O
; O
Lee O
et O
al O
. O
, O
2017 O
) O
, O
and O
these O
were O
applied O
to O
some O
NLP O
tasks O
, O
such O
as O
opinion O
mining O
( O
Shu O
et O
al O
. O
, O
2016 O
) O
, O
document O
classiÔ¨Åcation O
( O
Shu O
et O
al O
. O
, O
2017 O
) O
, O
and O
dialogue O
state O
tracking O
( O
Wu O
et O
al O
. O
, O
2019 O
) O
. O
3 O
Methodology O
As O
shown O
in O
Figure O
2 O
, O
our O
model O
consists O
of O
three O
parts O
: O
intra O
- O
modal O
encoder O
networks O
, O
emotion O
em O
- O
bedding O
mapping O
modules O
, O
and O
an O
inter O
- O
modal O
fusion O
module O
. O
In O
this O
section O
, O
we O
Ô¨Årst O
deÔ¨Åne O
the O
problem O
, O
and O
then O
we O
introduce O
the O
details O
of O
our O
model O
. O
3.1 O
Problem O
DeÔ¨Ånition O
We O
deÔ¨Åne O
the O
input O
multi O
- O
modal O
data O
samples O
as O
X={(ti O
, O
ai O
, O
vi)}I O
i=1 O
, O
in O
which O
Idenotes O
the O
total O
number O
of O
samples O
, O
and O
t O
, O
a O
, O
vdenote O
the O
textual O
, O
acoustic O
, O
and O
visual O
modalities O
, O
respectively O
. O
For O
each O
modality O
, O
there O
is O
a O
set O
of O
emotion O
embeddings O
that O
represent O
the O
semantic O
meanings O
for O
the O
emotion O
categories O
to O
be O
recognized O
. O
In O
the O
textual O
modality O
, O
we O
have O
Et={et O
k}K O
k=1 O
, O
which O
is O
from O
the O
pre O
- O
trained O
GloVe O
embeddings O
. O
In O
acoustic O
and O
visual O
modalities O
, O
we O
have O
Ea={ea O
k}K O
k=1 O
andEv={ev O
k}K O
k=1 O
, O
which O
are O
mapped O
from O
Etby O
the O
mapping O
function O
ft‚Üívandft‚Üía O
. O
Kdenotes O
the O
number O
of O
emotion O
categories O
, O
and O
it O
can O
be O
changed O
to O
Ô¨Åt O
different O
tasks O
and O
zero O
- O
shot O
learning O
. O
Y={yi}I O
i=1denotes O
the O
annotations O
for O
multilabel O
emotion O
recognition O
, O
where O
yiis O
a O
vector O
of O
length O
Kwith O
binary O
values O
. O
3.2 O
Intra O
- O
modality O
Encoder O
Networks O
As O
shown O
in O
Figure O
2 O
, O
for O
each O
data O
sample O
, O
there O
are O
three O
sequences O
of O
length O
Tfrom O
the O
three O
modalities O
. O
For O
each O
modality O
, O
we O
use O
a O
bi O
- O
directional O
Long O
- O
Short O
Term O
Memory O
( O
LSTM O
) O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
net-271work O
as O
the O
encoder O
to O
process O
the O
sequence O
and O
get O
a O
vector O
representation O
. O
In O
other O
words O
, O
for O
the O
ith O
data O
sample O
, O
we O
will O
have O
three O
vectors O
, O
r(i O
) O
t‚ààRdt O
, O
r(i O
) O
a‚ààRda O
, O
andr(i O
) O
v‚ààRdv O
, O
that O
represent O
the O
textual O
, O
acoustic O
, O
and O
visual O
modalities O
. O
Here O
, O
dt O
, O
da O
, O
anddvare O
the O
dimensions O
of O
the O
emotion O
embeddings O
of O
the O
textual O
, O
acoustic O
, O
and O
visual O
modalities O
, O
respectively O
. O
3.3 O
Modality O
Mapping O
Module O
As O
mentioned O
in O
Section O
1 O
, O
previous O
works O
do O
not O
consider O
the O
connections O
in O
different O
emotion O
categories O
, O
and O
the O
only O
information O
about O
emotions O
is O
in O
the O
annotations O
. O
In O
our O
model O
, O
we O
use O
emotion O
word O
embeddings O
to O
inject O
the O
semantic O
information O
of O
emotions O
into O
the O
model O
. O
Additionally O
, O
emotion O
embeddings O
also O
contain O
the O
relationships O
between O
emotion O
categories O
. O
For O
the O
textual O
modality O
, O
we O
use O
pre O
- O
trained O
GloVe O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
embeddings O
of O
Kemotion O
words O
, O
denoted O
asEt‚ààRK√ódt O
. O
For O
the O
other O
two O
modalities O
, O
because O
there O
are O
no O
off O
- O
the O
- O
shelf O
pre O
- O
trained O
emotion O
embeddings O
, O
our O
model O
learns O
two O
mapping O
functions O
which O
project O
the O
vectors O
from O
the O
textual O
space O
into O
the O
acoustic O
and O
visual O
spaces O
: O
Ea O
= O
ft‚Üía(Et)‚ààRK√óda(1 O
) O
Ev O
= O
ft‚Üív(Et)‚ààRK√ódv O
. O
( O
2 O
) O
3.4 O
Modality O
Fusion O
and O
Prediction O
To O
predict O
the O
emotions O
for O
input O
sentences O
, O
we O
calculate O
the O
similarity O
scores O
between O
the O
sequence O
representation O
and O
the O
emotion O
embeddings O
for O
each O
modality O
. O
As O
shown O
in O
Eq.3 O
, O
for O
a O
data O
sample O
i O
, O
every O
modality O
will O
have O
a O
vector O
of O
similarity O
scores O
of O
length O
Kby O
dot O
product O
attention O
. O
We O
further O
add O
a O
modality O
fusion O
module O
to O
weighted O
sum O
all O
the O
vectors O
, O
in O
which O
the O
weights O
are O
also O
optimized O
end O
- O
to O
- O
end O
( O
Eq.4 O
) O
. O
Finally O
, O
as O
the O
datasets O
are O
multi O
- O
labelled O
, O
the O
sigmoid O
activation O
function O
is O
applied O
to O
each O
score O
in O
the O
fused O
vector O
s(i O
) O
, O
and O
a O
threshold O
is O
used O
to O
decide O
whether O
an O
emotion O
exists O
or O
not O
. O
s(i O
) O
t O
= O
Etr(i O
) O
t O
, O
s(i O
) O
a O
= O
Ear(i O
) O
a O
, O
s(i O
) O
v O
= O
Evr(i O
) O
v O
( O
3 O
) O
s(i)=Sigmoid O
( O
wts(i O
) O
t+was(i O
) O
a+wvs(i O
) O
v)(4 O
) O
4 O
Unseen O
Emotion O
Prediction O
Collecting O
numerous O
training O
samples O
for O
a O
new O
emotion O
, O
especially O
for O
a O
low O
- O
resource O
emotion O
, O
is O
expensive O
and O
time O
- O
consuming O
. O
Therefore O
, O
inthis O
section O
, O
we O
concentrate O
on O
the O
ability O
of O
our O
model O
to O
generalize O
to O
an O
unseen O
target O
emotion O
by O
considering O
the O
scenario O
where O
we O
have O
zero O
or O
only O
a O
few O
training O
samples O
in O
an O
unseen O
emotion O
. O
4.1 O
Zero O
- O
Shot O
Emotion O
Prediction O
Ideally O
, O
our O
model O
is O
able O
to O
directly O
adapt O
to O
a O
new O
emotion O
based O
on O
its O
embedding O
. O
Given O
a O
new O
text O
emotion O
embedding O
et O
k+1 O
, O
we O
can O
generate O
the O
visual O
and O
acoustic O
emotion O
embeddings O
ev O
k+1 O
andea O
k+1 O
, O
respectively O
, O
using O
the O
already O
learned O
mapping O
functions O
ft‚Üívandft‚Üía O
. O
After O
that O
, O
the O
similarity O
scores O
between O
the O
input O
sentence O
and O
the O
new O
emotion O
can O
be O
computed O
for O
each O
modality O
. O
4.2 O
Few O
- O
Shot O
Emotion O
Prediction O
In O
this O
section O
, O
we O
assume O
1 O
% O
of O
the O
positive O
training O
samples O
in O
a O
new O
emotion O
are O
available O
, O
and O
to O
balance O
the O
training O
samples O
, O
we O
take O
the O
same O
amount O
of O
negative O
training O
samples O
for O
the O
new O
emotion O
. O
However O
, O
the O
model O
could O
lose O
its O
ability O
to O
predict O
the O
original O
emotions O
when O
we O
simply O
Ô¨Åne O
- O
tune O
it O
on O
the O
training O
samples O
of O
a O
new O
emotion O
. O
To O
cope O
with O
this O
issue O
, O
we O
propose O
two O
Ô¨Ånetuning O
settings O
. O
First O
, O
after O
we O
obtain O
the O
trained O
model O
in O
the O
source O
emotions O
, O
we O
jointly O
train O
it O
with O
the O
training O
samples O
of O
the O
source O
emotions O
and O
the O
new O
target O
emotion O
. O
Second O
, O
we O
utilize O
a O
continual O
learning O
method O
, O
gradient O
episodic O
memory O
( O
GEM O
) O
( O
Lopez O
- O
Paz O
and O
Ranzato O
, O
2017 O
) O
, O
to O
prevent O
the O
catastrophic O
forgetting O
of O
previously O
learned O
knowledge O
. O
The O
purpose O
of O
using O
continual O
learning O
is O
that O
we O
do O
not O
need O
to O
retrain O
with O
all O
the O
data O
from O
previously O
learned O
emotions O
since O
the O
data O
might O
not O
be O
available O
. O
We O
describe O
the O
training O
process O
of O
GEM O
as O
follows O
: O
We O
deÔ¨Åne O
ŒòSas O
the O
model O
‚Äôs O
parameters O
trained O
in O
the O
source O
emotions O
, O
and O
Œòdenotes O
the O
current O
optimized O
parameters O
based O
on O
the O
target O
emotion O
data O
. O
GEM O
keeps O
a O
small O
number O
of O
samples O
N O
from O
the O
source O
emotions O
, O
and O
a O
constraint O
is O
applied O
on O
the O
gradient O
to O
prevent O
the O
loss O
on O
the O
stored O
samples O
from O
increasing O
when O
the O
model O
learns O
the O
new O
target O
emotion O
. O
The O
training O
process O
can O
be O
formulated O
as O
Minimize O
ŒòL(Œò O
) O
Subject O
to O
L(Œò O
, O
N)‚â§L(ŒòS O
, O
N O
) O
, O
where O
L(Œò O
, O
N)is O
the O
loss O
value O
of O
the O
N O
stored O
samples.2725 O
Experiments O
In O
this O
section O
, O
we O
Ô¨Årst O
introduce O
the O
two O
public O
datasets O
we O
use O
and O
data O
feature O
extraction O
. O
Then O
, O
we O
discuss O
our O
evaluation O
metrics O
, O
including O
their O
advantages O
and O
defects O
. O
Finally O
, O
we O
introduce O
the O
baselines O
and O
our O
experimental O
settings O
. O
5.1 O
Datasets O
CMU O
- O
MOSEI O
CMU O
Multimodal O
Opinion O
Sentiment O
and O
Emotion O
Intensity O
( O
CMUMOSEI O
) O
( O
Zadeh O
et O
al O
. O
, O
2018b O
) O
is O
currently O
the O
largest O
public O
dataset O
for O
multi O
- O
modal O
sentiment O
analysis O
and O
emotion O
recognition O
. O
It O
comprises O
23,453 O
annotated O
data O
samples O
extracted O
from O
3228 O
videos O
. O
For O
emotion O
recognition O
, O
it O
consists O
of O
six O
basic O
categories O
: O
anger O
, O
disgust O
, O
fear O
, O
happy O
, O
sad O
, O
andsurprise O
. O
For O
zero O
- O
shot O
and O
few O
- O
shot O
learning O
evaluation O
, O
we O
use O
four O
relatively O
low O
- O
resource O
categories O
among O
them O
( O
anger O
, O
disgust O
, O
fear O
, O
surprise O
) O
. O
The O
model O
is O
trained O
on O
the O
other O
Ô¨Åve O
categories O
when O
evaluating O
one O
zero O
- O
shot O
category O
. O
A O
detailed O
statistical O
table O
about O
these O
categories O
is O
included O
in O
Appendix O
A. O
IEMOCAP O
The O
Interactive O
Emotional O
Dyadic O
Motion O
Capture O
( O
IEMOCAP O
) O
( O
Busso O
et O
al O
. O
, O
2008 O
) O
dataset O
was O
created O
for O
multi O
- O
modal O
human O
emotion O
analysis O
, O
and O
was O
collected O
from O
dialogues O
performed O
by O
ten O
actors O
. O
It O
is O
also O
a O
multi O
- O
labelled O
emotion O
recognition O
dataset O
which O
contains O
nine O
emotion O
categories O
. O
For O
comparison O
with O
prior O
works O
( O
Wang O
et O
al O
. O
, O
2018 O
; O
Liang O
et O
al O
. O
, O
2018 O
; O
Pham O
et O
al O
. O
, O
2018 O
; O
Tsai O
et O
al O
. O
, O
2019a O
) O
where O
four O
( O
out O
of O
the O
nine O
) O
emotion O
categories O
are O
selected O
for O
training O
and O
evaluating O
the O
models O
, O
we O
also O
follow O
the O
same O
four O
categories O
, O
namely O
, O
happy O
, O
sad O
, O
angry O
, O
and O
neutral O
, O
to O
train O
our O
model O
. O
For O
zero O
- O
shot O
learning O
evaluation O
, O
we O
consider O
three O
low O
- O
resource O
categories O
from O
the O
remaining O
Ô¨Åve O
, O
namely O
, O
excited O
, O
surprised O
, O
and O
frustrated O
, O
as O
unseen O
emotions O
. O
5.2 O
Data O
Feature O
Extraction O
We O
use O
CMU O
- O
Multimodal O
SDK O
( O
Zadeh O
et O
al O
. O
, O
2018c O
) O
for O
downloading O
and O
pre O
- O
processing O
the O
datasets O
. O
It O
helps O
to O
do O
data O
alignment O
and O
earlystage O
feature O
extraction O
for O
each O
modality O
. O
The O
textual O
data O
is O
tokenized O
in O
word O
level O
and O
represented O
using O
GloVe O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
embeddings O
. O
Facial O
action O
units O
are O
extracted O
by O
the O
Facet O
( O
iMotions O
, O
2017 O
) O
to O
indicate O
muscle O
movements O
and O
expressions O
( O
Ekman O
et O
al O
. O
, O
1980 O
) O
. O
These O
are O
a O
commonly O
used O
type O
of O
feature O
for O
facial O
expressionrecognition O
( O
Fan O
et O
al O
. O
, O
2020 O
) O
. O
For O
acoustic O
data O
, O
COV O
AREP O
( O
Degottex O
et O
al O
. O
, O
2014 O
) O
is O
used O
to O
extract O
fundamental O
features O
, O
such O
as O
mel O
- O
frequency O
cepstral O
coefÔ¨Åcients O
( O
MFCCs O
) O
, O
pitch O
tracking O
, O
glottal O
source O
parameters O
, O
etc O
. O
5.3 O
Evaluation O
Metrics O
Weighted O
Accuracy O
Due O
to O
the O
imbalanced O
nature O
of O
the O
emotion O
recognition O
dataset O
( O
for O
each O
emotion O
category O
, O
there O
are O
many O
more O
negative O
samples O
than O
positive O
samples O
) O
, O
we O
use O
binary O
weighted O
accuracy O
( O
Tong O
et O
al O
. O
, O
2017 O
) O
on O
each O
category O
to O
better O
measure O
the O
model O
‚Äôs O
performance O
. O
The O
formula O
is O
Weighted O
Acc O
. O
= O
TP√óN O
/ O
P O
+ O
TN O
2N O
where O
P O
means O
total O
positive O
, O
TP O
true O
positive O
, O
N O
total O
negative O
, O
and O
TN O
true O
negative O
. O
Weighted O
F1 O
In O
prior O
works O
( O
Zadeh O
et O
al O
. O
, O
2018b O
; O
Akhtar O
et O
al O
. O
, O
2019 O
; O
Tsai O
et O
al O
. O
, O
2019a O
) O
, O
the O
binary O
weighted O
F1 O
score O
metric O
is O
used O
on O
the O
CMUMOSEI O
dataset O
, O
and O
its O
formula O
is O
shown O
in O
Eq.5 O
. O
Weighted O
F1 O
= O
P O
I√óF1p+N O
I√óF1n(5 O
) O
Here O
, O
F1pis O
the O
F1 O
score O
that O
treats O
positive O
samples O
as O
positive O
, O
while O
F1ntreats O
negative O
samples O
aspositive O
, O
and O
they O
are O
weighted O
by O
their O
portion O
of O
the O
data O
. O
However O
, O
there O
is O
one O
defect O
of O
using O
binary O
weighted O
F1 O
in O
this O
task O
. O
As O
there O
are O
many O
more O
negative O
samples O
than O
positive O
ones O
, O
we O
Ô¨Ånd O
that O
with O
the O
increase O
of O
the O
threshold O
, O
the O
weighted O
F1 O
score O
will O
also O
increase O
because O
the O
true O
negativeincreases O
. O
Therefore O
, O
in O
this O
paper O
, O
we O
do O
not O
report O
this O
metric O
. O
A O
detailed O
analysis O
of O
this O
is O
given O
in O
Appendix O
B. O
AUC O
Score O
To O
eliminate O
the O
effect O
of O
threshold O
and O
mitigate O
the O
defect O
of O
the O
weighted O
F1 O
score O
, O
we O
also O
report O
Area O
under O
the O
ROC O
Curve O
( O
AUC O
) O
scores O
. O
The O
AUC O
score O
considers O
classiÔ¨Åcation O
performance O
on O
both O
positive O
and O
negative O
samples O
, O
and O
it O
is O
scale- O
and O
threshold O
- O
invariant O
. O
5.4 O
Baselines O
For O
both O
the O
CMU O
- O
MOSEI O
and O
IEMOCAP O
datasets O
, O
we O
use O
Early O
Fusion O
LSTM O
( O
EF O
- O
LSTM O
) O
and O
Late O
Fusion O
LSTM O
( O
LF O
- O
LSTM O
) O
as O
two O
baseline O
models O
. O
Additionally O
, O
for O
CMU O
- O
MOSEI O
, O
the O
Graph O
Memory O
Fusion O
Network O
( O
Graph O
- O
MFN O
) O
( O
Zadeh O
et O
al O
. O
, O
2018b O
) O
and O
a O
multi O
- O
task O
learning O
( O
MTL)273Emotion O
Anger O
Disgust O
Fear O
Happy O
Sad O
Surprise O
Average O
Metrics O
W O
- O
Acc O
AUC O
W O
- O
Acc O
AUC O
W O
- O
Acc O
AUC O
W O
- O
Acc O
AUC O
W O
- O
Acc O
AUC O
W O
- O
Acc O
AUC O
W O
- O
Acc O
AUC O
EF O
- O
LSTM O
58.5 O
62.2 O
59.9 O
63.9 O
50.1 O
69.8 O
65.1 O
68.9 O
55.1 O
58.6 O
50.6 O
54.3 O
56.6 O
63.0 O
LF O
- O
LSTM O
57.7 O
66.5 O
61.0 O
71.9 O
50.7 O
61.1 O
63.9 O
68.6 O
54.3 O
59.6 O
51.4 O
61.5 O
56.5 O
64.9 O
Graph O
- O
MFN O
62.6 O
- O
69.1 O
- O
62.0 O
- O
66.3 O
- O
60.4 O
- O
53.7 O
- O
62.3 O
MTL O
66.8 O
68.0‚Ä†72.7 O
76.7‚Ä†62.2 O
42.9‚Ä†53.6 O
71.4‚Ä†61.4 O
57.6‚Ä†60.6 O
65.1‚Ä†62.8 O
63.6‚Ä† O
Ours O
67.0 O
71.7 O
72.5 O
78.3 O
65.4 O
71.6 O
67.9 O
73.9 O
62.6 O
66.7 O
62.1 O
66.4 O
66.2 O
71.4 O
Table O
1 O
: O
Results O
of O
multi O
- O
modal O
emotion O
recognition O
on O
the O
CMU O
- O
MOSEI O
dataset O
. O
Baselines O
( O
EF O
- O
LSTM O
, O
LFLSTM O
) O
and O
previous O
state O
- O
of O
- O
the O
- O
art O
models O
( O
Graph O
- O
MFN O
( O
Zadeh O
et O
al O
. O
, O
2018b O
) O
, O
MTL O
( O
Akhtar O
et O
al O
. O
, O
2019 O
) O
) O
are O
compared O
. O
Results O
marked O
by‚Ä†are O
re O
- O
run O
and O
Ô¨Åne O
- O
tuned O
by O
us O
as O
they O
are O
not O
reported O
in O
the O
original O
paper O
. O
Emotion O
Happy O
Sad O
Angry O
Neutral O
Metrics O
Acc O
AUC O
Acc O
AUC O
Acc O
AUC O
Acc O
AUC O
EF O
- O
LSTM O
85.8 O
70.7 O
83.7 O
85.8 O
75.8 O
90.3 O
67.1 O
74.1 O
LF O
- O
LSTM O
85.2 O
71.7 O
83.4 O
84.4 O
79.5 O
86.8 O
66.5 O
72.2 O
RMFN O
( O
Liang O
et O
al O
. O
, O
2018 O
) O
87.5 O
- O
83.8 O
- O
85.1 O
- O
69.5 O
RA O
VEN O
( O
Wang O
et O
al O
. O
, O
2018 O
) O
87.3 O
- O
83.4 O
- O
87.3 O
- O
69.7 O
MCTN O
( O
Pham O
et O
al O
. O
, O
2018 O
) O
84.9 O
- O
80.5 O
- O
79.7 O
- O
62.3 O
MulT O
( O
Tsai O
et O
al O
. O
, O
2019a O
) O
83.5‚Ä†71.2‚Ä†85.0‚Ä†89.3‚Ä†85.5‚Ä†92.4‚Ä†71.0‚Ä†77.2‚Ä† O
Ours O
85.0 O
74.2 O
86.6 O
88.4 O
88.1 O
93.2 O
71.1 O
76.7 O
Table O
2 O
: O
Multi O
- O
modal O
emotion O
recognition O
results O
on O
IEMOCAP O
. O
We O
re O
- O
run O
MulT O
( O
marked O
by‚Ä† O
) O
with O
its O
reported O
best O
hyper O
- O
parameters O
to O
get O
the O
AUC O
scores O
. O
model O
( O
Akhtar O
et O
al O
. O
, O
2019 O
) O
are O
included O
for O
comparison O
with O
previous O
state O
- O
of O
- O
the O
- O
art O
models O
. O
For O
IEMOCAP O
, O
the O
Recurrent O
Multistage O
Fusion O
Network O
( O
RMFN O
) O
( O
Liang O
et O
al O
. O
, O
2018 O
) O
, O
Recurrent O
Attended O
Variation O
Embedding O
Network O
( O
RA O
VEN O
) O
( O
Wang O
et O
al O
. O
, O
2018 O
) O
, O
and O
the O
Multimodal O
Transformer O
( O
MulT O
) O
( O
Tsai O
et O
al O
. O
, O
2019a O
) O
are O
included O
. O
To O
compare O
the O
AUC O
scores O
and O
zeroshot O
performance O
with O
baselines O
, O
we O
re O
- O
run O
the O
MTL O
and O
MulT O
models O
based O
on O
their O
reported O
best O
hyper O
- O
parameters O
, O
and O
we O
also O
carry O
out O
hyperparameter O
search O
for O
a O
fair O
comparison O
. O
CMU O
- O
MOSEI O
IEMOCAP O
Best O
Epoch O
15 O
16 O
Batch O
size O
512 O
32 O
Learning O
rate O
1e-4 O
1e-3 O
# O
LSTM O
layers O
2 O
2 O
Hidden O
Size O
300/200/100 O
300/200/100 O
Dropout O
0.15 O
0.15 O
Gradient O
Clip O
10.0 O
1.0 O
Random O
Seed O
0 O
0 O
Table O
3 O
: O
The O
hyper O
- O
parameters O
of O
our O
best O
models O
. O
The O
hidden O
size O
means O
the O
size O
of O
the O
LSTM O
hidden O
state O
of O
the O
textual O
/ O
acoustic O
/ O
visual O
modality O
, O
respectively O
. O
5.5 O
Training O
Details O
The O
model O
is O
trained O
end O
- O
to O
- O
end O
with O
the O
Adam O
optimizer O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
and O
a O
scheduler O
that O
will O
reduce O
the O
learning O
rate O
by O
a O
factor O
of O
0.1 O
when O
the O
optimization O
stays O
on O
a O
plateau O
for O
more O
than O
5 O
epochs O
. O
The O
best O
hyper O
- O
parameters O
inour O
training O
for O
both O
datasets O
are O
shown O
in O
Table O
3 O
. O
Also O
, O
we O
use O
the O
largest O
GloVe O
word O
embeddings O
( O
glove.840B.300d2 O
) O
for O
both O
the O
input O
text O
data O
and O
the O
emotion O
embeddings O
in O
the O
textual O
modality O
. O
The O
weights O
of O
the O
textual O
embeddings O
are O
frozen O
during O
training O
to O
keep O
the O
pre O
- O
trained O
relations O
, O
which O
is O
also O
essential O
for O
doing O
zero O
- O
shot O
learning O
. O
6 O
Analysis O
6.1 O
Results O
Table O
1 O
shows O
our O
model O
‚Äôs O
performance O
on O
the O
CMU O
- O
MOSEI O
dataset O
. O
Compared O
to O
existing O
baselines O
, O
our O
model O
surpasses O
them O
by O
a O
large O
margin O
. O
The O
weighted O
accuracy O
( O
W O
- O
Acc O
) O
and O
AUC O
score O
are O
used O
for O
evaluation O
, O
with O
a O
threshold O
set O
to O
0.5 O
to O
calculate O
the O
W O
- O
Acc O
. O
As O
discussed O
in O
Section O
5.3 O
, O
we O
do O
not O
follow O
the O
previous O
papers O
in O
using O
the O
weighted O
F1 O
- O
score O
( O
W O
- O
F1 O
) O
because O
it O
does O
not O
provide O
an O
effective O
evaluation O
when O
the O
dataset O
is O
very O
imbalanced O
. O
For O
example O
, O
the O
weakest O
baseline O
, O
EF O
- O
LSTM O
, O
can O
even O
achieve O
90 O
% O
W O
- O
F1 O
by O
predicting O
almost O
all O
samples O
as O
negative O
. O
More O
plots O
and O
analysis O
of O
this O
defect O
of O
W O
- O
F1 O
are O
included O
in O
Appendix O
B. O
We O
further O
test O
our O
model O
on O
a O
second O
dataset O
called O
IEMOCAP O
, O
and O
the O
results O
are O
shown O
in O
Table O
2 O
. O
Similarly O
, O
our O
model O
achieves O
better O
results O
on O
most O
emotion O
categories O
, O
except O
happy O
. O
For O
a O
2https://nlp.stanford.edu/projects/ O
glove/274Metrics O
W O
- O
Acc O
AUC O
W O
- O
Acc O
AUC O
W O
- O
Acc O
AUC O
W O
- O
Acc O
AUC O
Unseen O
emotion O
Anger O
( O
unseen O
) O
Disgust O
( O
unseen O
) O
Fear O
( O
unseen O
) O
Surprise O
( O
unseen O
) O
Zero O
- O
ShotEF O
- O
LSTM O
50.6 O
50.9 O
50.3 O
48.2 O
45.8 O
42.3 O
50.2 O
46.9 O
LF O
- O
LSTM O
48.4 O
49.2 O
49.7 O
44.2 O
47.4 O
47.3 O
48.6 O
48.3 O
Ours O
55.9 O
61.6 O
67.5 O
72.7 O
41.8 O
40.6 O
53.4 O
55.5 O
1 O
% O
Few O
- O
ShotFT O
( O
Ours O
) O
58.9 O
61.9 O
67.9 O
71.5 O
43.1 O
43.1 O
51.8 O
53.9 O
CL O
( O
Ours O
) O
58.9 O
61.5 O
68.7 O
72.8 O
42.6 O
42.7 O
50.6 O
52.5 O
JT O
( O
Ours O
) O
59.0 O
61.1 O
69.2 O
74.2 O
41.9 O
41.7 O
55.2 O
58.1 O
Average O
on O
all O
categories O
Except O
Anger O
Except O
Disgust O
Except O
Fear O
Except O
Surprise O
Zero O
- O
shot O
Ours O
65.6 O
70.6 O
64.4 O
69.3 O
65.9 O
70.9 O
67.2 O
71.4 O
1 O
% O
Few O
- O
ShotFT O
( O
Ours O
) O
64.4 O
69.8 O
63.7 O
68.5 O
65.4 O
70.7 O
65.1 O
71.4 O
CL O
( O
Ours O
) O
64.6 O
69.8 O
63.8 O
68.9 O
65.6 O
70.9 O
65.5 O
71.5 O
JT O
( O
Ours O
) O
64.3 O
69.3 O
63.5 O
68.8 O
65.9 O
70.8 O
66.1 O
71.5 O
Table O
4 O
: O
Zero O
/ O
few O
- O
shot O
results O
on O
low O
- O
resource O
emotion O
categories O
in O
CMU O
- O
MOSEI O
dataset O
. O
Here O
, O
FT O
, O
CL O
, O
and O
JT O
stand O
for O
Fine O
- O
Tuning O
, O
Continual O
Learning O
, O
and O
Joint O
Training O
respectively O
. O
FT O
directly O
Ô¨Åne O
- O
tunes O
the O
trained O
model O
on O
the O
unseen O
emotions O
, O
and O
CL O
and O
JT O
are O
two O
different O
settings O
introduced O
in O
Section O
4.2 O
. O
Note O
that O
in O
the O
few O
- O
shot O
settings O
, O
we O
select O
the O
model O
based O
on O
the O
average O
performance O
of O
all O
emotions O
( O
including O
the O
unseen O
emotion O
) O
to O
ensure O
good O
overall O
performance O
of O
our O
model O
. O
Figure O
3 O
: O
Euclidean O
distances O
between O
different O
emotion O
embeddings O
in O
the O
textual O
, O
acoustic O
, O
and O
visual O
spaces O
. O
Although O
the O
absolute O
values O
are O
different O
, O
the O
relative O
distances O
between O
emotion O
categories O
are O
well O
reserved O
. O
This O
indicates O
that O
the O
two O
mapping O
functions O
ft‚Üívandft‚Üíatransfer O
the O
relationships O
of O
emotion O
categories O
well O
. O
fair O
comparison O
on O
IEMOCAP O
, O
we O
use O
accuracy O
instead O
of O
W O
- O
Acc O
, O
following O
the O
previous O
works O
compared O
in O
the O
table O
. O
6.2 O
Effects O
of O
Emotion O
Embeddings O
Quantitatively O
, O
our O
model O
makes O
a O
large O
improvement O
in O
the O
multi O
- O
modal O
emotion O
recognition O
task O
. O
We O
think O
it O
beneÔ¨Åts O
greatly O
from O
the O
emotion O
embeddings O
, O
which O
can O
model O
the O
relationships O
( O
or O
distances O
) O
between O
emotion O
categories O
. O
This O
is O
especially O
important O
for O
emotion O
recognition O
, O
which O
is O
a O
multi O
- O
label O
task O
by O
nature O
, O
as O
people O
can O
have O
multiple O
emotions O
at O
the O
same O
time O
. O
For O
example O
, O
if O
a O
person O
is O
surprised O
, O
it O
is O
more O
likely O
that O
this O
person O
is O
also O
happy O
andexcited O
and O
is O
less O
likely O
to O
bedisgusted O
orsad O
. O
This O
kind O
of O
information O
is O
expected O
to O
be O
modelled O
and O
captured O
by O
emotion O
embeddings O
. O
Intuitively O
, O
in O
the O
textual O
space O
, O
related O
emotions O
( O
e.g. O
, O
angry O
anddisgusted O
) O
tend O
to O
have O
closer O
word O
vectors O
than O
unrelated O
emotions O
( O
angry O
andhappy O
) O
. O
To O
ensure O
the O
effectiveness O
of O
word O
embeddings O
, O
for O
each O
emotion O
word O
, O
we O
investi O
- O
gated O
multiple O
forms O
of O
it O
. O
For O
example O
, O
for O
surprised O
, O
we O
also O
tried O
with O
Surprised O
, O
( O
S O
/ O
s)urprising O
, O
( O
S O
/ O
s)urprise O
. O
Generally O
, O
they O
all O
show O
a O
similar O
trend O
, O
and O
in O
most O
cases O
, O
the O
word O
form O
that O
is O
used O
to O
describe O
human O
shows O
the O
best O
results O
. O
In O
our O
Ô¨Ånal O
setting O
, O
we O
iterate O
and O
pick O
the O
best O
performing O
form O
for O
each O
emotion O
category O
. O
Moreover O
, O
our O
model O
can O
transfer O
the O
relationship O
of O
emotion O
categories O
from O
the O
textual O
space O
to O
the O
acoustic O
and O
visual O
spaces O
using O
end O
- O
to O
- O
end O
optimized O
mapping O
functions O
. O
In O
Figure O
3 O
, O
we O
show O
the O
Euclidean O
distances O
of O
emotion O
embeddings O
between O
categories O
. O
The O
relative O
positions O
are O
preserved O
very O
well O
after O
being O
transferred O
from O
the O
textual O
space O
to O
the O
visual O
and O
acoustic O
spaces O
. O
This O
indicates O
that O
the O
learned O
mapping O
functions O
( O
ft‚Üív O
andft‚Üía O
) O
are O
effective O
. O
Although O
it O
is O
not O
the O
main O
focus O
of O
this O
paper O
, O
we O
think O
improving O
the O
pre O
- O
trained O
textual O
emotion O
embeddings O
is O
an O
essential O
direction O
for O
future O
work O
. O
It O
can O
beneÔ¨Åt O
all O
modalities O
and O
further O
enhance O
the O
overall O
performance O
. O
For O
example O
, O
incorporate O
semantic O
emotion275information O
( O
Xu O
et O
al O
. O
, O
2018 O
) O
to O
the O
original O
word O
embeddings O
. O
6.3 O
Zero O
/ O
Few O
- O
Shot O
Results O
BeneÔ¨Åting O
from O
the O
pre O
- O
trained O
textual O
emotion O
embeddings O
and O
learned O
mapping O
functions O
, O
our O
model O
can O
recognize O
unseen O
emotion O
categories O
to O
a O
certain O
extent O
. O
We O
evaluate O
our O
model O
‚Äôs O
zero O
- O
shot O
learning O
ability O
on O
the O
low O
- O
resource O
categories O
in O
CMU O
- O
MOSEI O
( O
shown O
in O
Table O
4 O
) O
and O
IEMOCAP O
( O
shown O
in O
Table O
5 O
) O
. O
For O
a O
fair O
comparison O
, O
we O
use O
the O
same O
training O
setting O
that O
is O
used O
in O
Table O
1 O
. O
This O
can O
ensure O
that O
no O
downgrade O
happens O
on O
the O
seen O
emotions O
, O
and O
the O
model O
is O
not O
selected O
to O
overÔ¨Åt O
a O
single O
unseen O
category O
. O
As O
we O
can O
see O
, O
the O
zero O
- O
shot O
results O
of O
the O
baselines O
are O
similar O
to O
random O
guesses O
, O
because O
the O
weights O
related O
to O
that O
unseen O
emotion O
in O
the O
model O
are O
randomly O
initialized O
and O
have O
never O
been O
optimized O
. O
For O
our O
model O
, O
the O
zero O
- O
shot O
performance O
is O
much O
better O
than O
that O
of O
the O
baselines O
in O
almost O
all O
emotions O
. O
This O
is O
because O
our O
model O
learns O
to O
classify O
emotion O
categories O
based O
on O
the O
similarity O
between O
the O
sentence O
representation O
and O
emotion O
embeddings O
, O
which O
enables O
our O
model O
better O
generalization O
ability O
to O
other O
unseen O
emotions O
since O
emotion O
embeddings O
contain O
semantic O
information O
in O
the O
vector O
space O
. O
Furthermore O
, O
we O
perform O
few O
- O
shot O
learning O
using O
only O
1 O
% O
of O
data O
of O
these O
low O
- O
resource O
categories O
. O
As O
we O
can O
see O
from O
Table O
4 O
, O
using O
very O
few O
training O
samples O
, O
our O
model O
can O
adapt O
to O
unseen O
emotions O
without O
losing O
the O
performance O
in O
the O
source O
emotions O
. O
In O
addition O
, O
we O
observe O
that O
simply O
Ô¨Åne O
- O
tuning O
( O
FT O
) O
our O
trained O
model O
sometimes O
obtains O
inferior O
performance O
. O
This O
is O
because O
our O
model O
will O
gradually O
lose O
the O
ability O
to O
classify O
the O
source O
emotions O
, O
and O
we O
have O
to O
early O
stop O
the O
Ô¨Åne O
- O
tuning O
process O
, O
which O
leads O
to O
inferior O
performance O
. O
We O
can O
see O
that O
CL O
and O
JT O
prevent O
our O
model O
from O
catastrophic O
forgetting O
and O
improve O
the O
few O
- O
shot O
performance O
in O
the O
unseen O
emotion O
. O
Moreover O
, O
JT O
achieves O
slightly O
better O
performance O
than O
CL O
. O
This O
can O
be O
attributed O
to O
the O
fact O
that O
CL O
might O
still O
result O
in O
performance O
drops O
in O
source O
emotions O
since O
our O
model O
only O
observes O
partial O
samples O
from O
them O
. O
At O
the O
same O
time O
, O
JT O
directly O
optimizes O
the O
model O
on O
the O
data O
samples O
of O
such O
emotions O
. O
Unseen O
emotionExcited O
( O
unseen)Surprised O
( O
unseen)Frustrated O
( O
unseen O
) O
Metrics O
Acc O
F1 O
Acc O
F1 O
Acc O
F1 O
EF O
- O
LSTM O
13.1 O
23.1 O
11.3 O
5.1 O
22.9 O
37.2 O
LF O
- O
LSTM O
14.0 O
23.3 O
2.6 O
5.1 O
23.7 O
37.4 O
MulT O
45.1 O
27.3 O
41.4 O
7.5 O
48.7 O
40.9 O
Ours O
( O
TA O
V O
) O
82.0 O
56.1 O
78.8 O
13.1 O
73.6 O
57.9 O
Ours O
( O
TA O
) O
79.9 O
52.7 O
79.3 O
14.4 O
75.1 O
60.1 O
Ours O
( O
TV O
) O
75.9 O
42.7 O
58.6 O
9.1 O
54.1 O
13.6 O
Ours O
( O
A O
V O
) O
89.1 O
69.9 O
65.7 O
13.2 O
83.9 O
73.6 O
Ours O
( O
T O
) O
72.9 O
37.1 O
67.7 O
3.1 O
55.3 O
9.0 O
Ours O
( O
A O
) O
76.9 O
52.8 O
82.1 O
16.8 O
86.1 O
74.8 O
Ours O
( O
V O
) O
82.1 O
35.0 O
81.1 O
6.2 O
68.6 O
44.6 O
Table O
5 O
: O
Zero O
- O
shot O
results O
on O
the O
IEMOCAP O
dataset O
. O
T O
( O
textual O
) O
, O
A O
( O
acoustic O
) O
, O
and O
V O
( O
visual O
) O
indicate O
the O
existence O
of O
that O
modality O
during O
inference O
time O
. O
Metric O
W O
- O
Acc O
Emotion O
Anger O
Disgust O
Fear O
Happy O
Sad O
Surprise O
T+A+V O
67.0 O
72.5 O
65.4 O
67.9 O
62.6 O
62.1 O
T+A O
65.0 O
71.9 O
64.8 O
66.0 O
63.0 O
59.9 O
T+V O
64.9 O
71.2 O
66.7 O
67.6 O
61.0 O
60.4 O
A+V O
63.8 O
71.1 O
65.5 O
64.5 O
61.3 O
55.2 O
Only O
T O
61.5 O
69.0 O
64.3 O
64.2 O
59.7 O
61.2 O
Only O
A O
61.9 O
71.5 O
66.9 O
62.7 O
61.0 O
54.8 O
Only O
V O
63.4 O
69.7 O
63.2 O
63.2 O
58.5 O
53.3 O
Table O
6 O
: O
Ablation O
study O
on O
CMU O
- O
MOSEI O
dataset O
. O
Different O
combinations O
of O
subsets O
of O
modalities O
are O
used O
. O
6.4 O
Ablation O
Study O
To O
further O
investigate O
how O
each O
individual O
modality O
inÔ¨Çuences O
the O
model O
, O
we O
perform O
comprehensive O
ablation O
studies O
on O
supervised O
multi O
- O
modal O
emotion O
recognition O
and O
also O
zero O
- O
shot O
prediction O
. O
In O
Table O
6 O
, O
we O
enumerate O
different O
subsets O
of O
the O
( O
textual O
, O
acoustic O
, O
visual O
) O
modalities O
to O
evaluate O
the O
effect O
of O
each O
single O
modality O
. O
Generally O
, O
the O
performance O
will O
increase O
if O
more O
modalities O
are O
available O
. O
Compared O
to O
single O
- O
modal O
data O
, O
multimodal O
data O
can O
provide O
supplementary O
information O
, O
which O
leads O
to O
more O
accurate O
emotion O
recognition O
. O
In O
terms O
of O
a O
single O
modality O
, O
we O
Ô¨Ånd O
that O
textual O
andacoustic O
are O
more O
effective O
than O
visual O
. O
Similarly O
, O
in O
Table O
5 O
, O
we O
show O
the O
zero O
- O
shot O
performance O
with O
different O
combinations O
of O
modalities O
during O
the O
inference O
time O
( O
all O
modalities O
exist O
in O
the O
training O
phase O
) O
. O
As O
there O
are O
many O
more O
negative O
samples O
than O
positive O
ones O
in O
the O
ZSL O
setting O
, O
we O
also O
evaluate O
the O
models O
with O
the O
unweighted O
F1 O
score O
. O
Because O
if O
a O
model O
has O
high O
accuracy O
but O
a O
low O
F1 O
, O
it O
is O
heavily O
biased O
to O
the O
negative O
samples O
so O
it O
can O
not O
do O
classiÔ¨Åcation O
effectively O
. O
Empirical O
results O
indicate O
that O
zero O
- O
shot O
on O
only O
one O
modality O
is O
possible O
. O
Moreover O
, O
if O
the O
data O
of O
an O
emotion O
category O
has O
strong O
characteristics O
in276one O
modality O
and O
is O
ambiguous O
in O
other O
modalities O
, O
single O
- O
modality O
can O
even O
surpass O
multi O
- O
modality O
on O
zero O
- O
shot O
prediction O
. O
For O
example O
, O
the O
performance O
of O
single O
- O
modality O
zero O
- O
shot O
prediction O
using O
the O
acoustic O
modality O
on O
the O
surprised O
category O
is O
better O
than O
using O
all O
modalities O
. O
7 O
Conclusion O
In O
this O
paper O
, O
we O
introduce O
a O
modality O
- O
transferable O
model O
that O
leverages O
cross O
- O
modality O
emotion O
embeddings O
for O
multi O
- O
modal O
emotion O
recognition O
. O
It O
makes O
predictions O
by O
measuring O
the O
distances O
between O
input O
data O
and O
target O
emotion O
categories O
, O
which O
is O
especially O
effective O
for O
a O
multi O
- O
label O
problem O
. O
The O
model O
also O
learns O
two O
mapping O
functions O
to O
transfer O
pre O
- O
trained O
textual O
emotion O
embeddings O
to O
acoustic O
and O
visual O
spaces O
. O
The O
empirical O
results O
demonstrate O
that O
it O
exhibits O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
most O
of O
the O
categories O
. O
Enabled O
by O
the O
utilization O
of O
emotion O
embeddings O
, O
our O
model O
can O
carry O
out O
zero O
- O
shot O
learning O
for O
unseen O
emotion O
categories O
and O
can O
quickly O
adapt O
few O
- O
shot O
learning O
without O
downgrading O
trained O
categories O
. O
Acknowledgement O
This O
work O
is O
funded O
by O
MRP/055/18 O
of O
the O
Innovation O
Technology O
Commission O
, O
the O
Hong O
Kong O
SAR O
Government O
. O
Abstract O
In O
this O
paper O
, O
we O
aim O
at O
learning O
the O
relationships O
and O
similarities O
of O
a O
variety O
of O
tasks O
, O
such O
as O
humour O
detection O
, O
sarcasm O
detection O
, O
offensive O
content O
detection O
, O
motivational O
content O
detection O
and O
sentiment O
analysis O
on O
a O
somewhat O
complicated O
form O
of O
information O
, O
i.e. O
,memes O
. O
We O
propose O
a O
multi O
- O
task O
, O
multi O
- O
modal O
deep O
learning O
framework O
to O
solve O
multiple O
tasks O
simultaneously O
. O
For O
multi O
- O
tasking O
, O
we O
propose O
two O
attention O
- O
like O
mechanisms O
viz O
. O
, O
Inter O
- O
task O
Relationship O
Module O
( O
iTRM O
) O
and O
Inter O
- O
class O
Relationship O
Module O
( O
iCRM O
) O
. O
The O
main O
motivation O
of O
iTRM O
is O
to O
learn O
the O
relationship O
between O
the O
tasks O
to O
realize O
how O
they O
help O
each O
other O
. O
In O
contrast O
, O
iCRM O
develops O
relations O
between O
the O
different O
classes O
of O
tasks O
. O
Finally O
, O
representations O
from O
both O
the O
attentions O
are O
concatenated O
and O
shared O
across O
the O
Ô¨Åve O
tasks O
( O
i.e. O
, O
humour O
, O
sarcasm O
, O
offensive O
, O
motivational O
, O
and O
sentiment O
) O
for O
multi O
- O
tasking O
. O
We O
use O
the O
recently O
released O
dataset O
in O
the O
Memotion O
Analysis O
task O
@ O
SemEval O
2020 O
, O
which O
consists O
of O
memes O
annotated O
for O
the O
classes O
as O
mentioned O
above O
. O
Empirical O
results O
on O
Memotion O
dataset O
show O
the O
efÔ¨Åcacy O
of O
our O
proposed O
approach O
over O
the O
existing O
state O
- O
of O
- O
theart O
systems O
( O
Baseline O
and O
SemEval O
2020 O
winner O
) O
. O
The O
evaluation O
also O
indicates O
that O
the O
proposed O
multi O
- O
task O
framework O
yields O
better O
performance O
over O
the O
single O
- O
task O
learning O
. O
1 O
Introduction O
The O
content O
and O
form O
of O
content O
shared O
on O
online O
social O
media O
platforms O
have O
changed O
rapidly O
over O
time O
. O
Currently O
, O
one O
of O
the O
most O
popular O
forms O
of O
media O
shared O
on O
such O
platforms O
is O
‚Äô O
Memes O
‚Äô O
. O
According O
to O
its O
deÔ¨Ånition O
from O
Oxford O
Dictionary O
, O
a O
meme O
is O
a O
piece O
of O
data O
, O
often O
in O
the O
form O
of O
images O
, O
text O
or O
videos O
that O
carry O
cultural O
information O
through O
an O
imitable O
phenomenon O
with O
a O
mimicked O
theme O
, O
that O
is O
shared O
( O
sometimes O
with O
slight O
modiÔ¨Åcation O
) O
rapidly O
by O
internet O
users O
. O
Every O
meme O
can O
be O
associated O
with O
Ô¨Åve O
affect O
values O
, O
namely O
humour O
( O
Hu O
) O
, O
sarcastic O
( O
Sar O
) O
, O
offensive O
( O
Off O
) O
, O
motivational O
( O
Mo O
) O
, O
and O
sentiment O
( O
Sent O
) O
. O
Hence O
, O
in O
a O
broad O
sense O
, O
memes O
can O
be O
categorized O
into O
four O
intersecting O
setsviz.humorous O
memes O
, O
sarcastic O
memes O
, O
offensive O
memes O
, O
and O
motivational O
memes O
. O
Humour O
refers O
to O
the O
quality O
of O
being O
amusing O
or O
comic O
. O
Formally O
, O
humour O
is O
deÔ¨Åned O
as O
the O
nature O
of O
experiences O
to O
induce O
laughter O
and O
provide O
amusement O
. O
Humourous O
memes O
are O
the O
most O
popular O
and O
widely O
used O
on O
social O
media O
platforms O
. O
An O
example O
for O
humourous O
memes O
is O
shown O
in O
Figure O
1a O
. O
Sarcasm O
is O
often O
used O
to O
convey O
thinly O
veiled O
disapproval O
humorously O
. O
A O
sarcastic O
meme O
is O
a O
meme O
where O
an O
incongruity O
exists O
between O
the O
intended O
meaning O
and O
the O
way O
it O
is O
expressed O
. O
These O
are O
generally O
used O
to O
express O
dissatisfaction O
or O
to O
veil O
insult O
through O
humour O
. O
As O
we O
can O
see O
in O
Figure O
1a O
, O
the O
person O
on O
the O
right O
is O
made O
fun O
of O
, O
without O
explicitly O
expressing O
it O
, O
which O
is O
a O
typical O
example O
of O
a O
sarcastic O
meme O
. O
Offensive O
content O
include O
a O
lot O
of O
insulting O
, O
derogatory O
terms O
. O
It O
is O
contrary O
to O
the O
moral O
sense O
or O
good O
. O
As O
social O
media O
expands O
, O
offensive O
language O
has O
become O
a O
huge O
headache O
to O
maintain O
sanity O
on O
social O
media O
. O
As O
memes O
are O
growing O
to O
become O
more O
and O
more O
popular O
, O
detecting O
offensive O
memes O
on O
such O
platforms O
is O
becoming O
an O
important O
and O
challenging O
task O
. O
Figure O
1a O
, O
Figure O
1c O
and O
Figure O
1d O
are O
the O
instances O
of O
Offensive O
memes O
. O
Motivation O
is O
derived O
from O
the O
word O
‚Äô O
motive O
‚Äô O
which O
means O
needs O
or O
desires O
within O
the O
individuals O
. O
It O
is O
the O
process O
of O
stimulating O
people O
to O
actions O
to O
achieve O
their O
goals O
. O
By O
its O
deÔ¨Ånition O
, O
motivational O
memes O
are O
those O
that O
beneÔ¨Åt O
a O
certain O
group O
of O
people O
to O
achieve O
their O
plans O
or O
goals O
. O
Motivation O
can O
be O
both O
either O
positive O
or O
negative.281(a)Humour O
, O
sarcasm O
, O
offensive O
. O
  O
( O
b)Motivational O
, O
positive O
. O
( O
c)Sarcasm O
, O
offensive O
, O
Negative O
. O
  O
( O
d)Sarcasm O
, O
offensive O
, O
Funny O
. O
Figure O
1 O
: O
Few O
examples O
from O
the O
Memotion O
dataset O
to O
show O
the O
inter O
- O
dependency O
between O
different O
tasks O
. O
However O
, O
we O
usually O
consider O
motivation O
in O
a O
positive O
sense O
. O
Figure O
1b O
is O
an O
excellent O
example O
for O
the O
positive O
motivation O
. O
Sentiment O
analysis O
refers O
to O
the O
process O
of O
computationally O
identifying O
and O
categorizing O
opinions O
expressed O
in O
a O
piece O
of O
communication O
, O
especially O
to O
determine O
whether O
the O
writer O
‚Äôs O
attitude O
towards O
a O
particular O
topic O
, O
product O
, O
etc O
. O
is O
positive O
, O
negative O
, O
orneutral O
. O
This O
has O
been O
a O
very O
prominent O
and O
important O
task O
in O
Natural O
Language O
Processing O
. O
Sentiment O
analysis O
on O
memes O
refers O
to O
the O
task O
of O
systematically O
extracting O
its O
emotional O
tone O
in O
understanding O
the O
opinion O
expressed O
by O
the O
meme O
. O
Figure O
1b O
is O
an O
example O
for O
positive O
sentiment O
towards O
the O
government O
and O
Figure O
1c O
for O
negative O
sentiment O
towards O
Ph.D. O
in O
Electrical O
Engineering O
. O
Generally O
, O
speciÔ¨Åc O
labels O
of O
one O
task O
have O
a O
strong O
relation O
to O
the O
other O
labels O
of O
sarcasm O
, O
offensive O
, O
humour O
or O
motivational O
tasks O
. O
Through O
proper O
representation O
, O
training O
, O
and O
evaluation O
, O
these O
relations O
can O
be O
modelled O
to O
help O
each O
other O
for O
better O
classiÔ¨Åcation O
. O
For O
example O
, O
in O
Figure O
1b O
, O
just O
by O
seeing O
text O
, O
the O
meme O
can O
be O
either O
sarcastic O
or O
motivational O
, O
but O
the O
image O
in O
the O
meme O
conÔ¨Årms O
that O
this O
has O
an O
overall O
positive O
sentiment O
and O
hence O
motivational O
. O
Similarly O
, O
in O
Figure O
1c O
, O
knowing O
that O
the O
meme O
is O
sarcastic O
and O
has O
a O
negative O
sentiment O
makes O
it O
highly O
probable O
to O
being O
offensive O
. O
As O
seen O
above O
, O
humorous O
, O
motivational O
, O
offensive O
, O
and O
sarcastic O
nature O
of O
the O
memes O
are O
closely O
related O
. O
Thus O
, O
a O
multi O
- O
task O
learning O
framework O
would O
be O
extremely O
beneÔ¨Åcial O
in O
such O
scenarios O
. O
In O
this O
paper O
, O
we O
exploit O
these O
relationships O
and O
similarities O
in O
the O
tasks O
of O
humour O
detection O
, O
sarcasm O
detection O
, O
offensive O
content O
detection O
, O
motivational O
content O
detection O
, O
and O
sentiment O
in O
a O
multi O
- O
task O
manner O
. O
The O
main O
contributions O
and/or O
attributes O
are O
as O
follows O
: O
( O
a).We O
propose O
a O
multi O
- O
task O
multimodal O
deep O
learning O
framework O
to O
leverage O
the O
util O
- O
ity O
of O
each O
task O
to O
help O
each O
other O
in O
a O
multi O
- O
task O
framework O
; O
( O
b).We O
propose O
two O
attention O
mechanisms O
viz O
. O
iTRM O
and O
iCRM O
to O
better O
understand O
the O
relationship O
between O
the O
tasks O
and O
between O
the O
classes O
of O
tasks O
, O
respectively O
; O
and O
( O
c).We O
present O
the O
state O
- O
of O
- O
the O
- O
art O
results O
for O
meme O
prediction O
in O
the O
multi O
- O
modal O
scenario O
. O
2 O
Related O
Work O
Sentiment O
analysis O
and O
its O
related O
tasks O
, O
such O
as O
humour O
detection O
, O
sarcasm O
detection O
, O
and O
offensive O
content O
detection O
, O
are O
the O
topics O
of O
interest O
due O
to O
their O
needs O
in O
recent O
times O
. O
There O
has O
been O
a O
phenomenal O
growth O
in O
multi O
- O
modal O
information O
sources O
in O
social O
media O
, O
such O
as O
audio O
, O
video O
, O
and O
text O
. O
Multi O
- O
modal O
information O
analysis O
has O
attracted O
the O
attention O
of O
researchers O
and O
developers O
due O
to O
their O
complexity O
, O
and O
multi O
- O
tasking O
has O
been O
of O
keen O
interest O
in O
the O
Ô¨Åeld O
of O
affect O
analysis O
. O
Humour O
: O
Early O
feature O
- O
based O
models O
attempt O
to O
solve O
humour O
include O
the O
models O
based O
on O
word O
overlap O
with O
jokes O
, O
presence O
of O
ambiguity O
, O
and O
word O
overlap O
with O
common O
idioms O
( O
Sj O
¬®obergh O
and O
Araki O
, O
2007 O
) O
, O
human O
- O
centeredness O
, O
and O
negative O
polarity O
( O
Mihalcea O
and O
Pulman O
, O
2007 O
) O
. O
Some O
of O
the O
recent O
multi O
- O
modal O
approaches O
include O
utilizing O
information O
from O
the O
various O
modalities O
, O
such O
as O
acoustic O
, O
visual O
, O
and O
text O
, O
using O
deep O
learning O
models O
( O
Bertero O
and O
Fung O
, O
2016 O
; O
Yang O
et O
al O
. O
, O
2019 O
; O
Swamy O
et O
al O
. O
, O
2020 O
) O
. O
Yang O
et O
al O
. O
( O
2020 O
) O
employs O
a O
paragraph O
decomposition O
technique O
coupled O
with O
Ô¨Åne O
- O
tuning O
BERT O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
model O
for O
humour O
detection O
on O
three O
languages O
( O
Chinese O
, O
Spanish O
and O
Russian O
) O
. O
Sarcasm O
: O
Starting O
from O
the O
traditional O
approaches O
, O
such O
as O
rule O
- O
based O
methods O
( O
Veale O
and O
Hao O
, O
2010 O
) O
, O
lexical O
features O
( O
Carvalho O
et O
al O
. O
, O
2009 O
) O
, O
and O
incongruity O
( O
Joshi O
et O
al O
. O
, O
2015 O
) O
to O
all O
the O
way O
up O
to O
multi O
- O
modal O
deep O
learning O
techniques O
( O
Schi-282fanella O
et O
al O
. O
, O
2016 O
) O
, O
sarcasm O
detection O
has O
been O
showing O
its O
presence O
. O
Castro O
et O
al O
. O
( O
2019 O
) O
created O
a O
multi O
- O
modal O
conversational O
dataset O
, O
MUStARD O
from O
the O
famous O
TV O
shows O
, O
and O
provided O
baseline O
SVM O
approaches O
for O
sarcasm O
detection O
. O
Recently O
, O
Chauhan O
et O
al O
. O
( O
2020 O
) O
proposed O
a O
multi O
- O
task O
learning O
framework O
for O
multi O
- O
modal O
sarcasm O
, O
sentiment O
and O
emotion O
analysis O
to O
explore O
how O
sentiment O
and O
emotion O
helps O
sarcasm O
. O
The O
author O
used O
the O
MUStARD O
dataset O
and O
extended O
the O
MUStARD O
dataset O
with O
sentiment O
( O
implicit O
and O
explicit O
) O
and O
emotion O
( O
implicit O
and O
explicit O
) O
labels O
. O
Offensive O
: O
Razavi O
et O
al O
. O
( O
2010 O
) O
used O
a O
threelevel O
classiÔ¨Åcation O
model O
taking O
advantage O
of O
various O
features O
from O
statistical O
models O
and O
rulebased O
patterns O
and O
various O
dictionary O
- O
based O
features O
. O
Chen O
et O
al O
. O
( O
2012 O
) O
proposed O
a O
feature O
- O
based O
Lexical O
Syntactic O
Feature O
( O
LSF O
) O
architecture O
to O
detect O
the O
offensive O
contents O
. O
Gomez O
et O
al O
. O
( O
2020 O
) O
created O
a O
multi O
- O
modal O
hate O
- O
speech O
dataset O
from O
Twitter O
( O
MMHS150 O
K O
) O
to O
introduce O
a O
deep O
- O
learningbased O
multi O
- O
modal O
Textual O
Kernels O
Model O
( O
TKM O
) O
and O
compare O
it O
with O
various O
existing O
deep O
learning O
architectures O
on O
the O
proposed O
MMHS150 O
K O
dataset O
. O
Motivation O
: O
Swieczkowska O
et O
al O
. O
( O
2020 O
) O
proposes O
a O
novel O
chaining O
method O
of O
neural O
networks O
for O
identifying O
motivational O
texts O
where O
the O
output O
from O
one O
model O
is O
passed O
on O
to O
the O
second O
model O
. O
Sentiment O
: O
An O
important O
task O
to O
leverage O
multimodality O
information O
effectively O
is O
to O
combine O
them O
using O
various O
strategies O
. O
Mai O
et O
al O
. O
( O
2019 O
) O
employs O
a O
hierarchical O
feature O
fusion O
strategy O
, O
Divide O
, O
Conquer O
, O
and O
Combine O
for O
affective O
computing O
. O
Chauhan O
et O
al O
. O
( O
2019 O
) O
uses O
the O
Inter O
- O
modal O
Interaction O
Module O
( O
IIM O
) O
to O
combine O
information O
from O
a O
pair O
of O
modalities O
for O
multi O
- O
modal O
sentiment O
and O
emotion O
analysis O
. O
Some O
of O
the O
other O
techniques O
include O
a O
contextual O
inter O
- O
modal O
attention O
based O
framework O
for O
multi O
- O
modal O
sentiment O
classiÔ¨Åcation O
( O
Ghosal O
et O
al O
. O
, O
2018 O
; O
Akhtar O
et O
al O
. O
, O
2019 O
) O
. O
Multi O
- O
task O
: O
Some O
of O
the O
early O
attempts O
to O
correlate O
the O
tasks O
like O
sarcasm O
, O
humour O
, O
and O
offensive O
statements O
include O
a O
features O
based O
classiÔ¨Åcation O
using O
various O
syntactic O
and O
semantic O
features O
, O
such O
as O
frequency O
of O
words O
, O
the O
intensity O
of O
adverbs O
and O
adjectives O
, O
the O
gap O
between O
positive O
and O
negative O
terms O
, O
the O
structure O
of O
the O
sentence O
, O
synonyms O
and O
others O
( O
Barbieri O
and O
Saggion O
, O
2014 O
) O
. O
More O
recently O
, O
Badlani O
et O
al O
. O
( O
2019 O
) O
proposed O
a O
convolution O
- O
based O
model O
to O
extract O
the O
embedding O
by O
Ô¨Åne O
- O
tuning O
the O
same O
for O
the O
tasks O
of O
sentiment O
, O
sarcasm O
, O
humour O
, O
and O
hate O
- O
speech O
and O
then O
concatenating O
these O
representations O
to O
be O
used O
in O
a O
sentiment O
classiÔ¨Åer O
. O
In O
our O
current O
work O
, O
we O
propose O
a O
multi O
- O
task O
multi O
- O
modal O
deep O
learning O
framework O
to O
simultaneously O
solve O
the O
tasks O
of O
sarcasm O
, O
humour O
, O
offensive O
, O
and O
motivational O
on O
memes O
. O
Further O
, O
to O
the O
best O
of O
our O
knowledge O
, O
this O
is O
the O
very O
Ô¨Årst O
attempt O
at O
solving O
the O
multi O
- O
modal O
affect O
analysis O
on O
memes O
in O
a O
multi O
- O
task O
deep O
learning O
framework O
. O
We O
demonstrate O
through O
a O
detailed O
empirical O
evaluation O
that O
a O
multi O
- O
task O
learning O
framework O
can O
improve O
the O
performance O
of O
individual O
tasks O
over O
a O
single O
task O
learning O
framework O
. O
3 O
Proposed O
Methodology O
We O
propose O
an O
attention O
- O
based O
deep O
learning O
model O
to O
solve O
the O
problem O
of O
multi O
- O
task O
affect O
analysis O
of O
memes O
. O
The O
inputs O
to O
the O
model O
are O
the O
meme O
itself O
and O
the O
manually O
corrected O
text O
extracted O
through O
OCR O
. O
The O
overall O
architecture O
is O
depicted O
in O
Figure O
2 O
. O
The O
source O
code O
is O
available O
at O
http://www O
. O
iitp.ac.in/ O
Àúai O
- O
nlp O
- O
ml O
/ O
resources.html O
. O
3.1 O
Input O
Layer O
: O
We O
now O
describe O
the O
input O
features O
for O
our O
proposed O
model O
. O
3.1.1 O
Text O
Input O
Given O
Nnumber O
of O
samples O
, O
where O
each O
sample O
is O
associated O
with O
meme O
image O
and O
the O
corresponding O
text O
. O
Let O
us O
assume O
, O
in O
each O
sample O
, O
there O
are O
nTnumber O
of O
words O
w1 O
: O
nT O
= O
w1 O
, O
... O
, O
w O
nT O
, O
where O
wj‚ààRdT O
, O
dT= O
768 O
, O
and O
wjis O
obtained O
using O
BERT O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
. O
The O
maximum O
number O
of O
words O
for O
ithsample O
across O
the O
dataset O
is O
189 O
. O
3.1.2 O
Image O
Input O
Image O
is O
the O
prime O
component O
of O
any O
meme O
and O
contains O
the O
majority O
of O
the O
information O
. O
To O
leverage O
this O
information O
effectively O
, O
feature O
vectors O
from O
average O
pooling O
layer O
( O
avgpool O
) O
of O
the O
ImageNet O
pre O
- O
trained O
ResNet-152 O
( O
He O
et O
al O
. O
, O
2016 O
) O
image O
classiÔ¨Åcation O
model O
are O
extracted O
. O
Each O
image O
is O
Ô¨Årst O
pre O
- O
processed O
by O
resizing O
to O
224√ó224 O
and O
then O
normalized O
. O
The O
extracted O
feature O
vector O
for O
image O
of O
ithsample O
is O
represented O
by O
Vi‚ààRdv O
anddv= O
2048 O
. O
3.2 O
Attention O
Modules O
These O
vectors O
are O
concatenated O
and O
then O
passed O
through O
a O
set O
of O
four O
dense O
layers O
to O
obtain O
the O
vectors O
of O
equal O
length O
drepresented O
by O
TVt‚ààRd,283Figure O
2 O
: O
Overall O
architecture O
of O
the O
proposed O
multi O
- O
modal O
multi O
- O
task O
affect O
analysis O
framework O
for O
Memes O
. O
Here O
Vrefers O
to O
the O
Meme O
Image O
andTrefers O
to O
the O
text O
extracted O
from O
the O
Meme O
. O
where O
tis O
a O
task‚àà{humour O
, O
sarcasm O
, O
offensive O
, O
motivational O
} O
. O
These O
vectors O
are O
then O
passed O
through O
the O
Inter O
- O
class O
Relationship O
Module O
and O
Inter O
- O
task O
Relationship O
module O
. O
The O
output O
is O
then O
concatenated O
and O
passed O
through O
another O
set O
of O
four O
dense O
layers O
, O
and O
a O
layer O
of O
softmax O
is O
applied O
to O
obtain O
the O
Ô¨Ånal O
output O
. O
3.2.1 O
Inter O
- O
class O
Relationship O
Module O
This O
module O
is O
used O
to O
learn O
the O
relationship O
between O
the O
classes O
of O
all O
the O
tasks O
. O
This O
is O
done O
by O
passing O
TVtthrough O
another O
dense O
layer O
and O
softmax O
( O
conÔ¨Ådence O
score O
) O
. O
For O
each O
task O
, O
we O
Ô¨Årst O
group O
all O
the O
classes O
into O
two O
classes O
for O
the O
hierarchical O
classiÔ¨Åcation O
of O
the O
sample O
. O
At O
this O
level O
, O
the O
sample O
is O
labelled O
with O
either O
positive O
or O
negative O
for O
all O
the O
tasks O
. O
For O
instance O
, O
a O
sample O
will O
be O
labelled O
as O
either O
sarcastic O
or O
not O
sarcastic O
for O
sarcasm O
tasks O
. O
A O
loss O
is O
back O
- O
propagated O
using O
these O
conÔ¨Ådence O
scores O
for O
the O
corresponding O
tasks O
. O
This O
is O
done O
in O
order O
to O
control O
each O
dense O
layer O
so O
that O
it O
aligns O
with O
the O
respective O
tasks O
. O
Meanwhile O
, O
a O
dot O
- O
product O
of O
the O
softmax O
scores O
of O
each O
task O
is O
obtained O
and O
used O
to O
form O
the O
Score O
Matrix O
. O
This O
is O
then O
Ô¨Çattened O
and O
passed O
forward O
. O
3.2.2 O
Inter O
- O
task O
Relationship O
Module O
While O
the O
above O
module O
is O
used O
to O
Ô¨Ånd O
the O
correlation O
between O
the O
individual O
classes O
, O
this O
module O
is O
used O
to O
Ô¨Ånd O
the O
relationship O
between O
the O
different O
tasks O
in O
the O
model O
. O
This O
is O
done O
by O
initially O
Ô¨Ånding O
the O
cosine O
- O
similarity O
between O
TVtvectors O
. O
And O
apooling O
layer O
is O
used O
to O
collect O
information O
between O
the O
tasks O
and O
then O
normalized O
by O
the O
corresponding O
cosine O
- O
similarity O
score O
. O
The O
output O
from O
the O
pooling O
layer O
is O
then O
Ô¨Çattened O
and O
passed O
forward O
. O
3.3 O
Output O
Unit O
The O
Ô¨Çattened O
vectors O
from O
iTRM O
andiCRM O
are O
concatenated O
and O
then O
branched O
into O
four O
dense O
layers O
for O
each O
task O
. O
This O
is O
then O
forwarded O
through O
a O
softmax O
layer O
to O
obtain O
the O
Ô¨Ånal O
output O
for O
each O
task O
, O
and O
the O
loss O
is O
back O
- O
propagated O
to O
learn O
the O
parameters O
. O
In O
this O
layer O
, O
the O
information O
from O
both O
iCRM O
andiTRM O
modules O
will O
be O
leveraged O
and O
used O
to O
predict O
the O
Ô¨Ånal O
outcome O
. O
Please O
note O
that O
, O
there O
are O
two O
sets O
of O
loss O
used O
in O
the O
model O
, O
one O
in O
the O
iCRM O
module O
and O
second O
at O
the O
end O
the O
of O
Output O
Unit O
. O
4 O
Dataset O
We O
perform O
experiments O
using O
the O
dataset O
released O
in O
the O
Memotion O
Analysis O
1.0 O
@SemEval O
2020 O
Task O
( O
Sharma O
et O
al O
. O
, O
2020)1 O
. O
This O
dataset O
consists O
of O
6992 O
samples O
. O
Each O
sample O
consists O
of O
an O
image O
, O
corrected O
text O
extracted O
from O
the O
meme O
, O
and O
the O
Ô¨Åve O
labels O
associated O
with O
the O
Ô¨Åve O
tasks O
, O
viz O
. O
, O
Humour O
, O
Sarcasm O
, O
Offensive O
, O
Motivational O
, O
and O
Overall O
Sentiment O
. O
The O
distribution O
of O
the O
classes O
associated O
with O
each O
of O
the O
Ô¨Åve O
tasks O
with O
label O
is O
shown O
in O
Table O
1 O
and O
Table O
2 O
. O
1https://competitions.codalab.org/com O
petitions/20629284Task O
Classes O
Count O
RC O
( O
% O
) O
T O
- O
A O
Sentvery O
negative O
1033 O
17.34Ngnegative O
3127 O
52.48 O
neutral O
2201 O
36.94 O
Nu O
positive O
480 O
8.06Psvery O
positive O
151 O
2.53 O
Table O
1 O
: O
Dataset O
Distribution O
of O
Task O
- O
A O
, O
where O
RCand O
T O
- O
Adenotes O
the O
relative O
count O
and O
abbreviation O
for O
labels O
of O
Task O
- O
A O
, O
respectively O
. O
Task O
Classes O
Count O
RC O
( O
% O
) O
T O
- O
C O
T O
- O
B O
Hunotfunny O
1651 O
30.91 O
NfNh O
funny O
2452 O
45.91 O
Fn O
Hm O
very O
funny O
2238 O
41.90 O
Vf O
hilarious O
651 O
12.19 O
Hr O
Sarnotsarcastic O
1544 O
22.08 O
NsNs O
general O
3507 O
50.16 O
Gr O
Sr O
twisted O
meaning O
1547 O
22.13 O
Tm O
very O
twisted O
394 O
5.64 O
Vt O
Offnotoffensive O
2713 O
38.80 O
NoNo O
slight O
2592 O
37.07 O
Sg O
Of O
very O
offensive O
1466 O
20.97 O
Vo O
hateful O
offensive O
221 O
3.16 O
Ho O
Monotmotivational O
4525 O
64.72 O
NmNm O
motivational O
2467 O
35.28 O
MoMo O
Table O
2 O
: O
Dataset O
Distribution O
of O
Task O
- O
B O
and O
Task O
- O
C O
, O
where O
RC O
, O
T O
- O
BandT O
- O
Cdenotes O
the O
relative O
count O
, O
abbreviation O
for O
labels O
of O
Task O
- O
B O
, O
and O
abbreviation O
for O
labels O
of O
Task O
- O
C O
respectively O
. O
We O
address O
5 O
multi O
- O
modal O
affective O
analysis O
problems O
, O
namely O
humour O
classiÔ¨Åcation O
, O
sarcasm O
classiÔ¨Åcation O
, O
offensive O
classiÔ¨Åcation O
, O
motivational O
classiÔ¨Åcation O
, O
and O
sentiment O
classiÔ¨Åcation O
. O
A. O
Humour O
classiÔ¨Åcation O
: O
There O
are O
four O
classes O
associated O
with O
the O
humour O
task O
, O
namely O
not O
funny O
, O
funny O
, O
very O
funny O
, O
and O
hilarious O
, O
which O
are O
labelled O
as O
0 O
, O
1 O
, O
2 O
, O
and O
3 O
, O
respectively O
. O
B. O
Sarcasm O
classiÔ¨Åcation O
: O
There O
are O
four O
classes O
associated O
with O
the O
sarcasm O
task O
, O
namely O
not O
sarcastic O
, O
general O
, O
twisted O
meaning O
, O
and O
very O
twisted O
which O
are O
labelled O
as O
0 O
, O
1 O
, O
2 O
, O
and O
3 O
respectively O
. O
C. O
Offensive O
classiÔ¨Åcation O
: O
There O
are O
four O
classes O
associated O
with O
the O
offensive O
task O
, O
namely O
not O
offensive O
, O
slight O
, O
very O
offensive O
, O
and O
hateful O
offensive O
which O
are O
labelled O
as O
0 O
, O
1 O
, O
2 O
, O
and O
3 O
, O
respectively O
. O
D. O
Motivational O
classiÔ¨Åcation O
: O
There O
are O
two O
classes O
associated O
with O
the O
motivational O
task O
, O
namely O
not O
motivational O
and O
motivational O
, O
which O
are O
labelled O
as O
0 O
and O
1 O
, O
respectively O
. O
E. O
Sentiment O
classiÔ¨Åcation O
: O
There O
are O
Ô¨Åve O
classes O
associated O
with O
the O
sentiment O
task O
, O
namely O
very O
negative O
, O
negative O
, O
neutral O
, O
positive O
, O
and O
very O
positive O
, O
which O
are O
labelled O
as O
0 O
, O
1 O
, O
2 O
, O
3 O
, O
and O
4 O
, O
respectively O
. O
5 O
Experimental O
setup O
In O
accordance O
with O
the O
SemEval O
2020 O
( O
Sharma O
et O
al O
. O
, O
2020 O
) O
, O
the O
project O
is O
organized O
into O
three O
sets O
of O
tasks2 O
. O
‚Ä¢Task O
A O
: O
Sentiment O
ClassiÔ¨Åcation O
: O
In O
this O
task O
, O
memes O
are O
classiÔ¨Åed O
into O
3 O
classes O
viz O
. O
, O
-1 O
( O
negative O
, O
very O
negative O
) O
, O
0 O
( O
neutral O
) O
and O
+1 O
( O
positive O
, O
very O
positive O
) O
. O
‚Ä¢Task O
B O
: O
Binary O
- O
class O
ClassiÔ¨Åcation O
: O
In O
this O
set O
of O
tasks O
, O
the O
memes O
are O
classiÔ¨Åed O
as O
follows O
( O
c.f O
. O
T O
- O
B O
in O
Table O
2 O
) O
; O
1.Humour O
( O
funny O
, O
very O
funny O
, O
hilarious O
) O
and O
Non O
- O
humour O
( O
not O
funny O
) O
. O
2.Sarcasm O
( O
general O
, O
twisted O
meaning O
, O
very O
twisted O
) O
and O
Non O
- O
sarcasm O
( O
non O
sarcastic O
) O
3.Offensive O
( O
slight O
, O
very O
offensive O
, O
hateful O
offensive O
) O
and O
Non O
- O
Offensive O
( O
not O
offensive O
) O
, O
and O
4.Motivational O
( O
motivational O
) O
and O
Nonmotivational O
( O
not O
motivational O
) O
. O
‚Ä¢Task O
C O
: O
Multi O
- O
class O
ClassiÔ¨Åcation O
: O
In O
this O
set O
of O
task O
, O
the O
original O
labels O
are O
used O
as O
described O
in O
the O
dataset O
( O
c.f O
. O
T O
- O
C O
in O
Table O
2 O
) O
for O
the O
tasks O
of O
Humour O
, O
Sarcasm O
, O
Offensive O
and O
Motivational O
. O
Please O
note O
that O
, O
in O
Task O
A O
, O
as O
it O
is O
not O
a O
multitask O
scenario O
, O
iCRM O
andiTRM O
are O
not O
applicable O
. O
For O
all O
the O
other O
sets O
of O
tasks O
, O
the O
entire O
network O
is O
shown O
in O
Figure O
2 O
. O
We O
evaluate O
our O
proposed O
model O
on O
the O
multimodal O
Memotion O
dataset O
. O
We O
perform O
grid O
search O
to O
Ô¨Ånd O
the O
optimal O
hyper O
- O
parameters O
( O
c.f O
. O
Table O
3 O
) O
. O
Though O
we O
aim O
for O
a O
generic O
hyper O
- O
parameter O
conÔ¨Åguration O
for O
all O
the O
experiments O
, O
in O
some O
cases O
, O
a O
different O
choice O
of O
the O
parameter O
has O
a O
signiÔ¨Åcant O
effect O
. O
Therefore O
, O
we O
choose O
different O
parameters O
for O
a O
different O
set O
of O
experiments O
. O
2https://competitions.codalab.org/com O
petitions/20629#learn O
thedetails O
- O
task O
- O
la O
bels O
- O
format285Parameters O
Task O
- O
A O
Task O
- O
B O
Task O
- O
C O
Activations O
ReLu O
Optimizer O
Adam O
( O
lr=0.001 O
) O
Output O
Softmax O
Loss O
Categorical O
cross O
- O
entropy O
Batch O
16 O
Epochs O
30 O
Dropout O
- O
p O
0.3 O
0.5 O
0.7 O
# O
neurons O
( O
Dense O
) O
50 O
200 O
200 O
Table O
3 O
: O
Model O
conÔ¨Ågurations O
We O
implement O
our O
proposed O
model O
on O
the O
open O
source O
machine O
learning O
library O
PyTorch3 O
. O
Hugging O
Face4library O
is O
used O
for O
BERT O
implementation O
. O
As O
the O
evaluation O
metric O
, O
we O
employ O
precision O
( O
P O
) O
, O
recall O
( O
R O
) O
, O
macro O
- O
F1 O
( O
Ma O
- O
F1 O
) O
, O
and O
micro O
- O
F1 O
( O
Mi- O
F1 O
) O
for O
all O
the O
tasks O
i.e. O
, O
humour O
, O
sarcasm O
, O
offensive O
, O
motivational O
, O
and O
sentiment O
. O
We O
use O
Adam O
as O
an O
optimizer O
, O
Softmax O
as O
a O
classiÔ¨Åer O
, O
and O
the O
categorical O
cross O
- O
entropy O
as O
a O
loss O
function O
for O
all O
the O
tasks O
. O
6 O
Results O
and O
Analysis O
We O
evaluate O
our O
proposed O
architecture O
with O
bimodal O
inputs O
( O
i.e. O
, O
text O
and O
visual O
) O
. O
We O
show O
the O
obtained O
results O
for O
Task O
- O
A O
( O
i.e. O
, O
sentiment O
analysis)in O
Table O
4 O
. O
LabelsTask O
- O
A O
P O
RMa O
- O
F1Mi O
- O
F1 O
Sentiment O
36.99 O
35.70 O
35.81 O
50.58 O
Table O
4 O
: O
Memes O
: O
Sentiment O
ClassiÔ¨Åcation O
( O
Task O
A O
) O
Task O
- O
B O
has O
four O
different O
tasks O
, O
i.e. O
, O
humour O
, O
sarcasm O
, O
offensive O
, O
and O
sentiment O
with O
binary O
- O
class O
labels O
( O
c.f O
. O
binary O
- O
class O
classiÔ¨Åcation O
in O
Section O
5 O
) O
. O
The O
results O
are O
shown O
in O
Table O
5 O
. O
LabelsTask O
- O
B O
( O
Binary O
ClassiÔ¨Åcation O
) O
STL O
MTL O
P O
RMa O
- O
F1Mi O
- O
F1 O
P O
RMa O
- O
F1Mi O
- O
F1 O
Hu O
55.44 O
53.77 O
53.74 O
71.29 O
55.52 O
53.84 O
53.84 O
71.29 O
Sa O
51.94 O
51.34 O
50.98 O
70.76 O
52.99 O
52.48 O
52.52 O
70.94 O
Of O
52.33 O
52.19 O
52.13 O
56.28 O
51.35 O
51.37 O
51.36 O
54.10 O
Mo O
53.56 O
53.49 O
53.51 O
57.18 O
55.86 O
56.44 O
56.12 O
57.44 O
Table O
5 O
: O
Memes O
: O
Single O
- O
task O
vs O
Multi O
- O
task O
( O
Task O
B O
) O
Task O
- O
C O
has O
also O
four O
different O
tasks O
, O
i.e. O
, O
humour O
, O
sarcasm O
, O
offensive O
, O
and O
sentiment O
with O
multi O
- O
class O
labels O
( O
c.f O
. O
multi O
- O
class O
classiÔ¨Åcation O
in O
Section O
5 O
) O
. O
The O
results O
are O
shown O
in O
Table O
6 O
. O
3https://pytorch.org/ O
4https://github.com/huggingface/trans O
formersLabelsTask O
- O
C O
( O
Multi O
- O
class O
ClassiÔ¨Åcation O
) O
STL O
MTL O
P O
RMa O
- O
F1Mi O
- O
F1 O
P O
RMa O
- O
F1Mi O
- O
F1 O
Hu O
26.83 O
26.89 O
26.75 O
29.76 O
27.23 O
27.29 O
27.03 O
32.00 O
Sa O
25.16 O
26.71 O
25.74 O
36.52 O
26.30 O
27.33 O
26.80 O
39.94 O
Of O
27.21 O
27.30 O
26.93 O
35.30 O
25.05 O
26.04 O
25.53 O
35.94 O
Mo O
53.32 O
52.89 O
52.65 O
58.46 O
54.14 O
53.31 O
53.72 O
59.79 O
Table O
6 O
: O
Memes O
: O
Single O
- O
task O
vs O
Multi O
- O
task O
( O
Task O
C O
) O
In O
both O
the O
tasks O
B O
and O
C O
, O
we O
outline O
the O
comparison O
between O
the O
multi O
- O
task O
( O
MTL O
) O
and O
single O
- O
task O
( O
STL O
) O
learning O
frameworks O
in O
Table O
5 O
and O
Table O
6 O
. O
We O
observe O
that O
MTL O
shows O
better O
performance O
over O
the O
STL O
setups O
. O
For O
the O
offensive O
task O
, O
we O
Ô¨Ånd O
that O
STL O
performs O
better O
than O
MTL O
. O
We O
hypothesize O
that O
this O
is O
due O
to O
the O
model O
getting O
confused O
between O
the O
offensive O
and O
sarcastic O
( O
or O
humorous O
) O
memes O
. O
From O
Table O
9 O
, O
under O
Sarcasm O
, O
we O
can O
see O
that O
for O
the O
class O
Vt O
, O
MTL O
predicts O
a O
few O
samples O
as O
sarcastic O
, O
whereas O
in O
actuality O
it O
belongs O
to O
the O
other O
classes O
. O
However O
, O
we O
can O
see O
a O
decrease O
in O
performance O
for O
class O
Ho O
under O
Offensive O
. O
This O
is O
due O
to O
the O
lack O
of O
a O
larger O
dataset O
for O
the O
complex O
model O
to O
disambiguate O
the O
same O
. O
In O
the O
example O
, O
BRB O
... O
GOT O
TO O
TAKE O
CARE O
OF O
SOME O
SH*T O
IN O
UKRAIN O
( O
c.f O
. O
Figure O
1d O
) O
, O
the O
actual O
set O
of O
labels O
are O
Fn O
, O
Gn O
, O
Sg O
, O
Nm O
. O
The O
predicted O
labels O
in O
STL O
are O
Vf O
, O
Gn O
, O
Sg O
, O
Moand O
in O
MTL O
are O
Vf O
, O
Tm O
, O
Vo O
, O
Mo. O
This O
is O
supposed O
to O
be O
slightly O
offensive O
but O
got O
it O
confused O
with O
the O
sarcastic O
. O
7 O
Comparative O
Analysis O
We O
compare O
the O
results O
obtained O
in O
our O
proposed O
model O
against O
the O
baseline O
model O
and O
SemEval O
2020 O
winner O
, O
which O
also O
made O
use O
of O
the O
same O
dataset O
. O
The O
comparative O
analysis O
is O
shown O
in O
Table O
7 O
. O
Our O
proposed O
multi O
- O
modal O
framework O
achieves O
the O
best O
macro O
- O
F1 O
of O
35.8 O
% O
( O
0.4%‚Üë O
) O
and O
micro O
- O
F1 O
of O
50.6 O
% O
( O
1.9%‚Üë O
) O
as O
compared O
to O
macro O
- O
F1 O
of O
35.4 O
% O
and O
micro O
- O
F1 O
of O
48.7 O
% O
of O
the O
state O
- O
of O
- O
the O
- O
art O
system O
( O
i.e. O
, O
SemEval O
2020 O
Winner O
) O
for O
Task O
- O
A. O
Similarly O
, O
for O
Task O
- O
B O
, O
we O
obtain O
the O
macro O
- O
F1 O
of O
53.5 O
% O
( O
1.7%‚Üë O
) O
and O
micro O
- O
F1 O
of O
63.4 O
% O
( O
2.0%‚Üë O
) O
as O
compared O
to O
the O
macro O
- O
F1 O
of O
51.8 O
% O
and O
micro O
- O
F1 O
of O
61.4 O
% O
of O
the O
state O
- O
ofthe O
- O
art O
system O
, O
whereas O
for O
Task O
- O
C O
, O
we O
obtain O
the O
macro O
- O
F1 O
of O
33.3 O
% O
( O
1.1%‚Üë O
) O
and O
micro O
- O
F1 O
of O
41.9 O
% O
( O
4.1%‚Üë O
) O
as O
compared O
to O
the O
macro O
- O
F1 O
of O
32.2 O
% O
and O
micro O
- O
F1 O
of O
37.8 O
% O
of O
the O
state O
- O
of O
- O
theart O
system O
. O
It O
is O
evident O
from O
Table O
5 O
and O
Table O
6 O
that O
multitask O
learning O
framework O
successfully O
leverages O
the286SystemsTask O
A O
Task O
B O
Task O
C O
Ma O
- O
F1Mi O
- O
F1Ma O
- O
F1Mi O
- O
F1Ma O
- O
F1Mi O
- O
F1 O
Baseline O
21.76 O
30.77 O
50.02 O
56.86 O
30.08 O
33.28 O
SE‚Äô20 O
Winner O
35.46 O
48.72 O
51.83 O
61.44 O
32.24 O
37.79 O
Proposed O
35.81 O
50.58 O
53.46 O
63.44 O
33.27 O
41.92 O
Table O
7 O
: O
Comparative O
Analysis O
of O
the O
proposed O
approach O
with O
recent O
state O
- O
of O
- O
the O
- O
art O
systems O
. O
Here O
, O
SE‚Äô20 O
denotes O
the O
SemEval O
2020 O
winner O
, O
and O
‚Äô O
Proposed O
‚Äô O
refers O
to O
the O
models O
described O
in O
the O
paper O
for O
the O
respective O
tasks O
. O
Sentiment O
NgNuPs O
Ng17 O
19 O
127 O
Nu O
25 O
170 O
399 O
Ps58 O
290 O
763 O
( O
a O
) O
Task O
- O
ASetups O
Humour O
Sarcasm O
Offensive O
Motivational O
NhHm O
NsSr O
NoOf O
NmMo O
STLNh O
91 O
354 O
Ns68 O
353 O
No252 O
455Nm O
801 O
387 O
Hm O
185 O
1248 O
Sa196 O
1261 O
Of366 O
805Mo417 O
273 O
MTLNh O
92 O
353 O
Ns90 O
331 O
No285 O
422Nm O
801 O
387 O
Hm O
186 O
1247 O
Sa239 O
1218 O
Of440 O
731Mo431 O
259 O
( O
b O
) O
Task O
- O
B O
Table O
8 O
: O
Confusion O
Matrix O
for O
Task O
- O
A O
and O
Task O
- O
B O
( O
Refer O
Table O
1 O
and O
Table O
2 O
for O
Label O
deÔ¨Ånitions O
) O
. O
Setups O
Humour O
Sarcasm O
Offensive O
Motivational O
NfFnVfHr O
NsGrTmVt O
NoSgVoHo O
NmMo O
STLNf122 O
143 O
130 O
50Ns117 O
182 O
122 O
0No254 O
307 O
111 O
35Nm O
878 O
310Fn140 O
218 O
205 O
91Gr234 O
427 O
276 O
0Sg224 O
340 O
105 O
40 O
Vf129 O
201 O
193 O
82Tm O
94 O
188 O
142 O
0Vo109 O
198 O
62 O
18Mo470 O
220Hr36 O
65 O
47 O
26Vt O
19 O
52 O
25 O
0Ho O
20 O
37 O
11 O
7 O
MTLNf147 O
147 O
136 O
21Ns125 O
206 O
87 O
3No350 O
219 O
138 O
0Nm O
924 O
264Fn173 O
240 O
208 O
33Gr222 O
525 O
172 O
18Sg330 O
250 O
129 O
0 O
Vf172 O
195 O
204 O
34Tm O
112 O
210 O
100 O
2Vo181 O
131 O
75 O
0Mo491 O
199Hr51 O
70 O
43 O
10Vt O
23 O
57 O
16 O
0Ho O
43 O
22 O
10 O
0 O
Table O
9 O
: O
Confusion O
Matrix O
for O
Task O
C O
( O
Refer O
Table O
2 O
for O
Label O
deÔ¨Ånitions O
) O
. O
inter O
- O
dependence O
between O
all O
the O
tasks O
in O
improving O
the O
overall O
performance O
in O
comparison O
to O
the O
single O
- O
task O
learning O
. O
We O
also O
show O
the O
confusion O
matrices O
corresponding O
to O
each O
set O
of O
tasks O
in O
Table O
8a O
, O
Table O
8b O
, O
and O
Table O
9 O
, O
respectively O
. O
8 O
Error O
Analysis O
We O
perform O
error O
analysis O
( O
i.e. O
for O
Task O
- O
C O
) O
on O
the O
predictions O
of O
our O
proposed O
model O
. O
We O
take O
some O
utterances O
( O
c.f O
. O
Table O
10 O
) O
with O
corresponding O
image O
( O
c.f O
. O
Figure O
3 O
) O
, O
where O
we O
show O
that O
MTL O
is O
predicting O
correct O
while O
STLis O
not O
able O
to O
predict O
the O
right O
labels O
. O
We O
also O
present O
the O
attention O
heatmaps O
for O
iCRM O
andiTRM O
of O
the O
multi O
- O
task O
learning O
framework O
in O
Figure O
4 O
and O
Figure O
5 O
, O
respectively O
. O
We O
take O
the O
Ô¨Åfth O
utterance O
from O
Table O
10 O
( O
c.f O
. O
Figure O
3e O
) O
to O
illustrate O
the O
heatmap O
. O
For O
iCRM O
( O
c.f O
. O
Figure O
4 O
) O
, O
there O
are O
six O
matrices O
which O
show O
the O
interdependency O
between O
humour O
and O
sarcasm O
( O
HuSar O
) O
, O
humour O
and O
offensive O
( O
Hu O
- O
Off O
) O
, O
humour O
and O
motivational O
( O
Hu O
- O
Mo O
) O
, O
sarcasm O
and O
offensive O
( O
saroff O
) O
, O
sarcasm O
and O
motivational O
( O
Sar O
- O
Mo O
) O
, O
and O
offensive O
and O
motivational O
( O
Off O
- O
Mo O
) O
, O
respectively O
, O
where O
( O
a O
) O
1 O
. O
  O
( O
b O
) O
2 O
. O
  O
( O
c O
) O
3 O
. O
( O
d O
) O
4 O
. O
  O
( O
e O
) O
5 O
. O
  O
( O
f O
) O
6 O
. O
Figure O
3 O
: O
Few O
examples O
for O
Human O
Error O
Analysis O
corresponding O
to O
Table O
10 O
. O
the O
light O
shade O
to O
dark O
shade O
shows O
the O
amount O
of O
contributions O
in O
ascending O
sequence O
. O
The O
main O
objective O
of O
iCRM O
is O
to O
develop O
the O
relationship O
between O
the O
classes O
of O
tasks O
. O
Figure O
4 O
shows O
the O
established O
relationship O
between O
the O
tasks O
. O
We O
see O
the O
established O
relationship O
between O
the O
classes O
of O
tasks O
in O
Figure O
4 O
. O
For O
predicting O
the O
Ô¨Åfth O
utterance O
correctly O
in O
Table O
10 O
, O
humour O
and287UtterancesSTL O
MTL O
Hu O
Sar O
Off O
Mo O
Hu O
Sar O
Off O
Mo O
1 O
my O
name O
is O
giovanni O
giorgio O
but O
everybody O
calls O
me O
giorgio O
. O
NfGrNoNmVfTmVoMo O
2 O
i O
‚Äôm O
in O
shape O
. O
unfortunately O
that O
shape O
is O
a O
potato O
VfNsNoMoFnGrSgNm O
3 O
obama O
i O
‚Äôm O
coming O
after O
ur O
job O
as O
president O
memeshappen O
. O
Com O
FnGrNoNmVfTmVoMo O
4 O
look O
at O
me O
I O
‚Äôm O
the O
captain O
now O
. O
VfTmVoMoFnGrSgNm O
5 O
freshmen O
.0000000000127 O
seconds O
after O
the O
bell O
mr O
. O
bean O
go O
zoom O
zoom O
. O
HrNsSgMoFnGrMoNm O
6 O
sorry O
i O
was O
working O
. O
FnTmVoMoVfGrSgNm O
Table O
10 O
: O
Comparison O
between O
multi O
- O
task O
learning O
and O
single O
- O
task O
learning O
frameworks O
.Few O
error O
cases O
where O
MTL O
framework O
performs O
better O
than O
the O
STL O
framework O
. O
not O
sarcasm O
( O
Figure O
4a O
) O
, O
humour O
and O
not O
offensive O
( O
Figure O
4b O
) O
etc O
. O
are O
helping O
each O
other O
. O
( O
a)Hu O
- O
Sar O
  O
( O
b)Hu O
- O
Off O
  O
( O
c)Hu O
- O
Mo O
( O
d)Sar O
- O
Off O
  O
( O
e)Sar O
- O
Mo O
  O
( O
f)Off O
- O
Mo O
Figure O
4 O
: O
iCRM O
attention O
for O
Figure O
3e O
under O
Task O
C O
Similarly O
, O
the O
main O
objective O
of O
iTRM O
is O
to O
develop O
the O
relationship O
between O
the O
tasks O
. O
Figure O
5 O
shows O
the O
established O
relationship O
between O
the O
tasks O
, O
and O
we O
see O
that O
attention O
put O
more O
weight O
on O
sarcasm O
and O
offensive O
pair O
while O
less O
weight O
on O
humour O
and O
sarcasm O
. O
It O
is O
clear O
from O
the O
deÔ¨Ånition O
of O
sarcasm O
and O
humour O
( O
c.f O
. O
Section O
1 O
) O
that O
both O
of O
them O
have O
a O
very O
different O
meaning O
when O
used O
in O
a O
sentence O
while O
the O
actual O
sentence O
looks O
similar O
. O
Hence O
sarcasm O
and O
humour O
is O
found O
not O
be O
helping O
each O
other O
. O
Figure O
5 O
: O
iTRM O
attention O
for O
Figure O
3e O
under O
Task O
C O
9 O
Conclusion O
and O
Future O
Work O
In O
this O
paper O
, O
we O
have O
successfully O
established O
the O
concept O
of O
obtaining O
effective O
relationshipsbetween O
inter O
- O
tasks O
and O
between O
inter O
- O
classes O
for O
multi O
- O
modal O
affect O
analysis O
. O
We O
have O
proposed O
a O
deep O
attentive O
multi O
- O
task O
learning O
framework O
which O
helps O
to O
obtain O
very O
effective O
inter O
- O
tasks O
and O
interclasses O
relationship O
. O
To O
capture O
the O
interdependence O
, O
we O
have O
proposed O
two O
attention O
- O
like O
mechanisms O
viz O
. O
,Inter O
- O
task O
Relationship O
Module O
( O
iTRM O
) O
and O
Inter O
- O
class O
Relationship O
Module O
( O
iCRM O
) O
. O
The O
main O
motivation O
of O
iTRM O
is O
to O
learn O
the O
relationship O
between O
the O
tasks O
, O
i.e. O
which O
task O
helps O
the O
other O
tasks O
. O
In O
contrast O
, O
iCRM O
develops O
the O
relations O
between O
the O
classes O
of O
tasks O
. O
We O
have O
evaluated O
our O
proposed O
approach O
on O
a O
recently O
published O
Memotion O
dataset O
. O
Experimental O
results O
suggest O
the O
efÔ¨Åcacy O
of O
the O
proposed O
model O
over O
the O
existing O
state O
- O
of O
- O
the O
- O
art O
systems O
( O
Baseline O
and O
SemEval O
2020 O
winner O
) O
. O
The O
evaluation O
shows O
that O
the O
proposed O
multi O
- O
task O
framework O
yields O
better O
performance O
over O
single O
- O
task O
learning O
. O
The O
dataset O
used O
for O
the O
experiments O
is O
relatively O
small O
for O
training O
an O
effective O
deep O
learning O
model O
and O
is O
heavily O
biased O
. O
Therefore O
, O
assembling O
a O
large O
, O
and O
more O
balance O
dataset O
with O
quality O
annotations O
is O
an O
important O
job O
. O
Moreover O
, O
the O
memes O
are O
a O
complicated O
form O
of O
data O
which O
includes O
both O
text O
and O
image O
that O
repeat O
over O
numerous O
memes O
( O
meme O
templates O
) O
. O
Hence O
quality O
representation O
of O
memes O
for O
affect O
analysis O
is O
challenging O
future O
work O
. O
Acknowledgement O
The O
research O
reported O
here O
is O
partially O
supported O
by O
SkyMap O
Global O
India O
Private O
Limited O
. O
Dushyant O
Singh O
Chauhan O
acknowledges O
the O
support O
of O
Prime O
Minister O
Research O
Fellowship O
( O
PMRF O
) O
, O
Govt O
. O
of O
India O
. O
Asif O
Ekbal O
acknowledges O
the O
Young O
Faculty O
Research O
Fellowship O
( O
YFRF O
) O
, O
supported O
by O
Visvesvaraya O
PhD O
scheme O
for O
Electronics O
and O
IT O
, O
Ministry O
of O
Electronics O
and O
Information O
Technology O
( O
Meit/8Y O
) O
, O
Government O
of O
India O
, O
being O
implemented O
by O
Digital O
India O
Corporation O
( O
formerly O
Media O
Lab O
Asia).288References O
Md O
Shad O
Akhtar O
, O
Dushyant O
Chauhan O
, O
Deepanway O
Ghosal O
, O
Soujanya O
Poria O
, O
Asif O
Ekbal O
, O
and O
Pushpak O
Bhattacharyya O
. O
2019 O
. O
Multi O
- O
task O
learning O
for O
multimodal O
emotion O
recognition O
and O
sentiment O
analysis O
. O
InProceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
370‚Äì379 O
, O
Minneapolis O
, O
Minnesota O
. O
Association O
for O
Computational O
Linguistics O
. O
Rohan O
Badlani O
, O
Nishit O
Asnani O
, O
and O
Manan O
Rai O
. O
2019 O
. O
Disambiguating O
sentiment O
: O
An O
ensemble O
of O
humour O
, O
sarcasm O
, O
and O
hate O
speech O
features O
for O
sentiment O
classiÔ¨Åcation O
. O
W O
- O
NUT O
2019 O
, O
page O
337 O
. O
Francesco O
Barbieri O
and O
Horacio O
Saggion O
. O
2014 O
. O
Automatic O
detection O
of O
irony O
and O
humour O
in O
twitter O
. O
In O
ICCC O
, O
pages O
155‚Äì162 O
. O
Dario O
Bertero O
and O
Pascale O
Fung O
. O
2016 O
. O
Multimodal O
deep O
neural O
nets O
for O
detecting O
humor O
in O
tv O
sitcoms O
. O
In2016 O
IEEE O
Spoken O
Language O
Technology O
Workshop O
( O
SLT O
) O
, O
pages O
383‚Äì390 O
. O
IEEE O
. O
Paula O
Carvalho O
, O
Lu O
¬¥ O
ƒ±s O
Sarmento O
, O
M O
¬¥ O
ario O
J O
Silva O
, O
and O
Eug¬¥enio O
De O
Oliveira O
. O
2009 O
. O
Clues O
for O
detecting O
irony O
in O
user O
- O
generated O
contents O
: O
oh O
... O
! O
! O
it O
‚Äôs O
‚Äù O
so O
easy‚Äù;- O
. O
In O
Proceedings O
of O
the O
1st O
international O
CIKM O
workshop O
on O
Topic O
- O
sentiment O
analysis O
for O
mass O
opinion O
, O
pages O
53‚Äì56 O
. O
Santiago O
Castro O
, O
Devamanyu O
Hazarika O
, O
Ver O
¬¥ O
onica O
P O
¬¥ O
erezRosas O
, O
Roger O
Zimmermann O
, O
Rada O
Mihalcea O
, O
and O
Soujanya O
Poria O
. O
2019 O
. O
Towards O
multimodal O
sarcasm O
detection O
( O
an O
obviously O
perfect O
paper O
) O
. O
arXiv O
preprint O
arXiv:1906.01815 O
. O
Dushyant O
Singh O
Chauhan O
, O
Md O
Shad O
Akhtar O
, O
Asif O
Ekbal O
, O
and O
Pushpak O
Bhattacharyya O
. O
2019 O
. O
Contextaware O
interactive O
attention O
for O
multi O
- O
modal O
sentiment O
and O
emotion O
analysis O
. O
In O
Proceedings O
of O
the O
2019 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
and O
the O
9th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
( O
EMNLP O
- O
IJCNLP O
) O
, O
pages O
5651‚Äì5661 O
, O
Hong O
Kong O
, O
China O
. O
Association O
for O
Computational O
Linguistics O
. O
Dushyant O
Singh O
Chauhan O
, O
Dhanush O
S O
R O
, O
Asif O
Ekbal O
, O
and O
Pushpak O
Bhattacharyya O
. O
2020 O
. O
Sentiment O
and O
emotion O
help O
sarcasm O
? O
a O
multi O
- O
task O
learning O
framework O
for O
multi O
- O
modal O
sarcasm O
, O
sentiment O
and O
emotion O
analysis O
. O
In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
4351‚Äì4360 O
, O
Online O
. O
Association O
for O
Computational O
Linguistics O
. O
Ying O
Chen O
, O
Yilu O
Zhou O
, O
Sencun O
Zhu O
, O
and O
Heng O
Xu O
. O
2012 O
. O
Detecting O
offensive O
language O
in O
social O
media O
to O
protect O
adolescent O
online O
safety O
. O
In O
2012 O
International O
Conference O
on O
Privacy O
, O
Security O
, O
Risk O
and O
Trust O
and O
2012 O
International O
Confernece O
on O
Social O
Computing O
, O
pages O
71‚Äì80 O
. O
IEEE.Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2018 O
. O
Bert O
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
understanding O
. O
arXiv O
preprint O
arXiv:1810.04805 O
. O
Deepanway O
Ghosal O
, O
Md O
Shad O
Akhtar O
, O
Dushyant O
Singh O
Chauhan O
, O
Soujanya O
Poria O
, O
Asif O
Ekbal O
, O
and O
Pushpak O
Bhattacharyya O
. O
2018 O
. O
Contextual O
inter O
- O
modal O
attention O
for O
multi O
- O
modal O
sentiment O
analysis O
. O
In O
Proceedings O
of O
the O
2018 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
3454‚Äì3466 O
, O
Brussels O
, O
Belgium O
. O
Association O
for O
Computational O
Linguistics O
. O
Raul O
Gomez O
, O
Jaume O
Gibert O
, O
Lluis O
Gomez O
, O
and O
Dimosthenis O
Karatzas O
. O
2020 O
. O
Exploring O
hate O
speech O
detection O
in O
multimodal O
publications O
. O
In O
The O
IEEE O
Winter O
Conference O
on O
Applications O
of O
Computer O
Vision O
, O
pages O
1470‚Äì1478 O
. O
Kaiming O
He O
, O
Xiangyu O
Zhang O
, O
Shaoqing O
Ren O
, O
and O
Jian O
Sun O
. O
2016 O
. O
Deep O
residual O
learning O
for O
image O
recognition O
. O
In O
Proceedings O
of O
the O
IEEE O
conference O
on O
computer O
vision O
and O
pattern O
recognition O
, O
pages O
770 O
‚Äì O
778 O
. O
Aditya O
Joshi O
, O
Vinita O
Sharma O
, O
and O
Pushpak O
Bhattacharyya O
. O
2015 O
. O
Harnessing O
context O
incongruity O
for O
sarcasm O
detection O
. O
In O
Proceedings O
of O
the O
53rd O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
7th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
( O
Volume O
2 O
: O
Short O
Papers O
) O
, O
pages O
757‚Äì762 O
. O
Sijie O
Mai O
, O
Haifeng O
Hu O
, O
and O
Songlong O
Xing O
. O
2019 O
. O
Divide O
, O
conquer O
and O
combine O
: O
Hierarchical O
feature O
fusion O
network O
with O
local O
and O
global O
perspectives O
for O
multimodal O
affective O
computing O
. O
In O
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
481‚Äì492 O
. O
Rada O
Mihalcea O
and O
Stephen O
Pulman O
. O
2007 O
. O
Characterizing O
humour O
: O
An O
exploration O
of O
features O
in O
humorous O
texts O
. O
In O
International O
Conference O
on O
Intelligent O
Text O
Processing O
and O
Computational O
Linguistics O
, O
pages O
337‚Äì347 O
. O
Springer O
. O
Amir O
H O
Razavi O
, O
Diana O
Inkpen O
, O
Sasha O
Uritsky O
, O
and O
Stan O
Matwin O
. O
2010 O
. O
Offensive O
language O
detection O
using O
multi O
- O
level O
classiÔ¨Åcation O
. O
In O
Canadian O
Conference O
on O
ArtiÔ¨Åcial O
Intelligence O
, O
pages O
16‚Äì27 O
. O
Springer O
. O
Rossano O
Schifanella O
, O
Paloma O
de O
Juan O
, O
Joel O
Tetreault O
, O
and O
Liangliang O
Cao O
. O
2016 O
. O
Detecting O
sarcasm O
in O
multimodal O
social O
platforms O
. O
In O
Proceedings O
of O
the O
24th O
ACM O
international O
conference O
on O
Multimedia O
, O
pages O
1136‚Äì1145 O
. O
Chhavi O
Sharma O
, O
Deepesh O
Bhageria O
, O
William O
Paka O
, O
Scott O
, O
Srinivas O
P O
Y O
K O
L O
, O
Amitava O
Das O
, O
Tanmoy O
Chakraborty O
, O
and O
Bj O
¬®orn O
Gamb O
¬®ack O
. O
2020 O
. O
SemEval2020 O
Task O
8 O
: O
Memotion O
Analysis O
- O
The O
VisuoLingual O
Metaphor O
! O
In O
Proceedings O
of O
the O
14th O
International O
Workshop O
on O
Semantic O
Evaluation O
( O
SemEval-2020 O
) O
, O
Barcelona O
, O
Spain O
. O
Association O
for O
Computational O
Linguistics.289Jonas O
Sj O
¬®obergh O
and O
Kenji O
Araki O
. O
2007 O
. O
Recognizing O
humor O
without O
recognizing O
meaning O
. O
In O
International O
Workshop O
on O
Fuzzy O
Logic O
and O
Applications O
, O
pages O
469‚Äì476 O
. O
Springer O
. O
Steve O
Durairaj O
Swamy O
, O
Shubham O
Laddha O
, O
Basil O
Abdussalam O
, O
Debayan O
Datta O
, O
and O
Anupam O
Jamatia O
. O
2020 O
. O
Nit O
- O
agartala O
- O
nlp O
- O
team O
at O
semeval-2020 O
task O
8 O
: O
Building O
multimodal O
classiÔ¨Åers O
to O
tackle O
internet O
humor O
. O
arXiv O
preprint O
arXiv:2005.06943 O
. O
Patrycja O
Swieczkowska O
, O
Rafal O
Rzepka O
, O
and O
Kenji O
Araki O
. O
2020 O
. O
Stepwise O
noise O
elimination O
for O
better O
motivational O
and O
advisory O
texts O
classiÔ¨Åcation O
. O
Journal O
of O
Advanced O
Computational O
Intelligence O
and O
Intelligent O
Informatics O
, O
24(1):156‚Äì168 O
. O
Tony O
Veale O
and O
Yanfen O
Hao O
. O
2010 O
. O
Detecting O
ironic O
intent O
in O
creative O
comparisons O
. O
In O
ECAI O
, O
volume O
215 O
, O
pages O
765‚Äì770 O
. O
Hao O
Yang O
, O
Yao O
Deng O
, O
Minghan O
Wang O
, O
Ying O
Qin O
, O
and O
Shiliang O
Sun O
. O
2020 O
. O
Humor O
detection O
based O
on O
paragraph O
decomposition O
and O
bert O
Ô¨Åne O
- O
tuning O
. O
Zixiaofan O
Yang O
, O
Lin O
Ai O
, O
and O
Julia O
Hirschberg O
. O
2019 O
. O
Multimodal O
indicators O
of O
humor O
in O
videos O
. O
In O
2019 O
IEEE O
Conference O
on O
Multimedia O
Information O
Processing O
and O
Retrieval O
( O
MIPR O
) O
, O
pages O
538‚Äì543 O
. O
IEEE.290Proceedings O
of O
the O
1st O
Conference O
of O
the O
Asia O
- O
PaciÔ¨Åc O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
10th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
291‚Äì302 O
December O
4 O
- O
7 O
, O
2020 O
. O
¬© O
2020 O
Association O
for O
Computational O
Linguistics O
Identifying O
Implicit O
Quotes O
for O
Unsupervised O
Extractive O
Summarization O
of O
Conversations O
Ryuji O
Kano‚Ä†‚Ä°Yasuhide O
Miura‚Ä†Tomoki O
Taniguchi‚Ä†Tomoko O
Ohkuma‚Ä† O
‚Ä†Fuji O
Xerox O
Co. O
, O
Ltd. O
‚Ä°Institute O
of O
Innovative O
Research O
, O
Tokyo O
Institute O
of O
Technology O
{ O
kano.ryuji O
, O
yasuhide.miura O
, O
taniguchi.tomoki O
, O
ohkuma.tomoko O
} O
@fujixerox.co.jp O
Abstract O
We O
propose O
Implicit O
Quote O
Extractor O
, O
an O
endto O
- O
end O
unsupervised O
extractive O
neural O
summarization O
model O
for O
conversational O
texts O
. O
When O
we O
reply O
to O
posts O
, O
quotes O
are O
used O
to O
highlight O
important O
part O
of O
texts O
. O
We O
aim O
to O
extract O
quoted O
sentences O
as O
summaries O
. O
Most O
replies O
do O
not O
explicitly O
include O
quotes O
, O
so O
it O
is O
difÔ¨Åcult O
to O
use O
quotes O
as O
supervision O
. O
However O
, O
even O
if O
it O
is O
not O
explicitly O
shown O
, O
replies O
always O
refer O
to O
certain O
parts O
of O
texts O
; O
we O
call O
them O
implicit O
quotes O
. O
Implicit O
Quote O
Extractor O
aims O
to O
extract O
implicit O
quotes O
as O
summaries O
. O
The O
training O
task O
of O
the O
model O
is O
to O
predict O
whether O
a O
reply O
candidate O
is O
a O
true O
reply O
to O
a O
post O
. O
For O
prediction O
, O
the O
model O
has O
to O
choose O
a O
few O
sentences O
from O
the O
post O
. O
To O
predict O
accurately O
, O
the O
model O
learns O
to O
extract O
sentences O
that O
replies O
frequently O
refer O
to O
. O
We O
evaluate O
our O
model O
on O
two O
email O
datasets O
and O
one O
social O
media O
dataset O
, O
and O
conÔ¨Årm O
that O
our O
model O
is O
useful O
for O
extractive O
summarization O
. O
We O
further O
discuss O
two O
topics O
; O
one O
is O
whether O
quote O
extraction O
is O
an O
important O
factor O
for O
summarization O
, O
and O
the O
other O
is O
whether O
our O
model O
can O
capture O
salient O
sentences O
that O
conventional O
methods O
can O
not O
. O
1 O
Introduction O
As O
the O
amount O
of O
information O
exchanged O
via O
online O
conversations O
is O
growing O
rapidly O
, O
automated O
summarization O
of O
conversations O
is O
in O
demand O
. O
Neuralnetwork O
- O
based O
models O
have O
achieved O
great O
performance O
on O
supervised O
summarization O
, O
but O
its O
application O
to O
unsupervised O
summarization O
is O
not O
sufÔ¨Åciently O
explored O
. O
Supervised O
summarization O
requires O
tens O
of O
thousands O
of O
human O
- O
annotated O
summaries O
. O
Because O
it O
is O
not O
realistic O
to O
prepare O
such O
large O
datasets O
for O
every O
domain O
, O
there O
is O
a O
growing O
requirement O
for O
unsupervised O
methods O
. O
Previous O
research O
proposed O
diverse O
methods O
of O
unsupervised O
summarization O
. O
Graph O
- O
centrality O
Figure O
1 O
: O
Example O
of O
a O
post O
and O
a O
reply O
with O
a O
quote O
and O
a O
reply O
with O
no O
quote O
. O
Implicit O
quote O
is O
the O
part O
of O
post O
that O
reply O
refers O
to O
, O
but O
not O
explicitly O
shown O
in O
the O
reply O
. O
based O
on O
the O
similarity O
of O
sentences O
( O
Mihalcea O
and O
Tarau O
, O
2004 O
; O
Erkan O
and O
Radev O
, O
2004 O
; O
Zheng O
and O
Lapata O
, O
2019 O
) O
has O
long O
been O
a O
strong O
feature O
for O
unsupervised O
summarization O
, O
and O
is O
also O
used O
to O
summarize O
conversations O
( O
Mehdad O
et O
al O
. O
, O
2014 O
; O
Shang O
et O
al O
. O
, O
2018 O
) O
. O
Apart O
from O
centrality O
, O
centroid O
of O
vectors O
( O
Gholipour O
Ghalandari O
, O
2017 O
) O
, O
KullbackLeibler O
divergence O
( O
Haghighi O
and O
Vanderwende O
, O
2009 O
) O
, O
reconstruction O
loss O
( O
He O
et O
al O
. O
, O
2012 O
; O
Liu O
et O
al O
. O
, O
2015 O
; O
Ma O
et O
al O
. O
, O
2016 O
) O
, O
and O
path O
scores O
of O
word O
graphs O
( O
Mehdad O
et O
al O
. O
, O
2014 O
; O
Shang O
et O
al O
. O
, O
2018 O
) O
, O
are O
leveraged O
for O
summarization O
. O
The O
premise O
of O
these O
methods O
is O
that O
important O
topics O
appear O
frequently O
in O
a O
document O
. O
Therefore O
, O
if O
important O
topics O
appear O
only O
a O
few O
times O
, O
these O
methods O
fail O
to O
capture O
salient O
sentences O
. O
For O
more O
accurate O
summarization O
, O
relying O
solely O
on O
the O
frequency O
is O
not O
sufÔ¨Åcient O
and O
we O
need O
to O
focus O
on O
other O
aspects O
of O
texts O
. O
As O
an O
alternative O
aspect O
, O
we O
propose O
‚Äú O
the O
probability O
of O
being O
quoted O
‚Äù O
. O
When O
one O
replies O
to O
an O
email O
or O
a O
post O
, O
a O
quote O
is O
used O
to O
highlight O
the291important O
parts O
of O
the O
text O
; O
an O
example O
is O
shown O
in O
Figure O
1 O
. O
The O
reply O
on O
the O
bottom O
includes O
a O
quote O
, O
which O
generally O
starts O
with O
a O
symbol O
‚Äú O
> O
‚Äù O
. O
If O
we O
can O
predict O
quoted O
parts O
, O
we O
can O
extract O
important O
sentences O
irrespective O
of O
how O
frequently O
the O
same O
topic O
appears O
in O
the O
text O
. O
Thus O
, O
we O
aim O
to O
extract O
quotes O
as O
summaries O
. O
Previous O
research O
assigned O
weights O
to O
words O
that O
appear O
in O
quotes O
, O
and O
improved O
the O
centroidbased O
summarization O
( O
Carenini O
et O
al O
. O
, O
2007 O
; O
Oya O
and O
Carenini O
, O
2014 O
) O
. O
However O
, O
most O
replies O
do O
not O
include O
quotes O
, O
so O
it O
is O
difÔ¨Åcult O
to O
use O
quotes O
as O
the O
training O
labels O
of O
neural O
models O
. O
We O
propose O
a O
model O
that O
can O
be O
trained O
without O
explicit O
labels O
of O
quotes O
. O
The O
model O
is O
Implicit O
Quote O
Extractor O
( O
IQE O
) O
. O
As O
shown O
in O
Figure O
1 O
, O
implicit O
quotes O
are O
sentences O
of O
posts O
that O
are O
not O
explicitly O
quoted O
in O
replies O
, O
but O
are O
those O
the O
replies O
most O
likely O
refer O
to O
. O
The O
aim O
of O
our O
model O
is O
to O
extract O
these O
implicit O
quotes O
for O
extractive O
summarization O
. O
We O
use O
pairs O
of O
a O
post O
and O
reply O
candidate O
to O
train O
the O
model O
. O
The O
training O
task O
of O
the O
model O
is O
to O
predict O
if O
a O
reply O
candidate O
is O
an O
actual O
reply O
to O
the O
post O
. O
IQE O
extracts O
a O
few O
sentences O
of O
the O
post O
as O
a O
feature O
for O
prediction O
. O
To O
predict O
accurately O
, O
IQE O
has O
to O
extract O
sentences O
that O
replies O
frequently O
refer O
to O
. O
Summaries O
should O
not O
depend O
on O
replies O
, O
so O
IQE O
does O
not O
use O
reply O
features O
to O
extract O
sentences O
. O
The O
model O
requires O
replies O
only O
during O
the O
training O
and O
not O
during O
the O
evaluation O
. O
We O
evaluate O
our O
model O
with O
two O
datasets O
of O
Enron O
mail O
( O
Loza O
et O
al O
. O
, O
2014 O
) O
, O
corporate O
and O
private O
mails O
, O
and O
verify O
that O
our O
model O
outperforms O
baseline O
models O
. O
We O
also O
evaluated O
our O
model O
with O
Reddit O
TIFU O
dataset O
( O
Kim O
et O
al O
. O
, O
2019 O
) O
and O
achieved O
results O
competitive O
with O
those O
of O
the O
baseline O
models O
. O
Our O
model O
is O
based O
on O
a O
hypothesis O
that O
the O
ability O
of O
extracting O
quotes O
leads O
to O
a O
good O
result O
. O
Using O
the O
Reddit O
dataset O
where O
quotes O
are O
abundant O
, O
we O
obtain O
results O
that O
supports O
the O
hypothesis O
. O
Furthermore O
, O
we O
both O
quantitatively O
and O
qualitatively O
analyzed O
that O
our O
model O
can O
capture O
salient O
sentences O
that O
conventional O
frequency O
- O
based O
methods O
can O
not O
. O
The O
contributions O
of O
our O
research O
are O
as O
follows O
: O
‚Ä¢We O
veriÔ¨Åed O
that O
‚Äú O
the O
possibility O
of O
being O
quoted O
‚Äù O
is O
useful O
for O
summarization O
, O
and O
demonstrated O
that O
it O
reÔ¨Çects O
an O
important O
aspect O
of O
saliency O
that O
conventional O
methods O
do O
not.‚Ä¢We O
proposed O
an O
unsupervised O
extractive O
neural O
summarization O
model O
, O
Implicit O
Quote O
Extractor O
( O
IQE O
) O
, O
and O
demonstrated O
that O
the O
model O
outperformed O
or O
achieved O
results O
competitive O
to O
baseline O
models O
on O
two O
mail O
datasets O
and O
a O
Reddit O
dataset O
. O
‚Ä¢Using O
the O
Reddit O
dataset O
, O
we O
veriÔ¨Åed O
that O
quote O
extraction O
leads O
to O
a O
high O
performance O
of O
summarization O
. O
2 O
Related O
Works O
Summarization O
methods O
can O
be O
roughly O
grouped O
into O
two O
methods O
: O
extractive O
summarization O
and O
abstractive O
summarization O
. O
Most O
unsupervised O
summarization O
methods O
proposed O
are O
extractive O
methods O
. O
Despite O
the O
rise O
of O
neural O
networks O
, O
conventional O
non O
- O
neural O
methods O
are O
still O
powerful O
in O
the O
Ô¨Åeld O
of O
unsupervised O
extractive O
summarization O
. O
The O
graph O
- O
centrality O
- O
based O
method O
( O
Mihalcea O
and O
Tarau O
, O
2004 O
; O
Erkan O
and O
Radev O
, O
2004 O
; O
Zheng O
and O
Lapata O
, O
2019 O
) O
and O
centroid O
- O
based O
method O
( O
Gholipour O
Ghalandari O
, O
2017 O
) O
have O
been O
major O
methods O
in O
this O
Ô¨Åeld O
. O
Other O
models O
use O
reconstruction O
loss O
( O
He O
et O
al O
. O
, O
2012 O
; O
Liu O
et O
al O
. O
, O
2015 O
; O
Ma O
et O
al O
. O
, O
2016 O
) O
, O
Kullback O
- O
Leibler O
divergence O
( O
Haghighi O
and O
Vanderwende O
, O
2009 O
) O
or O
path O
score O
calculation O
( O
Mehdad O
et O
al O
. O
, O
2014 O
; O
Shang O
et O
al O
. O
, O
2018 O
) O
based O
on O
multi O
- O
sentence O
compression O
algorithm O
( O
Filippova O
, O
2010 O
) O
. O
These O
methods O
assume O
that O
important O
topics O
appear O
frequently O
in O
a O
document O
, O
but O
our O
model O
focuses O
on O
a O
different O
aspect O
of O
texts O
: O
the O
probability O
of O
being O
quoted O
. O
That O
is O
, O
our O
model O
can O
extract O
salient O
sentences O
that O
conventional O
methods O
fail O
to O
. O
A O
few O
neural O
- O
network O
- O
based O
unsupervised O
extractive O
summarization O
methods O
were O
proposed O
( O
KÀöageb O
¬®ack O
et O
al O
. O
, O
2014 O
; O
Yin O
and O
Pei O
, O
2015 O
; O
Ma O
et O
al O
. O
, O
2016 O
) O
. O
However O
, O
these O
methods O
use O
pretrained O
neural O
network O
models O
as O
a O
feature O
extractor O
, O
whereas O
we O
propose O
an O
end O
- O
to O
- O
end O
neural O
extractive O
summarization O
model O
. O
As O
for O
end O
- O
to O
- O
end O
unsupervised O
neural O
models O
, O
a O
few O
abstractive O
models O
have O
been O
proposed O
. O
For O
sentence O
compression O
, O
Fevry O
and O
Phang O
( O
2018 O
) O
employed O
the O
task O
to O
reorder O
the O
shufÔ¨Çed O
word O
order O
of O
sentences O
. O
Baziotis O
et O
al O
. O
( O
2019 O
) O
employed O
the O
reconstruction O
task O
of O
the O
original O
sentence O
from O
a O
compressed O
one O
. O
For O
review O
abstractive O
summarization O
, O
Isonuma O
et O
al O
. O
( O
2019 O
) O
revealed O
parent O
nodes O
of O
tree O
structures O
induce O
summaries O
, O
Chu O
and O
Liu292ÊöóÈªôÁöÑ„Å™ÂºïÁî®„ÅÆÂ≠¶ÁøíÔºöÊñáÊäΩÂá∫„Çí‰Ωø„ÅÜÊâãÊ≥ï O
! O
Post O
‚Ä¶ O
Reply O
CandidateCoattentionMatrixSplit O
to O
sentencesDecomposableAttentionPredict O
if O
Reply O
Candidate O
is O
an O
actual O
replyBiLSTMBiLSTMBiLSTMEncoderLSTMExtractor O
Extracted O
SentencesPredictor O
( O
only O
used O
during O
training O
) O
! O
" O
! O
! O
# O
! O
! O
$ O
! O
! O
" O
% O
& O
' O
! O
# O
% O
& O
' O
! O
( O
% O
& O
' O
‚Ä¶ O
! O
" O
) O
! O
# O
) O
! O
* O
) O
‚Ä¶ O
‚Ä¶ O
‚Ä¶ O
" O
" O
! O
Split O
to O
sentencesAttention O
& O
Gumbel O
Softmax#"%&'‚âì!+!#,%&'#(%&'‚âì!-!"#!"$!"")"#)"*)BiLSTMBiLSTMBiLSTM O
‚Ä¶ O
Figure O
2 O
: O
Description O
of O
our O
model O
, O
Implicit O
Quote O
Extractor O
( O
IQE O
) O
. O
The O
Extractor O
extracts O
sentences O
and O
uses O
them O
as O
summaries O
. O
kandjare O
indices O
of O
the O
extracted O
sentences O
. O
( O
2019 O
) O
generated O
summaries O
from O
mean O
vectors O
of O
review O
vectors O
, O
and O
Amplayo O
and O
Lapata O
( O
2020 O
) O
employed O
the O
prior O
distribution O
of O
Variational O
AutoEncoder O
to O
induce O
summaries O
. O
Another O
research O
employed O
a O
task O
to O
reconstruct O
masked O
sentences O
for O
summarization O
( O
Laban O
et O
al O
. O
, O
2020 O
) O
. O
Research O
on O
the O
summarization O
of O
online O
conversations O
such O
as O
mail O
, O
chat O
, O
social O
media O
, O
and O
online O
discussion O
fora O
has O
been O
conducted O
for O
a O
long O
time O
. O
Despite O
the O
rise O
of O
neural O
summarization O
models O
, O
most O
research O
on O
conversation O
summarization O
is O
based O
on O
non O
- O
neural O
models O
. O
A O
few O
used O
path O
scores O
of O
word O
graphs O
( O
Mehdad O
et O
al O
. O
, O
2014 O
; O
Shang O
et O
al O
. O
, O
2018 O
) O
. O
Dialogue O
act O
classiÔ¨Åcation O
is O
a O
classiÔ¨Åcation O
task O
that O
classiÔ¨Åes O
sentences O
depending O
on O
what O
their O
functions O
are O
( O
e.g. O
: O
questions O
, O
answers O
, O
greetings O
) O
, O
and O
has O
also O
been O
applied O
for O
summarization O
( O
Bhatia O
et O
al O
. O
, O
2014 O
; O
Oya O
and O
Carenini O
, O
2014 O
) O
. O
Quotes O
are O
also O
important O
factors O
of O
summarization O
. O
When O
we O
reply O
to O
a O
post O
or O
an O
email O
and O
when O
we O
want O
to O
emphasize O
a O
certain O
part O
of O
it O
, O
we O
quote O
the O
original O
text O
. O
A O
few O
studies O
used O
these O
quotes O
as O
features O
for O
summarization O
. O
Some O
previous O
work O
( O
Carenini O
et O
al O
. O
, O
2007 O
; O
Oya O
and O
Carenini O
, O
2014 O
) O
assigned O
weights O
to O
words O
that O
appeared O
in O
quotes O
, O
and O
improved O
the O
conventional O
centroidbased O
methods O
. O
The O
previous O
research O
used O
quotes O
as O
auxiliary O
features O
. O
In O
our O
research O
, O
we O
solely O
focus O
on O
quotes O
, O
and O
do O
not O
directly O
use O
quotes O
as O
supervision O
; O
rather O
, O
we O
aim O
to O
extract O
implicit O
quotes.3 O
Model O
We O
propose O
Implicit O
Quote O
Extractor O
( O
IQE O
) O
, O
an O
unsupervised O
extractive O
summarization O
model O
. O
Figure O
2 O
shows O
the O
structure O
of O
the O
model O
. O
The O
inputs O
to O
the O
model O
during O
training O
are O
a O
post O
and O
reply O
candidate O
. O
A O
reply O
candidate O
can O
be O
either O
a O
true O
or O
a O
false O
reply O
to O
the O
post O
. O
The O
training O
task O
of O
the O
model O
is O
to O
predict O
whether O
a O
reply O
candidate O
is O
true O
or O
not O
. O
The O
model O
comprises O
an O
Encoder O
, O
an O
Extractor O
, O
and O
a O
Predictor O
. O
The O
Encoder O
computes O
features O
of O
posts O
, O
the O
Extractor O
extracts O
sentences O
of O
a O
post O
to O
use O
for O
prediction O
, O
and O
the O
Predictor O
predicts O
whether O
a O
reply O
candidate O
is O
an O
actual O
reply O
or O
not O
. O
We O
describe O
each O
component O
below O
. O
Encoder O
The O
Encoder O
computes O
features O
of O
posts O
. O
First O
, O
the O
post O
is O
split O
into O
Nsentences O
{ O
sp O
1,sp O
2, O
... O
,sp O
N O
} O
. O
Each O
sentence O
sp O
icomprises O
KiwordsWp O
i={wp O
i1,wp O
i2, O
... O
,wp O
iK O
i O
} O
. O
Words O
are O
embedded O
to O
continuous O
vectors O
Xp O
i= O
{ O
xp O
i1,xp O
i2, O
... O
,xp O
iK O
i}through O
word O
embedding O
layers O
. O
We O
compute O
the O
features O
of O
each O
sentence O
hp O
iby O
inputting O
embedded O
vectors O
to O
Bidirectional O
Long O
Short O
- O
Term O
Memory O
( O
BiLSTM O
) O
and O
concatenating O
the O
last O
two O
hidden O
layers O
: O
hp O
i O
= O
BiLSTM O
( O
Xp O
i O
) O
( O
1 O
) O
Extractor O
The O
Extractor O
extracts O
a O
few O
sentences O
of O
a O
post O
for O
prediction O
. O
For O
accurate O
prediction O
, O
the O
Extractor O
learns O
to O
extract O
sentences O
that O
replies O
frequently O
refer O
to O
. O
Note O
that O
the O
Extractor O
does O
not O
use O
reply O
features O
for O
extraction O
. O
This O
is O
because293summaries O
should O
not O
depend O
on O
replies O
. O
IQE O
requires O
replies O
only O
during O
the O
training O
and O
can O
induce O
summaries O
without O
replies O
during O
the O
evaluation O
. O
We O
employ O
LSTM O
to O
sequentially O
compute O
features O
on O
the O
Extractor O
. O
We O
set O
the O
mean O
vector O
of O
the O
sentence O
features O
of O
the O
Encoder O
hp O
ias O
the O
initial O
hidden O
state O
of O
the O
Extractor O
hext O
0 O
. O
hext O
0=1 O
NN O
/ O
summationdisplay O
i=1hp O
i O
( O
2 O
) O
The O
Extractor O
computes O
attention O
weights O
using O
the O
hidden O
states O
of O
the O
Extractor O
hext O
tand O
the O
sentence O
featureshp O
icomputed O
on O
the O
Encoder O
. O
The O
sentence O
with O
the O
highest O
attention O
weight O
is O
extracted O
. O
During O
the O
training O
, O
we O
use O
Gumbel O
Softmax O
( O
Jang O
et O
al O
. O
, O
2017 O
) O
to O
make O
this O
discrete O
process O
differentiable O
. O
By O
adding O
Gumbel O
noise O
gusing O
noise O
u O
from O
a O
uniform O
distribution O
, O
the O
attention O
weights O
a O
become O
a O
one O
- O
hot O
vector O
. O
The O
discretized O
attention O
weightsŒ±are O
computed O
as O
follows O
: O
ui‚àºUniform O
( O
0,1 O
) O
( O
3 O
) O
gi=‚àílog O
( O
‚àílogui O
) O
( O
4 O
) O
ati O
= O
cTtanh(hext O
t+hp O
i O
) O
( O
5 O
) O
œÄti O
= O
expati O
/ O
summationtextN O
k=1expatk(6 O
) O
Œ±ti O
= O
exp O
( O
logœÄti+gi)/œÑ O
/ O
summationtextN O
k=1exp O
( O
logœÄtk+gk)/œÑ(7 O
) O
cis O
a O
parameter O
vector O
, O
and O
the O
temperature O
œÑis O
set O
to O
0.1 O
. O
We O
input O
the O
linear O
sum O
of O
the O
attention O
weightsŒ±and O
the O
sentence O
vectors O
hp O
ito O
LSTM O
and O
update O
the O
hidden O
state O
of O
the O
Extractor O
. O
We O
repeat O
this O
step O
Ltimes O
. O
xext O
t O
= O
N O
/ O
summationdisplay O
i=1Œ±tihp O
i O
( O
1‚â§t‚â§L)(8 O
) O
hext O
t+1 O
= O
LSTM O
( O
xext O
t O
) O
( O
0‚â§t‚â§L‚àí1 O
) O
( O
9 O
) O
The O
initial O
input O
vector O
xext O
0of O
the O
Extractor O
is O
a O
parameter O
, O
and O
Lis O
deÔ¨Åned O
by O
a O
user O
depending O
on O
the O
number O
of O
sentences O
required O
for O
a O
summary O
. O
Predictor O
Then O
, O
using O
only O
the O
extracted O
sentences O
and O
a O
reply O
candidate O
, O
the O
Predictor O
predicts O
whether O
the O
candidate O
is O
an O
actual O
reply O
or O
not O
. O
We O
labeled O
actual O
replies O
as O
positive O
, O
and O
randomly O
sampled O
posts O
as O
negative O
. O
Suppose O
a O
reply O
candidateR={sr O
1,sr O
2, O
... O
,sr O
M}hasMsentences O
. O
Sentence O
vectors{hr O
j}of O
each O
sentence{sr O
j}on O
the O
reply O
are O
computed O
similarly O
to O
the O
equation O
1 O
. O
To O
compute O
the O
relation O
between O
the O
post O
and O
the O
reply O
candidate O
, O
we O
employ O
Decomposable O
Attention O
( O
Parikh O
et O
al O
. O
, O
2016 O
) O
. O
From O
this O
architecture O
, O
we O
obtain O
the O
probability O
of O
binary O
- O
classiÔ¨Åcation O
ythrough O
the O
sigmoid O
function O
. O
y O
= O
sigmoid O
( O
DA(xext O
1, O
... O
,xext O
L‚àí1,hr O
1, O
... O
,hr O
M O
) O
) O
( O
10 O
) O
where O
DA O
denotes O
Decomposable O
Attention O
. O
The O
detail O
of O
the O
computation O
is O
described O
in O
Appendix O
A.1 O
. O
Decomposable O
Attention O
. O
The O
loss O
of O
this O
classiÔ¨Åcation O
Lrepis O
obtained O
by O
cross O
entropy O
as O
follows O
where O
trepis O
1 O
when O
a O
reply O
candidate O
is O
an O
actual O
reply O
, O
and O
otherwise O
0 O
. O
Lrep=‚àítreplogy‚àí(1‚àítrep O
) O
log O
( O
1‚àíy O
) O
( O
11 O
) O
Reranking O
As O
we O
mentioned O
in O
the O
Introduction O
, O
we O
are O
seeking O
for O
a O
criterion O
that O
is O
different O
from O
conventional O
methods O
. O
To O
take O
advantage O
of O
our O
method O
and O
conventional O
methods O
, O
we O
employ O
reranking O
; O
we O
simply O
reorder O
summaries O
( O
3 O
sentences O
) O
extracted O
by O
our O
model O
based O
on O
the O
ranking O
of O
TextRank O
( O
Mihalcea O
and O
Tarau O
, O
2004 O
) O
. O
4 O
Experiment O
We O
train O
and O
evaluate O
the O
model O
on O
two O
domains O
of O
datasets O
. O
One O
is O
a O
mail O
dataset O
, O
and O
the O
other O
is O
a O
dataset O
from O
the O
social O
media O
platform O
, O
Reddit O
. O
4.1 O
Mail O
Dataset O
We O
use O
Avocado O
collection1for O
the O
training O
. O
The O
Avocado O
collection O
is O
a O
public O
dataset O
that O
comprises O
emails O
obtained O
from O
279 O
custodians O
of O
a O
defunct O
information O
technology O
company O
. O
From O
this O
dataset O
, O
we O
use O
post O
- O
and O
- O
reply O
pairs O
to O
train O
our O
model O
. O
We O
exclude O
pairs O
where O
the O
number O
of O
words O
in O
a O
post O
or O
a O
reply O
is O
smaller O
than O
50 O
or O
25 O
. O
After O
the O
preprocessing O
, O
we O
have O
56,174 O
pairs O
. O
We O
labeled O
a O
pair O
with O
an O
actual O
reply O
as O
positive O
and O
a O
pair O
with O
a O
wrong O
reply O
that O
is O
randomly O
sampled O
from O
the O
whole O
dataset O
as O
negative O
. O
The O
number O
of O
positive O
labels O
and O
negative O
labels O
are O
equal O
. O
Therefore O
, O
we O
have O
112,348 O
pairs O
in O
total O
. O
For O
evaluation O
, O
we O
employ O
the O
Enron O
Summarization O
dataset O
( O
Loza O
et O
al O
. O
, O
2014 O
) O
. O
This O
dataset O
1https://catalog.ldc.upenn.edu/LDC2015T03294DataSample O
sizeSummary O
Source O
# O
of O
references O
# O
of O
sentences O
# O
of O
words O
# O
of O
sentences O
# O
of O
words O
# O
of O
words O
per O
sentence O
ECS O
109 O
2 O
4.7 O
78.0 O
11.0 O
179.4 O
16.3 O
EPS O
103 O
2 O
5.8 O
88.0 O
19.3 O
217.1 O
11.2 O
tldr O
3000 O
1 O
1.3 O
19.7 O
15.1 O
311.9 O
20.7 O
Table O
1 O
: O
Overview O
of O
the O
evaluation O
datasets O
. O
has O
two O
types O
of O
evaluation O
datasets O
: O
ECS O
( O
Enron O
Corporate O
Single O
) O
and O
EPS O
( O
Enron O
Personal O
Single O
) O
. O
An O
overview O
of O
these O
datasets O
is O
summarized O
in O
Table O
1 O
. O
Because O
the O
evaluation O
datasets O
do O
not O
have O
validation O
datasets O
, O
we O
use O
the O
ECS O
dataset O
as O
a O
validation O
dataset O
for O
the O
EPS O
dataset O
, O
and O
vice O
versa O
. O
We O
use O
the O
validation O
datasets O
to O
decide O
which O
model O
to O
use O
for O
the O
evaluation O
. O
4.2 O
Reddit O
TIFU O
Dataset O
The O
Reddit O
TIFU O
dataset O
( O
Kim O
et O
al O
. O
, O
2019 O
) O
is O
a O
dataset O
that O
leverages O
tldr O
tags O
for O
the O
summarization O
task O
, O
which O
is O
the O
abbreviation O
of O
‚Äú O
too O
long O
did O
n‚Äôt O
read O
‚Äù O
. O
On O
the O
discussion O
forum O
Reddit O
TIFU O
, O
users O
post O
a O
tldr O
along O
with O
the O
post O
. O
tldr O
brieÔ¨Çy O
explains O
what O
is O
written O
in O
the O
original O
post O
and O
thus O
can O
be O
regarded O
as O
a O
summary O
. O
We O
preprocess O
the O
TIFU O
dataset O
similarly O
as O
the O
mail O
datasets O
. O
Because O
the O
TIFU O
dataset O
does O
not O
include O
replies O
, O
we O
collected O
replies O
of O
the O
posts O
included O
in O
the O
TIFU O
dataset O
using O
praw2 O
. O
As O
a O
consequence O
, O
we O
obtained O
183,500 O
correct O
pairs O
of O
posts O
and O
replies O
and O
the O
same O
number O
of O
wrong O
pairs O
. O
We O
use O
that O
367,000 O
pairs O
of O
posts O
and O
replies O
as O
the O
training O
dataset O
. O
We O
use O
3,000 O
posts O
and O
tldrs O
that O
are O
not O
included O
in O
the O
training O
dataset O
as O
the O
validation O
dataset O
, O
and O
the O
same O
number O
of O
posts O
and O
tldrs O
as O
the O
evaluation O
dataset O
. O
An O
overview O
of O
the O
TIFU O
evaluation O
dataset O
is O
also O
summarized O
in O
Table O
1 O
. O
4.3 O
Training O
The O
dimensions O
of O
the O
embedding O
layers O
and O
hidden O
layers O
of O
the O
LSTM O
are O
100 O
. O
The O
size O
of O
the O
vocabulary O
is O
set O
to O
30,000 O
. O
We O
tokenize O
each O
email O
or O
post O
into O
sentences O
and O
each O
sentence O
into O
words O
using O
the O
nltk O
tokenizer3 O
. O
The O
upper O
limit O
of O
the O
number O
of O
sentences O
is O
set O
to O
30 O
, O
and O
that O
of O
words O
in O
each O
sentence O
is O
set O
to O
200 O
. O
The O
epoch O
size O
is O
10 O
, O
and O
we O
use O
Adam O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
as O
an O
optimizer O
. O
In O
the O
Ô¨Årst O
few O
epochs O
, O
we O
do O
not O
use O
the O
Extractor O
; O
all O
the O
post O
sentences O
are O
used O
for O
the O
prediction O
2https://praw.readthedocs.io/ O
3https://www.nltk.orgof O
post O
- O
reply O
relations O
. O
This O
is O
to O
train O
the O
Extractor O
and O
the O
Predictor O
efÔ¨Åciently O
. O
The O
Extractor O
learns O
to O
extract O
proper O
sentences O
and O
the O
Predictor O
learns O
to O
predict O
the O
relation O
between O
a O
post O
and O
a O
reply O
candidate O
. O
Models O
with O
several O
components O
generally O
achieve O
better O
results O
if O
each O
component O
is O
pretrained O
separately O
( O
Hashimoto O
et O
al O
. O
, O
2017 O
) O
. O
Thus O
, O
we O
train O
the O
Predictor O
in O
the O
Ô¨Årst O
few O
epochs O
before O
training O
the O
Extractor O
. O
We O
set O
this O
threshold O
as O
4 O
. O
During O
training O
, O
L O
, O
the O
number O
of O
sentences O
the O
Extractor O
extracts O
is O
randomly O
set O
from O
1 O
to O
4 O
, O
so O
that O
the O
model O
can O
extract O
an O
arbitrary O
number O
of O
sentences O
. O
We O
replace O
the O
named O
entities O
on O
the O
text O
data O
with O
tags O
( O
person O
, O
location O
, O
and O
organization O
) O
using O
the O
Stanford O
Named O
Entity O
Recognizer O
( O
NER)4 O
, O
to O
prevent O
the O
model O
from O
simply O
using O
named O
entities O
as O
a O
hint O
for O
the O
prediction O
. O
We O
pretrain O
word O
embeddings O
of O
the O
model O
with O
Skipgram O
, O
using O
the O
same O
data O
as O
the O
training O
. O
We O
conduct O
the O
same O
experiment O
Ô¨Åve O
times O
and O
use O
the O
average O
of O
the O
results O
to O
mitigate O
the O
effect O
of O
randomness O
rooting O
in O
initialization O
and O
optimization O
. O
4.4 O
Evaluation O
In O
the O
evaluation O
phase O
, O
we O
only O
use O
the O
Encoder O
and O
Extractor O
and O
do O
not O
use O
the O
Predictor O
. O
Each O
model O
extracts O
3 O
sentences O
as O
a O
summary O
. O
Following O
previous O
work O
, O
we O
report O
the O
average O
F1 O
of O
ROUGE-1 O
, O
ROUGE-2 O
, O
and O
ROUGE O
- O
L O
for O
the O
evaluation O
( O
Lin O
, O
2004 O
) O
. O
We O
use O
the O
Ô¨Årst O
20 O
, O
40 O
, O
and O
60 O
words O
of O
the O
extracted O
sentences O
. O
For O
ROUGE O
computation O
, O
we O
use O
ROUGE O
2.0 O
( O
Ganesan O
, O
2015 O
) O
. O
As O
a O
validation O
metric O
, O
we O
use O
an O
average O
of O
ROUGE1 O
- O
F O
, O
ROUGE-2 O
- O
F O
, O
and O
ROUGE O
- O
L O
- O
F. O
4.5 O
Baseline O
As O
baseline O
models O
, O
we O
employ O
TextRank O
( O
Mihalcea O
and O
Tarau O
, O
2004 O
) O
, O
LexRank O
( O
Erkan O
and O
Radev O
, O
2004 O
) O
, O
KLSum O
( O
Haghighi O
and O
Vanderwende O
, O
2009 O
) O
, O
PacSum O
( O
Zheng O
and O
Lapata O
, O
2019 O
) O
, O
Lead O
, O
and O
Random O
. O
TextRank O
and O
LexRank O
are O
graph O
- O
centrality O
based O
methods O
that O
have O
long O
been O
considered O
as O
4https://nlp.stanford.edu/software/CRF-NER.shtml295ModelROUGE-1-F O
ROUGE-2 O
- O
F O
ROUGE O
- O
L O
- O
F O
# O
of O
words O
# O
of O
words O
# O
of O
words O
20 O
40 O
60 O
20 O
40 O
60 O
20 O
40 O
60 O
Lead O
0.217 O
0.351 O
0.413 O
0.115 O
0.198 O
0.240 O
0.212 O
0.290 O
0.321 O
TextRank O
0.231 O
0.365 O
0.434 O
0.123 O
0.199 O
0.243 O
0.223 O
0.294 O
0.336 O
LexRank O
0.234 O
0.359 O
0.423 O
0.127 O
0.199 O
0.240 O
0.220 O
0.290 O
0.323 O
Random O
0.193 O
0.317 O
0.365 O
0.089 O
0.163 O
0.190 O
0.199 O
0.285 O
0.303 O
KLSum O
0.235 O
0.344 O
0.383 O
0.125 O
0.183 O
0.204 O
0.220 O
0.273 O
0.303 O
PacSum O
0.230 O
0.367 O
0.435 O
0.125 O
0.211 O
0.256 O
0.220 O
0.287 O
0.326 O
IQETextRank O
0.213 O
0.336 O
0.394 O
0.104 O
0.172 O
0.208 O
0.211 O
0.287 O
0.315 O
IQE O
0.241 O
0.374 O
0.445 O
0.130 O
0.206 O
0.251 O
0.220 O
0.292 O
0.333 O
IQE O
+ O
reranking O
0.242 O
0.374 O
0.443 O
0.131 O
0.207 O
0.246 O
0.227 O
0.298 O
0.332 O
Table O
2 O
: O
Results O
on O
ECS O
data O
. O
The O
best O
results O
are O
bolded O
and O
the O
second O
best O
results O
are O
underlined O
. O
ModelROUGE-1 O
- O
F O
ROUGE-2 O
- O
F O
ROUGE O
- O
L O
- O
F O
# O
of O
words O
# O
of O
words O
# O
of O
words O
20 O
40 O
60 O
20 O
40 O
60 O
20 O
40 O
60 O
Lead O
0.128 O
0.204 O
0.230 O
0.045 O
0.084 O
0.099 O
0.150 O
0.208 O
0.221 O
TextRank O
0.172 O
0.272 O
0.317 O
0.080 O
0.129 O
0.151 O
0.185 O
0.260 O
0.290 O
LexRank O
0.161 O
0.254 O
0.299 O
0.068 O
0.113 O
0.136 O
0.173 O
0.245 O
0.275 O
Random O
0.144 O
0.213 O
0.238 O
0.058 O
0.086 O
0.099 O
0.158 O
0.213 O
0.232 O
KLSum O
0.191 O
0.287 O
0.321 O
0.093 O
0.141 O
0.153 O
0.184 O
0.254 O
0.277 O
PacSum O
0.179 O
0.275 O
0.330 O
0.082 O
0.127 O
0.151 O
0.171 O
0.250 O
0.287 O
IQETextRank O
0.158 O
0.252 O
0.291 O
0.069 O
0.115 O
0.136 O
0.169 O
0.242 O
0.268 O
IQE O
0.189 O
0.292 O
0.342 O
0.091 O
0.143 O
0.168 O
0.189 O
0.268 O
0.302 O
IQE O
+ O
reranking O
0.185 O
0.290 O
0.340 O
0.087 O
0.138 O
0.164 O
0.189 O
0.264 O
0.299 O
Table O
3 O
: O
Results O
on O
EPS O
data O
. O
The O
best O
results O
are O
bolded O
and O
the O
second O
best O
results O
are O
underlined O
. O
strong O
methods O
for O
unsupervised O
summarization O
. O
PacSum O
is O
an O
improved O
model O
of O
TextRank O
, O
which O
harnesses O
the O
position O
of O
sentences O
as O
a O
feature O
. O
KLSum O
employs O
the O
Kullbuck O
‚Äì O
Leibler O
divergence O
to O
constrain O
extracted O
sentences O
and O
the O
source O
text O
to O
have O
the O
similar O
word O
distribution O
. O
Lead O
is O
a O
simple O
method O
that O
extracts O
the O
Ô¨Årst O
few O
sentences O
from O
the O
source O
text O
but O
is O
considered O
as O
a O
strong O
baseline O
for O
the O
summarization O
of O
news O
articles O
. O
PacSum O
and O
LexRank O
leverage O
idf O
. O
We O
compute O
idf O
using O
the O
validation O
data O
. O
As O
another O
baseline O
, O
we O
employ O
IQETextRank O
; O
the O
TextRank O
model O
that O
leverages O
cosine O
similarities O
of O
sentence O
vectors O
of O
IQE O
‚Äôs O
Encoder O
as O
similarities O
between O
sentences O
. O
This O
is O
added O
to O
verify O
that O
the O
success O
of O
our O
model O
is O
not O
only O
because O
our O
model O
uses O
neural O
networks O
. O
5 O
Results O
and O
Discussion O
Experimental O
results O
for O
each O
evaluation O
dataset O
are O
listed O
in O
Table O
2 O
, O
3 O
and O
4 O
. O
Our O
model O
outperforms O
baseline O
models O
on O
the O
mail O
datasets O
( O
ECS O
and O
EPS O
) O
in O
most O
metrics O
. O
On O
Reddit O
TIFU O
dataset O
, O
IQE O
with O
reranking O
outperforms O
most O
baseline O
models O
except O
TextRank O
. O
Reranking O
improves O
the O
accuracy O
on O
ECS O
and O
TIFU O
but O
not O
on O
EPS O
. O
PacSum O
signiÔ¨Åcantly O
outperformed O
TextRank O
onthe O
news O
article O
dataset O
( O
Zheng O
and O
Lapata O
, O
2019 O
) O
but O
does O
not O
work O
well O
on O
our O
datasets O
where O
the O
sentence O
position O
is O
not O
an O
important O
factor O
. O
IQETextRank O
performed O
worse O
than O
IQE O
with O
the O
mail O
datasets O
. O
This O
indicates O
that O
the O
performance O
of O
our O
model O
does O
not O
result O
from O
the O
use O
of O
neural O
networks O
. O
Our O
model O
outperforms O
the O
baseline O
models O
more O
with O
the O
EPS O
dataset O
than O
the O
ECS O
dataset O
. O
The O
overview O
of O
the O
datasets O
in O
Table O
1 O
explains O
the O
reason O
. O
The O
average O
number O
of O
words O
each O
sentence O
has O
is O
smaller O
in O
EPS O
. O
Baseline O
models O
such O
as O
LexRank O
and O
TextRank O
compute O
similarity O
of O
sentences O
using O
the O
co O
- O
occurrence O
of O
words O
. O
Thus O
, O
if O
the O
lengths O
of O
sentences O
are O
short O
, O
it O
fails O
to O
build O
decent O
co O
- O
occurrence O
networks O
and O
to O
capture O
the O
saliency O
of O
the O
sentences O
. O
IQE O
did O
not O
outperform O
TextRank O
on O
TIFU O
dataset O
. O
It O
is O
conceivable O
that O
Reddit O
users O
are O
less O
likely O
to O
refer O
to O
important O
topics O
on O
the O
post O
, O
given O
that O
anyone O
can O
reply O
. O
5.1 O
The O
Performance O
of O
Summarization O
and O
Quote O
Extraction O
Our O
model O
performed O
well O
on O
the O
Mail O
datasets O
but O
two O
questions O
remain O
unclear O
. O
First O
, O
because O
we O
did O
not O
use O
quotes O
as O
supervision O
, O
it O
is O
not O
clear O
how O
well O
our O
model O
extracts O
quotes O
. O
Second O
, O
following296ModelROUGE-1 O
- O
F O
ROUGE-2 O
- O
F O
ROUGE O
- O
L O
- O
F O
# O
of O
words O
# O
of O
words O
# O
of O
words O
20 O
40 O
60 O
20 O
40 O
60 O
20 O
40 O
60 O
Lead O
0.128 O
0.150 O
0.149 O
0.017 O
0.023 O
0.024 O
0.107 O
0.122 O
0.125 O
TextRank O
0.161 O
0.179 O
0.173 O
0.027 O
0.034 O
0.035 O
0.126 O
0.140 O
0.142 O
LexRank O
0.149 O
0.165 O
0.163 O
0.021 O
0.026 O
0.029 O
0.119 O
0.131 O
0.134 O
Random O
0.136 O
0.156 O
0.158 O
0.018 O
0.024 O
0.026 O
0.112 O
0.128 O
0.131 O
KL O
- O
Sum O
0.142 O
0.159 O
0.157 O
0.020 O
0.026 O
0.029 O
0.115 O
0.127 O
0.131 O
PacSum O
0.143 O
0.161 O
0.161 O
0.021 O
0.026 O
0.028 O
0.117 O
0.132 O
0.135 O
IQETextRank O
0.152 O
0.169 O
0.166 O
0.023 O
0.030 O
0.032 O
0.122 O
0.136 O
0.139 O
IQE O
0.153 O
0.172 O
0.169 O
0.024 O
0.031 O
0.033 O
0.122 O
0.136 O
0.139 O
IQE O
+ O
reranking O
0.161 O
0.177 O
0.171 O
0.026 O
0.033 O
0.034 O
0.126 O
0.138 O
0.139 O
Table O
4 O
: O
Results O
on O
TIFU O
tldr O
data O
. O
The O
best O
results O
are O
bolded O
and O
the O
second O
best O
results O
are O
underlined O
. O
Model O
MRR O
LexRank O
0.094 O
TextRank O
0.109 O
Random O
0.081 O
IQE O
0.135 O
Table O
5 O
: O
Ability O
of O
extracting O
quotes O
. O
Model O
ROUGE-1 O
- O
F O
ROUGE-2 O
- O
F O
ROUGE O
- O
L O
- O
F O
IQEquote O
0.184 O
0.030 O
0.126 O
IQEnonquote O
0.168 O
0.020 O
0.118 O
Table O
6 O
: O
ROUGE O
scores O
of O
extracted O
sentences O
that O
coincide O
with O
quote O
( O
IQEquote O
) O
and O
that O
does O
not O
coincide O
with O
quotes O
( O
IQEnonquote O
) O
. O
The O
ROUGE O
scores O
become O
higher O
when O
IQE O
succeeded O
in O
extracting O
quotes O
. O
Carenini O
‚Äôs O
work O
( O
Carenini O
et O
al O
. O
, O
2007 O
; O
Oya O
and O
Carenini O
, O
2014 O
) O
, O
we O
assumed O
quotes O
were O
useful O
for O
summarization O
but O
it O
is O
not O
clear O
whether O
the O
quote O
extraction O
leads O
to O
better O
results O
of O
summarization O
. O
To O
answer O
these O
questions O
, O
we O
conduct O
two O
experiments O
. O
For O
the O
experiments O
, O
we O
use O
the O
Reddit O
TIFU O
dataset O
and O
replies O
extracted O
via O
praw O
as O
described O
in O
4.2 O
. O
From O
the O
dataset O
, O
we O
extract O
replies O
that O
contain O
quotes O
, O
which O
start O
with O
the O
symbol O
‚Äú O
> O
‚Äù O
. O
In O
total O
, O
1,969 O
posts O
have O
replies O
that O
include O
quotes O
. O
We O
label O
sentences O
of O
the O
posts O
that O
are O
quoted O
by O
the O
replies O
and O
verify O
how O
accurately O
our O
model O
can O
extract O
the O
quoted O
sentences O
. O
How O
well O
our O
model O
extracts O
quotes O
? O
To O
assess O
the O
ability O
of O
quote O
extraction O
, O
we O
regard O
the O
extraction O
of O
quotes O
as O
an O
information O
retrieval O
task O
and O
evaluate O
with O
Mean O
Reciprocal O
Rank O
( O
MRR O
) O
. O
We O
compute O
MRR O
as O
follows O
. O
MRR O
= O
/braceleftbigg1 O
R(q)(R(q)‚â§4 O
) O
0 O
( O
R(q)>4)(12 O
) O
The O
function O
Rdenotes O
the O
rank O
of O
the O
saliency O
scores O
a O
model O
computes O
; O
our O
model O
does O
not O
compute O
the O
scores O
but O
sequentially O
extracts O
sentences O
, O
and O
the O
order O
is O
regarded O
as O
the O
rank O
here O
. O
If O
a O
model O
extracts O
quotes O
as O
salient O
sentences O
, O
the O
rank O
becomes O
higher O
. O
Therefore O
, O
the O
MRR O
in O
our O
study O
indicates O
the O
capability O
of O
a O
model O
to O
extract O
quotes O
. O
As O
explained O
in O
the O
section O
4.3 O
, O
we O
trained O
our O
model O
to O
extract O
up O
to O
four O
sentences O
. O
Thus O
we O
setthe O
threshold O
at O
four O
; O
if O
R(q)is O
larger O
than O
4 O
we O
set O
MRR O
0 O
. O
For O
each O
data O
, O
we O
compute O
MRR O
and O
use O
the O
mean O
value O
as O
a O
result O
. O
Table O
5 O
shows O
the O
results O
. O
IQE O
is O
more O
likely O
to O
extract O
quotes O
than O
TextRank O
, O
LexRank O
and O
Random O
. O
Does O
extracting O
quotes O
lead O
to O
good O
summarization O
? O
Next O
, O
we O
validate O
whether O
the O
ROUGE O
scores O
become O
better O
when O
our O
model O
succeeded O
in O
extracting O
quotes O
. O
We O
compute O
ROUGE O
scores O
when O
our O
model O
succeeds O
or O
fails O
in O
quote O
extraction O
( O
which O
means O
when O
MRR O
equals O
1 O
or O
otherwise O
) O
. O
IQEquote O
indicates O
the O
data O
where O
the O
extracted O
sentence O
coincides O
with O
a O
quote O
, O
and O
IQEnonquote O
vice O
versa O
. O
The O
result O
in O
the O
Table O
6 O
shows O
ROUGE O
scores O
are O
higher O
when O
the O
extracted O
sentence O
coincides O
with O
a O
quote O
. O
The O
results O
of O
the O
two O
analyses O
support O
the O
claim O
that O
our O
model O
is O
more O
likely O
to O
extract O
quotes O
and O
that O
the O
ability O
of O
extracting O
quotes O
leads O
to O
better O
summarization O
. O
5.2 O
Ablation O
Tests O
Effect O
of O
replacing O
named O
entities O
As O
explained O
in O
the O
section O
4.3 O
, O
our O
models O
shown O
in O
Tables O
2 O
, O
3 O
and O
4 O
all O
use O
the O
Stanford O
NER O
. O
To O
validate O
the O
effect O
of O
NER O
, O
we O
experiment O
without O
replacing O
named O
entities O
. O
Table O
7 O
lists O
the O
results O
. O
The O
table O
indicates O
that O
replacing O
named O
entities O
improves O
the O
performance O
on O
the O
mail O
datasets O
. O
This O
is O
because O
names O
of O
people O
, O
locations O
, O
and O
organizations O
can O
be O
signiÔ¨Åcant O
hints O
for O
distinguishing297Dataset O
ModelROUGE-1 O
- O
F O
ROUGE-2 O
- O
F O
ROUGE O
- O
L O
- O
F O
# O
of O
words O
# O
of O
words O
# O
of O
words O
20 O
40 O
60 O
20 O
40 O
60 O
20 O
40 O
60 O
ECSIQE O
0.241 O
0.374 O
0.445 O
0.130 O
0.206 O
0.251 O
0.220 O
0.292 O
0.333 O
IQE O
w/o O
NER O
0.215 O
0.351 O
0.424 O
0.110 O
0.189 O
0.237 O
0.208 O
0.290 O
0.329 O
IQE O
w/o O
Pretraining O
0.223 O
0.355 O
0.420 O
0.113 O
0.190 O
0.231 O
0.210 O
0.288 O
0.323 O
EPSIQE O
0.189 O
0.292 O
0.342 O
0.091 O
0.143 O
0.168 O
0.189 O
0.268 O
0.302 O
IQE O
w/o O
NER O
0.170 O
0.271 O
0.312 O
0.076 O
0.127 O
0.149 O
0.188 O
0.268 O
0.295 O
IQE O
w/o O
Pretraining O
0.176 O
0.274 O
0.318 O
0.078 O
0.124 O
0.147 O
0.186 O
0.260 O
0.291 O
TIFUIQE O
0.153 O
0.172 O
0.169 O
0.024 O
0.031 O
0.033 O
0.122 O
0.136 O
0.139 O
IQE O
w/o O
NER O
0.154 O
0.172 O
0.170 O
0.024 O
0.030 O
0.033 O
0.122 O
0.136 O
0.139 O
IQE O
w/o O
Pretraining O
0.143 O
0.161 O
0.160 O
0.020 O
0.027 O
0.029 O
0.116 O
0.131 O
0.133 O
Table O
7 O
: O
Results O
of O
ablation O
tests O
correct O
replies O
. O
For O
example O
, O
if O
a O
post O
and O
a O
reply O
candidate O
refer O
to O
the O
same O
person O
‚Äôs O
name O
, O
the O
model O
extracts O
sentences O
that O
contain O
the O
person O
‚Äôs O
name O
. O
The O
replacement O
of O
named O
entities O
encourages O
the O
model O
to O
extract O
sentences O
semantically O
relevant O
to O
replies O
rather O
than O
simply O
extracting O
sentences O
that O
include O
named O
entities O
. O
However O
, O
on O
the O
Reddit O
TIFU O
dataset O
, O
NER O
did O
not O
affect O
the O
accuracy O
. O
Reddit O
is O
an O
anonymized O
social O
media O
platform O
, O
and O
the O
posts O
are O
less O
likely O
to O
refer O
to O
people O
‚Äôs O
names O
. O
Thus O
, O
named O
entities O
will O
not O
be O
hints O
to O
predict O
reply O
- O
relation O
. O
Effect O
of O
pretraining O
Predictor O
As O
explained O
in O
the O
section O
4.3 O
, O
we O
pretrained O
the O
Predictor O
in O
the O
Ô¨Årst O
few O
epochs O
so O
that O
the O
model O
can O
learn O
the O
extraction O
and O
the O
prediction O
separately O
. O
Table O
7 O
shows O
the O
effect O
of O
pretraining O
. O
Without O
pretraining O
, O
the O
accuracy O
decreased O
. O
This O
shows O
the O
importance O
of O
the O
separate O
training O
of O
each O
component O
. O
5.3 O
Difference O
from O
Conventional O
Methods O
As O
explained O
in O
the O
Introduction O
, O
most O
conventional O
unsupervised O
summarization O
methods O
are O
based O
on O
the O
assumption O
that O
important O
topics O
appear O
frequently O
in O
a O
document O
. O
TextRank O
is O
a O
typical O
example O
; O
TextRank O
is O
a O
centrality O
- O
based O
method O
that O
extracts O
sentences O
with O
high O
PageRank O
as O
the O
summary O
. O
A O
sentence O
having O
high O
PageRank O
indicates O
that O
the O
sentence O
has O
high O
similarity O
with O
many O
other O
sentences O
, O
meaning O
that O
many O
sentences O
refer O
to O
the O
same O
topic O
. O
We O
suspected O
that O
important O
topics O
are O
not O
always O
referred O
to O
frequently O
, O
and O
suggested O
another O
criterion O
: O
the O
frequency O
of O
being O
referred O
to O
in O
replies O
. O
Comparing O
with O
TextRank O
, O
we O
verify O
that O
our O
method O
can O
capture O
salient O
sentences O
that O
the O
centrality O
- O
based O
method O
fails O
to O
. O
Figure O
3 O
shows O
the O
correlation O
between O
the O
maximum O
PageRank O
in O
each O
post O
of O
ECS O
/ O
EPS O
and O
ROUGE-1 O
- O
F O
scores O
Figure O
3 O
: O
Correlation O
between O
ROUGE-1 O
- O
F O
score O
and O
maximum O
PageRank O
of O
each O
post O
on O
ECS O
and O
EPS O
datasets O
. O
X O
- O
axis O
shows O
rounded O
maximum O
PageRank O
, O
and O
Y O
- O
axis O
shows O
ROUGE-1 O
- O
F O
and O
the O
error O
bar O
represents O
the O
standard O
error O
. O
of O
IQE O
and O
TextRank O
. O
As O
shown O
in O
the O
Figure O
, O
the O
ROUGE-1 O
- O
F O
scores O
of O
our O
model O
are O
higher O
than O
those O
of O
TextRank O
when O
the O
maximum O
PageRank O
in O
the O
sentence O
- O
similarity O
graph O
is O
low O
. O
This O
supports O
our O
hypothesis O
that O
our O
model O
can O
capture O
salient O
sentences O
even O
when O
the O
important O
topic O
is O
referred O
to O
only O
few O
times O
. O
Table O
8 O
shows O
a O
demonstrative O
example O
of O
extracted O
summaries O
of O
IQE O
and O
TextRank O
. O
The O
sample O
is O
from O
the O
EPS O
dataset O
. O
The O
summary O
includes O
descriptions O
regarding O
a O
promotion O
and O
that O
the O
sender O
is O
having O
a O
baby O
. O
However O
, O
those O
words298Source O
Text O
Just O
got O
your O
email O
address O
from O
Rachel O
. O
Congrats O
on O
your O
promotion O
. O
I O
‚Äôm O
sure O
it O
‚Äôs O
going O
to O
be O
alot O
different O
for O
you O
but O
it O
sounds O
like O
a O
great O
deal O
. O
My O
hubby O
and O
‚Äô O
I O
moved O
out O
to O
Katy O
a O
few O
months O
ago O
. O
I O
love O
it O
there O
- O
my O
parents O
live O
about O
10 O
minutes O
away O
. O
New O
news O
from O
me O
- O
I O
‚Äôm O
having O
a O
baby O
- O
due O
in O
June O
. O
I O
ca O
n‚Äôt O
even O
believe O
it O
myself O
. O
The O
thought O
of O
me O
being O
a O
mother O
is O
downright O
scary O
but O
I O
Ô¨Ågure O
since O
I O
‚Äôm O
almost O
30 O
, O
I O
probably O
need O
to O
start O
growing O
up O
. O
I O
‚Äôm O
really O
excited O
though O
. O
Rachel O
is O
coming O
to O
visit O
me O
in O
a O
couple O
of O
weeks O
. O
You O
planning O
on O
coming O
in O
for O
any O
of O
the O
rodeo O
stuff O
? O
You O
‚Äôll O
never O
guess O
who O
I O
got O
in O
touch O
with O
about O
a O
month O
ago O
. O
It O
was O
the O
weirdest O
thing O
- O
heather O
evans O
. O
I O
had O
n‚Äôt O
talked O
to O
her O
in O
about O
10 O
years O
. O
Seems O
like O
she O
‚Äôs O
doing O
well O
but O
I O
can O
never O
really O
tell O
with O
her O
. O
Anyway O
, O
I O
‚Äôll O
let O
you O
go O
. O
Got O
ta O
get O
back O
to O
work O
. O
Looking O
forward O
to O
hearing O
back O
from O
ya O
. O
Summary O
( O
Gold O
) O
The O
sender O
wants O
to O
congratulate O
the O
recipient O
for O
his O
/ O
her O
new O
promotion O
, O
as O
well O
as O
, O
updating O
him O
/ O
her O
about O
her O
life O
. O
The O
sender O
just O
move O
out O
to O
Katy O
few O
months O
ago O
. O
She O
is O
having O
a O
baby O
due O
in O
June O
. O
She O
is O
scared O
of O
being O
a O
mother O
but O
also O
pretty O
exited O
about O
it O
. O
Rachel O
is O
coming O
to O
visit O
her O
in O
couple O
of O
weeks O
and O
she O
is O
asking O
if O
he O
/ O
she O
will O
join O
for O
any O
of O
the O
rodeo O
stuff O
. O
She O
run O
into O
heather O
evans O
which O
she O
had O
n‚Äôt O
talked O
in O
10 O
years O
. O
Table O
8 O
: O
Example O
of O
sentences O
extracted O
by O
Implicit O
Quote O
Extractor O
( O
IQE O
) O
( O
bold O
) O
andTextRank O
( O
italic O
) O
. O
appear O
only O
once O
in O
the O
source O
text O
; O
thus O
TextRank O
fails O
to O
capture O
the O
salient O
sentences O
. O
Our O
model O
, O
by O
contrast O
, O
can O
capture O
them O
because O
they O
are O
topics O
that O
replies O
often O
refer O
to O
. O
6 O
Conclusion O
This O
paper O
proposes O
Implicit O
Quote O
Extractor O
, O
a O
model O
that O
extracts O
implicit O
quotes O
as O
summaries O
. O
We O
evaluated O
our O
model O
with O
two O
mail O
datasets O
, O
ECS O
and O
EPS O
, O
and O
one O
social O
media O
dataset O
TIFU O
, O
using O
ROUGE O
as O
an O
evaluation O
metric O
, O
and O
validated O
that O
our O
model O
is O
useful O
for O
summarization O
. O
We O
hypothesized O
that O
our O
model O
is O
more O
likely O
to O
extract O
quotes O
and O
that O
ability O
improved O
the O
performance O
of O
our O
model O
. O
We O
veriÔ¨Åed O
these O
hypotheses O
with O
the O
Reddit O
TIFU O
dataset O
, O
but O
not O
with O
the O
email O
datasets O
, O
because O
few O
emails O
included O
annotated O
summaries O
, O
and O
those O
emails O
did O
not O
have O
replies O
with O
quotes O
. O
For O
future O
work O
, O
we O
will O
examine O
whether O
our O
hypotheses O
are O
valid O
for O
emails O
and O
other O
datasets O
. O
References O
Reinald O
Kim O
Amplayo O
and O
Mirella O
Lapata O
. O
2020 O
. O
Unsupervised O
opinion O
summarization O
with O
noising O
and O
denoising O
. O
In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
1934‚Äì1945 O
, O
Online O
. O
Association O
for O
Computational O
Linguistics O
. O
Christos O
Baziotis O
, O
Ion O
Androutsopoulos O
, O
Ioannis O
Konstas O
, O
and O
Alexandros O
Potamianos O
. O
2019 O
. O
SEQÀÜ3 O
: O
Differentiable O
sequence O
- O
to O
- O
sequence O
- O
to O
- O
sequence O
autoencoder O
for O
unsupervised O
abstractive O
sentence O
compression O
. O
In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
673‚Äì681 O
. O
Association O
for O
Computational O
Linguistics O
. O
Sumit O
Bhatia O
, O
Prakhar O
Biyani O
, O
and O
Prasenjit O
Mitra O
. O
2014 O
. O
Summarizing O
online O
forum O
discussions O
‚Äì O
can O
dialog O
acts O
of O
individual O
messages O
help O
? O
In O
Proceedings O
of O
the O
2014 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
( O
EMNLP O
) O
, O
pages O
2127‚Äì2131 O
. O
Association O
for O
Computational O
Linguistics O
. O
Giuseppe O
Carenini O
, O
Raymond O
T. O
Ng O
, O
and O
Xiaodong O
Zhou O
. O
2007 O
. O
Summarizing O
email O
conversations O
with O
clue O
words O
. O
In O
Proceedings O
of O
the O
16th O
International O
Conference O
on O
World O
Wide O
Web O
, O
WWW O
‚Äô O
07 O
, O
pages O
91‚Äì100 O
. O
ACM O
. O
Eric O
Chu O
and O
Peter O
J. O
Liu O
. O
2019 O
. O
Meansum O
: O
A O
neural O
model O
for O
unsupervised O
multi O
- O
document O
abstractive O
summarization O
. O
In O
Proceedings O
of O
the O
36th O
International O
Conference O
on O
Machine O
Learning O
, O
ICML O
2019 O
, O
9 O
- O
15 O
June O
2019 O
, O
Long O
Beach O
, O
California O
, O
USA O
, O
pages O
1223‚Äì1232 O
. O
G¬®unes O
Erkan O
and O
Dragomir O
R. O
Radev O
. O
2004 O
. O
Lexrank O
: O
Graph O
- O
based O
lexical O
centrality O
as O
salience O
in O
text O
summarization O
. O
J. O
Artif O
. O
Int O
. O
Res O
. O
, O
22(1):457‚Äì479 O
. O
Thibault O
Fevry O
and O
Jason O
Phang O
. O
2018 O
. O
Unsupervised O
sentence O
compression O
using O
denoising O
autoencoders O
. O
In O
Proceedings O
of O
the O
22nd O
Conference O
on O
Computational O
Natural O
Language O
Learning O
, O
pages O
413‚Äì422 O
. O
Association O
for O
Computational O
Linguistics O
. O
Katja O
Filippova O
. O
2010 O
. O
Multi O
- O
sentence O
compression O
: O
Finding O
shortest O
paths O
in O
word O
graphs O
. O
In O
Proceedings O
of O
the O
23rd O
International O
Conference O
on O
Computational O
Linguistics O
( O
Coling O
2010 O
) O
, O
pages O
322‚Äì330 O
, O
Beijing O
, O
China O
. O
Coling O
2010 O
Organizing O
Committee O
. O
Kavita O
Ganesan O
. O
2015 O
. O
Rouge O
2.0 O
: O
Updated O
and O
improved O
measures O
for O
evaluation O
of O
summarization O
tasks O
. O
Demian O
Gholipour O
Ghalandari O
. O
2017 O
. O
Revisiting O
the O
centroid O
- O
based O
method O
: O
A O
strong O
baseline O
for O
multi O
- O
document O
summarization O
. O
In O
Proceedings O
of O
the O
Workshop O
on O
New O
Frontiers O
in O
Summarization O
, O
299pages O
85‚Äì90 O
. O
Association O
for O
Computational O
Linguistics O
. O
Aria O
Haghighi O
and O
Lucy O
Vanderwende O
. O
2009 O
. O
Exploring O
content O
models O
for O
multi O
- O
document O
summarization O
. O
In O
Proceedings O
of O
Human O
Language O
Technologies O
: O
The O
2009 O
Annual O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
362‚Äì370 O
, O
Boulder O
, O
Colorado O
. O
Association O
for O
Computational O
Linguistics O
. O
Kazuma O
Hashimoto O
, O
Caiming O
Xiong O
, O
Yoshimasa O
Tsuruoka O
, O
and O
Richard O
Socher O
. O
2017 O
. O
A O
joint O
many O
- O
task O
model O
: O
Growing O
a O
neural O
network O
for O
multiple O
NLP O
tasks O
. O
In O
Proceedings O
of O
the O
2017 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
1923‚Äì1933 O
, O
Copenhagen O
, O
Denmark O
. O
Association O
for O
Computational O
Linguistics O
. O
Zhanying O
He O
, O
Chun O
Chen O
, O
Jiajun O
Bu O
, O
Can O
Wang O
, O
Lijun O
Zhang O
, O
Deng O
Cai O
, O
and O
Xiaofei O
He O
. O
2012 O
. O
Document O
summarization O
based O
on O
data O
reconstruction O
. O
InProceedings O
of O
the O
Twenty O
- O
Sixth O
AAAI O
Conference O
on O
ArtiÔ¨Åcial O
Intelligence O
, O
AAAI‚Äô12 O
, O
pages O
620 O
‚Äì O
626 O
. O
AAAI O
Press O
. O
Masaru O
Isonuma O
, O
Junichiro O
Mori O
, O
and O
Ichiro O
Sakata O
. O
2019 O
. O
Unsupervised O
neural O
single O
- O
document O
summarization O
of O
reviews O
via O
learning O
latent O
discourse O
structure O
and O
its O
ranking O
. O
In O
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
2142‚Äì2152 O
. O
Association O
for O
Computational O
Linguistics O
. O
Eric O
Jang O
, O
Shixiang O
Gu O
, O
and O
Ben O
Poole O
. O
2017 O
. O
Categorical O
reparameterization O
with O
gumbel O
- O
softmax O
. O
In O
5th O
International O
Conference O
on O
Learning O
Representations O
, O
ICLR O
2017 O
, O
Toulon O
, O
France O
, O
April O
24 O
- O
26 O
, O
2017 O
, O
Conference O
Track O
Proceedings O
. O
Mikael O
K O
Àöageb O
¬®ack O
, O
Olof O
Mogren O
, O
Nina O
Tahmasebi O
, O
and O
Devdatt O
Dubhashi O
. O
2014 O
. O
Extractive O
summarization O
using O
continuous O
vector O
space O
models O
. O
In O
Proceedings O
of O
the O
2nd O
Workshop O
on O
Continuous O
Vector O
Space O
Models O
and O
their O
Compositionality O
( O
CVSC O
) O
, O
pages O
31‚Äì39 O
. O
Association O
for O
Computational O
Linguistics O
. O
Byeongchang O
Kim O
, O
Hyunwoo O
Kim O
, O
and O
Gunhee O
Kim O
. O
2019 O
. O
Abstractive O
summarization O
of O
Reddit O
posts O
with O
multi O
- O
level O
memory O
networks O
. O
In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
2519‚Äì2531 O
. O
Association O
for O
Computational O
Linguistics O
. O
Diederik O
P. O
Kingma O
and O
Jimmy O
Ba O
. O
2015 O
. O
Adam O
: O
A O
method O
for O
stochastic O
optimization O
. O
In O
3rd O
International O
Conference O
on O
Learning O
Representations O
, O
ICLR O
2015 O
, O
San O
Diego O
, O
CA O
, O
USA O
, O
May O
7 O
- O
9 O
, O
2015 O
, O
Conference O
Track O
Proceedings O
. O
Philippe O
Laban O
, O
Andrew O
Hsi O
, O
John O
Canny O
, O
and O
Marti O
A. O
Hearst O
. O
2020 O
. O
The O
summary O
loop O
: O
Learning O
to O
writeabstractive O
summaries O
without O
examples O
. O
In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
5135 O
‚Äì O
5150 O
, O
Online O
. O
Association O
for O
Computational O
Linguistics O
. O
Chin O
- O
Yew O
Lin O
. O
2004 O
. O
ROUGE O
: O
A O
package O
for O
automatic O
evaluation O
of O
summaries O
. O
In O
Text O
Summarization O
Branches O
Out O
, O
pages O
74‚Äì81 O
. O
Association O
for O
Computational O
Linguistics O
. O
He O
Liu O
, O
Hongliang O
Yu O
, O
and O
Zhi O
- O
Hong O
Deng O
. O
2015 O
. O
Multi O
- O
document O
summarization O
based O
on O
two O
- O
level O
sparse O
representation O
model O
. O
In O
Proceedings O
of O
the O
Twenty O
- O
Ninth O
AAAI O
Conference O
on O
ArtiÔ¨Åcial O
Intelligence O
, O
AAAI‚Äô15 O
, O
pages O
196‚Äì202 O
. O
AAAI O
Press O
. O
Vanessa O
Loza O
, O
Shibamouli O
Lahiri O
, O
Rada O
Mihalcea O
, O
and O
Po O
- O
Hsiang O
Lai O
. O
2014 O
. O
Building O
a O
dataset O
for O
summarization O
and O
keyword O
extraction O
from O
emails O
. O
In O
Proceedings O
of O
the O
Ninth O
International O
Conference O
on O
Language O
Resources O
and O
Evaluation O
( O
LREC-2014 O
) O
, O
pages O
2441‚Äì2446 O
. O
European O
Languages O
Resources O
Association O
( O
ELRA O
) O
. O
Shulei O
Ma O
, O
Zhi O
- O
Hong O
Deng O
, O
and O
Yunlun O
Yang O
. O
2016 O
. O
An O
unsupervised O
multi O
- O
document O
summarization O
framework O
based O
on O
neural O
document O
model O
. O
In O
Proceedings O
of O
COLING O
2016 O
, O
the O
26th O
International O
Conference O
on O
Computational O
Linguistics O
: O
Technical O
Papers O
, O
pages O
1514‚Äì1523 O
. O
The O
COLING O
2016 O
Organizing O
Committee O
. O
Yashar O
Mehdad O
, O
Giuseppe O
Carenini O
, O
and O
Raymond O
T. O
Ng O
. O
2014 O
. O
Abstractive O
summarization O
of O
spoken O
and O
written O
conversations O
based O
on O
phrasal O
queries O
. O
InProceedings O
of O
the O
52nd O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
1220‚Äì1230 O
. O
Association O
for O
Computational O
Linguistics O
. O
Rada O
Mihalcea O
and O
Paul O
Tarau O
. O
2004 O
. O
TextRank O
: O
Bringing O
order O
into O
text O
. O
In O
Proceedings O
of O
the O
2004 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
404‚Äì411 O
. O
Association O
for O
Computational O
Linguistics O
. O
Tatsuro O
Oya O
and O
Giuseppe O
Carenini O
. O
2014 O
. O
Extractive O
summarization O
and O
dialogue O
act O
modeling O
on O
email O
threads O
: O
An O
integrated O
probabilistic O
approach O
. O
InProceedings O
of O
the O
15th O
Annual O
Meeting O
of O
the O
Special O
Interest O
Group O
on O
Discourse O
and O
Dialogue O
( O
SIGDIAL O
) O
, O
pages O
133‚Äì140 O
. O
Association O
for O
Computational O
Linguistics O
. O
Ankur O
Parikh O
, O
Oscar O
T O
¬®ackstr O
¬®om O
, O
Dipanjan O
Das O
, O
and O
Jakob O
Uszkoreit O
. O
2016 O
. O
A O
decomposable O
attention O
model O
for O
natural O
language O
inference O
. O
In O
Proceedings O
of O
the O
2016 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
2249‚Äì2255 O
. O
Association O
for O
Computational O
Linguistics O
. O
Guokan O
Shang O
, O
Wensi O
Ding O
, O
Zekun O
Zhang O
, O
Antoine O
Tixier O
, O
Polykarpos O
Meladianos O
, O
Michalis O
Vazirgiannis O
, O
and O
Jean O
- O
Pierre O
Lorr O
¬¥ O
e. O
2018 O
. O
Unsupervised O
abstractive O
meeting O
summarization O
with O
multisentence O
compression O
and O
budgeted O
submodular300maximization O
. O
In O
Proceedings O
of O
the O
56th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
664‚Äì674 O
. O
Association O
for O
Computational O
Linguistics O
. O
Wenpeng O
Yin O
and O
Yulong O
Pei O
. O
2015 O
. O
Optimizing O
sentence O
modeling O
and O
selection O
for O
document O
summarization O
. O
In O
Proceedings O
of O
the O
24th O
International O
Conference O
on O
ArtiÔ¨Åcial O
Intelligence O
, O
IJCAI‚Äô15 O
, O
pages O
1383‚Äì1389 O
. O
AAAI O
Press O
. O
Hao O
Zheng O
and O
Mirella O
Lapata O
. O
2019 O
. O
Sentence O
centrality O
revisited O
for O
unsupervised O
summarization O
. O
In O
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
6236‚Äì6247 O
, O
Florence O
, O
Italy O
. O
Association O
for O
Computational O
Linguistics.301A O
Appendices O
A.1 O
Decomposable O
Attention O
As O
explained O
in O
section O
3 O
, O
the O
Predictor O
uses O
Decomposable O
Attention O
for O
prediction O
. O
Decomposable O
Attention O
computes O
a O
two O
- O
dimensional O
attention O
matrix O
, O
computed O
by O
two O
sets O
of O
vectors O
, O
and O
thus O
, O
captures O
detailed O
information O
useful O
for O
prediction O
. O
The O
computation O
uses O
the O
following O
equations O
: O
Etj= O
( O
xext O
t)Thr O
j O
( O
13 O
) O
Œ≤t O
= O
M O
/ O
summationdisplay O
j=1exp O
( O
Etj)/summationtextM O
k=1exp O
( O
Etk)hr O
j O
( O
14 O
) O
Œ±j O
= O
L O
/ O
summationdisplay O
t=1exp O
( O
Etj)/summationtextL O
k=1exp O
( O
Ekj)xext O
t O
( O
15 O
) O
The O
computation O
of O
xext O
tandhr O
jare O
explained O
in O
section O
3 O
. O
First O
, O
we O
compute O
a O
co O
- O
attention O
matrix O
Eas O
in O
( O
13 O
) O
. O
The O
weights O
of O
the O
co O
- O
attention O
matrix O
are O
normalized O
row O
- O
wise O
and O
column O
- O
wise O
in O
the O
equations O
( O
14 O
) O
and O
( O
15 O
) O
. O
Œ≤iis O
a O
linear O
sum O
of O
reply O
featureshr O
jthat O
is O
aligned O
to O
xext O
tand O
vice O
versa O
forŒ±j O
. O
v1,t O
= O
G([xext O
t;Œ≤t])v2,j O
= O
G([hr O
j;Œ±j])(16 O
) O
v1 O
= O
L O
/ O
summationdisplay O
t=1v1,tv2 O
= O
M O
/ O
summationdisplay O
j=1v2,j(17 O
) O
y O
= O
sigmoid O
( O
H([v1;v2]))(18 O
) O
Next O
, O
we O
separately O
compare O
the O
aligned O
phrases O
Œ≤tandxext O
t O
, O
Œ±jandhr O
j O
, O
using O
a O
function O
G.G O
denotes O
a O
feed O
- O
forward O
neural O
network O
, O
and O
[ O
; O
] O
denotes O
concatenation O
. O
Finally O
, O
we O
concatenate O
v1 O
andv2and O
obtain O
binary O
- O
classiÔ¨Åcation O
result O
y O
through O
a O
linear O
layer O
Hand O
the O
sigmoid O
function.302Proceedings O
of O
the O
1st O
Conference O
of O
the O
Asia O
- O
PaciÔ¨Åc O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
10th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
303‚Äì312 O
December O
4 O
- O
7 O
, O
2020 O
. O
¬© O
2020 O
Association O
for O
Computational O
Linguistics O
Unsupervised O
Aspect O
- O
Level O
Sentiment O
Controllable O
Style O
Transfer O
Mukuntha O
N O
S¬ß O
, O
Zishan O
Ahmad¬ß O
, O
Asif O
Ekbal O
, O
Pushpak O
Bhattacharyya O
Department O
of O
Computer O
Science O
and O
Engineering O
Indian O
Institute O
of O
Technology O
Patna O
Bihar O
, O
India O
{ O
mukuntha.cs16,1821cs18,asif O
, O
pb O
} O
@iitp.ac.in O
Abstract O
Unsupervised O
style O
transfer O
in O
text O
has O
previously O
been O
explored O
through O
the O
sentiment O
transfer O
task O
. O
The O
task O
entails O
inverting O
the O
overall O
sentiment O
polarity O
in O
a O
given O
input O
sentence O
, O
while O
preserving O
its O
content O
. O
From O
the O
Aspect O
- O
Based O
Sentiment O
Analysis O
( O
ABSA O
) O
task O
, O
we O
know O
that O
multiple O
sentiment O
polarities O
can O
often O
be O
present O
together O
in O
a O
sentence O
with O
multiple O
aspects O
. O
In O
this O
paper O
, O
the O
task O
of O
aspect O
- O
level O
sentiment O
controllable O
style O
transfer O
is O
introduced O
, O
where O
each O
of O
the O
aspect O
- O
level O
sentiments O
can O
individually O
be O
controlled O
at O
the O
output O
. O
To O
achieve O
this O
goal O
, O
a O
BERT O
- O
based O
encoder O
- O
decoder O
architecture O
with O
saliency O
weighted O
polarity O
injection O
is O
proposed O
, O
with O
unsupervised O
training O
strategies O
, O
such O
as O
ABSA O
masked O
- O
languagemodelling O
. O
Through O
both O
automatic O
and O
manual O
evaluation O
, O
we O
show O
that O
the O
system O
is O
successful O
in O
controlling O
aspect O
- O
level O
sentiments O
. O
1 O
Introduction O
With O
a O
rapid O
increase O
in O
the O
quality O
of O
generated O
text O
, O
due O
to O
the O
rise O
of O
neural O
text O
generation O
models O
( O
Kalchbrenner O
and O
Blunsom O
, O
2013 O
; O
Cho O
et O
al O
. O
, O
2014 O
; O
Sutskever O
et O
al O
. O
, O
2014 O
; O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
controllable O
text O
generation O
is O
quickly O
becoming O
the O
next O
frontier O
in O
the O
Ô¨Åeld O
of O
text O
generation O
. O
Controllable O
text O
generation O
is O
the O
task O
of O
generating O
realistic O
sentences O
whose O
attributes O
can O
be O
controlled O
. O
The O
attributes O
to O
control O
can O
be O
: O
( O
i O
) O
. O
Stylistic O
: O
Like O
politeness O
, O
sentiment O
, O
formality O
etc O
, O
( O
ii O
) O
. O
Content O
: O
Like O
information O
, O
entities O
, O
keywords O
etc O
. O
or O
( O
iii O
) O
. O
Ordering O
: O
Like O
ordering O
of O
information O
, O
events O
, O
plots O
etc O
. O
Controlling O
sentence O
level O
polarity O
has O
been O
well O
explored O
as O
a O
style O
transfer O
task O
. O
Zhang O
et O
al O
. O
( O
2018 O
) O
used O
unsupervised O
machine O
translation O
techniques O
for O
polarity O
transfer O
in O
sentences O
. O
Yang O
et O
al O
. O
( O
2018 O
) O
¬ß O
equal O
contribution O
The O
service O
  O
was O
speedy O
and O
the O
  O
salads O
  O
were O
great O
, O
but O
the O
chicken O
   O
was O
bland O
and O
stale O
. O
  O
The O
service O
  O
was O
slow O
, O
but O
the O
  O
salads O
  O
were O
great O
and O
the O
chicken O
   O
was O
tasty O
and O
fresh O
. O
Service O
- O
Positive O
  O
Salads O
- O
Positive O
  O
Chicken O
- O
Negative O
  O
Service O
- O
Negative O
  O
Salads O
- O
Positive O
  O
Chicken O
- O
Positive O
Query O
: O
  O
Service O
- O
Negative O
  O
Salads O
- O
Positive O
  O
Chicken O
- O
Positive O
Figure O
1 O
: O
An O
example O
of O
the O
proposed O
aspect O
- O
level O
sentiment O
style O
transfer O
task O
used O
language O
models O
as O
discriminators O
to O
achieve O
style O
( O
polarity O
) O
transfer O
in O
sentences O
. O
Li O
et O
al O
. O
( O
2018a O
) O
proposed O
a O
simpler O
method O
where O
they O
deleted O
the O
attribute O
markers O
and O
devise O
a O
method O
to O
replace O
or O
generate O
the O
target O
attribute O
- O
key O
phrases O
in O
the O
sentence O
. O
In O
this O
paper O
we O
explore O
a O
more O
Ô¨Åne O
- O
grained O
style O
transfer O
task O
, O
where O
each O
aspect O
‚Äôs O
polarities O
can O
be O
changed O
individually O
. O
Recent O
interest O
in O
Aspect O
- O
Based O
Sentiment O
Analysis O
( O
ABSA O
) O
( O
Pontiki O
et O
al O
. O
, O
2014 O
) O
has O
shown O
that O
sentiment O
information O
can O
vary O
within O
a O
sentence O
, O
with O
differing O
sentiments O
expressed O
towards O
different O
aspect O
terms O
of O
target O
entities O
( O
e.g. O
‚Äò O
food O
‚Äô O
, O
‚Äò O
service O
‚Äô O
in O
a O
restaurant O
domain O
) O
. O
We O
introduce O
the O
task O
of O
aspect O
- O
level O
sentiment O
transfer O
- O
the O
task O
of O
rewriting O
sentences O
to O
transfer O
them O
from O
a O
given O
set O
of O
aspect O
- O
term O
polarities O
( O
such O
as O
‚Äò O
positive O
sentiment O
‚Äô O
towards O
the O
service O
of O
a O
restaurant O
and O
a O
‚Äò O
positive O
sentiment O
‚Äô O
towards O
the O
taste O
of O
the O
food O
) O
to O
a O
different O
set O
of O
aspect O
- O
term O
polarities O
( O
such O
as O
‚Äò O
negative O
sentiment O
‚Äô O
towards O
the O
service O
of O
a O
restaurant O
and O
a O
‚Äò O
positive O
‚Äô O
sentiment O
towards O
the O
taste O
of O
the O
food O
) O
. O
This O
is O
a O
more O
challenging O
task O
than O
regular O
style O
transfer O
as O
the O
style O
attributes O
here O
are O
not O
the O
overall O
attributes O
for O
the O
whole O
sentence O
, O
but O
are O
localized O
to O
speciÔ¨Åc O
parts O
of O
the O
sentence O
, O
and O
multiple O
opposing O
at-303tributes O
could O
be O
present O
within O
the O
same O
sentence O
. O
The O
target O
of O
the O
transformation O
made O
needs O
to O
be O
localized O
and O
the O
other O
content O
expressed O
in O
the O
rest O
of O
the O
sentence O
need O
to O
be O
preserved O
at O
the O
output O
. O
An O
example O
of O
the O
task O
is O
shown O
in O
Figure O
1 O
. O
For O
successful O
manipulation O
of O
the O
generated O
sentences O
, O
a O
few O
challenges O
need O
to O
be O
addressed O
: O
( O
i O
) O
. O
The O
model O
should O
learn O
to O
associate O
the O
right O
polarities O
with O
the O
right O
aspects O
. O
( O
ii O
) O
. O
The O
model O
needs O
to O
be O
able O
to O
correctly O
process O
the O
aspectpolarity O
query O
and O
accordingly O
delete O
, O
replace O
and O
generate O
text O
sequence O
to O
satisfy O
the O
query O
. O
( O
iii O
) O
. O
The O
polarities O
of O
the O
aspects O
not O
in O
the O
query O
should O
not O
be O
affected O
. O
( O
iv O
) O
. O
The O
non O
- O
attribute O
content O
and O
Ô¨Çuency O
of O
the O
text O
should O
be O
preserved O
. O
We O
explore O
this O
task O
in O
an O
unsupervised O
setting O
( O
as O
is O
common O
with O
most O
style O
- O
transfer O
tasks O
due O
to O
the O
lack O
of O
an O
aligned O
parallel O
corpus O
) O
using O
only O
monolingual O
unaligned O
corpora O
. O
In O
this O
work O
, O
a O
novel O
encoder O
- O
decoder O
architecture O
is O
proposed O
to O
perform O
unsupervised O
aspect O
- O
level O
sentiment O
transfer O
. O
A O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
based O
encoder O
is O
used O
that O
is O
trained O
to O
understand O
aspect O
- O
speciÔ¨Åc O
polarity O
information O
. O
We O
also O
propose O
using O
a O
‚Äò O
polarity O
injection O
‚Äô O
method O
, O
where O
saliency O
- O
weighted O
aspect O
- O
speciÔ¨Åc O
polarity O
information O
is O
added O
to O
the O
hidden O
representations O
from O
the O
encoder O
to O
complete O
the O
query O
for O
the O
decoder O
. O
1.1 O
Motivation O
The O
Aspect O
- O
Based O
Sentiment O
Analysis O
( O
ABSA O
) O
task O
shows O
that O
differing O
sentiments O
can O
be O
present O
within O
the O
same O
sentence O
, O
localized O
to O
different O
entities O
or O
parts O
of O
the O
text O
. O
The O
notion O
of O
styles O
in O
natural O
language O
can O
be O
used O
to O
refer O
to O
the O
attributes O
, O
such O
as O
sentiment O
, O
formality O
in O
content O
, O
emotion O
, O
sarcasm O
, O
etc O
. O
Similar O
to O
the O
sentiment O
, O
these O
other O
attributes O
can O
also O
be O
present O
localized O
to O
different O
entities O
taking O
differing O
values O
at O
each O
location O
. O
If O
we O
consider O
the O
style O
‚Äò O
emotion O
‚Äô O
with O
the O
example O
‚Äú O
Although O
Alice O
infuriates O
me O
with O
her O
prattle O
and O
Bob O
scares O
me O
, O
I O
am O
quite O
happy O
about O
how O
things O
are O
turning O
out O
. O
‚Äù O
- O
A O
single O
piece O
of O
text O
( O
such O
as O
a O
single O
sentence O
) O
can O
express O
an O
emotion O
, O
such O
as O
‚Äò O
happiness O
‚Äô O
about O
an O
event O
while O
expressing O
‚Äò O
fear O
‚Äô O
towards O
some O
entity O
and O
‚Äò O
anger O
‚Äô O
towards O
a O
second O
entity O
. O
This O
shows O
that O
style O
transfer O
in O
language O
needs O
a O
more O
nuanced O
understanding O
. O
Especially O
when O
generating O
larger O
pieces O
of O
text O
, O
multiple O
such O
styles O
could O
intermingle O
, O
and O
differing O
styles O
can O
often O
be O
present O
together O
when O
discussing O
differenttopics O
and O
entities O
. O
Our O
work O
intends O
to O
take O
the O
Ô¨Årst O
step O
towards O
a O
more O
controllable O
form O
of O
Ô¨Ånegrained O
style O
transfer O
with O
the O
task O
of O
aspect O
- O
level O
sentiment O
style O
transfer O
. O
2 O
Related O
Work O
In O
this O
section O
we O
present O
an O
overview O
of O
the O
related O
literature O
. O
2.1 O
Sentiment O
Transfer O
To O
the O
best O
of O
our O
knowledge O
, O
our O
current O
work O
is O
the O
Ô¨Årst O
to O
tackle O
aspect O
- O
level O
sentiment O
transfer O
. O
Most O
of O
the O
previous O
works O
involving O
sentiment O
transfer O
( O
Li O
et O
al O
. O
, O
2018b O
; O
Yang O
et O
al O
. O
, O
2018 O
; O
Shen O
et O
al O
. O
, O
2017 O
; O
Xu O
et O
al O
. O
, O
2018 O
; O
Prabhumoye O
et O
al O
. O
, O
2018 O
; O
Wu O
et O
al O
. O
, O
2019 O
) O
consider O
the O
style O
that O
is O
present O
throughout O
the O
sentence O
and O
seek O
to O
transfer O
only O
the O
overall O
sentiment O
polarities O
expressed O
. O
Tian O
et O
al O
. O
( O
2018 O
) O
proposed O
a O
new O
training O
objective O
for O
content O
preservation O
during O
style O
transfer O
. O
They O
used O
Part O
- O
of O
- O
Speech O
( O
PoS O
) O
tagging O
to O
collect O
nouns O
at O
inputs O
, O
and O
expect O
them O
to O
be O
present O
at O
the O
output O
for O
content O
preservation O
. O
To O
achieve O
this O
, O
they O
proposed O
a O
PoS O
preservation O
constraint O
and O
‚Äò O
Content O
Conditional O
Language O
Modelling O
‚Äô O
. O
They O
tested O
their O
system O
on O
sentiment O
style O
transfer O
task O
. O
Wang O
et O
al O
. O
( O
2019 O
) O
proposed O
a O
method O
that O
can O
also O
control O
the O
degree O
of O
polarity O
transfer O
in O
a O
sentence O
with O
multiple O
aspect O
categories O
present O
in O
it O
. O
Unlike O
their O
task O
which O
deals O
with O
predeÔ¨Åned O
aspect O
categories O
, O
our O
task O
deals O
with O
opinion O
target O
expressions O
. O
Aspect O
categories O
are O
coarse O
entities O
that O
are O
few O
in O
number O
and O
predeÔ¨Åned O
for O
a O
certain O
domain O
, O
while O
aspect O
- O
terms O
or O
opinion O
target O
expressions O
are O
Ô¨Åne O
- O
grained O
entities O
that O
are O
present O
in O
the O
text O
. O
They O
also O
did O
not O
investigate O
selectively O
transferring O
the O
polarity O
over O
a O
subset O
of O
aspects O
with O
multiple O
differing O
polarities O
at O
the O
output O
and O
only O
invert O
the O
overall O
polarity O
expressed O
by O
the O
sentence O
. O
Our O
method O
works O
across O
thousands O
of O
unique O
opinion O
target O
expressions O
( O
Table O
1 O
shows O
the O
number O
unique O
target O
aspects O
present O
in O
each O
of O
our O
datasets).Our O
method O
also O
does O
not O
need O
these O
to O
be O
predeÔ¨Åned O
, O
and O
so O
could O
be O
used O
to O
control O
the O
polarities O
of O
previously O
unseen O
target O
expressions O
as O
well O
. O
2.2 O
Unsupervised O
Machine O
Translation O
Previous O
works O
in O
unsupervised O
neural O
machine O
translation O
( O
Artetxe O
et O
al O
. O
, O
2017 O
) O
and O
unsupervised O
style O
transfer O
( O
Zhang O
et O
al O
. O
, O
2018 O
) O
have O
shown O
that,304with O
only O
monolingual O
data O
, O
using O
a O
denoising O
autoencoder O
loss O
and O
an O
on O
- O
the-Ô¨Çy O
back O
- O
translation O
loss O
can O
be O
very O
successful O
in O
achieving O
transfer O
. O
Both O
of O
these O
training O
steps O
are O
used O
as O
part O
of O
our O
method O
to O
train O
the O
network O
in O
an O
unsupervised O
fashion O
. O
2.3 O
Natural O
Language O
Generation O
Architecture O
Lai O
et O
al O
. O
( O
2019 O
) O
proposed O
an O
adversarial O
training O
mechanism O
for O
Gated O
Recurrent O
Unit O
( O
GRU O
) O
based O
encoder O
- O
decoder O
model O
for O
sentiment O
polarity O
transfer O
and O
multiple O
- O
attribute O
transfer O
tasks O
. O
They O
split O
the O
training O
mechanism O
of O
their O
model O
into O
two O
phases O
, O
viz.(i O
) O
. O
Style O
transfer O
phase O
and O
( O
ii O
) O
. O
Reconstruction O
phase O
. O
Pryzant O
et O
al O
. O
( O
2020 O
) O
proposed O
a O
method O
to O
remove O
subjective O
bias O
in O
the O
sentences O
. O
They O
proposed O
adding O
a O
‚Äò O
join O
- O
embedding O
‚Äô O
weighted O
by O
a O
word O
subjective O
- O
bias O
probability O
to O
automatically O
edit O
the O
hidden O
states O
from O
the O
encoder O
. O
We O
adapt O
this O
‚Äò O
join O
- O
embedding O
‚Äô O
method O
to O
inject O
weighted O
polarities O
into O
our O
encoder O
outputs O
as O
described O
in O
Section O
3.5 O
. O
2.4 O
Aspect O
Based O
Sentiment O
Analysis O
Aspect O
based O
sentiment O
analysis O
( O
ABSA O
) O
has O
been O
explored O
in O
a O
series O
of O
SemEval O
shared O
tasks O
. O
The O
task O
consists O
of O
both O
aspect O
term O
extraction O
and O
aspect O
sentiment O
prediction O
. O
Tay O
et O
al O
. O
( O
2018 O
) O
proposed O
‚Äò O
Aspect O
Fusion O
LSTM O
‚Äô O
to O
attend O
on O
the O
associative O
relationships O
between O
sentence O
words O
and O
aspect O
words O
to O
classify O
aspect O
polarities O
. O
Xu O
et O
al O
. O
( O
2019 O
) O
proposed O
BERT O
based O
models O
for O
aspect O
term O
extraction O
and O
aspect O
- O
polarity O
classiÔ¨Åcation O
tasks O
. O
We O
build O
similar O
BERT O
based O
aspect O
termextraction O
and O
aspect O
- O
polarity O
classiÔ¨Åcation O
models O
and O
use O
them O
to O
label O
Yelp O
reviews O
dataset O
. O
This O
dataset O
is O
then O
used O
for O
aspect O
- O
level O
sentiment O
controllable O
style O
transfer O
task O
in O
this O
paper O
. O
3 O
Methodology O
3.1 O
Problem O
Statement O
Let O
us O
assume O
we O
have O
access O
to O
a O
corpora O
of O
labelled O
sentences O
D= O
( O
x1,l1) O
... O
(xn O
, O
ln O
) O
, O
wherexiis O
a O
sentence O
, O
and O
li O
= O
{ O
( O
ti1,pi1) O
... O
(tim O
, O
pim O
) O
} O
. O
Here O
, O
tijis O
an O
aspect O
- O
target O
or O
‚Äò O
Opinion O
Target O
Expression O
‚Äô O
( O
Pontiki O
et O
al O
. O
, O
2014 O
) O
, O
and O
pijis O
the O
corresponding O
sentiment O
- O
polarity O
expressed O
towards O
tij O
, O
where O
pij‚àà{‚Äúpositive O
‚Äù O
, O
‚Äú O
negative O
‚Äù O
} O
. O
A O
model O
is O
to O
be O
learned O
that O
takes O
as O
input O
( O
x O
, O
ltgt)wherexis O
the O
source O
sentence O
expressing O
some O
aspectpolarity O
set O
lsrc O
, O
and O
outputs O
ythat O
retain O
all O
the O
non O
- O
polarity O
content O
in O
xwhile O
expressing O
the O
aspect O
- O
polarity O
set O
ltgt O
. O
This O
is O
to O
be O
performed O
in O
an O
unsupervised O
manner O
, O
where O
we O
do O
not O
assume O
access O
to O
an O
aligned O
set O
of O
parallel O
sentences O
with O
the O
same O
content O
but O
different O
aspect O
- O
polarities O
. O
The O
overall O
architecture O
consists O
of O
a O
Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
encoder O
- O
decoder O
neural O
network O
, O
where O
the O
encoder O
is O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O
In O
this O
section O
, O
we O
describe O
the O
architecture O
and O
the O
training O
methodology O
used O
. O
The O
inputs O
provided O
to O
the O
model O
are O
the O
sentence O
, O
a O
list O
of O
aspects O
, O
their O
corresponding O
desired O
target O
polaritiesltgtand O
their O
corresponding O
per O
- O
token O
weights O
( O
explained O
in O
Section O
3.5 O
) O
. O
3.2 O
ABSA O
Input O
Representation O
The O
BERT O
model O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
was O
originally O
trained O
with O
two O
objectives O
: O
( O
i O
) O
. O
A O
cloze O
objective O
where O
the O
classiÔ¨Åer O
predicts O
missing O
words O
in O
a O
sequence O
, O
and O
( O
ii O
) O
. O
A O
sentence O
- O
pair O
classiÔ¨Åcation O
objective O
. O
For O
the O
sentence O
- O
pair O
objective O
, O
BERT O
was O
trained O
to O
take O
inputs O
as O
segment O
- O
pairs O
, O
where O
each O
segment O
has O
a O
different O
embedding O
added O
to O
it O
and O
are O
separated O
by O
a O
[ O
SEP O
] O
token O
. O
For O
our O
input O
representation O
, O
we O
construct O
such O
segment O
- O
pairs O
. O
The O
Ô¨Årst O
segment O
consists O
of O
an O
aspect O
- O
polarity O
sequence O
SEG O
A= O
‚Äú O
T1P1[SEP O
ASP] O
... O
T O
kPk[SEP O
ASP O
] O
‚Äù O
, O
whereTiis O
the O
tokenized O
target O
aspect O
term O
and O
Pi‚àà{[POS O
] O
, O
[ O
NEG O
] O
} O
is O
a O
polarity O
corresponding O
to O
it O
. O
[ O
SEP O
ASP]‚Äùis O
a O
separator O
token O
. O
[ O
POS O
] O
, O
[ O
NEG O
] O
are O
the O
special O
tokens O
corresponding O
to O
the O
‚Äô O
positive O
‚Äô O
and O
‚Äô O
negative O
‚Äô O
sentiments O
, O
for O
which O
the O
unused O
tokens O
from O
the O
BERT O
vocabulary O
were O
used O
. O
The O
second O
segment O
SEG O
Bconsists O
of O
a O
sentence O
expressing O
some O
sentiment O
towards O
these O
targets O
. O
3.3 O
Preconditioning O
the O
BERT O
- O
encoder O
for O
ABSA O
Input O
Representations O
We O
precondition O
the O
BERT O
encoder O
to O
better O
understand O
the O
ABSA O
task O
and O
to O
learn O
the O
tokenembeddings O
for O
[ O
POS O
] O
and[NEG O
] O
with O
MLM O
pre O
- O
training O
( O
a O
cloze O
objective O
) O
( O
c.f O
. O
Figure O
2 O
) O
. O
For O
each O
data O
instance O
, O
with O
an O
equal O
probability O
, O
we O
randomly O
mask O
out O
either O
( O
i O
) O
. O
all O
the O
polarity O
tokens O
from O
aspect O
- O
polarity O
sequence O
( O
SEG O
A O
) O
, O
or O
( O
ii O
) O
. O
random O
tokens O
from O
the O
sentence O
( O
SEG O
B O
) O
and O
train O
the O
encoder O
to O
correctly O
predict O
the O
masked-305[MASK O
] O
service O
was O
[ O
MASK O
] O
but O
the O
[ O
MASK O
] O
was O
delicious.service O
[ O
NEG O
] O
food O
[ O
POS O
] O
  O
SEGA O
: O
Source O
aspect O
- O
polarity O
sequence O
  O
constructed O
from O
lsrc O
SEGB O
: O
Randomly O
masked O
source O
sentence O
tokens O
( O
x O
) O
  O
( O
expressing O
aspect O
- O
level O
sentiments O
lsrc)[SEP O
] O
BERT O
Encoder O
The O
slow O
food O
The O
service O
was O
slow O
but O
the O
food O
was O
delicious.service O
[ O
MASK O
] O
food O
[ O
MASK O
] O
  O
SEGA O
: O
Aspect O
- O
polarity O
sequence O
with O
  O
Masked O
polarities O
, O
constructed O
from O
lsrc O
SEGB O
: O
Source O
sentence O
tokens O
( O
x O
) O
( O
expressing O
  O
aspect O
- O
level O
sentiments O
lsrc)[SEP O
] O
BERT O
Encoder O
[ O
POS O
] O
[ O
NEG O
] O
Figure O
2 O
: O
BERT O
Encoder O
pre O
- O
training O
x O
: O
input O
sentence O
Asp1 O
[ O
pol1 O
] O
[ O
SEPASP O
] O
‚Ä¶ O
Aspk O
[ O
polk O
] O
[ O
SEPASP O
] O
SEGA O
: O
Goal O
aspect O
- O
polarity O
sequence O
  O
constructed O
from O
ltgt O
SEGB O
: O
source O
sentence O
tokens O
  O
( O
expressing O
aspect O
- O
level O
sentiments O
lsrc)[SEP O
] O
BERT O
Encoder O
+ O
+ O
+ O
+ O
+ O
+ O
+ O
+ O
vivi‚àà{vpos O
, O
vneg O
} O
‚Ä¢‚Ä¢ O
‚Ä¢ O
‚Ä¢ O
‚Ä¢ O
‚Ä¢ O
‚Ä¢ O
‚Ä¢ O
  O
pi O
HH‚ÄôTransformer O
  O
Decoder O
y O
} O
( O
For O
each O
  O
aspect O
) O
‚àÄ O
i O
From O
classifier O
  O
saliency O
- O
maps O
( O
expressing O
  O
Aspect O
- O
level O
  O
sentiments O
ltgt)vposvneg O
Figure O
3 O
: O
The O
encoder O
- O
decoder O
network O
used O
, O
with O
the O
polarity O
injection O
. O
out O
tokens O
. O
When O
the O
polarities O
get O
masked O
, O
the O
encoder O
learns O
to O
correctly O
understand O
the O
aspectlevel O
sentiment O
polarities O
from O
a O
sentence O
. O
When O
the O
words O
from O
the O
sentence O
get O
masked O
, O
the O
encoder O
also O
learns O
to O
correctly O
predict O
attribute O
markers O
corresponding O
to O
a O
given O
aspect O
and O
a O
sentiment O
. O
For O
example O
, O
it O
would O
learn O
associations O
between O
the O
markers O
, O
such O
as O
‚Äò O
personable O
‚Äô O
( O
or O
‚Äò O
rude O
‚Äô O
) O
when O
given O
an O
aspect O
- O
term O
, O
such O
as O
‚Äò O
staff O
‚Äô O
with O
a O
polarity O
‚Äò O
[ O
POS O
] O
‚Äô O
( O
or O
‚Äò O
[ O
NEG O
] O
‚Äô O
) O
as O
opposed O
to O
an O
aspectmarker O
, O
such O
as O
‚Äò O
delicious O
‚Äô O
, O
which O
can O
not O
be O
used O
with O
‚Äò O
staff O
‚Äô O
. O
3.4 O
Encoder O
- O
Decoder O
Architecture O
To O
convert O
a O
sentence O
from O
one O
set O
of O
aspectlevel O
polarities O
to O
another O
, O
the O
input O
to O
the O
encoder O
consists O
of O
the O
target O
aspect O
- O
level O
polarities O
ltgt O
asSEG O
Awith O
the O
source O
sentence O
xpassed O
as O
SEG O
B. O
The O
full O
architecture O
is O
shown O
in O
Figure O
3 O
. O
The O
source O
sentence O
xexpresses O
some O
source O
aspect O
- O
level O
polarities O
lsrcwhich O
is O
not O
provided O
to O
the O
model O
. O
The O
polarity O
- O
injection O
( O
explained O
in O
Section O
3.5 O
) O
adds O
the O
weighted O
target O
polarities O
ltgtinto O
the O
hidden O
- O
representation O
Hfrom O
the O
BERTencoder O
to O
obtain O
H O
/ O
primewhich O
is O
passed O
to O
the O
decoder O
. O
The O
decoder O
is O
trained O
to O
output O
the O
target O
sentence O
ywhich O
consists O
of O
the O
same O
content O
as O
present O
in O
xbut O
expressing O
the O
target O
aspect O
- O
level O
polarities O
ltgt O
. O
This O
architecture O
is O
trained O
in O
an O
unsupervised O
fashion O
as O
explained O
in O
Section O
3.6 O
. O
3.5 O
Polarity O
Injection O
In O
Pryzant O
et O
al O
. O
( O
2020 O
) O
, O
authors O
showed O
that O
the O
hidden O
states O
of O
the O
encoder O
can O
be O
edited O
by O
adding O
weighted O
vectors O
to O
indicate O
subjective O
- O
bias O
, O
before O
being O
input O
to O
the O
decoder O
. O
They O
proposed O
this O
as O
a O
method O
to O
join O
the O
results O
from O
two O
sub O
- O
modules O
in O
their O
system O
. O
Here O
, O
we O
extend O
this O
to O
cover O
multiple O
attributes O
- O
the O
‚Äò O
positive O
‚Äô O
and O
‚Äò O
negative O
‚Äô O
sentiments O
, O
and O
substitute O
the O
supervised O
model O
they O
train O
with O
saliency O
- O
based O
weights O
. O
We O
inject O
( O
add O
) O
weighted O
amounts O
of O
two O
vectors O
corresponding O
to O
these O
two O
attributes O
to O
edit O
the O
hidden O
states O
output O
by O
the O
encoder O
. O
For O
each O
aspect O
, O
the O
vector O
added O
corresponds O
to O
the O
desired O
target O
polarity O
of O
this O
aspect O
, O
and O
the O
amount O
added O
to O
a O
given O
token O
depends O
on O
the O
saliency O
- O
based O
weight O
for O
this O
token O
calculated O
from O
the O
gradient O
for O
this O
aspect O
‚Äôs O
polarity O
from O
a O
classiÔ¨Åcation O
model O
( O
described O
in O
Section O
4.1.1 O
) O
. O
More O
formally O
, O
the O
polarity O
injection O
is O
calculated O
from O
equation O
1 O
. O
H= O
[ O
h1,h2, O
... O
h O
k]that O
denotes O
the O
hidden O
- O
state O
output O
from O
the O
encoder O
, O
andH O
/ O
prime= O
[ O
h O
/ O
prime O
1,h O
/ O
prime O
2, O
... O
h O
/ O
prime O
k]are O
the O
new O
hidden O
- O
states O
calculated O
after O
polarity O
- O
injection O
. O
The O
number O
pij O
denotes O
the O
saliency O
- O
based O
weightage O
for O
token O
j O
with O
respect O
to O
aspect O
i. O
Figure O
3 O
shows O
the O
polarityinjection O
architecture O
. O
vposandvnegare O
the O
special O
vector O
- O
embeddings O
, O
which O
have O
the O
same O
size O
as O
the O
hidden O
dimension O
, O
and O
trained O
to O
denote O
the O
positive O
and O
negative O
sentiment O
, O
respectively O
. O
h O
/ O
prime O
j O
= O
hj+k O
/ O
summationdisplay O
i=1pij¬∑vi O
( O
1 O
) O
vi=/braceleftBigg O
vpos O
ifpolidesired O
is O
positive O
vneg O
ifpolidesired O
is O
negative(2 O
) O
wherepoliis O
the O
target O
( O
desired O
) O
polarity O
from O
ltgt O
for O
the O
ithaspect O
- O
term O
. O
For O
calculating O
pij O
, O
saliencymaps O
obtained O
for O
each O
aspect O
from O
the O
polarity O
classiÔ¨Åer O
described O
in O
4.1 O
are O
used O
. O
Saliency O
- O
maps O
( O
Simonyan O
et O
al O
. O
, O
2014 O
) O
are O
calculated O
with O
the O
gradient O
of O
the O
loss O
at O
the O
input O
, O
as O
given O
in O
equation306Dataset O
No O
. O
of O
Sentences O
No O
. O
of O
Target O
Aspects O
No O
. O
of O
Unique O
Target O
Aspects O
SemEval O
( O
Train O
and O
Validation O
) O
2,242 O
4,016 O
1,437 O
SemEval O
( O
Test O
) O
401 O
513 O
269 O
Yelp O
( O
Train O
and O
Validation O
) O
361,968 O
471,820 O
47,750 O
Table O
1 O
: O
Data O
distribution O
for O
the O
restaurant O
domain O
. O
The O
Yelp O
dataset O
does O
not O
contain O
target O
aspects O
and O
their O
polarities O
extracted O
, O
and O
these O
were O
extracted O
with O
a O
classiÔ¨Åer O
trained O
on O
the O
SemEval O
training O
data O
3 O
. O
Thestokvalues O
for O
all O
the O
tokens O
tokin O
the O
sentencexare O
normalized O
between O
0 O
and O
1 O
for O
each O
( O
targett O
, O
sentencex O
) O
pair O
to O
obtain O
the O
pijvalues O
. O
Since O
the O
saliency O
- O
maps O
produce O
high O
values O
for O
the O
tokens O
that O
are O
important O
in O
calculating O
the O
sentiment O
‚Äôs O
polarity O
, O
adding O
the O
‚Äò O
positive O
‚Äô O
or O
‚Äò O
negative O
‚Äô O
embedding O
weighted O
by O
these O
probabilities O
would O
provide O
hints O
to O
the O
decoder O
about O
the O
important O
words O
to O
be O
rewritten O
with O
the O
required O
sentiment O
. O
Thepijvalues O
over O
Segment O
- O
A O
is O
set O
to O
1 O
over O
all O
the O
tokens O
corresponding O
to O
the O
ithaspect O
term O
( O
TiPi O
) O
( O
see O
Section O
3.2 O
) O
and O
0 O
over O
the O
other O
tokens O
. O
stok=/vextendsingle O
/ O
vextendsingle O
/ O
vextendsingle O
/ O
vextendsingle‚àÇL(yt;x O
, O
t O
, O
Œ∏ O
) O
‚àÇemb O
tok O
/ O
vextendsingle O
/ O
vextendsingle O
/ O
vextendsingle O
/ O
vextendsingle;‚àÄtok‚ààx O
( O
3 O
) O
One O
- O
Zero O
Alternative O
to O
Saliency O
: O
To O
test O
for O
performance O
in O
the O
absence O
of O
any O
saliency O
information O
, O
we O
also O
propose O
using O
a O
one O
- O
zero O
setup O
. O
Here O
, O
pijis O
set O
to O
1 O
over O
the O
tokens O
corresponding O
to O
the O
ithaspect O
- O
term O
and O
0 O
elsewhere O
. O
So O
in O
this O
setup O
, O
vposgets O
added O
to O
the O
tokens O
corresponding O
to O
the O
positive O
aspects O
and O
vneggets O
added O
to O
the O
tokens O
corresponding O
to O
the O
negative O
aspects O
. O
For O
example O
, O
in O
Figure O
1 O
, O
vposgets O
added O
to O
the O
subword O
tokens O
corresponding O
to O
the O
word O
‚Äò O
salads O
‚Äô O
and O
‚Äò O
chicken O
‚Äô O
, O
while O
vneggets O
added O
to O
the O
sub O
- O
word O
tokens O
corresponding O
to O
the O
word O
‚Äò O
service O
‚Äô O
. O
3.6 O
Unsupervised O
Training O
For O
training O
the O
model O
in O
an O
unsupervised O
setting O
, O
we O
alternate O
training O
steps O
between O
a O
denoising O
auto O
- O
encoding O
objective O
and O
a O
back O
- O
translation O
objective O
. O
During O
the O
denoising O
step O
, O
we O
add O
random O
noise O
to O
the O
sentence O
part O
of O
the O
input O
SEG O
B. O
We O
also O
randomly O
mask O
the O
polarities O
in O
the O
aspectpolarity O
sequence O
in O
SEG O
Awith O
a O
small O
probability O
to O
ensure O
the O
model O
learns O
to O
generate O
outputs O
using O
the O
polarity O
injection O
clues O
. O
During O
the O
backtranslation O
step O
, O
a O
random O
query O
ltgtaspect O
- O
polarity O
sequence O
is O
used O
to O
produce O
an O
intermediate O
translation O
( O
using O
the O
model O
) O
, O
and O
the O
same O
model O
is O
trained O
to O
regenerate O
the O
original O
input O
when O
provided O
the O
aspect O
- O
polarity O
sequence O
from O
the O
original O
input O
sentence.4 O
Experiments O
In O
this O
section O
we O
report O
the O
datasets O
used O
for O
the O
experiments O
and O
the O
implementation O
details O
. O
4.1 O
Datasets O
Text O
generation O
tasks O
require O
huge O
amounts O
of O
data O
, O
however O
there O
are O
no O
aspect O
- O
sentiment O
annotated O
datasets O
that O
are O
large O
enough O
for O
our O
task O
. O
Fortunately O
, O
aspect O
extraction O
and O
aspect O
- O
sentiment O
classiÔ¨Åcation O
tasks O
have O
been O
well O
explored O
and O
have O
several O
publicly O
available O
datasets O
. O
We O
used O
datasets O
( O
only O
restaurant O
domain O
) O
from O
SemEval O
2014 O
, O
2015 O
and O
2016 O
( O
ABSA O
task O
) O
to O
train O
BERT O
based O
aspect O
extraction O
and O
aspect O
- O
sentiment O
classiÔ¨Åcation O
systems O
. O
We O
only O
consider O
positive O
and O
negative O
polarities O
for O
our O
experiments O
. O
For O
the O
task O
of O
aspect O
- O
level O
sentiment O
style O
transfer O
, O
we O
use O
Yelp O
dataset O
. O
Since O
this O
dataset O
does O
not O
contain O
aspect O
- O
level O
polarity O
information O
or O
the O
target O
- O
aspects O
extracted O
, O
we O
use O
our O
BERT O
- O
based O
target O
- O
extraction O
model O
and O
BERT O
- O
based O
polarity O
classiÔ¨Åcation O
model O
which O
were O
trained O
on O
the O
SemEval O
ABSA O
training O
data O
, O
to O
generate O
aspect O
- O
level O
sentiment O
data O
from O
the O
Yelp O
reviews O
dataset O
. O
Table O
1 O
shows O
some O
statistics O
from O
the O
datasets O
. O
4.1.1 O
Aspect O
based O
Sentiment O
Analysis O
with O
BERT O
A O
pipeline O
of O
BERT O
- O
based O
models O
was O
trained O
for O
target O
- O
extraction O
and O
aspect O
- O
level O
polarity O
classiÔ¨Åcation O
over O
the O
SemEval O
dataset O
. O
These O
are O
the O
models O
used O
to O
extract O
target O
- O
aspects O
and O
their O
polarities O
from O
the O
Yelp O
dataset O
. O
The O
target O
extraction O
task O
was O
posed O
as O
a O
sequential O
token O
classiÔ¨Åcation O
problem O
with O
BERT O
using O
the O
IOB2 O
format O
( O
SANG O
, O
1999 O
) O
. O
This O
BERT O
model O
was O
fed O
the O
whole O
sentence O
as O
the O
input O
segment O
and O
it O
obtained O
an O
F1 O
- O
score O
of O
0.8012 O
( O
evaluation O
carried O
out O
similar O
to O
Sang O
and O
Buchholz O
( O
2000 O
) O
) O
. O
The O
sentiment O
- O
polarity O
prediction O
task O
is O
posed O
as O
a O
sentence O
- O
pair O
classiÔ¨Åcation O
problem O
using O
BERT O
, O
with O
the O
sentence O
provided O
as O
the O
Ô¨Årst O
segment O
and O
the O
aspect O
- O
term O
as O
the O
second O
segment O
. O
This O
model O
obtained O
an O
F1 O
- O
score O
of O
0.9080 O
for O
the O
positive O
po-307ModelClassiÔ¨Åer O
Score O
( O
Overall)ClassiÔ¨Åer O
Score O
( O
1 O
- O
Aspect)ClassiÔ¨Åer O
Score O
( O
2 O
- O
Aspects)ClassiÔ¨Åer O
Score O
( O
3 O
- O
or O
- O
more O
Aspects)BLEU O
Score O
BERT O
- O
Baseline O
( O
BB O
) O
0.5158 O
0.4983 O
0.5448 O
0.5036 O
36.0683 O
BB O
+ O
MLM O
pretraining O
( O
BBMLM)0.5298 O
0.5433 O
0.5310 O
0.5145 O
35.4601 O
BB O
- O
MLM O
+ O
one O
- O
zero O
polarity O
injection0.5415 O
0.5675 O
0.5276 O
0.5290 O
35.8244 O
BB O
- O
MLM O
+ O
saliency O
- O
based O
polarity O
injection0.5918 O
0.6125 O
0.5828 O
0.5797 O
39.3838 O
Table O
2 O
: O
Results O
of O
automatic O
evaluation O
. O
The O
overall O
classiÔ¨Åer O
score O
is O
calculated O
over O
all O
queries O
. O
The O
other O
columns O
show O
the O
score O
calculated O
only O
on O
queries O
with O
one O
aspect O
, O
two O
aspects O
or O
three O
or O
more O
aspects O
. O
The O
classiÔ¨Åer O
scores O
are O
calculated O
on O
the O
full O
test O
set O
, O
while O
the O
BLEU O
scores O
are O
measured O
with O
reference O
- O
outputs O
for O
a O
subset O
of O
100 O
queries O
. O
Model O
Att O
Con O
Gra O
BERT O
- O
Baseline O
( O
BB O
) O
2.48 O
3.99 O
3.96 O
BB O
+ O
MLM O
pretraining O
( O
BB O
- O
MLM O
) O
2.64 O
3.95 O
4.04 O
BB O
- O
MLM O
+ O
one O
- O
zero O
polarity O
injection O
2.80 O
4.00 O
4.05 O
BB O
- O
MLM O
+ O
saliency O
- O
based O
polarity O
injection O
2.98 O
4.08 O
4.05 O
Table O
3 O
: O
Results O
of O
manual O
evaluation O
. O
Here O
, O
‚Äò O
Att O
‚Äô O
stands O
for O
attribute O
match O
, O
‚Äò O
Con O
‚Äô O
stands O
for O
content O
preservation O
and O
‚Äò O
Gra O
‚Äô O
stands O
for O
grammaticality O
or O
Ô¨Çuency O
. O
Manual O
evaluation O
is O
performed O
on O
a O
subset O
of O
100 O
queries O
from O
all O
the O
test O
set O
queries O
, O
and O
averaged O
scores O
are O
shown O
. O
larity O
and O
0.8239 O
for O
the O
negative O
polarity O
on O
the O
ABSA O
restaurant O
dataset O
. O
Using O
this O
classiÔ¨Åer O
, O
for O
each O
( O
Sentence O
, O
Target O
) O
pair O
the O
gradient O
of O
the O
loss O
was O
taken O
at O
the O
input O
token O
embeddings O
and O
normalized O
to O
obtain O
the O
saliency O
- O
based O
weights O
used O
for O
polarity O
- O
injection O
. O
4.2 O
Implementation O
Details O
All O
the O
models O
were O
implemented O
using O
PyTorch O
( O
Paszke O
et O
al O
. O
, O
2017 O
) O
. O
The O
BERT O
model O
was O
implemented O
using O
the O
transformers O
library O
( O
Wolf O
et O
al O
. O
, O
2019 O
) O
. O
Models O
are O
trained O
with O
an O
initial O
learning O
rate O
of O
1e-4 O
with O
a O
linear O
schedule O
and O
a O
warmup O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
using O
the O
Adam O
Optimizer O
( O
Kingma O
and O
Ba O
, O
2019 O
) O
. O
Mini O
- O
batches O
of O
size O
32 O
were O
used O
during O
training O
. O
A O
linear O
schedule O
was O
used O
for O
the O
weight O
of O
the O
loss O
from O
the O
denoising O
auto O
- O
encoding O
step O
, O
which O
was O
set O
to O
decrease O
from O
1 O
to O
0.1 O
for O
the O
Ô¨Årst O
30,000 O
optimization O
steps O
and O
then O
decrease O
linearly O
to O
0 O
over O
the O
next O
70,000 O
steps O
. O
The O
models O
were O
each O
trained O
for O
8 O
epochs O
on O
the O
Yelp O
dataset O
. O
The O
random O
masking O
probability O
used O
during O
pre O
- O
training O
was O
0.25 O
. O
During O
the O
denoising O
step O
, O
a O
probability O
of O
0.25 O
was O
used O
for O
dropping O
words O
, O
and O
words O
were O
shufÔ¨Çed O
with O
a O
window O
- O
size O
of O
3.5 O
Results O
and O
Analysis O
5.1 O
Evaluation O
The O
evaluation O
metrics O
we O
use O
are O
an O
extension O
of O
the O
metrics O
used O
for O
evaluating O
the O
sentiment O
transfer O
task O
by O
previous O
work O
( O
such O
as O
Li O
et O
al O
. O
( O
2018b O
) O
; O
Wang O
et O
al O
. O
( O
2019 O
) O
) O
. O
The O
evaluation O
was O
done O
with O
the O
SemEval O
test O
dataset O
. O
Queries O
were O
generated O
from O
this O
data O
by O
randomly O
inverting O
a O
subset O
( O
non O
- O
null O
, O
improper O
subset O
) O
of O
the O
polarities O
expressed O
at O
the O
input O
. O
For O
queries O
with O
2 O
or O
more O
aspects O
, O
as O
many O
queries O
were O
generated O
as O
there O
were O
aspects O
in O
the O
sentence O
with O
different O
random O
inversions O
, O
resulting O
in O
a O
total O
of O
513 O
evaluation O
queries O
. O
A O
sample O
consisting O
of O
100 O
queries O
from O
the O
test O
set O
was O
used O
for O
manual O
evaluation O
. O
5.1.1 O
Automatic O
Evaluation O
For O
automatic O
evaluation O
, O
we O
use O
a O
classiÔ¨Åer O
score O
and O
a O
BLEU O
score O
. O
The O
results O
for O
automatic O
evaluation O
are O
shown O
in O
Table O
2 O
. O
ClassiÔ¨Åer O
Score O
: O
We O
use O
an O
aspect O
- O
level O
sentiment O
polarity O
classiÔ¨Åer O
to O
measure O
how O
many O
of O
the O
outputs O
express O
the O
necessary O
target O
polarities O
( O
Li O
et O
al O
. O
, O
2018b O
) O
. O
We O
use O
the O
classiÔ¨Åer O
described O
in O
4.1.1 O
for O
the O
polarity O
prediction O
. O
We O
deÔ¨Åne O
the O
classiÔ¨Åer O
score O
to O
be O
the O
fraction O
of O
aspect O
- O
level O
sentiment O
polarities O
( O
predicted O
by O
the O
classiÔ¨Åer O
from O
the O
output O
) O
that O
match O
with O
the O
desired O
aspect O
- O
level O
polarity O
( O
from O
the O
query O
) O
. O
While O
averaging O
, O
each308Input O
Query O
Model O
Output O
overall O
, O
decent O
food O
at O
a O
good O
price O
, O
with O
friendly O
people.food O
- O
negative O
people O
- O
positiveoverall O
, O
mediocre O
food O
at O
a O
good O
price O
, O
with O
friendly O
people O
. O
food O
- O
positive O
people O
- O
negativeoverall O
, O
decent O
food O
at O
a O
good O
price O
, O
with O
rude O
people O
. O
the O
waiter O
was O
attentive O
, O
the O
food O
was O
delicious O
and O
the O
views O
of O
the O
city O
were O
greatwaiter O
- O
negative O
food O
- O
positive O
views O
of O
the O
city O
- O
positivethe O
waiter O
was O
inattentive O
, O
the O
food O
was O
delicious O
and O
the O
views O
of O
the O
city O
were O
great O
. O
waiter O
- O
positive O
food O
- O
negative O
views O
of O
the O
city O
- O
positivethe O
waiter O
was O
attentive O
, O
the O
food O
was O
disappointing O
but O
the O
views O
of O
the O
city O
were O
great O
. O
waiter O
- O
positive O
food O
- O
negative O
views O
of O
the O
city O
- O
negativethe O
waiter O
was O
attentive O
, O
the O
food O
was O
disappointing O
but O
the O
views O
of O
the O
city O
were O
terrible O
. O
Table O
4 O
: O
Example O
outputs O
from O
the O
full O
model O
with O
saliency O
- O
based O
polarity O
injection O
with O
different O
aspect O
- O
level O
polarity O
queries O
. O
aspect O
- O
level O
sentiment O
in O
a O
query O
was O
treated O
as O
a O
separate O
instance O
. O
BLEU O
Scores O
: O
Like O
in O
Li O
et O
al O
. O
( O
2018b O
) O
; O
Gan O
et O
al O
. O
( O
2017 O
) O
, O
human O
reference O
outputs O
were O
written O
for O
100 O
of O
the O
queries O
. O
Three O
Human O
experts O
were O
asked O
to O
rewrite O
the O
reviews O
with O
as O
much O
content O
preserved O
as O
possible O
, O
without O
compromising O
Ô¨Çuency O
. O
These O
experts O
had O
good O
language O
abilities O
and O
having O
satisfactory O
knowledge O
in O
the O
relevant O
area O
. O
We O
report O
BLEU O
scores O
for O
the O
models O
against O
these O
references O
. O
A O
BLEU O
score O
could O
be O
treated O
as O
a O
measure O
of O
content O
preservation O
from O
the O
input O
or O
the O
output O
Ô¨Çuency O
. O
5.1.2 O
Manual O
Evaluation O
Following O
the O
previous O
methods O
( O
Li O
et O
al O
. O
, O
2018b O
; O
Wang O
et O
al O
. O
, O
2019 O
) O
for O
manual O
evaluation O
of O
style O
transfer O
, O
workers O
were O
asked O
to O
rate O
the O
output O
sentences O
on O
the O
Likert O
- O
scale O
( O
1 O
to O
5 O
) O
for O
three O
criteria O
- O
Attribute O
match O
to O
the O
query O
set O
of O
aspect O
- O
level O
polarities O
( O
Att O
) O
, O
Fluency O
( O
Gra O
) O
measuring O
the O
naturalness O
of O
the O
output O
and O
Content O
preservation O
( O
Con O
) O
. O
They O
were O
shown O
the O
source O
sentence O
, O
the O
query O
aspect O
- O
level O
polarities O
and O
the O
model O
output O
. O
The O
results O
of O
manual O
evaluation O
are O
shown O
in O
Table O
31 O
5.2 O
Error O
Analysis O
The O
importance O
of O
each O
component O
in O
our O
model O
is O
shown O
through O
an O
ablation O
study O
in O
Table O
2 O
and O
Table O
3 O
. O
From O
the O
classiÔ¨Åer O
- O
based O
score O
, O
we O
see O
that O
the O
full O
model O
with O
saliency O
- O
based O
polarity O
injection O
is O
the O
most O
successful O
in O
transferring O
sentimentlevel O
polarities O
. O
Polarity O
injection O
, O
even O
without O
1Inter O
- O
annotator O
agreement O
measured O
through O
the O
Krippendorff O
‚Äôs O
alpha O
was O
found O
to O
be O
0.92 O
, O
0.82 O
, O
0.87 O
for O
‚Äò O
Att O
‚Äô O
, O
‚Äò O
Con O
‚Äô O
, O
and O
‚Äò O
Gra O
‚Äô O
respectively.saliency O
information O
is O
seen O
to O
be O
useful O
. O
The O
models O
with O
polarity O
injection O
are O
especially O
better O
at O
transferring O
sentiments O
when O
three O
or O
more O
aspects O
are O
present O
, O
showing O
that O
the O
polarity O
signals O
are O
useful O
in O
localizing O
the O
style O
attributes O
with O
multiple O
targets O
present O
. O
The O
model O
using O
saliencybased O
weighting O
for O
the O
polarity O
injection O
has O
a O
signiÔ¨Åcantly O
higher O
classiÔ¨Åer O
score O
. O
This O
could O
be O
because O
of O
the O
saliency O
information O
acting O
like O
an O
adversarial O
white O
- O
box O
attack O
on O
the O
classiÔ¨Åer O
, O
making O
it O
easier O
to O
obtain O
higher O
classiÔ¨Åer O
scores O
. O
The O
Content O
preservation O
( O
Con O
) O
scores O
and O
BLEU O
scores O
for O
the O
baseline O
models O
are O
significantly O
high O
, O
but O
these O
models O
also O
show O
poor O
Attribute O
match O
( O
Att O
) O
scores O
. O
This O
means O
that O
many O
of O
the O
sentiments O
at O
the O
output O
were O
left O
untransferred O
resulting O
in O
the O
poor O
Att O
score O
, O
while O
large O
parts O
of O
the O
input O
text O
were O
copied O
over O
to O
the O
output O
resulting O
in O
the O
larger O
BLEU O
and O
Content O
preservation O
scores O
. O
The O
improved O
Content O
preservation O
( O
Con O
) O
scores O
and O
the O
Fluency O
( O
Gra O
) O
from O
the O
model O
without O
saliency O
information O
to O
the O
model O
with O
saliencybased O
weighting O
shows O
that O
the O
attribute O
transfer O
with O
saliency O
- O
based O
info O
is O
more O
successful O
in O
inverting O
the O
correct O
polarities O
, O
while O
maintaining O
the O
content O
and O
Ô¨Çuency O
, O
due O
to O
the O
added O
information O
about O
the O
words O
to O
be O
edited O
. O
The O
Table O
5 O
shows O
how O
the O
model O
outputs O
change O
with O
different O
components O
of O
the O
model O
are O
ablated O
. O
With O
queries O
involving O
mostly O
positive O
or O
negative O
attributes O
, O
the O
saliency O
- O
based O
polarity O
injection O
supports O
the O
localized O
inversion O
of O
sentiment O
in O
the O
output O
. O
Outputs O
also O
show O
how O
polarity O
injection O
helps O
produce O
the O
required O
change O
with O
more O
content O
and O
Ô¨Çuency O
preserved O
, O
by O
selectively O
editing O
the O
correct O
words.309Input O
the O
veal O
and O
the O
mushrooms O
were O
cooked O
perfectly O
. O
Query O
veal O
- O
positive O
, O
mushrooms O
- O
negative O
BERT O
- O
Baseline O
( O
BB O
) O
the O
veal O
and O
the O
mushrooms O
were O
not O
cooked O
perfectly O
. O
BB O
+ O
MLM O
pretraining O
( O
BB O
- O
MLM O
) O
the O
veal O
and O
the O
mushrooms O
were O
over O
cooked O
perfectly O
. O
BB O
- O
MLM O
+ O
one O
- O
zero O
polarity O
injection O
the O
veal O
was O
gross O
and O
the O
mushrooms O
were O
over O
cooked O
. O
BB O
- O
MLM O
+ O
saliency O
- O
based O
polarity O
injection O
loved O
the O
veal O
and O
the O
mushrooms O
were O
over O
cooked O
. O
Inputthe O
waiter O
was O
attentive O
, O
the O
food O
was O
delicious O
and O
the O
views O
of O
the O
city O
were O
great O
. O
Query O
waiter O
- O
negative O
, O
food O
- O
positive O
, O
views O
of O
the O
city O
- O
positive O
BERT O
- O
Baseline O
( O
BB)the O
waiter O
was O
attentive O
, O
the O
food O
was O
delicious O
and O
the O
city O
were O
great O
. O
BB O
+ O
MLM O
pretraining O
( O
BB O
- O
MLM)the O
waiter O
was O
attentive O
, O
the O
food O
was O
delicious O
and O
the O
views O
of O
the O
city O
were O
great O
. O
BB O
- O
MLM O
+ O
one O
- O
zero O
Polarity O
injectionthe O
waiter O
was O
attentive O
, O
the O
food O
was O
delicious O
and O
the O
views O
of O
the O
city O
were O
great O
. O
BB O
- O
MLM O
+ O
saliency O
- O
based O
polarity O
injectionthe O
waiter O
was O
inattentive O
, O
the O
food O
was O
delicious O
and O
the O
views O
of O
the O
city O
were O
great O
. O
Input O
for O
7 O
years O
they O
have O
put O
out O
the O
most O
tasty O
, O
most O
delicious O
food O
and O
kept O
it O
that O
way O
... O
Query O
food O
- O
negative O
BERT O
- O
Baseline O
( O
BB O
) O
for O
7 O
years O
they O
have O
put O
out O
the O
most O
tasty O
food O
and O
kept O
it O
that O
way O
. O
BB O
+ O
MLM O
pretraining O
( O
BB O
- O
MLM O
) O
for O
7 O
years O
they O
have O
put O
out O
the O
most O
greasy O
food O
and O
bland O
food O
that O
way O
... O
BB O
- O
MLM O
+ O
one O
- O
zero O
polarity O
injection O
for O
6 O
years O
they O
have O
put O
out O
the O
most O
bland O
food O
and O
kept O
it O
that O
way O
... O
BB O
- O
MLM O
+ O
saliency O
- O
based O
polarity O
injection O
for O
7 O
years O
they O
have O
put O
out O
the O
most O
bland O
food O
and O
kept O
it O
that O
way O
... O
Table O
5 O
: O
Example O
outputs O
from O
the O
SemEval O
data O
showing O
aspect O
- O
level O
sentiment O
transfer O
from O
the O
ablated O
models O
. O
Aspects O
colored O
red O
( O
negative O
) O
or O
green O
( O
positive O
) O
indicate O
their O
sentiment O
. O
Input O
i O
must O
say O
i O
am O
surprised O
by O
the O
bad O
reviews O
of O
the O
restaurant O
earlier O
in O
the O
year O
, O
though O
. O
Query O
restaurant O
- O
negative O
Output O
i O
must O
say O
i O
am O
surprised O
by O
the O
bad O
reviews O
of O
the O
restaurant O
earlier O
in O
the O
year O
, O
though O
. O
Comment O
No O
change O
. O
Sentiment O
here O
is O
implied O
and O
latent O
. O
Inputthe O
space O
is O
limited O
so O
be O
prepared O
to O
wait O
up O
to O
45 O
minutes O
- O
1 O
hour O
, O
but O
be O
richly O
rewarded O
when O
you O
savor O
the O
delicious O
indo O
- O
chinese O
food O
Query O
space O
- O
positive O
, O
indo O
- O
chinese O
food O
- O
positive O
Output O
the O
space O
is O
extensive O
so O
be O
prepared O
to O
10 O
- O
15 O
- O
20 O
+ O
minutes O
, O
but O
delicious O
chinese O
food O
. O
CommentDisÔ¨Çuency O
and O
dropped O
content O
due O
to O
the O
length O
of O
input O
and O
the O
negative O
sentiment O
implied O
through O
the O
word O
‚Äò O
waiting O
‚Äô O
. O
Input O
i O
‚Äôd O
be O
horriÔ¨Åed O
if O
my O
staff O
were O
turning O
away O
customers O
so O
early O
and O
so O
rudely O
! O
Query O
staff O
- O
positive O
Output O
i O
‚Äôd O
be O
delighted O
if O
my O
staff O
were O
turning O
away O
customers O
so O
early O
and O
nicely O
! O
Comment O
Lower O
naturalness O
of O
the O
output O
from O
real O
- O
world O
knowledge O
that O
turning O
away O
customers O
is O
bad O
. O
Input O
i O
had O
Ô¨Åsh O
and O
my O
husband O
had O
the O
Ô¨Ålet O
- O
both O
of O
which O
exceeded O
our O
expectations O
. O
Query O
Ô¨Åsh O
- O
negative O
, O
Ô¨Ålet O
- O
positive O
Output O
i O
had O
Ô¨Åsh O
and O
my O
husband O
had O
the O
Ô¨Ålet O
- O
both O
of O
which O
exceeded O
our O
expectations O
. O
CommentThe O
attribute O
markers O
for O
Ô¨Åsh O
and O
Ô¨Ålet O
are O
shared O
, O
making O
transfer O
difÔ¨Åcult O
. O
SigniÔ¨Åcant O
rewriting O
of O
the O
input O
in O
needed O
to O
produce O
acceptable O
Ô¨Çuent O
output O
. O
Table O
6 O
: O
Example O
sentences O
that O
show O
difÔ¨Åculty O
in O
transferring O
sentiment O
. O
To O
understand O
the O
errors O
in O
outputs O
better O
, O
the O
outputs O
marked O
with O
low O
Att O
, O
Con O
and O
Gra O
scores O
were O
examined O
. O
Some O
of O
these O
outputs O
are O
shown O
and O
discussed O
in O
Table O
6 O
. O
Many O
of O
the O
failures O
in O
Attribute O
match O
were O
found O
to O
be O
due O
to O
the O
complexity O
involved O
in O
the O
language O
, O
such O
as O
when O
the O
sentiment O
expressed O
towards O
a O
target O
is O
implicit O
from O
the O
content O
of O
the O
review O
. O
The O
absence O
ofattribute O
markers O
also O
makes O
it O
harder O
to O
convert O
sentiment O
. O
Most O
outputs O
with O
low O
Con O
and O
Gra O
scores O
were O
found O
to O
contain O
very O
long O
sentences O
. O
The O
models O
were O
trained O
on O
the O
Yelp O
dataset O
which O
mostly O
contained O
smaller O
sentences O
. O
Failed O
examples O
with O
multiple O
different O
polarities O
at O
the O
output O
were O
often O
also O
due O
to O
the O
attribute O
markers O
towards O
different O
aspects O
being O
shared O
in O
the O
input O
sentence.310Such O
examples O
require O
signiÔ¨Åcant O
rewriting O
and O
reordering O
to O
produce O
sentences O
of O
acceptable O
Ô¨Çuency O
, O
and O
our O
method O
seems O
most O
successful O
when O
making O
localized O
changes O
such O
as O
with O
word O
replacements O
. O
6 O
Conclusion O
and O
Future O
Work O
In O
this O
paper O
, O
the O
task O
of O
aspect O
- O
level O
sentiment O
style O
transfer O
has O
been O
introduced O
, O
where O
stylistic O
attributes O
can O
be O
localized O
to O
different O
parts O
of O
a O
sentence O
. O
We O
have O
proposed O
a O
BERT O
- O
based O
encoder O
- O
decoder O
architecture O
with O
saliency O
- O
based O
polarity O
injection O
and O
show O
that O
it O
can O
be O
successful O
at O
the O
task O
when O
trained O
in O
an O
unsupervised O
setting O
. O
The O
experiments O
have O
been O
conducted O
on O
an O
aspect O
level O
polarity O
tagged O
benchmark O
dataset O
related O
to O
the O
restaurant O
domain O
. O
This O
work O
is O
hopefully O
an O
important O
initial O
step O
in O
developing O
a O
Ô¨Åne O
- O
grained O
controllable O
style O
transfer O
system O
. O
In O
the O
future O
, O
we O
would O
like O
to O
explore O
the O
ability O
to O
transfer O
such O
systems O
to O
data O
- O
sparse O
domains O
, O
and O
explore O
injecting O
attributes O
such O
as O
emotions O
to O
targets O
attributes O
in O
larger O
pieces O
of O
text O
. O
Acknowledgments O
Authors O
duly O
acknowledge O
the O
support O
from O
the O
Project O
titled O
Sevak O
- O
An O
Intelligent O
Indian O
Language O
Chatbot O
, O
Sponsored O
by O
SERB O
, O
Govt O
. O
of O
India O
( O
IMP/2018/002072 O
) O
. O
Abstract O
Abstractive O
community O
detection O
is O
an O
important O
spoken O
language O
understanding O
task O
, O
whose O
goal O
is O
to O
group O
utterances O
in O
a O
conversation O
according O
to O
whether O
they O
can O
be O
jointly O
summarized O
by O
a O
common O
abstractive O
sentence O
. O
This O
paper O
provides O
a O
novel O
approach O
to O
this O
task O
. O
We O
Ô¨Årst O
introduce O
a O
neural O
contextual O
utterance O
encoder O
featuring O
three O
types O
of O
self O
- O
attention O
mechanisms O
. O
We O
then O
train O
it O
using O
the O
siamese O
and O
triplet O
energybased O
meta O
- O
architectures O
. O
Experiments O
on O
the O
AMI O
corpus O
show O
that O
our O
system O
outperforms O
multiple O
energy O
- O
based O
and O
non O
- O
energy O
based O
baselines O
from O
the O
state O
- O
of O
- O
the O
- O
art O
. O
Code O
and O
data O
are O
publicly O
available1 O
. O
1 O
Introduction O
Today O
, O
large O
amounts O
of O
digital O
text O
are O
generated O
by O
spoken O
or O
written O
conversations O
, O
let O
them O
be O
human O
- O
human O
( O
customer O
service O
, O
multi O
- O
party O
meetings O
) O
or O
human O
- O
machine O
( O
chatbots O
, O
virtual O
assistants O
) O
. O
Such O
text O
comes O
in O
the O
form O
of O
transcriptions O
. O
A O
transcription O
is O
a O
list O
of O
time O
- O
ordered O
text O
fragments O
called O
utterances O
. O
Abstractive O
summarization O
of O
conversations O
is O
an O
open O
problem O
in O
NLP O
. O
Previous O
work O
( O
Mehdad O
et O
al O
. O
, O
2013 O
; O
Oya O
et O
al O
. O
, O
2014 O
; O
Banerjee O
et O
al O
. O
, O
2015 O
; O
Shang O
et O
al O
. O
, O
2018 O
) O
decomposes O
this O
task O
into O
two O
subtasks O
a O
andbas O
shown O
in O
Fig O
. O
1 O
. O
Subtask O
a O
, O
orAbstractive O
Community O
Detection O
( O
ACD O
) O
, O
is O
the O
focus O
of O
this O
paper O
. O
It O
consists O
in O
grouping O
utterances O
according O
to O
whether O
they O
can O
be O
jointly O
summarized O
by O
a O
common O
abstractive O
sentence O
( O
Murray O
et O
al O
. O
, O
2012 O
) O
. O
Such O
groups O
of O
utterances O
are O
called O
abstractive O
communities O
. O
Once O
they O
are O
obtained O
, O
an O
abstractive O
sentence O
is O
generated O
for O
each O
group O
( O
subtask O
b O
) O
, O
thus O
forming O
the O
Ô¨Ånal O
summary O
. O
ACD O
includes O
, O
but O
is O
a O
more O
1https://bitbucket.org/guokan_shang/ O
abscomm O
transcription O
abstractive O
  O
communities O
abstractive O
  O
summary O
extracted O
  O
utterances O
a1a O
a2 O
bFigure O
1 O
: O
Abstractive O
summarization O
of O
conversations O
. O
general O
problem O
than O
, O
topic O
clustering O
. O
Indeed O
, O
as O
shown O
in O
Fig O
. O
2 O
, O
communities O
should O
capture O
more O
complex O
relationship O
than O
simple O
semantic O
similarity O
. O
Also O
, O
two O
utterances O
may O
be O
part O
of O
the O
same O
community O
even O
if O
they O
are O
not O
close O
to O
each O
other O
in O
the O
transcription O
. O
Finally O
, O
a O
given O
utterance O
may O
belong O
to O
more O
than O
one O
community O
, O
which O
results O
in O
overlapping O
groupings O
( O
e.g. O
, O
A O
and O
D O
in O
Fig O
. O
2 O
) O
, O
or O
be O
a O
singleton O
community O
( O
B O
in O
Fig O
. O
2 O
) O
. O
In O
this O
paper O
, O
we O
depart O
from O
previous O
work O
and O
argue O
that O
the O
ACD O
subtask O
should O
be O
broken O
down O
into O
two O
steps O
, O
a1anda2 O
in O
Fig O
. O
1 O
. O
That O
is O
, O
summary O
- O
worthy O
utterances O
should O
Ô¨Årst O
be O
extracted O
from O
the O
transcription O
( O
a1 O
) O
, O
and O
then O
, O
grouped O
into O
abstractive O
communities O
( O
a2 O
) O
. O
This O
process O
is O
more O
consistent O
with O
the O
way O
humans O
treat O
the O
summarization O
task O
. O
E.g. O
, O
during O
the O
creation O
of O
the O
AMI O
corpus O
( O
McCowan O
et O
al O
. O
, O
2005 O
) O
, O
annotators O
were O
Ô¨Årst O
asked O
to O
extract O
summaryworthy O
utterances O
from O
the O
transcription O
, O
and O
then O
to O
link O
the O
selected O
utterances O
to O
the O
sentences O
in O
the O
abstractive O
summary O
( O
links O
in O
Fig O
. O
2 O
) O
, O
i.e. O
, O
create O
communities O
. O
Abstractive O
summaries O
comprise O
four O
sections O
: O
ABSTRACT O
, O
ACTIONS O
, O
PROBLEMS O
, O
and O
DECISIONS O
. O
Step O
a1plays O
an O
important O
Ô¨Åltering O
role O
, O
since O
in O
practice O
, O
only O
a O
small O
part O
of O
the O
original O
utterances O
are O
used O
to O
construct O
the O
abstractive O
communities O
( O
17 O
% O
on O
average O
for O
AMI O
) O
. O
However O
, O
this O
step O
is O
closely O
related O
to O
extractive O
summarization O
, O
which O
has O
been O
extensively O
studied O
in O
the O
conversational O
domain O
( O
Murray O
et O
al O
. O
,313UI O
: O
¬† O
But O
¬† O
what O
¬† O
if O
¬† O
we O
¬† O
ha O
¬† O
what O
¬† O
if O
¬† O
we O
¬† O
had O
¬† O
like O
¬† O
a O
¬† O
Spongy O
sort O
¬† O
of O
¬† O
like O
¬† O
stress O
¬† O
balley O
¬† O
kinda O
¬† O
[ O
disfmarker O
] O
PM O
: O
¬† O
If O
¬† O
you O
¬† O
have O
¬† O
like O
¬† O
that O
¬† O
stress O
¬† O
ball O
¬† O
material O
¬† O
kind O
¬†  O
of O
¬† O
as O
¬† O
what O
¬† O
you O
‚Äôre O
¬† O
actually O
¬† O
holding O
¬† O
in O
¬† O
your O
¬† O
hand O
, O
PM O
: O
¬† O
and O
¬† O
then O
¬† O
there O
‚Äôs O
¬† O
more O
¬† O
of O
¬† O
a O
¬† O
hard O
¬† O
plastic O
¬† O
thing O
¬†  O
where O
¬† O
that O
¬† O
thing O
¬† O
is O
. O
PM O
: O
¬† O
And O
¬† O
on O
¬† O
that O
¬† O
hard O
¬† O
plastic O
¬† O
thing O
¬† O
you O
¬† O
can O
¬† O
change O
either O
¬† O
the O
¬† O
colour O
¬† O
or O
¬† O
the O
¬† O
fruit O
¬† O
or O
¬† O
vegetable O
¬† O
that O
‚Äôs O
¬†  O
on O
¬† O
there O
. O
ME O
: O
¬† O
see O
¬† O
you O
‚Äôre O
¬† O
thinking O
, O
¬† O
it O
‚Äôs O
¬† O
weird O
, O
¬† O
you O
‚Äôre O
¬† O
thinking O
the O
¬† O
opposite O
¬† O
of O
¬† O
me O
ME O
: O
¬† O
Because O
¬† O
I O
¬† O
was O
¬† O
thinking O
¬† O
if O
¬† O
you O
¬† O
have O
¬† O
a O
¬† O
cover O
¬† O
for O
the O
¬† O
squashy O
¬† O
bit O
, O
UI O
: O
¬† O
oh O
¬† O
so O
¬† O
so O
¬† O
you O
‚Äôre O
¬† O
saying O
¬† O
the O
¬† O
squishy O
¬† O
part O
‚Äôs O
¬† O
like O
detachable O
, O
UI O
: O
¬† O
so O
¬† O
so O
¬† O
maybe O
¬† O
one O
¬† O
you O
¬† O
know O
¬† O
[ O
disfmarker O
] O
¬† O
you O
¬†  O
can O
¬† O
have O
¬† O
like O
¬† O
the O
¬† O
broccoli O
¬† O
squishy O
¬† O
thing O
, O
¬† O
and O
¬† O
then O
you O
¬† O
could O
¬† O
have O
¬† O
like O
¬† O
the O
¬† O
banana O
¬† O
squishy O
¬† O
thing O
PM O
: O
¬† O
Oh O
¬† O
when O
¬† O
we O
¬† O
move O
¬† O
on O
, O
¬† O
you O
¬† O
two O
¬† O
are O
¬† O
going O
¬† O
to O
¬†  O
be O
¬† O
playing O
¬† O
with O
¬† O
play¬≠dough O
. O
ABSTRACT O
Some O
¬† O
part O
¬† O
of O
¬† O
the O
¬† O
casing O
¬† O
will O
¬† O
be O
¬† O
made O
¬† O
of O
¬† O
a O
¬† O
spongy O
material O
. O
... O
ACTIONS O
The O
¬† O
Project O
¬† O
Manager O
¬† O
instructed O
¬† O
the O
¬† O
User O
¬† O
Interface O
¬†  O
Designer O
¬† O
and O
¬† O
the O
¬† O
Industrial O
¬† O
Designer O
¬† O
to O
¬† O
construct O
¬†  O
the O
¬† O
prototype O
. O
... O
DECISIONS O
The O
¬† O
remote O
¬† O
will O
¬† O
feature O
¬† O
a O
¬† O
changeable O
¬† O
outer O
¬† O
casing O
. O
... O
PROBLEMS O
The O
¬† O
group O
¬† O
wanted O
¬† O
to O
¬† O
include O
¬† O
a O
¬† O
changeable O
¬† O
outer O
¬†  O
casing O
¬† O
but O
¬† O
could O
¬† O
not O
¬† O
decide O
¬† O
whether O
¬† O
the O
¬† O
spongy O
or O
¬† O
the O
¬† O
hard O
¬† O
plastic O
¬† O
component O
¬† O
should O
¬† O
be O
¬† O
the O
¬†  O
removable O
¬† O
casing O
. O
... O
‚¨á O
timeAA O
BB O
CC O
DD O
‚¨Ü O
Figure O
2 O
: O
Example O
of O
ground O
truth O
human O
annotations O
from O
the O
ES2011c O
AMI O
meeting O
. O
Successive O
grey O
nodes O
on O
the O
left O
denote O
utterances O
in O
the O
transcription O
. O
Black O
nodes O
correspond O
to O
the O
utterances O
judged O
important O
. O
Sentences O
( O
e.g. O
, O
A O
, O
B O
, O
C O
, O
D O
) O
from O
the O
abstractive O
summary O
are O
shown O
on O
the O
right O
. O
All O
utterances O
linked O
to O
the O
same O
abstractive O
sentence O
form O
one O
community O
. O
2005 O
; O
Garg O
et O
al O
. O
, O
2009 O
; O
Tixier O
et O
al O
. O
, O
2017 O
) O
. O
Rather O
, O
we O
focus O
in O
this O
paper O
on O
the O
rarely O
explored O
a2utterance O
clustering O
step O
, O
which O
we O
think O
is O
an O
important O
spoken O
language O
understanding O
problem O
, O
as O
it O
plays O
a O
crucial O
role O
of O
bridge O
between O
two O
major O
types O
of O
summaries O
: O
extractive O
and O
abstractive O
. O
2 O
Departure O
from O
previous O
work O
Prior O
work O
performed O
ACD O
either O
in O
a O
supervised O
( O
Murray O
et O
al O
. O
, O
2012 O
; O
Mehdad O
et O
al O
. O
, O
2013 O
) O
or O
unsupervised O
way O
( O
Oya O
et O
al O
. O
, O
2014 O
; O
Banerjee O
et O
al O
. O
, O
2015 O
; O
Singla O
et O
al O
. O
, O
2017 O
; O
Shang O
et O
al O
. O
, O
2018 O
) O
. O
In O
the O
supervised O
case O
, O
Murray O
et O
al O
. O
( O
2012 O
) O
train O
a O
logistic O
regression O
classiÔ¨Åer O
with O
handcrafted O
features O
to O
predict O
extractive O
- O
abstractive O
links O
, O
then O
build O
an O
utterance O
graph O
whose O
edges O
represent O
the O
binary O
predictions O
of O
the O
classiÔ¨Åer O
, O
and O
Ô¨Ånally O
apply O
an O
overlapping O
community O
detection O
algorithm O
to O
the O
graph O
. O
Mehdad O
et O
al O
. O
( O
2013 O
) O
add O
to O
the O
previous O
approach O
by O
building O
an O
entailment O
graph O
for O
each O
community O
, O
where O
edges O
are O
entailment O
relations O
between O
utterances O
, O
predicted O
by O
a O
SVM O
classiÔ¨Åer O
trained O
with O
handcrafted O
features O
on O
an O
external O
dataset O
. O
The O
entailment O
graph O
allows O
less O
informative O
utterances O
to O
be O
eliminated O
from O
each O
community O
. O
On O
the O
other O
hand O
, O
unsupervised O
approaches O
to O
ACD O
do O
not O
make O
use O
of O
extractive O
- O
abstractive O
links O
. O
Oya O
et O
al O
. O
( O
2014 O
) O
; O
Banerjee O
et O
al O
. O
( O
2015 O
) O
; O
Singla O
et O
al O
. O
( O
2017 O
) O
assume O
that O
disjoint O
topic O
segments O
( O
Galley O
et O
al O
. O
, O
2003 O
) O
align O
with O
abstractive O
communities O
, O
while O
Shang O
et O
al O
. O
( O
2018 O
) O
use O
the O
classical O
vector O
space O
representation O
with O
TF O
- O
IDF O
weights O
, O
and O
apply O
k O
- O
means O
to O
the O
LSAcompressed O
utterance O
- O
term O
matrix O
. O
To O
sum O
up O
, O
prior O
ACD O
methods O
either O
train O
multiple O
models O
on O
different O
labeled O
datasets O
and O
heavily O
rely O
on O
handcrafted O
features O
, O
or O
are O
incapable O
of O
capturing O
the O
complicated O
structure O
of O
abstractive O
communities O
described O
in O
the O
introduction O
. O
Motivated O
by O
the O
recent O
success O
of O
energy O
- O
based O
approaches O
to O
similarity O
learning O
tasks O
such O
as O
face O
veriÔ¨Åcation O
( O
Schroff O
et O
al O
. O
, O
2015 O
) O
and O
sentence O
matching O
( O
Mueller O
and O
Thyagarajan O
, O
2016 O
) O
, O
we O
introduce O
in O
this O
paper O
a O
novel O
utterance O
encoder O
, O
and O
train O
it O
within O
the O
siamese O
( O
Chopra O
et O
al O
. O
, O
2005 O
) O
and O
triplet O
( O
Hoffer O
and O
Ailon O
, O
2015 O
) O
energy O
- O
based O
meta O
- O
architectures O
. O
Our O
Ô¨Ånal O
network O
is O
able O
to O
accurately O
capture O
the O
complexity O
of O
abstractive O
community O
structure O
, O
while O
at O
the O
same O
time O
, O
it O
is O
trainable O
in O
an O
end O
- O
to O
- O
end O
fashion O
without O
the O
need O
for O
human O
intervention O
and O
handcrafted O
features O
. O
Our O
contributions O
are O
threefold O
: O
( O
1 O
) O
we O
formalize O
ACD O
, O
a O
crucial O
subtask O
for O
abstractive O
summarization O
of O
conversations O
, O
and O
publicly O
release O
a O
version O
of O
the O
AMI O
corpus O
preprocessed O
for O
this O
subtask O
, O
to O
foster O
research O
on O
this O
topic O
, O
( O
2 O
) O
we O
propose O
one O
of O
the O
Ô¨Årst O
applications O
of O
energy O
- O
based O
learning O
to O
spoken O
language O
understanding O
, O
and O
( O
3 O
) O
we O
introduce O
a O
novel O
utterance O
encoder O
featuring O
three O
types O
of O
self O
- O
attention O
mechanisms O
and O
taking O
contextual O
and O
temporal O
information O
into O
account O
. O
3 O
Energy O
framework O
Energy O
- O
Based O
Modeling O
( O
EBM O
) O
( O
LeCun O
and O
Huang O
, O
2005 O
; O
Lecun O
et O
al O
. O
, O
2006 O
) O
is O
a O
uniÔ¨Åed O
framework O
that O
can O
be O
applied O
to O
many O
machine O
learning314X O
X O
Y O
X O
Y O
Z O
Y O
  O
( O
a O
) O
( O
b O
) O
( O
c O
) O
Figure O
3 O
: O
Three O
EBM O
architectures O
. O
When O
all O
Gs O
andWs O
are O
equal O
, O
( O
b O
) O
and O
( O
c O
) O
correspond O
to O
the O
siamese O
/ O
triplet O
cases O
. O
problems O
. O
In O
EBM O
, O
an O
energy O
function O
assigns O
a O
scalar O
called O
energy O
to O
each O
pair O
of O
random O
variables O
( O
X O
, O
Y O
) O
. O
The O
energy O
can O
be O
interpreted O
as O
the O
incompatibility O
between O
the O
values O
of O
XandY. O
Training O
consists O
in O
Ô¨Ånding O
the O
parameters O
W‚àóof O
the O
energy O
function O
EWthat O
, O
for O
all O
( O
Xi O
, O
Yi)in O
the O
training O
setSof O
sizeP O
, O
assign O
low O
energy O
to O
compatible O
( O
correct O
) O
combinations O
and O
high O
energy O
to O
all O
other O
incompatible O
( O
incorrect O
) O
ones O
. O
This O
is O
done O
by O
minimizing O
a O
loss O
functional2L O
: O
W‚àó= O
arg O
min O
W‚ààWL(EW(X O
, O
Y O
) O
, O
S O
) O
( O
1 O
) O
For O
a O
given O
X O
, O
prediction O
consists O
in O
Ô¨Ånding O
the O
value O
ofYthat O
minimizes O
the O
energy O
. O
3.1 O
Single O
architecture O
In O
the O
EBM O
framework O
, O
a O
regression O
problem O
can O
be O
formulated O
as O
shown O
in O
Fig O
. O
3a O
, O
where O
the O
input O
Xis O
passed O
through O
a O
regressor O
model O
GWand O
the O
scalar O
output O
is O
compared O
to O
the O
desired O
outputYwith O
a O
dissimilarity O
measure O
Dsuch O
as O
the O
squared O
error O
. O
Here O
, O
the O
energy O
function O
is O
the O
loss O
functional O
to O
be O
minimized O
. O
L=1 O
PP O
/ O
summationdisplay O
i=1EW(Xi O
, O
Yi O
) O
= O
1 O
PP O
/ O
summationdisplay O
i=1 O
/ O
bardblGW(Xi)‚àíYi O
/ O
bardbl2(2 O
) O
3.2 O
Siamese O
architecture O
In O
the O
regression O
problem O
previously O
described O
, O
the O
dependence O
between O
XandYis O
expressed O
by O
a O
direct O
mapping O
Y O
= O
f(X O
) O
, O
and O
there O
is O
a O
single O
bestY‚àófor O
everyX. O
However O
, O
when O
XandY O
are O
not O
in O
a O
predictor O
/ O
predictand O
relationship O
but O
are O
exchangeable O
instances O
of O
the O
same O
family O
of O
objects O
, O
there O
is O
no O
such O
mapping O
. O
E.g. O
, O
in O
paraphrase O
identiÔ¨Åcation O
, O
a O
sentence O
may O
be O
similar O
2the O
loss O
functional O
is O
passed O
the O
output O
of O
the O
energy O
function O
, O
unlike O
a O
loss O
function O
which O
is O
directly O
fed O
the O
output O
of O
the O
model.to O
many O
other O
ones O
, O
or O
, O
in O
language O
modeling O
, O
a O
givenn O
- O
gram O
may O
be O
likely O
to O
be O
followed O
by O
many O
different O
words O
. O
Thereby O
, O
Lecun O
et O
al O
. O
( O
2006 O
) O
introduced O
EBM O
for O
implicit O
regression O
orconstraint O
satisfaction O
( O
see O
Fig O
. O
3b O
) O
, O
in O
which O
a O
constraint O
that O
XandYmust O
satisfy O
is O
deÔ¨Åned O
, O
and O
the O
energy O
function O
measures O
the O
extent O
to O
which O
that O
constraint O
is O
violated O
: O
EW1,W2(X O
, O
Y O
) O
= O
D(GW1(X),GW2(Y O
) O
) O
( O
3 O
) O
whereGW2andGW1are O
two O
functions O
parameterized O
byW1andW2 O
. O
WhenGW1 O
= O
GW2and O
W1 O
= O
W2 O
, O
we O
obtain O
the O
well O
- O
known O
siamese O
architecture O
( O
Bromley O
et O
al O
. O
, O
1994 O
; O
Chopra O
et O
al O
. O
, O
2005 O
) O
, O
which O
has O
been O
applied O
with O
success O
to O
many O
tasks O
, O
including O
sentence O
similarity O
( O
Mueller O
and O
Thyagarajan O
, O
2016 O
) O
. O
Here O
, O
the O
constraint O
is O
determined O
by O
a O
collection O
- O
level O
set O
of O
binary O
labels O
{ O
Ci}P O
i=1.Ci= O
0indicates O
that O
( O
Xi O
, O
Yi)is O
agenuine O
pair O
( O
e.g. O
, O
two O
paraphrases O
) O
, O
while O
Ci= O
1indicates O
that O
( O
Xi O
, O
Yi O
) O
is O
an O
impostor O
pair O
( O
e.g. O
, O
two O
sentences O
with O
different O
meanings O
) O
. O
The O
function O
GWprojects O
objects O
into O
an O
embedding O
space O
such O
that O
the O
deÔ¨Åned O
dissimilarity O
measureD(e.g O
. O
, O
Euclidean O
distance O
) O
in O
that O
space O
reÔ¨Çects O
the O
notion O
of O
dissimilarity O
in O
the O
input O
space O
. O
Thus O
, O
the O
energy O
function O
can O
be O
seen O
as O
a O
metric O
to O
be O
learned O
. O
We O
experiment O
with O
deep O
neural O
network O
encoders O
asGW O
, O
and O
, O
following O
( O
Mueller O
and O
Thyagarajan O
, O
2016 O
) O
, O
we O
adopt O
the O
exponential O
negative O
Manhattan O
distance O
as O
dissimilarity O
measure O
and O
the O
mean O
squared O
error O
as O
loss O
functional O
: O
EW(X O
, O
Y O
) O
= O
1‚àíexp(‚àí/bardblGW(X)‚àíGW(Y)/bardbl1)(4 O
) O
L=1 O
PP O
/ O
summationdisplay O
i=1 O
/ O
bardblEW(Xi O
, O
Yi)‚àíCi O
/ O
bardbl2(5 O
) O
3.3 O
Triplet O
architecture O
The O
triplet O
architecture O
( O
Schroff O
et O
al O
. O
, O
2015 O
; O
Hoffer O
and O
Ailon O
, O
2015 O
; O
Wang O
et O
al O
. O
, O
2014 O
) O
, O
as O
can O
be O
seen O
in O
Fig O
. O
3c O
, O
is O
a O
direct O
extension O
of O
the O
siamese O
architecture O
that O
takes O
as O
input O
a O
triplet O
( O
X O
, O
Y O
, O
Z O
) O
in O
lieu O
of O
a O
pair O
( O
X O
, O
Y O
) O
.X O
, O
Y O
, O
andZare O
referred O
to O
as O
the O
positive O
, O
anchor O
, O
and O
negative O
objects O
, O
respectively O
. O
XandYare O
similar O
, O
while O
both O
being O
dissimilar O
toZ. O
Learning O
consists O
in O
jointly O
minimizing O
the O
positive O
- O
anchor O
energy O
EW(Xi O
, O
Yi)while O
maximizing O
the O
anchor O
- O
negative O
energy O
EW(Yi O
, O
Zi O
) O
. O
Here O
, O
we O
use O
the O
softmax O
triplet O
loss O
( O
Hoffer O
and O
Ailon O
, O
2015 O
) O
as O
our O
loss O
functional:315L=1 O
2PP O
/ O
summationdisplay O
i=1 O
/ O
parenleftbig O
/bardblne+‚àí0 O
/ O
bardbl2+/bardblne‚àí‚àí1 O
/ O
bardbl2 O
/ O
parenrightbig O
( O
6 O
) O
ne+=eEW(Xi O
, O
Yi O
) O
eEW(Xi O
, O
Yi)+eEW(Yi O
, O
Zi)(7 O
) O
ne‚àí=eEW(Yi O
, O
Zi O
) O
eEW(Xi O
, O
Yi)+eEW(Yi O
, O
Zi)(8 O
) O
wherenestands O
for O
normalized O
energy O
, O
and O
the O
dissimilarity O
measure O
is O
the O
Euclidean O
distance O
, O
i.e. O
, O
EW(Xi O
, O
Yi O
) O
= O
/bardblGW(Xi)‚àíGW(Yi)/bardbl2 O
. O
Essentially O
, O
the O
softmax O
triplet O
loss O
is O
the O
mean O
squared O
error O
between O
the O
normalized O
energy O
vector O
[ O
ne+,ne‚àí O
] O
and[0,1 O
] O
. O
We O
justify O
our O
choice O
of O
loss O
functionals O
in O
App O
. O
F. O
3.4 O
Sampling O
procedures O
We O
sample O
tuples O
from O
the O
ground O
truth O
abstractive O
communities O
to O
train O
our O
utterance O
encoder O
GW O
under O
the O
siamese O
and O
triplet O
meta O
- O
architectures O
as O
follows O
. O
Pair O
sampling O
. O
All O
utterances O
belonging O
to O
the O
same O
community O
are O
paired O
as O
genuine O
pairs O
, O
while O
impostor O
pairs O
are O
any O
two O
utterances O
coming O
from O
different O
communities O
. O
Triplet O
sampling O
. O
Utterances O
from O
the O
same O
community O
provide O
positive O
and O
anchor O
items O
, O
while O
the O
negative O
item O
is O
taken O
from O
any O
other O
community O
. O
4 O
Proposed O
utterance O
encoder O
Notation O
. O
The O
timet(as O
superscript O
) O
denotes O
the O
position O
of O
a O
given O
utterance O
in O
the O
conversation O
of O
lengthT O
, O
and O
the O
positioni(as O
subscript O
) O
denotes O
the O
position O
of O
a O
token O
within O
a O
given O
utterance O
of O
lengthN. O
E.g. O
, O
ut O
1is O
the O
representation O
of O
the O
Ô¨Årst O
token O
of O
Ut O
, O
thetthutterance O
in O
the O
transcription O
. O
Word O
encoder O
. O
As O
shown O
in O
the O
upper O
right O
corner O
of O
Fig O
. O
4 O
, O
we O
obtain O
ut O
iby O
concatenating O
the O
pre O
- O
trained O
vector O
of O
the O
corresponding O
token O
with O
the O
discourse O
features O
of O
Ut(role O
, O
position O
and O
dialogue O
act O
) O
, O
and O
passing O
the O
resulting O
vector O
to O
a O
dense O
layer O
. O
Utterance O
encoder O
. O
As O
shown O
in O
the O
center O
of O
Fig O
. O
4 O
, O
we O
represent O
Utas O
a O
sequence O
of O
N O
ddimensional O
token O
representations O
/ O
braceleftbig O
ut O
1, O
... O
,ut O
N O
/ O
bracerightbig O
. O
In O
addition O
, O
because O
there O
is O
a O
strong O
time O
dependence O
between O
utterances O
( O
see O
Fig O
. O
2 O
) O
, O
we O
inform O
the O
model O
about O
the O
preceding O
and O
following O
utterances O
when O
encoding O
Ut O
. O
To O
accomplish O
this O
, O
we O
prepend O
( O
resp O
. O
append O
) O
to O
Uta O
context O
vector O
containing O
information O
about O
theprevious O
( O
resp O
. O
next O
) O
utterances O
, O
Ô¨Ånally O
obtainingUt=/braceleftbig O
ut O
pre O
, O
ut O
1, O
... O
,ut O
N O
, O
ut O
post O
/ O
bracerightbig O
‚ààR(N+2)√ód O
. O
We O
then O
use O
a O
non O
- O
stacked O
bidirectional O
Recurrent O
Neural O
Network O
( O
RNN O
) O
with O
Gated O
Recurrent O
Units O
( O
GRU O
) O
( O
Cho O
et O
al O
. O
, O
2014 O
) O
to O
transform O
Ut O
into O
a O
sequence O
of O
annotations O
Ht‚ààR(N+2)√ó2d O
. O
In O
practice O
, O
the O
pre O
and O
post O
- O
context O
vectors O
indirectly O
initialize O
the O
left O
- O
to O
- O
right O
and O
right O
- O
to O
- O
left O
RNNs O
with O
information O
about O
the O
utterances O
preceding O
and O
following O
Ut O
. O
This O
is O
similar O
in O
spirit O
to O
the O
warm O
- O
start O
method O
of O
Wang O
et O
al O
. O
( O
2017 O
) O
, O
that O
directly O
initializes O
the O
hidden O
states O
of O
the O
RNNs O
with O
the O
context O
vectors O
. O
Self O
- O
attention O
. O
We O
then O
pass O
the O
annotations O
Htto O
a O
self O
- O
attention O
mechanism O
( O
Vaswani O
et O
al O
. O
, O
2017 O
; O
Lin O
et O
al O
. O
, O
2017 O
) O
. O
More O
precisely O
, O
Htgo O
through O
a O
dense O
layer O
and O
the O
output O
is O
compared O
via O
dot O
product O
with O
a O
trainable O
vector O
uŒ≥ O
, O
initialized O
randomly O
. O
Then O
, O
a O
probability O
distribution O
over O
the O
N+ O
2tokens O
in O
Utis O
obtained O
via O
a O
softmax O
: O
Œ≥Œ≥Œ≥t= O
softmax O
( O
uŒ≥¬∑tanh(WŒ≥Ht O
) O
) O
( O
9 O
) O
The O
attentional O
vector O
for O
Utis O
Ô¨Ånally O
computed O
as O
a O
weighted O
sum O
of O
its O
annotations O
, O
and O
, O
as O
shown O
in O
Fig O
. O
4 O
, O
is O
Ô¨Ånally O
passed O
to O
a O
dense O
layer O
to O
obtain O
the O
utterance O
embedding O
ut‚ààRdf O
: O
ut= O
dense O
/ O
parenleftBiggN+2 O
/ O
summationdisplay O
i=1Œ≥t O
iht O
i O
/ O
parenrightBigg O
( O
10 O
) O
uŒ≥can O
be O
interpreted O
as O
a O
learned O
representation O
of O
the O
‚Äú O
ideal O
word O
‚Äù O
, O
on O
average O
. O
The O
more O
similar O
a O
token O
vector O
is O
to O
this O
representation O
, O
the O
more O
attention O
the O
model O
pays O
to O
the O
token O
. O
Context O
encoder O
: O
level O
1 O
. O
The O
pre O
and O
postcontext O
vectors O
that O
we O
prepend O
and O
append O
to O
Ut O
are O
obtained O
by O
aggregating O
information O
from O
the O
Cutterances O
preceding O
and O
following O
Ut O
: O
ut O
pre‚Üêaggregate O
pre O
/ O
parenleftbig O
/ O
braceleftbig O
Ut‚àíC, O
... O
,Ut‚àí1 O
/ O
bracerightbig O
/ O
parenrightbig O
( O
11 O
) O
ut O
post‚Üêaggregate O
post O
/ O
parenleftbig O
/ O
braceleftbig O
Ut+1, O
... O
,Ut+C O
/ O
bracerightbig O
/ O
parenrightbig O
( O
12 O
) O
whereC O
, O
the O
context O
size O
, O
is O
a O
hyperparameter O
. O
Since O
ut O
preandut O
postwill O
become O
part O
of O
utterance O
Utwhich O
is O
a O
sequence O
of O
token O
vectors O
, O
and O
fed O
to O
the O
RNN O
, O
we O
need O
them O
to O
live O
in O
the O
same O
space O
as O
any O
other O
token O
vector O
. O
This O
forbids O
the O
use O
of O
any O
nonlinear O
or O
dimension O
- O
changing O
transformation O
inaggregate O
, O
such O
as O
convolutional O
or O
recurrent O
operations O
. O
Therefore O
, O
we O
use O
self O
- O
attention O
only O
. O
More O
precisely O
, O
we O
propose O
a O
two O
- O
level O
hierarchical O
architecture O
that O
makes O
use O
of O
a O
different O
type O
of O
self O
- O
attention O
at O
each O
level O
( O
see O
left O
part O
of O
Fig O
. O
4 O
) O
. O
The O
pre O
and O
post O
- O
context O
encoders O
share O
the O
exact316utterance O
encoder O
ùë¢1ùë°‚àíùê∂ùë¢2ùë°‚àíùê∂ùë¢ùëÅùë°‚àíùê∂ O
‚Ä¶ O
ùë¢1ùë°‚àí1ùë¢2ùë°‚àí1ùë¢ùëÅùë°‚àí1 O
‚Ä¶‚Ä¶ O
ùë¢1ùë°‚àí2ùë¢2ùë°‚àí2ùë¢ùëÅùë°‚àí2 O
‚Ä¶ O
Œ≤ O
  O
Œ± O
  O
Œ± O
  O
Œ±pre O
- O
context O
encoder O
( O
CEpre O
) O
  O
Œ± O
  O
Œ≤ O
  O
Œ≥content O
- O
aware O
self O
- O
attention O
time O
- O
aware O
self O
- O
attention O
self O
- O
attention O
ùëàùë°‚àí1ùëàùë°‚àí2ùëàùë°‚àíùê∂DA O
wordvecdense O
embword O
encoder O
( O
WE O
) O
pos O
roleùë¢ùëñùë°to O
energy O
functionùë¢ùëùùëüùëíùë° O
  O
Œ≥ O
‚Ä¶ O
bidirGRU O
ùë¢1ùë°ùë¢ùëÅùë°ùë¢ùëùùëüùëíùë° O
WEùë¢2ùë° O
WE O
WE O
‚Ä¶ O
CEpre O
CEpostùë¢ùëùùëúùë†ùë°ùë° O
ùëàùë°denseFigure O
4 O
: O
Our O
proposed O
utterance O
encoder O
GW O
. O
Only O
the O
pre O
- O
context O
encoder O
is O
shown O
. O
Cis O
the O
context O
size O
. O
same O
architecture O
, O
so O
we O
only O
describe O
the O
precontext O
encoder O
in O
what O
follows O
. O
Content O
- O
aware O
self O
- O
attention O
. O
At O
level O
1 O
, O
we O
apply O
the O
same O
attention O
mechanism O
to O
each O
utterance O
in O
/ O
braceleftbig O
Ut‚àíC, O
... O
,Ut‚àí1 O
/ O
bracerightbig O
. O
E.g. O
, O
for O
Ut‚àí1 O
: O
Œ±Œ±Œ±t‚àí1= O
softmax O
/ O
parenleftbigg O
uŒ±¬∑tanh O
/ O
parenleftBig O
WŒ±Ut‚àí1+W O
/ O
primeN O
/ O
summationdisplay O
i=1ut O
i O
/ O
parenrightBig O
/ O
parenrightbigg O
( O
13 O
) O
This O
mechanism O
is O
the O
same O
as O
in O
Eq O
. O
9 O
, O
except O
for O
two O
differences O
. O
First O
, O
we O
operate O
directly O
on O
the O
matrix O
of O
token O
vectors O
of O
the O
previous O
utterance O
Ut‚àí1rather O
than O
on O
RNN O
annotations O
. O
Second O
, O
there O
is O
an O
extra O
input O
that O
consists O
of O
the O
elementwise O
sum O
of O
the O
token O
vectors O
of O
the O
current O
utterance O
Ut O
. O
The O
latter O
modiÔ¨Åcation O
is O
inspired O
by O
the O
coverage O
vectors O
used O
in O
translation O
and O
summarization O
to O
address O
under(over)-translation O
and O
repetition O
, O
e.g. O
, O
( O
Tu O
et O
al O
. O
, O
2016 O
; O
See O
et O
al O
. O
, O
2017 O
) O
. O
In O
our O
case O
, O
we O
hope O
that O
by O
letting O
the O
model O
know O
about O
the O
tokens O
in O
the O
current O
utterance O
Ut O
, O
it O
will O
be O
able O
to O
extract O
complementary O
-rather O
than O
redundant- O
information O
from O
its O
context O
, O
and O
thus O
produce O
a O
richer O
embedding O
. O
To O
recapitulate O
, O
the O
content O
- O
aware O
attention O
mechanism O
transforms O
the O
sequence O
of O
utterance O
matrices O
/ O
braceleftbig O
Ut‚àíC, O
... O
,Ut‚àí1 O
/ O
bracerightbig O
‚ààRC√óN√ódinto O
a O
sequence O
of O
vectors O
/ O
braceleftbig O
ut‚àíC, O
... O
,ut‚àí1 O
/ O
bracerightbig O
‚ààRC√ód O
. O
These O
vectors O
are O
then O
aggregated O
into O
a O
single O
precontext O
vector O
ut O
pre‚ààRdas O
described O
next O
. O
Note O
that O
since O
there O
is O
no O
inherent O
difference O
between O
preceding O
and O
following O
utterances3 O
, O
we O
use O
the O
same O
content O
- O
aware O
self O
- O
attention O
mechanism O
for O
the O
pre O
and O
post O
contexts O
. O
This O
also O
gives O
us O
a O
more O
parsimonious O
and O
faster O
model O
. O
One O
should O
note O
, O
however O
, O
that O
the O
pre O
and O
post O
- O
context O
encoders O
still O
differ O
in O
terms O
of O
their O
time O
- O
aware O
attention O
mechanisms O
at O
level O
2 O
, O
described O
next O
. O
3indeed O
, O
the O
latter O
become O
the O
former O
as O
we O
slide O
the O
window O
over O
the O
transcriptionContext O
encoder O
: O
level O
2 O
. O
As O
can O
be O
seen O
in O
Fig O
. O
2 O
, O
two O
utterances O
close O
to O
each O
other O
in O
time O
are O
much O
more O
likely O
to O
be O
related O
( O
e.g. O
, O
adjacency O
pair O
, O
elaboration O
... O
) O
than O
any O
two O
randomly O
selected O
utterances O
. O
To O
enable O
our O
model O
to O
capture O
such O
time O
dependence O
, O
we O
use O
the O
trainable O
universal O
time O
- O
decay O
attention O
mechanism O
of O
Su O
et O
al O
. O
( O
2018 O
) O
. O
Time O
- O
aware O
self O
- O
attention O
. O
The O
mechanism O
combines O
three O
types O
of O
time O
- O
decay O
functions O
via O
weightswi O
. O
The O
attentional O
coefÔ¨Åcient O
for O
ut‚àí1 O
is O
: O
Œ≤t‚àí1 O
= O
w1Œ≤convt‚àí1+w2Œ≤lint‚àí1+w3Œ≤conct‚àí1(14 O
) O
= O
w1 O
a(dt‚àí1)b+w2[edt‚àí1+k]++w3 O
1 O
+ O
/parenleftbigdt‚àí1 O
D0 O
/ O
parenrightbigl O
where O
[ O
‚àó]+=max(‚àó,0)(ReLU),dt‚àí1is O
the O
offset O
between O
the O
positions O
of O
Ut‚àí1andUt O
, O
i.e. O
,dt‚àí1= O
|t‚àí(t‚àí1)|= O
1 O
, O
and O
thewi‚Äôs O
, O
a O
, O
b O
, O
e O
, O
k O
, O
D0 O
, O
andl O
are O
scalar O
parameters O
learned O
during O
training O
. O
The O
convex O
( O
conv O
) O
, O
linear O
( O
lin O
) O
, O
and O
concave O
( O
conc O
) O
terms O
respectively O
assume O
the O
strength O
of O
dependence O
to O
weaken O
rapidly O
, O
linearly O
, O
and O
slowly O
, O
as O
the O
distance O
in O
time O
increases O
. O
The O
post O
- O
context O
mechanism O
can O
be O
obtained O
by O
symmetry O
. O
It O
has O
different O
parameters O
. O
5 O
Community O
detection O
Once O
the O
utterance O
encoder O
GWpresented O
in O
Section O
4 O
has O
been O
trained O
within O
the O
siamese O
or O
triplet O
meta O
- O
architecture O
presented O
in O
Section O
3 O
, O
it O
is O
used O
to O
project O
the O
summary O
- O
worthy O
utterances O
from O
a O
given O
test O
transcription O
to O
a O
compact O
embedding O
space O
. O
We O
assume O
that O
if O
training O
was O
successful O
, O
the O
distance O
in O
that O
space O
encodes O
community O
structure O
, O
so O
that O
a O
basic O
clustering O
algorithm O
such O
as O
k O
- O
means O
( O
MacQueen O
, O
1967 O
) O
is O
enough O
to O
capture O
it O
. O
However O
, O
since O
we O
need O
to O
detect O
overlapping O
communities O
, O
we O
use O
a O
probabilistic O
version O
of O
k O
- O
means O
, O
the O
Fuzzy O
c O
- O
Means O
( O
FCM O
) O
algorithm O
( O
Bezdek O
et O
al O
. O
,3171984 O
) O
. O
FCM O
returns O
a O
probability O
distribution O
over O
all O
communities O
for O
each O
utterance O
. O
More O
details O
are O
provided O
in O
App O
. O
E. O
6 O
Experimental O
setup O
6.1 O
Dataset O
We O
experiment O
on O
the O
AMI O
corpus O
( O
McCowan O
et O
al O
. O
, O
2005 O
) O
, O
with O
the O
manual O
annotations O
v1.6.2 O
. O
The O
corpus O
contains O
data O
for O
more O
than O
100 O
meetings O
, O
in O
which O
participants O
play O
4 O
roles O
within O
a O
design O
team O
whose O
task O
is O
to O
develop O
a O
prototype O
of O
TV O
remote O
control O
. O
Each O
meeting O
is O
associated O
with O
the O
annotations O
described O
in O
the O
introduction O
and O
shown O
in O
Fig O
. O
2 O
. O
There O
are O
2368 O
unique O
abstractive O
communities O
in O
total O
, O
whose O
statistics O
are O
shown O
in O
Table O
1 O
. O
We O
adopt O
the O
ofÔ¨Åcially O
suggested O
scenario O
- O
only O
partition4 O
, O
which O
provides O
97 O
, O
20 O
, O
and O
20 O
meetings O
respectively O
for O
training O
, O
validation O
and O
testing O
. O
We O
use O
manual O
transcriptions O
, O
and O
do O
not O
apply O
any O
particular O
preprocessing O
except O
Ô¨Åltering O
out O
speciÔ¨Åc O
ASR O
tags O
, O
such O
as O
vocalsound O
. O
typeabstract O
action O
problem O
decision O
total O
unique O
1147 O
247 O
380 O
594 O
2368 O
disjoint O
528 O
124 O
69 O
45 O
766 O
overlapping O
349 O
17 O
163 O
149 O
678 O
singleton O
49 O
162 O
38 O
244 O
493 O
Table O
1 O
: O
Statistics O
of O
abstractive O
communities O
. O
6.2 O
Baselines O
Full O
baseline O
details O
are O
provided O
in O
App O
. O
B. O
‚Ä¢Encoders O
. O
First O
, O
we O
evaluate O
our O
utterance O
encoder O
against O
two O
encoders O
that O
are O
trained O
within O
the O
energy O
framework O
: O
( O
1 O
) O
LD(Lee O
and O
Dernoncourt O
, O
2016 O
) O
, O
a O
sequential O
sentence O
encoder O
developed O
for O
dialogue O
act O
classiÔ¨Åcation O
; O
and O
( O
2 O
) O
HAN O
( O
Yang O
et O
al O
. O
, O
2016 O
) O
, O
a O
hierarchical O
self O
- O
attentive O
network O
for O
document O
embedding O
. O
Note O
that O
to O
be O
fair O
, O
we O
ensure O
that O
both O
LD O
and O
HAN O
have O
access O
to O
context O
( O
see O
details O
in O
App O
. O
B O
) O
. O
We O
also O
compare O
our O
full O
pipeline O
against O
unsupervised O
and O
supervised O
systems O
. O
‚Ä¢Unsupervised O
systems O
. O
In O
( O
1 O
) O
tf O
- O
idf O
, O
we O
combine O
the O
TF O
- O
IDF O
vectors O
of O
the O
current O
utterance O
and O
the O
context O
utterances O
, O
each O
concatenated O
with O
their O
discourse O
features O
, O
and O
apply O
FCM O
. O
In O
( O
2)w2v O
, O
we O
repeat O
the O
same O
approach O
with O
the O
word2vec O
centroids O
of O
the O
words O
in O
each O
utterance O
. O
We O
also O
compare O
our O
full O
pipeline O
against O
LCseg O
( O
Galley O
et O
al O
. O
, O
2003 O
) O
, O
a O
lexical O
- O
cohesion O
based O
topic O
4http://groups.inf.ed.ac.uk/ami/ O
corpus O
/ O
datasets.shtmlsegmenter O
that O
directly O
clusters O
utterances O
without O
computing O
embeddings O
. O
‚Ä¢Supervised O
systems O
. O
Finally O
, O
here O
, O
we O
use O
an O
approach O
similar O
to O
that O
of O
Murray O
et O
al O
. O
( O
2012 O
) O
. O
More O
precisely O
, O
we O
train O
a O
MLP O
to O
learn O
abstractive O
links O
between O
utterances O
, O
and O
then O
apply O
the O
CONGA O
community O
detection O
algorithm O
to O
the O
utterance O
graph O
. O
We O
also O
considered O
4 O
variants O
of O
our O
model O
: O
( O
1 O
) O
CA O
- O
S O
: O
we O
replace O
the O
time O
- O
aware O
self O
- O
attention O
mechanism O
of O
the O
context O
encoder O
with O
basic O
selfattention O
. O
( O
2 O
) O
S O
- O
S O
: O
we O
replace O
both O
the O
contentaware O
and O
the O
time O
- O
aware O
self O
- O
attention O
mechanisms O
of O
the O
context O
encoder O
with O
basic O
selfattention O
. O
( O
3 O
) O
( O
0,0 O
) O
: O
our O
model O
, O
without O
using O
the O
contextual O
encoder O
. O
( O
4 O
) O
( O
3,0 O
) O
: O
our O
model O
, O
using O
only O
pre O
- O
context O
, O
with O
a O
small O
window O
of O
3 O
, O
to O
enable O
fair O
comparison O
with O
the O
LD O
baseline O
. O
6.3 O
Training O
details O
Word O
encoder O
. O
Discourse O
features O
consist O
of O
two O
one O
- O
hot O
vectors O
of O
dimensions O
4 O
and O
16 O
, O
respectively O
for O
speaker O
role O
and O
dialogue O
act O
. O
The O
positional O
feature O
is O
a O
scalar O
in O
[ O
0,1 O
] O
, O
indicating O
the O
normalized O
position O
of O
the O
utterance O
in O
the O
transcription O
. O
We O
used O
the O
pre O
- O
trained O
vectors O
learned O
on O
the O
Google O
News O
corpus O
with O
word2vec O
by O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
, O
and O
randomly O
initialized O
out O
- O
of O
- O
vocabulary O
words O
( O
1645 O
out O
of O
12412 O
) O
. O
As O
a O
preprocessing O
step O
, O
we O
reduced O
the O
dimensionality O
of O
the O
pre O
- O
trained O
word O
vectors O
from O
300 O
to O
21 O
with O
PCA O
, O
in O
order O
to O
give O
equal O
importance O
to O
discourse O
and O
textual O
features O
. O
In O
the O
end O
, O
tokens O
are O
thus O
represented O
by O
a O
d= O
42 O
-dimensional O
vector O
. O
Layer O
sizes O
. O
For O
our O
model O
, O
and O
the O
LD O
and O
HAN O
baselines O
, O
we O
set O
df= O
32 O
( O
output O
dimension O
of O
the O
Ô¨Ånal O
dense O
layer O
) O
. O
LD O
. O
We O
set O
d1=3 O
and O
d2=0 O
, O
which O
is O
very O
close O
to O
( O
2,0 O
) O
, O
the O
best O
conÔ¨Åguration O
reported O
in O
the O
original O
paper O
. O
HAN O
. O
Again O
, O
for O
the O
sake O
of O
fairness O
, O
we O
give O
the O
HAN O
baseline O
access O
to O
contextual O
information O
, O
by O
feeding O
it O
the O
current O
utterance O
surrounded O
by O
the O
Cbpreceding O
and O
Cbfollowing O
utterances O
in O
the O
transcription O
, O
where O
Cbdenotes O
the O
best O
context O
size O
reported O
in O
Section O
7 O
. O
Training O
details O
. O
The O
exact O
same O
token O
representations O
and O
settings O
were O
used O
for O
our O
model O
, O
its O
variants O
, O
and O
the O
baselines O
. O
Models O
were O
trained O
on O
the O
training O
set O
for O
30 O
epochs O
with O
the O
Adam O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
optimizer O
. O
The O
best O
epoch318was O
selected O
as O
the O
one O
associated O
with O
the O
lowest O
validation O
loss O
. O
Batch O
size O
and O
dropout O
( O
Srivastava O
et O
al O
. O
, O
2014 O
) O
were O
set O
to O
16 O
and O
0.5 O
. O
Dropout O
was O
applied O
to O
the O
word O
embedding O
layer O
only O
. O
To O
account O
for O
randomness O
, O
we O
average O
results O
over O
10 O
runs O
. O
Also O
, O
following O
( O
Hoffer O
and O
Ailon O
, O
2015 O
; O
Liu O
et O
al O
. O
, O
2019 O
) O
, O
we O
use O
a O
different O
, O
small O
subset O
of O
all O
possible O
triplets O
for O
training O
at O
each O
epoch O
( O
more O
precisely O
, O
15594 O
triplets O
) O
. O
This O
intelligently O
maximizes O
data O
usage O
while O
preventing O
overÔ¨Åtting O
. O
To O
enable O
fair O
comparison O
with O
the O
siamese O
approach O
, O
15594 O
genuine O
and O
15594 O
impostor O
pairs O
were O
sampled O
at O
the O
beginning O
of O
each O
epoch O
, O
since O
we O
consider O
that O
one O
triplet O
essentially O
equates O
one O
genuine O
pair O
and O
one O
impostor O
pair O
. O
Performance O
evaluation O
. O
We O
evaluate O
performance O
at O
the O
distance O
and O
the O
clustering O
level O
, O
using O
respectively O
precision O
, O
recall O
, O
and O
F1 O
score O
at O
k O
, O
and O
the O
Omega O
Index O
( O
Collins O
and O
Dent O
, O
1988 O
) O
following O
the O
previous O
ACD O
work O
of O
Murray O
et O
al O
. O
( O
2012 O
) O
. O
For O
P O
, O
R O
, O
and O
F1 O
, O
we O
evaluate O
the O
quality O
of O
the O
ranking O
of O
the O
closest O
utterances O
to O
a O
given O
query O
utterance O
. O
We O
use O
a O
Ô¨Åxed O
k=10 O
and O
also O
a O
variablek(denoted O
as O
k O
= O
v O
) O
, O
wherekis O
equal O
to O
the O
size O
of O
the O
community O
of O
the O
query O
utterance O
minus O
one O
. O
In O
that O
case O
, O
P O
= O
R O
= O
F1 O
. O
More O
details O
and O
examples O
are O
given O
in O
Appendices O
C O
and O
D. O
For O
the O
Omega O
Index O
, O
we O
report O
results O
with O
a O
Ô¨Åxed O
number O
of O
communities O
|Q|=11 O
, O
and O
also O
a O
variable|Q|(|Q|=v O
) O
, O
where|Q|is O
equal O
to O
the O
number O
of O
ground O
truth O
communities O
. O
More O
details O
and O
examples O
are O
given O
in O
App O
. O
C. O
Due O
to O
the O
stochastic O
nature O
of O
the O
FCM O
algorithm O
, O
we O
select O
the O
run O
yielding O
the O
smallest O
objective O
function O
value O
over O
20 O
runs O
. O
7 O
Results O
Context O
sizes O
. O
Larger O
contexts O
bring O
richer O
information O
, O
but O
increase O
the O
risk O
of O
considering O
unrelated O
utterances O
. O
Using O
our O
proposed O
encoder O
within O
the O
triplet O
meta O
- O
architecture O
, O
we O
tried O
different O
values O
of O
Con O
the O
validation O
set O
, O
under O
two O
settings O
: O
( O
pre O
, O
post O
) O
= O
( O
C,0 O
) O
, O
and O
( O
pre O
, O
post O
) O
= O
( O
C O
, O
C O
) O
. O
Results O
are O
shown O
in O
Fig O
. O
5 O
. O
We O
can O
observe O
that O
increasing O
Calways O
brings O
improvement O
, O
with O
diminishing O
returns O
. O
Results O
also O
clearly O
show O
that O
considering O
the O
following O
utterances O
in O
addition O
to O
the O
preceding O
ones O
is O
useful O
. O
Note O
that O
the O
curves O
look O
similar O
for O
F1@k= O
10 O
. O
In O
the O
end O
, O
we O
selected O
( O
11,11 O
) O
as O
our O
best O
context O
sizes O
. O
Quantitative O
results O
. O
Final O
test O
set O
results O
are O
0 O
3 O
5 O
7 O
9 O
11 O
130.480.490.500.510.52 O
with O
pre O
context O
only O
with O
pre O
and O
post O
contextsFigure O
5 O
: O
Impact O
of O
context O
size O
on O
the O
validation O
P@k O
= O
v O
, O
for O
our O
model O
trained O
within O
the O
triplet O
meta O
- O
architecture O
. O
shown O
in O
Table O
2 O
. O
All O
variants O
of O
our O
model O
signiÔ¨Åcantly O
outperform O
LD O
. O
While O
HAN O
is O
much O
stronger O
than O
LD O
, O
our O
model O
and O
its O
variants O
using O
best O
context O
sizes O
manage O
to O
outperform O
it O
everywhere O
, O
except O
in O
the O
siamese O
/ O
P@k O
= O
v O
case O
( O
row O
j O
) O
. O
One O
of O
the O
reasons O
for O
the O
superiority O
of O
our O
utterance O
encoder O
is O
probably O
that O
it O
considers O
contextual O
information O
while O
encoding O
the O
current O
utterance O
, O
while O
HAN O
and O
LD O
take O
as O
input O
the O
context O
utterances O
together O
with O
the O
current O
utterance O
, O
without O
distinguishing O
between O
them O
. O
Moreover O
, O
we O
use O
an O
attention O
mechanism O
dedicated O
to O
temporality O
, O
whereas O
HAN O
is O
only O
able O
to O
capture O
an O
implicit O
notion O
of O
time O
through O
the O
use O
of O
recurrence O
( O
RNN O
) O
, O
and O
LD O
, O
with O
its O
dense O
layers O
, O
completely O
ignores O
it O
. O
Also O
, O
all O
variants O
of O
our O
model O
using O
best O
context O
sizes O
( O
11,11 O
) O
outperform O
the O
ones O
using O
reduced O
( O
3,0 O
) O
or O
no O
( O
0,0 O
) O
context O
, O
regardless O
of O
the O
metaarchitecture O
. O
This O
conÔ¨Årms O
the O
value O
added O
by O
our O
context O
encoder O
. O
For O
siamese O
, O
our O
model O
outperforms O
its O
two O
variants O
( O
CA O
- O
S O
andS O
- O
S O
) O
for O
all O
metrics O
, O
indicating O
that O
both O
the O
content O
- O
aware O
and O
the O
time O
- O
aware O
selfattention O
mechanisms O
are O
useful O
. O
However O
, O
it O
is O
interesting O
to O
note O
that O
when O
training O
under O
the O
triplet O
conÔ¨Åguration O
, O
the O
CA O
- O
S O
variant O
of O
our O
model O
is O
better O
, O
suggesting O
that O
in O
that O
case O
, O
the O
content O
- O
aware O
mechanism O
is O
beneÔ¨Åcial O
, O
but O
the O
time O
- O
aware O
one O
is O
not O
. O
LCseg O
( O
row O
m O
) O
and O
tf O
- O
idf O
( O
11,11 O
) O
( O
row O
n3 O
) O
are O
the O
best O
of O
all O
( O
un)supervised O
baseline O
systems O
, O
but O
both O
perform O
signiÔ¨Åcantly O
worse O
than O
all O
energybased O
approaches O
, O
highlighting O
that O
training O
with O
the O
energy O
framework O
is O
beneÔ¨Åcial O
. O
In O
terms O
of O
Omega O
Index O
, O
supervised O
baseline O
systems O
are O
logically O
better O
than O
unsupervised O
ones O
. O
w2v O
generally O
outperforms O
tf O
- O
idf O
when O
there O
is O
no O
context O
( O
rows O
k1,l1,n1,o1 O
) O
or O
short O
context O
( O
k2,l2,n2,o2 O
) O
, O
but O
not O
with O
large O
contexts O
( O
k3,l3,n3,o3 O
) O
. O
Results O
also O
show O
that O
overall O
, O
using319(pre O
, O
P O
P O
R O
F1 O
Omega O
index√ó100 O
post O
) O
@k O
= O
v O
@k= O
10 O
|Q|=v|Q|= O
11 O
a1 O
) O
our O
model O
( O
0 O
, O
0 O
) O
54.59 O
46.05 O
62.45 O
43.18 O
49.09 O
48.81 O
a2 O
) O
our O
model O
( O
3 O
, O
0 O
) O
55.17 O
46.17 O
62.80 O
43.25 O
49.78 O
49.70 O
a3 O
) O
our O
model O
( O
11 O
, O
11 O
) O
58.58 O
46.73 O
63.82 O
43.83 O
49.90 O
49.28 O
Triplet O
b O
) O
our O
model O
( O
CA O
- O
S O
) O
( O
11 O
, O
11 O
) O
59.52‚ãÜ46.98‚ãÜ64.01‚ãÜ44.06‚ãÜ50.11 O
49.73 O
c O
) O
our O
model O
( O
S O
- O
S O
) O
( O
11 O
, O
11 O
) O
58.96 O
46.81 O
63.65 O
43.87 O
49.59 O
49.88 O
d O
) O
LD O
( O
3 O
, O
0 O
) O
52.04 O
44.82 O
60.41 O
41.82 O
48.70 O
48.14 O
e O
) O
HAN O
( O
11 O
, O
11 O
) O
58.72 O
45.76 O
62.60 O
42.89 O
49.32 O
48.88 O
f1 O
) O
our O
model O
( O
0 O
, O
0 O
) O
53.01 O
45.10 O
60.97 O
42.12 O
50.56 O
49.65 O
f2 O
) O
our O
model O
( O
3 O
, O
0 O
) O
53.78 O
45.54 O
61.33 O
42.48 O
51.01 O
50.00 O
f3 O
) O
our O
model O
( O
11 O
, O
11 O
) O
56.64 O
46.47 O
62.54 O
43.40 O
52.44‚ãÜ51.88‚ãÜ O
Siamese O
g O
) O
our O
model O
( O
CA O
- O
S O
) O
( O
11 O
, O
11 O
) O
56.46 O
46.08 O
61.92 O
43.02 O
51.60 O
50.98 O
h O
) O
our O
model O
( O
S O
- O
S O
) O
( O
11 O
, O
11 O
) O
55.68 O
45.64 O
61.17 O
42.53 O
52.26 O
51.11 O
i O
) O
LD O
( O
3 O
, O
0 O
) O
52.13 O
44.83 O
60.85 O
41.86 O
51.18 O
50.70 O
j O
) O
HAN O
( O
11 O
, O
11 O
) O
58.54 O
45.72 O
61.55 O
42.74 O
50.51 O
49.82 O
k1 O
) O
tf O
- O
idf O
( O
0 O
, O
0 O
) O
29.28 O
26.67 O
34.69 O
24.19 O
13.12 O
13.66 O
k2 O
) O
tf O
- O
idf O
( O
3 O
, O
0 O
) O
34.77 O
30.27 O
40.83 O
27.79 O
10.22 O
10.17 O
k3 O
) O
tf O
- O
idf O
( O
11 O
, O
11 O
) O
58.94 O
43.94 O
61.36 O
41.45 O
38.09 O
39.47 O
Unsupervised O
l1 O
) O
w2v O
( O
0 O
, O
0 O
) O
29.02 O
27.46 O
37.39 O
25.11 O
13.89 O
13.50 O
l2 O
) O
w2v O
( O
3 O
, O
0 O
) O
34.11 O
29.92 O
39.55 O
27.32 O
10.61 O
10.77 O
l3 O
) O
w2v O
( O
11 O
, O
11 O
) O
58.30 O
44.08 O
61.59 O
41.59 O
37.75 O
38.28 O
m O
) O
LCSeg O
- O
- O
- O
- O
- O
38.98 O
41.57 O
n1 O
) O
tf O
- O
idf O
( O
0 O
, O
0 O
) O
- O
- O
- O
- O
25.04 O
25.14 O
n2 O
) O
tf O
- O
idf O
( O
3 O
, O
0 O
) O
- O
- O
- O
- O
27.33 O
26.95 O
Supervised O
n3 O
) O
tf O
- O
idf O
( O
11 O
, O
11 O
) O
- O
- O
- O
- O
45.26 O
44.91 O
o1 O
) O
w2v O
( O
0 O
, O
0 O
) O
- O
- O
- O
- O
25.32 O
25.25 O
o2 O
) O
w2v O
( O
3 O
, O
0 O
) O
- O
- O
- O
- O
29.14 O
29.02 O
o3 O
) O
w2v O
( O
11 O
, O
11 O
) O
- O
- O
- O
- O
43.31 O
43.08 O
Table O
2 O
: O
Results O
( O
averaged O
over O
10 O
runs).‚ãÜ O
: O
best O
score O
per O
column O
. O
Bold O
: O
best O
score O
per O
section O
. O
- O
: O
does O
not O
apply O
as O
the O
method O
does O
not O
produce O
utterance O
embeddings O
. O
larger O
contexts O
always O
brings O
improvement O
. O
Qualitative O
results O
. O
We O
visualize O
in O
App O
. O
A O
that O
the O
three O
self O
- O
attention O
mechanisms O
behave O
in O
a O
cooperative O
manner O
to O
produce O
a O
meaningful O
utterance O
representation O
. O
We O
also O
inspect O
the O
closest O
utterances O
to O
a O
given O
query O
utterance O
in O
App O
. O
D. O
We O
also O
visualize O
the O
time O
- O
aware O
self O
- O
attention O
coefÔ¨Åcients O
in O
Fig O
. O
6 O
, O
and O
Ô¨Ånd O
that O
interestingly O
, O
the O
distributions O
over O
the O
pre O
and O
post O
- O
context O
are O
not O
symmetric O
. O
Indeed O
, O
only O
the O
utterances O
immediately O
following O
Ut(t+ O
1‚Üít+ O
5 O
) O
seem O
to O
matter O
, O
while O
the O
attention O
weights O
are O
much O
more O
uniform O
across O
the O
utterances O
preceding O
Ut O
. O
This O
suggests O
that O
in O
dialogues O
, O
considering O
a O
long O
history O
of O
preceding O
utterances O
helps O
understanding O
the O
current O
one O
. O
Overall O
, O
the O
three O
terms O
( O
see O
Eq O
. O
14 O
) O
altogether O
do O
produce O
a O
universal O
function O
that O
decreases O
as O
time O
distance O
increases O
, O
which O
is O
in O
accordance O
with O
intuition O
. O
SimpliÔ¨Åed O
task O
. O
Finally O
, O
we O
also O
experimented O
on O
a O
much O
simpler O
task O
, O
where O
only O
the O
communities O
of O
type O
ABSTRACT O
were O
considered O
. O
This O
makes O
ACD O
much O
simpler O
, O
because O
most O
of O
the O
overlapping O
communities O
are O
of O
the O
other O
types O
( O
see O
Table O
1 O
) O
. O
For O
this O
simpliÔ¨Åed O
task O
, O
we O
have O
1147 O
unique O
-1 O
- O
2 O
- O
3 O
- O
4 O
- O
5 O
- O
6 O
- O
7 O
- O
8 O
- O
9 O
- O
10 O
- O
11 O
t+1 O
+ O
2 O
+ O
3 O
+ O
4 O
+ O
5 O
+ O
6 O
+ O
7 O
+ O
8 O
+ O
9 O
+ O
10 O
+ O
110.000.050.100.150.200.250.300.35 O
  O
convex O
linear O
concav O
universalFigure O
6 O
: O
Normalized O
time O
- O
aware O
self O
- O
attention O
weights O
for O
pre O
and O
post O
- O
contexts O
, O
averaged O
over O
10 O
runs O
. O
communities O
, O
of O
which O
78.99 O
% O
are O
disjoint O
. O
our O
model O
achieves O
72.09 O
in O
terms O
of O
P@k O
= O
vand O
55.67 O
in O
terms O
of O
Omega O
Index O
when O
|Q|=v O
. O
P O
, O
R O
, O
F O
1@k= O
15 O
are O
respectively O
equal O
to O
55.07 O
, O
74.37 O
, O
and O
54.00 O
, O
and O
the O
Omega O
Index O
is O
54.30 O
when|Q|= O
8 O
. O
8 O
Conclusion O
This O
paper O
proposes O
one O
of O
the O
Ô¨Årst O
applications O
of O
energy O
- O
based O
learning O
to O
ACD O
. O
Using O
the O
siamese O
and O
triplet O
meta O
- O
architectures O
, O
we O
showed O
that O
our O
novel O
contextual O
utterance O
encoder O
learns O
better O
distance O
and O
communities O
than O
state O
- O
of O
- O
the O
- O
art O
competitors.320Acknowledgments O
This O
research O
was O
supported O
in O
part O
by O
the O
OpenPaaS::NG O
and O
LinTo O
( O
Lorr O
¬¥ O
e O
et O
al O
. O
, O
2019 O
) O
projects O
. O
Abstract O
Modern O
task O
- O
oriented O
dialog O
systems O
need O
to O
reliably O
understand O
users O
‚Äô O
intents O
. O
Intent O
detection O
is O
even O
more O
challenging O
when O
moving O
to O
new O
domains O
or O
new O
languages O
, O
since O
there O
is O
little O
annotated O
data O
. O
To O
address O
this O
challenge O
, O
we O
present O
a O
suite O
of O
pretrained O
intent O
detection O
models O
which O
can O
predict O
a O
broad O
range O
of O
intended O
goals O
from O
many O
actions O
because O
they O
are O
trained O
on O
wikiHow O
, O
a O
comprehensive O
instructional O
website O
. O
Our O
models O
achieve O
state O
- O
of O
- O
the O
- O
art O
results O
on O
the O
Snips O
dataset O
, O
the O
Schema O
- O
Guided O
Dialogue O
dataset O
, O
and O
all O
3 O
languages O
of O
the O
Facebook O
multilingual O
dialog O
datasets O
. O
Our O
models O
also O
demonstrate O
strong O
zero- O
and O
few O
- O
shot O
performance O
, O
reaching O
over O
75 O
% O
accuracy O
using O
only O
100 O
training O
examples O
in O
all O
datasets.1 O
1 O
Introduction O
Task O
- O
oriented O
dialog O
systems O
like O
Apple O
‚Äôs O
Siri O
, O
Amazon O
Alexa O
, O
and O
Google O
Assistant O
have O
become O
pervasive O
in O
smartphones O
and O
smart O
speakers O
. O
To O
support O
a O
wide O
range O
of O
functions O
, O
dialog O
systems O
must O
be O
able O
to O
map O
a O
user O
‚Äôs O
natural O
language O
instruction O
onto O
the O
desired O
skill O
or O
API O
. O
Performing O
this O
mapping O
is O
called O
intent O
detection O
. O
Intent O
detection O
is O
usually O
formulated O
as O
a O
sentence O
classiÔ¨Åcation O
task O
. O
Given O
an O
utterance O
( O
e.g. O
‚Äú O
wake O
me O
up O
at O
8 O
‚Äù O
) O
, O
a O
system O
needs O
to O
predict O
its O
intent O
( O
e.g. O
‚Äú O
Set O
an O
Alarm O
‚Äù O
) O
. O
Most O
modern O
approaches O
use O
neural O
networks O
to O
jointly O
model O
intent O
detection O
and O
slot O
Ô¨Ålling O
( O
Xu O
and O
Sarikaya O
, O
2013 O
; O
Liu O
and O
Lane O
, O
2016 O
; O
Goo O
et O
al O
. O
, O
2018 O
; O
Zhang O
et O
al O
. O
, O
2019 O
) O
. O
In O
response O
to O
a O
rapidly O
growing O
range O
of O
services O
, O
more O
attention O
has O
been O
given O
to O
zero O
- O
shot O
intent O
detection O
( O
Ferreira O
et O
al O
. O
, O
2015a O
, O
b O
; O
Yazdani O
and O
Henderson O
, O
2015 O
; O
Chen O
et O
al O
. O
, O
2016 O
; O
Kumar O
et O
al O
. O
, O
2017 O
; O
Gangadharaiah O
and O
1The O
data O
and O
models O
are O
available O
at O
https:// O
github.com/zharry29/wikihow-intent O
.Narayanaswamy O
, O
2019 O
) O
. O
While O
most O
existing O
research O
on O
intent O
detection O
proposed O
novel O
model O
architectures O
, O
few O
have O
attempted O
data O
augmentation O
. O
One O
such O
work O
( O
Hu O
et O
al O
. O
, O
2009 O
) O
showed O
that O
models O
can O
learn O
much O
knowledge O
that O
is O
important O
for O
intent O
detection O
from O
massive O
online O
resources O
such O
as O
Wikipedia O
. O
We O
propose O
a O
pretraining O
task O
based O
on O
wikiHow O
, O
a O
comprehensive O
instructional O
website O
with O
over O
110,000 O
professionally O
edited O
articles O
. O
Their O
topics O
span O
from O
common O
sense O
such O
as O
‚Äú O
How O
to O
Download O
Music O
‚Äù O
to O
more O
niche O
tasks O
like O
‚Äú O
How O
to O
Crochet O
a O
Teddy O
Bear O
. O
‚Äù O
We O
observe O
that O
the O
header O
of O
each O
step O
in O
a O
wikiHow O
article O
describes O
an O
action O
and O
can O
be O
approximated O
as O
an O
utterance O
, O
while O
the O
title O
describes O
a O
goal O
and O
can O
be O
seen O
as O
an O
intent O
. O
For O
example O
, O
‚Äú O
Ô¨Ånd O
good O
gas O
prices O
‚Äù O
in O
the O
article O
‚Äú O
How O
to O
Save O
Money O
on O
Gas O
‚Äù O
is O
similar O
to O
the O
utterance O
‚Äú O
where O
can O
I O
Ô¨Ånd O
cheap O
gas O
? O
‚Äù O
with O
the O
intent O
‚Äú O
Save O
Money O
on O
Gas O
. O
‚Äù O
Hence O
, O
we O
introduce O
a O
dataset O
based O
on O
wikiHow O
, O
where O
a O
model O
predicts O
the O
goal O
of O
an O
action O
given O
some O
candidates O
. O
Although O
most O
of O
wikiHow O
‚Äôs O
domains O
are O
far O
beyond O
the O
scope O
of O
any O
present O
dialog O
system O
, O
models O
pretrained O
on O
our O
dataset O
would O
be O
robust O
to O
emerging O
services O
and O
scenarios O
. O
Also O
, O
as O
wikiHow O
is O
available O
in O
18 O
languages O
, O
our O
pretraining O
task O
can O
be O
readily O
extended O
to O
multilingual O
settings O
. O
Using O
our O
pretraining O
task O
, O
we O
Ô¨Åne O
- O
tune O
transformer O
language O
models O
, O
achieving O
state O
- O
of O
- O
the O
- O
art O
results O
on O
the O
intent O
detection O
task O
of O
the O
Snips O
dataset O
( O
Coucke O
et O
al O
. O
, O
2018 O
) O
, O
the O
Schema O
- O
Guided O
Dialog O
( O
SGD O
) O
dataset O
( O
Rastogi O
et O
al O
. O
, O
2019 O
) O
, O
and O
all O
3 O
languages O
( O
English O
, O
Spanish O
, O
and O
Thai O
) O
of O
the O
Facebook O
multilingual O
dialog O
datasets O
( O
Schuster O
et O
al O
. O
, O
2019 O
) O
, O
with O
statistically O
signiÔ¨Åcant O
improvements O
. O
As O
our O
accuracy O
is O
close O
to O
100 O
% O
on O
all O
these O
datasets O
, O
we O
further O
experiment O
with O
zero- O
or O
few O
- O
shot O
settings O
. O
Our O
models O
achieve O
over O
70 O
% O
accuracy O
with O
no O
in O
- O
domain O
training O
data O
on O
Snips328and O
SGD O
, O
and O
over O
75 O
% O
with O
only O
100 O
training O
examples O
on O
all O
datasets O
. O
This O
highlights O
our O
models O
‚Äô O
ability O
to O
quickly O
adapt O
to O
new O
utterances O
and O
intents O
in O
unseen O
domains O
. O
2 O
WikiHow O
Pretraining O
Task O
2.1 O
Corpus O
We O
crawl O
the O
wikiHow O
website O
in O
English O
, O
Spanish O
, O
and O
Thai O
( O
the O
languages O
were O
chosen O
to O
match O
those O
in O
the O
Facebook O
multilingual O
dialog O
datasets O
) O
. O
We O
deÔ¨Åne O
the O
goal O
of O
each O
artcle O
as O
its O
title O
stripped O
of O
the O
preÔ¨Åx O
‚Äú O
How O
to O
‚Äù O
( O
and O
its O
equivalent O
in O
other O
languages O
) O
. O
We O
extract O
a O
set O
of O
steps O
for O
each O
article O
, O
by O
taking O
the O
bolded O
header O
of O
each O
paragraph O
. O
2.2 O
WikiHow O
Pretraining O
Dataset O
A O
wikiHow O
article O
‚Äôs O
goal O
can O
approximate O
an O
intent O
, O
and O
each O
step O
in O
it O
can O
approximate O
an O
associated O
utterance O
. O
We O
formulate O
the O
pretraining O
task O
as O
a O
4choose-1 O
multiple O
choice O
format O
: O
given O
a O
step O
, O
the O
model O
infers O
the O
correct O
goal O
among O
4 O
candidates O
. O
For O
example O
, O
given O
the O
step O
‚Äú O
let O
check O
- O
in O
agents O
and O
Ô¨Çight O
attendants O
know O
if O
it O
‚Äôs O
a O
special O
occasion O
‚Äù O
and O
the O
candidate O
goals O
: O
A. O
Get O
Upgraded O
to O
Business O
Class O
B. O
Change O
a O
Flight O
Reservation O
C. O
Check O
Flight O
Reservations O
D. O
Use O
a O
Discount O
Airline O
Broker O
the O
correct O
goal O
would O
be O
A. O
This O
is O
similar O
to O
intent O
detection O
, O
where O
a O
system O
is O
given O
a O
user O
utterance O
and O
then O
must O
select O
a O
supported O
intent O
. O
We O
create O
intent O
detection O
pretraining O
data O
using O
goal O
- O
step O
pairs O
from O
each O
wikiHow O
article O
. O
Each O
article O
contributes O
at O
least O
one O
positive O
goal O
- O
step O
pair O
. O
However O
, O
it O
is O
challenging O
to O
sample O
negative O
candidate O
goals O
for O
a O
given O
step O
. O
There O
are O
two O
reasons O
for O
this O
. O
First O
, O
random O
sampling O
of O
goals O
correctly O
results O
in O
true O
negatives O
, O
but O
they O
tend O
to O
be O
so O
distant O
from O
the O
positive O
goal O
that O
the O
classiÔ¨Åcation O
task O
becomes O
trivial O
and O
the O
model O
does O
not O
learn O
sufÔ¨Åciently O
. O
Second O
, O
if O
we O
sample O
goals O
that O
are O
similar O
to O
the O
positive O
goal O
, O
then O
they O
might O
not O
be O
true O
negatives O
, O
since O
there O
are O
many O
steps O
in O
wikiHow O
often O
with O
overlapping O
goals O
. O
To O
sample O
high O
- O
quality O
negative O
training O
instances O
, O
we O
start O
with O
the O
correct O
goal O
and O
search O
in O
its O
article O
‚Äôs O
‚Äú O
related O
articles O
‚Äù O
section O
for O
an O
article O
whose O
title O
has O
the O
least O
lexical O
overlap O
with O
the O
current O
goal O
. O
We O
recursively O
do O
this O
until O
we O
have O
enough O
candidates O
. O
Empirically O
, O
examples O
created O
this O
way O
are O
mostly O
clean O
, O
with O
an O
example O
shown O
above O
. O
We O
select O
onepositive O
goal O
- O
step O
pair O
from O
each O
article O
by O
picking O
its O
longest O
step O
. O
In O
total O
, O
our O
wikiHow O
pretraining O
datasets O
have O
107,298 O
English O
examples O
, O
64,803 O
Spanish O
examples O
, O
and O
6,342 O
Thai O
examples O
. O
3 O
Experiments O
We O
Ô¨Åne O
- O
tune O
a O
suite O
of O
off O
- O
the O
- O
shelf O
language O
models O
pretrained O
on O
our O
wikiHow O
data O
, O
and O
evaluate O
them O
on O
3 O
major O
intent O
detection O
benchmarks O
. O
3.1 O
Models O
We O
Ô¨Åne O
- O
tune O
a O
pretrained O
RoBERTa O
model O
( O
Liu O
et O
al O
. O
, O
2019 O
) O
for O
the O
English O
datasets O
and O
a O
pretrained O
XLM O
- O
RoBERTa O
model O
( O
Conneau O
et O
al O
. O
, O
2019 O
) O
for O
the O
multilingual O
datasets O
. O
We O
cast O
the O
instances O
of O
the O
intent O
detection O
datasets O
into O
a O
multiple O
- O
choice O
format O
, O
where O
the O
utterance O
is O
the O
input O
and O
the O
full O
set O
of O
intents O
are O
the O
possible O
candidates O
, O
consistent O
with O
our O
wikiHow O
pretraining O
task O
. O
For O
each O
model O
, O
we O
append O
a O
linear O
classiÔ¨Åcation O
layer O
with O
cross O
- O
entropy O
loss O
to O
calculate O
a O
likelihood O
for O
each O
candidate O
, O
and O
output O
the O
candidate O
with O
the O
maximum O
likelihood O
. O
For O
each O
intent O
detection O
dataset O
in O
any O
language O
, O
we O
consider O
the O
following O
settings O
: O
+ O
in O
- O
domain O
( O
+ O
ID O
): O
a O
model O
is O
only O
trained O
on O
the O
dataset O
‚Äôs O
in O
- O
domain O
training O
data O
; O
+ O
wikiHow O
+ O
in O
- O
domain O
( O
+ O
WH+ID O
): O
a O
model O
is O
Ô¨Årst O
trained O
on O
our O
wikiHow O
data O
in O
the O
corresponding O
language O
, O
and O
then O
trained O
on O
the O
dataset O
‚Äôs O
indomain O
training O
data O
; O
+ O
wikiHow O
zero O
- O
shot O
( O
+ O
WH O
0 O
- O
shot O
): O
a O
model O
is O
trained O
only O
on O
our O
wikiHow O
data O
in O
the O
corresponding O
language O
, O
and O
then O
applied O
directly O
to O
the O
dataset O
‚Äôs O
evaluation O
data O
. O
For O
non O
- O
English O
languages O
, O
the O
corresponding O
wikiHow O
data O
might O
suffer O
from O
smaller O
sizes O
and O
lower O
quality O
. O
Hence O
, O
we O
additionally O
consider O
the O
following O
cross O
- O
lingual O
transfer O
settings O
for O
non O
- O
English O
datasets O
: O
+ O
en O
wikiHow O
+ O
in O
- O
domain O
( O
+ O
enWH+ID O
) O
, O
a O
model O
is O
trained O
on O
wikiHow O
data O
in O
English O
, O
before O
it O
is O
trained O
on O
the O
dataset O
‚Äôs O
in O
- O
domain O
training O
data O
; O
+ O
en O
wikiHow O
zero O
- O
shot O
( O
+ O
enWH O
0 O
- O
shot O
) O
, O
a O
model O
is O
trained O
on O
wikiHow O
data O
in O
English O
, O
before O
it O
is O
directly O
applied O
to O
the O
dataset O
‚Äôs O
evaluation O
data O
. O
3.2 O
Datasets O
We O
consider O
the O
3 O
following O
benchmarks O
: O
The O
Snips O
dataset O
( O
Coucke O
et O
al O
. O
, O
2018 O
) O
is O
a O
single O
- O
turn O
English O
dataset O
. O
It O
is O
one O
of O
the O
most O
cited O
dialog O
benchmarks O
in O
recent O
years O
, O
containing329Training O
SizeValid O
. O
SizeTest O
SizeNum O
. O
Intents O
Snips O
2,100 O
700 O
N O
/ O
A O
7 O
SGD O
163,197 O
24,320 O
42,922 O
4 O
FB O
- O
en O
30,521 O
4,181 O
8,621 O
12 O
FB O
- O
es O
3,617 O
1,983 O
3,043 O
12 O
FB O
- O
th O
2,156 O
1,235 O
1,692 O
12 O
Table O
1 O
: O
Statistics O
of O
the O
dialog O
benchmark O
datasets O
. O
utterances O
collected O
from O
the O
Snips O
personal O
voice O
assistant O
. O
While O
its O
full O
training O
data O
has O
13,784 O
examples O
, O
we O
Ô¨Ånd O
that O
our O
models O
only O
need O
its O
smaller O
training O
split O
consisting O
of O
2,100 O
examples O
to O
achieve O
high O
performance O
. O
Since O
Snips O
does O
not O
provide O
test O
sets O
, O
we O
use O
the O
validation O
set O
for O
testing O
and O
the O
full O
training O
set O
for O
validation O
. O
Snips O
involves O
7 O
intents O
, O
including O
Add O
to O
Playlist O
, O
Rate O
Book O
, O
Book O
Restaurant O
, O
Get O
Weather O
, O
Play O
Music O
, O
Search O
Creative O
Work O
, O
and O
Search O
Screening O
Event O
. O
Some O
example O
utterances O
include O
‚Äú O
Play O
the O
newest O
melody O
on O
Last O
Fm O
by O
Eddie O
Vinson O
, O
‚Äù O
‚Äú O
Find O
the O
movie O
schedule O
in O
the O
area O
, O
‚Äù O
etc O
. O
The O
Schema O
- O
Guided O
Dialogue O
dataset O
( O
SGD O
) O
( O
Rastogi O
et O
al O
. O
, O
2019 O
) O
is O
a O
multi O
- O
turn O
English O
dataset O
. O
It O
is O
the O
largest O
dialog O
corpus O
to O
date O
spanning O
dozens O
of O
domains O
and O
services O
, O
used O
in O
the O
DSTC8 O
challenge O
( O
Rastogi O
et O
al O
. O
, O
2020 O
) O
with O
dozens O
of O
team O
submissions O
. O
Schemas O
are O
provided O
with O
at O
most O
4 O
intents O
per O
dialog O
turn O
. O
Examples O
of O
these O
intents O
include O
Buy O
Movie O
Tickets O
for O
a O
Particular O
show O
, O
Make O
a O
Reservation O
with O
the O
Therapist O
, O
Book O
an O
Appointment O
at O
a O
Hair O
Stylist O
, O
Browse O
attractions O
in O
a O
given O
city O
, O
etc O
. O
At O
each O
turn O
, O
we O
use O
the O
last O
3 O
utterances O
as O
input O
. O
An O
example O
: O
‚Äú O
That O
sounds O
fun O
. O
What O
other O
attractions O
do O
you O
recommend O
? O
There O
is O
a O
famous O
place O
of O
worship O
called O
Akshardham O
. O
‚Äù O
The O
Facebook O
multilingual O
datasets O
( O
FBen O
/ O
es O
/ O
th O
) O
( O
Schuster O
et O
al O
. O
, O
2019 O
) O
is O
a O
single O
- O
turn O
multilingual O
dataset O
. O
It O
is O
the O
only O
multilingual O
dialog O
dataset O
to O
the O
best O
of O
our O
knowledge O
, O
containing O
utterances O
annotated O
with O
intents O
and O
slots O
in O
English O
( O
en O
) O
, O
Spanish O
( O
es O
) O
, O
and O
Thai O
( O
th O
) O
. O
It O
involves O
12 O
intents O
, O
including O
Set O
Reminder O
, O
Check O
Sunrise O
, O
Show O
Alarms O
, O
Check O
Sunset O
, O
Cancel O
Reminder O
, O
Show O
Reminders O
, O
Check O
Time O
Left O
on O
Alarm O
, O
Modify O
Alarm O
, O
Cancel O
Alarm O
, O
Find O
Weather O
, O
Set O
Alarm O
, O
and O
Snooze O
Alarm O
. O
Some O
example O
utterances O
are O
‚Äú O
Is O
my O
alarm O
set O
for O
10 O
am O
today O
? O
‚Äù O
‚Äú O
Colocar O
una O
alarma O
para O
ma O
Àúnana O
a O
las O
3 O
am O
, O
‚Äù O
  O
etc O
. O
Snips O
SGD O
FB O
- O
en O
( O
Ren O
and O
Xue O
, O
2020 O
) O
.993 O
N O
/ O
A O
.993 O
( O
Ma O
et O
al O
. O
, O
2019 O
) O
N O
/ O
A O
.948 O
N O
/ O
A O
+ O
in O
- O
domain O
( O
+ O
ID O
) O
.990 O
.942 O
.993 O
( O
ours O
) O
+ O
WH+ID O
.994 O
.951‚Ä† O
.995‚Ä† O
( O
ours O
) O
+ O
WH O
0 O
- O
shot O
.713 O
.787 O
.445 O
Chance O
.143 O
.250 O
.083 O
Table O
2 O
: O
The O
accuracy O
of O
intent O
detection O
on O
English O
datasets O
using O
RoBERTa O
. O
State O
- O
of O
- O
the O
- O
art O
performances O
are O
in O
bold O
; O
‚Ä†indicates O
statistically O
signiÔ¨Åcant O
improvement O
from O
the O
previous O
state O
- O
of O
- O
the O
- O
art O
. O
FB O
- O
en O
FB O
- O
es O
FB O
- O
th O
( O
Ren O
and O
Xue O
, O
2020 O
) O
.993 O
N O
/ O
A O
N O
/ O
A O
( O
Zhang O
et O
al O
. O
, O
2019 O
) O
N O
/ O
A O
.978 O
.967 O
+ O
in O
- O
domain O
( O
+ O
ID O
) O
.993 O
.986 O
.962 O
( O
ours O
) O
+ O
WH+ID O
.995 O
.988 O
.971 O
( O
ours O
) O
+ O
enWH+ID O
.995 O
.990‚Ä† O
.976‚Ä† O
( O
ours O
) O
+ O
WH O
0 O
- O
shot O
.416 O
.129 O
.119 O
( O
ours O
) O
+ O
enWH O
0 O
- O
shot O
.416 O
.288 O
.124 O
Chance O
.083 O
.083 O
.083 O
Table O
3 O
: O
The O
accuracy O
of O
intent O
detection O
on O
multilingual O
datasets O
using O
XLM O
- O
RoBERTa O
. O
Statistics O
of O
the O
datasets O
are O
shown O
in O
Table O
1 O
. O
3.3 O
Baselines O
We O
compare O
our O
models O
with O
the O
previous O
state O
- O
ofthe O
- O
art O
results O
of O
each O
dataset O
: O
‚Ä¢Ren O
and O
Xue O
( O
2020 O
) O
proposed O
a O
Siamese O
neural O
network O
with O
triplet O
loss O
, O
achieving O
state O
- O
of O
- O
the O
- O
art O
results O
on O
Snips O
and O
FB O
- O
en O
; O
‚Ä¢Zhang O
et O
al O
. O
( O
2019 O
) O
used O
multi O
- O
task O
learning O
to O
jointly O
learn O
intent O
detection O
and O
slot O
Ô¨Ålling O
, O
achieving O
state O
- O
of O
- O
the O
- O
art O
results O
on O
FB O
- O
es O
and O
FB O
- O
th O
; O
‚Ä¢Ma O
et O
al O
. O
( O
2019 O
) O
augmented O
the O
data O
via O
backtranslation O
to O
and O
from O
Chinese O
, O
achieving O
state O
- O
ofthe O
- O
art O
results O
on O
SGD O
. O
3.4 O
Modelling O
Details O
After O
experimenting O
with O
base O
and O
large O
models O
, O
we O
use O
RoBERTa O
- O
large O
for O
the O
English O
datasets O
and O
XLM O
- O
RoBERTa O
- O
base O
for O
the O
multilingual O
dataset O
for O
best O
performances O
. O
All O
our O
models O
are O
implemented O
using O
the O
HuggingFace O
Transformer O
library2 O
. O
We O
tune O
our O
model O
hyperparameters O
on O
the O
validation O
sets O
of O
the O
datasets O
we O
experiment O
with O
. O
However O
, O
in O
all O
cases O
, O
we O
use O
a O
uniÔ¨Åed O
setting O
2https://github.com/huggingface/ O
transformers33000.20.40.60.81 O
.953 O
.470Snips O
( O
RoBERTa O
) O
.531.755SGD O
( O
RoBERTa O
) O
.458.884FB O
- O
en O
( O
RoBERTa O
) O
10 O
100 O
1,00000.20.40.60.81.894 O
.481FB O
- O
en O
( O
XLM O
- O
RoBERTa O
) O
10 O
100 O
1,000.845 O
.663 O
.349FB O
- O
es O
( O
XLM O
- O
RoBERTa O
) O
10 O
100 O
1,000.853 O
.851 O
.341FB O
- O
th O
( O
XLM O
- O
RoBERTa O
) O
+ O
ID O
( O
ours)+WH+ID O
( O
ours)+enWH+ID O
Chance O
Figure O
1 O
: O
Learning O
curves O
of O
models O
in O
low O
- O
resource O
settings O
. O
The O
vertical O
axis O
is O
the O
accuracy O
of O
intent O
detection O
, O
while O
the O
horizontal O
axis O
is O
the O
number O
of O
in O
- O
domain O
training O
examples O
of O
each O
task O
, O
distorted O
to O
log O
- O
scale O
. O
which O
empirically O
performs O
well O
, O
using O
the O
Adam O
optimizer O
( O
Kingma O
and O
Ba O
, O
2014 O
) O
with O
an O
epsilon O
of1e‚àí8 O
, O
a O
learning O
rate O
of O
5e‚àí6 O
, O
maximum O
sequence O
length O
of O
80 O
and O
3 O
epochs O
. O
We O
variate O
the O
batch O
size O
from O
2 O
to O
16 O
according O
to O
the O
number O
of O
candidates O
in O
the O
multiple O
- O
choice O
task O
, O
to O
avoid O
running O
out O
of O
memory O
. O
We O
save O
the O
model O
every O
1,000 O
training O
steps O
, O
and O
choose O
the O
model O
with O
the O
highest O
validation O
performance O
to O
be O
evaluated O
on O
the O
test O
set O
. O
We O
run O
our O
experiments O
on O
an O
NVIDIA O
GeForce O
RTX O
2080 O
Ti O
GPU O
, O
with O
half O
- O
precision O
Ô¨Çoating O
point O
format O
( O
FP16 O
) O
with O
O1 O
optimization O
. O
Each O
epoch O
takes O
up O
to O
90 O
minutes O
in O
the O
most O
resource O
intensive O
setting O
, O
i.e. O
running O
a O
RoBERTa O
- O
large O
on O
around O
100,000 O
training O
examples O
of O
our O
wikiHow O
pretraining O
dataset O
. O
3.5 O
Results O
The O
performance O
of O
RoBERTa O
on O
the O
English O
datasets O
( O
Snips O
, O
SGD O
, O
and O
FB O
- O
en O
) O
are O
shown O
in O
Table O
2 O
. O
We O
repeat O
each O
experiment O
20 O
times O
, O
report O
the O
mean O
accuracy O
, O
and O
calculate O
its O
p O
- O
value O
against O
the O
previous O
state O
- O
of O
- O
the O
- O
art O
result O
, O
using O
a O
one O
- O
sample O
and O
one O
- O
tailed O
t O
- O
test O
with O
a O
signiÔ¨Åcance O
level O
of O
0.05 O
. O
Our O
models O
achieve O
state O
- O
of O
- O
the O
- O
art O
results O
using O
the O
available O
in O
- O
domain O
training O
data O
. O
Moreover O
, O
our O
wikiHow O
data O
enables O
our O
models O
todemonstrate O
strong O
performances O
in O
zero O
- O
shot O
settings O
with O
no O
in O
- O
domain O
training O
data O
, O
implying O
our O
models O
‚Äô O
strong O
potential O
to O
adapt O
to O
new O
domains O
. O
The O
performance O
of O
XLM O
- O
RoBERTa O
on O
the O
multilingual O
datasets O
( O
FB O
- O
en O
, O
FB O
- O
es O
, O
and O
FB O
- O
th O
) O
are O
shown O
in O
Table O
3 O
. O
Our O
models O
achieve O
state O
- O
of O
- O
theart O
results O
on O
all O
3 O
languages O
. O
While O
our O
wikiHow O
data O
in O
Spanish O
and O
Thai O
does O
improve O
models O
‚Äô O
performances O
, O
its O
effect O
is O
less O
salient O
than O
the O
English O
wikiHow O
data O
. O
Our O
experiments O
above O
focus O
on O
settings O
where O
all O
available O
in O
- O
domain O
training O
data O
are O
used O
. O
However O
, O
modern O
task O
- O
oriented O
dialog O
systems O
must O
rapidly O
adapt O
to O
burgeoning O
services O
( O
e.g. O
Alexa O
Skills O
) O
in O
different O
languages O
, O
where O
little O
training O
data O
are O
available O
. O
To O
simulate O
low O
- O
resource O
settings O
, O
we O
repeat O
the O
experiments O
with O
exponentially O
increasing O
number O
of O
training O
examples O
up O
to O
1,000 O
. O
We O
consider O
the O
models O
trained O
only O
on O
in O
- O
domain O
data O
( O
+ O
ID O
) O
, O
those O
Ô¨Årst O
pretrained O
on O
our O
wikiHow O
data O
in O
corresponding O
languages O
( O
+ O
WH+ID O
) O
, O
and O
those O
Ô¨Årst O
pretrained O
on O
our O
English O
wikiHow O
data O
( O
+ O
enWH+ID O
) O
for O
FB O
- O
es O
and O
FB O
- O
th O
. O
The O
learning O
curves O
of O
each O
dataset O
are O
shown O
in O
Figure O
1 O
. O
Though O
the O
vanilla O
transformers O
models O
( O
+ O
ID O
) O
achieve O
close O
to O
state O
- O
of O
- O
the O
- O
art O
performance O
with O
access O
to O
the O
full O
training O
data O
( O
see O
Table O
2331and O
3 O
) O
, O
they O
struggle O
in O
the O
low O
- O
resource O
settings O
. O
When O
given O
up O
to O
100 O
in O
- O
domain O
training O
examples O
, O
their O
accuracies O
are O
less O
than O
50 O
% O
on O
most O
datasets O
. O
In O
contrast O
, O
our O
models O
pretrained O
on O
our O
wikiHow O
data O
( O
+ O
WH+ID O
) O
can O
reach O
over O
75 O
% O
accuracy O
given O
only O
100 O
training O
examples O
on O
all O
datasets O
. O
4 O
Discussion O
and O
Future O
Work O
As O
our O
model O
performances O
exceed O
99 O
% O
on O
Snips O
and O
FB O
- O
en O
, O
the O
concern O
arises O
that O
these O
intent O
detection O
datasets O
are O
‚Äú O
solved O
‚Äù O
. O
We O
address O
this O
by O
performing O
error O
analysis O
and O
providing O
future O
outlooks O
for O
intent O
detection O
. O
4.1 O
Error O
Analysis O
Our O
model O
misclassiÔ¨Åes O
7 O
instances O
in O
the O
Snips O
test O
set O
. O
Among O
them O
, O
6 O
utterances O
include O
proper O
nouns O
on O
which O
intent O
classiÔ¨Åcation O
is O
contingent O
. O
For O
example O
, O
the O
utterance O
‚Äú O
please O
open O
Zvooq O
‚Äù O
assumes O
the O
knowledge O
that O
Zvooq O
is O
a O
streaming O
service O
, O
and O
its O
labelled O
intent O
is O
‚Äú O
Play O
Music O
. O
‚Äù O
Our O
model O
misclassiÔ¨Åes O
43 O
instances O
in O
the O
FBen O
test O
set O
. O
Among O
them O
, O
10 O
has O
incorrect O
labels O
: O
e.g. O
the O
labelled O
intent O
of O
‚Äú O
have O
alarm O
go O
off O
at O
5 O
pm O
‚Äù O
is O
‚Äú O
Show O
Alarms O
, O
‚Äù O
while O
our O
model O
prediction O
‚Äú O
Set O
Alarm O
‚Äù O
is O
in O
fact O
correct O
. O
28 O
are O
ambiguous O
: O
e.g. O
the O
labelled O
intent O
of O
‚Äú O
repeat O
alarm O
every O
weekday O
‚Äù O
is O
‚Äú O
Set O
Alarm O
, O
‚Äù O
whereas O
that O
of O
‚Äú O
add O
an O
alarm O
for O
2:45 O
on O
every O
Monday O
‚Äù O
is O
‚Äú O
Modify O
Alarm O
. O
‚Äù O
We O
only O
Ô¨Ånd O
1 O
example O
an O
interesting O
edge O
case O
: O
the O
gold O
intent O
of O
‚Äú O
remind O
me O
if O
there O
will O
be O
a O
rain O
forecast O
tomorrow O
‚Äù O
is O
‚Äú O
Find O
Weather O
, O
‚Äù O
while O
our O
model O
incorrectly O
chooses O
‚Äú O
Set O
Reminder O
. O
‚Äù O
By O
performing O
manual O
error O
analyses O
on O
our O
model O
predictions O
, O
we O
observe O
that O
most O
misclassiÔ¨Åed O
examples O
involve O
ambiguous O
wordings O
, O
wrong O
labels O
, O
or O
obscure O
proper O
nouns O
. O
Our O
observations O
imply O
that O
Snips O
and O
FB O
- O
en O
might O
be O
too O
easy O
to O
effectively O
evaluate O
future O
models O
. O
4.2 O
Open O
- O
Domain O
Intent O
Detection O
State O
- O
of O
- O
the O
- O
art O
models O
now O
achieve O
greater O
than O
99 O
% O
percent O
accuracy O
on O
standard O
benchmarks O
for O
intent O
detection O
. O
However O
, O
intent O
detection O
is O
far O
from O
being O
solved O
. O
The O
standard O
benchmarks O
only O
have O
a O
dozen O
intents O
, O
but O
future O
dialog O
systems O
will O
need O
to O
support O
many O
more O
functions O
with O
intents O
from O
a O
wide O
range O
of O
domains O
. O
To O
demonstrate O
that O
our O
pretrained O
models O
can O
adapt O
to O
unseen O
, O
open O
- O
domain O
intents O
, O
we O
hold O
out O
5,000 O
steps O
( O
as O
utterances O
) O
with O
their O
corresponding O
goals O
( O
as O
intents O
) O
from O
our O
wikiHow O
dataset O
as O
a O
proxy O
of O
anintent O
detection O
dataset O
with O
more O
than O
100,000 O
possible O
intents O
( O
all O
goals O
in O
wikiHow O
) O
. O
For O
each O
step O
, O
we O
sample O
100 O
goals O
with O
the O
highest O
embedding O
similarity O
to O
the O
correct O
goal O
, O
as O
most O
other O
goals O
are O
irrelevant O
. O
We O
then O
rank O
them O
for O
the O
likelihood O
that O
the O
step O
helps O
achieve O
them O
. O
Our O
RoBERTa O
model O
achieves O
a O
mean O
reciprocal O
rank O
of O
0.462 O
and O
a O
36 O
% O
accuracy O
of O
ranking O
the O
correct O
goal O
Ô¨Årst O
. O
As O
a O
qualitative O
example O
, O
given O
the O
step O
‚Äú O
Ô¨Ånd O
the O
order O
that O
you O
want O
to O
cancel O
, O
‚Äù O
the O
top O
3 O
ranked O
steps O
are O
‚Äú O
Cancel O
an O
Order O
on O
eBay O
‚Äù O
, O
‚Äú O
Cancel O
an O
Online O
Order O
‚Äù O
, O
‚Äú O
Cancel O
an O
Order O
on O
Amazon O
. O
‚Äù O
This O
hints O
that O
our O
pretrained O
models O
‚Äô O
can O
work O
with O
a O
much O
wider O
range O
of O
intents O
than O
those O
in O
current O
benchmarks O
, O
and O
suggests O
that O
future O
intent O
detection O
research O
should O
focus O
on O
open O
domains O
, O
especially O
those O
with O
little O
data O
. O
5 O
Conclusion O
By O
pretraining O
language O
models O
on O
wikiHow O
, O
we O
attain O
state O
- O
of O
- O
the O
- O
art O
results O
in O
5 O
major O
intent O
detection O
datasets O
spanning O
3 O
languages O
. O
The O
wideranging O
domains O
and O
languages O
of O
our O
pretraining O
resource O
enable O
our O
models O
to O
excel O
with O
few O
labelled O
examples O
in O
multilingual O
settings O
, O
and O
suggest O
open O
- O
domain O
intent O
detection O
is O
now O
feasible O
. O
Acknowledgments O
This O
research O
is O
based O
upon O
work O
supported O
in O
part O
by O
the O
DARPA O
KAIROS O
Program O
( O
contract O
FA8750 O
- O
19 O
- O
2 O
- O
1004 O
) O
, O
the O
DARPA O
LwLL O
Program O
( O
contract O
FA8750 O
- O
19 O
- O
2 O
- O
0201 O
) O
, O
and O
the O
IARPA O
BETTER O
Program O
( O
contract O
2019 O
- O
19051600004 O
) O
. O
Approved O
for O
Public O
Release O
, O
Distribution O
Unlimited O
. O
The O
views O
and O
conclusions O
contained O
herein O
are O
those O
of O
the O
authors O
and O
should O
not O
be O
interpreted O
as O
necessarily O
representing O
the O
ofÔ¨Åcial O
policies O
, O
either O
expressed O
or O
implied O
, O
of O
DARPA O
, O
IARPA O
, O
or O
the O
U.S. O
Government O
. O
We O
thank O
the O
anonymous O
reviewers O
for O
their O
valuable O
feedback O
. O
Abstract O
This O
work O
studies O
the O
widely O
adopted O
ancestral O
sampling O
algorithms O
for O
auto O
- O
regressive O
language O
models O
, O
which O
is O
not O
widely O
studied O
in O
the O
literature O
. O
We O
use O
the O
quality O
- O
diversity O
( O
QD O
) O
trade O
- O
off O
to O
investigate O
three O
popular O
sampling O
algorithms O
( O
top- O
k O
, O
nucleus O
and O
tempered O
sampling O
) O
. O
We O
focus O
on O
the O
task O
of O
open O
- O
ended O
language O
generation O
. O
We O
Ô¨Årst O
show O
that O
the O
existing O
sampling O
algorithms O
have O
similar O
performance O
. O
After O
carefully O
inspecting O
the O
transformations O
deÔ¨Åned O
by O
different O
sampling O
algorithms O
, O
we O
identify O
three O
key O
properties O
that O
are O
shared O
among O
them O
: O
entropy O
reduction O
, O
order O
preservation O
, O
and O
slope O
preservation O
. O
To O
validate O
the O
importance O
of O
the O
identiÔ¨Åed O
properties O
, O
we O
design O
two O
sets O
of O
new O
sampling O
algorithms O
: O
one O
set O
in O
which O
each O
algorithm O
satisÔ¨Åes O
all O
three O
properties O
, O
and O
one O
set O
in O
which O
each O
algorithm O
violates O
at O
least O
one O
of O
the O
properties O
. O
We O
compare O
their O
performance O
with O
existing O
sampling O
algorithms O
, O
and O
Ô¨Ånd O
that O
violating O
the O
identiÔ¨Åed O
properties O
could O
lead O
to O
drastic O
performance O
degradation O
, O
as O
measured O
by O
the O
Q O
- O
D O
trade O
- O
off O
. O
On O
the O
other O
hand O
, O
we O
Ô¨Ånd O
that O
the O
set O
of O
sampling O
algorithms O
that O
satisÔ¨Åes O
these O
properties O
performs O
on O
par O
with O
the O
existing O
sampling O
algorithms.1 O
1 O
Introduction O
A O
language O
model O
( O
LM O
) O
is O
a O
central O
module O
for O
natural O
language O
generation O
( O
NLG O
) O
tasks O
( O
Young O
et O
al O
. O
, O
2018 O
) O
such O
as O
machine O
translation O
( O
Wu O
et O
al O
. O
, O
2018 O
) O
, O
dialogue O
response O
generation O
( O
Li O
et O
al O
. O
, O
2017 O
) O
, O
image O
captioning O
( O
Lin O
et O
al O
. O
) O
, O
and O
related O
tasks O
. O
Given O
a O
trained O
LM O
, O
Ô¨Ånding O
the O
best O
way O
to O
generate O
a O
sample O
from O
it O
has O
been O
an O
important O
challenge O
for O
NLG O
applications O
. O
‚àóEqual O
contribution O
. O
1Our O
data O
and O
code O
are O
available O
at O
https://github.com/moinnadeem/ O
characterizing O
- O
sampling O
- O
algorithms O
. O
Figure O
1 O
: O
Human O
evaluation O
( O
y O
- O
axis O
: O
quality O
, O
x O
- O
axis O
: O
diversity O
, O
both O
are O
the O
bigger O
the O
better O
) O
shows O
that O
the O
generation O
performance O
of O
existing O
sampling O
algorithms O
are O
on O
par O
with O
each O
other O
. O
Decoding O
, O
i.e. O
, O
Ô¨Ånding O
the O
most O
probable O
output O
sequence O
from O
a O
trained O
model O
, O
is O
a O
natural O
principle O
for O
generation O
. O
The O
beam O
- O
search O
decoding O
algorithm O
approximately O
Ô¨Ånds O
the O
most O
likely O
sequence O
by O
performing O
breadth-Ô¨Årst O
search O
over O
a O
restricted O
search O
space O
. O
It O
has O
achieved O
success O
in O
machine O
translation O
, O
summarization O
, O
image O
captioning O
, O
and O
other O
subÔ¨Åelds O
. O
However O
, O
in O
the O
task O
of O
open O
- O
ended O
language O
generation O
( O
which O
is O
the O
focus O
of O
this O
work O
) O
, O
a O
signiÔ¨Åcant O
degree O
of O
diversity O
is O
required O
. O
For O
example O
, O
conditioned O
on O
the O
prompt O
‚Äú O
The O
news O
says O
that O
... O
‚Äù O
, O
the O
LM O
is O
expected O
to O
be O
able O
to O
generate O
a O
wide O
range O
of O
interesting O
continuations O
. O
While O
the O
deterministic O
behavior O
of O
decoding O
algorithms O
could O
give O
high O
- O
quality O
samples O
, O
they O
suffer O
from O
a O
serious O
lack O
of O
diversity O
. O
This O
need O
for O
diversity O
gives O
rise O
to O
a O
wide O
adoption O
of O
various O
sampling O
algorithms O
. O
Notably O
, O
topksampling O
( O
Fan O
et O
al O
. O
, O
2018 O
) O
, O
nucleus O
sampling O
( O
Holtzman O
et O
al O
. O
, O
2020 O
) O
, O
and O
tempered O
sampling O
( O
Caccia O
et O
al O
. O
, O
2020 O
) O
have O
been O
used O
in O
open O
- O
ended334generation O
( O
Radford O
et O
al O
. O
, O
2018 O
; O
Caccia O
et O
al O
. O
, O
2020 O
) O
, O
story O
generation O
( O
Fan O
et O
al O
. O
, O
2018 O
) O
, O
and O
dialogue O
response O
generation O
( O
Zhang O
et O
al O
. O
, O
2020b O
) O
. O
However O
, O
the O
sampling O
algorithm O
and O
the O
hyperparameter O
are O
usually O
chosen O
via O
heuristics O
, O
and O
a O
comprehensive O
comparison O
between O
existing O
sampling O
algorithm O
is O
lacking O
in O
the O
literature O
. O
More O
importantly O
, O
the O
underlying O
reasons O
behind O
the O
success O
of O
the O
existing O
sampling O
algorithms O
still O
remains O
poorly O
understood O
. O
In O
this O
work O
, O
we O
begin O
by O
using O
the O
qualitydiversity O
( O
Q O
- O
D O
) O
trade O
- O
off O
( O
Caccia O
et O
al O
. O
, O
2020 O
) O
to O
compare O
the O
three O
existing O
sampling O
algorithms O
. O
For O
automatic O
metrics O
, O
we O
use O
the O
BLEU O
score O
for O
quality O
and O
n O
- O
gram O
entropy O
for O
diversity O
. O
We O
also O
correlate O
these O
automatic O
metrics O
with O
human O
judgements O
. O
The O
Ô¨Årst O
observation O
we O
draw O
is O
that O
top O
- O
k O
, O
nucleus O
and O
tempered O
sampling O
perform O
on O
par O
in O
the O
Q O
- O
D O
trade O
- O
off O
, O
as O
shown O
in O
Figure O
1 O
. O
Motivated O
by O
this O
result O
, O
we O
extract O
three O
key O
properties O
by O
inspecting O
the O
transformations O
deÔ¨Åned O
by O
the O
sampling O
algorithms O
: O
( O
1 O
) O
entropy O
reduction O
, O
( O
2 O
) O
order O
preservation O
and O
( O
3 O
) O
slope O
preservation O
. O
We O
prove O
all O
three O
properties O
hold O
for O
the O
three O
existing O
sampling O
algorithms O
. O
We O
then O
set O
out O
to O
systematically O
validate O
the O
importance O
of O
the O
identiÔ¨Åed O
properties O
. O
To O
do O
so O
, O
we O
design O
two O
sets O
of O
new O
sampling O
algorithms O
in O
which O
each O
algorithm O
either O
violates O
one O
of O
the O
identiÔ¨Åed O
properties O
, O
or O
satisÔ¨Åes O
all O
properties O
. O
Using O
the O
Q O
- O
D O
trade O
- O
off O
, O
we O
compare O
their O
efÔ¨Åcacy O
against O
existing O
algorithms O
, O
and O
Ô¨Ånd O
that O
violating O
these O
identiÔ¨Åed O
properties O
could O
result O
in O
signiÔ¨Åcant O
performance O
degradation O
. O
More O
interestingly O
, O
we O
Ô¨Ånd O
that O
the O
set O
of O
sampling O
algorithms O
that O
satisÔ¨Åes O
these O
properties O
has O
generation O
performance O
that O
matches O
the O
performance O
of O
existing O
sampling O
algorithms O
. O
2 O
Sampling O
Algorithms O
for O
Autoregressive O
Language O
Models O
2.1 O
Autoregressive O
Language O
Modeling O
The O
task O
of O
autoregressive O
language O
modeling O
is O
to O
learn O
the O
probability O
distribution O
of O
the O
( O
l+ O
1)-th O
wordWl+1 O
in O
a O
sentence O
Wconditioned O
on O
the O
word O
history O
W1 O
: O
l:= O
( O
W1, O
... O
,Wl)and O
contextC. O
Here O
, O
we O
use O
Wi‚ààVto O
denote O
a O
discrete O
random O
variable O
distributed O
across O
a O
Ô¨Åxed O
vocabulary O
V. O
In O
this O
work O
, O
the O
vocabulary O
is O
constructed O
on O
subword O
level O
( O
Sennrich O
et O
al O
. O
, O
2016 O
) O
. O
Given O
a O
training O
set O
D O
, O
maximum O
likelihood O
es O
- O
timation O
( O
MLE O
) O
has O
been O
the O
most O
popular O
framework O
to O
train O
an O
autoregressive O
LM O
( O
Mikolov O
et O
al O
. O
, O
2010 O
) O
. O
MLE O
training O
minimizes O
the O
negative O
loglikelihood O
( O
NLL O
) O
objective O
below O
: O
LMLE=1 O
|D|/summationdisplay O
( O
W O
, O
C)‚ààD‚àíŒ£L‚àí1 O
l=0logPŒ∏(Wl+1|W1 O
: O
l O
, O
C O
) O
, O
( O
1 O
) O
whereŒ∏denotes O
model O
parameters O
, O
and O
PŒ∏(¬∑|W1 O
: O
l O
) O
denotes O
the O
conditional O
model O
distribution O
of O
Wl+1 O
given O
a O
preÔ¨Åx O
W1 O
: O
l. O
For O
simplicity O
, O
we O
assume O
all O
sentences O
are O
of O
length O
Lin O
the O
formulations O
. O
Since O
this O
work O
focuses O
on O
sampling O
from O
a O
given O
model O
instead O
of O
training O
it O
, O
in O
the O
rest O
of O
the O
paper O
, O
we O
abbreviatePŒ∏(¬∑)asP(¬∑)for O
brevity O
. O
2.2 O
Existing O
Sampling O
Algorithms O
Given O
a O
trained O
LM O
and O
a O
context O
C O
, O
an O
ancestral O
sampling O
algorithm O
seeks O
to O
generate O
a O
sequence O
fromP(W|C)by O
sampling O
token O
- O
by O
- O
token O
from O
a O
transformed O
version O
of O
P(Wl+1|W1 O
.. O
l O
, O
C O
) O
. O
We O
now O
review O
and O
formulate O
three O
popular O
sampling O
algorithms O
: O
top O
- O
k(Fan O
et O
al O
. O
, O
2018 O
) O
, O
nucleus O
( O
Holtzman O
et O
al O
. O
, O
2020 O
) O
, O
and O
tempered O
( O
Ackley O
et O
al O
. O
, O
1985 O
; O
Caccia O
et O
al O
. O
, O
2020 O
) O
sampling O
. O
We O
view O
these O
algorithms O
as O
different O
transformations O
applied O
to O
the O
distribution O
P(Wl+1|W1 O
.. O
l O
, O
C O
) O
. O
First O
, O
we O
treat O
the O
conditional O
distribution O
P(Wl+1|W1 O
.. O
l O
, O
C)as O
a O
sorted O
vector O
pof O
length|V| O
. O
By O
sorting O
, O
we O
rearrange O
the O
elements O
such O
that O
if O
i O
< O
j‚Üípi>=pj.2We O
list O
the O
transformations O
and O
their O
intuition O
below O
: O
DeÔ¨Ånition O
2.1 O
. O
( O
Top O
- O
k O
) O
In O
top O
- O
ksampling O
, O
we O
only O
sample O
from O
the O
top O
Ktokens O
: O
ÀÜpi O
= O
pi O
¬∑ O
1{i‚â§K}/summationtextK O
j=1pj O
, O
( O
2 O
) O
where O
1is O
the O
indicator O
function O
, O
and O
K(1‚â§K‚â§ O
|V| O
) O
is O
the O
hyperparameter O
. O
DeÔ¨Ånition O
2.2 O
. O
( O
Nucleus O
) O
With O
a O
hyperparameter O
P(0 O
< O
P‚â§1 O
) O
, O
in O
nucleus O
sampling O
, O
we O
sample O
from O
the O
top- O
Pmass O
of O
p O
: O
ÀÜpi O
= O
p O
/ O
prime O
i O
/ O
summationtext|V| O
j=1p O
/ O
prime O
j O
, O
( O
3 O
) O
wherep O
/ O
prime O
i O
= O
pi O
¬∑ O
1{/summationtexti‚àí1 O
j=1pj O
< O
P O
} O
. O
2The O
token O
indexes O
are O
also O
permutated O
accordingly.335DeÔ¨Ånition O
2.3 O
. O
( O
Tempered O
) O
In O
tempered O
sampling O
, O
the O
log O
probabilities O
are O
scaled O
by1 O
T O
: O
ÀÜpi O
= O
exp(log(pi)/T O
) O
/summationtext|V| O
j=1exp(log(pj)/T O
) O
. O
( O
4 O
) O
In O
this O
work O
, O
we O
assume O
0 O
< O
T O
< O
1 O
, O
i.e. O
, O
the O
distribution O
is O
only O
made O
sharper3 O
. O
We O
additionally O
experiment O
with O
a O
combined O
version O
of O
top- O
kand O
tempered O
sampling O
: O
DeÔ¨Ånition O
2.4 O
. O
( O
Tempered O
Top- O
k O
) O
We O
combine O
the O
transformation O
deÔ¨Åned O
by O
top- O
kand O
tempered O
sampling O
: O
ÀÜpi O
= O
p O
/ O
prime O
i O
/ O
summationtext|V| O
j=1p O
/ O
prime O
j O
, O
( O
5 O
) O
wherep O
/ O
prime O
i= O
exp(log(pi)/T O
) O
¬∑ O
1{i‚â§K O
} O
. O
We O
set O
1‚â§K‚â§|V|and0 O
< O
T O
< O
1 O
. O
Throughout O
this O
work O
we O
use O
ÀÜpto O
denote O
the O
normalized O
version O
of O
the O
transformed O
distribution O
. O
All O
algorithms O
have O
hyperparameters O
to O
control O
the O
entropy O
of O
the O
transformed O
distribution O
. O
For O
example O
, O
Kin O
top O
- O
ksampling O
controls O
the O
size O
of O
the O
support O
of O
the O
resulting O
distribution O
. O
We O
will O
formalize O
this O
statement O
in O
Property O
1 O
below O
. O
3 O
Properties O
of O
Sampling O
Algorithms O
As O
we O
will O
show O
in O
Section O
5.1 O
( O
also O
Figure O
1 O
) O
, O
top O
- O
k O
, O
nucleus O
and O
tempered O
sampling O
perform O
on O
par O
with O
each O
other O
under O
our O
evaluation O
. O
This O
key O
observation O
makes O
us O
question O
: O
What O
are O
the O
core O
principles O
underlying O
the O
different O
algorithms O
that O
lead O
to O
their O
similar O
performance O
? O
To O
answer O
this O
question O
, O
in O
this O
section O
, O
we O
identify O
three O
core O
properties O
that O
are O
provably O
shared O
by O
the O
existing O
sampling O
algorithms O
. O
We O
then O
design O
experiments O
to O
validate O
their O
importance O
. O
3.1 O
Identifying O
Core O
Properties O
By O
inspecting O
the O
transformations O
listed O
in O
DeÔ¨Ånition O
2.1 O
, O
2.2 O
and O
2.3 O
, O
we O
extract O
the O
following O
three O
properties O
: O
Property O
1 O
. O
( O
Entropy O
Reduction O
) O
: O
The O
transformation O
strictly O
decrease O
the O
entropy O
of O
the O
distribution O
. O
Formally O
, O
H(ÀÜp)<H(p O
) O
, O
whereH(p O
) O
= O
‚àí/summationtext|V| O
i=1pilogpi O
. O
3One O
could O
also O
use O
T O
> O
1 O
, O
but O
it O
does O
not O
work O
well O
in O
practice O
. O
Property O
2 O
. O
( O
Order O
Preservation O
) O
: O
The O
order O
of O
the O
elements O
in O
the O
distribution O
is O
preserved O
. O
Formally O
, O
pi‚â•pj‚ÜíÀÜpi‚â•ÀÜpj O
. O
Property O
3 O
. O
( O
Slope O
Preservation O
) O
: O
The O
‚Äú O
slope O
‚Äù O
of O
the O
distribution O
is O
preserved O
. O
Formally O
, O
‚àÄÀÜpi O
> O
ÀÜpj>ÀÜpk>0(i.e O
. O
, O
they O
are O
not O
truncated O
) O
, O
we O
have O
logpi‚àílogpj O
logpj‚àílogpk O
= O
log O
ÀÜpi‚àílog O
ÀÜpj O
log O
ÀÜpj‚àílog O
ÀÜpk O
. O
The O
order O
preservation O
property O
implies O
that O
truncation O
can O
only O
happen O
in O
the O
tail O
of O
the O
distribution O
, O
which O
aligns O
with O
top- O
kand O
nucleus O
sampling O
. O
The O
slope O
preservation O
property O
is O
stronger O
than O
the O
order O
preservation O
property O
in O
that O
not O
only O
the O
ordering O
, O
but O
also O
the O
relative O
magnitude O
of O
the O
elements O
in O
the O
distribution O
needs O
to O
be O
somewhat O
preserved O
by O
the O
transformation O
. O
All O
these O
three O
properties O
are O
shared O
by O
the O
three O
existing O
sampling O
algorithms O
: O
Proposition O
1 O
. O
Property O
1 O
, O
2 O
and O
3 O
hold O
for O
the O
topk O
, O
nucleus O
and O
tempered O
sampling O
transformations O
formulated O
in O
DeÔ¨Ånitions O
2.1 O
, O
2.2 O
and O
2.3 O
. O
Proof O
. O
See O
Appendix O
B. O
We O
then O
set O
out O
to O
validate O
the O
importance O
of O
these O
identiÔ¨Åed O
properties O
in O
the O
aspects O
of O
necessityandsufÔ¨Åciency O
. O
To O
do O
so O
, O
we O
design O
two O
sets O
of O
new O
sampling O
algorithms O
in O
which O
each O
algorithm O
either O
violates O
one O
of O
the O
identiÔ¨Åed O
properties O
, O
or O
satisÔ¨Åes O
all O
properties O
. O
We O
list O
them O
in O
the O
next O
section O
. O
3.2 O
Designed O
Sampling O
Algorithms O
Property O
- O
violating O
algorithms O
To O
validate O
the O
necessity O
of O
each O
property O
, O
we O
design O
several O
sampling O
algorithms O
which O
violate O
at O
least O
one O
of O
the O
identiÔ¨Åed O
properties O
. O
In O
our O
experiments O
, O
we O
check O
whether O
that O
violation O
leads O
to O
a O
signiÔ¨Åcant O
degradation O
in O
performance O
. O
We O
list O
them O
below O
: O
DeÔ¨Ånition O
3.1 O
. O
( O
Target O
Entropy O
) O
Based O
on O
tempered O
sampling O
, O
target O
entropy O
sampling O
tunes O
the O
temperature O
tsuch O
that O
the O
transformed O
distribution O
has O
entropy O
value O
equal O
to O
the O
hyperparameter O
E O
( O
0 O
< O
E‚â§log|V| O
) O
. O
We O
formulate O
it O
below O
: O
ÀÜpi O
= O
exp(log(pi)/t O
) O
/summationtext|V| O
j=1exp(log(pj)/t O
) O
, O
( O
6 O
) O
wheretis O
selected O
such O
that O
H(ÀÜp O
) O
= O
E. O
Target O
entropy O
sampling O
violates O
entropy O
reduction O
, O
because O
when O
H(p)<E O
, O
the O
entropy O
will O
be O
tuned O
up O
( O
i.e. O
, O
H(ÀÜp)>H(p)).336DeÔ¨Ånition O
3.2 O
. O
( O
Random O
Mask O
) O
In O
random O
mask O
sampling O
, O
we O
randomly O
mask O
out O
tokens O
in O
the O
distribution O
with O
rate O
R. O
We O
formluate O
it O
below O
: O
ÀÜpi O
= O
p O
/ O
prime O
i O
/ O
summationtext|V| O
j=1p O
/ O
prime O
j O
, O
( O
7 O
) O
wherep O
/ O
prime O
i O
= O
pi O
¬∑ O
1{i= O
1 O
orui O
> O
R}and O
ui‚àºU(0,1 O
) O
. O
The O
hyperparameter O
R(0 O
< O
R‚â§1 O
) O
controls O
the O
size O
of O
the O
support O
of O
the O
resulting O
distribution O
. O
In O
Appendix O
A O
, O
we O
show O
it O
is O
crucial O
that O
the O
token O
which O
is O
assigned O
the O
largest O
probability O
( O
p1 O
) O
is O
never O
be O
masked O
. O
Random O
mask O
sampling O
is O
different O
from O
top- O
k O
or O
nucleus O
sampling O
in O
that O
the O
masking O
not O
only O
happens O
in O
the O
tail O
of O
the O
distribution O
. O
Therefore O
, O
it O
violates O
the O
order O
preservation O
property O
. O
DeÔ¨Ånition O
3.3 O
. O
( O
Noised O
Top- O
k O
) O
We O
add O
a O
sorted O
noise O
distribution O
to O
the O
result O
from O
top- O
Ktransformation O
, O
and O
the O
weight O
of O
the O
noise O
distribution O
is O
controlled O
by O
a O
hyperparameter O
W(0‚â§W‚â§1 O
) O
. O
We O
formulate O
it O
below O
: O
ÀÜp= O
( O
1‚àíW)ÀÜptop O
- O
K+Wpnoise O
- O
K O
, O
( O
8) O
where O
pnoise O
- O
Kis O
a O
uniformly O
sampled O
sorted O
Ksimplex O
, O
which O
satisÔ¨Åes O
/ O
summationtextK O
i=1pnoise O
- O
K O
i O
= O
1andi O
< O
j‚Üípnoise O
- O
K O
i‚â•pnoise O
- O
K O
j‚â•0 O
. O
The O
sorted O
nature O
of O
the O
noise O
distribution O
pnoise O
- O
Kmaintains O
order O
preservation O
. O
However O
, O
it O
violates O
slope O
preservation O
, O
and O
the O
noise O
weight O
Wcontrols O
the O
degree O
of O
the O
violation O
. O
Property O
- O
satisfying O
algorithms O
To O
validate O
the O
sufÔ¨Åciency O
of O
the O
identiÔ¨Åed O
properties O
, O
we O
design O
two O
new O
sampling O
algorithms O
for O
which O
all O
three O
properties O
hold O
. O
And O
in O
our O
experiments O
we O
check O
whether O
their O
performance O
is O
on O
par O
with O
the O
existing O
sampling O
algorithms O
. O
We O
list O
them O
below O
: O
DeÔ¨Ånition O
3.4 O
. O
( O
Random O
Top- O
k O
) O
We O
design O
a O
randomized O
version O
of O
top- O
ksampling O
: O
At O
each O
time O
step O
, O
we O
sample O
a O
uniformly O
random O
Ô¨Çoat O
number O
u‚àºU(0,1 O
) O
, O
and O
use O
it O
to O
specify O
a O
top- O
ktruncation O
: O
ÀÜpi O
= O
pi O
¬∑ O
1{i‚â§k}/summationtextk O
j=1pj O
, O
( O
9 O
) O
wherek=‚åä1 O
+ O
M¬∑u‚åã. O
The O
hyperparameter O
M O
( O
1‚â§M O
< O
|V| O
) O
controls O
the O
maximum O
truncation O
threshold O
. O
DeÔ¨Ånition O
3.5 O
. O
( O
Max O
Entropy O
) O
Max O
entropy O
sampling O
is O
similar O
to O
target O
entropy O
sampling O
( O
Definition O
3.1 O
) O
. O
However O
to O
match O
entropy O
reduction O
( O
Property O
1 O
) O
, O
we O
only O
tune O
the O
temperature O
whenH(p)>E O
, O
whereEis O
the O
hyperparameter O
( O
0 O
< O
E‚â§log|V| O
): O
ÀÜpi=Ô£± O
Ô£≤ O
Ô£≥exp(log(pi)/t)/summationtext|V| O
j=1exp(log(pj)/t),ifH(p)>E O
pi O
, O
otherwise O
, O
( O
10 O
) O
wheretis O
selected O
so O
thatH(ÀÜp O
) O
= O
E. O
It O
is O
easy O
to O
prove O
that O
Property O
1 O
, O
2 O
, O
and O
3 O
holds O
for O
the O
transformations O
deÔ¨Åned O
by O
random O
top- O
k O
and O
max O
entropy O
sampling O
, O
and O
we O
omit O
the O
proof O
for O
brevity O
. O
4 O
Experiment O
Setup O
In O
this O
section O
, O
we O
Ô¨Årst O
establish O
evaluation O
protocols O
, O
and O
then O
describe O
the O
model O
and O
data O
we O
use O
for O
the O
open O
- O
ended O
language O
generation O
task O
. O
4.1 O
Evaluation O
via O
the O
Q O
- O
D O
Trade O
- O
off O
How O
to O
efÔ¨Åciently O
measure O
the O
generation O
performance O
of O
a O
NLG O
model O
has O
been O
an O
important O
open O
question O
. O
Most O
existing O
metrics O
either O
measure O
the O
quality O
aspect O
( O
e.g. O
BLEU O
score O
) O
or O
the O
diversity O
( O
e.g. O
n O
- O
gram O
entropy O
) O
aspect O
. O
To O
make O
the O
situation O
more O
complicated O
, O
each O
sampling O
algorithm O
has O
its O
own O
hyperparameters O
which O
controls O
the O
trade O
- O
off O
between O
quality O
and O
diversity O
. O
To O
address O
the O
challenges O
above O
, O
we O
adopt O
the O
quality O
- O
diversity O
trade O
- O
off O
proposed O
by O
Caccia O
et O
al O
. O
( O
2020 O
) O
. O
In O
the O
Q O
- O
D O
trade O
- O
off O
, O
we O
perform O
a O
Ô¨Ånegrained O
sweep O
of O
hyperparameters O
for O
each O
sampling O
algorithm O
, O
and O
compute O
the O
quality O
and O
diversity O
score O
for O
each O
conÔ¨Åguration O
. O
We O
report O
two O
pairs O
of O
Q O
/ O
D O
metrics O
, O
with O
one O
pair O
using O
automatic O
evaluation O
and O
the O
other O
using O
human O
evaluation O
. O
In O
the O
next O
two O
sections O
, O
we O
describe O
the O
metrics O
we O
use O
, O
and O
refer O
readers O
to O
Caccia O
et O
al O
. O
( O
2020 O
) O
for O
more O
intuition O
behind O
the O
Q O
- O
D O
trade O
- O
off O
. O
4.1.1 O
Automatic O
Evaluation O
For O
automatic O
metrics O
, O
we O
adopt O
the O
corpus O
- O
BLEU O
( O
Yu O
et O
al O
. O
, O
2016 O
) O
metric O
to O
measure O
quality O
and O
the O
self O
- O
BLEU O
( O
Zhu O
et O
al O
. O
, O
2018 O
) O
metric O
to O
measure O
diversity O
. O
We O
formulate O
them O
below O
. O
Given O
a O
batch O
of O
generated O
sentences O
Sgenand O
a O
batch O
of O
sentences O
from O
ground O
- O
truth O
data O
as O
referencesSref O
, O
corpus O
- O
BLEU O
returns O
the O
average337BLEU O
score O
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
of O
every O
model O
generated O
sentence O
against O
the O
reference O
set O
: O
corpus O
- O
BLEU O
( O
Sgen O
, O
Sref O
) O
= O
1 O
|Sgen|/summationdisplay O
W‚ààSgenBLEU O
( O
W O
, O
S O
ref O
) O
. O
( O
11 O
) O
A O
higher O
corpus O
- O
BLEU O
score O
means O
that O
the O
generated O
sequences O
has O
better O
quality O
in O
that O
it O
has O
higher O
ngram O
- O
level O
overlap O
with O
the O
reference O
data O
. O
Based O
on O
the O
same O
intuition O
, O
we O
deÔ¨Åne O
the O
selfBLEU O
metric O
to O
quantify O
the O
diversity O
aspect O
: O
self O
- O
BLEU O
( O
Sgen O
) O
= O
corpus O
- O
BLEU O
( O
Sgen O
, O
Sgen),(12 O
) O
where O
a O
lower O
self O
- O
BLEU O
score O
means O
that O
the O
samples O
have O
better O
diversity O
. O
In O
our O
experiments O
, O
we O
feed O
the O
Ô¨Årst O
ten O
subwords O
of O
every O
sample O
from O
test O
set O
to O
the O
model O
, O
and O
compare O
the O
model O
- O
generated O
sequences O
to O
the O
reference O
samples O
in O
the O
validation O
set O
. O
We O
use O
10,000 O
samples O
to O
compute O
corpus O
- O
BLEU O
or O
selfBLEU O
, O
i.e. O
,|Sgen|=|Sref|= O
10,000 O
. O
Automatic O
evaluation O
enables O
us O
to O
do O
a O
Ô¨Ånegrained O
sweep O
of O
the O
hyperparameters O
for O
each O
sampling O
algorithm O
, O
and O
compare O
them O
in O
the O
qualitydiversity O
trade O
- O
off O
. O
However O
, O
observations O
from O
automatic O
evaluation O
could O
be O
misaligned O
with O
human O
evaluation O
( O
Belz O
and O
Reiter O
, O
2006 O
) O
. O
Therefore O
, O
we O
conÔ¨Årm O
our O
key O
observations O
with O
human O
evaluation O
. O
4.1.2 O
Human O
Evaluation O
Quality O
We O
ask O
a O
pool O
of O
602 O
crowdworkers O
on O
Amazon O
Mechanical O
Turk O
to O
evaluate O
various O
sampling O
conÔ¨Ågurations O
in O
the O
quality O
aspect O
. O
Each O
worker O
is O
presented O
a O
set O
of O
ten O
samples O
along O
with O
the O
prompts O
( O
preÔ¨Åxes O
) O
. O
They O
are O
then O
asked O
to O
rate O
how O
likely O
the O
sentence O
would O
appear O
in O
a O
news O
article O
between O
0 O
and O
5 O
( O
Invalid O
, O
Confusing O
, O
UnspeciÔ¨Åc O
, O
Average O
, O
Expected O
, O
and O
Very O
Expected O
respectively O
) O
. O
We O
focus O
on O
the O
Gigaword O
dataset O
for O
human O
evaluation O
since O
news O
articles O
are O
ubiquitous O
and O
do O
not O
often O
require O
expert O
knowledge O
for O
quality O
judgement O
. O
For O
each O
conÔ¨Åguration O
( O
sampling O
algorithm O
and O
hyperparameter O
pair O
) O
we O
ask O
crowdworkers O
to O
rate O
200 O
samples O
in O
total O
. O
To O
get O
an O
accurate O
rating O
for O
each O
sample O
, O
we O
enlist O
25 O
different O
crowdworkers O
to O
rate O
each O
sample O
. O
We O
report O
mean O
and O
standard O
deviation O
from O
5 O
independent O
runs O
( O
each O
with O
40 O
samples O
) O
as O
error O
bar O
. O
By O
manual O
inspection O
, O
we O
Ô¨Ånd O
that O
the O
time O
spent O
in O
the O
annotations O
is O
a O
good O
indicator O
of O
the O
qualityof O
the O
rating O
. O
Therefore O
, O
we O
estimate O
the O
human O
judgement O
score O
for O
a O
sample O
as O
the O
average O
rating O
of O
the O
20 O
crowdworkers O
( O
out O
of O
25 O
) O
who O
took O
the O
most O
time O
to O
rate O
the O
samples O
. O
We O
provide O
further O
details O
about O
our O
setup O
in O
Appendix O
C O
and O
D. O
Diversity O
It O
is O
difÔ¨Åcult O
for O
human O
annotators O
to O
estimate O
diversity O
of O
text O
( O
Hashimoto O
et O
al O
. O
, O
2019 O
) O
. O
Therefore O
, O
we O
use O
the O
n O
- O
gram O
entropy O
metric O
( O
Zhang O
et O
al O
. O
, O
2018 O
; O
He O
and O
Glass O
, O
2019 O
) O
. O
Given O
Sgenwhich O
contains O
a O
large O
number O
of O
samples O
, O
we O
measure O
its O
diversity O
using O
the O
following O
formulation O
: O
Hn O
- O
gram(Sgen O
) O
= O
/summationdisplay O
g‚ààGn‚àír(g O
) O
logr(g),(13 O
) O
whereGnis O
the O
set O
of O
all O
n O
- O
grams O
that O
appeared O
inSgen O
, O
andr(g)refers O
to O
the O
ratio O
( O
frequency O
) O
of O
n O
- O
gramgw.r.t O
. O
all O
n O
- O
grams O
in O
the O
Sgen O
. O
For O
the O
estimation O
of O
n O
- O
gram O
entropy O
, O
we O
generate O
50,000 O
samples O
from O
each O
sampling O
conÔ¨Åguration O
. O
We O
will O
report O
human O
quality O
score O
either O
paired O
with O
n O
- O
gram O
entropy O
or O
with O
self O
- O
BLEU O
as O
diversity O
metric O
. O
We O
Ô¨Ånd O
they O
give O
similar O
observations O
. O
4.2 O
Model O
and O
Datasets O
We O
separately O
Ô¨Åne O
- O
tune O
GPT2 O
- O
small O
( O
Radford O
et O
al O
. O
, O
2018 O
; O
Wolf O
et O
al O
. O
, O
2019 O
) O
( O
110 O
M O
parameters O
) O
on O
the O
Gigaword O
( O
Graff O
et O
al O
. O
, O
2003 O
; O
Napoles O
et O
al O
. O
, O
2012 O
) O
and O
the O
Wikitext-103 O
( O
Merity O
et O
al O
. O
, O
2017 O
) O
datasets O
. O
We O
use O
the O
same O
tokenization O
as O
GPT-2 O
, O
and O
add O
additional O
padding O
and O
end O
- O
of O
- O
sequence O
tokens O
( O
[ O
EOS O
] O
) O
to O
the O
sentences O
. O
To O
generate O
a O
sequence O
, O
we O
feed O
a O
length-10 O
preÔ¨Åx O
from O
test O
data O
into O
the O
Ô¨Åne O
- O
tuned O
GPT-2 O
model O
, O
and O
use O
a O
sampling O
algorithm O
to O
complete O
the O
sentence O
. O
Since O
shorter O
samples O
are O
more O
difÔ¨Åcult O
to O
judge O
in O
quality O
( O
Ippolito O
et O
al O
. O
, O
2020 O
) O
, O
we O
Ô¨Ålter O
all O
generated O
sentence O
completions O
to O
be O
between O
40 O
and O
50 O
subwords O
, O
and O
Ô¨Ålter O
our O
validation O
and O
test O
set O
to O
meet O
the O
same O
requirements O
. O
To O
permit O
validation O
and O
test O
sets O
that O
are O
large O
enough O
to O
preÔ¨Åx O
10,000 O
sentences O
for O
the O
corpus O
- O
BLEU O
metric O
, O
we O
re O
- O
chunk O
the O
Ô¨Årst O
80 O
% O
of O
the O
Gigaword O
dataset O
for O
the O
training O
set O
, O
15 O
% O
for O
validation O
, O
and O
the O
last O
5 O
% O
for O
the O
test O
set O
. O
Similarly O
, O
we O
re O
- O
chunk O
the O
Ô¨Årst O
97 O
% O
of O
the O
Wikitext-103 O
dataset O
for O
training O
, O
and O
leave O
1.5 O
% O
for O
validation O
and O
1.5 O
% O
for O
test O
. O
5 O
Empirical O
Results O
First O
, O
we O
compare O
existing O
sampling O
algorithms O
, O
and O
then O
move O
on O
to O
validate O
the O
necessity O
and338Figure O
2 O
: O
The O
performance O
( O
x O
- O
axis O
: O
quality O
, O
y O
- O
axis O
: O
diversity O
, O
both O
are O
the O
smaller O
the O
better O
) O
of O
top- O
k O
, O
nucleus O
, O
tempered O
and O
tempered O
top- O
ksampling O
are O
on O
par O
on O
the O
Gigaword O
dataset O
, O
as O
shown O
by O
automatic O
evaluation O
. O
sufÔ¨Åciency O
of O
the O
identiÔ¨Åed O
properties O
. O
5.1 O
Comparison O
of O
Existing O
Algorithms O
We O
compare O
top- O
k O
, O
nucleus O
, O
and O
tempered O
sampling O
via O
automatic O
and O
human O
evaluation O
. O
We O
do O
a O
Ô¨Åne O
- O
grained O
sweep O
of O
hyperparameters O
for O
each O
sampling O
algorithm O
on O
the O
Gigaword O
dataset O
. O
The O
results O
are O
shown O
in O
Figure O
1 O
( O
human O
evaluation O
) O
and O
Figure O
2 O
( O
automatic O
evaluation O
) O
. O
We O
also O
show O
the O
quality O
and O
diversity O
score O
for O
human O
text O
in O
the O
test O
data O
for O
reference O
, O
which O
is O
labeled O
as O
gold O
. O
Both O
automatic O
and O
human O
evaluations O
demonstrate O
that O
the O
performance O
of O
top- O
k O
, O
nucleus O
and O
tempered O
sampling O
are O
on O
par O
with O
each O
other O
, O
with O
no O
signiÔ¨Åcant O
gap O
. O
When O
the O
hyperparameters O
( O
K O
, O
PandT O
) O
are O
tuned O
so O
that O
different O
sampling O
has O
the O
same O
diversity O
( O
measured O
by O
self O
- O
BLEU O
or O
ngram O
entropy O
) O
, O
their O
quality O
( O
measured O
by O
corpusBLEU O
or O
human O
rating O
) O
are O
close O
. O
Additionally O
, O
we O
compare O
tempered O
top- O
ksampling O
with O
the O
existing O
algorithm O
also O
in O
Figure O
2 O
. O
We O
Ô¨Ånd O
that O
adding O
the O
tempered O
transformation O
only O
moves O
top- O
ksampling O
along O
the O
Q O
- O
D O
tradeoff O
, O
instead O
of O
yielding O
a O
better O
or O
a O
worse O
sampling O
algorithm O
. O
For O
example O
, O
the O
performance O
of O
the O
K= O
500,T= O
0.8conÔ¨Åguration O
for O
tempered O
top O
- O
ksampling O
is O
very O
close O
to O
the O
K= O
30 O
conÔ¨Åguration O
for O
the O
top- O
ksampling O
. O
Motivated O
by O
these O
observations O
, O
we O
identify O
three O
core O
properties O
( O
elaborated O
in O
Section O
3.1 O
) O
that O
are O
shared O
among O
the O
sampling O
algorithms O
: O
entropy O
reduction O
, O
order O
preservation O
andslope O
preservation O
. O
In O
the O
following O
two O
sections O
, O
we O
Figure O
3 O
: O
Automatic O
evaluation O
of O
the O
noised O
top- O
k O
, O
target O
entropy O
, O
and O
random O
mask O
sampling O
proposed O
to O
validate O
the O
necessity O
of O
the O
identiÔ¨Åed O
properties O
. O
The O
results O
show O
that O
violation O
of O
entropy O
reduction O
and O
slope O
preservation O
could O
lead O
to O
drastic O
performance O
degradation O
, O
while O
the O
order O
preservation O
property O
could O
be O
further O
relaxed O
. O
present O
experiments O
validating O
the O
necessity O
or O
sufÔ¨Åciency O
aspect O
of O
the O
properties O
. O
5.2 O
Property O
- O
violating O
Algorithms O
In O
Figure O
3 O
, O
we O
compare O
the O
generation O
performance O
of O
the O
property O
- O
violating O
sampling O
algorithms O
( O
designed O
in O
Section O
3.2 O
) O
, O
against O
the O
existing O
algorithms O
using O
automatic O
evaluation O
on O
the O
Gigaword O
dataset O
. O
We O
make O
the O
following O
observations O
: O
First O
, O
the O
target O
entropy O
sampling O
, O
which O
violates O
entropy O
reduction O
, O
has O
signiÔ¨Åcantly O
worse O
performance O
; O
Second O
, O
even O
with O
small O
noise O
weight O
W O
, O
the O
performance O
of O
noised O
top- O
ksampling O
degrades O
from O
the O
original O
top- O
ksampling O
, O
and O
the O
gap O
becomes O
larger O
as O
Wincreases O
; O
Last O
, O
the O
random O
mask O
sampling O
is O
on O
par O
with O
the O
existing O
sampling O
algorithms O
in O
performance O
. O
We O
further O
conÔ¨Årm O
this O
observation O
with O
human O
evaluation O
in O
Figure O
5 O
. O
These O
results O
suggest O
that O
the O
violation O
of O
entropy O
reduction O
or O
slope O
preservation O
could O
lead O
to O
drastic O
performance O
degradation O
. O
On O
the O
other O
hand O
, O
the O
competitive O
performance O
of O
random O
mask O
sampling O
suggests O
that O
order O
preservation O
could O
be O
further O
relaxed O
. O
In O
the O
next O
section O
, O
we O
investigate O
the O
sufÔ¨Åciency O
aspect O
of O
the O
identiÔ¨Åed O
properties O
. O
5.3 O
Property O
- O
satisfying O
Algorithms O
We O
now O
compare O
the O
generation O
performance O
of O
the O
property O
- O
satisfying O
sampling O
algorithms O
( O
designed O
in O
Section O
3.2 O
) O
with O
the O
existing O
sampling O
algorithms O
. O
The O
results O
from O
the O
Gigaword O
dataset339Sampling O
Conditional O
Samples O
Existing O
Sampling O
Algorithms O
Top O
- O
k O
( O
K O
= O
30)steven O
spielberg O
‚Äôs O
dreamworks O
movie O
studio O
said O
monday O
it O
was O
Ô¨Åling O
a O
lawsuit O
, O
accusing O
us O
studio O
executives O
of O
defrauding O
hundreds O
of O
thousands O
of O
dollars O
in O
refunds O
and O
other O
damages O
. O
Nucleus O
( O
P O
= O
0.80)steven O
spielberg O
‚Äôs O
dreamworks O
movie O
studio O
has O
failed O
to O
attract O
the O
kind O
of O
business O
and O
development O
investors O
that O
jeffrey O
hutchinson O
dreamed O
up O
in O
the O
past O
. O
Tempered O
( O
T O
= O
0.85)steven O
spielberg O
‚Äôs O
dreamworks O
movie O
studio O
plans O
to O
spend O
the O
rest O
of O
the O
year O
producing O
the O
high O
- O
speed O
thriller O
‚Äù O
the O
earth O
‚Äôs O
path O
‚Äù O
and O
an O
upcoming O
sequel O
, O
the O
studio O
announced O
on O
wednesday O
. O
Property O
- O
satisfying O
Sampling O
Algorithms O
Random O
Top- O
k O
( O
R O
= O
90)steven O
spielberg O
‚Äôs O
dreamworks O
movie O
studio O
is O
planning O
to O
make O
a O
movie O
about O
a O
young O
man O
who O
is O
a O
< O
unk O
> O
, O
a O
man O
who O
has O
a O
dream O
of O
being O
the O
Ô¨Årst O
man O
to O
be O
born O
with O
the O
ability O
to O
walk O
on O
water O
. O
Max O
Entropy O
( O
E O
= O
2.75)steven O
spielberg O
‚Äôs O
dreamworks O
movie O
studio O
has O
agreed O
to O
pay O
$ O
# O
. O
# O
million O
to O
director O
john O
nichols O
( O
¬£ O
# O
. O
# O
million O
, O
# O
# O
# O
, O
a O
record O
in O
the O
studio O
circulation O
) O
, O
the O
studio O
announced O
sunday O
.. O
Property O
- O
violating O
Sampling O
Algorithms O
Random O
Mask O
( O
R O
= O
0.75)steven O
spielberg O
‚Äôs O
dreamworks O
movie O
studio O
scored O
a O
big O
win O
with O
a O
$ O
# O
# O
. O
# O
million O
( O
euro O
# O
# O
. O
# O
million O
) O
direct O
- O
to O
- O
video O
( O
dvds O
) O
deal O
to O
develop O
the O
# O
# O
# O
# O
short O
story O
‚Äù O
the O
rose O
garden O
‚Äù O
. O
Noised O
Top O
- O
k O
( O
K=50 O
, O
W=5e-3)steven O
spielberg O
‚Äôs O
dreamworks O
movie O
studio O
is O
in O
disarray O
and O
has O
a O
few O
directors O
and O
a O
lot O
of O
stock O
involved O
, O
leaving O
it O
only O
a O
matter O
of O
time O
before O
spielberg O
‚Äôs O
departure O
from O
the O
nobel O
peace O
prize O
. O
Target O
Entropy O
( O
E O
= O
2.75)steven O
spielberg O
‚Äôs O
dreamworks O
movie O
studio O
production O
scored O
an O
action O
boost O
m O
boom O
, O
nabbing O
an O
‚Äôd O
after O
the O
# O
# O
th O
instal O
specialization O
with O
nominations O
of O
fritz O
, O
ika O
, O
ivan O
english O
ape O
and O
evlyn O
mcready O
. O
Table O
1 O
: O
Generated O
sequences O
with O
the O
same O
preÔ¨Åx O
steven O
spielberg O
‚Äôs O
dreamworks O
movie O
studio O
by O
different O
sampling O
algorithms O
. O
The O
hyperparameters O
are O
chosen O
such O
that O
the O
algorithms O
yield O
roughly O
the O
same O
diversity O
measured O
by O
self O
- O
BLEU O
. O
The O
poor O
- O
quality O
spans O
are O
higlighted O
in O
red O
. O
Figure O
4 O
: O
The O
proposed O
random O
top- O
kand O
max O
entropy O
schedulers O
, O
which O
meet O
the O
identiÔ¨Åed O
properties O
, O
are O
on O
par O
in O
performance O
with O
existing O
methods O
in O
automatic O
evaluation O
on O
the O
Gigaword O
dataset O
. O
are O
shown O
in O
Figure O
3 O
( O
for O
automatic O
evaluation O
) O
and O
Figure O
5 O
( O
for O
human O
evaluation O
) O
. O
For O
completeness O
, O
we O
also O
replicate O
Figure O
5 O
with O
self O
- O
BLEU O
as O
the O
diversity O
measure O
in O
Appendix O
F. O
We O
also O
present O
results O
from O
automatic O
evaluation O
on O
the O
Wikitext-103 O
dataset O
in O
Figure O
6 O
, O
with O
consistent O
observations O
. O
The O
evaluations O
consistently O
show O
that O
the O
performance O
of O
random O
top- O
kand O
max O
entropy O
sampling O
( O
and O
random O
mask O
sampling O
in O
last O
section O
) O
is O
on O
par O
with O
top- O
k O
, O
nucleus O
, O
and O
tempered O
sampling O
. O
These O
results O
strengthen O
the O
importance O
of O
the O
idenFigure O
5 O
: O
Human O
evaluation O
also O
shows O
that O
the O
proposed O
sampling O
algorithms O
has O
performance O
on O
par O
with O
the O
existing O
methods O
on O
the O
Gigaword O
dataset O
. O
Appendix O
F O
repeats O
this O
plot O
with O
self O
- O
BLEU O
. O
tiÔ¨Åed O
properties O
in O
that O
, O
new O
sampling O
algorithms O
could O
get O
competitive O
generation O
performance O
as O
long O
as O
they O
meet O
the O
identiÔ¨Åed O
properties O
. O
5.4 O
Qualitative O
Analysis O
We O
list O
samples O
from O
the O
proposed O
sampling O
algorithms O
and O
compare O
them O
with O
the O
existing O
ones O
in O
Table O
1 O
. O
We O
choose O
the O
hyperparameter O
of O
each O
sampling O
algorithm O
so O
that O
each O
algorithm O
exhibits O
a O
similar O
level O
of O
diversity O
( O
as O
measured O
by O
selfBLEU O
) O
. O
By O
manual O
inspection O
, O
we O
Ô¨Ånd O
that O
the O
quality O
of O
samples O
from O
property O
- O
satisfying O
sam-340Figure O
6 O
: O
Automatic O
evaluation O
on O
the O
Wikitext-103 O
dataset O
: O
The O
performance O
of O
proposed O
sampling O
algorithms O
are O
on O
par O
with O
top- O
k O
, O
nucleus O
, O
and O
tempered O
sampling O
. O
pling O
algorithms O
is O
on O
par O
with O
samples O
from O
the O
existing O
algorithms O
. O
In O
particular O
, O
the O
samples O
from O
random O
top- O
k O
, O
max O
entropy O
, O
and O
random O
masked O
sampling O
are O
all O
coherent O
and O
informative O
. O
In O
contrast O
, O
the O
samples O
from O
noised O
top- O
kand O
target O
entropy O
algorithms O
, O
tend O
to O
be O
less O
semantically O
and O
syntatically O
coherent O
. O
In O
particular O
, O
the O
target O
entropy O
sampling O
algorithm O
, O
which O
obtains O
the O
lowest O
quality O
score O
measured O
by O
corpusBLEU O
, O
lacks O
basic O
language O
structure O
. O
In O
comparison O
to O
target O
entropy O
, O
noised O
top- O
kis O
syntatically O
coherent O
, O
but O
exhibits O
logical O
and O
factual O
inconsistencies O
. O
These O
observations O
aligns O
with O
the O
results O
we O
get O
from O
automatic O
evaluation O
. O
6 O
Related O
Works O
Despite O
the O
popularity O
of O
sampling O
algorithms O
in O
natural O
language O
generation O
, O
a O
rigorous O
comparison O
or O
scrutiny O
of O
existing O
algorithms O
is O
lacking O
in O
the O
literature O
. O
Ippolito O
et O
al O
. O
( O
2019 O
) O
provides O
a O
comparison O
between O
sampling O
and O
decoding O
algorithms O
. O
Holtzman O
et O
al O
. O
( O
2020 O
) O
proposes O
nucleus O
sampling O
, O
and O
compare O
it O
with O
top- O
ksampling O
( O
Fan O
et O
al O
. O
, O
2018 O
) O
. O
However O
, O
only O
a O
few O
hyperparameter O
conÔ¨Ågurations O
are O
tested O
. O
In O
Hashimoto O
et O
al O
. O
( O
2019 O
) O
and O
Caccia O
et O
al O
. O
( O
2020 O
) O
, O
temperature O
sampling O
is O
used O
and O
the O
hyperparameter O
Tis O
tuned O
to O
trade O
- O
off O
between O
diversity O
and O
quality O
, O
but O
it O
lacks O
comparisons O
with O
other O
sampling O
algorithms O
. O
Welleck O
et O
al O
. O
( O
2020 O
) O
studies O
the O
consistency O
of O
existing O
sampling O
and O
decoding O
algorithms O
, O
without O
comparing O
the O
generation O
performance O
. O
In O
this O
work O
we O
mainly O
use O
the O
quality O
- O
diversity O
trade O
- O
off O
( O
Caccia O
et O
al O
. O
, O
2020 O
) O
to O
conduct O
a O
compar O
- O
ison O
of O
different O
sampling O
algorithms O
. O
Parallel O
to O
our O
work O
, O
Zhang O
et O
al O
. O
( O
2020a O
) O
also O
uses O
the O
qualitydiversity O
trade O
- O
off O
to O
compare O
top- O
k O
, O
nucleus O
, O
and O
tempered O
sampling O
. O
Their O
observation O
is O
similar O
to O
ours O
: O
The O
performance O
of O
the O
existing O
algorithms O
are O
close O
with O
no O
signiÔ¨Åcant O
gap O
. O
More O
importantly O
, O
the O
underlying O
reasons O
for O
the O
success O
of O
various O
sampling O
algorithms O
remain O
poorly O
understood O
. O
Zhang O
et O
al O
. O
( O
2020a O
) O
proposes O
theselective O
sampling O
algorithm O
, O
which O
fails O
to O
outperform O
existing O
approaches O
. O
This O
failed O
attempt O
suggests O
the O
need O
for O
a O
better O
understanding O
of O
the O
strengths O
and O
weaknesses O
of O
existing O
methods O
. O
To O
the O
best O
of O
our O
knowledge O
, O
our O
work O
provides O
the O
Ô¨Årst O
systematic O
characterization O
of O
sampling O
algorithms O
, O
where O
we O
attribute O
the O
success O
of O
existing O
sampling O
algorithms O
to O
a O
shared O
set O
of O
properties O
. O
We O
show O
that O
we O
can O
propose O
novel O
sampling O
algorithms O
based O
on O
the O
identiÔ¨Åed O
properties O
, O
and O
reach O
competitive O
generation O
performance O
as O
measured O
by O
both O
automatic O
and O
human O
evaluation O
. O
7 O
Limitations O
and O
Future O
Work O
Our O
core O
contribution O
is O
the O
three O
properties O
of O
sampling O
algorithms O
that O
we O
conjecture O
are O
crucial O
for O
competitive O
generation O
performance O
. O
While O
we O
design O
a O
set O
of O
experiments O
to O
validate O
their O
necessity O
and O
sufÔ¨Åciency O
, O
the O
observations O
we O
make O
are O
still O
empirical O
. O
We O
emphasize O
that O
it O
is O
completely O
possible O
that O
there O
exists O
some O
crucial O
property O
, O
that O
is O
yet O
to O
be O
discovered O
, O
and O
can O
lead O
to O
signiÔ¨Åcantly O
better O
generation O
performance O
. O
Therefore O
, O
the O
exploration O
of O
novel O
sampling O
algorithms O
( O
Zhang O
et O
al O
. O
, O
2020a O
) O
should O
still O
be O
encouraged O
. O
On O
the O
other O
hand O
, O
to O
provide O
a O
comprehensive O
study O
, O
we O
focus O
on O
the O
open O
- O
ended O
language O
generation O
task O
with O
the O
GPT-2 O
model O
. O
As O
future O
work O
, O
it O
would O
be O
interesting O
to O
check O
whether O
our O
observations O
also O
hold O
on O
other O
tasks O
such O
story O
generation O
or O
dialogue O
response O
generation O
, O
or O
with O
weaker O
language O
models O
in O
low O
- O
resource O
setting O
. O
8 O
Conclusion O
This O
work O
studies O
sampling O
algorithms O
for O
the O
openended O
language O
generation O
task O
. O
We O
show O
that O
the O
existing O
algorithms O
, O
namely O
top- O
k O
, O
nucleus O
, O
and O
tempered O
sampling O
, O
have O
similar O
generation O
performance O
as O
measured O
by O
the O
quality O
- O
diversity O
tradeoff O
evaluation O
. O
Motivated O
by O
this O
result O
, O
we O
identify O
three O
key O
properties O
that O
we O
prove O
are O
shared O
by341the O
existing O
algorithms O
. O
To O
validate O
the O
importance O
of O
these O
identiÔ¨Åed O
properties O
, O
we O
design O
a O
set O
of O
new O
sampling O
algorithms O
, O
and O
compare O
their O
performance O
with O
the O
existing O
sampling O
algorithms O
. O
We O
Ô¨Ånd O
that O
violation O
of O
the O
identiÔ¨Åed O
properties O
may O
lead O
to O
drastic O
performance O
degradation O
. O
On O
the O
other O
hand O
, O
we O
propose O
several O
novel O
algorithms O
, O
namely O
random O
top- O
kand O
max O
entropy O
sampling O
, O
that O
meet O
the O
identiÔ¨Åed O
properties O
. O
We O
Ô¨Ånd O
that O
their O
generation O
performance O
is O
on O
par O
with O
the O
existing O
algorithms O
. O
Acknowledgments O
The O
authors O
sincerely O
thank O
Yixin O
Tao O
, O
Jingzhao O
Zhang O
and O
Yonatan O
Belinkov O
for O
useful O
discussions O
. O
This O
work O
was O
partly O
supported O
by O
Samsung O
Advanced O
Institute O
of O
Technology O
( O
Next O
Generation O
Deep O
Learning O
: O
from O
pattern O
recognition O
to O
AI O
) O
, O
Samsung O
Electronics O
( O
Improving O
Deep O
Learning O
using O
Latent O
Structure O
) O
. O
Kyunghyun O
Cho O
thanks O
CIFAR O
, O
Naver O
, O
eBay O
, O
NVIDIA O
and O
Google O
for O
their O
support O
. O
This O
research O
was O
sponsored O
in O
part O
by O
the O
United O
States O
Air O
Force O
Research O
Laboratory O
and O
was O
accomplished O
under O
Cooperative O
Agreement O
Number O
FA8750 O
- O
19 O
- O
2 O
- O
1000 O
. O
The O
views O
and O
conclusions O
contained O
in O
this O
document O
are O
those O
of O
the O
authors O
and O
should O
not O
be O
interpreted O
as O
representing O
the O
ofÔ¨Åcial O
policies O
, O
either O
expressed O
or O
implied O
, O
of O
the O
United O
States O
Air O
Force O
or O
the O
U.S. O
Government O
. O
The O
U.S. O
Government O
is O
authorized O
to O
reproduce O
and O
distribute O
reprints O
for O
Government O
purposes O
not O
withstanding O
any O
copyright O
notation O
herein O
. O
Abstract O
In O
this O
paper O
, O
we O
analyse O
the O
challenges O
of O
Chinese O
content O
scoring O
in O
comparison O
to O
English O
. O
As O
a O
review O
of O
prior O
work O
for O
Chinese O
content O
scoring O
shows O
a O
lack O
of O
openaccess O
data O
in O
the O
field O
, O
we O
present O
two O
short O
- O
answer O
data O
sets O
for O
Chinese O
. O
The O
Chinese O
Educational O
Short O
Answers O
data O
set O
( O
CESA O
) O
contains O
1800 O
student O
answers O
for O
five O
science O
- O
related O
questions O
. O
As O
a O
second O
data O
set O
, O
we O
collected O
ASAP O
- O
ZH O
with O
942 O
answers O
by O
re O
- O
using O
three O
existing O
prompts O
from O
the O
ASAP O
data O
set O
. O
W O
e O
adapt O
a O
state O
- O
of O
- O
the O
- O
art O
content O
scoring O
system O
for O
Chinese O
and O
evaluate O
it O
in O
several O
settings O
on O
these O
data O
sets O
. O
Results O
show O
that O
features O
on O
lower O
segmentation O
levels O
such O
as O
character O
n O
- O
grams O
tend O
to O
have O
better O
performance O
than O
features O
on O
token O
level O
. O
1 O
Introduction O
Short O
answer O
questions O
are O
a O
type O
of O
educational O
assessment O
that O
requires O
respondents O
to O
give O
natural O
language O
answers O
in O
response O
to O
a O
question O
or O
some O
reading O
material O
( O
Rademakers O
et O
al O
. O
, O
2005 O
) O
. O
The O
applications O
used O
to O
automatically O
score O
such O
questions O
are O
usually O
thought O
of O
as O
content O
scoring O
systems O
, O
because O
content O
( O
and O
not O
linguistic O
form O
) O
is O
taken O
into O
consideration O
for O
automatic O
scoring O
( O
Ziai O
et O
al O
. O
, O
2012 O
) O
. O
While O
there O
is O
a O
large O
research O
body O
for O
English O
content O
scoring O
, O
there O
is O
less O
research O
for O
Chinese.1The O
largest O
obstacle O
for O
more O
research O
on O
Chinese O
is O
the O
lack O
of O
publicly O
available O
data O
sets O
of O
Chinese O
short O
answer O
questions O
. O
1In O
this O
work O
, O
we O
use O
the O
term O
‚Äò O
Chinese O
‚Äô O
as O
abbreviation O
for O
Mandarin O
Chinese O
, O
which O
includes O
simplified O
and O
traditional O
written O
Chinese O
. O
Cantonese O
, O
W O
u O
, O
Min O
Nan O
and O
other O
dialects O
are O
not O
included O
. O
W O
orking O
with O
Chinese O
poses O
substantially O
different O
challenges O
than O
work O
on O
English O
data O
. O
Unlike O
English O
, O
which O
uses O
spaces O
as O
natural O
separators O
between O
words O
, O
segmentation O
of O
Chinese O
texts O
into O
tokens O
is O
challenging O
( O
Chen O
and O
Liu O
, O
1992 O
) O
. O
F O
urthermore O
, O
there O
are O
more O
options O
on O
which O
level O
to O
segment O
Chinese O
text O
. O
Apart O
from O
tokenization O
and O
segmentation O
into O
characters O
, O
which O
are O
two O
options O
also O
available O
and O
often O
used O
for O
English O
, O
segmentation O
into O
components O
, O
radicals O
and O
even O
individual O
strokes O
are O
additionally O
possible O
for O
Chinese O
. O
T O
able O
1gives O
an O
example O
for O
the O
segmentation O
options O
in O
both O
languages O
. O
Orthographic O
variance O
can O
be O
challenging O
in O
both O
languages O
, O
but O
behaves O
very O
differently O
. O
Nonword O
errors O
, O
which O
is O
the O
main O
source O
of O
orthographic O
problems O
in O
English O
( O
Mitton O
, O
1987 O
) O
, O
can O
by O
definition O
not O
happen O
in O
Chinese O
, O
due O
to O
the O
input O
modalities O
. O
Language O
Level O
Unigrams O
English O
word O
panda O
characters O
p O
, O
a O
, O
n O
, O
d O
, O
a O
Chinese O
word O
‡æß‡™≠ O
characters O
‡æßƒë‡™≠ O
components O
radicals O
·ßØ O
ƒë·óØ O
strokes O
  O
... O
T O
able O
1 O
: O
Comparison O
of O
segmentation O
possibilities O
in O
English O
and O
Chinese O
In O
the O
remainder O
of O
this O
paper O
, O
we O
will O
discuss O
these O
challenges O
in O
more O
detail O
( O
Section O
2 O
) O
. O
W O
e O
review O
prior O
work O
on O
Chinese O
content O
scoring O
( O
Section O
3 O
) O
and O
present O
two O
new O
freely O
- O
available O
data O
sets O
of O
short O
answers O
in O
Chinese O
( O
Section O
4 O
) O
. O
In O
Section O
5 O
, O
we O
adapt O
a347machine O
learning O
pipeline O
for O
automatic O
scoring O
with O
state O
- O
of O
- O
art O
NLP O
tools O
for O
Chinese O
. O
W O
e O
investigate O
the O
extraction O
of O
n O
- O
gram O
features O
on O
all O
possible O
segmentation O
levels O
. O
In O
addition O
, O
we O
use O
features O
based O
on O
the O
Pinyin O
transcription O
of O
Chinese O
texts O
and O
experiment O
with O
the O
removal O
of O
auxiliary O
words O
as O
an O
equivalent O
to O
lemmatization O
in O
English O
. O
W O
e O
evaluate O
these O
features O
on O
our O
new O
data O
sets O
as O
well O
as O
, O
for O
comparison O
, O
an O
English O
data O
set O
translated O
into O
Chinese O
. O
2 O
Challenges O
in O
Chinese O
Content O
Scoring O
In O
this O
section O
, O
we O
highlight O
the O
main O
challenges O
when O
processing O
Chinese O
learner O
data O
in O
comparison O
to O
English O
data O
sets O
. O
W O
e O
first O
focus O
on O
segmentation O
, O
as O
tokenization O
is O
more O
difficult O
in O
Chinese O
than O
in O
English O
and O
there O
are O
more O
linguistic O
levels O
on O
which O
to O
segment O
a O
Chinese O
text O
compared O
to O
English O
. O
Next O
, O
we O
discuss O
variance O
in O
learner O
answers O
, O
which O
is O
a O
challenge O
for O
content O
scoring O
in O
any O
language O
but O
manifests O
itself O
in O
Chinese O
differently O
than O
in O
English O
. O
2.1 O
Segmentation O
English O
has O
an O
alphabetic O
writing O
system O
with O
some O
degree O
of O
grapheme O
- O
to O
- O
phoneme O
correspondence O
. O
The O
Chinese O
language O
, O
in O
contrast O
, O
uses O
a O
logosyllabic O
writing O
system O
, O
where O
characters O
represent O
lexical O
morphemes O
. O
Chinese O
words O
can O
be O
formed O
by O
one O
or O
more O
characters O
( O
Chen O
, O
1992 O
) O
. O
Unlike O
English O
, O
where O
words O
are O
separated O
by O
white O
- O
spaces O
, O
the O
fact O
that O
Chinese O
writing O
does O
not O
mark O
word O
boundaries O
makes O
word O
segmentation O
a O
much O
harder O
task O
in O
Chinese O
NLP O
( O
e.g. O
, O
Chen O
and O
Liu O
( O
1992 O
) O
; O
Huang O
et O
al O
. O
( O
1996 O
) O
) O
. O
According O
to O
a O
recent O
literature O
review O
on O
Chinese O
word O
segmentation O
( O
Zhao O
et O
al O
. O
, O
2019 O
) O
, O
the O
best O
- O
performing O
segmentation O
tool O
has O
an O
average O
F1 O
- O
value O
of O
only O
around O
97 O
% O
. O
A O
major O
challenge O
is O
the O
handling O
of O
out O
- O
of O
- O
vocabulary O
words O
. O
In O
English O
content O
scoring O
, O
word O
level O
features O
such O
as O
word O
n O
- O
grams O
or O
word O
embeddings O
have O
proven O
to O
be O
effective O
( O
e.g. O
, O
Sakaguchi O
et O
al O
. O
( O
2015 O
) O
; O
Riordan O
et O
al O
. O
( O
2017 O
) O
) O
. O
Additionally O
, O
character O
features O
are O
frequently O
used O
to O
capture O
orthographic O
as O
well O
as O
morphological O
variance O
( O
e.g. O
, O
Heilman O
and O
Mad O
- O
nani O
( O
2013 O
) O
; O
Zesch O
et O
al O
. O
( O
2015 O
) O
) O
. O
In O
the O
light O
of O
the O
tokenziation O
challenges O
mentioned O
above O
, O
it O
is O
surprising O
that O
although O
most O
prior O
work O
on O
Chinese O
also O
applies O
word O
- O
level O
features O
( O
see O
Section O
3 O
) O
, O
the O
performance O
of O
their O
tokenizers O
are O
barely O
discussed O
and O
character O
- O
level O
features O
are O
neglected O
altogether O
. O
Apart O
from O
words O
and O
characters O
, O
there O
are O
more O
possibilities O
of O
segmentation O
in O
Chinese O
as O
discussed O
above O
. O
Consider O
, O
for O
example O
, O
a O
Chinese O
bi O
- O
morphemic O
word O
such O
aspanda O
bear O
‡æß‡™≠ O
. O
It O
can O
additionally O
be O
segmented O
on O
the O
stroke O
, O
component O
and O
radical O
level O
as O
shown O
in O
T O
able O
1 O
. O
It O
has O
been O
argued O
that O
the O
morphological O
information O
of O
characters O
in O
Chinese O
consists O
of O
the O
sequential O
information O
hidden O
in O
stroke O
order O
and O
the O
spatial O
information O
hidden O
in O
character O
components O
( O
T O
ao O
et O
al O
. O
, O
2019 O
) O
. O
Each O
Chinese O
character O
can O
directly O
be O
mapped O
into O
a O
series O
of O
strokes O
( O
with O
a O
particular O
order O
) O
. O
On O
the O
component O
level O
, O
it O
has O
been O
estimated O
that O
about O
80 O
% O
of O
modern O
Chinese O
characters O
are O
phonetic O
- O
logographic O
compounds O
, O
each O
of O
which O
consists O
of O
two O
components O
: O
One O
carries O
the O
sound O
of O
the O
character O
( O
the O
stem O
) O
and O
the O
other O
the O
meaning O
of O
the O
character O
( O
the O
radical O
) O
( O
Li,1977 O
) O
. O
W O
e O
argue O
that O
, O
together O
with O
strokes O
, O
both O
kinds O
of O
components O
may O
be O
used O
as O
features O
in O
content O
scoring O
. O
Note O
that O
in O
some O
cases O
, O
a O
character O
has O
only O
one O
component O
, O
which O
in O
the O
extreme O
case O
consists O
of O
one O
stroke O
only O
, O
so O
that O
for O
the O
characterone·ÅÇ O
, O
all O
four O
segmentation O
levels O
yield O
the O
same O
result O
, O
somewhat O
comparable O
to O
an O
English O
onecharacter O
word O
, O
such O
as O
‚Äú O
I O
‚Äù O
. O
2.2 O
Linguistic O
V O
ariance O
V O
ariance O
in O
learner O
answers O
has O
a O
major O
influence O
on O
content O
scoring O
performance O
( O
Horbach O
and O
Zesch O
, O
2019 O
) O
, O
i.e. O
, O
the O
more O
variance O
between O
the O
answers O
to O
a O
specific O
prompt O
, O
the O
harder O
it O
is O
to O
score O
automatically O
. O
If O
we O
ignore O
cases O
of O
conceptually O
different O
answers O
, O
variance O
means O
different O
realizations O
with O
approximately O
the O
same O
semantic O
meaning O
. O
As O
shown O
in O
T O
able O
2 O
, O
if O
we O
have O
a O
question O
about O
the O
eating O
habits O
of O
pandas O
, O
Chinese O
short O
answers O
can O
contain O
similar O
variance O
as O
in O
English O
, O
which O
is O
realized O
as O
both O
orthographic348variance O
caused O
by O
spelling O
errors O
as O
well O
as O
variance O
of O
linguistic O
expression O
. O
Note O
that O
these O
types O
of O
variance O
should O
not O
influence O
the O
score O
of O
an O
answer O
as O
it O
depends O
only O
from O
the O
content O
of O
the O
answer O
. O
Both O
types O
of O
variance O
are O
further O
discussed O
in O
the O
following O
. O
Spelling O
errors O
in O
English O
can O
be O
classified O
into O
non O
- O
word O
and O
real O
- O
word O
spelling O
errors O
. O
In O
our O
example O
, O
‚Äú O
bambu O
‚Äù O
is O
a O
non O
- O
word O
, O
while O
‚Äú O
beer O
‚Äù O
is O
a O
real O
word O
spelling O
error O
. O
Both O
error O
types O
occur O
frequently O
in O
English O
short O
answer O
data O
sets O
, O
with O
non O
- O
word O
errors O
being O
more O
frequent O
( O
Mitton O
, O
1987 O
, O
1996 O
) O
. O
A O
content O
scoring O
system O
must O
therefore O
be O
able O
to O
generalize O
by O
taking O
variance O
in O
spelling O
into O
account O
( O
Leacock O
and O
Chodorow O
, O
2003 O
) O
. O
T O
o O
do O
so O
, O
many O
systems O
for O
English O
data O
use O
character O
- O
level O
features O
( O
Heilman O
and O
Madnani O
, O
2013 O
; O
Horbach O
et O
al O
. O
, O
2017 O
) O
, O
such O
that O
‚Äú O
bamboo O
‚Äù O
and O
‚Äú O
bambu O
‚Äù O
, O
while O
being O
different O
tokens O
, O
share O
, O
for O
example O
, O
the O
character O
3 O
- O
grams O
mbamnand O
mambn O
. O
F O
or O
Chinese O
, O
the O
situation O
is O
entirely O
different O
. O
Non O
- O
word O
spelling O
errors O
are O
rare O
and O
even O
impossible O
for O
digitized O
data O
because O
of O
the O
input O
modalities O
typically O
used O
for O
Chinese O
text O
. O
When O
entering O
a O
Chinese O
text O
on O
the O
computer O
, O
a O
writer O
would O
normally O
type O
the O
phonetic O
transcription O
Pinyin O
, O
which O
is O
the O
Romanization O
of O
Chinese O
characters O
based O
on O
their O
pronunciation O
. O
After O
typing O
a O
Pinyin O
, O
the O
writer O
is O
shown O
all O
corresponding O
characters O
from O
which O
they O
choose O
the O
right O
one O
. O
As O
this O
selection O
list O
contains O
only O
valid O
Chinese O
characters O
, O
non O
- O
word O
errors O
can O
not O
occur O
by O
definition O
. O
Even O
if O
the O
original O
data O
set O
was O
collected O
in O
hand O
- O
written O
format O
, O
the O
transcription O
process O
forces O
transcribers O
to O
correct O
any O
non O
- O
word O
error O
that O
might O
occur O
in O
the O
data O
. O
F O
or O
example O
, O
if O
the O
learner O
accidentally O
wrotepanda O
bear O
‡æß‡™≠ O
as O
  O
, O
the O
transcriber O
has O
no O
choice O
but O
to O
correct O
such O
an O
error O
, O
since O
the O
non O
- O
word O
character O
simply O
does O
not O
exist O
in O
the O
Chinese O
character O
set O
. O
There O
are O
two O
steps O
in O
the O
writing O
/ O
transcription O
process O
where O
errors O
can O
still O
occur O
: O
typing O
letters O
to O
spell O
a O
Pinyin O
and O
choosing O
a O
character O
out O
of O
a O
list O
for O
this O
Pinyin O
. O
Previous O
experiments O
showed O
that O
people O
usually O
do O
not O
check O
Pinyin O
for O
errors O
, O
but O
wait O
until O
the O
Chinese O
characters O
start O
to O
show O
up O
( O
Chenand O
Lee O
, O
2000 O
) O
. O
This O
behaviour O
generates O
two O
types O
of O
real O
- O
word O
spelling O
errors O
. O
In O
our O
example O
, O
spelling O
errors O
like O
confusingpoor O
‡±´ O
( O
qi«íng O
) O
withbear O
‡æß O
( O
xi«íng O
) O
are O
normally O
caused O
by O
wrong O
letters O
typed O
in O
the O
first O
step O
. O
The O
other O
error O
type O
, O
i.e. O
, O
choosing O
a O
wrong O
word O
from O
the O
homophones O
, O
leads O
to O
spelling O
errors O
like O
pearl O
·á®·à∞ O
( O
zh≈´ O
zi O
) O
instead O
ofbamboo O
·á∞·à∞ O
( O
zh√∫ O
zi O
) O
. O
Researchers O
found O
that O
nearly O
95 O
% O
of O
errors O
are O
due O
to O
the O
misuse O
of O
homophones O
( O
Y O
ang O
et O
al O
. O
, O
2012 O
) O
, O
i.e. O
, O
are O
errors O
of O
the O
second O
type O
. O
In O
order O
to O
reduce O
the O
influence O
of O
these O
errors O
in O
content O
scoring O
, O
introducing O
features O
presented O
as O
Pinyin O
might O
be O
beneficial O
. O
V O
ariance O
of O
linguistic O
expression O
is O
obviously O
found O
in O
both O
English O
and O
Chinese O
short O
answers O
. O
As O
shown O
in O
T O
able O
2 O
, O
nearly O
the O
same O
content O
can O
be O
expressed O
using O
different O
lexical O
and O
syntactic O
choices O
. O
Human O
annotators O
can O
usually O
abstract O
away O
from O
these O
differences O
and O
treat O
all O
answers O
the O
same O
. O
However O
, O
linguistic O
variance O
is O
a O
challenge O
for O
automatic O
scoring O
systems O
. O
In O
English O
content O
scoring O
, O
lemmatization O
is O
often O
considered O
a O
useful O
method O
to O
reduce O
part O
of O
the O
variance O
( O
Koleva O
et O
al O
. O
, O
2014 O
) O
. O
In O
this O
process O
, O
words O
are O
reduced O
to O
their O
base O
forms O
, O
such O
as O
substituting O
‚Äú O
ate O
‚Äù O
with O
‚Äú O
eat O
‚Äù O
and O
deleting O
the O
‚Äú O
s O
‚Äù O
after O
‚Äú O
bamboo O
‚Äù O
. O
In O
Chinese O
, O
similar O
grammatical O
morphemes O
such O
as O
‚Äú O
‡®î O
‚Äù O
and O
‚Äú O
‡´å O
‚Äù O
, O
termed O
auxiliary O
words O
( O
Zan O
and O
Zhu O
, O
2009 O
) O
, O
which O
indicate O
the O
past O
tense O
and O
plural O
, O
can O
also O
be O
deleted O
in O
a O
preprocessing O
step O
to O
achieve O
a O
similar O
effect O
. O
Another O
type O
of O
variance O
is O
caused O
by O
synonyms O
. O
F O
or O
such O
cases O
of O
lexical O
variance O
, O
external O
knowledge O
is O
often O
needed O
to O
decide O
that O
two O
different O
words O
are O
interchangeable O
. O
However O
, O
as O
we O
can O
see O
in O
T O
able O
2 O
, O
some O
synonyms O
, O
such O
as O
‚Äú O
panda O
bears O
‚Äù O
vs. O
‚Äú O
pandas O
‚Äù O
andbamboo O
·á∞·à∞ O
vs.bamboo O
·á∞ O
share O
some O
character(s O
) O
. O
Such O
similarities O
can O
be O
covered O
by O
character O
features O
, O
but O
not O
token O
n O
- O
grams O
. O
In O
summary O
, O
there O
is O
the O
challenge O
of O
the O
segmentation O
of O
Chinese O
texts O
into O
tokens O
. O
F O
eatures O
extracted O
on O
other O
segmentation O
levels O
might O
be O
more O
robust O
and O
therefore O
helpful O
for O
automatic O
scoring O
. O
At O
the O
same O
time O
, O
NLP O
techniques O
which O
are O
useful O
to O
reduce O
variance349English O
Chinese O
Reference O
Answer O
Panda O
bears O
eat O
bamboo.panda O
bear O
‡æß‡™≠eat O
”πbamboo O
·á∞·à∞ O
b O
Orthographic O
V O
ariance O
Panda O
beers O
eat O
bambu O
.poor O
‡±´cat O
‡™≠eat O
”πpearl O
·á®·à∞b O
Expression O
V O
ariance O
Panda O
bears O
ate O
bamboos O
.panda O
‡æßbear O
‡™≠eat O
”π<grammatical O
morpheme O
for O
past O
tense O
> O
›ñ O
bamboo O
·á∞·à∞ O
< O
grammatical O
morpheme O
for O
plural O
> O
‡´å O
b O
Pandas O
eat O
bamboo.panda O
bear O
‡æß‡™≠eat O
”πbamboo O
·á∞b O
T O
able O
2 O
: O
Example O
answers O
showing O
variance O
in O
English O
and O
Chinese O
for O
the O
question O
: O
What O
do O
panda O
bears O
eat O
? O
in O
English O
, O
especially O
lemmatization O
, O
have O
not O
yet O
been O
transferred O
to O
Chinese O
. O
Thus O
, O
we O
will O
explore O
in O
our O
experiments O
both O
n O
- O
gram O
features O
on O
different O
levels O
and O
the O
removal O
of O
auxiliary O
words O
. O
3 O
Prior O
W O
ork O
on O
Chinese O
Content O
Scoring O
As O
shown O
in O
T O
able O
3 O
, O
all O
prior O
work O
on O
Chinese O
content O
scoring O
uses O
lexical O
features O
on O
the O
word O
level O
, O
such O
as O
word O
n O
- O
grams O
and O
sentence O
length O
in O
tokens O
. O
They O
are O
not O
only O
used O
in O
shallow O
learning O
methods O
like O
support O
vector O
machines O
( O
SVM O
) O
or O
support O
vector O
regression O
( O
SVR O
) O
( O
W O
ang O
et O
al O
. O
, O
2008 O
; O
W O
u O
and O
Shih O
, O
2018 O
) O
, O
but O
also O
applied O
to O
deep O
learning O
methods O
like O
long O
- O
short O
term O
memory O
recurrent O
neural O
networks O
( O
LSTM O
) O
( O
Y O
ang O
et O
al O
. O
, O
2017 O
; O
Huang O
et O
al O
. O
, O
2018 O
) O
or O
deep O
autoencoders O
( O
Y O
ang O
et O
al O
. O
, O
2018 O
) O
. O
Also O
for O
neural O
models O
using O
word O
embeddings O
, O
word O
- O
level O
tokenization O
is O
necessary O
. O
W O
u O
and O
Y O
eh O
( O
2019 O
) O
train O
300 O
- O
dimensional O
word2vec O
word O
embeddings O
on O
sentences O
from O
their O
data O
set O
along O
with O
Chinese O
Wikipedia O
articles O
and O
classify O
student O
answers O
with O
a O
convolution O
neural O
network O
( O
CNN O
) O
. O
Li O
et O
al O
. O
( O
2019 O
) O
use O
a O
Bidirectional O
Long O
ShortT O
erm O
Memory O
( O
Bi O
- O
LSTM O
) O
network O
for O
semantic O
feature O
extraction O
from O
pre O
- O
trained O
300dimensional O
word O
embeddings O
( O
Li O
et O
al O
. O
, O
2018 O
) O
and O
score O
student O
answers O
based O
on O
their O
similarity O
to O
the O
reference O
answer O
using O
a O
mutual O
attention O
mechanism O
. O
F O
or O
segmentation O
, O
most O
prior O
work O
uses O
the O
jieba O
tokenizer2for O
pre O
- O
processing O
. O
However O
, O
2https://github.com/fxsjy/jiebathe O
performance O
of O
the O
tokenization O
is O
rarely O
discussed O
. O
W O
e O
also O
notice O
that O
no O
related O
work O
uses O
segmentation O
on O
character O
or O
component O
level O
. O
Y O
ang O
et O
al O
. O
( O
2018 O
) O
perform O
stop O
word O
removal O
, O
but O
they O
do O
not O
mention O
if O
it O
included O
some O
kind O
of O
removal O
of O
grammatical O
markers O
. O
4 O
Chinese O
Scoring O
Data O
Sets O
In O
this O
section O
, O
we O
review O
existing O
Chinese O
content O
scoring O
data O
sets O
. O
They O
are O
not O
publicly O
available O
, O
which O
is O
a O
major O
obstacle O
to O
reproducibility O
in O
the O
field O
. O
W O
e O
thus O
produce O
two O
new O
Chinese O
data O
sets O
( O
see O
detailed O
description O
in O
Section O
4.2 O
) O
, O
which O
are O
available O
online3to O
foster O
future O
research O
. O
4.1 O
Existing O
Data O
Sets O
Horbach O
and O
Zesch O
( O
2019 O
) O
give O
an O
overview O
of O
publicly O
available O
data O
sets O
for O
content O
scoring O
, O
five O
of O
which O
are O
for O
English O
, O
and O
compare O
them O
based O
on O
properties O
such O
as O
prompt O
type O
, O
learner O
population O
and O
data O
set O
size O
. O
Unfortunately O
, O
we O
did O
not O
find O
any O
freely O
available O
Chinese O
content O
scoring O
data O
sets O
. O
Since O
we O
could O
not O
access O
the O
data O
sets O
used O
in O
related O
work O
, O
we O
can O
only O
compare O
them O
based O
on O
their O
brief O
descriptions O
, O
according O
to O
the O
aspects O
of O
comparison O
mentioned O
above O
. O
Results O
are O
shown O
in O
T O
able O
4 O
. O
The O
Debris O
Flow O
Hazard O
( O
DFH O
) O
data O
set O
is O
used O
in O
the O
earliest O
work O
. O
It O
contains O
more O
than O
1000 O
answers O
for O
2 O
prompts O
in O
a O
creative O
problem O
- O
solving O
task O
. O
The O
learner O
population O
are O
high O
- O
school O
students O
from O
T O
aiwan O
, O
who O
speak O
native O
Chinese O
( O
W O
ang O
et O
al O
. O
, O
2008 O
) O
. O
3https://github.com/ltlude/ChineseShortAnswerDatasets350Reference O
Data O
Set O
Preprocessing O
F O
eatures O
Classifier O
Evaluation O
W O
ang O
et O
al O
. O
( O
2008 O
) O
DFH O
tasktokenization O
, O
POS O
taggingword O
uni-/bigrams O
, O
POS O
bigramsSVM O
r=.92 O
W O
u O
and O
Shih O
( O
2018)SCB O
- O
ZHMT O
CS O
- O
ENMTtokenization O
( O
jieba)sentence O
length O
, O
word O
unigrams O
, O
BLEU O
scoreSVR O
, O
SVMacc=.60 O
RMSE=1.17 O
Y O
ang O
et O
al O
. O
( O
2017 O
) O
CRCCtokenization O
( O
jieba)word O
unigrams O
LSTMacc=.76 O
, O
Cohen O
‚Äôs O
Œ∫=.61 O
Y O
ang O
et O
al O
. O
( O
2018 O
) O
CRCCpunctuation O
and O
stop O
word O
removal O
, O
tokenization O
( O
jieba)word O
unigramsAutoencoderacc=.74 O
, O
qwk=.64 O
Huang O
et O
al O
. O
( O
2018 O
) O
CRCCtokenization O
( O
jieba)word O
vector O
trained O
on O
CBOWLSTMacc=.74 O
, O
qwk=.62 O
W O
u O
and O
Y O
eh O
( O
2019)ML_SQA O
SCB O
- O
ZHMTtokenization O
( O
jieba)300D O
pre O
- O
trained O
word O
embeddingCNNacc=.91 O
, O
recall=.82 O
Li O
et O
al O
. O
( O
2019 O
) O
Law O
Questions O
tokenization300D O
pre O
- O
trained O
word O
embeddingBi O
- O
LSTM O
acc=.88 O
T O
able O
3 O
: O
Overview O
of O
related O
work O
in O
Chinese O
content O
scoring O
. O
The O
Chinese O
Reading O
Comprehension O
Corpus O
( O
CRCC O
) O
( O
Y O
ang O
et O
al O
. O
, O
2018 O
) O
, O
contains O
five O
reading O
comprehension O
questions O
. O
Each O
question O
has O
on O
average O
2500 O
answers O
from O
students O
in O
grade O
8 O
. O
Instead O
of O
collecting O
and O
annotating O
a O
data O
set O
from O
scratch O
, O
W O
u O
and O
Shih O
( O
2018 O
) O
translated O
the O
English O
SciEntBank O
( O
Dzikovska O
et O
al O
. O
, O
2013 O
) O
and O
the O
computer O
science O
( O
CS O
) O
( O
Mohler O
and O
Mihalcea O
, O
2009 O
) O
data O
sets O
to O
Chinese O
. O
The O
data O
set O
was O
first O
translated O
using O
machine O
translation O
. O
In O
order O
to O
solve O
word O
usage O
and O
grammar O
problems O
, O
12 O
% O
of O
the O
sentences O
were O
manually O
corrected O
. O
In O
their O
most O
recent O
work O
, O
the O
authors O
also O
collected O
a O
data O
set O
with O
12 O
short O
answer O
questions O
and O
overall O
600 O
answers O
related O
to O
machine O
learning O
( O
ML_SQA O
) O
to O
compare O
with O
the O
CS O
- O
ZHMT O
data O
set O
( O
W O
u O
and O
Y O
eh O
, O
2019 O
) O
. O
In O
the O
most O
recent O
work O
( O
Li O
et O
al O
. O
, O
2019 O
) O
, O
a O
large O
data O
set O
containing O
85.000 O
student O
and O
reference O
answers O
was O
collected O
from O
a O
national O
specialty O
examination O
related O
to O
law O
. O
4.2 O
Collection O
of O
Open O
- O
access O
Data O
Sets O
As O
part O
of O
the O
contribution O
in O
this O
paper O
, O
we O
collected O
two O
new O
data O
sets O
for O
Chinese O
content O
scoring O
: O
Chinese O
Short O
Answer O
( O
CESA O
) O
and O
ASAP O
- O
ZH O
. O
In O
addition O
, O
we O
provide O
a O
machine O
- O
translated O
version O
of O
the O
the O
originalASAP O
- O
SAS O
English O
data O
, O
ASAP O
- O
ZHMT O
. O
T O
able O
4shows O
key O
properties O
, O
while O
T O
able O
5gives O
example O
answers O
of O
each O
data O
set O
. O
Chinese O
Educational O
Short O
Answers O
( O
CESA O
) O
contains O
five O
questions O
from O
the O
physics O
and O
computer O
science O
domain O
( O
see O
T O
able O
6 O
) O
. O
Answers O
are O
collected O
from O
360 O
students O
in O
the O
computer O
science O
department O
of O
Zhengzhou O
University O
. O
Each O
participant O
was O
required O
to O
answer O
each O
question O
with O
a O
maximum O
of O
20 O
characters O
, O
resulting O
in O
an O
average O
answer O
length O
of O
13.5 O
characters O
. O
T O
wo O
annotators O
speaking O
native O
Chinese O
with O
computer O
science O
background O
scored O
the O
answers O
into O
three O
classes O
, O
0 O
, O
1 O
and O
2 O
points O
, O
with O
an O
average O
inter O
- O
annotator O
agreement O
of O
0.9 O
quadratically O
weighted O
kappa O
( O
QWK O
) O
. O
ASAP O
- O
ZH O
This O
data O
set O
is O
based O
on O
the O
ASAP O
short O
- O
answer O
scoring O
data O
set O
released O
by O
the O
Hewlett O
F O
oundation.4ASAP O
contains O
ten O
short O
answer O
prompts O
covering O
different O
subjects O
and O
about O
2000 O
student O
answers O
per O
prompt O
. O
Prompt O
1 O
, O
2 O
and O
10 O
are O
sciencerelated O
tasks O
, O
which O
do O
not O
have O
a O
strong O
cultural O
background O
, O
and O
are O
therefore O
considered O
as O
appropriate O
to O
be O
transferred O
to O
other O
languages O
. O
Therefore O
, O
we O
collected O
answers O
in O
Chinese O
4http://www.kaggle.com/c/asap-sas351Data O
Set O
Type O
# O
Answers O
# O
Prompts O
Labels O
Level O
DFHcreative O
problem O
solving2,698 O
2 O
[ O
0,1, O
... O
,28 O
] O
high O
school O
CRCCreading O
comprehension12,528 O
5 O
[ O
0,1,2,3,(4),(5 O
) O
] O
middle O
school O
SciEntsBank O
- O
ZHMTscience O
9,804 O
197 O
binary&diagnostic O
high O
school O
CS O
- O
ZHMTcomputer O
science O
630 O
21 O
[ O
0 O
, O
0.5 O
, O
... O
, O
5 O
] O
university O
ML_SQA O
computer O
science O
608 O
12 O
binary O
university O
Law O
Questions O
law O
85,000 O
2 O
[ O
0,1.5,3]/[0,1,1.5 O
] O
CESAphysics O
, O
computer O
science1,800 O
5 O
[ O
0,1,2 O
] O
university O
ASAP O
- O
ZH O
science O
942 O
3 O
[ O
0,1,2,(3 O
) O
] O
high O
school O
ASAP O
- O
ZHMTscience O
6,119 O
3 O
[ O
0,1,2,(3 O
) O
] O
high O
school O
T O
able O
4 O
: O
Chinese O
content O
scoring O
data O
sets O
: O
data O
sets O
from O
previous O
work O
( O
upper O
part O
) O
and O
our O
new O
data O
sets O
( O
lower O
part O
) O
for O
these O
three O
prompts O
after O
manually O
translating O
the O
prompt O
material O
. O
The O
data O
collection O
provider O
BasicFinder5helped O
us O
to O
collect O
942 O
answers O
altogether O
, O
314 O
answers O
for O
each O
prompt O
. O
They O
are O
collected O
from O
students O
in O
high O
school O
from O
grades O
9 O
- O
12 O
, O
which O
is O
comparable O
with O
the O
set O
of O
English O
answers O
in O
the O
ASAP O
- O
SAS O
data O
set O
. O
The O
answers O
are O
transcribed O
into O
digital O
form O
manually O
after O
being O
collected O
in O
handwriting O
. O
After O
reaching O
an O
acceptable O
agreement O
on O
a O
set O
of O
answers O
from O
the O
original O
ASAP O
- O
SAS O
, O
two O
annotators O
speaking O
native O
Chinese O
scored O
the O
ASAP O
- O
ZH O
data O
on O
a O
scale O
from O
0 O
to O
3 O
points O
( O
prompt O
1 O
and O
2 O
) O
or O
0 O
to O
2 O
points O
( O
prompt O
10 O
) O
with O
an O
average O
QWK O
of O
0.7 O
. O
Key O
statistics O
for O
the O
data O
set O
can O
be O
found O
in O
T O
able O
7 O
. O
ASAP O
- O
ZHMTF O
or O
comparison O
, O
we O
also O
translated O
the O
English O
answers O
in O
prompts O
1,2 O
and O
10 O
in O
the O
original O
ASAP O
- O
SAS O
data O
set O
to O
Chinese O
using O
the O
Google O
T O
ranslate O
API.6The O
examples O
in O
T O
able O
5show O
that O
some O
translation O
errors O
can O
be O
found O
, O
especially O
when O
errors O
exist O
already O
in O
the O
original O
text O
. O
W O
ords O
containing O
spelling O
errors O
like O
‚Äú O
wat O
‚Äù O
instead O
of O
‚Äù O
what O
‚Äù O
are O
simply O
not O
translated O
at O
all O
. O
The O
overall O
translation O
quality O
is O
also O
not O
perfect O
, O
for O
example O
, O
the O
word O
‚Äú O
coolest O
‚Äù O
is O
wrongly O
translated O
intomost O
·âãfashioned O
‡•Æ÷• O
instead O
of O
the O
correctmost O
·âãcoldest O
‡ßè÷• O
. O
  O
5https://www.basicfinder.com/en O
6https://cloud.google.com/translateAs O
shown O
in O
T O
ables O
7 O
and O
8 O
, O
the O
average O
length O
of O
the O
translated O
answers O
is O
larger O
than O
the O
length O
of O
the O
original O
Chinese O
answers O
to O
the O
same O
prompt O
in O
our O
re O
- O
collected O
data O
set O
. O
One O
explanation O
could O
be O
that O
paid O
crowd O
workers O
are O
less O
motivated O
than O
actual O
students O
and O
therefore O
write O
shorter O
answers O
. O
5 O
Experimental O
Setup O
In O
this O
section O
, O
we O
adapt O
a O
state O
- O
of O
- O
the O
- O
art O
content O
scoring O
system O
to O
Chinese O
. O
W O
e O
evaluate O
it O
in O
six O
settings O
with O
different O
feature O
sets O
on O
the O
data O
sets O
described O
above O
in O
order O
to O
investigate O
different O
options O
for O
segmentation O
of O
Chinese O
text O
. O
T O
able O
9gives O
an O
example O
for O
the O
different O
segmentation O
options O
, O
which O
will O
also O
be O
detailed O
in O
Section O
5.2 O
. O
Additionally O
, O
we O
add O
a O
pre O
- O
processing O
step O
to O
remove O
all O
auxiliary O
words O
in O
the O
data O
in O
order O
to O
simulate O
the O
effect O
of O
lemmatization O
in O
English O
content O
scoring O
. O
5.1 O
General O
Experimental O
Setup O
F O
or O
all O
our O
experiments O
, O
we O
use O
the O
ESCRITO O
( O
Zesch O
and O
Horbach O
, O
2018 O
) O
toolkit O
and O
extended O
it O
with O
readers O
and O
tokenization O
for O
Chinese O
text O
. O
ESCRITO O
is O
a O
publicly O
available O
general O
- O
purpose O
scoring O
framework O
based O
on O
DKPro O
TC O
( O
Daxenberger O
et O
al O
. O
, O
2014 O
) O
, O
which O
uses O
an O
SVM O
classifier O
( O
Cortes O
and O
V O
apnik O
, O
1995 O
) O
using O
the O
SMO O
algorithm O
as O
provided O
by O
WEKA O
( O
Witten O
et O
al O
. O
, O
1999 O
) O
. O
F O
or O
all O
kinds O
of O
features O
, O
we O
use O
the O
top O
10000 O
most O
frequent352Data O
Set O
ID O
Score O
Example O
CESA O
52 O
The O
machine O
summarizes O
a O
large O
amount O
of O
data O
and O
finds O
the O
pattern O
from O
it O
‡†è‡∞ñ·àπ‡¢≤’∂‡®à‡∂î‡§åƒë’ñ·áè·Ö≥‹ø÷û‡©∞ O
1 O
Machines O
can O
learn O
things O
by O
themselves O
‡†è‡∞ñ‡¨ø·à±‡†≠‡øê O
‡ºù O
◊™‡ºÜ O
0 O
Let O
the O
machine O
learn O
human O
thinking O
ability O
‡≤û‡†è‡∞ñ‡øê O
‡ºù O
‡≤¶÷•‡∂±‡Ωò‡¨ø‡ßØ O
ASAP O
- O
ZH O
10 O
2 O
White O
: O
make O
the O
indoor O
temperature O
not O
too O
high O
, O
œ¢‡≥§‡µê‡µ©‡¨Ω‡∞ó‡ªë O
“Ç O
‡∑æ€ö O
experiments O
show O
that O
white O
has O
the O
lowest O
light O
energy O
absorption O
rate O
‡µå·Äí—ñ‡´ºœ¢‡≥§÷•‹ªÿì‡¨ø‡®à‡ºã‡µ¨‡©±·âã÷Æ O
1 O
Black O
allows O
the O
doghouse O
to O
absorb O
more O
heat O
in O
the O
light O
, O
making O
it O
warm O
ﬁë‡≥§‡¨ø‡≤û‹ê‡ª†·Ñù‹ª‡ºØ‡ºã÷•ÿü€∑‡≤£ƒë‡µê‡∞É‡ªë O
‡≠∞ O
0 O
Dark O
gray O
: O
keep O
the O
temperature O
unchanged O
, O
‡¥ßﬂß‡≥§ƒü O
ƒü‡µê‡ªëÿá O
“Ç O
—çƒë O
the O
lighter O
the O
color O
, O
the O
lower O
the O
temperature O
‡øæ‡≥§·ÑÄ‡∞∞‡ªëÿá·ÑÄ÷Æ O
ASAP O
- O
ZHMT10 O
2 O
white O
: O
: O
having O
white O
paint O
would O
make O
the O
dog O
house O
colder O
, O
œ¢‡≥§ O
: O
: O
·Çµœ¢‡≥§·Ç≤‡∞Ä O
ﬂ∂ O
‡µê‹ê‡ª¨€∑‡ßèƒë O
so O
in O
the O
summer O
the O
dog O
would O
not O
be O
hot O
. O
‡∑Æ·Åõ·Ñù‡º±‡∏ø‹ê O
“Ç O
ﬁì O
ﬂ∂ O
‡≤£b O
The O
average O
for O
white O
is O
the O
coolest O
temperature O
( O
42 O
( O
DEG O
) O
) O
œ¢‡≥§÷•‡Øú‡§®·Ü¥‡µû·âã‡•Æ÷• O
‡ªëÿáƒç O
42ƒçDEGƒé O
ƒé O
1 O
black O
: O
: O
Because O
, O
the O
darker O
the O
lid O
color O
, O
ﬁë‡≥§ O
: O
: O
·Åπ‡∫πƒë€Ç·à∞‡øæ‡≥§·ÑÄ‡¥ßƒë O
the O
greater O
the O
increase O
in O
the O
air O
temperature O
in O
the O
glass O
jar O
. O
—™‡ß∞‹∑·áè‡•¢‡∞ó‡ªë÷•ÿá‡¥∂‡£º€ö·ÑÄ’∂b O
0 O
light O
gray O
: O
: O
The O
light O
grey O
will O
effect O
the O
doghouse O
by O
making O
it O
more O
noticable O
‡∞∞ﬂß‡≥§ƒü‡∞∞ﬂß‡≥§ O
ﬂ∂ O
‡µê‡°Ü€∑‹ê‹ê‡ΩÅ·ÄÑƒë O
and O
plus O
dogs O
can O
only O
see O
black O
, O
white O
and O
grey O
. O
‡°Ü‡¥à‹ê·Ü∫‡¨ø‡•Åﬁë÷û‡≥§ƒëœ¢‡≥§ﬂßﬁÑ‡≥§b O
T O
able O
5 O
: O
Example O
answers O
in O
our O
data O
sets O
. O
1- O
to O
5 O
- O
grams O
. O
Due O
to O
the O
limited O
amount O
of O
data O
, O
we O
use O
10 O
- O
fold O
cross O
- O
validation O
on O
both O
data O
sets O
. O
F O
or O
evaluation O
, O
we O
use O
accuracy O
, O
i.e. O
, O
the O
percentage O
of O
student O
answers O
scored O
correctly O
, O
as O
well O
as O
QWK O
, O
which O
does O
not O
only O
consider O
whether O
an O
answer O
is O
classified O
correctly O
or O
not O
, O
but O
also O
how O
far O
it O
is O
from O
the O
gold O
standard O
classification O
. O
5.2 O
F O
eature O
Sets O
T O
oken O
Baseline O
As O
a O
baseline O
, O
we O
follow O
previous O
work O
and O
use O
tokenization O
as O
segmentation O
, O
based O
on O
the O
HanLP O
tokenizer O
( O
He O
, O
2020 O
) O
. O
Pinyin O
F O
eatures O
In O
order O
to O
reduce O
the O
variance O
caused O
by O
spelling O
errors O
, O
we O
transcribe O
the O
text O
into O
Pinyin O
using O
cnchar O
( O
Chen O
, O
2020 O
) O
and O
extract O
ngrams O
on O
the O
level O
of O
transcribed O
characters O
. O
Note O
that O
we O
did O
not O
include O
information O
about O
tones O
in O
Pinyinon O
purpose O
, O
in O
order O
to O
cover O
spelling O
errors O
caused O
by O
homophones O
. O
Character O
F O
eatures O
F O
or O
this O
segmentation O
level O
, O
we O
simply O
split O
a O
text O
into O
individual O
characters O
. O
Component O
F O
eatures O
T O
o O
extract O
these O
features O
on O
sub O
- O
character O
level O
, O
we O
use O
a O
dictionary O
with O
17,803 O
Chinese O
characters7and O
their O
components O
to O
decompose O
all O
characters O
. O
Radical O
F O
eatures O
Remember O
that O
radicals O
are O
only O
those O
components O
carrying O
the O
meaning O
of O
characters O
and O
might O
therefore O
be O
particularly O
useful O
in O
content O
scoring O
. O
W O
e O
use O
XMNLP O
( O
Li,2019 O
) O
to O
extract O
the O
radicals O
of O
each O
character O
and O
use O
only O
those O
as O
features O
. O
Note O
that O
some O
radicals O
as O
defined O
by O
the O
‚Äú O
T O
able O
of O
Indexing O
Chinese O
Character O
Compo7https://github.com/kfcd/chaizi353ID O
Prompt O
IAA O
avg O
. O
Distribution O
( O
QWK O
) O
Length O
1why O
‡∫π‡µâ‡™πwe O
‡ª°‡´åcan O
‡¨øuse O
·Ç®diamond O
·âá‡µÜcut O
‡±çglass O
—™‡ß∞ O
? O
.94 O
9.6 O
2why O
‡∫π‡µâ‡™πred O
ﬁ£‡≥§clothes O
·Åâ⁄õlooks O
‡•Å‡∞è‡¶üas O
‡µûred O
ﬁ£‡≥§÷• O
. O
? O
83 O
14.7 O
3what O
‡µâ‡™πis O
‡µûartificial O
‡≤¶ O
€Ω O
intelligence O
·áÜ‡¨ø O
? O
.91 O
15.3 O
4what O
‡µâ‡™πis O
‡µûnatural O
·à±‡≤ñlanguage O
·Éî‡øΩ O
? O
.93 O
12.1 O
5what O
‡µâ‡™πis O
‡µûmachine O
‡†è‡∞ñlearning O
‡øê O
‡ºù O
? O
.89 O
15.7 O
T O
able O
6 O
: O
Overview O
of O
prompts O
in O
CESA O
ID O
IAA O
avg O
. O
Distribution O
( O
QWK O
) O
Length O
1 O
.72 O
35.3 O
2 O
.70 O
38.2 O
10 O
.69 O
37.6 O
T O
able O
7 O
: O
Overview O
of O
prompts O
in O
ASAP O
- O
ZH O
ID O
IAA O
avg O
. O
Distribution O
( O
QWK O
) O
Length O
1 O
.96 O
68 O
2 O
.94 O
94 O
10 O
.91 O
61 O
T O
able O
8 O
: O
Overview O
of O
prompts O
in O
ASAP O
- O
ZHMT O
nents‚Äù8can O
consist O
of O
more O
than O
one O
component O
, O
therefore O
the O
radicals O
are O
not O
a O
proper O
subset O
of O
the O
components O
extracted O
above O
. O
Stroke O
F O
eatures O
W O
e O
use O
the O
cnchar O
tool O
to O
represent O
each O
answer O
as O
a O
sequence O
of O
individual O
strokes O
, O
following O
the O
stroke O
order O
for O
each O
character O
. O
Although O
we O
show O
the O
strokes O
in O
their O
original O
shapes O
in O
T O
able O
9 O
, O
a O
letter O
encoding O
is O
used O
in O
the O
experiment O
for O
an O
efficient O
processing O
. O
Auxiliary O
W O
ords O
Removal O
Based O
on O
the O
knowledge O
database O
released O
by O
Han O
et O
al O
. O
( O
2011 O
) O
, O
which O
contains O
45 O
common O
auxiliary O
words O
in O
modern O
Chinese O
, O
we O
remove O
all O
these O
grammatical O
morphemes O
on O
token O
level O
to O
reduce O
the O
influence O
of O
expression O
variance O
. O
In O
our O
example O
shown O
in O
T O
able O
9 O
, O
the O
possessive O
8http://www.moe.gov.cn/s78/A19/yxs_left/moe O
_ O
810 O
/ O
s230/201001 O
/ O
t20100115_75694.htmlAnswerdiamond O
·âá‡µÜ O
‚Äôs O
÷•hardness O
·Çóÿágreat O
’∂ O
T O
okens O
·âá‡µÜƒë÷•ƒë·Çóÿáƒë’∂ O
PinyinZuan O
, O
Shi O
, O
De O
, O
Ying O
, O
Du O
, O
Da O
Characters O
·âáƒë‡µÜƒë÷•ƒë·Çóƒëÿáƒë’∂ O
Components‡£Å·Öùƒë·ÅÇ·â•‡•ßƒëœ¢‡¥êƒë O
‡µÜ€∑ƒë‹º·âõ·Çªƒë‡≤¶·ÅÇ O
Radicals O
·™éƒë‡µÜƒëœ¢ƒë‡µÜƒë‹ºƒë’∂ O
Strokes O
T O
able O
9 O
: O
Different O
segmentation O
levels O
for O
an O
answer O
in O
CESA O
, O
prompt O
1 O
. O
markerms O
÷• O
is O
eliminated O
. O
6 O
Results O
and O
Discussion O
T O
able O
10 O
shows O
the O
performance O
of O
the O
different O
system O
configurations O
for O
the O
individual O
data O
sets O
, O
per O
prompt O
as O
well O
as O
averaged O
over O
all O
prompts O
from O
the O
same O
data O
set O
. O
First O
, O
we O
see O
that O
all O
feature O
sets O
were O
able O
to O
learn O
something O
meaningful O
from O
the O
training O
data O
. O
Although O
the O
performance O
of O
different O
feature O
sets O
is O
quite O
close O
to O
each O
other O
, O
we O
see O
a O
slight O
but O
significant O
advantage O
across O
data O
sets O
of O
component O
and O
character O
features O
over O
the O
token O
baseline O
. O
In O
order O
to O
check O
if O
tokenization O
caused354Data O
Set O
CESA O
ASAP O
- O
ZH O
ASAP O
- O
ZHMT O
Prompt O
1 O
2 O
3 O
4 O
5 O
avg O
. O
1 O
2 O
10 O
avg O
. O
1 O
2 O
10 O
avg O
. O
T O
oken O
.91 O
.84 O
.59 O
.66 O
.48 O
.70 O
.54 O
.40 O
.50 O
.48 O
.66 O
.59 O
.63 O
.63 O
Pinyin-.02+.03-.03+.01-.03+.01**+.13+.01+.04+.09**-.02+.01+.01¬±0 O
Character-.01+.03 O
¬±0+.11+.05+.04**+.13+.03+.06+.07**¬±0+.04+.04+.02 O
* O
Component-.03+.03-.01+.10+.02+.02**+.17+.04+.08+.10**-.01 O
¬±0+.04+.01 O
* O
* O
Radical-.02+.02+.03+.07 O
¬±0+.02**+.08+.08+.02+.06**+.02-.02+.04+.01 O
Stroke-.01-.02-.02+.06-.04-.01**+.14+.07+.04+.08**-.01-.02-.03-.02 O
* O
* O
- O
Auxiliary O
¬±0¬±0+.03+.02+.01+.01**-.01 O
¬±0-.01-.01**-.01-.01+.01-.01 O
* O
* O
* O
* O
p O
< O
0.01,*p O
< O
0.05 O
T O
able O
10 O
: O
Classification O
results O
on O
different O
feature O
sets O
in O
QWK O
values O
. O
problems O
in O
scoring O
, O
we O
manually O
inspected O
100 O
answers O
from O
prompt O
1 O
and O
4 O
in O
CESA O
. O
However O
, O
we O
found O
that O
tokenization O
was O
only O
erroneous O
in O
12 O
cases O
. O
Surprisingly O
, O
most O
of O
them O
occurred O
in O
prompt O
1 O
, O
where O
the O
token O
baseline O
even O
outperformed O
the O
character O
features O
and O
not O
in O
prompt O
4 O
, O
where O
character O
features O
performed O
better O
. O
W O
e O
also O
had O
a O
closer O
look O
at O
a O
number O
of O
student O
answers O
which O
are O
assigned O
a O
wrong O
score O
by O
the O
token O
baseline O
model O
but O
not O
by O
models O
with O
more O
fine O
- O
grained O
features O
. O
7 O
out O
of O
18 O
instances O
contain O
indeed O
variants O
of O
more O
frequent O
words O
in O
the O
data O
set O
. O
F O
or O
example O
, O
human O
‡≤¶‡´å O
andhuman O
‡≤¶ O
are O
less O
- O
frequently O
seen O
variants O
ofhuman O
‡≤¶‡ßã O
, O
all O
of O
which O
are O
indicators O
of O
a O
correct O
answer O
. O
This O
supports O
the O
assumption O
that O
, O
like O
in O
English O
, O
character O
- O
level O
features O
can O
capture O
variance O
in O
learner O
answers O
, O
in O
this O
case O
by O
handling O
variance O
in O
lexical O
choice O
. O
The O
usage O
of O
Pinyin O
did O
not O
bring O
the O
expected O
benefit O
, O
possibly O
because O
the O
amount O
of O
spelling O
errors O
is O
not O
substantial O
enough O
in O
the O
data O
. O
Similarly O
, O
removing O
auxiliary O
words O
appears O
to O
have O
little O
influence O
on O
scoring O
performance O
. O
7 O
Summary O
& O
F O
uture O
W O
ork O
In O
this O
paper O
, O
we O
discussed O
the O
main O
challenges O
in O
Chinese O
content O
scoring O
in O
comparison O
with O
English O
, O
namely O
segmentation O
and O
a O
different O
form O
of O
linguistic O
variance O
. O
W O
e O
reviewed O
related O
work O
in O
Chinese O
content O
scoring O
and O
saw O
a O
need O
for O
open O
- O
access O
scoring O
data O
sets O
in O
Chinese O
. O
Therefore O
, O
we O
collected O
two O
new O
datasets O
, O
CESA O
and O
ASAP O
- O
ZH O
, O
and O
release O
them O
for O
research O
in O
the O
future O
. O
While O
previous O
work O
has O
been O
limited O
to O
word O
- O
level O
features O
, O
we O
conducted O
a O
comparison O
of O
features O
on O
different O
segmentation O
levels O
. O
Although O
the O
difference O
between O
feature O
sets O
was O
in O
general O
small O
, O
we O
found O
that O
some O
answers O
with O
unusual O
expressions O
have O
a O
tendency O
to O
be O
better O
scored O
with O
models O
trained O
on O
lower O
level O
features O
, O
such O
as O
character O
ngrams O
. O
In O
the O
future O
, O
we O
will O
extend O
our O
comparison O
of O
segmentation O
levels O
also O
to O
a O
deep O
learning O
setting O
, O
using O
embeddings O
of O
different O
granularity O
( O
Yin O
et O
al O
. O
, O
2016 O
) O
. O
Abstract O
This O
paper O
presents O
a O
new O
dataset O
, O
B O
- O
SHARP O
, O
that O
can O
be O
used O
to O
develop O
NLP O
models O
for O
the O
detection O
of O
Mild O
Cognitive O
Impairment O
( O
MCI O
) O
known O
as O
an O
early O
sign O
of O
Alzheimer O
‚Äôs O
disease O
. O
Our O
dataset O
contains O
1 O
- O
2 O
min O
speech O
segments O
from O
326 O
human O
subjects O
for O
3 O
topics O
, O
( O
1 O
) O
daily O
activity O
, O
( O
2 O
) O
room O
environment O
, O
and O
( O
3 O
) O
picture O
description O
, O
and O
their O
transcripts O
so O
that O
a O
total O
of O
650 O
speech O
segments O
are O
collected O
. O
Given O
theB O
- O
SHARP O
dataset O
, O
several O
hierarchical O
text O
classiÔ¨Åcation O
models O
are O
developed O
that O
jointly O
learn O
combinatory O
features O
across O
all O
3 O
topics O
. O
The O
best O
performance O
of O
74.1 O
% O
is O
achieved O
by O
an O
ensemble O
model O
that O
adapts O
3 O
types O
of O
transformer O
encoders O
. O
To O
the O
best O
of O
our O
knowledge O
, O
this O
is O
the O
Ô¨Årst O
work O
that O
builds O
deep O
learningbased O
text O
classiÔ¨Åcation O
models O
on O
multiple O
contents O
for O
the O
detection O
of O
MCI O
. O
1 O
Introduction O
Alzheimer O
‚Äôs O
Disease O
( O
AD O
) O
is O
a O
progressive O
neurodegenerative O
disorder O
that O
is O
associated O
with O
memory O
loss O
and O
declines O
in O
major O
brain O
functions O
including O
semantic O
and O
pragmatic O
levels O
of O
language O
processing O
( O
Vestal O
et O
al O
. O
, O
2006 O
; O
Ferris O
and O
Farlow O
, O
2013 O
) O
. O
Traditional O
cognitive O
assessments O
such O
as O
positron O
emission O
tomography O
or O
cerebrospinal O
Ô¨Çuid O
analysis O
are O
expensive O
and O
time O
- O
consuming O
( O
Fyffe O
et O
al O
. O
, O
2011 O
) O
. O
This O
may O
cause O
delay O
in O
treating O
AD O
, O
known O
to O
be O
irreversible O
and O
incurable O
( O
Korczyn O
, O
2012 O
) O
, O
and O
put O
an O
increasing O
pressure O
on O
public O
health O
, O
especially O
for O
seniors O
whose O
life O
expectancy O
is O
rapidly O
growing O
yet O
are O
more O
likely O
to O
develop O
AD O
. O
Thus O
, O
it O
is O
crucial O
to O
Ô¨Ånd O
a O
more O
intelligent O
way O
of O
detecting O
AD O
in O
the O
earliest O
stage O
possible O
( O
Karr O
et O
al O
. O
, O
2018 O
) O
. O
Mild O
Cognitive O
Impairment O
( O
MCI O
) O
is O
considered O
the O
Ô¨Årst O
phase O
that O
patients O
start O
having O
biomarker O
evidence O
of O
brain O
changes O
that O
can O
eventually O
lead O
to O
AD O
( O
Albert O
et O
al O
. O
, O
2011 O
) O
. O
MCI O
involves O
subtle O
language O
changes O
from O
impairment O
in O
reasoningthat O
may O
not O
be O
noticeable O
to O
people O
other O
than O
friends O
and O
relatives O
. O
Because O
of O
this O
, O
the O
detection O
of O
MCI O
is O
a O
much O
more O
challenging O
task O
than O
detecting O
dementia O
( O
Suzman O
and O
Beard O
, O
2011 O
) O
. O
Recent O
studies O
in O
NLP O
have O
shown O
that O
it O
is O
possible O
to O
detect O
early O
stages O
of O
AD O
by O
analyzing O
patients O
‚Äô O
language O
patterns O
; O
however O
, O
most O
previous O
works O
have O
focused O
on O
the O
detection O
of O
dementia O
instead O
and O
researches O
tackling O
the O
detection O
of O
MCI O
have O
been O
based O
on O
relatively O
small O
datasets O
( O
Section O
2 O
) O
. O
This O
paper O
presents O
a O
new O
dataset O
that O
comprises O
three O
types O
of O
speech O
segments O
from O
both O
normal O
controls O
and O
MCI O
patients O
( O
Section O
3 O
) O
. O
Then O
, O
a O
hierarchical O
text O
classiÔ¨Åcation O
model O
is O
proposed O
, O
which O
jointly O
learns O
features O
from O
all O
three O
types O
of O
speech O
segments O
to O
determine O
whether O
or O
not O
each O
subject O
has O
MCI O
( O
Section O
4 O
) O
. O
Individual O
and O
ensemble O
models O
using O
three O
types O
of O
transformer O
encoders O
are O
evaluated O
on O
our O
dataset O
and O
show O
that O
different O
transformer O
encoders O
reveal O
strengths O
in O
distinct O
types O
of O
speeches O
( O
Section O
5 O
) O
. O
We O
believe O
that O
this O
work O
takes O
the O
initiative O
of O
deep O
learningbased O
NLP O
for O
detecting O
MCI O
that O
will O
be O
broadly O
beneÔ¨Åcial O
to O
global O
public O
health O
. O
2 O
Related O
Work O
Only O
few O
studies O
have O
tackled O
the O
detection O
of O
MCI O
using O
NLP.1Asgari O
et O
al O
. O
( O
2017 O
) O
conducted O
interviews O
with O
( O
27 O
C O
, O
14 O
M O
) O
, O
and O
developed O
SVM O
and O
random O
forest O
models O
on O
their O
transcribed O
speeches O
. O
Beltrami O
et O
al O
. O
( O
2018 O
) O
conducted O
three O
speech O
tasks O
with O
( O
48 O
C O
, O
32 O
M O
, O
16 O
D O
) O
, O
and O
analyzed O
phonetic O
and O
linguistic O
features O
of O
their O
speeches O
and O
transcripts O
. O
Fraser O
et O
al O
. O
( O
2019 O
) O
conducted O
3 O
language O
tasks O
with O
( O
29C O
, O
26 O
M O
) O
, O
and O
built O
a O
cascade O
model O
to O
learn O
multimodal O
features O
such O
as O
audio O
, O
text O
, O
eye O
- O
tracking O
. O
Gosztolya O
et O
al O
. O
( O
2019 O
) O
conducted O
question O
answer1#C O
: O
the O
number O
of O
normal O
controls O
, O
# O
M O
/ O
D O
/ O
A O
: O
the O
number O
of O
MCI O
/ O
Dementia O
/ O
AD O
patients.358Tokens O
Sentences O
Nouns O
Verbs O
Conjuncts O
Complex O
Discourse O
Q1Control O
186.6 O
( O
¬±60.4 O
) O
10.4 O
( O
¬±4.5 O
) O
28.1 O
( O
¬±9.6 O
) O
30.4 O
( O
¬±11.5 O
) O
8.5 O
( O
¬±4.5 O
) O
2.3 O
( O
¬±1.7 O
) O
8.1 O
( O
¬±5.4 O
) O
MCI O
175.6 O
( O
¬±54.5 O
) O
9.8 O
( O
¬±4.1 O
) O
23.7 O
( O
¬±8.3 O
) O
29.3 O
( O
¬±10.4 O
) O
8.5 O
( O
¬±4.2 O
) O
2.0 O
( O
¬±1.6 O
) O
9.2 O
( O
¬±6.0 O
) O
Q2Control O
191.5 O
( O
¬±11.8 O
) O
11.7 O
( O
¬±4.7 O
) O
41.1 O
( O
¬±13.3 O
) O
24.3 O
( O
¬±11.2 O
) O
6.6 O
( O
¬±4.5 O
) O
3.6 O
( O
¬±2.7 O
) O
7.1 O
( O
¬±4.8 O
) O
MCI O
178.6 O
( O
¬±11.7 O
) O
11.6 O
( O
¬±4.7 O
) O
36.7 O
( O
¬±12.1 O
) O
23.2 O
( O
¬±10.6 O
) O
6.4 O
( O
¬±4.4 O
) O
2.9 O
( O
¬±2.3 O
) O
8.4 O
( O
¬±5.3 O
) O
Q3Control O
193.4 O
( O
¬±63.4 O
) O
12.6 O
( O
¬±5.4 O
) O
39.5 O
( O
¬±13.5 O
) O
28.4 O
( O
¬±10.1 O
) O
8.0 O
( O
¬±4.8 O
) O
3.3 O
( O
¬±2.1 O
) O
6.1 O
( O
¬±5.5 O
) O
MCI O
187.8 O
( O
¬±63.4 O
) O
12.7 O
( O
¬±5.1 O
) O
36.2 O
( O
¬±13.2 O
) O
27.7 O
( O
¬±10.9 O
) O
7.2 O
( O
¬±4.2 O
) O
2.6 O
( O
¬±2.0 O
) O
7.3 O
( O
¬±5.5 O
) O
AllControl O
578.1 O
( O
¬±149.8 O
) O
34.5 O
( O
¬±10.7 O
) O
110.5 O
( O
¬±27.9 O
) O
84.2 O
( O
¬±25.4 O
) O
23.5 O
( O
¬±10.1 O
) O
9.3(¬±4.5 O
) O
21.4 O
( O
¬±13.0 O
) O
MCI O
548.7 O
( O
¬±140.6 O
) O
34.0 O
( O
¬±10.5 O
) O
98.1 O
( O
¬±26.1 O
) O
81.2 O
( O
¬±24.1 O
) O
22.5 O
( O
¬±9.7 O
) O
7.7 O
( O
¬±4.2 O
) O
25.3 O
( O
¬±15.0 O
) O
p O
0.0110 O
0.5541 O
< O
0.0001 O
0.1277 O
0 O
.2046 O
< O
0.0001 O
0 O
.0006 O
Table O
1 O
: O
Average O
counts O
and O
their O
standard O
deviations O
of O
linguistic O
features O
per O
transcript O
in O
the O
B O
- O
SHARP O
dataset O
. O
Complex O
: O
occurrences O
of O
complex O
structures O
( O
e.g. O
, O
relative O
clauses O
, O
non-Ô¨Ånite O
clauses O
) O
, O
Discourse O
: O
occurrences O
of O
discourse O
elements O
( O
e.g. O
, O
interjections O
, O
disÔ¨Çuency O
) O
. O
ing O
sessions O
with O
( O
25 O
C O
, O
25 O
M O
, O
25 O
A O
) O
, O
and O
trained O
a O
SVM O
model O
using O
acoustic O
and O
linguistic O
features O
. O
All O
of O
the O
previous O
works O
were O
based O
on O
fewer O
than O
100 O
subjects O
using O
traditional O
linguistic O
features O
to O
develop O
NLP O
models O
, O
compared O
to O
our O
work O
that O
is O
based O
on O
326 O
subjects O
and O
650 O
recordings O
using O
the O
latest O
transformer O
- O
based O
deep O
neural O
models O
. O
The O
task O
of O
dementia O
detection O
has O
been O
more O
explored O
by O
the O
NLP O
community O
. O
Becker O
et O
al O
. O
( O
1994 O
) O
presented O
the O
DementiaBank O
, O
that O
consists O
of O
552 O
audio O
recordings O
describing O
the O
picture O
called O
‚Äú O
The O
Boston O
Cookie O
Theft O
‚Äù O
from O
99 O
normal O
controls O
and O
194 O
dementia O
patients O
, O
that O
have O
been O
used O
by O
the O
following O
works O
. O
Orimaye O
et O
al O
. O
( O
2016 O
) O
presented O
deep O
- O
deep O
neural O
network O
language O
models O
using O
higher O
- O
order O
n O
- O
grams O
and O
skip O
- O
grams O
. O
Pou O
- O
Prom O
and O
Rudzicz O
( O
2018 O
) O
leveraged O
linguistic O
features O
and O
multiview O
embeddings O
by O
applying O
generalized O
canonical O
correlation O
analysis O
. O
Karleka O
et O
al O
. O
( O
2018 O
) O
proposed O
a O
model O
based O
on O
convolutional O
and O
recurrent O
neural O
networks O
and O
gave O
interpretations O
of O
this O
model O
to O
explain O
linguistic O
characteristics O
for O
detecting O
dementia O
. O
Our O
work O
is O
distinguished O
as O
: O
‚Ä¢We O
tackle O
the O
detection O
of O
MCI O
, O
not O
dementia O
, O
‚Ä¢Our O
documents O
are O
multi O
- O
contents O
compared O
to O
single O
- O
content O
documents O
in O
the O
DementiaBank O
. O
‚Ä¢Our O
approach O
is O
based O
on O
the O
latest O
contextualized O
embeddings O
compared O
to O
the O
distributional O
embeddings O
adapted O
by O
the O
previous O
works O
. O
3 O
Dataset O
3.1 O
B O
- O
SHARP O
Our O
work O
is O
based O
on O
data O
collected O
as O
part O
of O
the O
Brain O
, O
Stress O
, O
Hypertension O
, O
and O
AgingResearch O
Program O
( O
B O
- O
SHARP O
) O
.2In O
this O
dataset O
, O
185 O
normal O
2B O
- O
SHARP O
: O
http://medicine.emory.edu/bsharpcontrols O
and O
141 O
MCI O
patients O
are O
selected O
based O
on O
neuropsychological O
and O
clinical O
assessments O
. O
Every O
subject O
has O
been O
examined O
with O
multiple O
cognitive O
tests O
including O
the O
Montreal O
Cognitive O
Assessment O
( O
MoCA O
; O
Nasreddine O
et O
al O
. O
2005 O
) O
and O
the O
Boston O
Naming O
Test O
( O
BNT O
; O
Kaplan O
et O
al O
. O
1983 O
) O
, O
followed O
by O
a O
speech O
task O
protocol O
for O
recording O
. O
51.5 O
% O
and O
23.9 O
% O
of O
the O
subjects O
have O
so O
far O
come O
back O
for O
their O
2nd O
and O
3rd O
visits O
to O
take O
new O
voice O
recordings O
, O
respectively O
. O
B O
- O
SHARP O
is O
an O
ongoing O
program O
; O
recordings O
of O
20 O
- O
25 O
subjects O
are O
taken O
every O
month O
; O
thus O
, O
the O
data O
is O
still O
growing O
. O
Sbj O
2nd O
3rd O
Rec O
MoCA O
BNT O
C O
185 O
100 O
50 O
385 O
26.2 O
( O
¬±2.6 O
) O
14.2 O
( O
¬±1.2 O
) O
M O
141 O
68 O
28 O
265 O
21.5 O
( O
¬±3.5 O
) O
13.4 O
( O
¬±1.5 O
) O
Œ£ O
326 O
168 O
78 O
650 O
24.2 O
( O
¬±3.8 O
) O
13.9 O
( O
¬±1.4 O
) O
Table O
2 O
: O
Statistics O
of O
control O
( O
C O
) O
and O
MCI O
( O
M O
) O
groups O
. O
Sbj O
: O
# O
of O
subjects O
, O
2nd/3rd O
: O
# O
of O
subjects O
who O
made O
the O
2nd/3rd O
visits O
, O
Rec O
: O
# O
of O
voice O
recordings O
, O
MoCA O
/ O
BNT O
: O
average O
scores O
and O
stdevs O
from O
MoCA O
/ O
BNT O
. O
Note O
that O
subjects O
with O
the O
2nd/3rd O
visits O
take O
one O
/ O
two O
additional O
recordings O
; O
thus O
, O
Rec O
= O
Sbj O
+ O
1 O
¬∑ O
( O
2nd O
) O
+ O
2¬∑(3rd O
) O
. O
Table O
2 O
shows O
the O
statistics O
of O
the O
control O
and O
the O
MCI O
groups O
in O
B O
- O
SHARP O
. O
Note O
that O
when O
subjects O
make O
multiple O
visits O
, O
there O
is O
a O
year O
gap O
in O
between O
so O
that O
subjects O
generally O
do O
not O
remember O
so O
much O
from O
their O
previous O
visits O
. O
Thus O
, O
speeches O
from O
the O
same O
subject O
are O
not O
necessarily O
more O
similar O
than O
ones O
from O
the O
other O
subjects O
. O
In O
fact O
, O
most O
speeches O
across O
subjects O
, O
regardless O
of O
their O
groups O
, O
are O
very O
similar O
when O
they O
are O
transcribed O
since O
all O
subjects O
follow O
the O
same O
speech O
protocol O
in O
Section O
3.2.3 O
3.2 O
Speech O
Task O
Protocol O
A O
speech O
task O
protocol O
has O
been O
conducted O
to O
collect O
recordings O
of O
both O
control O
and O
MCI O
subjects O
3A.3 O
compares O
B O
- O
SHARP O
with O
the O
DementiaBank O
in O
details.359Transformer1 O
( O
T1)w11w12 O
/ O
uni22EFw1n[CLS1]w21w22 O
/ O
uni22EFw2n[CLS2]w31w32 O
/ O
uni22EFw3n[CLS3 O
] O
c O
1 O
e O
11 O
e O
12 O
/ O
uni22EF O
e O
1 O
n O
c O
2 O
e O
21 O
e O
22 O
/ O
uni22EF O
e O
2 O
n O
c O
3 O
e O
31 O
e O
32 O
/ O
uni22EF O
e O
3 O
n O
c O
1 O
c O
2 O
c O
3MLP1MLP2MLP3 O
/ O
uni2295 O
/ O
uni2295o2o1o3MLPeoe O
Transformer2 O
( O
T2)Transformer3 O
( O
T3)Figure O
1 O
: O
Overview O
of O
hierarchical O
transformer O
to O
combine O
content O
features O
from O
the O
three O
types O
of O
speech O
tasks O
. O
who O
are O
asked O
to O
speak O
about O
Q1 O
: O
daily O
activity O
, O
Q2 O
: O
room O
environment O
, O
and O
Q3 O
: O
picture O
description O
for O
1 O
- O
2 O
minutes O
each O
. O
All O
subjects O
are O
provided O
with O
the O
same O
instructions O
in O
A.2 O
, O
and O
visual O
abilities O
of O
the O
subjects O
are O
conÔ¨Årmed O
before O
recording O
. O
To O
reduce O
potential O
variance O
, O
the O
subjects O
are O
guided O
to O
follow O
similar O
activities O
before O
Q1 O
, O
located O
to O
similar O
room O
settings O
before O
Q2 O
, O
and O
shown O
the O
same O
picture O
in O
Fig O
2 O
, O
‚Äú O
The O
Circus O
Procession O
‚Äù O
, O
for O
Q3 O
. O
The O
collected O
voice O
recordings O
are O
automatically O
transcribed O
by O
the O
online O
tool O
called O
Temi.4Table O
1 O
shows O
linguistic O
features O
about O
our O
dataset O
analyzed O
by O
the O
open O
- O
source O
NLP O
toolkit O
, O
ELIT.5Transcripts O
from O
the O
control O
group O
depict O
signiÔ¨Åcantly O
higher O
numbers O
of O
tokens O
, O
nouns O
, O
and O
complex O
structures O
while O
transcripts O
from O
the O
MCI O
group O
gives O
signiÔ¨Åcantly O
more O
discourse O
elements O
, O
implying O
that O
the O
control O
subjects O
are O
more O
expressive O
while O
the O
MCI O
subjects O
include O
more O
disÔ¨Çuency O
in O
their O
speeches O
. O
4 O
Hierarchical O
Transformer O
Although O
transformer O
encoders O
have O
recently O
established O
the O
state O
- O
of O
- O
the O
- O
art O
results O
on O
most O
document O
classiÔ¨Åcation O
tasks O
, O
they O
have O
a O
limit O
on O
the O
input O
size O
. O
As O
in O
Table O
1 O
, O
the O
average O
number O
of O
tokens O
in O
our O
input O
documents O
well O
- O
exceeds O
512 O
when O
combining O
transcripts O
from O
all O
three O
tasks O
, O
which O
is O
the O
max O
- O
number O
of O
tokens O
that O
the O
pretrained O
models O
of O
these O
transformers O
allow O
in O
general O
. O
This O
makes O
it O
difÔ¨Åcult O
to O
simply O
join O
all O
transcripts O
together O
and O
feed O
into O
a O
transformer O
encoder O
. O
Thus O
, O
this O
section O
presents O
a O
hierarchical O
transformer O
to O
overcome O
the O
challenge O
of O
long O
documents O
while O
jointly O
training O
transcript O
contents O
from O
all O
three O
tasks O
( O
Figure O
1 O
) O
. O
4Temi O
: O
https://www.temi.com O
5ELIT O
: O
https://github.com/elitcloud/elitLetWi={wi1 O
, O
. O
. O
. O
, O
w O
in}be O
a O
transcript O
, O
where O
wijrepresents O
the O
j‚Äôth O
token O
in O
the O
transcript O
produced O
by O
the O
i‚Äôth O
task O
Qi(in O
our O
case O
, O
i={1,2,3 O
} O
) O
. O
Wiis O
prepended O
by O
the O
special O
token O
[ O
CLS O
i]that O
is O
used O
to O
learn O
the O
transcript O
embedding O
, O
and O
fed O
into O
the O
transformer O
Ti O
. O
The O
transformer O
then O
generates O
Ei={ci O
, O
ei1 O
, O
. O
. O
. O
, O
e O
in O
} O
, O
where O
ciandeijare O
the O
embeddings O
for O
[ O
CLS O
i]andwij O
, O
respectively O
. O
ci‚ààRdis O
used O
to O
make O
two O
types O
of O
predictions O
. O
First O
, O
ciis O
fed O
into O
a O
multilayer O
perceptron O
layer O
, O
MLP O
i O
, O
that O
generates O
the O
output O
vector O
oi‚ààR2to O
predict O
whether O
or O
not O
the O
subject O
has O
MCI O
based O
on O
the O
transcript O
from O
Qialone O
. O
Second O
, O
the O
transcript O
embeddings O
from O
all O
three O
tasks O
are O
concatenated O
such O
that O
ce O
= O
c1‚äïc2‚äïc3‚ààR3d O
, O
which O
gets O
fed O
into O
another O
MLP O
eto O
generate O
the O
output O
vector O
oe‚ààR2 O
, O
and O
makes O
the O
binary O
decision O
based O
on O
the O
transcripts O
from O
all O
three O
tasks O
, O
Q1,Q2andQ3 O
. O
5 O
Experiments O
There O
are O
650 O
recordings O
in O
our O
dataset O
( O
Table O
2 O
) O
, O
that O
is O
rather O
small O
to O
divide O
into O
train O
, O
development O
, O
and O
test O
sets O
. O
Thus O
, O
5 O
- O
fold O
cross O
- O
validation O
( O
CV O
) O
is O
used O
to O
evaluate O
the O
performance O
of O
our O
models O
. O
Table O
5 O
shows O
the O
distributions O
of O
the O
Ô¨Åve O
CVsets O
for O
our O
experiments O
, O
where O
the O
transcript O
of O
each O
recording O
is O
treated O
as O
an O
independent O
document O
. O
Notice O
that O
the O
distributions O
are O
calculated O
based O
on O
analysis O
of O
the O
last O
MLP O
layer O
instead O
of O
simple O
majority O
vote O
on O
individual O
models O
. O
It O
is O
worth O
mentioning O
that O
all O
recordings O
from O
the O
same O
subject O
given O
multiple O
visits O
are O
assigned O
to O
the O
same O
CVset O
; O
thus O
, O
there O
is O
no O
overlap O
in O
terms O
of O
subjects O
across O
these O
CVsets O
. O
This O
allows O
us O
to O
avoid O
potential O
inÔ¨Çation O
in O
accuracy O
due O
to O
unique O
language O
patterns O
used O
by O
individual O
subjects.360BERT O
RoBERTa O
ALBERT O
Q1 O
Q2 O
Q3 O
Q1 O
Q2 O
Q3 O
Q1 O
Q2 O
Q3 O
ACC O
67.6 O
( O
¬±0.4 O
) O
69.0 O
( O
¬±1.2 O
) O
67.7 O
( O
¬±0.7 O
) O
69.0 O
( O
¬±1.5 O
) O
69.9 O
( O
¬±0.2 O
) O
65.2 O
( O
¬±0.3 O
) O
67.6 O
( O
¬±1.5 O
) O
69.5 O
( O
¬±0.3 O
) O
66.6 O
( O
¬±1.3 O
) O
SEN O
48.9 O
( O
¬±1.8 O
) O
57.1 O
( O
¬±2.5 O
) O
41.5 O
( O
¬±3.6 O
) O
44.3 O
( O
¬±4.5 O
) O
55.3 O
( O
¬±1.2 O
) O
37.1 O
( O
¬±3.7 O
) O
45.9 O
( O
¬±1.9 O
) O
52.2 O
( O
¬±0.6 O
) O
37.4 O
( O
¬±3.3 O
) O
SPE O
80.4 O
( O
¬±1.2 O
) O
77.3 O
( O
¬±2.8 O
) O
85.2 O
( O
¬±3.0 O
) O
85.8 O
( O
¬±2.1 O
) O
79.7 O
( O
¬±0.7 O
) O
84.5 O
( O
¬±3.0 O
) O
82.6 O
( O
¬±3.7 O
) O
81.4 O
( O
¬±0.3 O
) O
86.8 O
( O
¬±3.3 O
) O
Table O
3 O
: O
Model O
performance O
on O
the O
individual O
tasks O
. O
ACC O
: O
accuracy O
, O
SEN O
: O
sensitivity O
, O
SPE O
: O
speciÔ¨Åcity O
. O
CNN O
BERT O
e O
RoBERTa O
e O
ALBERT O
e O
Be+ O
Re O
Ae+ O
Re O
Be+ O
Ae+ O
Re O
ACC O
69.5 O
( O
¬±0.2 O
) O
69.9 O
( O
¬±1.1 O
) O
71.6 O
( O
¬±1.5 O
) O
69.7 O
( O
¬±2.9 O
) O
72.2 O
( O
¬±0.7 O
) O
71.5 O
( O
¬±1.9 O
) O
74.1 O
( O
¬±0.3 O
) O
SEN O
49.2 O
( O
¬±0.8 O
) O
57.6 O
( O
¬±3.4 O
) O
48.5 O
( O
¬±6.1 O
) O
46.2 O
( O
¬±8.3 O
) O
56.5 O
( O
¬±2.5 O
) O
51.7 O
( O
¬±1.3 O
) O
60.9 O
( O
¬±5.2 O
) O
SPE O
83.5 O
( O
¬±0.9 O
) O
77.4 O
( O
¬±4.8 O
) O
87.5 O
( O
¬±1.8 O
) O
85.4 O
( O
¬±0.5 O
) O
83.1 O
( O
¬±0.9 O
) O
86.7 O
( O
¬±3.4 O
) O
84.0 O
( O
¬±2.4 O
) O
Table O
4 O
: O
Performance O
of O
ensemble O
models O
. O
Bert O
e O
/ O
RoBERTa O
e O
/ O
ALBERT O
euse O
transcript O
embeddings O
from O
all O
3 O
tasks O
trained O
by O
the O
BERT O
/ O
RoBERTa O
/ O
ALBERT O
models O
in O
Table O
3 O
, O
respectively O
. O
B O
e+Reuses O
transcript O
embeddings O
from O
both O
Bert O
eand O
RoBERTa O
e(so O
the O
total O
of O
6 O
embeddings O
) O
, O
A O
e+Reuses O
transcript O
embeddings O
from O
both O
ALBERT O
e O
and O
RoBERTa O
e(6 O
embeddings O
) O
, O
and O
B O
e+Ae+Reuses O
transcript O
embeddings O
from O
all O
three O
models O
( O
9 O
embeddings O
) O
. O
Three O
transformer O
encoders O
are O
used O
, O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
RoBERTa O
( O
Liu O
et O
al O
. O
, O
2020 O
) O
, O
and O
ALBERT O
( O
Lan O
et O
al O
. O
, O
2019 O
) O
for O
our O
experiments O
. O
Every O
model O
is O
trained O
3 O
times O
and O
its O
average O
performance O
with O
the O
standard O
deviation O
are O
reported.6 O
CV0CV1CV2CV3CV4ALL O
CRec O
77 O
77 O
77 O
77 O
77 O
385 O
MRec O
53 O
53 O
53 O
53 O
53 O
265 O
CSbj O
37 O
37 O
37 O
37 O
37 O
185 O
MSbj O
27 O
28 O
28 O
29 O
29 O
141 O
Table O
5 O
: O
Statistics O
of O
the O
CVsets O
for O
our O
experiments O
. O
Rec O
/ O
Sbj O
: O
# O
of O
recordings O
/ O
subjects O
, O
C O
/ O
M O
: O
in O
control O
/ O
MCI O
group O
. O
CVi O
: O
thei‚Äôth O
set O
. O
ALL:/summationtext4 O
i=0CVi O
. O
5.1 O
Performance O
of O
Individual O
Models O
Individual O
models O
are O
built O
by O
training O
transcripts O
from O
each O
task O
separately O
using O
MLP O
iin O
Section O
4 O
. O
Table O
3 O
shows O
the O
performance O
of O
the O
3 O
transformer O
models O
on O
the O
individual O
tasks O
. O
The O
performance O
onQ2shows O
the O
highest O
accuracy O
for O
all O
three O
models O
, O
achieving O
69.9 O
% O
with O
RoBERTa O
, O
implying O
that O
the O
room O
environment O
task O
of O
Q2 O
, O
involving O
many O
spatial O
descriptions O
, O
are O
the O
most O
effective O
to O
distinguish O
the O
MCI O
group O
. O
The O
highest O
sensitivity O
of O
57.1 O
% O
is O
achieved O
by O
BERT O
on O
Q2 O
, O
and O
the O
highest O
speciÔ¨Åcity O
of O
86.8 O
% O
is O
achieved O
by O
ALBERT O
on O
Q3 O
. O
Such O
a O
low O
sensitivity O
and O
a O
high O
speciÔ¨Åcity O
imply O
that O
it O
is O
easier O
to O
recognize O
the O
normal O
controls O
but O
not O
the O
MCI O
patients O
given O
the O
short O
speeches O
. O
5.2 O
Performance O
of O
Ensemble O
Models O
Ensemble O
models O
are O
developed O
by O
jointly O
training O
multiple O
transcript O
embeddings O
from O
the O
individual O
models O
using O
MLP O
ein O
Section O
4 O
. O
Table O
4 O
shows O
the O
6Details O
about O
the O
experimental O
settings O
are O
provided O
in O
A.1.model O
performance O
of O
the O
ensemble O
models O
. O
Additionally O
, O
results O
from O
a O
model O
that O
takes O
transcripts O
from O
the O
3 O
tasks O
as O
one O
input O
document O
and O
trains O
a O
convolutional O
neural O
network O
( O
CNN O
) O
are O
provided O
for O
comparison O
to O
Karleka O
et O
al O
. O
( O
2018).7Reshows O
1.7 O
% O
improvement O
on O
accuracy O
over O
the O
RoBERTa O
model O
in O
Table O
3 O
although O
its O
sensitivity O
is O
worse O
. O
Table O
6 O
shows O
the O
voting O
distributions O
of O
each O
task O
combination O
; O
given O
the O
samples O
correctly O
predicted O
by O
RoBERTa O
e O
, O
we O
count O
how O
often O
the O
individual O
models O
are O
correct O
for O
those O
samples O
by O
comparing O
the O
weights O
in O
MLP O
eand O
estimate O
the O
percentages O
. O
The O
combination O
of O
( O
Q1,Q3 O
) O
shows O
the O
highest O
percentage O
of O
30 O
% O
, O
meaning O
that O
30 O
% O
of O
the O
corrected O
predicted O
samples O
are O
voted O
by O
both O
Q1andQ3 O
. O
Q1Q2Q3Q1,2Q1,3Q2,3Q1,2,3 O
5.8 O
6.4 O
2.8 O
19.5 O
30.0 O
8.8 O
26.1 O
Table O
6 O
: O
V O
oting O
distributions O
of O
each O
task O
combination O
for O
RoBERTa O
e. O
Qi O
: O
% O
of O
only O
the O
Qimodel O
is O
correct O
, O
Qi O
, O
i O
, O
j O
: O
% O
of O
all O
Qi O
, O
Qi O
, O
andQjmodels O
are O
correct O
. O
A O
similar O
analysis O
is O
done O
for O
B O
e+Re+Aealthough O
displaying O
the O
distributions O
is O
quite O
infeasible O
since O
it O
involves O
29 O
- O
1 O
combinations O
. O
Among O
the O
samples O
correctly O
predicted O
by O
B O
e+Re+Ae O
, O
86 O
% O
are O
derived O
from O
majority O
votes O
; O
in O
other O
words O
, O
at O
least O
5 O
out O
of O
9 O
individual O
models O
agree O
with O
the O
predictions O
. O
V O
otes O
from O
6 O
and O
5 O
models O
are O
the O
largest O
groups O
, O
showing O
35 O
% O
and O
28 O
% O
, O
respectively O
. O
Only O
0.21 O
% O
are O
agreed O
by O
all O
9 O
models O
. O
No O
case O
of O
votes O
from O
3 O
or O
less O
models O
is O
found O
, O
implying O
that O
no O
individual O
model O
dominates O
the O
Ô¨Ånal O
decision O
of O
B O
e+Re+Ae O
. O
7We O
also O
experiemented O
with O
LSTM O
- O
RNN O
and O
CNN O
- O
LSTM O
models O
as O
suggested O
by O
Karleka O
et O
al O
. O
( O
2018 O
) O
; O
however O
, O
the O
CNN O
model O
gave O
the O
highest O
accuracy O
on O
our O
dataset.3616 O
Conclusion O
This O
paper O
presents O
the O
B O
- O
SHARP O
dataset O
, O
that O
is O
the O
largest O
dataset O
for O
the O
task O
of O
MCI O
detection O
feasible O
to O
develop O
robust O
deep O
neural O
models O
. O
Our O
best O
ensemble O
model O
using O
hierarchical O
transformer O
gives O
the O
accuracy O
of O
74 O
% O
to O
distinguish O
MCI O
patients O
from O
normal O
controls O
that O
is O
very O
promising O
. O
We O
will O
also O
explore O
models O
to O
make O
a O
longevity O
analysis O
per O
patient O
with O
this O
dataset.8 O
Abstract O
Predicting O
the O
quality O
of O
machine O
translation O
has O
traditionally O
been O
addressed O
with O
language O
- O
speciÔ¨Åc O
models O
, O
under O
the O
assumption O
that O
the O
quality O
label O
distribution O
or O
linguistic O
features O
exhibit O
traits O
that O
are O
not O
shared O
across O
languages O
. O
An O
obvious O
disadvantage O
of O
this O
approach O
is O
the O
need O
for O
labelled O
data O
for O
each O
given O
language O
pair O
. O
We O
challenge O
this O
assumption O
by O
exploring O
different O
approaches O
to O
multilingual O
Quality O
Estimation O
( O
QE O
) O
, O
including O
using O
scores O
from O
translation O
models O
. O
We O
show O
that O
these O
outperform O
singlelanguage O
models O
, O
particularly O
in O
less O
balanced O
quality O
label O
distributions O
and O
low O
- O
resource O
settings O
. O
In O
the O
extreme O
case O
of O
zero O
- O
shot O
QE O
, O
we O
show O
that O
it O
is O
possible O
to O
accurately O
predict O
quality O
for O
any O
given O
new O
language O
from O
models O
trained O
on O
other O
languages O
. O
Our O
Ô¨Åndings O
indicate O
that O
state O
- O
of O
- O
the O
- O
art O
neural O
QE O
models O
based O
on O
powerful O
pre O
- O
trained O
representations O
generalise O
well O
across O
languages O
, O
making O
them O
more O
applicable O
in O
real O
- O
world O
settings O
. O
1 O
Introduction O
Quality O
Estimation O
( O
QE O
) O
( O
Blatz O
et O
al O
. O
, O
2004a O
; O
Specia O
et O
al O
. O
, O
2009 O
) O
is O
the O
task O
of O
predicting O
the O
quality O
of O
an O
automatically O
generated O
translation O
at O
test O
time O
, O
when O
no O
reference O
translation O
is O
available O
for O
comparison O
. O
Instead O
of O
reference O
translations O
, O
QE O
turns O
to O
explicit O
quality O
indicators O
that O
are O
either O
provided O
by O
the O
Machine O
Translation O
( O
MT O
) O
system O
itself O
( O
the O
so O
- O
called O
glass O
- O
box O
features O
) O
or O
extracted O
from O
both O
the O
source O
and O
the O
target O
texts O
( O
the O
socalled O
black O
- O
box O
features O
) O
( O
Specia O
et O
al O
. O
, O
2018b O
) O
. O
In O
the O
current O
QE O
approaches O
, O
black O
- O
box O
features O
are O
learned O
representations O
extracted O
by O
Ô¨Ånetuning O
pre O
- O
trained O
multilingual O
or O
cross O
- O
lingual O
sentence O
encoders O
such O
as O
BERT O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
, O
‚àóEqual O
contribution O
. O
XLM O
- O
R O
( O
Conneau O
et O
al O
. O
, O
2019 O
) O
or O
LASER O
( O
Artetxe O
and O
Schwenk O
, O
2019 O
) O
. O
These O
supervised O
approaches O
have O
led O
to O
the O
state O
- O
of O
- O
the O
- O
art O
( O
SOTA O
) O
results O
in O
this O
task O
( O
Kepler O
et O
al O
. O
, O
2019 O
; O
Fonseca O
et O
al O
. O
, O
2019 O
) O
, O
similarly O
to O
what O
has O
been O
observed O
for O
a O
myriad O
of O
other O
downstream O
natural O
language O
processing O
applications O
that O
rely O
on O
cross O
- O
lingual O
sentence O
similarity O
. O
Glass O
- O
box O
features O
are O
usually O
obtained O
by O
extracting O
various O
types O
of O
information O
from O
the O
MT O
system O
, O
e.g. O
lexical O
probability O
or O
language O
model O
probability O
in O
the O
case O
of O
statistical O
MT O
systems O
( O
Blatz O
et O
al O
. O
, O
2004b O
) O
, O
or O
more O
recently O
softmax O
probability O
and O
attention O
weights O
from O
neural O
MT O
models O
( O
Fomicheva O
et O
al O
. O
, O
2020 O
) O
. O
Glass O
- O
box O
approach O
is O
potentially O
useful O
for O
low O
resource O
or O
zeroshot O
scenarios O
as O
it O
does O
not O
require O
large O
amounts O
of O
labelled O
data O
for O
training O
, O
but O
it O
does O
not O
perform O
as O
well O
as O
SOTA O
supervised O
models O
. O
QE O
is O
therefore O
generally O
framed O
as O
a O
supervised O
machine O
learning O
problem O
, O
with O
models O
trained O
on O
data O
labelled O
for O
quality O
for O
each O
language O
pair O
. O
Training O
data O
publicly O
available O
to O
build O
QE O
models O
is O
constrained O
to O
very O
few O
languages O
, O
which O
has O
made O
it O
difÔ¨Åcult O
to O
assess O
how O
well O
QE O
models O
generalise O
across O
languages O
. O
Therefore O
QE O
work O
to O
date O
has O
been O
addressed O
as O
a O
language O
- O
speciÔ¨Åc O
task O
. O
The O
recent O
availability O
of O
multilingual O
QE O
data O
in O
a O
diverse O
set O
of O
language O
pairs O
( O
see O
Section O
4.1 O
) O
has O
made O
it O
possible O
to O
explore O
the O
multilingual O
potential O
of O
the O
QE O
task O
and O
SOTA O
models O
. O
In O
this O
paper O
, O
we O
posit O
that O
it O
is O
possible O
and O
beneÔ¨Åcial O
to O
extend O
SOTA O
models O
to O
frame O
QE O
as O
a O
languageindependent O
task O
. O
We O
further O
explore O
the O
role O
of O
in O
- O
language O
supervision O
in O
comparison O
to O
supervision O
coming O
from O
other O
languages O
in O
a O
multi O
- O
task O
setting O
. O
Finally O
, O
we O
propose O
for O
the O
Ô¨Årst O
time O
to O
model O
QE O
as O
a O
zero O
- O
shot O
cross O
- O
lingual O
transfer O
task O
, O
enabling O
new O
avenues O
of O
research O
in O
which O
multilingual O
models366can O
be O
trained O
once O
and O
then O
serve O
a O
multitude O
of O
languages O
. O
The O
main O
contributions O
of O
this O
paper O
are O
: O
( O
i O
) O
we O
propose O
new O
multi O
- O
task O
learning O
approaches O
for O
multilingual O
QE O
( O
Section O
3 O
) O
; O
( O
ii O
) O
we O
show O
that O
multilingual O
system O
outperforms O
single O
language O
ones O
( O
Section O
5.1.1 O
) O
, O
especially O
in O
low O
- O
resource O
and O
less O
balanced O
label O
distribution O
settings O
( O
Section O
5.1.3 O
) O
, O
and O
‚Äì O
counter O
- O
intuitively O
‚Äì O
that O
sharing O
a O
source O
or O
target O
language O
with O
the O
test O
case O
does O
not O
prove O
beneÔ¨Åcial O
( O
Section O
5.1.2 O
) O
; O
and O
( O
iii O
) O
we O
study O
black O
- O
box O
and O
glass O
- O
box O
QE O
in O
a O
multilingual O
setting O
and O
show O
that O
zero O
- O
shot O
QE O
is O
possible O
for O
both O
( O
Section O
5.1.3 O
and O
5.2 O
) O
. O
2 O
Related O
Work O
QE O
Early O
QE O
models O
were O
trained O
upon O
a O
set O
of O
explicit O
features O
expressing O
either O
the O
conÔ¨Ådence O
of O
the O
MT O
system O
, O
the O
complexity O
of O
the O
source O
sentence O
, O
the O
Ô¨Çuency O
of O
the O
translation O
in O
the O
target O
language O
or O
its O
adequacy O
with O
regard O
to O
the O
source O
sentence O
( O
Specia O
et O
al O
. O
, O
2018b O
) O
. O
Current O
SOTA O
models O
are O
learnt O
with O
the O
use O
of O
neural O
networks O
( O
NN O
) O
( O
Specia O
et O
al O
. O
, O
2018a O
; O
Fonseca O
et O
al O
. O
, O
2019 O
) O
. O
The O
assumption O
is O
that O
representations O
learned O
can O
, O
to O
some O
extent O
, O
account O
for O
source O
complexity O
, O
target O
Ô¨Çuency O
and O
source O
- O
target O
adequacy O
. O
These O
are O
Ô¨Åne O
- O
tuned O
from O
pre O
- O
trained O
word O
representations O
extracted O
using O
multilingual O
or O
cross O
- O
lingual O
sentence O
encoders O
such O
as O
BERT O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
, O
XLM O
- O
R O
( O
Conneau O
et O
al O
. O
, O
2019 O
) O
or O
LASER O
( O
Artetxe O
and O
Schwenk O
, O
2019 O
) O
. O
Kim O
et O
al O
. O
( O
2017 O
) O
propose O
the O
Ô¨Årst O
breakthrough O
in O
neural O
- O
based O
QE O
with O
the O
Predictor O
- O
Estimator O
modular O
architecture O
. O
The O
Predictor O
model O
is O
an O
encoder O
- O
decoder O
Recurrent O
Neural O
Network O
( O
RNN O
) O
model O
trained O
on O
a O
huge O
amount O
of O
parallel O
data O
for O
a O
word O
prediction O
task O
. O
Its O
output O
is O
fed O
to O
the O
Estimator O
, O
a O
unidirectional O
RNN O
trained O
on O
QE O
data O
, O
to O
produce O
the O
quality O
estimates O
. O
Kepler O
et O
al O
. O
( O
2019 O
) O
use O
a O
similar O
architecture O
where O
the O
Predictor O
model O
is O
replaced O
by O
pretrained O
contextualised O
word O
representations O
such O
as O
BERT O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
or O
XLM O
- O
R O
( O
Conneau O
et O
al O
. O
, O
2019 O
) O
. O
Despite O
achieving O
strong O
performances O
, O
such O
models O
are O
resource O
heavy O
and O
need O
to O
be O
Ô¨Åne O
- O
tuned O
for O
each O
language O
- O
pair O
under O
consideration O
. O
In O
a O
very O
different O
approach O
, O
Fomicheva O
et O
al O
. O
( O
2020 O
) O
propose O
exploiting O
information O
provided O
by O
the O
NMT O
system O
itself O
. O
By O
exploring O
uncertainty O
quantiÔ¨Åcation O
methods O
, O
they O
show O
that O
theconÔ¨Ådence O
with O
which O
the O
NMT O
system O
produces O
its O
translation O
correlates O
well O
with O
its O
quality O
. O
Although O
not O
performing O
as O
well O
as O
SOTA O
supervised O
models O
, O
their O
approach O
has O
the O
main O
advantage O
to O
be O
unsupervised O
and O
not O
rely O
on O
labelled O
data O
. O
Multilinguality O
Multilinguality O
allows O
training O
a O
single O
model O
to O
perform O
a O
task O
from O
and O
to O
multiple O
languages O
. O
This O
principle O
has O
been O
successfully O
applied O
to O
NMT O
( O
Dong O
et O
al O
. O
, O
2015 O
; O
Firat O
et O
al O
. O
, O
2016b O
, O
a O
; O
Nguyen O
and O
Chiang O
, O
2017 O
) O
. O
Aharoni O
et O
al O
. O
( O
2019 O
) O
stretches O
this O
approach O
by O
translating O
up O
to O
102 O
languages O
from O
and O
to O
English O
using O
a O
Transformer O
model O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O
They O
show O
that O
multilingual O
many O
- O
to O
- O
many O
models O
are O
effective O
in O
low O
resource O
settings O
. O
Multilinguality O
also O
allows O
for O
zero O
- O
shot O
translation O
( O
Johnson O
et O
al O
. O
, O
2017 O
) O
. O
With O
a O
simple O
encoder O
- O
decoder O
architecture O
and O
without O
explicit O
bridging O
between O
source O
and O
target O
languages O
, O
they O
show O
that O
their O
model O
is O
able O
to O
build O
a O
form O
of O
inter O
- O
lingual O
representation O
between O
all O
involved O
language O
pairs O
. O
Shah O
and O
Specia O
( O
2016 O
) O
is O
the O
only O
work O
in O
QE O
that O
attempted O
to O
explore O
models O
for O
more O
than O
one O
language O
. O
They O
use O
multitask O
learning O
with O
annotators O
or O
languages O
as O
multiple O
tasks O
. O
In O
a O
traditional O
black O
- O
box O
feature O
- O
based O
approach O
with O
Gaussian O
Processes O
as O
learning O
algorithm O
, O
their O
results O
suggest O
that O
adequately O
modelling O
the O
additional O
data O
is O
as O
important O
as O
the O
additional O
data O
itself O
. O
The O
multilingual O
models O
led O
to O
marginal O
improvements O
over O
bilingual O
ones O
. O
In O
addition O
, O
the O
experiments O
were O
only O
conducted O
with O
English O
translation O
into O
two O
closely O
related O
languages O
( O
French O
and O
Spanish O
) O
. O
3 O
Multilingual O
QE O
In O
this O
section O
, O
we O
describe O
the O
QE O
models O
we O
propose O
and O
experiment O
with O
. O
They O
build O
upon O
pretrained O
representations O
and O
represent O
the O
SOTA O
in O
QE O
, O
as O
we O
will O
show O
in O
Section O
5 O
. O
Pre O
- O
trained O
contextualised O
representations O
such O
as O
BERT O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
and O
XLM O
- O
R O
( O
Conneau O
et O
al O
. O
, O
2019 O
) O
are O
deep O
contextualised O
language O
models O
based O
on O
the O
transformer O
neural O
architecture O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O
These O
models O
are O
pre O
- O
trained O
on O
a O
large O
amount O
of O
texts O
in O
multiple O
languages O
and O
optimised O
with O
self O
- O
supervised O
loss O
functions O
. O
They O
use O
shared O
subword O
vocabularies O
that O
directly O
support O
more O
than O
a O
hundred O
languages O
without O
the O
need O
for367Figure O
1 O
: O
Baseline O
QE O
model O
. O
any O
language O
- O
speciÔ¨Åc O
pre O
- O
processing O
. O
We O
explore O
QE O
models O
built O
on O
top O
of O
XLM O
- O
R O
, O
a O
pre O
- O
trained O
contextualised O
language O
model O
that O
achieves O
SOTA O
performance O
on O
multiple O
benchmark O
datasets O
. O
Baseline O
QE O
model O
( O
BASE O
) O
Given O
a O
source O
sentence O
sXin O
language O
Xand O
a O
target O
sentence O
sY O
in O
language O
Y O
, O
we O
model O
the O
QE O
function O
fby O
stacking O
a O
2 O
- O
layer O
multilayer O
perceptron O
( O
MLP O
) O
on O
the O
vector O
representation O
of O
the O
[ O
CLS O
] O
token O
from O
XLM O
- O
R O
: O
f(sX O
, O
sY O
) O
= O
W2¬∑ReLU O
( O
W1¬∑Ecls(sX O
, O
sY O
) O
+ O
b1 O
) O
+ O
b2(1 O
) O
where O
W2‚ààR1√ó4096,b2‚ààR O
, O
W1‚ààR4096√ó1024 O
andb1‚ààR4096.Eclsis O
a O
function O
that O
extracts O
the O
vector O
representation O
of O
the O
[ O
CLS O
] O
token O
after O
encoding O
the O
concatenation O
of O
sXandsYwith O
XLM O
- O
R O
and O
ReLU O
is O
the O
RectiÔ¨Åed O
Linear O
Unit O
activation O
function O
. O
We O
explore O
two O
training O
strategies O
: O
Thebilingual O
( O
BL O
) O
strategy O
trains O
a O
QE O
model O
for O
every O
language O
pair O
while O
the O
multilingual O
( O
ML O
) O
strategy O
trains O
a O
single O
multilingual O
QE O
model O
for O
all O
language O
pairs O
, O
where O
the O
training O
data O
is O
simply O
pooled O
together O
without O
any O
language O
identiÔ¨Åer O
. O
We O
note O
that O
this O
multilingual O
model O
here O
corresponds O
to O
a O
pooled O
, O
single O
- O
task O
learning O
approach O
. O
Multi O
- O
task O
Learning O
QE O
Model O
( O
MTL O
) O
Multitask O
learning O
has O
shown O
promising O
results O
in O
different O
NLP O
tasks O
( O
Ruder O
, O
2017 O
) O
. O
Here O
, O
we O
want O
to O
explore O
whether O
having O
parameter O
sharing O
across O
languages O
is O
beneÔ¨Åcial O
, O
and O
to O
what O
extent O
having O
language O
- O
speciÔ¨Åc O
predictors O
can O
boost O
performance O
. O
Therefore O
, O
we O
experiment O
with O
a O
simple O
multi O
- O
task O
approach O
where O
we O
concurrently O
optimise O
multiple O
QE O
BASE O
models O
that O
use O
a O
language O
- O
speciÔ¨Åc O
( O
LS O
) O
training O
strategy O
. O
To O
allow O
for O
testing O
in O
zero O
- O
shot O
conditions O
, O
we O
also O
train O
Figure O
2 O
: O
Multi O
- O
task O
learning O
QE O
model O
( O
MTL O
) O
with O
a O
shared O
XLM O
- O
R O
encoder O
. O
a O
language O
- O
agnostic O
( O
LA O
) O
component O
, O
which O
receives O
sampled O
data O
from O
every O
language O
. O
We O
refer O
to O
these O
two O
models O
as O
MTL O
- O
LA O
andMTL O
- O
LS O
. O
As O
seen O
in O
Figure O
2 O
, O
the O
MTL O
- O
LS O
submodels O
and O
MTL O
- O
LA O
submodel O
share O
a O
common O
XLM O
- O
R O
encoder O
, O
while O
each O
submodel O
has O
its O
own O
dedicated O
language O
- O
speciÔ¨Åc O
MLP O
. O
The O
intuition O
of O
this O
approach O
is O
that O
it O
can O
result O
in O
improved O
learning O
efÔ¨Åciency O
and O
prediction O
accuracy O
by O
exploiting O
the O
similarities O
and O
differences O
in O
the O
QE O
tasks O
for O
different O
language O
directions O
( O
Thrun O
, O
1996 O
; O
Baxter O
, O
2000 O
) O
. O
At O
training O
time O
, O
we O
iterate O
through O
the O
MTL O
- O
LS O
submodels O
in O
a O
round O
- O
robin O
fashion O
and O
alternate O
between O
training O
the O
MTL O
- O
LA O
submodel O
and O
training O
the O
chosen O
MTL O
- O
LS O
submodel O
. O
At O
test O
time O
, O
we O
can O
evaluate O
a O
test O
set O
with O
either O
the O
MTL O
- O
LA O
submodel O
or O
the O
MTL O
- O
LS O
submodel O
trained O
on O
the O
same O
language O
pair O
as O
the O
test O
set O
. O
4 O
Experimental O
Setup O
4.1 O
QE O
Dataset O
We O
use O
the O
ofÔ¨Åcial O
data O
from O
the O
WMT O
2020 O
QE O
Shared O
Task O
11 O
. O
This O
dataset O
contains O
sentences O
extracted O
from O
Wikipedia O
( O
Fomicheva O
et O
al O
. O
, O
2020 O
) O
and O
Reddit O
for O
Ru O
- O
En O
, O
translated O
to O
and O
from O
English O
for O
a O
total O
of O
7 O
language O
pairs O
. O
The O
language O
pairs O
are O
divided O
into O
3 O
categories O
: O
the O
high O
- O
resource O
English O
‚Äì O
German O
( O
En O
- O
De O
) O
, O
English O
‚Äì O
Chinese O
( O
En O
- O
Zh O
) O
and O
Russian O
‚Äì O
English O
( O
Ru O
- O
En O
) O
pairs O
; O
the O
medium O
- O
resource O
Romanian O
‚Äì O
English O
( O
Ro O
- O
En O
) O
and O
Estonian O
‚Äì O
English O
( O
Et O
- O
En O
) O
pairs O
; O
and O
1http://statmt.org/wmt20/ O
quality O
- O
estimation O
- O
task.html368the O
low O
- O
resource O
Sinhala O
‚Äì O
English O
( O
Si O
- O
En O
) O
and O
Nepali O
‚Äì O
English O
( O
Ne O
- O
En O
) O
pairs O
. O
Each O
translation O
was O
produced O
with O
SOTA O
transformer O
- O
based O
NMT O
models O
and O
manually O
annotated O
for O
quality O
using O
an O
annotation O
scheme O
inspired O
by O
the O
Direct O
Assessment O
( O
DA O
) O
methodology O
proposed O
by O
Graham O
et O
al O
. O
( O
2013 O
) O
. O
SpeciÔ¨Åcally O
, O
translations O
were O
annotated O
on O
a O
0 O
- O
100 O
scale O
, O
where O
the O
0 O
- O
10 O
range O
represents O
an O
incorrect O
translation O
; O
11 O
- O
29 O
, O
a O
translation O
with O
few O
correct O
keywords O
, O
but O
the O
overall O
meaning O
is O
different O
from O
the O
source O
; O
30 O
- O
50 O
, O
a O
translation O
with O
major O
mistakes O
; O
51 O
- O
69 O
, O
a O
translation O
which O
is O
understandable O
and O
conveys O
the O
overall O
meaning O
of O
the O
source O
but O
contains O
typos O
or O
grammatical O
errors O
; O
70 O
- O
90 O
, O
a O
translation O
that O
closely O
preserves O
the O
semantics O
of O
the O
source O
sentence O
; O
and O
90 O
- O
100 O
, O
a O
perfect O
translation O
. O
Figure O
3 O
shows O
the O
distribution O
of O
DA O
scores O
for O
the O
different O
language O
pairs O
. O
Figure O
3 O
: O
Distribution O
of O
DA O
judgments O
for O
different O
language O
pairs O
. O
4.2 O
Settings O
We O
train O
and O
test O
our O
models O
in O
the O
following O
conditions O
: O
Data O
splits O
we O
use O
the O
training O
and O
development O
sets O
provided O
for O
the O
WMT2020 O
shared O
task O
on O
QE.2Since O
the O
test O
set O
is O
not O
publicly O
available O
, O
we O
further O
split O
the O
7,000 O
- O
instance O
training O
set O
for O
each O
language O
pair O
by O
using O
the O
Ô¨Årst O
6,000 O
instances O
for O
training O
and O
the O
last O
1,000 O
instances O
for O
development O
, O
and O
report O
results O
on O
the O
ofÔ¨Åcial O
( O
1,000 O
) O
development O
set O
. O
Training O
details O
We O
optimise O
our O
models O
with O
Adam O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
and O
use O
the O
same O
2http://www.statmt.org/wmt20/ O
quality-estimation-task.htmllearning O
rate O
( O
1e‚àí6 O
) O
for O
all O
experiments O
. O
We O
use O
a O
batch O
size O
of O
8 O
and O
train O
on O
Nvidia O
V100 O
GPUs O
for O
20 O
epochs O
. O
Each O
model O
is O
trained O
5 O
times O
with O
different O
random O
seeds O
. O
Evaluation O
All O
results O
in O
this O
paper O
are O
in O
terms O
of O
the O
average O
Pearson O
‚Äôs O
correlation O
for O
predicted O
QE O
scores O
against O
gold O
QE O
scores O
over O
the O
5 O
different O
runs O
. O
Pearson O
correlation O
is O
the O
standard O
metric O
for O
this O
task O
, O
but O
we O
also O
compute O
error O
using O
Root O
Mean O
Squared O
Error O
( O
RMSE O
) O
( O
see O
Appendix O
) O
. O
5 O
Results O
In O
what O
follows O
, O
we O
pose O
and O
discuss O
various O
hypotheses O
on O
multilinguality O
for O
QE O
. O
First O
we O
focus O
on O
our O
black O
- O
box O
approach O
from O
Section O
3 O
( O
Section O
5.1 O
) O
. O
Second O
, O
we O
examine O
the O
behavior O
of O
a O
glassbox O
approach O
which O
does O
not O
directly O
model O
the O
source O
and O
target O
texts O
in O
multilingual O
settings O
( O
Section O
5.2 O
) O
. O
In O
all O
cases O
, O
we O
deÔ¨Åne O
TrainL O
as O
the O
set O
of O
language O
pairs O
used O
for O
training O
the O
QE O
model O
, O
andTestL O
as O
the O
set O
of O
language O
pairs O
used O
at O
test O
time O
. O
5.1 O
Black O
- O
box O
QE O
Approach O
5.1.1 O
Multilingual O
models O
are O
better O
than O
bilingual O
models O
As O
we O
can O
see O
from O
the O
results O
in O
Table O
13 O
, O
the O
average O
Pearson O
‚Äôs O
correlation O
scores O
of O
the O
multilingual O
models O
are O
always O
higher O
the O
bilingual O
ones O
, O
in O
some O
cases O
by O
a O
large O
margin O
. O
This O
is O
particularly O
true O
for O
En O
- O
De O
where O
the O
best O
BL O
model O
performs O
at O
Pearson O
‚Äôs O
correlation O
of O
0.39 O
, O
while O
both O
BASEML O
and O
MTL O
- O
LA O
achieve O
0.47 O
, O
which O
is O
a O
20.5 O
% O
relative O
improvement O
over O
the O
best O
BL O
model O
. O
Furthermore O
, O
the O
average O
score O
of O
Base O
- O
ML O
across O
all O
TestL O
is O
0.69 O
, O
0.03 O
( O
4.5 O
% O
) O
higher O
than O
the O
average O
score O
( O
0.66 O
) O
of O
the O
best O
BASE O
- O
BL O
scores O
across O
all O
TestL O
( O
diagonal O
in O
the O
top O
part O
of O
Table O
1 O
) O
. O
The O
results O
clearly O
show O
that O
multilingual O
models O
generally O
outperform O
bilingual O
models O
, O
even O
when O
the O
latter O
are O
optimised O
individually O
for O
different O
TestL. O
An O
interesting O
observation O
in O
Table O
1 O
is O
that O
some O
BASE O
- O
BL O
models O
trained O
on O
different O
TrainL O
than O
TestL O
can O
perform O
almost O
as O
well O
as O
the O
models O
trained O
on O
the O
same O
TrainL O
as O
TestL. O
For O
example O
, O
3The O
best O
results O
for O
BASE O
- O
BL O
are O
underlined O
and O
bold O
marks O
the O
best O
results O
across O
all O
models O
. O
SigniÔ¨Åcant O
improvements O
over O
BASE O
BL O
are O
marked O
with O
* O
. O
We O
use O
the O
HotellingWilliams O
test O
for O
dependent O
correlations O
to O
compute O
signiÔ¨Åcance O
of O
the O
difference O
between O
correlations O
( O
Williams O
, O
1959 O
) O
with O
p O
- O
value O
< O
0.05.369Model O
Strategy O
TrainLTestL O
En O
- O
De O
En O
- O
Zh O
Et O
- O
En O
Ro O
- O
En O
Si O
- O
En O
Ne O
- O
En O
Ru O
- O
En O
Avg O
BASEBLEn O
- O
De O
0.39 O
( O
-0.17 O
) O
( O
-0.39 O
) O
( O
-0.51 O
) O
( O
-0.32 O
) O
( O
-0.51 O
) O
( O
-0.35 O
) O
0.34 O
En O
- O
Zh O
( O
-0.02 O
) O
0.47 O
( O
-0.19 O
) O
( O
-0.36 O
) O
( O
-0.16 O
) O
( O
-0.24 O
) O
( O
-0.17 O
) O
0.50 O
Et O
- O
En O
( O
-0.10 O
) O
( O
-0.08 O
) O
0.75 O
( O
-0.20 O
) O
( O
-0.07 O
) O
( O
-0.10 O
) O
( O
-0.08 O
) O
0.57 O
Ro O
- O
En O
( O
-0.10 O
) O
( O
-0.14 O
) O
( O
-0.02 O
) O
0.89 O
( O
-0.02 O
) O
( O
-0.04 O
) O
( O
-0.08 O
) O
0.60 O
Si O
- O
En O
( O
-0.13 O
) O
( O
-0.13 O
) O
( O
-0.08 O
) O
( O
-0.15 O
) O
0.66 O
( O
-0.05 O
) O
( O
-0.07 O
) O
0.57 O
Ne O
- O
En O
( O
-0.10 O
) O
( O
-0.11 O
) O
( O
-0.06 O
) O
( O
-0.08 O
) O
( O
-0.01 O
) O
0.77 O
( O
-0.08 O
) O
0.60 O
Ru O
- O
En O
( O
-0.04 O
) O
( O
-0.09 O
) O
( O
-0.19 O
) O
( O
-0.26 O
) O
( O
-0.11 O
) O
( O
-0.16 O
) O
0.70 O
0.54 O
ML O
All O
0.47 O
* O
0.49 O
0.78 O
* O
0.89 O
0.70 O
* O
0.78 O
0.73 O
0.69 O
MTLLS O
All O
0.45 O
0.48 O
0.77 O
0.89 O
0.66 O
0.79 O
0.72 O
0.68 O
LA O
All O
0.47 O
* O
0.49 O
0.76 O
0.89 O
0.66 O
0.78 O
0.72 O
0.68 O
LS O
En- O
* O
0.41 O
0.46 O
- O
- O
- O
- O
- O
LA O
En- O
* O
0.45 O
0.46 O
- O
- O
- O
- O
- O
LS O
* O
-En O
- O
- O
0.78 O
* O
0.90 O
0.69 O
0.79 O
0.73 O
LA O
* O
-En O
- O
- O
0.78 O
* O
0.89 O
0.69 O
0.78 O
0.73 O
‚Ä°BERT O
- O
BiRNN O
( O
Fomicheva O
et O
al O
. O
, O
2020 O
) O
0.27 O
0.37 O
0.64 O
0.76 O
0.47 O
0.55 O
- O
‚Ä°WMT20 O
QE O
Shared O
Task O
1 O
Leaderboard O
( O
June O
2020 O
) O
0.47 O
0.48 O
0.79 O
0.90 O
0.65 O
0.79 O
0.78 O
0.69 O
Table O
1 O
: O
Results O
for O
BASE O
and O
MTL O
QE O
models O
. O
We O
train O
different O
BASE O
- O
BL O
models O
for O
every O
language O
pair O
and O
a O
single O
BASE O
- O
ML O
model O
on O
all O
language O
pairs O
. O
We O
also O
train O
a O
single O
MTL O
QE O
model O
consists O
of O
multiple O
MTL O
- O
LS O
and O
MTL O
- O
LA O
submodels O
. O
For O
each O
TestL O
, O
we O
evaluate O
it O
with O
the O
MTL O
- O
LS O
submodel O
trained O
on O
the O
same O
language O
pair O
. O
We O
bold O
the O
best O
results O
across O
all O
models O
. O
SigniÔ¨Åcant O
improvements O
over O
BASE O
BL O
are O
marked O
with O
* O
.‚Ä°identiÔ¨Åes O
systems O
trained O
on O
the O
full O
7,000 O
- O
instance O
training O
set O
with O
performances O
reported O
on O
the O
ofÔ¨Åcial O
test O
set O
of O
the O
WMT‚Äô20 O
QE O
Shared O
Task O
1 O
( O
https://competitions.codalab.org/competitions/24447 O
) O
, O
which O
we O
assume O
to O
come O
from O
the O
same O
distribution O
as O
the O
dev O
set O
. O
a O
BASE O
- O
BL O
model O
trained O
on O
En O
- O
Zh O
and O
tested O
on O
En O
- O
De O
performs O
at O
average O
Pearson O
‚Äôs O
correlation O
of O
0.37 O
, O
which O
is O
only O
0.02 O
below O
the O
best O
result O
. O
We O
hypothesize O
that O
XLM O
- O
R O
might O
be O
capturing O
certain O
traits O
in O
TrainL O
that O
can O
generalise O
well O
to O
other O
TestL O
, O
i.e. O
the O
complexity O
of O
source O
sentences O
or O
the O
Ô¨Çuency O
of O
the O
target O
sentences O
( O
Sun O
et O
al O
. O
, O
2020 O
) O
. O
5.1.2 O
There O
is O
little O
beneÔ¨Åt O
from O
specialisation O
Here O
we O
investigate O
whether O
having O
specialised O
language O
- O
speciÔ¨Åc O
sub O
- O
models O
which O
can O
beneÔ¨Åt O
from O
the O
shared O
supervision O
from O
other O
languages O
while O
keeping O
their O
focus O
on O
a O
language O
- O
speciÔ¨Åc O
task O
can O
help O
to O
improve O
performance O
. O
Furthermore O
, O
it O
is O
possible O
that O
multi O
- O
task O
learning O
works O
better O
when O
language O
pairs O
share O
certain O
characteristics O
. O
Therefore O
, O
we O
also O
investigate O
whether O
combining O
language O
pairs O
that O
share O
either O
source O
or O
target O
languages O
can O
be O
more O
beneÔ¨Åcial O
. O
For O
that O
, O
we O
use O
the O
MTL O
models O
but O
with O
a O
reduced O
set O
of O
languages O
. O
From O
the O
results O
in O
Table O
1 O
, O
we O
observe O
that O
language O
- O
specialised O
predictors O
do O
not O
help O
improve O
performance O
. O
There O
is O
no O
clear O
advantage O
in O
using O
the O
multi O
- O
task O
learning O
QE O
approach O
( O
MTL O
- O
LS O
and O
MTL O
- O
LA O
) O
where O
each O
language O
pair O
is O
treated O
as O
a O
separate O
task O
; O
over O
the O
simple O
singletask O
multi O
- O
lingual O
learning O
approach O
( O
BASE O
- O
ML O
) O
, O
despite O
the O
former O
having O
more O
parameters O
and O
language O
- O
speciÔ¨Åc O
MLP O
layers O
. O
In O
the O
table O
, O
we O
compare O
MTL O
models O
trained O
on O
language O
pairs O
that O
share O
the O
source O
language O
( O
En- O
* O
) O
or O
the O
target O
language O
( O
* O
-En O
) O
against O
MTL O
models O
trained O
on O
all O
languages O
( O
All O
) O
. O
As O
we O
can O
see O
from O
the O
results O
, O
the O
MTL O
model O
trained O
on O
En O
* O
perform O
worse O
than O
the O
MTL O
model O
trained O
on O
all O
language O
pairs O
. O
In O
contrast O
, O
the O
MTL O
model O
trained O
on O
* O
-En O
performs O
a O
little O
bit O
better O
than O
the O
MTL O
model O
trained O
on O
all O
language O
pairs O
on O
4 O
out O
of O
the O
5 O
language O
pairs O
and O
is O
comparable O
to O
Base O
- O
ML O
on O
those O
language O
directions O
. O
5.1.3 O
Multilingual O
models O
help O
zero- O
and O
few O
- O
shot O
QE O
To O
test O
whether O
a O
multilingual O
model O
for O
QE O
can O
generalise O
beyond O
the O
language O
pairs O
observed O
during O
training O
, O
we O
also O
conduct O
experiments O
varying O
amounts O
of O
in O
- O
language O
data O
( O
i.e. O
0 O
% O
‚Äì O
zeroshot O
, O
5 O
% O
, O
10 O
% O
, O
25 O
% O
, O
50 O
% O
, O
75 O
% O
and O
100 O
% O
) O
. O
We O
build O
and O
compare O
BASE O
- O
BL O
and O
BASE O
- O
ML O
models O
. O
We O
train O
BASE O
- O
BL O
models O
only O
on O
the O
sub-370TestL O
% O
in O
- O
lang O
Model O
Strategy O
En O
- O
De O
En O
- O
Zh O
Et O
- O
En O
Ro O
- O
En O
Si O
- O
En O
Ne O
- O
En O
Ru O
- O
En O
Avg O
0 O
BASE O
ML O
0.45 O
0.42 O
0.75 O
0.80 O
0.68 O
0.76 O
0.68 O
0.65 O
5 O
BASEBL O
0.13 O
0.39 O
0.65 O
0.70 O
0.58 O
0.63 O
0.63 O
0.53 O
ML O
0.38 O
0.44 O
0.74 O
0.85 O
0.67 O
0.76 O
0.71 O
0.65 O
10 O
BASEBL O
0.24 O
0.43 O
0.69 O
0.85 O
0.56 O
0.68 O
0.64 O
0.58 O
ML O
0.37 O
0.46 O
0.75 O
0.87 O
0.64 O
0.77 O
0.71 O
0.65 O
25 O
BASEBL O
0.27 O
0.45 O
0.70 O
0.87 O
0.61 O
0.72 O
0.70 O
0.62 O
ML O
0.40 O
0.46 O
0.75 O
0.88 O
0.66 O
0.76 O
0.71 O
0.66 O
50 O
BASEBL O
0.33 O
0.47 O
0.74 O
0.88 O
0.62 O
0.74 O
0.69 O
0.64 O
ML O
0.41 O
0.48 O
0.76 O
0.89 O
0.69 O
0.77 O
0.72 O
0.67 O
75 O
BASEBL O
0.39 O
0.47 O
0.75 O
0.88 O
0.64 O
0.76 O
0.70 O
0.66 O
ML O
0.46 O
0.49 O
0.78 O
0.89 O
0.70 O
0.78 O
0.71 O
0.69 O
100 O
BASEBL O
0.39 O
0.47 O
0.75 O
0.89 O
0.66 O
0.77 O
0.70 O
0.66 O
ML O
0.47 O
0.49 O
0.78 O
0.89 O
0.70 O
0.78 O
0.73 O
0.69 O
Table O
2 O
: O
Results O
of O
BASE O
QE O
models O
for O
different O
portions O
of O
training O
data O
( O
% O
data O
) O
. O
For O
BASE O
- O
ML O
, O
we O
train O
the O
models O
on O
subsampled O
training O
data O
in O
the O
test O
language O
pair O
and O
all O
training O
data O
in O
other O
language O
pairs O
. O
For O
BASE O
- O
BL O
, O
we O
train O
the O
models O
on O
only O
subsampled O
training O
data O
in O
the O
test O
language O
pair O
. O
We O
underline O
the O
best O
results O
for O
each O
% O
data O
setting O
. O
sampled O
in O
- O
language O
training O
data O
and O
train O
BASEML O
on O
both O
sub O
- O
sampled O
in O
- O
language O
training O
data O
and O
all O
training O
data O
in O
other O
language O
pairs O
. O
In O
other O
words O
, O
we O
want O
to O
know O
whether O
multilingual O
QE O
helps O
if O
we O
have O
limited O
or O
no O
training O
data O
in O
our O
desired O
test O
language O
pair O
. O
Results O
are O
shown O
in O
Table O
2 O
. O
For O
ease O
of O
visualisation O
, O
we O
also O
plot O
the O
Pearson O
‚Äôs O
correlation O
results O
against O
the O
percentage O
of O
in O
- O
language O
training O
data O
in O
Figure O
4 O
. O
As O
seen O
in O
Table O
2 O
, O
the O
multilingual O
model O
performs O
better O
than O
the O
bilingual O
models O
on O
all O
language O
pairs O
for O
every O
conÔ¨Åguration O
of O
training O
data O
. O
Moreover O
, O
in O
3 O
out O
of O
7 O
cases O
, O
the O
zero O
- O
shot O
models O
perform O
better O
than O
the O
fully O
- O
trained O
bilingual O
models O
. O
This O
provides O
strong O
evidence O
that O
the O
QE O
task O
can O
be O
solved O
in O
a O
multilingual O
way O
, O
without O
loss O
of O
performance O
compared O
to O
bilingual O
performance O
. O
It O
also O
shows O
strong O
evidence O
for O
the O
zero O
- O
shot O
applicability O
of O
our O
models O
. O
5.2 O
Glass O
- O
box O
QE O
Approach O
Having O
pre O
- O
trained O
representations O
can O
help O
build O
state O
- O
of O
- O
the O
- O
art O
multilingual O
systems O
. O
However O
, O
these O
representations O
are O
costly O
to O
compute O
in O
practice O
, O
which O
limits O
their O
applicability O
for O
building O
QE O
systems O
for O
real O
- O
time O
scenarios O
. O
Glass O
- O
box O
approaches O
to O
QE O
extract O
information O
from O
the O
NMT O
system O
itself O
to O
predict O
quality O
, O
without O
directly O
relying O
on O
the O
source O
and O
target O
text O
or O
using O
any O
external O
resources O
. O
To O
test O
how O
well O
this O
information O
can O
generalise O
across O
different O
languages O
, O
we O
lever-0 O
5 O
10 O
25 O
50 O
75 O
10000.20.40.60.81 O
% O
DataCorrelationEn O
- O
De O
En O
- O
Zh O
Et O
- O
En O
Ro O
- O
En O
Si O
- O
En O
Ne O
- O
En O
Ru O
- O
En O
Avg O
Figure O
4 O
: O
Results O
of O
BASE O
QE O
models O
for O
various O
zeroshot O
and O
few O
- O
shot O
cross O
- O
lingual O
transfer O
settings O
. O
The O
solid O
lines O
represent O
the O
BASE O
ML O
models O
while O
the O
dashed O
lines O
are O
the O
BASE O
BL O
models O
. O
age O
existing O
work O
on O
glass O
- O
box O
QE O
by O
Fomicheva O
et O
al O
. O
( O
2020 O
) O
that O
explores O
NMT O
output O
distribution O
to O
capture O
predictive O
uncertainty O
as O
a O
proxy O
for O
MT O
quality O
. O
We O
use O
the O
following O
5 O
best O
- O
performing O
glass O
- O
box O
indicators O
from O
their O
work O
: O
‚Ä¢Average O
NMT O
log O
- O
probability O
of O
the O
translated O
sentence O
; O
‚Ä¢Variance O
of O
word O
- O
level O
log O
- O
probabilities O
; O
‚Ä¢Entropy O
of O
NMT O
softmax O
output O
distribution O
; O
‚Ä¢NMT O
log O
- O
probability O
of O
translations O
generated O
with O
Monte O
Carlo O
dropout O
( O
Gal O
and O
Ghahramani O
, O
2016);4 O
4This O
method O
consists O
in O
performing O
several O
forward371TrainLTestL O
En O
- O
De O
En O
- O
Zh O
Et O
- O
En O
Ro O
- O
En O
Si O
- O
En O
Ne O
- O
En O
En O
- O
De O
0.24 O
( O
-0.25 O
) O
( O
-0.36 O
) O
( O
-0.22 O
) O
( O
-0.24 O
) O
( O
-0.32 O
) O
En O
- O
Zh O
( O
+0.08 O
) O
0.44 O
( O
-0.05 O
) O
( O
-0.04 O
) O
( O
-0.03 O
) O
( O
-0.08 O
) O
Et O
- O
En O
( O
+0.07 O
) O
( O
-0.03 O
) O
0.61 O
( O
-0.02 O
) O
( O
-0.02 O
) O
( O
-0.06 O
) O
Ro O
- O
En O
( O
+0.05 O
) O
( O
-0.05 O
) O
( O
-0.03 O
) O
0.76 O
( O
-0.02 O
) O
( O
-0.06 O
) O
Si O
- O
En O
( O
+0.06 O
) O
( O
-0.04 O
) O
( O
-0.04 O
) O
( O
-0.03 O
) O
0.54 O
( O
-0.03 O
) O
Ne O
- O
En O
( O
-0.00 O
) O
( O
-0.09 O
) O
( O
-0.09 O
) O
( O
-0.09 O
) O
( O
-0.04 O
) O
0.58 O
All O
langs O
0.32 O
0.44 O
0.60 O
0.75 O
0.55 O
0.56 O
Best O
feature O
0.26 O
0.32 O
0.64 O
0.69 O
0.51 O
0.60 O
Table O
3 O
: O
Pearson O
correlation O
for O
regression O
models O
based O
on O
glass O
- O
box O
features O
trained O
on O
each O
language O
pair O
and O
evaluated O
either O
on O
the O
same O
language O
pair O
or O
other O
language O
pairs O
. O
For O
testing O
on O
a O
different O
language O
pair O
we O
report O
the O
difference O
in O
Pearson O
correlation O
with O
respect O
to O
training O
and O
testing O
on O
the O
same O
language O
pair O
. O
For O
comparison O
we O
show O
the O
correlation O
individual O
best O
performing O
feature O
with O
no O
learning O
involved O
. O
‚Ä¢Lexical O
similarity O
between O
MT O
hypotheses O
generated O
with O
Monte O
Carlo O
dropout O
. O
We O
train O
an O
XGboost O
regression O
model O
( O
Chen O
and O
Guestrin O
, O
2016)5to O
combine O
these O
features O
to O
predict O
DA O
judgments O
and O
test O
the O
performance O
of O
the O
model O
in O
multilingual O
settings O
. O
Table O
3 O
shows O
Pearson O
correlation O
for O
the O
regression O
models O
trained O
on O
each O
language O
pair O
and O
evaluated O
either O
on O
the O
same O
language O
pair O
or O
other O
language O
pairs.6The O
‚Äô O
All O
langs O
‚Äô O
row O
indicates O
the O
results O
when O
training O
on O
all O
language O
pairs O
, O
whereas O
‚Äô O
Best O
feature O
‚Äô O
indicates O
the O
correlation O
obtained O
by O
the O
best O
performing O
feature O
individually O
. O
Comparing O
these O
results O
to O
the O
results O
for O
pre O
- O
trained O
representations O
in O
Table O
1 O
we O
can O
make O
three O
observations O
. O
5.2.1 O
Glass O
- O
box O
features O
are O
more O
comparable O
across O
languages O
First O
, O
although O
the O
correlation O
is O
generally O
lower O
for O
the O
glass O
- O
box O
approach O
, O
performance O
degradation O
when O
testing O
on O
different O
language O
pairs O
is O
smaller O
. O
For O
all O
language O
pairs O
except O
EnglishGerman O
, O
we O
observe O
a O
relatively O
small O
decrease O
in O
performance O
( O
up O
to O
0.09 O
) O
when O
training O
and O
test O
language O
pairs O
are O
different O
. O
This O
suggests O
that O
the O
indicators O
extracted O
from O
the O
NMT O
model O
are O
more O
passes O
through O
the O
network O
, O
collecting O
posterior O
probabilities O
generated O
by O
the O
model O
with O
parameters O
perturbed O
by O
dropout O
and O
using O
the O
resulting O
distribution O
to O
approximate O
model O
uncertainty O
. O
5We O
chose O
a O
regression O
model O
over O
an O
NN O
given O
the O
smaller O
number O
of O
features O
available O
. O
6These O
experiments O
do O
not O
include O
Russian O
- O
English O
, O
as O
the O
corresponding O
NMT O
system O
is O
an O
ensemble O
and O
it O
is O
not O
evident O
how O
the O
glass O
- O
box O
features O
proposed O
by O
Fomicheva O
et O
al O
. O
( O
2020 O
) O
should O
be O
extracted O
in O
this O
case.comparable O
across O
languages O
than O
input O
features O
from O
pre O
- O
trained O
representations O
. O
We O
note O
that O
the O
NMT O
systems O
in O
MLQE O
dataset O
were O
all O
based O
on O
Transformer O
architecture O
but O
trained O
using O
different O
amount O
of O
data O
and O
have O
different O
overall O
output O
quality O
. O
Interestingly O
, O
the O
results O
of O
this O
experiment O
indicate O
that O
glass O
- O
box O
information O
extracted O
from O
these O
systems O
could O
be O
language O
- O
independent O
. O
More O
experiments O
are O
needed O
to O
conÔ¨Årm O
if O
this O
observation O
can O
be O
extrapolated O
to O
other O
datasets O
, O
language O
pairs O
, O
domains O
and O
MT O
systems O
. O
5.2.2 O
Multilingual O
gains O
are O
limited O
by O
learning O
algorithm O
Second O
, O
by O
contrast O
to O
the O
results O
in O
Table O
1 O
where O
multilingual O
training O
brings O
signiÔ¨Åcant O
improvements O
, O
we O
do O
not O
see O
any O
gains O
in O
performance O
from O
training O
with O
all O
available O
data O
. O
The O
reason O
could O
be O
that O
training O
a O
regression O
model O
with O
a O
small O
number O
of O
features O
does O
not O
require O
large O
amounts O
of O
training O
data O
, O
and O
therefore O
performance O
does O
not O
improve O
with O
additional O
data O
. O
English O
- O
German O
is O
an O
exception O
with O
a O
large O
gain O
in O
correlation O
when O
training O
on O
all O
language O
pairs O
. O
5.2.3 O
The O
output O
label O
distribution O
matters O
Finally O
, O
similarly O
to O
the O
black O
- O
box O
approach O
in O
Table O
1 O
, O
the O
performance O
for O
English O
- O
German O
beneÔ¨Åts O
from O
using O
the O
data O
from O
other O
language O
pairs O
for O
training O
. O
This O
indicates O
that O
the O
results O
are O
affected O
by O
factors O
that O
are O
independent O
of O
the O
approach O
used O
for O
prediction O
. O
To O
better O
understand O
these O
results O
we O
look O
at O
the O
distribution O
of O
NMT O
logprobabilities O
( O
Figure O
5 O
) O
and O
the O
distribution O
of O
DA O
scores O
( O
Figure O
3).372Figure O
5 O
: O
Distribution O
of O
NMT O
log O
- O
probabilities O
for O
different O
language O
pairs O
While O
log O
- O
probability O
distributions O
are O
comparable O
across O
language O
pairs O
, O
the O
distributions O
of O
DA O
scores O
are O
very O
different O
. O
We O
suggest O
, O
therefore O
, O
that O
the O
decrease O
in O
performance O
when O
testing O
on O
a O
different O
language O
is O
related O
to O
a O
higher O
extent O
to O
the O
shift O
in O
the O
output O
distribution O
across O
languages O
( O
i.e. O
DA O
judgments O
) O
than O
to O
the O
shift O
in O
the O
input O
features O
. O
This O
also O
explains O
the O
difÔ¨Åculty O
for O
training O
and O
predicting O
on O
English O
- O
German O
data O
where O
the O
distribution O
of O
DA O
scores O
is O
highly O
skewed O
with O
minimal O
variability O
in O
the O
quality O
range O
. O
6 O
Discussion O
and O
Conclusions O
From O
our O
various O
experiments O
, O
one O
setting O
that O
stood O
out O
is O
that O
of O
English O
- O
German O
. O
We O
suggest O
that O
the O
difÔ¨Åculty O
for O
predicting O
quality O
for O
this O
language O
pair O
was O
exacerbated O
by O
the O
metric O
used O
for O
evaluation O
. O
Because O
of O
its O
sample O
- O
dependence O
, O
Pearson O
correlation O
can O
be O
more O
sensitive O
to O
the O
output O
distribution O
. O
In O
contrast O
, O
an O
error O
- O
based O
metric O
like O
RMSE O
will O
be O
less O
sensitive O
to O
these O
variations O
. O
To O
illustrate O
these O
effects O
, O
in O
Figure O
6 O
, O
we O
show O
the O
hierarchical O
clustering O
of O
language O
directions O
obtained O
by O
using O
the O
metric O
value O
from O
training O
on O
one O
direction O
and O
testing O
on O
another O
one O
as O
a O
notion O
of O
distance O
. O
In O
subÔ¨Ågure O
( O
a O
) O
, O
we O
observe O
the O
clusters O
based O
on O
Pearson O
correlation O
as O
shown O
in O
Table O
1 O
. O
In O
subÔ¨Ågure O
( O
b O
) O
, O
we O
observe O
the O
same O
clustering O
done O
based O
on O
RMSE O
. O
It O
should O
be O
noted O
that O
in O
the O
former O
, O
En O
- O
De O
is O
a O
clear O
outlier O
, O
whereas O
in O
the O
latter O
, O
we O
have O
a O
clustering O
that O
is O
more O
consistent O
with O
the O
general O
maturity O
of O
the O
language O
pairs O
: O
Ne O
- O
En O
and O
Si O
- O
En O
are O
low O
resource O
, O
Ro O
- O
En O
and O
Et O
- O
En O
are O
medium O
resource O
, O
etc O
. O
We O
explored O
the O
use O
of O
multilingual O
contextual O
( O
a O
) O
Pearson O
correlation O
( O
b O
) O
RMSE O
Figure O
6 O
: O
Language O
hierarchical O
clustering O
according O
to O
the O
results O
of O
training O
on O
one O
language O
and O
testing O
on O
another O
. O
In O
subÔ¨Ågure O
( O
a O
) O
we O
plot O
the O
clustering O
based O
on O
Pearson O
correlation O
. O
In O
subÔ¨Ågure O
( O
b O
) O
we O
plot O
the O
same O
clustering O
based O
on O
RMSE O
. O
The O
y O
axis O
denotes O
thedistance O
between O
language O
pairs O
according O
to O
each O
evaluation O
. O
representations O
to O
build O
state O
- O
of O
- O
the O
- O
art O
multilingual O
QE O
models O
. O
From O
our O
experiments O
, O
we O
observed O
that O
: O
1 O
) O
multilingual O
systems O
are O
always O
better O
than O
bilingual O
systems O
; O
2 O
) O
having O
multi O
- O
task O
models O
, O
which O
share O
parts O
of O
the O
model O
across O
languages O
and O
specialise O
others O
, O
does O
not O
necessarily O
yield O
better O
results O
; O
and O
3 O
) O
multilingual O
systems O
for O
QE O
generalise O
well O
across O
languages O
and O
are O
powerful O
even O
in O
zero O
- O
shot O
scenarios O
. O
We O
also O
contrasted O
the O
use O
of O
pre O
- O
trained O
representations O
which O
are O
costly O
to O
obtain O
, O
to O
the O
use O
of O
glass O
- O
box O
features O
which O
can O
be O
extracted O
from O
the O
NMT O
system O
. O
We O
observed O
that O
glass O
- O
box O
features O
are O
very O
comparable O
across O
languages O
, O
and O
training O
multilingual O
systems O
with O
them O
adds O
little O
value O
. O
Finally O
, O
we O
observed O
that O
the O
distribution O
of O
the O
output O
labels O
matters O
for O
the O
evaluation O
of O
QE.373Acknowledgments O
Marina O
Fomicheva O
, O
Fr O
¬¥ O
ed¬¥eric O
Blain O
and O
Lucia O
Specia O
were O
supported O
by O
funding O
from O
the O
Bergamot O
project O
( O
EU O
H2020 O
Grant O
No O
. O
825303 O
) O
. O
Abstract O
Approaching O
named O
entities O
transliteration O
as O
a O
Neural O
Machine O
Translation O
( O
NMT O
) O
problem O
is O
common O
practice O
. O
While O
many O
have O
applied O
various O
NMT O
techniques O
to O
enhance O
machine O
transliteration O
models O
, O
few O
focus O
on O
the O
linguistic O
features O
particular O
to O
the O
relevant O
languages O
. O
In O
this O
paper O
, O
we O
investigate O
the O
effect O
of O
incorporating O
phonetic O
features O
for O
English O
- O
to O
- O
Chinese O
transliteration O
under O
the O
multi O
- O
task O
learning O
( O
MTL O
) O
setting O
‚Äî O
where O
we O
deÔ¨Åne O
a O
phonetic O
auxiliary O
task O
aimed O
to O
improve O
the O
generalization O
performance O
of O
the O
main O
transliteration O
task O
. O
In O
addition O
to O
our O
system O
, O
we O
also O
release O
a O
new O
English O
- O
toChinese O
dataset O
and O
propose O
a O
novel O
evaluation O
metric O
which O
considers O
multiple O
possible O
transliterations O
given O
a O
source O
name O
. O
Our O
results O
show O
that O
the O
multi O
- O
task O
model O
achieves O
similar O
performance O
as O
the O
previous O
state O
of O
the O
art O
with O
a O
model O
of O
a O
much O
smaller O
size.1 O
1 O
Introduction O
Transliteration O
, O
the O
act O
of O
mapping O
a O
name O
from O
the O
orthographic O
system O
of O
one O
language O
to O
another O
, O
is O
directed O
by O
the O
pronunciation O
in O
the O
source O
and O
target O
languages O
, O
and O
often O
by O
historical O
reasons O
or O
conventions O
. O
It O
plays O
an O
important O
role O
in O
tasks O
like O
information O
retrieval O
and O
machine O
translation O
( O
Marton O
and O
Zitouni O
, O
2014 O
; O
Hermjakob O
et O
al O
. O
, O
2008 O
) O
. O
Over O
the O
recent O
years O
, O
many O
have O
addressed O
transliteration O
using O
sequence O
- O
to O
- O
sequence O
( O
seq2seq O
) O
deep O
learning O
models O
( O
Rosca O
and O
Breuel O
, O
2016 O
; O
Merhav O
and O
Ash O
, O
2018 O
; O
Grundkiewicz O
and O
HeaÔ¨Åeld O
, O
2018 O
) O
, O
enhanced O
with O
several O
NMT O
techniques O
( O
Grundkiewicz O
and O
HeaÔ¨Åeld O
, O
2018 O
) O
. O
However O
, O
this O
recent O
work O
neglects O
the O
most O
crucial O
feature O
for O
transliteration O
, O
i.e. O
pronunciation O
. O
To O
* O
Work O
done O
at O
The O
University O
of O
Edinburgh O
. O
1Our O
code O
and O
data O
are O
available O
at O
https://github O
. O
com O
/ O
Lawhy O
/ O
Multi O
- O
task O
- O
NMTransliteration O
.English O
IPA O
Chinese O
Pinyin O
A O
/"eI./ O
Ëâæ O
` O
ai O
my O
/mi/ O
Á±≥ O
mÀáƒ± O
Table O
1 O
: O
An O
example O
of O
English O
- O
to O
- O
Chinese O
transliteration O
, O
from O
Amy O
toËâæÁ±≥ O
. O
Each O
row O
presents O
a O
group O
of O
corresponding O
subsequences O
in O
different O
representations O
. O
bridge O
this O
gap O
, O
we O
deÔ¨Åne O
a O
phonetic O
auxiliary O
task O
that O
shares O
the O
sound O
information O
with O
the O
main O
transliteration O
task O
under O
the O
multi O
- O
task O
learning O
( O
MTL O
) O
setting O
. O
Depending O
on O
the O
speciÔ¨Åc O
language O
, O
the O
written O
form O
of O
a O
word O
reveals O
its O
pronunciation O
to O
various O
extents O
. O
For O
alphabetical O
languages O
such O
as O
English O
and O
French O
, O
a O
letter O
, O
or O
a O
sequence O
of O
letters O
, O
usually O
reÔ¨Çects O
the O
word O
pronunciation O
. O
For O
example O
, O
the O
word O
Amy O
( O
in O
the O
International O
Phonetic O
Alphabet O
, O
IPA O
, O
/"eI.mi/ O
) O
has O
the O
sub O
- O
word O
Acorresponding O
to O
/"eI./ O
andmycorresponding O
to O
/mi/ O
. O
In O
contrast O
, O
characters O
in O
a O
logographic2writing O
system O
for O
languages O
like O
Chinese O
or O
Japanese O
do O
not O
explicitly O
indicate O
sound O
( O
Xing O
et O
al O
. O
, O
2006 O
) O
. O
In O
this O
paper O
, O
we O
give O
a O
treatment O
to O
the O
problem O
of O
transliteration O
from O
English O
( O
alphabet O
) O
to O
Chinese3(logogram O
) O
using O
an O
RNN O
- O
based O
MTL O
model O
with O
a O
phonetic O
auxiliary O
task O
. O
We O
transform O
each O
Chinese O
character O
to O
the O
alphabetical O
representation O
of O
its O
pronunciation O
via O
the O
ofÔ¨Åcial O
phonetic O
writing O
system O
, O
Pinyin,4which O
uses O
Latin O
letters O
with O
four O
diacritics O
denoting O
tones O
to O
represent O
the O
sounds O
. O
2A O
logogram O
is O
an O
individual O
character O
that O
represents O
a O
whole O
word O
or O
phrase O
. O
3The O
Chinese O
language O
we O
mention O
in O
this O
paper O
refers O
explicitly O
to O
Mandarin O
, O
which O
is O
the O
ofÔ¨Åcial O
language O
originated O
from O
the O
northern O
dialect O
in O
China O
. O
4Pinyin O
is O
the O
ofÔ¨Åcial O
romanization O
system O
for O
Standard O
Chinese O
( O
Mandarin O
) O
in O
mainland O
China O
and O
to O
some O
extent O
in O
Taiwan O
. O
It O
does O
not O
apply O
to O
other O
Chinese O
dialects.378For O
example O
, O
the O
Chinese O
transliteration O
for O
Amy O
is O
ËâæÁ±≥and O
the O
associated O
Pinyin O
representation O
is O
` O
ai O
mÀáƒ± O
. O
We O
summarize O
the O
correspondences O
occurring O
in O
this O
example O
in O
Table O
1 O
. O
Due O
to O
the O
similarity O
between O
the O
source O
name O
and O
the O
Pinyin O
representation O
, O
Jiang O
et O
al O
. O
( O
2009 O
) O
proposed O
a O
sequential O
transliteration O
model O
that O
uses O
Pinyin O
as O
an O
intermediate O
representation O
before O
transliterating O
a O
Chinese O
name O
to O
English O
. O
In O
contrast O
, O
our O
idea O
is O
to O
build O
a O
model O
with O
a O
shared O
encoder O
and O
dual O
decoders O
, O
that O
can O
learn O
the O
mapping O
from O
English O
to O
Chinese O
and O
Pinyin O
simultaneously O
. O
By O
jointly O
learning O
source O
- O
to O
- O
target O
and O
source O
- O
to O
- O
sound O
mappings O
, O
the O
encoder O
is O
expected O
to O
generalize O
better O
( O
Ruder O
, O
2017 O
) O
and O
pass O
more O
reÔ¨Åned O
information O
to O
the O
decoders O
. O
Transliteration O
datasets O
are O
often O
extracted O
from O
dictionaries O
, O
or O
aligned O
corpus O
generated O
from O
applying O
named O
entity O
recognition O
( O
NER O
) O
system O
to O
parallel O
newspaper O
articles O
in O
different O
languages O
( O
Sproat O
et O
al O
. O
, O
2006 O
) O
. O
We O
use O
two O
datasets O
for O
our O
experiments O
, O
one O
taken O
from O
NEWS O
Machine O
Transliteration O
Shared O
Task O
( O
Chen O
et O
al O
. O
, O
2018 O
) O
and O
the O
other O
extracted O
from O
a O
large O
dictionary O
. O
We O
evaluate O
the O
transliteration O
system O
using O
both O
the O
conventional O
word O
accuracy O
and O
a O
novel O
metric O
designed O
for O
English O
- O
to O
- O
Chinese O
transliteration O
( O
see O
Section O
5 O
) O
. O
Our O
contributions O
are O
as O
follows O
: O
1.We O
make O
available O
a O
new O
English O
- O
to O
- O
Chinese O
named O
entities O
dataset O
( O
‚Äú O
DICT O
‚Äù O
) O
particular O
to O
names O
of O
people O
. O
This O
dataset O
is O
based O
on O
the O
dictionary O
A O
Comprehensive O
Dictionary O
of O
Names O
in O
Roman O
- O
Chinese O
( O
Xinhua O
News O
Agency O
, O
2007 O
) O
. O
2.We O
propose O
a O
substitution O
- O
based O
metric O
called O
Accuracy O
with O
Alternating O
Character O
Table O
( O
ACC O
- O
ACT O
) O
, O
which O
gives O
a O
better O
estimation O
of O
the O
system O
‚Äôs O
quality O
than O
the O
traditional O
word O
accuracy O
( O
ACC O
) O
. O
3.We O
propose O
a O
multi O
- O
task O
learning O
transliteration O
model O
with O
a O
phonetic O
auxiliary O
task O
, O
and O
run O
experiments O
to O
demonstrate O
that O
it O
attains O
better O
scores O
than O
single O
- O
main O
- O
task O
or O
single O
- O
auxiliarytask O
models O
. O
We O
report O
accuracy O
and O
F O
- O
score O
of O
0.299and O
0.6799 O
, O
respectively O
, O
on O
the O
NEWS O
dataset O
, O
with O
a O
model O
of O
size O
22 O
M O
parameters O
, O
compared O
to O
the O
previous O
state O
of O
the O
art O
( O
Grundkiewicz O
and O
HeaÔ¨Åeld O
, O
2018 O
) O
, O
which O
achieves O
accuracy O
and O
F O
- O
score O
of O
0.304and0.6791 O
, O
respectively O
, O
with O
a O
model O
of O
size133 O
M O
parameters O
. O
On O
the O
DICT O
dataset O
, O
forSource O
( O
x)Target O
( O
y)Pinyin O
( O
p O
) O
Caleigh O
ÂáØËéâ O
kai O
li O
Table O
2 O
: O
An O
example O
data O
point O
under O
our O
multi O
- O
task O
learning O
setting O
. O
the O
same O
model O
sizes O
, O
we O
report O
accuracy O
of O
0.729 O
as O
compared O
to O
their O
0.732 O
. O
2 O
Problem O
Formulation O
We O
use O
the O
word O
vocabulary O
to O
describe O
the O
set O
of O
characters O
for O
the O
purpose O
of O
our O
task O
speciÔ¨Åcation O
. O
LetVsrcandVtgtdenote O
the O
source O
and O
target O
vocabularies O
, O
respectively O
. O
For O
a O
source O
word O
xof O
lengthIand O
a O
target O
word O
yof O
lengthJ O
, O
we O
have O
: O
x= O
( O
x1,x2, O
... O
,x O
I)‚ààVI O
src O
, O
y= O
( O
y1,y2, O
... O
,y O
J)‚ààVJ O
tgt O
. O
where O
thekth O
element O
in O
the O
vector O
denotes O
a O
character O
at O
position O
k. O
We O
formulate O
the O
task O
of O
transliteration O
as O
a O
supervised O
learning O
problem O
: O
given O
a O
collection O
of O
n O
training O
examples O
, O
{ O
( O
x(i),y(i))}n O
i=0 O
, O
the O
objective O
is O
to O
learn O
a O
predictor O
function O
, O
f O
: O
x‚Üíy O
, O
of O
which O
the O
parameter O
space O
maximizes O
the O
following O
conditional O
probability O
: O
P(y|x)Chain O
Rule O
= O
J O
/ O
productdisplay O
j=1P(yj|y1, O
... O
,y O
j‚àí1,x O
) O
. O
For O
our O
multi O
- O
task O
transliteration O
model O
, O
the O
predictor O
becomes O
fMTL O
: O
x‚Üí(y O
, O
p O
) O
, O
where O
pdenotes O
the O
written O
representation O
of O
the O
pronunciation O
of O
the O
target O
word O
y. O
For O
decoding O
, O
we O
maximize O
the O
conditional O
probabilities O
, O
P(p|x,Àú O
y)and O
P(y|x,Àú O
p O
) O
, O
where O
Àú O
yandÀú O
prefers O
to O
the O
implicit O
information O
channeled O
by O
one O
task O
to O
the O
other O
. O
The O
phonetic O
information O
we O
use O
for O
our O
task O
refers O
to O
the O
Pinyin O
version O
of O
the O
name O
in O
Chinese O
, O
without O
tone O
marks,5because O
they O
are O
often O
removed O
for O
spelling O
Chinese O
names O
in O
an O
alphabetical O
language O
. O
We O
present O
an O
example O
data O
point O
in O
the O
form O
of O
( O
x O
, O
y O
, O
p)in O
Table O
2 O
. O
3 O
Dataset O
Preparation O
We O
experiment O
with O
two O
different O
English O
- O
toChinese O
datasets O
. O
For O
simplicity O
, O
we O
denote O
the O
one O
5For O
example O
, O
the O
Pinyins O
, O
ch¬Øƒ± O
, O
ch¬¥ƒ± O
, O
chÀáƒ±andch`ƒ± O
, O
are O
all O
transformed O
to O
chi O
. O
Note O
that O
this O
process O
will O
decrease O
the O
vocabulary O
size.379taken O
from O
NEWS O
Machine O
Transliteration O
Shared O
Task O
( O
Chen O
et O
al O
. O
, O
2018 O
) O
as O
‚Äú O
NEWS O
, O
‚Äù O
and O
the O
one O
extracted O
from O
the O
dictionary O
( O
Xinhua O
News O
Agency O
, O
2007 O
) O
as O
‚Äú O
DICT O
. O
‚Äù O
3.1 O
NEWS O
Dataset O
We O
use O
the O
preprocessing O
script6created O
by O
Grundkiewicz O
and O
HeaÔ¨Åeld O
( O
2018 O
) O
to O
construct O
the O
NEWS O
dataset O
from O
raw O
data O
provided O
in O
the O
Shared O
Task O
( O
Chen O
et O
al O
. O
, O
2018 O
) O
. O
This O
script O
merges O
the O
raw O
English O
- O
to O
- O
Chinese O
and O
Chinese O
- O
to O
- O
English O
datasets O
into O
a O
single O
one O
, O
then O
transforms O
it O
to O
uppercase7and O
tokenizes O
all O
names O
into O
sequences O
of O
characters O
( O
words O
are O
treated O
as O
sentences O
, O
characters O
are O
treated O
as O
words O
) O
. O
In O
addition O
, O
it O
takes O
513 O
examples O
from O
the O
training O
data O
to O
form O
the O
internal O
development O
set O
and O
uses O
the O
ofÔ¨Åcial O
development O
set O
as O
the O
internal O
test O
set O
. O
To O
make O
the O
Ô¨Ånal O
comparison O
, O
we O
download O
the O
source O
- O
side O
data O
of O
the O
ofÔ¨Åcial O
test O
set O
from O
the O
Shared O
Task O
‚Äôs O
website,8and O
submit O
the O
transliteration O
results O
( O
see O
Section O
6.4 O
) O
. O
3.2 O
DICT O
Dataset O
The O
source O
dictionary O
contains O
approximately O
680 O
K O
name O
pairs O
for O
transliteration O
from O
other O
languages O
than O
Chinese O
. O
We O
extracted O
58,456 O
pairs O
that O
originated O
in O
English O
and O
performed O
the O
following O
preprocessing O
steps O
: O
1.For O
the O
source O
side O
( O
English O
) O
, O
we O
remove O
the O
inverted O
commas O
and O
white O
spaces O
from O
names O
that O
contain O
them O
( O
e.g. O
A‚ÄôCourt O
, O
Le O
Gresley O
) O
. O
2.For O
both O
sides O
, O
we O
lowercase9all O
the O
words O
and O
tokenize O
them O
into O
sequences O
of O
characters O
. O
3.Name O
pairs O
with O
multiple O
target O
transliterations O
are O
removed O
from O
the O
dataset O
and O
saved O
in O
a O
separate O
Ô¨Åle O
for O
the O
construction O
of O
the O
ACT O
( O
see O
next O
paragraph O
) O
. O
As O
such O
, O
every O
name O
pair O
becomes O
unique O
in O
our O
preprocessed O
dataset O
. O
We O
randomly O
divide O
the O
rest O
into O
the O
ratio O
of O
8 O
: O
1 O
: O
1 O
, O
to O
form O
training O
, O
development O
and O
test O
sets O
. O
We O
report O
the O
Ô¨Ånal O
partitions O
of O
both O
datasets O
in O
Table O
3 O
. O
6Available O
at O
https://github.com/snukky/ O
news O
- O
translit O
- O
nmt O
. O
7We O
lowercase O
all O
the O
words O
in O
both O
NEWS O
and O
DICT O
datasets O
as O
evaluating O
transliteration O
is O
case O
- O
insensitive O
. O
8The O
ofÔ¨Åcial O
test O
set O
with O
task O
ID O
T O
- O
EnCh O
is O
available O
at O
: O
http://workshop.colips.org/news2018/ O
dataset.html O
. O
9Lowercasing O
does O
not O
affect O
Chinese O
characters O
as O
they O
are O
not O
alphabetical O
. O
Source O
Train O
Dev O
Test O
NEWS O
81,252 O
513 O
1,000 O
DICT O
46,620 O
5,828 O
5,828 O
Table O
3 O
: O
Numbers O
of O
data O
points O
in O
training O
, O
development O
and O
test O
sets O
of O
NEWS O
and O
DICT O
datasets O
. O
Dev O
and O
Test O
for O
the O
NEWS O
dataset O
( O
Ô¨Årst O
row O
) O
refer O
to O
the O
internal O
development O
and O
test O
set O
, O
respectively O
. O
3.3 O
Alternating O
Character O
Table O
Chinese O
characters10that O
sound O
alike O
can O
often O
replace O
each O
other O
in O
the O
transliteration O
of O
a O
name O
from O
other O
languages O
. O
Unlike O
an O
alphabetical O
language O
where O
a O
similar O
pronunciation O
is O
bounded O
to O
sub O
- O
words O
of O
various O
lengths O
, O
characters O
in O
Chinese O
have O
concrete O
and O
independent O
pronunciations O
. O
Thus O
, O
we O
can O
conveniently O
build O
the O
Alternating O
Character O
Table O
( O
ACT O
) O
with O
each O
row O
storing O
a O
list O
of O
interchangeable O
characters O
. O
We O
construct O
the O
ACT O
based O
on O
the O
DICT O
dataset O
because O
it O
contains O
less O
noise O
after O
applying O
significant O
data O
cleansing O
. O
In O
total O
, O
449English O
names O
from O
the O
DICT O
dataset O
have O
more O
than O
one O
transliterations O
in O
Chinese O
. O
We O
purposely O
removed O
all O
these O
names O
from O
the O
DICT O
data O
during O
the O
preprocessing O
so O
as O
to O
ensure O
that O
we O
are O
not O
using O
any O
knowledge O
from O
the O
test O
set O
. O
The O
Ô¨Ånal O
ACT O
contains O
29 O
rows O
( O
see O
Appendix O
) O
and O
we O
use O
it O
with O
our O
adaptive O
evaluation O
metric O
( O
see O
Section O
5 O
) O
. O
3.4 O
Pinyin O
Conversion O
In O
transliteration O
, O
the O
pronunciations O
of O
the O
Chinese O
characters O
are O
often O
unique O
( O
even O
for O
a O
polyphonic O
character O
, O
e.g. O
‰ªÄ O
, O
that O
has O
more O
than O
one O
Pinyins O
, O
sh¬¥ƒ±andsh¬¥en O
, O
only O
sh¬¥ƒ±is O
commonly O
used O
in O
transliteration O
) O
. O
Therefore O
, O
we O
can O
directly O
transform O
each O
Chinese O
character O
into O
a O
unique O
Pinyin O
, O
thus O
forming O
the O
target O
data O
for O
the O
auxiliary O
task O
. O
The O
procedure O
is O
as O
follows O
: O
for O
each O
character O
ytin O
the O
target O
name O
y O
, O
we O
use O
the O
Python O
package O
pypinyin11 O
to O
mapytto O
the O
corresponding O
Pinyin O
( O
without O
the O
tone O
mark O
) O
. O
The O
tool O
will O
generate O
the O
most O
frequently O
used O
Pinyin O
for O
each O
ytbased O
on O
dictionary O
data O
. O
We O
then O
apply O
further O
manual O
correction O
on O
the O
Pinyins O
because O
the O
most O
frequent O
Pinyin O
is O
not O
necessarily O
the O
one O
used O
in O
transliteration O
. O
10Limited O
to O
the O
set O
of O
characters O
( O
with O
size O
‚âà1 O
K O
out O
of O
80 O
K O
) O
commonly O
used O
in O
transliteration O
. O
11Available O
at O
: O
https://github.com/mozillazg/ O
python O
- O
pinyin O
. O
We O
use O
the O
lazy O
pinyin O
feature O
to O
generate O
Pinyins O
without O
tone O
marks.380Figure O
1 O
: O
Visualization O
of O
the O
Seq2MultiSeq O
model O
. O
The O
left O
half O
illustrates O
the O
components O
involved O
in O
the O
main O
task O
and O
the O
right O
half O
is O
for O
the O
auxiliary O
task O
. O
The O
shared O
part O
is O
the O
encoder O
that O
consists O
of O
a O
source O
embedding O
layer O
and O
a O
stacked O
biRNN O
( O
top O
middle O
) O
. O
4 O
Model O
Our O
model O
is O
intent O
on O
solving O
English O
- O
to O
- O
Chinese O
transliteration O
through O
joint O
supervised O
learning O
of O
source O
- O
to O
- O
target O
( O
main O
) O
and O
source O
- O
to O
- O
Pinyin O
( O
auxiliary O
) O
tasks O
. O
Training O
closely O
related O
tasks O
together O
can O
help O
the O
model O
to O
learn O
information O
that O
is O
often O
ignored O
in O
single O
- O
task O
learning O
, O
thus O
obtaining O
a O
better O
representation O
in O
the O
shared O
layers O
( O
in O
our O
case O
, O
encoder O
) O
. O
Moreover O
, O
the O
auxiliary O
task O
implicitly O
provides O
the O
phonetic O
information O
that O
is O
not O
easily O
learned O
through O
the O
single O
main O
task O
given O
the O
characteristics O
of O
Chinese O
( O
see O
Section O
1 O
) O
. O
Our O
model O
has O
a O
sequence O
- O
to O
- O
multiple O
- O
sequence O
( O
Seq2MultiSeq O
) O
architecture O
that O
contains O
a O
shared O
encoder O
and O
dual O
decoders O
. O
Between O
the O
encoder O
and O
decoder O
is O
a O
bridge O
layer12that O
transforms O
the O
12We O
call O
it O
‚Äú O
bridge O
‚Äù O
because O
it O
connects O
the O
shared O
encoder O
to O
each O
decoder O
. O
It O
allows O
Ô¨Çexible O
choices O
of O
the O
hidden O
sizes O
of O
the O
encoder O
and O
decoder O
and O
serves O
as O
the O
intermediate O
‚Äú O
buffer O
‚Äù O
before O
passing O
the O
encoder O
Ô¨Ånal O
state O
to O
each O
decoder.encoder O
‚Äôs O
Ô¨Ånal O
state O
into O
the O
decoder O
‚Äôs O
initial O
state O
( O
see O
Figure O
1 O
) O
. O
The O
encoder O
has O
an O
embedding O
layer O
with O
dropout O
( O
Hinton O
et O
al O
. O
, O
2012 O
) O
, O
followed O
by O
a O
2layer O
biLSTM O
( O
Schuster O
and O
Paliwal O
, O
1997 O
) O
. O
The O
bridge O
layer O
consists O
of O
a O
linear O
layer O
followed O
by O
tanh O
activation O
. O
The O
shared O
encoder O
passes O
its O
Ô¨Ånal O
state O
to O
the O
main O
- O
task O
decoder O
and O
the O
auxiliarytask O
decoder O
via O
separate O
bridge O
layers O
. O
In O
each O
decoder O
, O
we O
use O
additive O
attention O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
) O
to O
compute O
the O
context O
vector O
( O
weighted O
sum O
of O
the O
encoder O
outputs O
according O
to O
the O
attention O
scores O
) O
, O
then O
concatenate O
it O
with O
the O
target O
embedding O
to O
form O
the O
input O
of O
the O
subsequent O
2 O
- O
layer O
feed O
- O
forward O
LSTM O
. O
The O
prediction O
is O
made O
by O
feeding O
the O
concatenation O
of O
the O
LSTM O
‚Äôs O
output O
, O
the O
context O
vector O
and O
the O
target O
embedding O
into O
a O
linear O
layer O
followed O
by O
log O
- O
softmax O
. O
Our O
model O
is O
expected O
to O
simultaneously O
maximize O
the O
conditional O
probabilities O
mentioned O
in O
Section O
2 O
. O
To O
achieve O
this O
goal O
, O
we O
use O
the O
linear O
combination O
of O
the O
main O
- O
task O
decoder O
‚Äôs O
loss13 O
( O
negative O
log O
likelihood O
; O
ly O
) O
and O
the O
auxiliary O
- O
task O
decoder O
‚Äôs O
loss O
( O
lp O
) O
as O
the O
model O
‚Äôs O
objective O
function O
: O
lMTL O
= O
Œª¬∑ly+ O
( O
1‚àíŒª)¬∑lp O
, O
where O
the O
subscript O
MTL O
stands O
for O
multi O
- O
task O
learning O
and O
0 O
< O
Œª O
< O
1 O
. O
Note O
that O
for O
Œª= O
0 O
andŒª= O
1 O
, O
it O
is O
equivalent O
to O
train O
on O
a O
single O
auxiliary O
task O
and O
a O
single O
main O
task O
, O
respectively O
. O
The O
whole O
system O
is O
implemented O
using O
the O
deep O
learning O
framework O
PyTorch O
( O
Paszke O
et O
al O
. O
, O
2019).14 O
5 O
Adaptive O
Evaluation O
Metrics O
We O
evaluate O
the O
transliteration O
system O
using O
word O
accuracy O
( O
ACC O
) O
and O
its O
variants O
on O
the O
1 O
- O
best O
output O
: O
ACC O
= O
1 O
N O
/ O
summationdisplay O
( O
y,ÀÜy)Icriterion O
( O
ÀÜy O
, O
y O
) O
, O
whereNis O
the O
total O
number O
of O
test O
- O
set O
samples O
, O
Icriterion O
( O
ÀÜy O
, O
y)is O
an O
indicator O
function O
with O
value O
1if O
the O
prediction O
( O
top O
candidate O
) O
ÀÜymatches O
the O
referenceyunder O
certain O
criterion O
. O
The O
simplest O
criterion O
is O
exact O
string O
match O
between O
ÀÜyandy O
. O
If O
the O
test O
set O
contains O
multiple O
target O
words O
for O
a O
single O
source O
word O
, O
we O
let O
indicator O
be O
1if O
the O
prediction O
matches O
one O
of O
the O
references O
( O
Chen O
et O
al O
. O
, O
2018 O
) O
. O
13We O
use O
nn O
. O
NLLLoss O
( O
) O
from O
the O
PyTorch O
library O
. O
14Available O
at O
https://pytorch.org/ O
.381Source O
Target O
( O
F O
) O
Target O
( O
M O
) O
MED O
Mona O
Ëé´Â®ú O
Ëé´Á∫≥ O
1 O
Colina O
ÁßëËéâÂ®úÁßëÂà©Á∫≥ O
2 O
Table O
4 O
: O
Examples O
of O
a O
single O
source O
name O
with O
more O
than O
one O
target O
transliterations O
, O
with O
( O
F O
) O
and O
( O
M O
) O
indicating O
female O
and O
male O
, O
respectively O
. O
We O
use O
ACC O
and O
ACC+ O
to O
denote O
the O
original O
accuracy O
and O
its O
variant O
with O
multiple O
references O
. O
The O
drawback O
of O
ACC O
is O
that O
it O
may O
underestimate O
the O
quality O
of O
the O
system O
because O
it O
neglects O
the O
possibility O
of O
having O
more O
than O
one O
transliteration O
for O
a O
given O
source O
name O
, O
as O
is O
the O
case O
for O
English O
- O
to O
- O
Chinese O
transliteration O
. O
For O
example O
in O
Table O
4 O
, O
if O
the O
test O
set O
only O
includes O
Target O
( O
F O
) O
for O
a O
Source O
while O
the O
model O
predicts O
Target O
( O
M O
) O
, O
ACC O
will O
mistakenly O
count O
it O
as O
wrong O
. O
Although O
ACC+ O
considers O
the O
alternatives O
appearing O
in O
the O
dataset O
, O
it O
is O
unrealistic O
to O
expect O
the O
dataset O
to O
contain O
all O
possible O
references O
. O
To O
resolve O
this O
issue O
, O
we O
propose O
a O
new O
variant O
of O
word O
accuracy O
speciÔ¨Åc O
to O
English O
- O
to O
- O
Chinese O
transliteration O
. O
Based O
on O
the O
knowledge O
of O
a O
native O
Chinese O
speaker O
, O
we O
analyze O
the O
English O
- O
to O
- O
Chinese O
dataset O
and O
summarize O
the O
key O
observations O
for O
source O
names O
with O
multiple O
target O
transliterations O
as O
follows O
: O
the O
minimum O
edit O
distance O
( O
MED O
) O
between O
any O
two O
target O
names O
‚â§2 O
, O
and O
the O
lengths O
are O
the O
same O
; O
for O
any O
two O
such O
target O
names O
, O
distinct O
characters O
occur O
in O
the O
same O
position O
, O
and O
they O
often O
indicate O
the O
gender O
of O
the O
name O
( O
see O
Table O
4 O
) O
. O
To O
use O
ACT O
in O
accord O
with O
the O
above O
observations O
, O
we O
propose O
the O
following O
criterion O
for O
the O
accuracy O
indicator O
function O
( O
we O
refer O
to O
it O
as O
ACC O
- O
ACT O
) O
. O
Let O
subscript O
tdenote O
the O
position O
of O
a O
character O
, O
then O
Icriterion O
( O
ÀÜy O
, O
y)= O
1 O
if O
either O
MED O
( O
ÀÜy O
, O
y O
) O
= O
0 O
( O
which O
covers O
all O
the O
cases O
for O
ACC O
) O
or O
the O
following O
conditions O
are O
met O
in O
order O
: O
1.ÀÜyandyare O
of O
the O
same O
length O
, O
L O
; O
2.MED O
( O
ÀÜy O
, O
y)‚â§2and O
distinct O
characters O
of O
ÀÜyand O
ymust O
occur O
in O
the O
same O
position(s O
) O
; O
3.IfÀÜyt O
/ O
negationslash O
= O
ytfor1‚â§t‚â§L O
, O
replace O
ÀÜytby O
looking O
up O
the O
ACT O
and O
this O
condition O
will O
be O
satisÔ¨Åed O
if O
any O
of O
the O
modiÔ¨Åed O
ÀÜy(s O
) O
can O
match O
yexactly O
. O
There O
is O
no O
guarantee O
that O
characters O
that O
are O
interchangeable O
according O
to O
ACT O
can O
replace O
each O
other O
in O
every O
scenario O
. O
But O
since O
we O
only O
applyEnc O
Dec O
- O
M O
Dec O
- O
A O
Emb.h256 O
256 O
128 O
Œ¥0.1 O
0.1 O
0.1 O
RNNh512 O
512 O
128 O
Œ¥0.2 O
0.2 O
0.1 O
Table O
5 O
: O
Illustration O
of O
the O
model O
settings O
, O
where O
Emb O
. O
and O
RNN O
stand O
for O
the O
embedding O
layers O
and O
RNN O
units O
in O
each O
part O
( O
column O
) O
of O
the O
model O
, O
handŒ¥are O
the O
hidden O
size O
and O
dropout O
value O
, O
respectively O
. O
The O
column O
names O
( O
from O
left O
to O
right O
) O
stand O
for O
encoder O
, O
main O
- O
task O
decoder O
and O
auxiliary O
- O
task O
decoder O
. O
substitution O
on O
the O
output O
predictions O
rather O
than O
the O
references O
, O
we O
are O
not O
manipulating O
the O
test O
set O
by O
creating O
any O
new O
instance O
. O
This O
new O
metric O
( O
ACC O
- O
ACT O
) O
will O
ensure O
cases O
like O
in O
Table O
4 O
are O
captured O
without O
requiring O
extra O
data O
in O
the O
test O
set O
, O
thus O
giving O
a O
more O
reasonable O
estimate O
of O
the O
system O
‚Äôs O
quality O
than O
both O
ACC O
and O
ACC+ O
. O
6 O
Experimental O
Setup O
Recall O
from O
Section O
4 O
that O
we O
use O
Œªto O
denote O
the O
weighting O
of O
the O
two O
tasks O
we O
train O
. O
We O
set O
the O
single O
- O
main O
- O
task O
( O
Œª= O
1 O
) O
and O
the O
single O
- O
auxiliarytask O
( O
Œª= O
0 O
) O
models O
as O
the O
baselines O
, O
and O
compare O
the O
multi O
- O
task O
models O
of O
different O
weightings O
( O
Œª‚àà{1 O
6,1 O
4,1 O
2,2 O
3,5 O
6,8 O
9 O
} O
) O
against O
them O
. O
We O
conduct O
experiments O
on O
both O
the O
NEWS O
and O
DICT O
datasets O
and O
select O
the O
best O
model O
for O
each O
of O
them O
to O
compare O
to O
the O
previous O
state O
of O
the O
art O
. O
6.1 O
Model O
and O
Training O
Settings O
The O
conÔ¨Ågurations O
of O
hidden O
sizes O
and O
dropout O
values O
of O
embedding O
layers O
and O
RNN O
units O
are O
presented O
in O
Table O
5 O
. O
The O
type O
of O
all O
RNN O
units O
is O
LSTM O
and O
the O
number O
of O
layers O
is O
set O
to O
2 O
. O
Besides O
the O
bridge O
layer O
that O
transforms O
the O
encoder O
‚Äôs O
Ô¨Ånal O
hidden O
state O
to O
the O
decoder O
‚Äôs O
initial O
hidden O
state O
, O
we O
add O
another O
one O
to O
carry O
the O
Ô¨Ånal O
cell O
state O
for O
using O
LSTM O
( O
in O
total O
, O
we O
have O
4‚Äúbridges O
‚Äù O
) O
. O
We O
use O
the O
Adam O
optimizer O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
with O
the O
batch O
size O
set O
to O
64 O
. O
Evaluation O
of O
the O
development O
set O
is O
carried O
out O
on O
every O
500 O
batches O
. O
We O
record O
the O
validation O
score O
( O
ACC O
) O
and O
decrease O
the O
learning O
rate O
( O
initially O
set O
to O
0.003 O
) O
by O
90 O
% O
if O
the O
score O
does O
not O
surpass O
the O
previous O
best O
. O
We O
pick O
the O
Ô¨Ånal O
model O
that O
attains O
the O
highest O
validation O
score O
within O
100training O
epochs O
. O
For O
decoding O
in O
the O
training O
phase O
, O
we O
apply O
teacher O
forcing O
( O
Williams O
and O
Zipser O
, O
1989 O
) O
with382NEWS O
DICT O
Main O
Auxiliary O
Main O
Auxiliary O
Œª O
ACC O
ACC+ O
ACC O
- O
ACT O
ACC O
ACC O
ACC O
- O
ACT O
ACC O
1 O
0.723 O
0.731 O
0.746 O
NA O
0.725 O
0.748 O
NA O
1/6 O
0.666 O
0.672 O
0.688 O
0.698 O
0.728 O
0.750 O
0.744 O
1/4 O
0.734 O
0.743 O
0.751 O
0.755 O
0.725 O
0.747 O
0.746 O
1/2 O
0.724 O
0.733 O
0.740 O
0.738 O
0.723 O
0.748 O
0.739 O
2/3 O
0.698 O
0.707 O
0.715 O
0.705 O
0.722 O
0.746 O
0.739 O
5/6 O
0.739 O
0.749 O
0.760 O
0.757 O
0.729 O
0.752 O
0.746 O
8/9 O
0.670 O
0.679 O
0.686 O
0.705 O
0.722 O
0.746 O
0.734 O
0 O
NA O
NA O
NA O
0.743 O
NA O
NA O
0.743 O
Table O
6 O
: O
Experiment O
results O
on O
NEWS O
internal O
test O
set O
and O
DICT O
development O
set O
, O
where O
Œª= O
1 O
andŒª= O
0 O
are O
baselines O
of O
main O
task O
and O
auxiliary O
task O
, O
respectively O
. O
Maximum O
score O
in O
each O
metric O
is O
is O
bold O
. O
Figure O
2 O
: O
The O
plots O
of O
main O
- O
task O
ACC O
against O
auxiliary O
- O
task O
ACC O
on O
the O
NEWS O
( O
left O
) O
and O
DICT O
( O
right O
) O
development O
sets O
. O
Colors O
indicate O
which O
multi O
- O
task O
model O
( O
by O
Œªvalue O
) O
the O
evaluation O
points O
belong O
to O
. O
To O
highlight O
the O
dense O
regions O
, O
we O
set O
the O
minimum O
of O
the O
x O
- O
axis O
to O
0.5and0.6for O
NEWS O
and O
DICT O
datasets O
, O
respectively O
. O
the O
following O
empirical O
decay O
function O
: O
tfr= O
max O
/ O
parenleftbigg O
1‚àí10 O
+ O
epoch√ó1.5 O
50,0.2 O
/ O
parenrightbigg O
, O
where O
tfrrefers O
to O
the O
teacher O
forcing O
ratio O
, O
i.e. O
the O
probability O
of O
feeding O
the O
true O
reference O
instead O
of O
the O
predicted O
token O
. O
We O
use O
beam O
search O
decoding O
with O
beam O
size O
10and O
length O
normalization O
( O
Wu O
et O
al O
. O
, O
2016 O
) O
for O
evaluation O
. O
6.2 O
Evaluation O
We O
use O
ACC O
and O
ACC O
- O
ACT O
to O
evaluate O
the O
performance O
on O
the O
main O
task O
and O
ACC O
on O
the O
auxiliary O
task O
. O
Note O
that O
since O
the O
only O
data O
portion O
we O
have O
that O
contains O
multiple O
references O
given O
a O
source O
word O
is O
the O
internal O
test O
set O
of O
NEWS O
data O
, O
we O
apply O
ACC+ O
on O
this O
particular O
set O
exclusively.6.3 O
Model O
Selection O
In O
the O
experiments O
in O
this O
section O
, O
we O
tune O
Œªon O
the O
NEWS O
internal O
test O
set O
and O
DICT O
development O
set O
, O
and O
select O
the O
model O
with O
the O
highest O
ACC O
on O
the O
main O
task O
. O
The O
experiment O
results O
in O
Table O
6 O
show O
that O
Œª=5 O
6yields O
the O
best O
models O
on O
both O
datasets O
. O
We O
observe O
a O
signiÔ¨Åcant O
improvement O
against O
the O
baselines O
on O
NEWS O
while O
a O
less O
noticeable O
increase O
on O
DICT O
. O
Besides O
, O
the O
models O
are O
more O
sensitive O
to O
Œªon O
NEWS O
than O
DICT O
( O
with O
standard O
deviation O
0.03 O
and O
0.003 O
on O
ACC O
, O
respectively O
) O
. O
Furthermore O
, O
we O
investigate O
the O
relationship O
between O
the O
main O
and O
the O
auxiliary O
tasks O
based O
on O
the O
evaluation O
points O
of O
the O
development O
set O
. O
In O
Figure O
2 O
, O
we O
observe O
a O
nearly O
- O
total O
positive O
linear O
correlation O
between O
the O
main O
- O
task O
ACC O
and O
auxiliary O
- O
task O
ACC O
, O
and O
this O
is O
further O
evident O
in O
the O
Pearson O
cor-383Internal O
Test O
OfÔ¨Åcial O
Test O
Main O
Auxiliary O
Main O
System O
ACC O
ACC+ O
ACC O
- O
ACT O
ACC O
ACC+ O
Baseline O
0.724 O
0.733 O
0.742 O
0.736 O
NA O
Multi O
- O
task O
0.739 O
0.749 O
0.760 O
0.757 O
0.299 O
BiDeep O
0.731 O
0.739 O
0.746 O
0.740 O
NA O
BiDeep+ O
NA O
0.765 O
NA O
NA O
0.304 O
Table O
7 O
: O
Experiment O
results O
on O
the O
NEWS O
internal O
test O
( O
ofÔ¨Åcial O
development O
) O
set O
and O
ofÔ¨Åcial O
test O
set O
, O
where O
‚Äú O
Baseline O
‚Äù O
refers O
to O
the O
single O
- O
task O
model O
and O
‚Äú O
BiDeep+ O
‚Äù O
refers O
to O
the O
best O
system O
Grundkiewicz O
and O
HeaÔ¨Åeld O
( O
2018 O
) O
submitted O
to O
the O
NEWS O
workshop O
, O
and O
the O
corresponding O
scores O
are O
taken O
from O
their O
paper O
. O
Main O
Auxiliary O
System O
ACC O
ACC O
- O
ACT O
ACC O
Baseline O
0.726 O
0.748 O
0.738 O
Multi O
- O
task O
0.729 O
0.751 O
0.749 O
BiDeep O
0.732 O
0.755 O
0.760 O
Table O
8 O
: O
Experiment O
results O
on O
the O
DICT O
test O
set O
, O
where O
Baseline O
refers O
to O
the O
single O
- O
task O
model O
. O
User O
ACC+ O
F O
- O
score O
romang O
0.3040 O
( O
1 O
) O
0.6791 O
( O
2 O
) O
Ours O
0.2990 O
( O
2 O
) O
0.6799 O
( O
1 O
) O
saeednajaÔ¨Å O
0.2820 O
( O
3 O
) O
0.6680 O
( O
3 O
) O
soumyadeep O
0.2610 O
( O
4 O
) O
0.6603 O
( O
4 O
) O
Table O
9 O
: O
Table O
of O
the O
NEWS O
leaderboard O
( O
available O
at O
https://competitions.codalab.org/ O
competitions/18905#results O
, O
accessed O
19 O
June O
2020 O
) O
. O
User O
‚Äú O
romang O
‚Äù O
refers O
to O
Grundkiewicz O
and O
HeaÔ¨Åeld O
( O
2018 O
) O
. O
relation O
coefÔ¨Åcients15 O
, O
which O
are O
0.982and0.992 O
for O
NEWS O
and O
DICT O
, O
respectively O
. O
This O
means O
the O
multi O
- O
task O
model O
improves O
the O
performance O
on O
both O
tasks O
simultaneously O
. O
6.4 O
Test O
- O
set O
Results O
and O
System O
Comparison O
We O
submit O
our O
1 O
- O
best O
transliteration O
results O
on O
the O
NEWS O
ofÔ¨Åcial O
test O
set O
through O
the O
CodaLab O
link O
provided O
by O
the O
Shared O
Task O
‚Äôs O
Committee O
and O
we O
present O
the O
leaderboard O
partially O
in O
Table O
9 O
. O
Note O
that O
in O
addition O
to O
ACC+ O
, O
the O
leaderboard O
also O
15Computed O
by O
pearsonr O
( O
) O
fromScipy O
library O
, O
which O
is O
available O
at O
: O
https://docs.scipy.org/doc/ O
scipy-0.14.0 O
/ O
reference O
/ O
generated O
/ O
scipy O
. O
stats.pearsonr.html O
.records O
mean O
F O
- O
score16on O
which O
we O
rank O
Ô¨Årst O
. O
We O
report O
the O
test O
- O
set O
performance O
of O
our O
best O
multi O
- O
task O
model O
on O
NEWS O
in O
Table O
7 O
and O
DICT O
in O
Table O
8 O
, O
in O
comparison O
to O
the O
system O
built O
by O
Grundkiewicz O
and O
HeaÔ¨Åeld O
( O
2018 O
) O
. O
The O
baseline O
model O
of O
their O
work O
employs O
the O
RNN O
- O
based O
BiDeep17architecture O
( O
Miceli O
Barone O
et O
al O
. O
, O
2017 O
) O
which O
consists O
of O
4 O
bidirectional O
alternating O
stacked O
encoder O
, O
each O
with O
a O
2 O
- O
layer O
transition O
RNN O
cell O
, O
and O
4 O
stacked O
decoders O
with O
base O
RNN O
of O
depth O
2 O
and O
higher O
RNN O
of O
depth O
4 O
( O
Zhou O
et O
al O
. O
, O
2016 O
; O
Pascanu O
et O
al O
. O
, O
2014 O
; O
Wu O
et O
al O
. O
, O
2016 O
) O
. O
Besides O
, O
they O
strengthen O
the O
model O
by O
applying O
layer O
normalization O
( O
Ba O
et O
al O
. O
, O
2016 O
) O
, O
skip O
connections O
( O
Zhang O
et O
al O
. O
, O
2016 O
) O
and O
parameter O
tying O
( O
Press O
and O
Wolf O
, O
2017 O
) O
. O
We O
reproduce O
their O
model O
without O
changing O
any O
conÔ¨Ågurations O
in O
their O
paper O
( O
Grundkiewicz O
and O
HeaÔ¨Åeld O
, O
2018 O
) O
, O
and O
train O
it O
on O
both O
tasks O
separately O
. O
In O
Table O
7 O
, O
we O
can O
see O
that O
the O
multi O
- O
task O
model O
performs O
signiÔ¨Åcantly O
better O
than O
both O
the O
singletask O
baseline O
and O
the O
BiDeep O
model O
in O
all O
metrics O
on O
NEWS O
. O
Note O
that O
the O
BiDeep O
model O
we O
reproduce O
achieves O
the O
same O
ACC+ O
as O
reported O
in O
the O
work O
of O
Grundkiewicz O
and O
HeaÔ¨Åeld O
( O
2018 O
) O
and O
ACC+ O
is O
the O
only O
evaluation O
metric O
used O
in O
their O
paper O
. O
‚Äú O
BiDeep+ O
‚Äù O
in O
the O
third O
row O
refers O
to O
the O
Ô¨Ånal O
system O
they O
submitted O
to O
the O
Shared O
Task O
, O
on O
which O
they O
adopted O
additional O
NMT O
techniques O
including O
ensemble O
modeling O
for O
re O
- O
ranking O
and O
synthetic O
data O
generated O
from O
back O
translation O
( O
Sennrich O
et O
al O
. O
, O
2017 O
) O
. O
Our O
ACC+ O
score O
on O
16The O
F O
- O
score O
metric O
measures O
the O
similarity O
between O
the O
target O
prediction O
and O
reference O
. O
Precision O
and O
Recall O
in O
this O
particular O
F O
- O
score O
are O
computed O
based O
on O
the O
length O
of O
the O
Longest O
Common O
Subsequence O
. O
See O
details O
in O
the O
NEWS O
whitepaper O
( O
Chen O
et O
al O
. O
, O
2018 O
) O
. O
17Implemented O
with O
the O
Marian O
toolkit O
available O
at O
https O
: O
//marian O
- O
nmt.github.io O
/ O
docs/ O
.384Source O
Output O
( O
ST O
) O
Output O
( O
MT O
) O
ocallaghan O
Â••Âç°ÊãâÊ†πÂ••Âç°ÊãâÊ±â O
/ O
check O
holleran O
ÈúçÂ∞î‰º¶ O
ÈúçÂãí‰º¶ O
/ O
check O
ajemian O
ÈòøËµ´Á±≥ÂÆâÈòøÊù∞Á±≥ÂÆâ O
Table O
10 O
: O
Example O
outputs O
and O
the O
corresponding O
source O
words O
of O
our O
systems O
, O
where O
‚Äú O
ST O
‚Äù O
and O
‚Äú O
MT O
‚Äù O
refer O
to O
‚Äú O
single O
- O
task O
‚Äù O
and O
‚Äú O
multi O
- O
task O
‚Äù O
models O
. O
The O
tick O
symbols O
indicate O
which O
outputs O
match O
the O
references O
. O
the O
anonymized O
ofÔ¨Åcial O
test O
set O
is O
0.299which O
is O
slightly O
worse O
than O
their O
0.304 O
. O
However O
, O
we O
attain O
a O
better O
F O
- O
score O
( O
0.6799 O
) O
than O
them O
( O
0.6791 O
) O
as O
shown O
in O
Table O
9 O
. O
Moreover O
, O
our O
model O
is O
of O
size22 O
M O
parameters O
, O
which O
is O
much O
smaller O
than O
their O
baseline O
BiDeep O
of O
size O
133 O
M O
parameters,18 O
and O
we O
do O
not O
apply O
as O
many O
NMT O
techniques O
as O
they O
did O
. O
Nevertheless O
, O
on O
the O
DICT O
test O
set O
, O
there O
is O
no O
prominent O
difference O
among O
the O
single O
- O
task O
baseline O
, O
multi O
- O
task O
and O
BiDeep O
model O
, O
possibly O
because O
the O
noise O
pattern O
in O
the O
DICT O
dataset O
is O
not O
complex O
enough O
to O
reÔ¨Çect O
the O
learning O
ability O
of O
these O
models O
. O
7 O
Discussion O
In O
our O
experiments O
, O
a O
system O
has O
ACCACT O
> O
ACC+>ACC O
because O
both O
ACC O
- O
ACT O
and O
ACC+ O
consider O
the O
cases O
of O
ACC O
but O
ACC O
- O
ACT O
can O
capture O
more O
acceptable O
transliterations O
. O
Despite O
a O
consistent O
ranking O
given O
by O
the O
three O
metrics O
, O
ACC O
- O
ACT O
reveals O
different O
information O
from O
ACC O
and O
ACC+ O
. O
For O
example O
, O
in O
Table O
6 O
, O
the O
model O
of O
Œª=5 O
6outperforms O
Œª=1 O
2by O
0.015and0.016 O
in O
ACC O
and O
ACC+ O
, O
respectively O
, O
but O
the O
difference O
is O
0.020 O
in O
ACC O
- O
ACT O
, O
on O
the O
NEWS O
dataset O
. O
This O
suggests O
a O
more O
prominent O
gap O
between O
these O
two O
models O
. O
In O
contrast O
, O
by O
looking O
at O
the O
same O
two O
rows O
but O
on O
the O
DICT O
dataset O
, O
ACC O
- O
ACT O
indicates O
a O
smaller O
gap O
( O
0.004 O
) O
than O
ACC O
( O
0.006 O
) O
. O
If O
we O
conduct O
experiments O
on O
another O
dataset O
, O
the O
disagreement O
among O
the O
metrics O
might O
be O
signiÔ¨Åcant O
enough O
to O
render O
an O
inconsistent O
ranking O
. O
Furthermore O
, O
we O
present O
some O
typical O
examples O
in O
which O
the O
multi O
- O
task O
model O
generates O
better O
predictions O
than O
the O
single O
- O
task O
in O
Table O
10 O
. O
In O
the O
Ô¨Årst O
18We O
compute O
the O
size O
of O
our O
multi O
- O
task O
model O
by O
counting O
the O
number O
of O
trainable O
parameters O
extracted O
from O
model.parameters O
( O
) O
; O
For O
the O
BiDeep O
model O
, O
we O
use O
thenumpy O
package O
to O
load O
the O
model O
in O
.npz O
format O
and O
calculate O
the O
number O
of O
parameters O
via O
a O
simple O
for-loop.example O
, O
the O
single O
- O
task O
model O
wrongly O
maps O
the O
sub O
- O
word O
ghan O
toÊ†π(emphasizing O
on O
the O
character O
g O
) O
while O
the O
multi O
- O
task O
model O
correctly O
maps O
han O
toÊ±â O
. O
The O
erroneous O
grouping O
of O
the O
English O
characters O
also O
occurs O
in O
the O
second O
example O
where O
the O
single O
- O
task O
model O
maps O
ertoÂ∞îinstead O
of O
more O
reasonably O
lertoÂãí O
. O
Even O
in O
the O
third O
example O
where O
both O
outputs O
are O
mismatched O
, O
the O
multi O
- O
task O
model O
predicts O
the O
character O
Êù∞ O
, O
which O
is O
closer O
to O
the O
source O
sub O
- O
word O
jethan O
the O
single O
- O
task O
model O
‚Äôs O
Ëµ´in O
terms O
of O
pronunciation O
. O
Overall O
, O
it O
seems O
that O
the O
multi O
- O
task O
model O
can O
capture O
the O
source O
- O
word O
pronunciation O
better O
than O
the O
single O
- O
task O
one O
. O
Still O
, O
the O
multi O
- O
task O
model O
does O
not O
consistently O
handle O
all O
names O
better O
than O
the O
single O
- O
task O
model O
‚Äì O
especially O
for O
exceptional O
names O
that O
do O
not O
have O
a O
regular O
transliteration O
. O
For O
instance O
, O
the O
name O
Fyleman O
is O
transliterated O
into O
Ê≥ï‰ºäÂ∞îÊõº O
, O
but O
the O
character‰ºädoes O
not O
have O
any O
source O
- O
word O
correspondence O
if O
we O
consider O
the O
pronunciation O
of O
the O
source O
name O
. O
Finally O
, O
our O
model O
can O
be O
generalized O
to O
other O
transliteration O
tasks O
by O
replacing O
Pinyin O
with O
other O
phonetic O
representations O
such O
as O
IPA O
for O
English O
and O
r O
¬Øomaji O
for O
Japanese O
. O
In O
addition O
, O
ACC O
- O
ACT O
can O
be O
extended O
to O
alphabetical O
languages O
by O
, O
for O
instance O
, O
constructing O
the O
Alternating O
Sub O
- O
word O
Table O
which O
stores O
lists O
of O
interchangeable O
subsequences O
. O
Another O
possible O
future O
work O
is O
to O
redesign O
the O
objective O
function O
by O
treating O
Œªas O
a O
trainable O
parameter O
or O
including O
the O
correlation O
information O
( O
Papasarantopoulos O
et O
al O
. O
, O
2019 O
) O
. O
8 O
Related O
Work O
Previous O
work O
has O
demonstrated O
the O
effectiveness O
of O
using O
MTL O
on O
models O
through O
joint O
learning O
of O
various O
NLP O
tasks O
such O
as O
machine O
translation O
, O
syntactic O
and O
dependency O
parsing O
( O
Luong O
et O
al O
. O
, O
2016 O
; O
Dong O
et O
al O
. O
, O
2015 O
; O
Li O
et O
al O
. O
, O
2014 O
) O
. O
In O
most O
of O
this O
work O
, O
underlies O
a O
similar O
idea O
to O
create O
a O
uniÔ¨Åed O
training O
setting O
for O
several O
tasks O
by O
sharing O
the O
core O
parameters O
. O
Besides O
, O
machine O
transliteration O
has O
a O
long O
history O
of O
using O
phonetic O
information O
, O
for O
example O
, O
by O
mapping O
a O
phrase O
to O
its O
pronunciation O
in O
the O
source O
language O
and O
then O
convert O
the O
sound O
to O
the O
target O
word O
( O
Knight O
and O
Graehl O
, O
1997 O
) O
. O
There O
is O
also O
relevant O
work O
that O
uses O
both O
graphemes O
and O
phonemes O
to O
various O
extents O
for O
transliteration O
, O
such O
as O
the O
correspondence O
- O
based O
( O
Oh O
et O
al O
. O
, O
2006 O
) O
and O
G2P O
- O
based O
( O
Le O
and O
Sadat O
, O
2018 O
) O
approaches O
. O
Our O
work O
is O
inspired O
by O
the O
intu-385itive O
understanding O
that O
pronunciation O
is O
essential O
for O
transliteration O
, O
and O
the O
success O
of O
incorporating O
phonetic O
information O
such O
as O
Pinyin O
( O
Jiang O
et O
al O
. O
, O
2009 O
) O
and O
IPA O
( O
Salam O
et O
al O
. O
, O
2011 O
) O
, O
in O
the O
model O
design O
. O
9 O
Conclusion O
We O
argue O
in O
this O
paper O
that O
language O
- O
speciÔ¨Åc O
features O
should O
be O
used O
when O
solving O
transliteration O
in O
a O
neural O
setting O
, O
and O
we O
exemplify O
a O
way O
of O
using O
phonetic O
information O
as O
the O
transferred O
knowledge O
to O
improve O
a O
neural O
machine O
transliteration O
system O
. O
Our O
results O
demonstrate O
that O
the O
main O
transliteration O
task O
and O
the O
auxiliary O
phonetic O
task O
are O
indeed O
mutually O
beneÔ¨Åcial O
in O
English O
- O
to O
- O
Chinese O
transliteration O
, O
and O
we O
discuss O
the O
possibility O
of O
applying O
this O
idea O
on O
other O
language O
pairs O
. O
Acknowledgements O
We O
thank O
the O
anonymous O
reviewers O
for O
their O
insightful O
feedback O
. O
We O
would O
also O
like O
to O
thank O
Zheng O
Zhao O
, O
Zhijiang O
Guo O
, O
Waylon O
Li O
and O
Pinzhen O
Chen O
for O
their O
help O
and O
comments O
. O
Abstract O
Attention O
- O
based O
encoder O
- O
decoder O
models O
have O
achieved O
great O
success O
in O
neural O
machine O
translation O
tasks O
. O
However O
, O
the O
lengths O
of O
the O
target O
sequences O
are O
not O
explicitly O
predicted O
in O
these O
models O
. O
This O
work O
proposes O
length O
prediction O
as O
an O
auxiliary O
task O
and O
set O
up O
a O
sub O
- O
network O
to O
obtain O
the O
length O
information O
from O
the O
encoder O
. O
Experimental O
results O
show O
that O
the O
length O
prediction O
sub O
- O
network O
brings O
improvements O
over O
the O
strong O
baseline O
system O
and O
that O
the O
predicted O
length O
can O
be O
used O
as O
an O
alternative O
to O
length O
normalization O
during O
decoding O
. O
1 O
Introduction O
In O
recent O
years O
, O
neural O
network O
( O
NN O
) O
models O
have O
achieved O
great O
improvements O
in O
machine O
translation O
( O
MT O
) O
tasks O
. O
Sutskever O
et O
al O
. O
( O
2014 O
) O
introduced O
the O
encoder O
- O
decoder O
network O
, O
Bahdanau O
et O
al O
. O
( O
2015 O
) O
developed O
the O
attention O
- O
based O
architecture O
, O
and O
Vaswani O
et O
al O
. O
( O
2017 O
) O
proposed O
the O
transformer O
model O
with O
self O
- O
attentions O
, O
which O
delivers O
state O
- O
of O
- O
the O
- O
art O
performances O
. O
Despite O
the O
success O
achieved O
in O
neural O
machine O
translation O
( O
NMT O
) O
, O
current O
NMT O
systems O
do O
not O
model O
the O
length O
of O
the O
output O
explicitly O
, O
and O
thus O
various O
length O
normalization O
approaches O
are O
often O
used O
in O
decoding O
. O
Length O
normalization O
is O
a O
common O
technique O
used O
in O
the O
beam O
search O
of O
NMT O
systems O
to O
enable O
a O
fair O
comparison O
of O
partial O
hypotheses O
with O
different O
lengths O
. O
Without O
any O
form O
of O
length O
normalization O
, O
regular O
beam O
searches O
will O
prefer O
shorter O
hypotheses O
to O
longer O
ones O
on O
average O
, O
as O
a O
negative O
logarithmic O
probability O
is O
added O
at O
each O
step O
, O
resulting O
in O
lower O
( O
more O
negative O
) O
scores O
for O
longer O
sentences O
. O
The O
simplest O
way O
is O
to O
normalize O
the O
score O
of O
the O
current O
partial O
hypothesis O
( O
ei O
1 O
) O
by O
its O
length O
( O
|i| O
): O
s(ei O
1,fJ O
1 O
) O
= O
logp(ei O
1|fJ O
1 O
) O
|i|(1)wherefJ O
1is O
the O
source O
sequence O
. O
To O
use O
a O
softer O
approach O
, O
the O
denominator O
|i|can O
also O
be O
raised O
to O
the O
power O
of O
a O
number O
between O
0 O
and O
1 O
or O
replaced O
by O
more O
complex O
functions O
, O
as O
proposed O
in O
Wu O
et O
al O
. O
( O
2016 O
) O
. O
Moreover O
, O
a O
constant O
word O
reward O
is O
used O
in O
He O
et O
al O
. O
( O
2016 O
) O
as O
an O
alternative O
to O
length O
normalization O
. O
All O
of O
these O
approaches O
tackle O
the O
length O
problem O
in O
decoding O
, O
and O
all O
NMT O
systems O
use O
at O
least O
one O
of O
them O
to O
ensure O
the O
performance O
. O
In O
addition O
to O
investigating O
various O
types O
of O
length O
normalization O
, O
their O
rationality O
is O
rarely O
explored O
. O
Although O
length O
normalization O
appears O
to O
be O
simple O
and O
effective O
, O
it O
is O
still O
an O
additional O
technique O
to O
help O
a O
‚Äú O
weak O
‚Äù O
machine O
translation O
model O
that O
can O
not O
handle O
the O
hypothesis O
length O
properly O
. O
In O
this O
work O
it O
is O
proposed O
to O
model O
the O
target O
length O
using O
the O
neural O
network O
itself O
in O
a O
multi O
- O
task O
learning O
way O
. O
The O
estimated O
length O
information O
can O
either O
be O
implicitly O
included O
in O
the O
network O
to O
‚Äú O
guide O
‚Äù O
translation O
, O
or O
it O
can O
be O
used O
explicitly O
as O
an O
alternative O
to O
length O
normalization O
during O
decoding O
. O
The O
experimental O
results O
on O
various O
datasets O
show O
that O
the O
proposed O
system O
achieves O
improvements O
compared O
to O
the O
baseline O
model O
and O
the O
predicted O
length O
can O
easily O
be O
used O
to O
replace O
the O
length O
normalization O
. O
2 O
Related O
Work O
Multi O
- O
task O
learning O
is O
an O
important O
training O
strategy O
that O
aims O
to O
improve O
the O
generalization O
performance O
of O
the O
main O
task O
with O
some O
other O
related O
tasks O
( O
Luong O
et O
al O
. O
, O
2016 O
; O
Mart O
¬¥ O
ƒ±nez O
Alonso O
and O
Plank O
, O
2017 O
) O
. O
With O
regard O
to O
deep O
learning O
, O
multitask O
learning O
is O
applied O
successfully O
in O
many O
areas O
, O
such O
as O
natural O
language O
processing O
( O
Liu O
et O
al O
. O
, O
2015 O
) O
, O
computer O
vision O
( O
Donahue O
et O
al O
. O
, O
2014 O
) O
, O
and O
speech O
processing O
( O
Heigold O
et O
al O
. O
, O
2013 O
) O
. O
In O
this O
work O
, O
the O
prediction O
of O
the O
target O
length O
while O
generating O
translation O
hypotheses O
can O
be O
seen O
as O
a389multi O
- O
task O
learning O
application O
. O
Murray O
and O
Chiang O
( O
2018 O
) O
and O
Stahlberg O
and O
Byrne O
( O
2019 O
) O
attribute O
the O
fact O
that O
beam O
search O
prefers O
shorter O
candidates O
due O
to O
the O
local O
normalization O
of O
NMT O
. O
To O
address O
this O
problem O
, O
in O
addition O
to O
the O
standard O
length O
normalization O
technique O
, O
Wu O
et O
al O
. O
( O
2016 O
) O
propose O
a O
more O
complicated O
correction O
with O
a O
hyperparameter O
that O
can O
be O
adjusted O
for O
different O
language O
pairs O
. O
In O
He O
et O
al O
. O
( O
2016 O
) O
, O
a O
word O
reward O
function O
is O
proposed O
that O
simulates O
the O
coverage O
vector O
in O
statistical O
machine O
translation O
so O
that O
the O
decoder O
prefers O
a O
long O
translation O
. O
Huang O
et O
al O
. O
( O
2017 O
) O
and O
Yang O
et O
al O
. O
( O
2018 O
) O
suggest O
variations O
of O
this O
reward O
that O
provide O
better O
guarantees O
during O
search O
. O
There O
are O
also O
works O
on O
target O
vocabulary O
prediction O
in O
the O
encoder O
- O
decoder O
model O
that O
implicitly O
predicts O
the O
target O
length O
( O
Weng O
et O
al O
. O
, O
2017 O
; O
Suzuki O
and O
Nagata O
, O
2017 O
) O
. O
In O
our O
work O
, O
the O
target O
length O
is O
explicitly O
modeled O
by O
the O
neural O
network O
itself O
, O
which O
indicates O
that O
the O
entire O
system O
relies O
more O
on O
statistics O
rather O
than O
heuristics O
. O
3 O
Neural O
Length O
Model O
To O
predict O
the O
target O
length O
based O
on O
the O
standard O
transformer O
architecture O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
we O
build O
a O
multi O
- O
layer O
sub O
- O
network O
that O
only O
requires O
information O
from O
the O
source O
sequence O
( O
or O
the O
encoder O
) O
. O
In O
this O
work O
the O
length O
prediction O
task O
is O
considered O
as O
a O
classiÔ¨Åcation O
task O
for O
different O
lengths O
. O
Other O
methods O
, O
such O
as O
directly O
generating O
a O
real O
number O
, O
binarizing O
the O
length O
, O
or O
performing O
multiple O
binary O
classiÔ¨Åcation O
tasks O
, O
are O
also O
being O
tested O
, O
but O
the O
classiÔ¨Åcation O
method O
performs O
best O
. O
3.1 O
Modeling O
We O
predict O
the O
length O
of O
the O
target O
sequence O
by O
a O
classiÔ¨Åer O
in O
the O
range O
of O
[ O
0,200 O
] O
, O
the O
input O
of O
which O
is O
a O
single O
vector O
without O
time O
dimension O
, O
which O
is O
extracted O
from O
the O
encoder O
. O
To O
obtain O
this O
vector O
, O
we O
Ô¨Årst O
concatenate O
the O
encoder O
output O
and O
the O
embedding O
of O
the O
source O
tokens O
, O
followed O
by O
a O
linear O
layer O
with O
an O
activation O
function O
to O
map O
the O
vectors O
to O
the O
same O
dimension O
as O
the O
original O
encoder O
output O
. O
Then O
we O
set O
the O
length O
of O
the O
concatenated O
vectors O
to O
200 O
by O
clipping O
or O
zero O
padding O
, O
in O
order O
to O
have O
a O
Ô¨Åxed O
length O
of O
time O
dimension O
, O
which O
could O
be O
compressed O
to O
a O
single O
vector O
by O
convolution O
and O
max O
- O
pooling O
. O
Then O
, O
the O
vectors O
run O
through O
a O
convolutional O
layer O
with O
an O
activation O
function O
and O
a O
max O
- O
pooling O
layer O
. O
A O
linear O
layer O
is O
encoder O
output O
word O
embeddinglinear O
layer O
( O
f O
dim)convolutional O
layer O
( O
j O
dim)activation O
( O
ReLU)max O
pooling O
( O
j O
dim)linear O
layer O
( O
j O
dim O
) O
source O
lengthlinear O
layer O
( O
f O
dim O
) O
length O
embeddinglinear O
layer O
( O
f O
dim)softmaxFigure O
1 O
: O
The O
architecture O
of O
the O
length O
prediction O
subnetwork O
. O
then O
used O
to O
project O
the O
max O
- O
pooled O
vector O
into O
a O
single O
vector O
. O
We O
also O
embed O
the O
length O
of O
source O
sequence O
into O
a O
201 O
dimension O
vector O
with O
a O
length O
embedding O
matrix O
, O
which O
is O
initialized O
by O
the O
empirical O
distribution O
of O
the O
length O
. O
This O
length O
embedding O
is O
then O
concatenated O
with O
the O
output O
logit O
of O
the O
length O
prediction O
sub O
- O
network O
. O
Again O
, O
this O
concatenated O
vector O
is O
projected O
through O
a O
linear O
layer O
onto O
a O
vectorswith O
201 O
dimensions O
. O
Finally O
, O
the O
length O
distribution O
qlis O
given O
by O
a O
softmax O
over O
s. O
And O
the O
predicted O
length O
lpredis O
thelwith O
the O
highest O
probability O
. O
The O
complete O
structure O
of O
the O
proposed O
length O
prediction O
sub O
- O
network O
is O
illustrated O
in O
Figure O
1 O
. O
When O
we O
train O
the O
model O
with O
the O
translation O
and O
length O
prediction O
tasks O
jointly O
, O
the O
gradient O
of O
the O
length O
model O
will O
propagate O
to O
the O
translation O
model O
( O
referred O
to O
as O
no O
- O
connection O
in O
this O
paper O
) O
. O
Thus O
, O
these O
two O
models O
will O
inÔ¨Çuence O
each O
other O
during O
multi O
- O
task O
training O
. O
In O
addition O
, O
the O
translation O
model O
could O
beneÔ¨Åt O
from O
concatenating O
the O
length O
prediction O
output O
vector O
sto O
the O
outputs O
of O
each O
decoder O
layer O
( O
referred O
to O
as O
cross O
- O
concat O
in O
this O
paper O
) O
. O
After O
the O
vector O
is O
concatenated O
, O
a O
linear O
projection O
is O
run O
through O
to O
maintain O
the O
feature O
dimension O
of O
the O
vector O
as O
the O
original O
one O
, O
so O
that O
it O
can O
be O
used O
without O
modifying O
the O
rest O
of O
the O
original O
transformer O
model O
. O
Here O
we O
detach O
sfrom O
the O
backpropagation O
graph O
so O
that O
the O
length O
prediction O
is O
not O
affected O
by O
this O
connection O
. O
In O
this O
method O
, O
we O
think O
that O
with O
the O
concatenation O
, O
the390length O
information O
could O
be O
passed O
to O
the O
decoder O
and O
used O
implicitly O
. O
3.2 O
Training O
During O
training O
, O
Kullback O
‚Äì O
Leibler O
( O
KL O
) O
divergence O
( O
Kullback O
and O
Leibler O
, O
1951 O
) O
is O
used O
as O
the O
loss O
of O
the O
length O
prediction O
task O
: O
Loss O
length O
= O
DKL(P||Q O
) O
= O
/summationdisplay O
lpllogpl O
ql(2 O
) O
whereqlis O
the O
probability O
from O
model O
output O
. O
Supposeltarget O
is O
the O
actual O
length O
of O
the O
target O
sequence O
, O
plis O
the O
target O
distribution O
given O
by O
a O
Gaussian O
function O
added O
with O
a O
neighborhood O
reward O
d(l O
, O
ltarget O
) O
. O
Formally O
, O
plis O
given O
as O
: O
pl O
= O
al O
/ O
summationtext O
l O
/ O
primeal O
/ O
prime(3 O
) O
where O
al= O
exp O
/ O
parenleftBigg O
‚àí/parenleftbiggl‚àíltarget O
œÉ O
/ O
parenrightbigg2 O
/ O
parenrightBigg O
+ O
d(l O
, O
ltarget)(4 O
) O
where O
d(l O
, O
ltarget O
) O
= O
Ô£± O
Ô£≤ O
Ô£≥1 O
ifl O
= O
ltarget O
0.1 O
if|l‚àíltarget|= O
1 O
0 O
others O
( O
5 O
) O
hereœÉis O
a O
constant O
and O
is O
used O
to O
control O
the O
shape O
of O
the O
distribution O
. O
In O
contrast O
to O
cross O
entropy O
with O
label O
smoothing O
, O
in O
which O
there O
is O
only O
one O
true O
label O
with O
a O
high O
probability O
and O
others O
are O
treated O
equally O
, O
the O
probability O
plbecomes O
smaller O
if O
lis O
further O
away O
from O
ltarget O
, O
which O
creates O
the O
desired O
relationship O
between O
each O
class O
in O
the O
classiÔ¨Åer O
. O
We O
use O
cross O
entropy O
with O
label O
smoothing O
as O
the O
training O
loss O
for O
the O
translation O
task O
. O
We O
linearly O
combine O
the O
translation O
loss O
with O
the O
length O
loss O
, O
so O
that O
the O
training O
loss O
is O
given O
by O
Loss O
all O
= O
Œª1Loss O
translation O
+ O
Œª2Loss O
length O
( O
6 O
) O
3.3 O
Decoding O
Besides O
using O
the O
length O
information O
implicitly O
( O
as O
the O
two O
methods O
mentioned O
above O
) O
, O
we O
can O
also O
guide O
the O
decoding O
step O
with O
the O
length O
prediction O
explicitly O
. O
With O
the O
help O
of O
the O
length O
prediction O
, O
we O
have O
a O
mathematically O
reasonable O
control O
of O
the O
output O
length O
in O
comparison O
to O
the O
length O
normalization O
in O
beam O
search O
. O
Since O
the O
predicted O
target O
length O
can O
not O
be O
100 O
% O
accurate O
and O
a O
sourcesentence O
can O
have O
multiple O
possible O
translations O
of O
different O
lengths O
, O
we O
control O
the O
length O
of O
the O
inference O
by O
penalizing O
the O
score O
( O
logarithmic O
probability O
) O
of O
the O
end O
- O
of O
- O
sentence O
( O
EOS O
) O
token O
during O
beam O
search O
, O
rather O
than O
forcing O
the O
length O
of O
the O
inference O
to O
match O
the O
predicted O
length O
. O
More O
specifically O
, O
if O
the O
length O
of O
the O
hypothesis O
is O
shorter O
than O
the O
predicted O
length O
, O
the O
EOS O
token O
score O
is O
penalized O
; O
if O
the O
hypothesis O
is O
longer O
than O
the O
predicted O
length O
, O
the O
EOS O
token O
score O
is O
rewarded O
to O
facilitate O
the O
selection O
of O
the O
EOS O
token O
in O
beam O
search O
to O
Ô¨Ånalize O
the O
hypothesis O
. O
A O
logarithmic O
linear O
penalty O
is O
introduced O
, O
which O
is O
added O
to O
the O
score O
of O
EOS O
token O
at O
each O
time O
step O
during O
beam O
search O
: O
P O
= O
Œ±logLhyp O
Lpred(7 O
) O
whereLhypis O
the O
length O
of O
the O
hypothesis O
, O
Lpredis O
the O
predicted O
length O
of O
the O
target O
sentence O
, O
and O
Œ± O
is O
a O
hyperparameter O
to O
control O
the O
penalty O
. O
4 O
Experiments O
4.1 O
Experimental O
Setup O
We O
Ô¨Årst O
conduct O
experiments O
on O
a O
relatively O
small O
dataset O
, O
IWSLT2014 O
German O
‚ÜíEnglish O
( O
160k O
sentence O
pairs O
) O
( O
Cettolo O
et O
al O
. O
, O
2014 O
) O
, O
to O
tune O
hyperparameters O
and O
analyze O
the O
performance O
. O
Then O
we O
train O
our O
model O
on O
other O
four O
different O
language O
pairs O
, O
which O
are O
Spanish O
- O
English O
( O
es O
- O
en O
) O
, O
Italian O
- O
English O
( O
it O
- O
en O
) O
, O
Dutch O
- O
English O
( O
nl O
- O
en O
) O
and O
Romanian O
- O
English O
( O
ro O
- O
en O
) O
. O
At O
last O
, O
the O
experiments O
are O
carried O
out O
on O
the O
WMT O
( O
Barrault O
et O
al O
. O
, O
2019 O
) O
German‚ÜîEnglish O
( O
4 O
M O
sentence O
pairs O
) O
datasets O
in O
order O
to O
compare O
our O
system O
with O
the O
baseline O
model O
. O
All O
datasets O
used O
in O
this O
work O
are O
preprocessed O
by O
fairseq1(Ott O
et O
al O
. O
, O
2019 O
) O
. O
Data O
statistics O
can O
be O
found O
in O
Table O
1 O
. O
data O
set O
language O
pairnumber O
of O
sentence O
pairs O
train O
valid O
test O
IWSLTde O
- O
en O
160k O
7.3k O
6.8k O
es O
- O
en O
169k O
7.7k O
5.6k O
it O
- O
en O
167k O
7.6k O
6.6k O
nl O
- O
en O
154k O
7.0k O
5.4k O
ro O
- O
en O
168k O
7.6k O
5.6k O
WMT O
en‚Üîde O
4.5 O
M O
3.0k O
3.0k O
Table O
1 O
: O
Data O
statistics O
of O
IWSLT O
and O
WMT O
datasets O
. O
We O
employ O
the O
transformer O
base O
architecture O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
as O
the O
baseline O
model O
and O
1https://github.com/pytorch/fairseq391this O
work O
is O
implemented O
in O
fairseq O
. O
All O
model O
hyperparameters O
of O
the O
baseline O
model O
for O
IWSLT O
match O
the O
settings O
in O
fairseq O
. O
For O
the O
WMT O
experiments O
, O
the O
settings O
are O
the O
same O
as O
for O
the O
original O
base O
transformer O
model O
. O
The O
sub O
- O
network O
used O
for O
the O
length O
prediction O
only O
increases O
the O
number O
of O
free O
parameters O
by O
less O
than O
10 O
% O
, O
the O
inÔ¨Çuence O
on O
the O
training O
and O
decoding O
speed O
is O
also O
marginal O
. O
Experimental O
performance O
is O
measured O
using O
BLEU O
( O
Papineni O
et O
al O
. O
, O
2002 O
; O
Post O
, O
2018 O
) O
and O
CHARAC O
TER(Wang O
et O
al O
. O
, O
2016 O
) O
( O
CTER O
) O
metrics O
. O
4.2 O
Experimental O
Results O
For O
the O
length O
prediction O
task O
, O
the O
inference O
length O
does O
not O
have O
to O
correspond O
exactly O
to O
the O
reference O
length O
, O
since O
there O
can O
be O
multiple O
correct O
translations O
with O
different O
lengths O
. O
Therefore O
, O
we O
consider O
the O
predictions O
that O
fulÔ¨Åll O
|lpredict‚àíltarget|/ltarget‚â§ O
Tto O
be O
accurate O
, O
where O
Tis O
a O
threshold O
. O
Œª1Œª2model O
acc.[%]BLEU[% O
] O
1 O
0baseline O
- O
34.8 O
0 O
1length O
model O
83.4 O
1 O
1no O
- O
connection O
86.7 O
35.3 O
cross O
- O
concat O
86.1 O
35.3 O
Table O
2 O
: O
Accuracy O
rate O
and O
B O
LEU O
scores O
of O
the O
proposed O
system O
with O
the O
length O
model O
on O
the O
IWSLT O
German‚ÜíEnglish O
task O
. O
The O
accuracy O
of O
the O
length O
prediction O
task O
is O
reported O
on O
the O
validation O
dataset O
. O
language O
pair O
es O
- O
en O
it O
- O
en O
nl O
- O
en O
ro O
- O
en O
baseline O
41.2 O
32.6 O
37.8 O
38.4 O
no O
- O
connection O
41.3 O
32.8 O
37.8 O
38.8 O
cross O
- O
concat O
41.3 O
32.7 O
38.3 O
38.7 O
Table O
3 O
: O
Performance O
( O
in O
B O
LEU[%]scores O
) O
using O
different O
methods O
for O
different O
language O
pairs O
. O
Table O
2 O
shows O
the O
experiments O
carried O
out O
with O
the O
standard O
translation O
model O
( O
Œª1= O
1andŒª2= O
0 O
) O
, O
the O
pure O
length O
model O
( O
Œª1= O
0 O
andŒª2= O
1 O
) O
and O
the O
combination O
of O
the O
two O
models O
( O
Œª1= O
1 O
andŒª2= O
1 O
) O
. O
For O
the O
accuracy O
, O
here O
we O
choose O
the O
thresholdT= O
20 O
% O
. O
It O
is O
observed O
that O
the O
joint O
training O
of O
the O
two O
models O
performs O
better O
for O
both O
the O
translation O
and O
the O
length O
prediction O
task O
. O
Due O
to O
the O
multi O
- O
task O
learning O
, O
although O
the O
translation O
task O
does O
not O
explicitly O
inÔ¨Çuence O
the O
length O
prediction O
, O
it O
helps O
to O
bring O
model O
parameters O
to O
a O
better O
local O
optimum O
. O
We O
use O
no O
- O
connection O
, O
cross O
- O
concat O
model O
totrain O
on O
other O
language O
pairs O
with O
the O
same O
hyperparameters O
as O
on O
IWSLT O
de O
- O
en O
to O
test O
the O
performance O
, O
as O
shown O
in O
Table O
3 O
. O
For O
cross O
- O
concat O
, O
theBLEU O
score O
of O
the O
nl O
- O
en O
system O
is O
improved O
by O
0.5 O
% O
. O
For O
other O
language O
pairs O
, O
the O
results O
of O
two O
methods O
are O
almost O
the O
same O
, O
all O
of O
which O
are O
better O
than O
the O
baseline O
. O
Figure O
2 O
shows O
the O
relative O
differences O
‚àÜlbetween O
the O
predicted O
and O
actual O
lengths O
for O
different O
target O
sequence O
lengths O
. O
When O
ltarget O
is O
between O
about O
10and40 O
, O
the O
prediction O
is O
pretty O
good O
: O
for O
mostltarget O
in[10,40],‚àÜlis O
less O
than O
15 O
% O
. O
When O
ltarget O
is O
in[40,100 O
] O
, O
the O
prediction O
becomes O
worse O
, O
but O
most O
of O
them O
are O
still O
less O
than O
20 O
% O
. O
After O
100 O
, O
the O
prediction O
is O
pretty O
bad O
. O
There O
are O
two O
reasons O
for O
this O
: O
Ô¨Årst O
, O
the O
length O
of O
most O
target O
sequences O
in O
training O
data O
is O
between O
10 O
and O
40 O
, O
so O
the O
model O
does O
not O
often O
see O
the O
cases O
that O
the O
sequence O
is O
too O
long O
; O
second O
, O
there O
are O
very O
few O
long O
sequences O
in O
the O
validation O
dataset O
, O
so O
the O
results O
for O
these O
points O
lack O
statistical O
meaning O
. O
Figure O
2 O
: O
The O
left O
y O
- O
axis O
shows O
the O
relative O
difference O
between O
the O
target O
and O
the O
predicted O
length O
and O
the O
righty O
- O
axis O
is O
for O
the O
empirical O
distribution O
of O
ltarget O
. O
Table O
4 O
shows O
the O
comparison O
between O
the O
proposed O
approach O
and O
the O
baseline O
model O
. O
Here O
we O
setŒ±= O
10 O
according O
to O
the O
experiments O
that O
are O
carried O
out O
on O
the O
IWSLT O
dataset O
. O
The O
additional O
sub O
- O
network O
for O
length O
prediction O
improves O
the O
BLEU O
score O
by O
up O
to O
0.9%over O
the O
strong O
baseline O
model O
. O
Moreover O
, O
the O
predicted O
length O
successfully O
serves O
as O
an O
alternative O
to O
length O
normalization O
. O
Regardless O
of O
whether O
the O
inference O
tends O
to O
be O
longer O
or O
shorter O
than O
the O
reference O
, O
the O
ratio O
when O
using O
the O
predicted O
length O
is O
slightly O
better O
than O
using O
the O
length O
normalization O
, O
which O
shows O
better O
control O
of O
the O
length.392model O
English‚ÜíGerman O
German‚ÜíEnglish O
newstest2014 O
newstest2017 O
BLEU[%]‚ÜëCTER[%]‚Üìlen O
. O
ratio O
BLEU[%]‚ÜëCTER[%]‚Üìlen O
. O
ratio O
baseline O
27.3 O
45.8 O
1.024 O
33.0 O
41.8 O
0.974 O
+ O
len O
. O
model O
no O
- O
connection O
27.6 O
45.5 O
1.020 O
33.4 O
41.5 O
0.972 O
cross O
- O
concat O
27.4 O
45.7 O
1.024 O
33.4 O
41.3 O
0.970 O
- O
len O
. O
norm O
. O
no O
- O
connection O
27.6 O
45.6 O
1.018 O
33.9 O
40.9 O
0.973 O
cross O
- O
concat O
27.3 O
45.8 O
1.021 O
33.7 O
41.2 O
0.974 O
Table O
4 O
: O
Comparison O
between O
the O
proposed O
system O
and O
the O
baseline O
model O
. O
‚Äú O
+ O
len O
. O
model O
‚Äù O
indicates O
that O
the O
length O
prediction O
sub O
- O
network O
is O
added O
to O
the O
baseline O
architecture O
. O
‚Äú O
- O
len O
. O
normalization O
‚Äù O
denotes O
that O
the O
predicted O
length O
is O
used O
during O
decoding O
as O
an O
alternative O
to O
the O
length O
normalization O
as O
described O
in O
Section O
3.3 O
. O
‚Äú O
len O
. O
ratio O
‚Äù O
gives O
the O
length O
ratio O
between O
the O
hypothesis O
length O
and O
the O
reference O
length O
: O
the O
closer O
to O
1 O
, O
the O
better O
. O
Figure O
3 O
: O
Length O
prediction O
model O
outperforms O
the O
baseline O
model O
in O
length O
prediction O
test O
accuracy O
. O
Figure O
3 O
shows O
the O
relationship O
between O
the O
length O
prediction O
accuracy O
of O
the O
baseline O
model O
and O
the O
length O
prediction O
model O
, O
and O
the O
threshold O
Tfor O
calculating O
accuracy O
. O
Since O
the O
transformer O
baseline O
model O
does O
not O
predict O
the O
target O
length O
, O
the O
length O
prediction O
of O
baseline O
is O
obtained O
from O
the O
average O
ratio O
of O
source O
sentence O
length O
to O
target O
sentence O
length O
. O
For O
the O
length O
prediction O
task O
, O
the O
accuracy O
of O
our O
model O
is O
always O
better O
than O
the O
baseline O
, O
which O
indicates O
that O
on O
WMT O
data O
, O
our O
model O
can O
still O
predict O
the O
target O
length O
well O
. O
5 O
Conclusion O
In O
this O
paper O
, O
we O
propose O
a O
length O
prediction O
subnetwork O
based O
on O
the O
transformer O
architecture O
, O
and O
a O
method O
of O
using O
the O
length O
prediction O
information O
on O
the O
decoder O
side O
, O
namely O
cross O
- O
concat O
. O
In O
decoding O
, O
we O
use O
the O
predicted O
length O
to O
calculate O
a O
logarithmic O
linear O
penalty O
in O
the O
beam O
search O
in O
order O
to O
replace O
the O
length O
normalization O
. O
Experimental O
results O
show O
that O
the O
sub O
- O
network O
canpredict O
target O
length O
well O
and O
further O
improve O
translation O
quality O
. O
In O
addition O
, O
the O
predicted O
length O
can O
be O
used O
to O
replace O
the O
length O
normalization O
with O
a O
better O
and O
more O
mathematically O
explainable O
control O
of O
the O
output O
length O
. O
For O
future O
work O
, O
the O
use O
of O
length O
prediction O
in O
positional O
encoding O
( O
Lakew O
et O
al O
. O
, O
2019 O
; O
Takase O
and O
Okazaki O
, O
2019 O
) O
and O
nonautoregressive O
( O
or O
partially O
autoregressive O
) O
NMT O
( O
Gu O
et O
al O
. O
, O
2017 O
; O
Lee O
et O
al O
. O
, O
2018 O
; O
Stern O
et O
al O
. O
, O
2019 O
) O
could O
be O
further O
investigated O
. O
Acknowledgements O
This O
work O
has O
received O
funding O
from O
the O
European O
Research O
Council O
( O
ERC O
) O
( O
under O
the O
European O
Union O
‚Äôs O
Horizon O
2020 O
research O
and O
innovation O
programme O
, O
grant O
agreement O
No O
694537 O
, O
project O
‚Äú O
SEQCLAS O
‚Äù O
) O
and O
the O
Deutsche O
Forschungsgemeinschaft O
( O
DFG O
; O
grant O
agreement O
NE O
572/8 O
- O
1 O
, O
project O
‚Äú O
CoreTec O
‚Äù O
) O
. O
The O
GPU O
computing O
cluster O
was O
supported O
by O
DFG O
( O
Deutsche O
Forschungsgemeinschaft O
) O
under O
grant O
INST O
222/1168 O
- O
1 O
FUGG O
. O
Abstract O
Recent O
work O
in O
unsupervised O
parsing O
has O
tried O
to O
incorporate O
visual O
information O
into O
learning O
, O
but O
results O
suggest O
that O
these O
models O
need O
linguistic O
bias O
to O
compete O
against O
models O
that O
only O
rely O
on O
text O
. O
This O
work O
proposes O
grammar O
induction O
models O
which O
use O
visual O
information O
from O
images O
for O
labeled O
parsing O
, O
and O
achieve O
state O
- O
of O
- O
the O
- O
art O
results O
on O
grounded O
grammar O
induction O
on O
several O
languages O
. O
Results O
indicate O
that O
visual O
information O
is O
especially O
helpful O
in O
languages O
where O
high O
frequency O
words O
are O
more O
broadly O
distributed O
. O
Comparison O
between O
models O
with O
and O
without O
visual O
information O
shows O
that O
the O
grounded O
models O
are O
able O
to O
use O
visual O
information O
for O
proposing O
noun O
phrases O
, O
gathering O
useful O
information O
from O
images O
for O
unknown O
words O
, O
and O
achieving O
better O
performance O
at O
prepositional O
phrase O
attachment O
prediction.1 O
1 O
Introduction O
Recent O
grammar O
induction O
models O
are O
able O
to O
produce O
accurate O
grammars O
and O
labeled O
parses O
with O
raw O
text O
only O
( O
Jin O
et O
al O
. O
, O
2018b O
, O
2019 O
; O
Kim O
et O
al O
. O
, O
2019b O
, O
a O
; O
Drozdov O
et O
al O
. O
, O
2019 O
) O
, O
providing O
evidence O
against O
the O
poverty O
of O
the O
stimulus O
argument O
( O
Chomsky O
, O
1965 O
) O
, O
and O
showing O
that O
many O
linguistic O
distinctions O
like O
lexical O
and O
phrasal O
categories O
can O
be O
directly O
induced O
from O
raw O
text O
statistics O
. O
However O
, O
as O
computational O
- O
level O
models O
of O
human O
syntax O
acquisition O
, O
they O
lack O
semantic O
, O
pragmatic O
and O
environmental O
information O
which O
human O
learners O
seem O
to O
use O
( O
Gleitman O
, O
1990 O
; O
Pinker O
and O
MacWhinney O
, O
1987 O
; O
Tomasello O
, O
2003 O
) O
. O
This O
paper O
proposes O
novel O
grounded O
neuralnetwork O
- O
based O
models O
of O
grammar O
induction O
which O
take O
into O
account O
information O
extracted O
from O
images O
in O
learning O
. O
Performance O
comparisons O
show O
1The O
system O
implementation O
and O
translated O
datasets O
used O
in O
this O
work O
can O
be O
found O
at O
https://github.com/ O
lifengjin O
/ O
imagepcfg O
. O
( O
a O
) O
friend O
as O
companion O
  O
( O
b O
) O
friend O
as O
condiment O
Figure O
1 O
: O
Examples O
of O
disambiguating O
information O
provided O
by O
images O
for O
the O
prepositional O
phrase O
attachment O
of O
the O
sentence O
Mary O
eats O
spaghetti O
with O
a O
friend O
( O
Gokcen O
et O
al O
. O
, O
2018 O
) O
. O
that O
the O
proposed O
models O
achieve O
state O
- O
of O
- O
the O
- O
art O
results O
on O
multilingual O
induction O
datasets O
, O
even O
without O
help O
from O
linguistic O
knowledge O
or O
pretrained O
image O
encoders O
. O
Experiments O
show O
several O
speciÔ¨Åc O
beneÔ¨Åts O
attributable O
to O
the O
use O
of O
visual O
information O
in O
induction O
. O
First O
, O
as O
a O
proxy O
to O
semantics O
, O
the O
co O
- O
occurrences O
between O
objects O
in O
images O
and O
referring O
words O
and O
expressions O
, O
such O
as O
the O
word O
spaghetti O
and O
the O
plate O
of O
spaghetti O
in O
Figure O
1,2 O
provide O
clues O
to O
the O
induction O
model O
about O
the O
syntactic O
categories O
of O
such O
linguistic O
units O
, O
which O
may O
complement O
distributional O
cues O
from O
word O
collocation O
which O
normal O
grammar O
inducers O
rely O
on O
solely O
for O
induction O
. O
Also O
, O
pictures O
may O
help O
disambiguate O
di O
Ô¨Äerent O
syntactic O
relations O
: O
induction O
models O
are O
not O
able O
to O
resolve O
many O
prepositional O
phrase O
attachment O
ambiguities O
with O
only O
text O
‚Äî O
for O
example O
in O
Figure O
1 O
, O
there O
is O
little O
information O
in O
the O
text O
of O
Mary O
eats O
spaghetti O
with O
a O
friend O
for O
the O
induction O
models O
to O
induce O
a O
high O
attachment O
structure O
where O
a O
friend O
is O
a O
companion O
‚Äî O
and O
images O
may O
provide O
information O
to O
resolve O
these O
ambiguities O
. O
Finally O
, O
images O
may O
provide O
grounding O
information O
for O
unknown O
words O
when O
their O
syntactic O
properties O
are O
not O
clearly O
indicated O
by O
sentential O
context O
. O
2https://github.com/ajdagokcen/ O
madlyambiguous O
- O
repo3962 O
Related O
work O
Existing O
unsupervised O
PCFG O
inducers O
exploit O
naturally O
- O
occurring O
cognitive O
and O
developmental O
constraints O
, O
such O
as O
punctuation O
as O
a O
proxy O
to O
prosody O
( O
Seginer O
, O
2007 O
) O
, O
human O
memory O
constraints O
( O
Noji O
and O
Johnson O
, O
2016 O
; O
Shain O
et O
al O
. O
, O
2016 O
; O
Jin O
et O
al O
. O
, O
2018b O
) O
, O
and O
morphology O
( O
Jin O
and O
Schuler O
, O
2019 O
) O
, O
to O
regulate O
the O
posterior O
of O
grammars O
which O
are O
known O
to O
be O
extremely O
multimodal O
( O
Johnson O
et O
al O
. O
, O
2007 O
) O
. O
Models O
in O
Shi O
et O
al O
. O
( O
2019 O
) O
also O
match O
embeddings O
of O
word O
spans O
to O
encoded O
images O
to O
induce O
unlabeled O
hierarchical O
structures O
with O
a O
concreteness O
measure O
( O
Hill O
et O
al O
. O
, O
2014 O
; O
Hill O
and O
Korhonen O
, O
2014 O
) O
. O
Additionally O
, O
visual O
information O
is O
observed O
to O
provide O
grounding O
for O
words O
describing O
concrete O
objects O
, O
helping O
to O
identify O
and O
categorize O
such O
words O
. O
This O
hypothesis O
is O
termed O
‚Äò O
noun O
bias O
‚Äô O
in O
language O
acquisition O
( O
Gentner O
, O
1982 O
, O
2006 O
; O
Waxman O
et O
al O
. O
, O
2013 O
) O
, O
through O
which O
the O
early O
acquisition O
of O
nouns O
is O
attributed O
to O
nouns O
referring O
to O
observable O
objects O
. O
However O
, O
the O
models O
in O
Shi O
et O
al O
. O
( O
2019 O
) O
also O
rely O
on O
language O
- O
speciÔ¨Åc O
branching O
bias O
to O
outperform O
other O
text O
- O
based O
models O
, O
and O
images O
are O
encoded O
by O
pretrained O
object O
classiÔ¨Åers O
trained O
with O
large O
datasets O
, O
with O
no O
ablation O
to O
show O
the O
beneÔ¨Åt O
of O
visual O
information O
for O
unsupervised O
parsing O
. O
Visual O
information O
has O
also O
been O
used O
for O
joint O
training O
of O
prepositional O
phrase O
attachment O
models O
( O
Christie O
et O
al O
. O
, O
2016 O
) O
suggesting O
that O
visual O
information O
may O
contain O
semantic O
information O
to O
help O
disambiguate O
prepositional O
phrase O
attachment O
. O
3 O
Grounded O
Grammar O
Induction O
Model O
The O
full O
grounded O
grammar O
induction O
model O
used O
in O
these O
experiments O
, O
ImagePCFG O
, O
consists O
of O
two O
parts O
: O
a O
word O
- O
based O
PCFG O
induction O
model O
and O
a O
vision O
model O
, O
as O
shown O
in O
Figure O
2 O
. O
The O
two O
parts O
have O
their O
own O
objective O
functions O
. O
The O
PCFG O
induction O
model O
, O
called O
NoImagePCFG O
when O
trained O
by O
itself O
, O
can O
be O
trained O
by O
maximizing O
the O
marginal O
probability O
P(œÉ O
) O
of O
sentences O
œÉ O
. O
This O
part O
functions O
similarly O
to O
previously O
proposed O
PCFG O
induction O
models O
( O
Jin O
et O
al O
. O
, O
2018a O
; O
Kim O
et O
al O
. O
, O
2019a O
) O
where O
a O
PCFG O
is O
induced O
through O
maximization O
of O
the O
data O
likelihood O
of O
the O
training O
corpus O
marginalized O
over O
latent O
syntactic O
trees O
. O
The O
image O
encoder O
- O
decoder O
network O
in O
the O
vision O
model O
is O
trained O
to O
reconstruct O
the O
original O
image O
after O
passing O
through O
an O
information O
bottleneck O
. O
The O
latent O
encoding O
from O
the O
image O
encoder O
may O
be O
seen O
as O
a O
compressed O
representation O
of O
vi O
- O
sual O
information O
in O
the O
image O
, O
some O
of O
which O
is O
semantic O
, O
relating O
to O
objects O
in O
the O
image O
. O
We O
hypothesize O
that O
semantic O
information O
can O
be O
helpful O
in O
syntax O
induction O
, O
potentially O
through O
helping O
three O
tasks O
mentioned O
above O
. O
In O
contrast O
to O
the O
full O
model O
where O
the O
encoded O
visual O
representations O
are O
trained O
from O
scratch O
, O
the O
ImagePrePCFG O
model O
uses O
image O
embeddings O
encoded O
by O
pretrained O
image O
classiÔ¨Åers O
with O
parameters O
Ô¨Åxed O
during O
induction O
training O
. O
We O
hypothesize O
that O
pretrained O
image O
classiÔ¨Åers O
may O
provide O
useful O
information O
about O
objects O
in O
an O
image O
, O
but O
for O
grammar O
induction O
it O
is O
better O
to O
allow O
the O
inducer O
to O
decide O
which O
kind O
of O
information O
may O
help O
induction O
. O
The O
two O
parts O
are O
connected O
through O
a O
syntacticvisual O
loss O
function O
connecting O
a O
syntactic O
sentence O
embedding O
projected O
from O
word O
embeddings O
and O
an O
image O
embedding O
. O
We O
hypothesize O
that O
visual O
information O
in O
the O
encoded O
images O
may O
help O
constrain O
the O
search O
space O
of O
syntactic O
embeddings O
of O
words O
with O
supporting O
evidence O
of O
lexical O
attributes O
such O
as O
concreteness O
for O
nouns O
or O
correlating O
adjectives O
with O
properties O
of O
objects.3 O
3.1 O
Induction O
model O
The O
PCFG O
induction O
model O
is O
factored O
into O
three O
submodels O
: O
a O
nonterminal O
expansion O
model O
, O
a O
terminal O
expansion O
model O
and O
a O
split O
model O
, O
which O
distinguishes O
terminal O
and O
nonterminal O
expansions O
. O
The O
binary O
- O
branching O
non O
- O
terminal O
expansion O
rule O
probabilities,4and O
unary O
- O
branching O
terminal O
expansion O
rule O
probabilities O
in O
a O
factored O
Chomskynormal O
- O
form O
PCFG O
can O
be O
parameterized O
with O
these O
three O
submodels O
. O
Given O
a O
tree O
as O
a O
set O
œÑ O
of O
nodesŒ∑undergoing O
non O
- O
terminal O
expansions O
cŒ∑‚ÜícŒ∑1cŒ∑2(whereŒ∑‚àà{1,2}‚àóis O
a O
Gorn O
address O
specifying O
a O
path O
of O
left O
or O
right O
branches O
from O
the O
root O
) O
, O
and O
a O
set O
œÑ O
/ O
primeof O
nodesŒ∑undergoing O
terminal O
expansions O
cŒ∑‚ÜíwŒ∑(where O
wŒ∑is O
the O
word O
at O
nodeŒ∑ O
) O
in O
a O
parse O
of O
sentence O
œÉ O
, O
the O
marginal O
3The O
syntactic O
nature O
of O
word O
embeddings O
indicates O
that O
any O
lexical O
- O
speciÔ¨Åc O
semantic O
information O
in O
these O
embeddings O
may O
be O
abstract O
, O
which O
is O
generally O
not O
su O
Ô¨Écient O
for O
visual O
reconstruction O
. O
Experiments O
with O
syntactic O
embeddings O
show O
that O
it O
is O
di O
Ô¨Écult O
to O
extract O
semantic O
information O
from O
them O
and O
present O
visually O
. O
4These O
include O
the O
expansion O
rules O
generating O
the O
top O
node O
in O
the O
tree.397a O
giraffe O
is O
eating O
leaves O
   O
Syntactic O
- O
visual O
projectorImage O
encoderImage O
decoderGrammarInductionModelembeddingsInside O
/ O
ViterbiSNPVP O
‚Ä¶ O
P(  O
) O
, O
< O
latexit O
sha1_base64="/t+RkoYiTPP2qCO O
/ O
l471Vfuq114=">AAACHnicbZBLSwMxFIUz9VXrq9alm2ARKkiZKQV1V3DjsoJ9QGcomTTThuYxJBm1DPNXxJ3+EnfiVn+Ie9PHQlsPBA7n3EsuXxgzqo3rfjm5tfWNza38dmFnd2//oHhYamuZKExaWDKpuiHShFFBWoYaRrqxIoiHjHTC8fW079wTpakUd2YSk4CjoaARxcjYqF8sQSs O
/ O
VjJMfU2HHGXn O
/ O
WLZrbozwVXjLUwZLNTsF7/9gcQJJ8JghrTueW5sghQpQzEjWcFPNIkRHqMh6VkrECc6SGe3Z O
/ O
DUJgMYSWWfMHCW O
/ O
t5IEdd6wkM7yZEZ6eVuGv7X9RITXQYpFXFiiMDzj6KEQSPhFAQcUEWwYRNrEFbU3grxCCmEjcVV8AV5wJJzJAbpDE O
/ O
W84LUMopgM6uUvbOsYDl5y1RWTbtW9erVq9t6uVFbEMuDY3ACKsADF6ABbkATtAAGj+AJvIBX59l5c96dj O
/ O
lozlnsHIE O
/ O
cj5 O
/ O
ANoooQY=</latexit O
> O
e  O
< O
latexit O
sha1_base64="D1DjNBk1Y1GUg9GLi2rV5g4KEO4=">AAACHXicbVDJSgNBFOyJW4xb1KOXxiDoJcwEQb0JXjxGMImQGUNP503SpJehu0cJw3yKeNMv8SZexQ O
/ O
xbmc5uBU8KKre4xUVp5wZ6 O
/ O
sfXmlhcWl5pbxaWVvf2Nyqbu+0jco0hRZVXOmbmBjgTELLMsvhJtVARMyhE48uJn7nDrRhSl7bcQqRIAPJEkaJdVKvuh0KYodxkkNxGxo2EKRXrfl1fwr8lwRzUkNzNHvVz7CvaCZAWsqJMd3AT22UE20Z5VBUwsxASuiIDKDrqCQCTJRPoxf4wCl9nCjtRlo8Vb9f5EQYMxax25wENb+9ifif181schrlTKaZBUlnj5KMY6vwpAfcZxqo5WNHCNXMZcV0SDSh1rVVCSXcUyUEkf08TLWKi24Q5XloEtwsDmvBUVFxPQW O
/ O
W O
/ O
lL2o16cFw O
/ O
uzqunTfmjZXRHtpHhyhAJ+gcXaImaiGK7tEDekLP3qP34r16b7PVkje/2UU/4L1 O
/ O
ASCSodI=</latexit O
> O
em O
< O
latexit O
sha1_base64="qRMW3nBDU4fMlvosuWaXb4Q9BLs=">AAACFnicbVC7SgNBFJ2NrxhfUUubwSDEJuyGgNoFbCwjGBWyq8xO7sbBeSwzs0pY9jfETr O
/ O
ETmxt O
/ O
RB7Z5MUvg5cOJxzL O
/ O
dw4pQzY33 O
/ O
w6vMzS8sLlWXayura+sb9c2tc6MyTaFPFVf6MiYGOJPQt8xyuEw1EBFzuIhvj0v/4g60YUqe2XEKkSAjyRJGiXVSGApib+Ikh+JKXNcbfsufAP8lwYw00Ay96 O
/ O
pnOFQ0EyAt5cSYQeCnNsqJtoxyKGphZiAl9JaMYOCoJAJMlE8yF3jPKUOcKO1GWjxRv1 O
/ O
kRBgzFrHbLDOa314p O
/ O
ucNMpscRjmTaWZB0umjJOPYKlwWgIdMA7V87AihmrmsmN4QTah1NdVCCfdUCUHkMA9TreJiEER5HpoE94pmI9gvaq6n4Hcrf8l5uxV0WkennUa3PWusinbQLmqiAB2gLjpBPdRHFKXoAT2hZ+/Re O
/ O
FevbfpasWb3WyjH O
/ O
DevwB O
/ O
t59v</latexit O
> O
L( ,m O
) O
< O
latexit O
sha1_base64="XhjR5wFMxemIrYcvEDz O
/ O
Tq5oUls=">AAACFXicbVDLSsNAFJ3Ud31VXboZLEILUpJSUHcFNy5cVLBaSEKZTCft0HmEmYlSQj5D3OmXuBO3rv0Q905rFtp64MLhnHu5954oYVQb1/10SkvLK6tr6xvlza3tnd3K3v6tlqnCpIslk6oXIU0YFaRrqGGklyiCeMTIXTS+mPp390RpKsWNmSQk5GgoaEwxMlbyr2qBpkOOTni9X6m6DXcGuEi8glRBgU6/8hUMJE45EQYzpLXvuYkJM6QMxYzk5SDVJEF4jIbEt1QgTnSYzU7O4bFVBjCWypYwcKb+nsgQ13rCI9vJkRnpeW8q O
/ O
uf5qYnPwoyKJDVE4J9FccqgkXD6PxxQRbBhE0sQVtTeCvEIKYSNTakcCPKAJedIDLIgUTLKfS O
/ O
MskDHsJPXql49L9ucvPlUFslts+G1GufXrWq7WSS2Dg7BEagBD5yCNrgEHdAFGEjwCJ7Bi O
/ O
PkvDpvzvtPa8kpZg7AHzgf3yH O
/ O
nhI=</latexit O
> O
L(em O
, O
e  O
) O
< O
latexit O
sha1_base64="Z6eDJl1Ek+4i O
/ O
v+xmfjhKsWlDjU=">AAACLXicbVC7SgNBFJ31GeMramkzGoQIEnaDoHaCjYVFBKNCdg2zk7txcB7LzKwSlq39GrHTL7EQxNYvsHfyKHwduHDmnHuZe0+ccmas7796E5NT0zOzpbny O
/ O
MLi0nJlZfXcqExTaFHFlb6MiQHOJLQssxwuUw1ExBwu4pujgX9xC9owJc9sP4VIkJ5kCaPEOqlT2TiphYLY6zjJobgSO98eoWE9QbY7lapf94fAf0kwJlU0RrNT+Qy7imYCpKWcGNMO O
/ O
NRGOdGWUQ5FOcwMpITekB60HZVEgIny4SkF3nJKFydKu5IWD9XvEzkRxvRF7DoHm5rf3kD8z2tnNtmPcibTzIKko4+SjGOr8CAX3GUaqOV9RwjVzO2K6TXRhFqXXjmUcEeVEER28zDVKi7aQZTnoUlws6hVg+2i7HIKfqfyl5w36sFu O
/ O
eB0t3rYGCdWQutoE9VQgPbQITpGTdRCFN2jB O
/ O
SEnr1H78V7895HrRPeeGYN O
/ O
YD38QWGq6gx</latexit O
> O
NoImagePCFGImagePrePCFGImagePCFGFigure O
2 O
: O
Di O
Ô¨Äerent O
conÔ¨Ågurations O
of O
PCFG O
induction O
models O
: O
the O
model O
without O
vision O
( O
NoImagePCFG O
) O
, O
the O
model O
with O
a O
pretrained O
image O
encoder O
( O
ImagePrePCFG O
) O
and O
the O
model O
with O
images O
( O
ImagePCFG O
. O
) O
probability O
of O
œÉcan O
be O
computed O
as O
: O
P(œÉ)=/summationdisplay O
œÑ O
, O
œÑ O
/ O
prime O
/ O
productdisplay O
Œ∑‚ààœÑP(cŒ∑‚ÜícŒ∑1cŒ∑2)¬∑/productdisplay O
Œ∑‚ààœÑ O
/ O
primeP(cŒ∑‚ÜíwŒ∑ O
) O
( O
1 O
) O
We O
Ô¨Årst O
deÔ¨Åne O
a O
set O
of O
Bernoulli O
distributions O
that O
distribute O
probability O
mass O
between O
terminal O
and O
nonterminal O
rules O
, O
so O
that O
the O
lexical O
expansion O
model O
can O
be O
tied O
to O
the O
image O
model O
( O
see O
Section O
4.2 O
): O
P(Term|cŒ∑)=softmax O
{ O
0,1}(ReLU O
( O
WsplxB O
, O
cŒ∑+bspl O
) O
) O
, O
( O
2 O
) O
where O
cŒ∑is O
a O
non O
- O
terminal O
category O
, O
Wspl‚ààR2√óh O
andbspl‚ààR2are O
model O
parameters O
for O
hidden O
vectors O
of O
size O
h O
, O
and O
xB O
, O
cŒ∑‚ààRhthe O
result O
of O
a O
multilayered O
residual O
network O
( O
Kim O
et O
al O
. O
, O
2019a O
) O
. O
The O
residual O
network O
consists O
of O
Barchitecturally O
identical O
residual O
blocks O
. O
For O
an O
input O
vector O
xb‚àí1,c O
each O
residual O
block O
bperforms O
the O
following O
computation O
: O
xb O
, O
c O
= O
ReLU O
( O
WbReLU O
( O
W O
/ O
prime O
bxb‚àí1,c+b O
/ O
prime O
b O
) O
+ O
bb)+xb‚àí1,c O
, O
( O
3 O
) O
with O
base O
case O
: O
x0,c O
= O
ReLU O
( O
W0EŒ¥c+b0 O
) O
( O
4 O
) O
whereŒ¥cis O
a O
Kronecker O
delta O
function O
‚Äì O
a O
vector O
with O
value O
one O
at O
index O
cand O
zeros O
everywhere O
else O
‚Äì O
and O
E‚ààRd√óCis O
an O
embedding O
matrix O
for O
eachnonterminal O
category O
cwith O
embedding O
size O
d O
, O
and O
W0‚ààRh√ód O
, O
Wb O
, O
W O
/ O
prime O
b‚ààRh√óhandb0,bb O
, O
b O
/ O
prime O
b‚ààRh O
are O
model O
parameters O
with O
latent O
representations O
of O
size O
h. O
Bis O
set O
to O
2 O
in O
all O
models O
following O
Kim O
et O
al O
. O
( O
2019a O
) O
. O
Binary O
- O
branching O
non O
- O
terminal O
expansion O
rule O
probabilities O
for O
each O
non O
- O
terminal O
category O
cŒ∑and O
left O
and O
right O
children O
cŒ∑1cŒ∑2are O
deÔ¨Åned O
as O
: O
P(cŒ∑‚ÜícŒ∑1cŒ∑2)=P(Term O
= O
0|cŒ∑ O
) O
¬∑ O
softmax O
cŒ∑1,cŒ∑2(WnontEŒ¥cŒ∑+bnont),(5 O
) O
where O
Wnont‚ààRC2√ódandbnont‚ààRC2are O
parameters O
of O
the O
model O
. O
The O
lexical O
unary O
- O
expansion O
rule O
probabilities O
for O
a O
preterminal O
category O
cŒ∑and O
a O
word O
wŒ∑at O
nodeŒ∑are O
deÔ¨Åned O
as O
: O
P(cŒ∑‚ÜíwŒ∑)=P(Term O
= O
1|cŒ∑)¬∑exp(ncŒ∑ O
, O
wŒ∑ O
) O
/summationtext O
wexp(ncŒ∑ O
, O
w O
) O
( O
6 O
) O
nc O
, O
w O
= O
ReLU O
( O
w O
/ O
latticetop O
lexnB O
, O
c O
, O
w+blex O
) O
( O
7 O
) O
where O
wis O
the O
generated O
word O
type O
, O
and O
wlex‚ààRh O
andblex‚ààRare O
model O
parameters O
. O
Similarly O
, O
nb O
, O
c O
, O
w O
= O
ReLU O
( O
W O
/ O
prime O
/ O
prime O
bReLU O
( O
W O
/ O
prime O
/ O
prime O
/ O
prime O
bnb‚àí1,c O
, O
w+b O
/ O
prime O
/ O
prime O
/ O
prime O
b O
) O
+ O
b O
/ O
prime O
/ O
prime O
b)+nb‚àí1,c O
, O
w O
, O
( O
8) O
with O
base O
case O
: O
n0,c O
, O
w O
= O
ReLU O
( O
W O
/ O
prime O
0 O
/ O
bracketleftBiggEŒ¥c O
LŒ¥w O
/ O
bracketrightBigg O
) O
+ O
b O
/ O
prime O
0 O
) O
( O
9)398where O
W O
/ O
prime O
0‚ààRh√ó2d O
, O
W O
/ O
prime O
/ O
prime O
b O
, O
W O
/ O
prime O
/ O
prime O
/ O
prime O
b‚ààRh√óhand O
b0,b O
/ O
prime O
/ O
prime O
b O
, O
b O
/ O
prime O
/ O
prime O
/ O
prime O
b‚ààRhare O
model O
parameters O
for O
latent O
representations O
of O
size O
h. O
Lis O
a O
matrix O
of O
syntactic O
word O
embeddings O
for O
all O
words O
in O
vocabulary O
. O
4 O
Vision O
model O
The O
vision O
model O
consists O
of O
an O
image O
encoderdecoder O
network O
and O
a O
syntactic O
- O
visual O
projector O
. O
The O
image O
encoder O
- O
decoder O
network O
encodes O
an O
image O
into O
an O
image O
embedding O
and O
then O
decodes O
that O
back O
into O
the O
original O
image O
. O
This O
reconstruction O
constrains O
the O
information O
in O
the O
image O
embedding O
to O
be O
closely O
representative O
of O
the O
original O
image O
. O
The O
syntactic O
- O
visual O
projector O
projects O
word O
embeddings O
used O
in O
the O
calculation O
of O
lexical O
expansion O
probabilities O
into O
the O
space O
of O
image O
embeddings O
, O
building O
a O
connection O
between O
the O
space O
of O
syntactic O
information O
and O
the O
space O
of O
visual O
information O
. O
4.1 O
The O
image O
encoder O
- O
decoder O
network O
The O
image O
encoder O
employs O
a O
ResNet18 O
architecture O
( O
He O
et O
al O
. O
, O
2016 O
) O
which O
encodes O
an O
image O
with O
3 O
channels O
into O
a O
single O
vector O
. O
The O
encoder O
consists O
of O
four O
blocks O
of O
residual O
convolutional O
networks O
. O
The O
image O
decoder O
decodes O
an O
image O
from O
a O
visual O
vector O
generated O
by O
the O
image O
encoder O
. O
The O
image O
decoder O
used O
in O
the O
joint O
model O
is O
the O
image O
generator O
from O
DCGAN O
( O
Radford O
et O
al O
. O
, O
2016 O
) O
, O
where O
a O
series O
of O
transposed O
convolutions O
and O
batch O
normalizations O
attempts O
to O
recover O
an O
image O
from O
an O
image O
embedding.5 O
4.2 O
The O
syntactic O
- O
visual O
projector O
The O
projector O
model O
is O
a O
CNN O
- O
based O
neural O
network O
which O
takes O
a O
concatenated O
sentence O
embedding O
matrix O
MœÉ‚ààR|œÉ|√ódas O
input O
, O
where O
embeddings O
in O
MœÉare O
taken O
from O
L O
, O
and O
returns O
the O
syntactic O
- O
visual O
embedding O
eœÉ O
. O
The O
jth O
full O
lengthwise O
convolutional O
kernel O
is O
deÔ¨Åned O
as O
a O
matrix O
Kj‚ààRuj√ókjdwhich O
slides O
across O
the O
sentence O
matrixMto O
produce O
a O
feature O
map O
, O
where O
ujis O
the O
number O
of O
channels O
in O
the O
kernel O
, O
kjis O
the O
width O
of O
the O
kernel O
, O
and O
dis O
the O
height O
of O
the O
kernel O
which O
is O
equal O
to O
the O
size O
of O
the O
syntactic O
word O
embeddings O
. O
Because O
the O
kernel O
is O
as O
high O
as O
the O
embeddings O
, O
it O
produces O
one O
vector O
of O
length O
ujfor O
each O
window O
. O
The O
full O
feature O
map O
Fj‚ààRuj√óHj O
, O
where O
Hjis O
total O
5Details O
of O
these O
models O
can O
be O
found O
in O
the O
cited O
work O
and O
the O
appendix.number O
of O
valid O
submatrices O
for O
the O
kernel O
, O
is O
: O
Fj=/summationdisplay O
h(Kjvec(MœÉ[h O
.. O
kj+h‚àí1,‚àó])+bj)Œ¥ O
/ O
latticetop O
h.(10 O
) O
Finally O
, O
an O
average O
pooling O
layer O
and O
a O
linear O
transform O
are O
applied O
to O
feature O
maps O
from O
di O
Ô¨Äerent O
kernels O
: O
ÀÜf=[mean O
( O
F1) O
... O
mean O
( O
Fj)]/latticetop O
, O
( O
11 O
) O
eœÉ O
= O
tanh(WpoolReLU O
( O
ÀÜf)+bpool O
) O
. O
( O
12 O
) O
AllKs O
, O
bs O
and O
Ws O
here O
are O
parameters O
of O
the O
projector O
. O
5 O
Optimization O
There O
are O
three O
di O
Ô¨Äerent O
kinds O
of O
objectives O
used O
in O
the O
optimization O
of O
the O
full O
grounded O
induction O
model O
. O
The O
Ô¨Årst O
loss O
is O
the O
marginal O
likelihood O
loss O
for O
the O
PCFG O
induction O
model O
described O
in O
Equation O
1 O
, O
which O
can O
be O
calculated O
with O
the O
Inside O
algorithm O
. O
The O
second O
loss O
is O
the O
syntactic O
- O
visual O
loss O
. O
Given O
the O
encoded O
image O
embedding O
emand O
the O
projected O
syntactic O
- O
visual O
embedding O
eœÉof O
a O
sentenceœÉ O
, O
the O
syntactic O
- O
visual O
loss O
is O
the O
mean O
squared O
error O
of O
these O
two O
embeddings O
: O
L(em O
, O
eœÉ)=(em‚àíeœÉ)/latticetop(em‚àíeœÉ O
) O
. O
( O
13 O
) O
The O
third O
loss O
is O
the O
reconstruction O
loss O
of O
the O
image O
. O
Given O
the O
original O
image O
represented O
as O
a O
vector O
imand O
the O
reconstructed O
image O
ÀÜim O
, O
the O
reconstruction O
objective O
is O
the O
mean O
squared O
error O
of O
the O
corresponding O
pixel O
values O
of O
the O
two O
images O
: O
L(m)=(im‚àíÀÜim)/latticetop(im‚àíÀÜim O
) O
. O
( O
14 O
) O
Models O
with O
di O
Ô¨Äerent O
sets O
of O
input O
optimize O
the O
three O
losses O
di O
Ô¨Äerently O
for O
clean O
ablation O
. O
NoImagePCFG O
, O
which O
learns O
from O
text O
only O
, O
optimizes O
the O
negative O
marginal O
likelihood O
loss O
( O
the O
negative O
of O
Equation O
1 O
) O
using O
gradient O
descent O
. O
The O
model O
with O
pretrained O
image O
encoders O
, O
ImagePrePCFG O
, O
optimizes O
the O
negative O
marginal O
likelihood O
and O
the O
syntactic O
- O
visual O
loss O
( O
Equation O
13 O
) O
simultaneously O
. O
The O
full O
grounded O
grammar O
induction O
model O
ImagePCFG O
learns O
from O
text O
and O
images O
jointly O
by O
minimizing O
all O
three O
objectives O
: O
negative O
marginal O
likelihood O
, O
syntactic O
- O
visual O
loss O
and O
image O
reconstruction O
loss O
( O
Equation O
14 O
): O
L(œÉ O
, O
m)=‚àíP(œÉ)+L(em O
, O
eœÉ)+L(m O
) O
. O
( O
15)3996 O
Experiment O
methods O
Experiments O
described O
in O
this O
paper O
use O
the O
MSCOCO O
caption O
data O
set O
( O
Lin O
et O
al O
. O
, O
2015 O
) O
and O
the O
Multi30k O
dataset O
( O
Elliott O
et O
al O
. O
, O
2016 O
) O
, O
which O
contains O
pairs O
of O
images O
and O
descriptions O
of O
images O
written O
by O
human O
annotators O
. O
Captions O
in O
the O
MSCOCO O
data O
set O
are O
in O
English O
, O
whereas O
captions O
in O
the O
Multi30k O
dataset O
are O
in O
English O
, O
German O
and O
French O
. O
Captions O
are O
automatically O
parsed O
( O
Kitaev O
and O
Klein O
, O
2018 O
) O
to O
generate O
a O
version O
of O
the O
reference O
set O
with O
constituency O
trees.6In O
addition O
to O
these O
datasets O
with O
captions O
generated O
by O
human O
annotators O
, O
we O
automatically O
translate O
the O
English O
captions O
into O
Chinese O
, O
Polish O
and O
Korean O
using O
Google O
Translate,7and O
parse O
the O
resulting O
translations O
into O
constituency O
trees O
, O
which O
are O
then O
used O
in O
experiments O
to O
probe O
the O
interactions O
between O
visual O
information O
and O
grammar O
induction O
. O
Results O
from O
models O
proposed O
in O
this O
paper O
‚Äî O
NoImagePCFG O
, O
ImagePrePCFG O
and O
ImagePCFG O
‚Äî O
are O
compared O
with O
published O
results O
from O
Shi O
et O
al O
. O
( O
2019 O
) O
, O
which O
include O
PRPN O
( O
Shen O
et O
al O
. O
, O
2018 O
) O
, O
ON O
- O
LSTM O
( O
Shen O
et O
al O
. O
, O
2019 O
) O
as O
well O
as O
the O
grounded O
VG O
- O
NSL O
models O
which O
uses O
either O
head O
Ô¨Ånal O
bias O
( O
VG O
- O
NSL O
+ O
H O
) O
or O
head O
Ô¨Ånal O
bias O
and O
Fasttext O
embeddings O
( O
VG O
- O
NSL O
+ O
H+F O
) O
as O
inductive O
biases O
from O
external O
sources O
. O
All O
of O
these O
models O
only O
induce O
unlabeled O
structures O
and O
have O
been O
evaluated O
with O
unlabeled O
F1 O
scores O
. O
We O
additionally O
report O
the O
labeled O
evaluation O
score O
Recall O
- O
Homogeneity O
( O
Rosenberg O
and O
Hirschberg O
, O
2007 O
; O
Jin O
and O
Schuler O
, O
2020 O
) O
for O
better O
comparison O
between O
the O
proposed O
models O
. O
All O
evaluation O
is O
done O
on O
Viterbi O
parse O
trees O
of O
the O
test O
set O
from O
5 O
di O
Ô¨Äerent O
runs O
. O
Details O
about O
hyper O
- O
parameters O
and O
results O
on O
development O
data O
sets O
can O
be O
found O
in O
the O
appendix O
. O
However O
, O
importantly O
, O
the O
tuned O
hyperparameters O
for O
the O
grammar O
induction O
model O
are O
the O
same O
across O
the O
three O
proposed O
models O
, O
which O
facilitates O
direct O
comparisons O
among O
these O
models O
to O
determine O
the O
eÔ¨Äect O
of O
visual O
information O
on O
induction O
. O
6.1 O
Standard O
set O
: O
no O
replication O
of O
e O
Ô¨Äect O
for O
visual O
information O
Both O
unlabeled O
and O
labeled O
evaluation O
results O
are O
shown O
in O
Table O
1 O
with O
left- O
and O
right O
- O
branching O
baselines O
. O
First O
, O
trees O
induced O
by O
the O
PCFG O
induction O
models O
are O
more O
accurate O
than O
trees O
induced O
6The O
multilingual O
parsing O
accuracy O
for O
all O
languages O
used O
in O
this O
work O
has O
been O
validated O
in O
Fried O
et O
al O
. O
( O
2019 O
) O
and O
veriÔ¨Åed O
in O
Shi O
et O
al O
. O
( O
2019 O
) O
. O
7https://translate.google.com/ O
.with O
all O
other O
models O
, O
showing O
that O
the O
family O
of O
PCFG O
induction O
models O
is O
better O
at O
capturing O
syntactic O
regularities O
and O
provides O
a O
much O
stronger O
baseline O
for O
grammar O
induction O
. O
Second O
, O
using O
the O
NoImagePCFG O
model O
as O
a O
baseline O
, O
results O
from O
both O
the O
ImagePCFG O
model O
, O
where O
raw O
images O
are O
used O
as O
input O
, O
and O
the O
ImagePrePCFG O
model O
, O
where O
images O
encoded O
by O
pretrained O
image O
classiÔ¨Åers O
are O
used O
as O
input O
, O
do O
not O
show O
strong O
indication O
of O
beneÔ¨Åts O
of O
visual O
information O
in O
induction O
. O
The O
baseline O
NoImagePCFG O
outperforms O
other O
models O
by O
signiÔ¨Åcant O
margins O
on O
all O
languages O
in O
unlabeled O
evaluation O
. O
Compared O
to O
seemingly O
large O
gains O
between O
text O
- O
based O
models O
like O
PRPN O
and O
ON O
- O
LSTM8and O
the O
grounded O
models O
like O
VGNSL O
+ O
Hon O
French O
and O
German O
observed O
by O
Shi O
et O
al O
. O
( O
2019 O
) O
, O
the O
only O
positive O
gain O
between O
NoImagePCFG O
and O
ImagePCFG O
shown O
in O
Table O
1 O
is O
the O
labeled O
evaluation O
on O
French O
where O
ImagePCFG O
outperforms O
NoImagePCFG O
by O
a O
small O
margin O
. O
Because O
the O
only O
di O
Ô¨Äerence O
between O
NoImagePCFG O
and O
ImagePCFG O
models O
is O
whether O
the O
visual O
information O
inÔ¨Çuences O
the O
syntactic O
word O
embeddings O
, O
the O
results O
indicate O
that O
on O
these O
languages O
, O
visual O
information O
does O
not O
seem O
to O
help O
induction O
. O
The O
gain O
seen O
in O
previous O
results O
may O
therefore O
be O
from O
external O
inductive O
biases O
. O
Finally O
, O
the O
ImagePrePCFG O
model O
performs O
at O
slightly O
lower O
accuracies O
than O
the O
ImagePCFG O
model O
consistently O
across O
di O
Ô¨Äerent O
languages O
, O
datasets O
and O
evaluation O
metrics O
, O
showing O
that O
the O
information O
needed O
in O
grammar O
induction O
from O
images O
is O
not O
the O
same O
as O
information O
needed O
for O
image O
classiÔ¨Åcation O
, O
and O
such O
information O
can O
be O
extracted O
from O
images O
without O
annotated O
image O
classiÔ¨Åcation O
data O
. O
6.2 O
Languages O
with O
wider O
distribution O
of O
high O
- O
frequency O
word O
types O
: O
positive O
eÔ¨Äect O
One O
potential O
advantage O
of O
using O
visual O
information O
in O
induction O
is O
to O
ground O
nouns O
and O
noun O
phrases O
. O
For O
example O
, O
if O
images O
like O
in O
Figure O
1 O
are O
consistently O
presented O
to O
models O
with O
sentences O
describing O
spaghetti O
, O
the O
models O
may O
learn O
the O
categorize O
words O
and O
phrases O
which O
could O
be O
linked O
with O
objects O
in O
images O
as O
nominal O
units O
and O
then O
bootstrap O
other O
lexical O
categories O
. O
However O
, O
in O
the O
test O
languages O
above O
, O
a O
narrow O
set O
of O
very O
high O
fre8PCFG O
induction O
models O
where O
a O
grammar O
is O
induced O
generally O
perform O
better O
in O
parsing O
evaluation O
than O
sequence O
models O
where O
only O
syntactic O
structures O
are O
induced O
( O
Kim O
et O
al O
. O
, O
2019a O
; O
Jin O
et O
al O
. O
, O
2019).400ModelsMSCOCO O
Multi30k O
English O
* O
* O
English O
* O
* O
German O
* O
* O
French O
* O
* O
F1 O
RH O
F1 O
RH O
F1 O
RH O
F1 O
RH O
Left O
- O
branching O
23.3 O
- O
22.6 O
- O
34.7 O
- O
19.0 O
Right O
- O
branching O
21.4 O
- O
11.3 O
- O
12.1 O
- O
11.0 O
PRPN O
52.5 O
¬±2.6- O
30.8 O
¬±17.9- O
31.5 O
¬±8.9- O
27.5 O
¬±7.0ON O
- O
LSTM O
45.5 O
¬±3.3- O
38.7 O
¬±12.7- O
34.9 O
¬±12.3- O
27.7 O
¬±5.6VG O
- O
NSL O
+ O
H53.3¬±0.2- O
38.7 O
¬±0.2- O
38.3 O
¬±0.2- O
38.1 O
¬±0.6VG O
- O
NSL O
+ O
H+F54.4¬±0.4- O
- O
- O
- O
- O
- O
NoImagePCFG O
60.0¬±8.247.6¬±10.059.4¬±7.751.6¬±8.548.1¬±5.253.7¬±5.244.3¬±5.143.8¬±5.2 O
ImagePrePCFG O
55.6 O
¬±7.542.3¬±7.347.0¬±7.040.5¬±7.246.2¬±7.451.1¬±8.042.6¬±10.343.4¬±10.8 O
ImagePCFG O
55.1 O
¬±2.742.5¬±1.548.2¬±4.940.5¬±5.047.0¬±5.551.8¬±8.443.6¬±5.544.5¬±6.3 O
Table O
1 O
: O
Averages O
and O
standard O
deviations O
of O
labeled O
Recall O
- O
Homogeneity O
and O
unlabeled O
F1 O
scores O
of O
various O
unsupervised O
grammar O
inducers O
on O
the O
MSCOCO O
and O
Multi30k O
caption O
datasets O
. O
VG O
- O
NSL O
+ O
H O
: O
VG O
- O
NSL O
system O
with O
head O
Ô¨Ånal O
bias O
. O
VG O
- O
NSL O
+ O
H+F O
: O
VG O
- O
NSL O
system O
with O
head O
Ô¨Ånal O
bias O
and O
Fasttext O
word O
embeddings O
. O
( O
* O
* O
: O
the O
unlabeled O
performance O
di O
Ô¨Äerence O
between O
NoImagePCFG O
and O
ImagePCFG O
is O
signiÔ¨Åcant O
p<0.01 O
. O
) O
quency O
words O
such O
as O
determiners O
provide O
strong O
identifying O
information O
for O
nouns O
and O
noun O
phrases O
, O
which O
may O
greatly O
diminish O
the O
advantage O
contributed O
by O
visual O
information O
. O
In O
such O
cases O
, O
visual O
information O
may O
even O
be O
harmful O
, O
as O
models O
may O
attend O
to O
other O
information O
in O
images O
which O
is O
irrelevant O
to O
induction O
. O
Korean O
, O
Polish O
and O
Chinese O
are O
chosen O
as O
representatives O
of O
languages O
with O
no O
deÔ¨Ånite O
articles O
, O
and O
in O
which O
statistical O
information O
provided O
by O
high O
frequency O
words O
is O
less O
reliable O
because O
there O
are O
more O
such O
word O
types O
. O
Table O
2 O
shows O
the O
performance O
scores O
of O
the O
three O
proposed O
systems O
on O
these O
languages O
. O
Comparing O
to O
results O
in O
Table O
1 O
, O
the O
models O
with O
visual O
information O
in O
the O
input O
signiÔ¨Åcantly O
outperform O
the O
baseline O
model O
, O
NoImagePCFG O
, O
on O
a O
majority O
of O
the O
additional O
test O
datasets O
. O
Figure O
3 O
shows O
the O
correlation O
between O
the O
RH O
di O
Ô¨Äerence O
between O
the O
ImagePCFG O
model O
and O
the O
NoImagePCFG O
model O
on O
each O
language O
in O
an O
image O
dataset O
, O
and O
the O
distribution O
of O
high O
frequency O
words O
in O
that O
language O
, O
deÔ¨Åned O
as O
the O
number O
of O
word O
types O
needed O
to O
account O
for O
10 O
% O
of O
the O
number O
of O
word O
tokens O
in O
the O
Universal O
Dependency O
( O
Nivre O
et O
al O
. O
, O
2016 O
) O
corpus O
of O
a O
language.9The O
Ô¨Ågure O
shows O
that O
the O
largest O
gain O
brought O
by O
visual O
information O
in O
induction O
is O
on O
Korean O
, O
where O
the O
number O
of O
high O
frequency O
word O
types O
is O
also O
highest O
. O
Results O
on O
Chinese O
and O
Polish O
9Korean O
has O
41 O
, O
Chinese O
and O
Polish O
have O
5 O
, O
German O
has O
4 O
, O
English O
has O
3 O
and O
French O
has O
2 O
. O
101 O
log O
# O
High O
Freq O
Words6 O
4 O
2 O
024MSCOCO O
RH O
Diff O
English O
PolishKorean O
Chinese O
101 O
log O
# O
High O
Freq O
Words10 O
5 O
0510Multi30k O
RH O
Diff O
EnglishPolishKorean O
ChineseFrench O
GermanFigure O
3 O
: O
The O
correlation O
between O
number O
of O
word O
types O
needed O
to O
account O
for O
10 O
% O
of O
word O
tokens O
in O
a O
language O
( O
log O
# O
High O
Freq O
Words O
) O
and O
the O
RH O
gain O
from O
NoImagePCFG O
to O
ImagePCFG O
on O
di O
Ô¨Äerent O
languages O
on O
the O
two O
di O
Ô¨Äerent O
image O
datasets O
. O
also O
show O
a O
beneÔ¨Åt O
for O
visual O
information O
, O
although O
the O
gain O
is O
much O
smaller O
and O
less O
consistent O
. O
It O
also O
shows O
that O
when O
there O
is O
a O
trend O
of O
positive O
correlation O
between O
the O
number O
of O
high O
frequency O
words O
and O
the O
gain O
brought O
by O
visual O
information O
, O
factors O
other O
than O
high O
frequency O
words O
are O
at O
play O
as O
well O
in O
determining O
the O
Ô¨Ånal O
induction O
outcome O
for O
each O
dataset O
in O
each O
language O
in O
the O
visually O
grounded O
setup O
, O
which O
are O
left O
for O
investigation O
in O
future O
work O
. O
7 O
Analysis O
of O
advantages O
of O
visual O
information O
We O
hypothesize O
three O
speciÔ¨Åc O
ways O
that O
visual O
information O
may O
help O
grammar O
induction O
. O
First O
, O
a O
strong O
correlation O
between O
words O
and O
objects O
in O
images O
can O
help O
identiÔ¨Åcation O
and O
categorization401Models O
on O
MSCOCOKorean O
* O
* O
Polish O
* O
* O
Chinese O
* O
* O
F1 O
RH O
F1 O
RH O
F1 O
RH O
NoImagePCFG O
38.1 O
¬±8.522.3¬±6.858.9¬±3.747.1¬±3.861.2¬±3.548.5¬±3.7 O
ImagePrePCFG O
39.0 O
¬±4.123.5¬±3.260.5¬±1.849.8¬±3.360.0¬±4.647.2¬±4.5 O
ImagePCFG O
45.0¬±2.227.1¬±2.653.6¬±8.341.3¬±7.864.9¬±6.651.2¬±8.6 O
Models O
on O
Multi30kKorean O
* O
* O
Polish O
Chinese O
* O
* O
F1 O
RH O
F1 O
RH O
F1 O
RH O
NoImagePCFG O
30.7 O
¬±5.622.8¬±3.149.6¬±4.639.9¬±5.159.1¬±3.353.2¬±4.7 O
ImagePrePCFG O
27.1 O
¬±4.419.9¬±3.448.4¬±3.138.3¬±2.957.9¬±7.051.0¬±7.7 O
ImagePCFG O
44.9¬±1.333.8¬±2.149.7¬±7.240.4¬±6.158.5¬±3.252.8¬±4.6 O
Table O
2 O
: O
Averages O
and O
standard O
deviations O
of O
labeled O
Recall O
- O
Homogeneity O
and O
unlabeled O
F1 O
scores O
of O
various O
unsupervised O
grammar O
inducers O
on O
the O
MSCOCO O
and O
Multi30k O
caption O
datasets O
in O
the O
additional O
languages O
with O
high O
numbers O
of O
high O
- O
frequency O
word O
types O
. O
( O
* O
* O
: O
the O
unlabeled O
performance O
di O
Ô¨Äerence O
between O
NoImagePCFG O
and O
ImagePCFG O
is O
signiÔ¨Åcant O
p<0.01 O
. O
) O
of O
nouns O
and O
noun O
phrases O
, O
especially O
on O
languages O
where O
nouns O
and O
noun O
phrases O
are O
not O
readily O
identiÔ¨Åable O
by O
neighboring O
high O
frequency O
words O
. O
Second O
, O
visual O
information O
may O
provide O
bottom O
- O
up O
information O
for O
unknown O
word O
embeddings O
. O
Languages O
where O
neighboring O
words O
can O
reliably O
predict O
the O
grammatical O
category O
of O
an O
unknown O
word O
may O
build O
robust O
representations O
of O
unknown O
word O
embeddings O
, O
but O
the O
construction O
of O
the O
UNK O
embedding O
may O
also O
beneÔ¨Åt O
from O
bottom O
- O
up O
information O
from O
images O
, O
especially O
when O
sentential O
context O
is O
not O
enough O
to O
build O
informative O
UNK O
embeddings O
. O
Finally O
, O
semantic O
information O
inside O
images O
may O
be O
helpful O
in O
solving O
syntactic O
ambiguities O
like O
prepositional O
phrase O
attachment O
in O
languages O
like O
English O
. O
Results O
from O
experiments O
described O
below O
with O
the O
ImagePCFG O
and O
NoImagePCFG O
models O
show O
evidence O
of O
all O
three O
ways O
. O
7.1 O
Grounding O
of O
nouns O
and O
noun O
phrases O
The O
‚Äò O
Noun O
bias O
‚Äô O
hypothesis O
( O
Gentner O
, O
1982 O
) O
postulates O
that O
visual O
information O
in O
the O
induction O
process O
may O
impact O
how O
words O
are O
categorized O
grammatically O
, O
and O
nouns O
may O
receive O
an O
advantage O
because O
they O
correspond O
to O
objects O
in O
images O
. O
However O
, O
objects O
in O
images O
are O
often O
described O
with O
phrases O
, O
not O
single O
words O
. O
For O
example O
, O
captions O
likea O
red O
car O
is O
parked O
on O
the O
street O
, O
are O
common O
in O
both O
caption O
datasets O
, O
where O
the O
objects O
in O
the O
image O
may O
associate O
more O
strongly O
with O
modiÔ¨Åer O
words O
like O
redthan O
the O
head O
noun O
car O
. O
Evaluations O
are O
carried O
out O
on O
the O
parsed O
sentences O
of O
all O
languages O
from O
two O
caption O
datasetsusing O
a O
part O
- O
of O
- O
speech O
homogeneity O
metric O
( O
Rosenberg O
and O
Hirschberg O
, O
2007 O
) O
for O
measuring O
the O
partof O
- O
speech O
accuracy O
, O
and O
an O
unlabeled O
NP O
recall O
score O
for O
measuring O
how O
many O
noun O
phrases O
in O
gold O
annotation O
are O
also O
found O
in O
the O
induced O
trees O
. O
Results O
in O
Figure O
4 O
Ô¨Årst O
show O
that O
the O
POS O
homogeneity O
scores O
from O
di O
Ô¨Äerent O
models O
on O
the O
same O
induction O
dataset O
are O
extremely O
close O
to O
each O
other O
. O
Given O
that O
nouns O
are O
one O
of O
the O
categories O
with O
the O
most O
numerous O
tokens O
, O
the O
almost O
identical O
performance O
of O
POS O
homogeneity O
across O
di O
Ô¨Äerent O
models O
indicates O
that O
the O
unsupervised O
clustering O
accuracy O
for O
nouns O
across O
di O
Ô¨Äerent O
models O
is O
also O
very O
close O
, O
in O
contrast O
to O
substantial O
RH O
score O
differences O
on O
English O
and O
Korean O
. O
However O
, O
NP O
recall O
scores O
show O
a O
pattern O
of O
performance O
ranking O
that O
resembles O
the O
ranking O
observed O
in O
Tables O
1 O
and O
2 O
. O
For O
all O
datasets O
except O
for O
the O
Polish O
Multi30k O
dataset O
, O
when O
the O
RH O
score O
of O
ImagePCFG O
is O
higher O
than O
NoImagePCFG O
, O
the O
NP O
recall O
score O
for O
the O
ImagePCFG O
model O
is O
also O
higher O
. O
SigniÔ¨Åcance O
testing O
with O
permutation O
sampling O
shows O
that O
all O
performance O
di O
Ô¨Äerences O
are O
signiÔ¨Åcant O
( O
p<0.01).10High O
accuracy O
on O
noun O
phrases O
is O
crucial O
to O
high O
accuracy O
of O
other O
constituents O
such O
as O
prepositional O
phrases O
and O
verb O
phrases O
, O
which O
usually O
contain O
noun O
phrases O
, O
and O
eventually O
leads O
to O
high O
overall O
accuracy O
. O
This O
result O
suggests O
that O
the O
beneÔ¨Åt O
contributed O
by O
visual O
information O
works O
at O
phrasal O
levels O
, O
most O
likely O
10SigniÔ¨Åcance O
testing O
is O
not O
done O
on O
POS O
homogeneity O
due O
to O
the O
possibility O
that O
the O
same O
induced O
POS O
label O
may O
mean O
diÔ¨Äerent O
things O
in O
di O
Ô¨Äerent O
induced O
grammars.402English O
KoreanPolishChinese O
German O
English O
French O
KoreanPolishChinese0.00.20.40.60.81.0POS O
Homogeneity O
MSCOCO O
Multi30kNoImagePCFG O
ImagePCFG O
English O
KoreanPolishChinese O
German O
English O
French O
KoreanPolishChinese0.00.20.40.60.81.0NP O
Recall O
MSCOCO O
Multi30k O
* O
* O
* O
* O
* O
* O
* O
* O
* O
* O
* O
* O
* O
* O
* O
* O
* O
* O
* O
* O
NoImagePCFG O
ImagePCFGFigure O
4 O
: O
The O
POS O
Homogeneity O
and O
NP O
Recall O
scores O
for O
the O
ImagePCFG O
and O
NoImagePCFG O
models O
across O
the O
test O
languages O
( O
* O
* O
: O
p<0.01 O
) O
. O
Total O
High O
LowTotal O
High O
Low0.00.20.40.60.81.0Accuracy O
MSCOCO O
Multi30k******NoImagePCFG O
ImagePCFG O
Figure O
5 O
: O
The O
average O
overall O
accuracy O
as O
well O
as O
accuracies O
for O
high O
and O
low O
attachment O
sentences O
in O
PP O
attachment O
evaluation O
for O
models O
with O
and O
without O
visual O
information O
( O
* O
* O
: O
p<0.01 O
, O
* O
: O
p<0.05 O
) O
. O
grounding O
phrases O
, O
not O
words O
, O
with O
objects O
in O
images O
. O
7.2 O
Informativeness O
of O
the O
UNK O
embedding O
The O
informativeness O
of O
unknown O
word O
embeddings O
is O
tested O
among O
the O
induction O
models O
across O
different O
languages O
. O
An O
UNK O
test O
set O
is O
created O
by O
randomly O
replacing O
one O
word O
in O
one O
sentence O
with O
an O
UNK O
symbol O
if O
the O
sentence O
has O
no O
unknown O
words O
present O
. O
Table O
3 O
shows O
the O
labeled O
evaluation O
results O
on O
the O
multilingual O
datasets.11First O
, O
performance O
on O
the O
UNK O
test O
sets O
on O
all O
languages O
is O
lower O
than O
on O
the O
normal O
test O
sets O
, O
showing O
that O
replacing O
random O
words O
with O
UNK O
symbols O
does O
impact O
performance O
. O
The O
performance O
ranking O
of O
the O
models O
on O
a O
majority O
of O
the O
languages O
is O
consistent O
with O
the O
ranking O
on O
the O
normal O
test O
set O
. O
The O
ranking O
of O
the O
models O
on O
one O
dataset O
, O
the O
Chinese O
Multi30k O
, O
is O
reversed O
on O
the O
UNK O
test O
set O
, O
where O
the O
ImagePCFG O
models O
show O
signiÔ¨Åcantly O
higher O
performance O
than O
the O
NoImagePCFG O
models O
( O
Chinese O
: O
p<0.01 O
, O
permutation O
test O
on O
unlabeled O
F1 O
) O
. O
This O
result O
indicates O
that O
the O
ImagePCFG O
model O
in O
which O
visual O
information O
is O
supplied O
during O
train11The O
unlabeled O
evaluation O
results O
can O
be O
found O
in O
the O
appendix.ing O
may O
have O
built O
more O
informative O
embeddings O
for O
the O
unknown O
word O
symbols O
, O
helping O
the O
model O
to O
outperform O
the O
model O
without O
visual O
information O
on O
a O
majority O
of O
datasets O
where O
UNK O
symbols O
are O
frequent O
. O
7.3 O
Prepositional O
phrase O
attachment O
Finally O
, O
visual O
information O
may O
provide O
semantic O
information O
to O
resolve O
structural O
ambiguities O
. O
Word O
quintuples O
such O
as O
( O
a O
) O
hotel O
caught O
Ô¨Åre O
during O
( O
a O
) O
storm O
were O
extracted O
from O
English O
Wikipedia O
and O
the O
attachment O
locations O
were O
automatically O
labeled O
either O
as O
‚Äò O
n O
‚Äô O
for O
low O
attachment O
, O
where O
the O
prepositional O
phrase O
adjoins O
the O
direct O
object O
, O
or O
‚Äò O
v O
‚Äô O
for O
high O
attachment O
, O
where O
the O
prepositional O
phrase O
adjoins O
the O
main O
verb O
( O
Nakashole O
and O
Mitchell O
, O
2015 O
) O
. O
168 O
test O
items O
are O
selected O
by O
human O
annotators O
for O
evaluation O
, O
within O
which O
119 O
are O
sentences O
with O
high O
attached O
PPs O
and O
49 O
are O
with O
low O
attached O
PPs O
. O
For O
evaluation O
of O
PP O
attachment O
with O
induced O
trees O
, O
one O
test O
item O
is O
labeled O
correct O
when O
the O
induced O
tree O
puts O
the O
main O
verb O
and O
the O
direct O
object O
into O
one O
constituent O
and O
it O
is O
labeled O
as O
‚Äò O
v O
‚Äô O
. O
For O
example O
, O
if O
the O
induced O
tree O
has O
caught O
Ô¨Åre O
as O
a O
constituent O
, O
it O
counts O
as O
correct O
for O
the O
above O
example O
with O
high O
attachment O
. O
Low O
attachment O
trees O
must O
have O
a O
constituent O
with O
the O
direct O
object O
and O
the O
prepositional O
phrase O
. O
For O
example O
, O
for O
the O
sentence O
( O
a O
) O
guide O
gives O
talks O
about O
animals O
, O
the O
induced O
tree O
must O
have O
talks O
about O
animals O
. O
Average O
accuracies O
for O
all O
sentences O
as O
well O
as O
for O
sentences O
with O
high O
attachment O
or O
low O
attachment O
with O
induced O
grammars O
are O
shown O
in O
Figure O
5 O
. O
Results O
show O
that O
the O
models O
trained O
with O
visual O
information O
on O
both O
datasets O
show O
signiÔ¨Åcantly O
higher O
performance O
on O
the O
PP O
attachment O
task O
in O
most O
of O
the O
categories O
, O
except O
for O
the O
low O
attachment O
category O
with O
Multi30k O
models O
where O
the O
performance O
from O
both O
models O
is O
not O
signiÔ¨Åcantly O
di O
Ô¨Äerent O
. O
This O
is O
in O
contrast O
to O
the403ModelsMSCOCO O
Multi30k O
En O
Ko O
Pl O
Zh O
De O
En O
Fr O
Ko O
Pl O
Zh O
NoImagePCFG O
46.2 O
21.7 O
45.8 O
46.0 O
52.8 O
49.9 O
42.2 O
22.8 O
38.9 O
51.6 O
ImagePCFG O
41.2 O
26.4 O
40.2 O
48.1 O
51.3 O
39.9 O
42.6 O
33.2 O
39.7 O
53.2 O
Table O
3 O
: O
Average O
labeled O
Recall O
- O
Homogeneity O
of O
the O
NoImagePCFG O
and O
ImagePCFG O
models O
on O
the O
MSCOCO O
and O
Multi30k O
caption O
datasets O
with O
random O
words O
replaced O
by O
the O
UNK O
symbol O
. O
Standard O
deviations O
across O
the O
datasets O
are O
similar O
to O
what O
is O
reported O
in O
Table O
1 O
and O
2 O
. O
Chinese O
Multi30k O
is O
the O
one O
on O
which O
the O
NoImagePCFG O
model O
outperforms O
the O
ImagePCFG O
model O
on O
the O
normal O
test O
set O
but O
not O
on O
the O
UNK O
test O
set O
. O
higher O
performance O
of O
the O
NoImagePCFG O
models O
on O
unlabeled O
F1 O
and O
labeled O
RH O
than O
that O
of O
the O
ImagePCFG O
models O
on O
English O
from O
both O
caption O
datasets O
. O
Results O
indicate O
that O
induction O
models O
use O
visual O
information O
for O
weighting O
competing O
latent O
syntactic O
trees O
for O
a O
sentence O
, O
which O
is O
consistent O
with O
the O
third O
hypothesized O
advantage O
of O
visual O
information O
for O
induction O
. O
This O
also O
indicates O
that O
the O
reason O
that O
the O
overall O
parsing O
performance O
of O
ImagePCFG O
on O
English O
is O
lower O
than O
NoImagePCFG O
lies O
within O
other O
syntactic O
structures O
, O
which O
is O
left O
for O
future O
work O
. O
8 O
Conclusion O
This O
work O
proposed O
several O
novel O
neural O
networkbased O
models O
of O
grammar O
induction O
which O
take O
into O
account O
visual O
information O
in O
induction O
. O
These O
models O
achieve O
state O
- O
of O
- O
the O
- O
art O
results O
on O
multilingual O
induction O
datasets O
without O
any O
help O
from O
linguistic O
knowledge O
or O
pretrained O
image O
encoders O
. O
Further O
analyses O
isolated O
three O
hypothesized O
beneÔ¨Åts O
of O
visual O
information O
: O
it O
helps O
categorize O
noun O
phrases O
, O
represent O
unknown O
words O
and O
resolve O
syntactic O
ambiguities O
. O
Acknowledgments O
The O
authors O
would O
like O
to O
thank O
the O
anonymous O
reviewers O
for O
their O
helpful O
comments O
. O
Computations O
for O
this O
project O
were O
partly O
run O
on O
the O
Ohio O
Supercomputer O
Center O
( O
1987 O
) O
. O
This O
work O
was O
supported O
by O
the O
Presidential O
Fellowship O
from O
the O
Ohio O
State O
University O
. O
The O
content O
of O
the O
information O
does O
not O
necessarily O
reÔ¨Çect O
the O
position O
or O
the O
policy O
of O
the O
Government O
, O
and O
no O
o O
Ô¨Écial O
endorsement O
should O
be O
inferred O
. O
This O
work O
was O
also O
supported O
by O
the O
National O
Science O
Foundation O
grant O
# O
1816891 O
. O
All O
views O
expressed O
are O
those O
of O
the O
authors O
and O
do O
not O
necessarily O
reÔ¨Çect O
the O
views O
of O
the O
National O
Science O
Foundation O
. O
References O
Noam O
Chomsky O
. O
1965 O
. O
Aspects O
of O
the O
Theory O
of O
Syntax O
. O
MIT O
Press O
, O
Cambridge O
, O
MA O
. O
Gordon O
Christie O
, O
Ankit O
Laddha O
, O
Aishwarya O
Agrawal O
, O
Stanislaw O
Antol O
, O
Yash O
Goyal O
, O
Kevin O
Kochersberger O
, O
and O
Dhruv O
Batra O
. O
2016 O
. O
Resolving O
language O
and O
vision O
ambiguities O
together O
: O
Joint O
segmentation O
& O
Prepositional O
attachment O
resolution O
in O
captioned O
scenes O
. O
In O
EMNLP O
2016 O
- O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
Proceedings O
, O
pages O
1493‚Äì1503 O
. O
Jia O
Deng O
, O
Wei O
Dong O
, O
Richard O
Socher O
, O
Li O
- O
Jia O
Li O
, O
Kai O
Li O
, O
and O
Li O
Fei O
- O
Fei O
. O
2009 O
. O
Imagenet O
: O
A O
large O
- O
scale O
hierarchical O
image O
database O
. O
In O
2009 O
IEEE O
conference O
on O
computer O
vision O
and O
pattern O
recognition O
, O
pages O
248‚Äì255 O
. O
Ieee O
. O
Andrew O
Drozdov O
, O
Patrick O
Verga O
, O
Yi O
- O
Pei O
Chen O
, O
Mohit O
Iyyer O
, O
and O
Andrew O
McCallum O
. O
2019 O
. O
Unsupervised O
labeled O
parsing O
with O
deep O
inside O
- O
outside O
recursive O
autoencoders O
. O
In O
Proceedings O
of O
the O
2019 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
and O
the O
9th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
( O
EMNLPIJCNLP O
) O
, O
pages O
1507‚Äì1512 O
, O
Hong O
Kong O
, O
China O
. O
Association O
for O
Computational O
Linguistics O
. O
Desmond O
Elliott O
, O
Stella O
Frank O
, O
Khalil O
Sima‚Äôan O
, O
and O
Lucia O
Specia O
. O
2016 O
. O
Multi30 O
K O
: O
Multilingual O
EnglishGerman O
Image O
Descriptions O
. O
In O
Proceedings O
of O
the O
5th O
Workshop O
on O
Vision O
and O
Language O
, O
pages O
70 O
‚Äì O
74 O
, O
Berlin O
, O
Germany O
. O
Association O
for O
Computational O
Linguistics O
. O
Daniel O
Fried O
, O
Nikita O
Kitaev O
, O
and O
Dan O
Klein O
. O
2019 O
. O
Cross O
- O
domain O
generalization O
of O
neural O
constituency O
parsers O
. O
In O
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
323‚Äì330 O
, O
Florence O
, O
Italy O
. O
Association O
for O
Computational O
Linguistics O
. O
Dedre O
Gentner O
. O
1982 O
. O
Why O
nouns O
are O
learned O
before O
verbs O
: O
Linguistic O
relativity O
versus O
natural O
partitioning O
. O
Language O
development O
: O
Vol O
. O
2 O
. O
Language O
, O
thought O
, O
and O
culture O
, O
2(1):301‚Äì334 O
. O
Dedre O
Gentner O
. O
2006 O
. O
Why O
Verbs O
Are O
Hard O
to O
Learn O
. O
In O
K. O
Hirsh O
- O
Pasek O
and O
R. O
Golinko O
Ô¨Ä O
, O
editors O
, O
Action O
Meets O
Word O
: O
How O
Children O
Learn O
Verbs O
, O
pages O
544 O
‚Äì O
564 O
. O
Oxford O
University O
Press.404Lila O
Gleitman O
. O
1990 O
. O
The O
Structural O
Sources O
of O
Verb O
Meanings O
. O
Language O
Acquisition O
, O
1(1):3‚Äì55 O
. O
Ajda O
Gokcen O
, O
Ethan O
Hill O
, O
and O
Michael O
White O
. O
2018 O
. O
Madly O
ambiguous O
: O
A O
game O
for O
learning O
about O
structural O
ambiguity O
and O
why O
it O
‚Äôs O
hard O
for O
computers O
. O
In O
Proceedings O
of O
the O
2018 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Demonstrations O
, O
pages O
51‚Äì55 O
, O
New O
Orleans O
, O
Louisiana O
. O
Association O
for O
Computational O
Linguistics O
. O
Kaiming O
He O
, O
Xiangyu O
Zhang O
, O
Shaoqing O
Ren O
, O
and O
Jian O
Sun O
. O
2016 O
. O
Deep O
residual O
learning O
for O
image O
recognition O
. O
In O
Proceedings O
of O
the O
IEEE O
Computer O
Society O
Conference O
on O
Computer O
Vision O
and O
Pattern O
Recognition O
, O
volume O
2016 O
- O
Decem O
, O
pages O
770‚Äì778 O
. O
Felix O
Hill O
and O
Anna O
Korhonen O
. O
2014 O
. O
Concreteness O
and O
subjectivity O
as O
dimensions O
of O
lexical O
meaning O
. O
In52nd O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
ACL O
2014 O
- O
Proceedings O
of O
the O
Conference O
, O
volume O
2 O
, O
pages O
725‚Äì731 O
. O
Felix O
Hill O
, O
Roi O
Reichart O
, O
and O
Anna O
Korhonen O
. O
2014 O
. O
Multi O
- O
Modal O
Models O
for O
Concrete O
and O
Abstract O
Concept O
Meaning O
. O
Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
2:285‚Äì296 O
. O
Lifeng O
Jin O
, O
Finale O
Doshi O
- O
Velez O
, O
Timothy O
Miller O
, O
William O
Schuler O
, O
and O
Lane O
Schwartz O
. O
2018a O
. O
Depthbounding O
is O
e O
Ô¨Äective O
: O
Improvements O
and O
evaluation O
of O
unsupervised O
PCFG O
induction O
. O
In O
Proceedings O
of O
the O
2018 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
2721‚Äì2731 O
, O
Brussels O
, O
Belgium O
. O
Association O
for O
Computational O
Linguistics O
. O
Lifeng O
Jin O
, O
Finale O
Doshi O
- O
Velez O
, O
Timothy O
Miller O
, O
William O
Schuler O
, O
and O
Lane O
Schwartz O
. O
2018b O
. O
Unsupervised O
grammar O
induction O
with O
depth O
- O
bounded O
PCFG O
. O
Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
6:211‚Äì224 O
. O
Lifeng O
Jin O
, O
Finale O
Doshi O
- O
Velez O
, O
Timothy O
Miller O
, O
Lane O
Schwartz O
, O
and O
William O
Schuler O
. O
2019 O
. O
Unsupervised O
learning O
of O
PCFGs O
with O
normalizing O
Ô¨Çow O
. O
InProceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
2442‚Äì2452 O
, O
Florence O
, O
Italy O
. O
Association O
for O
Computational O
Linguistics O
. O
Lifeng O
Jin O
and O
William O
Schuler O
. O
2019 O
. O
Variance O
of O
average O
surprisal O
: O
A O
better O
predictor O
for O
quality O
of O
grammar O
from O
unsupervised O
PCFG O
induction O
. O
In O
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
2453‚Äì2463 O
, O
Florence O
, O
Italy O
. O
Association O
for O
Computational O
Linguistics O
. O
Lifeng O
Jin O
and O
William O
Schuler O
. O
2020 O
. O
The O
Importance O
of O
Category O
Labels O
in O
Grammar O
Induction O
with O
Child O
- O
directed O
Utterances O
. O
In O
Proceedings O
of O
16th O
International O
Conference O
on O
Parsing O
Technologies O
, O
Seattle O
, O
USA O
. O
Association O
for O
Computational O
Linguistics O
. O
Mark O
Johnson O
, O
Thomas O
L. O
Gri O
Ô¨Éths O
, O
and O
Sharon O
Goldwater O
. O
2007 O
. O
Bayesian O
Inference O
for O
PCFGs O
via O
Markov O
chain O
Monte O
Carlo O
. O
Proceedings O
of O
Human O
Language O
Technologies O
: O
The O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
139‚Äì146 O
. O
Yoon O
Kim O
, O
Chris O
Dyer O
, O
and O
Alexander O
Rush O
. O
2019a O
. O
Compound O
probabilistic O
context O
- O
free O
grammars O
for O
grammar O
induction O
. O
In O
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
2369‚Äì2385 O
, O
Florence O
, O
Italy O
. O
Association O
for O
Computational O
Linguistics O
. O
Yoon O
Kim O
, O
Alexander O
Rush O
, O
Lei O
Yu O
, O
Adhiguna O
Kuncoro O
, O
Chris O
Dyer O
, O
and O
G O
¬¥ O
abor O
Melis O
. O
2019b O
. O
Unsupervised O
recurrent O
neural O
network O
grammars O
. O
In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
1105‚Äì1117 O
, O
Minneapolis O
, O
Minnesota O
. O
Association O
for O
Computational O
Linguistics O
. O
Nikita O
Kitaev O
and O
Dan O
Klein O
. O
2018 O
. O
Constituency O
parsing O
with O
a O
self O
- O
attentive O
encoder O
. O
In O
Proceedings O
of O
the O
56th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
2676‚Äì2686 O
, O
Melbourne O
, O
Australia O
. O
Association O
for O
Computational O
Linguistics O
. O
Tsung O
- O
Yi O
Lin O
, O
Michael O
Maire O
, O
Serge O
Belongie O
, O
Lubomir O
Bourdev O
, O
Ross O
Girshick O
, O
James O
Hays O
, O
Pietro O
Perona O
, O
Deva O
Ramanan O
, O
C. O
Lawrence O
Zitnick O
, O
and O
Piotr O
Doll O
¬¥ O
ar O
. O
2015 O
. O
Microsoft O
COCO O
: O
Common O
Objects O
in O
Context O
. O
In O
Proceedings O
of O
the O
IEEE O
Computer O
Society O
Conference O
on O
Computer O
Vision O
and O
Pattern O
Recognition O
, O
pages O
3686‚Äì3693 O
. O
Ndapandula O
Nakashole O
and O
Tom O
M O
Mitchell O
. O
2015 O
. O
A O
knowledge O
- O
intensive O
model O
for O
prepositional O
phrase O
attachment O
. O
In O
ACL O
- O
IJCNLP O
2015 O
- O
53rd O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
7th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
of O
the O
Asian O
Federation O
of O
Natural O
Language O
Processing O
, O
Proceedings O
of O
the O
Conference O
, O
volume O
1 O
, O
pages O
365‚Äì375 O
. O
Joakim O
Nivre O
, O
Marie O
- O
Catherine O
De O
Marne O
Ô¨Äe O
, O
Filip O
Ginter O
, O
Yoav O
Goldberg O
, O
Jan O
Haji O
Àác O
, O
Christopher O
D O
Manning O
, O
Ryan O
Mcdonald O
, O
Slav O
Petrov O
, O
Sampo O
Pyysalo O
, O
Natalia O
Silveira O
, O
Reut O
Tsarfaty O
, O
and O
Daniel O
Zeman O
. O
2016 O
. O
Universal O
Dependencies O
v1 O
: O
A O
Multilingual O
Treebank O
Collection O
. O
In O
Proceedings O
of O
Language O
Resources O
and O
Evaluation O
Conference O
. O
Hiroshi O
Noji O
and O
Mark O
Johnson O
. O
2016 O
. O
Using O
Leftcorner O
Parsing O
to O
Encode O
Universal O
Structural O
Constraints O
in O
Grammar O
Induction O
. O
In O
Proceedings O
of O
the O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
33‚Äì43 O
. O
The O
Ohio O
Supercomputer O
Center O
. O
1987 O
. O
Ohio O
Supercomputer O
Center O
. O
\url{http://osc.edu O
/ark:/19495 O
/f5s1ph73}.405Steven O
Pinker O
and O
B O
MacWhinney O
. O
1987 O
. O
The O
bootstrapping O
problem O
in O
language O
acquisition O
. O
Mechanisms O
of O
language O
acquisition O
, O
pages O
399‚Äì441 O
. O
Alec O
Radford O
, O
Luke O
Metz O
, O
and O
Soumith O
Chintala O
. O
2016 O
. O
Unsupervised O
representation O
learning O
with O
deep O
convolutional O
generative O
adversarial O
networks O
. O
In O
Proceedings O
of O
the O
4th O
International O
Conference O
on O
Learning O
Representations O
. O
International O
Conference O
on O
Learning O
Representations O
, O
ICLR O
. O
Andrew O
Rosenberg O
and O
Julia O
Hirschberg O
. O
2007 O
. O
Vmeasure O
: O
A O
conditional O
entropy O
- O
based O
external O
cluster O
evaluation O
measure O
. O
In O
Proceedings O
of O
the O
2007 O
joint O
conference O
on O
empirical O
methods O
in O
natural O
language O
processing O
and O
computational O
natural O
language O
learning O
( O
EMNLP O
- O
CoNLL O
) O
. O
Yoav O
Seginer O
. O
2007 O
. O
Fast O
Unsupervised O
Incremental O
Parsing O
. O
In O
Proceedings O
of O
the O
Annual O
Meeting O
of O
the O
Association O
of O
Computational O
Linguistics O
, O
pages O
384‚Äì391 O
. O
Cory O
Shain O
, O
William O
Bryce O
, O
Lifeng O
Jin O
, O
Victoria O
Krakovna O
, O
Finale O
Doshi O
- O
Velez O
, O
Timothy O
Miller O
, O
William O
Schuler O
, O
and O
Lane O
Schwartz O
. O
2016 O
. O
Memory O
- O
bounded O
left O
- O
corner O
unsupervised O
grammar O
induction O
on O
child O
- O
directed O
input O
. O
In O
Proceedings O
of O
COLING O
2016 O
, O
the O
26th O
International O
Conference O
on O
Computational O
Linguistics O
: O
Technical O
Papers O
, O
pages O
964‚Äì975 O
, O
Osaka O
, O
Japan O
. O
The O
COLING O
2016 O
Organizing O
Committee O
. O
Yikang O
Shen O
, O
Zhouhan O
Lin O
, O
Chin O
- O
Wei O
Huang O
, O
and O
Aaron O
Courville O
. O
2018 O
. O
Neural O
Language O
Modeling O
by O
Jointly O
Learning O
Syntax O
and O
Lexicon O
. O
In O
ICLR O
. O
Yikang O
Shen O
, O
Shawn O
Tan O
, O
Alessandro O
Sordoni O
, O
and O
Aaron O
Courville O
. O
2019 O
. O
Ordered O
Neurons O
: O
Integrating O
Tree O
Structures O
into O
Recurrent O
Neural O
Networks O
. O
InICLR O
. O
Haoyue O
Shi O
, O
Jiayuan O
Mao O
, O
Kevin O
Gimpel O
, O
and O
Karen O
Livescu O
. O
2019 O
. O
Visually O
grounded O
neural O
syntax O
acquisition O
. O
In O
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
1842‚Äì1861 O
, O
Florence O
, O
Italy O
. O
Association O
for O
Computational O
Linguistics O
. O
Michael O
Tomasello O
. O
2003 O
. O
Constructing O
a O
language O
: O
A O
usage O
- O
based O
theory O
of O
language O
acquisition O
. O
Harvard O
University O
Press O
, O
Cambridge O
, O
MA O
, O
US O
. O
Sandra O
Waxman O
, O
Xiaolan O
Fu O
, O
Sudha O
Arunachalam O
, O
Erin O
Leddon O
, O
Kathleen O
Geraghty O
, O
Hyun O
- O
Joo O
Song O
, O
Child O
Dev O
, O
and O
Perspect O
Author O
. O
2013 O
. O
Are O
Nouns O
Learned O
Before O
Verbs O
? O
Infants O
Provide O
Insight O
into O
a O
Longstanding O
Debate O
NIH O
Public O
Access O
Author O
Manuscript O
. O
Child O
Dev O
Perspect O
, O
7(3).406A O
Details O
of O
datasets O
The O
MSCOCO O
caption O
dataset O
used O
in O
Shi O
et O
al O
. O
( O
2019 O
) O
contains O
413,915 O
sentences O
in O
the O
training O
set O
, O
and O
5000 O
sentences O
in O
the O
development O
and O
test O
sets O
respectively.12Every O
image O
is O
accompanied O
by O
5 O
captions O
, O
and O
there O
are O
82,783 O
images O
in O
total O
in O
the O
training O
set O
. O
The O
image O
embeddings O
of O
size O
2048 O
used O
in O
Shi O
et O
al O
. O
( O
2019 O
) O
are O
encoded O
by O
an O
image O
classiÔ¨Åer O
with O
ResNet128 O
architecture O
trained O
with O
on O
the O
ImageNet O
classiÔ¨Åcation O
task O
( O
Deng O
et O
al O
. O
, O
2009 O
) O
. O
The O
Multi30k O
caption O
dataset O
contains O
29,000 O
sentences O
in O
the O
training O
set O
, O
and O
1,014 O
sentences O
in O
the O
development O
and O
1,000 O
in O
the O
test O
set O
in O
four O
diÔ¨Äerent O
languages O
, O
all O
of O
which O
except O
Czech O
are O
used O
in O
this O
work O
thanks O
to O
the O
availability O
of O
high O
accuracy O
constituency O
parsers O
in O
these O
languages.13 O
There O
are O
as O
many O
images O
as O
there O
are O
captions O
in O
the O
training O
set O
. O
The O
image O
embeddings O
of O
size O
2048 O
provided O
with O
the O
dataset O
are O
encoded O
by O
an O
image O
classiÔ¨Åer O
with O
ResNet50 O
architecture O
also O
trained O
with O
on O
the O
ImageNet O
classiÔ¨Åcation O
task O
. O
For O
data O
preprocessing O
, O
following O
Shi O
et O
al O
. O
( O
2019 O
) O
, O
the O
size O
of O
the O
vocabulary O
is O
limited O
to O
10,000 O
for O
all O
languages O
and O
datasets O
. O
All O
raw O
images O
are O
resized O
to O
3 O
√ó64√ó64 O
and O
normalized O
with O
means O
[ O
0 O
.485,0.456,0.406 O
] O
and O
standard O
deviations O
[ O
0.229,0.224,0.225 O
] O
, O
calculated O
from O
images O
in O
ImageNet O
. O
B O
Hyperparameters O
The O
hyperparameters O
used O
in O
all O
proposed O
models O
are O
tuned O
with O
the O
MSCOCO O
English O
development O
set O
. O
For O
the O
grammar O
induction O
model O
, O
the O
size O
of O
word O
and O
syntactic O
category O
embeddings O
, O
as O
well O
as O
the O
size O
of O
hidden O
intermediary O
representations O
is O
64 O
. O
The O
size O
of O
the O
image O
embedding O
in O
the O
ImagePCFG O
system O
is O
also O
64 O
. O
All O
out O
- O
ofvocabulary O
words O
are O
replaced O
by O
the O
UNK O
symbol O
. O
Sentences O
with O
more O
than O
40 O
words O
in O
the O
training O
set O
are O
trimmed O
down O
to O
40 O
words O
. O
For O
the O
projector O
model O
, O
Ô¨Åve O
di O
Ô¨Äerent O
convolutional O
kernels O
, O
from O
( O
1,64 O
) O
to O
( O
5,64 O
) O
, O
are O
used O
with O
128 O
output O
channels O
. O
The O
trainable O
image O
encoder O
employs O
a O
12The O
data O
set O
can O
be O
found O
at O
https://github.com/ O
ExplorerFreda O
/ O
VGNSL O
along O
with O
image O
embeddings O
encoded O
by O
pretrained O
image O
encoders O
. O
13The O
data O
set O
can O
be O
found O
at O
https://github.com/ O
multi30k O
/ O
dataset O
along O
with O
image O
embeddings O
encoded O
by O
pretrained O
image O
encoders O
. O
ResNet18 O
architecture,14and O
the O
decoder O
employs O
the O
decoder O
architecture O
in O
the O
DCGAN O
model.15 O
A O
batch O
size O
of O
2 O
is O
used O
in O
training O
. O
Adam O
is O
used O
as O
the O
optimizer O
, O
with O
the O
initial O
learning O
rate O
at O
5√ó10‚àí4 O
. O
The O
loss O
on O
the O
validation O
set O
is O
checked O
every O
20000 O
batches O
, O
and O
training O
is O
stopped O
when O
the O
validation O
loss O
has O
not O
been O
lowered O
for O
10 O
checkpoints O
. O
The O
model O
with O
the O
lowest O
validation O
loss O
is O
used O
as O
the O
candidate O
model O
for O
test O
evaluation O
, O
where O
best O
parses O
are O
generated O
with O
the O
Viterbi O
algorithm O
on O
an O
inside O
chart O
. O
C O
Development O
Table O
4 O
and O
5 O
report O
unlabeled O
F1 O
and O
labeled O
RH O
results O
on O
the O
development O
sets O
in O
the O
multilingual O
caption O
datasets O
. O
Results O
show O
that O
development O
and O
test O
results O
are O
very O
similar O
, O
indicating O
that O
the O
general O
characteristics O
of O
the O
two O
sets O
are O
very O
close O
. O
14https://pytorch.org/docs/stable/_modules/ O
torchvision O
/ O
models O
/ O
resnet.html#resnet18 O
15https://github.com/pytorch/examples/blob/ O
master O
/ O
dcgan O
/ O
main.py407ModelsEnglish O
Korean O
Polish O
Chinese O
F1 O
RH O
F1 O
RH O
F1 O
RH O
F1 O
RH O
NoImagePCFG O
60.3 O
¬±8.246.4¬±11.038.6¬±8.722.6¬±6.959.5¬±3.847.5¬±3.9 O
ImagePrePCFG O
55.7 O
¬±7.539.6¬±5.439.5¬±4.224.1¬±3.461.2¬±1.650.1¬±3.3 O
ImagePCFG O
55.4 O
¬±2.743.2¬±1.845.1¬±2.327.5¬±2.654.3¬±8.341.6¬±7.9 O
Table O
4 O
: O
Averages O
and O
standard O
deviations O
of O
labeled O
Recall O
- O
Homogeneity O
and O
unlabeled O
F1 O
scores O
of O
various O
unsupervised O
grammar O
inducers O
on O
the O
MSCOCO O
caption O
development O
datasets O
. O
ModelsGerman O
English O
French O
F1 O
RH O
F1 O
RH O
F1 O
RH O
NoImagePCFG O
47.2 O
¬±5.753.6¬±5.759.1¬±8.152.2¬±8.543.8¬±4.943.2¬±5.2 O
ImagePrePCFG O
44.8 O
¬±7.950.0¬±8.346.7¬±7.340.7¬±7.542.3¬±10.342.8¬±10.5 O
ImagePCFG O
45.6 O
¬±5.250.6¬±8.547.7¬±5.440.9¬±5.243.1¬±5.143.9¬±5.5 O
ModelsKorean O
Polish O
Chinese O
F1 O
RH O
F1 O
RH O
F1 O
RH O
NoImagePCFG O
30.6 O
¬±5.722.2¬±3.049.4¬±4.940.0¬±5.359.7¬±3.353.6¬±4.7 O
ImagePrePCFG O
27.0 O
¬±4.819.2¬±3.648.5¬±3.138.5¬±3.155.5¬±9.348.3¬±10.4 O
ImagePCFG O
45.1 O
¬±1.133.4¬±1.949.5¬±7.640.8¬±6.358.3¬±3.252.1¬±4.3 O
Table O
5 O
: O
Averages O
and O
standard O
deviations O
of O
labeled O
Recall O
- O
Homogeneity O
and O
unlabeled O
F1 O
scores O
of O
various O
unsupervised O
grammar O
inducers O
on O
the O
Multi30k O
caption O
development O
datasets.408Proceedings O
of O
the O
1st O
Conference O
of O
the O
Asia O
- O
PaciÔ¨Åc O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
10th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
409‚Äì424 O
December O
4 O
- O
7 O
, O
2020 O
. O
¬© O
2020 O
Association O
for O
Computational O
Linguistics O
Heads O
- O
up O
! O
Unsupervised O
Constituency O
Parsing O
via O
Self O
- O
Attention O
Heads O
Bowen O
Li‚Ä†Taeuk O
Kim‚Ä°Reinald O
Kim O
Amplayo‚Ä†Frank O
Keller‚Ä† O
‚Ä†ILCC O
, O
School O
of O
Informatics O
, O
University O
of O
Edinburgh O
, O
UK O
‚Ä°Dept O
. O
of O
Computer O
Science O
and O
Engineering O
, O
Seoul O
National O
University O
, O
Korea O
{ O
bowen.li O
, O
reinald.kim O
} O
@ed.ac.uk O
taeuk@europa.snu.ac.kr O
keller@inf.ed.ac.uk O
Abstract O
Transformer O
- O
based O
pre O
- O
trained O
language O
models O
( O
PLMs O
) O
have O
dramatically O
improved O
the O
state O
of O
the O
art O
in O
NLP O
across O
many O
tasks O
. O
This O
has O
led O
to O
substantial O
interest O
in O
analyzing O
the O
syntactic O
knowledge O
PLMs O
learn O
. O
Previous O
approaches O
to O
this O
question O
have O
been O
limited O
, O
mostly O
using O
test O
suites O
or O
probes O
. O
Here O
, O
we O
propose O
a O
novel O
fully O
unsupervised O
parsing O
approach O
that O
extracts O
constituency O
trees O
from O
PLM O
attention O
heads O
. O
We O
rank O
transformer O
attention O
heads O
based O
on O
their O
inherent O
properties O
, O
and O
create O
an O
ensemble O
of O
high O
- O
ranking O
heads O
to O
produce O
the O
Ô¨Ånal O
tree O
. O
Our O
method O
is O
adaptable O
to O
low O
- O
resource O
languages O
, O
as O
it O
does O
not O
rely O
on O
development O
sets O
, O
which O
can O
be O
expensive O
to O
annotate O
. O
Our O
experiments O
show O
that O
the O
proposed O
method O
often O
outperform O
existing O
approaches O
if O
there O
is O
no O
development O
set O
present O
. O
Our O
unsupervised O
parser O
can O
also O
be O
used O
as O
a O
tool O
to O
analyze O
the O
grammars O
PLMs O
learn O
implicitly O
. O
For O
this O
, O
we O
use O
the O
parse O
trees O
induced O
by O
our O
method O
to O
train O
a O
neural O
PCFG O
and O
compare O
it O
to O
a O
grammar O
derived O
from O
a O
human O
- O
annotated O
treebank O
. O
1 O
Introduction O
Pre O
- O
trained O
language O
models O
( O
PLMs O
) O
, O
particularly O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
and O
others O
( O
Yang O
et O
al O
. O
, O
2019 O
; O
Liu O
et O
al O
. O
, O
2019b O
; O
Radford O
et O
al O
. O
, O
2019 O
) O
based O
on O
the O
transformer O
architecture O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
have O
dramatically O
improved O
the O
state O
of O
the O
art O
in O
NLP O
. O
Such O
models O
make O
it O
possible O
to O
train O
a O
large O
, O
generic O
language O
model O
on O
vast O
unannotated O
datasets O
, O
and O
then O
Ô¨Åne O
- O
tune O
it O
for O
a O
speciÔ¨Åc O
task O
using O
a O
small O
amount O
of O
annotated O
data O
. O
The O
success O
of O
PLMs O
has O
led O
to O
a O
large O
literature O
investigating O
the O
linguistic O
knowledge O
that O
PLMs O
learn O
implicitly O
during O
pre O
- O
training O
( O
Liu O
et O
al O
. O
, O
2019a O
; O
Clark O
et O
al O
. O
, O
2019 O
; O
Kovaleva O
et O
al O
. O
, O
2019 O
; O
Pimentel O
et O
al O
. O
, O
2020 O
) O
, O
sometimes O
referred O
to O
as O
BERTology O
( O
Rogers O
et O
al O
. O
, O
2020).BERTology O
has O
been O
particularly O
concerned O
with O
the O
question O
whether O
BERT O
- O
type O
models O
learn O
syntactic O
structure O
. O
Typical O
approaches O
include O
test O
suites O
of O
sentences O
that O
instantiate O
speciÔ¨Åc O
syntactic O
structures O
( O
Goldberg O
, O
2019 O
) O
, O
general O
probes O
( O
also O
known O
as O
diagnostic O
classiÔ¨Åers O
, O
Belinkov O
and O
Glass O
2019 O
) O
or O
structural O
probes O
( O
Hewitt O
and O
Manning O
, O
2019 O
) O
. O
All O
of O
these O
approaches O
are O
limited O
: O
the O
Ô¨Årst O
one O
requires O
the O
laborious O
compilation O
of O
languageand O
construction O
- O
speciÔ¨Åc O
suites O
of O
sentences O
; O
the O
second O
one O
sometimes O
fails O
to O
adequately O
reÔ¨Çect O
differences O
in O
representations O
( O
Zhang O
and O
Bowman O
, O
2018 O
; O
Hewitt O
and O
Liang O
, O
2019 O
; O
V O
oita O
and O
Titov O
, O
2020 O
) O
; O
the O
third O
one O
involves O
designing O
a O
novel O
extraction O
model O
that O
is O
not O
applicable O
to O
tasks O
other O
than O
probing O
( O
Maudslay O
et O
al O
. O
, O
2020 O
) O
. O
It O
is O
therefore O
natural O
to O
use O
a O
parsing O
task O
to O
test O
whether O
the O
representations O
learned O
by O
PLMs O
contain O
usable O
syntactic O
information O
. O
This O
enables O
us O
to O
test O
syntactic O
structure O
in O
general O
, O
rather O
than O
speciÔ¨Åc O
constructions O
, O
and O
does O
n‚Äôt O
require O
a O
specialized O
probe O
. O
In O
this O
paper O
, O
we O
will O
therefore O
use O
PLM O
attention O
heads O
to O
construct O
an O
unsupervised O
constituency O
parser O
. O
Previously O
, O
related O
approaches O
have O
been O
proposed O
under O
the O
heading O
of O
zero O
- O
shot O
constituency O
parsing O
( O
Kim O
et O
al O
. O
, O
2020a O
, O
b).1However O
, O
this O
prior O
work O
crucially O
relies O
on O
an O
annotated O
development O
set O
in O
order O
to O
identify O
transformer O
heads O
that O
are O
sensitive O
to O
syntactic O
structure O
. O
Existing O
approaches O
therefore O
are O
not O
truly O
unsupervised O
. O
For O
most O
low O
resource O
languages O
, O
no O
such O
annotated O
data O
is O
available O
, O
and O
often O
not O
even O
an O
annotation O
scheme O
exists O
. O
Thus O
, O
assuming O
a O
development O
set O
is O
not O
a O
realistic O
experimental O
setup O
( O
Kann O
et O
al O
. O
, O
2019 O
) O
. O
If O
a O
suitable O
development O
set O
is O
available O
, O
Shi O
et O
al O
. O
( O
2020 O
) O
shows O
that O
an O
existing O
supervised O
parser O
trained O
on O
a O
few O
- O
shot O
setting O
can O
outperform O
all O
the O
unsupervised O
parsing O
1Like O
Kim O
et O
al O
. O
( O
2020b O
) O
, O
we O
use O
zero O
- O
shot O
to O
refer O
to O
the O
transfer O
from O
language O
modeling O
to O
constituency O
parsing.409methods O
by O
a O
signiÔ¨Åcant O
margin O
. O
It O
strongly O
challenges O
tuning O
on O
an O
annotated O
development O
set O
for O
unsupervised O
parsing O
. O
In O
this O
paper O
, O
we O
propose O
a O
novel O
approach O
to O
build O
a O
PLM O
- O
based O
unsupervised O
parser O
that O
does O
not O
require O
a O
development O
set O
: O
we O
rank O
transformer O
heads O
based O
on O
their O
inherent O
properties O
, O
such O
as O
how O
likely O
tokens O
are O
to O
be O
grouped O
in O
a O
hierarchical O
structure O
. O
We O
then O
ensemble O
the O
top- O
Kheads O
to O
produce O
constituency O
trees O
. O
We O
evaluate O
our O
approach O
and O
previous O
zero O
- O
shot O
approaches O
on O
the O
English O
Penn O
Treebank O
( O
PTB O
) O
and O
eight O
other O
languages O
on O
the O
SPMRL O
dataset O
. O
On O
the O
one O
hand O
, O
if O
the O
development O
set O
is O
absent O
, O
our O
approach O
largely O
outperforms O
previous O
zero O
- O
shot O
approaches O
on O
the O
English O
PTB O
. O
On O
the O
other O
hand O
, O
if O
previous O
zero O
- O
shot O
approaches O
are O
equipped O
with O
the O
development O
set O
, O
our O
approach O
can O
still O
match O
the O
parsing O
performance O
of O
these O
approaches O
using O
the O
single O
best O
head O
or O
layerwise O
ensembling O
. O
For O
the O
multilingual O
experiment O
, O
we O
take O
advantage O
of O
the O
top- O
Kheads O
selected O
in O
English O
and O
directly O
parse O
other O
languages O
using O
our O
approach O
. O
Surprisingly O
, O
on O
Ô¨Åve O
out O
of O
nine O
languages O
, O
this O
crosslingual O
unsupervised O
parser O
matches O
previous O
approaches O
that O
rely O
on O
a O
development O
set O
in O
each O
target O
language O
with O
the O
single O
best O
head O
or O
layer O
- O
wise O
ensembling O
. O
However O
, O
our O
fully O
unsupervised O
method O
lags O
behind O
the O
previous O
state O
- O
of O
- O
the O
- O
art O
zero O
- O
shot O
parser O
if O
a O
top- O
Kensemble O
is O
used O
. O
Furthermore O
, O
our O
approach O
can O
be O
use O
as O
a O
tool O
to O
analyze O
the O
capability O
of O
PLMs O
in O
learning O
syntactic O
knowledge O
. O
As O
no O
human O
annotation O
is O
required O
, O
our O
approach O
has O
the O
potential O
to O
reveal O
the O
grammar O
PLMs O
learn O
implicitly O
. O
Here O
, O
we O
use O
the O
tree O
structures O
generated O
by O
our O
parser O
to O
train O
a O
neural O
PCFG O
. O
We O
evaluate O
the O
learned O
grammar O
against O
the O
English O
PTB O
on O
internal O
tags O
and O
production O
rules O
both O
qualitatively O
and O
quantitatively O
. O
2 O
Related O
Work O
Recently O
, O
neural O
models O
have O
renewed O
interest O
in O
grammar O
induction O
. O
Earlier O
work O
( O
Choi O
et O
al O
. O
, O
2018 O
; O
Williams O
et O
al O
. O
, O
2018 O
) O
attempted O
to O
induce O
grammar O
by O
optimizing O
a O
sentence O
classiÔ¨Åcation O
objective O
, O
while O
follow O
- O
up O
work O
( O
Htut O
et O
al O
. O
, O
2018 O
; O
Shen O
et O
al O
. O
, O
2018a O
, O
2019 O
) O
showed O
that O
a O
language O
modeling O
objective O
performs O
better O
. O
Latest O
work O
employed O
autoencoders O
or O
probabilistic O
grammars O
( O
Drozdov O
et O
al O
. O
, O
2019 O
; O
Kim O
et O
al O
. O
, O
2019a O
, O
b O
; O
Zhu O
et O
al O
. O
, O
2020).A O
new O
line O
of O
work O
is O
zero O
- O
shot O
constituency O
parsing O
, O
whose O
goal O
is O
to O
automatically O
extract O
trees O
from O
PLMs O
in O
a O
parameter O
- O
free O
fashion O
. O
The O
top O
- O
down O
zero O
- O
shot O
parser O
( O
Kim O
et O
al O
. O
, O
2020a O
) O
utilizes O
the O
concept O
of O
syntactic O
distance O
( O
Shen O
et O
al O
. O
, O
2018b O
) O
, O
where O
trees O
are O
induced O
by O
an O
algorithm O
that O
recursively O
splits O
a O
sequence O
of O
words O
in O
a O
topdown O
manner O
. O
However O
, O
this O
approach O
suffers O
from O
its O
greedy O
search O
mode O
, O
failing O
to O
take O
into O
account O
all O
possible O
subtrees O
. O
The O
chart O
- O
based O
zero O
- O
shot O
parser O
( O
Kim O
et O
al O
. O
, O
2020b O
) O
applies O
chart O
parsing O
to O
address O
this O
problem O
. O
Wu O
et O
al O
. O
( O
2020 O
) O
introduced O
a O
parameter O
- O
free O
probing O
technique O
to O
analyze O
PLMs O
via O
perturbed O
masking O
. O
There O
is O
also O
prior O
work O
on O
extracting O
constituency O
trees O
from O
self O
- O
attention O
mechanisms O
of O
transformers O
. O
Mare O
Àácek O
and O
Rosa O
( O
2018 O
) O
proposed O
heuristic O
approaches O
to O
convert O
attention O
weights O
to O
trees O
. O
Mare O
Àácek O
and O
Rosa O
( O
2019 O
) O
introduced O
a O
chartbased O
tree O
extraction O
method O
in O
transformer O
- O
based O
neural O
machine O
translation O
encoders O
and O
provide O
a O
quantitative O
study O
. O
3 O
Zero O
- O
shot O
Constituency O
Parsing O
via O
PLMs O
In O
this O
section O
, O
we O
brieÔ¨Çy O
review O
the O
chart O
- O
based O
zero O
- O
shot O
parser O
and O
then O
introduce O
our O
rankingbased O
zero O
- O
shot O
parser O
. O
3.1 O
Chart O
- O
based O
Zero O
- O
shot O
Parsing O
In O
chart O
- O
based O
zero O
- O
shot O
parsing O
, O
a O
real O
- O
valued O
scorestree(t)is O
assigned O
for O
each O
tree O
candidate O
t O
, O
which O
decomposes O
as O
: O
stree(t O
) O
= O
/summationdisplay O
( O
i O
, O
j)‚ààtsspan(i O
, O
j O
) O
, O
wheresspan(i O
, O
j)is O
the O
score O
( O
or O
cost O
) O
for O
a O
constituent O
that O
is O
located O
between O
positions O
iandj O
( O
1‚â§i‚â§j‚â§n O
, O
wherenis O
the O
length O
of O
the O
sentence O
) O
. O
SpeciÔ¨Åcally O
, O
for O
a O
span O
of O
length O
1 O
, O
sspan(i O
, O
j)is O
deÔ¨Åned O
as O
0 O
when O
i O
= O
j. O
For O
a O
span O
longer O
than O
1 O
, O
the O
following O
recursion O
applies O
: O
sspan(i O
, O
j O
) O
= O
scomp(i O
, O
j O
) O
+ O
mini‚â§k O
< O
jssplit(i O
, O
k O
, O
j O
) O
( O
1 O
) O
ssplit(i O
, O
k O
, O
j O
) O
= O
sspan(i O
, O
k O
) O
+ O
sspan(k+ O
1,j),(2 O
) O
wherescomp(¬∑,¬∑)measures O
the O
validity O
or O
compositionality O
of O
the O
span O
( O
i O
, O
j)itself O
, O
whilessplit(i O
, O
k O
, O
j O
) O
indicates O
how O
plausible O
it O
is O
to O
split O
the O
span O
( O
i O
, O
j O
) O
at O
positionk O
. O
Two O
alternatives O
have O
been O
developed O
in O
Kim O
et O
al O
. O
( O
2020b O
) O
for O
scomp O
( O
¬∑ O
, O
¬∑ O
): O
the O
pair410score O
function O
sp(¬∑,¬∑)and O
the O
characteristic O
score O
functionsc O
( O
¬∑ O
, O
¬∑ O
) O
. O
The O
pair O
score O
function O
sp(¬∑,¬∑)computes O
the O
average O
pair O
- O
wise O
distance O
in O
a O
given O
span O
: O
sp(i O
, O
j O
) O
= O
1 O
/ O
parenleftbigj‚àíi+1 O
2 O
/ O
parenrightbig O
/ O
summationdisplay O
( O
wx O
, O
wy)‚ààpair O
( O
i O
, O
j)f(g(wx),g(wy)),(3 O
) O
where O
pair O
( O
i O
, O
j)returns O
a O
set O
consisting O
of O
all O
combinations O
of O
two O
words O
( O
e.g. O
, O
wx O
, O
wy O
) O
inside O
the O
span O
( O
i O
, O
j O
) O
. O
Functionsf(¬∑,¬∑)andg(¬∑)are O
the O
distance O
measure O
function O
and O
the O
representation O
extractor O
function O
, O
respectively O
. O
For O
g O
, O
givenlas O
the O
number O
of O
layers O
in O
a O
PLM O
, O
gis O
actually O
a O
set O
of O
functions O
g={gd O
( O
u O
, O
v)|u= O
1, O
... O
,l O
, O
v O
= O
1, O
... O
,a O
} O
, O
each O
of O
which O
outputs O
the O
attention O
distribution O
of O
the O
vth O
attention O
head O
on O
the O
uthlayer O
of O
the O
PLM.2In O
case O
of O
the O
function O
f O
, O
there O
are O
also O
two O
options O
, O
Jensen O
- O
Shannon O
( O
JSD O
) O
and O
Hellinger O
( O
HEL O
) O
distance O
. O
Thus O
, O
f={JSD O
, O
HEL O
} O
. O
The O
characteristic O
score O
function O
sc(¬∑,¬∑)measures O
the O
distance O
between O
each O
word O
in O
the O
constituent O
and O
a O
predeÔ¨Åned O
characteristic O
value O
c O
( O
e.g. O
, O
the O
center O
of O
the O
constituent O
): O
sc(i O
, O
j O
) O
= O
1 O
j‚àíi+ O
1 O
/ O
summationdisplay O
i‚â§x‚â§jf(g(wx),c),(4 O
) O
where O
c=1 O
j‚àíi+1 O
/ O
summationtext O
i‚â§y‚â§jg(wy O
) O
. O
Sincescomp(¬∑,¬∑)is O
well O
deÔ¨Åned O
, O
it O
is O
straightforward O
to O
compute O
every O
possible O
case O
of O
sspan(i O
, O
j O
) O
using O
the O
CKY O
algorithm O
( O
Cocke O
, O
1969 O
; O
Kasami O
, O
1966 O
; O
Younger O
, O
1967 O
) O
. O
Finally O
, O
the O
parser O
outputs O
ÀÜt O
, O
the O
tree O
that O
requires O
the O
lowest O
score O
( O
cost O
) O
to O
build O
, O
as O
a O
prediction O
for O
the O
parse O
tree O
of O
the O
input O
sentence O
: O
ÀÜt= O
arg O
mintstree(t O
) O
. O
For O
attention O
heads O
ensembling O
, O
both O
a O
layerwise O
ensemble O
and O
a O
top- O
Kensemble O
are O
considered O
. O
The O
Ô¨Årst O
one O
averages O
all O
attention O
heads O
from O
a O
speciÔ¨Åc O
layer O
, O
while O
the O
second O
one O
averages O
the O
top O
- O
Kheads O
from O
across O
different O
layers O
. O
At O
test O
time O
, O
separate O
trees O
produced O
by O
different O
heads O
are O
merged O
to O
one O
Ô¨Ånal O
tree O
via O
syntactic O
distance O
.3 O
The O
chart O
- O
based O
zero O
- O
shot O
parser O
achieves O
the O
state O
of O
the O
art O
in O
zero O
- O
shot O
constituency O
parsing O
. O
2The O
hidden O
representations O
of O
the O
given O
words O
can O
also O
serve O
as O
an O
alternative O
for O
g. O
But O
Kim O
et O
al O
. O
( O
2020a O
) O
show O
that O
the O
attention O
distributions O
provide O
more O
syntactic O
clues O
under O
the O
zero O
- O
shot O
setting O
. O
3Details O
can O
be O
found O
in O
Kim O
et O
al O
. O
( O
2020b O
) O
. O
For O
the O
ensemble O
parsing O
, O
marrying O
chart O
- O
based O
parser O
and O
top O
- O
down O
parser O
yields O
better O
results O
than O
averaging O
the O
attention O
distributions.3.2 O
Ranking O
- O
based O
Zero O
- O
shot O
Parsing O
The O
chart O
- O
based O
zero O
- O
shot O
parser O
relies O
on O
the O
existing O
development O
set O
of O
a O
treebank O
( O
e.g. O
, O
the O
English O
PTB O
) O
to O
select O
the O
best O
conÔ¨Åguration O
, O
i.e. O
, O
the O
combination O
of{g|gd O
( O
u O
, O
v),u= O
1, O
... O
,l O
, O
v O
= O
1, O
... O
,a O
} O
, O
{ O
f|JSD O
, O
HEL},{scomp|sp O
, O
sc O
} O
, O
and O
heads O
ensemble O
that O
achieves O
the O
best O
parsing O
accuracy O
. O
Such O
a O
development O
set O
always O
contains O
hundreds O
of O
sentences O
, O
hence O
considerable O
annotation O
effort O
is O
still O
required O
. O
From O
the O
perspective O
of O
unsupervised O
parsing O
, O
such O
results O
arguably O
are O
not O
fully O
unsupervised.4Another O
argument O
against O
using O
a O
development O
set O
is O
that O
the O
linguistic O
assumptions O
inherent O
in O
the O
expert O
annotation O
required O
to O
create O
the O
development O
set O
potentially O
restrict O
our O
exploration O
of O
how O
PLMs O
model O
the O
constituency O
structures O
. O
It O
could O
be O
that O
the O
PLM O
learns O
valid O
constituency O
structures O
, O
which O
however O
do O
not O
match O
the O
annotation O
guidelines O
that O
were O
used O
to O
create O
the O
development O
set O
. O
Here O
, O
we O
take O
a O
radical O
departure O
from O
the O
previous O
work O
in O
order O
to O
extract O
constituency O
trees O
from O
PLMs O
in O
a O
fully O
unsupervised O
manner O
. O
We O
propose O
a O
two O
- O
step O
procedure O
for O
unsupervised O
parsing O
: O
( O
1 O
) O
identify O
syntax O
- O
related O
attention O
heads O
directly O
from O
PLMs O
without O
relying O
on O
a O
development O
set O
of O
a O
treebank O
; O
( O
2 O
) O
ensemble O
the O
selected O
top- O
Kheads O
to O
produce O
the O
constituency O
trees O
. O
For O
identiÔ¨Åcation O
of O
the O
syntax O
- O
related O
attention O
heads O
, O
we O
rank O
all O
heads O
by O
scoring O
them O
with O
a O
chart O
- O
based O
ranker O
. O
We O
borrow O
the O
idea O
of O
the O
chartbased O
zero O
- O
shot O
parser O
to O
build O
our O
ranker O
. O
Given O
an O
input O
sentence O
and O
a O
speciÔ¨Åc O
choice O
of O
fandscomp O
, O
each O
attention O
head O
gd O
( O
u O
, O
v)in O
the O
PLM O
yields O
one O
unique O
attention O
distribution O
. O
Using O
the O
chart O
- O
based O
zero O
- O
shot O
parser O
in O
Section O
3.1 O
, O
we O
can O
obtain O
the O
score O
of O
the O
best O
constituency O
tree O
as:5 O
sparsing O
( O
u O
, O
v O
) O
= O
stree(ÀÜt O
) O
= O
/summationtext O
( O
i O
, O
j)‚ààÀÜtsspan(i O
, O
j),(5 O
) O
where O
ÀÜt= O
arg O
mintstree(t O
) O
. O
It O
is O
obvious O
4Some O
previous O
work O
( O
Shen O
et O
al O
. O
, O
2018a O
, O
2019 O
; O
Drozdov O
et O
al O
. O
, O
2019 O
; O
Kim O
et O
al O
. O
, O
2019a O
) O
also O
use O
a O
development O
set O
to O
tune O
hyperparameters O
or O
early O
- O
stop O
training O
. O
5Our O
ranking O
method O
works O
approximately O
as O
a O
maximum O
a O
posteriori O
probability O
( O
MAP O
) O
estimate O
, O
since O
we O
only O
consider O
the O
best O
tree O
the O
attention O
head O
generates O
. O
In O
unsupervised O
parsing O
, O
marginalization O
is O
a O
standard O
method O
for O
model O
development O
. O
We O
have O
tried O
to O
apply O
marginalization O
to O
our O
ranking O
algorithm O
where O
all O
possible O
trees O
are O
considered O
and O
the O
sum O
score O
is O
calculated O
( O
using O
the O
logsumexp O
trick O
) O
for O
ranking O
. O
But O
marginalization O
does O
not O
work O
well O
for O
attention O
distributions O
, O
where O
an O
‚Äú O
attending O
broadly O
‚Äù O
head O
with O
higher O
entropy O
is O
more O
favorable O
than O
a O
syntax O
- O
related O
head O
with O
lower O
entropy O
. O
So O
we O
only O
consider O
the O
score O
of O
the O
best O
tree.411that O
all O
combinations O
of O
{ O
f|JSD O
, O
HEL}and O
{ O
scomp|sp O
, O
sc}will O
produce O
multiple O
scores O
for O
a O
given O
head O
. O
Here O
we O
average O
the O
scores O
of O
all O
such O
combinations O
to O
get O
one O
single O
score O
. O
Then O
we O
rank O
all O
attention O
heads O
and O
select O
the O
syntax O
- O
related O
heads O
for O
parsing O
. O
However O
, O
directly O
applying O
the O
chart O
- O
based O
zero O
- O
shot O
parser O
in O
Section O
3.1 O
for O
ranking O
delivers O
a O
trivial O
, O
ill O
- O
posed O
solution O
. O
The O
recursion O
in O
Eq O
. O
( O
2 O
) O
only O
encourages O
the O
intra O
- O
similarity O
inside O
the O
span O
. O
Intuitively O
, O
one O
attention O
head O
that O
produces O
the O
same O
attention O
distribution O
for O
each O
token O
( O
e.g. O
, O
a O
uniform O
attention O
distribution O
or O
one O
that O
forces O
every O
token O
to O
attend O
to O
one O
speciÔ¨Åc O
token O
) O
will O
get O
the O
lowest O
score O
( O
cost O
) O
and O
the O
highest O
ranking.6 O
To O
address O
this O
issue O
, O
we O
Ô¨Årst O
introduce O
intersimilarity O
into O
the O
recursion O
in O
Eq O
. O
( O
2 O
) O
and O
get O
the O
following O
: O
ssplit(i O
, O
k O
, O
j O
) O
= O
sspan(i O
, O
k O
) O
+ O
sspan(k+ O
1,j)‚àíscross(i O
, O
k O
, O
j O
) O
, O
( O
6 O
) O
where O
the O
cross O
score O
scross(i O
, O
k O
, O
j O
) O
is O
the O
similarity O
between O
two O
subspans O
( O
i O
, O
k)and(k+ O
1,j O
) O
. O
However O
, O
this O
formulation O
forces O
the O
algorithm O
to O
go O
to O
the O
other O
extreme O
: O
one O
attention O
head O
that O
produces O
a O
totally O
different O
distribution O
for O
each O
token O
( O
e.g. O
, O
force O
each O
token O
to O
attend O
to O
itself O
or O
the O
previous O
/ O
next O
token O
) O
will O
get O
the O
highest O
ranking O
. O
To O
balance O
the O
inter- O
and O
intra O
- O
similarity O
and O
avoid O
having O
to O
introduce O
a O
tunable O
coefÔ¨Åcient O
, O
we O
simply O
add O
a O
length O
- O
based O
weighting O
term O
to O
Eq O
. O
( O
1 O
) O
and O
get O
: O
sspan(i O
, O
j O
) O
= O
j‚àíi+ O
1 O
n(scomp(i O
, O
j O
) O
+ O
min O
i‚â§k O
< O
jssplit(i O
, O
k O
, O
j O
) O
) O
, O
( O
7 O
) O
wherej‚àíi+ O
1 O
is O
the O
length O
of O
the O
span O
( O
i O
, O
j O
) O
. O
The O
length O
ratio O
functions O
as O
a O
regulator O
to O
assign O
larger O
weights O
to O
longer O
spans O
. O
This O
is O
motivated O
by O
the O
fact O
that O
longer O
constituents O
should O
contribute O
more O
to O
the O
scoring O
of O
the O
parse O
tree O
, O
since O
the O
inter O
- O
similarity O
always O
has O
strong O
effects O
on O
shorter O
spans O
. O
In O
this O
way O
, O
the O
inter- O
and O
intra O
- O
similarity O
can O
be O
balanced O
. O
With O
respect O
to O
the O
choice O
for O
scross(i O
, O
k O
, O
j O
) O
, O
we O
follow O
the O
idea O
of O
spandscin O
Eq O
. O
( O
3 O
) O
and O
( O
4 O
) O
6Such O
cases O
do O
exist O
in O
PLMs O
. O
Clark O
et O
al O
. O
( O
2019 O
) O
shows O
that O
BERT O
exhibits O
clear O
surface O
- O
level O
attention O
patterns O
. O
Some O
of O
these O
patterns O
will O
deliver O
ill O
- O
posed O
solutions O
in O
ranking O
: O
attend O
broadly O
, O
attend O
to O
a O
special O
tokens O
( O
e.g. O
, O
[ O
SEP O
] O
) O
, O
attend O
to O
punctuation O
( O
e.g. O
, O
period O
) O
. O
One O
can O
also O
observe O
these O
patterns O
using O
the O
visualization O
tool O
provided O
by O
Vig O
( O
2019).and O
propose O
the O
pair O
score O
function O
spxand O
the O
characteristic O
score O
function O
scx7for O
cross O
score O
computation O
. O
spxis O
deÔ¨Åned O
as O
: O
spx(i O
, O
j O
) O
= O
1 O
( O
k‚àíi+ O
1)(j‚àík)/summationdisplay O
( O
wx O
, O
wy)‚ààprod O
( O
i O
, O
k O
, O
j)f(g(wx),g(wy O
) O
) O
, O
where O
prod O
( O
i O
, O
k O
, O
j O
) O
returns O
a O
set O
of O
the O
product O
of O
words O
from O
the O
two O
subspans O
( O
i O
, O
k)and(k+ O
1,j O
) O
. O
Andscxis O
deÔ¨Åned O
as O
: O
scx(i O
, O
j O
) O
= O
f(ci O
, O
k O
, O
ck+1,j O
) O
, O
where O
ci O
, O
k=1 O
k‚àíi+1 O
/ O
summationtext O
i‚â§x‚â§kg(wx),ck+1,j= O
1 O
j‚àík O
/ O
summationtext O
k+1‚â§y‚â§jg(wy O
) O
. O
We O
average O
all O
the O
combinations O
of O
{ O
f|JSD O
, O
HEL},{scomp|sp O
, O
sc}and O
{ O
scross|spx O
, O
scx}to O
rank O
all O
the O
attention O
heads O
and O
select O
the O
top- O
Kheads O
. O
After O
the O
ranking O
step O
, O
we O
perform O
constituency O
parsing O
by O
ensembling O
the O
selected O
heads O
. O
We O
simply O
employ O
the O
ensemble O
method O
in O
Section O
3.1 O
and O
average O
all O
the O
combinations O
of O
{ O
f|JSD O
, O
HEL O
} O
and{scomp|sp O
, O
sc}to O
get O
a O
single O
predicted O
parse O
tree O
for O
a O
given O
sentence O
. O
3.3 O
How O
to O
select O
K O
For O
ensemble O
parsing O
, O
Kim O
et O
al O
. O
( O
2020b O
) O
proposed O
three O
settings O
: O
the O
best O
head O
, O
layerwise O
ensemble O
, O
and O
top O
- O
Kensemble O
. O
To O
prevent O
introducing O
a O
tunable O
hyperparameter O
, O
we O
propose O
to O
select O
a O
value O
forKdynamically O
based O
on O
a O
property O
of O
the O
ranking O
score O
in O
Eq O
. O
( O
5 O
) O
. O
Since O
we O
use O
a O
similarity O
- O
based O
distance O
, O
the O
lower O
the O
ranking O
score O
, O
the O
higher O
the O
ranking O
. O
Assuming O
that O
scores O
are O
computed O
for O
all O
attention O
heads O
, O
we O
can O
sort O
the O
scores O
in O
ascending O
order O
. O
Intuitively O
, O
given O
the O
order O
, O
we O
would O
like O
to O
choose O
thekfor O
which O
ranking O
score O
increases O
the O
most O
, O
which O
means O
syntactic O
relatedness O
drops O
the O
most O
. O
Supposesparsing O
( O
k)is O
the O
ranking O
score O
where O
kis O
the O
head O
index O
in O
the O
ascending O
order O
, O
then O
this O
is O
equivalent O
to O
Ô¨Ånding O
the O
kwith O
the O
greatest O
gradient O
on O
the O
curve O
of O
the O
score O
. O
We O
Ô¨Årst O
estimate O
the O
gradient O
of O
sparsing O
( O
k)and O
then O
Ô¨Ånd O
the O
kwith O
the O
greatest O
gradient O
. O
Finally O
, O
Kis O
computed O
as O
: O
K= O
arg O
max O
k O
/ O
summationdisplay O
k‚àíŒ¥‚â§j‚â§k+Œ¥ O
j O
/ O
negationslash O
= O
ksparsing O
( O
k+j)‚àísparsing O
( O
k O
) O
j O
, O
7Subscripts O
in O
the O
naming O
of O
functions O
in O
this O
paper O
: O
p O
‚Äì O
pair O
score O
, O
c O
‚Äì O
characteristic O
score O
, O
x O
‚Äì O
cross O
score.412where O
we O
smooth O
the O
gradient O
by O
considering O
Œ¥ O
steps O
. O
Here O
, O
we O
set O
Œ¥= O
3 O
. O
In O
practice O
, O
we O
Ô¨Ånd O
that O
the O
greatest O
gradient O
always O
happens O
in O
the O
head O
or O
the O
tail O
of O
the O
curve O
. O
For O
the O
robustness O
, O
we O
select O
the O
Kfrom O
the O
middle O
range O
of O
the O
score O
function O
curve O
, O
i.e. O
, O
starting O
from O
30 O
and O
ending O
with O
75 O
% O
of O
all O
heads.8We O
also O
provide O
a O
lazyoption O
forKselection O
, O
which O
simply O
assume O
a O
Ô¨Åxed O
value O
of O
30 O
for O
the O
top- O
Kensemble O
. O
4 O
Grammar O
Learning O
We O
are O
also O
interested O
in O
exploring O
to O
what O
extent O
the O
syntactic O
knowledge O
acquired O
by O
PLMs O
resembles O
human O
- O
annotated O
constituency O
grammars O
. O
For O
this O
exploration O
, O
we O
infer O
a O
constituency O
grammar O
, O
in O
the O
form O
of O
probabilistic O
production O
rules O
, O
from O
the O
trees O
induced O
from O
PLMs O
. O
This O
grammar O
can O
then O
be O
analyzed O
further O
, O
and O
compared O
to O
humanderived O
grammars O
. O
Thanks O
to O
the O
recent O
progress O
in O
neural O
parameterization O
, O
neural O
PCFGs O
have O
been O
successfully O
applied O
to O
unsupervised O
constituency O
parsing O
( O
Kim O
et O
al O
. O
, O
2019a O
) O
. O
We O
harness O
this O
model9 O
to O
learn O
probabilistic O
constituency O
grammars O
from O
PLMs O
by O
maximizing O
the O
joint O
likelihood O
of O
sentences O
and O
parse O
trees O
induced O
from O
PLMs O
. O
In O
the O
following O
, O
we O
Ô¨Årst O
brieÔ¨Çy O
review O
the O
neural O
PCFG O
and O
then O
introduce O
our O
training O
algorithm O
. O
4.1 O
Neural O
PCFGs O
A O
probabilistic O
context O
- O
free O
grammar O
( O
PCFG O
) O
consists O
of O
a O
5 O
- O
tuple O
grammar O
G= O
( O
S O
, O
N O
, O
P O
, O
Œ£ O
, O
R O
) O
and O
rule O
probabilities O
œÄ={œÄr}r‚ààR O
, O
whereSis O
the O
start O
symbol O
, O
Nis O
a O
Ô¨Ånite O
set O
of O
nonterminals O
, O
Pis O
a O
Ô¨Ånite O
set O
of O
preterminals O
, O
Œ£is O
a O
Ô¨Ånite O
set O
of O
terminal O
symbols O
, O
and O
Ris O
a O
Ô¨Ånite O
set O
of O
rules O
associated O
with O
probabilities O
œÄ O
. O
The O
rules O
are O
of O
the O
form O
: O
S‚ÜíA O
, O
A‚ààN O
A‚ÜíBC O
, O
A‚ààN O
, O
B O
, O
C‚ààN‚à™P O
T‚Üíw O
, O
T‚ààP O
, O
w‚ààŒ£. O
8Although O
our O
ranking O
algorithm O
can O
Ô¨Ålter O
out O
noisy O
heads O
, O
by O
observing O
the O
attention O
heatmaps O
, O
we O
Ô¨Ånd O
that O
noisy O
heads O
sometimes O
still O
rank O
high O
. O
We O
do O
not O
do O
any O
post O
- O
processing O
to O
further O
Ô¨Ålter O
out O
the O
noisy O
heads O
, O
so O
we O
empirically O
search O
kstarting O
at O
30 O
. O
9A O
more O
advanced O
version O
of O
the O
neural O
PCFG O
, O
the O
compound O
PCFG O
, O
has O
also O
been O
developed O
in O
Kim O
et O
al O
. O
( O
2019a O
) O
. O
In O
this O
model O
variant O
, O
a O
compound O
probability O
distribution O
is O
built O
upon O
the O
parameters O
of O
a O
neural O
PCFG O
. O
In O
preliminary O
experiments O
, O
we O
found O
the O
compound O
PCFG O
learns O
similar O
grammars O
as O
the O
neural O
PCFG O
. O
So O
we O
only O
use O
the O
more O
lightweight O
neural O
PCFG O
in O
this O
work O
. O
AssumingTGis O
the O
set O
of O
all O
possible O
parse O
trees O
ofG O
, O
the O
probability O
of O
a O
parse O
tree O
t‚àà O
TGis O
deÔ¨Åned O
asp(t O
) O
= O
/producttext O
r‚ààtRœÄr O
, O
wheretRis O
the O
set O
of O
rules O
used O
in O
the O
derivation O
of O
t. O
A O
PCFG O
also O
deÔ¨Ånes O
the O
probability O
of O
a O
given O
sentence O
x(string O
of O
terminals O
x‚ààŒ£‚àó O
) O
viap(x O
) O
= O
/summationtext O
t‚ààTG(x)p(t O
) O
, O
whereTG(x O
) O
= O
{ O
t|yield O
( O
t O
) O
= O
x O
} O
, O
i.e. O
, O
the O
set O
of O
trees O
tsuch O
that O
t O
‚Äôs O
leaves O
are O
x. O
The O
traditional O
way O
to O
parameterize O
a O
PCFG O
is O
to O
assign O
a O
scalar O
to O
each O
rule O
œÄrunder O
the O
constraint O
that O
valid O
probability O
distributions O
must O
be O
formed O
. O
For O
unsupervised O
parsing O
, O
however O
, O
this O
parameterization O
has O
been O
shown O
to O
be O
unable O
to O
learn O
meaningful O
grammars O
from O
natural O
language O
data O
( O
Carroll O
and O
Charniak O
, O
1992 O
) O
. O
Distributed O
representations O
, O
the O
core O
concept O
of O
the O
modern O
deep O
learning O
, O
have O
been O
introduced O
to O
address O
this O
issue O
( O
Kim O
et O
al O
. O
, O
2019a O
) O
. O
SpeciÔ¨Åcally O
, O
embeddings O
are O
associated O
with O
symbols O
and O
rules O
are O
modeled O
based O
on O
such O
distributed O
and O
shared O
representations O
. O
In O
the O
neural O
PCFG O
, O
the O
log O
marginal O
likelihood O
: O
logpŒ∏(x O
) O
= O
log O
/ O
summationdisplay O
t‚ààTG(x)pŒ∏(t O
) O
can O
be O
computed O
by O
summing O
out O
the O
latent O
parse O
trees O
using O
the O
inside O
algorithm O
( O
Baker O
, O
1979 O
) O
, O
which O
is O
differentiable O
and O
amenable O
to O
gradient O
based O
optimization O
. O
We O
refer O
readers O
to O
the O
original O
paper O
of O
Kim O
et O
al O
. O
( O
2019a O
) O
for O
details O
on O
the O
model O
architecture O
and O
training O
scheme O
. O
4.2 O
Learning O
Grammars O
from O
Induced O
Trees O
Given O
the O
trees O
induced O
from O
PLMs O
( O
described O
in O
Section O
3.2 O
) O
, O
we O
use O
neural O
PCFGs O
to O
learn O
constituency O
grammars O
. O
In O
contrast O
to O
unsupervised O
parsing O
, O
where O
neural O
PCFGs O
are O
trained O
solely O
on O
raw O
natural O
language O
data O
, O
we O
train O
them O
on O
the O
sentences O
and O
the O
corresponding O
tree O
structures O
induced O
from O
PLMs O
. O
Note O
that O
this O
differs O
from O
a O
fully O
supervised O
parsing O
setting O
, O
where O
both O
tree O
structures O
and O
internal O
constituency O
tags O
( O
nonterminals O
and O
preterminals O
) O
are O
provided O
in O
the O
treebank O
. O
In O
our O
case O
, O
the O
trees O
induced O
from O
PLMs O
have O
no O
internal O
annotations O
. O
For O
the O
neural O
PCFG O
training O
, O
the O
joint O
likelihood O
is O
given O
by O
: O
logp(x,ÀÜt O
) O
= O
/summationdisplay O
r‚ààÀÜtRlogœÄr O
, O
where O
ÀÜtis O
the O
induced O
tree O
and O
ÀÜtRis O
the O
set O
of O
rules O
applied O
in O
the O
derivation O
of O
ÀÜt O
. O
Although O
tree O
struc-413tures O
are O
given O
during O
training O
, O
marginalization O
is O
still O
involved O
: O
all O
internal O
tags O
will O
be O
marginalized O
to O
compute O
the O
joint O
likelihood O
. O
Therefore O
, O
the O
grammars O
learned O
by O
our O
method O
are O
anonymized O
: O
nonterminals O
and O
preterminals O
will O
be O
annotated O
as O
NT O
- O
idand O
T O
- O
id O
, O
respectively O
, O
where O
idis O
an O
arbitrary O
ID O
number O
. O
5 O
Experiments O
We O
conduct O
experiments O
to O
evaluate O
the O
unsupervised O
parsing O
performance O
of O
our O
ranking O
- O
based O
zero O
- O
shot O
parser O
on O
English O
and O
eight O
other O
languages O
( O
Basque O
, O
French O
, O
German O
, O
Hebrew O
, O
Hungarian O
, O
Korean O
, O
Polish O
, O
Swedish O
) O
. O
For O
the O
grammars O
learned O
from O
the O
induced O
parse O
trees O
, O
we O
perform O
qualitative O
and O
quantitative O
analysis O
on O
how O
the O
learned O
grammars O
resemble O
the O
human O
- O
crafted O
grammar O
of O
the O
English O
PTB O
. O
5.1 O
General O
Setup O
We O
prepare O
the O
PTB O
( O
Marcus O
et O
al O
. O
, O
1993 O
) O
for O
English O
and O
the O
SPMRL O
dataset O
( O
Seddah O
et O
al O
. O
, O
2013 O
) O
for O
eight O
other O
languages O
. O
We O
adopt O
the O
standard O
split O
of O
each O
dataset O
to O
divide O
it O
into O
development O
and O
test O
sets O
. O
For O
preprocessing O
, O
we O
follow O
the O
setting O
in O
Kim O
et O
al O
. O
( O
2019a O
, O
b O
) O
. O
We O
run O
our O
ranking O
algorithm O
on O
the O
development O
set O
to O
select O
the O
syntax O
- O
related O
heads O
and O
the O
ensemble O
parsing O
algorithm O
on O
the O
test O
set O
. O
We O
only O
use O
the O
raw O
sentences O
in O
the O
development O
set O
, O
without O
any O
syntactic O
annotations O
. O
We O
average O
all O
conÔ¨Ågurations O
both O
for O
ranking O
( O
f O
, O
scomp O
andscross O
) O
and O
parsing O
( O
fandscomp O
) O
; O
hence O
we O
do O
not O
tune O
any O
hyperparameters O
for O
our O
algorithm O
. O
ForKselection O
, O
we O
experiment O
with O
Ô¨Åxed O
top- O
K O
( O
i.e. O
, O
top-30 O
) O
and O
dynamically O
searching O
the O
best O
Kdescribed O
in O
Section O
3.3 O
, O
dubbed O
dynamic O
K. O
We O
report O
the O
unlabeled O
sentence O
- O
level O
F1score O
to O
evaluate O
the O
extent O
to O
which O
the O
induced O
trees O
resemble O
the O
corresponding O
gold O
standard O
trees O
. O
For O
neural O
PCFG O
training O
, O
we O
modify O
some O
details O
but O
keep O
most O
of O
the O
model O
conÔ¨Ågurations O
of O
Kim O
et O
al O
. O
( O
2019a O
) O
; O
we O
refer O
readers O
to O
the O
original O
paper O
for O
more O
information O
. O
We O
train O
the O
models O
on O
longer O
sentences O
for O
more O
epochs O
. O
SpeciÔ¨Åcally O
, O
we O
train O
on O
sentences O
of O
length O
up O
to O
30 O
in O
the O
Ô¨Årst O
epoch O
, O
and O
increase O
this O
length O
limit O
by O
Ô¨Åve O
until O
the O
length O
reaches O
80 O
. O
We O
train O
for O
30 O
epochs O
and O
use O
a O
learning O
rate O
scheduler O
. O
Model O
Top O
- O
down O
Chart O
- O
based O
Our O
ranking O
- O
based O
ConÔ¨ÅgurationSingle O
Single O
Top O
Top O
Top O
Dynamic O
Full O
/Layer‚Ä†/Layer‚Ä†-K O
-K‚Ä°-K O
K O
heads O
w/ O
dev O
trees O
w/o O
dev O
trees O
BERT O
- O
base O
- O
cased O
32.6 O
37.5 O
42.7 O
29.3 O
34.8 O
37.1 O
35.8 O
BERT O
- O
large O
- O
cased O
36.7 O
41.5 O
44.6 O
21.5 O
36.1 O
38.7 O
33.2 O
XLNet O
- O
base O
- O
cased O
39.0 O
40.5 O
46.4 O
38.4 O
41.2 O
42.7 O
42.4 O
XLNet O
- O
large O
- O
cased O
37.3 O
39.7 O
46.4 O
34.1 O
40.6 O
41.1 O
41.2 O
RoBERTa O
- O
base O
38.0 O
41.0 O
45.0 O
35.9 O
41.7 O
42.1 O
39.6 O
RoBERTa O
- O
large O
33.8 O
38.6 O
42.8 O
30.2 O
33.1 O
37.5 O
35.7 O
GPT2 O
35.4 O
34.5 O
38.5 O
21.9 O
26.1 O
27.2 O
26.1 O
GPT2 O
- O
medium O
37.8 O
38.5 O
39.8 O
19.4 O
29.1 O
29.1 O
27.2 O
A O
VG O
36.3 O
39.0 O
43.3 O
28.8 O
35.3 O
36.9 O
35.1 O
A O
VG O
w/o O
GPT2 O
* O
36.2 O
39.8 O
44.7 O
31.6 O
37.9 O
39.8 O
38.0 O
Table O
1 O
: O
Unlabeled O
sentence O
- O
level O
parsing O
F1scores O
on O
the O
English O
PTB O
test O
set O
. O
‚Ä† O
: O
the O
best O
results O
of O
the O
top O
single O
head O
and O
layer O
- O
wise O
ensemble O
. O
‚Ä° O
: O
directly O
applying O
the O
chart O
- O
based O
parser O
for O
ranking O
( O
no O
development O
set O
trees O
) O
and O
ensembling O
the O
top- O
Kheads O
for O
parsing O
. O
* O
: O
averageF1scores O
without O
GPT2 O
and O
GPT2 O
- O
medium O
. O
Bold O
Ô¨Ågures O
highlight O
the O
best O
scores O
for O
the O
two O
different O
groups O
: O
with O
and O
without O
development O
trees O
. O
Model O
F1SBAR O
NP O
VP O
PP O
ADJP O
ADVP O
Balanced O
18.5 O
7 O
27 O
8 O
18 O
27 O
25 O
Left O
branching O
8.7 O
5 O
11 O
0 O
5 O
2 O
8 O
Right O
branching O
39.4 O
68 O
2471 O
42 O
27 O
38 O
BERT O
- O
base O
- O
cased O
37.1 O
36 O
49 O
30 O
42 O
40 O
69 O
BERT O
- O
large O
- O
cased O
38.7 O
38 O
50 O
30 O
46 O
42 O
72 O
XLNet O
- O
base O
- O
cased O
42.7 O
45 O
58 O
31 O
46 O
46 O
72 O
XLNet O
- O
large O
- O
cased O
41.1 O
44 O
54 O
30 O
42 O
48 O
64 O
RoBERTa O
- O
base O
42.1 O
38 O
58 O
3147 O
42 O
71 O
RoBERTa O
- O
large O
37.5 O
35 O
53 O
29 O
33 O
36 O
54 O
Table O
2 O
: O
Unlabeled O
parsing O
scores O
and O
recall O
scores O
on O
six O
constituency O
tags O
of O
trivial O
baseline O
parse O
trees O
as O
well O
as O
ones O
achieved O
by O
our O
parser O
using O
dynamic O
K O
on O
different O
PLMs O
. O
5.2 O
Results O
on O
the O
English O
PTB O
We O
Ô¨Årst O
evaluate O
our O
ranking O
- O
based O
zero O
- O
shot O
parser O
on O
the O
English O
PTB O
dataset O
. O
We O
apply O
our O
methods O
to O
four O
different O
PLMs O
for O
English O
: O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
XLNet O
( O
Yang O
et O
al O
. O
, O
2019 O
) O
, O
RoBERTa O
( O
Liu O
et O
al O
. O
, O
2019b O
) O
, O
and O
GPT2 O
( O
Radford O
et O
al O
. O
, O
2019).10 O
Table O
1 O
shows O
the O
unlabeled O
F1scores O
for O
our O
ranking O
- O
based O
zero O
- O
shot O
parser O
as O
well O
as O
for O
previous O
zero O
- O
shot O
parsers O
in O
two O
settings O
, O
with O
and O
without O
an O
annotated O
development O
set O
. O
We O
employ O
the O
chart O
- O
based O
parser O
in O
a O
setting O
without O
development O
trees O
, O
where O
Eqs O
. O
( O
1 O
) O
and O
( O
2 O
) O
are O
used O
for O
10We O
follow O
previous O
work O
( O
Kim O
et O
al O
. O
, O
2020a O
, O
b O
) O
in O
using O
two O
variants O
for O
each O
PLM O
, O
where O
the O
X O
- O
base O
variants O
consist O
of O
12 O
layers O
, O
12 O
attention O
heads O
, O
and O
768 O
hidden O
dimensions O
, O
while O
the O
X O
- O
large O
ones O
have O
24 O
layers O
, O
16 O
heads O
, O
and O
1024 O
dimensions O
. O
With O
regard O
to O
GPT2 O
, O
the O
GPT2 O
model O
corresponds O
to O
X O
- O
base O
while O
GPT2 O
- O
medium O
to O
X O
- O
large.4140 O
20 O
40 O
60 O
80 O
100 O
120 O
140 O
k O
heads3132333435363738parsing O
f1 O
0.6 O
0.5 O
0.4 O
0.3 O
0.2 O
score O
parsing O
f1 O
score(a O
) O
BERT O
- O
base O
- O
cased O
0 O
20 O
40 O
60 O
80 O
100 O
120 O
140 O
k O
heads353637383940414243parsing O
f1 O
0.7 O
0.6 O
0.5 O
0.4 O
0.3 O
0.2 O
score O
parsing O
f1 O
score O
( O
b O
) O
XLNet O
- O
base O
- O
cased O
0 O
20 O
40 O
60 O
80 O
100 O
120 O
140 O
k O
heads373839404142parsing O
f1 O
0.6 O
0.5 O
0.4 O
0.3 O
0.2 O
score O
parsing O
f1 O
score O
( O
c O
) O
RoBERTa O
- O
base O
0 O
50 O
100 O
150 O
200 O
250 O
300 O
350 O
400 O
k O
heads313233343536373839parsing O
f1 O
0.6 O
0.5 O
0.4 O
0.3 O
0.2 O
0.1 O
score O
parsing O
f1 O
score O
( O
d O
) O
BERT O
- O
large O
- O
cased O
0 O
50 O
100 O
150 O
200 O
250 O
300 O
350 O
400 O
k O
heads3637383940414243parsing O
f1 O
0.7 O
0.6 O
0.5 O
0.4 O
0.3 O
0.2 O
score O
parsing O
f1 O
score O
( O
e O
) O
XLNet O
- O
large O
- O
cased O
0 O
50 O
100 O
150 O
200 O
250 O
300 O
350 O
400 O
k O
heads333435363738parsing O
f1 O
0.6 O
0.5 O
0.4 O
0.3 O
0.2 O
score O
parsing O
f1 O
score O
( O
f O
) O
RoBERTa O
- O
large O
Figure O
1 O
: O
Relation O
between O
Kfor O
top O
- O
Kand O
parsing O
performance O
on O
different O
PLMs O
. O
The O
blue O
curve O
shows O
the O
ranking O
score O
of O
heads O
where O
heads O
are O
sorted O
in O
an O
ascending O
order O
. O
The O
red O
curve O
shows O
the O
parsing O
performance O
that O
is O
evaluated O
on O
the O
PTB O
test O
set O
given O
every O
10 O
heads O
. O
The O
green O
dashed O
line O
indicates O
the O
dynamic O
K. O
ranking O
and O
ensembling O
the O
top- O
K(i.e O
. O
, O
top-30 O
) O
heads O
. O
Compared O
to O
our O
method O
under O
the O
same O
conÔ¨Åguration O
, O
its O
poor O
performance O
conÔ¨Årms O
the O
effectiveness O
of O
our O
ranking O
algorithm O
. O
With O
respect O
to O
the O
Kselection O
, O
our O
dynamic O
Kmethod O
beats O
both O
Ô¨Åxed O
top-30 O
and O
full O
heads O
. O
Surprisingly O
, O
using O
all O
attention O
heads O
for O
ensemble O
parsing O
yields O
nearly O
the O
same O
performance O
as O
using O
top-30 O
heads O
. O
This O
suggests O
that O
although O
our O
ranking O
algorithm O
Ô¨Ålters O
out O
some O
noisy O
heads O
, O
it O
is O
still O
not O
perfect O
. O
On O
the O
other O
hand O
, O
the O
ensemble O
parsing O
method O
is O
robust O
to O
noisy O
heads O
when O
full O
attention O
heads O
are O
used O
. O
Figure O
1 O
shows O
how O
the O
ensemble O
parsing O
performance O
changes O
given O
different O
Kselection O
. O
We O
can O
identify O
a O
roughly O
concave O
shape O
of O
the O
parsing O
performance O
curve O
, O
which O
indicates O
why O
our O
ranking O
algorithm O
works O
. O
Interestingly O
, O
the O
parsing O
performance O
does O
not O
drop O
too O
much O
when O
Kreaches O
the O
maximum O
for O
XLNet O
. O
We O
conjecture O
that O
syntactic O
knowledge O
is O
more O
broadly O
distributed O
across O
heads O
in O
XLNet O
. O
Our O
ranking O
- O
based O
parser O
performs O
badly O
on O
GPT2 O
and O
GPT2 O
- O
medium O
, O
which O
is O
not O
unexpected O
. O
Unlike O
other O
PLMs O
, O
models O
in O
the O
GPT2 O
category O
are O
auto O
- O
regressive O
language O
models O
, O
whose O
attention O
matrix O
is O
strictly O
lower O
triangular O
. O
It O
makes O
it O
hard O
for O
our O
ranking O
algorithm O
to O
work O
properly O
. O
But O
for O
top O
- O
down O
and O
chart O
- O
based O
zero O
- O
shot O
parsers O
, O
tuning O
against O
an O
annotated O
development O
set O
canalleviate O
this O
problem O
. O
We O
focus O
on O
BERT O
, O
XLNet O
and O
RoBERTa O
and O
only O
evaluate O
these O
three O
models O
in O
the O
rest O
of O
our O
experiments O
. O
Except O
for O
GPT2 O
variants O
, O
our O
parser O
with O
dynamic O
Koutperforms O
the O
top O
- O
down O
parser O
in O
all O
cases O
. O
On O
average O
( O
without O
GPT2 O
variants O
) O
, O
even O
though O
our O
parser O
only O
requires O
raw O
sentence O
data O
, O
it O
still O
matches O
the O
chartbased O
parser O
with O
the O
top O
single O
head O
or O
layer O
- O
wise O
ensemble O
. O
To O
explore O
the O
limit O
of O
the O
chart O
- O
based O
parser O
, O
we O
also O
present O
the O
results O
by O
selecting O
the O
top O
- O
K(i.e O
. O
, O
top-20 O
) O
heads O
using O
the O
annotated O
development O
set O
( O
Kim O
et O
al O
. O
, O
2020b).11Note O
that O
in O
this O
setting O
, O
the O
best O
conÔ¨Åguration O
, O
i.e. O
, O
the O
combination O
ofg O
, O
fandscomp O
as O
well O
asKare O
selected O
against O
the O
development O
set O
. O
This O
setting O
serves O
as O
an O
upper O
bound O
of O
the O
chart O
- O
based O
zero O
- O
shot O
parsing O
and O
largely O
outperforms O
our O
ranking O
- O
based O
method O
. O
Table O
2 O
presents O
the O
parsing O
scores O
as O
well O
as O
recall O
scores O
on O
different O
constituents O
of O
trivial O
baselines O
and O
our O
parser O
. O
It O
indicates O
that O
trees O
induced O
from O
XLNet O
- O
base O
- O
cased O
, O
XLNet O
- O
large O
- O
cased O
and O
RoBERTa O
- O
base O
can O
outperform O
the O
right O
- O
branching O
baseline O
without O
resembling O
it O
. O
This O
conÔ¨Årms O
that O
PLMs O
can O
produce O
non O
- O
trivial O
parse O
trees O
. O
Large O
gains O
on O
NP O
, O
ADJP O
and O
ADVP O
compared O
to O
the O
11Selecting O
heads O
against O
a O
development O
set O
ensures O
the O
quality O
of O
high O
ranking O
heads O
; O
top-20 O
heads O
are O
optimal O
in O
this O
setting O
( O
Kim O
et O
al O
. O
, O
2020b O
) O
, O
unlike O
top-30 O
in O
our O
setting.415Model O
English O
Basque O
French O
German O
Hebrew O
Hungarian O
Korean O
Polish O
Swedish O
A O
VG O
Trivial O
baselines O
Balanced O
18.5 O
24.4 O
12.9 O
15.2 O
18.1 O
14.0 O
20.4 O
26.1 O
13.3 O
18.1 O
Left O
branching O
8.7 O
14.8 O
5.4 O
14.1 O
7.7 O
10.6 O
16.5 O
28.7 O
7.6 O
12.7 O
Right O
branching O
39.4 O
22.4 O
1.3 O
3.0 O
0.0 O
0.0 O
21.1 O
0.7 O
1.7 O
10.0w/ O
dev O
treesChart O
- O
based O
( O
Single O
/ O
Layer)‚Ä† O
M O
- O
BERT O
41.2 O
38.1 O
30.6 O
32.1 O
31.9 O
30.4 O
46.4 O
43.5 O
27.5 O
35.7 O
XLM O
43.0 O
35.3 O
35.6 O
41.6 O
39.9 O
34.5 O
35.7 O
51.7 O
33.7 O
39.0 O
XLM O
- O
R O
44.4 O
40.4 O
31.0 O
32.8 O
34.1 O
32.4 O
47.5 O
44.7 O
29.2 O
37.4 O
XLM O
- O
R O
- O
large O
40.8 O
36.5 O
26.4 O
30.2 O
32.1 O
26.8 O
45.6 O
47.9 O
25.8 O
34.7 O
A O
VG O
42.4 O
37.6 O
30.9 O
34.2 O
34.5 O
31.0 O
43.8 O
46.9 O
29.1 O
36.7 O
Chart O
- O
based O
( O
top- O
K)‚Ä† O
M O
- O
BERT O
45.0 O
41.2 O
35.9 O
35.9 O
37.8 O
33.2 O
47.6 O
51.1 O
32.6 O
40.0 O
XLM O
47.7 O
41.3 O
36.7 O
43.8 O
41.0 O
36.3 O
35.7 O
58.5 O
36.5 O
41.9 O
XLM O
- O
R O
47.0 O
42.2 O
35.8 O
37.7 O
40.1 O
36.6 O
51.0 O
52.7 O
32.9 O
41.8 O
XLM O
- O
R O
- O
large O
45.1 O
40.2 O
29.7 O
37.1 O
36.2 O
31.0 O
46.9 O
47.9 O
27.8 O
38.0 O
A O
VG O
46.2 O
41.2 O
34.5 O
38.6 O
38.8 O
34.3 O
45.3 O
52.6 O
32.5 O
40.4w O
/ O
o O
dev O
treesCrosslingual O
ranking O
- O
based O
( O
Dynamic O
K)‚Ä° O
M O
- O
BERT O
40.7 O
38.2 O
31.0 O
31.0 O
29.0 O
27.1 O
43.3 O
30.7 O
25.8 O
33.0 O
XLM O
44.9 O
26.6 O
35.8 O
39.7 O
39.6 O
32.9 O
28.0 O
50.1 O
34.1 O
36.9 O
XLM O
- O
R O
45.5 O
38.2 O
34.0 O
35.5 O
36.7 O
33.5 O
45.2 O
39.4 O
29.9 O
37.6 O
XLM O
- O
R O
- O
large O
41.0 O
37.9 O
28.0 O
28.0 O
31.3 O
24.6 O
44.4 O
32.2 O
24.9 O
32.5 O
A O
VG O
43.0 O
34.7 O
32.4 O
33.5 O
35.0 O
29.8 O
40.4 O
39.2 O
29.2 O
35.3 O
Table O
3 O
: O
Parsing O
results O
on O
nine O
languages O
with O
multilingual O
PLMs.‚Ä† O
: O
attention O
heads O
are O
selected O
on O
the O
development O
trees O
in O
the O
target O
language.‚Ä° O
: O
attention O
heads O
are O
selected O
on O
raw O
sentences O
in O
English O
. O
Bold O
Ô¨Ågures O
highlight O
the O
best O
scores O
for O
the O
two O
different O
groups O
: O
with O
and O
without O
development O
trees O
. O
right O
branching O
baseline O
show O
that O
PLMs O
can O
better O
identify O
such O
constituents O
. O
5.3 O
Results O
for O
Languages O
other O
than O
English O
Low O
- O
resource O
language O
parsing O
is O
one O
of O
the O
main O
motivations O
for O
the O
development O
of O
unsupervised O
parsing O
algorithms O
, O
which O
makes O
a O
multilingual O
setting O
ideal O
for O
evaluation O
. O
Multilingual O
PLMs O
are O
attractive O
in O
this O
setting O
because O
they O
are O
trained O
to O
process O
over O
one O
hundred O
languages O
in O
a O
languageagnostic O
manner O
. O
Kim O
et O
al O
. O
( O
2020b O
) O
has O
investigated O
the O
zero O
- O
shot O
parsing O
capability O
of O
multilingual O
PLMs O
assuming O
that O
a O
small O
annotated O
development O
set O
is O
available O
. O
Here O
, O
by O
taking O
advantage O
of O
our O
ranking O
- O
based O
parsing O
algorithm O
, O
we O
use O
a O
more O
radical O
crosslingual O
setting O
. O
We O
rank O
attention O
heads O
only O
on O
sentences O
in O
English O
and O
directly O
apply O
the O
parser O
to O
eight O
other O
languages O
. O
We O
follow O
Kim O
et O
al O
. O
( O
2020b O
) O
and O
use O
four O
multilingual O
PLMs O
: O
a O
multilingual O
version O
of O
the O
BERT O
- O
base O
model O
( O
M O
- O
BERT O
, O
Devlin O
et O
al O
. O
2019 O
) O
, O
the O
XLM O
model O
( O
Conneau O
and O
Lample O
, O
2019 O
) O
, O
the O
XLM O
- O
R O
and O
XLM O
- O
R O
- O
large O
models O
( O
Conneau O
et O
al O
. O
, O
2020 O
) O
. O
Each O
multilingual O
PLM O
differs O
in O
architecture O
and O
pre O
- O
training O
data O
, O
and O
we O
refer O
readers O
to O
the O
original O
papers O
for O
more O
details O
. O
In O
Table O
3 O
, O
our O
crosslingual O
parser O
outperforms O
the O
trivial O
baselines O
in O
all O
cases O
by O
a O
large O
margin O
. O
Compared O
with O
the O
chart O
- O
based O
parser O
with O
the O
top O
head O
or O
layer O
- O
wise O
ensemble O
, O
our O
crosslingual O
parser O
can O
match O
the O
performance O
on O
Ô¨Åve O
out O
of O
nine O
languages O
. O
Among O
four O
model O
variants O
, O
XLM O
- O
R O
and O
XLM O
- O
R O
- O
large O
have O
identical O
training O
settings O
and O
pre O
- O
training O
data O
, O
and O
so O
form O
a O
controlled O
experiment O
. O
By O
directly O
comparing O
XLM O
- O
R O
and O
XLM O
- O
R O
- O
large O
, O
we O
conjecture O
that O
, O
as O
the O
capacity O
of O
the O
PLM O
scales O
, O
the O
model O
has O
more O
of O
a O
chance O
to O
learn O
separate O
hidden O
spaces O
for O
different O
languages O
. O
This O
is O
consistent O
with O
a O
recent O
study O
on O
multilingual O
BERT O
( O
Dufter O
and O
Sch O
¬®utze O
, O
2020 O
) O
showing O
that O
underparameterization O
is O
one O
of O
the O
main O
factors O
that O
contribute O
to O
multilinguality O
. O
Again O
, O
our O
method O
lags O
behind O
the O
chart O
- O
base O
zero O
- O
shot O
parser O
with O
a O
top- O
Kensemble O
. O
More O
experimental O
results O
including O
using O
target O
language O
for O
head O
selection O
in O
our O
method O
can O
be O
found O
in O
Appendix O
A.1 O
. O
5.4 O
Grammar O
Analysis O
By O
not O
relying O
on O
an O
annotated O
development O
set O
, O
we O
have O
an O
unbiased O
way O
of O
investigating O
the O
tree O
structures O
as O
well O
as O
the O
grammars O
that O
are O
inher-416TreesPreterminal O
Rule O
Parsing O
Acc‚Ä†Acc‚Ä°F1 O
Gold O
* O
66.1 O
46.2 O
BERT O
- O
base O
- O
cased O
64.4 O
24.8 O
37.1 O
BERT O
- O
large O
- O
cased O
64.0 O
22.3 O
38.7 O
XLNet O
- O
base O
- O
cased O
67.7 O
26.1 O
42.7 O
XLNet O
- O
large O
- O
cased O
65.8 O
27.3 O
41.1 O
RoBERTa O
- O
base O
65.7 O
27.2 O
42.1 O
RoBERTa O
- O
large O
62.4 O
25.1 O
37.5 O
Table O
4 O
: O
Preterminal O
( O
PoS O
tag O
) O
and O
production O
rule O
accuracies O
of O
PCFG O
PLMand O
PCFG O
Goldon O
the O
entire O
PTB O
. O
‚Ä† O
: O
PoS O
tagging O
accuracy O
using O
the O
many O
- O
to O
- O
one O
mapping O
( O
Johnson O
, O
2007 O
) O
. O
‚Ä° O
: O
production O
rule O
accuracy O
where O
anonymized O
nonterminals O
and O
preterminals O
are O
mapped O
to O
the O
gold O
tags O
using O
the O
many O
- O
to O
- O
one O
mapping O
. O
* O
: O
PCFG O
Gold O
. O
ent O
in O
PLMs O
. O
SpeciÔ¨Åcally O
, O
we O
Ô¨Årst O
parse O
the O
raw O
sentences O
using O
our O
ranking O
- O
based O
parser O
described O
in O
Section O
3.2 O
and O
then O
train O
a O
neural O
PCFG O
given O
the O
induced O
trees O
using O
the O
method O
in O
Section O
4.2 O
. O
We O
conduct O
our O
experiments O
on O
the O
English O
PTB O
and O
evaluate O
how O
the O
learned O
grammar O
resembles O
PTB O
syntax O
in O
a O
quantitative O
way O
on O
preterminals O
( O
PoS O
tags O
) O
and O
production O
rules O
. O
We O
visualize O
the O
alignment O
of O
preterminals O
and O
nonterminals O
of O
the O
learned O
grammar O
and O
the O
gold O
labels O
in O
Appendix O
A.2 O
as O
a O
qualitative O
study O
. O
We O
also O
showcase O
parse O
trees O
of O
the O
learned O
grammar O
to O
get O
a O
glimpse O
of O
some O
distinctive O
characteristics O
of O
the O
learned O
grammar O
in O
Appendix O
A.3 O
. O
For O
brevity O
, O
we O
refer O
to O
a O
neural O
PCFG O
learned O
from O
trees O
induced O
of O
a O
PLM O
as O
PCFG O
PLMand O
to O
a O
neural O
PCFG O
learned O
from O
the O
gold O
parse O
trees O
as O
PCFG O
Gold O
. O
In O
Table O
4 O
, O
we O
report O
preterminal O
( O
unsupervised O
PoS O
tagging O
) O
accuracies O
and O
production O
rule O
accuracies O
of O
PCFG O
PLM O
and O
PCFG O
Goldon O
the O
corpus O
level O
. O
For O
preterminal O
evaluation O
, O
we O
map O
the O
anonymized O
preterminals O
to O
gold O
PoS O
tags O
using O
many O
- O
to O
- O
one O
( O
M-1 O
) O
mapping O
( O
Johnson O
, O
2007 O
) O
, O
where O
each O
anonymized O
preterminal O
is O
matched O
onto O
the O
gold O
PoS O
tag O
with O
which O
it O
shares O
the O
most O
tokens O
. O
For O
production O
rule O
evaluation O
, O
we O
map O
both O
nonterminals O
and O
preterminals O
to O
gold O
tags O
using O
M-1 O
mapping O
to O
get O
the O
binary O
production O
rules.12We O
Ô¨Ånd O
that O
all O
PCFG O
PLMgrammars O
except O
for O
PCFG O
RoBERTa O
- O
large O
outperform O
a O
discrete O
HMM O
baseline O
( O
62.7 O
, O
He O
et O
al O
. O
2018 O
) O
but O
are O
far O
from O
the O
state O
of O
the O
art O
for O
neural O
grammar O
induc12For O
the O
gold O
annotations O
, O
we O
drop O
all O
unary O
rules O
. O
For O
n O
- O
ary O
rules O
( O
n O
> O
2 O
) O
, O
we O
convert O
them O
to O
binary O
rules O
by O
right O
branching O
and O
propagating O
the O
parent O
tag O
. O
For O
example O
, O
a O
n O
- O
ary O
rule O
A‚ÜíB O
C O
D O
yields O
A‚ÜíB O
A O
andA‚ÜíC O
D O
.tion O
( O
80.8 O
, O
He O
et O
al O
. O
2018 O
) O
. O
All O
PCFG O
PLMproduce O
similar O
accuracies O
on O
preterminals O
as O
PCFG O
Gold O
. O
However O
, O
for O
the O
production O
rules O
, O
PCFG O
PLMlags O
behind O
PCFG O
Goldby O
a O
large O
margin O
. O
This O
makes O
sense O
as O
presumably O
the O
tree O
structures O
heavily O
affect O
nonterminal O
learning O
. O
We O
also O
present O
the O
parsingF1scores O
of O
corresponding O
trees O
against O
the O
gold O
trees O
in O
Table O
4 O
for O
comparison O
. O
We O
observe O
that O
for O
all O
PCFG O
PLM O
, O
both O
preterminal O
accuracies O
and O
production O
rule O
accuracies O
correlate O
well O
with O
the O
parsingF1scores O
of O
the O
corresponding O
trees O
. O
6 O
Conclusion O
In O
this O
paper O
, O
we O
set O
out O
to O
analyze O
the O
syntactic O
knowledge O
learned O
by O
transformer O
- O
based O
pretrained O
language O
models O
. O
In O
contrast O
to O
previous O
work O
relying O
on O
test O
suites O
and O
probes O
, O
we O
proposed O
to O
use O
a O
zero O
- O
shot O
unsupervised O
parsing O
approach O
. O
This O
approach O
is O
able O
to O
parse O
sentences O
by O
ranking O
the O
attention O
heads O
of O
the O
PLM O
and O
ensembling O
them O
. O
Our O
approach O
is O
able O
to O
completely O
do O
away O
with O
a O
development O
set O
annotated O
with O
syntactic O
structures O
, O
which O
makes O
it O
ideal O
in O
a O
strictly O
unsupervised O
setting O
, O
e.g. O
, O
for O
low O
resource O
languages O
. O
We O
evaluated O
our O
method O
against O
previous O
methods O
on O
nine O
languages O
. O
When O
development O
sets O
are O
available O
for O
previous O
methods O
, O
our O
method O
can O
match O
them O
or O
produce O
competitive O
results O
if O
they O
use O
the O
top O
single O
head O
or O
layer O
- O
wise O
ensembling O
of O
attention O
heads O
, O
but O
lags O
behind O
them O
if O
they O
ensemble O
the O
top O
- O
Kheads O
. O
Furthermore O
, O
we O
present O
an O
analysis O
of O
the O
grammars O
learned O
by O
our O
approach O
: O
we O
use O
the O
induced O
trees O
to O
train O
a O
neural O
PCFG O
and O
evaluate O
the O
pre O
- O
terminal O
and O
non O
- O
terminal O
symbols O
of O
that O
grammar O
. O
In O
future O
work O
, O
we O
will O
develop O
further O
methods O
for O
analyzing O
the O
resulting O
grammar O
rules O
. O
Another O
avenue O
for O
follow O
- O
up O
research O
is O
to O
use O
our O
method O
to O
determine O
how O
the O
syntactic O
structures O
inherent O
in O
PLMs O
change O
when O
these O
models O
are O
Ô¨Åne O
- O
tuned O
on O
a O
speciÔ¨Åc O
task O
. O
Acknowledgments O
We O
thank O
the O
reviewers O
for O
their O
valuable O
suggestions O
regarding O
this O
work O
. O
Abstract O
Word O
embedding O
methods O
have O
become O
the O
defacto O
way O
to O
represent O
words O
, O
having O
been O
successfully O
applied O
to O
a O
wide O
array O
of O
natural O
language O
processing O
tasks O
. O
In O
this O
paper O
, O
we O
explore O
the O
hypothesis O
that O
embedding O
methods O
can O
also O
be O
effectively O
used O
to O
represent O
spatial O
locations O
. O
Using O
a O
new O
dataset O
consisting O
of O
the O
location O
trajectories O
of O
729 O
students O
over O
a O
seven O
month O
period O
and O
text O
data O
related O
to O
those O
locations O
, O
we O
implement O
several O
strategies O
to O
create O
location O
embeddings O
, O
which O
we O
then O
use O
to O
create O
embeddings O
of O
the O
sequences O
of O
locations O
a O
student O
has O
visited O
. O
To O
identify O
the O
surface O
level O
properties O
captured O
in O
the O
representations O
, O
we O
propose O
a O
number O
of O
probing O
tasks O
such O
as O
the O
presence O
of O
a O
speciÔ¨Åc O
location O
in O
a O
sequence O
or O
the O
type O
of O
activities O
that O
take O
place O
at O
a O
location O
. O
We O
then O
leverage O
the O
representations O
we O
generated O
and O
employ O
them O
in O
more O
complex O
downstream O
tasks O
ranging O
from O
predicting O
a O
student O
‚Äôs O
area O
of O
study O
to O
a O
student O
‚Äôs O
depression O
level O
, O
showing O
the O
effectiveness O
of O
these O
location O
embeddings O
. O
1 O
Introduction O
Due O
to O
the O
rising O
adoption O
of O
smartphones O
over O
the O
past O
decade O
, O
the O
number O
of O
services O
with O
full O
or O
partial O
information O
about O
people O
‚Äôs O
spatial O
mobility O
has O
skyrocketed O
. O
Inspired O
by O
the O
natural O
language O
processing O
( O
NLP O
) O
literature O
, O
we O
investigate O
various O
properties O
of O
location O
embeddings O
. O
We O
explore O
whether O
valuable O
information O
is O
encoded O
in O
individual O
location O
embeddings O
, O
as O
well O
as O
embeddings O
that O
encompass O
a O
sequence O
of O
locations O
. O
We O
begin O
by O
exploring O
whether O
they O
are O
able O
to O
represent O
aspects O
such O
as O
location O
presence O
or O
location O
functionality O
. O
Ultimately O
, O
we O
test O
the O
hypothesis O
that O
if O
enough O
underlying O
information O
is O
encoded O
, O
embedding O
models O
should O
aid O
in O
predicting O
user O
- O
centered O
descriptors O
, O
such O
as O
area O
of O
study O
, O
academic O
status O
, O
or O
mental O
health O
. O
Location O
data O
can O
be O
used O
by O
university O
administrators O
for O
applications O
that O
improve O
student O
life O
. O
From O
the O
frequency O
and O
the O
type O
of O
locations O
accessed O
in O
one O
‚Äôs O
daily O
routine O
, O
we O
may O
be O
able O
to O
identify O
someone O
who O
is O
depressed O
or O
someone O
who O
is O
overworked O
. O
Importantly O
, O
opt O
- O
in O
frameworks O
can O
be O
established O
to O
supplement O
existing O
counseling O
and O
advising O
ofÔ¨Åces O
, O
allowing O
for O
early O
intervention O
in O
the O
case O
of O
mental O
health O
and O
academic O
concerns O
. O
With O
proper O
privacy O
safeguards O
in O
place O
, O
such O
models O
could O
readily O
be O
applied O
on O
most O
university O
campuses O
, O
as O
WiFi O
connection O
data O
( O
from O
which O
we O
infer O
location O
) O
is O
likely O
already O
available O
. O
In O
addition O
, O
universities O
could O
use O
this O
data O
in O
an O
aggregate O
form O
to O
better O
understand O
student O
life O
and O
well O
- O
being O
, O
and O
Ô¨Ånd O
ways O
to O
promote O
healthy O
and O
engaging O
behaviors O
on O
campus O
. O
Such O
aggregate O
location O
information O
can O
also O
be O
used O
by O
architectural O
Ô¨Årms O
or O
municipalities O
to O
help O
with O
the O
selection O
of O
buildings O
‚Äô O
locations O
, O
architecture O
, O
and O
design O
; O
with O
road O
and O
pedestrian O
trafÔ¨Åc O
optimization O
; O
or O
for O
emergency O
response O
. O
We O
also O
know O
that O
such O
data O
is O
already O
available O
to O
large O
technology O
companies O
that O
track O
their O
users O
, O
and O
it O
is O
important O
to O
spread O
awareness O
about O
the O
personal O
information O
that O
can O
be O
gleaned O
. O
Research O
like O
ours O
helps O
inform O
users O
about O
privacy O
concerns O
, O
and O
may O
open O
up O
a O
path O
to O
stricter O
legislation O
regarding O
the O
use O
of O
such O
data O
in O
the O
future O
. O
While O
we O
envision O
numerous O
positive O
applications O
of O
these O
methods O
, O
there O
are O
clear O
privacy O
drawbacks O
that O
the O
public O
should O
be O
aware O
of O
in O
the O
current O
technological O
environment O
. O
Our O
work O
focuses O
on O
building O
an O
understanding O
of O
what O
information O
is O
encoded O
in O
location O
embeddings O
. O
In O
addition O
to O
creating O
embeddings O
using O
location O
trajectories O
, O
we O
propose O
an O
alternative O
method O
that O
synthesizes O
text O
from O
online O
sources O
to O
build O
representations O
that O
we O
hypothesize O
will O
better O
encode O
certain O
properties O
of O
locations O
. O
We425show O
that O
using O
dense O
location O
embeddings O
that O
incorporate O
both O
movement O
patterns O
and O
text O
data O
improves O
our O
ability O
to O
model O
downstream O
tasks O
. O
We O
see O
that O
although O
we O
are O
not O
able O
to O
recover O
as O
much O
surface O
level O
information O
from O
embeddings O
of O
location O
sequences O
as O
we O
are O
from O
a O
simpler O
representation O
, O
the O
additional O
semantic O
information O
that O
is O
encoded O
allows O
us O
to O
better O
predict O
some O
user O
attributes O
. O
2 O
Related O
Work O
Embedding O
Evaluation O
and O
Probing O
. O
Word O
embeddings O
are O
now O
widely O
used O
to O
create O
word O
representations O
using O
methods O
such O
as O
word2vec O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
, O
GloVe O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
, O
ELMo O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
, O
and O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O
BERT O
and O
ELMo O
can O
be O
used O
to O
create O
contextualized O
word O
embeddings O
, O
in O
which O
the O
vector O
representing O
an O
individual O
word O
varies O
depending O
on O
the O
context O
in O
which O
it O
appears O
. O
Previous O
methods O
including O
word2vec O
and O
GloVe O
did O
not O
make O
this O
distinction O
; O
adding O
context O
helped O
BERT O
achieve O
state O
- O
of O
- O
the O
- O
art O
results O
on O
many O
downstream O
NLP O
tasks O
. O
One O
traditional O
benchmark O
for O
word O
embeddings O
is O
performance O
on O
synthetic O
tasks O
, O
such O
as O
word O
similarity O
and O
word O
analogy O
tasks O
( O
Mikolov O
et O
al O
. O
, O
2013 O
; O
Pennington O
et O
al O
. O
, O
2014 O
) O
. O
However O
, O
word O
embeddings O
are O
widely O
used O
because O
of O
their O
superior O
performance O
on O
a O
variety O
of O
downstream O
NLP O
tasks O
when O
compared O
to O
other O
word O
representations O
. O
Performance O
on O
downstream O
tasks O
has O
been O
used O
to O
evaluate O
sentence O
embeddings O
, O
however O
such O
approaches O
can O
not O
gauge O
the O
content O
that O
is O
actually O
captured O
in O
the O
embeddings O
. O
To O
systematically O
ascertain O
what O
information O
is O
encoded O
in O
sentence O
vectors O
, O
researchers O
have O
turned O
to O
probing O
tasks O
( O
Shi O
et O
al O
. O
, O
2016 O
; O
Adi O
et O
al O
. O
, O
2017 O
; O
Conneau O
et O
al O
. O
, O
2018 O
) O
. O
These O
are O
meant O
to O
address O
the O
question O
‚Äú O
what O
information O
is O
encoded O
in O
a O
sentence O
vector O
‚Äù O
at O
a O
higher O
level O
. O
In O
our O
work O
, O
we O
Ô¨Ånd O
inspiration O
in O
the O
research O
by O
Conneau O
et O
al O
. O
( O
2018 O
) O
, O
who O
propose O
a O
formalized O
evaluation O
technique O
for O
sentence O
embeddings O
using O
a O
suite O
of O
ten O
classiÔ¨Åcation O
tasks O
focusing O
on O
: O
( O
1 O
) O
surface O
information O
( O
e.g. O
, O
length O
, O
word O
content O
) O
, O
( O
2 O
) O
syntactic O
information O
( O
e.g. O
, O
bigram O
shift O
, O
tree O
depth O
) O
, O
and O
( O
3 O
) O
semantic O
information O
( O
e.g. O
, O
tense O
) O
. O
The O
deep O
learning O
methods O
gave O
the O
best O
results O
overall O
, O
but O
the O
bag O
- O
of O
- O
vectors O
approach O
was O
a O
solid O
baseline O
for O
the O
word O
content O
task O
, O
where O
it O
outper O
- O
formed O
the O
deep O
learning O
models O
. O
Applications O
of O
Embeddings O
for O
Location O
Data O
. O
Liu O
et O
al O
. O
( O
2016 O
) O
were O
among O
the O
Ô¨Årst O
to O
use O
the O
skip O
- O
gram O
model O
on O
location O
data O
. O
They O
use O
locations O
visited O
before O
and O
after O
a O
target O
location O
as O
context O
to O
create O
location O
embeddings O
. O
These O
are O
then O
used O
in O
a O
personalized O
location O
recommendation O
system O
. O
Feng O
et O
al O
. O
( O
2017 O
) O
similarly O
create O
embeddings O
of O
check O
- O
in O
data O
, O
but O
use O
the O
CBOW O
model O
. O
Their O
application O
task O
is O
reversed O
, O
predicting O
future O
visitors O
for O
a O
location O
instead O
of O
predicting O
locations O
that O
a O
user O
will O
visit O
. O
Chang O
et O
al O
. O
( O
2018 O
) O
also O
predict O
next O
check O
- O
ins O
for O
users O
using O
a O
model O
based O
on O
skip O
- O
gram O
. O
Their O
work O
is O
uniquely O
related O
to O
ours O
in O
that O
they O
also O
build O
prediction O
of O
the O
text O
content O
of O
check O
- O
ins O
into O
the O
objective O
function O
. O
Zhu O
et O
al O
. O
( O
2019 O
) O
trained O
a O
skip O
- O
gram O
model O
to O
build O
location O
embeddings O
, O
and O
use O
them O
to O
understand O
the O
Ô¨Çow O
between O
urban O
locations O
. O
Crivellari O
and O
Beinat O
( O
2019 O
) O
explore O
location O
embeddings O
from O
the O
perspective O
of O
geoinformatics O
, O
paving O
the O
way O
for O
our O
probing O
tasks O
. O
The O
work O
of O
Solomon O
et O
al O
. O
( O
2018 O
) O
is O
most O
similar O
to O
our O
own O
. O
They O
use O
GPS O
data O
from O
cell O
phones O
as O
input O
to O
create O
embeddings O
and O
use O
data O
from O
a O
university O
setting O
. O
Our O
work O
differs O
in O
that O
we O
use O
the O
skip O
- O
gram O
model O
and O
incorporate O
text O
- O
based O
embeddings O
. O
We O
also O
propose O
probing O
tasks O
to O
better O
understand O
the O
embeddings O
that O
we O
create O
, O
and O
predict O
additional O
user O
attributes O
from O
our O
new O
dataset O
that O
go O
beyond O
demographic O
information O
. O
3 O
Data O
3.1 O
Student O
and O
Location O
Data O
Our O
dataset O
consists O
of O
location O
data O
collected O
from O
729 O
undergraduate O
university O
students O
who O
agreed O
to O
participate O
in O
our O
study O
in O
2018 O
and O
2019 O
over O
a O
period O
of O
seven O
months.1Two O
- O
thirds O
of O
the O
students O
participated O
during O
the O
winter O
semester O
, O
and O
the O
other O
third O
during O
the O
fall O
semester O
. O
Dataset O
statistics O
are O
presented O
in O
Table O
1 O
. O
Due O
to O
the O
sensitivity O
and O
scope O
of O
the O
data O
, O
it O
is O
infeasible O
for O
our O
study O
to O
include O
other O
universities O
; O
nonetheless O
, O
we O
believe O
that O
similar O
patterns O
would O
hold O
on O
other O
campuses O
as O
well O
. O
Because O
of O
privacy O
concerns O
, O
we O
are O
not O
able O
to O
publicly O
release O
this O
dataset O
. O
1The O
data O
was O
collected O
as O
part O
of O
a O
study O
that O
underwent O
a O
full O
board O
review O
and O
was O
approved O
by O
the O
IRB O
at O
the O
University O
of O
Michigan O
( O
study O
number O
HUM00126298 O
) O
. O
All O
participants O
in O
the O
study O
have O
signed O
an O
informed O
consent O
form.426Number O
of O
Participants O
729 O
Valid O
Location O
Visits O
After O
Pre O
- O
Processing O
478,329 O
Unique O
Locations O
194 O
Mean O
Locations O
per O
Participant O
656.2 O
Mean O
Locations O
per O
Day O
4.7 O
Table O
1 O
: O
Statistical O
summary O
of O
the O
location O
dataset O
. O
While O
most O
similar O
research O
uses O
GPS O
( O
Solomon O
et O
al O
. O
, O
2018 O
) O
, O
mobile O
check O
- O
ins O
( O
Feng O
et O
al O
. O
, O
2017 O
; O
Liu O
et O
al O
. O
, O
2016 O
) O
, O
or O
cell O
phone O
pings O
( O
Zhu O
et O
al O
. O
, O
2019 O
) O
for O
location O
tracking O
, O
we O
collect O
location O
data O
from O
WiFi O
access O
logs O
. O
WiFi O
access O
logs O
provide O
a O
strong O
and O
unbiased O
location O
signal O
on O
campus O
, O
as O
most O
students O
carry O
their O
smart O
phones O
with O
them O
at O
all O
times O
; O
however O
, O
a O
downside O
is O
that O
we O
do O
not O
have O
location O
data O
for O
large O
time O
chunks O
when O
students O
are O
not O
connected O
to O
the O
campus O
WiFi O
. O
The O
original O
data O
consists O
of O
20,766,750 O
WiFi O
session O
updates O
across O
all O
the O
students O
. O
We O
only O
consider O
connections O
with O
uninterrupted O
updates O
from O
a O
single O
building O
( O
without O
a O
connection O
to O
a O
network O
in O
another O
building O
) O
for O
at O
least O
ten O
minutes O
. O
This O
ensures O
that O
a O
student O
‚Äôs O
location O
will O
not O
be O
mapped O
to O
multiple O
points O
during O
overlapping O
time O
spans O
, O
and O
that O
locations O
where O
a O
student O
does O
not O
spend O
a O
notable O
amount O
of O
time O
are O
excluded O
. O
After O
collecting O
this O
list O
of O
locations O
, O
start O
, O
and O
stop O
times O
, O
we O
perform O
a O
merging O
operation O
on O
the O
data O
, O
sorted O
by O
start O
time O
. O
If O
spans O
for O
the O
same O
location O
occur O
consecutively O
in O
the O
series O
with O
start O
and O
stop O
times O
less O
than O
30 O
minutes O
apart O
, O
those O
spans O
are O
merged O
together O
. O
After O
this O
pre O
- O
processing O
, O
we O
are O
left O
with O
478,329 O
valid O
location O
spans O
with O
start O
and O
stop O
times O
. O
Since O
our O
dataset O
covers O
a O
single O
campus O
( O
194 O
locations O
) O
, O
each O
location O
was O
manually O
labeled O
with O
its O
functionality O
, O
for O
a O
total O
of O
thirteen O
functionalities O
. O
The O
Ô¨Åve O
most O
frequent O
are O
: O
class O
, O
study O
, O
dorm O
, O
lab O
, O
and O
library O
. O
While O
there O
are O
194 O
locations O
in O
the O
location O
dataset O
, O
we O
utilize O
132 O
in O
our O
analysis O
because O
this O
set O
of O
locations O
appears O
in O
all O
of O
the O
text O
- O
based O
datasets O
( O
described O
in O
Section O
3.2 O
) O
; O
the O
ones O
that O
are O
left O
out O
are O
not O
among O
the O
most O
frequently O
visited O
. O
In O
addition O
to O
location O
data O
, O
we O
collected O
a O
rich O
dataset O
containing O
information O
about O
the O
729 O
students O
, O
consisting O
of O
a O
series O
of O
extensive O
surveys O
taken O
by O
the O
students O
throughout O
the O
semester O
and O
academic O
data O
from O
the O
registrar O
. O
From O
the O
survey O
data O
, O
we O
use O
information O
on O
class O
year O
, O
gender O
, O
depression O
, O
and O
sleep O
satisfaction O
. O
From O
theCampus O
Dataset O
Website O
Reddit O
Twitter O
Overall O
Tokens O
581 O
K O
882 O
K O
655 O
K O
Unique O
Tokens O
GloVe O
9 O
K O
11 O
K O
18 O
K O
Median O
Instances O
Per O
Loc O
. O
3.5 O
20.0 O
166.5 O
Start O
Date O
( O
year O
- O
month O
) O
N O
/ O
A O
2011 O
- O
05 O
2010 O
- O
09 O
End O
Date O
( O
year O
- O
month O
) O
2019 O
- O
05 O
2019 O
- O
07 O
2019 O
- O
08 O
Table O
2 O
: O
Statistical O
information O
about O
text O
datasets O
. O
academic O
data O
, O
we O
utilize O
the O
GPA O
and O
the O
school O
where O
the O
student O
is O
enrolled O
. O
These O
combined O
data O
sources O
are O
used O
for O
our O
downstream O
classiÔ¨Åcation O
tasks O
. O
We O
chose O
students O
for O
the O
study O
covering O
all O
undergraduate O
class O
years O
, O
genders O
, O
and O
academic O
disciplines O
. O
3.2 O
Text O
Data O
In O
addition O
to O
location O
trajectories O
, O
we O
use O
text O
data O
from O
three O
sources O
( O
campus O
website O
, O
Reddit O
, O
Twitter O
) O
that O
illustrate O
various O
ways O
in O
which O
text O
can O
be O
used O
to O
represent O
places O
. O
Statistics O
of O
the O
text O
datasets O
are O
shown O
in O
Table O
2 O
. O
Campus O
Website O
. O
With O
this O
dataset O
, O
we O
capture O
how O
people O
formally O
deÔ¨Åne O
locations O
. O
The O
university O
hosts O
a O
building O
search O
website O
that O
links O
to O
pages O
containing O
information O
about O
campus O
buildings O
, O
including O
the O
departments O
hosted O
inside O
. O
We O
manually O
link O
the O
locations O
in O
our O
dataset O
with O
building O
pages O
on O
this O
site O
, O
then O
scrape O
the O
Ô¨Årst O
Google O
search O
result O
constrained O
within O
the O
university O
domain O
for O
each O
listed O
department O
, O
and O
use O
that O
text O
to O
represent O
the O
location O
. O
In O
addition O
to O
the O
departments O
, O
some O
pages O
directly O
link O
to O
a O
website O
( O
e.g. O
, O
a O
gym O
links O
to O
recreational O
sports O
) O
, O
from O
which O
we O
also O
scrape O
text O
. O
Reddit O
. O
With O
this O
dataset O
, O
we O
capture O
how O
people O
informally O
discuss O
locations O
. O
From O
the O
university O
Reddit O
page O
, O
we O
search O
for O
building O
names O
. O
We O
increase O
the O
search O
term O
list O
using O
OpenStreetMap,2 O
which O
lists O
alternate O
names O
for O
many O
buildings O
. O
We O
include O
text O
from O
posts O
and O
comments O
that O
speciÔ¨Åcally O
mention O
a O
building O
. O
Twitter O
. O
With O
this O
dataset O
, O
we O
capture O
how O
people O
express O
themselves O
in O
various O
locations O
. O
We O
collect O
tweets O
that O
have O
been O
geotagged O
with O
GPS O
points O
within O
0.05 O
kilometers O
of O
campus O
buildings O
. O
2https://www.openstreetmap.org/4274 O
Representing O
Locations O
We O
use O
location O
trajectories O
and O
text O
data O
to O
create O
vector O
representations O
of O
locations O
and O
, O
subsequently O
, O
embeddings O
of O
sequences O
of O
locations O
that O
are O
visited O
by O
a O
single O
person O
. O
After O
pre O
- O
processing O
using O
the O
method O
described O
in O
Section O
3.1 O
, O
the O
location O
input O
data O
consists O
of O
a O
series O
of O
sorted O
, O
nonoverlapping O
locations O
for O
a O
number O
of O
users O
with O
start O
and O
end O
times O
. O
We O
discuss O
multiple O
methods O
to O
create O
vector O
representations O
based O
on O
this O
data O
. O
4.1 O
Location O
Trajectory O
- O
Based O
Representations O
To O
create O
embeddings O
of O
locations O
, O
we O
make O
use O
of O
the O
temporal O
nature O
of O
the O
location O
trajectories O
to O
create O
a O
sequence O
of O
names O
of O
locations O
visited O
by O
a O
user O
over O
a O
period O
of O
time O
( O
e.g. O
, O
the O
seven O
month O
period O
of O
our O
data O
collection O
, O
see O
Section O
3.1 O
) O
. O
A O
skip O
- O
gram O
model O
is O
trained O
to O
use O
a O
location O
to O
predict O
locations O
around O
it O
in O
a O
user O
‚Äôs O
schedule O
, O
creating O
location O
embeddings O
that O
we O
expect O
will O
encode O
semantic O
information O
about O
locations.3 O
We O
represent O
each O
hour O
during O
the O
data O
collection O
period O
as O
a O
distinct O
token O
in O
the O
input O
trajectories O
. O
If O
a O
user O
has O
visited O
a O
single O
location O
in O
one O
hour O
, O
that O
location O
will O
be O
used O
in O
the O
slot O
for O
the O
hour O
; O
if O
they O
visited O
multiple O
locations O
, O
their O
predominant O
location O
will O
be O
used O
. O
If O
we O
do O
not O
have O
any O
location O
data O
for O
the O
user O
during O
that O
hour O
, O
we O
use O
the O
EXTERNAL O
token O
. O
This O
approach O
gives O
an O
exact O
meaning O
to O
the O
distance O
between O
locations O
in O
a O
sequence O
, O
while O
a O
raw O
sequence O
would O
ignore O
gaps O
in O
the O
data O
. O
The O
approach O
of O
using O
one O
token O
per O
set O
time O
interval O
is O
also O
used O
in O
Zhu O
et O
al O
. O
( O
2019 O
) O
. O
We O
refer O
to O
the O
method O
as O
Loc2V O
, O
and O
show O
a O
visualization O
in O
Figure O
1 O
. O
4.2 O
Text O
- O
Based O
Representations O
In O
addition O
to O
creating O
location O
representations O
from O
trajectories O
in O
the O
physical O
world O
, O
we O
explore O
the O
idea O
of O
using O
relevant O
text O
to O
deÔ¨Åne O
locations O
. O
Such O
text O
can O
reveal O
information O
about O
locations O
that O
may O
not O
be O
discernible O
from O
location O
trajectories O
, O
e.g. O
that O
people O
meet O
friends O
in O
a O
certain O
place O
. O
Therefore O
, O
for O
the O
same O
locations O
that O
appear O
in O
3We O
use O
the O
default O
window O
size O
of O
5 O
and O
generate O
embeddings O
with O
25 O
dimensions O
. O
While O
25 O
dimensions O
is O
fairly O
small O
in O
the O
context O
of O
word O
embeddings O
, O
since O
our O
dataset O
has O
fewer O
than O
two O
hundred O
locations O
that O
we O
seek O
to O
embed O
, O
higher O
values O
can O
not O
be O
considered O
as O
leading O
to O
a O
dimensionality O
reduction O
. O
We O
use O
a O
negative O
sampling O
value O
of O
20 O
, O
as O
is O
suggested O
by O
Mikolov O
et O
al O
. O
( O
2013 O
) O
for O
small O
datasets O
. O
Figure O
1 O
: O
A O
sample O
sequence O
of O
locations O
, O
and O
the O
corresponding O
sequences O
that O
are O
used O
as O
Loc2V O
input O
. O
our O
trajectories O
, O
we O
collect O
textual O
data O
that O
enables O
us O
to O
derive O
text O
- O
based O
representations O
from O
three O
sources O
as O
described O
in O
Section O
3.2 O
. O
Using O
each O
textual O
data O
source O
, O
we O
map O
a O
location O
name O
to O
a O
set O
of O
relevant O
words O
. O
We O
calculate O
tf.idf O
( O
Salton O
and O
Buckley O
, O
1988 O
) O
weights O
for O
each O
word O
, O
then O
use O
those O
weights O
to O
compute O
a O
weighted O
average O
of O
pre O
- O
trained O
word O
embeddings O
. O
Because O
our O
datasets O
are O
primarily O
from O
social O
media O
, O
we O
use O
pre O
- O
trained O
GloVe O
embeddings O
that O
were O
obtained O
from O
Twitter O
data.4The O
resulting O
vector O
is O
used O
as O
a O
location O
representation O
. O
4.3 O
Combining O
Representations O
We O
hypothesize O
that O
trajectory O
based O
and O
text O
- O
based O
representations O
may O
encode O
different O
aspects O
of O
locations O
. O
Therefore O
, O
in O
addition O
to O
representing O
locations O
using O
text O
and O
physical O
trajectories O
, O
we O
experiment O
with O
combining O
the O
two O
. O
Our O
Ô¨Årst O
method O
concatenates O
embedding O
vectors O
created O
from O
physical O
trajectories O
and O
vectors O
created O
from O
text O
data O
. O
Our O
second O
method O
performs O
retroÔ¨Åtting O
on O
top O
of O
text O
- O
based O
vectors O
. O
In O
the O
context O
of O
embeddings O
, O
‚Äú O
retroÔ¨Åtting O
‚Äù O
describes O
the O
process O
of O
modifying O
vectors O
that O
have O
already O
been O
created O
to O
better O
encode O
additional O
criteria O
. O
We O
Ô¨Ånd O
inspiration O
in O
the O
method O
from O
Faruqui O
et O
al O
. O
( O
2015 O
) O
, O
which O
retroÔ¨Åts O
word O
embeddings O
to O
a O
graph O
representing O
a O
semantic O
lexicon O
. O
In O
our O
work O
, O
we O
retroÔ¨Åt O
text O
embeddings O
to O
the O
graph O
that O
represents O
the O
transitions O
between O
locations O
; O
the O
nodes O
are O
locations O
, O
and O
the O
edges O
are O
weighted O
by O
the O
number O
of O
times O
there O
was O
a O
transition O
between O
those O
two O
locations O
in O
our O
dataset O
. O
The O
retroÔ¨Åtting O
method O
takes O
a O
matrix O
ÀÜQ O
, O
the O
initial O
vectors O
, O
and O
updates O
matrix O
Q(initialized O
to O
ÀÜQ O
) O
using O
a O
location O
transition O
graph O
. O
The O
objective O
4https://nlp.stanford.edu/projects/glove/428Figure O
2 O
: O
Comparison O
of O
the O
concatenation O
and O
retroÔ¨Åtting O
methods O
. O
function O
incorporates O
the O
set O
of O
edges O
E O
, O
bringing O
vectors O
that O
share O
an O
edge O
closer O
together O
in O
the O
vector O
space O
: O
Œ®(Q O
) O
= O
/summationtextn O
i=1 O
/ O
bracketleftBig O
Œ±i O
/ O
bardblqi‚àíÀÜqi O
/ O
bardbl2+/summationtext O
( O
i O
, O
j)‚ààEŒ≤ij O
/ O
bardblqi‚àíqj O
/ O
bardbl2 O
/ O
bracketrightBig O
An O
iterative O
method O
is O
used O
to O
update O
matrix O
Q O
: O
qi=/summationtext O
j:(i O
, O
j)‚ààEŒ≤ijqj+Œ±iÀÜqi O
/ O
summationtext O
j:(i O
, O
j)‚ààEŒ≤ij+Œ±i O
We O
perform O
ten O
iterations O
, O
as O
was O
done O
in O
previous O
work O
. O
The O
parameters O
Œ±andŒ≤control O
the O
relative O
importance O
of O
the O
two O
components O
( O
initial O
vectors O
and O
location O
graph O
) O
. O
In O
their O
implementation O
, O
Faruqui O
et O
al O
. O
set O
Œ±i= O
1andŒ≤ij O
= O
degree O
( O
i)‚àí1 O
. O
As O
the O
graph O
we O
use O
is O
weighted O
, O
we O
introduce O
a O
weighted O
version O
that O
incorporates O
edge O
weights O
W O
, O
using O
a O
weighted O
inverse O
degree O
for O
Œ≤ O
. O
The O
retroÔ¨Åtting O
method O
enhances O
the O
text O
- O
based O
information O
by O
adding O
the O
assumption O
that O
locations O
that O
are O
visited O
sequentially O
are O
similar O
( O
in O
the O
sense O
that O
a O
person O
who O
visits O
one O
would O
visit O
the O
other O
) O
, O
bringing O
them O
closer O
in O
the O
vector O
space O
. O
This O
method O
aims O
to O
infuse O
the O
text O
- O
based O
representations O
with O
information O
related O
to O
the O
cooccurrence O
of O
locations O
in O
a O
student O
‚Äôs O
trajectory O
; O
locations O
that O
co O
- O
occur O
may O
be O
suggestive O
of O
, O
for O
instance O
, O
areas O
of O
campus O
that O
tend O
to O
be O
visited O
by O
engineering O
students O
. O
It O
is O
not O
used O
on O
the O
trajectorybased O
representations O
, O
as O
these O
already O
incorporate O
location O
transitions O
. O
Figure O
2 O
compares O
the O
concatenation O
and O
retroÔ¨Åtting O
methods O
. O
As O
outlined O
above O
, O
the O
concatenation O
method O
directly O
combines O
the O
two O
vectors O
into O
one O
with O
the O
same O
content O
, O
while O
the O
retroÔ¨Åtting O
method O
takes O
information O
from O
a O
graph O
structure O
representing O
trajectories O
into O
account O
to O
create O
a O
modiÔ¨Åed O
version O
of O
the O
original O
vector O
. O
Figure O
3 O
: O
Fictional O
examples O
of O
locations O
visited O
by O
students O
; O
a O
larger O
pin O
reÔ¨Çects O
more O
time O
spent O
at O
a O
location O
. O
4.4 O
Representing O
a O
Sequence O
of O
Locations O
To O
represent O
a O
sequence O
of O
locations O
, O
we O
use O
a O
vector O
representing O
the O
locations O
that O
a O
person O
has O
visited O
in O
a O
month O
, O
instead O
of O
the O
individual O
locations O
. O
We O
settled O
on O
this O
time O
interval O
since O
a O
shorter O
time O
span O
( O
such O
as O
a O
day O
) O
contains O
very O
little O
predictive O
information O
, O
while O
a O
longer O
span O
( O
one O
semester O
) O
groups O
together O
distinct O
time O
spans O
that O
may O
lead O
to O
divergent O
behaviors O
, O
such O
as O
exam O
periods O
. O
We O
create O
a O
sequence O
embedding O
by O
taking O
a O
weighted O
average O
of O
the O
location O
vectors O
included O
in O
the O
sequence O
, O
using O
the O
time O
spent O
at O
each O
location O
as O
weights O
, O
thus O
increasing O
the O
importance O
of O
locations O
at O
which O
the O
person O
spent O
more O
time O
. O
5 O
Probing O
Location O
Representations O
While O
some O
of O
the O
methods O
we O
use O
( O
i.e. O
, O
skip O
- O
gram O
) O
have O
been O
used O
in O
the O
past O
to O
represent O
locations O
for O
certain O
tasks O
, O
there O
has O
been O
less O
work O
studying O
them O
intrinsically O
. O
We O
propose O
surface O
level O
tasks O
to O
probe O
the O
properties O
encoded O
in O
location O
embeddings O
, O
which O
are O
important O
to O
gain O
a O
deeper O
understanding O
of O
the O
type O
of O
information O
they O
capture O
. O
We O
split O
surface O
level O
tasks O
into O
two O
categories O
: O
those O
that O
focus O
on O
individual O
locations O
and O
those O
that O
focus O
on O
location O
sequences O
. O
In O
addition O
to O
these O
surface O
level O
tasks O
, O
we O
propose O
a O
set O
of O
downstream O
prediction O
tasks O
to O
validate O
the O
utility O
of O
such O
embeddings O
. O
5.1 O
Surface O
Level O
Location O
Tasks O
With O
these O
tasks O
, O
we O
examine O
two O
properties O
that O
should O
be O
encoded O
in O
location O
representations O
: O
location O
functionality O
and O
physical O
proximity O
. O
To O
directly O
compare O
how O
well O
each O
method O
encodes O
these O
semantic O
properties O
, O
we O
propose O
a O
metric O
to O
measure O
each O
property O
. O
We O
are O
inspired O
by O
Ye O
and O
Skiena O
( O
2019 O
) O
, O
who O
use O
similar O
methods O
to O
analyze O
properties O
of O
name O
embeddings O
( O
representations O
of429people O
‚Äôs O
names O
) O
. O
We O
borrow O
their O
method O
of O
analysis O
, O
measuring O
overlaps O
in O
the O
Nnearest O
neighbors O
for O
various O
values O
of O
N O
, O
but O
they O
analyze O
a O
different O
property O
, O
namely O
the O
gender O
associated O
with O
the O
name O
. O
Functionality O
Overlap O
. O
Each O
location O
in O
our O
dataset O
is O
annotated O
with O
its O
functionality O
, O
including O
two O
functionalities O
for O
mixed O
- O
use O
buildings O
, O
e.g. O
, O
a O
class O
building O
that O
also O
contains O
labs O
. O
For O
each O
location O
, O
we O
calculate O
the O
percentage O
of O
its O
nearest O
neighbors O
in O
the O
vector O
space O
that O
share O
at O
least O
one O
functionality O
; O
a O
higher O
value O
indicates O
that O
the O
embeddings O
more O
distinctly O
capture O
functionality O
. O
We O
compute O
nearest O
neighbors O
using O
cosine O
similarity O
. O
Physical O
Distance O
. O
We O
compute O
the O
distance O
in O
kilometers O
between O
a O
location O
and O
its O
nearest O
neighbors O
, O
and O
average O
the O
distances O
. O
This O
allows O
us O
to O
measure O
exactly O
how O
far O
a O
location O
is O
from O
its O
nearest O
neighbors O
; O
a O
lower O
number O
for O
this O
metric O
correlates O
with O
an O
increased O
physical O
proximity O
. O
5.2 O
Surface O
Level O
Sequence O
Tasks O
Our O
surface O
level O
sequence O
tasks O
are O
inspired O
by O
the O
methodology O
proposed O
by O
Conneau O
et O
al O
. O
( O
2018 O
) O
to O
probe O
sentence O
embeddings O
. O
Many O
of O
those O
tasks O
focus O
on O
syntax O
, O
which O
is O
not O
relevant O
for O
our O
use O
case O
, O
but O
we O
adapt O
their O
task O
for O
location O
- O
presence O
and O
propose O
probing O
for O
functionality O
- O
presence O
. O
Location O
Presence O
. O
We O
propose O
a O
binary O
locationpresence O
classiÔ¨Åcation O
task O
( O
LocPres O
) O
. O
We O
create O
classiÔ¨Åers O
for O
each O
location O
, O
predicting O
if O
the O
location O
appears O
in O
a O
sequence O
. O
We O
average O
the O
results O
across O
all O
locations O
with O
at O
least O
one O
hundred O
positive O
and O
negative O
examples O
( O
resulting O
in O
being O
able O
to O
assess O
83 O
locations O
out O
of O
132 O
) O
. O
Functionality O
Presence O
. O
We O
also O
propose O
a O
functionality O
- O
presence O
task O
( O
FuncPres O
) O
. O
Given O
a O
sequence O
embedding O
, O
we O
predict O
if O
it O
includes O
locations O
of O
a O
certain O
functionality O
. O
We O
use O
a O
binary O
classiÔ¨Åcation O
setup O
that O
mirrors O
the O
one O
used O
for O
the O
location O
- O
presence O
task O
. O
We O
treat O
the O
classiÔ¨Åcation O
of O
either O
the O
primary O
or O
secondary O
functionalities O
assigned O
to O
locations O
as O
correct O
. O
As O
with O
the O
location O
- O
presence O
task O
, O
we O
average O
results O
over O
all O
functionalities O
with O
at O
least O
one O
hundred O
training O
instances O
from O
each O
class O
( O
accounting O
for O
11 O
functionalities O
out O
of O
13 O
) O
. O
5.3 O
Downstream O
Application O
- O
Based O
Tasks O
In O
addition O
to O
surface O
level O
tasks O
, O
we O
want O
to O
understand O
what O
other O
human O
- O
centric O
information O
isencoded O
in O
location O
sequence O
embeddings O
. O
Our O
hypothesis O
is O
that O
the O
way O
in O
which O
students O
spend O
their O
time O
may O
be O
indicative O
of O
certain O
information O
about O
them O
; O
an O
example O
of O
students O
‚Äô O
diverse O
behavior O
on O
campus O
is O
shown O
in O
Figure O
3 O
. O
Using O
the O
dataset O
described O
in O
Section O
3.1 O
, O
we O
propose O
seven O
classiÔ¨Åcation O
tasks O
: O
Ô¨Åve O
tasks O
with O
two O
classes O
( O
major O
depression O
, O
all O
depression O
, O
gender O
, O
sleep O
satisfaction O
, O
and O
GPA O
) O
, O
one O
task O
with O
three O
classes O
( O
to O
predict O
which O
school O
a O
student O
is O
enrolled O
in O
, O
e.g. O
business O
or O
engineering O
) O
, O
and O
one O
task O
with O
four O
classes O
( O
to O
predict O
class O
year O
) O
. O
Sleep O
satisfaction O
is O
reported O
in O
a O
survey O
( O
Section O
3.1 O
) O
on O
a O
Ô¨Åve O
- O
point O
Likert O
scale O
; O
the O
top O
three O
responses O
are O
mapped O
to O
a O
positive O
class O
, O
and O
the O
bottom O
two O
to O
a O
negative O
class O
. O
As O
semester O
GPA O
is O
continuous O
, O
we O
formulate O
the O
binary O
classiÔ¨Åcation O
as O
less O
than O
or O
greater O
than O
3.5 O
( O
between O
Aand O
B+ O
) O
. O
Depression O
is O
measured O
using O
the O
standard O
PHQ-8 O
survey O
; O
using O
a O
clinically O
validated O
algorithm O
( O
Kroenke O
et O
al O
. O
, O
2001 O
) O
, O
we O
classify O
major O
depression O
( O
binary O
) O
, O
along O
with O
major O
and O
other O
depression O
( O
a O
weaker O
diagnosis O
) O
; O
we O
label O
the O
former O
as O
‚Äú O
major O
depression O
‚Äù O
and O
the O
latter O
as O
‚Äú O
all O
depression O
. O
‚Äù O
For O
the O
other O
tasks O
, O
we O
Ô¨Ålter O
out O
underpopulated O
classes O
, O
going O
from O
18 O
to O
three O
classes O
for O
school O
, O
from O
Ô¨Åve O
to O
two O
for O
gender O
, O
and O
from O
Ô¨Åve O
to O
four O
for O
class O
year O
. O
We O
use O
a O
classiÔ¨Åcation O
approach O
over O
regression O
because O
we O
hope O
that O
this O
work O
can O
be O
used O
to O
identify O
at O
- O
risk O
students O
. O
6 O
Experimental O
Setup O
We O
perform O
10 O
- O
fold O
cross O
validation O
on O
729 O
instances O
, O
where O
each O
instance O
represents O
a O
student O
. O
Preliminary O
classiÔ¨Åcation O
experiments O
were O
conducted O
on O
a O
small O
subset O
using O
SVM O
with O
linear O
and O
RBF O
kernels O
, O
random O
forests O
, O
decision O
trees O
, O
and O
Na O
¬®ƒ±ve O
Bayes O
, O
yet O
linear O
SVM O
had O
the O
most O
robust O
performance O
. O
Accordingly O
, O
our O
experiments O
consist O
of O
classiÔ¨Åcation O
tasks O
using O
linear O
SVM O
. O
As O
many O
of O
the O
classes O
are O
unbalanced O
, O
we O
more O
heavily O
weight O
updates O
for O
the O
minority O
class(es O
) O
by O
modifying O
the O
loss O
function O
to O
use O
a O
weight O
that O
is O
inversely O
proportional O
to O
the O
class O
‚Äôs O
prevalence O
. O
To O
predict O
a O
student O
attribute O
, O
we O
create O
one O
vector O
for O
each O
month O
of O
data O
collection O
pertaining O
to O
each O
student O
, O
using O
the O
process O
described O
in O
Section O
4.4 O
. O
Our O
training O
framework O
is O
illustrated O
in O
Figure O
4 O
. O
We O
start O
by O
feeding O
the O
sequence O
vectors O
through O
a O
SVM O
classiÔ¨Åer O
, O
which O
predicts O
monthlevel O
labels O
. O
These O
are O
then O
concatenated O
to O
form O
a430L4L9L7L1L1L7 O
L9L4locationvectorsmonthsequencevectorsLabel O
1 O
SVMLabel O
2meta O
classifierSVMJanFebJanFebPredicted O
Label O
‚Ä¶ O
‚Ä¶ O
Œ£Œ£Figure O
4 O
: O
The O
framework O
for O
downstream O
prediction O
tasks O
. O
student O
instance O
and O
are O
passed O
to O
a O
meta O
- O
classiÔ¨Åer O
that O
decides O
the O
Ô¨Ånal O
class O
label O
for O
that O
student O
. O
We O
use O
the O
meta O
- O
classiÔ¨Åcation O
approach O
to O
allow O
the O
Ô¨Årst O
classiÔ¨Åer O
more O
data O
to O
learn O
from O
; O
without O
this O
approach O
, O
the O
number O
of O
input O
samples O
is O
relatively O
small O
( O
729 O
) O
. O
The O
process O
for O
surface O
level O
sequence O
tasks O
is O
similar O
, O
but O
no O
meta O
- O
classiÔ¨Åer O
is O
used O
, O
as O
the O
gold O
standard O
labels O
have O
a O
month O
- O
level O
granularity O
. O
7 O
Results O
and O
Discussion O
Figure O
5 O
and O
Tables O
3 O
and O
5 O
show O
the O
results O
obtained O
for O
the O
probing O
tasks O
. O
In O
addition O
to O
the O
loc2vec O
trajectory O
and O
text O
- O
based O
models O
, O
we O
run O
our O
experiments O
with O
two O
combination O
models O
, O
using O
the O
methods O
discussed O
in O
Section O
4.3 O
. O
We O
employ O
the O
Reddit O
variation O
for O
these O
combination O
models O
due O
to O
its O
strong O
performance O
on O
downstream O
tasks O
; O
we O
incorporate O
one O
model O
using O
concatenation O
and O
a O
model O
using O
retroÔ¨Åtting O
. O
We O
refer O
to O
these O
models O
as O
‚Äú O
Loc2V O
- O
Reddit O
, O
‚Äù O
and O
‚Äú O
RedditRetroÔ¨Åt O
, O
‚Äù O
respectively O
. O
We O
compare O
our O
classiÔ¨Åcation O
performance O
against O
a O
random O
baseline O
. O
In O
order O
to O
introduce O
a O
stronger O
supervised O
baseline O
for O
our O
methods O
, O
we O
employ O
simpler O
location O
representations O
, O
in O
the O
form O
of O
one O
- O
hot O
vectors O
, O
which O
are O
passed O
as O
input O
in O
our O
supervised O
evaluation O
framework O
( O
Figure O
4 O
) O
. O
We O
take O
the O
mean O
of O
those O
one O
- O
hot O
vectors O
to O
create O
month O
sequence O
vectors O
as O
we O
do O
for O
the O
embeddings O
. O
7.1 O
Surface O
Level O
Location O
Tasks O
For O
these O
tasks O
, O
we O
include O
an O
overall O
average O
baseline O
, O
where O
we O
compute O
the O
metric O
for O
all O
locaFigure O
5 O
: O
Results O
on O
surface O
level O
location O
tasks O
. O
tions O
. O
The O
results O
, O
shown O
in O
Figure O
5 O
, O
lead O
to O
two O
unsurprising O
Ô¨Åndings O
: O
text O
- O
based O
methods O
are O
better O
at O
encoding O
functionality O
, O
and O
the O
methods O
rooted O
in O
physical O
location O
are O
better O
at O
encoding O
distance O
. O
The O
results O
are O
somewhat O
skewed O
for O
the O
text O
- O
based O
representations O
such O
as O
‚Äú O
CampusWebsite O
, O
‚Äù O
as O
some O
locations O
share O
a O
single O
page O
; O
however O
, O
this O
effect O
alone O
does O
not O
entirely O
explain O
the O
performance O
of O
that O
model O
on O
the O
functionality O
overlap O
task O
, O
as O
it O
is O
outperformed O
on O
the O
physical O
distance O
task O
. O
One O
fascinating O
result O
is O
that O
the O
Twitter O
embeddings O
offer O
the O
best O
performance O
on O
the O
physical O
distance O
task O
by O
a O
method O
that O
does O
not O
utilize O
physical O
trajectories O
, O
which O
may O
be O
because O
this O
data O
is O
collected O
using O
geotags O
. O
People O
may O
tweet O
as O
they O
move O
between O
buildings O
, O
blurring O
the O
line O
between O
tweets O
in O
adjacent O
locations O
. O
We O
also O
observe O
that O
the O
methods O
that O
account O
for O
physical O
trajectories O
and O
text O
data O
can O
outperform O
those O
that O
use O
only O
text O
data O
; O
this O
is O
especially O
clear O
from O
the O
results O
for O
Loc2V O
- O
Reddit O
, O
which O
show O
stronger O
performance O
than O
Loc2Vec O
andReddit O
individually O
for O
functionality O
overlap O
, O
and O
slightly O
stronger O
performance O
than O
Reddit O
for O
physical O
distance O
. O
This O
demonstrates O
one O
way O
in O
which O
we O
can O
create O
more O
robust O
representations O
of O
locations O
. O
7.2 O
Surface O
Level O
Sequence O
Tasks O
Overall O
, O
we O
note O
that O
all O
of O
our O
methods O
are O
easily O
able O
to O
surpass O
the O
random O
baseline O
. O
However O
, O
when O
it O
comes O
to O
the O
supervised O
one O
- O
hot O
vectorial O
representation O
, O
we O
see O
that O
traditional O
ways O
of O
representing O
text O
are O
able O
to O
best O
encode O
surface O
level O
information O
. O
This O
is O
because O
the O
sparse O
one O
- O
hot O
representation O
explicitly O
encodes O
information O
necessary O
for O
solving O
each O
task O
; O
location O
- O
presence O
is O
denoted O
by O
a O
value O
greater O
than O
one O
for O
the O
particular O
dimension O
, O
and O
functionality O
- O
presence O
is O
denoted O
by O
a O
value O
greater O
than O
one O
for O
various431Loc O
Pres O
Func O
Pres O
Random O
Baseline O
41.0 O
45.0 O
One O
- O
Hot O
Avg O
61.4 O
62.6 O
Loc2V O
54.8 O
55.6 O
Twitter O
56.9 O
57.8 O
Reddit O
56.9 O
58.3 O
Campus O
- O
Website O
55.8 O
57.8 O
Loc2V O
- O
Reddit O
57.9 O
59.7 O
Reddit O
- O
RetroÔ¨Åt O
55.2 O
56.5 O
Table O
3 O
: O
Macro O
F1 O
scores O
( O
% O
) O
on O
surface O
level O
sequence O
tasks O
. O
Task O
# O
Cls O
Inst O
% O
in O
minority O
class O
Class O
Year O
4 O
721 O
22.33 O
Gender O
2 O
714 O
49.44 O
School O
3 O
522 O
9.77 O
Sleep O
2 O
729 O
41.02 O
GPA O
2 O
729 O
38.13 O
All O
Depression O
2 O
729 O
18.93 O
Major O
Depression O
2 O
729 O
11.66 O
Table O
4 O
: O
Class O
balance O
for O
downstream O
tasks O
. O
Instances O
are O
reported O
after O
Ô¨Åltering O
small O
classes O
. O
dimensions O
. O
We O
Ô¨Ånd O
that O
the O
text O
- O
based O
methods O
lead O
to O
stronger O
performance O
, O
as O
compared O
to O
their O
location O
- O
trajectory O
- O
based O
counterpart O
. O
This O
conÔ¨Årms O
that O
the O
superior O
encoding O
of O
functionality O
discussed O
in O
Section O
7.1 O
is O
still O
discernible O
with O
aggregated O
sequence O
vectors O
. O
Among O
all O
of O
our O
proposed O
methods O
, O
the O
concatenation O
of O
trajectory O
- O
embeddings O
and O
text O
- O
based O
embeddings O
( O
Loc2V O
- O
Reddit O
) O
leads O
to O
the O
strongest O
results O
on O
these O
tasks O
. O
The O
results O
on O
both O
tasks O
are O
completely O
unmatched O
by O
the O
other O
methods O
, O
indicating O
that O
the O
additional O
semantic O
information O
from O
concatenation O
leads O
to O
stronger O
representations O
. O
7.3 O
Downstream O
Tasks O
We O
evaluate O
our O
embedding O
methods O
on O
the O
seven O
downstream O
tasks O
introduced O
in O
Section O
5.3 O
: O
class O
year O
, O
gender O
, O
school O
enrollment O
, O
sleep O
satisfaction O
, O
GPA O
, O
all O
depression O
, O
and O
major O
depression O
. O
These O
tasks O
were O
designed O
to O
demonstrate O
the O
utility O
of O
various O
location O
representations O
in O
predicting O
a O
diverse O
set O
of O
attributes O
. O
The O
overall O
results O
for O
each O
model O
are O
listed O
in O
Table O
5 O
; O
we O
use O
macro O
F1 O
score O
as O
our O
metric O
. O
Table O
4 O
shows O
the O
size O
of O
the O
minority O
class O
for O
each O
task O
. O
This O
imbalance O
and O
ourrelatively O
small O
data O
size O
made O
it O
challenging O
to O
achieve O
strong O
results O
on O
some O
tasks O
, O
although O
we O
generally O
were O
able O
to O
improve O
upon O
the O
baselines O
. O
Across O
all O
the O
tasks O
, O
predicting O
depression O
has O
the O
most O
potential O
for O
real O
- O
world O
impact O
, O
but O
also O
showcases O
the O
most O
imbalanced O
data O
distribution O
. O
With O
more O
data O
, O
we O
believe O
that O
patterns O
could O
be O
learned O
in O
a O
more O
robust O
way O
. O
For O
the O
task O
of O
school O
prediction O
, O
we O
greatly O
improve O
upon O
the O
random O
baseline O
even O
though O
the O
data O
is O
very O
imbalanced O
; O
this O
could O
be O
because O
this O
attribute O
is O
clearly O
linked O
to O
where O
people O
go O
on O
campus O
, O
as O
is O
class O
year O
. O
For O
example O
, O
freshmen O
typically O
live O
in O
dorms O
and O
eat O
in O
dining O
halls O
, O
while O
seniors O
often O
live O
off O
campus O
; O
computer O
science O
students O
attend O
classes O
in O
different O
places O
than O
English O
students O
. O
The O
strong O
performance O
on O
the O
gender O
prediction O
task O
may O
be O
explained O
by O
the O
real O
- O
world O
bias O
entailed O
in O
the O
school O
of O
enrollment O
; O
e.g. O
, O
fewer O
women O
are O
enrolled O
in O
engineering O
, O
so O
they O
are O
less O
likely O
to O
visit O
engineering O
buildings O
. O
The O
strong O
performance O
on O
predicting O
class O
year O
with O
one O
- O
hot O
encodings O
can O
be O
directly O
linked O
to O
the O
surface O
level O
task O
improvement O
: O
freshmen O
are O
more O
likely O
to O
visit O
certain O
types O
of O
locations O
like O
dorms O
( O
functionalitypresence O
) O
; O
performance O
is O
best O
among O
freshmen O
. O
Among O
text O
- O
based O
methods O
, O
we O
see O
that O
the O
Reddit O
embeddings O
enable O
the O
best O
performance O
on O
most O
downstream O
tasks O
. O
Reddit O
contains O
the O
most O
expressive O
language O
compared O
to O
the O
other O
venues O
, O
because O
its O
users O
are O
able O
to O
write O
at O
length O
without O
a O
strict O
character O
limit O
or O
other O
formalities O
imposed O
by O
media O
such O
as O
Twitter O
. O
Furthermore O
, O
from O
manually O
examining O
a O
sample O
of O
the O
posts O
, O
the O
community O
seems O
to O
primarily O
encompass O
current O
and O
former O
undergraduate O
students O
, O
therefore O
establishing O
a O
community O
that O
is O
above O
all O
else O
a O
place O
for O
students O
to O
share O
and O
discuss O
their O
daily O
lives O
. O
Meanwhile O
, O
the O
tweets O
that O
we O
link O
to O
locations O
may O
encompass O
musings O
from O
faculty O
or O
visiting O
scholars O
, O
and O
brief O
statements O
that O
are O
unrelated O
to O
campus O
life O
. O
The O
campus O
website O
data O
is O
the O
furthest O
from O
the O
student O
experience O
, O
as O
it O
is O
devoid O
of O
any O
dynamic O
content O
, O
written O
in O
the O
dry O
format O
of O
informational O
style O
. O
As O
a O
result O
, O
it O
seems O
intuitive O
that O
Reddit O
, O
in O
addition O
to O
providing O
definitional O
information O
about O
locations O
( O
e.g. O
, O
there O
are O
many O
posts O
comparing O
and O
discussing O
dormitories O
) O
, O
also O
provides O
student O
‚Äôs O
emotional O
perspectives O
on O
them O
. O
We O
hypothesize O
that O
this O
closeness O
to O
student O
thoughts O
and O
feelings O
is O
what O
yields O
bet-432Depression O
Class O
Year O
Gender O
School O
Sleep O
GPA O
All O
Major O
Random O
Baseline O
25.0 O
50.0 O
30.0 O
50.0 O
49.0 O
45.0 O
41.0 O
One O
- O
Hot O
Avg O
52.1 O
56.8 O
61.8 O
49.4 O
51.8 O
48.2 O
46.6 O
Loc2V O
50.8 O
61.0 O
62.0 O
52.9 O
51.9 O
49.6 O
43.6 O
Twitter O
49.4 O
57.4 O
65.4 O
49.3 O
51.9 O
48.5 O
44.8 O
Reddit O
50.2 O
59.8 O
66.3 O
52.7 O
49.1 O
50.5 O
47.7 O
Campus O
- O
Website O
48.8 O
58.1 O
60.1 O
46.4 O
51.9 O
49.4 O
42.9 O
Loc2V O
- O
Reddit O
50.3 O
59.4 O
64.5 O
53.7 O
52.7 O
50.8 O
44.7 O
Reddit O
- O
RetroÔ¨Åt O
50.2 O
60.8 O
66.0 O
52.6 O
47.7 O
48.7 O
39.6 O
Table O
5 O
: O
Macro O
F1 O
scores O
( O
% O
) O
on O
downstream O
tasks O
. O
ter O
performance O
when O
predicting O
student O
attributes O
, O
compared O
to O
the O
other O
text O
- O
based O
methods O
. O
Overall O
, O
while O
results O
vary O
between O
different O
tasks O
, O
we O
Ô¨Ånd O
that O
a O
method O
that O
accounts O
for O
both O
physical O
location O
trajectories O
and O
text O
data O
describing O
locations O
( O
Loc2V O
- O
Reddit O
) O
has O
a O
strong O
overall O
performance O
. O
Notably O
, O
it O
is O
the O
best O
performing O
model O
on O
three O
tasks O
and O
achieves O
large O
improvements O
over O
the O
supervised O
baseline O
on O
two O
additional O
tasks O
. O
Such O
a O
model O
should O
be O
considered O
in O
future O
work O
on O
location O
embeddings O
because O
of O
its O
robustness O
on O
varied O
tasks O
. O
8 O
Conclusions O
In O
this O
paper O
, O
we O
addressed O
the O
task O
of O
building O
and O
probing O
location O
embeddings O
. O
We O
investigated O
several O
strategies O
to O
construct O
them O
, O
as O
well O
as O
a O
suite O
of O
probing O
tasks O
to O
understand O
the O
type O
of O
information O
encoded O
within O
. O
First O
, O
we O
showed O
that O
while O
all O
embedding O
methods O
encode O
both O
physical O
distance O
and O
functionality O
, O
methods O
using O
trajectories O
yield O
better O
spatial O
representations O
and O
methods O
using O
text O
data O
better O
encode O
location O
functionality O
. O
We O
showed O
that O
, O
like O
in O
the O
case O
of O
sentence O
embeddings O
from O
natural O
language O
, O
sequence O
embeddings O
of O
location O
data O
are O
able O
to O
encode O
surface O
level O
information O
( O
location O
- O
presence O
, O
and O
functionalitypresence O
) O
, O
as O
well O
as O
information O
that O
can O
be O
effectively O
used O
in O
downstream O
tasks O
. O
Overall O
, O
we O
found O
that O
an O
embedding O
model O
that O
accounts O
for O
both O
location O
trajectories O
and O
text O
related O
to O
locations O
( O
Loc2V O
- O
Reddit O
) O
gives O
the O
best O
performance O
over O
a O
diverse O
range O
of O
downstream O
tasks O
, O
from O
prediction O
of O
depression O
or O
sleep O
to O
prediction O
of O
academic O
area O
of O
study O
. O
Importantly O
, O
we O
also O
found O
that O
embeddings O
of O
locations O
tend O
to O
underperform O
more O
traditional O
one O
- O
hot O
encodings O
on O
surface O
- O
level O
tasks O
, O
yet O
they O
generally O
outperform O
these O
representations O
on O
downstream O
tasks O
. O
This O
suggests O
that O
while O
such O
embeddings O
do O
not O
explicitly O
record O
distinct O
locations O
that O
people O
visit O
( O
thus O
being O
more O
privacy O
preserving O
and O
counteracting O
negative O
actions O
like O
stalking O
) O
, O
they O
may O
be O
more O
effective O
for O
downstream O
applications O
that O
can O
yield O
positive O
outcomes O
, O
such O
as O
population O
- O
level O
mental O
health O
tracking O
or O
opt O
- O
in O
tracking O
for O
individuals O
who O
are O
in O
therapy O
. O
Our O
code O
is O
publicly O
available O
at O
http://lit O
. O
eecs.umich.edu/downloads.html O
. O
Acknowledgment O
We O
are O
grateful O
to O
Shiyu O
Qu O
, O
Jiaxin O
Ye O
, O
and O
Xinyi O
Zheng O
for O
assisting O
us O
with O
this O
study O
. O
This O
material O
is O
based O
in O
part O
upon O
work O
supported O
by O
the O
Precision O
Health O
initiative O
at O
the O
University O
of O
Michigan O
, O
by O
the O
Michigan O
Institute O
for O
Data O
Science O
( O
MIDAS O
) O
, O
by O
the O
National O
Science O
Foundation O
( O
grant O
# O
1815291 O
) O
, O
and O
by O
the O
John O
Templeton O
Foundation O
( O
grant O
# O
61156 O
) O
. O
Any O
opinions O
, O
Ô¨Åndings O
, O
and O
conclusions O
or O
recommendations O
expressed O
in O
this O
material O
are O
those O
of O
the O
author O
and O
do O
not O
necessarily O
reÔ¨Çect O
the O
views O
of O
the O
Precision O
Health O
initiative O
, O
MIDAS O
, O
the O
National O
Science O
Foundation O
, O
or O
John O
Templeton O
Foundation O
. O
Abstract O
Pairwise O
data O
automatically O
constructed O
from O
weakly O
supervised O
signals O
has O
been O
widely O
used O
for O
training O
deep O
learning O
models O
. O
Pairwise O
datasets O
such O
as O
parallel O
texts O
can O
have O
uneven O
quality O
levels O
overall O
, O
but O
usually O
contain O
data O
subsets O
that O
are O
more O
useful O
as O
learning O
examples O
. O
We O
present O
two O
methods O
to O
reÔ¨Åne O
data O
that O
are O
aimed O
at O
obtaining O
that O
kind O
of O
subsets O
in O
a O
self O
- O
supervised O
way O
. O
Our O
methods O
are O
based O
on O
iteratively O
training O
dualencoder O
models O
to O
compute O
similarity O
scores O
. O
We O
evaluate O
our O
methods O
on O
de O
- O
noising O
parallel O
texts O
and O
training O
neural O
machine O
translation O
models O
. O
We O
Ô¨Ånd O
that O
: O
( O
i O
) O
The O
self O
- O
supervised O
reÔ¨Ånement O
achieves O
most O
machine O
translation O
gains O
in O
the O
Ô¨Årst O
iteration O
, O
but O
following O
iterations O
further O
improve O
its O
intrinsic O
evaluation O
. O
( O
ii O
) O
Machine O
translations O
can O
improve O
the O
de O
- O
noising O
performance O
when O
combined O
with O
selection O
steps O
. O
( O
iii O
) O
Our O
methods O
are O
able O
to O
reach O
the O
performance O
of O
a O
supervised O
method O
. O
Being O
entirely O
self O
- O
supervised O
, O
our O
methods O
are O
well O
- O
suited O
to O
handle O
pairwise O
data O
without O
the O
need O
of O
prior O
knowledge O
or O
human O
annotations O
. O
1 O
Introduction O
Deep O
learning O
models O
are O
widely O
adopted O
and O
have O
demonstrated O
their O
usefulness O
in O
many O
areas O
and O
applications O
. O
Despite O
their O
diversity O
, O
one O
common O
characteristic O
of O
these O
models O
is O
the O
large O
number O
of O
parameters O
that O
need O
to O
be O
adjusted O
during O
training O
( O
some O
recent O
models O
that O
have O
billions O
of O
parameters O
include O
T5 O
( O
Raffel O
et O
al O
. O
, O
2019 O
) O
and O
GPT2 O
( O
Radford O
et O
al O
. O
, O
2019 O
) O
) O
. O
This O
leads O
to O
the O
need O
of O
collecting O
large O
amounts O
of O
training O
examples O
. O
Pairwise O
data O
, O
that O
captures O
the O
relationship O
in O
two O
modalities O
, O
is O
used O
to O
train O
deep O
learning O
models O
such O
as O
Neural O
Machine O
Translation O
( O
NMT O
) O
( O
Wu O
et O
al O
. O
, O
2016 O
) O
, O
Question O
Answering O
( O
Wang O
et O
al O
. O
, O
2007 O
) O
, O
Image O
Captioning O
( O
Sharma O
et O
al O
. O
, O
2018 O
) O
, O
etc O
. O
To O
train O
this O
kind O
of O
models O
, O
large O
- O
scale O
data O
can O
often O
be O
obtained O
from O
weak O
signals O
like O
text O
cooccurrence O
( O
Yang O
et O
al O
. O
, O
2018 O
) O
or O
dictionary O
n O
- O
gram O
matching O
( O
Uszkoreit O
et O
al O
. O
, O
2010 O
) O
. O
For O
example O
, O
in O
the O
machine O
translation O
community O
, O
the O
large O
amount O
of O
multilingual O
text O
available O
on O
the O
internet O
has O
naturally O
led O
to O
the O
idea O
of O
using O
internet O
data O
to O
train O
NMT O
models O
( O
Resnik O
, O
1999 O
) O
. O
This O
approach O
has O
proven O
advantageous O
but O
it O
has O
the O
drawback O
that O
data O
mined O
this O
way O
is O
intrinsically O
noisy O
( O
Resnik O
and O
Smith O
, O
2003 O
) O
. O
Despite O
the O
poor O
quality O
, O
usually O
this O
kind O
of O
data O
contains O
a O
helpful O
subset O
that O
can O
be O
recovered O
through O
a O
process O
of O
data O
cleaning O
or O
reÔ¨Ånement O
. O
Data O
cleaning O
could O
be O
implemented O
with O
linguistic O
knowledge O
such O
as O
its O
script O
, O
vocabulary O
, O
syntax O
, O
etc O
. O
Alternatively O
, O
a O
model O
can O
be O
trained O
on O
‚Äú O
clean O
‚Äù O
or O
‚Äú O
trusted O
‚Äù O
pairs O
that O
are O
veriÔ¨Åed O
through O
manual O
annotation O
. O
Both O
options O
can O
be O
highly O
effective O
, O
but O
the O
former O
is O
limited O
in O
scope O
and O
error O
- O
prone O
, O
while O
the O
latter O
can O
be O
costly O
due O
to O
the O
number O
of O
required O
annotated O
examples O
. O
In O
this O
paper O
we O
introduce O
two O
self O
- O
supervised O
methods O
to O
obtain O
data O
subsets O
from O
noisy O
pairwise O
data O
that O
can O
be O
helpful O
to O
train O
dual O
- O
encoder O
( O
D O
- O
E O
) O
and O
neural O
machine O
translation O
( O
NMT O
) O
models O
. O
As O
noisy O
pairwise O
data O
, O
in O
our O
experiments O
we O
use O
parallel O
texts O
mined O
from O
the O
internet O
. O
Our O
methods O
do O
not O
require O
external O
knowledge O
( O
e.g. O
syntactic O
rules O
) O
, O
language O
- O
dependent O
heuristics O
( O
e.g. O
script O
veriÔ¨Åcation O
) O
or O
synthetic O
positive O
or O
negative O
training O
examples O
. O
By O
eliminating O
the O
need O
of O
annotations O
, O
our O
methods O
directly O
address O
the O
data O
labelling O
bottleneck O
. O
Our O
methods O
employ O
D O
- O
E O
models O
( O
Gillick O
et O
al O
. O
, O
2018 O
) O
to O
learn O
a O
shared O
embedded O
space O
from O
the O
co O
- O
located O
text O
in O
the O
sentence O
pairs O
mined O
from O
the O
internet O
. O
Following O
Chidambaram O
et O
al O
. O
( O
2018 O
) O
we O
use O
the O
embedding O
distance O
in O
the O
learned O
space O
as O
a O
measure O
of O
cross O
- O
lingual O
similarity O
between O
sentences O
. O
Our O
hypothesis O
is O
that435if O
higher O
scores O
are O
associated O
with O
cross O
- O
lingual O
similarity O
, O
pairs O
with O
higher O
scores O
will O
be O
closer O
to O
be O
actual O
translations O
of O
each O
other O
and O
, O
in O
that O
case O
, O
may O
be O
part O
of O
the O
data O
subset O
useful O
to O
train O
the O
models O
. O
In O
our O
experiments O
, O
our O
methods O
show O
effective O
reÔ¨Åning O
parallel O
texts O
mined O
from O
the O
internet O
. O
Much O
of O
the O
gains O
in O
the O
downstream O
evaluation O
are O
achieved O
in O
the O
Ô¨Årst O
iteration O
of O
the O
method O
, O
but O
later O
iterations O
keep O
improving O
the O
D O
- O
E O
models O
. O
Despite O
being O
self O
- O
supervised O
, O
our O
methods O
show O
competitive O
performance O
when O
compared O
against O
a O
de O
- O
noising O
method O
that O
uses O
supervision O
. O
2 O
Related O
Work O
One O
line O
of O
the O
research O
that O
directly O
relates O
to O
our O
work O
is O
corpus O
Ô¨Åltering O
for O
training O
NMT O
models O
. O
Below O
we O
classify O
the O
related O
work O
into O
two O
categories O
depending O
on O
the O
amount O
of O
supervision O
needed O
( O
e.g. O
high O
quality O
parallel O
texts O
) O
. O
( O
Semi-)Supervised O
Methods O
Some O
data O
denoising O
methods O
simply O
use O
Ô¨Åltering O
rules O
or O
heuristics O
such O
as O
language O
identiÔ¨Åcation O
of O
both O
the O
source O
and O
target O
texts O
, O
vocabulary O
checks O
, O
language O
model O
( O
syntactic O
) O
veriÔ¨Åcation O
, O
and O
so O
on O
. O
In O
contrast O
to O
rule O
- O
based O
approaches O
, O
approaches O
like O
Chen O
and O
Huang O
( O
2016 O
) O
and O
Wang O
et O
al O
. O
( O
2018c O
) O
train O
classiÔ¨Åers O
to O
distinguish O
in O
- O
domain O
vs. O
outof O
- O
domain O
( O
or O
clean O
vs. O
noisy O
) O
data O
with O
a O
small O
parallel O
corpus O
, O
while O
other O
approaches O
build O
reference O
models O
on O
larger O
amounts O
of O
high O
- O
quality O
data O
( O
Junczys O
- O
Dowmunt O
, O
2018 O
; O
Defauw O
et O
al O
. O
, O
2019 O
) O
. O
There O
are O
approaches O
that O
combine O
rules O
and O
heuristics O
with O
probabilistic O
models O
to O
determine O
the O
amount O
of O
noise O
in O
each O
sentence O
pair O
. O
In O
some O
cases O
these O
systems O
are O
designed O
as O
targeted O
efforts O
to O
denoise O
a O
particular O
dataset O
. O
Bicleaner O
( O
S¬¥anchez O
- O
Cartagena O
et O
al O
. O
, O
2018 O
) O
, O
in O
relationship O
to O
the O
ParaCrawl O
( O
Espl O
` O
a O
et O
al O
. O
, O
2019 O
) O
data O
, O
is O
an O
example O
of O
that O
approach O
. O
Unsupervised O
Methods O
In O
contrast O
to O
the O
supervised O
methods O
, O
unsupervised O
methods O
do O
not O
require O
good O
- O
quality O
data O
to O
be O
available O
. O
Recent O
work O
( O
Zhang O
et O
al O
. O
, O
2020 O
) O
leverages O
pre O
- O
trained O
language O
models O
and O
synthetic O
data O
( O
Vyas O
et O
al O
. O
, O
2018 O
) O
, O
in O
place O
of O
true O
supervision O
. O
Some O
efforts O
focus O
on O
using O
monolingual O
corpora O
and O
align O
them O
through O
bootstrapping O
in O
order O
to O
generate O
sentence O
pairs O
( O
Tran O
et O
al O
. O
, O
2020 O
; O
Ruiter O
et O
al O
. O
, O
2020 O
) O
, O
while O
others O
train O
a O
model O
with O
noisy O
data O
directly O
to O
gen O
- O
erate O
embeddings O
and O
score O
the O
data O
( O
Chaudhary O
et O
al O
. O
, O
2019 O
) O
. O
Wang O
et O
al O
. O
( O
2018b O
) O
use O
twoNMT O
models O
taken O
from O
two O
training O
epochs O
to O
decide O
which O
data O
to O
use O
in O
order O
to O
improve O
the O
training O
efÔ¨Åciency O
and O
to O
show O
a O
de O
- O
noising O
effect O
. O
Our O
methods O
here O
try O
to O
take O
advantages O
of O
all O
of O
these O
approaches O
. O
Koehn O
et O
al O
. O
( O
2018 O
) O
and O
Koehn O
et O
al O
. O
( O
2019 O
) O
summarize O
Ô¨Åndings O
of O
the O
WMT O
corpus O
Ô¨Åltering O
efforts O
, O
though O
our O
work O
here O
primarily O
examines O
a O
self O
- O
supervised O
method O
in O
the O
context O
of O
de O
- O
noising O
, O
rather O
than O
on O
a O
targeted O
Ô¨Åltering O
effort O
. O
Our O
methods O
are O
unsupervised O
. O
We O
use O
dualencoder O
models O
, O
rather O
than O
an O
encoder O
- O
decoder O
architecture O
, O
to O
model O
pairwise O
data O
and O
let O
the O
model O
self O
- O
supervise O
itself O
or O
, O
further O
, O
be O
co O
- O
trained O
with O
an O
NMT O
model O
to O
reÔ¨Åne O
the O
training O
data O
. O
3 O
Dual O
- O
Encoder O
Model O
Dual O
- O
encoder O
( O
D O
- O
E O
) O
models O
have O
demonstrated O
to O
be O
an O
effective O
learning O
framework O
applied O
to O
both O
supervised O
( O
Henderson O
et O
al O
. O
, O
2017 O
; O
Gillick O
et O
al O
. O
, O
2019 O
) O
and O
unsupervised O
tasks O
( O
Cer O
et O
al O
. O
, O
2018 O
; O
Chidambaram O
et O
al O
. O
, O
2018 O
) O
. O
A O
multi O
- O
task O
D O
- O
E O
model O
consists O
of O
two O
encoders O
and O
a O
combination O
function O
for O
each O
of O
the O
tasks O
. O
In O
the O
context O
of O
the O
D O
- O
E O
framework O
, O
the O
selection O
of O
bilingual O
text O
can O
be O
interpreted O
as O
a O
ranking O
problem O
where O
, O
with O
yias O
the O
true O
target O
of O
source O
sentence O
xi O
, O
P(yi|xi)is O
ranked O
above O
all O
the O
other O
target O
candidates O
in O
Y. O
P(yi|xi)can O
be O
expressed O
as O
a O
log O
- O
linear O
model O
but O
, O
for O
practical O
reasons O
, O
we O
approximate O
the O
full O
set O
of O
target O
candidates O
Ywith O
a O
sample O
( O
Henderson O
et O
al O
. O
, O
2017 O
) O
. O
When O
training O
in O
a O
batch O
, O
P(yi|xi O
) O
can O
be O
approximated O
as O
: O
P(yi|xi)‚âàeœÜ(xi O
, O
yi O
) O
eœÜ(xi O
, O
yi)+ O
Œ£N O
n=1,n O
/negationslash O
= O
ieœÜ(xi O
, O
yn)(1 O
) O
whereNis O
the O
size O
of O
a O
batch O
and O
œÜis O
a O
similarity O
function O
. O
In O
such O
a O
way O
, O
model O
training O
can O
be O
done O
by O
optimizing O
a O
log O
- O
likelihood O
loss O
function O
: O
L= O
‚àí1 O
N O
/ O
summationtextN O
i=1logeœÜ(xi O
, O
yi O
) O
eœÜ(xi O
, O
yi)+/summationtextN O
n=1,n O
/ O
negationslash O
= O
ieœÜ(xi O
, O
yn)(2 O
) O
Based O
on O
the O
results O
of O
Yang O
et O
al O
. O
( O
2019a O
) O
with O
additive O
margin O
softmax O
( O
Wang O
et O
al O
. O
, O
2018a O
) O
, O
we O
modify O
our O
loss O
function O
to O
include O
margin O
m:436Source O
  O
Encoder O
  O
Source O
  O
Negatives O
  O
Target O
  O
Encoder O
   O
Target O
  O
Target O
  O
Encoder O
  O
Target O
  O
Negatives O
0.010.02 O
... O
0.01 O
0.010.02 O
... O
0.01 O
... O
... O
... O
... O
0.2 O
0.01 O
... O
0.010.1 O
0.01 O
... O
0.05 O
0.020.1 O
... O
0.02 O
... O
... O
... O
... O
0.1 O
0.06 O
... O
0.01 O
Batched O
  O
Input O
  O
Additive O
Margin O
  O
Softmax O
  O
Dual O
  O
Encoder O
  O
Source O
  O
Encoder O
  O
Source O
0.6 O
0.1 O
... O
0.01 O
0.010.8 O
... O
0.01 O
... O
... O
... O
... O
0.2 O
0.01 O
... O
0.7Figure O
1 O
: O
D O
- O
E O
model O
training O
with O
hard O
negatives O
. O
The O
encoders O
with O
the O
same O
color O
share O
parameters O
. O
The O
dot O
product O
scoring O
function O
makes O
it O
easy O
to O
compute O
pairwise O
scores O
by O
doing O
matrix O
multiplications O
. O
The O
highlighted O
diagonal O
indicates O
the O
dot O
products O
of O
the O
source O
and O
target O
texts O
. O
The O
additive O
margin O
softmax O
is O
applied O
at O
every O
row O
( O
source‚Üítarget O
) O
and O
column O
( O
target O
‚Üísource O
) O
. O
Lams= O
‚àí1 O
N O
/ O
summationtextN O
i=1logeœÜ(xi O
, O
yi)‚àím O
eœÜ(xi O
, O
yi)‚àím+/summationtextN O
n=1,n O
/ O
negationslash O
= O
ieœÜ(xi O
, O
yn)(3 O
) O
When O
using O
the O
dot O
product O
as O
similarity O
functionœÜ O
, O
a O
single O
matrix O
multiplication O
can O
be O
used O
to O
efÔ¨Åciently O
compute O
scores O
for O
all O
the O
examples O
in O
the O
batch O
. O
When O
set O
to O
learn O
from O
clean O
crosslingual O
paired O
texts O
, O
a O
D O
- O
E O
model O
can O
be O
used O
to O
learn O
strong O
cross O
- O
lingual O
embeddings O
for O
bitext O
retrieval O
as O
shown O
in O
Guo O
et O
al O
. O
( O
2018 O
) O
and O
Yang O
et O
al O
. O
( O
2019a O
) O
. O
The O
challenge O
is O
to O
learn O
similar O
embeddings O
when O
training O
D O
- O
E O
models O
on O
noisy O
data O
. O
3.1 O
Model O
ConÔ¨Åguration O
In O
our O
experiments O
we O
use O
D O
- O
E O
models O
with O
hard O
negatives O
sampling O
( O
Guo O
et O
al O
. O
, O
2018 O
) O
. O
Similar O
to O
Yang O
et O
al O
. O
( O
2019a O
) O
, O
our O
models O
are O
trained O
bidirectional O
so O
the O
rankings O
in O
both O
directions O
, O
source O
to O
target O
and O
target O
to O
source O
, O
are O
optimized O
. O
But O
in O
contrast O
to O
Yang O
et O
al O
. O
( O
2019a O
) O
we O
do O
not O
share O
the O
parameters O
between O
the O
source O
and O
target O
encoders O
. O
In O
our O
initial O
experiments O
training O
NMT O
models O
we O
found O
that O
, O
under O
noisy O
conditions O
, O
there O
is O
improvement O
of O
close O
to O
1 O
BLEU O
point O
when O
using O
D O
- O
E O
models O
that O
use O
speciÔ¨Åc O
encoders O
for O
each O
language O
. O
Figure O
1 O
illustrates O
our O
training O
approach O
. O
For O
our O
encoders O
we O
use O
3 O
- O
layer O
transformers O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
in O
the O
encoders O
with O
hidden O
layers O
of O
size O
512 O
and O
8 O
attention O
heads O
. O
We O
build O
vocabularies O
for O
each O
language O
separately O
. O
Given O
the O
noise O
in O
the O
data O
, O
the O
vocabulariesmight O
not O
include O
all O
words O
in O
the O
source O
or O
target O
languages O
. O
We O
control O
the O
prevalence O
of O
words O
in O
the O
expected O
language O
with O
the O
vocabulary O
size O
. O
Our O
reasoning O
is O
that O
large O
vocabularies O
are O
more O
likely O
to O
include O
words O
in O
languages O
other O
than O
the O
expected O
. O
200k O
most O
frequent O
words O
are O
used O
and O
200k O
extra O
buckets O
are O
reserved O
for O
the O
outof O
- O
vocabulary O
words O
found O
in O
the O
training O
. O
We O
use O
character- O
and O
word O
- O
level O
features O
to O
model O
the O
source O
and O
target O
inputs O
. O
For O
character O
- O
level O
representations O
, O
we O
decompose O
each O
word O
into O
all O
character O
n O
- O
grams O
within O
a O
range O
. O
For O
word O
- O
level O
representation O
, O
we O
sum O
the O
embeddings O
for O
its O
character O
n O
- O
grams O
and O
its O
word O
embedding O
. O
The O
Ô¨Ånal O
sentence O
representation O
is O
the O
output O
of O
the O
transformer O
layers O
as O
a O
500 O
- O
dimensional O
vector O
. O
We O
train O
the O
D O
- O
E O
models O
using O
SGD O
for O
40 O
M O
steps O
with O
a O
learning O
rate O
of O
0.001 O
. O
A O
Ô¨Åxed O
value O
of O
margin O
0.2 O
is O
used O
in O
equation O
3 O
. O
4 O
Our O
Approach O
: O
Self O
- O
Supervised O
Learning O
for O
Data O
ReÔ¨Ånement O
4.1 O
Training O
with O
Hard O
Negatives O
As O
described O
in O
equation O
1 O
, O
and O
illustrated O
in O
Ô¨Ågure O
1 O
, O
for O
every O
source O
sentence O
we O
use O
all O
target O
sentences O
, O
except O
its O
own O
, O
as O
negatives O
in O
a O
batch O
. O
We O
also O
augment O
the O
batch O
with O
hard O
negatives O
to O
improve O
the O
contrast O
between O
true O
translations O
and O
any O
other O
random O
sentence O
pairing O
. O
We O
mine O
the O
hard O
negatives O
using O
a O
separate O
D O
- O
E O
model O
to O
retrieve O
, O
for O
every O
sentence O
, O
the O
top O
Ncandidates O
that O
are O
not O
its O
counterpart O
in O
the O
pair O
. O
It O
is O
important437Initial O
Dual O
  O
Encoder O
  O
Model O
Noisy O
data O
  O
Score O
, O
Rank O
, O
  O
Select O
  O
Selected O
  O
data O
Train O
Dual O
  O
Encoder O
  O
Dual O
Encoder O
  O
Model O
Train O
Initial O
  O
Dual O
Encoder O
( O
a O
) O
Iterative O
Ô¨Åltering O
. O
Score O
, O
rank O
, O
  O
select O
Selected O
data O
Train O
NMT O
NMT O
Model O
Initial O
NMT O
  O
Model O
Noisy O
data O
  O
Translate O
  O
Translated O
  O
data O
Train O
Dual O
  O
Encoder O
  O
Dual O
Encoder O
  O
Model O
Train O
Initial O
  O
NMT O
( O
b O
) O
Machine O
translation O
iterative O
Ô¨Åltering O
. O
Figure O
2 O
: O
[ O
Iterative O
Ô¨Åltering O
( O
IF O
) O
] O
: O
the O
scores O
of O
the O
dual O
- O
encoder O
are O
used O
to O
select O
the O
training O
material O
for O
the O
next O
model O
. O
[ O
Machine O
translation O
iterative O
Ô¨Åltering O
( O
MT O
- O
IF O
) O
] O
: O
The O
D O
- O
E O
model O
is O
used O
to O
score O
the O
forwardtranslations O
from O
the O
NMT O
model O
, O
only O
the O
top O
- O
ranking O
sentence O
pairs O
are O
used O
to O
train O
the O
next O
NMT O
model O
. O
to O
notice O
that O
the O
hard O
negatives O
in O
our O
method O
are O
retrieved O
, O
not O
generated O
or O
synthesized O
. O
We O
mine O
the O
hard O
negatives O
ofÔ¨Çine O
from O
the O
sentences O
in O
the O
ParaCrawl O
v1.0 O
data O
, O
or O
from O
the O
translations O
only O
when O
using O
translations O
as O
target O
sentences O
. O
Our O
negative O
- O
mining O
D O
- O
E O
has O
DNN O
layers O
, O
instead O
of O
transformer O
ones O
, O
with O
a O
reduced O
embedding O
size O
( O
25 O
- O
dimensional O
) O
. O
We O
mine O
hard O
negatives O
for O
both O
the O
source O
and O
target O
sentences O
. O
As O
shown O
in O
Ô¨Ågure O
1 O
, O
the O
hard O
negatives O
are O
speciÔ¨Åc O
to O
each O
one O
of O
the O
sentence O
pairs O
but O
, O
when O
added O
to O
the O
batch O
, O
we O
use O
them O
as O
additional O
random O
negatives O
for O
all O
the O
other O
source O
sentences O
in O
the O
batch O
. O
We O
use O
a O
batch O
size O
of O
128 O
examples O
and O
5 O
hard O
- O
negatives O
per O
example O
. O
We O
augment O
the O
batch O
row O
- O
wise O
with O
hard O
negatives O
mined O
for O
the O
target O
sentences O
, O
and O
column O
- O
wise O
with O
hard O
negatives O
for O
the O
source O
. O
In O
our O
self O
- O
supervised O
approach O
, O
we O
train O
D O
- O
E O
models O
with O
one O
dataset O
and O
use O
the O
models O
that O
we O
train O
to O
score O
the O
same O
data O
. O
Our O
hypothesis O
is O
that O
the O
scores O
are O
useful O
to O
rank O
the O
data O
in O
a O
way O
that O
makes O
it O
easy O
to O
Ô¨Ålter O
out O
the O
noise O
. O
It O
is O
natural O
to O
believe O
that O
, O
in O
principle O
, O
a O
data O
- O
model O
cycle O
like O
this O
may O
not O
lead O
to O
much O
improvement O
because O
the O
trained O
models O
tend O
memorize O
the O
training O
data O
, O
including O
the O
noise O
. O
We O
break O
this O
cycle O
by O
adding O
a O
selection O
step O
to O
the O
process O
and O
avoiding O
to O
train O
the O
models O
with O
the O
same O
examples O
all O
the O
time O
. O
We O
propose O
a O
self O
- O
supervised O
method O
for O
pairwise O
data O
reÔ¨Ånement O
based O
on O
data O
‚Äú O
iterative O
Ô¨Åltering O
‚Äù O
( O
IF O
) O
. O
With O
this O
method O
we O
reÔ¨Åne O
data O
that O
we O
use O
to O
train O
NMT O
models O
. O
By O
including O
the O
downstream O
task O
in O
our O
method O
, O
we O
formulate O
a O
second O
method O
asan O
extension O
of O
the O
Ô¨Årst O
one O
. O
We O
regard O
this O
second O
method O
as O
‚Äú O
machine O
translation O
- O
iterative O
Ô¨Åltering O
‚Äù O
( O
MT O
- O
IF O
) O
. O
Both O
methods O
are O
illustrated O
in O
Ô¨Ågure O
2 O
. O
4.2 O
Iterative O
Filtering O
We O
use O
the O
dot O
product O
between O
source O
and O
target O
embeddings O
as O
proxy O
of O
cross O
- O
lingual O
similarity O
. O
Once O
we O
score O
and O
select O
data O
to O
train O
one O
model O
, O
we O
can O
use O
that O
model O
to O
score O
and O
select O
data O
for O
the O
next O
one O
in O
an O
iterative O
way O
. O
The O
details O
of O
this O
method O
are O
shown O
in O
Ô¨Ågure O
2a O
and O
explained O
in O
algorithm O
1 O
. O
We O
bootstrap O
this O
method O
by O
training O
an O
initial O
D O
- O
E O
model O
with O
all O
the O
pre-Ô¨Åltered O
data O
. O
It O
is O
important O
to O
notice O
that O
in O
each O
iteration O
we O
train O
the O
D O
- O
E O
model O
with O
a O
subset O
of O
the O
data O
( O
the O
selected O
data O
) O
, O
but O
we O
score O
the O
entire O
set O
. O
This O
allows O
the O
method O
to O
recover O
useful O
data O
that O
may O
have O
been O
discarded O
in O
earlier O
iterations O
. O
Algorithm O
1 O
Iterative O
Ô¨Åltering O
1 O
: O
œÑ‚Üêselection O
threshold O
2 O
:D O
- O
E O
= O
TrainDualEncoder(data O
) O
3 O
: O
while O
D O
- O
E O
improves O
do O
4 O
: O
scored O
data O
= O
Score(data O
; O
D O
- O
E O
) O
5 O
: O
ranked O
data O
= O
Rank(scored O
data O
) O
6 O
: O
selected O
data O
= O
Select(ranked O
data O
; O
œÑ O
) O
7 O
: O
D O
- O
E O
= O
TrainDualEncoder(selected O
data O
) O
8 O
: O
end O
while4384.3 O
Machine O
Translation O
Iterative O
Filtering O
In O
this O
method O
, O
the O
D O
- O
E O
model O
selects O
data O
to O
train O
an O
NMT O
model O
, O
rather O
than O
to O
train O
another O
D O
- O
E O
model O
. O
The O
NMT O
model O
then O
produces O
translations O
to O
train O
the O
D O
- O
E O
model O
. O
This O
way O
, O
the O
D O
- O
E O
and O
NMT O
models O
boost O
each O
other O
in O
a O
‚Äú O
co O
- O
training O
‚Äù O
way O
. O
The O
key O
to O
this O
method O
is O
to O
use O
the O
NMT O
model O
to O
generate O
the O
training O
data O
for O
the O
D O
- O
E O
model O
in O
order O
to O
improve O
its O
de O
- O
noising O
capabilities O
. O
Algorithm O
2 O
explains O
this O
idea O
and O
Ô¨Ågure O
2b O
illustrates O
it O
. O
As O
before O
, O
in O
every O
iteration O
the O
whole O
dataset O
is O
scored O
and O
ranked O
so O
sentence O
pairs O
that O
ranked O
low O
early O
on O
can O
be O
recovered O
in O
later O
iterations O
. O
In O
principle O
, O
forward O
- O
translation O
does O
not O
seem O
to O
be O
a O
good O
way O
to O
generate O
training O
data O
. O
One O
can O
anticipate O
that O
the O
models O
are O
prone O
to O
mimic O
the O
training O
data O
, O
including O
the O
noise O
. O
Just O
as O
in O
our O
Ô¨Årst O
method O
, O
we O
break O
the O
cycle O
by O
adding O
a O
selection O
step O
based O
on O
the O
D O
- O
E O
scores O
and O
using O
only O
the O
top O
- O
ranking O
data O
to O
train O
the O
next O
NMT O
model O
. O
5 O
Experimental O
Setup O
Machine O
Translation O
Model O
To O
assess O
if O
we O
can O
recover O
useful O
subsets O
from O
noisy O
data O
, O
we O
train O
Transformer O
- O
Big O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
NMT O
models O
using O
data O
reÔ¨Åned O
with O
our O
methods O
. O
To O
train O
the O
models O
, O
we O
split O
the O
source O
and O
target O
texts O
into O
pieces O
using O
bilingual O
sentence O
piece O
models O
( O
Kudo O
and O
Richardson O
, O
2018 O
) O
that O
were O
trained O
with O
the O
ParaCrawl O
v1.0 O
data O
only O
. O
We O
train O
for O
a O
maximum O
of O
200k O
steps O
using O
( O
Shazeer O
and O
Stern O
, O
2018 O
) O
and O
pick O
the O
best O
checkpoint O
according O
to O
the O
performance O
on O
a O
validation O
set O
. O
The O
models O
are O
trained O
on O
Google O
‚Äôs O
Cloud O
TPU O
v3 O
with O
batch O
size O
3072 O
. O
In O
all O
our O
experiments O
, O
the O
conÔ¨Åguration O
of O
the O
NMT O
models O
is O
kept O
the O
same O
with O
the O
only O
difference O
being O
the O
training O
data O
. O
Algorithm O
2 O
Machine O
translation O
iterative O
Ô¨Åltering O
1 O
: O
œÑ‚Üêselection O
threshold O
2 O
: O
NMT O
= O
TrainNMT(data O
) O
3 O
: O
while O
D O
- O
E O
improves O
orNMT O
improves O
do O
4 O
: O
translated O
data O
= O
Translate(data O
; O
NMT O
) O
5 O
: O
D O
- O
E O
= O
TrainDualEncoder(translated O
data O
) O
6 O
: O
scored O
data O
= O
Score(data O
; O
D O
- O
E O
) O
7 O
: O
ranked O
data O
= O
Rank(scored O
data O
) O
8 O
: O
selected O
data O
= O
Select(ranked O
data O
; O
œÑ O
) O
9 O
: O
NMT O
= O
TrainNMT(selected O
data O
) O
10 O
: O
end O
whileen O
- O
fr O
en O
- O
de O
All O
sentence O
pairs O
4,235 O
M O
4,591 O
M O
Pre-Ô¨Åltered O
289 O
M O
282 O
M O
70th O
percentile O
( O
for O
NMT O
) O
87 O
M O
85 O
M O
80th O
percentile O
( O
for O
D O
- O
E O
) O
58 O
M O
56 O
M O
Table O
1 O
: O
Number O
of O
sentence O
pairs O
in O
the O
ParaCrawl O
v1.0 O
data O
, O
and O
after O
preÔ¨Åltering O
and O
selection O
. O
Data O
In O
our O
experiments O
we O
use O
two O
language O
pairs O
: O
English O
to O
French O
( O
en O
- O
fr O
) O
and O
English O
to O
German O
( O
en O
- O
de O
) O
. O
We O
use O
ParaCrawl O
v1.0 O
( O
Espl O
` O
a O
et O
al O
. O
, O
2019 O
) O
as O
training O
data O
. O
We O
apply O
light O
- O
weight O
preÔ¨Åltering O
steps O
to O
remove O
sentence O
pairs O
that O
: O
( O
i O
) O
are O
duplicated O
, O
( O
ii O
) O
have O
identical O
source O
and O
target O
texts O
, O
( O
iii O
) O
have O
empty O
sentences O
, O
or O
( O
iv O
) O
have O
a O
large O
difference O
in O
the O
number O
of O
tokens O
. O
For O
the O
last O
case O
, O
we O
compute O
the O
ratio O
of O
source O
over O
target O
tokens O
as O
: O
œÅ O
= O
nS+Œ± O
nT+Œ±withnSandnTbeing O
the O
number O
of O
tokens O
in O
the O
source O
and O
in O
the O
target O
respectively O
, O
and O
Œ±a O
token O
count O
tolerance O
. O
With O
anŒ±of O
15 O
, O
we O
discard O
a O
sentence O
pair O
if O
œÅis O
greater O
than O
1.5 O
. O
Similarly O
for O
the O
ratio O
of O
target O
over O
source O
tokens O
. O
We O
use O
WMT O
newstest O
20122013 O
( O
Bojar O
et O
al O
. O
, O
2014 O
) O
as O
the O
development O
set O
and O
we O
evaluate O
on O
two O
sets O
: O
WMT O
newstest O
2014 O
and O
news O
discussion O
test O
2015 O
for O
en O
- O
fr O
; O
WMT O
newstest O
2014 O
and O
2015 O
for O
en O
- O
de O
. O
Evaluation O
As O
described O
in O
section O
3 O
, O
we O
trained O
the O
D O
- O
E O
models O
as O
rankers O
. O
Thus O
, O
we O
use O
the O
BUCC O
2018 O
mining O
task O
( O
Zweigenbaum O
et O
al O
. O
, O
2018 O
) O
as O
an O
intrinsic O
metric O
for O
the O
model O
. O
The O
task O
data O
consists O
of O
corpora O
for O
four O
language O
pairs O
including O
fr O
- O
en O
and O
de O
- O
en O
. O
For O
each O
language O
pair O
, O
the O
shared O
task O
provides O
a O
monolingual O
corpus O
for O
each O
language O
and O
a O
ground O
truth O
list O
containing O
true O
translation O
pairs O
. O
The O
task O
is O
to O
construct O
a O
list O
of O
translation O
pairs O
from O
the O
monolingual O
corpora O
, O
and O
evaluate O
them O
in O
terms O
of O
the O
F1 O
compared O
to O
the O
ground O
truth O
. O
To O
test O
the O
end O
- O
performance O
of O
the O
NMT O
models O
in O
terms O
BLEU O
scores O
, O
we O
compute O
the O
detokenized O
and O
case O
- O
sensitive O
BLEU O
scores O
against O
the O
original O
references O
using O
an O
in O
- O
house O
reimplementation O
of O
themteval-v14.pl O
script O
. O
Iterative O
Selection O
In O
our O
experiments O
we O
ran O
3 O
iterations O
of O
the O
IFmethod O
and O
3 O
iterations O
of O
the O
MT O
- O
IF O
one O
. O
To O
deÔ¨Åne O
the O
value O
of O
the O
selection O
thresholds O
, O
we O
conducted O
initial O
experiments O
to O
explore O
the439impact O
of O
the O
threshold O
when O
selecting O
the O
data O
to O
train O
the O
D O
- O
E O
models O
. O
Figure O
3 O
shows O
the O
BUCC O
results O
, O
in O
terms O
of O
the O
best O
F1 O
measure O
and O
the O
area O
under O
the O
precision O
- O
recall O
curve O
( O
AUCPR O
) O
, O
for O
D O
- O
E O
models O
trained O
with O
data O
selected O
using O
different O
thresholds O
. O
Even O
though O
there O
is O
not O
a O
single O
threshold O
that O
works O
best O
for O
both O
languages O
, O
models O
trained O
with O
data O
selected O
from O
the O
70th O
or O
80th O
percentiles O
produce O
the O
best O
results O
. O
Using O
either O
very O
low O
( O
below O
0.2 O
) O
or O
very O
high O
thresholds O
( O
above O
0.95 O
) O
leads O
to O
D O
- O
E O
models O
with O
lower O
results O
. O
We O
set O
the O
selection O
thresholds O
for O
the O
data O
to O
train O
the O
D O
- O
E O
models O
and O
to O
train O
the O
NMT O
models O
separately O
. O
For O
the O
former O
we O
use O
data O
on O
the O
80th O
percentile O
, O
and O
on O
the O
70th O
percentile O
for O
the O
latter O
. O
Our O
intuition O
was O
that O
we O
can O
be O
more O
stringent O
when O
selecting O
data O
to O
train O
the O
D O
- O
E O
because O
only O
high O
- O
ranking O
examples O
may O
be O
true O
translations O
to O
learn O
from O
. O
Table O
1 O
shows O
the O
number O
of O
sentences O
in O
the O
ParaCrawl O
v1.0 O
en O
- O
fr O
and O
en O
- O
de O
datasets O
and O
the O
amount O
of O
sentences O
that O
the O
pre-Ô¨Åltering O
and O
selection O
steps O
, O
at O
the O
different O
thresholds O
, O
let O
through O
. O
The O
large O
number O
of O
sentence O
pairs O
that O
are O
eliminated O
via O
pre-Ô¨Åltering O
give O
an O
indication O
of O
how O
much O
noise O
there O
is O
in O
the O
data O
. O
It O
is O
worth O
noticing O
that O
the O
subset O
of O
data O
that O
we O
deem O
‚Äú O
useful O
‚Äù O
is O
two O
orders O
of O
magnitude O
smaller O
than O
the O
original O
data O
. O
6 O
Results O
6.1 O
Intrinsic O
Dual O
- O
Encoder O
Evaluation O
Table O
2 O
shows O
the O
BUCC O
mining O
task O
results O
for O
the O
D O
- O
E O
models O
trained O
with O
our O
methods O
in O
terms O
of O
F1 O
AUCPR O
/ O
Best O
F1 O
0 O
0.2 O
0.4 O
0.6 O
0.8 O
1.000.20.40.60.81.0 O
Threshold O
( O
œÑ)en O
- O
fr O
AUCPR O
en O
- O
fr O
Best O
F1 O
en O
- O
de O
AUCPR O
en O
- O
de O
Best O
F1 O
Figure O
3 O
: O
BUCC O
mining O
results O
of O
dual O
encoder O
models O
trained O
with O
data O
selected O
at O
different O
thresholds.and O
AUCPR O
. O
As O
baseline O
we O
include O
the O
results O
of O
a O
D O
- O
E O
model O
trained O
with O
all O
the O
ParaCrawl O
v1.0 O
data O
after O
pre-Ô¨Åltering O
. O
The O
baseline O
performs O
poorly O
in O
both O
en O
- O
fr O
and O
en O
- O
de O
. O
The O
D O
- O
E O
models O
trained O
with O
theIFdata O
produce O
good O
mining O
results O
starting O
from O
the O
very O
initial O
models O
, O
i.e. O
when O
using O
D O
- O
E O
models O
trained O
using O
hard O
negatives O
but O
no O
selection O
yet O
. O
The O
signiÔ¨Åcant O
gains O
of O
IF0over O
the O
baseline O
conÔ¨Årm O
our O
observations O
about O
the O
positive O
impact O
of O
hard O
negatives O
in O
cross O
- O
lingual O
tasks O
( O
Guo O
et O
al O
. O
, O
2018 O
) O
. O
In O
subsequent O
iterations O
( O
indices O
1 O
to O
3 O
in O
table O
2 O
) O
selection O
is O
used O
and O
the O
D O
- O
E O
models O
show O
steady O
improvement O
. O
The O
improvement O
in O
the O
AUCPR O
and O
F1 O
of O
the O
D O
- O
E O
models O
trained O
with O
the O
MT O
- O
IFdata O
is O
quite O
remarkable O
. O
The O
performance O
for O
models O
trained O
with O
data O
from O
the O
Ô¨Årst O
iteration O
of O
this O
method O
surpass O
the O
performance O
of O
models O
trained O
with O
the O
the O
third O
iteration O
of O
the O
IFdata O
and O
keep O
improving O
, O
but O
seem O
to O
plateau O
around O
the O
second O
iteration O
. O
For O
reference O
, O
we O
include O
in O
table O
2 O
the O
AUCPR O
and O
F1 O
from O
embeddings O
generated O
with O
the O
public O
‚Äú O
universal O
- O
sentence O
- O
encodermultilingual O
- O
large O
‚Äù O
v2 O
( O
Yang O
et O
al O
. O
, O
2019b O
) O
from O
TFHub1to O
show O
the O
performance O
of O
a O
D O
- O
E O
model O
trained O
on O
multiple O
large O
and O
non O
- O
public O
industry O
datasets O
. O
As O
expected O
, O
training O
on O
this O
kind O
of O
data O
is O
far O
better O
than O
de O
- O
noising O
, O
but O
the O
evaluation O
shows O
that O
our O
methods O
do O
a O
good O
job O
reÔ¨Åning O
data O
, O
especially O
considering O
how O
much O
noise O
there O
is O
in O
the O
ParaCrawl O
datasets O
to O
start O
with O
. O
6.2 O
Translation O
Evaluation O
To O
illustrate O
the O
end O
- O
performance O
of O
our O
methods O
, O
table O
3 O
shows O
the O
BLEU O
scores O
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
of O
NMT O
models O
trained O
with O
data O
subsets O
selected O
with O
our O
methods O
. O
The O
D O
- O
E O
models O
used O
to O
score O
the O
data O
in O
each O
iteration O
correspond O
to O
the O
same O
models O
reported O
in O
table O
2 O
. O
As O
baseline O
we O
use O
an O
NMT O
model O
trained O
with O
all O
the O
sentence O
pairs O
just O
after O
pre-Ô¨Åltering O
, O
i.e. O
selection O
is O
not O
used O
yet O
. O
For O
both O
our O
methods O
the O
NMT O
models O
show O
considerable O
improvement O
over O
the O
baseline O
. O
It O
is O
interesting O
that O
the O
initial O
NMT O
( O
IF0 O
in O
table O
3 O
) O
, O
shows O
good O
improvement O
in O
spite O
of O
using O
a O
D O
- O
E O
whose O
only O
difference O
over O
baseline O
is O
the O
use O
of O
hard O
negatives O
. O
There O
is O
also O
noticeable O
improvement O
between O
the O
IF0andIF1results O
pointing O
to O
the O
fact O
that O
our O
process O
of O
scoring O
, O
ranking O
and O
selection O
is O
also O
useful O
to O
improve O
the O
1https://tfhub.dev/google/universal-sentence-encodermultilingual-large/2440Methoden-fr O
en O
- O
de O
AUCPR O
Best O
F1 O
AUCPR O
Best O
F1 O
Pre-Ô¨Åltered O
data O
( O
baseline O
) O
0.068 O
0.149 O
0.020 O
0.069 O
IF0 O
0.246 O
0.330 O
0.094 O
0.179 O
IF1 O
0.380 O
0.445 O
0.291 O
0.359 O
IF2 O
0.570 O
0.600 O
0.372 O
0.415 O
IF3 O
0.622 O
0.642 O
0.390 O
0.432 O
MT O
- O
IF1 O
0.641 O
0.673 O
0.545 O
0.566 O
MT O
- O
IF2 O
0.664 O
0.697 O
0.600 O
0.620 O
MT O
- O
IF3 O
0.676 O
0.707 O
0.593 O
0.608 O
USE O
multi O
- O
lingual O
0.824 O
0.812 O
0.861 O
0.815 O
Table O
2 O
: O
BUCC O
mining O
results O
of O
the O
dual O
- O
encoder O
models O
. O
The O
index O
in O
each O
experiment O
denotes O
the O
iteration O
. O
The O
USE O
multi O
- O
lingual O
model O
was O
trained O
using O
non O
- O
public O
industry O
datasets O
. O
Methoden O
- O
fr O
en O
- O
de O
newstest2014 O
newsdiscusstest2015 O
newstest2014 O
newstest2015 O
Pre-Ô¨Åltered O
data O
( O
baseline O
) O
0.303 O
0.297 O
0.196 O
0.239 O
IF0 O
0.324 O
0.315 O
0.237 O
0.276 O
IF1 O
0.342 O
0.343 O
0.239 O
0.281 O
IF2 O
0.340 O
0.352 O
0.237 O
0.279 O
IF3 O
0.342 O
0.348 O
0.236 O
0.283 O
Forward O
- O
translated O
data O
0.305 O
0.306 O
0.203 O
0.243 O
MT O
- O
IF1 O
0.342 O
0.346 O
0.237 O
0.280 O
MT O
- O
IF2 O
0.343 O
0.348 O
0.235 O
0.283 O
MT O
- O
IF3 O
0.346 O
0.349 O
0.236 O
0.286 O
Table O
3 O
: O
BLEU O
scores O
of O
the O
trained O
NMT O
models O
and O
the O
baseline O
models O
. O
The O
index O
in O
each O
experiment O
denotes O
the O
iteration O
. O
NMT O
models O
. O
The O
second O
half O
of O
table O
3 O
shows O
the O
BLEU O
scores O
when O
the O
NMT O
models O
are O
added O
to O
the O
reÔ¨Ånement O
process O
in O
the O
MT O
- O
IFmethod O
. O
For O
a O
better O
reference O
, O
we O
train O
an O
NMT O
model O
with O
forward O
- O
translated O
sentence O
pairs O
using O
the O
baseline O
NMT O
model O
. O
Crucially O
, O
there O
is O
no O
selection O
on O
the O
forward O
- O
translated O
data O
to O
train O
this O
model O
. O
This O
NMT O
model O
does O
not O
show O
improvement O
relative O
to O
the O
baseline O
NMT O
model O
and O
conÔ¨Årms O
that O
distilling O
new O
training O
examples O
from O
forward O
translations O
provides O
little O
or O
no O
gain O
. O
In O
contrast O
to O
the O
BUCC O
evaluation O
from O
table O
2 O
, O
the O
downstream O
task O
does O
not O
seem O
to O
require O
several O
iterations O
to O
show O
good O
results O
. O
The O
BLEU O
scores O
of O
later O
iterations O
in O
the O
process O
only O
improve O
marginally O
as O
opposed O
to O
the O
steady O
improvement O
observed O
in O
the O
BUCC O
task O
. O
6.3 O
Supervised O
vs O
Self O
- O
Supervised O
We O
use O
Bicleaner O
to O
compare O
our O
methods O
against O
a O
supervised O
approach O
on O
the O
task O
of O
de O
- O
noising O
the O
ParaCrawl O
data O
, O
with O
the O
important O
caveat O
thaten O
- O
fr O
en O
- O
de O
70th O
percentile O
after O
lang O
ID O
29 O
M O
27 O
M O
Bicleaner O
v1.2 O
25 O
M O
17 O
M O
Table O
4 O
: O
Number O
of O
sentence O
pairs O
of O
selected O
data O
after O
language O
identiÔ¨Åcation O
and O
in O
Bicleaner O
v1.2 O
. O
Bicleaner O
is O
not O
only O
supervised O
but O
tailored O
to O
denoise O
this O
data O
. O
In O
that O
sense O
, O
our O
method O
would O
be O
in O
disadvantage O
especially O
because O
our O
D O
- O
E O
models O
were O
not O
trained O
with O
any O
signal O
related O
to O
the O
identity O
of O
the O
language O
. O
To O
add O
this O
missing O
component O
to O
our O
method O
, O
we O
use O
language O
identiÔ¨Åcation O
as O
a O
post O
- O
processing O
step O
on O
the O
reÔ¨Åned O
data O
. O
We O
use O
a O
pre O
- O
trained O
language O
identiÔ¨Åcation O
method O
from O
Zhang O
et O
al O
. O
( O
2018 O
) O
to O
Ô¨Ålter O
out O
pairs O
where O
the O
source O
or O
target O
texts O
do O
not O
match O
the O
expected O
language O
. O
As O
around O
30 O
% O
of O
the O
training O
data O
gets O
discarded O
( O
table O
1 O
vs O
table O
4 O
) O
, O
the O
scores O
of O
the O
remaining O
data O
need O
to O
be O
re O
- O
ranked O
in O
preparation O
for O
the O
selection O
step O
. O
We O
train O
new O
NMT O
models O
using O
only O
the O
sentence O
pairs O
that O
get O
ranked O
in O
the44170th O
percentile O
and O
Ô¨Åltered O
by O
the O
language O
identiÔ¨Åcation O
. O
We O
compare O
the O
models O
against O
similar O
NMT O
models O
trained O
with O
the O
Bicleaner O
v1.2 O
data O
downloaded O
from O
the O
ParaCrawl O
website2 O
. O
Table O
4 O
shows O
the O
number O
of O
sentence O
pairs O
used O
to O
train O
the O
NMT O
models O
after O
applying O
language O
identiÔ¨Åcation O
and O
in O
the O
Bicleaner O
v1.2 O
data O
. O
To O
isolate O
the O
effects O
of O
language O
identiÔ¨Åcation O
, O
we O
compare O
NMT O
models O
trained O
with O
data O
from O
our O
methods O
against O
similar O
models O
trained O
with O
data O
that O
went O
through O
language O
identiÔ¨Åcation O
also O
but O
, O
as O
in O
previous O
baselines O
, O
no O
selection O
was O
used O
. O
As O
shown O
in O
Table O
5 O
, O
using O
language O
identiÔ¨Åcation O
on O
the O
training O
data O
boosts O
the O
performance O
. O
The O
NMT O
models O
trained O
with O
the O
data O
reÔ¨Åned O
with O
our O
methods O
still O
show O
considerable O
improvement O
over O
not O
using O
selection O
, O
making O
evident O
that O
there O
is O
still O
much O
room O
for O
data O
reÔ¨Ånement O
after O
language O
identiÔ¨Åcation O
. O
Our O
method O
shows O
very O
competitive O
results O
against O
the O
NMT O
models O
trained O
using O
the O
Bicleaner O
v1.2 O
data O
, O
surpassing O
the O
BLEU O
scores O
in O
en O
- O
fr O
and O
getting O
very O
similar O
performance O
in O
en O
- O
de O
. O
It O
is O
interesting O
that O
, O
with O
the O
addition O
of O
language O
identiÔ¨Åcation O
, O
our O
self O
- O
supervised O
method O
can O
remove O
noise O
just O
as O
effectively O
as O
a O
targeted O
effort O
to O
denoise O
the O
ParaCrawl O
data O
. O
6.4 O
Iterative O
Data O
ReÔ¨Ånement O
To O
verify O
the O
effectiveness O
of O
our O
methods O
in O
Ô¨Ånding O
useful O
subsets O
contained O
in O
the O
noisy O
data O
, O
we O
analyze O
the O
results O
of O
our O
models O
when O
scoring O
true O
sentence O
pairs O
versus O
scoring O
pairs O
that O
are O
not O
actual O
translations O
. O
For O
this O
analysis O
, O
we O
leverage O
the O
BUCC O
mining O
task O
and O
compute O
the O
dot O
products O
of O
‚Äú O
ground O
truth O
‚Äù O
pairs O
using O
our O
D O
- O
E O
models O
. O
Figure O
4 O
shows O
box O
plots O
of O
the O
dot O
products O
for O
both O
en O
- O
fr O
and O
en O
- O
de O
BUCC O
data O
. O
For O
reference O
, O
we O
compute O
the O
dot O
products O
of O
the O
‚Äú O
nearest O
negative O
‚Äù O
of O
each O
source O
sentence O
. O
We O
reuse O
the O
retrieval O
results O
from O
the O
D O
- O
E O
intrinsic O
evaluation O
( O
subsection O
6.1 O
) O
to O
deÔ¨Åne O
the O
nearest O
negative O
as O
the O
target O
sentence O
with O
the O
highest O
dot O
product O
that O
is O
not O
its O
actual O
translation O
. O
This O
leads O
to O
9,086 O
ground O
truth O
and O
nearest O
negative O
dot O
products O
for O
en O
- O
fr O
and O
9,580 O
for O
en O
- O
de O
whose O
distributions O
are O
displayed O
in O
the O
box O
plots O
in O
Ô¨Ågure O
4 O
. O
Starting O
with O
the O
baseline O
D O
- O
E O
models O
, O
the O
dot O
products O
of O
the O
ground O
truth O
and O
the O
nearest O
negative O
are O
very O
close O
in O
value O
. O
This O
is O
evident O
by O
the O
fact O
that O
their O
difference O
( O
also O
plotted O
in O
Ô¨Ågure O
4 O
) O
is O
very O
close O
to O
0 O
. O
The O
differ2https://paracrawl.eu/v1ence O
starts O
to O
grow O
with O
the O
IF0models O
, O
showing O
that O
hard O
- O
negatives O
are O
useful O
to O
increase O
the O
separation O
between O
the O
dot O
products O
of O
both O
classes O
. O
For O
theIFmethod O
, O
the O
difference O
between O
ground O
truth O
and O
nearest O
negative O
keeps O
growing O
steadily O
with O
every O
iteration O
. O
This O
conÔ¨Årms O
the O
progression O
observed O
in O
the O
AUCPR O
and O
F1 O
measures O
in O
table O
2 O
. O
For O
theMT O
- O
IFmodels O
, O
the O
score O
difference O
between O
ground O
truth O
and O
nearest O
negative O
is O
already O
signiÔ¨Åcant O
in O
the O
Ô¨Årst O
iteration O
, O
but O
it O
does O
not O
progress O
much O
further O
in O
later O
iterations O
. O
This O
also O
conÔ¨Årms O
the O
observations O
for O
these O
models O
in O
the O
BUCC O
mining O
results O
from O
table O
2 O
. O
The O
fact O
that O
the O
dot O
products O
of O
our O
models O
show O
good O
levels O
of O
separation O
of O
each O
class O
corroborate O
, O
from O
the O
data O
analysis O
standpoint O
, O
that O
both O
our O
methods O
are O
effective O
in O
separating O
useful O
data O
samples O
from O
the O
noisy O
dataset O
. O
6.5 O
Discussion O
Intrinsic O
vs O
downstream O
evaluations O
Our O
selfsupervised O
methods O
seem O
to O
naturally O
improve O
the O
quality O
of O
the O
reÔ¨Åned O
data O
, O
as O
measured O
by O
the O
results O
of O
the O
BUCC O
parallel O
text O
mining O
task O
. O
However O
, O
most O
of O
the O
BLEU O
score O
gains O
are O
achieved O
on O
the O
Ô¨Årst O
iteration O
. O
One O
possible O
explanation O
is O
that O
the O
BUCC O
evaluation O
is O
a O
closer O
match O
to O
the O
ranking O
task O
used O
to O
train O
the O
D O
- O
E O
model O
. O
Another O
possibility O
is O
that O
, O
given O
that O
different O
sequences O
can O
produce O
the O
same O
BLEU O
scores O
, O
there O
may O
be O
improvements O
in O
the O
translation O
quality O
that O
the O
BLEU O
scores O
do O
not O
reÔ¨Çect O
. O
Making O
the O
method O
more O
aware O
of O
the O
downstream O
translation O
task O
and O
gaining O
insight O
into O
the O
translation O
quality O
are O
interesting O
lines O
of O
future O
work O
. O
Language O
identiÔ¨Åcation O
impact O
In O
noisy O
data O
, O
language O
identiÔ¨Åcation O
seems O
to O
play O
a O
signiÔ¨Åcant O
role O
. O
In O
our O
experiments O
we O
applied O
it O
as O
a O
postprocess O
but O
we O
are O
interested O
in O
applying O
it O
as O
part O
of O
the O
pre-Ô¨Åltering O
process O
, O
or O
integrated O
as O
part O
of O
our O
scoring O
models O
in O
the O
future O
. O
Breaking O
the O
data O
- O
model O
memorization O
cycle O
Training O
NMT O
models O
directly O
with O
translated O
data O
did O
not O
produce O
gains O
over O
the O
baseline O
. O
But O
we O
found O
signiÔ¨Åcant O
gains O
when O
instead O
we O
used O
the O
translated O
data O
to O
train O
D O
- O
E O
models O
and O
used O
the O
models O
to O
score O
and O
select O
data O
to O
in O
turn O
train O
the O
NMT O
models O
. O
We O
see O
this O
as O
conÔ¨Årmation O
that O
it O
is O
possible O
to O
break O
the O
data O
- O
model O
memorization O
cycle O
by O
co O
- O
training O
models O
using O
different O
training O
goals.442Methoden O
- O
fr O
en O
- O
de O
newstest2014 O
newsdiscusstest2015 O
newstest2014 O
newstest2015 O
Pre-Ô¨Åltered O
data O
lang O
ID O
0.336 O
0.346 O
0.239 O
0.279 O
Bicleaner O
v1.2 O
data O
0.363 O
0.370 O
0.274 O
0.316 O
IF1 O
0.369 O
0.373 O
0.263 O
0.306 O
IF2 O
0.369 O
0.369 O
0.267 O
0.308 O
IF3 O
0.366 O
0.372 O
0.269 O
0.314 O
MT O
- O
IF1 O
0.361 O
0.365 O
0.263 O
0.308 O
MT O
- O
IF2 O
0.363 O
0.370 O
0.262 O
0.303 O
MT O
- O
IF3 O
0.360 O
0.364 O
0.259 O
0.303 O
Table O
5 O
: O
BLEU O
scores O
of O
the O
NMT O
models O
using O
language O
identiÔ¨Åcation O
and O
compared O
against O
Bicleaner O
. O
Figure O
4 O
: O
Dot O
product O
distributions O
for O
the O
ground O
truth O
and O
nearest O
negative O
from O
the O
BUCC O
mining O
task O
. O
The O
box O
plots O
represent O
the O
( O
5,25,50,75,95)-percentile O
of O
the O
dot O
product O
distribution O
for O
each O
method O
and O
iteration O
. O
7 O
Conclusions O
We O
introduced O
two O
self O
- O
supervised O
methods O
to O
reÔ¨Åne O
pairwise O
data O
aimed O
at O
selecting O
useful O
subsets O
from O
noisy O
data O
. O
In O
our O
experiments O
we O
used O
parallel O
texts O
mined O
from O
the O
internet O
as O
example O
of O
the O
weakly O
constructed O
pairwise O
data O
to O
reÔ¨Åne O
. O
Our O
methods O
do O
not O
require O
linguistic O
knowledge O
or O
human O
annotated O
data O
. O
They O
use O
iterative O
selection O
of O
the O
data O
to O
train O
two O
kinds O
of O
models O
. O
Our O
Ô¨Årst O
method O
is O
based O
on O
self O
- O
boosting O
dualencoder O
models O
iteratively O
. O
We O
applied O
this O
method O
to O
denoise O
data O
to O
train O
NMT O
models O
. O
Our O
second O
method O
integrates O
the O
NMT O
models O
into O
the O
iterative O
process O
to O
generate O
translations O
that O
, O
after O
a O
selection O
step O
, O
are O
used O
to O
train O
the O
dual O
- O
encoder O
models O
. O
Our O
results O
show O
that O
most O
of O
the O
gains O
in O
terms O
of O
BLEU O
score O
can O
be O
achieved O
in O
the O
Ô¨Årst O
iteration O
of O
our O
methods O
, O
but O
later O
iterations O
keep O
improving O
the O
performance O
of O
the O
dual O
- O
encoder O
models O
in O
the O
BUCC O
evaluation O
. O
In O
our O
experiments O
, O
using O
translated O
text O
in O
combination O
with O
a O
selection O
step O
helped O
to O
improve O
the O
de O
- O
noising O
capabilities O
of O
the O
dual O
- O
encoder O
models O
. O
We O
observed O
that O
selection O
is O
effective O
to O
break O
the O
model O
- O
datamemorization O
cycle O
. O
One O
characteristic O
that O
our O
self O
- O
supervised O
methods O
do O
not O
seem O
to O
capture O
well O
is O
an O
indication O
of O
the O
language O
identity O
. O
If O
we O
use O
language O
identiÔ¨Åcation O
on O
the O
denoised O
data O
as O
a O
post O
- O
processing O
step O
, O
the O
performance O
, O
in O
terms O
of O
BLEU O
scores O
, O
turns O
very O
competitive O
against O
supervised O
targeted O
efforts O
tailored O
to O
remove O
noise O
from O
the O
dataset O
. O
These O
results O
encourage O
us O
to O
pursue O
future O
lines O
of O
work O
that O
include O
using O
cross O
- O
attention O
in O
the O
pairwise O
data O
to O
better O
capture O
the O
relationship O
in O
the O
pairs O
. O
Also O
, O
speciÔ¨Åc O
to O
parallel O
sentences O
mined O
from O
the O
internet O
, O
we O
would O
like O
to O
explore O
ways O
to O
include O
language O
identiÔ¨Åcation O
in O
the O
models O
. O
On O
the O
other O
hand O
, O
it O
seems O
natural O
to O
leverage O
the O
self O
- O
supervision O
characteristics O
of O
our O
methods O
and O
apply O
them O
to O
language O
pairs O
where O
noisy O
internet O
data O
may O
be O
available O
but O
annotated O
data O
is O
not O
. O
Lastly O
, O
we O
are O
interested O
in O
expanding O
our O
methods O
to O
other O
pairwise O
data O
such O
as O
text O
- O
image O
pairs O
. O
Acknowledgments O
The O
authors O
would O
like O
to O
thank O
Noah O
Constant O
and O
the O
anonymous O
reviewers O
for O
their O
valuable O
feedback O
and O
suggestions O
to O
improve O
this O
paper.443References O
OndÀárej O
Bojar O
, O
Christian O
Buck O
, O
Christian O
Federmann O
, O
Barry O
Haddow O
, O
Philipp O
Koehn O
, O
Johannes O
Leveling O
, O
Christof O
Monz O
, O
Pavel O
Pecina O
, O
Matt O
Post O
, O
Herve O
Saint O
- O
Amand O
, O
et O
al O
. O
2014 O
. O
Findings O
of O
the O
2014 O
workshop O
on O
statistical O
machine O
translation O
. O
In O
Proceedings O
of O
the O
ninth O
workshop O
on O
statistical O
machine O
translation O
, O
pages O
12‚Äì58 O
. O
Daniel O
Cer O
, O
Yinfei O
Yang O
, O
Sheng O
- O
yi O
Kong O
, O
Nan O
Hua O
, O
Nicole O
Limtiaco O
, O
Rhomni O
St. O
John O
, O
Noah O
Constant O
, O
Mario O
Guajardo O
- O
Cespedes O
, O
Steve O
Yuan O
, O
Chris O
Tar O
, O
Brian O
Strope O
, O
and O
Ray O
Kurzweil O
. O
2018 O
. O
Universal O
sentence O
encoder O
for O
English O
. O
In O
Proceedings O
of O
the O
2018 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
: O
System O
Demonstrations O
, O
pages O
169‚Äì174 O
, O
Brussels O
, O
Belgium O
. O
Association O
for O
Computational O
Linguistics O
. O
Vishrav O
Chaudhary O
, O
Yuqing O
Tang O
, O
Francisco O
Guzmn O
, O
Holger O
Schwenk O
, O
and O
Philipp O
Koehn O
. O
2019 O
. O
Lowresource O
corpus O
Ô¨Åltering O
using O
multilingual O
sentence O
embeddings O
. O
Proceedings O
of O
the O
Fourth O
Conference O
on O
Machine O
Translation O
( O
Volume O
3 O
: O
Shared O
Task O
Papers O
, O
Day O
2 O
) O
. O
Boxing O
Chen O
and O
Fei O
Huang O
. O
2016 O
. O
Semi O
- O
supervised O
convolutional O
networks O
for O
translation O
adaptation O
with O
tiny O
amount O
of O
in O
- O
domain O
data O
. O
In O
Proceedings O
of O
the O
20th O
SIGNLL O
Conference O
on O
Computational O
Natural O
Language O
Learning O
( O
CoNLL O
) O
, O
pages O
314‚Äì323 O
. O
Muthuraman O
Chidambaram O
, O
Yinfei O
Yang O
, O
Daniel O
Cer O
, O
Steve O
Yuan O
, O
Yun O
- O
Hsuan O
Sung O
, O
Brian O
Strope O
, O
and O
Ray O
Kurzweil O
. O
2018 O
. O
Learning O
cross O
- O
lingual O
sentence O
representations O
via O
a O
multi O
- O
task O
dual O
- O
encoder O
model O
. O
CoRR O
, O
abs/1810.12836 O
. O
A. O
Defauw O
, O
Sara O
Szoc O
, O
Anna O
Bardadym O
, O
Joris O
Brabers O
, O
Frederic O
Everaert O
, O
Roko O
Mijic O
, O
K. O
Scholte O
, O
Tom O
Vanallemeersch O
, O
Koen O
Van O
Winckel O
, O
and O
J. O
V O
. O
D. O
Bogaert O
. O
2019 O
. O
Misalignment O
detection O
for O
webscraped O
corpora O
: O
A O
supervised O
regression O
approach O
. O
Informatics O
, O
6:35 O
. O
Miquel O
Espl O
` O
a O
, O
Mikel O
Forcada O
, O
Gema O
Ram O
¬¥ O
ƒ±rez O
- O
S O
¬¥ O
anchez O
, O
and O
Hieu O
Hoang O
. O
2019 O
. O
ParaCrawl O
: O
Web O
- O
scale O
parallel O
corpora O
for O
the O
languages O
of O
the O
EU O
. O
In O
Proceedings O
of O
Machine O
Translation O
Summit O
XVII O
Volume O
2 O
: O
Translator O
, O
Project O
and O
User O
Tracks O
, O
pages O
118‚Äì119 O
, O
Dublin O
, O
Ireland O
. O
European O
Association O
for O
Machine O
Translation O
. O
Daniel O
Gillick O
, O
Sayali O
Kulkarni O
, O
Larry O
Lansing O
, O
Alessandro O
Presta O
, O
Jason O
Baldridge O
, O
Eugene O
Ie O
, O
and O
Diego O
Garcia O
- O
Olano O
. O
2019 O
. O
Learning O
dense O
representations O
for O
entity O
retrieval O
. O
In O
Proceedings O
of O
the O
23rd O
Conference O
on O
Computational O
Natural O
Language O
Learning O
( O
CoNLL O
) O
, O
pages O
528‚Äì537 O
, O
Hong O
Kong O
, O
China O
. O
Association O
for O
Computational O
Linguistics O
. O
Daniel O
Gillick O
, O
Alessandro O
Presta O
, O
and O
Gaurav O
Singh O
Tomar O
. O
2018 O
. O
End O
- O
to O
- O
end O
retrieval O
in O
continuous O
space O
. O
arXiv O
preprint O
arXiv:1811.08008 O
.Mandy O
Guo O
, O
Qinlan O
Shen O
, O
Yinfei O
Yang O
, O
Heming O
Ge O
, O
Daniel O
Cer O
, O
Gustavo O
Hern O
¬¥ O
andez O
¬¥ O
Abrego O
, O
Keith O
Stevens O
, O
Noah O
Constant O
, O
Yun O
- O
hsuan O
Sung O
, O
Brian O
Strope O
, O
and O
Ray O
Kurzweil O
. O
2018 O
. O
Effective O
parallel O
corpus O
mining O
using O
bilingual O
sentence O
embeddings O
. O
InProceedings O
of O
the O
Third O
Conference O
on O
Machine O
Translation O
: O
Research O
Papers O
, O
pages O
165‚Äì176 O
. O
Association O
for O
Computational O
Linguistics O
. O
Matthew O
Henderson O
, O
Rami O
Al O
- O
Rfou O
, O
Brian O
Strope O
, O
YunHsuan O
Sung O
, O
L O
¬¥ O
aszl¬¥o O
Luk O
¬¥ O
acs O
, O
Ruiqi O
Guo O
, O
Sanjiv O
Kumar O
, O
Balint O
Miklos O
, O
and O
Ray O
Kurzweil O
. O
2017 O
. O
EfÔ¨Åcient O
natural O
language O
response O
suggestion O
for O
smart O
reply O
. O
CoRR O
, O
abs/1705.00652 O
. O
Marcin O
Junczys O
- O
Dowmunt O
. O
2018 O
. O
Dual O
conditional O
cross O
- O
entropy O
Ô¨Åltering O
of O
noisy O
parallel O
corpora O
. O
In O
Proceedings O
of O
the O
Third O
Conference O
on O
Machine O
Translation O
, O
Volume O
2 O
: O
Shared O
Task O
Papers O
, O
pages O
901‚Äì908 O
, O
Belgium O
, O
Brussels O
. O
Association O
for O
Computational O
Linguistics O
. O
Philipp O
Koehn O
, O
Francisco O
Guzman O
, O
Vishrav O
Chaudhary O
, O
and O
Juan O
Pino O
. O
2019 O
. O
Findings O
of O
the O
wmt O
2019 O
shared O
task O
on O
parallel O
corpus O
Ô¨Åltering O
for O
lowresource O
conditions O
. O
In O
Proceedings O
of O
the O
Fourth O
Conference O
on O
Machine O
Translation O
, O
Florence O
, O
Italy O
. O
Association O
for O
Computational O
Linguistics O
. O
Philipp O
Koehn O
, O
Huda O
Khayrallah O
, O
Kenneth O
HeaÔ¨Åeld O
, O
and O
Mikel O
L. O
Forcada O
. O
2018 O
. O
Findings O
of O
the O
wmt O
2018 O
shared O
task O
on O
parallel O
corpus O
Ô¨Åltering O
. O
In O
Proceedings O
of O
the O
Third O
Conference O
on O
Machine O
Translation O
, O
Belgium O
, O
Brussels O
. O
Association O
for O
Computational O
Linguistics O
. O
Taku O
Kudo O
and O
John O
Richardson O
. O
2018 O
. O
SentencePiece O
: O
A O
simple O
and O
language O
independent O
subword O
tokenizer O
and O
detokenizer O
for O
neural O
text O
processing O
. O
In O
Proceedings O
of O
the O
2018 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
: O
System O
Demonstrations O
, O
pages O
66‚Äì71 O
, O
Brussels O
, O
Belgium O
. O
Association O
for O
Computational O
Linguistics O
. O
Kishore O
Papineni O
, O
Salim O
Roukos O
, O
Todd O
Ward O
, O
and O
WeiJing O
Zhu O
. O
2002 O
. O
Bleu O
: O
a O
method O
for O
automatic O
evaluation O
of O
machine O
translation O
. O
In O
Proceedings O
of O
the O
40th O
annual O
meeting O
on O
association O
for O
computational O
linguistics O
, O
pages O
311‚Äì318 O
. O
Association O
for O
Computational O
Linguistics O
. O
Alec O
Radford O
, O
Jeffrey O
Wu O
, O
Rewon O
Child O
, O
David O
Luan O
, O
Dario O
Amodei O
, O
and O
Ilya O
Sutskever O
. O
2019 O
. O
Language O
models O
are O
unsupervised O
multitask O
learners O
. O
OpenAI O
Blog O
, O
1(8):9 O
. O
Colin O
Raffel O
, O
Noam O
Shazeer O
, O
Adam O
Roberts O
, O
Katherine O
Lee O
, O
Sharan O
Narang O
, O
Michael O
Matena O
, O
Yanqi O
Zhou O
, O
Wei O
Li O
, O
and O
Peter O
J. O
Liu O
. O
2019 O
. O
Exploring O
the O
limits O
of O
transfer O
learning O
with O
a O
uniÔ¨Åed O
text O
- O
to O
- O
text O
transformer O
. O
arXiv O
e O
- O
prints O
. O
Philip O
Resnik O
. O
1999 O
. O
Mining O
the O
web O
for O
bilingual O
text O
. O
InProceedings O
of O
the O
37th O
Annual O
Meeting O
of O
the444Association O
for O
Computational O
Linguistics O
on O
Computational O
Linguistics O
, O
pages O
527‚Äì534 O
. O
Association O
for O
Computational O
Linguistics O
. O
Philip O
Resnik O
and O
Noah O
A O
Smith O
. O
2003 O
. O
The O
web O
as O
a O
parallel O
corpus O
. O
Computational O
Linguistics O
, O
29(3):349‚Äì380 O
. O
Dana O
Ruiter O
, O
Cristina O
Espa O
Àúna O
- O
Bonet O
, O
and O
Josef O
van O
Genabith O
. O
2020 O
. O
Self O
- O
induced O
curriculum O
learning O
in O
neural O
machine O
translation O
. O
arXiv O
preprint O
arXiv:2004.03151 O
. O
V¬¥ƒ±ctor O
M. O
S O
¬¥ O
anchez O
- O
Cartagena O
, O
Marta O
Ba O
Àún¬¥on O
, O
Sergio O
Ortiz O
- O
Rojas O
, O
and O
Gema O
Ram O
¬¥ O
ƒ±rez O
- O
S O
¬¥ O
anchez O
. O
2018 O
. O
Prompsit O
‚Äôs O
submission O
to O
wmt O
2018 O
parallel O
corpus O
Ô¨Åltering O
shared O
task O
. O
In O
Proceedings O
of O
the O
Third O
Conference O
on O
Machine O
Translation O
, O
Volume O
2 O
: O
Shared O
Task O
Papers O
, O
Brussels O
, O
Belgium O
. O
Association O
for O
Computational O
Linguistics O
. O
Piyush O
Sharma O
, O
Nan O
Ding O
, O
Sebastian O
Goodman O
, O
and O
Radu O
Soricut O
. O
2018 O
. O
Conceptual O
captions O
: O
A O
cleaned O
, O
hypernymed O
, O
image O
alt O
- O
text O
dataset O
for O
automatic O
image O
captioning O
. O
In O
Proceedings O
of O
the O
56th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
2556‚Äì2565 O
, O
Melbourne O
, O
Australia O
. O
Association O
for O
Computational O
Linguistics O
. O
Noam O
Shazeer O
and O
Mitchell O
Stern O
. O
2018 O
. O
Adafactor O
: O
Adaptive O
learning O
rates O
with O
sublinear O
memory O
cost O
. O
arXiv O
preprint O
arXiv:1804.04235 O
. O
Chau O
Tran O
, O
Yuqing O
Tang O
, O
Xian O
Li O
, O
and O
Jiatao O
Gu O
. O
2020 O
. O
Cross O
- O
lingual O
retrieval O
for O
iterative O
self O
- O
supervised O
training O
. O
arXiv O
preprint O
arXiv:2006.09526 O
. O
Jakob O
Uszkoreit O
, O
Jay O
M. O
Ponte O
, O
Ashok O
C. O
Popat O
, O
and O
Moshe O
Dubiner O
. O
2010 O
. O
Large O
scale O
parallel O
document O
mining O
for O
machine O
translation O
. O
In O
Proceedings O
of O
the O
23rd O
International O
Conference O
on O
Computational O
Linguistics O
, O
COLING O
‚Äô O
10 O
, O
pages O
1101 O
‚Äì O
1109 O
, O
Stroudsburg O
, O
PA O
, O
USA O
. O
Association O
for O
Computational O
Linguistics O
. O
Ashish O
Vaswani O
, O
Noam O
Shazeer O
, O
Niki O
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion O
Jones O
, O
Aidan O
N O
Gomez O
, O
Lukasz O
Kaiser O
, O
and O
Illia O
Polosukhin O
. O
2017 O
. O
Attention O
is O
all O
you O
need O
. O
In O
I. O
Guyon O
, O
U. O
V O
. O
Luxburg O
, O
S. O
Bengio O
, O
H. O
Wallach O
, O
R. O
Fergus O
, O
S. O
Vishwanathan O
, O
and O
R. O
Garnett O
, O
editors O
, O
Advances O
in O
Neural O
Information O
Processing O
Systems O
30 O
, O
pages O
5998‚Äì6008 O
. O
Curran O
Associates O
, O
Inc. O
Yogarshi O
Vyas O
, O
Xing O
Niu O
, O
and O
Marine O
Carpuat O
. O
2018 O
. O
Identifying O
semantic O
divergences O
in O
parallel O
text O
without O
annotations O
. O
In O
Proceedings O
of O
the O
2018 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
Papers O
) O
, O
pages O
1503‚Äì1515 O
, O
New O
Orleans O
, O
Louisiana O
. O
Association O
for O
Computational O
Linguistics O
. O
Feng O
Wang O
, O
Jian O
Cheng O
, O
Weiyang O
Liu O
, O
and O
Haijun O
Liu O
. O
2018a O
. O
Additive O
margin O
softmax O
for O
face O
veriÔ¨Åcation O
. O
IEEE O
Signal O
Processing O
Letters O
, O
25(7):926 O
‚Äì O
930 O
. O
Mengqiu O
Wang O
, O
Noah O
A. O
Smith O
, O
and O
Teruko O
Mitamura O
. O
2007 O
. O
What O
is O
the O
Jeopardy O
model O
? O
a O
quasi O
- O
synchronous O
grammar O
for O
QA O
. O
In O
Proceedings O
of O
the O
2007 O
Joint O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
and O
Computational O
Natural O
Language O
Learning O
( O
EMNLPCoNLL O
) O
, O
pages O
22‚Äì32 O
, O
Prague O
, O
Czech O
Republic O
. O
Association O
for O
Computational O
Linguistics O
. O
Rui O
Wang O
, O
Masao O
Utiyama O
, O
and O
Eiichiro O
Sumita O
. O
2018b O
. O
Dynamic O
sentence O
sampling O
for O
efÔ¨Åcient O
training O
of O
neural O
machine O
translation O
. O
In O
Proceedings O
of O
the O
56th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
2 O
: O
Short O
Papers O
) O
, O
pages O
298‚Äì304 O
, O
Melbourne O
, O
Australia O
. O
Association O
for O
Computational O
Linguistics O
. O
Wei O
Wang O
, O
Taro O
Watanabe O
, O
Macduff O
Hughes O
, O
Tetsuji O
Nakagawa O
, O
and O
Ciprian O
Chelba O
. O
2018c O
. O
Denoising O
neural O
machine O
translation O
training O
with O
trusted O
data O
and O
online O
data O
selection O
. O
In O
Proceedings O
of O
the O
Third O
Conference O
on O
Machine O
Translation O
, O
pages O
133‚Äì143 O
. O
Association O
for O
Computational O
Linguistics O
. O
Yonghui O
Wu O
, O
Mike O
Schuster O
, O
Zhifeng O
Chen O
, O
Quoc O
V O
. O
Le O
, O
Mohammad O
Norouzi O
, O
Wolfgang O
Macherey O
, O
Maxim O
Krikun O
, O
Yuan O
Cao O
, O
Qin O
Gao O
, O
Klaus O
Macherey O
, O
Jeff O
Klingner O
, O
Apurva O
Shah O
, O
Melvin O
Johnson O
, O
Xiaobing O
Liu O
, O
Lukasz O
Kaiser O
, O
Stephan O
Gouws O
, O
Yoshikiyo O
Kato O
, O
Taku O
Kudo O
, O
Hideto O
Kazawa O
, O
Keith O
Stevens O
, O
George O
Kurian O
, O
Nishant O
Patil O
, O
Wei O
Wang O
, O
Cliff O
Young O
, O
Jason O
Smith O
, O
Jason O
Riesa O
, O
Alex O
Rudnick O
, O
Oriol O
Vinyals O
, O
Greg O
Corrado O
, O
Macduff O
Hughes O
, O
and O
Jeffrey O
Dean O
. O
2016 O
. O
Google O
‚Äôs O
neural O
machine O
translation O
system O
: O
Bridging O
the O
gap O
between O
human O
and O
machine O
translation O
. O
CoRR O
, O
abs/1609.08144 O
. O
Yinfei O
Yang O
, O
Gustavo O
Hern O
¬¥ O
andez O
¬¥ O
Abrego O
, O
Steve O
Yuan O
, O
Mandy O
Guo O
, O
Qinlan O
Shen O
, O
Daniel O
Cer O
, O
Yun O
- O
Hsuan O
Sung O
, O
Brian O
Strope O
, O
and O
Ray O
Kurzweil O
. O
2019a O
. O
Improving O
multilingual O
sentence O
embedding O
using O
bidirectional O
dual O
encoder O
with O
additive O
margin O
softmax O
. O
CoRR O
, O
abs/1902.08564 O
. O
Yinfei O
Yang O
, O
Daniel O
Cer O
, O
Amin O
Ahmad O
, O
Mandy O
Guo O
, O
Jax O
Law O
, O
Noah O
Constant O
, O
Gustavo O
Hern O
¬¥ O
andez O
¬¥ O
Abrego O
, O
Steve O
Yuan O
, O
Chris O
Tar O
, O
YunHsuan O
Sung O
, O
Brian O
Strope O
, O
and O
Ray O
Kurzweil O
. O
2019b O
. O
Multilingual O
universal O
sentence O
encoder O
for O
semantic O
retrieval O
. O
arXiv O
preprint O
arXiv:1907.04307 O
. O
Yinfei O
Yang O
, O
Steve O
Yuan O
, O
Daniel O
Cer O
, O
Sheng O
- O
yi O
Kong O
, O
Noah O
Constant O
, O
Petr O
Pilar O
, O
Heming O
Ge O
, O
Yun O
- O
Hsuan O
Sung O
, O
Brian O
Strope O
, O
and O
Ray O
Kurzweil O
. O
2018 O
. O
Learning O
semantic O
textual O
similarity O
from O
conversations O
. O
InProceedings O
of O
The O
Third O
Workshop O
on O
Representation O
Learning O
for O
NLP O
, O
pages O
164‚Äì174 O
, O
Melbourne O
, O
Australia O
. O
Association O
for O
Computational O
Linguistics.445Boliang O
Zhang O
, O
Ajay O
Nagesh O
, O
and O
Kevin O
Knight O
. O
2020 O
. O
Parallel O
corpus O
Ô¨Åltering O
via O
pre O
- O
trained O
language O
models O
. O
arXiv:2005.06166 O
. O
Yuan O
Zhang O
, O
Jason O
Riesa O
, O
Daniel O
Gillick O
, O
Anton O
Bakalov O
, O
Jason O
Baldridge O
, O
and O
David O
Weiss O
. O
2018 O
. O
A O
fast O
, O
compact O
, O
accurate O
model O
for O
language O
identiÔ¨Åcation O
of O
codemixed O
text O
. O
In O
Proceedings O
of O
the O
2018 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
328‚Äì337 O
, O
Brussels O
, O
Belgium O
. O
Association O
for O
Computational O
Linguistics O
. O
Pierre O
Zweigenbaum O
, O
Serge O
Sharoff O
, O
and O
Reinhard O
Rapp O
. O
2018 O
. O
Overview O
of O
the O
third O
bucc O
shared O
task O
: O
Spotting O
parallel O
sentences O
in O
comparable O
corpora O
. O
InProceedings O
of O
the O
Eleventh O
International O
Conference O
on O
Language O
Resources O
and O
Evaluation O
( O
LREC O
2018 O
) O
, O
Paris O
, O
France O
. O
European O
Language O
Resources O
Association O
( O
ELRA).446Proceedings O
of O
the O
1st O
Conference O
of O
the O
Asia O
- O
PaciÔ¨Åc O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
10th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
447‚Äì459 O
December O
4 O
- O
7 O
, O
2020 O
. O
¬© O
2020 O
Association O
for O
Computational O
Linguistics O
A O
Survey O
of O
the O
State O
of O
Explainable O
AI O
for O
Natural O
Language O
Processing O
Marina O
Danilevsky O
, O
Kun O
Qian O
, O
Ranit O
Aharonov O
, O
Yannis O
Katsis O
, O
Ban O
Kawas O
, O
Prithviraj O
Sen O
IBM O
Research O
‚Äì O
Almadem O
mdanile@us.ibm.com O
, O
{ O
qian.kun O
, O
Ranit O
. O
Aharonov2 O
} O
@ibm.com O
yannis.katsis@ibm.com O
, O
{ O
bkawas O
, O
senp O
} O
@us.ibm.com O
Abstract O
Recent O
years O
have O
seen O
important O
advances O
in O
the O
quality O
of O
state O
- O
of O
- O
the O
- O
art O
models O
, O
but O
this O
has O
come O
at O
the O
expense O
of O
models O
becoming O
less O
interpretable O
. O
This O
survey O
presents O
an O
overview O
of O
the O
current O
state O
of O
Explainable O
AI O
( O
XAI O
) O
, O
considered O
within O
the O
domain O
of O
Natural O
Language O
Processing O
( O
NLP O
) O
. O
We O
discuss O
the O
main O
categorization O
of O
explanations O
, O
as O
well O
as O
the O
various O
ways O
explanations O
can O
be O
arrived O
at O
and O
visualized O
. O
We O
detail O
the O
operations O
and O
explainability O
techniques O
currently O
available O
for O
generating O
explanations O
for O
NLP O
model O
predictions O
, O
to O
serve O
as O
a O
resource O
for O
model O
developers O
in O
the O
community O
. O
Finally O
, O
we O
point O
out O
the O
current O
gaps O
and O
encourage O
directions O
for O
future O
work O
in O
this O
important O
research O
area O
. O
1 O
Introduction O
Traditionally O
, O
Natural O
Language O
Processing O
( O
NLP O
) O
systems O
have O
been O
mostly O
based O
on O
techniques O
that O
are O
inherently O
explainable O
. O
Examples O
of O
such O
approaches O
, O
often O
referred O
to O
as O
white O
box O
techniques O
, O
include O
rules O
, O
decision O
trees O
, O
hidden O
Markov O
models O
, O
logistic O
regressions O
, O
and O
others O
. O
Recent O
years O
, O
though O
, O
have O
brought O
the O
advent O
and O
popularity O
of O
black O
box O
techniques O
, O
such O
as O
deep O
learning O
models O
and O
the O
use O
of O
language O
embeddings O
as O
features O
. O
While O
these O
methods O
in O
many O
cases O
substantially O
advance O
model O
quality O
, O
they O
come O
at O
the O
expense O
of O
models O
becoming O
less O
interpretable O
. O
This O
obfuscation O
of O
the O
process O
by O
which O
a O
model O
arrives O
at O
its O
results O
can O
be O
problematic O
, O
as O
it O
may O
erode O
trust O
in O
the O
many O
AI O
systems O
humans O
interact O
with O
daily O
( O
e.g. O
, O
chatbots O
, O
recommendation O
systems O
, O
information O
retrieval O
algorithms O
, O
and O
many O
others O
) O
. O
In O
the O
broader O
AI O
community O
, O
this O
growing O
understanding O
of O
the O
importance O
of O
explainability O
has O
created O
an O
emerging O
Ô¨Åeld O
called O
Explainable O
AI O
( O
XAI O
) O
. O
However O
, O
just O
as O
tasks O
in O
different O
Ô¨Åelds O
are O
more O
amenable O
to O
particular O
approaches O
, O
explainabilitymust O
also O
be O
considered O
within O
the O
context O
of O
each O
discipline O
. O
We O
therefore O
focus O
this O
survey O
on O
XAI O
works O
in O
the O
domain O
of O
NLP O
, O
as O
represented O
in O
the O
main O
NLP O
conferences O
in O
the O
last O
seven O
years O
. O
This O
is O
, O
to O
the O
best O
of O
our O
knowledge O
, O
the O
Ô¨Årst O
XAI O
survey O
focusing O
on O
the O
NLP O
domain O
. O
As O
will O
become O
clear O
in O
this O
survey O
, O
explainability O
is O
in O
itself O
a O
term O
that O
requires O
an O
explanation O
. O
While O
explainability O
may O
generally O
serve O
many O
purposes O
( O
see O
, O
e.g. O
, O
Lertvittayakumjorn O
and O
Toni O
, O
2019 O
) O
, O
our O
focus O
is O
on O
explainability O
from O
the O
perspective O
of O
an O
end O
user O
whose O
goal O
is O
to O
understand O
how O
a O
model O
arrives O
at O
its O
result O
, O
also O
referred O
to O
as O
theoutcome O
explanation O
problem O
( O
Guidotti O
et O
al O
. O
, O
2018 O
) O
. O
In O
this O
regard O
, O
explanations O
can O
help O
users O
of O
NLP O
- O
based O
AI O
systems O
build O
trust O
in O
these O
systems O
‚Äô O
predictions O
. O
Additionally O
, O
understanding O
the O
model O
‚Äôs O
operation O
may O
also O
allow O
users O
to O
provide O
useful O
feedback O
, O
which O
in O
turn O
can O
help O
developers O
improve O
model O
quality O
( O
Adadi O
and O
Berrada O
, O
2018 O
) O
. O
Explanations O
of O
model O
predictions O
have O
previously O
been O
categorized O
in O
a O
fairly O
simple O
way O
that O
differentiates O
between O
( O
1 O
) O
whether O
the O
explanation O
is O
for O
each O
prediction O
individually O
or O
the O
model O
‚Äôs O
prediction O
process O
as O
a O
whole O
, O
and O
( O
2 O
) O
determining O
whether O
generating O
the O
explanation O
requires O
post O
- O
processing O
or O
not O
( O
see O
Section O
3 O
) O
. O
However O
, O
although O
rarely O
studied O
, O
there O
are O
many O
additional O
characterizations O
of O
explanations O
, O
the O
most O
important O
being O
the O
techniques O
used O
to O
either O
generate O
or O
visualize O
explanations O
. O
In O
this O
survey O
, O
we O
analyze O
the O
NLP O
literature O
with O
respect O
to O
both O
these O
dimensions O
and O
identify O
the O
most O
commonly O
used O
explainability O
and O
visualization O
techniques O
, O
in O
addition O
to O
operations O
used O
to O
generate O
explanations O
( O
Sections O
4.1 O
- O
Section O
4.3 O
) O
. O
We O
brieÔ¨Çy O
describe O
each O
technique O
and O
point O
to O
representative O
papers O
adopting O
it O
. O
Finally O
, O
we O
discuss O
the O
common O
evaluation O
techniques O
used O
to O
measure O
the O
quality O
of O
explanations O
( O
Section O
5 O
) O
, O
and O
conclude O
with O
a O
discussion O
of O
gaps O
and O
challenges O
in O
developing O
success-447ful O
explainability O
approaches O
in O
the O
NLP O
domain O
( O
Section O
6 O
) O
. O
Related O
Surveys O
: O
Earlier O
surveys O
on O
XAI O
include O
Adadi O
and O
Berrada O
( O
2018 O
) O
and O
Guidotti O
et O
al O
. O
( O
2018 O
) O
. O
While O
Adadi O
and O
Berrada O
provide O
a O
comprehensive O
review O
of O
basic O
terminology O
and O
fundamental O
concepts O
relevant O
to O
XAI O
in O
general O
, O
our O
goal O
is O
to O
survey O
more O
recent O
works O
in O
NLP O
in O
an O
effort O
to O
understand O
how O
these O
achieve O
XAI O
and O
how O
well O
they O
achieve O
it O
. O
Guidotti O
et O
al O
. O
adopt O
a O
four O
dimensional O
classiÔ¨Åcation O
scheme O
to O
rate O
various O
approaches O
. O
Crucially O
, O
they O
differentiate O
between O
the O
‚Äú O
explanator O
‚Äù O
and O
the O
black O
- O
box O
model O
it O
explains O
. O
This O
makes O
most O
sense O
when O
a O
surrogate O
model O
is O
used O
to O
explain O
a O
black O
- O
box O
model O
. O
As O
we O
shall O
subsequently O
see O
, O
such O
a O
distinction O
applies O
less O
well O
to O
the O
majority O
of O
NLP O
works O
published O
in O
the O
past O
few O
years O
where O
the O
same O
neural O
network O
( O
NN O
) O
can O
be O
used O
not O
only O
to O
make O
predictions O
but O
also O
to O
derive O
explanations O
. O
In O
a O
series O
of O
tutorials O
, O
Lecue O
et O
al O
. O
( O
2020 O
) O
discuss O
fairness O
and O
trust O
in O
machine O
learning O
( O
ML O
) O
that O
are O
clearly O
related O
to O
XAI O
but O
not O
the O
focus O
of O
this O
survey O
. O
Finally O
, O
we O
adapt O
some O
nomenclature O
from O
Arya O
et O
al O
. O
( O
2019 O
) O
which O
presents O
a O
software O
toolkit O
that O
can O
help O
users O
lend O
explainability O
to O
their O
models O
and O
ML O
pipelines O
. O
Our O
goal O
for O
this O
survey O
is O
to O
: O
( O
1 O
) O
provide O
the O
reader O
with O
a O
better O
understanding O
of O
the O
state O
of O
XAI O
in O
NLP O
, O
( O
2 O
) O
point O
developers O
interested O
in O
building O
explainable O
NLP O
models O
to O
currently O
available O
techniques O
, O
and O
( O
3 O
) O
bring O
to O
the O
attention O
of O
the O
research O
community O
the O
gaps O
that O
exist O
; O
mainly O
a O
lack O
of O
formal O
deÔ¨Ånitions O
and O
evaluation O
for O
explainability O
. O
We O
have O
also O
built O
an O
interactive O
website O
providing O
interested O
readers O
with O
all O
relevant O
aspects O
for O
every O
paper O
covered O
in O
this O
survey.1 O
2 O
Methodology O
We O
identiÔ¨Åed O
relevant O
papers O
( O
see O
Appendix O
A O
) O
and O
classiÔ¨Åed O
them O
based O
on O
the O
aspects O
deÔ¨Åned O
in O
Sections O
3 O
and O
4 O
. O
To O
ensure O
a O
consistent O
classiÔ¨Åcation O
, O
each O
paper O
was O
individually O
analyzed O
by O
at O
least O
two O
reviewers O
, O
consulting O
additional O
reviewers O
in O
the O
case O
of O
disagreement O
. O
For O
simplicity O
of O
presentation O
, O
we O
label O
each O
paper O
with O
its O
main O
applicable O
category O
for O
each O
aspect O
, O
though O
some O
papers O
may O
span O
multiple O
categories O
( O
usually O
with O
varying O
degrees O
of O
emphasis O
. O
) O
All O
relevant O
aspects O
for O
every O
1https://xainlp2020.github.io/xainlp/ O
( O
we O
plan O
to O
maintain O
this O
website O
as O
a O
contribution O
to O
the O
community.)paper O
covered O
in O
this O
survey O
can O
be O
found O
at O
the O
aforementioned O
website O
; O
to O
enable O
readers O
of O
this O
survey O
to O
discover O
interesting O
explainability O
techniques O
and O
ideas O
, O
even O
if O
they O
have O
not O
been O
fully O
developed O
in O
the O
respective O
publications O
. O
3 O
Categorization O
of O
Explanations O
Explanations O
are O
often O
categorized O
along O
two O
main O
aspects O
( O
Guidotti O
et O
al O
. O
, O
2018 O
; O
Adadi O
and O
Berrada O
, O
2018 O
) O
. O
The O
Ô¨Årst O
distinguishes O
whether O
the O
explanation O
is O
for O
an O
individual O
prediction O
( O
local O
) O
or O
the O
model O
‚Äôs O
prediction O
process O
as O
a O
whole O
( O
global O
) O
. O
The O
second O
differentiates O
between O
the O
explanation O
emerging O
directly O
from O
the O
prediction O
process O
( O
selfexplaining O
) O
versus O
requiring O
post O
- O
processing O
( O
posthoc O
) O
. O
We O
next O
describe O
both O
of O
these O
aspects O
in O
detail O
, O
and O
provide O
a O
summary O
of O
the O
four O
categories O
they O
induce O
in O
Table O
1 O
. O
3.1 O
Local O
vs O
Global O
Alocal O
explanation O
provides O
information O
or O
justiÔ¨Åcation O
for O
the O
model O
‚Äôs O
prediction O
on O
a O
speciÔ¨Åc O
input O
; O
46 O
of O
the O
50 O
papers O
fall O
into O
this O
category O
. O
Aglobal O
explanation O
provides O
similar O
justiÔ¨Åcation O
by O
revealing O
how O
the O
model O
‚Äôs O
predictive O
process O
works O
, O
independently O
of O
any O
particular O
input O
. O
This O
category O
holds O
the O
remaining O
4 O
papers O
covered O
by O
this O
survey O
. O
This O
low O
number O
is O
not O
surprising O
given O
the O
focus O
of O
this O
survey O
being O
on O
explanations O
that O
justify O
predictions O
, O
as O
opposed O
to O
explanations O
that O
help O
understand O
a O
model O
‚Äôs O
behavior O
in O
general O
( O
which O
lie O
outside O
the O
scope O
of O
this O
survey O
) O
. O
3.2 O
Self O
- O
Explaining O
vs O
Post O
- O
Hoc O
Regardless O
of O
whether O
the O
explanation O
is O
local O
or O
global O
, O
explanations O
differ O
on O
whether O
they O
arise O
as O
part O
of O
the O
prediction O
process O
, O
or O
whether O
their O
generation O
requires O
post O
- O
processing O
following O
the O
model O
making O
a O
prediction O
. O
A O
self O
- O
explaining O
approach O
, O
which O
may O
also O
be O
referred O
to O
as O
directly O
interpretable O
( O
Arya O
et O
al O
. O
, O
2019 O
) O
, O
generates O
the O
explanation O
at O
the O
same O
time O
as O
the O
prediction O
, O
using O
information O
emitted O
by O
the O
model O
as O
a O
result O
of O
the O
process O
of O
making O
that O
prediction O
. O
Decision O
trees O
and O
rule O
- O
based O
models O
are O
examples O
of O
global O
self O
- O
explaining O
models O
, O
while O
feature O
saliency O
approaches O
such O
as O
attention O
are O
examples O
of O
local O
self O
- O
explaining O
models O
. O
In O
contrast O
, O
a O
post O
- O
hoc O
approach O
requires O
that O
an O
additional O
operation O
is O
performed O
after O
the O
predictions O
are O
made O
. O
LIME O
( O
Ribeiro O
et O
al O
. O
, O
2016 O
) O
is448an O
example O
of O
producing O
a O
local O
explanation O
using O
a O
surrogate O
model O
applied O
following O
the O
predictor O
‚Äôs O
operation O
. O
A O
paper O
might O
also O
be O
considered O
to O
span O
both O
categories O
‚Äì O
for O
example O
, O
( O
Sydorova O
et O
al O
. O
, O
2019 O
) O
actually O
presents O
both O
self O
- O
explaining O
and O
post O
- O
hoc O
explanation O
techniques O
. O
Local O
Post O
- O
HocExplain O
a O
single O
prediction O
by O
performing O
additional O
operations O
( O
after O
the O
model O
has O
emitted O
a O
prediction O
) O
Local O
SelfExplainingExplain O
a O
single O
prediction O
using O
the O
model O
itself O
( O
calculated O
from O
information O
made O
available O
from O
the O
model O
as O
part O
of O
making O
the O
prediction O
) O
Global O
Post O
- O
HocPerform O
additional O
operations O
to O
explain O
the O
entire O
model O
‚Äôs O
predictive O
reasoning O
Global O
SelfExplainingUse O
the O
predictive O
model O
itself O
to O
explain O
the O
entire O
model O
‚Äôs O
predictive O
reasoning O
( O
a.k.a O
. O
directly O
interpretable O
model O
) O
Table O
1 O
: O
Overview O
of O
the O
high O
- O
level O
categories O
of O
explanations O
( O
Section O
3 O
) O
. O
4 O
Aspects O
of O
Explanations O
While O
the O
previous O
categorization O
serves O
as O
a O
convenient O
high O
- O
level O
classiÔ¨Åcation O
of O
explanations O
, O
it O
does O
not O
cover O
other O
important O
characteristics O
. O
We O
now O
introduce O
two O
additional O
aspects O
of O
explanations O
: O
( O
1 O
) O
techniques O
for O
deriving O
the O
explanation O
and O
( O
2 O
) O
presentation O
to O
the O
end O
user O
. O
We O
discuss O
the O
most O
commonly O
used O
explainability O
techniques O
, O
along O
with O
basic O
operations O
that O
enable O
explainability O
, O
as O
well O
as O
the O
visualization O
techniques O
commonly O
used O
to O
present O
the O
output O
of O
associated O
explainability O
techniques O
. O
We O
identify O
the O
most O
common O
combinations O
of O
explainability O
techniques O
, O
operations O
, O
and O
visualization O
techniques O
for O
each O
of O
the O
four O
high O
- O
level O
categories O
of O
explanations O
presented O
above O
, O
and O
summarize O
them O
, O
together O
with O
representative O
papers O
, O
in O
Table O
2 O
. O
Although O
explainability O
techniques O
and O
visualizations O
are O
often O
intermixed O
, O
there O
are O
fundamental O
differences O
between O
them O
that O
motivated O
us O
to O
treat O
them O
separately O
. O
Concretely O
, O
explanation O
derivation O
- O
typically O
done O
by O
AI O
scientists O
and O
engineers O
- O
focuses O
on O
mathematically O
motivated O
justiÔ¨Åcations O
of O
models O
‚Äô O
output O
, O
leveraging O
various O
explainability O
techniques O
to O
produce O
‚Äú O
raw O
explanations O
‚Äù O
( O
such O
as O
attention O
scores O
) O
. O
On O
the O
other O
hand O
, O
explanation O
presentation O
- O
ideally O
done O
by O
UX O
engineers O
focuses O
on O
how O
these O
‚Äú O
raw O
explanations O
‚Äù O
are O
best O
presented O
to O
the O
end O
users O
using O
suitable O
visualization O
techniques O
( O
such O
as O
saliency O
heatmaps).4.1 O
Explainability O
Techniques O
In O
the O
papers O
surveyed O
, O
we O
identiÔ¨Åed O
Ô¨Åve O
major O
explainability O
techniques O
that O
differ O
in O
the O
mechanisms O
they O
adopt O
to O
generate O
the O
raw O
mathematical O
justiÔ¨Åcations O
that O
lead O
to O
the O
Ô¨Ånal O
explanation O
presented O
to O
the O
end O
users O
. O
Feature O
importance O
. O
The O
main O
idea O
is O
to O
derive O
explanation O
by O
investigating O
the O
importance O
scores O
of O
different O
features O
used O
to O
output O
the O
Ô¨Ånal O
prediction O
. O
Such O
approaches O
can O
be O
built O
on O
different O
types O
of O
features O
, O
such O
as O
manual O
features O
obtained O
from O
feature O
engineering O
( O
e.g. O
, O
V O
oskarides O
et O
al O
. O
, O
2015 O
) O
, O
lexical O
features O
including O
word O
/ O
tokens O
and O
n O
- O
gram O
( O
e.g. O
, O
Godin O
et O
al O
. O
, O
2018 O
; O
Mullenbach O
et O
al O
. O
, O
2018 O
) O
, O
or O
latent O
features O
learned O
by O
NNs O
( O
e.g. O
, O
Xie O
et O
al O
. O
, O
2017 O
) O
. O
Attention O
mechanism O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
) O
and O
Ô¨Årst O
- O
derivative O
saliency O
( O
Li O
et O
al O
. O
, O
2015 O
) O
are O
two O
widely O
used O
operations O
to O
enable O
feature O
importance O
- O
based O
explanations O
. O
Text O
- O
based O
features O
are O
inherently O
more O
interpretable O
by O
humans O
than O
general O
features O
, O
which O
may O
explain O
the O
widespread O
use O
of O
attention O
- O
based O
approaches O
in O
the O
NLP O
domain O
. O
Surrogate O
model O
. O
Model O
predictions O
are O
explained O
by O
learning O
a O
second O
, O
usually O
more O
explainable O
model O
, O
as O
a O
proxy O
. O
One O
well O
- O
known O
example O
is O
LIME O
( O
Ribeiro O
et O
al O
. O
, O
2016 O
) O
, O
which O
learns O
surrogate O
models O
using O
an O
operation O
called O
input O
perturbation O
. O
Surrogate O
model O
- O
based O
approaches O
are O
model O
- O
agnostic O
and O
can O
be O
used O
to O
achieve O
either O
local O
( O
e.g. O
, O
Alvarez O
- O
Melis O
and O
Jaakkola O
, O
2017 O
) O
or O
global O
( O
e.g. O
, O
Liu O
et O
al O
. O
, O
2018 O
) O
explanations O
. O
However O
, O
the O
learned O
surrogate O
models O
and O
the O
original O
models O
may O
have O
completely O
different O
mechanisms O
to O
make O
predictions O
, O
leading O
to O
concerns O
about O
the O
Ô¨Ådelity O
of O
surrogate O
model O
- O
based O
approaches O
. O
Example O
- O
driven O
. O
Such O
approaches O
explain O
the O
prediction O
of O
an O
input O
instance O
by O
identifying O
and O
presenting O
other O
instances O
, O
usually O
from O
available O
labeled O
data O
, O
that O
are O
semantically O
similar O
to O
the O
input O
instance O
. O
They O
are O
similar O
in O
spirit O
to O
nearest O
neighbor O
- O
based O
approaches O
( O
Dudani O
, O
1976 O
) O
, O
and O
have O
been O
applied O
to O
different O
NLP O
tasks O
such O
as O
text O
classiÔ¨Åcation O
( O
Croce O
et O
al O
. O
, O
2019 O
) O
and O
question O
answering O
( O
Abujabal O
et O
al O
. O
, O
2017 O
) O
. O
Provenance O
- O
based O
. O
Explanations O
are O
provided O
by O
illustrating O
some O
or O
all O
of O
the O
prediction O
derivation O
process O
, O
which O
is O
an O
intuitive O
and O
effective O
explainability O
technique O
when O
the O
Ô¨Ånal O
prediction O
is O
the O
result O
of O
a O
series O
of O
reasoning O
steps O
. O
We O
observe O
several O
question O
answering O
papers O
adopt O
such O
ap-449Category O
Explainability O
Operations O
to O
Visualization O
Representative O
( O
# O
) O
Technique O
Enable O
Explainability O
Technique O
# O
Paper(s O
) O
Local O
Post O
- O
Hoc O
( O
11)feature O
importanceÔ¨Årst O
derivative O
saliency O
, O
example O
drivensaliency O
5 O
( O
Wallace O
et O
al O
. O
, O
2018 O
; O
Ross O
et O
al O
. O
, O
2017 O
) O
surrogate O
model O
Ô¨Årst O
derivative O
saliency O
, O
layer O
- O
wise O
relevance O
propagation O
, O
input O
perturbationsaliency O
4 O
( O
Alvarez O
- O
Melis O
and O
Jaakkola O
, O
2017 O
; O
Poerner O
et O
al O
. O
, O
2018 O
; O
Ribeiro O
et O
al O
. O
, O
2016 O
) O
example O
driven O
layer O
- O
wise O
relevance O
propagation O
, O
explainability O
- O
aware O
architectureraw O
examples O
2 O
( O
Croce O
et O
al O
. O
, O
2018 O
; O
Jiang O
et O
al O
. O
, O
2019 O
) O
Local O
Self O
- O
Exp O
( O
35)feature O
importanceattention O
, O
Ô¨Årst O
derivative O
saliency O
, O
LSTM O
gating O
signals O
, O
explainabilityaware O
architecturesaliency O
22 O
( O
Mullenbach O
et O
al O
. O
, O
2018 O
; O
Ghaeini O
et O
al O
. O
, O
2018 O
; O
Xie O
et O
al O
. O
, O
2017 O
; O
Aubakirova O
and O
Bansal O
, O
2016 O
) O
induction O
explainability O
- O
aware O
architecture O
, O
rule O
inductionraw O
declarative O
representation6 O
( O
Ling O
et O
al O
. O
, O
2017 O
; O
Dong O
et O
al O
. O
, O
2019 O
; O
Pezeshkpour O
et O
al O
. O
, O
2019a O
) O
provenance O
template O
- O
based O
natural O
language O
, O
other3 O
( O
Abujabal O
et O
al O
. O
, O
2017 O
) O
surrogate O
model O
attention O
, O
input O
perturbation O
, O
explainability O
- O
aware O
architecturenatural O
language3 O
( O
Rajani O
et O
al O
. O
, O
2019a O
; O
Sydorova O
et O
al O
. O
, O
2019 O
) O
example O
driven O
layer O
- O
wise O
relevance O
propagation O
raw O
examples O
1 O
( O
Croce O
et O
al O
. O
, O
2019 O
) O
Global O
Post O
- O
Hoc O
( O
3)feature O
importanceclass O
activation O
mapping O
, O
attention O
, O
gradient O
reversalsaliency O
2 O
( O
Pryzant O
et O
al O
. O
, O
2018a O
, O
b O
) O
surrogate O
model O
taxonomy O
induction O
raw O
declarative O
representation1 O
( O
Liu O
et O
al O
. O
, O
2018 O
) O
Global O
Self O
- O
Exp O
( O
1)induction O
reinforcement O
learning O
raw O
declarative O
representation1 O
( O
Pr O
¬®ollochs O
et O
al O
. O
, O
2019 O
) O
Table O
2 O
: O
Overview O
of O
common O
combinations O
of O
explanation O
aspects O
: O
columns O
2 O
, O
3 O
, O
and O
4 O
capture O
explainability O
techniques O
, O
operations O
, O
and O
visualization O
techniques O
, O
respectively O
( O
see O
Sections O
4.1 O
, O
4.2 O
, O
and O
4.3 O
for O
details O
) O
. O
These O
are O
grouped O
by O
the O
high O
- O
level O
categories O
detailed O
in O
Section O
3 O
, O
as O
shown O
in O
the O
Ô¨Årst O
column O
. O
The O
last O
two O
columns O
show O
the O
number O
of O
papers O
in O
this O
survey O
that O
fall O
within O
each O
subgroup O
, O
and O
a O
list O
of O
representative O
references O
. O
proaches O
( O
Abujabal O
et O
al O
. O
, O
2017 O
; O
Zhou O
et O
al O
. O
, O
2018 O
; O
Amini O
et O
al O
. O
, O
2019 O
) O
. O
Declarative O
induction O
. O
Human O
- O
readable O
representations O
, O
such O
as O
rules O
( O
Pr O
¬®ollochs O
et O
al O
. O
, O
2019 O
) O
, O
trees O
( O
V O
oskarides O
et O
al O
. O
, O
2015 O
) O
, O
and O
programs O
( O
Ling O
et O
al O
. O
, O
2017 O
) O
are O
induced O
as O
explanations O
. O
As O
shown O
in O
Table O
2 O
, O
feature O
importance O
- O
based O
and O
surrogate O
model O
- O
based O
approaches O
have O
been O
in O
frequent O
use O
( O
accounting O
for O
29 O
and O
8 O
, O
respectively O
, O
of O
the O
50 O
papers O
reviewed O
) O
. O
This O
should O
not O
come O
as O
a O
surprise O
, O
as O
features O
serve O
as O
building O
blocks O
for O
machine O
learning O
models O
( O
explaining O
the O
proliferation O
of O
feature O
importance O
- O
based O
approaches O
) O
and O
most O
recent O
NLP O
papers O
employ O
NNbased O
models O
, O
which O
are O
generally O
black O
- O
box O
models O
( O
explaining O
the O
popularity O
of O
surrogate O
modelbased O
approaches O
) O
. O
Finally O
note O
that O
a O
complex O
NLP O
approach O
consisting O
of O
different O
componentsmay O
employ O
more O
than O
one O
of O
these O
explainability O
techniques O
. O
A O
representative O
example O
is O
the O
QA O
system O
QUINT O
( O
Abujabal O
et O
al O
. O
, O
2017 O
) O
, O
which O
displays O
the O
query O
template O
that O
best O
matches O
the O
user O
input O
query O
( O
example O
- O
driven O
) O
as O
well O
as O
the O
instantiated O
knowledge O
- O
base O
entities O
( O
provenance O
) O
. O
4.2 O
Operations O
to O
Enable O
Explainability O
We O
now O
present O
the O
most O
common O
set O
of O
operations O
encountered O
in O
our O
literature O
review O
that O
are O
used O
to O
enable O
explainability O
, O
in O
conjunction O
with O
relevant O
work O
employing O
each O
one O
. O
First O
- O
derivative O
saliency O
. O
Gradient O
- O
based O
explanations O
estimate O
the O
contribution O
of O
input O
itowards O
output O
oby O
computing O
the O
partial O
derivative O
ofowith O
respect O
to O
i. O
This O
is O
closely O
related O
to O
older O
concepts O
such O
as O
sensitivity O
( O
Saltelli O
et O
al O
. O
, O
2008 O
) O
. O
First O
- O
derivative O
saliency O
is O
particularly O
con-450venient O
for O
NN O
- O
based O
models O
because O
these O
can O
be O
computed O
for O
any O
layer O
using O
a O
single O
call O
to O
auto O
- O
differentiation O
, O
which O
most O
deep O
learning O
engines O
provide O
out O
- O
of O
- O
the O
- O
box O
. O
Recent O
work O
has O
also O
proposed O
improvements O
to O
Ô¨Årst O
- O
derivative O
saliency O
( O
Sundararajan O
et O
al O
. O
, O
2017 O
) O
. O
As O
suggested O
by O
its O
name O
and O
deÔ¨Ånition O
, O
Ô¨Årst O
- O
derivative O
saliency O
can O
be O
used O
to O
enable O
feature O
importance O
explainability O
, O
especially O
on O
word O
/ O
token O
- O
level O
features O
( O
Aubakirova O
and O
Bansal O
, O
2016 O
; O
Karlekar O
et O
al O
. O
, O
2018 O
) O
. O
Layer O
- O
wise O
relevance O
propagation O
. O
This O
is O
another O
way O
to O
attribute O
relevance O
to O
features O
computed O
in O
any O
intermediate O
layer O
of O
an O
NN O
. O
DeÔ¨Ånitions O
are O
available O
for O
most O
common O
NN O
layers O
including O
fully O
connected O
layers O
, O
convolution O
layers O
and O
recurrent O
layers O
. O
Layer O
- O
wise O
relevance O
propagation O
has O
been O
used O
to O
, O
for O
example O
, O
enable O
feature O
importance O
explainability O
( O
Poerner O
et O
al O
. O
, O
2018 O
) O
and O
example O
- O
driven O
explainability O
( O
Croce O
et O
al O
. O
, O
2018 O
) O
. O
Input O
perturbations O
. O
Pioneered O
by O
LIME O
( O
Ribeiro O
et O
al O
. O
, O
2016 O
) O
, O
input O
perturbations O
can O
explain O
the O
output O
for O
input O
xby O
generating O
random O
perturbations O
of O
xand O
training O
an O
explainable O
model O
( O
usually O
a O
linear O
model O
) O
. O
They O
are O
mainly O
used O
to O
enable O
surrogate O
models O
( O
e.g. O
, O
Ribeiro O
et O
al O
. O
, O
2016 O
; O
Alvarez O
- O
Melis O
and O
Jaakkola O
, O
2017 O
) O
. O
Attention O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
; O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O
Less O
an O
operation O
and O
more O
of O
a O
strategy O
to O
enable O
the O
NN O
to O
explain O
predictions O
, O
attention O
layers O
can O
be O
added O
to O
most O
NN O
architectures O
and O
, O
because O
they O
appeal O
to O
human O
intuition O
, O
can O
help O
indicate O
where O
the O
NN O
model O
is O
‚Äú O
focusing O
‚Äù O
. O
While O
previous O
work O
has O
widely O
used O
attention O
layers O
( O
Luo O
et O
al O
. O
, O
2018 O
; O
Xie O
et O
al O
. O
, O
2017 O
; O
Mullenbach O
et O
al O
. O
, O
2018 O
) O
to O
enable O
feature O
importance O
explainability O
, O
the O
jury O
is O
still O
out O
as O
to O
how O
much O
explainability O
attention O
provides O
( O
Jain O
and O
Wallace O
, O
2019 O
; O
Serrano O
and O
Smith O
, O
2019 O
; O
Wiegreffe O
and O
Pinter O
, O
2019 O
) O
. O
LSTM O
gating O
signals O
. O
Given O
the O
sequential O
nature O
of O
language O
, O
recurrent O
layers O
, O
in O
particular O
LSTMs O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
, O
are O
commonplace O
. O
While O
it O
is O
common O
to O
mine O
the O
outputs O
of O
LSTM O
cells O
to O
explain O
outputs O
, O
there O
may O
also O
be O
information O
present O
in O
the O
outputs O
of O
the O
gates O
produced O
within O
the O
cells O
. O
It O
is O
possible O
to O
utilize O
( O
and O
even O
combine O
) O
other O
operations O
presented O
here O
to O
interpret O
gating O
signals O
to O
aid O
feature O
importance O
explainability O
( O
Ghaeini O
et O
al O
. O
, O
2018 O
) O
. O
Explainability O
- O
aware O
architecture O
design O
. O
One O
way O
to O
exploit O
the O
Ô¨Çexibility O
of O
deep O
learning O
is O
to O
devise O
an O
NN O
architecture O
that O
mimics O
the O
processhumans O
employ O
to O
arrive O
at O
a O
solution O
. O
This O
makes O
the O
learned O
model O
( O
partially O
) O
interpretable O
since O
the O
architecture O
contains O
human O
- O
recognizable O
components O
. O
Implementing O
such O
a O
model O
architecture O
can O
be O
used O
to O
enable O
the O
induction O
of O
human O
- O
readable O
programs O
for O
solving O
math O
problems O
( O
Amini O
et O
al O
. O
, O
2019 O
; O
Ling O
et O
al O
. O
, O
2017 O
) O
or O
sentence O
simpliÔ¨Åcation O
problems O
( O
Dong O
et O
al O
. O
, O
2019 O
) O
. O
This O
design O
may O
also O
be O
applied O
to O
surrogate O
models O
that O
generate O
explanations O
for O
predictions O
( O
Rajani O
et O
al O
. O
, O
2019a O
; O
Liu O
et O
al O
. O
, O
2019 O
) O
. O
Previous O
works O
have O
also O
attempted O
to O
compare O
these O
operations O
in O
terms O
of O
efÔ¨Åcacy O
with O
respect O
to O
speciÔ¨Åc O
NLP O
tasks O
( O
Poerner O
et O
al O
. O
, O
2018 O
) O
. O
Operations O
outside O
of O
this O
list O
exist O
and O
are O
popular O
for O
particular O
categories O
of O
explanations O
. O
Table O
2 O
mentions O
some O
of O
these O
. O
For O
instance O
, O
Pr O
¬®ollochs O
et O
al O
. O
( O
2019 O
) O
use O
reinforcement O
learning O
to O
learn O
simple O
negation O
rules O
, O
Liu O
et O
al O
. O
( O
2018 O
) O
learns O
a O
taxonomy O
post O
- O
hoc O
to O
better O
interpret O
network O
embeddings O
, O
and O
Pryzant O
et O
al O
. O
( O
2018b O
) O
uses O
gradient O
reversal O
( O
Ganin O
et O
al O
. O
, O
2016 O
) O
to O
deconfound O
lexicons O
. O
4.3 O
Visualization O
Techniques O
An O
explanation O
may O
be O
presented O
in O
different O
ways O
to O
the O
end O
user O
, O
and O
making O
the O
appropriate O
choice O
is O
crucial O
for O
the O
overall O
success O
of O
an O
XAI O
approach O
. O
For O
example O
, O
the O
widely O
used O
attention O
mechanism O
, O
which O
learns O
the O
importance O
scores O
of O
a O
set O
of O
features O
, O
can O
be O
visualized O
as O
raw O
attention O
scores O
or O
as O
a O
saliency O
heatmap O
( O
see O
Figure O
1a O
) O
. O
Although O
the O
former O
is O
acceptable O
, O
the O
latter O
is O
more O
user O
- O
friendly O
and O
has O
become O
the O
standard O
way O
to O
visualize O
attention O
- O
based O
approaches O
. O
We O
now O
present O
the O
major O
visualization O
techniques O
identiÔ¨Åed O
in O
our O
literature O
review O
. O
Saliency O
. O
This O
has O
been O
primarily O
used O
to O
visualize O
the O
importance O
scores O
of O
different O
types O
of O
elements O
in O
XAI O
learning O
systems O
, O
such O
as O
showing O
input O
- O
output O
word O
alignment O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
) O
( O
Figure O
1a O
) O
, O
highlighting O
words O
in O
input O
text O
( O
Mullenbach O
et O
al O
. O
, O
2018 O
) O
( O
Figure O
1b O
) O
or O
displaying O
extracted O
relations O
( O
Xie O
et O
al O
. O
, O
2017 O
) O
. O
We O
observe O
a O
strong O
correspondence O
between O
feature O
importancebased O
explainability O
and O
saliency O
- O
based O
visualizations O
; O
namely O
, O
all O
papers O
using O
feature O
importance O
to O
generate O
explanations O
also O
chose O
saliency O
- O
based O
visualization O
techniques O
. O
Saliency O
- O
based O
visualizations O
are O
popular O
because O
they O
present O
visually O
perceptive O
explanations O
and O
can O
be O
easily O
understood O
by O
different O
types O
of O
end O
users O
. O
They O
are O
there-451(a O
) O
Saliency O
heatmap O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
) O
( O
b O
) O
Saliency O
highlighting O
( O
Mullenbach O
et O
al O
. O
, O
2018 O
) O
( O
c O
) O
Raw O
declarative O
rules O
( O
Pezeshkpour O
et O
al O
. O
, O
2019b O
) O
( O
d O
) O
Raw O
declarative O
program O
( O
Amini O
et O
al O
. O
, O
2019 O
) O
  O
( O
e O
) O
Raw O
examples O
( O
Croce O
et O
al O
. O
, O
2019 O
) O
Figure O
1 O
: O
Examples O
of O
different O
visualization O
techniques O
fore O
frequently O
seen O
across O
different O
AI O
domains O
( O
e.g. O
, O
computer O
vision O
( O
Simonyan O
et O
al O
. O
, O
2013 O
) O
and O
speech O
( O
Aldeneh O
and O
Provost O
, O
2017 O
) O
) O
. O
As O
shown O
in O
Table O
2 O
, O
saliency O
is O
the O
most O
dominant O
visualization O
technique O
among O
the O
papers O
covered O
by O
this O
survey O
. O
Raw O
declarative O
representations O
. O
As O
suggested O
by O
its O
name O
, O
this O
visualization O
technique O
directly O
presents O
the O
learned O
declarative O
representations O
, O
such O
as O
logic O
rules O
, O
trees O
, O
and O
programs O
( O
Figure O
1c O
and O
1d O
) O
. O
Such O
techniques O
assume O
that O
end O
users O
can O
understand O
speciÔ¨Åc O
representations O
, O
such O
as O
Ô¨Årstorder O
logic O
rules O
( O
Pezeshkpour O
et O
al O
. O
, O
2019a O
) O
and O
reasoning O
trees O
( O
Liang O
et O
al O
. O
, O
2016 O
) O
, O
and O
therefore O
may O
implicitly O
target O
more O
advanced O
users O
. O
Natural O
language O
explanation O
. O
The O
explanation O
is O
verbalized O
in O
human O
- O
comprehensible O
natural O
language O
( O
Figure O
2 O
) O
. O
The O
natural O
language O
can O
be O
generated O
using O
sophisticated O
deep O
learning O
models O
, O
e.g. O
, O
by O
training O
a O
language O
model O
with O
human O
natural O
language O
explanations O
and O
coupling O
with O
a O
deep O
generative O
model O
( O
Rajani O
et O
al O
. O
, O
2019a O
) O
. O
It O
can O
also O
be O
generated O
by O
using O
simple O
templatebased O
approaches O
( O
Abujabal O
et O
al O
. O
, O
2017 O
) O
. O
In O
fact O
, O
many O
declarative O
induction O
- O
based O
techniques O
can O
use O
template O
- O
based O
natural O
language O
generation O
( O
Reiter O
and O
Dale O
, O
1997 O
) O
to O
turn O
rules O
and O
programs O
into O
human O
- O
comprehensible O
language O
, O
and O
this O
minor O
extension O
can O
potentially O
make O
the O
explanation O
more O
accessible O
to O
lay O
users O
. O
Table O
2 O
references O
some O
additional O
visualization O
techniques O
, O
such O
as O
using O
raw O
examples O
to O
Figure O
2 O
: O
Template O
- O
based O
natural O
language O
explanation O
for O
a O
QA O
system O
( O
Abujabal O
et O
al O
. O
, O
2017 O
) O
. O
present O
example O
- O
driven O
approaches O
( O
Jiang O
et O
al O
. O
, O
2019 O
; O
Croce O
et O
al O
. O
, O
2019 O
) O
( O
e.g. O
, O
Figure O
1e O
) O
, O
and O
dependency O
parse O
trees O
to O
represent O
input O
questions O
( O
Abujabal O
et O
al O
. O
, O
2017 O
) O
. O
5 O
Explanation O
Quality O
Following O
the O
goals O
of O
XAI O
, O
a O
model O
‚Äôs O
quality O
should O
be O
evaluated O
not O
only O
by O
its O
accuracy O
and O
performance O
, O
but O
also O
by O
how O
well O
it O
provides O
explanations O
for O
its O
predictions O
. O
In O
this O
section O
we O
discuss O
the O
state O
of O
the O
Ô¨Åeld O
in O
terms O
of O
deÔ¨Åning O
and O
measuring O
explanation O
quality O
. O
5.1 O
Evaluation O
Given O
the O
young O
age O
of O
the O
Ô¨Åeld O
, O
unsurprisingly O
there O
is O
little O
agreement O
on O
how O
explanations O
should O
be O
evaluated O
. O
The O
majority O
of O
the O
works O
reviewed O
( O
32 O
out O
of O
50 O
) O
either O
lack O
a O
standardized O
evaluation O
or O
include O
only O
an O
informal O
evaluation O
, O
while O
a O
smaller O
number O
of O
papers O
looked O
at O
more O
formal O
evaluation O
approaches O
, O
including O
leveraging O
ground O
truth O
data O
and O
human O
evaluation O
. O
We O
next O
present O
the O
major O
categories O
of O
evaluation O
tech-452niques O
we O
encountered O
( O
summarized O
in O
Table O
3 O
) O
. O
None O
or O
Informal O
Comparison O
to O
Human O
Examination O
only O
Ground O
Truth O
Evaluation O
32 O
12 O
9 O
Table O
3 O
: O
Common O
evaluation O
techniques O
and O
number O
of O
papers O
adopting O
them O
, O
out O
of O
the O
50 O
papers O
surveyed O
( O
note O
that O
some O
papers O
adopt O
more O
than O
one O
technique O
) O
Informal O
examination O
of O
explanations O
. O
This O
typically O
takes O
the O
form O
of O
high O
- O
level O
discussions O
of O
how O
examples O
of O
generated O
explanations O
align O
with O
human O
intuition O
. O
This O
includes O
cases O
where O
the O
output O
of O
a O
single O
explainability O
approach O
is O
examined O
in O
isolation O
( O
Xie O
et O
al O
. O
, O
2017 O
) O
as O
well O
as O
when O
explanations O
are O
compared O
to O
those O
of O
other O
reference O
approaches O
( O
Ross O
et O
al O
. O
, O
2017 O
) O
( O
such O
as O
LIME O
, O
which O
is O
a O
frequently O
used O
baseline O
) O
. O
Comparison O
to O
ground O
truth O
. O
Several O
works O
compare O
generated O
explanations O
to O
ground O
truth O
data O
in O
order O
to O
quantify O
the O
performance O
of O
explainability O
techniques O
. O
Employed O
metrics O
vary O
based O
on O
task O
and O
explainability O
technique O
, O
but O
commonly O
encountered O
metrics O
include O
P O
/ O
R O
/ O
F1 O
( O
Carton O
et O
al O
. O
, O
2018 O
) O
, O
perplexity O
, O
and O
BLEU O
( O
Ling O
et O
al O
. O
, O
2017 O
; O
Rajani O
et O
al O
. O
, O
2019b O
) O
. O
While O
having O
a O
quantitative O
way O
to O
measure O
explainability O
is O
a O
promising O
direction O
, O
care O
should O
be O
taken O
during O
ground O
truth O
acquisition O
to O
ensure O
its O
quality O
and O
account O
for O
cases O
where O
there O
may O
be O
alternative O
valid O
explanations O
. O
Approaches O
employed O
to O
address O
this O
issue O
involve O
having O
multiple O
annotators O
and O
reporting O
inter O
- O
annotator O
agreement O
or O
mean O
human O
performance O
, O
as O
well O
as O
evaluating O
the O
explanations O
at O
different O
granularities O
( O
e.g. O
, O
token O
- O
wise O
vs O
phrasewise O
) O
to O
account O
for O
disagreements O
on O
the O
precise O
value O
of O
the O
ground O
truth O
( O
Carton O
et O
al O
. O
, O
2018 O
) O
. O
Human O
evaluation O
. O
A O
more O
direct O
way O
to O
assess O
the O
explanation O
quality O
is O
to O
ask O
humans O
to O
evaluate O
the O
effectiveness O
of O
the O
generated O
explanations O
. O
This O
has O
the O
advantage O
of O
avoiding O
the O
assumption O
that O
there O
is O
only O
one O
good O
explanation O
that O
could O
serve O
as O
ground O
truth O
, O
as O
well O
as O
sidestepping O
the O
need O
to O
measure O
similarity O
of O
explanations O
. O
Here O
as O
well O
, O
it O
is O
important O
to O
have O
multiple O
annotators O
, O
report O
inter O
- O
annotator O
agreement O
, O
and O
correctly O
deal O
with O
subjectivity O
and O
variance O
in O
the O
responses O
. O
The O
approaches O
found O
in O
this O
survey O
vary O
in O
several O
dimensions O
, O
including O
the O
number O
of O
humans O
involved O
( O
ranging O
from O
1 O
( O
Mullenbach O
et O
al O
. O
, O
2018 O
) O
to O
25 O
( O
Sydorova O
et O
al O
. O
, O
2019 O
) O
humans O
) O
, O
as O
well O
as O
thehigh O
- O
level O
task O
that O
they O
were O
asked O
to O
perform O
( O
including O
rating O
the O
explanations O
of O
a O
single O
approach O
( O
Dong O
et O
al O
. O
, O
2019 O
) O
and O
comparing O
explanations O
of O
multiple O
techniques O
( O
Sydorova O
et O
al O
. O
, O
2019 O
) O
) O
. O
Other O
operation O
- O
speciÔ¨Åc O
techniques O
. O
Given O
the O
prevalence O
of O
attention O
layers O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
; O
Vaswani O
et O
al O
. O
, O
2017 O
) O
in O
NLP O
, O
recent O
work O
( O
Jain O
and O
Wallace O
, O
2019 O
; O
Serrano O
and O
Smith O
, O
2019 O
; O
Wiegreffe O
and O
Pinter O
, O
2019 O
) O
has O
developed O
speciÔ¨Åc O
techniques O
to O
evaluate O
such O
explanations O
based O
on O
counterfactuals O
or O
erasure O
- O
based O
tests O
( O
Feng O
et O
al O
. O
, O
2018 O
) O
. O
Serrano O
and O
Smith O
repeatedly O
set O
to O
zero O
the O
maximal O
entry O
produced O
by O
the O
attention O
layer O
. O
If O
attention O
weights O
indeed O
‚Äú O
explain O
‚Äù O
the O
output O
prediction O
, O
then O
turning O
off O
the O
dominant O
weights O
should O
result O
in O
an O
altered O
prediction O
. O
Similar O
experiments O
have O
been O
devised O
by O
others O
( O
Jain O
and O
Wallace O
, O
2019 O
) O
. O
In O
particular O
, O
Wiegreffe O
and O
Pinter O
caution O
against O
assuming O
that O
there O
exists O
only O
one O
true O
explanation O
to O
suggest O
accounting O
for O
the O
natural O
variance O
of O
attention O
layers O
. O
On O
a O
broader O
note O
, O
causality O
has O
thoroughly O
explored O
such O
counterfactualbased O
notions O
of O
explanation O
( O
Halpern O
, O
2016 O
) O
. O
While O
the O
above O
overview O
summarizes O
how O
explainability O
approaches O
are O
commonly O
evaluated O
, O
another O
important O
aspect O
is O
what O
is O
being O
evaluated O
. O
Explanations O
are O
multi O
- O
faceted O
objects O
that O
can O
be O
evaluated O
on O
multiple O
aspects O
, O
such O
as O
Ô¨Ådelity O
( O
how O
much O
they O
reÔ¨Çect O
the O
actual O
workings O
of O
the O
underlying O
model O
) O
, O
comprehensibility O
( O
how O
easy O
they O
are O
to O
understand O
by O
humans O
) O
, O
and O
others O
. O
Therefore O
, O
understanding O
the O
target O
of O
the O
evaluation O
is O
important O
for O
interpreting O
the O
evaluation O
results O
. O
We O
refer O
interested O
readers O
to O
( O
Carvalho O
et O
al O
. O
, O
2019 O
) O
for O
a O
comprehensive O
presentation O
of O
aspects O
of O
evaluating O
approaches O
. O
Many O
works O
do O
not O
explicitly O
state O
what O
is O
being O
evaluated O
. O
As O
a O
notable O
exception O
, O
( O
Lertvittayakumjorn O
and O
Toni O
, O
2019 O
) O
outlines O
three O
goals O
of O
explanations O
( O
reveal O
model O
behavior O
, O
justify O
model O
predictions O
, O
and O
assist O
humans O
in O
investigating O
uncertain O
predictions O
) O
and O
proposes O
human O
evaluation O
experiments O
targeting O
each O
of O
them O
. O
5.2 O
Predictive O
Process O
Coverage O
An O
important O
and O
often O
overlooked O
aspect O
of O
explanation O
quality O
is O
the O
part O
of O
the O
prediction O
process O
( O
starting O
with O
the O
input O
and O
ending O
with O
the O
model O
output O
) O
covered O
by O
an O
explanation O
. O
We O
have O
observed O
that O
many O
explainability O
approaches O
explain O
only O
part O
of O
this O
process O
, O
leaving O
it O
up O
to O
the O
end453user O
to O
Ô¨Åll O
in O
the O
gaps O
. O
As O
an O
example O
, O
consider O
the O
MathQA O
task O
of O
solving O
math O
word O
problems O
. O
As O
readers O
may O
be O
familiar O
from O
past O
education O
experience O
, O
in O
math O
exams O
, O
one O
is O
often O
asked O
to O
provide O
a O
step O
- O
by O
- O
step O
explanation O
of O
how O
the O
answer O
was O
derived O
. O
Usually O
, O
full O
credit O
is O
not O
given O
if O
any O
of O
the O
critical O
steps O
used O
in O
the O
derivation O
are O
missing O
. O
Recent O
works O
have O
studied O
the O
explainability O
of O
MathQA O
models O
, O
which O
seek O
to O
reproduce O
this O
process O
( O
Amini O
et O
al O
. O
, O
2019 O
; O
Ling O
et O
al O
. O
, O
2017 O
) O
, O
and O
have O
employed O
different O
approaches O
in O
the O
type O
of O
explanations O
produced O
. O
While O
( O
Amini O
et O
al O
. O
, O
2019 O
) O
explains O
the O
predicted O
answer O
by O
showing O
the O
sequence O
of O
mathematical O
operations O
leading O
to O
it O
, O
this O
provides O
only O
partial O
coverage O
, O
as O
it O
does O
not O
explain O
how O
these O
operations O
were O
derived O
from O
the O
input O
text O
. O
On O
the O
other O
hand O
, O
the O
explanations O
produced O
by O
( O
Ling O
et O
al O
. O
, O
2017 O
) O
augment O
the O
mathematical O
formulas O
with O
text O
describing O
the O
thought O
process O
behind O
the O
derived O
solution O
, O
thus O
covering O
a O
bigger O
part O
of O
the O
prediction O
process O
. O
The O
level O
of O
coverage O
may O
be O
an O
artifact O
of O
explainability O
techniques O
used O
: O
provenance O
- O
based O
approaches O
tend O
to O
provide O
more O
coverage O
, O
while O
example O
- O
driven O
approaches O
, O
may O
provide O
little O
to O
no O
coverage O
. O
Moreover O
, O
while O
our O
math O
teacher O
would O
argue O
that O
providing O
higher O
coverage O
is O
always O
beneÔ¨Åcial O
to O
the O
student O
, O
in O
reality O
this O
may O
depend O
on O
the O
end O
use O
of O
the O
explanation O
. O
For O
instance O
, O
the O
coverage O
of O
explanations O
of O
( O
Amini O
et O
al O
. O
, O
2019 O
) O
may O
be O
potentially O
sufÔ¨Åcient O
for O
advanced O
technical O
users O
. O
Thus O
, O
higher O
coverage O
, O
while O
in O
general O
a O
positive O
aspect O
, O
should O
always O
be O
considered O
in O
combination O
with O
the O
target O
use O
and O
audience O
of O
the O
produced O
explanations O
. O
6 O
Insights O
and O
Future O
Directions O
This O
survey O
showcases O
recent O
advances O
of O
XAI O
research O
in O
NLP O
, O
as O
evidenced O
by O
publications O
in O
major O
NLP O
conferences O
in O
the O
last O
7 O
years O
. O
We O
have O
discussed O
the O
main O
categorization O
of O
explanations O
( O
Local O
vs O
Global O
, O
Self O
- O
Explaining O
vs O
Post O
- O
Hoc O
) O
as O
well O
as O
the O
various O
ways O
explanations O
can O
be O
arrived O
at O
and O
visualized O
, O
together O
with O
the O
common O
techniques O
used O
. O
We O
have O
also O
detailed O
operations O
and O
explainability O
techniques O
currently O
available O
for O
generating O
explanations O
of O
model O
predictions O
, O
in O
the O
hopes O
of O
serving O
as O
a O
resource O
for O
developers O
interested O
in O
building O
explainable O
NLP O
models O
. O
We O
hope O
this O
survey O
encourages O
the O
researchcommunity O
to O
work O
in O
bridging O
the O
current O
gaps O
in O
the O
Ô¨Åeld O
of O
XAI O
in O
NLP O
. O
The O
Ô¨Årst O
research O
direction O
is O
a O
need O
for O
clearer O
terminology O
and O
understanding O
of O
what O
constitutes O
explainability O
and O
how O
it O
connects O
to O
the O
target O
audience O
. O
For O
example O
, O
is O
a O
model O
that O
displays O
an O
induced O
program O
that O
, O
when O
executed O
, O
yields O
a O
prediction O
, O
and O
yet O
conceals O
the O
process O
of O
inducing O
the O
program O
, O
explainable O
in O
general O
? O
Or O
is O
it O
explainable O
for O
some O
target O
users O
but O
not O
for O
others O
? O
The O
second O
is O
an O
expansion O
of O
the O
evaluation O
processes O
and O
metrics O
, O
especially O
for O
human O
evaluation O
. O
The O
Ô¨Åeld O
of O
XAI O
is O
aimed O
at O
adding O
explainability O
as O
a O
desired O
feature O
of O
models O
, O
in O
addition O
to O
the O
model O
‚Äôs O
predictive O
quality O
, O
and O
other O
features O
such O
as O
runtime O
performance O
, O
complexity O
or O
memory O
usage O
. O
In O
general O
, O
trade O
- O
offs O
exist O
between O
desired O
characteristics O
of O
models O
, O
such O
as O
more O
complex O
models O
achieving O
better O
predictive O
power O
at O
the O
expense O
of O
slower O
runtime O
. O
In O
XAI O
, O
some O
works O
have O
claimed O
that O
explainability O
may O
come O
at O
the O
price O
of O
losing O
predictive O
quality O
( O
Bertsimas O
et O
al O
. O
, O
2019 O
) O
, O
while O
other O
have O
claimed O
the O
opposite O
( O
Garneau O
et O
al O
. O
, O
2018 O
; O
Liang O
et O
al O
. O
, O
2016 O
) O
. O
Studying O
such O
possible O
trade O
- O
offs O
is O
an O
important O
research O
area O
for O
XAI O
, O
but O
one O
that O
can O
not O
advance O
until O
standardized O
metrics O
are O
developed O
for O
evaluating O
the O
quality O
of O
explanations O
. O
The O
third O
research O
direction O
is O
a O
call O
to O
more O
critically O
address O
the O
issue O
of O
Ô¨Ådelity O
( O
or O
causality O
) O
, O
and O
to O
ask O
hard O
questions O
about O
whether O
a O
claimed O
explanation O
is O
faithfully O
explaining O
the O
model O
‚Äôs O
prediction O
. O
Finally O
, O
it O
is O
interesting O
to O
note O
that O
we O
found O
only O
four O
papers O
that O
fall O
into O
the O
global O
explanations O
category O
. O
This O
might O
seem O
surprising O
given O
that O
white O
box O
models O
, O
which O
have O
been O
fundamental O
in O
NLP O
, O
are O
explainable O
in O
the O
global O
sense O
. O
We O
believe O
this O
stems O
from O
the O
fact O
that O
because O
white O
box O
models O
are O
clearly O
explainable O
, O
the O
focus O
of O
the O
explicit O
XAI O
Ô¨Åeld O
is O
in O
explaining O
black O
box O
models O
, O
which O
comprise O
mostly O
local O
explanations O
. O
White O
box O
models O
, O
like O
rule O
based O
models O
and O
decision O
trees O
, O
while O
still O
in O
use O
, O
are O
less O
frequently O
framed O
as O
explainable O
or O
interpretable O
, O
and O
are O
hence O
not O
the O
main O
thrust O
of O
where O
the O
Ô¨Åeld O
is O
going O
. O
We O
think O
that O
this O
may O
be O
an O
oversight O
of O
the O
Ô¨Åeld O
since O
white O
box O
models O
can O
be O
a O
great O
test O
bed O
for O
studying O
techniques O
for O
evaluating O
explanations O
. O
Acknowledgments O
We O
thank O
the O
anonymous O
reviewers O
for O
their O
valuable O
feedback O
. O
We O
also O
thank O
Shipi O
Dhanorkar,454Yunyao O
Li O
, O
Lucian O
Popa O
, O
Christine O
T O
Wolf O
, O
and O
Anbang O
Xu O
for O
their O
efforts O
at O
the O
early O
stage O
of O
this O
work O
. O
Abstract O
Fine O
- O
tuning O
( O
FT O
) O
pre O
- O
trained O
sentence O
embedding O
models O
on O
small O
datasets O
has O
been O
shown O
to O
have O
limitations O
. O
In O
this O
paper O
we O
show O
that O
concatenating O
the O
embeddings O
from O
the O
pretrained O
model O
with O
those O
from O
a O
simple O
sentence O
embedding O
model O
trained O
only O
on O
the O
target O
data O
, O
can O
improve O
over O
the O
performance O
of O
FT O
for O
few O
- O
sample O
tasks O
. O
To O
this O
end O
, O
a O
linear O
classiÔ¨Åer O
is O
trained O
on O
the O
combined O
embeddings O
, O
either O
by O
freezing O
the O
embedding O
model O
weights O
or O
training O
the O
classiÔ¨Åer O
and O
embedding O
models O
end O
- O
to O
- O
end O
. O
We O
perform O
evaluation O
on O
seven O
small O
datasets O
from O
NLP O
tasks O
and O
show O
that O
our O
approach O
with O
end O
- O
to O
- O
end O
training O
outperforms O
FT O
with O
negligible O
computational O
overhead O
. O
Further O
, O
we O
also O
show O
that O
sophisticated O
combination O
techniques O
like O
CCA O
and O
KCCA O
do O
not O
work O
as O
well O
in O
practice O
as O
concatenation O
. O
We O
provide O
theoretical O
analysis O
to O
explain O
this O
empirical O
observation O
. O
1 O
Introduction O
Fine O
- O
tuning O
( O
FT O
) O
powerful O
pre O
- O
trained O
sentence O
embedding O
models O
like O
BERT O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
has O
recently O
become O
the O
de O
- O
facto O
standard O
for O
downstream O
NLP O
tasks O
. O
Typically O
, O
FT O
entails O
jointly O
learning O
a O
classiÔ¨Åer O
over O
the O
pre O
- O
trained O
model O
while O
tuning O
the O
weights O
of O
the O
latter O
. O
While O
FT O
has O
been O
shown O
to O
improve O
performance O
on O
tasks O
like O
GLUE O
( O
Wang O
et O
al O
. O
, O
2018 O
) O
having O
large O
datasets O
( O
QQP O
, O
MNLI O
, O
QNLI O
) O
, O
similar O
trends O
have O
not O
been O
observed O
on O
small O
datasets O
, O
where O
one O
would O
expect O
the O
maximum O
beneÔ¨Åts O
of O
using O
a O
pre O
- O
trained O
model O
. O
Several O
works O
( O
Phang O
et O
al O
. O
, O
2018 O
; O
Garg O
et O
al O
. O
, O
2019 O
; O
Dodge O
et O
al O
. O
, O
2020 O
; O
Lee O
et O
al O
. O
, O
2020 O
) O
have O
demonstrated O
that O
FT O
with O
a O
few O
target O
domain O
samples O
is O
unstable O
with O
high O
variance O
, O
thereby O
often O
leading O
to O
sub O
- O
par O
gains O
. O
Furthermore O
, O
this O
‚àóEqual O
contribution O
by O
authors O
‚Ä†Work O
completed O
at O
the O
University O
of O
Wisconsin O
- O
Madisonissue O
has O
also O
been O
well O
documented O
in O
practice1 O
. O
Learning O
with O
low O
resources O
has O
recently O
become O
an O
active O
research O
area O
in O
NLP O
, O
and O
arguably O
one O
of O
the O
most O
interesting O
scenarios O
for O
which O
pre O
- O
trained O
models O
are O
useful O
( O
e.g. O
, O
( O
Cherry O
et O
al O
. O
, O
2019 O
) O
) O
. O
Many O
practical O
applications O
have O
small O
datasets O
( O
e.g. O
, O
in O
social O
science O
, O
medical O
studies O
, O
etc O
) O
, O
which O
are O
different O
from O
large O
- O
scale O
academic O
benchmarks O
having O
hundreds O
of O
thousands O
of O
training O
samples O
( O
e.g O
, O
DBpedia O
( O
Lehmann O
et O
al O
. O
, O
2015 O
) O
, O
Sogou O
News O
( O
Wang O
et O
al O
. O
, O
2008 O
) O
, O
etc O
) O
. O
This O
necessitates O
effective O
transfer O
learning O
approaches O
using O
pre O
- O
trained O
sentence O
embedding O
models O
for O
fewsample O
tasks O
. O
In O
this O
work O
, O
we O
show O
that O
concatenating O
sentence O
embeddings O
from O
a O
pre O
- O
trained O
model O
and O
those O
from O
a O
smaller O
model O
trained O
solely O
on O
the O
target O
data O
, O
can O
improve O
over O
the O
performance O
of O
FT O
. O
SpeciÔ¨Åcally O
, O
we O
Ô¨Årst O
learn O
a O
simple O
sentence O
embedding O
model O
on O
the O
target O
data O
. O
Then O
we O
concatenate O
( O
CAT O
) O
the O
embeddings O
from O
this O
model O
with O
those O
from O
a O
pre O
- O
trained O
model O
, O
and O
train O
a O
linear O
classiÔ¨Åer O
on O
the O
combined O
representation O
. O
The O
latter O
can O
be O
done O
by O
either O
freezing O
the O
embedding O
model O
weights O
or O
training O
the O
whole O
network O
( O
classiÔ¨Åer O
plus O
the O
two O
embedding O
models O
) O
end O
- O
to O
- O
end O
. O
We O
evaluate O
our O
approach O
on O
seven O
small O
datasets O
from O
NLP O
tasks O
. O
Our O
results O
show O
that O
our O
approach O
with O
end O
- O
to O
- O
end O
training O
can O
signiÔ¨Åcantly O
improve O
the O
prediction O
performance O
of O
FT O
, O
with O
less O
than O
a O
10 O
% O
increase O
in O
the O
run O
time O
. O
Furthermore O
, O
our O
approach O
with O
frozen O
embedding O
models O
performs O
better O
than O
FT O
for O
very O
small O
datasets O
while O
reducing O
the O
run O
time O
by O
30%‚àí50 O
% O
, O
and O
without O
the O
requirement O
of O
large O
memory O
GPUs O
. O
We O
also O
conduct O
evaluations O
of O
multiple O
techniques O
for O
combining O
the O
pre O
- O
trained O
and O
domainspeciÔ¨Åc O
embeddings O
, O
comparing O
concatenation O
to O
1Issues O
numbered O
265 O
, O
1211 O
onhttps://github.com/huggi O
ngface O
/ O
transformers O
/ O
issues/460CCA O
and O
KCCA O
. O
We O
observe O
that O
the O
simplest O
approach O
of O
concatenation O
works O
best O
in O
practice O
. O
Moreover O
, O
we O
provide O
theoretical O
analysis O
to O
explain O
this O
empirical O
observation O
. O
Finally O
, O
our O
results O
also O
have O
implications O
on O
the O
semantics O
learning O
ability O
of O
small O
domainspeciÔ¨Åc O
models O
compared O
to O
large O
pre O
- O
trained O
models O
. O
While O
intuition O
dictates O
that O
a O
large O
pre O
- O
trained O
model O
should O
capture O
the O
entire O
semantics O
learned O
by O
a O
small O
domain O
- O
speciÔ¨Åc O
model O
, O
our O
results O
show O
that O
there O
exist O
semantic O
features O
captured O
solely O
by O
the O
latter O
and O
not O
by O
the O
former O
, O
in O
spite O
of O
pretraining O
on O
billions O
of O
words O
. O
Hence O
combining O
the O
embeddings O
can O
improve O
the O
performance O
of O
directly O
FT O
the O
pre O
- O
trained O
model O
. O
Related O
Work O
Recently O
, O
several O
pre O
- O
trained O
models O
have O
been O
studied O
, O
of O
which O
some O
provide O
explicit O
sentence O
embeddings O
( O
Conneau O
et O
al O
. O
, O
2017 O
; O
Subramanian O
et O
al O
. O
, O
2018 O
) O
, O
while O
others O
provide O
implicit O
ones O
( O
Howard O
and O
Ruder O
, O
2018 O
; O
Radford O
et O
al O
. O
, O
2018 O
) O
. O
Peters O
et O
al O
. O
( O
2019 O
) O
compare O
the O
performance O
of O
feature O
extraction O
( O
by O
freezing O
the O
pre O
- O
trained O
weights O
) O
and O
FT O
. O
There O
exists O
other O
more O
sophisticated O
transferring O
methods O
, O
but O
they O
are O
typically O
much O
more O
expensive O
or O
complicated O
. O
For O
example O
, O
Xu O
et O
al O
. O
( O
2019 O
) O
‚Äú O
post O
- O
train O
‚Äù O
the O
pretrained O
model O
on O
the O
target O
dataset O
, O
Houlsby O
et O
al O
. O
( O
2019 O
) O
inject O
speciÔ¨Åcally O
designed O
new O
adapter O
layers O
, O
Arase O
and O
Tsujii O
( O
2019 O
) O
inject O
phrasal O
paraphrase O
relations O
into O
BERT O
, O
Sun O
et O
al O
. O
( O
2019 O
) O
use O
multi O
- O
task O
FT O
, O
and O
Wang O
et O
al O
. O
( O
2019 O
) O
Ô¨Årst O
train O
a O
deep O
network O
classiÔ¨Åer O
on O
the O
Ô¨Åxed O
pre O
- O
trained O
embedding O
and O
then O
Ô¨Åne O
- O
tune O
it O
. O
Our O
focus O
is O
to O
propose O
alternatives O
to O
FT O
with O
similar O
simplicity O
and O
computational O
efÔ¨Åciency O
, O
and O
study O
conditions O
where O
it O
has O
signiÔ¨Åcant O
advantages O
. O
While O
the O
idea O
of O
concatenating O
multiple O
embeddings O
has O
been O
previously O
used O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
, O
we O
use O
it O
for O
transfer O
learning O
in O
a O
low O
resource O
target O
domain O
. O
2 O
Methodology O
We O
are O
given O
a O
set O
of O
labeled O
training O
sentences O
S={(si O
, O
yi)}m O
i=1from O
a O
target O
domain O
and O
a O
pretrained O
sentence O
embedding O
model O
f1 O
. O
Denote O
the O
embedding O
of O
sfromf1byv1s O
= O
f1(s)‚ààRd1 O
. O
Here O
f1is O
assumed O
to O
be O
a O
large O
and O
powerful O
embedding O
model O
such O
as O
BERT O
. O
Our O
goal O
is O
to O
transfer O
f1effectively O
to O
the O
target O
domain O
using O
S. O
We O
propose O
to O
use O
a O
second O
sentence O
embedding O
model O
f2 O
, O
which O
is O
different O
from O
and O
typically O
much O
smaller O
thanf1 O
, O
which O
has O
been O
trained O
solely O
on O
S. O
Thesmall O
size O
of O
f2is O
necessary O
for O
efÔ¨Åcient O
learning O
on O
the O
small O
target O
dataset O
. O
Let O
v2s O
= O
f2(s)‚ààRd2 O
denote O
the O
embedding O
for O
sobtained O
from O
f2 O
. O
Our O
method O
CATconcatenates O
v1sandv2sto O
get O
an O
adaptive O
representation O
¬Øvs=[v O
/ O
latticetop O
1s O
, O
Œ±v O
/ O
latticetop O
2s]/latticetopfors O
. O
HereŒ±>0is O
a O
hyper O
- O
parameter O
to O
modify O
emphasis O
onv1sandv2s O
. O
It O
then O
trains O
a O
linear O
classiÔ¨Åer O
c(¬Øvs O
) O
usingSin O
the O
following O
two O
ways O
: O
( O
a O
) O
Frozen O
Embedding O
Models O
·Ωë2Only O
training O
the O
classiÔ¨Åer O
cwhile O
Ô¨Åxing O
the O
weights O
of O
embedding O
models O
f1andf2 O
. O
This O
approach O
is O
computationally O
cheaper O
than O
FT O
f1since O
onlycis O
trained O
. O
We O
denote O
this O
by O
C O
AT·Ωë2(Lockedf1,f2weights O
) O
. O
( O
b O
) O
Trainable O
Embedding O
Models O
/lock O
- O
open O
Jointly O
training O
classiÔ¨Åer O
c O
, O
and O
embedding O
models O
f1,f2 O
in O
an O
end O
- O
to O
- O
end O
fashion O
. O
We O
refer O
to O
this O
as O
C O
AT O
/ O
lock O
- O
open O
. O
The O
inspiration O
for O
combining O
embeddings O
from O
two O
different O
models O
f1,f2stems O
from O
the O
impressive O
empirical O
gains O
of O
ensembling O
( O
Dietterich O
, O
2000 O
) O
in O
machine O
learning O
. O
While O
typical O
ensembling O
techniques O
like O
bagging O
and O
boosting O
aggregate O
predictions O
from O
individual O
models O
, O
C O
AT·Ωë2and O
CAT O
/ O
lock O
- O
openaggregate O
the O
embeddings O
from O
individual O
models O
and O
train O
a O
classiÔ¨Åer O
using O
Sto O
get O
the O
predictions O
. O
Note O
that O
C O
AT·Ωë2keeps O
the O
model O
weights O
off1,f2frozen O
, O
while O
C O
AT O
/ O
lock O
- O
openinitializes O
the O
weights O
off2after O
initially O
training O
on O
S2 O
. O
One O
of O
the O
beneÔ¨Åts O
of O
C O
AT·Ωë2and O
C O
AT O
/ O
lock O
- O
openis O
that O
they O
treatf1as O
a O
black O
box O
and O
do O
not O
access O
its O
internal O
architecture O
like O
other O
variants O
of O
FT O
( O
Houlsby O
et O
al O
. O
, O
2019 O
) O
. O
Additionally O
, O
we O
can O
theoretically O
guarantee O
that O
the O
concatenated O
embedding O
will O
generalize O
well O
to O
the O
target O
domain O
under O
assumptions O
on O
the O
loss O
function O
and O
embedding O
models O
. O
2.1 O
Theoretical O
Analysis O
Assume O
there O
exists O
a O
‚Äú O
ground O
- O
truth O
‚Äù O
embedding O
vectorv‚àó O
sfor O
each O
sentence O
swith O
labelys O
, O
and O
a O
‚Äú O
ground O
- O
truth O
‚Äù O
linear O
classiÔ¨Åer O
f‚àó(s)=/angbracketleftw‚àó,v‚àó O
s O
/ O
angbracketright O
with O
a O
small O
loss O
L(f‚àó)=Es[/lscript(f‚àó(s),ys)]w.r.t O
. O
some O
loss O
function O
/lscript(such O
as O
cross O
- O
entropy O
) O
, O
where O
Esdenotes O
the O
expectation O
over O
the O
true O
data O
distribution O
. O
The O
superior O
performance O
of O
C O
AT O
/ O
lock O
- O
openin O
practice O
( O
see O
Section O
3 O
) O
suggests O
that O
there O
exists O
a O
linear O
relationship O
between O
the O
embeddings O
v1s O
, O
v2s O
andv‚àó O
s. O
Thus O
we O
assume O
a O
theoretical O
model O
: O
v1s O
= O
P1v‚àó O
s+/epsilon11;v2s O
= O
P2v‚àó O
s+/epsilon12where O
/ O
epsilon1i O
‚Äôs O
are O
noises O
independent O
of O
v‚àó O
swith O
variances O
œÉ2 O
i O
‚Äôs O
. O
If O
we O
denoteP O
/ O
latticetop=[P O
/ O
latticetop O
1,P O
/ O
latticetop O
2]and O
/ O
epsilon1 O
/ O
latticetop=[/epsilon1 O
/ O
latticetop O
1,/epsilon1 O
/ O
latticetop O
2 O
] O
, O
then O
2We O
empirically O
observe O
that O
C O
AT O
/ O
lock O
- O
openby O
randomly O
initializing O
weights O
of O
f2performs O
similar O
to O
Ô¨Åne O
- O
tuning O
only O
f1461the O
concatenation O
¬Øvs=[v O
/ O
latticetop O
1s O
, O
v O
/ O
latticetop O
2s]/latticetopis¬Øvs O
= O
Pv‚àó O
s+/epsilon1 O
. O
LetœÉ=/radicalbig O
œÉ2 O
1+œÉ2 O
2 O
. O
We O
present O
the O
following O
theorem O
which O
guarantees O
the O
existence O
of O
a O
‚Äú O
good O
‚Äù O
classiÔ¨Åer O
¬Øfover¬Øvs O
: O
Theorem O
1 O
. O
If O
the O
loss O
function O
LisŒª O
- O
Lipschitz O
for O
the O
Ô¨Årst O
parameter O
, O
and O
Phas O
full O
column O
rank O
, O
then O
there O
exists O
a O
linear O
classiÔ¨Åer O
¬Øfover¬Øvssuch O
thatL(¬Øf)‚â§L(f‚àó O
) O
+ O
ŒªœÉ O
/ O
bardbl(P‚Ä†)/latticetopw‚àó/bardbl2whereP‚Ä†is O
the O
pseudo O
- O
inverse O
of O
P. O
Proof O
. O
Let¬Øfhave O
weight O
¬Øw= O
( O
P‚Ä†)/latticetopw‚àó. O
Then O
/angbracketleft¬Øw,¬Øvs O
/ O
angbracketright=/angbracketleft(P‚Ä†)/latticetopw‚àó,Pv‚àó O
s+/epsilon1 O
/ O
angbracketright O
= O
/angbracketleft(P‚Ä†)/latticetopw‚àó,Pv‚àó O
s O
/ O
angbracketright+/angbracketleft(P‚Ä†)/latticetopw‚àó,/epsilon1 O
/ O
angbracketright O
= O
/angbracketleftw‚àó,P‚Ä†Pv‚àó O
s O
/ O
angbracketright+/angbracketleft(P‚Ä†)/latticetopw‚àó,/epsilon1 O
/ O
angbracketright O
= O
/angbracketleftw‚àó,v‚àó O
s O
/ O
angbracketright+/angbracketleft(P‚Ä†)/latticetopw‚àó,/epsilon1 O
/ O
angbracketright O
. O
( O
1 O
) O
Then O
the O
difference O
in O
the O
losses O
is O
given O
by O
L(¬Øf)‚àíL(f‚àó O
) O
= O
Es[/lscript(¬Øf(s),ys)‚àí/lscript(f‚àó(s),ys O
) O
] O
‚â§ŒªEs|¬Øf(s)‚àíf‚àó(s)| O
( O
2 O
) O
= O
ŒªEs|/angbracketleft(P‚Ä†)/latticetopw‚àó,/epsilon1 O
/ O
angbracketright| O
. O
‚â§Œª O
/ O
radicalBig O
Es O
/ O
angbracketleft(P‚Ä†)/latticetopw‚àó,/epsilon1 O
/ O
angbracketright2 O
( O
3 O
) O
‚â§Œª O
/ O
radicalBig O
Es O
/ O
bardbl(P‚Ä†)/latticetopw‚àó/bardbl2 O
2 O
/ O
bardbl O
/ O
epsilon1 O
/ O
bardbl2 O
2 O
( O
4 O
) O
= O
ŒªœÉ O
/ O
bardbl(P‚Ä†)/latticetopw‚àó/bardbl2 O
where O
we O
use O
the O
Lipschitz O
- O
ness O
of O
Lin O
Equation O
2 O
, O
Jensen O
‚Äôs O
inequality O
in O
Equation O
3 O
, O
and O
CauchySchwarz O
inequality O
in O
Equation O
4 O
. O
More O
intuitively O
, O
if O
the O
SVD O
of O
P O
= O
UŒ£V O
/ O
latticetop O
, O
then O
/bardbl(P‚Ä†)/latticetopw‚àó/bardbl2=/bardbl(Œ£‚Ä†)/latticetopV O
/ O
latticetopw‚àó/bardbl2 O
. O
So O
if O
the O
top O
right O
singular O
vectors O
in O
Valign O
well O
with O
w‚àó O
, O
then O
/bardbl(P‚Ä†)/latticetopw‚àó/bardbl2will O
be O
small O
in O
magnitude O
. O
This O
means O
that O
if O
P1andP2together O
cover O
the O
directionw‚àó O
, O
they O
can O
capture O
information O
important O
for O
classiÔ¨Åcation O
. O
And O
thus O
there O
exists O
a O
good O
classiÔ¨Åer¬Øfon¬Øvs O
. O
Additional O
explanation O
is O
presented O
in O
Appendix O
A.1 O
. O
2.2 O
Do O
Other O
Combination O
Methods O
Work O
? O
There O
are O
several O
sophisticated O
techniques O
to O
combinev1sandv2sother O
than O
concatenation O
. O
Since O
v1sandv2smay O
be O
in O
different O
dimensions O
, O
a O
dimension O
reduction O
technique O
which O
projects O
them O
on O
the O
same O
dimensional O
space O
might O
work O
better O
at O
capturing O
the O
general O
and O
domain O
speciÔ¨Åc O
information O
. O
We O
consider O
two O
popular O
techniques O
: O
CCA O
Canonical O
Correlation O
Analysis O
( O
Hotelling O
, O
1936 O
) O
learns O
linear O
projections O
Œ¶1andŒ¶2into O
dimensiondto O
maximize O
the O
correlations O
betweenthe O
projections{Œ¶1v1si}and{Œ¶2v2si O
} O
. O
We O
use O
¬Øv O
/ O
latticetop O
s=1 O
2Œ¶1v1si+1 O
2Œ¶2v2siwithd= O
min{d1,d2 O
} O
. O
KCCA O
Kernel O
Canonical O
Correlation O
Analysis O
( O
Sch O
¬®olkopf O
et O
al O
. O
, O
1998 O
) O
Ô¨Årst O
applies O
nonlinear O
projections O
g1andg2and O
then O
CCA O
on O
{ O
g1(v1si)}m O
i=1and{g2(v2si)}m O
i=1 O
. O
We O
used= O
min{d1,d2}and¬Øv O
/ O
latticetop O
s=1 O
2g1(v1si O
) O
+1 O
2g2(v2si O
) O
. O
We O
empirically O
evaluate O
C O
CA·Ωë2and O
K O
CCA·Ωë2and O
our O
results O
( O
see O
Section O
3 O
) O
show O
that O
the O
former O
two O
perform O
worse O
than O
C O
AT·Ωë2 O
. O
Further O
, O
C O
CA·Ωë2performs O
even O
worse O
than O
the O
individual O
embedding O
models O
. O
This O
is O
a O
very O
interesting O
negative O
observation O
, O
and O
below O
we O
provide O
an O
explanation O
for O
this O
. O
We O
argue O
that O
even O
when O
v1sandv2scontain O
information O
important O
for O
classiÔ¨Åcation O
, O
CCA O
of O
the O
two O
embeddings O
can O
eliminate O
this O
and O
just O
retain O
the O
noise O
in O
the O
embeddings O
, O
thereby O
leading O
to O
inferior O
prediction O
performance O
. O
Theorem O
2 O
constructs O
such O
an O
example O
. O
Theorem O
2 O
. O
Let¬Øvsdenote O
the O
embedding O
for O
sentencesobtained O
by O
concatenation O
, O
and O
Àúvsdenote O
that O
obtained O
by O
CCA O
. O
There O
exists O
a O
setting O
of O
the O
data O
andw‚àó,P,/epsilon1 O
such O
that O
there O
exists O
a O
linear O
classiÔ¨Åer O
¬Øfon¬Øvswith O
the O
same O
loss O
as O
f‚àó O
, O
while O
CCA O
achieves O
the O
maximum O
correlation O
but O
any O
classiÔ¨Åer O
on O
Àúvsis O
at O
best O
random O
guessing O
. O
Proof O
. O
Suppose O
we O
perform O
CCA O
to O
get O
ddimensional O
Àúvs O
. O
Supposev‚àó O
shasd+ O
2dimensions O
, O
each O
dimension O
being O
an O
independent O
Gaussian O
. O
Supposew‚àó=[1,1,0 O
, O
... O
, O
0]/latticetop O
, O
and O
the O
label O
for O
the O
sentencesisys=1if O
/ O
angbracketleftw‚àó,v‚àó O
s O
/ O
angbracketright‚â•0andys=0otherwise O
. O
Suppose O
/epsilon1=0,P1 O
= O
diag(1,0,1 O
, O
... O
, O
1 O
) O
, O
and O
P2 O
= O
diag(0,1,1 O
, O
... O
, O
1 O
) O
. O
Let O
the O
linear O
classiÔ¨Åer O
¬Øfhave O
weights O
[ O
1,0,0,0,1,0]/latticetopwhere O
0is O
the O
zero O
vector O
ofddimensions O
. O
Clearly O
, O
¬Øf(s)=f‚àó(s)for O
anys O
, O
so O
it O
has O
the O
same O
loss O
as O
f‚àó. O
For O
CCA O
, O
since O
the O
coordinates O
of O
v‚àó O
sare O
independent O
Gaussians O
, O
v1sandv2sonly O
have O
correlation O
in O
the O
lastddimensions O
. O
Solving O
the O
CCA O
optimization O
, O
the O
projection O
matrices O
for O
both O
embeddings O
are O
the O
same O
œÜ O
= O
diag(0,0,1 O
, O
... O
, O
1 O
) O
which O
achieves O
the O
maximum O
correlation O
. O
Then O
the O
CCA O
embedding O
is O
Àúvs= O
[ O
0,0,(v‚àó O
s)3:(d+2)]where O
( O
v‚àó O
s)3:(d+2)are O
the O
lastddimensions O
of O
v‚àó O
s O
, O
which O
contains O
no O
information O
about O
the O
label O
. O
Therefore O
, O
any O
classiÔ¨Åer O
on O
Àúvsis O
at O
best O
random O
guessing O
. O
The O
intuition O
for O
this O
is O
that O
v1sandv2sshare O
com-462mon O
information O
while O
each O
has O
some O
special O
information O
for O
the O
classiÔ¨Åcation O
. O
If O
the O
two O
sets O
of O
special O
information O
are O
uncorrelated O
, O
then O
they O
will O
be O
eliminated O
by O
CCA O
. O
Now O
, O
if O
the O
common O
information O
is O
irrelevant O
to O
the O
labels O
, O
then O
the O
best O
any O
classiÔ¨Åer O
can O
do O
with O
the O
CCA O
embeddings O
is O
just O
random O
guessing O
. O
This O
is O
a O
fundamental O
drawback O
of O
the O
unsupervised O
CCA O
technique O
, O
clearly O
demonstrated O
by O
the O
extreme O
example O
in O
the O
theorem O
. O
In O
practice O
, O
the O
common O
information O
can O
contain O
some O
relevant O
information O
, O
so O
CCA O
embeddings O
are O
worse O
than O
concatenation O
but O
better O
than O
random O
guessing O
. O
KCCA O
can O
be O
viewed O
as O
CCA O
on O
a O
nonlinear O
transformation O
of O
v1sandv2swhere O
the O
special O
information O
gets O
mixed O
non O
- O
linearly O
and O
can O
not O
be O
separated O
out O
and O
eliminated O
by O
CCA O
. O
This O
explains O
why O
the O
poor O
performance O
of O
C O
CA·Ωë2 O
is O
not O
observed O
for O
K O
CCA·Ωë2 O
in O
Table O
2 O
. O
We O
present O
additional O
empirical O
veriÔ¨Åcation O
of O
Theorem O
2 O
in O
Appendix O
A.2 O
. O
3 O
Experiments O
Datasets O
We O
evaluate O
our O
approach O
on O
seven O
low O
resource O
datasets O
from O
NLP O
text O
classiÔ¨Åcation O
tasks O
like O
sentiment O
classiÔ¨Åcation O
, O
question O
type O
classiÔ¨Åcation O
, O
opinion O
polarity O
detection O
, O
subjectivity O
classiÔ¨Åcation O
, O
etc O
. O
We O
group O
these O
datasets O
into O
2 O
categories O
: O
the O
Ô¨Årst O
having O
a O
few O
hundred O
training O
samples O
( O
which O
we O
term O
as O
very O
small O
datasets O
for O
the O
remainder O
of O
the O
paper O
) O
, O
and O
the O
second O
having O
a O
few O
thousand O
training O
samples O
( O
which O
we O
term O
as O
small O
datasets O
) O
. O
We O
consider O
the O
following O
3 O
very O
small O
datasets O
: O
Amazon O
( O
product O
reviews O
) O
, O
IMDB O
( O
movie O
reviews O
) O
and O
Yelp O
( O
food O
article O
reviews O
) O
; O
and O
the O
following O
4 O
small O
datasets O
: O
MR O
( O
movie O
reviews O
) O
, O
MPQA O
( O
opinion O
polarity O
) O
, O
TREC O
( O
question O
- O
type O
classiÔ¨Åcation O
) O
and O
SUBJ O
( O
subjectivity O
classiÔ¨Åcation O
) O
. O
We O
present O
the O
statistics O
of O
the O
datasets O
in O
Table O
1 O
and O
provide O
the O
details O
and O
downloadable O
links O
in O
Appendix O
B.1 O
. O
Dataset O
c O
N O
|V| O
Test O
Amazon O
( O
Sarma O
et O
al O
. O
, O
2018 O
) O
2 O
1000 O
1865 O
100 O
IMDB O
( O
Sarma O
et O
al O
. O
, O
2018 O
) O
2 O
1000 O
3075 O
100 O
Yelp O
( O
Sarma O
et O
al O
. O
, O
2018 O
) O
2 O
1000 O
2049 O
100 O
MR O
( O
Pang O
and O
Lee O
, O
2005 O
) O
2 O
10662 O
18765 O
1067 O
MPQA O
( O
Wiebe O
and O
Wilson O
, O
2005 O
) O
2 O
10606 O
6246 O
1060 O
TREC O
( O
Li O
and O
Roth O
, O
2002 O
) O
6 O
5952 O
9592 O
500 O
SUBJ O
( O
Pang O
and O
Lee O
, O
2004 O
) O
2 O
10000 O
21323 O
1000 O
Table O
1 O
: O
Dataset O
statistics O
. O
c O
: O
Number O
of O
classes O
, O
N O
: O
Dataset O
size,|V| O
: O
V O
ocabulary O
size O
, O
Test O
: O
Test O
set O
size O
( O
if O
no O
standard O
test O
set O
is O
provided O
, O
we O
use O
a O
random O
train O
/ O
dev O
/ O
test O
split O
of O
80 O
/ O
10 O
/ O
10 O
% O
) O
Amazon O
Yelp O
IMDB O
BERT O
No O
- O
FT O
93.1 O
90.2 O
91.6 O
BERT O
FT O
94.0 O
91.7 O
92.3 O
Adapter O
94.3 O
93.5 O
90.5 O
CNN O
- O
R O
91.1 O
92.7 O
93.2 O
CCA·Ωë2(CNN O
- O
R O
) O
79.1 O
71.5 O
80.8 O
KCCA·Ωë2(CNN O
- O
R O
) O
91.5 O
91.5 O
94.1 O
CAT·Ωë2(CNN O
- O
R O
) O
93.2 O
96.5 O
96.2 O
CAT O
/ O
lock O
- O
open(CNN O
- O
R O
) O
94.0 O
96.2 O
97.0 O
CNN O
- O
S O
94.7 O
95.2 O
96.6 O
CCA·Ωë2(CNN O
- O
S O
) O
83.6 O
67.8 O
83.3 O
KCCA·Ωë2(CNN O
- O
S O
) O
94.3 O
91.9 O
97.9 O
CAT·Ωë2(CNN O
- O
S O
) O
95.3 O
97.1 O
98.1 O
CAT O
/ O
lock O
- O
open(CNN O
- O
S O
) O
95.7 O
97.2 O
98.3 O
CNN O
- O
NS O
95.9 O
95.8 O
96.8 O
CCA·Ωë2(CNN O
- O
NS O
) O
81.3 O
69.4 O
85.0 O
KCCA·Ωë2(CNN O
- O
NS O
) O
95.8 O
96.2 O
97.2 O
CAT·Ωë2(CNN O
- O
NS O
) O
96.4 O
98.3 O
98.3 O
CAT O
/ O
lock O
- O
open(CNN O
- O
NS O
) O
96.8 O
98.3 O
98.4 O
Table O
2 O
: O
Evaluation O
on O
very O
small O
datasets O
. O
C O
CA·Ωë2 O
( O
¬∑ O
) O
/ O
KCCA·Ωë2 O
( O
¬∑ O
) O
/ O
C O
AT·Ωë2 O
( O
¬∑ O
) O
/ O
C O
AT O
/ O
lock O
- O
open O
( O
¬∑ O
) O
refers O
to O
using O
a O
speciÔ¨Åc O
CNN O
variant O
as O
f2 O
. O
Best O
results O
for O
each O
CNN O
variant O
in O
boldface O
. O
Models O
for O
Evaluation O
We O
use O
the O
BERT O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
base O
uncased O
model O
as O
the O
pre O
- O
trained O
modelf1 O
. O
We O
choose O
a O
Text O
- O
CNN O
( O
Kim O
, O
2014 O
) O
model O
as O
the O
domain O
speciÔ¨Åc O
model O
f2with O
3 O
approaches O
to O
initialize O
the O
word O
embeddings O
: O
randomly O
initialized O
( O
C O
NN O
- O
R O
) O
, O
static O
GloVe O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
vectors O
( O
C O
NN O
- O
S O
) O
and O
trainable O
GloVe O
vectors O
( O
C O
NN O
- O
NS O
) O
. O
We O
use O
a O
regularized O
logistic O
regression O
as O
the O
classiÔ¨Åer O
c. O
We O
present O
the O
model O
and O
training O
details O
along O
with O
the O
chosen O
hyperparameters O
in O
Appendix O
B.2 O
- O
B.3 O
. O
We O
also O
present O
results O
with O
two O
other O
popular O
pre O
- O
trained O
models O
: O
GenSen O
and O
InferSent O
in O
Appendix O
C.2 O
. O
We O
consider O
two O
baselines O
: O
( O
i O
) O
BERT O
Ô¨Ånetuning O
( O
denoted O
by O
BERT O
FT O
) O
and O
( O
ii O
) O
learningcover O
frozen O
pre O
- O
trained O
BERT O
weights O
( O
denoted O
by O
BERT O
No O
- O
FT O
) O
. O
We O
also O
present O
the O
Adapter O
( O
Houlsby O
et O
al O
. O
, O
2019 O
) O
approach O
as O
a O
baseline O
, O
which O
injects O
new O
adapters O
in O
BERT O
followed O
by O
selectively O
training O
the O
adapters O
while O
freezing O
the O
BERT O
weights O
, O
to O
compare O
with O
C O
AT·Ωë2since O
neither O
Ô¨Åne O
- O
tunes O
the O
BERT O
parameters O
. O
Results O
on O
Very O
Small O
Datasets O
On O
the O
3 O
very O
small O
datasets O
, O
we O
present O
results O
averaged O
over O
10 O
runs O
in O
Table O
2 O
. O
The O
key O
observations O
are O
summarized O
as O
follows O
: O
( O
i O
) O
C O
AT·Ωë2and O
C O
AT O
/ O
lock O
- O
openalmost O
always O
beat O
the O
accuracy O
of O
the O
baselines O
( O
BERT O
FT O
, O
Adapter O
) O
showing O
their O
effectiveness O
in O
transferring O
knowledge O
from O
the463MR O
MPQA O
SUBJ O
TREC O
BERT O
No O
- O
FT O
83.26 O
87.44 O
95.96 O
88.06 O
BERT O
FT O
86.22 O
90.47 O
96.95 O
96.40 O
Adapter O
85.55 O
90.40 O
97.40 O
96.55 O
CNN O
- O
NS O
80.93 O
88.38 O
89.25 O
92.98 O
CAT·Ωë2(CNN O
- O
NS O
) O
85.60 O
90.06 O
95.92 O
96.64 O
CAT O
/ O
lock O
- O
open(CNN O
- O
NS)87.15 O
91.19 O
97.60 O
97.06 O
Table O
3 O
: O
Performance O
of O
C O
AT·Ωë2and O
C O
AT O
/ O
lock O
- O
openusing O
C O
NN O
- O
NS O
and O
BERT O
on O
small O
datasets O
. O
Best O
results O
in O
boldface O
. O
general O
domain O
to O
the O
target O
domain O
. O
( O
ii O
) O
Both O
the O
C O
CA·Ωë2 O
, O
K O
CCA·Ωë2(computationally O
expensive O
) O
get O
inferior O
performance O
than O
C O
AT·Ωë2 O
. O
Similar O
trends O
for O
GenSen O
and O
InferSent O
in O
Appendix O
C.2 O
. O
( O
iii O
) O
C O
AT O
/ O
lock O
- O
openperforms O
better O
than O
C O
AT·Ωë2 O
, O
but O
at O
an O
increased O
computational O
cost O
. O
The O
execution O
time O
for O
the O
latter O
is O
the O
time O
taken O
to O
train O
the O
text O
- O
CNN O
, O
extract O
BERT O
embeddings O
, O
concatenate O
them O
, O
and O
train O
a O
classiÔ¨Åer O
on O
the O
combination O
. O
On O
an O
average O
run O
on O
the O
Amazon O
dataset O
, O
C O
AT·Ωë2requires O
about O
125 O
s O
, O
reducing O
around O
30 O
% O
of O
the O
180 O
s O
for O
BERT O
FT O
. O
Additionally O
, O
C O
AT·Ωë2has O
small O
memory O
requirements O
as O
it O
can O
be O
computed O
on O
a O
CPU O
in O
contrast O
to O
BERT O
FT O
which O
requires O
, O
at O
minimum O
, O
a O
12 O
GB O
memory O
GPU O
. O
The O
total O
time O
for O
C O
AT O
/ O
lock O
- O
openis O
195 O
s O
, O
which O
is O
less O
than O
a O
9%increase O
over O
FT O
. O
It O
also O
has O
a O
negligible O
1.04 O
% O
increase O
in O
memory O
( O
the O
number O
of O
parameters O
increases O
from O
109,483,778 O
to O
110,630,332 O
due O
to O
the O
text O
- O
CNN O
) O
. O
Results O
on O
Small O
Datasets O
We O
use O
the O
best O
performing O
C O
NN O
- O
NS O
model O
and O
present O
the O
results O
in O
Table O
3 O
. O
Again O
, O
C O
AT O
/ O
lock O
- O
openachieves O
the O
best O
performance O
on O
all O
the O
datasets O
improving O
the O
performance O
of O
BERT O
FT O
and O
Adapter O
. O
C O
AT·Ωë2can O
achieve O
comparable O
test O
accuracy O
to O
BERT O
FT O
on O
all O
the O
tasks O
while O
being O
much O
more O
computationally O
efÔ¨Åcient O
. O
On O
an O
average O
run O
on O
the O
MR O
dataset O
, O
C O
AT·Ωë2(290 O
s O
) O
reduces O
the O
time O
of O
BERT O
FT O
( O
560 O
s O
) O
by O
about O
50 O
% O
, O
while O
C O
AT O
/ O
lock O
- O
open(610 O
s O
) O
only O
incurs O
an O
increase O
of O
about O
9%over O
BERT O
FT O
. O
Comparison O
with O
Adapter O
CAT·Ωë2can O
outperform O
Adapter O
for O
very O
small O
datasets O
and O
perform O
comparably O
on O
small O
datasets O
having O
2 O
advantages O
: O
( O
i O
) O
We O
do O
not O
need O
to O
open O
the O
BERT O
model O
and O
access O
its O
parameters O
to O
introduce O
intermediate O
layers O
and O
hence O
our O
method O
is O
modular O
applicable O
to O
multiple O
pre O
- O
trained O
models O
. O
( O
ii O
) O
On O
very O
small O
datasets O
like O
Amazon O
, O
C O
AT·Ωë2introduces O
roughly O
only O
1%extra O
parameters O
as O
compared O
to O
the O
3‚àí4%of O
Adapter O
thereby O
being O
more O
parameter O
efÔ¨Åcient O
. O
However O
note O
that O
this O
increase O
Figure O
1 O
: O
Comparing O
test O
accuracy O
of O
C O
AT·Ωë2and O
C O
AT O
/ O
lock O
- O
open O
on O
MR O
dataset O
with O
varying O
training O
dataset O
size O
. O
in O
the O
number O
of O
parameters O
due O
to O
the O
text O
- O
CNN O
is O
a O
function O
of O
the O
vocabulary O
size O
of O
the O
dataset O
as O
it O
includes O
the O
word O
embeddings O
which O
are O
fed O
as O
input O
to O
the O
text O
- O
CNN O
. O
For O
a O
dataset O
having O
a O
larger O
vocabulary O
size O
like O
SUBJ3 O
, O
Adapter O
might O
be O
more O
parameter O
efÔ¨Åcient O
than O
C O
AT·Ωë2 O
. O
Effect O
of O
Dataset O
Size O
We O
study O
the O
effect O
of O
size O
of O
data O
on O
the O
performance O
of O
our O
method O
by O
varying O
the O
training O
data O
of O
the O
MR O
dataset O
via O
random O
sub O
- O
sampling O
. O
From O
Figure O
1 O
, O
we O
observe O
that O
C O
AT O
/ O
lock O
- O
opengets O
the O
best O
results O
across O
all O
training O
data O
sizes O
, O
signiÔ¨Åcantly O
improving O
over O
BERT O
FT O
. O
CAT·Ωë2gets O
performance O
comparable O
to O
BERT O
FT O
on O
a O
wide O
range O
of O
data O
sizes O
, O
from O
500 O
points O
on O
. O
We O
present O
qualitative O
analysis O
and O
complete O
results O
with O
error O
bounds O
in O
Appendix O
C. O
4 O
Conclusion O
In O
this O
paper O
we O
have O
proposed O
a O
simple O
method O
for O
transferring O
a O
pre O
- O
trained O
sentence O
embedding O
model O
for O
text O
classiÔ¨Åcation O
tasks O
. O
We O
empirically O
show O
that O
concatenating O
pre O
- O
trained O
and O
domain O
speciÔ¨Åc O
sentence O
embeddings O
, O
learned O
on O
the O
target O
dataset O
, O
with O
or O
without O
Ô¨Åne O
- O
tuning O
can O
improve O
the O
classiÔ¨Åcation O
performance O
of O
pre O
- O
trained O
models O
like O
BERT O
on O
small O
datasets O
. O
We O
have O
also O
provided O
theoretical O
analysis O
identifying O
the O
conditions O
when O
this O
method O
is O
successful O
and O
to O
explain O
the O
experimental O
results O
. O
Acknowledgements O
This O
work O
was O
supported O
in O
part O
by O
FA9550 O
- O
181 O
- O
0166 O
. O
The O
authors O
would O
also O
like O
to O
acknowledge O
the O
support O
provided O
by O
the O
University O
of O
Wisconsin O
- O
Madison O
OfÔ¨Åce O
of O
the O
Vice O
Chancellor O
for O
Research O
and O
Graduate O
Education O
with O
funding O
from O
the O
Wisconsin O
Alumni O
Research O
Foundation O
. O
3For O
SUBJ O
, O
the O
embeddings O
alone O
contribute O
6,396,900 O
additional O
parameters O
( O
5.84 O
% O
of O
parameters O
of O
BERT O
- O
Base)464References O
Yuki O
Arase O
and O
Jun‚Äôichi O
Tsujii O
. O
2019 O
. O
Transfer O
Ô¨Ånetuning O
: O
A O
BERT O
case O
study O
. O
In O
Proceedings O
of O
the O
2019 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
and O
the O
9th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
( O
EMNLP O
- O
IJCNLP O
) O
, O
pages O
5393‚Äì5404 O
, O
Hong O
Kong O
, O
China O
. O
Association O
for O
Computational O
Linguistics O
. O
Colin O
Cherry O
, O
Greg O
Durrett O
, O
George O
Foster O
, O
Reza O
Haffari O
, O
Shahram O
Khadivi O
, O
Nanyun O
Peng O
, O
Xiang O
Ren O
, O
and O
Swabha O
Swayamdipta O
, O
editors O
. O
2019 O
. O
Proceedings O
of O
the O
2nd O
Workshop O
on O
Deep O
Learning O
Approaches O
for O
Low O
- O
Resource O
NLP O
( O
DeepLo O
2019 O
) O
. O
Association O
for O
Computational O
Linguistics O
, O
Hong O
Kong O
, O
China O
. O
Ronan O
Collobert O
, O
Jason O
Weston O
, O
L O
¬¥ O
eon O
Bottou O
, O
Michael O
Karlen O
, O
Koray O
Kavukcuoglu O
, O
and O
Pavel O
Kuksa O
. O
2011 O
. O
Natural O
language O
processing O
( O
almost O
) O
from O
scratch O
. O
J. O
Mach O
. O
Learn O
. O
Res O
. O
, O
12(null):2493‚Äì2537 O
. O
Alexis O
Conneau O
, O
Douwe O
Kiela O
, O
Holger O
Schwenk O
, O
Lo O
¬®ƒ±c O
Barrault O
, O
and O
Antoine O
Bordes O
. O
2017 O
. O
Supervised O
learning O
of O
universal O
sentence O
representations O
from O
natural O
language O
inference O
data O
. O
In O
Proceedings O
of O
the O
2017 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
670‚Äì680 O
. O
Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2018 O
. O
Bert O
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
understanding O
. O
arXiv O
preprint O
arXiv:1810.04805 O
. O
Thomas O
G. O
Dietterich O
. O
2000 O
. O
Ensemble O
methods O
in O
machine O
learning O
. O
In O
Proceedings O
of O
the O
First O
International O
Workshop O
on O
Multiple O
ClassiÔ¨Åer O
Systems O
, O
MCS O
‚Äô O
00 O
, O
page O
1‚Äì15 O
, O
Berlin O
, O
Heidelberg O
. O
SpringerVerlag O
. O
Jesse O
Dodge O
, O
Gabriel O
Ilharco O
, O
Roy O
Schwartz O
, O
Ali O
Farhadi O
, O
Hannaneh O
Hajishirzi O
, O
and O
Noah O
Smith O
. O
2020 O
. O
Fine O
- O
tuning O
pretrained O
language O
models O
: O
Weight O
initializations O
, O
data O
orders O
, O
and O
early O
stopping O
. O
Siddhant O
Garg O
, O
Thuy O
Vu O
, O
and O
Alessandro O
Moschitti O
. O
2019 O
. O
Tanda O
: O
Transfer O
and O
adapt O
pre O
- O
trained O
transformer O
models O
for O
answer O
sentence O
selection O
. O
H O
Hotelling O
. O
1936 O
. O
Relations O
between O
two O
sets O
of O
variates O
. O
Biometrika O
. O
Neil O
Houlsby O
, O
Andrei O
Giurgiu O
, O
Stanislaw O
Jastrzebski O
, O
Bruna O
Morrone O
, O
Quentin O
De O
Laroussilhe O
, O
Andrea O
Gesmundo O
, O
Mona O
Attariyan O
, O
and O
Sylvain O
Gelly O
. O
2019 O
. O
Parameter O
- O
efÔ¨Åcient O
transfer O
learning O
for O
nlp O
. O
InInternational O
Conference O
on O
Machine O
Learning O
, O
pages O
2790‚Äì2799 O
. O
Jeremy O
Howard O
and O
Sebastian O
Ruder O
. O
2018 O
. O
Universal O
language O
model O
Ô¨Åne O
- O
tuning O
for O
text O
classiÔ¨Åcation O
. O
In O
Proceedings O
of O
the O
56th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
328‚Äì339.Yoon O
Kim O
. O
2014 O
. O
Convolutional O
neural O
networks O
for O
sentence O
classiÔ¨Åcation O
. O
In O
Proceedings O
of O
the O
2014 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
EMNLP O
2014 O
, O
October O
25 O
- O
29 O
, O
2014 O
, O
Doha O
, O
Qatar O
, O
A O
meeting O
of O
SIGDAT O
, O
a O
Special O
Interest O
Group O
of O
the O
ACL O
, O
pages O
1746‚Äì1751 O
. O
Cheolhyoung O
Lee O
, O
Kyunghyun O
Cho O
, O
and O
Wanmo O
Kang O
. O
2020 O
. O
Mixout O
: O
Effective O
regularization O
to O
Ô¨Ånetune O
large O
- O
scale O
pretrained O
language O
models O
. O
In O
International O
Conference O
on O
Learning O
Representations O
. O
Jens O
Lehmann O
, O
Robert O
Isele O
, O
Max O
Jakob O
, O
Anja O
Jentzsch O
, O
Dimitris O
Kontokostas O
, O
Pablo O
N. O
Mendes O
, O
Sebastian O
Hellmann O
, O
Mohamed O
Morsey O
, O
Patrick O
van O
Kleef O
, O
S¬®oren O
Auer O
, O
and O
Christian O
Bizer O
. O
2015 O
. O
DBpedia O
a O
large O
- O
scale O
, O
multilingual O
knowledge O
base O
extracted O
from O
wikipedia O
. O
Semantic O
Web O
Journal O
, O
6(2 O
) O
. O
Xin O
Li O
and O
Dan O
Roth O
. O
2002 O
. O
Learning O
question O
classiÔ¨Åers O
. O
In O
Proceedings O
of O
the O
19th O
International O
Conference O
on O
Computational O
Linguistics O
- O
Volume O
1 O
, O
COLING O
‚Äô O
02 O
, O
pages O
1‚Äì7 O
, O
Stroudsburg O
, O
PA O
, O
USA O
. O
Association O
for O
Computational O
Linguistics O
. O
Bo O
Pang O
and O
Lillian O
Lee O
. O
2004 O
. O
A O
sentimental O
education O
: O
Sentiment O
analysis O
using O
subjectivity O
. O
In O
Proceedings O
of O
ACL O
, O
pages O
271‚Äì278 O
. O
Bo O
Pang O
and O
Lillian O
Lee O
. O
2005 O
. O
Seeing O
stars O
: O
Exploiting O
class O
relationships O
for O
sentiment O
categorization O
with O
respect O
to O
rating O
scales O
. O
In O
Proceedings O
of O
ACL O
, O
pages O
115‚Äì124 O
. O
Jeffrey O
Pennington O
, O
Richard O
Socher O
, O
and O
Christopher O
Manning O
. O
2014 O
. O
GloVe O
: O
Global O
vectors O
for O
word O
representation O
. O
In O
Proceedings O
of O
the O
2014 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
( O
EMNLP O
) O
, O
pages O
1532‚Äì1543 O
, O
Doha O
, O
Qatar O
. O
Association O
for O
Computational O
Linguistics O
. O
Matthew O
Peters O
, O
Sebastian O
Ruder O
, O
and O
Noah O
A O
Smith O
. O
2019 O
. O
To O
tune O
or O
not O
to O
tune O
? O
adapting O
pretrained O
representations O
to O
diverse O
tasks O
. O
arXiv O
preprint O
arXiv:1903.05987 O
. O
Matthew O
E O
Peters O
, O
Mark O
Neumann O
, O
Mohit O
Iyyer O
, O
Matt O
Gardner O
, O
Christopher O
Clark O
, O
Kenton O
Lee O
, O
and O
Luke O
Zettlemoyer O
. O
2018 O
. O
Deep O
contextualized O
word O
representations O
. O
In O
Proceedings O
of O
NAACL O
- O
HLT O
, O
pages O
2227‚Äì2237 O
. O
Jason O
Phang O
, O
Thibault O
F O
¬¥ O
evry O
, O
and O
Samuel O
R. O
Bowman O
. O
2018 O
. O
Sentence O
encoders O
on O
stilts O
: O
Supplementary O
training O
on O
intermediate O
labeled O
- O
data O
tasks O
. O
Alec O
Radford O
, O
Karthik O
Narasimhan O
, O
Tim O
Salimans O
, O
and O
Ilya O
Sutskever O
. O
2018 O
. O
Improving O
language O
understanding O
by O
generative O
pre O
- O
training O
. O
URL O
https://s3us O
- O
west-2 O
. O
amazonaws O
. O
com O
/ O
openai O
- O
assets O
/ O
researchcovers O
/ O
languageunsupervised O
/ O
language O
understanding O
paper O
. O
pdf O
. O
Prathusha O
K O
Sarma O
, O
Yingyu O
Liang O
, O
and O
Bill O
Sethares O
. O
2018 O
. O
Domain O
adapted O
word O
embeddings O
for O
improved O
sentiment O
classiÔ¨Åcation O
. O
In O
Proceedings O
of465the O
56th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
2 O
: O
Short O
Papers O
) O
, O
pages O
37‚Äì42 O
. O
Bernhard O
Sch O
¬®olkopf O
, O
Alexander O
Smola O
, O
and O
KlausRobert O
M O
¬®uller O
. O
1998 O
. O
Nonlinear O
component O
analysis O
as O
a O
kernel O
eigenvalue O
problem O
. O
Neural O
computation O
, O
10(5):1299‚Äì1319 O
. O
Sandeep O
Subramanian O
, O
Adam O
Trischler O
, O
Yoshua O
Bengio O
, O
and O
Christopher O
J O
Pal O
. O
2018 O
. O
Learning O
general O
purpose O
distributed O
sentence O
representations O
via O
large O
scale O
multi O
- O
task O
learning O
. O
arXiv O
preprint O
arXiv:1804.00079 O
. O
Chi O
Sun O
, O
Xipeng O
Qiu O
, O
Yige O
Xu O
, O
and O
Xuanjing O
Huang O
. O
2019 O
. O
How O
to O
Ô¨Åne O
- O
tune O
BERT O
for O
text O
classiÔ¨Åcation O
? O
CoRR O
, O
abs/1905.05583 O
. O
Alex O
Wang O
, O
Amanpreet O
Singh O
, O
Julian O
Michael O
, O
Felix O
Hill O
, O
Omer O
Levy O
, O
and O
Samuel O
Bowman O
. O
2018 O
. O
GLUE O
: O
A O
multi O
- O
task O
benchmark O
and O
analysis O
platform O
for O
natural O
language O
understanding O
. O
In O
Proceedings O
of O
the O
2018 O
EMNLP O
Workshop O
BlackboxNLP O
, O
Brussels O
, O
Belgium O
. O
Association O
for O
Computational O
Linguistics O
. O
Canhui O
Wang O
, O
Min O
Zhang O
, O
Shaoping O
Ma O
, O
and O
Liyun O
Ru O
. O
2008 O
. O
Automatic O
online O
news O
issue O
construction O
in O
web O
environment O
. O
In O
Proceedings O
of O
the O
17th O
WWW O
, O
page O
457‚Äì466 O
, O
New O
York O
, O
NY O
, O
USA O
. O
Association O
for O
Computing O
Machinery O
. O
Ran O
Wang O
, O
Haibo O
Su O
, O
Chunye O
Wang O
, O
Kailin O
Ji O
, O
and O
Jupeng O
Ding O
. O
2019 O
. O
To O
tune O
or O
not O
to O
tune O
? O
how O
about O
the O
best O
of O
both O
worlds O
? O
ArXiv O
. O
Janyce O
Wiebe O
and O
Theresa O
Wilson O
. O
2005 O
. O
Annotating O
expressions O
of O
opinions O
and O
emotions O
in O
language O
. O
Language O
Resources O
and O
Evaluation O
, O
39(2):165 O
‚Äì O
210 O
. O
Hu O
Xu O
, O
Bing O
Liu O
, O
Lei O
Shu O
, O
and O
S O
Yu O
Philip O
. O
2019 O
. O
Bert O
post O
- O
training O
for O
review O
reading O
comprehension O
and O
aspect O
- O
based O
sentiment O
analysis O
. O
In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
2324‚Äì2335 O
. O
Appendix O
A O
Theorems O
: O
Additional O
Explanation O
A.1 O
Concatenation O
Theorem O
1 O
. O
If O
the O
loss O
function O
LisŒª O
- O
Lipschitz O
for O
the O
Ô¨Årst O
parameter O
, O
and O
Phas O
full O
column O
rank O
, O
then O
there O
exists O
a O
linear O
classiÔ¨Åer O
¬Øfover¬Øvssuch O
thatL(¬Øf)‚â§L(f‚àó O
) O
+ O
ŒªœÉ O
/ O
bardbl(P‚Ä†)/latticetopw‚àó/bardbl2whereP‚Ä†is O
the O
pseudo O
- O
inverse O
of O
P.JustiÔ¨Åcation O
of O
Assumptions O
The O
assumption O
of O
Lipschitz O
- O
ness O
of O
the O
loss O
means O
that O
the O
loss O
changes O
smoothly O
with O
the O
prediction O
, O
which O
is O
a O
standard O
assumption O
in O
machine O
learning O
. O
The O
assumption O
on O
Phaving O
full O
column O
rank O
means O
that O
v1s O
, O
v2scontain O
the O
information O
of O
v‚àó O
sand O
ensures O
thatP‚Ä†exists.4 O
Explanation O
For O
intuition O
about O
the O
term O
/bardbl(P‚Ä†)/latticetopw‚àó/bardbl2 O
, O
consider O
the O
following O
simple O
example O
. O
Suppose O
v‚àó O
shas4dimensions O
, O
and O
w‚àó= O
[ O
1,1,0,0]/latticetop O
, O
i.e. O
, O
only O
the O
Ô¨Årst O
two O
dimensions O
are O
useful O
for O
classiÔ¨Åcation O
. O
Suppose O
P1= O
diag(c,0,1,0)is O
a O
diagonal O
matrix O
, O
so O
that O
v1s O
captures O
the O
Ô¨Årst O
dimension O
with O
scaling O
factor O
c O
> O
0and O
the O
third O
dimension O
with O
factor O
1 O
, O
and O
P2 O
= O
diag(0,c,0,1)so O
thatv2scaptures O
the O
other O
two O
dimensions O
. O
Hence O
we O
have O
( O
P‚Ä†)/latticetopw‚àó= O
[ O
1 O
/ O
c,1 O
/ O
c,0,0]/latticetop O
, O
and O
thus O
L(¬Øf)‚â§L(f‚àó O
) O
+ O
‚àö O
2ŒªœÉ O
c O
Thus O
the O
quality O
of O
the O
classiÔ¨Åer O
is O
determined O
by O
the O
noise O
- O
signal O
ratio O
œÉ O
/ O
c. O
Ifcis O
small O
, O
meaning O
thatv1sandv2smostly O
contain O
nuisance O
, O
then O
the O
loss O
is O
large O
. O
If O
cis O
large O
, O
meaning O
that O
v1sandv2s O
mostly O
capture O
the O
information O
along O
with O
some O
nuisance O
while O
the O
noise O
is O
relatively O
small O
, O
then O
the O
loss O
is O
close O
to O
that O
of O
f‚àó. O
Note O
that O
¬Øfcan O
be O
much O
better O
than O
any O
classiÔ¨Åer O
using O
only O
v1sor O
v2sthat O
has O
only O
part O
of O
the O
features O
determining O
the O
class O
labels O
. O
A.2 O
CCA O
Theorem O
2 O
. O
Let¬Øvsdenote O
the O
embedding O
for O
sentencesobtained O
by O
concatenation O
, O
and O
Àúvsdenote O
that O
obtained O
by O
CCA O
. O
There O
exists O
a O
setting O
of O
the O
data O
andw‚àó,P,/epsilon1 O
such O
that O
there O
exists O
a O
linear O
classiÔ¨Åer O
¬Øfon¬Øvswith O
the O
same O
loss O
as O
f‚àó O
, O
while O
CCA O
achieves O
the O
maximum O
correlation O
but O
any O
classiÔ¨Åer O
on O
Àúvsis O
at O
best O
random O
guessing O
. O
Empirical O
VeriÔ¨Åcation O
One O
important O
insight O
from O
Theorem O
2 O
is O
that O
when O
the O
two O
sets O
of O
embeddings O
have O
special O
information O
that O
is O
not O
shared O
with O
each O
other O
but O
is O
important O
for O
classiÔ¨Åcation O
, O
then O
CCA O
will O
eliminate O
such O
information O
and O
have O
bad O
prediction O
performance O
. O
Let O
r2s O
= O
v2s‚àíŒ¶ O
/ O
latticetop O
2Œ¶2v2sbe O
the O
residue O
vector O
for O
the O
projection O
Œ¶2learned O
by O
CCA O
for O
the O
special O
domain O
, O
and O
similarly O
deÔ¨Åne O
r1s O
. O
Then O
the O
analysis O
4One O
can O
still O
do O
analysis O
dropping O
the O
full O
- O
rank O
assumption O
, O
but O
it O
will O
become O
more O
involved O
and O
non O
- O
intuitive466suggests O
that O
the O
residues O
r1sandr2scontain O
information O
important O
for O
prediction O
. O
We O
conduct O
experiments O
for O
BERT+CNN O
- O
non O
- O
static O
on O
Amazon O
reviews O
, O
and O
Ô¨Ånd O
that O
a O
classiÔ¨Åer O
on O
the O
concatenation O
ofr1sandr2shas O
accuracy O
96.4 O
% O
. O
This O
is O
much O
better O
than O
81.3%on O
the O
combined O
embeddings O
via O
CCA O
. O
These O
observations O
provide O
positive O
support O
for O
our O
analysis O
. O
B O
Experiment O
Details O
B.1 O
Datasets O
In O
addition O
to O
Table O
1 O
, O
here O
we O
provide O
details O
on O
the O
tasks O
of O
the O
datasets O
and O
links O
to O
download O
them O
for O
reproducibility O
of O
results O
. O
‚Ä¢Amazon O
: O
A O
sentiment O
classiÔ¨Åcation O
dataset O
on O
Amazon O
product O
reviews O
where O
reviews O
are O
classiÔ¨Åed O
as O
‚Äò O
Positive O
‚Äô O
or O
‚Äò O
Negative‚Äô.5 O
. O
‚Ä¢IMDB O
: O
A O
sentiment O
classiÔ¨Åcation O
dataset O
of O
movie O
reviews O
on O
IMDB O
where O
reviews O
are O
classiÔ¨Åed O
as O
‚Äò O
Positive O
‚Äô O
or O
‚Äò O
Negative‚Äô3 O
. O
‚Ä¢Yelp O
: O
A O
sentiment O
classiÔ¨Åcation O
dataset O
of O
restaurant O
reviews O
from O
Yelp O
where O
reviews O
are O
classiÔ¨Åed O
as O
‚Äò O
Positive O
‚Äô O
or O
‚Äò O
Negative‚Äô3 O
. O
‚Ä¢MR O
: O
A O
sentiment O
classiÔ¨Åcation O
dataset O
of O
movie O
reviews O
based O
on O
sentiment O
polarity O
and O
subjective O
rating O
( O
Pang O
and O
Lee O
, O
2005)6 O
. O
‚Ä¢MPQA O
: O
An O
unbalanced O
polarity O
classiÔ¨Åcation O
dataset O
( O
70 O
% O
negative O
examples O
) O
for O
opinion O
polarity O
detection O
( O
Wiebe O
and O
Wilson O
, O
2005)7 O
. O
‚Ä¢TREC O
: O
A O
question O
type O
classiÔ¨Åcation O
dataset O
with O
6 O
classes O
for O
questions O
about O
a O
person O
, O
location O
, O
numeric O
information O
, O
etc O
. O
( O
Li O
and O
Roth O
, O
2002)8 O
. O
‚Ä¢SUBJ O
: O
A O
dataset O
for O
classifying O
a O
sentence O
as O
having O
subjective O
or O
objective O
opinions O
( O
Pang O
and O
Lee O
, O
2004 O
) O
. O
The O
Amazon O
, O
Yelp O
and O
IMDB O
review O
datasets O
have O
previously O
been O
used O
for O
research O
on O
few O
- O
sample O
learning O
by O
Sarma O
et O
al O
. O
( O
2018 O
) O
and O
capture O
sentiment O
information O
from O
target O
domains O
very O
different O
from O
the O
general O
text O
corpora O
of O
the O
pre O
- O
trained O
models O
. O
5https://archive.ics.uci.edu/ml/ O
datasets O
/ O
Sentiment+Labelled+Sentences O
6https://www.cs.cornell.edu/people/ O
pabo O
/ O
movie O
- O
review O
- O
data/ O
7http://mpqa.cs.pitt.edu/ O
8http://cogcomp.org/Data/QA/QC/B.2 O
Embedding O
Models O
B.2.1 O
Domain O
SpeciÔ¨Åc O
f2 O
We O
use O
the O
text O
- O
CNN O
model O
( O
Kim O
, O
2014 O
) O
for O
domain O
speciÔ¨Åc O
embeddings O
f2the O
details O
of O
which O
are O
provided O
below O
. O
Text O
- O
CNN O
The O
model O
restricts O
the O
maximum O
sequence O
length O
of O
the O
input O
sentence O
to O
128tokens O
, O
and O
uses O
convolutional O
Ô¨Ålter O
windows O
of O
sizes O
3 O
, O
4,5with O
100feature O
maps O
for O
each O
size O
. O
A O
maxovertime O
pooling O
operation O
( O
Collobert O
et O
al O
. O
, O
2011 O
) O
is O
used O
over O
the O
feature O
maps O
to O
get O
a O
384dimensional O
sentence O
embeddings O
( O
128dimensions O
corresponding O
to O
each O
Ô¨Ålter O
size O
) O
. O
We O
train O
the O
model O
using O
the O
Cross O
Entropy O
loss O
with O
an O
/lscript2norm O
penalty O
on O
the O
classiÔ¨Åer O
weights O
similar O
to O
Kim O
( O
2014 O
) O
. O
We O
use O
a O
dropout O
rate O
of O
0.5while O
training O
. O
For O
each O
dataset O
, O
we O
create O
a O
vocabulary O
speciÔ¨Åc O
to O
the O
dataset O
which O
includes O
any O
token O
present O
in O
the O
train O
/ O
dev O
/ O
test O
split O
. O
The O
input O
word O
embeddings O
can O
be O
chosen O
in O
the O
following O
three O
ways O
: O
‚Ä¢CNN O
- O
R O
: O
Randomly O
initialized O
300 O
- O
dimensional O
word O
embeddings O
trained O
together O
with O
the O
text O
- O
CNN O
. O
‚Ä¢CNN O
- O
S O
: O
Initialised O
with O
GloVe O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
pre O
- O
trained O
word O
embeddings O
and O
made O
static O
during O
training O
the O
text O
- O
CNN O
. O
‚Ä¢CNN O
- O
NS O
: O
Initialised O
with O
GloVe O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
pre O
- O
trained O
word O
embeddings O
and O
made O
trainable O
during O
training O
the O
text O
- O
CNN O
. O
For O
very O
small O
datasets O
we O
additionally O
compare O
with O
sentence O
embeddings O
obtained O
using O
the O
Bag O
of O
Words O
approach O
. O
B.2.2 O
Pre O
- O
Trained O
f1 O
We O
use O
the O
following O
three O
models O
for O
pre O
- O
trained O
embeddings O
f1 O
: O
BERT O
We O
use O
the O
BERT9 O
- O
base O
uncased O
model O
with O
WordPiece O
tokenizer O
having O
12 O
transformer O
layers O
. O
We O
obtain O
768dimensional O
sentence O
embeddings O
corresponding O
to O
the O
[ O
CLS O
] O
token O
from O
the O
Ô¨Ånal O
layer O
. O
We O
perform O
Ô¨Åne O
- O
tuning O
for O
20 O
epochs O
with O
early O
stopping O
by O
choosing O
the O
best O
performing O
model O
on O
the O
validation O
data O
. O
The O
additional O
Ô¨Åne O
- O
tuning O
epochs O
( O
20 O
compared O
to O
the O
typical O
3 O
) O
allows O
for O
a O
better O
performance O
of O
the O
Ô¨Åne O
- O
tuning O
baseline O
since O
we O
use O
early O
stopping O
. O
9https://github.com/google-research/bert467InferSent O
We O
use O
the O
pre O
- O
trained O
InferSent O
model O
( O
Conneau O
et O
al O
. O
, O
2017 O
) O
to O
obtain O
4096 O
dimensional O
sentence O
embeddings O
using O
the O
implementation O
provided O
in O
the O
SentEval10repository O
. O
We O
use O
InferSent O
v1 O
for O
all O
our O
experiments O
. O
GenSen O
We O
use O
the O
pre O
- O
trained O
GenSen O
model O
( O
Subramanian O
et O
al O
. O
, O
2018 O
) O
implemented O
in O
the O
SentEval O
repository O
to O
obtain O
4096 O
dimensional O
sentence O
embeddings O
. O
B.3 O
Training O
Details O
We O
train O
domain O
speciÔ¨Åc O
embeddings O
on O
the O
training O
data O
and O
extract O
the O
embeddings O
. O
We O
combine O
these O
with O
the O
embeddings O
from O
the O
pre O
- O
trained O
models O
and O
train O
a O
regularized O
logistic O
regression O
classiÔ¨Åer O
on O
top O
. O
This O
classiÔ¨Åer O
is O
learned O
on O
the O
training O
data O
, O
while O
using O
the O
dev O
data O
for O
hyperparameter O
tuning O
the O
regularizer O
penalty O
on O
the O
weights O
. O
The O
classiÔ¨Åer O
can O
be O
trained O
either O
by O
freezing O
the O
weights O
of O
the O
embedding O
models O
or O
training O
the O
whole O
network O
end O
- O
to O
- O
end O
. O
The O
performance O
is O
tested O
on O
the O
test O
set O
. O
We O
use O
test O
accuracy O
as O
the O
performance O
metric O
and O
report O
all O
results O
averaged O
over O
10experiments O
unless O
mentioned O
otherwise O
. O
The O
experiments O
are O
performed O
on O
an O
NVIDIA O
Titan O
Xp O
12 O
GB O
GPU O
. O
B.3.1 O
Hyperparameters O
We O
use O
an O
Adam O
optimizer O
with O
a O
learning O
rate O
of2e‚àí5as O
per O
the O
standard O
Ô¨Åne O
- O
tuning O
practice O
. O
For O
C O
CA·Ωë2 O
, O
we O
used O
a O
regularized O
CCA O
implementation O
and O
tune O
the O
regularization O
parameter O
via O
grid O
search O
in O
[ O
0.00001 O
, O
10 O
] O
in O
multiplicative O
steps O
of O
10over O
the O
validation O
data O
. O
For O
K O
CCA·Ωë2 O
, O
we O
use O
a O
Gaussian O
kernel O
with O
a O
regularized O
KCCA O
implementation O
where O
the O
Gaussian O
sigma O
and O
the O
regularization O
parameter O
are O
tuned O
via O
grid O
search O
in[0.05,10]and[0.00001,10]respectively O
in O
multiplicative O
steps O
of O
10 O
over O
the O
validation O
data O
. O
For O
CAT·Ωë2and O
C O
AT O
/ O
lock O
- O
open O
, O
the O
weighting O
parameter O
Œ±is O
tuned O
via O
grid O
search O
in O
the O
range O
[ O
0.002,500 O
] O
in O
multiplicative O
steps O
of O
10 O
over O
the O
validation O
data O
. O
C O
Additional O
Results O
C.1 O
Qualitative O
Analysis O
We O
present O
some O
qualitative O
examples O
from O
the O
Amazon O
, O
IMDB O
and O
Yelp O
datasets O
on O
which O
BERT O
and O
C O
NN O
- O
NS O
are O
unable O
to O
provide O
the O
correct O
class O
predictions O
, O
while O
C O
AT·Ωë2or O
K O
CCA·Ωë2can O
successfully O
provide O
the O
correct O
class O
predictions O
in O
Table O
4 O
. O
10https://github.com/facebookresearch/SentEvalCorrectly O
classiÔ¨Åed O
by O
K O
CCA·Ωë2 O
However O
- O
the O
ringtones O
are O
not O
the O
best O
, O
and O
neither O
are O
the O
games O
. O
This O
is O
cool O
because O
most O
cases O
are O
just O
open O
there O
allowing O
the O
screen O
to O
get O
all O
scratched O
up O
. O
Correctly O
classiÔ¨Åed O
by O
C O
AT·Ωë2 O
TNot O
nearly O
as O
good O
looking O
as O
the O
amazon O
picture O
makes O
it O
look O
. O
Magical O
Help O
. O
( O
a O
) O
Amazon O
Correctly O
classiÔ¨Åed O
by O
K O
CCA·Ωë2 O
I O
would O
have O
casted O
her O
in O
that O
role O
after O
ready O
the O
script O
. O
Predictable O
, O
but O
not O
a O
bad O
watch O
. O
Correctly O
classiÔ¨Åed O
by O
C O
AT·Ωë2 O
I O
would O
have O
casted O
her O
in O
that O
role O
after O
ready O
the O
script O
. O
Predictable O
, O
but O
not O
a O
bad O
watch O
. O
( O
b O
) O
IMDB O
Correctly O
classiÔ¨Åed O
by O
K O
CCA·Ωë2 O
The O
lighting O
is O
just O
dark O
enough O
to O
set O
the O
mood O
. O
I O
went O
to O
Bachi O
Burger O
on O
a O
friend O
‚Äôs O
recommendation O
and O
was O
not O
disappointed O
. O
do O
nt O
go O
here O
. O
I O
found O
this O
place O
by O
accident O
and O
I O
could O
not O
be O
happier O
. O
Correctly O
classiÔ¨Åed O
by O
C O
AT·Ωë2 O
The O
lighting O
is O
just O
dark O
enough O
to O
set O
the O
mood O
. O
I O
went O
to O
Bachi O
Burger O
on O
a O
friend O
‚Äôs O
recommendation O
and O
was O
not O
disappointed O
. O
do O
nt O
go O
here O
. O
I O
found O
this O
place O
by O
accident O
and O
I O
could O
not O
be O
happier O
. O
( O
c O
) O
Yelp O
Table O
4 O
: O
Sentences O
from O
Amazon O
, O
IMDB O
, O
Yelp O
datasets O
where O
K O
CCA·Ωë2and O
C O
AT·Ωë2of O
BERT O
and O
C O
NN O
- O
NS O
embeddings O
succeeds O
while O
they O
individually O
give O
wrong O
predictions O
. O
We O
observe O
that O
these O
are O
either O
short O
sentences O
or O
ones O
where O
the O
content O
is O
tied O
to O
the O
speciÔ¨Åc O
reviewing O
context O
as O
well O
as O
the O
involved O
structure O
to O
be O
parsed O
with O
general O
knowledge O
. O
Such O
input O
sentences O
thus O
require O
combining O
both O
the O
general O
semantics O
of O
BERT O
and O
the O
domain O
speciÔ¨Åc O
semantics O
of O
C O
NN O
- O
NS O
to O
predict O
the O
correct O
class O
labels O
. O
C.2 O
Complete O
Results O
with O
Error O
Bounds O
We O
present O
a O
comprehensive O
set O
of O
results O
along O
with O
error O
bounds O
on O
very O
small O
datasets O
( O
Amazon O
, O
IMDB O
and O
Yelp O
reviews O
) O
in O
Table O
2 O
, O
where O
we O
evaluate O
three O
popularly O
used O
pre O
- O
trained O
sentence O
embedding O
models O
, O
namely O
BERT O
, O
GenSen O
and O
InferSent O
. O
We O
present O
the O
error O
bounds O
on O
the O
results O
for O
small O
datasets O
in O
Table O
3 O
. O
For O
small O
datasets O
, O
we O
additionally O
present O
results O
from O
using O
C O
CA·Ωë2 O
( O
We O
omit O
K O
CCA·Ωë2here O
due O
to O
high O
computational O
memory O
requirements).468BOW O
CNN O
- O
R O
CNN O
- O
S O
CNN O
- O
NS O
AmazonDefault O
79.20¬±2.31 O
91.10¬±1.64 O
94.70¬±0.64 O
95.90¬±0.70 O
BERT94.00¬±0.02CAT O
/ O
lock O
- O
open O
- O
94.05¬±0.23 O
95.70¬±0.50 O
96.75¬±0.76 O
CAT·Ωë2 O
89.59¬±1.22 O
93.20¬±0.98 O
95.30¬±0.46 O
96.40¬±1.11 O
KCCA·Ωë289.12¬±0.47 O
91.50¬±1.63 O
94.30¬±0.46 O
95.80¬±0.40 O
CCA·Ωë2 O
50.91¬±1.12 O
79.10¬±2.51 O
83.60¬±1.69 O
81.30¬±3.16 O
GenSen O
82.55¬±0.82CAT·Ωë2 O
82.82¬±0.97 O
92.80¬±1.25 O
94.10¬±0.70 O
95.00¬±1.0 O
KCCA·Ωë279.21¬±2.28 O
91.30¬±1.42 O
94.80¬±0.75 O
95.90¬±0.30 O
CCA·Ωë2 O
52.80¬±0.74 O
80.60¬±4.87 O
83.00¬±2.45 O
84.95¬±1.45 O
InferSent O
85.29¬±1.61CAT·Ωë2 O
51.89¬±0.62 O
90.30¬±1.48 O
94.70¬±1.10 O
95.90¬±0.70 O
KCCA·Ωë252.29¬±0.74 O
91.70¬±1.49 O
95.00¬±0.00 O
96.00¬±0.00 O
CCA·Ωë2 O
53.10¬±0.82 O
61.10¬±3.47 O
65.50¬±3.69 O
71.40¬±3.04 O
YelpDefault O
81.3¬±2.72 O
92.71¬±0.46 O
95.25¬±0.39 O
95.83¬±0.14 O
BERT91.67¬±0.00CAT O
/ O
lock O
- O
open O
- O
96.23¬±1.04 O
97.23¬±0.70 O
98.34¬±0.62 O
CAT·Ωë2 O
89.03¬±0.70 O
96.50¬±1.33 O
97.10¬±0.70 O
98.30¬±0.78 O
KCCA·Ωë288.51¬±1.22 O
91.54¬±4.63 O
91.91¬±1.13 O
96.2¬±0.87 O
CCA·Ωë2 O
50.27¬±1.33 O
71.53¬±2.46 O
67.83¬±3.07 O
69.4¬±3.35 O
GenSen O
86.75¬±0.79CAT·Ωë2 O
85.94¬±1.04 O
94.24¬±0.53 O
95.77¬±0.36 O
96.03¬±0.23 O
KCCA·Ωë283.35¬±1.79 O
92.58¬±0.31 O
95.41¬±0.45 O
95.06¬±0.56 O
CCA·Ωë2 O
57.14¬±0.84 O
84.27¬±1.68 O
86.94¬±1.62 O
87.27¬±1.81 O
InferSent O
85.7¬±1.12CAT·Ωë2 O
50.83¬±0.42 O
91.94¬±0.46 O
96.10¬±1.30 O
97.00¬±0.77 O
KCCA·Ωë250.80¬±0.65 O
91.13¬±1.63 O
95.45¬±0.23 O
95.57¬±0.55 O
CCA·Ωë2 O
55.91¬±1.23 O
60.80¬±2.22 O
54.70¬±1.34 O
59.50¬±1.85 O
IMDBDefault O
89.30¬±1.00 O
93.25¬±0.38 O
96.62¬±0.46 O
96.76¬±0.26 O
BERT92.33¬±0.00CAT O
/ O
lock O
- O
open O
- O
97.07¬±0.95 O
98.31¬±0.83 O
98.42¬±0.78 O
CAT·Ωë2 O
89.27¬±0.97 O
96.20¬±2.18 O
98.10¬±0.94 O
98.30¬±1.35 O
KCCA·Ωë288.29¬±0.65 O
94.10¬±1.87 O
97.90¬±0.30 O
97.20¬±0.40 O
CCA·Ωë2 O
51.03¬±1.20 O
80.80¬±2.75 O
83.30¬±4.47 O
84.97¬±1.44 O
GenSen O
86.41¬±0.66CAT·Ωë2 O
86.86¬±0.62 O
95.63¬±0.47 O
97.22¬±0.27 O
97.42¬±0.31 O
KCCA·Ωë284.72¬±0.93 O
93.23¬±0.38 O
96.19¬±0.21 O
96.60¬±0.37 O
CCA·Ωë2 O
51.48¬±1.02 O
86.28¬±1.76 O
87.30¬±2.12 O
87.47¬±2.17 O
InferSent O
84.3¬±0.63CAT·Ωë2 O
50.36¬±0.62 O
92.30¬±1.26 O
97.90¬±1.37 O
97.10¬±1.22 O
KCCA·Ωë250.09¬±0.68 O
92.40¬±1.11 O
97.62¬±0.48 O
98.20¬±1.40 O
CCA·Ωë2 O
52.56¬±1.15 O
54.50¬±4.92 O
54.20¬±5.15 O
61.00¬±4.64 O
Table O
5 O
: O
Test O
accuracy O
( O
¬±std O
dev O
) O
for O
Amazon O
, O
Yelp O
and O
IMDB O
review O
datasets O
. O
Default O
values O
are O
performance O
of O
the O
domain O
speciÔ¨Åc O
models O
. O
Default O
values O
for O
BERT O
, O
Gensen O
and O
InferSent O
correspond O
to O
Ô¨Åne O
- O
tuning O
them O
. O
Best O
results O
for O
each O
pre O
- O
trained O
model O
are O
highlighted O
in O
boldface O
. O
MR O
MPQA O
SUBJ O
TREC O
BERT O
No O
- O
FT O
83.26¬±0.67 O
87.44¬±1.37 O
95.96¬±0.27 O
88.06¬±1.90 O
BERT O
FT O
86.22¬±0.85 O
90.47¬±1.04 O
96.95¬±0.14 O
96.40¬±0.67 O
CNN O
- O
NS O
80.93¬±0.16 O
88.38¬±0.28 O
89.25¬±0.08 O
92.98¬±0.89 O
CCA·Ωë2(C O
NN O
- O
NS)85.41¬±1.18 O
77.22¬±1.82 O
94.55¬±0.44 O
84.28¬±2.96 O
CAT·Ωë2(C O
NN O
- O
NS)85.60¬±0.95 O
90.06¬±0.48 O
95.92¬±0.26 O
96.64¬±1.07 O
CAT O
/ O
lock O
- O
open(C O
NN O
- O
NS O
) O
87.15¬±0.70 O
91.19¬±0.84 O
97.60¬±0.23 O
97.06¬±0.48 O
Table O
6 O
: O
Test O
accuracy O
( O
¬±std O
dev O
) O
for O
MR O
, O
MPQA O
, O
SUBJ O
and O
TREC O
datasets O
. O
Best O
results O
on O
the O
datasets O
are O
highlighted O
in O
boldface O
. O
The O
domain O
speciÔ¨Åc O
embedding O
model O
used O
is O
CNN O
- O
non O
- O
static O
, O
and O
the O
pre O
- O
trained O
model O
used O
is O
BERT.469Proceedings O
of O
the O
1st O
Conference O
of O
the O
Asia O
- O
PaciÔ¨Åc O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
10th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
470‚Äì490 O
December O
4 O
- O
7 O
, O
2020 O
. O
¬© O
2020 O
Association O
for O
Computational O
Linguistics O
Multimodal O
Pretraining O
for O
Dense O
Video O
Captioning O
Gabriel O
Huang1‚àó O
, O
Bo O
Pang2 O
, O
Zhenhai O
Zhu2 O
, O
Clara O
Rivera2 O
, O
Radu O
Soricut2 O
1Mila O
& O
University O
of O
Montreal O
2Google O
Research O
gabriel.huang@umontreal.ca O
{ O
bopang O
, O
zhenhai O
, O
rivera O
, O
rsoricut O
} O
@google.com O
Abstract O
Learning O
speciÔ¨Åc O
hands O
- O
on O
skills O
such O
as O
cooking O
, O
car O
maintenance O
, O
and O
home O
repairs O
increasingly O
happens O
via O
instructional O
videos O
. O
The O
user O
experience O
with O
such O
videos O
is O
known O
to O
be O
improved O
by O
meta O
- O
information O
such O
as O
time O
- O
stamped O
annotations O
for O
the O
main O
steps O
involved O
. O
Generating O
such O
annotations O
automatically O
is O
challenging O
, O
and O
we O
describe O
here O
two O
relevant O
contributions O
. O
First O
, O
we O
construct O
and O
release O
a O
new O
dense O
video O
captioning O
dataset O
, O
VideoTimeline O
Tags O
( O
ViTT O
) O
, O
featuring O
a O
variety O
of O
instructional O
videos O
together O
with O
time O
- O
stamped O
annotations O
. O
Second O
, O
we O
explore O
several O
multimodal O
sequenceto O
- O
sequence O
pretraining O
strategies O
that O
leverage O
large O
unsupervised O
datasets O
of O
videos O
and O
caption O
- O
like O
texts O
. O
We O
pretrain O
and O
subsequently O
Ô¨Ånetune O
dense O
video O
captioning O
models O
using O
both O
YouCook2 O
and O
ViTT O
. O
We O
show O
that O
such O
models O
generalize O
well O
and O
are O
robust O
over O
a O
wide O
variety O
of O
instructional O
videos O
. O
1 O
Introduction O
YouTube O
recently O
reported O
that O
a O
billion O
hours O
of O
videos O
were O
being O
watched O
on O
the O
platform O
every O
day O
( O
YouTubeBlog O
, O
2017 O
) O
. O
In O
addition O
, O
the O
amount O
of O
time O
people O
spent O
watching O
online O
videos O
was O
estimated O
to O
grow O
at O
an O
average O
rate O
of O
32 O
% O
a O
year O
between O
2013 O
and O
2018 O
, O
with O
an O
average O
person O
forecasted O
to O
watch O
100 O
minutes O
of O
online O
videos O
per O
day O
in O
2021 O
( O
ZenithMedia O
, O
2019 O
) O
. O
An O
important O
reason O
for O
this O
fast O
- O
growing O
video O
consumption O
is O
information O
- O
seeking O
. O
For O
instance O
, O
people O
turn O
to O
YouTube O
‚Äú O
hungry O
for O
how O
- O
to O
and O
learning O
content O
‚Äù O
( O
O‚ÄôNeil O
- O
Hart O
, O
2018 O
) O
. O
Indeed O
, O
compared O
to O
traditional O
content O
format O
such O
as O
text O
, O
video O
carries O
richer O
information O
to O
satisfy O
such O
‚àóThis O
work O
was O
done O
while O
Gabriel O
Huang O
was O
an O
intern O
at O
Google O
Research O
. O
Groundtruth O
Varying O
stiching O
speeds O
√ò O
- O
Pretraining O
Showing O
other O
parts O
MASS O
- O
Pretraining O
Explaining O
how O
to O
do O
a O
stitch O
Figure O
1 O
: O
Dense O
video O
captioning O
using O
ViTT O
‚Äì O
trained O
models O
. O
For O
the O
given O
video O
scene O
, O
we O
show O
the O
ViTT O
annotation O
( O
Groundtruth O
) O
and O
model O
outputs O
( O
no O
pretraining O
and O
MASS O
- O
based O
pretraining O
) O
. O
needs O
. O
But O
as O
a O
content O
media O
, O
videos O
are O
also O
inherently O
more O
difÔ¨Åcult O
to O
skim O
through O
, O
making O
it O
harder O
to O
quickly O
target O
the O
relevant O
part(s O
) O
of O
a O
video O
. O
Recognizing O
this O
difÔ¨Åculty O
, O
search O
engines O
started O
showing O
links O
to O
‚Äú O
key O
moments O
‚Äù O
within O
videos O
in O
search O
results O
, O
based O
on O
timestamps O
and O
short O
descriptions O
provided O
by O
the O
content O
creators O
themselves.1This O
enables O
users O
to O
get O
a O
quick O
sense O
of O
what O
the O
video O
covers O
, O
and O
also O
to O
jump O
to O
a O
particular O
time O
in O
the O
video O
if O
so O
desired O
. O
This O
effort O
echoes O
prior O
work O
in O
the O
literature O
showing O
how O
users O
of O
instructional O
videos O
can O
beneÔ¨Åt O
from O
human O
- O
curated O
meta O
- O
data O
, O
such O
as O
a O
timeline O
pointing O
to O
the O
successive O
steps O
of O
a O
tutorial O
( O
Kim O
et O
al O
. O
, O
2014 O
; O
Margulieux O
et O
al O
. O
, O
2012 O
; O
Weir O
et O
al O
. O
, O
2015 O
) O
. O
Producing O
such O
meta O
- O
data O
in O
an O
automatic O
way O
would O
greatly O
scale O
up O
the O
efforts O
of O
providing O
easier O
information O
access O
to O
videos O
. O
This O
task O
is O
closely O
related O
to O
the O
dense O
video O
captioning O
task O
considered O
in O
prior O
work O
( O
Zhou O
et O
al O
. O
, O
2018a O
, O
c O
; O
Krishna O
et O
al O
. O
, O
2017 O
) O
, O
where O
an O
instructional O
video O
is O
Ô¨Årst O
segmented O
into O
its O
main O
steps O
, O
followed O
by O
segment O
- O
level O
caption O
generation O
. O
To O
date O
, O
the O
YouCook2 O
data O
set O
( O
Zhou O
et O
al O
. O
, O
2018a O
) O
is O
the O
largest O
annotated O
data O
set O
for O
dense O
1https://www.blog.google/products/ O
search O
/ O
key O
- O
moments O
- O
video O
- O
search/470video O
captioning O
. O
It O
contains O
annotations O
for O
2,000 O
cooking O
videos O
covering O
89 O
recipes O
, O
with O
per O
- O
recipe O
training O
/ O
validation O
split O
. O
Restricting O
to O
a O
small O
number O
of O
recipes O
is O
helpful O
for O
early O
exploratory O
work O
, O
but O
such O
restrictions O
impose O
barriers O
to O
model O
generalization O
and O
adoption O
that O
are O
hard O
to O
overcome O
. O
We O
directly O
address O
this O
problem O
by O
constructing O
a O
larger O
and O
broader O
- O
coverage O
annotated O
dataset O
that O
covers O
a O
wide O
range O
of O
instructional O
topics O
( O
cooking O
, O
repairs O
, O
maintenance O
, O
etc O
. O
) O
We O
make O
the O
results O
of O
our O
annotation O
efforts O
publicly O
available O
as O
VideoTimeline O
Tags O
( O
ViTT O
) O
2 O
, O
consisting O
of O
around O
8,000 O
videos O
annotated O
with O
timelines O
( O
on O
average O
7.1 O
segments O
per O
video O
, O
each O
segment O
with O
a O
short O
free O
- O
text O
description O
) O
. O
Using O
YouCook2 O
and O
the O
new O
ViTT O
dataset O
as O
benchmarks O
for O
testing O
model O
performance O
and O
generalization O
, O
we O
further O
focus O
on O
the O
subproblem O
of O
video O
- O
segment O
‚Äì O
level O
caption O
generation O
, O
assuming O
segment O
boundaries O
are O
given O
( O
Hessel O
et O
al O
. O
, O
2019 O
; O
Sun O
et O
al O
. O
, O
2019b O
; O
Luo O
et O
al O
. O
, O
2020 O
) O
. O
Motivated O
by O
the O
high O
cost O
of O
collecting O
human O
annotations O
, O
we O
investigate O
pretraining O
a O
video O
segment O
captioning O
model O
using O
unsupervised O
signals O
‚Äì O
ASR O
( O
Automatic O
Speech O
Recognition O
) O
tokens O
and O
visual O
features O
from O
instructional O
videos O
, O
and O
unpaired O
instruction O
steps O
extracted O
from O
independent O
sources O
: O
Recipe1 O
M O
( O
Marin O
et O
al O
. O
, O
2019 O
) O
and O
WikiHow O
( O
Koupaee O
and O
Wang O
, O
2018 O
) O
. O
In O
contrast O
to O
prior O
work O
that O
focused O
on O
BERT O
- O
style O
pretraining O
of O
encoder O
networks O
( O
Sun O
et O
al O
. O
, O
2019b O
, O
a O
) O
, O
our O
approach O
entails O
jointly O
pretraining O
both O
multimodal O
encoder O
and O
text O
- O
based O
decoder O
models O
via O
MASSstyle O
pretraining O
( O
Song O
et O
al O
. O
, O
2019 O
) O
. O
Our O
experiments O
show O
that O
pretraining O
with O
either O
text O
- O
only O
or O
multi O
- O
modal O
data O
provides O
signiÔ¨Åcant O
gains O
over O
no O
pretraining O
, O
on O
both O
the O
established O
YouCook2 O
benchmark O
and O
the O
new O
ViTT O
benchmark O
. O
The O
results O
we O
obtain O
establish O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
YouCook2 O
, O
and O
present O
strong O
performance O
numbers O
on O
the O
ViTT O
benchmark O
. O
These O
Ô¨Åndings O
help O
us O
conclude O
that O
the O
resulting O
models O
generalize O
well O
and O
are O
quite O
robust O
over O
a O
wide O
variety O
of O
instructional O
videos O
. O
2 O
Related O
Work O
Text O
- O
only O
Pretraining O
. O
Language O
pretraining O
models O
based O
on O
the O
Transformer O
neural O
net2Available O
at O
https://github O
. O
com O
/ O
google O
- O
research O
- O
datasets/ O
Video O
- O
Timeline O
- O
Tags O
- O
ViTTwork O
architecture O
( O
Vaswani O
et O
al O
. O
, O
2017a O
) O
such O
as O
BERT O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
, O
GPT O
( O
Radford O
et O
al O
. O
, O
2018 O
) O
, O
RoBERTa O
( O
Liu O
et O
al O
. O
, O
2019 O
) O
, O
MASS O
( O
Song O
et O
al O
. O
, O
2019 O
) O
and O
ALBERT O
( O
Lan O
et O
al O
. O
, O
2020 O
) O
have O
achieved O
state O
- O
of O
- O
the O
- O
art O
results O
on O
many O
NLP O
tasks O
. O
MASS O
( O
Song O
et O
al O
. O
, O
2019 O
) O
has O
been O
recently O
proposed O
as O
a O
joint O
encoder O
- O
decoder O
pretraining O
strategy O
. O
For O
sequence O
- O
to O
- O
sequence O
tasks O
, O
this O
strategy O
is O
shown O
to O
outperform O
approaches O
that O
separately O
pretrain O
the O
encoder O
( O
using O
a O
BERT O
- O
style O
objective O
) O
and O
the O
decoder O
( O
using O
a O
language O
modeling O
objective O
) O
. O
UniLM O
( O
Dong O
et O
al O
. O
, O
2019 O
) O
, O
BART O
( O
Lewis O
et O
al O
. O
, O
2019 O
) O
, O
and O
T5 O
( O
Raffel O
et O
al O
. O
, O
2019 O
) O
propose O
uniÔ¨Åed O
pretraining O
approaches O
for O
both O
understanding O
and O
generation O
tasks O
. O
Multimodal O
Pretraining O
. O
VideoBERT O
( O
Sun O
et O
al O
. O
, O
2019b O
) O
, O
CBT O
( O
Sun O
et O
al O
. O
, O
2019a O
) O
and O
ActBERT O
( O
Zhu O
and O
Yang O
, O
2020 O
) O
use O
a O
BERT O
- O
style O
objective O
to O
train O
both O
video O
and O
ASR O
text O
encoders O
. O
Alayrac O
et O
al O
. O
( O
2016 O
) O
and O
Miech O
et O
al O
. O
( O
2020 O
) O
use O
margin O
- O
based O
loss O
functions O
to O
learn O
joint O
representations O
for O
video O
and O
ASR O
, O
and O
evaluate O
them O
on O
downstream O
tasks O
such O
as O
video O
captioning O
, O
action O
segmentation O
and O
anticipation O
, O
and O
action O
localization O
. O
An O
independent O
and O
concurrent O
work O
( O
UniViLM O
) O
by O
Luo O
et O
al O
. O
( O
2020 O
) O
is O
closely O
related O
to O
ours O
in O
that O
we O
share O
some O
similar O
pretraining O
objectives O
, O
some O
of O
the O
pretraining O
setup O
‚Äì O
HowTo100 O
M O
( O
Alayrac O
et O
al O
. O
, O
2016 O
) O
, O
and O
the O
down O
- O
stream O
video O
captioning O
benchmark O
using O
YouCook2 O
( O
Zhou O
et O
al O
. O
, O
2018a O
) O
. O
The O
main O
difference O
is O
that O
they O
use O
BERT O
- O
style O
pretraining O
for O
encoder O
and O
language O
- O
modeling O
style O
pretraining O
for O
decoder O
, O
whereas O
we O
use O
MASS O
- O
style O
pre O
- O
training O
to O
pretrain O
encoder O
and O
decoder O
jointly O
. O
Other O
approaches O
such O
as O
ViLBERT O
( O
Lu O
et O
al O
. O
, O
2019 O
) O
, O
LXMERT O
( O
Tan O
and O
Bansal O
, O
2019 O
) O
, O
Unicoder O
- O
VL O
( O
Li O
et O
al O
. O
, O
2019 O
) O
, O
VL O
- O
BERT O
( O
Su O
et O
al O
. O
, O
2019 O
) O
, O
and O
UNITER O
( O
Chen O
et O
al O
. O
, O
2019 O
) O
focus O
on O
pretraining O
joint O
representations O
for O
text O
and O
image O
, O
evaluating O
them O
on O
downstream O
tasks O
such O
as O
visual O
question O
answering O
, O
image O
- O
text O
retrieval O
and O
referring O
expressions O
. O
Dense O
Video O
Captioning O
. O
In O
this O
paper O
, O
we O
focus O
on O
generating O
captions O
at O
the O
segment O
- O
level O
, O
which O
is O
a O
sub O
- O
task O
of O
the O
so O
- O
called O
dense O
video O
captioning O
task O
( O
Krishna O
et O
al O
. O
, O
2017 O
) O
, O
where O
Ô¨Ånegrained O
captions O
are O
generated O
for O
video O
segments O
, O
conditioned O
on O
an O
input O
video O
with O
pre O
- O
deÔ¨Åned471Name O
Type O
# O
segments O
Pretraining O
datasets O
YT8M O
- O
cook O
ASR+video O
186 O
K O
HowTo100 O
M O
ASR+video O
8.0 O
M O
Recipe1 O
M O
CAP O
- O
style O
10.8 O
M O
WikiHow O
CAP O
- O
style O
1.3 O
M O
Finetuning O
datasets O
YouCook2 O
ASR+video+CAP O
11.5 O
K O
ViTT O
- O
All O
ASR+video+CAP O
88.5 O
K O
Table O
1 O
: O
Datasets O
used O
in O
this O
work O
, O
along O
with O
size O
of O
the O
data O
measured O
by O
the O
total O
number O
of O
segments O
. O
event O
segments O
. O
This O
is O
different O
from O
the O
video O
captioning O
models O
that O
generate O
a O
single O
summary O
for O
the O
entire O
video O
( O
Wang O
et O
al O
. O
, O
2019 O
) O
. O
Hessel O
et O
al O
. O
( O
2019 O
) O
make O
use O
of O
ASR O
and O
video O
for O
segment O
- O
level O
captioning O
on O
YouCook2 O
and O
show O
that O
most O
of O
the O
performance O
comes O
from O
ASR O
. O
Shi O
et O
al O
. O
( O
2019 O
) O
; O
Luo O
et O
al O
. O
( O
2020 O
) O
train O
their O
dense O
video O
captioning O
models O
on O
both O
video O
frames O
and O
ASR O
text O
and O
demonstrate O
the O
beneÔ¨Åts O
of O
adding O
ASR O
as O
an O
input O
to O
the O
model O
. O
There O
are O
also O
a O
number O
of O
video O
captioning O
approaches O
that O
do O
not O
use O
ASR O
directly O
( O
Zhou O
et O
al O
. O
, O
2018c O
; O
Pan O
et O
al O
. O
, O
2020 O
; O
Zheng O
et O
al O
. O
, O
2020 O
; O
Zhang O
et O
al O
. O
, O
2020 O
; O
Lei O
et O
al O
. O
, O
2020 O
) O
. O
Instructional O
video O
captioning O
data O
sets O
. O
In O
addition O
to O
YouCook2 O
( O
Zhou O
et O
al O
. O
, O
2018a O
) O
, O
there O
are O
two O
other O
smaller O
data O
sets O
in O
the O
instructional O
video O
captioning O
category O
. O
Epic O
Kitchen O
( O
Damen O
et O
al O
. O
, O
2018 O
) O
features O
55 O
hours O
of O
video O
consisting O
of O
11.5 O
M O
frames O
, O
which O
were O
densely O
labeled O
for O
a O
total O
of O
39.6 O
K O
action O
segments O
and O
454.3 O
K O
object O
bounding O
boxes O
. O
How2 O
( O
Sanabria O
et O
al O
. O
, O
2018 O
) O
consists O
of O
instructional O
videos O
with O
video O
- O
level O
( O
as O
opposed O
to O
segment O
- O
level O
) O
descriptions O
, O
authored O
by O
the O
video O
creators O
themselves O
. O
3 O
Data O
We O
present O
the O
datasets O
used O
for O
pretraining O
, O
Ô¨Ånetuning O
, O
and O
evaluation O
in O
Table O
1 O
. O
We O
also O
describe O
in O
detail O
the O
newly O
introduced O
dense O
video O
captioning O
dataset O
, O
VideoTimeline O
Tags O
( O
ViTT O
) O
. O
3.1 O
Dense O
Video O
- O
Captioning O
Datasets O
Our O
goal O
is O
to O
generate O
captions O
( O
CAP O
) O
for O
video O
segments O
. O
We O
consider O
two O
datasets O
with O
segment O
- O
level O
captions O
for O
Ô¨Åne O
- O
tuning O
and O
evaluating O
ASR+Video‚ÜíCAP O
models O
. O
YouCook2 O
. O
Up O
to O
this O
point O
, O
YouCook2 O
( O
Zhou O
et O
al O
. O
, O
2018a O
) O
has O
been O
the O
largest O
human O
- O
annotated O
dense O
- O
captioning O
dataset O
of O
instructional O
videos O
publicly O
available O
. O
It O
originally O
contained O
2,000 O
cooking O
videos O
from O
YouTube O
. O
Starting O
from O
110 O
recipe O
types O
( O
e.g. O
, O
‚Äú O
shrimp O
tempura O
‚Äù O
) O
, O
25 O
unique O
videos O
per O
recipe O
type O
were O
collected O
; O
the O
recipe O
types O
that O
did O
not O
gather O
enough O
videos O
were O
dropped O
, O
resulting O
in O
a O
total O
of O
89 O
recipe O
types O
in O
the O
Ô¨Ånal O
dataset O
. O
In O
addition O
, O
Zhou O
et O
al O
. O
( O
2018b O
) O
‚Äú O
randomly O
split O
the O
videos O
belonging O
to O
each O
recipe O
into O
67%:23%:10 O
% O
as O
training O
, O
validation O
and O
test O
sets3 O
, O
‚Äù O
which O
effectively O
guarantees O
that O
videos O
in O
the O
validation O
and O
test O
sets O
are O
never O
about O
unseen O
recipes O
. O
Annotators O
were O
then O
asked O
to O
construct O
recipe O
steps O
for O
each O
video O
‚Äî O
that O
is O
, O
identify O
the O
start O
and O
end O
times O
for O
each O
step O
, O
and O
provide O
a O
recipe O
- O
like O
description O
of O
each O
step O
. O
Overall O
, O
they O
reported O
an O
average O
of O
7.7 O
segments O
per O
video O
, O
and O
8.8 O
words O
per O
description O
. O
After O
removing O
videos O
that O
had O
been O
deleted O
by O
users O
, O
we O
obtained O
a O
total O
of O
11,549 O
segments O
. O
ViTT O
. O
One O
limitation O
of O
the O
YouCook2 O
dataset O
is O
the O
artiÔ¨Åcially O
imposed O
( O
almost O
) O
uniform O
distribution O
of O
videos O
over O
89 O
recipes O
. O
While O
this O
may O
help O
making O
the O
task O
more O
tractable O
, O
it O
is O
difÔ¨Åcult O
to O
judge O
whether O
performance O
on O
its O
validation O
/ O
test O
sets O
can O
be O
generalized O
to O
unseen O
topics O
. O
The O
design O
of O
our O
ViTT O
dataset O
annotation O
process O
is O
aimed O
at O
Ô¨Åxing O
some O
of O
these O
drawbacks O
. O
We O
started O
by O
collecting O
a O
large O
dataset O
of O
videos O
containing O
a O
broader O
variety O
of O
topics O
to O
better O
reÔ¨Çect O
topic O
distribution O
in O
the O
wild O
. O
SpeciÔ¨Åcally O
, O
we O
randomly O
sampled O
instructional O
videos O
from O
the O
YouTube-8 O
M O
dataset O
( O
Abu O
- O
El O
- O
Haija O
et O
al O
. O
, O
2016 O
) O
, O
a O
large O
- O
scale O
collection O
of O
YouTube O
videos O
that O
also O
contain O
topical O
labels O
. O
Since O
much O
of O
prior O
work O
in O
this O
area O
revolved O
around O
cooking O
videos O
, O
we O
aimed O
at O
sampling O
a O
signiÔ¨Åcant O
proportion O
of O
our O
data O
from O
videos O
with O
cooking O
labels O
( O
speciÔ¨Åcally O
, O
‚Äú O
Cooking O
‚Äù O
and O
‚Äú O
Recipe O
‚Äù O
) O
. O
Aside O
from O
the O
intentional O
bias O
regarding O
cooking O
videos O
, O
the O
rest O
of O
the O
videos O
were O
selected O
by O
randomly O
sampling O
non O
- O
cooking O
videos O
, O
including O
only O
those O
that O
were O
considered O
to O
be O
instructional O
videos O
by O
our O
human O
annotators O
. O
Once O
candidate O
videos O
were O
identiÔ¨Åed O
, O
timeline O
annotations O
and O
descriptive O
tags O
were O
collected O
. O
3Note O
that O
no O
annotations O
are O
provided O
for O
the O
test O
split O
; O
we O
conducted O
our O
own O
training O
/ O
dev O
/ O
test O
split O
over O
available O
videos.472Our O
motivation O
was O
to O
enable O
downstream O
applications O
to O
allow O
navigating O
to O
speciÔ¨Åc O
content O
sections O
. O
Therefore O
, O
annotators O
were O
asked O
to O
identify O
the O
main O
steps O
in O
a O
video O
and O
mark O
their O
start O
time O
. O
They O
were O
also O
asked O
to O
produce O
a O
descriptive O
- O
yetconcise O
, O
free O
- O
text O
tag O
for O
each O
step O
( O
e.g. O
, O
‚Äú O
shaping O
the O
cookies O
‚Äù O
, O
‚Äú O
removing O
any O
leftover O
glass O
‚Äù O
) O
. O
A O
subset O
of O
the O
videos O
has O
received O
more O
than O
one O
complete O
annotation O
( O
main O
steps O
plus O
tags O
) O
. O
The O
resulting O
ViTT O
dataset O
consists O
of O
a O
total O
of O
8,169 O
videos O
, O
of O
which O
3,381 O
are O
cooking O
- O
related O
. O
A O
total O
of O
5,840 O
videos O
have O
received O
only O
one O
annotation O
, O
and O
have O
been O
designated O
as O
the O
training O
split O
. O
Videos O
with O
more O
than O
one O
annotation O
have O
been O
designated O
as O
validation O
/ O
test O
data O
. O
Overall O
, O
there O
are O
7.1 O
segments O
per O
video O
on O
average O
( O
max O
: O
19 O
) O
. O
Given O
the O
dataset O
design O
, O
descriptions O
are O
much O
shorter O
in O
length O
compared O
to O
YouCook2 O
: O
on O
average O
there O
are O
2.97 O
words O
per O
tag O
( O
max O
: O
16 O
) O
‚Äî O
20 O
% O
of O
the O
captions O
are O
single O
- O
word O
, O
22 O
% O
are O
two O
- O
words O
, O
and O
25 O
% O
are O
three O
words O
. O
Note O
that O
the O
average O
caption O
length O
is O
signiÔ¨Åcantly O
shorter O
than O
for O
YouCook2 O
, O
which O
is O
not O
surprising O
given O
our O
motivation O
of O
providing O
short O
and O
concise O
timeline O
tags O
for O
video O
navigation O
. O
We O
standardized O
the O
paraphrases O
among O
the O
top-20 O
most O
frequent O
captions O
. O
For O
instance,{‚Äúintro O
‚Äù O
, O
‚Äú O
introduction O
‚Äù O
} O
‚Üí O
‚Äú O
intro O
‚Äù O
. O
Otherwise O
, O
we O
have O
preserved O
the O
original O
tags O
asis O
, O
even O
though O
additional O
paraphrasing O
most O
definitely O
exists O
. O
Annotators O
were O
instructed O
to O
start O
and O
end O
the O
video O
with O
an O
opening O
and O
closing O
segment O
as O
possible O
. O
As O
a O
result O
, O
the O
most O
frequent O
tag O
( O
post O
- O
standardization O
) O
in O
the O
dataset O
is O
‚Äú O
intro O
‚Äù O
, O
which O
accounts O
for O
roughly O
11 O
% O
of O
the O
88,455 O
segments O
. O
More O
details O
on O
the O
data O
collection O
process O
and O
additional O
analysis O
can O
be O
found O
in O
the O
Supplementary O
Material O
( O
Section O
A.1 O
) O
. O
Overall O
, O
this O
results O
in O
56,027 O
unique O
tags O
, O
with O
a O
vocabulary O
size O
of O
12,509 O
token O
types O
over O
88,455 O
segments O
. O
In O
this O
paper O
, O
we O
consider O
two O
variants O
: O
the O
full O
dataset O
( O
ViTT O
- O
All O
) O
, O
and O
the O
cooking O
subset O
( O
ViTT O
- O
Cooking O
) O
. O
3.2 O
Pretraining O
Datasets O
: O
ASR+Video O
We O
consider O
two O
large O
- O
scale O
unannotated O
video O
datasets O
for O
pretraining O
, O
as O
described O
below O
. O
Timestamped O
ASR O
tokens O
were O
obtained O
via O
YouTube O
Data O
API,4and O
split O
into O
ASR O
segments O
if O
the O
timestamps O
of O
two O
consecutive O
words O
are O
more O
4https://developers.google.com/ O
youtube O
/ O
v3 O
/ O
docs O
/ O
captionsthan O
2 O
seconds O
apart O
, O
or O
if O
a O
segment O
is O
longer O
than O
a O
pre O
- O
speciÔ¨Åed O
max O
length O
( O
in O
our O
case O
, O
320 O
words O
) O
. O
They O
were O
paired O
with O
concurrent O
video O
frames O
in O
the O
same O
segment O
. O
YT8M O
- O
cook O
We O
extract O
the O
cooking O
subset O
of O
YouTube-8 O
M O
( O
Abu O
- O
El O
- O
Haija O
et O
al O
. O
, O
2016 O
) O
by O
taking O
, O
from O
its O
training O
split O
, O
videos O
with O
‚Äú O
Cooking O
‚Äù O
or O
‚Äú O
Recipe O
‚Äù O
labels O
, O
and O
retain O
those O
with O
English O
ASR O
, O
subject O
to O
YouTube O
policies O
. O
After O
preprocessing O
, O
we O
obtain O
186 O
K O
ASR+video O
segments O
with O
an O
average O
length O
of O
64 O
words O
( O
24 O
seconds O
) O
per O
segment O
. O
HowTo100M. O
This O
is O
based O
on O
the O
1.2 O
M O
YouTube O
instructional O
videos O
released O
by O
Miech O
et O
al O
. O
( O
2019 O
) O
, O
covering O
a O
broad O
range O
of O
topics O
. O
After O
preprocessing O
, O
we O
obtain O
7.99 O
M O
ASR+video O
segments O
with O
an O
average O
of O
78 O
words O
( O
28.7 O
seconds O
) O
per O
segment O
. O
3.3 O
Pretraining O
Datasets O
: O
CAP O
- O
style O
We O
also O
consider O
two O
text O
- O
only O
datasets O
for O
pretraining O
, O
containing O
unpaired O
instruction O
steps O
similar O
in O
style O
to O
the O
target O
captions O
. O
Recipe1 O
M O
is O
a O
collection O
of O
1 O
M O
recipes O
scraped O
from O
a O
number O
of O
popular O
cooking O
websites O
( O
Marin O
et O
al O
. O
, O
2019 O
) O
. O
We O
use O
the O
sequence O
of O
instructions O
extracted O
for O
each O
recipe O
in O
this O
dataset O
, O
and O
treat O
each O
recipe O
step O
as O
a O
separate O
example O
during O
pretraining O
. O
This O
results O
in O
10,767,594 O
CAP O
- O
style O
segments O
, O
with O
12.8 O
words O
per O
segment O
. O
WikiHow O
is O
a O
collection O
of O
230,843 O
articles O
extracted O
from O
the O
WikiHow O
knowledge O
base O
( O
Koupaee O
and O
Wang O
, O
2018 O
) O
. O
Each O
article O
comes O
with O
a O
title O
starting O
with O
‚Äú O
How O
to O
‚Äù O
. O
Each O
associated O
step O
starts O
with O
a O
step O
summary O
( O
in O
bold O
) O
followed O
by O
a O
detailed O
explanation O
. O
We O
extract O
the O
all O
step O
summaries O
, O
resulting O
in O
1,360,145 O
CAP O
- O
style O
segments O
, O
with O
8.2 O
words O
per O
segment O
. O
Again O
, O
each O
instruction O
step O
is O
considered O
as O
a O
separate O
example O
during O
pretraining O
. O
3.4 O
Differences O
between O
Pretraining O
and O
Finetuning O
Datasets O
First O
, O
note O
that O
video O
segments O
are O
deÔ¨Åned O
differently O
for O
pretraining O
and O
Ô¨Ånetuning O
datasets O
, O
and O
may O
not O
match O
exactly O
. O
For O
ASR+Video O
pretraining O
datasets O
, O
which O
are O
unsupervised O
, O
the O
segments O
are O
divided O
following O
a O
simple O
heuristic O
( O
e.g. O
, O
two O
consecutive O
words O
more O
than O
2 O
seconds O
apart O
) O
, O
whereas O
for O
Ô¨Ånetuning O
ASR+Video O
‚ÜíCAP O
datasets O
, O
which O
are O
supervised O
, O
the O
segments O
are O
deÔ¨Åned O
by473human O
annotators O
to O
correspond O
to O
instruction O
steps O
. O
Otherwise O
, O
the O
ASR O
data O
are O
relatively O
similar O
between O
pretraining O
and O
Ô¨Ånetuning O
datasets O
, O
as O
both O
come O
from O
instructional O
videos O
and O
are O
in O
the O
style O
of O
spoken O
language O
. O
Second O
, O
compared O
to O
the O
target O
captions O
in O
Ô¨Ånetuning O
datasets O
, O
the O
CAP O
- O
like O
pretraining O
datasets O
are O
similar O
in O
spirit O
‚Äî O
they O
all O
represent O
summaries O
ofsteps O
, O
but O
they O
may O
differ O
in O
length O
, O
style O
and O
granularity O
. O
In O
particular O
, O
the O
CAP O
- O
like O
pretraining O
datasets O
are O
closer O
in O
style O
to O
captions O
in O
YouCook2 O
, O
where O
annotators O
were O
instructed O
to O
produce O
a O
recipe O
- O
like O
description O
for O
each O
step O
. O
This O
is O
reÔ¨Çected O
in O
their O
similar O
average O
length O
( O
YouCook2 O
: O
8.8 O
words O
, O
Recipe1 O
M O
: O
12.8 O
words O
, O
WikiHow O
: O
8.2 O
words O
) O
; O
whereas O
captions O
in O
ViTT O
are O
signiÔ¨Åcantly O
shorter O
( O
2.97 O
words O
on O
average O
) O
. O
Despite O
these O
differences O
‚Äî O
some O
are O
inevitable O
due O
to O
the O
unsupervised O
nature O
of O
pretraining O
datasets O
‚Äî O
the O
pretraining O
data O
is O
very O
helpful O
for O
our O
task O
as O
shown O
in O
the O
experimental O
results O
. O
4 O
Method O
To O
model O
segment O
- O
level O
caption O
generation O
, O
we O
adopt O
MASS O
- O
style O
pretraining O
( O
Song O
et O
al O
. O
, O
2019 O
) O
with O
Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017b O
) O
as O
the O
backbone O
architecture O
. O
For O
both O
pre O
- O
training O
and O
Ô¨Åne O
- O
tuning O
objectives O
, O
we O
have O
considered O
two O
variants O
: O
text O
- O
only O
and O
multi O
- O
modal O
. O
They O
are O
summarized O
in O
Table O
2 O
and O
more O
details O
are O
given O
below O
. O
4.1 O
Separate O
- O
Modality O
Architecture O
Both O
ASR O
tokens O
and O
video O
segment O
features O
are O
given O
as O
input O
in O
the O
multimodal O
variants O
. O
We O
consider O
an O
architecture O
with O
a O
separate O
transformer O
for O
each O
modality O
( O
text O
or O
video O
) O
, O
see O
Figure O
2 O
for O
details O
. O
When O
available O
, O
the O
text O
and O
video O
encoders O
attend O
to O
each O
other O
at O
every O
layer O
using O
cross O
- O
modal O
attention O
, O
as O
in O
ViLBERT O
( O
Lu O
et O
al O
. O
, O
2019 O
) O
. O
The O
text O
decoder O
attends O
over O
the O
Ô¨Ånal O
- O
layer O
output O
of O
both O
encoders O
. O
We O
discuss O
in O
more O
detail O
the O
differences O
between O
using O
a O
separate O
- O
modality O
architecture O
vs. O
a O
vanilla O
- O
Transformer O
approach O
for O
all O
modalities O
in O
Appendix O
A.2 O
. O
The O
inputs O
to O
the O
text O
encoder O
is O
the O
sum O
of O
three O
components O
: O
text O
token O
embeddings O
, O
positional O
embeddings O
and O
the O
corresponding O
style O
embeddings,5depending O
on O
the O
style O
of O
the O
text O
( O
ASR O
or O
Caption O
- O
like O
) O
. O
The O
inputs O
to O
the O
video O
encoder O
5This O
is O
similar O
to O
the O
way O
language O
- O
ID O
embeddings O
are O
used O
in O
machine O
translation.could O
be O
either O
precomputed O
frame O
- O
level O
2D O
CNN O
features O
or O
3D O
CNN O
features O
, O
pretrained O
on O
the O
Kinetics O
( O
Carreira O
and O
Zisserman O
, O
2017 O
; O
Kay O
et O
al O
. O
, O
2017 O
) O
data O
set O
. O
The O
visual O
features O
are O
projected O
with O
fully O
- O
connected O
layers O
to O
the O
same O
dimension O
as O
the O
text O
embeddings O
. O
The O
main O
architecture O
we O
consider O
is O
a O
2 O
- O
layer O
encoder O
( O
E2 O
) O
, O
6 O
- O
layer O
decoder O
( O
D6 O
) O
Transformer O
. O
We O
use O
E2D6 O
to O
refer O
to O
the O
text O
- O
only O
version O
, O
and O
E2vidD6 O
to O
refer O
to O
the O
multimodal O
version O
with O
an O
active O
video O
encoder O
. O
We O
also O
experiment O
with O
E2D2 O
and O
E2vidD2 O
( O
2 O
- O
layer O
decoder).6 O
4.2 O
Pretraining O
with O
Text O
- O
only O
MASS O
Text O
- O
only O
pretraining O
is O
essentially O
the O
unsupervised O
learning O
of O
the O
style O
transfer O
between O
ASRstyle O
and O
caption O
- O
style O
texts O
using O
unpaired O
data O
sources O
: O
ASR O
strings O
from O
video O
segments O
in O
YT8M O
- O
cook O
or O
HowTo100 O
M O
; O
and O
CAP O
- O
style O
instruction O
steps O
found O
in O
Recipe1 O
M O
or O
HowTo100M. O
Just O
like O
using O
MASS O
for O
unsupervised O
machine O
translation O
involves O
pretraining O
the O
model O
on O
unpaired O
monolingual O
datasets O
, O
we O
alternate O
between O
ASR‚ÜíASR O
and O
CAP‚ÜíCAP O
MASS O
steps O
during O
our O
pretraining O
stage O
, O
which O
does O
not O
require O
the O
‚Äú O
source O
‚Äù O
( O
ASR+Video O
) O
and O
‚Äú O
target O
‚Äù O
( O
CAP O
- O
style O
) O
data O
to O
be O
aligned O
. O
In O
an O
ASR‚ÜíASR O
step O
, O
we O
mask O
a O
random O
subsequence O
of O
the O
ASR O
and O
feed O
the O
masked O
ASR O
to O
the O
text O
encoder O
. O
The O
text O
decoder O
must O
reconstruct O
the O
hidden O
subsequence O
while O
attending O
to O
the O
encoder O
output O
. O
A O
CAP‚ÜíCAP O
step O
works O
similarly O
by O
trying O
to O
reconstruct O
a O
masked O
sequence O
of O
a O
CAP O
- O
style O
text O
. O
The O
encoder O
and O
decoder O
are O
trained O
jointly O
using O
teacher O
- O
forcing O
on O
the O
decoder O
. O
We O
denote O
this O
text O
- O
only O
strategy O
as O
MASS O
in O
the O
experiments O
. O
4.3 O
Pretraining O
with O
Multimodal O
MASS O
During O
multimodal O
pretraining O
, O
we O
alternate O
between O
text O
- O
only O
CAP‚ÜíCAP O
MASS O
steps O
and O
multimodal O
MASS O
steps O
. O
During O
each O
multimodal O
MASS O
step O
ASR+video‚ÜíASR O
, O
we O
feed O
a O
masked O
ASR O
to O
the O
text O
- O
encoder O
and O
the O
co O
- O
occurring O
video O
features O
to O
the O
video O
- O
encoder O
. O
The O
text O
decoder O
must O
reconstruct O
the O
masked O
ASR O
subsequence O
. O
We O
denote O
this O
pretraining O
strategy O
as O
MASSvid O
in O
the O
experiments O
. O
This O
trains O
cross O
- O
modal O
attention O
between O
the O
text O
- O
encoder O
and O
video O
- O
encoder O
6We O
found O
in O
a O
preliminary O
study O
that O
using O
6 O
- O
layer O
encoders O
did O
not O
improve O
performance O
for O
our O
application.474+++++ O
+ O
+ O
+ O
+ O
+ O
Text O
Encoder O
Layer O
1 O
  O
text O
CLStext O
after O
text O
spread@ O
text O
ingText O
Encoder O
Layer O
2 O
  O
Video O
Encoder O
Layer O
1 O
  O
video O
  O
emb O
. O
video O
  O
emb O
. O
video O
  O
emb O
. O
video O
  O
emb O
. O
Video O
Encoder O
Layer O
2 O
  O
[ O
CLS O
] O
   O
after O
spread@@ O
  O
ing O
    O
the O
  O
text O
the O
Text O
Embedding O
Layer O
Feature O
Projection O
Layers O
  O
( O
Masked O
) O
ASR O
  O
‚Äú O
after O
spreading O
the O
  O
[ O
MASK O
] O
[ O
MASK O
] O
the O
bread O
‚Äù O
Tokenize O
& O
Truncate O
f0 O
f1 O
f2 O
f3pos O
  O
0pos O
  O
1pos O
  O
2pos O
  O
3pos O
  O
4style O
   O
asrstyle O
   O
asrstyle O
   O
capstyle O
   O
cap O
Pretrained O
Feature O
Extractor O
+ O
+ O
+ O
+ O
pos O
  O
0pos O
  O
1pos O
  O
2pos O
  O
3Cross O
- O
Modal O
  O
Attention O
CLS O
output O
text O
output O
text O
output O
text O
output O
text O
output O
video O
  O
output O
video O
  O
output O
video O
  O
output O
video O
  O
output O
Segment O
  O
Alignment O
  O
0/1Segment O
  O
Ordering O
  O
0/1 O
Decoder O
( O
Text O
- O
only O
) O
Captioning O
( O
asr O
to O
cap O
) O
‚Äú O
spread O
butter O
‚Äù O
  O
( O
Masked O
) O
Cap O
  O
‚Äú O
spread O
butter O
‚Äù O
or O
orEncoder O
- O
Decoder O
  O
Multimodal O
Attention O
  O
Decoder O
Input O
( O
teacher O
forcing O
) O
MASS O
for O
ASR O
     O
‚Äú O
butter O
on O
‚Äù O
  O
MASS O
for O
Cap O
  O
‚Äú O
spread O
butter O
‚Äù O
  O
Pretraining O
Objectives O
Fine O
- O
tuning O
Objectives O
  O
Reverse O
Captioning O
( O
cap O
to O
asr O
) O
‚Äú O
after O
spread@@ O
ing O
the O
‚Äù O
  O
Encoder O
  O
( O
Multimodal O
) O
+ O
+ O
+ O
+ O
+ O
+ O
+ O
+ O
+ O
+ O
Text O
Decoder O
Layer O
1 O
  O
text O
CLStext O
after O
text O
spread@ O
text O
ingText O
Decoder O
Layer O
2 O
  O
[ O
CLS O
] O
   O
after O
spread@@ O
  O
ing O
    O
the O
  O
text O
the O
Text O
Embedding O
Layer O
  O
( O
Masked O
) O
ASR O
  O
‚Äú O
[ O
BOS O
] O
butter O
‚Äù O
Tokenize O
& O
Truncate O
pos O
  O
0pos O
  O
1pos O
  O
2pos O
  O
3pos O
  O
4 O
( O
Masked O
) O
Cap O
  O
‚Äú O
spread O
butter O
‚Äù O
or O
or O
Text O
Encoder O
Input O
Video O
Encoder O
Input O
style O
   O
asrstyle O
   O
asrstyle O
   O
capstyle O
   O
capFigure O
2 O
: O
A O
diagram O
for O
the O
separate O
- O
modality O
architecture O
. O
It O
consists O
of O
a O
two O
- O
stream O
( O
text O
and O
video O
inputs O
) O
encoder O
with O
cross O
- O
modal O
attention O
and O
a O
text O
- O
only O
decoder O
, O
jointly O
trained O
using O
the O
MASS O
objective O
. O
at O
every O
layer O
, O
jointly O
with O
the O
text O
decoder O
that O
attends O
to O
the O
output O
layer O
of O
both O
the O
text O
and O
video O
encoders.7 O
To O
force O
more O
cross O
- O
modal O
attention O
between O
encoder O
and O
decoder O
, O
we O
also O
investigate O
a O
strategy O
of O
hiding O
the O
text O
- O
encoder O
output O
from O
the O
decoder O
for O
some O
fraction O
of O
training O
examples O
. O
We O
refer O
to O
this O
strategy O
as O
MASSdrop O
in O
the O
experiments O
. O
4.4 O
Pretraining O
with O
Alignment O
and O
Ordering O
Tasks O
We O
also O
explore O
encoder O
- O
only O
multimodal O
pretraining O
strategies O
. O
We O
take O
the O
last O
- O
layer O
representation O
for O
the O
CLS O
( O
beginning O
of O
sentence O
) O
token O
from O
the O
encoder O
, O
and O
add O
a O
multi O
- O
layer O
perceptron O
on O
top O
of O
it O
for O
binary O
predictions O
( O
Figure O
2 O
) O
. O
Given O
a O
pair O
of O
ASR O
and O
video O
segment O
, O
we O
train O
the O
encoder O
to O
predict O
the O
following O
objectives O
: O
‚Ä¢Segment O
- O
Level O
Alignment O
. O
An O
( O
ASR O
, O
video O
) O
pair O
is O
aligned O
if O
they O
occur O
in O
the O
same O
pretraining O
segment O
; O
negative O
examples O
are O
constructed O
by O
sampling O
pairs O
from O
the O
same O
video O
but O
at O
least O
2 O
segments O
away O
. O
7In O
preliminary O
experiments O
, O
we O
had O
attempted O
to O
directly O
adapt O
the O
MASS O
objective O
( O
Song O
et O
al O
. O
, O
2019 O
) O
to O
video O
reconstruction O
‚Äî O
by O
masking O
a O
subsequence O
of O
the O
input O
video O
and O
making O
the O
video O
decoder O
reconstruct O
the O
input O
using O
the O
Noise O
Constrastive O
Estimator O
Loss O
( O
Sun O
et O
al O
. O
, O
2019a O
) O
. O
Due O
to O
limited O
success O
, O
we O
did O
not O
further O
pursue O
this O
approach.‚Ä¢Segment O
- O
Level O
Ordering O
. O
We O
sample O
( O
ASR O
, O
video O
) O
pairs O
that O
are O
at O
least O
2 O
segments O
away O
, O
and O
train O
the O
model O
to O
predict O
whether O
the O
ASR O
occurs O
before O
or O
after O
the O
video O
clip O
. O
During O
this O
MASSalign O
pretraining O
stage O
, O
we O
alternate O
between O
two O
text O
- O
only O
MASS O
steps O
( O
CAP‚ÜíCAP O
, O
ASR‚ÜíASR O
) O
and O
the O
two O
binary O
predictions O
( O
Alignment O
andOrdering O
) O
described O
above O
. O
4.5 O
Finetuning O
on O
Video O
Captioning O
For O
text O
- O
only O
Ô¨Ånetuning O
, O
we O
feed O
ASR O
to O
the O
text O
encoder O
and O
the O
decoder O
has O
to O
predict O
the O
corresponding O
CAP O
( O
ASR‚ÜíCAP O
) O
. O
For O
multimodal O
Ô¨Ånetuning O
, O
we O
also O
feed O
additional O
video O
representations O
to O
the O
video O
encoder O
( O
ASR+video‚ÜíCAP O
) O
. O
When O
Ô¨Ånetuning O
a O
multimodal O
model O
from O
textonly O
pretraining O
, O
everything O
related O
to O
video O
( O
weights O
in O
the O
video O
encoder O
and O
any O
cross O
- O
modal O
attention O
modules O
) O
will O
be O
initialized O
randomly O
. O
In O
addition O
to O
these O
uni O
- O
directional O
( O
UniD O
) O
Ô¨Ånetuning O
, O
we O
also O
experiment O
with O
several O
variants O
of O
bidirectional O
( O
BiD O
) O
Ô¨Ånetuning O
( O
Table O
2 O
) O
. O
For O
instance O
, O
adding O
CAP‚ÜíASR O
( O
predicting O
ASR O
from O
CAP O
) O
to O
text O
- O
only O
Ô¨Ånetuning O
. O
In O
the O
experiments O
, O
we O
Ô¨Ånd O
some O
variants O
of O
bidirectional O
Ô¨Ånetuning O
beneÔ¨Åcial O
whether O
training O
from O
scratch O
or O
Ô¨Ånetuning O
from O
a O
pretrained O
model.475Pretraining O
Objectives O
Name O
T O
V O
Input O
‚ÜíOutput O
MASS O
 O
 O
CAP‚ÜíCAP O
, O
ASR‚ÜíASR O
MASSvid O
 O
 O
CAP‚ÜíCAP O
, O
ASR+video‚ÜíASR O
MASSdrop O
 O
 O
CAP‚ÜíCAP O
, O
ASR+video‚ÜíASR O
MASSalign O
 O
CAP‚ÜíCAP O
, O
ASR‚ÜíASR O
, O
ASR+video‚Üí{0,1 O
} O
Finetuning O
Objectives O
Name O
T O
V O
Input O
‚ÜíOutput O
UniD O
 O
 O
ASR‚ÜíCAP O
BiD O
 O
 O
ASR‚ÜíCAP O
, O
CAP‚ÜíASR O
UniD O
 O
 O
ASR+video‚ÜíCAP O
BiD O
 O
 O
ASR+video‚ÜíCAP O
, O
CAP‚ÜíASR O
BiDalt O
 O
 O
ASR+video‚ÜíCAP O
, O
CAP+video‚ÜíASR O
Table O
2 O
: O
Pretraining O
and O
Fine O
- O
tuning O
objectives O
. O
For O
each O
strategy O
, O
indicates O
whether O
the O
text O
( O
T O
) O
and O
video O
( O
V O
) O
encoders O
are O
active O
, O
followed O
by O
a O
summary O
of O
training O
objectives O
involved O
in O
one O
training O
step O
. O
5 O
Experiments O
5.1 O
Implementation O
Details O
We O
tokenize O
ASR O
and O
CAP O
inputs O
using O
byte O
- O
pair O
‚Äì O
encoding O
subwords O
( O
Sennrich O
et O
al O
. O
, O
2015 O
) O
, O
and O
truncate O
them O
to O
240 O
subwords O
. O
We O
truncate O
video O
sequences O
to O
40 O
frames O
( O
40 O
seconds O
of O
video O
) O
, O
compute O
the O
128 O
- O
dim O
features O
proposed O
by O
Wang O
et O
al O
. O
( O
2014 O
) O
( O
which O
we O
will O
refer O
to O
as O
Compact O
2D O
features O
) O
, O
and O
project O
them O
to O
the O
embedding O
space O
using O
a O
two O
- O
layer O
perceptron O
with O
layer O
normalization O
and O
GeLU O
activations O
. O
We O
instantiate O
the O
E2xDx O
models O
deÔ¨Åned O
in O
Section O
4.1 O
with O
128 O
- O
dimensional O
embeddings O
and O
8 O
heads O
respectively O
for O
self O
- O
attention O
, O
encoderdecoder O
, O
and O
cross O
- O
modal O
attention O
modules O
. O
We O
deÔ¨Åne O
each O
epoch O
to O
be O
3,125 O
iterations O
, O
where O
each O
iteration O
contains O
one O
repetition O
of O
each O
training O
step O
as O
represented O
in O
Table O
2 O
. O
We O
pretrain O
for O
200 O
epochs O
and O
Ô¨Ånetune O
for O
30 O
epochs O
. O
For O
evaluation O
, O
we O
consider O
BLEU-4 O
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
, O
METEOR O
( O
Denkowski O
and O
Lavie O
, O
2014 O
) O
, O
ROUGE O
-L O
( O
Lin O
and O
Och O
, O
2004 O
) O
and O
CIDEr O
( O
Vedantam O
et O
al O
. O
, O
2015 O
) O
metrics O
. O
Please O
refer O
to O
Appendix O
A.3 O
for O
full O
implementation O
details O
, O
hyperparameters O
and O
computation O
cost O
. O
Notes O
on O
ViTT O
evaluation O
: O
With O
the O
exception O
ofROUGE O
-L O
, O
all O
other O
metrics O
are O
sensitive O
to O
short O
groundtruth O
. O
67 O
% O
of O
the O
groundtruth O
tags O
in O
ViTT O
have O
less O
than O
4 O
words O
, O
where O
a O
perfect O
prediction O
will O
not O
yield O
a O
full O
score O
in O
, O
say O
, O
BLEU-4 O
. O
Thus O
, O
wefocus O
mainly O
on O
ROUGE O
-L O
, O
report O
BLEU-1instead O
ofBLEU-4 O
for O
ViTT O
, O
and O
provide O
the O
other O
two O
metrics O
only O
as O
reference O
points O
. O
We O
had O
originally O
decided O
to O
use O
videos O
with O
multiple O
annotations O
as O
validation O
and O
test O
data O
, O
so O
that O
we O
could O
explore O
evaluation O
with O
multiple O
reference O
groundtruth O
captions O
. O
But O
as O
annotators O
do O
not O
always O
yield O
the O
same O
set O
of O
segment O
boundaries O
, O
this O
became O
tricky O
. O
Instead O
, O
we O
simply O
treat O
each O
segment O
as O
a O
separate O
instance O
with O
one O
single O
reference O
caption O
. O
Note O
that O
all O
segments O
annotated O
for O
the O
same O
video O
will O
be O
in O
either O
validation O
or O
test O
to O
ensure O
no O
content O
overlap O
. O
5.2 O
Main O
Results O
We O
run O
several O
variants O
of O
our O
method O
on O
YouCook2 O
, O
ViTT O
- O
All O
and O
ViTT O
- O
Cooking O
, O
using O
different O
architectures O
, O
modalities O
, O
pretraining O
datasets O
, O
pretraining O
and O
Ô¨Ånetuning O
strategies O
. O
Comparing O
with O
other O
methods O
on O
YouCook2 O
For O
YouCook2 O
, O
we O
report O
our O
method O
alongside O
several O
methods O
from O
the O
literature O
( O
Hessel O
et O
al O
. O
, O
2019 O
; O
Sun O
et O
al O
. O
, O
2019b O
; O
Zhou O
et O
al O
. O
, O
2018c O
; O
Lei O
et O
al O
. O
, O
2020 O
) O
, O
as O
well O
as O
state O
- O
of O
- O
the O
- O
art O
concurrent O
work O
( O
Luo O
et O
al O
. O
, O
2020 O
) O
. O
The O
related O
work O
is O
provided O
for O
reference O
and O
to O
give O
a O
ballpark O
estimate O
of O
the O
relative O
performance O
of O
each O
method O
, O
but O
results O
are O
not O
always O
strictly O
and O
directly O
comparable O
. O
Beyond O
the O
usual O
sources O
of O
discrepancy O
in O
data O
processing O
, O
tokenization O
, O
or O
even O
different O
splits O
, O
an O
additional O
source O
of O
complication O
comes O
from O
the O
fact O
that O
videos O
are O
regularly O
deleted O
by O
content O
creators O
, O
causing O
video O
datasets O
to O
shrink O
over O
time O
. O
Additionally O
, O
when O
comparing O
to O
other O
work O
incorporating O
pretraining O
, O
we O
could O
differ O
in O
( O
videos O
available O
in O
) O
pretraining O
datasets O
, O
segmentation O
strategies O
, O
etc O
. O
To O
this O
end O
, O
we O
perform O
an O
extensive O
ablation O
study O
, O
which O
at O
least O
helps O
us O
to O
understand O
the O
effectiveness O
of O
different O
components O
in O
our O
approach O
. O
Effect O
of O
pretraining O
The O
main O
experimental O
results O
for O
the O
three O
datasets O
we O
consider O
are O
summarized O
in O
Table O
3 O
( O
YouCook2 O
) O
and O
Table O
4 O
( O
ViTT O
- O
All O
and O
ViTT O
- O
Cooking O
) O
. O
Across O
all O
three O
datasets O
, O
the O
best O
performance O
is O
achieved O
by O
Ô¨Ånetuning O
a O
multimodal O
captioning O
model O
under O
the O
Multimodal O
Pretraining O
condition O
. O
For O
instance O
, O
on O
YouCook2 O
, O
E2vidD6 O
- O
MASSvid O
- O
BiD O
improves O
over O
the O
nopretraining O
model O
E2vidD6 O
- O
BiD O
by O
4.37 O
ROUGE O
-L O
, O
a O
larger O
improvement O
than O
UniViLM O
with O
pretraining O
( O
# O
5 O
) O
vs O
without O
( O
# O
2 O
) O
( O
Luo O
et O
al O
. O
, O
2020 O
) O
. O
This476Method O
Input O
Pretraining O
B O
LEU-4 O
M O
ETEOR O
ROUGE O
-L O
CIDE O
R O
Constant O
Pred O
( O
Hessel O
et O
al O
. O
, O
2019 O
) O
- O
- O
2.70 O
10.30 O
21.70 O
0.15 O
MART O
( O
Lei O
et O
al O
. O
, O
2020 O
) O
Video O
- O
8.00 O
15.90 O
- O
0.36 O
EMT O
( O
Zhou O
et O
al O
. O
, O
2018c O
) O
Video O
- O
4.38 O
11.55 O
27.44 O
0.38 O
CBT O
( O
Sun O
et O
al O
. O
, O
2019a O
) O
Video O
Kinetics O
+ O
HowTo100 O
M O
5.12 O
12.97 O
30.44 O
0.64 O
AT O
( O
Hessel O
et O
al O
. O
, O
2019 O
) O
ASR O
- O
8.55 O
16.93 O
35.54 O
1.06 O
AT+Video O
( O
Hessel O
et O
al O
. O
, O
2019 O
) O
Video O
+ O
ASR O
- O
9.01 O
17.77 O
36.65 O
1.12 O
UniViLM O
# O
1 O
( O
Luo O
et O
al O
. O
, O
2020 O
) O
Video O
- O
6.06 O
12.47 O
31.48 O
0.64 O
UniViLM O
# O
2 O
( O
Luo O
et O
al O
. O
, O
2020 O
) O
Video O
+ O
ASR O
- O
8.67 O
15.38 O
35.02 O
1.00 O
UniViLM O
# O
5 O
( O
Luo O
et O
al O
. O
, O
2020 O
) O
Video O
+ O
ASR O
HowTo100 O
M O
10.42 O
16.93 O
38.02 O
1.20 O
√ò O
Pretraining O
E2D6 O
- O
BiD O
ASR O
- O
7.90 O
15.70 O
34.86 O
0.93 O
E2vidD6 O
- O
BiD O
Video O
+ O
ASR O
- O
8.01 O
16.19 O
34.66 O
0.91 O
Text O
Pretraining O
E2D6 O
- O
MASS O
- O
BiD O
ASR O
YT8M O
- O
cook O
+ O
Recipe1 O
M O
10.60 O
17.42 O
38.08 O
1.20 O
E2vidD6 O
- O
MASS O
- O
BiD O
Video O
+ O
ASR O
YT8M O
- O
cook O
+ O
Recipe1 O
M O
11.47 O
17.70 O
38.80 O
1.25 O
Multimodal O
Pretraining O
E2vidD6 O
- O
MASSalign O
- O
BiD O
Video O
+ O
ASR O
YT8M O
- O
cook O
+ O
Recipe1 O
M O
11.53 O
17.62 O
39.03 O
1.22 O
E2vidD6 O
- O
MASSvid O
- O
BiD O
Video O
+ O
ASR O
YT8M O
- O
cook O
+ O
Recipe1 O
M O
12.04 O
18.32 O
39.03 O
1.23 O
E2vidD6 O
- O
MASSdrop O
- O
BiD O
Video O
+ O
ASR O
YT8M O
- O
cook O
+ O
Recipe1 O
M O
10.45 O
17.74 O
38.82 O
1.22 O
Human O
( O
Hessel O
et O
al O
. O
, O
2019 O
) O
Video O
+ O
ASR O
- O
15.20 O
25.90 O
45.10 O
3.80 O
Table O
3 O
: O
Segment O
- O
level O
captioning O
results O
on O
YouCook2 O
. O
We O
use O
YT8M O
- O
cook O
and O
Recipe1 O
M O
for O
pretraining O
. O
The O
numbers O
for O
the O
related O
work O
( O
Ô¨Årst O
group O
) O
are O
directly O
reported O
from O
the O
corresponding O
papers O
. O
The O
last O
line O
is O
an O
estimate O
of O
human O
performance O
as O
reported O
by O
Hessel O
et O
al O
. O
( O
2019 O
) O
, O
and O
can O
be O
taken O
as O
a O
rough O
upper O
bound O
of O
the O
best O
performance O
achievable O
. O
improvement O
also O
holds O
in O
ViTT O
- O
Cooking O
( O
+4.22 O
inROUGE O
-L O
) O
and O
ViTT O
- O
All O
( O
+2.97 O
in O
ROUGE O
-L O
) O
. O
We O
do O
not O
observe O
consistent O
and O
signiÔ¨Åcant O
trends O
among O
the O
different O
multimodal O
pretraining O
strategies O
: O
MASS O
pretraining O
with O
video O
( O
MASSvid O
) O
, O
with O
video O
and O
droptext O
( O
MASSdrop O
) O
, O
or O
with O
alignment O
tasks O
( O
MASSalign O
) O
.8Furthermore O
, O
we O
observe O
that O
most O
of O
the O
pretraining O
improvement O
is O
achievable O
via O
text O
- O
only O
MASS O
pretraining O
. O
Across O
all O
three O
datasets O
, O
while O
Multimodal O
Pretraining O
( O
E2vidD6 O
- O
MASSvid O
- O
BiD O
) O
is O
consistently O
better O
than O
Text O
Pretraining O
( O
E2vidD6 O
- O
MASS O
- O
BiD O
) O
, O
the O
differences O
are O
quite O
small O
( O
under O
one O
ROUGE O
-L O
point O
) O
. O
It O
‚Äôs O
worthy O
noting O
that O
for O
MASSalign O
, O
the O
best O
validation O
accuracies O
for O
the O
pretraining O
tasks O
are O
reasonably O
high O
: O
for O
YT8M O
- O
cook O
, O
we O
observed O
90 O
% O
accuracy O
for O
the O
alignment O
task O
, O
and O
80 O
% O
for O
the O
ordering O
task O
( O
for O
HowTo100 O
M O
: O
87 O
% O
and O
71.4 O
% O
, O
respectively O
) O
, O
where O
random O
guess O
would O
yield O
50 O
% O
. O
This O
suggests O
that O
our O
video O
features O
are O
reasonably O
strong O
, O
and O
the O
Ô¨Åndings O
above O
are O
not O
due O
to O
weak O
visual O
representations O
. O
8Limited O
improvement O
with O
MASSalign O
suggests O
that O
such O
alignment O
tasks O
are O
better O
suited O
for O
retrieval O
( O
Luo O
et O
al O
. O
, O
2020).Effect O
of O
other O
modeling O
choices O
We O
experiment O
with O
2 O
- O
layer O
decoder O
( O
D2 O
) O
vs O
6 O
- O
layer O
decoder O
( O
D6 O
) O
, O
combined O
with O
either O
unidirectional O
Ô¨Ånetuning O
( O
UniD O
) O
or O
bidirectional O
Ô¨Åne O
- O
tuning O
( O
BiD O
) O
. O
Table O
5 O
shows O
ablation O
results O
of O
the O
four O
possible O
combinations O
when O
Ô¨Ånetuning O
a O
multimodal O
model O
using O
text O
- O
only O
pretraining O
on O
YouCook2 O
( O
a O
more O
complete O
list O
of O
results O
can O
be O
found O
in O
Appendix O
A.5 O
, O
showing O
similar O
trends O
) O
. O
The O
D6xBiD O
combination O
tends O
to O
yield O
the O
best O
performance O
, O
with O
the O
differences O
among O
the O
four O
conÔ¨Ågurations O
being O
relatively O
small O
( O
under O
one O
ROUGE O
-L O
point O
) O
. O
For O
visual O
features O
, O
we O
also O
explored O
using O
3D O
features O
( O
Xie O
et O
al O
. O
, O
2018 O
) O
instead O
of O
2D O
features O
during O
Ô¨Ånetuning O
( O
with O
no O
pretraining O
or O
text O
- O
only O
pretraining O
) O
, O
and O
do O
not O
Ô¨Ånd O
much O
difference O
in O
model O
performance O
on O
YouCook2 O
. O
As O
a O
result O
, O
we O
use O
the O
simpler O
2D O
features O
in O
our O
multimodal O
pretraining O
. O
We O
leave O
more O
extensive O
experiments O
with O
visual O
features O
as O
future O
work O
. O
Generalization O
implications O
An O
important O
motivation O
for O
constructing O
the O
ViTT O
dataset O
and O
evaluating O
our O
models O
on O
it O
has O
been O
related O
to O
generalization O
. O
Since O
the O
YouCook2 O
benchmark O
is O
restricted O
to O
a O
small O
number O
of O
cooking O
recipes O
, O
there O
is O
little O
to O
be O
understood O
about O
how O
well O
models477Method O
InputViTT O
- O
All O
ViTT O
- O
Cooking O
BLEU-1 O
M O
ETEOR O
ROUGE O
-L O
CIDEr O
B O
LEU-1 O
M O
ETEOR O
ROUGE O
-L O
CIDEr O
Constant O
baseline O
( O
‚Äú O
intro O
‚Äù O
) O
- O
1.42 O
3.32 O
11.15 O
0.28 O
1.16 O
2.93 O
10.21 O
0.25 O
√ò O
Pretraining O
E2D6 O
- O
BiD O
ASR O
19.60 O
9.12 O
27.88 O
0.68 O
20.77 O
10.08 O
28.63 O
0.72 O
E2vidD6 O
- O
BiD O
Video O
+ O
ASR O
19.49 O
9.23 O
28.53 O
0.69 O
20.45 O
9.88 O
28.88 O
0.69 O
Text O
Pretraining O
E2D6 O
- O
MASS O
- O
BiD O
ASR O
21.93 O
10.60 O
30.45 O
0.79 O
24.79 O
12.25 O
32.40 O
0.88 O
E2vidD6 O
- O
MASS O
- O
BiD O
Video O
+ O
ASR O
22.44 O
10.83 O
31.27 O
0.81 O
24.22 O
12.22 O
32.60 O
0.89 O
Multimodal O
Pretraining O
E2vidD6 O
- O
MASSalign O
- O
BiD O
Video O
+ O
ASR O
22.31 O
10.66 O
31.13 O
0.79 O
24.92 O
12.25 O
33.09 O
0.90 O
E2vidD6 O
- O
MASSvid O
- O
BiD O
Video O
+ O
ASR O
22.45 O
10.76 O
31.49 O
0.80 O
24.87 O
12.43 O
32.97 O
0.90 O
E2vidD6 O
- O
MASSdrop O
- O
BiD O
Video O
+ O
ASR O
22.37 O
11.00 O
31.40 O
0.82 O
24.48 O
12.22 O
33.10 O
0.89 O
Human O
Video O
+ O
ASR O
43.34 O
33.56 O
41.88 O
1.26 O
41.61 O
32.50 O
41.59 O
1.21 O
Table O
4 O
: O
Segment O
- O
level O
captioning O
results O
on O
ViTT O
. O
For O
ViTT O
- O
All O
we O
pretrain O
on O
HowTo100 O
M O
and O
WikiHow O
; O
for O
ViTT O
- O
Cooking O
we O
pretrain O
on O
YT8M O
- O
cook O
and O
Recipe1M. O
We O
report O
baseline O
scores O
for O
predicting O
the O
most O
common O
caption O
‚Äú O
intro O
‚Äù O
. O
We O
also O
estimate O
the O
human O
performance O
as O
a O
rough O
upper O
bound O
( O
details O
in O
Supplementary O
Material O
A.1 O
; O
Table O
9 O
) O
. O
Method O
B O
LEU-4 O
M O
ETEOR O
ROUGE O
-L O
CIDEr O
D2 O
- O
UniD O
10.84 O
17.39 O
38.24 O
1.16 O
D6 O
- O
UniD O
11.39 O
18.00 O
38.71 O
1.22 O
D2 O
- O
BiD O
11.38 O
18.04 O
38.67 O
1.19 O
D6 O
- O
BiD O
11.47 O
17.70 O
38.80 O
1.25 O
D6 O
- O
BiDalt O
11.07 O
17.68 O
38.43 O
1.22 O
D6 O
- O
BiD O
( O
S3D O
) O
11.64 O
18.04 O
38.75 O
1.24 O
Table O
5 O
: O
Ablation O
study O
on O
YouCook2 O
. O
We O
Ô¨Ånetune O
a O
multimodal O
captioning O
model O
( O
E2vid O
) O
with O
either O
2 O
- O
layer O
decoder O
( O
D2 O
) O
or O
6 O
- O
layer O
decoder O
( O
D6 O
) O
using O
YT8M O
- O
cook O
/Recipe1 O
M O
for O
MASS O
pretraining O
, O
combined O
with O
either O
unidirectional O
( O
UniD O
) O
or O
bidirectional O
( O
BiD O
) O
Ô¨Ånetuning O
. O
We O
Ô¨Ånd O
no O
signiÔ¨Åcant O
difference O
between O
using O
2D O
and O
3D O
features O
( O
marked O
as O
S3D O
) O
. O
trained O
and O
evaluated O
on O
it O
generalize O
. O
In O
contrast O
, O
the O
ViTT O
benchmark O
has O
a O
much O
wider O
coverage O
( O
for O
both O
cooking O
- O
related O
videos O
and O
general O
instructional O
videos O
) O
, O
and O
no O
imposed O
topic O
overlap O
between O
train O
/ O
dev O
/ O
test O
. O
As O
such O
, O
there O
are O
two O
Ô¨Åndings O
here O
that O
are O
relevant O
with O
respect O
to O
generalization O
: O
( O
a O
) O
the O
absolute O
performance O
of O
the O
models O
on O
the O
ViTT O
benchmark O
is O
quite O
high O
( O
ROUGE O
- O
L O
scores O
above O
0.30 O
are O
usually O
indicative O
of O
decent O
performance O
) O
, O
and O
( O
b O
) O
the O
performance O
on O
ViTT O
vs. O
YouCook2 O
is O
clearly O
lower O
( O
31.5 O
ROUGE O
- O
L O
vs. O
39.0 O
ROUGE O
- O
L O
, O
reÔ¨Çecting O
the O
increased O
difÔ¨Åculty O
of O
the O
new O
benchmark O
) O
, O
but O
it O
is O
maximized O
under O
similar O
pretraining O
and O
Ô¨Ånetuning O
conditions O
, O
which O
allows O
us O
to O
claim O
that O
the O
resulting O
models O
generalize O
well O
and O
are O
quite O
robust O
over O
a O
wide O
variety O
of O
instructional O
videos.6 O
Conclusions O
Motivated O
to O
improve O
information O
- O
seeking O
capabilities O
for O
videos O
, O
we O
have O
collected O
and O
annotated O
a O
new O
dense O
video O
captioning O
dataset O
, O
ViTT O
, O
which O
is O
larger O
with O
higher O
diversity O
compared O
to O
YouCook2 O
. O
We O
investigated O
several O
multimodal O
pretraining O
strategies O
for O
segment O
- O
level O
video O
captioning O
, O
and O
conducted O
extensive O
ablation O
studies O
. O
We O
concluded O
that O
MASS O
- O
style O
pretraining O
is O
the O
most O
decisive O
factor O
in O
improving O
the O
performance O
on O
all O
the O
benchmarks O
used O
. O
Even O
more O
to O
the O
point O
, O
our O
results O
indicate O
that O
most O
of O
the O
performance O
can O
be O
attributed O
to O
leveraging O
the O
ASR O
signal O
. O
We O
achieve O
new O
state O
- O
of O
- O
the O
- O
art O
results O
on O
the O
YouCook2 O
benchmark O
, O
and O
establish O
strong O
performance O
baselines O
for O
the O
new O
ViTT O
benchmark O
, O
which O
can O
be O
used O
as O
starting O
points O
for O
driving O
more O
progress O
in O
this O
direction O
. O
Acknowledgements O
We O
send O
warm O
thanks O
to O
Ashish O
Thapliyal O
for O
helping O
the O
Ô¨Årst O
author O
debug O
his O
code O
and O
navigate O
the O
computing O
infrastructure O
, O
and O
to O
Sebastian O
Goodman O
for O
his O
technical O
help O
( O
and O
lightning O
fast O
responses O
! O
) O
. O
We O
also O
thank O
the O
anonymous O
reviewers O
for O
their O
comments O
and O
suggestions O
. O
Abstract O
Systematic O
Generalization O
refers O
to O
a O
learning O
algorithm O
‚Äôs O
ability O
to O
extrapolate O
learned O
behavior O
to O
unseen O
situations O
that O
are O
distinct O
but O
semantically O
similar O
to O
its O
training O
data O
. O
As O
shown O
in O
recent O
work O
, O
state O
- O
of O
- O
the O
- O
art O
deep O
learning O
models O
fail O
dramatically O
even O
on O
tasks O
for O
which O
they O
are O
designed O
when O
the O
test O
set O
is O
systematically O
different O
from O
the O
training O
data O
. O
We O
hypothesize O
that O
explicitly O
modeling O
the O
relations O
between O
objects O
in O
their O
contexts O
while O
learning O
their O
representations O
will O
help O
achieve O
systematic O
generalization O
. O
Therefore O
, O
we O
propose O
a O
novel O
method O
that O
learns O
objects O
‚Äô O
contextualized O
embedding O
with O
dynamic O
message O
passing O
conditioned O
on O
the O
input O
natural O
language O
and O
is O
end O
- O
to O
- O
end O
trainable O
with O
other O
downstream O
deep O
learning O
modules O
. O
To O
our O
knowledge O
, O
this O
model O
is O
the O
Ô¨Årst O
one O
that O
signiÔ¨Åcantly O
outperforms O
the O
provided O
baseline O
and O
reaches O
state O
- O
of O
- O
the O
- O
art O
performance O
ongrounded O
SCAN O
( O
gSCAN O
) O
, O
a O
grounded O
natural O
language O
navigation O
dataset O
designed O
to O
require O
systematic O
generalization O
in O
its O
test O
splits O
. O
1 O
Introduction O
Systematic O
Generalization O
refers O
to O
a O
learning O
algorithm O
‚Äôs O
ability O
to O
extrapolate O
learned O
behavior O
to O
unseen O
situations O
that O
are O
distinct O
but O
semantically O
similar O
to O
its O
training O
data O
. O
It O
has O
long O
been O
recognized O
as O
a O
key O
aspect O
of O
humans O
‚Äô O
cognitive O
capacities O
( O
Fodor O
et O
al O
. O
, O
1988 O
) O
. O
SpeciÔ¨Åcally O
, O
humans O
‚Äô O
mastery O
of O
systematic O
generalization O
is O
prevalent O
in O
grounded O
natural O
language O
understanding O
. O
For O
example O
, O
humans O
can O
reason O
about O
the O
relations O
between O
all O
pairs O
of O
concepts O
from O
two O
domains O
, O
even O
if O
they O
have O
only O
seen O
a O
small O
subset O
of O
pairs O
during O
training O
. O
If O
a O
child O
observes O
‚Äù O
red O
squares O
‚Äù O
, O
‚Äù O
green O
squares O
‚Äù O
and O
‚Äù O
yellow O
circles O
‚Äù O
, O
he O
or O
she O
can O
‚àó O
( O
* O
) O
denotes O
co-Ô¨Årst O
authorship O
, O
authors O
contribute O
equally O
and O
are O
listed O
in O
alphabetical O
order.recognize O
‚Äù O
red O
circles O
‚Äù O
at O
their O
Ô¨Årst O
encounter O
. O
Humans O
can O
also O
contextualize O
their O
reasoning O
about O
objects O
‚Äô O
attributes O
. O
For O
example O
, O
a O
city O
being O
referred O
to O
as O
‚Äù O
the O
larger O
one O
‚Äù O
within O
a O
state O
might O
be O
referred O
to O
as O
‚Äù O
the O
smaller O
one O
‚Äù O
nationwide O
. O
In O
the O
past O
decade O
, O
deep O
neural O
networks O
have O
shown O
tremendous O
success O
on O
a O
collection O
of O
grounded O
natural O
language O
processing O
tasks O
, O
such O
as O
visual O
question O
answering O
( O
VQA O
) O
, O
image O
captioning O
, O
and O
vision O
- O
and O
- O
language O
navigation O
( O
Hudson O
and O
Manning O
, O
2018 O
; O
Anderson O
et O
al O
. O
, O
2018a O
, O
b O
) O
. O
Despite O
all O
the O
success O
, O
recent O
literature O
shows O
that O
current O
deep O
learning O
approaches O
are O
exploiting O
statistical O
patterns O
discovered O
in O
the O
datasets O
to O
achieve O
high O
performance O
, O
an O
approach O
that O
does O
not O
achieve O
systematic O
generalization O
. O
Gururangan O
et O
al O
. O
( O
2018 O
) O
discovered O
that O
annotation O
artifacts O
like O
negation O
words O
or O
purpose O
clauses O
in O
natural O
language O
inference O
data O
can O
be O
used O
by O
simple O
text O
classiÔ¨Åcation O
categorization O
model O
to O
solve O
the O
given O
task O
. O
Jia O
and O
Liang O
( O
2017 O
) O
demonstrated O
that O
adversarial O
examples O
can O
fool O
reading O
comprehension O
systems O
. O
Indeed O
, O
deep O
learning O
models O
often O
fail O
to O
achieve O
systematic O
generalizations O
even O
on O
tasks O
on O
which O
they O
are O
claimed O
to O
perform O
well O
. O
As O
shown O
by O
Bahdanau O
et O
al O
. O
( O
2018 O
) O
, O
state O
- O
of O
- O
the O
- O
art O
Visual O
Questioning O
Answering O
( O
VQA O
) O
( O
Hudson O
and O
Manning O
, O
2018 O
; O
Perez O
et O
al O
. O
, O
2018 O
) O
models O
fail O
dramatically O
even O
on O
a O
synthetic O
VQA O
dataset O
designed O
with O
systematic O
difference O
between O
training O
and O
test O
sets O
. O
In O
this O
work O
, O
we O
focus O
on O
approaching O
systematic O
generalization O
in O
grounded O
natural O
language O
understanding O
tasks O
. O
We O
experiment O
with O
a O
recently O
introduced O
synthetic O
dataset O
, O
grounded O
SCAN O
( O
gSCAN O
) O
, O
that O
requires O
systematic O
generalization O
to O
solve O
( O
Ruis O
et O
al O
. O
, O
2020 O
) O
. O
For O
example O
, O
after O
observing O
how O
to O
‚Äù O
walk O
hesitantly O
‚Äù O
to O
a O
target O
object O
in O
a O
grid O
world O
, O
the O
learning O
agent O
is O
tested O
with O
instruction O
that O
requires O
it O
to O
‚Äù O
pull O
hesitantly O
‚Äù O
, O
therefore O
testing O
its O
ability O
to O
generalize O
adverbs O
to491unseen O
adverb O
- O
verb O
combinations O
. O
When O
presented O
with O
a O
world O
of O
objects O
with O
different O
attributes O
, O
and O
natural O
language O
sentences O
that O
describe O
such O
objects O
, O
the O
goal O
of O
the O
model O
is O
to O
generalize O
its O
ability O
to O
understand O
unseen O
sentences O
describing O
novel O
combinations O
of O
observed O
objects O
, O
or O
even O
novel O
objects O
with O
observed O
attributes O
. O
One O
of O
the O
essential O
steps O
in O
achieving O
this O
goal O
is O
to O
obtain O
good O
object O
embeddings O
to O
which O
natural O
language O
can O
be O
grounded O
. O
By O
considering O
each O
object O
as O
a O
bag O
of O
its O
descriptive O
attributes O
, O
this O
problem O
is O
further O
transformed O
into O
learning O
good O
representations O
for O
those O
attributes O
based O
on O
the O
training O
data O
. O
This O
requires O
: O
1 O
) O
learning O
good O
representations O
of O
attributes O
whose O
actual O
meanings O
are O
contextualized O
, O
for O
example O
, O
‚Äù O
smaller O
‚Äù O
and O
‚Äù O
lighter O
‚Äù O
, O
etc O
. O
; O
2 O
) O
learning O
good O
representations O
for O
attributes O
so O
that O
conceptually O
similar O
attributes O
, O
e.g. O
, O
‚Äù O
yellow O
‚Äù O
and O
‚Äù O
red O
‚Äù O
, O
have O
similar O
representations O
. O
We O
hypothesize O
that O
explicitly O
modeling O
the O
relations O
between O
objects O
in O
their O
contexts O
, O
i.e. O
, O
learning O
contextualized O
object O
embeddings O
, O
will O
help O
achieve O
systematic O
generalization O
. O
This O
is O
intuitively O
helpful O
for O
learning O
concepts O
with O
contextualized O
meaning O
, O
just O
as O
learning O
to O
recognize O
the O
‚Äù O
smaller O
‚Äù O
object O
in O
a O
novel O
pair O
requires O
experience O
of O
comparison O
between O
semantically O
similar O
object O
pairs O
. O
Learning O
contextualized O
object O
embeddings O
can O
also O
be O
helpful O
for O
obtaining O
good O
representations O
for O
semantically O
similar O
concepts O
when O
such O
concepts O
are O
the O
only O
difference O
between O
two O
contexts O
. O
Inspired O
by O
Hu O
et O
al O
. O
( O
2019 O
) O
, O
we O
propose O
a O
novel O
method O
that O
learns O
an O
object O
‚Äôs O
contextualized O
embedding O
with O
dynamic O
message O
passing O
conditioned O
on O
the O
input O
natural O
language O
. O
At O
each O
round O
of O
message O
passing O
, O
our O
model O
collects O
relational O
information O
between O
each O
object O
pair O
, O
and O
constructs O
an O
object O
‚Äôs O
contextualized O
embedding O
as O
a O
weighted O
combination O
of O
them O
. O
Such O
weights O
are O
dynamically O
computed O
conditioned O
on O
the O
input O
natural O
sentence O
. O
This O
contextualized O
object O
embedding O
scheme O
is O
trained O
end O
- O
to O
- O
end O
with O
downstream O
deep O
modules O
for O
speciÔ¨Åc O
grounded O
natural O
language O
processing O
tasks O
, O
such O
as O
navigation O
. O
Experiments O
show O
that O
our O
approach O
signiÔ¨Åcantly O
outperforms O
a O
strong O
baseline O
on O
gSCAN O
. O
2 O
Related O
Work O
Research O
on O
deep O
learning O
models O
‚Äô O
systematic O
generalization O
behavior O
has O
gained O
traction O
in O
recent O
years O
, O
with O
particular O
focus O
on O
natural O
languageprocessing O
tasks O
. O
2.1 O
Compositionality O
An O
idea O
that O
is O
closely O
related O
to O
systematic O
generalization O
is O
compositionality O
. O
Kamp O
and O
Partee O
( O
1995 O
) O
deÔ¨Åne O
the O
principle O
of O
compositionality O
as O
‚Äú O
The O
meaning O
of O
a O
whole O
is O
a O
function O
of O
the O
meanings O
of O
the O
parts O
and O
of O
the O
way O
they O
are O
syntactically O
combined O
‚Äù O
. O
Hupkes O
et O
al O
. O
( O
2020 O
) O
synthesizes O
different O
interpretations O
of O
this O
abstract O
principle O
into O
5 O
theoretically O
grounded O
tests O
to O
evaluate O
a O
model O
‚Äôs O
ability O
to O
represent O
compositionality O
: O
1 O
) O
Systematicity O
: O
if O
the O
model O
can O
systematically O
recombine O
known O
parts O
and O
rules O
; O
2 O
) O
Productivity O
: O
if O
the O
model O
can O
extend O
their O
predictions O
beyond O
what O
they O
have O
seen O
in O
the O
training O
data O
; O
3 O
) O
Substitutivity O
; O
if O
the O
model O
is O
robust O
to O
synonym O
substitutions O
; O
4 O
) O
Localism O
: O
if O
the O
model O
‚Äôs O
composition O
operations O
are O
local O
or O
global O
; O
and O
5 O
) O
Overgeneralisation O
: O
if O
the O
model O
favors O
rules O
or O
exceptions O
during O
training O
. O
The O
gSCAN O
dataset O
focuses O
more O
on O
capturing O
the O
Ô¨Årst O
three O
tests O
in O
a O
grounded O
natural O
language O
understanding O
setting O
, O
and O
our O
proposed O
model O
achieves O
signiÔ¨Åcant O
performance O
improvement O
on O
test O
sets O
relating O
to O
systematicity O
and O
substitutivity O
. O
2.2 O
Systematic O
Generalization O
Datasets O
Many O
systematic O
generalization O
datasets O
have O
been O
proposed O
in O
recent O
years O
( O
Bahdanau O
et O
al O
. O
, O
2018 O
; O
Chevalier O
- O
Boisvert O
et O
al O
. O
, O
2019 O
; O
Hill O
et O
al O
. O
, O
2020 O
; O
Lake O
and O
Baroni O
, O
2017 O
; O
Ruis O
et O
al O
. O
, O
2020 O
) O
. O
This O
paper O
is O
conceptually O
most O
related O
to O
the O
SQOOP O
dataset O
proposed O
by O
Bahdanau O
et O
al O
. O
( O
2018 O
) O
, O
the O
SCAN O
dataset O
proposed O
by O
Lake O
and O
Baroni O
( O
2017 O
) O
, O
and O
the O
gSCAN O
dataset O
proposed O
by O
Ruis O
et O
al O
. O
( O
2020 O
) O
. O
The O
SQOOP O
dataset O
consists O
of O
a O
random O
number O
of O
MNIST O
- O
style O
alphanumeric O
characters O
scattered O
in O
an O
image O
with O
speciÔ¨Åc O
spatial O
relations O
( O
‚Äù O
left O
‚Äù O
, O
‚Äù O
right O
‚Äù O
, O
‚Äù O
up O
‚Äù O
, O
‚Äù O
down O
‚Äù O
) O
among O
them O
( O
Bahdanau O
et O
al O
. O
, O
2018 O
) O
. O
The O
algorithm O
is O
tested O
with O
a O
binary O
decision O
task O
of O
reasoning O
about O
whether O
a O
speciÔ¨Åc O
relation O
holds O
between O
a O
pair O
of O
alphanumeric O
characters O
. O
Systematic O
difference O
is O
created O
between O
the O
testing O
and O
training O
set O
by O
only O
providing O
supervision O
on O
relations O
for O
a O
subset O
of O
digit O
pairs O
to O
the O
learner O
, O
while O
testing O
its O
ability O
to O
reason O
about O
relations O
between O
unseen O
alphanumeric O
character O
pairs O
. O
For O
example O
, O
the O
algorithm O
is O
tested O
with O
questions O
like O
‚Äú O
is O
S O
above O
T O
‚Äù O
while O
it O
never O
sees O
a O
relation O
involving O
both O
S O
and O
T O
during O
train-492ing O
. O
Therefore O
, O
to O
fully O
solve O
this O
dataset O
, O
it O
must O
learn O
to O
generalize O
its O
understanding O
of O
the O
relation O
‚Äú O
above O
‚Äù O
to O
unseen O
pairs O
of O
characters O
. O
Lake O
and O
Baroni O
( O
2017 O
) O
proposed O
the O
SCAN O
dataset O
and O
its O
related O
benchmark O
that O
tests O
a O
learning O
algorithm O
‚Äôs O
ability O
to O
perform O
compositional O
learning O
and O
zeroshot O
generalization O
on O
a O
natural O
language O
command O
translation O
task O
. O
Given O
a O
natural O
language O
command O
with O
a O
limited O
vocabulary O
, O
an O
algorithm O
needs O
to O
translate O
it O
into O
a O
corresponding O
action O
sequence O
consisting O
of O
action O
tokens O
from O
a O
Ô¨Ånite O
token O
set O
. O
Compared O
to O
SQOOP O
, O
SCAN O
tests O
the O
algorithm O
‚Äôs O
ability O
to O
learn O
more O
complicated O
linguistic O
generalizations O
like O
‚Äù O
walk O
around O
left O
‚Äù O
to O
‚Äù O
walk O
around O
right O
‚Äù O
. O
SCAN O
also O
ensures O
that O
the O
target O
action O
sequence O
is O
unique O
, O
and O
an O
oracle O
solution O
exists O
by O
providing O
an O
interpreter O
function O
that O
can O
unambiguously O
translate O
any O
given O
command O
to O
its O
target O
action O
sequence O
. O
Going O
beyond O
SCAN O
that O
focuses O
purely O
on O
syntactic O
aspects O
of O
systematic O
generalization O
, O
the O
gSCAN O
dataset O
proposed O
by O
Ruis O
et O
al O
. O
( O
2020 O
) O
is O
an O
extension O
of O
SCAN O
. O
It O
contains O
a O
series O
of O
systematic O
generalization O
tasks O
that O
require O
the O
learning O
agent O
to O
ground O
its O
understanding O
of O
natural O
language O
commands O
in O
a O
given O
grid O
world O
to O
produce O
the O
correct O
action O
token O
sequence O
. O
We O
choose O
gSCAN O
as O
our O
benchmark O
dataset O
, O
as O
its O
input O
command O
sentences O
are O
linguistically O
more O
complex O
, O
and O
requires O
processing O
multi O
- O
modal O
input O
. O
2.3 O
Systematic O
Generliazation O
Algorithms O
Bahdanau O
et O
al O
. O
( O
2018 O
) O
demonstrated O
that O
modular O
networks O
, O
with O
a O
carefully O
chosen O
module O
layout O
, O
can O
achieve O
nearly O
perfect O
systematic O
generalization O
on O
SQOOP O
dataset O
. O
Our O
approach O
can O
be O
considered O
as O
a O
conceptual O
generalization O
of O
theirs O
. O
Each O
object O
‚Äôs O
initial O
embedding O
can O
be O
considered O
as O
a O
simple O
afÔ¨Åne O
encoder O
module O
, O
and O
we O
learn O
the O
connection O
scheme O
among O
these O
modules O
conditioned O
on O
natural O
language O
instead O
of O
handdesigning O
it O
. O
Gordon O
et O
al O
. O
( O
2019 O
) O
proposed O
solving O
the O
SCAN O
benchmark O
by O
hard O
- O
coding O
their O
model O
to O
be O
equivariant O
to O
all O
permutations O
of O
SCAN O
‚Äôs O
verb O
primitives O
. O
Andreas O
( O
2020 O
) O
proposed O
GECA O
( O
‚Äú O
Good O
- O
Enough O
Compositional O
Augmentation O
‚Äù O
) O
that O
systematically O
augments O
the O
SCAN O
dataset O
by O
identifying O
sentence O
fragments O
with O
similar O
syntactic O
context O
, O
and O
permuting O
them O
to O
generate O
novel O
training O
examples O
. O
This O
line O
of O
permutationinvariant O
approaches O
is O
shown O
to O
not O
generalizewell O
on O
the O
gSCAN O
dataset O
( O
Ruis O
et O
al O
. O
, O
2020 O
) O
. O
At O
the O
time O
of O
submission O
, O
our O
method O
was O
the O
Ô¨Årst O
to O
outperform O
the O
strong O
baseline O
provided O
in O
the O
gSCAN O
benchmark O
, O
and O
also O
the O
Ô¨Årst O
one O
to O
apply O
language O
- O
conditioned O
message O
passing O
to O
learn O
contextualized O
input O
embedding O
for O
systematic O
generalization O
tasks O
. O
Concurrent O
to O
our O
work O
, O
Kuo O
et O
al O
. O
( O
2020 O
) O
proposed O
a O
family O
of O
parse O
- O
tree O
- O
based O
compositional O
RNN O
networks O
to O
enable O
systematic O
generalization O
, O
and O
heavily O
relies O
on O
off O
- O
the O
- O
shelf O
parsers O
to O
produce O
the O
network O
hierarchy O
. O
HeinzeDeml O
and O
Bouchacourt O
( O
2020 O
) O
use O
an O
attentionbased O
prediction O
of O
the O
target O
object O
‚Äôs O
location O
as O
an O
auxilary O
training O
task O
to O
regularize O
the O
model O
. O
However O
, O
it O
only O
improves O
over O
the O
baseline O
model O
in O
Ruis O
et O
al O
. O
( O
2020 O
) O
in O
a O
limited O
subset O
of O
test O
splits O
. O
For O
completeness O
, O
we O
also O
compare O
our O
model O
‚Äôs O
result O
with O
the O
above O
two O
concurrent O
works O
. O
3 O
Problem O
DeÔ¨Ånition O
& O
Algorithm O
3.1 O
Task O
DeÔ¨Ånition O
gSCAN O
contains O
a O
series O
of O
systematic O
generalization O
tasks O
in O
a O
grounded O
natural O
language O
understanding O
setting O
. O
In O
gSCAN O
, O
the O
learning O
agent O
is O
tested O
with O
the O
task O
of O
following O
a O
given O
natural O
language O
instruction O
to O
navigate O
in O
a O
twodimensional O
grid O
world O
with O
objects O
. O
This O
is O
achieved O
in O
the O
form O
of O
generating O
a O
sequence O
of O
action O
tokens O
from O
a O
Ô¨Ånite O
action O
token O
set O
A={walk O
, O
push O
, O
pull O
, O
stay O
, O
L O
turn O
, O
Rturn}that O
brings O
the O
agent O
from O
its O
starting O
location O
to O
the O
target O
location O
. O
An O
object O
in O
gSCAN O
‚Äôs O
world O
state O
is O
encoded O
with O
a O
one O
- O
hot O
encoding O
describing O
its O
attributes O
in O
three O
property O
types O
: O
1 O
) O
colorC={red O
, O
green O
, O
blue O
, O
yellow O
} O
2 O
) O
shape O
S={circle O
, O
square O
, O
cylinder O
} O
3 O
) O
sizeD= O
{ O
1,2,3,4 O
} O
. O
The O
agent O
is O
also O
encoded O
as O
an O
‚Äú O
object O
‚Äù O
in O
the O
grid O
world O
, O
with O
properties O
including O
orientationO={left O
, O
right O
, O
up O
, O
down O
} O
and O
a O
binary O
variableB={yes O
, O
no}denoting O
the O
presence O
of O
the O
agent O
. O
Therefore O
, O
the O
whole O
grid O
is O
represented O
as O
a O
tensor O
xS‚ààRd√ód√óc O
, O
wheredis O
the O
dimension O
of O
the O
grid O
, O
and O
c=|C|+|S|+|D|+ O
|O|+|B| O
. O
Mathematically O
, O
given O
an O
input O
tuple O
x= O
( O
xc O
, O
xS O
) O
, O
wherexc={xc O
1,xc O
2, O
... O
,xc O
n}represents O
the O
navigation O
instruction O
, O
the O
agent O
needs O
to O
predict O
the O
correct O
output O
action O
token O
sequence O
y={y1,y2, O
... O
,y O
m O
} O
. O
Despite O
its O
simple O
form O
, O
this O
task O
is O
quite O
challenging O
. O
For O
one O
, O
generating O
the O
correct O
action O
token O
sequence O
requires O
understanding O
the O
instruction O
within O
the O
context O
of O
the O
agent‚Äôs493current O
grid O
world O
. O
It O
also O
involves O
connecting O
speciÔ¨Åc O
instructions O
with O
complex O
dynamic O
patterns O
. O
For O
example O
, O
‚Äú O
pulling O
‚Äù O
a O
square O
will O
be O
mapped O
to O
a O
‚Äú O
pull O
‚Äù O
command O
when O
the O
square O
has O
a O
size O
of O
1 O
or O
2 O
, O
but O
to O
‚Äú O
pull O
pull O
‚Äù O
when O
the O
square O
has O
a O
size O
of O
3 O
or O
4 O
( O
a O
‚Äú O
heavy O
‚Äù O
square O
) O
; O
‚Äú O
move O
cautiously O
‚Äù O
requires O
the O
agent O
to O
turn O
left O
then O
turn O
right O
before O
making O
the O
actual O
move O
. O
gSCAN O
also O
introduces O
a O
series O
of O
test O
sets O
that O
have O
systematic O
differences O
from O
the O
training O
set O
. O
Computing O
the O
correct O
action O
token O
sequences O
on O
these O
test O
sets O
requires O
the O
model O
to O
learn O
to O
combine O
seen O
concepts O
into O
novel O
combinations O
, O
including O
novel O
object O
property O
combinations O
, O
novel O
contextual O
references O
, O
etc O
.. O
3.2 O
Model O
Architecture O
The O
overview O
of O
our O
model O
architecture O
is O
shown O
in O
Figure O
1 O
. O
At O
the O
highest O
level O
, O
it O
follows O
the O
same O
encoder O
- O
decoder O
framework O
used O
by O
the O
baseline O
model O
in O
Ruis O
et O
al O
. O
( O
2020 O
) O
to O
extract O
information O
from O
the O
input O
sentence O
/ O
grid O
- O
world O
representation O
and O
to O
output O
navigation O
instructions O
. O
However O
, O
there O
is O
a O
paradigm O
shift O
in O
how O
we O
represent O
and O
encode O
the O
grid O
world O
. O
Instead O
of O
viewing O
the O
grid O
world O
as O
a O
whole O
, O
we O
treat O
it O
as O
a O
collection O
of O
objects O
whose O
semantic O
meanings O
should O
be O
contextualized O
by O
their O
relations O
with O
one O
another O
. O
We O
also O
hypothesize O
that O
inter O
- O
object O
relations O
that O
are O
salient O
in O
a O
given O
grid O
world O
can O
be O
inferred O
from O
the O
accompanied O
natural O
language O
instruction O
. O
Therefore O
, O
we O
expand O
the O
vanilla O
CNN O
- O
based O
grid O
world O
encoder O
with O
a O
message O
passing O
module O
guided O
by O
the O
accompanied O
natural O
language O
instruction O
to O
obtain O
the O
contextualized O
grid O
- O
world O
embedding O
. O
3.2.1 O
Input O
Extraction O
Given O
the O
input O
sentence O
and O
the O
grid O
world O
state O
, O
we O
Ô¨Årst O
project O
them O
into O
higher O
dimensional O
embedding O
. O
For O
the O
input O
instruction O
I= O
{ O
w1,w2, O
... O
,w O
S}wherewiis O
the O
embedding O
vector O
of O
wordi O
, O
following O
the O
practice O
of O
Ruis O
et O
al O
. O
( O
2020 O
) O
and O
Hu O
et O
al O
. O
( O
2019 O
) O
, O
we O
Ô¨Årst O
encode O
it O
as O
the O
hidden O
states{hs}S O
s=1and O
the O
summary O
vector O
sobtained O
by O
feeding O
the O
input O
Ito O
a O
Bi O
- O
LSTM O
as O
: O
[ O
h1,h2, O
... O
,h O
S O
] O
= O
BiLSTM O
( O
I)ands= O
[ O
h1;hS](1 O
) O
Where O
we O
use O
semi O
- O
colon O
to O
represent O
concatenation O
, O
andhi= O
[ O
‚àí O
‚Üíhi;‚Üê O
‚àíhi]is O
the O
concatenation O
of O
the O
forward O
and O
backward O
direction O
of O
the O
LSTM O
hidden O
state O
for O
input O
word O
i. O
For O
each O
round O
of O
mes O
- O
sage O
passing O
between O
the O
objects O
embedding O
, O
we O
further O
apply O
a O
transformation O
using O
a O
multi O
- O
step O
textual O
attention O
module O
similar O
to O
that O
of O
Hudson O
and O
Manning O
( O
2018 O
) O
and O
Hu O
et O
al O
. O
( O
2018 O
) O
to O
extract O
the O
round O
- O
speciÔ¨Åc O
textual O
context O
. O
Given O
a O
round O
- O
speciÔ¨Åc O
projection O
matrix O
Wt O
2 O
, O
the O
textual O
attention O
score O
for O
word O
iat O
message O
passing O
round O
tis O
computed O
as O
: O
Œ±t O
, O
i O
= O
softmax O
s(W1(hi‚äô(Wt O
2ReLU O
( O
W3s O
) O
) O
) O
) O
( O
2 O
) O
The O
Ô¨Ånal O
textual O
context O
embedding O
for O
message O
passing O
round O
tis O
computed O
as O
: O
ct O
= O
S O
/ O
summationdisplay O
i=1Œ±t O
, O
i¬∑hi O
( O
3 O
) O
Details O
of O
the O
message O
passing O
mechanism O
will O
be O
described O
in O
later O
sections O
. O
As O
for O
the O
grid O
- O
world O
representation O
, O
from O
each O
grid O
, O
we O
extract O
one O
- O
hot O
representations O
of O
color O
C O
, O
shapeS O
, O
sizeDand O
agent O
orientation O
O O
, O
and O
embed O
each O
property O
with O
a O
16 O
- O
dimensional O
vector O
. O
We O
Ô¨Ånally O
concatenate O
them O
back O
into O
one O
vector O
and O
use O
this O
vector O
as O
the O
object O
‚Äôs O
local O
embedding O
. O
3.2.2 O
Language O
- O
conditioned O
Message O
Passing O
After O
extracting O
a O
textual O
context O
embedding O
and O
the O
objects O
‚Äô O
local O
embedding O
, O
we O
perform O
a O
language O
- O
conditioned O
iterative O
message O
passing O
forTrounds O
to O
obtain O
the O
contextualized O
object O
embeddings O
, O
where O
Tis O
a O
hyper O
- O
parameter O
. O
1 O
) O
Denoting O
the O
extracted O
object O
local O
embedding O
asxloc O
, O
and O
previous O
round O
‚Äôs O
object O
context O
embedding O
as O
xctx O
, O
we O
Ô¨Årst O
construct O
a O
fused O
representation O
of O
an O
object O
iat O
roundtby O
concatenating O
its O
local O
, O
context O
embedding O
as O
well O
their O
elementwise O
product O
: O
xfuse O
i O
, O
t= O
[ O
xloc O
i O
, O
xctx O
i O
, O
t‚àí1,(W4xloc O
i)‚äô(W5xctx O
i O
, O
t‚àí1 O
) O
] O
( O
4 O
) O
We O
use O
an O
object O
‚Äôs O
local O
embedding O
to O
initialize O
its O
context O
embedding O
at O
round O
0 O
. O
2 O
) O
For O
each O
pair O
of O
objects O
( O
i O
, O
j O
) O
, O
we O
use O
their O
fused O
representations O
, O
together O
with O
this O
round O
‚Äôs O
textual O
context O
embedding O
to O
compute O
their O
message O
passing O
weight O
as O
: O
wt O
i O
, O
j O
= O
softmax O
s(W6xfuse O
j O
, O
t)T((W7xfuse O
i O
, O
t)‚äô(W8ct))(5 O
) O
Note O
that O
the O
computation O
of O
the O
raw O
weight O
logits O
is O
asymmetric.494Figure O
1 O
: O
Model O
Overview O
3 O
) O
We O
consider O
all O
the O
objects O
in O
a O
grid O
world O
as O
nodes O
, O
and O
they O
together O
form O
a O
complete O
graph O
. O
Each O
nodeicomputes O
its O
message O
to O
receiver O
node O
jas O
: O
mt O
i O
, O
j O
= O
wt O
i O
, O
j¬∑((W9xfuse O
i O
, O
t‚äô(W10ct O
) O
) O
( O
6 O
) O
and O
each O
receiver O
node O
jupdates O
its O
context O
embedding O
as O
: O
xctx O
j O
, O
t O
= O
W11[xctx O
j O
, O
t‚àí1;N O
/ O
summationdisplay O
i=1mt O
i O
, O
j O
] O
( O
7 O
) O
AfterTrounds O
of O
iterative O
message O
passing O
, O
the O
Ô¨Ånal O
contextualized O
embedding O
for O
object O
iwill O
be O
: O
xout O
i O
= O
W12[xloc O
i;xctx O
i O
, O
T O
] O
( O
8) O
3.2.3 O
Encoding O
the O
Grid O
World O
After O
obtaining O
contextualized O
embeddings O
for O
all O
objects O
in O
a O
grid O
world O
xsas{xout}n= O
{ O
xout O
1,xout O
2, O
... O
,xout O
n}each O
of O
dimensionality O
Rout O
, O
we O
map O
them O
back O
to O
their O
locations O
in O
the O
grid O
world O
, O
and O
construct O
a O
new O
grid O
world O
representationXs O
/ O
prime‚ààRd√ód√óoutby O
zero O
- O
padding O
cells O
without O
any O
object O
. O
This O
is O
then O
fed O
into O
three O
parallel O
single O
convolutional O
layers O
with O
different O
kernel O
sizes O
to O
obtain O
a O
grid O
world O
‚Äôs O
embedding O
at O
multiple O
scales O
, O
as O
done O
by O
Wang O
and O
Lake O
( O
2019 O
) O
. O
The O
Ô¨Ånal O
grid O
world O
encoding O
is O
as O
follows O
: O
Hs= O
[ O
Hs O
1;Hs O
2;Hs O
3 O
] O
, O
Hs O
i O
= O
Conv O
i(Xs O
/ O
prime O
) O
( O
9)whereConv O
idenotes O
the O
ith O
convolutional O
network O
, O
andHs‚ààRd2√óhid O
. O
3.2.4 O
Decoding O
Action O
Sequences O
We O
use O
a O
Bi O
- O
LSTM O
with O
multi O
- O
modal O
attention O
to O
both O
the O
grid O
world O
embedding O
and O
the O
input O
instruction O
embedding O
to O
decode O
the O
Ô¨Ånal O
action O
sequence O
, O
following O
the O
baseline O
model O
provided O
by O
Ruis O
et O
al O
. O
( O
2020 O
) O
. O
At O
each O
step O
i O
, O
the O
hidden O
state O
of O
the O
decoder O
hd O
iis O
computed O
as O
: O
hd O
i O
= O
LSTM O
( O
[ O
ed O
i;cc O
i;cs O
i],hd O
i‚àí1 O
) O
( O
10 O
) O
whereed O
iis O
the O
embedding O
of O
the O
previous O
output O
action O
token O
yi‚àí1,cc O
iis O
the O
instruction O
context O
computed O
with O
attention O
over O
textual O
encoder O
‚Äôs O
hidden O
states O
[ O
hc O
1,hc O
2, O
... O
,hc O
S O
] O
, O
andcs O
iis O
the O
grid O
world O
context O
computed O
with O
attention O
over O
all O
locations O
in O
the O
grid O
world O
embedding O
HS O
. O
We O
set O
the O
decoder O
‚Äôs O
hidden O
size O
to O
64 O
so O
that O
it O
aligns O
with O
the O
textual O
encoder O
, O
and O
use O
the O
attention O
implementation O
proposed O
by O
Bahdanau O
et O
al O
. O
( O
2016 O
) O
. O
The O
instruction O
context O
is O
computed O
as O
: O
ec O
ij O
= O
vT O
ctanhW O
c(hd O
i‚àí1+hc O
j O
) O
( O
11 O
) O
Œ±c O
ij O
= O
exp(ec O
ij O
) O
/summationtextS O
j=1exp(ec O
ij)(12 O
) O
cc O
i O
= O
S O
/ O
summationdisplay O
j=1Œ±c O
ijhc O
j,‚àÄj‚àà{1,2, O
... O
,S O
} O
( O
13)495Similarly O
, O
the O
grid O
world O
context O
is O
computed O
as O
: O
es O
ij O
= O
vT O
stanhW O
s(hd O
i‚àí1+cc O
i O
) O
( O
14 O
) O
Œ±s O
ij O
= O
exp(es O
ij O
) O
/summationtextd2 O
j=1exp(es O
ij)(15 O
) O
cs O
i O
= O
d2 O
/ O
summationdisplay O
j=1Œ±s O
ijhs O
j,‚àÄj‚àà{1,2, O
... O
,d2 O
} O
( O
16 O
) O
wherevs O
, O
vc O
, O
Wc O
, O
Wsare O
learnable O
parameters O
, O
and O
hs O
jis O
the O
embedding O
of O
grid O
jobtained O
from O
HS O
. O
The O
distribution O
of O
next O
action O
token O
can O
then O
be O
computed O
as O
p(yi|x O
, O
y O
1,y2, O
... O
,y O
i‚àí1 O
) O
= O
softmax O
( O
Wohd O
i O
) O
. O
4 O
Experimental O
Evaluation O
4.1 O
Methodology O
& O
Implementation O
We O
run O
experiments O
to O
test O
the O
hypothesis O
that O
contextualized O
embeddings O
help O
systematic O
generalization1 O
. O
Since O
this O
task O
has O
a O
limited O
vocabulary O
size O
, O
word O
- O
level O
accuracy O
is O
no O
longer O
a O
proper O
metric O
to O
reÔ¨Çect O
the O
model O
‚Äôs O
performance O
. O
We O
follow O
the O
baseline O
and O
use O
the O
exact O
match O
percentage O
as O
our O
metric O
, O
where O
an O
exact O
match O
means O
that O
the O
produced O
action O
token O
sequence O
is O
exactly O
the O
same O
as O
the O
target O
sequence O
. O
We O
compare O
our O
model O
with O
the O
baseline O
on O
different O
test O
sets O
, O
and O
use O
early O
stopping O
based O
on O
the O
exact O
match O
score O
on O
the O
validation O
set O
. O
We O
set O
the O
learning O
rate O
as O
1e-4 O
, O
decaying O
by O
0.9 O
every O
20,000 O
steps O
. O
We O
choose O
the O
number O
of O
message O
passing O
iterations O
to O
be O
4 O
. O
Our O
model O
is O
trained O
for O
6 O
separate O
runs O
, O
and O
the O
average O
performance O
as O
well O
as O
the O
standard O
deviation O
are O
reported O
. O
Our O
encoder O
/ O
decoder O
model O
is O
implemented O
in O
PyTorch O
( O
Paszke O
et O
al O
. O
, O
2017 O
) O
and O
the O
message O
passing O
graph O
network O
is O
backed O
by O
DGL O
( O
Wang O
et O
al O
. O
, O
2020 O
) O
. O
For O
comparison O
, O
we O
use O
test O
set O
, O
validation O
set O
, O
and O
baseline O
model O
released O
by O
Ruis O
et O
al O
. O
( O
2020 O
) O
. O
4.2 O
Results O
Table O
1 O
is O
an O
overview O
of O
7 O
test O
splits O
used O
for O
evaluation O
, O
and O
table O
2 O
shows O
our O
experiment O
results O
as O
well O
as O
other O
models O
‚Äô O
performance O
for O
comparison O
. O
In O
the O
following O
sections O
, O
we O
present O
the O
results O
on O
each O
systematic O
generalization O
test O
split O
, O
and O
also O
introduce O
the O
conÔ¨Åguration O
of O
test O
splits O
. O
Note O
that O
test O
split O
A O
is O
a O
random O
split O
set O
that O
has O
no O
systematic O
difference O
from O
the O
training O
set O
. O
1Code O
is O
available O
hereSplit O
B O
: O
This O
tests O
the O
model O
‚Äôs O
ability O
to O
generalize O
to O
navigation O
in O
a O
novel O
direction O
. O
For O
example O
, O
a O
testing O
example O
would O
require O
the O
agent O
to O
move O
to O
a O
target O
object O
that O
is O
to O
its O
southwest O
, O
even O
though O
during O
training O
target O
objects O
are O
never O
placed O
south O
- O
west O
of O
the O
agent O
. O
Although O
our O
model O
manages O
to O
predict O
some O
correct O
action O
sequences O
compared O
to O
the O
baseline O
‚Äôs O
complete O
failure O
, O
our O
model O
still O
fails O
on O
the O
majority O
of O
cases O
. O
We O
further O
analyze O
the O
failure O
on O
Split O
B O
in O
the O
discussion O
section O
. O
Split O
C O
, O
G O
: O
Split O
C O
tests O
the O
model O
‚Äôs O
ability O
to O
generalize O
to O
novel O
contextual O
references O
. O
In O
the O
training O
set O
, O
a O
circle O
of O
size O
2 O
is O
never O
referred O
to O
as O
‚Äú O
the O
small O
circle O
‚Äù O
, O
while O
in O
the O
test O
set O
the O
agent O
needs O
to O
generalize O
the O
notion O
‚Äú O
small O
‚Äù O
to O
it O
based O
on O
its O
size O
comparison O
with O
other O
circles O
in O
the O
grid O
world O
. O
The O
message O
passing O
mechanism O
helps O
the O
model O
comprehend O
the O
relative O
sizes O
of O
objects O
, O
and O
boost O
the O
performance O
on O
split O
C. O
Besides O
, O
our O
model O
shows O
promising O
results O
on O
exploring O
the O
interrelationship O
between O
an O
agent O
and O
other O
objects O
in O
the O
scene O
, O
as O
well O
as O
learning O
abstract O
concepts O
by O
contextual O
comparison O
as O
shown O
in O
split O
G. O
This O
test O
split O
asks O
the O
model O
to O
push O
a O
square O
of O
size O
3 O
. O
An O
object O
with O
the O
size O
of O
3 O
or O
4 O
is O
deÔ¨Åned O
as O
‚Äú O
heavy O
‚Äù O
, O
according O
to O
the O
conÔ¨Åguration O
, O
and O
requires O
two O
consecutive O
push O
/ O
pull O
actions O
applied O
on O
it O
before O
it O
actually O
moves O
. O
The O
challenge O
here O
is O
that O
the O
model O
has O
been O
trained O
to‚Äúpull O
‚Äù O
heavy O
squares O
and O
‚Äú O
push O
‚Äù O
squares O
with O
size O
of O
4 O
, O
but O
was O
never O
trained O
to O
‚Äú O
push O
‚Äù O
a O
size-3 O
square O
. O
Thus O
, O
it O
needs O
to O
generalize O
the O
concept O
of O
‚Äú O
heavy O
‚Äù O
and O
act O
accordingly O
. O
Split O
D O
, O
E O
: O
Split O
D O
and O
E O
are O
similar O
, O
as O
they O
both O
deÔ¨Åne O
the O
target O
object O
with O
novel O
combinations O
of O
color O
and O
shape O
. O
Split O
E O
is O
generally O
easier O
because O
the O
target O
object O
, O
a O
yellow O
square O
, O
appears O
as O
the O
target O
in O
training O
examples O
, O
but O
is O
only O
referred O
to O
as O
‚Äú O
the O
square O
‚Äù O
, O
‚Äú O
the O
smaller O
square O
‚Äù O
, O
or O
‚Äú O
the O
bigger O
square O
‚Äù O
. O
Split O
D O
increases O
the O
difÔ¨Åculty O
by O
referring O
to O
the O
red O
square O
, O
which O
never O
appears O
in O
the O
training O
set O
as O
a O
target O
but O
does O
appear O
as O
a O
background O
object O
. O
We O
Ô¨Ånd O
that O
while O
the O
baseline O
model O
understands O
the O
concept O
of O
‚Äú O
square O
‚Äù O
, O
it O
gets O
confused O
by O
target O
objects O
with O
a O
new O
color O
- O
shape O
combination O
. O
In O
contrast O
, O
our O
model O
can O
generalize O
to O
novel O
compositions O
of O
object O
properties O
and O
correctly O
Ô¨Ånd O
the O
target O
object O
, O
performing O
signiÔ¨Åcantly O
better O
on O
these O
two O
splits O
. O
Split O
F O
: O
This O
split O
is O
designed O
to O
test O
the O
model‚Äôs496Split O
Description O
A O
: O
Random O
Randomly O
split O
test O
sets O
B O
: O
Novel O
Direction O
Target O
object O
is O
to O
the O
South O
- O
West O
of O
the O
agent O
C O
: O
Relativity O
Target O
object O
is O
a O
size O
2circle O
, O
referred O
to O
with O
the O
small O
modiÔ¨Åer O
D O
: O
Red O
Squares O
Red O
squares O
are O
the O
target O
object O
E O
: O
Yellow O
Squares O
Yellow O
squares O
are O
referred O
to O
with O
a O
color O
and O
a O
shape O
at O
least O
F O
: O
Adverb O
to O
Verb O
All O
examples O
with O
the O
adverb O
‚Äô O
while O
spinning O
‚Äô O
and O
the O
verb O
‚Äô O
pull O
‚Äô O
G O
: O
Class O
Inference O
All O
examples O
where O
the O
agent O
needs O
to O
push O
a O
square O
of O
size O
3 O
Table O
1 O
: O
Description O
of O
test O
splits O
Split O
Baseline O
Kuo O
et O
al O
. O
( O
2020 O
) O
Heinze O
- O
Deml O
and O
Bouchacourt O
( O
2020 O
) O
Ours O
A O
: O
Random O
97.69¬±0.22 O
97.32 O
94.19¬±0.71 O
98.6¬±0.95 O
B O
: O
Novel O
Direction O
0¬±0 O
5.73 O
N O
/ O
A O
0.16¬±0.12 O
C O
: O
Relativity O
35.02¬±2.35 O
75.19 O
43.43¬±7.0 O
87.32¬±27.38 O
D O
: O
Red O
Squares O
23.51¬±21.82 O
80.16 O
81.07¬±10.12 O
80.31¬±24.51 O
E O
: O
Yellow O
Squares O
54.96¬±39.39 O
95.35 O
86.45¬±6.28 O
99.08¬±0.69 O
F O
: O
Adverb O
to O
Verb O
22.7¬±4.59 O
0 O
N O
/ O
A O
33.6¬±20.81 O
G O
: O
Class O
Inference O
92.95¬±6.75 O
98.63 O
N O
/ O
A O
99.33¬±0.46 O
Table O
2 O
: O
Exact O
match O
accuracy O
of O
test O
splits O
ability O
to O
generalize O
to O
novel O
adverb O
- O
verb O
combinations O
, O
where O
the O
model O
is O
tested O
under O
different O
situations O
but O
always O
with O
the O
terms O
‚Äú O
while O
spinning O
‚Äù O
and O
‚Äú O
pull O
‚Äù O
in O
the O
commands O
. O
However O
, O
they O
never O
appear O
in O
the O
training O
set O
together O
, O
consequently O
the O
model O
needs O
to O
generalize O
to O
this O
novel O
combination O
of O
adverb O
and O
verb O
. O
The O
results O
shows O
that O
our O
model O
does O
a O
bit O
better O
than O
the O
baseline O
, O
but O
suffers O
from O
high O
variance O
across O
different O
runs O
. O
Comparing O
to O
the O
two O
concurrent O
works O
Kuo O
et O
al O
. O
( O
2020 O
) O
and O
Heinze O
- O
Deml O
and O
Bouchacourt O
( O
2020 O
) O
, O
our O
model O
yields O
better O
performance O
in O
general O
. O
Notice O
that O
Heinze O
- O
Deml O
and O
Bouchacourt O
( O
2020 O
) O
and O
our O
model O
also O
report O
the O
standard O
deviation O
of O
multiple O
runs O
, O
while O
Kuo O
et O
al O
. O
( O
2020 O
) O
does O
not O
. O
4.3 O
Discussion O
Model O
Comparison O
. O
We O
reveal O
the O
strength O
of O
our O
model O
by O
analyzing O
two O
test O
examples O
where O
it O
succeeds O
and O
the O
baseline O
fails O
. O
For O
each O
example O
, O
we O
visualize O
the O
grid O
world O
that O
the O
agent O
is O
in O
, O
where O
each O
cell O
is O
colored O
with O
different O
grey O
- O
scale O
levels O
indicating O
its O
assigned O
attention O
score O
. O
For O
reader O
‚Äôs O
convenience O
, O
we O
also O
visualize O
the O
model O
‚Äôs O
prediction O
and O
the O
target O
sequence O
by O
the O
red O
path O
and O
green O
path O
, O
respectively O
. O
Figure O
2 O
from O
split O
G O
visualizes O
the O
prediction O
sequence O
as O
well O
as O
the O
attention O
weights O
generated O
by O
the O
baseline O
. O
The O
baseline O
attends O
to O
the O
position O
of O
the O
target O
object O
but O
is O
unable O
to O
capture O
the O
dynamic O
relationship O
between O
the O
target O
object O
and O
the O
green O
cylinder O
. O
It O
tries O
to O
push O
the O
target O
object O
over O
it O
, O
while O
our O
model O
correctly O
predicts O
Figure O
2 O
: O
While O
the O
target O
is O
correctly O
chosen O
, O
the O
baseline O
did O
not O
stop O
pushing O
even O
after O
encountering O
an O
obstacle O
. O
the O
incoming O
collision O
and O
stops O
at O
the O
right O
time O
. O
Another O
example O
from O
split O
D O
where O
our O
model O
outperforms O
the O
baseline O
is O
shown O
in O
Figure O
3 O
. O
The O
baseline O
model O
incorrectly O
attends O
to O
two O
small O
blue O
squares O
and O
picks O
one O
as O
the O
target O
rather O
than O
the O
correct O
small O
red O
square O
. O
Note O
that O
the O
model O
has O
seen O
blue O
and O
green O
squares O
as O
targets O
in O
the O
training O
set O
, O
but O
has O
never O
seen O
a O
red O
square O
. O
This O
is O
a O
common O
mistake O
since O
the O
baseline O
struggles O
to O
choose O
target O
objects O
with O
novel O
property O
combinations O
when O
there O
are O
similar O
objects O
in O
the O
scene O
that O
were O
seen O
during O
training O
. O
On O
the O
contrary,497Figure O
3 O
: O
Baseline O
can O
not O
distinguish O
the O
correct O
square O
from O
similar O
candidates O
. O
our O
model O
handles O
these O
cases O
well O
, O
demonstrating O
its O
ability O
to O
generalize O
to O
novel O
color O
- O
shape O
combinations O
with O
the O
help O
of O
contextualized O
object O
embedding O
. O
Split O
No O
Message O
Passing O
Full O
Model O
A O
91.07¬±0.61 O
98.6¬±0.95 O
B O
0.16¬±0.04 O
0.16¬±0.12 O
C O
50.26¬±5.9 O
87.32¬±27.38 O
D O
35.95¬±13.13 O
80.31¬±24.51 O
E O
44.18¬±24.56 O
99.08¬±0.69 O
F O
44.82¬±1.95 O
33.6¬±20.81 O
G O
93.02¬±0.33 O
99.33¬±0.46 O
Table O
3 O
: O
Ablation O
study O
Ablation O
Study O
. O
We O
conduct O
an O
ablation O
study O
to O
test O
the O
signiÔ¨Åcance O
of O
the O
languageconditioned O
message O
passing O
component O
in O
our O
network O
. O
We O
built O
a O
model O
whose O
architecture O
and O
hyper O
- O
parameters O
are O
the O
same O
as O
our O
full O
model O
, O
except O
that O
we O
remove O
the O
language O
- O
conditioned O
message O
passing O
module O
described O
in O
section O
3.2.2 O
. O
That O
is O
, O
we O
follow O
all O
the O
steps O
in O
section O
3.2.1 O
and O
obtain O
every O
object O
‚Äôs O
local O
embedding O
, O
then O
map O
new O
embedding O
back O
to O
the O
their O
locations O
as O
stated O
in O
section O
3.2.3 O
. O
The O
results O
in O
Table O
3 O
indicate O
that O
language O
- O
conditioned O
message O
passing O
does O
help O
achieve O
higher O
exact O
match O
accuracy O
in O
many O
test O
splits O
, O
though O
it O
sometimes O
hurts O
the O
performance O
on O
split O
F. O
We O
conclude O
that O
the O
model O
is O
getting O
better O
at O
understanding O
object O
- O
related O
com O
- O
mands O
( O
‚Äú O
pull O
‚Äù O
moves O
the O
object O
) O
, O
sacriÔ¨Åcing O
some O
ability O
to O
discover O
the O
meaning O
of O
easy O
- O
to O
- O
translate O
adverbs O
that O
are O
irrelevant O
to O
the O
interaction O
with O
objects O
( O
‚Äú O
while O
spinning O
‚Äù O
only O
describes O
the O
behavior O
of O
agent O
with O
no O
impact O
on O
the O
scene O
) O
. O
Failure O
on O
Split O
B. O
Here O
we O
analyze O
a O
failure O
case O
to O
understand O
why O
split O
B O
is O
notably O
difÔ¨Åcult O
for O
our O
model O
. O
Figure O
4 O
demonstrates O
an O
example O
that O
leads O
to O
both O
models O
‚Äô O
failure O
. O
The O
attention O
scores O
indicate O
that O
the O
model O
has O
identiÔ¨Åed O
the O
correct O
target O
position O
, O
but O
does O
not O
know O
the O
correct O
action O
sequence O
to O
get O
there O
. O
The O
LSTM O
decoder O
can O
not O
generalize O
the O
meaning O
of O
action O
tokens O
that O
direct O
the O
agent O
towards O
an O
unseen O
direction O
. O
We O
can O
observe O
from O
our O
model O
‚Äôs O
output O
prediction O
that O
, O
even O
if O
it O
manages O
to O
correctly O
predict O
the O
Ô¨Årst O
few O
steps O
( O
‚Äù O
turn O
left O
turn O
left O
walk O
‚Äù O
) O
, O
it O
quickly O
gets O
lost O
and O
fails O
to O
navigate O
to O
the O
target O
location O
. O
The O
model O
only O
observes O
the O
initial O
world O
state O
and O
the O
command O
, O
then O
generates O
a O
sequence O
of O
actions O
toward O
the O
target O
. O
In O
other O
words O
, O
it O
is O
blindly O
generating O
the O
action O
sequence O
with O
only O
a O
static O
image O
of O
the O
agent O
and O
the O
target O
‚Äôs O
location O
, O
not O
really O
modeling O
the O
movement O
of O
the O
agent O
. O
However O
, O
humans O
usually O
do O
not O
handle O
navigation O
in O
a O
novel O
direction O
in O
this O
way O
. O
Instead O
, O
they O
will O
Ô¨Årst O
turn O
to O
the O
correct O
direction O
, O
and O
transform O
the O
novel O
task O
into O
a O
familiar O
task O
( O
‚Äù O
walk O
southwest O
is O
equivalent O
to O
turn O
southwest O
then O
walk O
the O
same O
as O
you O
walk O
north O
‚Äù O
) O
. O
This O
naturally O
requires O
a O
change O
of O
perspective O
and O
conditioning O
on O
the O
agent O
‚Äôs O
previous O
action O
. O
A O
possible O
improvement O
is O
to O
introduce O
clues O
to O
inform O
the O
model O
of O
possible O
changes O
in O
its O
view O
as O
it O
takes O
actions O
. O
5 O
Conclusion O
and O
Future O
Work O
In O
this O
paper O
, O
we O
proposed O
a O
language O
- O
conditioned O
message O
passing O
model O
for O
a O
grounded O
language O
navigation O
task O
that O
can O
dynamically O
extract O
contextualized O
embeddings O
based O
on O
input O
command O
sentences O
, O
and O
can O
be O
trained O
end O
- O
to O
- O
end O
with O
the O
downstream O
action O
- O
sequence O
decoder O
. O
We O
showed O
that O
obtaining O
such O
contextualized O
embeddings O
improves O
performance O
on O
a O
recently O
introduced O
challenge O
problem O
, O
gSCAN O
, O
signiÔ¨Åcantly O
outperforming O
the O
state O
- O
of O
- O
the O
- O
art O
across O
several O
test O
splits O
designed O
to O
test O
a O
model O
‚Äôs O
ability O
to O
represent O
novel O
concept O
compositions O
and O
achieve O
systematic O
generalization O
. O
Nonetheless O
, O
our O
model O
‚Äôs O
fairly O
poor O
performance O
on O
split O
B O
and O
F O
shows O
that O
challenges O
still498Figure O
4 O
: O
Failure O
case O
on O
split O
B O
, O
prediction O
and O
attention O
scores O
were O
generated O
by O
our O
model O
. O
remain O
. O
As O
explained O
in O
the O
discussion O
section O
, O
our O
model O
is O
falling O
short O
of O
estimating O
the O
effect O
of O
each O
action O
on O
the O
agent O
‚Äôs O
state O
. O
An O
alternative O
view O
of O
this O
problem O
is O
as O
a O
reinforcement O
learning O
task O
with O
sparse O
reward O
. O
Sample O
- O
efÔ¨Åcient O
model O
- O
based O
reinforcement O
learning O
( O
Buckman O
et O
al O
. O
, O
2018 O
) O
could O
then O
be O
used O
, O
and O
its O
natural O
ability O
to O
explicitly O
model O
environment O
change O
should O
improve O
performance O
on O
this O
task O
. O
It O
would O
also O
be O
beneÔ¨Åcial O
to O
visualize O
the O
dynamically O
generated O
edge O
weights O
during O
message O
passing O
to O
have O
a O
more O
intuitive O
understanding O
of O
what O
contextual O
information O
is O
integrated O
during O
the O
message O
passing O
phase O
. O
Currently O
, O
we O
consider O
all O
objects O
appearing O
on O
the O
grid O
, O
including O
the O
agent O
, O
as O
homogeneous O
nodes O
during O
message O
passing O
, O
and O
all O
edges O
in O
the O
message O
passing O
graph O
are O
modelled O
in O
the O
same O
way O
. O
However O
, O
intuitively O
, O
we O
should O
model O
the O
relation O
between O
different O
types O
of O
objects O
differently O
. O
For O
example O
, O
the O
relation O
between O
the O
agent O
and O
the O
target O
object O
of O
pulling O
might O
be O
different O
from O
the O
relation O
between O
two O
objects O
on O
the O
grid O
. O
Inspired O
by O
Bahdanau O
et O
al O
. O
( O
2018 O
) O
, O
it O
would O
be O
interesting O
to O
try O
modeling O
different O
edge O
types O
explicitly O
with O
neural O
modules O
, O
and O
perform O
type O
- O
speciÔ¨Åc O
message O
passing O
to O
obtain O
better O
contextualized O
embeddings O
. O
References O
Peter O
Anderson O
, O
Xiaodong O
He O
, O
Chris O
Buehler O
, O
Damien O
Teney O
, O
Mark O
Johnson O
, O
Stephen O
Gould O
, O
and O
Lei O
Zhang O
. O
2018a O
. O
Bottom O
- O
up O
and O
top O
- O
down O
attention O
for O
image O
captioning O
and O
visual O
question O
answering O
. O
InProceedings O
of O
the O
IEEE O
conference O
on O
computer O
vision O
and O
pattern O
recognition O
, O
pages O
6077‚Äì6086 O
. O
Peter O
Anderson O
, O
Qi O
Wu O
, O
Damien O
Teney O
, O
Jake O
Bruce O
, O
Mark O
Johnson O
, O
Niko O
S O
¬®underhauf O
, O
Ian O
Reid O
, O
Stephen O
Gould O
, O
and O
Anton O
van O
den O
Hengel O
. O
2018b O
. O
Visionand O
- O
language O
navigation O
: O
Interpreting O
visuallygrounded O
navigation O
instructions O
in O
real O
environments O
. O
In O
Proceedings O
of O
the O
IEEE O
Conference O
on O
Computer O
Vision O
and O
Pattern O
Recognition O
, O
pages O
3674‚Äì3683 O
. O
Jacob O
Andreas O
. O
2020 O
. O
Good O
- O
enough O
compositional O
data O
augmentation O
. O
In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
7556‚Äì7566 O
, O
Online O
. O
Association O
for O
Computational O
Linguistics O
. O
Dzmitry O
Bahdanau O
, O
Kyunghyun O
Cho O
, O
and O
Yoshua O
Bengio O
. O
2016 O
. O
Neural O
machine O
translation O
by O
jointly O
learning O
to O
align O
and O
translate O
. O
Dzmitry O
Bahdanau O
, O
Shikhar O
Murty O
, O
Michael O
Noukhovitch O
, O
Thien O
Huu O
Nguyen O
, O
Harm O
de O
Vries O
, O
and O
Aaron O
C. O
Courville O
. O
2018 O
. O
Systematic O
generalization O
: O
What O
is O
required O
and O
can O
it O
be O
learned O
? O
CoRR O
, O
abs/1811.12889 O
. O
Jacob O
Buckman O
, O
Danijar O
Hafner O
, O
George O
Tucker O
, O
Eugene O
Brevdo O
, O
and O
Honglak O
Lee O
. O
2018 O
. O
SampleefÔ¨Åcient O
reinforcement O
learning O
with O
stochastic O
ensemble O
value O
expansion O
. O
In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
, O
pages O
8224‚Äì8234 O
. O
Maxime O
Chevalier O
- O
Boisvert O
, O
Dzmitry O
Bahdanau O
, O
Salem O
Lahlou O
, O
Lucas O
Willems O
, O
Chitwan O
Saharia O
, O
Thien O
Huu O
Nguyen O
, O
and O
Yoshua O
Bengio O
. O
2019 O
. O
Babyai O
: O
A O
platform O
to O
study O
the O
sample O
efÔ¨Åciency O
of O
grounded O
language O
learning O
. O
Jerry O
A O
Fodor O
, O
Zenon O
W O
Pylyshyn O
, O
et O
al O
. O
1988 O
. O
Connectionism O
and O
cognitive O
architecture O
: O
A O
critical O
analysis O
. O
Cognition O
, O
28(1 O
- O
2):3‚Äì71 O
. O
Jonathan O
Gordon O
, O
David O
Lopez O
- O
Paz O
, O
Marco O
Baroni O
, O
and O
Diane O
Bouchacourt O
. O
2019 O
. O
Permutation O
equivariant O
models O
for O
compositional O
generalization O
in O
language O
. O
In O
International O
Conference O
on O
Learning O
Representations O
. O
Suchin O
Gururangan O
, O
Swabha O
Swayamdipta O
, O
Omer O
Levy O
, O
Roy O
Schwartz O
, O
Samuel O
Bowman O
, O
and O
Noah O
A. O
Smith O
. O
2018 O
. O
Annotation O
artifacts O
in O
natural O
language O
inference O
data O
. O
In O
Proceedings O
of O
the O
2018 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
2 O
( O
Short O
Papers O
) O
, O
pages O
107‚Äì112 O
, O
New O
Orleans O
, O
Louisiana O
. O
Association O
for O
Computational O
Linguistics.499Christina O
Heinze O
- O
Deml O
and O
Diane O
Bouchacourt O
. O
2020 O
. O
Think O
before O
you O
act O
: O
A O
simple O
baseline O
for O
compositional O
generalization O
. O
Felix O
Hill O
, O
Andrew O
Lampinen O
, O
Rosalia O
Schneider O
, O
Stephen O
Clark O
, O
Matthew O
Botvinick O
, O
James O
L. O
McClelland O
, O
and O
Adam O
Santoro O
. O
2020 O
. O
Environmental O
drivers O
of O
systematicity O
and O
generalization O
in O
a O
situated O
agent O
. O
Ronghang O
Hu O
, O
Jacob O
Andreas O
, O
Trevor O
Darrell O
, O
and O
Kate O
Saenko O
. O
2018 O
. O
Explainable O
neural O
computation O
via O
stack O
neural O
module O
networks O
. O
In O
Proceedings O
of O
the O
European O
conference O
on O
computer O
vision O
( O
ECCV O
) O
, O
pages O
53‚Äì69 O
. O
Ronghang O
Hu O
, O
Anna O
Rohrbach O
, O
Trevor O
Darrell O
, O
and O
Kate O
Saenko O
. O
2019 O
. O
Language O
- O
conditioned O
graph O
networks O
for O
relational O
reasoning O
. O
2019 O
IEEE O
/ O
CVF O
International O
Conference O
on O
Computer O
Vision O
( O
ICCV O
) O
. O
Drew O
Arad O
Hudson O
and O
Christopher O
D. O
Manning O
. O
2018 O
. O
Compositional O
attention O
networks O
for O
machine O
reasoning O
. O
In O
International O
Conference O
on O
Learning O
Representations O
. O
Dieuwke O
Hupkes O
, O
Verna O
Dankers O
, O
Mathijs O
Mul O
, O
and O
Elia O
Bruni O
. O
2020 O
. O
Compositionality O
decomposed O
: O
How O
do O
neural O
networks O
generalise O
? O
Journal O
of O
ArtiÔ¨Åcial O
Intelligence O
Research O
, O
67:757‚Äì795 O
. O
Robin O
Jia O
and O
Percy O
Liang O
. O
2017 O
. O
Adversarial O
examples O
for O
evaluating O
reading O
comprehension O
systems O
. O
InProceedings O
of O
the O
2017 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
2021‚Äì2031 O
, O
Copenhagen O
, O
Denmark O
. O
Association O
for O
Computational O
Linguistics O
. O
Hans O
Kamp O
and O
Barbara O
Partee O
. O
1995 O
. O
Prototype O
theory O
and O
compositionality O
. O
Cognition O
, O
57(2):129 O
‚Äì O
191 O
. O
Yen O
- O
Ling O
Kuo O
, O
Boris O
Katz O
, O
and O
Andrei O
Barbu O
. O
2020 O
. O
Compositional O
networks O
enable O
systematic O
generalization O
for O
grounded O
language O
understanding O
. O
Brenden O
M. O
Lake O
and O
Marco O
Baroni O
. O
2017 O
. O
Generalization O
without O
systematicity O
: O
On O
the O
compositional O
skills O
of O
sequence O
- O
to O
- O
sequence O
recurrent O
networks O
. O
Adam O
Paszke O
, O
Sam O
Gross O
, O
Soumith O
Chintala O
, O
Gregory O
Chanan O
, O
Edward O
Yang O
, O
Zachary O
DeVito O
, O
Zeming O
Lin O
, O
Alban O
Desmaison O
, O
Luca O
Antiga O
, O
and O
Adam O
Lerer O
. O
2017 O
. O
Automatic O
differentiation O
in O
pytorch O
. O
Ethan O
Perez O
, O
Florian O
Strub O
, O
Harm O
De O
Vries O
, O
Vincent O
Dumoulin O
, O
and O
Aaron O
Courville O
. O
2018 O
. O
Film O
: O
Visual O
reasoning O
with O
a O
general O
conditioning O
layer O
. O
In O
Thirty O
- O
Second O
AAAI O
Conference O
on O
ArtiÔ¨Åcial O
Intelligence O
. O
Laura O
Ruis O
, O
Jacob O
Andreas O
, O
Marco O
Baroni O
, O
Diane O
Bouchacourt O
, O
and O
Brenden O
M. O
Lake O
. O
2020 O
. O
A O
benchmark O
for O
systematic O
generalization O
in O
grounded O
language O
understanding O
. O
Minjie O
Wang O
, O
Da O
Zheng O
, O
Zihao O
Ye O
, O
Quan O
Gan O
, O
Mufei O
Li O
, O
Xiang O
Song O
, O
Jinjing O
Zhou O
, O
Chao O
Ma O
, O
Lingfan O
Yu O
, O
Yu O
Gai O
, O
Tianjun O
Xiao O
, O
Tong O
He O
, O
George O
Karypis O
, O
Jinyang O
Li O
, O
and O
Zheng O
Zhang O
. O
2020 O
. O
Deep O
graph O
library O
: O
A O
graph O
- O
centric O
, O
highly O
- O
performant O
package O
for O
graph O
neural O
networks O
. O
Ziyun O
Wang O
and O
Brenden O
M. O
Lake O
. O
2019 O
. O
Modeling O
question O
asking O
using O
neural O
program O
generation.500A O
Appendix O
A.1 O
Implementation O
Details O
Our O
implementation O
is O
based O
on O
the O
gSCAN O
dataset O
used O
by O
the O
Ruis O
et O
al O
. O
( O
2020 O
) O
and O
the O
world O
size O
is O
d= O
6 O
. O
For O
equation O
1 O
, O
each O
token O
is O
embedded O
to O
a O
randomly O
initialized O
vector O
of O
size O
32 O
, O
and O
the O
hidden O
size O
of O
the O
encoder O
BiLSTM O
is O
32 O
. O
For O
equation O
9 O
, O
we O
use O
three O
convolutional O
networks O
with O
kernel O
size O
k= O
1,5,7and O
padding O
size O
‚åäk O
2‚åã= O
0,2,3to O
ensure O
that O
the O
resulting O
dimensionality O
is O
synchronized O
with O
input O
. O
They O
share O
the O
same O
Ô¨Ålter O
size O
of O
64 O
. O
The O
concatenation O
of O
Hs O
i O
is O
also O
Ô¨Çattened O
to O
the O
shape O
of O
36√ó192 O
. O
Table O
4 O
presents O
the O
shapes O
of O
other O
trainable O
parameters O
mentioned O
in O
section O
3 O
. O
We O
simply O
set O
dcmd O
= O
dh O
= O
dloc O
= O
dm O
= O
ds O
= O
dctx= O
64 O
. O
Parameter O
Shape O
W1 O
1√ódcmd O
W2,W O
3dcmd√ódcmd O
W4dh√ódloc O
W5dh√ódctx O
W6,W O
7dh√ó(dloc+dctx+dh O
) O
W8dh√óds O
W9dm√ó(dloc+dctx+dh O
) O
W10dm√óds O
W11dctx√ódctx O
W12dh√ó(dloc+dctx O
) O
Table O
4 O
: O
Parameter O
Shapes O
A.2 O
Example O
Visualization O
Here O
we O
present O
more O
examples O
demonstrating O
our O
model O
‚Äôs O
strengths O
and O
weaknesses O
. O
Figures O
5 O
- O
8 O
are O
cases O
where O
our O
model O
‚Äôs O
prediction O
exactly O
matches O
the O
target O
while O
the O
baseline O
‚Äôs O
does O
not O
. O
Some O
of O
the O
common O
failures O
of O
our O
model O
are O
illustrated O
in O
Figures O
9 O
- O
11 O
. O
Figure O
5 O
: O
Baseline O
incorrectly O
picked O
a O
yellow O
square O
as O
the O
target O
. O
Figure O
6 O
: O
Baseline O
incorrectly O
picked O
a O
red O
square O
as O
the O
target.501Figure O
7 O
: O
Baseline O
falsely O
predicted O
the O
consequential O
interaction O
and O
decided O
not O
to O
push O
. O
Figure O
8 O
: O
Baseline O
incorrectly O
picked O
the O
bigger O
circle O
instead O
of O
the O
smaller O
one O
. O
Figure O
9 O
: O
Getting O
lost O
along O
a O
long O
sequence O
: O
Our O
model O
fails O
when O
the O
target O
sequence O
repeats O
the O
same O
actions O
several O
times O
. O
Figure O
10 O
: O
Incorrect O
path O
plan O
: O
Our O
model O
generates O
the O
path O
plan O
in O
a O
partially O
- O
reversed O
order.502Figure O
11 O
: O
Early O
stop O
before O
reaching O
boundary O
: O
Our O
model O
stops O
pushing O
when O
the O
target O
object O
is O
next O
to O
the O
boundary O
grid.503Proceedings O
of O
the O
1st O
Conference O
of O
the O
Asia O
- O
PaciÔ¨Åc O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
10th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
504‚Äì515 O
December O
4 O
- O
7 O
, O
2020 O
. O
¬© O
2020 O
Association O
for O
Computational O
Linguistics O
Are O
scene O
graphs O
good O
enough O
to O
improve O
Image O
Captioning O
? O
Victor O
Milewski1Marie O
- O
Francine O
Moens1Iacer O
Calixto2,3 O
1KU O
Leuven2New O
York O
University3ILLC O
, O
University O
of O
Amsterdam O
{ O
victor.milewski O
, O
sien.moens O
} O
@cs.kuleuven.be O
iacer.calixto@nyu.edu O
Abstract O
Many O
top O
- O
performing O
image O
captioning O
models O
rely O
solely O
on O
object O
features O
computed O
with O
an O
object O
detection O
model O
to O
generate O
image O
descriptions O
. O
However O
, O
recent O
studies O
propose O
to O
directly O
use O
scene O
graphs O
to O
introduce O
information O
about O
object O
relations O
into O
captioning O
, O
hoping O
to O
better O
describe O
interactions O
between O
objects O
. O
In O
this O
work O
, O
we O
thoroughly O
investigate O
the O
use O
of O
scene O
graphs O
in O
image O
captioning O
. O
We O
empirically O
study O
whether O
using O
additional O
scene O
graph O
encoders O
can O
lead O
to O
better O
image O
descriptions O
and O
propose O
a O
conditional O
graph O
attention O
network O
( O
CGAT O
) O
, O
where O
the O
image O
captioning O
decoder O
state O
is O
used O
to O
condition O
the O
graph O
updates O
. O
Finally O
, O
we O
determine O
to O
what O
extent O
noise O
in O
the O
predicted O
scene O
graphs O
inÔ¨Çuence O
caption O
quality O
. O
Overall O
, O
we O
Ô¨Ånd O
no O
signiÔ¨Åcant O
difference O
between O
models O
that O
use O
scene O
graph O
features O
and O
models O
that O
only O
use O
object O
detection O
features O
across O
different O
captioning O
metrics O
, O
which O
suggests O
that O
existing O
scene O
graph O
generation O
models O
are O
still O
too O
noisy O
to O
be O
useful O
in O
image O
captioning O
. O
Moreover O
, O
although O
the O
quality O
of O
predicted O
scene O
graphs O
is O
very O
low O
in O
general O
, O
when O
using O
high O
quality O
scene O
graphs O
we O
obtain O
gains O
of O
up O
to O
3.3 O
CIDEr O
compared O
to O
a O
strong O
Bottom O
- O
Up O
Top O
- O
Down O
baseline.1 O
1 O
Introduction O
Scene O
understanding O
is O
a O
complex O
and O
intricate O
activity O
which O
humans O
perform O
effortlessly O
but O
that O
computational O
models O
still O
struggle O
with O
. O
An O
important O
backbone O
of O
scene O
understanding O
is O
being O
able O
to O
detect O
objects O
and O
relations O
between O
objects O
in O
an O
image O
, O
and O
scene O
graphs O
( O
Johnson O
et O
al O
. O
, O
2015 O
; O
Anderson O
et O
al O
. O
, O
2016 O
) O
are O
a O
closely O
related O
data O
1We O
open O
source O
the O
codebase O
to O
reproduce O
all O
our O
experiments O
in O
https://github.com/iacercalixto/ O
butd O
- O
image O
- O
captioning O
.structure O
that O
explicitly O
annotates O
an O
image O
with O
its O
objects O
and O
relations O
in O
context O
. O
Scene O
graphs O
can O
be O
used O
to O
improve O
important O
visual O
tasks O
that O
require O
scene O
understanding O
, O
e.g. O
image O
indexing O
and O
search O
( O
Johnson O
et O
al O
. O
, O
2015 O
) O
or O
scene O
construction O
and O
generation O
( O
Johnson O
et O
al O
. O
, O
2017 O
, O
2018 O
) O
, O
and O
there O
is O
evidence O
that O
they O
can O
also O
be O
used O
to O
improve O
image O
captioning O
( O
Yang O
et O
al O
. O
, O
2019 O
; O
Li O
and O
Jiang O
, O
2019 O
) O
. O
However O
, O
the O
de O
facto O
standard O
in O
top O
- O
performing O
image O
captioning O
models O
to O
date O
use O
strong O
object O
features O
only O
, O
e.g. O
obtained O
with O
a O
pretrained O
Faster O
R O
- O
CNN O
( O
Ren O
et O
al O
. O
, O
2015 O
) O
, O
and O
no O
explicit O
relation O
information O
( O
Anderson O
et O
al O
. O
, O
2018 O
; O
Lu O
et O
al O
. O
, O
2018 O
; O
Yu O
et O
al O
. O
, O
2019a O
) O
. O
One O
possible O
explanation O
to O
this O
observation O
is O
that O
by O
using O
detected O
objects O
we O
already O
capture O
the O
more O
important O
information O
that O
characterises O
a O
scene O
, O
and O
that O
relation O
information O
is O
already O
implicitly O
learned O
in O
such O
models O
. O
Another O
explanation O
is O
that O
relations O
are O
simply O
not O
as O
important O
as O
we O
hypothesise O
and O
that O
we O
gain O
no O
valuable O
extra O
information O
by O
adding O
them O
. O
In O
this O
work O
, O
we O
investigate O
these O
empirical O
observations O
in O
more O
detail O
and O
strive O
to O
answer O
the O
following O
research O
questions O
: O
( O
i O
) O
Can O
we O
improve O
image O
captioning O
by O
explicitly O
supervising O
a O
model O
with O
information O
about O
object O
relations O
? O
( O
ii O
) O
How O
does O
the O
content O
of O
the O
captions O
improve O
when O
utilising O
scene O
graphs O
? O
( O
iii O
) O
How O
does O
scene O
graph O
quality O
impact O
the O
quality O
of O
the O
captions O
? O
The O
most O
recent O
best O
- O
performing O
image O
captioning O
models O
make O
use O
of O
the O
Transformer O
architecture O
( O
Vaswani O
et O
al O
. O
, O
2017 O
; O
Li O
et O
al O
. O
, O
2019 O
; O
Yu O
et O
al O
. O
, O
2019b O
) O
. O
However O
, O
in O
this O
paper O
we O
build O
upon O
the O
inÔ¨Çuential O
Bottom O
- O
Up O
Top O
- O
Down O
architecture O
( O
Anderson O
et O
al O
. O
, O
2018 O
) O
which O
uses O
LSTMs O
, O
and O
since O
we O
want O
to O
measure O
to O
what O
extent O
scene O
graphs O
are O
helpful O
or O
not O
, O
we O
remove O
any O
‚Äú O
extras O
‚Äù O
to O
make O
model O
comparison O
easier O
, O
e.g. O
reinforcement O
learning O
step O
after O
cross O
- O
entropy O
training O
, O
ensembling O
at504inference O
time O
, O
etc O
. O
Scene O
graph O
generation O
( O
SGG O
) O
is O
the O
task O
where O
given O
an O
image O
a O
model O
predicts O
a O
graph O
with O
its O
objects O
and O
their O
relations O
. O
We O
use O
a O
pretrained O
SGG O
model O
( O
Xu O
et O
al O
. O
, O
2017 O
) O
to O
obtain O
and O
inject O
explicit O
relation O
information O
into O
image O
captioning O
, O
and O
investigate O
different O
image O
captioning O
model O
architectures O
that O
incorporate O
object O
and O
relation O
features O
, O
similarly O
to O
Li O
and O
Jiang O
( O
2019 O
) O
; O
Wang O
et O
al O
. O
( O
2019 O
) O
. O
We O
propose O
an O
extension O
to O
graph O
attention O
networks O
( O
Veli O
Àáckovi O
¬¥ O
c O
et O
al O
. O
, O
2018 O
) O
which O
we O
call O
conditional O
graph O
attention O
( O
C O
- O
GAT O
) O
, O
where O
we O
condition O
the O
updates O
of O
the O
scene O
graph O
features O
on O
the O
current O
image O
captioning O
decoder O
state O
. O
Finally O
, O
we O
conduct O
an O
in O
- O
depth O
analysis O
of O
the O
captions O
produced O
by O
different O
models O
and O
determine O
if O
scene O
graphs O
actually O
improve O
the O
content O
of O
the O
captions O
. O
Our O
approach O
is O
illustrated O
in O
Figure O
1 O
. O
Our O
main O
contributions O
are O
: O
‚Ä¢We O
investigate O
different O
graph O
- O
based O
architectures O
to O
fuse O
object O
and O
relation O
information O
derived O
from O
scene O
graph O
generation O
models O
in O
the O
context O
of O
image O
captioning O
. O
‚Ä¢We O
introduce O
conditional O
graph O
attention O
networks O
to O
condition O
scene O
graph O
updates O
on O
the O
current O
state O
of O
an O
image O
captioning O
decoder O
and O
Ô¨Ånd O
that O
it O
leads O
to O
improvements O
of O
up O
to O
0.8 O
CIDEr O
. O
‚Ä¢We O
compare O
the O
quality O
of O
the O
generated O
scene O
graphs O
and O
the O
quality O
of O
the O
corresponding O
captions O
and O
Ô¨Ånd O
that O
by O
using O
high O
quality O
scene O
graphs O
we O
can O
improve O
captions O
quality O
by O
up O
to O
3.3 O
CIDEr O
. O
‚Ä¢We O
systematically O
analyse O
captions O
generated O
by O
standard O
image O
captioning O
models O
and O
by O
models O
with O
access O
to O
scene O
graphs O
using O
SPICE O
scores O
for O
objects O
and O
relations O
( O
Anderson O
et O
al O
. O
, O
2016 O
) O
and O
Ô¨Ånd O
that O
when O
using O
scene O
graphs O
there O
is O
an O
increase O
of O
0.4 O
F1 O
for O
relations O
and O
decrease O
of O
0.1 O
F1 O
for O
objects O
. O
2 O
Background O
2.1 O
Object O
Detection O
Object O
detection O
is O
a O
task O
where O
given O
an O
input O
image O
the O
goal O
is O
to O
locate O
and O
label O
all O
its O
objects O
. O
TheFaster O
R O
- O
CNN O
, O
which O
builds O
on O
the O
R O
- O
CNN O
and O
Fast O
R O
- O
CNN O
( O
Girshick O
et O
al O
. O
, O
2014 O
; O
Girshick O
, O
2015 O
) O
, O
is O
a O
widely O
adopted O
model O
proposed O
for O
object O
detection O
( O
Ren O
et O
al O
. O
, O
2015 O
) O
. O
It O
uses O
a O
pretrained O
convolutional O
neural O
network O
( O
CNN O
) O
as O
a O
backbone O
to O
extract O
feature O
maps O
for O
an O
input O
image O
. O
A O
regionproposal O
network O
( O
RPN O
) O
uses O
these O
feature O
maps O
to O
propose O
a O
set O
of O
regions O
with O
a O
high O
likelihood O
of O
containing O
an O
object O
. O
For O
each O
region O
, O
a O
feature O
vector O
is O
generated O
using O
the O
feature O
map O
, O
which O
is O
then O
passed O
to O
an O
object O
classiÔ¨Åcation O
layer O
. O
In O
our O
experiments O
, O
we O
use O
the O
Faster O
R O
- O
CNN O
with O
a O
ResNet-101 O
backbone O
( O
He O
et O
al O
. O
, O
2016 O
) O
. O
2.2 O
Graph O
Neural O
Networks O
Graph O
neural O
networks O
( O
GNNs O
; O
Battaglia O
et O
al O
. O
, O
2018 O
) O
are O
neural O
architectures O
designed O
to O
operate O
on O
arbitrarily O
structured O
graphs O
G= O
( O
V O
, O
E O
) O
, O
whereVandEare O
the O
set O
of O
vertices O
and O
edges O
inG O
, O
respectively O
. O
In O
GNNs O
, O
representations O
for O
a O
vertex O
v‚ààV O
are O
computed O
by O
using O
information O
from O
neighbouring O
vertices O
N(v)which O
are O
deÔ¨Åned O
to O
include O
all O
vertices O
connected O
through O
an O
edge O
. O
In O
this O
work O
, O
we O
use O
a O
neighbourhood O
N(v1)that O
contains O
vertices O
v2connected O
through O
incoming O
edges O
, O
i.e. O
v2‚Üív1‚ààE. O
Graph O
Attention O
Networks O
Graph O
attention O
networks O
( O
GATs O
; O
Veli O
Àáckovi O
¬¥ O
c O
et O
al O
. O
, O
2018 O
) O
combine O
features O
from O
neighbour O
vertices O
N(v1)through O
an O
attention O
mechanism O
( O
Bahdanau O
et O
al O
. O
, O
2014 O
) O
to O
generate O
representations O
for O
vertex O
v1 O
. O
Vertex O
v1 O
‚Äôs O
statevt‚àí1 O
1at O
time O
step O
t‚àí1is O
used O
as O
the O
query O
to O
soft O
- O
select O
the O
information O
from O
neighbours O
relevant O
to O
its O
updated O
state O
vt O
1 O
. O
2.3 O
Scene O
Graphs O
Scene O
graphs O
consist O
of O
a O
data O
structure O
devised O
to O
annotate O
an O
image O
with O
its O
objects O
and O
the O
existing O
relations O
between O
objects O
and O
were O
Ô¨Årst O
introduced O
for O
image O
retrieval O
( O
Johnson O
et O
al O
. O
, O
2015 O
) O
. O
We O
consider O
scene O
graphs O
Gfor O
an O
image O
with O
two O
types O
of O
vertices O
: O
objects O
and O
relations.2Object O
vertices O
describe O
the O
different O
objects O
in O
the O
image O
, O
and O
relation O
vertices O
describe O
how O
different O
objects O
interact O
with O
each O
other O
. O
This O
gives O
us O
the O
following O
rules O
for O
edges O
E:(i)All O
existing O
edges O
are O
between O
an O
object O
vertex O
and O
a O
relation O
vertex O
; O
( O
ii)If O
an O
object O
o1is O
connected O
to O
another O
object O
o2 O
via O
a O
relation O
vertex O
r3 O
, O
then O
vertex O
r3hasonly O
two O
connected O
edges O
: O
one O
incoming O
from O
o1and O
one O
outgoing O
to O
o2 O
. O
Finally O
, O
object O
( O
relation O
) O
vertices O
are O
also O
associated O
to O
an O
object O
( O
relation O
) O
label O
. O
Scene O
Graph O
Generation O
Scene O
graph O
generation O
( O
SGG O
) O
was O
introduced O
by O
Xu O
et O
al O
. O
( O
2017 O
) O
and O
2Attributes O
are O
also O
originally O
present O
in O
scene O
graphs O
as O
vertices O
, O
but O
we O
do O
not O
use O
them.505LSTM1 O
LSTM2 O
dog O
on O
cake O
chair O
  O
fruitnext O
  O
tobehind O
behind O
  O
C O
- O
GAT O
   O
a O
   O
dog O
  O
on O
    O
‚Ä¶ O
Object O
  O
Detection O
Scene O
Graph O
  O
Generation O
  O
Hier O
: O
scene O
  O
graph O
first O
Hier O
: O
objects O
  O
firstFlatAttention O
Figure O
1 O
: O
We O
use O
object O
features O
from O
an O
object O
detection O
model O
and O
scene O
graph O
features O
from O
a O
scene O
graph O
generation O
model O
in O
image O
captioning O
. O
We O
use O
conditional O
graph O
attention O
( O
C O
- O
GAT O
) O
to O
encode O
scene O
graph O
features O
, O
and O
Ô¨Çat O
vs. O
hierarchical O
attention O
mechanisms O
are O
used O
to O
incorporate O
both O
feature O
sets O
into O
an O
LSTM O
decoder O
. O
has O
since O
received O
growing O
attention O
( O
Zellers O
et O
al O
. O
, O
2018 O
; O
Li O
et O
al O
. O
, O
2018 O
; O
Yang O
et O
al O
. O
, O
2018 O
; O
Knyazev O
et O
al O
. O
, O
2020 O
) O
. O
One O
can O
compare O
it O
to O
object O
detection O
( O
Section O
2.1 O
) O
, O
where O
instead O
of O
only O
predicting O
objects O
a O
model O
must O
additionally O
predict O
which O
objects O
have O
relations O
and O
what O
are O
these O
relations O
. O
This O
similarity O
makes O
it O
natural O
that O
SGG O
models O
build O
on O
object O
detection O
architectures O
. O
Most O
SGG O
models O
use O
a O
pretrained O
Faster O
R O
- O
CNN O
or O
similar O
architecture O
to O
predict O
objects O
and O
have O
an O
additional O
component O
to O
predict O
relations O
for O
pairs O
of O
objects O
. O
In O
addition O
to O
the O
original O
object O
loss O
components O
in O
the O
Faster O
R O
- O
CNN O
, O
they O
include O
a O
mechanism O
to O
update O
object O
feature O
representations O
using O
neighbourhood O
information O
, O
and O
a O
component O
to O
predict O
relations O
and O
their O
label O
. O
Iterative O
Message O
Passing O
The O
Iterative O
Message O
Passing O
SGG O
model O
( O
Xu O
et O
al O
. O
, O
2017 O
) O
keeps O
two O
sets O
of O
states O
, O
i.e. O
for O
object O
vertices O
and O
relation O
vertices O
. O
The O
object O
vertices O
are O
initialised O
directly O
from O
Faster O
R O
- O
CNN O
features O
, O
while O
a O
relation O
vertex O
is O
computed O
by O
the O
box O
union O
of O
each O
of O
its O
two O
objects O
boxes O
, O
which O
is O
encoded O
with O
the O
Faster O
R O
- O
CNN O
to O
obtain O
a O
relation O
vector O
. O
Hidden O
states O
in O
each O
set O
are O
updated O
using O
an O
attention O
mechanism O
over O
neighbour O
vertices O
, O
i.e. O
objects O
are O
informed O
by O
all O
connected O
relation O
vertices O
, O
and O
relations O
are O
informed O
by O
the O
two O
objects O
it O
links O
. O
Since O
there O
are O
two O
sets O
of O
states O
it O
is O
easy O
to O
efÔ¨Åciently O
send O
messages O
from O
one O
set O
to O
the O
other O
by O
the O
means O
of O
an O
adjacency O
matrix O
. O
This O
procedure O
is O
repeated O
for O
kiterations O
, O
and O
Xu O
et O
al O
. O
( O
2017 O
) O
found O
that O
k= O
2gives O
optimal O
results O
. O
Relation O
proposal O
network O
( O
RelPN O
) O
Xu O
et O
al O
. O
( O
2017 O
) O
Ô¨Årst O
proposed O
to O
build O
a O
fully O
connected O
graph O
connecting O
all O
object O
pairs O
and O
scoring O
relations O
between O
all O
possible O
object O
pairs O
; O
however O
, O
this O
model O
is O
expensive O
and O
grows O
exponentially O
with O
the O
number O
of O
objects O
. O
Yang O
et O
al O
. O
( O
2018 O
) O
introduced O
a O
relation O
proposal O
network O
( O
RelPN O
) O
, O
which O
works O
similarly O
to O
an O
object O
detection O
RPN O
but O
that O
selectively O
proposes O
relations O
between O
pairs O
of O
objects O
. O
In O
all O
our O
experiments O
, O
we O
use O
the O
Iterative O
Message O
Passing O
model O
trained O
using O
a O
RelPN O
. O
3 O
Conditional O
Graph O
Attention O
( O
C O
- O
GAT O
) O
Standard O
graph O
neural O
architectures O
encode O
information O
about O
neighbour O
nodes O
N(v)into O
representations O
of O
node O
v‚ààV. O
Therefore O
, O
these O
GNNs O
arecontextual O
because O
they O
encode O
graph O
- O
internal O
context O
. O
We O
propose O
the O
conditional O
graph O
attention O
( O
C O
- O
GAT O
) O
architecture O
, O
a O
novel O
extension O
for O
graph O
attention O
networks O
( O
Veli O
Àáckovi O
¬¥ O
c O
et O
al O
. O
, O
2018).3Our O
goal O
is O
to O
make O
these O
networks O
conditional O
in O
addition O
to O
contextual O
. O
By O
conditional O
we O
mean O
that O
a O
C O
- O
GAT O
layer O
is O
conditioned O
on O
external O
context O
, O
e.g. O
a O
vector O
representing O
knowledge O
that O
is O
not O
part O
of O
the O
original O
input O
graph O
. O
Our O
motivation O
is O
that O
when O
using O
graph O
- O
based O
inputs O
such O
as O
a O
scene O
graph O
, O
a O
C O
- O
GAT O
layer O
allows O
us O
to O
condition O
the O
message O
propagation O
between O
connected O
nodes O
in O
the O
graph O
on O
the O
current O
state O
of O
the O
model O
, O
e.g. O
on O
the O
decoder O
state O
in O
the O
3This O
architecture O
is O
novel O
to O
the O
best O
of O
our O
knowledge.506 O
vMAN O
  O
eABO O
VE O
eRIGHT O
‚Äâ  O
OF O
  O
vTREE O
  O
vS O
UN O
  O
q O
LEA O
VE O
S O
  O
TREE O
  O
MAN O
S O
UN O
  O
B O
A O
G O
RIGHT O
  O
OF O
HOLDS O
HAS O
SKY O
  O
FIELD O
ON O
ON O
IN O
  O
ABO O
VE O
  O
vMAN O
   O
( O
l O
Ôºã1)Figure O
2 O
: O
C O
- O
GAT O
layer O
where O
we O
illustrate O
the O
update O
of O
vertex O
MAN O
by O
combining O
the O
features O
of O
all O
incoming O
relations O
( O
and O
objects O
) O
through O
an O
attention O
mechanism O
. O
The O
attention O
scores O
are O
conditioned O
on O
the O
external O
query O
vector O
q. O
captioning O
decoder O
in O
Figure O
1 O
. O
Whereas O
a O
standard O
GAT O
layer O
contextually O
updates O
object O
hidden O
states O
, O
it O
can O
not O
condition O
on O
context O
outside O
the O
scene O
graph.4With O
a O
C O
- O
GAT O
layer O
, O
we O
provide O
a O
mechanism O
for O
the O
model O
to O
learn O
to O
update O
object O
hidden O
states O
in O
the O
context O
of O
the O
current O
state O
of O
the O
decoder O
language O
model O
, O
which O
we O
expect O
to O
lead O
to O
better O
contextual O
features O
. O
In O
Figure O
2 O
, O
a O
C O
- O
GAT O
layer O
is O
applied O
to O
an O
input O
scene O
graphGand O
conditioned O
on O
a O
query O
vector O
q O
, O
i.e. O
the O
decoder O
state O
. O
We O
illustrate O
the O
update O
of O
vertex O
vman‚ààV O
using O
features O
from O
its O
neighbourhoodN(vman O
) O
. O
The O
self O
- O
relation O
is O
assumed O
to O
always O
be O
present O
and O
for O
readability O
is O
not O
shown O
. O
Neighbour O
nodes O
‚Äô O
features O
are O
combined O
with O
an O
MLP O
attention O
mechanism O
( O
Bahdanau O
et O
al O
. O
, O
2014 O
) O
and O
scores O
are O
computed O
using O
query O
q. O
As O
described O
in O
Section O
2.3 O
, O
neighbours O
of O
an O
object O
vertex O
vi‚àà O
V O
in O
a O
scene O
graph O
only O
include O
relation O
vertices O
. O
To O
include O
neighbour O
object O
features O
as O
well O
as O
relation O
features O
, O
we O
collect O
features O
for O
all O
vj‚àà O
N O
obj(vi O
) O
, O
deÔ¨Åned O
as O
nodes O
accessible O
by O
all O
relation O
vertices O
vrsuch O
that{vi‚Üêvr O
, O
vr‚Üêvj}‚ààE O
. O
4 O
Model O
Setup O
In O
this O
section O
, O
we O
Ô¨Årst O
introduce O
image O
captioning O
models O
that O
do O
not O
explicitly O
use O
relation O
features O
( O
Section O
4.1 O
) O
and O
contrast O
them O
with O
those O
that O
use O
explicit O
relation O
features O
( O
Section O
4.2 O
) O
. O
4.1 O
Baseline O
Image O
Captioning O
( O
IC O
) O
Bottom O
- O
Up O
Top O
- O
Down O
( O
BUTD O
) O
The O
bottom O
- O
up O
top O
- O
down O
( O
BUTD O
; O
Anderson O
et O
al O
. O
, O
2018 O
) O
model O
4This O
is O
generally O
true O
for O
standard O
GNN O
architectures O
and O
not O
just O
GATs.consists O
of O
a O
Faster O
R O
- O
CNN O
image O
encoder O
( O
Ren O
et O
al O
. O
, O
2015 O
) O
that O
computes O
object O
proposal O
features O
for O
an O
input O
image O
, O
and O
a O
2 O
- O
layer O
LSTM O
language O
model O
decoder O
with O
a O
MLP O
attention O
mechanism O
over O
the O
object O
features O
that O
generates O
a O
caption O
for O
the O
image O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
; O
Bahdanau O
et O
al O
. O
, O
2014 O
) O
. O
We O
denote O
the O
set O
of O
object O
features O
X‚ààRn√ód O
, O
where O
nis O
the O
number O
of O
objects O
in O
the O
image O
and O
dthe O
features O
dimensionality O
. O
The O
2 O
- O
layer O
LSTM O
is O
designed O
so O
that O
the O
Ô¨Årst O
layer O
is O
used O
to O
compute O
an O
attention O
over O
the O
image O
features O
and O
the O
second O
layer O
is O
used O
to O
generate O
the O
captions O
‚Äô O
tokens O
. O
LSTM O
states O
at O
time O
steptare O
denoted O
as O
h(t O
) O
1andh(t O
) O
2for O
layer O
1 O
and O
2 O
, O
respectively O
. O
The O
hidden O
state O
of O
LSTM O
1is O
used O
to O
derive O
an O
attention O
over O
image O
features O
: O
x(t)=Att(X O
, O
h(t O
) O
1 O
) O
, O
( O
1 O
) O
where O
x(t)‚ààRdis O
the O
output O
of O
the O
attention O
layer O
and O
denotes O
the O
image O
features O
used O
at O
time O
step O
t. O
Update O
rules O
for O
each O
LSTM O
layer O
are O
deÔ¨Åned O
by O
: O
h(t O
) O
1 O
= O
LSTM O
1([h(t‚àí1 O
) O
2;w(t‚àí1);¬ØX],h(t‚àí1 O
) O
1 O
) O
, O
( O
2 O
) O
h(t O
) O
2 O
= O
LSTM O
2([h(t O
) O
1;x(t)],h(t‚àí1 O
) O
2 O
) O
, O
( O
3 O
) O
where O
w(t‚àí1)is O
the O
embedding O
of O
the O
previously O
generated O
word O
, O
and O
¬ØX‚ààRdare O
the O
mean O
image O
features O
. O
Next O
word O
probabilities O
are O
computed O
using O
a O
softmax O
over O
the O
vocabulary O
and O
parameterised O
by O
a O
linear O
projection O
of O
the O
hidden O
state O
of O
LSTM O
2 O
: O
p(w(t)=k|w(1):(t‚àí1))‚àùexp(W O
h(t O
) O
2 O
) O
. O
4.2 O
Relation O
- O
aware O
Image O
Captioning O
( O
RIC O
) O
We O
now O
describe O
models O
that O
incorporate O
explicit O
relation O
information O
into O
image O
captioning O
by O
using O
scene O
graphs O
as O
additional O
inputs O
. O
We O
use O
the O
pretrained O
Iterative O
Message O
Passing O
model O
with O
a O
relation O
proposal O
network O
( O
Xu O
et O
al O
. O
, O
2017 O
; O
Yang O
et O
al O
. O
, O
2018 O
) O
to O
obtain O
scene O
graph O
features O
for O
all O
images O
. O
Scene O
graph O
features O
for O
an O
image O
are O
denoted O
Y‚ààR(o+r)√ók O
, O
where O
ois O
the O
number O
of O
objects O
, O
rthe O
number O
of O
relations O
between O
objects O
, O
and O
kis O
the O
object O
/ O
relation O
feature O
dimensionality O
. O
We O
follow O
Wang O
et O
al O
. O
( O
2019 O
) O
who O
have O
found O
that O
only O
using O
scene O
graph O
features O
led O
to O
poor O
results O
compared O
to O
using O
Faster O
R O
- O
CNN O
features O
only O
. O
Therefore O
, O
we O
propose O
to O
integrate O
scene507graph O
features O
Yand O
Faster O
R O
- O
CNN O
object O
features O
Xby O
experimenting O
with O
( O
i)using O
Ydirectly O
, O
applying O
a O
GAT O
layer O
on O
Y O
, O
or O
applying O
a O
C O
- O
GAT O
layer O
on O
Yprior O
to O
feeding O
scene O
graph O
features O
into O
the O
decoder O
, O
and O
( O
ii)using O
a O
Ô¨Çat O
attention O
mechanism O
versus O
a O
hierarchical O
attention O
mechanism O
. O
GAT O
over O
Scene O
Graphs O
We O
propose O
a O
model O
that O
encodes O
the O
scene O
graph O
features O
Ywith O
a O
standard O
GAT O
layer O
prior O
to O
using O
them O
in O
LSTM O
2 O
in O
the O
decoder O
. O
C O
- O
GAT O
over O
Scene O
Graphs O
In O
this O
setup O
, O
we O
apply O
a O
C O
- O
GAT O
layer O
on O
scene O
graph O
features O
Y O
using O
the O
current O
decoder O
state O
h(t O
) O
1from O
LSTM O
1 O
as O
the O
external O
context O
, O
and O
use O
the O
output O
of O
the O
C O
- O
GAT O
layer O
in O
LSTM O
2 O
in O
the O
decoder O
. O
Flat O
Attention O
The O
Ô¨Çat O
attention O
( O
FA O
) O
consists O
of O
two O
separate O
attention O
heads O
, O
one O
over O
scene O
graph O
features O
Yand O
the O
other O
over O
Faster O
R O
- O
CNN O
features O
X. O
We O
use O
two O
standard O
MLP O
attention O
mechanisms O
( O
Bahdanau O
et O
al O
. O
, O
2014 O
) O
, O
each O
using O
the O
hidden O
state O
from O
LSTM O
1as O
the O
query O
: O
x(t)=Attx(X O
, O
h(t O
) O
1 O
) O
, O
y(t)=Atty(Y O
, O
h(t O
) O
1 O
) O
. O
Each O
LSTM O
layer O
is O
now O
deÔ¨Åned O
as O
follows O
: O
h(t O
) O
1 O
= O
LSTM O
1([h(t‚àí1 O
) O
2;w(t‚àí1);¬ØX;¬ØY],h(t‚àí1 O
) O
1 O
) O
, O
h(t O
) O
2 O
= O
LSTM O
2([h(t O
) O
1;x(t);y(t)],h(t‚àí1 O
) O
2),(4 O
) O
where O
x(t)andy(t)are O
computed O
by O
the O
two O
attention O
heads O
AttxandAtty O
, O
respectively O
, O
and O
¬ØY O
denote O
the O
mean O
scene O
graph O
features O
. O
Hierarchical O
Attention O
In O
a O
hierarchical O
attention O
( O
HA O
) O
mechanism O
the O
output O
of O
the O
Ô¨Årst O
attention O
head O
is O
used O
as O
input O
to O
derive O
the O
attention O
of O
the O
second O
head O
. O
We O
again O
have O
two O
sets O
of O
inputs O
, O
scene O
graph O
features O
Yand O
Faster O
R O
- O
CNN O
object O
features O
X. O
We O
experiment O
Ô¨Årst O
using O
Yas O
input O
to O
the O
Ô¨Årst O
head O
, O
and O
its O
output O
y(t)as O
additional O
input O
to O
the O
second O
head O
: O
y(t)=Atty(Y O
, O
h(t O
) O
1 O
) O
, O
x(t)=Attx(X,[h(t O
) O
1;y(t O
) O
] O
) O
. O
( O
5 O
) O
This O
setup O
is O
similar O
to O
the O
cascade O
attention O
from O
Wang O
et O
al O
. O
( O
2019 O
) O
. O
We O
also O
try O
using O
Xas O
input O
to O
the O
Ô¨Årst O
head O
, O
and O
the O
Ô¨Årst O
head O
‚Äôs O
output O
x(t)as O
additional O
input O
to O
the O
second O
head O
: O
x(t)=Attx(X O
, O
h(t O
) O
1 O
) O
. O
y(t)=Atty(Y,[h(t O
) O
1;x(t O
) O
] O
) O
. O
( O
6 O
) O
In O
both O
cases O
, O
the O
hidden O
states O
for O
LSTM O
1and O
LSTM O
2are O
computed O
as O
in O
Equation O
4 O
. O
5 O
Experimental O
Setup O
and O
Results O
We O
compare O
our O
models O
with O
the O
following O
external O
baselines O
with O
no O
access O
to O
scene O
graphs O
: O
( O
1 O
) O
the O
adaptive O
attention O
model O
Add O
- O
Att O
which O
determines O
at O
each O
decoder O
time O
step O
how O
much O
of O
the O
visual O
features O
should O
be O
used O
( O
Lu O
et O
al O
. O
, O
2017 O
) O
; O
( O
2 O
) O
the O
Neural O
Baby O
Talk O
model O
NBT O
generates O
a O
sentence O
with O
gaps O
and O
Ô¨Ålls O
the O
gaps O
using O
detected O
object O
labels O
( O
Lu O
et O
al O
. O
, O
2018 O
) O
; O
( O
3 O
) O
and O
the O
BUTD O
model O
( O
Anderson O
et O
al O
. O
, O
2018 O
) O
described O
in O
Section O
4.1 O
. O
We O
also O
compare O
with O
the O
following O
baselines O
that O
use O
scene O
graphs O
: O
( O
1 O
) O
The O
‚Äú O
Know O
more O
, O
say O
less O
‚Äù O
model O
KMSL O
extracts O
features O
for O
objects O
and O
relations O
based O
on O
the O
scene O
graph O
, O
which O
are O
passed O
through O
two O
attention O
heads O
and O
Ô¨Ånally O
combined O
using O
a O
Ô¨Çat O
attention O
head O
( O
Li O
and O
Jiang O
, O
2019 O
) O
; O
and O
( O
2 O
) O
the O
Cascade O
model O
( O
Wang O
et O
al O
. O
, O
2019 O
) O
which O
is O
similar O
to O
our O
hierarchical O
attention O
model O
with O
a O
GAT O
layer O
, O
but O
that O
instead O
uses O
a O
relational O
graph O
convolutional O
network O
( O
Marcheggiani O
et O
al O
. O
, O
2017 O
) O
. O
We O
do O
not O
discuss O
model O
variants O
/ O
results O
that O
are O
trained O
with O
an O
additional O
reinforcement O
learning O
step O
( O
Rennie O
et O
al O
. O
, O
2017 O
; O
Yang O
et O
al O
. O
, O
2019 O
) O
and O
only O
compare O
single O
model O
results O
, O
since O
training O
and O
performing O
inference O
with O
such O
models O
is O
very O
costly O
and O
orthogonal O
to O
our O
research O
questions O
. O
Our O
proposed O
models O
are O
: O
Ô¨Çat O
attention O
( O
FA O
) O
, O
hierarchical O
attention O
with O
scene O
graph O
Ô¨Årst O
( O
HA O
- O
SG O
) O
following O
Equation O
5 O
, O
hierarchical O
attention O
with O
objects O
detected O
Ô¨Årst O
( O
HA O
- O
IM O
) O
following O
Equation O
6 O
, O
HA O
- O
SG O
with O
graph O
attention O
network O
( O
HASG+GAT O
) O
, O
and O
HA O
- O
SG O
with O
conditional O
graph O
attention O
( O
HA O
- O
SG+C O
- O
GAT O
) O
. O
We O
choose O
the O
last O
two O
variants O
to O
extend O
HA O
- O
SG O
following O
the O
setup O
used O
by O
( O
Wang O
et O
al O
. O
, O
2019 O
) O
. O
We O
evaluate O
captions O
generated O
by O
different O
models O
by O
investigating O
their O
SPICE O
scores O
( O
Anderson O
et O
al O
. O
, O
2016 O
) O
, O
i.e. O
an O
F1 O
based O
semantic O
captioning O
evaluation O
metric O
computed O
over O
scene O
graphs O
. O
It O
uses O
the O
semantic O
structure O
of O
the O
scene O
graph O
to508B4 O
C O
R O
S O
Add O
- O
Att‚àó‚Ä†33.2 O
108.5 O
‚Äî O
‚Äî O
NBT‚Ä†34.7 O
107.2 O
‚Äî O
20.1 O
BUTD‚Ä†36.2 O
113.5 O
56.4 O
20.3 O
BUTD O
34.8 O
109.2 O
55.7 O
20.0 O
Cascade‚Ä†34.1 O
108.6 O
55.9 O
20.3 O
KMSL‚Ä†33.8 O
110.3 O
54.9 O
19.8 O
FA O
33.7 O
102.5 O
54.7 O
18.8 O
HA O
- O
IM O
35.7 O
109.9 O
55.9 O
19.9 O
HA O
- O
SG O
35.0 O
109.1 O
55.7 O
19.8 O
+ O
GAT O
34.7 O
106.4 O
55.4 O
19.4 O
+ O
C O
- O
GAT O
35.5 O
109.9 O
56.0 O
19.8 O
Table O
1 O
: O
Results O
on O
the O
MSCOCO O
test O
set O
, O
with O
models O
selected O
on O
the O
validation O
set O
( O
karpathy O
splits O
) O
. O
Models O
in O
the O
upper O
section O
do O
not O
use O
scene O
graphs O
, O
while O
those O
in O
the O
bottom O
section O
do O
. O
All O
models O
are O
trained O
to O
convergence O
for O
a O
maximum O
of O
50 O
epochs O
. O
Metrics O
reported O
are O
: O
BLEU-4 O
( O
B4 O
) O
, O
CIDEr O
( O
C O
) O
, O
ROUGE O
- O
L O
( O
R O
) O
, O
and O
SPICE O
( O
S O
) O
. O
See O
Section O
5 O
for O
details O
on O
all O
models O
and O
acronyms O
. O
We O
bold O
- O
face O
the O
best O
and O
underscore O
the O
second O
- O
best O
scores O
per O
metric O
( O
models O
that O
use O
scene O
graph).‚àóModel O
uses O
features O
from O
last O
convolutional O
layer O
in O
CNN O
, O
i.e. O
no O
Faster O
R O
- O
CNN O
features O
. O
‚Ä†Results O
reported O
in O
the O
authors O
‚Äô O
original O
papers O
. O
compute O
scores O
over O
several O
dimensions O
( O
object O
, O
relation O
, O
attribute O
, O
colour O
, O
count O
, O
and O
size O
) O
. O
We O
use O
the O
MSCOCO O
karpathy O
split O
( O
Lin O
et O
al O
. O
, O
2014 O
; O
Karpathy O
and O
Fei O
- O
Fei O
, O
2015 O
) O
which O
has O
5k O
images O
each O
in O
validation O
and O
test O
sets O
, O
and O
we O
use O
the O
remaining O
113k O
images O
for O
training O
. O
We O
build O
a O
vocabulary O
based O
on O
all O
words O
in O
the O
train O
split O
that O
occur O
at O
least O
5 O
times O
. O
We O
use O
MSCOCO O
evaluation O
scripts O
( O
Lin O
et O
al O
. O
, O
2014 O
) O
and O
report O
BLEU4 O
( O
B4 O
; O
Papineni O
et O
al O
. O
, O
2002 O
) O
, O
CIDEr O
( O
C O
; O
Vedantam O
et O
al O
. O
, O
2015 O
) O
, O
ROUGE O
- O
L O
( O
R O
; O
Lin O
, O
2004 O
) O
, O
and O
SPICE O
( O
S O
; O
Anderson O
et O
al O
. O
, O
2016 O
) O
. O
See O
Appendix O
A O
for O
extra O
information O
on O
our O
implementation O
and O
training O
procedures O
. O
5.1 O
Image O
Captioning O
without O
Relational O
Features O
Our O
re O
- O
implementation O
of O
the O
BUTD O
baseline O
scores O
slightly O
worse O
compared O
to O
the O
results O
reported O
by O
Anderson O
et O
al O
. O
( O
2018 O
) O
. O
This O
difference O
can O
be O
attributed O
to O
the O
Faster O
R O
- O
CNN O
features O
used O
, O
i.e. O
we O
always O
use O
36 O
objects O
per O
image O
whereas O
Anderson O
et O
al O
. O
( O
2018 O
) O
use O
a O
variable O
number O
of O
objects O
per O
image O
( O
i.e. O
10 O
to O
100 O
) O
, O
and O
there O
are O
other O
smaller O
differences O
in O
their O
training O
procedure O
. O
Since O
all O
our O
models O
use O
these O
settings O
, O
in O
further O
experiments O
we O
compare O
to O
our O
implementation O
of O
the O
BUTD O
baseline O
. O
5.2 O
Image O
Captioning O
with O
relational O
features O
We O
notice O
that O
the O
KMSL O
model O
by O
Li O
and O
Jiang O
( O
2019 O
) O
slightly O
outperforms O
the O
other O
models O
according O
to O
CIDEr O
, O
while O
it O
performs O
worse O
in O
all O
other O
metrics O
. O
Li O
and O
Jiang O
( O
2019 O
) O
found O
performance O
increases O
when O
restricting O
the O
number O
of O
relations O
and O
report O
scores O
using O
this O
restriction O
, O
whereas O
we O
decided O
to O
use O
the O
full O
set O
of O
relations O
to O
test O
the O
effect O
of O
scene O
graph O
quality O
( O
see O
Section O
5.4 O
) O
. O
Furthermore O
, O
the O
features O
used O
in O
the O
KMSL O
model O
are O
not O
directly O
extracted O
from O
the O
SGG O
model O
as O
is O
the O
case O
for O
the O
other O
models O
, O
but O
an O
additional O
architecture O
is O
used O
for O
computing O
stronger O
features O
. O
Flat O
vs. O
Hierarchical O
attention O
According O
to O
Table O
1 O
, O
FA O
performs O
worse O
not O
only O
compared O
to O
HA O
models O
, O
but O
also O
compared O
to O
other O
baselines O
. O
The O
HA O
model O
using O
Faster O
R O
- O
CNN O
object O
features O
in O
the O
Ô¨Årst O
head O
, O
i.e. O
HA O
- O
IM O
setup O
, O
performs O
better O
than O
using O
the O
scene O
graph O
features O
Ô¨Årst O
, O
i.e. O
HA O
- O
SG O
setup O
. O
We O
hypothesise O
that O
this O
difference O
comes O
from O
the O
additional O
guidance O
from O
x(t)helping O
with O
a O
better O
attention O
selection O
over O
possibly O
more O
noisy O
features O
present O
in O
Y. O
Additional O
GNN O
updates O
Directly O
using O
a O
GAT O
layer O
over O
scene O
graph O
features O
negatively O
impacts O
model O
performance O
. O
Comparing O
these O
results O
to O
the O
related O
Cascade O
model O
from O
Wang O
et O
al O
. O
( O
2019 O
) O
, O
we O
hypothesise O
that O
the O
R O
- O
GCN O
architecture O
works O
better O
in O
this O
setting O
, O
although O
compared O
to O
other O
models O
it O
still O
has O
lower O
scores O
according O
to O
most O
metrics O
. O
The O
reason O
may O
be O
that O
the O
Cascade O
model O
by O
Wang O
et O
al O
. O
( O
2019 O
) O
was O
undertrained O
or O
could O
have O
used O
better O
hyperparameters O
, O
as O
indicated O
by O
our O
BUTD O
baseline O
performing O
comparably O
or O
better O
than O
their O
strongest O
model.5 O
Combining O
a O
C O
- O
GAT O
layer O
on O
the O
decoder O
improves O
overall O
results O
according O
to O
most O
metrics O
, O
though O
by O
a O
small O
margin O
. O
This O
suggests O
that O
using O
additional O
GNNs O
in O
the O
context O
of O
image O
captioning O
have O
a O
positive O
effect O
. O
Furthermore O
, O
graph O
features O
learned O
using O
C O
- O
GAT O
always O
outperform O
standard O
GAT O
, O
which O
coincides O
with O
our O
intuition O
that O
taking O
5Our O
BUTD O
baseline O
scores O
109.2 O
CIDEr O
, O
whereas O
their O
best O
model O
achieves O
108.6 O
CIDEr.509All O
Obj O
Rel O
BUTD O
19.8 O
36.0 O
5.2 O
FA O
18.5 O
34.7 O
5.0 O
HA O
- O
IM O
19.5 O
35.9 O
5.2 O
HA O
- O
SG O
19.5 O
35.9 O
5.3 O
+ O
GAT O
19.2 O
35.5 O
5.6 O
+ O
C O
- O
GAT O
19.4 O
35.8 O
5.3 O
Table O
2 O
: O
Breakdown O
of O
overall O
SPICE O
scores O
( O
All O
) O
into O
object O
( O
Obj O
) O
and O
relation O
( O
Rel O
) O
F1 O
scores O
. O
See O
Section O
5 O
for O
details O
on O
all O
models O
and O
acronyms O
. O
We O
bold O
- O
face O
the O
best O
overall O
scores O
and O
underline O
the O
best O
scores O
obtained O
by O
our O
models O
. O
the O
current O
decoder O
hidden O
context O
into O
consideration O
can O
improve O
graph O
features O
. O
5.3 O
SPICE O
breakdown O
In O
our O
analysis O
, O
in O
addition O
to O
the O
overall O
SPICE O
F1 O
score O
for O
an O
entire O
caption O
, O
we O
break O
it O
down O
into O
scores O
over O
objects O
and O
over O
relations.6This O
allows O
us O
to O
investigate O
how O
models O
are O
better O
or O
worse O
on O
describing O
objects O
and O
relations O
independently O
. O
These O
results O
, O
computed O
for O
the O
validation O
split O
, O
are O
shown O
in O
Table O
2 O
. O
When O
we O
look O
at O
individual O
scores O
for O
objects O
and O
relations O
, O
we O
notice O
a O
small O
and O
consistent O
gain O
in O
relation O
F1 O
by O
using O
scene O
graphs O
independently O
of O
the O
attention O
architecture O
or O
other O
design O
choices O
, O
but O
also O
observe O
lower O
object O
F1 O
scores O
with O
respect O
to O
the O
BUTD O
baseline O
. O
When O
object O
and O
relation O
scores O
are O
combined O
into O
a O
single O
F1 O
measure O
, O
it O
results O
in O
worse O
overall O
scores O
suggesting O
that O
the O
small O
increase O
in O
the O
relation O
scores O
is O
not O
sufÔ¨Åcient O
to O
have O
a O
positive O
impact O
on O
captioning O
insofar O
. O
5.4 O
Scene O
Graph O
Quality O
Since O
scene O
graph O
features O
are O
generated O
with O
a O
pretrained O
SGG O
model O
, O
we O
expect O
them O
to O
introduce O
a O
considerable O
amount O
of O
noise O
into O
the O
model O
. O
In O
this O
section O
, O
we O
investigate O
the O
effect O
that O
the O
quality O
of O
the O
scene O
graph O
has O
on O
the O
quality O
of O
captions O
. O
VG O
- O
COCO O
In O
this O
set O
of O
experiments O
, O
we O
need O
images O
with O
both O
captions O
and O
scene O
graph O
annotations O
. O
Thus O
, O
we O
use O
a O
subset O
of O
MSCOCO O
which O
overlaps O
with O
Visual O
Genome O
( O
Krishna O
et O
al O
. O
, O
2017 O
) O
, O
using O
captions O
from O
the O
former O
and O
scene O
graphs O
6The O
SPICE O
score O
also O
includes O
the O
components O
attribute O
, O
colour O
, O
count O
, O
and O
size O
, O
but O
we O
do O
not O
report O
them O
directly O
. O
Figure O
3 O
: O
Distribution O
of O
the O
scores O
for O
the O
scene O
graphs O
in O
the O
validation O
split O
of O
the O
VG O
- O
COCO O
dataset O
. O
from O
the O
latter O
. O
We O
refer O
to O
this O
dataset O
as O
VGCOCO O
, O
as O
similarly O
done O
by O
Li O
and O
Jiang O
( O
2019 O
) O
. O
We O
compute O
scores O
for O
each O
scene O
graph O
predicted O
by O
the O
Iterative O
Message O
Passing O
model O
using O
the O
common O
SGDet O
recall@100 O
as O
deÔ¨Åned O
by O
Yang O
et O
al O
. O
( O
2018 O
) O
. O
SGDet O
recall@100 O
is O
computed O
by O
using O
the O
100 O
highest O
scoring O
triplets O
among O
all O
triplets O
predicted O
by O
the O
model,7and O
reporting O
the O
percentage O
of O
gold O
- O
standard O
triplets O
. O
The O
distribution O
of O
scores O
across O
images O
( O
Figure O
3 O
) O
shows O
that O
most O
scene O
graphs O
have O
extremely O
low O
scores O
close O
to O
zero O
, O
thus O
containing O
a O
lot O
of O
noise O
. O
We O
separate O
images O
in O
the O
VG O
- O
COCO O
validation O
set O
in O
three O
groups O
: O
low O
( O
R O
<33 O
% O
) O
, O
average O
( O
33%‚â§R<67 O
% O
) O
, O
and O
high O
scoring O
graphs O
( O
67%‚â§R O
) O
, O
where O
R O
is O
SGDet O
recall@100 O
. O
For O
each O
set O
of O
images O
in O
each O
of O
these O
groups O
, O
we O
compute O
captioning O
metrics O
and O
also O
report O
a O
SPICE O
breakdown O
in O
Table O
3 O
. O
Effect O
of O
scene O
graph O
quality O
Due O
to O
the O
imbalance O
in O
scene O
graph O
quality O
, O
the O
low O
, O
average O
, O
and O
high O
quality O
subsets O
have O
around O
1000 O
, O
500 O
, O
and O
200 O
images O
, O
respectively O
. O
By O
reporting O
results O
for O
the O
BUTD O
baseline O
, O
we O
show O
the O
performance O
a O
strong O
baseline O
obtains O
on O
the O
same O
set O
of O
images O
. O
In O
Table O
3b O
, O
scores O
across O
all O
metrics O
are O
similar O
and O
only O
model O
FA O
performs O
clearly O
worse O
than O
others O
. O
Though O
the O
BUTD O
baseline O
never O
performs O
best O
, O
it O
is O
often O
not O
more O
than O
a O
point O
behind O
the O
best O
performing O
model O
( O
except O
for O
CIDEr O
where O
it O
is O
2.5 O
points O
lower O
compared O
to O
HA O
- O
IM O
) O
. O
When O
comparing O
Table O
3b O
to O
Table O
3c O
, O
we O
observe O
that O
all O
models O
tend O
to O
increase O
scores O
, O
and O
that O
BUTD O
tends O
to O
perform O
best O
overall O
. O
In O
Table O
3d O
, O
we O
see O
an O
increase O
in O
the O
difference O
between O
7A O
triplet O
is O
an O
object O
- O
predicate O
- O
subject O
phrase.510SPICE O
Captioning O
All O
Obj O
Rel O
B4 O
C O
R O
BUTD O
19.8 O
36.0 O
5.0 O
35.4 O
109.8 O
56.0 O
FA O
18.5 O
34.9 O
4.8 O
33.2 O
103.6 O
54.8 O
HA O
- O
IM O
19.6 O
36.0 O
5.2 O
35.0 O
110.8 O
55.7 O
HA O
- O
SG O
19.8 O
36.2 O
5.5 O
35.7 O
111.0 O
56.0 O
+ O
GAT O
19.4 O
35.5 O
5.6 O
35.0 O
108.3 O
55.6 O
+ O
C O
- O
GAT O
19.5 O
35.8 O
5.2 O
35.7 O
110.4 O
56.0 O
( O
a O
) O
Full O
VG O
- O
COCO O
datasetSPICE O
Captioning O
All O
Obj O
Rel O
B4 O
C O
R O
19.5 O
35.6 O
4.7 O
34.0 O
109.2 O
55.2 O
18.2 O
34.9 O
4.4 O
32.4 O
102.7 O
54.3 O
19.5 O
36.0 O
5.2 O
34.3 O
111.7 O
55.5 O
19.6 O
36.0 O
5.0 O
34.6 O
110.3 O
55.4 O
19.5 O
35.7 O
5.7 O
34.8 O
109.5 O
55.4 O
19.5 O
35.7 O
5.1 O
34.8 O
109.9 O
55.7 O
( O
b O
) O
Low O
VG O
- O
COCO O
dataset O
SPICE O
Captioning O
All O
Obj O
Rel O
B4 O
C O
R O
BUTD O
20.5 O
37.0 O
5.5 O
38.4 O
117.5 O
57.1 O
FA O
18.8 O
35.0 O
5.3 O
34.6 O
111.1 O
55.1 O
HA O
- O
IM O
19.8 O
36.2 O
5.3 O
36.2 O
115.1 O
55.3 O
HA O
- O
SG O
19.6 O
35.9 O
5.4 O
37.0 O
114.4 O
56.3 O
+ O
GAT O
19.4 O
35.8 O
5.7 O
36.2 O
112.7 O
56.0 O
+ O
C O
- O
GAT O
19.5 O
36.0 O
5.1 O
37.6 O
116.4 O
56.1 O
( O
c O
) O
Average O
VG O
- O
COCO O
datasetSPICE O
Captioning O
All O
Obj O
Rel O
B4 O
C O
R O
20.9 O
36.8 O
5.1 O
37.2 O
126.5 O
57.0 O
19.8 O
35.3 O
5.6 O
35.9 O
117.4 O
56.6 O
20.3 O
36.2 O
5.8 O
36.2 O
124.1 O
56.8 O
20.9 O
37.1 O
6.0 O
38.1 O
129.8 O
57.6 O
19.7 O
35.3 O
5.9 O
36.2 O
123.6 O
57.1 O
20.8 O
36.6 O
6.0 O
37.2 O
127.3 O
56.9 O
( O
d O
) O
High O
VG O
- O
COCO O
dataset O
Table O
3 O
: O
SPICE O
breakdown O
and O
captioning O
metrics O
for O
images O
in O
VG O
- O
COCO O
validation O
split O
. O
Results O
for O
the O
full O
VG O
- O
COCO O
, O
and O
for O
subsets O
of O
images O
collected O
according O
to O
the O
quality O
of O
their O
corresponding O
predicted O
scene O
graphs O
: O
low O
, O
average O
, O
and O
high O
. O
See O
Sections O
5 O
for O
details O
on O
all O
models O
and O
acronyms O
. O
Metrics O
reported O
are O
: O
overall O
SPICE O
F1 O
score O
( O
All O
) O
, O
object O
( O
Obj O
) O
and O
relation O
( O
Rel O
) O
F1 O
score O
components O
, O
BLEU-4 O
( O
B4 O
) O
, O
CIDEr O
( O
C O
) O
, O
and O
ROUGE O
- O
L O
( O
R O
) O
. O
We O
bold O
- O
face O
the O
best O
and O
underline O
the O
second O
- O
best O
overall O
scores O
per O
metric O
and O
per O
data O
subset O
. O
the O
baseline O
and O
our O
best O
models O
according O
to O
all O
metrics O
. O
All O
these O
gains O
are O
very O
promising O
and O
suggest O
that O
when O
we O
have O
high O
quality O
scene O
graphs O
, O
we O
can O
expect O
a O
consistent O
positive O
transfer O
into O
image O
captioning O
models O
. O
However O
, O
the O
overall O
SPICE O
score O
is O
the O
highest O
for O
both O
BUTD O
and O
HA O
- O
SG O
, O
while O
BUTD O
has O
lower O
scores O
for O
objects O
and O
relations O
F O
- O
measure O
. O
That O
suggests O
that O
other O
components O
part O
of O
SPICE O
were O
worsened O
with O
the O
addition O
of O
scene O
graphs O
. O
Since O
this O
is O
not O
the O
focus O
of O
this O
paper O
, O
we O
did O
not O
investigate O
this O
further O
and O
leave O
that O
for O
future O
work O
. O
Overall O
, O
these O
results O
show O
that O
indiscriminately O
using O
scene O
graphs O
from O
pretrained O
SGG O
models O
downstream O
on O
image O
captioning O
can O
be O
harmful O
because O
of O
the O
amount O
of O
noise O
present O
in O
these O
scene O
graphs O
. O
However O
, O
when O
this O
noise O
is O
smaller O
and O
the O
scene O
graphs O
of O
higher O
quality O
, O
our O
Ô¨Åndings O
together O
suggest O
that O
scene O
graphs O
can O
be O
useful O
in O
image O
captioning O
models O
. O
Ground O
- O
truth O
graphs O
Finally O
, O
we O
also O
conduct O
a O
small O
- O
scale O
experiment O
using O
ground O
- O
truth O
scenegraphs O
and O
evaluate O
how O
using O
these O
instead O
of O
predicted O
scene O
graphs O
at O
inference O
time O
impacts O
models O
, O
which O
can O
be O
found O
in O
Appendix O
B. O
Qualitative O
Results O
Here O
, O
we O
try O
to O
determine O
if O
there O
is O
a O
clear O
difference O
in O
the O
difÔ¨Åculty O
in O
captioning O
images O
in O
low O
, O
average O
, O
and O
high O
quality O
sets O
, O
which O
might O
help O
explain O
the O
result O
in O
Table O
3 O
. O
In O
Figures O
4 O
and O
5 O
we O
show O
some O
images O
for O
the O
low O
and O
high O
scoring O
graphs O
, O
respectively O
. O
At O
a O
Ô¨Årst O
glance O
, O
images O
from O
both O
sets O
appear O
equally O
cluttered O
with O
objects O
( O
i.e. O
, O
which O
we O
hypothesise O
should O
correlate O
with O
the O
image O
being O
harder O
to O
describe O
) O
. O
Furthermore O
, O
for O
both O
low O
and O
high O
scoring O
scene O
graphs O
, O
the O
average O
number O
of O
objects O
and O
relations O
is O
23 O
and O
22 O
respectively O
. O
However O
, O
we O
note O
that O
even O
scene O
graphs O
in O
the O
high O
quality O
set O
often O
include O
tiny O
objects O
and O
details O
, O
e.g. O
the O
image O
in O
the O
right O
of O
Figure O
4 O
shows O
a O
single O
aircraft O
, O
but O
there O
are O
17 O
annotated O
objects O
describing O
components O
such O
as O
wings O
, O
windows O
, O
etc.511Figure O
4 O
: O
Images O
, O
captions O
and O
ground O
- O
truth O
number O
of O
objects O
and O
relations O
for O
high O
scoring O
scene O
graph O
. O
Figure O
5 O
: O
Images O
, O
captions O
and O
ground O
- O
truth O
number O
of O
objects O
and O
relations O
for O
lowscoring O
scene O
graph O
. O
6 O
Conclusions O
and O
Future O
Work O
In O
this O
work O
, O
we O
investigate O
the O
impact O
scene O
graphs O
have O
on O
image O
captioning O
. O
We O
introduced O
conditional O
graph O
attention O
( O
C O
- O
GAT O
) O
networks O
and O
applied O
it O
to O
image O
captioning O
, O
and O
report O
promising O
results O
( O
Table O
1 O
) O
. O
Overall O
, O
we O
found O
that O
improvements O
in O
captioning O
when O
using O
scene O
graphs O
generated O
with O
publicly O
available O
SGG O
models O
are O
minor O
. O
We O
observe O
a O
very O
small O
increase O
in O
the O
ability O
to O
describe O
relations O
as O
measured O
by O
relation O
SPICE O
F O
- O
scores O
, O
however O
, O
this O
is O
associated O
with O
models O
producing O
worse O
overall O
descriptions O
and O
producing O
lower O
object O
SPICE O
F O
- O
scores O
. O
In O
an O
in O
- O
depth O
analysis O
, O
we O
found O
that O
the O
predicted O
scene O
graphs O
contain O
a O
large O
amount O
of O
noise O
which O
harms O
the O
captioning O
process O
. O
When O
this O
noise O
is O
reduced O
, O
large O
gains O
can O
be O
achieved O
across O
all O
image O
captioning O
metrics O
, O
e.g. O
3.3 O
CIDEr O
points O
in O
the O
high O
VG O
- O
COCO O
split O
( O
Table O
3d O
) O
. O
This O
indicates O
that O
with O
further O
research O
and O
improved O
scene O
graph O
generation O
models O
, O
we O
will O
likely O
be O
able O
to O
observe O
consistent O
gains O
in O
image O
captioning O
and O
possibly O
other O
tasks O
by O
leveraging O
silver O
- O
standard O
scene O
graphs O
. O
Future O
work O
In O
further O
research O
, O
we O
will O
conduct O
an O
in O
- O
depth O
analysis O
of O
our O
proposed O
condi O
- O
tional O
graph O
attention O
to O
determine O
what O
tasks O
other O
than O
image O
captioning O
we O
can O
apply O
it O
to O
. O
We O
envision O
using O
it O
for O
visual O
question O
- O
answering O
also O
with O
generated O
scene O
graphs O
, O
and O
on O
syntax O
- O
aware O
neural O
machine O
translation O
( O
Bastings O
et O
al O
. O
, O
2017 O
) O
, O
fake O
news O
detection O
( O
Monti O
et O
al O
. O
, O
2019 O
) O
, O
and O
question O
answering O
( O
Zhang O
et O
al O
. O
, O
2018 O
) O
. O
In O
a O
focused O
qualitative O
analysis O
, O
we O
found O
that O
the O
scene O
graphs O
represent O
objects O
and O
relations O
in O
images O
sometimes O
with O
great O
detail O
. O
We O
plan O
to O
investigate O
how O
to O
account O
for O
such O
highly O
detailed O
objects O
/ O
relations O
in O
the O
context O
of O
image O
captioning O
. O
Finally O
, O
we O
will O
look O
into O
a O
method O
to O
use O
predicted O
scene O
graphs O
selectively O
according O
to O
their O
estimated O
quality O
, O
possibly O
selecting O
the O
best O
graph O
between O
those O
generated O
by O
different O
SGG O
models O
. O
Acknowledgments O
We O
would O
like O
to O
thank O
the O
COST O
Action O
CA18231 O
for O
funding O
a O
research O
visit O
to O
collaborate O
on O
this O
project O
. O
This O
work O
is O
funded O
by O
the O
European O
Research O
Council O
( O
ERC O
) O
under O
the O
ERC O
Advanced O
Grant O
788506 O
. O
IC O
has O
received O
funding O
from O
the O
European O
Union O
‚Äôs O
Horizon O
2020 O
research O
and O
innovation O
program O
under O
the O
Marie O
Sk≈Çodowska O
- O
Curie O
grant O
agreement O
No O
838188.512References O
Peter O
Anderson O
, O
Basura O
Fernando O
, O
Mark O
Johnson O
, O
and O
Stephen O
Gould O
. O
2016 O
. O
Spice O
: O
Semantic O
propositional O
image O
caption O
evaluation O
. O
In O
Computer O
Vision O
‚Äì O
ECCV O
2016 O
, O
pages O
382‚Äì398 O
, O
Cham O
. O
Springer O
International O
Publishing O
. O
Peter O
Anderson O
, O
Xiaodong O
He O
, O
Chris O
Buehler O
, O
Damien O
Teney O
, O
Mark O
Johnson O
, O
Stephen O
Gould O
, O
and O
Lei O
Zhang O
. O
2018 O
. O
Bottom O
- O
up O
and O
top O
- O
down O
attention O
for O
image O
captioning O
and O
visual O
question O
answering O
. O
In O
The O
IEEE O
Conference O
on O
Computer O
Vision O
and O
Pattern O
Recognition O
( O
CVPR O
) O
. O
Dzmitry O
Bahdanau O
, O
Kyunghyun O
Cho O
, O
and O
Yoshua O
Bengio O
. O
2014 O
. O
Neural O
machine O
translation O
by O
jointly O
learning O
to O
align O
and O
translate O
. O
arXiv O
preprint O
arXiv:1409.0473 O
. O
Joost O
Bastings O
, O
Ivan O
Titov O
, O
Wilker O
Aziz O
, O
Diego O
Marcheggiani O
, O
and O
Khalil O
Sima‚Äôan O
. O
2017 O
. O
Graph O
convolutional O
encoders O
for O
syntax O
- O
aware O
neural O
machine O
translation O
. O
In O
Proceedings O
of O
the O
2017 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
1957‚Äì1967 O
, O
Copenhagen O
, O
Denmark O
. O
Association O
for O
Computational O
Linguistics O
. O
Peter O
W O
Battaglia O
, O
Jessica O
B O
Hamrick O
, O
Victor O
Bapst O
, O
Alvaro O
Sanchez O
- O
Gonzalez O
, O
Vinicius O
Zambaldi O
, O
Mateusz O
Malinowski O
, O
Andrea O
Tacchetti O
, O
David O
Raposo O
, O
Adam O
Santoro O
, O
Ryan O
Faulkner O
, O
et O
al O
. O
2018 O
. O
Relational O
inductive O
biases O
, O
deep O
learning O
, O
and O
graph O
networks O
. O
arXiv O
preprint O
arXiv:1806.01261 O
. O
Ross O
Girshick O
. O
2015 O
. O
Fast O
R O
- O
CNN O
. O
In O
Proceedings O
of O
the O
2015 O
IEEE O
International O
Conference O
on O
Computer O
Vision O
( O
ICCV O
) O
, O
ICCV O
‚Äô O
15 O
, O
page O
1440‚Äì1448 O
, O
USA O
. O
IEEE O
Computer O
Society O
. O
Ross O
Girshick O
, O
Jeff O
Donahue O
, O
Trevor O
Darrell O
, O
and O
Jitendra O
Malik O
. O
2014 O
. O
Rich O
feature O
hierarchies O
for O
accurate O
object O
detection O
and O
semantic O
segmentation O
. O
In O
Proceedings O
of O
the O
2014 O
IEEE O
Conference O
on O
Computer O
Vision O
and O
Pattern O
Recognition O
, O
CVPR O
‚Äô O
14 O
, O
page O
580‚Äì587 O
, O
USA O
. O
IEEE O
Computer O
Society O
. O
Kaiming O
He O
, O
Xiangyu O
Zhang O
, O
Shaoqing O
Ren O
, O
and O
Jian O
Sun O
. O
2016 O
. O
Deep O
residual O
learning O
for O
image O
recognition O
. O
In O
Proceedings O
of O
the O
IEEE O
Conference O
on O
Computer O
Vision O
and O
Pattern O
Recognition O
, O
pages O
770‚Äì778 O
. O
Sepp O
Hochreiter O
and O
J O
¬®urgen O
Schmidhuber O
. O
1997 O
. O
Long O
short O
- O
term O
memory O
. O
Neural O
computation O
, O
9(8):1735‚Äì1780 O
. O
Justin O
Johnson O
, O
Agrim O
Gupta O
, O
and O
Li O
Fei O
- O
Fei O
. O
2018 O
. O
Image O
generation O
from O
scene O
graphs O
. O
In O
Proceedings O
of O
the O
IEEE O
Conference O
on O
Computer O
Vision O
and O
Pattern O
Recognition O
, O
pages O
1219‚Äì1228 O
. O
Justin O
Johnson O
, O
Bharath O
Hariharan O
, O
Laurens O
van O
der O
Maaten O
, O
Li O
Fei O
- O
Fei O
, O
C. O
Lawrence O
Zitnick O
, O
and O
Ross O
Girshick O
. O
2017 O
. O
CLEVR O
: O
A O
Diagnostic O
Dataset O
for O
Compositional O
Language O
and O
Elementary O
VisualReasoning O
. O
In O
The O
IEEE O
Conference O
on O
Computer O
Vision O
and O
Pattern O
Recognition O
( O
CVPR O
) O
. O
Justin O
Johnson O
, O
Ranjay O
Krishna O
, O
Michael O
Stark O
, O
Li O
- O
Jia O
Li O
, O
David O
Shamma O
, O
Michael O
Bernstein O
, O
and O
Li O
FeiFei O
. O
2015 O
. O
Image O
retrieval O
using O
scene O
graphs O
. O
In O
Proceedings O
of O
the O
IEEE O
conference O
on O
computer O
vision O
and O
pattern O
recognition O
, O
pages O
3668‚Äì3678 O
. O
Andrej O
Karpathy O
and O
Li O
Fei O
- O
Fei O
. O
2015 O
. O
Deep O
visualsemantic O
alignments O
for O
generating O
image O
descriptions O
. O
In O
Proceedings O
of O
the O
IEEE O
conference O
on O
computer O
vision O
and O
pattern O
recognition O
, O
pages O
3128‚Äì3137 O
. O
Diederik O
P O
Kingma O
and O
Jimmy O
Ba O
. O
2014 O
. O
Adam O
: O
A O
method O
for O
stochastic O
optimization O
. O
arXiv O
preprint O
arXiv:1412.6980 O
. O
Boris O
Knyazev O
, O
Harm O
de O
Vries O
, O
C O
ÀòatÀòalina O
Cangea O
, O
Graham O
W O
Taylor O
, O
Aaron O
Courville O
, O
and O
Eugene O
Belilovsky O
. O
2020 O
. O
Graph O
density O
- O
aware O
losses O
for O
novel O
compositions O
in O
scene O
graph O
generation O
. O
arXiv O
preprint O
arXiv:2005.08230 O
. O
Ranjay O
Krishna O
, O
Yuke O
Zhu O
, O
Oliver O
Groth O
, O
Justin O
Johnson O
, O
Kenji O
Hata O
, O
Joshua O
Kravitz O
, O
Stephanie O
Chen O
, O
Yannis O
Kalantidis O
, O
Li O
- O
Jia O
Li O
, O
David O
A. O
Shamma O
, O
and O
et O
al O
. O
2017 O
. O
Visual O
genome O
: O
Connecting O
language O
and O
vision O
using O
crowdsourced O
dense O
image O
annotations O
. O
International O
Journal O
of O
Computer O
Vision O
, O
123(1):32‚Äì73 O
. O
Guang O
Li O
, O
Linchao O
Zhu O
, O
Ping O
Liu O
, O
and O
Yi O
Yang O
. O
2019 O
. O
Entangled O
transformer O
for O
image O
captioning O
. O
In O
Proceedings O
of O
the O
IEEE O
/ O
CVF O
International O
Conference O
on O
Computer O
Vision O
( O
ICCV O
) O
. O
X. O
Li O
and O
S. O
Jiang O
. O
2019 O
. O
Know O
more O
say O
less O
: O
Image O
captioning O
based O
on O
scene O
graphs O
. O
IEEE O
Transactions O
on O
Multimedia O
, O
21(8):2117‚Äì2130 O
. O
Yikang O
Li O
, O
Wanli O
Ouyang O
, O
Bolei O
Zhou O
, O
Jianping O
Shi O
, O
Chao O
Zhang O
, O
and O
Xiaogang O
Wang O
. O
2018 O
. O
Factorizable O
net O
: O
An O
efÔ¨Åcient O
subgraph O
- O
based O
framework O
for O
scene O
graph O
generation O
. O
In O
Proceedings O
of O
the O
European O
Conference O
on O
Computer O
Vision O
( O
ECCV O
) O
, O
pages O
335‚Äì351 O
. O
Chin O
- O
Yew O
Lin O
. O
2004 O
. O
ROUGE O
: O
A O
package O
for O
automatic O
evaluation O
of O
summaries O
. O
In O
Text O
Summarization O
Branches O
Out O
, O
pages O
74‚Äì81 O
, O
Barcelona O
, O
Spain O
. O
Association O
for O
Computational O
Linguistics O
. O
Tsung O
- O
Yi O
Lin O
, O
Michael O
Maire O
, O
Serge O
Belongie O
, O
James O
Hays O
, O
Pietro O
Perona O
, O
Deva O
Ramanan O
, O
Piotr O
Doll O
¬¥ O
ar O
, O
and O
C O
Lawrence O
Zitnick O
. O
2014 O
. O
Microsoft O
coco O
: O
Common O
objects O
in O
context O
. O
In O
European O
conference O
on O
computer O
vision O
, O
pages O
740‚Äì755 O
. O
Springer O
. O
Jiasen O
Lu O
, O
Caiming O
Xiong O
, O
Devi O
Parikh O
, O
and O
Richard O
Socher O
. O
2017 O
. O
Knowing O
when O
to O
look O
: O
Adaptive O
attention O
via O
a O
visual O
sentinel O
for O
image O
captioning O
. O
In O
The O
IEEE O
Conference O
on O
Computer O
Vision O
and O
Pattern O
Recognition O
( O
CVPR O
) O
.513Jiasen O
Lu O
, O
Jianwei O
Yang O
, O
Dhruv O
Batra O
, O
and O
Devi O
Parikh O
. O
2018 O
. O
Neural O
baby O
talk O
. O
In O
Proceedings O
of O
the O
IEEE O
conference O
on O
computer O
vision O
and O
pattern O
recognition O
, O
pages O
7219‚Äì7228 O
. O
Diego O
Marcheggiani O
, O
Anton O
Frolov O
, O
and O
Ivan O
Titov O
. O
2017 O
. O
A O
simple O
and O
accurate O
syntax O
- O
agnostic O
neural O
model O
for O
dependency O
- O
based O
semantic O
role O
labeling O
. O
InProceedings O
of O
the O
21st O
Conference O
on O
Computational O
Natural O
Language O
Learning O
( O
CoNLL O
2017 O
) O
, O
pages O
411‚Äì420 O
, O
Vancouver O
, O
Canada O
. O
Association O
for O
Computational O
Linguistics O
. O
Federico O
Monti O
, O
Fabrizio O
Frasca O
, O
Davide O
Eynard O
, O
Damon O
Mannion O
, O
and O
Michael O
M O
Bronstein O
. O
2019 O
. O
Fake O
news O
detection O
on O
social O
media O
using O
geometric O
deep O
learning O
. O
arXiv O
preprint O
arXiv:1902.06673 O
. O
Kishore O
Papineni O
, O
Salim O
Roukos O
, O
Todd O
Ward O
, O
and O
WeiJing O
Zhu O
. O
2002 O
. O
Bleu O
: O
A O
method O
for O
automatic O
evaluation O
of O
machine O
translation O
. O
In O
Proceedings O
of O
the O
40th O
Annual O
Meeting O
on O
Association O
for O
Computational O
Linguistics O
, O
ACL O
‚Äô O
02 O
, O
pages O
311‚Äì318 O
, O
Stroudsburg O
, O
PA O
, O
USA O
. O
Association O
for O
Computational O
Linguistics O
. O
Shaoqing O
Ren O
, O
Kaiming O
He O
, O
Ross O
Girshick O
, O
and O
Jian O
Sun O
. O
2015 O
. O
Faster O
R O
- O
CNN O
: O
Towards O
real O
- O
time O
object O
detection O
with O
region O
proposal O
networks O
. O
In O
Advances O
in O
neural O
information O
processing O
systems O
, O
pages O
91‚Äì99 O
. O
Steven O
J. O
Rennie O
, O
Etienne O
Marcheret O
, O
Youssef O
Mroueh O
, O
Jerret O
Ross O
, O
and O
Vaibhava O
Goel O
. O
2017 O
. O
Self O
- O
critical O
sequence O
training O
for O
image O
captioning O
. O
In O
The O
IEEE O
Conference O
on O
Computer O
Vision O
and O
Pattern O
Recognition O
( O
CVPR O
) O
. O
Ashish O
Vaswani O
, O
Noam O
Shazeer O
, O
Niki O
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion O
Jones O
, O
Aidan O
N O
Gomez O
, O
≈Åukasz O
Kaiser O
, O
and O
Illia O
Polosukhin O
. O
2017 O
. O
Attention O
is O
all O
you O
need O
. O
In O
Advances O
in O
neural O
information O
processing O
systems O
, O
pages O
5998‚Äì6008 O
. O
Ramakrishna O
Vedantam O
, O
C. O
Lawrence O
Zitnick O
, O
and O
Devi O
Parikh O
. O
2015 O
. O
Cider O
: O
Consensus O
- O
based O
image O
description O
evaluation O
. O
In O
The O
IEEE O
Conference O
on O
Computer O
Vision O
and O
Pattern O
Recognition O
( O
CVPR O
) O
. O
Petar O
Veli O
Àáckovi O
¬¥ O
c O
, O
Guillem O
Cucurull O
, O
Arantxa O
Casanova O
, O
Adriana O
Romero O
, O
Pietro O
Li O
` O
o O
, O
and O
Yoshua O
Bengio O
. O
2018 O
. O
Graph O
attention O
networks O
. O
In O
International O
Conference O
on O
Learning O
Representations O
. O
Dalin O
Wang O
, O
Daniel O
Beck O
, O
and O
Trevor O
Cohn O
. O
2019 O
. O
On O
the O
role O
of O
scene O
graphs O
in O
image O
captioning O
. O
In O
Proceedings O
of O
the O
Beyond O
Vision O
and O
LANguage O
: O
inTEgrating O
Real O
- O
world O
kNowledge O
( O
LANTERN O
) O
, O
pages O
29‚Äì34 O
, O
Hong O
Kong O
, O
China O
. O
Association O
for O
Computational O
Linguistics O
. O
Danfei O
Xu O
, O
Yuke O
Zhu O
, O
Christopher O
B O
Choy O
, O
and O
Li O
FeiFei O
. O
2017 O
. O
Scene O
graph O
generation O
by O
iterative O
message O
passing O
. O
In O
Proceedings O
of O
the O
IEEE O
Conference O
on O
Computer O
Vision O
and O
Pattern O
Recognition O
, O
pages O
5410‚Äì5419.Jianwei O
Yang O
, O
Jiasen O
Lu O
, O
Stefan O
Lee O
, O
Dhruv O
Batra O
, O
and O
Devi O
Parikh O
. O
2018 O
. O
Graph O
R O
- O
CNN O
for O
scene O
graph O
generation O
. O
In O
Proceedings O
of O
the O
European O
Conference O
on O
Computer O
Vision O
( O
ECCV O
) O
, O
pages O
670‚Äì685 O
. O
X. O
Yang O
, O
K. O
Tang O
, O
H. O
Zhang O
, O
and O
J. O
Cai O
. O
2019 O
. O
Autoencoding O
scene O
graphs O
for O
image O
captioning O
. O
In O
2019 O
IEEE O
/ O
CVF O
Conference O
on O
Computer O
Vision O
and O
Pattern O
Recognition O
( O
CVPR O
) O
, O
pages O
10677 O
‚Äì O
10686 O
. O
J. O
Yu O
, O
J. O
Li O
, O
Z. O
Yu O
, O
and O
Q. O
Huang O
. O
2019a O
. O
Multimodal O
transformer O
with O
multi O
- O
view O
visual O
representation O
for O
image O
captioning O
. O
IEEE O
Transactions O
on O
Circuits O
and O
Systems O
for O
Video O
Technology O
, O
pages O
1 O
‚Äì O
1 O
. O
J. O
Yu O
, O
J. O
Li O
, O
Z. O
Yu O
, O
and O
Q. O
Huang O
. O
2019b O
. O
Multimodal O
transformer O
with O
multi O
- O
view O
visual O
representation O
for O
image O
captioning O
. O
IEEE O
Transactions O
on O
Circuits O
and O
Systems O
for O
Video O
Technology O
, O
pages O
1 O
‚Äì O
1 O
. O
Rowan O
Zellers O
, O
Mark O
Yatskar O
, O
Sam O
Thomson O
, O
and O
Yejin O
Choi O
. O
2018 O
. O
Neural O
motifs O
: O
Scene O
graph O
parsing O
with O
global O
context O
. O
In O
Conference O
on O
Computer O
Vision O
and O
Pattern O
Recognition O
. O
Yuyu O
Zhang O
, O
Hanjun O
Dai O
, O
Zornitsa O
Kozareva O
, O
Alexander O
J O
Smola O
, O
and O
Le O
Song O
. O
2018 O
. O
Variational O
reasoning O
for O
question O
answering O
with O
knowledge O
graph O
. O
InThirty O
- O
Second O
AAAI O
Conference O
on O
ArtiÔ¨Åcial O
Intelligence O
.514A O
Implementation O
Details O
All O
our O
models O
are O
trained O
until O
convergence O
using O
early O
stopping O
with O
a O
patience O
of O
20 O
epochs O
and O
a O
maximum O
of O
50 O
epochs O
. O
We O
use O
the O
Adamax O
optimizer O
( O
Kingma O
and O
Ba O
, O
2014 O
) O
with O
an O
initial O
learning O
rate O
of O
0.002 O
, O
which O
we O
decay O
with O
a O
factor O
of O
0.8 O
after O
8 O
epochs O
without O
improvements O
on O
the O
validation O
set O
. O
Dropout O
regularisation O
with O
a O
probability O
of O
50 O
% O
is O
applied O
on O
word O
embeddings O
and O
on O
the O
hidden O
state O
of O
the O
second O
LSTM O
layerh(t O
) O
2before O
it O
is O
projected O
to O
compute O
the O
next O
word O
probabilities O
. O
We O
use O
a O
beam O
size O
of O
5 O
during O
evaluation O
. O
All O
hidden O
layers O
and O
embedding O
sizes O
are O
set O
to O
1024 O
. O
Models O
are O
all O
trained O
on O
a O
single O
12 O
GB O
NVIDIA O
GPU O
. O
We O
use O
a O
Ô¨Åxed O
number O
of O
n= O
36 O
objects O
extracted O
with O
our O
pretrained O
Faster O
R O
- O
CNN O
. O
The O
number O
of O
objects O
and O
relations O
extracted O
with O
the O
pretrained O
Iterative O
Message O
Passing O
model O
varies O
according O
to O
the O
input O
image O
, O
i.e. O
a O
maximum O
of O
o= O
100 O
objects O
and O
of O
r= O
2500 O
relations O
. O
B O
Using O
Ground O
- O
Truth O
Graphs O
SPICE O
Captioning O
All O
Obj O
Rel O
B4 O
C O
R O
FA O
18.3 O
34.3 O
5.3 O
30.5 O
95.8 O
53.1 O
HA O
- O
IM O
19.0 O
35.2 O
4.8 O
33.5 O
106.0 O
54.8 O
HA O
- O
SG O
18.8 O
34.7 O
4.9 O
32.9 O
104.5 O
54.3 O
+ O
GAT O
18.6 O
34.7 O
5.2 O
32.5 O
100.9 O
53.9 O
+ O
C O
- O
GAT O
18.9 O
34.8 O
5.133.6 O
104.3 O
54.5 O
Table O
4 O
: O
Results O
for O
the O
full O
VG O
- O
COCO O
validation O
set O
using O
features O
extracted O
for O
ground O
- O
truth O
scene O
graphs O
. O
Models O
and O
acronyms O
are O
described O
in O
Sections O
5 O
. O
Metrics O
reported O
are O
: O
the O
overall O
SPICE O
F1 O
score O
( O
All O
) O
and O
its O
object O
( O
Obj O
) O
and O
relation O
( O
Rel O
) O
F1 O
score O
components O
, O
BLEU-4 O
( O
B4 O
) O
, O
CIDEr O
( O
C O
) O
, O
and O
ROUGE O
- O
L O
( O
R O
) O
. O
We O
bold O
- O
face O
the O
best O
and O
underline O
the O
second O
- O
best O
overall O
scores O
per O
metric O
. O
In O
this O
small O
- O
scale O
experiment O
, O
we O
generate O
features O
for O
ground O
- O
truth O
scene O
graphs O
to O
determine O
if O
more O
a O
positive O
transfer O
can O
be O
achieved O
on O
image O
captioning O
models O
. O
For O
the O
VG O
- O
COCO O
dataset O
, O
we O
take O
all O
the O
ground O
- O
truth O
object O
and O
relation O
boxes O
and O
pass O
these O
through O
the O
pretrained O
Iterative O
Message O
Passing O
( O
IMP O
) O
model O
, O
instead O
of O
the O
RPN O
and O
RelPN O
proposed O
boxes O
. O
This O
is O
the O
same O
pretrained O
( O
IMP O
) O
model O
used O
in O
the O
other O
experiments O
. O
Wang O
et O
al O
. O
( O
2019 O
) O
also O
did O
a O
similar O
experiment O
, O
however O
, O
they O
also O
trained O
their O
models O
using O
features O
from O
gold O
- O
standard O
scene O
graphs O
, O
whereas O
we O
only O
use O
them O
to O
evaluate O
models O
previously O
trained O
on O
predicted O
scene O
graph O
features O
. O
In O
Table O
4 O
we O
show O
that O
when O
using O
ground O
- O
truth O
scene O
graphs O
results O
are O
worse O
than O
those O
obtained O
using O
predicted O
ones O
( O
Table O
3a O
) O
. O
One O
obvious O
explanation O
is O
the O
mismatch O
between O
training O
and O
testing O
data O
, O
with O
regards O
to O
quality O
and O
number O
of O
features O
. O
Models O
are O
trained O
on O
the O
predicted O
scene O
graphs O
, O
which O
have O
an O
average O
of O
34 O
object O
and O
48 O
relation O
features O
per O
image O
( O
probably O
noisy O
, O
as O
seen O
in O
Section O
5.4 O
) O
, O
whereas O
ground O
- O
truth O
graphs O
have O
an O
average O
of O
21 O
objects O
and O
18 O
relations O
per O
image.515Proceedings O
of O
the O
1st O
Conference O
of O
the O
Asia O
- O
PaciÔ¨Åc O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
10th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
516‚Äì528 O
December O
4 O
- O
7 O
, O
2020 O
. O
¬© O
2020 O
Association O
for O
Computational O
Linguistics O
Systematically O
Exploring O
Redundancy O
Reduction O
in O
Summarizing O
Long O
Documents O
Wen O
Xiao O
and O
Giuseppe O
Carenini O
Department O
of O
Computer O
Science O
University O
of O
British O
Columbia O
Vancouver O
, O
BC O
, O
Canada O
, O
V6 O
T O
1Z4 O
{ O
xiaowen3 O
, O
carenini O
} O
@cs.ubc.ca O
Abstract O
Our O
analysis O
of O
large O
summarization O
datasets O
indicates O
that O
redundancy O
is O
a O
very O
serious O
problem O
when O
summarizing O
long O
documents O
. O
Yet O
, O
redundancy O
reduction O
has O
not O
been O
thoroughly O
investigated O
in O
neural O
summarization O
. O
In O
this O
work O
, O
we O
systematically O
explore O
and O
compare O
different O
ways O
to O
deal O
with O
redundancy O
when O
summarizing O
long O
documents O
. O
SpeciÔ¨Åcally O
, O
we O
organize O
the O
existing O
methods O
into O
categories O
based O
on O
when O
and O
how O
the O
redundancy O
is O
considered O
. O
Then O
, O
in O
the O
context O
of O
these O
categories O
, O
we O
propose O
three O
additional O
methods O
balancing O
non O
- O
redundancy O
and O
importance O
in O
a O
general O
and O
Ô¨Çexible O
way O
. O
In O
a O
series O
of O
experiments O
, O
we O
show O
that O
our O
proposed O
methods O
achieve O
the O
state O
- O
of O
- O
the O
- O
art O
with O
respect O
to O
ROUGE O
scores O
on O
two O
scientiÔ¨Åc O
paper O
datasets O
, O
Pubmed O
and O
arXiv O
, O
while O
reducing O
redundancy O
signiÔ¨Åcantly.1 O
1 O
Introduction O
Summarization O
is O
the O
task O
of O
shortening O
a O
given O
document(s O
) O
while O
maintaining O
the O
most O
important O
information O
. O
In O
general O
, O
a O
good O
summarizer O
should O
generate O
a O
summary O
that O
is O
syntactically O
accurate O
, O
semantically O
correct O
, O
coherent O
, O
and O
non O
- O
redundant O
( O
Saggion O
and O
Poibeau O
, O
2013 O
) O
. O
While O
extractive O
methods O
tend O
to O
have O
better O
performance O
on O
the O
Ô¨Årst O
two O
aspects O
, O
they O
are O
typically O
less O
coherent O
andmore O
redundant O
than O
abstractive O
ones O
, O
where O
new O
sentences O
are O
often O
generated O
by O
sentence O
fusion O
and O
compression O
, O
which O
helps O
detecting O
and O
removing O
redundancy O
( O
Lebanoff O
et O
al O
. O
, O
2019 O
) O
. O
Although O
eliminating O
redundancy O
has O
been O
initially O
and O
more O
intensely O
studied O
in O
the O
Ô¨Åeld O
of O
multidocument O
summarization O
( O
Lloret O
and O
Sanz O
, O
2013 O
) O
, O
because O
important O
sentences O
selected O
from O
multiple O
documents O
( O
about O
the O
same O
topic O
) O
are O
more O
1Our O
code O
can O
be O
found O
here O
- O
http://www.cs O
. O
ubc.ca/cs-research/lci/research-groups/ O
natural O
- O
language O
- O
processing O
/ O
likely O
to O
be O
redundant O
than O
sentences O
from O
the O
same O
document O
, O
generating O
a O
non O
- O
redundant O
summary O
should O
still O
be O
one O
of O
the O
goals O
for O
single O
document O
summarization O
( O
Lin O
et O
al O
. O
, O
2009 O
) O
. O
Generally O
speaking O
, O
there O
is O
a O
trade O
- O
off O
between O
importance O
and O
diversity O
( O
non O
- O
redundancy O
) O
( O
Jung O
et O
al O
. O
, O
2019 O
) O
, O
which O
is O
reÔ¨Çected O
in O
the O
two O
phases O
, O
sentence O
scoring O
and O
sentence O
selection O
( O
Zhou O
et O
al O
. O
, O
2018 O
) O
in O
which O
extractive O
summarization O
task O
can O
be O
naturally O
decomposed O
. O
The O
former O
typically O
scores O
sentences O
based O
on O
importance O
, O
while O
the O
latter O
selects O
sentences O
based O
on O
their O
scores O
, O
but O
also O
possibly O
taking O
other O
factors O
( O
including O
redundancy O
) O
into O
account O
. O
Traditionally O
, O
in O
non O
- O
neural O
approaches O
the O
tradeoff O
between O
importance O
and O
redundancy O
has O
been O
carefully O
considered O
, O
with O
sentence O
selection O
picking O
sentences O
by O
optimizing O
an O
objective O
function O
that O
balances O
the O
two O
aspects O
( O
Carbonell O
and O
Goldstein O
, O
1998 O
; O
Ren O
et O
al O
. O
, O
2016 O
) O
. O
In O
contrast O
, O
more O
recent O
works O
on O
neural O
extractive O
summarization O
models O
has O
so O
far O
over O
- O
emphasized O
sentence O
importance O
and O
the O
corresponding O
scoring O
phase O
, O
while O
paying O
little O
attention O
to O
how O
to O
reduce O
redundancy O
in O
the O
selection O
phase O
, O
where O
they O
simply O
apply O
a O
greedy O
algorithm O
to O
select O
sentences O
( O
e.g. O
,Cheng O
and O
Lapata O
( O
2016 O
) O
; O
Xiao O
and O
Carenini O
( O
2019 O
) O
) O
. O
Notice O
that O
this O
is O
especially O
problematic O
for O
long O
documents O
, O
where O
redundancy O
tends O
to O
be O
a O
more O
serious O
problem O
, O
as O
we O
have O
observed O
in O
key O
datasets O
. O
Improving O
redundancy O
reduction O
in O
neural O
extractive O
summarization O
for O
long O
documents O
is O
a O
major O
goal O
of O
this O
paper O
. O
Indeed O
, O
some O
recently O
proposed O
neural O
methods O
aim O
to O
reduce O
redundancy O
, O
but O
they O
either O
do O
that O
implicitly O
or O
inÔ¨Çexibly O
and O
only O
focusing O
on O
short O
documents O
( O
e.g. O
, O
news O
) O
. O
For O
instance O
, O
some O
models O
learn O
to O
reduce O
redundancy O
when O
predicting O
the O
scores O
( O
Nallapati O
et O
al O
. O
, O
2016a O
) O
, O
or O
jointly O
learn O
to O
score O
and O
select O
sentences O
( O
Zhou O
et O
al O
. O
, O
2018 O
) O
in516an O
implicit O
way O
. O
However O
, O
whether O
these O
strategies O
actually O
help O
reducing O
redundancy O
is O
still O
an O
open O
empirical O
question O
. O
The O
only O
neural O
attempt O
of O
explicitly O
reduce O
redundancy O
in O
the O
sentence O
selection O
phase O
is O
the O
Trigram O
Blocking O
technique O
, O
used O
in O
recent O
extractive O
summarization O
models O
on O
news O
datasets O
( O
e.g. O
, O
( O
Liu O
and O
Lapata O
, O
2019 O
) O
) O
. O
However O
, O
the O
effectiveness O
of O
such O
strategy O
on O
the O
summarization O
of O
long O
documents O
has O
not O
been O
tested O
. O
Finally O
, O
a O
very O
recent O
work O
by O
Bi O
et O
al O
. O
( O
2020 O
) O
attempts O
to O
reduce O
redundancy O
in O
more O
sophisticated O
ways O
, O
but O
still O
focusing O
on O
news O
. O
Furthermore O
, O
since O
it O
relies O
on O
BERT O
, O
such O
model O
is O
unsuitable O
to O
deal O
with O
long O
documents O
( O
with O
over O
3,000 O
words O
) O
. O
To O
address O
this O
rather O
confusing O
situation O
, O
characterized O
by O
unclear O
connections O
between O
all O
the O
proposed O
neural O
models O
, O
by O
their O
limited O
focus O
on O
short O
documents O
, O
and O
by O
spotty O
evaluations O
, O
in O
this O
paper O
we O
systematically O
organize O
existing O
redundancy O
reduction O
methods O
into O
three O
categories O
, O
and O
compare O
them O
with O
respect O
to O
the O
informativeness O
and O
redundancy O
of O
the O
generated O
summary O
for O
long O
documents O
. O
In O
particular O
, O
to O
perform O
a O
fair O
comparison O
we O
re O
- O
implement O
all O
methods O
by O
modifying O
a O
common O
basic O
model O
( O
Xiao O
and O
Carenini O
, O
2019 O
) O
, O
which O
is O
a O
top O
performer O
on O
long O
documents O
without O
considering O
redundancy O
. O
Additionally O
, O
we O
propose O
three O
new O
methods O
that O
we O
argue O
will O
reduce O
redundancy O
more O
explicitly O
and O
Ô¨Çexibly O
in O
the O
sentence O
scoring O
and O
sentence O
selection O
phase O
by O
deploying O
more O
suitable O
decoders O
, O
loss O
functions O
and/or O
sentence O
selection O
algorithms O
, O
again O
building O
for O
a O
fair O
comparison O
on O
the O
common O
basic O
model O
( O
Xiao O
and O
Carenini O
, O
2019 O
) O
. O
To O
summarize O
, O
our O
main O
contributions O
in O
this O
paper O
are O
: O
we O
Ô¨Årst O
examine O
popular O
datasets O
, O
and O
show O
that O
redundancy O
is O
a O
more O
serious O
problem O
when O
summarizing O
long O
documents O
( O
e.g. O
, O
scientiÔ¨Åc O
papers O
) O
than O
short O
ones O
( O
e.g. O
news O
) O
. O
Secondly O
, O
we O
not O
only O
reorganize O
and O
re O
- O
implement O
existing O
neural O
methods O
for O
redundancy O
reduction O
, O
but O
we O
also O
propose O
three O
new O
general O
and O
Ô¨Çexible O
methods O
. O
Finally O
, O
in O
a O
series O
of O
experiments O
, O
we O
compare O
existing O
and O
proposed O
methods O
on O
long O
documents O
( O
i.e. O
, O
the O
Pubmed O
and O
arXiv O
datasets O
) O
, O
with O
respect O
to O
ROUGE O
scores O
( O
Lin O
, O
2004 O
) O
and O
redundancy O
scores O
( O
Peyrard O
et O
al O
. O
, O
2017 O
; O
Feigenblat O
et O
al O
. O
, O
2017 O
) O
. O
As O
a O
preview O
, O
empirical O
results O
reveal O
that O
the O
proposed O
methods O
achieve O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
ROUGE O
scores O
, O
on O
the O
two O
scientiÔ¨Åc O
paper O
datasets O
, O
while O
also O
reducing O
the O
redundancysigniÔ¨Åcantly O
. O
2 O
Related O
Work O
In O
traditional O
extractive O
summarization O
, O
the O
process O
is O
treated O
as O
a O
discrete O
optimization O
problem O
balancing O
between O
importance O
scores O
and O
redundancy O
scores O
, O
with O
techniques O
like O
Maximal O
Marginal O
Relevance(MMR)(Carbonell O
and O
Goldstein O
, O
1998 O
) O
, O
redundancy O
- O
aware O
feature O
- O
based O
sentence O
classiÔ¨Åers O
( O
Ren O
et O
al O
. O
, O
2016 O
) O
and O
graph O
- O
based O
submodular O
selection O
( O
Lin O
et O
al O
. O
, O
2009 O
) O
. O
In O
recent O
years O
, O
researchers O
have O
explored O
neural O
extractive O
summarization O
solutions O
, O
which O
score O
sentences O
by O
training O
the O
neural O
models O
on O
a O
large O
corpus O
, O
and O
simply O
apply O
a O
greedy O
algorithm O
for O
sentence O
selection O
( O
Cheng O
and O
Lapata O
, O
2016 O
; O
Nallapati O
et O
al O
. O
, O
2016a O
) O
. O
Although O
a O
model O
with O
a O
sequence O
decoder O
might O
plausibly O
encode O
redundancy O
information O
implicitly O
, O
Kedzie O
et O
al O
. O
( O
2018 O
) O
empirically O
show O
that O
this O
is O
not O
the O
case O
, O
since O
non O
auto O
- O
regressive O
models O
( O
the O
ones O
scoring O
each O
sentence O
independently O
) O
, O
perform O
on O
par O
with O
models O
with O
a O
sequence O
decoder O
. O
In O
one O
of O
our O
new O
methods O
, O
to O
effectively O
capture O
redundancy O
information O
, O
we O
specify O
a O
new O
loss O
that O
explicitly O
consider O
redundancy O
when O
training O
the O
neural O
model O
. O
Beyond O
a O
greedy O
algorithm O
, O
the O
Trigram O
Blocking O
is O
frequently O
used O
to O
explicitly O
reduce O
redundancy O
in O
the O
sentence O
selection O
phase O
( O
Liu O
and O
Lapata O
, O
2019 O
) O
. O
In O
essence O
, O
a O
new O
sentence O
is O
not O
added O
to O
the O
summary O
if O
it O
shares O
a O
3 O
- O
gram O
with O
the O
previously O
added O
one O
. O
Paulus O
et O
al O
. O
( O
2017 O
) O
Ô¨Årst O
adopt O
the O
strategy O
for O
abstractive O
summarization O
, O
which O
forces O
the O
model O
not O
to O
produce O
the O
same O
trigram O
twice O
in O
the O
generated O
summaries O
, O
as O
a O
simpliÔ¨Åed O
version O
of O
MMR O
( O
Carbonell O
and O
Goldstein O
, O
1998 O
) O
. O
Arguably O
, O
this O
method O
is O
too O
crude O
for O
documents O
with O
relatively O
long O
sentences O
or O
speciÔ¨Åc O
concentrations O
( O
e.g. O
scientiÔ¨Åc O
papers O
) O
, O
where O
some O
technical O
terms O
, O
possibly O
longer O
than O
2 O
- O
grams O
, O
are O
repeated O
frequently O
in O
the O
‚Äô O
important O
sentences O
‚Äô O
( O
even O
in O
the O
reference O
summaries O
) O
. O
To O
address O
this O
limitation O
, O
we O
propose O
a O
neural O
version O
of O
MMR O
to O
deal O
with O
redundancy O
within O
the O
sentence O
selection O
phase O
in O
a O
more O
Ô¨Çexible O
way O
, O
that O
can O
be O
tuned O
to O
balance O
importance O
and O
non O
- O
redundancy O
as O
needed O
. O
The O
idea O
of O
MMR O
has O
also O
inspired O
Zhou O
et O
al O
. O
( O
2018 O
) O
, O
who O
propose O
a O
model O
jointly O
learning O
to O
score O
and O
select O
the O
sentences O
. O
Yet O
, O
this O
work O
not O
only O
focuses O
on O
summarizing O
short O
documents O
( O
i.e. O
, O
news O
) O
, O
but O
also O
uses O
MMR O
implicitly O
, O
and O
arguably517sub O
- O
optimally O
, O
by O
learning O
a O
score O
that O
only O
indirectly O
captures O
the O
trade O
- O
off O
between O
relevance O
and O
redundancy O
. O
To O
improve O
on O
this O
approach O
, O
in O
this O
paper O
we O
propose O
a O
third O
new O
method O
, O
in O
which O
importance O
and O
redundancy O
are O
explicitly O
weighted O
, O
while O
still O
making O
the O
sentence O
scoring O
and O
selection O
beneÔ¨Åt O
from O
each O
other O
by O
Ô¨Åne O
tuning O
the O
trained O
neural O
model O
through O
a O
Reinforcement O
Learning O
( O
RL O
) O
mechanism O
. O
Finally O
, O
Bi O
et O
al O
. O
( O
2020 O
) O
is O
the O
most O
recent O
( O
still O
unpublished O
) O
work O
on O
reducing O
redundancy O
in O
neural O
single O
document O
summarization O
. O
However O
, O
their O
goal O
is O
very O
different O
form O
ours O
, O
since O
they O
focus O
on O
relatively O
short O
documents O
in O
the O
news O
domain O
. O
3 O
Measuring O
Redundancy O
: O
metrics O
and O
comparing O
long O
vs. O
short O
documents O
We O
use O
the O
following O
two O
relatively O
new O
metrics O
to O
measure O
redundancy O
in O
the O
source O
documents O
and O
in O
the O
generated O
summaries O
. O
Unique O
n O
- O
gram O
ratio2 O
: O
proposed O
in O
Peyrard O
et O
al O
. O
( O
2017 O
) O
, O
it O
measures O
n O
- O
grams O
uniqueness O
; O
the O
lower O
it O
is O
, O
the O
more O
redundant O
the O
document O
is O
. O
Uniqngramratio O
= O
count O
( O
uniqngram O
) O
count O
( O
ngram O
) O
Normalized O
Inverse O
of O
Diversity O
( O
NID O
): O
captures O
redundancy O
, O
as O
the O
inverse O
of O
a O
diversity O
metric O
with O
length O
normalization O
. O
Diversity O
is O
deÔ¨Åned O
as O
the O
entropy O
of O
unigrams O
in O
the O
document O
( O
Feigenblat O
et O
al O
. O
, O
2017 O
) O
. O
Since O
longer O
documents O
are O
more O
likely O
to O
have O
a O
higher O
entropy O
, O
we O
normalize O
the O
diversity O
with O
the O
maximum O
possible O
entropy O
for O
the O
document O
log(|D| O
) O
. O
Thus O
, O
we O
have O
: O
NID O
= O
1‚àíentropy O
( O
D O
) O
log(|D| O
) O
Note O
that O
higher O
NID O
indicates O
more O
redundancy O
. O
When O
we O
compare O
the O
redundancy O
of O
long O
vs. O
short O
documents O
with O
respect O
to O
these O
two O
metrics O
on O
four O
popular O
datasets O
for O
summarization O
( O
CNNDM O
( O
Nallapati O
et O
al O
. O
, O
2016b O
) O
, O
Xsum O
( O
Narayan O
et O
al O
. O
, O
2018 O
) O
, O
Pubmed O
and O
arXiv O
( O
Cohan O
et O
al O
. O
, O
2018 O
) O
) O
, O
we O
observe O
that O
long O
documents O
are O
substantially O
more O
redundant O
than O
short O
ones O
( O
as O
it O
was O
already O
pointed O
out O
in O
the O
past O
( O
Stewart O
and O
Carbonell O
, O
1998 O
) O
) O
. O
Table O
1 O
shows O
the O
basic O
statistics O
of O
each O
dataset O
, O
along O
with O
the O
average O
NID O
2In O
this O
paper O
, O
all O
the O
unique O
n O
- O
gram O
ratios O
are O
shown O
in O
percentage O
. O
Figure O
1 O
: O
The O
average O
unique O
n O
- O
gram O
ratio O
in O
the O
documents O
across O
different O
datasets O
. O
To O
reduce O
the O
effect O
of O
length O
difference O
, O
stopwords O
were O
removed O
. O
scores O
, O
while O
Figure O
1 O
shows O
the O
average O
Unique O
n O
- O
gram O
Ratio O
for O
the O
same O
datasets O
. O
These O
observations O
provide O
further O
evidence O
that O
redundancy O
is O
a O
more O
serious O
problem O
in O
long O
documents O
. O
In O
addition O
, O
notice O
that O
the O
sentences O
in O
the O
scientiÔ¨Åc O
paper O
datasets O
are O
much O
longer O
than O
in O
the O
news O
datasets O
, O
which O
plausibly O
makes O
it O
even O
harder O
to O
balance O
between O
importance O
and O
non O
- O
redundancy O
. O
Datasets O
# O
Doc O
. O
# O
words O
/ O
doc O
. O
# O
words O
/ O
sent O
. O
NID O
Xsum O
203k O
429 O
22.8 O
0.188 O
CNNDM O
270k O
823 O
19.9 O
0.205 O
Pubmed O
115k O
3142 O
35.1 O
0.255 O
arXiv O
201k O
6081 O
29.2 O
0.267 O
Table O
1 O
: O
Longer O
documents O
are O
more O
redundant O
4 O
Redundancy O
Reduction O
Methods O
We O
systematically O
organize O
neural O
redundancy O
reduction O
methods O
into O
three O
categories O
, O
and O
compare O
prototypical O
methods O
from O
each O
category O
. O
AThe O
decoder O
is O
designed O
to O
implicitly O
take O
redundancy O
into O
account O
. O
BIn O
the O
sentence O
scoring O
phase O
, O
explicitly O
learn O
to O
reduce O
the O
redundancy O
. O
CIn O
the O
sentence O
selection O
phase O
, O
select O
sentences O
with O
less O
redundancy O
. O
In O
this O
section O
, O
we O
describe O
different O
methods O
from O
each O
category O
. O
To O
compare O
them O
in O
a O
fair O
way O
, O
we O
build O
all O
of O
them O
on O
a O
basic O
ExtSum O
- O
LG O
model O
( O
see¬ß4.1 O
) O
, O
by O
modifying O
the O
decoder O
and O
the O
loss O
function O
in O
the O
sentence O
selection O
phase O
or O
the O
sentence O
selection O
algorithm O
. O
In O
Table O
2 O
, O
we O
summarize O
the O
architecture O
( O
Encoder O
, O
Decoder O
, O
Loss O
Function O
and O
sentence O
selection O
algorithm O
) O
of O
all O
the O
methods O
we O
compare.518Categ O
. O
MethodsSent O
. O
Scor O
. O
Sent O
. O
Sel O
. O
Encoder O
Decoder O
Loss O
Func O
. O
- O
Naive O
MMR O
Cosine O
Similarity O
MMR O
Select O
- O
ExtSum O
- O
LG O
Encoder O
- O
LG O
MLP O
Cross O
Entropy O
( O
CE O
) O
Greedy O
A O
+ O
SR O
Decoder O
Encoder O
- O
LG O
SR O
Decoder O
CE O
Greedy O
A O
+ O
NeuSum O
Decoder O
Encoder O
- O
LG O
NeuSum O
Decoder O
KL O
Divergence O
NeuSum O
Decoder O
B O
+ O
RdLoss O
Encoder O
- O
LG O
MLP O
CE O
+ O
Red O
. O
Loss1 O
Greedy O
C O
+ O
Trigram O
Blocking O
Encoder O
- O
LG O
MLP O
CE O
Trigram O
Blocking O
C O
+ O
MMR O
- O
Select O
Encoder O
- O
LG O
MLP O
CE O
MMR O
Select O
C O
+ O
MMR O
- O
Select+ O
Encoder O
- O
LG O
MLP O
CE O
+ O
Red O
. O
Loss2 O
MMR O
Select O
Table O
2 O
: O
The O
architecture O
of O
redundancy O
reduction O
methods O
. O
Bold O
methods O
are O
proposed O
in O
this O
paper O
. O
4.1 O
Baseline O
Models O
We O
consider O
two O
baseline O
models O
. O
One O
is O
an O
inÔ¨Çuential O
unsupervised O
method O
explicitly O
balancing O
importance O
and O
redundancy O
( O
Naive O
MMR O
) O
. O
The O
other O
is O
our O
basic O
neural O
supervised O
model O
not O
dealing O
with O
redundancy O
at O
all O
( O
ExtSum O
- O
LG O
) O
, O
to O
which O
we O
add O
different O
redundancy O
reduction O
mechanisms O
. O
Naive O
MMR O
MMR O
( O
Carbonell O
and O
Goldstein O
, O
1998 O
) O
is O
a O
traditional O
extractive O
summarization O
method O
, O
which O
re O
- O
ranks O
the O
candidate O
sentences O
with O
a O
balance O
between O
query O
- O
relevance(importance O
) O
and O
information O
novelty(non O
- O
redundancy O
) O
. O
Given O
a O
document O
D O
, O
at O
each O
step O
, O
MMR O
selects O
one O
sentence O
from O
the O
candidate O
set O
D\ÀÜSthat O
is O
relevant O
with O
the O
queryQ O
, O
while O
containing O
little O
redundancy O
with O
the O
current O
summary O
ÀÜS. O
Note O
that O
if O
there O
is O
no O
speciÔ¨Åc O
query O
, O
then O
the O
query O
is O
the O
representation O
of O
the O
whole O
document O
. O
The O
method O
can O
be O
formally O
speciÔ¨Åed O
as O
: O
MMR O
= O
arg O
max O
si‚ààD\ÀÜS[ŒªSim O
1(si O
, O
Q O
) O
‚àí(1‚àíŒª O
) O
max O
sj‚ààÀÜSSim O
2(si O
, O
sj O
) O
] O
whereSim O
1(si O
, O
Q)measures O
the O
similarity O
between O
the O
candidate O
sentence O
siand O
the O
query O
, O
indicating O
the O
importance O
of O
si O
, O
while O
maxsj‚ààÀÜSSim O
2(si O
, O
sj)measures O
the O
similarity O
between O
the O
candidate O
sentence O
siand O
the O
current O
summary O
ÀÜS O
, O
representing O
the O
redundancy O
, O
and O
Œª O
is O
the O
balancing O
factor O
. O
In O
this O
work O
, O
all O
the O
Sim O
are O
computed O
as O
the O
cosine O
similarity O
between O
the O
embeddings O
of O
the O
sentences O
. O
ExtSum O
- O
LG O
For O
the O
basic O
model O
, O
we O
use O
the O
current O
state O
- O
ofthe O
- O
art O
model O
( O
Xiao O
and O
Carenini O
, O
2019 O
) O
on O
the O
summarization O
of O
long O
documents O
. O
It O
is O
a O
novel O
extractive O
summarization O
model O
incorporating O
local O
context O
and O
global O
context O
in O
the O
encoder O
, O
withan O
MLP O
layer O
as O
decoder O
and O
cross O
- O
entropy O
as O
the O
loss O
function O
. O
For O
the O
sentence O
selection O
phase O
, O
it O
greedily O
picks O
the O
sentences O
according O
to O
the O
score O
predicted O
by O
the O
neural O
model O
. O
In O
this O
method O
, O
redundancy O
is O
not O
considered O
, O
so O
it O
is O
a O
good O
testbed O
for O
adding O
and O
comparing O
redundancy O
reduction O
methods O
. O
SpeciÔ¨Åcally O
, O
for O
a O
document O
D O
= O
{ O
s1,s2, O
... O
,s O
n O
} O
, O
the O
output O
of O
the O
encoder O
is O
hifor O
each O
sentence O
si O
, O
and O
the O
decoder O
gives O
outputP(yi)as O
the O
conÔ¨Ådence O
score O
on O
the O
importance O
of O
sentence O
si O
. O
Finally O
, O
the O
model O
is O
trained O
on O
the O
Cross O
Entropy O
Loss O
: O
Lce=‚àín O
/ O
summationdisplay O
i=1(yilogP(yi)+(1‚àíyi O
) O
log O
( O
1‚àíP(yi O
) O
) O
4.2 O
Implicitly O
Reduce O
Redundancy O
in O
the O
neural O
model O
( O
Category O
A O
, O
Table O
2 O
) O
In O
this O
section O
, O
we O
describe O
two O
decoders O
from O
previous O
work O
, O
in O
which O
the O
redundancy O
of O
the O
summary O
is O
considered O
implicitly O
. O
SummaRuNNer O
Decoder O
: O
Nallapati O
et O
al O
. O
( O
2016a O
) O
introduce O
a O
decoder O
that O
computes O
a O
sentence O
score O
based O
on O
its O
salience O
, O
novelty(nonredundancy O
) O
and O
position O
to O
decide O
whether O
it O
should O
be O
included O
in O
the O
summary O
. O
Formally O
: O
P(yi O
) O
= O
œÉ(Wchi O
# O
Content O
+ O
hiWsd O
# O
Salience O
‚àíhT O
iWrtanh(summ O
i O
) O
# O
Novelty O
+ O
Wappa O
i+Wrppr O
i O
# O
Position O
+ O
b O
) O
# O
Bias O
wherehiis O
the O
hidden O
state O
of O
sentence O
ifrom O
the O
encoder O
, O
dis O
the O
document O
representation O
, O
summ O
i O
is O
the O
summary O
representation O
, O
updated O
after O
each O
decoding O
step O
, O
and O
pa O
i O
, O
pr O
iare O
absolute O
and O
relative O
position O
embeddings O
, O
respectively O
. O
Once O
P(yi)is O
obtained O
for O
each O
sentence O
i O
, O
a O
greedy O
algorithm O
selects O
the O
sentences O
to O
form O
the O
Ô¨Ånal O
summary.519Notice O
that O
although O
SummaRuNNer O
does O
contain O
a O
component O
assessing O
novelty O
, O
it O
would O
be O
inappropriate O
to O
view O
this O
model O
as O
explicitly O
dealing O
with O
redunadany O
because O
the O
novelty O
component O
is O
not O
directly O
supervised O
. O
NeuSum O
Decoder O
: O
One O
of O
the O
main O
drawback O
of O
SummaRuNNer O
decoder O
is O
that O
it O
always O
score O
the O
sentences O
in O
order O
, O
i.e. O
, O
the O
former O
sentences O
are O
not O
inÔ¨Çuenced O
by O
the O
latter O
ones O
. O
In O
addition O
, O
it O
only O
considers O
redundancy O
in O
the O
sentence O
scoring O
phase O
, O
while O
simply O
using O
a O
greedy O
algorithm O
to O
select O
sentences O
according O
to O
the O
resulting O
scores O
. O
To O
address O
these O
problems O
, O
Zhou O
et O
al O
. O
( O
2018 O
) O
propose O
a O
new O
decoder O
to O
identify O
the O
relative O
gain O
of O
sentences O
, O
jointly O
learning O
to O
score O
and O
select O
sentences O
. O
In O
such O
decoder O
, O
instead O
of O
feeding O
the O
sentences O
and O
getting O
the O
scores O
in O
order O
, O
they O
use O
a O
mechanism O
similar O
to O
the O
pointer O
network O
( O
Vinyals O
et O
al O
. O
, O
2015 O
) O
to O
predict O
the O
scores O
of O
all O
the O
sentences O
at O
each O
step O
, O
select O
the O
sentence O
with O
the O
highest O
score O
, O
and O
feed O
it O
to O
the O
next O
step O
of O
sentence O
selection O
. O
As O
for O
the O
loss O
function O
, O
they O
use O
the O
KL O
divergence O
between O
the O
predicted O
score O
distribution O
and O
the O
relative O
ROUGE O
F1 O
gain O
at O
each O
step O
. O
To O
be O
speciÔ¨Åc O
, O
the O
loss O
computed O
at O
step O
tis O
: O
Lt O
= O
DKL(Pt||Qt O
) O
Pt(yi O
) O
= O
exp(œÉ(hi))/summationtextn O
j=1exp(œÉ(hj O
) O
) O
Qt(yi O
) O
= O
exp(œÑÀúgt(yi))/summationtextn O
j=1exp(œÑÀúgt(yj O
) O
) O
gt(yi O
) O
= O
r1(St‚àí1‚à™si)‚àír1(St‚àí1 O
) O
wherePt O
, O
Qtare O
the O
predicted O
and O
ground O
truth O
relative O
gain O
respectively O
, O
gt(yi)is O
the O
ROUGE O
F1 O
gain O
with O
respect O
to O
the O
current O
partial O
summary O
St‚àí1for O
sentence O
si O
, O
and O
Àúgt(yi)is O
the O
Min O
- O
Max O
normalizedgt(yi).œÑis O
a O
smoothing O
factor O
, O
which O
is O
set O
to O
200empirically O
on O
the O
Pubmed O
dataset.3 O
4.3 O
Explicitly O
Reduce O
Redundancy O
in O
Sentence O
Scoring O
( O
Category O
B O
, O
Table O
2 O
) O
We O
propose O
a O
new O
method O
to O
explicitly O
learn O
to O
reduce O
redundancy O
when O
scoring O
the O
sentences O
. O
RdLoss O
: O
Although O
Zhou O
et O
al O
. O
( O
2018 O
) O
jointly O
train O
the O
decoder O
to O
score O
and O
select O
sentences O
, O
it O
still O
learns O
to O
reduce O
redundancy O
implicitly O
, O
and O
the O
method O
does O
not O
allow O
controlling O
the O
degree O
of O
redundancy O
. O
To O
address O
this O
limitation O
, O
we O
propose O
a O
rather O
simple O
method O
to O
explicitly O
force O
the O
model O
3Due O
to O
the O
complexity O
of O
generating O
the O
target O
distribution O
Q O
, O
we O
only O
experiment O
with O
this O
method O
on O
Pubmed.to O
reduce O
redundancy O
in O
the O
sentence O
scoring O
phase O
by O
adding O
a O
redundancy O
loss O
term O
to O
the O
original O
loss O
function O
, O
motivated O
by O
the O
success O
of O
a O
similar O
strategy O
of O
adding O
a O
bias O
loss O
term O
in O
the O
gender O
debiasing O
task O
( O
Qian O
et O
al O
. O
, O
2019 O
) O
. O
Our O
new O
loss O
termLrdis O
naturally O
deÔ¨Åned O
as O
the O
expected O
redundancy O
contained O
in O
the O
resulting O
summary O
, O
as O
shown O
below O
: O
L O
= O
Œ≤Lce+ O
( O
1‚àíŒ≤)Lrd O
Lrd O
= O
n O
/ O
summationdisplay O
i=1n O
/ O
summationdisplay O
j=1P(yi)P(yj)Sim(si O
, O
sj O
) O
whereP(yi),P(yj)are O
the O
conÔ¨Ådence O
scores O
of O
sentenceiandjon O
whether O
to O
select O
the O
sentences O
in O
the O
generated O
summary O
, O
and O
Sim(si O
, O
sj)is O
the O
similarity O
, O
i.e. O
redundancy O
between O
sentence O
iand O
j.4By O
adding O
the O
redundancy O
loss O
term O
, O
we O
penalize O
it O
more O
if O
two O
sentences O
are O
similar O
to O
each O
other O
and O
both O
of O
them O
have O
high O
conÔ¨Ådence O
scores O
. O
Œ≤is O
a O
balance O
factor O
, O
controlling O
the O
degree O
of O
redundancy O
. O
4.4 O
Explicitly O
Reduce O
Redundancy O
in O
Sentence O
Selection O
( O
Category O
C O
, O
Table O
2 O
) O
We O
Ô¨Årst O
introduce O
an O
existing O
method O
and O
then O
propose O
two O
novel O
methods O
that O
explicitly O
reduce O
redundancy O
in O
the O
sentence O
selection O
phase O
. O
Trigram O
Blocking O
is O
widely O
used O
in O
recent O
extractive O
summarization O
models O
on O
the O
news O
dataset O
( O
e.g. O
Liu O
and O
Lapata O
( O
2019 O
) O
) O
. O
Intuitively O
, O
it O
borrows O
the O
idea O
of O
MMR O
to O
balance O
the O
importance O
and O
non O
- O
redundancy O
when O
selecting O
sentences O
. O
In O
particular O
, O
given O
the O
predicted O
sentence O
scores O
, O
instead O
of O
just O
selecting O
sentences O
greedily O
according O
to O
the O
scores O
, O
the O
current O
candidate O
is O
added O
to O
the O
summary O
only O
if O
it O
does O
not O
have O
trigram O
overlap O
with O
the O
previous O
selected O
sentences O
. O
Otherwise O
, O
the O
current O
candidate O
sentence O
is O
ignored O
and O
the O
next O
one O
is O
checked O
, O
until O
the O
length O
limit O
is O
reached O
. O
MMR O
- O
Select O
: O
Inspired O
by O
the O
existence O
of O
a O
relevance O
/ O
redundancy O
trade O
- O
off O
, O
we O
propose O
MMRSelect O
, O
a O
simple O
method O
to O
eliminate O
redundancy O
when O
a O
neural O
summarizer O
selects O
sentences O
to O
form O
a O
summary O
, O
in O
a O
way O
that O
is O
arguably O
more O
Ô¨Çexible O
than O
Trigram O
Blocking O
with O
a O
balance O
factor O
Œª O
. O
With O
the O
conÔ¨Ådence O
score O
computed O
by O
the O
basic O
model O
, O
P={P(y1),P(y2), O
... O
,P O
( O
yn O
) O
} O
, O
instead O
of O
picking O
sentences O
greedily O
, O
we O
pick O
the O
sentences O
according O
to O
the O
MMR O
- O
score O
, O
which O
is O
deÔ¨Åned O
4Noting O
that O
we O
deÔ¨Åne O
Sim O
( O
si O
, O
si)as0520Figure O
2 O
: O
The O
pipeline O
of O
the O
MMR O
- O
Select+ O
method O
, O
where O
ÀÜS,ÀÜYand¬ØS,¬ØYare O
the O
summary O
and O
labels O
generated O
by O
the O
MMR O
- O
Select O
algorithm O
and O
the O
normal O
greedy O
algorithm O
, O
respectively O
. O
SandYare O
the O
ground O
truth O
summary O
and O
the O
oracle O
labels O
. O
based O
on O
MMR O
and O
updated O
after O
each O
single O
sentence O
being O
selected O
. O
MMR O
- O
Select O
= O
arg O
max O
si‚ààD\ÀÜS[MMR O
- O
score O
i O
] O
MMR O
- O
score O
i O
= O
ŒªP(yi)‚àí(1‚àíŒª O
) O
max O
sj‚ààÀÜSSim(si O
, O
sj O
) O
] O
The O
main O
difference O
between O
the O
Naive O
MMR O
and O
MMR O
- O
Select O
falls O
into O
the O
computation O
of O
the O
importance O
score O
. O
In O
the O
Naive O
MMR O
, O
the O
importance O
score O
is O
the O
similarity O
between O
each O
sentence O
and O
the O
query O
, O
or O
the O
whole O
document O
, O
while O
in O
MMR O
- O
Select O
, O
the O
importance O
score O
is O
computed O
by O
a O
trained O
neural O
model O
. O
MMR O
- O
Select+ O
: O
The O
main O
limitation O
of O
MMRSelect O
is O
that O
the O
sentence O
scoring O
phase O
and O
the O
sentence O
selection O
phase O
can O
not O
beneÔ¨Åt O
from O
each O
other O
, O
because O
they O
are O
totally O
separate O
. O
To O
promote O
synergy O
between O
these O
two O
phases O
, O
we O
design O
a O
new O
method O
, O
MMR O
- O
Select+ O
, O
shown O
in O
Figure O
2 O
, O
which O
synergistically O
combines O
three O
components O
: O
the O
basic O
model O
, O
the O
original O
crossentropy O
loss O
Lce(in O
blue O
) O
, O
and O
an O
RL O
mechanism O
( O
in O
green O
) O
whose O
loss O
is O
Lrd O
. O
The O
neural O
model O
is O
then O
trained O
on O
a O
mixed O
objective O
loss O
LwithŒ≥as O
the O
scaling O
factor O
. O
Zooming O
on O
the O
details O
of O
the O
RL O
component O
, O
it O
Ô¨Årst O
generates O
a O
summary O
ÀÜSby O
applying O
the O
MMR O
selection O
described O
for O
MMRSelect O
, O
which O
is O
to O
greedily O
pick O
sentences O
according O
to O
MMR O
- O
score O
, O
as O
well O
as O
the O
corresponding O
label O
assignment O
ÀÜY={ÀÜy1,ÀÜy2, O
... O
,ÀÜyn}(ÀÜyi= O
1 O
if O
siis O
selected O
, O
ÀÜyi= O
0 O
otherwise O
) O
. O
Then O
, O
the O
expected O
reward O
is O
computed O
based O
on O
the O
ROUGE O
score O
between O
ÀÜSand O
the O
gold O
- O
standard O
human O
abstractive O
summary O
Sweighted O
by O
the O
probability O
of O
the O
ÀÜYlabels O
. O
Notice O
that O
we O
also O
adopt O
the O
self O
- O
critical O
strategy O
( O
Paulus O
et O
al O
. O
, O
2017 O
) O
to O
help O
accelerating O
the O
convergence O
by O
adding O
a O
baseline O
summary O
¬ØS O
, O
which O
is O
generated O
by O
greedily O
pickingthe O
sentences O
according O
to O
P.r(¬ØS)is O
the O
reward O
of O
this O
baseline O
summary O
and O
it O
is O
subtracted O
from O
r(ÀÜS)to O
only O
positively O
reward O
summaries O
which O
are O
better O
than O
the O
baseline O
. O
Formally O
, O
the O
whole O
MMR O
- O
Select+ O
model O
can O
be O
speciÔ¨Åed O
as O
follows O
: O
L O
= O
Œ≥Lrd+ O
( O
1‚àíŒ≥)Lce O
Lrd=‚àí(r(ÀÜS)‚àír(¬ØS))n O
/ O
summationdisplay O
i=1logP O
( O
ÀÜyi O
) O
r(S O
/ O
prime O
) O
= O
1 O
3 O
/ O
summationdisplay O
k‚àà{1,2,L}ROUGE O
- O
k O
( O
S O
/ O
prime O
, O
S O
) O
5 O
Experiments O
In O
this O
section O
, O
we O
describe O
the O
settings O
, O
results O
and O
analysis O
of O
the O
experiments O
of O
different O
methods O
on O
the O
Pubmed O
and O
arXiv O
datasets O
. O
5.1 O
Model O
Settings O
Following O
previous O
work O
, O
we O
use O
GloVe O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
as O
word O
embedding O
, O
and O
the O
average O
word O
embedding O
as O
the O
distributed O
representation O
of O
sentences O
. O
To O
be O
comparable O
with O
Xiao O
and O
Carenini O
( O
2019 O
) O
, O
we O
set O
word O
length O
limit O
of O
the O
generated O
summaries O
as O
200on O
both O
datasets O
. O
6We O
tune O
the O
hyperparameter O
ŒªandŒ≤in O
the O
respective O
methods O
on O
the O
validation O
set O
, O
and O
set O
Œª= O
0.6,Œ≤= O
0.3for O
both O
datasets O
. O
Following O
previous O
work O
( O
e.g. O
, O
Li O
et O
al O
. O
( O
2019 O
) O
) O
, O
Œ≥was O
set O
to0.99 O
. O
For O
training O
MMR O
- O
Select+ O
, O
the O
learning O
rate O
islr= O
1e‚àí6 O
; O
we O
start O
with O
the O
pretrained O
ExtSumm O
- O
LG O
model O
. O
As O
for O
the O
evaluation O
metric O
, O
we O
use O
ROUGE O
scores O
as O
the O
measurement O
of O
importance O
while O
using O
the O
Unique O
N O
- O
gram O
Ratio O
and O
NID O
deÔ¨Åned O
in O
Section O
3 O
as O
the O
measurements O
of O
redundancy O
. O
5The O
results O
of O
ExtSum O
- O
LG O
were O
obtained O
by O
re O
- O
running O
their O
model O
. O
6A O
document O
representation O
in O
Unsupervised O
MMR O
is O
similarly O
computed O
by O
averaging O
the O
embeddings O
of O
all O
the O
words.521Figure O
3 O
: O
The O
average O
ROUGE O
scores O
, O
average O
unique O
n O
- O
gram O
ratios O
, O
and O
average O
NID O
scores O
with O
different O
Œªused O
in O
the O
MMR O
- O
Select O
on O
the O
validation O
set O
. O
Remember O
that O
the O
higher O
the O
Unique O
n O
- O
gram O
Ratio O
, O
the O
lower O
NID O
, O
the O
less O
redundancy O
contained O
in O
the O
summary O
. O
Categ O
. O
ModelPubmed O
arXiv O
ROUGE-1 O
ROUGE-2 O
ROUGE O
- O
L O
ROUGE-1 O
ROUGE-2 O
ROUGE O
- O
L O
C O
Naive O
MMR O
37.46 O
11.25 O
32.22 O
33.74 O
8.50 O
28.36 O
- O
ExtSum O
- O
LG545.18 O
20.20 O
40.72 O
43.77 O
17.50 O
38.71 O
A O
+ O
SR O
Decoder O
45.18 O
20.16 O
40.69 O
43.92 O
17.65 O
38.83 O
A O
+ O
NeuSum O
Decoder O
44.54 O
19.66 O
40.42 O
- O
- O
B O
+ O
RdLoss O
45.30‚Ä†20.42‚Ä†40.95‚Ä†44.01‚Ä†17.79‚Ä†39.09‚Ä† O
C O
+ O
Trigram O
Blocking O
43.33 O
17.67 O
39.01 O
42.75 O
15.73 O
37.85 O
C O
+ O
MMR O
- O
Select O
45.29‚Ä†20.30‚Ä†40.90‚Ä†43.81 O
17.41 O
38.94 O
C O
+ O
MMR O
- O
Select+ O
45.39‚Ä†20.37‚Ä†40.99‚Ä†43.87‚Ä†17.50 O
38.97‚Ä† O
- O
Oracle O
55.05 O
27.48 O
49.11 O
53.89 O
23.07 O
46.54 O
Table O
3 O
: O
Rouge O
score O
of O
different O
summarization O
models O
on O
the O
Pubmed O
and O
arXiv O
datasets O
. O
‚Ä†indicates O
signiÔ¨Åcantly O
better O
than O
the O
ExtSum O
- O
LG O
with O
conÔ¨Ådence O
level O
99 O
% O
on O
the O
Bootstrap O
SigniÔ¨Åcance O
test O
. O
Green O
numbers O
means O
it‚Äôsbetter O
than O
ExtSum O
- O
LG O
on O
the O
certain O
metric O
, O
and O
the O
red O
numbers O
means O
worse O
. O
Categ O
. O
ModelPubmed O
arXiv O
Unigram% O
Bigram% O
Trigram% O
NID O
Unigram% O
Bigram% O
Trigram% O
NID O
C O
Naive O
MMR O
56.55 O
90.93 O
96.95 O
0.1881 O
53.01 O
88.82 O
96.28 O
0.1992 O
- O
ExtSum O
- O
LG O
53.02 O
87.29 O
94.37 O
0.2066 O
52.17 O
87.19 O
95.38 O
0.2088 O
A O
+ O
SR O
Decoder O
52.88 O
87.17 O
94.32 O
0.2070 O
51.98 O
87.08 O
95.31 O
0.2097 O
A O
+ O
NeuSum O
Decoder O
54.88‚Ä†88.71‚Ä†95.13‚Ä†0.1993‚Ä†- O
- O
- O
B O
+ O
RdLoss O
53.23‚Ä†87.41 O
94.43 O
0.2052‚Ä†52.17 O
87.20 O
95.36 O
0.2085 O
C O
+ O
Trigram O
Blocking O
57.58‚Ä†‚Ä°93.05‚Ä†‚Ä°98.56‚Ä†‚Ä°0.1818‚Ä†‚Ä°56.12‚Ä†‚Ä°92.38‚Ä†‚Ä°98.94‚Ä†‚Ä°0.1876‚Ä†‚Ä° O
C O
+ O
MMR O
- O
Select O
53.76‚Ä†88.04‚Ä†94.96‚Ä†0.2022 O
52.80‚Ä†87.64‚Ä†95.40 O
0.2055‚Ä† O
C O
+ O
MMR O
- O
Select+ O
53.93‚Ä†88.32 O
95.14 O
0.2014 O
52.76‚Ä†87.78‚Ä†95.70‚Ä†0.2055‚Ä† O
- O
Oracle O
56.66 O
89.25 O
95.55 O
0.2036 O
56.74 O
90.81 O
96.82 O
0.2029 O
- O
Reference O
56.69 O
89.45 O
95.95 O
0.2005 O
58.92 O
90.13 O
97.02 O
0.1970 O
Table O
4 O
: O
Unique O
n O
- O
gram O
ratio O
and O
NID O
score O
on O
the O
two O
datasets O
. O
‚Ä†indicates O
signiÔ¨Åcant O
differences O
from O
( O
Xiao O
and O
Carenini O
, O
2019 O
) O
with O
conÔ¨Ådence O
level O
99 O
% O
, O
while O
‚Ä°indicates O
signiÔ¨Åcant O
differences O
from O
all O
the O
other O
models O
with O
conÔ¨Ådence O
level O
99 O
% O
on O
the O
Bootstrap O
SigniÔ¨Åcance O
test O
. O
Noting O
the O
higher O
the O
Unique O
n O
- O
gram O
Ratio O
, O
the O
lower O
NID O
, O
the O
less O
redundancy O
contained O
in O
the O
summary O
. O
Green O
numbers O
means O
it O
‚Äôs O
better O
than O
ExtSum O
- O
LG O
on O
the O
certain O
metric O
, O
and O
the O
red O
numbers O
means O
worse O
. O
5.2 O
Finetuning O
Œª O
Consistently O
with O
previous O
work O
( O
Jung O
et O
al O
. O
, O
2019 O
) O
, O
when O
we O
Ô¨Ånetune O
Œªof O
MMR O
Select O
on O
the O
validation O
set O
, O
we O
pinpoint O
the O
trade O
off O
between O
importance O
and O
non O
- O
redundancy O
in O
the O
generated O
summary O
( O
see O
Figure O
3 O
) O
. O
For O
Œª‚â§0.6 O
, O
as O
we O
increase O
the O
weight O
of O
importance O
score O
, O
the O
average O
ROUGE O
scores O
continuously O
increase O
while O
the O
redundancy O
/ O
diversity O
increases O
/ O
drops O
rapidly O
. O
But O
since O
extractive O
methods O
can O
only O
reuse O
sentences O
from O
the O
input O
document O
, O
there O
is O
an O
upper O
bound O
on O
how O
much O
the O
generated O
summary O
can O
match O
the O
ground O
- O
truth O
summary O
, O
so O
whenŒª>0.6 O
, O
the O
ROUGE O
score O
even O
drops O
by O
a O
small O
margin O
, O
while O
the O
redundancy O
/ O
diversity O
still O
decreases O
/ O
drops O
. O
Then O
the O
problem O
to O
solve O
for O
future O
work O
is O
how O
to O
increase O
the O
peak O
, O
which O
could O
be O
done O
by O
either O
applying O
Ô¨Åner O
units O
( O
e.g. O
, O
clauses O
instead O
of O
sentences O
) O
or O
further O
improve O
the O
model O
that O
predicts O
the O
importance O
score O
. O
5.3 O
Overall O
Results O
and O
Analysis O
The O
experimental O
results O
for O
the O
ROUGE O
scores O
are O
shown O
in O
Table O
3 O
, O
whereas O
results O
for O
redundancy O
scores O
( O
Unique O
N O
- O
gram O
Ratio O
and O
NID O
score O
) O
are O
shown O
in O
Table O
4 O
. O
With O
respect O
to O
the O
balance O
be-522tween O
importance O
and O
non O
- O
redundancy O
, O
despite O
the O
trade O
- O
off O
between O
the O
two O
aspects O
, O
all O
of O
the O
three O
methods O
we O
propose O
can O
reduce O
redundancy O
signiÔ¨Åcantly O
while O
also O
improving O
the O
ROUGE O
score O
signiÔ¨Åcantly O
compared O
with O
the O
ExtSum O
- O
LG O
basic O
neural O
model O
. O
In O
contrast O
, O
the O
NeuSum O
Decoder O
and O
Trigram O
Blocking O
effectively O
reduce O
redundancy O
, O
but O
in O
doing O
that O
they O
hurt O
the O
importance O
aspect O
considerably O
. O
Even O
worse O
, O
the O
SR O
Decoder O
is O
dominated O
by O
the O
basic O
model O
on O
both O
aspects O
. O
Focusing O
on O
the O
redundancy O
aspect O
( O
Table O
4 O
) O
, O
Trigram O
Blocking O
makes O
the O
largest O
improvement O
on O
redundancy O
reduction O
, O
but O
with O
a O
large O
drop O
in O
ROUGE O
scores O
. O
This O
is O
in O
striking O
contrast O
with O
results O
on O
news O
datasets O
( O
Liu O
and O
Lapata O
, O
2019 O
) O
, O
where O
Trigram O
Blocking O
reduced O
redundancy O
while O
also O
improving O
the O
ROUGE O
score O
signiÔ¨Åcantly O
. O
Plausibly O
, O
the O
difference O
between O
the O
performances O
across O
datasets O
might O
be O
the O
result O
of O
the O
inÔ¨Çexibility O
of O
the O
method O
. O
In O
both O
Pubmed O
and O
arXiv O
datasets O
, O
the O
sentences O
are O
much O
longer O
than O
those O
in O
the O
news O
dataset O
( O
See O
Table O
1 O
) O
, O
and O
therefore O
, O
simply O
dropping O
candidate O
sentences O
with O
3 O
- O
gram O
overlap O
may O
lead O
to O
incorrectly O
missing O
sentences O
with O
substantial O
important O
information O
. O
Furthermore O
, O
another O
insight O
revealed O
in O
Table O
4 O
is O
that O
dealing O
with O
redundancy O
in O
the O
sentence O
selection O
phase O
is O
consistently O
more O
effective O
than O
doing O
it O
in O
the O
sentence O
scoring O
phase O
, O
regardless O
of O
whether O
this O
happens O
implicitly O
( O
NeuSum O
> O
SR O
Decoder O
) O
or O
explicitly O
( O
Trigram O
Blocking O
, O
MMRSelect/+>RdLoss O
) O
. O
Moving O
to O
more O
speciÔ¨Åc O
Ô¨Åndings O
about O
particular O
systems O
, O
we O
already O
noted O
that O
while O
the O
NeuSum O
Decoder O
reduces O
redundancy O
effectively O
, O
it O
performs O
poorly O
on O
the O
ROUGE O
score O
, O
something O
that O
did O
not O
happen O
with O
news O
datasets O
. O
A O
possible O
explanation O
is O
that O
the O
number O
of O
sentences O
selected O
for O
the O
scientiÔ¨Åc O
paper O
datasets O
( O
on O
average O
6 O
- O
7 O
sentences O
) O
is O
almost O
twice O
the O
number O
of O
sentences O
selected O
for O
news O
; O
and O
as O
it O
was O
mentioned O
in O
the O
original O
paper O
( O
Zhou O
et O
al O
. O
, O
2018 O
) O
, O
the O
precision O
of O
NeuSum O
drops O
rapidly O
after O
selecting O
a O
few O
sentences O
. O
Other O
results O
conÔ¨Årm O
established O
beliefs O
. O
The O
considerable O
difference O
between O
Naive O
MMR O
and O
MMR O
- O
Select O
was O
expected O
given O
the O
recognized O
power O
of O
neural O
network O
over O
unsupervised O
methods O
. O
Secondly O
, O
the O
unimpressive O
performance O
of O
the O
SR O
decoder O
conÔ¨Årms O
that O
the O
in O
- O
order O
sequence O
scoring O
is O
too O
limited O
for O
effectively O
predicting O
im O
- O
portance O
score O
and O
reducing O
redundancy O
. O
5.4 O
More O
Insights O
of O
the O
Experiments O
In O
addition O
to O
the O
main O
experiment O
results O
discussed O
above O
, O
we O
further O
explore O
the O
performance O
on O
informativeness O
( O
ROUGE O
score O
) O
and O
redundancy O
( O
Unique O
N O
- O
gram O
Ratio O
) O
of O
different O
redundancy O
reduction O
methods O
under O
two O
different O
conditions O
, O
namely O
the O
degree O
of O
redundancy O
and O
the O
length O
of O
the O
source O
documents O
. O
Figure O
4 O
shows O
the O
results O
on O
the O
Pubmed O
dataset O
, O
while O
further O
results O
of O
a O
similar O
analysis O
on O
the O
arXiv O
dataset O
can O
be O
found O
in O
the O
Appendices O
. O
With O
respect O
to O
the O
degree O
of O
redundancy O
, O
( O
upper O
part O
of O
Figure O
4 O
) O
, O
the O
less O
redundant O
the O
document O
is O
, O
the O
less O
impact O
the O
redundancy O
reduction O
methods O
have O
. O
Among O
all O
the O
methods O
, O
although O
Trigram O
Blocking O
works O
the O
best O
with O
respect O
to O
reducing O
redundancy O
, O
it O
hurts O
the O
informativeness O
the O
most O
. O
However O
, O
it O
is O
still O
a O
good O
choice O
for O
a O
rather O
less O
redundant O
document O
( O
e.g. O
the O
documents O
in O
the O
last O
two O
bins O
with O
avg O
Unique O
N O
- O
gram O
Ratio O
over O
0.7 O
) O
, O
which O
is O
also O
consistent O
with O
the O
previous O
works O
showing O
the O
Trigram O
Blocking O
works O
well O
on O
the O
news O
datasets O
, O
which O
tends O
to O
be O
less O
redundant O
( O
see O
¬ß O
3 O
) O
. O
As O
for O
all O
the O
other O
methods O
, O
although O
they O
have O
the O
same O
trends O
, O
MMR O
- O
Select+ O
performs O
the O
best O
on O
both O
informativeness O
and O
redundancy O
reduction O
, O
especially O
for O
the O
more O
redundant O
documents O
. O
Regarding O
to O
the O
length O
of O
the O
source O
document O
( O
bottom O
part O
of O
Figure O
4 O
) O
, O
as O
the O
document O
become O
longer O
, O
both O
informativeness O
and O
redundancy O
in O
the O
summary O
generated O
by O
all O
methods O
increases O
and O
then O
decrease O
once O
hitting O
the O
peak O
. O
MMRSelect+ O
and O
MMR O
- O
Select O
are O
the O
best O
choices O
to O
balance O
between O
the O
informativeness O
and O
redundancy O
- O
they O
are O
the O
only O
two O
methods O
having O
the O
higher O
ROUGE O
scores O
and O
higher O
Unique O
N O
- O
gram O
ratios O
across O
different O
lengths O
, O
even O
for O
the O
short O
documents O
with O
less O
than O
50 O
sentences O
. O
Besides O
, O
we O
also O
conduct O
experiments O
on O
generating O
summaries O
with O
different O
length O
limit O
, O
where O
we O
found O
that O
our O
new O
methods O
are O
stable O
across O
different O
summary O
lengths O
( O
Figure O
. O
5 O
) O
. O
6 O
Conclusion O
and O
Future O
work O
Balancing O
sentence O
importance O
and O
redundancy O
is O
a O
key O
problem O
in O
extractive O
summarization O
. O
By O
examining O
large O
summarization O
datasets O
, O
we O
Ô¨Ånd O
that O
longer O
documents O
tend O
to O
be O
more O
redundant O
. O
Therefore O
in O
this O
paper O
, O
we O
systematically O
explore O
and O
compare O
existing O
and O
newly O
proposed O
methods523Figure O
4 O
: O
Comparing O
the O
average O
ROUGE O
scores O
and O
average O
unique O
n O
- O
gram O
ratios O
of O
different O
models O
on O
the O
Pubmed O
dataset O
, O
conditioned O
on O
different O
degrees O
of O
redundancy O
and O
lengths O
of O
the O
document O
( O
extremely O
long O
documents O
- O
i.e. O
, O
1 O
% O
of O
the O
dataset O
are O
not O
shown O
because O
of O
space O
constraints).7 O
Figure O
5 O
: O
Comparing O
the O
average O
ROUGE O
scores O
and O
average O
unique O
n O
- O
gram O
ratios O
of O
different O
models O
with O
different O
word O
length O
limits O
on O
the O
Pubmed O
dataset O
. O
See O
Appendices O
for O
similar O
results O
on O
arXiv.for O
redundancy O
reduction O
in O
summarizing O
long O
documents O
. O
Experiments O
indicate O
that O
our O
novel O
methods O
achieve O
SOTA O
on O
the O
ROUGE O
scores O
, O
while O
signiÔ¨Åcantly O
reducing O
redundancy O
on O
two O
scientiÔ¨Åc O
paper O
datasets O
( O
Pubmed O
and O
arXiv O
) O
. O
Interestingly O
, O
we O
show O
that O
redundancy O
reduction O
in O
sentence O
selection O
is O
more O
effective O
than O
in O
the O
sentence O
scoring O
phase O
, O
a O
Ô¨Ånding O
to O
be O
further O
investigated O
. O
Additional O
venues O
for O
future O
work O
include O
experimenting O
with O
generating O
summaries O
at O
Ô¨Åner O
granularity O
than O
sentences O
, O
as O
suggested O
by O
our O
analysis O
of O
the O
Œªparameter O
. O
We O
also O
intend O
to O
explore O
other O
ways O
to O
assess O
redundancy O
, O
moving O
from O
computing O
the O
cosine O
similarity O
between O
sentence O
embeddings O
, O
to O
a O
pre O
- O
trained O
neural O
model O
for O
sentence O
similarity O
. O
Finally O
, O
we O
plan O
to O
run O
human O
evaluations O
to O
assess O
the O
quality O
of O
the O
generated O
summaries O
. O
This O
is O
quite O
challenging O
for O
scientiÔ¨Åc O
papers O
, O
as O
it O
requires O
participants O
to O
possess O
sophisticated O
domain O
- O
speciÔ¨Åc O
background O
knowledge O
. O
Acknowledgments O
We O
thank O
reviewers O
and O
the O
UBC O
- O
NLP O
group O
for O
their O
insightful O
comments O
. O
This O
research O
was O
supported O
by O
the O
Language O
& O
Speech O
Innovation O
Lab O
of O
Cloud O
BU O
, O
Huawei O
Technologies O
Co. O
, O
Ltd.524References O
Keping O
Bi O
, O
Rahul O
Jha O
, O
W. O
Bruce O
Croft O
, O
and O
Asli O
Celikyilmaz O
. O
2020 O
. O
AREDSUM O
: O
Adaptive O
RedundancyAware O
Iterative O
Sentence O
Ranking O
for O
Extractive O
Document O
Summarization O
. O
Jaime O
Carbonell O
and O
Jade O
Goldstein O
. O
1998 O
. O
The O
use O
of O
mmr O
, O
diversity O
- O
based O
reranking O
for O
reordering O
documents O
and O
producing O
summaries O
. O
In O
Proceedings O
of O
the O
21st O
Annual O
International O
ACM O
SIGIR O
Conference O
on O
Research O
and O
Development O
in O
Information O
Retrieval O
, O
SIGIR O
‚Äô O
98 O
, O
pages O
335‚Äì336 O
, O
New O
York O
, O
NY O
, O
USA O
. O
ACM O
. O
Jianpeng O
Cheng O
and O
Mirella O
Lapata O
. O
2016 O
. O
Neural O
summarization O
by O
extracting O
sentences O
and O
words O
. O
In O
Proceedings O
of O
the O
54th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
484‚Äì494 O
, O
Berlin O
, O
Germany O
. O
Association O
for O
Computational O
Linguistics O
. O
Arman O
Cohan O
, O
Franck O
Dernoncourt O
, O
Doo O
Soon O
Kim O
, O
Trung O
Bui O
, O
Seokhwan O
Kim O
, O
Walter O
Chang O
, O
and O
Nazli O
Goharian O
. O
2018 O
. O
A O
discourse O
- O
aware O
attention O
model O
for O
abstractive O
summarization O
of O
long O
documents O
. O
CoRR O
, O
abs/1804.05685 O
. O
Guy O
Feigenblat O
, O
Haggai O
Roitman O
, O
Odellia O
Boni O
, O
and O
David O
Konopnicki O
. O
2017 O
. O
Unsupervised O
queryfocused O
multi O
- O
document O
summarization O
using O
the O
cross O
entropy O
method O
. O
In O
Proceedings O
of O
the O
40th O
International O
ACM O
SIGIR O
Conference O
on O
Research O
and O
Development O
in O
Information O
Retrieval O
, O
SIGIR O
‚Äô O
17 O
, O
page O
961‚Äì964 O
, O
New O
York O
, O
NY O
, O
USA O
. O
Association O
for O
Computing O
Machinery O
. O
Taehee O
Jung O
, O
Dongyeop O
Kang O
, O
Lucas O
Mentch O
, O
and O
Eduard O
Hovy O
. O
2019 O
. O
Earlier O
Is O
n‚Äôt O
Always O
Better O
: O
Subaspect O
Analysis O
on O
Corpus O
and O
System O
Biases O
in O
Summarization O
. O
pages O
3322‚Äì3333 O
. O
Chris O
Kedzie O
, O
Kathleen O
McKeown O
, O
and O
Hal O
Daum O
¬¥ O
e O
III O
. O
2018 O
. O
Content O
selection O
in O
deep O
learning O
models O
of O
summarization O
. O
In O
Proceedings O
of O
the O
2018 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
1818‚Äì1828 O
, O
Brussels O
, O
Belgium O
. O
Association O
for O
Computational O
Linguistics O
. O
Logan O
Lebanoff O
, O
John O
Muchovej O
, O
Franck O
Dernoncourt O
, O
Doo O
Soon O
Kim O
, O
Seokhwan O
Kim O
, O
Walter O
Chang O
, O
and O
Fei O
Liu O
. O
2019 O
. O
Analyzing O
sentence O
fusion O
in O
abstractive O
summarization O
. O
In O
Proceedings O
of O
the O
2nd O
Workshop O
on O
New O
Frontiers O
in O
Summarization O
, O
pages O
104‚Äì110 O
, O
Hong O
Kong O
, O
China O
. O
Association O
for O
Computational O
Linguistics O
. O
Siyao O
Li O
, O
Deren O
Lei O
, O
Pengda O
Qin O
, O
and O
William O
Yang O
Wang O
. O
2019 O
. O
Deep O
reinforcement O
learning O
with O
distributional O
semantic O
rewards O
for O
abstractive O
summarization O
. O
In O
Proceedings O
of O
the O
2019 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
and O
the O
9th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
( O
EMNLP O
- O
IJCNLP O
) O
, O
pages O
6040‚Äì6046 O
, O
Hong O
Kong O
, O
China O
. O
Association O
for O
Computational O
Linguistics O
. O
Chin O
- O
Yew O
Lin O
. O
2004 O
. O
ROUGE O
: O
A O
package O
for O
automatic O
evaluation O
of O
summaries O
. O
In O
Text O
Summarization O
Branches O
Out O
, O
pages O
74‚Äì81 O
, O
Barcelona O
, O
Spain O
. O
Association O
for O
Computational O
Linguistics O
. O
Hui O
Lin O
, O
Jeff O
Bilmes O
, O
and O
Shasha O
Xie O
. O
2009 O
. O
Graphbased O
submodular O
selection O
for O
extractive O
summarization O
. O
Proceedings O
of O
the O
2009 O
IEEE O
Workshop O
on O
Automatic O
Speech O
Recognition O
and O
Understanding O
, O
ASRU O
2009 O
, O
pages O
381‚Äì386 O
. O
Yang O
Liu O
and O
Mirella O
Lapata O
. O
2019 O
. O
Text O
summarization O
with O
pretrained O
encoders O
. O
In O
Proceedings O
of O
the O
2019 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
and O
the O
9th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
( O
EMNLP O
- O
IJCNLP O
) O
, O
pages O
3730‚Äì3740 O
, O
Hong O
Kong O
, O
China O
. O
Association O
for O
Computational O
Linguistics O
. O
Elena O
Lloret O
and O
Manuel O
Sanz O
. O
2013 O
. O
Tackling O
redundancy O
in O
text O
summarization O
through O
different O
levels O
of O
language O
analysis O
. O
Computer O
Standards O
Interfaces O
, O
35:507‚Äì518 O
. O
Ramesh O
Nallapati O
, O
Feifei O
Zhai O
, O
and O
Bowen O
Zhou O
. O
2016a O
. O
Summarunner O
: O
A O
recurrent O
neural O
network O
based O
sequence O
model O
for O
extractive O
summarization O
of O
documents O
. O
CoRR O
, O
abs/1611.04230 O
. O
Ramesh O
Nallapati O
, O
Bowen O
Zhou O
, O
Cicero O
dos O
Santos O
, O
C O
¬∏ O
aÀòglar O
Gulc O
¬∏ehre O
, O
and O
Bing O
Xiang O
. O
2016b O
. O
Abstractive O
text O
summarization O
using O
sequence O
- O
to O
- O
sequence O
RNNs O
and O
beyond O
. O
In O
Proceedings O
of O
The O
20th O
SIGNLL O
Conference O
on O
Computational O
Natural O
Language O
Learning O
, O
pages O
280‚Äì290 O
, O
Berlin O
, O
Germany O
. O
Association O
for O
Computational O
Linguistics O
. O
Shashi O
Narayan O
, O
Shay O
B. O
Cohen O
, O
and O
Mirella O
Lapata O
. O
2018 O
. O
Do O
n‚Äôt O
give O
me O
the O
details O
, O
just O
the O
summary O
! O
topic O
- O
aware O
convolutional O
neural O
networks O
for O
extreme O
summarization O
. O
In O
Proceedings O
of O
the O
2018 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
1797‚Äì1807 O
, O
Brussels O
, O
Belgium O
. O
Association O
for O
Computational O
Linguistics O
. O
Romain O
Paulus O
, O
Caiming O
Xiong O
, O
and O
Richard O
Socher O
. O
2017 O
. O
A O
deep O
reinforced O
model O
for O
abstractive O
summarization O
. O
CoRR O
, O
abs/1705.04304 O
. O
Jeffrey O
Pennington O
, O
Richard O
Socher O
, O
and O
Christopher O
D. O
Manning O
. O
2014 O
. O
Glove O
: O
Global O
vectors O
for O
word O
representation O
. O
In O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
( O
EMNLP O
) O
, O
pages O
1532‚Äì1543 O
. O
Maxime O
Peyrard O
, O
Teresa O
Botschen O
, O
and O
Iryna O
Gurevych O
. O
2017 O
. O
Learning O
to O
score O
system O
summaries O
for O
better O
content O
selection O
evaluation O
. O
InProceedings O
of O
the O
Workshop O
on O
New O
Frontiers O
in O
Summarization O
, O
pages O
74‚Äì84 O
, O
Copenhagen O
, O
Denmark O
. O
Association O
for O
Computational O
Linguistics O
. O
Yusu O
Qian O
, O
Urwa O
Muaz O
, O
Ben O
Zhang O
, O
and O
Jae O
Won O
Hyun O
. O
2019 O
. O
Reducing O
gender O
bias O
in O
word O
- O
level O
language O
models O
with O
a O
gender O
- O
equalizing O
loss O
function O
. O
In O
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of525the O
Association O
for O
Computational O
Linguistics O
: O
Student O
Research O
Workshop O
, O
pages O
223‚Äì228 O
, O
Florence O
, O
Italy O
. O
Association O
for O
Computational O
Linguistics O
. O
Pengjie O
Ren O
, O
Furu O
Wei O
, O
Zhumin O
Chen O
, O
Jun O
Ma O
, O
and O
Ming O
Zhou O
. O
2016 O
. O
A O
redundancy O
- O
aware O
sentence O
regression O
framework O
for O
extractive O
summarization O
. O
InProceedings O
of O
COLING O
2016 O
, O
the O
26th O
International O
Conference O
on O
Computational O
Linguistics O
: O
Technical O
Papers O
, O
pages O
33‚Äì43 O
, O
Osaka O
, O
Japan O
. O
The O
COLING O
2016 O
Organizing O
Committee O
. O
Horacio O
Saggion O
and O
Thierry O
Poibeau O
. O
2013 O
. O
Automatic O
Text O
Summarization O
: O
Past O
, O
Present O
and O
Future O
, O
pages O
3‚Äì21 O
. O
Springer O
Berlin O
Heidelberg O
, O
Berlin O
, O
Heidelberg O
. O
Jade O
Stewart O
and O
Jaime O
Carbonell O
. O
1998 O
. O
Summarization O
: O
( O
1 O
) O
using O
mmr O
for O
diversity O
- O
based O
reranking O
and O
( O
2 O
) O
evaluating O
summaries O
. O
pages O
181‚Äì195 O
. O
Oriol O
Vinyals O
, O
Meire O
Fortunato O
, O
and O
Navdeep O
Jaitly O
. O
2015 O
. O
Pointer O
networks O
. O
In O
C. O
Cortes O
, O
N. O
D. O
Lawrence O
, O
D. O
D. O
Lee O
, O
M. O
Sugiyama O
, O
and O
R. O
Garnett O
, O
editors O
, O
Advances O
in O
Neural O
Information O
Processing O
Systems O
28 O
, O
pages O
2692‚Äì2700 O
. O
Curran O
Associates O
, O
Inc. O
Wen O
Xiao O
and O
Giuseppe O
Carenini O
. O
2019 O
. O
Extractive O
summarization O
of O
long O
documents O
by O
combining O
global O
and O
local O
context O
. O
In O
Proceedings O
of O
the O
2019 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
and O
the O
9th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
( O
EMNLP O
- O
IJCNLP O
) O
, O
pages O
3002‚Äì3012 O
, O
Hong O
Kong O
, O
China O
. O
Association O
for O
Computational O
Linguistics O
. O
Qingyu O
Zhou O
, O
Nan O
Yang O
, O
Furu O
Wei O
, O
Shaohan O
Huang O
, O
Ming O
Zhou O
, O
and O
Tiejun O
Zhao O
. O
2018 O
. O
Neural O
document O
summarization O
by O
jointly O
learning O
to O
score O
and O
select O
sentences O
. O
In O
Proceedings O
of O
the O
56th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
654 O
‚Äì O
663 O
, O
Melbourne O
, O
Australia O
. O
Association O
for O
Computational O
Linguistics O
. O
A O
Appendices O
In O
these O
Appendices O
, O
we O
show O
more O
analysis O
of O
the O
experimental O
results O
. O
A.1 O
Analysis O
on O
arXiv O
Dataset O
under O
conditions O
Figure O
6 O
shows O
the O
performance O
on O
informativeness O
( O
ROUGE O
score O
) O
and O
redundancy O
( O
Unique O
N O
- O
gram O
Ratio O
) O
of O
different O
redundancy O
reduction O
methods O
under O
different O
conditions O
on O
the O
arXiv O
dataset O
. O
Comparing O
with O
the O
Pubmed O
dataset O
, O
the O
documents O
in O
the O
arXiv O
dataset O
tend O
to O
be O
longer O
and O
more O
redundant O
, O
as O
the O
majority O
of O
the O
documents O
in O
the O
Pubmed O
dataset O
have O
less O
than O
100 O
sentences O
with O
average O
Unique O
N O
- O
gram O
Ratio O
in O
the O
0.5‚àí0.6interval O
, O
while O
the O
majority O
of O
the O
documents O
in O
the O
arXiv O
dataset O
have O
number O
of O
sentences O
in O
the O
range O
100 O
to O
300 O
with O
average O
Unique O
N O
- O
gram O
Ratio O
in O
the0.6‚àí0.7interval O
. O
Consistent O
with O
the O
result O
on O
the O
Pubmed O
dataset O
, O
the O
Trigram O
Blocking O
method O
is O
the O
best O
choice O
for O
rather O
less O
redundant O
documents O
( O
with O
average O
Unique O
N O
- O
gram O
Ratio O
larger O
than0.7 O
) O
, O
and O
the O
MMR O
- O
Select+ O
is O
the O
one O
better O
or O
equivalent O
to O
the O
original O
model O
across O
different O
degree O
of O
redundancy O
, O
ignoring O
the O
outliers O
. O
With O
respect O
to O
the O
length O
of O
the O
documents O
, O
the O
MMRSelect+ O
and O
MMR O
- O
Select O
are O
consistently O
the O
most O
effective O
methods O
for O
balancing O
redundancy O
and O
informativeness O
on O
documents O
with O
different O
length O
. O
A.2 O
Analysis O
on O
Selection O
Overlap O
To O
explore O
the O
difference O
made O
by O
applying O
different O
redundancy O
reduction O
methods O
on O
the O
original O
method(ExtSumLG O
) O
, O
we O
compare O
the O
selected O
sentences O
by O
all O
the O
methods O
, O
and O
show O
the O
overlap O
ratios O
between O
every O
two O
methods O
, O
as O
well O
as O
the O
total O
number O
and O
the O
average O
length O
of O
selected O
sentences O
in O
the O
test O
set O
, O
in O
Table O
5 O
and O
Table O
6 O
for O
Pubmed O
dataset O
and O
arXiv O
dataset O
respectively O
. O
As O
we O
can O
see O
from O
the O
tables O
, O
except O
for O
the O
SR O
Decoder O
, O
all O
the O
other O
methods O
tend O
to O
select O
more O
and O
shorter O
sentences O
than O
the O
original O
summarizer O
. O
Regarding O
the O
overlap O
between O
the O
original O
method O
and O
the O
others O
, O
we O
observe O
that O
among O
all O
the O
three O
categories O
, O
the O
methods O
in O
category O
A O
tend O
to O
produce O
large O
differences O
, O
since O
these O
methods O
change O
the O
structure O
of O
the O
original O
model O
. O
Comparing O
the O
methods O
in O
Category O
C O
, O
around O
36 O
% O
of O
the O
sentences O
are O
regarded O
as O
redundant O
by O
Trigram O
Blocking O
, O
which O
means O
36 O
% O
of O
the O
sentences O
have O
trigram O
- O
overlap O
with O
other O
selected O
sentences O
, O
while O
only O
around O
10 O
% O
sentences O
are O
regarded O
as O
redundant O
by O
MMR O
- O
Select O
. O
As O
the O
ROUGE O
scores O
of O
MMR O
- O
Select O
are O
much O
better O
than O
Trigram O
Blocking O
on O
both O
datasets O
, O
this O
is O
in O
line O
with O
our O
analysis O
in O
Section O
5.3 O
, O
Triagram O
Blocking O
dropping O
some O
important O
sentences O
incorrectly O
. O
Interestingly O
, O
we O
notice O
that O
the O
overlap O
ratio O
between O
Trigram O
Block O
and O
MMR O
- O
Select O
is O
considerably O
larger O
than O
the O
overlap O
ratio O
of O
Trigram O
Block O
with O
original O
method O
( O
ExtSumLG O
) O
on O
both O
datasets O
. O
This O
indicates O
that O
there O
are O
some O
sentences O
, O
not O
selected O
by O
the O
original O
method O
, O
which O
are O
considered O
to O
be O
important O
by O
both O
the O
Trigram O
Blocking O
and O
MMR O
- O
Select O
methods.526Figure O
6 O
: O
Comparing O
the O
average O
ROUGE O
scores O
and O
average O
unique O
n O
- O
gram O
ratios O
of O
different O
models O
on O
the O
arXiv O
dataset O
, O
conditioned O
on O
different O
degrees O
of O
redundancy O
and O
lengths O
of O
the O
document.8 O
- O
ExtSumLG O
+ O
SR O
+ O
NeuSum O
+ O
RdLoss O
+ O
Tri O
- O
Block O
+ O
MMR O
- O
Select O
+ O
MMR O
- O
Select+ O
ExtSumLG O
100.00 O
72.84 O
52.00 O
77.70 O
60.77 O
87.71 O
85.75 O
+ O
SR O
72.66 O
100.00 O
49.73 O
70.29 O
52.24 O
69.78 O
70.64 O
+ O
Neusum O
60.44 O
57.94 O
100.00 O
60.77 O
48.47 O
60.38 O
61.07 O
+ O
RdLoss O
80.84 O
73.32 O
54.40 O
100.00 O
57.67 O
79.03 O
80.08 O
+ O
Tri O
- O
Block O
64.85 O
55.89 O
44.51 O
59.15 O
100.00 O
64.72 O
64.38 O
+ O
MMR O
- O
Select O
90.49 O
72.17 O
53.59 O
78.37 O
62.56 O
100.00 O
91.15 O
+ O
MMR O
- O
Select+ O
88.66 O
73.22 O
54.33 O
79.58 O
62.38 O
91.35 O
100.00 O
# O
Sent O
. O
Sel O
. O
36979 O
36888 O
42981 O
38476 O
39463 O
38151 O
38236 O
# O
words O
/ O
Sent O
40.66 O
40.84 O
33.38 O
38.95 O
37.21 O
39.35 O
39.31 O
Table O
5 O
: O
Micro O
overlap O
ratio O
( O
% O
) O
between O
the O
selections O
of O
different O
methods O
and O
the O
total O
number O
and O
the O
average O
length O
of O
selected O
sentences O
in O
the O
test O
set O
of O
Pubmed O
. O
A.3 O
Analysis O
on O
Recall O
and O
Precision O
of O
ROUGE O
Scores O
We O
also O
provide O
the O
Precision O
and O
Recall O
of O
the O
ROUGE O
scores O
in O
the O
main O
experiment O
, O
the O
results O
of O
Pubmed O
and O
arXiv O
datasets O
are O
shown O
in O
Table O
7 O
and O
Table O
8 O
, O
respectively O
. O
It O
is O
interesting O
to O
see O
that O
the O
NeuSum O
Decoder O
tends O
to O
have O
a O
high O
precision O
but O
low O
recall O
, O
indicating O
that O
the O
generated O
summaries O
tend O
to O
be O
shorter O
and O
contain O
less O
useful O
information O
than O
the O
original O
method O
. O
A.4 O
Analysis O
on O
the O
Relative O
Position O
of O
Selections O
We O
also O
show O
the O
relative O
position O
distribution O
of O
the O
selected O
sentences O
on O
both O
datasets O
in O
Figure O
7 O
to O
verify O
if O
any O
redundancy O
reduction O
method O
has O
aparticular O
tendency O
to O
select O
sentences O
in O
particular O
position O
of O
the O
documents O
. O
However O
, O
as O
shown O
in O
the O
Ô¨Ågure O
, O
the O
trends O
are O
all O
rather O
similar O
for O
all O
methods.527- O
ExtSumLG O
+ O
SR O
+ O
NeuSum O
+ O
RdLoss O
+ O
Tri O
- O
Block O
+ O
MMR O
- O
Select O
+ O
MMR O
- O
Select+ O
ExtSumLG O
100.00 O
72.06 O
- O
76.51 O
56.22 O
75.04 O
80.21 O
+ O
SR O
73.84 O
100.00 O
- O
69.07 O
49.16 O
62.82 O
67.03 O
+ O
Neusum O
- O
- O
- O
- O
- O
- O
+ O
RdLoss O
79.88 O
70.38 O
- O
100.00 O
53.00 O
67.81 O
72.57 O
+ O
Tri O
- O
Block O
64.59 O
55.12 O
- O
58.33 O
100.00 O
60.34 O
62.55 O
+ O
MMR O
- O
Select O
88.93 O
72.65 O
- O
76.97 O
62.23 O
100.00 O
93.13 O
+ O
MMR O
- O
Select+ O
89.96 O
73.36 O
- O
77.96 O
61.06 O
88.14 O
100.00 O
# O
Sent O
. O
Sel O
. O
39698 O
40681 O
- O
41448 O
45611 O
47045 O
44526 O
# O
words O
/ O
Sent O
36.26 O
35.52 O
- O
34.50 O
30.86 O
30.73 O
32.40 O
Table O
6 O
: O
Micro O
overlap O
ratio O
( O
% O
) O
between O
the O
selections O
of O
different O
methods O
and O
the O
total O
number O
and O
the O
average O
length O
of O
selected O
sentences O
in O
the O
test O
set O
of O
arXiv O
. O
Categ O
. O
ModelPubmed O
ROUGE-1 O
ROUGE-2 O
ROUGE O
- O
L O
Prec O
. O
Recall O
Prec O
. O
Recall O
Prec O
. O
Recall O
C O
Naive O
MMR O
36.45 O
42.56 O
11.05 O
12.64 O
31.39 O
36.53 O
- O
ExtSum O
- O
LG944.05 O
51.08 O
19.82 O
22.71 O
39.74 O
45.97 O
A O
+ O
SR O
Decoder O
44.00 O
51.10 O
19.75 O
22.68 O
39.66 O
45.96 O
A O
+ O
NeuSum O
Decoder O
44.36 O
49.24 O
19.74 O
21.58 O
40.29 O
44.62 O
B O
+ O
RdLoss O
44.30 O
51.09 O
20.11 O
22.88 O
40.09 O
46.11 O
C O
+ O
Trigram O
Blocking O
42.67 O
48.54 O
17.51 O
19.73 O
38.45 O
43.64 O
C O
+ O
MMR O
- O
Select O
44.25 O
51.09 O
19.98 O
22.75 O
40.08 O
46.07 O
C O
+ O
MMR O
- O
Select+ O
44.28 O
51.27 O
20.01 O
22.86 O
40.03 O
46.24 O
Table O
7 O
: O
Rouge O
Recall O
and O
Precision O
of O
different O
summarization O
models O
on O
the O
Pubmed O
dataset O
. O
Green O
numbers O
means O
it O
‚Äôs O
better O
than O
ExtSum O
- O
LG O
on O
the O
certain O
metric O
, O
and O
the O
red O
numbers O
means O
worse O
. O
Categ O
. O
ModelArxiv O
ROUGE-1 O
ROUGE-2 O
ROUGE O
- O
L O
Prec O
. O
Recall O
Prec O
. O
Recall O
Prec O
. O
Recall O
C O
Naive O
MMR O
29.61 O
42.69 O
7.45 O
10.78 O
24.92 O
35.82 O
- O
ExtSum O
- O
LG1038.60 O
54.64 O
15.38 O
22.00 O
34.17 O
48.26 O
A O
+ O
SR O
Decoder O
38.65 O
54.99 O
15.47 O
22.28 O
34.24 O
48.64 O
A O
+ O
NeuSum O
Decoder O
- O
- O
- O
- O
- O
B O
+ O
RdLoss O
38.92 O
54.77 O
15.68 O
22.29 O
34.60 O
48.59 O
C O
+ O
Trigram O
Blocking O
38.04 O
52.71 O
13.98 O
19.47 O
33.71 O
46.61 O
C O
+ O
MMR O
- O
Select O
38.85 O
54.33 O
15.39 O
21.74 O
34.56 O
48.24 O
C O
+ O
MMR O
- O
Select+ O
38.75 O
54.67 O
15.41 O
21.96 O
34.44 O
48.51 O
Table O
8 O
: O
Rouge O
Recall O
and O
Precision O
of O
different O
summarization O
models O
on O
the O
Pubmed O
dataset O
. O
Green O
numbers O
means O
it O
‚Äôs O
better O
than O
ExtSum O
- O
LG O
on O
the O
certain O
metric O
, O
and O
the O
red O
numbers O
means O
worse O
. O
Figure O
7 O
: O
The O
relative O
position O
distribution O
of O
different O
redundancy O
reduction O
methods O
on O
Pubmed(left O
) O
and O
arXiv(right O
) O
datasets.528Proceedings O
of O
the O
1st O
Conference O
of O
the O
Asia O
- O
PaciÔ¨Åc O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
10th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
529‚Äì535 O
December O
4 O
- O
7 O
, O
2020 O
. O
¬© O
2020 O
Association O
for O
Computational O
Linguistics O
A O
Cascade O
Approach O
to O
Neural O
Abstractive O
Summarization O
with O
Content O
Selection O
and O
Fusion O
Logan O
Lebanoff, O
‚ô† O
Franck O
Dernoncourt, O
‚ô¶ O
Doo O
Soon O
Kim, O
‚ô¶ O
Walter O
Chang, O
‚ô¶ O
Fei O
Liu O
‚ô† O
‚ô† O
Computer O
Science O
Department O
, O
University O
of O
Central O
Florida O
, O
Orlando O
, O
FL O
‚ô¶ O
Adobe O
Research O
, O
San O
Jose O
, O
CA O
loganlebanoff@knights.ucf.edu O
{ O
dernonco O
, O
dkim O
, O
wachang O
} O
@adobe.com O
feiliu@cs.ucf.edu O
Abstract O
We O
present O
an O
empirical O
study O
in O
favor O
of O
a O
cascade O
architecture O
to O
neural O
text O
summarization O
. O
Summarization O
practices O
vary O
widely O
but O
few O
other O
than O
news O
summarization O
can O
provide O
a O
sufÔ¨Åcient O
amount O
of O
training O
data O
enough O
to O
meet O
the O
requirement O
of O
end O
- O
to O
- O
end O
neural O
abstractive O
systems O
which O
perform O
content O
selection O
and O
surface O
realization O
jointly O
to O
generate O
abstracts O
. O
Such O
systems O
also O
pose O
a O
challenge O
to O
summarization O
evaluation O
, O
as O
they O
force O
content O
selection O
to O
be O
evaluated O
along O
with O
text O
generation O
, O
yet O
evaluation O
of O
the O
latter O
remains O
an O
unsolved O
problem O
. O
In O
this O
paper O
, O
we O
present O
empirical O
results O
showing O
that O
the O
performance O
of O
a O
cascaded O
pipeline O
that O
separately O
identiÔ¨Åes O
important O
content O
pieces O
and O
stitches O
them O
together O
into O
a O
coherent O
text O
is O
comparable O
to O
or O
outranks O
that O
of O
end O
- O
to O
- O
end O
systems O
, O
whereas O
a O
pipeline O
architecture O
allows O
for O
Ô¨Çexible O
content O
selection O
. O
We O
Ô¨Ånally O
discuss O
how O
we O
can O
take O
advantage O
of O
a O
cascaded O
pipeline O
in O
neural O
text O
summarization O
and O
shed O
light O
on O
important O
directions O
for O
future O
research O
. O
1 O
Introduction O
There O
is O
a O
variety O
of O
successful O
summarization O
applications O
but O
few O
can O
afford O
to O
have O
a O
large O
number O
of O
annotated O
examples O
that O
are O
sufÔ¨Åcient O
to O
meet O
the O
requirement O
of O
end O
- O
to O
- O
end O
neural O
abstractive O
summarization O
. O
Examples O
range O
from O
summarizing O
radiology O
reports O
( O
Jing O
et O
al O
. O
, O
2019 O
; O
Zhang O
et O
al O
. O
, O
2020 O
) O
to O
congressional O
bills O
( O
Kornilova O
and O
Eidelman O
, O
2019 O
) O
and O
meeting O
conversations O
( O
Mehdad O
et O
al O
. O
, O
2013 O
; O
Li O
et O
al O
. O
, O
2019 O
; O
Koay O
et O
al O
. O
, O
2020 O
) O
. O
The O
lack O
of O
annotated O
resources O
suggests O
that O
end O
- O
toend O
systems O
may O
not O
be O
a O
‚Äú O
one O
- O
size-Ô¨Åts O
- O
all O
‚Äù O
solution O
to O
neural O
text O
summarization O
. O
There O
is O
an O
increasing O
need O
to O
develop O
cascaded O
architectures O
to O
allow O
for O
customized O
content O
selectors O
to O
be O
combined O
with O
general O
- O
purpose O
neural O
text O
generatorsto O
realize O
the O
full O
potential O
of O
neural O
abstractive O
summarization O
. O
We O
advocate O
for O
explicit O
content O
selection O
as O
it O
allows O
for O
a O
rigorous O
evaluation O
and O
visualization O
of O
intermediate O
results O
of O
such O
a O
module O
, O
rather O
than O
associating O
it O
with O
text O
generation O
. O
Existing O
neural O
abstractive O
systems O
can O
perform O
content O
selection O
implicitly O
using O
end O
- O
to O
- O
end O
models O
( O
See O
et O
al O
. O
, O
2017 O
; O
Celikyilmaz O
et O
al O
. O
, O
2018 O
; O
Raffel O
et O
al O
. O
, O
2019 O
; O
Lewis O
et O
al O
. O
, O
2020 O
) O
, O
or O
more O
explicitly O
, O
with O
an O
external O
module O
to O
select O
important O
sentences O
or O
words O
to O
aid O
generation O
( O
Tan O
et O
al O
. O
, O
2017 O
; O
Gehrmann O
et O
al O
. O
, O
2018 O
; O
Chen O
and O
Bansal O
, O
2018 O
; O
Kry O
¬¥ O
sci¬¥nski O
et O
al O
. O
, O
2018 O
; O
Hsu O
et O
al O
. O
, O
2018 O
; O
Lebanoff O
et O
al O
. O
, O
2018 O
, O
2019b O
; O
Liu O
and O
Lapata O
, O
2019 O
) O
. O
However O
, O
content O
selection O
concerns O
not O
only O
the O
selection O
of O
important O
segments O
from O
a O
document O
, O
but O
also O
the O
cohesiveness O
of O
selected O
segments O
and O
the O
amount O
of O
text O
to O
be O
selected O
in O
order O
for O
a O
neural O
text O
generator O
to O
produce O
a O
summary O
. O
In O
this O
paper O
, O
we O
aim O
to O
investigate O
the O
feasibility O
of O
a O
cascade O
approach O
to O
neural O
text O
summarization O
. O
We O
explore O
a O
constrained O
summarization O
task O
, O
where O
an O
abstract O
is O
created O
one O
sentence O
at O
a O
time O
through O
a O
cascaded O
pipeline O
. O
Our O
pipeline O
architecture O
chooses O
one O
or O
two O
sentences O
from O
the O
source O
document O
, O
then O
highlights O
their O
summary O
- O
worthy O
segments O
and O
uses O
those O
as O
a O
basis O
for O
composing O
a O
summary O
sentence O
. O
When O
a O
pair O
of O
sentences O
are O
selected O
, O
it O
is O
important O
to O
ensure O
that O
they O
are O
fusible O
‚Äî O
there O
exists O
cohesive O
devices O
that O
tie O
the O
two O
sentences O
together O
into O
a O
coherent O
text O
‚Äî O
to O
avoid O
generating O
nonsensical O
outputs O
( O
Geva O
et O
al O
. O
, O
2019 O
; O
Lebanoff O
et O
al O
. O
, O
2020 O
) O
. O
Highlighting O
sentence O
segments O
allows O
us O
to O
perform O
Ô¨Åne O
- O
grained O
content O
selection O
that O
guides O
the O
neural O
text O
generator O
to O
stitch O
selected O
segments O
into O
a O
coherent O
sentence O
. O
The O
contributions O
of O
this O
work O
are O
summarized O
as O
follows.529SentPredHighlightNon O
- O
Highlight O
[ O
CLS]aduke O
thursdaySentenceSelectionFine O
- O
GrainedContent O
Selection O
Highlight O
studentNon O
- O
Highlight O
... O
... O
...... O
aduke O
thursdaystudent++++ O
duke O
< O
START O
> O
duke O
studentstudent O
has O
... O
Encoder O
Decoder O
wordembeddinghighlightembeddingGeneration O
Figure O
1 O
: O
Model O
architecture O
. O
We O
divide O
the O
task O
between O
two O
main O
components O
: O
the O
Ô¨Årst O
component O
performs O
sentence O
selection O
and O
Ô¨Åne O
- O
grained O
content O
selection O
, O
which O
are O
posed O
as O
a O
classiÔ¨Åcation O
problem O
and O
a O
sequencetagging O
problem O
, O
respectively O
. O
The O
second O
component O
receives O
the O
Ô¨Årst O
component O
‚Äôs O
outputs O
as O
supplementary O
information O
to O
generate O
the O
summary O
. O
A O
cascade O
architecture O
provides O
the O
necessary O
Ô¨Çexibility O
to O
separate O
content O
selection O
from O
surface O
realization O
in O
abstractive O
summarization O
. O
‚Ä¢We O
present O
an O
empirical O
study O
in O
favor O
of O
a O
cascade O
architecture O
for O
neural O
text O
summarization O
. O
Our O
cascaded O
pipeline O
chooses O
one O
or O
two O
sentences O
from O
the O
document O
and O
highlights O
their O
important O
segments O
; O
these O
segments O
are O
passed O
to O
a O
neural O
generator O
to O
produce O
a O
summary O
sentence O
. O
‚Ä¢Our O
quantitative O
results O
show O
that O
the O
performance O
of O
a O
cascaded O
pipeline O
is O
comparable O
to O
or O
outranks O
that O
of O
end O
- O
to O
- O
end O
systems O
, O
with O
added O
beneÔ¨Åt O
of O
Ô¨Çexible O
content O
selection O
. O
We O
discuss O
how O
we O
can O
take O
advantage O
of O
a O
cascade O
architecture O
and O
shed O
light O
on O
important O
directions O
for O
future O
research.1 O
2 O
A O
Cascade O
Approach O
Our O
cascaded O
summarization O
approach O
focuses O
on O
shallow O
abstraction O
. O
It O
makes O
use O
of O
text O
transformations O
such O
as O
sentence O
shortening O
, O
paraphrasing O
and O
fusion O
( O
Jing O
and O
McKeown O
, O
2000 O
) O
and O
is O
in O
contrast O
to O
deep O
abstraction O
, O
where O
a O
full O
semantic O
analysis O
of O
the O
document O
is O
often O
required O
. O
A O
shallow O
approach O
helps O
produce O
abstracts O
that O
convey O
important O
information O
while O
, O
crucially O
, O
remaining O
faithful O
to O
the O
original O
. O
In O
what O
follows O
, O
we O
describe O
our O
approach O
to O
select O
single O
sentences O
and O
sentence O
pairs O
from O
the O
document O
, O
highlight O
summary O
- O
worthy O
segments O
and O
perform O
summary O
generation O
conditioned O
on O
highlights O
. O
Selection O
of O
Singletons O
and O
Pairs O
Our O
approach O
iteratively O
selects O
one O
or O
two O
sentences O
from O
the O
input O
document O
; O
they O
serve O
as O
the O
basis O
for O
composing O
a O
single O
summary O
sentence O
. O
Previous O
research O
suggests O
that O
60 O
- O
85 O
% O
of O
human O
- O
written O
summary O
1Our O
code O
is O
publicly O
available O
at O
https://github O
. O
com O
/ O
ucfnlp O
/ O
cascaded O
- O
summsentences O
are O
created O
by O
shortening O
a O
single O
sentence O
or O
merging O
a O
pair O
of O
sentences O
( O
Lebanoff O
et O
al O
. O
, O
2019b O
) O
. O
We O
adopt O
this O
setting O
and O
present O
a O
coarse O
- O
to-Ô¨Åne O
strategy O
for O
content O
selection O
. O
Our O
strategy O
begins O
with O
selecting O
sentence O
singletons O
and O
pairs O
, O
followed O
by O
highlighting O
important O
segments O
of O
the O
sentences O
. O
Importantly O
, O
the O
strategy O
allows O
us O
to O
control O
which O
segments O
will O
be O
combined O
into O
a O
summary O
sentence‚Äî‚Äúcompatible O
‚Äù O
segments O
come O
from O
either O
a O
single O
document O
sentence O
or O
a O
pair O
of O
fusible O
sentences O
. O
In O
contrast O
, O
when O
all O
important O
segments O
of O
the O
document O
are O
provided O
to O
a O
neural O
generator O
all O
at O
once O
( O
Gehrmann O
et O
al O
. O
, O
2018 O
) O
, O
it O
can O
happen O
that O
the O
generator O
arbitrarily O
stitches O
together O
text O
segments O
from O
unrelated O
sentences O
, O
yielding O
a O
summary O
that O
contains O
hallucinated O
content O
and O
fails O
to O
retain O
the O
meaning O
of O
the O
original O
document O
( O
Falke O
et O
al O
. O
, O
2019 O
; O
Lebanoff O
et O
al O
. O
, O
2019a O
; O
Kryscinski O
et O
al O
. O
, O
2019 O
) O
. O
We O
expect O
a O
sentence O
singleton O
or O
pair O
to O
be O
selected O
from O
the O
document O
if O
it O
contains O
salient O
content O
. O
Moreover O
, O
a O
pair O
of O
sentences O
should O
contain O
content O
that O
is O
compatible O
with O
each O
other O
. O
Given O
a O
sentence O
or O
pair O
of O
sentences O
from O
the O
document O
, O
our O
model O
predicts O
whether O
it O
is O
a O
valid O
instance O
to O
be O
compressed O
or O
merged O
to O
form O
a O
summary O
sentence O
. O
We O
follow O
( O
Lebanoff O
et O
al O
. O
, O
2019b O
) O
to O
use O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
to O
perform O
the O
classiÔ¨Åcation O
. O
BERT O
is O
a O
natural O
choice O
since O
it O
takes O
one O
or O
two O
sentences O
and O
generates O
a O
classiÔ¨Åcation O
prediction O
. O
It O
treats O
an O
input O
singleton O
or O
pair O
of O
sentences O
as O
a O
sequence O
of O
tokens O
. O
The O
tokens O
are O
fed O
to O
a O
series O
of O
Transformer O
block O
layers O
, O
consisting O
of O
multi O
- O
head O
self O
- O
attention O
modules O
. O
The O
Ô¨Årst O
Transformer O
layer O
creates O
a O
contextual O
representation O
for O
each O
token O
, O
and O
each O
successive O
layer O
further O
reÔ¨Ånes O
those O
representations O
. O
An O
additional530R O
- O
L O
F O
- O
score O
( O
% O
) O
R-2 O
F O
- O
score O
( O
% O
) O
R-1 O
F O
- O
score O
( O
% O
) O
Probability O
ThresholdingProportional O
to O
Input O
( O
All O
Sents)Proportional O
to O
Input O
( O
1~2 O
Sents)Threshold O
ValueThreshold O
ValueThreshold O
Value O
Percentage O
of O
Words O
( O
% O
) O
Percentage O
of O
Words O
( O
% O
) O
Percentage O
of O
Words O
( O
% O
) O
Figure O
2 O
: O
Comparison O
of O
various O
highlighting O
strategies O
. O
Thresholding O
obtains O
the O
best O
performance O
. O
[ O
CLS O
] O
token O
is O
added O
to O
contain O
the O
sentence O
representation O
. O
BERT O
is O
Ô¨Åne O
- O
tuned O
for O
our O
task O
by O
adding O
an O
output O
layer O
on O
top O
of O
the O
Ô¨Ånal O
layer O
representation O
hL O
[ O
CLS O
] O
for O
sequence O
s O
, O
as O
seen O
in O
Eq O
. O
( O
1 O
) O
. O
psent(s O
) O
= O
œÉ(u O
/ O
latticetophL O
[ O
CLS O
] O
) O
( O
1 O
) O
where O
uis O
a O
vector O
of O
weights O
and O
œÉis O
the O
sigmoid O
function O
. O
The O
model O
predicts O
psent O
‚Äì O
whether O
the O
sentence O
singleton O
or O
pair O
is O
an O
appropriate O
one O
based O
on O
the O
[ O
CLS O
] O
token O
representation O
. O
We O
describe O
the O
training O
data O
for O
this O
task O
in O
¬ß O
3 O
. O
Fine O
- O
Grained O
Content O
Selection O
It O
is O
interesting O
to O
note O
that O
the O
previous O
architecture O
can O
be O
naturally O
extended O
to O
perform O
Ô¨Åne O
- O
grained O
content O
selection O
by O
highlighting O
important O
words O
of O
sentences O
. O
When O
two O
sentences O
are O
selected O
to O
generate O
a O
fusion O
sentence O
, O
it O
is O
desirable O
to O
identify O
segments O
of O
text O
from O
these O
sentences O
that O
are O
potentially O
compatible O
with O
each O
other O
. O
The O
coarse O
- O
toÔ¨Åne O
method O
allows O
us O
to O
examine O
the O
intermediate O
results O
and O
compare O
them O
with O
ground O
- O
truth O
. O
Concretely O
, O
we O
add O
a O
classiÔ¨Åcation O
layer O
to O
the O
Ô¨Ånal O
layer O
representation O
hL O
ifor O
each O
token O
wi(Eq O
. O
( O
2 O
) O
) O
. O
The O
per O
- O
target O
- O
word O
loss O
is O
then O
interpolated O
with O
instance O
prediction O
( O
one O
or O
two O
sentences O
) O
loss O
using O
a O
coefÔ¨Åcient O
Œª O
. O
Such O
a O
multi O
- O
task O
learning O
objective O
has O
been O
shown O
to O
improve O
performance O
on O
a O
number O
of O
tasks O
( O
Guo O
et O
al O
. O
, O
2019 O
) O
. O
phighlight O
( O
wi O
) O
= O
œÉ(v O
/ O
latticetophL O
i O
) O
( O
2 O
) O
where O
vis O
a O
vector O
of O
weights O
and O
œÉis O
the O
sigmoid O
function O
. O
The O
model O
predicts O
phighlight O
for O
each O
token O
‚Äì O
whether O
the O
token O
should O
be O
included O
in O
the O
output O
fusion O
, O
calculated O
based O
on O
the O
given O
token O
‚Äôs O
representation O
. O
Information O
Fusion O
Given O
one O
or O
two O
sentences O
taken O
from O
a O
document O
and O
their O
Ô¨Åne O
- O
grained O
highlights O
, O
we O
proceed O
by O
describing O
a O
fusion O
process O
that O
generates O
a O
summary O
sentence O
from O
the O
selected O
content O
. O
Our O
model O
employs O
an O
encoderdecoder O
architecture O
based O
on O
pointer O
- O
generatornetworks O
that O
has O
shown O
strong O
performance O
on O
its O
own O
and O
with O
adaptations O
( O
See O
et O
al O
. O
, O
2017 O
; O
Gehrmann O
et O
al O
. O
, O
2018 O
) O
. O
We O
feed O
the O
sentence O
singleton O
or O
pair O
to O
the O
encoder O
along O
with O
highlights O
derived O
by O
the O
Ô¨Åne O
- O
grained O
content O
selector O
, O
the O
latter O
come O
in O
the O
form O
of O
binary O
tags O
. O
The O
tags O
are O
transformed O
to O
a O
‚Äú O
highlight O
- O
on O
‚Äù O
embedding O
for O
each O
token O
if O
it O
is O
chosen O
by O
the O
content O
selector O
, O
and O
a O
‚Äú O
highlight O
- O
off O
‚Äù O
embedding O
for O
each O
token O
not O
chosen O
. O
The O
highlight O
- O
on O
/ O
off O
embeddings O
are O
added O
to O
token O
embeddings O
in O
an O
element O
- O
wise O
manner O
; O
both O
highlight O
and O
token O
embeddings O
are O
learned O
. O
An O
illustration O
is O
shown O
in O
Figure O
1 O
. O
Highlights O
provide O
a O
valuable O
intermediate O
representation O
suitable O
for O
shallow O
abstraction O
. O
Our O
approach O
thus O
provides O
an O
alternative O
to O
methods O
that O
use O
more O
sophisticated O
representations O
such O
as O
syntactic O
/ O
semantic O
graphs O
( O
Filippova O
and O
Strube O
, O
2008 O
; O
Banarescu O
et O
al O
. O
, O
2013 O
; O
Liu O
et O
al O
. O
, O
2015 O
) O
. O
It O
is O
more O
straightforward O
to O
incorporate O
highlights O
into O
an O
encoder O
- O
decoder O
fusion O
model O
, O
and O
obtaining O
highlights O
through O
sequence O
tagging O
can O
be O
potentially O
adapted O
to O
new O
domains O
. O
3 O
Experimental O
Results O
Data O
and O
Annotation O
To O
enable O
direct O
comparison O
with O
end O
- O
to O
- O
end O
systems O
, O
we O
conduct O
experiments O
on O
the O
widely O
used O
CNN O
/ O
DM O
dataset O
( O
See O
et O
al O
. O
, O
2017 O
) O
to O
report O
results O
of O
our O
cascade O
approach O
. O
We O
use O
the O
procedure O
described O
in O
Lebanoff O
et O
al O
. O
( O
2019b O
) O
to O
create O
training O
instances O
for O
the O
sentence O
selector O
and O
Ô¨Åne O
- O
grained O
content O
selector O
. O
Our O
training O
data O
contains O
1,053,993 O
instances O
; O
every O
instance O
contains O
one O
or O
two O
candidate O
sentences O
. O
It O
is O
a O
positive O
instance O
if O
a O
groundtruth O
summary O
sentence O
can O
be O
formed O
by O
compressing O
or O
merging O
sentences O
of O
the O
instance O
, O
negative O
otherwise O
. O
For O
positive O
instances O
, O
we O
highlight O
all O
lemmatized O
unigrams O
appearing O
in O
the O
summary O
, O
excluding O
punctuation O
. O
We O
further O
add O
smoothing O
to O
the O
labels O
by O
highlighting O
single O
words O
that O
con-531System O
R-1 O
R-2 O
R O
- O
L O
SumBasic O
( O
Vanderwende O
et O
al O
. O
, O
2007 O
) O
34.11 O
11.13 O
31.14 O
LexRank O
( O
Erkan O
and O
Radev O
, O
2004 O
) O
35.34 O
13.31 O
31.93 O
Pointer O
- O
Generator O
( O
See O
et O
al O
. O
, O
2017 O
) O
39.53 O
17.28 O
36.38 O
FastAbsSum O
( O
Chen O
and O
Bansal O
, O
2018 O
) O
40.88 O
17.80 O
38.54 O
BERT O
- O
Extr O
( O
Lebanoff O
et O
al O
. O
, O
2019b O
) O
41.13 O
18.68 O
37.75 O
BottomUp O
( O
Gehrmann O
et O
al O
. O
, O
2018 O
) O
41.22 O
18.68 O
38.34 O
BERT O
- O
Abs O
( O
Lebanoff O
et O
al O
. O
, O
2019b O
) O
37.15 O
15.22 O
34.60 O
Cascade O
- O
Fusion O
( O
Ours O
) O
40.10 O
17.61 O
36.71 O
Cascade O
- O
Tag O
( O
Ours O
) O
40.24 O
18.33 O
36.14 O
GT O
- O
Sent O
+ O
Sys O
- O
Tag O
50.40 O
27.74 O
46.25 O
GT O
- O
Sent O
+ O
Sys O
- O
Tag O
+ O
Fusion O
51.33 O
28.08 O
47.50 O
GT O
- O
Sent O
+ O
GT O
- O
Tag O
74.80 O
48.21 O
67.40 O
GT O
- O
Sent O
+ O
GT O
- O
Tag O
+ O
Fusion O
72.70 O
48.33 O
67.06(SYSTEM O
SENTS O
) O
ADuke O
student O
hasadmittedtohang O
inganoose O
made O
ofrope O
from O
atreenear O
astudent O
union O
, O
university O
ofÔ¨Åcials O
said O
Thursday O
. O
Thestudent O
was O
identiÔ¨ÅedduringaninvestigationbycampuspoliceandtheofÔ¨Åceofstudent O
affairs O
andadmittedtoplacingthe O
noose O
onthetreeearly O
Wednes O
day O
, O
the O
university O
said O
. O
( O
CASCADE O
-FUSION O
) O
ADuke O
student O
wasidentiÔ¨ÅedduringaninvestigationbycampuspoliceandtheofÔ¨Åceofstudent O
affairs O
andadmittedto O
placingthenoose O
onthetreeearly O
Wednes O
day O
. O
( O
GT O
S O
ENTS O
) O
In O
a O
news O
release O
, O
it O
said O
the O
student O
wasnolonger O
oncampusandwillface O
student O
conduct O
review O
. O
Duke O
Universityis O
a O
private O
college O
with O
about O
15,000 O
students O
in O
Durham O
, O
North O
Carolina O
. O
( O
GT O
S O
ENTS O
+ O
F O
USION O
) O
Duke O
Universitystudent O
wasnolonger O
oncampusandwillface O
student O
conduct O
review O
. O
( O
REFERENCE O
) O
Student O
isnolonger O
onDuke O
Universitycampusandwill O
face O
disciplinary O
review O
. O
Table O
1 O
: O
( O
L O
EFT O
) O
Summarization O
results O
on O
CNN O
/ O
DM O
test O
set O
. O
Our O
cascade O
approach O
performs O
comparable O
to O
strong O
extractive O
and O
abstractive O
baselines O
; O
oracle O
models O
using O
ground O
- O
truth O
sentences O
and O
segment O
highlights O
perform O
the O
best O
. O
( O
R O
IGHT O
) O
Example O
source O
sentences O
and O
their O
fusions O
. O
Dark O
highlighting O
is O
content O
taken O
from O
the O
Ô¨Årst O
sentence O
, O
and O
light O
highlighting O
comes O
from O
the O
second O
. O
Our O
Cascade O
- O
Fusion O
approach O
effectively O
performs O
entity O
replacement O
by O
replacing O
‚Äú O
student O
‚Äù O
in O
the O
second O
sentence O
with O
‚Äú O
a O
Duke O
student O
‚Äù O
from O
the O
Ô¨Årst O
sentence O
. O
nect O
two O
highlighted O
phrases O
and O
by O
dehighlighting O
isolated O
stopwords O
. O
At O
test O
time O
, O
four O
highestscored O
instances O
are O
selected O
per O
document O
; O
their O
important O
segments O
are O
highlighted O
by O
content O
selector O
then O
passed O
to O
the O
fusion O
step O
to O
produce O
a O
summary O
sentence O
each O
. O
The O
hyperparameter O
Œªfor O
weighing O
the O
per O
- O
target O
- O
word O
loss O
is O
set O
to O
0.2 O
and O
highlighting O
threshold O
value O
is O
0.15 O
. O
The O
model O
hyperparameters O
are O
tuned O
on O
the O
validation O
split O
. O
Summarization O
Results O
We O
show O
experimental O
results O
on O
the O
standard O
test O
set O
and O
evaluated O
by O
ROUGE O
metrics O
( O
Lin O
, O
2004 O
) O
in O
Table O
1 O
. O
The O
performance O
of O
our O
cascade O
approaches O
, O
Cascade O
- O
Fusion O
andCascade O
- O
Tag O
, O
is O
comparable O
to O
or O
outranks O
a O
number O
of O
extractive O
and O
abstractive O
baselines O
. O
Particularly O
, O
Cascade O
- O
Tag O
does O
not O
use O
a O
fusion O
step O
( O
¬ß O
2 O
) O
and O
is O
the O
output O
of O
Ô¨Åne O
- O
grained O
content O
selection O
. O
Cascade O
- O
Fusion O
provides O
a O
direct O
comparison O
against O
BERT O
- O
Abs O
( O
Lebanoff O
et O
al O
. O
, O
2019b O
) O
that O
uses O
sentence O
selection O
and O
fusion O
but O
lacks O
a O
Ô¨Åne O
- O
grained O
content O
selector O
. O
Our O
results O
suggest O
that O
a O
coarse O
- O
to-Ô¨Åne O
content O
selection O
strategy O
remains O
necessary O
to O
guide O
the O
fusion O
model O
to O
produce O
informative O
sentences O
. O
We O
observe O
that O
the O
addition O
of O
the O
fusion O
model O
has O
only O
a O
moderate O
impact O
on O
ROUGE O
scores O
, O
but O
the O
fusion O
process O
can O
reorder O
text O
segments O
to O
create O
true O
and O
grammatical O
sentences O
, O
as O
shown O
in O
Table O
1 O
. O
We O
analyze O
the O
performance O
of O
a O
number O
of O
oracle O
models O
that O
use O
ground O
- O
truth O
sentence O
selection O
( O
GT O
- O
Sent O
) O
and O
tagging O
( O
GT O
- O
Tag O
) O
. O
When O
given O
ground O
- O
truth O
sentences O
as O
input O
, O
our O
cascademodels O
achieve‚àº10 O
points O
of O
improvement O
in O
all O
ROUGE O
metrics O
. O
When O
the O
models O
are O
also O
given O
ground O
- O
truth O
highlights O
, O
they O
achieve O
an O
additional O
20 O
points O
of O
improvement O
. O
In O
a O
preliminary O
examination O
, O
we O
observe O
that O
not O
all O
highlights O
are O
included O
in O
the O
summary O
during O
fusion O
, O
indicating O
there O
is O
space O
for O
improvement O
. O
These O
results O
show O
that O
cascade O
architectures O
have O
great O
potential O
to O
generate O
shallow O
abstracts O
and O
future O
emphasis O
may O
be O
placed O
on O
accurate O
content O
selection O
. O
How O
much O
should O
we O
highlight O
? O
It O
is O
important O
to O
quantify O
the O
amount O
of O
highlighting O
required O
for O
generating O
a O
summary O
sentence O
. O
Highlighting O
too O
much O
or O
too O
little O
can O
be O
unhelpful O
. O
We O
experiment O
with O
three O
methods O
to O
determine O
the O
appropriate O
amount O
of O
words O
to O
highlight O
. O
Probability O
Thresholding O
chooses O
a O
set O
threshold O
whereby O
all O
words O
that O
have O
a O
probability O
higher O
than O
the O
threshold O
are O
highlighted O
. O
When O
Proportional O
to O
Input O
is O
used O
, O
the O
highest O
probability O
words O
are O
iteratively O
highlighted O
until O
a O
target O
rate O
is O
reached O
. O
The O
amount O
of O
highlighting O
can O
be O
proportional O
to O
the O
total O
number O
of O
words O
per O
instance O
( O
one O
or O
two O
sentences O
) O
or O
per O
document O
, O
containing O
all O
sentences O
selected O
for O
the O
document O
. O
We O
investigate O
the O
effect O
of O
varying O
the O
amount O
of O
highlighting O
in O
Figure O
2 O
. O
Among O
the O
three O
methods O
, O
probability O
thresholding O
performs O
the O
best O
, O
as O
it O
gives O
more O
freedom O
to O
content O
selection O
. O
If O
the O
model O
scores O
all O
of O
the O
words O
in O
sentences O
highly O
, O
then O
we O
should O
correspondingly O
highlight O
all O
of O
the O
words O
. O
If O
only O
very O
few O
words O
score O
highly O
, O
then532we O
should O
only O
pick O
those O
few O
. O
Highlighting O
a O
certain O
percentage O
of O
words O
tend O
to O
perform O
less O
well O
. O
On O
our O
dataset O
, O
a O
threshold O
value O
of O
0.15‚Äì0.20 O
produces O
the O
best O
ROUGE O
scores O
. O
Interestingly O
, O
these O
thresholds O
end O
up O
highlighting O
58‚Äì78 O
% O
of O
the O
words O
of O
each O
sentence O
. O
Compared O
to O
what O
the O
generator O
was O
trained O
on O
, O
which O
had O
a O
median O
of O
31 O
% O
of O
each O
sentence O
highlighted O
, O
the O
system O
‚Äôs O
rate O
of O
highlighting O
is O
higher O
. O
If O
the O
model O
‚Äôs O
highlighting O
rate O
is O
set O
to O
be O
similar O
to O
that O
of O
the O
ground O
- O
truth O
, O
it O
yields O
much O
lower O
ROUGE O
scores O
( O
cf O
. O
threshold O
value O
of O
0.3 O
in O
Figure O
2 O
) O
. O
This O
observation O
suggests O
that O
the O
amount O
of O
highlighting O
can O
be O
related O
to O
the O
effectiveness O
of O
content O
selector O
and O
it O
may O
be O
better O
to O
highlight O
more O
than O
less O
. O
4 O
Conclusion O
We O
present O
a O
cascade O
approach O
to O
neural O
abstractive O
summarization O
that O
separates O
content O
selection O
from O
surface O
realization O
. O
Importantly O
, O
our O
approach O
makes O
use O
of O
text O
highlights O
as O
intermediate O
representation O
; O
they O
are O
derived O
from O
one O
or O
two O
sentences O
using O
a O
coarse O
- O
to-Ô¨Åne O
content O
selection O
strategy O
, O
then O
passed O
to O
a O
neural O
text O
generator O
to O
compose O
a O
summary O
sentence O
. O
A O
successful O
cascade O
approach O
is O
expected O
to O
accurately O
select O
sentences O
and O
highlight O
an O
appropriate O
amount O
of O
text O
, O
both O
can O
be O
customized O
for O
domain O
- O
speciÔ¨Åc O
tasks O
. O
Acknowledgments O
We O
are O
grateful O
to O
the O
anonymous O
reviewers O
for O
their O
comments O
and O
suggestions O
. O
This O
research O
was O
supported O
in O
part O
by O
the O
National O
Science O
Foundation O
grant O
IIS-1909603 O
. O
Abstract O
Cross O
- O
lingual O
Summarization O
( O
CLS O
) O
aims O
at O
producing O
a O
summary O
in O
the O
target O
language O
for O
an O
article O
in O
the O
source O
language O
. O
Traditional O
solutions O
employ O
a O
twostep O
approach O
, O
i.e. O
translate O
‚Üísummarize O
or O
summarize O
‚Üítranslate O
. O
Recently O
, O
end O
- O
to O
- O
end O
models O
have O
achieved O
better O
results O
, O
but O
these O
approaches O
are O
mostly O
limited O
by O
their O
dependence O
on O
large O
- O
scale O
labeled O
data O
. O
We O
propose O
a O
solution O
based O
on O
mixed O
- O
lingual O
pretraining O
that O
leverages O
both O
cross O
- O
lingual O
tasks O
such O
as O
translation O
and O
monolingual O
tasks O
like O
masked O
language O
models O
. O
Thus O
, O
our O
model O
can O
leverage O
the O
massive O
monolingual O
data O
to O
enhance O
its O
modeling O
of O
language O
. O
Moreover O
, O
the O
architecture O
has O
no O
task O
- O
speciÔ¨Åc O
components O
, O
which O
saves O
memory O
and O
increases O
optimization O
efÔ¨Åciency O
. O
We O
show O
in O
experiments O
that O
this O
pre O
- O
training O
scheme O
can O
effectively O
boost O
the O
performance O
of O
cross O
- O
lingual O
summarization O
. O
In O
Neural O
Cross O
- O
Lingual O
Summarization O
( O
NCLS O
) O
( O
Zhu O
et O
al O
. O
, O
2019b O
) O
dataset O
, O
our O
model O
achieves O
an O
improvement O
of O
2.82 O
( O
English O
to O
Chinese O
) O
and O
1.15 O
( O
Chinese O
to O
English O
) O
ROUGE-1 O
scores O
over O
state O
- O
of O
- O
the O
- O
art O
results O
. O
1 O
Introduction O
Text O
summarization O
can O
facilitate O
the O
propagation O
of O
information O
by O
providing O
an O
abridged O
version O
for O
long O
articles O
and O
documents O
. O
Meanwhile O
, O
the O
globalization O
progress O
has O
prompted O
a O
high O
demand O
of O
information O
dissemination O
across O
language O
barriers O
. O
Thus O
, O
the O
cross O
- O
lingual O
summarization O
( O
CLS O
) O
task O
emerges O
to O
provide O
accurate O
gist O
of O
articles O
in O
a O
foreign O
language O
. O
Traditionally O
, O
most O
CLS O
methods O
follow O
the O
twostep O
pipeline O
approach O
: O
either O
translate O
the O
article O
into O
the O
target O
language O
and O
then O
summarize O
it O
( O
Leuski O
et O
al O
. O
, O
2003 O
) O
, O
or O
summarize O
the O
article O
in O
the O
source O
language O
and O
then O
translate O
it O
( O
Wan O
‚àóEqual O
contributionet O
al O
. O
, O
2010 O
) O
. O
Although O
this O
method O
can O
leverage O
off O
- O
the O
- O
shelf O
summarization O
and O
MT O
models O
, O
it O
suffers O
from O
error O
accumulation O
from O
two O
independent O
subtasks O
. O
Therefore O
, O
several O
end O
- O
to O
- O
end O
approaches O
have O
been O
proposed O
recently O
( O
Zhu O
et O
al O
. O
, O
2019b O
; O
Ouyang O
et O
al O
. O
, O
2019 O
; O
Duan O
et O
al O
. O
, O
2019 O
) O
, O
which O
conduct O
both O
translation O
and O
summarization O
simultaneously O
. O
Easy O
to O
optimize O
as O
these O
methods O
are O
, O
they O
typically O
require O
a O
large O
amount O
of O
cross O
- O
lingual O
summarization O
data O
, O
which O
may O
not O
be O
available O
especially O
for O
low O
- O
resource O
languages O
. O
For O
instance O
, O
NCLS O
( O
Zhu O
et O
al O
. O
, O
2019b O
) O
proposes O
to O
co O
- O
train O
on O
monolingual O
summarization O
( O
MS O
) O
and O
machine O
translation O
( O
MT O
) O
tasks O
, O
both O
of O
which O
require O
tremendous O
labeling O
efforts O
. O
On O
the O
other O
hand O
, O
the O
pre O
- O
training O
strategy O
has O
proved O
to O
be O
very O
effective O
for O
language O
understanding O
( O
Devlin O
et O
al O
. O
, O
2018 O
; O
Holtzman O
et O
al O
. O
, O
2019 O
) O
and O
cross O
- O
lingual O
learning O
( O
Lample O
and O
Conneau O
, O
2019 O
; O
Chi O
et O
al O
. O
, O
2019 O
) O
. O
One O
of O
the O
advantages O
of O
pre O
- O
training O
is O
that O
many O
associated O
tasks O
are O
selflearning O
by O
nature O
, O
which O
means O
no O
labeled O
data O
is O
required O
. O
This O
greatly O
increases O
the O
amount O
of O
training O
data O
exposed O
to O
the O
model O
, O
thereby O
enhancing O
its O
performance O
on O
downstream O
tasks O
. O
Therefore O
, O
we O
leverage O
large O
- O
scale O
pre O
- O
training O
to O
improve O
the O
quality O
of O
cross O
- O
lingual O
summarization O
. O
Built O
upon O
a O
transformer O
- O
based O
encoderdecoder O
architecture O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
our O
model O
is O
pre O
- O
trained O
on O
both O
monolingual O
tasks O
including O
masked O
language O
model O
( O
MLM O
) O
, O
denoising O
autoencoder O
( O
DAE O
) O
and O
monolingual O
summarization O
( O
MS O
) O
, O
and O
cross O
- O
lingual O
tasks O
such O
as O
crosslingual O
masked O
language O
model O
( O
CMLM O
) O
and O
machine O
translation O
( O
MT O
) O
. O
This O
mixed O
- O
lingual O
pretraining O
scheme O
can O
take O
advantage O
of O
massive O
unlabeled O
monolingual O
data O
to O
improve O
the O
model O
‚Äôs O
language O
modeling O
capability O
, O
and O
leverage O
crosslingual O
tasks O
to O
improve O
the O
model O
‚Äôs O
cross O
- O
lingual O
representation O
. O
We O
then O
Ô¨Ånetune O
the O
model O
on O
the536downstream O
cross O
- O
lingual O
summarization O
task O
. O
Furthermore O
, O
based O
on O
a O
shared O
multi O
- O
lingual O
vocabulary O
, O
our O
model O
has O
a O
shared O
encoder O
- O
decoder O
architecture O
for O
all O
pre O
- O
training O
and O
Ô¨Ånetuning O
tasks O
, O
whereas O
NCLS O
( O
Zhu O
et O
al O
. O
, O
2019b O
) O
sets O
aside O
taskspeciÔ¨Åc O
decoders O
for O
machine O
translation O
, O
monolingual O
summarization O
, O
and O
cross O
- O
lingual O
summarization O
. O
In O
the O
experiments O
, O
our O
model O
outperforms O
various O
baseline O
systems O
on O
the O
benchmark O
dataset O
NCLS O
( O
Zhu O
et O
al O
. O
, O
2019b O
) O
. O
For O
example O
, O
our O
model O
achieves O
3.27 O
higher O
ROUGE-1 O
score O
in O
Chinese O
to O
English O
summarization O
than O
the O
state O
- O
of O
- O
the O
- O
art O
result O
and O
1.28 O
higher O
ROUGE-1 O
score O
in O
English O
to O
Chinese O
summarization O
. O
We O
further O
conduct O
an O
ablation O
study O
to O
show O
that O
each O
pretraining O
task O
contributes O
to O
the O
performance O
, O
especially O
our O
proposed O
unsupervised O
pretraining O
tasks O
. O
2 O
Related O
Work O
2.1 O
Pre O
- O
training O
Pre O
- O
training O
language O
models O
( O
Devlin O
et O
al O
. O
, O
2018 O
; O
Dong O
et O
al O
. O
, O
2019 O
) O
have O
been O
widely O
used O
in O
NLP O
applications O
such O
as O
question O
answering O
( O
Zhu O
et O
al O
. O
, O
2018 O
) O
, O
sentiment O
analysis O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
, O
and O
summarization O
( O
Zhu O
et O
al O
. O
, O
2019a O
; O
Yang O
et O
al O
. O
, O
2020 O
) O
. O
In O
multi O
- O
lingual O
scenarios O
, O
recent O
works O
take O
input O
from O
multiple O
languages O
and O
shows O
great O
improvements O
on O
cross O
- O
lingual O
classiÔ¨Åcation O
( O
Lample O
and O
Conneau O
, O
2019 O
; O
Pires O
et O
al O
. O
, O
2019 O
; O
Huang O
et O
al O
. O
, O
2019 O
) O
and O
unsupervised O
machine O
translation O
( O
Liu O
et O
al O
. O
, O
2020 O
) O
. O
Artetxe O
and O
Schwenk O
( O
2019 O
) O
employs O
the O
sequence O
encoder O
from O
a O
machine O
translation O
model O
to O
produce O
cross O
- O
lingual O
sentence O
embeddings O
. O
Chi O
et O
al O
. O
( O
2019 O
) O
uses O
multi O
- O
lingual O
pre O
- O
training O
to O
improve O
cross O
- O
lingual O
question O
generation O
and O
zero O
- O
shot O
cross O
- O
lingual O
summarization O
. O
Their O
model O
trained O
on O
articles O
and O
summaries O
in O
one O
language O
is O
directly O
used O
to O
produce O
summaries O
for O
articles O
in O
another O
language O
, O
which O
is O
different O
from O
our O
task O
of O
producing O
summaries O
of O
one O
language O
for O
an O
article O
from O
a O
foreign O
language O
. O
2.2 O
Cross O
- O
lingual O
Summarization O
Early O
literatures O
on O
cross O
- O
lingual O
summarization O
focus O
on O
the O
two O
- O
step O
approach O
involving O
machine O
translation O
and O
summarization O
( O
Leuski O
et O
al O
. O
, O
2003 O
; O
Wan O
et O
al O
. O
, O
2010 O
) O
, O
which O
often O
suffer O
from O
error O
propagation O
issues O
due O
to O
the O
imperfect O
modular O
systems O
. O
Recent O
end O
- O
to O
- O
end O
deep O
learning O
models O
have O
greatly O
enhanced O
the O
performance O
. O
Shen O
et O
al.(2018 O
) O
presents O
a O
solution O
to O
zero O
- O
shot O
cross O
- O
lingual O
headline O
generation O
by O
using O
machine O
translation O
and O
summarization O
datasets O
. O
Duan O
et O
al O
. O
( O
2019 O
) O
leverages O
monolingual O
abstractive O
summarization O
to O
achieve O
zero O
- O
shot O
cross O
- O
lingual O
abstractive O
sentence O
summarization O
. O
NCLS O
( O
Zhu O
et O
al O
. O
, O
2019b O
) O
proposes O
a O
cross O
- O
lingual O
summarization O
system O
for O
large O
- O
scale O
datasets O
for O
the O
Ô¨Årst O
time O
. O
It O
uses O
multitask O
supervised O
learning O
and O
shares O
the O
encoder O
for O
monolingual O
summarization O
, O
cross O
- O
lingual O
summarization O
, O
and O
machine O
translation O
. O
However O
, O
each O
of O
these O
tasks O
requires O
a O
separate O
decoder O
. O
In O
comparison O
, O
our O
model O
shares O
the O
entire O
encoder O
- O
decoder O
architecture O
among O
all O
pre O
- O
training O
and O
Ô¨Ånetuning O
tasks O
, O
and O
leverages O
unlabeled O
data O
for O
monolingual O
masked O
language O
model O
training O
. O
A O
concurrent O
work O
by O
Zhu O
et O
al O
. O
( O
2020 O
) O
improves O
the O
performance O
by O
combining O
the O
neural O
model O
with O
an O
external O
probabilistic O
bilingual O
lexicon O
. O
3 O
Method O
3.1 O
Pre O
- O
training O
Objectives O
We O
propose O
a O
set O
of O
multi O
- O
task O
pre O
- O
training O
objectives O
on O
both O
monolingual O
and O
cross O
- O
lingual O
corpus O
. O
For O
monolingual O
corpus O
, O
we O
use O
the O
masked O
language O
model O
( O
MLM O
) O
from O
Raffel O
et O
al O
. O
( O
2019 O
) O
. O
The O
input O
is O
the O
original O
sentence O
masked O
by O
sentinel O
tokens O
, O
and O
the O
target O
is O
the O
sequence O
consists O
of O
each O
sentinel O
token O
followed O
by O
the O
corresponding O
masked O
token O
. O
The O
other O
monolingual O
task O
is O
the O
denoising O
auto O
- O
encoder O
( O
DAE O
) O
, O
where O
the O
corrupted O
input O
is O
constructed O
by O
randomly O
dropping O
, O
masking O
, O
and O
shufÔ¨Çing O
a O
sentence O
and O
the O
target O
is O
the O
original O
sentence O
. O
Since O
our O
Ô¨Ånal O
task O
is O
summarization O
, O
we O
also O
include O
monolingual O
summarization O
( O
MS O
) O
as O
a O
pre O
- O
training O
task O
. O
To O
leverage O
cross O
- O
lingual O
parallel O
corpus O
, O
we O
introduce O
the O
cross O
- O
lingual O
masked O
language O
model O
( O
CMLM O
) O
. O
CMLM O
is O
an O
extension O
of O
MLM O
on O
the O
parallel O
corpus O
. O
The O
input O
is O
the O
concatenation O
of O
a O
sentence O
in O
language O
A O
and O
its O
translation O
in O
language O
B. O
We O
then O
randomly O
select O
one O
sentence O
and O
mask O
some O
of O
its O
tokens O
by O
sentinels O
. O
The O
target O
is O
to O
predict O
the O
masked O
tokens O
in O
the O
same O
way O
as O
MLM O
. O
Different O
from O
MLM O
, O
the O
masked O
tokens O
in O
CMLM O
are O
predicted O
not O
only O
from O
the O
context O
within O
the O
same O
language O
but O
also O
from O
their O
translations O
in O
another O
language O
, O
which O
encourages O
the O
model O
to O
learn O
language O
- O
invariant O
representations O
. O
Note O
that O
CMLM O
is O
similar O
to O
the O
Translation O
Language O
Model O
( O
TLM O
) O
loss O
proposed O
in O
Lample537Objective O
Supervised O
Multi O
- O
lingual O
Inputs O
Targets O
Masked O
Language O
Model O
France O
< O
X O
> O
Morocco O
in O
< O
Y O
> O
exhibition O
match O
. O
< O
X O
> O
beats O
< O
Y O
> O
an O
Denoising O
Auto O
- O
Encoder O
France O
beats O
< O
M O
> O
in O
< O
M O
> O
exhibition O
. O
France O
beats O
Morocco O
in O
an O
exhibition O
match O
. O
Monolingual O
Summarization O
/checkWorld O
champion O
France O
overcame O
a O
stuttering O
start O
to O
beat O
Morocco O
1 O
- O
0 O
in O
a O
scrappy O
exhibition O
match O
on O
Wednesday O
night O
. O
France O
beats O
Morocco O
in O
an O
exhibition O
match O
. O
Cross O
- O
lingual O
MLM O
/check O
/checkFrance O
< O
X O
> O
Morocco O
in O
< O
Y O
> O
exhibition O
match O
. O
Ê≥ïÂõΩÈòüÂú®‰∏ÄÂú∫Ë°®ÊºîËµõ‰∏≠ÂáªË¥•Êë©Ê¥õÂì•Èòü„ÄÇ O
< O
X O
> O
beats O
< O
Y O
> O
an O
Cross O
- O
lingual O
MLM O
/check O
/checkFrance O
beats O
Morocco O
in O
an O
exhibition O
match O
. O
< O
X O
> O
ÈòüÂú®‰∏ÄÂú∫Ë°®ÊºîËµõ‰∏≠ O
< O
Y O
> O
Êë©Ê¥õÂì•Èòü„ÄÇ O
< O
X O
> O
Ê≥ïÂõΩ O
< O
Y O
> O
ÂáªË¥• O
Machine O
Translation O
/check O
/check O
France O
beats O
Morocco O
in O
an O
exhibition O
match O
. O
Ê≥ïÂõΩÈòüÂú®‰∏ÄÂú∫Ë°®ÊºîËµõ‰∏≠ÂáªË¥•Êë©Ê¥õÂì•Èòü O
„ÄÇ O
Table O
1 O
: O
Examples O
of O
inputs O
and O
targets O
used O
by O
different O
objectives O
for O
the O
sentence O
‚Äú O
France O
beats O
Morocco O
in O
an O
exhibition O
match O
‚Äù O
with O
its O
Chinese O
translation O
. O
We O
use O
< O
X O
> O
and O
< O
Y O
> O
to O
denote O
sentinel O
tokens O
and O
< O
M O
> O
to O
denote O
shared O
mask O
tokens O
. O
and O
Conneau O
( O
2019 O
) O
. O
The O
key O
differences O
are O
: O
1 O
) O
TLM O
randomly O
masks O
tokens O
in O
sentences O
from O
both O
languages O
, O
while O
CMLM O
only O
masks O
tokens O
from O
one O
language O
; O
2 O
) O
TLM O
is O
applied O
on O
encoderonly O
networks O
while O
we O
employ O
CMLM O
on O
the O
encoder O
- O
decoder O
network O
. O
In O
addition O
to O
CMLM O
, O
we O
also O
include O
standard O
machine O
translation O
( O
MT O
) O
objective O
, O
in O
which O
the O
input O
and O
output O
are O
the O
unchanged O
source O
and O
target O
sentences O
, O
respectively O
. O
The O
examples O
of O
inputs O
and O
targets O
used O
by O
our O
pre O
- O
training O
objectives O
are O
shown O
in O
Table O
1 O
. O
3.2 O
UniÔ¨Åed O
Model O
for O
Pre O
- O
training O
and O
Finetuning O
While O
NCLS O
( O
Zhu O
et O
al O
. O
, O
2019b O
) O
uses O
different O
decoders O
for O
various O
pre O
- O
training O
objectives O
, O
we O
employ O
a O
uniÔ¨Åed O
Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
encoder O
- O
decoder O
model O
for O
all O
pre O
- O
training O
and O
Ô¨Ånetuning O
tasks O
. O
This O
makes O
our O
model O
learn O
a O
crosslingual O
representation O
efÔ¨Åciently O
. O
A O
shared O
dictionary O
across O
all O
languages O
is O
used O
. O
To O
accommodate O
multi O
- O
task O
and O
multilingual O
objectives O
, O
we O
introduce O
language O
i O
d O
symbols O
to O
indicate O
the O
target O
language O
, O
and O
task O
symbols O
to O
indicate O
the O
target O
task O
. O
For O
instance O
, O
for O
the O
CMLM O
objective O
where O
the O
target O
language O
is O
Chinese O
, O
the O
decoder O
takes O
< O
cmlm O
> O
and O
< O
zh O
> O
as O
the O
Ô¨Årst O
two O
input O
tokens O
. O
We O
empirically O
Ô¨Ånd O
that O
our O
model O
does O
not O
suffer O
from O
the O
phenomenon O
of O
forgetting O
target O
language O
controllability O
as O
in O
Chi O
et O
al O
. O
( O
2019 O
) O
, O
which O
requires O
manual O
freezing O
of O
encoder O
or O
decoder O
during O
Ô¨Ånetuning O
. O
After O
pretraining O
, O
we O
conduct O
Ô¨Ånetuning O
on O
cross O
- O
lingual O
summarization O
data.4 O
Experiments O
4.1 O
Dataset O
We O
conduct O
our O
experiment O
on O
NCLS O
dataset O
( O
Zhu O
et O
al O
. O
, O
2019b O
) O
, O
which O
contains O
paired O
data O
of O
English O
articles O
with O
Chinese O
summaries O
, O
and O
Chinese O
articles O
with O
English O
summaries O
. O
The O
cross O
- O
lingual O
training O
data O
is O
automatically O
generated O
by O
a O
machine O
translation O
model O
. O
For O
Ô¨Ånetuning O
and O
testing O
, O
we O
followed O
the O
same O
train O
/ O
valid O
/ O
test O
split O
of O
the O
original O
dataset O
. O
We O
refer O
readers O
to O
Table O
1 O
in O
Zhu O
et O
al O
. O
( O
2019b O
) O
for O
detailed O
statistics O
of O
the O
dataset O
. O
For O
pre O
- O
training O
, O
we O
obtain O
monolingual O
data O
for O
English O
and O
Chinese O
from O
the O
corresponding O
Wikipedia O
dump O
. O
There O
are O
83 O
million O
sentences O
for O
English O
monolingual O
corpus O
and O
20 O
million O
sentences O
for O
Chinese O
corpus O
. O
For O
parallel O
data O
between O
English O
and O
Chinese O
, O
we O
use O
the O
parallel O
corpus O
from O
Lample O
and O
Conneau O
( O
2019 O
) O
, O
which O
contains O
9.6 O
million O
paired O
sentences O
. O
For O
monolingual O
summarization O
objective O
, O
we O
use O
CNN O
/ O
DailyMail O
dataset O
( O
Nallapati O
et O
al O
. O
, O
2016 O
) O
for O
English O
summarization O
and O
LCSTS O
dataset O
( O
Hu O
et O
al O
. O
, O
2015 O
) O
for O
Chinese O
summarization O
. O
4.2 O
Implementation O
Details O
Our O
transformer O
model O
has O
6 O
layers O
and O
8 O
heads O
in O
attention O
. O
The O
input O
and O
output O
dimensions O
dmodel O
for O
all O
transformer O
blocks O
are O
512 O
and O
the O
inner O
dimensiondffis O
2048 O
. O
We O
use O
a O
dropout O
probability O
of O
0.1 O
on O
all O
layers O
. O
We O
build O
a O
shared O
SentencePiece O
( O
Kudo O
and O
Richardson O
, O
2018 O
) O
vocabulary O
of O
size O
33,000from O
a O
balanced O
mix O
of O
the O
monolingual O
Wikipedia O
corpus O
. O
The O
model O
has O
approximately O
61 O
M O
parameters O
. O
For O
MLM O
we O
use O
a O
mask O
probability O
of O
0.15 O
. O
For O
DAE O
, O
we O
set O
both O
the O
mask O
and O
drop O
out O
rate538English O
‚ÜíChinese O
Chinese O
‚ÜíEnglish O
ROUGE-1 O
ROUGE-2 O
ROUGE O
- O
L O
ROUGE-1 O
ROUGE-2 O
ROUGE O
- O
L O
TETran O
26.15 O
10.60 O
23.24 O
23.09 O
7.33 O
18.74 O
GETran O
28.19 O
11.40 O
25.77 O
24.34 O
9.14 O
20.13 O
TLTran O
30.22 O
12.20 O
27.04 O
33.92 O
15.81 O
29.86 O
GLTran O
32.17 O
13.85 O
29.43 O
35.45 O
16.86 O
31.28 O
NCLS O
36.82 O
18.72 O
33.20 O
38.85 O
21.93 O
35.05 O
NCLS O
- O
MS O
38.25 O
20.20 O
34.76 O
40.34 O
22.65 O
36.39 O
NCLS O
- O
MT O
40.23 O
22.32 O
36.59 O
40.25 O
22.58 O
36.21 O
XNLG O
39.85 O
24.47 O
28.28 O
38.34 O
19.65 O
33.66 O
ATS O
40.68 O
24.12 O
36.97 O
40.47 O
22.21 O
36.89 O
Ours O
43.50 O
25.41 O
29.66 O
41.62 O
23.35 O
37.26 O
Table O
2 O
: O
ROUGE-1 O
, O
ROUGE-2 O
, O
ROUGE O
- O
L O
for O
English O
to O
Chinese O
and O
Chinese O
to O
English O
summarization O
on O
NCLS O
dataset O
. O
to O
0.1 O
. O
For O
all O
pre O
- O
training O
and O
Ô¨Ånetuning O
we O
use O
RAdam O
optimizer O
( O
Liu O
et O
al O
. O
, O
2019 O
) O
with O
Œ≤1= O
0.9 O
, O
Œ≤2= O
0.999 O
. O
The O
initial O
learning O
rate O
is O
set O
to O
10‚àí9for O
pre O
- O
training O
and O
10‚àí4for O
Ô¨Ånetuning O
. O
The O
learning O
rate O
is O
linearly O
increased O
to O
0.001with O
16,000warmup O
steps O
followed O
by O
an O
exponential O
decay O
. O
For O
decoding O
, O
we O
use O
a O
beam O
size O
of O
6 O
and O
a O
maximum O
generation O
length O
of O
200 O
tokens O
for O
all O
experiments O
. O
English O
‚ÜíChinese O
ROUGE-1 O
ROUGE-2 O
ROUGE O
- O
L O
Ours O
43.50 O
25.41 O
29.66 O
- O
MS O
42.48 O
24.45 O
28.49 O
- O
MT O
42.12 O
23.97 O
28.74 O
- O
MLM O
, O
DAE O
41.82 O
23.85 O
28.40 O
- O
All O
Pretraining O
41.12 O
23.67 O
28.53 O
Table O
3 O
: O
Finetuning O
performance O
on O
English O
‚ÜíChinese O
summarization O
starting O
with O
various O
ablated O
pre O
- O
trained O
models O
. O
4.3 O
Baselines O
We O
Ô¨Årst O
include O
a O
set O
of O
pipeline O
methods O
from O
Zhu O
et O
al O
. O
( O
2019b O
) O
which O
combines O
monolingual O
summarization O
and O
machine O
translation O
. O
TETran O
Ô¨Årst O
translates O
the O
source O
document O
and O
then O
uses O
LexRank O
( O
Erkan O
and O
Radev O
, O
2004 O
) O
to O
summarize O
the O
translated O
document O
. O
TLTran O
Ô¨Årst O
summarizes O
the O
source O
document O
and O
then O
translates O
the O
summary O
. O
GETran O
andGLTran O
replace O
the O
translation O
model O
in O
TETran O
and O
TLTran O
with O
Google O
Translator1respectively O
. O
We O
also O
include O
three O
strong O
baselines O
from O
Zhu O
et O
al O
. O
( O
2019b O
): O
NCLS O
, O
NCLS O
- O
MS O
andNCLS O
- O
MT O
. O
1https://translate.google.com/NCLS O
trains O
a O
standard O
Transformer O
model O
on O
the O
cross O
- O
lingual O
summarization O
dataset O
. O
NCLS O
- O
MS O
and O
NCLS O
- O
MT O
both O
use O
one O
encoder O
and O
multiple O
decoders O
for O
multi O
- O
task O
scenarios O
. O
NCLS O
- O
MS O
combines O
the O
cross O
- O
lingual O
summarization O
task O
with O
monolingual O
summarization O
while O
NCLS O
- O
MT O
combines O
it O
with O
machine O
translation O
. O
We O
Ô¨Ånetune O
XNLG O
model O
from O
Chi O
et O
al O
. O
( O
2019 O
) O
on O
the O
same O
cross O
- O
lingual O
summarization O
data O
. O
We O
Ô¨Ånetune O
all O
layers O
of O
XNLG O
in O
the O
same O
way O
as O
our O
pretrained O
model O
. O
Finally O
, O
we O
include O
the O
result O
of O
ATS O
from O
the O
concurrent O
work O
of O
Zhu O
et O
al O
. O
( O
2020 O
) O
. O
4.4 O
Results O
Table O
2 O
shows O
the O
ROUGE O
scores O
of O
generated O
summaries O
in O
English O
- O
to O
- O
Chinese O
and O
Chinese O
- O
toEnglish O
summarization O
. O
As O
shown O
, O
pipeline O
models O
, O
although O
incorporating O
state O
- O
of O
- O
the O
- O
art O
machine O
translation O
systems O
, O
achieve O
sub O
- O
optimal O
performance O
in O
both O
directions O
, O
proving O
the O
advantages O
of O
end O
- O
to O
- O
end O
models O
. O
Our O
model O
outperforms O
all O
baseline O
models O
in O
all O
metrics O
except O
for O
ROUGE O
- O
L O
in O
English O
- O
toChinese O
. O
For O
instance O
, O
our O
model O
achieves O
2.82 O
higher O
ROUGE-1 O
score O
in O
Chinese O
to O
English O
summarization O
than O
the O
previously O
best O
result O
and O
1.15 O
higher O
ROUGE-1 O
score O
in O
English O
to O
Chinese O
summarization O
, O
which O
shows O
the O
effectiveness O
of O
utilizing O
multilingual O
and O
multi O
- O
task O
data O
to O
improve O
cross O
- O
lingual O
summarization O
. O
4.5 O
Ablation O
Study O
Table O
3 O
shows O
the O
ablation O
study O
of O
our O
model O
on O
English O
to O
Chinese O
summarization O
. O
We O
remove5390.005.0010.0015.0020.0025.0030.0035.0040.0045.0050.00 O
1 O
K O
10 O
K O
364 O
K O
( O
Full O
) O
Ours O
Ours O
w/o O
Pretraining O
# O
Train O
Data O
( O
a O
) O
English O
‚ÜíChinese O
ROUGE-1 O
0.005.0010.0015.0020.0025.0030.0035.0040.0045.00 O
1 O
K O
10 O
K O
1693 O
K O
( O
Full O
) O
Ours O
Ours O
w/o O
Pretraining O
# O
Train O
Data O
( O
b O
) O
Chinese O
‚ÜíEnglish O
ROUGE-1 O
Figure O
1 O
: O
ROUGE-1 O
performance O
on O
NCLS O
dataset O
when O
the O
cross O
- O
lingual O
summarization O
training O
data O
is O
subsampled O
to O
size O
of O
1k O
and O
10k O
. O
The O
result O
on O
the O
full O
dataset O
is O
also O
shown O
. O
from O
the O
pre O
- O
training O
objectives O
i O
) O
all O
monolingual O
unsupervised O
tasks O
( O
MLM O
, O
DAE O
) O
, O
ii O
) O
machine O
translation O
( O
MT O
) O
, O
iii O
) O
monolingual O
summarization O
( O
MS O
) O
, O
and O
iv O
) O
all O
the O
objectives O
. O
Note O
that O
‚Äù O
- O
All O
Pretraining O
‚Äù O
and O
NCLS O
both O
only O
train O
on O
the O
cross O
- O
lingual O
summarization O
data O
. O
The O
performance O
difference O
between O
the O
two O
is O
most O
likely O
due O
to O
the O
difference O
in O
model O
size O
, O
vocabulary O
, O
and O
other O
hyperparameters O
. O
As O
shown O
, O
the O
pre O
- O
training O
can O
improve O
ROUGE1 O
, O
ROUGE-2 O
, O
and O
ROUGE O
- O
L O
by O
2.38 O
, O
1.74 O
, O
and O
1.13 O
points O
respectively O
on O
Chinese O
- O
to O
- O
English O
summarization O
. O
Moreover O
, O
all O
pre O
- O
training O
objectives O
have O
various O
degrees O
of O
contribution O
to O
the O
results O
, O
and O
the O
monolingual O
unsupervised O
objectives O
( O
MLM O
and O
DAE O
) O
are O
relatively O
the O
most O
important O
. O
This O
veriÔ¨Åes O
the O
effectiveness O
of O
leveraging O
unsupervised O
data O
in O
the O
pre O
- O
training O
. O
Low O
- O
resource O
scenario O
. O
We O
sample O
subsets O
of O
size O
1 O
K O
and O
10 O
K O
from O
the O
training O
data O
of O
crosslingual O
summarization O
and O
Ô¨Ånetune O
our O
pre O
- O
trained O
model O
on O
those O
subsets O
. O
Figure O
1 O
shows O
the O
the O
performance O
of O
the O
pre O
- O
trained O
model O
and O
the O
model O
trained O
from O
scratch O
on O
the O
same O
subsets O
. O
As O
shown O
, O
the O
gain O
from O
pre O
- O
training O
is O
larger O
when O
the O
size O
of O
training O
data O
is O
relatively O
small O
. O
This O
proves O
the O
effectiveness O
of O
our O
approach O
to O
deal O
with O
low O
- O
resource O
language O
in O
cross O
- O
lingual O
summarization O
. O
5 O
Conclusion O
We O
present O
a O
mix O
- O
lingual O
pre O
- O
training O
model O
for O
cross O
- O
lingual O
summarization O
. O
We O
optimize O
a O
shared O
encoder O
- O
decoder O
architecture O
for O
multi O
- O
lingual O
and O
multi O
- O
task O
objectives O
. O
Experiments O
on O
a O
benchmarkdataset O
show O
that O
our O
model O
outperforms O
pipelinebased O
and O
other O
end O
- O
to O
- O
end O
baselines O
. O
Through O
an O
ablation O
study O
, O
we O
show O
that O
all O
pretraining O
objectives O
contribute O
to O
the O
model O
‚Äôs O
performance O
. O
Abstract O
Point O
- O
of O
- O
Interest O
( O
POI O
) O
oriented O
question O
answering O
( O
QA O
) O
aims O
to O
return O
a O
list O
of O
POIs O
given O
a O
question O
issued O
by O
a O
user O
. O
Recent O
advances O
in O
intelligent O
virtual O
assistants O
have O
opened O
the O
possibility O
of O
engaging O
the O
client O
software O
more O
actively O
in O
the O
provision O
of O
location O
- O
based O
services O
, O
thereby O
showing O
great O
promise O
for O
automatic O
POI O
retrieval O
. O
Some O
existing O
QA O
methods O
can O
be O
adopted O
on O
this O
task O
such O
as O
QA O
similarity O
calculation O
and O
semantic O
parsing O
using O
pre O
- O
deÔ¨Åned O
rules O
. O
The O
returned O
results O
, O
however O
, O
are O
subject O
to O
inherent O
limitations O
due O
to O
the O
lack O
of O
the O
ability O
for O
handling O
some O
important O
POI O
related O
information O
, O
including O
tags O
, O
location O
entities O
, O
and O
proximityrelated O
terms O
( O
e.g. O
‚Äú O
nearby O
‚Äù O
, O
‚Äú O
close O
‚Äù O
) O
. O
In O
this O
paper O
, O
we O
present O
a O
novel O
deep O
learning O
framework O
integrated O
with O
joint O
inference O
to O
capture O
both O
tag O
semantic O
and O
geographic O
correlation O
between O
question O
and O
POIs O
. O
One O
characteristic O
of O
our O
model O
is O
to O
propose O
a O
special O
cross O
attention O
question O
embedding O
neural O
network O
structure O
to O
obtain O
question O
- O
to O
- O
POI O
and O
POI O
- O
to O
- O
question O
information O
. O
Besides O
, O
we O
utilize O
a O
skewed O
distribution O
to O
simulate O
the O
spatial O
relationship O
between O
questions O
and O
POIs O
. O
By O
measuring O
the O
results O
offered O
by O
the O
model O
against O
existing O
methods O
, O
we O
demonstrate O
its O
robustness O
and O
practicability O
, O
and O
supplement O
our O
conclusions O
with O
empirical O
evidence O
. O
1 O
Introduction O
Point O
- O
of O
- O
Interest O
( O
POI O
) O
oriented O
question O
answering O
( O
QA O
) O
problem O
is O
a O
special O
QA O
task O
which O
aims O
to O
answer O
users O
‚Äô O
questions O
by O
generating O
a O
list O
of O
POIs O
. O
With O
the O
rapid O
development O
of O
smart O
agents O
( O
e.g. O
Amazon O
Echo O
) O
and O
intelligent O
virtual O
assistants O
( O
e.g. O
Apple O
Siri O
and O
Google O
Assistant O
) O
, O
there O
are O
many O
POI O
oriented O
queries O
being O
requested O
everyday O
. O
Some O
examples O
of O
these O
questions O
are O
‚Äú O
Where O
can O
I O
take O
my O
kid O
to O
have O
fun O
nearby O
New O
York O
City O
‚Äù O
or O
‚Äú O
Where O
can O
we O
go O
in O
LA O
with O
my O
friends O
‚Äù O
. O
The O
answers O
are O
typically O
a O
list O
of O
POIssuch O
as O
parks O
, O
malls O
, O
or O
restaurants O
corresponding O
to O
the O
details O
provided O
by O
users O
in O
the O
questions O
. O
According O
to O
some O
statistics O
, O
there O
are O
millions O
of O
POI O
oriented O
QA O
questions O
being O
requested O
per O
day O
on O
a O
mobile O
search O
engine O
in O
China O
. O
Generally O
speaking O
, O
semantic O
parsing O
and O
similarity O
matching O
methods O
are O
utilized O
to O
tackle O
the O
POI O
oriented O
QA O
problem O
in O
current O
solutions O
. O
Nevertheless O
, O
both O
of O
them O
are O
subject O
to O
inherent O
limitations O
and O
deserve O
to O
be O
improved O
. O
Semantic O
parsing O
based O
methods O
convert O
the O
questions O
to O
formal O
representations O
( O
such O
as O
SQL O
queries O
) O
using O
pre O
- O
deÔ¨Åned O
rules O
, O
then O
get O
the O
POI O
results O
from O
the O
query O
. O
DifÔ¨Åculties O
arise O
, O
however O
, O
when O
the O
form O
of O
the O
questions O
varies O
from O
person O
to O
person O
. O
Besides O
, O
due O
to O
ambiguous O
expressions O
of O
a O
speciÔ¨Åc O
tag O
, O
semantic O
parsing O
methods O
always O
fail O
to O
match O
mentioned O
tags O
to O
the O
POI O
database O
. O
Furthermore O
, O
based O
only O
on O
tag O
information O
, O
it O
is O
almost O
impossible O
for O
semantic O
parsing O
methods O
to O
make O
use O
of O
the O
distance O
correlation O
between O
questions O
and O
POIs O
. O
Another O
line O
of O
solution O
is O
adopting O
similarity O
matching O
models O
for O
calculating O
the O
similarity O
score O
between O
questions O
and O
POIs O
. O
Recent O
years O
have O
witnessed O
rapid O
growth O
in O
various O
kinds O
of O
semantic O
similarity O
based O
QA O
systems O
such O
as O
Convolutional O
Neural O
Network O
Architecture O
( O
Hu O
et O
al O
. O
, O
2014 O
) O
, O
LSTM O
Based O
Answer O
Selection O
( O
Tan O
et O
al O
. O
, O
2015 O
, O
2016 O
) O
, O
and O
Cross O
- O
Attention O
Based O
Question O
Answering O
System O
( O
Hao O
et O
al O
. O
, O
2017 O
) O
. O
Despite O
the O
success O
in O
common O
landscapes O
, O
most O
existing O
studies O
of O
this O
family O
can O
not O
work O
well O
for O
POI O
oriented O
QA O
, O
since O
it O
is O
ineffective O
for O
them O
to O
handle O
the O
unique O
properties O
of O
POI O
elements O
such O
as O
tags O
and O
locations O
. O
As O
a O
result O
, O
a O
signiÔ¨Åcant O
gap O
remains O
between O
academic O
proposals O
and O
the O
industry O
standard O
of O
implementing O
location O
based O
services O
. O
It O
is O
nontrivial O
to O
extend O
existing O
QA O
models O
to O
handle O
the O
challenges O
of O
POI O
oriented O
QA O
. O
In O
general O
, O
the O
unique O
challenges O
for O
this O
problem O
mainly542Figure O
1 O
: O
The O
overall O
architecture O
of O
our O
model O
. O
Generally O
, O
the O
model O
is O
made O
up O
of O
two O
parts O
, O
namely O
tag O
semantic O
moduleptand O
distance O
correlation O
module O
pd O
. O
The O
model O
takes O
the O
Question O
- O
POI O
pair O
as O
input O
, O
and O
the O
probability O
of O
choosing O
a O
POI O
given O
the O
question O
as O
output O
. O
come O
from O
two O
aspects O
. O
First O
of O
all O
, O
when O
asking O
POI O
oriented O
questions O
, O
people O
tend O
to O
emphasize O
certain O
needs O
, O
which O
correspond O
to O
some O
POI O
properties O
, O
such O
as O
the O
popular O
users O
of O
the O
POI O
, O
the O
service O
provided O
by O
the O
POI O
, O
and O
the O
types O
of O
the O
POI O
, O
etc O
. O
Hereafter O
we O
name O
all O
such O
POI O
properties O
as O
tags O
. O
Identifying O
such O
information O
in O
the O
question O
that O
is O
related O
to O
the O
tags O
of O
POI O
is O
crucial O
in O
this O
task O
, O
thus O
creating O
a O
bottleneck O
. O
Take O
the O
question O
‚Äú O
Where O
can O
children O
go O
nearby O
New O
York O
City O
‚Äù O
as O
an O
example O
, O
the O
word O
‚Äú O
children O
‚Äù O
, O
being O
regarded O
as O
both O
a O
question O
term O
and O
a O
POI O
tag O
, O
plays O
an O
important O
role O
in O
identifying O
the O
corresponding O
POIs O
. O
Second O
, O
proximity O
- O
related O
terms O
such O
as O
‚Äú O
nearby O
‚Äù O
and O
‚Äú O
close O
‚Äù O
deserve O
special O
treatment O
. O
Considering O
the O
same O
example O
, O
if O
there O
is O
‚Äú O
nearby O
‚Äù O
in O
the O
question O
, O
the O
candidate O
POIs O
should O
be O
mainly O
located O
outside O
New O
York O
City O
; O
whereas O
if O
without O
, O
the O
candidate O
POIs O
should O
be O
within O
New O
York O
City O
. O
Furthermore O
, O
for O
different O
location O
entities O
such O
as O
‚Äú O
nearby O
New O
York O
City O
‚Äù O
v.s. O
‚Äú O
nearby O
Manhattan O
‚Äù O
, O
the O
distance O
scopes O
of O
‚Äú O
nearby O
‚Äù O
are O
also O
different O
. O
In O
contrast O
, O
traditional O
QA O
methods O
are O
not O
able O
to O
treat O
these O
terms O
in O
their O
models O
properly O
and O
thus O
leading O
to O
a O
poor O
performance O
on O
POI O
oriented O
QA O
. O
In O
this O
paper O
, O
we O
propose O
a O
POI O
oriented O
QA O
model O
with O
JointInference O
( O
named O
as O
PJI O
for O
short O
) O
to O
tackle O
the O
challenges O
mentioned O
before O
. O
PJI O
mainly O
has O
two O
modules O
which O
are O
named O
as O
tag O
semantic O
module O
and O
distance O
correlation O
module O
. O
The O
tag O
semantic O
module O
is O
used O
to O
automatically O
search O
for O
relevant O
POIs O
based O
on O
semantic O
tag O
in O
- O
formation O
. O
Besides O
, O
in O
order O
to O
capture O
speciÔ¨Åc O
patterns O
buried O
in O
questions O
and O
POIs O
, O
we O
develop O
a O
novel O
cross O
attention O
based O
question O
embedding O
structure O
. O
Therefore O
the O
mutual O
inÔ¨Çuence O
between O
questions O
and O
POIs O
is O
taken O
into O
account O
. O
In O
the O
distance O
correlation O
module O
, O
we O
adopt O
a O
skewed O
distribution O
on O
three O
- O
level O
locations O
including O
city O
, O
district O
, O
Area O
of O
Interest O
( O
AOI O
) O
to O
Ô¨Åt O
the O
distance O
distribution O
between O
candidate O
POIs O
and O
mentioned O
location O
terms O
in O
the O
question O
. O
Both O
modules O
are O
fused O
together O
and O
optimized O
in O
an O
end O
- O
to O
- O
end O
manner O
for O
retrieving O
the O
Ô¨Ånal O
POI O
list O
. O
Our O
major O
contributions O
can O
be O
summarized O
as O
follows O
: O
‚Ä¢We O
tackle O
the O
POI O
oriented O
QA O
problem O
by O
proposing O
a O
new O
deep O
learning O
model O
with O
joint O
inference O
. O
‚Ä¢We O
leverage O
two O
neural O
network O
modules O
to O
build O
a O
bridge O
between O
questions O
and O
POIs O
on O
both O
POI O
tags O
and O
question O
location O
terms O
. O
We O
also O
adopt O
a O
skewed O
distribution O
method O
to O
deal O
with O
proximity O
- O
related O
terms O
. O
‚Ä¢We O
design O
a O
special O
embedding O
structure O
using O
cross O
attention O
mechanism O
to O
obtain O
a O
more O
precise O
and O
Ô¨Çexible O
representation O
of O
questions O
. O
‚Ä¢We O
conduct O
comprehensive O
experiments O
on O
two O
real O
- O
world O
datasets O
enabling O
the O
evaluation O
of O
the O
results O
from O
different O
perspectives O
. O
Experimental O
results O
demonstrate O
signiÔ¨Åcant O
improvements O
of O
PJI O
over O
all O
the O
state O
- O
of O
- O
theart O
baselines.5432 O
Related O
Work O
QA O
with O
Semantic O
Parser O
Semantic O
parsing O
shines O
at O
handling O
complex O
linguistic O
constructions O
and O
obtains O
reasonable O
performance O
on O
question O
answering O
problems O
. O
Traditionally O
, O
semantic O
parsers O
like O
AMR O
( O
Banarescu O
et O
al O
. O
, O
2012 O
) O
and O
SQL O
( O
Androutsopoulos O
et O
al O
. O
, O
1995 O
) O
map O
sentences O
to O
formal O
representations O
of O
their O
underlying O
meaning O
( O
Shen O
and O
Lapata O
, O
2007 O
; O
Yao O
et O
al O
. O
, O
2014 O
; O
Hill O
et O
al O
. O
, O
2015 O
; O
Talmor O
et O
al O
. O
, O
2017 O
) O
. O
By O
leveraging O
a O
knowledge O
base O
, O
semantic O
parsing O
is O
reduced O
to O
query O
graph O
generation O
and O
stage O
searching O
. O
Neural O
Approaches O
for O
QA O
With O
the O
recent O
development O
in O
deep O
learning O
, O
neural O
networks O
have O
achieved O
great O
success O
in O
question O
answer O
problems O
( O
Salakhutdinov O
and O
Hinton O
, O
2009 O
; O
Collobert O
et O
al O
. O
, O
2011 O
; O
Socher O
et O
al O
. O
, O
2012 O
; O
Hu O
et O
al O
. O
, O
2014 O
; O
Tan O
et O
al O
. O
, O
2015 O
, O
2016 O
) O
. O
Most O
of O
these O
models O
use O
a O
deep O
neural O
network O
like O
GRU O
( O
Chung O
et O
al O
. O
, O
2014 O
) O
and O
LSTM O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
to O
handle O
the O
long O
texts O
required O
for O
QA O
. O
Further O
improvements O
like O
attention O
mechanism O
are O
applied O
to O
focus O
on O
the O
most O
relevant O
facts O
( O
Hao O
et O
al O
. O
, O
2017 O
; O
Zhao O
et O
al O
. O
, O
2019 O
) O
. O
The O
relevance O
score O
of O
each O
QA O
pair O
is O
the O
cosine O
similarity O
of O
the O
semantic O
vectors O
. O
The O
Ô¨Ånal O
answers O
to O
each O
question O
are O
then O
sorted O
by O
the O
similarity O
score O
. O
Probabilistic O
Deep O
Learning O
Models O
The O
base O
of O
probabilistic O
deep O
learning O
models O
is O
to O
use O
the O
neural O
network O
as O
a O
conditional O
model O
parameterised O
by O
the O
weights O
in O
the O
network O
when O
some O
inputs O
are O
given O
. O
The O
output O
is O
obtained O
by O
optimizing O
the O
parameters O
in O
the O
model O
with O
the O
estimates O
provided O
by O
Bayesian O
framework O
. O
Several O
probabilistic O
models O
have O
been O
used O
in O
tasks O
like O
question O
answering O
with O
knowledge O
graph O
and O
link O
prediction O
( O
Wang O
et O
al O
. O
, O
2007 O
, O
2014 O
; O
Zhang O
et O
al O
. O
, O
2018 O
) O
. O
The O
main O
advantage O
of O
this O
complete O
separation O
of O
the O
neural O
network O
from O
Bayesian O
model O
is O
that O
the O
good O
features O
generated O
by O
the O
network O
are O
well O
used O
to O
make O
predictions O
, O
which O
gives O
the O
model O
high O
Ô¨Çexibility O
and O
accuracy O
. O
3 O
Our O
Model O
3.1 O
Preliminaries O
Point O
of O
interest O
( O
POI O
) O
is O
a O
dedicated O
geographic O
entity O
on O
an O
online O
map O
where O
someone O
may O
Ô¨Ånd O
useful O
information O
, O
like O
a O
restaurant O
, O
a O
hotel O
, O
or O
a O
travel O
spot O
. O
Compared O
with O
the O
common O
entities O
inknowledge O
graph O
, O
POI O
has O
two O
important O
properties O
which O
are O
tags O
and O
location O
. O
Tags O
refer O
to O
a O
short O
text O
( O
one O
or O
several O
words O
) O
in O
a O
POI O
describing O
its O
service O
( O
e.g. O
fast O
food O
or O
entertainment O
) O
, O
its O
major O
users O
( O
e.g. O
kids O
or O
lovers O
) O
, O
its O
types O
( O
e.g. O
restaurants O
or O
shopping O
mal O
) O
, O
etc O
. O
Users O
are O
greatly O
facilitated O
by O
informative O
tags O
when O
searching O
for O
the O
POIs O
. O
In O
addition O
, O
each O
POI O
has O
three O
location O
properties O
named O
as O
location O
entities O
recording O
the O
POI O
located O
city O
, O
district O
, O
and O
area O
of O
interest O
( O
AOI O
) O
. O
Here O
AOI O
refers O
to O
a O
polygonal O
area O
in O
a O
2D O
map O
which O
usually O
contains O
several O
POIs O
, O
New O
York O
Central O
Park O
for O
example O
. O
For O
each O
location O
entity O
, O
it O
is O
possible O
to O
Ô¨Ånd O
a O
set O
of O
POIs O
within O
the O
entity O
. O
Given O
a O
question O
q O
, O
the O
POI O
oriented O
question O
answering O
seeks O
to O
parse O
the O
question O
, O
then O
return O
a O
set O
of O
POIs O
which O
can O
be O
seen O
as O
the O
answer O
result O
according O
to O
the O
question O
. O
For O
example O
when O
qis O
the O
question O
‚Äú O
Where O
can O
children O
go O
on O
weekend O
in O
New O
York O
City O
? O
‚Äù O
, O
the O
answer O
is O
a O
set O
of O
POIs O
which O
are O
places O
for O
kids O
to O
play O
in O
New O
York O
City O
satisfying O
the O
information O
request O
conveyed O
by O
the O
user O
. O
It O
is O
possible O
to O
further O
rank O
the O
POIs O
according O
to O
some O
POI O
recommendation O
algorithms O
but O
it O
is O
beyond O
the O
scope O
of O
this O
paper O
. O
3.2 O
Model O
Overview O
In O
our O
framework O
, O
the O
dataset O
is O
a O
set O
of O
question O
- O
POI O
pairs O
which O
can O
be O
represented O
as O
D O
= O
{ O
qi O
, O
ai}N O
i=1 O
, O
whereqirefers O
to O
a O
question O
, O
ai O
refers O
to O
a O
POI O
answering O
the O
question O
. O
Our O
model O
PJI O
aims O
to O
retrieve O
the O
correct O
POIs O
with O
respect O
to O
each O
question O
which O
corresponds O
to O
the O
function O
P(ai|qj)returning O
the O
probability O
that O
POI O
aisatisÔ¨Åes O
the O
question O
qj O
. O
The O
overall O
structure O
of O
our O
model O
is O
illustrated O
in O
Fig O
1 O
. O
Our O
model O
consists O
of O
two O
neural O
network O
modules O
, O
as O
described O
below O
: O
Tag O
Semantic O
Module O
In O
the O
POI O
oriented O
QA O
, O
some O
question O
terms O
correspond O
to O
POI O
tags O
and O
thus O
serving O
as O
a O
bridge O
between O
questions O
and O
POIs O
. O
In O
Fig O
1 O
, O
‚Äú O
children O
‚Äù O
is O
both O
a O
term O
in O
the O
question O
and O
a O
tag O
of O
POI O
. O
However O
, O
it O
is O
not O
easy O
to O
match O
the O
query O
terms O
to O
POI O
tags O
directly O
. O
For O
example O
, O
terms O
such O
as O
‚Äú O
kids O
‚Äù O
, O
‚Äú O
baby O
‚Äù O
can O
also O
correspond O
to O
the O
tag O
‚Äú O
children O
‚Äù O
. O
In O
our O
model O
, O
for O
each O
question O
qj O
, O
we O
learn O
the O
probability O
that O
a O
tagyais O
included O
in O
the O
question O
qj O
, O
which O
can O
be O
represented O
as O
P(ya|qj O
) O
. O
Given O
the O
question O
embedding O
and O
tags O
, O
POIs O
with O
the O
corresponding O
tags O
are O
chosen O
as O
the O
answers O
at O
the O
tag O
level O
. O
We O
then O
develop O
a O
neural O
network O
module O
specialized O
for544calculatingP(ai|ya O
, O
qj O
) O
, O
which O
is O
the O
likelihood O
of O
POIaibeing O
selected O
given O
the O
tag O
yaand O
the O
questionqj O
. O
Above O
all O
, O
the O
likelihood O
of O
choosing O
POI O
aias O
the O
answer O
to O
the O
question O
qjis O
the O
marginal O
probability O
mass O
function O
over O
all O
tags O
: O
pt(ai|qj O
, O
Œ∏t O
) O
= O
/summationdisplay O
ya‚ààVtP(ai|ya O
, O
qj)‚àóP(ya|qj)(1 O
) O
which O
sums O
out O
all O
possibilities O
of O
tag O
variables O
, O
whereVtrefers O
to O
the O
tag O
set O
. O
Distance O
Correlation O
Module O
Apart O
from O
tags O
, O
there O
also O
exists O
a O
distance O
correlation O
between O
questions O
and O
POIs O
. O
In O
this O
module O
, O
we O
Ô¨Årst O
extract O
the O
location O
entity O
using O
NER O
tools O
, O
since O
the O
location O
entity O
vocabulary O
is O
very O
large O
and O
has O
Ô¨Åxed O
names O
. O
The O
questions O
usually O
contain O
some O
proximity O
- O
related O
terms O
, O
such O
as O
‚Äú O
nearby O
‚Äù O
and O
‚Äú O
close O
to O
‚Äù O
. O
It O
is O
hard O
to O
conÔ¨Ådently O
determine O
whether O
a O
POI O
is O
in O
or O
out O
an O
extracted O
location O
entity O
polygon O
considering O
such O
proximity O
- O
related O
terms O
. O
Thus O
, O
instead O
of O
directly O
identifying O
the O
candidate O
POIs O
by O
the O
location O
entity O
appeared O
in O
the O
question O
, O
we O
also O
calculate O
the O
probability O
of O
candidate O
POIs O
considering O
both O
the O
distance O
to O
the O
extracted O
entity O
polygon O
and O
the O
question O
context O
. O
With O
this O
motivation O
, O
we O
introduce O
another O
probability O
function O
P(ai|yl O
, O
qj O
) O
, O
which O
captures O
the O
probability O
of O
POI O
aibeing O
the O
answer O
of O
the O
questionqjif O
the O
location O
entity O
ylappears O
inqj O
. O
We O
denote O
the O
likelihood O
of O
choosing O
POI O
aigiven O
the O
question O
qjbased O
on O
distance O
correlation O
as O
: O
pd(ai|qj O
, O
Œ∏d O
) O
= O
/summationdisplay O
yl‚ààVdP(ai|yl O
, O
qj)‚àóP(yl|qj)(2 O
) O
whereP(yl|qj O
) O
= O
1 O
ifylappears O
inqj O
, O
otherwise O
P(yl|qj O
) O
= O
0 O
, O
yl‚ààVdandVdrefers O
to O
the O
location O
entity O
set O
. O
Overall O
Formulation O
With O
the O
two O
modules O
above O
, O
the O
parameters O
of O
the O
function O
p(ai|qj)can O
be O
estimated O
by O
maximizing O
the O
log O
- O
likelihood O
as O
follows O
: O
max O
Œ∏t O
, O
Œ∏d(1 O
NN O
/ O
summationdisplay O
i=1logpt+1 O
NN O
/ O
summationdisplay O
i=1logpd O
) O
( O
3 O
) O
3.3 O
Neural O
Network O
Module O
for O
Tag O
Semantic O
Matching O
Due O
to O
the O
linguistic O
diversity O
of O
describing O
a O
certain O
tag O
, O
it O
is O
almost O
impossible O
to O
recognize O
the O
tag O
with O
exact O
matching O
. O
Therefore O
, O
we O
build O
atag O
recognizer O
which O
can O
be O
jointly O
trained O
with O
the O
model O
. O
After O
that O
, O
we O
can O
get O
the O
POIs O
given O
the O
tag O
and O
question O
representations O
with O
a O
cross O
attention O
architecture O
. O
QA O
Embedding O
We O
use O
two O
dense O
ddimensional O
vector O
representations O
of O
questions O
in O
the O
module O
. O
The O
Ô¨Årst O
one O
is O
represented O
as O
fent O
( O
¬∑ O
) O
: O
q‚ÜíRd O
, O
which O
takes O
the O
Word2Vec O
vectors O
as O
input O
, O
then O
feeds O
them O
into O
a O
Bi O
LSTM O
neural O
network O
with O
a O
pooling O
layer O
. O
It O
helps O
to O
capture O
the O
sequence O
information O
in O
the O
question O
and O
is O
used O
in O
POI O
tag O
recognition O
. O
The O
other O
one O
is O
denoted O
as O
fpr O
( O
¬∑ O
) O
: O
q‚ÜíRd O
, O
which O
leverages O
attention O
mechanism O
to O
distinguish O
and O
catch O
the O
most O
important O
information O
in O
questions O
. O
Rather O
than O
apply O
a O
simple O
attention O
layer O
, O
we O
introduce O
a O
special O
cross O
attention O
mechanism O
tailored O
to O
this O
task O
originally O
Ô¨Årst O
brought O
by O
Hao O
et O
al O
. O
( O
2017 O
) O
. O
The O
answer O
POI O
is O
embedded O
with O
function O
g O
( O
¬∑ O
) O
: O
a‚ÜíRd O
, O
which O
calculates O
the O
average O
value O
of O
POI O
tag O
vectors O
obtained O
from O
Word2Vec O
. O
Cross O
Attention O
Mechanism O
Similar O
tofent O
, O
the O
structure O
fprconsists O
of O
a O
Bi O
LSTM O
network O
with O
a O
pooling O
layer O
, O
whereas O
the O
output O
of O
it O
interacts O
with O
the O
POI O
representation O
and O
takes O
the O
attention O
weights O
into O
account O
. O
The O
Ô¨Ånal O
attentive O
embedding O
consists O
of O
POI O
- O
towards O
- O
question O
embedding O
and O
question O
- O
towards O
- O
POI O
embedding O
. O
In O
POI O
- O
towards O
- O
question O
step O
, O
we O
train O
weights O
between O
every O
state O
in O
the O
Bi O
LSTM O
hidden O
layer O
and O
POI O
tag O
, O
then O
get O
a O
set O
of O
weighted O
question O
vectors O
regarding O
each O
POI O
tag O
. O
The O
following O
formulas O
are O
proposed O
to O
calculate O
the O
vectors O
: O
Œ±mn O
= O
softmax O
( O
h(WT[hn;em O
] O
+ O
b O
) O
) O
( O
4 O
) O
fpr(q)m=/summationdisplay O
nŒ±mnhn O
( O
5 O
) O
wherehndenotes O
the O
question O
hidden O
layer O
vector O
. O
emdenotes O
the O
POI O
tag O
embedding O
vector O
. O
Œ±mnis O
the O
weight O
of O
attention O
from O
the O
tag O
emto O
thenth O
word O
in O
the O
question O
. O
h(¬∑)is O
an O
activation O
function O
. O
In O
question O
- O
towards O
- O
POI O
step O
, O
we O
learn O
a O
set O
of O
weights O
between O
the O
question O
pooling O
layer O
vector O
and O
POI O
tag O
. O
Using O
the O
weighted O
question O
vectors O
in O
the O
Ô¨Årst O
step O
and O
the O
weights O
in O
the O
second O
step O
, O
we O
can O
then O
get O
the O
Ô¨Ånal O
weighted O
double O
- O
sided O
attentive O
question O
vector O
by O
multiplying O
and O
adding O
them O
up O
. O
fpr(q O
) O
= O
/summationdisplay O
mŒ≤mfpr(q)m O
( O
6)545Œ≤m O
= O
softmax O
( O
h(WT[fent(q);em O
] O
+ O
b))(7 O
) O
whereŒ≤mdenotes O
the O
attention O
of O
question O
towards O
answer O
aspects O
. O
POI O
Tag O
Recognition O
We O
exploit O
the O
question O
context O
to O
build O
the O
tag O
recognizer O
. O
For O
instance O
, O
if O
the O
question O
contains O
the O
word O
‚Äú O
dating O
‚Äù O
, O
it O
means O
that O
the O
target O
audience O
is O
lovers O
and O
the O
POI O
type O
should O
be O
like O
parks O
and O
restaurants O
. O
SpeciÔ¨Åcally O
, O
we O
embed O
the O
question O
to O
a O
ddimensional O
vector O
using O
embedding O
function O
fent O
( O
¬∑ O
) O
: O
q‚ÜíRdas O
described O
above O
. O
Then O
given O
the O
embedding O
vector O
of O
the O
question O
q O
, O
we O
set O
the O
likelihood O
of O
choosing O
tagyaby O
adding O
a O
softmax O
layer O
as O
follows O
: O
P(ya|q O
) O
= O
softmax O
( O
WT O
yfent(q O
) O
) O
( O
8) O
= O
exp(WT O
yfent(q))/summationtext O
y O
/ O
prime‚ààVtexp(WT O
y O
/ O
primefent(q))(9 O
) O
whereVtrefers O
to O
the O
tag O
set O
in O
the O
POI O
dataset O
. O
Tag O
Based O
POI O
Retrieval O
Since O
the O
number O
of O
POIs O
in O
the O
dataset O
is O
often O
very O
large O
, O
it O
is O
necessary O
to O
obtain O
some O
candidate O
POIs O
based O
on O
tag O
information O
and O
discard O
the O
irrelevant O
ones O
. O
Having O
P(ya|q O
) O
, O
we O
can O
get O
POI O
tag O
yawith O
the O
highest O
score O
. O
We O
then O
Ô¨Ålter O
out O
POIs O
with O
the O
tag O
yafrom O
the O
dataset O
and O
form O
the O
candidate O
set O
. O
Precisely O
, O
we O
introduce O
a O
Dirac O
delta O
function O
/epsilon1to O
accomplish O
this O
process O
. O
For O
POIs O
with O
ya O
, O
the O
function O
/epsilon1ya(a O
) O
is O
set O
to O
1 O
, O
while O
for O
POIs O
without O
, O
/epsilon1ya(a)is O
set O
to O
0 O
. O
The O
Ô¨Åltering O
of O
POI O
greatly O
reduces O
the O
workload O
of O
subsequent O
process O
, O
and O
has O
a O
signiÔ¨Åcant O
effect O
for O
large O
- O
scale O
data O
. O
After O
obtaining O
the O
tag O
yain O
the O
question O
qj O
and O
the O
function O
/epsilon1 O
, O
the O
next O
step O
is O
to O
retrieve O
the O
corresponding O
POIs O
, O
which O
is O
represented O
as O
P(ai|ya O
, O
qj)in O
Section O
3.2 O
. O
Suppose O
questions O
are O
embedded O
using O
the O
embedding O
function O
fpr O
( O
¬∑ O
) O
: O
q‚ÜíRd O
. O
In O
this O
function O
, O
the O
Ô¨Ånal O
output O
question O
embedding O
is O
the O
weighted O
cross O
- O
attentive O
vectors O
where O
informative O
patterns O
in O
questions O
are O
strongly O
focused O
. O
The O
likelihood O
of O
choosing O
ai O
given O
question O
answer O
embedding O
and O
POI O
tag O
can O
be O
represented O
as O
follows O
: O
P(ai|ya O
, O
q O
) O
= O
sigmoid O
( O
fpr(q)Tg(ai))¬∑/epsilon1ya(ai O
) O
( O
10)3.4 O
Neural O
Network O
Module O
for O
Distance O
Correlation O
This O
section O
mainly O
discusses O
the O
approach O
for O
matching O
the O
POIs O
to O
the O
question O
based O
on O
the O
aspect O
of O
distance O
correlation O
. O
As O
discussed O
in O
Section O
3.2 O
, O
the O
Ô¨Årst O
step O
of O
distance O
correlation O
module O
is O
to O
Ô¨Ånd O
location O
entities O
in O
the O
question O
. O
In O
our O
model O
, O
we O
assume O
that O
there O
are O
three O
types O
of O
location O
entities O
: O
city O
, O
district O
, O
AOI O
( O
Area O
of O
Interest O
) O
. O
AOI O
is O
a O
location O
entity O
on O
the O
map O
with O
boundaries O
( O
e.g. O
Central O
Park O
) O
which O
usually O
belongs O
to O
a O
district O
( O
e.g. O
Manhattan O
) O
of O
a O
city O
( O
e.g. O
New O
York O
) O
. O
We O
Ô¨Årst O
build O
a O
dictionary O
storing O
all O
of O
the O
location O
entities O
and O
their O
corresponding O
scopes O
as O
well O
as O
types O
. O
For O
every O
question O
, O
we O
extract O
the O
location O
terms O
in O
the O
question O
with O
an O
NER O
( O
named O
entity O
recognition O
) O
tool O
before O
mapping O
them O
onto O
the O
dictionary O
. O
Note O
that O
the O
location O
term O
extracted O
directly O
from O
questions O
can O
be O
hierarchical O
. O
For O
example O
, O
AOIs O
may O
appear O
in O
the O
form O
of O
District+AOI O
( O
e.g. O
Manhattan O
Central O
Park O
) O
or O
City+AOI O
( O
New O
York O
Central O
Park O
) O
or O
City+District+AOI O
( O
New O
York O
Manhattan O
Central O
Park O
) O
or O
just O
itself O
( O
Central O
Park O
) O
. O
Thus O
, O
we O
set O
the O
priority O
order O
to O
AOI O
> O
district O
> O
city O
when O
conducting O
entity O
mapping O
. O
Proximity O
- O
related O
Terms O
While O
retrieving O
the O
POIs O
according O
the O
location O
entity O
, O
another O
factor O
we O
should O
consider O
is O
whether O
the O
question O
contains O
some O
proximity O
- O
related O
terms O
such O
as O
‚Äú O
nearby O
‚Äù O
, O
‚Äú O
close O
to O
‚Äù O
, O
or O
‚Äú O
neighboring O
‚Äù O
. O
When O
these O
terms O
appear O
in O
the O
question O
, O
people O
are O
actually O
expecting O
POIs O
which O
are O
close O
to O
, O
or O
outside O
the O
location O
boarder O
. O
It O
implies O
that O
the O
model O
should O
avoid O
simply O
returning O
POIs O
within O
the O
location O
polygon O
. O
Fig O
2(a O
) O
shows O
the O
real O
- O
data O
distribution O
of O
POI O
with O
respect O
to O
questions O
with O
and O
without O
proximityrelated O
terms O
according O
to O
real O
- O
world O
data O
used O
in O
our O
experiments O
. O
In O
addition O
, O
concerning O
questions O
with O
proximity O
- O
related O
terms O
, O
the O
area O
of O
the O
location O
entity O
also O
has O
an O
important O
impact O
on O
the O
probability O
distribution O
of O
the O
distance O
between O
the O
selected O
POI O
coordinate O
and O
location O
entity O
polygon O
. O
As O
shown O
in O
Fig O
2(b O
) O
, O
when O
asking O
city O
- O
level O
questions O
with O
proximity O
- O
related O
terms O
( O
e.g. O
‚Äú O
Where O
can O
children O
go O
nearby O
New O
York O
? O
‚Äù O
) O
, O
the O
result O
may O
contain O
POIs O
located O
in O
city O
suburban O
district O
or O
outside O
the O
city O
; O
while O
as O
for O
AOI O
- O
level O
questions O
( O
e.g. O
‚Äú O
Where O
can O
children O
go O
nearby O
Central O
Park O
? O
‚Äù O
) O
, O
the O
result O
may O
only O
contain O
POIs O
outside546(a O
) O
  O
( O
b O
) O
Figure O
2 O
: O
The O
POI O
probability O
distribution O
concerning O
distance O
. O
X O
axis O
is O
the O
log O
distance O
between O
the O
POI O
and O
the O
location O
entity O
polygon O
. O
If O
POI O
is O
outside O
the O
polygon O
, O
the O
distance O
is O
positive O
, O
otherwise O
is O
negative O
. O
Y O
axis O
is O
the O
probability O
the O
POI O
is O
recommended O
. O
( O
PRT O
denotes O
proximity O
- O
related O
terms O
. O
but O
close O
to O
the O
border O
of O
the O
AOI O
. O
This O
is O
because O
the O
area O
of O
a O
city O
is O
much O
bigger O
than O
that O
of O
an O
AOI O
. O
With O
different O
area O
sizes O
of O
the O
location O
entity O
, O
the O
probability O
distribution O
functions O
are O
quite O
different O
. O
Distance O
Correlation O
Calculation O
The O
probability O
of O
choosing O
POI O
aigiven O
the O
location O
entity O
in O
questionqjhas O
a O
proportional O
relationship O
with O
the O
distance O
between O
the O
POI O
and O
the O
location O
entity O
polygon O
. O
That O
is O
, O
if O
a O
POI O
is O
very O
far O
away O
from O
the O
expected O
location O
, O
the O
probability O
we O
recommend O
it O
is O
close O
to O
zero O
. O
Apart O
from O
the O
distance O
, O
as O
discussed O
above O
, O
proximity O
- O
related O
terms O
and O
location O
entity O
areas O
should O
also O
be O
taken O
into O
account O
when O
calculating O
the O
likelihood O
. O
Given O
the O
location O
entity O
ylextracted O
from O
the O
questionqj O
, O
all O
factors O
, O
including O
the O
distance O
between O
the O
polygon O
of O
yland O
POIai O
, O
the O
area O
size O
ofyland O
proximity O
- O
related O
terms O
, O
have O
an O
impact O
on O
the O
likelihood O
distribution O
. O
SpeciÔ¨Åcally O
, O
we O
propose O
a O
skewed O
distribution O
based O
model O
, O
which O
takes O
the O
distance O
from O
POI O
to O
the O
location O
entity O
d(ai O
, O
yl O
) O
, O
indicator O
function O
œÑ(qj O
) O
, O
as O
well O
as O
the O
area O
of O
location O
entity O
s(yl)as O
inputs.œÑ(qj)is O
the O
indicator O
function O
that O
œÑ(qj O
) O
= O
‚àí1if O
the O
question O
contains O
proximity O
- O
related O
terms O
, O
otherwise O
œÑ(qj O
) O
= O
1 O
. O
The O
probability O
of O
choosing O
POI O
ai O
having O
the O
location O
entity O
yland O
the O
question O
qjis O
: O
P(ai|yl O
, O
qj O
) O
= O
sigmoid O
( O
Wdf(œÑ(qj)d(ai O
, O
yl O
) O
sy O
) O
) O
( O
11 O
) O
f(x O
) O
= O
2 O
œâœÜ(x‚àíŒæ O
œâ)Œ¶(Œ±x‚àíŒæ O
œâ O
) O
( O
12)œÜ(x O
) O
= O
1‚àö O
2œÄe‚àíx2 O
2 O
( O
13 O
) O
Œ¶(x O
) O
= O
/integraldisplayx O
‚àí‚àûœÜ(t)dt=1 O
2(1 O
+ O
erf(x‚àö O
2))(14 O
) O
Wheref(x)is O
the O
skewed O
normal O
distribution O
of O
x O
, O
Wdis O
what O
we O
want O
to O
optimize O
. O
Note O
that O
Œ± O
, O
Œæ O
, O
œâare O
hyper O
parameters O
. O
Given O
the O
formulation O
above O
, O
we O
can O
see O
if O
the O
questions O
do O
not O
contain O
proximity O
- O
related O
terms O
, O
œÑ(qj)value O
is O
equal O
to O
1 O
, O
POIs O
inside O
the O
polygon O
scope O
are O
what O
we O
need O
. O
As O
for O
questions O
containing O
proximityrelated O
terms O
, O
the O
smaller O
polygon O
area O
syis O
, O
the O
steeper O
the O
distribution O
curve O
will O
be O
, O
as O
a O
result O
, O
POIs O
closer O
to O
the O
polygon O
boundary O
will O
be O
more O
likely O
to O
be O
selected O
. O
3.5 O
Inference O
During O
inference O
, O
ideally O
we O
want O
to O
Ô¨Ånd O
the O
candidate O
POIs O
given O
the O
question O
qj O
. O
In O
the O
aspect O
of O
POI O
tags O
, O
we O
select O
the O
tag O
yareceiving O
the O
maximum O
score O
from O
P(ya|qj O
) O
. O
Then O
we O
reduce O
the O
candidate O
POI O
number O
by O
Ô¨Åltering O
out O
the O
POIs O
whose O
corresponding O
tag O
is O
equal O
to O
ya O
. O
After O
that O
, O
we O
calculate O
the O
semantic O
probability O
of O
choosing O
the O
POI O
as O
the O
answer O
. O
In O
distance O
correlation O
stage O
, O
the O
computation O
is O
quadratic O
in O
the O
number O
of O
location O
entities O
and O
thus O
is O
too O
expensive O
. O
We O
Ô¨Årst O
extract O
the O
location O
entity O
ylby O
NER O
and O
calculate O
the O
distance O
from O
POI O
coordinates O
to O
the O
polygon O
ofylafterwards O
. O
Take O
the O
question O
in O
Fig O
1 O
as O
an O
example O
, O
after O
the O
extraction O
step O
, O
we O
obtain O
the O
location O
entity O
‚Äú O
New O
York O
City O
‚Äù O
. O
Finally O
, O
we O
select O
the O
top O
5 O
candidate O
POIs O
with O
top O
scores O
as O
the O
result O
. O
4 O
Experiments O
4.1 O
Experiment O
Setup O
Datasets O
We O
construct O
two O
large O
- O
scale O
datasets O
, O
both O
of O
which O
are O
based O
on O
queries O
extracted O
from O
query O
logs O
of O
a O
widely O
used O
mobile O
search O
engine O
App O
and O
POIs O
obtained O
from O
an O
online O
map O
service O
provider O
. O
In O
order O
to O
Ô¨Ålter O
the O
POI O
related O
questions O
from O
the O
search O
engine O
App O
, O
we O
design O
a O
set O
of O
templates O
such O
as O
‚Äú O
where O
can O
[ O
* O
] O
go O
in O
[ O
* O
] O
‚Äù O
and O
keep O
all O
the O
queries O
that O
match O
with O
the O
templates O
. O
To O
construct O
the O
ground O
truth O
of O
question O
- O
POI O
pairs O
used O
in O
the O
training O
period O
, O
given O
questions O
satisfying O
the O
templates O
, O
we O
crawl O
the O
related O
website O
clicked547by O
the O
user O
inquiring O
the O
question O
, O
then O
calculate O
the O
similarity O
between O
the O
website O
text O
and O
POIs O
. O
Finally O
we O
choose O
POIs O
that O
are O
most O
similar O
to O
the O
website O
as O
the O
answer O
POI O
. O
For O
determining O
the O
answer O
POIs O
, O
we O
sort O
the O
POIs O
according O
to O
their O
probability O
and O
choose O
the O
top O
- O
K O
result O
as O
the O
Ô¨Ånal O
output O
. O
Moreover O
, O
all O
the O
datasets O
are O
anonymized O
due O
to O
privacy O
concerns O
. O
‚Ä¢Dataset O
A. O
This O
dataset O
mainly O
contains O
POI O
related O
questions O
whose O
geographic O
entities O
are O
located O
in O
Beijing O
. O
We O
sample O
questions O
out O
of O
one O
month O
records O
satisfying O
the O
template O
, O
and O
construct O
11,000 O
question O
- O
POI O
pairs O
. O
The O
question O
data O
is O
divided O
into O
two O
parts O
randomly O
. O
The O
training O
set O
contains O
10,900 O
question O
- O
POI O
pairs O
. O
The O
testing O
set O
is O
made O
up O
of O
100 O
questions O
which O
do O
not O
appear O
in O
the O
training O
set O
. O
‚Ä¢Dataset O
B. O
In O
this O
dataset O
, O
the O
location O
of O
the O
questions O
is O
not O
restricted O
in O
Beijing O
. O
We O
randomly O
sample O
questions O
to O
construct O
the O
question O
- O
POI O
pairs O
which O
covers O
most O
of O
the O
cities O
and O
many O
popular O
visited O
districts O
and O
AOIs O
in O
China O
. O
Similarly O
, O
there O
are O
350,900 O
question O
- O
POI O
pairs O
in O
the O
training O
set O
, O
and O
100 O
questions O
in O
the O
testing O
set O
. O
On O
average O
, O
the O
length O
of O
questions O
in O
2 O
datasets O
is O
37.8 O
Chinese O
characters O
. O
The O
average O
length O
of O
POIs O
including O
its O
tag O
information O
( O
name O
, O
tags O
, O
city O
, O
district O
and O
AOI O
) O
is O
30.3 O
characters O
. O
We O
later O
evaluate O
our O
model O
on O
these O
two O
datasets O
by O
the O
percent O
of O
hits O
at O
K O
( O
% O
hits@K O
) O
which O
is O
the O
percent O
of O
question O
- O
POI O
pairs O
whose O
POI O
appears O
in O
top O
- O
K O
retrieved O
POI O
. O
Baselines O
We O
compare O
our O
model O
with O
several O
state O
- O
of O
- O
the O
- O
art O
baselines O
to O
show O
the O
effectiveness O
of O
our O
model O
. O
The O
Ô¨Årst O
two O
are O
semantic O
parser O
based O
methods O
using O
tag O
information O
and O
the O
left O
ones O
are O
deep O
learning O
methods O
based O
on O
semantic O
matching O
. O
‚Ä¢Template O
Matching O
Method O
( O
TMM O
) O
This O
method O
Ô¨Årst O
converts O
the O
questions O
into O
SQL O
queries O
according O
to O
the O
templates O
, O
then O
retrieves O
POIs O
from O
database O
. O
‚Ä¢StanfordCoreNLP O
Stanford O
CoreNLP O
is O
an O
integrated O
NLP O
toolkit O
providing O
a O
wide O
range O
of O
linguistic O
analysis O
tools O
. O
We O
use O
it O
as O
a O
Chinese O
semantic O
parser O
to O
recognize O
the O
tags O
. O
Based O
on O
the O
tool O
, O
we O
can O
turn O
the O
question O
into O
SQL O
queries O
according O
to O
the O
semantic O
characteristics O
of O
the O
tags O
. O
‚Ä¢Bi O
- O
LSTM O
It O
is O
a O
basic O
deep O
neural O
network O
model O
which O
takes O
the O
Word2Vec O
vectors O
of O
query O
and O
answer O
as O
input O
and O
their O
cosine O
similarity O
as O
output O
( O
Tan O
et O
al O
. O
, O
2015 O
) O
. O
It O
utilizes O
a O
Bi O
- O
LSTM O
layer O
to O
capture O
question O
semantic O
features O
and O
then O
feed O
them O
into O
a O
pooling O
layer O
. O
This O
model O
takes O
the O
max O
margin O
hinge O
loss O
as O
the O
loss O
function O
. O
‚Ä¢Bi O
- O
LSTM+ATT O
( O
AQA O
) O
Compared O
with O
BiLSTM O
, O
in O
this O
model O
, O
each O
Bi O
- O
LSTM O
output O
vector O
will O
be O
multiplied O
by O
a O
softmax O
weight O
, O
which O
is O
determined O
by O
the O
answer O
embedding O
. O
‚Ä¢Bi O
- O
LSTM+C O
- O
ATT O
( O
CAQA O
) O
This O
is O
a O
state O
- O
ofthe O
- O
art O
end O
- O
to O
- O
end O
neural O
question O
answering O
model O
introduced O
by O
( O
Hao O
et O
al O
. O
, O
2017 O
) O
. O
It O
considers O
the O
double O
- O
sided O
attention O
containing O
question O
- O
to O
- O
answer O
attention O
and O
answer O
- O
toquestion O
attention O
. O
4.2 O
Experiment O
Results O
Overall O
Performance O
We O
compare O
our O
model O
with O
all O
the O
baselines O
whose O
results O
are O
shown O
in O
Table O
1 O
. O
Conclusions O
observed O
are O
listed O
as O
follows O
. O
( O
1 O
) O
Compared O
with O
typical O
neural O
network O
based O
models O
, O
semantic O
parsing O
based O
methods O
have O
a O
higher O
% O
hits@K O
rate O
on O
the O
whole O
. O
However O
, O
with O
the O
lack O
of O
Ô¨Çexibility O
, O
their O
% O
hits@K O
rate O
is O
worse O
than O
our O
PJI O
model O
. O
( O
2 O
) O
In O
general O
, O
models O
with O
attention O
mechanism O
reach O
better O
performance O
than O
models O
without O
. O
Bidirectional O
attention O
models O
achieve O
higher O
% O
hits@K O
rate O
than O
unidirectional O
one O
, O
which O
indicates O
there O
exists O
several O
parts O
in O
the O
questions O
as O
well O
as O
POI O
attributes O
that O
should O
be O
put O
emphasis O
on O
. O
( O
3 O
) O
Our O
model O
achieves O
the O
best O
overall O
performance O
among O
all O
the O
models O
. O
In O
terms O
of O
% O
hits@K O
rate O
, O
no O
matter O
what O
K O
is O
, O
the O
rate O
of O
our O
model O
is O
beyond O
95 O
% O
. O
Our O
model O
utilizes O
several O
neural O
network O
modules O
instead O
of O
calculating O
the O
semantic O
similarity O
directly O
. O
Moreover O
, O
thanks O
to O
the O
cross O
- O
attention O
question O
embedding O
structure O
, O
our O
model O
puts O
strong O
emphasis O
on O
the O
distance O
and O
tag O
related O
patterns O
of O
both O
questions O
and O
POIs O
. O
In O
addition O
, O
we O
use O
a O
special O
probability O
distribution O
to O
handle O
questions O
with O
proximityrelated O
geographic O
terms O
which O
are O
treated O
the O
same O
as O
normal O
questions O
in O
the O
baselines.548Dataset O
A O
Dataset O
B O
hits@1 O
hits@3 O
hits@5 O
hits@1 O
hits@3 O
hits@5 O
TMM O
84.9 O
% O
84.9 O
% O
84.9 O
% O
82.8 O
% O
82.8 O
% O
82.8 O
% O
CoreNLP O
88.9 O
% O
88.9 O
% O
88.9 O
% O
86.5 O
% O
86.5 O
% O
86.5 O
% O
Bi O
- O
LSTM O
55.6 O
% O
56.0 O
% O
61.7 O
% O
29.1 O
% O
29.4 O
% O
31.1 O
% O
AQA O
65.2 O
% O
67.3 O
% O
68.8 O
% O
37.5 O
% O
38.9 O
% O
35.6 O
% O
CAQA O
69.2 O
% O
56.1 O
% O
68.1 O
% O
42.1 O
% O
42.8 O
% O
49.0 O
% O
PJI O
98.6 O
% O
98.9 O
% O
99.0 O
% O
97.2 O
% O
97.9 O
% O
99.1 O
% O
Table O
1 O
: O
The O
overall O
performance O
over O
two O
datasets O
. O
Three O
- O
level O
Location O
Performance O
. O
Table O
2 O
shows O
the O
% O
hits@5 O
of O
two O
datasets O
where O
questions O
contain O
city O
, O
district O
and O
AOI O
location O
entities O
, O
respectively O
. O
As O
shown O
in O
the O
table O
, O
no O
matter O
which O
model O
we O
use O
, O
city O
level O
questions O
obtain O
the O
best O
result O
compared O
to O
other O
two O
types O
. O
The O
reason O
is O
that O
the O
number O
of O
cities O
in O
the O
whole O
nation O
is O
rather O
small O
and O
there O
is O
almost O
no O
duplicate O
city O
names O
among O
them O
. O
However O
, O
both O
AOI O
and O
district O
names O
can O
have O
a O
lot O
of O
duplications O
thus O
causing O
ambiguity O
and O
noise O
. O
Moreover O
, O
due O
to O
the O
hierarchical O
nature O
of O
the O
location O
entity O
, O
AOI O
names O
appear O
in O
different O
formats O
, O
which O
increases O
the O
difÔ¨Åculty O
of O
POI O
retrieval O
. O
Therefore O
, O
the O
template O
- O
based O
method O
and O
the O
end O
- O
to O
- O
end O
similarity O
matching O
method O
may O
be O
far O
from O
meeting O
the O
real O
- O
world O
demands O
of O
POI O
oriented O
QA O
. O
Despite O
the O
challenges O
we O
mentioned O
above O
, O
our O
model O
still O
outperforms O
all O
the O
baselines O
on O
city O
, O
district O
and O
AOI O
questions O
. O
Dataset O
A O
Dataset O
B O
Cit O
. O
Dis O
. O
AOI O
Cit O
. O
Dis O
. O
AOI O
TMM O
94.1 O
% O
92.0 O
% O
91.3 O
% O
93.2 O
% O
91.6 O
% O
90.2 O
% O
CoreNLP O
96.0 O
% O
93.4 O
% O
58.3 O
% O
90.3 O
% O
90.1 O
% O
31.8 O
% O
Bi O
- O
LSTM O
92.1 O
% O
76.2 O
% O
8.4 O
% O
90.9 O
% O
62.1 O
% O
3.5 O
% O
AQA O
92.4 O
% O
78.5 O
% O
9.1 O
% O
92.2 O
% O
62.8 O
% O
9.3 O
% O
CAQA O
92.9 O
% O
79.4 O
% O
9.7 O
% O
92.6 O
% O
63.1 O
% O
9.7 O
% O
PJI O
100 O
% O
99.6 O
% O
97.3 O
% O
99.8 O
% O
99.2 O
% O
97.0 O
% O
Table O
2 O
: O
The O
% O
hits@5 O
rate O
on O
questions O
containing O
different O
location O
entities O
. O
4.3 O
Proximity O
- O
related O
Term O
Analysis O
Fig O
3 O
shows O
the O
% O
hits@5 O
with O
and O
without O
proximity O
- O
related O
terms O
on O
Dataset O
A O
and O
B. O
From O
the O
result O
we O
can O
conclude O
that O
all O
existing O
baselines O
can O
not O
handle O
questions O
with O
proximityrelated O
geographic O
terms O
. O
For O
traditional O
neural O
network O
QA O
models O
, O
the O
model O
has O
no O
idea O
how O
important O
these O
words O
are O
and O
considers O
them O
just O
as O
normal O
words O
. O
As O
a O
result O
, O
the O
results O
returned O
do O
not O
make O
sense O
to O
the O
users O
. O
Nevertheless O
, O
this O
problem O
gets O
tackled O
by O
the O
distance O
probability O
module O
in O
our O
model O
. O
Therefore O
, O
our O
model O
outperforms O
the O
baselines O
when O
it O
comes O
to O
these O
kinds O
of O
problems O
to O
a O
great O
extent O
. O
( O
a O
) O
Dataset O
A O
  O
( O
b O
) O
Dataset O
B O
Figure O
3 O
: O
The O
% O
hits@5 O
rate O
concerning O
questions O
with O
and O
without O
proximity O
- O
related O
terms O
on O
two O
datasets O
. O
5 O
Conclusion O
In O
this O
paper O
, O
we O
propose O
a O
novel O
deep O
learning O
framework O
with O
joint O
inference O
to O
solve O
the O
POI O
oriented O
question O
answering O
task O
. O
Our O
main O
contributions O
lie O
in O
three O
aspects O
. O
First O
, O
this O
model O
handles O
the O
POI O
oriented O
QA O
with O
the O
help O
of O
tag O
semantic O
module O
and O
distance O
correlation O
module O
. O
Second O
, O
by O
introducing O
a O
cross O
attention O
based O
question O
embedding O
structure O
, O
we O
achieve O
a O
precise O
and O
Ô¨Çexible O
representation O
of O
questions O
. O
Third O
, O
the O
proposed O
model O
can O
overcome O
several O
challenges O
of O
POI O
oriented O
QA O
including O
POI O
tag O
recognition O
, O
proximity O
- O
related O
term O
processing O
and O
diverse O
distance O
correlation O
. O
Extensive O
experiments O
on O
two O
real O
- O
world O
datasets O
are O
carried O
out O
to O
demonstrate O
the O
effectiveness O
of O
our O
model O
. O
The O
result O
shows O
that O
our O
approach O
outperforms O
all O
the O
baselines O
and O
state O
- O
of O
- O
the O
- O
art O
models O
. O
Acknowledgments O
The O
work O
described O
in O
this O
paper O
is O
partially O
supported O
by O
a O
grant O
from O
the O
Research O
Grant O
Council O
of O
the O
Hong O
Kong O
Special O
Administrative O
Region O
, O
China O
( O
Project O
Codes O
: O
14200719 O
) O
. O
Abstract O
We O
show O
that O
leveraging O
metadata O
information O
from O
web O
pages O
can O
improve O
the O
performance O
of O
models O
for O
answer O
passage O
selection O
/ O
reranking O
. O
We O
propose O
a O
neural O
passage O
selection O
model O
that O
leverages O
metadata O
information O
with O
a O
Ô¨Åne O
- O
grained O
encoding O
strategy O
, O
which O
learns O
the O
representation O
for O
metadata O
predicates O
in O
a O
hierarchical O
way O
. O
The O
models O
are O
evaluated O
on O
the O
MS O
MARCO O
( O
Nguyen O
et O
al O
. O
, O
2016 O
) O
and O
Recipe O
- O
MARCO O
datasets O
. O
Results O
show O
that O
our O
models O
signiÔ¨Åcantly O
outperform O
baseline O
models O
, O
which O
do O
not O
incorporate O
metadata O
. O
We O
also O
show O
that O
the O
Ô¨Ånegrained O
encoding O
‚Äôs O
advantage O
over O
other O
strategies O
for O
encoding O
the O
metadata O
. O
1 O
Introduction O
Question O
answering O
( O
QA O
) O
is O
a O
long O
- O
standing O
task O
in O
NLP O
and O
IR O
. O
Having O
QA O
systems O
that O
perform O
well O
on O
real O
- O
world O
questions O
is O
of O
signiÔ¨Åcant O
value O
for O
search O
engines O
and O
intelligent O
assistants O
. O
While O
some O
of O
the O
earliest O
work O
tackled O
the O
task O
of O
answering O
questions O
based O
on O
a O
large O
corpus O
( O
V O
oorhees O
and O
Tice O
, O
2000 O
; O
V O
oorhees O
, O
2003 O
; O
Wang O
et O
al O
. O
, O
2007 O
) O
( O
albeit O
mostly O
focusing O
on O
simple O
fact O
- O
oriented O
questions O
) O
, O
much O
of O
the O
recent O
work O
on O
QA O
has O
focused O
on O
answering O
questions O
in O
a O
less O
realistic O
setting O
‚Äì O
drawing O
the O
answer O
from O
a O
paragraph O
of O
text O
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
; O
Joshi O
et O
al O
. O
, O
2017 O
) O
, O
which O
is O
commonly O
referred O
to O
as O
machine O
reading O
comprehension O
( O
MRC O
) O
. O
In O
this O
work O
, O
we O
tackle O
the O
more O
realistic O
problem O
‚Äî O
candidate O
answers O
passages O
selection O
/ O
reranking O
for O
real O
- O
world O
questions O
on O
the O
web O
. O
In O
contrast O
to O
both O
MRC O
and O
early O
work O
on O
QA O
from O
a O
large O
corpus O
, O
web O
pages O
often O
provide O
an O
additional O
source O
of O
knowledge O
. O
In O
particular O
, O
and O
thanks O
in O
part O
to O
the O
Semantic O
Web O
initiative O
( O
Berners O
- O
Lee O
‚àóWork O
conducted O
during O
internship O
at O
Microsoft O
Research O
. O
‚Ä¶ O
Textual O
ObjectPredicateClassic O
Meatloaf O
/ O
recipe O
/ O
name20 O
minutes O
/ O
recipe O
/ O
preptime1 O
hour O
, O
10 O
minutes O
/ O
recipe O
/ O
cookTime1 O
celery O
rib O
, O
‚Ä¶ O
/recipe O
/ O
ingredients O
Metadata O
Object O
- O
predicate O
Pairs O
: O
Figure O
1 O
: O
Metadata O
Example O
from O
SimplyRecipes O
. O
RecipecookTimeprepTimerecipeInstructionsrecipeyieldetc O
. O
Figure O
2 O
: O
Hierarchy O
diagram O
showing O
properties O
of O
‚Äú O
recipe O
‚Äù O
from O
schema.org/recipe O
. O
et O
al O
. O
, O
2001 O
) O
, O
it O
is O
estimated O
that O
a O
non O
- O
trivial O
portion O
of O
web O
pages O
contain O
metadata O
annotations O
that O
provide O
a O
deeper O
understanding O
of O
the O
website O
content O
. O
The O
Web O
Data O
Commons O
project O
( O
M√ºhleisen O
and O
Bizer O
, O
2012 O
) O
estimates O
that O
0.9 O
billion O
HTML O
pages O
out O
of O
the O
2.5 O
billion O
pages O
( O
37.1 O
% O
) O
in O
the O
Common O
Crawl O
web O
corpus1contain O
structured O
metadata O
. O
Figure O
1 O
shows O
an O
example O
of O
this O
metadata O
which O
comes O
in O
the O
form O
of O
objectpredicate O
pairs O
annotated O
with O
schema.org O
tags O
‚Äì O
a O
set O
of O
tags O
/ O
predicates O
deÔ¨Åned O
in O
the O
schema.org2 O
hierarchy O
. O
In O
the O
example O
, O
the O
hierarchical O
metadata O
is O
used O
to O
add O
more O
structure O
to O
the O
web O
page O
of O
a O
recipe O
, O
providing O
meaning O
to O
the O
otherwise O
unstructured O
content O
. O
This O
makes O
several O
aspects O
of O
the O
recipe O
explicit O
‚Äì O
the O
preparation O
time O
( O
PREP O
TIME O
) O
, O
cooking O
time O
( O
COOK O
TIME O
) O
, O
ingredients O
( O
INGREDIENTS O
) O
, O
etc O
. O
Figure O
2 O
shows O
the O
‚Äú O
recipe O
‚Äù O
object O
in O
schema.org O
; O
it O
contains O
several O
properties O
such O
as O
COOK O
TIME O
, O
PREP O
TIME O
, O
1http://commoncrawl.org O
2http://schema.org551Is O
selected O
URL O
Passage O
Text O
allrecipes.com O
... O
Preheat O
oven O
to350degrees O
F O
andlightly O
grease O
a O
... O
instructions O
simplyrecipes.com O
...... O
Bake O
for O
1hour O
and10min O
cookTime O
or O
until O
a O
meat O
thermometer O
inserted O
... O
 O
thekitchn.com O
... O
Any O
ground O
meat O
can O
be O
used O
to O
make O
meatloaf O
: O
beef O
, O
pork O
, O
veal O
ingredients O
... O
 O
livestrong.com O
...... O
loaf O
to O
stand O
for O
10to15min O
cookTime O
before O
slicing O
and O
servingitto4 O
- O
6 O
yield O
... O
Table O
1 O
: O
Example O
of O
answer O
passage O
selection O
on O
the O
Web O
. O
There O
are O
4 O
candidate O
passages O
the O
query O
‚Äú O
How O
long O
should O
I O
cook O
ground O
beef O
meat O
loaf O
in O
the O
oven O
? O
‚Äù O
RECIPE O
INSTRUCTIONS O
, O
etc O
. O
We O
hypothesize O
that O
leveraging O
this O
metadata O
, O
in O
addition O
to O
the O
textual O
content O
, O
will O
improve O
the O
performance O
of O
QA O
systems O
on O
the O
Web O
. O
Table O
1 O
presents O
an O
example O
of O
a O
query O
and O
several O
candidate O
passages O
. O
The O
candidate O
answer O
passages O
are O
decorated O
by O
colored O
spans O
that O
denote O
a O
corresponding O
schema.org O
predicate O
property O
. O
The O
correct O
answer O
( O
‚Äú O
1 O
hour O
and O
10 O
min O
‚Äù O
) O
could O
be O
inferred O
from O
the O
metadata O
tag O
COOK O
TIME O
. O
While O
it O
seems O
clear O
from O
the O
example O
that O
the O
hierarchical O
schema.org O
metadata O
can O
be O
exploited O
in O
web O
QA O
, O
it O
will O
only O
be O
of O
true O
beneÔ¨Åt O
if O
the O
use O
of O
metadata O
is O
prevalent O
in O
web O
pages O
. O
Luckily O
, O
this O
is O
the O
case O
as O
shown O
by O
Guha O
et O
al O
. O
who O
studied O
a O
sample O
of O
10 O
billion O
web O
pages O
and O
showed O
that O
one O
third O
( O
31.3 O
% O
) O
of O
the O
pages O
have O
schema.org O
markup O
. O
To O
date O
, O
the O
end O
- O
to O
- O
end O
web O
QA O
systems O
have O
not O
made O
use O
of O
this O
metadata O
information O
. O
We O
Ô¨Årst O
explore O
how O
to O
incorporate O
( O
and O
the O
effect O
of O
incorporating O
) O
semantic O
web O
hierarchical O
metadata O
into O
statistical O
NLP O
models O
for O
web O
- O
based O
QA O
. O
More O
speciÔ¨Åcally O
, O
we O
introduce O
a O
Ô¨Åne O
- O
grained O
encoding O
method O
for O
metadata O
predicates O
, O
to O
better O
leverage O
the O
semantic O
information O
in O
it O
. O
We O
evaluate O
the O
models O
on O
the O
answer O
passage O
selection O
/ O
re O
- O
ranking O
task O
of O
MS O
MARCO O
( O
Nguyen O
et O
al O
. O
, O
2016 O
) O
, O
that O
contains O
real O
user O
queries O
sampled O
from O
the O
Bing O
search O
engine O
, O
with O
the O
answer O
passages O
extracted O
from O
real O
- O
world O
web O
pages O
. O
Results O
show O
that O
our O
approaches O
outperform O
the O
baseline O
systems O
substantially O
, O
with O
more O
signiÔ¨Åcant O
gains O
on O
the O
subset O
of O
queries O
whose O
candidate O
passages O
contain O
richer O
metadata O
tags O
. O
Our O
work O
demonstrates O
the O
importance O
of O
encoding O
metadata O
information O
for O
QA O
, O
and O
veriÔ¨Åes O
our O
hypothesis O
that O
the O
metadata O
knowledge O
can O
signiÔ¨Åcantly O
beneÔ¨Åt O
the O
performance O
of O
the O
neural O
models O
. O
We O
also O
provide O
qualitative O
analysis O
that O
includes O
performance O
comparisons O
acrossdomains O
. O
Our O
Ô¨Åndings O
further O
provide O
motivation O
for O
webmasters O
to O
annotate O
their O
web O
pages O
with O
semantic O
schema.org O
markup O
and O
for O
question O
answering O
systems O
developer O
to O
leverage O
them O
. O
2 O
Related O
Work O
Our O
work O
is O
related O
to O
several O
directions O
of O
work O
in O
semantic O
web O
, O
NLP O
and O
ML O
. O
Metadata O
for O
NLP O
and O
ML O
Metadata O
like O
time O
stamp O
( O
Blei O
and O
Lafferty O
, O
2006 O
) O
and O
rating O
( O
Mcauliffe O
and O
Blei O
, O
2008 O
) O
have O
been O
successfully O
incorporated O
in O
document O
modeling O
. O
In O
community O
question O
answering O
, O
metadata O
is O
often O
used O
as O
hard O
features O
to O
improve O
the O
model O
performance O
‚Äì O
category O
metadata O
( O
Cao O
et O
al O
. O
, O
2010 O
; O
Zhou O
et O
al O
. O
, O
2015 O
) O
and O
user O
- O
level O
information O
and O
question- O
and O
answer O
- O
speciÔ¨Åc O
data O
( O
Joty O
et O
al O
. O
, O
2018 O
; O
Xu O
et O
al O
. O
, O
2018 O
) O
. O
For O
answer O
quality O
prediction O
, O
author O
information O
( O
Burel O
et O
al O
. O
, O
2012 O
; O
Suggu O
et O
al O
. O
, O
2016 O
) O
has O
been O
often O
incorporated O
. O
In O
our O
work O
, O
we O
investigate O
how O
to O
leverage O
the O
general O
metadata O
knowledge O
from O
schema.org O
in O
web O
answer O
passage O
selection O
. O
Our O
metadata O
schema O
used O
, O
as O
compared O
to O
prior O
work O
mentioned O
, O
is O
structural O
and O
hierarchical O
, O
and O
applies O
to O
general O
web O
pages O
. O
The O
metadata O
could O
provide O
rich O
information O
to O
better O
understand O
the O
textual O
content O
on O
the O
web O
. O
Semantic O
Web O
Berners O
- O
Lee O
et O
al O
. O
( O
2001 O
) O
described O
the O
vision O
of O
the O
Semantic O
Web O
. O
The O
authors O
envisioned O
an O
extension O
of O
the O
World O
Wide O
Web O
, O
in O
which O
information O
is O
given O
well O
- O
deÔ¨Åned O
meaning O
by O
bringing O
structure O
to O
the O
content O
of O
web O
pages O
. O
Ten O
years O
later O
, O
several O
major O
search O
engines O
have O
come O
together O
to O
launch O
the O
schema.org O
initiative O
, O
that O
to O
focus O
on O
creating O
, O
maintaining O
and O
promoting O
a O
common O
set O
of O
schemas O
for O
structured O
data O
markup O
on O
web O
pages O
. O
Webmasters O
use O
this O
schema O
to O
add O
metadata O
tags O
to O
their O
websites O
in O
order O
to O
help O
search O
engines O
understand O
the O
content O
. O
The O
use O
of O
such O
metadata O
has O
gained O
more O
popularity O
over O
the O
years O
. O
3 O
Leveraging O
Metadata O
for O
Answer O
Passage O
Selection O
In O
our O
setting O
of O
answer O
passage O
selection O
, O
the O
input O
to O
the O
system O
is O
a O
set O
of O
candidate O
passages O
p1 O
, O
... O
, O
p O
n O
, O
and O
a O
query O
q O
, O
the O
goal O
is O
to O
identify O
the O
passage O
that O
best O
answers O
the O
question.552For O
each O
candidate O
passage O
pi O
, O
we O
have O
the O
URL O
iof O
the O
web O
page O
from O
where O
it O
is O
extracted O
. O
The O
web O
document O
from O
URL O
i O
, O
may O
contain O
a O
list O
of O
metadata O
object O
- O
predicate O
pairs O
( O
obj1,pred1 O
) O
, O
... O
, O
( O
objm O
, O
predm O
) O
. O
The O
detailed O
approach O
of O
obtaining O
the O
pairs O
is O
presented O
in O
Section O
( O
3.1 O
) O
. O
Each O
predicate O
pred O
jconsists O
of O
a O
root O
rjand O
a O
property O
pro O
j(e.g O
. O
, O
RECIPE O
and O
COOK O
TIME O
for O
/ O
RECIPE O
/COOKTIME O
, O
respectively O
) O
. O
We O
denote O
the O
path O
between O
rjandpro O
jasptj O
. O
3.1 O
Generate O
Metadata O
- O
Decorated O
Passages O
Algorithm O
1 O
generates O
the O
decorated O
answer O
passages O
with O
metadata O
. O
The O
example O
for O
a O
decorated O
passage O
is O
shown O
immediately O
after O
the O
algorithm O
. O
The O
spans O
are O
marked O
up O
with O
the O
metadata O
predicate O
features O
. O
The O
decorated O
results O
are O
later O
used O
as O
input O
for O
our O
models O
. O
To O
be O
more O
speciÔ¨Åc O
, O
given O
the O
queryPsgExample O
( O
including O
query O
, O
candidate O
answer O
passage O
, O
URL O
, O
label O
of O
whether O
is O
selected O
) O
and O
metadata O
object O
- O
predicate O
pairs O
as O
input O
, O
we O
aim O
to O
obtain O
the O
queryPsgExamples O
whose O
candidate O
answer O
passages O
are O
decorated O
. O
We O
Ô¨Årst O
obtain O
all O
the O
metadata O
pairs O
( O
matchingMetaPairs O
) O
for O
the O
URL O
where O
the O
passage O
text O
appears O
( O
line O
1 O
) O
. O
Then O
, O
for O
each O
metadata O
pair O
in O
matchingMetaPairs O
, O
we O
employ O
a O
similarity O
function O
( O
MetaSim O
in O
line O
6 O
) O
to O
Ô¨Årst O
compute O
the O
similarity O
between O
all O
possible O
text O
spans O
of O
the O
passage O
and O
the O
object O
text O
in O
the O
metadata O
object O
- O
predicate O
pair O
; O
afterwards O
the O
function O
records O
the O
start O
and O
end O
offset O
of O
the O
text O
spans O
which O
have O
a O
similarity O
score O
higher O
than O
the O
threshold O
. O
In O
our O
case O
, O
we O
use O
BLEU-4 O
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
as O
MetaSim O
. O
It O
calculates O
a O
score O
for O
up O
to O
4 O
- O
grams O
overlap O
using O
uniform O
weights O
. O
A O
metadata O
- O
decorated O
candidate O
passage O
with O
the O
algorithm O
is O
presented O
in O
Table O
2 O
. O
Algorithm O
1 O
: O
How O
to O
obtain O
for O
metadata O
for O
each O
URL O
and O
generate O
metadata O
- O
decorated O
passage O
Data O
: O
queryPsgEg O
( O
query O
, O
psgText O
, O
URL O
, O
label O
) O
, O
metaPairs O
( O
subj O
, O
pred O
, O
obj O
, O
URL O
) O
; O
1matchingMetaPairs O
‚ÜêJoin(queryPsgEg[URL O
] O
= O
= O
metaPairs[URL O
] O
) O
; O
2foreach O
pair‚ààmatchingMetaPairs O
do O
3 O
ifpair[obj O
] O
is O
not O
text O
then O
4 O
continue O
; O
5 O
else O
6 O
startOffsets O
, O
endOffsets O
, O
score O
‚Üê O
MetaSim O
( O
queryPsgEg[psgText O
] O
, O
pair[obj O
] O
) O
; O
7 O
Decorate O
( O
queryPsgEg[psgText O
] O
, O
startOffsets O
, O
endOffsets O
, O
pred O
) O
; O
8 O
end O
9endword O
Rinse O
tilapia O
Ô¨Ållets O
in O
cold O
water O
... O
Season O
both O
sides O
with O
salt O
and O
pepper O
pred O
. O
O O
B_R_ING O
I_R_ING O
O O
O O
O O
O O
feature O
O O
O O
O O
O O
B_R_ING O
I_R_ING O
I_R_ING O
Table O
2 O
: O
Metadata O
- O
Decorated O
Candidate O
Passage O
3.2 O
Neural O
Passage O
Selection O
with O
Fine O
- O
grained O
Metadata O
Encoding O
We O
propose O
a O
simple O
but O
effective O
neural O
network O
structure O
for O
building O
our O
base O
neural O
passage O
selector O
( O
NPS O
) O
. O
Similar O
to O
the O
neural O
reader O
( O
Hermann O
et O
al O
. O
, O
2015 O
; O
Chen O
et O
al O
. O
, O
2017 O
) O
for O
MRC O
, O
we O
Ô¨Årst O
obtain O
a O
feature O
- O
rich O
( O
including O
the O
Ô¨Åne O
- O
grained O
encoding O
of O
the O
metadata O
) O
contextualized O
representation O
for O
each O
token O
in O
the O
passage O
and O
query O
. O
The O
output O
layer O
takes O
the O
passage O
and O
query O
representations O
as O
input O
and O
makes O
the O
prediction O
. O
Fine O
- O
grained O
metadata O
embedding O
each O
predicate O
feature O
pred O
( O
e.g. O
, O
/RECIPE O
/COOK O
TIME O
) O
includes O
the O
root O
r(RECIPE O
) O
and O
the O
property O
pro O
( O
COOK O
TIME O
) O
. O
To O
leverage O
this O
information O
, O
we O
propose O
to O
leverage O
the O
hierarchy O
present O
on O
the O
predicate O
by O
learning O
the O
root O
embedding O
Er O
, O
the O
property O
embedding O
Epro O
, O
as O
well O
as O
the O
path O
embedding O
Ept(RECIPE‚ÜíCOOK O
TIME O
) O
, O
instead O
of O
only O
learning O
an O
embedding O
of O
the O
entire O
predicate O
( O
/RECIPE O
/COOK O
TIME O
) O
. O
Thus O
, O
the O
Ô¨Ånal O
predicate O
feature O
encoding O
for O
token O
tiis O
the O
concatenation O
of O
the O
three O
components O
: O
Epred(pred O
i O
) O
= O
concat O
( O
Er(ri),Epro(pro O
i),Ept(pti O
) O
) O
. O
Passage O
& O
Query O
encoding O
We O
Ô¨Årst O
represent O
each O
token O
tiin O
the O
passage O
with O
a O
vector O
representation O
and O
pass O
it O
through O
a O
multi O
- O
layer O
BiLSTM O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
network O
to O
get O
the O
contextualized O
representation O
for O
each O
token O
( O
t1,t2 O
, O
... O
) O
, O
where O
tiis O
the O
concatenation O
of O
: O
‚Ä¢(Contextualized O
) O
word O
embedding O
: O
GloVe O
840B.300d O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
embeddings O
is O
used O
to O
initialize O
the O
embedding O
layer O
and O
is O
Ô¨Åne O
- O
tuned O
during O
training O
, O
we O
denote O
it O
as O
Àútifor O
token O
ti O
. O
Besides O
, O
we O
also O
use O
the O
pretrained O
contextualized O
representations O
produced O
by O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
ÀÜ O
q1 O
, O
... O
, O
ÀÜ O
qm O
, O
... O
, O
ÀÜt1 O
, O
... O
, O
ÀÜtn= O
BERT O
( O
[ O
CLS O
] O
, O
q1 O
, O
... O
, O
q O
m,[SEP O
] O
, O
t1 O
, O
... O
, O
t O
n O
) O
. O
For O
the O
ithtoken O
, O
the O
word O
embedding O
E(ti O
) O
is O
the O
concatenation O
of O
the O
two O
. O
‚Ä¢Metadata O
predicate O
embedding O
: O
We O
use O
the O
Ô¨Åne O
- O
grained O
predicate O
encoding O
of O
metadata553pair O
( O
Epred(pred O
i O
) O
) O
, O
as O
described O
above O
. O
Embedding O
for O
beginning O
( O
B O
_ O
) O
and O
intermediate O
( O
I O
_ O
) O
tokens O
of O
a O
decorated O
span O
are O
different O
and O
learned O
during O
training O
; O
For O
the O
other O
passage O
tokens O
that O
are O
not O
metadata O
- O
decorated O
, O
their O
predicate O
( O
O O
) O
embedding O
are O
Ô¨Ålled O
with O
zero O
vectors O
. O
‚Ä¢Aligned O
query O
embedding O
: O
Similar O
to O
( O
Chen O
et O
al O
. O
, O
2017 O
) O
, O
we O
also O
incorporate O
the O
aligned O
query O
embedding O
. O
This O
feature O
is O
intended O
to O
capture O
the O
similarity O
between O
tiand O
each O
query O
word O
qj O
. O
For O
the O
ithtoken O
ti O
. O
It O
is O
calculated O
as:/summationtext O
jE(qj)‚àósim(E(ti),E(qj O
) O
) O
. O
The O
encoding O
pkfor O
candidate O
passage O
kis O
the O
sum O
of O
the O
token O
representations O
after O
the O
BiLSTM O
. O
Similarly O
, O
query O
token O
embedding O
qjis O
the O
concatenation O
of O
its O
contextualized O
word O
embedding O
( O
ÀÜ O
qj O
) O
and O
the O
GloVe O
embedding O
. O
We O
pass O
it O
through O
another O
BiLSTM O
, O
and O
use O
the O
sum O
operation O
to O
obtain O
the O
query O
encoding O
q. O
Prediction O
Finally O
, O
the O
‚Äú O
Is_selected O
‚Äù O
score O
for O
passage O
kis O
calculated O
as O
a O
function O
of O
the O
passage O
encoding O
pkand O
the O
query O
encoding O
q O
: O
score O
( O
k O
) O
= O
softmax O
( O
pkWq O
) O
. O
At O
test O
time O
, O
we O
calculate O
score O
( O
1 O
) O
, O
... O
, O
score O
( O
n)for O
all O
the O
candidate O
answer O
passages O
, O
and O
select O
the O
passage O
with O
highest O
score O
: O
argmaxk(score O
( O
k O
) O
) O
. O
4 O
Experiments O
and O
Analysis O
This O
section O
Ô¨Årst O
presents O
the O
QA O
dataset O
that O
is O
used O
for O
evaluation O
, O
and O
then O
describe O
results O
comparing O
different O
methods O
( O
with O
or O
without O
leveraging O
the O
metadata O
information O
) O
. O
4.1 O
Datasets O
and O
Models O
We O
evaluate O
our O
models O
on O
the O
passage O
selection O
task O
of O
MS O
MARCO O
( O
Nguyen O
et O
al O
. O
, O
2016 O
) O
, O
to O
our O
knowledge O
, O
this O
is O
currently O
the O
only O
large O
- O
scale O
real O
- O
world O
QA O
/ O
MRC O
dataset O
on O
general O
web O
pages O
, O
that O
is O
paired O
with O
URLs O
from O
which O
the O
candidate O
passages O
are O
extracted O
. O
To O
measure O
how O
the O
models O
perform O
when O
trained O
and O
tested O
on O
a O
subset O
of O
queries O
from O
a O
focused O
domain O
, O
where O
the O
usage O
of O
schema.org O
metadata O
is O
more O
prevalent O
, O
we O
extract O
the O
QA O
pairs O
of O
the O
recipes O
domain O
from O
MS O
MARCO O
dataset O
and O
extend O
it O
with O
extra O
QA O
pairs O
in O
this O
domain O
( O
Recipe O
- O
MARCO O
) O
. O
Table O
3 O
shows O
the O
number O
of O
queries O
for O
the O
datasets O
. O
Although O
WikiQA O
( O
Yang O
et O
al O
. O
, O
2015 O
) O
and O
Natural O
Questions O
( O
Kwiatkowski O
et O
al O
. O
, O
2019 O
) O
also O
containMARCO O
Recipe O
MARCO O
Train O
82,326 O
7515 O
Dev O
10,047 O
835 O
Test O
9650 O
846 O
Table O
3 O
: O
Statistics O
of O
Datasets O
. O
queries O
from O
real O
users O
, O
their O
answer O
candidates O
are O
restricted O
to O
be O
from O
Wikipedia O
. O
However O
, O
the O
adoption O
of O
schema.org O
tags O
in O
Wikipedia O
pages O
is O
very O
low O
( O
< O
2.2%3 O
) O
. O
This O
is O
signiÔ¨Åcantly O
less O
than O
general O
web O
pages O
where O
the O
adoption O
rate O
of O
schema.org O
metadata O
is O
around O
31.3 O
% O
. O
Thus O
we O
do O
not O
use O
these O
datasets O
for O
evaluation O
. O
We O
follow O
previous O
work O
( O
Yang O
et O
al O
. O
, O
2015 O
; O
Tan O
et O
al O
. O
, O
2018 O
) O
on O
reporting O
precision@1 O
( O
P@1 O
) O
and O
Mean O
Reciprocal O
Rank O
( O
MRR O
) O
. O
P@1 O
measures O
whether O
the O
highest O
scoring O
answer O
passage O
returned O
matches O
the O
correct O
passage O
. O
MRR O
( O
V O
oorhees O
and O
Tice O
, O
2000 O
) O
evaluates O
the O
relative O
rank O
of O
the O
correct O
passage O
in O
the O
candidate O
passages O
. O
We O
compare O
our O
models O
to O
several O
baselines O
, O
S O
- O
Net O
( O
Tan O
et O
al O
. O
, O
2018 O
) O
is O
a O
prior O
state O
- O
of O
- O
theart O
model O
on O
MS O
MARCO O
, O
it O
also O
produces O
synthetic O
answers O
and O
use O
text O
generation O
metrics O
( O
e.g. O
, O
BLEU O
and O
ROUGE O
- O
L O
) O
. O
In O
this O
work O
, O
we O
only O
compare O
to O
its O
capability O
of O
passage O
re O
- O
ranking O
. O
NPS O
is O
the O
baseline O
‚Äú O
neural O
passage O
selector O
‚Äù O
which O
does O
not O
encode O
metadata O
information O
. O
It O
‚Äôs O
similar O
to O
the O
implementation O
in O
Dai O
and O
Callan O
( O
2019 O
) O
. O
B O
- O
NPS O
is O
a O
version O
of O
our O
model O
which O
builds O
upon O
NPS O
anddirectly O
encodes O
the O
entire O
predicate O
. O
F O
- O
NPS O
is O
our O
main O
model O
‚Äì O
Ô¨Åne O
- O
grained O
metadata O
encoding O
enriched O
neural O
passage O
selector O
. O
We O
also O
report O
the O
results O
of O
selecting O
the O
Ô¨Årst O
and O
a O
random O
passage O
. O
4.2 O
Results O
and O
Analysis O
Table O
4 O
shows O
the O
comparison O
of O
different O
methods O
on O
the O
candidate O
passage O
selection O
task O
. O
We O
see O
that O
: O
( O
1 O
) O
By O
leveraging O
the O
metadata O
, O
both O
versions O
of O
our O
model O
( O
B O
- O
NPS O
and O
F O
- O
NPS O
) O
outperform O
the O
baseline O
NPS O
model O
; O
( O
2 O
) O
With O
Ô¨Åne O
- O
grained O
encoding O
, O
F O
- O
NPS O
signiÔ¨Åcantly O
outperforms O
all O
models O
in O
both O
P@1 O
and O
MRR O
. O
Particularly O
, O
F O
- O
NPS O
achieves O
higher O
P@1 O
than O
NPS O
by O
around O
2 O
% O
; O
( O
3 O
) O
From O
the O
ablation O
study O
, O
we O
see O
the O
BERT O
pretrained O
representations O
consistently O
improve O
the O
performance O
, O
and O
leveraging O
the O
metadata O
information O
further O
improves O
it O
. O
We O
also O
present O
the O
results O
of O
different O
methods O
when O
trained O
and O
tested O
on O
Recipe3http://webdatacommons.org/ O
structureddata/2018 O
- O
12 O
/ O
stats O
/ O
stats.html554MARCO O
Recipe O
- O
MARCO O
P@1 O
MRR O
P@1 O
MRR O
First O
Passage O
13.89 O
- O
15.13 O
Random O
13.76 O
34.76 O
11.35 O
30.67 O
S O
- O
Net O
( O
Tan O
et O
al O
. O
, O
2018 O
) O
28.30 O
- O
- O
NPS O
32.80 O
51.72 O
41.68 O
59.73 O
w/o O
BERT O
29.57 O
50.10 O
40.24 O
58.39 O
B O
- O
NPS O
33.52 O
52.83 O
43.58 O
61.37 O
F O
- O
NPS O
34.70‚àó54.21 O
44.37‚àó62.46 O
w/o O
BERT O
33.01 O
52.96 O
43.42 O
61.13 O
Table O
4 O
: O
Evaluation O
results O
on O
datasets O
. O
Statistic O
signiÔ¨Åcance O
is O
indicated O
with‚àó(p O
< O
0.05 O
) O
. O
Prop O
. O
( O
% O
) O
NPS O
F O
- O
NPS O
book O
6.37 O
29.06 O
32.81 O
medical O
13.20 O
30.69 O
34.46 O
person O
11.75 O
29.30 O
32.51 O
organization O
13.32 O
30.12 O
33.86 O
review O
3.09 O
27.42 O
35.48 O
Table O
5 O
: O
Analysis O
of O
P@1 O
performance O
for O
models O
w/ O
and O
w/o O
metadata O
information O
in O
diverse O
domains O
. O
MARCO O
. O
We O
see O
that O
the O
relative O
increase O
of O
performances O
for O
F O
- O
NPS O
is O
more O
substantial O
. O
Finally O
, O
we O
provide O
analysis O
on O
both O
the O
models O
and O
the O
effect O
of O
encoding O
metadata O
. O
Since O
not O
all O
web O
pages O
come O
with O
metadata O
, O
we O
turn O
our O
attention O
to O
the O
results O
describing O
the O
model O
performance O
on O
the O
portion O
of O
queries O
of O
MS O
MARCO O
that O
come O
with O
at O
least O
one O
metadata O
item O
( O
‚Äú O
MRich O
- O
MARCO O
‚Äù O
) O
. O
We O
Ô¨Årst O
perform O
analysis O
to O
understand O
how O
often O
the O
web O
pages O
in O
the O
dataset O
contain O
markup O
and O
how O
it O
affects O
the O
models O
performance O
. O
We O
see O
that O
for O
each O
query O
in O
MS O
MARCO O
, O
there O
are O
around O
7.9 O
metadata O
pairs O
for O
its O
candidate O
passages O
; O
and O
31.6 O
for O
queries O
in O
MRich O
- O
MARCO O
. O
On O
M O
- O
Rich O
- O
MARCO O
, O
the O
results O
we O
get O
on O
P@1 O
( O
F O
- O
NPS O
: O
33.13 O
, O
NPS O
28.79 O
) O
demonstrate O
that O
the O
performance O
gap O
between O
the O
model O
that O
leverages O
the O
metadata O
is O
larger O
than O
the O
general O
case O
. O
This O
, O
once O
again O
, O
demonstrates O
the O
effect O
of O
encoding O
metadata O
knowledge O
. O
To O
better O
understand O
how O
the O
models O
perform O
and O
the O
effect O
of O
metadata O
on O
speciÔ¨Åc O
web O
domains O
, O
we O
report O
in O
Table O
5 O
P@1 O
of O
models O
( O
trained O
on O
entireMS O
MARCO O
) O
on O
domains O
that O
are O
richer O
with O
metadata O
( O
i.e. O
, O
book O
, O
medical O
, O
person O
, O
organization O
and O
review O
) O
. O
We O
observe O
that O
queries O
in O
‚Äú O
medical O
‚Äù O
, O
‚Äù O
person O
‚Äù O
and O
‚Äù O
organization O
‚Äù O
domains O
have O
a O
larger O
presence O
in O
the O
dataset O
( O
> O
10 O
% O
) O
. O
The O
table O
also O
shows O
the O
performance O
of O
NPS O
and O
F O
- O
NPS O
on O
each O
domain O
. O
We O
see O
that O
F O
- O
NPS O
outperform O
NPS O
across O
all O
these O
domains O
. O
And O
the O
improvement O
ismore O
substantial O
as O
compared O
to O
evaluating O
on O
the O
entire O
test O
set O
( O
the O
second O
column O
of O
Table O
4 O
) O
. O
5 O
Conclusion O
We O
demonstrate O
beneÔ¨Åts O
of O
incorporating O
metadata O
information O
from O
web O
pages O
for O
improving O
answer O
passage O
selection O
model O
. O
We O
describe O
methods O
for O
obtaining O
metadata O
and O
decorating O
passages O
with O
metadata O
object O
- O
predicate O
pairs O
, O
and O
a O
Ô¨Ånegrained O
encoding O
strategy O
for O
leveraging O
metadata O
information O
in O
neural O
models O
. O
For O
future O
work O
, O
we O
‚Äôll O
investigate O
metadata O
for O
other O
tasks O
such O
as O
web O
entity O
linking O
and O
extraction O
. O
Acknowledgments O
We O
thank O
the O
anonymous O
reviewers O
for O
helpful O
feedback O
and O
comments O
. O
Abstract O
Intermediate O
- O
task O
training‚ÄîÔ¨Åne O
- O
tuning O
a O
pretrained O
model O
on O
an O
intermediate O
task O
before O
Ô¨Åne O
- O
tuning O
again O
on O
the O
target O
task O
‚Äî O
often O
improves O
model O
performance O
substantially O
on O
language O
understanding O
tasks O
in O
monolingual O
English O
settings O
. O
We O
investigate O
whether O
English O
intermediate O
- O
task O
training O
is O
still O
helpful O
onnon O
- O
English O
target O
tasks O
. O
Using O
nine O
intermediate O
language O
- O
understanding O
tasks O
, O
we O
evaluate O
intermediate O
- O
task O
transfer O
in O
a O
zeroshot O
cross O
- O
lingual O
setting O
on O
the O
XTREME O
benchmark O
. O
We O
see O
large O
improvements O
from O
intermediate O
training O
on O
the O
BUCC O
and O
Tatoeba O
sentence O
retrieval O
tasks O
and O
moderate O
improvements O
on O
question O
- O
answering O
target O
tasks O
. O
MNLI O
, O
SQuAD O
and O
HellaSwag O
achieve O
the O
best O
overall O
results O
as O
intermediate O
tasks O
, O
while O
multi O
- O
task O
intermediate O
offers O
small O
additional O
improvements O
. O
Using O
our O
best O
intermediate O
- O
task O
models O
for O
each O
target O
task O
, O
we O
obtain O
a O
5.4 O
point O
improvement O
over O
XLM O
- O
R O
Large O
on O
the O
XTREME O
benchmark O
, O
setting O
the O
state O
of O
the O
art1as O
of O
June O
2020 O
. O
We O
also O
investigate O
continuing O
multilingual O
MLM O
during O
intermediate O
- O
task O
training O
and O
using O
machine O
- O
translated O
intermediatetask O
data O
, O
but O
neither O
consistently O
outperforms O
simply O
performing O
English O
intermediate O
- O
task O
training O
. O
1 O
Introduction O
Zero O
- O
shot O
cross O
- O
lingual O
transfer O
involves O
training O
a O
model O
on O
task O
data O
in O
one O
set O
of O
languages O
( O
or O
language O
pairs O
, O
in O
the O
case O
of O
translation O
) O
and O
evaluating O
the O
model O
on O
the O
same O
task O
in O
unseen O
languages O
( O
or O
pairs O
) O
. O
In O
the O
context O
of O
natural O
language O
understanding O
tasks O
, O
this O
is O
generally O
done O
using O
a O
pretrained O
multilingual O
language O
- O
encoding O
model O
‚á§ O
‚á§ O
Equal O
contribution O
. O
1The O
state O
of O
art O
on O
XTREME O
at O
the O
time O
of O
Ô¨Ånal O
publication O
in O
September O
2020 O
is O
held O
by O
Fang O
et O
al O
. O
( O
2020 O
) O
, O
who O
introduce O
an O
orthogonal O
method.such O
as O
mBERT O
( O
Devlin O
et O
al O
. O
, O
2019a O
) O
, O
XLM O
( O
Conneau O
and O
Lample O
, O
2019 O
) O
or O
XLM O
- O
R O
( O
Conneau O
et O
al O
. O
, O
2020 O
) O
that O
has O
been O
pretrained O
with O
a O
masked O
language O
modeling O
( O
MLM O
) O
objective O
on O
large O
corpora O
of O
multilingual O
data O
, O
Ô¨Åne O
- O
tune O
it O
on O
task O
data O
in O
one O
language O
, O
and O
evaluate O
the O
tuned O
model O
on O
the O
same O
task O
in O
other O
languages O
. O
Intermediate O
- O
task O
training O
( O
STILTs O
; O
Phang O
et O
al O
. O
, O
2018 O
) O
consists O
of O
Ô¨Åne O
- O
tuning O
a O
pretrained O
model O
on O
a O
data O
- O
rich O
intermediate O
task O
, O
before O
Ô¨Åne O
- O
tuning O
a O
second O
time O
on O
the O
target O
task O
. O
Despite O
its O
simplicity O
, O
this O
two O
- O
phase O
training O
setup O
has O
been O
shown O
to O
be O
helpful O
across O
a O
range O
of O
Transformer O
models O
and O
target O
tasks O
( O
Wang O
et O
al O
. O
, O
2019a O
; O
Pruksachatkun O
et O
al O
. O
, O
2020 O
) O
, O
at O
least O
within O
English O
settings O
. O
In O
this O
work O
, O
we O
propose O
to O
use O
intermediate O
training O
on O
English O
tasks O
to O
improve O
zero O
- O
shot O
cross O
- O
lingual O
transfer O
performance O
. O
Starting O
with O
a O
pretrained O
multilingual O
language O
encoder O
, O
we O
perform O
intermediate O
- O
task O
training O
on O
one O
or O
more O
English O
tasks O
, O
then O
Ô¨Åne O
- O
tune O
on O
the O
target O
task O
in O
English O
, O
and O
Ô¨Ånally O
evaluate O
zero O
- O
shot O
on O
the O
same O
task O
in O
other O
languages O
. O
Intermediate O
- O
task O
training O
on O
English O
data O
introduces O
a O
potential O
issue O
: O
We O
train O
the O
pretrained O
multilingual O
model O
extensively O
on O
only O
English O
data O
before O
evaluating O
it O
on O
non O
- O
English O
target O
task O
data O
, O
potentially O
causing O
the O
model O
to O
lose O
the O
knowledge O
of O
the O
other O
languages O
that O
was O
acquired O
during O
pretraining O
( O
Kirkpatrick O
et O
al O
. O
, O
2017 O
; O
Yogatama O
et O
al O
. O
, O
2019 O
) O
. O
To O
mitigate O
this O
issue O
, O
we O
experiment O
with O
mixing O
in O
multilingual O
MLM O
training O
updates O
during O
the O
intermediate O
- O
task O
training O
. O
In O
the O
same O
vein O
, O
we O
also O
conduct O
a O
case O
study O
where O
we O
machine O
- O
translate O
intermediate O
task O
data O
from O
English O
into O
three O
other O
languages O
( O
German O
, O
Russian O
and O
Swahili O
) O
to O
investigate O
whether O
intermediate O
training O
on O
these O
languages O
improves O
target O
task O
performance O
in O
the O
same O
languages O
. O
Concretely O
, O
we O
use O
the O
pretrained O
XLM O
- O
R O
( O
Con-557Self O
- O
supervisedmultilingual O
pre O
- O
trainingIntermediate O
tasktrainingin O
EnglishTarget O
taskfine O
- O
tuningin O
EnglishXLM O
- O
RENÂÆø–ñÿ®MLM O
ÂÆø–ñÿ®Target O
taskevaluationin O
each O
language O
... O
#1#MTarget O
Task O
: O
# O
mENTarget O
Task O
: O
# O
mÂÆøSingle O
- O
task O
fine O
- O
tuning O
on O
task O
# O
n O
... O
# O
1#NInterm O
. O
Task O
: O
# O
nEN O
Interm O
. O
Task O
: O
# O
1 O
to O
# O
NInterm O
. O
Task O
: O
# O
n O
& O
MLMÂÆø–ñÿ®EN O
Multi O
- O
task O
fine O
- O
tuningOREnglishMultilingualMasked O
Language O
Modeling O
trainingTarget O
Task O
: O
# O
m–ñTarget O
Task O
: O
# O
mÿ®OR O
ENFigure O
1 O
: O
We O
investigate O
the O
beneÔ¨Åt O
of O
injecting O
an O
additional O
phase O
of O
intermediate O
- O
task O
training O
on O
English O
language O
task O
data O
. O
We O
also O
consider O
variants O
using O
multi O
- O
task O
intermediate O
- O
task O
training O
, O
as O
well O
as O
continuing O
multilingual O
MLM O
during O
intermediate O
- O
task O
training O
. O
Best O
viewed O
in O
color O
. O
neau O
et O
al O
. O
, O
2020 O
) O
encoder O
and O
perform O
experiments O
on O
9 O
target O
tasks O
from O
the O
recently O
introduced O
XTREME O
benchmark O
( O
Hu O
et O
al O
. O
, O
2020 O
) O
, O
which O
aims O
to O
evaluate O
zero O
- O
shot O
cross O
- O
lingual O
transfer O
performance O
across O
diverse O
target O
tasks O
across O
up O
to O
40 O
languages O
each O
. O
We O
investigate O
how O
training O
on O
9 O
different O
intermediate O
tasks O
, O
including O
question O
answering O
, O
sentence O
tagging O
, O
sentence O
completion O
, O
paraphrase O
detection O
, O
and O
natural O
language O
inference O
impacts O
zero O
- O
shot O
cross O
- O
lingual O
transfer O
performance O
. O
We O
Ô¨Ånd O
the O
following O
: O
‚Ä¢Intermediate O
- O
task O
training O
on O
SQuAD O
, O
MNLI O
, O
and O
HellaSwag O
yields O
large O
target O
- O
task O
improvements O
of O
8.2 O
, O
7.5 O
, O
and O
7.0 O
points O
on O
the O
development O
set O
, O
respectively O
. O
Multi O
- O
task O
intermediate O
- O
task O
training O
on O
all O
9 O
tasks O
performs O
best O
, O
improving O
by O
8.7 O
points O
. O
‚Ä¢Applying O
intermediate O
- O
task O
training O
to O
BUCC O
and O
Tatoeba O
, O
the O
two O
sentence O
retrieval O
target O
tasks O
that O
have O
no O
training O
data O
of O
their O
own O
, O
yields O
dramatic O
improvements O
with O
almost O
every O
intermediate O
training O
conÔ¨Åguration O
. O
TyDiQA O
shows O
consistent O
improvements O
with O
many O
intermediate O
tasks O
, O
whereas O
XNLI O
does O
not O
see O
beneÔ¨Åts O
from O
intermediate O
training O
. O
‚Ä¢Evaluating O
our O
best O
performing O
models O
for O
each O
target O
task O
on O
the O
XTREME O
benchmark O
yields O
an O
average O
improvement O
of O
5.4 O
points O
, O
setting O
the O
state O
of O
the O
art O
as O
of O
writing.‚Ä¢Training O
on O
English O
intermediate O
tasks O
outperforms O
the O
more O
complex O
alternatives O
of O
( O
i O
) O
continuing O
multilingual O
MLM O
during O
intermediate O
- O
task O
training O
, O
and O
( O
ii O
) O
using O
machine O
- O
translated O
intermediate O
- O
task O
data O
. O
2 O
Approach O
We O
follow O
a O
three O
- O
phase O
approach O
to O
training O
, O
illustrated O
in O
Figure O
1 O
: O
( O
i O
) O
we O
use O
a O
publicly O
available O
model O
pretrained O
on O
raw O
multilingual O
text O
using O
MLM O
; O
( O
ii O
) O
we O
perform O
intermediate O
- O
task O
training O
on O
one O
or O
more O
English O
intermediate O
tasks O
; O
and O
( O
iii O
) O
we O
Ô¨Åne O
- O
tune O
the O
model O
on O
English O
target O
- O
task O
training O
data O
, O
before O
evaluating O
it O
on O
target O
- O
task O
test O
data O
in O
each O
target O
language O
. O
In O
phase O
( O
ii O
) O
, O
our O
intermediate O
tasks O
have O
English O
input O
data O
. O
In O
Section O
2.4 O
, O
we O
investigate O
an O
alternative O
where O
we O
machine O
- O
translate O
intermediate O
- O
task O
data O
to O
other O
languages O
, O
which O
we O
use O
for O
training O
. O
We O
experiment O
with O
both O
single- O
and O
multi O
- O
task O
training O
for O
intermediate O
- O
task O
training O
. O
We O
use O
target O
tasks O
from O
the O
recent O
XTREME O
benchmark O
for O
zero O
- O
shot O
cross O
- O
lingual O
transfer O
. O
2.1 O
Intermediate O
Tasks O
We O
study O
the O
effect O
of O
intermediate O
- O
task O
training O
( O
STILTs O
; O
Phang O
et O
al O
. O
, O
2018 O
) O
with O
nine O
different O
English O
intermediate O
tasks O
, O
described O
in O
Table O
1 O
. O
We O
choose O
the O
tasks O
below O
based O
to O
cover O
a O
variety O
of O
task O
formats O
( O
classiÔ¨Åcation O
, O
question O
answering O
, O
and O
multiple O
choice O
) O
and O
based O
on O
evidence558Name O
|Train O
|| O
Dev|| O
Test| O
Task O
Genre O
/ O
Source O
Intermediate O
tasks O
ANLI+1,104,934 O
22,857 O
‚Äì O
natural O
language O
inference O
Misc O
. O
MNLI O
392,702 O
20,000 O
‚Äì O
natural O
language O
inference O
Misc O
. O
QQP O
363,846 O
40,430 O
‚Äì O
paraphrase O
detection O
Quora O
questions O
SQuAD O
v2.0 O
130,319 O
11,873 O
‚Äì O
span O
extraction O
Wikipedia O
SQuAD O
v1.1 O
87,599 O
10,570 O
‚Äì O
span O
extraction O
Wikipedia O
HellaSwag O
39,905 O
10,042 O
‚Äì O
sentence O
completion O
Video O
captions O
& O
Wikihow O
CCG O
38,015 O
5,484 O
‚Äì O
tagging O
Wall O
Street O
Journal O
Cosmos O
QA O
25,588 O
3,000 O
‚Äì O
question O
answering O
Blogs O
CommonsenseQA O
9,741 O
1,221 O
‚Äì O
question O
answering O
Crowdsourced O
responses O
Target O
tasks O
( O
XTREME O
Benchmark O
) O
XNLI O
392,702 O
2,490 O
5,010 O
natural O
language O
inference O
Misc O
. O
PAWS O
- O
X O
49,401 O
2,000 O
2,000 O
paraphrase O
detection O
Wiki O
/ O
Quora O
POS O
21,253 O
3,974 O
47‚Äì20,436 O
tagging O
Misc O
. O
NER O
20,000 O
10,000 O
1,000‚Äì10,000 O
named O
entity O
recognition O
Wikipedia O
XQuAD O
87,599 O
34,726 O
1,190 O
question O
answering O
Wikipedia O
MLQA O
87,599 O
34,726 O
4,517‚Äì11,590 O
question O
answering O
Wikipedia O
TyDiQA O
- O
GoldP O
3,696 O
634 O
323‚Äì2,719 O
question O
answering O
Wikipedia O
BUCC O
‚Äì O
‚Äì O
1,896‚Äì14,330 O
sentence O
retrieval O
Wiki O
/ O
news O
Tatoeba O
‚Äì O
‚Äì O
1,000 O
sentence O
retrieval O
Misc O
. O
Table O
1 O
: O
Overview O
of O
the O
intermediate O
tasks O
( O
top O
) O
and O
target O
tasks O
( O
bottom O
) O
in O
our O
experiments O
. O
For O
target O
tasks O
, O
Train O
andDevcorrespond O
to O
the O
English O
training O
and O
development O
sets O
, O
while O
Testshows O
the O
range O
of O
sizes O
for O
the O
target O
- O
language O
test O
sets O
for O
each O
task O
. O
XQuAD O
, O
TyDiQA O
and O
Tateoba O
do O
not O
have O
separate O
held O
- O
out O
development O
sets O
. O
of O
positive O
transfer O
from O
literature O
. O
Pruksachatkun O
et O
al O
. O
( O
2020 O
) O
shows O
that O
MNLI O
( O
of O
which O
ANLI+is O
a O
superset O
) O
, O
CommonsenseQA O
, O
Cosmos O
QA O
and O
HellaSwag O
yield O
positive O
transfer O
to O
a O
range O
of O
downstream O
English O
- O
language O
tasks O
in O
intermediate O
training O
. O
CCG O
involves O
token O
- O
wise O
prediction O
and O
is O
similar O
to O
the O
POS O
and O
NER O
target O
tasks O
. O
Both O
versions O
of O
SQuAD O
are O
widely O
- O
used O
questionanswering O
tasks O
, O
while O
QQP O
is O
semantically O
similar O
to O
sentence O
retrieval O
target O
tasks O
( O
BUCC O
and O
Tatoeba O
) O
as O
well O
as O
PAWS O
- O
X O
, O
another O
paraphrasedetection O
task O
. O
ANLI O
+ O
MNLI O
+ O
SNLI O
( O
ANLI+)The O
Adversarial O
Natural O
Language O
Inference O
dataset O
( O
Nie O
et O
al O
. O
, O
2020 O
) O
is O
collected O
using O
model O
- O
in O
- O
the O
- O
loop O
crowdsourcing O
as O
an O
extension O
of O
the O
Stanford O
Natural O
Language O
Inference O
( O
SNLI O
; O
Bowman O
et O
al O
. O
, O
2015 O
) O
and O
Multi O
- O
Genre O
Natural O
Language O
Inference O
( O
MNLI O
; O
Williams O
et O
al O
. O
, O
2018 O
) O
corpora O
. O
We O
follow O
Nie O
et O
al O
. O
( O
2020 O
) O
and O
use O
the O
concatenated O
ANLI O
, O
MNLI O
and O
SNLI O
training O
sets O
, O
which O
we O
refer O
to O
as O
ANLI+ O
. O
For O
all O
three O
natural O
language O
inference O
tasks O
, O
examples O
consist O
of O
premise O
and O
hypothesis O
sentence O
pairs O
, O
and O
the O
task O
is O
to O
classify O
the O
relationship O
between O
the O
premise O
and O
hypothesis O
as O
entailment O
, O
contradiction O
, O
or O
neutral O
. O
CCG O
CCGbank O
( O
Hockenmaier O
and O
Steedman O
, O
2007 O
) O
is O
a O
conversion O
of O
the O
Penn O
Treebank O
into O
Combinatory O
Categorial O
Grammar O
( O
CCG O
) O
derivations O
. O
The O
CCG O
supertagging O
task O
that O
we O
use O
consists O
of O
assigning O
lexical O
categories O
to O
individual O
word O
tokens O
, O
which O
together O
roughly O
determine O
a O
full O
parse.2 O
CommonsenseQA O
CommonsenseQA O
( O
Talmor O
et O
al O
. O
, O
2019 O
) O
is O
a O
multiple O
- O
choice O
QA O
dataset O
generated O
by O
crowdworkers O
based O
on O
clusters O
of O
concepts O
from O
ConceptNet O
( O
Speer O
et O
al O
. O
, O
2017 O
) O
. O
Cosmos O
QA O
Cosmos O
QA O
is O
multiple O
- O
choice O
commonsense O
- O
based O
reading O
comprehension O
dataset O
( O
Huang O
et O
al O
. O
, O
2019b O
) O
generated O
by O
crowdworkers O
, O
with O
a O
focus O
on O
the O
causes O
and O
effects O
of O
events O
. O
HellaSwag O
HellaSwag O
( O
Zellers O
et O
al O
. O
, O
2019 O
) O
is O
a O
commonsense O
reasoning O
dataset O
framed O
as O
a O
fourway O
multiple O
choice O
task O
, O
where O
examples O
consist O
of O
an O
incomplete O
paragraph O
and O
four O
choices O
of O
spans O
, O
only O
one O
of O
which O
is O
a O
plausible O
continuation O
of O
the O
scenario O
. O
It O
is O
built O
using O
adversarial O
Ô¨Åltering O
( O
Zellers O
et O
al O
. O
, O
2018 O
; O
Le O
Bras O
et O
al O
. O
, O
2020 O
) O
with O
BERT O
. O
2If O
a O
word O
is O
tokenized O
into O
sub O
- O
word O
tokens O
, O
we O
use O
the O
representation O
of O
the O
Ô¨Årst O
token O
for O
the O
tag O
prediction O
for O
that O
word O
as O
in O
Devlin O
et O
al O
. O
( O
2019a O
) O
.559MNLI O
In O
additional O
to O
the O
full O
ANLI+ O
, O
we O
also O
consider O
the O
MNLI O
task O
as O
a O
standalone O
intermediate O
task O
because O
of O
its O
already O
large O
and O
diverse O
training O
set O
. O
QQP O
Quora O
Question O
Pairs3is O
a O
paraphrase O
detection O
dataset O
. O
Examples O
in O
the O
dataset O
consist O
of O
two O
questions O
, O
labeled O
for O
whether O
they O
are O
semantically O
equivalent O
. O
SQuAD O
Stanford O
Question O
Answering O
Dataset O
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
, O
2018 O
) O
is O
a O
questionanswering O
dataset O
consisting O
of O
passages O
extracted O
from O
Wikipedia O
articles O
and O
crowd O
- O
sourced O
questions O
and O
answers O
. O
In O
SQuAD O
version O
1.1 O
, O
each O
example O
consists O
of O
a O
context O
passage O
and O
a O
question O
, O
and O
the O
answer O
is O
a O
text O
span O
from O
the O
context O
. O
SQuAD O
version O
2.0 O
includes O
additional O
questions O
with O
no O
answers O
, O
written O
adversarially O
by O
crowdworkers O
. O
We O
use O
both O
versions O
in O
our O
experiments O
. O
2.2 O
Target O
Tasks O
We O
use O
the O
9 O
target O
tasks O
from O
the O
XTREME O
benchmark O
, O
which O
span O
40 O
different O
languages O
( O
hereafter O
referred O
to O
as O
the O
target O
languages O
): O
Crosslingual O
Question O
Answering O
( O
XQuAD O
; O
Artetxe O
et O
al O
. O
, O
2020b O
) O
; O
Multilingual O
Question O
Answering O
( O
MLQA O
; O
Lewis O
et O
al O
. O
, O
2020 O
) O
; O
Typologically O
Diverse O
Question O
Answering O
( O
TyDiQA O
- O
GoldP O
; O
Clark O
et O
al O
. O
, O
2020 O
) O
; O
Cross O
- O
lingual O
Natural O
Language O
Inference O
( O
XNLI O
; O
Conneau O
et O
al O
. O
, O
2018 O
) O
; O
Crosslingual O
Paraphrase O
Adversaries O
from O
Word O
Scrambling O
( O
PAWS O
- O
X O
; O
Yang O
et O
al O
. O
, O
2019 O
) O
; O
Universal O
Dependencies O
v2.5 O
( O
Nivre O
et O
al O
. O
, O
2018 O
) O
POS O
tagging O
; O
Wikiann O
NER O
( O
Pan O
et O
al O
. O
, O
2017 O
) O
; O
BUCC O
( O
Zweigenbaum O
et O
al O
. O
, O
2017 O
, O
2018 O
) O
, O
which O
requires O
identifying O
parallel O
sentences O
from O
corpora O
of O
different O
languages O
; O
and O
Tatoeba O
( O
Artetxe O
and O
Schwenk O
, O
2019 O
) O
, O
which O
involves O
aligning O
pairs O
of O
sentences O
with O
the O
same O
meaning O
. O
Among O
the O
9 O
tasks O
, O
BUCC O
and O
Tatoeba O
are O
sentence O
retrieval O
tasks O
that O
do O
not O
include O
training O
sets O
, O
and O
are O
scored O
based O
on O
the O
similarity O
of O
learned O
representations O
( O
see O
Appendix O
A O
) O
. O
XQuAD O
, O
TyDiQA O
and O
Tatoeba O
do O
not O
include O
development O
sets O
separate O
from O
the O
test O
sets.4For O
all O
XTREME O
tasks O
, O
we O
follow O
the O
training O
and O
evaluation O
protocol O
described O
in O
the O
benchmark O
paper O
( O
Hu O
et O
al O
. O
, O
2020 O
) O
3http://data.quora.com/ O
First O
- O
Quora O
- O
DatasetRelease O
- O
Question O
- O
Pairs O
4UDPOS O
also O
does O
not O
include O
development O
sets O
for O
Kazakh O
, O
Thai O
, O
Tagalog O
or O
Yoruba.and O
their O
sample O
implementation.5Intermediateand O
target O
- O
task O
statistics O
are O
shown O
in O
Table O
1 O
. O
2.3 O
Multilingual O
Masked O
Language O
Modeling O
Our O
setup O
requires O
that O
we O
train O
the O
pretrained O
multilingual O
model O
extensively O
on O
English O
data O
before O
using O
it O
on O
a O
non O
- O
English O
target O
task O
, O
which O
can O
lead O
to O
the O
catastrophic O
forgetting O
of O
other O
languages O
acquired O
during O
pretraining O
. O
We O
investigate O
whether O
continuing O
to O
train O
on O
the O
multilingual O
MLM O
pretraining O
objective O
while O
Ô¨Åne O
- O
tuning O
on O
an O
English O
intermediate O
task O
can O
prevent O
catastrophic O
forgetting O
of O
the O
target O
languages O
and O
improve O
downstream O
transfer O
performance O
. O
We O
construct O
a O
multilingual O
corpus O
across O
the O
40 O
languages O
covered O
by O
the O
XTREME O
benchmark O
using O
Wikipedia O
dumps O
from O
April O
14 O
, O
2020 O
for O
each O
language O
and O
the O
MLM O
data O
creation O
scripts O
from O
thejiant O
1.3 O
library O
( O
Phang O
et O
al O
. O
, O
2020 O
) O
. O
In O
total O
, O
we O
use O
2 O
million O
sentences O
sampled O
across O
all O
40 O
languages O
using O
the O
sampling O
ratio O
from O
Conneau O
and O
Lample O
( O
2019 O
) O
with O
‚Üµ O
= O
0.3 O
. O
2.4 O
Translated O
Intermediate O
- O
Task O
Training O
Large O
- O
scale O
labeled O
datasets O
are O
rarely O
available O
in O
languages O
other O
than O
English O
for O
most O
languageunderstanding O
benchmark O
tasks O
. O
Given O
the O
availability O
of O
increasingly O
performant O
machine O
translation O
models O
, O
we O
investigate O
if O
using O
machinetranslated O
intermediate O
- O
task O
data O
can O
improve O
samelanguage O
transfer O
performance O
, O
compared O
to O
using O
English O
intermediate O
task O
data O
. O
We O
translate O
training O
and O
validation O
data O
of O
three O
intermediate O
tasks O
: O
QQP O
, O
HellaSwag O
, O
and O
MNLI O
. O
We O
choose O
these O
tasks O
based O
on O
the O
size O
of O
the O
training O
sets O
and O
because O
their O
examplelevel O
( O
rather O
than O
word O
- O
level O
) O
labels O
can O
be O
easily O
mapped O
onto O
translated O
data O
. O
To O
translate O
QQP O
and O
HellaSwag O
, O
we O
use O
pretrained O
machine O
translation O
models O
from O
OPUS O
- O
MT O
( O
Tiedemann O
and O
Thottingal O
, O
2020 O
) O
. O
These O
models O
are O
trained O
with O
Marian O
- O
NMT O
( O
Junczys O
- O
Dowmunt O
et O
al O
. O
, O
2018 O
) O
on O
OPUS O
data O
( O
Tiedemann O
, O
2012 O
) O
, O
which O
integrates O
several O
resources O
depending O
on O
the O
available O
corpora O
for O
the O
language O
pair O
. O
For O
MNLI O
, O
we O
use O
the O
publicly O
available O
machine O
- O
translated O
training O
data O
of O
XNLI O
provided O
by O
the O
XNLI O
authors.6We O
use O
German O
, O
Russian O
, O
and O
Swahili O
translations O
of O
5https://github.com/google-research/ O
xtreme O
6According O
to O
Conneau O
et O
al O
. O
( O
2018 O
) O
, O
these O
data O
are O
translated O
using O
a O
Facebook O
internal O
machine O
translation O
system.560all O
three O
datasets O
instead O
of O
English O
data O
for O
the O
intermediate O
- O
task O
training O
. O
3 O
Experiments O
and O
Results O
3.1 O
Models O
We O
use O
the O
pretrained O
XLM O
- O
R O
Large O
model O
( O
Conneau O
et O
al O
. O
, O
2020 O
) O
as O
a O
starting O
point O
for O
all O
our O
experiments O
, O
as O
it O
currently O
achieves O
state O
- O
of O
- O
theart O
performance O
on O
many O
zero O
- O
shot O
cross O
- O
lingual O
transfer O
tasks.7Details O
on O
intermediate- O
and O
targettask O
training O
can O
be O
found O
in O
Appendix O
A. O
XLM O
- O
R O
For O
our O
baseline O
, O
we O
directly O
Ô¨Åne O
- O
tune O
the O
pretrained O
XLM O
- O
R O
model O
on O
each O
target O
task O
‚Äôs O
English O
training O
data O
( O
if O
available O
) O
and O
evaluate O
zero O
- O
shot O
on O
non O
- O
English O
data O
, O
closely O
following O
the O
sample O
implementation O
for O
the O
XTREME O
benchmark O
. O
XLM O
- O
R O
+ O
Intermediate O
Task O
In O
our O
main O
approach O
, O
as O
described O
in O
Figure O
1 O
, O
we O
include O
an O
additional O
intermediate O
- O
task O
training O
phase O
before O
training O
and O
evaluating O
on O
the O
target O
tasks O
as O
described O
above O
. O
We O
also O
experiment O
with O
multi O
- O
task O
training O
on O
all O
available O
intermediate O
tasks O
. O
We O
follow O
Raffel O
et O
al O
. O
( O
2020 O
) O
and O
sample O
batches O
of O
examples O
for O
each O
task O
with O
probability O
rm O
= O
min(em O
, O
K)P(min(em O
, O
K O
) O
, O
where O
emis O
the O
number O
of O
examples O
in O
task O
mand O
the O
constant O
K=217limits O
the O
oversampling O
of O
data O
- O
rich O
tasks O
. O
XLM O
- O
R O
+ O
Intermediate O
Task O
+ O
MLM O
To O
incorporate O
multilingual O
MLM O
into O
the O
intermediatetask O
training O
, O
we O
treat O
multilingual O
MLM O
as O
an O
additional O
task O
for O
intermediate O
training O
, O
using O
the O
same O
multi O
- O
task O
sampling O
strategy O
as O
above O
. O
XLM O
- O
R O
+ O
Translated O
Intermediate O
Task O
We O
translate O
intermediate O
- O
task O
training O
and O
validation O
data O
for O
three O
tasks O
and O
Ô¨Åne O
- O
tune O
XLM O
- O
R O
on O
translated O
intermediate O
- O
task O
data O
before O
we O
train O
and O
evaluate O
on O
the O
target O
tasks O
. O
3.2 O
Software O
Experiments O
were O
carried O
out O
using O
the O
jiant O
( O
Phang O
et O
al O
. O
, O
2020 O
) O
library O
( O
2.0 O
alpha O
) O
, O
based O
on O
PyTorch O
( O
Paszke O
et O
al O
. O
, O
2019 O
) O
and O
Transformers O
( O
Wolf O
et O
al O
. O
, O
2019 O
) O
. O
7XLM O
- O
R O
Large O
( O
Conneau O
et O
al O
. O
, O
2020 O
) O
is O
a O
550m O
- O
parameter O
variant O
of O
the O
RoBERTa O
masked O
language O
model O
( O
Liu O
et O
al O
. O
, O
2019b O
) O
trained O
on O
a O
cleaned O
version O
of O
CommonCrawl O
on O
100 O
languages O
. O
Notably O
, O
Yoruba O
is O
used O
in O
the O
POS O
and O
NER O
XTREME O
tasks O
but O
not O
is O
not O
in O
the O
set O
of O
100 O
languages.3.3 O
Results O
We O
train O
three O
versions O
of O
each O
intermediate O
- O
task O
model O
with O
different O
random O
seeds O
. O
For O
each O
run O
, O
we O
compute O
the O
average O
target O
- O
task O
performance O
across O
languages O
, O
and O
report O
the O
median O
performance O
across O
the O
three O
random O
seeds O
. O
Intermediate O
- O
Task O
Training O
As O
shown O
in O
Table2 O
, O
no O
single O
intermediate O
task O
yields O
positive O
transfer O
across O
all O
target O
tasks O
. O
The O
target O
tasks O
TyDiQA O
, O
BUCC O
and O
Tatoeba O
see O
consistent O
gains O
from O
most O
or O
all O
intermediate O
tasks O
. O
In O
particular O
, O
BUCC O
and O
Tatoeba O
, O
the O
two O
sentence O
retrieval O
tasks O
with O
no O
training O
data O
, O
beneÔ¨Åt O
universally O
from O
intermediate O
- O
task O
training O
. O
PAWS O
- O
X O
, O
NER O
, O
XQuAD O
and O
MLQA O
also O
exhibit O
gains O
with O
the O
additional O
intermediate O
- O
task O
training O
on O
some O
intermediate O
tasks O
. O
On O
the O
other O
hand O
, O
we O
Ô¨Ånd O
generally O
no O
or O
negative O
transfer O
to O
XNLI O
and O
POS O
. O
Among O
the O
intermediate O
tasks O
, O
we O
Ô¨Ånd O
that O
MNLI O
performs O
best O
; O
with O
meaningful O
improvements O
across O
the O
PAWS O
- O
X O
, O
TyDiQA O
, O
BUCC O
and O
Tatoeba O
tasks O
. O
ANLI+ O
, O
SQuAD O
v1.1 O
, O
SQuAD O
v2.0 O
and O
HellaSwag O
also O
show O
strong O
positive O
transfer O
performance O
: O
SQuAD O
v1.1 O
shows O
strong O
positive O
transfer O
across O
all O
three O
QA O
tasks O
, O
SQuAD O
v2.0 O
shows O
the O
most O
positive O
transfer O
to O
TyDiQA O
, O
while O
HellaSwag O
shows O
the O
most O
positive O
transfer O
to O
NER O
and O
BUCC O
tasks O
. O
ANLI+does O
not O
show O
any O
improvement O
over O
MNLI O
( O
of O
which O
it O
is O
a O
superset O
) O
, O
even O
on O
XNLI O
for O
which O
it O
offers O
additional O
directly O
relevant O
training O
data O
. O
This O
mirrors O
negative O
Ô¨Åndings O
from O
Nie O
et O
al O
. O
( O
2020 O
) O
on O
NLI O
evaluations O
and O
Bowman O
et O
al O
. O
( O
2020 O
) O
on O
transfer O
within O
English O
. O
QQP O
signiÔ¨Åcantly O
improves O
sentence O
retrieval O
- O
task O
performance O
, O
but O
has O
broadly O
negative O
transfer O
to O
the O
other O
target O
tasks.8CCG O
also O
has O
relatively O
poor O
transfer O
performance O
, O
consistent O
with O
Pruksachatkun O
et O
al O
. O
( O
2020 O
) O
. O
Among O
our O
intermediate O
tasks O
, O
both O
SQuAD O
v1.1 O
and O
MNLI O
also O
serve O
as O
training O
sets O
for O
target O
tasks O
( O
for O
XNLI O
and O
XQuAD O
/ O
MLQA O
respectively O
) O
. O
While O
both O
tasks O
show O
overall O
positive O
transfer O
, O
SQuAD O
v1.1 O
actually O
markedly O
improves O
the O
performance O
in O
XQuAD O
and O
MLQA O
, O
while O
MNLI O
slightly O
hurts O
XNLI O
performance O
. O
We O
hypothesize O
that O
the O
somewhat O
surprising O
improvements O
to O
XQuAD O
and O
MLQA O
performance O
from O
SQuAD O
v1.1 O
arise O
due O
to O
the O
baseline O
XQuAD O
and O
MLQA O
8For O
QQP O
, O
on O
2 O
of O
the O
3 O
random O
seeds O
the O
NER O
model O
performed O
extremely O
poorly O
, O
leading O
to O
the O
large O
negative O
transfer O
of O
-45.4.561Target O
tasksXNLI O
PAWS O
- O
X O
POS O
NER O
XQuAD O
MLQA O
TyDiQA O
BUCC O
Tatoeba O
Avg O
. O
Metricacc O
. O
acc O
. O
F1 O
F1 O
F1 O
/ O
EM O
F1 O
/ O
EM O
F1 O
/ O
EM O
F1 O
acc O
. O
‚Äì O
# O
langs.15 O
7 O
33 O
40 O
11 O
7 O
9 O
5 O
37 O
‚Äì O
XLM O
- O
R O
80.186.5 O
75.7 O
62.8 O
76.1 O
/ O
60.0 O
70.1 O
/ O
51.5 O
65.6 O
/ O
48.2 O
71.5 O
31.0 O
67.2Without O
MLMANLI+- O
0.8- O
0.0- O
1.4- O
3.5- O
1.1/- O
0.5- O
0.6/- O
0.8- O
0.6/- O
3.0 O
+ O
19.9 O
+ O
48.2 O
+ O
6.6MNLI- O
1.2 O
+ O
1.4- O
0.7 O
+ O
0.5- O
0.3/- O
0.1 O
+ O
0.2/+ O
0.2- O
1.0/- O
1.6 O
+ O
20.0 O
+ O
48.8 O
+ O
7.5QQP- O
4.4- O
4.8- O
6.5 O
- O
45.4- O
3.8/- O
3.8- O
3.9/- O
4.4 O
- O
11.1/-10.2 O
+ O
17.1 O
+ O
49.5- O
1.5SQuADv1.1- O
1.9 O
+ O
1.2- O
0.8- O
0.4 O
+ O
1.8/+ O
2.5 O
+ O
2.2/+ O
2.6 O
+ O
9.7/+10.8 O
+ O
18.9 O
+ O
41.3 O
+ O
8.1SQuADv2- O
1.6 O
+ O
1.9- O
1.1 O
+ O
0.8- O
0.5/+ O
0.7- O
0.4/+ O
0.1 O
+ O
10.4/+11.3 O
+ O
19.3 O
+ O
43.4 O
+ O
8.2HellaSwag- O
7.1 O
+ O
1.8- O
0.7 O
+ O
1.6- O
0.0/+ O
0.5- O
0.1/+ O
0.2- O
0.0/- O
1.0 O
+ O
20.3 O
+ O
47.6 O
+ O
7.0CCG- O
2.6- O
3.4- O
2.0- O
1.5- O
1.5/- O
1.3- O
1.6/- O
1.5- O
2.8/- O
6.2 O
+ O
11.7 O
+ O
41.9 O
+ O
4.1CosmosQA- O
2.1- O
0.3- O
1.4- O
1.5- O
0.9/- O
1.3- O
1.5/- O
2.0 O
+ O
0.5/- O
0.6 O
+ O
19.2 O
+ O
43.9 O
+ O
6.1CSQA- O
2.9- O
2.8- O
1.7- O
1.6- O
1.0/- O
1.8- O
1.0/- O
0.6 O
+ O
3.5/+ O
2.9 O
+ O
18.1 O
+ O
48.6 O
+ O
6.5Multi O
- O
task- O
0.9 O
+ O
1.7- O
1.0 O
+ O
1.8 O
+ O
0.3/+ O
0.9 O
+ O
0.2/+ O
0.5 O
+ O
5.8/+ O
6.0 O
+ O
19.6 O
+ O
49.9 O
+ O
8.7With O
MLMANLI+- O
1.1 O
+ O
1.4 O
+ O
0.0 O
+ O
0.4- O
1.9/- O
1.7- O
0.7/- O
0.6 O
+ O
0.9/+ O
0.5 O
+ O
18.6 O
+ O
46.2 O
+ O
7.1MNLI- O
0.7 O
+ O
1.6- O
1.6 O
+ O
1.0- O
0.7/+ O
0.1 O
+ O
0.4/+ O
0.8- O
1.8/- O
3.2 O
+ O
17.1 O
+ O
44.3 O
+ O
6.6QQP- O
1.3- O
1.1- O
2.4- O
0.9- O
0.3/- O
0.2 O
+ O
0.0/+ O
0.2- O
1.6/- O
4.2 O
+ O
14.4 O
+ O
39.8 O
+ O
5.0SQuADv1.1- O
2.6 O
+ O
0.3- O
2.0- O
0.9 O
+ O
0.2/+ O
1.6 O
+ O
0.1/+ O
1.1 O
+ O
8.5/+ O
9.5 O
+ O
16.0 O
+ O
40.3 O
+ O
6.8SQuADv2- O
1.7 O
+ O
2.1- O
1.4 O
+ O
1.0- O
0.8/+ O
0.1- O
0.8/- O
0.5 O
+ O
8.3/+ O
8.9 O
+ O
15.6 O
+ O
31.3 O
+ O
6.1HellaSwag- O
3.3 O
+ O
2.0- O
0.7 O
+ O
0.8- O
0.8/- O
0.0 O
+ O
0.1/+ O
0.6 O
+ O
0.3/+ O
1.0 O
+ O
6.3 O
+ O
22.3 O
+ O
3.1CCG- O
1.0- O
1.3- O
1.2- O
1.9- O
1.9/- O
2.2- O
2.1/- O
2.6- O
5.5/- O
6.2 O
+ O
8.8 O
+ O
36.1 O
+ O
3.3CosmosQA- O
1.0- O
1.0- O
1.6- O
3.8- O
3.1/- O
3.3- O
3.7/- O
4.2- O
0.6/- O
3.2 O
+ O
15.5 O
+ O
42.7 O
+ O
4.7CSQA- O
0.5 O
+ O
0.3- O
1.0- O
0.7- O
0.9/- O
1.0- O
0.7/- O
0.6 O
+ O
2.1/+ O
0.4 O
+ O
11.6 O
+ O
17.2 O
+ O
2.9XTREME O
Benchmark O
Scores‚Ä†XLM O
- O
R O
( O
Hu O
et O
al O
. O
,2020)79.2 O
86.4 O
72.665.476.6 O
/ O
60.8 O
71.6 O
/ O
53.2 O
65.1 O
/ O
45.0 O
66.0 O
57.3 O
68.1XLM O
- O
R O
( O
Ours)79.5 O
86.2 O
74.0 O
62.6 O
76.1 O
/ O
60.0 O
70.2 O
/ O
51.2 O
65.6 O
/ O
48.2 O
64.5 O
31.0 O
64.8Our O
Best O
Models‚Ä°80.0 O
87.9 O
74.464.078.7/63.3 O
72.4/53.7 O
76.0/59.5 O
71.9 O
81.2 O
73.5Human O
( O
Hu O
et O
al O
. O
,2020)92.8 O
97.5 O
97.0 O
- O
91.2 O
/ O
82.3 O
91.2 O
/ O
82.3 O
90.1 O
/ O
- O
- O
- O
-Table O
2 O
: O
Intermediate O
- O
task O
training O
results O
. O
We O
compute O
the O
average O
target O
task O
performance O
across O
all O
languages O
, O
and O
report O
the O
median O
over O
3 O
separate O
runs O
with O
different O
random O
seeds O
. O
Multi O
- O
task O
experiments O
use O
all O
intermediate O
tasks O
. O
We O
underline O
the O
best O
results O
per O
target O
task O
with O
and O
without O
intermediate O
MLM O
co O
- O
training O
, O
and O
bold O
- O
face O
the O
best O
overall O
scores O
for O
each O
target O
task.‚Ä† O
: O
XQuAD O
, O
TyDiQA O
and O
Tatoeba O
do O
not O
have O
held O
- O
out O
test O
data O
and O
are O
scored O
using O
development O
sets O
in O
the O
benchmark.‚Ä° O
: O
Results O
obtained O
with O
our O
best O
- O
performing O
intermediate O
task O
conÔ¨Åguration O
for O
each O
target O
task O
, O
selected O
based O
on O
the O
development O
set O
. O
The O
results O
for O
individual O
languages O
can O
be O
found O
in O
Appendix O
B. O
models O
being O
under O
- O
trained O
. O
For O
all O
target O
- O
task O
Ô¨Ånetuning O
, O
we O
follow O
the O
sample O
implementation O
for O
target O
task O
training O
in O
the O
XTREME O
benchmark O
, O
which O
trains O
on O
SQuAD O
for O
only O
2 O
epochs O
. O
This O
may O
explain O
why O
an O
additional O
phase O
of O
SQuAD O
training O
can O
improve O
performance O
. O
Conversely O
, O
the O
MNLI O
- O
to O
- O
XNLI O
model O
might O
be O
over O
- O
trained O
, O
given O
the O
MNLI O
training O
set O
is O
approximately O
4 O
times O
as O
large O
as O
the O
SQuAD O
v1.1 O
training O
set O
. O
Multi O
- O
Task O
Training O
Multi O
- O
task O
training O
on O
all O
intermediate O
tasks O
attains O
the O
best O
overall O
average O
performance O
on O
the O
XTREME O
tasks O
, O
and O
has O
the O
most O
positive O
transfer O
to O
NER O
and O
Tatoeba O
tasks O
. O
However O
, O
the O
overall O
margin O
of O
improvement O
over O
the O
best O
single O
intermediate O
- O
task O
model O
is O
relatively O
small O
( O
only O
0.3 O
, O
over O
MNLI O
) O
, O
while O
requiring O
signiÔ¨Åcantly O
more O
training O
resources O
. O
Many O
single O
intermediate O
- O
task O
models O
also O
outperform O
the O
multitask O
model O
in O
individual O
target O
tasks O
. O
Wang O
et O
al O
. O
( O
2019b O
) O
also O
found O
more O
mixed O
results O
from O
a O
having O
an O
initial O
phase O
of O
multi O
- O
task O
training O
, O
albeitonly O
among O
English O
language O
tasks O
across O
a O
different O
set O
of O
tasks O
. O
On O
the O
other O
hand O
, O
multi O
- O
task O
training O
precludes O
the O
need O
to O
do O
intermediate O
- O
task O
model O
selection O
, O
and O
is O
a O
useful O
method O
for O
incorporating O
multiple O
, O
diverse O
intermediate O
tasks O
. O
MLM O
Incorporating O
MLM O
during O
intermediatetask O
training O
shows O
no O
clear O
trend O
. O
It O
reduces O
negative O
transfer O
, O
as O
seen O
in O
the O
cases O
of O
CommonsenseQA O
and O
QQP O
, O
but O
it O
also O
tends O
to O
somewhat O
reduce O
positive O
transfer O
. O
The O
reductions O
in O
positive O
transfer O
are O
particularly O
signiÔ¨Åcant O
for O
the O
BUCC O
and O
Tatoeba O
tasks O
, O
although O
the O
impact O
on O
TyDiQA O
is O
more O
mixed O
. O
On O
balance O
, O
we O
do O
not O
see O
that O
incorporating O
MLM O
improves O
transfer O
performance O
. O
XTREME O
Benchmark O
Results O
At O
the O
bottom O
of O
Table O
2 O
, O
we O
show O
results O
obtained O
by O
XLM O
- O
R O
on O
the O
XTREME O
benchmark O
as O
reported O
by O
Hu O
et O
al O
. O
( O
2020 O
) O
, O
results O
obtained O
with O
our O
reimplementation O
of O
XLM O
- O
R O
( O
i.e. O
our O
baseline O
) O
, O
and O
results O
obtained O
with O
our O
best O
models O
, O
which O
use O
intermediate O
- O
task O
conÔ¨Åguration O
selected O
according562TL O
Model O
XNLI O
PAWS O
- O
X O
POS O
NER O
XQuAD O
MLQA O
TyDiQA O
BUCC O
TatoebaEnglishXLM O
- O
R O
89.393.4 O
95.9 O
81.686.3/ O
74.2 O
81.6 O
/ O
68.6 O
70.4 O
/ O
56.6 O
‚Äì O
‚Äì O
MNLIen- O
1.2 O
+ O
1.6 O
+ O
0.3 O
+ O
2.6- O
2.1/- O
1.6 O
+ O
1.1/+ O
1.4 O
+ O
1.1/+ O
1.1‚Äì‚ÄìQQPen- O
3.2- O
0.4- O
2.2- O
5.8- O
4.0/- O
3.6- O
2.6/- O
2.6- O
6.2/- O
5.0‚Äì‚ÄìHellaSwagen- O
0.8 O
+ O
1.5 O
+ O
0.6 O
+ O
2.7- O
0.2/+ O
1.4 O
+ O
1.8/+ O
2.3 O
+ O
1.7/+ O
2.5‚Äì‚ÄìGermanXLM O
- O
R O
83.888.1 O
88.6 O
78.6 O
77.7 O
/ O
61.269.1/ O
52.0 O
‚Äì O
77.7 O
63.9MNLIen- O
0.8 O
+ O
0.9- O
0.1- O
0.8- O
0.3/- O
1.0- O
1.0/- O
0.2‚Äì+16.5 O
+ O
32.7MNLIde- O
0.4 O
+ O
0.5- O
0.3- O
0.9 O
+ O
0.2/- O
0.3- O
2.4/- O
2.0‚Äì+17.0 O
+ O
33.7QQPen- O
2.2- O
4.2- O
3.2- O
7.3- O
4.5/- O
4.7- O
6.7/- O
6.4‚Äì+16.5 O
+ O
32.6QQPde- O
2.6- O
9.1- O
3.2 O
- O
22.9- O
6.6/- O
5.9- O
7.7/- O
6.6‚Äì+16.0 O
+ O
33.5HellaSwagen- O
0.3 O
+ O
0.3 O
+ O
0.1 O
+ O
0.5 O
+ O
1.0/+ O
0.2- O
0.3/+ O
0.4‚Äì+16.9 O
+ O
33.8HellaSwagde- O
0.2 O
+ O
0.2- O
0.4- O
0.4 O
+ O
0.2/- O
0.2- O
3.5/- O
2.5‚Äì+16.3 O
+ O
33.5RussianXLM O
- O
R79.2 O
‚Äì O
89.569.3 O
77.7 O
/ O
59.8 O
‚Äì O
65.4 O
/ O
43.6 O
79.2 O
42.1MNLIen+ O
0.3‚Äì- O
0.0 O
+ O
0.8 O
+ O
0.1/+ O
1.5‚Äì- O
1.5/- O
4.6 O
+ O
14.3 O
+ O
47.1MNLIru- O
0.6‚Äì- O
0.3 O
+ O
1.9- O
0.4/+ O
1.3‚Äì+11.2/+16.1 O
+ O
13.1 O
+ O
48.3QQPen- O
0.7‚Äì- O
2.9 O
- O
18.6- O
3.5/- O
2.4‚Äì- O
8.1/- O
5.4 O
+ O
14.1 O
+ O
49.5QQPru- O
3.0‚Äì-10.6 O
- O
59.1- O
5.2/- O
3.9‚Äì-14.4/-12.1 O
+ O
13.3 O
+ O
46.7HellaSwagen- O
0.9‚Äì- O
0.0 O
+ O
1.4 O
+ O
0.8/+ O
2.9‚Äì- O
4.0/-10.6 O
+ O
14.7 O
+ O
49.9HellaSwagru- O
0.3‚Äì- O
0.4 O
+ O
2.8 O
+ O
0.2/+ O
0.2‚Äì+ O
8.5/+13.2 O
- O
71.6 O
- O
23.5SwahiliXLM O
- O
R O
72.4 O
‚Äì O
‚Äì O
69.8 O
‚Äì O
‚Äì O
67.2 O
/ O
48.7 O
‚Äì O
7.9MNLIen- O
3.0‚Äì‚Äì+ O
0.6‚Äì‚Äì- O
0.3/- O
0.2‚Äì+24.9MNLIsw- O
1.1‚Äì‚Äì- O
2.4‚Äì‚Äì+13.8/+23.4‚Äì+47.9QQPen- O
2.8‚Äì‚Äì- O
4.6‚Äì‚Äì-12.7/-12.2‚Äì+27.2QQPsw- O
7.1‚Äì‚Äì-32.1‚Äì‚Äì- O
7.0/- O
0.4‚Äì+41.8HellaSwagen- O
0.4‚Äì‚Äì+ O
0.1‚Äì‚Äì- O
0.9/- O
0.4‚Äì+27.2HellaSwagsw- O
9.8‚Äì‚Äì+ O
0.4‚Äì‚Äì+15.6/+26.3‚Äì- O
0.5Table O
3 O
: O
Experiments O
with O
translated O
intermediate O
- O
task O
training O
and O
validation O
data O
evaluated O
on O
all O
XTREME O
target O
tasks O
. O
In O
each O
target O
language O
( O
TL O
) O
block O
, O
models O
are O
evaluated O
on O
a O
single O
target O
language O
. O
We O
show O
results O
for O
models O
trained O
on O
original O
intermediate O
- O
task O
training O
data O
( O
en O
) O
and O
compare O
it O
to O
models O
trained O
on O
translated O
data{de O
, O
ru O
, O
sw O
} O
. O
‚Äò O
‚Äì O
‚Äô O
indicates O
that O
target O
task O
data O
is O
not O
available O
for O
that O
target O
language O
. O
to O
development O
set O
performance O
on O
each O
target O
task O
. O
Based O
on O
the O
results O
in O
Table O
2 O
, O
which O
reÔ¨Çect O
the O
median O
over O
3 O
runs O
, O
we O
pick O
the O
best O
intermediatetask O
conÔ¨Åguration O
for O
each O
target O
task O
, O
and O
then O
choose O
the O
best O
model O
out O
of O
the O
3 O
runs O
. O
Scores O
on O
the O
XTREME O
benchmark O
are O
computed O
based O
on O
the O
respective O
test O
sets O
where O
available O
, O
and O
based O
on O
development O
sets O
for O
target O
tasks O
without O
separate O
held O
- O
out O
test O
sets O
. O
We O
are O
generally O
able O
to O
replicate O
the O
best O
reported O
XLM O
- O
R O
baseline O
results O
, O
except O
for O
Tatoeba O
, O
where O
our O
implementation O
signiÔ¨Åcantly O
underperforms O
the O
reported O
scores O
in O
Hu O
et O
al O
. O
( O
2020 O
) O
, O
and O
TyDiQA O
, O
where O
our O
implementation O
outperforms O
the O
reported O
scores O
. O
We O
also O
highlight O
that O
there O
is O
a O
large O
margin O
of O
difference O
between O
development O
and O
test O
set O
scores O
for O
BUCC O
‚Äì O
this O
is O
likely O
because O
BUCC O
is O
evaluated O
based O
on O
sentence O
retrieval O
over O
the O
given O
set O
of O
input O
sentences O
, O
and O
the O
test O
sets O
for O
BUCC O
are O
generally O
much O
larger O
than O
the O
development O
sets O
. O
Our O
best O
models O
show O
gains O
in O
8 O
out O
of O
the O
9 O
XTREME O
tasks O
relative O
to O
both O
baseline O
implementations O
, O
attaining O
an O
average O
score O
of O
73.5 O
across O
target O
tasks O
, O
a O
5.4 O
point O
improvement O
over O
the O
pre O
- O
vious O
best O
reported O
average O
score O
of O
68.1 O
. O
We O
set O
the O
state O
of O
the O
art O
on O
the O
XTREME O
benchmark O
as O
of O
June O
2020 O
, O
though O
Fang O
et O
al O
. O
( O
2020 O
) O
achieve O
higher O
results O
and O
hold O
the O
state O
of O
the O
art O
using O
an O
orthogonal O
approach O
at O
the O
time O
of O
our O
Ô¨Ånal O
publication O
in O
September O
2020 O
. O
Translated O
Intermediate O
- O
Task O
Training O
Data O
In O
Table O
3 O
, O
we O
show O
results O
for O
experiments O
using O
machine O
- O
translated O
intermediate O
- O
training O
data O
, O
and O
evaluated O
on O
the O
available O
target O
- O
task O
languages O
. O
Surprisingly O
, O
even O
when O
evaluating O
inlanguage O
, O
using O
target O
- O
language O
intermediate O
- O
task O
data O
does O
not O
consistently O
outperform O
using O
English O
intermediate O
- O
task O
data O
in O
any O
of O
the O
intermediate O
tasks O
on O
average O
. O
In O
general O
, O
cross O
- O
lingual O
transfer O
to O
XNLI O
is O
negative O
regardless O
of O
the O
intermediate O
- O
task O
or O
the O
target O
language O
. O
In O
contrast O
, O
we O
observe O
mostly O
positive O
transfer O
on O
BUCC O
, O
and O
Tatoeba O
, O
with O
a O
few O
notable O
exceptions O
where O
models O
fail O
catastrophically O
. O
TyDiQA O
exhibits O
positive O
transfer O
where O
the O
intermediate- O
and O
target O
- O
task O
languages O
aligned O
: O
intermediate O
training O
on O
Russian O
or O
German O
helps O
TyDiQA O
performance O
in O
that O
respective O
language,563whereas O
intermediate O
training O
on O
English O
hurts O
nonEnglish O
performance O
somewhat O
. O
For O
the O
remaining O
tasks O
, O
there O
appears O
to O
be O
little O
correlation O
between O
performance O
and O
the O
alignment O
of O
intermediateand O
target O
- O
task O
languages O
. O
English O
language O
QQP O
already O
has O
mostly O
negative O
transfer O
to O
all O
target O
tasks O
except O
for O
BUCC O
and O
Tatoeba O
( O
see O
Table O
2 O
) O
, O
and O
also O
shows O
a O
similar O
trend O
when O
translated O
into O
any O
of O
the O
three O
target O
languages O
. O
We O
note O
that O
the O
quality O
of O
translations O
may O
affect O
the O
transfer O
performance O
. O
While O
validation O
performance O
on O
the O
translated O
intermediate O
tasks O
( O
Table O
15 O
) O
for O
MNLI O
and O
QQP O
is O
only O
slightly O
worse O
than O
the O
original O
English O
versions O
, O
the O
performance O
for O
the O
Russian O
and O
Swahili O
HellaSwag O
is O
much O
worse O
and O
close O
to O
chance O
. O
Despite O
this O
, O
intermediate O
- O
task O
training O
on O
Russian O
and O
Swahili O
HellaSwag O
improve O
performance O
on O
PAN O
- O
X O
and O
TyDiQA O
, O
while O
we O
see O
generally O
poor O
transfer O
performance O
from O
QQP O
. O
The O
interaction O
between O
translated O
intermediate O
- O
task O
data O
and O
transfer O
performance O
continues O
to O
be O
a O
complex O
open O
question O
. O
Artetxe O
et O
al O
. O
( O
2020a O
) O
found O
that O
translating O
or O
back O
- O
translating O
training O
data O
for O
a O
task O
can O
improve O
zero O
- O
shot O
cross O
- O
lingual O
performance O
for O
tasks O
such O
as O
XNLI O
depending O
on O
how O
the O
multilingual O
datasets O
are O
created O
. O
In O
contrast O
, O
we O
train O
on O
translated O
intermediate O
- O
task O
data O
and O
then O
Ô¨Åne O
- O
tune O
on O
a O
target O
task O
with O
English O
training O
data O
( O
excluding O
BUCC2018 O
and O
Tatoeba O
) O
. O
The O
authors O
of O
the O
XTREME O
benchmark O
have O
also O
recently O
released O
translated O
versions O
of O
all O
the O
XTREME O
task O
training O
data O
, O
which O
we O
hope O
will O
prompt O
further O
investigation O
into O
this O
matter O
. O
4 O
Related O
work O
Sequential O
transfer O
learning O
using O
pretrained O
Transformer O
- O
based O
encoders O
( O
Phang O
et O
al O
. O
, O
2018 O
) O
has O
been O
shown O
to O
be O
effective O
for O
many O
text O
classiÔ¨Åcation O
tasks O
. O
This O
setup O
generally O
involves O
Ô¨Ånetuning O
on O
a O
single O
task O
( O
Pruksachatkun O
et O
al O
. O
, O
2020 O
; O
Vu O
et O
al O
. O
, O
2020 O
) O
or O
multiple O
tasks O
( O
Liu O
et O
al O
. O
, O
2019a O
; O
Wang O
et O
al O
. O
, O
2019b O
; O
Raffel O
et O
al O
. O
, O
2020 O
) O
, O
sometimes O
referred O
to O
as O
the O
intermediate O
task(s O
) O
, O
before O
Ô¨Ånetuning O
on O
the O
target O
task O
. O
We O
build O
upon O
this O
line O
of O
work O
, O
focusing O
on O
intermediate O
- O
task O
training O
for O
improving O
cross O
- O
lingual O
transfer O
. O
Early O
work O
on O
cross O
- O
lingual O
transfer O
mostly O
relies O
on O
the O
availability O
of O
parallel O
data O
, O
where O
one O
can O
perform O
translation O
( O
Mayhew O
et O
al O
. O
, O
2017 O
) O
or O
project O
annotations O
from O
one O
language O
into O
another(Hwa O
et O
al O
. O
, O
2005 O
; O
Agi¬¥c O
et O
al O
. O
, O
2016 O
) O
. O
For O
dependency O
parsing O
, O
McDonald O
et O
al O
. O
( O
2011 O
) O
use O
delexicalized O
parsers O
trained O
on O
source O
languages O
and O
labeled O
training O
data O
for O
parsing O
target O
- O
language O
data O
. O
Agi¬¥c(2017 O
) O
proposes O
a O
parser O
selection O
method O
to O
select O
the O
single O
best O
parser O
for O
a O
target O
language O
. O
For O
large O
- O
scale O
cross O
- O
lingual O
transfer O
outside O
NLU O
, O
Johnson O
et O
al O
. O
( O
2017 O
) O
train O
a O
single O
multilingual O
neural O
machine O
translation O
system O
with O
up O
to O
7 O
languages O
and O
perform O
zero O
- O
shot O
translation O
without O
explicit O
bridging O
between O
the O
source O
and O
target O
languages O
. O
Aharoni O
et O
al O
. O
( O
2019 O
) O
expand O
this O
approach O
to O
cover O
over O
100 O
languages O
in O
a O
single O
model O
. O
Recent O
works O
on O
extending O
pretrained O
Transformer O
- O
based O
encoders O
to O
multilingual O
settings O
show O
that O
these O
models O
are O
effective O
for O
cross O
- O
lingual O
tasks O
and O
competitive O
with O
strong O
monolingual O
models O
on O
the O
XNLI O
benchmark O
( O
Devlin O
et O
al O
. O
, O
2019b O
; O
Conneau O
and O
Lample O
, O
2019 O
; O
Conneau O
et O
al O
. O
, O
2020 O
; O
Huang O
et O
al O
. O
, O
2019a O
) O
. O
More O
recently O
, O
Artetxe O
et O
al O
. O
( O
2020a O
) O
showed O
that O
cross O
- O
lingual O
transfer O
performance O
can O
be O
sensitive O
to O
translation O
artifacts O
arising O
from O
a O
multilingual O
datasets O
‚Äô O
creation O
procedure O
. O
Finally O
, O
Pfeiffer O
et O
al O
. O
( O
2020 O
) O
propose O
adapter O
modules O
that O
learn O
language O
and O
task O
representations O
for O
cross O
- O
lingual O
transfer O
, O
which O
allow O
adaptation O
to O
languages O
not O
seen O
during O
pretraining O
. O
5 O
Conclusion O
We O
evaluate O
the O
impact O
of O
intermediate O
- O
task O
training O
on O
zero O
- O
shot O
cross O
- O
lingual O
transfer O
. O
We O
investigate O
9 O
intermediate O
tasks O
and O
how O
intermediate O
- O
task O
training O
impacts O
the O
zero O
- O
shot O
cross O
- O
lingual O
transfer O
to O
the O
9 O
target O
tasks O
in O
the O
XTREME O
benchmark O
. O
Overall O
, O
intermediate O
- O
task O
training O
signiÔ¨Åcantly O
improves O
the O
performance O
on O
BUCC O
and O
Tatoeba O
, O
the O
two O
sentence O
retrieval O
target O
tasks O
in O
the O
XTREME O
benchmark O
, O
across O
almost O
every O
intermediate O
- O
task O
conÔ¨Åguration O
. O
Our O
best O
models O
obtain O
5.9 O
and O
23.9 O
point O
gains O
on O
BUCC O
and O
Tatoeba O
, O
respectively O
, O
compared O
to O
the O
best O
available O
XLM O
- O
R O
baseline O
scores O
( O
Hu O
et O
al O
. O
, O
2020 O
) O
. O
We O
also O
observed O
gains O
in O
question O
- O
answering O
tasks O
, O
particularly O
using O
SQuAD O
v1.1 O
and O
v2.0 O
as O
intermediate O
tasks O
, O
with O
absolute O
gains O
of O
2.1 O
F1 O
for O
XQuAD O
, O
0.8 O
F1 O
for O
MLQA O
, O
and O
10.4 O
for O
F1 O
TyDiQA O
, O
again O
over O
the O
best O
available O
baseline O
scores O
. O
We O
improve O
over O
XLM O
- O
R O
by O
5.4 O
points O
on O
average O
on O
the O
XTREME O
benchmark O
. O
Additionally O
, O
we O
found O
multi O
- O
task O
training O
on O
all O
9 O
intermedi-564ate O
tasks O
to O
slightly O
outperform O
individual O
intermediate O
training O
. O
On O
the O
other O
hand O
, O
we O
found O
that O
neither O
incorporating O
multilingual O
MLM O
into O
the O
intermediate O
- O
task O
training O
phase O
nor O
translating O
intermediate O
- O
task O
data O
consistently O
led O
to O
improved O
transfer O
performance O
. O
While O
we O
have O
explored O
the O
extent O
to O
which O
English O
intermediate O
- O
task O
training O
can O
improve O
crosslingual O
transfer O
, O
a O
clear O
next O
avenue O
of O
investigation O
for O
future O
work O
is O
how O
the O
choice O
of O
intermediateand O
target O
- O
task O
languages O
inÔ¨Çuences O
transfer O
across O
different O
tasks O
. O
Acknowledgments O
This O
project O
has O
beneÔ¨Åted O
from O
support O
to O
SB O
by O
Eric O
and O
Wendy O
Schmidt O
( O
made O
by O
recommendation O
of O
the O
Schmidt O
Futures O
program O
) O
, O
by O
Samsung O
Research O
( O
under O
the O
project O
Improving O
Deep O
Learning O
using O
Latent O
Structure O
) O
, O
by O
Intuit O
, O
Inc. O
, O
by O
NVIDIA O
Corporation O
( O
with O
the O
donation O
of O
a O
Titan O
V O
GPU O
) O
, O
by O
Google O
( O
with O
the O
donation O
of O
Google O
Cloud O
credits O
) O
. O
IC O
has O
received O
funding O
from O
the O
European O
Union O
‚Äôs O
Horizon O
2020 O
research O
and O
innovation O
program O
under O
the O
Marie O
Sk≈Çodowska O
- O
Curie O
grant O
agreement O
No O
838188 O
. O
This O
project O
has O
beneÔ¨Åted O
from O
direct O
support O
by O
the O
NYU O
IT O
High O
Performance O
Computing O
Center O
. O
This O
material O
is O
based O
upon O
work O
supported O
by O
the O
National O
Science O
Foundation O
under O
Grant O
No O
. O
1922658 O
. O
Any O
opinions O
, O
Ô¨Åndings O
, O
and O
conclusions O
or O
recommendations O
expressed O
in O
this O
material O
are O
those O
of O
the O
author(s O
) O
and O
do O
not O
necessarily O
reÔ¨Çect O
the O
views O
of O
the O
National O
Science O
Foundation O
. O
Abstract O
Slot-Ô¨Ålling O
, O
Translation O
, O
Intent O
classiÔ¨Åcation O
, O
and O
Language O
identiÔ¨Åcation O
, O
or O
STIL O
, O
is O
a O
newly O
- O
proposed O
task O
for O
multilingual O
Natural O
Language O
Understanding O
( O
NLU O
) O
. O
By O
performing O
simultaneous O
slot O
Ô¨Ålling O
and O
translation O
into O
a O
single O
output O
language O
( O
English O
in O
this O
case O
) O
, O
some O
portion O
of O
downstream O
system O
components O
can O
be O
monolingual O
, O
reducing O
development O
and O
maintenance O
cost O
. O
Results O
are O
given O
using O
the O
multilingual O
BART O
model O
( O
Liu O
et O
al O
. O
, O
2020 O
) O
Ô¨Åne O
- O
tuned O
on O
7 O
languages O
using O
the O
MultiATIS++ O
dataset O
. O
When O
no O
translation O
is O
performed O
, O
mBART O
‚Äôs O
performance O
is O
comparable O
to O
the O
current O
state O
of O
the O
art O
system O
( O
Cross O
- O
Lingual O
BERT O
by O
Xu O
et O
al O
. O
( O
2020 O
) O
) O
for O
the O
languages O
tested O
, O
with O
better O
average O
intent O
classiÔ¨Åcation O
accuracy O
( O
96.07 O
% O
versus O
95.50 O
% O
) O
but O
worse O
average O
slot O
F1 O
( O
89.87 O
% O
versus O
90.81 O
% O
) O
. O
When O
simultaneous O
translation O
is O
performed O
, O
average O
intent O
classiÔ¨Åcation O
accuracy O
degrades O
by O
only O
1.7 O
% O
relative O
and O
average O
slot O
F1 O
degrades O
by O
only O
1.2 O
% O
relative O
. O
1 O
Introduction O
Multilingual O
Natural O
Language O
Understanding O
( O
NLU O
) O
, O
also O
called O
cross O
- O
lingual O
NLU O
, O
is O
a O
technique O
by O
which O
an O
NLU O
- O
based O
system O
can O
scale O
to O
multiple O
languages O
. O
A O
single O
model O
is O
trained O
on O
more O
than O
one O
language O
, O
and O
it O
can O
accept O
input O
from O
more O
than O
one O
language O
during O
inference O
. O
In O
most O
recent O
high O
- O
performing O
systems O
, O
a O
model O
is O
Ô¨Årst O
pre O
- O
trained O
using O
unlabeled O
data O
for O
all O
supported O
languages O
and O
then O
Ô¨Åne O
tuned O
for O
a O
speciÔ¨Åc O
task O
using O
a O
small O
set O
of O
labeled O
data O
( O
Conneau O
and O
Lample O
, O
2019 O
; O
Pires O
et O
al O
. O
, O
2019 O
) O
. O
Two O
typical O
tasks O
for O
goal O
- O
based O
systems O
, O
such O
as O
virtual O
assistants O
and O
chatbots O
, O
are O
intent O
classiÔ¨Åcation O
and O
slot O
Ô¨Ålling O
( O
Gupta O
et O
al O
. O
, O
2006 O
) O
. O
Though O
intent O
classiÔ¨Åcation O
creates O
a O
language O
agnostic O
output O
( O
the O
intent O
of O
the O
user O
) O
, O
slot O
Ô¨Ålling O
does O
not O
. O
Input O
‰ªéÁõêÊπñÂüéÂà∞Âä†Â∑ûÂ••ÂÖãÂÖ∞ÁöÑËà™Áè≠ O
Traditional O
Outputintent O
: O
Ô¨Çight O
slots O
: O
( O
ÁõêÊπñÂüé O
, O
fromloc.cityname O
) O
, O
. O
. O
. O
( O
Â••ÂÖãÂÖ∞ O
, O
toloc.cityname O
) O
, O
. O
. O
. O
( O
Âä†Â∑û O
, O
toloc.statename O
) O
STIL O
Outputintent O
: O
Ô¨Çight O
slots O
: O
( O
salt O
lake O
city O
, O
fromloc.cityname O
) O
, O
. O
. O
. O
( O
oakland O
, O
toloc.cityname O
) O
, O
. O
. O
. O
( O
california O
, O
toloc.statename O
) O
lang O
: O
zh O
Table O
1 O
: O
Today O
‚Äôs O
slot O
Ô¨Ålling O
systems O
do O
not O
translate O
the O
slot O
content O
, O
as O
shown O
in O
‚Äú O
Traditional O
Ouput O
. O
‚Äù O
With O
a O
STIL O
model O
, O
the O
slot O
content O
is O
translated O
and O
language O
identiÔ¨Åcation O
is O
performed O
. O
Instead O
, O
a O
slot-Ô¨Ålling O
model O
outputs O
the O
labels O
for O
each O
of O
input O
tokens O
from O
the O
user O
. O
Suppose O
the O
slot-Ô¨Ålling O
model O
can O
handle O
Llanguages O
. O
Downstream O
components O
must O
therefore O
handle O
all O
L O
languages O
for O
the O
full O
system O
to O
be O
multilingual O
acrossLlanguages O
. O
Machine O
translation O
could O
be O
performed O
before O
the O
slot O
Ô¨Ålling O
model O
at O
system O
runtime O
, O
though O
the O
latency O
would O
be O
fully O
additive O
, O
and O
some O
amount O
of O
information O
useful O
to O
the O
slotÔ¨Ålling O
model O
may O
be O
lost O
. O
Similarly O
, O
translation O
could O
occur O
after O
the O
slot-Ô¨Ålling O
model O
at O
runtime O
, O
but O
slot O
alignment O
between O
the O
source O
and O
target O
language O
is O
a O
non O
- O
trivial O
task O
( O
Jain O
et O
al O
. O
, O
2019 O
; O
Xu O
et O
al O
. O
, O
2020 O
) O
. O
Instead O
, O
the O
goal O
of O
this O
work O
was O
to O
build O
a O
single O
model O
that O
can O
simultaneously O
translate O
the O
input O
, O
output O
slotted O
text O
in O
a O
single O
language O
( O
English O
) O
, O
classify O
the O
intent O
, O
and O
classify O
the O
input O
language O
( O
See O
Table O
1 O
) O
. O
The O
STIL O
task O
is O
deÔ¨Åned O
such O
that O
the O
input O
language O
tag O
is O
not O
given O
to O
the O
model O
as O
input O
. O
Thus O
, O
language O
identiÔ¨Åcation O
is O
necessary O
so O
that O
the O
system O
can O
communicate O
back O
to O
the O
user O
in O
the O
correct O
language O
. O
Contributions O
of O
this O
work O
include O
( O
1 O
) O
the O
introduction O
of O
a O
new O
task O
for O
multilingual O
NLU O
, O
namely O
simultaneous O
Slot O
Ô¨Ålling O
, O
Translation O
, O
Intent O
clas-576Example O
Input O
Example O
Output O
Ô¨Ç¬®uge O
von O
salt O
lake O
city O
nach O
oakland O
kaliforniensalt O
< O
B O
- O
fromloc.city O
name O
> O
lake O
< O
I O
- O
fromloc.city O
name O
> O
city O
< O
I O
- O
fromloc.city O
name O
> O
oakland O
< O
B-toloc.city O
name O
> O
california O
< O
B-toloc.state O
name O
> O
< O
intent-Ô¨Çight O
> O
< O
lang O
- O
de O
> O
‰ªéÁõêÊπñÂüéÂà∞Âä†Â∑ûÂ••ÂÖãÂÖ∞ O
ÁöÑËà™Áè≠salt O
< O
B O
- O
fromloc.city O
name O
> O
lake O
< O
I O
- O
fromloc.city O
name O
> O
city O
< O
I O
- O
fromloc.city O
name O
> O
oakland O
< O
B-toloc.city O
name O
> O
california O
< O
B-toloc.state O
name O
> O
< O
intent-Ô¨Çight O
> O
< O
lang O
- O
zh O
> O
Table O
2 O
: O
Two O
text O
- O
to O
- O
text O
STIL O
examples O
. O
In O
all O
STIL O
cases O
, O
the O
output O
is O
in O
English O
. O
Each O
token O
is O
followed O
by O
its O
BIO O
- O
tagged O
slot O
label O
. O
The O
sequence O
of O
tokens O
and O
slots O
are O
followed O
by O
the O
intent O
and O
then O
the O
language O
. O
siÔ¨Åcation O
, O
and O
Language O
identiÔ¨Åcation O
( O
STIL O
) O
; O
( O
2 O
) O
both O
non O
- O
translated O
and O
STIL O
results O
using O
the O
mBART O
model O
( O
Liu O
et O
al O
. O
, O
2020 O
) O
trained O
using O
a O
fully O
text O
- O
to O
- O
text O
data O
format O
; O
and O
( O
3 O
) O
public O
release O
of O
source O
code O
used O
in O
this O
study O
, O
with O
a O
goal O
toward O
reproducibility O
and O
future O
work O
on O
the O
STIL O
task1 O
. O
2 O
Dataset O
The O
Airline O
Travel O
Information O
System O
( O
ATIS O
) O
dataset O
is O
a O
classic O
benchmark O
for O
goal O
- O
oriented O
NLU O
( O
Price O
, O
1990 O
; O
Tur O
et O
al O
. O
, O
2010 O
) O
. O
It O
contains O
utterances O
focused O
on O
airline O
travel O
, O
such O
as O
how O
much O
is O
the O
cheapest O
Ô¨Çight O
from O
Boston O
to O
New O
York O
tomorrow O
morning O
? O
The O
dataset O
is O
annotated O
with O
17 O
intents O
, O
though O
the O
distribution O
is O
skewed O
, O
with O
70 O
% O
of O
intents O
being O
the O
Ô¨Çight O
intent O
. O
Slots O
are O
labeled O
using O
the O
Beginning O
Inside O
Outside O
( O
BIO O
) O
format O
. O
ATIS O
was O
localized O
to O
Turkish O
and O
Hindi O
in O
2018 O
, O
forming O
MultiATIS O
( O
Upadhyay O
et O
al O
. O
, O
2018 O
) O
, O
and O
then O
to O
Spanish O
, O
Portuguese O
, O
German O
, O
French O
, O
Chinese O
, O
and O
Japanese O
in O
2020 O
, O
forming O
MultiATIS++ O
( O
Xu O
et O
al O
. O
, O
2020 O
) O
. O
In O
this O
work O
, O
Portuguese O
was O
excluded O
due O
to O
a O
lack O
of O
Portuguese O
pretraining O
in O
the O
publicly O
available O
mBART O
model O
, O
and O
Japanese O
was O
excluded O
due O
to O
a O
current O
lack O
of O
alignment O
between O
Japanese O
and O
English O
samples O
in O
MultiATIS++ O
. O
Hindi O
and O
Turkish O
data O
were O
taken O
from O
MultiATIS O
, O
and O
the O
training O
data O
were O
upsampled O
by O
3x O
for O
Hindi O
and O
7x O
for O
Turkish O
. O
Prior O
to O
any O
upsampling O
, O
there O
were O
4,488 O
training O
samples O
for O
English O
, O
Spanish O
, O
German O
, O
French O
, O
and O
Chinese O
. O
The O
test O
sets O
contained O
893 O
samples O
for O
all O
languages O
except O
Turkish O
, O
which O
had O
715 O
samples O
. O
For O
English O
, O
Spanish O
, O
German O
, O
French O
, O
and O
Chinese O
, O
validation O
sets O
of O
490 O
samples O
were O
used O
in O
all O
cases O
. O
Given O
the O
smaller O
data O
quantities O
for O
Hindi O
and O
Turkish O
, O
two O
training O
and O
validation O
set O
conÔ¨Ågurations O
were O
considered O
. O
The O
Ô¨Årst O
conÔ¨Åguration O
1https://github.com/jgmÔ¨Åtz/stil-mbart-multiatisppaacl2020matched O
that O
of O
Xu O
et O
al O
. O
( O
2020 O
) O
, O
using O
training O
sets O
of O
1,495 O
for O
Hindi O
and O
626 O
for O
Turkish O
along O
with O
validation O
sets O
of O
160 O
for O
Hindi O
and O
60 O
for O
Turkish O
. O
In O
the O
second O
conÔ¨Åguration O
, O
no O
validation O
sets O
were O
made O
for O
Hindi O
and O
Turkish O
( O
though O
there O
were O
still O
validation O
sets O
for O
the O
other O
languages O
) O
, O
and O
the O
training O
sets O
of O
1,600 O
Hindi O
samples O
and O
638 O
samples O
from O
MultiATIS O
were O
used O
. O
Two O
output O
formats O
are O
considered O
, O
being O
( O
1 O
) O
the O
non O
- O
translated O
, O
traditional O
case O
, O
in O
which O
translation O
of O
slot O
content O
is O
not O
performed O
, O
and O
( O
2 O
) O
the O
translated O
, O
STIL O
case O
, O
in O
which O
translation O
of O
slot O
content O
is O
performed O
. O
In O
both O
cases O
, O
the O
tokens O
, O
the O
labels O
, O
the O
intent O
, O
and O
the O
detected O
language O
are O
all O
output O
from O
the O
model O
as O
a O
single O
ordered O
text O
sequence O
, O
as O
shown O
in O
Table O
2 O
. O
3 O
Related O
Work O
Previous O
approaches O
for O
intent O
classiÔ¨Åcation O
and O
slot O
Ô¨Ålling O
have O
used O
either O
( O
1 O
) O
separate O
models O
for O
slot O
Ô¨Ålling O
, O
including O
support O
vector O
machines O
( O
Moschitti O
et O
al O
. O
, O
2007 O
) O
, O
conditional O
random O
Ô¨Åelds O
( O
Xu O
and O
Sarikaya O
, O
2014 O
) O
, O
and O
recurrent O
neural O
networks O
of O
various O
types O
( O
Kurata O
et O
al O
. O
, O
2016 O
) O
or O
( O
2 O
) O
joint O
models O
that O
diverge O
into O
separate O
decoders O
or O
layers O
for O
intent O
classiÔ¨Åcation O
and O
slot O
Ô¨Ålling O
( O
Xu O
and O
Sarikaya O
, O
2013 O
; O
Guo O
et O
al O
. O
, O
2014 O
; O
Liu O
and O
Lane O
, O
2016 O
; O
Hakkani O
- O
T O
¬®ur O
et O
al O
. O
, O
2016 O
) O
or O
that O
share O
hidden O
states O
( O
Wang O
et O
al O
. O
, O
2018 O
) O
. O
In O
this O
work O
, O
a O
fully O
text O
- O
to O
- O
text O
approach O
similar O
to O
that O
of O
the O
T5 O
model O
was O
used O
, O
such O
that O
the O
model O
would O
have O
maximum O
information O
sharing O
across O
the O
four O
STIL O
sub O
- O
tasks O
. O
Encoder O
- O
decoder O
models O
, O
Ô¨Årst O
introduced O
in O
2014 O
( O
Sutskever O
et O
al O
. O
, O
2014 O
) O
, O
are O
a O
mainstay O
of O
neural O
machine O
translation O
. O
The O
original O
transformer O
model O
included O
both O
an O
encoder O
and O
a O
decoder O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O
Since O
then O
, O
much O
of O
the O
work O
on O
transformers O
focuses O
on O
models O
with O
only O
an O
encoder O
pretrained O
with O
autoencoding O
techniques O
( O
e.g. O
BERT O
by O
Devlin O
et O
al O
. O
( O
2018 O
) O
) O
or O
auto O
- O
regressive O
models O
with O
only O
a O
decoder O
( O
e.g.577GPT O
by O
Radford O
( O
2018 O
) O
) O
. O
In O
this O
work O
, O
it O
was O
assumed O
that O
encoder O
- O
decoder O
models O
, O
such O
as O
BART O
( O
Lewis O
et O
al O
. O
, O
2019 O
) O
and O
T5 O
( O
Raffel O
et O
al O
. O
, O
2019 O
) O
, O
are O
the O
best O
architectural O
candidates O
given O
the O
translation O
component O
of O
the O
STIL O
task O
, O
as O
well O
as O
past O
state O
of O
the O
art O
advancement O
by O
encoder O
- O
decoder O
models O
on O
ATIS O
, O
cited O
above O
. O
Rigorous O
architectural O
comparisons O
are O
left O
to O
future O
work O
. O
4 O
The O
Model O
4.1 O
The O
Pretrained O
mBART O
Model O
The O
multilingual O
BART O
( O
mBART O
) O
model O
architecture O
was O
used O
( O
Liu O
et O
al O
. O
, O
2020 O
) O
, O
as O
well O
as O
the O
pretrained O
mBART.cc25 O
model O
described O
in O
the O
same O
paper O
. O
The O
model O
consists O
of O
12 O
encoder O
layers O
, O
12 O
decoder O
layers O
, O
a O
hidden O
layer O
size O
of O
1,024 O
, O
and O
16 O
attention O
heads O
, O
yielding O
a O
parameter O
count O
of O
680M. O
The O
mBART.cc25 O
model O
was O
trained O
on O
25 O
languages O
for O
500k O
steps O
using O
a O
1.4 O
TB O
corpus O
of O
scraped O
website O
data O
taken O
from O
Common O
Crawl O
( O
Wenzek O
et O
al O
. O
, O
2019 O
) O
. O
The O
model O
was O
trained O
to O
reconstruct O
masked O
tokens O
and O
to O
rearrange O
scrambled O
sentences O
. O
SentencePiece O
tokenization O
( O
Kudo O
and O
Richardson O
, O
2018 O
) O
was O
used O
for O
mBART.cc25 O
with O
a O
sub O
- O
word O
vocabulary O
size O
of O
250k O
. O
4.2 O
This O
Work O
The O
same O
vocabulary O
as O
that O
of O
the O
pretrained O
model O
was O
used O
for O
this O
work O
, O
and O
SentencePiece O
tokenization O
was O
performed O
on O
the O
full O
sequence O
, O
including O
the O
slot O
tags O
, O
intent O
tags O
, O
and O
language O
tags O
. O
For O
all O
mBART O
experiments O
and O
datasets O
, O
data O
from O
all O
languages O
were O
shufÔ¨Çed O
together O
. O
The O
fairseq O
library O
was O
used O
for O
all O
experimentation O
( O
Ott O
et O
al O
. O
, O
2019 O
) O
. O
Training O
was O
performed O
on O
8 O
Nvidia O
V100 O
GPUs O
( O
16 O
GB O
) O
using O
a O
batch O
size O
of O
32 O
, O
layer O
normalization O
for O
both O
the O
encoder O
and O
the O
decoder O
( O
Xu O
et O
al O
. O
, O
2019 O
) O
; O
label O
smoothed O
cross O
entropy O
with O
/epsilon1= O
0.2 O
( O
Szegedy O
et O
al O
. O
, O
2016 O
) O
; O
the O
ADAM O
optimizer O
with O
Œ≤1= O
0.9andŒ≤2= O
0.999(Kingma O
and O
Ba O
, O
2014 O
) O
; O
an O
initial O
learning O
rate O
of O
3√ó10‚àí5with O
polynomial O
decay O
over O
20,000 O
updates O
after O
1 O
epoch O
of O
warmup O
; O
attention O
dropout O
of O
0.1 O
and O
dropout O
of O
0.2 O
elsewhere O
; O
and O
FP16 O
type O
for O
weights O
. O
Each O
model O
was O
trained O
for O
19 O
epochs O
, O
which O
took O
5 O
- O
6 O
hours O
. O
5 O
Results O
and O
Discussion O
Results O
from O
the O
models O
are O
given O
in O
Table O
3 O
. O
Statistical O
signiÔ¨Åcance O
was O
evaluated O
using O
the O
Wilsonmethod O
( O
Wilson O
, O
1927 O
) O
with O
95 O
% O
conÔ¨Ådence O
. O
5.1 O
Comparing O
to O
Xu O
et O
al O
. O
( O
2020 O
) O
Examining O
the O
Ô¨Årst O
training O
conÔ¨Åguration O
( O
1,496 O
samples O
for O
Hindi O
and O
626 O
for O
Turkish O
) O
, O
the O
nontranslated O
mBART O
‚Äôs O
macro O
- O
averaged O
intent O
classiÔ¨Åcation O
( O
96.07 O
% O
) O
outperforms O
Cross O
- O
Lingual O
BERT O
by O
Xu O
et O
al O
. O
( O
2020 O
) O
( O
95.50 O
% O
) O
, O
but O
slot O
F1 O
is O
worse O
( O
89.87 O
% O
for O
non O
- O
translated O
mBART O
and O
90.81 O
% O
for O
Cross O
- O
Lingual O
BERT O
) O
. O
The O
differences O
are O
statistically O
signiÔ¨Åcant O
in O
both O
cases O
. O
5.2 O
With O
and O
Without O
Translation O
When O
translation O
is O
performed O
( O
the O
STIL O
task O
) O
, O
intent O
classiÔ¨Åcation O
accuracy O
degrades O
by O
1.7 O
% O
relative O
from O
96.07 O
% O
to O
94.40 O
% O
, O
and O
slot O
F1 O
degrades O
by O
1.2 O
% O
relative O
from O
89.87 O
% O
to O
88.79 O
% O
. O
The O
greatest O
degradation O
occurred O
for O
utterances O
involving O
Ô¨Çight O
number O
, O
airfare O
, O
and O
airport O
name O
( O
in O
that O
order O
) O
. O
5.3 O
Additional O
Hindi O
and O
Turkish O
Training O
Data O
Adding O
105 O
more O
Hindi O
and O
12 O
more O
Turkish O
training O
examples O
results O
in O
improved O
performance O
for O
the O
translated O
, O
STIL O
mBART O
model O
. O
Macro O
- O
averaged O
intent O
classiÔ¨Åcation O
improves O
from O
94.40 O
% O
to O
95.94 O
% O
, O
and O
slot O
F1 O
improves O
from O
88.79 O
% O
to O
90.10 O
% O
, O
both O
of O
which O
are O
statistically O
signiÔ¨Åcant O
. O
By O
adding O
these O
117 O
samples O
, O
the O
STIL O
mBART O
model O
matches O
the O
performance O
( O
within O
conÔ¨Ådence O
intervals O
) O
of O
the O
non O
- O
translated O
mBART O
model O
. O
This O
Ô¨Ånding O
suggests O
that O
the O
STIL O
models O
may O
require O
more O
training O
data O
than O
traditional O
, O
non O
- O
translated O
slot O
Ô¨Ålling O
models O
. O
Additionally O
, O
by O
adding O
more O
Hindi O
and O
Turkish O
data O
, O
both O
the O
intent O
accuracy O
and O
the O
slot O
Ô¨Ålling O
F1 O
improves O
for O
every O
individual O
language O
of O
the O
translated O
, O
STIL O
models O
, O
suggesting O
that O
some O
portion O
of O
the O
internal O
, O
learned O
representation O
is O
language O
agnostic O
. O
Finally O
, O
the O
results O
suggest O
that O
there O
is O
a O
trainingsize O
- O
dependent O
performance O
advantage O
in O
using O
a O
single O
output O
language O
, O
as O
contrasted O
with O
the O
nontranslated O
mBART O
model O
, O
for O
which O
the O
intent O
classiÔ¨Åcation O
accuracy O
and O
slot O
F1 O
does O
not O
improve O
( O
with O
statistical O
signiÔ¨Åcance O
) O
when O
using O
the O
additional O
Hindi O
and O
Turkish O
training O
samples O
. O
5.4 O
Language O
IdentiÔ¨Åcation O
Language O
identiÔ¨Åcation O
F1 O
is O
above O
99.7 O
% O
for O
all O
languages O
, O
with O
perfect O
performance O
in O
many O
cases.578Intent O
accuracy O
en O
es O
de O
zh O
fr O
hi O
tr O
Mac O
Avg O
Cross O
- O
Lingual O
BERT O
( O
Xu O
et O
al O
. O
, O
2020 O
) O
97.20 O
96.77 O
96.86 O
95.54 O
97.24 O
92.70 O
tr=149592.20 O
tr=62695.50 O
Seq2Seq O
- O
Ptr O
( O
Rongali O
et O
al O
. O
, O
2020 O
) O
97.42 O
Stack O
Propagation O
( O
Qin O
et O
al O
. O
, O
2019 O
) O
97.5 O
Joint O
BERT O
+ O
CRF O
( O
Chen O
et O
al O
. O
, O
2019 O
) O
97.9 O
Non O
- O
translated O
mBART O
, O
with O
hi O
- O
tr O
val O
96.98 O
96.98 O
97.09 O
96.08 O
97.65 O
95.07 O
tr=149592.73 O
tr=62696.07 O
Translated O
/ O
STIL O
mBART O
, O
with O
hi O
- O
tr O
val O
95.86 O
94.62 O
95.63 O
93.84 O
95.97 O
93.84 O
tr=149591.05 O
tr=62694.40 O
Non O
- O
translated O
mBART O
, O
no O
hi O
- O
tr O
val O
97.09 O
97.20 O
97.20 O
96.30 O
97.42 O
94.74 O
tr=160094.27 O
tr=63896.32 O
Translated O
/ O
STIL O
mBART O
, O
no O
hi O
- O
tr O
val O
96.98 O
96.53 O
96.64 O
96.42 O
97.31 O
94.85 O
tr=160092.87 O
tr=63895.94 O
Slot O
F1 O
en O
es O
de O
zh O
fr O
hi O
tr O
Mac O
Avg O
Bi O
- O
RNN O
( O
Upadhyay O
et O
al O
. O
, O
2018 O
) O
95.2 O
80.6 O
tr=60078.9 O
tr=60084.90 O
Cross O
- O
Lingual O
BERT O
( O
Xu O
et O
al O
. O
, O
2020 O
) O
95.90 O
87.95 O
95.00 O
93.67 O
90.39 O
86.73 O
tr=149586.04 O
tr=62690.81 O
Stack O
Propagation O
( O
Qin O
et O
al O
. O
, O
2019 O
) O
96.1 O
Joint O
BERT O
( O
Chen O
et O
al O
. O
, O
2019 O
) O
96.1 O
Non O
- O
translated O
mBART O
, O
with O
hi O
- O
tr O
val O
95.03 O
86.76 O
94.42 O
92.13 O
89.31 O
86.91 O
tr=149584.53 O
tr=62689.87 O
Translated O
/ O
STIL O
mBART O
, O
with O
hi O
- O
tr O
val O
93.81 O
90.38 O
91.41 O
85.93 O
91.24 O
83.98 O
tr=149584.79 O
tr=62688.79 O
Non O
- O
translated O
mBART O
, O
no O
hi O
- O
tr O
val O
95.00 O
86.87 O
94.14 O
92.22 O
89.32 O
87.42 O
tr=160084.33 O
tr=63889.90 O
Translated O
/ O
STIL O
mBART O
, O
no O
hi O
- O
tr O
val O
94.66 O
91.55 O
92.61 O
87.73 O
92.15 O
86.74 O
tr=160085.23 O
tr=63890.10 O
Language O
IdentiÔ¨Åcation O
F1 O
en O
es O
de O
zh O
fr O
hi O
tr O
Mac O
Avg O
Translated O
/ O
STIL O
mBART O
, O
with O
hi O
- O
tr O
val O
100.00 O
98.87 O
100.00 O
100.00 O
98.95 O
100.00 O
99.93 O
99.68 O
Translated O
/ O
STIL O
mBART O
, O
no O
hi O
- O
tr O
val O
99.78 O
99.83 O
100.00 O
100.00 O
99.72 O
100.00 O
99.86 O
99.88 O
Table O
3 O
: O
Results O
are O
shown O
for O
intent O
accuracy O
, O
slot O
F1 O
score O
, O
and O
language O
identiÔ¨Åcation O
F1 O
score O
. O
For O
English O
, O
Spanish O
, O
German O
, O
Chinese O
, O
and O
French O
in O
all O
of O
the O
models O
shown O
above O
( O
including O
other O
work O
) O
, O
training O
sets O
were O
between O
4,478 O
and O
4,488 O
samples O
, O
and O
validation O
sets O
were O
between O
490 O
and O
500 O
samples O
. O
In O
this O
work O
, O
two O
training O
set O
sizes O
were O
used O
for O
Hindi O
and O
Turkish O
, O
denoted O
by O
‚Äú O
tr= O
‚Äù O
and O
‚Äú O
with O
hi O
- O
tr O
val[idation O
set O
] O
‚Äù O
or O
‚Äú O
no O
hi O
- O
tr O
val[idation O
set O
] O
‚Äù O
. O
Across O
all O
work O
shown O
above O
, O
the O
tests O
sets O
contained O
893 O
samples O
for O
all O
languages O
except O
Turkish O
, O
for O
which O
the O
test O
set O
was O
715 O
samples O
. O
Perfect O
performance O
on O
Chinese O
and O
Hindi O
is O
unsurprising O
given O
their O
unique O
scripts O
versus O
the O
other O
languages O
tested O
. O
6 O
Conclusion O
This O
preliminary O
work O
demonstrates O
that O
a O
single O
NLU O
model O
can O
perform O
simultaneous O
slot O
Ô¨Ålling O
, O
translation O
, O
intent O
classiÔ¨Åcation O
, O
and O
language O
identiÔ¨Åcation O
across O
7 O
languages O
using O
MultiATIS++ O
. O
Such O
an O
NLU O
model O
would O
negate O
the O
need O
for O
multiple O
- O
language O
support O
in O
some O
portion O
of O
downstream O
system O
components O
. O
Performance O
is O
not O
irreconcilably O
worse O
than O
traditional O
slot-Ô¨Ålling O
models O
, O
and O
performance O
is O
statistically O
equivalent O
with O
a O
small O
amount O
of O
additional O
training O
data O
. O
Looking O
forward O
, O
a O
more O
challenging O
dataset O
is O
needed O
to O
further O
develop O
the O
translation O
compo O
- O
nent O
of O
the O
STIL O
task O
. O
The O
English O
MultiATIS++ O
test O
set O
only O
contains O
455 O
unique O
entity O
- O
slot O
pairs O
. O
An O
ideal O
future O
dataset O
would O
include O
freeform O
and O
varied O
content O
, O
such O
as O
text O
messages O
, O
song O
titles O
, O
or O
open O
- O
domain O
questions O
. O
Until O
then O
, O
work O
remains O
to O
achieve O
parity O
with O
English O
- O
only O
ATIS O
models O
. O
Acknowledgments O
The O
author O
would O
like O
to O
thank O
Saleh O
Soltan O
, O
Gokhan O
Tur O
, O
Saab O
Mansour O
, O
and O
Batool O
Haider O
for O
reviewing O
this O
work O
and O
providing O
valuable O
feedback O
. O
Abstract O
Simultaneous O
text O
translation O
and O
end O
- O
to O
- O
end O
speech O
translation O
have O
recently O
made O
great O
progress O
but O
little O
work O
has O
combined O
these O
tasks O
together O
. O
We O
investigate O
how O
to O
adapt O
simultaneous O
text O
translation O
methods O
such O
as O
wait O
- O
kand O
monotonic O
multihead O
attention O
to O
end O
- O
to O
- O
end O
simultaneous O
speech O
translation O
by O
introducing O
a O
pre O
- O
decision O
module O
. O
A O
detailed O
analysis O
is O
provided O
on O
the O
latency O
- O
quality O
trade O
- O
offs O
of O
combining O
Ô¨Åxed O
and O
Ô¨Çexible O
predecision O
with O
Ô¨Åxed O
and O
Ô¨Çexible O
policies O
. O
We O
also O
design O
a O
novel O
computation O
- O
aware O
latency O
metric O
, O
adapted O
from O
Average O
Lagging.1 O
1 O
Introduction O
Simultaneous O
speech O
translation O
( O
SimulST O
) O
generates O
a O
translation O
from O
an O
input O
speech O
utterance O
before O
the O
end O
of O
the O
utterance O
has O
been O
heard O
. O
SimulST O
systems O
aim O
at O
generating O
translations O
with O
maximum O
quality O
and O
minimum O
latency O
, O
targeting O
applications O
such O
as O
video O
caption O
translations O
and O
real O
- O
time O
language O
interpreter O
. O
While O
great O
progress O
has O
recently O
been O
achieved O
on O
both O
end O
- O
to O
- O
end O
speech O
translation O
( O
Ansari O
et O
al O
. O
, O
2020 O
) O
and O
simultaneous O
text O
translation O
( O
SimulMT O
) O
( O
Grissom O
II O
et O
al O
. O
, O
2014 O
; O
Gu O
et O
al O
. O
, O
2017 O
; O
Luo O
et O
al O
. O
, O
2017 O
; O
Lawson O
et O
al O
. O
, O
2018 O
; O
Alinejad O
et O
al O
. O
, O
2018 O
; O
Zheng O
et O
al O
. O
, O
2019b O
, O
a O
; O
Ma O
et O
al O
. O
, O
2020 O
; O
Arivazhagan O
et O
al O
. O
, O
2019 O
, O
2020 O
) O
, O
little O
work O
has O
combined O
the O
two O
tasks O
together O
( O
Ren O
et O
al O
. O
, O
2020 O
) O
. O
End O
- O
to O
- O
end O
SimulST O
models O
feature O
a O
smaller O
model O
size O
, O
greater O
inference O
speed O
and O
fewer O
compounding O
errors O
compared O
to O
their O
cascade O
counterpart O
, O
which O
perform O
streaming O
speech O
recognition O
followed O
by O
simultaneous O
machine O
translation O
. O
In O
addition O
, O
it O
has O
been O
demonstrated O
that O
end O
- O
to O
- O
end O
SimulST O
systems O
can O
have O
lower O
latency O
than O
cascade O
systems O
( O
Ren O
et O
al O
. O
, O
2020 O
) O
. O
1The O
code O
is O
available O
at O
https://github.com/ O
pytorch O
/ O
fairseqIn O
this O
paper O
, O
we O
study O
how O
to O
adapt O
methods O
developed O
for O
SimulMT O
to O
end O
- O
to O
- O
end O
SimulST O
. O
To O
this O
end O
, O
we O
introduce O
the O
concept O
of O
pre O
- O
decision O
module O
. O
Such O
module O
guides O
how O
to O
group O
encoder O
states O
into O
meaningful O
units O
prior O
to O
making O
a O
READ O
/ O
WRITE O
decision O
. O
A O
detailed O
analysis O
of O
the O
latency O
- O
quality O
trade O
- O
offs O
when O
combining O
a O
Ô¨Åxed O
or O
Ô¨Çexible O
pre O
- O
decision O
module O
with O
a O
Ô¨Åxed O
or O
Ô¨Çexible O
policy O
is O
provided O
. O
We O
also O
introduce O
a O
novel O
computation O
- O
aware O
latency O
metric O
, O
adapted O
from O
Average O
Lagging O
( O
AL O
) O
( O
Ma O
et O
al O
. O
, O
2019 O
) O
. O
2 O
Task O
formalization O
A O
SimulST O
model O
takes O
as O
input O
a O
sequence O
of O
acoustic O
features O
X= O
[ O
x1, O
... O
x|X|]extracted O
from O
speech O
samples O
every O
Tsms O
, O
and O
generates O
a O
sequence O
of O
text O
tokens O
Y= O
[ O
y1, O
... O
,y|Y|]in O
a O
target O
language O
. O
Additionally O
, O
it O
is O
able O
to O
generate O
yi O
with O
only O
partial O
input O
X1 O
: O
n(yi)= O
[ O
x1, O
... O
xn(yi O
) O
] O
, O
wheren(yi)‚â§|X|is O
the O
number O
of O
frames O
needed O
to O
generate O
the O
i O
- O
th O
target O
token O
yi O
. O
Note O
thatnis O
a O
monotonic O
function O
, O
i.e. O
n(yi‚àí1)‚â§n(yi O
) O
. O
A O
SimulST O
model O
is O
evaluated O
with O
respect O
to O
quality O
, O
using O
BLEU O
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
, O
and O
latency O
. O
We O
introduce O
two O
latency O
evaluation O
methods O
for O
SimulST O
that O
are O
adapted O
from O
SimulMT O
. O
We O
Ô¨Årst O
deÔ¨Åne O
two O
types O
of O
delays O
to O
generate O
the O
wordyi O
, O
a O
computation O
- O
aware O
( O
CA O
) O
and O
a O
non O
computation O
- O
aware O
( O
NCA O
) O
delay O
. O
The O
CA O
delay O
ofyi O
, O
dCA(yi O
) O
, O
is O
deÔ¨Åned O
as O
the O
time O
that O
elapses O
( O
speech O
duration O
) O
from O
the O
beginning O
of O
the O
process O
to O
the O
prediction O
of O
yi O
, O
while O
the O
NCA O
delay O
foryidCA(yi)is O
deÔ¨Åned O
by O
dNCA(yi O
) O
= O
Ts¬∑n(yi O
) O
. O
Note O
thatdNCAis O
an O
ideal O
case O
for O
dCAwhere O
the O
computational O
time O
for O
the O
model O
is O
ignored O
. O
Both O
delays O
are O
measured O
in O
milliseconds O
. O
Two O
types O
of O
latency O
measurement O
, O
LCAandLNCA O
, O
are O
calculated O
accordingly O
: O
L O
= O
C(D)whereCis O
a O
latency O
metric O
and O
D= O
[ O
d(y1), O
... O
,d O
( O
y|Y|)].582To O
better O
evaluate O
the O
latency O
for O
SimulST O
, O
we O
introduce O
a O
modiÔ¨Åcation O
to O
AL O
. O
We O
assume O
an O
oracle O
system O
that O
can O
perform O
perfect O
simultaneous O
translation O
for O
both O
latency O
and O
quality O
, O
while O
in O
Ma O
et O
al O
. O
( O
2019 O
) O
the O
oracle O
is O
ideal O
only O
from O
the O
latency O
perspective O
. O
We O
evaluate O
the O
lagging O
based O
on O
time O
rather O
than O
steps O
. O
The O
modiÔ¨Åed O
AL O
metric O
is O
deÔ¨Åned O
in O
Eq O
. O
( O
1 O
): O
AL=1 O
œÑ(|X|)œÑ(|X|)/summationdisplay O
i=1d(yi)‚àí|X| O
|Y‚àó|¬∑Ts¬∑(i‚àí1)(1 O
) O
where|Y‚àó|is O
the O
length O
of O
the O
reference O
translation O
, O
œÑ(|X|)is O
the O
index O
of O
the O
Ô¨Årst O
target O
token O
generated O
when O
the O
model O
read O
the O
full O
input O
. O
There O
are O
two O
beneÔ¨Åts O
from O
this O
modiÔ¨Åcation O
. O
The O
Ô¨Årst O
is O
that O
latency O
is O
measured O
using O
time O
instead O
of O
steps O
, O
which O
makes O
it O
agnostic O
to O
preprocessing O
and O
segmentation O
. O
The O
second O
is O
that O
it O
is O
more O
robust O
and O
can O
prevent O
an O
extremely O
low O
and O
trivial O
value O
when O
the O
prediction O
is O
signiÔ¨Åcantly O
shorter O
than O
the O
reference O
. O
3 O
Method O
3.1 O
Model O
Architecture O
End O
- O
to O
- O
end O
ST O
models O
directly O
map O
a O
source O
speech O
utterance O
into O
a O
sequence O
of O
target O
tokens O
. O
We O
use O
the O
S O
- O
Transformer O
architecture O
proposed O
by O
( O
Di O
Gangi O
et O
al O
. O
, O
2019b O
) O
, O
which O
achieves O
competitive O
performance O
on O
the O
MuST O
- O
C O
dataset O
( O
Di O
Gangi O
et O
al O
. O
, O
2019a O
) O
. O
In O
the O
encoder O
, O
a O
two O
- O
dimensional O
attention O
is O
applied O
after O
the O
CNN O
layers O
and O
a O
distance O
penalty O
is O
introduced O
to O
bias O
the O
attention O
towards O
short O
- O
range O
dependencies O
. O
We O
investigate O
two O
types O
of O
simultaneous O
translation O
mechanisms O
, O
Ô¨Çexible O
and O
Ô¨Åxed O
policy O
. O
In O
particular O
, O
we O
investigate O
monotonic O
multihead O
attention O
( O
Ma O
et O
al O
. O
, O
2020 O
) O
, O
which O
is O
an O
instance O
of O
Ô¨Çexible O
policy O
and O
the O
preÔ¨Åx O
- O
to O
- O
preÔ¨Åx O
model O
( O
Ma O
et O
al O
. O
, O
2019 O
) O
, O
an O
instance O
of O
Ô¨Åxed O
policy O
, O
designated O
by O
wait O
- O
kfrom O
now O
on O
. O
Monotonic O
Multihead O
Attention O
( O
MMA O
) O
( O
Ma O
et O
al O
. O
, O
2020 O
) O
extends O
monotonic O
attention O
( O
Raffel O
et O
al O
. O
, O
2017 O
; O
Arivazhagan O
et O
al O
. O
, O
2019 O
) O
to O
Transformer O
- O
based O
models O
. O
Each O
head O
in O
each O
layer O
has O
an O
independent O
step O
probability O
pijfor O
theith O
target O
andjth O
source O
step O
, O
and O
then O
uses O
a O
closed O
form O
expected O
attention O
for O
training O
. O
A O
weighted O
average O
and O
variance O
loss O
were O
proposed O
to O
control O
the O
behavior O
of O
the O
attention O
heads O
and O
thus O
the O
trade O
- O
offs O
between O
quality O
and O
latency O
. O
Wait O
- O
k(Ma O
et O
al O
. O
, O
2019 O
) O
is O
a O
Ô¨Åxed O
policy O
that O
waits O
forksource O
tokens O
, O
and O
then O
reads O
and O
writes O
alternatively O
. O
Wait- O
kcan O
be O
a O
special O
case O
of O
Monotonic O
InÔ¨Ånite O
- O
Lookback O
Attention O
( O
MILk O
) O
( O
Arivazhagan O
et O
al O
. O
, O
2019 O
) O
or O
MMA O
where O
the O
stepwise O
probability O
pij= O
0ifj‚àíi O
< O
k O
elsepij= O
1 O
. O
Figure O
1 O
: O
Simul O
- O
ST O
architecture O
with O
pre O
- O
decision O
module O
. O
Blue O
states O
in O
the O
Ô¨Ågure O
indicate O
the O
point O
Simul O
- O
SST O
model O
triggers O
the O
simultaneous O
making O
process O
3.2 O
Pre O
- O
Decision O
Module O
In O
SimulMT O
, O
READ O
or O
WRITE O
decisions O
are O
made O
at O
the O
token O
( O
word O
or O
BPE O
) O
level O
. O
However O
, O
with O
speech O
input O
, O
it O
is O
unclear O
when O
to O
make O
such O
decisions O
. O
For O
example O
, O
one O
could O
choose O
to O
read O
or O
write O
after O
each O
frame O
or O
after O
generating O
each O
encoder O
state O
. O
Meanwhile O
, O
a O
frame O
typically O
only O
covers O
10ms O
of O
the O
input O
while O
an O
encoder O
state O
generally O
covers O
40ms O
of O
the O
input O
( O
assuming O
a O
subsampling O
factor O
of O
4 O
) O
, O
while O
the O
average O
length O
of O
a O
word O
in O
our O
dataset O
is O
270ms O
. O
Intuitively O
, O
a O
policy O
like O
wait- O
kwill O
not O
have O
enough O
information O
to O
write O
a O
token O
after O
reading O
a O
frame O
or O
generating O
an O
encoder O
state O
. O
In O
principle O
, O
a O
Ô¨Çexible O
or O
modelbased O
policy O
such O
as O
MMA O
should O
be O
able O
to O
handle O
granulawhile O
MMA O
is O
more O
robust O
tr O
input O
. O
Our O
analysis O
will O
show O
, O
however O
, O
that O
o O
the O
granularity O
of O
the O
input O
, O
it O
also O
performs O
poorly O
when O
the O
input O
is O
too O
Ô¨Åne O
- O
grained O
. O
In O
order O
to O
overcome O
these O
issues O
, O
we O
introduce O
the O
notion O
of O
pre O
- O
decision O
module O
, O
which O
groups O
frames O
or O
encoder O
states O
, O
prior O
to O
making O
a O
decision O
. O
A O
pre O
- O
decision O
module O
generates O
a O
series O
of O
trigger O
probabilities O
ptron O
each O
encoder O
states O
to O
indicate O
whether O
a O
simultaneous O
decision O
should O
be583made O
. O
Ifptr>0.5 O
, O
the O
model O
triggers O
the O
simultaneous O
decision O
making O
, O
otherwise O
keeps O
reading O
new O
frames O
. O
We O
propose O
two O
types O
of O
pre O
- O
decision O
module O
. O
Fixed O
Pre O
- O
Decision O
A O
straightforward O
policy O
for O
a O
Ô¨Åxed O
pre O
- O
decision O
module O
is O
to O
trigger O
simultaneous O
decision O
making O
every O
Ô¨Åxed O
number O
of O
frames O
. O
Let O
‚àÜtbe O
the O
time O
corresponding O
to O
this O
Ô¨Åxed O
number O
of O
frames O
, O
with O
‚àÜta O
multiple O
of O
Ts O
, O
andre O
= O
int(|X|/|H|).ptrat O
encoder O
step O
jis O
deÔ¨Åned O
in O
Eq O
. O
( O
2 O
): O
ptr(j O
) O
= O
/braceleftBigg O
1if O
mod O
( O
j¬∑re¬∑Ts,‚àÜt O
) O
= O
0 O
, O
0Otherwise.(2 O
) O
Flexible O
Pre O
- O
Decision O
We O
use O
an O
oracle O
Ô¨Çexible O
pre O
- O
decision O
module O
that O
uses O
the O
source O
boundaries O
either O
at O
the O
word O
or O
phoneme O
level O
. O
Let O
Abe O
the O
alignment O
between O
encoder O
states O
and O
source O
labels O
( O
word O
or O
phoneme O
) O
. O
A(hi)represents O
the O
token O
thathialigns O
to O
. O
The O
trigger O
probability O
can O
then O
be O
deÔ¨Åned O
in O
Eq O
. O
( O
3 O
): O
ptr(j O
) O
= O
/braceleftBigg O
0ifA(hj O
) O
= O
A(hj‚àí1 O
) O
1Otherwise.(3 O
) O
4 O
Experiments O
We O
conduct O
experiments O
on O
the O
English O
- O
German O
portion O
of O
the O
MuST O
- O
C O
dataset O
( O
Di O
Gangi O
et O
al O
. O
, O
2019a O
) O
, O
where O
source O
audio O
, O
source O
transcript O
and O
target O
translation O
are O
available O
. O
We O
train O
on O
408 O
hours O
of O
speech O
and O
234k O
sentences O
of O
text O
data O
. O
We O
use O
Kaldi O
( O
Povey O
et O
al O
. O
, O
2011 O
) O
to O
extract O
80 O
dimensional O
log O
- O
mel O
Ô¨Ålter O
bank O
features O
, O
computed O
with O
a O
25mswindow O
size O
and O
a O
10 O
mswindow O
shift O
. O
For O
text O
, O
we O
use O
SentencePiece O
( O
Kudo O
and O
Richardson O
, O
2018 O
) O
to O
generate O
a O
unigram O
vocabulary O
of O
size O
10,000 O
. O
We O
use O
Gentle2to O
generate O
the O
alignment O
between O
source O
text O
and O
speech O
as O
the O
label O
to O
generate O
the O
oracle O
Ô¨Çexible O
predecision O
module O
. O
Translation O
quality O
is O
evaluated O
with O
case O
- O
sensitive O
detokenized O
BLEU O
with O
SACRE O
BLEU O
( O
Post O
, O
2018 O
) O
. O
The O
latency O
is O
evaluated O
with O
our O
proposed O
modiÔ¨Åcation O
of O
AL O
( O
Ma O
et O
al O
. O
, O
2019 O
) O
. O
All O
results O
are O
reported O
on O
the O
MuSTC O
dev O
set O
. O
All O
speech O
translation O
models O
are O
Ô¨Årst O
pretrained O
on O
the O
ASR O
task O
where O
the O
target O
vocabulary O
is O
character O
- O
based O
, O
in O
order O
to O
initialize O
the O
2https://lowerquality.com/gentle/encoder O
. O
We O
follow O
the O
same O
hyperparameter O
settings O
from O
( O
Di O
Gangi O
et O
al O
. O
, O
2019b O
) O
. O
We O
follow O
the O
latency O
regularization O
method O
introduced O
by O
( O
Ma O
et O
al O
. O
, O
2020 O
; O
Arivazhagan O
et O
al O
. O
, O
2019 O
) O
, O
The O
objective O
function O
to O
optimize O
is O
L=‚àílog(P(Y|X O
) O
) O
+ O
Œªmax(C(D),0 O
) O
( O
4 O
) O
WhereCis O
a O
latency O
metric O
( O
AL O
in O
this O
case O
) O
and O
Dis O
described O
in O
Section O
2 O
. O
Only O
samples O
with O
AL>0are O
regularized O
to O
avoid O
overÔ¨Åtting O
. O
For O
the O
models O
with O
monotonic O
multihead O
attention O
, O
we O
Ô¨Årst O
train O
a O
model O
without O
latency O
with O
Œªlatency O
= O
0 O
. O
After O
the O
model O
converges O
, O
Œªlatency O
is O
set O
to O
a O
desired O
value O
and O
the O
model O
is O
continue O
trained O
until O
convergence O
. O
The O
latency O
- O
quality O
trade O
- O
offs O
of O
the O
4 O
types O
of O
model O
from O
the O
combination O
of O
Ô¨Åxed O
or O
Ô¨Çexible O
predecision O
with O
Ô¨Åxed O
or O
Ô¨Çexible O
policy O
are O
presented O
in O
Fig O
. O
2 O
. O
The O
non O
computation O
- O
aware O
delays O
are O
used O
to O
calculate O
the O
latency O
metric O
in O
order O
to O
evaluate O
those O
trade O
- O
offs O
from O
a O
purely O
algorithmic O
perspective O
. O
Fixed O
Pre O
- O
Decision O
+ O
Fixed O
Policy3(Fig O
. O
2a O
) O
. O
As O
expected O
, O
both O
quality O
and O
latency O
increase O
with O
step O
size O
and O
lagging O
. O
In O
addition O
, O
the O
latencyquality O
trade O
- O
offs O
are O
highly O
dependent O
on O
the O
step O
size O
of O
the O
pre O
- O
decision O
module O
. O
For O
example O
, O
with O
step O
size O
120ms O
, O
the O
performance O
is O
very O
poor O
even O
with O
largekbecause O
of O
very O
limited O
information O
being O
read O
before O
writing O
a O
target O
token O
. O
Large O
step O
sizes O
improve O
the O
quality O
but O
introduce O
a O
lower O
bound O
on O
the O
latency O
. O
Note O
that O
step O
size O
280ms O
, O
which O
provides O
an O
effective O
latency O
- O
quality O
tradeoff O
compared O
to O
other O
step O
sizes O
, O
also O
matches O
the O
average O
word O
length O
of O
271ms O
. O
This O
motivates O
the O
study O
of O
a O
Ô¨Çexible O
pre O
- O
decision O
module O
based O
on O
word O
boundaries O
. O
Fixed O
Pre O
- O
Decision O
+ O
Flexible O
Policy4(Fig O
. O
2b O
) O
Similar O
to O
wait- O
k O
, O
MMA O
obtains O
very O
poor O
performance O
with O
a O
small O
step O
size O
of O
120ms O
. O
For O
other O
step O
sizes O
, O
MMA O
obtains O
similar O
latency O
- O
quality O
trade O
- O
offs O
, O
demonstrating O
some O
form O
of O
robustness O
to O
the O
step O
size O
. O
Flexible O
Pre O
- O
Decision O
Curve‚ãÜand O
in O
Ô¨Ågure O
Fig O
. O
2 O
show O
latency O
- O
quality O
trade O
- O
offs O
when O
the O
predecision O
module O
is O
determined O
by O
oracle O
word O
or O
phoneme O
boundaries O
. O
Note O
that O
a O
SimulST O
model O
3k= O
1,2,3,4,5,6,7,8,9,10 O
4Œª= O
0.001,0.004,0.01,0.02,0.04,0.06,0.08,0.15841000 O
2000 O
3000 O
4000 O
5000 O
AL0246810121416BLEU O
( O
a)Wait O
- O
k O
1000 O
2000 O
3000 O
4000 O
5000 O
AL46810121416BLEU O
Step O
size O
: O
120ms O
Step O
size O
: O
200ms O
Step O
size O
: O
280ms O
Step O
size O
: O
360ms O
Step O
size O
: O
440ms O
Step O
size O
: O
word O
Step O
size O
: O
phoneme O
( O
b)MMA O
Figure O
2 O
: O
Latency O
- O
Quality O
trade O
- O
off O
curves O
. O
The O
unit O
of O
AL O
is O
millisecond O
1000 O
2000 O
3000 O
4000 O
5000 O
6000 O
AL46810121416BLEU O
Fixed O
PD O
+ O
wait O
- O
k O
Fixed O
PD O
+ O
MMA O
Flexible O
PD O
( O
oracle O
word O
) O
+ O
wait O
- O
k O
Flexible O
PD O
( O
oracle O
word O
) O
+ O
MMA O
Figure O
3 O
: O
Comparison O
of O
best O
models O
in O
four O
settings O
500 O
1000 O
1500 O
2000 O
2500 O
3000 O
3500 O
4000 O
4500 O
5000 O
AL0246810121416BLEU O
Step O
size O
: O
120ms O
Step O
size O
: O
200ms O
Step O
size O
: O
280ms O
Step O
size O
: O
360ms O
Step O
size O
: O
440ms O
Figure O
4 O
: O
Computation O
- O
aware O
latency O
for O
Ô¨Åxed O
pre O
- O
decision O
+ O
wait O
- O
kpolicy O
. O
Points O
on O
dotted O
lines O
are O
computation O
- O
aware O
, O
without O
lines O
are O
non O
- O
computation O
- O
aware O
would O
not O
normally O
have O
access O
to O
this O
information O
and O
that O
the O
purpose O
of O
this O
experiment O
is O
to O
guide O
future O
design O
of O
a O
Ô¨Çexible O
pre O
- O
decision O
model O
. O
First O
, O
as O
previously O
observed O
, O
the O
granularity O
of O
the O
pre O
- O
decision O
greatly O
inÔ¨Çuences O
the O
latency O
- O
quality O
trade O
- O
offs O
. O
Models O
using O
phoneme O
boundaries O
obtain O
very O
poor O
translation O
quality O
because O
those O
boundaries O
are O
too O
granular O
, O
with O
an O
average O
phoneme O
duration O
of O
77ms O
. O
In O
addition O
, O
comparing O
MMA O
and O
wait- O
kwith O
phoneme O
boundaries O
, O
MMA O
is O
found O
to O
be O
more O
robust O
to O
the O
granularity O
of O
the O
pre O
- O
decision O
. O
Best O
Curves O
The O
best O
settings O
for O
each O
approachare O
compared O
in O
Fig O
. O
3 O
. O
For O
Ô¨Åxed O
pre O
- O
decision O
, O
we O
choose O
the O
setting O
that O
has O
the O
best O
quality O
for O
each O
latency O
bucket O
of O
500ms O
, O
while O
for O
the O
Ô¨Çexible O
pre O
- O
decision O
we O
use O
oracle O
word O
boundaries O
. O
For O
both O
wait O
- O
kand O
MMA O
, O
the O
Ô¨Çexible O
pre O
- O
decision O
module O
outperforms O
the O
Ô¨Åxed O
pre O
- O
decision O
module O
. O
This O
is O
expected O
since O
the O
Ô¨Çexible O
pre O
- O
decision O
module O
uses O
oracle O
information O
in O
the O
form O
of O
precomputed O
word O
boundaries O
but O
provides O
a O
direction O
for O
future O
research O
. O
The O
best O
latency O
- O
quality O
trade O
- O
offs O
are O
obtained O
with O
MMA O
and O
Ô¨Çexible O
predecision O
from O
word O
boundaries O
. O
4.1 O
Computation O
Aware O
Latency O
We O
also O
consider O
the O
computation O
- O
aware O
latency O
described O
in O
Section O
2 O
, O
shown O
in O
Fig O
. O
4 O
. O
The O
focus O
is O
on O
Ô¨Åxed O
pre O
- O
decision O
approaches O
in O
order O
to O
understand O
the O
relation O
between O
the O
granularity O
of O
the O
pre O
- O
decision O
and O
the O
computation O
time O
. O
Fig O
. O
4 O
shows O
that O
as O
the O
step O
size O
increases O
, O
the O
difference O
between O
the O
NCA O
and O
the O
CA O
latency O
shrinks O
. O
This O
is O
because O
with O
larger O
step O
sizes O
, O
there O
is O
less O
overhead O
of O
recomputing O
the O
bidirectional O
encoder O
states5 O
. O
We O
recommend O
future O
work O
on O
SimulST O
to O
make O
use O
of O
CA O
latency O
as O
it O
reÔ¨Çects O
a O
more O
realistic O
evaluation O
, O
especially O
in O
low O
- O
latency O
regimes O
, O
and O
is O
able O
to O
distinguish O
streaming O
capable O
systems O
. O
5 O
Conclusion O
We O
investigated O
how O
to O
adapt O
SimulMT O
methods O
to O
end O
- O
to O
- O
end O
SimulST O
by O
introducing O
the O
concept O
of O
pre O
- O
decision O
module O
. O
We O
also O
adapted O
Average O
Lagging O
to O
be O
computation O
- O
aware O
. O
The O
effects O
of O
combining O
a O
Ô¨Åxed O
or O
Ô¨Çexible O
pre O
- O
decision O
module O
5This O
is O
a O
common O
practice O
in O
SimulMT O
where O
the O
input O
length O
is O
signiÔ¨Åcantly O
shorter O
than O
in O
SimulST O
( O
Arivazhagan O
et O
al O
. O
, O
2019 O
; O
Ma O
et O
al O
. O
, O
2019 O
; O
Arivazhagan O
et O
al O
. O
, O
2020)585with O
a O
Ô¨Åxed O
or O
Ô¨Çexible O
policy O
were O
carefully O
analyzed O
. O
Future O
work O
includes O
building O
an O
incremental O
encoder O
to O
reduce O
the O
CA O
latency O
and O
design O
a O
learnable O
pre O
- O
decision O
module O
. O
Abstract O
Automatically O
generating O
stories O
is O
a O
challenging O
problem O
that O
requires O
producing O
causally O
related O
and O
logical O
sequences O
of O
events O
about O
a O
topic O
. O
Previous O
approaches O
in O
this O
domain O
have O
focused O
largely O
on O
one O
- O
shot O
generation O
, O
where O
a O
language O
model O
outputs O
a O
complete O
story O
based O
on O
limited O
initial O
input O
from O
a O
user O
. O
Here O
, O
we O
instead O
focus O
on O
the O
task O
of O
interactive O
story O
generation O
, O
where O
the O
user O
provides O
the O
model O
mid O
- O
level O
sentence O
abstractions O
in O
the O
form O
of O
cue O
phrases O
during O
the O
generation O
process O
. O
This O
provides O
an O
interface O
for O
human O
users O
to O
guide O
the O
story O
generation O
. O
We O
present O
two O
content O
- O
inducing O
approaches O
to O
effectively O
incorporate O
this O
additional O
information O
. O
Experimental O
results O
from O
both O
automatic O
and O
human O
evaluations O
show O
that O
these O
methods O
produce O
more O
topically O
coherent O
and O
personalized O
stories O
compared O
to O
baseline O
methods O
. O
1 O
Introduction O
Automatic O
story O
generation O
requires O
composing O
a O
coherent O
and O
Ô¨Çuent O
passage O
of O
text O
about O
a O
sequence O
of O
events O
. O
Prior O
studies O
on O
story O
generation O
mostly O
focused O
on O
symbolic O
planning O
( O
Lebowitz O
, O
1987 O
; O
P O
¬¥ O
erez O
y O
P O
¬¥ O
erez O
and O
Sharples O
, O
2001 O
; O
Porteous O
and O
Cavazza O
, O
2009 O
; O
Riedl O
and O
Young O
, O
2010 O
) O
or O
case O
- O
based O
reasoning O
( O
Gerv O
¬¥ O
as O
et O
al O
. O
, O
2005 O
) O
that O
heavily O
relied O
on O
manual O
knowledge O
engineering O
. O
Recent O
state O
- O
of O
- O
the O
- O
art O
methods O
for O
story O
generation O
( O
Martin O
et O
al O
. O
, O
2018 O
; O
Clark O
et O
al O
. O
, O
2018a O
) O
are O
based O
on O
sequence O
- O
to O
- O
sequence O
models O
( O
Sutskever O
et O
al O
. O
, O
2014 O
) O
that O
generate O
a O
story O
in O
one O
go O
. O
In O
this O
setting O
, O
the O
user O
has O
little O
control O
over O
the O
generated O
story O
. O
On O
the O
other O
hand O
, O
when O
humans O
write O
, O
they O
incrementally O
edit O
and O
reÔ¨Åne O
the O
text O
they O
produce O
. O
Motivated O
by O
this O
, O
rather O
than O
generating O
the O
entire O
story O
at O
once O
, O
we O
explore O
the O
problem O
of O
interactive O
story O
generation O
. O
In O
this O
setup O
, O
a O
user O
can O
provide O
Figure O
1 O
: O
Interactive O
story O
generation O
: O
the O
user O
inputs O
the O
Ô¨Årst O
sentence O
of O
the O
story O
( O
prompt O
) O
, O
and O
provides O
guiding O
cue O
phrases O
as O
the O
system O
generates O
the O
story O
one O
sentence O
at O
a O
time O
. O
the O
model O
mid O
- O
level O
sentence O
abstractions O
in O
the O
form O
of O
cue O
phrases O
as O
the O
story O
is O
being O
generated O
. O
Cue O
phrases O
enable O
the O
user O
to O
inform O
the O
system O
of O
what O
they O
want O
to O
happen O
next O
in O
the O
story O
and O
have O
more O
control O
over O
what O
is O
being O
generated O
. O
To O
achieve O
our O
goal O
, O
this O
paper O
primarily O
focuses O
on O
approaches O
for O
smoothly O
and O
effectively O
incorporating O
user O
- O
provided O
cues O
. O
The O
schematic O
in O
Fig O
. O
1 O
illustrates O
this O
scenario O
: O
the O
system O
generates O
the O
story O
one O
sentence O
at O
a O
time O
, O
and O
the O
user O
guides O
the O
content O
of O
the O
next O
sentence O
using O
cue O
phrases O
. O
We O
note O
that O
the O
generated O
sentences O
need O
to O
Ô¨Åt O
the O
context O
, O
and O
also O
be O
semantically O
related O
to O
the O
provided O
cue O
phrase O
. O
A O
fundamental O
advantage O
of O
using O
this O
framework O
as O
opposed O
to O
a O
fully O
automated O
one O
is O
that O
it O
can O
provide O
an O
interactive O
interface O
for O
human O
users O
to O
incrementally O
supervise O
the O
generation O
by O
giving O
signals O
to O
the O
model O
throughout O
the O
story O
generation O
process O
. O
This O
human O
- O
computer O
collaboration O
can O
result O
in O
generating O
richer O
and O
personalized O
stories O
. O
In O
particular O
, O
this O
Ô¨Åeld O
of O
research O
can O
be O
used O
in O
addressing O
the O
literacy O
needs O
of O
learners O
with O
disabilities O
and O
enabling O
children O
to O
explore588creative O
writing O
at O
an O
early O
age O
by O
crafting O
their O
own O
stories O
. O
In O
this O
paper O
, O
we O
present O
two O
content O
- O
inducing O
approaches O
based O
on O
the O
Transformer O
Network O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
for O
interactively O
incorporating O
external O
knowledge O
when O
automatically O
generating O
stories O
. O
Here O
, O
our O
external O
knowledge O
is O
in O
the O
form O
of O
cue O
phrases O
provided O
by O
the O
user O
to O
enable O
interaction O
, O
but O
can O
readily O
be O
replaced O
with O
knowledge O
accessible O
through O
other O
means1 O
. O
SpeciÔ¨Åcally O
, O
our O
models O
fuse O
information O
from O
the O
story O
context O
and O
cue O
phrases O
through O
a O
hierarchical O
attention O
mechanism O
. O
The O
Ô¨Årst O
approach O
, O
Cued O
Writer O
, O
employs O
two O
independent O
encoders O
( O
for O
incorporating O
context O
and O
cue O
phrases O
) O
and O
an O
additional O
attention O
component O
to O
capture O
the O
semantic O
agreement O
between O
the O
cue O
phrase O
and O
output O
sentence O
. O
The O
second O
approach O
, O
Relevance O
Cued O
Writer O
, O
additionally O
measures O
the O
relatedness O
between O
the O
context O
and O
cue O
phrase O
through O
a O
contextcue O
multi O
- O
head O
unit O
. O
In O
both O
cases O
, O
we O
introduce O
different O
attention O
units O
in O
a O
single O
end O
- O
to O
- O
end O
neural O
network O
. O
Our O
automatic O
and O
human O
evaluations O
demonstrate O
that O
the O
presented O
models O
outperform O
strong O
baselines O
and O
can O
successfully O
incorporate O
cues O
in O
generated O
stories O
. O
This O
capability O
is O
one O
step O
closer O
to O
an O
interactive O
setup O
, O
and O
unlike O
one O
- O
shot O
generation O
, O
it O
lets O
users O
have O
more O
control O
over O
the O
generation O
. O
Our O
contributions O
are O
twofold O
: O
‚Ä¢Two O
novel O
content O
- O
inducing O
approaches O
to O
incorporate O
additional O
information O
, O
in O
this O
case O
cue O
phrases O
, O
into O
the O
generation O
phase O
. O
‚Ä¢Experiments O
demonstrating O
utility O
of O
contentinducing O
approaches O
using O
automatic O
and O
human O
evaluations O
. O
2 O
Related O
Work O
Automatic O
story O
generation O
is O
a O
longstanding O
problem O
in O
AI O
, O
with O
early O
work O
dating O
back O
to O
the O
1970s O
based O
on O
symbolic O
planning O
( O
Lebowitz O
, O
1987 O
; O
P¬¥erez O
y O
P O
¬¥ O
erez O
and O
Sharples O
, O
2001 O
; O
Porteous O
and O
Cavazza O
, O
2009 O
; O
Riedl O
and O
Young O
, O
2010 O
) O
and O
casebased O
reasoning O
using O
ontologies O
( O
Gerv O
¬¥ O
as O
et O
al O
. O
, O
2005 O
) O
. O
Li O
et O
al O
. O
( O
2013 O
) O
extended O
prior O
works O
toward O
learning O
domain O
models O
( O
via O
corpus O
and/or O
crowdsourcing O
) O
to O
support O
open O
story O
generation O
about O
any O
topic O
. O
1For O
example O
, O
the O
user O
- O
provided O
cues O
can O
be O
replaced O
by O
the O
outputs O
of O
an O
automatic O
planner O
. O
Our O
models O
are O
Ô¨Çexible O
enough O
to O
work O
in O
other O
setups O
. O
With O
the O
advent O
of O
deep O
learning O
there O
has O
been O
a O
major O
shift O
towards O
using O
seq2seq O
models O
( O
Sutskever O
et O
al O
. O
, O
2014 O
; O
Bahdanau O
et O
al O
. O
, O
2015 O
) O
for O
various O
text O
generation O
tasks O
, O
including O
storytelling O
( O
Roemmele O
, O
2016 O
; O
Jain O
et O
al O
. O
, O
2017 O
; O
Hu O
et O
al O
. O
, O
2020 O
) O
. O
However O
, O
these O
models O
often O
fail O
to O
ensure O
coherence O
in O
the O
generated O
story O
. O
To O
address O
this O
problem O
, O
Clark O
et O
al O
. O
( O
2018a O
) O
incorporated O
entities O
given O
their O
vector O
representations O
, O
which O
get O
updated O
as O
the O
story O
unfolds O
. O
Similarly O
, O
Liu O
et O
al O
. O
( O
2020 O
) O
proposed O
a O
character O
- O
centric O
story O
generation O
by O
learning O
character O
embeddings O
directly O
from O
the O
corpus O
. O
Fan O
et O
al O
. O
( O
2018 O
) O
followed O
a O
twostep O
process O
to O
Ô¨Årst O
generate O
the O
premise O
and O
then O
condition O
on O
that O
to O
generate O
the O
story O
. O
Yu O
et O
al O
. O
( O
2020 O
) O
proposed O
a O
multi O
- O
pass O
CV O
AE O
to O
improve O
wording O
diversity O
and O
content O
consistency O
. O
Previous O
work O
has O
explored O
the O
potential O
of O
creative O
writing O
with O
a O
machine O
in O
the O
loop O
. O
Clark O
et O
al O
. O
( O
2018b O
) O
found O
that O
people O
generally O
enjoy O
collaborating O
with O
a O
machine O
. O
Traditional O
methods O
proposed O
to O
write O
stories O
collaboratively O
using O
a O
case O
- O
based O
reasoning O
architecture O
( O
Swanson O
and O
Gordon O
, O
2012 O
) O
. O
Recent O
work O
( O
Roemmele O
and O
Gordon O
, O
2015 O
) O
extended O
this O
to O
Ô¨Ånd O
relevant O
suggestions O
for O
the O
next O
sentence O
in O
a O
story O
from O
a O
large O
corpus O
. O
Other O
methods O
proposed O
GUI O
and O
tools O
to O
facilitate O
co O
- O
creative O
narrative O
generation O
( O
Manjavacas O
et O
al O
. O
, O
2017 O
; O
Kapadia O
et O
al O
. O
, O
2015 O
) O
. O
Unlike O
us O
, O
these O
approaches O
explore O
the O
value O
of O
and O
tools O
for O
interaction O
rather O
than O
designing O
methods O
for O
incorporating O
user O
input O
into O
the O
model O
. O
Another O
line O
of O
research O
decomposes O
story O
generation O
into O
two O
steps O
: O
story O
plot O
planning O
and O
plot O
- O
to O
- O
surface O
generation O
. O
Previous O
work O
produces O
story O
- O
plans O
based O
on O
sequences O
of O
events O
( O
Martin O
et O
al O
. O
, O
2018 O
; O
Tambwekar O
et O
al O
. O
, O
2019 O
; O
Ammanabrolu O
et O
al O
. O
, O
2020 O
) O
, O
critical O
phrases O
( O
Xu O
et O
al O
. O
, O
2018 O
) O
or O
both O
events O
and O
entities O
( O
Fan O
et O
al O
. O
, O
2019 O
) O
. O
Yao O
et O
al O
. O
( O
2019 O
) O
model O
the O
story O
- O
plan O
as O
a O
sequence O
of O
keywords O
. O
They O
proposed O
Static O
andDynamic O
paradigms O
that O
generate O
a O
story O
based O
on O
these O
story O
- O
plans O
. O
Goldfarb O
- O
Tarrant O
et O
al O
. O
( O
2019 O
) O
adopted O
thestatic O
model O
proposed O
in O
Yao O
et O
al O
. O
( O
2019 O
) O
to O
supervise O
story O
- O
writing O
. O
A O
major O
focus O
of O
these O
works O
is O
on O
generating O
a O
coherent O
plan O
for O
generating O
the O
story O
. O
In O
contrast O
, O
our O
contribution O
is O
complementary O
since O
we O
do O
not O
focus O
on O
planning O
but O
on O
generation O
. O
We O
present O
approaches O
to O
effectively O
incorporate O
external O
knowledge O
in O
the O
form O
of O
cue O
- O
phrases O
during589Figure O
2 O
: O
Overall O
model O
architecture O
for O
Cued O
Writer O
andRelevance O
Cued O
Writer O
. O
generation O
, O
and O
conduct O
extensive O
experiments O
to O
compare O
our O
models O
with O
those O
of O
Yao O
et O
al O
. O
( O
2019 O
) O
by O
modifying O
them O
to O
work O
in O
our O
setup O
. O
3 O
Interactive O
Story O
Generation O
We O
design O
models O
to O
generate O
a O
story O
one O
sentence O
at O
a O
time O
. O
Given O
the O
generated O
context O
so O
far O
( O
as O
a O
sequence O
of O
tokens O
) O
X={x1, O
... O
,xT O
} O
, O
and O
the O
cue O
phrase O
for O
the O
next O
sentence O
c={c1, O
... O
,cK O
} O
, O
our O
models O
generate O
the O
tokens O
of O
the O
next O
sentence O
of O
the O
storyY={y1, O
... O
,yM O
} O
. O
We O
train O
the O
models O
by O
minimizing O
the O
cross O
- O
entropy O
loss O
: O
LŒ∏=‚àíM O
/ O
summationdisplay O
i=1logP(yi|X O
, O
c O
, O
Œ∏ O
) O
( O
1 O
) O
Here O
, O
Œ∏refers O
to O
model O
parameters O
. O
Note O
that O
when O
generating O
the O
n O
- O
th O
sentence O
, O
the O
model O
takes O
the O
Ô¨Årstn‚àí1sentences O
in O
the O
story O
as O
the O
context O
along O
with O
the O
cue O
phrase O
. O
In O
the O
rest O
of O
this O
section O
, O
we O
describe O
our O
two O
novel O
content O
- O
inducing O
approaches O
for O
addressing O
the O
interactive O
story O
generation O
task O
: O
the O
Cued O
Writer O
, O
and O
the O
Relevance O
Cued O
Writer O
. O
These O
models O
share O
an O
overall O
encoder O
- O
decoder O
based O
architecture O
shown O
in O
Fig O
. O
2 O
. O
They O
adopt O
a O
dual O
encoding O
approach O
where O
two O
separate O
but O
architecturally O
similar O
encoders O
are O
used O
for O
encoding O
the O
context O
( O
Context O
Encoder O
represented O
in O
the O
green O
box O
) O
and O
the O
cue O
phrase O
( O
Cue O
Encoder O
represented O
in O
the O
purple O
box O
) O
. O
Both O
these O
encoders O
Figure O
3 O
: O
( O
a)Encoder O
Block O
consists O
of O
MultiHead O
and O
FFN O
. O
( O
b)MultiHead O
Attention O
. O
( O
c)Attention O
Module O
. O
advise O
the O
Decoder O
( O
represented O
in O
the O
blue O
box O
) O
, O
which O
in O
turn O
generates O
the O
next O
sentence O
. O
The O
two O
proposed O
models O
use O
the O
same O
encoding O
mechanism O
( O
described O
in O
¬ß O
3.1 O
) O
and O
differ O
only O
in O
their O
decoders O
( O
described O
in O
¬ß O
3.2 O
) O
. O
3.1 O
Encoder O
Our O
models O
use O
the O
Transformer O
encoder O
introduced O
in O
Vaswani O
et O
al O
. O
( O
2017 O
) O
. O
Here O
, O
we O
provide O
a O
generic O
description O
of O
the O
encoder O
architecture O
followed O
by O
the O
inputs O
to O
this O
architecture O
for O
the O
Context O
and O
Cue O
Encoders O
in O
our O
models O
. O
Each O
encoder O
layer O
lcontains O
architecturally O
identical O
Encoder O
Blocks O
, O
referred O
to O
as O
ENCBLOCK O
( O
with O
unique O
trainable O
parameters O
) O
. O
Fig O
. O
3(a O
) O
shows O
an O
Encoder O
Block O
which O
consists O
of O
a O
Multi O
- O
Head O
attention O
and O
an O
FFN O
that O
applies O
the O
following O
operations O
: O
Àúol= O
M O
ULTIHEAD(hl‚àí1 O
) O
( O
2a O
) O
ol= O
LAYER O
NORM(Àúol+hl‚àí1 O
) O
( O
2b O
) O
Àúhl= O
FFN(ol O
) O
( O
2c O
) O
hl= O
LAYER O
NORM(Àúhl+ol O
) O
( O
2d O
) O
Where O
MULTI O
HEAD O
represents O
Multi O
- O
Head O
Attention O
( O
described O
below O
) O
, O
FFN O
is O
a O
feed O
- O
forward O
neural O
network O
with O
ReLU O
activation O
( O
LeCun O
et O
al O
. O
, O
2015 O
) O
, O
and O
LAYER O
NORM O
is O
a O
layer O
normalization O
( O
Ba O
et O
al O
. O
, O
2016 O
) O
. O
In O
the O
rest O
of O
the O
paper O
, O
LAYER O
NORM O
( O
also O
shown O
as O
Add O
& O
Norm O
in O
Ô¨Ågures O
) O
is O
always O
applied O
after O
MULTI O
HEAD O
and O
FFN O
, O
but O
we O
do O
not O
explicitly O
mention O
that O
in O
text O
or O
equations O
for O
simplicity O
. O
Multi O
- O
Head O
Attention O
The O
multi O
- O
head O
attention O
, O
shown O
in O
Fig O
. O
3(b O
) O
, O
is O
similar O
to O
that O
used O
in O
Vaswani O
et O
al O
. O
( O
2017 O
) O
. O
It O
is O
made O
of O
multiple O
Attention O
heads O
, O
shown O
in O
Fig O
. O
3(c O
) O
. O
The O
Attention O
head O
has O
three O
types O
of O
inputs O
: O
the O
query O
sequence,590Q‚ààRnq√ódk O
, O
the O
key O
sequence O
, O
K‚ààRnk√ódk O
, O
and O
the O
value O
sequence O
, O
V‚ààRnv√ódk O
. O
The O
attention O
module O
takes O
each O
token O
in O
the O
query O
sequence O
and O
attends O
to O
tokens O
in O
the O
key O
sequence O
using O
a O
scaled O
dot O
product O
. O
The O
score O
for O
each O
token O
in O
the O
key O
sequence O
is O
then O
multiplied O
by O
the O
corresponding O
value O
vector O
to O
form O
a O
weighted O
sum O
: O
ATTN(Q O
, O
K O
, O
V O
) O
= O
softmax O
/ O
parenleftbiggQKT O
‚àödk O
/ O
parenrightbigg O
V O
( O
3 O
) O
For O
each O
head O
, O
all O
Q O
, O
K O
, O
andVare O
passed O
through O
a O
head O
- O
speciÔ¨Åc O
projection O
prior O
to O
the O
attention O
being O
computed O
. O
The O
output O
of O
a O
single O
head O
is O
: O
Hi= O
A O
TTN(QWQ O
i O
, O
KWK O
i O
, O
VWV O
i O
) O
( O
4 O
) O
WhereWs O
are O
head O
- O
speciÔ¨Åc O
projections O
. O
Attention O
headsHiare O
then O
concatenated O
: O
MULTIH(Q O
, O
K O
, O
V O
) O
= O
[ O
Hi; O
... O
;Hm]WO(5 O
) O
WhereWOis O
an O
output O
projection O
. O
In O
the O
encoder O
, O
all O
query O
, O
key O
, O
and O
value O
come O
from O
the O
previous O
layer O
and O
thus O
: O
MULTIHEAD(hl‚àí1 O
) O
= O
M O
ULTIH(hl‚àí1,hl‚àí1,hl‚àí1 O
) O
( O
6 O
) O
Encoder O
Input O
The O
Encoder O
Blocks O
described O
above O
form O
the O
constituent O
units O
of O
the O
Context O
and O
Cue O
Encoders O
, O
which O
process O
the O
context O
and O
cue O
phrase O
respectively O
. O
Each O
token O
in O
the O
context O
, O
xi O
, O
and O
cue O
phrase O
, O
ci O
, O
is O
assigned O
two O
kinds O
of O
embeddings O
: O
token O
embeddings O
indicating O
the O
meaning O
andposition O
embeddings O
indicating O
the O
position O
of O
each O
token O
within O
the O
sequence O
. O
These O
two O
are O
summed O
to O
obtain O
individual O
input O
vectors O
, O
X0 O
, O
and O
c0 O
, O
which O
are O
then O
fed O
to O
the O
Ô¨Årst O
layer O
of O
Context O
and O
Cue O
encoders O
, O
respectively O
. O
Thereafter O
, O
new O
representations O
are O
constructed O
through O
layers O
of O
encoder O
blocks O
: O
Xl+1= O
ENCBLOCK O
( O
Xl O
, O
Xl O
, O
Xl O
) O
( O
7a O
) O
cl+1= O
ENCBLOCK O
( O
cl O
, O
cl O
, O
cl O
) O
( O
7b O
) O
wherel‚àà[0,L‚àí1]denotes O
different O
layers O
. O
In O
Eqn O
. O
7a O
and O
7b O
, O
the O
output O
of O
the O
previous O
layer O
‚Äôs O
Encoder O
Block O
is O
used O
as O
Q O
, O
K O
, O
andVinput O
for O
the O
multi O
- O
head O
attention O
of O
the O
next O
block O
. O
3.2 O
Content O
- O
Inducing O
Decoders O
We O
now O
describe O
the O
decoders O
for O
our O
models O
. O
Cued O
Writer O
The O
main O
intuition O
behind O
our O
Ô¨Årst O
model O
, O
Cued O
Writer O
, O
is O
that O
since O
cue O
phrases O
( O
a)Cued O
Writer O
( O
b)Rel O
. O
Cued O
Writer O
Figure O
4 O
: O
Decoder O
architectures O
. O
XLandcLare O
the O
outputs O
of O
the O
top O
- O
layers O
of O
the O
Context O
and O
Cue O
encoders O
respectively O
, O
and O
KandVare O
the O
corresponding O
keys O
and O
values O
. O
indicate O
users O
‚Äô O
expectations O
of O
what O
they O
want O
to O
see O
in O
the O
next O
sentence O
of O
the O
story O
, O
they O
should O
be O
used O
by O
the O
model O
at O
the O
time O
of O
generation O
, O
i.e. O
, O
in O
the O
decoder O
. O
Below O
, O
we O
describe O
the O
decoder O
used O
by O
the O
Cued O
Writer O
. O
After O
processing O
the O
two O
types O
of O
inputs O
in O
the O
Context O
and O
Cue O
Encoders O
, O
the O
model O
includes O
their O
Ô¨Ånal O
encoded O
representations O
( O
XLandcL O
) O
in O
the O
decoder O
. O
The O
decoder O
consists O
of O
Llayers O
with O
architecturally O
identical O
Decoder O
Blocks O
. O
Each O
Decoder O
Block O
contains O
Enc O
- O
Dec O
MultiHead O
and O
the O
Cue O
MultiHead O
units O
( O
see O
Fig O
. O
4(a O
) O
) O
, O
which O
let O
the O
decoder O
to O
focus O
on O
the O
relevant O
parts O
of O
the O
context O
and O
the O
cue O
phrase O
, O
respectively O
. O
GivenY0as O
the O
word O
- O
level O
embedding O
represen-591tation O
for O
the O
output O
sentence O
, O
our O
Decoder O
Block O
is O
formulated O
as O
: O
Yl+1 O
self= O
M O
ULTIH(Yl O
, O
Yl O
, O
Yl O
) O
( O
8a O
) O
Yl+1 O
dec= O
M O
ULTIH(Yl+1 O
self O
, O
XL O
, O
XL O
) O
( O
8b O
) O
Yl+1 O
cued= O
M O
ULTIH(Yl+1 O
self O
, O
cL O
, O
cL O
) O
( O
8c O
) O
Eqn O
. O
8a O
is O
standard O
self O
- O
attention O
, O
which O
measures O
the O
intra O
- O
sentence O
agreement O
for O
the O
output O
sentence O
and O
corresponds O
to O
the O
MultiHead O
unit O
in O
Fig O
. O
4(a O
) O
. O
Eqn O
. O
8b O
, O
describing O
the O
Enc O
- O
Dec O
MultiHead O
unit O
, O
measures O
the O
agreement O
between O
context O
and O
output O
sentence O
, O
where O
queries O
come O
from O
the O
decoder O
Multi O
- O
Head O
unit O
( O
Yself O
) O
, O
and O
the O
keys O
and O
values O
come O
from O
the O
top O
layer O
of O
the O
context O
encoder O
( O
XL O
) O
. O
Similarly O
, O
Eqn O
. O
8c O
captures O
the O
agreement O
between O
output O
sentence O
and O
cue O
phrase O
through O
Cue O
MultiHead O
unit O
. O
Here O
, O
keys O
and O
values O
come O
from O
the O
top O
layer O
of O
the O
Cue O
encoder O
( O
cL O
) O
. O
Lastly O
, O
we O
adapt O
a O
gating O
mechanism O
( O
Sriram O
et O
al O
. O
, O
2018 O
) O
to O
integrate O
the O
semantic O
representations O
from O
both O
YdecandYcuedand O
pass O
the O
resulting O
output O
to O
FFN O
function O
: O
gl+1 O
= O
œÉ(W1[Yl+1 O
dec;Yl+1 O
cued O
] O
) O
( O
9a O
) O
Yl+1 O
int O
= O
W2(gl+1 O
‚ó¶ O
[Yl+1 O
dec;Yl+1 O
cued O
] O
) O
( O
9b O
) O
Yl+1= O
FFN(Yl+1 O
int O
) O
( O
9c O
) O
the O
representation O
from O
YdecandYcuedare O
concatenated O
to O
learn O
gates O
, O
g. O
The O
gated O
hidden O
layers O
are O
combined O
by O
concatenation O
and O
followed O
by O
a O
linear O
projection O
with O
the O
weight O
matrix O
W2 O
. O
Relevance O
Cued O
Writer O
The O
decoder O
of O
Cued O
Writer O
described O
above O
captures O
the O
relatedness O
of O
the O
context O
and O
the O
cue O
phrase O
to O
the O
generated O
sentence O
but O
does O
not O
study O
the O
relatedness O
or O
relevance O
of O
the O
cue O
phrase O
to O
the O
context O
. O
We O
incorporate O
this O
relevance O
in O
the O
decoder O
of O
our O
next O
model O
, O
Relevance O
Cued O
Writer O
. O
Its O
Decoder O
Block O
( O
shown O
in O
Fig O
. O
4(b O
) O
) O
is O
similar O
to O
that O
of O
Cued O
Writer O
except O
for O
two O
additional O
units O
: O
the O
Context O
- O
Cue O
and O
Relevance O
MultiHead O
units O
. O
The O
intuition O
behind O
theContext O
- O
Cue O
MultiHead O
unit O
( O
Eqn O
. O
10a O
) O
is O
to O
characterize O
the O
relevance O
between O
the O
context O
and O
the O
cue O
phrase O
, O
so O
as O
to O
highlight O
the O
effect O
of O
words O
in O
the O
cue O
phrase O
that O
are O
more O
relevant O
to O
the O
context O
thereby O
promoting O
topicality O
and O
Ô¨Çuency O
. O
This O
relevance O
is O
then O
provided O
to O
the O
decoder O
using O
theRelevance O
MultiHead O
unit O
( O
Eqn O
. O
10b O
): O
Xl+1 O
rel= O
M O
ULTIH(XL O
, O
cL O
, O
cL O
) O
( O
10a O
) O
Yl+1 O
rel= O
M O
ULTIH(Yl+1 O
self O
, O
Xl+1 O
rel O
, O
Xl+1 O
rel)(10b)We O
fuse O
the O
information O
from O
all O
three O
sources O
using O
a O
gating O
mechanism O
and O
pass O
the O
result O
to O
FFN O
: O
gl+1 O
= O
œÉ(W1[Yl+1 O
dec;Yl+1 O
cued;Yl+1 O
rel O
] O
) O
( O
11a O
) O
Yl+1 O
int O
= O
W2(gl+1 O
‚ó¶ O
[Yl+1 O
dec;Yl+1 O
cued;Yl+1 O
rel])(11b O
) O
Yl+1= O
FFN(Yl+1 O
int O
) O
( O
11c O
) O
Finally O
, O
for O
both O
models O
, O
a O
linear O
transformation O
and O
a O
softmax O
function O
( O
shown O
in O
Fig O
. O
2 O
) O
is O
applied O
to O
convert O
the O
output O
produced O
by O
the O
stack O
of O
decoders O
to O
predicted O
next O
- O
token O
probabilities O
: O
P(yi|y O
< O
i O
, O
X O
, O
c O
, O
Œ∏ O
) O
= O
softmax O
( O
YL O
iWy)(12 O
) O
whereP(yi|y O
< O
i O
, O
X O
, O
c O
, O
Œ∏ O
) O
is O
the O
likelihood O
of O
generatingyigiven O
the O
preceding O
text O
( O
y O
< O
i O
) O
, O
context O
and O
cue O
, O
andWyis O
the O
token O
embedding O
matrix O
. O
4 O
Empirical O
Evaluation O
4.1 O
Dataset O
We O
used O
the O
ROCStories O
corpus O
( O
Mostafazadeh O
et O
al O
. O
, O
2016 O
) O
for O
experiments O
. O
It O
contains O
98,161 O
Ô¨Åve O
- O
sentence O
long O
stories O
with O
a O
rich O
set O
of O
causal O
/ O
temporal O
sequences O
of O
events O
. O
We O
held O
out O
10 O
% O
of O
stories O
for O
validation O
and O
10 O
% O
for O
test O
set O
. O
4.2 O
Baselines O
SEQ2SEQ O
Our O
Ô¨Årst O
baseline O
is O
based O
on O
a O
LSTM O
sentence O
- O
to O
- O
sentence O
generator O
with O
attention O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
) O
. O
In O
order O
to O
incorporate O
userprovided O
cue O
phrases O
, O
we O
concatenate O
context O
and O
cue O
phrase O
with O
a O
delimiter O
token O
( O
< O
$ O
> O
) O
before O
passing O
it O
to O
the O
encoder O
. O
DYNAMIC O
This O
is O
the O
Dynamic O
model O
proposed O
by O
Yao O
et O
al O
. O
( O
2019 O
) O
modiÔ¨Åed O
to O
work O
in O
our O
setting O
. O
For O
a O
fair O
comparison O
, O
instead O
of O
generating O
a O
plan O
, O
we O
provide O
the O
model O
with O
cue O
phrases O
and O
generate O
the O
story O
one O
sentence O
at O
a O
time O
. O
STATIC O
TheSTATIC O
model O
( O
Yao O
et O
al O
. O
, O
2019 O
) O
gets O
all O
cue O
phrases O
at O
once O
to O
generate O
the O
entire O
story2 O
. O
By O
design O
, O
it O
has O
additional O
access O
to O
all O
, O
including O
future O
, O
cue O
phrases O
. O
Our O
models O
and O
other O
baselines O
do O
not O
have O
this O
information O
. O
VANILLA O
To O
verify O
the O
effectiveness O
of O
our O
content O
- O
inducing O
approaches O
, O
we O
use O
a O
Vanilla O
Transformer O
as O
another O
baseline O
and O
concatenate O
context O
and O
cue O
phrase O
using O
a O
delimiter O
token O
. O
2We O
used O
the O
implementation O
available O
at O
: O
https:// O
bitbucket.org/VioletPeng/language-model/592Models O
PPL O
( O
‚Üì)BLEU-1 O
( O
‚Üë)BLEU-2 O
( O
‚Üë)BLEU-3 O
( O
‚Üë)GM O
( O
‚Üë)Repetition-4 O
( O
‚Üì O
) O
DYNAMIC O
( O
Yao O
et O
al O
. O
, O
2019 O
) O
29.49 O
30.05 O
9.16 O
4.59 O
0.73 O
44.36 O
STATIC O
( O
Yao O
et O
al O
. O
, O
2019 O
) O
20.81 O
33.25 O
9.64 O
4.77 O
0.75 O
26.26 O
SEQ2SEQ O
20.97 O
33.91 O
10.01 O
3.09 O
0.82 O
33.23 O
VANILLA O
15.78 O
40.30 O
16.09 O
7.19 O
0.89 O
20.87 O
Cued O
Writer O
14.80 O
41.50 O
16.72 O
7.25 O
0.92 O
15.08 O
Rel O
. O
Cued O
Writer O
14.66 O
42.65 O
17.33 O
7.59 O
0.94 O
16.23 O
Table O
1 O
: O
Automatic O
evaluation O
results O
. O
Our O
models O
outperform O
all O
baselines O
across O
all O
metrics O
( O
p<0.05 O
) O
. O
4.3 O
Training O
details O
Following O
previous O
work O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
we O
initialize O
context O
encoders O
and O
decoders O
with O
6layers O
( O
512dimensional O
states O
and O
8attention O
heads O
) O
. O
Our O
models O
contain O
3 O
- O
layer O
encoders O
for O
encoding O
cue O
phrases O
( O
all O
other O
speciÔ¨Åcations O
are O
the O
same O
) O
. O
For O
the O
position O
- O
wise O
feed O
- O
forward O
networks O
, O
we O
use O
2048 O
dimensional O
inner O
states O
. O
We O
use O
the O
Adam O
optimizer O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
with O
a O
learning O
rate O
of O
0.0001 O
and O
residual O
, O
embedding O
, O
and O
attention O
dropouts O
with O
a O
rate O
of O
0.1 O
for O
regularization O
. O
Models O
are O
implemented O
in O
PyTorch O
, O
trained O
for O
30epochs O
with O
early O
stopping O
on O
validation O
loss O
. O
Cue O
- O
phrases O
for O
Training O
and O
Automatic O
Evaluation O
: O
For O
training O
all O
models O
, O
we O
need O
cue O
phrases O
, O
which O
are O
, O
in O
principle O
, O
to O
be O
entered O
by O
a O
user O
. O
However O
, O
to O
scale O
model O
training O
, O
we O
automatically O
extracted O
cue O
phrases O
from O
the O
target O
sentences O
in O
the O
training O
set O
using O
the O
previously O
proposed O
RAKE O
algorithm O
( O
Rose O
et O
al O
. O
, O
2010 O
) O
. O
It O
is O
important O
to O
note O
that O
cue O
phrases O
can O
represent O
a O
variety O
of O
information O
, O
and O
many O
other O
methods O
can O
be O
used O
to O
extract O
them O
for O
training O
purposes O
. O
For O
example O
, O
topic O
words O
, O
distinctive O
entities O
or O
noun O
phrases O
in O
the O
sentence O
, O
the O
headword O
in O
the O
dependency O
parse O
of O
the O
sentence O
, O
etc O
. O
Our O
automatic O
evaluations O
were O
done O
on O
a O
largescale O
, O
and O
so O
we O
followed O
a O
similar O
approach O
for O
extracting O
cue O
- O
phrases O
. O
Cue O
- O
phrases O
for O
Human O
Evaluation O
: O
In O
the O
interest O
of O
evaluating O
the O
interactive O
nature O
of O
our O
models O
, O
cue O
- O
phrases O
were O
provided O
manually O
during O
our O
interactive O
evaluations3 O
. O
General O
Statistics O
on O
Cue O
- O
phrases O
: O
Automatically O
extracted O
cue O
phrases O
has O
the O
vocabulary O
size O
of22,097 O
, O
and O
6,189on O
the O
train O
and O
test O
set O
, O
respectively O
with O
the O
average O
10 O
% O
coverage O
over O
the O
entire O
target O
sentence O
. O
Cue O
- O
phrases O
are O
typically O
1 O
- O
2 O
words O
. O
Comparing O
user O
- O
provided O
vs O
automati3We O
left O
the O
deÔ¨Ånition O
of O
cue O
- O
phrase O
open O
- O
ended O
to O
enable O
Ô¨Çexibility O
in O
user O
interaction O
. O
They O
are O
typically O
1 O
- O
2 O
words O
. O
Figure O
5 O
: O
Inter O
- O
story O
( O
left O
) O
and O
Intra O
- O
story O
( O
right O
) O
repetition O
scores O
. O
The O
proposed O
models O
have O
better O
scores O
. O
cally O
extracted O
cue O
- O
phrases O
, O
the O
average O
length O
of O
user O
- O
provided O
cue O
- O
phrases O
in O
interactive O
evaluation O
is1.56 O
, O
with O
a O
vocabulary O
size O
of O
206 O
, O
whereas O
these O
numbers O
are O
1.59and214for O
their O
corresponding O
automatically O
extracted O
cue O
phrases O
. O
4.4 O
Automatic O
Evaluation O
Following O
previous O
credible O
works O
( O
Martin O
et O
al O
. O
, O
2018 O
; O
Fan O
et O
al O
. O
, O
2018 O
) O
, O
we O
compare O
various O
methods O
using O
Perplexity O
and O
BLEU O
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
on O
the O
test O
set O
. O
We O
reported O
BLEU O
- O
n O
for O
n=1,2,3 O
. O
From O
Table O
1 O
, O
we O
can O
see O
that O
both O
our O
models O
outperform O
DYNAMIC O
andSTATIC O
by O
large O
margins O
on O
perplexity O
and O
BLEU O
scores O
. O
The O
proposed O
models O
are O
also O
superior O
to O
the O
SEQ2SEQ O
andVANILLA O
baseline O
on O
both O
measures O
. O
Comparing O
the O
last O
two O
rows O
of O
Table O
1 O
, O
we O
also O
see O
an O
additive O
gain O
from O
modeling O
the O
relevance O
in O
Rel O
. O
Cued O
Writer O
. O
All O
improvements O
are O
statistically O
signiÔ¨Åcant O
( O
approximate O
randomization O
( O
Noreen O
, O
1989),p<0.05 O
) O
. O
To O
evaluate O
how O
well O
the O
story O
generation O
model O
incorporates O
the O
cues O
, O
we O
use O
an O
embedding O
- O
based O
greedy O
matching O
score O
( O
GM O
) O
( O
Liu O
et O
al O
. O
, O
2016 O
) O
. O
The O
score O
measures O
the O
relatedness O
of O
the O
generated O
story O
with O
cues O
by O
greedily O
matching O
them O
with O
each O
token O
in O
a O
story O
based O
on O
the O
cosine O
similarity O
of O
their O
word O
embeddings O
( O
Yao O
et O
al O
. O
, O
2019 O
) O
. O
We O
can O
see O
from O
the O
5th O
column O
in O
Table O
1 O
that O
our O
models O
generate O
stories O
that O
are O
more O
related O
to O
the O
cue O
phrases.593Prompt O
( O
Ô¨Årst O
sentence O
): O
Jordan O
was O
watching O
TV O
on O
her O
couch O
. O
Cue O
phrases O
: O
watch O
football O
- O
change O
channel O
- O
comedy O
show O
- O
very O
funny O
She O
was O
trying O
to O
watch O
football O
on O
TV O
. O
Then O
she O
went O
to O
change O
channel O
. O
Finally O
, O
she O
decided O
to O
watch O
a O
comedy O
show O
. O
She O
saw O
the O
comedy O
that O
was O
playing O
and O
did O
n‚Äôt O
like O
. O
Cue O
phrases O
: O
soccer O
- O
cook O
- O
order O
pizza O
- O
tasty O
dinner O
Her O
brother O
was O
playing O
in O
a O
soccer O
. O
She O
was O
n‚Äôt O
able O
to O
cook O
. O
Instead O
, O
she O
ordered O
pizza O
. O
Her O
brother O
was O
happy O
with O
the O
tasty O
dinner O
. O
Table O
2 O
: O
Example O
of O
stories O
generated O
in O
interactive O
evaluation O
using O
two O
models O
given O
the O
same O
prompt O
and O
different O
set O
of O
cue O
- O
phrase O
. O
Previous O
works O
have O
shown O
that O
neural O
generation O
models O
suffer O
from O
repetition O
issue O
; O
and O
so O
we O
additionally O
evaluate O
the O
models O
using O
repetition-4 O
which O
measures O
the O
percentage O
of O
generated O
stories O
that O
repeat O
at O
least O
one O
4 O
- O
gram O
( O
Shao O
et O
al O
. O
, O
2019 O
) O
and O
inter- O
and O
intra O
- O
story O
repetition O
scores O
( O
Yao O
et O
al O
. O
, O
2019 O
) O
. O
A O
lower O
value O
is O
better O
for O
these O
scores O
. O
The O
result O
of O
repetition-4 O
is O
reported O
in O
the O
last O
column O
of O
Table O
1 O
. O
The O
proposed O
models O
signiÔ¨Åcantly O
outperform O
all O
baselines O
, O
and O
among O
the O
twoCued O
Writer O
is O
better O
. O
Inter O
and O
intra O
repetition O
scores O
are O
depicted O
in O
Fig O
. O
5 O
. O
Our O
two O
proposed O
models O
are O
almost O
comparable O
on O
these O
metrics O
but O
they O
show O
a O
general O
superior O
performance O
compared O
to O
all O
baselines O
. O
In O
particular O
, O
Rel O
. O
Cued O
Writer O
achieves O
a O
signiÔ¨Åcant O
performance O
increase O
of16 O
% O
and46 O
% O
on O
these O
scores O
over O
the O
stronger O
model O
of O
Yao O
et O
al O
. O
( O
2019)4 O
. O
4.5 O
Human O
Evaluation O
Automatic O
metrics O
can O
not O
evaluate O
all O
aspects O
of O
open O
- O
ended O
text O
generation O
( O
Fan O
et O
al O
. O
, O
2018 O
) O
, O
and O
so O
we O
also O
conduct O
several O
human O
evaluations O
. O
Interactive O
Evaluation O
In O
this O
experiment O
, O
human O
subjects O
compare O
our O
best O
model O
, O
Rel O
. O
Cued O
Writer O
, O
with O
the O
strongest O
baseline O
from O
the O
automatic O
evaluations O
( O
VANILLA O
) O
in O
an O
interactive O
, O
real O
- O
time O
setup O
. O
For O
robust O
evaluation O
, O
it O
is O
essential O
that O
the O
users O
generate O
a O
wide O
variety O
of O
stories O
. O
Since O
generating O
different O
prompts O
( O
Ô¨Årst O
sentence O
) O
requires O
creativity O
on O
the O
part O
of O
human O
judges O
and O
can O
be O
challenging O
, O
we O
provided O
participants O
with O
initial O
prompts O
that O
were O
randomly O
selected O
from O
the O
test O
set O
. O
For O
each O
prompt O
, O
the O
participants O
generated O
stories O
using O
both O
models O
by O
interactively O
provid4Note O
that O
the O
result O
of O
our O
SEQ2SEQbaseline O
is O
not O
directly O
comparable O
with O
that O
of O
Inc O
- O
S2S O
in O
( O
Yao O
et O
al O
. O
, O
2019 O
) O
, O
since O
we O
included O
cue O
phrases O
as O
additional O
input O
whereas O
Inc O
- O
S2S O
generate O
the O
whole O
story O
conditioned O
on O
the O
title O
. O
( O
a O
) O
  O
( O
b O
) O
Figure O
6 O
: O
Human O
evaluations O
on O
story O
- O
level O
( O
left O
) O
and O
sentence O
- O
level O
( O
right O
) O
. O
We O
Ô¨Ånd O
that O
human O
judges O
preferred O
stories O
generated O
by O
Rel O
. O
Cued O
Writer O
. O
ing O
cue O
- O
phrases5 O
. O
They O
were O
then O
asked O
to O
choose O
which O
story O
they O
prefer O
. O
Participants O
preferred O
stories O
generated O
by O
Rel O
. O
Cued O
Writer O
over O
VANILLA O
in57.5%of O
the O
cases O
( O
80stories O
in O
total O
, O
p‚àº0.1 O
) O
. O
Judges O
also O
rated O
the O
stories O
in O
terms O
of O
Ô¨Çuency O
and O
coherence O
on O
a O
5 O
- O
point O
Likert O
scale O
. O
Rel O
. O
Cued O
Writer O
achieved O
a O
higher O
Ô¨Çuency O
score O
of O
4.22compared O
with O
3.80achieved O
by O
VANILLA O
.VANILLA O
attained O
a O
slightly O
higher O
coherence O
score O
( O
3.40 O
vs.3.35 O
) O
. O
On O
manually O
inspecting O
the O
generated O
stories O
, O
we O
found O
that O
our O
model O
generates O
longer O
sentences O
( O
avg O
. O
9.18words O
) O
with O
more O
complex O
language O
, O
whereas O
VANILLA O
generated O
relatively O
shorter O
sentences O
( O
avg O
. O
7.46words O
) O
which O
might O
improve O
coherence O
. O
This O
experiment O
is O
promising O
but O
inconclusive O
because O
for O
the O
same O
prompt O
, O
the O
participants O
could O
provide O
different O
sets O
of O
cue O
- O
phrases O
for O
different O
models O
, O
resulting O
in O
generated O
stories O
that O
are O
too O
different O
to O
be O
comparable O
( O
Table O
2 O
shows O
an O
example O
) O
. O
This O
led O
us O
to O
conduct O
the O
following O
more O
controlled O
evaluations O
. O
Story O
- O
level O
Evaluation O
In O
this O
experiment O
, O
we O
again O
make O
pairwise O
comparisons O
, O
but O
both O
models O
are O
provided O
the O
same O
prompts O
, O
and O
sets O
of O
cue O
phrases6.3judges O
evaluated O
100pairs O
of O
stories O
( O
in O
shufÔ¨Çed O
order)7 O
. O
Fig O
. O
6(a O
) O
shows O
the O
percentage O
of O
preference O
for O
our O
stronger O
model O
, O
Rel O
. O
Cued O
Writer O
, O
over O
the O
baselines O
. O
Judges O
prefer O
our O
model O
over O
all O
other O
baselines O
. O
Also O
, O
judges O
preferred O
Rel O
. O
Cued O
Writer O
over O
Cued O
Writer O
, O
which O
demonstrates O
the O
effec5We O
included O
instructions O
and O
examples O
for O
participants O
. O
The O
order O
of O
the O
presentation O
of O
the O
models O
was O
random O
. O
The O
judges O
were O
self O
- O
identiÔ¨Åed O
native O
English O
speakers O
. O
6Cue O
- O
phrases O
were O
extracted O
automatically O
to O
alleviate O
any O
biases O
, O
then O
curated O
manually O
. O
7For O
all O
AMT O
evaluations O
, O
we O
only O
allow O
judges O
who O
have O
at O
least O
500previous O
jobs O
, O
and O
at O
least O
97 O
% O
approval O
rate.594Prompt O
I O
was O
coughing O
very O
badly O
. O
|Cue O
phrases O
notice O
blood O
- O
emergency O
room O
- O
waited O
- O
okay O
DYNAMIC O
I O
noticed O
that O
I O
had O
a O
headache O
. O
I O
went O
to O
the O
doctor O
. O
The O
doctor O
told O
me O
that O
I O
had O
to O
go O
to O
the O
doctor O
. O
The O
doctor O
said O
I O
was O
Ô¨Åne O
. O
[ O
waited O
] O
STATIC O
Shenoticed O
a O
huge O
blood O
pressure O
. O
She O
went O
to O
the O
doctor O
. O
The O
doctor O
told O
her O
to O
stop O
. O
She O
waited O
for O
a O
while O
. O
[ O
okay O
] O
SEQ2SEQI O
noticed O
blood O
and O
noticed O
blood O
in O
the O
closet O
. O
Now O
I O
have O
emergency O
room O
foremergency O
room O
. O
I O
found O
it O
and O
I O
waited O
until O
the O
next O
day O
. O
I O
feel O
guilty O
and O
now O
i O
do O
n‚Äôt O
have O
to O
do O
it O
again O
. O
[ O
okay O
] O
VANILLA O
I O
noticed O
blood O
in O
the O
bathroom O
. O
I O
had O
to O
go O
to O
the O
emergency O
room O
. O
I O
waited O
in O
the O
bathroom O
for O
a O
while O
. O
Luckily O
the O
doctor O
was O
okay O
. O
Cued O
Writer O
I O
noticed O
blood O
in O
my O
mouth O
. O
I O
had O
to O
go O
to O
the O
emergency O
room O
. O
I O
waited O
there O
a O
few O
hours O
. O
I O
felt O
okay O
. O
Rel O
Cued O
Writer O
I O
noticed O
blood O
on O
my O
shirt O
. O
I O
went O
to O
the O
emergency O
room O
. O
I O
waited O
for O
the O
doctor O
. O
He O
came O
and O
I O
was O
okay O
. O
Prompt O
Harry O
really O
wanted O
to O
see O
the O
new O
Star O
Wars O
movie O
. O
|Cue O
phrases O
opening O
day O
- O
ticket O
- O
entire O
movie O
- O
really O
DYNAMIC O
He O
went O
to O
the O
theater O
to O
show O
up O
. O
He O
bought O
a O
ticket O
. O
He O
had O
a O
great O
time O
. O
He O
had O
a O
great O
time O
. O
[ O
opening O
day O
] O
[ O
entire O
movie O
] O
STATICOne O
day O
, O
shewas O
in O
a O
hurry O
to O
get O
to O
the O
theater O
. O
she O
was O
n‚Äôt O
sure O
if O
she O
would O
like O
to O
go O
. O
She O
was O
excited O
to O
try O
the O
new O
movie O
. O
She O
was O
so O
excited O
to O
see O
the O
movie O
. O
[ O
opening O
day O
] O
[ O
ticket O
] O
SEQ2SEQThe O
day O
daywas O
opening O
day O
and O
his O
family O
was O
opening O
theopening O
day O
. O
Harry O
had O
bought O
a O
ticket O
and O
the O
ticket O
was O
n‚Äôt O
very O
good O
. O
The O
entire O
movie O
was O
very O
happy O
. O
Harry O
became O
very O
really O
disappointed O
. O
VANILLAOn O
opening O
day O
, O
Harry O
was O
very O
nervous O
. O
He O
bought O
a O
ticket O
tothe O
theater O
. O
He O
bought O
Harry O
ticket O
tickets O
to O
the O
theater O
. O
He O
really O
did O
n‚Äôt O
like O
the O
movie O
. O
[ O
entire O
movie O
] O
Cued O
Writer O
On O
opening O
day O
, O
he O
went O
to O
the O
theater O
. O
He O
bought O
a O
ticket O
at O
the O
theater O
. O
The O
entire O
movie O
was O
great O
. O
He O
really O
was O
excited O
. O
Rel O
Cued O
Writer O
He O
decided O
to O
watch O
it O
on O
opening O
day O
. O
He O
got O
to O
the O
theater O
and O
got O
a O
ticket O
. O
He O
watched O
the O
entire O
movie O
. O
He O
was O
really O
excited O
about O
it O
. O
Table O
3 O
: O
Sample O
stories O
generated O
by O
different O
models O
. O
We O
highlight O
in O
different O
color O
the O
[ O
missing O
] O
cue O
phrase O
, O
incoherent O
or O
unÔ¨Çuent O
, O
and O
repetitive O
parts O
of O
each O
story O
. O
We O
see O
that O
compared O
to O
baselines O
, O
our O
models O
correctly O
mention O
cue O
phrases O
and O
generate O
better O
stories O
. O
tiveness O
of O
the O
additional O
Context O
- O
Cue O
and O
Relevance O
Multi O
- O
Head O
units O
. O
All O
improvements O
are O
statistically O
signiÔ¨Åcant O
( O
app O
. O
rand O
. O
, O
p<0.05 O
) O
. O
Sentence O
- O
level O
Evaluation O
We O
also O
performed O
a O
more O
Ô¨Åne O
- O
grained O
evaluation O
of O
the O
models O
by O
evaluating O
generated O
sentences O
while O
the O
model O
is O
generating O
a O
story O
. O
The O
generated O
sentences O
are O
evaluated O
in O
light O
of O
the O
( O
incomplete O
) O
story O
. O
Specifically O
, O
we O
provide O
an O
( O
incomplete O
) O
story O
passage O
and O
a O
manually O
provided O
cue O
phrase O
to O
the O
two O
models O
to O
generate O
the O
next O
sentence O
. O
We O
asked O
human O
judges O
to O
identify O
which O
of O
the O
two O
sentences O
is O
better O
based O
on O
their O
Ô¨Çuency O
and O
semantic O
relevance O
to O
( O
1 O
) O
the O
input O
( O
incomplete O
) O
story O
and O
( O
2 O
) O
the O
cue O
phrase O
. O
We O
did O
this O
experiment O
for O
a O
set O
of O
100randomly O
selected O
stories O
( O
400sentences O
. O
3different O
judges O
evaluated O
each O
sentence O
pair O
. O
Fig O
. O
6(b O
) O
shows O
that O
the O
Rel O
. O
Cued O
Writer O
model O
was O
preferred O
over O
SEQ2SEQandVANILLA O
in72 O
% O
and64 O
% O
of O
the O
cases O
, O
respectively O
. O
Comparing O
the O
two O
proposed O
models O
, O
we O
again O
see O
additive O
gain O
by O
modeling O
Cue O
- O
Context O
relevance O
. O
All O
improvements O
are O
statistically O
signiÔ¨Åcant O
( O
app O
. O
rand O
. O
, O
p<0.001 O
) O
. O
5 O
Qualitative O
Results O
and O
Error O
Analysis O
Table O
3 O
presents O
examples O
of O
stories O
generated O
by O
different O
models O
for O
the O
same O
prompt O
and O
cue O
phrases O
. O
We O
highlight O
the O
[ O
missing O
] O
cue O
phrases O
, O
incoherent O
or O
unÔ¨Çuent O
, O
and O
repetitive O
parts O
of O
eachoff O
- O
topic O
: O
Kelly O
and O
her O
friends O
went O
to O
a O
new O
ice O
- O
cream O
shop O
. O
They O
decided O
to O
try O
the O
new O
Ô¨Çavors O
. O
They O
all O
tried O
on O
many O
different O
restaurants O
. O
To O
their O
surprise O
, O
they O
thought O
it O
tasted O
good O
. O
They O
were O
glad O
to O
Ô¨Ånd O
one O
online O
. O
Not O
- O
logically O
- O
consistent O
: O
Avery O
received O
a O
homework O
assignment O
due O
in O
two O
weeks O
. O
He O
immediately O
read O
it O
. O
When O
he O
turned O
it O
in O
, O
he O
made O
schedule O
. O
He O
completed O
tasks O
and O
turned O
it O
in O
time O
. O
When O
he O
Ô¨Ånished O
early O
, O
he O
was O
disappointed O
. O
non O
- O
coreferent O
- O
pronouns O
: O
Rob O
has O
never O
been O
on O
a O
rollercoaster O
. O
They O
go O
on O
all O
the O
way O
to O
six O
Ô¨Çags O
. O
He O
got O
on O
with O
a O
free O
ticket O
. O
Rob O
joined O
the O
rollercoaster O
. O
There O
was O
a O
long O
line O
of O
people O
in O
the O
line O
. O
Table O
4 O
: O
Examples O
of O
errors O
made O
by O
our O
model O
. O
story O
. O
Note O
that O
we O
did O
not O
highlight O
[ O
missing O
] O
, O
if O
the O
model O
mentions O
part O
of O
the O
cue O
phrase O
or O
incorporates O
it O
semantically O
. O
As O
we O
observe O
, O
all O
of O
the O
baselines O
suffered O
from O
several O
issues O
; O
however O
, O
our O
novel O
content O
inducing O
approaches O
generate O
more O
causally O
related O
sentences O
, O
which O
Ô¨Åt O
the O
given O
prompt O
and O
cue O
phrases O
more O
naturally O
. O
We O
also O
manually O
reviewed O
50stories O
, O
generated O
from O
our O
models O
and O
analyzed O
common O
errors O
. O
Table O
4 O
shows O
sample O
stories O
that O
depict O
different O
types O
of O
errors O
including O
‚Äú O
getting O
off O
- O
topic O
‚Äù O
, O
‚Äú O
not O
- O
logically O
- O
connected O
‚Äù O
and O
‚Äú O
non O
- O
coreferent O
pronouns O
‚Äù O
. O
The O
last O
type O
of O
error O
represents O
the O
cases O
where O
the O
model O
generates O
pronouns O
that O
do O
not O
refer O
to O
any O
previously O
mentioned O
entity O
. O
The O
examples O
demonstrate O
that O
there O
are O
still O
many O
challenges O
in O
this O
domain.5956 O
Conclusion O
and O
Future O
Work O
This O
paper O
explored O
the O
problem O
of O
interactive O
storytelling O
, O
which O
leverages O
human O
and O
computer O
collaboration O
for O
creative O
language O
generation O
. O
We O
presented O
two O
content O
- O
inducing O
approaches O
that O
take O
user O
- O
provided O
inputs O
as O
the O
story O
progresses O
and O
effectively O
incorporate O
them O
in O
the O
generated O
text O
. O
Experimental O
results O
show O
that O
our O
methods O
outperform O
competitive O
baselines O
. O
However O
, O
there O
are O
several O
other O
signiÔ¨Åcant O
aspects O
to O
be O
considered O
in O
story O
generation O
, O
such O
as O
modeling O
of O
discourse O
relations O
, O
and O
representation O
of O
key O
narrative O
elements O
, O
which O
lie O
beyond O
the O
scope O
of O
this O
investigation O
. O
Also O
, O
while O
we O
received O
encouraging O
feedback O
from O
users O
on O
this O
setup O
during O
the O
interactive O
evaluation O
, O
we O
did O
not O
explore O
important O
questions O
about O
user O
interfaces O
, O
design O
, O
and O
human O
computer O
interaction O
. O
Future O
work O
can O
explore O
these O
questions O
and O
also O
explore O
other O
forms O
of O
natural O
language O
interaction O
. O
Abstract O
In O
this O
paper O
, O
we O
introduce O
a O
large O
- O
scale O
Indonesian O
summarization O
dataset O
. O
We O
harvest O
articles O
from O
Liputan6.com O
, O
an O
online O
news O
portal O
, O
and O
obtain O
215,827 O
document O
‚Äì O
summary O
pairs O
. O
We O
leverage O
pre O
- O
trained O
language O
models O
to O
develop O
benchmark O
extractive O
and O
abstractive O
summarization O
methods O
over O
the O
dataset O
with O
multilingual O
and O
monolingual O
BERT O
- O
based O
models O
. O
We O
include O
a O
thorough O
error O
analysis O
by O
examining O
machinegenerated O
summaries O
that O
have O
low O
ROUGE O
scores O
, O
and O
expose O
both O
issues O
with O
ROUGE O
itself O
, O
as O
well O
as O
with O
extractive O
and O
abstractive O
summarization O
models O
. O
1 O
Introduction O
Despite O
having O
the O
fourth O
largest O
speaker O
population O
in O
the O
world O
, O
with O
200 O
million O
native O
speakers,1 O
Indonesian O
is O
under O
- O
represented O
in O
NLP O
. O
One O
reason O
is O
the O
scarcity O
of O
large O
datasets O
for O
different O
tasks O
, O
such O
as O
parsing O
, O
text O
classiÔ¨Åcation O
, O
and O
summarization O
. O
In O
this O
paper O
, O
we O
attempt O
to O
bridge O
this O
gap O
by O
introducing O
a O
large O
- O
scale O
Indonesian O
corpus O
for O
text O
summarization O
. O
Neural O
models O
have O
driven O
remarkable O
progress O
in O
summarization O
in O
recent O
years O
, O
particularly O
for O
abstractive O
summarization O
. O
One O
of O
the O
Ô¨Årst O
studies O
was O
Rush O
et O
al O
. O
( O
2015 O
) O
, O
where O
the O
authors O
proposed O
an O
encoder O
‚Äì O
decoder O
model O
with O
attention O
to O
generate O
headlines O
for O
English O
Gigaword O
documents O
( O
Graff O
et O
al O
. O
, O
2003 O
) O
. O
Subsequent O
studies O
introduced O
pointer O
networks O
( O
Nallapati O
et O
al O
. O
, O
2016b O
; O
See O
et O
al O
. O
, O
2017 O
) O
, O
summarization O
with O
content O
selection O
( O
Hsu O
et O
al O
. O
, O
2018 O
; O
Gehrmann O
et O
al O
. O
, O
2018 O
) O
, O
graph O
- O
based O
attentional O
models O
( O
Tan O
et O
al O
. O
, O
2017 O
) O
, O
and O
deep O
reinforcement O
learning O
( O
Paulus O
et O
al O
. O
, O
2018 O
) O
. O
More O
recently O
, O
we O
have O
seen O
the O
widespread O
adoption O
1https://www.visualcapitalist.com/ O
100 O
- O
most O
- O
spoken O
- O
languages/ O
.of O
pre O
- O
trained O
neural O
language O
models O
for O
summarization O
, O
e.g. O
BERT O
( O
Liu O
and O
Lapata O
, O
2019 O
) O
, O
BART O
( O
Lewis O
et O
al O
. O
, O
2020 O
) O
, O
and O
PEGASUS O
( O
Zhang O
et O
al O
. O
, O
2020a O
) O
. O
Progress O
in O
summarization O
research O
has O
been O
driven O
by O
the O
availability O
of O
large O
- O
scale O
English O
datasets O
, O
including O
320 O
K O
CNN O
/ O
Daily O
Mail O
document O
‚Äì O
summary O
pairs O
( O
Hermann O
et O
al O
. O
, O
2015 O
) O
and O
100k O
NYT O
articles O
( O
Sandhaus O
, O
2008 O
) O
which O
have O
been O
widely O
used O
in O
abstractive O
summarization O
research O
( O
See O
et O
al O
. O
, O
2017 O
; O
Gehrmann O
et O
al O
. O
, O
2018 O
; O
Paulus O
et O
al O
. O
, O
2018 O
; O
Lewis O
et O
al O
. O
, O
2020 O
; O
Zhang O
et O
al O
. O
, O
2020a O
) O
. O
News O
articles O
are O
a O
natural O
candidate O
for O
summarization O
datasets O
, O
as O
they O
tend O
to O
be O
well O
- O
structured O
and O
are O
available O
in O
large O
volumes O
. O
More O
recently O
, O
English O
summarization O
datasets O
in O
other O
Ô¨Çavours O
/ O
domains O
have O
been O
developed O
, O
e.g. O
XSum O
has O
226 O
K O
documents O
with O
highly O
abstractive O
summaries O
( O
Narayan O
et O
al O
. O
, O
2018 O
) O
, O
BIGPATENT O
is O
a O
summarization O
dataset O
for O
the O
legal O
domain O
( O
Sharma O
et O
al O
. O
, O
2019 O
) O
, O
Reddit O
TIFU O
is O
sourced O
from O
social O
media O
( O
Kim O
et O
al O
. O
, O
2019 O
) O
, O
and O
Cohan O
et O
al O
. O
( O
2018 O
) O
proposed O
using O
scientiÔ¨Åc O
publications O
from O
arXiv O
and O
PubMed O
for O
abstract O
summarization O
. O
This O
paper O
introduces O
the O
Ô¨Årst O
large O
- O
scale O
summarization O
dataset O
for O
Indonesian O
, O
sourced O
from O
theLiputan6.com O
online O
news O
portal O
over O
a O
10year O
period O
. O
It O
covers O
various O
topics O
and O
events O
that O
happened O
primarily O
in O
Indonesia O
, O
from O
October O
2000 O
to O
October O
2010 O
. O
Below O
, O
we O
present O
details O
of O
the O
dataset O
, O
propose O
benchmark O
extractive O
and O
abstractive O
summarization O
methods O
that O
leverage O
both O
multilingual O
and O
monolingual O
pre O
- O
trained O
BERT O
models O
. O
We O
further O
conduct O
error O
analysis O
to O
better O
understand O
the O
limitations O
of O
current O
models O
over O
the O
dataset O
, O
as O
part O
of O
which O
we O
reveal O
not O
just O
modelling O
issues O
but O
also O
problems O
with O
ROUGE O
. O
To O
summarize O
, O
our O
contributions O
are O
: O
( O
1 O
) O
we O
release O
a O
large O
- O
scale O
Indonesian O
summarization O
corpus O
with O
over O
200 O
K O
documents O
, O
an O
order O
of O
mag-598Dokumen O
: O
	  O
Liputan6.com O
, O
	 O
Jakarta O
	 O
: O
	 O
Gara O
- O
gara O
	  O
berusaha O
	 O
kabur O
	 O
saat O
	 O
diminta O
menunjukkan O
	 O
barang O
	 O
hasil O
	 O
curian O
, O
	 O
Rosihan O
	 O
bin O
	 O
Usman O
, O
	  O
tersangka O
pencurian O
	 O
tas O
	 O
wisatawan O
	 O
asing O
, O
	 O
baru O
- O
baru O
	 O
ini O
, O
	 O
tersungkur O
	  O
ditembak O
aparat O
	 O
Kepolisian O
	 O
Resor O
	 O
Denpasar O
	 O
Barat O
, O
	 O
Bali O
. O
	 O
Sebelumnya O
, O
	 O
Rosihan O
ditangkap O
	 O
massa O
	 O
setelah O
	 O
mencuri O
	 O
tas O
	 O
Nicholas O
	 O
Dreyden O
, O
	 O
wisatawan O
asing O
	 O
asal O
	 O
Inggris O
. O
	 O
T O
as O
	 O
yang O
	 O
berisi O
	 O
dokumen O
	 O
keimigrasian O
	 O
dan O
	 O
surat O
penting O
	 O
itu O
	 O
diambil O
	 O
Rosihan O
	 O
setelah O
	 O
mengelabui O
	 O
korban O
. O
	  O
[ O
7 O
	 O
kalimat O
	 O
dengan O
	 O
78 O
	 O
kata O
	 O
setelahnya O
	 O
tidak O
	 O
ditampilkan O
] O
Ringkasan O
: O
Seorang O
	 O
pencuri O
	 O
tas O
	 O
wisatawan O
	 O
asing O
	 O
ditembak O
	 O
polisi O
. O
	 O
Ia O
	 O
berusaha O
kabur O
	 O
saat O
	 O
diminta O
	 O
menunjukan O
	 O
hasil O
	 O
curian O
. O
	 O
Karena O
	 O
itu O
, O
	 O
polisi O
menembaknya O
. O
Example-2 O
Document O
: O
	  O
Liputan6.com O
, O
	 O
Jakarta O
: O
	 O
Because O
	 O
of O
	  O
trying O
	 O
to O
	 O
escape O
	 O
when O
	 O
asked O
	 O
to O
show O
	 O
stolen O
	 O
goods O
, O
	 O
Rosihan O
	 O
bin O
	 O
Usman O
, O
	  O
a O
	 O
suspect O
	 O
of O
	 O
the O
	 O
theft O
	 O
of O
	 O
a O
foreign O
	 O
tourist O
	 O
bag O
, O
	 O
recently O
	 O
fell O
	 O
down O
, O
	  O
shot O
	 O
by O
	 O
the O
	 O
W O
est O
	 O
Denpasar O
Resort O
	 O
Police O
, O
	 O
Bali O
. O
	 O
Previously O
, O
	 O
Rosihan O
	 O
was O
	 O
arrested O
	 O
by O
	 O
the O
	 O
mob O
	 O
after O
stealing O
	 O
the O
	 O
bag O
	 O
of O
	 O
Nicholas O
	 O
Dreyden O
, O
	 O
a O
	 O
foreign O
	 O
tourist O
	 O
from O
	 O
England O
. O
The O
	 O
bag O
	 O
containing O
	 O
immigration O
	 O
documents O
	 O
and O
	 O
important O
	 O
letters O
	 O
was O
taken O
	 O
by O
	 O
Rosihan O
	 O
after O
	 O
tricking O
	 O
the O
	 O
victim O
. O
	  O
[ O
7 O
	 O
sentences O
	 O
with O
	 O
78 O
	 O
words O
	 O
are O
	 O
abbreviated O
	 O
from O
	 O
here O
] O
Summary O
: O
A O
	 O
foreign O
	 O
tourist O
	 O
bag O
	 O
thief O
	 O
was O
	 O
shot O
	 O
by O
	 O
police O
. O
	 O
He O
	 O
tried O
	 O
to O
	 O
run O
	 O
away O
when O
	 O
asked O
	 O
to O
	 O
show O
	 O
the O
	 O
loot O
. O
	 O
Because O
	 O
of O
	 O
this O
, O
	 O
the O
	 O
police O
	 O
shot O
	 O
him O
. O
Dokumen O
: O
	  O
Liputan6.com O
, O
	 O
Jakarta O
	 O
: O
	 O
Or O
ganisasi O
	 O
Negara O
- O
negara O
	 O
Pengekspor O
Minyak O
	 O
( O
OPEC O
) O
	 O
mengakui O
	 O
mengalami O
	 O
kesulitan O
	 O
untuk O
	 O
menjaga O
stabilitas O
	 O
har O
ga O
	 O
minyak O
	 O
dunia O
. O
	 O
Itu O
	 O
lantaran O
	 O
har O
ga O
	 O
minyak O
	 O
terus O
melonjak O
	 O
sepanjang O
	 O
tahun O
	 O
ini O
. O
	 O
Hingga O
	 O
kini O
	 O
har O
ga O
	 O
minyak O
	 O
mentah O
dunia O
	 O
masih O
	 O
mencapai O
	 O
tingkat O
	 O
tertinggi O
	 O
sejak O
	 O
pecah O
	 O
perang O
	 O
teluk O
sepuluh O
	 O
tahun O
	 O
silam O
. O
[ O
3 O
	 O
kalimat O
	 O
dengan O
	 O
57 O
	 O
kata O
	 O
tidak O
	 O
ditampilkan O
] O
Padahal O
	 O
, O
	 O
sebelumnya O
	 O
OPEC O
	 O
telah O
	 O
merevisi O
	 O
produksi O
	 O
minyak O
sebanyak O
	 O
tiga O
	 O
kali O
	 O
dalam O
	 O
enam O
	 O
bulan O
	 O
terakhir O
. O
	 O
Pertama O
, O
	 O
April O
	 O
hingga O
Juni O
	 O
dengan O
	 O
kenaikan O
	 O
mencapai O
	 O
500 O
	 O
ribu O
	 O
barel O
	 O
dan O
	 O
terakhir O
, O
September O
	 O
ini O
, O
	 O
OPEC O
	 O
kembali O
	 O
menaikkan O
	 O
produksi O
	 O
sebesar O
	 O
800 O
	 O
ribu O
barel O
	 O
per O
	 O
hari O
. O
	  O
[ O
5 O
	 O
kalimat O
	 O
dengan O
	 O
96 O
	 O
kata O
	 O
setelahnya O
	 O
tidak O
	 O
ditampilkan O
] O
Ringkasan O
: O
OPEC O
	 O
kesulitan O
	 O
menjaga O
	 O
stabilitas O
	 O
har O
ga O
	 O
minyak O
	 O
dunia O
	 O
lantaran O
	 O
har O
ga O
minyak O
	 O
dipasaran O
	 O
terus O
	 O
melonjak O
. O
	 O
Padahal O
, O
	 O
OPEC O
	 O
telah O
	 O
tiga O
	 O
kali O
menaikkan O
	 O
produksi O
	 O
dalam O
	 O
enam O
	 O
bulan O
	 O
terakhir O
.Example-1 O
Document O
: O
	  O
Liputan6.com O
, O
	 O
Jakarta O
: O
	 O
The O
	 O
Or O
ganization O
	 O
of O
	 O
Petroleum O
	 O
Exporting O
Countries O
	  O
( O
OPEC O
) O
	 O
has O
	 O
admitted O
	 O
that O
	 O
it O
	 O
is O
	 O
having O
	 O
dif O
ficulty O
	 O
maintaining O
the O
	 O
stability O
	 O
of O
	 O
world O
	 O
oil O
	 O
prices O
. O
	 O
That O
's O
	 O
because O
	 O
oil O
	 O
prices O
	 O
continue O
	 O
to O
soar O
	 O
this O
	 O
year O
. O
	 O
Until O
	 O
now O
	 O
world O
	 O
crude O
	 O
oil O
	 O
prices O
	 O
have O
	 O
still O
	 O
reached O
	 O
the O
highest O
	 O
level O
	 O
since O
	 O
the O
	 O
gulf O
	 O
war O
	 O
broke O
	 O
out O
	 O
ten O
	 O
years O
	 O
ago O
. O
[ O
3 O
	 O
sentences O
	 O
with O
	 O
57 O
	 O
words O
	 O
are O
	 O
abbreviated O
	 O
from O
	 O
here O
] O
In O
	 O
fact O
, O
	 O
OPEC O
	 O
had O
	 O
previously O
	 O
revised O
	 O
oil O
	 O
production O
	 O
three O
	 O
times O
	 O
in O
	 O
the O
last O
	 O
six O
	 O
months O
. O
	 O
First O
, O
	 O
April O
	 O
to O
	 O
June O
	 O
with O
	 O
an O
	 O
increase O
	 O
of O
	 O
500 O
	 O
thousand O
barrels O
	 O
and O
	 O
last O
, O
	 O
this O
	 O
September O
, O
	 O
OPEC O
	 O
has O
	 O
again O
	 O
increased O
	 O
production O
by O
	 O
800 O
	 O
thousand O
	 O
barrels O
	 O
per O
	 O
day O
. O
[ O
5 O
	 O
sentences O
	 O
with O
	 O
96 O
	 O
words O
	 O
are O
	 O
abbreviated O
	 O
from O
	 O
here O
] O
Summary O
: O
OPEC O
	 O
is O
	 O
struggling O
	 O
to O
	 O
maintain O
	 O
the O
	 O
stability O
	 O
of O
	 O
world O
	 O
oil O
	 O
prices O
because O
	 O
oil O
	 O
prices O
	 O
on O
	 O
the O
	 O
market O
	 O
continue O
	 O
to O
	 O
soar O
. O
	 O
In O
	 O
fact O
, O
	 O
OPEC O
	 O
has O
raised O
	 O
production O
	 O
three O
	 O
times O
	 O
in O
	 O
the O
	 O
past O
	 O
six O
	 O
months O
. O
Figure O
1 O
: O
Example O
articles O
and O
summaries O
from O
Liputan6 O
. O
To O
the O
left O
is O
the O
original O
document O
and O
summary O
, O
and O
to O
the O
right O
is O
an O
English O
translation O
( O
for O
illustrative O
purposes O
) O
. O
We O
additionally O
highlight O
sentences O
that O
the O
summary O
is O
based O
on O
( O
noting O
that O
such O
highlighting O
is O
not O
available O
in O
the O
dataset O
) O
. O
nitude O
larger O
than O
the O
current O
largest O
Indonesian O
summarization O
dataset O
and O
one O
of O
the O
largest O
nonEnglish O
summarization O
datasets O
in O
existence;2(2 O
) O
we O
present O
statistics O
to O
show O
that O
the O
summaries O
in O
the O
dataset O
are O
reasonably O
abstractive O
, O
and O
provide O
two O
test O
partitions O
, O
a O
standard O
test O
set O
and O
an O
extremely O
abstractive O
test O
set O
; O
( O
3 O
) O
we O
develop O
benchmark O
extractive O
and O
abstractive O
summarization O
models O
based O
on O
pre O
- O
trained O
BERT O
models O
; O
and O
( O
4 O
) O
we O
conduct O
error O
analysis O
, O
on O
the O
basis O
of O
which O
we O
share O
insights O
to O
drive O
future O
research O
on O
Indonesian O
text O
summarization O
. O
2 O
Data O
Construction O
Liputan6.com O
is O
an O
online O
Indonesian O
news O
portal O
which O
has O
been O
running O
since O
August O
2000 O
, O
and O
provides O
news O
across O
a O
wide O
range O
of O
topics O
including O
politics O
, O
business O
, O
sport O
, O
technology O
, O
health O
, O
and O
entertainment O
. O
According O
to O
the O
Alexa O
ranking O
of O
websites O
at O
the O
time O
of O
writing,3 O
Liputan6.com O
is O
ranked O
9th O
in O
Indonesia O
and O
112th O
globally O
. O
The O
website O
produces O
daily O
articles O
along O
2The O
data O
can O
be O
accessed O
at O
https://github.com/ O
fajri91 O
/ O
sum_liputan6 O
3https://www.alexa.com/topsiteswith O
a O
short O
description O
for O
its O
RSS O
feed O
. O
The O
summary O
is O
encapsulated O
in O
the O
javascript O
variablewindow.kmklabs.article O
and O
the O
key O
shortDescription O
, O
while O
the O
article O
is O
in O
the O
main O
body O
of O
the O
associated O
HTML O
page O
. O
We O
harvest O
this O
data O
over O
a O
10 O
- O
year O
window O
‚Äî O
from O
October O
2000 O
to O
October O
2010 O
‚Äî O
to O
create O
a O
largescale O
summarization O
corpus O
, O
comprising O
215,827 O
document O
‚Äì O
summary O
pairs O
. O
In O
terms O
of O
preprocessing O
, O
we O
remove O
formatting O
and O
HTML O
entities O
( O
e.g. O
& O
quot O
, O
and O
) O
, O
lowercase O
all O
words O
, O
and O
segment O
sentences O
based O
on O
simple O
punctuation O
heuristics O
. O
We O
provide O
example O
articles O
and O
summaries O
, O
with O
English O
translations O
for O
expository O
purposes O
( O
noting O
that O
translations O
are O
not O
part O
of O
the O
dataset O
) O
, O
in O
Figure O
1 O
. O
As O
a O
preliminary O
analysis O
of O
the O
document O
‚Äì O
summary O
pairs O
over O
the O
10 O
- O
year O
period O
, O
we O
binned O
the O
pairs O
into O
5 O
chronologically O
- O
ordered O
groups O
containing O
20 O
% O
of O
the O
data O
each O
, O
and O
computed O
the O
proportion O
of O
novel O
n O
- O
grams O
( O
order O
1 O
to O
4 O
) O
in O
the O
summary O
( O
relative O
to O
the O
source O
document O
) O
. O
Based O
on O
the O
results O
in O
Figure O
2 O
, O
we O
can O
see O
that O
the O
proportion O
of O
novel O
n O
- O
grams O
drops O
over O
time O
, O
implying O
that O
the O
summaries O
of O
more O
recent O
articles O
are O
less599Variant#Doc O
% O
of O
Novel O
n O
- O
grams O
Train O
Dev O
Test O
1 O
2 O
3 O
4 O
Canonical O
193,883 O
10,972 O
10,972 O
16.2 O
52.5 O
71.8 O
82.4 O
Xtreme O
193,883 O
4,948 O
3,862 O
22.2 O
66.7 O
87.5 O
96.6 O
Table O
1 O
: O
Statistics O
for O
the O
canonical O
and O
Xtreme O
variants O
of O
our O
data O
. O
The O
percentage O
of O
novel O
n O
- O
grams O
is O
based O
on O
the O
combined O
Dev O
and O
Test O
set O
. O
TimePercentage O
0255075100 O
Oct O
2000 O
- O
Jul O
  O
2003Jul O
2003 O
- O
Aug O
  O
2006Aug O
2006 O
- O
Apr O
  O
2008Apr O
2008 O
- O
Aug O
  O
2009Aug O
2009 O
- O
Oct O
  O
20101 O
- O
gram O
2 O
- O
gram O
3 O
- O
gram O
4 O
- O
gram O
Figure O
2 O
: O
Proportion O
of O
novel O
n O
- O
grams O
over O
time O
in O
the O
summaries O
. O
abstractive O
. O
For O
this O
reason O
, O
we O
decide O
to O
use O
the O
earlier O
articles O
( O
October O
2000 O
to O
Jan O
2002 O
) O
as O
the O
development O
and O
test O
documents O
, O
to O
create O
a O
more O
challenging O
dataset O
. O
This O
setup O
also O
means O
there O
is O
less O
topic O
overlap O
between O
training O
and O
development O
/ O
test O
documents O
, O
allowing O
us O
to O
assess O
whether O
the O
summarization O
models O
are O
able O
to O
summarize O
unseen O
topics O
. O
For O
the O
training O
, O
development O
and O
test O
partitions O
, O
we O
use O
a O
splitting O
ratio O
of O
90:5:5 O
. O
In O
addition O
to O
this O
canonical O
partitioning O
of O
the O
data O
, O
we O
provide O
an O
‚Äú O
Xtreme O
‚Äù O
variant O
( O
inspired O
by O
Xsum O
; O
Narayan O
et O
al O
. O
( O
2018 O
) O
) O
whereby O
we O
discard O
development O
and O
test O
document O
‚Äì O
summary O
pairs O
where O
the O
summary O
has O
fewer O
than O
90 O
% O
novel O
4 O
- O
grams O
( O
leaving O
the O
training O
data O
unchanged O
) O
, O
creating O
a O
smaller O
, O
more O
challenging O
data O
conÔ¨Åguration O
. O
Summary O
statistics O
for O
the O
‚Äú O
canonical O
‚Äù O
and O
‚Äú O
Xtreme O
‚Äù O
variants O
are O
given O
in O
Table O
1 O
. O
We O
next O
present O
a O
comparison O
of O
Liputan6 O
( O
canonical O
partitioning O
) O
and O
IndoSum O
( O
the O
current O
largest O
Indonesian O
summarization O
dataset O
, O
as O
detailed O
in O
Section O
6 O
; O
Kurniawan O
and O
Louvan O
( O
2018 O
) O
) O
in O
Table O
2 O
. O
In O
terms O
of O
number O
of O
documents O
, O
Liputan6 O
is O
approximately O
11 O
times O
larger O
than O
IndoSum O
( O
the O
current O
largest O
Indonesian O
summarization O
dataset O
) O
, O
although O
articles O
and O
summaries O
in O
Liputan6 O
are O
slightly O
shorter O
. O
To O
understand O
the O
abstractiveness O
of O
the O
summaries O
in O
the O
two O
datasets O
, O
in O
Table O
3 O
we O
presentROUGE O
scores O
for O
the O
simple O
baseline O
of O
using O
the O
Ô¨ÅrstNsentences O
as O
an O
extractive O
summary O
( O
‚Äú O
LEAD O
- O
N O
‚Äù O
) O
, O
and O
the O
percentage O
of O
novel O
n O
- O
grams O
in O
the O
summary.4We O
use O
LEAD-3andLEAD-2 O
for O
IndoSum O
and O
Liputan6 O
respectively O
, O
based O
on O
the O
average O
number O
of O
sentences O
in O
the O
summaries O
( O
Table O
2 O
) O
. O
We O
see O
that O
Liputan6 O
has O
consistently O
lower O
ROUGE O
scores O
( O
R1 O
, O
R2 O
, O
and O
RL O
) O
for O
LEADN O
; O
it O
also O
has O
a O
substantially O
higher O
proportion O
of O
noveln O
- O
grams O
. O
This O
suggests O
that O
the O
summaries O
in O
Liputan6 O
are O
more O
abstractive O
than O
IndoSum O
. O
To O
create O
a O
ground O
truth O
for O
extractive O
summarization O
, O
we O
follow O
Cheng O
and O
Lapata O
( O
2016 O
) O
and O
Nallapati O
et O
al O
. O
( O
2016a O
) O
in O
greedily O
selecting O
the O
subset O
of O
sentences O
in O
the O
article O
that O
maximizes O
the O
ROUGE O
score O
based O
on O
the O
reference O
summary O
. O
As O
a O
result O
, O
each O
sentence O
in O
the O
article O
has O
a O
binary O
label O
to O
indicate O
whether O
they O
should O
be O
included O
as O
part O
of O
an O
extractive O
summary O
. O
Extractive O
summaries O
created O
this O
way O
will O
be O
referred O
to O
as O
‚Äú O
ORACLE O
‚Äù O
, O
to O
denote O
the O
upper O
bound O
performance O
of O
an O
extractive O
summarization O
system O
. O
3 O
Summarization O
Models O
We O
follow O
Liu O
and O
Lapata O
( O
2019 O
) O
in O
building O
extractive O
and O
abstractive O
summarization O
models O
using O
BERT O
as O
an O
encoder O
to O
produce O
contextual O
representations O
for O
the O
word O
tokens O
. O
The O
architecture O
of O
both O
models O
is O
presented O
in O
Figure O
3 O
. O
We O
tokenize O
words O
with O
WordPiece O
, O
and O
append O
[ O
CLS O
] O
( O
preÔ¨Åx O
) O
and O
[ O
SEP O
] O
( O
sufÔ¨Åx O
) O
tokens O
to O
each O
sentence O
. O
To O
further O
distinguish O
the O
sentences O
, O
we O
add O
even O
/ O
odd O
segment O
embeddings O
( O
TA O
/ O
TB O
) O
based O
on O
the O
order O
of O
the O
sentence O
to O
the O
word O
embeddings O
. O
For O
instance O
, O
for O
a O
document O
with O
sentences O
[ O
s1,s2,s3,s4 O
] O
, O
the O
segment O
embeddings O
are O
[ O
TA O
, O
TB O
, O
TA O
, O
TB O
] O
. O
Position O
embeddings O
( O
P O
) O
are O
also O
used O
to O
denote O
the O
position O
of O
each O
token O
. O
The O
WordPiece O
, O
segment O
, O
and O
position O
embeddings O
are O
summed O
together O
and O
provided O
as O
input O
to O
BERT O
. O
BERT O
produces O
a O
series O
of O
contextual O
representations O
for O
the O
word O
tokens O
, O
which O
we O
feed O
into O
a O
( O
second O
) O
transformer O
encoder O
/ O
decoder O
for O
the O
extractive O
/ O
abstractive O
summarization O
model O
. O
We O
detail O
the O
architecture O
of O
these O
two O
models O
in O
Sections O
3.1 O
and O
3.2 O
. O
Note O
that O
this O
second O
transformer O
is O
initialized O
with O
random O
parameters O
( O
i.e. O
it O
is O
not O
pre O
- O
trained O
) O
. O
For O
the O
pre O
- O
trained O
BERT O
encoder O
, O
we O
use O
mul4All O
statistics O
are O
based O
on O
the O
entire O
dataset O
, O
encompassing O
the O
training O
, O
dev O
, O
and O
test O
data.600Dataset#Doc O
Article O
Summary O
Train O
Dev O
Test O
¬µ(Word)¬µ(Sent O
) O
# O
Vocab O
¬µ(Word)¬µ(Sent O
) O
# O
Vocab O
IndoSum O
14,252 O
750 O
3,762 O
347.23 O
18.37 O
117 O
K O
68.09 O
3.47 O
53 O
K O
Liputan6 O
193,883 O
10,972 O
10,972 O
232.91 O
12.60 O
311 O
K O
30.43 O
2.09 O
100 O
K O
Table O
2 O
: O
A O
comparison O
of O
IndoSum O
and O
Liputan6 O
. O
¬µ(Word O
) O
and¬µ(Sent O
) O
denote O
the O
average O
number O
of O
words O
and O
sentences O
, O
respectively O
. O
DatasetLEAD O
- O
N O
% O
of O
Novel O
n O
- O
grams O
R1 O
R2 O
RL O
1 O
2 O
3 O
4 O
IndoSum O
65.6 O
58.9 O
64.8 O
3.1 O
10.8 O
16.2 O
20.3 O
Liputan6 O
41.2 O
27.1 O
38.7 O
12.9 O
41.6 O
57.6 O
66.9 O
Table O
3 O
: O
Abstractiveness O
of O
the O
summaries O
in O
IndoSum O
and O
Liputan6 O
. O
tilingual O
BERT O
( O
mBERT O
) O
and O
our O
own O
IndoBERT O
( O
Koto O
et O
al O
. O
, O
to O
appear).5IndoBERT O
is O
a O
BERTBase O
model O
we O
trained O
ourselves O
using O
Indonesian O
documents O
from O
three O
sources O
: O
( O
1 O
) O
Indonesian O
Wikipedia O
( O
74 O
M O
words O
) O
; O
( O
2 O
) O
news O
articles O
( O
55 O
M O
words O
) O
from O
Kompas,6Tempo O
( O
Tala O
et O
al O
. O
, O
2003),7and O
Liputan6;8and O
( O
3 O
) O
the O
Indonesian O
Web O
Corpus O
( O
90 O
M O
words O
; O
Medved O
and O
Suchomel O
( O
2017 O
) O
) O
. O
In O
total O
, O
the O
training O
data O
has O
220 O
M O
words O
. O
We O
implement O
IndoBERT O
using O
the O
Huggingface O
framework,9and O
follow O
the O
default O
conÔ¨Åguration O
of O
BERT O
- O
Base O
( O
uncased O
): O
hidden O
size O
= O
768d O
, O
hidden O
layers O
= O
12 O
, O
attention O
heads O
= O
12 O
, O
and O
feed O
- O
forward O
= O
3,072d O
. O
We O
train O
IndoBERT O
with O
31,923 O
WordPieces O
( O
vocabulary O
) O
for O
2 O
million O
steps O
. O
3.1 O
Extractive O
Model O
After O
the O
document O
is O
processed O
by O
BERT O
, O
we O
have O
a O
contextualized O
embedding O
for O
every O
word O
token O
in O
the O
document O
. O
To O
learn O
inter O
- O
sentential O
relationships O
, O
we O
use O
the O
[ O
CLS O
] O
embeddings O
( O
[ O
xS1,xS2, O
.. O
,x O
Sm O
] O
) O
to O
represent O
the O
sentences O
, O
to O
which O
we O
add O
a O
sentence O
- O
level O
positional O
embedding O
( O
P O
) O
, O
and O
feed O
them O
to O
a O
transformer O
encoder O
( O
Figure O
3 O
) O
. O
An O
MLP O
layer O
with O
sigmoid O
activation O
is O
applied O
to O
the O
output O
of O
the O
transformer O
encoder O
to O
predict O
whether O
a O
sentence O
should O
be O
extracted O
( O
i.e.ÀúyS‚àà{0,1 O
} O
) O
. O
We O
train O
the O
model O
with O
binary O
5The O
pre O
- O
trained O
mBERT O
is O
sourced O
from O
: O
https:// O
github.com/google-research/bert O
. O
6https://kompas.com O
7https://koran.tempo.co O
8For O
Liputan6 O
, O
we O
use O
only O
the O
articles O
from O
the O
training O
partition O
. O
9https://huggingface.co/ O
mBERT O
	 O
/ O
	 O
IndoBER O
T O
[ O
CLS]Sent1[SEP][CLS]Sent2[SEP O
] O
[ O
CLS]Sentm[SEP][PAD]TA O
TA O
TATBTBTB O
TA O
TA O
TATPP2 O
P1xS O
1xS O
2xS O
m O
W O
ord O
Segment O
Position O
Bert O
	 O
Out O
P4 O
P3 O
P6 O
P5 O
Pn-3 O
Pn-1 O
Pn-2 O
PnxS O
1xS O
e O
n O
t O
1xS O
E O
PxS O
2xS O
e O
n O
t O
2xS O
E O
PxS O
mxS O
e O
n O
t O
mxS O
E O
PxP O
A O
DExtractive O
	 O
Model O
Transformer O
	 O
Encoder·ªπS O
1 O
·ªπS O
m O
MLP O
	 O
Layer O
... O
... O
...... O
Pre O
- O
trained O
	 O
model·ªπS O
2 O
...... O
xS O
1xS O
e O
n O
t O
1xS O
e O
n O
t O
mxS O
E O
PxP O
A O
D O
... O
Transformer O
	 O
DecoderSoftmax O
	 O
LayerGeneration O
	 O
of O
	  O
YAbstractive O
	 O
ModelLearned O
	 O
from O
	 O
scratchP2 O
P1 O
Pm O
... O
P2 O
P1 O
... O
Pn-1 O
Pn-2 O
Pn O
PositionFigure O
3 O
: O
Architecture O
of O
the O
extractive O
and O
abstractive O
summarization O
models O
. O
cross O
entropy O
, O
and O
update O
all O
model O
parameters O
( O
including O
BERT O
) O
during O
training O
. O
Note O
that O
the O
parameters O
in O
the O
transformer O
encoder O
and O
the O
MLP O
layer O
are O
initialized O
randomly O
, O
and O
learned O
from O
scratch O
. O
The O
transformer O
encoder O
is O
conÔ¨Ågured O
as O
follows O
: O
layers O
= O
2 O
, O
hidden O
size O
= O
768 O
, O
feed O
- O
forward O
= O
2,048 O
, O
and O
heads O
= O
8 O
. O
In O
terms O
of O
training O
hyperparameters O
, O
we O
train O
using O
the O
Adam O
optimizer O
with O
learning O
rate O
lr= O
2e‚àí3¬∑min(step‚àí0.5,step O
¬∑ O
warmup‚àí1.5)where O
warmup O
= O
10,000 O
. O
We O
train O
for O
50,000 O
steps O
on O
3 O
√óV100 O
16 O
GB O
GPUs O
, O
and O
perform O
evaluation O
on O
the O
development O
set O
every O
2,500 O
steps O
. O
At O
test O
time O
, O
we O
select O
sentences O
for O
the O
extractive O
summary O
according O
to O
two O
conditions O
: O
the O
summary O
must O
consist O
of O
: O
( O
a O
) O
at O
least O
two O
sentences O
, O
and O
( O
b O
) O
at O
least O
15 O
words O
. O
These O
values O
were O
set O
based O
on O
the O
average O
number O
of O
sentences O
and O
the O
minimum O
number O
of O
words O
in O
a O
summary O
. O
We O
also O
apply O
trigram O
blocking O
to O
reduce O
redundancy O
( O
Paulus O
et O
al O
. O
, O
2018 O
) O
. O
Henceforth O
, O
we O
refer O
to O
this O
model O
as O
‚Äú O
B O
ERTEXT‚Äù.6013.2 O
Abstractive O
Model O
Similar O
to O
the O
extractive O
model O
, O
we O
have O
a O
second O
transformer O
to O
process O
the O
contextualized O
embeddings O
from O
BERT O
. O
In O
this O
case O
, O
we O
use O
a O
transformer O
decoder O
instead O
( O
i.e. O
an O
attention O
mask O
is O
used O
to O
prevent O
the O
decoder O
from O
attending O
to O
future O
time O
steps O
) O
, O
as O
we O
are O
learning O
to O
generate O
an O
abstractive O
summary O
. O
But O
unlike O
the O
extractive O
model O
, O
we O
use O
the O
BERT O
embeddings O
for O
all O
tokens O
as O
input O
to O
the O
transformer O
decoder O
( O
as O
we O
do O
not O
need O
sentence O
representations O
) O
. O
We O
add O
to O
these O
BERT O
embeddings O
a O
second O
positional O
encoding O
before O
feeding O
them O
to O
the O
transformer O
decoder O
( O
Figure O
3 O
) O
. O
The O
transformer O
decoder O
is O
initialized O
with O
random O
parameters O
( O
i.e. O
no O
pre O
- O
training O
) O
. O
The O
transformer O
decoder O
is O
conÔ¨Ågured O
as O
follows O
: O
layers O
= O
6 O
, O
hidden O
size O
= O
768 O
, O
feed O
- O
forward O
= O
2,048 O
, O
and O
heads O
= O
8 O
. O
Following O
Liu O
and O
Lapata O
( O
2019 O
) O
, O
we O
use O
a O
different O
learning O
rate O
for O
BERT O
and O
the O
decoder O
when O
training O
the O
model O
: O
lr= O
2e‚àí3¬∑min(step‚àí0.5,step¬∑20,000‚àí1.5)and O
0.1¬∑min(step‚àí0.5,step¬∑10,000‚àí1.5)for O
BERT O
and O
the O
transformer O
decoder O
, O
respectively O
. O
Both O
networks O
are O
trained O
with O
the O
Adam O
optimizer O
for O
200,000 O
steps O
on O
4 O
√óV100 O
16 O
GB O
GPUs O
and O
evaluated O
every O
10,000 O
steps O
. O
For O
summary O
generation O
, O
we O
use O
beam O
width O
= O
5 O
, O
trigram O
blocking O
, O
and O
a O
length O
penalty O
( O
Wu O
et O
al O
. O
, O
2016 O
) O
to O
generate O
at O
least O
two O
sentences O
and O
at O
least O
15 O
words O
( O
similar O
to O
the O
extractive O
model O
) O
. O
Henceforth O
the O
abstractive O
model O
will O
be O
referred O
to O
as O
‚Äú O
BERTABS O
‚Äù O
. O
We O
additionally O
experiment O
with O
a O
third O
variant O
, O
‚Äú O
BERTEXTABS O
‚Äù O
, O
where O
we O
use O
the O
weights O
of O
the O
Ô¨Åne O
- O
tuned O
BERT O
in O
BERTEXTfor O
the O
encoder O
( O
instead O
of O
off O
- O
the O
- O
shelf O
BERT O
weights O
) O
. O
4 O
Experiment O
and O
Results O
We O
use O
three O
ROUGE O
( O
Lin O
, O
2004 O
) O
F-1 O
scores O
as O
evaluation O
metrics O
: O
R1 O
( O
unigram O
overlap O
) O
, O
R2 O
( O
bigram O
overlap O
) O
, O
and O
RL O
( O
longest O
common O
subsequence O
overlap O
) O
. O
In O
addition O
, O
we O
also O
provide O
BERTS O
CORE O
( O
F-1 O
) O
, O
as O
has O
recently O
been O
used O
for O
machine O
translation O
evaluation O
( O
Zhang O
et O
al O
. O
, O
2020b).10We O
use O
the O
development O
set O
to O
select O
the O
best O
checkpoint O
during O
training O
, O
and O
report O
the O
evaluation O
scores O
for O
the O
canonical O
and O
Xtreme O
test O
sets O
in O
Table O
4 O
. O
For O
both O
test O
sets O
, O
the O
summarization O
models O
are O
trained O
using O
the O
same O
training O
10https://github.com/Tiiiger/bert_scoreset O
, O
but O
they O
are O
tuned O
with O
a O
different O
development O
set O
( O
see O
Section O
2 O
for O
details O
) O
. O
In O
addition O
to O
the O
BERT O
models O
, O
we O
also O
include O
two O
pointergenerator O
models O
( O
See O
et O
al O
. O
, O
2017 O
): O
( O
1 O
) O
the O
base O
model O
( O
PTG O
EN O
) O
; O
and O
( O
2 O
) O
the O
model O
with O
coverage O
penalty O
( O
PTG O
EN+COV).11 O
We O
Ô¨Årst O
look O
at O
the O
baseline O
LEAD O
- O
NandORACLEresults O
. O
LEAD-2is O
the O
best O
LEAD O
- O
Nbaseline O
for O
Liputan6 O
. O
This O
is O
unsurprising O
, O
given O
that O
in O
Table O
2 O
, O
the O
average O
summary O
length O
was O
2 O
sentences O
. O
We O
also O
notice O
there O
is O
a O
substantial O
gap O
between O
ORACLE O
andLEAD-2 O
: O
12‚Äì15 O
points O
for O
R1 O
and O
5‚Äì7 O
points O
for O
BERTS O
CORE O
, O
depending O
on O
the O
test O
set O
. O
This O
suggests O
that O
the O
baseline O
of O
using O
the O
Ô¨Årst O
few O
sentences O
as O
an O
extractive O
summary O
is O
ineffective O
. O
Comparing O
the O
performance O
between O
the O
canonical O
and O
Xtreme O
test O
sets O
, O
we O
see O
a O
substantial O
drop O
in O
performance O
for O
both O
LEAD O
- O
Nand O
ORACLE O
, O
highlighting O
the O
difÔ¨Åculty O
of O
the O
Xtreme O
test O
set O
due O
to O
its O
increased O
abstractiveness O
. O
For O
the O
pointer O
- O
generator O
models O
, O
we O
see O
little O
improvement O
when O
including O
the O
coverage O
mechanism O
( O
PTG O
EN+COVvs O
. O
PTG O
EN O
) O
, O
implying O
that O
there O
is O
minimal O
repetition O
in O
the O
output O
of O
PTG O
EN O
. O
We O
suspect O
this O
is O
due O
to O
the O
Liputan6 O
summaries O
being O
relatively O
short O
( O
2 O
sentences O
with O
30 O
words O
on O
average O
) O
. O
A O
similar O
observation O
is O
reported O
by O
Narayan O
et O
al O
. O
( O
2018 O
) O
for O
XSum O
, O
where O
the O
summaries O
are O
similarly O
short O
( O
a O
single O
sentence O
with O
23 O
words O
, O
on O
average O
) O
. O
Next O
we O
look O
at O
the O
BERT O
models O
. O
Overall O
they O
perform O
very O
well O
, O
with O
both O
the O
mBERT O
and O
IndoBERT O
models O
outperforming O
the O
LEAD O
- O
Nbaselines O
and O
PTG O
ENmodels O
by O
a O
comfortable O
margin O
. O
IndoBERT O
is O
better O
than O
mBERT O
( O
approximately O
1 O
ROUGE O
point O
better O
on O
average O
over O
most O
metrics O
) O
, O
showing O
that O
a O
monolingually O
- O
trained O
BERT O
is O
a O
more O
effective O
pre O
- O
trained O
model O
than O
the O
multilingual O
variant O
. O
The O
best O
performance O
is O
achieved O
by O
IndoBERT O
‚Äôs O
BERTEXTABS O
. O
In O
the O
canonical O
test O
set O
, O
the O
improvement O
over O
LEAD-2is+4.4 O
R1 O
, O
+2.62 O
R2 O
, O
+4.3 O
R3 O
, O
and O
+3.4BERTS O
CORE O
points O
. O
In O
the O
Xtreme O
test O
set O
, O
BERTEXTABSsuffers O
a O
substantial O
drop O
compared O
to O
the O
canonical O
test O
set O
( O
6‚Äì7 O
ROUGE O
and O
2 O
BERTS O
CORE O
points O
) O
, O
although O
the O
performance O
gap O
between O
it O
and O
LEAD-2is O
about O
the O
same O
. O
11We O
use O
the O
default O
hyper O
- O
parameter O
conÔ¨Åguration O
recommended O
by O
the O
original O
authors O
for O
the O
pointer O
- O
generator O
models.602ModelCanonical O
Test O
Set O
Xtreme O
Test O
Set O
R1 O
R2 O
RL O
BS O
R1 O
R2 O
RL O
BS O
LEAD-1 O
32.67 O
18.50 O
29.40 O
72.62 O
27.27 O
11.56 O
23.60 O
71.19 O
LEAD-2 O
36.68 O
20.23 O
33.71 O
74.58 O
31.10 O
12.78 O
27.63 O
72.98 O
LEAD-3 O
34.49 O
18.84 O
32.06 O
74.31 O
29.54 O
12.05 O
26.68 O
72.78 O
ORACLE O
51.54 O
30.56 O
47.75 O
79.24 O
43.69 O
18.57 O
38.84 O
76.75 O
PTG O
EN O
36.10 O
19.19 O
33.56 O
75.92 O
30.41 O
12.05 O
27.51 O
74.10 O
PTG O
EN+COV O
35.53 O
18.56 O
32.92 O
75.75 O
30.27 O
11.81 O
27.26 O
74.11 O
BERTEXT(mBERT O
) O
37.51 O
20.15 O
34.57 O
75.22 O
31.83 O
12.63 O
28.37 O
73.62 O
BERTABS(mBERT O
) O
39.48 O
21.59 O
36.72 O
77.19 O
33.26 O
13.82 O
30.12 O
75.40 O
BERTEXTABS(mBERT O
) O
39.81 O
21.84 O
37.02 O
77.39 O
33.86 O
14.13 O
30.73 O
75.69 O
BERTEXT(IndoBERT O
) O
38.03 O
20.72 O
35.07 O
75.33 O
31.95 O
12.74 O
28.47 O
73.64 O
BERTABS(IndoBERT O
) O
40.94 O
23.01 O
37.89 O
77.90 O
34.59 O
15.10 O
31.19 O
75.84 O
BERTEXTABS(IndoBERT O
) O
41.08 O
22.85 O
38.01 O
77.93 O
34.84 O
15.03 O
31.40 O
75.99 O
Table O
4 O
: O
ROUGE O
results O
for O
the O
canonical O
and O
Xtreme O
test O
sets O
. O
All O
ROUGE O
( O
‚Äú O
R1 O
‚Äù O
, O
‚Äú O
R2 O
‚Äù O
, O
and O
‚Äú O
RL O
‚Äù O
) O
scores O
have O
a O
conÔ¨Ådence O
interval O
of O
at O
most O
¬±0.3 O
, O
as O
reported O
by O
the O
ofÔ¨Åcial O
ROUGE O
script O
. O
‚Äú O
BS O
‚Äù O
is O
BERS O
CORE O
computed O
withbert O
- O
base O
- O
multilingual O
- O
cased O
( O
layer O
9 O
) O
, O
as O
suggested O
by O
Zhang O
et O
al O
. O
( O
2020b O
) O
. O
5 O
Error O
Analysis O
In O
this O
section O
, O
we O
analyze O
errors O
made O
by O
the O
extractive O
( O
BERTEXT O
) O
and O
abstractive O
( O
BERTEXTABS O
) O
models O
to O
better O
understand O
their O
behaviour O
. O
We O
use O
the O
mBERT O
version O
of O
these O
models O
in O
our O
analysis.12 O
5.1 O
Error O
Analysis O
of O
Extractive O
Summaries O
We O
hypothesized O
that O
the O
disparity O
between O
ORACLE O
andBERTEXT(14.03 O
point O
difference O
for O
R1 O
in O
the O
canonical O
test O
set O
) O
was O
due O
to O
the O
number O
of O
extracted O
sentences O
. O
To O
test O
this O
, O
when O
extracting O
sentences O
with O
BERTEXT O
, O
we O
set O
the O
total O
number O
of O
extracted O
sentences O
to O
be O
the O
same O
as O
the O
number O
of O
sentences O
in O
the O
ORACLE O
summary O
. O
However O
, O
we O
found O
minimal O
beneÔ¨Åt O
using O
this O
approach O
, O
suggesting O
that O
the O
disparity O
is O
not O
a O
result O
of O
the O
number O
of O
extracted O
sentences O
. O
To O
investigate O
this O
further O
, O
we O
present O
the O
frequency O
of O
sentence O
positions O
that O
are O
used O
in O
the O
summary O
in O
ORACLE O
andBERTEXTfor O
the O
canonical O
test O
set O
in O
Figure O
4a O
. O
We O
can O
see O
that O
BERTEXT O
tends O
to O
over O
- O
select O
the O
Ô¨Årst O
two O
sentences O
as O
the O
summary O
. O
In O
terms O
of O
proportion O
, O
65.47 O
% O
of O
12The O
error O
analysis O
is O
based O
on O
mBERT O
rather O
than O
IndoBERT O
simply O
because O
this O
was O
the O
best O
- O
performing O
model O
at O
the O
time O
the O
error O
analysis O
was O
performed O
. O
While O
IndoBERT O
ultimately O
performed O
slightly O
better O
, O
given O
that O
the O
two O
models O
are O
structurally O
identical O
, O
we O
would O
expect O
to O
see O
a O
similar O
pattern O
of O
results O
. O
BERTEXTsummaries O
involve O
the O
Ô¨Årst O
two O
sentences O
. O
In O
comparison O
, O
only O
42.54 O
% O
of O
ORACLE O
summaries O
use O
sentences O
in O
these O
positions O
. O
One O
may O
argue O
that O
this O
is O
because O
the O
training O
and O
test O
data O
have O
different O
distributions O
under O
our O
chronological O
partitioning O
strategy O
( O
recall O
that O
the O
test O
set O
is O
sampled O
from O
the O
earliest O
articles O
) O
, O
but O
that O
does O
not O
appear O
to O
be O
the O
case O
: O
as O
Figure O
4b O
shows O
, O
the O
distribution O
of O
sentence O
positions O
in O
the O
training O
data O
is O
very O
similar O
to O
the O
test O
data O
‚Äî O
43.14 O
% O
of O
ORACLE O
summaries O
involve O
the O
Ô¨Årst O
two O
sentences O
. O
5.2 O
Error O
Analysis O
of O
Abstractive O
Summaries O
To O
perform O
error O
analysis O
for O
BERTEXTABS O
, O
we O
randomly O
sample O
100 O
documents O
with O
an O
R1 O
score O
< O
0.4 O
in O
the O
canonical O
test O
set O
( O
which O
accounts O
for O
nearly O
50 O
% O
of O
the O
test O
documents O
) O
. O
Two O
native O
Indonesian O
speakers O
examined O
these O
100 O
samples O
to O
manually O
assess O
the O
quality O
of O
the O
summaries O
, O
and O
score O
them O
on O
a O
3 O
- O
point O
ordinal O
scale O
: O
( O
1 O
) O
bad O
; O
( O
2 O
) O
average O
; O
and O
( O
3 O
) O
good O
. O
Each O
annotator O
is O
presented O
with O
the O
source O
document O
, O
the O
reference O
summary O
, O
and O
the O
summary O
generated O
by O
BERTEXTABS O
. O
In O
addition O
to O
the O
overall O
quality O
evaluation O
, O
we O
also O
asked O
the O
annotators O
to O
analyze O
a O
number O
of O
( O
Ô¨Ånegrained O
) O
attributes O
in O
the O
summaries O
: O
‚Ä¢Abbreviations O
: O
the O
system O
summary O
uses O
abbreviations O
that O
are O
different O
to O
the O
reference O
summary.603Position02000400060008000 O
1 O
2 O
3 O
4 O
5 O
6 O
8 O
10 O
12 O
14 O
15 O
16 O
17 O
18 O
19 O
20 O
> O
20GOLD O
PRED(a O
) O
Distribution O
of O
sentence O
positions O
for O
ORACLE O
and O
BERTEXTin O
the O
canonical O
test O
set O
. O
Position0250005000075000100000125000 O
1234567891011121314151617181920>20(b O
) O
Distribution O
of O
sentence O
positions O
for O
ORACLE O
in O
the O
training O
set O
. O
Figure O
4 O
: O
Position O
of O
O O
RACLE O
and/or O
Predicted O
Extractive O
Summaries O
Category O
Bad O
Avg O
. O
Good O
# O
Samples O
( O
100 O
) O
32 O
8 O
60 O
Abbreviation O
( O
% O
) O
21.9 O
25.0 O
40.0 O
Morphology O
( O
% O
) O
12.5 O
25.0 O
36.7 O
Paraphrasing O
( O
% O
) O
50.0 O
87.5 O
86.7 O
Lack O
of O
coverage O
( O
% O
) O
90.6 O
100.0 O
40.0 O
Wrong O
focus O
( O
% O
) O
68.8 O
0.00 O
8.3 O
Un O
. O
details O
( O
from O
doc O
) O
( O
% O
) O
90.6 O
75.0 O
75.0 O
Un O
. O
details O
( O
not O
from O
doc O
) O
( O
% O
) O
18.8 O
12.5 O
5.0 O
Table O
5 O
: O
Error O
analysis O
for O
100 O
samples O
with O
R1 O
< O
0.4 O
. O
‚Ä¢Morphology O
: O
the O
system O
summary O
uses O
morphological O
variants O
of O
the O
same O
lemmas O
contained O
in O
the O
reference O
summary O
. O
‚Ä¢Synonyms O
/ O
paraphrasing O
: O
the O
system O
summary O
contains O
paraphrases O
of O
the O
reference O
summary O
. O
‚Ä¢Lack O
of O
coverage O
: O
the O
system O
summary O
lacks O
coverage O
of O
certain O
details O
that O
are O
present O
in O
the O
reference O
summary O
. O
‚Ä¢Wrong O
focus O
: O
the O
system O
summarizes O
a O
different O
aspect O
/ O
focus O
of O
the O
document O
to O
the O
reference O
summary O
. O
‚Ä¢Unnecessary O
details O
( O
from O
document O
): O
the O
system O
summary O
includes O
unimportant O
but O
factually O
correct O
information O
. O
‚Ä¢Unnecessary O
details O
( O
not O
from O
document O
): O
the O
system O
summary O
includes O
unimportant O
and O
factually O
incorrect O
information O
( O
hallucinations O
) O
. O
We O
present O
a O
breakdown O
of O
the O
different O
error O
types O
in O
Table O
5 O
. O
Inter O
- O
annotator O
agreement O
for O
the O
overall O
quality O
assessment O
is O
high O
( O
Pearson O
‚Äôs O
r=0.69 O
) O
. O
Disagreements O
in O
the O
quality O
label O
( O
bad O
, O
average O
, O
good O
) O
are O
resolved O
as O
follows O
: O
( O
1 O
) O
{ O
bad O
, O
average}‚Üí O
bad O
; O
and O
( O
2){good O
, O
average}‚Üí O
good O
. O
We O
only O
have O
four O
examples O
with O
{ O
bad O
, O
good}disagreement O
, O
which O
we O
resolved O
through O
discussion O
. O
Interestingly O
, O
more O
than O
half O
( O
60 O
) O
of O
our O
samples O
were O
found O
to O
have O
good O
summaries O
. O
The O
primary O
reasons O
why O
these O
summaries O
have O
low O
ROUGE O
scores O
are O
paraphrasing O
( O
86.7 O
% O
) O
, O
and O
the O
inclusion O
of O
additional O
( O
but O
valid O
) O
details O
( O
75.0 O
% O
) O
. O
Abbreviations O
and O
morphological O
differences O
also O
appear O
to O
be O
important O
factors O
. O
These O
results O
underline O
a O
problem O
with O
the O
ROUGE O
metric O
, O
in O
that O
it O
is O
unable O
to O
detect O
good O
summaries O
that O
use O
a O
different O
set O
of O
words O
to O
the O
reference O
summary O
. O
One O
way O
forward O
is O
to O
explore O
metrics O
that O
consider O
sentence O
semantics O
beyond O
word O
overlap O
such O
as O
METEOR O
( O
Banerjee O
and O
Lavie O
, O
2005 O
) O
and O
BERTS O
CORE O
, O
13 O
and O
question O
- O
answering O
system O
based O
evaluation O
such O
as O
APES O
( O
Eyal O
et O
al O
. O
, O
2019 O
) O
and O
QAGS O
( O
Wang O
et O
al O
. O
, O
2020 O
) O
. O
Another O
way O
is O
to O
create O
more O
reference O
summaries O
( O
which O
will O
help O
with O
the O
issue O
of O
the O
system O
summaries O
including O
[ O
validly O
] O
different O
details O
to O
the O
single O
reference O
) O
. O
Looking O
at O
the O
results O
for O
average O
summaries O
( O
middle O
column O
) O
, O
BERTEXTABSoccasionally O
fails O
to O
capture O
salient O
information O
: O
100 O
% O
of O
the O
summaries O
have O
coverage O
issues O
, O
and O
75.0 O
% O
contain O
unnecessary O
( O
but O
valid O
) O
details O
. O
They O
also O
tend O
to O
use O
paraphrases O
( O
87.5 O
% O
) O
, O
which O
further O
impacts O
on O
a O
lower O
ROUGE O
score O
. O
Finally O
, O
the O
bad O
system O
summaries O
have O
similar O
coverage O
issues O
, O
and O
also O
tend O
to O
have O
a O
very O
different O
focus O
compared O
to O
the O
13Indeed O
, O
we O
suggest O
that O
BERTS O
CORE O
should O
be O
used O
as O
the O
canonical O
evaluation O
metric O
for O
the O
dataset O
, O
but O
leave O
empirical O
validation O
of O
its O
superiority O
for O
Indonesian O
summarization O
evaluation O
to O
future O
work.604Dokumen O
: O
	  O
Liputan6.com O
	 O
, O
	 O
Jakarta O
	 O
: O
	 O
Langkah O
	 O
reshuf O
fle O
	 O
yang O
	 O
dilakukan O
	 O
Presiden O
Abdurrahman O
	 O
W O
ahid O
	 O
, O
	 O
agaknya O
	 O
tak O
	 O
mendapat O
	 O
restu O
	 O
. O
	 O
Buktinya O
	 O
, O
	 O
W O
akil O
Presiden O
	 O
Megawati O
	 O
Sukarnoputri O
	 O
kembali O
	 O
tidak O
	 O
hadir O
	 O
dalam O
pelantikan O
	 O
tiga O
	 O
menteri O
	 O
bidang O
	 O
ekonomi O
	 O
, O
	 O
Rabu O
	 O
( O
	 O
13/6 O
	 O
) O
	 O
. O
	  O
[ O
8 O
	 O
kalimat O
	 O
dengan O
	 O
1 O
13 O
	 O
kata O
	 O
setelahnya O
	 O
tidak O
	 O
ditampilkan O
] O
Ringkasan O
	 O
manusia O
: O
wapres O
	 O
megawati O
	 O
sukarnoputri O
	 O
, O
	 O
kembali O
	 O
tidak O
	 O
hadir O
	  O
dalam O
	 O
pelantikan O
tiga O
	 O
menteri O
	 O
baru O
	 O
. O
	 O
dalam O
	 O
reshufle O
	 O
1 O
	 O
juni O
	 O
, O
	 O
megawati O
	 O
juga O
	 O
tak O
	 O
muncul O
dalam O
	 O
pelantikan O
	 O
, O
	  O
karena O
	 O
merasa O
	 O
tak O
	 O
dilibatkan O
	 O
dalam O
	 O
reshuf O
fle O
kabinet O
	  O
. O
Ringkasan O
	 O
sistem O
	 O
[ O
Bad O
] O
: O
presiden O
	 O
abdurrahman O
	 O
wahid O
	 O
kembali O
	 O
tidak O
	 O
hadir O
	 O
dalam O
	 O
pelantikan O
tiga O
	 O
menteri O
	 O
bidang O
	 O
ekonomi O
	 O
. O
	  O
ketidaksepakatan O
	 O
soal O
	 O
perombakan O
kabinet O
	 O
itu O
	 O
juga O
	 O
terjadi O
	 O
1 O
	 O
juni O
	 O
silam O
	 O
. O
	  O
presiden O
	 O
meminta O
	 O
mereka O
	 O
lebih O
menjaga O
	 O
koordinasi O
	 O
antarmenteri O
	 O
.Example-2 O
	 O
of O
	 O
error O
	 O
analysis O
	 O
( O
Lack O
	 O
of O
	 O
coverage O
, O
	 O
wrong O
	 O
focus O
, O
	 O
and O
	  O
details O
	 O
that O
	 O
are O
	 O
not O
	 O
from O
	 O
the O
	 O
document O
) O
Document O
: O
	  O
Liputan6.com O
, O
	 O
Jakarta O
: O
	 O
The O
	 O
reshuf O
fle O
	 O
step O
	 O
was O
	 O
taken O
	 O
by O
	 O
President O
Abdurrahman O
	 O
W O
ahid O
, O
	 O
apparently O
	 O
did O
	 O
not O
	 O
get O
	 O
the O
	 O
blessing O
. O
	 O
The O
	 O
proof O
, O
V O
ice O
	 O
President O
	 O
Megawati O
	 O
Sukarnoputri O
	 O
was O
	 O
again O
	 O
not O
	 O
present O
	 O
at O
	 O
the O
inauguration O
	 O
of O
	 O
three O
	 O
ministers O
	 O
in O
	 O
the O
	 O
economic O
	 O
sector O
, O
	 O
W O
ednesday O
( O
6/13 O
) O
. O
	  O
[ O
8 O
	 O
sentences O
	 O
with O
	 O
1 O
13 O
	 O
words O
	 O
are O
	 O
abbreviated O
	 O
from O
	 O
here O
] O
Gold O
	 O
Summary O
: O
V O
ice O
	 O
President O
	 O
Megawati O
	 O
Sukarnoputri O
, O
	 O
is O
	 O
not O
	 O
present O
	 O
a O
t O
	 O
the O
inauguration O
	 O
of O
	 O
three O
	 O
new O
	 O
ministers O
	 O
again O
. O
	 O
In O
	 O
the O
	 O
reshuf O
fle O
	 O
on O
	 O
June O
	 O
1 O
, O
Megawati O
	 O
also O
	 O
did O
	 O
not O
	 O
appear O
	 O
in O
	 O
the O
	 O
inauguration O
, O
	  O
because O
	 O
she O
	 O
felt O
	 O
not O
involved O
	 O
in O
	 O
the O
	 O
cabinet O
	 O
reshuf O
fle O
. O
System O
	 O
Summary O
	 O
[ O
Bad O
] O
: O
President O
	 O
Abdurrahman O
	 O
W O
ahid O
	 O
was O
	 O
again O
	 O
absent O
	  O
from O
	 O
the O
inauguration O
	 O
of O
	 O
three O
	 O
ministers O
	 O
in O
	 O
the O
	 O
economic O
	 O
sector O
. O
	 O
disagreement O
about O
	 O
the O
	 O
cabinet O
	 O
reshuf O
fle O
	 O
also O
	 O
occurred O
	 O
1 O
	 O
June O
	 O
ago O
. O
	 O
the O
	 O
president O
asked O
	 O
them O
	 O
to O
	 O
maintain O
	 O
more O
	 O
coordination O
	 O
between O
	 O
ministries O
. O
Dokumen O
: O
	  O
Liputan6.com O
	 O
, O
	 O
Jakarta O
	 O
: O
	 O
Protes O
	 O
masih O
	 O
ber O
gema O
	 O
menyambut O
Keputusan O
	 O
Menteri O
	 O
T O
enaga O
	 O
Kerja O
	 O
dan O
	 O
T O
ransmigrasi O
	 O
Nomor O
	 O
78 O
	 O
T O
ahun O
2001 O
	 O
. O
	 O
Kebijakan O
	 O
yang O
	 O
sengaja O
	 O
dikeluarkan O
	 O
sebagai O
	 O
wujud O
	 O
perubahan O
keputusan O
	 O
sebelumnya O
	 O
ini O
	 O
, O
	 O
sampai O
	 O
sekarang O
	 O
, O
	 O
masih O
	 O
mengundang O
kecaman O
	 O
keras O
	 O
dari O
	 O
pekerja O
	 O
di O
	 O
Indonesia O
	 O
. O
	 O
Itulah O
	 O
sebabnya O
	 O
, O
	 O
mereka O
menuntut O
	 O
Kepmenakertrans O
	 O
baru O
	 O
ini O
	 O
dicabut O
	 O
karena O
	 O
dinilai O
	 O
merugikan O
pekerja O
	 O
. O
	  O
[ O
19 O
	 O
kalimat O
	 O
dengan O
	 O
406 O
	 O
kata O
	 O
tidak O
	 O
ditampilkan O
] O
Sementara O
	 O
itu O
	 O
, O
	  O
SPSI O
	 O
secara O
	 O
tegas O
	 O
menolak O
	 O
segala O
	 O
bentuk O
	 O
negosiasi O
	  O
. O
	  O
[ O
3 O
	 O
kalimat O
	 O
dengan O
	 O
45 O
	 O
kata O
	 O
setelahnya O
	 O
tidak O
	 O
ditampilkan O
] O
Ringkasan O
	 O
manusia O
: O
pemberlakuan O
	  O
kepmenakertrans O
	 O
78/2001 O
	 O
masih O
	 O
mengundang O
	 O
rasa O
tidak O
	 O
puas O
	 O
di O
	 O
dada O
	 O
sejumlah O
	 O
pekerja O
	 O
indonesia O
	 O
. O
	 O
maka O
	 O
, O
	 O
lahirlah O
tuntutan O
	 O
agar O
	 O
peraturan O
	 O
yang O
	 O
dinilai O
	 O
merugikan O
	 O
ini O
	 O
dicabut O
	 O
. O
Ringkasan O
	 O
sistem O
	 O
[ O
Good O
] O
: O
	  O
keputusan O
	 O
menteri O
	 O
tenaga O
	 O
kerja O
	 O
dan O
	 O
transmigrasi O
	 O
nomor O
	 O
78 O
	 O
tahun O
2001 O
	 O
mengundang O
	 O
kecaman O
	 O
keras O
	 O
dari O
	 O
pekerja O
	 O
di O
	 O
indonesia O
	 O
. O
	 O
mereka O
menuntut O
	 O
kepmenakertrans O
	 O
dicabut O
	 O
karena O
	 O
dinilai O
	 O
merugikan O
	 O
pekerja O
	 O
. O
spsi O
	 O
menolak O
	 O
negosiasi O
	  O
.Example-1 O
	 O
of O
	 O
error O
	 O
analysis O
	 O
( O
Abbreviation O
, O
	 O
morphoplogy O
, O
	 O
synonyms O
/ O
paraphrashing O
, O
	 O
and O
	  O
details O
	 O
from O
	 O
the O
	 O
document O
) O
Document O
: O
	  O
Liputan6.com O
, O
	 O
Jakarta O
: O
	 O
Protests O
	 O
still O
	 O
resonate O
	 O
with O
	 O
welcoming O
Minister O
	 O
of O
	 O
Manpower O
	 O
and O
	 O
T O
ransmigration O
	 O
Decree O
	 O
No O
. O
	 O
78/2001 O
. O
	 O
This O
policy O
, O
	 O
which O
	 O
was O
	 O
deliberately O
	 O
issued O
	 O
as O
	 O
an O
	 O
amendment O
	 O
to O
	 O
the O
previous O
	 O
decision O
, O
	 O
until O
	 O
now O
, O
	 O
still O
	 O
invites O
	 O
harsh O
	 O
criticism O
	 O
from O
	 O
workers O
in O
	 O
Indonesia O
. O
	 O
That O
	 O
is O
	 O
why O
	 O
they O
	 O
demand O
	 O
to O
	 O
revoke O
	 O
the O
	 O
new O
Kepmenakertrans O
	 O
because O
	 O
it O
	 O
is O
	 O
considered O
	 O
detrimental O
	 O
to O
	 O
workers O
. O
[ O
19 O
	 O
sentences O
	 O
with O
	 O
406 O
	 O
words O
	 O
are O
	 O
abbreviated O
	 O
from O
	 O
here O
] O
Meanwhile O
, O
	  O
SPSI O
	 O
firmly O
	 O
rejected O
	 O
all O
	 O
forms O
	 O
of O
	 O
negotiation O
. O
[ O
3 O
	 O
sentences O
	 O
with O
	 O
45 O
	 O
words O
	 O
are O
	 O
abbreviated O
	 O
from O
	 O
here O
] O
Gold O
	 O
Summary O
: O
The O
	 O
enactment O
	 O
of O
	  O
Kepmenakertrans O
	 O
78/2001 O
	 O
still O
	 O
invites O
	 O
the O
dissatisfaction O
	 O
of O
	 O
Indonesian O
	 O
workers O
. O
	 O
hence O
, O
	  O
demands O
	 O
to O
	 O
revoke O
	 O
the O
regulation O
	 O
arose O
	 O
as O
	 O
it O
	 O
was O
	 O
considered O
	 O
to O
	 O
be O
	 O
detrimental O
. O
System O
	 O
Summary O
	 O
[ O
Good O
] O
: O
Minister O
	 O
of O
	 O
Manpower O
	 O
and O
	 O
T O
ransmigration O
	 O
Decree O
	  O
number O
	 O
78 O
	 O
of O
	 O
2001 O
invited O
	 O
strong O
	 O
criticism O
	 O
from O
	 O
workers O
	 O
in O
	 O
Indonesia O
. O
	 O
They O
	  O
demand O
	 O
to O
revoke O
	 O
Kepmenakertrans O
	 O
because O
	 O
it O
	 O
is O
	 O
considered O
	 O
detrimental O
	 O
to O
workers O
. O
	  O
SPSI O
	 O
rejects O
	 O
negotiations O
. O
Figure O
5 O
: O
Two O
examples O
to O
highlight O
error O
categories O
used O
in O
our O
error O
analysis O
. O
reference O
summary O
( O
90.6 O
% O
) O
. O
In O
Figure O
5 O
we O
show O
two O
representative O
examples O
from O
BERTEXTABS O
. O
The O
Ô¨Årst O
example O
is O
considered O
good O
by O
our O
annotators O
, O
but O
due O
to O
abbreviations O
, O
morphological O
differences O
, O
paraphrasing O
, O
and O
additional O
details O
compared O
to O
the O
reference O
summary O
, O
the O
ROUGE O
score O
is O
< O
0.4 O
. O
In O
this O
example O
, O
the O
gold O
summary O
uses O
the O
abbreviation O
kepmenakertrans O
while O
BERTEXTABSgenerates O
the O
full O
phrase O
keputusan O
menteri O
tenaga O
kerja O
dan O
transmigrasi O
( O
which O
is O
correct O
) O
. O
The O
example O
also O
uses O
paraphrases O
( O
invites O
strong O
criticism O
to O
explain O
dissatisfaction O
) O
, O
and O
there O
are O
morphological O
differences O
in O
words O
such O
as O
tuntutan O
( O
noun O
) O
vs. O
menuntut(verb O
) O
. O
The O
low O
ROUGE O
score O
here O
highlights O
the O
fact O
that O
the O
bigger O
issue O
is O
with O
ROUGE O
itself O
rather O
than O
the O
summary O
. O
The O
second O
example O
is O
considered O
to O
be O
bad O
, O
with O
the O
following O
issues O
: O
lack O
of O
coverage O
, O
wrong O
focus O
, O
and O
contains O
unnecessary O
details O
that O
are O
not O
from O
the O
article O
. O
The O
Ô¨Årst O
sentence O
President O
Abdurrahman O
Wahid O
was O
absent O
has O
nothing O
to O
dowith O
the O
original O
article O
, O
creating O
a O
different O
focus O
( O
and O
confusion O
) O
in O
the O
overall O
summary O
. O
To O
summarize O
, O
coverage O
, O
focus O
, O
and O
the O
inclusion O
of O
other O
details O
are O
the O
main O
causes O
of O
low O
quality O
summaries O
. O
Our O
analysis O
reveals O
that O
abbreviations O
and O
paraphrases O
are O
another O
cause O
of O
summaries O
with O
low O
ROUGE O
scores O
, O
but O
that O
is O
an O
issue O
with O
ROUGE O
rather O
than O
the O
summaries O
. O
Encouragingly O
, O
hallucination O
( O
generating O
details O
not O
in O
the O
original O
document O
) O
is O
not O
a O
major O
issue O
for O
these O
models O
( O
notwithstanding O
that O
almost O
20 O
% O
of O
badsamples O
contain O
hallucinations O
) O
. O
6 O
Related O
Datasets O
Previous O
studies O
on O
Indonesian O
text O
summarization O
have O
largely O
been O
extractive O
and O
used O
small O
- O
scale O
datasets O
. O
Gunawan O
et O
al O
. O
( O
2017 O
) O
developed O
an O
unsupervised O
summarization O
model O
over O
3 O
K O
news O
articles O
using O
heuristics O
such O
as O
sentence O
length O
, O
keyword O
frequency O
, O
and O
title O
features O
. O
In O
a O
similar O
vein O
, O
Najibullah O
( O
2015 O
) O
trained O
a O
naive O
Bayes O
model O
to O
extract O
summary O
sentences O
in O
a O
100 O
- O
article O
dataset.605Aristoteles O
et O
al O
. O
( O
2012 O
) O
and O
Silvia O
et O
al O
. O
( O
2014 O
) O
apply O
genetic O
algorithms O
to O
a O
summarization O
dataset O
with O
less O
than O
200 O
articles O
. O
These O
studies O
do O
not O
use O
ROUGE O
for O
evaluation O
, O
and O
the O
datasets O
are O
not O
publicly O
available O
. O
Koto O
( O
2016 O
) O
released O
a O
dataset O
for O
chat O
summarization O
by O
manually O
annotating O
chat O
logs O
from O
WhatsApp O
.14However O
, O
this O
dataset O
contains O
only O
300 O
documents O
. O
The O
largest O
summarization O
data O
to O
date O
is O
IndoSum O
( O
Kurniawan O
and O
Louvan O
, O
2018 O
) O
, O
which O
has O
approximately O
19 O
K O
news O
articles O
with O
manually O
- O
written O
summaries O
. O
Based O
on O
our O
analysis O
, O
however O
, O
the O
summaries O
of O
IndoSum O
are O
highly O
extractive O
. O
Beyond O
Indonesian O
, O
there O
is O
only O
a O
handful O
of O
non O
- O
English O
summarization O
datasets O
that O
are O
of O
sufÔ¨Åcient O
size O
to O
train O
modern O
deep O
learning O
summarization O
methods O
over O
, O
including O
: O
( O
1 O
) O
LCSTS O
( O
Hu O
et O
al O
. O
, O
2015 O
) O
, O
which O
contains O
2 O
million O
Chinese O
short O
texts O
constructed O
from O
the O
Sina O
Weibo O
microblogging O
website O
; O
and O
( O
2 O
) O
ES O
- O
News O
( O
Gonzalez O
et O
al O
. O
, O
2019 O
) O
, O
which O
comprises O
270k O
Spanish O
news O
articles O
with O
summaries O
. O
LCSTS O
documents O
are O
relatively O
short O
( O
less O
than O
140 O
Chinese O
characters O
) O
, O
while O
ES O
- O
News O
is O
not O
publicly O
available O
. O
Our O
goal O
is O
to O
create O
a O
benchmark O
corpus O
for O
Indonesian O
text O
summarization O
that O
is O
both O
large O
scale O
and O
publicly O
available O
. O
7 O
Conclusion O
We O
release O
Liputan6 O
, O
a O
large O
- O
scale O
summarization O
corpus O
for O
Indonesian O
. O
Our O
dataset O
comes O
with O
two O
test O
sets O
: O
a O
canonical O
test O
set O
and O
an O
‚Äú O
Xtreme O
‚Äù O
variant O
that O
is O
more O
abstractive O
. O
We O
present O
results O
for O
several O
benchmark O
summarization O
models O
, O
in O
part O
based O
on O
IndoBERT O
, O
a O
new O
pre O
- O
trained O
BERT O
model O
for O
Indonesian O
. O
We O
further O
conducted O
extensive O
error O
analysis O
, O
as O
part O
of O
which O
we O
identiÔ¨Åed O
a O
number O
of O
issues O
with O
ROUGE O
- O
based O
evaluation O
for O
Indonesian O
. O
Acknowledgments O
We O
are O
grateful O
to O
the O
anonymous O
reviewers O
for O
their O
helpful O
feedback O
and O
suggestions O
. O
In O
this O
research O
, O
Fajri O
Koto O
is O
supported O
by O
the O
Australia O
Awards O
Scholarship O
( O
AAS O
) O
, O
funded O
by O
the O
Department O
of O
Foreign O
Affairs O
and O
Trade O
( O
DFAT O
) O
, O
Australia O
. O
This O
research O
was O
undertaken O
using O
the O
LIEF O
HPC O
- O
GPGPU O
Facility O
hosted O
at O
The O
University O
of O
14https://www.whatsapp.com/ O
.Melbourne O
. O
This O
facility O
was O
established O
with O
the O
assistance O
of O
LIEF O
Grant O
LE170100200 O
. O
Abstract O
Sports O
game O
summarization O
focuses O
on O
generating O
news O
articles O
from O
live O
commentaries O
. O
Unlike O
traditional O
summarization O
tasks O
, O
the O
source O
documents O
and O
the O
target O
summaries O
for O
sports O
game O
summarization O
tasks O
are O
written O
in O
quite O
different O
writing O
styles O
. O
In O
addition O
, O
live O
commentaries O
usually O
contain O
many O
named O
entities O
, O
which O
makes O
summarizing O
sports O
games O
precisely O
very O
challenging O
. O
To O
deeply O
study O
this O
task O
, O
we O
present O
S O
PORTS O
SUM1 O
, O
a O
Chinese O
sports O
game O
summarization O
dataset O
which O
contains O
5,428 O
soccer O
games O
of O
live O
commentaries O
and O
the O
corresponding O
news O
articles O
. O
Additionally O
, O
we O
propose O
a O
two O
- O
step O
summarization O
model O
consisting O
of O
a O
selector O
and O
a O
rewriter O
for O
S O
PORTS O
SUM O
. O
To O
evaluate O
the O
correctness O
of O
generated O
sports O
summaries O
, O
we O
design O
two O
novel O
score O
metrics O
: O
name O
matching O
score O
and O
event O
matching O
score O
. O
Experimental O
results O
show O
that O
our O
model O
performs O
better O
than O
other O
summarization O
baselines O
on O
ROUGE O
scores O
as O
well O
as O
the O
two O
designed O
scores O
. O
1 O
Introduction O
There O
are O
a O
large O
number O
of O
sports O
games O
playing O
every O
day O
. O
Apparently O
, O
manually O
writing O
sports O
news O
articles O
to O
summarize O
every O
game O
is O
laborintensive O
and O
infeasible O
. O
How O
to O
automatically O
generate O
sports O
summaries O
, O
therefore O
, O
becomes O
a O
popular O
and O
demanding O
task O
. O
Recently O
, O
generating O
news O
from O
live O
commentaries O
has O
gradually O
attracted O
attention O
in O
the O
academic O
community O
( O
Zhang O
et O
al O
. O
, O
2016 O
; O
Yao O
et O
al O
. O
, O
2017 O
) O
. O
At O
the O
same O
time O
, O
several O
trials O
have O
been O
done O
in O
the O
industry O
such O
as O
sports O
news O
from O
Toutiao O
‚Äôs O
Xiaoming O
Bot2 O
, O
Sohu O
Ruibao3 O
and O
AI O
football O
news4 O
. O
1The O
dataset O
is O
available O
at O
https://github.com/ O
ej0cl6 O
/ O
SportsSum O
2http://www.nbd.com.cn/columns/803 O
3https://mp.sohu.com/profile?xpt= O
c29odW1wMzZpdDlzQHNvaHUuY29 O
t O
4https://www.51zhanbao.comLive O
Commentary O
Time O
Scores O
Commentary O
Sentence O
66 O
‚Äô O
0 O
- O
0Â§öÁâπËíôÂæ∑ÁêÉÂëòÊ†ºÁ≠ñÊãºÊä¢ÁäØËßÑ O
, O
ÂØπÊâãËé∑ÂæóÊéßÁêÉÊùÉ O
. O
Dortmund O
‚Äôs O
player O
G O
¬®otze O
fouled O
, O
and O
the O
opponent O
got O
the O
possession O
of O
the O
ball O
. O
66 O
‚Äô O
0 O
- O
0ÊñΩÈ≠èÂõ†ÊñØÊ≥∞Ê†º‰∏∫Êãú‰ªÅÊÖïÂ∞ºÈªëËµ¢Âæó‰∏Ä‰∏™‰ªªÊÑèÁêÉ O
. O
Schweinsteiger O
got O
a O
free O
kick O
for O
Bayern O
Munich O
. O
67 O
‚Äô O
1 O
- O
0ËøõÁêÉÂï¶ÔºÅÔºÅÔºÅÊãú‰ªÅÊÖïÂ∞ºÈªëÁêÉÂëòÂÖãÁΩóÊñØÂ§ßÁ¶ÅÂå∫Â§ñ O
Â∑¶ËÑöÂ∞ÑÈó® O
, O
ÁêÉ‰ªéÂè≥‰∏ãËßíÈ£ûËøõÁêÉÈó® O
, O
ÁêÉËøõ‰∫Ü!Âä©ÊîªÁöÑ O
ÊòØÁ©ÜÂãí O
. O
Êãú‰ªÅÊÖïÂ∞ºÈªë1 O
- O
0Â§öÁâπËíôÂæ∑ O
. O
Goal O
! O
! O
! O
Bayern O
Munich O
‚Äôs O
player O
Kroos O
shot O
with O
his O
left O
foot O
from O
the O
outside O
of O
the O
penalty O
area O
. O
The O
ball O
Ô¨Çew O
into O
the O
goal O
through O
the O
lower O
right O
corner O
. O
The O
ball O
went O
in O
! O
Muller O
gave O
the O
assist O
. O
Bayern O
Munich O
1 O
- O
0 O
Dortmund O
. O
71 O
‚Äô O
1 O
- O
0Êãú‰ªÅÊÖïÂ∞ºÈªëÁêÉÂëòÈáåË¥ùÈáåÂ§ßÁ¶ÅÂå∫Â∑¶‰æßÂ∞ùËØïÂè≥ËÑöÂ∞Ñ O
Èó® O
, O
ÂèØÊÉúÁöÆÁêÉÈ´òÂá∫ÁêÉÈó® O
. O
Áªô‰ªñ‰º†ÁêÉÁöÑÊòØÊãâÂßÜ O
. O
Bayern O
Munich O
‚Äôs O
player O
Ribery O
tried O
to O
shoot O
with O
his O
right O
foot O
from O
the O
penalty O
area O
‚Äôs O
left O
side O
, O
but O
the O
ball O
was O
higher O
than O
the O
crossbar O
. O
Lahm O
passed O
the O
ball O
to O
him O
. O
Sports O
News O
Ariticle O
ÂºÄÂú∫3ÂàÜÈíüÔºåÂÖãÁΩóÊñØÂ∑¶‰æß‰ªªÊÑèÁêÉË¢´È°∂Âà∞ÂêéÁÇπÔºåÈáåË¥ùÈáåÁ¶ÅÂå∫ËæπÁºòÊäΩÂ∞Ñ O
ÂÅèÂá∫ËøëÈó®Êü±„ÄÇÁ¨¨8ÂàÜÈíüÔºåÁ©ÜÂãíÂè≥Ë∑Ø‰∏éÊõºÊú±Âü∫Â•áÊâìÂá∫Ë∏¢Â¢ôÈÖçÂêàÔºåÂú®Èó® O
Ââç12Á±≥Â§ÑÊé®Â∞ÑË¢´ËãèÂçöËíÇÂ•áÈì≤Âá∫Â∫ïÁ∫ø„ÄÇÁ¨¨13ÂàÜÈíüÔºåÈáåË¥ùÈáåÂè≥Ë∑ØÂ°ûÁêÉ O
Ôºå O
ÂÖãÁΩóÊñØÂú®Èó®Ââç27Á±≥Â§ÑÊäΩÂ∞ÑÂÅèÂá∫ËøëÈó®Êü± O
„ÄÇ O
( O
... O
) O
In O
the O
3rd O
minutes O
, O
Kroos O
‚Äôs O
free O
kick O
on O
the O
left O
was O
tipped O
to O
the O
back O
, O
and O
Ribery O
‚Äôs O
shot O
from O
the O
penalty O
area O
missed O
. O
In O
the O
8th O
minute O
, O
Muller O
and O
Mandzukic O
had O
teamwork O
, O
and O
Muller O
‚Äôs O
shot O
from O
the O
12 O
meters O
ahead O
the O
goal O
line O
was O
touched O
out O
by O
Suboti O
¬¥ O
c. O
In O
the O
13th O
minute O
, O
Ribery O
passed O
the O
ball O
from O
the O
right O
, O
and O
Kroos O
‚Äôs O
shot O
near O
the O
27 O
meters O
ahead O
the O
goal O
line O
missed O
. O
( O
... O
) O
Table O
1 O
: O
An O
example O
of O
S O
PORTS O
SUMdataset O
. O
Unlike O
traditional O
text O
summarization O
tasks O
( O
Hermann O
et O
al O
. O
, O
2015 O
; O
Rush O
et O
al O
. O
, O
2015 O
) O
, O
the O
source O
documents O
and O
the O
target O
summaries O
for O
sports O
game O
summarization O
tasks O
are O
written O
in O
quite O
different O
styles O
. O
Live O
commentaries O
are O
the O
real O
- O
time O
transcripts O
of O
the O
commentators O
. O
Accordingly O
, O
commentary O
sentences O
are O
more O
colloquial O
and O
informal O
. O
In O
contrast O
, O
news O
summaries O
are O
usually O
more O
narrative O
and O
well O
- O
organized O
since O
they O
are O
written O
after O
the O
games O
. O
In O
addition O
, O
commentaries O
contain O
a O
large O
number O
of O
player O
names O
. O
One O
player O
can O
be O
referred O
to O
multiple O
times O
in O
the O
whole O
game O
, O
and O
one O
commentary O
sentence O
may O
mention O
multiple O
player O
names O
simultaneously O
. O
Those O
properties609make O
sports O
games O
summarization O
tasks O
very O
challenging O
. O
In O
this O
paper O
, O
we O
present O
SPORTS O
SUM O
, O
a O
Chinese O
dataset O
for O
studying O
sports O
game O
summarization O
tasks O
. O
We O
collect O
5,428 O
pairs O
of O
live O
commentaries O
and O
news O
articles O
from O
seven O
famous O
soccer O
leagues O
. O
To O
the O
best O
of O
our O
knowledge O
, O
SPORTS O
SUMis O
the O
largest O
Chinese O
sports O
game O
summarization O
dataset O
. O
In O
addition O
, O
we O
propose O
a O
two O
- O
step O
summarization O
model O
for O
SPORTS O
SUM O
, O
which O
learns O
a O
selector O
to O
extract O
important O
commentary O
sentences O
and O
trains O
a O
rewriter O
to O
convert O
the O
selected O
sentences O
to O
a O
news O
article O
. O
To O
encourage O
the O
model O
to O
capture O
the O
relations O
between O
players O
and O
actions O
better O
, O
we O
replace O
all O
the O
player O
names O
in O
the O
training O
sentences O
with O
a O
special O
token O
and O
train O
the O
proposed O
model O
on O
the O
modiÔ¨Åed O
template O
- O
like O
sentences O
. O
The O
proposed O
model O
performs O
better O
than O
existing O
extractive O
and O
abstractive O
summarization O
baseline O
models O
in O
ROUGE O
scores O
( O
Lin O
, O
2004 O
) O
. O
However O
, O
we O
observe O
that O
ROUGE O
scores O
can O
not O
evaluate O
the O
correctness O
of O
generated O
summaries O
very O
well O
. O
Therefore O
, O
we O
design O
two O
new O
scores O
, O
name O
matching O
score O
andevent O
matching O
score O
, O
as O
the O
auxiliary O
metrics O
for O
SPORTS O
SUM O
. O
Our O
experimental O
results O
demonstrate O
that O
the O
proposed O
model O
is O
superior O
to O
the O
baseline O
models O
in O
all O
the O
metrics O
. O
Summarizing O
documents O
between O
two O
articles O
written O
in O
different O
styles O
and O
involving O
many O
named O
entities O
is O
not O
limited O
to O
the O
sports O
game O
summarization O
tasks O
. O
There O
are O
many O
possible O
applications O
, O
such O
as O
summarizing O
events O
from O
tweets O
and O
summarizing O
trends O
from O
forum O
comments O
. O
We O
hope O
that O
SPORTS O
SUMprovides O
a O
potential O
research O
platform O
to O
develop O
advanced O
techniques O
for O
this O
type O
of O
summarization O
tasks O
. O
2 O
Dataset O
We O
present O
SPORTS O
SUM O
, O
a O
sports O
game O
summarization O
dataset O
in O
Chinese O
. O
Data O
collection O
. O
We O
crawl O
the O
records O
of O
soccer O
games O
from O
Sina O
Sports O
Live5 O
. O
The O
collected O
records O
contain O
soccer O
games O
in O
seven O
different O
leagues O
( O
Bundesliga O
, O
CSL O
, O
Europa O
, O
La O
Liga O
, O
Ligue O
1 O
, O
PL O
, O
Series O
A O
, O
UCL O
) O
from O
2012 O
to O
2018 O
. O
For O
each O
game O
, O
we O
have O
a O
live O
commentary O
document O
Cand O
a O
news O
article O
R O
, O
as O
illustrated O
in O
Table O
1 O
. O
The O
live O
commentary O
document O
Ccon5https://match.sports.sina.com.cn/League O
# O
of O
games O
Bundesliga O
453 O
CSL O
1371 O
Europa O
143 O
La O
Liga O
713 O
Ligue O
1 O
161 O
Premier O
League O
1220 O
Serie O
A O
890 O
UCL O
477 O
All O
5428 O
Table O
2 O
: O
The O
number O
of O
games O
in O
different O
leagues O
. O
SourceAvg O
. O
# O
charsAvg O
. O
# O
wordsAvg O
. O
# O
sent O
. O
Total O
# O
vocab O
Commentary O
3459.97 O
1825.63 O
193.77 O
43482 O
News O
801.11 O
427.98 O
23.80 O
21294 O
Table O
3 O
: O
Statistics O
of O
S O
PORTS O
SUMdataset O
. O
sists O
of O
a O
series O
of O
tuples O
( O
ti O
, O
si O
, O
ci O
) O
, O
where O
tiis O
the O
timeline O
information O
, O
sirepresents O
the O
current O
scores O
, O
and O
cidenotes O
the O
commentary O
sentence O
. O
The O
news O
article O
Rconsists O
of O
several O
news O
sentences O
ri O
. O
In O
addition O
to O
commentaries O
and O
news O
reports O
, O
we O
also O
include O
some O
metadata O
, O
such O
as O
rosters O
, O
starting O
lineups O
, O
and O
player O
positions O
, O
which O
is O
potentially O
helpful O
for O
sports O
game O
summarization O
tasks O
. O
Data O
cleaning O
. O
The O
crawled O
live O
commentary O
documents O
and O
news O
articles O
are O
quite O
noisy O
. O
Therefore O
, O
we O
apply O
multiple O
steps O
of O
data O
cleaning O
to O
improve O
the O
quality O
of O
the O
dataset O
. O
We O
Ô¨Årst O
remove O
all O
the O
HTML O
tags O
from O
the O
commentary O
documents O
and O
the O
news O
articles O
. O
Then O
, O
we O
observe O
that O
there O
are O
usually O
some O
descriptions O
that O
can O
not O
be O
directly O
inferred O
from O
the O
commentaries O
at O
the O
beginning O
of O
news O
articles O
, O
such O
as O
matching O
history O
. O
Hence O
, O
we O
design O
a O
heuristic O
rule O
to O
remove O
those O
descriptions O
. O
We O
identify O
several O
starting O
keywords O
which O
can O
indicate O
the O
start O
of O
a O
game O
, O
such O
as O
‚Äú O
‰∏Ä O
ÂºÄÂú∫(at O
the O
beginning O
of O
the O
game O
) O
‚Äù O
and O
‚Äú O
ÂºÄÂú∫ O
Âêé(after O
the O
game O
started O
) O
‚Äù O
. O
The O
full O
list O
of O
starting O
keywords O
can O
be O
found O
in O
Appendix O
A. O
Once O
we O
see O
a O
starting O
keyword O
appearing O
in O
a O
news O
report O
, O
we O
remove O
all O
the O
sentences O
before O
the O
starting O
keyword O
. O
Finally O
, O
we O
discard O
those O
games O
with O
the O
number O
of O
news O
sentences O
being O
less O
than O
5 O
and O
the O
number O
of O
commentary O
sentences O
being O
less O
than O
20 O
. O
After O
data O
cleaning O
, O
we O
have O
5,428 O
games O
remaining O
( O
detailed O
numbers O
of O
games O
are O
shown O
in O
Table O
2 O
) O
. O
Notice O
that O
SPORTS O
SUM(5,428 O
games O
) O
is O
much O
larger O
than O
the O
only O
public O
sports O
game O
summariza-610tion O
dataset O
( O
150 O
games O
) O
( O
Zhang O
et O
al O
. O
, O
2016 O
) O
. O
Statistics O
and O
properties O
. O
Table O
3 O
shows O
the O
statistics O
of O
SPORTS O
SUM O
. O
On O
average O
, O
there O
are O
193.77 O
sentences O
per O
commentary O
document O
and O
23.80 O
sentences O
per O
news O
article O
. O
After O
applying O
word O
segmentation O
by O
pyltp O
tool6 O
, O
the O
average O
numbers O
of O
words O
for O
commentary O
documents O
and O
news O
reports O
are O
1825.63 O
and O
427.98 O
, O
respectively O
. O
As O
mentioned O
in O
Section O
1 O
, O
commentary O
sentences O
and O
news O
sentences O
are O
in O
quite O
different O
writing O
styles O
. O
Commentary O
sentences O
are O
more O
colloquial O
and O
informal O
, O
while O
news O
sentences O
are O
more O
narrative O
and O
well O
- O
organized O
. O
Also O
, O
commentaries O
contain O
a O
large O
number O
of O
player O
names O
, O
which O
makes O
the O
model O
easy O
to O
generate O
news O
reports O
with O
incorrect O
facts O
, O
as O
shown O
in O
Section O
3 O
. O
3 O
Sports O
Game O
Summarization O
The O
goal O
of O
sports O
game O
summarization O
is O
to O
generate O
a O
sports O
news O
report O
ÀúR={Àúr1,Àúr2 O
, O
.. O
, O
Àúrn O
} O
from O
a O
given O
live O
commentary O
document O
C= O
{ O
( O
t1 O
, O
s1 O
, O
c1 O
) O
, O
... O
, O
( O
tm O
, O
sm O
, O
cm O
) O
} O
. O
The O
generated O
news O
report O
ÀúRis O
expected O
to O
cover O
most O
of O
the O
important O
events O
in O
the O
games O
and O
describe O
those O
events O
correctly O
. O
In O
this O
paper O
, O
we O
propose O
a O
twostep O
model O
for O
SPORTS O
SUM O
. O
The O
proposed O
model O
Ô¨Årst O
learns O
a O
selector O
to O
extract O
important O
commentary O
sentences O
and O
then O
utilizes O
a O
rewriter O
to O
convert O
the O
selected O
sentences O
to O
a O
news O
article O
. O
Sentence O
mapping O
. O
To O
train O
the O
selector O
and O
rewriter O
, O
we O
need O
some O
labels O
to O
indicate O
the O
importance O
of O
commentary O
sentences O
and O
the O
corresponding O
news O
sentences O
. O
To O
obtain O
the O
labels O
, O
we O
consider O
the O
timeline O
information O
and O
BERTScore O
( O
Zhang O
et O
al O
. O
, O
2020 O
) O
, O
a O
metric O
to O
measure O
the O
sentence O
similarity O
, O
and O
map O
each O
news O
sentence O
to O
a O
commentary O
sentence O
. O
Although O
we O
have O
no O
explicit O
timeline O
information O
for O
news O
sentences O
, O
we O
observe O
that O
many O
news O
sentences O
start O
with O
‚Äú O
in O
the O
n O
- O
th O
minute O
‚Äù O
and O
thus O
we O
can O
extract O
the O
timeline O
information O
for O
some O
news O
sentences O
. O
We O
map O
sentences O
by O
the O
following O
steps O
: O
1 O
) O
For O
each O
news O
sentence O
ri O
, O
we O
extract O
the O
timeline O
information O
hiif O
possible O
. O
Otherwise O
, O
we O
do O
not O
map O
this O
news O
sentence O
. O
2)We O
consider O
those O
commentary O
sentences O
cjwithtjbeing O
close O
tohi O
. O
More O
speciÔ¨Åcally O
, O
we O
consider O
C(i)= O
{ O
ck O
, O
ck+1 O
, O
... O
c O
k+l O
} O
, O
where O
cjis O
the O
commentary O
sentence O
with O
timeline O
information O
tj‚àà[hi O
, O
hi+3 O
] O
6https://github.com/HIT-SCIR/pyltpfork‚â§j‚â§k+l.3)We O
compute O
BERTScore O
of O
the O
news O
sentence O
riand O
all O
the O
commentary O
sentences O
in O
C(i O
) O
. O
The O
commentary O
sentence O
cj‚ààC(i O
) O
with O
the O
highest O
score O
is O
considered O
to O
be O
mapped O
with O
the O
news O
sentences O
ri O
. O
With O
the O
above O
mapping O
process O
, O
we O
obtain O
a O
set O
of O
mapped O
commentary O
sentences O
and O
news O
sentencesD={(¬Øc1,¬Ør1),(¬Øc2,¬Ør2 O
) O
, O
... O
, O
( O
¬Øcs,¬Ørs O
) O
} O
, O
which O
can O
be O
used O
for O
training O
our O
selector O
and O
rewriter O
. O
Selector O
. O
There O
are O
many O
commentary O
sentences O
in O
a O
live O
commentary O
document O
, O
but O
only O
few O
of O
them O
contain O
valuable O
information O
and O
should O
be O
reported O
in O
the O
news O
article O
. O
Therefore O
, O
we O
learn O
aselector O
to O
pick O
up O
those O
important O
sentences O
. O
More O
speciÔ¨Åcally O
, O
Given O
a O
commentary O
document O
C={(t1 O
, O
s1 O
, O
c1 O
) O
, O
... O
, O
( O
tm O
, O
sm O
, O
cm O
) O
} O
, O
the O
selector O
outputs O
a O
set O
Cselect O
= O
{ O
Àúc1,Àúc2 O
, O
... O
, O
Àúcn}which O
contains O
only O
important O
commentary O
sentences O
. O
We O
train O
a O
binary O
classiÔ¨Åer O
as O
the O
selector O
to O
choose O
important O
commentary O
sentences O
. O
When O
training O
, O
for O
each O
commentary O
sentence O
ciinC O
, O
we O
assign O
a O
positive O
label O
if O
cican O
be O
mapped O
with O
a O
news O
sentence O
by O
the O
aforementioned O
mapping O
process O
. O
Otherwise O
, O
we O
give O
a O
negative O
label O
. O
Rewriter O
. O
The O
rewriter O
converts O
the O
selected O
commentary O
sentences O
Cselect O
= O
{ O
Àúc1,Àúc2 O
, O
... O
, O
Àúcn}to O
a O
news O
report O
ÀúR={Àúr1,Àúr2 O
, O
.. O
, O
Àúrn O
} O
. O
We O
focus O
on O
the O
sentence O
- O
level O
rewriter O
. O
That O
is O
, O
we O
convert O
each O
selected O
commentary O
sentence O
Àúcito O
a O
news O
sentence O
Àúri O
. O
An O
intuitive O
way O
to O
learn O
the O
sentencelevel O
rewriter O
is O
training O
a O
sequence O
- O
to O
- O
sequence O
( O
seq2seq O
) O
model O
, O
such O
as O
LSTM O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
and O
Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
on O
the O
mapped O
sentences O
D. O
However O
, O
as O
illustrated O
in O
Table O
4 O
, O
we O
observe O
that O
the O
seq2seq O
model O
tends O
to O
generate O
high O
- O
frequency O
player O
names O
rather O
than O
the O
correct O
player O
names O
even O
though O
the O
high O
- O
frequency O
player O
names O
do O
not O
appear O
in O
the O
commentary O
sentences O
. O
We O
call O
this O
situation O
name O
mismatch O
problem O
. O
To O
solve O
the O
name O
mismatch O
problem O
, O
we O
train O
the O
rewriter O
in O
a O
template O
- O
to O
- O
template O
( O
tem2tem O
) O
way O
instead O
of O
in O
a O
seq2seq O
way O
. O
We O
Ô¨Årst O
build O
a O
dictionary O
of O
player O
names O
from O
the O
lineup O
data O
( O
metadata O
) O
. O
Next O
, O
for O
each O
( O
¬Øci,¬Øri)inD O
, O
we O
replace O
all O
the O
player O
names O
in O
¬Øciand¬Øriwith O
a O
special O
token O
‚Äú O
[ O
player O
] O
‚Äù O
so O
that O
the O
new O
sentence O
is O
like O
a O
template O
. O
If O
there O
are O
multiple O
player O
names O
in O
a O
sentence O
, O
we O
append O
a O
number O
to O
the O
special O
token O
to O
distinguish O
them O
, O
as O
shown O
in O
Table O
5.611Live O
Commentary O
SentenceÈáå O
Èáå O
ÈáåË¥ù O
Ë¥ù O
Ë¥ùÈáå O
Èáå O
ÈáåÁ¶ÅÂå∫Â∑¶‰æßÂ∞ùËØïÂè≥ËÑöÂ∞ÑÈó® O
, O
ÁöÆÁêÉÈ´òÂá∫ÁêÉÈó® O
. O
Áªô‰ªñ‰º†ÁêÉÁöÑÊòØÊãâ O
Êãâ O
ÊãâÂßÜ O
ÂßÜ O
ÂßÜ O
. O
Ribery O
tried O
to O
shoot O
with O
his O
right O
foot O
from O
the O
left O
side O
of O
the O
penalty O
area O
, O
but O
the O
ball O
was O
higher O
than O
the O
crossbar O
. O
Lahm O
passed O
the O
ball O
to O
him O
. O
Gound O
Truth O
News O
SentenceÊãâ O
Êãâ O
ÊãâÂßÜ O
ÂßÜ O
ÂßÜËΩ¨ÁßªÂà∞Â∑¶‰æßÔºåÈáå O
Èáå O
ÈáåË¥ù O
Ë¥ù O
Ë¥ùÈáå O
Èáå O
ÈáåÁ™ÅÂÖ•Á¶ÅÂå∫Â∑¶‰æßË∑ùÈó®12Á±≥Â§ÑÊäΩÂ∞ÑÈ´òÂá∫ O
„ÄÇ O
Lahm O
passed O
the O
ball O
to O
the O
left O
, O
and O
Ribery O
cut O
in O
the O
left O
penalty O
area O
and O
shot O
from O
12 O
meters O
ahead O
the O
goal O
line O
. O
The O
shot O
was O
too O
high O
. O
News O
Sentence O
Generated O
by O
Seq2seq O
ModelÈáå O
Èáå O
ÈáåË¥ù O
Ë¥ù O
Ë¥ùÈáå O
Èáå O
Èáå‰º†ÁêÉÔºåÊõº O
Êõº O
ÊõºÊú± O
Êú± O
Êú±Âü∫ O
Âü∫ O
Âü∫Â•á O
Â•á O
Â•áÁ¶ÅÂå∫Â∑¶‰æßÂ∞ÑÈó®ÂÅèÂá∫ËøúÈó®Êü± O
„ÄÇ O
Ribery O
passed O
the O
ball O
and O
Mandzukic O
‚Äôs O
shot O
from O
the O
left O
side O
of O
the O
penalty O
area O
was O
out O
of O
the O
goalpost O
. O
Table O
4 O
: O
An O
example O
of O
the O
name O
mismatch O
problem O
. O
Seq2seq O
model O
tends O
to O
generate O
high O
- O
frequency O
player O
names O
rather O
than O
the O
correct O
names O
. O
Input O
Sentence O
Output O
Sentence O
Seq2seqÂ∞ÑÈó®!!!ÈáåË¥ùÈáåÁêÉÈó®Á∫øË∑üÂâçÂè≥ËÑöÂ∞ÑÈó® O
, O
Ë¢´ÈòøÂæ∑Âãí O
Ê®™Ë∫´ÊâëÂá∫ O
. O
Áªô‰ªñ‰º†ÁêÉÁöÑÊòØÊãâÂßÜ O
. O
ÊãâÂßÜÂè≥Ë∑Ø‰Ωé‰º†ÔºåÈáåË¥ùÈáåÂâçÁÇπÈì≤Â∞ÑË¢´ÈòøÂæ∑ÂãíÂ∞Å O
Âá∫ O
„ÄÇ O
Shoot O
! O
! O
! O
Ribery O
‚Äôs O
right O
foot O
shot O
in O
front O
of O
the O
goal O
line O
was O
saved O
by O
Adler O
. O
Lahm O
passed O
the O
ball O
to O
him O
. O
Lahm O
made O
a O
low O
pass O
on O
the O
right O
and O
Ribery O
‚Äôs O
shot O
from O
the O
front O
was O
blocked O
by O
Adler O
. O
Tem2temÂ∞ÑÈó®!!![player1 O
] O
ÁêÉÈó®Á∫øË∑üÂâçÂè≥ËÑöÂ∞ÑÈó® O
, O
Ë¢´[player2 O
] O
Ê®™Ë∫´ÊâëÂá∫ O
. O
Áªô‰ªñ‰º†ÁêÉÁöÑÊòØ[player3 O
] O
.[player3]Âè≥Ë∑Ø‰Ωé‰º†Ôºå[player1]ÂâçÁÇπÈì≤Â∞Ñ O
Ë¢´[player2 O
] O
Â∞ÅÂá∫ O
„ÄÇ O
Shoot O
! O
! O
! O
[ O
player1 O
] O
‚Äôs O
right O
foot O
shot O
in O
front O
of O
the O
goal O
line O
was O
saved O
by O
[ O
player2 O
] O
.[player3 O
] O
passed O
the O
ball O
to O
him.[player3 O
] O
made O
a O
low O
pass O
on O
the O
right O
and O
[ O
player1 O
] O
‚Äôs O
shot O
from O
the O
front O
was O
blocked O
by O
[ O
player2 O
] O
. O
Table O
5 O
: O
Training O
models O
by O
seq2seq O
versus O
training O
models O
by O
tem2tem O
. O
After O
converting O
¬Øciand¬Ørito O
the O
template O
sentences O
, O
we O
train O
a O
seq2seq O
model O
on O
the O
template O
sentences O
. O
By O
training O
models O
in O
a O
tem2tem O
way O
, O
the O
model O
focuses O
more O
on O
the O
relations O
between O
players O
and O
actions O
and O
is O
less O
inÔ¨Çuenced O
by O
the O
high O
- O
frequency O
player O
names O
. O
When O
predicting O
, O
for O
each O
commentary O
sentence O
ÀúciinCselect O
, O
we O
use O
the O
aforementioned O
way O
to O
convert O
Àúcito O
a O
commentary O
template O
sentence O
. O
Then O
, O
we O
generate O
a O
news O
template O
sentence O
by O
the O
rewriter O
and O
replace O
all O
the O
special O
tokens O
in O
the O
sentence O
with O
the O
original O
player O
names O
. O
4 O
Experiments O
SPORTS O
SUMcontains O
5,428 O
games O
and O
we O
split O
them O
into O
three O
sets O
: O
training O
( O
4,828 O
games O
) O
, O
validation O
( O
300 O
games O
) O
, O
and O
testing O
( O
300 O
games O
) O
sets O
. O
Evaluation O
. O
We O
consider O
ROUGE O
scores O
( O
Lin O
, O
2004 O
) O
, O
which O
are O
standard O
metrics O
for O
summarization O
tasks O
. O
More O
precisely O
, O
we O
focus O
on O
ROUGE-1 O
, O
ROUGE-2 O
, O
and O
ROUGE O
- O
L. O
However O
, O
we O
observe O
that O
ROUGE O
scores O
can O
not O
accurately O
evaluate O
the O
correctness O
of O
summaries O
. O
Some O
summaries O
may O
get O
high O
ROUGE O
scores O
but O
contain O
many O
incorrect O
facts O
. O
Therefore O
, O
we O
design O
two O
metrics O
: O
name O
matching O
score O
( O
NMS O
) O
and O
event O
matching O
score O
( O
EMS).The O
name O
matching O
score O
evaluates O
the O
closeness O
of O
the O
player O
names O
in O
the O
ground O
truth O
news O
article O
Rand O
the O
generated O
summaries O
ÀúR. O
LetNgandNp O
denote O
the O
set O
of O
the O
player O
names O
appearing O
in O
R O
andÀúR O
, O
respectively O
. O
We O
deÔ¨Åne O
the O
name O
matching O
score O
as O
NMS O
( O
R,ÀúR O
) O
= O
F O
- O
score O
( O
Ng O
, O
Np O
) O
. O
Similarly O
, O
the O
event O
matching O
score O
evaluates O
the O
closeness O
of O
the O
events O
in O
RandÀúR. O
We O
deÔ¨Åne O
an O
event O
as O
a O
pair O
( O
subject O
, O
verb O
) O
in O
the O
sentence O
. O
Two O
pairs O
( O
subject1,verb O
1)and O
( O
subject2,verb O
2 O
) O
are O
viewed O
as O
equivalent O
if O
and O
only O
if O
1)subject1 O
is O
the O
same O
as O
subject2and2)verb O
1andverb O
2are O
synonym7to O
each O
other O
. O
Let O
EgandEprepresent O
the O
set O
of O
events O
in O
RandÀúR O
, O
respectively O
, O
the O
event O
matching O
score O
is O
deÔ¨Åned O
as O
EMS(R,ÀúR O
) O
= O
F O
- O
score O
( O
Eg O
, O
Ep O
) O
. O
Implementations O
and O
Models O
. O
We O
consider O
the O
convolutional O
neural O
network O
( O
Kim O
, O
2014 O
) O
as O
the O
selector O
. O
For O
the O
rewriter O
, O
we O
consider O
the O
following O
: O
( O
1)LSTM O
: O
a O
bidirectional O
LSTM O
with O
attention O
mechanism O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
) O
. O
( O
2 O
) O
Transformer O
. O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
( O
3 O
) O
PGNet O
: O
pointergenerator O
network O
, O
an O
encoder O
- O
decoder O
model O
with O
copy O
mechanism O
( O
See O
et O
al O
. O
, O
2017 O
) O
. O
7Details O
to O
decide O
synonyms O
can O
be O
found O
in O
Appendix O
B.612Method O
Model O
ROUGE-1 O
ROUGE-2 O
ROUGE O
- O
L O
NMS O
EMS O
Extractive O
ModelsRawSent O
26.52 O
7.64 O
25.42 O
57.33 O
36.17 O
LTR O
24.44 O
6.39 O
23.19 O
51.63 O
29.03 O
Abstractive O
ModelsAbs O
- O
LSTM O
30.54 O
10.16 O
29.78 O
10.87 O
14.03 O
Abs O
- O
PGNet O
34.02 O
11.09 O
33.13 O
17.87 O
19.76 O
Selector O
+ O
Rewriter O
( O
Seq2seq)LSTM O
41.39 O
16.99 O
40.53 O
28.48 O
25.19 O
Transformer O
41.71 O
18.10 O
40.96 O
35.63 O
30.94 O
PGNet O
43.17 O
18.66 O
42.27 O
48.18 O
36.94 O
Selector O
+ O
Rewriter O
( O
Tem2tem)LSTM O
41.71 O
17.08 O
40.82 O
59.54 O
40.34 O
Transformer O
41.47 O
17.18 O
40.54 O
58.26 O
39.33 O
PGNet O
41.95 O
17.09 O
41.01 O
59.35 O
40.46 O
Table O
6 O
: O
Evaluation O
results O
. O
NMS O
and O
EMS O
represent O
the O
name O
matching O
score O
and O
the O
event O
matching O
score O
. O
For O
comparison O
, O
we O
consider O
two O
extractive O
summarization O
baselines O
: O
( O
1 O
) O
RawSent O
: O
the O
raw O
sentences O
selected O
by O
the O
selector O
without O
rewriting O
. O
( O
2)LTR O
: O
the O
learning O
- O
to O
- O
rank O
approach O
for O
sports O
game O
summarization O
proposed O
by O
the O
previous O
work O
( O
Zhang O
et O
al O
. O
, O
2016 O
) O
. O
In O
addition O
, O
we O
train O
a O
bidirectional O
LSTM O
with O
attention O
mechanism O
( O
Abs O
- O
LSTM O
) O
and O
a O
pointergenerator O
network O
( O
Abs O
- O
PGNet O
) O
on O
the O
paired O
commentaries O
and O
news O
articles O
as O
two O
simple O
abstractive O
summarization O
baselines O
. O
More O
implementation O
details O
can O
be O
found O
in O
Appendix O
C. O
Results O
. O
Table O
6 O
shows O
the O
experimental O
results O
. O
We O
observe O
that O
the O
extractive O
models O
( O
RawSent O
and O
LTR O
) O
get O
low O
ROUGE O
scores O
but O
high O
NMS O
and O
EMS O
. O
That O
means O
the O
extractive O
models O
can O
generate O
summaries O
with O
correct O
information O
, O
but O
the O
writing O
style O
is O
different O
from O
the O
ground O
truth O
. O
On O
the O
contrary O
, O
the O
abstractive O
models O
get O
higher O
ROUGE O
scores O
but O
lower O
NMS O
and O
EMS O
. O
That O
implies O
the O
summaries O
generated O
by O
the O
abstractive O
models O
usually O
contain O
incorrect O
facts O
. O
Our O
proposed O
two O
- O
step O
model O
performs O
better O
than O
the O
extractive O
models O
and O
the O
abstractive O
models O
on O
ROUGE O
scores O
, O
NMS O
, O
and O
EMS O
. O
This O
veriÔ¨Åes O
our O
design O
of O
the O
selector O
and O
the O
rewriter O
. O
In O
addition O
, O
we O
observe O
that O
when O
training O
the O
model O
in O
a O
tem2tem O
way O
, O
we O
can O
get O
better O
NMS O
and O
EMS O
, O
which O
implies O
that O
training O
by O
tem2tem O
can O
improve O
the O
correctness O
of O
summaries O
. O
5 O
Related O
Work O
Text O
summarization O
. O
Existing O
approaches O
can O
be O
grouped O
into O
two O
families O
: O
extractive O
models O
and O
abstractive O
models O
. O
Extractive O
models O
select O
a O
part O
of O
sentences O
from O
the O
source O
document O
as O
the O
summary O
. O
Traditional O
approaches O
( O
Carbonell O
and O
Goldstein O
, O
1998 O
; O
Erkan O
and O
Radev O
, O
2004 O
; O
Mc O
- O
Donald O
, O
2007 O
) O
utilize O
graph O
or O
optimization O
techniques O
. O
Recently O
, O
neural O
models O
achieve O
good O
performance O
( O
Cheng O
and O
Lapata O
, O
2016 O
; O
Nallapati O
et O
al O
. O
, O
2017 O
; O
Jadhav O
and O
Rajan O
, O
2018 O
) O
. O
Abstractive O
summarization O
models O
aim O
to O
rephrase O
the O
source O
document O
. O
Most O
work O
applies O
neural O
models O
for O
this O
task O
. O
( O
Rush O
et O
al O
. O
, O
2015 O
; O
Chopra O
et O
al O
. O
, O
2016 O
; O
Nallapati O
et O
al O
. O
, O
2016 O
; O
Zeng O
et O
al O
. O
, O
2016 O
; O
See O
et O
al O
. O
, O
2017 O
; O
Gehrmann O
et O
al O
. O
, O
2018 O
) O
. O
Factual O
correctness O
of O
summaries O
. O
There O
is O
a O
lot O
of O
work O
focusing O
on O
evaluation O
and O
improvement O
of O
the O
factual O
correctness O
of O
summaries O
( O
Falke O
et O
al O
. O
, O
2019 O
; O
Kryscinski O
et O
al O
. O
, O
2019 O
; O
Wang O
et O
al O
. O
, O
2020 O
; O
Maynez O
et O
al O
. O
, O
2020 O
; O
Zhu O
et O
al O
. O
, O
2020 O
) O
. O
Data O
- O
to O
- O
Text O
generation O
. O
Recently O
, O
generating O
news O
articles O
from O
different O
kinds O
of O
data O
- O
records O
becomes O
a O
popular O
research O
direction O
. O
Wiseman O
et O
al O
. O
( O
2017 O
) O
; O
Puduppully O
et O
al O
. O
( O
2019 O
) O
focus O
on O
generating O
news O
from O
boxed O
- O
data O
. O
Zhang O
et O
al O
. O
( O
2016 O
) O
and O
Yao O
et O
al O
. O
( O
2017 O
) O
study O
generating O
sports O
news O
from O
live O
commentaries O
, O
but O
their O
methods O
are O
based O
on O
hand O
- O
crafted O
features O
. O
6 O
Conclusion O
We O
present O
SPORTS O
SUM O
, O
a O
Chinese O
dataset O
for O
sports O
game O
summarization O
, O
as O
well O
as O
a O
model O
that O
consists O
of O
a O
selector O
and O
a O
rewriter O
. O
To O
improve O
the O
quality O
of O
generated O
news O
, O
we O
train O
the O
model O
in O
a O
tem2tem O
way O
. O
We O
design O
two O
metrics O
to O
evaluate O
the O
correctness O
of O
generated O
summaries O
. O
The O
experimental O
results O
demonstrate O
that O
the O
proposed O
model O
performs O
well O
on O
ROUGE O
scores O
and O
the O
two O
designed O
scores O
. O
Acknowledgments O
We O
thank O
Tecent O
AI O
Lab O
, O
UCLA O
- O
NLP O
group O
, O
and O
anonymous O
reviewers O
for O
their O
feedback.613References O
Dzmitry O
Bahdanau O
, O
Kyunghyun O
Cho O
, O
and O
Yoshua O
Bengio O
. O
2015 O
. O
Neural O
machine O
translation O
by O
jointly O
learning O
to O
align O
and O
translate O
. O
In O
ICLR O
. O
Jaime O
G. O
Carbonell O
and O
Jade O
Goldstein O
. O
1998 O
. O
The O
use O
of O
mmr O
, O
diversity O
- O
based O
reranking O
for O
reordering O
documents O
and O
producing O
summaries O
. O
In O
SIGIR O
. O
Jianpeng O
Cheng O
and O
Mirella O
Lapata O
. O
2016 O
. O
Neural O
summarization O
by O
extracting O
sentences O
and O
words O
. O
In O
ACL O
. O
Sumit O
Chopra O
, O
Michael O
Auli O
, O
and O
Alexander O
M. O
Rush O
. O
2016 O
. O
Abstractive O
sentence O
summarization O
with O
attentive O
recurrent O
neural O
networks O
. O
In O
NAACL O
. O
G¬®unes O
Erkan O
and O
Dragomir O
R. O
Radev O
. O
2004 O
. O
Lexrank O
: O
Graph O
- O
based O
lexical O
centrality O
as O
salience O
in O
text O
summarization O
. O
Journal O
of O
ArtiÔ¨Åcial O
Intelligence O
Research O
, O
22:457‚Äì479 O
. O
Tobias O
Falke O
, O
Leonardo O
F. O
R. O
Ribeiro O
, O
Prasetya O
Ajie O
Utama O
, O
Ido O
Dagan O
, O
and O
Iryna O
Gurevych O
. O
2019 O
. O
Ranking O
generated O
summaries O
by O
correctness O
: O
An O
interesting O
but O
challenging O
application O
for O
natural O
language O
inference O
. O
In O
ACL O
. O
Sebastian O
Gehrmann O
, O
Yuntian O
Deng O
, O
and O
Alexander O
M. O
Rush O
. O
2018 O
. O
Bottom O
- O
up O
abstractive O
summarization O
. O
InEMNLP O
. O
Karl O
Moritz O
Hermann O
, O
Tom O
¬¥ O
as O
Kocisk O
¬¥ O
y O
, O
Edward O
Grefenstette O
, O
Lasse O
Espeholt O
, O
Will O
Kay O
, O
Mustafa O
Suleyman O
, O
and O
Phil O
Blunsom O
. O
2015 O
. O
Teaching O
machines O
to O
read O
and O
comprehend O
. O
In O
NIPS O
. O
Sepp O
Hochreiter O
and O
J O
¬®urgen O
Schmidhuber O
. O
1997 O
. O
Long O
short O
- O
term O
memory O
. O
Neural O
Computation O
, O
9(8):1735‚Äì1780 O
. O
Aishwarya O
Jadhav O
and O
Vaibhav O
Rajan O
. O
2018 O
. O
Extractive O
summarization O
with O
SWAP O
- O
NET O
: O
sentences O
and O
words O
from O
alternating O
pointer O
networks O
. O
In O
ACL O
. O
Yoon O
Kim O
. O
2014 O
. O
Convolutional O
neural O
networks O
for O
sentence O
classiÔ¨Åcation O
. O
In O
EMNLP O
. O
Wojciech O
Kryscinski O
, O
Bryan O
McCann O
, O
Caiming O
Xiong O
, O
and O
Richard O
Socher O
. O
2019 O
. O
Evaluating O
the O
factual O
consistency O
of O
abstractive O
text O
summarization O
. O
Preprint O
arXiv:1910.12840 O
. O
Chin O
- O
Yew O
Lin O
. O
2004 O
. O
Rouge O
: O
A O
package O
for O
automatic O
evaluation O
of O
summaries O
. O
In O
Text O
summarization O
branches O
out O
. O
Joshua O
Maynez O
, O
Shashi O
Narayan O
, O
Bernd O
Bohnet O
, O
and O
Ryan O
T. O
McDonald O
. O
2020 O
. O
On O
faithfulness O
and O
factuality O
in O
abstractive O
summarization O
. O
In O
ACL O
. O
Ryan O
T. O
McDonald O
. O
2007 O
. O
A O
study O
of O
global O
inference O
algorithms O
in O
multi O
- O
document O
summarization O
. O
InECIR O
.Ramesh O
Nallapati O
, O
Feifei O
Zhai O
, O
and O
Bowen O
Zhou O
. O
2017 O
. O
SummaRuNNer O
: O
A O
recurrent O
neural O
network O
based O
sequence O
model O
for O
extractive O
summarization O
of O
documents O
. O
In O
AAAI O
. O
Ramesh O
Nallapati O
, O
Bowen O
Zhou O
, O
C O
¬¥ O
ƒ±cero O
Nogueira O
dos O
Santos O
, O
C O
¬∏ O
aglar O
G O
¬®ulc O
¬∏ehre O
, O
and O
Bing O
Xiang O
. O
2016 O
. O
Abstractive O
text O
summarization O
using O
sequence O
- O
tosequence O
RNNs O
and O
beyond O
. O
In O
CoNLL O
. O
Ratish O
Puduppully O
, O
Li O
Dong O
, O
and O
Mirella O
Lapata O
. O
2019 O
. O
Data O
- O
to O
- O
text O
generation O
with O
content O
selection O
and O
planning O
. O
In O
AAAI O
. O
Alexander O
M. O
Rush O
, O
Sumit O
Chopra O
, O
and O
Jason O
Weston O
. O
2015 O
. O
A O
neural O
attention O
model O
for O
abstractive O
sentence O
summarization O
. O
In O
EMNLP O
. O
Abigail O
See O
, O
Peter O
J. O
Liu O
, O
and O
Christopher O
D. O
Manning O
. O
2017 O
. O
Get O
to O
the O
point O
: O
Summarization O
with O
pointergenerator O
networks O
. O
In O
ACL O
. O
Ashish O
Vaswani O
, O
Noam O
Shazeer O
, O
Niki O
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion O
Jones O
, O
Aidan O
N. O
Gomez O
, O
Lukasz O
Kaiser O
, O
and O
Illia O
Polosukhin O
. O
2017 O
. O
Attention O
is O
all O
you O
need O
. O
In O
NeurIPS O
. O
Alex O
Wang O
, O
Kyunghyun O
Cho O
, O
and O
Mike O
Lewis O
. O
2020 O
. O
Asking O
and O
answering O
questions O
to O
evaluate O
the O
factual O
consistency O
of O
summaries O
. O
In O
ACL O
. O
Sam O
Wiseman O
, O
Stuart O
M. O
Shieber O
, O
and O
Alexander O
M. O
Rush O
. O
2017 O
. O
Challenges O
in O
data O
- O
to O
- O
document O
generation O
. O
In O
EMNLP O
. O
Jin O
- O
ge O
Yao O
, O
Jianmin O
Zhang O
, O
Xiaojun O
Wan O
, O
and O
Jianguo O
Xiao O
. O
2017 O
. O
Content O
selection O
for O
real O
- O
time O
sports O
news O
construction O
from O
commentary O
texts O
. O
In O
INLG O
. O
Wenyuan O
Zeng O
, O
Wenjie O
Luo O
, O
Sanja O
Fidler O
, O
and O
Raquel O
Urtasun O
. O
2016 O
. O
EfÔ¨Åcient O
summarization O
with O
read O
- O
again O
and O
copy O
mechanism O
. O
Preprint O
arXiv:1611.03382 O
. O
Jianmin O
Zhang O
, O
Jin O
- O
ge O
Yao O
, O
and O
Xiaojun O
Wan O
. O
2016 O
. O
Towards O
constructing O
sports O
news O
from O
live O
text O
commentary O
. O
In O
ACL O
. O
Tianyi O
Zhang O
, O
Varsha O
Kishore O
, O
Felix O
Wu O
, O
Kilian O
Q. O
Weinberger O
, O
and O
Yoav O
Artzi O
. O
2020 O
. O
BERTScore O
: O
Evaluating O
text O
generation O
with O
BERT O
. O
In O
ICLR O
. O
Chenguang O
Zhu O
, O
William O
Hinthorn O
, O
Ruochen O
Xu O
, O
Qingkai O
Zeng O
, O
Michael O
Zeng O
, O
Xuedong O
Huang O
, O
and O
Meng O
Jiang O
. O
2020 O
. O
Boosting O
factual O
correctness O
of O
abstractive O
summarization O
with O
knowledge O
graph O
. O
Preprint O
arXiv:2003.08612 O
.614A O
Starting O
Keywords O
We O
consider O
the O
following O
regular O
expressions O
as O
the O
starting O
keywords O
: O
‚Ä¢‰∏ÄÂºÄÂú∫ O
‚Ä¢ÂºÄÂú∫Âêé O
‚Ä¢ÂºÄÂú∫[\d]+ÂàÜÈíü O
‚Ä¢ÂºÄÂßã[\d]+ÂàÜÈíü O
‚Ä¢ÂºÄÂú∫[‰ªÖ][\d]+Áßí O
‚Ä¢[\d]+Áßí O
‚Ä¢Á¨¨[\d]+ÂàÜÈíü O
‚Ä¢[\d]+ÂàÜÈíü O
‚Ä¢[\d]+[Á±≥Á†Å O
] O
B O
Event O
Matching O
Score O
We O
pick O
up O
the O
top O
300 O
most O
frequent O
verbs O
and O
ask O
human O
to O
annotate O
if O
the O
verb O
is O
an O
important O
verb O
for O
soccer O
games O
or O
not O
. O
Then O
, O
we O
ask O
human O
to O
cluster O
those O
important O
verbs O
based O
on O
their O
meanings O
. O
When O
calculating O
the O
event O
matching O
score O
, O
we O
only O
consider O
those O
verbs O
. O
Two O
verbs O
are O
viewed O
as O
the O
synonym O
to O
each O
other O
if O
they O
are O
in O
the O
same O
group O
. O
The O
groups O
of O
verbs O
are O
as O
follows O
: O
‚Ä¢Shooting O
: O
Â∞ÑÈó® O
, O
ÊâìÈó® O
, O
ÊîªÈó® O
, O
ÊäΩÂ∞Ñ O
, O
Êé®Â∞Ñ O
, O
Âä≤ O
Â∞Ñ O
, O
ËøúÂ∞Ñ O
, O
‰ΩéÂ∞Ñ O
, O
Ë°•Â∞Ñ O
, O
Êâ´Â∞Ñ O
, O
ÊñúÂ∞Ñ O
, O
ÊçÖÂ∞Ñ O
, O
Â∞Ñ O
, O
ÊÄíÂ∞Ñ O
, O
Ëµ∑ËÑö O
, O
Èì≤Â∞Ñ O
, O
Âû´Â∞Ñ O
, O
ÂêäÂ∞Ñ O
, O
ÊåëÂ∞Ñ O
, O
ÂºπÂ∞Ñ O
, O
ÂãæÂ∞Ñ O
, O
ÁàÜÂ∞Ñ O
, O
Â§¥ÁêÉ O
, O
Áî©Â§¥ O
‚Ä¢Missed O
Shot O
: O
ÂÅèÂá∫ O
, O
È´òÂá∫ O
, O
ÊâìÂÅè O
, O
ÂºπÂá∫ O
, O
ÊâìÈ´ò O
, O
ÂºπÂõû O
, O
ÊâìÈ£û O
, O
È°∂È´ò O
, O
È°∂ÂÅè O
, O
Ë∂ÖÂá∫ O
, O
Â∞ÑÂÅè O
, O
Ëπ≠ÂÅè O
, O
Ëπ≠Âá∫ O
, O
ÊªëÂá∫ O
‚Ä¢Passing O
: O
‰º†‰∏≠ O
, O
‰º†ÁêÉ O
, O
Êñú‰º† O
, O
ÈÄÅÂá∫ O
, O
Â§¥ÁêÉÊëÜÊ∏° O
, O
Áõ¥Â°û O
, O
Ê®™‰º† O
, O
Êåë‰º† O
, O
Áõ¥‰º† O
, O
‰Ωé‰º† O
, O
Ê®™Êï≤ O
, O
ÁªôÂà∞ O
, O
‰º†ÂÖ• O
, O
‰º† O
, O
‰º†Âà∞ O
, O
Â¶ô‰º† O
, O
ÊñúÂ°û O
, O
Èïø‰º† O
, O
Áü≠‰º† O
, O
Âõû O
‰º† O
, O
ÂõûÊï≤ O
, O
ÂõûÁÇπ O
, O
ÂàÜÁêÉ O
‚Ä¢Blocking O
: O
ÊâëÂá∫ O
, O
Êå°Âá∫ O
, O
Ê≤°Êî∂ O
, O
Â∞ÅÂ†µ O
, O
ÂæóÂà∞ O
, O
Â∞Å O
Âá∫ O
, O
ÊâòÂá∫ O
, O
Êâë‰Ωè O
, O
Êïë‰∏ã O
, O
Êä±‰Ωè O
, O
ÊïëÂá∫ O
‚Ä¢Defense O
: O
Ëß£Âõ¥ O
, O
Á†¥Âùè O
, O
Èì≤Âá∫ O
, O
ÂåñËß£ O
‚Ä¢Foul O
: O
ÁäØËßÑ O
, O
ÂêÉÂà∞ O
, O
Ë≠¶Âëä O
, O
Âà§ÁΩö O
, O
Ë¢´Âà§ O
, O
È¢ÜÂà∞ O
, O
ÁΩö‰∏ã O
, O
Âá∫Á§∫ O
, O
Ë¢´ÁΩö O
C O
Implementation O
Details O
For O
the O
selector O
, O
we O
consider O
CNN O
with O
the O
same O
architecture O
in O
( O
Kim O
, O
2014 O
) O
and O
set O
the O
learning O
rate O
to O
10‚àí3 O
. O
For O
the O
rewriter O
, O
the O
implementation O
details O
are O
as O
follows:‚Ä¢LSTM O
: O
we O
use O
a O
bidirectional O
LSTM O
with O
the O
attention O
mechanism O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
) O
. O
The O
size O
of O
hidden O
state O
is O
set O
to O
300 O
. O
We O
set O
the O
learning O
rate O
to O
10‚àí3 O
. O
‚Ä¢Transformer O
: O
we O
use O
the O
Transformer O
with O
the O
same O
architecture O
in O
the O
original O
paper O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O
We O
set O
the O
learning O
rate O
to O
10‚àí4 O
. O
‚Ä¢PGNet O
: O
we O
implement O
the O
pointer O
- O
generator O
network O
( O
See O
et O
al O
. O
, O
2017 O
) O
and O
set O
the O
size O
of O
hidden O
state O
to O
300 O
. O
We O
set O
the O
learning O
rate O
to10‚àí3 O
. O
‚Ä¢LSTM O
- O
abs O
: O
we O
use O
a O
bidirectional O
LSTM O
with O
the O
attention O
mechanism O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
) O
. O
The O
size O
of O
hidden O
state O
is O
set O
to O
300 O
. O
We O
set O
the O
learning O
rate O
to O
10‚àí3 O
. O
‚Ä¢PGNet O
- O
abs O
: O
we O
implement O
the O
pointergenerator O
network O
( O
See O
et O
al O
. O
, O
2017 O
) O
and O
set O
the O
size O
of O
hidden O
state O
to O
300 O
. O
We O
set O
the O
learning O
rate O
to O
10‚àí3 O
. O
For O
all O
the O
models O
, O
we O
use O
the O
200 O
- O
dimensional O
pre O
- O
trained O
Chinese O
word O
embedding O
from O
Tecent O
AI O
Lab8 O
. O
8https://ai.tencent.com/ailab/nlp/ O
embedding.html615Proceedings O
of O
the O
1st O
Conference O
of O
the O
Asia O
- O
PaciÔ¨Åc O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
10th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
616‚Äì625 O
December O
4 O
- O
7 O
, O
2020 O
. O
¬© O
2020 O
Association O
for O
Computational O
Linguistics O
Massively O
Multilingual O
Document O
Alignment O
with O
Cross O
- O
lingual O
Sentence O
- O
Mover O
‚Äôs O
Distance O
Ahmed O
El O
- O
Kishky O
Facebook O
AI O
ahelk@fb.comFrancisco O
Guzm O
¬¥ O
an O
Facebook O
AI O
fguzman@fb.com O
Abstract O
Document O
alignment O
aims O
to O
identify O
pairs O
of O
documents O
in O
two O
distinct O
languages O
that O
are O
of O
comparable O
content O
or O
translations O
of O
each O
other O
. O
Such O
aligned O
data O
can O
be O
used O
for O
a O
variety O
of O
NLP O
tasks O
from O
training O
cross O
- O
lingual O
representations O
to O
mining O
parallel O
data O
for O
machine O
translation O
. O
In O
this O
paper O
we O
develop O
an O
unsupervised O
scoring O
function O
that O
leverages O
cross O
- O
lingual O
sentence O
embeddings O
to O
compute O
the O
semantic O
distance O
between O
documents O
in O
different O
languages O
. O
These O
semantic O
distances O
are O
then O
used O
to O
guide O
a O
document O
alignment O
algorithm O
to O
properly O
pair O
cross O
- O
lingual O
web O
documents O
across O
a O
variety O
of O
low O
, O
mid O
, O
and O
high O
- O
resource O
language O
pairs O
. O
Recognizing O
that O
our O
proposed O
scoring O
function O
and O
other O
state O
of O
the O
art O
methods O
are O
computationally O
intractable O
for O
long O
web O
documents O
, O
we O
utilize O
a O
more O
tractable O
greedy O
algorithm O
that O
performs O
comparably O
. O
We O
experimentally O
demonstrate O
that O
our O
distance O
metric O
performs O
better O
alignment O
than O
current O
baselines O
outperforming O
them O
by O
7 O
% O
on O
high O
- O
resource O
language O
pairs O
, O
15 O
% O
on O
mid O
- O
resource O
language O
pairs O
, O
and O
22 O
% O
on O
low O
- O
resource O
language O
pairs O
. O
1 O
Introduction O
While O
the O
Web O
provides O
a O
large O
amount O
of O
monolingual O
text O
, O
cross O
- O
lingual O
parallel O
data O
is O
more O
difÔ¨Åcult O
to O
obtain O
. O
Despite O
its O
scarcity O
, O
parallel O
cross O
- O
lingual O
data O
plays O
a O
crucial O
role O
in O
a O
variety O
of O
tasks O
in O
natural O
language O
processing O
such O
as O
machine O
translation O
. O
Previous O
works O
have O
shown O
that O
training O
on O
sentences O
extracted O
from O
parallel O
or O
comparable O
documents O
mined O
from O
the O
Web O
can O
improve O
machine O
translation O
models O
( O
Munteanu O
and O
Marcu O
, O
2005 O
) O
or O
learning O
word O
- O
level O
translation O
lexicons O
( O
Fung O
and O
Yee O
, O
1998 O
; O
Rapp O
, O
1999 O
) O
. O
Other O
tasks O
that O
leverage O
these O
parallel O
texts O
include O
cross O
- O
lingual O
information O
retrieval O
, O
document O
classiÔ¨Åcation O
, O
and O
multilingual O
representations O
such O
as O
SourceTargetFigure O
1 O
: O
Documents O
in O
a O
source O
and O
target O
langauge O
in O
the O
same O
web O
- O
domain O
. O
Solid O
lines O
indicate O
crosslingual O
document O
pairs O
. O
XLM O
( O
Lample O
and O
Conneau O
, O
2019 O
) O
. O
Document O
alignment O
is O
a O
method O
for O
obtaining O
cross O
- O
lingual O
parallel O
data O
that O
seeks O
to O
pair O
documents O
in O
different O
languages O
such O
that O
pairs O
are O
translations O
or O
near O
translations O
of O
each O
other O
. O
As O
seen O
in O
Figure O
1 O
, O
this O
involves O
a O
one O
- O
to O
- O
one O
pairing O
of O
documents O
in O
a O
source O
language O
with O
documents O
in O
a O
target O
language O
. O
To O
automate O
and O
scale O
the O
process O
of O
identifying O
these O
documents O
pairs O
, O
we O
introduce O
an O
approach O
to O
accurately O
mine O
comparable O
web O
documents O
across O
a O
variety O
of O
low O
, O
mid O
, O
and O
high O
- O
resource O
language O
directions O
. O
Previous O
approaches O
have O
been O
applied O
to O
homogeneous O
corpora O
, O
however O
mining O
the O
Web O
involves O
analyzing O
a O
variety O
of O
heterogeneous O
data O
sources O
( O
Koehn O
et O
al O
. O
, O
2002 O
) O
. O
Other O
approaches O
rely O
on O
corpus O
- O
speciÔ¨Åc O
features O
such O
as O
metadata O
and O
publication O
date O
which O
can O
be O
inconsistent O
and O
unreliable O
( O
Munteanu O
and O
Marcu O
, O
2005 O
; O
AbduI O
- O
Rauf O
and O
Schwenk O
, O
2009 O
) O
. O
Related O
methods O
utilize O
document O
structure O
when O
calculating O
document O
similarity O
( O
Resnik O
and O
Smith O
, O
2003 O
; O
Chen O
and O
Nie O
, O
2000 O
) O
. O
However O
, O
when O
mining O
large O
, O
unstructured O
collections O
of O
web O
documents O
these O
features O
are O
often O
missing O
or O
unreliable O
. O
As O
such O
, O
we O
introduce O
an O
approach O
that O
aligns O
documents O
based O
solely O
on O
semantic O
distances O
between O
their O
textual O
content O
. O
For O
our O
approach O
, O
we O
Ô¨Årst O
decompose O
documents O
into O
sentences O
, O
and O
encode O
each O
sentence O
into O
a O
cross O
- O
lingual O
semantic O
space O
yielding O
a O
bag-616of O
- O
sentences O
representation O
. O
Utilizing O
the O
dense O
, O
cross O
- O
lingual O
representation O
of O
sentences O
, O
we O
then O
compute O
document O
distances O
using O
a O
variant O
of O
earth O
mover O
‚Äôs O
distance O
where O
probability O
mass O
is O
moved O
from O
the O
source O
document O
to O
the O
target O
document O
. O
We O
then O
leverage O
these O
document O
distances O
as O
a O
guiding O
metric O
for O
identifying O
cross O
- O
lingual O
document O
pairs O
and O
demonstrate O
experimentally O
that O
our O
proposed O
method O
outperforms O
state O
- O
of O
- O
theart O
baselines O
that O
utilize O
cross O
- O
lingual O
document O
representations O
. O
2 O
Related O
Works O
Crawling O
and O
mining O
the O
web O
for O
parallel O
data O
has O
been O
previously O
explored O
by O
Resnik O
( O
1999 O
) O
where O
the O
focus O
is O
on O
identifying O
parallel O
text O
from O
multilingual O
data O
obtained O
from O
a O
single O
source O
. O
For O
example O
, O
parallel O
corpora O
were O
curated O
from O
the O
United O
Nations O
General O
Assembly O
Resolutions O
( O
Rafalovitch O
et O
al O
. O
, O
2009 O
; O
Ziemski O
et O
al O
. O
, O
2016 O
) O
and O
from O
the O
European O
Parliament O
( O
Koehn O
, O
2005 O
) O
. O
However O
, O
curating O
from O
homogeneous O
sources O
by O
deriving O
domain O
- O
speciÔ¨Åc O
rules O
does O
not O
generalize O
to O
arbitrary O
web O
- O
domains O
. O
Other O
approaches O
rely O
on O
metadata O
for O
mining O
parallel O
documents O
in O
unstructured O
web O
corpora O
. O
Some O
methods O
leveraged O
publication O
date O
and O
other O
temporal O
heuristics O
to O
identifying O
parallel O
documents O
( O
Munteanu O
and O
Marcu O
, O
2005 O
, O
2006 O
; O
Udupa O
et O
al O
. O
, O
2009 O
; O
Do O
et O
al O
. O
, O
2009 O
; O
AbduI O
- O
Rauf O
and O
Schwenk O
, O
2009 O
) O
. O
However O
, O
temporal O
features O
are O
often O
sparse O
, O
noisy O
, O
and O
unreliable O
. O
Another O
class O
of O
alignment O
methods O
rely O
on O
document O
structure O
( O
Resnik O
and O
Smith O
, O
2003 O
; O
Chen O
and O
Nie O
, O
2000 O
) O
yet O
these O
structure O
signals O
can O
be O
sparse O
and O
may O
not O
generalize O
to O
new O
domains O
. O
In O
the O
WMT-2016 O
bilingual O
document O
alignment O
shared O
task O
( O
Buck O
and O
Koehn O
, O
2016a O
) O
, O
many O
techniques O
were O
proposed O
to O
retrieve O
, O
score O
, O
and O
align O
cross O
- O
lingual O
document O
pairs O
. O
However O
this O
shared O
task O
only O
considered O
English O
to O
French O
‚Äì O
a O
highresource O
direction O
and O
the O
proposed O
techniques O
were O
not O
readily O
extendable O
to O
more O
languages O
. O
Several O
approaches O
translate O
the O
target O
corpus O
into O
the O
source O
language O
, O
then O
apply O
retrieval O
and O
matching O
approaches O
on O
translated O
2 O
- O
grams O
and O
5grams O
to O
query O
, O
retrieve O
, O
and O
align O
documents O
( O
Dara O
and O
Lin O
, O
2016 O
; O
Gomes O
and O
Lopes O
, O
2016 O
) O
. O
These O
methods O
rely O
on O
high O
- O
quality O
translation O
systems O
to O
translate O
, O
however O
such O
models O
may O
not O
exist O
, O
especially O
for O
low O
- O
resource O
language O
directions O
. O
Ad O
- O
ditionally O
, O
these O
methods O
leverage O
rare O
n O
- O
grams O
to O
identify O
likely O
candidates O
, O
yet O
low O
- O
frequency O
words O
and O
phrases O
that O
are O
likely O
to O
be O
mistranslated O
by O
machine O
translation O
systems O
. O
In O
the O
shared O
task O
, O
many O
document O
similarity O
measures O
were O
investigated O
for O
use O
in O
aligning O
English O
to O
French O
web O
documents O
. O
One O
method O
utilized O
a O
phrase O
table O
from O
a O
phrase O
- O
based O
statistical O
machine O
translation O
system O
to O
compute O
coverage O
scores O
, O
based O
on O
the O
ratio O
of O
phrase O
pairs O
covered O
by O
a O
document O
pair O
( O
Gomes O
and O
Lopes O
, O
2016 O
) O
. O
Other O
methods O
utilize O
the O
translated O
content O
of O
the O
target O
( O
French O
) O
document O
, O
and O
Ô¨Ånd O
the O
source O
( O
English O
) O
corresponding O
document O
based O
on O
n O
- O
gram O
matches O
in O
conjunction O
with O
a O
heuristic O
document O
length O
ratio O
( O
Dara O
and O
Lin O
, O
2016 O
; O
Shchukin O
et O
al O
. O
, O
2016 O
) O
. O
Other O
methods O
translate O
the O
target O
documents O
into O
the O
source O
language O
and O
apply O
cosine O
similarity O
between O
tf O
/ O
idf O
weighted O
vectors O
on O
unigrams O
and O
n O
- O
grams O
( O
Buck O
and O
Koehn O
, O
2016b O
; O
Medve O
Àád O
et O
al O
. O
, O
2016 O
; O
Jakubina O
and O
Langlais O
, O
2016 O
) O
. O
Finally O
, O
several O
methods O
were O
introduced O
that O
score O
pairs O
using O
metadata O
in O
each O
document O
such O
as O
links O
to O
documents O
, O
URLs O
, O
digits O
, O
and O
HTML O
structure O
( O
Espl O
` O
aGomis O
et O
al O
. O
, O
2016 O
; O
Papavassiliou O
et O
al O
. O
, O
2016 O
) O
. O
Recently O
, O
the O
use O
of O
neural O
embedding O
methods O
has O
been O
explored O
for O
bilingual O
alignment O
of O
text O
at O
the O
sentence O
and O
document O
level O
. O
One O
method O
proposes O
using O
hierarchical O
document O
embeddings O
, O
constructed O
from O
sentence O
embeddings O
, O
for O
bilingual O
document O
alignment O
( O
Guo O
et O
al O
. O
, O
2019 O
) O
. O
Another O
method O
leverages O
a O
multilingual O
sentence O
encoder O
to O
embed O
individual O
sentences O
from O
each O
document O
, O
then O
performs O
a O
simple O
vector O
average O
across O
all O
sentence O
embeddings O
to O
form O
a O
dense O
document O
representation O
with O
cosine O
similarity O
guiding O
document O
alignment O
( O
El O
- O
Kishky O
et O
al O
. O
, O
2019 O
) O
. O
Word O
mover O
‚Äôs O
distance O
( O
WMD O
) O
is O
an O
adaptation O
of O
earth O
mover O
‚Äôs O
distance O
( O
EMD O
) O
( O
Rubner O
et O
al O
. O
, O
1998 O
) O
that O
has O
been O
recently O
used O
for O
document O
similarity O
and O
classiÔ¨Åcation O
( O
Kusner O
et O
al O
. O
, O
2015 O
; O
Huang O
et O
al O
. O
, O
2016 O
; O
Atasu O
et O
al O
. O
, O
2017 O
) O
. O
Other O
methods O
have O
leveraged O
the O
distance O
for O
cross O
- O
lingual O
document O
retrieval O
( O
Balikas O
et O
al O
. O
, O
2018 O
) O
. O
However O
these O
methods O
treat O
individual O
words O
as O
the O
base O
semantic O
unit O
for O
comparison O
which O
are O
intractable O
for O
large O
web O
- O
document O
alignment O
. O
Finally O
, O
sentence O
mover O
‚Äôs O
similarity O
has O
been O
proposed O
for O
automatically O
evaluating O
machinegenerated O
texts O
outperforming O
ROUGE O
( O
Clark O
et O
al O
. O
, O
2019 O
) O
. O
This O
method O
is O
purely O
monolingual617and O
sentence O
representations O
are O
constructed O
by O
summing O
individual O
word O
embeddings O
. O
3 O
Problem O
DeÔ¨Ånition O
Given O
a O
set O
of O
source O
documents O
, O
Dsand O
a O
set O
of O
target O
documents O
Dt O
, O
there O
exist|Ds|√ó|Dt| O
potential O
pairs O
of O
documents O
of O
the O
form O
( O
ds O
, O
dt O
) O
. O
LetPbe O
the O
set O
of O
all O
candidate O
pairs O
( O
Ds√óDt O
) O
. O
Then O
cross O
- O
lingual O
document O
alignment O
aims O
to O
Ô¨Ånd O
the O
largest O
mapping O
from O
source O
documents O
to O
target O
documents O
, O
P O
/ O
prime‚äÇP O
, O
s.t O
. O
given O
an O
DsandDt O
where O
, O
without O
a O
loss O
of O
generality O
, O
|Ds|‚â§|Dt| O
, O
the O
largest O
injective O
function O
mapping O
betweenDs O
andDt O
: O
‚àÄa O
, O
b‚ààDs,(a O
, O
c)‚ààP O
/ O
prime‚àß(b O
, O
c)‚ààP O
/ O
prime=‚áía O
= O
b O
In O
other O
words O
, O
each O
source O
document O
and O
target O
document O
can O
only O
be O
used O
in O
at O
most O
a O
single O
pair O
. O
This O
can O
be O
seen O
in O
Figure O
1 O
where O
within O
the O
same O
web O
- O
domain O
, O
given O
source O
and O
target O
documents O
, O
the O
task O
is O
to O
match O
each O
source O
document O
to O
a O
unique O
target O
document O
where O
possible O
. O
To O
Ô¨Ånd O
the O
best O
possible O
mapping O
between O
Ds O
andDtwe O
require O
two O
components O
: O
1 O
) O
a O
similarity O
functionœÜ(ds O
, O
dt)which O
is O
used O
to O
score O
a O
set O
of O
candidate O
document O
pairs O
according O
to O
their O
semantic O
relatedness O
; O
and O
2 O
) O
an O
alignment O
or O
matching O
algorithm O
which O
uses O
the O
scores O
for O
each O
of O
the O
pairs O
inDs√óDtto O
produce O
an O
alignment O
of O
size O
min(|Ds|,|Dt|)representing O
the O
best O
mapping O
according O
toœÜ(ds O
, O
dt O
) O
. O
4 O
Cross O
- O
Lingual O
Sentence O
Mover O
‚Äôs O
Distance O
WMD O
fails O
to O
generalize O
to O
our O
use O
case O
for O
two O
reasons O
: O
( O
1 O
) O
it O
relies O
on O
monolingual O
word O
representations O
which O
fail O
to O
capture O
the O
semantic O
distances O
between O
different O
language O
documents O
( O
2 O
) O
intractability O
due O
to O
long O
web O
documents O
or O
lack O
word O
boundaries O
in O
certain O
languages O
. O
To O
address O
this O
, O
we O
introduce O
cross O
- O
lingual O
sentence O
mover O
‚Äôs O
distance O
( O
SMD O
) O
and O
show O
that O
representing O
each O
document O
as O
a O
bag O
- O
of O
- O
sentences O
( O
BOS O
) O
and O
leveraging O
recent O
improvements O
in O
multilingual O
sentence O
representations O
, O
SMD O
can O
better O
identify O
cross O
- O
lingual O
document O
pairs O
. O
4.1 O
Cross O
- O
Lingual O
Sentence O
Mover O
‚Äôs O
Distance O
Our O
proposed O
SMD O
solves O
the O
same O
optimization O
problem O
as O
WMD O
, O
but O
utilizes O
cross O
- O
lingual O
sentence O
embeddings O
instead O
of O
word O
embeddings O
asthe O
base O
semantic O
. O
In O
particular O
, O
we O
utilize O
LASER O
sentence O
representations O
( O
Artetxe O
and O
Schwenk O
, O
2019 O
) O
. O
LASER O
learns O
to O
simultaneously O
embed O
93 O
languages O
covering O
23 O
different O
alphabets O
into O
a O
joint O
embedding O
space O
by O
training O
a O
sequence O
- O
tosequence O
system O
on O
many O
language O
pairs O
at O
once O
using O
a O
shared O
encoder O
and O
a O
shared O
byte O
- O
pair O
encoding O
( O
BPE O
) O
vocabulary O
for O
all O
languages O
. O
Utilizing O
LASER O
, O
each O
sentence O
is O
encoded O
using O
an O
LSTM O
encoder O
into O
a O
Ô¨Åxed O
- O
length O
dense O
representation O
. O
We O
adapt O
EMD O
to O
measure O
the O
distance O
between O
two O
documents O
by O
comparing O
the O
distributions O
of O
sentences O
within O
each O
document O
. O
More O
speciÔ¨Åcally O
, O
SMD O
represents O
each O
document O
as O
a O
normalized O
bag O
- O
of O
- O
sentences O
( O
nBOS O
) O
where O
each O
sentence O
has O
associated O
with O
it O
some O
probability O
mass O
. O
As O
distances O
can O
be O
computed O
between O
dense O
sentence O
embeddings O
, O
the O
overall O
document O
distance O
can O
then O
be O
computed O
by O
examining O
how O
close O
the O
distribution O
of O
sentences O
in O
the O
source O
document O
is O
to O
sentences O
in O
the O
target O
document O
. O
We O
formulate O
this O
distance O
as O
the O
minimum O
cost O
of O
transforming O
one O
document O
into O
the O
other O
. O
For O
our O
basic O
formulation O
of O
SMD O
, O
each O
document O
is O
represented O
by O
the O
relative O
frequencies O
of O
sentences O
, O
i.e. O
, O
for O
the O
ithsentence O
in O
the O
document O
, O
dA O
, O
i O
= O
cnt(i)/|A| O
( O
1 O
) O
where|A|is O
the O
total O
number O
of O
sentence O
in O
document O
A O
, O
and O
d O
B O
, O
iis O
deÔ¨Åned O
similarly O
for O
document O
B. O
Under O
this O
assumption O
, O
each O
individual O
sentence O
in O
a O
document O
is O
equally O
important O
and O
probability O
mass O
is O
allocated O
uniformly O
to O
each O
sentence O
. O
Later O
, O
we O
will O
investigate O
alternative O
schemes O
to O
allocating O
probability O
mass O
to O
sentences O
. O
Now O
let O
the O
ithsentence O
be O
represented O
by O
a O
vectorvi‚ààRm O
. O
This O
length O
- O
m O
dense O
embedding O
representation O
for O
each O
sentence O
allows O
us O
to O
deÔ¨Åne O
distances O
between O
the O
i O
thand O
j O
thsentences O
. O
We O
denote O
‚àÜ(i O
, O
j)as O
the O
distance O
between O
the O
ith O
andjthsentences O
and O
let O
Vdenote O
the O
vocabulary O
size O
where O
the O
vocabulary O
is O
the O
unique O
set O
of O
sentences O
within O
a O
document O
pair O
. O
We O
follow O
previous O
works O
( O
Kusner O
et O
al O
. O
, O
2015 O
) O
and O
use O
the O
Euclidean O
distance O
, O
‚àÜ(i O
, O
j O
) O
= O
||vi‚àívj|| O
. O
The O
SMD O
between O
a O
document O
pair O
is O
then O
the O
solution O
to O
the O
linear O
program O
: O
SMD O
( O
A O
, O
B O
) O
= O
min O
T‚â•0V O
/ O
summationdisplay O
i=1V O
/ O
summationdisplay O
j=1Ti O
, O
j√ó‚àÜ(i O
, O
j)(2)618subject O
to O
: O
‚àÄiV O
/ O
summationdisplay O
j=1Ti O
, O
j O
= O
dA O
, O
i O
‚àÄjV O
/ O
summationdisplay O
i=1Ti O
, O
j O
= O
dB O
, O
j O
WhereT‚ààRV√óVis O
a O
nonnegative O
matrix O
, O
where O
each O
Ti O
, O
jdenotes O
how O
much O
of O
sentence O
iin O
document O
Ais O
assigned O
to O
sentences O
jin O
documentB O
, O
and O
constraints O
ensure O
the O
Ô¨Çow O
of O
a O
given O
sentence O
can O
not O
exceed O
its O
allocated O
mass O
. O
Specifically O
, O
SMD O
ensures O
the O
the O
entire O
outgoing O
Ô¨Çow O
from O
sentence O
iequalsdA O
, O
i O
, O
i.e./summationtext O
jTi O
, O
j O
= O
dA O
, O
i. O
Additionally O
, O
the O
amount O
of O
incoming O
Ô¨Çow O
to O
sentencejmust O
match O
dB O
, O
j O
, O
i.e. O
,/summationtext O
iTi O
, O
j O
= O
dB O
, O
j. O
4.2 O
Alternative O
Sentence O
Weighting O
Schemes O
In O
Equation O
1 O
, O
each O
document O
is O
represented O
as O
a O
normalized O
bag O
- O
of O
- O
sentences O
( O
nBOS O
) O
where O
sentences O
are O
equally O
weighted O
. O
However O
, O
we O
posit O
that O
some O
sentences O
may O
be O
more O
semantically O
important O
than O
others O
. O
Sentence O
Length O
Weighting O
The O
Ô¨Årst O
insight O
we O
investigate O
is O
that O
documents O
will O
naturally O
be O
segmented O
into O
sentences O
of O
different O
lengths O
based O
on O
the O
language O
, O
content O
, O
and O
choice O
of O
segmentation O
. O
While O
Equation O
1 O
, O
treats O
each O
sentence O
equally O
, O
we O
posit O
that O
longer O
sentences O
should O
be O
assigned O
larger O
weighting O
than O
shorter O
sentences O
. O
As O
such O
, O
we O
weight O
each O
sentence O
by O
the O
number O
of O
tokens O
in O
the O
sentence O
relative O
to O
the O
total O
number O
of O
tokens O
in O
the O
entire O
document O
, O
i.e. O
, O
for O
theithsentence O
in O
the O
document O
A O
, O
we O
compute O
the O
weighting O
SL(i O
) O
as O
follows O
: O
dA O
, O
i O
= O
cnt(i)¬∑|i|//summationdisplay O
s‚ààAcnt(s)¬∑|s| O
( O
3 O
) O
where|i|and|s|indicate O
the O
number O
of O
tokens O
in O
sentencesiandsrespectively O
. O
As O
such O
, O
longer O
sentence O
receive O
larger O
probability O
mass O
than O
shorter O
sentences O
. O
IDF O
Weighting O
The O
second O
insight O
we O
investigate O
is O
that O
text O
segments O
such O
as O
titles O
and O
navigation O
text O
is O
ubiquitous O
in O
crawled O
data O
yet O
less O
semantically O
informative O
. O
Based O
on O
this O
insight O
, O
we O
apply O
a O
variant O
of O
inverse O
document O
frequency O
( O
IDF O
) O
‚Äì O
a O
weighting O
scheme O
common O
in O
the O
information O
retrieval O
space O
‚Äì O
to O
individual O
sentences O
( O
Robertson O
, O
2004 O
) O
. O
Under O
this O
scheme O
, O
themore O
common O
a O
sentence O
is O
within O
a O
webdomain O
, O
the O
less O
mass O
the O
sentence O
will O
be O
allocated O
. O
For O
sentence O
iin O
a O
web O
- O
domain O
D O
, O
we O
compute O
IDF(i O
) O
as O
follows O
: O
dA O
, O
i= O
1 O
+ O
log|D| O
|{d‚ààD O
: O
i‚ààd}|(4 O
) O
where|{d‚ààD O
: O
s‚ààd}|is O
the O
number O
of O
documents O
where O
the O
sentence O
soccurs O
and O
smoothing O
by1is O
performed O
to O
prevent O
0 O
IDF O
. O
SLIDF O
Weighting O
Finally O
, O
we O
propose O
combining O
both O
sentence O
length O
and O
inverse O
document O
frequency O
into O
a O
joint O
weighting O
scheme O
: O
dA O
, O
i O
= O
SL(i)¬∑IDF O
( O
i O
) O
( O
5 O
) O
In O
this O
scheme O
, O
each O
sentence O
is O
weighted O
proportionally O
to O
the O
number O
of O
tokens O
it O
contains O
as O
well O
as O
by O
the O
IDF O
of O
the O
sentence O
within O
the O
domain O
. O
This O
weighting O
scheme O
is O
reminiscent O
of O
the O
use O
of O
tf O
- O
idf O
to O
determine O
word O
relevance O
( O
Ramos O
et O
al O
. O
, O
2003 O
) O
, O
but O
instead O
sentence O
length O
and O
idf O
are O
used O
to O
determine O
sentence O
importance O
. O
4.3 O
Fast O
Distance O
Approximation O
While O
EMD O
and O
other O
variants O
have O
demonstrated O
superior O
performance O
in O
many O
retrieval O
and O
classiÔ¨Åcation O
tasks O
, O
they O
have O
also O
been O
shown O
to O
suffer O
from O
high O
computational O
complexity O
O(p3logp O
) O
, O
wherepdenotes O
the O
number O
of O
unique O
semantic O
units O
in O
a O
document O
pair O
. O
As O
such O
, O
we O
investigate O
techniques O
to O
speed O
up O
this O
computation O
. O
Relaxed O
SMD O
Given O
the O
scalability O
challenges O
for O
computing O
WMD O
, O
simpliÔ¨Åed O
version O
of O
WMD O
was O
proposed O
that O
relaxes O
one O
of O
the O
two O
constraints O
in O
the O
original O
formulation O
( O
Kusner O
et O
al O
. O
, O
2015 O
) O
. O
Applying O
the O
same O
principle O
to O
SMD O
, O
we O
formulate O
: O
SMD O
( O
A O
, O
B O
) O
= O
min O
T‚â•0V O
/ O
summationdisplay O
i=1V O
/ O
summationdisplay O
j=1Ti O
, O
j√ó‚àÜ(i O
, O
j O
) O
subject O
to:‚àÄi O
/ O
summationtextV O
j=1Ti O
, O
j O
= O
dA O
, O
i. O
Analogous O
to O
the O
relaxed O
- O
WMD O
, O
this O
relaxed O
problem O
yields O
a O
lower O
- O
bound O
to O
the O
SMD O
as O
every O
SMD O
solution O
satisfying O
both O
constraints O
remains O
a O
feasible O
solution O
if O
one O
constraint O
is O
removed O
. O
The O
optimal O
solution O
can O
be O
found O
by O
simply O
allocating O
the O
mass O
in O
each O
source O
sentence O
to O
the O
closest O
sentence O
in O
the O
target O
document.619The O
same O
computation O
can O
be O
performed O
in O
the O
reverse O
direction O
by O
removing O
the O
second O
constraint:‚àÄj O
/ O
summationtextV O
i=1Ti O
, O
j O
= O
dB O
, O
j. O
Similarly O
, O
the O
optimal O
solution O
allocates O
the O
mass O
sentences O
in O
the O
target O
document O
to O
the O
closest O
sentence O
in O
the O
source O
document O
. O
Both O
these O
distances O
can O
be O
calculated O
by O
computing O
the O
distance O
matrix O
between O
all O
pairs O
of O
sentences O
inO(p2)time O
. O
For O
a O
tighter O
estimate O
of O
distance O
, O
the O
maximum O
of O
the O
two O
resultant O
distances O
can O
be O
used O
. O
Greedy O
Mover O
‚Äôs O
Distance O
We O
introduce O
an O
alternative O
to O
the O
relaxed O
- O
EMD O
variant O
wherein O
we O
keep O
both O
constraints O
in O
the O
transportation O
problem O
, O
but O
identify O
an O
approximate O
transportation O
scheme O
. O
This O
greedy O
mover O
‚Äôs O
distance O
( O
GMD O
) O
Ô¨Ånds O
the O
closest O
sentence O
pair O
between O
the O
source O
and O
target O
and O
moves O
as O
much O
mass O
between O
the O
two O
sentences O
as O
possible O
; O
the O
algorithm O
moves O
to O
the O
next O
closest O
until O
all O
mass O
has O
been O
moved O
while O
maintaining O
both O
constraints O
. O
Algorithm O
1 O
: O
Greedy O
Mover O
‚Äôs O
Distance O
Input O
: O
ds O
, O
dt O
, O
ws O
, O
wt O
Output O
: O
‚àÜ(ds O
, O
dt O
) O
1pairs‚Üê{(ss O
, O
st)forss O
, O
st‚ààds√ódt O
} O
in O
ascending O
order O
by O
/bardblss‚àíst O
/ O
bardbl O
2distance‚Üê0.0 O
3forss O
, O
st‚ààpairs O
do O
4 O
Ô¨Çow‚Üêmin(ws[ss O
] O
, O
wt[st O
] O
) O
5 O
ws[ss]‚Üêws[ss]‚àíÔ¨Çow O
6 O
wt[st]‚Üêwt[st]‚àíÔ¨Çow O
7 O
distance‚Üêdistance O
+ O
/bardblss‚àíst O
/ O
bardbl√óÔ¨Çow O
8end O
9return O
total O
As O
seen O
in O
Algorithm O
1 O
, O
the O
algorithm O
takes O
a O
source O
document O
( O
ds O
) O
and O
a O
target O
document O
( O
dt O
) O
as O
well O
as O
the O
probability O
mass O
for O
the O
sentences O
in O
each O
: O
respectively O
wsandwt O
. O
The O
algorithm O
Ô¨Årst O
computes O
the O
euclidean O
distance O
between O
each O
sentence O
pair O
from O
source O
to O
target O
and O
sorts O
these O
pairs O
in O
ascending O
order O
by O
their O
euclidean O
distance O
. O
The O
algorithm O
then O
iteratively O
chooses O
the O
closest O
sentence O
pair O
and O
moves O
the O
mass O
of O
the O
smallest O
sentence O
from O
the O
source O
to O
the O
target O
and O
subtracting O
this O
moved O
math O
from O
both O
. O
The O
algorithm O
terminates O
when O
all O
moveable O
mass O
has O
been O
moved O
. O
Unlike O
the O
exact O
solution O
to O
EMD O
, O
the O
runtime O
complexity O
is O
a O
more O
tractable O
O(|ds||dt|√ólog(|ds||dt|))which O
is O
dominated O
by O
the O
cost O
of O
sorting O
all O
candidate O
pairs O
. O
Unlike O
the O
relaxation O
, O
both O
constraints O
are O
satisÔ¨Åed O
but O
the O
transport O
is O
not O
necessarily O
optimal O
. O
As O
such O
, O
GMDyields O
an O
upper O
- O
bound O
to O
the O
exact O
computation O
. O
We O
experimentally O
compare O
the O
effect O
of O
both O
approximation O
strategies O
on O
downstream O
document O
alignment O
in O
Section O
7 O
. O
5 O
Document O
Matching O
Algorithm O
In O
addition O
to O
a O
distance O
metric O
( O
i.e. O
SMD O
) O
, O
we O
need O
a O
document O
matching O
algorithm O
to O
determine O
the O
best O
mapping O
between O
documents O
in O
two O
languages O
. O
In O
our O
case O
, O
this O
works O
as O
follows O
: O
for O
any O
given O
webdomain O
, O
each O
document O
in O
the O
source O
document O
set O
, O
Dsis O
paired O
with O
each O
document O
in O
the O
target O
set O
, O
Dt O
, O
yielding|Ds√óDt|scored O
pairs O
‚Äì O
a O
fully O
connected O
bipartite O
graph O
representing O
all O
candidate O
pairings O
. O
Similar O
to O
previous O
works O
( O
Buck O
and O
Koehn O
, O
2016b O
) O
, O
the O
expected O
output O
assumes O
that O
each O
webpage O
in O
the O
non O
- O
dominant O
language O
has O
a O
translated O
or O
comparable O
counterpart O
. O
As O
visualized O
in O
Figure O
1 O
, O
this O
yields O
a O
min(|Ds|,|Dt| O
) O
expected O
number O
of O
aligned O
pairs O
. O
While O
an O
optimal O
matching O
maximizing O
scoring O
can O
be O
solved O
using O
the O
Hungarian O
algorithm O
( O
Munkres O
, O
1957 O
) O
, O
the O
complexity O
of O
this O
algorithm O
isO(max(|Ds||Dt|)3)which O
is O
intractable O
to O
even O
moderately O
sized O
web O
domains O
. O
As O
such O
, O
similar O
to O
the O
work O
in O
( O
Buck O
and O
Koehn O
, O
2016b O
) O
, O
a O
one O
- O
to O
- O
one O
matching O
between O
English O
and O
nonEnglish O
documents O
is O
enforced O
by O
applying O
, O
competitive O
matching O
, O
a O
greedy O
bipartite O
matching O
algorithm O
. O
Algorithm O
2 O
: O
Competitive O
Matching O
Input O
: O
P={(ds O
, O
dt)|ds‚ààDs O
, O
dt‚ààDt O
} O
Output O
: O
P O
/ O
prime={(ds O
, O
i O
, O
dt O
, O
i O
) O
, O
... O
} O
‚äÇP O
1scored‚Üê{(p O
, O
score O
( O
p))forp‚ààP O
} O
2sorted‚Üêsort(scored O
) O
in O
ascending O
order O
3aligned‚Üê‚àÖ O
4Ss‚Üê‚àÖ O
5St‚Üê‚àÖ O
6fords O
, O
dt‚ààsorted O
do O
7 O
ifds/‚ààSs‚àßdt/‚ààStthen O
8 O
aligned‚Üêaligned‚à™{(ds O
, O
dt O
) O
} O
9 O
Ss‚ÜêSs‚à™ds O
10 O
St‚ÜêSt‚à™dt O
11end O
12return O
aligned O
In O
Algorithm O
2 O
, O
the O
algorithm O
Ô¨Årst O
scores O
each O
candidate O
document O
pair O
using O
a O
distance O
function O
and O
then O
sorts O
pairs O
from O
closest O
to O
farthest O
. O
The O
algorithm O
then O
iteratively O
selects O
the O
closest O
document O
pair O
as O
long O
as O
the O
dsanddtof O
each O
pair O
have O
not O
been O
used O
in O
a O
previous O
( O
closer O
) O
pair O
. O
The620algorithm O
terminates O
when O
min(|Ds|,|Dt|)pairs O
have O
been O
selected O
. O
Unlike O
the O
Hungarian O
algorithm O
, O
the O
runtime O
complexity O
is O
a O
more O
tractable O
O(|Ds||Dt|√ólog(|Ds||Dt|))which O
is O
dominated O
by O
the O
cost O
of O
sorting O
all O
candidate O
pairs O
. O
6 O
Experiments O
and O
Results O
In O
this O
section O
, O
we O
explore O
the O
question O
of O
whether O
SMD O
can O
be O
used O
as O
a O
dissimilarity O
metric O
for O
the O
document O
alignment O
problem O
. O
Moreover O
, O
we O
explore O
which O
sentence O
weighting O
schemes O
yield O
the O
best O
results O
. O
6.1 O
Experimental O
Setup O
Dataset O
We O
evaluate O
on O
the O
test O
set O
from O
the O
URL O
- O
Aligned O
CommonCrawl O
dataset O
( O
El O
- O
Kishky O
et O
al O
. O
, O
2019 O
) O
across O
47 O
language O
directions O
. O
Baseline O
Methods O
For O
comparison O
, O
we O
implemented O
two O
existing O
and O
intuitive O
document O
scoring O
baselines O
from O
( O
El O
- O
Kishky O
et O
al O
. O
, O
2019 O
) O
. O
The O
direct O
embedding O
( O
DE O
) O
, O
directly O
embeds O
the O
entire O
content O
of O
a O
document O
using O
LASER O
. O
The O
second O
method O
sentence O
averaging O
( O
SA O
) O
embeds O
all O
sentences O
in O
a O
document O
using O
LASER O
and O
averages O
all O
embeddings O
to O
get O
a O
document O
representation O
. O
Cosine O
similarity O
on O
the O
embedded O
representation O
is O
used O
to O
compare O
documents O
. O
SMD O
Weightings O
We O
evaluate O
four O
weighting O
schemes O
for O
SMD O
: O
( O
1 O
) O
vanilla O
SMD O
with O
each O
sentence O
equally O
weighted(2 O
) O
weighting O
by O
sentence O
length O
( O
SL O
) O
where O
SMD O
is O
computed O
under O
a O
scheme O
where O
each O
sentence O
is O
weighted O
by O
its O
length O
( O
number O
of O
tokens O
) O
normalized O
by O
the O
length O
of O
the O
entire O
document O
( O
3 O
) O
weighting O
by O
inverse O
document O
frequence O
( O
IDF O
) O
where O
SMD O
is O
computed O
under O
a O
scheme O
where O
each O
sentence O
is O
weighted O
by O
the O
idf O
of O
the O
sentence O
( O
4 O
) O
computing O
SMD O
under O
a O
scheme O
where O
each O
sentence O
is O
weighted O
by O
both O
sentence O
length O
and O
inverse O
document O
frequency O
( O
SLIDF O
) O
. O
Under O
all O
these O
schemes O
, O
all O
weights O
are O
normalized O
to O
unit O
measure O
. O
Distance O
approximation O
We O
use O
the O
greedy O
mover O
‚Äôs O
distance O
approximation O
for O
all O
variants O
reported O
. O
In O
Section O
7 O
we O
further O
explore O
the O
performance O
of O
the O
full O
distance O
computation O
and O
relaxed O
variants O
that O
were O
described O
in O
Section O
4.3 O
. O
Evaluation O
Metric O
for O
Document O
Alignment O
Because O
the O
ground O
- O
truth O
document O
pairs O
only O
reÔ¨Çect O
a O
high O
- O
precision O
set O
of O
web O
- O
document O
pairsthat O
are O
translations O
or O
of O
comparable O
content O
, O
there O
may O
be O
many O
other O
valid O
cross O
- O
lingual O
document O
pairs O
within O
each O
web O
- O
domain O
that O
are O
not O
included O
in O
the O
ground O
truth O
set O
. O
As O
such O
, O
we O
evaluate O
each O
method O
‚Äôs O
generated O
document O
pairs O
solely O
on O
the O
recall O
( O
i.e. O
what O
percentage O
of O
the O
aligned O
pages O
in O
the O
test O
set O
are O
found O
) O
from O
the O
ground O
truth O
pairs O
. O
For O
each O
scoring O
method O
, O
we O
score O
document O
pairs O
from O
the O
source O
and O
target O
languages O
within O
the O
same O
web O
- O
domain O
using O
the O
proposed O
document O
distance O
metrics O
described O
above O
. O
For O
the O
alignment O
, O
we O
report O
the O
performance O
for O
each O
distance O
metric O
after O
applying O
the O
competitive O
matching O
alignment O
algorithm O
as O
described O
in O
Algorithm O
2 O
. O
6.2 O
Results O
In O
Table O
1 O
, O
we O
Ô¨Årst O
notice O
that O
constructing O
document O
representations O
by O
directly O
embedding O
( O
DE O
) O
the O
entire O
content O
of O
each O
document O
and O
computing O
document O
similarity O
using O
cosine O
similarity O
of O
the O
representation O
severely O
under O
- O
performs O
compared O
to O
individually O
embedding O
sentences O
and O
constructing O
the O
document O
representations O
by O
averaging O
the O
individual O
sentence O
representations O
within O
the O
document O
( O
SA O
) O
. O
This O
is O
intuitive O
as O
LASER O
embeddings O
were O
trained O
on O
parallel O
sentences O
and O
embedding O
much O
larger O
documents O
directly O
using O
LASER O
results O
in O
poorer O
representations O
than O
by O
Ô¨Årst O
embedding O
smaller O
sentences O
and O
combining O
them O
into O
the O
Ô¨Ånal O
document O
representation O
. O
Comparing O
the O
basic O
SMD O
to O
the O
best O
performing O
baseline O
( O
SA O
) O
, O
we O
see O
a O
4 O
% O
, O
12 O
% O
, O
and O
20 O
% O
improvement O
across O
high O
, O
mid O
, O
and O
low O
- O
resource O
directions O
respectively O
. O
This O
improvement O
suggests O
that O
summing O
sentence O
embeddings O
into O
a O
single O
document O
representation O
degrades O
the O
quality O
of O
the O
resultant O
document O
distances O
over O
computing O
document O
distances O
by O
keeping O
all O
sentence O
representations O
separate O
and O
computing O
distances O
between O
individual O
sentence O
pairs O
and O
combining O
these O
distances O
into O
a O
Ô¨Ånal O
document O
distance O
. O
This O
is O
more O
pronounced O
in O
lower O
- O
resource O
over O
higher O
- O
resource O
pairs O
which O
may O
be O
due O
to O
poorer O
lower O
- O
resource O
embeddings O
due O
to O
LASER O
being O
trained O
on O
fewer O
low O
- O
resource O
sentence O
pairs O
. O
As O
such O
averaging O
is O
more O
destructive O
to O
these O
representations O
while O
SMD O
avoids O
this O
degradation O
. O
Further O
analysis O
veriÔ¨Åed O
the O
intuition O
that O
different O
sentences O
should O
be O
allocated O
different O
weighting O
in O
SMD O
. O
Assigning O
mass O
proportional O
to O
the621Recall O
Language O
DE O
SA O
SMD O
SL O
IDF O
SLIDF O
French O
0.39 O
0.84 O
0.81 O
0.84 O
0.83 O
0.85 O
Spanish O
0.34 O
0.53 O
0.59 O
0.63 O
0.62 O
0.64 O
Russian O
0.06 O
0.64 O
0.69 O
0.69 O
0.70 O
0.71 O
German O
0.52 O
0.74 O
0.78 O
0.76 O
0.77 O
0.77 O
Italian O
0.22 O
0.47 O
0.55 O
0.56 O
0.56 O
0.59 O
Portuguese O
0.17 O
0.36 O
0.39 O
0.41 O
0.38 O
0.40 O
Dutch O
0.28 O
0.49 O
0.54 O
0.54 O
0.54 O
0.56 O
Indonesian O
0.11 O
0.47 O
0.49 O
0.52 O
0.51 O
0.53 O
Polish O
0.17 O
0.38 O
0.45 O
0.45 O
0.46 O
0.46 O
Turkish O
0.12 O
0.38 O
0.52 O
0.56 O
0.57 O
0.59 O
Swedish O
0.19 O
0.40 O
0.44 O
0.44 O
0.46 O
0.45 O
Danish O
0.27 O
0.62 O
0.63 O
0.69 O
0.65 O
0.69 O
Czech O
0.15 O
0.40 O
0.43 O
0.44 O
0.44 O
0.43 O
Bulgarian O
0.07 O
0.43 O
0.52 O
0.54 O
0.55 O
0.52 O
Finnish O
0.06 O
0.47 O
0.51 O
0.51 O
0.54 O
0.52 O
Norwegian O
0.13 O
0.33 O
0.37 O
0.39 O
0.42 O
0.41 O
A O
VG O
0.20 O
0.50 O
0.54 O
0.56 O
0.56 O
0.57 O
( O
a O
) O
High O
- O
resource O
directions O
. O
Recall O
Language O
DE O
SA O
SMD O
SL O
IDF O
SLIDF O
Romanian O
0.15 O
0.40 O
0.44 O
0.43 O
0.45 O
0.43 O
Vietnamese O
0.06 O
0.28 O
0.29 O
0.29 O
0.29 O
0.32 O
Ukrainian O
0.05 O
0.68 O
0.67 O
0.78 O
0.78 O
0.82 O
Greek O
0.05 O
0.31 O
0.47 O
0.48 O
0.49 O
0.49 O
Korean O
0.06 O
0.34 O
0.60 O
0.54 O
0.61 O
0.60 O
Arabic O
0.04 O
0.32 O
0.63 O
0.59 O
0.65 O
0.61 O
Croatian O
0.16 O
0.37 O
0.40 O
0.40 O
0.41 O
0.40 O
Slovak O
0.20 O
0.41 O
0.46 O
0.46 O
0.46 O
0.44 O
Thai O
0.02 O
0.19 O
0.41 O
0.33 O
0.47 O
0.41 O
Hebrew O
0.05 O
0.18 O
0.39 O
0.43 O
0.41 O
0.41 O
Hindi O
0.04 O
0.27 O
0.34 O
0.54 O
0.52 O
0.53 O
Hungarian O
0.15 O
0.49 O
0.50 O
0.54 O
0.51 O
0.54 O
Lithuanian O
0.11 O
0.73 O
0.79 O
0.79 O
0.80 O
0.80 O
Slovenian O
0.13 O
0.33 O
0.34 O
0.35 O
0.36 O
0.36 O
Persian O
0.06 O
0.32 O
0.56 O
0.57 O
0.53 O
0.59 O
A O
VG O
0.09 O
0.37 O
0.49 O
0.50 O
0.52 O
0.52 O
( O
b O
) O
Mid O
- O
resource O
directions O
. O
Recall O
Language O
DE O
SA O
SMD O
SL O
IDF O
SLIDF O
Estonian O
0.28 O
0.52 O
0.69 O
0.66 O
0.74 O
0.72 O
Bengali O
0.05 O
0.32 O
0.78 O
0.72 O
0.77 O
0.79 O
Albanian O
0.23 O
0.56 O
0.66 O
0.65 O
0.65 O
0.66 O
Macedonian O
0.02 O
0.33 O
0.32 O
0.36 O
0.38 O
0.33 O
Urdu O
0.06 O
0.22 O
0.60 O
0.60 O
0.49 O
0.56 O
Serbian O
0.06 O
0.59 O
0.75 O
0.74 O
0.74 O
0.71 O
Azerbaijani O
0.08 O
0.34 O
0.74 O
0.74 O
0.75 O
0.74 O
Armenian O
0.02 O
0.18 O
0.32 O
0.35 O
0.34 O
0.38 O
Belarusian O
0.07 O
0.47 O
0.67 O
0.69 O
0.73 O
0.71 O
Georgian O
0.06 O
0.24 O
0.46 O
0.48 O
0.45 O
0.45 O
Tamil O
0.02 O
0.20 O
0.51 O
0.45 O
0.51 O
0.53 O
Marathi O
0.02 O
0.11 O
0.43 O
0.46 O
0.33 O
0.39 O
Kazakh O
0.05 O
0.31 O
0.44 O
0.46 O
0.45 O
0.45 O
Mongolian O
0.03 O
0.13 O
0.18 O
0.22 O
0.21 O
0.23 O
Burmese O
0.01 O
0.10 O
0.26 O
0.33 O
0.46 O
0.46 O
Bosnian O
0.18 O
0.64 O
0.61 O
0.69 O
0.65 O
0.72 O
A O
VG O
0.08 O
0.33 O
0.53 O
0.54 O
0.54 O
0.55 O
( O
c O
) O
Low O
- O
resource O
directions O
. O
Table O
1 O
: O
Alignment O
recall O
on O
URL O
- O
aligned O
CommonCrawl O
dataset O
. O
number O
of O
tokens O
in O
the O
sentence O
( O
SL O
) O
, O
we O
see O
a O
2 O
% O
, O
1 O
% O
and O
1 O
% O
absolute O
improvement O
in O
recall O
in O
high O
, O
mid O
, O
and O
low O
- O
resource O
directions O
over O
assigning O
equal O
probability O
mass O
. O
This O
supports O
the O
claim O
that O
longer O
sentences O
should O
be O
allocated O
higher O
importance O
weight O
over O
shorter O
sentences O
as O
they O
contain O
more O
semantic O
content O
. O
The O
second O
assumption O
we O
investigated O
is O
that O
sentences O
that O
are O
common O
within O
a O
webdomain O
have O
less O
semantic O
importance O
and O
should O
be O
allocated O
less O
probability O
mass O
when O
computing O
SMD O
. O
After O
computing O
SMD O
with O
each O
sentence O
allocated O
mass O
according O
to O
inverse O
document O
frequency O
( O
IDF O
) O
and O
normalized O
to O
unit O
measure O
, O
we O
see O
a O
2 O
% O
, O
3 O
% O
, O
and O
1 O
% O
improvement O
over O
SMD O
for O
high O
, O
mid O
, O
and O
lowresource O
directions O
. O
Finally O
, O
when O
combining O
both O
sentence O
length O
and O
inverse O
document O
frequency O
( O
SLIDF O
) O
and O
normalizing O
to O
unit O
measure O
, O
we O
see O
a O
3 O
% O
, O
3 O
% O
and O
2 O
% O
absolute O
improvement O
in O
recall O
for O
high O
, O
mid O
, O
and O
low O
- O
resource O
directions O
. O
Overall O
, O
our O
SMD O
with O
SLIDF O
weighting O
scheme O
outperforms O
the O
sentence O
averaging O
baseline O
by O
7 O
% O
on O
high O
- O
resource O
directions O
, O
15 O
% O
on O
mid O
- O
resource O
directions O
, O
and O
22 O
% O
on O
low O
- O
resource O
directions O
. O
7 O
Discussion O
Although O
using O
sentences O
over O
words O
as O
the O
base O
semantic O
unit O
drastically O
reduces O
the O
overall O
cost O
of O
computing O
EMD O
- O
based O
metrics O
, O
the O
cubic O
computation O
still O
prohibits O
its O
use O
as O
a O
fast O
distance O
metric O
for O
large O
- O
scale O
alignment O
efforts O
. O
As O
such O
, O
in O
Section O
4.3 O
we O
described O
two O
faster O
approximations O
to O
EMD O
computation O
: O
( O
1 O
) O
a O
relaxation O
of O
constraints O
resulting O
in O
a O
lower O
bound O
and O
( O
2 O
) O
a O
greedy O
algo O
- O
rithm O
for O
computing O
assigning O
transport O
representing O
an O
upper O
bound O
. O
We O
Ô¨Årst O
analyze O
and O
compare O
the O
distances O
from O
each O
approximation O
scheme O
to O
the O
exact O
SMD O
computation O
. O
Method O
Tau O
Recall O
MAE O
Runtime O
( O
s O
) O
Exact O
- O
SMD O
1.00 O
0.69 O
0.000 O
0.402 O
Relaxed O
- O
SMD O
0.70 O
0.58 O
0.084 O
0.031 O
Greedy O
- O
SMD O
0.98 O
0.69 O
0.010 O
0.107 O
Table O
2 O
: O
Comparing O
exact O
SMD O
computation O
to O
approximation O
schemes O
for O
computing O
SMD O
on O
10 O
webdomains O
. O
In O
Figure O
3 O
, O
we O
see O
that O
the O
distance O
computations O
for O
exact O
SMD O
and O
the O
greedy O
SMD O
approximation O
are O
highly O
correlated O
with O
small O
variance O
, O
while O
the O
relaxed O
approximation O
is O
less O
so O
with O
high O
variance O
. O
Additionally O
, O
as O
discussed O
in O
Section O
4.3 O
, O
the O
visualizations O
empirically O
suggest O
that O
our O
greedy O
approximation O
is O
a O
fairly O
tight O
upper O
bound O
while O
the O
relaxed O
approximation O
is O
a O
looser O
lower O
bound O
. O
In O
Table O
2 O
, O
we O
compare O
quantitative O
metrics O
for O
the O
relaxed O
and O
greedy O
approximations O
to O
the O
exact O
solution O
of O
SMD O
on O
ten O
webdomains O
. O
Our O
Ô¨Årst O
evaluation O
investigates O
how O
the O
approximate O
computation O
of O
distances O
affects O
the O
resultant O
ordering O
of O
document O
pairs O
. O
For O
the O
ten O
selected O
webdomains O
, O
we O
sort O
the O
document O
pairs O
in O
order O
by O
their O
computed O
distances O
and O
compare O
the O
ordering O
to O
the O
ordering O
induced O
by O
the O
exact O
computation O
of O
SMD O
. O
We O
evaluate O
the O
orderings O
using O
the O
Kendall O
- O
Tau O
metric O
( O
Kendall O
, O
1938 O
) O
which O
measures O
the O
agreement O
between O
the O
two O
rankings O
; O
if O
the O
agreement O
between O
the O
two O
rankings O
is O
perfect O
( O
i.e. O
, O
the O
two622French O
German O
Russian O
Danish O
Spanish O
Italian O
Dutch O
Turkish O
Bulgarian O
Indonesian O
Finnish O
Swedish O
Polish O
Czech O
Portugese O
Norwegian0.00.20.40.60.8RecallHigh O
- O
Resource O
Relaxed O
vs O
Greedy O
Mover O
's O
Distance O
Greedy O
Mover O
's O
Distance O
Relaxed O
Mover O
's O
Distance(a O
) O
High O
- O
resource O
directions O
. O
Lithuanian O
Ukrainian O
Persian O
Arabic O
Hindi O
Hungarian O
Korean O
Slovak O
Romanian O
Croatian O
Hebrew O
Greek O
Slovenian O
Thai O
Vietnamese0.00.20.40.60.8RecallMid O
- O
Resource O
Relaxed O
vs O
Greedy O
Mover O
's O
Distance O
Greedy O
Mover O
's O
Distance O
Relaxed O
Mover O
's O
Distance O
( O
b O
) O
Mid O
- O
resource O
directions O
. O
Serbian O
Bengali O
Belarusian O
Bosnian O
Estonian O
Albanian O
Urdu O
Azerbaijani O
Kazakh O
Tamil O
Georgian O
Burmese O
Marathi O
Macedonian O
Armenian O
Mongolian0.00.20.40.6RecallLow O
Resource O
Relaxed O
vs O
Greedy O
Mover O
's O
Distance O
Greedy O
Mover O
's O
Distance O
Relaxed O
Mover O
's O
Distance O
( O
c O
) O
Low O
- O
resource O
directions O
. O
Figure O
2 O
: O
Document O
alignment O
results O
for O
different O
distance O
approximation O
techniques O
. O
0 O
100 O
200 O
300 O
400 O
500 O
Document O
/ O
uni00A0Pairs0.20.40.60.81.0Distances O
Distance O
/ O
uni00A0Computations O
Exact O
/ O
uni00ADXLSMD O
Relaxed O
/ O
uni00ADXLSMD O
Greedy O
/ O
uni00ADXLSMD O
Figure O
3 O
: O
Exact O
, O
relaxed O
, O
and O
greedy O
- O
SMD O
distances O
sorted O
by O
Exact O
- O
SMD O
for O
a O
random O
selection O
of O
document O
pairs O
. O
rankings O
are O
the O
same O
) O
the O
coefÔ¨Åcient O
has O
value O
1 O
and O
if O
the O
disagreement O
between O
the O
two O
rankings O
is O
perfect O
( O
i.e. O
, O
one O
ranking O
is O
the O
reverse O
of O
the O
other O
) O
the O
coefÔ¨Åcient O
has O
value O
-1 O
. O
Intuitively O
, O
we O
would O
like O
the O
distances O
computed O
by O
an O
approximation O
to O
induce O
a O
similar O
ordering O
to O
the O
ordering O
by O
the O
exact O
distance O
computation O
. O
Comparing O
the O
KendallTau O
for O
the O
relaxed O
and O
greedy O
approximations O
in O
relation O
to O
the O
exact O
computation O
shows O
that O
the O
order O
induced O
by O
the O
greedy O
approximation O
is O
very O
similar O
to O
the O
ordering O
induced O
by O
the O
exact O
computation O
while O
the O
relaxed O
approximation O
varies O
considerably O
. O
Additionally O
, O
the O
relaxed O
approximation O
demonstrates O
fairly O
high O
mean O
absolute O
error O
( O
MAE O
) O
and O
results O
in O
lower O
document O
alignment O
recall O
when O
compared O
to O
the O
exact O
computation O
of O
SMD O
, O
while O
our O
greedy O
approximation O
performs O
comparably O
and O
shows O
insigniÔ¨Åcant O
MAE O
. O
Finally O
, O
while O
the O
runtime O
of O
the O
relaxed O
computation O
is O
the O
fastest O
at O
13 O
times O
faster O
than O
the O
exact O
computation O
, O
our O
greedy O
algorithm O
is O
approximately O
4 O
times O
faster O
while O
delivering O
comparable O
document O
alignment O
performance O
to O
the O
exact O
computation O
and O
superior O
performance O
to O
the O
relaxed O
computation O
. O
To O
ensure O
that O
the O
greedy O
algorithm O
consistently O
outperforms O
the O
relaxed O
algorithm O
on O
document O
alignment O
, O
we O
investigate O
the O
effect O
of O
using O
eachapproximation O
method O
on O
the O
downstream O
document O
alignment O
performance O
across O
47 O
language O
pairs O
of O
varying O
resource O
availability O
. O
Approximation O
Low O
Mid O
High O
All O
Relaxed O
- O
SMD O
0.44 O
0.43 O
0.50 O
0.46 O
Greedy O
- O
SMD O
0.54 O
0.50 O
0.56 O
0.54 O
Table O
3 O
: O
Document O
alignment O
performance O
of O
fast O
methods O
for O
approximating O
the O
same O
variant O
of O
SMD O
. O
As O
seen O
in O
Figure O
2 O
, O
in O
45 O
of O
the O
47 O
evaluated O
language O
pairs O
, O
our O
proposed O
Greedy O
Mover O
‚Äôs O
Distance O
approximation O
yielded O
higher O
downstream O
recall O
in O
our O
alignment O
task O
over O
using O
the O
relaxed O
distance O
proposed O
for O
use O
in O
WMD O
( O
Kusner O
et O
al O
. O
, O
2015 O
) O
. O
In O
Table O
3 O
, O
we O
see O
a O
10 O
% O
, O
7 O
% O
, O
and O
6 O
% O
improvement O
in O
downstream O
recall O
across O
low O
, O
mid O
, O
and O
high O
- O
resource O
directions O
respectively O
. O
These O
results O
indicate O
that O
relaxing O
one O
of O
the O
two O
constraints O
in O
EMD O
is O
too O
lax O
for O
measuring O
an O
accurate O
distance O
. O
We O
posit O
this O
is O
because O
there O
are O
many O
sentences O
that O
can O
be O
considered O
‚Äú O
hubs O
‚Äù O
that O
are O
semantically O
close O
to O
many O
other O
sentences O
. O
These O
sentences O
can O
have O
a O
lot O
of O
probability O
mass O
allocated O
to O
them O
, O
resulting O
in O
a O
lower O
approximate O
EMD O
. O
Our O
greedy O
approximation O
ensures O
that O
both O
constraints O
are O
maintained O
even O
if O
the O
Ô¨Ånal O
result O
does O
not O
reÔ¨Çect O
the O
optimal O
transport O
. O
8 O
Conclusion O
In O
this O
paper O
, O
we O
introduce O
SMD O
a O
cross O
- O
lingual O
sentence O
mover O
‚Äôs O
distance O
metric O
for O
automatically O
assessing O
the O
semantic O
similarity O
of O
two O
documents O
in O
different O
languages O
. O
We O
leverage O
state O
- O
of O
- O
the O
- O
art O
multilingual O
sentence O
embeddings O
and O
apply O
SMD O
to O
the O
task O
of O
cross O
- O
lingual O
document O
alignment O
. O
We O
demonstrate O
that O
our O
new O
metric O
outperforms O
other O
unsupervised O
metrics O
by O
a O
margin O
, O
especially O
in O
medium O
and O
low O
- O
resourced O
conditions.623References O
Sadaf O
AbduI O
- O
Rauf O
and O
Holger O
Schwenk O
. O
2009 O
. O
On O
the O
use O
of O
comparable O
corpora O
to O
improve O
smt O
performance O
. O
In O
Proceedings O
of O
the O
12th O
Conference O
of O
the O
European O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
16‚Äì23 O
. O
Association O
for O
Computational O
Linguistics O
. O
Mikel O
Artetxe O
and O
Holger O
Schwenk O
. O
2019 O
. O
Massively O
multilingual O
sentence O
embeddings O
for O
zeroshot O
cross O
- O
lingual O
transfer O
and O
beyond O
. O
Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
7:597‚Äì610 O
. O
Kubilay O
Atasu O
, O
Thomas O
Parnell O
, O
Celestine O
D O
¬®unner O
, O
Manolis O
Sifalakis O
, O
Haralampos O
Pozidis O
, O
Vasileios O
Vasileiadis O
, O
Michail O
Vlachos O
, O
Cesar O
Berrospi O
, O
and O
Abdel O
Labbi O
. O
2017 O
. O
Linear O
- O
complexity O
relaxed O
word O
mover O
‚Äôs O
distance O
with O
gpu O
acceleration O
. O
In O
2017 O
IEEE O
International O
Conference O
on O
Big O
Data O
( O
Big O
Data O
) O
, O
pages O
889‚Äì896 O
. O
IEEE O
. O
Georgios O
Balikas O
, O
Charlotte O
Laclau O
, O
Ievgen O
Redko O
, O
and O
Massih O
- O
Reza O
Amini O
. O
2018 O
. O
Cross O
- O
lingual O
document O
retrieval O
using O
regularized O
wasserstein O
distance O
. O
In O
European O
Conference O
on O
Information O
Retrieval O
, O
pages O
398‚Äì410 O
. O
Springer O
. O
Christian O
Buck O
and O
Philipp O
Koehn O
. O
2016a O
. O
Findings O
of O
the O
wmt O
2016 O
bilingual O
document O
alignment O
shared O
task O
. O
In O
Proceedings O
of O
the O
First O
Conference O
on O
Machine O
Translation O
: O
Volume O
2 O
, O
Shared O
Task O
Papers O
, O
pages O
554‚Äì563 O
. O
Christian O
Buck O
and O
Philipp O
Koehn O
. O
2016b O
. O
Quick O
and O
reliable O
document O
alignment O
via O
tf O
/ O
idf O
- O
weighted O
cosine O
distance O
. O
In O
Proceedings O
of O
the O
First O
Conference O
on O
Machine O
Translation O
: O
Volume O
2 O
, O
Shared O
Task O
Papers O
, O
pages O
672‚Äì678 O
. O
Jiang O
Chen O
and O
Jian O
- O
Yun O
Nie O
. O
2000 O
. O
Parallel O
web O
text O
mining O
for O
cross O
- O
language O
ir O
. O
In O
Content O
- O
Based O
Multimedia O
Information O
Access O
- O
Volume O
1 O
, O
pages O
62 O
‚Äì O
77 O
. O
LE O
CENTRE O
DE O
HAUTES O
ETUDES O
INTERNATIONALES O
D‚ÄôINFORMATIQUE O
DOCUMENTAIRE O
. O
Elizabeth O
Clark O
, O
Asli O
Celikyilmaz O
, O
and O
Noah O
A O
Smith O
. O
2019 O
. O
Sentence O
mover O
‚Äôs O
similarity O
: O
Automatic O
evaluation O
for O
multi O
- O
sentence O
texts O
. O
In O
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
2748‚Äì2760 O
. O
Aswarth O
Abhilash O
Dara O
and O
Yiu O
- O
Chang O
Lin O
. O
2016 O
. O
Yoda O
system O
for O
wmt16 O
shared O
task O
: O
Bilingual O
document O
alignment O
. O
In O
Proceedings O
of O
the O
First O
Conference O
on O
Machine O
Translation O
: O
Volume O
2 O
, O
Shared O
Task O
Papers O
, O
pages O
679‚Äì684 O
. O
Thi O
- O
Ngoc O
- O
Diep O
Do O
, O
Viet O
- O
Bac O
Le O
, O
Brigitte O
Bigi O
, O
Laurent O
Besacier O
, O
and O
Eric O
Castelli O
. O
2009 O
. O
Mining O
a O
comparable O
text O
corpus O
for O
a O
vietnamese O
- O
french O
statistical O
machine O
translation O
system O
. O
In O
Proceedings O
of O
the O
Fourth O
Workshop O
on O
Statistical O
Machine O
Translation O
, O
pages O
165‚Äì172 O
. O
Association O
for O
Computational O
Linguistics O
. O
Ahmed O
El O
- O
Kishky O
, O
Vishrav O
Chaudhary O
, O
Francisco O
Guzman O
, O
and O
Philipp O
Koehn O
. O
2019 O
. O
A O
massive O
collection O
of O
cross O
- O
lingual O
web O
- O
document O
pairs O
. O
arXiv O
preprint O
arXiv:1911.06154 O
. O
Miquel O
Espl O
` O
a O
- O
Gomis O
, O
Mikel O
Forcada O
, O
Sergio O
Ortiz O
Rojas O
, O
and O
Jorge O
Ferr O
¬¥ O
andez O
- O
Tordera O
. O
2016 O
. O
Bitextor O
‚Äôs O
participation O
in O
wmt‚Äô16 O
: O
shared O
task O
on O
document O
alignment O
. O
In O
Proceedings O
of O
the O
First O
Conference O
on O
Machine O
Translation O
: O
Volume O
2 O
, O
Shared O
Task O
Papers O
, O
pages O
685‚Äì691 O
. O
Pascale O
Fung O
and O
Lo O
Yuen O
Yee O
. O
1998 O
. O
An O
ir O
approach O
for O
translating O
new O
words O
from O
nonparallel O
, O
comparable O
texts O
. O
In O
COLING O
1998 O
Volume O
1 O
: O
The O
17th O
International O
Conference O
on O
Computational O
Linguistics O
. O
Lu¬¥ƒ±s O
Gomes O
and O
Gabriel O
Pereira O
Lopes O
. O
2016 O
. O
First O
steps O
towards O
coverage O
- O
based O
document O
alignment O
. O
InProceedings O
of O
the O
First O
Conference O
on O
Machine O
Translation O
: O
Volume O
2 O
, O
Shared O
Task O
Papers O
, O
pages O
697‚Äì702 O
. O
Mandy O
Guo O
, O
Yinfei O
Yang O
, O
Keith O
Stevens O
, O
Daniel O
Cer O
, O
Heming O
Ge O
, O
Yun O
- O
hsuan O
Sung O
, O
Brian O
Strope O
, O
and O
Ray O
Kurzweil O
. O
2019 O
. O
Hierarchical O
document O
encoder O
for O
parallel O
corpus O
mining O
. O
In O
Proceedings O
of O
the O
Fourth O
Conference O
on O
Machine O
Translation O
, O
pages O
64‚Äì72 O
, O
Florence O
, O
Italy O
. O
Association O
for O
Computational O
Linguistics O
. O
Gao O
Huang O
, O
Chuan O
Guo O
, O
Matt O
J O
Kusner O
, O
Yu O
Sun O
, O
Fei O
Sha O
, O
and O
Kilian O
Q O
Weinberger O
. O
2016 O
. O
Supervised O
word O
mover O
‚Äôs O
distance O
. O
In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
, O
pages O
4862‚Äì4870 O
. O
Laurent O
Jakubina O
and O
Phillippe O
Langlais O
. O
2016 O
. O
Bad O
luc@ O
wmt O
2016 O
: O
a O
bilingual O
document O
alignment O
platform O
based O
on O
lucene O
. O
In O
Proceedings O
of O
the O
First O
Conference O
on O
Machine O
Translation O
: O
Volume O
2 O
, O
Shared O
Task O
Papers O
, O
pages O
703‚Äì709 O
. O
Maurice O
G O
Kendall O
. O
1938 O
. O
A O
new O
measure O
of O
rank O
correlation O
. O
Biometrika O
, O
30(1/2):81‚Äì93 O
. O
Philipp O
Koehn O
. O
2005 O
. O
Europarl O
: O
A O
parallel O
corpus O
for O
statistical O
machine O
translation O
. O
In O
MT O
summit O
, O
volume O
5 O
, O
pages O
79‚Äì86 O
. O
Philipp O
Koehn O
et O
al O
. O
2002 O
. O
Europarl O
: O
A O
multilingual O
corpus O
for O
evaluation O
of O
machine O
translation O
. O
Matt O
Kusner O
, O
Yu O
Sun O
, O
Nicholas O
Kolkin O
, O
and O
Kilian O
Weinberger O
. O
2015 O
. O
From O
word O
embeddings O
to O
document O
distances O
. O
In O
International O
conference O
on O
machine O
learning O
, O
pages O
957‚Äì966 O
. O
Guillaume O
Lample O
and O
Alexis O
Conneau O
. O
2019 O
. O
Crosslingual O
language O
model O
pretraining O
. O
arXiv O
preprint O
arXiv:1901.07291 O
.624Marek O
Medve O
Àád O
, O
Milo O
Àás O
Jakub O
¬¥ O
ƒ±cek O
, O
and O
V O
ojtech O
Kov O
¬¥ O
ar O
. O
2016 O
. O
English O
- O
french O
document O
alignment O
based O
on O
keywords O
and O
statistical O
translation O
. O
In O
Proceedings O
of O
the O
First O
Conference O
on O
Machine O
Translation O
: O
Volume O
2 O
, O
Shared O
Task O
Papers O
, O
pages O
728‚Äì732 O
. O
James O
Munkres O
. O
1957 O
. O
Algorithms O
for O
the O
assignment O
and O
transportation O
problems O
. O
Journal O
of O
the O
society O
for O
industrial O
and O
applied O
mathematics O
, O
5(1):32‚Äì38 O
. O
Dragos O
Stefan O
Munteanu O
and O
Daniel O
Marcu O
. O
2005 O
. O
Improving O
machine O
translation O
performance O
by O
exploiting O
non O
- O
parallel O
corpora O
. O
Computational O
Linguistics O
, O
31(4):477‚Äì504 O
. O
Dragos O
Stefan O
Munteanu O
and O
Daniel O
Marcu O
. O
2006 O
. O
Extracting O
parallel O
sub O
- O
sentential O
fragments O
from O
nonparallel O
corpora O
. O
In O
Proceedings O
of O
the O
21st O
International O
Conference O
on O
Computational O
Linguistics O
and O
the O
44th O
annual O
meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
81‚Äì88 O
. O
Association O
for O
Computational O
Linguistics O
. O
Vassilis O
Papavassiliou O
, O
Prokopis O
Prokopidis O
, O
and O
Stelios O
Piperidis O
. O
2016 O
. O
The O
ilsp O
/ O
arc O
submission O
to O
the O
wmt O
2016 O
bilingual O
document O
alignment O
shared O
task O
. O
InProceedings O
of O
the O
First O
Conference O
on O
Machine O
Translation O
: O
Volume O
2 O
, O
Shared O
Task O
Papers O
, O
pages O
733‚Äì739 O
. O
Alexandre O
Rafalovitch O
, O
Robert O
Dale O
, O
et O
al O
. O
2009 O
. O
United O
nations O
general O
assembly O
resolutions O
: O
A O
sixlanguage O
parallel O
corpus O
. O
In O
Proceedings O
of O
Machine O
Translation O
Summit O
XII O
. O
Juan O
Ramos O
et O
al O
. O
2003 O
. O
Using O
tf O
- O
idf O
to O
determine O
word O
relevance O
in O
document O
queries O
. O
In O
Proceedings O
of O
the O
Ô¨Årst O
instructional O
conference O
on O
machine O
learning O
, O
volume O
242 O
, O
pages O
133‚Äì142 O
. O
Piscataway O
, O
NJ O
. O
Reinhard O
Rapp O
. O
1999 O
. O
Automatic O
identiÔ¨Åcation O
of O
word O
translations O
from O
unrelated O
english O
and O
german O
corpora O
. O
In O
Proceedings O
of O
the O
37th O
annual O
meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
onComputational O
Linguistics O
, O
pages O
519‚Äì526 O
. O
Association O
for O
Computational O
Linguistics O
. O
Philip O
Resnik O
. O
1999 O
. O
Mining O
the O
web O
for O
bilingual O
text O
. O
InProceedings O
of O
the O
37th O
annual O
meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
on O
Computational O
Linguistics O
, O
pages O
527‚Äì534 O
. O
Association O
for O
Computational O
Linguistics O
. O
Philip O
Resnik O
and O
Noah O
A O
Smith O
. O
2003 O
. O
The O
web O
as O
a O
parallel O
corpus O
. O
Computational O
Linguistics O
, O
29(3):349‚Äì380 O
. O
Stephen O
Robertson O
. O
2004 O
. O
Understanding O
inverse O
document O
frequency O
: O
on O
theoretical O
arguments O
for O
idf O
. O
Journal O
of O
documentation O
, O
60(5):503‚Äì520 O
. O
Yossi O
Rubner O
, O
Carlo O
Tomasi O
, O
and O
Leonidas O
J O
Guibas O
. O
1998 O
. O
A O
metric O
for O
distributions O
with O
applications O
to O
image O
databases O
. O
In O
Sixth O
International O
Conference O
on O
Computer O
Vision O
( O
IEEE O
Cat O
. O
No O
. O
98CH36271 O
) O
, O
pages O
59‚Äì66 O
. O
IEEE O
. O
Vadim O
Shchukin O
, O
Dmitry O
Khristich O
, O
and O
Irina O
Galinskaya O
. O
2016 O
. O
Word O
clustering O
approach O
to O
bilingual O
document O
alignment O
( O
wmt O
2016 O
shared O
task O
) O
. O
InProceedings O
of O
the O
First O
Conference O
on O
Machine O
Translation O
: O
Volume O
2 O
, O
Shared O
Task O
Papers O
, O
pages O
740‚Äì744 O
. O
Raghavendra O
Udupa O
, O
K O
Saravanan O
, O
A O
Kumaran O
, O
and O
Jagadeesh O
Jagarlamudi O
. O
2009 O
. O
Mint O
: O
A O
method O
for O
effective O
and O
scalable O
mining O
of O
named O
entity O
transliterations O
from O
large O
comparable O
corpora O
. O
In O
Proceedings O
of O
the O
12th O
Conference O
of O
the O
European O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
799‚Äì807 O
. O
Association O
for O
Computational O
Linguistics O
. O
Micha≈Ç O
Ziemski O
, O
Marcin O
Junczys O
- O
Dowmunt O
, O
and O
Bruno O
Pouliquen O
. O
2016 O
. O
The O
United O
Nations O
parallel O
corpus O
v1 O
. O
0 O
. O
In O
Proceedings O
of O
the O
Tenth O
International O
Conference O
on O
Language O
Resources O
and O
Evaluation O
( O
LREC O
2016 O
) O
, O
pages O
3530‚Äì3534.625Proceedings O
of O
the O
1st O
Conference O
of O
the O
Asia O
- O
PaciÔ¨Åc O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
10th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
626‚Äì636 O
December O
4 O
- O
7 O
, O
2020 O
. O
¬© O
2020 O
Association O
for O
Computational O
Linguistics O
Improving O
Context O
Modeling O
in O
Neural O
Topic O
Segmentation O
Linzi O
Xing‚Ä† O
, O
Brad O
Hackinen‚Ä° O
, O
Giuseppe O
Carenini‚Ä† O
, O
Francesco O
Trebbi¬ß O
‚Ä†University O
of O
British O
Columbia O
, O
Vancouver O
, O
Canada O
‚Ä°Ivey O
Business O
School O
, O
London O
, O
Canada O
¬ß O
University O
of O
California O
Berkeley O
, O
California O
, O
USA O
{ O
lzxing O
, O
carenini O
} O
@cs.ubc.ca O
bhackinen@ivey.ca O
ftrebbi@berkeley.edu O
Abstract O
Topic O
segmentation O
is O
critical O
in O
key O
NLP O
tasks O
and O
recent O
works O
favor O
highly O
effective O
neural O
supervised O
approaches O
. O
However O
, O
current O
neural O
solutions O
are O
arguably O
limited O
in O
how O
they O
model O
context O
. O
In O
this O
paper O
, O
we O
enhance O
a O
segmenter O
based O
on O
a O
hierarchical O
attention O
BiLSTM O
network O
to O
better O
model O
context O
, O
by O
adding O
a O
coherence O
- O
related O
auxiliary O
task O
and O
restricted O
self O
- O
attention O
. O
Our O
optimized O
segmenter1outperforms O
SOTA O
approaches O
when O
trained O
and O
tested O
on O
three O
datasets O
. O
We O
also O
the O
robustness O
of O
our O
proposed O
model O
in O
domain O
transfer O
setting O
by O
training O
a O
model O
on O
a O
large O
- O
scale O
dataset O
and O
testing O
it O
on O
four O
challenging O
real O
- O
world O
benchmarks O
. O
Furthermore O
, O
we O
apply O
our O
proposed O
strategy O
to O
two O
other O
languages O
( O
German O
and O
Chinese O
) O
, O
and O
show O
its O
effectiveness O
in O
multilingual O
scenarios O
. O
1 O
Introduction O
Topic O
segmentation O
is O
a O
fundamental O
NLP O
task O
that O
has O
received O
considerable O
attention O
in O
recent O
years O
( O
Barrow O
et O
al O
. O
, O
2020 O
; O
Glavas O
and O
Somasundaran O
, O
2020 O
; O
Lukasik O
et O
al O
. O
, O
2020 O
) O
. O
It O
can O
reveal O
important O
aspects O
of O
a O
document O
semantic O
structure O
by O
splitting O
the O
document O
into O
topical O
- O
coherent O
textual O
units O
. O
Taking O
the O
Wikipedia O
article O
in O
Table O
1 O
as O
an O
example O
, O
without O
the O
section O
marks O
, O
a O
reliable O
topic O
segmenter O
should O
be O
able O
to O
detect O
the O
correct O
boundaries O
within O
the O
text O
and O
chunk O
this O
article O
into O
the O
topical O
- O
coherent O
units O
T1,T2and O
T3 O
. O
The O
results O
of O
topic O
segmentation O
can O
further O
beneÔ¨Åt O
other O
key O
downstream O
NLP O
tasks O
such O
as O
document O
summarization O
( O
Mitra O
et O
al O
. O
, O
1997 O
; O
Riedl O
and O
Biemann O
, O
2012a O
; O
Xiao O
and O
Carenini O
, O
2019 O
) O
, O
question O
answering O
( O
Oh O
et O
al O
. O
, O
2007 O
; O
Diefenbach O
et O
al O
. O
, O
2018 O
) O
, O
machine O
reading O
( O
van O
Dijk O
, O
1981 O
; O
1Our O
code O
will O
be O
publicly O
available O
at O
www.cs O
. O
ubc.ca/cs-research/lci/research-groups/ O
natural O
- O
language O
- O
processing O
/ O
Preface O
: O
Marcus O
is O
a O
city O
in O
Cherokee O
County O
, O
Iowa O
, O
United O
States O
. O
[ O
T1 O
] O
History O
: O
S1 O
: O
The O
Ô¨Årst O
building O
in O
Marcus O
was O
erected O
in O
1871 O
. O
S2 O
: O
Marcus O
was O
incorporated O
on O
May O
15 O
, O
1882 O
. O
[ O
T2 O
] O
Geography O
: O
S3 O
: O
Marcus O
is O
located O
at O
( O
42.822892 O
, O
-95.804894 O
) O
. O
S4 O
: O
According O
to O
the O
United O
States O
Census O
Bureau O
, O
the O
city O
has O
a O
total O
area O
of O
1.54 O
square O
miles O
, O
all O
land O
. O
[ O
T3 O
] O
Demographics O
: O
S5 O
: O
As O
of O
the O
census O
of O
2010 O
, O
there O
were O
1,117 O
people O
, O
494 O
households O
, O
and O
310 O
families O
residing O
in O
the O
city O
. O
... O
... O
Table O
1 O
: O
A O
Wikipedia O
sample O
article O
about O
City O
Marcus O
covering O
three O
topics O
: O
T1,T2andT3 O
Saha O
et O
al O
. O
, O
2019 O
) O
and O
dialogue O
modeling O
( O
Xu O
et O
al O
. O
, O
2020 O
; O
Zhang O
et O
al O
. O
, O
2020 O
) O
. O
A O
wide O
variety O
of O
techniques O
have O
been O
proposed O
for O
topic O
segmentation O
. O
Early O
unsupervised O
models O
exploit O
word O
statistic O
overlaps O
( O
Hearst O
, O
1997 O
; O
Galley O
et O
al O
. O
, O
2003 O
) O
, O
Bayesian O
contexts O
( O
Eisenstein O
and O
Barzilay O
, O
2008 O
) O
or O
semantic O
relatedness O
graphs O
( O
Glava O
Àás O
et O
al O
. O
, O
2016 O
) O
to O
measure O
the O
lexical O
or O
semantic O
cohesion O
between O
the O
sentences O
or O
paragraphs O
and O
infer O
the O
segment O
boundaries O
from O
them O
. O
More O
recently O
, O
several O
works O
have O
framed O
topic O
segmentation O
as O
neural O
supervised O
learning O
, O
because O
of O
the O
remarkable O
success O
achieved O
by O
such O
models O
in O
most O
NLP O
tasks O
( O
Wang O
et O
al O
. O
, O
2016 O
, O
2017 O
; O
Sehikh O
et O
al O
. O
, O
2017 O
; O
Koshorek O
et O
al O
. O
, O
2018 O
; O
Arnold O
et O
al O
. O
, O
2019 O
) O
. O
Despite O
minor O
architectural O
differences O
, O
most O
of O
these O
neural O
solutions O
adopt O
Recurrent O
Neural O
Network O
( O
Schuster O
and O
Paliwal O
, O
1997 O
) O
and O
its O
variants O
( O
RNNs O
) O
as O
their O
main O
framework O
. O
On O
the O
one O
hand O
, O
RNNs O
are O
appropriate O
because O
topic O
segmentation O
can O
be O
modelled O
as O
a O
sequence O
labeling O
task O
where O
each O
sentence O
is O
either O
the O
end O
of O
a O
segment O
or O
not O
. O
On O
the O
other O
hand O
, O
this O
choice O
makes O
these O
neural O
models O
limited O
in O
how O
to O
model O
the O
context O
. O
Because O
some O
sophisticated O
RNNs O
( O
eg O
. O
,626LSTM O
, O
GRU O
) O
are O
able O
to O
preserve O
long O
- O
distance O
information O
( O
Lipton O
et O
al O
. O
, O
2015 O
; O
Sehikh O
et O
al O
. O
, O
2017 O
; O
Wang O
et O
al O
. O
, O
2018 O
) O
, O
which O
can O
largely O
help O
language O
models O
. O
But O
for O
topic O
segmentation O
, O
it O
is O
critical O
to O
supervise O
the O
model O
to O
focus O
more O
on O
the O
local O
context O
. O
As O
illustrated O
in O
Table O
1 O
, O
the O
prediction O
of O
the O
segment O
boundary O
between O
T1andT2hardly O
depends O
on O
the O
content O
in O
T3 O
. O
Bringing O
in O
excessive O
long O
- O
distance O
signals O
may O
cause O
unnecessary O
noise O
and O
hurt O
performance O
. O
Moreover O
, O
text O
coherence O
has O
strong O
relation O
with O
topic O
segmentation O
( O
Wang O
et O
al O
. O
, O
2017 O
; O
Glavas O
and O
Somasundaran O
, O
2020 O
) O
. O
For O
instance O
, O
in O
Table O
1 O
, O
sentence O
pairs O
from O
the O
same O
segment O
( O
like O
< O
S1,S2 O
> O
or O
< O
S3,S4 O
> O
) O
are O
more O
coherent O
than O
sentence O
pairs O
across O
segments O
( O
like O
S2andS3 O
) O
. O
Arguably O
, O
with O
a O
proper O
way O
of O
modeling O
the O
coherence O
between O
adjacent O
sentences O
, O
a O
topic O
segmenter O
can O
be O
further O
enhanced O
. O
In O
this O
paper O
, O
we O
propose O
to O
enhance O
a O
state O
- O
ofthe O
- O
art O
( O
SOTA O
) O
topic O
segmenter O
( O
Koshorek O
et O
al O
. O
, O
2018 O
) O
based O
on O
hierarchical O
attention O
BiLSTM O
network O
to O
better O
model O
the O
local O
context O
of O
a O
sentence O
in O
two O
complementary O
ways O
. O
First O
, O
we O
add O
a O
coherence O
- O
related O
auxiliary O
task O
to O
make O
our O
model O
learn O
more O
informative O
hidden O
states O
for O
all O
the O
sentences O
in O
a O
document O
. O
More O
speciÔ¨Åcally O
, O
we O
reÔ¨Åne O
the O
objective O
of O
our O
model O
to O
encourage O
smaller O
coherence O
for O
the O
sentences O
from O
different O
segments O
and O
larger O
coherence O
for O
the O
sentences O
from O
the O
same O
segment O
. O
Secondly O
, O
we O
enhance O
context O
modeling O
by O
utilizing O
restricted O
self O
- O
attention O
( O
Wang O
et O
al O
. O
, O
2018 O
) O
, O
which O
enables O
our O
model O
to O
pay O
attention O
to O
the O
local O
context O
and O
make O
better O
use O
of O
the O
information O
from O
the O
closer O
neighbors O
of O
each O
sentence O
( O
i.e. O
, O
with O
respect O
to O
a O
window O
of O
explicitly O
Ô¨Åxed O
size O
k O
) O
. O
Our O
empirical O
results O
show O
( O
1 O
) O
that O
our O
proposed O
context O
modeling O
strategy O
signiÔ¨Åcantly O
improves O
the O
performance O
of O
the O
SOTA O
neural O
segmenter O
on O
three O
datasets O
, O
( O
2 O
) O
that O
the O
enhanced O
segmenter O
is O
more O
robust O
in O
domain O
transfer O
setting O
when O
applied O
to O
four O
challenging O
real O
- O
world O
test O
sets O
, O
sampled O
differently O
from O
the O
training O
data O
, O
( O
3 O
) O
that O
our O
context O
modeling O
strategy O
is O
also O
effective O
for O
the O
segmenters O
trained O
on O
other O
challenging O
languages O
( O
eg O
. O
, O
German O
and O
Chinese O
) O
, O
rather O
than O
just O
English O
. O
2 O
Related O
Work O
Topic O
Segmentation O
Early O
unsupervised O
models O
exploit O
the O
lexical O
overlaps O
of O
sentences O
tomeasure O
the O
lexical O
cohesion O
between O
sentences O
or O
paragraphs O
( O
Hearst O
, O
1997 O
; O
Galley O
et O
al O
. O
, O
2003 O
; O
Eisenstein O
and O
Barzilay O
, O
2008 O
; O
Riedl O
and O
Biemann O
, O
2012b O
) O
. O
Then O
, O
by O
moving O
two O
sliding O
windows O
over O
the O
text O
, O
the O
cohesion O
between O
successive O
text O
units O
could O
be O
measured O
and O
a O
cohesion O
drop O
would O
signal O
a O
segment O
boundary O
. O
Even O
if O
these O
models O
do O
not O
require O
any O
training O
data O
, O
they O
only O
show O
limited O
performance O
in O
practice O
and O
are O
not O
general O
enough O
to O
handle O
the O
temporal O
change O
of O
the O
languages O
( O
Huang O
and O
Paul O
, O
2019 O
) O
. O
More O
recently O
, O
neural O
- O
based O
supervised O
methods O
have O
been O
devised O
for O
topic O
segmentation O
because O
of O
their O
more O
accurate O
predictions O
and O
greater O
efÔ¨Åciency O
. O
One O
line O
of O
research O
frames O
topic O
segmentation O
as O
a O
sequence O
labeling O
problem O
and O
builds O
neural O
models O
to O
predict O
segment O
boundaries O
directly O
. O
Wang O
et O
al O
. O
( O
2016 O
) O
proposed O
a O
simple O
BiLSTM O
model O
to O
label O
if O
a O
sentence O
is O
a O
segment O
boundary O
or O
not O
. O
They O
demonstrated O
that O
along O
with O
engineered O
features O
based O
on O
cue O
phrases O
( O
eg O
. O
, O
‚Äò O
Ô¨Årst O
of O
all O
‚Äô O
, O
‚Äò O
second O
‚Äô O
) O
, O
their O
model O
can O
achieve O
marginally O
better O
performance O
than O
early O
unsupervised O
methods O
. O
Later O
, O
Koshorek O
et O
al O
. O
( O
2018 O
) O
proposed O
a O
hierarchical O
neural O
sequence O
labeling O
model O
for O
topic O
segmentation O
and O
showed O
its O
superiority O
compared O
with O
their O
selected O
supervised O
and O
unsupervised O
baselines O
. O
Around O
the O
same O
time O
, O
Badjatiya O
et O
al O
. O
( O
2018 O
) O
proposed O
an O
attention O
- O
based O
BiLSTM O
model O
to O
classify O
whether O
a O
sentence O
was O
a O
segment O
boundary O
or O
not O
, O
by O
considering O
the O
context O
around O
it O
. O
The O
work O
we O
present O
in O
this O
paper O
can O
be O
seen O
as O
pushing O
this O
line O
of O
research O
even O
further O
by O
encouraging O
the O
model O
to O
more O
explicitly O
consider O
contextual O
coherence O
, O
as O
well O
as O
to O
prefer O
more O
information O
from O
the O
neighbor O
context O
through O
restricted O
self O
- O
attention O
. O
Another O
rather O
different O
line O
of O
works O
Ô¨Årst O
trains O
neural O
models O
for O
other O
tasks O
, O
and O
then O
uses O
these O
models O
‚Äô O
outputs O
to O
predict O
boundaries O
. O
Wang O
et O
al O
. O
( O
2017 O
) O
trained O
a O
Convolutional O
Neural O
Network O
( O
CNN O
) O
network O
to O
predict O
the O
coherence O
scores O
for O
text O
pairs O
. O
Sentences O
in O
a O
pair O
with O
large O
cohesion O
are O
supposed O
to O
belong O
to O
the O
same O
segment O
. O
However O
, O
their O
‚Äú O
learning O
to O
rank O
‚Äù O
framework O
asks O
for O
the O
pre O
- O
deÔ¨Åned O
number O
of O
segments O
, O
which O
limits O
their O
model O
‚Äôs O
applicability O
in O
practice O
. O
Our O
selected O
framework O
overcomes O
this O
constraint O
by O
tuning O
a O
conÔ¨Ådence O
threshold O
during O
the O
training O
stage O
. O
A O
sentence O
with O
the O
output O
probability O
above O
this O
threshold O
will O
be O
predicted O
as O
the O
end O
of O
a O
seg-627ment O
. O
Following O
a O
very O
different O
approach O
, O
Arnold O
et O
al O
. O
( O
2019 O
) O
introduced O
a O
topic O
embedding O
layer O
into O
a O
BiLSTM O
model O
. O
After O
training O
their O
model O
to O
predict O
the O
sentence O
topics O
, O
the O
learned O
topic O
embeddings O
can O
be O
utilized O
for O
topic O
segmentation O
. O
However O
, O
one O
critical O
Ô¨Çaw O
of O
their O
method O
is O
that O
it O
requires O
a O
complicated O
pre O
- O
processing O
pipeline O
, O
which O
includes O
topic O
extraction O
and O
synset O
clustering O
, O
whose O
errors O
can O
propagate O
to O
the O
main O
topic O
segmentation O
task O
. O
In O
contrast O
, O
our O
proposal O
only O
requires O
the O
plain O
content O
of O
the O
training O
data O
without O
any O
complex O
pre O
- O
processing O
. O
Coherence O
Modeling O
Early O
works O
on O
coherence O
modeling O
merely O
predict O
the O
coherence O
score O
for O
documents O
by O
tracking O
the O
patterns O
of O
entities O
‚Äô O
grammatical O
role O
transition O
( O
Barzilay O
and O
Lapata O
, O
2005 O
, O
2008 O
) O
. O
More O
recently O
, O
researchers O
started O
modeling O
the O
coherence O
for O
sentence O
pairs O
by O
their O
semantic O
similarities O
and O
used O
them O
for O
higher O
level O
coherence O
prediction O
or O
even O
other O
tasks O
, O
including O
topic O
segmentation O
. O
Wang O
et O
al O
. O
( O
2017 O
) O
demonstrated O
the O
strong O
relation O
between O
text O
- O
pair O
coherence O
modeling O
and O
topic O
segmentation O
. O
They O
assumed O
that O
( O
1 O
) O
a O
pair O
of O
texts O
from O
the O
same O
document O
should O
be O
ranked O
more O
coherent O
than O
a O
pair O
of O
texts O
from O
different O
documents O
; O
( O
2 O
) O
a O
pair O
of O
texts O
from O
the O
same O
segment O
should O
be O
ranked O
more O
coherent O
than O
a O
pair O
of O
texts O
from O
different O
segments O
of O
a O
document O
. O
With O
these O
assumptions O
, O
they O
created O
a O
‚Äú O
quasi O
‚Äù O
training O
corpus O
for O
text O
- O
pair O
coherence O
prediction O
by O
assigning O
different O
coherence O
scores O
to O
the O
texts O
from O
the O
same O
segment O
, O
different O
segments O
but O
the O
same O
document O
, O
and O
different O
documents O
. O
Then O
they O
proposed O
the O
corresponding O
model O
, O
and O
further O
use O
this O
model O
to O
directly O
conduct O
topic O
segmentation O
. O
Following O
their O
second O
assumption O
, O
we O
propose O
a O
neural O
solution O
in O
which O
by O
injecting O
a O
coherence O
- O
related O
auxiliary O
task O
, O
topic O
segmentation O
and O
sentence O
level O
coherence O
modeling O
can O
mutually O
beneÔ¨Åt O
each O
other O
. O
3 O
Neural O
Topic O
Segmentation O
Model O
Since O
RNN O
- O
based O
topic O
segmenters O
have O
shown O
success O
with O
high O
- O
quality O
training O
data O
, O
we O
adopt O
a O
state O
- O
of O
- O
the O
- O
art O
RNN O
- O
based O
topic O
segmenter O
enhanced O
with O
attention O
and O
BERT O
embeddings O
as O
our O
basic O
model O
. O
Then O
, O
we O
extend O
such O
model O
to O
make O
better O
use O
of O
the O
local O
context O
, O
something O
that O
can O
not O
be O
done O
effectively O
within O
the O
RNN O
framework O
( O
Wang O
et O
al O
. O
, O
2018 O
) O
. O
In O
particular O
, O
we O
add O
a O
coherence O
- O
related O
auxiliary O
task O
and O
a O
restricted O
Figure O
1 O
: O
The O
architecture O
of O
our O
basic O
model O
. O
seiis O
the O
produced O
sentence O
embedding O
for O
sentence O
Si O
. O
self O
- O
attention O
mechanisms O
to O
the O
basic O
model O
, O
so O
that O
predictions O
are O
more O
strongly O
inÔ¨Çuenced O
by O
the O
coherence O
between O
the O
nearby O
sentences O
. O
As O
a O
preview O
of O
this O
section O
, O
we O
Ô¨Årst O
deÔ¨Åne O
the O
problem O
of O
topic O
segmentation O
and O
introduce O
the O
basic O
model O
. O
In O
the O
next O
section O
, O
we O
motivate O
and O
describe O
our O
proposed O
extensions O
. O
3.1 O
Problem O
DeÔ¨Ånition O
Topic O
segmentation O
is O
usually O
framed O
as O
a O
sequence O
labeling O
task O
. O
More O
precisely O
, O
given O
a O
document O
represented O
as O
a O
sequence O
of O
sentences O
, O
our O
model O
will O
predict O
the O
binary O
label O
for O
each O
sentence O
to O
indicate O
if O
the O
sentence O
is O
the O
end O
of O
a O
topical O
coherent O
segment O
or O
not O
. O
Formally O
, O
Given O
: O
A O
document O
din O
the O
form O
of O
a O
sequence O
of O
sentences{s1,s2,s3, O
... O
,s O
k O
} O
. O
Predict O
: O
A O
sequence O
of O
labels O
assigned O
to O
a O
sequence O
of O
sentences O
{ O
l1,l2,l3, O
... O
,l O
k‚àí1 O
} O
, O
wherelis O
a O
binary O
label O
, O
1means O
the O
corresponding O
sentence O
is O
the O
end O
of O
a O
segment O
, O
0means O
the O
corresponding O
sentence O
is O
not O
the O
end O
of O
a O
segment O
. O
We O
do O
not O
predict O
the O
label O
for O
the O
last O
sentence O
sk O
, O
since O
it O
is O
always O
the O
end O
of O
the O
last O
segment O
. O
3.2 O
Basic O
Model O
: O
Enhanced O
Hierarchical O
Attention O
Bi O
- O
LSTM O
Network O
( O
HAN O
) O
Figure O
1 O
illustrates O
the O
detailed O
architecture O
of O
our O
basic O
model O
comprising O
the O
two O
steps O
of O
sentence O
encoding O
and O
label O
prediction O
. O
Formally O
, O
a O
sentence O
encoding O
network O
returns O
sentence O
embeddings O
from O
pre O
- O
trained O
word O
embeddings O
. O
Then O
a O
label O
prediction O
network O
processes O
the O
sentence O
embeddings O
generated O
earlier O
and O
outputs O
the O
probabilities O
to O
indicate O
if O
sentences O
are O
the O
segment628boundaries O
or O
not O
. O
Finally O
, O
to O
convert O
the O
numerical O
probabilities O
into O
binary O
labels O
, O
we O
follow O
the O
greedy O
decoding O
strategy O
in O
Koshorek O
et O
al O
. O
( O
2018 O
) O
by O
setting O
a O
threshold O
œÑ O
. O
All O
the O
sentences O
with O
their O
probabilities O
over O
œÑwill O
be O
labeled O
1 O
, O
and O
0 O
otherwise O
. O
This O
parameter O
œÑis O
set O
in O
the O
validation O
stage O
. O
For O
training O
, O
we O
compute O
the O
cross O
- O
entropy O
loss O
between O
the O
ground O
truth O
labels O
Y= O
{ O
y1, O
... O
,y O
k‚àí1}and O
our O
predicted O
probabilities O
P= O
{ O
p1, O
... O
,p O
k‚àí1}for O
a O
document O
with O
ksentences O
: O
L1=‚àík‚àí1 O
/ O
summationdisplay O
i=1[yilogpi+ O
( O
1‚àíyi O
) O
log(1‚àípi)](1 O
) O
Looking O
at O
the O
details O
of O
the O
architecture O
in O
Figure O
1 O
, O
our O
basic O
model O
constitutes O
a O
strong O
baseline O
by O
extending O
the O
segmenter O
presented O
in O
Koshorek O
et O
al O
. O
( O
2018 O
) O
in O
two O
ways O
( O
colored O
parts O
) O
; O
namely O
, O
by O
improving O
the O
sentence O
encoder O
with O
an O
attention O
mechanism O
( O
orange O
) O
and O
with O
BERT O
embeddings O
( O
blue O
) O
. O
Enhancing O
Task O
- O
SpeciÔ¨Åc O
Sentence O
Representations O
- O
While O
Koshorek O
et O
al O
. O
( O
2018 O
) O
applied O
maxpooling O
to O
build O
sentence O
embeddings O
from O
sentence O
encoding O
network O
, O
we O
applied O
an O
attention O
mechanism O
( O
Yang O
et O
al O
. O
, O
2016 O
) O
to O
make O
the O
model O
better O
capture O
task O
- O
wise O
sentence O
semantics O
. O
The O
beneÔ¨Åt O
of O
this O
enhancement O
is O
veriÔ¨Åed O
empirically O
by O
the O
results O
in O
Table O
2 O
. O
As O
it O
can O
be O
seen O
, O
replacing O
the O
max O
- O
pooling O
with O
the O
attention O
based O
BiLSTM O
sentence O
encoder O
yields O
better O
performance O
. O
Enhancing O
Generality O
with O
BERT O
Embeddings O
In O
order O
to O
better O
deal O
with O
unseen O
text O
in O
test O
data O
and O
hence O
improve O
the O
model O
‚Äôs O
generality O
, O
we O
utilize O
a O
pre O
- O
trained O
BERT O
sentence O
encoder2 O
which O
complements O
our O
sentence O
encoding O
network O
. O
The O
transformer O
- O
based O
BERT O
model O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
was O
trained O
on O
multi O
- O
billion O
sentences O
publicly O
available O
on O
the O
web O
for O
several O
generic O
sentence O
- O
level O
semantic O
tasks O
, O
such O
as O
Natural O
Language O
Inference O
and O
Question O
Answering O
, O
which O
implies O
that O
it O
can O
arguably O
capture O
more O
general O
aspects O
of O
sentence O
semantics O
in O
a O
reliable O
way O
. O
To O
combine O
task O
- O
speciÔ¨Åc O
information O
with O
generic O
semantic O
signals O
from O
BERT O
, O
we O
simply O
concatenate O
the O
BERT O
sentence O
embeddings O
with O
the O
sentence O
embeddings O
derived O
from O
our O
encoder O
. O
Such O
concatenation O
then O
becomes O
the O
input O
of O
the O
next O
level O
2github.com/hanxiao/bert-as-service O
. O
For O
languages O
other O
than O
English O
, O
we O
use O
their O
corresponding O
pretrained O
BERT O
models O
. O
Dataset O
CHOI O
RULES O
SECTION O
MEAN O
MaxPooling O
1.04 O
7.74 O
12.62 O
7.14 O
BiLSTM O
0.92 O
7.47 O
11.60 O
6.66 O
BERT O
0.93 O
8.35 O
12.08 O
7.12 O
BiLSTM+BERT O
0.81 O
6.90 O
11.30 O
6.34 O
Table O
2 O
: O
Pkerror O
score O
( O
lower O
the O
better O
, O
see O
Section O
4.3 O
for O
details O
) O
of O
different O
sentence O
encoding O
strategies O
on O
three O
datasets O
( O
Section O
4.1 O
) O
. O
To O
Ô¨Åt O
in O
the O
table O
, O
we O
shorten O
Att O
- O
BiLSTM O
to O
BiLSTM O
. O
Results O
in O
bold O
are O
the O
best O
performance O
across O
the O
comparisons O
. O
network O
( O
see O
Figure O
1 O
) O
. O
The O
beneÔ¨Åt O
of O
injecting O
BERT O
embedding O
is O
also O
veriÔ¨Åed O
empirically O
by O
the O
results O
reported O
in O
Table O
2 O
. O
We O
can O
see O
that O
concatenating O
BERT O
embedding O
and O
the O
output O
of O
Att O
- O
BiLSTM O
yields O
the O
best O
performance O
compared O
with O
only O
BERT O
embedding O
or O
the O
output O
of O
Att O
- O
BiLSTM O
. O
3.3 O
Auxiliary O
Task O
Learning O
In O
a O
well O
- O
structured O
document O
, O
the O
semantic O
coherence O
of O
a O
pair O
of O
sentences O
from O
the O
same O
segment O
should O
tend O
to O
be O
greater O
than O
the O
coherence O
of O
a O
pair O
of O
sentences O
from O
different O
segments O
. O
This O
observation O
provides O
us O
with O
an O
alternative O
way O
to O
enable O
better O
context O
modeling O
by O
formulating O
a O
coherence O
- O
related O
auxiliary O
task O
whose O
objective O
can O
be O
jointly O
optimized O
with O
our O
original O
objective O
( O
Equation O
1 O
) O
. O
This O
task O
thereby O
is O
to O
predict O
the O
consecutive O
sentence O
- O
pair O
coherence O
by O
using O
the O
sentence O
hidden O
states O
generated O
from O
the O
BiLSTM O
network O
. O
Concurrently O
minimizing O
the O
loss O
of O
this O
task O
can O
regulate O
our O
model O
to O
learn O
better O
semantic O
coherence O
relation O
between O
sentences O
by O
reducing O
the O
semantic O
coherence O
scores O
for O
the O
sentence O
pairs O
across O
segments O
and O
increasing O
the O
semantic O
coherence O
scores O
for O
the O
sentence O
pairs O
within O
a O
segment O
. O
To O
obtain O
the O
ground O
truth O
for O
our O
introduced O
auxiliary O
task O
( O
sentence O
- O
pair O
coherence O
prediction O
) O
, O
we O
leverage O
the O
ground O
truth O
of O
our O
segmented O
training O
set O
rather O
than O
requiring O
external O
annotations O
. O
For O
a O
document O
which O
contains O
msentences O
, O
there O
arem‚àí1consecutive O
sentence O
pairs O
. O
If O
this O
document O
hasnsegment O
boundaries O
, O
then O
among O
those O
m‚àí1sentence O
pairs O
, O
nsentence O
pairs O
are O
from O
different O
segments O
, O
while O
the O
remaining O
m‚àín‚àí1 O
sentence O
pairs O
are O
from O
the O
same O
segment O
. O
In O
order O
to O
concurrently O
minimize O
the O
coherence O
of O
the O
sentences O
from O
different O
segments O
and O
maximize O
the O
coherence O
of O
the O
sentences O
in O
the O
same O
segment O
, O
we O
give O
a O
sentence O
pair O
< O
si O
, O
si+1 O
> O
a O
coherence O
label629Figure O
2 O
: O
Our O
full O
model O
with O
context O
modeling O
components O
: O
restricted O
self O
- O
attention O
, O
auxiliary O
task O
module O
. O
li= O
1if O
sentences O
in O
this O
pair O
are O
from O
the O
same O
segment O
, O
and O
li= O
0otherwise O
. O
The O
embeddings O
ei O
andei+1of O
adjacent O
sentences O
pairs O
< O
si O
, O
si+1 O
> O
used O
for O
coherence O
computing O
are O
calculated O
from O
BiLSTM O
forward O
and O
backward O
hidden O
states‚àí O
‚Üíh O
and‚Üê O
‚àíh O
, O
following O
the O
equations O
below O
: O
ei= O
tanh(We(‚àí O
‚Üíhi‚àí‚àí‚àí‚Üíhi‚àí1 O
) O
+ O
be O
) O
( O
2 O
) O
ei+1= O
tanh(We(‚Üê‚àí‚àíhi+1‚àí‚Üê‚àí‚àíhi+2 O
) O
+ O
be)(3 O
) O
However O
, O
notice O
that O
instead O
of O
using O
the O
conventional O
[ O
‚àí O
‚Üíhi;‚Üê O
‚àíhi]as O
the O
embedding O
of O
sentence O
i O
, O
here O
, O
similarly O
to O
Wang O
and O
Chang O
( O
2016 O
) O
, O
we O
subtract O
forward O
/ O
backward O
states O
to O
focus O
on O
the O
semantics O
of O
sentences O
in O
the O
current O
sentence O
pair O
. O
The O
semantic O
coherence O
between O
two O
sentence O
embeddings O
is O
then O
computed O
as O
the O
sigmoid O
of O
their O
cosine O
similarity O
: O
Coh O
i O
= O
œÉ(cos(ei O
, O
ei+1 O
) O
) O
( O
4 O
) O
We O
use O
binary O
cross O
- O
entropy O
loss O
to O
formulate O
the O
objective O
of O
our O
auxiliary O
task O
. O
For O
a O
document O
withksentences O
, O
the O
loss O
can O
be O
calculated O
as O
: O
L2=‚àík‚àí1 O
/ O
summationdisplay O
i=1,li=1logCoh O
i‚àík‚àí1 O
/ O
summationdisplay O
i=1,li=0log(1‚àíCoh O
i O
) O
( O
5 O
) O
which O
penalizes O
high O
Coh O
across O
segments O
and O
low O
Coh O
within O
segments O
. O
Combining O
Equation O
1 O
and O
5 O
, O
we O
form O
the O
loss O
function O
of O
our O
new O
segmenter O
as O
: O
Ltotal O
= O
Œ±L1 O
+ O
( O
1‚àíŒ±)L2 O
( O
6)with O
the O
trade O
- O
off O
parameter O
Œ±tuned O
in O
validation O
stage O
, O
topic O
segmentation O
and O
the O
coherence O
- O
related O
auxiliary O
task O
are O
jointly O
optimized O
. O
The O
architecture O
of O
the O
auxiliary O
task O
module O
and O
its O
integration O
in O
our O
segmenter O
is O
shown O
in O
red O
in O
Figure O
2 O
. O
3.4 O
Sentence O
- O
Level O
Restricted O
Self O
- O
Attention O
The O
self O
- O
attention O
mechanism O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
has O
been O
widely O
applied O
to O
many O
sequence O
labeling O
tasks O
due O
to O
its O
superiority O
in O
modeling O
long O
- O
distance O
dependencies O
in O
text O
. O
However O
, O
when O
the O
task O
mainly O
requires O
modelling O
local O
context O
, O
long O
- O
distance O
dependencies O
will O
instead O
introduce O
noise O
. O
Wang O
et O
al O
. O
( O
2018 O
) O
noticed O
this O
problem O
for O
discourse O
segmentation O
, O
where O
the O
crucial O
information O
for O
a O
clause O
- O
like O
Elementary O
Discourse O
Unit O
( O
EDU O
) O
boundary O
prediction O
comes O
usually O
only O
from O
the O
adjacent O
EDUs O
. O
Thus O
, O
they O
proposed O
aword O
- O
level O
restricted O
self O
- O
attention O
mechanism O
by O
adding O
a O
Ô¨Åxed O
size O
window O
constraint O
on O
the O
standard O
self O
- O
attention O
. O
In O
essence O
, O
this O
mechanism O
encourages O
the O
model O
to O
absorb O
more O
information O
directly O
from O
adjacent O
context O
words O
within O
a O
Ô¨Åxed O
range O
of O
neighborhood O
. O
We O
hypothesize O
that O
the O
similar O
restricted O
dependencies O
also O
play O
a O
dominant O
role O
in O
topic O
segmentation O
due O
to O
their O
close O
relation O
. O
Hence O
, O
instead O
of O
at O
word O
- O
level O
, O
we O
add O
asentence O
- O
level O
restricted O
self O
- O
attention O
on O
top O
of O
the O
label O
prediction O
network O
of O
the O
basic O
model O
, O
as O
shown O
in O
green O
in O
Figure O
2 O
. O
In O
particular O
, O
once O
hidden O
states O
are O
obtained O
for O
all O
the O
sentences O
of O
document O
d O
, O
we O
compute O
the630Dataset O
CHOI O
RULES O
SECTION O
WIKI-50 O
CITIES O
ELEMENTS O
CLINICAL O
documents O
920 O
4,461 O
21,376 O
50 O
100 O
118 O
227 O
# O
sent O
/ O
seg O
7.4 O
7.4 O
7.2 O
13.6 O
5.2 O
3.3 O
28.0 O
# O
seg O
/ O
doc O
10.0 O
16.6 O
7.9 O
3.5 O
12.2 O
6.8 O
5.0 O
real O
world O
Table O
3 O
: O
Statistics O
of O
all O
the O
English O
topic O
segmentation O
datasets O
used O
in O
our O
experiments O
. O
Dataset O
EN O
DE O
ZH O
documents O
21,376 O
12,993 O
10,000 O
# O
sent O
/ O
seg O
7.2 O
6.3 O
5.1 O
# O
seg O
/ O
doc O
7.9 O
7.0 O
6.4 O
real O
world O
Table O
4 O
: O
Statistics O
of O
the O
the O
WIKI O
- O
SECTION O
data O
in O
English(EN O
) O
, O
German(DE O
) O
and O
Chinese(ZH O
) O
. O
similarities O
between O
the O
current O
sentence O
iand O
its O
nearby O
sentences O
within O
a O
window O
of O
size O
S. O
For O
example O
, O
the O
similarity O
between O
sentence O
siandsj O
which O
is O
within O
the O
window O
size O
is O
computed O
as O
: O
simi O
, O
j O
= O
Wa[hi;hj O
; O
( O
hi‚äôhj O
) O
] O
+ O
ba O
( O
7 O
) O
wherehi O
, O
hjare O
the O
hidden O
state O
of O
siandsj O
. O
Wa O
andbaare O
both O
attention O
parameters O
. O
; O
is O
the O
concatenation O
operation O
and O
‚äôis O
the O
dot O
product O
operation O
. O
The O
attention O
weights O
for O
all O
the O
sentences O
in O
the O
Ô¨Åxed O
window O
are O
: O
ai O
, O
j O
= O
esimi O
, O
j O
/summationtextS O
s=‚àíSesimi O
, O
i+s(8 O
) O
The O
output O
for O
sentence O
iafter O
the O
restricted O
selfattention O
mechanism O
is O
the O
weighted O
sum O
of O
all O
the O
sentence O
hidden O
states O
within O
the O
window O
: O
ci O
= O
S O
/ O
summationdisplay O
s=‚àíSai O
, O
i+shi+s O
( O
9 O
) O
wherecidenotes O
the O
local O
context O
embedding O
of O
sentenceigenerated O
by O
restricted O
self O
- O
attention O
. O
After O
getting O
the O
local O
context O
embeddings O
for O
all O
the O
sentences O
, O
we O
concatenate O
them O
with O
the O
original O
sentence O
hidden O
states O
and O
input O
them O
to O
another O
BiLSTM O
layer O
( O
top O
of O
Figure O
2 O
) O
. O
4 O
Experimental O
Setup O
In O
order O
to O
comprehensively O
evaluate O
the O
effectiveness O
of O
our O
context O
modeling O
strategy O
of O
adding O
a O
coherence O
- O
related O
auxiliary O
task O
and O
a O
restricted O
self O
- O
attention O
mechanisms O
to O
the O
basic O
model O
, O
we O
conduct O
three O
sets O
of O
experiments O
for O
evaluation:(i O
) O
Intra O
Domain O
: O
we O
train O
and O
test O
the O
models O
in O
the O
same O
domain O
, O
repeating O
this O
evaluation O
for O
three O
different O
domains O
( O
datasets O
) O
. O
( O
ii O
) O
Domain O
Transfer O
: O
we O
train O
the O
models O
on O
a O
large O
dataset O
which O
covers O
a O
variety O
of O
topics O
and O
test O
them O
on O
four O
challenging O
real O
- O
world O
datasets O
. O
( O
iii O
) O
Multilingual O
: O
we O
train O
and O
test O
our O
model O
on O
three O
datasets O
within O
different O
languages O
( O
English O
, O
German O
and O
Chinese O
) O
, O
to O
assess O
our O
proposed O
strategy O
‚Äôs O
generality O
within O
different O
languages O
. O
4.1 O
Datasets O
Data O
for O
Intra O
- O
Domain O
Evaluation O
High O
quality O
training O
dataset O
for O
topic O
segmentation O
usually O
satisÔ¨Åes O
the O
following O
criteria O
: O
( O
1 O
) O
large O
size O
; O
( O
2 O
) O
cover O
a O
variety O
of O
topics O
; O
( O
3 O
) O
contains O
real O
documents O
with O
reliable O
segmentation O
either O
from O
human O
annotations O
or O
already O
speciÔ¨Åed O
in O
the O
documents O
e.g. O
, O
sections O
. O
In O
order O
to O
comprehensively O
evaluate O
the O
effectiveness O
of O
our O
context O
modeling O
strategy O
when O
dealing O
with O
data O
of O
different O
quality O
, O
we O
train O
and O
test O
models O
on O
the O
following O
three O
datasets O
: O
CHOI O
( O
Choi O
, O
2000 O
) O
whose O
articles O
are O
synthesized O
artiÔ¨Åcially O
by O
stitching O
together O
different O
sources O
( O
i.e. O
, O
they O
were O
not O
written O
as O
one O
document O
by O
one O
author O
) O
. O
Hence O
, O
it O
does O
not O
really O
reÔ¨Çect O
naturally O
occurring O
topic O
drifts O
. O
While O
the O
quality O
of O
this O
dataset O
is O
low O
, O
it O
is O
an O
early O
but O
popular O
benchmark O
for O
topic O
segmentation O
evaluation O
. O
We O
include O
this O
dataset O
to O
allow O
comparison O
with O
the O
previous O
work O
. O
RULES O
( O
Bertrand O
et O
al O
. O
, O
2018 O
) O
is O
a O
dataset O
collected O
from O
the O
U.S. O
Federal O
Register O
issues3 O
. O
When O
U.S. O
federal O
agencies O
make O
changes O
to O
regulations O
or O
other O
policies O
, O
they O
must O
publish O
a O
document O
called O
a O
‚Äú O
Rule O
‚Äù O
in O
the O
Federal O
Register O
. O
The O
Rule O
describes O
what O
is O
being O
changed O
and O
discusses O
the O
motivation O
and O
legal O
justiÔ¨Åcation O
for O
the O
action O
. O
Since O
each O
paragraph O
in O
a O
document O
discusses O
one O
topic O
, O
we O
consider O
the O
last O
sentence O
of O
each O
paragraph O
as O
a O
ground O
truth O
topic O
boundary O
. O
The O
discussion O
paragraphs O
usually O
cover O
diverse O
topics O
in O
formal O
, O
3www.govinfo.gov/631technical O
language O
that O
can O
be O
hard O
to O
Ô¨Ånd O
online O
, O
so O
we O
deem O
it O
as O
an O
additional O
well O
- O
labelled O
dataset O
for O
testing O
topic O
segmentation O
to O
complement O
our O
other O
datasets O
which O
contain O
more O
informal O
use O
of O
the O
language O
. O
WIKI O
- O
SECTION O
( O
Arnold O
et O
al O
. O
, O
2019 O
) O
is O
a O
newly O
released O
dataset O
which O
was O
originally O
generated O
from O
the O
most O
recent O
English O
and O
German O
Wikipedia O
dumps O
. O
To O
better O
align O
with O
the O
purpose O
of O
intra O
- O
domain O
experiment O
, O
we O
only O
select O
the O
English O
samples O
for O
training O
and O
the O
German O
samples O
will O
be O
used O
in O
the O
experiments O
of O
multilingual O
evaluation O
. O
The O
English O
WIKI O
- O
SECTION O
( O
labeled O
SECTION O
in O
the O
tables O
) O
consists O
of O
Wikipedia O
articles O
from O
domain O
diseases O
andcities O
. O
We O
deem O
this O
dataset O
as O
the O
most O
reliable O
training O
source O
among O
the O
three O
datasets O
. O
It O
has O
the O
largest O
size O
and O
the O
two O
domains O
( O
cities O
anddiseases O
) O
cover O
news O
- O
based O
samples O
and O
scientiÔ¨Åc O
- O
based O
samples O
respectively O
. O
We O
split O
CHOI O
andRULES O
into O
80 O
% O
training O
, O
10 O
% O
validation O
, O
10 O
% O
testing O
. O
For O
SECTION O
, O
we O
follow O
Arnold O
et O
al O
. O
( O
2019 O
) O
and O
split O
it O
into O
70 O
% O
training O
, O
10 O
% O
validation O
, O
20 O
% O
testing O
. O
Table O
3 O
( O
left O
) O
contains O
the O
statistical O
details O
for O
these O
three O
sets O
. O
Data O
for O
Domain O
Transfer O
Evaluation O
We O
pick O
WIKI O
- O
SECTION O
as O
our O
training O
set O
in O
this O
line O
of O
experiments O
, O
due O
to O
its O
largest O
size O
and O
variety O
of O
covered O
topics O
. O
Following O
previous O
work O
, O
we O
evaluate O
our O
model O
and O
baselines O
on O
four O
datasets O
that O
originate O
from O
different O
source O
distributions O
: O
WIKI-50 O
( O
Koshorek O
et O
al O
. O
, O
2018 O
) O
which O
consists O
of O
50 O
samples O
randomly O
generated O
from O
the O
latest O
English O
Wikipedia O
dump O
, O
with O
no O
overlap O
with O
training O
and O
validation O
data O
. O
Cities O
( O
Chen O
et O
al O
. O
, O
2009 O
) O
which O
consists O
of O
100 O
samples O
generated O
from O
Wikipedia O
about O
cities O
. O
We O
also O
ensure O
that O
this O
dataset O
has O
no O
overlap O
with O
training O
and O
validation O
data O
. O
Elements O
( O
Chen O
et O
al O
. O
, O
2009 O
) O
which O
consists O
of O
118 O
samples O
generated O
from O
Wikipedia O
about O
chemical O
elements O
. O
Clinical O
Books O
( O
Malioutov O
and O
Barzilay O
, O
2006 O
) O
which O
consists O
of O
227 O
chapters O
from O
a O
medical O
textbook O
. O
Table O
3 O
( O
right O
) O
gives O
more O
detailed O
statistics O
for O
these O
datasets O
. O
Data O
For O
Multilingual O
Evaluation O
In O
order O
to O
test O
the O
effectiveness O
of O
our O
context O
modeling O
strategy O
across O
languages O
, O
besides O
the O
English O
WIKISECTION O
, O
we O
train O
and O
test O
our O
model O
on O
two O
other O
Wikipedia O
datasets O
in O
German O
and O
Chinese O
: O
SECTION O
- O
DE O
which O
was O
released O
together O
with O
English O
WIKI O
- O
SECTION O
in O
Arnold O
et O
al O
. O
( O
2019 O
) O
. O
ItDataset O
CHOI O
RULES O
SECTION O
MEAN O
Random O
49.4 O
50.6 O
51.3 O
50.4 O
BayesSeg O
20.8 O
41.5 O
39.5 O
33.9 O
GraphSeg O
6.6 O
39.3 O
44.9 O
30.3 O
TextSeg O
1.0 O
7.7 O
12.6 O
7.1 O
Sector O
- O
- O
12.7 O
Transformer O
4.8 O
9.6 O
13.6 O
9.3 O
Basic O
Model O
0.81 O
7.0 O
11.3 O
6.4 O
+ O
AUX O
0.64‚Ä†6.1‚Ä†10.4‚Ä†5.7 O
+ O
RSA O
0.72‚Ä†6.3‚Ä†10.0‚Ä†5.7 O
+ O
AUX+RSA O
0.54‚Ä†5.8‚Ä†9.7‚Ä†5.3 O
Table O
5 O
: O
Pkerror O
score O
on O
three O
datasets O
. O
Results O
in O
bold O
indicate O
the O
best O
performance O
across O
all O
comparisons O
. O
Underlined O
results O
indicate O
the O
best O
performance O
in O
the O
bottom O
section O
. O
‚Ä†indicates O
the O
result O
is O
signiÔ¨Åcantly O
different O
( O
p<0.05 O
) O
from O
basic O
model O
. O
also O
contains O
articles O
about O
cities O
and O
diseases O
. O
The O
section O
marks O
are O
used O
as O
the O
ground O
truth O
labels O
. O
SECTION O
- O
ZH O
which O
was O
randomly O
generated O
from O
the O
Chinese O
Wikipedia O
dump4mentioned O
in O
Hao O
and O
Paul O
( O
2020 O
) O
. O
As O
before O
, O
section O
marks O
are O
also O
used O
here O
as O
ground O
truth O
boundaries O
. O
The O
statistical O
details O
of O
these O
two O
datasets O
can O
be O
found O
in O
Table O
4 O
. O
4.2 O
Baselines O
These O
include O
two O
popular O
unsupervised O
topic O
segmentation O
methods O
, O
BayesSeg O
( O
Eisenstein O
and O
Barzilay O
, O
2008 O
) O
and O
GraphSeg O
( O
Glava O
Àás O
et O
al O
. O
, O
2016 O
) O
, O
as O
well O
as O
the O
three O
recently O
proposed O
supervised O
neural O
models O
, O
TextSeg O
( O
Koshorek O
et O
al O
. O
, O
2018 O
) O
( O
from O
which O
we O
derive O
our O
basic O
model O
) O
, O
Sector O
( O
Arnold O
et O
al O
. O
, O
2019 O
) O
and O
Hierarchical O
Transformer O
( O
labeled O
Transformer O
in O
the O
tables O
) O
( O
Glavas O
and O
Somasundaran O
, O
2020 O
) O
. O
We O
use O
the O
original O
implementation O
of O
BayesSeg O
, O
GraphSeg O
andTextSeg O
. O
We O
reimplement O
the O
Hierarchical O
Transformer O
ourselves O
. O
In O
Table O
6 O
, O
we O
adopt O
the O
results O
of O
BayesSeg O
, O
GraphSeg O
andSector O
from O
Arnold O
et O
al O
. O
( O
2019)5 O
. O
4.3 O
Evaluation O
Metric O
We O
use O
the O
standard O
Pkerror O
score O
( O
Beeferman O
et O
al O
. O
, O
1999 O
) O
as O
our O
evaluation O
metric O
, O
since O
it O
has O
become O
the O
standard O
for O
comparing O
topic O
segmenters O
. O
Pkis O
calculated O
as O
: O
Pk(ref O
, O
hyp O
) O
= O
/summationtextn‚àík O
i=0Œ¥ref(i O
, O
i+k)/negationslash O
= O
Œ¥hyp(i O
, O
i+k O
) O
4https://linguatools.org/tools/ O
corpora O
/ O
wikipedia O
- O
monolingual O
- O
corpora/ O
5Arnold O
et O
al O
. O
( O
2019 O
) O
reported O
Sector O
‚Äôs O
performance O
on O
multiple O
model O
settings O
. O
Here O
we O
pick O
the O
performance O
of O
the O
model O
trained O
on O
wikifull O
to O
be O
close O
to O
our O
training O
setting.632Dataset O
Wiki-50 O
Cities O
Elements O
Clinical O
Random O
52.7 O
47.1 O
50.1 O
44.1 O
BayesSeg O
49.2 O
36.2 O
35.6 O
57.2 O
GraphSeg O
63.6 O
40.0 O
49.1 O
64.6 O
TextSeg O
28.5 O
19.8 O
43.9 O
36.6 O
Sector O
28.6 O
33.4 O
42.8 O
36.9 O
Transformer O
29.3 O
20.2 O
45.2 O
35.6 O
Basic O
Model O
28.7 O
17.9 O
43.5 O
33.8 O
+ O
AUX O
27.9 O
17.0‚Ä†41.8‚Ä†31.5‚Ä† O
+ O
RSA O
27.8‚Ä†16.8‚Ä†42.7 O
31.9‚Ä† O
+ O
AUX+RSA O
26.8‚Ä†16.1‚Ä†39.4‚Ä†30.5‚Ä† O
Table O
6 O
: O
Pkerror O
score O
on O
four O
test O
sets O
. O
Results O
in O
bold O
indicate O
the O
best O
performance O
across O
all O
comparisons O
. O
Underlined O
results O
indicate O
the O
best O
performance O
in O
the O
bottom O
section O
. O
‚Ä†indicates O
the O
result O
is O
signiÔ¨Åcantly O
different O
( O
p<0.05 O
) O
from O
basic O
model O
. O
whereŒ¥is O
an O
indicator O
function O
which O
is O
1 O
if O
sentenceiandi+kare O
in O
the O
same O
segment O
, O
0 O
otherwise O
. O
It O
measures O
the O
probability O
of O
mismatches O
between O
the O
ground O
truth O
segments O
( O
ref O
) O
and O
model O
predictions O
( O
hyp O
) O
within O
a O
sliding O
window O
k. O
As O
a O
standard O
setting O
which O
has O
been O
used O
in O
previous O
work O
, O
window O
size O
kis O
the O
average O
segment O
length O
ofref O
. O
SincePkis O
a O
penalty O
metric O
, O
lower O
score O
indicates O
better O
performance O
. O
4.4 O
Neural O
Model O
Setup O
Following O
Koshorek O
et O
al O
. O
( O
2018 O
) O
, O
our O
initial O
word O
embeddings O
are O
GoogleNews O
word2vec O
( O
d= O
300 O
) O
. O
We O
also O
use O
word2vec O
embeddings O
( O
d= O
300 O
) O
and O
Fasttext O
embeddings O
( O
d= O
300 O
) O
, O
which O
are O
both O
derived O
from O
Wikipedia O
corpora O
for O
German O
and O
Chinese O
respectively O
. O
We O
use O
the O
Adam O
optimizer O
, O
setting O
the O
learning O
rate O
to O
0.001 O
and O
batch O
size O
to O
8 O
. O
The O
BiLSTM O
hidden O
state O
size O
is O
256 O
following O
Koshorek O
et O
al O
. O
( O
2018 O
) O
. O
Model O
training O
is O
done O
for O
10 O
epochs O
and O
performance O
is O
monitored O
over O
the O
validation O
set O
. O
We O
generate O
BERT O
sentence O
embeddings O
with O
the O
pre O
- O
trained O
12 O
- O
layer O
model O
released O
by O
Google O
AI O
( O
embedding O
size O
768 O
) O
. O
The O
window O
size O
of O
restricted O
self O
- O
attention O
is O
3 O
and O
Œ± O
is O
0.8 O
. O
These O
were O
tuned O
on O
the O
validation O
sets O
of O
the O
datasets O
we O
use O
. O
5 O
Results O
and O
Discussion O
5.1 O
Intra O
- O
Domain O
Evaluation O
Table O
5 O
shows O
the O
models O
‚Äô O
performance O
on O
the O
three O
datasets O
, O
when O
all O
supervised O
models O
are O
trained O
and O
evaluated O
on O
the O
training O
and O
test O
set O
from O
the O
same O
domain O
. O
To O
investigate O
the O
effectiveness O
of O
auxiliary O
task O
( O
AUX O
) O
and O
restricted O
self O
- O
attentionDataset O
EN O
DE O
ZH O
Random O
51.3 O
48.7 O
52.2 O
Basic O
Model O
11.3 O
18.2 O
20.5 O
+ O
AUX O
10.4‚Ä†17.7 O
20.5 O
+ O
RSA O
10.0‚Ä†16.6‚Ä†19.8‚Ä† O
+ O
AUX+RSA O
9.7‚Ä†15.9‚Ä†20.0‚Ä† O
Table O
7 O
: O
Pkerror O
score O
on O
the O
datasets O
in O
three O
languages O
( O
English O
, O
German O
and O
Chinese O
) O
. O
( O
RSA O
) O
, O
Table O
5 O
also O
shows O
the O
results O
of O
individually O
adding O
each O
component O
to O
our O
basic O
segmenter O
. O
The O
most O
important O
observation O
from O
the O
table O
is O
that O
our O
model O
enhanced O
by O
context O
modeling O
outperforms O
all O
the O
supervised O
and O
unsupervised O
baselines O
with O
a O
substantial O
performance O
gain O
. O
With O
our O
context O
modeling O
strategy O
, O
the O
average O
Pkscores O
of O
our O
model O
over O
the O
three O
datasets O
improves O
on O
the O
best O
model O
( O
TextSeg O
) O
among O
the O
baselines O
by O
25 O
% O
. O
Compared O
with O
the O
basic O
model O
, O
adding O
AUX O
or O
RSA O
equally O
gives O
signiÔ¨Åcant O
and O
consistent O
improvement O
across O
all O
three O
sets O
. O
Adding O
both O
AUX O
and O
RSA O
results O
in O
the O
biggest O
improvement O
by O
up O
to O
17 O
% O
on O
the O
mean O
across O
the O
three O
datasets O
. O
5.2 O
Domain O
Transfer O
Evaluation O
Table O
6 O
compares O
the O
performance O
of O
the O
baselines O
and O
our O
model O
on O
four O
challenging O
real O
- O
world O
test O
datasets O
. O
All O
supervised O
models O
are O
trained O
on O
the O
training O
set O
of O
WIKI O
- O
SECTION O
. O
One O
important O
observation O
is O
that O
our O
model O
enhanced O
by O
context O
modeling O
outperforms O
all O
the O
baseline O
methods O
on O
three O
out O
of O
four O
test O
sets O
with O
a O
substantial O
performance O
gap O
. O
Admittedly O
, O
BayesSeg O
performs O
better O
on O
Elements O
, O
possibly O
because O
that O
merely O
word O
embedding O
similarity O
is O
sufÔ¨Åcient O
to O
indicate O
segment O
boundaries O
in O
this O
dataset O
. O
However O
, O
BayesSeg O
is O
completely O
dominated O
by O
our O
model O
on O
the O
other O
test O
sets O
. O
Overall O
, O
this O
indicates O
that O
our O
proposed O
context O
modeling O
strategy O
can O
not O
only O
enhance O
the O
model O
under O
the O
intra O
- O
domain O
setting O
, O
but O
also O
produce O
robust O
models O
that O
transfer O
to O
other O
unseen O
domains O
. O
Furthermore O
, O
we O
observe O
that O
AUX O
and O
RSA O
are O
both O
necessary O
for O
our O
model O
, O
since O
they O
do O
not O
only O
improve O
performance O
individually O
, O
but O
they O
achieve O
the O
best O
results O
when O
synergistically O
combined O
. O
5.3 O
Multilingual O
Evaluation O
Table O
7 O
shows O
results O
for O
our O
context O
modeling O
strategy O
across O
three O
different O
languages O
: O
English O
( O
EN O
) O
, O
German O
( O
DE O
) O
and O
Chinese O
( O
ZH O
) O
. O
Remark-633ably O
, O
even O
our O
basic O
model O
without O
any O
add O
- O
on O
component O
outperforms O
the O
random O
baseline O
by O
a O
wide O
margin O
. O
Looking O
at O
the O
gains O
from O
AUX O
and O
RSA O
, O
for O
German O
we O
observe O
a O
pattern O
similar O
to O
English O
, O
with O
our O
complete O
context O
modeling O
strategy O
( O
AUX+RSA O
) O
delivering O
the O
strongest O
gains O
. O
However O
, O
the O
performance O
on O
Chinese O
is O
not O
as O
strong O
as O
on O
English O
and O
German O
. O
Employing O
RSA O
still O
achieves O
a O
statistically O
signiÔ¨Åcant O
0.7 O
Pk O
score O
drop O
, O
but O
introducing O
AUX O
does O
not O
help O
. O
One O
possible O
reason O
is O
that O
the O
sentences O
in O
the O
Chinese O
Wikipedia O
pages O
are O
relatively O
short O
and O
fragmented O
. O
Thus O
, O
the O
semantics O
of O
these O
sentences O
may O
be O
too O
simple O
to O
sufÔ¨Åciently O
guide O
the O
coherence O
auxiliary O
task O
. O
In O
general O
, O
when O
comparing O
the O
behavior O
of O
our O
context O
modeling O
strategy O
across O
these O
three O
languages O
, O
RSA O
appears O
to O
yield O
stable O
beneÔ¨Åts O
, O
while O
the O
effectiveness O
of O
AUX O
seems O
to O
depend O
more O
on O
peculiarities O
of O
the O
dataset O
in O
the O
target O
language O
. O
6 O
Conclusions O
and O
Future O
Work O
We O
address O
a O
serious O
limitation O
of O
current O
neural O
topic O
segmenters O
, O
namely O
their O
inability O
to O
effectively O
model O
context O
. O
To O
this O
end O
, O
we O
propose O
a O
novel O
neural O
model O
that O
adds O
a O
coherence O
- O
related O
auxiliary O
task O
and O
restricted O
self O
- O
attention O
on O
top O
of O
a O
hierarchical O
BiLSTM O
attention O
segmenter O
to O
make O
better O
use O
of O
the O
contextual O
information O
. O
Experimental O
results O
of O
intra O
- O
domain O
on O
three O
datasets O
show O
that O
our O
strategy O
is O
effective O
within O
domains O
. O
Further O
, O
results O
on O
four O
challenging O
real O
- O
world O
benchmarks O
demonstrate O
its O
effectiveness O
in O
domain O
transfer O
settings O
. O
Finally O
, O
the O
application O
to O
other O
two O
languages O
( O
German O
and O
Chinese O
) O
suggests O
that O
our O
strategy O
has O
its O
potential O
in O
multilingual O
scenarios O
. O
As O
future O
work O
, O
we O
will O
investigate O
whether O
our O
proposed O
context O
modeling O
strategy O
is O
also O
effective O
for O
segmenting O
dialogues O
( O
Takanobu O
et O
al O
. O
, O
2018 O
) O
rather O
than O
just O
standard O
articles O
. O
Secondly O
, O
we O
will O
explore O
how O
to O
capture O
even O
more O
accurate O
and O
informative O
contextual O
information O
by O
integrating O
document O
structures O
or O
sentence O
dependencies O
obtained O
from O
other O
NLP O
tasks O
( O
e.g. O
, O
discourse O
parsing O
( O
Huber O
and O
Carenini O
, O
2019 O
, O
2020 O
) O
or O
discourse O
role O
labeling O
( O
Zeng O
et O
al O
. O
, O
2019 O
) O
) O
. O
Acknowledgments O
We O
thank O
the O
anonymous O
reviewers O
and O
the O
UBCNLP O
group O
for O
their O
insightful O
comments O
. O
References O
Sebastian O
Arnold O
, O
Rudolf O
Schneider O
, O
Philippe O
Cudr O
¬¥ O
eMauroux O
, O
Felix O
A. O
Gers O
, O
and O
Alexander O
L O
¬®oser O
. O
2019 O
. O
Sector O
: O
A O
neural O
model O
for O
coherent O
topic O
segmentation O
and O
classiÔ¨Åcation O
. O
Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
7:169‚Äì184 O
. O
Pinkesh O
Badjatiya O
, O
Litton O
J. O
Kurisinkel O
, O
Manish O
Gupta O
, O
and O
Vasudeva O
Varma O
. O
2018 O
. O
Attention O
- O
based O
neural O
text O
segmentation O
. O
In O
European O
Conference O
on O
Information O
Retrieval O
2018 O
, O
pages O
180‚Äì193 O
. O
Joe O
Barrow O
, O
Rajiv O
Jain O
, O
Vlad O
Morariu O
, O
Varun O
Manjunatha O
, O
Douglas O
Oard O
, O
and O
Philip O
Resnik O
. O
2020 O
. O
A O
joint O
model O
for O
document O
segmentation O
and O
segment O
labeling O
. O
In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
313‚Äì322 O
. O
Regina O
Barzilay O
and O
Mirella O
Lapata O
. O
2005 O
. O
Modeling O
local O
coherence O
: O
An O
entity O
- O
based O
approach O
. O
In O
Proceedings O
of O
the O
43rd O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
ACL‚Äô05 O
) O
, O
pages O
141‚Äì148 O
. O
Regina O
Barzilay O
and O
Mirella O
Lapata O
. O
2008 O
. O
Modeling O
local O
coherence O
: O
An O
entity O
- O
based O
approach O
. O
Computational O
Linguistics O
, O
34(1):1‚Äì34 O
. O
Doug O
Beeferman O
, O
Adam O
Berger O
, O
and O
John O
Lafferty O
. O
1999 O
. O
Statistical O
models O
for O
text O
segmentation O
. O
Machine O
Learning O
, O
34(1):177‚Äì210 O
. O
Marianne O
Bertrand O
, O
Matilde O
Bombardini O
, O
Raymond O
Fisman O
, O
Bradley O
Hackinen O
, O
and O
Francesco O
Trebbi O
. O
2018 O
. O
Hall O
of O
mirrors O
: O
Corporate O
philanthropy O
and O
strategic O
advocacy O
. O
Technical O
report O
, O
National O
Bureau O
of O
Economic O
Research O
. O
Harr O
Chen O
, O
S.R.K. O
Branavan O
, O
Regina O
Barzilay O
, O
and O
David O
R. O
Karger O
. O
2009 O
. O
Global O
models O
of O
document O
structure O
using O
latent O
permutations O
. O
In O
Proceedings O
of O
Human O
Language O
Technologies O
: O
The O
2009 O
Annual O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
371‚Äì379 O
. O
Freddy O
Y O
. O
Y O
. O
Choi O
. O
2000 O
. O
Advances O
in O
domain O
independent O
linear O
text O
segmentation O
. O
In O
1st O
Meeting O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
. O
Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O
BERT O
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
understanding O
. O
In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
4171‚Äì4186 O
. O
Dennis O
Diefenbach O
, O
Vanessa O
Lopez O
, O
Kamal O
Singh O
, O
and O
Pierre O
Maret O
. O
2018 O
. O
Core O
techniques O
of O
question O
answering O
systems O
over O
knowledge O
bases O
: O
a O
survey O
. O
Knowledge O
and O
Information O
Systems O
, O
55(3):529 O
‚Äì O
569.634Teun O
van O
Dijk O
. O
1981 O
. O
Episodes O
as O
units O
of O
discourse O
analysis O
. O
Analyzing O
Discourse O
: O
Text O
and O
Talk O
. O
Jacob O
Eisenstein O
and O
Regina O
Barzilay O
. O
2008 O
. O
Bayesian O
unsupervised O
topic O
segmentation O
. O
In O
Proceedings O
of O
the O
2008 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
334‚Äì343 O
. O
Michel O
Galley O
, O
Kathleen O
McKeown O
, O
Eric O
FoslerLussier O
, O
and O
Hongyan O
Jing O
. O
2003 O
. O
Discourse O
segmentation O
of O
multi O
- O
party O
conversation O
. O
In O
Proceedings O
of O
the O
41st O
Annual O
Meeting O
on O
Association O
for O
Computational O
Linguistics O
- O
Volume O
1 O
, O
pages O
562 O
‚Äì O
569 O
. O
Goran O
Glava O
Àás O
, O
Federico O
Nanni O
, O
and O
Simone O
Paolo O
Ponzetto O
. O
2016 O
. O
Unsupervised O
text O
segmentation O
using O
semantic O
relatedness O
graphs O
. O
In O
Proceedings O
of O
the O
Fifth O
Joint O
Conference O
on O
Lexical O
and O
Computational O
Semantics O
, O
pages O
125‚Äì130 O
. O
Association O
for O
Computational O
Linguistics O
. O
Goran O
Glavas O
and O
Swapna O
Somasundaran O
. O
2020 O
. O
Twolevel O
transformer O
and O
auxiliary O
coherence O
modeling O
for O
improved O
text O
segmentation O
. O
In O
The O
ThirtyFourth O
AAAI O
Conference O
on O
ArtiÔ¨Åcial O
Intelligence O
( O
AAAI-20 O
) O
, O
pages O
2306‚Äì2315 O
. O
Shudong O
Hao O
and O
Michael O
J. O
Paul O
. O
2020 O
. O
An O
empirical O
study O
on O
crosslingual O
transfer O
in O
probabilistic O
topic O
models O
. O
Computational O
Linguistics O
, O
46(1):95‚Äì134 O
. O
Marti O
A. O
Hearst O
. O
1997 O
. O
Text O
tiling O
: O
Segmenting O
text O
into O
multi O
- O
paragraph O
subtopic O
passages O
. O
Computational O
Linguistics O
, O
23(1):33‚Äì64 O
. O
Xiaolei O
Huang O
and O
Michael O
J. O
Paul O
. O
2019 O
. O
Neural O
temporality O
adaptation O
for O
document O
classiÔ¨Åcation O
: O
Diachronic O
word O
embeddings O
and O
domain O
adaptation O
models O
. O
In O
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
4113‚Äì4123 O
. O
Association O
for O
Computational O
Linguistics O
. O
Patrick O
Huber O
and O
Giuseppe O
Carenini O
. O
2019 O
. O
Predicting O
discourse O
structure O
using O
distant O
supervision O
from O
sentiment O
. O
In O
Proceedings O
of O
the O
2019 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
and O
the O
9th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
( O
EMNLPIJCNLP O
) O
, O
pages O
2306‚Äì2316 O
. O
Patrick O
Huber O
and O
Giuseppe O
Carenini O
. O
2020 O
. O
Mega O
rst O
discourse O
treebanks O
with O
structure O
and O
nuclearity O
from O
scalable O
distant O
sentiment O
supervision O
. O
In O
Proceedings O
of O
the O
2020 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
. O
Association O
for O
Computational O
Linguistics O
. O
Omri O
Koshorek O
, O
Adir O
Cohen O
, O
Noam O
Mor O
, O
Michael O
Rotman O
, O
and O
Jonathan O
Berant O
. O
2018 O
. O
Text O
segmentation O
as O
a O
supervised O
learning O
task O
. O
In O
Proceedings O
of O
the O
2018 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
2 O
( O
Short O
Papers O
) O
, O
pages O
469‚Äì473.Zachary O
C. O
Lipton O
, O
John O
Berkowitz O
, O
and O
Charles O
Elkan O
. O
2015 O
. O
A O
critical O
review O
of O
recurrent O
neural O
networks O
for O
sequence O
learning O
. O
CoRR O
, O
abs/1506.00019 O
. O
Michal O
Lukasik O
, O
Boris O
Dadachev O
, O
Gonc O
¬∏alo O
Sim O
Àúoes O
, O
and O
Kishore O
Papineni O
. O
2020 O
. O
Text O
segmentation O
by O
cross O
segment O
attention O
. O
CoRR O
, O
abs/2004.14535 O
. O
Igor O
Malioutov O
and O
Regina O
Barzilay O
. O
2006 O
. O
Minimum O
cut O
model O
for O
spoken O
lecture O
segmentation O
. O
In O
Proceedings O
of O
the O
21st O
International O
Conference O
on O
Computational O
Linguistics O
and O
44th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
25‚Äì32 O
. O
Mandar O
Mitra O
, O
Amit O
Singhal O
, O
and O
Chris O
Buckley O
. O
1997 O
. O
Automatic O
text O
summarization O
by O
paragraph O
extraction O
. O
In O
Intelligent O
Scalable O
Text O
Summarization O
. O
HyoJung O
Oh O
, O
Sung O
Hyon O
Myaeng O
, O
and O
Myung O
- O
Gil O
Jang O
. O
2007 O
. O
Semantic O
passage O
segmentation O
based O
on O
sentence O
topics O
for O
question O
answering O
. O
Information O
Sciences O
, O
177(18):3696‚Äì3717 O
. O
Martin O
Riedl O
and O
Chris O
Biemann O
. O
2012a O
. O
How O
text O
segmentation O
algorithms O
gain O
from O
topic O
models O
. O
In O
Proceedings O
of O
the O
2012 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
pages O
553‚Äì557 O
. O
Martin O
Riedl O
and O
Chris O
Biemann O
. O
2012b O
. O
Topictiling O
: O
A O
text O
segmentation O
algorithm O
based O
on O
lda O
. O
In O
Proceedings O
of O
ACL O
2012 O
Student O
Research O
Workshop O
, O
pages O
37‚Äì42 O
. O
Swarnadeep O
Saha O
, O
Malolan O
Chetlur O
, O
Tejas O
Indulal O
Dhamecha O
, O
W O
M O
Gayathri O
K O
Wijayarathna O
, O
Red O
Mendoza O
, O
Paul O
Gagnon O
, O
Nabil O
Zary O
, O
and O
Shantanu O
Godbole O
. O
2019 O
. O
Aligning O
learning O
outcomes O
to O
learning O
resources O
: O
A O
lexico O
- O
semantic O
spatial O
approach O
. O
In O
Proceedings O
of O
the O
Twenty O
- O
Eighth O
International O
Joint O
Conference O
on O
ArtiÔ¨Åcial O
Intelligence O
, O
IJCAI-19 O
, O
pages O
5168‚Äì5174 O
. O
Mike O
Schuster O
and O
Kuldip O
K. O
Paliwal O
. O
1997 O
. O
Bidirectional O
recurrent O
neural O
networks O
. O
IEEE O
Transactions O
on O
Signal O
Processing O
, O
45:2673‚Äì2681 O
. O
Imran O
Sehikh O
, O
Dominique O
Fohr O
, O
and O
Irina O
Illina O
. O
2017 O
. O
Topic O
segmentation O
in O
asr O
transcripts O
using O
bidirectional O
rnns O
for O
change O
detection O
. O
In O
2017 O
IEEE O
Automatic O
Speech O
Recognition O
and O
Understanding O
Workshop O
( O
ASRU O
) O
. O
Ryuichi O
Takanobu O
, O
Minlie O
Huang O
, O
Zhongzhou O
Zhao O
, O
Fenglin O
Li O
, O
Haiqing O
Chen O
, O
Xiaoyan O
Zhu O
, O
and O
Liqiang O
Nie O
. O
2018 O
. O
A O
weakly O
supervised O
method O
for O
topic O
segmentation O
and O
labeling O
in O
goal O
- O
oriented O
dialogues O
via O
reinforcement O
learning O
. O
In O
Proceedings O
of O
the O
Twenty O
- O
Seventh O
International O
Joint O
Conference O
on O
ArtiÔ¨Åcial O
Intelligence O
, O
IJCAI-18 O
, O
pages O
4403‚Äì4410.635Ashish O
Vaswani O
, O
Noam O
Shazeer O
, O
Niki O
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion O
Jones O
, O
Aidan O
N O
Gomez O
, O
≈Å O
ukasz O
Kaiser O
, O
and O
Illia O
Polosukhin O
. O
2017 O
. O
Attention O
is O
all O
you O
need O
. O
In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
30 O
, O
pages O
5998‚Äì6008 O
. O
Liang O
Wang O
, O
Sujian O
Li O
, O
Yajuan O
Lv O
, O
and O
Houfeng O
Wang O
. O
2017 O
. O
Learning O
to O
rank O
semantic O
coherence O
for O
topic O
segmentation O
. O
In O
Proceedings O
of O
the O
2017 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
1340‚Äì1344 O
. O
Liang O
Wang O
, O
Sujian O
Li O
, O
Xinyan O
Xiao O
, O
and O
Yajuan O
Lyu O
. O
2016 O
. O
Topic O
segmentation O
of O
web O
documents O
with O
automatic O
cue O
phrase O
identiÔ¨Åcation O
and O
blstm O
- O
cnn O
. O
InNatural O
Language O
Understanding O
and O
Intelligent O
Applications O
, O
pages O
177‚Äì188 O
. O
Wenhui O
Wang O
and O
Baobao O
Chang O
. O
2016 O
. O
Graph O
- O
based O
dependency O
parsing O
with O
bidirectional O
LSTM O
. O
In O
Proceedings O
of O
the O
54th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
2306‚Äì2315 O
. O
Yizhong O
Wang O
, O
Sujian O
Li O
, O
and O
Jingfeng O
Yang O
. O
2018 O
. O
Toward O
fast O
and O
accurate O
neural O
discourse O
segmentation O
. O
In O
Proceedings O
of O
the O
2018 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
962‚Äì967 O
. O
Wen O
Xiao O
and O
Giuseppe O
Carenini O
. O
2019 O
. O
Extractive O
summarization O
of O
long O
documents O
by O
combining O
global O
and O
local O
context O
. O
In O
Proceedings O
of O
the O
2019 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
3009‚Äì3019 O
. O
Yi O
Xu O
, O
Hai O
Zhao O
, O
and O
Zhuosheng O
Zhang O
. O
2020 O
. O
Topic O
- O
aware O
multi O
- O
turn O
dialogue O
modeling O
. O
CoRR O
, O
abs/2009.12539 O
. O
Zichao O
Yang O
, O
Diyi O
Yang O
, O
Chris O
Dyer O
, O
Xiaodong O
He O
, O
Alex O
Smola O
, O
and O
Eduard O
Hovy O
. O
2016 O
. O
Hierarchical O
attention O
networks O
for O
document O
classiÔ¨Åcation O
. O
In O
Proceedings O
of O
the O
2016 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
pages O
1480‚Äì1489 O
. O
Jichuan O
Zeng O
, O
Jing O
Li O
, O
Yulan O
He O
, O
Cuiyun O
Gao O
, O
Michael O
R. O
Lyu O
, O
and O
Irwin O
King O
. O
2019 O
. O
What O
you O
say O
and O
how O
you O
say O
it O
: O
Joint O
modeling O
of O
topics O
and O
discourse O
in O
microblog O
conversations O
. O
Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
7:267‚Äì281 O
. O
Hainan O
Zhang O
, O
Yanyan O
Lan O
, O
Liang O
Pang O
, O
Hongshen O
Chen O
, O
Zhuoye O
Ding O
, O
and O
Dawei O
Yin O
. O
2020 O
. O
Modeling O
topical O
relevance O
for O
multi O
- O
turn O
dialogue O
generation O
. O
In O
Proceedings O
of O
the O
Twenty O
- O
Ninth O
International O
Joint O
Conference O
on O
ArtiÔ¨Åcial O
Intelligence O
, O
IJCAI-20 O
, O
pages O
3737‚Äì3743 O
. O
International O
Joint O
Conferences O
on O
ArtiÔ¨Åcial O
Intelligence O
Organization.636Proceedings O
of O
the O
1st O
Conference O
of O
the O
Asia O
- O
PaciÔ¨Åc O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
10th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
637‚Äì642 O
December O
4 O
- O
7 O
, O
2020 O
. O
¬© O
2020 O
Association O
for O
Computational O
Linguistics O
Contextualized O
End O
- O
to O
- O
End O
Neural O
Entity O
Linking O
Haotian O
Chen O
BlackRock O
haotian.chen@blackrock.comAndrej O
Zukov O
- O
Gregoric O
BlackRock O
andrej.zukovgregoric@blackrock.com O
Xi O
( O
David O
) O
Li O
BlackRock O
david.li@blackrock.comSahil O
Wadhwa O
University O
of O
Illinois O
at O
Urbana O
- O
Champaign‚àó O
sahilw2@illinois.edu O
Abstract O
We O
propose O
an O
entity O
linking O
( O
EL O
) O
model O
that O
jointly O
learns O
mention O
detection O
( O
MD O
) O
and O
entity O
disambiguation O
( O
ED O
) O
. O
Our O
model O
applies O
task O
- O
speciÔ¨Åc O
heads O
on O
top O
of O
shared O
BERT O
contextualized O
embeddings O
. O
We O
achieve O
stateof O
- O
the O
- O
art O
results O
across O
a O
standard O
EL O
dataset O
using O
our O
model O
; O
we O
also O
study O
our O
model O
‚Äôs O
performance O
under O
the O
setting O
when O
handcrafted O
entity O
candidate O
sets O
are O
not O
available O
and O
Ô¨Ånd O
that O
the O
model O
performs O
well O
under O
such O
a O
setting O
also O
. O
1 O
Introduction O
Entity O
linking O
( O
EL)1 O
, O
in O
our O
context O
, O
refers O
to O
the O
joint O
task O
of O
recognizing O
named O
entity O
mentions O
in O
text O
through O
mention O
detection O
( O
MD O
) O
and O
linking O
each O
mention O
to O
a O
unique O
entity O
in O
a O
knowledge O
base O
( O
KB O
) O
through O
entity O
disambiguation O
( O
ED)2 O
. O
For O
example O
, O
in O
the O
sentence O
‚Äú O
The O
Times O
began O
publication O
under O
its O
current O
name O
in O
1788 O
, O
‚Äù O
the O
span O
The O
Times O
should O
be O
detected O
as O
a O
named O
entity O
mention O
and O
then O
linked O
to O
the O
corresponding O
entity O
: O
TheTimes O
, O
a O
British O
newspaper O
. O
However O
, O
an O
EL O
model O
which O
disjointly O
applies O
MD O
and O
ED O
might O
easily O
mistake O
this O
mention O
with O
TheNew O
York O
Times O
, O
an O
American O
newspaper O
. O
Since O
our O
model O
jointly O
learns O
MD O
and O
ED O
from O
the O
same O
contextualized O
BERT O
embeddings O
, O
its O
Ô¨Ånal O
EL O
prediction O
is O
partially O
informed O
by O
both O
. O
As O
a O
result O
, O
it O
is O
able O
to O
generalize O
better O
. O
Another O
common O
approach O
employed O
in O
previous O
EL O
research O
is O
candidate O
generation O
, O
where O
for O
each O
detected O
mention O
, O
a O
set O
of O
candidate O
entities O
is O
generated O
and O
the O
entities O
within O
it O
are O
ranked O
by O
a O
model O
to O
Ô¨Ånd O
the O
best O
match O
. O
Such O
sets O
are O
‚àóWork O
done O
while O
at O
BlackRock O
. O
1Also O
known O
as O
A2 O
KB O
task O
in O
GERBIL O
evaluation O
platform O
( O
R O
¬®oder O
et O
al O
. O
, O
2018 O
) O
and O
end O
- O
to O
- O
end O
entity O
linking O
in O
some O
literature O
2Also O
known O
as O
D2 O
KB O
task O
in O
GERBILbuilt O
using O
hand O
- O
crafted O
rules O
which O
deÔ¨Åne O
which O
entities O
make O
it O
in O
and O
which O
do O
not O
. O
This O
risks O
( O
1 O
) O
skipping O
out O
on O
valid O
entities O
which O
should O
be O
in O
the O
candidate O
set O
and O
( O
2 O
) O
inÔ¨Çating O
model O
performance O
since O
often O
times O
candidate O
sets O
contain O
only O
one O
or O
two O
items O
. O
These O
sets O
are O
almost O
always O
used O
at O
prediction O
time O
and O
sometimes O
even O
during O
training O
. O
Our O
model O
has O
the O
option O
of O
not O
relying O
on O
them O
during O
prediction O
, O
and O
never O
uses O
them O
during O
training O
. O
We O
introduce O
two O
main O
contributions O
: O
( O
i)We O
propose O
a O
new O
end O
- O
to O
- O
end O
differentiable O
neural O
EL O
model O
that O
jointly O
performs O
MD O
and O
ED O
and O
achieves O
state O
- O
of O
- O
the O
- O
art O
performance O
. O
( O
ii)We O
study O
the O
performance O
of O
our O
model O
when O
candidate O
sets O
are O
removed O
to O
see O
whether O
EL O
can O
perform O
well O
without O
them O
. O
2 O
Related O
Work O
Neural O
- O
network O
based O
models O
have O
recently O
achieved O
strong O
results O
across O
standard O
EL O
datasets O
. O
Research O
has O
focused O
on O
learning O
better O
entity O
representations O
and O
extracting O
better O
local O
and O
global O
features O
through O
novel O
model O
architectures O
. O
Entity O
representation O
. O
Good O
KB O
entity O
representations O
are O
a O
key O
component O
of O
most O
ED O
and O
EL O
models O
. O
Representation O
learning O
has O
been O
addressed O
by O
Yamada O
et O
al O
. O
( O
2016 O
) O
, O
Ganea O
and O
Hofmann O
( O
2017 O
) O
, O
Cao O
et O
al O
. O
( O
2017 O
) O
and O
Yamada O
et O
al O
. O
( O
2017 O
) O
. O
Sil O
et O
al O
. O
( O
2018 O
) O
and O
Cao O
et O
al O
. O
( O
2018 O
) O
extend O
it O
to O
the O
cross O
- O
lingual O
setting O
. O
More O
recently O
, O
Yamada O
and O
Shindo O
( O
2019 O
) O
have O
suggested O
learning O
entity O
representations O
using O
BERT O
which O
achieves O
state O
- O
of O
- O
the O
- O
art O
results O
in O
ED O
. O
Entity O
Disambiguation O
( O
ED O
) O
. O
The O
ED O
task O
assumes O
already O
- O
labelled O
mention O
spans O
which O
are O
then O
disambiguated O
. O
Recent O
work O
on O
ED O
has O
focused O
on O
extracting O
global O
features O
( O
Ratinov O
et O
al O
. O
,6372011 O
; O
Globerson O
et O
al O
. O
, O
2016 O
; O
Ganea O
and O
Hofmann O
, O
2017 O
; O
Le O
and O
Titov O
, O
2018 O
) O
, O
extending O
the O
scope O
of O
ED O
to O
more O
non O
- O
standard O
datasets O
( O
Eshel O
et O
al O
. O
, O
2017 O
) O
, O
and O
positing O
the O
problem O
in O
new O
ways O
such O
as O
building O
separate O
classiÔ¨Åers O
for O
KB O
entities O
( O
Barrena O
et O
al O
. O
, O
2018 O
) O
. O
Entity O
Linking O
( O
EL O
) O
. O
Early O
work O
by O
Sil O
and O
Yates O
( O
2013 O
) O
, O
Luo O
et O
al O
. O
( O
2015 O
) O
and O
Nguyen O
et O
al O
. O
( O
2016 O
) O
introduced O
models O
that O
jointly O
learn O
NER O
and O
ED O
using O
engineered O
features O
. O
More O
recently O
, O
Kolitsas O
et O
al O
. O
( O
2018 O
) O
propose O
a O
neural O
model O
that O
Ô¨Årst O
generates O
all O
combinations O
of O
spans O
as O
potential O
mentions O
and O
then O
learns O
similarity O
scores O
over O
their O
entity O
candidates O
. O
MD O
is O
handled O
implicitly O
by O
only O
considering O
mention O
spans O
which O
have O
non O
- O
empty O
candidate O
entity O
sets O
. O
Martins O
et O
al O
. O
( O
2019 O
) O
propose O
training O
a O
multi O
- O
task O
NER O
and O
ED O
objective O
using O
a O
Stack O
- O
LSTM O
( O
Dyer O
et O
al O
. O
, O
2015 O
) O
. O
Finally O
, O
Poerner O
et O
al O
. O
( O
2019 O
) O
and O
Broscheit O
( O
2019 O
) O
both O
propose O
end O
- O
to O
- O
end O
EL O
models O
based O
onBERT O
. O
Poerner O
et O
al O
. O
( O
2019 O
) O
model O
the O
similarity O
between O
entity O
embeddings O
and O
contextualized O
word O
embeddings O
by O
mapping O
the O
former O
onto O
the O
latter O
whereas O
Broscheit O
( O
2019 O
) O
in O
essence O
do O
the O
opposite O
. O
Our O
work O
is O
different O
in O
three O
important O
ways O
: O
our O
training O
objective O
is O
different O
in O
that O
we O
explicitly O
model O
MD O
; O
we O
analyze O
the O
performance O
of O
our O
model O
when O
candidate O
sets O
are O
expanded O
to O
include O
the O
entire O
universe O
of O
entity O
embeddings O
; O
and O
we O
outperform O
both O
models O
by O
a O
wide O
margin O
. O
3 O
Model O
Description O
Given O
a O
document O
containing O
a O
sequence O
of O
ntokensw={w1, O
... O
,w O
n}with O
mention O
label O
indicators3ymd={I O
, O
O O
, O
B}nand O
entity O
IDs O
yed= O
{ O
j‚ààZ O
: O
j‚àà[1,k]}nwhich O
index O
a O
pre O
- O
trained O
entity O
embedding O
matrix O
E‚ààRk√ódof O
entity O
universe O
sizekand O
entity O
embedding O
dimension O
d O
, O
the O
model O
is O
trained O
to O
tag O
each O
token O
with O
its O
correct O
mention O
indicator O
and O
link O
each O
mention O
with O
its O
correct O
entity O
ID O
. O
3.1 O
Text O
Encoder O
The O
text O
input O
to O
our O
model O
is O
encoded O
by O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O
We O
initialize O
the O
pre O
- O
trained O
weights O
from O
BERT O
- O
BASE O
.4The O
text O
input O
is O
tokenized O
by O
the O
cased O
WordPiece O
( O
Johnson O
et O
al O
. O
, O
3We O
use O
standard O
inside O
- O
outside O
- O
beginning O
( O
IOB O
) O
tagging O
format O
introduced O
by O
( O
Ramshaw O
and O
Marcus O
, O
1995 O
) O
4https://github.com/google-research/bert2017 O
) O
sub O
- O
word O
tokenizer O
. O
The O
text O
encoder O
outputsncontextualized O
WordPiece O
embeddings O
h O
which O
are O
grouped O
to O
form O
the O
embedding O
matrix O
H‚ààRn√óm O
, O
wheremis O
the O
embedding O
dimension O
. O
In O
the O
case O
of O
BERT O
- O
BASE O
, O
mis O
equal O
to O
768 O
. O
The O
transformation O
from O
word O
level O
to O
WordPiece O
sub O
- O
word O
level O
labels O
is O
handled O
similarly O
to O
theBERT O
NER O
task O
, O
where O
the O
head O
WordPiece O
token O
represents O
the O
entire O
word O
, O
disregarding O
tail O
tokens O
. O
BERT O
comes O
in O
two O
settings O
: O
feature O
- O
based O
and O
Ô¨Åne O
- O
tuned O
. O
Under O
the O
feature O
- O
based O
setting O
, O
BERT O
parameters O
are O
not O
trainable O
in O
the O
domain O
task O
( O
EL O
) O
, O
whereas O
the O
Ô¨Åne O
- O
tuned O
setting O
allows O
BERT O
parameters O
to O
adapt O
to O
the O
domain O
task O
. O
3.2 O
EL O
model O
MD O
is O
modeled O
as O
a O
sequence O
labelling O
task O
. O
Contextualized O
embeddings O
hare O
passed O
through O
a O
feed O
- O
forward O
neural O
network O
and O
then O
softmaxed O
for O
classiÔ¨Åcation O
over O
IOB O
: O
mmd O
= O
Wmdh+bmd O
( O
1 O
) O
pmd O
= O
softmax O
( O
mmd O
) O
( O
2 O
) O
where O
bmd‚ààR3is O
the O
bias O
term O
, O
Wmd‚ààR3√óm O
is O
a O
weight O
matrix O
, O
and O
pmd‚ààR3is O
the O
predicted O
distribution O
across O
the O
{ O
I O
, O
O O
, O
B}tag O
set O
. O
The O
predicted O
tag O
is O
then O
simply O
: O
ÀÜ O
ymd= O
arg O
max O
i{pmd(i O
) O
} O
( O
3 O
) O
EDis O
modeled O
by O
Ô¨Ånding O
the O
entity O
( O
during O
inference O
this O
can O
be O
from O
either O
the O
entire O
entity O
universe O
or O
some O
candidate O
set O
) O
closest O
to O
the O
predicted O
entity O
embedding O
. O
We O
do O
this O
by O
applying O
an O
additional O
ED O
- O
speciÔ¨Åc O
feed O
- O
forward O
neural O
network O
to O
h O
: O
med O
= O
tanh(Wedh+bed O
) O
ped O
= O
s(med O
, O
E O
) O
ÀÜ O
yed= O
arg O
max O
j{ped(j)}(4 O
) O
where O
bed‚ààRdis O
the O
bias O
term O
, O
Wed‚ààRd√ómis O
a O
weight O
matrix O
, O
and O
med‚ààRdis O
the O
same O
size O
as O
the O
entity O
embedding O
and O
sis O
any O
similarity O
measure O
which O
relates O
medto O
every O
entity O
embedding O
inE. O
In O
our O
case O
, O
we O
use O
cosine O
similarity O
. O
Our638O O
4123 O
Leicester O
# O
# O
shire O
beat O
Somerset O
County O
Cricket O
Club O
[ O
CLS O
] O
[ O
SEP]BERThL O
e O
i O
c O
e O
s O
t O
e O
r O
h O
# O
# O
s O
h O
i O
r O
e O
hb O
e O
a O
t O
hS O
o O
m O
e O
r O
s O
e O
t O
hC O
o O
u O
n O
t O
y O
hC O
r O
i O
c O
k O
e O
t O
hC O
l O
u O
b O
h O
[ O
C O
L O
S O
] O
h O
[ O
S O
E O
P O
] O
B O
1622318 O
I O
1622318 O
O O
3221 O
B O
1622178 O
I O
2221 O
I O
2221 O
I O
2221 O
0 O
1223 O
Output O
Layer O
FFNMD O
hS O
o O
m O
e O
r O
s O
e O
tB O
1622178 O
FFNEDleicestershire O
county O
cricket_clubleicestershire O
county O
cricket_club O
- O
somerset O
county O
cricket_clubsomerset O
county O
cricket_clubsomerset O
county O
cricket_clubsomerset O
county O
cricket_club- O
-Figure O
1 O
: O
Architecture O
of O
the O
proposed O
model O
. O
WordPiece O
tokens O
are O
passed O
through O
BERT O
forming O
contextualized O
embeddings O
. O
Each O
contextualized O
embedding O
is O
passed O
through O
two O
task O
- O
speciÔ¨Åc O
feed O
- O
forward O
neural O
networks O
for O
MD O
and O
ED O
, O
respectively O
. O
Entity O
ID O
prediction O
on O
the O
‚Äò O
B O
‚Äô O
MD O
tag O
is O
extended O
to O
the O
entire O
mention O
span O
. O
predicted O
entity O
is O
the O
index O
of O
pedwith O
the O
highest O
similarity O
score O
. O
We O
use O
pre O
- O
trained O
entity O
embeddings O
from O
wikipedia2vec O
( O
Yamada O
et O
al O
. O
, O
2018 O
) O
, O
as O
pretraining O
optimal O
entity O
representation O
is O
beyond O
the O
scope O
of O
this O
work O
. O
Ideally O
, O
pre O
- O
trained O
entity O
embeddings O
should O
be O
from O
a O
similar O
architecture O
to O
our O
EL O
model O
, O
but O
experiments O
show O
strong O
results O
even O
if O
they O
are O
not O
. O
The O
wikipedia2vec O
entity O
embeddings O
used O
in O
our O
model O
are O
trained O
on O
the O
2018 O
Wikipedia O
with O
100dimensions O
and O
link O
graph O
support.5 O
During O
inference O
, O
after O
receiving O
results O
for O
each O
token O
from O
both O
the O
MD O
and O
ED O
tasks O
, O
the O
mention O
spans O
are O
tagged O
with O
{ O
B O
, O
I}tags O
as O
shown O
in O
Figure O
1 O
. O
For O
each O
mention O
span O
, O
the O
entity O
ID O
prediction O
of O
Ô¨Årst O
token O
represents O
the O
entire O
mention O
span O
. O
The O
remaining O
non O
- O
mention O
and O
non-Ô¨Årst O
entity O
ID O
prediction O
are O
masked O
out O
. O
Such O
behavior O
is O
facilitated O
by O
the O
training O
objective O
below O
. O
During O
training O
, O
we O
minimize O
the O
following O
multi O
- O
task O
objective O
which O
is O
inspired O
by O
Redmon O
and O
Farhadi O
( O
2017 O
) O
from O
the O
object O
detection O
domain:6 O
J(Œ∏ O
) O
= O
ŒªLmd(Œ∏ O
) O
+ O
( O
1‚àíŒª)Led(Œ∏ O
) O
( O
5 O
) O
whereLmdis O
the O
cross O
entropy O
between O
predicted O
and O
actual O
distributions O
of O
IOB O
and O
Ledis O
the O
cosine O
similarity O
between O
projected O
entity O
embeddings O
and O
actual O
entity O
embeddings O
. O
We O
tentatively O
explored O
triplet O
loss O
and O
contrastive O
loss O
with O
some O
simple O
negative O
mining O
strategies O
for O
ED O
but O
did O
not O
observe O
signiÔ¨Åcant O
gains O
in O
performance O
. O
The O
two O
loss O
functions O
are O
weighted O
by O
5https://wikipedia2vec.github.io/wikipedia2vec/pretrained/ O
6Similar O
to O
EL O
, O
object O
detection O
has O
two O
sub O
- O
tasks O
: O
locating O
bounding O
boxes O
and O
identifying O
objects O
in O
each O
box.a O
hyperparameter O
Œª(in O
our O
case O
Œª= O
0.1 O
) O
. O
Note O
thatLmdis O
calculated O
for O
all O
non O
- O
pad O
head O
WordPiece O
tokens O
butLedis O
calculated O
only O
for O
the O
Ô¨Årst O
WordPiece O
token O
of O
every O
labeled O
entity O
mention O
with O
a O
linkable O
and O
valid O
entity O
ID O
label O
. O
4 O
Experiments O
4.1 O
Dataset O
and O
Performance O
Metrics O
We O
train O
and O
evaluate O
our O
model O
on O
the O
widely O
used O
AIDA O
/ O
CoNLL O
dataset O
( O
Hoffart O
et O
al O
. O
, O
2011 O
) O
. O
It O
is O
a O
collection O
of O
news O
articles O
from O
Thomson O
Reuters O
, O
which O
is O
split O
into O
training O
, O
validation O
( O
testa O
) O
and O
test O
( O
testb O
) O
sets O
. O
Following O
convention O
, O
the O
evaluation O
metric O
is O
strong O
- O
matching O
span O
- O
level O
InKB O
micro O
and O
macro O
F1 O
score O
over O
gold O
mentions O
, O
where O
entity O
annotation O
is O
available O
( O
R O
¬®oder O
et O
al O
. O
, O
2018 O
) O
. O
Note O
that O
ED O
models O
are O
evaluated O
by O
accuracy O
metric O
while O
EL O
models O
are O
evaluated O
by O
F1 O
, O
which O
penalizes O
the O
tagging O
of O
non O
- O
mention O
spans O
as O
entity O
mentions O
. O
4.2 O
Candidate O
Sets O
All O
EL O
models O
cited O
rely O
on O
candidate O
sets O
. O
As O
for O
our O
model O
, O
mentions O
can O
be O
efÔ¨Åciently O
disambiguated O
with O
respect O
to O
the O
entire O
entity O
universe O
, O
which O
we O
take O
to O
be O
the O
one O
million O
most O
frequent O
entities O
in O
2018 O
Wikipedia O
. O
Consequently O
, O
our O
model O
can O
circumvent O
candidate O
generation O
, O
as O
well O
as O
the O
external O
knowledge O
that O
comes O
with O
it O
. O
In O
order O
to O
study O
the O
impact O
of O
candidate O
sets O
on O
our O
model O
, O
we O
apply O
candidate O
sets O
from O
Hoffart O
et O
al O
. O
( O
2011 O
) O
backed O
by O
the O
YAGO O
knowledge O
graph O
( O
Suchanek O
et O
al O
. O
, O
2007 O
) O
. O
Importantly O
, O
we O
do O
not O
arbitrarily O
limit O
the O
size O
of O
the O
candidate O
sets O
. O
4.3 O
Training O
Details O
and O
Settings O
We O
train O
the O
EL O
model O
on O
the O
training O
split O
with O
a O
batch O
size O
of O
4 O
for O
50,000 O
steps O
. O
As O
in O
the O
original O
BERT O
paper O
, O
the O
model O
is O
optimized O
by O
the O
Adam639optimizer O
( O
Kingma O
and O
Ba O
, O
2014 O
) O
with O
the O
same O
hyperparameters O
except O
the O
learning O
rate O
, O
which O
we O
set O
to O
be O
2e-5 O
. O
Training O
was O
performed O
on O
a O
Tesla O
V100 O
GPU O
. O
A O
0.1dropout O
rate O
was O
used O
on O
the O
prediction O
heads O
. O
Experiments O
are O
repeated O
three O
times O
to O
calculate O
an O
error O
range O
. O
4.4 O
Results O
Comparison O
with O
Other O
EL O
Models O
. O
We O
compare O
our O
model O
with O
six O
of O
the O
most O
recent O
, O
and O
best O
performing O
, O
EL O
models O
in O
Table O
1 O
. O
We O
study O
the O
performance O
of O
our O
model O
with O
, O
and O
without O
candidate O
sets O
( O
see O
Section O
4.2 O
) O
. O
We O
Ô¨Ånd O
that O
when O
candidate O
sets O
are O
provided O
, O
our O
model O
outperforms O
existing O
models O
by O
a O
signiÔ¨Åcant O
margin O
. O
One O
of O
the O
problems O
of O
comparing O
results O
in O
the O
EL O
and O
ED O
space O
is O
that O
candidate O
sets O
are O
usually O
paper O
- O
speciÔ¨Åc O
and O
many O
works O
suggest O
their O
own O
methodologies O
for O
generating O
them O
. O
In O
addition O
to O
using O
candidate O
sets O
from O
Hoffart O
et O
al O
. O
( O
2011 O
) O
( O
which O
makes O
us O
comparable O
to O
Kolitsas O
et O
al O
. O
( O
2018 O
) O
who O
use O
the O
same O
sets O
) O
, O
we O
impose O
no O
arbitrary O
limit O
on O
candidate O
set O
size O
. O
This O
means O
that O
many O
of O
our O
candidate O
sets O
have O
more O
than O
the O
standard O
20 O
- O
30 O
candidates O
, O
which O
are O
normally O
considered O
in O
past O
works O
. O
Without O
candidate O
sets O
our O
model O
also O
shows O
good O
results O
and O
validation O
performance O
is O
on O
par O
with O
recent O
work O
by O
Martins O
et O
al O
. O
( O
2019 O
) O
who O
used O
stack O
LSTMs O
with O
candidate O
sets O
. O
We O
improve O
upon O
work O
by O
Broscheit O
( O
2019 O
) O
who O
, O
like O
us O
, O
do O
not O
use O
candidate O
sets O
. O
We O
use O
a O
larger O
overall O
entity O
universe O
( O
1 O
M O
instead O
of O
700 O
K O
) O
. O
Interestingly O
, O
Broscheit O
( O
2019 O
) O
note O
that O
during O
their O
error O
analysis O
only O
3 O
% O
of O
wrong O
predictions O
were O
due O
to O
erroneous O
span O
detection O
. O
This O
could O
potentially O
explain O
our O
margin O
of O
improvement O
in O
the O
test O
set O
since O
our O
model O
is O
span O
- O
aware O
unlike O
theirs O
. O
For O
more O
details O
on O
the O
properties O
of O
the O
AIDA O
dataset O
we O
recommend O
Ilievski O
et O
al O
. O
( O
2018 O
) O
. O
OverÔ¨Åtting O
. O
There O
are O
considerable O
drops O
in O
performance O
between O
validation O
and O
test O
both O
when O
BERT O
is O
Ô¨Åne O
- O
tuned O
or O
Ô¨Åxed O
, O
pointing O
to O
potential O
problems O
with O
overÔ¨Åtting O
. O
Identical O
behaviour O
is O
seen O
in O
Broscheit O
( O
2019 O
) O
and O
Poerner O
et O
al O
. O
( O
2019 O
) O
, O
who O
propose O
similar O
BERT O
-based O
models O
. O
Whether O
overÔ¨Åtting O
is O
due O
to O
BERT O
or O
the O
downstream O
models O
requires O
further O
research O
. O
Even O
more O
considerable O
drops O
in O
performance O
between O
validation O
and O
test O
are O
experienced O
when O
candidates O
sets O
are O
not O
used O
and O
entities O
are O
linkedAIDA O
/ O
testa O
F1 O
( O
val O
) O
AIDA O
/ O
testb O
F1 O
( O
test O
) O
Macro O
Micro O
Macro O
Micro O
Martins O
et O
al O
. O
( O
2019 O
) O
82.8 O
85.2 O
81.2 O
81.9 O
Kolitsas O
et O
al O
. O
( O
2018 O
) O
86.6 O
89.4 O
82.6 O
82.4 O
Cao O
et O
al O
. O
( O
2018 O
) O
77.0 O
79.0 O
80.0 O
80.0 O
Nguyen O
et O
al O
. O
( O
2016 O
) O
- O
- O
- O
78.7 O
Broscheit O
( O
2019 O
) O
- O
76.5 O
- O
67.8 O
Poerner O
et O
al O
. O
( O
2019 O
) O
89.1 O
90.8 O
84.2 O
85.0 O
Fine O
- O
tuned O
BERT O
with O
candidate O
sets O
92.6¬±0.293.6¬±0.287.5¬±0.387.7¬±0.3 O
Fine O
- O
tuned O
BERT O
without O
candidate O
sets O
82.6 O
¬±0.283.5¬±0.270.7¬±0.369.4¬±0.3 O
Table O
1 O
: O
Strong O
- O
matching O
span O
- O
level O
InKB O
macro O
& O
micro O
F1 O
results O
on O
validation O
and O
test O
splits O
of O
AIDA O
/ O
CoNLL O
dataset O
. O
Note O
that O
the O
other O
models O
cited O
all O
use O
candidate O
sets O
. O
We O
run O
our O
models O
three O
times O
with O
different O
seeds O
to O
get O
bounds O
around O
our O
results O
. O
Ablation O
Validation O
F1 O
Test O
F1 O
Macro O
Micro O
Macro O
Micro O
Feature O
- O
based O
BERT O
with O
candidate O
sets O
87.1 O
¬±0.190.3¬±0.183.5¬±0.384.8¬±0.4 O
Feature O
- O
based O
BERT O
without O
candidate O
sets O
63.3 O
¬±1.164.1¬±0.257.2¬±0.254.1¬±0.3 O
With O
fasttext O
entity O
embedding O
90.4 O
91.4 O
82.8 O
82.9 O
Table O
2 O
: O
Ablation O
results O
on O
validation O
and O
test O
sets O
of O
AIDA O
/ O
CoNLL O
. O
By O
feature O
- O
based O
BERT O
we O
mean O
BERT O
which O
is O
not O
Ô¨Åne O
- O
tuned O
to O
the O
task O
. O
across O
the O
entire O
entity O
universe O
. O
We O
can O
not O
be O
sure O
whether O
these O
drops O
are O
speciÔ¨Åc O
to O
BERT O
since O
no O
non O
- O
BERT O
works O
cite O
results O
over O
the O
entire O
entity O
universe O
. O
Ablation O
Study O
. O
We O
perform O
a O
simple O
ablation O
study O
, O
the O
results O
of O
which O
are O
shown O
in O
Table O
2 O
. O
We O
note O
that O
performance O
suffers O
in O
the O
EL O
task O
when O
BERT O
is O
not O
Ô¨Åne O
- O
tuned O
but O
still O
maintains O
strong O
results O
comparable O
to O
the O
state O
- O
of O
- O
theart O
. O
Without O
Ô¨Åne O
- O
tuning O
, O
validation O
set O
performance O
decreases O
and O
becomes O
more O
comparable O
to O
test O
set O
performance O
, O
indicating O
that O
the O
Ô¨Ånetuned O
BERT O
overÔ¨Åts O
in O
such O
a O
setting O
- O
we O
Ô¨Ånd O
this O
to O
be O
an O
interesting O
future O
direction O
of O
study O
. O
Other O
Results O
. O
Finally O
, O
during O
research O
, O
we O
swapped O
the O
Wikipedia2Vec O
entities O
with O
averaged O
- O
out O
300 O
- O
dimensional O
FastText O
embeddings O
( O
Bojanowski O
et O
al O
. O
, O
2017 O
) O
to O
see O
what O
the O
impact O
of O
not O
having O
entity O
- O
speciÔ¨Åc O
embeddings O
would O
be O
. O
To O
our O
surprise O
, O
the O
model O
performs O
on O
par O
with O
existing O
results O
which O
, O
we O
think O
, O
points O
to O
a O
combination O
of O
( O
1 O
) O
BERT O
already O
having O
internal O
knowledge O
of O
entity O
- O
mentions O
given O
their O
context O
; O
and O
( O
2 O
) O
many O
AIDA O
mentions O
being O
easily O
linkable O
by O
simply O
considering O
their O
surface O
- O
form O
. O
We O
think O
this O
too O
is O
an O
interesting O
direction O
of O
future O
study O
. O
Point O
( O
2 O
) O
speciÔ¨Åcally O
points O
to O
the O
need O
for O
better O
EL O
datasets O
than O
AIDA O
, O
which O
was O
originally O
meant O
to O
be O
an O
ED O
dataset O
. O
A O
great O
study O
of O
point O
( O
1 O
) O
can O
be O
found O
in O
Poerner O
et O
al O
. O
( O
2019).6405 O
Conclusions O
and O
Future O
Work O
We O
propose O
an O
EL O
model O
that O
jointly O
learns O
the O
MD O
and O
ED O
task O
, O
achieving O
state O
- O
of O
- O
the O
- O
art O
results O
. O
We O
also O
show O
that O
training O
and O
inference O
without O
candidate O
sets O
is O
possible O
. O
We O
think O
that O
interesting O
future O
directions O
of O
study O
include O
a O
better O
understanding O
of O
how O
BERT O
already O
comprehends O
entities O
in O
text O
without O
reference O
to O
external O
entity O
embeddings O
. O
Finally O
, O
we O
think O
that O
moving O
forward O
, O
reducing O
the O
EL O
community O
‚Äôs O
dependence O
on O
candidate O
sets O
could O
be O
a O
good O
thing O
and O
requires O
more O
research O
. O
Dropping O
candidate O
sets O
could O
make O
models O
more O
easily O
comparable O
. O
Abstract O
Research O
in O
building O
intelligent O
agents O
have O
emphasized O
the O
need O
for O
understanding O
characteristic O
behavior O
of O
people O
. O
In O
order O
to O
reÔ¨Çect O
human O
- O
like O
behavior O
, O
agents O
require O
the O
capability O
to O
comprehend O
the O
context O
, O
infer O
individualized O
persona O
patterns O
and O
incrementally O
learn O
from O
experience O
. O
In O
this O
paper O
, O
we O
present O
a O
model O
called O
D O
APPER O
that O
can O
learn O
to O
embed O
persona O
from O
natural O
language O
and O
alleviate O
task O
or O
domain O
- O
speciÔ¨Åc O
data O
sparsity O
issues O
related O
to O
personas O
. O
To O
this O
end O
, O
we O
implement O
a O
text O
encoding O
strategy O
that O
leverages O
a O
pretrained O
language O
model O
and O
an O
external O
memory O
to O
produce O
domain O
- O
adapted O
persona O
representations O
. O
Further O
, O
we O
evaluate O
the O
transferability O
of O
these O
embeddings O
by O
simulating O
low O
- O
resource O
scenarios O
. O
Our O
comparative O
study O
demonstrates O
the O
capability O
of O
our O
method O
over O
other O
approaches O
towards O
learning O
rich O
transferable O
persona O
embeddings O
. O
Empirical O
evidence O
suggests O
that O
the O
learnt O
persona O
embeddings O
can O
be O
effective O
in O
downstream O
tasks O
like O
hate O
speech O
detection O
. O
1 O
Introduction O
With O
increasing O
human O
- O
machine O
hybrid O
technologies O
, O
the O
real O
- O
world O
interactions O
with O
AI O
systems O
are O
often O
stilted O
. O
This O
shortcoming O
can O
be O
attributed O
to O
the O
lack O
of O
shared O
common O
knowledge O
about O
how O
people O
will O
act O
, O
communicate O
and O
react O
under O
different O
circumstances O
. O
Several O
studies O
in O
the O
Ô¨Åeld O
of O
psychology O
( O
Goldberg O
, O
1990 O
; O
Barrick O
and O
Mount O
, O
1993 O
, O
1991 O
) O
have O
established O
the O
role O
of O
personas O
in O
governing O
how O
people O
process O
information O
, O
attend O
to O
and O
interpret O
life O
- O
experiences O
, O
and O
respond O
to O
social O
situations O
. O
SpeciÔ¨Åcally O
, O
the O
relationship O
between O
personality O
and O
natural O
language O
have O
been O
widely O
studied O
( O
Digman O
and O
TakemotoChock O
, O
1981 O
; O
Pennebaker O
et O
al O
. O
, O
2003 O
) O
. O
For O
example O
, O
a O
narcissistic O
person O
might O
make O
frequent O
use O
of O
Ô¨Årst O
- O
person O
expressions O
( O
I O
, O
me O
, O
myself O
, O
forme O
, O
etc O
. O
) O
. O
Therefore O
, O
endowing O
machines O
with O
the O
persona O
information O
can O
lead O
to O
the O
development O
of O
psychologically O
plausible O
intelligent O
systems O
. O
Though O
computational O
models O
of O
personality O
have O
generally O
followed O
prior O
psychological O
models O
or O
theories O
( O
Hjelle O
and O
Ziegler O
, O
1992 O
; O
Costa O
and O
PAUL O
, O
1996 O
) O
, O
multiple O
deÔ¨Ånitions O
of O
personas O
have O
been O
in O
use O
depending O
on O
the O
nature O
of O
the O
domain O
or O
task O
at O
hand O
. O
There O
has O
been O
considerable O
amount O
of O
interest O
in O
the O
past O
that O
used O
NLP O
tools O
to O
conduct O
persona O
analysis O
of O
Ô¨Åctional O
characters O
in O
literary O
texts O
( O
Flekova O
and O
Gurevych O
, O
2015 O
; O
Mairesse O
et O
al O
. O
, O
2007 O
) O
. O
Motivated O
by O
such O
works O
, O
we O
focus O
on O
deriving O
persona O
representations O
that O
explain O
human O
social O
behavior O
categorized O
according O
to O
their O
inÔ¨Çuences O
on O
language O
, O
conversations O
and O
actions O
in O
different O
social O
contexts O
. O
In O
this O
work O
, O
we O
deÔ¨Åne O
persona O
as O
the O
sum O
total O
of O
mental O
, O
emotional O
, O
and O
social O
characteristics O
of O
an O
individual O
( O
Soloff O
, O
1985 O
) O
. O
This O
broad O
deÔ¨Ånition O
, O
while O
basing O
on O
theoretical O
foundations O
, O
allows O
us O
to O
learn O
persona O
embeddings O
from O
annotated O
text O
that O
span O
across O
multiple O
domains O
and O
social O
contexts O
. O
Often O
these O
persona O
- O
annotated O
domain O
data O
are O
either O
too O
small O
or O
not O
representative O
of O
all O
the O
domain O
aspects O
of O
persona O
. O
Therefore O
, O
we O
address O
these O
challenges O
by O
formulating O
our O
representation O
learning O
problem O
through O
the O
lens O
of O
domain O
adaptation O
. O
We O
propose O
a O
model O
called O
DAPPER1to O
learn O
a O
domain O
- O
adapted O
persona O
embedding O
that O
promotes O
positive O
knowledge O
transfer O
across O
multiple O
text O
domains O
: O
movies O
dialogue O
, O
forum O
discussion O
posts O
and O
personal O
life O
stories O
or O
essays O
. O
Towards O
this O
goal O
, O
we O
use O
a O
pretrained O
BERTmodel O
to O
extract O
rich O
semantic O
features O
from O
text O
and O
Ô¨Ånetune O
them O
by O
introducing O
Adaptive O
Knowledge O
Transformer O
that O
serve O
as O
adaptive O
layers O
on O
top O
of O
the O
representations O
obtained O
from O
BERTmodel O
. O
1Short O
for O
Domain O
Adapted O
Pretraining O
- O
based O
PErsona O
Representation643These O
adaptive O
layers O
enrich O
the O
representations O
with O
domain O
- O
related O
persona O
knowledge O
. O
We O
explore O
variants O
of O
Transformer O
encoder O
layer O
as O
our O
adaptive O
layers O
. O
In O
our O
experiments O
, O
we O
compare O
our O
Transformer O
- O
based O
DAPPER O
model O
with O
RNNbased O
techniques O
on O
data O
from O
three O
different O
text O
domains O
. O
Finally O
, O
we O
showcase O
the O
advantages O
of O
using O
our O
representations O
in O
a O
downstream O
hate O
speech O
detection O
task O
. O
Thus O
, O
our O
contributions O
are O
as O
follows O
: O
‚Ä¢We O
propose O
a O
model O
called O
DAPPER O
that O
integrates O
pretrained O
language O
model O
with O
adaptive O
knowledge O
Transformer O
layers O
to O
learn O
better O
domain O
- O
adapted O
representation O
of O
personas O
. O
‚Ä¢We O
evaluate O
our O
model O
on O
texts O
from O
multiple O
text O
domains O
: O
Movies O
dialogue O
( O
Chu O
et O
al O
. O
, O
2018 O
) O
, O
forum O
discussion O
posts O
and O
personal O
essays O
or O
life O
stories O
( O
Pennebaker O
and O
King O
, O
1999 O
) O
. O
Our O
DAPPER O
model O
outperforms O
the O
baseline O
models O
signiÔ¨Åcantly O
across O
these O
domains O
. O
‚Ä¢We O
determine O
how O
our O
model O
performs O
in O
domains O
with O
limited O
labeled O
data O
by O
simulating O
such O
scenarios O
within O
our O
existing O
datasets O
. O
We O
show O
that O
our O
domain O
- O
knowledge O
enriched O
persona O
representations O
are O
capable O
of O
adapting O
to O
such O
domains O
. O
Further O
, O
they O
show O
promise O
in O
an O
unrelated O
downstream O
hate O
speech O
detection O
task O
. O
2 O
Related O
Work O
Considering O
that O
personality O
compels O
a O
tendency O
on O
a O
lot O
of O
aspects O
of O
human O
behavior O
, O
there O
have O
been O
several O
studies O
intended O
to O
model O
personality O
traits O
from O
text O
. O
An O
earlier O
work O
by O
( O
Pennebaker O
and O
King O
, O
1999 O
) O
compiled O
stream O
- O
of O
- O
consciousness O
essay O
dataset O
for O
an O
automated O
personality O
detection O
task O
. O
Since O
the O
Five O
Factor O
Model O
is O
widely O
accepted O
, O
several O
attempts O
have O
been O
made O
to O
detect O
personality O
from O
these O
essays O
including O
LIWC O
features O
or O
deep O
learning O
techniques O
( O
Majumder O
et O
al O
. O
, O
2017 O
; O
Mairesse O
et O
al O
. O
, O
2007 O
) O
. O
( O
Chaudhary O
et O
al O
. O
, O
2013 O
) O
compared O
different O
machine O
learning O
models O
to O
predict O
Myers O
- O
Brigg O
Type O
Indicator O
. O
Another O
line O
of O
work O
( O
Liu O
et O
al O
. O
, O
2016 O
) O
related O
to O
personas O
focused O
on O
developing O
a O
language O
independent O
and O
compositional O
model O
for O
personality O
trait O
recognition O
for O
short O
tweets O
. O
Additionally O
, O
there O
have O
beenDatasets O
Label O
Type O
Size O
# O
Categories O
Personal O
Essays O
Big O
- O
Five O
2,400 O
5 O
Forum O
Posts O
MBTI O
52,648 O
16 O
Movies O
Dialogue O
Tropes O
17,342 O
72 O
Table O
1 O
: O
Details O
of O
the O
datasets O
from O
different O
domains O
other O
efforts O
that O
model O
personas O
of O
movie O
characters O
and O
incorporate O
speaker O
persona O
in O
dialogue O
models O
based O
on O
speaking O
style O
characterized O
by O
natural O
language O
sentences O
( O
Bamman O
et O
al O
. O
, O
2013 O
) O
. O
We O
observe O
that O
most O
of O
these O
works O
use O
different O
theories O
and O
deÔ¨Ånitions O
for O
modeling O
personas O
‚Äì O
ranging O
from O
widely O
accepted O
psychological O
tests O
to O
simple O
emotion O
states O
of O
people O
as O
means O
of O
ascertaining O
personality O
( O
Shuster O
et O
al O
. O
, O
2018 O
) O
. O
However O
, O
there O
is O
very O
limited O
work O
( O
Li O
et O
al O
. O
, O
2016 O
; O
Chu O
et O
al O
. O
, O
2018 O
) O
focusing O
on O
persona O
embeddings O
that O
can O
be O
adapted O
to O
different O
domains O
. O
In O
this O
work O
, O
our O
goal O
is O
to O
produce O
general O
purpose O
persona O
embeddings O
computed O
using O
texts O
from O
various O
domains O
. O
3 O
Datasets O
Towards O
learning O
a O
domain O
- O
adapted O
persona O
embedding O
, O
we O
aggregate O
different O
forms O
of O
text O
data O
: O
( O
a O
) O
personal O
stories O
/ O
essays O
, O
( O
b O
) O
dialogues O
and O
( O
c O
) O
discussion O
forum O
posts O
. O
Each O
of O
these O
datasets O
have O
distinct O
persona O
categories O
. O
Table O
1 O
shows O
the O
details O
of O
the O
dataset O
. O
We O
elaborate O
them O
in O
the O
following O
sections O
. O
3.1 O
Personal O
Essays O
Corpus O
Personal O
stories O
or O
reÔ¨Çections O
explain O
important O
parts O
of O
one O
‚Äôs O
personality O
including O
their O
goals O
and O
values O
( O
McAdams O
and O
Manczak O
, O
2015 O
) O
. O
For O
our O
purpose O
, O
we O
make O
use O
of O
personal O
essays O
originally O
from O
Pennebaker O
et O
al O
. O
( O
Pennebaker O
and O
King O
, O
1999 O
) O
. O
This O
corpus O
consists O
of O
2400 O
essays O
collected O
between O
1997 O
and O
2004 O
. O
Students O
who O
produced O
these O
texts O
were O
assessed O
based O
on O
Big O
Five2Questionnaires O
. O
To O
obtain O
labels O
from O
the O
self O
- O
assessments O
, O
z O
- O
scores O
were O
computed O
from O
them O
by O
( O
Mairesse O
et O
al O
. O
, O
2007 O
) O
and O
the O
resulting O
scores O
were O
discretized O
to O
categories O
by O
( O
Celli O
et O
al O
. O
, O
2013 O
) O
. O
3.2 O
Forum O
Posts O
Corpus O
One O
of O
the O
most O
commonly O
administered O
psychological O
tests O
is O
Myers O
- O
Briggs O
Type O
Indicator O
2https://en.wikipedia.org/wiki/Big O
Five O
personality O
traits644(MBTI3 O
) O
. O
Based O
on O
Jung O
‚Äôs O
theory O
of O
psychological O
types O
, O
16 O
personality O
types O
were O
recognized O
as O
useful O
reference O
points O
to O
understand O
one O
‚Äôs O
personality O
. O
In O
this O
work O
, O
we O
collect O
a O
text O
corpus O
from O
a O
discussion O
forum O
called O
PersonalityCafe4 O
, O
that O
has O
dedicated O
communities O
for O
each O
of O
the O
16MBTI O
personality O
types O
. O
The O
members O
of O
these O
communities O
generally O
self O
- O
identify O
with O
the O
corresponding O
personality O
type O
and O
post O
various O
forms O
of O
text O
including O
those O
written O
in O
a O
stream O
- O
of O
- O
consciousness O
style O
. O
To O
obtain O
these O
posts O
, O
we O
crawled O
speciÔ¨Åc O
sections O
of O
the O
forum O
related O
to O
each O
personality O
type O
. O
Further O
, O
we O
Ô¨Ålter O
the O
posts O
that O
are O
too O
short O
( O
i.e. O
less O
than O
75characters O
in O
length O
) O
and O
replace O
explicit O
mentions O
of O
their O
personality O
type O
in O
the O
text O
with O
markers O
. O
Though O
the O
prevalence O
of O
MBTI O
personality O
types O
in O
general O
population O
is O
highly O
disproportional O
, O
the O
forum O
posts O
might O
not O
always O
reÔ¨Çect O
that O
distribution O
. O
Therefore O
, O
we O
create O
a O
more O
or O
less O
balanced O
dataset O
to O
avoid O
any O
skewed O
representation O
of O
personality O
types O
. O
In O
total O
, O
our O
Forum O
Posts O
dataset O
contains O
52,648 O
posts O
. O
The O
dataset O
will O
be O
made O
publicly O
available O
. O
3.3 O
Movies O
Dialogue O
Corpus O
In O
a O
contrast O
to O
prior O
datasets O
which O
has O
welldeÔ¨Åned O
persona O
categories O
based O
on O
personality O
tests O
/ O
theories O
, O
we O
use O
a O
dataset O
that O
views O
character O
tropes O
as O
a O
proxy O
for O
persona O
labels O
. O
In O
the O
context O
of O
Ô¨Åction O
, O
character O
trope O
refers O
to O
the O
aspects O
of O
a O
story O
that O
conveys O
information O
about O
a O
character O
including O
its O
role O
in O
the O
plot O
, O
personality O
, O
motivations O
and O
perceived O
behavior O
. O
Thus O
, O
we O
utilize O
the O
IMDB O
dialogue O
snippet O
dataset5(Chu O
et O
al O
. O
, O
2018 O
) O
containing O
utterances O
of O
characters O
in O
movies O
obtained O
from O
CMU O
Movie O
Summary O
datasets O
( O
Bamman O
et O
al O
. O
, O
2013 O
) O
. O
Each O
of O
the O
433characters O
in O
the O
dataset O
is O
associated O
with O
one O
among O
72 O
different O
trope O
labels O
. O
Additionally O
, O
we O
collect O
more O
personarelated O
domain O
- O
speciÔ¨Åc O
knowledge O
from O
TVTropes O
. O
TVTropes O
is O
a O
wiki O
that O
collects O
document O
descriptions O
about O
plot O
conventions O
and O
devices O
. O
It O
also O
contains O
useful O
notes O
describing O
MBTI6and O
Big O
Five7personality O
traits O
with O
references O
to O
character O
tropes O
that O
closely O
relate O
to O
each O
of O
those O
categories O
. O
3https://en.wikipedia.org/wiki/Myers‚ÄìBriggs O
Type O
Indicator O
4https://www.personalitycafe.com O
5https://pralav.github.io/emnlp O
personas/ O
6https://tvtropes.org/pmwiki/pmwiki.php/UsefulNotes/ O
MyersBriggs O
7https://tvtropes.org/pmwiki/pmwiki.php/UsefulNotes/ O
BigFivePersonalityTraitsFigure O
1 O
displays O
samples O
from O
the O
datasets O
used O
in O
this O
work O
. O
Using O
these O
datasets O
and O
persona O
category O
knowledge O
, O
we O
focus O
on O
developing O
domainadapted O
persona O
embeddings O
. O
# O
13 O
‚Ä¢ O
May O
15 O
, O
2011 O
I O
‚Äôm O
tired O
of O
people O
making O
ad O
hominem O
attacks O
. O
I O
‚Äôm O
tired O
of O
people O
thinking O
they O
‚Äôre O
better O
than O
me O
because O
I O
‚Äôm O
an O
F. O
I O
still O
do O
n‚Äôt O
believe O
that O
Americans O
care O
as O
much O
about O
‚Äú O
immigration O
status O
‚Äù O
as O
they O
care O
about O
the O
color O
of O
your O
skin O
. O
Forum O
	 O
Posts O
	 O
Corpus O
: O
	 O
PersonalityCafe O
	 O
‚Äî O
	 O
ISFJ O
Stacks O
Edwards O
: O
What O
time O
is O
it O
? O
Tommy O
DeVito O
: O
It O
‚Äôs O
eleven O
thirty O
, O
we O
‚Äôre O
supposed O
to O
be O
there O
by O
nine O
. O
Stacks O
Edwards O
: O
Be O
ready O
in O
a O
minute O
. O
Tommy O
DeVito O
: O
Yeah O
, O
you O
were O
always O
fuckin O
‚Äô O
late O
, O
you O
were O
late O
for O
your O
own O
fuckin O
‚Äô O
funeral O
. O
  O
[ O
shoots O
	 O
him]Movie O
	 O
Dialogue O
	 O
Corpus O
: O
	 O
IMDB O
	 O
Dialogue O
	 O
Snippet O
‚Ä¶ O
. O
I O
have O
some O
really O
random O
thoughts O
. O
I O
want O
the O
best O
things O
. O
But O
I O
fear O
that O
I O
want O
too O
much O
! O
What O
if O
I O
fall O
Ô¨Çat O
on O
my O
face O
and O
do O
n‚Äôt O
amount O
to O
anything O
. O
But O
I O
feel O
like O
I O
was O
born O
to O
do O
BIG O
things O
on O
this O
earth O
. O
But O
who O
knows O
‚Ä¶ O
There O
is O
this O
Persian O
party O
today O
. O
My O
neck O
hurts O
‚Ä¶ O
.Personal O
	 O
Essays O
	 O
Corpus O
: O
	 O
PersonalityCafe O
	 O
‚Äî O
	 O
Extrovert O
Figure O
1 O
: O
Samples O
from O
different O
datasets O
used O
for O
learning O
domain O
- O
adapted O
persona O
embeddings O
. O
4 O
Problem O
Setup O
The O
overall O
goal O
of O
our O
model O
is O
to O
learn O
persona O
embeddings O
using O
documents O
from O
different O
domainsD O
: O
dialogue O
utterances O
, O
forum O
posts O
and O
personal O
essays O
. O
This O
persona O
representation O
learning O
problem O
is O
formulated O
as O
a O
supervised O
classiÔ¨Åcation O
problem O
. O
Let O
us O
denote O
the O
/u1D456 O
/ O
u1D461 O
/ O
uni210Einput O
document O
asI(/u1D456)=[I(/u1D456 O
) O
1,I(/u1D456 O
) O
2, O
... O
,I(/u1D456 O
) O
|/u1D43C| O
] O
. O
Here O
, O
a O
document O
refers O
to O
a O
list O
of O
sentences O
from O
the O
personal O
essays O
or O
forum O
Posts O
corpus O
and O
dialogue O
snippets O
in O
case O
of O
movies O
dialogue O
corpus O
( O
explained O
in O
Section O
3 O
) O
. O
Each O
input O
I(/u1D456)in O
our O
data O
is O
associated O
with O
their O
domain O
- O
speciÔ¨Åc O
persona O
label O
/u1D45D(/u1D456 O
) O
/u1D458 O
where O
/ O
u1D458‚àà{1,2, O
.. O
|D| O
} O
, O
/u1D45D(/u1D456 O
) O
/u1D458‚ààY O
/ O
u1D458andY O
/ O
u1D458is O
the O
personal O
categories O
related O
to O
the O
/u1D458 O
/ O
u1D461 O
/ O
uni210E O
- O
domain O
. O
5 O
Proposed O
Model O
In O
this O
work O
, O
we O
explore O
the O
idea O
of O
leveraging O
a O
pretrained O
BERTmodel O
towards O
our O
goal O
of O
learning O
domain O
- O
adaptive O
persona O
embeddings O
. O
Instead O
of O
relying O
only O
on O
the O
domain O
- O
speciÔ¨Åc O
training O
data O
, O
we O
allow O
additional O
domain O
knowledge O
to O
be O
injected O
into O
our O
model O
using O
an O
external O
memory O
. O
Our O
model O
architecture O
is O
illustrated O
in O
Figure O
2 O
. O
5.1 O
Input O
Processing O
The O
input O
to O
our O
DAPPER O
model O
can O
take O
different O
forms O
depending O
on O
the O
domain O
under O
considera-645Persona O
Repr O
esentationsFeed O
- O
ForwardSelf O
- O
Attention O
KnowledgeAttentionB O
ER O
TInput O
	 O
Pr O
ocessor O
K O
VMS O
K O
VDomain O
	 O
KnowledgeN O
adapt O
	  O
x O
Adaptive O
	 O
Knowledge O
Memory[CLS O
] O
snippet1 O
[ O
SEP O
] O
[ O
CLS O
] O
dialog O
snippet2 O
[ O
SEP O
] O
Es O
n O
i O
p O
p O
e O
t O
1E O
[ O
SEP O
] O
E O
[ O
CLS O
] O
E O
dialog O
Es O
n O
i O
p O
p O
e O
t O
2E O
[ O
SEP O
] O
EA O
EA O
EAEA O
EB O
EB O
EBEBInput O
	 O
with O
Special O
	 O
T O
okens O
T O
oken O
	 O
Embeddings O
Interval O
	 O
Segment O
Embeddings O
Position O
	 O
Embeddingsdialog O
E O
[ O
CLS O
] O
E O
dialog O
E O
1 O
E O
2 O
E3E O
4 O
E O
5 O
E O
5 O
E O
6 O
E7E O
8Figure O
2 O
: O
Illustration O
of O
our O
D O
APPER O
model O
. O
tion O
: O
( O
a O
) O
long O
essays O
or O
forum O
posts O
containing O
several O
sentences O
representing O
personal O
details O
, O
goals O
and O
values O
, O
and O
( O
b O
) O
dialogue O
snippets O
having O
character O
‚Äôs O
own O
lines O
and O
additional O
contextual O
information O
such O
as O
narrator O
or O
interacting O
characters O
‚Äô O
lines O
. O
The O
varying O
nature O
of O
the O
data O
from O
these O
domains O
can O
pose O
a O
challenge O
to O
our O
modeling O
objective O
. O
In O
order O
to O
represent O
data O
from O
these O
domains O
, O
we O
deÔ¨Åne O
the O
following O
procedure O
: O
‚Ä¢For O
Personal O
Essays O
and O
Forum O
Posts O
Corpus O
, O
we O
insert O
a O
special O
[ O
/u1D436 O
/ O
u1D43F O
/ O
u1D446]token O
at O
the O
beginning O
of O
each O
sentence O
/u1D460 O
/ O
u1D457 O
in O
an O
essay O
or O
post O
with O
an O
intention O
that O
each O
[ O
/u1D436 O
/ O
u1D43F O
/ O
u1D446]token O
will O
accumulate O
the O
features O
of O
the O
tokens O
following O
it O
. O
‚Ä¢For O
Dialogue O
Corpus O
, O
we O
introduce O
a O
[ O
/u1D436 O
/ O
u1D43F O
/ O
u1D446 O
] O
before O
every O
dialogue O
snippet O
/u1D451 O
/ O
u1D457while O
the O
character O
‚Äôs O
own O
lines O
and O
additional O
context O
are O
separated O
by O
a[/u1D446 O
/ O
u1D438 O
/ O
u1D443]token O
. O
‚Ä¢Next O
, O
we O
apply O
interval O
segment O
embeddings O
, O
/u1D438 O
/ O
u1D434or O
/ O
u1D438 O
/ O
u1D435 O
, O
to O
distinguish O
sentences O
or O
dialogue O
snippets O
in O
our O
data O
. O
This O
is O
done O
by O
alternating O
assignments O
between O
two O
consecutive O
sentences O
or O
dialogue O
snippets O
. O
For O
example O
, O
we O
would O
assign[/u1D438 O
/ O
u1D434,/u1D438 O
/ O
u1D435,/u1D438 O
/ O
u1D434,/u1D438 O
/ O
u1D435]to O
a O
list O
of O
dialogue O
snippets O
denoted O
as O
[ O
/u1D4511,/u1D4512,/u1D4513,/u1D4514 O
] O
. O
We O
also O
incorporate O
position O
embeddings O
into O
ourinput O
data O
processing O
step O
. O
Thus O
, O
we O
obtain O
a O
uniform O
way O
of O
representing O
our O
inputs O
texts O
from O
different O
domains O
. O
This O
allows O
us O
to O
hierarchically O
learn O
abstract O
persona O
representations O
. O
5.2 O
Encoder O
Our O
input O
document O
I(/u1D456)is O
passed O
to O
our O
input O
processing O
module O
/u1D453I O
( O
¬∑ O
) O
. O
The O
output O
of O
this O
module O
is O
a O
document O
representation O
augmented O
with O
special O
tokens O
and O
processed O
with O
interval O
segment O
and O
position O
embeddings O
. O
The O
processed O
input O
is O
passed O
to O
the O
pretrained O
BERT O
model O
. O
Formally O
, O
this O
is O
computed O
as O
: O
/u1D43B(/u1D456)=BERT(/u1D453I(I(/u1D456 O
) O
) O
) O
( O
1 O
) O
where O
/ O
u1D453Iis O
the O
input O
processing O
function O
, O
/u1D43B(/u1D456 O
) O
contains O
contextualized O
embeddings O
related O
to O
each O
token O
in O
the O
processed O
input O
document O
. O
We O
obtain O
/u1D457 O
/ O
u1D461 O
/ O
uni210Esentence O
or O
snippet O
embeddings O
by O
extracting O
the O
corresponding O
vector O
of O
/u1D457 O
/ O
u1D461 O
/ O
uni210E[/u1D436 O
/ O
u1D43F O
/ O
u1D446]token O
from O
the O
topmost O
BERTlayer O
. O
We O
denote O
this O
as O
/u1D445(/u1D456)‚àà O
R|/u1D43C|√ó/u1D451 O
/ O
uni210E,/u1D451 O
/ O
uni210Eis O
the O
set O
to O
the O
hidden O
dimensions O
of O
the O
B O
ERTmodel O
. O
5.3 O
Adaptive O
Knowledge O
Transformer O
Inspired O
by O
a O
prior O
work O
by O
( O
Miller O
et O
al O
. O
, O
2016 O
; O
Zhang O
et O
al O
. O
, O
2017 O
) O
, O
we O
integrate O
an O
external O
memory O
module O
with O
the O
Transformer O
architecture O
and O
refer O
it O
as O
Adaptive O
Knowledge O
Transformer O
( O
AKT).646This O
component O
aids O
to O
create O
persistent O
latent O
embeddings O
related O
to O
persona O
categories O
and O
further O
accumulate O
more O
knowledge O
as O
we O
process O
data O
from O
new O
domains O
. O
We O
conceptualize O
this O
component O
to O
be O
composed O
of O
: O
( O
a O
) O
a O
Key O
- O
Value O
Memory O
Store O
( O
KVMS O
) O
that O
speciÔ¨Åcally O
facilitates O
adaptivity O
to O
new O
domains O
or O
data O
( O
b O
) O
Transformerbased O
adaptive O
layers O
that O
attends O
over O
the O
contents O
of O
the O
memory O
to O
enrich O
the O
representation O
with O
persona O
- O
related O
domain O
knowledge O
. O
By O
feeding O
the O
computed O
/ O
u1D445(/u1D456)into O
our O
AKT O
, O
we O
obtain O
domainknowledge O
enriched O
persona O
embeddings O
. O
This O
is O
given O
as O
: O
P(/u1D456)=AKT(/u1D445(/u1D456 O
) O
) O
( O
2 O
) O
5.3.1 O
K O
VMS O
: O
Key O
- O
Value O
Memory O
Store O
OurKVMS O
module O
consists O
of O
a O
mutable O
key O
matrix O
( O
K O
‚àà O
R O
/ O
u1D441 O
/ O
u1D440√ó/u1D451 O
/ O
u1D43E O
) O
that O
accumulates O
personarelated O
knowledge O
across O
multiple O
domains O
and O
a O
non O
- O
updatable O
value O
matrix O
( O
V‚ààR O
/ O
u1D441 O
/ O
u1D440√ó/u1D451 O
/ O
u1D449 O
) O
containing O
a O
learnable O
persona O
category O
embedding O
. O
The O
key O
matrix O
, O
K O
, O
is O
initialized O
with O
representations O
of O
text O
descriptions O
of O
character O
tropes O
, O
MBTI O
types O
and O
Big O
- O
Five O
traits O
collected O
from O
TVTropes O
wiki O
( O
explained O
in O
3.3 O
) O
while O
the O
value O
matrix O
, O
V O
, O
is O
set O
to O
their O
corresponding O
learnable O
persona O
category O
embeddings O
. O
We O
feed O
the O
text O
descriptions O
through O
the O
input O
processing O
model O
and O
compute O
the O
sum O
of O
the O
sentence O
embeddings O
obtained O
from O
the O
topmost O
layer O
of O
B O
ERT O
. O
5.3.2 O
Knowledge O
- O
Attention O
Conventionally O
, O
a O
Transformer O
encoder O
layer O
consists O
of O
two O
sub O
- O
layers O
: O
( O
a O
) O
a O
multi O
- O
headed O
selfattention O
network O
and O
( O
b O
) O
a O
point O
- O
wise O
fullyconnected O
network O
. O
Each O
sub O
- O
layer O
has O
a O
residual O
connection O
followed O
by O
layer O
normalization O
. O
For O
the O
sake O
of O
brevity O
, O
we O
avoid O
the O
residual O
connections O
and O
layer O
normalization O
functions O
in O
our O
model O
illustration O
( O
Figure O
2 O
) O
and O
explanation O
. O
Our O
Transformer O
- O
based O
adaptive O
layers O
contain O
an O
additional O
sub O
- O
layer O
to O
integrate O
the O
personarelevant O
domain O
knowledge O
into O
the O
contextual O
representation O
obtained O
from O
the O
encoder O
. O
We O
refer O
to O
this O
sub O
- O
layer O
as O
Knowledge O
- O
Attention O
. O
This O
is O
Ô¨Åne O
- O
tuned O
using O
domain O
- O
speciÔ¨Åc O
categories O
based O
on O
a O
supervised O
classiÔ¨Åcation O
objective O
. O
The O
steps O
involved O
in O
Transformer O
adaptive O
layers O
are O
givenas O
follows O
: O
/u1D444(/u1D45B)=MHA(/u1D436(/u1D45B‚àí1),/u1D436(/u1D45B‚àí1),/u1D436(/u1D45B‚àí1))(3 O
) O
/u1D434(/u1D45B)=MHA(/u1D444(/u1D45B),K O
, O
V O
) O
( O
4 O
) O
/u1D436(/u1D45B)=FFN(/u1D434(/u1D45B O
) O
) O
( O
5 O
) O
P(/u1D456)=/u1D436 O
/ O
u1D441 O
/ O
u1D44E O
/ O
u1D451 O
/ O
u1D44E O
/ O
u1D45D O
/ O
u1D461 O
( O
6 O
) O
where O
MHAis O
a O
multi O
- O
head O
attention O
function O
as O
explained O
in O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
/u1D45B= O
{ O
1,2, O
.. O
,/u1D441 O
/ O
u1D44E O
/ O
u1D451 O
/ O
u1D44E O
/ O
u1D45D O
/ O
u1D461},/u1D436(0)=/u1D445(/u1D456),/u1D436(/u1D45B‚àí1)is O
the O
output O
from O
the O
previous O
Transformer O
layer O
, O
/u1D434(/u1D45B O
) O
is O
the O
output O
from O
the O
knowledge O
- O
attention O
sublayer O
. O
Our O
knowledge O
- O
attention O
mechanism O
identiÔ¨Åes O
the O
most O
correlated O
and O
relevant O
knowledge O
from O
the O
KVMS O
component O
with O
respect O
to O
the O
input O
document O
embeddings O
. O
The O
resulting O
domain O
knowledge O
- O
enhanced O
representations O
are O
fed O
to O
the O
point O
- O
wise O
feed O
- O
forward O
sub O
- O
layer O
( O
FFN O
) O
. O
We O
stack O
such O
adaptive O
layers O
on O
top O
of O
each O
other O
and O
the O
output O
from O
/u1D441 O
/ O
u1D461 O
/ O
uni210E O
/u1D44E O
/ O
u1D451 O
/ O
u1D44E O
/ O
u1D45D O
/ O
u1D461layer O
is O
our O
Ô¨Ånal O
domainadapted O
persona O
representation O
, O
P(/u1D456 O
) O
. O
5.3.3 O
Memory O
Update O
Intuitively O
, O
accumulation O
of O
persona O
- O
related O
knowledge O
extracted O
from O
the O
training O
documents O
into O
our O
memory O
store O
can O
enhance O
the O
quality O
of O
the O
learned O
persona O
embeddings O
. O
Therefore O
, O
we O
perform O
a O
memory O
update O
operation O
on O
selective O
rows O
in O
the O
key O
matrixKbased O
on O
the O
persona O
- O
related O
features O
derived O
from O
the O
input O
document O
and O
its O
corresponding O
ground O
truth O
persona O
labels O
. O
The O
update O
step O
is O
deÔ¨Åned O
as O
follows O
: O
/u1D706=/u1D70E(/u1D44A O
/ O
u1D458K[/u1D454 O
/ O
u1D457]+/u1D44A O
/ O
u1D45F O
/ O
u1D719(P(/u1D456 O
) O
) O
( O
7 O
) O
K[/u1D454 O
/ O
u1D457]=/u1D706‚äôK[/u1D454 O
/ O
u1D457]+(1‚àí/u1D706)‚äô/u1D719(P(/u1D456))(8 O
) O
where O
/ O
u1D454 O
/ O
u1D457refers O
to O
the O
indices O
of O
the O
rows O
in O
KVMS O
containing O
knowledge O
about O
ground O
truth O
persona O
label O
/ O
u1D45D(/u1D456 O
) O
/u1D458,/u1D719is O
aggregation O
function O
that O
compresses O
the O
information O
from O
P(/u1D456)into O
a O
single O
vector O
. O
We O
Ô¨Ånd O
from O
preliminary O
experiments O
that O
the O
mean[/u1D436 O
/ O
u1D43F O
/ O
u1D446]token O
embedding O
serves O
as O
an O
effective O
alternative O
to O
computing O
an O
average O
embedding O
related O
to O
the O
tokens O
in O
the O
input O
document O
. O
5.4 O
Training O
Objective O
Our O
model O
learns O
persona O
embeddings O
using O
a O
supervised O
classiÔ¨Åcation O
objective O
. O
We O
feed O
the O
output O
of O
the O
aggregation O
function O
/u1D719to O
a O
domainspeciÔ¨Åc O
softmax O
layer O
to O
get O
/u1D45E O
, O
where O
/ O
u1D45E=647 O
/ O
u1D460 O
/ O
u1D45C O
/ O
u1D453 O
/ O
u1D461 O
/ O
u1D45A O
/ O
u1D44E O
/ O
u1D465(/u1D453 O
/ O
u1D45E(/u1D719(P(/u1D456 O
) O
) O
) O
) O
. O
Note O
that O
the O
categories O
vary O
across O
each O
domain O
. O
L O
/ O
u1D436 O
/ O
u1D438=/u1D441 O
/ O
u1D458 O
/ O
summationdisplay.1 O
/u1D457=1‚àí/u1D45D O
/ O
u1D457 O
/ O
u1D459 O
/ O
u1D45C O
/ O
u1D454(/u1D45E O
/ O
u1D457 O
) O
( O
9 O
) O
L O
/ O
u1D44E O
/ O
u1D461 O
/ O
u1D461 O
/ O
u1D45B=1 O
/u1D440 O
/ O
u1D440 O
/ O
summationdisplay.1 O
/u1D457=1‚àí/u1D459 O
/ O
u1D45C O
/ O
u1D454(/u1D45F O
/ O
u1D457[/u1D454 O
/ O
u1D457 O
] O
) O
( O
10 O
) O
L=/u1D6FC1L O
/ O
u1D436 O
/ O
u1D438+/u1D6FC2L O
/ O
u1D44E O
/ O
u1D461 O
/ O
u1D461 O
/ O
u1D45B O
( O
11 O
) O
whereL O
/ O
u1D436 O
/ O
u1D438is O
the O
cross O
- O
entropy O
loss O
, O
/u1D6FC1,/u1D6FC2are O
learnable O
parameters O
, O
/u1D45D O
/ O
u1D457‚àà O
{ O
0,1}denotes O
the O
ground O
- O
truth O
label O
that O
reÔ¨Çects O
if O
the O
input O
document O
belongs O
to O
/u1D457 O
/ O
u1D461 O
/ O
uni210Epersona O
category O
, O
L O
/ O
u1D44E O
/ O
u1D461 O
/ O
u1D461 O
/ O
u1D45Bis O
the O
attention O
loss O
that O
promotes O
focus O
on O
rows O
with O
ground O
truth O
persona O
, O
/u1D45F O
/ O
u1D457[/u1D454 O
/ O
u1D457]is O
the O
attention O
score O
for O
the O
row O
inKreÔ¨Çecting O
/ O
u1D45D O
/ O
u1D456 O
/u1D458 O
‚Äôs O
knowledge O
. O
6 O
Experiments O
In O
this O
section O
, O
we O
describe O
the O
various O
evaluations O
settings O
: O
datasets O
, O
baselines O
, O
our O
model O
variants O
, O
modes O
and O
metrics O
. O
Our O
experiments O
are O
designed O
to O
study O
the O
following O
research O
questions O
: O
RQ1 O
: O
How O
well O
does O
our O
DAPPER O
model O
perform O
in O
comparison O
to O
baselines O
and O
its O
variants O
on O
domain O
- O
speciÔ¨Åc O
persona O
classiÔ¨Åcation O
task O
? O
RQ2 O
: O
Is O
our O
model O
capable O
of O
adapting O
to O
new O
domains O
with O
limited O
labeled O
data O
? O
RQ3 O
: O
How O
good O
are O
the O
learned O
persona O
embeddings O
? O
Do O
they O
exhibit O
transfer O
capability O
to O
a O
downstream O
task O
? O
6.1 O
Dataset O
Preparation O
We O
evaluate O
our O
models O
using O
persona O
- O
related O
datasets O
from O
different O
domains O
: O
movies O
dialogue O
, O
forum O
posts O
and O
personal O
essays O
as O
explained O
in O
Section O
3 O
. O
Using O
a O
70 O
- O
10 O
- O
20 O
split O
, O
we O
divide O
our O
persona O
dataset O
associated O
with O
each O
domain O
into O
training O
, O
validation O
and O
test O
sets O
. O
6.2 O
Baselines O
& O
Model O
Variants O
( O
RQ1 O
) O
Since O
we O
collect O
persona O
datasets O
from O
different O
domains O
, O
we O
also O
compare O
our O
model O
‚Äôs O
performance O
to O
domain O
- O
speciÔ¨Åc O
baseline O
methods O
. O
All O
these O
methods O
are O
enlisted O
as O
follows O
: O
‚Ä¢AFF2VEC(Khosla O
et O
al O
. O
, O
2018 O
) O
is O
a O
method O
for O
enriched O
word O
embeddings O
that O
are O
representative O
of O
affective O
interpretations O
of O
words O
. O
‚Ä¢CNN(Kim O
, O
2014 O
) O
is O
a O
single O
- O
layer O
CNNwhere O
the O
input O
document O
is O
passed O
in O
entirety O
without O
any O
additional O
knowledge O
. O
For O
PersonalEssays O
corpus O
, O
we O
report O
the O
best O
results O
from O
( O
Majumder O
et O
al O
. O
, O
2017 O
) O
as O
they O
use O
additional O
features O
to O
improve O
persona O
classiÔ¨Åcation O
task O
. O
‚Ä¢AMN(Chu O
et O
al O
. O
, O
2018 O
) O
learns O
persona O
embeddings O
from O
movies O
dialogue O
using O
a O
multilevel O
attention O
mechanism O
augmented O
with O
prior O
knowledge O
about O
persona O
categories O
. O
Note O
that O
this O
model O
is O
one O
of O
the O
closest O
relevant O
work O
to O
our O
model O
. O
For O
movies O
dialogue O
corpus O
, O
we O
report O
scores O
only O
for O
the O
best O
performing O
conÔ¨Åguration O
, O
i.e. O
, O
/u1D45B O
/ O
u1D451 O
/ O
u1D456 O
/ O
u1D44E O
/ O
u1D459 O
/ O
u1D45C O
/ O
u1D454 O
= O
32 O
. O
For O
the O
remaining O
datasets O
, O
we O
treat O
each O
sentence O
from O
the O
text O
as O
a O
character O
utterance O
and O
train O
the O
model O
accordingly O
. O
‚Ä¢TTSis O
a O
non O
- O
pretrained O
Transformer O
baseline O
trained O
with O
the O
same O
settings O
as O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O
We O
do O
not O
feed O
additional O
domain O
knowledge O
to O
this O
model O
. O
It O
is O
randomly O
initialized O
and O
trained O
for O
our O
task O
from O
the O
scratch O
. O
‚Ä¢BERTFT(Devlin O
et O
al O
. O
, O
2018 O
) O
is O
a O
Ô¨Åne O
- O
tuned O
( O
FT O
) O
version O
of O
BERT O
/ O
u1D44F O
/ O
u1D44E O
/ O
u1D460 O
/ O
u1D452 O
model O
. O
We O
do O
not O
feed O
additional O
domain O
knowledge O
to O
this O
model O
. O
We O
refrain O
from O
training O
BERT O
/ O
u1D459 O
/ O
u1D44E O
/ O
u1D45F O
/ O
u1D454 O
/ O
u1D452 O
due O
to O
memory O
constraints O
. O
‚Ä¢BERT+ O
G O
RUFT(Devlin O
et O
al O
. O
, O
2018 O
; O
Chung O
et O
al O
. O
, O
2014 O
) O
is O
a O
similar O
to O
our O
DAPPER O
model O
, O
but O
applies O
GRU O
- O
based O
adaptive O
for O
persona O
classiÔ¨Åcation O
task O
. O
For O
this O
setting O
, O
we O
experiment O
with O
and O
without O
additional O
knowledge O
using O
a O
sufÔ¨Åx O
‚Äú O
+ O
K O
‚Äù O
. O
In O
‚Äú O
+ O
K O
‚Äù O
setting O
, O
we O
use O
GRU O
as O
the O
controller O
and O
apply O
an O
approach O
similar O
to O
AMN O
to O
enrich O
the O
learnt O
embeddings O
with O
domain O
knowledge O
. O
Without O
the O
sufÔ¨Åx O
, O
GRU O
is O
used O
for O
Ô¨Åne O
- O
tuning O
only O
. O
‚Ä¢DAPPER O
is O
our O
complete O
model O
by O
default O
. O
We O
also O
experiment O
with O
its O
variants O
using O
sufÔ¨Åx O
‚Äú O
-K O
‚Äù O
indicating O
no O
knowledge O
attention O
. O
The O
various O
BERT O
- O
based O
models O
can O
be O
considered O
as O
variants O
of O
our O
DAPPER O
model O
. O
While O
we O
report O
/u1D4391 O
- O
scores O
for O
movies O
dialogue O
and O
forum O
discussion O
post O
datasets O
, O
we O
report O
accuracy O
scores O
for O
personal O
essays O
corpus O
in O
order O
to O
remain O
consistent O
with O
prior O
work O
evaluation O
metrics O
( O
Majumder O
et O
al O
. O
, O
2017).6486.3 O
Model O
Modes O
( O
RQ2 O
) O
We O
attribute O
the O
domain O
adaptive O
capability O
of O
our O
DAPPER O
model O
to O
three O
main O
aspects O
: O
pretrained O
language O
model O
, O
domain O
knowledge O
enrichment O
and O
joint O
training O
across O
multiple O
datasets O
. O
However O
, O
this O
ability O
can O
be O
demonstrated O
only O
when O
we O
apply O
it O
to O
domains O
with O
limited O
labeled O
data O
. O
Therefore O
, O
we O
run O
our O
model O
in O
‚Äú O
ADAPT O
‚Äù O
mode O
which O
simulates O
low O
- O
data O
regimes O
to O
analyze O
the O
importance O
of O
some O
of O
the O
above O
mentioned O
aspects O
. O
InADAPT O
mode O
, O
we O
restrain O
the O
amount O
of O
training O
data O
for O
only O
one O
of O
the O
domains O
while O
retaining O
the O
complete O
set O
for O
the O
remaining O
domains O
. O
Further O
, O
we O
vary O
the O
percentage O
of O
training O
examples O
from O
one O
domain O
to O
understand O
how O
early O
our O
models O
adapt O
to O
that O
domain O
( O
with O
decent O
performance O
) O
. O
We O
refer O
to O
the O
default O
model O
mode O
for O
experiments O
in O
Section O
6.2 O
as O
‚Äú O
FULL O
‚Äù O
. O
For O
this O
experiment O
, O
we O
plot O
the O
average O
prediction O
performance O
( O
/u1D4391 O
) O
for O
varying O
percentages O
of O
domain O
- O
speciÔ¨Åc O
training O
set O
. O
6.4 O
Other O
Experimental O
Settings O
For O
baselines O
, O
we O
initialize O
our O
word O
embedding O
layers O
using O
GloVe O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
embeddings O
. O
We O
use O
the O
publicly O
released O
pre O
- O
trained O
model O
parameters O
for O
BERT O
variants O
. O
We O
perform O
a O
grid O
- O
search O
and O
optimize O
the O
hyperparameters O
using O
the O
validation O
set O
. O
In O
our O
experiments O
, O
/u1D441 O
/ O
u1D44E O
/ O
u1D451 O
/ O
u1D44E O
/ O
u1D45D O
/ O
u1D461 O
=3 O
, O
resulted O
in O
best O
outcomes O
. O
We O
use O
Adam O
( O
Kingma O
and O
Ba O
, O
2014 O
) O
as O
our O
optimizer O
. O
In O
FULL O
mode O
, O
the O
model O
achieves O
the O
best O
performance O
after O
training O
for O
50 O
epochs O
with O
a O
learning O
rate O
of O
/ O
u1D6FC=0.00001 O
. O
For O
ADAPT O
mode O
, O
we O
perform O
a O
Ô¨Åxed O
number O
of O
epochs O
to O
train O
each O
variant O
. O
We O
use O
PyTorch O
to O
implement O
our O
model O
and O
train O
it O
on O
on O
4 O
GPUs O
. O
In O
order O
to O
alleviate O
the O
problem O
of O
unbalanced O
datasets O
, O
we O
utilize O
class O
weights O
in O
categorical O
cross O
- O
entropy O
loss O
for O
each O
domain O
based O
on O
the O
training O
and O
validation O
sets O
. O
6.5 O
Results O
6.5.1 O
D O
APPER O
Performance O
( O
RQ1 O
) O
Table O
2 O
presents O
the O
results O
of O
our O
evaluation O
under O
complete O
training O
data O
settings O
( O
FULL O
) O
. O
Our O
DAPPER O
model O
achieves O
an O
absolute O
improvement O
of O
14.53 O
% O
over O
previously O
reported O
model O
baseline O
( O
AMN O
) O
in O
the O
dialogues O
domain O
. O
While O
several O
models O
have O
shown O
only O
marginal O
improvement O
in O
prediction O
performance O
on O
Personal O
Essays O
corpus O
, O
our O
model O
shows O
promise O
by O
recording O
an O
improvement O
of O
8.67 O
% O
in O
comparison O
to O
thepreviously O
reported O
CNNbaseline O
. O
Overall O
, O
our O
DAPPER O
model O
outperforms O
the O
baselines O
across O
all O
the O
three O
datasets O
signiÔ¨Åcantly O
. O
Effect O
of O
Architecture O
Choices O
( O
RQ1 O
): O
Pretrained O
BERT O
- O
based O
models O
have O
consistently O
outperformed O
all O
the O
previous O
baselines O
including O
the O
non O
- O
pretrained O
TTSmodel O
. O
Moreover O
, O
the O
Transformer O
- O
based O
adaptive O
layers O
, O
with O
an O
average O
improvement O
of O
6.1 O
% O
( O
with O
knowledge O
- O
attention O
) O
and O
4.2 O
% O
( O
without O
knowledge O
- O
attention O
) O
, O
are O
much O
more O
powerful O
than O
RNN O
- O
based O
adaptive O
layers O
. O
Further O
, O
we O
observe O
that O
BERT+ O
G O
RUFTrecords O
only O
marginal O
gains O
over O
BERTwhen O
there O
is O
no O
knowledge O
- O
attention O
. O
Effect O
of O
Knowledge O
- O
Attention O
( O
RQ1 O
): O
From O
our O
results O
in O
Table O
2 O
, O
we O
analyze O
the O
importance O
of O
the O
knowledge O
- O
attention O
to O
the O
overall O
performance O
gain O
. O
We O
compute O
percentage O
performance O
gain O
between O
similar O
models O
with O
and O
without O
knowledgeattention O
sub O
- O
layer(eg O
. O
DAPPER O
, O
DAPPER‚àí/u1D43E O
) O
. O
We O
Ô¨Ånd O
that O
the O
performance O
boost O
provided O
by O
the O
knowledge O
- O
attention O
module O
is O
noteworthy O
. O
We O
posit O
that O
the O
higher O
percentage O
gain O
( O
7.38 O
% O
) O
for O
Forum O
Posts O
dataset O
is O
due O
to O
the O
additional O
domain O
knowledge O
( O
MBTI O
- O
related O
) O
ingested O
into O
our O
KVMS O
( O
explained O
in O
Section O
3 O
) O
. O
Inspecting O
further O
within O
individual O
domain O
, O
the O
percentage O
increase O
in O
prediction O
performance O
almost O
doubles8for O
Transformer O
- O
based O
adaptive O
layers O
( O
as O
in O
DAPPER O
) O
in O
comparison O
with O
RNN O
- O
based O
adaptive O
layers O
( O
BERT+ O
G O
RUFT O
+ O
K O
) O
. O
The O
reason O
for O
this O
phenomenon O
can O
be O
ascribed O
to O
the O
multi O
- O
hop O
knowledge O
enrichment O
facilitated O
by O
/u1D441 O
/ O
u1D44E O
/ O
u1D451 O
/ O
u1D44E O
/ O
u1D45D O
/ O
u1D461 O
encoder O
layers O
commonly O
observed O
in O
Memory O
networks O
literature O
( O
Miller O
et O
al O
. O
, O
2016 O
) O
. O
6.5.2 O
A O
DAPT O
Mode O
Performance O
( O
RQ2 O
) O
Figure O
3a O
and O
3b O
show O
the O
mean O
prediction O
performance O
on O
movies O
dialogue O
and O
forum O
posts O
datasets O
respectively O
. O
We O
measure O
the O
domain O
adaptive O
capability O
of O
models O
based O
on O
the O
distance O
from O
its O
lifetime O
best O
performance O
. O
By O
varying O
the O
percentage O
of O
training O
data O
, O
we O
notice O
that O
our O
DAPPER O
model O
stabilizes O
early O
and O
outperforms O
the O
other O
variants O
with O
limited O
amount O
of O
training O
data O
. O
Notably O
, O
AMN O
model O
performs O
better O
than O
TTSmodel O
under O
low O
- O
data O
regimes O
. O
The O
improved O
performance O
of O
AMN O
is O
due O
to O
the O
domain O
knowledge O
enrichment O
via O
an O
external O
memory O
module O
. O
8 O
% O
increase O
- O
RNN O
vs O
Transformer O
- O
based O
adaptive O
layers O
: O
Movies O
dialogue O
corpus O
: O
1.6 O
% O
vs O
3.24 O
% O
( O
dialogue O
) O
, O
5.86 O
% O
vs O
8.9 O
% O
( O
posts O
) O
, O
1.6 O
% O
to O
2.4 O
% O
( O
essays)649Models O
Domain O
- O
related O
Persona O
Datasets O
Movies O
Dialogues O
( O
/u1D4391)Forum O
Posts O
( O
/u1D4391)Personal O
Essays O
( O
/u1D434 O
/ O
u1D450 O
/ O
u1D450 O
. O
) O
AFF2VEC O
0.579 O
* O
CNN O
0.628 O
0.391 O
0.588 O
* O
AMN O
0.750 O
* O
0.453 O
0.591 O
TTS O
0.776 O
0.496 O
0.593 O
BERTFT O
0.804 O
0.539 O
0.607 O
BERT+ O
G O
RUFT O
+ O
K O
0.820 O
0.579 O
0.616 O
BERT+ O
G O
RUFT O
0.807 O
0.547 O
0.608 O
DAPPER O
0.859 O
0.636 O
0.639 O
DAPPER‚àíK O
0.832 O
0.584 O
0.624 O
( O
a)Models O
/u1D46D1 O
Text O
Only O
BCA O
0.744 O
* O
CNN O
- O
CHAR O
0.735 O
* O
1 O
- O
Extra O
Feature O
BCA+P O
0.776 O
BCA+ O
SC O
0.784 O
* O
All O
Features O
BCA+ O
SC+P(>/u1D44E O
/ O
u1D461 O
/ O
u1D461)0.812 O
BCA+ O
SC+P(</u1D44E O
/ O
u1D461 O
/ O
u1D461)0.824 O
( O
b O
) O
Table O
2 O
: O
Evaluation O
results O
of O
different O
models O
on O
: O
( O
a O
) O
three O
different O
Persona O
- O
related O
domain O
datasets O
in O
F O
ULL O
mode O
, O
and O
( O
b O
) O
a O
downstream O
application O
‚Äì O
Hate O
Speech O
detection O
. O
Results O
with O
* O
are O
taken O
from O
prior O
studies O
using O
the O
model O
on O
that O
dataset O
. O
This O
feature O
is O
absent O
in O
TTS O
. O
Furthermore O
, O
we O
note O
thatDAPPER‚àíKmodel O
is O
able O
to O
maintain O
a O
good O
performance O
even O
under O
low O
- O
data O
settings O
. O
We O
intuit O
that O
pretraining O
involved O
in O
DAPPER‚àíKmodel O
is O
one O
of O
the O
reasons O
behind O
this O
behavior O
. O
Therefore O
, O
we O
Ô¨Ånd O
that O
our O
DAPPER O
model O
is O
able O
to O
learn O
general O
purpose O
persona O
embeddings O
that O
can O
adapt O
to O
low O
- O
data O
settings O
. O
Moreover O
, O
the O
combination O
of O
pretraining O
and O
adaptive O
knowledge O
transformer O
facilitates O
domain O
adaptation O
effectively O
. O
F1 O
score00.180.360.540.720.9 O
Percentage O
of O
   O
Training O
data20%40%60%80%100 O
% O
DAPPER O
DAPPER O
- O
K O
Transformer O
AMNF1 O
score00.140.280.420.560.7 O
Percentage O
of O
   O
Training O
data20%40%60%80%100 O
% O
DAPPER O
DAPPER O
- O
K O
Transformer O
AMN O
( O
a O
) O
Movies O
Dialogue O
Corpus(b O
) O
Forum O
Posts O
Corpus O
Figure O
3 O
: O
Evaluation O
of O
D O
APPER O
model O
in O
A O
DAPT O
mode O
. O
We O
report O
the O
mean O
prediction O
performance O
( O
/u1D4391 O
) O
on O
Movies O
Dialogue O
and O
Forum O
Posts O
dataset O
. O
6.6 O
Cluster O
Analysis O
( O
RQ3 O
) O
In O
order O
to O
demonstrate O
the O
capabilities O
of O
our O
persona O
embedding O
, O
we O
Ô¨Årst O
perform O
a O
simple O
cluster O
analysis O
. O
Following O
prior O
studies O
( O
Bamman O
et O
al O
. O
, O
2013 O
; O
Chu O
et O
al O
. O
, O
2018 O
) O
, O
we O
measure O
the O
ability O
to O
recover O
persona O
- O
based O
clusters O
using O
our O
embeddings O
through O
the O
purity O
scores O
as O
in O
( O
Bamman O
et O
al O
. O
, O
2013 O
) O
. O
We O
compute O
the O
overlap O
between O
clusters O
as:/u1D443 O
/ O
u1D462 O
/ O
u1D45F O
/ O
u1D456 O
/ O
u1D461 O
/ O
u1D466 O
= O
1 O
/u1D441 O
/ O
summationtext.1 O
/u1D45B O
/ O
u1D45A O
/ O
u1D44E O
/ O
u1D465 O
/ O
u1D457|/u1D466 O
/ O
u1D45B‚à©/u1D450 O
/ O
u1D457| O
, O
where O
/ O
u1D466 O
/ O
u1D45B O
is O
the O
/ O
u1D45B O
- O
th O
ground O
truth O
cluster O
, O
/u1D441is O
total O
number O
of O
characters O
, O
/u1D450 O
/ O
u1D457is O
the O
/ O
u1D457 O
/ O
u1D461 O
/ O
uni210Epredicted O
cluster O
. O
By O
applying O
simple O
agglomerative O
clustering O
on O
our O
persona O
embeddings O
( O
/u1D458clusters O
) O
, O
we O
report O
thesek O
AMN O
DP O
DAPPER O
25 O
48.4 O
39.63 O
68.6 O
50 O
48.1 O
31.0 O
65.3 O
100 O
45.2 O
24.4 O
63.4 O
Table O
3 O
: O
Cluster O
purity O
scores O
. O
DP O
is O
the O
Dirichlet O
Persona O
as O
reported O
in O
( O
Bamman O
et O
al O
. O
, O
2013 O
) O
purity O
scores O
for O
movies O
dialogue O
corpus O
. O
SpeciÔ¨Åcally O
, O
we O
compare O
the O
results O
with O
AMN O
. O
Results O
in O
Table O
3 O
indicate O
that O
our O
DAPPER O
model O
sharpens O
the O
persona O
embeddings O
so O
as O
to O
form O
much O
better O
clusters O
. O
7 O
Application O
: O
Hate O
Speech O
Detection O
With O
concerns O
about O
hate O
crimes O
, O
harassment O
, O
and O
intimidation O
on O
the O
rise O
, O
the O
role O
of O
online O
hate O
in O
exacerbating O
such O
violence O
can O
not O
be O
discounted O
. O
Hence O
, O
there O
is O
an O
growing O
need O
to O
identify O
and O
counter O
the O
problem O
of O
hateful O
content O
on O
social O
media O
. O
While O
most O
prior O
modeling O
approaches O
have O
attempted O
to O
capture O
the O
semantics O
of O
hate O
from O
text O
, O
a O
few O
of O
them O
( O
Vijayaraghavan O
et O
al O
. O
) O
have O
used O
multi O
- O
modal O
information O
to O
detect O
hateful O
content O
. O
Few O
attempts O
have O
been O
made O
to O
study O
the O
personality O
of O
targets O
and O
instigators O
of O
hate O
. O
Since O
our O
DAPPER O
model O
learns O
persona O
embeddings O
from O
different O
forms O
of O
text O
such O
as O
dialogues O
, O
posts O
or O
personal O
essays O
, O
we O
deem O
it O
Ô¨Åt O
to O
explore O
how O
well O
our O
persona O
embeddings O
transfer O
knowledge O
to O
a O
hate O
speech O
detection O
task O
involving O
texts O
from O
a O
different O
domain O
( O
in O
our O
case O
, O
Twitter O
) O
. O
There O
are O
several O
publicly O
available O
labeled O
hate O
speech O
datasets O
( O
de O
Gibert O
et O
al O
. O
, O
2018 O
; O
Waseem O
, O
2016 O
) O
but O
very O
few O
include O
author O
metadata O
or650tweets O
. O
In O
this O
work O
, O
we O
take O
advantage O
of O
the O
models O
and O
datasets O
introduced O
by O
( O
Vijayaraghavan O
et O
al O
. O
) O
( O
hereafter O
referred O
as O
M O
M O
- O
HATE O
) O
. O
This O
weakly O
- O
labeled O
dataset O
contains O
author O
information O
and O
additional O
metadata O
about O
potential O
hate O
groups O
. O
Instead O
of O
training O
a O
powerful O
hate O
speech O
system O
from O
the O
scratch O
, O
we O
augment O
their O
base O
architecture O
with O
our O
persona O
embeddings O
and O
evaluate O
the O
prediction O
performance O
on O
the O
task O
at O
hand O
. O
We O
compute O
persona O
representations O
( O
P O
) O
for O
an O
author O
based O
on O
their O
past O
tweets O
. O
We O
train O
MM O
- O
HATE O
‚Äôs O
best O
performing O
model O
, O
BIGRU+C O
HAR+ATTN O
( O
BCA O
) O
, O
under O
the O
following O
settings O
: O
( O
a O
) O
BCA+P O
, O
which O
combines O
our O
persona O
embeddings O
with O
the O
extracted O
text O
features O
, O
( O
b O
) O
BCA+SC+P(>/u1D44E O
/ O
u1D461 O
/ O
u1D461 O
) O
, O
which O
integrates O
the O
persona O
embeddings O
at O
the O
penultimate O
layer O
. O
Note O
that O
the O
text O
and O
sociocultural O
( O
SC O
) O
features O
are O
already O
fused O
at O
that O
layer O
, O
and O
( O
c O
) O
BCA+SC+P(</u1D44E O
/ O
u1D461 O
/ O
u1D461)fuses O
the O
extracted O
text O
and O
socio O
- O
cultural O
features O
with O
persona O
embeddings O
using O
an O
attention O
layer O
( O
as O
in O
M O
M O
- O
HATE O
) O
. O
Table O
2b O
summarizes O
the O
results O
of O
our O
evaluation O
on O
hate O
speech O
detection O
task O
. O
We O
observe O
that O
SC O
- O
fused O
model O
( O
BCA+SC O
) O
performs O
marginally O
better O
than O
our O
persona O
- O
fused O
model O
( O
BCA+P O
) O
. O
This O
result O
can O
be O
ascribed O
to O
the O
domain O
speciÔ¨Åcity O
ofSCfeatures O
. O
We O
also O
note O
that O
the O
combination O
of O
all O
the O
extracted O
features O
leads O
to O
a O
marked O
improvement O
in O
prediction O
performance O
, O
and O
even O
more O
so O
when O
the O
persona O
embeddings O
are O
fed O
to O
the O
fusion O
layer O
( O
BCA+SC+P(</u1D44E O
/ O
u1D461 O
/ O
u1D461 O
) O
) O
. O
Thus O
, O
our O
DAPPER O
model O
is O
able O
to O
extract O
behavioral O
features O
from O
user O
texts O
allowing O
positive O
knowledge O
transfer O
to O
various O
domains O
and O
applications O
. O
8 O
Conclusion O
We O
proposed O
a O
DAPPER O
model O
that O
learns O
a O
domain O
adapted O
pretraining O
- O
based O
persona O
representation O
. O
Our O
DAPPER O
model O
leverages O
pretrained O
BERT O
model O
and O
Ô¨Åne O
- O
tunes O
it O
with O
additional O
domainadaptive O
layers O
. O
By O
introducing O
a O
knowledgeattention O
mechanism O
, O
we O
allow O
the O
domain O
knowledge O
to O
be O
integrated O
into O
our O
persona O
embeddings O
. O
The O
proposed O
model O
achieves O
signiÔ¨Åcant O
gains O
across O
persona O
classiÔ¨Åcation O
task O
in O
different O
domains O
. O
Our O
evaluations O
validate O
that O
our O
model O
is O
capable O
of O
adapting O
to O
a O
new O
domain O
with O
limited O
labeled O
data O
. O
We O
also O
highlight O
the O
transferability O
of O
our O
persona O
embeddings O
in O
a O
downstream O
hate O
speech O
detection O
task O
. O
References O
David O
Bamman O
, O
Brendan O
O‚ÄôConnor O
, O
and O
Noah O
A O
Smith O
. O
2013 O
. O
Learning O
latent O
personas O
of O
Ô¨Ålm O
characters O
. O
In O
Proceedings O
of O
the O
51st O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
352‚Äì361 O
. O
Murray O
R O
Barrick O
and O
Michael O
K O
Mount O
. O
1991 O
. O
The O
big O
Ô¨Åve O
personality O
dimensions O
and O
job O
performance O
: O
a O
meta O
- O
analysis O
. O
Personnel O
psychology O
, O
44(1):1‚Äì26 O
. O
Murray O
R O
Barrick O
and O
Michael O
K O
Mount O
. O
1993 O
. O
Autonomy O
as O
a O
moderator O
of O
the O
relationships O
between O
the O
big O
Ô¨Åve O
personality O
dimensions O
and O
job O
performance O
. O
Journal O
of O
applied O
Psychology O
, O
78(1):111 O
. O
Fabio O
Celli O
, O
Fabio O
Pianesi O
, O
David O
Stillwell O
, O
and O
Michal O
Kosinski O
. O
2013 O
. O
Workshop O
on O
computational O
personality O
recognition O
: O
Shared O
task O
. O
In O
Seventh O
International O
AAAI O
Conference O
on O
Weblogs O
and O
Social O
Media O
. O
Shristi O
Chaudhary O
, O
Ritu O
Singh O
, O
Syed O
Tausif O
Hasan O
, O
and O
Ms O
Inderpreet O
Kaur O
. O
2013 O
. O
A O
comparative O
study O
of O
different O
classiÔ¨Åers O
for O
myers O
- O
brigg O
personality O
prediction O
model O
. O
Linguistic O
analysis O
, O
page O
21 O
. O
Eric O
Chu O
, O
Prashanth O
Vijayaraghavan O
, O
and O
Deb O
Roy O
. O
2018 O
. O
Learning O
personas O
from O
dialogue O
with O
attentive O
memory O
networks O
. O
arXiv O
preprint O
arXiv:1810.08717 O
. O
Junyoung O
Chung O
, O
Caglar O
Gulcehre O
, O
KyungHyun O
Cho O
, O
and O
Yoshua O
Bengio O
. O
2014 O
. O
Empirical O
evaluation O
of O
gated O
recurrent O
neural O
networks O
on O
sequence O
modeling O
. O
arXiv O
preprint O
arXiv:1412.3555 O
. O
JR O
Costa O
and O
T O
PAUL O
. O
1996 O
. O
of O
personality O
theories O
: O
Theoretical O
contexts O
for O
the O
Ô¨Åve O
- O
factor O
model O
. O
The O
Ô¨Åve O
- O
factor O
model O
of O
personality O
: O
Theoretical O
perspectives O
, O
51 O
. O
Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2018 O
. O
Bert O
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
understanding O
. O
arXiv O
preprint O
arXiv:1810.04805 O
. O
John O
M O
Digman O
and O
Naomi O
K O
Takemoto O
- O
Chock O
. O
1981 O
. O
Factors O
in O
the O
natural O
language O
of O
personality O
: O
Re O
- O
analysis O
, O
comparison O
, O
and O
interpretation O
of O
six O
major O
studies O
. O
Multivariate O
behavioral O
research O
, O
16(2):149‚Äì170 O
. O
Lucie O
Flekova O
and O
Iryna O
Gurevych O
. O
2015 O
. O
Personality O
proÔ¨Åling O
of O
Ô¨Åctional O
characters O
using O
sense O
- O
level O
links O
between O
lexical O
resources O
. O
In O
Proceedings O
of O
the O
2015 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
1805‚Äì1816 O
. O
Ona O
de O
Gibert O
, O
Naiara O
Perez O
, O
Aitor O
Garc O
¬¥ O
ƒ±a O
- O
Pablos O
, O
and O
Montse O
Cuadros O
. O
2018 O
. O
Hate O
speech O
dataset O
from O
a O
white O
supremacy O
forum O
. O
arXiv O
preprint O
arXiv:1809.04444 O
.651Lewis O
R O
Goldberg O
. O
1990 O
. O
An O
alternative O
‚Äù O
description O
of O
personality O
‚Äù O
: O
the O
big-Ô¨Åve O
factor O
structure O
. O
Journal O
of O
personality O
and O
social O
psychology O
, O
59(6):1216 O
. O
Larry O
A O
Hjelle O
and O
Daniel O
J O
Ziegler O
. O
1992 O
. O
Personality O
theories O
: O
Basic O
assumptions O
, O
research O
, O
and O
applications O
. O
McGraw O
- O
Hill O
Book O
Company O
. O
Sopan O
Khosla O
, O
Niyati O
Chhaya O
, O
and O
Kushal O
Chawla O
. O
2018 O
. O
Aff2vec O
: O
Affect O
‚Äì O
enriched O
distributional O
word O
representations O
. O
arXiv O
preprint O
arXiv:1805.07966 O
. O
Yoon O
Kim O
. O
2014 O
. O
Convolutional O
neural O
networks O
for O
sentence O
classiÔ¨Åcation O
. O
arXiv O
preprint O
arXiv:1408.5882 O
. O
Diederik O
P O
Kingma O
and O
Jimmy O
Ba O
. O
2014 O
. O
Adam O
: O
A O
method O
for O
stochastic O
optimization O
. O
arXiv O
preprint O
arXiv:1412.6980 O
. O
Jiwei O
Li O
, O
Michel O
Galley O
, O
Chris O
Brockett O
, O
Georgios O
P O
Spithourakis O
, O
Jianfeng O
Gao O
, O
and O
Bill O
Dolan O
. O
2016 O
. O
A O
persona O
- O
based O
neural O
conversation O
model O
. O
arXiv O
preprint O
arXiv:1603.06155 O
. O
Fei O
Liu O
, O
Julien O
Perez O
, O
and O
Scott O
Nowson O
. O
2016 O
. O
A O
language O
- O
independent O
and O
compositional O
model O
for O
personality O
trait O
recognition O
from O
short O
texts O
. O
arXiv O
preprint O
arXiv:1610.04345 O
. O
Franc O
¬∏ois O
Mairesse O
, O
Marilyn O
A O
Walker O
, O
Matthias O
R O
Mehl O
, O
and O
Roger O
K O
Moore O
. O
2007 O
. O
Using O
linguistic O
cues O
for O
the O
automatic O
recognition O
of O
personality O
in O
conversation O
and O
text O
. O
Journal O
of O
artiÔ¨Åcial O
intelligence O
research O
, O
30:457‚Äì500 O
. O
Navonil O
Majumder O
, O
Soujanya O
Poria O
, O
Alexander O
Gelbukh O
, O
and O
Erik O
Cambria O
. O
2017 O
. O
Deep O
learning O
- O
based O
document O
modeling O
for O
personality O
detection O
from O
text O
. O
IEEE O
Intelligent O
Systems O
, O
32(2):74‚Äì79 O
. O
Dan O
P O
McAdams O
and O
Erika O
Manczak O
. O
2015 O
. O
Personality O
and O
the O
life O
story O
. O
Alexander O
Miller O
, O
Adam O
Fisch O
, O
Jesse O
Dodge O
, O
AmirHossein O
Karimi O
, O
Antoine O
Bordes O
, O
and O
Jason O
Weston O
. O
2016 O
. O
Key O
- O
value O
memory O
networks O
for O
directly O
reading O
documents O
. O
arXiv O
preprint O
arXiv:1606.03126 O
. O
James O
W O
Pennebaker O
and O
Laura O
A O
King O
. O
1999 O
. O
Linguistic O
styles O
: O
Language O
use O
as O
an O
individual O
difference O
. O
Journal O
of O
personality O
and O
social O
psychology O
, O
77(6):1296 O
. O
James O
W O
Pennebaker O
, O
Matthias O
R O
Mehl O
, O
and O
Kate O
G O
Niederhoffer O
. O
2003 O
. O
Psychological O
aspects O
of O
natural O
language O
use O
: O
Our O
words O
, O
our O
selves O
. O
Annual O
review O
of O
psychology O
, O
54(1):547‚Äì577 O
. O
Jeffrey O
Pennington O
, O
Richard O
Socher O
, O
and O
Christopher O
D O
Manning O
. O
2014 O
. O
Glove O
: O
Global O
vectors O
for O
word O
representation O
. O
In O
Proceedings O
of O
the O
2014 O
conference O
on O
empirical O
methods O
in O
natural O
language O
processing O
( O
EMNLP O
) O
, O
pages O
1532‚Äì1543.Kurt O
Shuster O
, O
Samuel O
Humeau O
, O
Antoine O
Bordes O
, O
and O
Jason O
Weston O
. O
2018 O
. O
Engaging O
image O
chat O
: O
Modeling O
personality O
in O
grounded O
dialogue O
. O
arXiv O
preprint O
arXiv:1811.00945 O
. O
Paul O
H O
Soloff O
. O
1985 O
. O
Personality O
disorders O
. O
In O
Diagnostic O
interviewing O
, O
pages O
131‚Äì159 O
. O
Springer O
. O
Ashish O
Vaswani O
, O
Noam O
Shazeer O
, O
Niki O
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion O
Jones O
, O
Aidan O
N O
Gomez O
, O
≈Åukasz O
Kaiser O
, O
and O
Illia O
Polosukhin O
. O
2017 O
. O
Attention O
is O
all O
you O
need O
. O
In O
Advances O
in O
neural O
information O
processing O
systems O
, O
pages O
5998‚Äì6008 O
. O
Prashanth O
Vijayaraghavan O
, O
Hugo O
Larochelle O
, O
and O
Deb O
Roy O
. O
Interpretable O
multi O
- O
modal O
hate O
speech O
detection O
. O
Zeerak O
Waseem O
. O
2016 O
. O
Are O
you O
a O
racist O
or O
am O
i O
seeing O
things O
? O
annotator O
inÔ¨Çuence O
on O
hate O
speech O
detection O
on O
twitter O
. O
In O
Proceedings O
of O
the O
Ô¨Årst O
workshop O
on O
NLP O
and O
computational O
social O
science O
, O
pages O
138 O
‚Äì O
142 O
. O
Jiani O
Zhang O
, O
Xingjian O
Shi O
, O
Irwin O
King O
, O
and O
Dit O
- O
Yan O
Yeung O
. O
2017 O
. O
Dynamic O
key O
- O
value O
memory O
networks O
for O
knowledge O
tracing O
. O
In O
Proceedings O
of O
the O
26th O
international O
conference O
on O
World O
Wide O
Web O
, O
pages O
765‚Äì774.652Proceedings O
of O
the O
1st O
Conference O
of O
the O
Asia O
- O
PaciÔ¨Åc O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
10th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
653‚Äì663 O
December O
4 O
- O
7 O
, O
2020 O
. O
¬© O
2020 O
Association O
for O
Computational O
Linguistics O
Event O
Coreference O
Resolution O
with O
Non O
- O
Local O
Information O
Jing O
Lu O
and O
Vincent O
Ng O
Human O
Language O
Technology O
Research O
Institute O
University O
of O
Texas O
at O
Dallas O
Richardson O
, O
TX O
75083 O
- O
0688 O
{ O
ljwinnie O
, O
vince O
} O
@hlt.utdallas.edu O
Abstract O
Existing O
event O
coreference O
resolvers O
have O
largely O
focused O
on O
exploiting O
the O
information O
extracted O
from O
the O
local O
contexts O
of O
the O
event O
mentions O
under O
consideration O
. O
Hypothesizing O
that O
non O
- O
local O
information O
could O
also O
be O
useful O
for O
event O
coreference O
resolution O
, O
we O
present O
two O
extensions O
to O
a O
state O
- O
of O
- O
the O
- O
art O
joint O
event O
coreference O
model O
that O
involve O
incorporating O
( O
1 O
) O
a O
supervised O
topic O
model O
for O
improving O
trigger O
detection O
by O
providing O
global O
context O
, O
and O
( O
2 O
) O
a O
preprocessing O
module O
that O
seeks O
to O
improve O
event O
coreference O
by O
discarding O
unlikely O
candidate O
antecedents O
of O
an O
event O
mention O
using O
discourse O
contexts O
computed O
based O
on O
salient O
entities O
. O
The O
resulting O
model O
yields O
the O
best O
results O
reported O
to O
date O
on O
the O
KBP O
2017 O
English O
and O
Chinese O
datasets O
. O
1 O
Introduction O
Event O
coreference O
resolution O
is O
the O
task O
of O
determining O
the O
event O
mentions O
in O
a O
document O
that O
refer O
to O
the O
same O
real O
- O
world O
event O
. O
One O
of O
its O
major O
challenges O
concerns O
error O
propagation O
: O
since O
the O
event O
coreference O
resolution O
component O
typically O
lies O
towards O
the O
end O
of O
the O
standard O
information O
extraction O
pipeline O
, O
the O
performance O
of O
an O
event O
coreference O
resolver O
can O
be O
adversely O
affected O
by O
errors O
propagated O
from O
its O
upstream O
components O
. O
The O
upstream O
component O
that O
has O
the O
largest O
impact O
on O
event O
coreference O
performance O
is O
arguably O
trigger O
detection O
. O
Recall O
that O
the O
goal O
of O
a O
trigger O
detector O
is O
to O
identify O
event O
triggers O
and O
assign O
an O
event O
subtype O
to O
each O
of O
them O
. O
Failure O
to O
detect O
triggers O
could O
therefore O
limit O
the O
upper O
bound O
on O
event O
coreference O
performance O
. O
To O
address O
error O
propagation O
, O
one O
way O
that O
has O
been O
shown O
to O
be O
effective O
for O
a O
variety O
of O
NLP O
tasks O
is O
to O
develop O
joint O
models O
, O
which O
allow O
crosstask O
output O
constraints O
to O
be O
learned O
from O
annotated O
training O
data O
. O
For O
event O
coreference O
, O
a O
learnercan O
easily O
learn O
, O
for O
instance O
, O
that O
two O
coreferent O
event O
mentions O
must O
have O
the O
same O
event O
subtype O
, O
thereby O
allowing O
event O
coreference O
to O
inÔ¨Çuence O
trigger O
detection O
. O
Unfortunately O
, O
the O
vast O
majority O
of O
existing O
event O
coreference O
resolvers O
have O
adopted O
a O
pipeline O
architecture O
where O
trigger O
detection O
precedes O
event O
coreference O
. O
In O
particular O
, O
joint O
models O
are O
both O
under O
- O
studied O
and O
under O
- O
exploited O
for O
event O
coreference O
given O
the O
usefulness O
they O
have O
demonstrated O
for O
other O
NLP O
tasks O
. O
One O
exception O
is O
Lu O
and O
Ng O
‚Äôs O
( O
2017a O
) O
joint O
model O
, O
which O
jointly O
learns O
trigger O
detection O
and O
event O
coreference O
and O
has O
achieved O
state O
- O
of O
- O
the O
- O
art O
results O
. O
As O
a O
structured O
conditional O
random O
Ô¨Åeld O
, O
the O
model O
employs O
unary O
factors O
to O
encode O
the O
features O
speciÔ¨Åc O
for O
each O
task O
and O
binary O
/ O
ternary O
factors O
to O
capture O
the O
interaction O
between O
each O
pair O
of O
tasks O
. O
The O
use O
of O
binary O
/ O
ternary O
factors O
is O
a O
particularly O
appealing O
aspect O
of O
this O
model O
: O
it O
allows O
these O
cross O
- O
task O
interactions O
to O
be O
captured O
in O
a O
softmanner O
, O
enabling O
the O
learner O
to O
learn O
which O
combinations O
of O
values O
of O
the O
output O
variables O
are O
more O
probable O
. O
We O
hypothesize O
that O
the O
power O
of O
this O
joint O
event O
coreference O
model O
has O
not O
been O
fully O
exploited O
and O
seek O
to O
extend O
it O
in O
this O
paper O
. O
Our O
extensions O
are O
based O
on O
the O
observation O
that O
the O
strength O
of O
a O
joint O
model O
stems O
from O
its O
ability O
to O
facilitate O
cross O
- O
task O
knowledge O
transfer O
. O
In O
other O
words O
, O
the O
better O
we O
can O
model O
each O
task O
involved O
, O
the O
more O
we O
can O
potentially O
get O
out O
of O
joint O
modeling O
. O
Given O
this O
observation O
, O
we O
seek O
to O
improve O
the O
modeling O
of O
these O
tasks O
in O
this O
joint O
model O
as O
follows O
. O
First O
, O
we O
improve O
trigger O
detection O
by O
exploiting O
topic O
information O
. O
State O
- O
of O
- O
the O
- O
art O
trigger O
detectors O
, O
including O
those O
based O
on O
deep O
neural O
networks O
( O
e.g. O
, O
Nguyen O
et O
al O
. O
( O
2016 O
) O
) O
, O
classify O
each O
candidate O
trigger O
using O
local O
information O
and O
largely O
ignore O
the O
fact O
that O
the O
topic O
of O
the O
document O
in O
which O
a O
trigger O
appears O
plays O
an O
important O
role O
in O
determining O
its O
event O
subtype O
. O
To O
understand O
the O
usefulness653Three O
journalists O
at O
The O
New O
York O
Times O
on O
Tuesday O
announced O
plans O
to O
{ O
leave}ev1the O
newspaper O
. O
The O
{ O
departures}ev2 O
follow O
moves O
last O
month O
by O
several O
other O
Times O
employees O
, O
all O
of O
whom O
were O
{ O
leaving}ev3to O
join O
digital O
companies O
. O
Pakistan O
‚Äôs O
Interior O
Ministry O
has O
ordered O
New O
York O
Times O
Reporter O
to O
{ O
leave}ev4 O
. O
The O
ministry O
gave O
no O
explanation O
for O
the O
expulsion O
order O
. O
‚Äú O
You O
are O
therefore O
advised O
to O
{ O
leave}ev5the O
country O
within O
72 O
hours O
, O
‚Äù O
the O
order O
stated O
. O
Table O
1 O
: O
Event O
coreference O
resolution O
examples O
. O
of O
document O
topics O
, O
consider O
the O
examples O
in O
Table O
1 O
: O
although O
all O
Ô¨Åve O
events O
have O
similar O
trigger O
words O
, O
we O
can O
see O
that O
the O
meaning O
of O
the O
triggers O
and O
their O
event O
subtypes O
are O
different O
in O
different O
contexts O
. O
Hence O
, O
if O
an O
event O
coreference O
model O
knows O
that O
the O
topics O
of O
these O
two O
documents O
are O
different O
, O
it O
can O
exploit O
this O
information O
to O
more O
accurately O
classify O
their O
event O
subtypes O
. O
In O
particular O
, O
we O
propose O
to O
train O
a O
supervised O
topic O
model O
to O
infer O
the O
topic O
of O
each O
word O
in O
a O
test O
document O
, O
with O
the O
goal O
of O
understanding O
each O
candidate O
trigger O
using O
its O
global O
in O
addition O
to O
local O
context O
. O
Second O
, O
we O
improve O
event O
coreference O
by O
exploiting O
discourse O
information O
. O
SpeciÔ¨Åcally O
, O
we O
introduce O
a O
preprocessing O
component O
for O
event O
coreference O
resolution O
where O
we O
prune O
the O
candidate O
antecedents O
of O
an O
event O
mention O
that O
are O
unlikely O
to O
be O
its O
correct O
antecedent O
based O
on O
discourse O
context O
. O
In O
essence O
, O
this O
discourse O
- O
based O
preprocessing O
step O
seeks O
to O
simplify O
the O
job O
of O
the O
event O
coreference O
model O
by O
reducing O
the O
number O
of O
candidate O
antecedents O
it O
has O
to O
consider O
for O
a O
given O
event O
mention O
. O
We O
encode O
the O
discourse O
context O
of O
an O
event O
mention O
using O
the O
entities O
that O
are O
salient O
at O
the O
point O
of O
the O
discourse O
in O
which O
the O
event O
mention O
appears O
. O
To O
our O
knowledge O
, O
we O
are O
the O
Ô¨Årst O
to O
show O
that O
event O
coreference O
performance O
can O
be O
improved O
using O
discourse O
contexts O
that O
are O
encoded O
using O
salient O
discourse O
entities O
. O
In O
sum O
, O
the O
contributions O
of O
this O
paper O
are O
twofold O
. O
First O
, O
while O
existing O
event O
coreference O
resolvers O
have O
largely O
focused O
on O
exploiting O
the O
information O
extracted O
from O
the O
local O
contexts O
of O
the O
event O
mentions O
under O
consideration O
, O
we O
show O
how O
a O
state O
- O
of O
- O
the O
- O
art O
joint O
event O
coreference O
model O
can O
be O
improved O
using O
the O
non O
- O
local O
information O
provided O
by O
a O
supervised O
topic O
model O
and O
salient O
discourse O
entities O
. O
Second O
, O
the O
resulting O
model O
achieves O
the O
best O
results O
to O
date O
on O
the O
KBP O
2017 O
English O
and O
Chinese O
event O
coreference O
datasets O
. O
2 O
DeÔ¨Ånitions O
and O
Corpora O
2.1 O
DeÔ¨Ånitions O
We O
employ O
the O
following O
deÔ¨Ånitions O
in O
our O
discussion O
of O
trigger O
detection O
and O
event O
coreference:‚Ä¢Anevent O
trigger O
is O
a O
string O
of O
text O
that O
most O
clearly O
expresses O
the O
occurrence O
of O
an O
event O
, O
usually O
a O
word O
or O
a O
multi O
- O
word O
phrase O
. O
‚Ä¢Anevent O
mention O
is O
an O
explicit O
occurrence O
of O
an O
event O
consisting O
of O
a O
textual O
trigger O
, O
arguments O
or O
participants O
( O
if O
any O
) O
, O
and O
the O
event O
type O
/ O
subtype O
. O
‚Ä¢Anevent O
coreference O
chain O
( O
a.k.a O
. O
an O
event O
hopper O
) O
is O
a O
group O
of O
event O
mentions O
that O
refer O
to O
the O
same O
real O
- O
world O
event O
. O
They O
must O
have O
the O
same O
event O
( O
sub)type O
. O
To O
understand O
these O
deÔ¨Ånitions O
, O
consider O
the O
example O
in O
Table O
1 O
, O
which O
contains O
Ô¨Åve O
event O
mentions O
from O
two O
documents O
. O
The O
Ô¨Årst O
one O
consists O
of O
three O
event O
mentions O
of O
subtype O
Personnel O
. O
Endposition O
, O
among O
which O
ev1andev2 O
, O
which O
are O
triggered O
by O
‚Äú O
leave O
‚Äù O
and O
‚Äú O
departures O
‚Äù O
respectively O
, O
are O
coreferent O
since O
they O
describe O
the O
event O
that O
three O
journalists O
resign O
. O
The O
second O
one O
consists O
of O
two O
coreferent O
event O
mentions O
, O
ev4andev5 O
, O
both O
of O
which O
are O
triggered O
by O
‚Äú O
leave O
‚Äù O
and O
have O
subtype O
Movement O
. O
Transport O
Person O
. O
2.2 O
Corpora O
We O
employ O
the O
English O
and O
Chinese O
corpora O
used O
in O
the O
TAC O
KBP O
2017 O
Event O
Nugget O
Detection O
and O
Coreference O
task O
for O
evaluation O
, O
which O
are O
composed O
of O
two O
types O
of O
documents O
, O
newswire O
documents O
and O
discussion O
forum O
documents O
. O
There O
are O
no O
ofÔ¨Åcial O
training O
sets O
: O
the O
task O
organizers O
have O
simply O
made O
available O
a O
number O
of O
event O
coreference O
- O
annotated O
corpora O
for O
training O
. O
For O
English O
, O
we O
use O
LDC2015E29 O
, O
E68 O
, O
E73 O
, O
E94 O
, O
and O
LDC2016E64 O
for O
training O
. O
Together O
they O
contain O
817 O
documents O
with O
22894 O
event O
mentions O
distributed O
over O
13146 O
coreference O
chains O
. O
For O
Chinese O
, O
we O
use O
LDC2015E78 O
, O
E105 O
, O
E112 O
, O
and O
LDC2016E64 O
for O
training O
. O
Together O
they O
contain O
548 O
documents O
with O
7388 O
event O
mentions O
distributed O
over O
5526 O
coreference O
chains O
. O
The O
KBP O
2017 O
English O
test O
set O
consists O
of O
167 O
documents O
with O
4375 O
event O
mentions O
distributed O
over O
2963 O
coreference O
chains O
. O
The O
Chinese O
test O
set O
consists O
of O
167 O
documents O
with O
3884 O
event O
mentions O
distributed O
over O
2558 O
coreference O
chains.6543 O
Model O
Following O
Lu O
and O
Ng O
( O
2017a O
) O
, O
we O
employ O
a O
structured O
conditional O
random O
Ô¨Åeld O
, O
which O
operates O
at O
the O
document O
level O
. O
SpeciÔ¨Åcally O
, O
given O
a O
test O
document O
, O
we O
Ô¨Årst O
extract O
from O
it O
all O
single- O
and O
multiword O
nouns O
and O
verbs O
that O
have O
appeared O
at O
least O
once O
as O
a O
trigger O
in O
the O
training O
data O
. O
We O
treat O
each O
of O
these O
extracted O
nouns O
and O
verbs O
as O
a O
candidate O
event O
mention O
. O
The O
goal O
of O
the O
model O
is O
to O
make O
joint O
predictions O
for O
the O
candidate O
event O
mentions O
in O
a O
document O
. O
Three O
predictions O
will O
be O
made O
for O
each O
candidate O
event O
mention O
that O
correspond O
to O
the O
three O
tasks O
in O
the O
model O
: O
its O
trigger O
subtype O
, O
its O
induced O
topic O
, O
and O
its O
antecedent O
. O
Given O
this O
formulation O
, O
we O
deÔ¨Åne O
three O
types O
of O
output O
variables O
. O
The O
Ô¨Årst O
type O
consists O
of O
event O
subtype O
variables O
s= O
( O
s1, O
... O
,s O
n O
) O
. O
Eachsitakes O
a O
value O
in O
the O
set O
of O
the O
18 O
event O
subtypes O
deÔ¨Åned O
in O
KBP O
2017 O
or O
NONE O
, O
which O
indicates O
that O
the O
event O
mention O
is O
not O
a O
trigger O
. O
The O
second O
type O
consists O
of O
coreference O
variables O
c= O
( O
c1, O
... O
,c O
n O
) O
, O
where O
ci‚àà{1 O
, O
. O
. O
. O
, O
i‚àí1,NEW O
} O
. O
In O
other O
words O
, O
the O
value O
of O
each O
ciis O
the O
i O
d O
of O
its O
antecedent O
, O
which O
can O
be O
one O
of O
the O
preceding O
event O
mentions O
, O
or O
NEW(if O
the O
mention O
underlying O
cistarts O
a O
new O
cluster O
) O
. O
The O
third O
type O
consists O
of O
topic O
variables O
t O
= O
( O
t1, O
... O
,t O
n O
) O
. O
Eachtitakes O
a O
value O
in O
a O
19 O
- O
element O
set O
in O
which O
the O
topics O
have O
a O
one O
- O
to O
- O
one O
correspondence O
with O
the O
event O
subtype O
labels O
deÔ¨Åned O
above O
. O
Despite O
this O
one O
- O
to O
- O
one O
mapping O
, O
these O
two O
types O
of O
labels O
should O
not O
be O
interpreted O
in O
the O
same O
manner O
. O
As O
we O
will O
see O
, O
a O
word O
‚Äôs O
induced O
topic O
label O
is O
inÔ¨Çuenced O
by O
our O
supervised O
topic O
model O
, O
whereas O
a O
word O
‚Äôs O
subtype O
is O
not O
. O
Each O
candidate O
event O
mention O
is O
associated O
with O
one O
coreference O
variable O
, O
one O
event O
subtype O
variable O
, O
and O
one O
topic O
variable O
. O
Our O
model O
induces O
a O
probability O
distribution O
over O
these O
variables O
: O
p(s O
, O
c O
, O
t|x O
; O
Œò)‚àùexp(/summationdisplay O
iŒ∏ifi(s O
, O
c O
, O
t O
, O
x O
) O
) O
whereŒ∏i‚ààŒòis O
the O
weight O
associated O
with O
feature O
functionfiandxis O
the O
input O
document O
. O
3.1 O
Independent O
Models O
3.1.1 O
Trigger O
Detection O
Model O
Each O
instance O
for O
training O
the O
trigger O
detection O
model O
corresponds O
to O
a O
candidate O
trigger O
in O
the O
training O
set O
, O
which O
is O
created O
as O
follows O
. O
For O
each O
wordwthat O
appears O
as O
a O
true O
trigger O
at O
least O
once O
in O
the O
training O
data O
, O
we O
create O
a O
candidate O
triggerfrom O
each O
occurrence O
of O
win O
the O
training O
data O
. O
If O
a O
given O
occurrence O
of O
wis O
a O
true O
trigger O
in O
the O
associated O
document O
, O
the O
class O
label O
of O
the O
corresponding O
training O
instance O
is O
its O
subtype O
label O
. O
Otherwise O
, O
we O
label O
the O
instance O
as O
N O
ONE O
. O
Each O
candidate O
trigger O
mis O
represented O
using O
features O
generated O
from O
the O
following O
feature O
templates O
: O
m O
‚Äôs O
word O
, O
m O
‚Äôs O
lemma O
, O
word O
bigrams O
formed O
with O
a O
window O
size O
of O
three O
from O
m O
; O
feature O
conjunctions O
created O
by O
pairing O
m O
‚Äôs O
lemma O
with O
each O
of O
the O
following O
features O
: O
the O
head O
word O
of O
the O
entity O
syntactically O
closest O
to O
m O
, O
the O
head O
word O
of O
the O
entity O
textually O
closest O
to O
m O
, O
the O
entity O
type O
of O
the O
entity O
that O
is O
syntactically O
closest O
to O
m O
, O
and O
the O
entity O
type O
of O
the O
entity O
that O
is O
textually O
closest O
to O
m.1In O
addition O
, O
for O
event O
mentions O
with O
verb O
triggers O
, O
we O
use O
the O
head O
words O
and O
the O
entity O
types O
of O
their O
subjects O
and O
objects O
as O
features O
, O
where O
the O
subjects O
and O
objects O
are O
extracted O
from O
the O
dependency O
parses O
produced O
by O
Stanford O
CoreNLP O
( O
Manning O
et O
al O
. O
, O
2014 O
) O
. O
For O
event O
mentions O
with O
noun O
triggers O
, O
we O
create O
the O
same O
features O
except O
that O
we O
replace O
the O
subjects O
and O
verbs O
with O
heuristically O
extracted O
agents O
and O
patients O
. O
3.1.2 O
Topic O
Model O
Our O
Ô¨Årst O
extension O
to O
Lu O
and O
Ng O
‚Äôs O
( O
2017a O
) O
model O
seeks O
to O
improve O
trigger O
detection O
using O
topic O
information O
. O
We O
train O
a O
supervised O
topic O
model O
to O
infer O
the O
topic O
of O
each O
word O
in O
a O
test O
document O
, O
with O
the O
goal O
of O
understanding O
each O
candidate O
trigger O
using O
itsglobal O
in O
addition O
to O
local O
context O
. O
Like O
the O
trigger O
detection O
model O
, O
each O
training O
instance O
corresponds O
to O
a O
candidate O
trigger O
. O
The O
class O
label O
is O
the O
topic O
label O
of O
the O
candidate O
trigger O
. O
We O
have O
19 O
topic O
labels O
in O
total O
: O
there O
is O
a O
one O
- O
to O
- O
one O
correspondence O
between O
the O
18 O
subtype O
labels O
and O
18 O
of O
the O
topic O
labels O
. O
The O
remaining O
topic O
label O
is O
OTHER O
, O
which O
is O
reserved O
for O
those O
words O
that O
do O
not O
belong O
to O
any O
of O
the O
18 O
topics O
. O
Topic O
labels O
can O
be O
derived O
directly O
from O
subtype O
labels O
given O
the O
one O
- O
to O
- O
one O
correspondence O
between O
them O
. O
Each O
candidate O
trigger O
is O
represented O
using O
19 O
features O
, O
which O
correspond O
to O
the O
19 O
topic O
labels O
. O
The O
value O
of O
a O
feature O
, O
which O
is O
derived O
from O
the O
output O
of O
a O
LabeledLDA O
model O
( O
Ramage O
et O
al O
. O
, O
2009 O
) O
, O
encodes O
the O
probability O
that O
the O
candidate O
trigger O
belongs O
to O
the O
corresponding O
topic O
. O
To O
train O
the O
LabeledLDA O
model O
, O
we O
Ô¨Årst O
apply O
LabeledLDA O
using O
the O
Mallet O
toolkit O
( O
McCallum O
, O
1We O
use O
an O
in O
- O
house O
CRF O
- O
based O
entity O
extraction O
model O
to O
jointly O
identify O
the O
entity O
mentions O
and O
their O
types.6552002 O
) O
to O
the O
training O
documents O
, O
which O
learns O
a O
distribution O
over O
words O
for O
each O
topic O
, O
Œ≤ O
. O
We O
represent O
each O
training O
document O
using O
the O
candidate O
triggers O
as O
well O
as O
the O
context O
words O
that O
are O
useful O
for O
distinguishing O
the O
topics.2To O
get O
the O
useful O
context O
words O
, O
we O
rank O
the O
words O
in O
the O
training O
documents O
by O
their O
weighted O
log O
- O
likelihood O
ratios O
: O
P(wi|mj O
, O
vk O
) O
logP(wi|mj O
, O
vk O
) O
P(wi|mj,¬¨vk O
) O
wherewi O
, O
mjandvkdenote O
theith O
word O
in O
the O
vocabulary O
, O
the O
jth O
candidate O
trigger O
word O
and O
the O
kth O
subtype O
( O
including O
NONE O
) O
, O
respectively O
. O
Intuitively O
, O
a O
word O
wiwill O
have O
a O
high O
rank O
with O
respect O
to O
a O
candidate O
trigger O
word O
mjof O
subtypevkif O
it O
appears O
frequently O
with O
mjof O
subtypevkand O
infrequently O
with O
mjof O
other O
subtypes O
. O
We O
employ O
as O
the O
useful O
context O
words O
the O
top O
125 O
words O
ranked O
by O
the O
weighted O
log O
likelihood O
ratio O
w.r.t O
. O
each O
pair O
of O
trigger O
and O
subtype O
. O
The O
label O
set O
of O
each O
training O
document O
is O
the O
set O
of O
subtypes O
collected O
from O
all O
the O
triggers O
in O
the O
document O
plus O
N O
ONE O
. O
After O
training O
, O
we O
apply O
the O
resulting O
LabeledLDA O
model O
to O
a O
test O
document O
, O
which O
is O
represented O
using O
the O
candidate O
triggers O
and O
the O
useful O
context O
words O
, O
as O
deÔ¨Åned O
above O
. O
SpeciÔ¨Åcally O
, O
given O
a O
test O
document O
, O
we O
( O
1 O
) O
apply O
the O
model O
to O
infer O
the O
distribution O
of O
topics O
in O
the O
document O
, O
and O
then O
( O
2 O
) O
compute O
the O
posterior O
distribution O
of O
topics O
given O
each O
candidate O
trigger O
in O
the O
document O
using O
Bayes O
rule O
as O
follows O
: O
P(z|m)‚àùP(m|z O
: O
Œ≤)P(z O
) O
whereP(z)is O
the O
distribution O
of O
topic O
zin O
the O
test O
document O
, O
P(m|z O
: O
Œ≤)is O
the O
topic O
- O
dependent O
distribution O
of O
candidate O
triggers O
mthat O
is O
learned O
from O
the O
training O
documents O
, O
and O
P(z|m)is O
the O
posterior O
distribution O
of O
zgivenmin O
the O
test O
document O
. O
We O
use O
this O
posterior O
distribution O
to O
generate O
features O
for O
representing O
each O
instance O
for O
training O
/ O
testing O
the O
topic O
model O
, O
as O
described O
above O
. O
Note O
that O
while O
the O
label O
sets O
used O
by O
the O
trigger O
detector O
and O
the O
topic O
model O
are O
functionally O
equivalent O
, O
they O
are O
trained O
using O
different O
feature O
sets O
. O
The O
features O
used O
by O
the O
trigger O
detector O
encodes O
a O
candidate O
trigger O
‚Äôs O
local O
context O
, O
while O
the O
features O
used O
by O
the O
topic O
model O
encodes O
its O
global O
context O
( O
e.g. O
, O
its O
relationship O
with O
other O
words O
) O
. O
2If O
a O
candidate O
trigger O
is O
a O
multi O
- O
word O
phrase O
, O
we O
treat O
it O
as O
a O
‚Äú O
word O
‚Äù O
by O
concatenating O
its O
constituent O
words O
using O
underscores O
( O
e.g. O
,‚Äústep O
down O
‚Äù O
is O
represented O
as O
‚Äú O
step O
down‚Äù).3.1.3 O
Event O
Coreference O
Model O
Our O
event O
coreference O
model O
is O
an O
adaptation O
of O
Durrett O
and O
Klein O
‚Äôs O
( O
2013 O
) O
mention O
- O
ranking O
model O
, O
which O
was O
originally O
developed O
for O
entity O
coreference O
, O
to O
the O
task O
of O
event O
coreference O
. O
This O
model O
selects O
the O
most O
probable O
antecedent O
for O
a O
mention O
to O
be O
resolved O
from O
its O
set O
of O
candidate O
antecedents O
( O
or O
N O
EWif O
the O
mention O
is O
non O
- O
anaphoric O
) O
. O
We O
employ O
two O
types O
of O
feature O
templates O
to O
represent O
the O
candidate O
antecedents O
for O
the O
event O
mention O
to O
be O
resolved O
, O
mj O
. O
The O
Ô¨Årst O
type O
is O
composed O
of O
features O
that O
represent O
the O
NULL O
candidate O
antecedent.3These O
include O
: O
mj O
‚Äôs O
word O
, O
mj O
‚Äôs O
lemma O
, O
a O
conjoined O
feature O
created O
by O
pairing O
mj O
‚Äôs O
lemma O
with O
the O
number O
of O
sentences O
preceding O
mj O
, O
and O
another O
conjoined O
feature O
created O
by O
pairing O
mj O
‚Äôs O
lemma O
with O
the O
number O
of O
mentions O
precedingmjin O
the O
document O
. O
The O
second O
type O
is O
composed O
of O
features O
that O
represent O
a O
non- O
NULL O
candidate O
antecedent O
, O
mi O
. O
These O
include O
mi O
‚Äôs O
word O
, O
mi O
‚Äôs O
lemma O
, O
whether O
miandmjhave O
the O
same O
lemma O
, O
and O
the O
following O
feature O
conjunctions O
: O
( O
1 O
) O
mi O
‚Äôs O
word O
paired O
with O
mj O
‚Äôs O
word O
, O
( O
2)mi O
‚Äôs O
lemma O
paired O
withmj O
‚Äôs O
lemma O
, O
( O
3 O
) O
the O
sentence O
distance O
betweenmiandmjpaired O
with O
mi O
‚Äôs O
lemma O
and O
mj O
‚Äôs O
lemma O
, O
( O
4 O
) O
the O
mention O
distance O
between O
mi O
andmjpaired O
withmi O
‚Äôs O
lemma O
and O
mj O
‚Äôs O
lemma O
, O
( O
5 O
) O
a O
quadruple O
consisting O
of O
miandmj O
‚Äôs O
subjects O
and O
their O
lemmas O
, O
and O
( O
6 O
) O
a O
quadruple O
consisting O
ofmiandmj O
‚Äôs O
objects O
and O
their O
lemmas O
. O
Our O
second O
extension O
to O
Lu O
and O
Ng O
‚Äôs O
( O
2017a O
) O
model O
involves O
leveraging O
discourse O
information O
to O
improve O
this O
event O
coreference O
model O
. O
Specifically O
, O
we O
introduce O
a O
preprocessing O
component O
for O
event O
coreference O
resolution O
where O
we O
prune O
the O
candidate O
antecedents O
of O
an O
event O
mention O
that O
are O
unlikely O
to O
be O
its O
correct O
antecedent O
based O
on O
discourse O
context O
. O
The O
idea O
is O
to O
( O
1 O
) O
encode O
the O
discourse O
context O
of O
each O
event O
mention O
in O
a O
document O
using O
the O
entities O
that O
are O
salient O
at O
the O
point O
of O
the O
discourse O
in O
which O
the O
event O
mention O
appears O
, O
and O
by O
hypothesizing O
that O
two O
event O
mentions O
that O
appear O
in O
different O
discourse O
contexts O
are O
unlikely O
to O
be O
coreferent O
, O
we O
( O
2 O
) O
prune O
any O
candidate O
antecedent O
of O
an O
event O
mention O
mwhose O
discourse O
context O
is O
different O
from O
that O
of O
m O
, O
allowing O
the O
event O
coreference O
model O
to O
resolve O
an O
event O
mention O
to O
one O
of O
the O
candidate O
antecedents O
that O
survive O
this O
discourse O
- O
based O
Ô¨Åltering O
step O
. O
In O
essence O
, O
this O
3Resolving O
a O
mention O
to O
the O
NULL O
antecedent O
is O
the O
same O
as O
having O
the O
mention O
starts O
a O
N O
EWcluster.656preprocessing O
step O
seeks O
to O
simplify O
the O
job O
of O
the O
event O
coreference O
model O
by O
reducing O
the O
number O
of O
candidate O
antecedents O
it O
has O
to O
consider O
for O
a O
given O
event O
mention O
. O
Since O
we O
aim O
to O
encode O
the O
discourse O
context O
of O
each O
event O
mention O
using O
the O
entities O
that O
are O
salient O
at O
the O
point O
of O
the O
discourse O
in O
which O
the O
event O
mention O
appears O
, O
we O
need O
to O
compute O
the O
salience O
score O
of O
each O
entity O
Ew.r.t O
. O
each O
event O
mentionm O
. O
We O
employ O
the O
following O
formula O
, O
which O
was O
proposed O
by O
Chen O
and O
Ng O
( O
2015b O
): O
/summationdisplay O
e‚ààEg(e)√ódecay O
( O
e O
) O
In O
this O
formula O
, O
eis O
a O
mention O
of O
entity O
Ethat O
appears O
in O
either O
the O
same O
sentence O
as O
mor O
one O
of O
its O
preceding O
sentences O
. O
g(e)is O
a O
score O
that O
is O
computed O
based O
on O
the O
grammatical O
role O
of O
ein O
the O
sentence O
: O
4 O
if O
eis O
a O
subject O
, O
2 O
if O
it O
is O
an O
object O
, O
and O
1 O
otherwise O
. O
decay O
( O
e)is O
a O
decay O
factor O
that O
is O
set O
to O
0.5dis O
, O
wheredisis O
the O
sentence O
distance O
between O
eandm O
. O
We O
compute O
discourse O
entities O
using O
Stanford O
CoreNLP O
‚Äôs O
neural O
entity O
coreference O
resolver O
and O
grammatical O
roles O
using O
CoreNLP O
‚Äôs O
syntactic O
dependency O
parser O
. O
Next O
, O
we O
deÔ¨Åne O
the O
discourse O
context O
of O
an O
event O
mentionmto O
be O
the O
list O
of O
entities O
whose O
salience O
score O
is O
at O
least O
1 O
when O
computed O
w.r.t O
. O
m. O
As O
noted O
before O
, O
we O
aim O
to O
prune O
the O
unlikely O
candidate O
antecedents O
of O
an O
event O
mention O
m O
, O
namely O
those O
candidates O
whose O
discourse O
contexts O
are O
different O
from O
that O
of O
m. O
Rather O
than O
heuristically O
deÔ¨Åning O
a O
function O
for O
computing O
the O
similarity O
between O
two O
different O
discourse O
contexts O
, O
we O
train O
a O
ranker O
that O
ranks O
the O
candidate O
antecedents O
of O
m O
based O
on O
two O
types O
of O
features O
derived O
from O
their O
discourse O
contexts O
: O
Salience O
score O
ratios O
( O
SSRs O
): O
For O
each O
entity O
E O
that O
appears O
in O
the O
discourse O
contexts O
of O
both O
candidate O
antecedent O
candm O
, O
we O
Ô¨Årst O
compute O
E O
‚Äôs O
SSR O
as O
the O
ratio O
of O
E O
‚Äôs O
salience O
score O
computed O
w.r.t.mtoE O
‚Äôs O
salience O
score O
computed O
w.r.t O
. O
c. O
( O
If O
this O
ratio O
is O
less O
than O
1 O
, O
we O
take O
its O
reciprocal O
. O
) O
Then O
, O
for O
each O
( O
c O
, O
m)pair O
, O
we O
create O
Ô¨Åve O
features O
that O
encode O
the O
number O
of O
entities O
whose O
SSR O
falls O
into O
each O
of O
these O
Ô¨Åve O
intervals O
: O
[ O
1,1 O
] O
, O
( O
1 O
, O
2 O
] O
, O
( O
2 O
, O
3 O
] O
, O
( O
3,4 O
] O
, O
( O
4,5 O
] O
, O
and O
[ O
5 O
, O
inf O
] O
. O
Intuitively O
, O
c O
‚Äôs O
andm O
‚Äôs O
discourse O
contexts O
tend O
to O
be O
more O
similar O
if O
they O
have O
more O
entities O
in O
the O
lower O
buckets O
. O
Lexical O
features O
: O
For O
each O
mention O
em1of O
each O
entity O
in O
candidate O
antecedent O
c O
‚Äôs O
discourse O
conFigure O
1 O
: O
Unary O
factors O
for O
the O
three O
tasks O
, O
the O
variables O
they O
are O
connected O
to O
, O
and O
the O
possible O
values O
of O
the O
variables O
. O
text O
and O
each O
mention O
em2of O
each O
entity O
in O
m O
‚Äôs O
discourse O
context O
, O
we O
create O
a O
lexical O
feature O
that O
pairsem1 O
‚Äôs O
head O
with O
em2 O
‚Äôs O
head O
. O
To O
train O
this O
ranker O
, O
we O
employ O
the O
same O
loglinear O
model O
as O
the O
one O
used O
for O
the O
event O
coreference O
model O
, O
where O
the O
training O
objective O
is O
to O
maximize O
the O
likelihood O
of O
selecting O
the O
correct O
antecedent O
for O
each O
event O
mention O
. O
After O
training O
, O
we O
apply O
this O
ranker O
to O
prune O
all O
but O
the O
topkcandidate O
antecedents O
of O
each O
event O
mention O
in O
a O
test O
document O
. O
These O
kcandidate O
antecedents O
, O
together O
with O
the O
NULL O
candidate O
antecedent O
, O
will O
be O
ranked O
by O
the O
event O
coreference O
model O
, O
and O
the O
highest O
- O
ranked O
candidate O
will O
be O
selected O
as O
the O
antecedent O
of O
the O
event O
mention O
under O
consideration.4We O
treatkas O
a O
hyperparameter O
and O
tune O
it O
on O
the O
development O
set O
. O
It O
is O
worth O
noting O
that O
we O
prune O
the O
candidate O
antecedents O
of O
the O
event O
mentions O
not O
only O
in O
the O
test O
set O
but O
also O
in O
the O
training O
set O
. O
We O
produce O
the O
topkcandidate O
antecedents O
of O
each O
event O
mention O
in O
the O
training O
set O
via O
Ô¨Åve O
- O
fold O
cross O
- O
validation O
over O
the O
training O
documents O
. O
Figure O
1 O
illustrates O
the O
unary O
factors O
, O
which O
encode O
the O
features O
used O
in O
the O
three O
independent O
models O
. O
SpeciÔ¨Åcally O
, O
the O
sentence O
fragment O
at O
the O
bottom O
of O
the O
Ô¨Ågure O
contains O
two O
event O
mentions O
, O
one O
triggered O
by O
leave O
and O
the O
other O
by O
departure O
. O
Each O
of O
them O
is O
associated O
with O
three O
variables O
, O
one O
for O
each O
of O
the O
three O
models O
. O
Next O
to O
each O
variable O
is O
the O
set O
of O
possible O
values O
of O
that O
variable O
. O
3.2 O
Joint O
Learning O
To O
perform O
joint O
training O
over O
the O
three O
models O
described O
in O
the O
previous O
subsection O
, O
we O
need O
to O
4The O
discourse O
preprocessing O
module O
does O
not O
handle O
NULL O
candidate O
antecedents O
, O
so O
they O
will O
always O
be O
available O
to O
the O
event O
coreference O
model.657Figure O
2 O
: O
Binary O
and O
ternary O
factors O
. O
deÔ¨Åne O
( O
1 O
) O
features O
that O
capture O
the O
interaction O
between O
the O
two O
tasks O
, O
( O
2 O
) O
the O
joint O
training O
scheme O
, O
and O
( O
3 O
) O
the O
inference O
mechanism O
. O
3.2.1 O
Cross O
- O
Task O
Interaction O
Features O
Our O
cross O
- O
task O
interaction O
features O
, O
which O
capture O
thepairwise O
interaction O
between O
our O
tasks O
, O
are O
associated O
with O
ternary O
factors O
, O
as O
described O
below O
. O
Trigger O
detection O
and O
coreference O
. O
We O
deÔ¨Åne O
our O
joint O
coreference O
and O
trigger O
detection O
factors O
such O
that O
the O
features O
deÔ¨Åned O
on O
subtype O
variables O
siandsjare O
Ô¨Åred O
only O
if O
current O
mention O
mjis O
coreferent O
with O
preceding O
mention O
mi O
. O
These O
features O
are O
: O
( O
1 O
) O
the O
pair O
of O
miandmj O
‚Äôs O
subtypes O
; O
( O
2 O
) O
the O
pair O
ofmj O
‚Äôs O
subtype O
and O
mi O
‚Äôs O
word O
; O
and O
( O
3 O
) O
the O
pair O
ofmi O
‚Äôs O
subtype O
and O
mj O
‚Äôs O
word O
. O
Trigger O
detection O
and O
topic O
modeling O
. O
We O
Ô¨Åre O
features O
( O
encoded O
as O
binary O
factors O
) O
that O
conjoin O
each O
candidate O
event O
mention O
‚Äôs O
event O
subtype O
, O
its O
topic O
and O
the O
lemma O
of O
its O
trigger O
. O
Topic O
modeling O
and O
coreference O
. O
Our O
joint O
coreference O
and O
topic O
modeling O
factors O
and O
features O
are O
the O
same O
as O
those O
for O
trigger O
detection O
and O
coreference O
, O
except O
that O
event O
subtype O
labels O
are O
replaced O
with O
topic O
labels O
. O
In O
other O
words O
, O
the O
features O
are O
deÔ¨Åned O
on O
the O
topic O
labels O
. O
Figure O
2 O
shows O
the O
cross O
- O
task O
interaction O
features O
. O
The O
green O
factor O
is O
binary O
, O
connecting O
a O
subtype O
variable O
and O
a O
topic O
variable O
. O
The O
red O
factor O
is O
ternary O
, O
connecting O
two O
subtype O
variables O
to O
a O
coreference O
variable O
. O
Finally O
, O
the O
blue O
factor O
is O
also O
ternary O
, O
connecting O
topic O
with O
coreference O
. O
3.2.2 O
Training O
The O
joint O
training O
scheme O
seeks O
to O
learn O
the O
model O
parameters O
Œòfrom O
a O
set O
of O
dtraining O
documents O
, O
where O
document O
icontains O
content O
xi O
, O
gold O
trigger O
annotations O
s‚àó O
i O
, O
topic O
labels O
t‚àó O
iinferred O
from O
the O
LabeledLDA O
model O
using O
Gibbs O
sampling O
, O
andgold O
event O
coreference O
partition O
C‚àó O
i O
, O
by O
maximizing O
the O
following O
conditional O
likelihood O
of O
the O
training O
data O
with O
L O
1regularization:5 O
L(Œò O
) O
= O
d O
/ O
summationdisplay O
i=1log O
/ O
summationdisplay O
c‚àó‚ààA(C‚àó O
i)p O
/ O
prime(s‚àó O
i O
, O
t‚àó O
i O
, O
c‚àó|xi O
; O
Œò O
) O
+ O
Œª O
/ O
bardblŒò O
/ O
bardbl1 O
wherep O
/ O
prime(s‚àó,t‚àó,c‚àó|x O
; O
Œò)isp(s‚àó,t‚àó,c‚àó|x O
; O
Œò)augmented O
with O
task O
- O
speciÔ¨Åc O
loss O
functions O
. O
SpeciÔ¨Åcally O
, O
p O
/ O
prime(s‚àó,t‚àó,c‚àó|x O
; O
Œò)‚àùp(s‚àó,t‚àó,c‚àó|x O
; O
Œò O
) O
exp O
[ O
Œ±sls(s O
, O
s‚àó O
) O
+ O
Œ±tlt(t O
, O
t‚àó O
) O
+ O
Œ±clc(c O
, O
C‚àó O
) O
] O
wherels O
, O
ltandlcare O
task O
- O
speciÔ¨Åc O
loss O
functions6 O
, O
andŒ±s O
, O
Œ±tandŒ±care O
the O
associated O
weight O
parameters O
that O
specify O
the O
relative O
importance O
of O
the O
three O
tasks O
in O
the O
objective O
function.7We O
use O
AdaGrad O
( O
Duchi O
et O
al O
. O
, O
2011 O
) O
to O
optimize O
our O
objective O
function O
with O
Œª= O
0.001 O
. O
3.2.3 O
Inference O
Inference O
, O
which O
is O
performed O
during O
training O
and O
decoding O
, O
involves O
computing O
the O
marginals O
for O
a O
variable O
or O
a O
set O
of O
variables O
to O
which O
a O
factor O
connects O
. O
For O
efÔ¨Åciency O
, O
we O
perform O
approximate O
inference O
using O
belief O
propagation O
, O
running O
it O
until O
convergence O
. O
We O
use O
minimum O
Bayes O
risk O
decoding O
, O
where O
we O
compute O
the O
marginals O
for O
each O
variable O
in O
our O
model O
and O
independently O
return O
the O
most O
likely O
setting O
of O
each O
variable O
. O
Marginals O
typically O
converge O
in O
3‚Äì5 O
iterations O
of O
belief O
propagation O
, O
so O
we O
use O
5 O
iterations O
in O
our O
experiments O
. O
4 O
Evaluation O
4.1 O
Experimental O
Setup O
We O
perform O
training O
and O
evaluation O
on O
the O
KBP O
2017 O
English O
and O
Chinese O
corpora O
. O
For O
English O
, O
5In O
the O
conditional O
log O
likelihood O
function O
, O
A(C‚àó O
i)is O
the O
set O
of O
antecedent O
structures O
that O
are O
consistent O
with O
C‚àó O
i. O
Since O
our O
model O
needs O
to O
be O
trained O
on O
antecedent O
vectors O
c‚àóbut O
the O
gold O
coreference O
annotation O
for O
each O
document O
iis O
provided O
in O
the O
form O
of O
a O
clustering O
C‚àó O
i O
, O
we O
need O
to O
sum O
over O
all O
consistent O
antecedent O
structures O
. O
6The O
loss O
function O
for O
event O
coreference O
, O
which O
is O
introduced O
by O
Durrett O
and O
Klein O
( O
2013 O
) O
for O
entity O
coreference O
resolution O
, O
is O
a O
weighted O
sum O
of O
( O
1 O
) O
the O
number O
of O
anaphoric O
mentions O
misclassiÔ¨Åed O
as O
non O
- O
anaphoric O
, O
( O
2 O
) O
the O
number O
of O
non O
- O
anaphoric O
mentions O
misclassiÔ¨Åed O
as O
anaphoric O
, O
and O
( O
3 O
) O
the O
number O
of O
incorrectly O
resolved O
mentions O
. O
The O
loss O
function O
for O
trigger O
detection O
is O
parameterized O
in O
a O
similar O
way O
, O
having O
three O
parameters O
associated O
with O
( O
1 O
) O
the O
number O
of O
nontriggers O
misclassiÔ¨Åed O
as O
triggers O
, O
( O
2 O
) O
the O
number O
of O
triggers O
misclassiÔ¨Åed O
as O
non O
- O
triggers O
, O
and O
( O
3 O
) O
the O
number O
of O
triggers O
labeled O
with O
the O
wrong O
subtype O
. O
The O
loss O
function O
for O
topic O
detection O
is O
deÔ¨Åned O
in O
a O
similar O
way O
as O
trigger O
detection O
. O
7These O
weight O
parameters O
, O
as O
well O
as O
those O
that O
are O
used O
within O
the O
loss O
functions O
, O
are O
tuned O
on O
the O
development O
set O
using O
grid O
search.658Event O
Coreference O
Trigger O
Detection O
English O
MUCB3CEAF O
eBLANC O
A O
VG O
- O
F O
‚àÜ O
P O
R O
F O
‚àÜ O
1 O
Huang O
et O
al O
. O
( O
2019 O
) O
35.7 O
43.2 O
40.0 O
32.4 O
36.8 O
56.8 O
46.4 O
51.1 O
2 O
Full O
37.11 O
44.49 O
40.03 O
29.93 O
37.89 O
64.45 O
46.92 O
54.30 O
3‚àíTopic O
34.16 O
43.76 O
40.78 O
28.20 O
36.72‚àí1.17 O
64.39 O
46.67 O
54.11‚àí0.19 O
4‚àíDiscourse O
34.53 O
43.06 O
40.07 O
27.95 O
36.40‚àí1.49 O
62.15 O
47.49 O
53.84‚àí0.46 O
5‚àíBoth O
31.94 O
42.84 O
40.21 O
26.49 O
35.37‚àí2.52 O
63.57 O
45.87 O
53.29‚àí0.89 O
Event O
Coreference O
Trigger O
Detection O
Chinese O
MUCB3CEAF O
eBLANC O
A O
VG O
- O
F O
‚àÜ O
P O
R O
F O
‚àÜ O
6 O
Lu O
and O
Ng O
( O
2017b O
) O
27.07 O
34.18 O
32.22 O
18.57 O
28.01 O
46.61 O
46.91 O
46.76 O
7 O
Full O
27.89 O
40.95 O
39.49 O
22.00 O
32.58 O
51.81 O
54.81 O
53.27 O
8‚àíTopic O
26.39 O
40.43 O
38.75 O
21.18 O
31.69‚àí0.89 O
51.81 O
53.28 O
52.53‚àí0.74 O
9‚àíDiscourse O
26.13 O
40.78 O
39.31 O
21.02 O
31.81‚àí0.77 O
51.65 O
54.65 O
53.11‚àí0.16 O
10‚àíBoth O
25.93 O
37.50 O
34.24 O
19.92 O
29.40‚àí3.18 O
56.78 O
44.63 O
49.98‚àí3.29 O
Table O
2 O
: O
Results O
of O
event O
coreference O
and O
trigger O
detection O
on O
the O
KBP O
2017 O
English O
and O
Chinese O
test O
sets O
. O
Baseline O
results O
( O
rows O
1 O
and O
6 O
) O
are O
copied O
verbatim O
from O
the O
original O
papers O
. O
we O
train O
models O
on O
646 O
of O
the O
training O
documents O
, O
tune O
parameters O
on O
171 O
training O
documents O
, O
and O
report O
results O
on O
the O
ofÔ¨Åcial O
KBP O
2017 O
English O
test O
set O
. O
For O
Chinese O
, O
we O
train O
models O
on O
438 O
of O
the O
training O
documents O
, O
tune O
parameters O
on O
110 O
training O
documents O
, O
and O
report O
results O
on O
the O
ofÔ¨Åcial O
KBP O
2017 O
Chinese O
test O
set O
. O
Results O
of O
event O
coreference O
and O
trigger O
detection O
are O
obtained O
using O
version O
1.8 O
of O
the O
ofÔ¨Åcial O
scorer O
provided O
by O
the O
KBP O
2017 O
organizers O
. O
To O
evaluate O
event O
coreference O
performance O
, O
the O
scorer O
employs O
four O
commonly O
- O
used O
scoring O
measures O
, O
namely O
MUC O
( O
Vilain O
et O
al O
. O
, O
1995 O
) O
, O
B3(Bagga O
and O
Baldwin O
, O
1998 O
) O
, O
CEAF O
e(Luo O
, O
2005 O
) O
and O
BLANC O
( O
Recasens O
and O
Hovy O
, O
2011 O
) O
, O
as O
well O
as O
the O
unweighted O
average O
of O
their O
F O
- O
scores O
( O
A O
VG O
- O
F O
) O
. O
The O
scorer O
reports O
event O
mention O
detection O
performance O
in O
terms O
of O
Precision O
( O
P O
) O
, O
Recall O
( O
R O
) O
and O
F O
- O
score O
, O
considering O
a O
mention O
correctly O
detected O
if O
it O
has O
an O
exact O
match O
with O
a O
gold O
mention O
in O
terms O
of O
boundary O
and O
event O
subtype O
. O
4.2 O
Results O
Results O
on O
the O
English O
test O
set O
are O
shown O
in O
the O
top O
half O
of O
Table O
2 O
. O
SpeciÔ¨Åcally O
, O
row O
1 O
shows O
the O
results O
of O
Huang O
et O
al O
. O
‚Äôs O
( O
2019 O
) O
resolver O
, O
which O
has O
produced O
best O
results O
to O
date O
on O
this O
test O
set O
. O
Row O
2 O
shows O
the O
results O
of O
our O
full O
model O
, O
which O
substantially O
outperforms O
the O
baseline O
system O
( O
row O
1 O
) O
, O
yielding O
an O
improvement O
of O
1.09 O
points O
in O
A O
VG O
- O
F O
for O
event O
coreference O
and O
3.2 O
points O
in O
F O
- O
score O
for O
trigger O
detection O
. O
Note O
that O
the O
improvement O
in O
the O
MUC O
and O
B3F O
- O
scores O
is O
largely O
offset O
by O
the O
precipitation O
in O
the O
BLANC O
F O
- O
score O
. O
Results O
on O
the O
Chinese O
test O
set O
are O
shown O
in O
the O
bottom O
half O
of O
Table O
2 O
. O
SpeciÔ¨Åcally O
, O
row O
6 O
shows O
the O
results O
of O
Lu O
and O
Ng O
‚Äôs O
( O
2017b O
) O
resolver O
, O
which O
is O
the O
top O
KBP O
2017 O
system O
for O
Chinese O
and O
hasproduced O
the O
best O
results O
to O
date O
on O
this O
test O
set O
. O
Our O
full O
model O
( O
row O
7 O
) O
outperforms O
this O
baseline O
by O
4.57 O
points O
in O
A O
VG O
- O
F O
for O
event O
coreference O
and O
6.51 O
points O
in O
F O
- O
score O
for O
trigger O
detection O
. O
Despite O
the O
large O
improvement O
in O
A O
VG O
- O
F O
, O
the O
MUC O
F O
- O
score O
only O
increases O
by O
0.82 O
points O
. O
Since O
MUC O
F O
- O
scores O
are O
computed O
solely O
based O
on O
coreference O
links O
, O
these O
results O
suggest O
that O
the O
improvement O
in O
A O
VG O
- O
F O
can O
largely O
be O
attributed O
to O
successful O
identiÔ¨Åcation O
singleton O
clusters O
rather O
than O
successful O
identiÔ¨Åcation O
of O
coreference O
links O
. O
4.3 O
Model O
Ablations O
To O
evaluate O
the O
importance O
of O
each O
of O
the O
two O
extensions O
in O
the O
full O
model O
, O
we O
perform O
ablation O
experiments O
. O
Rows O
3‚Äì5 O
and O
rows O
8‚Äì10 O
in O
Table O
2 O
show O
the O
English O
and O
Chinese O
results O
obtained O
using O
models O
that O
are O
retrained O
after O
one O
or O
both O
of O
the O
extensions O
are O
removed O
from O
the O
full O
model O
. O
The O
changes O
in O
A O
VG O
- O
F O
as O
a O
result O
of O
the O
ablations O
are O
shown O
in O
the O
‚àÜcolumns O
for O
both O
tasks O
. O
Similar O
conclusions O
can O
be O
drawn O
from O
the O
ablation O
results O
for O
both O
languages O
. O
First O
, O
ablating O
each O
of O
the O
two O
extensions O
causes O
a O
drop O
in O
performance O
for O
both O
event O
coreference O
and O
trigger O
detection O
. O
These O
results O
suggest O
that O
topic O
modeling O
and O
discourse O
pruning O
are O
both O
useful O
for O
the O
two O
tasks O
. O
Second O
, O
ablating O
both O
extensions O
causes O
a O
more O
abrupt O
drop O
in O
performance O
than O
ablating O
one O
of O
the O
extensions O
. O
This O
implies O
that O
each O
extension O
is O
providing O
useful O
information O
for O
each O
task O
that O
can O
not O
be O
provided O
by O
the O
other O
extension O
. O
Third O
, O
when O
both O
extensions O
are O
ablated O
, O
the O
resulting O
models O
still O
outperform O
the O
baselines O
for O
both O
tasks O
. O
Nevertheless O
, O
we O
can O
see O
that O
for O
English O
, O
discourse O
pruning O
contributes O
more O
to O
the O
performance O
of O
our O
full O
model O
than O
topic O
modeling O
, O
whereas O
the O
reverse O
is O
true O
for O
Chinese.659English O
Chinese O
Training O
Test O
Training O
Test O
1 O
Number O
of O
candidate O
event O
mentions O
to O
be O
resolved O
52370 O
9494 O
39758 O
9918 O
2 O
Number O
of O
candidate O
antecedents O
before O
pruning O
371718 O
48750 O
124292 O
26406 O
3 O
Number O
of O
candidate O
antecedents O
after O
pruning O
119416 O
20956 O
83378 O
20109 O
4 O
Number O
( O
% O
) O
of O
anaphoric O
event O
mentions4362 O
( O
8.3%)914 O
( O
9.6%)1713 O
( O
4.3%)821 O
( O
8.3 O
% O
) O
5Number O
( O
% O
) O
of O
anaphoric O
event O
mentions O
whose O
correct O
antecedent O
are O
among O
the O
candidates O
before O
pruning4317 O
( O
99.0%)803 O
( O
87.8%)1671 O
( O
97.6%)585 O
( O
71.3 O
% O
) O
6Number O
( O
% O
) O
of O
anaphoric O
event O
mentions O
whose O
correct O
antecedent O
are O
among O
the O
candidates O
after O
pruning3171 O
( O
72.7%)670 O
( O
73.3%)1610 O
( O
94.0%)565 O
( O
68.8 O
% O
) O
Table O
3 O
: O
Statistics O
on O
salience O
- O
based O
candidate O
pruning O
. O
4.4 O
Analysis O
of O
Salience O
- O
Based O
Pruning O
To O
gain O
insights O
into O
the O
effectiveness O
of O
discourse O
modeling O
in O
terms O
of O
pruning O
candidate O
antecedents O
, O
Table O
3 O
shows O
some O
statistics O
on O
the O
candidate O
antecedents O
before O
and O
after O
applying O
pruning O
. O
Concretely O
, O
row O
1 O
shows O
the O
total O
number O
of O
event O
mentions O
to O
be O
resolved O
in O
the O
English O
and O
Chinese O
training O
and O
test O
sets O
. O
For O
English O
, O
as O
we O
can O
see O
in O
rows O
2‚Äì3 O
, O
only O
32.1 O
% O
and O
43.0 O
% O
of O
the O
candidate O
antecedents O
remain O
in O
the O
training O
and O
test O
sets O
respectively O
after O
pruning O
. O
This O
can O
be O
attributed O
to O
the O
fact O
that O
we O
aggressively O
prune O
the O
candidate O
antecedents O
by O
allowing O
k(the O
number O
of O
top O
candidate O
antecedents O
that O
can O
survive O
the O
pruning O
for O
each O
event O
mention O
) O
to O
be O
in O
the O
range O
of O
1 O
to O
5 O
during O
parameter O
tuning.8Row O
4 O
shows O
that O
among O
all O
event O
mentions O
to O
be O
resolved O
, O
only O
8.3 O
% O
of O
them O
are O
anaphoric O
. O
Row O
5 O
shows O
that O
before O
pruning O
, O
the O
correct O
antecedent O
of O
almost O
all O
of O
the O
anaphoric O
event O
mentions O
in O
the O
training O
set O
is O
among O
the O
set O
of O
candidate O
antecedents O
, O
whereas O
the O
corresponding O
number O
on O
the O
test O
set O
is O
only O
87.8 O
% O
due O
to O
the O
presence O
of O
unseen O
event O
mentions O
. O
Row O
6 O
shows O
that O
72.7 O
% O
and O
73.3 O
% O
of O
the O
correct O
antecedents O
on O
the O
training O
set O
and O
the O
test O
set O
survive O
the O
pruning O
, O
respectively O
. O
Similar O
trends O
can O
be O
observed O
for O
the O
Chinese O
datasets O
. O
Overall O
, O
these O
statistics O
shed O
light O
on O
why O
discourse O
- O
based O
pruning O
is O
beneÔ¨Åcial O
: O
the O
percentage O
of O
correct O
antecedents O
that O
survive O
the O
pruning O
is O
far O
greater O
than O
the O
percentage O
of O
candidate O
antecedents O
that O
are O
pruned O
. O
4.5 O
Discussion O
One O
thing O
that O
the O
reader O
may O
not O
be O
able O
to O
appreciate O
just O
by O
looking O
at O
the O
performance O
numbers O
in O
Table O
2 O
is O
that O
our O
two O
extensions O
are O
starting O
to O
attack O
some O
of O
the O
non O
- O
trivial O
aspects O
of O
event O
8The O
bestkaccording O
to O
the O
development O
set O
is O
2 O
for O
English O
and O
3 O
for O
Chinese.coreference O
that O
involve O
semantics O
and O
discourse O
, O
as O
opposed O
to O
those O
previous O
approaches O
that O
focus O
on O
low O
- O
level O
issues O
( O
e.g. O
, O
string O
matching O
) O
. O
For O
this O
reason O
, O
we O
will O
take O
a O
look O
at O
some O
of O
the O
errors O
addressed O
by O
our O
extensions O
below O
. O
Let O
us O
Ô¨Årst O
consider O
the O
kind O
of O
errors O
topic O
modeling O
allows O
us O
to O
address O
. O
Consider O
the O
Ô¨Årst O
two O
sentences O
in O
Table O
4 O
, O
both O
of O
which O
contain O
the O
trigger O
candidate O
‚Äú O
struck O
‚Äù O
. O
While O
‚Äú O
struck O
‚Äù O
triggers O
a O
‚Äú O
ConÔ¨Çict O
. O
Attack O
‚Äù O
event O
in O
the O
Ô¨Årst O
sentence O
, O
neither O
of O
its O
occurrences O
in O
the O
second O
sentence O
corresponds O
to O
a O
true O
trigger O
( O
and O
therefore O
their O
subtypes O
should O
both O
be O
NONE O
) O
. O
Without O
topic O
modeling O
, O
the O
model O
predicts O
all O
occurrences O
of O
‚Äú O
struck O
‚Äù O
in O
these O
sentences O
as O
belonging O
to O
ConÔ¨Çict O
. O
Attack O
( O
and O
hence O
misclassiÔ¨Åes O
the O
subtypes O
ofm2andm3 O
) O
. O
The O
reasons O
are O
that O
( O
1 O
) O
‚Äú O
struck O
‚Äù O
is O
most O
frequently O
associated O
with O
‚Äú O
ConÔ¨Çict O
. O
Attack O
‚Äù O
in O
the O
training O
data O
, O
and O
( O
2 O
) O
since O
the O
two O
sentences O
have O
a O
similar O
syntactic O
structure O
and O
contain O
entities O
of O
the O
same O
type O
, O
the O
model O
fails O
to O
identify O
their O
differences O
. O
In O
contrast O
, O
with O
topic O
modeling O
, O
our O
model O
correctly O
predicts O
the O
topic O
of O
the O
document O
in O
which O
the O
second O
example O
appears O
as O
Contact O
. O
Meeting O
. O
Since O
the O
model O
manages O
to O
learn O
that O
the O
subtype O
of O
‚Äú O
struck O
‚Äù O
should O
be O
NONE O
when O
the O
topic O
is O
Contact O
. O
Meeting O
and O
that O
its O
subtype O
should O
be O
‚Äú O
ConÔ¨Çict O
. O
Attack O
‚Äù O
when O
the O
topic O
is O
‚Äú O
ConÔ¨Çict O
. O
Attack O
‚Äù O
, O
it O
correctly O
predicts O
m2and O
m3as O
having O
subtype O
NONE O
and O
, O
as O
a O
result O
, O
it O
also O
correctly O
determines O
that O
they O
are O
not O
coreferent O
. O
In O
other O
words O
, O
by O
using O
global O
information O
encoded O
by O
the O
topic O
model O
, O
our O
model O
can O
distinguish O
between O
words O
that O
have O
different O
meanings O
in O
different O
contexts O
. O
Next O
, O
consider O
the O
last O
example O
in O
Table O
4 O
, O
which O
aims O
to O
give O
the O
reader O
an O
idea O
of O
the O
usefulness O
of O
discourse O
- O
based O
pruning O
. O
In O
this O
example O
, O
m4,m5 O
, O
andm8refer O
to O
the O
event O
of O
the O
French O
soldier O
being O
stabbed O
and O
are O
coreferent O
, O
whereas O
m6andm7660A O
barrage O
of O
US O
missile O
{ O
struck}m1Pakistan O
‚Äôs O
North O
Waziristan O
tribal O
district O
on O
Tuesday O
, O
killing O
at O
least O
15 O
militants O
. O
President O
Vladimir O
Putin O
sent O
his O
condolences O
to O
U.S. O
President O
Barack O
Obama O
on O
Tuesday O
over O
the O
deadly O
tornado O
that O
{ O
struck}m2Oaklahoma O
City O
. O
The O
tornado O
{ O
struck}m3the O
southern O
suburbs O
of O
the O
Oklahoma O
state O
capital O
Monday O
afternoon O
, O
killing O
at O
least O
51 O
people O
and O
injuring O
at O
least O
140 O
others O
. O
The O
French O
police O
said O
they O
were O
continuing O
to O
search O
for O
the O
man O
responsible O
for O
{ O
stabbing}m4a O
uniformed O
soldier O
in O
the O
neck O
Saturday O
evening O
. O
The O
soldier O
was O
{ O
stabbed}m5 O
in O
the O
back O
of O
the O
neck O
with O
a O
box O
cutter O
or O
short O
knife O
as O
he O
patrolled O
with O
two O
colleagues O
through O
the O
transport O
station O
of O
La O
D O
¬¥ O
efense O
, O
a O
business O
area O
in O
a O
suburb O
of O
Paris O
. O
The O
police O
suggested O
that O
the O
deed O
may O
have O
been O
inspired O
by O
the O
{ O
attack}m6on O
a O
British O
soldier O
in O
a O
London O
street O
Wednesday O
. O
A O
spokesman O
for O
the O
police O
union O
UNSA O
, O
Christophe O
Cr O
¬¥ O
epin O
, O
said O
there O
were O
similarities O
with O
the O
London O
{ O
attack}m7 O
. O
The O
case O
of O
the O
{ O
wounded}m8soldier O
, O
Pfc O
. O
C O
¬¥ O
edric O
Cordier O
, O
23 O
, O
is O
being O
handled O
by O
France O
‚Äôs O
anti O
- O
terrorism O
court O
, O
ofÔ¨Åcials O
said O
Sunday O
. O
Table O
4 O
: O
Examples O
illustrating O
the O
usefulness O
of O
topic O
modeling O
and O
salience O
- O
based O
pruning O
. O
refer O
to O
the O
attack O
on O
the O
British O
solider O
and O
form O
another O
coreference O
cluster O
. O
Without O
discourse O
- O
based O
pruning O
, O
the O
model O
mistakenly O
links O
m8withm7 O
because O
they O
both O
have O
subtype O
‚Äú O
ConÔ¨Çict O
. O
Attack O
‚Äù O
. O
In O
contrast O
, O
discourse O
- O
based O
pruning O
ranks O
m4and O
m5higher O
than O
m6andm7inm8 O
‚Äôs O
list O
of O
candidate O
antecedents O
, O
the O
reason O
being O
that O
m4,m5 O
, O
and O
m8share O
the O
same O
entity O
( O
realized O
as O
‚Äú O
a O
uniformed O
soldier O
‚Äù O
, O
‚Äú O
The O
soldier O
‚Äù O
, O
and O
‚Äú O
the O
wounded O
soldier O
‚Äù O
) O
in O
their O
contexts O
. O
Since O
the O
model O
retains O
only O
the O
top O
two O
candidate O
antecedents O
for O
English O
, O
m6and O
m7are O
being O
pruned O
, O
and O
the O
model O
successfully O
resolvesm8tom5 O
. O
5 O
Related O
Work O
Using O
topics O
and O
salience O
. O
For O
event O
coreference O
, O
the O
notion O
of O
‚Äú O
topics O
‚Äù O
has O
thus O
far O
been O
exploited O
only O
for O
cross O
- O
document O
event O
coreference O
, O
where O
documents O
are O
clustered O
by O
topics O
so O
that O
no O
cross O
- O
document O
coreference O
links O
can O
be O
established O
between O
documents O
in O
different O
clusters O
( O
Lee O
et O
al O
. O
, O
2012 O
; O
Choubey O
and O
Huang O
, O
2017 O
) O
. O
These O
resolvers O
, O
unlike O
ours O
, O
are O
pipelined O
systems O
, O
meaning O
that O
topic O
detection O
can O
inÔ¨Çuence O
event O
coreference O
resolution O
but O
not O
the O
other O
way O
round O
. O
As O
for O
discourse O
salience O
, O
we O
are O
not O
aware O
of O
any O
event O
coreference O
work O
that O
attempts O
to O
explicitly O
model O
it O
, O
although O
one O
can O
argue O
that O
existing O
systems O
may O
have O
implicitly O
encoded O
it O
in O
a O
shallow O
manner O
via O
exploiting O
features O
that O
encode O
the O
distance O
between O
two O
event O
mentions O
( O
Liu O
et O
al O
. O
, O
2014 O
; O
Cybulska O
and O
V O
ossen O
, O
2015 O
) O
. O
Computing O
argument O
compatibility O
. O
In O
addition O
to O
discourse O
- O
based O
pruning O
, O
candidate O
antecedents O
can O
be O
pruned O
based O
on O
how O
compatible O
the O
arguments O
of O
the O
two O
event O
mentions O
are O
. O
To O
capture O
argument O
compatibility O
, O
argument O
features O
have O
been O
extensively O
exploited O
. O
Basic O
features O
such O
as O
the O
number O
of O
overlapping O
arguments O
and O
the O
number O
of O
unique O
arguments O
, O
and O
a O
binary O
feature O
encoding O
whether O
arguments O
are O
conÔ¨Çictinghave O
been O
proposed O
( O
Chen O
et O
al O
. O
, O
2009 O
; O
Chen O
and O
Ji O
, O
2009 O
; O
Chen O
and O
Ng O
, O
2016 O
) O
. O
More O
sophisticated O
features O
based O
on O
different O
kinds O
of O
similarity O
measures O
have O
also O
been O
considered O
, O
such O
as O
the O
surface O
similarity O
based O
on O
Dice O
coefÔ¨Åcient O
and O
the O
WuPalmer O
WordNet O
similarity O
between O
argument O
heads O
( O
McConky O
et O
al O
. O
, O
2012 O
; O
Cybulska O
and O
V O
ossen O
, O
2013 O
; O
Araki O
et O
al O
. O
, O
2014 O
; O
Krause O
et O
al O
. O
, O
2016 O
) O
. O
These O
features O
are O
computed O
using O
either O
the O
outputs O
of O
event O
argument O
extractors O
and O
entity O
coreference O
resolvers O
( O
Ahn O
, O
2006 O
; O
Chen O
and O
Ng O
, O
2014 O
, O
2015a O
; O
Lu O
and O
Ng O
, O
2016 O
) O
or O
the O
outputs O
of O
semantic O
parsers O
( O
Bejan O
and O
Harabagiu O
, O
2014 O
; O
Yang O
et O
al O
. O
, O
2015 O
; O
Peng O
et O
al O
. O
, O
2016 O
) O
, O
and O
therefore O
suffer O
from O
error O
propagation O
( O
see O
Lu O
and O
Ng O
( O
2018 O
) O
) O
. O
Several O
previous O
works O
proposed O
joint O
models O
to O
address O
this O
problem O
( O
Lee O
et O
al O
. O
, O
2012 O
; O
Lu O
et O
al O
. O
, O
2016 O
) O
, O
while O
others O
utilized O
iterative O
methods O
to O
propagate O
argument O
information O
( O
Liu O
et O
al O
. O
, O
2014 O
; O
Choubey O
and O
Huang O
, O
2017 O
) O
in O
order O
to O
alleviate O
this O
issue O
. O
Nevertheless O
, O
argument O
extraction O
remains O
a O
very O
challenging O
task O
, O
especially O
when O
the O
arguments O
do O
not O
appear O
in O
the O
same O
sentence O
as O
the O
trigger O
. O
Our O
discourse O
- O
based O
pruning O
method O
can O
be O
thought O
of O
as O
a O
way O
of O
approximating O
argument O
compatibility O
without O
performing O
argument O
extraction O
. O
6 O
Conclusion O
We O
incorporated O
non O
- O
local O
information O
into O
a O
stateof O
- O
the O
- O
art O
joint O
model O
for O
event O
coreference O
resolution O
via O
topic O
modeling O
and O
discourse O
- O
based O
pruning O
. O
The O
resulting O
model O
not O
only O
signiÔ¨Åcantly O
outperforms O
the O
independent O
models O
but O
also O
achieves O
the O
best O
results O
to O
date O
on O
the O
KBP O
2017 O
English O
and O
Chinese O
event O
coreference O
corpora O
. O
Acknowledgments O
We O
thank O
the O
three O
anonymous O
reviewers O
for O
their O
detailed O
and O
insightful O
comments O
on O
an O
earlier O
draft O
of O
the O
paper O
. O
This O
work O
was O
supported O
in O
part O
by O
NSF O
Grants O
IIS-1528037 O
and O
CCF-1848608.661References O
David O
Ahn O
. O
2006 O
. O
The O
stages O
of O
event O
extraction O
. O
InProceedings O
of O
the O
COLING O
/ O
ACL O
Workshop O
on O
Annotating O
and O
Reasoning O
about O
Time O
and O
Events O
, O
pages O
1‚Äì8 O
. O
Jun O
Araki O
, O
Zhengzhong O
Liu O
, O
Eduard O
Hovy O
, O
and O
Teruko O
Mitamura O
. O
2014 O
. O
Detecting O
subevent O
structure O
for O
event O
coreference O
resolution O
. O
In O
Proceedings O
of O
the O
9th O
International O
Conference O
on O
Language O
Resources O
and O
Evaluation O
, O
pages O
4553‚Äì4558 O
. O
Amit O
Bagga O
and O
Breck O
Baldwin O
. O
1998 O
. O
Algorithms O
for O
scoring O
coreference O
chains O
. O
In O
Proceedings O
of O
the O
LREC O
Workshop O
on O
Linguistic O
Coreference O
, O
pages O
563‚Äì566 O
. O
Cosmin O
Adrian O
Bejan O
and O
Sanda O
Harabagiu O
. O
2014 O
. O
Unsupervised O
event O
coreference O
resolution O
. O
Computational O
Linguistics O
, O
40(2):311‚Äì347 O
. O
Chen O
Chen O
and O
Vincent O
Ng O
. O
2014 O
. O
SinoCoreferencer O
: O
An O
end O
- O
to O
- O
end O
Chinese O
event O
coreference O
resolver O
. O
InProceedings O
of O
the O
9th O
International O
Conference O
on O
Language O
Resources O
and O
Evaluation O
, O
pages O
4532‚Äì4538 O
. O
Chen O
Chen O
and O
Vincent O
Ng O
. O
2015a O
. O
Chinese O
event O
coreference O
resolution O
: O
An O
unsupervised O
probabilistic O
model O
rivaling O
supervised O
resolvers O
. O
In O
Proceedings O
of O
the O
2015 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
pages O
1097‚Äì1107 O
. O
Chen O
Chen O
and O
Vincent O
Ng O
. O
2015b O
. O
Chinese O
zero O
pronoun O
resolution O
: O
A O
joint O
unsupervised O
discourseaware O
model O
rivaling O
state O
- O
of O
- O
the O
- O
art O
resolvers O
. O
In O
Proceedings O
of O
the O
53rd O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
7th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
( O
Volume O
2 O
: O
Short O
Papers O
) O
, O
pages O
320 O
‚Äì O
326 O
. O
Chen O
Chen O
and O
Vincent O
Ng O
. O
2016 O
. O
Joint O
inference O
over O
a O
lightly O
supervised O
information O
extraction O
pipeline O
: O
Towards O
event O
coreference O
resolution O
for O
resourcescarce O
languages O
. O
In O
Proceedings O
of O
the O
30th O
AAAI O
Conference O
on O
ArtiÔ¨Åcial O
Intelligence O
, O
pages O
2913 O
‚Äì O
2920 O
. O
Zheng O
Chen O
and O
Heng O
Ji O
. O
2009 O
. O
Graph O
- O
based O
event O
coreference O
resolution O
. O
In O
Proceedings O
of O
the O
2009 O
Workshop O
on O
Graph O
- O
based O
Methods O
for O
Natural O
Language O
Processing O
, O
pages O
54‚Äì57 O
. O
Zheng O
Chen O
, O
Heng O
Ji O
, O
and O
Robert O
Haralick O
. O
2009 O
. O
A O
pairwise O
event O
coreference O
model O
, O
feature O
impact O
and O
evaluation O
for O
event O
coreference O
resolution O
. O
InProceedings O
of O
the O
International O
Workshop O
on O
Events O
in O
Emerging O
Text O
Types O
, O
pages O
17‚Äì22.Prafulla O
Kumar O
Choubey O
and O
Ruihong O
Huang O
. O
2017 O
. O
Event O
coreference O
resolution O
by O
iteratively O
unfolding O
inter O
- O
dependencies O
among O
events O
. O
In O
Proceedings O
of O
the O
2017 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
2124‚Äì2133 O
. O
Agata O
Cybulska O
and O
Piek O
V O
ossen O
. O
2013 O
. O
Semantic O
relations O
between O
events O
and O
their O
time O
, O
locations O
and O
participants O
for O
event O
coreference O
resolution O
. O
In O
Proceedings O
of O
the O
International O
Conference O
Recent O
Advances O
in O
Natural O
Language O
Processing O
, O
pages O
156 O
‚Äì O
163 O
. O
Agata O
Cybulska O
and O
Piek O
V O
ossen O
. O
2015 O
. O
Translating O
granularity O
of O
event O
slots O
into O
features O
for O
event O
coreference O
resolution O
. O
In O
Proceedings O
of O
the O
3rd O
Workshop O
on O
EVENTS O
, O
pages O
1‚Äì10 O
. O
John O
Duchi O
, O
Elad O
Hazan O
, O
and O
Yoram O
Singer O
. O
2011 O
. O
Adaptive O
subgradient O
methods O
for O
online O
learning O
and O
stochastic O
optimization O
. O
Journal O
of O
Machine O
Learning O
Research O
, O
12:2121‚Äì2159 O
. O
Greg O
Durrett O
and O
Dan O
Klein O
. O
2013 O
. O
Easy O
victories O
and O
uphill O
battles O
in O
coreference O
resolution O
. O
In O
Proceedings O
of O
the O
2013 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
1971‚Äì1982 O
. O
Yin O
Jou O
Huang O
, O
Jing O
Lu O
, O
Sadao O
Kurohashi O
, O
and O
Vincent O
Ng O
. O
2019 O
. O
Improving O
event O
coreference O
resolution O
by O
learning O
argument O
compatibility O
from O
unlabeled O
data O
. O
In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
785 O
‚Äì O
795 O
. O
Sebastian O
Krause O
, O
Feiyu O
Xu O
, O
Hans O
Uszkoreit O
, O
and O
Dirk O
Weissenborn O
. O
2016 O
. O
Event O
linking O
with O
sentential O
features O
from O
convolutional O
neural O
networks O
. O
In O
Proceedings O
of O
the O
20th O
Conference O
on O
Computational O
Natural O
Language O
Learning O
, O
pages O
239‚Äì249 O
. O
Heeyoung O
Lee O
, O
Marta O
Recasens O
, O
Angel O
Chang O
, O
Mihai O
Surdeanu O
, O
and O
Dan O
Jurafsky O
. O
2012 O
. O
Joint O
entity O
and O
event O
coreference O
resolution O
across O
documents O
. O
In O
Proceedings O
of O
the O
2012 O
Joint O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
and O
Computational O
Natural O
Language O
Learning O
, O
pages O
489‚Äì500 O
. O
Zhengzhong O
Liu O
, O
Jun O
Araki O
, O
Eduard O
Hovy O
, O
and O
Teruko O
Mitamura O
. O
2014 O
. O
Supervised O
within O
- O
document O
event O
coreference O
using O
information O
propagation O
. O
In O
Proceedings O
of O
the O
9th O
International O
Conference O
on O
Language O
Resources O
and O
Evaluation O
, O
pages O
4539‚Äì4544 O
. O
Jing O
Lu O
and O
Vincent O
Ng O
. O
2016 O
. O
Event O
coreference O
resolution O
with O
multi O
- O
pass O
sieves O
. O
In O
Proceedings O
of O
the O
10th O
International O
Conference O
on O
Language O
Resources O
and O
Evaluation O
. O
Jing O
Lu O
and O
Vincent O
Ng O
. O
2017a O
. O
Joint O
learning O
for O
event O
coreference O
resolution O
. O
In O
Proceedings O
of O
the O
55th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
90‚Äì101.662Jing O
Lu O
and O
Vincent O
Ng O
. O
2017b O
. O
UTD O
‚Äôs O
event O
nugget O
detection O
and O
coreference O
system O
at O
KBP O
2017 O
. O
In O
Proceedings O
of O
the O
2017 O
Text O
Analysis O
Conference O
. O
Jing O
Lu O
and O
Vincent O
Ng O
. O
2018 O
. O
Event O
coreference O
resolution O
: O
A O
survey O
of O
two O
decades O
of O
research O
. O
In O
Proceedings O
of O
the O
27th O
International O
Joint O
Conference O
on O
ArtiÔ¨Åcial O
Intelligence O
, O
pages O
5479‚Äì5486 O
. O
Jing O
Lu O
, O
Deepak O
Venugopal O
, O
Vibhav O
Gogate O
, O
and O
Vincent O
Ng O
. O
2016 O
. O
Joint O
inference O
for O
event O
coreference O
resolution O
. O
In O
Proceedings O
of O
the O
26th O
International O
Conference O
on O
Computational O
Linguistics O
, O
pages O
3264‚Äì3275 O
. O
Xiaoqiang O
Luo O
. O
2005 O
. O
On O
coreference O
resolution O
performance O
metrics O
. O
In O
Proceedings O
of O
the O
Human O
Language O
Technology O
Conference O
and O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
25‚Äì32 O
. O
Christopher O
D. O
Manning O
, O
Mihai O
Surdeanu O
, O
John O
Bauer O
, O
Jenny O
Finkel O
, O
Steven O
J. O
Bethard O
, O
and O
David O
McClosky O
. O
2014 O
. O
The O
Stanford O
CoreNLP O
natural O
language O
processing O
toolkit O
. O
In O
Proceedings O
of O
the O
52nd O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
System O
Demonstrations O
, O
pages O
55‚Äì60 O
. O
Andrew O
Kachites O
McCallum O
. O
2002 O
. O
MALLET O
: O
A O
machine O
learning O
for O
language O
toolkit O
. O
http://www.cs O
. O
umass.edu/ O
‚àºmccallum O
/ O
mallet O
. O
Katie O
McConky O
, O
Rakesh O
Nagi O
, O
Moises O
Sudit O
, O
and O
William O
Hughes O
. O
2012 O
. O
Improving O
event O
coreference O
by O
context O
extraction O
and O
dynamic O
feature O
weighting O
. O
In O
Proceedings O
of O
the O
2012 O
IEEE O
International O
Multi O
- O
Disciplinary O
Conference O
on O
Cognitive O
Methods O
in O
Situation O
Awareness O
and O
Decision O
Support O
, O
pages O
38‚Äì43 O
. O
Thien O
Huu O
Nguyen O
, O
Kyunghyun O
Cho O
, O
and O
Ralph O
Grishman O
. O
2016 O
. O
Joint O
event O
extraction O
via O
recurrent O
neural O
networks O
. O
In O
Proceedings O
of O
the O
2016 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
pages O
300‚Äì309 O
. O
Haoruo O
Peng O
, O
Yangqiu O
Song O
, O
and O
Dan O
Roth O
. O
2016 O
. O
Event O
detection O
and O
co O
- O
reference O
with O
minimal O
supervision O
. O
In O
Proceedings O
of O
the O
2016 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
392‚Äì402 O
. O
Daniel O
Ramage O
, O
David O
Hall O
, O
Ramesh O
Nallapati O
, O
and O
Christopher O
D. O
Manning O
. O
2009 O
. O
Labeled O
LDA O
: O
A O
supervised O
topic O
model O
for O
credit O
attribution O
in O
multilabeled O
corpora O
. O
In O
Proceedings O
of O
the O
2009 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
248‚Äì256 O
. O
Marta O
Recasens O
and O
Eduard O
Hovy O
. O
2011 O
. O
BLANC O
: O
Implementing O
the O
Rand O
Index O
for O
coreference O
evaluation O
. O
Natural O
Language O
Engineering O
, O
17(4):485 O
‚Äì O
510.Marc O
Vilain O
, O
John O
Burger O
, O
John O
Aberdeen O
, O
Dennis O
Connolly O
, O
and O
Lynette O
Hirschman O
. O
1995 O
. O
A O
modeltheoretic O
coreference O
scoring O
scheme O
. O
In O
Proceedings O
of O
the O
Sixth O
Message O
Understanding O
Conference O
, O
pages O
45‚Äì52 O
. O
Bishan O
Yang O
, O
Claire O
Cardie O
, O
and O
Peter O
Frazier O
. O
2015 O
. O
A O
hierarchical O
distance O
- O
dependent O
Bayesian O
model O
for O
event O
coreference O
resolution O
. O
Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
3:517‚Äì528.663Proceedings O
of O
the O
1st O
Conference O
of O
the O
Asia O
- O
PaciÔ¨Åc O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
10th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
664‚Äì671 O
December O
4 O
- O
7 O
, O
2020 O
. O
¬© O
2020 O
Association O
for O
Computational O
Linguistics O
Neural O
RST O
- O
based O
Evaluation O
of O
Discourse O
Coherence O
Grigorii O
Guz‚àó1 O
, O
Peyman O
Bateni‚àó1,2 O
, O
Darius O
Muglich1 O
, O
Giuseppe O
Carenini1 O
University O
of O
British O
Columbia1 O
, O
Inverted O
AI2 O
{ O
g.guz@cs O
, O
pbateni@cs O
, O
darius.muglich@alumni O
, O
carenini@cs O
} O
.ubc.ca O
Abstract O
This O
paper O
evaluates O
the O
utility O
of O
Rhetorical O
Structure O
Theory O
( O
RST O
) O
trees O
and O
relations O
in O
discourse O
coherence O
evaluation O
. O
We O
show O
that O
incorporating O
silver O
- O
standard O
RST O
features O
can O
increase O
accuracy O
when O
classifying O
coherence O
. O
We O
demonstrate O
this O
through O
our O
tree O
- O
recursive O
neural O
model O
, O
namely O
RST O
- O
Recursive O
, O
which O
takes O
advantage O
of O
the O
text O
‚Äôs O
RST O
features O
produced O
by O
a O
state O
of O
the O
art O
RST O
parser O
. O
We O
evaluate O
our O
approach O
on O
the O
Grammarly O
Corpus O
for O
Discourse O
Coherence O
( O
GCDC O
) O
and O
show O
that O
when O
ensembled O
with O
the O
current O
state O
of O
the O
art O
, O
we O
can O
achieve O
the O
new O
state O
of O
the O
art O
accuracy O
on O
this O
benchmark O
. O
Furthermore O
, O
when O
deployed O
alone O
, O
RST O
- O
Recursive O
achieves O
competitive O
accuracy O
while O
having O
62 O
% O
fewer O
parameters O
. O
1 O
Introduction O
Discourse O
coherence O
has O
been O
the O
subject O
of O
much O
research O
in O
Computational O
Linguistics O
thanks O
to O
its O
widespread O
applications O
( O
Lai O
and O
Tetreault O
, O
2018 O
) O
. O
Most O
current O
methods O
can O
be O
described O
as O
either O
stemming O
from O
explicit O
representations O
based O
on O
the O
Centering O
Theory O
( O
Grosz O
et O
al O
. O
, O
1994 O
) O
, O
or O
deep O
learning O
approaches O
that O
learn O
without O
the O
use O
of O
hand O
- O
crafted O
linguistic O
features O
. O
Our O
work O
explores O
a O
third O
research O
avenue O
based O
on O
the O
Rhetorical O
Structure O
Theory O
( O
RST O
) O
( O
Mann O
and O
Thompson O
, O
1988 O
) O
. O
We O
hypothesize O
that O
texts O
of O
low O
/ O
high O
coherence O
tend O
to O
adhere O
to O
different O
discourse O
structures O
. O
Thus O
, O
we O
pose O
that O
using O
even O
silver O
- O
standard O
RST O
features O
should O
help O
in O
separating O
coherent O
texts O
from O
incoherent O
ones O
. O
This O
stems O
from O
the O
deÔ¨Ånition O
of O
the O
coherence O
itself O
as O
the O
writer O
of O
a O
document O
needs O
to O
follow O
speciÔ¨Åc O
rules O
for O
building O
a O
clear O
narrative O
or O
argument O
structure O
in O
which O
the O
role O
of O
each O
constituent O
of O
the O
document O
should O
be O
appropriate O
with O
respect O
‚àóAuthors O
contributed O
equally O
œÉœÉœÉtanhxh_left O
, O
r_left O
œÉh_right O
, O
r_right O
c_leftc_rightxx+xtanhc_out O
h_outf_leftf_rightinputupdateoutput O
people O
think O
he O
is O
smartbecause O
he O
did O
well O
in O
schoolLSTMLSTM O
ENSYou O
know O
Mark O
, O
BNSLSTMTree O
LSTMTree O
LSTM O
... O
R O
......... O
RST O
TreeRST O
- O
RecursiveNN O
RST O
NetworkRST O
Networkh_lefth_rightFCSoftmax O
Classifier O
Right O
BranchLeft O
BranchCoherence O
EmbeddingCoherence O
Level O
EvidenceBackgroundFigure O
1 O
: O
Overview O
of O
RST O
- O
Recursive O
; O
EDU O
embeddings O
are O
generated O
for O
the O
leaf O
nodes O
using O
the O
EDU O
network O
. O
Subsequently O
, O
the O
RST O
tree O
is O
recursively O
traversed O
bottom O
- O
up O
using O
the O
RST O
network O
. O
to O
its O
local O
and O
global O
context O
, O
and O
even O
existing O
discourse O
parsers O
should O
be O
able O
to O
predict O
a O
plausible O
structure O
that O
is O
consistent O
across O
all O
coherent O
documents O
. O
However O
, O
if O
a O
parser O
has O
difÔ¨Åculty O
interpreting O
a O
given O
document O
, O
it O
will O
be O
more O
likely O
to O
produce O
unrealistic O
trees O
with O
improbable O
patterns O
of O
discourse O
relations O
between O
constituents O
. O
This O
idea O
was O
Ô¨Årst O
explored O
by O
Feng O
et O
al O
. O
( O
2014 O
) O
, O
who O
followed O
an O
approach O
similar O
to O
Barzilay O
and O
Lapata O
( O
2008 O
) O
by O
estimating O
entity O
transition O
likelihoods O
, O
but O
instead O
using O
discourse O
relations O
( O
predicted O
by O
a O
state O
of O
the O
art O
discourse O
parser O
( O
Feng O
and O
Hirst O
, O
2014 O
) O
) O
that O
entities O
participate O
in O
as O
opposed O
to O
their O
grammatical O
roles O
. O
Their O
method O
achieved O
signiÔ¨Åcant O
improvements O
in O
performance O
even O
when O
using O
silver O
- O
standard O
discourse O
trees O
, O
showing O
potential O
in O
the O
use O
of O
parsed O
RST O
features O
for O
classifying O
textual O
coherence O
. O
Our O
work O
, O
however O
, O
is O
the O
Ô¨Årst O
to O
develop O
and O
test O
a O
neural O
approach O
to O
leveraging O
RST O
discourse O
representations O
in O
coherence O
evaluation O
. O
Furthermore O
, O
Feng O
et O
al O
. O
( O
2014 O
) O
only O
tested O
their O
proposal O
on O
the664œÉœÉœÉtanhxœÉc_leftc_rightxx+tanhc_out O
h_outf_leftf_rightinputupdateoutput O
people O
think O
he O
is O
smartbecause O
he O
did O
well O
in O
schoolLSTMLSTM O
ENSYou O
know O
Mark O
, O
BNSLSTMTree O
LSTMTree O
LSTM O
... O
R O
......... O
RST O
TreeRST O
- O
RecursiveNN O
RST O
NetworkRST O
Networkh_lefth_rightFCSoftmax O
Classifier O
Right O
BranchLeft O
BranchCoherence O
EmbeddingCoherence O
Level O
EvidenceBackgroundh_left O
, O
r_left O
h_right O
, O
r_right O
xFigure O
2 O
: O
Recursive O
LSTM O
architecture O
used O
in O
RSTRecursive O
adapted O
from O
( O
Tai O
et O
al O
. O
, O
2015 O
) O
. O
sentence O
permutation O
task O
, O
which O
involves O
ranking O
a O
sentence O
- O
permuted O
text O
against O
the O
original O
. O
As O
noted O
by O
Lai O
and O
Tetreault O
( O
2018 O
) O
, O
this O
is O
not O
an O
accurate O
proxy O
for O
realistic O
coherence O
evaluation O
. O
We O
evaluate O
our O
method O
on O
their O
more O
realistic O
Grammarly O
Corpus O
Of O
Discourse O
Coherence O
( O
GCDC O
) O
, O
where O
the O
model O
needs O
to O
classify O
a O
naturally O
produced O
text O
into O
one O
of O
three O
levels O
of O
coherence O
. O
Our O
contributions O
involve O
: O
( O
1)RST O
- O
Recursive O
, O
an O
RST O
- O
based O
neural O
tree O
- O
recursive O
method O
for O
coherence O
evaluation O
that O
achieves O
2 O
% O
below O
the O
state O
of O
the O
art O
performance O
on O
the O
GCDC O
while O
having O
62 O
% O
fewer O
parameters O
. O
( O
2)When O
ensembled O
with O
the O
current O
state O
of O
the O
art O
, O
namely O
Parseq O
( O
Lai O
and O
Tetreault O
, O
2018 O
) O
, O
we O
achieve O
a O
notable O
improvement O
over O
the O
plain O
ParSeq O
model O
. O
( O
3)We O
demonstrate O
the O
usefulness O
of O
silver O
- O
standard O
RST O
features O
in O
coherence O
classiÔ¨Åcation O
, O
and O
establish O
our O
results O
as O
a O
lower O
- O
bound O
for O
performance O
improvements O
to O
be O
gained O
using O
RST O
features O
. O
2 O
Related O
Work O
2.1 O
Coherence O
Evaluation O
of O
Text O
Centering O
Theory O
( O
Grosz O
et O
al O
. O
, O
1994 O
) O
states O
that O
subsequent O
sentences O
in O
coherent O
texts O
are O
likely O
to O
continue O
to O
focus O
on O
the O
same O
entities O
( O
i.e. O
, O
subjects O
, O
objects O
, O
etc O
. O
) O
as O
within O
the O
previous O
sentences O
. O
Building O
on O
top O
of O
this O
, O
Barzilay O
and O
Lapata O
( O
2008 O
) O
were O
the O
Ô¨Årst O
to O
propose O
the O
Entity O
- O
Grid O
model O
that O
constructs O
a O
two O
- O
dimensional O
array O
Gn O
, O
mfor O
a O
text O
ofnsentences O
and O
mentities O
, O
which O
are O
used O
to O
estimate O
transition O
probabilities O
for O
entity O
occurrence O
patterns O
. O
More O
recently O
, O
Elsner O
and O
Charniak O
( O
2011 O
) O
extended O
Entity O
- O
Grid O
using O
entity O
- O
speciÔ¨Åc O
features O
, O
while O
Tien O
Nguyen O
and O
Joty O
( O
2017 O
) O
used O
a O
Convolutional O
Neural O
Network O
( O
CNN O
) O
on O
top O
of O
Entity O
- O
Grid O
to O
learn O
more O
hierarchical O
patterns O
. O
On O
the O
other O
hand O
, O
feature O
- O
free O
deep O
neural O
techniques O
have O
dominated O
recent O
research O
. O
Li O
and O
Jurafsky O
( O
2017 O
) O
applied O
Recurrent O
Neural O
Networks O
( O
RNNs O
) O
to O
model O
the O
coherent O
generation O
of O
the O
œÉœÉœÉtanhxh_leftXœÉh_rightc_leftc_rightxx+xtanhc_out O
h_outf_leftf_rightinputupdateoutput O
people O
think O
he O
is O
smartbecause O
he O
did O
well O
in O
schoolEDU O
NetworkEDU O
Network O
ENSYou O
know O
Mark O
, O
BNSEDU O
NetworkRST O
NetworkRST O
Network O
... O
R O
......... O
RST O
TreeRST O
- O
RecursiveNN O
RST O
NetworkRST O
Networkh_lefth_rightFCSoftmax O
Classifier O
Right O
BranchLeft O
BranchCoherence O
EmbeddingCoherence O
LevelFigure O
3 O
: O
Overview O
of O
the O
classiÔ¨Åcation O
layer O
in O
RSTRecursive O
; O
At O
the O
root O
of O
the O
RST O
tree O
, O
children O
‚Äôs O
hidden O
states O
are O
concatenated O
to O
form O
the O
document O
representation O
d= O
[ O
hl O
, O
hr]which O
is O
then O
transformed O
into O
a O
3 O
- O
dimensional O
vector O
of O
Softmax O
probabilities O
. O
next O
sentence O
given O
the O
current O
sentence O
and O
viceversa O
. O
Mesgar O
and O
Strube O
( O
2018 O
) O
constructed O
a O
local O
coherence O
model O
that O
encodes O
patterns O
of O
changes O
on O
how O
adjacent O
sentences O
within O
the O
text O
are O
semantically O
related O
. O
Recently O
, O
Moon O
et O
al O
. O
( O
2019 O
) O
used O
a O
multi O
- O
component O
model O
to O
capture O
both O
local O
and O
global O
coherence O
perturbations O
. O
Lai O
and O
Tetreault O
( O
2018 O
) O
developed O
a O
hierarchical O
neural O
architecture O
named O
ParSeq O
with O
three O
stacked O
LSTM O
Networks O
, O
designed O
to O
encode O
the O
coherence O
at O
sentence O
, O
paragraph O
and O
document O
levels O
. O
2.2 O
Rhetorical O
Structure O
Theory O
( O
RST O
) O
RST O
describes O
the O
structure O
of O
a O
text O
in O
the O
following O
way O
: O
Ô¨Årst O
, O
the O
text O
is O
segmented O
into O
elementary O
discourse O
units O
( O
EDUs O
) O
, O
which O
describe O
spans O
of O
text O
constituting O
clauses O
or O
clause O
- O
like O
units O
( O
Mann O
and O
Thompson O
, O
1988 O
) O
. O
Second O
, O
the O
EDUs O
are O
recursively O
structured O
into O
a O
tree O
hierarchy O
where O
each O
node O
deÔ¨Ånes O
an O
RST O
relation O
between O
the O
constituting O
sub O
- O
trees O
. O
The O
sub O
- O
tree O
with O
the O
central O
purpose O
is O
called O
the O
nucleus O
, O
and O
the O
one O
bearing O
secondary O
intent O
is O
called O
the O
satellite O
while O
a O
connective O
discourse O
relation O
is O
assigned O
to O
both O
. O
An O
example O
of O
a O
‚Äú O
nucleus O
- O
satellite O
‚Äù O
relation O
pairing O
is O
presented O
in O
Figure O
1 O
where O
a O
claim O
is O
followed O
by O
the O
evidence O
for O
the O
claim O
; O
RST O
posits O
an O
‚Äú O
Evidence O
‚Äù O
relation O
between O
these O
two O
spans O
with O
the O
left O
sub O
- O
tree O
being O
the O
‚Äú O
nucleus O
‚Äù O
and O
the O
right O
sub O
- O
tree O
as O
‚Äú O
satellite O
‚Äù O
. O
3 O
Method O
3.1 O
RST O
- O
Recursive O
We O
parse O
silver O
- O
standard O
RST O
trees O
for O
documents O
using O
the O
CODRA O
( O
Joty O
et O
al O
. O
, O
2015 O
) O
RST O
parser O
, O
which O
we O
then O
employ O
as O
input O
to O
our O
recursive O
neural O
model O
, O
RST O
- O
Recursive O
. O
The O
overall O
procedure665for O
RST O
- O
Recursive O
is O
shown O
in O
Figure O
1 O
. O
Given O
a O
document O
of O
nEDUsE1 O
: O
nwith O
each O
EDUEirepresented O
as O
a O
list O
of O
GloVe O
embeddings O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
, O
we O
use O
an O
LSTM O
to O
process O
each O
Ei O
, O
using O
the O
Ô¨Ånal O
hidden O
state O
as O
the O
EDU O
embedding O
ei O
= O
LSTM O
( O
Ei)for O
each O
leaf O
iof O
the O
document O
‚Äôs O
RST O
tree O
. O
Afterwards O
, O
we O
apply O
a O
recursive O
LSTM O
architecture O
( O
Figure O
2 O
) O
that O
traverses O
the O
RST O
tree O
bottom O
- O
up O
. O
At O
each O
node O
s O
, O
we O
use O
the O
children O
‚Äôs O
sub O
- O
tree O
embeddings O
[ O
hl O
, O
cl O
, O
rl]and[hr O
, O
cr O
, O
rr]to O
form O
the O
node O
‚Äôs O
sub O
- O
tree O
embedding O
: O
[ O
hs O
, O
cs O
] O
= O
TreeLSTM O
( O
[ O
hl O
, O
cl O
, O
rl],[hr O
, O
cr O
, O
rr O
] O
) O
( O
1 O
) O
where O
hl O
/ O
clandhr O
/ O
crare O
the O
LSTM O
hidden O
and O
cell O
states O
from O
the O
left O
and O
right O
sub O
- O
trees O
respectively O
. O
The O
relation O
embeddings O
of O
the O
children O
sub O
- O
trees O
, O
rlandrr O
, O
are O
learned O
vector O
embeddings O
for O
each O
of O
the O
31 O
pre O
- O
deÔ¨Åned O
relation O
labels O
in O
the O
form O
of O
‚Äú O
[ O
relation O
] O
[ O
nucleus O
/ O
satellite O
] O
‚Äù O
( O
e.g. O
, O
‚Äú O
Evidence O
Satellite O
‚Äù O
for O
the O
last O
EDU O
in O
Figure O
1 O
) O
. O
At O
the O
root O
of O
the O
tree O
, O
the O
output O
hidden O
states O
from O
both O
children O
are O
concatenated O
into O
a O
single O
document O
embedding O
d= O
[ O
hl O
, O
hr O
] O
. O
As O
shown O
in O
Figure O
3 O
, O
a O
fully O
connected O
layer O
is O
applied O
to O
this O
representation O
before O
using O
a O
Softmax O
function O
to O
obtain O
the O
coherence O
class O
probabilities O
. O
3.2 O
Ensemble O
: O
ParSeq O
+ O
RST O
- O
Recursive O
To O
evaluate O
if O
the O
addition O
of O
silver O
- O
standard O
RST O
features O
to O
existing O
methods O
can O
improve O
coherence O
evaluation O
, O
we O
ensemble O
RST O
- O
Recursive O
with O
the O
current O
state O
of O
the O
art O
coherence O
classiÔ¨Åer O
: O
ParSeq O
. O
A O
deep O
learned O
non O
- O
linguistic O
classiÔ¨Åer O
, O
ParSeq O
employs O
three O
layers O
of O
LSTMs O
that O
intend O
to O
capture O
coherence O
at O
different O
granularities O
. O
An O
overview O
of O
the O
ParSeq O
architecture O
is O
presented O
in O
Figure O
4 O
. O
First O
, O
LSTM O
1(not O
shown O
) O
produces O
a O
single O
sentence O
embedding O
for O
each O
sentence O
in O
the O
text O
. O
Next O
, O
LSTM O
2generates O
paragraph O
embeddings O
using O
the O
corresponding O
sentence O
embeddings O
from O
LSTM O
1 O
. O
Finally O
, O
LSTM O
3reads O
the O
paragraph O
embeddings O
, O
generating O
the O
Ô¨Ånal O
document O
embedding O
, O
which O
is O
passed O
to O
a O
fully O
connected O
layer O
to O
produce O
Softmax O
label O
probabilities O
. O
In O
this O
augmented O
variation O
of O
our O
model O
, O
we O
operate O
ParSeq O
on O
the O
document O
independently O
until O
a O
document O
level O
embedding O
dpis O
obtained O
at O
the O
highest O
- O
level O
LSTM O
. O
This O
document O
embedding O
is O
then O
concatenated O
to O
the O
RST O
- O
Recursive O
coherence O
embedding O
d= O
[ O
hl O
, O
hr O
, O
dparseq O
] O
in O
Figure O
Figure O
4 O
: O
The O
architectural O
overview O
of O
ParSeq O
; O
an O
illustration O
of O
ParSeq O
‚Äôs O
structure O
, O
taken O
directly O
from O
the O
original O
paper O
( O
Lai O
and O
Tetreault O
, O
2018 O
) O
. O
3 O
to O
produce O
class O
probabilities O
. O
Note O
that O
in O
this O
ensemble O
variation O
, O
we O
initialize O
tree O
leaves O
e1 O
: O
n O
with O
zero O
- O
vectors O
as O
opposed O
to O
EDU O
embeddings O
since O
ParSeq O
is O
sufÔ¨Åciently O
capable O
of O
capturing O
semantic O
information O
on O
its O
own O
, O
and O
early O
experiments O
using O
5 O
- O
fold O
cross O
- O
validation O
on O
the O
training O
set O
revealed O
model O
overÔ¨Åtting O
when O
training O
with O
EDU O
embeddings O
simultaneously O
. O
4 O
Experiments O
4.1 O
Dataset O
We O
evaluate O
RST O
- O
Recursive O
and O
Ensemble O
on O
the O
GCDC O
dataset O
( O
Lai O
and O
Tetreault O
, O
2018 O
) O
. O
This O
dataset O
consists O
of O
4 O
separate O
sub O
- O
datasets O
: O
Clinton O
emails O
, O
Enron O
emails O
, O
Yahoo O
answers O
, O
and O
Yelp O
reviews O
, O
each O
containing O
1000 O
documents O
for O
training O
and O
200 O
documents O
for O
testing O
. O
Each O
document O
is O
assigned O
a O
discrete O
coherence O
label O
of O
incoherent O
( O
1 O
) O
, O
neutral O
( O
2 O
) O
, O
and O
coherent O
( O
3 O
) O
. O
We O
parse O
RST O
trees O
for O
each O
example O
within O
the O
GCDC O
dataset O
using O
CODRA O
( O
Joty O
et O
al O
. O
, O
2015 O
) O
. O
Due O
to O
CODRA O
‚Äôs O
imperfect O
parsing O
of O
documents O
, O
RST O
trees O
could O
not O
be O
obtained O
for O
approximately O
1.5%-2 O
% O
of O
the O
documents O
, O
which O
were O
then O
excluded O
from O
the O
study O
. O
In O
addition O
, O
we O
re O
- O
evaluated O
ParSeq O
on O
only O
the O
RST O
- O
parsed O
portion O
of O
documents O
to O
assure O
consistent O
comparability O
of O
results O
. O
For O
more O
details O
, O
see O
Appendix O
A O
/ O
B. O
Our O
code O
and O
dataset O
can O
be O
accessed O
below1 O
, O
and O
the O
access O
to O
the O
original O
GCDC O
corpus O
can O
be O
obtained O
here2 O
. O
We O
can O
share O
RST O
- O
parsings O
of O
GCDC O
examples O
with O
interested O
readers O
upon O
request O
once O
access O
to O
the O
GCDC O
dataset O
has O
also O
been O
obtained O
. O
1https://github.com/grig-guz/coherence-rst O
2https://github.com/aylai/GCDC-corpus666MODEL O
T O
NS O
R O
E O
CLINTON O
ENRON O
YAHOO O
YELP O
AVERAGE O
MAJORITY O
55.33 O
44.39 O
38.02 O
54.82 O
48.14 O
RST O
- O
R O
EC O
/ O
check O
55.33¬±0.00 O
44.39¬±0.00 O
38.02¬±0.00 O
54.82¬±0.00 O
48.14¬±0.00 O
RST O
- O
R O
EC O
/ O
check O
/check O
53.74¬±0.14 O
44.67¬±0.07 O
44.61¬±0.09 O
53.76¬±0.11 O
49.20¬±0.07 O
RST O
- O
R O
EC O
/ O
check O
/check O
/check O
54.07¬±0.10 O
43.99¬±0.07 O
49.39¬±0.10 O
54.39¬±0.12 O
50.46¬±0.05 O
RST O
- O
R O
EC O
/ O
check O
/check O
/check O
/check O
55.70¬±0.08 O
53.86¬±0.11 O
50.92¬±0.13 O
51.70¬±0.16 O
53.04¬±0.09 O
PARSEQ O
61.05¬±0.13 O
54.23¬±0.10 O
53.29¬±0.14 O
51.76¬±0.21 O
55.09¬±0.09 O
ENSEMBLE O
/check O
* O
61.12¬±0.13 O
54.20¬±0.12 O
52.87¬±0.16 O
51.52¬±0.22 O
54.93¬±0.10 O
ENSEMBLE O
/check O
/check O
* O
60.82¬±0.13 O
54.01¬±0.10 O
52.92¬±0.15 O
51.63¬±0.24 O
54.85¬±0.10 O
ENSEMBLE O
/check O
/check O
/check O
* O
61.17¬±0.12 O
53.99¬±0.10 O
53.99¬±0.14 O
52.40¬±0.21 O
55.39¬±0.09 O
Table O
1 O
: O
Overall O
and O
sub O
- O
dataset O
speciÔ¨Åc O
coherence O
classiÔ¨Åcation O
accuracy O
on O
the O
GCDC O
dataset O
. O
Error O
boundaries O
describe O
95 O
% O
conÔ¨Ådence O
intervals O
. O
Values O
in O
bold O
describe O
statistically O
signiÔ¨Åcant O
state O
of O
the O
art O
performance O
. O
* O
indicates O
availability O
of O
EDU O
- O
level O
semantic O
information O
through O
the O
ensembling O
with O
ParSeq O
. O
MODEL O
T O
NS O
R O
E O
CLINTON O
ENRON O
YAHOO O
YELP O
AVERAGE O
MAJORITY O
39.42 O
27.29 O
20.95 O
38.82 O
31.62 O
RST O
- O
R O
EC O
/ O
check O
39.42¬±0.00 O
27.29¬±0.00 O
20.95¬±0.00 O
38.82¬±0.00 O
31.62¬±0.00 O
RST O
- O
R O
EC O
/ O
check O
/check O
39.20¬±0.03 O
30.81¬±0.16 O
35.67¬±0.18 O
39.93¬±0.08 O
36.40¬±0.09 O
RST O
- O
R O
EC O
/ O
check O
/check O
/check O
41.08¬±0.07 O
31.21¬±0.13 O
41.97¬±0.14 O
42.27¬±0.09 O
39.13¬±0.08 O
RST O
- O
R O
EC O
/ O
check O
/check O
/check O
/check O
45.90¬±0.12 O
44.33¬±0.16 O
43.85¬±0.18 O
43.13¬±0.10 O
44.30¬±0.08 O
PARSEQ O
52.12¬±0.21 O
44.90¬±0.15 O
46.22¬±0.18 O
43.36¬±0.09 O
46.65¬±0.10 O
ENSEMBLE O
/check O
* O
52.35¬±0.22 O
44.92¬±0.16 O
45.48¬±0.22 O
43.70¬±0.11 O
46.61¬±0.11 O
ENSEMBLE O
/check O
/check O
* O
51.90¬±0.22 O
44.76¬±0.14 O
45.48¬±0.22 O
43.83¬±0.13 O
46.49¬±0.10 O
ENSEMBLE O
/check O
/check O
/check O
* O
52.42¬±0.19 O
44.69¬±0.15 O
46.88¬±0.17 O
43.94¬±0.09 O
46.98¬±0.09 O
Table O
2 O
: O
Overall O
and O
sub O
- O
dataset O
speciÔ¨Åc O
coherence O
classiÔ¨Åcation O
F1 O
scores O
on O
the O
GCDC O
dataset O
. O
Error O
boundaries O
describe O
95 O
% O
conÔ¨Ådence O
intervals O
. O
Values O
in O
bold O
describe O
statistically O
signiÔ¨Åcant O
state O
of O
the O
art O
performance O
. O
F1 O
scores O
are O
calculated O
by O
macro O
- O
averaging O
the O
corresponding O
class O
- O
wise O
F1 O
scores O
. O
* O
indicates O
availability O
of O
EDUlevel O
semantic O
information O
through O
the O
ensembling O
with O
ParSeq O
. O
4.2 O
Training O
We O
train O
all O
models O
with O
hyperparameter O
settings O
consistent O
with O
that O
of O
ParSeq O
reported O
by O
( O
Lai O
and O
Tetreault O
, O
2018 O
) O
. O
SpeciÔ¨Åcally O
, O
we O
use O
a O
learning O
rate O
of O
0.0001 O
, O
hidden O
size O
of O
100 O
, O
relation O
embedding O
size O
of O
50 O
, O
and O
300 O
- O
dimensional O
pre O
- O
trained O
GloVe O
embeddings O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
. O
We O
train O
with O
the O
Adam O
optimizer O
( O
Kingma O
and O
Ba O
, O
2014 O
) O
for O
2 O
epochs O
. O
For O
every O
model O
/ O
variation O
, O
the O
reported O
results O
represent O
the O
corresponding O
accuracies O
and O
F1 O
scores O
averaged O
over O
1000 O
independent O
runs O
, O
each O
initialized O
with O
a O
different O
random O
seed O
. O
4.3 O
RST O
- O
Recursive O
‚Äôs O
Performance O
Our O
full O
model O
incorporates O
the O
RST O
Tree O
( O
T O
) O
structure O
, O
nucleus O
/ O
satellite O
properties O
( O
nuclearity O
) O
of O
subtrees O
( O
NS O
) O
, O
RST O
speciÔ¨Åc O
connective O
relations O
( O
R O
) O
, O
and O
EDU O
embeddings O
at O
leaves O
of O
the O
RST O
tree O
( O
E),as O
previously O
described O
in O
3.1 O
. O
Here O
, O
( O
T O
) O
deÔ¨Ånes O
the O
tree O
traversal O
operation O
and O
( O
NS O
) O
and O
( O
R O
) O
are O
learned O
vector O
embeddings O
for O
nuclearity O
and O
relations O
. O
We O
examine O
three O
ablations O
, O
each O
removing O
one O
of O
( O
NS O
) O
, O
( O
R O
) O
and O
( O
E O
) O
from O
the O
model O
. O
The O
results O
are O
provided O
in O
Tables O
1 O
and O
2 O
. O
As O
shown O
, O
the O
complete O
model O
is O
able O
to O
achieve O
a O
competitive O
overall O
accuracy O
and O
F1 O
at O
53.04 O
% O
and O
44.30 O
% O
respectively O
, O
which O
is O
close O
to O
the O
state O
of O
the O
art O
. O
Although O
this O
lags O
behind O
ParSeq O
by O
a O
noticeable O
2 O
% O
margin O
, O
RST O
- O
Recursive O
is O
able O
to O
achieve O
this O
performance O
with O
62 O
% O
fewer O
parameters O
( O
1,230k O
vs. O
3,241k O
) O
, O
demonstrating O
the O
usefulness O
of O
linguistically O
- O
motivated O
features O
. O
Removing O
EDU O
embeddings O
reduces O
accuracy O
and O
F1 O
scores O
to O
50.46 O
% O
and O
39.13 O
% O
. O
This O
is O
still O
signiÔ¨Åcantly O
better O
than O
the O
majority O
class O
baseline O
, O
signifying O
that O
even O
without O
any O
semantic O
infor-667Recall O
Precision O
F1 O
Recall O
Precision O
F1 O
Recall O
Precision O
F1 O
0.000.250.500.751.00 O
Incoherent O
Incoherent O
Incoherent O
Neutral O
Neutral O
Neutral O
Coherent O
Coherent O
CoherentParSeq O
+ O
RST O
- O
Recursive O
( O
w O
/ O
out O
E O
) O
ParSeq O
RST O
- O
Recursive O
( O
w O
/ O
out O
E)Figure O
5 O
: O
Comparison O
of O
Recall O
, O
Precision O
and O
F1 O
on O
overall O
classiÔ¨Åcation O
of O
each O
coherence O
level O
. O
mation O
about O
the O
text O
and O
its O
contents O
, O
it O
is O
still O
possible O
to O
evaluate O
coherence O
using O
just O
the O
silverstandard O
RST O
features O
of O
the O
text O
. O
Removing O
RST O
relations O
and O
nuclearity O
, O
however O
, O
decreases O
performance O
substantially O
, O
dropping O
to O
the O
majority O
class O
level O
. O
This O
indicates O
that O
an O
RST O
tree O
structure O
alone O
( O
of O
the O
quality O
delivered O
by O
silver O
- O
standard O
parsers O
) O
is O
not O
sufÔ¨Åcient O
to O
classify O
coherence O
. O
It O
must O
also O
be O
noted O
that O
since O
we O
employ O
silverstandard O
RST O
parsing O
as O
performed O
by O
CODRA O
( O
Joty O
et O
al O
. O
, O
2015 O
) O
, O
the O
reported O
results O
act O
as O
a O
lower O
bound O
which O
we O
would O
expect O
to O
improve O
as O
parsing O
quality O
increases O
. O
4.4 O
Ensemble O
‚Äôs O
Performance O
We O
examine O
three O
variations O
of O
the O
Ensemble O
. O
The O
full O
model O
augments O
ParSeq O
with O
the O
text O
‚Äôs O
RST O
tree O
, O
relations O
and O
nuclearity O
. O
This O
model O
is O
able O
to O
achieve O
the O
new O
state O
of O
the O
art O
performance O
, O
at O
55.39 O
% O
accuracy O
and O
46.98 O
% O
F1 O
. O
Using O
Ô¨Ånal O
layer O
concatenation O
for O
ensembling O
is O
widely O
applicable O
to O
many O
other O
neural O
methods O
, O
and O
serves O
as O
a O
lower O
bound O
for O
the O
accuracy O
/ O
F1 O
boost O
to O
be O
appreciated O
by O
incorporating O
RST O
features O
into O
the O
model O
. O
Removing O
the O
RST O
relations O
and/or O
nuclearity O
information O
completely O
eliminates O
the O
performance O
gain O
, O
which O
shows O
that O
the O
RST O
tree O
on O
its O
own O
is O
not O
sufÔ¨Åcient O
as O
an O
RST O
source O
of O
information O
for O
distinguishing O
coherence O
, O
even O
when O
ensembled O
with O
ParSeq O
. O
4.5 O
ClassiÔ¨Åcation O
Trends O
As O
demonstrated O
in O
Figure O
5 O
, O
coherence O
classiÔ¨Åers O
have O
difÔ¨Åculty O
predicting O
the O
neutral O
class O
( O
2 O
) O
, O
experiencing O
modal O
collapse O
towards O
the O
extreme O
ends O
in O
the O
best O
performing O
models O
. O
Early O
experiments O
using O
alternative O
objective O
functions O
such O
as O
the O
Ordinal O
Loss O
or O
Mean O
Squared O
Error O
resulted O
in O
a O
similar O
modal O
collapse O
or O
poor O
overall O
performance O
. O
We O
leave O
further O
exploration O
of O
this O
problem O
to O
future O
research O
. O
Furthermore O
, O
RST O
- O
Recursive O
shows O
a O
notably O
stronger O
recall O
on O
the O
coherent O
class O
( O
3 O
) O
as O
compared O
to O
ParSeq O
. O
On O
the O
other O
hand O
, O
ParSeq O
has O
a O
higher O
recall O
/ O
precision O
on O
class O
( O
1 O
) O
and O
slightly O
higher O
precision O
on O
class O
( O
3 O
) O
. O
The O
Ensemble O
method O
, O
however O
, O
is O
able O
to O
take O
the O
best O
of O
both O
, O
achieving O
better O
recall O
, O
precision O
and O
F1 O
on O
both O
the O
incoherent O
and O
coherent O
classes O
as O
compared O
to O
ParSeq O
. O
5 O
Conclusions O
and O
Future O
Work O
In O
this O
paper O
, O
we O
explore O
the O
usefulness O
of O
silverstandard O
parsed O
RST O
features O
in O
neural O
coherence O
classiÔ¨Åcation O
. O
We O
propose O
two O
new O
methods O
, O
RSTRecursive O
and O
Ensemble O
. O
The O
former O
achieves O
reasonably O
good O
performance O
, O
only O
2 O
% O
short O
of O
state O
of O
the O
art O
, O
while O
more O
robust O
with O
62 O
% O
fewer O
parameters O
. O
The O
latter O
demonstrates O
the O
added O
advantage O
of O
RST O
features O
in O
improving O
classiÔ¨Åcation O
accuracy O
of O
the O
existing O
state O
of O
the O
art O
methods O
by O
setting O
new O
state O
of O
the O
art O
performance O
with O
a O
modest O
but O
promising O
margin O
. O
This O
signiÔ¨Åes O
that O
the O
document O
‚Äôs O
rhetorical O
structure O
is O
an O
important O
aspect O
of O
its O
perceived O
clarity O
. O
Naturally O
, O
this O
improvement O
in O
performance O
is O
bounded O
by O
the O
quality O
of O
parsed O
RST O
features O
and O
could O
increase O
as O
better O
discourse O
parsers O
are O
developed O
. O
In O
the O
future O
, O
exploring O
other O
RST O
- O
based O
architectures O
for O
coherence O
classiÔ¨Åcation O
, O
as O
well O
as O
better O
RST O
ensemble O
schemes O
and O
improving O
RST O
parsing O
can O
be O
avenues O
of O
potentially O
fruitful O
research O
. O
Additional O
research O
on O
multipronged O
approaches O
that O
draw O
from O
Centering O
Theory O
, O
RST O
and O
deep O
learning O
all O
together O
can O
also O
be O
of O
value O
. O
Abstract O
Large O
- O
scale O
natural O
language O
inference O
( O
NLI O
) O
datasets O
such O
as O
SNLI O
or O
MNLI O
have O
been O
created O
by O
asking O
crowdworkers O
to O
read O
a O
premise O
and O
write O
three O
new O
hypotheses O
, O
one O
for O
each O
possible O
semantic O
relationships O
( O
entailment O
, O
contradiction O
, O
and O
neutral O
) O
. O
While O
this O
protocol O
has O
been O
used O
to O
create O
useful O
benchmark O
data O
, O
it O
remains O
unclear O
whether O
the O
writing O
- O
based O
annotation O
protocol O
is O
optimal O
for O
any O
purpose O
, O
since O
it O
has O
not O
been O
evaluated O
directly O
. O
Furthermore O
, O
there O
is O
ample O
evidence O
that O
crowdworker O
writing O
can O
introduce O
artifacts O
in O
the O
data O
. O
We O
investigate O
two O
alternative O
protocols O
which O
automatically O
create O
candidate O
( O
premise O
, O
hypothesis O
) O
pairs O
for O
annotators O
to O
label O
. O
Using O
these O
protocols O
and O
a O
writing O
- O
based O
baseline O
, O
we O
collect O
several O
new O
English O
NLI O
datasets O
of O
over O
3k O
examples O
each O
, O
each O
using O
a O
Ô¨Åxed O
amount O
of O
annotator O
time O
, O
but O
a O
varying O
number O
of O
examples O
to O
Ô¨Åt O
that O
time O
budget O
. O
Our O
experiments O
on O
NLI O
and O
transfer O
learning O
show O
negative O
results O
: O
None O
of O
the O
alternative O
protocols O
outperforms O
the O
baseline O
in O
evaluations O
of O
generalization O
within O
NLI O
or O
on O
transfer O
to O
outside O
target O
tasks O
. O
We O
conclude O
that O
crowdworker O
writing O
still O
the O
best O
known O
option O
for O
entailment O
data O
, O
highlighting O
the O
need O
for O
further O
data O
collection O
work O
to O
focus O
on O
improving O
writing O
- O
based O
annotation O
processes O
. O
1 O
Introduction O
Research O
on O
natural O
language O
understanding O
has O
beneÔ¨Åted O
greatly O
from O
the O
availability O
of O
largescale O
, O
annotated O
data O
, O
especially O
for O
tasks O
like O
reading O
comprehension O
and O
natural O
language O
inference O
, O
which O
lend O
themselves O
to O
non O
- O
expert O
crowdsourcing O
. O
These O
datasets O
are O
useful O
in O
three O
settings O
: O
evaluation O
( O
Williams O
et O
al O
. O
, O
2018 O
; O
Rajpurkar O
et O
al O
. O
, O
2018 O
; O
Zellers O
et O
al O
. O
, O
2019 O
) O
; O
pretraining O
( O
Phang O
et O
al O
. O
, O
2018 O
; O
Conneau O
et O
al O
. O
, O
2018 O
; O
Pruksachatkun O
et O
al O
. O
,2020 O
) O
; O
and O
as O
training O
data O
for O
downstream O
tasks O
( O
Trivedi O
et O
al O
. O
, O
2019 O
; O
Portelli O
et O
al O
. O
, O
2020 O
) O
. O
Natural O
language O
inference O
( O
NLI O
) O
, O
also O
known O
as O
recognizing O
textual O
entailment O
( O
RTE O
; O
Dagan O
et O
al O
. O
, O
2005 O
) O
is O
the O
problem O
of O
determining O
whether O
or O
not O
a O
hypothesis O
semantically O
entails O
a O
premise O
. O
The O
two O
largest O
NLI O
corpora O
, O
SNLI O
( O
Bowman O
et O
al O
. O
, O
2015 O
) O
and O
MNLI O
( O
Williams O
et O
al O
. O
, O
2018 O
) O
are O
created O
by O
asking O
crowdworkers O
to O
write O
three O
labeled O
hypothesis O
sentences O
given O
a O
premise O
sentence O
taken O
from O
a O
preexisting O
text O
corpus O
. O
While O
these O
datasets O
have O
been O
widely O
used O
as O
benchmarks O
for O
NLU O
, O
there O
have O
been O
no O
studies O
evaluating O
writing O
- O
based O
annotation O
for O
collecting O
NLI O
data O
. O
Moreover O
, O
there O
is O
growing O
evidence O
that O
human O
writing O
can O
introduce O
annotation O
artifacts O
, O
which O
enable O
models O
to O
perform O
moderately O
well O
just O
by O
learning O
spurious O
statistical O
patterns O
in O
the O
data O
( O
Gururangan O
et O
al O
. O
, O
2018 O
; O
Tsuchiya O
, O
2018 O
; O
Poliak O
et O
al O
. O
, O
2018a O
) O
. O
This O
paper O
explores O
the O
possibility O
of O
collecting O
high O
- O
quality O
NLI O
data O
without O
asking O
crowdworkers O
to O
write O
hypotheses O
. O
We O
introduce O
two O
alternative O
protocols O
( O
Figure O
1 O
) O
which O
substitute O
crowdworker O
writing O
with O
fully O
- O
automated O
pipelines O
to O
generate O
premise O
- O
hypothesis O
sentence O
pairs O
, O
which O
annotators O
then O
simply O
label O
. O
The O
Ô¨Årst O
protocol O
uses O
a O
sentence O
- O
similarity O
- O
based O
method O
to O
pair O
similar O
sentences O
from O
large O
unannotated O
corpora O
. O
The O
second O
protocol O
uses O
parallel O
sentences O
and O
uses O
machine O
translation O
systems O
to O
generate O
sentence O
pairs O
. O
Using O
the O
MNLI O
protocol O
as O
our O
baseline O
, O
we O
collect O
Ô¨Åve O
datasets O
using O
premises O
taken O
from O
Gigaword O
news O
text O
( O
Parker O
et O
al O
. O
, O
2011 O
) O
and O
Wikipedia O
. O
We O
then O
compare O
models O
trained O
using O
these O
datasets O
for O
their O
generalization O
performance O
within O
NLI O
and O
for O
transfer O
learning O
to O
other O
tasks O
. O
We O
start O
from O
the O
assumption O
that O
writing O
a O
new O
hypothesis O
takes O
more O
time O
and O
effort O
than O
simply O
labeling O
a O
presented O
hypothesis O
. O
As O
a O
result O
, O
it O
is O
plausible O
that O
our O
protocols O
could O
offer O
some O
value672Similarity O
Retrieval O
  O
Unstructured O
Source O
Text O
  O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
, O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
. O
  O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
. O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
... O
   O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
. O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
. O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
. O
  O
Unlabeled O
Sentence O
Pairs O
  O
( O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
, O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
. O
, O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
. O
) O
( O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
, O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
, O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
. O
, O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
, O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
. O
) O
Use O
FAISS O
and O
FastText O
to O
pair O
- O
up O
  O
similar O
sentences O
. O
  O
Crowdworker O
Labeling O
  O
P O
: O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
, O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
. O
, O
H O
: O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
. O
  O
‚ö™ O
entailment O
    O
‚ö™ O
neutral O
    O
‚ö™ O
contradiction O
Use O
a O
tuned O
automatic O
Ô¨Ålter O
to O
identify O
a O
  O
diverse O
set O
of O
pairs O
to O
annotate O
. O
  O
? O
? O
   O
? O
? O
MNLI O
- O
Style O
Baseline O
  O
Unstructured O
Source O
Text O
  O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
, O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
. O
  O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
. O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
... O
  O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
. O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
. O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
. O
  O
Sample O
individual O
sentences O
to O
annotate O
. O
  O
Crowdworker O
Writing O
  O
P O
: O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
, O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
. O
   O
entailment O
: O
  O
contr O
adiction O
: O
  O
neutr O
al O
: O
| O
? O
? O
   O
? O
? O
Translation O
  O
Unlabeled O
Sentence O
Pairs O
  O
( O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
, O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
. O
, O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
. O
) O
( O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
, O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
, O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
. O
, O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
, O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
. O
) O
Identify O
pairs O
of O
similar O
sentences O
from O
  O
existing O
bilingual O
comparable O
corpora O
. O
  O
Translate O
the O
non O
- O
English O
sentence O
to O
  O
English O
automatically O
. O
  O
Crowdworker O
Labeling O
  O
P O
: O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
, O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
. O
, O
H O
: O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
. O
  O
‚ö™ O
entailment O
    O
‚ö™ O
neutral O
    O
‚ö™ O
contradiction O
Use O
a O
tuned O
automatic O
Ô¨Ålter O
to O
identify O
a O
  O
diverse O
set O
of O
pairs O
to O
annotate O
. O
  O
? O
? O
   O
? O
? O
Aligned O
Bilingual O
Text O
  O
Eng O
. O
: O
  O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
, O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
. O
  O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
. O
  O
Êó•Êú¨Ë™û O
: O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
. O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
‚Äì O
. O
  O
Using O
the O
sampled O
sentence O
as O
a O
  O
premise O
, O
collect O
a O
matching O
hypothesis O
  O
for O
each O
label O
. O
  O
Collect O
a O
label O
for O
each O
pair O
. O
  O
Collect O
a O
label O
for O
each O
pair O
. O
Figure O
1 O
: O
We O
introduce O
two O
new O
protocols O
for O
natural O
language O
inference O
data O
collection O
. O
Both O
use O
fully O
- O
automated O
pipelines O
to O
generate O
pairs O
of O
semantically O
- O
related O
sentences O
, O
which O
crowdworker O
annotators O
then O
label O
. O
even O
if O
the O
quality O
of O
the O
data O
they O
produce O
is O
no O
better O
than O
a O
writing O
- O
based O
baseline O
. O
To O
study O
the O
cost O
trade O
- O
off O
, O
we O
collect O
each O
dataset O
under O
the O
same O
Ô¨Åxed O
annotation O
budget O
with O
a O
Ô¨Åxed O
( O
‚àºUS O
$ O
15 O
) O
hourly O
wage O
. O
Using O
this O
constraint O
, O
we O
collect O
approximately O
twice O
as O
many O
examples O
from O
our O
new O
protocols O
. O
Our O
main O
results O
on O
natural O
language O
inference O
and O
transfer O
learning O
are O
clearly O
negative O
. O
Humanconstructed O
examples O
appear O
to O
be O
far O
superior O
to O
automatically O
- O
constructed O
examples O
in O
both O
settings O
. O
While O
crowdworker O
writing O
in O
data O
collection O
has O
known O
issues O
, O
it O
produces O
better O
training O
data O
than O
our O
automatic O
methods O
, O
or O
any O
known O
comparable O
methods O
which O
intervene O
the O
writingbased O
protocol O
to O
help O
crowdworkers O
with O
the O
writing O
process O
( O
Bowman O
et O
al O
. O
, O
2020 O
) O
. O
This O
strongly O
suggests O
that O
future O
work O
on O
data O
quality O
should O
focus O
on O
improving O
human O
- O
based O
generation O
processes O
. O
2 O
Collecting O
NLI O
Data O
We O
compare O
three O
protocols O
for O
collecting O
NLI O
data O
: O
( O
1 O
) O
a O
baseline O
MNLI O
- O
style O
protocol O
( O
BASE O
) O
, O
( O
2 O
) O
a O
sentence O
- O
similarity O
- O
based O
protocol O
( O
SIM O
) O
, O
and O
( O
3 O
) O
a O
translation O
- O
based O
protocol O
( O
TRANSLATE O
) O
. O
To O
test O
generalization O
performance O
across O
domains O
, O
we O
collect O
two O
datasets O
for O
BASE O
andSIM O
, O
using O
text O
from O
Gigaword O
( O
news O
) O
and O
Wikipedia O
( O
wiki O
) O
domains.1ForTRANSLATE O
, O
we O
collect O
a O
dataset O
from O
WikiMatrix O
( O
Schwenk O
et O
al O
. O
, O
2019 O
) O
, O
a O
collection O
of O
Wikipedia O
parallel O
sentences O
. O
Table O
1 O
shows O
examples O
of O
sentence O
pairs O
collected O
using O
1The O
premise O
sentences O
for O
each O
protocol O
can O
be O
different O
although O
they O
come O
from O
the O
same O
source.each O
protocol O
. O
Our O
new O
protocols O
( O
Figure O
1 O
) O
share O
a O
similar O
automated O
pipeline O
. O
Given O
an O
unstructured O
text O
, O
we O
automatically O
collect O
similar O
sentence O
pairs O
which O
annotators O
then O
label O
. O
There O
are O
two O
key O
differences O
between O
our O
new O
protocols O
and O
BASE O
. O
First O
, O
our O
automatically O
paired O
sentences O
are O
unlabeled O
, O
and O
thus O
require O
a O
further O
data O
labeling O
process O
( O
Section O
2.4 O
) O
. O
Second O
, O
our O
protocols O
might O
produce O
datasets O
with O
imbalanced O
label O
distributions O
. O
This O
is O
in O
contrast O
to O
BASE O
, O
which O
ensures O
each O
premise O
will O
have O
one O
hypothesis O
for O
each O
label O
in O
the O
annotation O
. O
The O
following O
subsections O
describe O
each O
protocol O
in O
more O
detail O
. O
2.1 O
Baseline O
( O
BASE O
) O
Our O
BASE O
protocol O
closely O
follows O
that O
used O
for O
MNLI O
. O
We O
randomly O
sample O
premise O
sentences O
from O
Gigaword O
and O
Wikipedia O
and O
ask O
crowdworkers O
to O
write O
three O
new O
hypotheses O
, O
one O
for O
each O
relation O
type.2 O
2.2 O
Sentence O
Similarity O
( O
SIM O
) O
Our O
SIMprotocol O
exploits O
the O
fact O
that O
, O
in O
large O
corpora O
, O
it O
should O
be O
easy O
to O
Ô¨Ånd O
pairs O
of O
sentences O
that O
describe O
similar O
events O
or O
situations O
. O
For O
example O
, O
in O
Gigaword O
, O
one O
event O
might O
be O
written O
differently O
by O
different O
news O
sources O
in O
ways O
that O
yield O
any O
of O
our O
three O
relationships O
. O
We O
collect O
similar O
sentences O
and O
automatically O
match O
them O
to O
form O
sentence O
pairs O
which O
annotators O
then O
label O
. O
The O
whole O
pipeline O
consists O
of O
three O
steps O
: O
2Our O
instructions O
can O
be O
found O
in O
the O
Appendix O
A O
, O
and O
our O
FAQs O
are O
available O
at O
https://sites.google.com/ O
nyu.edu/nlu-mturk-faq/writing-sentences O
.673Dataset O
Label O
Premise O
Hypothesis O
Base O
- O
News O
E O
The O
city O
reconsidered O
that O
position O
on O
Wednesday O
, O
saying O
it O
was O
seeking O
to O
raise O
an O
additional O
$ O
1.5 O
million O
to O
extend O
Mardi O
Gras O
over O
two O
weekends O
and O
to O
pay O
for O
overtime O
on O
several O
days O
. O
The O
city O
is O
looking O
to O
get O
more O
money O
for O
Mardi O
Gras O
. O
Base O
- O
Wiki O
C O
Service O
books O
were O
not O
included O
and O
a O
note O
at O
the O
end O
mentions O
many O
other O
books O
in O
French O
, O
English O
and O
Latin O
which O
were O
then O
considered O
worthless O
. O
Service O
books O
were O
included O
. O
Sim O
- O
News O
N O
All O
of O
them O
run O
out O
like O
college O
football O
players O
before O
a O
big O
bowl O
game O
. O
Pray O
before O
a O
college O
football O
game O
. O
Sim O
- O
Wiki O
C O
His O
work O
was O
heavily O
criticised O
as O
unscientiÔ¨Åc O
by O
his O
contemporaries O
. O
His O
work O
was O
recognized O
and O
admired O
by O
his O
contemporaries O
. O
Translate O
- O
Wiki O
E O
This O
was O
used O
to O
indicate O
a O
positive O
response O
, O
or O
truth O
, O
or O
approval O
of O
the O
item O
in O
front O
of O
it O
. O
This O
was O
used O
to O
indicate O
yes O
, O
true O
, O
or O
conÔ¨Årmed O
on O
items O
in O
a O
list O
. O
Table O
1 O
: O
Examples O
of O
sentence O
pairs O
chosen O
randomly O
from O
each O
test O
set O
, O
along O
with O
their O
assigned O
labels O
. O
E O
: O
entailment O
, O
C O
: O
contradiction O
, O
N O
: O
neutral O
. O
indexing O
and O
retrieval O
, O
reranking O
, O
and O
crowdworker O
labeling O
. O
Indexing O
and O
Retrieval O
Given O
a O
raw O
text O
, O
we O
Ô¨Årst O
split O
it O
into O
sentences.3We O
encode O
each O
sentence O
as O
a O
300 O
- O
dimensional O
vector O
using O
fastText O
( O
Bojanowski O
et O
al O
. O
, O
2017 O
) O
and O
index O
them O
using O
FAISS O
( O
Johnson O
et O
al O
. O
, O
2019 O
) O
, O
an O
open O
- O
source O
library O
for O
large O
- O
scale O
similarity O
search O
on O
vectors.4 O
Since O
Gigaword O
and O
Wikipedia O
consist O
of O
billions O
of O
sentences O
, O
we O
perform O
dimensionality O
reduction O
using O
PCA O
and O
cluster O
the O
search O
space O
to O
allow O
efÔ¨Åcient O
index O
and O
retrieval O
. O
We O
randomly O
sample O
query O
sentences O
from O
the O
text O
corpus O
and O
retrieve O
the O
top O
1k O
most O
similar O
sentences O
for O
each O
query O
. O
This O
is O
done O
by O
building O
an O
index O
with O
type O
" O
PCAR64,IVFx O
, O
Flat O
" O
in O
FAISS O
terms O
, O
where O
xvaries O
depending O
on O
the O
corpus O
size O
. O
Details O
of O
our O
indexing O
and O
retrieval O
procedures O
can O
be O
found O
in O
Appendix O
A.1 O
. O
Reranking O
FastText O
uses O
a O
Continuous O
Bag O
- O
ofWords O
( O
CBoW O
) O
model O
to O
learn O
word O
representations O
. O
This O
means O
given O
a O
query O
, O
we O
will O
sometimes O
have O
top O
matches O
which O
are O
syntactically O
similar O
but O
describe O
different O
events O
or O
situations O
. O
While O
unrelated O
sentences O
can O
be O
contradictory O
or O
neutral O
, O
directly O
using O
the O
top- O
nsentences O
from O
FAISS O
will O
give O
us O
too O
few O
entailment O
pairs O
. O
Furthermore O
, O
because O
we O
use O
randomly O
sampled O
sentences O
as O
queries O
, O
there O
could O
be O
no O
good O
match O
at O
all O
for O
a O
given O
query O
. O
3We O
use O
Spacy O
‚Äôs O
" O
encore O
weblg O
" O
model O
to O
segment O
sentences O
and O
extract O
noun O
phrase O
and O
entities O
for O
later O
use O
in O
reranking O
. O
4https://github.com/facebookresearch/ O
faissTo O
collect O
a O
set O
of O
sentence O
pairs O
with O
a O
reasonable O
label O
distribution O
, O
for O
each O
query O
, O
we O
retrieve O
top O
- O
Kmatches O
and O
rerank O
the O
( O
query O
, O
retrieved O
sentence O
) O
pairs O
using O
the O
following O
features O
: O
‚Ä¢FAISS O
similarity O
score O
: O
The O
raw O
similarity O
score O
from O
FAISS O
. O
‚Ä¢Word O
types O
: O
The O
proportion O
of O
word O
types O
in O
the O
query O
sentence O
seen O
in O
the O
retrieved O
sentence O
. O
‚Ä¢Noun O
phrase O
: O
The O
proportion O
of O
noun O
phrases O
in O
the O
query O
sentence O
seen O
in O
the O
retrieved O
sentence O
. O
‚Ä¢Subjects O
: O
The O
proportion O
of O
complete O
subject O
spans O
( O
some O
sentences O
with O
embedded O
clauses O
can O
have O
more O
than O
one O
subject O
) O
in O
the O
query O
sentence O
seen O
in O
the O
retrieved O
sentence O
. O
‚Ä¢Named O
entity O
: O
The O
proportion O
of O
named O
entities O
in O
the O
query O
sentence O
seen O
in O
the O
retrieved O
sentence O
. O
‚Ä¢Time O
: O
A O
boolean O
feature O
which O
denotes O
whether O
two O
sentences O
are O
written O
in O
the O
same O
month O
and O
year O
( O
only O
for O
Gigaword O
) O
‚Ä¢Wiki O
article O
: O
A O
boolean O
feature O
which O
denotes O
whether O
the O
pairs O
come O
from O
the O
same O
article O
. O
( O
only O
for O
Wikipedia O
) O
‚Ä¢Wiki O
link O
: O
The O
proportion O
of O
hyperlink O
tokens O
in O
the O
query O
sentence O
seen O
in O
the O
retrieved O
sentence O
( O
only O
for O
Wikipedia O
) O
The O
choice O
of O
these O
hand O
- O
crafted O
features O
will O
likely O
impact O
the O
distribution O
of O
our O
Ô¨Ånal O
dataset O
, O
but O
we674don‚Äôt O
expect O
these O
choices O
to O
inject O
signiÔ¨Åcant O
labelassociation O
artifacts O
, O
since O
our O
methods O
play O
no O
role O
in O
setting O
labels O
. O
We O
calculate O
the O
score O
for O
each O
sentence O
pair O
using O
a O
weighted O
sum O
of O
these O
features O
. O
We O
populate O
pairs O
from O
all O
queries O
and O
sort O
them O
based O
on O
their O
feature O
scores O
. O
We O
then O
select O
the O
top O
N% O
pairs O
as O
our O
Ô¨Ånal O
pairs O
. O
We O
use O
a O
Bayesian O
hyperparameter O
optimization O
to O
tune O
the O
feature O
weights O
, O
K O
, O
andN. O
In O
an O
ideal O
case O
, O
we O
want O
our O
dataset O
to O
have O
a O
balanced O
distribution O
so O
that O
all O
classes O
will O
be O
represented O
equally O
. O
To O
push O
for O
this O
, O
we O
tune O
these O
parameters O
to O
minimize O
the O
Kullback O
‚Äì O
Leibler O
( O
KL O
) O
divergence O
between O
a O
uniform O
distribution O
across O
three O
entailment O
classes O
, O
P(x O
) O
, O
and O
an O
empirical O
distribution O
, O
Q(x O
) O
, O
computed O
based O
on O
the O
predictions O
of O
an O
NLI O
model O
. O
We O
run O
Bayesian O
optimization O
for O
100 O
iterations O
using O
Optuna O
( O
Akiba O
et O
al O
. O
, O
2019 O
) O
. O
For O
the O
NLI O
model O
, O
we O
use O
a O
RoBERTa O
Large O
model O
Ô¨Åne O
- O
tuned O
on O
a O
combination O
of O
SNLI O
, O
MNLI O
, O
and O
ANLI O
. O
2.3 O
Translation O
( O
TRANSLATE O
) O
Multilingual O
comparable O
corpora O
contain O
similar O
texts O
in O
at O
least O
two O
different O
languages O
. O
If O
they O
are O
sentence O
- O
aligned O
, O
we O
can O
automatically O
translate O
text O
from O
one O
language O
to O
one O
of O
the O
others O
to O
yield O
candidate O
sentence O
pairs O
. O
Since O
the O
alignment O
behind O
the O
corpus O
can O
be O
noisy O
, O
the O
resulting O
sentence O
pairs O
range O
almost O
continuously O
from O
being O
parallel O
to O
being O
semantically O
unrelated O
, O
potentially O
Ô¨Åtting O
any O
of O
the O
three O
entailment O
relationships O
. O
In O
the O
TRANSLATE O
protocol O
, O
we O
investigate O
whether O
we O
can O
use O
such O
sentence O
pairs O
as O
entailment O
data O
. O
We O
use O
WikiMatrix O
( O
Schwenk O
et O
al O
. O
, O
2019 O
) O
, O
a O
collection O
of O
135 O
million O
Wikipedia O
parallel O
sentences O
, O
which O
was O
constructed O
by O
aligning O
similar O
sentences O
in O
different O
languages O
in O
a O
joint O
sentence O
embedding O
space O
( O
Schwenk O
, O
2018 O
; O
Artetxe O
and O
Schwenk O
, O
2019 O
) O
. O
It O
is O
a O
mix O
of O
translated O
sentence O
pairs O
and O
comparable O
sentences O
written O
independently O
about O
the O
same O
information O
. O
We O
collect O
parallel O
sentences O
where O
one O
of O
the O
sentences O
is O
in O
English O
, O
sE. O
For O
the O
paired O
non O
- O
English O
languages O
, O
we O
pick O
5 O
languages O
: O
German O
, O
French O
, O
Indonesian O
, O
Japanese O
, O
and O
Czech O
. O
We O
then O
translate O
the O
aligned O
non O
- O
English O
sentence O
into O
an O
English O
sentence O
, O
sÀÜEusing O
the O
OPUS O
- O
MT O
( O
Tiedemann O
and O
Thottingal O
, O
2020 O
) O
machine O
translation O
systems O
, O
and O
treat O
( O
sE O
, O
sÀÜE)as O
a O
sentence O
pair O
. O
The O
diverse O
set O
of O
languages O
allows O
us O
to O
collect O
a O
more O
diverse O
setIndividual O
= O
= O
Gold O
No O
Gold O
Label O
MNLI O
( O
Full O
) O
88.7 O
% O
1.8 O
% O
Base O
- O
News O
78.7 O
% O
13.1 O
% O
Base O
- O
Wiki O
76.4 O
% O
10.0 O
% O
Sim O
- O
News O
72.9 O
% O
15.8 O
% O
Sim O
- O
Wiki O
74.1 O
% O
11.9 O
% O
Translate O
- O
Wiki O
72.8 O
% O
14.6 O
% O
Table O
2 O
: O
Validation O
statistics O
for O
each O
protocol O
, O
compared O
to O
MNLI O
Full O
. O
of O
sentence O
pairs O
coming O
from O
the O
structural O
differences O
across O
languages O
. O
We O
do O
not O
perform O
any O
reranking O
as O
our O
predictions O
using O
an O
NLI O
model O
on O
the O
initially O
retrieved O
data O
( O
the O
same O
one O
that O
we O
used O
in¬ß2.2 O
) O
shows O
a O
near O
- O
balanced O
distribution O
. O
2.4 O
Data O
Labeling O
We O
use O
Amazon O
Mechanical O
Turk O
to O
label O
the O
automatically O
- O
collected O
sentence O
pairs O
( O
SIMand O
TRANSLATE O
) O
. O
We O
hire O
crowdworkers O
which O
have O
completed O
at O
least O
5000 O
HITs O
with O
at O
least O
a O
99 O
% O
acceptance O
rate O
. O
In O
each O
task O
, O
we O
present O
crowdworkers O
with O
a O
sentence O
pair O
and O
ask O
them O
to O
provide O
a O
single O
label O
( O
entailment O
, O
contradiction O
, O
neutral O
or‚ÄúI O
do O
n‚Äôt O
understand O
‚Äù O
) O
for O
the O
pair O
. O
The O
latter O
is O
used O
if O
there O
are O
problems O
with O
either O
sentence O
, O
e.g. O
, O
because O
of O
errors O
during O
preprocessing O
. O
We O
collect O
one O
label O
per O
sentence O
pair O
. O
We O
use O
the O
same O
HIT O
setup O
for O
validating O
our O
test O
sets O
( O
Section O
3 O
) O
. O
3 O
The O
Resulting O
Datasets O
Using O
BASE O
, O
we O
collect O
3k O
examples O
for O
BaseNews O
and O
Base O
- O
Wiki.5ForSIMandTRANSLATE O
, O
we O
increase O
the O
number O
of O
pairs O
to O
exhaust O
the O
same O
budget O
that O
was O
used O
for O
the O
corresponding O
baseline O
dataset O
( O
$ O
1,791 O
for O
Base O
- O
News O
and O
$ O
1,445 O
for O
Base O
- O
Wiki O
) O
, O
allowing O
us O
to O
collect O
around O
twice O
as O
many O
examples O
for O
each O
protocol.6 O
For O
each O
dataset O
, O
we O
randomly O
select O
250 O
sentence O
pairs O
as O
the O
test O
set O
and O
use O
the O
rest O
as O
the O
training O
set O
. O
To O
ensure O
accurate O
labeling O
, O
we O
perform O
an O
additional O
round O
of O
annotation O
on O
the O
test O
sets O
. O
We O
ask O
four O
crowdworkers O
to O
label O
each O
pair O
using O
the O
same O
instructions O
that O
we O
use O
for O
data O
labeling O
, O
giving O
us O
a O
total O
of O
5 O
annotations O
per O
example O
. O
We O
assign O
the O
majority O
vote O
as O
the O
gold O
5Our O
preliminary O
experiments O
on O
subsets O
of O
MNLI O
show O
that O
RoBERTa O
performance O
starts O
to O
stabilize O
once O
we O
use O
at O
least O
3k O
training O
examples O
. O
6The O
resulting O
datasets O
are O
available O
at O
https:// O
github.com/nyu-mll/semi-automatic-nli O
. O
We O
provide O
anonymized O
worker O
- O
ids.675#Pairs O
Label O
Distribution O
HL O
E O
HL O
C O
HL O
N O
Word O
Type O
Overlap O
E O
C O
N O
¬µ O
( O
œÉ O
) O
¬µ O
( O
œÉ O
) O
¬µ O
( O
œÉ O
) O
E O
C O
NTrainingMNLI-3k O
2750 O
33.4 O
33.9 O
32.7 O
9.7 O
4.4 O
9.4 O
4.0 O
11.0 O
4.4 O
25.2 O
17.3 O
15.4 O
Base O
- O
News O
2734 O
33.5 O
33.4 O
33.2 O
12.1 O
6.0 O
11.8 O
5.8 O
12.4 O
6.2 O
23.5 O
18.4 O
18.1 O
Base O
- O
Wiki O
2740 O
33.3 O
33.7 O
33.0 O
11.1 O
7.7 O
10.5 O
4.5 O
11.6 O
7.1 O
31.2 O
23.4 O
22.7 O
Sim O
- O
News O
6627 O
21.8 O
39.1 O
39.2 O
23.2 O
9.7 O
22.7 O
10.0 O
23.3 O
9.9 O
46.6 O
21.8 O
23.0 O
Sim O
- O
Wiki O
6174 O
23.5 O
40.4 O
36.1 O
12.8 O
6.0 O
12.7 O
5.2 O
13.1 O
5.3 O
52.7 O
31.7 O
29.7 O
Translate O
- O
Wiki O
6189 O
34.7 O
31.4 O
34.0 O
18.6 O
9.6 O
14.2 O
7.5 O
16.0 O
8.8 O
41.3 O
20.0 O
24.6TestMNLI-3k O
250 O
29.2 O
37.6 O
33.2 O
10.6 O
4.6 O
9.4 O
3.7 O
10.7 O
4.2 O
26.3 O
14.6 O
15.9 O
Base O
- O
News O
226 O
38.1 O
33.2 O
28.8 O
12.8 O
5.7 O
11.5 O
5.1 O
11.6 O
4.6 O
22.8 O
14.4 O
13.5 O
Base O
- O
Wiki O
234 O
32.5 O
32.1 O
35.5 O
12.5 O
8.6 O
11.7 O
8.2 O
11.5 O
4.8 O
32.9 O
24.6 O
21.1 O
Sim O
- O
News O
219 O
20.1 O
44.3 O
35.6 O
22.5 O
11.1 O
24.9 O
11.1 O
23.9 O
10.9 O
69.3 O
20.9 O
20.6 O
Sim O
- O
Wiki O
229 O
20.5 O
45.0 O
34.5 O
12.6 O
7.6 O
13.7 O
5.8 O
12.0 O
4.5 O
60.5 O
32.8 O
28.7 O
Translate O
- O
Wiki O
222 O
40.5 O
29.3 O
30.2 O
18.7 O
8.5 O
13.0 O
6.9 O
14.3 O
6.7 O
46.3 O
15.1 O
21.1 O
Table O
3 O
: O
Dataset O
statistics O
. O
HLdenotes O
the O
average O
andstandard O
deviation O
of O
the O
hypothesis O
length O
of O
each O
label O
. O
label O
. O
Table O
2 O
shows O
the O
agreement O
statistics O
for O
each O
protocol O
. O
BASE O
shows O
a O
higher O
agreement O
than O
SIMandTRANSLATE O
, O
although O
it O
is O
lower O
than O
MNLI O
. O
Compared O
to O
MNLI O
, O
all O
of O
our O
datasets O
show O
higher O
number O
of O
examples O
with O
no O
gold O
label O
( O
no O
consensus O
between O
annotators O
) O
. O
As O
we O
strictly O
follow O
the O
MNLI O
protocol O
for O
BASE O
, O
this O
suggests O
that O
the O
different O
population O
of O
crowdworkers O
is O
likely O
responsible O
for O
these O
differences.7 O
3.1 O
Dataset O
Statistics O
Table O
3 O
shows O
the O
statistics O
of O
our O
collected O
data O
. O
As O
anticipated O
, O
datasets O
collected O
using O
SIMand O
TRANSLATE O
have O
slightly O
unbalanced O
distributions O
compared O
to O
BASE O
. O
In O
particular O
, O
for O
SIM O
, O
we O
observe O
that O
the O
entailment O
class O
has O
the O
lowest O
distribution O
in O
the O
training O
and O
test O
data O
. O
One O
clear O
difference O
between O
BASE O
and O
our O
new O
protocols O
is O
the O
hypothesis O
length O
. O
SIMand O
TRANSLATE O
tend O
to O
create O
longer O
hypothesis O
than O
BASE O
. O
We O
suspect O
that O
this O
is O
an O
artifact O
of O
the O
sentence O
- O
similarity O
method O
, O
which O
prefers O
identicalsentences O
( O
both O
syntax O
and O
semantics O
) O
over O
semantically O
similar O
sentences O
. O
Across O
domains O
, O
we O
observe O
that O
sentences O
from O
news O
texts O
are O
longer O
than O
Wikipedia O
. O
Recent O
work O
by O
McCoy O
et O
al O
. O
( O
2019 O
) O
shows O
that O
popular O
NLI O
models O
might O
learn O
a O
simple O
lexical O
overlap O
heuristic O
for O
predicting O
entailment O
labels O
. O
While O
this O
heuristic O
is O
natural O
for O
entailment O
, O
it O
can O
affect O
the O
model O
‚Äôs O
generalization O
especially O
when O
it O
is O
strongly O
reÔ¨Çected O
in O
the O
data O
. O
We O
calculate O
word O
type O
overlap O
by O
using O
the O
intersection O
of O
premise O
7MNLI O
used O
an O
organized O
group O
of O
crowdworkers O
hired O
through O
Hybrid O
( O
gethybrid.io).and O
hypothesis O
word O
types O
, O
divided O
by O
the O
union O
of O
the O
two O
sets O
. O
The O
last O
three O
columns O
in O
Table O
3 O
reports O
word O
type O
overlap O
in O
each O
dataset O
for O
each O
entailment O
label O
. O
We O
Ô¨Ånd O
that O
word O
type O
overlap O
is O
amuch O
stronger O
predictor O
of O
the O
label O
in O
our O
new O
protocols O
than O
in O
BASE O
. O
This O
could O
be O
a O
signiÔ¨Åcant O
driver O
of O
our O
results O
and O
might O
hurt O
the O
generalization O
performance O
of O
models O
trained O
using O
our O
new O
protocols O
‚Äô O
data O
. O
3.2 O
Annotation O
Cost O
We O
use O
the O
FairWork O
platform O
to O
set O
payment O
for O
each O
of O
our O
HITs O
( O
Whiting O
et O
al O
. O
, O
2019 O
) O
. O
FairWork O
surveys O
workers O
to O
estimate O
the O
time O
that O
each O
HIT O
takes O
and O
adjusts O
pay O
to O
a O
target O
of O
US O
$ O
15 O
/ O
hr O
. O
Based O
on O
its O
estimation O
, O
we O
pay O
$ O
0.4and O
$ O
0.3for O
each O
written O
hypothesis O
of O
Base O
- O
News O
and O
Base O
- O
Wiki O
, O
respectively O
. O
For O
Sim O
- O
News O
, O
Sim O
- O
Wiki O
, O
and O
Translate O
- O
Wiki O
, O
we O
pay O
$ O
0.175,$0.15,$0.15 O
for O
each O
labeled O
sentence O
pair O
, O
respectively O
. O
In O
total O
, O
we O
spend O
$ O
1791 O
for O
each O
dataset O
collected O
from O
Gigaword O
and O
$ O
1445 O
for O
each O
dataset O
collected O
from O
Wikipedia O
. O
4 O
Experiments O
We O
aim O
to O
test O
whether O
our O
alternative O
protocols O
can O
produce O
high O
- O
quality O
data O
that O
yield O
models O
that O
generalize O
well O
within O
NLI O
and O
in O
transfer O
learning O
. O
For O
the O
NLI O
evaluation O
, O
we O
evaluate O
each O
model O
on O
nine O
test O
sets O
: O
( O
i O
) O
the O
Ô¨Åve O
new O
individual O
test O
sets O
, O
each O
containing O
‚àº250 O
examples O
; O
( O
ii O
) O
the O
MNLI O
development O
set O
; O
and O
( O
iii O
) O
the O
three O
development O
sets O
of O
Adversarial O
NLI O
( O
ANLI O
; O
Nie O
et O
al O
. O
, O
2020 O
) O
, O
collected O
from O
three O
rounds O
of O
annotation O
( O
A1 O
, O
A2 O
, O
A3 O
) O
. O
ANLI O
is O
collected O
using O
an O
iterative O
adversarial O
approach O
that O
follows O
MNLI O
but O
encourages676Test O
Data O
Training O
Data O
BN O
BW O
SN O
SW O
TW O
MNLI O
A1 O
A2 O
A3 O
Avg O
. O
CBoWBase O
- O
News O
33.4 O
37.8 O
32.4 O
30.1 O
35.8 O
35.6 O
32.8 O
32.8 O
33.4 O
34.0 O
Base O
- O
Wiki O
34.1 O
33.1 O
37.9 O
35.4 O
39.0 O
35.6 O
33.1 O
31.6 O
33.2 O
34.8 O
Sim O
- O
News O
35.4 O
35.9 O
32.0 O
32.3 O
37.8 O
35.8 O
33.1 O
32.8 O
33.4 O
34.3 O
Sim O
- O
Wiki O
32.3 O
37.2 O
52.1 O
49.1 O
44.6 O
36.6 O
33.1 O
32.4 O
32.1 O
38.8 O
Translate O
- O
Wiki O
37.4 O
39.3 O
35.4 O
35.8 O
45.5 O
35.4 O
33.0 O
32.9 O
32.8 O
36.4RoBERTaMNLI-3k O
79.0 O
61.3 O
76.7 O
57.5 O
58.1 O
83.9 O
33.4 O
27.0 O
28.7 O
56.2 O
Base O
- O
News O
79.4 O
76.1 O
57.5 O
61.6 O
58.1 O
83.1 O
35.8 O
29.5 O
28.0 O
56.6 O
Base O
- O
Wiki O
77.0 O
74.2 O
58.5 O
62.0 O
61.3 O
54.0 O
30.9 O
31.8 O
33.1 O
53.6 O
Sim O
- O
News O
53.3 O
56.0 O
65.8 O
59.8 O
66.2 O
79.5 O
35.8 O
30.2 O
28.2 O
52.8 O
Sim O
- O
Wiki O
62.0 O
62.8 O
64.8 O
64.9 O
69.1 O
64.7 O
32.2 O
32.0 O
31.5 O
53.8 O
Translate O
- O
Wiki O
48.5 O
54.9 O
60.7 O
58.1 O
67.1 O
50.9 O
32.5 O
32.7 O
33.2 O
48.7 O
Average O
per O
test O
set O
52.0 O
51.7 O
52.2 O
49.7 O
53.0 O
54.1 O
33.2 O
31.4 O
31.6 O
45.4 O
Table O
4 O
: O
Model O
performance O
on O
individual O
test O
sets O
, O
as O
a O
median O
over O
10 O
random O
restarts O
. O
BN O
: O
Base O
- O
News O
, O
BW O
: O
Base O
- O
Wiki O
, O
SN O
: O
Sim O
- O
News O
, O
SW O
: O
Sim O
- O
Wiki O
, O
TW O
: O
Translate O
- O
Wiki O
. O
The O
last O
row O
shows O
the O
average O
performance O
across O
models O
on O
each O
test O
set O
. O
Test O
Data O
Training O
Data O
BN O
BW O
SN O
SW O
TW O
MNLI O
A1 O
A2 O
A3 O
Avg O
. O
MNLI-3k O
46.5 O
50.4 O
33.3 O
38.4 O
36.2 O
52.8 O
33.3 O
33.1 O
33.0 O
39.7 O
Base O
- O
News O
47.8 O
46.6 O
33.8 O
33.6 O
37.4 O
51.5 O
32.5 O
33.3 O
33.1 O
38.8 O
Base O
- O
Wiki O
33.2 O
32.1 O
44.3 O
45.0 O
29.3 O
32.8 O
33.3 O
33.3 O
33.0 O
35.1 O
Sim O
- O
News O
33.2 O
35.5 O
38.8 O
38.9 O
29.3 O
32.8 O
33.3 O
33.3 O
33.5 O
34.3 O
Sim O
- O
Wiki O
33.2 O
30.8 O
44.3 O
44.6 O
28.8 O
32.8 O
33.3 O
33.3 O
33.0 O
34.9 O
Translate O
- O
Wiki O
31.4 O
34.6 O
34.3 O
34.5 O
32.4 O
33.6 O
33.3 O
33.3 O
33.5 O
33.4 O
Average O
per O
test O
set O
37.5 O
38.3 O
38.1 O
39.2 O
32.2 O
39.4 O
33.2 O
33.3 O
33.2 O
36.0 O
Table O
5 O
: O
RoBERTa O
performance O
on O
individual O
test O
sets O
for O
hypothesis O
- O
only O
models O
. O
crowdworkers O
to O
write O
sentences O
that O
are O
difÔ¨Åcult O
for O
a O
trained O
NLI O
model O
. O
We O
experiment O
with O
two O
sentence O
encoders O
: O
a O
CBoW O
baseline O
initialized O
with O
fastText O
embeddings O
( O
Bojanowski O
et O
al O
. O
, O
2017 O
) O
, O
and O
a O
more O
powerful O
RoBERTa O
Large O
( O
Liu O
et O
al O
. O
, O
2019 O
) O
model O
, O
Ô¨Ånetuned O
on O
individual O
training O
sets O
. O
We O
perform O
a O
hyperparameter O
sweep O
, O
varying O
the O
learning O
rate O
‚àà{1e‚àí3,1e‚àí4,1e‚àí5}and O
the O
dropout O
rate O
‚àà O
{ O
0.1,0.2 O
} O
. O
We O
use O
batch O
size O
of O
16 O
and O
4 O
for O
CBoW O
and O
RoBERTA O
, O
respectively O
. O
We O
train O
each O
model O
using O
the O
best O
hyperparameters O
for O
10 O
epochs O
, O
with O
10 O
random O
restarts O
. O
In O
initial O
experiments O
, O
we O
Ô¨Ånd O
that O
this O
setup O
yields O
sTable O
performance O
given O
our O
relatively O
small O
datasets O
, O
especially O
when O
using O
RoBERTa.8 O
For O
transfer O
learning O
, O
we O
test O
whether O
each O
dataset O
can O
improve O
downstream O
task O
performance O
when O
it O
is O
used O
as O
intermediate O
- O
task O
data O
( O
Phang O
et O
al O
. O
, O
2018 O
; O
Pruksachatkun O
et O
al O
. O
, O
2020 O
) O
. O
As O
our O
col8This O
is O
consistent O
with O
the O
recent O
Ô¨Åndings O
of O
Zhang O
et O
al O
. O
( O
2020 O
) O
and O
Mosbach O
et O
al O
. O
( O
2020 O
) O
regarding O
Ô¨Åne O
- O
tuning O
BERTstyle O
models O
on O
small O
data.lected O
datasets O
are O
fairly O
small O
( O
< O
10 O
K O
examples O
) O
, O
we O
use O
Ô¨Åve O
data O
- O
poor O
downstream O
target O
tasks O
in O
the O
SuperGLUE O
benchmark O
( O
Wang O
et O
al O
. O
, O
2019a O
): O
COPA O
( O
Roemmele O
et O
al O
. O
, O
2011 O
) O
; O
WSC O
( O
Levesque O
et O
al O
. O
, O
2012 O
) O
; O
RTE O
( O
Dagan O
et O
al O
. O
, O
2005 O
, O
et O
seq O
) O
, O
WiC O
( O
Pilehvar O
and O
Camacho O
- O
Collados O
, O
2019 O
) O
; O
and O
MultiRC O
( O
Khashabi O
et O
al O
. O
, O
2018 O
) O
. O
We O
experiment O
with O
the O
BERT O
Large O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
and O
RoBERTa O
Large O
models O
. O
We O
follow O
Pruksachatkun O
et O
al O
. O
( O
2020 O
) O
for O
training O
hyperparameters O
. O
We O
use O
the O
Adam O
optimizer O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
. O
We O
run O
experiments O
using O
the O
jiant O
toolkit O
( O
Wang O
et O
al O
. O
, O
2019b O
) O
, O
which O
is O
the O
recommended O
baseline O
package O
for O
SuperGLUE O
, O
and O
is O
based O
on O
Pytorch O
( O
Paszke O
et O
al O
. O
, O
2019 O
) O
, O
HuggingFace O
Transformers O
( O
Wolf O
et O
al O
. O
, O
2020 O
) O
, O
and O
AllenNLP O
( O
Gardner O
et O
al O
. O
, O
2017 O
) O
. O
4.1 O
NLI O
Experiments O
Table O
4 O
reports O
the O
model O
performance O
on O
individual O
test O
sets O
. O
We O
include O
a O
baseline O
training O
data O
, O
a O
3k O
randomly O
sampled O
training O
examples O
from O
MNLI O
( O
MNLI-3k O
) O
. O
We O
observe O
that O
all O
the677Intermediate O
COPA O
MultiRC O
RTE O
WiC O
WSCAvg.training O
data O
acc O
. O
F1 O
Œ± O
acc O
. O
acc O
. O
acc O
. O
BERTNone O
70.0 O
70.9 O
73.3 O
72.7 O
62.5 O
69.9 O
MNLI-3k O
+0.0 O
-0.1 O
+4.0 O
-0.8 O
-2.9 O
+0.0 O
Base O
- O
News O
+1.0 O
-0.5 O
+4.3 O
-1.7 O
+1.0 O
+0.8 O
Base O
- O
Wiki O
+2.0 O
+0.3 O
+3.2 O
-1.2 O
-1.0 O
+0.7 O
Sim O
- O
News O
+3.0 O
-0.3 O
+2.2 O
-2.3 O
+0.0 O
+0.5 O
Sim O
- O
Wiki O
+7.0 O
-0.2 O
+4.0 O
-2.6 O
-3.8 O
+0.9 O
Translate O
- O
Wiki O
+4.0 O
+0.1 O
+2.5 O
-3.7 O
0.0 O
+0.6RoBERTaNone O
88.0 O
77.0 O
85.2 O
71.9 O
67.3 O
77.9 O
MNLI-3k O
-4.0 O
-0.1 O
+0.7 O
+0.2 O
-3.8 O
-1.5 O
Base O
- O
News O
+1.0 O
+0.4 O
+1.1 O
+0.7 O
-1.9 O
+0.3 O
Base O
- O
Wiki O
-2.0 O
-1.2 O
+1.1 O
+0.5 O
-1.0 O
-0.5 O
Sim O
- O
News O
-6.0 O
-3.6 O
-6.1 O
-0.1 O
-3.8 O
-3.9 O
Sim O
- O
Wiki O
-5.0 O
-1.9 O
-2.2 O
-1.2 O
-16.3 O
-5.3 O
Translate O
- O
Wiki O
-5.0 O
-2.7 O
-2.5 O
-1.8 O
-6.7 O
-3.7 O
Table O
6 O
: O
Results O
on O
using O
each O
collected O
dataset O
as O
intermediate O
training O
data O
on O
Ô¨Åve O
SuperGLUE O
tasks O
. O
We O
report O
the O
median O
performance O
over O
3 O
random O
restarts O
on O
the O
intermediate O
NLI O
models O
. O
None O
denotes O
experiments O
without O
intermediate O
- O
task O
training O
, O
i.e. O
, O
direct O
Ô¨Åne O
- O
tuning O
on O
target O
tasks O
. O
The O
last O
column O
shows O
the O
average O
score O
across O
the O
Ô¨Åve O
tasks O
. O
We O
report O
the O
difference O
with O
respect O
to O
None O
using O
BERT O
and O
RoBERTa O
. O
CBoW O
baselines O
obtain O
near O
chance O
performance O
. O
Using O
RoBERTa O
, O
the O
top O
performing O
models O
are O
all O
trained O
on O
datasets O
collected O
using O
BASE O
: O
BaseNews O
and O
MNLI-3k O
. O
We O
Ô¨Ånd O
that O
models O
trained O
using O
Translate O
- O
Wiki O
obtain O
the O
worst O
performance O
. O
On O
average O
across O
all O
training O
sets O
, O
ANLI O
development O
sets O
seem O
to O
be O
the O
hardest O
, O
while O
MNLI O
seems O
to O
be O
the O
easiest O
. O
Unsurprisingly O
, O
we O
do O
not O
Ô¨Ånd O
a O
single O
training O
set O
which O
yields O
the O
best O
model O
across O
all O
test O
sets O
. O
We O
observe O
that O
models O
trained O
on O
Base O
- O
News O
perform O
the O
best O
for O
Base O
- O
News O
and O
Base O
- O
Wiki O
test O
sets O
. O
Similarly O
, O
Sim O
- O
Wiki O
performs O
the O
best O
on O
both O
Sim O
- O
Wiki O
and O
Sim O
- O
News O
test O
sets O
. O
We O
Ô¨Ånd O
that O
all O
models O
do O
poorly O
on O
all O
ANLI O
development O
sets O
. O
Overall O
, O
we O
Ô¨Ånd O
that O
Base O
- O
News O
outperforms O
all O
other O
datasets O
. O
However O
, O
it O
is O
also O
better O
than O
SIMandTRANSLATE O
which O
suggests O
that O
our O
new O
protocols O
failed O
. O
The O
lower O
accuracy O
for O
SIMand O
TRANSLATE O
on O
their O
respective O
test O
sets O
also O
suggests O
that O
they O
produce O
datasets O
with O
noisier O
labels O
. O
4.2 O
Hypothesis O
- O
Only O
Results O
Next O
, O
we O
experiment O
with O
a O
hypothesis O
- O
only O
model O
( O
Poliak O
et O
al O
. O
, O
2018b O
) O
to O
investigate O
spurious O
statistical O
patterns O
in O
the O
hypotheses O
which O
might O
signal O
the O
actual O
labels O
to O
the O
model O
. O
Table O
5 O
reports O
the O
results O
for O
all O
Ô¨Åve O
datasets O
and O
MNLI O
. O
On O
the O
Ô¨Åve O
new O
test O
sets O
, O
we O
observe O
that O
MNLI O
and O
BaseNews O
are O
the O
most O
solvable O
by O
the O
hypothesis O
- O
only O
models O
, O
though O
their O
numbers O
are O
still O
much O
lowerthan O
with O
SNLI O
with O
accuracy O
69.17 O
. O
On O
average O
across O
all O
test O
sets O
, O
none O
of O
the O
training O
sets O
obtain O
much O
higher O
performance O
than O
chance O
. O
All O
models O
achieve O
chance O
performance O
on O
ANLI O
. O
However O
, O
all O
of O
our O
training O
sets O
are O
fairly O
small O
, O
and O
these O
numbers O
might O
not O
be O
very O
informative O
. O
This O
also O
explains O
why O
these O
numbers O
are O
relatively O
lower O
than O
other O
NLI O
datasets O
( O
Poliak O
et O
al O
. O
, O
2018b O
) O
. O
Across O
all O
training O
sets O
, O
we O
again O
see O
that O
the O
MNLI O
test O
set O
is O
the O
most O
solvable O
by O
the O
hypothesis O
- O
only O
models O
. O
Our O
new O
protocols O
show O
lower O
performance O
than O
theBASE O
, O
but O
that O
may O
just O
be O
because O
they O
are O
of O
lower O
overall O
quality O
and O
not O
because O
they O
are O
less O
solvable O
by O
the O
hypothesis O
- O
only O
models O
. O
We O
verify O
this O
by O
looking O
at O
their O
transfer O
learning O
performance O
in O
the O
following O
section O
. O
4.3 O
Transfer O
Learning O
Table O
6 O
shows O
our O
results O
when O
using O
each O
collected O
data O
as O
intermediate O
- O
training O
data O
on O
the O
Ô¨Åve O
target O
tasks O
. O
We O
report O
the O
median O
performance O
of O
three O
random O
restarts O
on O
the O
validation O
sets O
. O
Using O
BERT O
, O
we O
observe O
that O
all O
our O
new O
datasets O
yield O
models O
with O
better O
performance O
than O
plain O
BERT O
or O
MNLI-3k O
as O
intermediate O
- O
training O
data O
. O
We O
see O
less O
positive O
transfer O
when O
we O
use O
RoBERTa O
. O
If O
we O
look O
at O
individual O
target O
task O
performance O
, O
both O
Base O
- O
News O
and O
Base O
- O
Wiki O
data O
give O
consistent O
positive O
transfer O
for O
RTE O
, O
a O
natural O
language O
inference O
task O
. O
We O
also O
see O
some O
positive O
trans-678Entailment O
Contradiction O
NeutralM-3klooked O
0.44 O
no O
1.03 O
also O
0.75 O
capital O
0.43 O
never O
0.95 O
because O
0.71 O
population O
0.43 O
any O
0.88 O
better O
0.63B O
- O
Newsaccording O
0.58 O
never O
1.07 O
also O
0.62 O
position O
0.45 O
no O
1.02 O
many O
0.52 O
set O
0.42 O
any O
0.90 O
most O
0.52B O
- O
Wikiboth O
0.45 O
never O
1.18 O
most O
0.78 O
named O
0.38 O
not O
1.01 O
well O
0.64 O
early O
0.35 O
any O
0.96 O
many O
0.56S O
- O
Gigasummit O
0.53 O
points O
0.66 O
very O
0.54 O
roads O
0.51 O
we O
0.65 O
research O
0.48 O
weighted O
0.46 O
‚Äì O
0.59 O
weeks O
0.48S O
- O
Wikidivision O
0.56 O
census O
0.88 O
through O
0.57 O
team O
0.48 O
population O
0.86 O
such O
0.54 O
candidate O
0.47 O
2010 O
0.82 O
number O
0.49T O
- O
Wiki O
; O
0.68 O
brought O
0.45 O
each O
0.57 O
album O
0.58 O
maintain O
0.40 O
{ O
0.56 O
f O
0.55 O
will O
0.39 O
} O
0.56 O
Table O
7 O
: O
Top O
three O
words O
most O
associated O
with O
each O
label O
by O
PMI O
. O
M O
: O
MNLI O
, O
B O
: O
Base O
, O
S O
: O
Sim O
, O
T O
: O
Translate O
. O
fer O
for O
COPA O
, O
however O
since O
its O
validation O
set O
is O
very O
small O
( O
100 O
examples O
) O
, O
we O
can O
not O
conclude O
anything O
with O
conÔ¨Ådence O
. O
Overall O
, O
our O
BASEshows O
better O
transfer O
learning O
performance O
compared O
to O
MNLI O
, O
suggesting O
that O
our O
setup O
is O
sound O
. O
However O
, O
we O
also O
see O
that O
our O
new O
protocols O
perform O
worse O
than O
BASE O
, O
showing O
that O
they O
produce O
less O
useful O
training O
data O
than O
the O
strong O
baseline O
of O
crowdworker O
writing O
. O
5 O
Dataset O
Analysis O
5.1 O
Annotation O
Artifacts O
Following O
Gururangan O
et O
al O
. O
( O
2018 O
) O
, O
we O
compute O
the O
PMI O
between O
each O
hypothesis O
word O
and O
label O
in O
the O
training O
set O
to O
examine O
whether O
certain O
words O
have O
high O
associations O
with O
its O
inference O
label O
. O
For O
a O
fair O
comparison O
, O
we O
only O
use O
‚àº3k O
training O
examples O
from O
each O
dataset O
, O
and O
sub O
- O
sample O
data O
collected O
using O
S O
IMand O
T O
RANSLATE O
. O
Table O
7 O
shows O
the O
top O
three O
most O
associated O
words O
for O
each O
label O
, O
sorted O
by O
their O
PMI O
scores O
. O
We O
Ô¨Ånd O
that O
BASE O
has O
similar O
associations O
to O
MNLI O
, O
especially O
for O
the O
neutral O
and O
contradiction O
labels O
where O
we O
found O
many O
negations O
and O
adverbs O
. O
We O
observe O
that O
both O
SIMandTRANS O
LATE O
are O
less O
susceptible O
to O
this O
artifact O
. O
However O
, O
this O
might O
be O
a O
side O
- O
effect O
of O
high O
word O
overlap O
in O
the O
data O
, O
which O
prefers O
similar O
words O
in O
the O
premise O
and O
hypothesis O
. O
This O
is O
also O
a O
well O
- O
known O
artifact O
for O
NLI O
data O
( O
McCoy O
et O
al O
. O
, O
2019).5.2 O
Qualitative O
Analysis O
Our O
new O
protocols O
use O
a O
vector O
- O
distance O
based O
measurement O
to O
Ô¨Ånd O
similar O
sentences O
, O
and O
we O
Ô¨Ånd O
that O
many O
of O
the O
sentence O
pairs O
share O
similar O
syntactic O
structure O
in O
their O
premise O
and O
hypothesis O
, O
even O
when O
both O
describe O
different O
events O
or O
entities O
. O
We O
also O
Ô¨Ånd O
that O
hypothesis O
in O
several O
SimNews O
examples O
differs O
by O
only O
a O
few O
words O
with O
its O
premise O
. O
For O
Translate O
- O
Wiki O
, O
we O
observe O
some O
effects O
of O
translation O
divergence O
, O
where O
the O
translation O
of O
the O
sentence O
changes O
semantically O
because O
of O
cross O
- O
linguistic O
distinctions O
between O
languages O
. O
We O
provide O
some O
examples O
of O
these O
observations O
in O
Table O
8 O
. O
6 O
Related O
Work O
There O
is O
a O
large O
body O
of O
work O
on O
constructing O
data O
for O
natural O
language O
inference O
. O
The O
Ô¨Årst O
test O
suite O
for O
entailment O
problems O
, O
FraCas O
( O
Consortium O
et O
al O
. O
, O
1996 O
) O
, O
is O
a O
very O
small O
set O
created O
manually O
by O
experts O
to O
isolate O
phenomena O
of O
interest O
. O
The O
RTE O
challenge O
corpora O
( O
Dagan O
et O
al O
. O
, O
2005 O
, O
et O
seq O
) O
were O
built O
by O
asking O
human O
annotators O
to O
judge O
whether O
a O
text O
entails O
a O
hypothesis O
. O
The O
SICK O
dataset O
( O
Marelli O
et O
al O
. O
, O
2014 O
) O
is O
constructed O
by O
mining O
existing O
paraphrase O
sentence O
pairs O
from O
image O
and O
video O
captions O
, O
which O
annotators O
then O
label O
. O
Some O
recent O
works O
also O
use O
automatic O
methods O
for O
generating O
sentence O
pairs O
for O
entailment O
data O
. O
Zhang O
et O
al O
. O
( O
2017 O
) O
propose O
a O
framework O
to O
generate O
hypotheses O
based O
on O
context O
from O
general O
world O
knowledge O
or O
neural O
sequence O
- O
to O
- O
sequence O
methods O
. O
The O
DNC O
corpus O
( O
Poliak O
et O
al O
. O
, O
2018a O
) O
is O
an O
NLI O
dataset O
with O
ordinal O
judgments O
constructed O
by O
recasting O
several O
NLP O
datasets O
to O
NLI O
examples O
and O
labeling O
them O
using O
custom O
automatic O
procedures O
. O
QA O
- O
NLI O
( O
Demszky O
et O
al O
. O
, O
2018 O
) O
is O
an O
NLI O
dataset O
derived O
from O
existing O
QA O
datasets O
. O
Similar O
to O
ours O
, O
both O
DNC O
and O
QA O
- O
NLI O
use O
automatic O
methods O
to O
generate O
sentence O
pairs O
. O
However O
, O
neither O
of O
them O
explicitly O
evaluates O
whether O
machinegenerated O
pairs O
are O
better O
than O
human O
- O
generated O
pairs O
. O
Bowman O
et O
al O
. O
( O
2020 O
) O
propose O
four O
potential O
modiÔ¨Åcations O
to O
the O
SNLI O
/ O
MNLI O
protocol O
, O
all O
still O
involving O
crowdworker O
writing O
, O
and O
show O
that O
none O
yields O
improvements O
in O
the O
resulting O
data O
. O
SWAG O
( O
Zellers O
et O
al O
. O
, O
2018 O
) O
and O
HellaSwag O
( O
Zellers O
et O
al O
. O
, O
2019 O
) O
construct O
sentence O
pairs O
from O
speciÔ¨Åc O
data O
sources O
and O
use O
language O
models O
to O
generate O
challenging O
negative O
examples.679Type O
Dataset O
Premise O
Hypothesis O
Label O
Syntactic O
structureSim O
- O
NewsFor O
many O
people O
, O
choosing O
wallpaper O
is O
one O
of O
decorating O
‚Äôs O
more O
stressful O
experiences O
, O
fraught O
with O
anxiety O
over O
color O
, O
pattern O
and O
cost O
. O
For O
many O
people O
, O
anxiety O
about O
decorating O
stems O
from O
not O
understanding O
the O
language O
of O
furniture O
, O
fabrics O
and O
decorative O
styles O
. O
E O
Sim O
- O
WikiIts O
Ô¨Çowers O
are O
pale O
yellow O
towhite O
andspherical O
. O
Its O
Ô¨Çowers O
are O
funnel O
- O
shaped O
and O
pink O
towhite O
. O
C O
Translate O
- O
WikiBut O
now O
, O
in O
the O
early O
1990s O
, O
the O
Jakarta O
- O
Begor O
railway O
had O
turned O
into O
a O
double O
rail O
. O
However O
, O
by O
the O
early O
1990s O
, O
McCreery O
‚Äôs O
position O
within O
the O
UDA O
became O
less O
secure O
. O
N O
Lexical O
overlapSim O
- O
NewsGrandMet O
owns O
Burger O
King O
, O
the O
world O
‚Äôs O
second O
- O
biggest O
hamburger O
chain O
, O
as O
well O
as O
US O
frozen O
foods O
manufacturer O
Pillsbury O
, O
which O
produces O
the O
luxury O
ice O
- O
cream O
HaagenDaazs O
. O
GrandMet O
owns O
Burger O
King O
, O
the O
world O
‚Äôs O
second O
- O
biggest O
hamburger O
chain O
, O
as O
well O
as O
US O
food O
group O
Pillsbury O
, O
which O
produces O
the O
luxury O
icecream O
Haagen O
- O
Daazs O
. O
E O
Translation O
divergenceTranslate O
- O
WikiMarcus O
Claudius O
then O
abducted O
her O
while O
shewas O
on O
herway O
to O
school O
. O
Marcus O
Claudius O
then O
kidnapped O
him O
while O
hewas O
on O
hisway O
to O
school O
. O
N O
Table O
8 O
: O
Dataset O
observations O
from O
our O
new O
protocols O
. O
On O
the O
topic O
of O
cost O
- O
effective O
crowdsourcing O
, O
Gao O
et O
al O
. O
( O
2015 O
) O
develop O
a O
method O
to O
reduce O
redundant O
translations O
when O
collecting O
human O
translated O
data O
. O
When O
the O
annotation O
budget O
is O
Ô¨Åxed O
, O
Khetan O
et O
al O
. O
( O
2018 O
) O
suggest O
that O
it O
is O
better O
to O
label O
collect O
single O
label O
per O
training O
example O
as O
many O
as O
possible O
, O
rather O
than O
collecting O
less O
training O
examples O
with O
multiple O
labels O
. O
7 O
Conclusion O
In O
this O
paper O
, O
we O
introduce O
two O
data O
collection O
protocols O
which O
use O
fully O
- O
automatic O
pipelines O
to O
collect O
hypotheses O
, O
replacing O
crowdworker O
writing O
in O
the O
MNLI O
baseline O
protocol O
. O
We O
Ô¨Ånd O
that O
switching O
to O
a O
writing O
- O
free O
process O
with O
the O
same O
source O
data O
and O
annotator O
pool O
yields O
poor O
- O
quality O
data O
. O
Our O
main O
experiments O
show O
strong O
negative O
results O
both O
in O
NLI O
generalization O
and O
transfer O
learning O
, O
and O
mixed O
results O
on O
annotation O
artifacts O
, O
suggesting O
that O
MNLI O
- O
style O
crowdworker O
writing O
examples O
are O
broadly O
better O
than O
automatically O
paired O
ones O
. O
This O
Ô¨Ånding O
dovetails O
with O
that O
of O
Bowman O
et O
al O
. O
( O
2020 O
) O
, O
who O
Ô¨Ånd O
that O
they O
are O
unable O
to O
improve O
upon O
a O
base O
MNLI O
- O
style O
prompt O
when O
introducing O
aids O
meant O
to O
improve O
annotator O
speed O
or O
creativity O
. O
Future O
work O
along O
this O
line O
might O
focus O
on O
crowdsourcing O
strategies O
( O
beyond O
the O
basic O
HIT O
design O
) O
which O
encourage O
crowdworkers O
to O
produce O
high O
- O
quality O
data O
with O
reduced O
artifacts O
. O
While O
our O
fully O
- O
automatic O
methods O
to O
construct O
sentence O
pairs O
yield O
negative O
results O
, O
we O
have O
not O
exhausted O
all O
possible O
automatic O
techniques O
for O
collecting O
similar O
sentences O
. O
However O
, O
giventhat O
we O
use O
state O
- O
of O
- O
the O
- O
art O
tools O
including O
FAISS O
, O
RoBERTa O
, O
and O
OPUS O
, O
and O
reÔ¨Åne O
our O
methods O
with O
several O
rounds O
of O
piloting O
and O
tuning O
, O
we O
are O
skeptical O
that O
there O
is O
low O
- O
hanging O
fruit O
in O
the O
two O
directions O
we O
explored O
. O
A O
more O
radically O
different O
direction O
might O
involve O
generating O
pairs O
from O
scratch O
, O
using O
a O
large O
language O
model O
like O
GPT-3 O
( O
Brown O
et O
al O
. O
, O
2020 O
) O
. O
However O
, O
this O
would O
still O
require O
training O
data O
from O
crowdworker O
- O
written O
dataset O
, O
and O
might O
add O
a O
major O
source O
of O
potentially O
difÔ¨Åcult O
- O
todiagnose O
bias O
. O
Finally O
, O
despite O
its O
known O
issues O
, O
we O
Ô¨Ånd O
that O
MNLI O
- O
style O
data O
is O
still O
the O
most O
effective O
for O
both O
NLI O
evaluation O
and O
transfer O
learning O
, O
and O
future O
efforts O
to O
create O
similar O
data O
should O
work O
from O
that O
starting O
point O
. O
Acknowledgments O
This O
project O
has O
beneÔ¨Åted O
from O
Ô¨Ånancial O
support O
to O
SB O
by O
Eric O
and O
Wendy O
Schmidt O
( O
made O
by O
recommendation O
of O
the O
Schmidt O
Futures O
program O
) O
, O
by O
Samsung O
Research O
( O
under O
the O
project O
Improving O
Deep O
Learning O
using O
Latent O
Structure O
) O
, O
by O
Intuit O
, O
Inc. O
, O
and O
in O
- O
kind O
support O
by O
the O
NYU O
HighPerformance O
Computing O
Center O
and O
by O
NVIDIA O
Corporation O
( O
with O
the O
donation O
of O
a O
Titan O
V O
GPU O
) O
. O
This O
material O
is O
based O
upon O
work O
supported O
by O
the O
National O
Science O
Foundation O
under O
Grant O
No O
. O
1922658 O
. O
Any O
opinions O
, O
Ô¨Åndings O
, O
and O
conclusions O
or O
recommendations O
expressed O
in O
this O
material O
are O
those O
of O
the O
author(s O
) O
and O
do O
not O
necessarily O
reÔ¨Çect O
the O
views O
of O
the O
National O
Science O
Foundation.680References O
Takuya O
Akiba O
, O
Shotaro O
Sano O
, O
Toshihiko O
Yanase O
, O
Takeru O
Ohta O
, O
and O
Masanori O
Koyama O
. O
2019 O
. O
Optuna O
: O
A O
Next O
- O
generation O
Hyperparameter O
Optimization O
Framework O
. O
In O
Proceedings O
of O
the O
25rd O
ACM O
SIGKDD O
International O
Conference O
on O
Knowledge O
Discovery O
and O
Data O
Mining O
. O
Mikel O
Artetxe O
and O
Holger O
Schwenk O
. O
2019 O
. O
Marginbased O
parallel O
corpus O
mining O
with O
multilingual O
sentence O
embeddings O
. O
In O
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
3197‚Äì3203 O
, O
Florence O
, O
Italy O
. O
Association O
for O
Computational O
Linguistics O
. O
Piotr O
Bojanowski O
, O
Edouard O
Grave O
, O
Armand O
Joulin O
, O
and O
Tomas O
Mikolov O
. O
2017 O
. O
Enriching O
word O
vectors O
with O
subword O
information O
. O
Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
5:135‚Äì146 O
. O
Samuel O
R. O
Bowman O
, O
Gabor O
Angeli O
, O
Christopher O
Potts O
, O
and O
Christopher O
D. O
Manning O
. O
2015 O
. O
A O
large O
annotated O
corpus O
for O
learning O
natural O
language O
inference O
. O
InProceedings O
of O
the O
2015 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
632‚Äì642 O
, O
Lisbon O
, O
Portugal O
. O
Association O
for O
Computational O
Linguistics O
. O
Samuel O
R. O
Bowman O
, O
Jennimaria O
Palomaki O
, O
Livio O
Baldini O
Soares O
, O
and O
Emily O
Pitler O
. O
2020 O
. O
Collecting O
Entailment O
Data O
for O
Pretraining O
: O
New O
Protocols O
and O
Negative O
Results O
. O
Tom O
B. O
Brown O
, O
Benjamin O
Mann O
, O
Nick O
Ryder O
, O
Melanie O
Subbiah O
, O
Jared O
Kaplan O
, O
Prafulla O
Dhariwal O
, O
Arvind O
Neelakantan O
, O
Pranav O
Shyam O
, O
Girish O
Sastry O
, O
Amanda O
Askell O
, O
Sandhini O
Agarwal O
, O
Ariel O
Herbert O
- O
V O
oss O
, O
Gretchen O
Krueger O
, O
Tom O
Henighan O
, O
Rewon O
Child O
, O
Aditya O
Ramesh O
, O
Daniel O
M. O
Ziegler O
, O
Jeffrey O
Wu O
, O
Clemens O
Winter O
, O
Christopher O
Hesse O
, O
Mark O
Chen O
, O
Eric O
Sigler O
, O
Mateusz O
Litwin O
, O
Scott O
Gray O
, O
Benjamin O
Chess O
, O
Jack O
Clark O
, O
Christopher O
Berner O
, O
Sam O
McCandlish O
, O
Alec O
Radford O
, O
Ilya O
Sutskever O
, O
and O
Dario O
Amodei O
. O
2020 O
. O
Language O
Models O
are O
Few O
- O
Shot O
Learners O
. O
Alexis O
Conneau O
, O
Ruty O
Rinott O
, O
Guillaume O
Lample O
, O
Adina O
Williams O
, O
Samuel O
Bowman O
, O
Holger O
Schwenk O
, O
and O
Veselin O
Stoyanov O
. O
2018 O
. O
XNLI O
: O
Evaluating O
cross O
- O
lingual O
sentence O
representations O
. O
In O
Proceedings O
of O
the O
2018 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
2475‚Äì2485 O
, O
Brussels O
, O
Belgium O
. O
Association O
for O
Computational O
Linguistics O
. O
The O
Fracas O
Consortium O
, O
Robin O
Cooper O
, O
Dick O
Crouch O
, O
Jan O
Van O
Eijck O
, O
Chris O
Fox O
, O
Josef O
Van O
Genabith O
, O
Jan O
Jaspars O
, O
Hans O
Kamp O
, O
David O
Milward O
, O
Manfred O
Pinkal O
, O
Massimo O
Poesio O
, O
Steve O
Pulman O
, O
Ted O
Briscoe O
, O
Holger O
Maier O
, O
and O
Karsten O
Konrad O
. O
1996 O
. O
Using O
the O
Framework O
. O
Ido O
Dagan O
, O
Oren O
Glickman O
, O
and O
Bernardo O
Magnini O
. O
2005 O
. O
The O
pascal O
recognising O
textual O
entailmentchallenge O
. O
In O
Machine O
Learning O
Challenges O
Workshop O
, O
pages O
177‚Äì190 O
. O
Springer O
. O
Dorottya O
Demszky O
, O
Kelvin O
Guu O
, O
and O
Percy O
Liang O
. O
2018 O
. O
Transforming O
Question O
Answering O
Datasets O
Into O
Natural O
Language O
Inference O
Datasets O
. O
Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O
BERT O
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
understanding O
. O
In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
4171‚Äì4186 O
, O
Minneapolis O
, O
Minnesota O
. O
Association O
for O
Computational O
Linguistics O
. O
Mingkun O
Gao O
, O
Wei O
Xu O
, O
and O
Chris O
Callison O
- O
Burch O
. O
2015 O
. O
Cost O
optimization O
in O
crowdsourcing O
translation O
: O
Low O
cost O
translations O
made O
even O
cheaper O
. O
In O
Proceedings O
of O
the O
2015 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
pages O
705‚Äì713 O
, O
Denver O
, O
Colorado O
. O
Association O
for O
Computational O
Linguistics O
. O
Matt O
Gardner O
, O
Joel O
Grus O
, O
Mark O
Neumann O
, O
Oyvind O
Tafjord O
, O
Pradeep O
Dasigi O
, O
Nelson O
F. O
Liu O
, O
Matthew O
Peters O
, O
Michael O
Schmitz O
, O
and O
Luke O
S. O
Zettlemoyer O
. O
2017 O
. O
AllenNLP O
: O
A O
Deep O
Semantic O
Natural O
Language O
Processing O
Platform O
. O
Unpublished O
manuscript O
available O
on O
arXiv O
. O
Suchin O
Gururangan O
, O
Swabha O
Swayamdipta O
, O
Omer O
Levy O
, O
Roy O
Schwartz O
, O
Samuel O
Bowman O
, O
and O
Noah O
A. O
Smith O
. O
2018 O
. O
Annotation O
artifacts O
in O
natural O
language O
inference O
data O
. O
In O
Proceedings O
of O
the O
2018 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
2 O
( O
Short O
Papers O
) O
, O
pages O
107‚Äì112 O
, O
New O
Orleans O
, O
Louisiana O
. O
Association O
for O
Computational O
Linguistics O
. O
J. O
Johnson O
, O
M. O
Douze O
, O
and O
H. O
J O
¬¥ O
egou O
. O
2019 O
. O
Billionscale O
similarity O
search O
with O
GPUs O
. O
IEEE O
Transactions O
on O
Big O
Data O
. O
Daniel O
Khashabi O
, O
Snigdha O
Chaturvedi O
, O
Michael O
Roth O
, O
Shyam O
Upadhyay O
, O
and O
Dan O
Roth O
. O
2018 O
. O
Looking O
Beyond O
the O
Surface O
: O
A O
Challenge O
Set O
for O
Reading O
Comprehension O
over O
Multiple O
Sentences O
. O
In O
Proceedings O
of O
the O
2018 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
Papers O
) O
, O
pages O
252‚Äì262 O
, O
New O
Orleans O
, O
Louisiana O
. O
Association O
for O
Computational O
Linguistics O
. O
Ashish O
Khetan O
, O
Zachary O
C. O
Lipton O
, O
and O
Anima O
Anandkumar O
. O
2018 O
. O
Learning O
From O
Noisy O
Singly O
- O
labeled O
Data O
. O
In O
International O
Conference O
on O
Learning O
Representations O
. O
Diederik O
P. O
Kingma O
and O
Jimmy O
Ba O
. O
2015 O
. O
Adam O
: O
A O
Method O
for O
Stochastic O
Optimization O
. O
In O
3rd O
International O
Conference O
on O
Learning O
Representations,681ICLR O
2015 O
, O
San O
Diego O
, O
CA O
, O
USA O
, O
May O
7 O
- O
9 O
, O
2015 O
, O
Conference O
Track O
Proceedings O
. O
Hector O
J. O
Levesque O
, O
Ernest O
Davis O
, O
and O
Leora O
Morgenstern O
. O
2012 O
. O
The O
Winograd O
Schema O
Challenge O
. O
InProceedings O
of O
the O
Thirteenth O
International O
Conference O
on O
Principles O
of O
Knowledge O
Representation O
and O
Reasoning O
, O
KR‚Äô12 O
, O
pages O
552‚Äì561 O
. O
AAAI O
Press O
. O
Yinhan O
Liu O
, O
Myle O
Ott O
, O
Naman O
Goyal O
, O
Jingfei O
Du O
, O
Mandar O
Joshi O
, O
Danqi O
Chen O
, O
Omer O
Levy O
, O
Mike O
Lewis O
, O
Luke O
Zettlemoyer O
, O
and O
Veselin O
Stoyanov O
. O
2019 O
. O
RoBERTa O
: O
A O
Robustly O
Optimized O
BERT O
Pretraining O
Approach O
. O
Marco O
Marelli O
, O
Stefano O
Menini O
, O
Marco O
Baroni O
, O
Luisa O
Bentivogli O
, O
Raffaella O
Bernardi O
, O
and O
Roberto O
Zamparelli O
. O
2014 O
. O
A O
SICK O
cure O
for O
the O
evaluation O
of O
compositional O
distributional O
semantic O
models O
. O
In O
Proceedings O
of O
the O
Ninth O
International O
Conference O
on O
Language O
Resources O
and O
Evaluation O
( O
LREC-2014 O
) O
, O
pages O
216‚Äì223 O
, O
Reykjavik O
, O
Iceland O
. O
European O
Languages O
Resources O
Association O
( O
ELRA O
) O
. O
Tom O
McCoy O
, O
Ellie O
Pavlick O
, O
and O
Tal O
Linzen O
. O
2019 O
. O
Right O
for O
the O
wrong O
reasons O
: O
Diagnosing O
syntactic O
heuristics O
in O
natural O
language O
inference O
. O
In O
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
3428‚Äì3448 O
, O
Florence O
, O
Italy O
. O
Association O
for O
Computational O
Linguistics O
. O
Marius O
Mosbach O
, O
Maksym O
Andriushchenko O
, O
and O
Dietrich O
Klakow O
. O
2020 O
. O
On O
the O
Stability O
of O
Fine O
- O
tuning O
BERT O
: O
Misconceptions O
, O
Explanations O
, O
and O
Strong O
Baselines O
. O
Yixin O
Nie O
, O
Adina O
Williams O
, O
Emily O
Dinan O
, O
Mohit O
Bansal O
, O
Jason O
Weston O
, O
and O
Douwe O
Kiela O
. O
2020 O
. O
Adversarial O
NLI O
: O
A O
new O
benchmark O
for O
natural O
language O
understanding O
. O
In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
4885‚Äì4901 O
, O
Online O
. O
Association O
for O
Computational O
Linguistics O
. O
Robert O
Parker O
, O
David O
Graff O
, O
Junbo O
Kong O
, O
Ke O
Chen O
, O
and O
Kazuaki O
Maeda O
. O
2011 O
. O
English O
Gigaword O
Fifth O
Edition O
LDC2011T07 O
. O
Adam O
Paszke O
, O
Sam O
Gross O
, O
Francisco O
Massa O
, O
Adam O
Lerer O
, O
James O
Bradbury O
, O
Gregory O
Chanan O
, O
Trevor O
Killeen O
, O
Zeming O
Lin O
, O
Natalia O
Gimelshein O
, O
Luca O
Antiga O
, O
Alban O
Desmaison O
, O
Andreas O
Kopf O
, O
Edward O
Yang O
, O
Zachary O
DeVito O
, O
Martin O
Raison O
, O
Alykhan O
Tejani O
, O
Sasank O
Chilamkurthy O
, O
Benoit O
Steiner O
, O
Lu O
Fang O
, O
Junjie O
Bai O
, O
and O
Soumith O
Chintala O
. O
2019 O
. O
PyTorch O
: O
An O
Imperative O
Style O
, O
High O
- O
Performance O
Deep O
Learning O
Library O
. O
In O
H. O
Wallach O
, O
H. O
Larochelle O
, O
A. O
Beygelzimer O
, O
F. O
d O
‚Äô O
Alch O
¬¥ O
e O
- O
Buc O
, O
E. O
Fox O
, O
and O
R. O
Garnett O
, O
editors O
, O
Advances O
in O
Neural O
Information O
Processing O
Systems O
32 O
, O
pages O
8024‚Äì8035 O
. O
Curran O
Associates O
, O
Inc. O
Jason O
Phang O
, O
Thibault O
F O
¬¥ O
evry O
, O
and O
Samuel O
R. O
Bowman O
. O
2018 O
. O
Sentence O
Encoders O
on O
STILTs O
: O
Supplementary O
Training O
on O
Intermediate O
Labeled O
- O
data O
Tasks O
. O
Mohammad O
Taher O
Pilehvar O
and O
Jose O
CamachoCollados O
. O
2019 O
. O
WiC O
: O
the O
Word O
- O
in O
- O
Context O
Dataset O
for O
Evaluating O
Context O
- O
Sensitive O
Meaning O
Representations O
. O
In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
1267‚Äì1273 O
, O
Minneapolis O
, O
Minnesota O
. O
Association O
for O
Computational O
Linguistics O
. O
Adam O
Poliak O
, O
Aparajita O
Haldar O
, O
Rachel O
Rudinger O
, O
J. O
Edward O
Hu O
, O
Ellie O
Pavlick O
, O
Aaron O
Steven O
White O
, O
and O
Benjamin O
Van O
Durme O
. O
2018a O
. O
Collecting O
diverse O
natural O
language O
inference O
problems O
for O
sentence O
representation O
evaluation O
. O
In O
Proceedings O
of O
the O
2018 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
67‚Äì81 O
, O
Brussels O
, O
Belgium O
. O
Association O
for O
Computational O
Linguistics O
. O
Adam O
Poliak O
, O
Jason O
Naradowsky O
, O
Aparajita O
Haldar O
, O
Rachel O
Rudinger O
, O
and O
Benjamin O
Van O
Durme O
. O
2018b O
. O
Hypothesis O
only O
baselines O
in O
natural O
language O
inference O
. O
In O
Proceedings O
of O
the O
Seventh O
Joint O
Conference O
on O
Lexical O
and O
Computational O
Semantics O
, O
pages O
180‚Äì191 O
, O
New O
Orleans O
, O
Louisiana O
. O
Association O
for O
Computational O
Linguistics O
. O
Beatrice O
Portelli O
, O
Jason O
Zhao O
, O
Tal O
Schuster O
, O
Giuseppe O
Serra O
, O
and O
Enrico O
Santus O
. O
2020 O
. O
Distilling O
the O
Evidence O
to O
Augment O
Fact O
VeriÔ¨Åcation O
Models O
. O
In O
Proceedings O
of O
the O
Third O
Workshop O
on O
Fact O
Extraction O
and O
VERiÔ¨Åcation O
( O
FEVER O
) O
, O
pages O
47‚Äì51 O
, O
Online O
. O
Association O
for O
Computational O
Linguistics O
. O
Yada O
Pruksachatkun O
, O
Jason O
Phang O
, O
Haokun O
Liu O
, O
Phu O
Mon O
Htut O
, O
Xiaoyi O
Zhang O
, O
Richard O
Yuanzhe O
Pang O
, O
Clara O
Vania O
, O
Katharina O
Kann O
, O
and O
Samuel O
R. O
Bowman O
. O
2020 O
. O
Intermediate O
- O
task O
transfer O
learning O
with O
pretrained O
language O
models O
: O
When O
and O
why O
does O
it O
work O
? O
In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
5231‚Äì5247 O
, O
Online O
. O
Association O
for O
Computational O
Linguistics O
. O
Pranav O
Rajpurkar O
, O
Robin O
Jia O
, O
and O
Percy O
Liang O
. O
2018 O
. O
Know O
what O
you O
do O
n‚Äôt O
know O
: O
Unanswerable O
questions O
for O
SQuAD O
. O
In O
Proceedings O
of O
the O
56th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
2 O
: O
Short O
Papers O
) O
, O
pages O
784 O
‚Äì O
789 O
, O
Melbourne O
, O
Australia O
. O
Association O
for O
Computational O
Linguistics O
. O
Melissa O
Roemmele O
, O
Cosmin O
Adrian O
Bejan O
, O
and O
Andrew O
S O
Gordon O
. O
2011 O
. O
Choice O
of O
Plausible O
Alternatives O
: O
An O
evaluation O
of O
commonsense O
causal O
reasoning O
. O
In O
2011 O
AAAI O
Spring O
Symposium O
Series O
. O
Holger O
Schwenk O
. O
2018 O
. O
Filtering O
and O
mining O
parallel O
data O
in O
a O
joint O
multilingual O
space O
. O
In O
Proceedings O
of O
the O
56th O
Annual O
Meeting O
of O
the O
Association682for O
Computational O
Linguistics O
( O
Volume O
2 O
: O
Short O
Papers O
) O
, O
pages O
228‚Äì234 O
, O
Melbourne O
, O
Australia O
. O
Association O
for O
Computational O
Linguistics O
. O
Holger O
Schwenk O
, O
Vishrav O
Chaudhary O
, O
Shuo O
Sun O
, O
Hongyu O
Gong O
, O
and O
Francisco O
Guzm O
¬¥ O
an O
. O
2019 O
. O
WikiMatrix O
: O
Mining O
135 O
M O
Parallel O
Sentences O
in O
1620 O
Language O
Pairs O
from O
Wikipedia O
. O
CoRR O
, O
abs/1907.05791 O
. O
J¬®org O
Tiedemann O
and O
Santhosh O
Thottingal O
. O
2020 O
. O
OPUS O
- O
MT O
‚Äî O
Building O
open O
translation O
services O
for O
the O
World O
. O
In O
Proceedings O
of O
the O
22nd O
Annual O
Conferenec O
of O
the O
European O
Association O
for O
Machine O
Translation O
( O
EAMT O
) O
, O
Lisbon O
, O
Portugal O
. O
Harsh O
Trivedi O
, O
Heeyoung O
Kwon O
, O
Tushar O
Khot O
, O
Ashish O
Sabharwal O
, O
and O
Niranjan O
Balasubramanian O
. O
2019 O
. O
Repurposing O
entailment O
for O
multi O
- O
hop O
question O
answering O
tasks O
. O
In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
2948‚Äì2958 O
, O
Minneapolis O
, O
Minnesota O
. O
Association O
for O
Computational O
Linguistics O
. O
Masatoshi O
Tsuchiya O
. O
2018 O
. O
Performance O
impact O
caused O
by O
hidden O
bias O
of O
training O
data O
for O
recognizing O
textual O
entailment O
. O
In O
Proceedings O
of O
the O
Eleventh O
International O
Conference O
on O
Language O
Resources O
and O
Evaluation O
( O
LREC-2018 O
) O
, O
Miyazaki O
, O
Japan O
. O
European O
Languages O
Resources O
Association O
( O
ELRA O
) O
. O
Alex O
Wang O
, O
Yada O
Pruksachatkun O
, O
Nikita O
Nangia O
, O
Amanpreet O
Singh O
, O
Julian O
Michael O
, O
Felix O
Hill O
, O
Omer O
Levy O
, O
and O
Samuel O
Bowman O
. O
2019a O
. O
SuperGLUE O
: O
A O
stickier O
benchmark O
for O
general O
- O
purpose O
language O
understanding O
systems O
. O
In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
. O
Alex O
Wang O
, O
Ian O
F. O
Tenney O
, O
Yada O
Pruksachatkun O
, O
Phil O
Yeres O
, O
Jason O
Phang O
, O
Haokun O
Liu O
, O
Phu O
Mon O
Htut O
, O
Katherin O
Yu O
, O
Jan O
Hula O
, O
Patrick O
Xia O
, O
Raghu O
Pappagari O
, O
Shuning O
Jin O
, O
R. O
Thomas O
McCoy O
, O
Roma O
Patel O
, O
Yinghui O
Huang O
, O
Edouard O
Grave O
, O
Najoung O
Kim O
, O
Thibault O
F O
¬¥ O
evry O
, O
Berlin O
Chen O
, O
Nikita O
Nangia O
, O
Anhad O
Mohananey O
, O
Katharina O
Kann O
, O
Shikha O
Bordia O
, O
Nicolas O
Patry O
, O
David O
Benton O
, O
Ellie O
Pavlick O
, O
and O
Samuel O
R. O
Bowman O
. O
2019b O
. O
jiant O
1.3 O
: O
A O
software O
toolkit O
for O
research O
on O
general O
- O
purpose O
text O
understanding O
models O
. O
http://jiant.info/ O
. O
Mark O
E O
Whiting O
, O
Grant O
Hugh O
, O
and O
Michael O
S O
Bernstein O
. O
2019 O
. O
Fair O
Work O
: O
Crowd O
Work O
Minimum O
Wage O
with O
One O
Line O
of O
Code O
. O
In O
Proceedings O
of O
the O
AAAI O
Conference O
on O
Human O
Computation O
and O
Crowdsourcing O
, O
pages O
197‚Äì206 O
. O
Adina O
Williams O
, O
Nikita O
Nangia O
, O
and O
Samuel O
Bowman O
. O
2018 O
. O
A O
broad O
- O
coverage O
challenge O
corpus O
for O
sentence O
understanding O
through O
inference O
. O
In O
Proceedings O
of O
the O
2018 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume1 O
( O
Long O
Papers O
) O
, O
pages O
1112‚Äì1122 O
, O
New O
Orleans O
, O
Louisiana O
. O
Association O
for O
Computational O
Linguistics O
. O
Thomas O
Wolf O
, O
Lysandre O
Debut O
, O
Victor O
Sanh O
, O
Julien O
Chaumond O
, O
Clement O
Delangue O
, O
Anthony O
Moi O
, O
Pierric O
Cistac O
, O
Tim O
Rault O
, O
R O
¬¥ O
emi O
Louf O
, O
Morgan O
Funtowicz O
, O
Joe O
Davison O
, O
Sam O
Shleifer O
, O
Patrick O
von O
Platen O
, O
Clara O
Ma O
, O
Yacine O
Jernite O
, O
Julien O
Plu O
, O
Canwen O
Xu O
, O
Teven O
Le O
Scao O
, O
Sylvain O
Gugger O
, O
Mariama O
Drame O
, O
Quentin O
Lhoest O
, O
and O
Alexander O
M. O
Rush O
. O
2020 O
. O
Huggingface O
‚Äôs O
transformers O
: O
State O
- O
of O
- O
the O
- O
art O
natural O
language O
processing O
. O
Rowan O
Zellers O
, O
Yonatan O
Bisk O
, O
Roy O
Schwartz O
, O
and O
Yejin O
Choi O
. O
2018 O
. O
SWAG O
: O
A O
large O
- O
scale O
adversarial O
dataset O
for O
grounded O
commonsense O
inference O
. O
In O
Proceedings O
of O
the O
2018 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
93 O
‚Äì O
104 O
, O
Brussels O
, O
Belgium O
. O
Association O
for O
Computational O
Linguistics O
. O
Rowan O
Zellers O
, O
Ari O
Holtzman O
, O
Yonatan O
Bisk O
, O
Ali O
Farhadi O
, O
and O
Yejin O
Choi O
. O
2019 O
. O
HellaSwag O
: O
Can O
a O
machine O
really O
Ô¨Ånish O
your O
sentence O
? O
In O
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
4791 O
‚Äì O
4800 O
, O
Florence O
, O
Italy O
. O
Association O
for O
Computational O
Linguistics O
. O
Sheng O
Zhang O
, O
Rachel O
Rudinger O
, O
Kevin O
Duh O
, O
and O
Benjamin O
Van O
Durme O
. O
2017 O
. O
Ordinal O
common O
- O
sense O
inference O
. O
Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
5:379‚Äì395 O
. O
Tianyi O
Zhang O
, O
Felix O
Wu O
, O
Arzoo O
Katiyar O
, O
Kilian O
Q. O
Weinberger O
, O
and O
Yoav O
Artzi O
. O
2020 O
. O
Revisiting O
Fewsample O
BERT O
Fine O
- O
tuning.683A O
Appendices O
A.1 O
Indexing O
and O
Retrieval O
Gigaword O
The O
corpus O
contains O
texts O
from O
seven O
news O
sources O
: O
afp O
eng O
, O
apw O
eng O
, O
cna O
eng O
, O
ltw O
eng O
, O
nyteng O
, O
wpb O
eng O
, O
and O
xin O
eng O
. O
We O
build O
one O
index O
for O
each O
news O
source O
with O
type O
‚Äú O
PCAR64,IVFx O
, O
Flat O
‚Äù O
, O
where O
xdeÔ¨Ånes O
the O
number O
of O
clusters O
in O
the O
index O
. O
This O
type O
of O
index O
allows O
faster O
retrieval O
, O
however O
it O
requires O
a O
training O
stage O
to O
assign O
a O
centroid O
to O
each O
cluster O
. O
We O
refer O
readers O
to O
FAISS O
documentation O
for O
more O
detail O
explanations.9 O
For O
each O
news O
source O
, O
we O
randomly O
sample O
100 O
sentences O
from O
its O
monthly O
articles O
and O
use O
them O
as O
seed O
sentences O
to O
train O
the O
clusters O
. O
We O
then O
set O
the O
number O
of O
clusters O
xtoN O
100(rounded O
to O
the O
nearest O
hundred O
) O
, O
where O
Nis O
the O
number O
of O
seed O
sentences O
. O
Table O
9 O
lists O
the O
number O
of O
seed O
sentences O
and O
clusters O
used O
for O
each O
news O
source O
index O
. O
Source O
# O
seed O
sentences O
x O
afpeng O
111,147 O
1,100 O
apw O
eng O
146,119 O
1,400 O
cnaeng O
125,508 O
1,200 O
ltweng O
90,195 O
900 O
nyteng O
136,827 O
1,300 O
wpb O
eng O
9,144 O
100 O
xineng O
157,760 O
1,500 O
Table O
9 O
: O
Number O
of O
seed O
sentences O
and O
number O
of O
clusters O
for O
each O
news O
source O
index O
. O
During O
retrieval O
, O
for O
each O
query O
, O
we O
retrieve O
top O
1000 O
sentences O
from O
each O
index O
and O
perform O
reranking O
on O
the O
combined O
list O
, O
i.e. O
, O
7,000 O
sentence O
pairs O
, O
as O
described O
in O
Section O
2.2 O
. O
Wikipedia O
We O
build O
one O
index O
for O
the O
whole O
Wikipedia O
corpus O
. O
For O
seed O
sentences O
, O
we O
use O
sentences O
taken O
from O
the O
Ô¨Årst O
paragraph O
of O
each O
article O
as O
it O
usually O
contains O
the O
summary O
of O
the O
article O
. O
We O
set O
the O
number O
of O
clusters O
xto O
15,000 O
. O
9https://github.com/facebookresearch/ O
faiss684A.2 O
Writing O
HIT O
Instructions O
‚óè O
‚óè O
‚óè O
Prompt O
: O
  O
‚Äú O
Security O
and O
reliability O
are O
two O
important O
aspects O
of O
this O
service O
because O
of O
the O
sensitivity O
and O
urgency O
of O
the O
data O
sent O
over O
. O
‚Äù O
  O
Definitely O
correct O
  O
Example O
: O
For O
the O
prompt O
‚Äú O
The O
cottages O
near O
the O
shoreline O
, O
styled O
like O
plantation O
homes O
with O
large O
covered O
porches O
, O
are O
luxurious O
within O
; O
some O
come O
with O
private O
hot O
tubs O
. O
‚Äù O
, O
you O
  O
could O
write O
‚Äú O
The O
shoreline O
has O
plantation O
style O
homes O
near O
it O
, O
which O
are O
luxurious O
and O
often O
have O
covered O
porches O
or O
hot O
tubs O
. O
‚Äù O
  O
Maybe O
correct O
  O
Example O
: O
For O
the O
prompt O
‚Äú O
Government O
Executive O
magazine O
annually O
presents O
Government O
Technology O
Leadership O
Awards O
to O
recognize O
federal O
agencies O
and O
state O
governments O
  O
for O
their O
excellent O
performance O
with O
information O
technology O
programs O
. O
‚Äù O
, O
you O
could O
write O
‚Äú O
In O
addition O
to O
their O
annual O
Government O
Technology O
Leadership O
Award O
, O
Government O
  O
Executive O
magazine O
also O
presents O
a O
cash O
prize O
for O
best O
dressed O
agent O
from O
a O
federal O
agency O
. O
‚Äù O
  O
Definitely O
incorrect O
  O
Example O
: O
For O
the O
prompt O
‚Äú O
Yes O
, O
he O
‚Äôs O
still O
under O
arrest O
, O
which O
is O
why O
USAT O
‚Äôs O
front O
- O
page O
refer O
headline O
British O
Court O
Frees O
Chile O
‚Äôs O
Pinochet O
is O
a O
bit O
off O
. O
‚Äù O
, O
you O
could O
write O
‚Äú O
The O
  O
headline O
` O
British O
Court O
Frees O
Chile O
‚Äôs O
Pinochet O
` O
is O
correct O
, O
since O
the O
man O
is O
freely O
roaming O
the O
streets O
. O
‚Äù O
  O
Problems O
( O
optional O
) O
If O
something O
is O
wrong O
with O
the O
prompt O
that O
makes O
it O
difficult O
to O
understand O
, O
let O
us O
know O
here O
. O
  O
Figure O
2 O
: O
Writing O
HIT O
instructions.685A.3 O
Data O
Labeling O
and O
Validation O
HIT O
Instructions O
‚óè O
‚óè O
‚óè O
‚óè O
Figure O
3 O
: O
Data O
Labeling O
and O
Validation O
HIT O
instructions O
. O
We O
collect O
one O
annotation O
per O
example O
for O
data O
labeling O
and O
Ô¨Åve O
annotations O
per O
example O
for O
validation.686Proceedings O
of O
the O
1st O
Conference O
of O
the O
Asia O
- O
PaciÔ¨Åc O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
10th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
687‚Äì695 O
December O
4 O
- O
7 O
, O
2020 O
. O
¬© O
2020 O
Association O
for O
Computational O
Linguistics O
MaP O
: O
A O
Matrix O
- O
based O
Prediction O
Approach O
to O
Improve O
Span O
Extraction O
in O
Machine O
Reading O
Comprehension O
Huaishao O
Luo1‚àó O
, O
Yu O
Shi2 O
, O
Ming O
Gong3 O
, O
Linjun O
Shou3 O
, O
Tianrui O
Li1 O
1School O
of O
Information O
Science O
and O
Technology O
, O
Southwest O
Jiaotong O
University O
2Microsoft O
Cognitive O
Services O
Research O
Group O
3Microsoft O
STCA O
NLP O
Group O
huaishaoluo@gmail.com O
, O
trli@swjtu.edu.cn O
{ O
yushi O
, O
migon O
, O
lisho O
} O
@microsoft.com O
Abstract O
Span O
extraction O
is O
an O
essential O
problem O
in O
machine O
reading O
comprehension O
. O
Most O
of O
the O
existing O
algorithms O
predict O
the O
start O
and O
end O
positions O
of O
an O
answer O
span O
in O
the O
given O
corresponding O
context O
by O
generating O
two O
probability O
vectors O
. O
In O
this O
paper O
, O
we O
propose O
a O
novel O
approach O
that O
extends O
the O
probability O
vector O
to O
a O
probability O
matrix O
. O
Such O
a O
matrix O
can O
cover O
more O
start O
- O
end O
position O
pairs O
. O
Precisely O
, O
to O
each O
possible O
start O
index O
, O
the O
method O
always O
generates O
an O
end O
probability O
vector O
. O
Besides O
, O
we O
propose O
a O
sampling O
- O
based O
training O
strategy O
to O
address O
the O
computational O
cost O
and O
memory O
issue O
in O
the O
matrix O
training O
phase O
. O
We O
evaluate O
our O
method O
on O
SQuAD O
1.1 O
and O
three O
other O
question O
answering O
benchmarks O
. O
Leveraging O
the O
most O
competitive O
models O
BERT O
and O
BiDAF O
as O
the O
backbone O
, O
our O
proposed O
approach O
can O
get O
consistent O
improvements O
in O
all O
datasets O
, O
demonstrating O
the O
effectiveness O
of O
the O
proposed O
method O
. O
1 O
Introduction O
Machine O
reading O
comprehension O
( O
MRC O
) O
, O
which O
requires O
the O
machine O
to O
answer O
comprehension O
questions O
based O
on O
the O
given O
passage O
of O
text O
, O
has O
been O
studied O
extensively O
in O
the O
past O
decades O
( O
Liu O
et O
al O
. O
, O
2019 O
) O
. O
Due O
to O
the O
increase O
of O
various O
large O
- O
scale O
datasets O
( O
e.g. O
, O
SQuAD O
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
and O
MS O
MARCO O
( O
Nguyen O
et O
al O
. O
, O
2016 O
) O
) O
, O
and O
the O
enhancement O
of O
pre O
- O
trained O
models O
( O
e.g. O
, O
ELMo O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
, O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
and O
XLNet O
( O
Yang O
et O
al O
. O
, O
2019 O
) O
) O
, O
remarkable O
advancements O
have O
been O
made O
recently O
in O
this O
area O
. O
Among O
various O
MRC O
tasks O
, O
span O
extraction O
is O
one O
of O
the O
essential O
tasks O
. O
Given O
the O
context O
and O
question O
, O
the O
span O
extraction O
task O
is O
to O
extract O
a O
span O
of O
the O
most O
plausible O
text O
from O
the O
corresponding O
context O
as O
a O
‚àóThis O
work O
was O
done O
during O
the O
Ô¨Årst O
author O
‚Äôs O
internship O
at O
Microsoft O
Passage O
: O
,Begun O
as O
a O
one O
- O
page O
journal O
in O
September O
1876 O
, O
the O
Scholastic O
magazine O
is O
issued O
twice O
monthly O
and O
... O
Question O
: O
When O
did O
the O
Scholastic O
Magazine O
of O
Notre O
dame O
begin O
publishing?Answer O
: O
September O
1876 O
condi O
ÔøΩ O
onal O
condi O
ÔøΩ O
onal O
p(end|start)p(end|start)p(start|end)p(start|end O
) O
Vector O
- O
BasedMatrix O
- O
BasedFigure O
1 O
: O
An O
illustration O
of O
a O
machine O
reading O
comprehension O
framework O
. O
Most O
of O
previous O
works O
are O
vectorbased O
approaches O
shown O
as O
the O
left O
part O
. O
Our O
matrixbased O
conditional O
approach O
is O
shown O
in O
the O
right O
part O
. O
In O
our O
setting O
, O
every O
start O
( O
or O
end O
) O
position O
has O
an O
end O
( O
or O
start O
) O
probability O
vector O
, O
which O
leads O
that O
the O
output O
probabilities O
is O
a O
matrix O
( O
best O
seen O
in O
color O
) O
. O
candidate O
answer O
. O
Although O
there O
exist O
unanswerable O
cases O
beyond O
the O
span O
extraction O
, O
the O
spanbased O
task O
is O
still O
fundamental O
and O
signiÔ¨Åcant O
in O
the O
MRC O
Ô¨Åeld O
. O
Previous O
methods O
used O
to O
predict O
the O
start O
and O
end O
position O
of O
an O
answer O
span O
can O
be O
divided O
into O
two O
categories O
. O
The O
Ô¨Årst O
one O
regards O
the O
generation O
of O
begin O
position O
and O
end O
position O
independently O
. O
We O
refer O
to O
this O
category O
as O
independent O
approach O
. O
It O
can O
be O
written O
as O
p‚àó=p(‚àó|H‚àó O
) O
, O
where‚àó‚àà O
{ O
s O
, O
e O
} O
, O
thesandedenote O
start O
and O
end O
, O
respectively O
. O
H‚àóis O
the O
hidden O
representation O
, O
in O
which O
HsandHeusually O
have O
shared O
features O
. O
The O
other O
one O
constructs O
a O
dependent O
route O
from O
the O
start O
position O
when O
predicting O
the O
end O
position O
. O
We O
refer O
to O
this O
category O
asconditional O
approach O
. O
It O
can O
be O
formalized O
as O
ps O
= O
p(s|Hs),pe O
= O
p(e|s O
, O
He O
) O
. O
This O
category O
usu-687ally O
reuses O
the O
predicted O
position O
information O
( O
e.g. O
, O
s O
) O
to O
assist O
in O
the O
subsequent O
prediction O
. O
The O
difference O
between O
these O
two O
approaches O
is O
that O
the O
conditional O
approach O
considers O
the O
relationship O
between O
start O
and O
end O
positions O
, O
but O
the O
independent O
approach O
does O
not O
. O
In O
the O
literature O
, O
AMANDA O
( O
Kundu O
and O
Ng O
, O
2018b O
) O
, O
QANet O
( O
Yu O
et O
al O
. O
, O
2018 O
) O
, O
and O
SEBert O
( O
Keskar O
et O
al O
. O
, O
2019 O
) O
can O
be O
regarded O
as O
the O
independent O
approach O
, O
where O
the O
probabilities O
of O
the O
start O
and O
end O
positions O
are O
calculated O
separately O
with O
different O
representations O
. O
DCN O
( O
Xiong O
et O
al O
. O
, O
2017 O
) O
, O
R O
- O
NET O
( O
Wang O
et O
al O
. O
, O
2017 O
) O
, O
BiDAF1 O
( O
Seo O
et O
al O
. O
, O
2017 O
) O
, O
Match O
- O
LSTM O
( O
Wang O
and O
Jiang O
, O
2017 O
) O
, O
S O
- O
Net O
( O
Tan O
et O
al O
. O
, O
2018 O
) O
, O
SDNet O
( O
Zhu O
et O
al O
. O
, O
2018 O
) O
, O
and O
HAS O
- O
QA O
( O
Pang O
et O
al O
. O
, O
2019 O
) O
belong O
to O
the O
conditional O
approach O
. O
The O
probabilities O
are O
generated O
in O
sequence O
. O
The O
conditional O
approach O
empirically O
has O
an O
advantage O
over O
the O
independent O
approach O
. O
However O
, O
the O
output O
distributions O
of O
the O
previous O
conditional O
approaches O
are O
two O
probability O
vectors O
. O
It O
ignores O
some O
more O
possible O
start O
- O
end O
pairs O
. O
As O
an O
extension O
, O
every O
possible O
start O
( O
or O
end O
) O
position O
should O
have O
an O
end O
( O
or O
start O
) O
probability O
vector O
. O
Thus O
, O
the O
output O
conditional O
probabilities O
is O
a O
matrix O
. O
We O
propose O
a O
Matrix O
- O
based O
Prediction O
approach O
( O
MaP O
) O
based O
on O
the O
above O
consideration O
in O
this O
paper O
. O
As O
Figure O
1 O
shown O
, O
the O
key O
point O
is O
to O
consider O
as O
many O
probabilities O
as O
possible O
in O
training O
and O
inference O
phases O
. O
SpeciÔ¨Åcally O
, O
we O
calculate O
a O
conditional O
probability O
matrix O
instead O
of O
a O
probability O
vector O
to O
expand O
the O
choices O
of O
start O
- O
end O
pairs O
. O
Because O
of O
more O
values O
contained O
in O
a O
matrix O
than O
a O
vector O
, O
there O
is O
a O
big O
challenge O
in O
the O
training O
phase O
of O
the O
MaP. O
That O
is O
the O
high O
computational O
cost O
and O
memory O
issues O
if O
the O
input O
sequence O
is O
long O
. O
As O
an O
instance O
, O
the O
matrix O
contains O
262 O
, O
144probability O
values O
if O
the O
sequence O
length O
is O
512 O
. O
Therefore O
, O
we O
propose O
a O
sampling O
- O
based O
training O
strategy O
to O
speed O
up O
the O
training O
and O
reduce O
the O
memory O
cost O
. O
The O
main O
contributions O
of O
our O
work O
are O
fourfold O
. O
‚Ä¢A O
novel O
conditional O
approach O
is O
proposed O
to O
address O
the O
limitation O
of O
the O
probability O
vector O
generated O
by O
the O
vector O
- O
based O
conditional O
approach O
. O
It O
increases O
the O
likelihood O
of O
hitting O
the O
ground O
- O
truth O
start O
and O
end O
positions O
. O
‚Ä¢A O
sampling O
- O
based O
training O
strategy O
is O
pro1We O
classify O
BiDAF O
as O
a O
conditional O
approach O
by O
its O
ofÔ¨Åcial O
implementation O
: O
https://github.com/allenai/ O
bi O
- O
att O
- O
flowposed O
to O
overcome O
the O
computation O
and O
memory O
issues O
in O
the O
training O
phase O
of O
the O
matrixbased O
conditional O
approach O
. O
‚Ä¢An O
ensemble O
approach O
on O
both O
start O
- O
to O
- O
end O
and O
end O
- O
to O
- O
start O
directions O
of O
conditional O
probability O
is O
investigated O
to O
improve O
the O
accuracy O
of O
the O
answer O
span O
. O
‚Ä¢We O
evaluate O
our O
strategy O
on O
SQuAD O
1.1 O
and O
three O
other O
question O
answering O
benchmarks O
. O
The O
implementation O
of O
the O
matrix O
- O
based O
conditional O
approach O
is O
designed O
based O
on O
the O
BERT O
and O
BiDAF O
, O
which O
are O
the O
most O
competitive O
models O
, O
to O
test O
the O
generalization O
of O
our O
strategy O
. O
The O
consistent O
improvements O
in O
all O
datasets O
demonstrate O
the O
effectiveness O
of O
the O
strategy O
. O
2 O
Methodology O
In O
this O
section O
, O
we O
Ô¨Årst O
give O
the O
problem O
deÔ¨Ånition O
. O
Then O
we O
introduce O
a O
typical O
vector O
- O
based O
conditional O
approach O
. O
Next O
, O
we O
mainly O
introduce O
our O
matrix O
- O
based O
conditional O
approach O
and O
samplingbased O
training O
strategy O
. O
Finally O
, O
an O
ensemble O
approach O
on O
both O
start O
- O
to O
- O
end O
and O
end O
- O
to O
- O
start O
directions O
of O
conditional O
probability O
is O
discussed O
. O
2.1 O
Problem O
Statement O
Given O
the O
passage O
P={t1,t2 O
, O
¬∑ O
¬∑ O
¬∑ O
, O
tn}and O
the O
questionQ={q1,q2 O
, O
¬∑ O
¬∑ O
¬∑ O
, O
qm O
} O
, O
the O
span O
extraction O
task O
needs O
to O
extract O
the O
continuous O
subsequence O
A= O
{ O
ts O
, O
¬∑ O
¬∑ O
¬∑ O
, O
te}(1‚â§s‚â§e‚â§n O
) O
from O
the O
passage O
as O
the O
right O
answer O
to O
the O
question O
, O
where O
nandmare O
the O
length O
of O
the O
passage O
and O
question O
respectively O
, O
sandeare O
the O
start O
and O
end O
position O
in O
the O
passage O
. O
Usually O
, O
the O
objective O
to O
predict O
a= O
( O
s O
, O
e)is O
maximizing O
the O
conditional O
probability O
p(a|P O
, O
Q O
) O
. O
2.2 O
A O
Typical O
Vector O
- O
based O
Approach O
We O
summarize O
a O
typical O
implementation O
of O
the O
vector O
- O
based O
conditional O
approach O
shown O
in O
Figure O
2 O
. O
Previous O
mentioned O
R O
- O
NET O
, O
BiDAF O
, O
MatchLSTM O
, O
S O
- O
Net O
, O
and O
SDNet O
can O
be O
regarded O
as O
such O
implementation O
. O
Its O
backbone O
is O
the O
Pointer O
Network O
proposed O
by O
Vinyals O
et O
al O
. O
( O
2015 O
) O
. O
The O
interactive O
representation O
H‚ààRn√ódbetween O
the O
given O
question O
Qand O
passage O
Pis O
calculated O
as O
follows O
, O
H O
= O
M(Q O
, O
P O
) O
, O
( O
1 O
) O
whereMis O
a O
neural O
network O
, O
e.g. O
, O
Match O
- O
LSTM O
, O
QANet O
, O
BERT O
, O
and O
XLNet O
, O
dis O
the O
dimension O
size688Model O
Ques O
ÔøΩ O
on O
PassageRNNso O
ÔøΩ O
max O
so O
ÔøΩ O
max(|)psH O
( O
|,)pesH O
specsheh O
Init O
Œó O
QHFigure O
2 O
: O
A O
typical O
implementation O
of O
the O
vector O
- O
based O
conditional O
approach O
. O
of O
the O
representation O
. O
After O
generating O
the O
interactive O
representation O
, O
the O
next O
step O
is O
to O
predict O
the O
answer O
span O
. O
The O
main O
architecture O
of O
the O
span O
prediction O
is O
an O
RNN O
. O
As O
an O
instance O
, O
LSTM O
is O
used O
in O
( O
Wang O
and O
Jiang O
, O
2017 O
) O
, O
and O
GRU O
is O
adopted O
in O
( O
Tan O
et O
al O
. O
, O
2018 O
; O
Zhu O
et O
al O
. O
, O
2018 O
) O
. O
Take O
the O
hidden O
representationhe‚ààRkof O
end O
position O
as O
an O
example O
, O
which O
is O
calculated O
as O
follows O
, O
he O
= O
RNN(hs O
, O
ce O
) O
, O
( O
2 O
) O
ce O
= O
H O
/ O
latticetopps O
, O
( O
3 O
) O
where O
ps O
= O
p(s|H)is O
the O
start O
probability O
and O
ps‚ààRn O
, O
kis O
the O
dimension O
size O
of O
he O
. O
Then O
pe O
= O
p(e|s O
, O
H)(pe‚ààRn)can O
be O
calculated O
using O
heas O
follows O
, O
p(e|s O
, O
H O
) O
= O
softmax O
/ O
parenleftBig O
v O
/ O
latticetoptanh O
/ O
parenleftbig O
VH O
/ O
latticetop+/llbracketWehe O
/ O
rrbracketn O
/ O
parenrightbig O
/ O
parenrightBig O
( O
4 O
) O
where O
/ O
llbracket¬∑/rrbracketnis O
an O
operation O
that O
generates O
a O
matrix O
by O
repeating O
the O
vector O
on O
the O
left O
ntimes O
, O
v‚ààRl O
, O
V‚ààRl√ód O
, O
and O
We‚ààRl√ókare O
parameters O
to O
be O
learned O
. O
The O
calculation O
of O
p(s|H)is O
similar O
to O
p(e|s O
, O
H O
) O
. O
The O
key O
is O
to O
obtain O
the O
hidden O
state O
hs O
. O
A O
choice O
is O
to O
use O
an O
attention O
approach O
to O
condense O
the O
question O
representation O
into O
a O
vector O
. O
The O
process O
is O
as O
follows O
, O
pinit O
= O
softmax O
/ O
parenleftBig O
v O
/ O
latticetop O
Qtanh O
/ O
parenleftbig O
VQH O
/ O
latticetop O
Q O
/ O
parenrightbig O
/ O
parenrightBig O
, O
( O
5 O
) O
hs O
= O
H O
/ O
latticetop O
Qpinit O
, O
( O
6 O
) O
where O
HQ‚ààRm√ódis O
the O
representation O
corresponding O
to O
Q O
, O
vQ‚ààRl O
, O
and O
VQ‚ààRl√ódare O
parameters O
. O
Œó O
i O
Œó O
Pn[]iHÔÇ©ÔÇ¨ÔÇ™ÔÇ≠ÔÇ´ÔÇÆFigure O
3 O
: O
Matrix O
- O
based O
conditional O
approach O
. O
There O
is O
a O
vast O
number O
of O
works O
on O
MRC O
. O
However O
, O
most O
of O
these O
works O
focus O
on O
the O
design O
of O
M O
and O
generate O
the O
answer O
span O
based O
on O
the O
vectorbased O
conditional O
approach O
. O
In O
this O
paper O
, O
we O
expand O
the O
vector O
to O
a O
probability O
matrix O
. O
Thus O
, O
many O
more O
possibilities O
can O
be O
covered O
. O
It O
is O
also O
a O
natural O
manner O
because O
that O
every O
start O
( O
or O
end O
) O
position O
should O
have O
an O
end O
( O
or O
start O
) O
probability O
vector O
. O
2.3 O
Matrix O
- O
based O
Conditional O
approach O
As O
the O
previous O
description O
, O
the O
implementation O
of O
the O
vector O
- O
based O
conditional O
approach O
has O
a O
uniÔ¨Åed O
and O
important O
implementation O
step O
: O
create O
a O
‚Äò O
condition O
‚Äô O
. O
Take O
the O
forward O
direction O
( O
‚Äò O
condition O
‚Äô O
constructed O
from O
the O
start O
position O
to O
end O
position O
) O
of O
the O
vector O
- O
based O
conditional O
approach O
as O
an O
example O
, O
the O
‚Äò O
condition O
‚Äô O
is O
the O
probability O
vector O
ps O
. O
The O
end O
probability O
vector O
pecan O
not O
be O
calculated O
until O
generating O
ps O
. O
However O
, O
there O
is O
only O
one O
probability O
vector O
pewhatever O
the O
start O
position O
is O
. O
In O
this O
paper O
, O
we O
keep O
the O
‚Äò O
condition O
‚Äô O
step O
but O
propose O
calculating O
an O
individual O
pefor O
each O
start O
position O
. O
SpeciÔ¨Åcally O
, O
the O
probability O
matrix O
Pe‚ààRn√ónis O
calculated O
as O
follows O
, O
P(i O
) O
e O
= O
softmax O
/ O
parenleftBigg O
v O
/ O
latticetoptanh O
/ O
parenleftbigg O
V O
/ O
bracketleftbigg O
H O
/ O
latticetop;/Largellbracket O
/ O
parenleftbig O
H[i]/parenrightbig O
/ O
latticetop O
/ O
Largerrbracketn O
/ O
bracketrightbigg O
/ O
parenrightbigg O
/ O
parenrightBigg O
( O
7 O
) O
where O
P(i O
) O
edenotes O
the O
i O
- O
th O
row O
of O
Pe,[;]is O
a O
concatenate O
operation O
, O
/llbracket¬∑/rrbracketnis O
an O
operation O
that O
generates O
a O
matrix O
by O
repeating O
the O
vector O
on O
the O
left O
n O
times O
, O
[ O
i]means O
to O
choose O
the O
i O
- O
th O
row O
from O
the O
matrix O
H O
, O
v‚ààRlandV‚ààRl√ó2dare O
parameters O
. O
Figure O
3 O
illustrates O
the O
calculation O
process O
of O
Eq O
. O
( O
7 O
) O
. O
Although O
the O
calculation O
is O
brief O
and O
can O
cover O
more O
probabilities O
than O
the O
vector O
- O
based O
approach O
, O
there O
is O
a O
big O
question O
on O
computation O
cost O
and O
memory O
occupation O
. O
The O
main O
computation O
cost O
comes O
from O
the O
matrix O
multiplication O
between O
V O
and O
/ O
bracketleftbigg O
H O
/ O
latticetop;/Largellbracket O
/ O
parenleftbig O
H[i]/parenrightbig O
/ O
latticetop O
/ O
Largerrbracketn O
/ O
bracketrightbigg O
in O
Eq O
. O
( O
7 O
) O
, O
totally O
ntimes689such O
computation O
for O
Pe O
. O
The O
number O
of O
probabilities O
is O
also O
ntimes O
bigger O
than O
the O
vector O
- O
based O
conditional O
approach O
. O
It O
also O
causes O
the O
issue O
of O
out O
of O
memory O
( O
OOM O
) O
, O
especially O
with O
a O
big O
n O
, O
due O
to O
intermediate O
gradient O
values O
needing O
cache O
in O
the O
training O
phase O
. O
We O
propose O
a O
sampling O
- O
based O
training O
strategy O
to O
solve O
the O
above O
issues O
. O
2.4 O
Sampling O
- O
based O
Training O
Strategy O
In O
order O
to O
train O
the O
probability O
matrix O
effectively O
, O
we O
propose O
a O
sampling O
- O
based O
strategy O
in O
the O
training O
phase O
. O
Given O
the O
hyper O
- O
parameter O
k O
, O
we O
Ô¨Årst O
choose O
the O
indexes O
ÀÜIof O
top O
k‚àí1possibilities O
from O
p(- O
ÀÜs O
) O
s O
, O
ÀÜI O
= O
top O
/ O
parenleftBig O
p(- O
ÀÜs O
) O
s O
, O
k‚àí1 O
/ O
parenrightBig O
, O
( O
8) O
where O
top(p O
, O
v)is O
an O
operation O
used O
to O
get O
the O
indexes O
of O
top O
vvalues O
in O
p O
, O
p(-w)contains O
all O
but O
w O
- O
th O
value O
of O
p O
, O
and O
ÀÜsis O
the O
truth O
start O
position O
used O
as O
the O
supervised O
information O
in O
the O
training O
phase O
. O
Then O
, O
the O
ÀÜ O
smust O
merge O
to O
ÀÜI O
, O
I=ÀÜI+{ÀÜs O
} O
, O
( O
9 O
) O
whereIcontains O
kindexes O
. O
Eq O
. O
( O
8) O
and O
Eq O
. O
( O
9 O
) O
promise O
that O
the O
sampled O
start O
probabilities O
must O
contain O
and O
only O
contain O
the O
target O
probability O
which O
we O
need O
to O
train O
in O
each O
iteration O
. O
The O
target O
probability O
is O
the O
ÀÜs O
- O
th O
value O
in O
ps O
, O
and O
the O
bigger O
, O
the O
better O
. O
After O
sampling O
the O
start O
probability O
vector O
, O
the O
computation O
cost O
of O
Pedecrease O
. O
For O
each O
i‚ààI O
, O
executing O
Eq O
. O
( O
7 O
) O
repeatedly O
can O
generate O
a O
samplingbased O
end O
probability O
matrix O
. O
It O
is O
noted O
that O
this O
sampling O
- O
based O
matrix O
is O
a O
part O
of O
the O
original O
Pe O
. O
We O
refer O
to O
it O
as O
ÀúPe O
, O
and O
ÀúPe‚ààRk√ón O
. O
It O
is O
still O
a O
big O
issue O
of O
computation O
cost O
and O
memory O
occupation O
for O
ÀúPewith O
a O
long O
sequence O
. O
So O
, O
we O
carry O
out O
similar O
operations O
in O
Eq O
. O
( O
8) O
and O
Eq O
. O
( O
9 O
) O
for O
each O
row O
of O
ÀúPeusing O
ÀÜeinstead O
of O
ÀÜs O
, O
where O
ÀÜeis O
the O
end O
truth O
position O
. O
Finally O
, O
the O
sampling O
- O
based O
matrix O
ÀÜPe‚ààRk√ókis O
generated O
. O
It O
is O
small O
enough O
to O
train O
compared O
with O
Pe O
. O
Figure O
4 O
shows O
the O
sampling O
results O
colored O
with O
a O
yellow O
background O
on O
the O
left O
and O
corresponding O
ground O
truth O
matrix O
on O
the O
right O
. O
2.5 O
Training O
In O
the O
training O
phase O
, O
the O
objective O
function O
is O
to O
minimize O
the O
cross O
- O
entropy O
error O
averaged O
over O
start O
and O
end O
positions O
, O
L=1 O
2(Ls+Le O
) O
, O
( O
10 O
) O
          O
Ground O
Truthij O
e O
ePsÔÄ§ÔÄ§Figure O
4 O
: O
A O
sampling O
of O
probability O
matrix O
. O
Left O
: O
the O
calculated O
probability O
matrix O
with O
sampled O
top O
four O
positions O
( O
in O
both O
row O
and O
column O
directions O
colored O
with O
yellow O
background O
) O
. O
Right O
: O
the O
ground O
truth O
matrix O
, O
where O
position O
( O
ÀÜs,ÀÜe)with O
the O
red O
background O
has O
probability O
1 O
. O
Ls=‚àí1 O
NN O
‚àë O
i=1 O
/ O
parenleftBig O
I(ÀÜs)/parenleftbig O
log(ps)/parenrightbig O
/ O
latticetop O
/ O
parenrightBig O
, O
( O
11 O
) O
Le=‚àí1 O
NN O
‚àë O
i=1 O
/ O
parenleftbigg O
T O
/ O
parenleftbig O
I(ÀÜs,ÀÜe)/parenrightbig O
/ O
parenleftBig O
log O
/ O
parenleftbig O
T(ÀÜPe)/parenrightbig O
/ O
parenrightBig O
/ O
latticetop O
/ O
parenrightbigg O
, O
( O
12 O
) O
where O
Nis O
the O
number O
of O
data O
, O
I(ÀÜs)means O
the O
onehot O
vector O
of O
ÀÜs O
, O
I(ÀÜs,ÀÜe)means O
a O
zero O
matrix O
with O
a O
value O
of O
1 O
in O
row O
ÀÜsand O
column O
ÀÜe O
, O
andT()is O
a O
row O
wise O
Ô¨Çatten O
operation O
. O
The O
Ô¨Çatten O
operation O
makes O
the O
loss O
function O
on O
matrix O
- O
based O
distribution O
similar O
to O
that O
on O
vector O
- O
based O
distribution O
. O
As O
the O
introduction O
of O
the O
sampling O
- O
based O
training O
strategy O
, O
there O
are O
limited O
end O
probabilities O
that O
could O
be O
trained O
in O
each O
iteration O
. O
The O
extreme O
situation O
is O
kequals O
to O
n O
, O
which O
makes O
all O
probability O
matrix O
calculate O
each O
time O
. O
As O
our O
previous O
argumentation O
, O
it O
is O
almost O
impossible O
for O
time O
and O
memory O
limitations O
. O
However O
, O
there O
is O
a O
question O
of O
what O
makes O
sampling O
strategy O
works O
. O
The O
following O
content O
gives O
some O
explanation O
based O
on O
gradient O
backpropagation O
. O
The O
gradient O
of O
the O
cross O
- O
entropy O
L‚àóto O
the O
predicted O
logits O
z‚àóis O
, O
‚àÇL‚àó O
‚àÇz‚àó=/braceleftBigg O
p(i O
) O
‚àó‚àí1,ifiis O
the O
ground O
- O
truth O
; O
p(j O
) O
‚àó O
, O
others(13 O
) O
where O
p‚àó=softmax O
( O
z‚àó)is O
probabilities O
in O
which O
values O
are O
between O
0 O
and O
1 O
( O
exclusion O
) O
. O
Thus O
p(i O
) O
‚àó‚àí1is O
negative O
, O
and O
p(j O
) O
‚àóis O
positive O
in O
most O
cases O
. O
As O
the O
parameters O
Œ∏update O
usually O
follows O
Œ∏t O
= O
Œ∏t‚àí1‚àíŒ∑¬∑‚àáŒ∏L(Œ∏)and O
learning O
rate O
Œ∑is O
a O
positive O
value O
, O
the O
probability O
in O
ground O
- O
truth O
position O
should O
go O
up O
, O
and O
the O
probabilities O
in O
other O
sampled O
positions O
should O
go O
down.690Itera O
ÔøΩ O
on O
# O
1Ground O
- O
truth O
Itera O
ÔøΩ O
on O
# O
2 O
Itera O
ÔøΩ O
on O
# O
3Figure O
5 O
: O
Sampling O
- O
based O
probabilities O
training O
( O
k=5 O
) O
. O
Block O
with O
red O
color O
is O
the O
ground O
- O
truth O
, O
blocks O
with O
blue O
color O
are O
the O
sampled O
probabilities O
. O
Probabilities O
with O
a O
gray O
background O
will O
not O
change O
their O
values O
in O
each O
iteration O
. O
Figure O
5 O
illustrates O
the O
sampling O
- O
based O
training O
process O
, O
where O
the O
parameter O
kis O
set O
to O
5 O
. O
It O
means O
that O
there O
are O
extra O
top-4 O
probabilities O
( O
blue O
background O
) O
except O
ground O
- O
truth O
( O
red O
background O
) O
will O
be O
chosen O
to O
calculate O
. O
With O
the O
iteration O
going O
from O
# O
1 O
to O
# O
3 O
, O
the O
probability O
in O
ground O
- O
truth O
position O
goes O
up O
, O
and O
that O
in O
sampled O
top-4 O
positions O
goes O
down O
. O
Such O
a O
sampling O
- O
based O
training O
approach O
has O
the O
same O
goal O
with O
the O
training O
on O
the O
whole O
probabilities O
, O
thus O
should O
have O
proximity O
results O
. O
2.6 O
Ensemble O
for O
Inference O
The O
vector O
- O
based O
conditional O
approach O
usually O
searches O
the O
span O
( O
s O
, O
e)via O
the O
computation O
of O
p(i O
) O
s√óp(j O
) O
eunder O
the O
condition O
of O
i‚â§j O
, O
and O
choices O
the(i‚àó,j‚àó)with O
the O
highest O
p(i‚àó O
) O
s√óp(j‚àó O
) O
eas O
the O
output O
in O
the O
inference O
phase O
. O
The O
matrix O
- O
based O
conditional O
approach O
follows O
the O
same O
idea O
, O
but O
the O
calculation O
of O
the O
probability O
is O
p(i O
) O
s√óP(i O
, O
j O
) O
einstead O
ofp(i O
) O
s√óp(j O
) O
e. O
The O
p(i O
) O
sis O
the O
i O
- O
th O
probability O
in O
ps O
, O
andP(i O
, O
j O
) O
eis O
the O
probability O
in O
row O
i O
, O
column O
jof O
Pe O
. O
The O
above O
inference O
strategy O
only O
involves O
one O
direction O
, O
e.g. O
, O
start O
- O
to O
- O
end O
direction O
( O
generate O
start O
position O
Ô¨Årstly O
, O
then O
generate O
end O
position O
) O
, O
which O
is O
the O
most O
cases O
in O
previous O
works O
. O
An O
ensemble O
of O
both O
start O
- O
to O
- O
end O
and O
end O
- O
to O
- O
start O
directions O
is O
a O
good O
choice O
to O
improve O
the O
performance O
. O
The O
difference O
in O
end O
- O
to O
- O
start O
direction O
is O
that O
Eqs O
. O
( O
712 O
) O
should O
be O
repeated O
in O
the O
opposite O
direction O
. O
In O
other O
words O
, O
the O
start O
is O
replaced O
by O
e O
, O
and O
the O
end O
is O
replaced O
by O
s. O
Totally O
, O
there O
are O
two O
groups O
of O
probabilities O
, O
( O
ps O
, O
Pe)and(pe O
, O
Ps O
) O
. O
In O
this O
paper O
, O
we O
design O
a O
type O
of O
ensemble O
strategy O
, O
which O
Ô¨Årst O
chooses O
top O
kpairs O
F={(if O
, O
jf)}withAlgorithm O
1 O
MaP O
Training O
Algorithm O
Input O
: O
Npairs O
of O
passage O
Pand O
question O
Q O
, O
k O
used O
to O
choose O
top O
probabilities O
; O
Output O
: O
Learned O
MaP O
model O
1 O
: O
Initialize O
all O
learnable O
parameters O
Œò O
; O
2 O
: O
repeat O
3 O
: O
Select O
a O
batch O
of O
pairs O
from O
corpus O
; O
4 O
: O
foreach O
pair O
( O
P O
, O
Q)do O
5 O
: O
Use O
a O
neural O
network O
Mto O
generate O
the O
representation O
H O
; O
( O
Eq O
. O
1 O
) O
6 O
: O
Compute O
start O
probability O
vector O
ps O
; O
( O
Eqs O
. O
4 O
- O
6 O
) O
7 O
: O
Sample O
indexesIby O
choosing O
top O
k‚àí1 O
probabilities O
of O
ps O
; O
( O
Eqs O
. O
8,9 O
) O
8 O
: O
Compute O
end O
probability O
matrix O
Pe O
; O
( O
Eq O
. O
7 O
) O
9 O
: O
Compute O
objective O
L O
; O
( O
Eq O
. O
10 O
- O
12 O
) O
10 O
: O
end O
for O
11 O
: O
Use O
the O
backpropagation O
algorithm O
to O
update O
parameters O
Œòby O
minimizing O
the O
objective O
with O
the O
batch O
update O
mode O
12 O
: O
until O
stopping O
criteria O
is O
met O
highest O
probability O
p(if O
) O
s√óP(if O
, O
jf O
) O
e O
, O
then O
chooses O
topkpairs O
B={(jb O
, O
ib)}with O
highest O
probabilityp(jb O
) O
e√óP(jb O
, O
ib O
) O
s O
. O
It O
is O
noted O
that O
some O
pairs O
may O
have O
the O
same O
position O
, O
e.g. O
, O
( O
3f,5f)and(5b,3b O
) O
. O
If O
there O
are O
the O
same O
elements O
, O
we O
prune O
away O
them O
inB. O
Then O
, O
we O
choose O
the O
( O
i‚àó,j‚àó)with O
highest O
probability O
in O
F‚à™B. O
The O
overall O
training O
procedure O
of O
MaP O
is O
summarized O
in O
Algorithm O
1 O
. O
3 O
Experiments O
In O
this O
section O
, O
we O
conduct O
experiments O
to O
evaluate O
the O
effectiveness O
of O
the O
proposed O
MaP. O
3.1 O
Datasets O
We O
Ô¨Årst O
evaluate O
our O
strategy O
on O
SQuAD O
1.1 O
, O
which O
is O
a O
reading O
comprehension O
benchmark O
. O
The O
benchmark O
beneÔ¨Åts O
to O
our O
evaluation O
compared O
with O
its O
augmented O
version O
SQuAD O
2.0 O
due O
to O
its O
questions O
always O
have O
a O
corresponding O
answer O
in O
the O
given O
passages O
. O
We O
also O
evaluate O
our O
strategy O
on O
three O
other O
datasets O
from O
the O
MRQA O
2019 O
Shared O
Task2 O
: O
NewsQA O
( O
Trischler O
et O
al O
. O
, O
2017 O
) O
, O
HotpotQA O
( O
Yang O
et O
al O
. O
, O
2018 O
) O
, O
Natural O
Questions O
( O
Kwiatkowski O
et O
al O
. O
, O
2019 O
) O
. O
As O
the O
SQuAD O
1.1 O
dataset O
, O
the O
format O
of O
2https://github.com/mrqa/ O
MRQA O
- O
Shared O
- O
Task-2019691ModelsSQuAD O
NewsQA O
HotpotQA O
Natural O
Questions O
EM O
F1 O
EM O
F1 O
EM O
F1 O
EM O
F1 O
BERT O
- O
Base O
InD O
81.24 O
88.38 O
52.59 O
67.12 O
59.01 O
75.69 O
67.31 O
78.96 O
MaP O
F O
81.78 O
88.59 O
52.66 O
66.50 O
59.82 O
75.81 O
67.68 O
78.99 O
MaP O
E82.12 O
88.63 O
53.06 O
67.37 O
60.55 O
76.12 O
68.21 O
79.09 O
BERT O
- O
Large O
InD O
84.05 O
90.85 O
54.46 O
69.61 O
62.26 O
78.18 O
69.44 O
80.93 O
MaP O
F O
84.50 O
90.89 O
54.84 O
68.73 O
63.19 O
78.99 O
69.56 O
80.49 O
MaP O
E84.79 O
90.89 O
55.29 O
69.98 O
63.70 O
79.25 O
69.91 O
81.22 O
BiDAF O
VCP O
68.57 O
78.23 O
44.04 O
58.07 O
47.31 O
62.42 O
56.95 O
68.79 O
MaP O
F O
68.85 O
78.06 O
44.19 O
58.65 O
50.25 O
65.21 O
57.04 O
68.87 O
MaP O
E69.55 O
78.91 O
44.25 O
58.91 O
51.45 O
66.74 O
57.21 O
69.08 O
Table O
1 O
: O
The O
performance O
( O
% O
) O
of O
EM O
and O
F1 O
on O
SQuAD O
1.1 O
and O
three O
MRQA O
extractive O
question O
answering O
tasks O
. O
MaP O
Fis O
the O
matrix O
- O
based O
conditional O
approach O
calculating O
on O
start O
- O
to O
- O
end O
direction O
. O
MaP O
Emeans O
the O
ensemble O
of O
both O
directions O
of O
matrix O
- O
based O
conditional O
approach O
. O
InD O
denotes O
the O
independent O
approach O
. O
VCP O
is O
vector O
- O
based O
conditional O
approach O
. O
the O
task O
is O
extractive O
question O
answering O
. O
It O
contains O
no O
unanswerable O
or O
non O
- O
span O
answer O
questions O
. O
Besides O
, O
the O
fact O
that O
these O
datasets O
vary O
in O
both O
domain O
and O
collection O
pattern O
beneÔ¨Åts O
for O
the O
evaluation O
of O
our O
strategy O
on O
generalization O
across O
different O
data O
distributions O
. O
Table O
2 O
shows O
the O
statistics O
of O
these O
datasets O
. O
Dataset O
Training O
Development O
SQuAD O
1.1 O
86,588 O
10,507 O
NewsQA O
74,160 O
4,212 O
HotpotQA O
72,928 O
5,904 O
Natural O
Questions O
104,071 O
12,836 O
Table O
2 O
: O
The O
statistics O
of O
datasets O
. O
3.2 O
Baselines O
To O
validate O
the O
effectiveness O
and O
generalization O
of O
our O
proposed O
strategy O
on O
the O
span O
extraction O
, O
we O
implement O
it O
using O
two O
strong O
backbones O
, O
BERT O
and O
BiDAF O
. O
SpeciÔ¨Åcally O
, O
we O
borrow O
their O
main O
bodies O
except O
the O
top O
layer O
to O
implement O
the O
proposed O
strategy O
to O
Ô¨Ånish O
the O
span O
extraction O
on O
different O
datasets O
. O
Some O
more O
tests O
on O
other O
models O
, O
e.g. O
, O
XLNet O
( O
Yang O
et O
al O
. O
, O
2019 O
) O
and O
SpanBERT O
( O
Joshi O
et O
al O
. O
, O
2019 O
) O
, O
and O
datasets O
will O
be O
our O
future O
work O
. O
‚Ä¢BERT O
: O
BERT O
is O
an O
empirically O
powerful O
language O
model O
, O
which O
obtained O
state O
- O
of O
- O
the O
- O
art O
results O
on O
eleven O
natural O
language O
processing O
tasks O
in O
the O
past O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O
The O
original O
implementation O
in O
their O
paper O
on O
the O
span O
prediction O
task O
belongs O
to O
the O
independent O
approach O
. O
Both O
BERT O
- O
base O
and O
BERT O
- O
large O
withuncased O
pre O
- O
trained O
weights O
are O
used O
in O
comparison O
to O
investigating O
the O
effect O
of O
the O
ability O
of O
language O
model O
on O
span O
extraction O
with O
different O
prediction O
approaches O
. O
‚Ä¢BiDAF O
: O
BiDAF O
is O
used O
as O
a O
baseline O
of O
the O
vector O
- O
based O
conditional O
approach O
( O
Seo O
et O
al O
. O
, O
2017 O
) O
. O
The O
use O
of O
a O
multi O
- O
stage O
hierarchical O
process O
and O
a O
bidirectional O
attention O
Ô¨Çow O
mechanism O
makes O
its O
representation O
powerful O
. O
There O
are O
four O
strategies O
of O
span O
extraction O
involved O
in O
our O
comparison O
: O
InD O
denotes O
the O
independent O
approach O
; O
VCP O
is O
the O
vector O
- O
based O
conditional O
approach O
; O
MaP O
Fis O
our O
matrix O
- O
based O
conditional O
approach O
calculating O
on O
start O
- O
to O
- O
end O
direction O
; O
MaP O
Emeans O
the O
ensemble O
of O
both O
directions O
of O
matrix O
- O
based O
conditional O
approach O
. O
The O
InD O
is O
used O
to O
compare O
with O
MaP O
Fand O
MaP O
Ein O
BERT O
, O
and O
the O
VCP O
is O
used O
to O
compare O
with O
MaP O
Fand O
MaP O
Ein O
BiDAF O
. O
3.3 O
Experimental O
Settings O
We O
implement O
the O
BERT O
and O
BiDAF O
following O
the O
ofÔ¨Åcial O
settings O
for O
a O
fair O
comparison O
. O
For O
the O
BERT O
, O
we O
train O
for O
3 O
epochs O
with O
a O
learning O
rate O
of5e-5and O
a O
batch O
size O
of O
32 O
. O
The O
max O
sequence O
length O
is O
384 O
for O
SQuAD O
1.1 O
and O
512 O
for O
other O
datasets O
, O
and O
a O
sliding O
window O
of O
size O
128 O
is O
used O
for O
all O
datasets O
is O
the O
sentence O
is O
longer O
than O
the O
max O
length O
. O
For O
the O
BiDAF O
, O
we O
keep O
all O
original O
settings O
except O
a O
difference O
that O
we O
use O
ADAM O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
optimizer O
with O
a O
learning O
rate O
of O
1e-3 O
in O
the O
training O
phase O
instead O
of O
AdaDelta O
( O
Zeiler O
, O
2012 O
) O
for O
a O
stable O
performance.692Following O
the O
work O
from O
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
, O
we O
evaluate O
the O
results O
using O
Exact O
Match O
( O
EM O
) O
and O
Macro O
- O
averaged O
F1 O
score O
. O
The O
sampling O
parameter O
kis O
set O
to O
20 O
for O
our O
strategy O
. O
We O
implement O
our O
model O
in O
python O
using O
the O
pytorch O
- O
transformers O
library3for O
BERT O
and O
the O
AllenNLP O
library4for O
BiDAF O
. O
The O
reported O
results O
are O
average O
scores O
of O
5 O
runs O
with O
different O
random O
seeds O
. O
All O
computations O
are O
done O
on O
4 O
NVIDIA O
Tesla O
V100 O
GPUs O
. O
3.4 O
Main O
Results O
The O
results O
of O
our O
strategies O
as O
well O
as O
the O
baselines O
are O
shown O
in O
Table O
1 O
. O
All O
these O
values O
come O
from O
the O
evaluation O
of O
the O
development O
sets O
in O
each O
dataset O
due O
to O
the O
test O
sets O
are O
withheld O
. O
Nevertheless O
, O
our O
strategy O
achieves O
a O
consistent O
improvement O
compared O
with O
the O
independent O
approach O
and O
the O
vector O
- O
based O
conditional O
approach O
. O
The O
values O
with O
a O
bold O
type O
mean O
the O
winner O
across O
all O
strategies O
. O
As O
we O
can O
observe O
, O
the O
MaP O
Ewins O
16 O
out O
of O
16 O
in O
both O
BERT O
- O
base O
and O
BERT O
- O
large O
groups O
. O
It O
proves O
that O
the O
ensemble O
of O
both O
directions O
is O
helpful O
for O
the O
span O
extraction O
. O
In O
the O
BiDAF O
group O
, O
The O
MaP O
Eis O
also O
the O
best O
on O
all O
datasets O
compared O
with O
VCP O
. O
It O
shows O
the O
robustness O
of O
our O
matrix O
- O
based O
conditional O
approach O
in O
language O
models O
. O
The O
fact O
that O
the O
MaP O
Fwins O
12 O
out O
of O
12 O
in O
EM O
, O
and O
8 O
out O
of O
12 O
in O
F1 O
demonstrates O
that O
the O
matrix O
- O
based O
conditional O
approach O
is O
capable O
of O
predicting O
a O
clean O
answer O
span O
that O
matches O
human O
judgment O
exactly O
. O
We O
suppose O
the O
reason O
is O
that O
more O
start O
- O
end O
position O
pairs O
considered O
in O
the O
probability O
matrix O
can O
enhance O
the O
interaction O
and O
constraint O
between O
the O
start O
and O
end O
, O
thus O
, O
make O
the O
MaP O
Fperform O
more O
consistently O
in O
EM O
than O
in O
F1 O
. O
05001000150020002500 O
0.020.040.060.080.0 O
123456789 O
> O
9 O
NO O
. O
of O
Samples O
by O
Answer O
LengthEM O
& O
F1 O
( O
% O
) O
Answer O
Length O
of O
Ground O
- O
truthNO O
. O
VCP O
- O
EM O
VCP O
- O
F1 O
MaP O
- O
EM O
MaP O
- O
F1 O
Figure O
6 O
: O
EM O
and O
F1 O
of O
MaP O
Fand O
VCP O
based O
on O
BiDAF O
under O
different O
answer O
length O
. O
3https://github.com/huggingface/ O
pytorch O
- O
transformers O
4https://github.com/allenai/allennlp3.5 O
Strategy O
Analysis O
Figure O
6 O
shows O
how O
the O
performance O
changes O
with O
respect O
to O
the O
answer O
length O
, O
which O
is O
designed O
on O
HotpotQA O
. O
We O
can O
see O
that O
the O
matrix O
- O
based O
conditional O
approach O
works O
better O
than O
the O
vectorbased O
conditional O
approach O
as O
the O
span O
decrease O
in O
length O
. O
Since O
the O
short O
answers O
have O
a O
high O
rate O
in O
all O
answer O
spans O
, O
so O
the O
matrix O
- O
based O
conditional O
approach O
is O
better O
for O
the O
answer O
span O
task O
. O
In O
other O
words O
, O
this O
observation O
supports O
the O
ensemble O
of O
both O
directions O
as O
Edoes O
. O
The O
MaP O
Ecombining O
the O
MaP O
F O
‚Äôs O
advantage O
in O
short O
answers O
and O
the O
VCP O
‚Äôs O
advantage O
in O
long O
answers O
can O
get O
a O
better O
result O
than O
any O
of O
them O
. O
81.281.481.681.882.088.088.288.488.688.8102030405060708090100 O
EM O
( O
% O
) O
F1 O
( O
% O
) O
F1 O
EM O
k O
Figure O
7 O
: O
Impact O
of O
hyper O
- O
parameter O
kin O
MaP O
Fon O
SQuAD O
1.1 O
with O
BERT O
- O
base O
as O
the O
backbone O
. O
We O
investigate O
the O
impact O
of O
kused O
to O
choose O
the O
top O
probabilities O
in O
the O
training O
phase O
. O
The O
results O
are O
shown O
in O
Figure O
7 O
. O
With O
the O
increase O
of O
k O
, O
the O
EM O
and O
F1 O
show O
a O
downtrend O
. O
The O
best O
performance O
happens O
at O
k=20 O
. O
We O
guess O
that O
choosing O
more O
probabilities O
makes O
the O
training O
difÔ¨Åcult O
and O
brings O
extra O
noises O
to O
the O
candidate O
positions O
. O
E.g. O
, O
ifkis O
set O
to O
30 O
, O
the O
number O
of O
candidate O
probabilities O
will O
be O
900 O
, O
which O
is O
larger O
than O
the O
sequence O
length O
512 O
in O
vector O
- O
based O
conditional O
approach O
. O
02468 O
05k10k15k20k25k30k35kTraining O
Loss O
Training O
StepsNo O
- O
Sampling O
( O
Vector O
- O
Based O
) O
Sampling O
- O
Top20 O
( O
Vector O
- O
Based O
) O
Figure O
8 O
: O
Convergence O
of O
sampling O
- O
based O
training O
strategy O
on O
BERT O
. O
We O
analyze O
the O
convergence O
of O
the O
sampling-693based O
training O
strategy O
on O
SQuAD O
1.1 O
. O
Due O
to O
the O
effectiveness O
of O
the O
sampling O
- O
based O
training O
strategy O
is O
proved O
in O
MaP O
, O
we O
conduct O
an O
further O
experiment O
under O
the O
VCP O
to O
prove O
its O
generalization O
. O
Figure O
8 O
demonstrates O
the O
results O
. O
As O
our O
expectation O
, O
the O
sampling O
- O
based O
training O
strategy O
optimizes O
the O
model O
as O
training O
in O
whole O
samples O
. O
However O
, O
it O
will O
cost O
longer O
training O
steps O
to O
get O
the O
same O
loss O
compared O
with O
standard O
training O
. O
So O
our O
samplingbased O
training O
strategy O
is O
good O
for O
the O
training O
of O
the O
matrix O
- O
based O
conditional O
approach O
. O
4 O
Related O
Work O
Machine O
reading O
comprehension O
is O
an O
important O
topic O
in O
the O
NLP O
community O
. O
More O
and O
more O
neural O
network O
models O
are O
proposed O
to O
tackle O
this O
problem O
, O
including O
DCN O
( O
Xiong O
et O
al O
. O
, O
2017 O
) O
, O
RNET O
( O
Wang O
et O
al O
. O
, O
2017 O
) O
, O
BiDAF O
( O
Seo O
et O
al O
. O
, O
2017 O
) O
, O
Match O
- O
LSTM O
( O
Wang O
and O
Jiang O
, O
2017 O
) O
, O
S O
- O
Net O
( O
Tan O
et O
al O
. O
, O
2018 O
) O
, O
SDNet O
( O
Zhu O
et O
al O
. O
, O
2018 O
) O
, O
QANet O
( O
Yu O
et O
al O
. O
, O
2018 O
) O
, O
HAS O
- O
QA O
( O
Pang O
et O
al O
. O
, O
2019 O
) O
. O
Among O
various O
MRC O
tasks O
, O
span O
extraction O
is O
a O
typical O
task O
that O
extracting O
a O
span O
of O
text O
from O
the O
corresponding O
passage O
as O
the O
answer O
of O
a O
given O
question O
. O
It O
can O
well O
overcome O
the O
weakness O
that O
words O
or O
entities O
are O
not O
sufÔ¨Åcient O
to O
answer O
questions O
( O
Liu O
et O
al O
. O
, O
2019 O
) O
. O
Previous O
models O
proposed O
for O
span O
extraction O
mostly O
focus O
on O
the O
design O
of O
architecture O
, O
especially O
on O
the O
representation O
of O
question O
and O
passage O
, O
and O
the O
interaction O
between O
them O
. O
There O
are O
few O
works O
devoted O
to O
the O
top O
- O
level O
design O
of O
span O
output O
, O
which O
refers O
to O
the O
probabilities O
generation O
from O
the O
representation O
. O
We O
divide O
the O
previous O
toplevel O
design O
into O
two O
categories O
, O
independent O
approach O
and O
conditional O
approach O
. O
The O
independent O
approach O
is O
to O
predict O
the O
start O
and O
end O
positions O
in O
the O
given O
passage O
independently O
( O
Kundu O
and O
Ng O
, O
2018a O
; O
Yu O
et O
al O
. O
, O
2018 O
) O
. O
Although O
the O
independent O
approach O
has O
a O
simple O
assumption O
, O
it O
works O
well O
when O
the O
input O
features O
are O
strong O
enough O
, O
e.g. O
, O
combining O
with O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
XLNet O
( O
Yang O
and O
Song O
, O
2019 O
) O
, O
and O
SpanBERT O
( O
Joshi O
et O
al O
. O
, O
2019 O
) O
. O
Nevertheless O
, O
since O
there O
is O
a O
kind O
of O
dependency O
relationship O
between O
start O
and O
end O
positions O
, O
the O
conditional O
approach O
has O
advancements O
over O
the O
independent O
approach O
. O
A O
typical O
work O
on O
the O
conditional O
approach O
comes O
from O
Wang O
and O
Jiang O
( O
2017 O
) O
. O
They O
proposed O
two O
different O
models O
based O
on O
the O
Pointer O
Network O
. O
One O
is O
the O
sequence O
model O
which O
produces O
a O
se O
- O
quence O
of O
answer O
tokens O
as O
the O
Ô¨Ånal O
output O
, O
and O
another O
is O
the O
boundary O
model O
which O
produces O
only O
the O
start O
token O
and O
the O
end O
token O
of O
the O
answer O
. O
The O
experimental O
results O
demonstrate O
that O
the O
boundary O
model O
( O
span O
extraction O
) O
is O
superior O
to O
the O
sequence O
model O
on O
both O
EM O
and O
F1 O
. O
The O
R O
- O
NET O
( O
Wang O
et O
al O
. O
, O
2017 O
) O
, O
BiDAF O
( O
Seo O
et O
al O
. O
, O
2017 O
) O
, O
S O
- O
Net O
( O
Tan O
et O
al O
. O
, O
2018 O
) O
, O
SDNet O
( O
Zhu O
et O
al O
. O
, O
2018 O
) O
have O
the O
same O
output O
layer O
and O
inference O
phase O
with O
the O
boundary O
model O
in O
( O
Wang O
and O
Jiang O
, O
2017 O
) O
. O
Lee O
et O
al O
. O
( O
2016 O
) O
presented O
an O
architecture O
that O
builds O
Ô¨Åxed O
length O
representations O
of O
all O
spans O
in O
the O
passage O
with O
a O
recurrent O
network O
to O
address O
the O
answer O
extraction O
task O
. O
The O
computation O
cost O
is O
decided O
by O
the O
max O
- O
length O
of O
the O
possible O
span O
and O
the O
sequence O
length O
. O
The O
experimental O
results O
show O
an O
improvement O
on O
EM O
compared O
with O
the O
endpoints O
prediction O
that O
independently O
predicts O
the O
two O
endpoints O
of O
the O
answer O
span O
. O
However O
, O
previous O
works O
related O
to O
the O
conditional O
approach O
are O
always O
based O
on O
a O
probability O
vector O
. O
We O
investigate O
another O
possible O
matrixbased O
conditional O
approach O
in O
this O
paper O
. O
Besides O
, O
a O
well O
- O
matched O
training O
strategy O
is O
proposed O
to O
our O
approach O
, O
and O
forward O
and O
backward O
conditional O
possibilities O
are O
also O
integrated O
to O
improve O
the O
performance O
. O
5 O
Conclusion O
In O
this O
paper O
, O
we O
Ô¨Årst O
investigate O
different O
approaches O
of O
span O
extraction O
in O
MRC O
. O
To O
improve O
the O
current O
vector O
- O
based O
conditional O
approach O
, O
we O
propose O
a O
matrix O
- O
based O
conditional O
approach O
. O
More O
careful O
consideration O
of O
the O
dependencies O
between O
the O
start O
and O
end O
positions O
of O
the O
answer O
span O
can O
predict O
their O
values O
better O
. O
We O
also O
propose O
a O
sampling O
- O
based O
training O
strategy O
to O
address O
the O
training O
process O
of O
the O
matrix O
- O
based O
conditional O
approach O
. O
The O
Ô¨Ånal O
experimental O
results O
on O
a O
wide O
of O
datasets O
demonstrate O
the O
effectiveness O
of O
our O
approach O
and O
training O
strategy O
. O
Acknowledgments O
This O
work O
was O
supported O
by O
National O
Key O
R&D O
Program O
of O
China O
( O
2019YFB2101802 O
) O
and O
Sichuan O
Key O
R&D O
project O
( O
2020YFG0035 O
) O
. O
Abstract O
Providing O
instant O
response O
for O
product O
- O
related O
questions O
in O
E O
- O
commerce O
question O
answering O
platforms O
can O
greatly O
improve O
users O
‚Äô O
online O
shopping O
experience O
. O
However O
, O
existing O
product O
question O
answering O
( O
PQA O
) O
methods O
only O
consider O
a O
single O
information O
source O
such O
as O
user O
reviews O
and/or O
require O
large O
amounts O
of O
labeled O
data O
. O
In O
this O
paper O
, O
we O
propose O
a O
novel O
framework O
to O
tackle O
the O
PQA O
task O
via O
exploiting O
heterogeneous O
information O
including O
natural O
language O
text O
and O
attribute O
- O
value O
pairs O
from O
two O
information O
sources O
of O
the O
concerned O
product O
, O
namely O
product O
details O
and O
user O
reviews O
. O
A O
heterogeneous O
information O
encoding O
component O
is O
then O
designed O
for O
obtaining O
uniÔ¨Åed O
representations O
of O
information O
with O
different O
formats O
. O
The O
sources O
of O
the O
candidate O
snippets O
are O
also O
incorporated O
when O
measuring O
the O
question O
- O
snippet O
relevance O
. O
Moreover O
, O
the O
framework O
is O
trained O
with O
a O
speciÔ¨Åcally O
designed O
weak O
supervision O
paradigm O
making O
use O
of O
available O
answers O
in O
the O
training O
phase O
. O
Experiments O
on O
a O
real O
- O
world O
dataset O
show O
that O
our O
proposed O
framework O
achieves O
superior O
performance O
over O
state O
- O
of O
- O
the O
- O
art O
models O
. O
1 O
Introduction O
To O
help O
potential O
consumers O
address O
their O
concerns O
during O
online O
shopping O
, O
many O
E O
- O
commerce O
sites O
now O
provide O
a O
community O
question O
answering O
( O
CQA O
) O
platform O
, O
where O
users O
can O
post O
questions O
for O
a O
speciÔ¨Åc O
product O
, O
and O
others O
can O
voluntarily O
answer O
them O
. O
Very O
often O
, O
it O
takes O
a O
long O
time O
for O
an O
asker O
to O
wait O
for O
an O
answer O
on O
such O
platforms O
. O
Therefore O
, O
automatically O
providing O
a O
proper O
response O
to O
a O
product O
- O
related O
question O
can O
greatly O
improve O
user O
online O
shopping O
experience O
and O
stimulate O
purchase O
decisions O
. O
‚àóThe O
work O
described O
in O
this O
paper O
is O
substantially O
supported O
by O
a O
grant O
from O
the O
Research O
Grant O
Council O
of O
the O
Hong O
Kong O
Special O
Administrative O
Region O
, O
China O
( O
Project O
Code O
: O
14200719).Several O
efforts O
have O
been O
made O
to O
tackle O
such O
product O
- O
related O
question O
answering O
( O
PQA O
) O
task O
( O
McAuley O
and O
Yang O
, O
2016 O
; O
Yu O
et O
al O
. O
, O
2018a O
; O
Gao O
et O
al O
. O
, O
2019 O
; O
Chen O
et O
al O
. O
, O
2019b O
; O
Deng O
et O
al O
. O
, O
2020b O
) O
. O
The O
existing O
methods O
can O
be O
generally O
categorized O
regarding O
the O
involved O
information O
source O
, O
i.e. O
, O
from O
where O
the O
responses O
are O
obtained O
. O
A O
pioneer O
work O
by O
McAuley O
and O
Yang O
( O
McAuley O
and O
Yang O
, O
2016 O
) O
investigates O
answer O
selection O
via O
detecting O
clues O
from O
user O
reviews O
. O
From O
then O
on O
, O
the O
review O
set O
becomes O
a O
commonly O
used O
auxiliary O
information O
for O
predicting O
the O
answer O
types O
or O
distinguishing O
true O
answers O
from O
randomly O
sampled O
ones O
( O
Wan O
and O
McAuley O
, O
2016 O
; O
Yu O
and O
Lam O
, O
2018 O
) O
. O
However O
, O
these O
methods O
are O
not O
feasible O
for O
newly O
- O
posted O
questions O
without O
candidate O
answers O
. O
A O
recent O
approach O
for O
PQA O
task O
is O
to O
directly O
extract O
review O
sentences O
as O
the O
response O
for O
a O
given O
question O
( O
Chen O
et O
al O
. O
, O
2019a O
) O
. O
But O
it O
requires O
a O
large O
number O
of O
labeled O
question O
- O
review O
pairs O
, O
whose O
annotation O
is O
a O
time O
- O
consuming O
and O
laborious O
work O
. O
Other O
information O
sources O
, O
such O
as O
existing O
QA O
collections O
, O
are O
also O
exploited O
( O
Yu O
et O
al O
. O
, O
2018b O
) O
, O
but O
relevant O
QA O
pairs O
are O
assumed O
to O
be O
always O
available O
for O
a O
new O
question O
in O
their O
setting O
, O
which O
is O
uncommon O
in O
practice O
. O
Besides O
user O
reviews O
, O
another O
kind O
of O
information O
, O
namely O
product O
details O
provided O
by O
the O
manufacturer O
are O
always O
available O
and O
can O
be O
an O
important O
information O
source O
for O
addressing O
productrelated O
questions O
. O
For O
example O
, O
considering O
the O
question O
‚Äú O
How O
large O
is O
the O
keyboard O
‚Äù O
for O
the O
product O
shown O
in O
Figure O
1 O
, O
the O
attribute O
- O
value O
pair O
‚Äú O
Item O
Dimensions O
: O
10.9√ó4.8√ó0.6 O
in O
‚Äù O
from O
the O
speciÔ¨Åcation O
table O
can O
be O
a O
good O
response O
. O
Such O
information O
can O
be O
essential O
for O
questions O
looking O
for O
factual O
type O
information O
due O
to O
their O
reliability O
and O
preciseness O
, O
but O
they O
are O
often O
underutilized O
in O
previous O
works O
. O
The O
above O
scenario O
motivates O
our O
task O
of O
answering O
product O
- O
related O
questions O
via O
exploit-696Logitech O
K380 O
  O
Multi O
- O
Device O
Bluetooth O
KeyboardProduct O
Details O
Product O
DetailsUser O
Reviews O
QA O
PairsFigure O
1 O
: O
A O
sample O
E O
- O
commerce O
product O
associated O
with O
its O
product O
details O
, O
user O
reviews O
, O
and O
QA O
pairs O
ing O
the O
information O
from O
both O
product O
details O
and O
user O
reviews O
to O
obtain O
relevant O
snippets O
serving O
as O
responses O
for O
improving O
user O
satisfaction O
. O
This O
task O
presents O
some O
new O
research O
challenges O
: O
( O
i O
) O
The O
heterogeneity O
of O
candidate O
information O
needs O
to O
be O
appropriately O
handled O
. O
From O
the O
above O
example O
, O
we O
can O
see O
that O
there O
exists O
both O
attributevalue O
pairs O
and O
natural O
language O
texts O
as O
candidate O
responses O
, O
which O
implies O
that O
typical O
answer O
selection O
approaches O
( O
Tan O
et O
al O
. O
, O
2016 O
; O
Wang O
et O
al O
. O
, O
2017 O
; O
Rao O
et O
al O
. O
, O
2019 O
) O
are O
incapable O
of O
handling O
the O
concerned O
task O
. O
( O
ii O
) O
Product O
details O
and O
user O
reviews O
contain O
different O
types O
of O
information O
, O
which O
are O
suitable O
for O
answering O
questions O
with O
different O
information O
needs O
. O
Returning O
to O
the O
example O
in O
Figure O
1 O
, O
considering O
a O
more O
subjective O
question O
asking O
about O
user O
experience O
‚Äú O
How O
is O
the O
key O
travel O
‚Äù O
, O
snippets O
from O
reviews O
such O
as O
‚Äú O
... O
good O
key O
travel O
and O
solid O
feel O
.. O
‚Äù O
can O
provide O
more O
appropriate O
responses O
. O
Thus O
, O
we O
can O
observe O
that O
questions O
with O
different O
intents O
can O
be O
better O
answered O
by O
snippets O
from O
different O
sources O
, O
which O
should O
be O
exploited O
when O
measuring O
the O
question O
- O
snippet O
relevance O
. O
( O
iii O
) O
Training O
a O
model O
to O
capture O
the O
relevance O
between O
a O
question O
and O
a O
candidate O
snippet O
with O
typical O
supervised O
paradigms O
requires O
a O
large O
volume O
of O
labeled O
data O
. O
However O
, O
it O
is O
very O
timeconsuming O
to O
manually O
label O
the O
question O
- O
snippet O
pairs O
in O
the O
PQA O
task O
due O
to O
the O
product O
- O
speciÔ¨Åc O
nature O
of O
questions O
and O
candidate O
snippets O
( O
Chen O
et O
al O
. O
, O
2019a O
) O
, O
which O
demands O
a O
better O
solution O
for O
training O
such O
models O
. O
To O
tackle O
these O
challenges O
, O
we O
propose O
a O
novel O
framework O
for O
the O
PQA O
task O
using O
Heterogenous O
Information O
via O
a O
Weak O
Supervision O
paradigm O
( O
HIWS O
) O
. O
Given O
a O
product O
- O
related O
question O
, O
HIWS O
exploits O
the O
corresponding O
product O
details O
and O
user O
reviews O
to O
return O
a O
ranked O
snippet O
list O
serving O
as O
the O
response O
. O
SpeciÔ¨Åcally O
, O
a O
heterogeneous O
infor O
- O
mation O
encoding O
component O
is O
Ô¨Årst O
developed O
to O
encode O
different O
information O
formats O
into O
a O
uniÔ¨Åed O
representation O
composed O
of O
a O
free O
text O
sentence O
and O
a O
set O
of O
focused O
aspects O
. O
Then O
for O
measuring O
the O
question O
- O
snippet O
relevance O
, O
a O
gated O
fusion O
approach O
is O
designed O
to O
get O
aspect O
- O
enhanced O
representations O
. O
Also O
, O
a O
question O
intent O
analysis O
module O
is O
designed O
to O
better O
determine O
which O
information O
source O
is O
more O
suitable O
for O
providing O
responses O
. O
To O
handle O
the O
shortage O
of O
labeled O
data O
for O
model O
training O
, O
we O
develop O
a O
weak O
supervision O
paradigm O
making O
use O
of O
the O
original O
user O
- O
posted O
answers O
during O
training O
. O
Some O
external O
resources O
including O
pre O
- O
trained O
language O
models O
such O
as O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
are O
utilized O
to O
obtain O
weak O
supervision O
signals O
to O
facilitate O
the O
training O
process O
. O
Our O
main O
contributions O
are O
as O
follows O
: O
‚Ä¢We O
explore O
to O
utilize O
heterogeneous O
information O
including O
attribute O
- O
value O
pairs O
and O
natural O
language O
sentences O
from O
both O
product O
details O
and O
user O
reviews O
to O
tackle O
the O
PQA O
task O
. O
‚Ä¢To O
handle O
the O
lack O
of O
labeled O
data O
, O
we O
design O
an O
effective O
weak O
supervision O
paradigm O
making O
use O
of O
available O
answers O
in O
training O
phase O
. O
‚Ä¢Experiments O
on O
real O
- O
world O
E O
- O
commerce O
dataset O
show O
that O
our O
proposed O
model O
achieves O
superior O
performance O
over O
state O
- O
of O
- O
the O
- O
art O
models O
. O
2 O
The O
Proposed O
Framework O
For O
a O
product O
p O
, O
its O
associated O
information O
can O
be O
represented O
as O
a O
tuple O
Cp= O
( O
A O
, O
D O
, O
R O
) O
, O
where O
A={(ai O
, O
vi)}is O
a O
set O
of O
attribute O
- O
value O
pairs O
extracted O
from O
the O
corresponding O
speciÔ¨Åcation O
table O
. O
D={di}denotes O
the O
textual O
product O
description O
snippets O
represented O
by O
di O
, O
R={ri}denotes O
the O
review O
set O
composed O
of O
review O
snippets O
represented O
byri O
. O
Now O
given O
a O
question O
qregarding O
the O
product O
p O
, O
our O
task O
is O
to O
automatically O
rank O
the O
candidate697snippets O
inCp O
, O
which O
can O
either O
be O
a O
textual O
sentence O
fromDorR O
, O
or O
an O
attribute O
- O
value O
pair O
from O
Afor O
providing O
responses O
to O
the O
question O
q. O
As O
shown O
in O
Figure O
2 O
, O
HIWS O
mainly O
consists O
of O
three O
components O
: O
heterogeneous O
information O
encoding O
, O
question O
- O
snippet O
relevance O
matching O
, O
and O
automatic O
label O
construction O
. O
Concretely O
, O
the O
candidate O
snippets O
are O
Ô¨Årst O
transformed O
into O
uniÔ¨Åed O
representations O
. O
Then O
we O
measure O
the O
questionsnippet O
relevance O
both O
from O
their O
aspect O
- O
enhanced O
representations O
and O
the O
intent O
matching O
. O
The O
overall O
model O
is O
then O
trained O
using O
the O
automaticallyconstructed O
labels O
via O
making O
use O
of O
the O
original O
answer O
to O
the O
given O
question O
. O
2.1 O
Heterogeneous O
Information O
Encoding O
Heterogeneous O
Information O
UniÔ¨Åcation O
Given O
the O
heterogeneous O
candidate O
snippets O
including O
natural O
language O
sentences O
and O
attribute O
- O
value O
pairs O
, O
we O
transform O
them O
into O
uniÔ¨Åed O
representations O
. O
It O
can O
be O
observed O
that O
these O
two O
types O
of O
information O
are O
actually O
complementary O
to O
each O
other O
where O
the O
attribute O
term O
in O
an O
attribute O
- O
value O
pair O
can O
well O
indicate O
the O
major O
focus O
of O
such O
snippet O
, O
while O
a O
textual O
sentence O
can O
usually O
provide O
more O
detailed O
semantic O
information O
. O
To O
highlight O
the O
focus O
of O
a O
natural O
language O
sentence O
¬Øc‚ààD‚à™R O
, O
we O
can O
extract O
maspect O
terms O
: O
ca={ca O
1,ca O
2, O
... O
,ca O
m}= O
AE(¬Øc O
) O
( O
1 O
) O
where O
AE(¬∑)refers O
to O
a O
reasonable O
aspect O
extraction O
algorithm O
such O
as O
( O
He O
et O
al O
. O
, O
2017 O
) O
used O
in O
our O
experiments O
. O
caare O
the O
extracted O
maspects O
. O
These O
extracted O
aspects O
are O
typically O
not O
exactly O
the O
same O
as O
the O
terms O
in O
the O
attribute O
set O
, O
but O
they O
play O
a O
similar O
role O
as O
characterizing O
the O
focus O
of O
the O
candidate O
snippet O
. O
For O
an O
attribute O
- O
value O
pair O
( O
ai O
, O
vi)‚ààA O
, O
since O
the O
main O
focus O
of O
such O
a O
snippet O
is O
already O
highlighted O
by O
the O
attribute O
term O
ai O
, O
we O
directly O
treat O
aias O
the O
aspectcaand O
construct O
a O
pesudo O
- O
sentence O
ctby O
concatenating O
the O
attribute O
and O
value O
terms O
. O
To O
this O
end O
, O
any O
raw O
snippet O
ÀÜc‚ààC O
p O
, O
regardless O
of O
its O
original O
information O
type O
( O
i.e. O
, O
whether O
it O
is O
an O
attribute O
- O
value O
pair O
or O
a O
natural O
language O
sentence O
) O
, O
is O
mapped O
to O
a O
uniÔ¨Åed O
representation O
, O
denoted O
as O
c O
, O
as O
follows O
: O
c= O
( O
ct O
, O
ca),whereca={ca O
1,ca O
2, O
... O
,ca O
m}(2 O
) O
wherectis O
the O
textual O
sentence O
of O
ÀÜc O
. O
Such O
a O
uniÔ¨Åed O
representation O
facilitates O
effective O
processing O
ofdifferent O
input O
formats O
and O
also O
enriches O
the O
input O
representation O
for O
later O
process O
. O
Snippet O
Encoding O
We O
next O
encode O
the O
uniÔ¨Åed O
candidate O
snippet O
representation O
cand O
the O
questionqto O
vector O
representations O
. O
We O
Ô¨Årst O
employ O
an O
embedding O
layer O
to O
transform O
each O
word O
into O
their O
corresponding O
word O
vector O
. O
The O
embedding O
of O
the O
word O
wis O
denoted O
as O
ew= O
[ O
ec O
w;eg O
w O
] O
, O
which O
is O
a O
concatenation O
of O
character O
- O
level O
embedding O
ec O
w O
and O
word O
- O
level O
embedding O
eg O
w. O
A O
bidirectional O
long O
short O
- O
term O
memory O
( O
Bi O
- O
LSTM O
) O
network O
is O
then O
employed O
to O
encode O
the O
local O
context O
information O
for O
each O
word O
in O
the O
question O
and O
the O
textual O
sentence O
ctof O
the O
candidate O
snippet O
, O
which O
generates O
the O
context O
- O
aware O
question O
and O
snippet O
representations O
as O
follows O
: O
hq O
i= O
Bi O
- O
LSTM O
( O
eq O
i O
, O
hq O
i‚àí1),i‚àà[1,lq](3 O
) O
hc O
i= O
Bi O
- O
LSTM O
( O
ec O
i O
, O
hc O
i‚àí1),i‚àà[1,lc](4 O
) O
whereh‚àó O
iis O
the O
hidden O
state O
of O
the O
encoder O
at O
thei O
- O
th O
time O
step O
. O
lqandlcare O
the O
length O
of O
the O
corresponding O
sequence O
. O
We O
denote O
the O
contextaware O
question O
and O
snippet O
representation O
as O
Hq‚àà O
Rlq√ódhandHc‚ààRlc√ódhrespectively O
, O
where O
dhis O
the O
number O
of O
hidden O
units O
of O
the O
LSTM O
network O
. O
Besides O
the O
free O
text O
part O
, O
there O
are O
also O
maspects O
for O
each O
candidate O
snippet O
c. O
They O
are O
useful O
when O
measuring O
the O
relevance O
between O
qandc O
since O
they O
can O
be O
regarded O
as O
the O
most O
salient O
part O
of O
the O
candidate O
snippet O
. O
Unlike O
a O
textual O
sentence O
, O
aspect O
terms O
are O
often O
quite O
short O
, O
so O
we O
directly O
employ O
the O
character O
- O
level O
embedding O
to O
transform O
each O
aspect O
term O
ca O
ito O
a O
vector O
representation O
denoted O
asha O
i O
: O
ha O
i O
= O
ec O
ca O
i= O
MaxPool(Conv O
( O
ca O
i O
) O
) O
( O
5 O
) O
where O
MaxPool(¬∑)andConv(¬∑)denote O
the O
maxpooling O
and O
convolutional O
operations O
( O
Kim O
, O
2014 O
) O
. O
2.2 O
Question O
- O
Snippet O
Relevance O
Matching O
Aspect O
- O
enhanced O
Representations O
To O
utilize O
the O
aspect O
information O
, O
we O
design O
a O
gated O
attention O
mechanism O
to O
highlight O
the O
relevant O
information O
in O
the O
question O
q. O
SpeciÔ¨Åcally O
, O
for O
the O
k O
- O
th O
word O
in O
the O
context O
- O
aware O
question O
representation O
, O
denoted O
asHq O
k O
, O
we O
measure O
the O
relative O
importance O
Œ±k[i]of O
this O
word O
given O
the O
i O
- O
th O
aspect O
term O
: O
Œ±k[i]=exp O
/ O
parenleftbig O
( O
Hq O
k)Tha O
i O
/ O
parenrightbig O
/summationtextlq O
j=1exp O
/ O
parenleftBig O
( O
Hq O
j)Tha O
i O
/ O
parenrightBig O
( O
6)698concat O
HqHcha1ha2 O
‚Ä¶ O
ÀúHqfwdxqc O
( O
info O
source)Bilinear O
Attentionu‚äóvqvcSemantic O
EncodingoqocAŒ≤ÃÇyOriginal O
QA O
Pair(q O
, O
a)weak O
  O
supervisionyLLoss O
QuestionqWord O
- O
level O
EmbeddingChar O
- O
level O
EmbeddingContext O
- O
aware O
Encodingfree O
textctaspectsca1,ca2, O
... O
,camRaw O
snippet O
ÃÇcHeterogeneous O
Information O
DistillingHeterogeneous O
  O
Info O
EncodingQ O
- O
S O
Relevance O
  O
MatchingAutomatic O
Label O
  O
ConstructionFigure O
2 O
: O
The O
architecture O
of O
proposed O
HIWS O
model O
Since O
there O
are O
in O
total O
maspects O
for O
a O
given O
candidate O
snippet O
c O
, O
we O
can O
similarly O
obtain O
Œ±k[1],Œ±k[2], O
... O
,Œ± O
k[m]attention O
scores O
for O
the O
k O
- O
th O
question O
word O
. O
These O
attention O
scores O
reÔ¨Çect O
different O
relative O
associations O
of O
the O
concerned O
word O
with O
different O
aspects O
. O
Then O
for O
every O
word O
in O
the O
questionq O
, O
we O
can O
obtain O
these O
attention O
scores O
, O
giving O
us O
an O
attention O
matrix O
A‚ààRlq√óm O
. O
To O
get O
one O
compositive O
attention O
weight O
for O
each O
word O
in O
the O
question O
, O
we O
apply O
a O
gated O
fusion O
approach O
to O
combine O
these O
aspects O
. O
SpeciÔ¨Åcally O
, O
a O
linear O
transformation O
is O
employed O
as O
a O
gate O
to O
learn O
an O
appropriate O
combination O
between O
these O
different O
attention O
weights O
as O
follows O
: O
Œ≤ O
= O
tanh(WaAT+ba O
) O
( O
7 O
) O
whereŒ≤‚ààRlqdenotes O
the O
relative O
importance O
of O
each O
word O
in O
the O
question O
q O
, O
Waandbaare O
trainable O
parameters O
. O
Then O
we O
can O
utilize O
the O
combined O
attention O
weight O
to O
obtain O
an O
aspect O
- O
enhanced O
question O
representation O
oq O
: O
oq=/summationdisplaylq O
k=1Hq O
k¬∑Œ≤k O
( O
8) O
Hereoqrepresents O
the O
question O
representation O
with O
an O
enhancement O
from O
multiple O
aspects O
of O
the O
candidate O
snippet O
, O
which O
captures O
the O
relevance O
information O
between O
qandcfrom O
the O
view O
of O
aspect O
terms O
. O
Based O
on O
the O
intuition O
that O
explicitly O
highlighting O
these O
aspects O
in O
ctis O
also O
helpful O
to O
capture O
its O
major O
information O
, O
we O
apply O
similar O
operations O
to O
Hc O
, O
giving O
an O
aspect O
- O
aware O
snippet O
representation O
oc O
. O
Question O
Intent O
Analysis O
for O
Multi O
- O
source O
Candidate O
Information O
The O
question O
intent O
helpsidentify O
what O
type O
of O
information O
the O
user O
is O
looking O
for O
and O
how O
to O
respond O
them O
. O
For O
example O
, O
it O
can O
be O
much O
more O
helpful O
to O
respond O
a O
question O
asking O
about O
personal O
experience O
with O
snippets O
from O
reviews O
. O
In O
contrast O
, O
the O
product O
details O
will O
be O
more O
suitable O
and O
convincing O
for O
a O
question O
looking O
for O
concrete O
product O
speciÔ¨Åcations O
. O
Thus O
, O
a O
question O
intent O
matching O
module O
is O
designed O
to O
detect O
such O
matching O
signals O
. O
It O
can O
be O
observed O
that O
the O
beginning O
words O
of O
a O
question O
often O
have O
stronger O
ability O
for O
indicating O
the O
question O
intent O
. O
Thus O
, O
given O
the O
question O
representationHq O
, O
a O
weight O
decay O
function O
fwd()is O
applied O
on O
it O
to O
emphasize O
the O
importance O
of O
the O
beginning O
words O
. O
Precisely O
, O
for O
the O
i O
- O
th O
word O
in O
the O
question O
, O
we O
multiply O
Hq O
ibyni O
, O
wheren‚àà(0,1 O
) O
can O
be O
set O
in O
advance O
such O
as O
n= O
0.9used O
in O
our O
experiments O
or O
learned O
with O
the O
model O
. O
Then O
we O
can O
obtain O
the O
encoded O
question O
representation O
rq O
as O
follows O
: O
/tildewideHq O
i O
= O
fwd O
i(Hq O
i O
) O
= O
ni‚äóHq O
i O
( O
9 O
) O
rq=/summationdisplaylq O
i=1 O
/ O
tildewideHq O
i O
( O
10 O
) O
where‚äórefers O
to O
the O
element O
- O
wise O
multiplication O
. O
We O
denote O
the O
question O
representation O
after O
such O
transformation O
as O
rq O
. O
Then O
given O
a O
one O
- O
hot O
feature O
vectoru‚ààR2of O
the O
candidate O
snippet O
cindicating O
its O
information O
source O
i.e. O
, O
from O
product O
details O
or O
user O
reviews O
. O
A O
bilinear O
attention O
layer O
is O
employed O
to O
achieve O
the O
question O
intent O
matching O
analysis O
: O
xqc= O
tanh O
( O
rqWmu+bm O
) O
( O
11 O
) O
whereWmandbmare O
trainable O
parameters O
, O
xqcdenotes O
a O
low O
- O
dimensional O
vector O
reÔ¨Çecting O
the O
intent O
matching O
between O
the O
question O
and O
the O
candidate O
snippet O
. O
Matching O
Signal O
Aggregation O
and O
Prediction O
After O
obtaining O
the O
aspect O
- O
enhanced O
representations O
and O
the O
question O
intent O
matching O
signals O
, O
we O
also O
employ O
a O
Siamese O
architecture O
to O
encode O
Hq O
andHcwith O
another O
Bi O
- O
LSTM O
encoder O
for O
capturing O
their O
main O
semantic O
information O
: O
vq= O
Bi O
- O
LSTM O
lq(Hq O
) O
( O
12 O
) O
vc= O
Bi O
- O
LSTM O
lc(Hc O
) O
( O
13 O
) O
We O
usel‚àóas O
the O
subscripts O
in O
the O
above O
equations O
to O
differentiate O
it O
from O
Equation O
( O
3 O
) O
indicating O
that O
only O
the O
last O
hidden O
state O
is O
taken O
as O
the O
encoded699representation O
. O
By O
utilizing O
the O
same O
sentence O
encoder O
, O
it O
helps O
map O
them O
into O
the O
same O
semantic O
space O
for O
determining O
their O
semantic O
relevance O
. O
Then O
these O
different O
matching O
signals O
can O
be O
aggregated O
and O
fed O
to O
a O
MLP O
layer O
to O
make O
the O
Ô¨Ånal O
prediction O
ÀÜy O
: O
ÀÜy= O
MLP([vq;vc;oq;oc;xqc O
] O
) O
( O
14 O
) O
where O
the O
aggregated O
vector O
contains O
matching O
features O
from O
different O
perspectives O
including O
the O
core O
semantic O
information O
vqandvc O
, O
the O
aspectenhanced O
representations O
oqandocwhich O
highlight O
the O
major O
focuses O
discussed O
in O
each O
sequence O
, O
as O
well O
as O
the O
question O
intent O
matching O
signals O
xqc O
containing O
information O
about O
which O
information O
source O
is O
better O
for O
answering O
the O
concerned O
question O
regarding O
its O
intent O
. O
The O
overall O
model O
is O
then O
trained O
to O
minimize O
the O
cross O
entropy O
loss O
between O
the O
predicted O
relevance O
score O
ÀÜyand O
the O
automatically O
- O
constructed O
label O
y O
which O
will O
be O
introduced O
in O
the O
next O
section O
: O
L=‚àí1 O
NN O
/ O
summationdisplay O
n=1[ÀÜynlogyn+ O
( O
1‚àíÀÜyn O
) O
log O
( O
1‚àíyn O
) O
] O
( O
15 O
) O
where O
ÀÜynandyndenote O
the O
prediction O
and O
label O
of O
then O
- O
th O
training O
instance O
, O
Nis O
the O
total O
number O
of O
training O
instances O
. O
2.3 O
Automatic O
Label O
Construction O
In O
order O
to O
learn O
a O
matching O
function O
between O
the O
question O
and O
candidates O
, O
the O
most O
typical O
approach O
is O
to O
utilize O
a O
large O
number O
of O
annotated O
sentence O
pairs O
( O
Chen O
et O
al O
. O
, O
2019a O
) O
to O
conduct O
the O
training O
. O
However O
, O
this O
manual O
solution O
is O
not O
effective O
in O
PQA O
settings O
due O
to O
the O
large O
volume O
of O
candidate O
snippets O
and O
the O
product O
- O
speciÔ¨Åc O
nature O
of O
questions O
and O
candidates O
. O
Fortunately O
, O
we O
can O
take O
advantage O
of O
the O
original O
user O
- O
posted O
answers O
to O
their O
corresponding O
questions O
via O
a O
weak O
supervision O
paradigm O
during O
the O
training O
phase O
which O
has O
been O
successfully O
applied O
to O
provide O
imperfect O
labels O
but O
with O
far O
more O
less O
human O
efforts O
in O
many O
NLP O
tasks O
such O
as O
knowledge O
- O
base O
completion O
( O
Hoffmann O
et O
al O
. O
, O
2011 O
) O
and O
sentiment O
analysis O
( O
Severyn O
and O
Moschitti O
, O
2015b O
) O
etc O
. O
Given O
a O
question O
q O
, O
we O
have O
its O
answer O
aduring O
the O
training O
phase O
as O
auxiliary O
information O
to O
obtain O
the O
labelyfor O
the O
candidate O
snippet O
c. O
To O
make O
use O
of O
the O
information O
of O
the O
whole O
QA O
pair O
, O
the O
entire O
QA O
pair O
( O
q O
, O
a)is O
Ô¨Årst O
fused O
to O
an O
integrated O
textualsnippetpqawith O
some O
heuristic O
rules O
( O
details O
are O
given O
in O
Sec O
3.3 O
) O
. O
Then O
the O
problem O
of O
obtaining O
the O
relevance O
label O
between O
candqare O
cast O
as O
measuring O
the O
relation O
between O
ctwithpqa O
. O
We O
measure O
such O
relation O
from O
two O
perspectives O
, O
namely O
, O
syntactic O
relevance O
and O
semantic O
relevance O
. O
Syntactic O
Relevance O
. O
Word O
overlapping O
between O
two O
text O
items O
can O
be O
a O
strong O
signal O
indicating O
their O
relevance O
. O
Here O
we O
adopt O
the O
idea O
of O
ROUGE O
( O
Lin O
, O
2004 O
) O
which O
is O
initially O
proposed O
for O
computing O
a O
recall O
- O
based O
word O
overlapping O
score O
to O
compute O
the O
syntactic O
- O
level O
relevance O
score O
s1 O
: O
s1= O
ROUGE-1 O
( O
ct O
, O
pqa O
) O
+ O
ROUGE-2 O
( O
ct O
, O
pqa O
) O
( O
16 O
) O
where O
ROUGE O
- O
N O
refers O
to O
the O
overlap O
of O
N O
- O
grams O
betweenctandpqa O
. O
Semantic O
Relevance O
. O
To O
address O
the O
issue O
of O
the O
semantic O
gap O
between O
two O
text O
items O
, O
many O
word O
and O
sentence O
embedding O
models O
have O
been O
proposed O
and O
successfully O
applied O
to O
many O
NLP O
tasks O
recently O
. O
Here O
, O
we O
utilize O
some O
pre O
- O
trained O
text O
embedding O
models O
to O
compute O
the O
semantic O
relevance O
between O
the O
integrated O
QA O
snippet O
pqaandct O
: O
si= O
cos(Pre O
- O
TE O
i(pqa),Pre O
- O
TE O
i(ct O
) O
) O
( O
17 O
) O
where O
Pre O
- O
TE O
refers O
to O
a O
pre O
- O
trained O
text O
encoder O
. O
We O
adopt O
GloVe O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
, O
Elmo O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
and O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
in O
our O
experiments O
. O
cos(¬∑)denotes O
the O
cosine O
similarity O
score O
between O
the O
two O
encoded O
sentence O
representations O
. O
We O
denote O
the O
computed O
relevance O
scores O
with O
the O
aforementioned O
pre O
- O
trained O
models O
ass2,s3,s4respectively O
. O
After O
obtaining O
these O
relevance O
signals O
, O
a O
small O
amount O
of O
human O
- O
annotated O
question O
- O
snippet O
pairs O
are O
used O
to O
train O
a O
simple O
classiÔ¨Åer O
for O
learning O
to O
combine O
these O
signals O
into O
the O
single O
label O
y1 O
. O
Note O
that O
it O
seems O
to O
be O
unnecessary O
to O
design O
any O
framework O
if O
a O
simple O
classiÔ¨Åer O
with O
a O
few O
amount O
of O
labeled O
data O
and O
some O
pre O
- O
trained O
models O
can O
achieve O
a O
high O
accuracy O
. O
This O
is O
because O
we O
use O
the O
information O
of O
the O
entire O
QA O
pair O
to O
obtain O
the O
labelydenoting O
the O
question O
- O
snippet O
relevance O
, O
which O
is O
different O
when O
we O
only O
have O
the O
question O
qand O
needs O
to O
retrieve O
relevant O
snippets O
during O
the O
testing O
phase O
. O
Thus O
a O
simple O
classiÔ¨Åer O
with O
a O
few O
amount O
of O
labeled O
data O
can O
learn O
to O
integrate O
these O
relevance O
scores O
for O
the O
construction O
of O
‚Äú O
gold O
‚Äù O
labels O
with O
the O
help O
of O
original O
answers O
. O
140 O
questions O
with O
their O
candidate O
snippets O
are O
annotated O
for O
this O
purpose O
, O
a O
SVM O
classiÔ¨Åer O
is O
used O
in O
our O
experiment.7003 O
Experiments O
3.1 O
Dataset O
We O
perform O
experiments O
on O
real O
- O
world O
data O
to O
validate O
the O
model O
effectiveness O
. O
The O
question O
- O
answer O
pairs O
and O
reviews O
are O
drawn O
from O
the O
Amazon O
QA O
dataset O
( O
McAuley O
and O
Yang O
, O
2016 O
) O
and O
Amazon O
review O
dataset O
( O
Ni O
et O
al O
. O
, O
2019 O
) O
. O
Product O
details O
are O
crawled O
from O
the O
corresponding O
products O
‚Äô O
pages O
and O
incorporated O
into O
our O
dataset O
. O
In O
this O
way O
, O
we O
construct O
a O
heterogeneous O
dataset O
, O
which O
includes O
in O
total O
5,395 O
QA O
pairs O
of O
3,840 O
products O
spanning O
three O
product O
categories O
, O
namely O
, O
‚Äú O
Cell O
Phones O
and O
Accessories O
‚Äù O
, O
‚Äú O
Sports O
and O
Outdoors O
‚Äù O
and‚ÄúTools O
and O
Home O
Improvement O
‚Äù O
. O
For O
each O
question O
, O
we O
Ô¨Årst O
utilize O
the O
BM25 O
algorithm O
to O
conduct O
an O
initial O
Ô¨Åltering O
and O
collect O
the O
50 O
top O
- O
ranked O
snippets O
from O
the O
corresponding O
product O
information O
as O
candidate O
snippets O
. O
After O
discarding O
empty O
or O
meaningless O
strings O
, O
we O
obtain O
219,563 O
question O
- O
candidate O
snippet O
pairs O
in O
total O
. O
The O
dataset O
is O
split O
for O
training O
/ O
validation O
/ O
testing O
as O
4,023 O
/ O
779 O
/ O
593 O
questions O
respectively O
, O
which O
results O
in O
163,063 O
/ O
32,178 O
/ O
24,322 O
question O
- O
snippet O
pairs O
in O
each O
set O
. O
To O
obtain O
training O
and O
validation O
set O
, O
we O
utilize O
the O
weak O
supervision O
paradigm O
described O
in O
Sec O
2.3 O
to O
automatically O
construct O
labels O
. O
For O
the O
testing O
set O
, O
in O
order O
to O
evaluate O
the O
effectiveness O
of O
the O
whole O
framework O
, O
the O
relevance O
labels O
between O
the O
questions O
and O
candidate O
snippets O
are O
annotated O
manually O
by O
two O
trained O
human O
annotators O
, O
the O
disagreements O
of O
the O
annotations O
are O
resolved O
by O
another O
experienced O
annotator O
3.2 O
Baselines O
and O
Evaluation O
Metrics O
To O
compare O
with O
our O
proposed O
framework O
, O
we O
adopt O
several O
strong O
baseline O
and O
state O
- O
of O
- O
the O
- O
art O
question O
answering O
models O
, O
including O
CNN O
( O
Severyn O
and O
Moschitti O
, O
2015a O
) O
, O
QA O
- O
LSTM O
( O
Tan O
et O
al O
. O
, O
2016 O
) O
, O
MatchPyramid O
( O
Pang O
et O
al O
. O
, O
2016 O
) O
, O
BiMPM O
( O
Wang O
et O
al O
. O
, O
2017 O
) O
, O
Conv O
- O
KNRM O
( O
Dai O
et O
al O
. O
, O
2018 O
) O
, O
HCAN O
( O
Rao O
et O
al O
. O
, O
2019 O
) O
for O
comparisons O
. O
These O
models O
take O
the O
question O
and O
natural O
language O
sentence O
part O
of O
the O
candidate O
snippet O
as O
input O
, O
and O
are O
trained O
using O
the O
same O
automaticallyconstructed O
labels O
derived O
from O
original O
QA O
pairs O
as O
our O
proposed O
HIWS O
framework O
. O
Two O
retrieval O
- O
based O
unsupervised O
models O
are O
also O
adopted O
: O
( O
1 O
) O
BM25 O
: O
It O
is O
a O
widely O
- O
used O
bagof O
- O
words O
retrieval O
model O
. O
( O
2 O
) O
QCEM O
: O
Question O
Candidate O
Embedding O
Matching O
is O
an O
unsupervised O
method O
that O
sums O
the O
word O
vectors O
of O
each O
sentenceTable O
1 O
: O
Response O
Selection O
Performance O
MAP O
MRR O
P@5 O
P@10 O
BM25 O
0.417 O
0.549 O
0.296 O
0.234 O
QCEM O
0.479 O
0.623 O
0.385 O
0.278 O
CNN O
0.576 O
0.665 O
0.430 O
0.329 O
QA O
- O
LSTM O
0.561 O
0.656 O
0.419 O
0.327 O
MatchPyramid O
0.630 O
0.700 O
0.466 O
0.353 O
BiMPM O
0.613 O
0.683 O
0.458 O
0.336 O
Conv O
- O
KNRM O
0.615 O
0.696 O
0.457 O
0.337 O
HCAN O
0.632 O
0.710 O
0.459 O
0.339 O
HIWS O
0.674 O
0.749 O
0.498 O
0.363 O
as O
the O
sentence O
embedding O
, O
and O
cosine O
similarity O
is O
utilized O
for O
predicting O
sentence O
relevance O
. O
For O
evaluation O
metrics O
, O
Mean O
Average O
Precision O
( O
MAP O
) O
, O
Mean O
Reciprocal O
Rank O
( O
MRR O
) O
, O
and O
Precision O
at O
N O
( O
P@N O
) O
are O
used O
to O
measure O
the O
performance O
. O
Precision O
at O
N O
( O
P@N O
) O
is O
the O
precision O
of O
the O
N O
retrieved O
snippets O
. O
We O
set O
N=5 O
and O
N=10 O
which O
correspond O
to O
P@5 O
and O
P@10 O
respectively O
in O
our O
experiments O
. O
3.3 O
Implementation O
Details O
For O
the O
automatic O
label O
construction O
, O
we O
Ô¨Årst O
utilize O
the O
user O
- O
posted O
answer O
to O
paraphrase O
the O
question O
for O
obtaining O
the O
integrated O
snippet O
pqaaccording O
to O
the O
part O
- O
of O
- O
speech O
tags O
and O
syntactic O
structure O
of O
the O
question O
with O
heuristic O
rules O
. O
For O
example O
, O
for O
a O
question O
‚Äú O
does O
it O
have O
a O
front O
- O
facing O
camera O
? O
‚Äù O
with O
the O
answer O
‚Äú O
No O
. O
‚Äù O
, O
it O
will O
be O
combined O
to O
‚Äú O
It O
does O
not O
have O
a O
front O
- O
facing O
camera O
‚Äù O
. O
For O
the O
network O
architecture O
, O
we O
initialize O
the O
word O
embedding O
layer O
with O
the O
pre O
- O
trained O
300D O
GloVe O
word O
vectors O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
. O
The O
sizes O
of O
the O
CNN O
Ô¨Ålters O
in O
the O
character O
- O
level O
embedding O
are O
set O
to O
[ O
2,3,4,5 O
] O
, O
each O
with O
75 O
Ô¨Ålters O
, O
resulting O
in O
300D O
character O
- O
level O
embedding O
for O
each O
word O
. O
The O
hidden O
dimension O
of O
the O
contextaware O
Bi O
- O
LSTM O
encoder O
is O
set O
to O
150 O
, O
with O
the O
dropout O
rate O
being O
0.3 O
. O
The O
hidden O
dimension O
of O
the O
sentence O
encoder O
in O
Eq O
. O
( O
12 O
) O
is O
set O
to O
64 O
, O
with O
the O
dropout O
rate O
also O
being O
0.3 O
. O
The O
hidden O
dimensions O
of O
the O
MLP O
layer O
in O
the O
Ô¨Ånal O
prediction O
layer O
are O
set O
to O
300 O
and O
100 O
respectively O
, O
with O
ReLU O
as O
the O
activation O
function O
. O
All O
models O
are O
trained O
with O
the O
batch O
size O
of O
100 O
. O
The O
number O
of O
aspects O
m O
for O
each O
candidate O
snippet O
is O
set O
to O
be O
3 O
which O
is O
a O
moderate O
number O
for O
a O
single O
sentence.701Table O
2 O
: O
Effectiveness O
of O
Weak O
Supervision O
Paradigm O
BiMPM O
HCAN O
HIWS O
MAP O
MRR O
MAP O
MRR O
MAP O
MRR O
with O
QA O
0.338 O
0.409 O
0.329 O
0.402 O
0.310 O
0.393 O
with O
SQS O
0.443 O
0.492 O
0.432 O
0.495 O
0.479 O
0.556 O
with O
WS O
0.613 O
0.683 O
0.632 O
0.710 O
0.674 O
0.749 O
3.4 O
Quantitative O
Evaluation O
Results O
Response O
Selection O
Performance O
The O
evaluation O
results O
are O
presented O
in O
Table O
1 O
, O
which O
demonstrates O
that O
our O
proposed O
HIWS O
achieves O
the O
best O
performance O
among O
all O
evaluation O
metrics O
compared O
with O
both O
retrieval O
- O
based O
solutions O
and O
supervised O
QA O
matching O
methods O
. O
We O
can O
observe O
that O
some O
simple O
QA O
models O
such O
as O
QA O
- O
LSTM O
and O
unsupervised O
models O
such O
as O
QCEM O
can O
still O
achieve O
reasonable O
performance O
. O
For O
those O
state O
- O
of O
- O
theart O
models O
such O
as O
BiMPM O
and O
HCAN O
, O
although O
equipped O
with O
complicated O
network O
architecture O
, O
they O
do O
not O
perform O
as O
promising O
as O
expected O
. O
Such O
a O
result O
is O
due O
to O
the O
fact O
that O
these O
QA O
models O
merely O
focus O
on O
the O
matching O
between O
text O
items O
and O
ignore O
some O
important O
characteristics O
in O
the O
Ecommerce O
scenario O
such O
as O
the O
heterogeneous O
information O
formats O
and O
multiple O
information O
sources O
of O
the O
candidate O
snippets O
. O
HIWS O
exploits O
such O
characteristics O
and O
utilize O
the O
extracted O
aspects O
to O
obtain O
enriched O
representations O
, O
leading O
to O
its O
superior O
performance O
. O
Effectiveness O
of O
Proposed O
Weak O
Supervision O
Paradigm O
We O
investigate O
two O
alternative O
strategies O
for O
tackling O
the O
shortage O
of O
labeled O
data O
and O
compare O
them O
with O
our O
proposed O
weak O
supervision O
strategy O
to O
examine O
its O
effectiveness O
. O
The O
results O
on O
the O
same O
test O
set O
are O
reported O
in O
Table O
2 O
. O
SpeciÔ¨Åcally O
, O
we O
train O
HIWS O
and O
two O
baselines O
, O
namely O
BiMPM O
and O
HCAN O
with O
different O
methods O
: O
‚Äú O
with O
QA O
‚Äù O
denotes O
training O
with O
the O
QA O
pairs O
instead O
of O
question O
- O
snippet O
pairs O
as O
in O
Table O
1 O
. O
We O
treat O
questions O
with O
their O
original O
answers O
as O
the O
positive O
samples O
and O
other O
randomly O
selected O
answers O
as O
negative O
samples O
for O
model O
training O
; O
‚Äú O
with O
SQS O
‚Äù O
refers O
to O
models O
which O
are O
Ô¨Årst O
trained O
with O
QA O
pairs O
, O
then O
the O
Small O
number O
of O
annotated O
Question- O
Snippet O
pairs O
introduced O
in O
Sec O
2.3 O
are O
used O
to O
Ô¨Åne O
- O
tune O
the O
model O
; O
‚Äú O
with O
WS O
‚Äù O
means O
the O
model O
is O
trained O
with O
the O
proposed O
weak O
supervision O
approach O
. O
Comparing O
these O
model O
variants O
, O
we O
can O
observe O
that O
models O
trained O
with O
the O
original O
QA O
pairs O
perform O
quite O
worse O
, O
showing O
theTable O
3 O
: O
Ablation O
study O
for O
components O
in O
HIWS O
Ablation O
of O
HIWS O
MAP O
MRR O
w/o O
syntactic O
relevance O
score O
0.273 O
0.434 O
w/o O
semantic O
relevance O
score O
0.543 O
0.626 O
w/o O
question O
intent O
matching O
0.667 O
0.737 O
w/o O
aspect O
- O
enhanced O
representations O
0.631 O
0.704 O
HIWS O
0.674 O
0.749 O
semantic O
gap O
between O
the O
original O
answers O
and O
the O
candidate O
snippets O
needs O
to O
be O
handled O
properly O
. O
Models O
with O
SQS O
outperform O
models O
with O
QA O
via O
Ô¨Åne O
- O
tuning O
with O
proper O
data O
, O
but O
it O
still O
failed O
to O
achieve O
satisfactory O
results O
due O
to O
the O
limited O
amount O
of O
labeled O
data O
. O
However O
, O
performance O
for O
all O
models O
can O
be O
improved O
with O
our O
proposed O
weak O
supervision O
paradigm O
, O
demonstrating O
its O
effectiveness O
on O
utilizing O
original O
answer O
information O
for O
bridging O
the O
connection O
between O
the O
question O
and O
snippets O
in O
the O
E O
- O
commerce O
settings O
. O
Ablation O
Analysis O
We O
conduct O
ablation O
analysis O
to O
investigate O
the O
effectiveness O
of O
some O
important O
components O
in O
HIWS O
as O
shown O
in O
Table O
3 O
. O
We O
Ô¨Årst O
create O
two O
sets O
of O
training O
labels O
whose O
construction O
step O
only O
involves O
one O
kind O
of O
relevance O
scores O
introduced O
in O
Sec O
2.3 O
, O
denoted O
as O
‚Äú O
w/o O
syntactic O
relevance O
score O
‚Äù O
and O
‚Äú O
w/o O
semantic O
relevance O
score O
‚Äù O
respectively O
. O
It O
can O
be O
observed O
that O
these O
two O
kinds O
of O
linguistic O
considerations O
, O
especially O
the O
syntactic O
relevance O
, O
are O
quite O
essential O
for O
automatically O
obtaining O
the O
labels O
for O
conducting O
training O
and O
thus O
directly O
inÔ¨Çuence O
the O
Ô¨Ånal O
performance O
of O
our O
model O
. O
Another O
two O
important O
components O
in O
HIWS O
are O
the O
aspect O
- O
enhanced O
representations O
and O
the O
question O
intent O
matching O
. O
As O
shown O
in O
Table O
3 O
, O
these O
two O
components O
contribute O
to O
some O
performance O
boost O
, O
especially O
the O
aspect O
- O
enhanced O
module O
. O
For O
constructing O
the O
variant O
model O
without O
aspect O
- O
enhanced O
representations O
, O
we O
still O
feed O
the O
embedded O
aspect O
ha O
iinto O
the O
aggregation O
layer O
. O
Thus O
, O
even O
without O
considering O
the O
interaction O
between O
aspects O
and O
the O
question O
as O
in O
HIWS O
, O
this O
variant O
still O
outperforms O
some O
baselines O
. O
Performance O
with O
Different O
Amount O
of O
Data O
We O
further O
investigate O
the O
robustness O
of O
HIWS O
via O
examining O
its O
performance O
with O
different O
amount O
of O
training O
data O
. O
The O
MAP O
and O
MRR O
scores O
under O
each O
product O
category O
are O
reported O
in O
Figure O
3 O
, O
where O
‚Äù O
w O
/ O
ndata O
‚Äù O
refers O
to O
HIWS O
trained O
with O
n O
proportion O
of O
the O
entire O
training O
data O
. O
It O
can O
be O
observed O
that O
even O
when O
we O
use O
a O
moderate O
amount O
of O
training O
data O
such O
as O
3/4 O
training O
data O
, O
the O
per-702Figure O
3 O
: O
Performance O
with O
Different O
Amount O
of O
Data O
formance O
does O
not O
drop O
signiÔ¨Åcantly O
. O
Such O
results O
show O
the O
robustness O
of O
our O
proposed O
model O
implying O
that O
it O
can O
effectively O
utilize O
the O
available O
QA O
pairs O
to O
automatically O
construct O
useful O
training O
signals O
for O
learning O
the O
question O
- O
snippet O
relevance O
relation O
. O
3.5 O
Case O
Study O
To O
gain O
some O
insights O
into O
HIWS O
, O
we O
present O
two O
sample O
questions O
with O
the O
top O
- O
one O
responses O
given O
by O
HIWS O
and O
two O
strong O
existing O
methods O
in O
Table O
4 O
. O
The O
information O
sources O
of O
each O
snippet O
are O
marked O
, O
whereA O
, O
D O
, O
Rrefers O
to O
attribute O
- O
value O
pairs O
, O
product O
textual O
descriptions O
and O
reviews O
respectively O
. O
Following O
each O
information O
source O
symbol O
, O
the O
correctness O
of O
the O
retrieved O
response O
is O
given O
. O
From O
the O
results O
, O
we O
can O
observe O
that O
HIWS O
successfully O
handles O
candidate O
snippets O
from O
different O
sources O
to O
answer O
the O
product O
questions O
with O
different O
information O
needs O
. O
For O
example O
, O
it O
precisely O
retrieves O
the O
corresponding O
attribute O
of O
the O
product O
for O
Question-1 O
, O
which O
is O
more O
reliable O
and O
precise O
than O
the O
snippet O
retrieved O
from O
the O
review O
set O
by O
the O
existing O
models O
. O
Moreover O
, O
HIWS O
correctly O
handles O
the O
second O
question O
while O
the O
focused O
aspect O
is O
missing O
in O
responses O
from O
other O
methods O
. O
This O
is O
likely O
due O
to O
the O
aspect O
- O
enhanced O
representations O
for O
highlighting O
the O
major O
focus O
in O
the O
question O
and O
snippets O
. O
This O
result O
shows O
the O
necessity O
of O
effectively O
exploring O
different O
types O
of O
information O
of O
the O
concerned O
product O
instead O
of O
considering O
a O
single O
information O
source O
as O
in O
previous O
works O
. O
4 O
Related O
Work O
In O
recent O
years O
, O
many O
deep O
learning O
based O
methods O
have O
been O
proposed O
for O
the O
answer O
selection O
task O
in O
community O
question O
answering O
( O
CQA O
) O
platforms O
. O
These O
models O
can O
be O
generally O
categorized O
into O
two O
types O
according O
to O
their O
network O
architecture O
( O
Lai O
et O
al O
. O
, O
2018 O
) O
, O
namely O
Siamese O
networks O
( O
Tan O
et O
al O
. O
, O
2016 O
; O
Mueller O
and O
Thyagarajan O
, O
2016 O
) O
and O
Compare O
- O
Aggregate O
networks O
( O
WangTable O
4 O
: O
Two O
sample O
questions O
with O
the O
top O
- O
one O
responses O
returned O
by O
HIWS O
and O
two O
existing O
models O
. O
The O
information O
sources O
and O
the O
gold O
labels O
of O
the O
snippets O
are O
also O
marked O
out O
in O
the O
parentheses O
at O
the O
end O
of O
each O
snippet O
respectively O
. O
Question-1 O
: O
What O
is O
the O
overall O
length O
of O
this O
bulb O
? O
HIWS O
: O
Product O
Dimensions O
: O
6.5 O
x O
2.5 O
x O
2.5 O
inches O
( O
A O
) O
( O
/check O
) O
MatchPyramid O
: O
I O
decided O
to O
try O
using O
these O
before O
i O
went O
more O
expensive O
route O
, O
the O
bulb O
are O
indeed O
quite O
large O
the O
length O
of O
a O
hand O
perhaps O
. O
( O
R O
) O
( O
√ó O
) O
HCAN O
: O
I O
will O
update O
this O
review O
to O
render O
my O
durability O
opinion O
, O
one O
last O
note O
pay O
attention O
to O
the O
length O
of O
these O
bulb O
. O
( O
R O
) O
( O
√ó O
) O
Question-2 O
: O
Will O
this O
work O
with O
my O
unlocked O
Ô¨Åre O
phone O
i O
have O
straight O
talk O
i O
want O
to O
switch O
to O
the O
amazon O
Ô¨Åre O
phone O
. O
HIWS O
: O
Sim O
card O
will O
only O
work O
with O
an O
att O
compatible O
or O
unlocked O
gsm O
phone O
( O
D O
) O
( O
/check O
) O
MatchPyramid O
: O
Keep O
your O
current O
phone O
number O
. O
Works O
with O
SIMs O
, O
IM O
, O
social O
networks O
, O
email O
, O
and O
web O
. O
( O
D O
) O
( O
√ó O
) O
HCAN O
: O
I O
have O
tmobile O
and O
the O
service O
is O
not O
good O
in O
my O
area O
so O
i O
want O
to O
switch O
to O
straight O
talk O
( O
R O
) O
( O
√ó O
) O
et O
al O
. O
, O
2017 O
; O
Rao O
et O
al O
. O
, O
2019 O
; O
Deng O
et O
al O
. O
, O
2020a O
) O
. O
Product O
- O
related O
Question O
Answering O
( O
PQA O
) O
problem O
has O
drawn O
a O
lot O
of O
attention O
recently O
, O
due O
to O
the O
increasing O
popularity O
of O
online O
shopping O
. O
Most O
of O
the O
existing O
works O
utilize O
reviews O
as O
their O
major O
information O
to O
provide O
responses O
for O
a O
given O
question O
. O
McAuley O
and O
Yang O
( O
2016 O
) O
treat O
reviews O
as O
‚Äú O
experts O
‚Äù O
to O
handle O
the O
answer O
selection O
task O
. O
Later O
, O
product O
aspects O
are O
considered O
to O
further O
improve O
the O
performance O
( O
Yu O
and O
Lam O
, O
2018 O
) O
. O
Chen O
et O
al O
. O
( O
2019a O
) O
propose O
to O
tackle O
PQA O
task O
by O
directly O
retrieving O
review O
sentences O
as O
answers O
. O
However O
, O
it O
requires O
a O
large O
number O
of O
labeled O
questionreview O
pairs O
. O
Yu O
et O
al O
. O
( O
2018b O
) O
assume O
that O
relevant O
QA O
pairs O
are O
always O
available O
for O
a O
given O
question O
which O
can O
be O
utilized O
to O
provide O
the O
responses O
. O
Some O
other O
works O
formulate O
the O
PQA O
task O
as O
a O
reading O
comprehension O
problem O
( O
Xu O
et O
al O
. O
, O
2019 O
) O
, O
where O
the O
main O
focus O
is O
to O
extract O
a O
text O
span O
as O
the O
answer O
given O
a O
relevant O
review O
, O
which O
is O
unavailable O
in O
many O
cases O
. O
Given O
some O
successful O
applications O
of O
text O
generation O
models O
such O
as O
text O
summarization O
( O
Rush O
et O
al O
. O
, O
2015 O
) O
and O
response O
generation O
( O
Tao O
et O
al O
. O
, O
2018 O
) O
, O
some O
models O
are O
proposed O
to O
generate O
an O
answer O
sentence O
( O
Gao O
et O
al O
. O
, O
2019 O
; O
Chen O
et O
al O
. O
, O
2019b O
) O
given O
relevant O
product O
information O
, O
some O
later O
works O
speciÔ¨Åcally O
consider O
the O
user O
opinion O
information O
during O
such O
generation O
process O
( O
Deng O
et O
al O
. O
, O
2020b O
) O
. O
Since O
most O
product O
- O
related O
questions O
are O
looking O
for O
diverse O
answers O
, O
we O
argue O
that O
information O
extracted O
from O
reliable O
sources O
is O
more O
effective O
and O
explainable703solution O
for O
the O
PQA O
task O
. O
More O
recently O
, O
some O
studies O
consider O
the O
answer O
helpfulness O
prediction O
task O
( O
Zhang O
et O
al O
. O
, O
2020b O
) O
and O
answer O
ranking O
problem O
( O
Zhang O
et O
al O
. O
, O
2020a O
) O
in O
the O
context O
of O
PQA O
, O
assuming O
the O
existence O
of O
user O
- O
provided O
answers O
to O
a O
given O
question O
. O
Different O
from O
them O
, O
we O
aim O
to O
provide O
instant O
responses O
for O
a O
newly O
- O
posted O
question O
in O
E O
- O
commerce O
. O
5 O
Conclusions O
We O
propose O
a O
novel O
framework O
for O
answering O
product O
- O
related O
questions O
via O
exploiting O
heterogeneous O
information O
including O
attribute O
- O
value O
pairs O
and O
free O
text O
sentences O
from O
both O
product O
details O
and O
user O
reviews O
. O
To O
tackle O
the O
shortage O
of O
labeled O
data O
, O
we O
design O
a O
weak O
supervision O
paradigm O
by O
making O
use O
of O
the O
existing O
QA O
pairs O
to O
automatically O
construct O
labels O
for O
training O
. O
Extensive O
experiments O
conducted O
on O
a O
real O
- O
word O
dataset O
demonstrate O
the O
superiority O
of O
our O
proposed O
framework O
. O
Abstract O
An O
NLP O
model O
‚Äôs O
ability O
to O
reason O
should O
be O
independent O
of O
language O
. O
Previous O
works O
utilize O
Natural O
Language O
Inference O
( O
NLI O
) O
to O
understand O
the O
reasoning O
ability O
of O
models O
, O
mostly O
focusing O
on O
high O
resource O
languages O
like O
English O
. O
T O
o O
address O
scarcity O
of O
data O
in O
low O
- O
resource O
languages O
such O
as O
Hindi O
, O
we O
use O
data O
recasting O
to O
create O
four O
NLI O
datasets O
from O
existing O
four O
text O
classification O
datasets O
in O
Hindi O
language O
. O
Through O
experiments O
, O
we O
show O
that O
our O
recasted O
dataset1is O
devoid O
of O
statistical O
irregularities O
and O
spurious O
patterns O
. O
W O
e O
study O
the O
consistency O
in O
predictions O
of O
the O
textual O
entailment O
models O
and O
propose O
a O
consistency O
regulariser O
to O
remove O
pairwise O
- O
inconsistencies O
in O
predictions O
. O
F O
urthermore O
, O
we O
propose O
a O
novel O
two O
- O
step O
classification O
method O
which O
uses O
textual O
- O
entailment O
predictions O
for O
classification O
task O
. O
W O
e O
further O
improve O
the O
classification O
performance O
by O
jointly O
training O
the O
classification O
and O
textual O
entailment O
tasks O
together O
. O
W O
e O
therefore O
highlight O
the O
benefits O
of O
data O
recasting O
and O
our O
approach2 O
with O
supporting O
experimental O
results O
. O
1 O
Introduction O
T O
extual O
entailment O
( O
TE O
) O
is O
the O
task O
of O
determining O
if O
a O
hypothesis O
sentence O
can O
be O
inferred O
from O
a O
given O
context O
sentence O
. O
Figure O
1shows O
examples O
of O
context O
- O
hypothesis O
pairs O
for O
TE O
. O
Previous O
works O
( O
W O
ang O
and O
Zhang O
, O
2009 O
; O
T O
atu O
and O
Moldovan O
, O
2005 O
; O
Sammons O
et O
al O
. O
, O
2010 O
) O
investigated O
several O
semantic O
approaches O
for O
TE O
and O
demonstrated O
how O
they O
can O
be O
used O
to O
evaluate O
inference O
- O
related O
tasks O
such O
as O
Ques1https://github.com/midas-research/ O
hindi O
- O
nli O
- O
data O
2https://github.com/midas-research/ O
hindi O
- O
nli O
- O
codetion O
Answering O
( O
QA O
) O
, O
reading O
comprehension O
( O
RC O
) O
and O
paraphrase O
acquisition O
( O
PA O
) O
. O
Context O
- O
Hypothesis O
Label O
p O
: O
The O
kid O
exclaimed O
with O
joy O
. O
entailed O
h O
: O
The O
kid O
is O
happy O
. O
p O
: O
I O
am O
feeling O
happy O
. O
not O
- O
entailed O
h O
: O
I O
am O
angry O
. O
( O
contradictory O
) O
T O
able O
1 O
: O
Example O
illustrating O
context O
( O
c O
) O
- O
hypothesis O
( O
h O
) O
pairs O
for O
the O
task O
of O
textual O
entailment O
. O
Researchers O
have O
curated O
many O
resources3 O
and O
benchmark O
datasets O
for O
TE O
in O
English O
( O
Bowman O
et O
al O
. O
, O
2015 O
; O
Williams O
et O
al O
. O
, O
2018 O
; O
Khot O
et O
al O
. O
, O
2018 O
) O
. O
However O
, O
to O
our O
knowledge O
, O
there O
is O
only O
one O
TE O
dataset O
( O
XNLI O
) O
in O
Hindi O
, O
which O
was O
created O
by O
translating O
English O
data O
( O
Conneau O
et O
al O
. O
, O
2018 O
) O
and O
another O
in O
HindiEnglish O
code O
- O
switched O
setting O
( O
Khanuja O
et O
al O
. O
, O
2020 O
) O
. O
Hindi O
is O
the O
language O
with O
the O
fourth O
most O
native O
speakers O
in O
the O
world4 O
. O
Despite O
its O
wide O
prevalence O
, O
Hindi O
is O
still O
considered O
a O
low O
- O
resource O
language O
by O
NLP O
practitioners O
because O
there O
are O
a O
rather O
limited O
number O
of O
publicly O
available O
annotated O
datasets O
. O
Developing O
models O
that O
can O
accurately O
process O
text O
from O
low O
- O
resource O
languages O
, O
such O
as O
Hindi O
, O
is O
critical O
for O
the O
proliferation O
and O
broader O
adoption O
of O
NLP O
technologies O
. O
Creating O
a O
high O
- O
quality O
labeled O
corpus O
for O
TE O
in O
Hindi O
through O
crowd O
- O
sourcing O
could O
be O
challenging O
. O
In O
this O
paper O
, O
we O
employ O
a O
recasting O
technique O
from O
Poliak O
et O
al O
. O
( O
2018a O
, O
b O
) O
to O
convert O
four O
publicly O
available O
text O
classification O
datasets O
in O
Hindi O
and O
pose O
them O
as O
TE O
problems O
. O
In O
this O
recasting O
process O
, O
we O
build O
template O
hypotheses O
for O
each O
class O
in O
the O
label O
taxonomy O
. O
Then O
, O
we O
pair O
the O
original O
anno3https://aclweb.org/aclwiki/Textual O
_ O
Entailment_Resource_Pool O
4https://en.wikipedia.org/wiki/List_of O
_ O
languages_by_number_of_native_speakers706tated O
sentence O
with O
each O
of O
the O
template O
hypotheses O
to O
create O
TE O
samples O
. O
Unlike O
XNLI O
, O
our O
dataset O
is O
based O
on O
the O
original O
Hindi O
text O
and O
is O
not O
translated O
. O
F O
urthermore O
, O
the O
multiple O
annotation O
artefacts O
( O
T O
an O
et O
al O
. O
, O
2019 O
) O
present O
in O
the O
original O
classification O
data O
are O
leveled O
out O
for O
the O
T O
extual O
entailment O
task O
on O
the O
recasted O
data O
due O
to O
label O
balance5 O
. O
W O
e O
evaluated O
state O
- O
of O
- O
the O
- O
art O
language O
models O
( O
Conneau O
et O
al O
. O
, O
2019 O
) O
performance O
on O
the O
recasted O
TE O
data O
. O
W O
e O
then O
combine O
the O
predictions O
of O
related O
pairs O
( O
same O
premise O
) O
from O
TE O
task O
to O
predict O
the O
classification O
labels O
of O
the O
original O
data O
( O
premise O
sentence O
) O
, O
a O
twostep O
classification O
. O
W O
e O
observed O
that O
a O
better O
TE O
performance O
on O
the O
recasted O
data O
leads O
to O
higher O
accuracy O
on O
the O
followed O
classification O
task O
. O
W O
e O
also O
observed O
that O
TE O
models O
can O
make O
inconsistent O
predictions O
across O
samples O
derived O
from O
the O
same O
context O
sentence O
. O
Driven O
by O
these O
observations O
, O
we O
propose O
two O
improvements O
to O
TE O
and O
classification O
modeling O
. O
First O
, O
we O
introduce O
a O
regularisation O
constraint O
based O
on O
the O
work O
of O
( O
Li O
et O
al O
. O
, O
2019 O
) O
that O
enforces O
consistency O
across O
pairs O
of O
training O
samples O
, O
thus O
correcting O
inconsistent O
predictions O
. O
Second O
, O
we O
propose O
a O
joint O
objective O
for O
training O
TE O
and O
classification O
simultaneously O
. O
Our O
results O
demonstrate O
that O
the O
regularization O
constraint O
and O
joint O
training O
helps O
improve O
the O
performance O
of O
both O
the O
TE O
models O
and O
the O
followed O
classification O
task O
. O
Though O
our O
work O
demonstrates O
the O
use O
of O
recasting O
and O
modeling O
improvements O
for O
TE O
in O
Hindi O
, O
we O
expect O
these O
techniques O
can O
be O
applied O
to O
other O
low O
- O
resource O
languages O
and O
other O
semantic O
phenomenon O
beyond O
textual O
classification O
. O
F O
ollowing O
are O
the O
main O
contributions O
of O
this O
work O
: O
1 O
. O
W O
e O
develop O
new O
NLI O
datasets O
for O
a O
lowresource O
language O
Hindi O
using O
recasting O
( O
Section O
3 O
) O
and O
evaluated O
state O
- O
of O
- O
the O
- O
art O
language O
models O
on O
them O
( O
Section O
4.1 O
) O
. O
2 O
. O
Based O
on O
our O
analysis O
of O
inconsistencies O
in O
the O
predictions O
of O
TE O
models O
, O
we O
propose O
a O
new O
regularisation O
constraint O
( O
Section O
4.1.1 O
) O
. O
5See O
Appendix O
Section O
A.4 O
for O
other O
benefits O
of O
recasting O
data.3 O
. O
W O
e O
propose O
a O
two O
- O
step O
classification O
approach O
that O
uses O
TE O
predictions O
from O
context O
- O
hypothesis O
pairs O
to O
predict O
the O
labels O
of O
the O
original O
classification O
task O
( O
Section O
4.2 O
) O
. O
4 O
. O
W O
e O
propose O
a O
novel O
joint O
- O
training O
objective O
paired O
with O
consistency O
regularisation O
to O
obtain O
state O
- O
of O
- O
the O
- O
art O
performance O
for O
text O
classification O
on O
four O
Hindi O
datasets O
( O
Section O
4.2.1 O
) O
. O
2 O
Related O
W O
ork O
In O
this O
section O
, O
we O
list O
some O
of O
the O
related O
works O
in O
the O
field O
of O
NLI O
as O
well O
as O
challenges O
encountered O
in O
low O
- O
resource O
settings O
. O
2.1 O
Natural O
Language O
Inference O
Recent O
studies O
in O
the O
field O
of O
NLI O
have O
emphasized O
the O
role O
of O
TE O
for O
estimating O
language O
comprehensibility O
of O
the O
models O
. O
White O
et O
al O
. O
( O
2017 O
) O
takes O
into O
consideration O
the O
need O
to O
leverage O
the O
existing O
pool O
of O
annotated O
collections O
as O
targeted O
textual O
inference O
examples O
( O
such O
as O
pronoun O
resolution O
and O
sentence O
paraphrasing O
) O
. O
Poliak O
et O
al O
. O
( O
2018b O
) O
discussed O
existing O
biases O
in O
NLI O
datasets O
which O
helps O
the O
models O
to O
perform O
well O
on O
Hypothesisonly O
baselines O
. O
Poliak O
et O
al O
. O
( O
2018a O
) O
analysed O
NLI O
datasets O
based O
on O
various O
semantic O
phenomenon O
to O
verify O
the O
ability O
of O
a O
model O
to O
perform O
unique O
, O
varied O
levels O
of O
reasoning O
. O
It O
performs O
data O
recasting O
on O
existing O
classification O
datasets O
to O
obtain O
a O
conventional O
context O
/ O
hypothesis O
/ O
label O
for O
common O
NLI O
tasks O
. O
Several O
modifications O
have O
been O
tried O
over O
baseline O
models O
for O
enhanced O
NLI O
and O
NLU O
. O
Liu O
et O
al O
. O
( O
2019 O
) O
focuses O
on O
NLU O
over O
crosstask O
data O
to O
achieve O
generalisability O
over O
new O
unseen O
tasks O
. O
Li O
et O
al O
. O
( O
2018 O
) O
incorporates O
attention O
mechanism O
to O
capture O
semantic O
relations O
in O
between O
individual O
words O
of O
the O
sentence O
for O
robust O
encodings O
. O
However O
, O
NLI O
has O
mostly O
revolved O
around O
English O
language O
. O
Our O
approach O
is O
motivated O
by O
such O
studies O
to O
analyse O
NLU O
using O
current O
embeddings O
for O
low O
- O
resource O
languages O
likeHindi O
. O
Bhattacharyya O
( O
2012 O
) O
discusses O
some O
of O
the O
key O
challenges O
associated O
with O
Hindi O
, O
for O
example O
, O
grammatical O
constraints O
for O
most O
words O
to O
be O
masculine O
/ O
feminine O
( O
similar O
to O
F O
rench O
and O
unlike O
English O
) O
, O
which O
makes707semantic O
tasks O
like O
pronoun O
resolution O
, O
paraphrasing O
tough O
. O
2.2 O
NLP O
for O
Low O
- O
Resource O
Languages O
In O
a O
plethora O
of O
diverse O
languages O
, O
only O
a O
handful O
of O
them O
have O
plenty O
of O
labeled O
resources O
for O
data O
- O
driven O
analysis O
and O
advancements O
( O
Joshi O
et O
al O
. O
, O
2020 O
) O
. O
Data O
in O
low O
- O
resource O
languages O
is O
either O
unlabeled O
or O
resides O
in O
spoken O
dialect O
than O
texts O
. O
There O
have O
been O
recent O
efforts O
using O
curriculum O
learning O
for O
making O
pretrained O
language O
models O
for O
several O
multi O
- O
lingual O
tasks O
( O
Conneau O
et O
al O
. O
, O
2018 O
, O
2019 O
) O
. O
However O
, O
many O
such O
languages O
give O
rise O
to O
creoles O
, O
building O
new O
mixed O
languages O
at O
the O
interface O
of O
existing O
languages O
. O
One O
such O
example O
is O
Hinglish O
( O
Hindi O
+ O
English O
) O
that O
has O
widely O
been O
taken O
over O
in O
the O
form O
of O
tweets O
and O
social O
media O
messages O
. O
Attempts O
have O
been O
made O
to O
study O
linguistic O
tasks O
like O
language O
identification O
, O
NER O
( O
Singh O
et O
al O
. O
, O
2018 O
) O
and O
detection O
of O
hate O
speech O
from O
social O
media O
( O
Mathur O
et O
al O
. O
, O
2018 O
) O
. O
( O
Sitaram O
et O
al O
. O
, O
2019 O
) O
looks O
at O
the O
challenges O
and O
opportunities O
of O
code O
- O
switching O
. O
Joshi O
et O
al O
. O
( O
2019 O
) O
compares O
the O
current O
deep O
learning O
methods O
for O
classification O
tasks O
in O
Hindi O
and O
concludes O
the O
need O
of O
more O
efficient O
models O
for O
the O
same O
. O
Apart O
from O
that O
, O
low O
- O
resource O
languages O
also O
challenge O
us O
to O
shift O
from O
data O
- O
driven O
modelling O
to O
intelligent O
neural O
modelling O
. O
This O
improves O
language O
understanding O
from O
limited O
available O
data O
and O
also O
diminishes O
the O
need O
of O
hand O
- O
engineered O
feature O
representations O
similar O
to O
generative O
modelling O
. O
Some O
such O
efforts O
have O
been O
put O
forth O
by O
Kumar O
et O
al O
. O
( O
2019 O
) O
and O
Akhtar O
et O
al O
. O
( O
2016 O
) O
. O
Keeping O
these O
challenges O
in O
mind O
, O
this O
work O
is O
a O
step O
towards O
understanding O
of O
a O
lowresource O
language O
- O
Hindi O
using O
TE O
. O
3 O
Recasting O
Classification O
Datasets O
One O
of O
the O
main O
challenges O
for O
TE O
evaluation O
for O
low O
- O
resource O
languages O
is O
the O
lack O
of O
labeled O
data O
. O
In O
this O
work O
, O
we O
employ O
recasting O
to O
convert O
annotated O
classification O
datasets O
in O
Hindi O
to O
labeled O
TE O
samples O
. O
As O
in O
( O
Poliak O
et O
al O
. O
, O
2018a O
) O
, O
we O
selected O
four O
different O
datasets O
for O
recasting O
thus O
introducing O
linguistic O
diversity O
in O
the O
resulting O
TE O
dataset O
. O
Product O
Review O
- O
The O
first O
dataset O
( O
PR O
) O
contains O
5,417 O
samples O
of O
online O
user O
reviewsin O
Hindi O
for O
different O
products O
( O
Akhtar O
et O
al O
. O
, O
2016 O
) O
. O
These O
samples O
were O
annotated O
into O
one O
of O
the O
following O
four O
sentiment O
classes O
: O
positive O
, O
negative O
, O
neutral O
, O
andconflict O
. O
F O
or O
recasting O
the O
samples O
in O
this O
dataset O
, O
we O
first O
built O
8 O
hypothesis O
templates O
: O
2 O
per O
class O
label O
. O
F O
or O
each O
label O
, O
we O
create O
one O
positive O
and O
one O
negative O
hypothesis O
which O
roughly O
translate O
to:‚ÄòThis O
product O
got O
< O
label O
> O
reviews O
‚Äô O
and O
‚Äò O
This O
product O
did O
not O
get O
< O
label O
> O
reviews O
‚Äô O
. O
Given O
a O
sample O
from O
the O
PR O
dataset O
, O
we O
treat O
it O
as O
the O
context O
sentence O
and O
combine O
with O
the O
8 O
hypotheses O
sentences O
to O
create O
NLI O
samples O
. O
If O
the O
< O
label O
> O
of O
the O
premise O
matches O
that O
of O
the O
positive O
hypothesis O
, O
then O
the O
NLI O
sample O
is O
marked O
as O
‚Äò O
entailed O
‚Äô O
. O
Likewise O
, O
if O
the O
< O
label O
> O
of O
the O
premise O
does O
not O
match O
the O
negative O
hypothesis O
, O
then O
the O
NLI O
sample O
is O
also O
marked O
as O
‚Äò O
entailed O
‚Äô O
. O
F O
or O
the O
remaining O
cases O
, O
the O
sample O
is O
marked O
as O
‚Äò O
non O
- O
entailed O
‚Äô O
. O
This O
process O
is O
summarized O
with O
an O
example O
in O
Figure O
1 O
. O
F O
or O
more O
detailed O
recasting O
illustration O
, O
see O
Appendix O
Section O
A.1 O
Figure O
5 O
. O
BHAA O
V O
- O
The O
second O
dataset O
BHAA O
V O
( O
BH O
) O
( O
Kumar O
et O
al O
. O
, O
2019 O
) O
contains O
20,304 O
sentences O
from O
Hindi O
short O
stories O
annotated O
for O
one O
of O
the O
following O
five O
emotion O
categories O
: O
joy O
, O
anger O
, O
suspense O
, O
sad O
, O
andneutral O
. O
W O
e O
used O
a O
similar O
process O
as O
PR O
to O
recast O
BH O
using O
the O
following O
templates O
to O
create O
the O
hypothesis O
: O
‚Äò O
It O
is O
a O
matter O
of O
great O
< O
label O
> O
‚Äô O
and‚ÄòIt O
is O
not O
a O
matter O
of O
great O
< O
label O
> O
‚Äô O
. O
Hindi O
Discourse O
Modes O
Dataset O
( O
HDA O
) O
- O
This O
dataset O
( O
Dhanwal O
et O
al O
. O
, O
2020 O
) O
consists O
of10,472 O
sentences O
from O
Hindi O
short O
stories O
annotated O
for O
five O
different O
discourse O
modes O
argumentative O
, O
narrative O
, O
descriptive O
, O
dialogic O
and O
informative O
. O
Hindi O
BBC O
News O
Dataset O
( O
BBC O
) O
- O
This O
dataset6contains O
4,335 O
Hindi O
news O
headlines O
tagged O
across O
14 O
categories O
: O
India O
, O
Pakistan O
, O
news O
, O
International O
, O
entertainment O
, O
sport O
, O
science O
, O
China O
, O
learning O
english O
, O
social O
, O
southasia O
, O
business O
, O
institutional O
, O
multimedia O
. O
W O
e O
processed O
this O
dataset O
to O
combine O
two O
sets O
of O
relevant O
but O
low O
prevalence O
classes O
. O
Namely O
, O
we O
merged O
the O
samples O
from O
Pakistan O
, O
China O
, O
international O
, O
andsouthasia O
as O
one O
class O
called O
6https://tinyurl.com/y8hxtbn8708 O
    O
RECASTED O
NLI O
DATASET O
  O
c1 O
: O
Has O
good O
streaming O
quality O
. O
c1 O
‚Äô O
: O
Has O
good O
streaming O
quality O
. O
  O
h1 O
: O
The O
product O
got O
positive O
h1 O
‚Äô O
: O
The O
product O
did O
not O
get O
positive O
         O
reviews O
from O
its O
users O
. O
               O
reviews O
from O
its O
users O
. O
  O
TE O
label O
: O
entailed O
TE O
label O
: O
not O
- O
entailed O
  O
c2 O
: O
Has O
good O
streaming O
quality O
. O
c2 O
‚Äô O
: O
Has O
good O
streaming O
quality O
. O
  O
h2 O
: O
The O
product O
got O
negative O
h2 O
‚Äô O
: O
The O
product O
did O
not O
get O
negative O
         O
reviews O
from O
its O
users O
. O
               O
reviews O
from O
its O
users O
. O
  O
TE O
label O
: O
not O
- O
entailed O
TE O
label O
: O
entailed O
  O
c3 O
: O
Has O
good O
streaming O
quality O
. O
c3 O
‚Äô O
: O
Has O
good O
streaming O
quality O
. O
  O
h3 O
: O
The O
product O
got O
neutral O
h3 O
‚Äô O
: O
The O
product O
did O
not O
get O
neutral O
         O
reviews O
from O
its O
users O
. O
               O
reviews O
from O
its O
users O
. O
  O
TE O
label O
: O
not O
- O
entailed O
TE O
label O
: O
entailed O
  O
c4 O
Has O
good O
streaming O
quality O
. O
c4 O
‚Äô O
: O
Has O
good O
streaming O
quality O
. O
  O
h4 O
: O
The O
product O
got O
conÔ¨Çicting O
h4 O
‚Äô O
: O
The O
product O
did O
not O
get O
conÔ¨Çicting O
         O
reviews O
from O
its O
users O
. O
               O
reviews O
from O
its O
users O
. O
  O
TE O
label O
: O
not O
- O
entailed O
TE O
label O
: O
entailed O
    O
ORIGINAL O
DATASET O
  O
Sentence O
: O
Has O
good O
streaming O
quality O
. O
  O
Annotation O
: O
Positive O
  O
Set O
of O
classes O
: O
Positive O
, O
Negative O
, O
Neutral O
, O
ConÔ¨Çict O
  O
CLASSIFICATION O
Recasting O
Textual O
  O
Entailment O
  O
Model O
p1 O
p1 O
‚Äô O
p2 O
p2 O
‚Äô O
p3 O
p3 O
‚Äô O
p4 O
p4 O
‚Äô O
Entailment O
  O
Vector O
ClassiÔ¨Åer O
1 O
- O
p1 O
  O
1 O
- O
p2 O
  O
1 O
- O
p3 O
  O
1 O
- O
p4 O
CONSTRAINT O
  O
REGULARISATION O
     O
c O
: O
Context O
          O
h O
: O
Hypothesis O
         O
TE O
: O
Textual O
Entailment O
  O
DIRECT O
CLASSIFICATION O
TWO O
- O
STEP O
CLASSIFICATION O
           O
Entailment O
probability O
> O
= O
0.5 O
                     O
Entailment O
probability O
< O
0.5 O
Figure O
1 O
: O
Illustration O
of O
the O
proposed O
approach O
international O
. O
Likewise O
, O
we O
also O
merged O
samples O
from O
news O
, O
business O
, O
social O
, O
learning O
english O
, O
andinstitutional O
as O
news O
. O
Lastly O
, O
we O
also O
removed O
the O
class O
multimedia O
because O
there O
were O
very O
few O
samples O
. O
T O
able O
2shows O
statistics O
about O
the O
datasets O
and O
T O
able O
3shows O
examples O
from O
each O
. O
Datasets O
PR O
BH O
HDA O
BBC O
Original O
datasets O
# O
Classes O
4 O
5 O
5 O
6 O
# O
T O
rain O
4334 O
16243 O
8377 O
3889 O
# O
Dev O
541 O
2030 O
1047 O
216 O
# O
T O
est O
542 O
2031 O
1048 O
217 O
Recasted O
TE O
data O
# O
Classes O
2 O
2 O
2 O
2 O
# O
T O
rain O
17336 O
64972 O
33508 O
15556 O
# O
Dev O
4328 O
20300 O
10470 O
2592 O
# O
T O
est O
4336 O
20310 O
10480 O
2604 O
T O
able O
2 O
: O
Statistics O
of O
the O
original O
classification O
data O
and O
recasted O
NLI O
data O
. O
4 O
Methodology O
Our O
objective O
in O
this O
paper O
is O
not O
only O
to O
use O
recasting O
to O
create O
a O
NLI O
dataset O
in O
low O
- O
resource O
settings O
but O
also O
to O
understand O
how O
different O
models O
are O
effective O
in O
both O
TE O
and O
classification O
task O
. O
F O
urthermore O
, O
we O
also O
discuss O
our O
novel O
two O
- O
step O
classification O
technique O
with O
joint O
objective O
and O
regularization O
constraints O
. O
4.1 O
T O
extual O
Entailment O
One O
straightforward O
application O
of O
NLI O
comes O
with O
evaluating O
the O
task O
of O
T O
extual O
Entailment(TE O
) O
. O
It O
analyses O
if O
the O
TE O
model O
can O
draw O
reasonable O
inferences O
from O
the O
context O
to O
hypothesise O
over O
other O
related O
/ O
unrelated O
data O
, O
as O
shown O
in O
T O
able O
1 O
. O
However O
, O
apart O
from O
being O
correct O
/ O
incorrect O
, O
certain O
times O
, O
TE O
models O
are O
not O
always O
consistent O
with O
their O
own O
beliefs O
( O
Li O
et O
al O
. O
, O
2019 O
) O
due O
to O
spurious O
patterns O
in O
the O
dataset O
( O
Poliak O
et O
al O
. O
, O
2018a O
) O
. O
Consider O
two O
context O
- O
hypothesis O
pairs O
P O
and O
P‚Ä≤generated O
from O
the O
same O
context O
sentence O
and O
opposing O
hypotheses O
statements O
( O
as O
illustrated O
in O
Figure O
1 O
) O
. O
Consequently O
, O
P O
and O
P‚Ä≤would O
have O
opposing O
TE O
labels O
. O
When O
a O
TE O
model O
makes O
predictions O
on O
these O
two O
pairs O
, O
there O
are O
three O
possibilities O
( O
T O
able O
5 O
) O
. O
The O
model O
can O
get O
both O
predictions O
right O
, O
in O
which O
case O
the O
predictions O
are O
consistent O
. O
It O
can O
also O
get O
both O
predictions O
wrong O
but O
still O
they O
are O
consistent O
. O
Lastly O
, O
it O
can O
get O
one O
of O
the O
predictions O
wrong O
, O
in O
which O
case O
they O
are O
inconsistent7 O
. O
T O
o O
mitigate O
this O
inconsistency O
problem O
, O
we O
propose O
consistency O
regularisation O
loss O
. O
4.1.1 O
Consistency O
Regularisation O
( O
CR O
) O
T O
o O
enforce O
this O
pairwise O
- O
consistency O
, O
we O
add O
a O
regularisation O
loss8 O
, O
inspired O
from O
( O
Li O
et O
al O
. O
, O
7See O
Appendix O
Section O
A.3 O
T O
able O
11 O
for O
additional O
inconsistency O
examples O
. O
8Other O
suitable O
loss O
function O
also O
works O
( O
Li O
et O
al O
. O
, O
2019 O
) O
.709Dataset O
Sentence O
( O
Hindi O
) O
Sentence O
( O
English O
) O
Sentiment O
PR O
—û‡§´‡§≤‡§π‡§æ‡§≤ O
, O
‡§á‡§∏‡§Æ”í O
‡§ï‡•ã‡§à O
‡§µ‡•Ä—û‡§°‡§Ø‡•ã O
‡§Ø‡§æ O
‡§µ‡•â‡§Ø‡§∏ O
‡§ï‡•â‡§≤ O
‡§∏‡§™‡•ã‡§ü O
À®‡§®‡§π“∞‡§Ç‡§π‡•à‡•§At O
the O
moment O
, O
there O
is O
no O
video O
or O
voice O
call O
support.negative O
BH O
‡§á‡§§‡§®‡•Ä O
—†‡§Æ‡§†‡§æ‡§á‡§Ø‡§æ O
‡§Å‡§≤“∞‡§Ç O
, O
‡§Æ‡•Å‡§ù‡•á—û‡§ï‡§∏‡•Ä O
‡§® O
‡•á‡§è‡§ï O
‡§≠‡•Ä O
‡§® O
‡§¶“∞‡•§T O
ook O
so O
many O
sweets O
, O
nobody O
gave O
me O
one.anger O
HDA O
‡§∏‡•å‡§∞ O
‡§Æ O
‡§Ç‡§°‡§≤ O
‡§ï O
‡•á‡§∏‡§æ‡§∞‡•á‡§Æ‡§π‡•á O
‡§¨‡•É‡§π‡§É‡§™‡§ø‡§§ O
‡§Æ”í O
‡§∏‡§Æ‡§æ O
‡§Ç O
‡§∏‡§ï‡§§‡•á O
‡§π O
”î|All O
the O
planets O
in O
the O
solar O
system O
can O
be O
contained O
within O
the O
Jupiter.informative O
BBC O
‡§Ö‡§ñ‡§¨‡§æ‡§∞ O
‡§® O
‡•á‡§¨‡§§‡§æ‡§Ø‡§æ O
—û‡§ï O
‡§´ O
‡•á O
‡§∏‡§¨ O
‡•Å‡§ï O
‡§™‡§∞ O
—†‡§Æ‡§≤ O
‡•á‡§ó‡•Ä O
‡§Ö‡§∏‡§≤ O
‡§ú‡§æ‡§¶ O
‡•Ç‡§ï“¥ O
‡§ùÕ©‡§™‡•Ä O
”î‡•§The O
newspaper O
said O
that O
real O
magic O
hug O
will O
be O
found O
on O
F O
acebook.entertainment O
T O
able O
3 O
: O
Sample O
sentences O
from O
the O
four O
datasets O
and O
the O
corresponding O
annotation O
labels O
. O
2019 O
) O
, O
for O
our O
settings O
, O
where O
the O
entailment O
probabilities O
pandp‚Ä≤of O
pairs O
P O
andP‚Ä≤respectively O
, O
is O
required O
to O
always O
sum O
up O
to O
one O
as O
illustrated O
in O
Figure O
1 O
. O
Mathematically O
, O
we O
define O
the O
regularisation O
term O
as O
depicted O
in O
Equation O
1 O
. O
Lreg=/vextenddouble O
/ O
vextenddoublep+p‚Ä≤‚àí1 O
/ O
vextenddouble O
/ O
vextenddouble2 O
2(1 O
) O
Our O
regularisation O
is O
different O
from O
( O
Li O
et O
al O
. O
, O
2019 O
) O
in O
terms O
of O
different O
consistency O
problem O
being O
considered O
, O
which O
in O
- O
term O
diversifies O
a O
very O
different O
inductive O
bias O
from O
former O
. O
4.2 O
T O
wo O
- O
step O
classification O
W O
e O
further O
extend O
the O
knowledge O
accumulated O
by O
TE O
predictions O
for O
multi O
- O
class O
classification O
. O
Consider O
a O
TE O
model O
with O
binary O
output O
where O
1 O
( O
entailed O
) O
represents O
entailed O
and O
0 O
( O
not O
- O
entailed O
) O
represents O
not O
- O
entailed O
. O
One O
can O
co O
- O
relate O
model O
predictions O
for O
related O
TE O
pairs O
with O
same O
context O
but O
different O
hypothesis O
during O
prediction O
( O
inference O
) O
to O
retrieve O
the O
classification O
label O
. O
This O
is O
depicted O
by O
an O
example O
in O
T O
able O
4 O
. O
W O
e O
call O
our O
approach O
a O
two O
- O
step O
classification O
method O
, O
where O
we O
obtain O
TE O
predictions O
in O
the O
first O
step O
and O
use O
them O
to O
obtain O
classification O
label O
in O
step O
two O
. O
F O
or O
demarcation O
, O
we O
refer O
to O
the O
straightforward O
task O
( O
without O
the O
recasted O
data O
) O
as O
direct O
classification O
. O
Therefore O
, O
a O
perfect O
TE O
model O
would O
lead O
to O
a O
100 O
% O
accuracy O
over O
the O
two O
- O
step O
classification O
task O
. O
However O
, O
having O
a O
completely O
accurate O
TE O
model O
is O
often O
a O
bottleneck O
due O
to O
inaccurate O
and O
inconsistent O
predictions O
. O
Here O
, O
inconsistency O
can O
even O
occur O
across O
pairs O
, O
for O
example O
, O
two O
different O
pairs O
can O
predict O
two O
different O
labels O
. O
So O
instead O
of O
binary O
outputs O
, O
we O
use O
soft O
TE O
probabilities O
( O
pi O
) O
of O
each O
context O
- O
hypothesis O
pair O
( O
ci O
- O
hi O
) O
and O
concatenate O
them O
together O
to O
form O
an O
entailment O
vector O
( O
E O
) O
, O
see O
Figure O
1 O
. O
The O
classifier O
C O
: O
E O
‚Üí O
   O
‡§≤ O
‡•á O
‡§ï‡§® O
       O
‡§ï‡§∞‡§§‡§æ O
          O
‡§∏‡§ï‡§§‡§æ O
        O
‡§¨‡•à‡§ü‡§∞‡•Ä O
         O
‡§π‡§æ‡§≤‡§æ‡§Å‡§ï O
    O
but O
         O
does O
           O
can O
         O
battery O
     O
however O
    O
‡§≤ O
‡•á O
‡§ï‡§® O
       O
‡§ï‡§∞‡§§‡§æ O
          O
‡§∏‡§ï‡§§‡§æ O
        O
‡§¨‡•à‡§ü‡§∞‡•Ä O
         O
‡§π‡§æ‡§≤‡§æ‡§Å‡§ï O
    O
but O
         O
does O
           O
can O
         O
battery O
     O
however O
Figure O
2 O
: O
Plot O
showing O
statistics O
of O
unigram O
patterns O
in O
PR O
dataset O
for O
train O
( O
top O
) O
and O
test O
( O
bottom O
) O
across O
different O
classes O
for O
some O
sentiment O
as O
well O
as O
non O
- O
sentiment O
keywords O
. O
The O
x O
- O
axis O
represents O
the O
keyword O
with O
the O
percentage O
of O
occurrence O
on O
the O
y O
- O
axis O
. O
Y O
, O
then O
takes O
as O
input O
the O
entailment O
vector O
( O
E O
) O
to O
retrieve O
the O
classification O
label O
( O
Y O
) O
. O
Here O
, O
the O
entailment O
vector O
works O
as O
an O
added O
weaker O
supervision O
at O
the O
group O
level O
( O
group O
of O
all O
recasted O
pairs O
for O
a O
given O
context O
) O
to O
the O
classifier O
. O
Thus O
the O
classifier O
identify O
the O
correct O
boundary O
for O
the O
final O
classification O
task O
. O
F O
urthermore O
, O
two O
- O
step O
classification O
adds O
an O
interpretable O
advantage O
over O
the O
direct O
classification O
. O
This O
is O
because O
, O
direct O
- O
classifcation O
is O
driven O
by O
a O
lot O
of O
spurious O
unigram O
patterns O
present O
in O
the O
original O
dataset O
. O
These O
patterns O
are O
leveled O
in O
the O
two O
- O
step O
classification O
approach O
due O
to O
the O
balanced O
set O
of O
text O
tokens710for O
both O
entailed O
and O
not O
- O
entailed O
pairs O
( O
both O
labels O
) O
with O
data O
recasting O
. O
Figure O
2shows O
some O
of O
the O
unigram O
statistics O
for O
PR O
dataset O
over O
some O
sentiment O
as O
well O
as O
non O
- O
sentiment O
words O
to O
depict O
the O
type O
of O
artefact O
patterns O
in O
the O
classification O
datasets O
, O
similar O
to O
( O
T O
an O
et O
al O
. O
, O
2019 O
) O
. O
These O
annotation O
aretefacts O
are O
nullified O
in O
the O
recasted O
TE O
task O
due O
to O
balanced O
label O
balanced O
for O
every O
premise O
tokens O
. O
4.2.1 O
Joint O
Objective O
( O
JO O
) O
One O
simple O
method O
for O
two O
- O
step O
classification O
is O
to O
first O
train O
a O
TE O
model O
and O
then O
train O
the O
classifier O
on O
its O
predictions O
. O
However O
, O
using O
a O
fixed O
TE O
model O
prediction O
imposes O
a O
prior O
bottleneck O
on O
the O
classification O
accuracy O
. O
Since O
both O
the O
tasks O
i.e. O
the O
TE O
and O
the O
follow O
- O
up O
classification O
, O
can O
influence O
each O
other O
, O
thus O
we O
propose O
a O
joint O
training O
objective O
as O
shown O
in O
Equation O
2 O
Ljoint O
= O
LT O
E+ŒªLclf O
( O
2 O
) O
where O
Œªis O
the O
weight O
of O
the O
follow O
- O
up O
classification O
loss O
, O
LT O
E O
andLclf O
are O
cross O
- O
entropy O
loss O
for O
the O
task O
of O
TE O
and O
classification O
respectively O
as O
defined O
in O
Equations O
3and O
4 O
. O
LT O
E=/summationdisplay O
km O
/ O
summationdisplay O
j=1‚àíptrue O
k O
, O
j O
logpk O
, O
j O
( O
3 O
) O
Lclf=/summationdisplay O
km O
/ O
summationdisplay O
j=1‚àíctrue O
k O
, O
j O
logck O
, O
j O
( O
4 O
) O
Here O
, O
m O
represents O
the O
total O
classes O
, O
ptrue O
k O
, O
j O
andctrue O
k O
, O
jrepresent O
the O
binary O
label O
of O
sample O
k O
to O
belong O
to O
class O
j O
, O
and O
pk O
, O
j O
andck O
, O
j O
represent O
the O
probability O
of O
predicted O
label O
for O
sample O
k O
to O
be O
class O
j. O
Benefit O
of O
Joint O
Objective O
. O
Satisfying O
the O
joint O
objective O
not O
only O
ensures O
that O
the O
model O
predictions O
are O
correct O
but O
also O
ensures O
that O
they O
are O
correct O
for O
the O
right O
reasons O
. O
The O
true O
classification O
label O
can O
be O
retrieved O
from O
the O
entailment O
vector O
only O
when O
the O
model O
draws O
necessary O
inferences O
correctly O
. O
Otherwise O
the O
multi O
- O
class O
classification O
would O
fail O
. O
F O
urthermore O
, O
combining O
the O
joint O
objective O
( O
Equation O
2 O
) O
with O
consistency O
regulariser O
( O
Equation O
1 O
) O
for O
the O
intermediate O
TE O
prediction O
further O
force O
pairwise O
- O
consistency O
between O
prediction O
of O
related O
TE O
pairs O
. O
Context O
sentence O
: O
He O
cried O
over O
his O
lost O
pet O
. O
Hypotheses O
TE O
Prediction O
1 O
. O
He O
is O
happy O
. O
not O
- O
entailed O
2 O
. O
He O
is O
not O
happy O
. O
entailed O
3 O
. O
He O
is O
angry O
. O
not O
- O
entailed O
4 O
. O
He O
is O
not O
angry O
. O
entailed O
5 O
. O
He O
is O
sad O
. O
entailed O
6 O
. O
He O
is O
not O
sad O
. O
not O
- O
entailed O
Inferred O
label O
: O
Sad O
T O
able O
4 O
: O
An O
example O
demonstrating O
inference O
of O
the O
label O
for O
the O
original O
classification O
task O
based O
on O
predictions O
from O
TE O
model O
. O
5 O
Experiments O
Most O
of O
the O
sentence O
embedding O
models O
have O
been O
designed O
and O
evaluated O
to O
perform O
well O
onEnglish O
language O
. O
The O
experiments O
in O
this O
work O
are O
motivated O
to O
answer O
the O
following O
questions O
for O
a O
low O
- O
resource O
language O
, O
Hindi O
: O
‚Ä¢ O
Are O
these O
representations O
effective O
to O
derive O
logical O
entailment O
in O
contexthypothesis O
pairs O
on O
recasted O
data O
? O
. O
F O
urthermore O
, O
how O
consistent O
/ O
inconsistent O
are O
such O
models O
with O
their O
own O
decisions O
? O
Also O
, O
does O
consistency O
regulariser O
help O
to O
mitigate O
model O
inconsistency O
? O
‚Ä¢ O
Do O
sentence O
representation O
models O
work O
well O
for O
direct O
classification O
? O
Can O
models O
trained O
on O
recasted O
NLI O
data O
be O
used O
to O
retrieve O
ground O
truth O
classification O
annotations O
using O
two O
- O
step O
classification O
? O
Does O
our O
joint O
training O
objective O
with O
consistency O
regularization O
improve O
performance O
? O
Baselines O
- O
F O
or O
evaluating O
our O
approach O
, O
we O
use O
the O
following O
baselines O
: O
InferSent O
( O
Conneau O
et O
al O
. O
, O
2017 O
) O
, O
Sent2V O
ec O
( O
Pagliardini O
et O
al O
. O
, O
2018 O
) O
, O
Bag O
- O
of O
- O
words O
( O
BoW O
) O
and O
XLMRoBER O
T O
a O
( O
Conneau O
et O
al O
. O
, O
2019 O
) O
which O
is O
state O
- O
of O
- O
the O
- O
art O
for O
multilingual O
language O
modelling O
. O
Also O
, O
we O
evaluate O
a O
hypothesis O
- O
only O
analogue O
for O
each O
one O
of O
them O
as O
well O
. O
F O
or O
experiments O
with O
recasted O
data O
, O
we O
use O
embeddings O
of O
context O
- O
hypothesis O
pair O
for O
baselines O
whereas O
for O
the O
hypothesis O
- O
only O
( O
Poliak O
et O
al O
. O
, O
2018b O
) O
models O
, O
we O
only O
use O
embeddings O
of O
thehypothesis O
sentence O
, O
keeping O
it O
blind O
to O
thecontext O
. O
Hypothesis O
only O
Baselines O
- O
Evaluating O
hypothesis O
- O
only O
models O
is O
motivated O
by O
irregularities O
and O
biases O
presented O
in O
entailment711Context O
( O
Hindi O
): O
‡§µ‡§π O
‡§∞‡•ã‡§Ø‡§æ O
‡§ú‡§¨ O
‡§â‡§∏‡§® O
‡•á‡§Ö‡§™‡§®‡§æ O
‡§™‡§æ‡§≤‡§§ O
‡•Ç‡§ñ‡•ã O
—û‡§¶‡§Ø‡§æ O
Emotion O
class O
( O
Hindi O
): O
‡§¶‡•Å‡§ñ O
( O
English O
): O
He O
cried O
over O
his O
lost O
pet O
. O
( O
English O
): O
Sad O
Hypothesis O
( O
Hindi O
) O
Hypothesis O
( O
English O
) O
TE O
label O
Consistency O
Prediction O
h1 O
: O
‡§µ‡§π O
‡§ñ O
‡•Å‡§∂ O
‡§π‡•à O
h1 O
: O
He O
is O
happy O
. O
not O
- O
entailedConsistentCorrect O
h1‚Ä≤:‡§µ‡§π O
‡§ñ O
‡•Å‡§∂ O
‡§®‡§π“∞ O
‡§Ç‡§π‡•à O
h1‚Ä≤:He O
is O
not O
happy O
. O
entailed O
Correct O
h1 O
: O
‡§µ‡§π O
‡§ñ O
‡•Å‡§∂ O
‡§π‡•à O
h1 O
: O
He O
is O
happy O
. O
not O
- O
entailedInconsistentCorrect O
h1‚Ä≤:‡§µ‡§π O
‡§ñ O
‡•Å‡§∂ O
‡§®‡§π“∞ O
‡§Ç‡§π‡•à O
h1‚Ä≤:He O
is O
not O
happy O
. O
not O
- O
entailed O
Incorrect O
h1 O
: O
‡§µ‡§π O
‡§ñ O
‡•Å‡§∂ O
‡§π‡•à O
h1 O
: O
He O
is O
happy O
. O
entailedInconsistentIncorrect O
h1‚Ä≤:‡§µ‡§π O
‡§ñ O
‡•Å‡§∂ O
‡§®‡§π“∞ O
‡§Ç‡§π‡•à O
h1‚Ä≤:He O
is O
not O
happy O
. O
entailed O
Correct O
h1 O
: O
‡§µ‡§π O
‡§ñ O
‡•Å‡§∂ O
‡§π‡•à O
h1 O
: O
He O
is O
happy O
. O
entailedConsistentIncorrect O
h1‚Ä≤:‡§µ‡§π O
‡§ñ O
‡•Å‡§∂ O
‡§®‡§π“∞ O
‡§Ç‡§π‡•à O
h1‚Ä≤:He O
is O
not O
happy O
. O
not O
- O
entailed O
Incorrect O
T O
able O
5 O
: O
A O
simple O
example O
illustrating O
the O
concept O
of O
consistency O
in O
model O
prediction O
for O
TE O
task O
for O
the O
task O
of O
emotion O
analysis O
. O
datasets O
. O
Such O
biases O
often O
lead O
to O
high O
performance O
over O
NLI O
tasks O
without O
completely O
comprehending O
the O
semantic O
reasonings O
in O
data O
and O
language O
. O
When O
the O
accuracy O
of O
a O
hypothesis O
- O
only O
model O
is O
much O
lower O
than O
the O
baseline O
and O
closer O
to O
random O
( O
50 O
% O
) O
, O
it O
exhibits O
that O
learning O
is O
not O
boosted O
due O
to O
statistical O
irregularities O
in O
data O
such O
as O
word O
count O
, O
unigram O
/ O
bi O
- O
gram O
pattern O
or O
any O
other O
spurious O
pattern O
( O
artefacts O
) O
. O
W O
e O
achieve O
this O
using O
our O
approach O
since O
recasting O
ensures O
label O
balance O
for O
the O
augmentations O
of O
each O
class O
label O
for O
every O
sentence O
and O
its O
tokens O
. O
Experimental O
Settings O
- O
F O
or O
each O
of O
the O
models O
, O
we O
use O
the O
initial O
learning O
rate O
1 O
x O
10‚àí3and O
a O
decay O
rate O
of O
0.9 O
, O
using O
Adam O
optimizer O
with O
the O
embedding O
dimension O
kept O
as O
1024 O
for O
all O
the O
models O
. O
F O
or O
all O
the O
experiments O
associated O
with O
XLM O
- O
RoBER O
T O
a O
, O
W O
e O
use O
XLM O
- O
RoBER O
T O
a O
large O
with O
1024 O
- O
hidden O
. O
F O
or O
InferSent O
and O
Sent2V O
ec O
we O
use O
the O
default O
parameter O
for O
NLI O
model O
architecture O
as O
stated O
in O
the O
paper O
. O
F O
or O
hypothesis O
only O
baseline O
we O
use O
the O
single O
sent O
model O
of O
XLMRoBER O
T O
a O
, O
InferSent O
and O
Sent2V O
ec O
as O
reported O
in O
paper O
for O
binary O
classification O
. O
After O
the O
embeddings O
are O
obtained O
, O
we O
use O
an O
MLP O
classifier O
for O
performing O
all O
the O
classification O
experiments O
. O
F O
or O
a O
hypothesis O
- O
only O
baseline O
, O
only O
the O
hypothesis O
embedding O
is O
passed O
as O
an O
input O
to O
the O
MLP O
, O
whereas O
for O
a O
premise O
- O
hypothesis O
baseline O
, O
we O
concatenate O
the O
embeddings O
of O
premise O
, O
hypothesis O
, O
as O
well O
as O
their O
element O
- O
wise O
product O
and O
elementwise O
subtraction O
. O
F O
or O
the O
joint O
objective O
training O
( O
see O
Eq O
. O
2 O
) O
, O
we O
use O
Œª=2.0 O
. O
W O
e O
train O
our O
model O
for O
15 O
epochs O
on O
a O
machine O
with O
GeF O
orce O
R O
TX O
2080 O
GPU O
using O
the O
PyT O
orch O
framework.5.1 O
T O
extual O
Entailment O
Results O
F O
or O
all O
four O
semantic O
phenomenon O
considered O
, O
we O
use O
recasted O
data O
to O
predict O
the O
performance O
on O
textual O
entailment O
task O
. O
While O
training O
, O
we O
use O
four O
context O
- O
hypothesis O
pairs O
- O
with O
hypothesis O
having O
true O
classification O
label O
, O
its O
negation O
( O
hypothesis O
5 O
and O
6 O
in O
T O
able O
4 O
) O
, O
a O
random O
label O
from O
the O
remaining O
classes O
and O
its O
negation O
( O
hypothesis O
1 O
and O
2 O
in O
T O
able O
4 O
) O
. O
This O
ensures O
that O
neither O
original O
classification O
label O
nor O
the O
negation O
( O
we O
choose O
only O
one O
random O
pair O
) O
correlate O
with O
entailment O
labels O
. O
F O
or O
development O
and O
test O
sets O
, O
we O
use O
all O
possible O
2nrecasted O
pairs O
( O
where O
nis O
the O
number O
of O
classes O
in O
classification O
data O
) O
since O
ideally O
, O
while O
testing O
we O
have O
no O
prior O
knowledge O
of O
the O
ground O
- O
truth O
label O
. O
Context O
- O
Hypothesis O
Baselines O
Sentence O
Dataset O
Representation O
PR O
BH O
HDA O
BBC O
BoW O
47.32 O
51.00 O
54.20 O
57.00 O
Sent2V O
ec O
61.21 O
62.67 O
64.00 O
65.42 O
InferSent O
68.00 O
65.04 O
67.9 O
68.84 O
XLM O
- O
RoBER O
T O
a O
74.02 O
74.48 O
75.29 O
73.56 O
Hypothesis O
- O
only O
Baselines O
BoW O
44.89 O
47.01 O
44.82 O
43.00 O
Sent2V O
ec O
51.91 O
50.84 O
50.88 O
48.80 O
InferSent O
54.32 O
52.14 O
53.54 O
51.08 O
XLM O
- O
RoBER O
T O
a O
55.00 O
52.60 O
53.92 O
55.00 O
T O
able O
6 O
: O
TE O
classification O
accuracies O
using O
different O
sentence O
embeddings O
for O
all O
four O
datasets O
. O
With O
T O
able O
6 O
, O
we O
establish O
that O
XLMRoBER O
T O
a O
( O
Conneau O
et O
al O
. O
, O
2019 O
) O
gives O
the O
best O
performance O
as O
compared O
to O
all O
the O
other O
baselines O
. O
Therefore O
, O
we O
use O
it O
for O
all O
the O
following O
experiments O
. O
Also O
, O
random O
performance O
on O
hypothesis O
- O
only O
baseline O
ensures O
that O
our O
recasted O
data O
does O
not O
contain O
hypothesis O
- O
bias.712Consistency O
- O
W O
e O
analyse O
the O
effect O
of O
consistency O
regulariser O
( O
CR O
) O
by O
comparing O
the O
percentage O
of O
inconsistent O
model O
predictions O
for O
TE O
models O
with O
and O
without O
CR O
. O
Figure O
3 O
clearly O
depicts O
that O
the O
constraint O
regularisation O
helps O
in O
reducing O
the O
percentage O
of O
inconsistent O
pairs O
and O
hence O
makes O
the O
model O
predictions O
congruent O
with O
its O
own O
internal O
representation O
in O
the O
model O
parameters O
. O
5.2 O
T O
wo O
- O
step O
Classification O
Results O
W O
e O
now O
use O
the O
TE O
model O
to O
perform O
two O
- O
step O
classification O
as O
explained O
in O
section O
4.2 O
. O
T O
able O
10 O
shows O
the O
classification O
accuracies O
obtained O
via O
direct O
as O
well O
as O
two O
- O
step O
classification O
with O
consistency O
regularisation O
and O
joint O
- O
objective O
. O
As O
reported O
in O
T O
able O
9and O
10 O
, O
we O
observe O
a O
jump O
in O
both O
the O
TE O
as O
well O
as O
two O
- O
step O
classification O
accuracies O
with O
the O
addition O
of O
consistency O
regularisation O
. O
Such O
a O
constraint O
restricts O
the O
model O
predictions O
to O
be O
either O
correct O
or O
incorrect O
but O
not O
pairwise O
- O
inconsistent O
with O
its O
other O
beliefs O
. O
Joint O
Objective O
- O
In O
T O
able O
9and O
10 O
, O
we O
observe O
that O
joint O
objective O
proves O
to O
be O
much O
more O
beneficial O
than O
independent O
TE O
and O
classifier O
training O
. O
The O
two O
- O
step O
classification O
accuracy O
with O
joint O
- O
objective O
( O
+ O
JO+CR O
) O
surpasses O
the O
direct O
classification O
performance O
. O
W O
e O
observe O
an O
increment O
of O
5 O
% O
in O
TE O
and O
2 O
% O
in O
classification O
accuracy O
across O
all O
the O
datasets O
. O
F O
urthermore O
, O
from O
Figure O
3 O
, O
we O
observe O
that O
, O
JO O
also O
improve O
the O
prediction O
consistency O
across O
all O
the O
datasets O
. O
T O
able O
7shows O
the O
exact O
percentage O
of O
correct O
/ O
incorrect O
and O
inconsistent O
pairs O
. O
Improved O
Performance O
Analysis O
- O
The O
two O
- O
step O
classification O
is O
able O
to O
achieve O
overall O
improvement O
over O
direct O
classification O
approach O
mainly O
due O
to O
following O
two O
factors O
. O
Firstly O
, O
the O
joint O
objective O
( O
JO O
) O
helps O
in O
creating O
a O
feedback O
loop O
with O
the O
two O
tasks O
of O
textual O
entailment O
and O
classification O
, O
which O
enforce O
consistency O
in O
the O
model O
predictions O
for O
the O
two O
tasks O
. O
Secondly O
, O
the O
consistency O
regularisation O
( O
CR O
) O
for O
the O
TE O
helps O
in O
making O
the O
model O
decisions O
congruent O
across O
same O
context O
premise O
but O
different O
related O
hypothesis O
. O
Thus O
, O
both O
the O
JO O
and O
CR O
imposes O
indirect O
and O
direct O
inductive O
bias O
through O
constrained O
loss O
objective O
which O
improves O
model O
performance O
Figure O
3 O
: O
Plot O
depicting O
percentage O
( O
% O
) O
of O
inconsistent O
predictions O
for O
all O
the O
datasets O
using O
XLMRoBERTa O
with O
and O
without O
consistency O
regularisation O
( O
CR O
) O
and O
Joint O
Objective O
( O
JO O
) O
. O
compared O
to O
the O
direct O
classification O
task O
. O
5.3 O
Direct O
vs O
T O
wo O
- O
Step O
Classification O
W O
e O
analyse O
the O
classification O
predictions O
obtained O
by O
direct O
as O
well O
as O
two O
- O
step O
classification O
to O
compare O
the O
differences O
. O
Figure O
4 O
shows O
the O
percentage O
( O
% O
) O
of O
correct O
and O
incorrect O
predictions O
obtained O
for O
the O
two O
approaches O
considered O
. O
More O
generally O
, O
we O
see O
a O
maximum O
consensus O
across O
the O
main O
diagonal O
between O
the O
two O
approaches O
. O
However O
, O
there O
are O
irregularities O
wherein O
one O
of O
the O
predictions O
contradicts O
the O
other O
. O
As O
illustrated O
in O
T O
able O
8 O
, O
we O
depict O
qualitative O
examples O
corresponding O
to O
these O
irregularities O
. O
W O
e O
analyse O
their O
entailment O
vectors O
to O
interpret O
intermediate O
predictions O
and O
realise O
that O
the O
high O
entailments O
corresponding O
to O
the O
gold O
label O
and O
certain O
incorrect O
label O
lead O
to O
incorrect O
predictions O
. O
F O
or O
example O
, O
for O
the O
first O
sentence O
in O
T O
able O
8 O
, O
we O
observe O
that O
the O
context O
- O
hypothesis O
pairs O
with O
hypothesis O
corresponding O
to O
The O
product O
received O
negative O
reviews O
from O
its O
users O
, O
and‚ÄòThe O
product O
received O
conflicting O
reviews O
from O
its O
users O
‚Äô O
get O
the O
entailment O
probabilities O
0.64 O
and O
0.58 O
, O
respectively O
. O
This O
shows O
that O
apart O
from O
the O
gold O
label O
i.e. O
negative O
here O
, O
there O
is O
an O
inclination O
towards O
the O
class O
label O
conflict O
. O
Moreover O
, O
we O
see O
certain O
statistical O
word O
patterns O
like O
the O
usage O
of O
the O
keyword O
but O
in O
most O
of O
the O
sentences O
corresponding O
to O
the O
classconflict O
, O
thereby O
ensuring O
a O
certain O
degree O
of O
artefact O
learning O
which O
governs O
the O
decisions O
in O
direct O
classification O
. O
One O
advan-713Dataset O
Correct O
Incorrect O
Inconsistent O
TE O
+ O
CR O
+ O
JO O
+ O
CR O
TE O
+ O
CR O
+ O
JO O
+ O
CR O
TE O
+ O
CR O
+ O
JO O
+ O
CR O
+ O
JO O
+ O
JO O
+ O
JO O
PR O
71.43 O
72.18 O
72.50 O
74.00 O
13.82 O
18.6 O
18.6 O
18.2 O
14.75 O
9.22 O
8.90 O
7.80 O
BH O
73.20 O
74.50 O
74.76 O
75.80 O
14.32 O
17.50 O
17.66 O
17.99 O
12.48 O
8.00 O
7.58 O
6.21 O
HDA O
72.00 O
74.88 O
75.22 O
76.8 O
11.50 O
14.66 O
14.78 O
13.9 O
16.50 O
10.46 O
10.00 O
9.30 O
BBC O
71.17 O
74.56 O
74.84 O
76.00 O
17.75 O
18.2 O
18.16 O
17.2 O
11.08 O
7.24 O
7.00 O
6.80 O
T O
able O
7 O
: O
Percentage O
( O
% O
) O
of O
correct O
, O
incorrect O
and O
inconsistent O
prediction O
pairs O
for O
all O
the O
datasets O
using O
XLM O
- O
RoBER O
T O
a. O
Sentence O
T O
rue O
Label O
Direct O
clf O
. O
T O
wo O
- O
step O
clf O
. O
‡§Ø‡§π‡§æ‡§Å‡§ñ‡§æ‡§®‡§æ O
‡§™‡•Ä‡§®‡§æ O
‡§â‡§§‡§®‡§æ O
‡§Æ O
‡§Å‡§π‡§ó‡§æ O
‡§®‡§π“∞ O
‡§Ç‡§™‡§∞ O
‡§∞‡§π‡§®‡§æ O
‡§ú O
‡•á‡§¨ O
‡§ï‡•ã O
‡§ï‡§æ‡§´“¥ O
‡§≠‡§æ‡§∞“∞ O
‡§™‡•ú‡§§‡§æ O
‡§π O
‡•à‡•§negative O
conflict O
negative O
English O
: O
Drinking O
here O
is O
not O
that O
expensive O
but O
living O
on O
the O
pocket O
is O
very O
heavy O
. O
‡§∞‡§æ‡§ú‡§ó‡•Å“ä O
, O
‡§Æ‡§π‡§æ‡§∞‡§æ‡§ú O
‡§ï O
‡•É O
‡§Ç‡§£‡§¶ O
‡•á‡§µ O
‡§∞‡§æ‡§Ø O
‡§ï‡•ã O
‡§ï‡§π‡§§‡•á O
‡§π O
‡•à‡§ï‡•á‡§§‡•á‡§®‡§æ‡§≤“∞‡§∞‡§æ‡§Æ O
‡§ù O
‡•Ç‡§† O
‡§¨‡•ã‡§≤ O
‡§∞‡§π‡•á O
‡§π O
‡•à|anger O
anger O
sad O
English O
: O
Rajguru O
tells O
Maharaja O
Krishnadeva O
Raya O
that O
T O
enaliram O
is O
lying O
. O
T O
able O
8 O
: O
Qualitative O
examples O
where O
direct O
and O
two O
- O
step O
classification O
methods O
contradict O
predictions O
. O
DatasetT O
extual O
Entailment O
w/o O
CR O
/ O
JO O
+ O
CR O
+ O
JO O
+ O
CR+JO O
PR O
74.02 O
77.80 O
78.40 O
81.40 O
BH O
74.48 O
76.57 O
77.01 O
80.05 O
HDA O
75.29 O
78.00 O
78.22 O
81.67 O
BBC O
73.56 O
76.24 O
77.69 O
79.22 O
T O
able O
9 O
: O
TE O
accuracies O
for O
all O
the O
four O
datasets O
using O
XLM O
- O
RoBER O
T O
a O
( O
Conneau O
et O
al O
. O
, O
2019 O
) O
. O
DatasetDirect O
T O
wo O
- O
step O
clf O
. O
clf O
. O
TE O
TE+ O
CRTE+ O
JOTE+ O
CR+JO O
PR O
71.65 O
66.24 O
69.38 O
70.58 O
73.70 O
BH O
73.03 O
68.06 O
70.91 O
71.82 O
74.80 O
HDA O
74.25 O
68.22 O
71.45 O
72.45 O
75.96 O
BBC O
70.22 O
65.98 O
68.20 O
70.30 O
72.18 O
T O
able O
10 O
: O
Classification O
( O
direct O
and O
two O
- O
step O
) O
accuracies O
for O
all O
the O
four O
datasets O
using O
XLMRoBER O
T O
a O
( O
Conneau O
et O
al O
. O
, O
2019 O
) O
. O
tage O
of O
two O
- O
step O
classification O
is O
that O
it O
is O
more O
transparent O
about O
it O
‚Äôs O
predictions O
. O
This O
ensures O
more O
interpretability O
in O
the O
model O
decisions O
. O
W O
e O
also O
compare O
class O
- O
wise O
accuracies O
of O
both O
the O
approaches O
for O
each O
of O
the O
datasets O
and O
see O
improvements O
with O
the O
twostep O
method O
in O
all O
classes9 O
. O
6 O
Conclusion O
In O
this O
work O
, O
we O
share O
the O
first O
recasted O
NLI O
dataset O
in O
a O
low O
- O
resource O
language O
Hindi O
, O
and O
show O
how O
a O
large O
- O
scale O
NLI O
data O
can O
be O
developed O
for O
low O
- O
resource O
languages O
without O
un9See O
Appendix O
Section O
A.2 O
Figure O
6for O
class O
- O
wise O
results O
65 O
12.5 O
5 O
17.568 O
12 O
6.5 O
12.570 O
10 O
5 O
1563 O
15.5 O
4 O
17.5DIRECT O
CLASSIFICATION O
TWO O
- O
STEP O
CLASSIFICATION O
‚úî O
       O
‚ùå O
PR O
  O
BH O
   O
HDA O
  O
BBC O
  O
Correct O
: O
‚úî O
  O
Incorrect O
: O
‚ùå O
‚úî O
‚ùå O
‚úî O
       O
‚ùå O
  O
‚úî O
       O
‚ùå O
  O
‚úî O
       O
‚ùå O
Figure O
4 O
: O
Correct O
vs O
Incorrect O
Predictions O
( O
% O
) O
for O
Direct O
and O
T O
wo O
- O
Step O
classification O
. O
dergoing O
costly O
and O
time O
taking O
human O
annotations O
. O
W O
e O
perform O
TE O
experiments O
and O
introduce O
a O
consistency O
regulariser O
to O
avoid O
pairwise O
- O
inconsistent O
TE O
predictions O
. O
F O
urthermore O
, O
we O
propose O
a O
two O
- O
step O
classification O
approach O
with O
a O
joint O
training O
objective O
. O
Our O
results O
with O
the O
joint O
objective O
shows O
significant O
improvement O
in O
performance O
. O
As O
a O
future O
work O
, O
we O
aim O
to O
analyse O
the O
proposed O
methodology O
which O
is O
language O
independent O
on O
other O
low O
- O
resource O
languages O
. O
W O
e O
also O
aim O
to O
use O
more O
generalisable O
templates O
for O
linguistic O
diversity O
in O
recating O
data O
. O
It O
would O
be O
interesting O
to O
analyse O
how O
extending O
textual O
entailment O
knowledge O
especially O
the O
consistency O
regularization O
constraint O
affect O
other O
downstream O
NLP O
tasks O
apart O
from O
textual O
classification O
, O
not O
only O
in O
terms O
of O
the O
performance O
, O
but O
also O
in O
enhancing O
the O
model O
interpretability O
.714References O
Md O
Shad O
Akhtar O
, O
A O
yush O
Kumar O
, O
Asif O
Ekbal O
, O
and O
Pushpak O
Bhattacharyya O
. O
2016 O
. O
A O
hybrid O
deep O
learning O
architecture O
for O
sentiment O
analysis O
. O
In O
Proceedings O
of O
COLING O
2016 O
, O
the O
26th O
International O
Conference O
on O
Computational O
Linguistics O
: O
Technical O
Papers O
, O
pages O
482‚Äì493 O
. O
Pushpak O
Bhattacharyya O
. O
2012 O
. O
Natural O
language O
processing O
: O
A O
perspective O
from O
computation O
in O
presence O
of O
ambiguity O
, O
resource O
constraint O
and O
multilinguality O
. O
CSI O
journal O
of O
computing O
, O
1(2):1‚Äì13 O
. O
Samuel O
Bowman O
, O
Gabor O
Angeli O
, O
Christopher O
Potts O
, O
and O
Christopher O
D O
Manning O
. O
2015 O
. O
A O
large O
annotated O
corpus O
for O
learning O
natural O
language O
inference O
. O
In O
Proceedings O
of O
the O
2015 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
632‚Äì642 O
. O
Alexis O
Conneau O
, O
Kartikay O
Khandelwal O
, O
Naman O
Goyal O
, O
Vishrav O
Chaudhary O
, O
Guillaume O
W O
enzek O
, O
F. O
Guzm√°n O
, O
Edouard O
Grave O
, O
Myle O
Ott O
, O
Luke O
Zettlemoyer O
, O
and O
V O
eselin O
Stoyanov O
. O
2019 O
. O
Unsupervised O
cross O
- O
lingual O
representation O
learning O
at O
scale O
. O
ArXiv O
, O
abs/1911.02116 O
. O
Alexis O
Conneau O
, O
Douwe O
Kiela O
, O
Holger O
Schwenk O
, O
Lo√Øc O
Barrault O
, O
and O
Antoine O
Bordes O
. O
2017 O
. O
Supervised O
learning O
of O
universal O
sentence O
representations O
from O
natural O
language O
inference O
data O
. O
In O
Proceedings O
of O
the O
2017 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
670‚Äì680 O
. O
Alexis O
Conneau O
, O
Ruty O
Rinott O
, O
Guillaume O
Lample O
, O
Adina O
Williams O
, O
Samuel O
Bowman O
, O
Holger O
Schwenk O
, O
and O
V O
eselin O
Stoyanov O
. O
2018 O
. O
Xnli O
: O
Evaluating O
cross O
- O
lingual O
sentence O
representations O
. O
In O
Proceedings O
of O
the O
2018 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
2475‚Äì2485 O
. O
Swapnil O
Dhanwal O
, O
Hritwik O
Dutta O
, O
Hitesh O
Nankani O
, O
Nilay O
Shrivastava O
, O
Y O
aman O
Kumar O
, O
Junyi O
Jessy O
Li O
, O
Debanjan O
Mahata O
, O
Rakesh O
Gosangi O
, O
Haimin O
Zhang O
, O
Rajiv O
Ratn O
Shah O
, O
and O
Amanda O
Stent O
. O
2020 O
. O
An O
annotated O
dataset O
of O
discourse O
modes O
in O
Hindi O
stories O
. O
InProceedings O
of O
the O
12th O
Language O
Resources O
and O
Evaluation O
Conference O
, O
pages O
1191‚Äì1196 O
, O
Marseille O
, O
F O
rance O
. O
European O
Language O
Resources O
Association O
. O
Pratik O
Joshi O
, O
Sebastin O
Santy O
, O
Amar O
Budhiraja O
, O
Kalika O
Bali O
, O
and O
Monojit O
Choudhury O
. O
2020 O
. O
The O
state O
and O
fate O
of O
linguistic O
diversity O
and O
inclusion O
in O
the O
NLP O
world O
. O
In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
Online O
. O
Association O
for O
Computational O
Linguistics O
. O
Ramchandra O
Joshi O
, O
Purvi O
Goel O
, O
and O
Raviraj O
Joshi O
. O
2019 O
. O
Deep O
learning O
for O
hindi O
text O
classification O
: O
A O
comparison O
. O
In O
International O
Conference O
onIntelligent O
Human O
Computer O
Interaction O
, O
pages O
94‚Äì101 O
. O
Springer O
. O
Simran O
Khanuja O
, O
S. O
Dandapat O
, O
S. O
Sitaram O
, O
and O
M. O
Choudhury O
. O
2020 O
. O
A O
new O
dataset O
for O
natural O
language O
inference O
from O
code O
- O
mixed O
conversations O
. O
In O
CodeSwitch@LREC O
. O
T O
ushar O
Khot O
, O
Ashish O
Sabharwal O
, O
and O
Peter O
Clark O
. O
2018 O
. O
Scitail O
: O
A O
textual O
entailment O
dataset O
from O
science O
question O
answering O
. O
In O
Thirty O
- O
Second O
AAAI O
Conference O
on O
Artificial O
Intelligence O
. O
Y O
aman O
Kumar O
, O
Debanjan O
Mahata O
, O
Sagar O
Aggarwal O
, O
Anmol O
Chugh O
, O
Rajat O
Maheshwari O
, O
and O
Rajiv O
Ratn O
Shah O
. O
2019 O
. O
Bhaav- O
a O
text O
corpus O
for O
emotion O
analysis O
from O
hindi O
stories O
. O
ArXiv O
, O
abs/1910.04073 O
. O
Guanyu O
Li O
, O
Pengfei O
Zhang O
, O
and O
Caiyan O
Jia O
. O
2018 O
. O
Attention O
boosted O
sequential O
inference O
model O
. O
CoRR O
, O
abs/1812.01840 O
. O
T O
ao O
Li O
, O
Vivek O
Gupta O
, O
Maitrey O
Mehta O
, O
and O
Vivek O
Srikumar O
. O
2019 O
. O
A O
logic O
- O
driven O
framework O
for O
consistency O
of O
neural O
models O
. O
In O
Proceedings O
of O
the O
2019 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
. O
Xiaodong O
Liu O
, O
Pengcheng O
He O
, O
W O
eizhu O
Chen O
, O
and O
Jianfeng O
Gao O
. O
2019 O
. O
Multi O
- O
task O
deep O
neural O
networks O
for O
natural O
language O
understanding O
. O
InProceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
4487‚Äì4496 O
. O
Puneet O
Mathur O
, O
Rajiv O
Shah O
, O
Ramit O
Sawhney O
, O
and O
Debanjan O
Mahata O
. O
2018 O
. O
Detecting O
offensive O
tweets O
in O
hindi O
- O
english O
code O
- O
switched O
language O
. O
InProceedings O
of O
the O
Sixth O
International O
Workshop O
on O
Natural O
Language O
Processing O
for O
Social O
Media O
, O
pages O
18‚Äì26 O
. O
Matteo O
Pagliardini O
, O
Prakhar O
Gupta O
, O
and O
Martin O
Jaggi O
. O
2018 O
. O
Unsupervised O
learning O
of O
sentence O
embeddings O
using O
compositional O
n O
- O
gram O
features O
. O
In O
Proceedings O
of O
the O
2018 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
AssociationforComputationalLinguistics O
: O
HumanLanguage O
Technologies O
, O
Volume O
1 O
( O
Long O
Papers O
) O
, O
pages O
528‚Äì540 O
. O
Adam O
Poliak O
, O
Aparajita O
Haldar O
, O
Rachel O
Rudinger O
, O
J. O
Edward O
Hu O
, O
Ellie O
Pavlick O
, O
Aaron O
Steven O
White O
, O
and O
Benjamin O
V O
an O
Durme O
. O
2018a O
. O
Collecting O
diverse O
natural O
language O
inference O
problems O
for O
sentence O
representation O
evaluation O
. O
In O
BlackboxNLP@EMNLP O
. O
Adam O
Poliak O
, O
Jason O
Naradowsky O
, O
Aparajita O
Haldar O
, O
Rachel O
Rudinger O
, O
and O
Benjamin O
V O
an O
Durme O
. O
2018b O
. O
Hypothesis O
only O
baselines O
in O
natural O
language O
inference O
. O
In O
* O
SEM@NAACLHLT O
.715Mark O
Sammons O
, O
V.G.Vinod O
V O
ydiswaran O
, O
and O
Dan O
Roth O
. O
2010 O
. O
‚Äú O
ask O
not O
what O
textual O
entailment O
can O
do O
for O
you O
... O
‚Äù O
. O
InProceedings O
of O
the O
48th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
1199‚Äì1208 O
, O
Uppsala O
, O
Sweden O
. O
Association O
for O
Computational O
Linguistics O
. O
Kushagra O
Singh O
, O
Indira O
Sen O
, O
and O
Ponnurangam O
Kumaraguru O
. O
2018 O
. O
Language O
identification O
and O
named O
entity O
recognition O
in O
hinglish O
code O
mixed O
tweets O
. O
In O
Proceedings O
of O
ACL O
2018 O
, O
Student O
Research O
Workshop O
, O
pages O
52‚Äì58 O
. O
Sunayana O
Sitaram O
, O
Khyathi O
Raghavi O
Chandu O
, O
Sai O
Krishna O
Rallabandi O
, O
and O
Alan O
W O
Black O
. O
2019 O
. O
A O
survey O
of O
code O
- O
switched O
speech O
and O
language O
processing O
. O
arXiv O
preprint O
arXiv:1904.00784 O
. O
Shawn O
T O
an O
, O
Yikang O
Shen O
, O
Chin O
- O
W O
ei O
Huang O
, O
and O
Aaron O
C. O
Courville O
. O
2019 O
. O
Investigating O
biases O
in O
textual O
entailment O
datasets O
. O
ArXiv O
, O
abs/1906.09635 O
. O
Marta O
T O
atu O
and O
Dan O
Moldovan O
. O
2005 O
. O
A O
semantic O
approach O
to O
recognizing O
textual O
entailment O
. O
InProceedings O
of O
Human O
Language O
Technology O
Conference O
and O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
371 O
‚Äì O
378 O
, O
V O
ancouver O
, O
British O
Columbia O
, O
Canada O
. O
Association O
for O
Computational O
Linguistics O
. O
Rui O
W O
ang O
and O
Yi O
Zhang O
. O
2009 O
. O
Recognizing O
textual O
relatedness O
with O
predicate O
- O
argument O
structures O
. O
In O
Proceedings O
of O
the O
2009 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
: O
Volume O
2 O
- O
Volume O
2 O
, O
pages O
784‚Äì792 O
. O
Association O
for O
Computational O
Linguistics O
. O
Aaron O
Steven O
White O
, O
Pushpendre O
Rastogi O
, O
Kevin O
Duh O
, O
and O
Benjamin O
V O
an O
Durme O
. O
2017 O
. O
Inference O
is O
everything O
: O
Recasting O
semantic O
resources O
into O
a O
unified O
evaluation O
framework O
. O
In O
Proceedings O
of O
the O
Eighth O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
996‚Äì1005 O
. O
Adina O
Williams O
, O
Nikita O
Nangia O
, O
and O
Samuel O
Bowman O
. O
2018 O
. O
A O
broad O
- O
coverage O
challenge O
corpus O
for O
sentence O
understanding O
through O
inference O
. O
InProceedings O
of O
the O
2018 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
Papers O
) O
, O
pages O
1112‚Äì1122 O
, O
New O
Orleans O
, O
Louisiana O
. O
Association O
for O
Computational O
Linguistics O
. O
A O
Appendix O
A.1 O
Illustration O
of O
Recasting O
Approach O
W O
e O
illustrate O
the O
proposed O
recasting O
approach O
in O
more O
detail O
with O
example O
templates O
in O
Fig O
- O
ure O
5 O
. O
W O
e O
show O
how O
each O
classification O
sentence O
is O
used O
to O
create O
a O
context O
- O
hypothesis O
pair O
for O
NLI O
task O
for O
different O
datasets O
corresponding O
to O
the O
diverse O
semantic O
phenomenon O
considered O
. O
A.2 O
Additional O
Results O
Development O
Set O
Results O
- O
W O
e O
report O
the O
results O
on O
development O
set O
for O
textual O
entailment O
as O
well O
as O
classification O
in O
T O
able O
12 O
and O
13 O
respectively O
. O
W O
e O
observe O
similar O
trends O
in O
the O
development O
set O
as O
depicted O
in O
the O
test O
set O
performance O
for O
both O
the O
tasks O
of O
textual O
entailment O
as O
well O
as O
the O
two O
- O
step O
classification O
task O
. O
Class O
- O
wise O
Performance O
- O
In O
Figure O
6 O
, O
we O
show O
class O
- O
wise O
accuracies O
obtained O
by O
the O
two O
classification O
approaches O
- O
direct O
vs O
two O
- O
step O
. O
Broadly O
, O
we O
obtain O
a O
considerable O
improvement O
in O
the O
performance O
of O
two O
- O
step O
classification O
over O
direct O
classification O
, O
over O
all O
classes O
across O
all O
the O
four O
datasets O
. O
This O
ensures O
that O
the O
obtained O
performance O
improvement O
is O
balanced O
across O
all O
classes O
. O
Semi O
- O
supervised O
setting O
- O
W O
e O
extend O
our O
analysis O
to O
a O
semi O
- O
supervised O
setting O
( O
with O
fewer O
labels O
) O
wherein O
we O
retain O
the O
true O
labels O
for O
only O
40 O
% O
, O
60 O
% O
and O
80 O
% O
of O
the O
data O
while O
training O
and O
analyse O
its O
effect O
on O
the O
performance O
of O
TE O
and O
classification O
tasks O
. O
T O
able O
14,16 O
and O
18 O
show O
the O
results O
obtained O
with O
different O
ablations O
with O
80 O
% O
, O
60 O
% O
and O
40 O
% O
of O
the O
labelled O
data O
respectively O
for O
the O
TE O
task O
. O
Similarly O
, O
T O
able O
15,17 O
and O
19 O
report O
the O
results O
for O
direct O
and O
two O
- O
step O
classification O
in O
the O
semi O
- O
supervised O
approach O
highlighting O
the O
effect O
of O
joint O
objective O
and O
consistency O
regularisation O
in O
obtaining O
improvement O
. O
Although O
, O
we O
utilize O
the O
consistency O
regularisation O
, O
since O
it O
does O
not O
depend O
on O
the O
true O
label O
, O
rather O
operated O
on O
pairwise O
contexthypothesis O
groupings O
. O
W O
e O
observe O
that O
TE O
with O
consistency O
regularisation O
and O
joint O
objective O
surpasses O
the O
trivial O
TE O
task O
without O
any O
added O
constraints O
. O
This O
depicts O
that O
our O
regularisation O
and O
joint O
objective O
approach O
add O
robust O
improvements O
in O
TE O
model O
performance O
even O
with O
minimum O
supervision.716Recasting O
Datasets O
  O
Original O
Sentence O
Sentiment O
Label O
  O
Recasting O
Template O
  O
The O
product O
got O
< O
label O
> O
  O
reviews O
from O
its O
users O
. O
It O
is O
a O
matter O
of O
< O
label O
> O
. O
  O
Context O
: O
  O
Original O
Sentence O
Premise O
: O
  O
Recasting O
Template O
The O
product O
did O
not O
get O
< O
label O
> O
  O
reviews O
from O
its O
users O
. O
It O
is O
not O
a O
matter O
of O
< O
label O
> O
. O
  O
TE O
Label O
Context O
: O
  O
Original O
Sentence O
Premise O
: O
  O
Recasting O
Template O
  O
< O
ground O
truth O
label O
> O
: O
entailed O
  O
< O
any O
other O
label O
> O
: O
not O
- O
entailed O
< O
ground O
truth O
label O
> O
: O
not O
- O
entailed O
  O
< O
any O
other O
label O
> O
: O
entailed O
PR O
BH O
The O
sentence O
depicts O
< O
label O
> O
  O
statement O
. O
  O
The O
sentence O
does O
not O
depicts O
  O
< O
label O
> O
statement O
. O
  O
HDA O
Positive O
Hypothesis O
  O
Negative O
Hypothesis O
Figure O
5 O
: O
Illustration O
of O
the O
proposed O
recasting O
approach O
. O
Original O
Sentence(Hindi O
) O
Original O
Sentence O
( O
English O
) O
Sentiment O
‡§á‡§® O
‡§™—ü‡§µ‡§Ω O
‡§≠‡§æ‡§µ”ñ O
‡§∏ O
‡•á‡§â‡§∏‡§ï“¥ O
‡§ÜÕ§‡§Æ‡§æ O
—ü‡§µ—ã‡§≤ O
‡§π‡•ã O
‡§ó‡§Ø‡•Ä‡•§His O
soul O
was O
overwhelmed O
by O
these O
holy O
feelings O
. O
Joy O
Model O
Consistency O
/ O
Inconsistency O
Contradictory O
TE O
pairs O
( O
Hindi O
) O
Contradictory O
TE O
pairs O
( O
English O
) O
Prediction O
Label O
p O
- O
h1p O
- O
h2 O
p O
: O
‡§á‡§® O
‡§™—ü‡§µ‡§Ω O
‡§≠‡§æ‡§µ”ñ O
‡§∏ O
‡•á‡§â‡§∏‡§ï“¥ O
‡§ÜÕ§‡§Æ‡§æ O
—ü‡§µ—ã‡§≤ O
‡§π‡•ã O
‡§ó‡§Ø‡•Ä‡•§p O
: O
His O
soul O
was O
overwhelmed O
by O
these O
holy O
feelings.e O
e O
Inconsistent O
h1:‘π‡§æ O
‡§Ø‡§π O
‡§ñ O
‡•Å‡§∂‡•Ä O
‡§ï“¥ O
‡§¨‡§æ‡§§ O
‡§π O
‡•à O
? O
h1 O
: O
Is O
this O
a O
matter O
of O
joy O
? O
e O
ne O
Correct O
p O
: O
‡§á‡§® O
‡§™—ü‡§µ‡§Ω O
‡§≠‡§æ‡§µ”ñ O
‡§∏ O
‡•á‡§â‡§∏‡§ï“¥ O
‡§ÜÕ§‡§Æ‡§æ O
—ü‡§µ—ã‡§≤ O
‡§π‡•ã O
‡§ó‡§Ø‡•Ä‡•§p O
: O
His O
soul O
was O
overwhelmed O
by O
these O
holy O
feelings.ne O
e O
Incorrect O
h2:‘π‡§æ O
‡§Ø‡§π O
‡§ñ O
‡•Å‡§∂‡•Ä O
‡§ï“¥ O
‡§¨‡§æ‡§§ O
‡§®‡§π“∞ O
‡§Ç‡§π‡•à O
? O
h2 O
: O
Is O
this O
not O
a O
matter O
of O
joy O
? O
ne O
ne O
Inconsistent O
T O
able O
11 O
: O
Example O
sentences O
for O
contradictory O
premise O
( O
p O
) O
- O
( O
h O
) O
pairs O
for O
measuring O
inconsistency O
in O
the O
recasted O
model O
predictions O
with O
erepresenting O
entailed O
andne O
representing O
not O
- O
entailed O
. O
DatasetT O
extual O
Entailment O
‚Üë O
w/o O
+ O
CR O
+ O
JO O
+ O
CR+JO O
PR O
74.26 O
78.44 O
78.02 O
80.60 O
BH O
73.88 O
76.46 O
76.82 O
80.95 O
HDA O
75.90 O
78.54 O
78.48 O
81.86 O
BBC O
73.45 O
76.48 O
77.96 O
79.02 O
T O
able O
12 O
: O
TE O
accuracies O
for O
all O
the O
four O
datasets O
using O
XLM O
- O
RoBER O
T O
a O
on O
the O
development O
set O
. O
DatasetDirect O
T O
wo O
- O
step O
clf O
. O
‚Üë O
clf O
. O
TE O
TE+ O
CRTE+ O
JOTE+ O
CR+JO O
PR O
71.40 O
65.48 O
68.76 O
70.84 O
72.98 O
BH O
73.50 O
69.24 O
70.88 O
71.46 O
75.66 O
HDA O
74.85 O
68.46 O
72.34 O
73.50 O
75.56 O
BBC O
71.36 O
66.40 O
68.38 O
70.47 O
73.08 O
T O
able O
13 O
: O
Classification O
( O
direct O
and O
two O
- O
step O
) O
accuracies O
for O
all O
the O
four O
datasets O
using O
XLM O
- O
RoBER O
T O
a O
on O
the O
development O
set.717HDA O
BBC O
  O
PR O
BH O
  O
Figure O
6 O
: O
Class O
- O
wise O
comparison O
of O
Direct O
vs O
T O
wo O
- O
Step O
Classification O
. O
DatasetT O
extual O
Entailment O
‚Üë O
w/o O
+ O
CR O
+ O
JO O
+ O
CR+JO O
PR O
69.23 O
72.68 O
70.48 O
74.04 O
BH O
70.65 O
71.09 O
70.99 O
73.98 O
HDA O
70.29 O
72.23 O
71.32 O
74.67 O
BBC O
70.36 O
73.84 O
71.65 O
74.52 O
T O
able O
14 O
: O
TE O
accuracies O
for O
all O
the O
four O
datasets O
using O
XLM O
- O
RoBER O
T O
a O
with O
fewer O
labels O
( O
80%).DatasetDirect O
T O
wo O
- O
step O
clf O
. O
‚Üë O
clf O
. O
TE O
TE+ O
CRTE+ O
JOTE+ O
CR+JO O
PR O
67.20 O
61.28 O
64.87 O
62.49 O
68.98 O
BH O
68.51 O
64.22 O
66.71 O
71.46 O
69.46 O
HDA O
68.82 O
62.62 O
65.13 O
63.75 O
69.95 O
BBC O
66.93 O
60.94 O
63.14 O
61.47 O
67.73 O
T O
able O
15 O
: O
Classification O
( O
direct O
and O
two O
- O
step O
) O
accuracies O
for O
all O
the O
four O
datasets O
using O
XLM O
- O
RoBER O
T O
a O
with O
fewer O
labels O
( O
80 O
% O
) O
. O
DatasetT O
extual O
Entailment O
‚Üë O
w/o O
+ O
CR O
+ O
JO O
+ O
CR+JO O
PR O
65.12 O
67.46 O
65.58 O
70.06 O
BH O
66.12 O
68.57 O
67.22 O
70.69 O
HDA O
65.29 O
67.25 O
66.34 O
70.59 O
BBC O
66.87 O
68.22 O
67.19 O
71.42 O
T O
able O
16 O
: O
TE O
accuracies O
for O
all O
the O
four O
datasets O
using O
XLM O
- O
RoBER O
T O
a O
with O
fewer O
labels O
( O
60%).DatasetDirect O
T O
wo O
- O
step O
clf O
. O
‚Üë O
clf O
. O
TE O
TE+ O
CRTE+ O
JOTE+ O
CR+JO O
PR O
60.29 O
61.82 O
62.37 O
62.00 O
63.98 O
BH O
61.52 O
62.14 O
64.18 O
62.45 O
64.81 O
HDA O
61.82 O
63.47 O
63.94 O
63.33 O
65.56 O
BBC O
60.23 O
61.24 O
62.16 O
62.09 O
64.73 O
T O
able O
17 O
: O
Classification O
( O
direct O
and O
two O
- O
step O
) O
accuracies O
for O
all O
the O
four O
datasets O
using O
XLM O
- O
RoBER O
T O
a O
with O
fewer O
labels O
( O
60 O
% O
) O
. O
DatasetT O
extual O
Entailment O
‚Üë O
w/o O
+ O
CR O
+ O
JO O
+ O
CR+JO O
PR O
57.12 O
58.46 O
58.08 O
59.56 O
BH O
59.12 O
59.57 O
59.22 O
60.69 O
HDA O
59.29 O
59.25 O
60.19 O
60.78 O
BBC O
58.42 O
58.70 O
58.10 O
59.02 O
T O
able O
18 O
: O
TE O
accuracies O
for O
all O
the O
four O
datasets O
using O
XLM O
- O
RoBER O
T O
a O
with O
fewer O
labels O
( O
40%).DatasetDirect O
T O
wo O
- O
step O
clf O
. O
‚Üë O
clf O
. O
TE O
TE+ O
CRTE+ O
JOTE+ O
CR+JO O
PR O
55.29 O
56.28 O
56.48 O
57.00 O
59.89 O
BH O
58.52 O
59.17 O
59.18 O
59.59 O
60.11 O
HDA O
58.82 O
58.43 O
58.94 O
59.23 O
60.68 O
BBC O
55.23 O
57.24 O
56.46 O
58.01 O
60.78 O
T O
able O
19 O
: O
Classification O
( O
direct O
and O
two O
- O
step O
) O
accuracies O
for O
all O
the O
four O
datasets O
using O
XLM O
- O
RoBER O
T O
a O
with O
fewer O
labels O
( O
40%).718A.3 O
Another O
Inconsistency O
Example O
In O
T O
able O
11 O
, O
we O
explain O
the O
concept O
of O
pairwise O
consistencies O
and O
inconsistencies O
in O
the O
context O
- O
hypothesis O
pairs O
in O
the O
recasted O
data O
with O
an O
example O
. O
It O
depicts O
how O
different O
entailment O
results O
for O
the O
same O
context O
but O
different O
hypothesis O
can O
lead O
to O
inconsistencies O
within O
the O
model O
predictions O
. O
A.4 O
Benefits O
of O
Data O
Recasting O
There O
are O
several O
benefits O
of O
data O
recasting O
( O
Conneau O
et O
al O
. O
, O
2019 O
) O
especially O
for O
lowresource O
languages O
‚Ä¢ O
Recasting O
is O
an O
automated O
process O
and O
hence O
remove O
the O
need O
of O
expensive O
human O
annotation O
to O
labelled O
data O
. O
‚Ä¢ O
Uniform O
procedure O
of O
recasting O
data O
has O
equal O
number O
of O
context O
- O
hypothesis O
pairs O
for O
each O
label O
, O
hence O
making O
it O
neutral O
to O
statistical O
irregularities O
( O
see O
hypothesis O
bias O
experiments O
in O
Section O
5 O
) O
. O
‚Ä¢ O
Diverse O
semantic O
phenomenon O
for O
various O
classification O
tasks O
can O
be O
unified O
as O
a O
single O
task O
using O
data O
recasting.719Proceedings O
of O
the O
1st O
Conference O
of O
the O
Asia O
- O
PaciÔ¨Åc O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
10th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
720‚Äì725 O
December O
4 O
- O
7 O
, O
2020 O
. O
¬© O
2020 O
Association O
for O
Computational O
Linguistics O
Explaining O
Word O
Embeddings O
via O
Disentangled O
Representation O
Keng O
- O
Te O
Liao O
National O
Taiwan O
University O
d05922001@ntu.edu.twCheng O
- O
Syuan O
Lee O
National O
Taiwan O
University O
r07922055@ntu.edu.tw O
Zhong O
- O
Yu O
Huang O
National O
Taiwan O
University O
r06944047@ntu.edu.twShou O
- O
de O
Lin O
National O
Taiwan O
University O
sdlin@csie.ntu.edu.tw O
Abstract O
Disentangled O
representations O
have O
attracted O
increasing O
attention O
recently O
. O
However O
, O
how O
to O
transfer O
the O
desired O
properties O
of O
disentanglement O
to O
word O
representations O
is O
unclear O
. O
In O
this O
work O
, O
we O
propose O
to O
transform O
typical O
dense O
word O
vectors O
into O
disentangled O
embeddings O
featuring O
improved O
interpretability O
via O
encoding O
polysemous O
semantics O
separately O
. O
We O
also O
found O
the O
modular O
structure O
of O
our O
disentangled O
word O
embeddings O
helps O
generate O
more O
efÔ¨Åcient O
and O
effective O
features O
for O
natural O
language O
processing O
tasks O
. O
1 O
Introduction O
Disentangled O
representations O
are O
known O
to O
represent O
interpretable O
factors O
in O
separated O
dimensions O
. O
This O
property O
can O
potentially O
help O
people O
understand O
or O
discover O
knowledge O
in O
the O
embeddings O
. O
In O
natural O
language O
processing O
( O
NLP O
) O
, O
works O
of O
disentangled O
representations O
have O
shown O
notable O
impacts O
on O
sentence O
and O
document O
- O
level O
applications O
. O
For O
example O
, O
Larsson O
et O
al O
. O
( O
2017 O
) O
and O
Melnyk O
et O
al O
. O
( O
2017 O
) O
proposed O
to O
disentangle O
sentiment O
and O
semantic O
of O
sentences O
. O
By O
manipulating O
sentiment O
factors O
, O
the O
machine O
can O
rewrite O
a O
sentence O
with O
different O
sentiment O
. O
Brunner O
et O
al O
. O
( O
2018 O
) O
also O
demonstrated O
sentence O
generation O
while O
more O
focusing O
on O
syntactic O
factors O
such O
as O
part O
- O
of O
- O
speech O
tags O
. O
For O
document O
- O
level O
applications O
, O
Jain O
et O
al O
. O
( O
2018 O
) O
presented O
a O
learning O
algorithm O
which O
embeds O
biomedical O
abstracts O
disentangling O
populations O
, O
interventions O
and O
outcomes O
. O
Regarding O
word O
- O
level O
disentanglement O
, O
Athiwaratkun O
and O
Wilson O
( O
2017 O
) O
proposed O
mixture O
of O
Gaussian O
models O
which O
can O
disentangle O
meanings O
of O
polysemous O
words O
into O
two O
or O
three O
clusters O
. O
It O
has O
a O
connection O
with O
unsupervised O
sense O
representations O
( O
Camacho O
- O
Collados O
and O
Pilehvar O
, O
2018 O
) O
which O
is O
an O
active O
research O
topic O
in O
the O
community O
. O
In O
this O
work O
, O
we O
focus O
on O
word O
- O
level O
disentanglement O
and O
introduce O
an O
idea O
of O
transforming O
dense O
word O
embeddings O
such O
as O
GloVe O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
or O
word2vec O
( O
Mikolov O
et O
al O
. O
, O
2013b O
) O
into O
disentangled O
word O
embeddings O
( O
DWE O
) O
. O
The O
main O
feature O
of O
our O
DWE O
is O
that O
it O
can O
be O
segmented O
into O
multiple O
sub O
- O
embeddings O
or O
sub O
- O
areas O
as O
illustrated O
in O
Figure O
1 O
. O
In O
the O
Ô¨Ågure O
, O
each O
sub O
- O
area O
encodes O
information O
relevant O
to O
one O
speciÔ¨Åc O
topical O
factor O
such O
as O
Animal O
orLocation O
. O
As O
an O
example O
, O
we O
found O
words O
similar O
to O
‚Äú O
turkey O
‚Äù O
are O
‚Äú O
geese O
‚Äù O
, O
‚Äú O
Ô¨Çock O
‚Äù O
and O
‚Äú O
goose O
‚Äù O
in O
the O
Animal O
area O
, O
and O
the O
similar O
words O
turn O
into O
‚Äú O
Greece O
‚Äù O
, O
‚Äú O
Cyprus O
‚Äù O
and O
‚Äú O
Ankara O
‚Äù O
in O
the O
Location O
area O
. O
Figure O
1 O
: O
Disentangled O
embedding O
with O
factors O
Animal O
, O
Location O
andUnseen O
. O
We O
also O
found O
our O
DWE O
generally O
satisÔ¨Åes O
the O
Modularity O
andCompactness O
properties O
proposed O
by O
Higgins O
et O
al O
. O
( O
2018 O
) O
and O
Ridgeway O
and O
Mozer O
( O
2018 O
) O
which O
can O
be O
a O
deÔ¨Ånition O
of O
general O
- O
purpose O
disentangled O
representations O
. O
Also O
, O
our O
DWE O
can O
have O
the O
following O
advantages O
: O
‚Ä¢Explaining O
Underlying O
Knowledge O
The O
multi O
- O
senses O
of O
words O
can O
be O
extracted O
and O
separately O
encoded O
despite O
the O
learning O
algorithm O
of O
the O
original O
word O
embeddings O
( O
e.g. O
GloVe O
) O
does O
not O
do O
disambiguation O
. O
As O
a O
result O
, O
the O
encoded O
semantic O
can O
be O
presented O
in O
an O
intuitive O
way O
for O
examination O
. O
‚Ä¢Modular O
and O
Compact O
Features O
Each O
sub O
- O
area O
of O
our O
DWE O
can O
itself O
be O
informative O
features O
. O
The O
advantage O
is O
that O
people720are O
free O
to O
abandon O
features O
in O
sub O
- O
areas O
irrelevant O
to O
the O
given O
downstream O
tasks O
while O
still O
achieving O
competitive O
performance O
. O
In O
Section O
4 O
, O
we O
show O
that O
using O
the O
compact O
features O
is O
not O
only O
efÔ¨Åcient O
but O
also O
helps O
improve O
performance O
on O
downstream O
tasks O
. O
‚Ä¢Quality O
Preservation O
In O
addition O
to O
higher O
interpretability O
, O
our O
DWE O
preserves O
co O
- O
occurrence O
statistics O
information O
in O
the O
original O
word O
embeddings O
. O
We O
found O
it O
also O
helps O
preserve O
the O
performance O
on O
downstream O
tasks O
including O
word O
similarity O
, O
word O
analogy O
, O
POS O
- O
tagging O
, O
chunking O
, O
and O
named O
entity O
recognition O
. O
2 O
Obtaining O
Disentangled O
Word O
Representations O
2.1 O
Problem O
DeÔ¨Ånition O
Our O
goal O
is O
transforming O
N O
d O
- O
dimensional O
dense O
word O
vectors O
X‚ààRN√ódinto O
disentangled O
embeddingsZ‚ààRN√ódby O
leveraging O
a O
set O
of O
binary O
attributesA={a1, O
... O
,aM}labelled O
on O
words O
. O
Zis O
expected O
to O
have O
two O
properties O
. O
The O
Ô¨Årst O
one O
is O
preserving O
word O
features O
encoded O
in O
X. O
More O
speciÔ¨Åcally O
, O
we O
require O
XXT‚âàZZTas O
pointed O
out O
by O
Levy O
and O
Goldberg O
( O
2014 O
) O
that O
typical O
dense O
word O
embeddings O
can O
be O
regarded O
as O
factorizing O
co O
- O
ocurrence O
statistics O
matrices O
. O
The O
second O
property O
is O
that O
Zcan O
be O
decomposed O
intoM+1sub O
- O
embedding O
sets O
Za1, O
... O
,ZaM O
andZunseen O
, O
where O
each O
sub O
- O
embedding O
set O
encodes O
information O
only O
relevant O
to O
the O
corresponding O
attribute O
. O
For O
example O
, O
Za1is O
expected O
to O
be O
relevant O
toa1and O
irrelevant O
to O
a2, O
... O
,aM. O
Information O
inXnot O
relevant O
to O
any O
attributes O
in O
Ais O
then O
encoded O
inZunseen O
. O
An O
example O
of O
transforming O
XintoZwith O
two O
attributes O
, O
Animal O
andLocation O
, O
is O
illustrated O
in O
Figure O
1 O
. O
For O
modelling O
the O
relevance O
between O
subembeddings O
and O
attributes O
, O
we O
use O
mutual O
informationI(Za O
, O
a)as O
learning O
objectives O
, O
where O
ais O
an O
arbitrary O
attribute O
in O
A. O
2.2 O
Transformation O
with O
Quality O
Preservation O
We O
obtain O
Zby O
transforming O
Xby O
a O
matrix O
W‚ààRd√ód O
. O
That O
is O
, O
Z O
= O
XW O
. O
To O
ensure O
XXT‚âàZZT O
, O
an O
additional O
constraint O
WWT= O
Iis O
included O
. O
ZZT= O
( O
XW)(XW)T= O
X(WWT)XT O
= O
XXTifWWT O
= O
Iholds.2.3 O
OptimizingI(Za O
, O
a O
) O
Letza O
, O
ibe O
thei O
- O
th O
row O
inZa O
. O
By O
derivation O
, O
I(Za O
, O
a O
) O
= O
Œ£N O
i=1p(zi)p(a|za O
, O
i)/bracketleftbig O
logp(a|za O
, O
i)‚àílogp(a)/bracketrightbig O
‚âà1 O
NŒ£N O
i=1p(a|za O
, O
i)/bracketleftbig O
logp(a|za O
, O
i)‚àílogp(a)/bracketrightbig O
We O
let O
logp(a)be O
constant O
and O
replace O
p(a|z)with O
a O
parametrized O
model O
qŒ∏(a|z O
) O
. O
By O
experiments O
, O
we O
found O
logistic O
regression O
with O
parameter O
Œ∏is O
sufÔ¨Åcient O
to O
be O
qŒ∏(a|z O
) O
. O
Intuitively O
, O
highI(Za O
, O
a O
) O
meansZaare O
informative O
features O
for O
a O
classiÔ¨Åer O
to O
distinguish O
whether O
words O
has O
attribute O
a. O
When O
increasing O
I(Za O
, O
a)by O
optimizing O
qŒ∏(a|z O
) O
, O
we O
found O
a O
strategy O
helping O
generate O
higher O
quality O
Z. O
The O
strategy O
is O
letting O
Zabe O
features O
to O
reconstruct O
original O
vectors O
for O
words O
having O
attribute O
a. O
For O
words O
with O
a O
, O
the O
approach O
becomes O
a O
semi O
- O
supervised O
learning O
architecture O
which O
attempts O
to O
predict O
labels O
and O
reconstruct O
inputs O
simultaneously O
. O
The O
loss O
function O
L(W O
, O
Œ∏ O
, O
œÜ O
) O
for O
maximizing O
I(Za O
, O
a)is O
as O
follow O
: O
‚àí1 O
NŒ£N O
i=1qŒ∏(a|za O
, O
i O
) O
+ O
ŒªIa O
, O
i||xi‚àíœÜ(za O
, O
i)||2 O
2 O
Ia O
, O
i=/braceleftBigg O
1wheni O
- O
th O
word O
has O
attribute O
a O
0wheni O
- O
th O
word O
does O
not O
have O
a O
( O
1 O
) O
whereœÜis O
single O
and O
fully O
- O
connected O
layer O
, O
xiis O
the O
original O
i O
- O
th O
word O
‚Äôs O
vector O
in O
X O
, O
andŒªis O
a O
hyper O
- O
parameter O
. O
We O
set O
Œª=1 O
din O
all O
experiments O
. O
2.4 O
Learning O
to O
Generate O
Sub O
- O
embedding O
Za O
As O
discussed O
in O
2.3 O
that O
high O
I(Za O
, O
a)indicates O
Zaare O
informative O
features O
for O
classiÔ¨Åcation O
, O
we O
propose O
to O
regard O
sub O
- O
embedding O
generation O
as O
a O
feature O
selection O
problem O
. O
More O
speciÔ¨Åcally O
, O
we O
apply O
sparsity O
constraint O
on O
Z. O
Ideally O
, O
when O
predictinga O
, O
a O
smaller O
number O
of O
dimensions O
of O
Z O
are O
selected O
as O
the O
informative O
features O
, O
which O
are O
regarded O
asZa O
. O
In O
this O
work O
, O
we O
use O
Variational O
Dropout O
( O
Kingma O
et O
al O
. O
, O
2015 O
; O
Molchanov O
et O
al O
. O
, O
2017 O
) O
as O
the O
sparsity O
constraint O
. O
At O
each O
iteration O
of O
training O
, O
a O
set O
of O
multiplicative O
noise O
Œæis O
sampled O
from O
a O
normal O
distribution O
N(1,Œ±a O
= O
pa O
1‚àípa)and O
injected O
onZ. O
That O
is O
, O
the O
prediction O
and O
reconstruction O
is O
done O
by O
Œ∏(Œæ‚äôZ)andœÜ(Œæ‚äôZ O
) O
. O
The O
parameterŒ±a‚ààRdis O
jointly O
learned O
with O
W O
, O
Œ∏ O
, O
andœÜ O
. O
Afterwards O
, O
d O
- O
dimensional O
dropout O
rates721pa O
= O
sigmoid O
( O
logŒ±a)can O
be O
obtained O
. O
For O
each O
attributeainA O
, O
the O
dimensions O
with O
dropout O
rates O
lower O
than O
50 O
% O
are O
normally O
regarded O
as O
Za O
. O
We O
would O
like O
to O
emphasize O
that O
the O
learned O
dropout O
rates O
are O
not O
binary O
values O
. O
Therefore O
, O
deciding O
the O
length O
of O
sub O
- O
embeddings O
can O
actually O
depend O
on O
users O
preferences O
or O
tasks O
requirements O
. O
For O
example O
, O
users O
can O
obtain O
more O
compact O
and O
pureZaby O
selecting O
dimensions O
with O
dropout O
rates O
lower O
than O
10 O
% O
, O
or O
get O
more O
thorough O
yet O
less O
disentangledZaby O
setting O
the O
threshold O
be O
70 O
% O
. O
To O
encourage O
disentanglement O
when O
handling O
multiple O
attributes O
, O
we O
include O
additional O
loss O
functions O
on O
dropout O
rates O
. O
Let O
a O
M O
- O
dimensional O
vector O
Pbe1‚àípafor O
allainAin O
a O
speciÔ¨Åc O
dimension O
. O
The O
idea O
is O
to O
minimize O
/ O
producttextM O
i=1Piwith O
constraint O
Œ£M O
i=1Pi= O
1 O
. O
The O
optimal O
solution O
is O
that O
the O
dimension O
is O
relevant O
to O
only O
one O
attribute O
a O
/ O
primewhere O
1‚àípa O
/ O
prime‚âà1 O
. O
In O
implementation O
, O
we O
minimize O
the O
following O
loss O
function O
Œ£M O
i=1logPi+Œ≤||Œ£M O
i=1Pi‚àí1||2 O
2 O
( O
2 O
) O
We O
setŒ≤= O
1 O
in O
the O
experiments O
, O
and O
equation O
1 O
and O
2 O
are O
optimized O
jointly O
. O
To O
generate O
Zunseen O
, O
we O
initially O
select O
a O
set O
of O
dimensions O
and O
constrain O
their O
dropout O
rates O
be O
always O
larger O
than O
50 O
% O
. O
The O
number O
of O
dimensions O
ofZunseen O
is O
a O
hyper O
- O
parameter O
. O
After O
selection O
, O
we O
do O
not O
apply O
equation O
2 O
on O
the O
selected O
dimensions O
. O
3 O
Evaluation O
3.1 O
Word O
Embeddings O
and O
Attributes O
We O
transform O
300 O
- O
dimensional O
GloVe1into O
DWE O
. O
The O
300 O
- O
dimensional O
GloVe O
is O
denoted O
by O
GloVe300 O
. O
For O
word O
attributes O
A O
, O
we O
use O
labels O
in O
WordStat2 O
. O
WordStat O
contains O
45 O
kinds O
of O
attributes O
labeled O
on O
70,651 O
words O
. O
Among O
the O
attributes O
, O
we O
select O
5 O
high O
- O
level O
and O
easily O
understandable O
attributes O
: O
Artifact O
, O
Location O
, O
Animal O
, O
Adjective O
( O
ADJ O
) O
andAdverb O
( O
ADV O
) O
for O
our O
experiments O
. O
The O
number O
of O
words O
labelled O
with O
these O
5 O
attributes O
is O
13,337 O
. O
After O
training O
, O
all O
pre O
- O
trained O
GloVe O
vectors O
are O
transformed O
by O
the O
learned O
matrix O
W(i.e O
. O
XW O
) O
for O
downstream O
evaluations O
. O
The O
number O
of O
learned O
dimensions O
for O
each O
attribute O
is O
illustrated O
in O
Figure O
2 O
, O
where O
the O
threshold O
of O
dropout O
rates O
for O
dimension O
selection O
is O
50 O
% O
. O
1https://nlp.stanford.edu/projects/glove/ O
2https://provalisresearch.com/products/content-analysissoftware/ O
Figure O
2 O
: O
Disentangled O
embedding O
with O
Ô¨Åve O
attributes O
: O
Artifact O
, O
Location O
, O
Animal O
, O
Adjective O
andAdverb O
. O
The O
remaining O
dimensions O
are O
viewed O
as O
Unseen O
. O
MEN O
SimLex O
BATS O
GA O
GloVe-300 O
0.749 O
0.369 O
18.83 O
63.58 O
DWE O
0.764 O
0.390 O
18.75 O
62.30 O
Table O
1 O
: O
Word O
similarity O
and O
analogy O
performance O
. O
POS O
Chunking O
NER O
GloVe-300 O
65.0 O
64.9 O
65.2 O
DWE O
67.2 O
66.3 O
66.1 O
Table O
2 O
: O
POS O
- O
tag O
, O
chunking O
and O
NER O
performance O
. O
3.2 O
Evaluation O
of O
Quality O
Preservation O
We O
Ô¨Årstly O
examine O
whether O
DWE O
can O
preserve O
features O
encoded O
in O
GloVe-300 O
. O
The O
examination O
is O
done O
by O
intrinsic O
evaluations O
including O
the O
following O
tasks O
and O
datasets O
. O
‚Ä¢Word O
Similarity O
: O
Marco O
, O
Elia O
and O
Nam O
( O
MEN O
) O
( O
Bruni O
et O
al O
. O
, O
2014 O
) O
and O
SimLex-999 O
( O
Hill O
et O
al O
. O
, O
2015 O
) O
. O
‚Ä¢Word O
Analogy O
: O
Bigger O
Analogy O
Test O
Set O
( O
BATS O
) O
( O
Gladkova O
et O
al O
. O
, O
2016 O
) O
, O
Google O
Analogy O
( O
GA O
) O
( O
Mikolov O
et O
al O
. O
, O
2013a O
) O
. O
‚Ä¢POS O
tagging O
, O
Chunking O
and O
Named O
Entity O
Recognition O
( O
NER O
): O
CoNLL O
2003 O
( O
Sang O
and O
Meulder O
, O
2003 O
; O
Li O
et O
al O
. O
, O
2017 O
) O
. O
‚Ä¢QVEC O
- O
CCA3(Tsvetkov O
et O
al O
. O
, O
2015 O
): O
The O
performance O
is O
measured O
by O
semantic O
and O
syntactic O
CCA O
. O
As O
shown O
in O
Table O
1 O
, O
2 O
and O
3 O
, O
DWE O
can O
preserve O
performance O
of O
GloVe-300 O
on O
various O
NLP O
tasks O
. O
Probably O
due O
to O
the O
additional O
information O
of O
word O
attributes O
, O
DWE O
can O
have O
slightly O
better O
performance O
than O
GloVe-300 O
on O
seven O
of O
the O
tasks O
3.3 O
Attribute O
ClassiÔ¨Åcation O
We O
design O
an O
attribute O
classiÔ¨Åcation O
task O
for O
examining O
whether O
the O
DWE O
can O
meet O
requirements O
described O
in O
Section O
2.3 O
. O
We O
use O
logistic O
regression O
and O
take O
sub O
- O
embeddings O
Zaas O
input O
features O
for O
3https://github.com/ytsvetko/qvec722Semantic O
Syntactic O
GloVe-300 O
0.473 O
0.341 O
DWE O
0.474 O
0.348 O
Table O
3 O
: O
QVEC O
- O
CCA O
evaluation O
. O
Artifact O
Location O
Animal O
ADJ O
ADV O
Zartifact O
77.8 O
71.0 O
68.0 O
65.5 O
71.2 O
Zlocation O
59.2 O
83.8 O
64.0 O
60.5 O
69.8 O
Zanimal O
58.5 O
67.5 O
84.2 O
60.2 O
71.0 O
Zadj O
69.8 O
70.7 O
68.2 O
82.0 O
72.5 O
Zadv O
59.0 O
72.5 O
71.8 O
71.5 O
84.2 O
Zunseen O
54.8 O
70.0 O
66.5 O
60.2 O
68.8 O
Table O
4 O
: O
Attribute O
classiÔ¨Åcation O
accuracies O
( O
% O
) O
. O
verifying O
the O
performance O
of O
classiÔ¨Åcation O
by O
crossvalidation O
. O
For O
each O
attribute O
, O
We O
randomly O
sample O
400 O
data O
for O
testing O
. O
The O
numbers O
of O
positive O
and O
negative O
data O
for O
testing O
are O
balanced O
. O
Therefore O
, O
a O
random O
predictor O
would O
get O
around O
50 O
% O
accuracy O
in O
each O
classiÔ¨Åcation O
task O
. O
The O
binary O
classiÔ¨Åcation O
accuracies O
are O
shown O
in O
Table O
4 O
. O
Take O
the O
second O
column O
of O
Table O
4 O
for O
example O
. O
For O
distinguishing O
whether O
a O
word O
can O
be O
location O
, O
taking O
Zlocation O
as O
features O
for O
training O
a O
classiÔ¨Åer O
achieves O
the O
highest O
accuracy O
83.8 O
% O
. O
On O
the O
other O
hand O
, O
the O
accuracy O
reported O
in O
the O
second O
row O
of O
Table O
4 O
implies O
that O
Zlocation O
are O
less O
informative O
features O
for O
other O
attributes O
. O
Similar O
results O
can O
also O
be O
observed O
for O
other O
attributes O
. O
3.4 O
Disentangled O
Interpretability O
We O
provide O
some O
examples O
to O
demonstrate O
that O
words O
having O
ambiguous O
or O
different O
aspects O
of O
semantics O
can O
be O
disentangled O
. O
Table O
5 O
shows O
the O
results O
of O
nearby O
words O
. O
As O
can O
be O
seen O
, O
querying O
a O
word O
inZawith O
different O
attributes O
can O
help O
discover O
the O
ambiguous O
semantics O
implicitly O
encoded O
in O
the O
original O
word O
vectors O
X. O
The O
results O
also O
show O
thatZunseen O
does O
capture O
meaningful O
information O
having O
little O
relevance O
to O
given O
attributes O
. O
4 O
Application O
: O
Compact O
Features O
for O
Downstream O
Tasks O
Here O
we O
demonstrate O
an O
application O
of O
the O
modularity O
andcompactness O
properties O
of O
our O
DWE O
. O
We O
Ô¨Årstly O
aim O
to O
show O
the O
sub O
- O
embeddings O
can O
directly O
be O
informative O
features O
and O
can O
outperform O
GloVe O
with O
the O
same O
number O
of O
dimensions O
. O
With O
the O
high O
interpretability O
, O
selecting O
relevantQuery O
Vectors O
Nearby O
Words O
turkeyZanimal O
geese O
, O
Ô¨Çock O
, O
goose O
turkeyZlocation O
greece O
, O
cyprus O
, O
ankara O
mouseZanimal O
mice O
, O
rat O
, O
rats O
mouseZartifact O
keyboard O
, O
joystick O
, O
buttons O
japanZlocation O
korea O
, O
vietnam O
, O
singapore O
japanZunseen O
japanese O
, O
yakuza O
, O
yen O
appleZartifact O
macintosh O
, O
software O
, O
mac O
appleZunseen O
mango O
, O
cherry O
, O
tomato O
Table O
5 O
: O
Results O
of O
nearby O
words O
. O
sub O
- O
embeddings O
could O
be O
intuitive O
. O
Secondly O
, O
we O
will O
demonstrate O
that O
if O
deciding O
to O
Ô¨Åne O
- O
tune O
word O
vectors O
for O
a O
given O
downstream O
task O
, O
by O
using O
our O
DWE O
, O
we O
can O
focus O
on O
updating O
the O
relevant O
subembedding O
instead O
of O
the O
whole O
embedding O
. O
The O
advantage O
is O
that O
it O
reduces O
the O
number O
of O
learning O
parameters O
. O
Also O
, O
it O
could O
be O
regarded O
as O
a O
dimensional O
and O
interpretable O
regularization O
technique O
reducing O
overÔ¨Åtting O
. O
We O
take O
a O
sentiment O
analysis O
task O
, O
IMDB O
movie O
review O
classiÔ¨Åcation(Maas O
et O
al O
. O
, O
2011 O
) O
, O
for O
experiments O
. O
Intuitively O
, O
ADJ O
andADV O
should O
be O
the O
most O
relevant O
attributes O
in O
A. O
We O
then O
select O
50 O
dimensions O
from O
Z‚ààR300with O
the O
lowest O
dropout O
rates O
in O
ADJ O
andADV O
sub O
- O
areas O
for O
comparing O
with O
50 O
- O
dimensional O
GloVe4(GloVe-50 O
) O
. O
The O
embeddings O
with O
the O
selected O
dimensions O
are O
denoted O
byZadj+adv-50 O
. O
When O
tuning O
our O
DWE O
with O
the O
classiÔ¨Åer O
, O
we O
update O
the O
52 O
dimensions O
( O
Zadj‚ààR23andZadv‚ààR29 O
) O
of O
DWE O
and O
compare O
it O
with O
GloVe-300 O
. O
The O
document O
representations O
for O
classiÔ¨Åcation O
is O
averaged O
word O
embeddings O
. O
The O
classiÔ¨Åer O
is O
a O
logistic O
regression O
. O
When O
tuning O
the O
input O
word O
embeddings O
, O
we O
update O
the O
embeddings O
with O
gradient O
propagated O
from O
the O
classiÔ¨Åer O
. O
The O
results O
are O
listed O
in O
Table O
6 O
. O
From O
the O
table O
, O
we O
can O
see O
Zadj+adv-50 O
directly O
outperforms O
GloVe-50 O
without O
tuning O
. O
A O
possible O
explanation O
is O
that O
GloVe-50 O
is O
forced O
to O
encode O
information O
less O
relevant O
to O
the O
sentiments O
, O
making O
it O
less O
effective O
thanZadj+adv-50 O
in O
this O
task O
. O
In O
the O
Ô¨Åne O
- O
tuning O
experiments O
, O
DWE O
can O
show O
slightly O
higher O
accuracy O
than O
GloVe-300 O
by O
updating O
only O
52 O
instead O
of O
300 O
dimensional O
features O
. O
4https://nlp.stanford.edu/projects/glove/723Feature O
Without O
Tuning O
After O
Tuning O
GloVe-50 O
76.55 O
86.72 O
Zadj+adv-50 O
79.78 O
87.60 O
GloVe-300 O
83.85 O
87.72 O
DWE O
83.67 O
87.84 O
Table O
6 O
: O
ClassiÔ¨Åcation O
accuracies O
( O
% O
) O
on O
IMDB O
dataset O
. O
5 O
Conclusion O
In O
this O
work O
, O
we O
propose O
a O
new O
deÔ¨Ånition O
and O
learning O
algorithm O
for O
obtaining O
disentangled O
word O
representations O
. O
As O
a O
result O
, O
the O
disentangled O
word O
vectors O
can O
show O
higher O
interpretability O
and O
preserve O
performance O
on O
various O
NLP O
tasks O
. O
We O
can O
also O
see O
the O
ambiguous O
semantics O
hidden O
in O
typical O
dense O
word O
embeddings O
can O
be O
extracted O
and O
separately O
encoded O
. O
Finally O
, O
we O
showed O
the O
disentangled O
word O
vectors O
can O
help O
generate O
compact O
and O
effective O
features O
for O
NLP O
applications O
. O
In O
the O
future O
, O
we O
would O
like O
to O
investigate O
whether O
similar O
effects O
can O
be O
found O
from O
non O
- O
distributional O
or O
contextualized O
word O
embeddings O
. O
Abstract O
Most O
previous O
work O
on O
knowledge O
graph O
completion O
conducted O
single O
- O
view O
prediction O
or O
calculation O
for O
candidate O
triple O
evaluation O
, O
based O
only O
on O
the O
content O
information O
of O
the O
candidate O
triples O
. O
This O
paper O
describes O
a O
novel O
multi O
- O
view O
classiÔ¨Åcation O
model O
for O
knowledge O
graph O
completion O
, O
where O
multiple O
classiÔ¨Åcation O
views O
are O
performed O
based O
on O
both O
content O
and O
context O
information O
for O
candidate O
triple O
evaluation O
. O
Each O
classiÔ¨Åcation O
view O
evaluates O
the O
validity O
of O
a O
candidate O
triple O
from O
a O
speciÔ¨Åc O
viewpoint O
, O
based O
on O
the O
content O
information O
inside O
the O
candidate O
triple O
and O
the O
context O
information O
nearby O
the O
triple O
. O
These O
classiÔ¨Åcation O
views O
are O
implemented O
by O
a O
uniÔ¨Åed O
neural O
network O
and O
the O
classiÔ¨Åcation O
predictions O
are O
weightedly O
integrated O
to O
obtain O
the O
Ô¨Ånal O
evaluation O
. O
Experiments O
show O
that O
, O
the O
multi O
- O
view O
model O
brings O
very O
signiÔ¨Åcant O
improvements O
over O
previous O
methods O
, O
and O
achieves O
the O
new O
state O
- O
of O
- O
the O
- O
art O
on O
two O
representative O
datasets O
. O
We O
believe O
that O
, O
the O
Ô¨Çexibility O
and O
the O
scalability O
of O
the O
multi O
- O
view O
classiÔ¨Åcation O
model O
facilitates O
the O
introduction O
of O
additional O
information O
and O
resources O
for O
better O
performance O
. O
1 O
Introduction O
Knowledge O
graph O
( O
KG O
) O
is O
a O
typical O
kind O
of O
graphstructured O
knowledge O
base O
( O
KB O
) O
. O
Nowdays O
, O
there O
exist O
many O
famous O
KGs O
such O
as O
YAGO O
( O
Suchanek O
et O
al O
. O
, O
2007 O
) O
, O
Freebase O
( O
Bollacker O
et O
al O
. O
, O
2008 O
) O
and O
DBpedia O
( O
Lehmann O
et O
al O
. O
, O
2015 O
) O
. O
Large O
- O
scale O
KGs O
are O
widely O
used O
in O
many O
applications O
such O
as O
semantic O
searching O
( O
Kasneci O
et O
al O
. O
, O
2008 O
; O
Schuhmacher O
and O
Ponzetto O
, O
2014 O
; O
Xiong O
et O
al O
. O
, O
2017 O
) O
, O
question O
answering O
( O
Zhang O
et O
al O
. O
, O
2016 O
; O
Hao O
et O
al O
. O
, O
2017 O
) O
and O
machine O
reading O
( O
Yang O
and O
Mitchell O
, O
‚àóJoint O
Ô¨Årst O
author O
. O
Guo O
participated O
in O
the O
optimization O
of O
this O
work O
during O
the O
internship O
in O
Baidu.2017 O
) O
. O
A O
KG O
contains O
a O
set O
of O
triples O
indicating O
facts O
, O
each O
of O
which O
is O
composed O
of O
a O
head O
entity O
, O
a O
tailentity O
, O
and O
a O
relation O
indicating O
the O
relationship O
between O
the O
two O
entities O
. O
It O
is O
nearly O
impossible O
to O
collect O
a O
complete O
set O
of O
facts O
or O
triples O
for O
a O
KG O
, O
especially O
in O
open O
domains O
. O
In O
fact O
, O
many O
valuable O
valid O
triples O
are O
missing O
even O
for O
the O
existing O
wellbuilt O
large O
- O
scale O
KGs O
such O
as O
Freebase O
( O
Socher O
et O
al O
. O
, O
2013 O
; O
West O
et O
al O
. O
, O
2014 O
) O
. O
Many O
researchers O
devote O
their O
efforts O
to O
the O
problem O
of O
knowledge O
graph O
completion O
( O
KGC O
) O
, O
the O
core O
operation O
of O
which O
is O
to O
evaluate O
the O
validity O
of O
candidate O
triples O
. O
Previous O
work O
on O
KGC O
mainly O
include O
two O
groups O
, O
embedding O
- O
based O
methods O
and O
classiÔ¨Åcation O
- O
based O
methods O
. O
Embedding O
- O
based O
models O
learn O
embeddings O
for O
entities O
and O
relations O
, O
and O
evaluate O
candidate O
triples O
based O
on O
the O
embeddings O
and O
speciÔ¨Åc O
distance O
metrics O
. O
Representative O
models O
include O
TransE O
( O
Bordes O
et O
al O
. O
, O
2013 O
) O
and O
its O
extensions O
( O
Wang O
et O
al O
. O
, O
2014 O
; O
Lin O
et O
al O
. O
, O
2015b O
; O
Ji O
et O
al O
. O
, O
2015 O
; O
Nguyen O
et O
al O
. O
, O
2016 O
) O
, O
DistMult O
( O
Yang O
et O
al O
. O
, O
2015 O
) O
and O
ComplEx O
( O
Trouillon O
et O
al O
. O
, O
2016 O
) O
. O
ClassiÔ¨Åcation O
- O
based O
models O
learn O
neural O
networks O
to O
evaluate O
the O
validity O
of O
candidate O
triples O
. O
Representative O
models O
include O
ConvE O
( O
Dettmers O
et O
al O
. O
, O
2018 O
) O
and O
ConvKB O
( O
Nguyen O
, O
2017 O
) O
. O
The O
major O
advantage O
of O
classiÔ¨Åcation O
- O
based O
methods O
is O
that O
they O
directly O
model O
the O
evaluation O
of O
the O
validity O
of O
candidate O
triples O
, O
probably O
leading O
to O
better O
performance O
. O
Most O
of O
these O
previous O
work O
conducted O
single O
- O
view O
prediction O
based O
on O
content O
information O
, O
that O
is O
, O
evaluating O
a O
candidate O
triple O
according O
to O
a O
single O
distance O
metric O
or O
classiÔ¨Åcation O
schema O
, O
resorting O
to O
information O
restricted O
in O
the O
scope O
of O
the O
candidate O
triple O
. O
We O
believe O
that O
multiple O
learning O
views O
for O
triple O
evaluation O
as O
well O
as O
context O
information O
of O
the O
candidate O
triple O
would O
contribute O
to O
better O
performance O
. O
In O
this O
work O
, O
we O
propose O
for O
KGC O
a O
novel O
multiview O
classiÔ¨Åcation O
model O
, O
where O
multiple O
classiÔ¨Å-726Figure O
1 O
: O
Illustration O
of O
sub O
- O
graphs O
corresponding O
to O
the O
learning O
views O
. O
The O
colored O
nodes O
indicate O
the O
head O
and O
tail O
entities O
of O
the O
candidate O
triple O
. O
The O
bold O
nodes O
and O
edges O
are O
the O
elements O
in O
the O
retrieved O
sub O
- O
graphs O
. O
The O
question O
marks O
indicate O
the O
elements O
to O
be O
predicted O
. O
cation O
views O
are O
performed O
to O
estimate O
the O
validity O
of O
a O
candidate O
triple O
, O
based O
on O
both O
content O
and O
context O
information O
of O
the O
triple O
. O
There O
are O
four O
classiÔ¨Åcation O
views O
for O
candidate O
triple O
evaluation O
. O
Each O
of O
the O
Ô¨Årst O
three O
views O
performs O
component O
prediction O
, O
where O
a O
speciÔ¨Åc O
component O
of O
the O
candidate O
triple O
is O
predicted O
according O
to O
the O
other O
two O
components O
as O
well O
as O
its O
nearby O
triples O
. O
The O
last O
view O
performs O
plausibility O
prediction O
, O
where O
the O
plausibility O
of O
the O
candidate O
triple O
is O
predicted O
according O
to O
its O
components O
as O
well O
as O
its O
nearby O
triples O
. O
The O
prediction O
conditions O
of O
these O
views O
investigate O
both O
content O
and O
context O
information O
of O
the O
candidate O
triple O
, O
that O
is O
, O
the O
components O
in O
the O
candidate O
triple O
, O
and O
the O
triples O
nearby O
the O
candidate O
triple O
. O
The O
content O
and O
context O
information O
can O
be O
represented O
as O
a O
sub O
- O
graph O
surrounding O
the O
candidate O
triple O
. O
These O
classiÔ¨Åcation O
views O
are O
implemented O
by O
a O
uniÔ¨Åed O
neural O
network O
with O
shared O
embedding O
and O
encoding O
layers O
and O
separated O
prediction O
layers O
, O
and O
the O
classiÔ¨Åcation O
predictions O
are O
integrated O
by O
a O
weighted O
integration O
procedure O
for O
better O
candidate O
triple O
evaluation O
. O
In O
the O
uniÔ¨Åed O
neural O
network O
, O
the O
sub O
- O
graphs O
indicating O
the O
content O
and O
context O
of O
the O
candidate O
triples O
are O
encoded O
in O
a O
sequencial O
manner O
, O
by O
converting O
the O
sub O
- O
graphs O
into O
sequential O
tree O
representations O
. O
It O
facilitates O
the O
utilization O
of O
advanced O
encoders O
such O
as O
BiLSTM O
or O
Transformer O
. O
We O
experiment O
on O
two O
widely O
used O
benchmark O
datasets O
, O
FB15k-237 O
and O
WN18RR O
, O
speciÔ¨Åc O
versions O
of O
Freebase O
and O
WordNet O
. O
We O
Ô¨Ånd O
that O
the O
multi O
- O
view O
model O
achieves O
the O
new O
state O
- O
of O
- O
the O
- O
art O
, O
signiÔ¨Åcantly O
outperforming O
pervious O
work O
on O
KGC O
. O
We O
also O
Ô¨Ånd O
that O
we O
can O
promote O
the O
efÔ¨Åciency O
of O
the O
multi O
- O
view O
model O
in O
realistic O
applications O
, O
by O
a O
coarse O
- O
to-Ô¨Åne O
strategy O
where O
the O
Ô¨Årst O
two O
views O
are O
performed O
to O
give O
a O
list O
of O
candidates O
, O
and O
theoverall O
model O
is O
then O
performed O
to O
evaluate O
these O
candidates O
. O
We O
believe O
that O
, O
the O
Ô¨Çexibility O
and O
the O
scalability O
of O
the O
multi O
- O
view O
classiÔ¨Åcation O
model O
facilitates O
the O
introduction O
of O
additional O
information O
and O
resources O
for O
better O
performance O
. O
2 O
Related O
Work O
Most O
existing O
KGC O
models O
are O
based O
on O
KG O
embeddings O
, O
which O
aims O
at O
learning O
distributed O
representations O
for O
entities O
and O
relations O
in O
a O
KG O
. O
In O
these O
models O
, O
the O
candidate O
triples O
are O
evaluated O
by O
some O
speciÔ¨Åc O
distance O
metrics O
based O
on O
the O
embeddings O
. O
These O
models O
perform O
embedding O
learning O
with O
local O
information O
in O
individual O
triples O
, O
including O
translation O
- O
based O
models O
( O
Bordes O
et O
al O
. O
, O
2013 O
; O
Wang O
et O
al O
. O
, O
2014 O
; O
Lin O
et O
al O
. O
, O
2015b O
) O
, O
semantic O
matching O
models O
( O
Yang O
et O
al O
. O
, O
2015 O
; O
Nickel O
et O
al O
. O
, O
2016 O
; O
Trouillon O
et O
al O
. O
, O
2016 O
) O
, O
and O
neural O
network O
models O
( O
Dettmers O
et O
al O
. O
, O
2018 O
; O
Jiang O
et O
al O
. O
, O
2019 O
; O
Nguyen O
, O
2017 O
) O
. O
There O
also O
exist O
KGC O
models O
based O
on O
classiÔ¨Åcation O
, O
where O
classiÔ¨Åers O
are O
learnt O
to O
evaluate O
the O
validity O
of O
candidate O
triples O
( O
Dettmers O
et O
al O
. O
, O
2018 O
; O
Nguyen O
, O
2017 O
) O
. O
Both O
kinds O
of O
previous O
work O
consider O
only O
one O
view O
, O
with O
simple O
distance O
metrics O
and O
classiÔ¨Åcation O
operations O
. O
In O
contrast O
, O
multi O
- O
view O
learning O
enables O
the O
incorporation O
of O
much O
more O
views O
that O
utilize O
internal O
and O
external O
information O
for O
triple O
evaluation O
. O
In O
recent O
years O
, O
many O
efforts O
were O
devoted O
to O
embedding O
learning O
based O
on O
non O
- O
local O
information O
such O
as O
multi O
- O
hop O
paths O
( O
Lin O
et O
al O
. O
, O
2015a O
; O
Das O
et O
al O
. O
, O
2017 O
) O
and O
k O
- O
degree O
neighborhoods O
( O
Feng O
et O
al O
. O
, O
2016 O
; O
Schlichtkrull O
et O
al O
. O
, O
2017 O
) O
. O
Some O
researchers O
also O
investigated O
graph O
embeddings O
in O
social O
network O
and O
other O
areas O
( O
Perozzi O
et O
al O
. O
, O
2014 O
; O
Grover O
and O
Leskovec O
, O
2016 O
; O
Ristoski O
and O
Paulheim O
, O
2016 O
; O
Cochez O
et O
al O
. O
, O
2017 O
) O
. O
Compared O
with O
these O
work O
, O
our O
method O
not O
only O
learns O
em-727View O
Type O
Instance O
from O
gv O
Instance O
from O
g‚àí O
v O
hr‚Üítghr‚Üít=/angbracketleftG(h O
, O
r,?),t O
/ O
angbracketrightg‚àí O
hr‚Üít=/angbracketleftG(S(h O
, O
r,?)),none O
/ O
angbracketright O
, O
s.t O
. O
S(h O
, O
r,?)/‚ààKG O
rt‚Üíhgrt‚Üíh=/angbracketleftG(?,r O
, O
t),h O
/ O
angbracketrightg‚àí O
rt‚Üíh=/angbracketleftG(S(?,r O
, O
t)),none O
/ O
angbracketright O
, O
s.t O
. O
S(?,r O
, O
t)/‚ààKG O
ht‚Üírght‚Üír=/angbracketleftG(h,?,t),r O
/ O
angbracketrightg‚àí O
ht‚Üír=/angbracketleftG(S(h,?,t)),none O
/ O
angbracketright O
, O
s.t O
. O
S(h,?,t)/‚ààKG O
hrt‚Üíghrt‚Üí=/angbracketleftG(h O
, O
r O
, O
t O
) O
, O
true O
/ O
angbracketrightg‚àí O
hrt‚Üí=/angbracketleftG(S(h O
, O
r O
, O
t O
) O
) O
, O
false O
/ O
angbracketright O
, O
s.t O
. O
S(h O
, O
r O
, O
t O
) O
/‚ààKG O
Table O
1 O
: O
Instance O
generation O
for O
each O
learning O
view O
. O
The O
Ô¨Årst O
/ O
second O
part O
in O
an O
instance O
is O
used O
as O
the O
input O
/ O
output O
for O
classiÔ¨Åcation O
. O
The O
function O
Gretrieves O
the O
sub O
- O
graph O
surrounding O
the O
candidate O
triple O
with O
the O
maximum O
height O
and O
width O
limitations O
. O
The O
function O
Sreceives O
a O
tuple O
and O
returns O
a O
randomly O
corrupted O
tuple O
that O
not O
exists O
in O
the O
KG O
, O
by O
randomly O
replacing O
a O
known O
component O
which O
is O
not O
denoted O
by O
the O
question O
mark O
. O
The O
operator O
‚àà O
incidates O
that O
a O
tuple O
is O
equal O
to O
orinside O
of O
a O
triple O
. O
beddings O
for O
individual O
entities O
and O
relations O
based O
on O
non O
- O
local O
information O
, O
but O
also O
obtains O
representations O
for O
sub O
- O
graphs O
resorting O
to O
complicated O
neural O
encoders O
. O
This O
manner O
probably O
brings O
better O
KGC O
performance O
by O
leveraging O
global O
information O
more O
effectively O
. O
3 O
Method O
: O
Multi O
- O
view O
ClassiÔ¨Åcation O
A O
knowledge O
graph O
KGcontains O
a O
set O
of O
triples O
indicating O
facts,{(h O
, O
r O
, O
t O
) O
} O
‚äÜE√óR√óE O
. O
Each O
triple O
( O
h O
, O
r O
, O
t O
) O
consists O
of O
two O
entities O
handtreferred O
to O
the O
subject O
and O
object O
of O
the O
triple O
, O
and O
a O
relation O
rreferred O
to O
the O
relationship O
between O
the O
two O
entities O
. O
EandRindicates O
the O
possible O
entity O
set O
and O
the O
possible O
relation O
set O
, O
respectively O
. O
The O
fundamental O
problem O
for O
KGC O
is O
to O
deÔ¨Åne O
a O
candidate O
triple O
evaluation O
model O
f O
: O
E√óR√óE‚Üí O
R O
, O
giving O
each O
candidate O
triple O
( O
h O
, O
r O
, O
t O
) O
a O
score O
indicting O
the O
validity O
of O
the O
triple O
. O
3.1 O
ClassiÔ¨Åcation O
Views O
We O
adopt O
a O
multi O
- O
view O
classiÔ¨Åcation O
model O
for O
KGC O
, O
where O
a O
candidate O
triple O
is O
evaluated O
from O
four O
different O
views O
. O
The O
Ô¨Årst O
three O
views O
adopt O
the O
generative O
methodology O
, O
each O
view O
predicts O
a O
speciÔ¨Åc O
component O
of O
the O
candidate O
triple O
according O
to O
the O
other O
two O
components O
and O
the O
nearby O
triples O
. O
The O
last O
view O
adopts O
the O
discriminative O
methodology O
, O
it O
predicts O
the O
plausibility O
of O
the O
whole O
triple O
according O
to O
its O
components O
as O
well O
as O
its O
nearby O
triples O
. O
In O
the O
prediction O
conditions O
of O
these O
views O
, O
the O
components O
in O
the O
candidate O
triple O
are O
content O
information O
inside O
the O
triple O
, O
and O
the O
triples O
nearby O
the O
candidate O
triple O
are O
context O
information O
outside O
the O
triple O
. O
In O
details O
, O
the O
Ô¨Årst O
view O
hr‚Üítpredictstbased O
onh O
, O
rand O
their O
context O
, O
the O
second O
view O
rt‚Üíh O
predictshbased O
onr O
, O
tand O
their O
context O
, O
the O
third O
view O
ht‚Üírpredictsrbased O
onh O
, O
tand O
their O
context O
, O
and O
the O
fourth O
view O
hrt‚Üípredicts O
the O
plausi O
- O
bility O
givenh O
, O
r O
, O
tand O
their O
context O
. O
We O
denote O
the O
view O
set O
asV O
, O
containing O
the O
four O
views O
mentions O
above O
. O
These O
views O
evaluate O
the O
candidate O
triple O
from O
different O
viewpoints O
and O
can O
be O
integrated O
to O
give O
better O
prediction O
. O
In O
the O
prediction O
condition O
of O
each O
view O
, O
the O
context O
information O
includes O
the O
entities O
and O
relations O
nearby O
the O
candidate O
triple O
, O
and O
excludes O
the O
entities O
and O
relations O
that O
can O
only O
be O
reached O
by O
way O
of O
the O
entity O
or O
relation O
to O
be O
predicted O
. O
The O
content O
and O
context O
can O
be O
jointly O
represented O
as O
the O
subgraph O
surrounding O
the O
candidate O
triple O
. O
For O
each O
of O
the O
Ô¨Årst O
three O
views O
, O
the O
entity O
of O
relation O
to O
be O
predicted O
is O
replaced O
by O
a O
speciÔ¨Åc O
placeholder O
. O
The O
sub O
- O
graph O
can O
be O
extracted O
by O
breadth-Ô¨Årst O
traversal O
from O
the O
candidate O
triple O
, O
without O
passing O
by O
the O
entity O
or O
relation O
to O
be O
predicted O
. O
In O
the O
traversal O
procedure O
, O
two O
hyperparameters O
dandware O
introduced O
to O
restrict O
the O
depth O
and O
width O
of O
the O
sub O
- O
graph O
. O
SpeciÔ¨Åcally O
, O
ddeÔ¨Ånes O
the O
maximum O
distance O
between O
an O
entity O
and O
the O
candidate O
triple O
, O
andwdeÔ¨Ånes O
the O
maximum O
branch O
count O
when O
passing O
by O
an O
entity O
. O
The O
sub O
- O
graphs O
can O
be O
linearized O
as O
sequences O
of O
of O
symbols O
with O
paired O
brackets O
in O
speciÔ¨Åc O
positions O
. O
The O
linearization O
facilitates O
the O
sequential O
encoding O
of O
graphic O
structures O
, O
which O
is O
proved O
to O
be O
effective O
and O
efÔ¨Åcient O
in O
syntactic O
parsing O
. O
Table O
1 O
shows O
the O
learning O
views O
and O
Figure O
1 O
shows O
the O
content O
and O
context O
information O
for O
each O
view O
. O
3.2 O
Instance O
Generation O
Given O
a O
learning O
view O
v‚ààV O
, O
we O
deÔ¨Åne O
a O
pair O
of O
instance O
generation O
functions O
, O
gvandg‚àí O
v O
, O
to O
generate O
positive O
and O
negative O
classiÔ¨Åcation O
instances O
for O
a O
candidate O
triple O
under O
this O
view O
. O
The O
instances O
are O
used O
as O
classiÔ¨Åcation O
instances O
for O
triple O
evaluation O
. O
In O
an O
instance O
/angbracketleftx O
, O
y O
/ O
angbracketright O
, O
the O
source O
part O
xis O
a O
linearized O
sequence O
representing O
a O
sub O
- O
graph O
, O
and O
the O
target O
part O
yis O
a O
label O
indicating O
an O
entity O
, O
a728Figure O
2 O
: O
The O
overall O
multi O
- O
task O
learning O
architecture O
for O
the O
multi O
- O
view O
learning O
model O
. O
relation O
or O
a O
boolean O
symbol O
. O
They O
correspond O
to O
the O
input O
and O
output O
for O
the O
learning O
of O
the O
classiÔ¨Åcation O
models O
. O
For O
a O
given O
triple O
and O
a O
given O
viewv O
, O
we O
always O
generate O
one O
positive O
view O
instance O
, O
but O
only O
generate O
a O
negative O
instance O
with O
a O
certain O
frequency O
œÅv O
. O
The O
frequencies O
for O
the O
Ô¨Årst O
three O
views O
should O
be O
much O
smaller O
than O
1 O
in O
order O
to O
balance O
the O
instances O
with O
respect O
to O
the O
classiÔ¨Åcation O
labels O
. O
The O
positive O
instances O
are O
generated O
directly O
according O
to O
the O
schemas O
of O
the O
views O
. O
The O
negative O
instances O
are O
necessary O
for O
the O
learning O
of O
the O
triple O
evaluation O
model O
especially O
for O
the O
forth O
view O
. O
The O
source O
part O
of O
a O
negative O
instance O
can O
be O
generated O
by O
replacing O
a O
random O
component O
in O
the O
tuple O
with O
a O
random O
symbol O
of O
the O
same O
type O
, O
to O
satisfy O
the O
condition O
that O
the O
changed O
tuple O
is O
not O
equal O
to O
or O
inside O
of O
a O
triple O
in O
the O
KG O
. O
The O
target O
part O
for O
a O
negative O
instance O
is O
none O
for O
the O
Ô¨Årst O
three O
views O
, O
andfalse O
for O
the O
fourth O
view O
. O
Table O
1 O
shows O
the O
instance O
generation O
functions O
and O
their O
instances O
. O
For O
each O
learning O
view O
, O
both O
positive O
and O
negative O
instances O
generated O
from O
the O
training O
triples O
are O
used O
for O
training O
, O
while O
only O
positive O
instances O
generated O
from O
the O
candidate O
triple O
are O
needed O
for O
testing O
. O
The O
classiÔ¨Åcation O
models O
for O
the O
learning O
views O
can O
be O
trained O
with O
separated O
classiÔ¨Åers O
or O
in O
a O
multi O
- O
task O
framework O
. O
To O
promote O
the O
information O
sharing O
and O
interaction O
between O
learning O
views O
, O
we O
realized O
the O
multi O
- O
view O
model O
in O
a O
multi O
- O
task O
learning O
architecture O
, O
where O
each O
subtask O
takes O
charge O
of O
a O
speciÔ¨Åc O
learning O
view O
. O
In O
the O
multi O
- O
task O
architecture O
, O
the O
instances O
for O
a O
trainingor O
testing O
triple O
are O
simultaneously O
assigned O
to O
the O
sub O
- O
tasks O
according O
to O
their O
corresponding O
views O
. O
The O
details O
for O
realization O
will O
be O
described O
in O
the O
next O
section O
. O
3.3 O
Triple O
Evaluation O
Given O
a O
candidate O
triple O
, O
four O
classiÔ¨Åcation O
instances O
are O
generated O
for O
the O
learning O
views O
by O
the O
corresponding O
positive O
instance O
generation O
functions O
. O
The O
evaluation O
given O
by O
each O
learning O
view O
is O
obtained O
by O
evaluating O
the O
corresponding O
instance O
with O
the O
corresponding O
classiÔ¨Åer O
. O
The O
evaluation O
given O
by O
the O
whole O
multi O
- O
view O
model O
is O
the O
weightedly O
summation O
of O
the O
evaluations O
given O
by O
these O
views O
: O
f(h O
, O
r O
, O
t O
) O
= O
/summationdisplay O
v‚ààVwvfv(h O
, O
r O
, O
t O
) O
The O
function O
fvand O
the O
hyperparameter O
wvindicate O
the O
view O
- O
speciÔ¨Åc O
evaluation O
function O
and O
its O
weighting O
coefÔ¨Åcient O
, O
respectively O
. O
The O
view O
- O
speciÔ¨Åc O
evaluation O
function O
invokes O
the O
classiÔ¨Åcation O
model O
of O
the O
view O
with O
the O
source O
part O
of O
the O
instance O
as O
input O
, O
and O
returns O
the O
prediction O
score O
corresponding O
to O
the O
target O
part O
of O
the O
instance O
: O
fv(h O
, O
r O
, O
t O
) O
= O
/summationdisplay O
v‚ààVwvFv(g+ O
v(h O
, O
r O
, O
t O
) O
¬∑ O
x)[g+ O
v(h O
, O
r O
, O
t O
) O
¬∑ O
y O
] O
The O
functionFindicates O
the O
classiÔ¨Åcation O
procedure O
of O
the O
sub O
- O
task O
corresponding O
to O
a O
speciÔ¨Åc O
learning O
view O
, O
it O
takes O
the O
source O
part O
of O
the O
instance O
as O
input O
and O
gives O
the O
prediction O
scores729on O
all O
possible O
labels O
. O
The O
operator O
¬∑ O
indexes O
the O
source O
or O
target O
part O
of O
the O
instance O
, O
and O
the O
operator O
[ O
] O
indexes O
the O
score O
corresponding O
to O
the O
target O
part O
. O
For O
each O
triple O
in O
the O
testing O
set O
, O
we O
should O
compare O
its O
validity O
with O
those O
of O
the O
candidate O
triples O
, O
which O
are O
generated O
by O
replacing O
the O
head O
or O
tail O
entity O
with O
another O
entity O
. O
This O
means O
that O
, O
for O
a O
KG O
with O
millions O
of O
entities O
, O
millions O
of O
candidate O
triples O
should O
be O
evaluated O
by O
the O
multi O
- O
view O
model O
for O
each O
testing O
triple O
. O
To O
promote O
the O
efÔ¨Åciency O
of O
the O
multi O
- O
view O
model O
, O
we O
adopt O
a O
coarse O
- O
to-Ô¨Åne O
strategy O
in O
testing O
, O
where O
the O
Ô¨Årst O
or O
second O
view O
is O
performed O
to O
give O
a O
list O
of O
k O
- O
best O
candidates O
, O
and O
the O
overall O
model O
is O
then O
performed O
to O
evaluate O
these O
candidates O
. O
4 O
Realization O
: O
Multi O
- O
task O
Architecture O
We O
implement O
the O
multi O
- O
view O
learning O
in O
a O
multitask O
architecture O
, O
where O
each O
sub O
- O
task O
takes O
charge O
of O
a O
speciÔ¨Åc O
learning O
view O
. O
The O
multi O
- O
task O
learning O
strategy O
enables O
information O
sharing O
and O
interaction O
between O
the O
sub O
- O
tasks O
, O
thus O
leading O
to O
better O
performance O
. O
4.1 O
Overall O
Pipeline O
We O
design O
a O
uniÔ¨Åed O
neural O
multi O
- O
task O
learning O
architecture O
for O
the O
multi O
- O
view O
model O
. O
The O
overall O
procedure O
of O
the O
multi O
- O
task O
architecture O
is O
shown O
in O
Figure O
2 O
. O
The O
overall O
procedure O
is O
composed O
of O
three O
stages O
, O
instance O
generation O
, O
instance O
classiÔ¨Åcation O
and O
prediction O
integration O
. O
The O
instance O
generation O
stage O
takes O
as O
input O
the O
given O
triple O
, O
and O
generates O
classiÔ¨Åcation O
instances O
for O
all O
learning O
views O
by O
the O
instance O
generation O
functions O
. O
The O
instance O
classiÔ¨Åcation O
stage O
takes O
as O
input O
the O
source O
parts O
of O
these O
instances O
, O
and O
predicts O
the O
labels O
for O
each O
input O
with O
the O
corresponding O
view O
- O
speciÔ¨Åc O
classiÔ¨Åcation O
model O
. O
The O
prediction O
integration O
stage O
takes O
as O
input O
the O
predictions O
of O
all O
the O
classiÔ¨Åcation O
models O
, O
and O
computes O
the O
overall O
training O
cost O
and O
evaluation O
score O
according O
to O
the O
target O
parts O
of O
the O
instances O
. O
Note O
that O
we O
need O
not O
compute O
the O
overall O
evaluation O
score O
for O
training O
, O
nor O
generate O
the O
negative O
instances O
for O
testing O
. O
In O
the O
instance O
classiÔ¨Åcation O
stage O
, O
all O
the O
classiÔ¨Åcation O
models O
follow O
the O
same O
pipeline O
composed O
of O
embedding O
, O
encoding O
and O
predicting O
. O
For O
predicting O
, O
these O
models O
adopt O
separated O
predicting O
layers O
due O
to O
their O
essentially O
different O
learning O
objects O
. O
For O
embedding O
and O
encoding O
, O
these O
modelsadopt O
the O
shared O
layers O
following O
the O
conventional O
strategy O
in O
NLP O
multi O
- O
task O
learning O
work O
. O
This O
is O
reasonable O
because O
the O
relationship O
between O
an O
instance O
and O
its O
components O
is O
analogous O
to O
that O
between O
a O
sentence O
and O
its O
words O
. O
The O
architecture O
in O
Figure O
2 O
shows O
the O
multi O
- O
task O
learning O
architecture O
with O
shared O
embedding O
and O
encoding O
layers O
. O
We O
add O
a O
speciÔ¨Åc O
symbol O
indicating O
the O
learning O
view O
at O
the O
beginning O
of O
the O
source O
part O
of O
the O
instance O
. O
This O
is O
similar O
to O
the O
idea O
in O
multilingual O
NMT O
that O
a O
speciÔ¨Åc O
markup O
is O
added O
at O
the O
beginning O
of O
a O
source O
language O
sentence O
to O
indicate O
the O
target O
language O
. O
The O
marked O
source O
parts O
of O
the O
instances O
are O
input O
into O
the O
same O
encoding O
layer O
. O
According O
to O
the O
added O
markups O
, O
the O
neural O
network O
learns O
and O
applies O
different O
information O
propagation O
regularities O
for O
instances O
of O
different O
views O
, O
while O
sharing O
network O
parameters O
as O
much O
as O
possible O
. O
4.2 O
Neural O
ClassiÔ¨Åer O
We O
use O
multi O
- O
layer O
Transformer O
as O
the O
encoding O
layers O
and O
logistic O
regression O
with O
softmax O
as O
the O
classiÔ¨Åcation O
layers O
. O
Given O
the O
source O
part O
of O
an O
instance O
, O
x= O
( O
x1,x2, O
... O
,x O
n O
) O
, O
which O
is O
a O
sequence O
of O
entities O
and O
relations O
with O
paired O
brackets O
indicating O
an O
linearized O
sub O
- O
graph O
, O
we O
construct O
the O
representation O
for O
each O
element O
xi‚ààxas O
: O
h0 O
i O
= O
xe O
i+xp O
i O
wherexe O
iis O
the O
element O
embedding O
and O
xp O
ithe O
position O
embedding O
, O
indicating O
the O
current O
element O
and O
its O
position O
in O
the O
sequence O
, O
respectively O
. O
We O
feed O
these O
representations O
into O
a O
stack O
of O
Lsuccessive O
Transformer O
encoders O
as O
: O
hl O
i O
= O
Transformer O
( O
hl‚àí1 O
i O
) O
, O
l= O
1,2, O
... O
,L O
where O
hl O
iis O
the O
hidden O
state O
of O
xiafter O
thel O
- O
th O
encoding O
layer O
. O
We O
omit O
the O
detailed O
description O
of O
Transformer O
since O
it O
is O
already O
ubiquitous O
recently O
. O
The O
representation O
used O
for O
the O
subsequent O
classiÔ¨Åcation O
layer O
is O
the O
concatenation O
of O
the O
Ô¨Ånal O
hidden O
states O
corresponding O
to O
the O
components O
of O
the O
triple O
for O
evaluation O
. O
Note O
that O
for O
the O
Ô¨Årst O
three O
views O
, O
one O
of O
the O
three O
components O
is O
a O
placeholder O
. O
The O
training O
procedure O
aims O
to O
Ô¨Ånd O
the O
parameters O
minimizing O
the O
cross O
- O
entropy O
loss O
: O
L(Œ∏ O
) O
= O
/summationdisplay O
z‚ààKG O
/ O
summationdisplay O
v‚ààV O
/ O
summationdisplay O
/angbracketleftx O
, O
y O
/ O
angbracketright‚àà{g+ O
v(z),g‚àí O
v(z)}C(Fv(x O
, O
Œ∏),y)730SettingFB15k-237 O
WN18RR O
Content O
+ O
Context O
Content O
+ O
Context O
MR O
MRR O
H@10 O
MR O
MRR O
H@10 O
MR O
MRR O
H@10 O
MR O
MRR O
H@10 O
V- O
hr‚Üít161 O
.267 O
.431 O
209 O
.289 O
.485 O
2420 O
.408 O
.477 O
2262 O
.412 O
.498 O
V- O
rt‚Üíh155 O
.277 O
.443 O
178 O
.296 O
.476 O
3318 O
.377 O
.437 O
3573 O
.393 O
.473 O
V- O
ht‚Üír150 O
.294 O
.468 O
215 O
.310 O
.481 O
2824 O
.424 O
.491 O
2713 O
.462 O
.522 O
V- O
hrt‚Üí O
156 O
.290 O
.475 O
161 O
.335 O
.492 O
3011 O
.421 O
.477 O
2713 O
.436 O
.509 O
V O
139 O
.330 O
.491 O
151 O
.359 O
.521 O
2193 O
.446 O
.526 O
2210 O
.484 O
.540 O
Table O
2 O
: O
The O
contributions O
of O
the O
individual O
views O
to O
the O
overall O
model O
, O
evaluated O
on O
the O
development O
sets O
. O
FB15k-237 O
WN18RR O
Statistics O
# O
entrity O
14,541 O
40,943 O
# O
relation O
237 O
11 O
Train O
272,115 O
86,835 O
Partition O
Develop O
17,535 O
3,034 O
Test O
20,466 O
3,134 O
Table O
3 O
: O
The O
statistics O
of O
FB15k-237 O
and O
WN18RR O
, O
including O
number O
of O
entities O
, O
relations O
, O
and O
triples O
in O
each O
partition O
. O
Here O
, O
we O
useFto O
indicate O
the O
feedforward O
procedure O
, O
Cto O
indicate O
the O
cross O
- O
entropy O
cost O
function O
, O
andKGto O
indicate O
the O
set O
of O
training O
triples O
. O
In O
the O
testing O
procedure O
, O
only O
positive O
instances O
are O
used O
for O
a O
testing O
triple O
. O
The O
testing O
procedure O
evaluates O
a O
triple O
by O
integrating O
the O
four O
views O
as O
mentioned O
before O
. O
5 O
Experiments O
5.1 O
Datasets O
and O
Evaluation O
Protocol O
We O
evaluate O
the O
multi O
- O
view O
model O
on O
two O
widely O
used O
benchmark O
datasets O
, O
FB15k-237 O
and O
WN18RR O
, O
which O
are O
subsets O
of O
two O
common O
datasets O
FB15k O
and O
WN18 O
. O
The O
original O
FB15k O
and O
WN18 O
are O
easy O
for O
KGC O
due O
to O
the O
reversible O
relations O
, O
it O
could O
not O
reÔ¨Çect O
the O
real O
performance O
of O
KGC O
models O
. O
Therefore O
, O
researchers O
create O
FB15k-237 O
and O
WN18RR O
to O
Ô¨Åx O
the O
reversible O
relation O
problem O
, O
and O
make O
the O
KGC O
task O
more O
realistic O
( O
Toutanova O
and O
Chen O
, O
2015 O
; O
Dettmers O
et O
al O
. O
, O
2018 O
) O
. O
The O
statistics O
of O
the O
datasets O
are O
summarized O
in O
Table O
3 O
. O
The O
purpose O
of O
KGC O
is O
to O
predict O
a O
missing O
entity O
given O
a O
relation O
and O
another O
entity O
. O
Following O
Bordes O
et O
al O
. O
( O
2013 O
) O
, O
for O
every O
testing O
triple O
, O
we O
replace O
the O
head O
or O
tail O
entities O
with O
all O
entities O
existed O
in O
the O
knowledge O
graph O
, O
and O
rank O
these O
triples O
in O
ascending O
order O
according O
to O
the O
triple O
evaluation O
function O
, O
following O
the O
Ô¨Åltered O
setting O
protocolwhich O
does O
not O
consider O
any O
corrupted O
triples O
that O
appear O
in O
the O
original O
KG O
. O
Following O
( O
Nguyen O
, O
2017 O
) O
, O
we O
use O
three O
common O
evaluation O
metrics O
, O
mean O
rank O
( O
MR O
) O
, O
mean O
reciprocal O
rank O
( O
MRR O
) O
, O
and O
the O
proportion O
of O
the O
valid O
test O
triples O
ranking O
in O
topnpredictions O
( O
H@n O
) O
with O
n‚àà{1,3,10 O
} O
. O
5.2 O
Details O
for O
Training O
and O
Testing O
The O
multi O
- O
view O
model O
is O
trained O
with O
instances O
generated O
from O
the O
training O
triples O
, O
and O
is O
used O
to O
evaluate O
the O
instances O
generated O
from O
the O
testing O
triples O
. O
There O
are O
parameters O
to O
be O
tuned O
in O
the O
procedures O
of O
instance O
generation O
, O
model O
training O
, O
and O
model O
testing O
. O
For O
the O
deÔ¨Ånition O
of O
the O
subgraph O
indicating O
the O
content O
and O
context O
of O
a O
triple O
, O
the O
maximum O
depth O
dand O
widthwwill O
be O
determined O
in O
the O
developing O
procedure O
. O
For O
the O
transformer O
used O
for O
classiÔ¨Åcation O
, O
the O
number O
of O
Transformer O
blocks O
isL= O
6 O
, O
the O
number O
of O
self O
- O
attention O
heads O
is O
A= O
4 O
, O
and O
the O
hidden O
size O
and O
the O
feed O
- O
forward O
size O
areD= O
256 O
and2D= O
512 O
, O
respectively O
. O
The O
dropout O
strategy O
is O
applied O
on O
embedding O
and O
encoding O
layers O
with O
dropout O
rate O
0.5 O
. O
We O
adopt O
the O
Adam O
algorithm O
( O
Kingma O
and O
Ba O
, O
2014 O
) O
for O
tuning O
with O
a O
learning O
rate O
Œ∑= O
5√ó10‚àí4 O
. O
The O
multi O
- O
view O
model O
is O
trained O
with O
batch O
size O
B= O
256 O
for O
at O
most O
1000 O
epochs O
. O
For O
the O
coarse O
- O
to-Ô¨Åne O
prediction O
strategy O
in O
the O
testing O
procedure O
, O
the O
number O
k O
of O
best O
candidates O
given O
by O
the O
Ô¨Årst O
or O
second O
view O
is O
determined O
on O
the O
development O
set O
. O
We O
choose O
œÅ= O
[ O
0.001,0.001,0.01,1.0]for O
negative O
instance O
generation O
, O
d= O
2 O
andw= O
3 O
for O
sub O
- O
graph O
retrieval O
, O
and O
w= O
[ O
0.30,0.30,0.25,0.15]for O
view O
combination O
by O
grid O
search O
experiments O
on O
development O
sets O
. O
The O
above O
models O
are O
implemented O
on O
PaddlePaddle1 O
. O
1https://github.com/PaddlePaddle/Paddle731ModelFB15k-237 O
WN18RR O
MR O
MRR O
H@1 O
H@3 O
H@10 O
MR O
MRR O
H@1 O
H@3 O
H@10 O
R O
- O
GCN+ O
- O
.249 O
.151 O
.264 O
.417 O
- O
- O
- O
- O
KB O
- O
LRN O
209 O
.309 O
.219 O
- O
.493 O
- O
- O
- O
- O
ConvE O
246 O
.316 O
.239 O
.350 O
.491 O
5277 O
.460 O
.390 O
.430 O
.480 O
ConvR O
- O
.350 O
.261 O
.385 O
.528 O
- O
.475 O
.443 O
.489 O
.537 O
RotatE O
177 O
.338 O
.241 O
.375 O
.533 O
3340 O
.476 O
.428 O
.492 O
.571 O
TuckER O
- O
.358 O
.266 O
.394 O
.544 O
- O
.470 O
.443 O
.482 O
.526 O
pLogicNet O
173 O
.332 O
.237 O
.367 O
.524 O
3408 O
.441 O
.398 O
.446 O
.537 O
SimpleClassiÔ¨Åcation O
161 O
.307 O
.223 O
.382 O
.525 O
2193 O
.446 O
.393 O
.456 O
.522 O
MultiView O
134 O
.320 O
.276 O
.412 O
.544 O
1738 O
.463 O
.462 O
.494 O
.549 O
Table O
4 O
: O
Performance O
of O
multi O
- O
view O
learning O
compared O
with O
previous O
methods O
, O
on O
the O
testing O
sets O
of O
FB15k237 O
and O
WN18RR O
. O
R O
- O
GCN+ O
: O
( O
Schlichtkrull O
et O
al O
. O
, O
2017 O
) O
, O
KB O
- O
LRN O
: O
( O
Garcia O
- O
Duran O
and O
Niepert O
, O
2017 O
) O
, O
ConvE O
: O
( O
Dettmers O
et O
al O
. O
, O
2018 O
) O
, O
ConvR O
: O
( O
Jiang O
et O
al O
. O
, O
2019 O
) O
, O
RotatE O
: O
( O
Sun O
et O
al O
. O
, O
2019 O
) O
, O
TuckER O
: O
( O
Bala O
Àázevi¬¥c O
et O
al O
. O
, O
2019 O
) O
, O
pLogicNet O
: O
( O
Qu O
and O
Tang O
, O
2019 O
) O
. O
SimpleClassiÔ¨Åcation O
: O
multi O
- O
view O
model O
based O
on O
simple O
classiÔ¨Åcation O
( O
hr‚Üít O
and O
rt‚Üíh O
) O
, O
MultiView O
: O
multi O
- O
view O
model O
with O
all O
components O
( O
hr‚Üít+rt‚Üíh+ht‚Üír+hrt‚Üí O
) O
. O
5.3 O
Main O
Results O
and O
Analysis O
We O
verify O
the O
effectiveness O
of O
the O
multi O
- O
view O
model O
, O
by O
investigating O
the O
contributions O
of O
the O
learning O
views O
to O
the O
overall O
model O
. O
Table O
2 O
shows O
the O
performance O
on O
the O
development O
sets O
of O
the O
two O
datasets O
. O
Note O
that O
for O
each O
experimental O
setting O
, O
the O
model O
is O
retrained O
on O
the O
classiÔ¨Åcation O
instances O
generated O
according O
to O
the O
views O
in O
the O
setting O
. O
We O
Ô¨Ånd O
that O
each O
of O
the O
learning O
views O
contributes O
to O
the O
Ô¨Ånal O
performance O
, O
and O
context O
information O
brings O
further O
improvement O
. O
The O
performance O
of O
the O
multi O
- O
view O
model O
on O
the O
testing O
sets O
of O
the O
two O
datasets O
is O
shown O
in O
Table O
4 O
, O
where O
the O
performance O
of O
methods O
in O
previous O
work O
is O
also O
listed O
. O
The O
multi O
- O
view O
learning O
model O
achieves O
the O
new O
state O
- O
of O
- O
the O
- O
art O
on O
both O
benchmark O
datasets O
. O
Compared O
with O
previous O
work O
, O
it O
gives O
signiÔ¨Åcantly O
better O
MR O
on O
both O
datasets O
. O
It O
reveals O
that O
in O
the O
multi O
- O
view O
model O
, O
the O
answers O
are O
high O
in O
the O
ranked O
lists O
on O
average O
. O
Considering O
that O
it O
does O
not O
use O
any O
optimization O
tricks O
, O
we O
think O
that O
it O
still O
has O
potential O
for O
further O
improvement O
by O
intruding O
additional O
information O
and O
resources O
, O
such O
as O
pre O
- O
trained O
embeddings O
, O
text O
descriptions O
and O
surface O
morphologies O
of O
entities O
and O
relations O
. O
We O
also O
Ô¨Ånd O
that O
, O
the O
simple O
classiÔ¨Åcation O
model O
based O
on O
the O
Ô¨Årst O
two O
views O
, O
which O
brutally O
predict O
the O
head O
and O
tail O
entities O
according O
to O
the O
rest O
components O
, O
achieves O
very O
promising O
results O
. O
In O
other O
words O
, O
the O
Ô¨Årst O
two O
views O
lead O
to O
simple O
but O
effective O
classiÔ¨Åcation O
- O
based O
KGC O
models O
. O
The O
simple O
classiÔ¨Åcation O
model O
works O
very O
fast O
in O
evaluation O
of O
candidate O
triples O
, O
since O
direct O
pre200 O
400 O
600 O
800 O
1000 O
1200 O
1400 O
1600 O
1800 O
2000 O
k O
- O
best707580859095Recall O
FB15k-237 O
WN18RRFigure O
3 O
: O
The O
recall O
curves O
of O
k O
- O
best O
pre-Ô¨Åltering O
. O
diction O
of O
the O
missing O
entities O
is O
equivalent O
to O
evaluating O
thousands O
of O
candidate O
triples O
simultaneously O
. O
We O
can O
adopt O
a O
coarse O
- O
to-Ô¨Åne O
strategy O
in O
realistic O
applications O
. O
It O
pre O
- O
selects O
the O
k O
- O
best O
candidates O
by O
the O
Ô¨Årst O
two O
views O
, O
and O
reranks O
the O
candidates O
by O
the O
whole O
multi O
- O
view O
model O
. O
Figure O
3 O
shows O
the O
experimental O
results O
. O
The O
quality O
of O
the O
candidate O
list O
is O
measured O
with O
recall O
, O
indicating O
the O
percentage O
of O
the O
instances O
for O
which O
the O
candidate O
lists O
contain O
the O
answers O
. O
We O
Ô¨Ånd O
that O
the O
pre O
- O
selection O
of2000 O
-best O
list O
achieves O
very O
high O
recalls O
on O
the O
two O
datasets O
, O
especially O
on O
FB15k-237 O
. O
Therefore O
, O
we O
can O
safely O
Ô¨Ålter O
out O
most O
of O
the O
candidates O
with O
little O
loss O
of O
Ô¨Ånal O
precision O
. O
It O
facilitates O
the O
introduction O
of O
more O
features O
in O
the O
multi O
- O
view O
model O
by O
restricting O
the O
search O
space O
to O
a O
small O
but O
precise O
k O
- O
best O
list.7326 O
Conclusion O
We O
propose O
a O
novel O
multi O
- O
view O
classiÔ¨Åcation O
model O
for O
knowledge O
graph O
completion O
, O
where O
multiple O
classiÔ¨Åcation O
views O
are O
performed O
based O
on O
both O
content O
and O
context O
information O
for O
candidate O
triple O
evaluation O
. O
The O
multi O
- O
view O
model O
is O
implemented O
with O
a O
simple O
and O
uniÔ¨Åed O
multi O
- O
task O
learning O
architecture O
where O
the O
parameters O
are O
shared O
across O
all O
the O
learning O
views O
. O
It O
achieves O
the O
new O
stateof O
- O
the O
- O
art O
although O
without O
using O
any O
optimization O
tricks O
. O
The O
multi O
- O
view O
model O
can O
be O
improve O
from O
two O
perspectives O
in O
the O
future O
. O
First O
, O
the O
multi O
- O
view O
model O
can O
leverage O
more O
kinds O
of O
information O
and O
resources O
for O
better O
performance O
, O
such O
as O
the O
descriptions O
of O
the O
entities O
and O
relations O
, O
as O
well O
as O
related O
information O
in O
external O
knowledge O
bases O
. O
Second O
, O
the O
multi O
- O
task O
learning O
architecture O
can O
introduce O
different O
kinds O
of O
neural O
networks O
to O
better O
model O
different O
kinds O
of O
information O
, O
for O
example O
, O
sequential O
neural O
networks O
for O
sequences O
and O
graph O
neural O
networks O
for O
graphs O
. O
Acknowledgments O
The O
authors O
Chen O
and O
Xu O
were O
also O
supported O
by O
the O
National O
Nature O
Science O
Foundation O
of O
China O
( O
No O
. O
61876198 O
, O
61976015 O
, O
61370130 O
and O
61473294 O
) O
, O
the O
Fundamental O
Research O
Funds O
for O
the O
Central O
Universities O
( O
No O
. O
2018YJS025 O
) O
, O
the O
Beijing O
Municipal O
Natural O
Science O
Foundation O
( O
No O
. O
4172047 O
) O
, O
and O
the O
International O
Science O
and O
Technology O
Cooperation O
Program O
of O
China O
under O
Grant O
No O
. O
K11F100010 O
. O
We O
sincerely O
thank O
Quan O
Wang O
in O
Baidu O
for O
the O
enlightening O
suggestions O
in O
the O
research O
procedure O
, O
and O
the O
anonymous O
reviewers O
for O
their O
valuable O
comments O
and O
suggestions O
. O
Abstract O
Named O
entity O
disambiguation O
is O
an O
important O
task O
that O
plays O
the O
role O
of O
bridge O
between O
text O
and O
knowledge O
. O
However O
, O
the O
performance O
of O
existing O
methods O
drops O
dramatically O
for O
short O
text O
, O
which O
is O
widely O
used O
in O
actual O
application O
scenarios O
, O
such O
as O
information O
retrieval O
and O
question O
answering O
. O
In O
this O
work O
, O
we O
propose O
a O
novel O
knowledge O
- O
enhanced O
method O
for O
named O
entity O
disambiguation O
. O
Considering O
the O
problem O
of O
information O
ambiguity O
and O
incompleteness O
for O
short O
text O
, O
two O
kinds O
of O
knowledge O
, O
factual O
knowledge O
graph O
and O
conceptual O
knowledge O
graph O
, O
are O
introduced O
to O
provide O
additional O
knowledge O
for O
the O
semantic O
matching O
between O
candidate O
entity O
and O
mention O
context O
. O
Our O
proposed O
method O
achieves O
signiÔ¨Åcant O
improvement O
over O
previous O
methods O
on O
a O
large O
manually O
annotated O
short O
- O
text O
dataset O
, O
and O
also O
achieves O
the O
state O
- O
of O
- O
the O
- O
art O
on O
three O
standard O
datasets O
. O
The O
short O
- O
text O
dataset O
and O
the O
proposed O
model O
will O
be O
publicly O
available O
for O
research O
use O
. O
1 O
Introduction O
Name O
entity O
disambiguation O
( O
NED O
) O
aims O
to O
associate O
each O
entity O
mention O
in O
the O
text O
with O
its O
corresponding O
entity O
in O
the O
knowledge O
graph O
( O
KG O
) O
. O
It O
plays O
an O
important O
role O
in O
many O
text O
- O
related O
artiÔ¨Åcial O
intelligent O
tasks O
such O
as O
recommendation O
and O
conversation O
, O
since O
it O
works O
as O
a O
bridge O
between O
text O
and O
knowledge O
. O
In O
decades O
, O
researchers O
devoted O
their O
efforts O
to O
NED O
in O
many O
ways O
, O
including O
the O
rule O
- O
based O
methods O
( O
Shen O
et O
al O
. O
, O
2014 O
) O
, O
the O
conventional O
statistic O
methods O
( O
Shen O
et O
al O
. O
, O
2014 O
) O
and O
the O
deep O
learning O
methods O
( O
OctavianEugen O
Ganea O
, O
2017 O
) O
. O
On O
formal O
text O
, O
state O
- O
of O
- O
theart O
methods O
achieve O
high O
performance O
thanks O
to O
the O
well O
- O
written O
utterance O
and O
rich O
context O
. O
However O
, O
experiments O
show O
that O
the O
performance O
of O
these O
methods O
degrades O
dramatically O
on O
informal O
text O
, O
for O
example O
, O
the O
short O
text O
widely O
used O
in O
many O
real O
application O
scenarios O
such O
as O
information O
retrieval O
and O
human O
- O
machine O
interaction O
. O
It O
is O
difÔ¨Åcult O
for O
existing O
methods O
to O
make O
decisions O
on O
the O
non O
- O
standard O
utterance O
without O
adequate O
context O
. O
The O
discrimination O
procedure O
of O
NED O
depends O
on O
sufÔ¨Åcient O
context O
in O
the O
input O
text O
, O
which O
is O
usually O
noisy O
and O
scarce O
in O
the O
short O
text O
used O
in O
information O
retrieval O
and O
human O
- O
machine O
interaction O
. O
For O
example O
, O
an O
analysis O
based O
on O
search O
engine O
logs O
demonstrates O
that O
a O
search O
query O
contains O
2.35 O
words O
on O
average O
( O
Yi O
Fang O
, O
2011 O
) O
. O
Such O
short O
text O
could O
not O
provide O
adequate O
context O
which O
is O
necessary O
for O
NED O
models O
. O
In O
recent O
years O
, O
many O
efforts O
improve O
NED O
by O
exploiting O
more O
powerful O
models O
and O
richer O
context O
information O
( O
Shen O
et O
al O
. O
, O
2014 O
) O
. O
These O
methods O
mainly O
focus O
on O
the O
better O
utilization O
of O
existing O
context O
. O
Therefore O
, O
they O
can O
not O
improve O
NED O
effectively O
on O
short O
text O
since O
the O
problem O
of O
information O
shortage O
still O
exists O
. O
Intuitively O
, O
it O
is O
hard O
for O
NED O
to O
achieve O
essential O
improvement O
on O
short O
text O
if O
it O
can O
not O
exploit O
external O
information O
to O
enhance O
the O
recognition O
procedure O
. O
In O
information O
scarce O
situations O
, O
human O
beings O
can O
still O
perform O
recognition O
by O
association O
with O
related O
external O
information O
, O
such O
as O
commonsense O
or O
domain O
- O
speciÔ¨Åc O
knowledge O
. O
It O
inspires O
us O
that O
, O
NED O
on O
short O
text O
could O
be O
improved O
if O
appropriate O
external O
knowledge O
can O
be O
retrieved O
and O
considered O
. O
We O
propose O
a O
novel O
knowledge O
- O
enhanced O
NED O
model O
, O
where O
the O
prediction O
procedure O
of O
NED O
is O
enhanced O
by O
two O
kinds O
of O
knowledge O
formalized O
as O
two O
different O
KGs O
. O
The O
one O
kind O
is O
conceptual O
knowledge O
formalized O
as O
a O
conceptual O
KG O
, O
it O
is O
used O
to O
augment O
the O
representation O
of O
the O
entity O
mention O
by O
giving O
the O
mention O
a O
concept O
embedding O
. O
The O
other O
kind O
is O
factual O
knowledge O
formalized O
as O
a O
factual O
KG O
, O
it O
is O
used O
to O
augment O
the O
representation O
of O
each O
candidate O
entity O
by O
giving O
the O
entity O
an O
entity O
embedding O
. O
The O
augmented735Which O
singer O
sang O
Li O
Bai O
? O
( O
Wedding O
photos O
of O
Li O
Bai O
and O
Sa O
Beining O
? O
) O
( O
QWLW\'LVDPELJXDWLRQ O
IRU6KRUW7H[W‡π´·åÆ[·Éã‡±≠·•Ø·úã]‡π´·åÆ[·¶∏’à]‡π´·åÆ[‡∏Å‡∏à]‡π´·åÆ[‡æà‡πÉ O
] O
‡π´·åÆ[’à·áî O
] O
‡π´·åÆ[·ä™·•§€Ç O
] O
* O
Li O
Bai O
( O
‡π´·åÆ O
) O
has O
21 O
ambiguous O
entities O
  O
in O
KB O
. O
. O
. O
.‡π´·åÆ‡∏é O
·ß° O
‡†ñ·å±‡æà·ùï·µú‡π´·åÆ‡ØÜ‘ç·àª‡π´·åÆŸü·¨¶ﬂ∫‘∂‡πç›∑·å±·¶∏‡π´·åÆ”®‡∂Ü·®¨‡®ò·å±·ïÆ‡ßí·Üô O
. O
QRZOHGJH%DVH O
( O
How O
to O
play O
the O
hero O
Li O
Bai O
? O
) O
( O
What O
famous O
poems O
written O
by O
Li O
Bai O
? O
) O
( O
Li O
Bai O
[ O
Song O
] O
) O
( O
Li O
Bai O
[ O
Game O
Character O
] O
) O
( O
Li O
Bai O
[ O
Poet O
] O
) O
( O
Li O
Bai O
[ O
Celebrity O
] O
) O
( O
Li O
Bai O
[ O
Person O
] O
) O
( O
Li O
Bai O
[ O
Tvshow O
] O
) O
isa O
, O
                            O
Songisa O
, O
  O
Game O
Character O
, O
Heroisa O
, O
                O
Poet O
, O
PersonDynamic O
Conceptualizaition O
Li O
BaiPoet O
( O
·¶∏’à)Li O
BaiCelebrity O
( O
‡∏Å‡∏à)Sa O
Beining‡∂Ü·®¨‡®ò|‡©ú‡∂ÜspousePoemrelatedWritingrelatedEntity O
Context O
EmbeddingFigure O
1 O
: O
Short O
text O
entity O
disambiguation O
and O
our O
method O
. O
We O
solve O
the O
problem O
of O
entity O
disambiguation O
of O
sparse O
short O
texts O
through O
dynamic O
conceptualization O
and O
entity O
context O
embedding O
. O
representations O
of O
the O
mention O
context O
and O
the O
candidate O
entities O
are O
used O
in O
a O
matching O
network O
for O
better O
NED O
prediction O
. O
We O
validate O
the O
knowledge O
- O
enhanced O
NED O
model O
on O
three O
public O
NED O
datasets O
for O
short O
text O
( O
NEEL O
, O
KORE50 O
and O
FUDAN O
) O
as O
well O
as O
our O
new O
dataset O
( O
DUEL O
) O
, O
which O
is O
constructed O
for O
information O
acquiring O
scenarios O
and O
will O
be O
publicly O
available O
for O
research O
use O
. O
Experiments O
show O
that O
the O
knowledge O
- O
enhance O
NED O
model O
performs O
signiÔ¨Åcantly O
better O
than O
previous O
methods O
. O
It O
shows O
the O
effectiveness O
of O
external O
knowledge O
in O
improving O
the O
prediction O
of O
NED O
in O
information O
scarce O
situations O
. O
The O
contribution O
of O
our O
work O
includes O
two O
aspects O
. O
First O
, O
we O
introduce O
conceptual O
and O
factual O
knowledge O
to O
improve O
NED O
for O
short O
text O
for O
the O
Ô¨Årst O
time O
, O
and O
achieve O
signiÔ¨Åcant O
improvement O
. O
Second O
, O
we O
release O
a O
large O
- O
scale O
good O
- O
quality O
NED O
dataset O
for O
short O
text O
for O
information O
acquisition O
scenarios O
, O
which O
is O
complementary O
existing O
datasets O
. O
The O
rest O
of O
this O
paper O
is O
organized O
as O
follows O
. O
We O
Ô¨Årst O
introduce O
the O
NED O
task O
and O
the O
baseline O
method O
( O
section O
2 O
) O
, O
and O
then O
describe O
the O
architecture O
and O
details O
of O
our O
knowledge O
- O
enhanced O
model O
( O
section O
3 O
) O
. O
After O
giving O
the O
detailed O
experimental O
analysis O
( O
section O
4 O
) O
, O
we O
give O
the O
related O
work O
( O
section O
5 O
) O
and O
conclude O
the O
work O
. O
2 O
Task O
DeÔ¨Ånition O
and O
Baseline O
Model O
NED O
is O
a O
fundamental O
task O
in O
the O
area O
of O
natural O
language O
processing O
and O
knowledge O
base O
. O
It O
aims O
toassociate O
each O
entity O
mention O
in O
the O
given O
text O
with O
its O
corresponding O
entity O
in O
the O
given O
KG O
. O
Formally O
, O
given O
a O
KGGand O
a O
piece O
of O
textT O
, O
it O
assigns O
each O
mentionm‚ààT O
with O
an O
entity O
e‚ààG O
indicating O
thatmrefers O
toe O
, O
or O
with O
the O
symbol O
œÜindicating O
that O
there O
is O
no O
corresponding O
entity O
. O
The O
disambiguation O
procedure O
can O
be O
formalized O
as O
matching O
between O
the O
context O
of O
the O
mention O
and O
each O
candidate O
entities O
. O
f(m O
) O
= O
Ô£± O
Ô£≤ O
Ô£≥arg O
maxe‚ààe(m)(s(e O
, O
c(m O
) O
) O
) O
, O
e(m)/negationslash=‚àÖ O
œÜ O
, O
otherwise(1 O
) O
Here O
, O
the O
function O
ereturns O
the O
entity O
candidate O
set O
for O
a O
given O
mention O
, O
and O
the O
function O
creturns O
the O
context O
of O
the O
given O
mention O
. O
The O
function O
s O
is O
used O
to O
evaluate O
the O
matching O
degree O
between O
context O
and O
candidate O
, O
and O
is O
usually O
implemented O
as O
matching O
networks O
. O
If O
the O
entity O
candidate O
set O
is O
empty O
or O
the O
highest O
matching O
score O
is O
bellow O
a O
given O
threshold O
, O
the O
function O
freturnsœÜfor O
the O
given O
mention O
. O
The O
conditions O
GandTin O
the O
functions O
eandcare O
omitted O
in O
the O
equation O
for O
simplicity O
. O
We O
adopt O
the O
deep O
structured O
semantic O
model O
( O
DSSM O
) O
( O
Huang O
et O
al O
. O
, O
2013 O
) O
as O
the O
baseline O
model O
for O
NED(Nie O
and O
Pan O
, O
2018 O
) O
. O
Based O
on O
a O
selfattention O
matching O
network O
, O
DSSM O
maps O
the O
candidate O
entities O
and O
the O
context O
to O
the O
same O
semantic O
space O
, O
and O
Ô¨Ånds O
the O
candidate O
entity O
that O
best O
semantically O
matches O
the O
context O
. O
The O
representation O
learning O
for O
both O
entities O
and O
contexts O
is O
enhanced736Product O
AttentionFCSim O
Loss O
Self O
AttentionLSTMembeddingCandidateEntity O
( O
CE)Mention O
and O
Context O
( O
MC)Entity O
Context O
EmbeddingEntity O
Text O
MetaEntity O
RelationEntity O
Embedding O
Knowledge O
Feature O
NetworkGraph O
ConstructionConceptualizeDynamic O
ConceptualizationEntity O
Context O
Emb O
Conceptualization O
EmbEntity O
Emb O
Context O
EmbKnowledge O
BaseScore(CE O
, O
MC)Who O
is O
the O
singer O
of O
Li O
Bai?Li O
Bai6HOI$WWHQWLRQ0DWFKLQJ1HWZRUNEDVHG'660Figure O
2 O
: O
The O
architecture O
of O
our O
proposed O
entity O
disambiguation O
model O
K O
- O
NED O
. O
by O
word2vec O
( O
Tomas O
Mikolov O
, O
2013 O
) O
. O
In O
this O
work O
, O
we O
focus O
on O
the O
problem O
of O
NED O
itself O
, O
that O
is O
, O
predicting O
the O
right O
entity O
candidate O
for O
each O
given O
entity O
mention O
. O
The O
entity O
mentions O
needed O
for O
NED O
are O
simply O
derived O
from O
the O
results O
of O
named O
entity O
recognition O
( O
NER O
) O
. O
3 O
Knowledge O
- O
Enhanced O
NED O
Model O
For O
short O
text O
used O
in O
information O
acquisition O
scenarios O
such O
as O
information O
retrieval O
and O
question O
answering O
, O
the O
lack O
of O
both O
lexical O
and O
syntactical O
information O
obstacles O
the O
precise O
disambiguation O
of O
entity O
mentions O
. O
For O
human O
beings O
, O
however O
, O
the O
problem O
of O
information O
scarcity O
does O
not O
hinder O
the O
disambiguation O
procedure O
. O
This O
is O
because O
there O
are O
many O
implicit O
assumptions O
and O
apriori O
knowledges O
in O
these O
information O
acquisition O
scenarios O
, O
which O
can O
be O
effectively O
considered O
by O
association O
and O
imagination O
during O
disambiguation O
procedure O
. O
Inspired O
by O
this O
, O
we O
propose O
a O
knowledge O
- O
enhanced O
NED O
model O
( O
K O
- O
NED O
) O
for O
short O
text O
, O
where O
two O
kinds O
of O
knowledge O
are O
introduce O
to O
provide O
additional O
information O
for O
better O
disambiguation O
performance O
. O
Figure O
2 O
gives O
the O
overall O
architecture O
of O
the O
model O
. O
The O
overall O
procedure O
of O
the O
K O
- O
NED O
model O
is O
a O
pipeline O
including O
feature O
extraction O
and O
semantic O
matching O
, O
where O
the O
former O
is O
composed O
of O
two O
sub O
- O
procedures O
, O
taking O
charge O
of O
feature O
extraction O
for O
mention O
context O
and O
candidate O
entity O
, O
respectively O
. O
Rather O
than O
considering O
only O
the O
utterance O
of O
the O
input O
text O
, O
the O
feature O
extraction O
procedure O
also O
considers O
external O
knowledge O
for O
betterrepresentation O
. O
In O
details O
, O
the O
feature O
extraction O
sub O
- O
procedure O
for O
mention O
context O
is O
enhanced O
by O
conceptual O
knowledge O
formalized O
as O
a O
conceptual O
KG O
, O
which O
augments O
the O
representation O
of O
the O
mention O
context O
by O
giving O
each O
word O
a O
concept O
embedding O
; O
while O
the O
sub O
- O
procedure O
for O
candidate O
entity O
is O
enhanced O
by O
factual O
knowledge O
formalized O
as O
a O
factual O
KG O
, O
which O
augments O
the O
representation O
of O
each O
candidate O
entity O
by O
giving O
the O
entity O
an O
entity O
embedding O
. O
The O
augmented O
representation O
is O
used O
in O
the O
following O
semantic O
matching O
procedure O
for O
better O
prediction O
. O
The O
major O
difference O
of O
the O
K O
- O
NED O
model O
is O
the O
introduction O
of O
external O
knowledge O
in O
the O
representation O
learning O
procedure O
. O
For O
the O
representation O
learning O
procedure O
, O
it O
simply O
uses O
the O
pre O
- O
trained O
word2vec O
language O
model O
to O
take O
charge O
of O
the O
conventional O
utterance O
representation O
learning O
. O
For O
the O
semantic O
matching O
procedure O
, O
it O
directly O
adopt O
the O
self O
- O
attention O
matching O
network O
based O
on O
DSSM O
. O
Given O
a O
mention O
mand O
a O
candidate O
entity O
e O
, O
the O
word2vec O
- O
based O
module O
gives O
two O
representation O
vectors O
, O
rlm O
mandrlm O
e O
, O
while O
the O
KG O
- O
based O
modules O
give O
another O
two O
representation O
vectors O
, O
rkg O
mand O
rkg O
e. O
The O
concatenation O
of O
the O
four O
representation O
vectors O
is O
fed O
into O
the O
matching O
network O
to O
obtain O
the O
matching O
degree O
. O
Based O
on O
the O
utterance O
of O
more O
, O
the O
word2vec O
- O
based O
representation O
vector O
rlm O
morrlm O
eis O
obtained O
by O
averaging O
the O
hidden O
representations O
for O
the O
words O
or O
characters O
in O
the O
utterance O
. O
We O
omit O
the O
detailed O
descriptions O
of O
the O
word2vec O
- O
based O
feature O
extraction O
and O
the O
DSSM-737based O
semantic O
matching O
owing O
to O
space O
limitations O
. O
In O
the O
following O
subsections O
, O
we O
describe O
in O
details O
the O
computation O
procedures O
for O
the O
KGbased O
feature O
extraction O
. O
3.1 O
KG O
- O
enhanced O
Representation O
of O
Mention O
The O
concepts O
in O
a O
conceptual O
KG O
can O
be O
treated O
as O
upper O
classes O
of O
the O
entities O
in O
the O
factual O
KG O
. O
A O
concept O
is O
a O
name O
or O
label O
representing O
a O
concrete O
or O
material O
existence O
such O
as O
a O
person O
, O
a O
place O
or O
a O
thing O
. O
For O
example O
, O
the O
entity O
apple O
, O
maybe O
corresponds O
to O
the O
concept O
of O
fruits O
, O
companies O
and O
songs O
. O
For O
a O
mention O
, O
we O
label O
the O
mention O
word O
with O
a O
concept O
and O
use O
the O
concept O
representation O
as O
additional O
feature O
representation O
of O
the O
mention O
. O
Intuitively O
, O
the O
concept O
labeling O
procedure O
works O
as O
a O
semantic O
bridge O
between O
the O
mentions O
and O
the O
entities O
. O
Different O
from O
traditional O
methods O
where O
mentions O
are O
classiÔ¨Åed O
into O
coarse O
- O
grained O
entity O
types O
, O
the O
concept O
labeling O
procedure O
in O
our O
work O
classiÔ¨Åes O
the O
mentions O
into O
Ô¨Åne O
- O
grained O
concepts O
, O
which O
can O
better O
utilize O
the O
context O
of O
the O
mentions O
and O
provide O
more O
information O
for O
disambiguation O
. O
We O
adopt O
a O
graph O
- O
based O
labeling O
algorithm O
for O
concept O
labeling O
, O
as O
shown O
in O
Figure O
3 O
. O
Given O
a O
short O
sentence O
, O
it O
Ô¨Årst O
builds O
a O
knowledge O
feature O
network O
( O
KFN O
) O
based O
on O
the O
short O
sentence O
and O
reference O
conceptual O
/ O
factual O
KGs O
, O
and O
then O
searches O
for O
the O
appropriate O
concept O
for O
the O
mention O
by O
a O
random O
walking O
algorithm O
. O
The O
KFN O
is O
built O
according O
to O
the O
correspondence O
between O
the O
symbols O
in O
the O
short O
sentence O
and O
the O
reference O
KGs O
. O
The O
symbols O
include O
words O
, O
entity O
mentions O
and O
candidate O
concepts O
, O
where O
the O
words O
and O
mentions O
are O
obtained O
by O
lexical O
analysis O
and O
entity O
recognition O
, O
and O
the O
concepts O
are O
obtained O
by O
matching O
on O
the O
reference O
KGs O
. O
The O
KFN O
describes O
three O
kinds O
of O
relationships O
, O
that O
is O
, O
the O
entity O
- O
concept O
relationship O
, O
the O
concept O
- O
concept O
relationship O
and O
the O
word O
- O
concept O
relationship O
. O
The O
concept O
- O
entity O
relationship O
is O
represented O
by O
the O
generation O
probability O
from O
concept O
cto O
entity O
e. O
The O
probability O
p(c|e)is O
calculated O
based O
on O
the O
page O
- O
view O
( O
PV O
) O
statistics O
of O
the O
Wikipedia O
entity O
pages O
: O
P(c|e O
) O
= O
NPV(e)/summationtext O
e O
/ O
prime‚ààcNPV(e O
/ O
prime)(2 O
) O
The O
concept O
- O
concept O
relationship O
is O
represented O
by O
the O
transition O
probability O
between O
two O
concepts O
, O
isA O
networkconcept O
cooc O
networklexical O
networkKnowledge O
BaseNQRZOHGJHIHDWXUHQHWZRUNVXEJUDSKFRQVWUXFWLRQ O
wordsegmententity O
recognitionsubgraphbuilding O
random O
walk O
with O
restartFRQFHSWXDOL]DWLRQ O
Short O
text O
: O
Which O
singer O
sang O
LiBai?LiBaisingersangSongGameCharacterPoetCelebrityPerson0203040.030.070.020.1conceptsignal O
wordentity O
Figure O
3 O
: O
Architecture O
of O
Ô¨Åne O
- O
grained O
conceptualization O
, O
which O
consists O
of O
three O
parts O
: O
( O
a O
) O
Knowledge O
Feature O
Network O
. O
( O
b O
) O
Sub O
- O
graph O
construction O
. O
( O
c O
) O
Conceptualization O
. O
ciandcj O
. O
The O
probability O
P(ci|cj)is O
calculated O
based O
on O
the O
co O
- O
occurrence O
frequencies O
of O
the O
entities O
under O
the O
two O
concepts O
: O
P(ci|cj O
) O
= O
/summationtext O
ej‚ààcj O
, O
ei‚ààciN(ej O
, O
ei O
) O
/summationtext O
c‚ààC O
/ O
summationtext O
ej‚ààc O
, O
ei‚ààcN(ej O
, O
ei)(3 O
) O
where O
the O
co O
- O
occurrence O
frequency O
N(ej O
, O
ei O
) O
, O
is O
calculated O
based O
on O
the O
statistics O
of O
anchor O
links O
of O
Baidu O
Encyclopedia O
, O
and O
wis O
the O
size O
of O
the O
window O
that O
counts O
the O
co O
- O
occurrences O
frequencies O
of O
the O
entity O
pair O
in O
Baidu O
Encyclopedia O
. O
In O
this O
paper O
, O
wis O
set O
to O
25 O
. O
N(ej O
, O
ei O
) O
= O
freq O
w(ej O
, O
ei O
) O
( O
4 O
) O
The O
word O
- O
concept O
relationship O
is O
represented O
by O
the O
labeling O
probability O
between O
the O
word O
wand O
the O
related O
concept O
c. O
The O
probability O
is O
calculated O
based O
on O
the O
word O
frequency O
and O
word O
- O
concept O
co O
- O
occurrence O
frequency O
: O
P(c|w O
) O
= O
N(c O
, O
w O
) O
N(w)(5 O
) O
We O
perform O
a O
random O
walk O
algorithm O
( O
JiaYu O
Pan O
, O
2004 O
) O
on O
the O
KFN O
to O
get O
the O
appropriate O
concept O
of O
entity O
mention O
. O
First O
, O
we O
initialize O
the O
weights O
of O
the O
nodes O
and O
the O
edges O
by O
: O
E0(e O
) O
= O
/braceleftbiggP(c|t)ifeisc‚Üít O
P(ci|cj)ifeiscj‚Üíci(6 O
) O
N0(n O
) O
= O
/braceleftbigg1/|T|ifnisentity O
0 O
ifnisconcept(7 O
) O
Second O
, O
we O
iteratively O
update O
the O
node O
and O
edge O
by O
: O
Nk= O
( O
1‚àíŒ±)E O
/ O
prime√óNk‚àí1+Œ±N0(8)738Ek‚Üê(1‚àíŒ≤)Nk+Œ≤Ek(9 O
) O
whereŒ±andŒ≤are O
hyper O
- O
parameters O
tuned O
on O
developing O
sets O
. O
Finally O
, O
we O
normalize O
the O
edge O
weights O
and O
obtain O
the O
concept O
type O
with O
the O
highest O
weight O
: O
c‚àó= O
arg O
max O
cP(c|t O
) O
= O
arg O
max O
cE(t‚Üíc)/summationtext O
ciE(t‚Üíci)(10 O
) O
3.2 O
KG O
- O
enhanced O
Representation O
of O
Entity O
The O
conventional O
representation O
for O
an O
entity O
is O
the O
textual O
representation O
of O
the O
entity O
. O
Inspired O
by O
the O
wide O
usage O
of O
distributed O
representation O
of O
KG O
entities O
in O
many O
NLP O
applications O
, O
we O
think O
that O
such O
knowledge O
representation O
is O
also O
helpful O
in O
NED O
. O
In O
this O
work O
, O
we O
use O
both O
textual O
and O
knowledge O
representation O
to O
better O
represent O
the O
semantics O
of O
candidate O
entities O
. O
We O
propose O
a O
novel O
representation O
learning O
method O
which O
can O
simultaneously O
learns O
both O
kinds O
of O
knowledge O
. O
Based O
on O
the O
related O
textual O
context O
and O
other O
information O
of O
the O
entities O
, O
it O
uses O
the O
CBOW O
model O
with O
a O
sigmoid O
layer O
to O
generate O
the O
distributed O
representation O
of O
the O
entities O
. O
Knowledge O
BaseText O
entity O
embedding(entity O
, O
desc)Positive O
Samplekeytexts O
and O
entities O
related O
to O
entitiesNegative O
SampleHierachical O
negative O
sampleCBOW O
+ O
single O
layer O
NNskipgramsequence O
of O
( O
entity O
, O
, O
entity O
, O
entity≈è)pretrain O
vectorentity O
embeddingb O
) O
entity O
relationsa O
) O
entity O
text O
meta O
     O
Figure O
4 O
: O
Entity O
context O
embedding O
architecture O
which O
combines O
entity O
relations O
and O
the O
entity O
context O
. O
The O
detailed O
training O
process O
for O
the O
two O
models O
will O
now O
be O
introduced O
. O
Figure O
4 O
shows O
that O
the O
entityeand O
its O
description O
generate O
the O
entity O
embedding O
. O
First O
, O
a O
positive O
sample O
is O
generated O
by O
entity O
description O
from O
KB O
( O
Wikipedia O
and O
Baidu O
Encyclopedia O
) O
, O
and O
then O
word O
segmentation O
is O
applied O
to O
the O
entity O
description O
text O
. O
We O
have O
counted O
the O
word O
frequency O
in O
positive O
samples O
, O
and O
negative O
samples O
are O
generated O
by O
band O
- O
frequency O
random O
sampling O
. O
In O
order O
to O
learn O
the O
relationship O
between O
entities O
and O
enhanced O
entity O
representations O
, O
we O
use O
entity O
co O
- O
occurrence O
data O
and O
KB O
S O
- O
P O
- O
O O
data O
to O
generate O
training O
samples O
: O
‚Ä¢entity O
co O
- O
occurrence O
sequence{e1,e2,¬∑¬∑¬∑,en O
} O
, O
which O
are O
extracted O
from O
KB O
hyperlinks O
. O
‚Ä¢S O
- O
P O
- O
O O
triples O
from O
KB O
, O
which O
are O
extracted O
from O
the O
key O
- O
value O
block O
of O
Wikipedia O
and O
Baidu O
Encyclopedia O
. O
We O
obtained O
entity O
sequences O
as O
training O
sample O
, O
where O
each O
entity O
has O
an O
entity O
embedding O
. O
Then O
we O
updated O
the O
entity O
embedding O
representation O
with O
Skip O
- O
Gram O
Model O
to O
enhance O
the O
inter O
- O
entity O
relationships O
. O
Finally O
, O
we O
obtain O
as O
the O
Ô¨Ånal O
entity O
embedding O
. O
Entity O
embedding O
vector O
are O
input O
as O
feature O
representations O
of O
entities O
into O
an O
KGenhanced O
entity O
disambiguation O
network O
, O
as O
shown O
in O
Figure O
2 O
. O
4 O
Experiments O
and O
Analysis O
In O
this O
section O
, O
we O
Ô¨Årst O
introduce O
the O
experimental O
dataset O
, O
and O
construction O
methods O
of O
the O
dataset O
we O
published O
with O
this O
paper O
. O
Then O
, O
we O
present O
evaluation O
metrics O
, O
the O
experiments O
conducted O
for O
both O
the O
English O
and O
the O
Chinese O
datasets O
with O
existing O
approaches O
, O
and O
we O
analyze O
the O
experimental O
results O
in O
detail O
. O
4.1 O
Datasets O
We O
have O
experimented O
on O
both O
Chinese O
and O
English O
datasets O
. O
For O
the O
English O
experiment O
, O
we O
use O
Wikipedia O
with O
a O
release O
time O
of O
202003 O
as O
KB O
and O
apply O
the O
framework O
to O
NEEL O
and O
KORE50 O
datasets O
. O
For O
the O
Chinese O
experiment O
, O
due O
to O
the O
lack O
of O
large O
- O
scale O
short O
text O
entity O
disambiguation O
datasets O
, O
we O
constructed O
a O
dataset O
called O
DUEL O
and O
use O
it O
as O
the O
Chinese O
experiment O
dataset O
alongside O
the O
FUDAN O
dataset O
( O
Xu O
et O
al O
. O
, O
2017 O
) O
. O
4.1.1 O
English O
Datasets O
Most O
of O
the O
existing O
datasets O
on O
NED O
are O
based O
on O
long O
text O
, O
which O
are O
not O
suitable O
for O
our O
task O
. O
Two O
English O
datasets O
could O
be O
found O
that O
were O
suitable O
for O
short O
text O
entity O
disambiguation O
. O
Because O
KORE50 O
only O
has O
test O
data O
, O
but O
no O
training O
data O
, O
we O
use O
the O
training O
samples O
of O
NEEL O
as O
the O
training O
samples O
of O
KORE50 O
as O
well O
, O
to O
compare O
their O
performances O
. O
‚Ä¢NEEL(Rizzo O
et O
al O
. O
, O
2017 O
): O
The O
training O
dataset O
consists O
of O
6,025 O
tweets O
, O
the O
validation O
dataset O
consists O
of O
100 O
tweets O
, O
and O
the O
testing O
dataset O
consists O
of O
300 O
tweets.739‚Ä¢KORE50(Hoffart O
et O
al O
. O
, O
2012 O
): O
It O
contains O
50 O
short O
sentences O
with O
highly O
ambiguous O
mentioned O
entities O
. O
It O
is O
considered O
to O
be O
among O
the O
most O
challenging O
for O
NED O
. O
Average O
sentence O
length O
( O
after O
removing O
stop O
words O
) O
is O
6.88 O
words O
per O
sentence O
and O
each O
sentence O
has O
2.96 O
mentioned O
entities O
on O
average O
. O
4.1.2 O
Chinese O
Datasets O
The O
typical O
size O
of O
existing O
Chinese O
NED O
datasets O
is O
about O
a O
few O
thousand O
annotated O
words O
( O
Rizzo O
et O
al O
. O
, O
2017 O
; O
Hoffart O
et O
al O
. O
, O
2012 O
) O
. O
Because O
there O
is O
a O
lack O
of O
existing O
data O
sets O
for O
short O
text O
NED O
, O
we O
manually O
construct O
the O
largest O
available O
human O
annotated O
Chinese O
dataset O
, O
and O
we O
have O
released O
it O
to O
the O
global O
research O
community O
, O
please O
refer O
to O
this O
( O
DUEL O
) O
for O
more O
data O
details O
. O
4.1.3 O
Construction O
of O
Our O
Dataset O
Our O
dataset O
provides O
a O
high O
- O
precision O
manuallyannotated O
entity O
disambiguation O
dataset O
consistin O
of O
100,000 O
short O
texts O
. O
The O
text O
corpus O
consists O
of O
queries O
and O
web O
page O
titles O
. O
The O
annotated O
entities O
are O
in O
the O
general O
domain O
, O
including O
instances O
( O
e.g. O
Barack O
Obama O
) O
and O
concepts O
( O
e.g. O
Basketball O
player O
) O
. O
Table O
1 O
and O
2 O
depict O
the O
statistical O
data O
of O
the O
KB O
and O
the O
annotated O
text O
. O
Table O
1 O
: O
Statistics O
of O
knowledge O
base O
in O
our O
dataset O
. O
AvgNumOfEntityProperties O
is O
the O
average O
number O
of O
attributes O
for O
all O
entities O
. O
Statistic O
KB O
# O
Entities O
398082 O
# O
SPO O
3564565 O
# O
EntitiyDesc O
361778 O
# O
AvgNumOfEntityProperties O
9 O
Data O
Annotation O
Method O
: O
We O
annotated O
the O
entire O
short O
text O
in O
the O
dataset O
by O
crowd O
- O
sourcing O
. O
The O
same O
data O
was O
repeatedly O
labeled O
by O
three O
domain O
experts O
, O
then O
reviewed O
and O
released O
by O
additional O
experts O
. O
The O
average O
precision O
of O
annotating O
entities O
is O
about O
95.2 O
% O
. O
The O
evaluation O
method O
of O
dataset O
is O
as O
follows O
: O
given O
an O
input O
of O
a O
short O
text O
q O
, O
the O
annotated O
entities O
is O
E O
/ O
prime O
q O
= O
e O
/ O
prime O
1,e O
/ O
prime O
2,e O
/ O
prime O
3 O
, O
.... O
By O
comparing O
the O
outputs O
E O
/ O
prime O
qwith O
additional O
expertsannotated O
set O
Eq O
= O
e1,e2,e3 O
, O
... O
, O
precisionPis O
deÔ¨Åned O
as O
follows O
. O
P=/summationtext O
q‚ààQEq‚à©E O
/ O
prime O
q O
/ O
summationtext O
q‚ààQEq(11)Comparison O
with O
previous O
datasets O
: O
As O
summarized O
in O
Table O
2 O
, O
FUDAN O
is O
a O
representative O
evaluation O
dataset O
for O
Chinese O
short O
text O
entity O
disambiguation O
, O
which O
consists O
of O
manually O
annotated O
short O
text O
. O
Both O
FUDAN O
and O
DUEL O
consist O
of O
entities O
in O
various O
domains O
( O
including O
instances O
and O
concepts O
) O
, O
such O
as O
persons O
, O
movies O
, O
and O
general O
concepts O
. O
However O
, O
DUEL O
is O
much O
larger O
. O
4.2 O
Results O
and O
Analysis O
4.2.1 O
Evaluation O
Metrics O
We O
directly O
use O
the O
gold O
standard O
in O
mentioned O
entities O
- O
the O
NER O
results O
in O
the O
dataset O
, O
and O
choose O
standard O
micro O
F1 O
score O
as O
our O
performance O
metric O
for O
NED O
task O
( O
aggregated O
over O
all O
mentions O
) O
. O
4.2.2 O
Performance O
Comparison O
with O
Other O
Approaches O
In O
order O
to O
verify O
the O
enhancement O
of O
different O
methods O
used O
in O
NED O
, O
we O
compare O
the O
proposed O
method O
with O
several O
state O
- O
of O
- O
the O
- O
art O
approaches O
both O
for O
the O
Chinese O
and O
the O
English O
datasets O
. O
All O
of O
these O
methods O
are O
effective O
and O
comparable O
in O
the O
case O
of O
short O
text O
. O
Our O
method O
is O
called O
knowledge O
- O
enhanced O
NED O
( O
K O
- O
NED O
) O
. O
‚Ä¢FEL(Blanco O
et O
al O
. O
, O
2015 O
): O
A O
toolkit O
for O
training O
models O
to O
link O
entities O
to O
KB O
in O
documents O
and O
queries O
. O
And O
we O
use O
DSSM O
model O
to O
use O
this O
entity O
embedding O
for O
comparing O
. O
We O
experiment O
with O
default O
parameters O
. O
‚Ä¢NTEE O
( O
Yamada O
et O
al O
. O
, O
2017 O
): O
A O
neural O
network O
model O
that O
learns O
embedding O
of O
texts O
and O
Wikipedia O
entities O
, O
and O
then O
use O
them O
in O
entity O
linking O
task O
. O
We O
experiment O
with O
default O
parameters O
. O
‚Ä¢Mulrel O
- O
nel O
( O
Le O
and O
Titov O
. O
, O
2018 O
): O
A O
python O
implementation O
of O
multi O
- O
relational O
NED O
. O
We O
experiment O
with O
default O
parameters O
. O
‚Ä¢Fudan O
( O
Xu O
et O
al O
. O
, O
2017 O
): O
Entity O
linking O
of O
Fudan O
University O
which O
is O
a O
Chinese O
entity O
linking O
service O
API O
. O
As O
summarized O
in O
Tables O
3 O
, O
the O
experimental O
results O
indicate O
that O
our O
approach O
K O
- O
NED O
outperforms O
existing O
state O
- O
of O
- O
the O
- O
art O
methods O
such O
as O
FEL O
, O
NTEE O
, O
Mulrel O
- O
nil O
and O
Fudan O
on O
Chinese O
and O
English O
datasets O
except O
on O
KORE50 O
. O
In O
particular O
, O
our O
method O
disambiguate O
to O
all O
correct O
result O
of O
the O
examples O
in O
Figure O
1 O
. O
We O
found O
that O
72%740Table O
2 O
: O
Comparisons O
between O
DUEL O
and O
the O
FUDAN O
dataset O
. O
AvgLen O
is O
the O
average O
length O
of O
the O
annotated O
text O
. O
AvgNumEntity O
is O
the O
average O
number O
of O
entities O
in O
the O
annotated O
text O
. O
Statistic O
DUEL O
FUDAN O
NEEL O
KORE50 O
# O
Train O
90000 O
- O
6025 O
# O
Dev O
10000 O
- O
100 O
# O
Test O
10000 O
1037 O
300 O
50 O
# O
AvgLen O
21.73 O
23.38 O
16.5157 O
6.88 O
# O
AvgNumEntity O
3.43 O
2.08 O
2.1 O
2.96 O
# O
Accuracy O
95.2 O
% O
- O
- O
Table O
3 O
: O
F1 O
scores O
on O
Chinese O
and O
English O
datasets O
. O
Method O
Datasets O
Chinese O
datasets O
DUEL O
FUDAN O
Fudan O
0.861 O
0.945 O
Mulrel O
- O
nel O
0.889 O
0.893 O
K O
- O
NED(Ours O
) O
0.897 O
0.947 O
English O
datasets O
NEEL O
KORE50 O
FEL O
0.601 O
0.360 O
NTEE O
0.748 O
0.618 O
Mulrel O
- O
nel O
0.805 O
0.625 O
K O
- O
NED(Ours O
) O
0.811 O
0.544 O
of O
the O
types O
of O
annotated O
entities O
in O
the O
KORE50 O
dataset O
belong O
to O
the O
category O
‚Äù O
Person O
‚Äù O
, O
and O
so O
it O
is O
possible O
that O
this O
dataset O
distribution O
is O
biased O
. O
Compared O
to O
KORE50 O
, O
NEEL O
, O
FUDAN O
and O
DUEL O
datasets O
are O
more O
consistent O
with O
the O
entity O
type O
distribution O
of O
practical O
scenarios O
. O
NTEE O
and O
FEL O
use O
representational O
learning O
to O
improve O
performance O
, O
Mulrel O
- O
nel O
relied O
on O
supervised O
systems O
or O
heuristics O
to O
predict O
these O
relations O
and O
treat O
relations O
as O
latent O
variables O
in O
neural O
entity O
disambiguation O
model O
, O
and O
our O
approach O
uses O
knowledge O
enhancement O
to O
improve O
the O
performance O
without O
using O
other O
complex O
features O
. O
Data O
analyses O
demonstrate O
that O
each O
short O
text O
contains O
3 O
mentioned O
entities O
on O
average O
, O
each O
of O
which O
includes O
20 O
ambiguous O
entities O
to O
be O
linked O
, O
and O
the O
context O
is O
sparse O
. O
Experiments O
demonstrate O
that O
knowledge O
enhancement O
is O
helpful O
for O
short O
text O
entity O
disambiguation O
. O
4.2.3 O
Performance O
of O
Knowledge O
- O
Enhancement O
Components O
In O
order O
to O
gain O
a O
deeper O
understanding O
of O
the O
various O
components O
of O
our O
model O
, O
we O
compare O
the O
difference O
in O
performance O
after O
removing O
two O
com O
- O
ponents O
separately O
, O
where O
all O
models O
are O
trained O
using O
the O
same O
settings O
. O
Table O
4 O
: O
F1 O
scores O
of O
each O
component O
on O
Chinese O
and O
English O
datasets O
. O
Feature O
DUEL O
NEEL O
K O
- O
NED O
0.897 O
0.811 O
K O
- O
NED O
-DC O
0.804 O
0.755 O
K O
- O
NED O
-ECE O
0.874 O
0.779 O
K O
- O
NED O
-DC O
- O
ECE O
0.759 O
0.577 O
As O
listed O
in O
Table O
4 O
, O
K O
- O
NED O
is O
the O
result O
of O
our O
complete O
model O
. O
DC O
represents O
the O
Ô¨Åne O
- O
grained O
dynamic O
conceptualization O
component O
, O
and O
ECE O
represents O
entity O
context O
embedding O
components O
. O
We O
Ô¨Ånd O
that O
dynamic O
conceptualization O
exhibits O
a O
10.36%improvement O
in O
performance O
, O
and O
entity O
context O
embedding O
exhibits O
a O
2.56 O
% O
improvement O
in O
performance O
on O
our O
Chinese O
dataset O
: O
DUEL O
. O
By O
the O
analysis O
of O
examples O
in O
Figure O
1 O
, O
we O
Ô¨Ånd O
that O
dynamic O
conceptualization O
can O
mark O
the O
concepts O
of O
‚Äù O
Li O
Bai O
‚Äù O
in O
‚Äù O
Who O
is O
the O
singer O
of O
Li O
Bai O
? O
‚Äù O
, O
‚Äù O
How O
to O
play O
the O
hero O
Li O
Bai O
? O
‚Äù O
and O
‚Äù O
Which O
famous O
poems O
are O
written O
by O
Li O
Bai O
? O
‚Äù O
as O
‚Äù O
songs O
‚Äù O
, O
‚Äù O
game O
characters O
‚Äù O
and O
‚Äù O
poets O
‚Äù O
respectively O
. O
The O
correct O
conceptualization O
greatly O
facilitates O
the O
entity O
disambiguation O
. O
On O
the O
other O
hand O
, O
however O
, O
although O
we O
successfully O
mark O
the O
concept O
of O
‚Äù O
Li O
Bai O
‚Äù O
in O
‚Äù O
Wedding O
photos O
of O
Li O
Bai O
and O
Sa O
Beining O
‚Äù O
as O
‚Äù O
person O
‚Äù O
, O
it O
still O
disambiguates O
incorrectly O
without O
the O
help O
of O
entity O
context O
embedding O
, O
which O
indicates O
that O
dynamic O
conceptualization O
and O
entity O
context O
embedding O
can O
be O
complementary O
in O
NED O
. O
This O
result O
demonstrates O
that O
the O
conceptualization O
of O
entities O
is O
more O
direct O
and O
effective O
for O
the O
semantic O
disambiguation O
in O
short O
text O
entity O
disambiguation O
. O
In O
addition O
, O
we O
Ô¨Ånd O
that O
Ô¨Åne O
- O
grained O
conceptualization O
plays O
a O
signiÔ¨Åcant O
role O
in O
dynamic O
conceptualization.7415 O
Related O
works O
Many O
efforts O
have O
been O
devoted O
to O
NED O
in O
recent O
years O
. O
Some O
methods O
( O
Shen O
et O
al O
. O
, O
2014 O
; O
Ratinov O
et O
al O
. O
, O
2011 O
; O
Shen O
et O
al O
. O
, O
2012b O
, O
a O
; O
Han O
, O
2015 O
) O
exploit O
the O
Learning O
To O
Rank O
framework O
( O
LTR O
) O
( O
Liu O
, O
2009 O
) O
to O
rank O
the O
candidate O
entities O
, O
taking O
advantage O
of O
the O
relationships O
between O
all O
candidates O
. O
Most O
commonly O
used O
ranking O
models O
are O
the O
pairwise O
framework O
( O
Perceptron O
( O
Shen O
and O
Joshi O
, O
2005 O
) O
, O
RankSvm(Chingpei O
Lee O
, O
2014 O
) O
) O
and O
the O
listwise O
framework O
( O
ListNet O
( O
Cao O
et O
al O
. O
, O
2007 O
) O
) O
. O
( O
BaoXing O
et O
al O
. O
, O
2014 O
) O
proposed O
a O
named O
entity O
linking O
method O
based O
on O
a O
probabilistic O
topic O
model(Blei O
, O
2012 O
) O
, O
which O
employs O
the O
conceptual O
topic O
model O
to O
map O
words O
and O
mentioned O
entities O
into O
the O
same O
topic O
space O
. O
( O
Nakashole O
et O
al O
. O
, O
2013 O
) O
used O
a O
graphbased O
collaborative O
entity O
linking O
model O
. O
( O
Bilenko O
et O
al O
. O
, O
2003 O
) O
proposed O
using O
random O
walks O
for O
entity O
linking O
. O
Some O
models O
choosed O
to O
rely O
solely O
on O
the O
context O
of O
the O
links O
to O
learn O
entity O
representations O
, O
such O
as O
( O
Lazic O
et O
al O
. O
, O
2015 O
) O
, O
and O
some O
methods O
used O
a O
pipeline O
of O
existing O
annotators O
to O
Ô¨Ålter O
entity O
candidates O
such O
as O
( O
Ling O
et O
al O
. O
, O
2015 O
) O
. O
Different O
from O
these O
conventional O
work O
, O
we O
use O
multiple O
sources O
of O
information O
and O
a O
deep O
structured O
semantic O
model O
to O
achieve O
better O
NED O
performance O
. O
Many O
efforts O
have O
been O
devoted O
to O
NED O
for O
queries O
, O
such O
as O
( O
Hasibi O
et O
al O
. O
, O
2015 O
) O
and O
( O
Hasibi O
et O
al O
. O
, O
2017 O
) O
. O
Some O
approaches O
try O
to O
solve O
NED O
by O
making O
extensive O
use O
of O
deep O
neural O
networks O
( O
Globerson O
et O
al O
. O
, O
2016 O
) O
, O
or O
by O
adopting O
distributed O
representations O
of O
words O
or O
entities O
( O
Yamada O
et O
al O
. O
, O
2016 O
, O
2017 O
) O
. O
Other O
existing O
approaches O
take O
advantage O
of O
global O
context O
, O
which O
captures O
the O
coherence O
between O
mapped O
entities O
of O
the O
related O
keywords O
in O
a O
document O
( O
Cucerzan O
, O
2007 O
; O
Han O
et O
al O
. O
, O
2011 O
) O
. O
In O
( O
Globerson O
et O
al O
. O
, O
2016 O
) O
, O
the O
neural O
network O
model O
uses O
attention O
mechanism O
to O
focus O
on O
the O
contextual O
entities O
to O
be O
disambiguated O
. O
In O
( O
Yamada O
et O
al O
. O
, O
2016 O
, O
2017 O
) O
, O
the O
distributed O
representation O
of O
contexts O
models O
the O
relationships O
between O
words O
and O
entities O
or O
between O
documents O
and O
entities O
, O
where O
the O
distances O
between O
various O
vectors O
provides O
useful O
information O
for O
disambiguation O
. O
Different O
from O
these O
work O
where O
complicated O
techniques O
or O
features O
are O
used O
, O
we O
adopt O
external O
knowledge O
including O
factual O
and O
conceptual O
knowledge O
graphs O
for O
better O
NED O
performance O
. O
( O
Radhakrishnan O
and O
Varma O
, O
2018 O
) O
proposes O
a O
method O
to O
train O
entity O
embedding O
for O
entity O
similarity O
, O
but O
this O
method O
relies O
on O
a O
dense O
knowledge O
map O
, O
weuse O
the O
text O
and O
relationship O
information O
of O
entity O
to O
model O
the O
similarity O
between O
entity O
and O
context O
to O
improve O
the O
effect O
of O
NED O
. O
There O
are O
also O
previous O
work O
using O
concept O
or O
type O
information O
to O
improve O
NED O
performance O
. O
The O
models O
of O
( O
Hua O
et O
al O
. O
, O
2015 O
; O
Wang O
et O
al O
. O
, O
2015 O
; O
Priya O
Radhakrishnan O
, O
2018 O
; O
Isaiah O
Onando O
Mulang O
, O
2020 O
) O
try O
to O
map O
short O
text O
to O
a O
concept O
space O
, O
and O
then O
generate O
comprehensive O
concept O
vectors O
to O
represent O
the O
short O
text O
. O
( O
Raiman O
and O
Raiman O
, O
2018 O
) O
constructs O
a O
type O
ontology O
and O
a O
type O
classiÔ¨Åer O
to O
map O
entities O
to O
a O
closed O
type O
ontology O
. O
( O
Chen O
and O
Xiao O
, O
2018 O
) O
proposes O
to O
modeling O
context O
explicitly O
by O
entity O
concept O
, O
but O
we O
use O
a O
complementary O
way O
of O
coarse O
- O
grained O
and O
Ô¨Åne O
- O
grained O
to O
dynamically O
predict O
the O
concept O
according O
to O
the O
context O
and O
improve O
the O
effect O
of O
disambiguation O
. O
( O
Derczynski O
et O
al O
. O
, O
2015 O
) O
studies O
named O
entity O
recognition O
( O
NER O
) O
and O
named O
entity O
linking O
( O
NEL O
) O
for O
tweets O
. O
Unlike O
these O
work O
, O
our O
method O
uses O
Ô¨Åne O
- O
grained O
entity O
concepts O
and O
predicts O
concepts O
more O
accurately O
by O
using O
an O
advanced O
knowledge O
feature O
network O
. O
6 O
Conclusion O
We O
propose O
a O
knowledge O
- O
enhanced O
approach O
to O
short O
text O
entity O
disambiguation O
. O
Through O
bridging O
and O
facilitating O
semantic O
understanding O
of O
the O
Ô¨Ånegrained O
concept O
associated O
to O
a O
mentioned O
entity O
and O
entity O
embedding O
, O
the O
performance O
of O
entity O
disambiguation O
can O
be O
signiÔ¨Åcantly O
improved O
. O
The O
experimental O
results O
demonstrate O
that O
our O
approach O
outperforms O
existing O
SOTA O
methods O
on O
English O
and O
Chinese O
datasets O
for O
this O
task O
. O
At O
the O
same O
time O
, O
we O
constructed O
a O
large O
- O
scale O
manual O
- O
annotated O
Chinese O
dataset O
for O
short O
text O
entity O
disambiguation O
, O
which O
has O
been O
released O
with O
the O
paper O
for O
use O
by O
researchers O
. O
As O
a O
future O
direction O
of O
research O
, O
we O
plan O
to O
explore O
better O
conceptualization O
and O
semantic O
understanding O
methods O
, O
and O
further O
improve O
the O
performance O
of O
the O
short O
text O
entity O
disambiguation O
task O
. O
We O
intend O
to O
continue O
to O
update O
our O
Chinese O
dataset O
. O
In O
the O
future O
, O
We O
will O
use O
more O
modern O
embedding(such O
as O
BERT O
) O
or O
encoder(such O
as O
transformer O
) O
to O
obtain O
better O
embedding O
, O
and O
we O
also O
plan O
to O
conduct O
experiments O
to O
verify O
the O
effectiveness O
of O
our O
methods O
in O
other O
tasks O
related O
to O
semantic O
understanding O
such O
as O
Q&A O
, O
Dialogue O
, O
etc.742References O
HUAI O
Bao O
- O
Xing O
, O
BAO O
Teng O
- O
Fei O
, O
ZHU O
Heng O
- O
Shu O
, O
and O
LIU O
Qi O
. O
2014 O
. O
Topic O
modeling O
approach O
to O
named O
entity O
linking O
. O
Journal O
of O
Software O
. O
M. O
Bilenko O
, O
R. O
Mooney O
, O
W. O
Cohen O
, O
P. O
Ravikumar O
, O
and O
S. O
Fien O
- O
berg O
. O
2003 O
. O
Adaptive O
name O
matching O
in O
information O
integration O
. O
IEEE O
Intelligent O
Systems O
. O
Roi O
Blanco O
, O
Giuseppe O
Ottaviano O
, O
and O
Edgar O
Meij O
. O
2015 O
. O
Fast O
and O
space O
- O
efÔ¨Åcient O
entity O
linking O
for O
queries O
. O
In O
Proceedings O
of O
the O
Eighth O
ACM O
International O
Conference O
on O
Web O
Search O
and O
Data O
Mining O
, O
pages O
179 O
‚Äì O
188 O
. O
ACM O
. O
David O
M. O
Blei O
. O
2012 O
. O
Probabilistic O
Topic O
Models O
. O
Magazine O
Communications O
of O
the O
ACM O
, O
ACM O
New O
York O
, O
NY O
, O
USA O
. O
Z. O
Cao O
, O
T. O
Qin O
, O
T.-Y O
. O
Liu O
, O
M.-F. O
Tsai O
, O
and O
H. O
Li O
. O
2007 O
. O
Learning O
to O
rank O
: O
from O
pairwise O
approach O
to O
listwise O
approach O
. O
ICML O
. O
Liang O
J. O
Xie O
C. O
Chen O
, O
L. O
and O
Y O
. O
Xiao O
. O
2018 O
. O
Short O
text O
entity O
linking O
with O
Ô¨Åne O
- O
grained O
topics O
. O
pages O
457 O
‚Äì O
466 O
. O
CIKM O
. O
Chihjen O
Lin O
Chingpei O
Lee O
. O
2014 O
. O
Large O
- O
scale O
linear O
ranksvm O
. O
pages O
781‚Äì817 O
. O
Neural O
Computation O
. O
S O
Cucerzan O
. O
2007 O
. O
Large O
- O
scale O
named O
entity O
disambiguation O
based O
on O
wikipedia O
data O
. O
EMNLP O
CoNLL O
. O
Leon O
Derczynski O
, O
Diana O
Maynard O
, O
Giuseppe O
Rizzo O
, O
Marieke O
Van O
Erp O
, O
Genevieve O
Gorrell O
, O
Rapha O
¬®el O
Troncy O
, O
Johann O
Petrak O
, O
and O
Kalina O
Bontcheva O
. O
2015 O
. O
Analysis O
of O
named O
entity O
recognition O
and O
linking O
for O
tweets O
. O
Information O
Processing O
& O
Management O
, O
51(2):32‚Äì49 O
. O
A. O
Globerson O
, O
N. O
Lazic O
, O
S. O
Chakrabarti O
, O
A. O
Subramanya O
, O
M. O
Ringaard O
, O
and O
F.n O
Pereira O
. O
2016 O
. O
Collective O
entity O
resolution O
with O
multifocal O
attention O
. O
ACL O
. O
Wei O
Shen O
; O
Jianyong O
Wang O
; O
Jiawei O
Han O
. O
2015 O
. O
Entity O
linking O
with O
a O
knowledge O
base O
: O
Issues O
, O
techniques O
, O
and O
solutions O
. O
pages O
443 O
‚Äì O
460 O
. O
IEEE O
Transactions O
on O
Knowledge O
and O
Data O
Engineering O
. O
Xianpei O
Han O
, O
Le O
Sun O
, O
and O
Jun O
Zhao O
. O
2011 O
. O
Collective O
entity O
linking O
in O
web O
text O
: O
a O
graph O
- O
based O
method O
. O
page O
765‚Äì774 O
. O
SIGIR O
. O
Faegheh O
Hasibi O
, O
Krisztian O
Balog O
, O
and O
Svein O
Erik O
Bratsberg O
. O
2015 O
. O
Entity O
linking O
in O
queries O
: O
Tasks O
and O
evaluation O
. O
In O
Proceedings O
of O
the O
2015 O
International O
Conference O
on O
The O
Theory O
of O
Information O
Retrieval O
, O
pages O
171‚Äì180 O
. O
ACM O
. O
Faegheh O
Hasibi O
, O
Krisztian O
Balog O
, O
and O
Svein O
Erik O
Bratsberg O
. O
2017 O
. O
Entity O
linking O
in O
queries O
: O
EfÔ¨Åciency O
vs. O
effectiveness O
. O
In O
European O
Conference O
on O
Information O
Retrieval O
, O
pages O
40‚Äì53 O
. O
Springer O
. O
J. O
Hoffart O
, O
S. O
Seufert O
, O
D.B. O
Nguyen O
, O
M. O
Theobald O
, O
and O
G. O
Weikum O
. O
2012 O
. O
Kore O
: O
Keyphrase O
overlap O
relatedness O
for O
entity O
disambiguation O
. O
pages O
545‚Äì554 O
. O
ACM O
international O
conference O
on O
Information O
and O
knowledge O
management O
. O
Wen O
Hua O
, O
Zhongyuan O
Wang O
, O
Haixun O
Wang O
, O
Kai O
Zheng O
, O
and O
Xiaofang O
Zhou O
. O
2015 O
. O
Short O
text O
understanding O
through O
lexical O
- O
semantic O
analysis O
. O
IEEE O
31st O
International O
Conference O
on O
Data O
Engineering O
. O
Po O
- O
Sen O
Huang O
, O
Xiaodong O
He O
, O
Jianfeng O
Gao O
, O
Li O
Deng O
, O
Alex O
Acero O
, O
and O
Larry O
Heck O
. O
2013 O
. O
Learning O
deep O
structured O
semantic O
models O
for O
web O
search O
using O
clickthrough O
data O
. O
pages O
2333‚Äì2338 O
. O
CIKM O
. O
Akhilesh O
Vyas O
Saeedeh O
Shekarpour O
Maria O
Esther O
Vidal O
Soren O
Auer O
Jens O
Lehmann O
Isaiah O
Onando O
Mulang O
, O
Kuldeep O
Singh O
. O
2020 O
. O
Encoding O
knowledge O
graph O
entity O
aliases O
in O
attentive O
neural O
network O
for O
wikidata O
entity O
linking O
. O
page O
15 O
. O
WISE O
. O
Christos O
Faloutsos O
Pinar O
Duygulu O
Jia O
- O
Yu O
Pan O
, O
HyungJeong O
Yang O
. O
2004 O
. O
Automatic O
multimedia O
crossmodal O
correlation O
discovery O
. O
pages O
653‚Äì658 O
. O
KDD O
. O
Nevena O
Lazic O
, O
Amarnag O
Subramanya O
, O
Michael O
Ringgaard O
, O
and O
Fernando O
Pereira O
. O
2015 O
. O
Plato O
: O
A O
selective O
context O
model O
for O
entity O
resolution O
. O
TACL O
. O
Phong O
Le O
and O
Ivan O
Titov O
. O
2018 O
. O
Improving O
entity O
linking O
by O
modeling O
latent O
relations O
between O
mentions O
. O
ACL O
. O
Xiao O
Ling O
, O
Sameer O
Singh O
, O
and O
Daniel O
S O
Weld O
. O
2015 O
. O
Design O
challenges O
for O
entity O
linking O
. O
TACL O
. O
T.Y O
. O
Liu O
. O
2009 O
. O
Learning O
to O
rank O
for O
information O
retrieval O
. O
Foundations O
and O
Trends O
in O
Information O
Retrieval O
. O
N. O
Nakashole O
, O
T. O
Tylenda O
, O
and O
G. O
Weikum O
. O
2013 O
. O
Finegrained O
semantic O
typing O
of O
emerging O
entities O
. O
ACL O
. O
Zhou O
S.-Liu O
J. O
Wang O
J. O
Lin O
C.Y O
. O
Nie O
, O
F. O
and O
R. O
Pan O
. O
2018 O
. O
Aggregated O
semantic O
matching O
for O
short O
text O
entity O
linking O
. O
pages O
476‚Äì485 O
. O
CoNLL O
. O
Thomas O
Hofmann O
Octavian O
- O
Eugen O
Ganea O
. O
2017 O
. O
Deep O
joint O
entity O
disambiguation O
with O
local O
neural O
attention O
. O
page O
2619‚Äì2629 O
. O
ACL O
. O
Vasudeva O
Varma O
Priya O
Radhakrishnan O
, O
Partha O
Talukdar O
. O
2018 O
. O
Elden O
: O
Improved O
entity O
linking O
using O
densiÔ¨Åed O
knowledge O
graphs O
. O
page O
1844‚Äì1853 O
. O
NAACL O
. O
Talukdar O
P. O
Radhakrishnan O
, O
P. O
and O
V O
. O
Varma O
. O
2018 O
. O
Elden O
: O
Improved O
entity O
linking O
using O
densiÔ¨Åed O
knowledge O
graphs O
. O
pages O
1844‚Äì1853 O
. O
NAACL O
. O
Jonathan O
Raiman O
and O
Olivier O
Raiman O
. O
2018 O
. O
Deeptype O
: O
Multilingual O
entity O
linking O
by O
neural O
type O
system O
evolution O
. O
AAAI O
. O
L. O
Ratinov O
, O
D. O
Roth O
, O
D. O
Downey O
, O
and O
M. O
Anderson O
. O
2011 O
. O
Local O
and O
global O
algorithms O
for O
disambiguation O
to O
wikipedia O
. O
ACL.743G. O
Rizzo O
, O
B. O
Pereira O
, O
A. O
Varga O
, O
M. O
van O
Erp O
M O
, O
and O
A.E. O
Cano O
Basave O
. O
2017 O
. O
the O
named O
entity O
recognition O
and O
linking O
( O
neel O
) O
challenge O
series O
. O
Semantic O
Web O
Journal O
( O
in O
press O
) O
. O
L. O
Shen O
and O
A. O
K. O
Joshi O
. O
2005 O
. O
Ranking O
and O
reranking O
with O
per O
- O
ceptron O
. O
Mach O
. O
Learn O
. O
W. O
Shen O
, O
J. O
Wang O
, O
P. O
Luo O
, O
and O
M. O
Wang O
. O
2012a O
. O
Liege O
: O
Link O
entities O
in O
web O
lists O
with O
knowledge O
base O
. O
SIGKDD O
. O
W. O
Shen O
, O
J. O
Wang O
, O
P. O
Luo O
, O
and O
M. O
Wang O
. O
2012b O
. O
Linden O
: O
linking O
named O
entities O
with O
knowledge O
base O
via O
semantic O
knowledge O
. O
WWW O
. O
Wei O
Shen O
, O
Jianyong O
Wang O
, O
and O
Jiawei O
Han O
. O
2014 O
. O
Entity O
linking O
with O
a O
knowledge O
base O
: O
Issues O
, O
techniques O
, O
and O
solutions O
. O
pages O
443‚Äì460 O
. O
IEEE O
Transactions O
on O
Knowledge O
and O
Data O
Engineering O
. O
Kai O
Chen O
Greg O
Corrado O
Jeffrey O
Dean O
Tomas O
Mikolov O
, O
Ilya O
Sutskever O
. O
2013 O
. O
Distributed O
representations O
of O
words O
and O
phrases O
and O
their O
compositionality O
. O
pages O
3111‚Äì3119 O
. O
NIPS O
. O
Zhongyuan O
Wang O
, O
Kejun O
Zhao O
, O
Haixun O
Wang O
, O
Xiaofeng O
Meng O
, O
and O
Ji O
- O
RongWen O
. O
2015 O
. O
Query O
understanding O
through O
knowledge O
- O
based O
conceptualization O
. O
IJCAI O
. O
Bo O
Xu O
, O
Yong O
Xu O
, O
Jiaqing O
Liang O
, O
Chenhao O
Xie O
, O
Bin O
Liang O
, O
Wanyun O
Cui O
, O
and O
Yanghua O
Xiao O
. O
2017 O
. O
Cndbpedia O
: O
A O
never O
- O
ending O
chinese O
knowledge O
extraction O
system O
. O
In O
International O
Conference O
on O
Industrial O
, O
Engineering O
and O
Other O
Applications O
of O
Applied O
Intelligent O
Systems O
, O
pages O
428‚Äì438 O
. O
Springer O
. O
I. O
Yamada O
, O
H. O
Shindo O
, O
H. O
Takeda O
, O
and O
Y O
. O
Takefuji O
. O
2017 O
. O
Learning O
distributed O
representations O
of O
texts O
and O
entities O
from O
knowledge O
base O
. O
arXiv O
preprint O
. O
I. O
Yamada O
, O
H. O
Shindo O
, O
and O
H. O
Takedaand O
Y O
. O
Takefuji O
. O
2016 O
. O
Joint O
learning O
of O
the O
embedding O
of O
words O
and O
entities O
for O
named O
entity O
disambiguation O
. O
arXiv O
preprint O
. O
Luo O
Si O
Jeongwoo O
Ko O
Aditya O
P. O
Mathur O
Yi O
Fang O
, O
Naveen O
Somasundaram O
. O
2011 O
. O
Analysis O
of O
an O
expert O
search O
query O
log O
. O
pages O
1189‚Äì1190 O
. O
SIGIR.744Proceedings O
of O
the O
1st O
Conference O
of O
the O
Asia O
- O
PaciÔ¨Åc O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
10th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
745‚Äì758 O
December O
4 O
- O
7 O
, O
2020 O
. O
¬© O
2020 O
Association O
for O
Computational O
Linguistics O
More O
Data O
, O
More O
Relations O
, O
More O
Context O
and O
More O
Openness O
: O
A O
Review O
and O
Outlook O
for O
Relation O
Extraction O
Xu O
Han1‚àó O
, O
Tianyu O
Gao2‚àó O
, O
Yankai O
Lin3‚àó O
, O
Hao O
Peng1 O
, O
Yaoliang O
Yang1 O
, O
Chaojun O
Xiao1 O
, O
Zhiyuan O
Liu1‚Ä†,Peng O
Li3,Maosong O
Sun1 O
, O
Jie O
Zhou3 O
1Department O
of O
Computer O
Science O
and O
Technology O
, O
Tsinghua O
University O
, O
Beijing O
, O
China O
2Princeton O
University O
, O
Princeton O
, O
NJ O
, O
USA O
3Pattern O
Recognition O
Center O
, O
WeChat O
AI O
, O
Tencent O
Inc O
, O
China O
hanxu17@mails.tsinghua.edu.cn O
, O
tianyug@princeton.edu O
, O
yankailin@tencent.com O
Abstract O
Relational O
facts O
are O
an O
important O
component O
of O
human O
knowledge O
, O
which O
are O
hidden O
in O
vast O
amounts O
of O
text O
. O
In O
order O
to O
extract O
these O
facts O
from O
text O
, O
people O
have O
been O
working O
on O
relation O
extraction O
( O
RE O
) O
for O
years O
. O
From O
early O
pattern O
matching O
to O
current O
neural O
networks O
, O
existing O
RE O
methods O
have O
achieved O
signiÔ¨Åcant O
progress O
. O
Yet O
with O
explosion O
of O
Web O
text O
and O
emergence O
of O
new O
relations O
, O
human O
knowledge O
is O
increasing O
drastically O
, O
and O
we O
thus O
require O
‚Äú O
more O
‚Äù O
from O
RE O
: O
a O
more O
powerful O
RE O
system O
that O
can O
robustly O
utilize O
more O
data O
, O
efÔ¨Åciently O
learn O
more O
relations O
, O
easily O
handle O
more O
complicated O
context O
, O
and O
Ô¨Çexibly O
generalize O
to O
more O
open O
domains O
. O
In O
this O
paper O
, O
we O
look O
back O
at O
existing O
RE O
methods O
, O
analyze O
key O
challenges O
we O
are O
facing O
nowadays O
, O
and O
show O
promising O
directions O
towards O
more O
powerful O
RE O
. O
We O
hope O
our O
view O
can O
advance O
this O
Ô¨Åeld O
and O
inspire O
more O
efforts O
in O
the O
community.1 O
1 O
Introduction O
Relational O
facts O
organize O
knowledge O
of O
the O
world O
in O
a O
triplet O
format O
. O
These O
structured O
facts O
act O
as O
an O
import O
role O
of O
human O
knowledge O
and O
are O
explicitly O
or O
implicitly O
hidden O
in O
the O
text O
. O
For O
example O
, O
‚Äú O
Steve O
Jobs O
co O
- O
founded O
Apple O
‚Äù O
indicates O
the O
fact O
( O
Apple O
Inc. O
,founded O
by O
, O
Steve O
Jobs O
) O
, O
and O
we O
can O
also O
infer O
the O
fact O
( O
USA O
, O
contains O
, O
New O
York O
) O
from O
‚Äú O
Hamilton O
made O
its O
debut O
in O
New O
York O
, O
USA O
‚Äù O
. O
As O
these O
structured O
facts O
could O
beneÔ¨Åt O
downstream O
applications O
, O
e.g O
, O
knowledge O
graph O
completion O
( O
Bordes O
et O
al O
. O
, O
2013 O
; O
Wang O
et O
al O
. O
, O
2014 O
) O
, O
search O
engine O
( O
Xiong O
et O
al O
. O
, O
2017 O
; O
Schlichtkrull O
et O
al O
. O
, O
2018 O
) O
and O
question O
answering O
( O
Bordes O
et O
al O
. O
, O
2014 O
; O
Dong O
et O
al O
. O
, O
2015 O
) O
, O
many O
efforts O
have O
been O
devoted O
‚àóindicates O
equal O
contribution O
‚Ä†Corresponding O
author O
e O
- O
mail O
: O
liuzy@tsinghua.edu.cn O
1Most O
of O
the O
papers O
mentioned O
in O
this O
work O
are O
collected O
into O
the O
following O
paper O
list O
https://github O
. O
com O
/ O
thunlp O
/ O
NREPapers O
.to O
researching O
relation O
extraction O
( O
RE O
) O
, O
which O
aims O
at O
extracting O
relational O
facts O
from O
plain O
text O
. O
More O
speciÔ¨Åcally O
, O
after O
identifying O
entity O
mentions O
( O
e.g. O
, O
USA O
andNew O
York O
) O
in O
text O
, O
the O
main O
goal O
of O
RE O
is O
to O
classify O
relations O
( O
e.g. O
, O
contains O
) O
between O
these O
entity O
mentions O
from O
their O
context O
. O
The O
pioneering O
explorations O
of O
RE O
lie O
in O
statistical O
approaches O
, O
such O
as O
pattern O
mining O
( O
Huffman O
, O
1995 O
; O
Califf O
and O
Mooney O
, O
1997 O
) O
, O
feature O
- O
based O
methods O
( O
Kambhatla O
, O
2004 O
) O
and O
graphical O
models O
( O
Roth O
and O
Yih O
, O
2002 O
) O
. O
Recently O
, O
with O
the O
development O
of O
deep O
learning O
, O
neural O
models O
have O
been O
widely O
adopted O
for O
RE O
( O
Zeng O
et O
al O
. O
, O
2014 O
; O
Zhang O
et O
al O
. O
, O
2015 O
) O
and O
achieved O
superior O
results O
. O
These O
RE O
methods O
have O
bridged O
the O
gap O
between O
unstructured O
text O
and O
structured O
knowledge O
, O
and O
shown O
their O
effectiveness O
on O
several O
public O
benchmarks O
. O
Despite O
the O
success O
of O
existing O
RE O
methods O
, O
most O
of O
them O
still O
work O
in O
a O
simpliÔ¨Åed O
setting O
. O
These O
methods O
mainly O
focus O
on O
training O
models O
with O
large O
amounts O
ofhuman O
annotations O
to O
classify O
two O
given O
entities O
within O
one O
sentence O
into O
pre O
- O
deÔ¨Åned O
relations O
. O
However O
, O
the O
real O
world O
is O
much O
more O
complicated O
than O
this O
simple O
setting O
: O
( O
1 O
) O
collecting O
high O
- O
quality O
human O
annotations O
is O
expensive O
and O
time O
- O
consuming O
, O
( O
2 O
) O
many O
long O
- O
tail O
relations O
can O
not O
provide O
large O
amounts O
of O
training O
examples O
, O
( O
3 O
) O
most O
facts O
are O
expressed O
by O
long O
context O
consisting O
of O
multiple O
sentences O
, O
and O
moreover O
( O
4 O
) O
using O
a O
pre O
- O
deÔ¨Åned O
set O
to O
cover O
those O
relations O
with O
open O
- O
ended O
growth O
is O
difÔ¨Åcult O
. O
Hence O
, O
to O
build O
an O
effective O
and O
robust O
RE O
system O
for O
real O
- O
world O
deployment O
, O
there O
are O
still O
some O
more O
complex O
scenarios O
to O
be O
further O
investigated O
. O
In O
this O
paper O
, O
we O
review O
existing O
RE O
methods O
( O
Section O
2 O
) O
as O
well O
as O
latest O
RE O
explorations O
( O
Section O
3 O
) O
targeting O
more O
complex O
RE O
scenarios O
. O
Those O
feasible O
approaches O
leading O
to O
better O
RE O
abilities O
still O
require O
further O
efforts O
, O
and O
here O
we O
summarize O
them O
into O
four O
directions:745(1)Utilizing O
More O
Data O
( O
Section O
3.1 O
) O
. O
Supervised O
RE O
methods O
heavily O
rely O
on O
expensive O
human O
annotations O
, O
while O
distant O
supervision O
( O
Mintz O
et O
al O
. O
, O
2009 O
) O
introduces O
more O
auto O
- O
labeled O
data O
to O
alleviate O
this O
issue O
. O
Yet O
distant O
methods O
bring O
noise O
examples O
and O
just O
utilize O
single O
sentences O
mentioning O
entity O
pairs O
, O
which O
signiÔ¨Åcantly O
weaken O
extraction O
performance O
. O
Designing O
schemas O
to O
obtain O
highquality O
and O
high O
- O
coverage O
data O
to O
train O
robust O
RE O
models O
still O
remains O
a O
problem O
to O
be O
explored O
. O
( O
2)Performing O
More O
EfÔ¨Åcient O
Learning O
( O
Section O
3.2 O
) O
. O
Lots O
of O
long O
- O
tail O
relations O
only O
contain O
a O
handful O
of O
training O
examples O
. O
However O
, O
it O
is O
hard O
for O
conventional O
RE O
methods O
to O
well O
generalize O
relation O
patterns O
from O
limited O
examples O
like O
humans O
. O
Therefore O
, O
developing O
efÔ¨Åcient O
learning O
schemas O
to O
make O
better O
use O
of O
limited O
or O
few O
- O
shot O
examples O
is O
a O
potential O
research O
direction O
. O
( O
3)Handling O
More O
Complicated O
Context O
( O
Section O
3.3 O
) O
. O
Many O
relational O
facts O
are O
expressed O
in O
complicated O
context O
( O
e.g. O
multiple O
sentences O
or O
even O
documents O
) O
, O
while O
most O
existing O
RE O
models O
focus O
on O
extracting O
intra O
- O
sentence O
relations O
. O
To O
cover O
those O
complex O
facts O
, O
it O
is O
valuable O
to O
investigate O
RE O
in O
more O
complicated O
context O
. O
( O
4)Orienting O
More O
Open O
Domains O
( O
Section O
3.4 O
) O
. O
New O
relations O
emerge O
every O
day O
from O
different O
domains O
in O
the O
real O
world O
, O
and O
thus O
it O
is O
hard O
to O
cover O
all O
of O
them O
by O
hand O
. O
However O
, O
conventional O
RE O
frameworks O
are O
generally O
designed O
for O
pre O
- O
deÔ¨Åned O
relations O
. O
Therefore O
, O
how O
to O
automatically O
detect O
undeÔ¨Åned O
relations O
in O
open O
domains O
remains O
an O
open O
problem O
. O
Besides O
the O
introduction O
of O
promising O
directions O
, O
we O
also O
point O
out O
two O
key O
challenges O
for O
existing O
methods O
: O
( O
1 O
) O
learning O
from O
text O
or O
names O
( O
Section O
4.1 O
) O
and O
( O
2 O
) O
datasets O
towards O
special O
interests(Section O
4.2 O
) O
. O
We O
hope O
that O
all O
these O
contents O
could O
encourage O
the O
community O
to O
make O
further O
exploration O
and O
breakthrough O
towards O
better O
RE O
. O
2 O
Background O
and O
Existing O
Work O
Information O
extraction O
( O
IE O
) O
aims O
at O
extracting O
structural O
information O
from O
unstructured O
text O
, O
which O
is O
an O
important O
Ô¨Åeld O
in O
natural O
language O
processing O
( O
NLP O
) O
. O
Relation O
extraction O
( O
RE O
) O
, O
as O
an O
important O
task O
in O
IE O
, O
particularly O
focuses O
on O
extracting O
relations O
between O
entities O
. O
A O
complete O
relation O
extraction O
system O
consists O
of O
a O
named O
entity O
recognizer O
to O
identify O
named O
entities O
( O
e.g. O
, O
people O
, O
organizations O
, O
locations O
) O
from O
text O
, O
an O
entity O
linker O
to O
link O
entiTim O
Cook O
is O
Apple O
‚Äôs O
current O
CEO.0.050.010.89 O
... O
FounderPlace O
of O
BirthCEO O
Figure O
1 O
: O
An O
example O
of O
RE O
. O
Given O
two O
entities O
and O
one O
sentence O
mentioning O
them O
, O
RE O
models O
classify O
the O
relation O
between O
them O
within O
a O
pre O
- O
deÔ¨Åned O
relation O
set O
. O
ties O
to O
existing O
knowledge O
graphs O
( O
KGs O
, O
necessary O
when O
using O
relation O
extraction O
for O
knowledge O
graph O
completion O
) O
, O
and O
a O
relational O
classiÔ¨Åer O
to O
determine O
relations O
between O
entities O
by O
given O
context O
. O
Among O
these O
steps O
, O
identifying O
the O
relation O
is O
the O
most O
crucial O
and O
difÔ¨Åcult O
task O
, O
since O
it O
requires O
models O
to O
well O
understand O
the O
semantics O
of O
the O
context O
. O
Hence O
, O
RE O
generally O
focuses O
on O
researching O
the O
classiÔ¨Åcation O
part O
, O
which O
is O
also O
known O
as O
relation O
classiÔ¨Åcation O
. O
As O
shown O
in O
Figure O
1 O
, O
a O
typical O
RE O
setting O
is O
that O
given O
a O
sentence O
with O
two O
marked O
entities O
, O
models O
need O
to O
classify O
the O
sentence O
into O
one O
of O
the O
pre O
- O
deÔ¨Åned O
relations2 O
. O
In O
this O
section O
, O
we O
introduce O
the O
development O
of O
RE O
methods O
following O
the O
typical O
supervised O
setting O
, O
from O
early O
pattern O
- O
based O
methods O
, O
statistical O
approaches O
, O
to O
recent O
neural O
models O
. O
2.1 O
Pattern O
Extraction O
Models O
The O
pioneering O
methods O
use O
sentence O
analysis O
tools O
to O
identify O
syntactic O
elements O
in O
text O
, O
then O
automatically O
construct O
pattern O
rules O
from O
these O
elements O
( O
Soderland O
et O
al O
. O
, O
1995 O
; O
Kim O
and O
Moldovan O
, O
1995 O
; O
Huffman O
, O
1995 O
; O
Califf O
and O
Mooney O
, O
1997 O
) O
. O
In O
order O
to O
extract O
patterns O
with O
better O
coverage O
and O
accuracy O
, O
later O
work O
involves O
larger O
corpora O
( O
Carlson O
et O
al O
. O
, O
2010 O
) O
, O
more O
formats O
of O
patterns O
( O
Nakashole O
et O
al O
. O
, O
2012 O
; O
Jiang O
et O
al O
. O
, O
2017 O
) O
, O
and O
more O
efÔ¨Åcient O
ways O
of O
extraction O
( O
Zheng O
et O
al O
. O
, O
2019 O
) O
. O
As O
automatically O
constructed O
patterns O
may O
have O
mistakes O
, O
most O
of O
the O
above O
methods O
require O
further O
examinations O
from O
human O
experts O
, O
which O
is O
the O
main O
limitation O
of O
pattern O
- O
based O
models O
. O
2.2 O
Statistical O
Relation O
Extraction O
Models O
As O
compared O
to O
using O
pattern O
rules O
, O
statistical O
methods O
bring O
better O
coverage O
and O
require O
less O
human O
efforts O
. O
Thus O
statistical O
relation O
extraction O
( O
SRE O
) O
has O
been O
extensively O
studied O
. O
2Sometimes O
there O
is O
a O
special O
class O
in O
the O
relation O
set O
indicating O
that O
the O
sentence O
does O
not O
express O
any O
pre O
- O
speciÔ¨Åed O
relation O
( O
usually O
named O
as O
N O
/ O
A).746One O
typical O
SRE O
approach O
is O
feature O
- O
based O
methods O
( O
Kambhatla O
, O
2004 O
; O
Zhou O
et O
al O
. O
, O
2005 O
; O
Jiang O
and O
Zhai O
, O
2007 O
; O
Nguyen O
et O
al O
. O
, O
2007 O
) O
, O
which O
design O
lexical O
, O
syntactic O
and O
semantic O
features O
for O
entity O
pairs O
and O
their O
corresponding O
context O
, O
and O
then O
input O
these O
features O
into O
relation O
classiÔ¨Åers O
. O
Due O
to O
the O
wide O
use O
of O
support O
vector O
machines O
( O
SVM O
) O
, O
kernel O
- O
based O
methods O
have O
been O
widely O
explored O
, O
which O
design O
kernel O
functions O
for O
SVM O
to O
measure O
the O
similarities O
between O
relation O
representations O
and O
textual O
instances O
( O
Culotta O
and O
Sorensen O
, O
2004 O
; O
Bunescu O
and O
Mooney O
, O
2005 O
; O
Zhao O
and O
Grishman O
, O
2005 O
; O
Mooney O
and O
Bunescu O
, O
2006 O
; O
Zhang O
et O
al O
. O
, O
2006b O
, O
a O
; O
Wang O
, O
2008 O
) O
. O
There O
are O
also O
some O
other O
statistical O
methods O
focusing O
on O
extracting O
and O
inferring O
the O
latent O
information O
hidden O
in O
the O
text O
. O
Graphical O
methods(Roth O
and O
Yih O
, O
2002 O
, O
2004 O
; O
Sarawagi O
and O
Cohen O
, O
2005 O
; O
Yu O
and O
Lam O
, O
2010 O
) O
abstract O
the O
dependencies O
between O
entities O
, O
text O
and O
relations O
in O
the O
form O
of O
directed O
acyclic O
graphs O
, O
and O
then O
use O
inference O
models O
to O
identify O
the O
correct O
relations O
. O
Inspired O
by O
the O
success O
of O
embedding O
models O
in O
other O
NLP O
tasks O
( O
Mikolov O
et O
al O
. O
, O
2013a O
, O
b O
) O
, O
there O
are O
also O
efforts O
in O
encoding O
text O
into O
low O
- O
dimensional O
semantic O
spaces O
and O
extracting O
relations O
from O
textual O
embeddings O
( O
Weston O
et O
al O
. O
, O
2013 O
; O
Riedel O
et O
al O
. O
, O
2013 O
; O
Gormley O
et O
al O
. O
, O
2015 O
) O
. O
Furthermore O
, O
Bordes O
et O
al O
. O
( O
2013),Wang O
et O
al O
. O
( O
2014 O
) O
and O
Lin O
et O
al O
. O
( O
2015 O
) O
utilize O
KG O
embeddings O
for O
RE O
. O
Although O
SRE O
has O
been O
widely O
studied O
, O
it O
still O
faces O
some O
challenges O
. O
Feature O
- O
based O
and O
kernelbased O
models O
require O
many O
efforts O
to O
design O
features O
or O
kernel O
functions O
. O
While O
graphical O
and O
embedding O
methods O
can O
predict O
relations O
without O
too O
much O
human O
intervention O
, O
they O
are O
still O
limited O
in O
model O
capacities O
. O
There O
are O
some O
surveys O
systematically O
introducing O
SRE O
models O
( O
Zelenko O
et O
al O
. O
, O
2003 O
; O
Bach O
and O
Badaskar O
, O
2007 O
; O
Pawar O
et O
al O
. O
, O
2017 O
) O
. O
In O
this O
paper O
, O
we O
do O
not O
spend O
too O
much O
space O
for O
SRE O
and O
focus O
more O
on O
neural O
- O
based O
models O
. O
2.3 O
Neural O
Relation O
Extraction O
Models O
Neural O
relation O
extraction O
( O
NRE O
) O
models O
introduce O
neural O
networks O
to O
automatically O
extract O
semantic O
features O
from O
text O
. O
Compared O
with O
SRE O
models O
, O
NRE O
methods O
can O
effectively O
capture O
textual O
information O
and O
generalize O
to O
wider O
range O
of O
data O
. O
Studies O
in O
NRE O
mainly O
focus O
on O
designing O
and O
utilizing O
various O
network O
architectures O
to O
capture O
the O
relational O
semantics O
within O
text O
, O
such O
as O
recurBefore O
2013 O
2013 O
2014 O
2015 O
2016 O
NowF1 O
Score O
( O
% O
) O
77.6 O
( O
SRE)82.4 O
82.784.386.389.5Figure O
2 O
: O
The O
performance O
of O
state O
- O
of O
- O
the O
- O
art O
RE O
models O
in O
different O
years O
on O
widely O
- O
used O
dataset O
SemEval2010 O
Task O
8 O
. O
The O
adoption O
of O
neural O
models O
( O
since O
2013 O
) O
has O
brought O
great O
improvement O
in O
performance O
. O
sive O
neural O
networks O
( O
Socher O
et O
al O
. O
, O
2012 O
; O
Miwa O
and O
Bansal O
, O
2016 O
) O
that O
learn O
compositional O
representations O
for O
sentences O
recursively O
, O
convolutional O
neural O
networks O
( O
CNNs O
) O
( O
Liu O
et O
al O
. O
, O
2013 O
; O
Zeng O
et O
al O
. O
, O
2014 O
; O
Santos O
et O
al O
. O
, O
2015 O
; O
Nguyen O
and O
Grishman O
, O
2015b O
; O
Zeng O
et O
al O
. O
, O
2015 O
; O
Huang O
and O
Wang O
, O
2017 O
) O
that O
effectively O
model O
local O
textual O
patterns O
, O
recurrent O
neural O
networks O
( O
RNNs O
) O
( O
Zhang O
and O
Wang O
, O
2015 O
; O
Nguyen O
and O
Grishman O
, O
2015a O
; O
Vu O
et O
al O
. O
, O
2016 O
; O
Zhang O
et O
al O
. O
, O
2015 O
) O
that O
can O
better O
handle O
long O
sequential O
data O
, O
graph O
neural O
networks O
( O
GNNs O
) O
( O
Zhang O
et O
al O
. O
, O
2018 O
; O
Zhu O
et O
al O
. O
, O
2019a O
) O
that O
build O
word O
/ O
entity O
graphs O
for O
reasoning O
, O
and O
attention O
- O
based O
neural O
networks O
( O
Zhou O
et O
al O
. O
, O
2016 O
; O
Wang O
et O
al O
. O
, O
2016 O
; O
Xiao O
and O
Liu O
, O
2016 O
) O
that O
utilize O
attention O
mechanism O
to O
aggregate O
global O
relational O
information O
. O
Different O
from O
SRE O
models O
, O
NRE O
mainly O
utilizes O
word O
embeddings O
and O
position O
embeddings O
instead O
of O
hand O
- O
craft O
features O
as O
inputs O
. O
Word O
embeddings O
( O
Turian O
et O
al O
. O
, O
2010 O
; O
Mikolov O
et O
al O
. O
, O
2013b O
) O
are O
the O
most O
used O
input O
representations O
in O
NLP O
, O
which O
encode O
the O
semantic O
meaning O
of O
words O
into O
vectors O
. O
In O
order O
to O
capture O
the O
entity O
information O
in O
text O
, O
position O
embeddings O
( O
Zeng O
et O
al O
. O
, O
2014 O
) O
are O
introduced O
to O
specify O
the O
relative O
distances O
between O
words O
and O
entities O
. O
Except O
for O
word O
embeddings O
and O
position O
embeddings O
, O
there O
are O
also O
other O
works O
integrating O
syntactic O
information O
into O
NRE O
models O
. O
Xu O
et O
al O
. O
( O
2015a O
) O
and O
Xu O
et O
al O
. O
( O
2015b O
) O
adopt O
CNNs O
and O
RNNs O
over O
shortest O
dependency O
paths O
respectively O
. O
Liu O
et O
al O
. O
( O
2015 O
) O
propose O
a O
recursive O
neural O
network O
based O
on O
augmented O
dependency O
paths O
. O
Xu O
et O
al O
. O
( O
2016 O
) O
and O
Cai O
et O
al O
. O
( O
2016 O
) O
utilize O
deep O
RNNs O
to O
make O
further O
use O
of O
dependency O
paths O
. O
Besides O
, O
there O
are O
some O
efforts O
combining O
NRE O
with O
universal O
schemas O
( O
Verga O
et O
al O
. O
, O
2016 O
; O
Verga O
and O
McCallum,747CEOfounderproduct O
I O
looked O
up O
Apple O
Inc. O
on O
my O
iPhone.iPhone O
is O
designed O
by O
Apple O
Inc.iPhone O
is O
a O
iconic O
product O
of O
Apple O
. O
productApple O
Inc.iPhoneApple O
Inc. O
Steve O
JobsTim O
CookiPhoneFigure O
3 O
: O
An O
example O
of O
distantly O
supervised O
relation O
extraction O
. O
With O
the O
fact O
( O
Apple O
Inc. O
, O
product O
, O
iPhone O
) O
, O
DS O
Ô¨Ånds O
all O
sentences O
mentioning O
the O
two O
entities O
and O
annotates O
them O
with O
the O
relation O
product O
, O
which O
inevitably O
brings O
noise O
labels O
. O
2016 O
; O
Riedel O
et O
al O
. O
, O
2013 O
) O
. O
Recently O
, O
Transformers O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
and O
pre O
- O
trained O
language O
models O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
have O
also O
been O
explored O
for O
NRE O
( O
Du O
et O
al O
. O
, O
2018 O
; O
Verga O
et O
al O
. O
, O
2018 O
; O
Wu O
and O
He O
, O
2019 O
; O
Baldini O
Soares O
et O
al O
. O
, O
2019 O
) O
and O
have O
achieved O
new O
state O
- O
of O
- O
the O
- O
arts O
. O
By O
concisely O
reviewing O
the O
above O
techniques O
, O
we O
are O
able O
to O
track O
the O
development O
of O
RE O
from O
pattern O
and O
statistical O
methods O
to O
neural O
models O
. O
Comparing O
the O
performance O
of O
state O
- O
of O
- O
the O
- O
art O
RE O
models O
in O
years O
( O
Figure O
2 O
) O
, O
we O
can O
see O
the O
vast O
increase O
since O
the O
emergence O
of O
NRE O
, O
which O
demonstrates O
the O
power O
of O
neural O
methods O
. O
3 O
‚Äú O
More O
‚Äù O
Directions O
for O
RE O
Although O
the O
above O
- O
mentioned O
NRE O
models O
have O
achieved O
superior O
results O
on O
benchmarks O
, O
they O
are O
still O
far O
from O
solving O
the O
problem O
of O
RE O
. O
Most O
of O
these O
models O
utilize O
abundant O
human O
annotations O
and O
just O
aim O
at O
extracting O
pre O
- O
deÔ¨Åned O
relations O
within O
single O
sentences O
. O
Hence O
, O
it O
is O
hard O
for O
them O
to O
work O
well O
in O
complex O
cases O
. O
In O
fact O
, O
there O
have O
been O
various O
works O
exploring O
feasible O
approaches O
that O
lead O
to O
better O
RE O
abilities O
on O
realworld O
scenarios O
. O
In O
this O
section O
, O
we O
summarize O
these O
exploratory O
efforts O
into O
four O
directions O
, O
and O
give O
our O
review O
and O
outlook O
about O
these O
directions O
. O
3.1 O
Utilizing O
More O
Data O
Supervised O
NRE O
models O
suffer O
from O
the O
lack O
of O
large O
- O
scale O
high O
- O
quality O
training O
data O
, O
since O
manually O
labeling O
data O
is O
time O
- O
consuming O
and O
humanintensive O
. O
To O
alleviate O
this O
issue O
, O
distant O
supervision O
( O
DS O
) O
assumption O
has O
been O
used O
to O
automatically O
label O
data O
by O
aligning O
existing O
KGs O
with O
plain O
text O
( O
Mintz O
et O
al O
. O
, O
2009 O
; O
Nguyen O
and O
Moschitti O
, O
2011 O
; O
Min O
et O
al O
. O
, O
2013 O
) O
. O
As O
shown O
in O
Figure O
3 O
, O
forDataset O
# O
Rel O
. O
# O
Fact O
# O
Inst O
. O
N O
/ O
A O
NYT-10 O
53 O
377,980 O
694,491 O
79.43 O
% O
Wiki O
- O
Distant O
454 O
605,877 O
1,108,288 O
47.61 O
% O
Table O
1 O
: O
Statistics O
for O
NYT-10 O
and O
Wiki O
- O
Distant O
. O
Four O
columns O
stand O
for O
numbers O
of O
relations O
, O
facts O
and O
instances O
, O
and O
proportions O
of O
N O
/ O
A O
instances O
respectively O
. O
Model O
NYT-10 O
Wiki O
- O
Distant O
PCNN O
- O
ONE O
0.340 O
0.214 O
PCNN O
- O
ATT O
0.349 O
0.222 O
BERT O
0.458 O
0.361 O
Table O
2 O
: O
Area O
under O
the O
curve O
( O
AUC O
) O
of O
PCNN O
- O
ONE O
( O
Zeng O
et O
al O
. O
, O
2015 O
) O
, O
PCNN O
- O
ATT O
( O
Lin O
et O
al O
. O
, O
2016 O
) O
and O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
on O
two O
datasets O
. O
any O
entity O
pair O
in O
KGs O
, O
sentences O
mentioning O
both O
the O
entities O
will O
be O
labeled O
with O
their O
corresponding O
relations O
in O
KGs O
. O
Large O
- O
scale O
training O
examples O
can O
be O
easily O
constructed O
by O
this O
heuristic O
scheme O
. O
Although O
DS O
provides O
a O
feasible O
approach O
to O
utilize O
more O
data O
, O
this O
automatic O
labeling O
mechanism O
is O
inevitably O
accompanied O
by O
the O
wrong O
labeling O
problem O
. O
The O
reason O
is O
that O
not O
all O
sentences O
mentioning O
the O
two O
entities O
express O
their O
relations O
in O
KGs O
exactly O
. O
For O
example O
, O
we O
may O
mistakenly O
label O
‚Äú O
Bill O
Gates O
retired O
from O
Microsoft O
‚Äù O
with O
the O
relation O
founder O
, O
if O
( O
Bill O
Gates O
, O
founder O
, O
Microsoft O
) O
is O
a O
relational O
fact O
in O
KGs O
. O
The O
existing O
methods O
to O
alleviate O
the O
noise O
problem O
can O
be O
divided O
into O
three O
major O
approaches O
: O
( O
1 O
) O
Some O
methods O
adopt O
multi O
- O
instance O
learning O
by O
combining O
sentences O
with O
same O
entity O
pairs O
and O
then O
selecting O
informative O
instances O
from O
them O
. O
Riedel O
et O
al O
. O
( O
2010 O
) O
; O
Hoffmann O
et O
al O
. O
( O
2011 O
) O
; O
Surdeanu O
et O
al O
. O
( O
2012 O
) O
utilize O
graphical O
model O
to O
infer O
the O
informative O
sentences O
, O
while O
Zeng O
et O
al O
. O
( O
2015 O
) O
use O
a O
simple O
heuristic O
selection O
strategy O
. O
Later O
on O
, O
Lin O
et O
al O
. O
( O
2016 O
) O
; O
Zhang O
et O
al O
. O
( O
2017 O
) O
; O
Han O
et O
al O
. O
( O
2018c O
) O
; O
Li O
et O
al O
. O
( O
2020 O
) O
; O
Zhu O
et O
al O
. O
( O
2019c O
) O
; O
Hu O
et O
al O
. O
( O
2019 O
) O
design O
attention O
mechanisms O
to O
highlight O
informative O
instances O
for O
RE O
. O
( O
2)Incorporating O
extra O
context O
information O
to O
denoise O
DS O
data O
has O
also O
been O
explored O
, O
such O
as O
incorporating O
KGs O
as O
external O
information O
to O
guide O
instance O
selection O
( O
Ji O
et O
al O
. O
, O
2017 O
; O
Han O
et O
al O
. O
, O
2018b O
; O
Zhang O
et O
al O
. O
, O
2019a O
; O
Qu O
et O
al O
. O
, O
2019 O
) O
and O
adopting O
multi O
- O
lingual O
corpora O
for O
the O
information O
consistency O
and O
complementarity O
( O
Verga O
et O
al O
. O
, O
2016 O
; O
Lin O
et O
al O
. O
, O
2017 O
; O
Wang O
et O
al O
. O
, O
2018 O
) O
. O
( O
3 O
) O
Many O
methods O
tend O
to O
utilize O
sophisticated7480 O
10 O
20 O
30 O
40 O
Relations100101102103104105Numbers O
of O
InstancesRelation O
Distribution O
on O
NYT-10 O
0 O
100 O
200 O
300 O
400 O
Relations100101102103104Numbers O
of O
InstancesRelation O
Distribution O
on O
Wiki O
- O
DistantFigure O
4 O
: O
Relation O
distributions O
( O
log O
- O
scale O
) O
on O
the O
training O
part O
of O
DS O
datasets O
NYT-10 O
and O
Wiki O
- O
Distant O
, O
suggesting O
that O
real O
- O
world O
relation O
distributions O
suffer O
from O
the O
long O
- O
tail O
problem O
. O
mechanisms O
and O
training O
strategies O
to O
enhance O
distantly O
supervised O
NRE O
models O
. O
Vu O
et O
al O
. O
( O
2016 O
) O
; O
Beltagy O
et O
al O
. O
( O
2019 O
) O
combine O
different O
architectures O
and O
training O
strategies O
to O
construct O
hybrid O
frameworks O
. O
Liu O
et O
al O
. O
( O
2017 O
) O
incorporate O
a O
softlabel O
scheme O
by O
changing O
unconÔ¨Ådent O
labels O
during O
training O
. O
Furthermore O
, O
reinforcement O
learning O
( O
Feng O
et O
al O
. O
, O
2018 O
; O
Zeng O
et O
al O
. O
, O
2018 O
) O
and O
adversarial O
training O
( O
Wu O
et O
al O
. O
, O
2017 O
; O
Wang O
et O
al O
. O
, O
2018 O
; O
Han O
et O
al O
. O
, O
2018a O
) O
have O
also O
been O
adopted O
in O
DS O
. O
The O
researchers O
have O
formed O
a O
consensus O
that O
utilizing O
more O
data O
is O
a O
potential O
way O
towards O
more O
powerful O
RE O
models O
, O
and O
there O
still O
remains O
some O
open O
problems O
worth O
exploring O
: O
( O
1 O
) O
Existing O
DS O
methods O
focus O
on O
denoising O
auto O
- O
labeled O
instances O
and O
it O
is O
certainly O
meaningful O
to O
follow O
this O
research O
direction O
. O
Besides O
, O
current O
DS O
schemes O
are O
still O
similar O
to O
the O
original O
one O
in O
( O
Mintz O
et O
al O
. O
, O
2009 O
) O
, O
which O
just O
covers O
the O
case O
that O
the O
entity O
pairs O
are O
mentioned O
in O
the O
same O
sentences O
. O
To O
achieve O
better O
coverage O
and O
less O
noise O
, O
exploring O
better O
DS O
schemes O
for O
autolabeling O
data O
is O
also O
valuable O
. O
( O
2 O
) O
Inspired O
by O
recent O
work O
in O
adopting O
pretrained O
language O
models O
( O
Zhang O
et O
al O
. O
, O
2019b O
; O
Wu O
and O
He O
, O
2019 O
; O
Baldini O
Soares O
et O
al O
. O
, O
2019 O
) O
and O
active O
learning O
( O
Zheng O
et O
al O
. O
, O
2019 O
) O
for O
RE O
, O
to O
perform O
unsupervised O
or O
semi O
- O
supervised O
learning O
for O
utilizing O
large O
- O
scale O
unlabeled O
data O
as O
well O
as O
using O
knowledge O
from O
KGs O
and O
introducing O
human O
experts O
in O
the O
loop O
is O
also O
promising O
. O
Besides O
addressing O
existing O
approaches O
and O
future O
directions O
, O
we O
also O
propose O
a O
new O
DS O
dataset O
to O
advance O
this O
Ô¨Åeld O
, O
which O
will O
be O
released O
once O
the O
paper O
is O
published O
. O
The O
most O
used O
benchmark O
for O
DS O
, O
NYT-10 O
( O
Riedel O
et O
al O
. O
, O
2010 O
) O
, O
suffers O
from O
small O
amount O
of O
relations O
, O
limited O
relation O
domains O
and O
extreme O
long O
- O
tail O
relation O
performance O
. O
To O
Query O
InstancefounderproductiPhone O
is O
designed O
by O
Apple O
Inc. O
Steve O
Jobs O
is O
the O
co O
- O
founder O
of O
Apple O
Inc. O
Bill O
Gates O
founded O
Microsoft.founder?Tim O
Cook O
is O
Apple O
‚Äôs O
current O
CEO.CEO O
Supporting O
SetFigure O
5 O
: O
An O
example O
of O
few O
- O
shot O
RE O
. O
Give O
a O
few O
instances O
for O
new O
relation O
types O
, O
few O
- O
shot O
RE O
models O
classify O
query O
sentences O
into O
one O
of O
the O
given O
relations O
. O
alleviate O
these O
drawbacks O
, O
we O
utilize O
Wikipedia O
and O
Wikidata O
( O
Vrande O
Àáci¬¥c O
and O
Kr O
¬®otzsch O
, O
2014 O
) O
to O
construct O
Wiki O
- O
Distant O
in O
the O
same O
way O
as O
Riedel O
et O
al O
. O
( O
2010 O
) O
. O
As O
demonstrated O
in O
Table O
1 O
, O
WikiDistant O
covers O
more O
relations O
and O
possesses O
more O
instances O
, O
with O
a O
more O
reasonable O
N O
/ O
A O
proportion O
. O
Comparison O
results O
of O
state O
- O
of O
- O
the O
- O
art O
models O
on O
these O
two O
datasets3are O
shown O
in O
Table O
2 O
, O
indicating O
that O
Wiki O
- O
Distant O
is O
more O
challenging O
and O
there O
is O
a O
long O
way O
to O
resolve O
distantly O
supervised O
RE O
. O
3.2 O
Performing O
More O
EfÔ¨Åcient O
Learning O
Real O
- O
world O
relation O
distributions O
are O
long O
- O
tail O
: O
Only O
the O
common O
relations O
obtain O
sufÔ¨Åcient O
training O
instances O
and O
most O
relations O
have O
very O
limited O
relational O
facts O
and O
corresponding O
sentences O
. O
We O
can O
see O
the O
long O
- O
tail O
relation O
distributions O
on O
two O
DS O
datasets O
from O
Figure O
4 O
, O
where O
many O
relations O
even O
have O
less O
than O
10 O
training O
instances O
. O
This O
phenomenon O
calls O
for O
models O
that O
can O
learn O
longtail O
relations O
more O
efÔ¨Åciently O
. O
Few O
- O
shot O
learning O
, O
which O
focuses O
on O
grasping O
tasks O
with O
only O
a O
few O
training O
examples O
, O
is O
a O
good O
Ô¨Åt O
for O
this O
need O
. O
To O
advance O
this O
Ô¨Åeld O
, O
Han O
et O
al O
. O
( O
2018d O
) O
Ô¨Årst O
built O
a O
large O
- O
scale O
few O
- O
shot O
relation O
extraction O
dataset O
( O
FewRel O
) O
. O
This O
benchmark O
takes O
the O
NwayK O
- O
shot O
setting O
, O
where O
models O
are O
given O
N O
random O
- O
sampled O
new O
relations O
, O
along O
with O
Ktraining O
examples O
for O
each O
relation O
. O
With O
limited O
information O
, O
RE O
models O
are O
required O
to O
classify O
query O
instances O
into O
given O
relations O
( O
Figure O
5 O
) O
. O
The O
general O
idea O
of O
few O
- O
shot O
models O
is O
to O
train O
good O
representations O
of O
instances O
or O
learn O
ways O
of O
fast O
adaptation O
from O
existing O
large O
- O
scale O
data O
, O
and O
then O
transfer O
to O
new O
tasks O
. O
There O
are O
mainly O
two O
ways O
for O
handling O
few O
- O
shot O
learning O
: O
( O
1 O
) O
Metric O
learning O
learns O
a O
semantic O
metric O
on O
existing O
3Due O
to O
the O
large O
size O
, O
we O
do O
not O
use O
any O
denoise O
mechanism O
for O
BERT O
, O
which O
still O
achieves O
the O
best O
results.7495 O
10 O
15 O
20 O
Numbers O
of O
Relations405060708090100Accuracy O
( O
% O
) O
Results O
with O
Increasing O
N O
BERT O
- O
PAIR O
Proto O
( O
CNN O
) O
5 O
- O
1 O
BERT O
PAIR5 O
- O
1 O
Proto O
( O
CNN)5 O
- O
5 O
BERT O
PAIR5 O
- O
5 O
Proto O
( O
CNN)405060708090100Results O
on O
Similar O
Relations O
Random O
SimilarFigure O
6 O
: O
Few O
- O
shot O
RE O
results O
with O
( O
A O
) O
increasing O
N O
and O
( O
B O
) O
similar O
relations O
. O
The O
left O
Ô¨Ågure O
shows O
the O
accuracy O
( O
% O
) O
of O
two O
models O
in O
N O
- O
way O
1 O
- O
shot O
RE O
. O
In O
the O
right O
Ô¨Ågure O
, O
‚Äú O
random O
‚Äù O
stands O
for O
the O
standard O
few O
- O
shot O
setting O
and O
‚Äú O
similar O
‚Äù O
stands O
for O
evaluating O
with O
selected O
similar O
relations O
. O
data O
and O
classiÔ¨Åes O
queries O
by O
comparing O
them O
with O
training O
examples O
( O
Koch O
et O
al O
. O
, O
2015 O
; O
Vinyals O
et O
al O
. O
, O
2016 O
; O
Snell O
et O
al O
. O
, O
2017 O
; O
Baldini O
Soares O
et O
al O
. O
, O
2019 O
) O
. O
While O
most O
metric O
learning O
models O
perform O
distance O
measurement O
on O
sentence O
- O
level O
representation O
, O
Ye O
and O
Ling O
( O
2019 O
) O
; O
Gao O
et O
al O
. O
( O
2019 O
) O
utilize O
token O
- O
level O
attention O
for O
Ô¨Åner O
- O
grained O
comparison O
. O
( O
2 O
) O
Meta O
- O
learning O
, O
also O
known O
as O
‚Äú O
learning O
to O
learn O
‚Äù O
, O
aims O
at O
grasping O
the O
way O
of O
parameter O
initialization O
and O
optimization O
through O
the O
experience O
gained O
on O
the O
meta O
- O
train O
data O
( O
Ravi O
and O
Larochelle O
, O
2017 O
; O
Finn O
et O
al O
. O
, O
2017 O
; O
Mishra O
et O
al O
. O
, O
2018 O
) O
. O
Researchers O
have O
made O
great O
progress O
in O
fewshot O
RE O
. O
However O
, O
there O
remain O
many O
challenges O
that O
are O
important O
for O
its O
applications O
and O
have O
not O
yet O
been O
discussed O
. O
Gao O
et O
al O
. O
( O
2019 O
) O
propose O
two O
problems O
worth O
further O
investigation O
: O
( O
1)Few O
- O
shot O
domain O
adaptation O
studies O
how O
few O
- O
shot O
models O
can O
transfer O
across O
domains O
. O
It O
is O
argued O
that O
in O
the O
real O
- O
world O
application O
, O
the O
test O
domains O
are O
typically O
lacking O
annotations O
and O
could O
differ O
vastly O
from O
the O
training O
domains O
. O
Thus O
, O
it O
is O
crucial O
to O
evaluate O
the O
transferabilities O
of O
fewshot O
models O
across O
domains O
. O
( O
2)Few O
- O
shot O
none O
- O
of O
- O
the O
- O
above O
detection O
is O
about O
detecting O
query O
instances O
that O
do O
not O
belong O
to O
any O
of O
the O
sampled O
Nrelations O
. O
In O
the O
N O
- O
way O
K O
- O
shot O
setting O
, O
it O
is O
assumed O
that O
all O
queries O
express O
one O
of O
the O
given O
relations O
. O
However O
, O
the O
real O
case O
is O
that O
most O
sentences O
are O
not O
related O
to O
the O
relations O
of O
our O
interest O
. O
Conventional O
few O
- O
shot O
models O
can O
not O
well O
handle O
this O
problem O
due O
to O
the O
difÔ¨Åculty O
to O
form O
a O
good O
representation O
for O
the O
none O
- O
of O
- O
the O
- O
above O
( O
NOTA O
) O
relation O
. O
Therefore O
, O
it O
is O
crucial O
to O
study O
how O
to O
identify O
NOTA O
instances O
. O
( O
3 O
) O
Besides O
the O
above O
challenges O
, O
it O
is O
also O
imporApple O
Inc. O
is O
a O
technology O
company O
founded O
by O
Steve O
Jobs O
, O
Steve O
Wozniak O
and O
Ronald O
Wayne O
. O
Its O
current O
CEO O
is O
Tim O
Cook O
. O
Apple O
is O
well O
known O
for O
its O
product O
iPhone O
. O
productSteve O
JobsTim O
CookiPhone O
co O
- O
founderCEOApple O
Inc. O
Ronald O
WayneSteve O
WozniakFigure O
7 O
: O
An O
example O
of O
document O
- O
level O
RE O
. O
Given O
a O
paragraph O
with O
several O
sentences O
and O
multiple O
entities O
, O
models O
are O
required O
to O
extract O
all O
possible O
relations O
between O
these O
entities O
expressed O
in O
the O
document O
. O
tant O
to O
see O
that O
, O
the O
existing O
evaluation O
protocol O
may O
over O
- O
estimate O
the O
progress O
we O
made O
on O
fewshot O
RE O
. O
Unlike O
conventional O
RE O
tasks O
, O
few O
- O
shot O
RE O
randomly O
samples O
Nrelations O
for O
each O
evaluation O
episode O
; O
in O
this O
setting O
, O
the O
number O
of O
relations O
is O
usually O
very O
small O
( O
5 O
or O
10 O
) O
and O
it O
is O
very O
likely O
to O
sample O
Ndistinct O
relations O
and O
thus O
reduce O
to O
a O
very O
easy O
classiÔ¨Åcation O
task O
. O
We O
carry O
out O
two O
simple O
experiments O
to O
show O
the O
problems O
( O
Figure O
6 O
): O
( O
A O
) O
We O
evaluate O
few O
- O
shot O
models O
with O
increasing O
Nand O
the O
performance O
drops O
drastically O
with O
larger O
relation O
numbers O
. O
Considering O
that O
the O
real O
- O
world O
case O
contains O
much O
more O
relations O
, O
it O
shows O
that O
existing O
models O
are O
still O
far O
from O
being O
applied O
. O
( O
B O
) O
Instead O
of O
randomly O
sampling O
Nrelations O
, O
we O
hand O
- O
pick O
5relations O
similar O
in O
semantics O
and O
evaluate O
few O
- O
shot O
RE O
models O
on O
them O
. O
It O
is O
no O
surprise O
to O
observe O
a O
sharp O
decrease O
in O
the O
results O
, O
which O
suggests O
that O
existing O
few O
- O
shot O
models O
may O
overÔ¨Åt O
simple O
textual O
cues O
between O
relations O
instead O
of O
really O
understanding O
the O
semantics O
of O
the O
context O
. O
More O
details O
about O
the O
experiments O
are O
in O
Appendix O
A. O
3.3 O
Handling O
More O
Complicated O
Context O
As O
shown O
in O
Figure O
7 O
, O
one O
document O
generally O
mentions O
many O
entities O
exhibiting O
complex O
crosssentence O
relations O
. O
Most O
existing O
methods O
focus O
on O
intra O
- O
sentence O
RE O
and O
thus O
are O
inadequate O
for O
collectively O
identifying O
these O
relational O
facts O
expressed O
in O
a O
long O
paragraph O
. O
In O
fact O
, O
most O
relational O
facts O
can O
only O
be O
extracted O
from O
complicated O
context O
like O
documents O
rather O
than O
single O
sentences O
( O
Yao O
et O
al O
. O
, O
2019 O
) O
, O
which O
should O
not O
be O
neglected O
. O
There O
are O
already O
some O
works O
proposed O
to O
extract O
relations O
across O
multiple O
sentences:750(1)Syntactic O
methods O
( O
Wick O
et O
al O
. O
, O
2006 O
; O
Gerber O
and O
Chai O
, O
2010 O
; O
Swampillai O
and O
Stevenson O
, O
2011 O
; O
Yoshikawa O
et O
al O
. O
, O
2011 O
; O
Quirk O
and O
Poon O
, O
2017 O
) O
rely O
on O
textual O
features O
extracted O
from O
various O
syntactic O
structures O
, O
such O
as O
coreference O
annotations O
, O
dependency O
parsing O
trees O
and O
discourse O
relations O
, O
to O
connect O
sentences O
in O
documents O
. O
( O
2 O
) O
Zeng O
et O
al O
. O
( O
2017 O
) O
; O
Christopoulou O
et O
al O
. O
( O
2018 O
) O
build O
inter O
- O
sentence O
entity O
graphs O
, O
which O
can O
utilize O
multi O
- O
hop O
paths O
between O
entities O
for O
inferring O
the O
correct O
relations O
. O
( O
3 O
) O
Peng O
et O
al O
. O
( O
2017 O
) O
; O
Song O
et O
al O
. O
( O
2018 O
) O
; O
Zhu O
et O
al O
. O
( O
2019b O
) O
employ O
graph O
- O
structured O
neural O
networks O
to O
model O
cross O
- O
sentence O
dependencies O
for O
relation O
extraction O
, O
which O
bring O
in O
memory O
and O
reasoning O
abilities O
. O
To O
advance O
this O
Ô¨Åeld O
, O
some O
document O
- O
level O
RE O
datasets O
have O
been O
proposed O
. O
Quirk O
and O
Poon O
( O
2017 O
) O
; O
Peng O
et O
al O
. O
( O
2017 O
) O
build O
datasets O
by O
DS O
. O
Li O
et O
al O
. O
( O
2016 O
) O
; O
Peng O
et O
al O
. O
( O
2017 O
) O
propose O
datasets O
for O
speciÔ¨Åc O
domains O
. O
Yao O
et O
al O
. O
( O
2019 O
) O
construct O
a O
general O
document O
- O
level O
RE O
dataset O
annotated O
by O
crowdsourcing O
workers O
, O
suitable O
for O
evaluating O
general O
- O
purpose O
document O
- O
level O
RE O
systems O
. O
Although O
there O
are O
some O
efforts O
investing O
into O
extracting O
relations O
from O
complicated O
context O
( O
e.g. O
, O
documents O
) O
, O
the O
current O
RE O
models O
for O
this O
challenge O
are O
still O
crude O
and O
straightforward O
. O
Followings O
are O
some O
directions O
worth O
further O
investigation O
: O
( O
1 O
) O
Extracting O
relations O
from O
complicated O
context O
is O
a O
challenging O
task O
requiring O
reading O
, O
memorizing O
and O
reasoning O
for O
discovering O
relational O
facts O
across O
multiple O
sentences O
. O
Most O
of O
current O
RE O
models O
are O
still O
very O
weak O
in O
these O
abilities O
. O
( O
2 O
) O
Besides O
documents O
, O
more O
forms O
of O
context O
is O
also O
worth O
exploring O
, O
such O
as O
extracting O
relational O
facts O
across O
documents O
, O
or O
understanding O
relational O
information O
based O
on O
heterogeneous O
data O
. O
( O
3 O
) O
Inspired O
by O
Narasimhan O
et O
al O
. O
( O
2016 O
) O
, O
which O
utilizes O
search O
engines O
for O
acquiring O
external O
information O
, O
automatically O
searching O
and O
analysing O
context O
for O
RE O
may O
help O
RE O
models O
identify O
relational O
facts O
with O
more O
coverage O
and O
become O
practical O
for O
daily O
scenarios O
. O
3.4 O
Orienting O
More O
Open O
Domains O
Most O
RE O
systems O
work O
within O
pre O
- O
speciÔ¨Åed O
relation O
sets O
designed O
by O
human O
experts O
. O
However O
, O
our O
world O
undergoes O
open O
- O
ended O
growth O
of O
relations O
and O
it O
is O
not O
possible O
to O
handle O
all O
these O
emerging O
JeÔ¨Ä O
Bezos O
, O
an O
American O
entrepreneur O
, O
graduated O
from O
Princeton O
in O
1986.graduated O
fromJeÔ¨Ä O
BezosPrincetonFigure O
8 O
: O
An O
example O
of O
open O
information O
extraction O
, O
which O
extracts O
relation O
arguments O
( O
entities O
) O
and O
phrases O
without O
relying O
on O
any O
pre O
- O
deÔ¨Åned O
relation O
types O
. O
Relation O
BRelation O
ABill O
Gates O
founded O
Microsoft O
. O
Larry O
and O
Sergey O
founded O
Google O
. O
Steve O
Jobs O
is O
one O
of O
the O
co O
- O
founder O
of O
Apple O
. O
Tim O
Cook O
is O
Apple O
‚Äôs O
current O
CEO.Satya O
Nadella O
became O
the O
CEO O
of O
Microsoft O
in O
2014 O
. O
Figure O
9 O
: O
An O
example O
of O
clustering O
- O
based O
relation O
discovery O
, O
which O
identifying O
potential O
relation O
types O
by O
clustering O
unlabeled O
relational O
instances O
. O
relation O
types O
only O
by O
humans O
. O
Thus O
, O
we O
need O
RE O
systems O
that O
do O
not O
rely O
on O
pre O
- O
deÔ¨Åned O
relation O
schemas O
and O
can O
work O
in O
open O
scenarios O
. O
There O
are O
already O
some O
explorations O
in O
handling O
open O
relations O
: O
( O
1 O
) O
Open O
information O
extraction O
( O
Open O
IE O
) O
, O
as O
shown O
in O
Figure O
8 O
, O
extracts O
relation O
phrases O
and O
arguments O
( O
entities O
) O
from O
text O
( O
Banko O
et O
al O
. O
, O
2007 O
; O
Fader O
et O
al O
. O
, O
2011 O
; O
Mausam O
et O
al O
. O
, O
2012 O
; O
Del O
Corro O
and O
Gemulla O
, O
2013 O
; O
Angeli O
et O
al O
. O
, O
2015 O
; O
Stanovsky O
and O
Dagan O
, O
2016 O
; O
Mausam O
, O
2016 O
; O
Cui O
et O
al O
. O
, O
2018 O
) O
. O
Open O
IE O
does O
not O
rely O
on O
speciÔ¨Åc O
relation O
types O
and O
thus O
can O
handle O
all O
kinds O
of O
relational O
facts O
. O
( O
2 O
) O
Relation O
discovery O
, O
as O
shown O
in O
Figure O
9 O
, O
aims O
at O
discovering O
unseen O
relation O
types O
from O
unsupervised O
data O
. O
Yao O
et O
al O
. O
( O
2011 O
) O
; O
Marcheggiani O
and O
Titov O
( O
2016 O
) O
propose O
to O
use O
generative O
models O
and O
treat O
these O
relations O
as O
latent O
variables O
, O
while O
Shinyama O
and O
Sekine O
( O
2006 O
) O
; O
Elsahar O
et O
al O
. O
( O
2017 O
) O
; O
Wu O
et O
al O
. O
( O
2019 O
) O
cast O
relation O
discovery O
as O
a O
clustering O
task O
. O
Though O
relation O
extraction O
in O
open O
domains O
has O
been O
widely O
studied O
, O
there O
are O
still O
lots O
of O
unsolved O
research O
questions O
remained O
to O
be O
answered O
: O
( O
1)Canonicalizing O
relation O
phrases O
and O
arguments O
in O
Open O
IE O
is O
crucial O
for O
downstream O
tasks O
( O
Niklaus O
et O
al O
. O
, O
2018 O
) O
. O
If O
not O
canonicalized O
, O
the O
extracted O
relational O
facts O
could O
be O
redundant O
and O
ambiguous O
. O
For O
example O
, O
Open O
IE O
may O
extract O
two O
triples O
( O
Barack O
Obama O
, O
was O
born O
in O
, O
Honolulu O
) O
and O
( O
Obama O
, O
place O
of O
birth O
, O
Honolulu O
) O
indicating O
an O
identical O
fact O
. O
Thus O
, O
normalizing O
extracted O
results O
will O
largely O
beneÔ¨Åt O
the O
applications O
of O
Open O
IE O
. O
There O
are O
already O
some O
preliminary O
works O
in O
this O
area O
( O
Gal O
¬¥ O
arraga O
et O
al O
. O
, O
2014;751Vashishth O
et O
al O
. O
, O
2018 O
) O
and O
more O
efforts O
are O
needed O
. O
( O
2 O
) O
The O
not O
applicable O
( O
N O
/ O
A O
) O
relation O
has O
been O
hardly O
addressed O
in O
relation O
discovery O
. O
In O
previous O
work O
, O
it O
is O
usually O
assumed O
that O
the O
sentence O
always O
expresses O
a O
relation O
between O
the O
two O
entities O
( O
Marcheggiani O
and O
Titov O
, O
2016 O
) O
. O
However O
, O
in O
the O
real O
- O
world O
scenario O
, O
a O
large O
proportion O
of O
entity O
pairs O
appearing O
in O
a O
sentence O
do O
not O
have O
a O
relation O
, O
and O
ignoring O
them O
or O
using O
simple O
heuristics O
to O
get O
rid O
of O
them O
may O
lead O
to O
poor O
results O
. O
Thus O
, O
it O
would O
be O
of O
interest O
to O
study O
how O
to O
handle O
these O
N O
/ O
A O
instances O
in O
relation O
discovery O
. O
4 O
Other O
Challenges O
In O
this O
section O
, O
we O
analyze O
two O
key O
challenges O
faced O
by O
RE O
models O
, O
address O
them O
with O
experiments O
and O
show O
their O
signiÔ¨Åcance O
in O
the O
research O
and O
development O
of O
RE O
systems4 O
. O
4.1 O
Learning O
from O
Text O
or O
Names O
In O
the O
process O
of O
RE O
, O
both O
entity O
names O
and O
their O
context O
provide O
useful O
information O
for O
classiÔ¨Åcation O
. O
Entity O
names O
provide O
typing O
information O
( O
e.g. O
, O
we O
can O
easily O
tell O
JFK O
International O
Airport O
is O
an O
airport O
) O
and O
help O
to O
narrow O
down O
the O
range O
of O
possible O
relations O
; O
In O
the O
training O
process O
, O
entity O
embeddings O
may O
also O
be O
formed O
to O
help O
relation O
classiÔ¨Åcation O
( O
like O
in O
the O
link O
prediction O
task O
of O
KG O
) O
. O
On O
the O
other O
hand O
, O
relations O
can O
usually O
be O
extracted O
from O
the O
semantics O
of O
textaround O
entity O
pairs O
. O
In O
some O
cases O
, O
relations O
can O
only O
be O
inferred O
implicitly O
by O
reasoning O
over O
the O
context O
. O
Since O
there O
are O
two O
sources O
of O
information O
, O
it O
is O
interesting O
to O
study O
how O
much O
each O
of O
them O
contributes O
to O
the O
RE O
performance O
. O
Therefore O
, O
we O
design O
three O
different O
settings O
for O
the O
experiments O
: O
( O
1)normal O
setting O
, O
where O
both O
names O
and O
text O
are O
taken O
as O
inputs O
; O
( O
2 O
) O
masked O
- O
entity O
( O
ME O
) O
setting O
, O
where O
entity O
names O
are O
replaced O
with O
a O
special O
token O
; O
( O
3 O
) O
only O
- O
entity O
( O
OE O
) O
setting O
, O
where O
only O
names O
of O
the O
two O
entities O
are O
provided O
. O
Results O
from O
Table O
3 O
show O
that O
compared O
to O
the O
normal O
setting O
, O
models O
suffer O
a O
huge O
performance O
drop O
in O
both O
the O
ME O
and O
OE O
settings O
. O
Besides O
, O
it O
is O
surprising O
to O
see O
that O
in O
some O
cases O
, O
only O
using O
entity O
names O
outperforms O
only O
using O
text O
with O
entities O
masked O
. O
It O
suggests O
that O
( O
1 O
) O
both O
entity O
names O
and O
text O
provide O
crucial O
information O
for O
RE O
, O
and O
4For O
more O
details O
about O
these O
experiments O
, O
please O
refer O
to O
our O
open O
- O
source O
toolkit O
https://github.com/ O
thunlp O
/ O
OpenNRE O
.Benchmark O
Normal O
ME O
OE O
Wiki80 O
( O
Acc O
) O
0.861 O
0.734 O
0.763 O
TACRED O
( O
F-1 O
) O
0.666 O
0.554 O
0.412 O
NYT-10 O
( O
AUC O
) O
0.349 O
0.216 O
0.185 O
Wiki O
- O
Distant O
( O
AUC O
) O
0.222 O
0.145 O
0.173 O
Table O
3 O
: O
Results O
of O
state O
- O
of O
- O
the O
- O
arts O
models O
on O
the O
normal O
setting O
, O
masked O
- O
entity O
( O
ME O
) O
setting O
and O
only O
- O
entity O
( O
OE O
) O
setting O
. O
We O
report O
accuracies O
of O
BERT O
on O
Wiki80 O
, O
F-1 O
scores O
of O
BERT O
on O
TACRED O
and O
AUC O
of O
PCNNATT O
on O
NYT-10 O
and O
Wiki O
- O
Distant O
. O
All O
models O
are O
from O
the O
OpenNRE O
package O
( O
Han O
et O
al O
. O
, O
2019 O
) O
. O
( O
2 O
) O
for O
some O
existing O
state O
- O
of O
- O
the O
- O
art O
models O
and O
benchmarks O
, O
entity O
names O
contribute O
even O
more O
. O
The O
observation O
is O
contrary O
to O
human O
intuition O
: O
we O
classify O
the O
relations O
between O
given O
entities O
mainly O
from O
the O
text O
description O
, O
yet O
models O
learn O
more O
from O
their O
names O
. O
To O
make O
real O
progress O
in O
understanding O
how O
language O
expresses O
relational O
facts O
, O
this O
problem O
should O
be O
further O
investigated O
and O
more O
efforts O
are O
needed O
. O
4.2 O
RE O
Datasets O
towards O
Special O
Interests O
There O
are O
already O
many O
datasets O
that O
beneÔ¨Åt O
RE O
research O
: O
For O
supervised O
RE O
, O
there O
are O
MUC O
( O
Grishman O
and O
Sundheim O
, O
1996 O
) O
, O
ACE-2005 O
( O
Ntroduction O
, O
2005 O
) O
, O
SemEval-2010 O
Task O
8 O
( O
Hendrickx O
et O
al O
. O
, O
2009 O
) O
, O
KBP37 O
( O
Zhang O
and O
Wang O
, O
2015 O
) O
and O
TACRED O
( O
Zhang O
et O
al O
. O
, O
2017 O
) O
; O
and O
we O
have O
NYT10 O
( O
Riedel O
et O
al O
. O
, O
2010 O
) O
, O
FewRel O
( O
Han O
et O
al O
. O
, O
2018d O
) O
and O
DocRED O
( O
Yao O
et O
al O
. O
, O
2019 O
) O
for O
distant O
supervision O
, O
few O
- O
shot O
and O
document O
- O
level O
RE O
respectively O
. O
However O
, O
there O
are O
barely O
datasets O
targeting O
special O
problems O
of O
interest O
. O
For O
example O
, O
RE O
across O
sentences O
( O
e.g. O
, O
two O
entities O
are O
mentioned O
in O
two O
different O
sentences O
) O
is O
an O
important O
problem O
, O
yet O
there O
is O
no O
speciÔ¨Åc O
datasets O
that O
can O
help O
researchers O
study O
it O
. O
Though O
existing O
document O
- O
level O
RE O
datasets O
contain O
instances O
of O
this O
case O
, O
it O
is O
hard O
to O
analyze O
the O
exact O
performance O
gain O
towards O
this O
speciÔ¨Åc O
aspect O
. O
Usually O
, O
researchers O
( O
1 O
) O
use O
handcrafted O
sub O
- O
sets O
of O
general O
datasets O
or O
( O
2 O
) O
carry O
out O
case O
studies O
to O
show O
the O
effectiveness O
of O
their O
models O
in O
speciÔ¨Åc O
problems O
, O
which O
is O
lacking O
of O
convincing O
and O
quantitative O
analysis O
. O
Therefore O
, O
to O
further O
study O
these O
problems O
of O
great O
importance O
in O
the O
development O
of O
RE O
, O
it O
is O
necessary O
for O
the O
community O
to O
construct O
well O
- O
recognized O
, O
well O
- O
designed O
and O
Ô¨Åne O
- O
grained O
datasets O
towards O
special O
interests.7525 O
Conclusion O
In O
this O
paper O
, O
we O
give O
a O
comprehensive O
and O
detailed O
review O
on O
the O
development O
of O
relation O
extraction O
models O
, O
generalize O
four O
promising O
directions O
leading O
to O
more O
powerful O
RE O
systems O
( O
utilizing O
more O
data O
, O
performing O
more O
efÔ¨Åcient O
learning O
, O
handling O
more O
complicated O
context O
and O
orienting O
more O
open O
domains O
) O
, O
and O
further O
investigate O
two O
key O
challenges O
faced O
by O
existing O
RE O
models O
. O
We O
thoroughly O
survey O
the O
previous O
RE O
literature O
as O
well O
as O
supporting O
our O
points O
with O
statistics O
and O
experiments O
. O
Through O
this O
paper O
, O
we O
hope O
to O
demonstrate O
the O
progress O
and O
problems O
in O
existing O
RE O
research O
and O
encourage O
more O
efforts O
in O
this O
area O
. O
Acknowledgments O
This O
work O
is O
supported O
by O
the O
Natural O
Science O
Foundation O
of O
China O
( O
NSFC O
) O
and O
the O
German O
Research O
Foundation O
( O
DFG O
) O
in O
Project O
Crossmodal O
Learning O
, O
NSFC O
61621136008 O
/ O
DFG O
TRR-169 O
, O
and O
Beijing O
Academy O
of O
ArtiÔ¨Åcial O
Intelligence O
( O
BAAI O
) O
. O
This O
work O
is O
also O
supported O
by O
the O
Pattern O
Recognition O
Center O
, O
WeChat O
AI O
, O
Tencent O
Inc. O
Gao O
is O
supported O
by O
2019 O
Tencent O
Rhino O
- O
Bird O
Elite O
Training O
Program O
. O
Gao O
is O
also O
supported O
by O
Tsinghua O
University O
Initiative O
ScientiÔ¨Åc O
Research O
Program O
. O
Abstract O
It O
has O
been O
shown O
that O
word O
embeddings O
can O
exhibit O
gender O
bias O
, O
and O
various O
methods O
have O
been O
proposed O
to O
quantify O
this O
. O
However O
, O
the O
extent O
to O
which O
the O
methods O
are O
capturing O
social O
stereotypes O
inherited O
from O
the O
data O
has O
been O
debated O
. O
Bias O
is O
a O
complex O
concept O
and O
there O
exist O
multiple O
ways O
to O
deÔ¨Åne O
it O
. O
Previous O
work O
has O
leveraged O
gender O
word O
pairs O
to O
measure O
bias O
and O
extract O
biased O
analogies O
. O
We O
show O
that O
the O
reliance O
on O
these O
gendered O
pairs O
has O
strong O
limitations O
: O
bias O
measures O
based O
off O
of O
them O
are O
not O
robust O
and O
can O
not O
identify O
common O
types O
of O
real O
- O
world O
bias O
, O
whilst O
analogies O
utilising O
them O
are O
unsuitable O
indicators O
of O
bias O
. O
In O
particular O
, O
the O
well O
- O
known O
analogy O
‚Äú O
man O
is O
to O
computer O
- O
programmer O
as O
woman O
is O
to O
homemaker O
‚Äù O
is O
due O
to O
word O
similarity O
rather O
than O
societal O
bias O
. O
This O
has O
important O
implications O
for O
work O
on O
measuring O
bias O
in O
embeddings O
and O
related O
work O
debiasing O
embeddings O
. O
1 O
Introduction O
Word O
embeddings O
, O
distributed O
representations O
of O
words O
in O
a O
low O
- O
dimensional O
vector O
space O
, O
are O
used O
in O
many O
downstream O
NLP O
tasks O
( O
Mikolov O
et O
al O
. O
, O
2013a O
, O
b O
; O
Pennington O
et O
al O
. O
, O
2014 O
; O
Peters O
et O
al O
. O
, O
2018 O
; O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O
Recent O
work O
has O
shown O
they O
can O
contain O
harmful O
bias O
and O
proposed O
techniques O
to O
quantify O
it O
( O
Bolukbasi O
et O
al O
. O
, O
2016 O
; O
Caliskan O
et O
al O
. O
, O
2017 O
; O
Ethayarajh O
et O
al O
. O
, O
2019 O
; O
Gonen O
and O
Goldberg O
, O
2019 O
) O
. O
These O
techniques O
leverage O
cosine O
similarity O
to O
a O
base O
pair O
of O
gender O
words O
, O
such O
as(man O
, O
woman O
) O
. O
They O
include O
bias O
measures O
, O
which O
return O
a O
magnitude O
of O
bias O
for O
a O
given O
word O
, O
and O
analogies O
. O
A O
well O
- O
known O
example O
of O
the O
latter O
is O
‚Äú O
Man O
is O
to O
computer O
programmer O
as O
woman O
is O
to O
homemaker O
‚Äù O
( O
Bolukbasi O
et O
al O
. O
, O
2016 O
) O
, O
which O
has O
been O
widely O
interpreted O
as O
demonstrating O
bias O
. O
There O
have O
also O
been O
related O
attempts O
to O
debias O
* O
denotes O
equal O
contribution.embeddings O
( O
Bolukbasi O
et O
al O
. O
, O
2016 O
; O
Zhao O
et O
al O
. O
, O
2018 O
; O
Dev O
and O
Phillips O
, O
2019 O
; O
Kaneko O
and O
Bollegala O
, O
2019 O
; O
Manzini O
et O
al O
. O
, O
2019 O
) O
. O
However O
, O
to O
remove O
bias O
effectively O
, O
an O
accurate O
method O
of O
identifying O
it O
is O
Ô¨Årst O
required O
. O
This O
is O
a O
complex O
task O
, O
not O
least O
because O
the O
concept O
of O
‚Äú O
bias O
‚Äù O
has O
multiple O
interpretations O
: O
Mehrabi O
et O
al O
. O
( O
2019 O
) O
identify O
23 O
types O
of O
bias O
that O
can O
occur O
in O
machine O
learning O
applications O
, O
including O
historic O
( O
pre O
- O
existing O
in O
society O
) O
, O
algorithmic O
( O
introduced O
by O
the O
algorithm O
) O
and O
evaluation O
( O
occurs O
during O
model O
evaluation O
) O
. O
In O
the O
case O
of O
word O
embeddings O
, O
it O
remains O
an O
open O
question O
if O
bias O
identifying O
techniques O
reÔ¨Çect O
social O
stereotypes O
in O
the O
training O
data O
, O
an O
artifact O
of O
the O
embedding O
process O
or O
noise O
. O
While O
it O
is O
often O
assumed O
the O
Ô¨Årst O
is O
true O
, O
and O
thus O
that O
bias O
in O
embeddings O
can O
perpetuate O
harmful O
stereotypes O
( O
Bolukbasi O
et O
al O
. O
, O
2016 O
; O
Caliskan O
et O
al O
. O
, O
2017 O
) O
, O
this O
has O
not O
been O
conclusively O
established O
( O
Gonen O
and O
Goldberg O
, O
2019 O
; O
Nissim O
et O
al O
. O
, O
2019 O
; O
Ethayarajh O
et O
al O
. O
, O
2019 O
) O
. O
To O
further O
complicate O
matters O
, O
multiple O
methods O
of O
quantifying O
bias O
have O
been O
proposed O
, O
often O
in O
response O
to O
one O
another O
‚Äôs O
limitations O
( O
see O
Section O
2.1 O
) O
. O
It O
is O
unclear O
how O
they O
compare O
and O
which O
are O
more O
reliable O
. O
This O
work O
shows O
that O
the O
use O
of O
gender O
base O
pairs O
in O
bias O
identifying O
techniques O
has O
serious O
limitations O
. O
We O
propose O
three O
criteria O
to O
evaluate O
the O
performance O
of O
gender O
bias O
measures O
using O
base O
pairs O
and O
systematically O
compare O
four O
popular O
measures O
, O
showing O
both O
that O
they O
not O
robust O
, O
and O
that O
they O
do O
not O
accurately O
reÔ¨Çect O
common O
types O
of O
societal O
bias O
. O
In O
addition O
, O
we O
demonstrate O
that O
the O
types O
of O
analogies O
proposed O
in O
Bolukbasi O
et O
al O
. O
( O
2016 O
) O
are O
unsuitable O
indicators O
of O
bias O
; O
what O
is O
ascribed O
to O
social O
bias O
in O
analogies O
is O
actually O
an O
artifact O
of O
high O
cosine O
similarity O
in O
the O
base O
pair O
, O
which O
is O
arguably O
positive O
. O
Our O
argument O
is O
not O
that O
embeddings O
are O
free O
of O
bias O
; O
rather O
it O
is O
that O
bias O
is O
a O
complex O
problem O
and O
current O
bias O
measures O
do759not O
completely O
solve O
it O
. O
This O
has O
important O
implications O
for O
future O
work O
on O
bias O
in O
embeddings O
and O
debiasing O
techniques O
. O
The O
primary O
contributions O
of O
this O
work O
are O
to O
: O
( O
1 O
) O
demonstrate O
the O
output O
of O
gender O
bias O
measures O
is O
heavily O
dependant O
on O
a O
chosen O
gendered O
base O
pair O
( O
e.g. O
( O
she O
, O
he O
) O
) O
and O
on O
the O
form O
of O
a O
word O
considered O
( O
e.g. O
singular O
versus O
plural O
) O
; O
( O
2 O
) O
show O
the O
measures O
can O
not O
accurately O
predict O
either O
the O
socially O
stereotyped O
gender O
of O
human O
traits O
or O
the O
correct O
gender O
of O
words O
when O
this O
is O
encoded O
linguistically O
( O
e.g. O
lioness O
) O
; O
( O
3 O
) O
show O
that O
analogies O
generated O
by O
gender O
base O
pairs O
( O
e.g. O
( O
she O
, O
he O
) O
) O
are O
Ô¨Çawed O
indicators O
of O
bias O
and O
the O
widely O
- O
known O
example O
‚Äú O
Man O
is O
to O
computer O
programmer O
as O
woman O
is O
to O
homemaker O
‚Äù O
is O
not O
due O
to O
gender O
bias O
and O
( O
4 O
) O
highlight O
the O
complexities O
of O
identifying O
bias O
in O
word O
embeddings O
, O
and O
the O
limitations O
of O
these O
measures O
. O
2 O
Related O
Work O
2.1 O
Bias O
Measures O
A O
variety O
of O
gender O
bias O
measures O
for O
word O
embeddings O
have O
been O
proposed O
in O
the O
literature O
. O
Each O
takes O
as O
input O
a O
word O
wand O
a O
gendered O
base O
pair O
( O
such O
as O
( O
she O
, O
he O
) O
) O
, O
and O
returns O
a O
numerical O
output O
. O
This O
output O
indicates O
both O
the O
magnitude O
of O
w O
‚Äôs O
gender O
bias O
with O
respect O
to O
the O
base O
pair O
used O
, O
and O
the O
direction O
of O
w O
‚Äôs O
bias O
( O
male O
or O
female O
) O
, O
which O
is O
determined O
by O
the O
sign O
of O
the O
score O
. O
Direct O
Bias O
( O
DB O
) O
( O
Bolukbasi O
et O
al O
. O
, O
2016 O
) O
deÔ¨Ånes O
bias O
as O
a O
projection O
onto O
a O
gender O
subspace O
, O
which O
is O
constructed O
from O
a O
set O
of O
gender O
base O
pairs O
such O
as O
( O
she O
, O
he O
) O
. O
The O
DB O
of O
a O
word O
wis O
computed O
as O
wB=/summationtextk O
j=1(‚àí O
‚Üíw¬∑bj)bj O
, O
where‚àí O
‚Üíwis O
the O
embedding O
vector O
of O
w O
, O
the O
subspace O
Bis O
deÔ¨Åned O
by O
k O
orthogonal O
unit O
vectors O
b1 O
, O
... O
, O
b O
kand O
vectors O
are O
normalised O
. O
In O
addition O
, O
the O
authors O
propsed O
a O
method O
of O
debiasing O
embeddings O
based O
off O
of O
DB O
. O
There O
is O
ambiguity O
in O
Bolukbasi O
et O
al O
. O
( O
2016 O
) O
about O
how O
many O
base O
pairs O
should O
be O
used O
with O
DB O
; O
while O
experiments O
to O
identify O
bias O
use O
only O
one O
( O
namely O
( O
she O
, O
he O
) O
) O
, O
a O
set O
of O
ten O
is O
used O
for O
debiasing.1It O
is O
unclear O
why O
the O
particular O
ten O
pairs O
used O
were O
chosen O
, O
and O
the O
extent O
to O
which O
their O
choice O
matters O
. O
We O
follow O
recent O
work O
( O
Gonen O
and O
Goldberg O
, O
2019 O
; O
Ethayarajh O
et O
al O
. O
, O
2019 O
) O
that O
evaluates O
DB O
and O
focus O
on O
the O
case O
of O
a O
single O
base O
1The O
set O
of O
gender O
- O
deÔ¨Åning O
pairs O
used O
is O
{ O
( O
she O
, O
he),(her O
, O
his O
) O
, O
( O
woman O
, O
man O
) O
, O
( O
mary O
, O
john O
) O
, O
( O
herself O
, O
himself O
) O
, O
( O
daughter O
, O
son O
) O
, O
( O
mother O
, O
father O
) O
, O
( O
gal O
, O
guy O
) O
, O
( O
girl O
, O
boy O
) O
, O
( O
female O
, O
male O
) O
} O
.pair O
, O
i.e. O
k= O
1 O
. O
The O
DB O
of O
wwith O
respect O
to O
the O
gender O
base O
pair O
( O
x O
, O
y)is O
then‚àí O
‚Üíw¬∑(‚àí O
‚Üíx‚àí‚àí O
‚Üíy O
) O
. O
Caliskan O
et O
al O
. O
( O
2017 O
) O
created O
an O
association O
test O
for O
word O
embeddings O
called O
WEAT O
to O
identify O
human O
- O
like O
biases O
. O
The O
Word O
Association O
( O
WA O
) O
, O
the O
key O
component O
of O
WEAT O
, O
measures O
the O
association O
of O
wwith O
two O
sets O
of O
attribute O
words O
, O
Xand O
Y. O
More O
formally O
, O
WA O
is O
computed O
as O
: O
mean O
x‚ààXcos O
( O
‚àí O
‚Üíw O
, O
‚àí O
‚Üíx)‚àímean O
y‚ààYcos O
( O
‚àí O
‚Üíw O
, O
‚àí O
‚Üíy O
) O
To O
allow O
for O
a O
fair O
comparison O
with O
other O
methods O
being O
evaluated O
, O
we O
focus O
on O
the O
case O
where O
the O
attribute O
sets O
contain O
a O
single O
word O
, O
i.e. O
, O
X={x O
} O
andY={y O
} O
. O
Then O
WA O
and O
DB O
are O
equivalent O
as O
: O
cos O
( O
‚àí O
‚Üíw O
, O
‚àí O
‚Üíx)‚àícos O
( O
‚àí O
‚Üíw O
, O
‚àí O
‚Üíy O
) O
= O
‚àí O
‚Üíw O
||w||¬∑/parenleftbigg‚àí O
‚Üíx O
||x||‚àí‚àí O
‚Üíy O
||y||/parenrightbigg O
Since O
DB O
and O
WA O
assign O
a O
word O
the O
same O
score O
, O
we O
will O
use O
DB O
/ O
WA O
to O
refer O
to O
both O
measures O
. O
Gonen O
and O
Goldberg O
( O
2019 O
) O
argued O
that O
bias O
can O
not O
be O
directly O
observed O
, O
as O
assumed O
in O
methods O
such O
as O
DB O
, O
and O
that O
the O
debiasing O
method O
of O
Bolukbasi O
et O
al O
. O
( O
2016 O
) O
is O
ineffective O
. O
They O
proposed O
the O
Neighbourhood O
Bias O
Metric O
( O
NBM O
) O
, O
which O
measures O
the O
bias O
of O
a O
word O
was O
the O
percentage O
of O
socially O
female O
- O
biased O
words O
and O
malebiased O
words O
among O
its O
Knearest O
neighbours O
in O
a O
set O
of O
predeÔ¨Åned O
gender O
- O
neutral O
words O
. O
Setting O
K= O
100 O
, O
the O
NBM O
bias O
of O
a O
target O
word O
wis O
measured O
as O
: O
|female O
( O
w)|‚àí|male O
( O
w)| O
100 O
, O
where O
female O
( O
w)andmale O
( O
w)are O
sets O
of O
socially O
biased O
and O
male O
words O
in O
the O
neighborhood O
ofw O
. O
The O
bias O
direction O
of O
words O
in O
w O
‚Äôs O
neighborhood O
is O
computed O
using O
the O
DB O
metric O
with O
a O
single O
base O
pair O
. O
Gonen O
and O
Goldberg O
( O
2019 O
) O
use O
DB O
with O
base O
pair O
( O
she O
, O
he O
) O
; O
our O
work O
considers O
a O
more O
general O
form O
with O
base O
pair O
( O
x O
, O
y O
) O
. O
Ethayarajh O
et O
al O
. O
( O
2019 O
) O
draw O
attention O
to O
the O
lack O
of O
theoretical O
guarantees O
surrounding O
previous O
work O
on O
bias O
and O
debiasing O
. O
They O
argue O
WEAT O
overestimates O
bias O
and O
is O
not O
robust O
to O
the O
choice O
of O
deÔ¨Åning O
sets O
. O
In O
addition O
, O
and O
in O
contrast O
Gonen O
and O
Goldberg O
( O
2019 O
) O
, O
they O
argue O
that O
DB O
and O
the O
debiasing O
method O
based O
off O
it O
are O
effective O
, O
but O
state O
vectors O
used O
with O
DB O
should O
not O
be O
normalised O
. O
They O
propose O
Relational O
Inner O
Product O
Association O
( O
RIPA O
) O
and O
state O
that O
RIPA O
is O
most O
interpretable O
with O
a O
single O
base O
pair O
, O
a O
key O
advantage760of O
it O
being O
that O
it O
( O
unike O
WEAT O
) O
does O
not O
depend O
on O
the O
base O
pair O
used O
. O
With O
a O
single O
base O
pair O
, O
the O
RIPA O
bias O
of O
wwith O
the O
base O
pair O
( O
x O
, O
y)is O
: O
‚àí O
‚Üíw¬∑/parenleftbigg‚àí O
‚Üíx‚àí‚àí O
‚Üíy O
||‚àí O
‚Üíx‚àí‚àí O
‚Üíy||/parenrightbigg O
. O
2.2 O
Analogies O
An O
alternative O
approach O
to O
identifying O
gender O
bias O
in O
embeddings O
is O
via O
word O
analogies O
. O
Unlike O
the O
gender O
bias O
measures O
dicussed O
in O
Section O
2.1 O
, O
analogies O
do O
not O
measure O
the O
bias O
of O
a O
particular O
word O
. O
Instead O
, O
they O
identify O
pairs O
of O
words O
which O
are O
assumed O
to O
have O
a O
gendered O
relationship O
. O
Analogies O
in O
word O
embeddings O
are O
important O
because O
it O
has O
been O
observed O
that O
embedding O
vectors O
seem O
to O
possess O
unexpected O
linear O
properties O
: O
vectors O
associated O
with O
word O
pairs O
sharing O
the O
same O
analogical O
relationship O
can O
be O
identiÔ¨Åed O
using O
vector O
arithmetic O
( O
Mikolov O
et O
al O
. O
, O
2013a O
; O
Levy O
and O
Goldberg O
, O
2014 O
; O
Ethayarajh O
et O
al O
. O
, O
2018 O
) O
. O
A O
notable O
example O
of O
this O
phenomena O
is‚àí‚àí‚Üíking O
-‚àí‚àí‚Üíman O
+ O
‚àí‚àí‚àí‚àí‚àí‚Üíwoman‚âà‚àí‚àí‚àí‚Üíqueen O
( O
Mikolov O
et O
al O
. O
, O
2013c O
) O
. O
This O
relationship O
is O
frequently O
attributed O
to O
a O
gender O
difference O
vector O
between‚àí‚àí‚Üíman O
and‚àí‚àí‚àí‚àí‚àí‚Üíwoman O
, O
and O
between‚àí‚àí‚Üíking O
and‚àí‚àí‚àí‚Üíqueen O
( O
Mikolov O
et O
al O
. O
, O
2013c O
; O
Ethayarajh O
et O
al O
. O
, O
2018 O
) O
. O
Analogies O
are O
considered O
a O
benchmark O
method O
of O
measuring O
the O
quality O
of O
embeddings O
, O
though O
their O
suitability O
has O
been O
debated O
( O
Linzen O
, O
2016 O
; O
Drozd O
et O
al O
. O
, O
2016 O
; O
Gladkova O
et O
al O
. O
, O
2016 O
) O
. O
The O
standard O
approach O
to O
solving O
‚Äò O
a O
is O
tobascis O
to O
? O
, O
‚Äù O
is O
to O
return O
: O
‚àí O
‚Üíd‚àó=argmax O
w‚ààV O
/ O
primeCosSim O
( O
‚àí O
‚Üíw O
, O
‚àí O
‚Üíb‚àí‚àí O
‚Üía+‚àí O
‚Üíc O
) O
, O
where O
V O
/ O
primeis O
the O
embedding O
vocabulary O
excluding O
{ O
a O
, O
b O
, O
c}(Levy O
and O
Goldberg O
, O
2014 O
) O
. O
Bolukbasi O
et O
al O
. O
( O
2016 O
) O
proposed O
using O
analogies O
to O
quantify O
gender O
bias O
in O
embeddings O
and O
proposed O
a O
modiÔ¨Åed O
analogy O
task O
to O
produce O
analogies O
from O
the O
gender O
base O
pair O
( O
she O
, O
he O
) O
. O
The O
task O
identiÔ¨Åes O
word O
pairs O
( O
x O
, O
y O
) O
, O
such O
that O
‚Äú O
heis O
toxas O
sheis O
toy O
‚Äù O
, O
where||‚àí O
‚Üíx‚àí‚àí O
‚Üíy||= O
1 O
. O
This O
method O
was O
expanded O
to O
mutli O
- O
class O
forms O
of O
bias O
such O
as O
racial O
bias O
by O
Mehrabi O
et O
al O
. O
( O
2019 O
) O
. O
However O
, O
the O
suitability O
of O
analogies O
as O
indicators O
of O
bias O
was O
questioned O
by O
Nissim O
et O
al O
. O
( O
2019 O
) O
, O
who O
highlighted O
the O
fact O
that O
the O
approach O
used O
by O
Bolukbasi O
et O
al O
. O
( O
2016 O
) O
did O
not O
allow O
analogies O
to O
return O
their O
input O
words O
, O
thus O
artiÔ¨Åcially O
increasing O
the O
perception O
of O
bias.3 O
Approach O
Our O
aim O
is O
to O
examine O
the O
extent O
to O
which O
bias O
identifying O
techniques O
are O
reliabily O
capturing O
societal O
gender O
bias O
. O
Bias O
is O
a O
highly O
complex O
concept O
, O
and O
although O
the O
four O
bias O
measures O
( O
DB O
, O
WA O
, O
NBM O
and O
RIPA O
) O
may O
detect O
certain O
kinds O
of O
bias O
, O
there O
is O
no O
theoretical O
guarantee O
they O
will O
detect O
all O
forms O
, O
that O
the O
‚Äú O
bias O
‚Äù O
they O
Ô¨Ånd O
will O
be O
accurate O
or O
that O
different O
choices O
of O
base O
pair O
will O
behave O
similarly O
. O
We O
therefore O
explore O
whether O
the O
bias O
measures O
are O
robust O
in O
detecting O
the O
bias O
they O
appear O
to O
detect O
and O
if O
there O
are O
forms O
of O
bias O
they O
are O
not O
sensitive O
to O
. O
We O
propose O
three O
conditions O
to O
test O
this O
: O
1 O
) O
Base O
pair O
stability O
: O
If O
bias O
measures O
captured O
real O
- O
world O
information O
in O
a O
reliable O
way O
, O
it O
would O
be O
expected O
that O
reasonable O
changes O
of O
the O
base O
pair O
, O
such O
as O
( O
she O
, O
he O
) O
to(woman O
, O
man O
) O
or(she O
, O
he O
) O
to(She O
, O
He O
) O
, O
would O
not O
frequently O
cause O
a O
signiÔ¨Åcant O
change O
in O
bias O
. O
2 O
) O
Word O
form O
stability O
: O
While O
different O
forms O
of O
a O
word O
, O
such O
as O
plurals O
, O
have O
different O
contexts O
and O
word O
vectors O
, O
their O
social O
bias O
will O
not O
signiÔ¨Åcantly O
change O
and O
they O
should O
have O
similar O
bias O
scores O
. O
3 O
) O
Linguistic O
correspondence O
: O
We O
explore O
the O
extent O
to O
which O
the O
measures O
predict O
the O
expected O
gender O
of O
terms O
containing O
explicit O
gender O
information O
( O
e.g. O
‚Äú O
lioness O
‚Äù O
) O
or O
, O
based O
on O
some O
accounts O
, O
stereotypically O
( O
e.g. O
‚Äú O
compassionate O
‚Äù O
) O
. O
Of O
course O
, O
due O
to O
noise O
and O
the O
problem O
of O
implicit O
bias O
, O
these O
three O
conditions O
may O
not O
always O
be O
true O
. O
However O
, O
if O
they O
do O
not O
hold O
the O
majority O
of O
the O
time O
, O
it O
must O
be O
questioned O
if O
the O
measures O
are O
reliably O
identifying O
social O
bias O
. O
4 O
Data O
To O
allow O
for O
fair O
comparisons O
, O
we O
use O
the O
same O
datasets O
as O
previous O
work O
where O
possible O
: O
Embeddings O
: O
300 O
- O
dimensional O
Google O
News O
word2vec O
( O
Mikolov O
et O
al O
. O
, O
2013a O
, O
b O
) O
. O
Professions O
: O
A O
list O
of O
320 O
professions O
( O
Bolukbasi O
et O
al O
. O
, O
2016 O
) O
, O
often O
used O
to O
analyse O
bias O
measures O
. O
Base O
pairs O
: O
A O
standard O
list O
of O
10 O
gender O
base O
pairs O
, O
including O
( O
she O
, O
he O
) O
( O
Bolukbasi O
et O
al O
. O
, O
2016 O
) O
. O
Gender O
neutral O
: O
For O
NBM O
, O
we O
use O
the O
set O
of O
26,145 O
gender O
neutral O
words O
deÔ¨Åned O
in O
( O
Gonen O
and O
Goldberg O
, O
2019 O
) O
. O
In O
addition O
, O
we O
construct O
two O
new O
test O
sets O
, O
both O
listed O
in O
Appendix O
A O
: O
BSRI O
: O
To O
assess O
whether O
word O
embeddings O
contain O
undesirable O
gender O
stereotypes O
, O
we O
utilise O
the O
Bem O
Sex O
Role O
Inventory O
( O
BSRI O
) O
which O
developed761a O
list O
of O
20 O
traits O
for O
men O
and O
20 O
for O
women O
that O
are O
considered O
to O
be O
socially O
desirable O
, O
such O
as O
‚Äú O
assertive O
‚Äù O
and O
‚Äú O
compassionate O
‚Äù O
respectively O
( O
Bem O
, O
1974).2Although O
derived O
in O
the O
1970s O
, O
this O
work O
remains O
one O
of O
the O
most O
inÔ¨Çuential O
and O
widely O
accepted O
measures O
of O
socially O
constructed O
gender O
roles O
within O
the O
social O
sciences O
, O
e.g. O
( O
Holt O
and O
Ellis O
, O
1998 O
; O
Dean O
and O
Tate O
, O
2016 O
; O
Starr O
and O
Zurbriggen O
, O
2016 O
; O
Matud O
et O
al O
. O
, O
2019 O
) O
. O
Of O
particular O
relevance O
to O
NLP O
applications O
, O
Gaucher O
et O
al O
. O
( O
2011 O
) O
use O
BSRI O
to O
identify O
gender O
- O
biased O
language O
in O
job O
advertisements O
and O
demonstrate O
this O
language O
can O
contribute O
to O
workplace O
gender O
inequality O
. O
BSRI O
traits O
not O
in O
the O
embedding O
vocabulary O
( O
e.g. O
‚Äú O
willing O
to O
take O
risks O
‚Äù O
) O
were O
removed O
. O
For O
each O
remaining O
trait O
, O
we O
queried O
Merriam O
Webster O
for O
other O
forms O
of O
that O
word O
( O
for O
example O
, O
‚Äú O
assertiveness O
‚Äù O
is O
a O
form O
of O
‚Äú O
assertive O
‚Äù O
) O
, O
resulting O
in O
a O
list O
of O
58 O
characteristics O
( O
27 O
male O
and O
31 O
female O
) O
. O
Animals O
: O
Some O
words O
, O
including O
the O
names O
of O
certain O
animals O
, O
encode O
gender O
linguistically O
( O
e.g. O
‚Äú O
lioness O
‚Äù O
) O
. O
Wikipedia O
provides O
a O
table O
of O
male O
and O
female O
versions O
of O
animal O
names.3This O
table O
was O
downloaded O
, O
and O
duplicates O
, O
rare O
words O
and O
terms O
whose O
animal O
usage O
is O
uncommon O
( O
for O
example O
, O
a O
‚Äú O
cob O
‚Äù O
is O
a O
male O
swan O
) O
were O
removed O
. O
This O
resulted O
a O
set O
of O
26 O
terms O
consisting O
13 O
female O
- O
male O
pairs O
such O
as O
( O
hen O
, O
rooster O
) O
. O
5 O
Evaluation O
Evaluating O
gender O
bias O
measures O
is O
a O
complex O
task O
as O
there O
is O
no O
inherent O
ground O
truth O
interpretation O
of O
the O
measure O
‚Äôs O
results O
. O
For O
example O
, O
it O
is O
unclear O
when O
a O
bias O
score O
is O
problematic O
. O
We O
choose O
to O
evaluate O
the O
four O
bias O
measures O
( O
DB O
, O
WA O
, O
NBM O
andRIPA O
) O
in O
two O
ways O
, O
Ô¨Årst O
by O
considering O
whether O
a O
word O
is O
assigned O
a O
male O
or O
female O
bias O
, O
and O
second O
what O
the O
magnitude O
of O
that O
score O
is O
. O
The O
bias O
direction O
( O
male O
or O
female O
) O
assigned O
by O
a O
measure O
to O
a O
word O
is O
determined O
by O
the O
sign O
of O
the O
score O
( O
whether O
a O
positive O
score O
denotes O
male O
or O
female O
bias O
depends O
on O
the O
ordering O
of O
the O
base O
pair O
words O
) O
. O
The O
assignment O
of O
bias O
direction O
is O
viewed O
an O
annotation O
task O
in O
which O
a O
bias O
measure O
( O
with O
a O
speciÔ¨Åed O
base O
pair O
) O
is O
considered O
an O
‚Äú O
annotator O
‚Äù O
making O
assignments O
. O
Consistency O
between O
annotators O
( O
i.e. O
versions O
of O
bias O
measures O
) O
can O
be O
2Our O
use O
of O
BSRI O
should O
not O
be O
interpreted O
as O
an O
endorsement O
of O
these O
traits O
as O
either O
accurate O
or O
desirable O
; O
rather O
we O
use O
them O
as O
a O
dataset O
of O
commonly O
held O
stereotypes O
. O
3https://en.wikipedia.org/wiki/List O
ofanimal O
namescomputed O
using O
Cohen O
‚Äôs O
kappa O
to O
determine O
pairwise O
agreement O
( O
Cohen O
, O
1960 O
) O
and O
Fleiss O
‚Äô O
kappa O
( O
Fleiss O
, O
1971 O
) O
for O
multiple O
annotators O
. O
We O
follow O
a O
widely O
used O
interpretation O
of O
kappa O
scores O
( O
Landis O
and O
Koch O
, O
1977 O
) O
. O
The O
second O
method O
of O
evaluation O
is O
an O
analysis O
of O
the O
magnitude O
of O
bias O
assigned O
. O
Previous O
work O
in O
this O
area O
does O
not O
deÔ¨Åne O
what O
constitutes O
a O
‚Äú O
signiÔ¨Åcant O
‚Äù O
change O
of O
the O
magnitude O
of O
a O
bias O
score O
. O
Therefore O
, O
we O
estimate O
the O
mean O
bias O
in O
the O
embedding O
space O
as O
follows O
: O
The O
50,000 O
most O
frequent O
words O
in O
the O
embedding O
vocabulary O
were O
selected O
and O
, O
following O
Bolukbasi O
et O
al O
. O
( O
2016 O
) O
, O
all O
words O
containing O
digits O
, O
punctuation O
or O
that O
were O
more O
than O
20 O
characters O
long O
were O
removed O
. O
For O
each O
of O
the O
remaining O
48,088 O
words O
, O
their O
bias O
score O
was O
calculated O
with O
respect O
to O
each O
of O
the O
10 O
base O
pairs O
( O
so O
for O
each O
measure O
, O
there O
are O
480,880 O
scores O
) O
. O
An O
examination O
of O
these O
scores O
revealed O
them O
to O
appear O
approximately O
normally O
distributed O
and O
so O
their O
mean O
and O
standard O
deviation O
are O
used O
as O
an O
approximation O
of O
the O
population O
mean O
and O
standard O
deviation O
( O
see O
Table O
1 O
) O
. O
We O
consider O
a O
relevant O
change O
in O
magnitude O
to O
be O
a O
change O
of O
at O
least O
one O
standard O
deviation O
. O
DB O
/ O
WA O
RIPA O
NBM O
Mean O
-0.001 O
0.024 O
-0.038 O
Standard O
Dev O
. O
0.053 O
0.239 O
0.431 O
Table O
1 O
: O
Mean O
and O
standard O
deviation O
of O
bias O
scores O
for O
each O
measure O
. O
6 O
Results O
Base O
pair O
stability O
: O
The O
Ô¨Årst O
experiment O
explored O
the O
robustness O
of O
the O
four O
measures O
( O
DB O
, O
WA O
, O
RIPA O
andNBM O
) O
to O
changing O
the O
base O
pair O
. O
For O
example O
, O
Figure O
1 O
illustrates O
the O
effects O
of O
changing O
the O
base O
pair O
on O
the O
bias O
score O
of O
the O
word O
‚Äú O
professor O
. O
‚Äù O
More O
comprehensively O
, O
for O
each O
bias O
measure O
we O
computed O
the O
bias O
assigned O
to O
each O
profession O
for O
each O
base O
pair O
, O
and O
then O
calculated O
the O
agreement O
between O
the O
10 O
base O
pairs O
via O
Fleiss O
‚Äô O
kappa O
coefÔ¨Åcient O
. O
The O
changes O
in O
the O
bias O
magnitude O
of O
a O
word O
between O
base O
pairs O
were O
also O
computed O
. O
Results O
are O
shown O
in O
Table O
2 O
. O
The O
level O
of O
agreement O
of O
bias O
direction O
between O
base O
pairs O
was O
fair O
( O
0.29 O
) O
for O
NBM O
and O
moderate O
( O
0.42 O
and O
0.45 O
) O
for O
RIPA O
and O
DB O
/ O
WA O
. O
This O
means O
that O
changing O
the O
base O
pair O
frequently O
caused O
a O
profession‚Äôs762Figure O
1 O
: O
Graphs O
demonstrating O
bias O
score O
variations O
. O
Each O
graph O
represents O
a O
measure O
, O
with O
the O
mean O
and O
standard O
deviation O
of O
that O
measure O
( O
Section O
5 O
) O
denoted O
by O
dashed O
lines O
. O
Positive O
and O
negative O
scores O
indicate O
female O
and O
male O
bias O
respectively O
, O
while O
larger O
absolute O
values O
show O
higher O
levels O
of O
bias O
. O
The O
bias O
scores O
of O
the O
word O
‚Äú O
professor O
‚Äù O
and O
and O
its O
variations O
( O
‚Äú O
professors O
, O
‚Äù O
Professor O
‚Äù O
and O
‚Äú O
PROFESSOR O
‚Äù O
) O
are O
shown O
, O
as O
calculated O
according O
to O
each O
base O
pair O
( O
such O
as O
( O
she O
, O
he O
) O
and(her O
, O
his O
) O
) O
. O
The O
graphs O
demonstrate O
that O
the O
bias O
direction O
and O
magnitude O
of O
bias O
of O
each O
word O
depend O
heavily O
on O
which O
base O
pair O
is O
chosen O
. O
They O
also O
show O
that O
the O
different O
forms O
of O
the O
word O
exhibit O
different O
behaviour O
. O
Kappa O
Magnitude O
DB O
/ O
WA O
0.45 O
0.69 O
RIPA O
0.42 O
0.66 O
NBM O
0.29 O
0.71 O
Table O
2 O
: O
For O
the O
320 O
professions O
1 O
) O
the O
level O
of O
agreement O
kappa O
between O
bias O
directions O
assigned O
by O
each O
of O
the O
ten O
base O
pairs O
and O
2 O
) O
the O
mean O
proportion O
of O
signiÔ¨Åcant O
magnitude O
changes O
over O
the O
10 O
base O
pairs O
. O
For O
1 O
) O
, O
higher O
is O
better O
, O
and O
for O
2 O
) O
, O
lower O
is O
better O
. O
bias O
direction O
to O
change O
. O
For O
example O
, O
the O
RIPA O
direction O
of O
‚Äú O
surgeon O
‚Äù O
is O
male O
for O
( O
she O
, O
he O
) O
but O
female O
for O
( O
woman O
, O
man O
) O
. O
For O
a O
given O
profession O
, O
only O
about O
a O
quarter O
of O
DB O
/ O
WA O
and O
RIPA O
directions O
were O
the O
same O
for O
every O
base O
pair O
, O
and O
fewer O
than O
15 O
% O
of O
NBM O
directions O
were O
. O
With O
regards O
to O
score O
magnitudes O
, O
on O
average O
over O
the O
professions O
, O
66 O
% O
of O
base O
pair O
changes O
saw O
a O
relevant O
change O
in O
magnitude O
( O
more O
than O
one O
standard O
deviation O
) O
forRIPA O
, O
69 O
% O
for O
DB O
/ O
WA O
and O
71 O
% O
for O
NBM O
. O
Next O
, O
to O
explore O
the O
robustness O
of O
the O
form O
of O
the O
base O
pairs O
chosen O
, O
we O
compared O
the O
bias O
direction O
assigned O
to O
each O
of O
the O
320 O
professions O
by O
a O
base O
pair O
to O
the O
bias O
direction O
assigned O
by O
the O
capitalised O
form O
( O
Ô¨Årst O
letter O
capitalised O
) O
of O
that O
base O
pair O
( O
for O
example O
, O
( O
she O
, O
he O
) O
versus O
( O
She O
, O
He O
) O
) O
. O
The O
level O
of O
agreement O
of O
bias O
direction O
between O
each O
two O
base O
pair O
forms O
was O
calculated O
using O
Cohen O
‚Äôs O
kappa O
coefÔ¨Åcient O
, O
results O
are O
shown O
in O
Table O
3 O
. O
The O
mean O
of O
the O
level O
of O
agreement O
over O
each O
of O
the O
10 O
base O
pairs O
ranged O
from O
0.39 O
( O
fair O
) O
to O
0.43 O
( O
moderate O
) O
, O
with O
many O
individual O
agreements O
below O
moderate O
level O
. O
In O
particular O
, O
an O
agreement O
level O
of O
only O
0.03 O
( O
very O
slight O
) O
is O
found O
for O
the O
base O
pair O
( O
gal O
, O
guy O
) O
compared O
with O
( O
Gal O
, O
Guy O
) O
for O
DB O
/ O
WA O
. O
Word O
form O
stability O
: O
The O
second O
experiment O
examined O
the O
measures O
‚Äô O
robustness O
to O
changing O
the O
form O
of O
a O
word O
considered O
by O
comparing O
a O
word O
‚Äôs O
plural O
, O
capitalised O
( O
Ô¨Årst O
letter O
capitalised O
) O
and O
up-763percase O
( O
all O
letters O
capitalised O
) O
forms O
to O
its O
base O
form O
. O
For O
example O
, O
‚Äú O
professors O
, O
‚Äù O
‚Äú O
Professor O
‚Äù O
and O
‚Äú O
PROFESSOR O
‚Äù O
were O
compared O
to O
‚Äú O
professor O
‚Äù O
( O
see O
Figure O
1 O
) O
. O
For O
this O
experiment O
, O
only O
the O
230 O
words O
in O
the O
professions O
list O
whose O
plural O
, O
capitalised O
and O
uppercase O
forms O
are O
all O
included O
in O
the O
embedding O
vocabulary O
were O
used O
. O
For O
each O
measure O
and O
base O
pair O
, O
the O
direction O
of O
gender O
bias O
of O
each O
word O
form O
was O
computed O
, O
and O
the O
pairwise O
level O
of O
agreement O
( O
Cohen O
‚Äôs O
kappa O
) O
between O
the O
original O
form O
of O
a O
word O
and O
each O
of O
its O
variants O
was O
calculated O
, O
see O
Table O
4 O
. O
All O
four O
measures O
were O
found O
to O
give O
different O
versions O
of O
the O
same O
word O
( O
plural O
, O
capital O
and O
uppercase O
forms O
) O
different O
bias O
directions O
. O
For O
example O
, O
the O
DB O
/ O
WA O
of O
‚Äú O
surgeon O
‚Äù O
is O
male O
but O
of O
‚Äú O
surgeons O
‚Äù O
is O
female O
( O
base O
pair O
( O
she O
, O
he O
) O
) O
. O
For O
each O
measure O
, O
the O
mean O
kappa O
coefÔ¨Åcients O
were O
moderate O
for O
the O
plural O
category O
and O
fair O
for O
the O
uppercase O
category O
. O
For O
the O
capital O
category O
, O
they O
were O
moderate O
for O
DB O
/ O
WA O
and O
RIPA O
, O
and O
substantial O
for O
NBM O
. O
Since O
changing O
word O
form O
frequently O
changes O
bias O
direction O
, O
these O
results O
indicate O
the O
bias O
measures O
are O
not O
reliably O
reÔ¨Çecting O
any O
inherent O
social O
bias O
encoded O
into O
the O
word O
vectors O
, O
and O
that O
the O
gender O
bias O
direction O
assigned O
to O
a O
profession O
is O
not O
robust O
. O
Linguistic O
Correspondence O
: O
The O
Ô¨Ånal O
experiment O
examined O
the O
measures O
‚Äô O
prediction O
for O
terms O
containing O
explicit O
or O
stereotypical O
gender O
information O
, O
in O
the O
form O
of O
social O
stereotypes O
( O
BSRI O
) O
and O
linguistic O
gender O
( O
Animals O
) O
. O
The O
predicted O
gender O
bias O
direction O
of O
the O
words O
in O
the O
Animals O
and O
BSRI O
lists O
was O
computed O
for O
each O
base O
pair O
and O
measure O
, O
and O
compared O
with O
the O
ground O
- O
truth O
gender O
of O
the O
words O
. O
Table O
5 O
shows O
the O
pairwise O
agreement O
( O
Cohen O
‚Äôs O
kappa O
) O
between O
prediction O
and O
ground O
- O
truth O
for O
each O
base O
pair O
, O
as O
well O
as O
the O
mean O
agreement O
over O
all O
10 O
base O
pairs O
. O
The O
bias O
measures O
did O
not O
predict O
the O
groundtruth O
gender O
of O
either O
set O
of O
words O
with O
high O
accuracy O
; O
mean O
agreement O
levels O
varied O
from O
0.17 O
( O
slight O
) O
to O
0.42 O
( O
moderate O
) O
. O
For O
example O
, O
the O
NBM O
gender O
prediction O
for O
‚Äú O
bull O
, O
‚Äù O
a O
male O
animal O
, O
was O
female O
and O
the O
direction O
of O
the O
feminine O
BSRI O
trait O
‚Äú O
compassionate O
‚Äù O
was O
male O
( O
both O
for O
base O
pair(woman O
, O
man O
) O
) O
. O
As O
with O
the O
previous O
experiment O
, O
different O
forms O
of O
the O
BSRI O
words O
frequently O
were O
assigned O
opposite O
genders O
: O
unlike O
‚Äú O
compassionate O
‚Äù O
, O
‚Äú O
compassionately O
‚Äù O
had O
the O
correct O
NBM O
gender O
prediction O
, O
again O
with O
base O
pair O
( O
woman O
, O
man O
) O
. O
The O
BRSI O
results O
were O
overallpoorer O
than O
the O
Animal O
results O
, O
with O
some O
base O
pairs O
having O
negative O
kappa O
scores O
, O
indicating O
less O
agreement O
than O
random O
chance O
. O
This O
may O
be O
because O
the O
BSRI O
stereotypes O
are O
less O
likely O
to O
be O
mentioned O
in O
the O
context O
of O
base O
pair O
words O
like O
‚Äú O
he O
‚Äù O
and O
‚Äú O
she O
. O
‚Äù O
Interestingly O
, O
the O
highest O
scoring O
BSRI O
base O
pair O
was O
( O
mother O
, O
father O
) O
. O
Some O
of O
the O
inaccurate O
predictions O
for O
the O
animal O
words O
may O
come O
from O
the O
fact O
that O
some O
terms O
can O
both O
refer O
to O
males O
and O
be O
gender O
neutral O
, O
e.g. O
‚Äú O
lion O
. O
‚Äù O
7 O
Discussion O
Lack O
of O
Robustness O
: O
The O
experiments O
in O
this O
work O
empirically O
showed O
that O
the O
four O
bias O
measures O
are O
not O
robust O
to O
changing O
either O
the O
base O
pair O
or O
the O
form O
of O
a O
word O
used O
( O
such O
as O
singualar O
to O
plural O
) O
. O
We O
hypothesise O
there O
are O
two O
primary O
reasons O
for O
this O
: O
sociolinguistic O
factors O
and O
mathematical O
properties O
of O
the O
bias O
measure O
formulae O
. O
It O
is O
highly O
likely O
that O
linguistic O
properties O
of O
the O
base O
pair O
chosen O
effect O
bias O
measure O
robustness.4 O
For O
example O
, O
( O
she O
, O
he O
) O
has O
quite O
different O
sociolinguistic O
connotations O
to O
the O
more O
casual O
( O
gal O
, O
guy O
) O
, O
and O
‚Äú O
she O
‚Äù O
and O
‚Äú O
he O
‚Äù O
are O
clearly O
linguistic O
opposites O
, O
unlike O
‚Äú O
Mary O
‚Äù O
and O
‚Äú O
John O
. O
‚Äù O
Our O
results O
indicate O
that O
more O
neutral O
base O
pairs O
which O
are O
linguistic O
opposites O
, O
such O
as O
( O
she O
, O
he O
) O
or(man O
, O
woman O
) O
are O
the O
most O
robust O
. O
However O
, O
even O
they O
exhibit O
variation O
and O
struggle O
particularly O
to O
pick O
up O
on O
social O
stereotypes O
( O
the O
BSRI O
agreements O
for O
( O
man O
, O
woman O
) O
are O
all O
close O
to O
zero O
, O
indicating O
random O
chance O
) O
. O
A O
further O
reason O
that O
the O
bias O
measures O
are O
not O
robust O
is O
their O
reliance O
on O
the O
direct O
output O
of O
a O
dot O
product O
, O
which O
is O
sensitive O
to O
the O
input O
vectors O
used O
. O
Given O
a O
base O
pair O
( O
a O
, O
b O
) O
, O
we O
will O
refer O
to‚àí O
‚Üía‚àí‚àí O
‚Üíbas O
its O
difference O
vector O
. O
The O
10 O
base O
pairs O
have O
highly O
similar O
difference O
vectors O
: O
the O
mean O
over O
the O
10 O
base O
pairs O
of O
cos(‚àí O
‚Üía‚àí‚àí O
‚Üíb O
, O
‚àí O
‚Üíc‚àí‚àí O
‚Üíd O
) O
, O
where O
( O
a O
, O
b O
) O
and(c O
, O
d)are O
base O
pairs O
is O
0.5 O
. O
While O
this O
is O
very O
high O
for O
embedding O
vectors,5it O
does O
not O
guarantee O
‚àí O
‚Üíw¬∑(‚àí O
‚Üíx‚àí‚àí O
‚Üíy)and‚àí O
‚Üíw¬∑(‚àí O
‚Üía‚àí‚àí O
‚Üíb)will O
have O
the O
same O
sign O
for O
all O
words O
w O
, O
resulting O
in O
opposite O
bias O
directions O
. O
The O
same O
sensitivity O
explains O
why O
words O
and O
their O
plurals O
can O
be O
assigned O
opposite O
bias O
directions O
, O
even O
if O
they O
have O
similar O
embeddings O
. O
Furthermore O
, O
similarity O
between O
base O
pair O
difference O
vectors O
is O
highly O
correlated O
with O
agree4Our O
choice O
of O
base O
pairs O
follows O
previous O
work O
. O
5We O
randomly O
sampled O
100,000 O
sets O
of O
words O
{ O
a O
, O
d O
, O
c O
, O
d O
} O
and O
computed O
cos(‚àí O
‚Üía‚àí‚àí O
‚Üíb O
, O
‚àí O
‚Üíc‚àí‚àí O
‚Üíd O
) O
; O
the O
sample O
mean O
was O
0.00 O
, O
with O
standard O
deviation O
0.09.764She O
Her O
Woman O
Mary O
Herself O
Dgtr O
Mother O
Gal O
Girl O
Female O
He O
His O
Man O
John O
Himself O
Son O
Father O
Guy O
Boy O
MaleMean O
DB O
/ O
WA O
0.65 O
0.53 O
0.56 O
0.32 O
0.60 O
0.28 O
0.40 O
0.03 O
0.49 O
0.38 O
0.42 O
RIPA O
0.80 O
0.56 O
0.58 O
0.32 O
0.59 O
0.27 O
0.31 O
0.04 O
0.49 O
0.35 O
0.43 O
NBM O
0.58 O
0.65 O
0.61 O
0.19 O
0.69 O
0.18 O
0.23 O
0.10 O
0.53 O
0.18 O
0.39 O
Table O
3 O
: O
Results O
of O
the O
base O
pair O
stability O
experiments O
: O
Agreement O
between O
the O
bias O
directions O
assigned O
by O
a O
base O
pair O
and O
its O
capitalised O
form O
( O
e.g. O
( O
she O
, O
he O
) O
and O
( O
She O
, O
He O
) O
) O
for O
the O
320 O
professions O
, O
and O
the O
mean O
over O
all O
base O
pairs O
. O
she O
her O
woman O
mary O
herself O
dgtr O
mother O
gal O
girl O
female O
he O
his O
man O
john O
himself O
son O
father O
guy O
boy O
maleMean O
PluralDB O
/ O
WA O
0.50 O
0.51 O
0.53 O
0.35 O
0.47 O
0.33 O
0.42 O
0.47 O
0.52 O
0.53 O
0.46 O
RIPA O
0.57 O
0.58 O
0.63 O
0.39 O
0.53 O
0.46 O
0.44 O
0.53 O
0.53 O
0.50 O
0.52 O
NBM O
0.69 O
0.57 O
0.72 O
0.38 O
0.65 O
0.32 O
0.50 O
0.59 O
0.60 O
0.62 O
0.56 O
CapitalDB O
/ O
WA O
0.61 O
0.66 O
0.59 O
0.42 O
0.67 O
0.79 O
0.61 O
0.50 O
0.50 O
0.44 O
0.58 O
RIPA O
0.60 O
0.60 O
0.54 O
0.36 O
0.59 O
0.69 O
0.61 O
0.54 O
0.53 O
0.45 O
0.55 O
NBM O
0.77 O
0.63 O
0.68 O
0.54 O
0.74 O
0.68 O
0.61 O
0.71 O
0.65 O
0.63 O
0.66 O
UpperDB O
/ O
WA O
0.19 O
0.35 O
0.43 O
0.17 O
0.29 O
0.48 O
0.18 O
0.20 O
0.34 O
0.30 O
0.29 O
RIPA O
0.35 O
0.38 O
0.40 O
0.16 O
0.35 O
0.53 O
0.22 O
0.20 O
0.30 O
0.27 O
0.32 O
NBM O
0.50 O
0.52 O
0.49 O
0.25 O
0.52 O
0.40 O
0.22 O
0.46 O
0.54 O
0.13 O
0.40 O
Table O
4 O
: O
Results O
of O
the O
word O
form O
stability O
experiments O
: O
Agreement O
between O
the O
bias O
direction O
of O
a O
profession O
and O
its O
plural O
, O
capital O
and O
uppercase O
forms O
for O
each O
base O
pair O
, O
and O
the O
mean O
over O
all O
base O
pairs O
. O
she O
her O
woman O
mary O
herself O
dgtr O
mother O
gal O
girl O
female O
he O
his O
man O
john O
himself O
son O
father O
guy O
boy O
maleMean O
BSRIDB O
/ O
WA O
0.35 O
0.37 O
0.07 O
-0.03 O
0.14 O
0.03 O
0.45 O
0.39 O
-0.08 O
0.01 O
0.17 O
RIPA O
0.44 O
0.40 O
0.09 O
-0.08 O
0.12 O
0.16 O
0.45 O
0.39 O
-0.08 O
0.01 O
0.19 O
NBM O
0.27 O
0.32 O
-0.01 O
0.01 O
0.27 O
0.17 O
0.46 O
0.14 O
0.18 O
-0.04 O
0.18 O
AnimalDB O
/ O
WA O
0.54 O
0.38 O
0.54 O
0.54 O
0.54 O
0.31 O
0.23 O
0.46 O
0.54 O
0.08 O
0.42 O
RIPA O
0.31 O
0.38 O
0.31 O
0.46 O
0.46 O
0.23 O
0.23 O
0.54 O
0.46 O
0.08 O
0.35 O
NBM O
0.31 O
0.08 O
0.15 O
0.15 O
0.15 O
0.00 O
0.08 O
0.46 O
0.15 O
0.00 O
0.15 O
Table O
5 O
: O
Results O
of O
the O
linguistic O
correspondence O
experiments O
: O
Agreement O
between O
the O
ground O
- O
truth O
and O
predicted O
gender O
for O
each O
base O
pair O
, O
and O
the O
mean O
over O
all O
10 O
base O
pairs O
. O
she O
her O
woman O
mary O
herself O
dgtr O
mother O
gal O
girl O
female O
he O
his O
man O
john O
himself O
son O
father O
guy O
boy O
maleMean O
DB O
/ O
WA O
& O
RIPA O
0.69 O
0.86 O
0.64 O
0.90 O
0.82 O
0.79 O
0.92 O
0.85 O
0.89 O
0.96 O
0.83 O
DB O
/ O
WA O
& O
NBM O
0.54 O
0.37 O
0.62 O
0.44 O
0.55 O
0.54 O
0.46 O
0.34 O
0.48 O
0.47 O
0.48 O
RIPA O
& O
NBM O
0.52 O
0.42 O
0.66 O
0.41 O
0.57 O
0.57 O
0.47 O
0.29 O
0.47 O
0.50 O
0.49 O
Table O
6 O
: O
Comparing O
bias O
measures O
: O
Agreement O
between O
the O
bias O
direction O
assigned O
by O
each O
pair O
of O
bias O
measures O
( O
with O
a O
Ô¨Åxed O
base O
pair O
) O
for O
the O
320 O
professions O
, O
and O
the O
mean O
over O
the O
10 O
base O
pairs.765she O
- O
he O
her O
- O
his O
woman O
- O
man O
mary O
- O
john O
herself O
- O
himself O
daughter O
- O
son O
mother O
- O
father O
gal O
- O
guy O
girl O
- O
boy O
female O
- O
male0.00.20.40.60.81.0Pearson O
Correlation O
CoefficientFigure O
2 O
: O
Correlation O
between O
the O
cosine O
similarity O
of O
the O
base O
pair O
difference O
vectors O
and O
the O
corresponding O
pairwise O
kappa O
coefÔ¨Åcients O
for O
the O
DB O
/ O
WA O
professions O
bias O
directions O
. O
ment O
between O
bias O
directions O
: O
For O
each O
base O
pair O
( O
a O
, O
b O
) O
, O
we O
computed O
cos(‚àí O
‚Üía‚àí‚àí O
‚Üíb O
, O
‚àí O
‚Üíc‚àí‚àí O
‚Üíd O
) O
, O
for O
each O
of O
the O
other O
9 O
base O
pairs O
( O
c O
, O
d O
) O
, O
and O
compared O
these O
scores O
to O
the O
pairwise O
agreements O
between O
the O
corresponding O
DB O
/ O
WA O
bias O
directions O
assigned O
to O
the O
professions O
. O
There O
was O
a O
high O
Pearson O
correlation O
( O
max O
p O
- O
value O
0.005 O
) O
in O
each O
case O
, O
see O
Figure O
2 O
. O
The O
lack O
of O
robustness O
of O
the O
gender O
bias O
measures O
means O
care O
should O
be O
taken O
is O
ascribing O
their O
output O
to O
historic O
bias O
in O
the O
training O
data O
or O
algorithmic O
bias O
in O
the O
embedding O
process O
. O
Rather O
, O
our O
analysis O
indicates O
that O
a O
signiÔ¨Åcant O
proportion O
of O
the O
‚Äú O
bias O
‚Äù O
found O
is O
an O
artifact O
of O
the O
evaluation O
method O
( O
bias O
measures O
) O
used O
. O
Comparing O
Bias O
Measures O
: O
A O
limitation O
of O
previous O
work O
is O
it O
unclear O
which O
of O
the O
proposed O
gender O
bias O
measures O
is O
best O
, O
even O
though O
they O
are O
often O
introduced O
as O
alternatives O
to O
one O
another O
. O
The O
results O
of O
our O
study O
are O
mixed O
and O
no O
one O
measure O
emerges O
as O
reliable O
. O
Despite O
NBM O
being O
designed O
as O
an O
alternative O
to O
DB O
, O
which O
takes O
into O
account O
the O
socially O
biased O
neighbours O
of O
a O
word O
, O
our O
experiments O
found O
it O
performs O
more O
poorly O
on O
the O
socially O
biased O
terms O
( O
BSRI O
) O
than O
DB O
with O
its O
recommended O
base O
pair O
( O
she O
, O
he O
) O
( O
Table O
5 O
) O
. O
Conversely O
, O
it O
was O
less O
sensitive O
to O
different O
word O
- O
forms O
( O
Table O
4 O
) O
. O
This O
is O
likely O
because O
different O
forms O
of O
wshare O
c O
ommon O
subsets O
of O
top O
K O
- O
neighbors O
with O
w. O
Furthermore O
, O
Ethayarajh O
et O
al O
. O
( O
2019 O
) O
claim O
RIPA O
is O
an O
improvement O
on O
WA O
because O
RIPA O
is O
robust O
to O
changing O
the O
base O
pair O
if O
the O
two O
corresponding O
difference O
vectors O
are O
‚Äú O
roughly O
the O
same O
, O
‚Äù O
and O
give O
( O
man O
, O
woman O
) O
and O
( O
king O
, O
queen O
) O
as O
an O
example O
. O
However O
, O
we O
Ô¨Ånd O
this O
claim O
does O
not O
hold O
: O
this O
change O
of O
base O
pair O
causes O
28 O
% O
( O
91 O
) O
of O
the O
Professions O
words O
to O
alter O
their O
RIPA O
bias O
direction O
. O
Finally O
, O
we O
compared O
agreement O
between O
the O
bias O
directions O
assigned O
to O
the O
professions O
by O
different O
pairs O
of O
measures O
( O
Table O
6 O
) O
. O
The O
results O
show O
that O
on O
average O
, O
there O
is O
an O
almost O
perfect O
level O
of O
agreement O
( O
0.83 O
) O
between O
RIPA O
and O
DB O
/ O
WA O
, O
and O
moderate O
levels O
of O
agreement O
between O
NBM O
and O
the O
other O
measures O
. O
As O
RIPA O
and O
DB O
/ O
WA O
have O
very O
similar O
formulae O
, O
the O
high O
level O
of O
agreement O
between O
them O
for O
each O
base O
pair O
indicates O
that O
the O
choice O
of O
base O
pair O
is O
highly O
inÔ¨Çuential O
and O
more O
important O
than O
the O
difference O
in O
their O
formulae O
. O
Figure O
1 O
illustrates O
this O
point O
by O
showing O
that O
the O
measures O
tend O
to O
change O
in O
a O
similar O
manner O
from O
base O
pair O
to O
base O
pair O
for O
each O
word O
variant O
. O
Analogies O
do O
not O
indicate O
bias O
: O
Analogies O
are O
often O
used O
as O
evidence O
of O
bias O
in O
word O
embeddings O
( O
Bolukbasi O
et O
al O
. O
, O
2016 O
; O
Manzini O
et O
al O
. O
, O
2019 O
) O
. O
This O
section O
argues O
they O
are O
unsuitable O
indicators O
of O
bias O
as O
they O
primarily O
reÔ¨Çect O
similarity O
, O
and O
not O
necessarily O
linguistic O
relationships O
like O
gender O
. O
More O
formally O
, O
given O
an O
analogy O
‚Äú O
ais O
tobascis O
to O
? O
, O
‚Äù O
we O
show O
, O
using O
multi O
- O
dimensional O
vector O
- O
valued O
functions O
( O
Larson O
and O
Edwards O
, O
2016 O
) O
, O
that O
if O
there O
is O
a O
high O
cosine O
similarity O
between O
aandc O
, O
the O
predicted O
answer O
will O
be O
a O
word O
similar O
to O
b. O
Suppose O
a O
function O
F O
: O
Rm‚ÜíRnhas O
component O
functions O
fi O
: O
Rm‚ÜíR O
, O
i‚àà{1 O
, O
. O
. O
. O
, O
n O
} O
, O
where O
F(‚àí O
‚Üíx O
) O
= O
( O
fi(‚àí O
‚Üíx))n O
i=1and‚àí O
‚Üíx= O
( O
xj)m O
j=1 O
. O
Then O
the O
limit O
of O
F O
, O
if O
it O
exists O
, O
can O
be O
found O
by O
taking O
the O
limit O
of O
each O
component O
function O
: O
lim‚àí O
‚Üíx‚Üí‚àí O
‚ÜíaF(‚àí O
‚Üíx O
) O
= O
/parenleftbigg O
lim‚àí O
‚Üíx‚Üí‚àí O
‚Üíafi(‚àí O
‚Üíx)/parenrightbiggn O
i=1 O
. O
For O
Ô¨Åxed O
vectors‚àí O
‚Üía O
, O
‚àí O
‚Üíb‚ààRn O
, O
letF O
: O
Rn‚ÜíRn O
, O
‚àí O
‚Üíx O
/ O
mapsto‚Üí‚àí O
‚Üíx‚àí‚àí O
‚Üía+‚àí O
‚Üíb O
. O
Fcan O
be O
expressed O
componentwise O
as O
F(‚àí O
‚Üíx O
) O
= O
( O
fi(‚àí O
‚Üíx))n O
i=1= O
( O
xi‚àíai+bi)n O
i=1 O
. O
Then O
as O
each O
component O
function O
is O
continuous O
: O
lim‚àí O
‚Üíx‚Üí‚àí O
‚ÜíaF(‚àí O
‚Üíx O
) O
= O
/parenleftbigg O
lim‚àí O
‚Üíx‚Üí‚àí O
‚Üía(xi‚àíai+bi)/parenrightbiggn O
i=1 O
= O
( O
ai‚àíai+bi)n O
i=1 O
= O
‚àí O
‚Üíb O
. O
Thus O
as‚àí O
‚Üíxapproaches‚àí O
‚Üía,‚àí O
‚Üíx‚àí‚àí O
‚Üía+‚àí O
‚Üíbapproaches‚àí O
‚Üíb O
. O
For O
embeddings O
, O
this O
means O
if‚àí O
‚Üíais O
sufÔ¨Åciently O
similar O
to‚àí O
‚Üíc O
, O
by O
Equation O
2.2 O
, O
we O
expect O
the O
predicted O
answer O
d‚àóto O
the O
analogy O
‚Äú O
ais O
tobascis O
to O
? O
‚Äù O
to O
be O
a O
word O
whose O
vector O
is O
similar O
to‚àí O
‚Üíb O
. O
This O
was O
demonstrated O
empirically O
in O
( O
Linzen O
, O
2016 O
) O
. O
Implications O
of O
the O
well O
- O
known O
analogy O
‚Äú O
man O
is O
to O
computer O
programmer O
aswoman O
is O
to766homemaker O
‚Äù O
should O
be O
reinterpreted O
in O
light O
of O
this O
insight O
. O
Previous O
interpretations O
took O
this O
analogy O
to O
be O
evidence O
of O
systematic O
gender O
bias O
in O
the O
embedding O
space O
( O
Bolukbasi O
et O
al O
. O
, O
2016 O
) O
. O
However O
, O
there O
is O
a O
very O
high O
cosine O
similarity O
between‚àí‚àí‚Üíman O
and‚àí‚àí‚àí‚àí‚àí‚Üíwoman O
( O
0.77)6 O
; O
in O
fact O
, O
each O
is O
the O
most O
similar O
word O
to O
the O
other O
in O
the O
embedding O
space O
. O
The O
vectors O
for O
computer O
programmer O
andhomemaker O
are O
also O
highly O
similar O
( O
0.50 O
) O
. O
The O
presence O
of O
homemaker O
can O
therefore O
be O
explained O
by O
its O
similarity O
to O
computer O
programmer O
rather O
than O
gender O
bias O
. O
Of O
course O
, O
embedding O
vector O
similarity O
does O
frequently O
indicate O
word O
relatedness O
( O
e.g. O
‚Äú O
king O
‚Äù O
and O
‚Äú O
queen O
‚Äù O
) O
. O
However O
, O
vector O
similarity O
may O
also O
be O
due O
to O
noise O
. O
As O
there O
is O
no O
obvious O
linguistic O
relationship O
between O
the O
words O
homemaker O
andcomputer O
programmer O
and O
neither O
are O
common O
words O
in O
the O
embedding O
vocabulary O
, O
we O
posit O
the O
latter O
is O
the O
case O
. O
This O
analogy O
has O
been O
taken O
as O
evidence O
of O
a O
gendered O
relationship O
between O
computer O
programmer O
and O
homemaker O
because O
it O
has O
been O
assumed O
that O
the O
principal O
relation O
between O
the O
vectors O
for O
man O
andwoman O
is O
gender O
, O
and O
that O
this O
relation O
carries O
over O
tocomputer O
programmer O
andhomemaker O
. O
This O
argument O
rests O
on O
the O
supposition O
that O
the O
difference O
vector‚àí‚àí‚Üíman‚àí‚àí‚àí‚àí‚àí‚àí‚Üíwoman O
encodes O
gender O
. O
However O
, O
embeddings O
were O
not O
designed O
to O
have O
such O
linear O
properties O
and O
their O
existence O
has O
been O
debated O
( O
Linzen O
, O
2016 O
) O
. O
Furthermore O
, O
the O
top O
solution O
for O
‚Äú O
man O
is O
to O
apple O
aswoman O
is O
to O
? O
‚Äù O
isapples O
, O
but O
the O
relationship O
between O
apple O
andapples O
is O
clearly O
pluralisation O
rather O
than O
gender O
. O
More O
generally O
, O
we O
took O
the O
commonly O
used O
Google O
Analogy O
Test O
Set O
( O
Mikolov O
et O
al O
. O
, O
2013a O
) O
which O
contains O
19,544 O
analogies O
( O
8,869 O
semantic O
and O
10,675 O
syntactic O
) O
split O
into O
14 O
categories O
, O
such O
as O
countries O
and O
their O
capitals O
. O
This O
set O
contains O
550 O
unique O
word O
pairs O
( O
x O
, O
y O
) O
( O
such O
as O
( O
apple O
, O
apples O
) O
) O
unrelated O
to O
gender.7In O
general O
, O
the O
two O
words O
in O
each O
of O
the O
550 O
word O
pairs O
are O
highly O
similar O
to O
each O
other O
, O
with O
mean O
cosine O
similarity O
0.62 O
and O
standard O
deviation O
0.13 O
. O
We O
tested O
the O
analogy O
‚Äú O
man O
is O
to O
xas O
woman O
is O
to O
? O
‚Äù O
using O
Equation O
2.2 O
. O
This O
resulted O
6By O
comparison O
, O
the O
mean O
cosine O
similarity O
for O
100,000 O
pairs O
of O
words O
randomly O
sampled O
from O
the O
embedding O
space O
was O
0.13 O
, O
with O
standard O
deviation O
0.11 O
. O
7The O
category O
‚Äú O
family O
‚Äù O
was O
excluded O
as O
there O
are O
gender O
relationships O
between O
the O
word O
pairs.in O
22 O
% O
being O
correctly O
solved O
( O
i.e. O
returning O
y O
) O
, O
including O
76 O
% O
correct O
in O
the O
‚Äú O
gram8 O
- O
plural O
‚Äù O
category O
, O
which O
contains O
pluralised O
words O
( O
note O
that O
the O
analogy O
not O
being O
solved O
correctly O
does O
not O
imply O
a O
dissimilar O
vector O
is O
being O
returned O
) O
. O
This O
demonstrates O
that O
‚Äú O
man O
is O
toxaswoman O
is O
to O
? O
‚Äù O
frequently O
solves O
analogies O
by O
returning O
words O
whose O
vectors O
are O
similar O
to‚àí O
‚Üíx O
, O
without O
any O
need O
for O
a O
linguistically O
gendered O
relationship O
between O
xand O
the O
returned O
word O
. O
These O
observations O
have O
further O
implications O
for O
the O
biased O
analogy O
generating O
method O
of O
Bolukbasi O
et O
al O
. O
( O
2016 O
) O
, O
which O
was O
extended O
in O
( O
Manzini O
et O
al O
. O
, O
2019 O
) O
. O
This O
method O
leveraged O
the O
base O
pair O
( O
she O
, O
he O
) O
to O
Ô¨Ånd O
word O
pairs O
( O
x O
, O
y O
) O
, O
such O
that O
‚Äú O
heis O
toxassheis O
toy O
‚Äù O
, O
where||‚àí O
‚Üíx‚àí‚àí O
‚Üíy||= O
1 O
. O
However O
, O
the O
condition O
||‚àí O
‚Üíx‚àí‚àí O
‚Üíy||= O
1is O
equivalent O
to O
cos(‚àí O
‚Üíx O
, O
‚àí O
‚Üíy O
) O
= O
1 O
2 O
. O
This O
forced O
similarity O
between O
xandycombined O
with O
the O
high O
similarity O
of O
she O
andhe(0.61 O
) O
means O
this O
method O
is O
simply O
returning O
word O
pairs O
with O
a O
high O
similarity O
. O
Alternative O
choices O
of O
gender O
base O
pair O
such O
as O
( O
woman O
, O
man O
) O
would O
suffer O
from O
the O
same O
Ô¨Çaw O
. O
Consequently O
, O
analogies O
produced O
using O
this O
method O
should O
be O
treated O
with O
caution O
. O
8 O
Conclusions O
There O
has O
been O
a O
recent O
focus O
in O
the O
NLP O
community O
on O
identifying O
bias O
in O
word O
embeddings O
. O
While O
we O
strongly O
support O
the O
aim O
of O
such O
work O
, O
this O
paper O
highlights O
the O
complexity O
of O
trying O
to O
quantify O
bias O
in O
embeddings O
. O
We O
showed O
the O
reliance O
of O
popular O
gender O
bias O
measures O
on O
gender O
base O
pairs O
has O
strong O
limitations O
. O
None O
of O
the O
measures O
are O
robust O
enough O
to O
reliably O
capture O
social O
bias O
in O
embeddings O
, O
or O
to O
be O
leveraged O
in O
debiasing O
methods O
. O
In O
addition O
, O
we O
showed O
the O
use O
of O
gender O
base O
pairs O
to O
generate O
‚Äú O
biased O
‚Äù O
analogies O
is O
Ô¨Çawed O
. O
Our O
analysis O
can O
contribute O
to O
future O
work O
designing O
robust O
bias O
measures O
and O
effective O
debiasing O
methods O
. O
Although O
this O
paper O
focused O
on O
gender O
bias O
, O
it O
is O
relevant O
to O
work O
examining O
other O
forms O
of O
bias O
, O
such O
as O
racial O
stereotyping O
, O
in O
embeddings O
. O
Code O
to O
replicate O
our O
experiments O
can O
be O
found O
at O
: O
https://github.com O
/ O
alisonsneyd O
/ O
Gender O
_ O
bias_word_embeddings O
Acknowledgements O
This O
work O
was O
supported O
by O
the O
Institute O
of O
Coding O
which O
received O
funding O
from O
the O
OfÔ¨Åce O
for O
Students O
( O
OfS O
) O
in O
the O
United O
Kingdom.767References O
Sandra O
L. O
Bem O
. O
1974 O
. O
The O
measurement O
of O
psychological O
androgyny O
. O
Journal O
of O
consulting O
and O
clinical O
psychology O
, O
42:155‚Äì62 O
. O
Tolga O
Bolukbasi O
, O
Kai O
- O
Wei O
Chang O
, O
James O
Zou O
, O
Venkatesh O
Saligrama O
, O
and O
Adam O
Kalai O
. O
2016 O
. O
Man O
is O
to O
computer O
programmer O
as O
woman O
is O
to O
homemaker O
? O
debiasing O
word O
embeddings O
. O
In O
Proceedings O
of O
the O
30th O
International O
Conference O
on O
Neural O
Information O
Processing O
Systems O
, O
NIPS‚Äô16 O
, O
pages O
4356‚Äì4364 O
, O
USA O
. O
Curran O
Associates O
Inc. O
Aylin O
Caliskan O
, O
Joanna O
J O
Bryson O
, O
and O
Arvind O
Narayanan O
. O
2017 O
. O
Semantics O
derived O
automatically O
from O
language O
corpora O
contain O
human O
- O
like O
biases O
. O
Science O
, O
356(6334):183‚Äì186 O
. O
Jacob O
Cohen O
. O
1960 O
. O
CoefÔ¨Åcient O
of O
agreement O
for O
nominal O
scales O
. O
Educational O
and O
Psychological O
Measurement O
, O
20(1):37‚Äî-46 O
. O
M. O
Dean O
and O
Charlotte O
Tate O
. O
2016 O
. O
Extending O
the O
legacy O
of O
sandra O
bem O
: O
Psychological O
androgyny O
as O
a O
touchstone O
conceptual O
advance O
for O
the O
study O
of O
gender O
in O
psychological O
science O
. O
Sex O
Roles O
, O
76 O
. O
Sunipa O
Dev O
and O
Jeff O
M. O
Phillips O
. O
2019 O
. O
Attenuating O
bias O
in O
word O
vectors O
. O
In O
The O
22nd O
International O
Conference O
on O
ArtiÔ¨Åcial O
Intelligence O
and O
Statistics O
, O
AISTATS O
2019 O
, O
16 O
- O
18 O
April O
2019 O
, O
Naha O
, O
Okinawa O
, O
Japan O
, O
pages O
879‚Äì887 O
. O
Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O
BERT O
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
understanding O
. O
In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
4171‚Äì4186 O
, O
Minneapolis O
, O
Minnesota O
. O
Association O
for O
Computational O
Linguistics O
. O
Aleksandr O
Drozd O
, O
Anna O
Gladkova O
, O
and O
Satoshi O
Matsuoka O
. O
2016 O
. O
Word O
embeddings O
, O
analogies O
, O
and O
machine O
learning O
: O
Beyond O
king O
- O
man O
+ O
woman O
= O
queen O
. O
In O
Proceedings O
of O
COLING O
2016 O
, O
the O
26th O
International O
Conference O
on O
Computational O
Linguistics O
: O
Technical O
Papers O
, O
pages O
3519‚Äì3530 O
, O
Osaka O
, O
Japan O
. O
The O
COLING O
2016 O
Organizing O
Committee O
. O
Kawin O
Ethayarajh O
, O
David O
Duvenaud O
, O
and O
Graeme O
Hirst O
. O
2018 O
. O
Towards O
understanding O
linear O
word O
analogies O
. O
CoRR O
, O
abs/1810.04882 O
. O
Kawin O
Ethayarajh O
, O
David O
Duvenaud O
, O
and O
Graeme O
Hirst O
. O
2019 O
. O
Understanding O
undesirable O
word O
embedding O
associations O
. O
In O
Proceedings O
of O
the O
57th O
Conference O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
1696‚Äì1705 O
. O
Association O
for O
Computational O
Linguistics O
. O
Joseph O
L. O
Fleiss O
. O
1971 O
. O
Measuring O
nominal O
scale O
agreement O
among O
many O
raters O
. O
Psychological O
Bulletin O
, O
76(5):378‚Äì382.Danielle O
Gaucher O
, O
Justin O
P O
Friesen O
, O
and O
Aaron O
C. O
Kay O
. O
2011 O
. O
Evidence O
that O
gendered O
wording O
in O
job O
advertisements O
exists O
and O
sustains O
gender O
inequality O
. O
Journal O
of O
personality O
and O
social O
psychology O
, O
101 O
1:109‚Äì28 O
. O
Anna O
Gladkova O
, O
Aleksandr O
Drozd O
, O
and O
Satoshi O
Matsuoka O
. O
2016 O
. O
Analogy O
- O
based O
detection O
of O
morphological O
and O
semantic O
relations O
with O
word O
embeddings O
: O
what O
works O
and O
what O
does O
n‚Äôt O
. O
In O
Proceedings O
of O
the O
NAACL O
Student O
Research O
Workshop O
, O
pages O
8 O
‚Äì O
15 O
, O
San O
Diego O
, O
California O
. O
Association O
for O
Computational O
Linguistics O
. O
Hila O
Gonen O
and O
Yoav O
Goldberg O
. O
2019 O
. O
Lipstick O
on O
a O
pig O
: O
Debiasing O
methods O
cover O
up O
systematic O
gender O
biases O
in O
word O
embeddings O
but O
do O
not O
remove O
them O
. O
InNAACL O
- O
HLT O
. O
Cheryl O
L O
Holt O
and O
Jon O
B O
Ellis O
. O
1998 O
. O
Assessing O
the O
current O
validity O
of O
the O
bem O
sex O
- O
role O
inventory O
. O
Sex O
roles O
, O
39(11 O
- O
12):929‚Äì941 O
. O
Masahiro O
Kaneko O
and O
Danushka O
Bollegala O
. O
2019 O
. O
Gender O
- O
preserving O
debiasing O
for O
pre O
- O
trained O
word O
embeddings O
. O
In O
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
1641‚Äì1650 O
, O
Florence O
, O
Italy O
. O
Association O
for O
Computational O
Linguistics O
. O
J. O
Richard O
Landis O
and O
Gary O
G. O
Koch O
. O
1977 O
. O
The O
measurement O
of O
observer O
agreement O
for O
categorical O
data O
. O
Biometrics O
, O
33(1):159‚Äì174 O
. O
R. O
Larson O
and O
B.H. O
Edwards O
. O
2016 O
. O
Calculus O
. O
Cengage O
Learning O
. O
Omer O
Levy O
and O
Yoav O
Goldberg O
. O
2014 O
. O
Linguistic O
regularities O
in O
sparse O
and O
explicit O
word O
representations O
. O
In O
Proceedings O
of O
the O
Eighteenth O
Conference O
on O
Computational O
Natural O
Language O
Learning O
, O
pages O
171‚Äì180 O
, O
Ann O
Arbor O
, O
Michigan O
. O
Association O
for O
Computational O
Linguistics O
. O
Tal O
Linzen O
. O
2016 O
. O
Issues O
in O
evaluating O
semantic O
spaces O
using O
word O
analogies O
. O
In O
Proceedings O
of O
the O
1st O
Workshop O
on O
Evaluating O
Vector O
- O
Space O
Representations O
for O
NLP O
, O
pages O
13‚Äì18 O
, O
Berlin O
, O
Germany O
. O
Association O
for O
Computational O
Linguistics O
. O
Thomas O
Manzini O
, O
Yao O
Chong O
Lim O
, O
Yulia O
Tsvetkov O
, O
and O
Alan O
W. O
Black O
. O
2019 O
. O
Black O
is O
to O
criminal O
as O
caucasian O
is O
to O
police O
: O
Detecting O
and O
removing O
multiclass O
bias O
in O
word O
embeddings O
. O
In O
NAACL O
- O
HLT O
. O
M. O
Pilar O
Matud O
, O
Marisela O
L O
¬¥ O
opez O
- O
Curbelo O
, O
and O
Demelza O
Fortes O
. O
2019 O
. O
Gender O
and O
psychological O
well O
- O
being O
. O
International O
Journal O
of O
Environmental O
Research O
and O
Public O
Health O
, O
16(19):3531 O
. O
Ninareh O
Mehrabi O
, O
Fred O
Morstatter O
, O
Nripsuta O
Saxena O
, O
Kristina O
Lerman O
, O
and O
Aram O
Galstyan O
. O
2019 O
. O
A O
survey O
on O
bias O
and O
fairness O
in O
machine O
learning.768Tomas O
Mikolov O
, O
Kai O
Chen O
, O
Gregory O
S. O
Corrado O
, O
and O
Jeffrey O
Dean O
. O
2013a O
. O
EfÔ¨Åcient O
estimation O
of O
word O
representations O
in O
vector O
space O
. O
CoRR O
, O
abs/1301.3781 O
. O
Tomas O
Mikolov O
, O
Ilya O
Sutskever O
, O
Kai O
Chen O
, O
Greg O
Corrado O
, O
and O
Jeffrey O
Dean O
. O
2013b O
. O
Distributed O
representations O
of O
words O
and O
phrases O
and O
their O
compositionality O
. O
In O
Proceedings O
of O
the O
26th O
International O
Conference O
on O
Neural O
Information O
Processing O
Systems O
- O
Volume O
2 O
, O
NIPS‚Äô13 O
, O
pages O
3111‚Äì3119 O
, O
USA O
. O
Curran O
Associates O
Inc. O
Tomas O
Mikolov O
, O
Wen O
- O
tau O
Yih O
, O
and O
Geoffrey O
Zweig O
. O
2013c O
. O
Linguistic O
regularities O
in O
continuous O
space O
word O
representations O
. O
In O
Proceedings O
of O
the O
2013 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
pages O
746‚Äì751 O
. O
Malvina O
Nissim O
, O
Rik O
van O
Noord O
, O
and O
Rob O
van O
der O
Goot O
. O
2019 O
. O
Fair O
is O
better O
than O
sensational O
: O
Man O
is O
to O
doctor O
as O
woman O
is O
to O
doctor O
. O
CoRR O
, O
abs/1905.09866 O
. O
Jeffrey O
Pennington O
, O
Richard O
Socher O
, O
and O
Christopher O
D. O
Manning O
. O
2014 O
. O
Glove O
: O
Global O
vectors O
for O
word O
representation O
. O
In O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
( O
EMNLP O
) O
, O
pages O
1532‚Äì1543 O
. O
Matthew O
Peters O
, O
Mark O
Neumann O
, O
Mohit O
Iyyer O
, O
Matt O
Gardner O
, O
Christopher O
Clark O
, O
Kenton O
Lee O
, O
and O
Luke O
Zettlemoyer O
. O
2018 O
. O
Deep O
contextualized O
word O
representations O
. O
In O
Proceedings O
of O
the O
2018 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
Papers O
) O
, O
pages O
2227‚Äì2237 O
, O
New O
Orleans O
, O
Louisiana O
. O
Association O
for O
Computational O
Linguistics O
. O
Christine O
Starr O
and O
Eileen O
Zurbriggen O
. O
2016 O
. O
Sandra O
bem O
‚Äôs O
gender O
schema O
theory O
after O
34 O
years O
: O
A O
review O
of O
its O
reach O
and O
impact O
. O
Sex O
Roles O
. O
Jieyu O
Zhao O
, O
Yichao O
Zhou O
, O
Zeyu O
Li O
, O
Wei O
Wang O
, O
and O
KaiWei O
Chang O
. O
2018 O
. O
Learning O
gender O
- O
neutral O
word O
embeddings O
. O
In O
Proceedings O
of O
the O
2018 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
4847‚Äì4853 O
, O
Brussels O
, O
Belgium O
. O
Association O
for O
Computational O
Linguistics O
. O
A O
Appendix O
BSRI O
Female O
Terms O
: O
affectionate O
, O
affectionately O
, O
cheerful O
, O
cheerfully O
, O
cheerfulness O
, O
childlike O
, O
compassionate O
, O
compassionately O
, O
feminine O
, O
femininely O
, O
gentle O
, O
gently O
, O
gullible O
, O
gullibility O
, O
gullibly O
, O
loyal O
, O
loyally O
, O
shy O
, O
shyly O
, O
shyness O
, O
sympathetic O
, O
sympathetically O
, O
tender O
, O
tenderly O
, O
tenderness O
, O
understanding O
, O
understandingly O
, O
warm O
, O
warmish O
, O
warmness O
, O
yielding O
BSRI O
Male O
Terms O
: O
aggressive O
, O
aggressively O
, O
aggressiveness O
, O
aggressivity O
, O
ambitious O
, O
ambitiously O
, O
ambitiousness O
, O
analytical O
, O
analytically O
, O
assertive O
, O
assertiveness O
, O
assertively O
, O
athletic O
, O
athleticism O
, O
athletically O
, O
competitive O
, O
competitiveness O
, O
competitively O
, O
dominant O
, O
dominantly O
, O
forceful O
, O
forcefulness O
, O
independent O
, O
independently O
, O
individualistic O
, O
masculine O
, O
selfsufÔ¨Åcient O
Female O
Animal O
Terms O
: O
bitch O
, O
cow O
, O
doe O
, O
duck O
, O
ewe O
, O
goose O
, O
hen O
, O
leopardess O
, O
lioness O
, O
mare O
, O
queen O
, O
sow O
, O
tigress O
Male O
Animal O
Terms O
: O
dog O
, O
bull O
, O
buck O
, O
drake O
, O
ram O
, O
gander O
, O
rooster O
, O
leopard O
, O
lion O
, O
stallion O
, O
drone O
, O
boar O
, O
tiger769Proceedings O
of O
the O
1st O
Conference O
of O
the O
Asia O
- O
PaciÔ¨Åc O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
10th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
770‚Äì780 O
December O
4 O
- O
7 O
, O
2020 O
. O
¬© O
2020 O
Association O
for O
Computational O
Linguistics O
ExpanRL O
: O
Hierarchical O
Reinforcement O
Learning O
for O
Course O
Concept O
Expansion O
in O
MOOCs O
Jifan O
Yu1 O
, O
Chenyu O
Wang1 O
, O
Gan O
Luo1 O
, O
Lei O
Hou1,2,3‚àó O
, O
Juanzi O
Li1,2,3 O
, O
Jie O
Tang1,2,3 O
, O
Minlie O
Huang1,2,3 O
, O
Zhiyuan O
Liu1,2,3 O
1Dept O
. O
of O
Computer O
SCi O
. O
& O
Tech O
. O
, O
Tsinghua O
University O
, O
China O
100084 O
2KIRC O
, O
Institute O
for O
ArtiÔ¨Åcial O
Intelligence O
, O
Tsinghua O
University O
, O
China O
100084 O
3Beijing O
National O
Research O
Center O
for O
Information O
Science O
and O
Technology O
, O
China O
100084 O
{ O
yujf18,luog18 O
} O
@mails.tsinghua.edu.cn O
{ O
houlei O
, O
lijuanzi O
, O
jietang O
, O
aihuang O
, O
liuzy O
} O
@tsinghua.edu.cn O
Abstract O
Within O
the O
prosperity O
of O
Massive O
Open O
Online O
Courses O
( O
MOOCs O
) O
, O
the O
education O
applications O
that O
automatically O
provide O
extracurricular O
knowledge O
for O
MOOC O
users O
have O
become O
rising O
research O
topics O
. O
However O
, O
MOOC O
courses O
‚Äô O
diversity O
and O
rapid O
updates O
make O
it O
more O
challenging O
to O
Ô¨Ånd O
suitable O
new O
knowledge O
for O
students O
. O
In O
this O
paper O
, O
we O
present O
ExpanRL O
, O
an O
end O
- O
to O
- O
end O
hierarchical O
reinforcement O
learning O
( O
HRL O
) O
model O
for O
concept O
expansion O
in O
MOOCs O
. O
Employing O
a O
two O
- O
level O
HRL O
mechanism O
of O
seed O
selection O
and O
concept O
expansion O
, O
ExpanRL O
is O
more O
feasible O
to O
adjust O
the O
expansion O
strategy O
to O
Ô¨Ånd O
new O
concepts O
based O
on O
the O
students O
‚Äô O
feedback O
on O
expansion O
results O
. O
Our O
experiments O
on O
nine O
novel O
datasets O
from O
real O
MOOCs O
show O
that O
ExpanRL O
achieves O
signiÔ¨Åcant O
improvements O
over O
existing O
methods O
and O
maintain O
competitive O
performance O
under O
different O
settings O
. O
1 O
Introduction O
The O
cognitive O
- O
driven O
theory O
has O
been O
widely O
used O
in O
practical O
teaching O
since O
Ausubel O
Ô¨Årstly O
proposed O
it O
in O
( O
Ausubel O
, O
1968 O
) O
, O
which O
suggests O
educators O
provide O
new O
knowledge O
for O
students O
to O
motivate O
their O
learning O
continuously O
. O
In O
fact O
, O
in O
addition O
to O
the O
concepts O
taught O
in O
course O
, O
many O
related O
concepts O
are O
also O
attractive O
and O
worthy O
of O
learning O
. O
As O
shown O
in O
Figure O
1 O
, O
when O
a O
student O
studies O
the O
concept O
LSTM O
in O
‚Äú O
Deep O
Learning O
‚Äù O
course O
from O
Coursera1 O
, O
many O
related O
concepts O
, O
including O
its O
prerequisite O
concepts O
( O
RNN O
) O
, O
related O
scientists O
( O
J¬®urgen O
Schmidhuber O
) O
and O
its O
related O
applications O
( O
Machine O
Translation O
) O
can O
also O
beneÔ¨Åt O
his O
/ O
her O
further O
study O
. O
In O
traditional O
classrooms O
, O
these O
concepts O
are O
often O
considerately O
introduced O
by O
teachers O
. O
‚àóCorresponding O
author O
. O
1https://www.coursera.org O
Figure O
1 O
: O
An O
example O
of O
course O
- O
related O
concepts O
in O
the O
‚Äú O
Deep O
Learning O
‚Äù O
course O
from O
Coursera O
. O
However O
, O
in O
the O
era O
of O
Massive O
Open O
Online O
Courses O
( O
MOOCs O
) O
, O
thousands O
of O
courses O
are O
prerecorded O
for O
with O
millions O
of O
students O
with O
various O
backgrounds O
( O
Shah O
, O
2019 O
) O
, O
which O
makes O
it O
infeasible O
to O
pick O
out O
these O
essential O
concepts O
manually O
. O
Therefore O
, O
there O
is O
a O
clear O
need O
to O
automatically O
discover O
course O
- O
related O
concepts O
so O
that O
they O
can O
easily O
acquire O
additional O
knowledge O
and O
achieve O
better O
educational O
outcomes O
. O
This O
task O
is O
formally O
deÔ¨Åned O
as O
Course O
Concept O
Expansion O
( O
Yu O
et O
al O
. O
, O
2019a O
) O
, O
a O
special O
type O
of O
Concept O
Expansion O
orSet O
Expansion O
( O
Wang O
and O
Cohen O
, O
2007 O
) O
, O
which O
refers O
to O
the O
task O
of O
expanding O
a O
small O
set O
of O
seed O
concepts O
into O
a O
complete O
set O
of O
concepts O
that O
belong O
to O
the O
same O
course O
or O
subject O
from O
external O
resources O
. O
Despite O
abundant O
efforts O
in O
related O
topics O
( O
He O
and O
Xin O
, O
2011 O
; O
Shen O
et O
al O
. O
, O
2017 O
; O
Yan O
et O
al O
. O
, O
2019 O
) O
, O
existing O
methods O
still O
face O
three O
challenges O
when O
applied O
to O
MOOCs O
. O
First O
, O
distinct O
from O
the O
task O
of O
enriching O
a O
certain O
concept O
set O
, O
the O
purpose O
of O
course O
concept O
expansion O
is O
to O
beneÔ¨Åt O
students O
‚Äô O
learning O
, O
making O
the O
context O
information O
insufÔ¨Åcient O
to O
detect O
whether O
a O
concept O
is O
appropriate O
to O
be O
an O
expansion O
result O
. O
How O
to O
properly O
introduce O
student O
feedback O
in O
the O
model O
‚Äôs O
loop O
is O
a O
crucial O
challenge O
. O
Second O
, O
unlike O
the O
set O
expansion O
for O
a O
clear O
general O
category O
( O
e.g. O
, O
countries O
) O
, O
courses O
are O
often O
the770combinations O
of O
multiple O
categories O
, O
especially O
in O
interdisciplinary O
courses O
like O
Mathematics O
for O
Computer O
Science2 O
. O
Therefore O
, O
it O
is O
n‚Äôt O
easy O
to O
model O
the O
course O
‚Äôs O
semantic O
scope O
( O
Curran O
et O
al O
. O
, O
2007 O
) O
when O
applying O
existing O
expansion O
methods O
. O
Third O
, O
MOOCs O
are O
updated O
continuously O
, O
and O
numerous O
new O
courses O
arise O
everyday O
( O
Shah O
, O
2019 O
) O
, O
which O
requires O
a O
good O
generalization O
ability O
of O
the O
expansion O
model O
; O
otherwise O
, O
the O
frequent O
model O
retraining O
will O
cause O
severe O
waste O
of O
resources O
. O
To O
address O
the O
above O
problems O
, O
we O
construct O
a O
novel O
interactive O
environment O
on O
real O
MOOCs O
, O
which O
collects O
students O
‚Äô O
feedback O
on O
expansion O
results O
and O
provides O
new O
knowledge O
for O
MOOC O
students O
in O
an O
interesting O
way O
for O
better O
education O
. O
And O
based O
on O
the O
feedback O
, O
we O
propose O
ExpanRL O
, O
a O
hierarchical O
reinforcement O
learning O
framework O
for O
course O
concept O
expansion O
in O
MOOCs O
, O
which O
decomposes O
the O
concept O
expansion O
task O
into O
a O
hierarchy O
of O
two O
subtasks O
: O
high O
- O
level O
seed O
selection O
and O
low O
- O
level O
expansion O
. O
Boosted O
by O
user O
feedback O
on O
expansion O
results O
, O
ExpanRL O
jointly O
learns O
how O
to O
select O
seed O
concepts O
to O
model O
the O
semantic O
scope O
of O
the O
course O
better O
, O
and O
whether O
a O
concept O
is O
beneÔ¨Åcial O
for O
students O
. O
Moreover O
, O
the O
hierarchical O
reinforcement O
learning O
( O
HRL O
) O
structure O
enables O
ExpanRL O
to O
learn O
proper O
expansion O
strategies O
instead O
of O
the O
modeling O
of O
a O
particular O
course O
, O
making O
our O
model O
keep O
a O
high O
performance O
even O
in O
unobserved O
courses O
. O
The O
evaluation O
is O
conducted O
on O
9datasets O
from O
real O
MOOC O
courses O
, O
compared O
with O
5representative O
baseline O
methods O
. O
We O
further O
conduct O
an O
online O
evaluation O
to O
investigate O
whether O
students O
admit O
the O
expanded O
concepts O
. O
Our O
contributions O
include O
1 O
) O
an O
investigation O
on O
how O
to O
involve O
HRL O
framework O
into O
the O
task O
of O
concept O
expansion O
; O
2 O
) O
a O
paradigm O
that O
connects O
the O
NLP O
concept O
expansion O
task O
with O
the O
educational O
application O
; O
3 O
) O
an O
interactive O
MOOC O
environment O
, O
consisting O
of O
9 O
novel O
datasets O
of O
different O
subjects O
, O
6,553 O
extracted O
course O
concepts O
, O
and O
495,324 O
user O
behaviors O
from O
a O
real O
MOOC O
website O
. O
2 O
Preliminaries O
2.1 O
Problem O
Formulation O
Following O
( O
Yu O
et O
al O
. O
, O
2019a O
) O
, O
Course O
Concept O
Expansion O
is O
formally O
deÔ¨Åned O
as O
: O
given O
the O
course O
corpusD O
, O
course O
conceptsM O
, O
and O
a O
knowledge O
2A O
course O
from O
the O
University O
of O
London O
in O
Coursera.baseKBas O
an O
external O
source O
, O
the O
task O
is O
to O
return O
a O
ranked O
list O
of O
expanded O
concepts O
Ec O
. O
In O
this O
formulation O
, O
a O
course O
corpus O
is O
deÔ¨Åned O
asD={Cj}|n| O
j=1 O
, O
which O
is O
composed O
of O
ncourses O
‚Äô O
video O
subtitles O
in O
the O
same O
subject O
area O
. O
Course O
concepts O
are O
the O
subjects O
taught O
in O
the O
course O
( O
such O
asLSTM O
in O
Figure O
1 O
) O
, O
denoted O
as O
M={ci}|M| O
i=1 O
. O
( O
Pan O
et O
al O
. O
, O
2017 O
) O
. O
Knowledge O
base O
KB= O
( O
E O
, O
R O
) O
is O
consist O
of O
concepts O
Eand O
relations O
R O
, O
which O
is O
utilized O
as O
an O
external O
source O
to O
obtain O
expansion O
candidates O
. O
Though O
other O
source O
( O
such O
as O
Web O
tables O
) O
can O
also O
take O
on O
this O
role O
, O
we O
still O
employ O
aKBto O
search O
for O
expansion O
candidates O
like O
the O
prior O
work O
, O
i.e. O
, O
Ec‚äÇE. O
2.2 O
Basic O
Model O
for O
Concept O
Expansion O
The O
general O
idea O
of O
concept O
expansion O
is O
Ô¨Årst O
to O
characterize O
the O
concept O
set O
according O
to O
its O
representative O
elements O
, O
then O
Ô¨Ånd O
new O
candidates O
and O
rank O
them O
to O
expand O
the O
set O
. O
Seed O
Selection O
Stage O
. O
A O
group O
of O
representative O
concepts O
are O
called O
seeds O
and O
formalized O
to O
K‚äÇ O
Ec(Wang O
and O
Cohen O
, O
2007 O
; O
Mamou O
et O
al O
. O
, O
2018 O
) O
. O
While O
the O
expansion O
process O
is O
often O
carried O
out O
iteratively O
, O
we O
also O
formalize O
the O
expansion O
set O
of O
roundttoEt O
c. O
Seed O
selection O
is O
to O
calculate O
the O
possibility O
that O
each O
concept O
in O
Et O
cbecomes O
a O
seed O
, O
i.e. O
,P(ci‚ààKt‚äÇEt O
c|t O
) O
, O
whereKtcontains O
the O
seeds O
oft O
- O
th O
round O
. O
Based O
on O
these O
seeds O
, O
we O
can O
extract O
features O
of O
the O
current O
set O
and O
search O
for O
candidate O
concepts O
for O
expansion O
from O
external O
sources O
. O
Expansion O
Stage O
. O
After O
Ô¨Ånding O
a O
new O
list O
of O
candidatesLt=/braceleftbig O
c1, O
... O
,ct O
/ O
prime, O
... O
,c|Lt|/bracerightbig O
, O
expansion O
stage O
aims O
to O
calculate O
the O
likelihood O
of O
ct O
/ O
primeto O
be O
a O
expanded O
concept O
. O
The O
top O
candidates O
ranked O
by O
ct O
/ O
primeare O
selected O
as O
new O
expanded O
concepts O
, O
denoted O
asNtthe O
likelihood O
can O
be O
formalized O
as O
P(ct O
/ O
prime‚ààNt‚äÇLt|Kt O
, O
t O
/ O
prime O
) O
. O
The O
expansion O
set O
is O
refreshed O
as O
Et+1 O
c O
= O
Et O
c‚à™ O
Ntuntil O
its O
size O
reaches O
the O
preset O
upper O
limit O
œÑor O
can O
not O
Ô¨Ånd O
new O
candidates O
( O
He O
and O
Xin O
, O
2011 O
) O
. O
2.3 O
Interactive O
MOOC O
Environment O
The O
workÔ¨Çow O
above O
has O
been O
experimentally O
proven O
to O
be O
effective O
in O
many O
concept O
expansion O
tasks O
( O
Shen O
et O
al O
. O
, O
2018 O
; O
Rastogi O
et O
al O
. O
, O
2019 O
) O
. O
However O
, O
such O
methods O
only O
consider O
the O
course O
concepts O
‚Äô O
semantic O
information O
, O
which O
makes O
their O
expansion O
results O
hard O
to O
match O
real O
learning O
needs O
, O
especially O
when O
dealing O
with O
the O
multi O
- O
category771MOOC O
courses O
. O
Meanwhile O
, O
since O
the O
models O
are O
trained O
before O
launching O
, O
how O
to O
maintain O
high O
performance O
on O
new O
arisen O
courses O
is O
challenging O
. O
Yu O
et O
al O
. O
( O
2019a O
) O
designs O
an O
online O
game O
in O
MOOCs O
to O
collect O
user O
feedback O
on O
the O
expansion O
result O
, O
thereby O
employing O
an O
active O
pipeline O
model O
to O
face O
the O
above O
problems O
, O
which O
provides O
an O
interactive O
MOOC O
environment O
for O
reinforcement O
learning O
models O
. O
However O
, O
the O
size O
of O
publicly O
published O
datasets O
( O
4 O
courses O
with O
800 O
concepts O
in O
each O
course O
) O
is O
still O
insufÔ¨Åcient O
to O
meet O
the O
need O
to O
train O
advanced O
deep O
learning O
models O
. O
Therefore O
, O
we O
extract O
68 O
real O
MOOC O
courses O
of O
six O
subjects O
and O
build O
a O
large O
- O
scale O
MOOC O
interactive O
environment O
, O
which O
contains O
a O
gameÔ¨Åed O
interface O
for O
feedback O
collection O
and O
several O
course O
datasets O
: O
‚Äú O
Mathematics O
‚Äù O
, O
‚Äú O
Chemistry O
‚Äù O
, O
‚Äú O
Architecture O
‚Äù O
, O
‚Äú O
Psychology O
‚Äù O
, O
‚Äú O
Material O
Science O
‚Äù O
and O
‚Äú O
Computer O
Science O
‚Äù O
, O
covering O
diverse O
subjects O
of O
natural O
science O
, O
social O
science O
and O
engineering O
. O
The O
details O
of O
the O
datasets O
are O
presented O
in O
the O
experiment O
section O
. O
We O
construct O
the O
environment O
through O
three O
stages O
. O
First O
, O
for O
each O
subject O
, O
we O
select O
its O
most O
relevant O
courses O
from O
a O
real O
MOOC O
website3 O
. O
We O
use O
the O
method O
of O
Pan O
( O
2017 O
) O
to O
extract O
the O
course O
concepts O
and O
manually O
select O
the O
high O
- O
quality O
ones O
as O
the O
course O
concepts O
M. O
Second O
, O
we O
take O
XLORE O
( O
Jin O
et O
al O
. O
, O
2019 O
) O
as O
KBto O
search O
for O
candidate O
expansion O
concepts O
. O
Figure O
2 O
: O
A O
demonstration O
of O
our O
interactive O
game O
in O
course O
Introduction O
to O
psychology O
. O
MOOC O
users O
can O
click O
irrelevant O
expansion O
candidates O
to O
get O
bonuses O
. O
The O
yellow O
concept O
on O
the O
left O
is O
from O
course O
, O
and O
the O
green O
concepts O
are O
expanded O
candidates O
. O
Finally O
, O
we O
set O
up O
a O
game O
to O
present O
the O
expansion O
candidates O
. O
As O
shown O
in O
Figure O
2 O
, O
real O
MOOC O
users O
are O
drawn O
to O
pick O
out O
the O
course O
- O
unrelated O
ones O
to O
get O
bonuses O
. O
To O
ensure O
data O
quality O
, O
we O
set O
the O
game O
bonus O
depending O
on O
the O
group O
voting O
3Anonymous O
for O
blind O
review.result O
. O
We O
also O
avoid O
their O
irresponsible O
operations O
by O
mixing O
some O
extracted O
course O
concepts O
among O
candidates O
to O
detect O
the O
spoilers O
. O
The O
operation O
records O
are O
employed O
to O
train O
our O
reinforcement O
learning O
model O
proposed O
in O
the O
next O
section O
. O
3 O
The O
Proposed O
Model O
In O
this O
section O
, O
we O
Ô¨Årst O
introduce O
our O
hierarchical O
reinforcement O
concept O
expansion O
framework O
, O
ExpanRL O
, O
then O
present O
our O
high O
- O
level O
seed O
selection O
model O
and O
low O
- O
level O
expansion O
model O
separately O
. O
Figure O
3 O
: O
Framework O
of O
ExpanRL O
. O
3.1 O
Overview O
To O
obtain O
high O
- O
quality O
expanded O
course O
concepts O
for O
serving O
students O
in O
MOOCs O
, O
ExpanRL O
still O
needs O
to O
address O
three O
crucial O
problems O
. O
1 O
. O
How O
to O
properly O
utilize O
user O
feedback O
? O
2 O
. O
How O
to O
keep O
accurate O
modeling O
of O
the O
course O
during O
iterations O
? O
3 O
. O
How O
to O
keep O
a O
good O
generalization O
ability O
of O
the O
model O
when O
expanding O
in O
new O
MOOC O
courses O
? O
Thanks O
to O
the O
interactive O
MOOC O
environment O
, O
we O
can O
deal O
with O
these O
issues O
by O
decomposing O
the O
basic O
concept O
expansion O
workÔ¨Çow O
into O
a O
hierarchical O
reinforcement O
learning O
framework O
. O
Figure O
3 O
shows O
that O
the O
model O
can O
learn O
the O
complex O
connection O
between O
concepts O
and O
courses O
from O
user O
feedback O
instead O
of O
simple O
contextual O
information O
. O
The O
main O
idea O
of O
ExpanRL O
is O
to O
upgrade O
expanding O
strategies O
via O
such O
an O
end O
- O
to O
- O
end O
model O
, O
whose O
entire O
expansion O
process O
works O
as O
the O
basic O
concept O
expansion O
methods O
in O
Section O
2.2 O
, O
which O
can O
be O
naturally O
formulated O
as O
a O
semi O
- O
Markov O
decision O
process O
( O
Sutton O
et O
al O
. O
, O
1999 O
) O
like O
: O
1 O
) O
a O
high O
- O
level O
RL O
process O
that O
selects O
seeds O
fromEt O
cto O
search O
for O
a O
list O
of O
candidates O
Lt O
; O
2 O
) O
a O
low O
- O
level O
RL O
process O
that O
detect O
the O
high O
- O
quality O
expansion O
results O
among O
candidates O
and O
obtain O
Ntto O
refresh O
the O
set772toEt+1 O
c. O
This O
process O
iterate O
until O
the O
size O
of O
the O
expansion O
set O
reaches O
the O
preset O
limit O
, O
œÑ O
. O
Specially O
, O
before O
the O
whole O
process O
, O
we O
Ô¨Årst O
utilize O
the O
method O
in O
( O
Pan O
et O
al O
. O
, O
2017 O
) O
to O
extract O
course O
conceptsMfrom O
the O
given O
course O
corpus O
Dand O
initialize O
E0 O
c O
= O
M. O
3.2 O
Seed O
Selection O
with O
High O
- O
level O
RL O
The O
high O
- O
level O
RL O
policy O
¬µaims O
to O
select O
kseeds O
from O
the O
existing O
set O
Ec O
, O
which O
can O
be O
regarded O
as O
a O
conventional O
RL O
over O
options O
. O
An O
option O
refers O
to O
a O
high O
- O
level O
action O
, O
and O
a O
low O
- O
level O
RL O
will O
be O
launched O
once O
the O
agent O
executes O
an O
option O
. O
The O
high O
- O
level O
time O
step O
tis O
the O
expansion O
round O
. O
Option O
: O
The O
option O
otis O
a O
vector O
consisting O
of O
0and1 O
, O
which O
represents O
the O
i O
- O
th O
concepts O
from O
expansion O
set O
Et O
cis O
or O
is O
not O
a O
selected O
seed O
for O
the O
current O
expansion O
round O
. O
Thus O
the O
dimension O
of O
otis O
the O
same O
as O
the O
size O
of O
Et O
c. O
When O
a O
low O
- O
level O
RL O
process O
enters O
a O
Ô¨Ånal O
state O
, O
the O
agent O
‚Äôs O
control O
will O
be O
taken O
over O
to O
the O
high O
- O
level O
RL O
process O
to O
execute O
the O
next O
options O
. O
State O
: O
The O
state O
sh O
t‚àà O
Shof O
the O
high O
level O
RL O
process O
at O
time O
step O
t O
, O
is O
represented O
by O
a O
k√óC O
matrix O
reshaped O
from O
the O
hidden O
state O
ht O
, O
where O
kis O
the O
size O
of O
seed O
set O
and O
Cis O
the O
size O
of O
a O
compressed O
word O
embedding O
. O
sh O
t O
= O
reshape O
( O
ht O
) O
( O
1 O
) O
To O
obtain O
the O
hidden O
state O
ht O
, O
we O
introduce O
a O
set O
representation O
RepSet O
( O
Skianis O
et O
al O
. O
, O
2019 O
) O
to O
encode O
the O
current O
expansion O
set O
Et O
c. O
RepSet O
is O
unsupervised O
, O
order O
independent O
and O
can O
encode O
an O
n√óVmatrix O
to O
aVdimension O
vector O
. O
Note O
that O
Et‚àí1 O
c‚äÇEt O
c O
, O
so O
the O
current O
state O
is O
effected O
by O
the O
last O
state O
ht‚àí1 O
. O
ht O
= O
RepSet O
( O
Et O
c O
) O
. O
( O
2 O
) O
Policy O
: O
The O
stochastic O
policy O
for O
seed O
selection O
¬µ O
: O
S‚ÜíO O
which O
speciÔ¨Åes O
a O
probability O
distribution O
over O
options O
: O
ot‚àº¬µ(ot|sh O
t O
) O
= O
Rt O
= O
softmax O
( O
sh O
tW(Et O
c)T O
) O
. O
( O
3 O
) O
where O
Wis O
a O
learnable O
parameter O
, O
which O
compresses O
aVlength O
word O
embedding O
to O
a O
Clength O
word O
embedding O
. O
Et O
cis O
the O
matrix O
which O
consists O
of O
all O
course O
concepts O
‚Äô O
word O
vector O
. O
Rtis O
a O
matrix O
, O
while O
Rt O
j O
, O
iindicates O
the O
possibility O
of O
the O
i O
- O
thconcept O
inEt O
cto O
be O
thej O
- O
th O
seed O
: O
p(Kt O
j O
= O
ci O
, O
ci‚ààEt O
c|t O
) O
= O
/braceleftbigg O
Rt O
j O
, O
i O
0 O
ifciis O
selected O
before O
. O
( O
4 O
) O
And O
the O
possibility O
of O
the O
high O
- O
level O
RL O
to O
select O
Ktis O
shown O
below O
. O
Note O
that O
this O
possibility O
pis O
independent O
of O
i. O
ph(Kt O
) O
= O
k O
/ O
productdisplay O
j=1p(Kt O
j O
= O
ci O
, O
ci‚ààEt O
c|t O
) O
( O
5 O
) O
Reward O
: O
Then O
, O
the O
environment O
provides O
intermediate O
reward O
rh O
tto O
estimate O
the O
future O
return O
when O
executingot O
. O
The O
reward O
is O
given O
by O
the O
total O
reward O
of O
the O
last O
round O
of O
concept O
expansion O
. O
rh O
t=/summationdisplay O
rl O
t O
/ O
prime(ot O
) O
, O
( O
6 O
) O
whererl O
t O
/ O
prime(ot)is O
the O
low O
- O
level O
reward O
in O
time O
t O
/ O
prime O
while O
the O
high O
- O
level O
option O
is O
ot O
. O
Candidate O
generation O
after O
high O
- O
level O
options O
: O
After O
the O
agent O
gives O
out O
an O
option O
ot O
, O
we O
link O
the O
seed O
concepts O
from O
KtintoKBand O
Ô¨Ånd O
their O
Ô¨Årstorder O
neighbor O
concepts O
as O
the O
candidate O
list O
Lt O
. O
Note O
thatLtis O
sorted O
using O
the O
pairwise O
similarity O
between O
newly O
found O
candidates O
and O
seeds O
. O
3.3 O
Concept O
Expansion O
with O
Low O
- O
level O
RL O
Once O
the O
high O
- O
level O
policy O
has O
selected O
the O
seed O
set O
and O
generated O
a O
candidate O
list O
Lt O
, O
the O
low O
- O
level O
policyœÄwill O
scan O
the O
list O
and O
select O
high O
- O
quality O
expansion O
concepts O
from O
it O
to O
update O
Ec O
. O
The O
lowlevel O
policy O
over O
actions O
is O
formulated O
very O
similarly O
as O
the O
high O
- O
level O
policy O
over O
options O
. O
The O
optionotandKtfrom O
the O
high O
- O
level O
RL O
is O
taken O
as O
additional O
input O
throughout O
the O
low O
- O
level O
expansion O
process O
. O
The O
time O
step O
t O
/ O
primein O
low O
- O
level O
means O
thet O
/ O
prime O
- O
th O
candidate O
inLtand O
the O
Ô¨Ånal O
expanded O
concepts O
in O
this O
round O
is O
Nt O
. O
Action O
: O
The O
action O
at O
each O
time O
step O
is O
to O
assign O
a O
tag O
to O
the O
current O
candidate O
concept O
. O
The O
action O
space O
, O
i.e. O
,A={1,0 O
} O
, O
where O
1represents O
the O
present O
concept O
is O
an O
expansion O
result O
of O
this O
set O
, O
0represents O
that O
the O
concept O
is O
not O
an O
expansion O
result O
. O
State O
: O
The O
low O
- O
level O
intra O
- O
option O
state O
sl O
tis O
represented O
by O
the O
word O
embedding O
of O
current O
expansion O
candidatect O
/ O
prime O
. O
sl O
t O
/ O
prime O
= O
ct O
/ O
prime O
( O
7 O
) O
Moreover O
, O
we O
use O
a O
Bi O
- O
LSTM O
( O
Huang O
et O
al O
. O
, O
2015 O
) O
to O
provide O
a O
hidden O
state O
of O
current O
candidate O
list773hl O
tby O
encoding O
: O
1 O
) O
the O
selected O
seeds O
Kt O
, O
2 O
) O
a O
zero O
vector O
as O
a O
segmentation O
, O
3 O
) O
the O
candidate O
list O
Lt O
, O
thereby O
utilizing O
the O
information O
of O
high O
- O
level O
optionotto O
help O
low O
- O
level O
decisions O
. O
hl O
t O
= O
BiLSTM O
( O
/bracketleftbig O
Kt;0;Lt O
/ O
bracketrightbig O
) O
( O
8) O
Policy O
: O
The O
stochastic O
policy O
for O
expansion O
œÄ O
: O
S‚ÜíA O
outputs O
an O
action O
distribution O
given O
intraoption O
state O
sl O
tand O
the O
high O
- O
level O
option O
ot O
/ O
primethat O
launches O
the O
current O
subtask O
. O
Here O
‚äôis O
the O
vector O
dot O
product O
. O
at O
/ O
prime‚àºœÄ(at O
/ O
prime|sl O
t;ot O
) O
= O
pl(ct O
/ O
prime O
) O
= O
p(ct O
/ O
prime‚ààNt|t O
/ O
prime O
) O
= O
sigmoid O
( O
hl O
t‚äôst O
/ O
prime O
) O
, O
( O
9 O
) O
Reward O
: O
As O
introduced O
in O
section O
of O
Preliminaries O
, O
we O
construct O
an O
interactive O
game O
on O
the O
MOOC O
website O
to O
collect O
feedback O
from O
users O
on O
the O
expanded O
concepts O
. O
Users O
can O
pick O
out O
the O
unrelated O
concepts O
of O
the O
course O
, O
and O
the O
picked O
times O
of O
each O
expansion O
result O
ciis O
recorded O
as O
œï(ci O
) O
. O
Since O
such O
operations O
indicate O
the O
users O
‚Äô O
disagreements O
of O
the O
result O
, O
the O
low O
- O
level O
reward O
is O
designed O
to O
be O
negatively O
correlated O
with O
œï(ci)as O
follows O
: O
rl O
t O
/ O
prime=/braceleftbigg‚àíœï(ci)/maxcj‚ààLt(œï(cj)),at‚Äò=1 O
œï(ci)/maxcj‚ààLt(œï(cj)),at O
/ O
prime=0(10 O
) O
The O
count O
of O
user O
clicks O
determines O
the O
degree O
of O
relevance O
of O
each O
candidate O
to O
the O
course O
. O
It O
is O
worth O
noting O
that O
this O
degree O
is O
dynamic O
and O
depends O
on O
the O
concept O
that O
is O
mostly O
picked O
. O
This O
setting O
effectively O
controls O
the O
range O
of O
rewards O
. O
Set O
refreshment O
after O
low O
- O
level O
actions O
: O
After O
the O
agent O
gives O
out O
an O
action O
at O
/ O
prime O
, O
we O
can O
Ô¨Ånally O
obtain O
the O
new O
expanded O
concepts O
Nt O
. O
The O
expansion O
set O
is O
updated O
as O
Et+1 O
c O
= O
Et O
c‚à™Ntand O
the O
process O
turn O
to O
another O
round O
. O
3.4 O
Hierarchical O
Policy O
Learning O
To O
optimize O
the O
high O
level O
policy O
, O
we O
aim O
to O
maximize O
the O
expected O
cumulative O
rewards O
from O
the O
main O
task O
at O
each O
step O
tas O
the O
agent O
samples O
trajectories O
following O
the O
high O
- O
level O
policy O
¬µ O
, O
which O
can O
be O
computed O
as O
follows O
: O
J(Œ∏¬µ,t O
) O
= O
Esh O
, O
o O
, O
rh‚àº¬µ(o|sh)[T O
/ O
summationdisplay O
t=0logph(Kt)T O
/ O
summationdisplay O
s O
= O
tŒ≥s‚àítrh O
s O
] O
, O
( O
11 O
) O
where¬µis O
parameterized O
by O
Œ∏¬µ,Œ≥is O
a O
discount O
factor O
in O
RL O
, O
and O
the O
whole O
sampling O
process O
¬µ O
takes O
T O
time O
steps O
before O
it O
terminates O
. O
Algorithm O
1 O
: O
Training O
Procedure O
of O
HRL O
1Extract O
course O
concepts O
from O
Dand O
initiate O
E0 O
c O
= O
M O
; O
2Initiate O
state O
sh O
0‚Üê0and O
time O
step O
t‚Üê0 O
; O
3while|Ec|<œÉdo O
4 O
Calculate O
sh O
tby O
Eq.(1 O
) O
; O
5 O
Sampleotfromsh O
tby O
Eq.(3 O
) O
; O
6 O
Search O
for O
candidates O
from O
KBand O
generate O
a O
ranked O
candidate O
list O
L O
; O
7 O
forj‚Üê1to|L|do O
8t O
/ O
prime‚Üêt O
/ O
prime+ O
1 O
; O
9 O
Calculate O
sl O
t O
/ O
primeby O
Eq.(7 O
) O
; O
10 O
Sampleal O
t O
/ O
primefromsl O
t O
/ O
primeby O
Eq.(9 O
) O
; O
11 O
Add O
the O
expansion O
result O
into O
game O
and O
get O
feedback O
; O
12 O
Obtain O
low O
- O
level O
reward O
rl O
t O
/ O
primeby O
Eq.(10 O
) O
; O
13 O
end O
14t‚Üêt+ O
1 O
, O
refreshEc O
; O
15 O
Obtain O
low O
- O
level O
Ô¨Ånal O
reward O
rl O
fin O
, O
high O
- O
level O
rewardrh O
t O
; O
16end O
17Obtain O
high O
- O
level O
Ô¨Ånal O
reward O
rh O
finby O
Eq.(6 O
) O
; O
18Optimize O
the O
model O
with O
Eq.(11 O
) O
and O
Eq.(12 O
) O
; O
Similarly O
, O
we O
learn O
the O
low O
- O
level O
policy O
by O
maximizing O
the O
expected O
cumulative O
intra O
- O
option O
rewards O
from O
the O
sub O
task O
over O
option O
otwhen O
the O
agent O
samples O
along O
low O
- O
level O
policy O
œÄ(¬∑|ot)at O
time O
step O
t O
: O
J(Œ∏œÄ O
, O
t;ot O
/ O
prime O
) O
= O
Esl O
, O
a O
, O
rl‚àºœÄ(a|sl;ot O
/ O
prime)[T O
/ O
prime O
/ O
summationdisplay O
t O
/ O
prime=0logpl(ct O
/ O
prime)T O
/ O
prime O
/ O
summationdisplay O
s O
= O
t O
/ O
primeŒ≥s‚àítrl O
s O
] O
, O
( O
12 O
) O
if O
the O
subtask O
ends O
at O
time O
step O
T O
/ O
prime O
. O
Then O
we O
use O
policy O
gradient O
methods O
( O
Sutton O
et O
al O
. O
, O
2000 O
) O
with O
the O
REINFORCE O
( O
Williams O
, O
1992 O
) O
algorithm O
to O
optimize O
both O
high O
- O
level O
and O
low O
- O
level O
policies O
. O
The O
entire O
training O
process O
is O
described O
at O
Algorithm O
1 O
. O
4 O
Experiments O
4.1 O
Experiment O
Setting O
4.1.1 O
Datasets O
We O
construct O
an O
interactive O
MOOC O
environment O
as O
Section O
2.3 O
to O
collect O
user O
feedback O
on O
expansion O
results O
. O
To O
build O
a O
solid O
evaluation O
, O
we O
randomly O
selected O
5 O
% O
expanded O
concepts O
to O
be O
manually O
labeled O
benchmarks O
. O
For O
each O
concept O
, O
three O
annotators O
majoring O
in O
the O
corresponding O
domain O
are O
asked O
to O
label O
them O
as O
‚Äú O
0 O
: O
Not O
helpful O
‚Äù O
or O
‚Äú O
1 O
: O
Helpful O
‚Äù O
based O
on O
their O
knowledge O
. O
Thus O
, O
each O
dataset O
is O
triply O
annotated O
, O
and O
Pearson O
correlation O
coefÔ¨Åcient O
is O
computed O
to O
assess O
the O
inter O
- O
annotator O
agreement O
. O
A O
candidate O
is O
labeled O
as O
a O
related O
concept O
when O
more O
than O
two O
annotators O
give O
positive774MAT O
CHEM O
PSY O
MS O
ARC O
CS O
MAT+CS O
CHEM+MS O
MS+ARC O
# O
courses O
12 O
6 O
16 O
8 O
14 O
12 O
4 O
4 O
5 O
|M| O
1,688 O
1,404 O
568 O
842 O
1,036 O
1,015 O
230 O
417 O
382 O
# O
operations O
93,762 O
103,652 O
48,492 O
40,254 O
120,384 O
88,779 O
33,521 O
52,467 O
56,787 O
0 O
- O
Label O
24,278 O
15,796 O
13,245 O
11,876 O
33,127 O
17,775 O
7,092 O
9,367 O
7,898 O
1 O
- O
Label O
6,976 O
18,755 O
2,919 O
1,542 O
7,001 O
11,818 O
3,533 O
4,790 O
1,229 O
correlation O
0.712 O
0.694 O
0.705 O
0.732 O
0.678 O
0.689 O
0.655 O
0.688 O
0.701 O
Table O
1 O
: O
Statistics O
of O
datasets O
tags O
. O
Table O
1 O
presents O
the O
detailed O
statistics O
, O
where O
# O
courses O
, O
|M|,1 O
- O
Label O
and0 O
- O
Label O
are O
the O
number O
of O
courses O
, O
course O
concepts O
, O
positive O
and O
negative O
labels O
. O
# O
operations O
are O
user O
click O
times O
which O
is O
obtained O
from O
the O
game O
. O
MAT O
, O
CHEM O
, O
PSY O
, O
MS O
, O
ARC O
andCScorrespond O
to O
Mathematics O
, O
Chemistry O
, O
Psychology O
, O
Material O
Science O
, O
Architecture O
and O
Computer O
Science O
. O
In O
particular O
, O
we O
select O
13 O
interdisciplinary O
courses4and O
build O
three O
multi O
- O
category O
course O
datasets O
as O
MAT+CS O
, O
CHEM+MS O
andMS+ARC O
to O
further O
estimate O
the O
performance O
of O
ExpanRL O
on O
interdisciplinary O
courses O
. O
Note O
that O
these O
three O
datasets O
are O
subsets O
of O
the O
above O
six O
‚Äôs O
. O
Dataset O
Usage O
. O
All O
the O
models O
are O
trained O
on O
the O
user O
operation O
data O
and O
evaluated O
on O
the O
expert O
annotated O
data O
. O
For O
the O
supervised O
learning O
baselines O
, O
we O
set O
the O
concepts O
with O
top O
70 O
% O
click O
records O
as O
negative O
, O
and O
the O
rest O
as O
positive O
samples O
. O
4.1.2 O
Basic O
Settings O
All O
hyper O
- O
parameters O
are O
tuned O
on O
the O
validation O
set O
. O
The O
dimension O
of O
word O
vectors O
in O
Eq O
. O
( O
2 O
) O
is O
768 O
. O
The O
dimension O
of O
the O
compressed O
word O
vector O
C O
in O
Eq O
. O
( O
1 O
) O
is O
128 O
. O
The O
word O
vectors O
of O
all O
baseline O
methods O
are O
initialized O
using O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O
The O
learning O
rate O
is O
1.0√ó10‚àí4for O
low O
- O
level O
RL O
, O
and O
1.0√ó10‚àí5for O
high O
- O
level O
. O
The O
discount O
factorŒ≥is O
0.99 O
. O
The O
seed O
size O
kis O
set O
to O
10 O
and O
the O
upper O
limitœÑofEcis O
20,000 O
. O
4.1.3 O
Baselines O
We O
compare O
our O
hierarchical O
RL O
model O
( O
denoted O
as O
HRL O
) O
with O
Ô¨Åve O
typical O
methods O
of O
set O
expansion O
. O
As O
these O
methods O
obtain O
expansion O
candidates O
from O
diverse O
resources O
, O
we O
mainly O
employ O
the O
different O
similarity O
metrics O
to O
rank O
the O
same O
expansion O
candidate O
list O
for O
evaluation O
. O
Especially O
to O
investigate O
the O
impact O
of O
seed O
selection O
strategies O
, O
we O
use O
a O
K O
- O
means O
clustering O
- O
based O
method O
and O
a O
pairwise O
similarity O
- O
based O
method O
to O
replace O
the O
high O
- O
level O
RL O
network O
, O
which O
are O
denoted O
as O
C O
- O
RL O
and O
P O
- O
RL O
. O
4Course O
list O
is O
shown O
in O
Appendix.‚Ä¢PR O
. O
Graph O
based O
method O
: O
We O
build O
the O
candidates O
and O
course O
concepts O
into O
a O
graph O
. O
When O
the O
similarity O
between O
two O
concepts O
exceeds O
a O
threshold5œÉPR O
, O
there O
is O
a O
link O
between O
them O
. O
The O
PageRank O
score O
of O
each O
candidate O
is O
Ô¨Ånally O
used O
for O
sorting O
. O
A O
most O
famous O
method O
employing O
graph O
based O
ranking O
is O
SEAL O
( O
Wang O
and O
Cohen O
, O
2007 O
) O
‚Ä¢SEISA O
. O
SEISA O
( O
He O
and O
Xin O
, O
2011 O
) O
is O
an O
entity O
set O
expansion O
system O
developed O
by O
Microsoft O
after O
SEAL O
and O
outperforms O
traditional O
graph O
- O
based O
methods O
by O
an O
original O
unsupervised O
similarity O
metric O
. O
We O
implement O
its O
Dynamic O
Thresholding O
algorithm O
to O
sort O
expanded O
concepts O
. O
‚Ä¢EMB O
. O
Embedding O
based O
method O
mainly O
utilizes O
context O
information O
to O
examine O
the O
similarity O
between O
expanded O
concepts O
and O
seeds O
according O
to O
( O
Mamou O
et O
al O
. O
, O
2018 O
) O
. O
For O
each O
expanded O
concept O
e O
, O
we O
calculate O
the O
sum O
of O
its O
cosine O
similarities O
with O
course O
concepts O
Min O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
and O
use O
the O
average O
as O
golden O
standard O
to O
rank O
the O
expanded O
concept O
list O
. O
‚Ä¢PUL O
. O
PU O
learning O
is O
a O
semi O
- O
supervised O
learning O
model O
regarding O
set O
expansion O
as O
a O
binary O
classiÔ¨Åcation O
task O
. O
We O
employ O
the O
same O
setting O
as O
( O
Wang O
et O
al O
. O
, O
2017 O
) O
to O
classify O
and O
sort O
concepts O
. O
‚Ä¢PIP O
. O
It O
is O
a O
pipeline O
method O
for O
course O
concept O
expansion O
( O
Yu O
et O
al O
. O
, O
2019a O
) O
, O
which O
Ô¨Årst O
uses O
an O
online O
clustering O
method O
during O
candidate O
generation O
and O
then O
classify O
them O
to O
obtain O
Ô¨Ånal O
expansion O
results O
. O
We O
follow O
the O
workÔ¨Çow O
of O
this O
work O
to O
sort O
expanded O
concepts O
. O
4.1.4 O
Evaluation O
Metrics O
Our O
objective O
is O
to O
generate O
a O
ranked O
list O
of O
expanded O
concepts O
. O
Thus O
, O
we O
use O
the O
Mean O
Average O
Precision O
( O
MAP O
) O
as O
our O
evaluation O
metric O
, O
which O
is O
the O
preferred O
metric O
in O
information O
retrieval O
for O
evaluating O
ranked O
lists O
. O
4.2 O
Overall O
Evaluation O
Table O
2 O
summarizes O
the O
comparing O
results O
of O
different O
methods O
on O
all O
datasets O
. O
The O
evaluation O
is O
5œÉPRis O
experimentally O
set O
to O
0.5.775MAT O
CHEM O
PSY O
MS O
ARC O
CS O
Avg O
MAT+CS O
CHEM+MS O
MS+ARC O
I O
- O
Avg O
PR O
0.763 O
0.705 O
0.482 O
0.470 O
0.300 O
0.690 O
0.568 O
0.659 O
0.664 O
0.401 O
0.575 O
SEISA O
0.805 O
0.711 O
0.473 O
0.524 O
0.570 O
0.713 O
0.632 O
0.797 O
0.691 O
0.377 O
0.622 O
EMB O
0.747 O
0.687 O
0.474 O
0.533 O
0.442 O
0.812 O
0.616 O
0.710 O
0.655 O
0.377 O
0.581 O
PUL O
0.878 O
0.811 O
0.845 O
0.745 O
0.757 O
0.850 O
0.822 O
0.880 O
0.782 O
0.646 O
0.769 O
PIP O
0.848 O
0.782 O
0.803 O
0.772 O
0.775 O
0.821 O
0.800 O
0.893 O
0.835 O
0.851 O
0.865 O
C O
- O
RL O
0.902 O
0.795 O
0.818 O
0.753 O
0.716 O
0.800 O
0.797 O
0.851 O
0.849 O
0.758 O
0.820 O
P O
- O
RL O
0.892 O
0.768 O
0.606 O
0.749 O
0.821 O
0.767 O
0.835 O
0.871 O
0.852 O
0.662 O
0.795 O
HRL O
0.903 O
0.857 O
0.901 O
0.806 O
0.828 O
0.878 O
0.862 O
0.909 O
0.903 O
0.886 O
0.898 O
Table O
2 O
: O
MAP O
of O
different O
methods O
on O
datasets O
. O
( O
Seed O
set O
size O
= O
10 O
) O
divided O
into O
two O
parts O
. O
The O
six O
datasets O
on O
the O
left O
are O
the O
performance O
of O
the O
model O
on O
various O
subjects O
, O
and O
Avg O
represents O
the O
average O
of O
their O
MAPs O
. O
The O
three O
datasets O
on O
the O
right O
are O
from O
the O
selected O
interdisciplinary O
courses O
, O
and O
I O
- O
Avg O
is O
the O
average O
of O
the O
model O
performance O
on O
them O
. O
We O
also O
divide O
the O
methods O
into O
unsupervised O
, O
supervised O
, O
and O
reinforcement O
learning O
models O
for O
further O
analysis O
. O
Overall O
, O
our O
approach O
HRL O
maintains O
an O
impressive O
performance O
( O
at O
0.862 O
of O
Avg O
and O
0.898 O
of O
I O
- O
Avg O
) O
over O
the O
existing O
methods O
, O
and O
unsupervised O
methods O
( O
such O
as O
SEISA O
, O
PR O
) O
are O
not O
so O
competitive O
when O
compared O
with O
methods O
with O
supervised O
information O
. O
We O
lead O
a O
detailed O
investigation O
to O
detect O
the O
performance O
among O
different O
datasets O
and O
the O
impact O
of O
seed O
selection O
in O
the O
following O
aspects O
: O
For O
different O
datasets O
, O
our O
methods O
achieve O
robust O
results O
. O
It O
is O
worth O
noting O
that O
the O
range O
of O
the O
MAP O
of O
our O
method O
on O
these O
datasets O
does O
not O
exceed O
0.097 O
, O
while O
other O
baselines O
suffering O
from O
severe O
oscillations O
( O
SEISA O
of O
0.428 O
, O
EMB O
of O
0.435 O
, O
and O
PUL O
of O
0.234 O
) O
. O
And O
these O
supervised O
methods O
( O
PUL O
, O
PIP O
) O
that O
perform O
well O
on O
a O
certain O
dataset O
are O
further O
analyzed O
in O
subsequent O
experiments O
. O
For O
the O
performance O
on O
interdisciplinary O
courses O
. O
Most O
of O
the O
baselines O
meet O
a O
decline O
when O
turned O
to O
interdisciplinary O
courses O
. O
From O
this O
angle O
, O
PUL O
can O
not O
face O
this O
challenge O
. O
But O
PIP O
, O
C O
- O
RL O
, O
and O
HRL O
perform O
even O
better O
( O
with O
a O
lift O
of O
0.04 O
on O
average O
) O
, O
most O
likely O
because O
they O
all O
have O
a O
clustering O
- O
like O
seed O
selection O
process O
. O
For O
different O
seed O
selection O
strategies O
. O
We O
also O
detect O
the O
impact O
of O
seed O
selection O
by O
replacing O
high O
- O
level O
RL O
. O
The O
comparison O
among O
three O
RL O
methods O
shows O
that O
: O
1 O
) O
P O
- O
RL O
performs O
better O
in O
one O
- O
category O
expansion O
tasks O
( O
beat O
C O
- O
RL O
at O
0.038 O
) O
; O
2 O
) O
C O
- O
RL O
deal O
with O
interdisciplinary O
courses O
better O
than O
P O
- O
RL O
( O
as O
discussed O
above O
) O
; O
3 O
) O
HRL O
is O
stronger O
than O
these O
two O
methods O
in O
all O
datasets O
. O
The O
results O
( O
a O
) O
The O
MAP O
of O
different O
number O
of O
training O
sets O
. O
( O
b O
) O
The O
MAP O
of O
seed O
sizes O
. O
Figure O
4 O
: O
Performance O
of O
different O
settings O
. O
( O
a O
) O
shows O
the O
average O
MAP O
when O
mask O
some O
of O
the O
datasets O
in O
training O
. O
( O
b O
) O
shows O
the O
MAP O
of O
different O
seed O
size O
. O
exactly O
prove O
the O
superiority O
of O
HRL O
‚Äôs O
seed O
selection O
over O
rule O
- O
based O
strategies O
. O
4.3 O
Result O
Analysis O
Generalization O
Ability O
. O
Expansion O
models O
in O
MOOCs O
need O
to O
face O
with O
plenty O
of O
new O
courses O
every O
day O
. O
Thus O
we O
lead O
strict O
experiments O
to O
estimate O
the O
generalization O
ability O
of O
the O
model O
by O
masking O
training O
datasets O
. O
For O
example O
, O
the O
bar O
of O
n= O
5 O
in O
Figure O
4(a O
) O
indicates O
the O
average O
MAP O
when O
the O
models O
are O
trained O
on O
Ô¨Åve O
subject O
datasets O
and O
tested O
on O
the O
other O
one O
. O
Thus O
n= O
6is O
the O
average O
MAP O
in O
Table O
2 O
while O
n= O
5 O
andn= O
4 O
present O
the O
results O
of O
facing O
one O
or O
two O
kinds O
of O
new O
courses O
. O
Here O
we O
select O
HRL O
, O
PUL O
, O
and O
PIP O
for O
observation O
. O
Such O
an O
experiment O
shows O
that O
HRL O
still O
maintains O
an O
outstanding O
performance O
in O
new O
courses O
. O
Still O
, O
PIP O
and O
PUL O
suffer O
from O
a O
sharp O
decline O
in O
untrained O
new O
datasets O
( O
even O
at O
the O
same O
level O
as O
unsupervised O
methods O
) O
. O
The O
size O
of O
seed O
set O
k. O
For O
different O
settings O
of O
seed O
sizes O
, O
we O
compare O
the O
performance O
of O
ExpanRL O
with O
other O
RL O
based O
baselines O
. O
As O
shown O
in O
Figure O
4(b O
) O
, O
HRL O
keeps O
a O
high O
level O
of O
MAP O
among O
these O
settings O
( O
all O
over O
0.8 O
on O
average O
) O
. O
Meanwhile O
, O
we O
Ô¨Ånd O
that O
all O
these O
RL O
- O
based O
methods O
perform776Cr@10 O
Cr@20 O
Cr@50 O
PR O
0.097 O
0.182 O
0.425 O
SEISA O
0.097 O
0.204 O
0.459 O
EMB O
0.071 O
0.150 O
0.359 O
PUL O
0.041 O
0.091 O
0.349 O
PIP O
0.069 O
0.126 O
0.342 O
HRL O
0.036 O
0.082 O
0.258 O
Table O
3 O
: O
Online O
Evaluation O
results O
. O
better O
in O
small O
or O
large O
seed O
size O
( O
less O
than O
10or O
larger O
than O
40 O
) O
, O
which O
requires O
future O
detection O
on O
this O
phenomenon O
. O
Discussion O
. O
Based O
on O
the O
above O
experimental O
results O
, O
we O
summarize O
the O
analysis O
as O
follows O
: O
1 O
) O
the O
performance O
of O
unsupervised O
methods O
on O
different O
datasets O
is O
not O
as O
stable O
as O
the O
supervised O
or O
RL O
methods O
; O
2 O
) O
except O
for O
models O
that O
have O
a O
clustering O
- O
like O
seed O
selection O
process O
( O
PIP O
, O
CRL O
, O
HRL O
) O
, O
most O
models O
suffer O
from O
declines O
on O
interdisciplinary O
datasets O
; O
3 O
) O
although O
supervised O
models O
( O
PIP O
, O
PUL O
) O
perform O
well O
in O
some O
cases O
, O
they O
drastically O
decline O
in O
untrained O
new O
courses O
; O
4 O
) O
HRL O
, O
consisting O
of O
a O
feasible O
seed O
selection O
RL O
and O
expansion O
strategies O
from O
human O
efforts O
, O
keep O
a O
high O
performance O
under O
different O
settings O
. O
HRL O
deal O
with O
the O
challenges O
in O
MOOC O
expansion O
tasks O
, O
as O
claimed O
in O
the O
introduction O
. O
4.4 O
MOOC O
Online O
Evaluation O
Utilizing O
user O
feedback O
on O
the O
expansion O
results O
from O
our O
interactive O
MOOC O
environment O
, O
we O
also O
set O
up O
an O
online O
evaluation O
to O
detect O
whether O
users O
agree O
on O
the O
expansion O
results O
. O
Following O
the O
same O
evaluation O
metric O
in O
( O
Yu O
et O
al O
. O
, O
2019a O
) O
, O
we O
denote O
Click O
Rate O
asCr@q O
, O
which O
means O
the O
click O
rate O
of O
topqexpanded O
concepts O
, O
i.e. O
, O
Cr@q O
= O
q O
/ O
summationdisplay O
i=1œï(ci)/|Ec|/summationdisplay O
j=1œï(cj O
) O
) O
( O
13 O
) O
A O
smallerCr@qindicates O
more O
users O
think O
the O
results O
are O
relevant O
to O
the O
course O
. O
We O
record O
the O
performance O
of O
each O
method O
in O
Table O
3 O
. O
Results O
show O
that O
ExpanRL O
obtains O
the O
best O
feedback O
from O
MOOC O
users O
under O
all O
three O
settings O
. O
It O
‚Äôs O
worth O
noting O
that O
the O
advantage O
of O
ExpanRL O
is O
evident O
while O
selecting O
larger O
- O
scale O
samples O
( O
The O
overlap O
rises O
from O
0.005 O
to O
0.091 O
) O
, O
which O
indicates O
that O
our O
model O
can O
provide O
more O
high O
- O
quality O
concepts O
. O
5 O
Related O
Work O
Our O
work O
follows O
the O
task O
of O
concept O
expansion O
in O
MOOCs O
( O
Yu O
et O
al O
. O
, O
2019a O
) O
, O
a O
particular O
type O
of O
setexpansion O
problem O
, O
which O
takes O
several O
seeds O
as O
input O
and O
expands O
the O
entity O
set O
. O
Set O
expansion O
was O
born O
to O
serve O
knowledge O
acquisition O
applications O
on O
the O
Internet O
. O
Google O
Sets O
was O
a O
pioneer O
which O
leaded O
a O
series O
of O
early O
research O
, O
e.g. O
Bayesian O
Sets O
( O
Ghahramani O
and O
Heller O
, O
2006 O
) O
, O
SEAL O
( O
Wang O
and O
Cohen O
, O
2007 O
) O
, O
SEISA O
( O
He O
and O
Xin O
, O
2011 O
) O
and O
others O
( O
Sarmento O
et O
al O
. O
, O
2007 O
; O
Shi O
et O
al O
. O
, O
2010 O
; O
Wang O
et O
al O
. O
, O
2015 O
) O
. O
These O
efforts O
utilize O
web O
tables O
as O
a O
resource O
and O
mainly O
serves O
for O
search O
engines O
. O
Recently O
, O
more O
related O
research O
has O
turned O
its O
attention O
to O
other O
application O
Ô¨Åelds O
, O
such O
as O
news O
mining O
( O
Redondo O
- O
Garc O
¬¥ O
ƒ±a O
et O
al O
. O
, O
2014 O
) O
, O
knowledge O
graphs O
( O
Zhang O
et O
al O
. O
, O
2017 O
) O
, O
education O
assistance O
( O
Yu O
et O
al O
. O
, O
2019a O
) O
, O
etc O
. O
Meanwhile O
, O
corpus O
- O
based O
expansion O
methods O
snowball O
, O
and O
iterative O
bootstrapping O
became O
a O
common O
solution O
( O
Shen O
et O
al O
. O
, O
2017 O
; O
Yu O
et O
al O
. O
, O
2019b O
; O
Yan O
et O
al O
. O
, O
2019 O
) O
, O
which O
expands O
the O
set O
in O
round O
and O
select O
high O
- O
quality O
results O
to O
extract O
feature O
iteratively O
. O
ExpanRL O
is O
inspired O
by O
this O
type O
of O
method O
and O
is O
designed O
to O
optimize O
the O
existing O
iterative O
process O
. O
ExpanRL O
also O
beneÔ¨Åts O
from O
hierarchical O
reinforcement O
learning O
( O
HRL O
) O
, O
which O
has O
been O
employed O
in O
many O
NLP O
tasks O
( O
Zhang O
et O
al O
. O
, O
2019 O
; O
Takanobu O
et O
al O
. O
, O
2019 O
) O
and O
achieved O
impressive O
results O
. O
By O
decomposing O
complex O
tasks O
into O
multiple O
small O
tasks O
to O
reduce O
the O
complexity O
of O
decision O
making O
( O
Barto O
and O
Mahadevan O
, O
2003 O
) O
, O
HRL O
naturally O
matches O
the O
iterative O
set O
expansion O
tasks O
. O
6 O
Conclusion O
and O
Future O
Work O
We O
investigate O
the O
task O
of O
course O
concept O
expansion O
, O
which O
utilizes O
the O
NLP O
approaches O
in O
improving O
MOOC O
education O
. O
After O
constructing O
a O
novel O
interactive O
MOOC O
environment O
to O
collect O
user O
feedback O
on O
expansion O
results O
, O
we O
design O
a O
paradigm O
, O
ExpanRL O
, O
which O
decomposes O
the O
concept O
expansion O
task O
into O
a O
hierarchy O
of O
two O
subtasks O
: O
highlevel O
seed O
selection O
and O
low O
- O
level O
concept O
expansion O
. O
Experiment O
results O
on O
nine O
datasets O
from O
real O
MOOCs O
prove O
that O
ExpanRL O
can O
better O
serve O
students O
by O
recognizing O
the O
helpful O
expanded O
results O
and O
maintaining O
good O
performance O
in O
interdisciplinary O
courses O
and O
even O
new O
courses O
. O
Promising O
future O
directions O
include O
detecting O
how O
to O
ensemble O
supervised O
learning O
and O
RL O
expansion O
models O
and O
applying O
the O
proposed O
model O
in O
related O
tasks O
. O
We O
also O
hope O
our O
design O
of O
interactive O
games O
can O
call O
for O
more O
fancy O
methods O
that O
utilize O
student O
feedback O
in O
NLP O
applications777 O
in O
Education O
. O
Acknowledgement O
This O
work O
is O
supported O
by O
the O
National O
Key O
Research O
and O
Development O
Program O
of O
China O
( O
2018YFB1004503 O
) O
, O
NSFC O
Key O
Projects O
( O
U1736204 O
, O
61533018 O
) O
, O
grants O
from O
Beijing O
Academy O
of O
ArtiÔ¨Åcial O
Intelligence O
( O
BAAI2019ZD0502 O
) O
, O
Institute O
for O
Guo O
Qiang O
, O
Tsinghua O
University O
( O
2019GQB0003 O
) O
, O
and O
XuetangX. O
Abstract O
Question O
generation O
( O
QG O
) O
has O
recently O
attracted O
considerable O
attention O
. O
Most O
of O
the O
current O
neural O
models O
take O
as O
input O
only O
one O
or O
two O
sentences O
and O
perform O
poorly O
when O
multiple O
sentences O
or O
complete O
paragraphs O
are O
given O
as O
input O
. O
However O
, O
in O
real O
- O
world O
scenarios O
, O
it O
is O
very O
important O
to O
be O
able O
to O
generate O
high O
- O
quality O
questions O
from O
complete O
paragraphs O
. O
In O
this O
paper O
, O
we O
present O
a O
simple O
yet O
effective O
technique O
for O
answer O
- O
aware O
question O
generation O
from O
paragraphs O
. O
We O
augment O
a O
basic O
sequence O
- O
to O
- O
sequence O
QG O
model O
with O
dynamic O
, O
paragraph O
- O
speciÔ¨Åc O
dictionary O
and O
copy O
attention O
that O
is O
persistent O
across O
the O
corpus O
, O
without O
requiring O
features O
generated O
by O
sophisticated O
NLP O
pipelines O
or O
handcrafted O
rules O
. O
Our O
evaluation O
on O
SQuAD O
shows O
that O
our O
model O
signiÔ¨Åcantly O
outperforms O
current O
state O
- O
of O
- O
theart O
systems O
in O
question O
generation O
from O
paragraphs O
in O
both O
automatic O
and O
human O
evaluation O
. O
We O
achieve O
a O
6 O
- O
point O
improvement O
over O
the O
best O
system O
on O
BLEU-4 O
, O
from O
16.38 O
to O
22.62 O
. O
1 O
Introduction O
and O
Related O
work O
Automatic O
question O
generation O
( O
QG O
) O
from O
text O
aims O
to O
generate O
meaningful O
, O
relevant O
, O
and O
answerable O
questions O
from O
a O
given O
textual O
input O
. O
Owing O
to O
its O
applicability O
in O
conversational O
systems O
such O
as O
Cortana O
, O
Siri O
, O
chatbots O
, O
and O
automated O
tutoring O
systems O
, O
QG O
has O
attracted O
considerable O
interest O
in O
both O
academia O
and O
industry O
. O
Recent O
neural O
network O
- O
based O
approaches O
( O
Du O
et O
al O
. O
, O
2017 O
; O
Kumar O
et O
al O
. O
, O
2018a O
, O
b O
; O
Du O
and O
Cardie O
, O
2018 O
; O
Zhao O
et O
al O
. O
, O
2018 O
; O
Song O
et O
al O
. O
, O
2018 O
; O
Subramanian O
et O
al O
. O
, O
2018 O
; O
Tang O
et O
al O
. O
, O
2017 O
; O
Wang O
et O
al O
. O
, O
2017 O
) O
represent O
the O
state O
- O
of O
- O
the O
- O
art O
in O
question O
generation O
. O
Most O
of O
these O
techniques O
learn O
to O
generate O
questions O
from O
short O
text O
, O
i.e. O
, O
one O
or O
two O
sentences O
( O
Du O
et O
al O
. O
, O
2017 O
; O
Kumar O
et O
al O
. O
, O
2018a O
, O
b O
; O
Du O
and O
Cardie O
, O
2018 O
) O
. O
On O
the O
other O
hand O
, O
the O
ability O
to O
generate O
high O
- O
quality O
questions O
from O
longer O
text O
such O
as O
from O
multiplesentences O
or O
from O
a O
paragraph O
in O
its O
entirety O
, O
is O
more O
useful O
in O
real O
- O
world O
settings O
. O
However O
, O
given O
that O
a O
paragraph O
contains O
a O
longer O
context O
and O
more O
information O
than O
a O
sentence O
, O
it O
is O
a O
signiÔ¨Åcantly O
more O
challenging O
problem O
to O
generate O
questions O
around O
a O
longer O
context O
. O
In O
Ô¨Ågure O
1 O
we O
present O
one O
motivating O
example O
demonstrating O
why O
the O
model O
needs O
information O
more O
than O
just O
a O
single O
sentence O
for O
generating O
question O
a O
meaningful O
and O
relevant O
question O
. O
As O
we O
can O
see O
in O
Ô¨Ågure O
1 O
, O
question O
2 O
, O
question O
generated O
by O
our O
model O
use O
multiple O
sentences O
as O
context O
. O
Du O
et O
al O
. O
( O
2017 O
) O
recently O
observed O
that O
20 O
% O
of O
the O
questions O
in O
the O
SQuAD O
dataset O
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
require O
paragraphlevel O
information O
to O
answer O
them O
. O
For O
the O
same O
reason O
, O
it O
is O
intuitive O
to O
conclude O
that O
the O
ability O
to O
consider O
the O
complete O
context O
; O
however O
long O
it O
may O
be O
, O
is O
critical O
for O
generating O
high O
- O
quality O
questions O
. O
Legislative O
power O
in O
Warsaw O
is O
vested O
in O
a O
unicameral O
Warsaw O
City O
Council O
( O
Rada O
  O
Miasta O
) O
, O
which O
comprises O
06 O
members O
. O
Council O
members O
are O
elected O
directly O
every O
   O
four O
years O
. O
Like O
most O
legislative O
bodies O
, O
the O
City O
Council O
divides O
itself O
into O
  O
committees O
which O
have O
the O
oversight O
of O
various O
functions O
of O
the O
city O
government O
. O
  O
Bills O
passed O
by O
a O
simple O
majority O
are O
sent O
to O
the O
mayor O
( O
the O
President O
of O
Warsaw O
) O
, O
  O
who O
may O
sign O
them O
into O
law O
. O
If O
the O
mayor O
vetoes O
a O
bill O
, O
the O
Council O
has O
30 O
days O
to O
  O
override O
the O
veto O
by O
a O
two O
- O
thirds O
majority O
vote O
. O
  O
Human O
Generated O
: O
How O
many O
members O
are O
on O
the O
Warsaw O
City O
Counil O
? O
  O
Our O
Model O
: O
  O
How O
many O
members O
are O
in O
the O
Warsaw O
City O
Council O
? O
  O
Human O
Generated O
: O
  O
How O
often O
are O
elections O
for O
the O
counsel O
held O
? O
  O
Our O
Model O
: O
  O
How O
often O
are O
the O
Rada O
Miasta O
elected O
? O
  O
Human O
Generated O
: O
What O
does O
the O
City O
Council O
divide O
itself O
into O
? O
  O
Our O
Model O
: O
The O
City O
Council O
divides O
itself O
into O
what O
? O
  O
Human O
Generated O
: O
  O
How O
many O
days O
does O
the O
Council O
have O
to O
override O
the O
mayor O
's O
veto O
? O
  O
Our O
Model O
: O
How O
long O
does O
it O
take O
to O
override O
the O
veto O
? O
  O
Figure O
1 O
: O
Examples O
of O
ground O
- O
truth O
questions O
and O
questions O
generated O
by O
our O
model O
from O
the O
same O
paragraph O
. O
Each O
question O
and O
its O
corresponding O
answer O
are O
highlighted O
using O
the O
same O
color O
. O
Zhao O
et O
al O
. O
( O
2018 O
) O
very O
recently O
proposed O
a O
technique O
( O
referred O
to O
MPGSN O
here O
) O
for O
paragraph O
- O
level O
question O
generation O
using O
a O
max O
out O
pointer O
mechanism O
and O
a O
gated O
self O
- O
attention O
encoder O
. O
Their O
best O
model O
achieves O
BLEU-4 O
of O
16.38 O
on O
SQuAD O
with O
paragraphs O
as O
input O
. O
Compared O
to O
( O
Zhao O
et O
al O
. O
,7812018 O
) O
, O
our O
model O
has O
less O
number O
of O
parameters O
( O
making O
it O
more O
computationally O
efÔ¨Åcient O
) O
, O
is O
relatively O
easy O
to O
train O
and O
is O
somewhat O
deterministically O
biased O
toward O
the O
generation O
of O
important O
words O
in O
the O
input O
paragraph O
. O
In O
this O
paper O
, O
we O
propose O
a O
simple O
yet O
effective O
paragraph O
- O
level O
question O
generation O
technique O
. O
We O
augment O
the O
standard O
sequence O
- O
to O
- O
sequence O
model O
based O
on O
bidirectional O
LSTM O
with O
two O
components O
: O
( O
1 O
) O
a O
dynamic O
, O
paragraph O
- O
speciÔ¨Åc O
dictionary O
and O
( O
2 O
) O
a O
copy O
attention O
mechanism O
that O
is O
persistent O
across O
paragraphs O
. O
Our O
evaluation O
on O
SQuAD O
shows O
signiÔ¨Åcant O
improvement O
over O
MPGSN O
in O
automatic O
evaluation O
. O
We O
achieve O
a O
6 O
- O
point O
increase O
with O
respect O
to O
BLEU-4 O
( O
from O
16.38 O
to O
22.62 O
) O
over O
MPGSN O
‚Äôs O
best O
system O
. O
We O
perform O
the O
human O
evaluation O
of O
our O
model O
with O
and O
without O
copy O
attention O
, O
and O
we O
observe O
that O
we O
obtain O
27 O
% O
more O
relevant O
questions O
when O
the O
copy O
attention O
is O
incorporated O
. O
For O
a O
given O
paragraph O
as O
input O
, O
we O
depict O
in O
Figure O
1 O
, O
the O
ground O
- O
truth O
questions O
as O
well O
as O
the O
questions O
generated O
along O
with O
the O
answers O
highlighted O
in O
the O
paragraph O
. O
As O
can O
be O
seen O
from O
the O
example O
, O
while O
generating O
the O
second O
question(highlighted O
in O
green O
color O
) O
, O
our O
model O
uses O
information O
not O
only O
from O
the O
sentence O
containing O
the O
answer O
, O
but O
also O
relevant O
context O
from O
the O
complete O
paragraph O
. O
... O
... O
+ O
  O
  O
ùúéx O
x+ O
Attention O
  O
weights O
  O
Paragraph O
  O
Encoder O
  O
Word O
  O
Vectors O
  O
Answer O
vector O
Context O
  O
Vector O
  O
Question O
  O
Decoder O
Source O
Vocab O
  O
Distribution O
  O
Answer O
encoded O
paragraph O
Generated O
Question O
Final O
   O
Distribution O
  O
p(cs=0 O
) O
  O
p(cs=1 O
) O
  O
... O
ùúé O
Figure O
2 O
: O
Overall O
architecture O
of O
our O
paragraph O
- O
level O
question O
generation O
model O
. O
2 O
Problem O
Formulation O
& O
Approach O
Given O
a O
paragraph O
‚Äò O
P O
‚Äô O
and O
answer O
‚Äò O
A O
‚Äô O
, O
a O
question O
generation O
model O
iteratively O
samples O
question O
word O
qt‚ààVQat O
every O
time O
step O
‚Äò O
t O
‚Äô O
from O
the O
probability O
distribution O
given O
by O
: O
Pr(Q|P O
, O
A;Œ∏)=|Q|/productdisplay O
t=1Pr(qt|P O
, O
A;Œ∏ O
) O
( O
1 O
) O
WhereVQis O
the O
question O
vocabulary O
, O
Œ∏is O
the O
set O
of O
parameters O
, O
and O
Ais O
the O
answer O
. O
Our O
question O
generation O
model O
consists O
of O
a O
two O
- O
layer O
paragraph O
encoder O
and O
a O
one O
- O
layer O
question O
decoder O
, O
equipped O
with O
a O
dynamic O
dictionary O
and O
copy O
attention O
. O
In O
Figure O
2 O
, O
we O
illustrate O
the O
overall O
architecture O
of O
our O
paragraph O
level O
question O
generation O
model O
. O
The O
dynamic O
dictionary O
allows O
every O
training O
instance O
( O
paragraph O
) O
to O
have O
its O
own O
vocabulary O
instead O
of O
relying O
on O
the O
preprocessed O
global O
vocabulary O
. O
Copy O
attention O
enables O
the O
model O
to O
predict O
question O
words O
from O
the O
extended O
vocabulary O
( O
complete O
vocabulary O
+ O
paragraph O
vocabulary O
) O
. O
Copy O
attention O
operates O
over O
the O
union O
of O
words O
in O
vocabulary O
and O
paragraph O
words O
. O
2.1 O
Paragraph O
encoder O
We O
use O
a O
two O
- O
layer O
bidirectional O
long O
short O
- O
term O
memory O
( O
Bi O
- O
LSTM O
) O
network O
stack O
as O
the O
paragraph O
encoder O
. O
The O
paragraph O
encoder O
takes O
an O
answer O
- O
tagged O
paragraph O
as O
input O
and O
outputs O
a O
representation O
of O
the O
paragraph O
. O
Note O
that O
the O
Bi O
- O
LSTM O
network O
processes O
the O
input O
paragraph O
in O
both O
the O
forward O
and O
backward O
directions:‚àí O
‚Üíht O
= O
LSTM O
( O
et,‚àí‚àí‚Üíht‚àí1)and‚Üê O
‚àíht O
= O
LSTM O
( O
et,‚Üê‚àí‚àíht+1 O
) O
, O
where‚àí O
‚Üíht(resp.‚Üê O
‚àíht O
) O
is O
the O
forward O
( O
resp O
. O
backward O
) O
hidden O
state O
at O
time O
step O
tandetis O
the O
vector O
representation O
of O
current O
input O
xtat O
time O
stept O
. O
The O
Ô¨Ånal O
hidden O
state O
for O
the O
current O
word O
input O
is O
the O
concatenation O
of O
the O
forward O
and O
backward O
hidden O
state O
vectors O
: O
ht=[‚àí O
‚Üíht,‚Üê O
‚àíht O
] O
. O
2.2 O
Dynamic O
, O
shared O
dictionary O
In O
the O
traditional O
approach O
, O
a O
new O
/ O
unknown O
word O
is O
typically O
replaced O
with O
the O
‚Äú O
< O
unk O
> O
‚Äù O
token O
. O
The O
copy O
mechanism O
( O
Gu O
et O
al O
. O
, O
2016 O
) O
then O
unfortunately O
learns O
to O
copy O
this O
‚Äú O
< O
unk O
> O
‚Äù O
token O
instead O
of O
the O
actual O
( O
unknown O
) O
word O
from O
the O
source O
paragraph O
. O
Instead O
, O
we O
use O
a O
separate O
dynamic O
dictionary O
unique O
to O
each O
source O
paragraph O
, O
which O
includes O
all O
and O
only O
words O
that O
occur O
in O
the O
paragraph O
. O
This O
allows O
our O
model O
to O
copy O
source O
words O
that O
may O
not O
be O
in O
the O
target O
dictionary O
into O
the O
target O
( O
question O
) O
. O
Using O
a O
dynamic O
dictionary O
consisting O
of O
the O
preprocessed O
vocabulary O
instead O
of O
a O
static O
one O
enables O
the O
copy O
mechanism O
to O
copy O
the O
exact O
words O
directly O
into O
the O
question O
, O
even O
if O
they O
are O
rare O
and O
unknown O
. O
Given O
a O
source O
paragraph O
p O
, O
we O
denote O
its O
dynamic O
vocabulary O
by O
Vp O
. O
Our O
copy O
attention O
mechanism O
takes O
into O
account O
Vpand O
the O
global O
vocabularyVto O
determine O
whether O
to O
copy O
a O
word O
from O
Vp O
or O
to O
predict O
a O
word O
from O
question O
vocabulary O
VQ O
. O
As O
our O
model O
‚Äôs O
source O
as O
well O
as O
target O
are O
in782Figure O
3 O
: O
Visualizing O
attention O
weights O
for O
the O
second O
generated O
question O
in O
Fig O
. O
1 O
. O
the O
same O
language O
, O
we O
work O
with O
a O
shared O
source O
and O
target O
vocabulary O
, O
though O
we O
learn O
different O
language O
models O
for O
the O
paragraph O
and O
the O
question O
. O
Sharing O
source O
and O
target O
vocabulary O
also O
decreases O
the O
memory O
requirement O
resulting O
from O
matrix O
multiplication O
( O
thus O
making O
faster O
training O
through O
larger O
batch O
size O
) O
possible O
. O
It O
also O
enables O
efÔ¨Åcient O
question O
decoding O
, O
thus O
reducing O
the O
time O
for O
inference O
on O
the O
test O
data O
. O
2.3 O
Question O
decoder O
Our O
question O
decoder O
is O
another O
Bi O
- O
LSTM O
that O
takes O
as O
input O
the O
last O
hidden O
state O
and O
context O
representation O
from O
the O
encoder O
and O
generates O
question O
words O
sequentially O
based O
on O
the O
previously O
generated O
words O
. O
The O
decoder O
hidden O
state O
( O
st= O
[ O
‚àí O
‚Üíst,‚Üê O
‚àíst O
] O
) O
at O
time O
step O
tis O
the O
concatenation O
of O
the O
forward O
and O
backward O
hidden O
state O
representations O
: O
‚àí O
‚Üíst O
= O
LSTM O
( O
ot,‚àí‚àí‚Üíst‚àí1)and‚Üê O
‚àíst O
= O
LSTM O
( O
ot,‚Üê‚àí‚àíst+1 O
) O
, O
whereotis O
the O
vector O
representation O
of O
decoder O
input O
( O
yt O
) O
at O
time O
step O
t. O
During O
training O
time O
the O
vector O
representation O
of O
words O
from O
the O
ground O
- O
truth O
question O
is O
fed O
as O
decoder O
input O
, O
and O
during O
test O
time O
the O
vector O
representation O
of O
the O
vocabulary O
word O
with O
maximum O
probability O
is O
fed O
as O
input O
. O
We O
feed O
EOS O
symbol O
as O
input O
to O
decoder O
from O
both O
forward O
and O
backward O
dircetion O
at O
time O
t0 O
. O
Bidirectional O
decoder O
factorizes O
the O
conditional O
decoding O
probabilities O
in O
both O
directions O
( O
left O
- O
to O
- O
right O
and O
right O
- O
to O
- O
left O
) O
into O
summation O
as O
: O
P O
/ O
parenleftBig O
yt|[ym]m O
/ O
negationslash O
= O
t O
/ O
parenrightBig O
= O
‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚Üí O
logp O
/ O
parenleftbig O
yt|Y[1 O
: O
t‚àí1]/parenrightbig O
+ O
‚Üê‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí O
logP O
/ O
parenleftbig O
yt|Y[t+1 O
: O
Ty]/parenrightbig(2 O
) O
The O
probability O
distribution O
over O
words O
in O
the O
vocabulary O
is O
calculated O
as O
: O
Pr(qt)=softmax O
( O
WgœÉ(Ws[st O
, O
ht]+bs)+bg O
) O
( O
3 O
) O
where O
Wg O
, O
Ws O
, O
bsandbgare O
trainable O
model O
parameters O
. O
Probability O
distribution O
P(qt)uses O
the O
standard O
softmax O
over O
the O
question O
vocabulary O
VQ O
. O
This O
is O
used O
to O
sample O
word O
with O
maximum O
probability O
while O
decoding O
a O
question.2.4 O
Copy O
attention O
We O
know O
that O
a O
good O
question O
should O
be O
relevant O
to O
( O
answerable O
from O
) O
the O
paragraph O
. O
So O
we O
learn O
a O
probabilistic O
mixture O
model O
over O
the O
question O
vocabularyVQand O
the O
current O
paragraph O
vocabulary O
VP O
. O
The O
current O
paragraph O
vocabulary O
is O
generated O
by O
a O
dynamic O
dictionary O
module O
. O
Our O
copy O
attention O
calculates O
two O
values O
: O
cs O
: O
a O
binary O
- O
valued O
variable O
which O
acts O
a O
switch O
between O
copying O
a O
word O
from O
the O
paragraph O
‚Äôs O
dynamic O
vocabulary O
VPor O
generating O
from O
the O
question O
vocabulary O
VQ O
Pr O
/ O
parenleftbig O
.|VP O
/ O
parenrightbig O
: O
probability O
of O
copying O
a O
particular O
word O
from O
paragraph O
vocabulary O
VP O
. O
Therefore O
, O
the O
Ô¨Ånal O
probability O
distribution O
from O
which O
a O
word O
will O
be O
sampled O
while O
generating O
a O
question O
is O
calculated O
over O
the O
extended O
vocabulary O
VQ‚à™Vp O
. O
Given O
a O
word O
from O
the O
extended O
vocabularyw‚ààVQ‚à™VP O
, O
its O
probability O
Pr(w)is O
computed O
as O
: O
Pr(w)=Pr(cs=1)Pr O
/ O
parenleftbig O
w|VP O
/ O
parenrightbig O
+ O
Pr(cs=0)Pr O
/ O
parenleftbig O
w|VQ O
/ O
parenrightbig O
( O
4 O
) O
The O
switch O
probability O
Pr(cs)is O
determined O
using O
the O
decoder O
hidden O
states O
as O
: O
Pr(cs=1)=œÉ(Wcsst+bcs O
) O
( O
5 O
) O
whereWcsandbcsare O
trainable O
model O
parameters O
. O
Pr O
/ O
parenleftbig O
w|VQ O
/ O
parenrightbig O
is O
the O
probability O
of O
predicting O
a O
word O
from O
complete O
vocabulary O
VQ O
. O
The O
copy O
attention O
weightatis O
computed O
as O
: O
et O
i O
= O
vTtanh O
( O
Whhi+Wsst+battn O
) O
( O
6 O
) O
at O
= O
sparsemax O
( O
et O
) O
( O
7 O
) O
Where O
v O
, O
Wh O
, O
Wsandbattnare O
trainable O
model O
parameters O
. O
The O
probability O
of O
copying O
a O
word O
from O
the O
paragraph O
vocabulary O
VPis O
estimated O
as O
: O
Pr O
/ O
parenleftbig O
w|VP O
/ O
parenrightbig O
= O
œÉ(Waat+ba O
) O
( O
8) O
whereWaandbaare O
trainable O
model O
parameters.783Model O
BLEU-1 O
BLEU-2 O
BLEU-3 O
BLEU-4 O
METEOR O
ROUGE O
- O
L O
MPGSN O
( O
Zhao O
et O
al O
. O
, O
2018 O
) O
45.07 O
29.58 O
21.60 O
16.38 O
20.25 O
44.48 O
L2A O
( O
Du O
et O
al O
. O
, O
2017 O
) O
42.54 O
25.33 O
16.98 O
11.86 O
16.28 O
39.37 O
NQG O
dd[w O
/ O
o O
copy O
attention O
] O
55.32 O
32.39 O
20.12 O
12.86 O
17.00 O
42.77 O
NQG O
dd[with O
copy O
attention O
] O
61.84 O
41.73 O
30.19 O
22.62 O
21.93 O
48.60 O
Table O
1 O
: O
Results O
on O
the O
test O
set O
on O
automatic O
evaluation O
metrics O
. O
Best O
results O
for O
each O
metric O
( O
column O
) O
are O
bolded O
. O
3 O
Experimental O
Setup O
We O
report O
the O
experimental O
result O
of O
our O
model O
( O
referred O
to O
as O
NQG O
dd O
) O
and O
compare O
it O
with O
the O
current O
state O
of O
the O
art O
MPGSN O
( O
Zhao O
et O
al O
. O
, O
2018 O
) O
. O
We O
employ O
the O
widely O
- O
used O
metrics O
BLEU O
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
, O
ROUGE O
- O
L O
and O
METEOR O
for O
automatic O
evaluation O
. O
We O
use O
evaluation O
script O
provided O
by O
( O
Chen O
et O
al O
. O
, O
2015 O
) O
. O
Similar O
to O
( O
Kumar O
et O
al O
. O
, O
2018a O
) O
we O
also O
report O
qualitative O
assessment O
on O
the O
syntax O
, O
semantics O
and O
relevance O
of O
the O
questions O
generated O
by O
our O
model O
. O
All O
experiments O
are O
performed O
on O
the O
SQuAD O
dataset O
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
, O
where O
complete O
paragraphs O
are O
taken O
as O
input O
instead O
of O
just O
one O
or O
two O
sentences O
. O
We O
reformat O
the O
SQuAD O
dataset O
such O
that O
during O
training O
time O
, O
each O
source O
instance O
is O
a O
( O
paragraph O
, O
question O
) O
pair O
annotated O
with O
the O
gold O
answers O
, O
and O
the O
target O
is O
a O
question O
. O
Following O
the O
exact O
setup O
from O
MPGSN O
( O
Zhao O
et O
al O
. O
, O
2018 O
) O
, O
we O
split O
the O
SQuAD O
train O
set O
into O
train O
and O
validation O
set O
containing O
77,526 O
and O
9,995 O
instances O
respectively O
, O
and O
take O
the O
separate O
SQuAD O
dev O
set O
containing O
10,556 O
instances O
as O
our O
test O
set O
. O
4 O
Results O
and O
Analysis O
Table O
1 O
summarizes O
results O
of O
the O
automatic O
evaluation O
of O
the O
test O
set O
. O
As O
can O
be O
seen O
, O
our O
model O
signiÔ¨Åcantly O
outperforms O
the O
state O
- O
of O
- O
the O
- O
art O
MPGSN O
on O
all O
metrics O
. O
The O
improvements O
on O
BLEU O
are O
especially O
substantial O
, O
the O
BLEU-4 O
score O
of O
MPGSN O
is O
16.38 O
, O
and O
ours O
( O
with O
copy O
incorporated O
) O
is O
22.62 O
, O
an O
improvement O
of O
6.24 O
, O
or O
38 O
% O
. O
This O
large O
performance O
difference O
demonstrates O
the O
effectiveness O
of O
our O
dynamic O
dictionary O
. O
In O
Table O
2 O
we O
present O
human O
evaluation O
results O
. O
We O
evaluate O
the O
quality O
of O
questions O
generated O
in O
terms O
on O
syntactic O
correctness O
, O
semantic O
correctness O
andrelevance O
to O
the O
paragraph O
. O
The O
evaluation O
is O
performed O
on O
a O
randomly O
selected O
subset O
of O
100 O
sentences O
from O
the O
test O
set O
. O
Each O
of O
the O
three O
evaluators O
are O
presented O
the O
100 O
paragraph O
- O
question O
pairs O
for O
two O
variants O
of O
our O
model O
( O
with O
and O
without O
copy O
) O
and O
asked O
for O
a O
binary O
responses O
for O
all O
threeparameters O
. O
We O
averaged O
responses O
received O
by O
all O
three O
evaluators O
to O
compute O
the O
Ô¨Ånal O
scores O
. O
As O
can O
be O
seen O
, O
the O
incorporation O
of O
the O
copy O
attention O
improves O
performance O
, O
especially O
on O
relevance O
. O
We O
also O
measure O
the O
inter O
- O
rater O
agreement O
using O
Randolph O
‚Äôs O
free O
- O
marginal O
multirater O
kappa O
( O
Randolph O
, O
2005 O
) O
. O
It O
can O
be O
observed O
that O
our O
quality O
metrics O
for O
both O
our O
models O
are O
rated O
as O
substantial O
agreement O
( O
Viera O
et O
al O
. O
, O
2005 O
) O
. O
To O
explain O
how O
our O
model O
attends O
to O
different O
words O
in O
the O
source O
paragraph O
we O
visualize O
attention O
weights O
in O
Figure O
3 O
, O
which O
shows O
attention O
weights O
between O
question O
2 O
generated O
by O
our O
model O
and O
the O
corresponding O
paragraph O
in O
Figure O
1 O
. O
We O
observe O
that O
the O
attention O
weight O
is O
high O
for O
words O
near O
the O
answer O
and O
the O
model O
attends O
to O
all O
relevant O
context O
rather O
that O
just O
the O
sentence O
containing O
the O
answer O
. O
ModelSyntax O
Semantics O
Relevance O
Score O
Kappa O
Score O
Kappa O
Score O
Kappa O
NQG O
dd[w O
/ O
o O
copy O
] O
89 O
0.68 O
83 O
0.69 O
43 O
0.67 O
NQG O
dd[with O
copy O
] O
94 O
0.64 O
82 O
0.68 O
71 O
0.73 O
Table O
2 O
: O
Human O
evaluation O
results O
( O
columns O
‚Äú O
Score O
‚Äù O
) O
as O
well O
as O
inter O
- O
rater O
agreement O
( O
columns O
‚Äú O
Kappa O
‚Äù O
) O
for O
each O
of O
our O
two O
models O
on O
100 O
questions O
from O
the O
test O
set O
. O
The O
scores O
are O
between O
0 O
( O
worst O
) O
and O
100 O
( O
best O
) O
. O
Best O
results O
for O
each O
metric O
( O
column O
) O
are O
in O
bold O
. O
We O
also O
note O
that O
our O
training O
is O
faster O
atleast O
by O
a O
factor O
of O
2 O
. O
We O
expected O
this O
since O
we O
replace O
a O
slightly O
expensive O
self O
- O
attention O
mechanism O
in O
the O
decoder O
of O
( O
Zhao O
et O
al O
. O
, O
2018 O
) O
with O
a O
simpler O
dynamic O
dictionary O
and O
reusable O
copy O
attention O
. O
5 O
Conclusion O
Paragraph O
- O
level O
question O
generation O
( O
QG O
) O
is O
an O
important O
but O
challenging O
problem O
, O
mainly O
due O
to O
the O
challenge O
in O
effectively O
handling O
a O
longer O
context O
. O
We O
present O
a O
simple O
yet O
effective O
approach O
for O
automatic O
question O
generation O
from O
paragraphs O
. O
Besides O
using O
a O
standard O
global O
source O
dictionary O
, O
our O
RNN O
- O
based O
model O
incorporates O
a O
dynamic O
, O
paragraph O
- O
speciÔ¨Åc O
dictionary O
, O
and O
learns O
to O
switch O
between O
copying O
from O
the O
combined784vocabulary O
and O
generating O
a O
new O
word O
. O
Through O
our O
experiments O
, O
we O
demonstrate O
how O
our O
model O
outperforms O
the O
current O
state O
- O
of O
- O
the O
- O
art O
model O
in O
paragraph O
- O
level O
QG O
by O
a O
wide O
margin O
, O
for O
example O
by O
6.24 O
BLEU-4 O
points O
, O
a O
38 O
% O
improvement O
. O
Abstract O
Adversarial O
attacks O
are O
label O
- O
preserving O
modiÔ¨Åcations O
to O
inputs O
of O
machine O
learning O
classiÔ¨Åers O
designed O
to O
fool O
machines O
but O
not O
humans O
. O
Natural O
Language O
Processing O
( O
NLP O
) O
has O
mostly O
focused O
on O
high O
- O
level O
attack O
scenarios O
such O
as O
paraphrasing O
input O
texts O
. O
We O
argue O
that O
these O
are O
less O
realistic O
in O
typical O
application O
scenarios O
such O
as O
in O
social O
media O
, O
and O
instead O
focus O
on O
low O
- O
level O
attacks O
on O
the O
character O
- O
level O
. O
Guided O
by O
human O
cognitive O
abilities O
and O
human O
robustness O
, O
we O
propose O
the O
Ô¨Årst O
large O
- O
scale O
catalogue O
and O
benchmark O
of O
low O
- O
level O
adversarial O
attacks O
, O
which O
we O
dub O
Z¬¥eroe O
, O
encompassing O
nine O
different O
attack O
modes O
including O
visual O
and O
phonetic O
adversaries O
. O
We O
show O
that O
RoBERTa O
, O
NLP O
‚Äôs O
current O
workhorse O
, O
fails O
on O
our O
attacks O
. O
Our O
dataset O
provides O
a O
benchmark O
for O
testing O
robustness O
of O
future O
more O
human O
- O
like O
NLP O
models O
. O
1 O
Introduction O
Adversarial O
examples O
are O
label O
- O
preserving O
modiÔ¨Åcations O
to O
inputs O
of O
machine O
learning O
architectures O
. O
Their O
typical O
characteristic O
is O
that O
they O
cause O
little O
damage O
to O
humans O
but O
may O
maximally O
affect O
classiÔ¨Åer O
performance O
, O
exposing O
their O
weaknesses O
and O
outlining O
the O
differences O
between O
human O
and O
machine O
text O
processing O
( O
Szegedy O
et O
al O
. O
, O
2014 O
; O
Goodfellow O
et O
al O
. O
, O
2014 O
; O
Eger O
et O
al O
. O
, O
2019 O
) O
. O
While O
in O
computer O
vision O
, O
pixel O
- O
level O
attacks O
, O
which O
go O
unnoticed O
by O
humans O
, O
may O
lead O
to O
catastrophic O
failure O
, O
attacks O
in O
NLP O
are O
more O
challenging O
. O
Some O
attacks O
in O
NLP O
replace O
individual O
words O
by O
synonyms O
or O
hyponyms O
( O
Alzantot O
et O
al O
. O
, O
2018 O
) O
or O
paraphrase O
whole O
sentences O
( O
Ribeiro O
et O
al O
. O
, O
2018 O
) O
. O
However O
, O
such O
high O
- O
level O
attacks O
are O
not O
only O
more O
difÔ¨Åcult O
to O
compute O
( O
requiring O
available O
resources O
such O
as O
dictionaries O
or O
word O
embeddings O
) O
but O
they O
are O
also O
implausible O
in O
real O
- O
world O
scenarios O
such O
as O
spamming O
or O
posting O
in O
social O
media O
, O
asusers O
would O
need O
to O
know O
the O
training O
data O
and/or O
the O
inner O
workings O
of O
the O
machine O
learning O
models O
in O
order O
to O
identify O
candidate O
substitutions O
( O
or O
have O
unrestrained O
access O
to O
model O
predictions O
) O
. O
In O
contrast O
, O
such O
users O
would O
typically O
use O
low O
- O
level O
attacks O
on O
characters O
, O
such O
as O
inserting O
placeholder O
symbols O
( O
e.g. O
, O
underscores O
) O
, O
mistyping O
words O
( O
e.g. O
, O
Hilter O
forHitler O
) O
, O
or O
using O
phonetically O
similar O
sounding O
words O
( O
Tagg O
, O
2011 O
) O
to O
fool O
online O
detection O
models O
. O
To O
identify O
plausible O
such O
attack O
scenarios O
, O
human O
perceptual O
abilities O
play O
a O
decisive O
role O
. O
For O
instance O
, O
humans O
are O
guided O
by O
their O
senses O
, O
making O
them O
robust O
to O
, O
e.g. O
, O
visual O
and O
phonetic O
attacks O
. O
Other O
scenarios O
to O
which O
humans O
have O
been O
shown O
robust O
include O
the O
removal O
of O
vowels O
from O
words O
or O
the O
shufÔ¨Çing O
of O
characters O
while O
keeping O
the O
initial O
and O
Ô¨Ånal O
letters O
Ô¨Åxed O
( O
see O
Section O
2 O
) O
. O
However O
, O
the O
varieties O
in O
which O
text O
can O
be O
perturbed O
is O
certainly O
far O
from O
inÔ¨Ånite O
, O
as O
( O
ordinary O
) O
humans O
, O
with O
all O
their O
cognitive O
constraints O
, O
still O
need O
to O
be O
able O
to O
decipher O
the O
text O
messages O
. O
In O
this O
work O
, O
we O
provide O
the O
Ô¨Årst O
large O
- O
scale O
catalogue O
for O
low O
- O
level O
( O
orthographic O
) O
attack O
scenarios O
. O
Our O
search O
is O
motivated O
by O
insights O
into O
human O
cognitive O
limitations O
and O
constraints O
and O
encompasses O
nine O
different O
attack O
modes O
( O
some O
of O
which O
are O
overlapping O
) O
; O
cf O
. O
Table O
1 O
. O
We O
then O
examine O
the O
robustness O
of O
RoBERTa O
( O
Liu O
et O
al O
. O
, O
2019 O
) O
to O
our O
attacks O
, O
Ô¨Ånding O
that O
its O
performance O
can O
sometimes O
be O
severely O
decreased O
for O
our O
selection O
of O
attackers O
( O
up O
to O
the O
random O
guessing O
baseline O
) O
; O
hence O
we O
call O
our O
benchmark O
Z¬¥eroe O
. O
The O
reason O
may O
be O
that O
our O
noises O
are O
not O
always O
natural O
, O
in O
the O
sense O
of O
having O
high O
support O
in O
large O
datasets O
such O
as O
CommonCrawl O
or O
Wikipedia O
, O
but O
they O
are O
still O
within O
the O
limits O
of O
cognitive O
abilities O
of O
ordinary O
humans O
. O
Finally O
, O
we O
show O
that O
under O
realistic O
conditions O
, O
standard O
adversarial O
training O
can O
restore786Attacker O
Sentence O
inner O
- O
shufÔ¨Çe O
Aadrreavsil O
aacttks O
are O
hmarsels O
. O
full O
- O
shufÔ¨Çe O
idaAasvrler O
tstkaac O
are O
harmless O
. O
intrude O
A O
d O
v O
e O
r O
sar O
ial O
at O
: O
ta O
: O
ck O
: O
s O
are O
h}ar}m}less O
. O
disemvowel O
dvrsrl O
ttcks O
r O
hrmlss O
. O
truncate O
Adversaria O
attack O
are O
harmles O
. O
segment O
Adversarial O
attacksare O
harmless O
. O
typo O
Adverssrial O
attaxks O
are O
harmless O
. O
natural O
noise O
Adversarial O
attacs O
rae O
harmless O
. O
phonetic O
Advorcariel O
attaks O
are O
harmless O
. O
visual¬Ø√ÑÀôdU√ársar¬Ø O
ƒ±a O
√´at¬Øtack.s.¬Ø O
aRe O
h¬Ø√§r√Æ√èÀú O
es O
.. O
Table O
1 O
: O
Ten O
different O
modiÔ¨Åcations O
of O
the O
sentence O
‚Äú O
Adversarial O
attacks O
are O
harmless O
. O
‚Äù O
RoBERTa O
‚Äôs O
performance O
only O
to O
a O
limited O
degree.1 O
2 O
Related O
Work O
We O
classify O
adversarial O
attacks O
into O
high- O
andlowlevel O
attacks.2 O
Attack O
Scenarios O
. O
There O
are O
a O
variety O
of O
works O
that O
introduce O
low O
- O
level O
orthographic O
attacks.3 O
Ebrahimi O
et O
al O
. O
( O
2017 O
) O
trick O
a O
character O
- O
level O
neural O
text O
classiÔ¨Åcation O
model O
by O
Ô¨Çipping O
the O
characters O
which O
cause O
most O
damage O
. O
Their O
approach O
is O
whitebox O
, O
i.e. O
, O
assumes O
access O
to O
the O
attack O
model O
‚Äôs O
parameters O
. O
Eger O
et O
al O
. O
( O
2019 O
) O
exchange O
characters O
with O
similar O
looking O
ones O
and O
show O
that O
humans O
are O
robust O
to O
such O
visual O
perturbations O
, O
while O
machines O
may O
suffer O
severe O
performance O
drops O
. O
Belinkov O
and O
Bisk O
( O
2017 O
) O
exchange O
adjacent O
letters O
on O
the O
keyboard O
with O
each O
other O
( O
keyboard O
typos O
) O
and O
introduce O
natural O
noise O
based O
on O
human O
typing O
errors O
extracted O
from O
different O
Wikipedia O
edit O
histories O
, O
as O
well O
as O
letter O
swaps O
. O
They O
use O
this O
natural O
and O
1Code O
and O
data O
are O
provided O
at O
https://github O
. O
com O
/ O
yannikbenz O
/ O
zeroe O
. O
2As O
one O
reviewer O
points O
out O
, O
a O
conceptual O
difference O
between O
high- O
and O
low O
- O
level O
attacks O
is O
that O
low O
- O
level O
attacks O
( O
as O
we O
deÔ¨Åne O
them O
) O
oftentimes O
induce O
linguistically O
corrupt O
text O
which O
can O
still O
be O
understood O
by O
humans O
, O
while O
highlevel O
attacks O
operate O
in O
a O
noise O
- O
free O
environment O
to O
show O
the O
brittleness O
of O
systems O
even O
under O
‚Äò O
normal O
‚Äô O
circumstances O
. O
3Low O
- O
level O
adversarial O
attacks O
are O
in O
part O
examined O
by O
approaches O
to O
handle O
noisy O
user O
- O
generated O
text O
( O
Baldwin O
et O
al O
. O
, O
2015 O
) O
, O
with O
one O
difference O
being O
that O
attacks O
are O
often O
malicious O
in O
nature O
and O
may O
thus O
come O
in O
different O
forms.synthetic O
noise O
to O
show O
the O
brittleness O
of O
machine O
translation O
( O
MT O
) O
systems O
, O
which O
contrasts O
with O
corresponding O
human O
robustness O
. O
Ebrahimi O
et O
al O
. O
( O
2018 O
) O
also O
fool O
MT O
systems O
with O
character O
- O
level O
modiÔ¨Åcations O
. O
Tan O
et O
al O
. O
( O
2020 O
) O
attack O
words O
by O
replacing O
them O
with O
morphological O
variants O
, O
which O
also O
mostly O
results O
in O
orthographic O
attacks O
( O
in O
English O
) O
. O
High O
- O
level O
attacks O
require O
a O
deeper O
understanding O
of O
the O
meaning O
and O
the O
syntactical O
structure O
of O
the O
sentence O
. O
Jin O
et O
al O
. O
( O
2019 O
) O
generate O
semantically O
similar O
and O
syntactically O
correct O
adversarial O
examples O
by O
replacing O
words O
with O
suitable O
synonyms O
. O
Hosseini O
et O
al O
. O
( O
2017 O
) O
and O
Rodriguez O
and O
RojasGaleano O
( O
2018 O
) O
attack O
toxic O
detection O
systems O
by O
obfuscation O
, O
i.e. O
, O
misspelling O
of O
the O
abusive O
words O
( O
a O
low O
- O
level O
attack O
) O
, O
and O
via O
polarization O
, O
i.e. O
, O
inverting O
the O
meaning O
of O
the O
sentences O
by O
inserting O
the O
word O
‚Äú O
not O
‚Äù O
. O
Alzantot O
et O
al O
. O
( O
2018 O
) O
introduce O
an O
optimization O
- O
based O
algorithm O
to O
generate O
adversarial O
examples O
by O
replacing O
words O
in O
the O
input O
. O
Their O
generated O
words O
are O
semantically O
similar O
because O
they O
are O
nearest O
neighbors O
in O
the O
GloVe O
embedding O
space O
. O
They O
are O
also O
syntactically O
correct O
because O
they O
need O
to O
Ô¨Åt O
into O
the O
surrounding O
context O
with O
respect O
to O
the O
1 O
billion O
words O
language O
model O
. O
Iyyer O
et O
al O
. O
( O
2018 O
) O
generate O
syntactically O
correct O
paraphrases O
for O
a O
sentence O
. O
Ribeiro O
et O
al O
. O
( O
2018 O
) O
use O
MT O
backtranslation O
to O
produce O
meaning O
- O
preserving O
adversaries O
. O
They O
generate O
adversarial O
examples O
for O
machine O
comprehension O
, O
sentiment O
analysis O
and O
visual O
question O
answering O
to O
show O
robustness O
issues O
in O
state O
- O
of O
- O
the O
- O
art O
models O
for O
each O
task O
. O
Jia O
and O
Liang O
( O
2017 O
) O
insert O
semantically O
correct O
but O
irrelevant O
paragraphs O
into O
texts O
to O
fool O
neural O
reading O
comprehension O
models O
. O
Robustness O
. O
Adversarial O
training O
is O
a O
commonly O
used O
technique O
to O
address O
adversarial O
attacks O
( O
Szegedy O
et O
al O
. O
, O
2014 O
) O
. O
The O
term O
may O
refer O
to O
calculating O
model O
gradients O
with O
respect O
to O
the O
input O
and O
inserting O
new O
training O
examples O
based O
on O
this O
gradient O
( O
Goodfellow O
et O
al O
. O
, O
2014 O
) O
. O
Alternatively O
, O
adversaries O
obtained O
from O
the O
attacker O
are O
inserted O
at O
train O
time O
( O
Belinkov O
and O
Bisk O
, O
2017 O
; O
Alzantot O
et O
al O
. O
, O
2018 O
; O
Eger O
et O
al O
. O
, O
2019 O
) O
. O
3 O
Catalogue O
of O
Attacks O
We O
propose O
a O
catalogue O
of O
ten O
different O
attacks O
. O
Our O
intention O
is O
to O
suggest O
a O
maximally O
inclusive O
list O
of O
potential O
attacks O
under O
the O
constraint O
that O
humans O
are O
robust O
to O
them.7873.1 O
Attack O
protocol O
Our O
attack O
protocol O
is O
black O
- O
box O
andnon O
- O
targeted O
( O
Xu O
et O
al O
. O
, O
2019 O
): O
we O
do O
not O
assume O
access O
to O
model O
parameters O
and O
our O
goal O
is O
to O
fool O
the O
system O
without O
any O
desired O
outcome O
in O
mind O
‚Äî O
in O
contrast O
, O
a O
spammer O
would O
want O
spam O
emails O
to O
be O
misclassiÔ¨Åed O
as O
non O
- O
spam O
, O
but O
not O
necessarily O
the O
reverse O
. O
We O
parameterize O
attack O
levels O
by O
a O
perturbation O
probabilityp‚àà[0,1 O
] O
. O
Withp O
, O
our O
goal O
is O
to O
attack O
p¬∑100 O
% O
of O
all O
tokens O
in O
each O
sample O
in O
our O
dataset O
. O
To O
do O
so O
, O
for O
each O
sample O
w= O
( O
x1, O
... O
,xn O
) O
, O
we O
randomly O
and O
without O
replacement O
draw O
a O
token O
indexito O
perturb O
. O
We O
independently O
Ô¨Çip O
a O
coin O
with O
tail O
probability O
pto O
determine O
whether O
the O
tokenxishould O
be O
attacked O
. O
We O
do O
so O
until O
either O
p¬∑100 O
% O
of O
all O
tokens O
in O
ware O
perturbed O
or O
else O
if O
there O
are O
no O
more O
indices O
left O
. O
3.2 O
Attacks O
Some O
of O
our O
attacks O
, O
each O
of O
which O
operates O
on O
the O
character O
- O
level O
of O
an O
attacked O
word O
, O
are O
parametrized O
by O
a O
character O
- O
level O
perturbation O
probabilityœÜ O
. O
For O
simplicity O
, O
we O
set O
œÜ O
= O
pthroughout O
, O
wherepis O
the O
above O
deÔ¨Åned O
word O
level O
perturbation O
probability O
. O
Inner O
ShufÔ¨Çe O
. O
This O
randomly O
shufÔ¨Çes O
all O
letters O
in O
a O
word O
except O
for O
the O
Ô¨Årst O
and O
last O
. O
This O
attacks O
builds O
on O
the O
human O
ability O
to O
still O
comprehend O
words O
if O
the O
Ô¨Årst O
and O
last O
letter O
remain O
intact O
( O
Rayner O
et O
al O
. O
, O
2006 O
) O
. O
We O
only O
allow O
change O
in O
words O
with O
length‚â•3 O
. O
Full O
ShufÔ¨Çe O
. O
This O
is O
the O
extreme O
case O
of O
the O
inner O
- O
shufÔ¨Çe O
perturbation O
where O
the O
constraint O
relating O
to O
initial O
and O
Ô¨Ånal O
letters O
is O
dropped O
. O
We O
include O
this O
attack O
for O
completeness O
, O
even O
though O
we O
do O
not O
assume O
high O
degrees O
of O
human O
robustness O
to O
it O
. O
We O
apply O
this O
to O
all O
words O
with O
length O
‚â•2 O
. O
Intruders O
. O
Inserting O
unobtrusive O
symbols O
( O
Hosseini O
et O
al O
. O
, O
2017 O
) O
in O
words O
is O
a O
typical O
phenomenon O
in O
social O
media O
, O
e.g. O
, O
to O
avoid O
censorship O
. O
Depending O
on O
the O
symbols O
chosen O
, O
an O
attack O
may O
have O
little O
effect O
on O
humans O
. O
We O
choose O
the O
inserted O
symbol O
randomly O
but O
in O
case O
of O
multiple O
insertions O
into O
one O
word O
keep O
the O
symbol O
identical O
. O
We O
allow O
the O
following O
symbols O
to O
be O
inserted O
: O
! O
‚Äù O
# O
$ O
% O
& O
/prime()‚àó+,‚àí./:;<=>?@[\]ÀÜ‚Äò{| O
} O
, O
including O
whitespace O
. O
The O
perturbation O
probability O
œÜadditionally O
inÔ¨Çuences O
the O
number O
of O
insertions O
taking O
place O
. O
For O
each O
two O
characters O
, O
œÜindicates O
howlikely O
the O
insertion O
of O
a O
symbol O
between O
them O
is O
. O
We O
apply O
this O
attack O
to O
all O
words O
with O
length O
‚â•3 O
. O
Disemvoweling O
. O
This O
removes O
all O
vowels O
( O
a O
, O
e O
, O
i O
, O
o O
, O
u O
) O
from O
a O
word O
. O
If O
a O
word O
only O
consists O
of O
vowels O
, O
it O
will O
be O
ignored O
to O
prevent O
it O
from O
being O
deleted O
. O
Words O
with O
length O
‚â§3are O
skipped O
to O
maintain O
readability O
. O
Disemvoweling O
is O
a O
common O
feature O
of O
SMS O
language O
and O
on O
social O
media O
presumed O
to O
require O
little O
cognitive O
effort O
for O
humans O
( O
Boyd O
et O
al O
. O
, O
2010 O
) O
. O
Truncating O
. O
This O
removes O
a O
Ô¨Åxed O
number O
of O
letters O
from O
the O
back O
of O
a O
word O
. O
We O
only O
cut O
the O
last O
letter O
from O
words O
of O
length O
‚â•3to O
maintain O
readability O
. O
Predicting O
word O
endings O
from O
beginnings O
is O
considered O
an O
easy O
task O
for O
humans O
( O
Elman O
, O
1995 O
) O
. O
Segmentation O
. O
This O
joins O
multiple O
words O
together O
into O
one O
word O
. O
Here O
, O
the O
perturbation O
level O
is O
the O
probability O
to O
merge O
the O
Ô¨Årst O
two O
adjacent O
words O
. O
Each O
following O
word O
gets O
a O
lower O
probability O
to O
get O
merged O
( O
œÜ2, O
... O
,œÜn O
) O
to O
prevent O
‚Äò O
giant O
‚Äô O
words O
. O
We O
do O
not O
apply O
this O
attack O
to O
sequence O
tagging O
tasks O
such O
as O
POS O
, O
because O
the O
joined O
words O
would O
have O
no O
proper O
tag O
, O
making O
evaluation O
more O
difÔ¨Åcult O
. O
The O
ability O
of O
humans O
to O
segment O
unsegmented O
input O
is O
already O
acquired O
during O
infancy O
( O
Goldwater O
et O
al O
. O
, O
2009 O
) O
. O
Keyboard O
Typos O
. O
We O
adopt O
this O
attack O
from O
Belinkov O
and O
Bisk O
( O
2017 O
) O
and O
adapt O
it O
to O
our O
workÔ¨Çow O
. O
Hereby O
, O
adjacent O
letters O
on O
the O
English O
keyboard O
are O
replaced O
by O
each O
other O
randomly O
. O
This O
simulates O
human O
typing O
errors O
. O
The O
higher O
the O
perturbation O
probability O
œÜ O
, O
the O
more O
characters O
are O
exchanged O
by O
adjacent O
letters O
. O
Natural O
Typos O
. O
Words O
are O
replaced O
by O
natural O
human O
errors O
from O
the O
Wikipedia O
edit O
history O
( O
Belinkov O
and O
Bisk O
, O
2017 O
) O
which O
contains O
multiple O
sources O
of O
error O
: O
phonetic O
errors O
, O
omissions O
, O
morphological O
errors O
, O
key O
- O
swap O
errors O
and O
combinations O
of O
them O
. O
Phonetic O
. O
An O
ideal O
phonetic O
attack O
leaves O
the O
pronunication O
of O
a O
word O
intact O
but O
alters O
its O
spelling O
. O
Phonetic O
attacks O
are O
common O
especially O
in O
English O
with O
its O
irregular O
mapping O
of O
pronunciation O
and O
spelling O
. O
They O
do O
not O
only O
occur O
as O
mistakes O
but O
also O
as O
a O
form O
of O
creative O
language O
use O
( O
Tagg O
, O
2011 O
) O
. O
Visual O
. O
Visual O
attacks O
are O
based O
on O
the O
idea O
that O
humans O
may O
easily O
recognize O
similar O
looking O
sym-788bols O
( O
Eger O
et O
al O
. O
, O
2019 O
) O
. O
We O
replace O
each O
character O
in O
the O
input O
sequence O
with O
one O
of O
its O
20 O
visual O
nearest O
neighbors O
in O
the O
visual O
space O
deÔ¨Åned O
below O
. O
This O
attack O
is O
also O
parameterized O
by O
œÜ O
: O
we O
replace O
each O
letter O
in O
a O
word O
i.i.d O
. O
randomly O
with O
probabilityœÜ O
. O
We O
observe O
that O
our O
attacks O
are O
notdirectly O
comparable O
. O
For O
example O
, O
at O
some O
perturbation O
level O
p O
, O
truncate O
removes O
O(p¬∑n)characters O
, O
where O
n O
is O
sentence O
length O
. O
In O
contrast O
, O
intruders O
inserts O
O(p2¬∑n¬∑m)characters O
, O
where O
mis O
a O
bound O
on O
word O
length O
. O
3.3 O
Implementation O
of O
Visual O
and O
Phonetic O
Attacks O
We O
describe O
details O
of O
phonetic O
and O
visual O
attacks O
below O
, O
as O
they O
are O
more O
involved O
. O
Phonetic O
Embeddings O
and O
Attacks O
. O
In O
order O
to O
replace O
words O
by O
phonetically O
similar O
ones O
, O
we O
use O
two O
stages O
. O
First O
, O
we O
train O
two O
Seq2Seq O
models O
to O
translate O
a O
letter O
string O
into O
its O
phonetic O
representation O
and O
vice O
versa O
. O
We O
use O
the O
Combilex O
dataset O
to O
do O
so O
( O
Richmond O
et O
al O
. O
, O
2010 O
) O
. O
In O
addition O
to O
that O
, O
we O
induce O
phonetic O
word O
representations O
, O
i.e. O
, O
a O
vector O
space O
where O
two O
words O
are O
close O
if O
they O
are O
pronounced O
alike O
. O
We O
use O
an O
InferSent O
- O
like O
architecture O
to O
do O
so O
( O
Conneau O
et O
al O
. O
, O
2017 O
) O
. O
Details O
are O
given O
in O
the O
appendix O
. O
When O
a O
word O
x O
should O
be O
phonetically O
perturbed O
, O
we O
run O
the O
Ô¨Årst O
Seq2Seq O
model O
to O
obtain O
a O
phonemic O
representation O
and O
then O
convert O
this O
back O
to O
a O
letter O
string O
Àúx O
( O
as O
in O
backtranslation O
in O
MT O
) O
. O
We O
Ô¨Ånally O
keep O
Àúx O
when O
it O
is O
phonetically O
similar O
to O
x. O
We O
added O
the O
latter O
step O
because O
we O
observed O
that O
some O
resulting O
words O
Àúxhad O
very O
different O
pronunciation O
than O
x O
after O
the O
backtranslation O
. O
Visual O
embeddings O
. O
In O
order O
to O
generate O
visual O
character O
embeddings O
, O
we O
used O
an O
architecture O
introduced O
by O
Larsen O
et O
al O
. O
( O
2016 O
) O
as O
a O
combination O
of O
GAN O
and O
V O
AE O
, O
called O
V O
AEGAN O
. O
The O
model O
is O
able O
to O
learn O
embeddings O
which O
encode O
highlevel O
abstract O
features O
. O
This O
property O
is O
desirable O
in O
our O
case O
, O
because O
humans O
rely O
on O
abstract O
features O
( O
Dehaene O
and O
Cohen O
, O
2011 O
) O
, O
i.e. O
, O
shape O
and O
spatial O
relation O
of O
the O
letter O
, O
instead O
of O
pixels O
while O
reading O
. O
The O
model O
is O
described O
in O
the O
appendix O
. O
To O
obtain O
visual O
character O
embeddings O
, O
we O
generate O
a O
grayscale O
image O
of O
size O
24√ó24for O
each O
character O
in O
the O
Basic O
Multilingual O
Plane O
( O
BMP O
; O
65k O
characters O
) O
of O
the O
standard O
Unicode O
character O
set O
with O
Pillow O
. O
The O
V O
AEGAN O
is O
trained O
onthe O
full O
BMP O
dataset O
. O
After O
training O
, O
we O
compute O
256 O
- O
dimensional O
visual O
letter O
embeddings O
by O
encoding O
the O
respective O
letter O
image O
with O
the O
encoder O
of O
the O
V O
AEGAN O
. O
The O
quality O
of O
the O
embeddings O
can O
be O
derived O
via O
the O
models O
‚Äô O
ability O
to O
properly O
reconstruct O
an O
image O
from O
them O
, O
see O
Figure O
7 O
in O
the O
appendix O
. O
4 O
Experimental O
Setup O
4.1 O
Base O
model O
and O
datasets O
Our O
base O
architecture O
used O
in O
all O
experiments O
is O
RoBERTa O
( O
Liu O
et O
al O
. O
, O
2019 O
) O
. O
RoBERTa O
is O
a O
robustly O
optimized O
extension O
of O
BERT O
that O
has O
been O
trained O
( O
i O
) O
for O
longer O
, O
( O
ii O
) O
on O
more O
data O
, O
and O
( O
iii O
) O
without O
the O
next O
sentence O
prediction O
task O
. O
RoBERTa O
has O
been O
shown O
to O
outperform O
BERT O
on O
a O
variety O
of O
benchmark O
tasks O
, O
including O
those O
contained O
in O
GLUE O
( O
Wang O
et O
al O
. O
, O
2018 O
) O
. O
We O
study O
the O
performance O
of O
RoBERTa O
in O
our O
attack O
scenarios O
on O
three O
different O
NLP O
tasks O
. O
Dataset O
statistics O
are O
shown O
in O
Table O
2 O
. O
POS O
tagging O
is O
a O
sequence O
tagging O
task O
where O
each O
token O
in O
the O
input O
needs O
to O
be O
labeled O
with O
its O
respective O
POS O
tag O
. O
We O
use O
the O
English O
universal O
dependency O
dataset O
with O
17 O
different O
tags O
( O
Nivre O
et O
al O
. O
, O
2016 O
) O
. O
NLI O
is O
a O
classiÔ¨Åcation O
task O
in O
which O
the O
relation O
of O
a O
sentence O
pair O
must O
be O
predicted O
. O
Relation O
labels O
are O
neutral O
, O
contradiction O
andentailment O
. O
We O
use O
SNLI O
( O
Bowman O
et O
al O
. O
, O
2015 O
) O
. O
Toxic O
Comment O
ClassiÔ¨Åcation O
( O
TC O
) O
labels O
sentences O
( O
typically O
from O
social O
media O
platforms O
) O
with O
one O
or O
several O
toxicity O
classes O
. O
Possible O
labels O
are O
: O
toxic O
, O
obscene O
, O
threat O
, O
insult O
andidentity O
hate O
. O
For O
this O
task O
, O
we O
choose O
the O
jigsaw O
toxic O
comment O
challenge O
dataset O
from O
kaggle4 O
. O
The O
current O
best O
performance O
on O
the O
leaderboard O
has O
an O
AUCROC O
( O
area O
under O
the O
receiver O
operations O
characteristic O
curve O
) O
score O
of O
98.8 O
% O
. O
4.2 O
Results O
We O
consider O
the O
cases O
of O
low(p= O
0.2),mid(p= O
0.5 O
) O
and O
high O
( O
p= O
0.8 O
) O
attack O
levels O
. O
In O
Figure O
1 O
, O
we O
plot O
the O
performance O
of O
RoBERTa O
for O
the O
three O
tasks O
POS O
, O
NLI O
and O
TC O
individually O
as O
we O
perturb O
the O
test O
data O
using O
our O
attackers O
. O
Detailed O
numbers O
are O
reported O
in O
Table O
6 O
4https://www.kaggle.com/c/jigsaw-toxic-commentclassiÔ¨Åcation-challenge789Task O
Dataset O
Train O
Test O
Clean O
score O
POS O
Tagging O
Universal O
Dependencies O
( O
part O
) O
13k O
2k O
96.95 O
NLI O
Stanford O
Natural O
Language O
Inference O
550k O
10k O
90.41 O
Multilabel O
Toxic O
Comment O
560k O
234k O
0.93 O
ClassiÔ¨Åcation O
Table O
2 O
: O
Overview O
of O
the O
NLP O
tasks O
used O
in O
this O
work O
. O
Clean O
scores O
are O
scores O
from O
training O
and O
testing O
on O
clean O
data O
. O
none O
low O
mid O
high00.10.20.30.40.50.60.70.80.91 O
Attack O
Levels‚àó(p)POS O
none O
low O
mid O
high00.10.20.30.40.50.60.70.80.91 O
Attack O
Levels‚àó(p)NLI O
none O
low O
mid O
high0.50.60.70.80.91 O
Attack O
Levels‚àó(p)TCFull O
- O
ShuÔ¨Ñe O
Inner O
- O
ShuÔ¨Ñe O
Intrude O
Disemvowel O
Truncate O
Segment O
Keyboard O
- O
Typo O
Natural O
- O
Noise O
Phonetic O
Visual O
Figure O
1 O
: O
Performance O
decreases O
of O
RoBERTa O
on O
the O
three O
downstream O
tasks O
: O
POS O
, O
SNLI O
an O
TC O
. O
Red O
lines O
indicate O
the O
random O
guessing O
baseline O
. O
in O
the O
appendix O
. O
We O
report O
scores O
relative O
to O
the O
model O
performances O
on O
the O
clean O
test O
set O
: O
s‚àó(p O
) O
= O
s(p O
) O
s(0 O
) O
, O
p‚àà{0,0.2,0.5,0.8}(1 O
) O
wheres(0)is O
the O
task O
speciÔ¨Åc O
performance O
on O
clean O
data O
listed O
in O
Table O
2 O
and O
s(p)is O
the O
performance O
for O
attack O
level O
p. O
Scores O
are O
measured O
in O
accuracy O
for O
POS O
and O
NLI O
, O
and O
in O
AUCROC O
for O
TC O
classiÔ¨Åcation O
. O
Clean O
performance O
scores O
depend O
on O
the O
speciÔ¨Åc O
task O
and O
dataset O
. O
For O
example O
, O
NLI O
has O
a O
worst O
score O
of O
around O
33 O
% O
accuracy O
( O
majority O
label O
) O
and O
POS O
has O
a O
corresponding O
worst O
score O
of O
around O
16 O
% O
accuracy O
. O
The O
worst O
performance O
of O
TC O
is O
reached O
at O
AUCROC O
score O
of O
50%‚Äîat O
this O
point O
, O
the O
model O
is O
no O
longer O
able O
to O
distinguish O
between O
the O
different O
classes O
. O
We O
mark O
these O
values O
relative O
to O
the O
tasks O
‚Äô O
best O
performance O
( O
s(0 O
) O
) O
in O
Figure O
1 O
as O
red O
lines O
. O
Each O
task O
suffers O
performance O
decreases O
from O
each O
attacker O
. O
The O
higher O
the O
perturbation O
level O
, O
the O
lower O
the O
model O
performance O
. O
Thephonetic O
attack O
is O
the O
least O
effective O
for O
all O
tasks O
with O
maximally O
10 O
percentage O
points O
( O
pp O
) O
performance O
decrease O
with O
the O
highest O
perturbation O
probability O
of O
0.8 O
. O
The O
truncate O
attack O
yields O
higher O
performances O
decreases O
in O
all O
three O
tasks O
, O
being O
roughly O
twice O
as O
effective O
. O
The O
performance O
decreases O
by O
10pp O
from O
none O
tolowand O
additional O
10pp O
from O
lowtomid O
. O
Increasing O
the O
attack O
level O
beyond O
that O
does O
not O
cause O
further O
harm O
, O
especially O
for O
NLI O
and O
TC O
. O
Concerning O
the O
segmentation O
attack O
, O
for O
NLI O
, O
it O
leads O
to O
a O
similar O
performance O
decrease O
as O
the O
truncate O
attack O
for O
smallp O
, O
but O
becomes O
more O
successful O
as O
the O
perturbation O
level O
increases O
to O
midandhigh O
. O
For O
TC O
, O
the O
performance O
decrease O
is O
almost O
identical O
to O
the O
phonetic O
attack O
. O
We O
notice O
a O
linear O
decrease O
in O
performance O
for O
each O
task O
when O
increasing O
the O
perturbation O
level O
of O
the O
natural O
- O
noise O
attack O
. O
Especially O
POS O
and O
NLI O
suffer O
a O
strong O
performance O
deterioration O
of O
around O
40pp O
and O
50pp O
for O
the O
highest O
attack O
level O
. O
Both O
lose O
15pp O
to O
20pp O
performance O
per O
attack O
level O
increase O
. O
Full- O
andinner O
- O
shufÔ¨Çe O
randomize O
the O
order O
in O
an O
input O
word O
but O
humans O
are O
more O
robust O
to O
inner O
- O
shufÔ¨Çe O
. O
Full O
- O
shufÔ¨Çe O
also O
affects O
RoBERTa O
more O
than O
inner O
- O
shufÔ¨Çe O
. O
It O
tends O
to O
be O
one O
of O
the O
strongest O
attack O
scenarios O
, O
while O
inner O
- O
shufÔ¨Çe O
typically O
ranks O
in O
the O
midÔ¨Åeld O
. O
Thedisemvowel O
attack O
has O
different O
effects O
in O
different O
tasks O
. O
For O
POS O
, O
it O
is O
almost O
identical O
to O
the O
natural O
- O
noise O
attack O
with O
a O
slightly O
stronger O
impact O
of O
5pp O
for O
midand O
3pp O
for O
high O
and O
a O
maxi-790mum O
on O
50pp O
. O
NLI O
loses O
around O
20pp O
performance O
onlowand O
it O
decreases O
an O
additional O
20pp O
by O
increasing O
the O
level O
to O
mid O
, O
and O
reaches O
its O
greatest O
decrease O
by O
55pp O
on O
high O
. O
In O
TC O
, O
model O
performances O
decrease O
linearly O
from O
none O
tolowandmid O
by O
8pp O
each O
. O
The O
high O
attack O
level O
doubles O
to O
a O
total O
of O
15pp O
performance O
loss O
. O
The O
keyboard O
- O
typo O
attacks O
have O
median O
impact O
throughout O
tasks O
and O
attack O
levels O
. O
Theintrude O
attack O
is O
among O
the O
most O
severe O
attacks O
across O
all O
three O
tasks O
. O
For O
TC O
, O
the O
lowand O
midattack O
levels O
have O
a O
relatively O
low O
impact O
compared O
to O
high O
which O
yields O
a O
performance O
loss O
of O
30pp O
. O
It O
decreases O
model O
performance O
the O
most O
on O
the O
POS O
task O
by O
above O
80pp O
. O
Especially O
for O
both O
sentence O
- O
based O
tasks O
NLI O
and O
TC O
, O
the O
visual O
attack O
decreases O
are O
also O
among O
the O
most O
severe O
, O
while O
RoBERTa O
is O
marginally O
more O
robust O
on O
the O
POS O
task O
. O
Even O
for O
the O
lowperturbation O
level O
, O
the O
NLI O
model O
suffers O
from O
more O
than O
40pp O
performance O
decrease O
. O
The O
performance O
for O
high O
peven O
falls O
below O
the O
red O
line O
marked O
as O
our O
lower O
bound O
baseline O
. O
4.3 O
Defenses O
In O
the O
following O
, O
we O
report O
the O
performance O
increase O
from O
shielding O
the O
methods O
with O
adversarial O
training O
: O
‚àÜœÑ(p O
) O
: O
= O
œÉ(p O
) O
s(0)‚àís‚àó(p O
) O
( O
2 O
) O
whereœÉ(p)is O
the O
score O
for O
each O
task O
with O
one O
of O
two O
defense O
methods O
œÑ O
: O
‚Ä¢1 O
- O
1 O
adversarial O
training O
( O
Œ± O
, O
Œ≤ O
): O
Here O
, O
we O
train O
on O
a O
mixture O
of O
low O
, O
mid O
, O
high O
attacked O
data O
( O
each O
perturbation O
level O
is O
roughly O
equally O
likely O
to O
appear O
in O
the O
training O
data O
) O
. O
We O
attack O
with O
some O
attacker O
Œ±and O
measure O
performance O
when O
the O
test O
data O
is O
attacked O
with O
attacker O
Œ≤ O
. O
‚Ä¢leave O
- O
one O
- O
out O
( O
LOO O
): O
Here O
, O
we O
train O
on O
a O
mix O
of O
all O
attackers O
except O
for O
the O
one O
with O
which O
the O
test O
data O
is O
attacked O
. O
The O
train O
data O
contains O
an O
equal O
mix O
of O
data O
from O
each O
attacker O
and O
attack O
level O
. O
4.3.1 O
Adversarial O
Training O
1 O
- O
1 O
( O
Œ± O
, O
Œ±)In O
Figure O
2 O
, O
we O
report O
the O
performance O
of O
our O
models O
each O
trained O
on O
perturbed O
data O
and O
evaluated O
against O
the O
same O
kind O
of O
perturbation O
. O
This O
gives O
an O
unrealistic O
upper O
bound O
since O
the O
defender O
would O
have O
to O
know O
how O
it O
is O
being O
attacked O
. O
For O
POS O
, O
the O
adversarially O
trained O
models O
lose O
a O
bit O
of O
their O
performance O
on O
clean O
data O
, O
but O
their O
performance O
on O
perturbed O
data O
improves O
, O
especially O
against O
intrude O
and O
truncate O
for O
the O
lowattack O
level O
. O
The O
robustness O
improvements O
for O
the O
remaining O
attackers O
are O
very O
similar O
and O
range O
from O
3pp O
increase O
for O
the O
natural O
- O
noise O
attack O
to O
8pp O
for O
the O
disemvowel O
attack O
. O
With O
one O
exception O
, O
the O
improvement O
at O
large O
perturbation O
levels O
pis O
highest O
, O
and O
obtains O
a O
maximum O
improvement O
of O
40pp O
for O
inner O
- O
shufÔ¨Çe O
. O
For O
NLI O
, O
the O
models O
again O
tend O
perform O
worse O
on O
clean O
data O
. O
As O
the O
perturbation O
level O
increases O
, O
we O
see O
a O
smooth O
and O
steady O
increase O
of O
the O
values O
‚àÜœÑ(p)across O
all O
attackers O
. O
Improvement O
is O
best O
for O
intrude O
which O
was O
also O
among O
the O
most O
damaging O
attacks O
. O
For O
TC O
, O
model O
performances O
increase O
also O
on O
clean O
data O
, O
which O
is O
likely O
due O
to O
the O
nature O
of O
the O
task O
. O
As O
the O
attack O
level O
increases O
, O
‚àÜœÑ(p)gradually O
further O
increases O
across O
tasks O
. O
For O
high O
, O
largest O
increase O
is O
again O
observed O
for O
intrude O
as O
well O
as O
for O
visual O
, O
which O
also O
had O
largest O
impact O
in O
the O
non O
- O
shielded O
setting O
. O
1 O
- O
1 O
( O
Œ± O
, O
Œ≤)In O
Figure O
3 O
, O
we O
show O
all O
1 O
- O
1 O
values O
for O
different O
combination O
of O
attackers O
on O
train O
( O
Œ± O
) O
and O
test O
data O
( O
Œ≤ O
) O
. O
We O
see O
that O
the O
diagonal O
( O
Œ± O
= O
Œ≤ O
) O
always O
proÔ¨Åts O
considerably O
, O
but O
the O
off O
- O
diagonal O
can O
be O
positive O
or O
negative O
, O
depending O
on O
the O
choice O
ofŒ±andŒ≤ O
. O
We O
clearly O
see O
that O
( O
1 O
) O
truncate O
, O
disemvowel O
, O
keyboard O
- O
typo O
, O
natural O
noise O
, O
visual O
, O
and O
intruders O
are O
similar O
in O
the O
sense O
that O
training O
on O
them O
shields O
against O
their O
attacks O
at O
test O
time O
. O
( O
2 O
) O
Full O
- O
shufÔ¨Çe O
and O
inner O
- O
shufÔ¨Çe O
form O
a O
second O
group O
and O
( O
3 O
) O
phonetic O
attacks O
a O
third O
group O
. O
This O
is O
to O
some O
degree O
a O
natural O
clustering O
, O
as O
( O
1 O
) O
removes O
or O
replaces O
characters O
, O
( O
2 O
) O
destroys O
the O
order O
of O
words O
, O
and O
( O
3 O
) O
modiÔ¨Åes O
entire O
words O
using O
more O
complex O
operations O
. O
visual O
is O
an O
outlier O
in O
group O
( O
1 O
) O
, O
since O
it O
improves O
no O
matter O
what O
attacks O
are O
added O
at O
train O
time O
. O
Leave O
- O
One O
- O
Out O
Figure O
4 O
shows O
the O
performance O
of O
our O
models O
when O
trained O
on O
a O
mixture O
of O
all O
attackers O
except O
the O
one O
evaluated O
on O
. O
This O
is O
the O
most O
plausible O
scenario O
of O
model O
defense O
in O
the O
case O
of O
an O
unknown O
new O
attack O
scenario O
at O
test O
time O
. O
For O
POS O
, O
the O
performance O
against O
the O
phonetic O
attack O
remains O
mostly O
unchanged O
, O
while O
‚àÜœÑ(p)increases O
as O
a O
function O
of O
pagainst O
natural O
- O
noise,791none O
low O
mid O
high‚àí0.100.10.20.30.40.5 O
Attack O
Level‚àÜ1‚àí1s‚àó(p)POS O
none O
low O
mid O
high‚àí0.100.10.20.30.40.50.60.70.8 O
Attack O
Level‚àÜ1‚àí1s‚àó(p)NLI O
none O
low O
mid O
high‚àí0.100.10.20.30.40.5 O
Attack O
Level‚àÜ1‚àí1s‚àó(p)TCFull O
- O
ShuÔ¨Ñe O
Inner O
- O
ShuÔ¨Ñe O
Intrude O
Disemvowel O
Truncate O
Segment O
Keyboard O
- O
Typo O
Natural O
- O
Noise O
Phonetic O
VisualFigure O
2 O
: O
Performance O
improvements O
of O
the O
models O
adversarial O
trained O
and O
evaluated O
individually O
on O
the O
attacker O
introduced O
in O
Section O
3 O
for O
POS O
left O
, O
NLI O
mid O
and O
TC O
right O
. O
Performance O
measured O
in O
‚àÜœÑ(p)deÔ¨Åned O
in O
Eq O
. O
2 O
. O
FS O
IS O
INT O
DIS O
TRUN O
KEY O
NAT O
PH O
VIS O
FS O
low O
6.35 O
-0.46 O
-2.13 O
0.49 O
-0.29 O
-0.69 O
-1.21 O
0.02 O
0.96 O
high O
17.59 O
-0.1 O
-4.1 O
4.03 O
1.81 O
-0.45 O
1.48 O
-1.23 O
2.24 O
mid O
21.8 O
0.48 O
-4.36 O
7.59 O
4.37 O
0.11 O
3.93 O
-1.68 O
1.97 O
IS O
low O
2.01 O
6.02 O
0.1 O
0.72 O
-0.31 O
1.13 O
-0.33 O
0.14 O
1.74 O
mid O
3.7 O
16.71 O
0.89 O
4.24 O
2.4 O
3.53 O
0.41 O
0.08 O
4.08 O
high O
3.77 O
18.29 O
1.25 O
5.25 O
2.52 O
3.89 O
0.59 O
-0.28 O
4.78 O
INT O
low O
-2.19 O
-1.04 O
11.3 O
0.4 O
0.63 O
5.46 O
0.85 O
-2.16 O
5.3 O
mid O
-7.79 O
-4.89 O
41.5 O
10.77 O
7.26 O
12.38 O
6.71 O
-2.54 O
13.93 O
high O
-7.69 O
-6.22 O
62.81 O
9.8 O
6.59 O
11.4 O
29.56 O
-2.27 O
4.3 O
DIS O
low O
-2.95 O
-2.79 O
-0.56 O
8.07 O
0.67 O
0.19 O
0.48 O
-0.97 O
0.35 O
mid O
-4.75 O
-1.42 O
1.44 O
27.73 O
6.05 O
5.18 O
5.68 O
-1.17 O
3.27 O
high O
-4.39 O
0.42 O
2.76 O
41.44 O
9.2 O
8.68 O
9.39 O
-1.82 O
4.62 O
TRUN O
low O
-0.4 O
-1.07 O
-0.23 O
0.4 O
6.01 O
0.88 O
1.46 O
0.06 O
0.23 O
mid O
-0.22 O
-1.38 O
0.93 O
2.18 O
15.87 O
3.18 O
4.93 O
-0.61 O
1.58 O
high O
-0.2 O
-1.27 O
1.17 O
2.79 O
17.88 O
3.53 O
5.73 O
-0.78 O
1.72 O
KEY O
low O
-1.36 O
-1.9 O
0.11 O
0.3 O
0.24 O
5.05 O
0.91 O
-0.95 O
1.93 O
mid O
-1.89 O
-1.42 O
1.4 O
2.82 O
1.66 O
11.04 O
4.65 O
-1.79 O
4.12 O
high O
-4.07 O
-2.49 O
3.96 O
3.26 O
4.65 O
22 O
6.72 O
-3.27 O
5.94 O
NAT O
low O
-1.42 O
-2.77 O
-0.13 O
-0.21 O
-0.6 O
0.49 O
3.18 O
-0.93 O
1.14 O
mid O
-0.31 O
-2.86 O
0.47 O
2.77 O
1.03 O
2.61 O
15.62 O
-0.77 O
2.15 O
high O
-2.23 O
-3.05 O
2.33 O
3.46 O
4.04 O
4.2 O
16.7 O
-1.27 O
3.19 O
PH O
low O
-1.8 O
-1.96 O
-1.73 O
-1.22 O
-1.41 O
-1.76 O
-0.81 O
4.27 O
-1.15 O
mid O
-2.41 O
-2.18 O
-1.97 O
-1.45 O
-1.73 O
-2.16 O
-1.02 O
5.3 O
-1.42 O
high O
-2.21 O
-2.16 O
-1.95 O
-1.44 O
-1.69 O
-2.1 O
-0.95 O
5.41 O
-1.39 O
VIS O
low O
1.55 O
1.61 O
2.58 O
2.12 O
2.09 O
3.69 O
1.74 O
0.68 O
7.43 O
mid O
5.01 O
6.65 O
7.91 O
9.13 O
7.41 O
9.92 O
5.09 O
1.18 O
18.44 O
high O
1.46 O
3.62 O
-0.25 O
7.87 O
6.6 O
7.99 O
3.81 O
1.22 O
8.94 O
Figure O
3 O
: O
1 O
- O
1 O
( O
Œ± O
, O
Œ≤)adversarial O
training O
for O
POS O
. O
Column O
: O
train O
, O
row O
: O
test O
. O
Numbers O
give O
values O
‚àÜœÑ(p O
) O
, O
see O
Eq O
. O
( O
2 O
) O
. O
Red O
colors O
give O
performance O
decreases O
, O
relative O
to O
the O
results O
on O
clean O
data O
; O
blue O
colors O
show O
increases O
. O
inner O
- O
shufÔ¨Çe O
, O
full O
- O
shufÔ¨Çe O
, O
truncate O
and O
keyboardtypo O
. O
The O
best O
defense O
is O
against O
natural O
- O
noise O
with O
3pp O
for O
lowand O
7pp O
for O
mid O
andhigh O
. O
Shielding O
against O
visual O
, O
intrude O
and O
disemvowel O
attacks O
yields O
lower O
values O
‚àÜœÑ(p)on O
attack O
level O
high O
compared O
to O
mid O
. O
Overall O
, O
we O
see O
mild O
improvements O
compared O
to O
the O
unshielded O
situation O
, O
but O
expectedly O
, O
these O
are O
lower O
than O
for O
1 O
- O
1 O
shielding O
. O
For O
NLI O
, O
the O
performance O
against O
keyboard O
- O
typo O
, O
full O
- O
shufÔ¨Çe O
, O
inner O
- O
shufÔ¨Çe O
, O
natural O
- O
noise O
and O
truncate O
exhibits O
steady O
improvements O
with O
increasing O
attack O
level O
which O
range O
from O
10pp O
to O
20pp O
for O
attack O
level O
midandhigh O
. O
The O
performances O
against O
intrude O
and O
disemvowel O
also O
show O
steady O
improvements O
with O
the O
attack O
levels O
but O
are O
generally O
higher O
with O
up O
to O
29pp O
. O
For O
attack O
level O
low O
, O
the O
perfor O
- O
mance O
improvement O
against O
the O
visual O
attacker O
is O
with O
20pp O
more O
than O
twice O
the O
value O
of O
the O
others O
. O
This O
improvement O
diminishes O
in O
the O
midand O
high O
attack O
levels O
and O
even O
drops O
there O
below O
the O
improvements O
against O
most O
of O
the O
other O
attackers O
. O
In O
the O
TC O
task O
, O
the O
performance O
against O
visual O
improves O
even O
for O
lowlevel O
to O
8pp O
, O
increases O
for O
midto O
23pp O
and O
maximizes O
to O
29pp O
total O
improvement O
for O
attack O
level O
high O
. O
The O
performance O
against O
the O
intrude O
attack O
is O
also O
very O
good O
: O
for O
lowattack O
level O
the O
improvement O
( O
11pp O
) O
is O
even O
higher O
compared O
to O
visual O
( O
8pp O
) O
. O
The O
performances O
against O
full O
- O
shufÔ¨Çe O
, O
inner O
- O
shufÔ¨Çe O
, O
disemvowel O
, O
segment O
, O
keyboard O
- O
typo O
, O
natural O
- O
noise O
and O
phonetic O
behave O
similar O
for O
attack O
level O
lowandmidwith O
4pp O
to O
7pp O
total O
improvement O
. O
Shielding O
against O
full O
- O
swap O
and792none O
low O
mid O
high‚àí0.100.10.20.3 O
Attack O
Level‚àÜLOOs‚àó(p)POS O
none O
low O
mid O
high‚àí0.100.10.20.3 O
Attack O
Level‚àÜLOOs‚àó(p)NLI O
none O
low O
mid O
high‚àí0.100.10.20.3 O
Attack O
Level‚àÜLOOs‚àó(p)TCFull O
- O
ShuÔ¨Ñe O
Inner O
- O
ShuÔ¨Ñe O
Intrude O
Disemvowel O
Truncate O
Segment O
Keyboard O
- O
Typo O
Natural O
- O
Noise O
Phonetic O
VisualFigure O
4 O
: O
Leave O
- O
one O
- O
out O
defense O
: O
Performance O
improvements O
of O
the O
models O
adversarial O
trained O
on O
all O
attackers O
introduced O
in O
Section O
3 O
except O
the O
one O
they O
are O
evaluated O
on O
for O
POS O
left O
, O
NLI O
mid O
and O
TC O
right O
. O
Performance O
measured O
in O
‚àÜœÑ(p)deÔ¨Åned O
in O
equation O
2 O
. O
disemvowel O
is O
slightly O
better O
than O
the O
last O
group O
. O
There O
is O
no O
overall O
positive O
effect O
for O
truncate O
. O
4.4 O
Discussion O
Overall O
, O
the O
phonetic O
attack O
was O
least O
effective O
. O
We O
assume O
this O
is O
because O
few O
words O
were O
changed O
overall O
as O
a O
considerable O
amount O
of O
phonetic O
replacements O
were O
either O
identical O
to O
the O
input O
and O
some O
were O
even O
discarded O
. O
The O
truncate O
attack O
performed O
better O
than O
the O
phonetic O
attack O
in O
all O
three O
tasks O
but O
it O
still O
remained O
low O
overall O
, O
possibly O
as O
we O
truncated O
only O
by O
1 O
character O
, O
leading O
to O
small O
changes O
in O
the O
appearance O
of O
a O
word O
. O
We O
attribute O
the O
low O
impact O
of O
the O
segmentation O
attack O
to O
RoBERTa O
‚Äôs O
BPE O
encoding O
, O
which O
apparently O
allows O
it O
to O
partly O
de O
- O
segment O
unsegmented O
input O
. O
We O
observe O
that O
some O
attacks O
( O
e.g. O
, O
segmentation O
, O
keyboard O
- O
typo O
, O
and O
natural O
- O
noise O
) O
have O
less O
effect O
in O
TC O
compared O
to O
POS O
and O
NLI O
, O
possibly O
because O
of O
higher O
natural O
occurrences O
of O
these O
phenomena O
in O
the O
TC O
dataset O
. O
The O
intrude O
and O
visual O
attacks O
are O
among O
the O
strongest O
. O
This O
is O
not O
only O
because O
they O
are O
doubly O
parametrized O
unlike O
many O
others O
‚Äî O
i.e. O
, O
for O
high O
attacks O
, O
not O
only O
the O
majority O
of O
words O
is O
attacked O
but O
also O
the O
majority O
of O
characters O
within O
a O
word O
‚Äî O
since O
they O
are O
also O
effective O
at O
lowattack O
levels O
. O
We O
partly O
attribute O
their O
success O
to O
the O
fact O
that O
they O
cause O
a O
high O
out O
- O
of O
- O
vocabulary O
rate O
for O
RoBERTa O
and O
tend O
to O
increase O
the O
number O
of O
input O
tokens O
, O
as O
they O
cause O
RoBERTa O
to O
segment O
the O
input O
at O
unknown O
characters O
. O
This O
may O
lead O
to O
the O
number O
of O
input O
tokens O
exceeding O
RoBERTa O
‚Äôs O
builti O
- O
in O
max O
token O
size O
, O
leading O
to O
cutting O
off O
the O
ending O
of O
the O
sentence O
. O
Rank O
POS O
NLI O
TC O
1 O
1 O
- O
1 O
( O
Œ± O
, O
Œ± O
) O
16 O
1 O
- O
1 O
20 O
1 O
- O
1 O
12 O
2 O
LOO O
4 O
LOO O
10 O
LOO O
9 O
3 O
1 O
- O
1 O
( O
Œ± O
, O
Œ≤ O
) O
1 O
1 O
- O
1 O
3 O
1 O
- O
1 O
7 O
Table O
3 O
: O
Different O
defense O
approaches O
ranked O
by O
the O
average O
robustness O
improvement O
over O
all O
attackers O
. O
Improvement O
in O
percentage O
points O
( O
pp O
; O
rounded O
) O
. O
In O
Table O
5 O
( O
appendix O
) O
, O
attacks O
are O
ranked O
( O
for O
high O
attack O
level O
) O
by O
the O
performance O
degradation O
caused O
to O
the O
model O
for O
each O
individual O
task O
. O
In O
line O
with O
our O
previous O
discussion O
, O
the O
visual O
and O
the O
intrude O
attackers O
are O
always O
the O
both O
best O
performing O
, O
followed O
by O
full O
- O
shufÔ¨Çe O
( O
which O
we O
deemed O
as O
unrealistic O
as O
it O
would O
also O
destroy O
human O
perception O
abilities O
) O
. O
Figure O
5 O
shows O
the O
relationship O
between O
the O
amount O
of O
text O
perturbed O
in O
a O
test O
dataset O
and O
the O
performance O
deterioration O
a O
model O
suffers O
. O
This O
shows O
a O
clear O
( O
linear O
) O
trend O
and O
indicates O
that O
a O
successful O
attacker O
most O
importantly O
needs O
to O
attack O
many O
characters O
of O
a O
text O
to O
be O
effective O
, O
despite O
all O
individual O
qualitative O
differences O
between O
the O
attackers O
discussed O
above O
. O
In O
Table O
3 O
, O
a O
ranking O
of O
defense O
strategies O
is O
given O
. O
1 O
- O
1 O
( O
Œ± O
, O
Œ± O
) O
performs O
best O
, O
but O
is O
unrealistic O
. O
LOO O
is O
a O
robust O
alternative O
for O
unknown O
new O
attacks O
. O
The O
effectiveness O
of O
LOO O
as O
defense O
is O
also O
a O
further O
justiÔ¨Åcation O
for O
designing O
multiple O
attack O
models O
. O
5 O
Conclusion O
We O
provided O
the O
Ô¨Årst O
large O
- O
scale O
catalogue O
for O
lowlevel O
adversarial O
attacks O
, O
providing O
a O
new O
simple O
benchmark O
for O
testing O
real O
- O
world O
robustness O
of O
future O
deep O
learning O
models O
. O
We O
further O
showed7930 O
10 O
20 O
30 O
40 O
500.20.40.60.81 O
avg O
. O
levenshtein O
distances‚àó(p)Figure O
5 O
: O
Relation O
between O
the O
amount O
of O
text O
perturbed O
( O
measured O
in O
edit O
distance O
) O
in O
a O
test O
data O
set O
and O
s‚àó(p O
) O
, O
the O
performance O
decrease O
a O
model O
suffers O
. O
that O
one O
of O
the O
currently O
most O
successful O
deep O
learning O
paradigms O
, O
RoBERTa O
, O
is O
not O
robust O
to O
our O
benchmark O
, O
sometimes O
suffering O
catastrophic O
failure O
. O
While O
many O
of O
our O
errors O
could O
probably O
be O
addressed O
by O
placing O
a O
correction O
layer O
in O
front O
of O
RoBERTa O
( O
Choudhury O
et O
al O
. O
, O
2007 O
; O
Pruthi O
et O
al O
. O
, O
2019 O
) O
, O
we O
believe O
that O
our O
Ô¨Åndings O
shed O
further O
light O
on O
the O
differences O
between O
human O
and O
machine O
text O
processing O
, O
which O
deep O
models O
eventually O
will O
have O
to O
innately O
overcome O
for O
true O
AI O
to O
become O
a O
viable O
prospect O
. O
Acknowledgments O
We O
thank O
the O
anonymous O
reviewers O
for O
their O
useful O
comments O
and O
suggestions O
. O
Steffen O
Eger O
has O
been O
funded O
by O
the O
HMWK O
( O
Hessisches O
Ministerium O
f¬®ur O
Wissenschaft O
und O
Kunst O
) O
as O
part O
of O
structural O
location O
promotion O
for O
TU O
Darmstadt O
in O
the O
context O
of O
the O
Hessian O
excellence O
cluster O
initiative O
‚Äú O
Content O
Analytics O
for O
the O
Social O
Good O
‚Äù O
( O
CA O
- O
SG O
) O
. O
A O
Appendices O
A.1 O
Phonetic O
and O
visual O
embeddings O
Phonetic O
Word O
Embeddings O
. O
To O
induce O
phonetic O
word O
embeddings O
, O
we O
adopt O
the O
Siamese O
network O
of O
InferSent O
( O
Conneau O
et O
al O
. O
, O
2017 O
) O
. O
InferSent O
was O
originally O
designed O
to O
induce O
vector O
representations O
for O
two O
sentences O
from O
which O
their O
entailment O
relation O
was O
inferred O
. O
We O
adapt O
InferSent O
to O
encode O
two O
words O
so O
that O
their O
phonological O
similarity O
can O
be O
inferred O
: O
identical O
, O
very O
similar O
, O
similar O
anddifferent O
. O
We O
use O
the O
BiLSTM O
max O
- O
pooling O
approach O
from O
the O
original O
InferSent O
paper O
, O
where O
we O
set O
the O
induced O
phonetic O
embeddings O
size O
to O
100 O
. O
We O
build O
our O
own O
dataset O
for O
phonetic O
similarity O
by O
leveraging O
data O
from O
different O
sources O
. O
Initially O
, O
we O
use O
Combilex O
( O
Richmond O
et O
al O
. O
, O
2010 O
) O
, O
which O
gives O
phonetic O
representations O
for O
standard O
( O
American O
) O
English O
words O
. O
We O
calculate O
the O
normalized O
edit O
distance O
between O
the O
phonemes O
of O
each O
word O
pair O
to O
determine O
the O
phonetic O
similarity O
of O
two O
words O
: O
sim O
ph(œÄ1,œÄ2 O
) O
= O
1‚àíd(œÄ1,œÄ2 O
) O
min(|œÄ1|,|œÄ2|)(3 O
) O
whereœÄiare O
phonetic O
sequences O
for O
underlying O
words O
anddis O
the O
edit O
- O
distance O
. O
We O
then O
map O
the O
words O
into O
4 O
different O
classes O
: O
identical O
( O
sim O
ph= O
0),very O
similar O
( O
0 O
< O
sim O
ph<0.1),similar O
( O
0.1 O
< O
sim O
ph<0.3 O
) O
and O
different O
( O
0.3 O
< O
sim O
ph O
) O
. O
To O
keep O
the O
training O
data O
for O
each O
class O
more O
balanced O
, O
we O
added O
handcrafted O
and O
crawled O
samples O
, O
e.g. O
, O
homophones O
. O
We O
also O
wanted O
to O
include O
‚Äú O
internet O
slang O
‚Äù O
style O
phonetic O
replacements O
like O
in O
Table O
4 O
. O
We O
therefore O
crawled O
them O
and O
added O
them O
to O
the O
bins O
identical O
andvery O
similar O
based O
upon O
manual O
inspection O
. O
Overall O
, O
we O
compiled O
5k O
examples O
for O
each O
of O
our O
four O
labels O
. O
The O
similaranddifferent O
bins O
consist O
only O
of O
data O
from O
Combilex O
, O
whereas O
the O
identical O
andvery O
similar O
bin O
contains O
1.3k O
samples O
from O
Combilex O
and O
3.7k O
crawled O
samples O
. O
References O
for O
crawled O
sites O
are O
given O
in O
A.2 O
. O
Visual O
Embeddings O
. O
The O
model O
reduces O
the O
dimension O
of O
input O
x O
, O
e.g. O
, O
an O
image O
, O
by O
applyingmultiple O
convolutional O
steps O
in O
the O
encoder O
to O
compute O
the O
latent O
representation O
zofx O
. O
Afterwards O
, O
it O
reconstructs O
the O
original O
input O
xin O
the O
decoder O
by O
applying O
multiple O
deconvolutional O
steps O
to O
z. O
This O
reconstructed O
version O
of O
xis O
called O
Àúx O
. O
Additionally O
, O
a O
second O
input O
zpsampled O
fromN(0,I)is O
inserted O
into O
the O
generator O
to O
obtainxp O
. O
Decoder O
andgenerator O
perform O
the O
same O
task O
on O
different O
inputs O
; O
they O
can O
be O
considered O
as O
identical O
and O
therefore O
share O
their O
parameters O
. O
The O
discriminator O
takes O
x,Àúxandxpas O
inputs O
and O
discriminates O
which O
input O
is O
a O
real O
training O
sample O
and O
which O
is O
a O
fake O
. O
Figure O
6 O
illustrates O
the O
working O
of O
the O
architecture O
. O
xz O
encode O
Àúxdecodezp O
xpgenerate O
discriminatortrue O
fake O
Figure O
6 O
: O
Schematic O
representation O
of O
Variational O
Autoencoder O
Generative O
Adversarial O
Network O
( O
V O
AEGAN O
) O
taken O
and O
adapted O
from O
Larsen O
et O
al O
. O
( O
2016 O
) O
. O
zcan O
be O
decomposed O
as O
z=¬µ+œÉand O
is O
used O
to O
sample O
zp=¬µ+œÉ O
/ O
epsilon1where O
/ O
epsilon1is O
noise O
deÔ¨Åned O
as O
/epsilon1‚àº O
N O
( O
0,I O
) O
Figure O
7 O
: O
Reconstruction O
of O
images O
after O
being O
compressed O
to O
its O
latent O
representation O
and O
decompressed O
back O
to O
the O
original O
data O
distribution O
. O
Figure O
8 O
gives O
an O
impression O
of O
the O
encoded O
visual O
similarity O
. O
A.2 O
Homophone O
resources O
List O
of O
used O
resources O
to O
gather O
homophones O
. O
‚Ä¢https://7esl.com O
/ O
homonyms/ O
‚Ä¢https://www.englishclub.com/ O
pronunciation O
/ O
homophones O
- O
list.html O
‚Ä¢https://www.thoughtco.com/ O
homonyms O
- O
homophones O
- O
and O
- O
homographs O
- O
a O
- O
b-1692660 O
‚Ä¢http://www.singularis.ltd.uk/ O
bifroest O
/ O
misc O
/ O
homophones O
- O
list.html796Ranking O
POS O
NLI O
TC O
1 O
Intrude O
Visual O
& O
Intrude O
Visual O
2 O
Visual O
- O
Intrude O
3 O
Full O
- O
ShufÔ¨Çe O
Full O
- O
ShufÔ¨Çe O
Full O
- O
ShufÔ¨Çe O
4 O
Keyboard O
- O
Typo O
Disemvowel O
Disemvowel O
5 O
Disemvowel O
Keyboard O
- O
Typo O
Inner O
- O
ShufÔ¨Çe O
6 O
Natural O
- O
Noise O
Inner O
- O
ShufÔ¨Çe O
Truncate O
7 O
Inner O
- O
ShufÔ¨Çe O
Natural O
- O
Noise O
Keyboard O
- O
Typo O
8 O
Truncate O
Segment O
Natural O
- O
Noise O
9 O
Phonetic O
Truncate O
Segment O
10 O
- O
Phonetic O
Phonetic O
Table O
5 O
: O
Ranking O
on O
harmfulness O
of O
the O
attackers O
on O
POS O
, O
NLI O
, O
TC O
on O
attack O
level O
high O
. O
Figure O
8 O
: O
tSNE O
plot O
of O
our O
character O
embedding O
space O
. O
As O
can O
be O
seen O
similar O
looking O
characters O
are O
clustered.797‚Ä¢https://web.archive.org O
/ O
web/ O
20160825095711/ O
‚Ä¢http://people.sc.fsu.edu/ O
Àújburkardt/ O
fun O
/ O
wordplay O
/ O
multinyms.html O
‚Ä¢http://homophonelist.com/ O
homophones O
- O
list/ O
‚Ä¢https://web.archive.org O
/ O
web/ O
20160825095711/ O
‚Ä¢http://homophonelist.com/ O
homophones O
- O
list/ O
‚Ä¢https://www.webopedia.com O
/ O
quick_ref/ O
textmessageabbreviations.asp O
‚Ä¢https://www.smart O
- O
words.org/ O
abbreviations O
/ O
text.html O
‚Ä¢https://en.wiktionary.org/ O
wiki O
/ O
Appendix O
: O
English O
_ O
dialect O
- O
independent_homophones O
‚Ä¢https://en.wiktionary.org O
/ O
wiki/ O
Appendix O
: O
English_dialect O
- O
dependent O
_ O
homophones O
A.3 O
Detailed O
Result O
Tables O
Hyperparameters O
of O
our O
models O
can O
be O
found O
in O
the O
github O
accompanying O
the O
publication O
( O
https:// O
github.com/yannikbenz/zeroe O
) O
. O
The O
following O
tables O
give O
detailed O
results O
of O
our O
experiments.798Attack O
ModeAccuracy O
AUCROC O
POS O
NLI O
TC O
None O
- O
96.65 O
90.41 O
0.93 O
Full O
- O
Swaplow O
82.14 O
70.35 O
0.90 O
mid O
58.14 O
45.70 O
0.83 O
high O
40.47 O
38.35 O
0.74 O
Inner O
- O
Swaplow O
85.96 O
67.70 O
0.90 O
mid O
70.53 O
53.35 O
0.83 O
high O
67.95 O
51.55 O
0.82 O
Intrudelow O
81.42 O
75.97 O
0.91 O
mid O
46.91 O
52.25 O
0.85 O
high O
18.15 O
34.70 O
0.66 O
Disemvowellow O
85.24 O
72.24 O
0.91 O
mid O
61.50 O
51.62 O
0.86 O
high O
44.69 O
41.00 O
0.79 O
Truncatelow O
88.57 O
79.83 O
0.90 O
mid O
77,40 O
72.87 O
0.84 O
high O
75,11 O
72.02 O
0.83 O
Segmentlow O
- O
86.08 O
0.93 O
mid O
- O
77.53 O
0.92 O
high O
- O
69.14 O
0.91 O
Keyboard O
- O
Typolow O
85.06 O
76.93 O
0.92 O
mid O
62.41 O
60.21 O
0.88 O
high O
40.99 O
44.16 O
0.84 O
Natural O
Noiselow O
85.34 O
78.43 O
0.92 O
mid O
65.36 O
65.60 O
0.91 O
high O
50.06 O
56.31 O
0.90 O
Phoneticlow O
90.62 O
87.40 O
0.93 O
mid O
89.09 O
84.75 O
0.92 O
high O
88.95 O
82.80 O
0.91 O
Visuallow O
80.52 O
53.07 O
0.86 O
mid O
48.14 O
35.26 O
0.64 O
high O
22.44 O
34.37 O
0.48 O
Table O
6 O
: O
Attacks O
against O
unshielded O
model.799TestTrainlevel O
FS O
IS O
INT O
DIS O
TRUN O
SEG O
KEY O
NAT O
PH O
VIS O
POS O
NLI O
TC O
FSnone O
 O
 O
 O
 O
 O
 O
 O
 O
95.57 O
89.56 O
0.97 O
low O
84.49 O
73.05 O
0.95 O
mid O
63.48 O
57.54 O
0.90 O
high O
45.72 O
51.73 O
0.86 O
ISnone O
 O
 O
 O
 O
 O
 O
 O
95.66 O
88.94 O
0.96 O
low O
88.29 O
75.51 O
0.94 O
mid O
75.90 O
69.07 O
0.91 O
high O
73.68 O
68.54 O
0.90 O
INTnone O
 O
 O
 O
 O
 O
 O
 O
 O
95.65 O
88.90 O
0.96 O
low O
87.54 O
84.27 O
0.95 O
mid O
57.58 O
74.92 O
0.93 O
high O
19.44 O
61.07 O
0.84 O
DISnone O
 O
 O
 O
 O
 O
 O
 O
 O
95.69 O
89.42 O
0.96 O
low O
86.00 O
80.00 O
0.94 O
mid O
64.39 O
70.98 O
0.91 O
high O
48.65 O
66.60 O
0.89 O
TRUNnone O
 O
 O
 O
 O
 O
 O
 O
 O
95.49 O
89.17 O
0.96 O
low O
89.98 O
84.55 O
0.84 O
mid O
81.23 O
81.97 O
0.83 O
high O
79.38 O
81.62 O
0.82 O
SEGnone O
 O
 O
 O
 O
 O
 O
 O
 O
- O
89.02 O
0.96 O
low O
- O
85.38 O
0.96 O
mid O
- O
76.92 O
0.95 O
high O
- O
62.83 O
0.95 O
KEYnone O
 O
 O
 O
 O
 O
 O
 O
 O
95.61 O
88.80 O
0.96 O
low O
87.71 O
80.47 O
0.95 O
mid O
68.64 O
70.69 O
0.94 O
high O
46.51 O
61.83 O
0.92 O
NATnone O
 O
 O
 O
 O
 O
 O
 O
 O
95.72 O
88.67 O
0.96 O
low O
88.17 O
81.27 O
0.96 O
mid O
72.30 O
73.05 O
0.96 O
high O
56.78 O
67.40 O
0.96 O
PHnone O
 O
 O
 O
 O
 O
 O
 O
 O
95.30 O
88.95 O
0.96 O
low O
89.74 O
87.54 O
0.96 O
mid O
87.82 O
86.27 O
0.95 O
high O
87.72 O
85.34 O
0.95 O
VISnone O
 O
 O
 O
 O
 O
 O
 O
 O
 O
95.72 O
89.02 O
0.96 O
low O
85.18 O
70.77 O
0.93 O
mid O
58.94 O
48.80 O
0.85 O
high O
24.99 O
40.22 O
0.75 O
Table O
7 O
: O
Adversarial O
training O
: O
leave O
- O
one O
- O
out.800TestTrainlevel O
FS O
IS O
INT O
DIS O
TRUN O
KEY O
NAT O
PH O
VIS O
Clean O
- O
95.17 O
95.18 O
95.04 O
95.44 O
95.48 O
95.40 O
95.40 O
96.06 O
95.66 O
FSlow O
88.49 O
81.68 O
80.01 O
82.63 O
81.85 O
81.45 O
80.93 O
82.16 O
83.10 O
mid O
75.73 O
58.04 O
54.04 O
62.17 O
59.95 O
57.69 O
59.62 O
56.91 O
60.38 O
high O
62.27 O
40.95 O
36.11 O
48.06 O
44.84 O
40.58 O
44.40 O
38.79 O
42.44 O
ISlow O
87.97 O
91.98 O
86.06 O
86.68 O
85.65 O
87.09 O
85.63 O
86.10 O
87.70 O
mid O
74.23 O
87.24 O
71.42 O
74.77 O
72.93 O
74.06 O
70.94 O
70.61 O
74.61 O
high O
71.72 O
86.24 O
69.20 O
73.20 O
70.47 O
71.84 O
68.54 O
67.67 O
72.73 O
INTlow O
79.23 O
80.38 O
92.72 O
81.82 O
82.05 O
86.88 O
82.27 O
79.26 O
86.72 O
mid O
39.12 O
42.02 O
88.41 O
57.68 O
54.17 O
59.29 O
53.62 O
44.37 O
60.84 O
high O
10.46 O
11.93 O
80.96 O
27.95 O
24.74 O
29.55 O
47.71 O
15.88 O
22.45 O
DISlow O
82.29 O
82.45 O
84.68 O
93.31 O
85.91 O
85.43 O
85.72 O
84.27 O
85.59 O
mid O
56.75 O
60.08 O
62.94 O
89.23 O
67.55 O
66.68 O
67.18 O
60.33 O
64.77 O
high O
40.30 O
45.11 O
47.45 O
86.13 O
53.89 O
53.37 O
54.08 O
42.87 O
49.31 O
TRUNlow O
88.17 O
87.50 O
88.34 O
88.97 O
94.58 O
89.45 O
90.03 O
88.63 O
88.80 O
mid O
77.18 O
76.02 O
78.33 O
79.58 O
93.27 O
80.58 O
82.33 O
76.79 O
78.98 O
high O
74.91 O
73.84 O
76.28 O
77.90 O
92.99 O
78.64 O
80.84 O
74.33 O
76.83 O
KEYlow O
83.70 O
83.16 O
85.17 O
85.36 O
85.30 O
90.11 O
85.97 O
84.11 O
86.99 O
mid O
60.52 O
60.99 O
63.81 O
65.23 O
64.07 O
73.45 O
67.06 O
60.62 O
66.53 O
high O
36.92 O
38.50 O
44.95 O
44.25 O
45.64 O
62.99 O
47.71 O
37.72 O
46.93 O
NATlow O
83.92 O
82.57 O
85.21 O
85.13 O
84.74 O
85.83 O
88.52 O
84.41 O
86.48 O
mid O
65.05 O
62.50 O
65.83 O
68.13 O
66.39 O
67.97 O
80.98 O
64.59 O
67.51 O
high O
47.83 O
47.01 O
52.39 O
53.52 O
54.10 O
54.26 O
66.76 O
48.79 O
53.25 O
PHlow O
88.82 O
88.66 O
88.89 O
89.40 O
89.21 O
88.86 O
89.81 O
94.89 O
89.47 O
mid O
86.68 O
86.91 O
87.12 O
87.64 O
87.36 O
86.93 O
88.07 O
94.39 O
87.67 O
high O
86.74 O
86.79 O
87.00 O
87.51 O
87.26 O
86.85 O
88.00 O
94.36 O
87.56 O
VISlow O
82.07 O
82.13 O
83.10 O
82.64 O
82.61 O
84.21 O
82.26 O
81.20 O
87.95 O
mid O
53.15 O
54.79 O
56.05 O
57.27 O
55.55 O
58.06 O
53.23 O
49.32 O
66.58 O
high O
23.90 O
26.06 O
22.19 O
30.31 O
29.04 O
30.43 O
26.25 O
23.66 O
31.38 O
Table O
8 O
: O
Part O
- O
of O
- O
Speech O
tagging O
adversarial O
training O
: O
1 O
- O
1.801TestTrainlevel O
FS O
IS O
INT O
DIS O
TRUN O
SEG O
KEY O
NAT O
PH O
VIS O
Clean O
- O
87.54 O
- O
88.29 O
88.59 O
88.91 O
89.90 O
89.17 O
89.12 O
90.24 O
FSlow O
83.04 O
- O
64.65 O
63.28 O
60.38 O
62.67 O
69.46 O
68.46 O
66.87 O
mid O
78.58 O
- O
47.62 O
48.15 O
42.21 O
46.05 O
52.63 O
50.81 O
48.11 O
high O
76.96 O
- O
42.75 O
44.52 O
39.16 O
41.55 O
47.77 O
46.21 O
41.53 O
ISlow O
81.31 O
- O
71.32 O
66.40 O
59.81 O
63.26 O
72.23 O
72.25 O
66.40 O
mid O
78.82 O
- O
64.82 O
58.42 O
51.17 O
53.63 O
63.21 O
64.24 O
57.16 O
high O
78.08 O
- O
64.06 O
58.25 O
50.86 O
53.53 O
62.72 O
62.89 O
56.61 O
INTlow O
76.22 O
- O
85.83 O
72.49 O
72.97 O
72.78 O
83.73 O
80.83 O
74.83 O
mid O
58.99 O
- O
82.61 O
53.87 O
51.09 O
48.45 O
69.53 O
62.84 O
51.53 O
high O
43.22 O
- O
80.76 O
39.93 O
36.18 O
36.60 O
48.77 O
40.91 O
37.07 O
DISlow O
79.14 O
- O
76.27 O
86.56 O
67.51 O
72.52 O
78.96 O
77.77 O
72.65 O
mid O
72.47 O
- O
67.43 O
84.70 O
57.60 O
56.88 O
69.72 O
64.85 O
57.03 O
high O
69.45 O
- O
63.90 O
84.16 O
54.50 O
48.25 O
65.29 O
58.44 O
48.92 O
TRUNlow O
81.63 O
- O
84.31 O
79.66 O
88.15 O
80.02 O
86.35 O
84.79 O
80.46 O
mid O
77.87 O
- O
82.45 O
75.86 O
87.52 O
76.11 O
84.08 O
81.83 O
76.19 O
high O
77.25 O
- O
82.46 O
75.10 O
87.44 O
75.79 O
83.80 O
81.76 O
75.80 O
SEGlow O
82.32 O
- O
84.32 O
83.75 O
84.00 O
89.07 O
86.15 O
85.53 O
85.69 O
mid O
68.85 O
- O
76.41 O
75.84 O
77.33 O
87.54 O
80.28 O
79.37 O
78.14 O
high O
50.94 O
- O
64.98 O
68.11 O
71.36 O
86.42 O
73.39 O
73.19 O
71.88 O
KEYlow O
74.23 O
- O
76.90 O
71.38 O
69.95 O
73.10 O
86.63 O
81.04 O
74.46 O
mid O
57.37 O
- O
62.86 O
55.62 O
54.30 O
58.67 O
82.98 O
70.74 O
56.98 O
high O
45.87 O
- O
52.26 O
46.29 O
44.75 O
47.07 O
79.82 O
61.76 O
44.54 O
NATlow O
77.87 O
- O
78.32 O
73.62 O
73.50 O
75.51 O
82.39 O
87.67 O
76.47 O
mid O
67.98 O
- O
67.98 O
62.27 O
60.10 O
62.97 O
74.25 O
85.45 O
62.85 O
high O
59.73 O
- O
60.81 O
55.16 O
53.18 O
55.41 O
69.52 O
84.06 O
54.89 O
PHlow O
85.68 O
- O
85.98 O
84.23 O
85.36 O
86.53 O
87.50 O
87.80 O
89.93 O
mid O
84.25 O
- O
84.21 O
80.98 O
82.67 O
84.17 O
85.98 O
86.50 O
89.40 O
high O
83.07 O
- O
82.71 O
80.28 O
81.68 O
82.68 O
84.74 O
85.42 O
89.19 O
VISlow O
59.79 O
- O
72.33 O
55.74 O
56.09 O
56.91 O
70.65 O
66.32 O
55.34 O
mid O
41.82 O
- O
50.95 O
37.87 O
37.01 O
36.38 O
45.21 O
41.84 O
36.31 O
high O
37.42 O
- O
39.08 O
33.81 O
34.26 O
34.51 O
36.04 O
35.37 O
34.10 O
Table O
9 O
: O
Natural O
language O
inference O
adversarial O
training O
: O
1 O
- O
1.802TestTrainlevel O
FS O
IS O
INT O
DIS O
TRUN O
SEG O
KEY O
NAT O
PH O
VIS O
Clean O
- O
0.96 O
0.96 O
0.96 O
0.97 O
0.97 O
0.98 O
0.97 O
0.96 O
0.97 O
0.96 O
FSlow O
0.94 O
0.94 O
0.94 O
0.94 O
0.94 O
0.95 O
0.95 O
0.93 O
0.95 O
0.93 O
mid O
0.92 O
0.90 O
0.87 O
0.88 O
0.87 O
0.87 O
0.89 O
0.87 O
0.87 O
0.88 O
high O
0.90 O
0.86 O
0.80 O
0.81 O
0.79 O
0.79 O
0.83 O
0.80 O
0.77 O
0.83 O
ISlow O
0.94 O
0.95 O
0.94 O
0.94 O
0.94 O
0.95 O
0.94 O
0.93 O
0.94 O
0.93 O
mid O
0.92 O
0.94 O
0.89 O
0.90 O
0.89 O
0.89 O
0.91 O
0.88 O
0.89 O
0.89 O
high O
0.91 O
0.94 O
0.88 O
0.90 O
0.88 O
0.88 O
0.90 O
0.87 O
0.88 O
0.89 O
INTlow O
0.95 O
0.95 O
0.96 O
0.96 O
0.96 O
0.96 O
0.96 O
0.95 O
0.96 O
0.95 O
mid O
0.92 O
0.92 O
0.95 O
0.92 O
0.92 O
0.91 O
0.94 O
0.90 O
0.91 O
0.91 O
high O
0.81 O
0.80 O
0.91 O
0.80 O
0.76 O
0.75 O
0.81 O
0.76 O
0.72 O
0.83 O
DISlow O
0.94 O
0.95 O
0.95 O
0.96 O
0.94 O
0.96 O
0.95 O
0.94 O
0.95 O
0.94 O
mid O
0.91 O
0.91 O
0.91 O
0.96 O
0.90 O
0.90 O
0.92 O
0.90 O
0.89 O
0.90 O
high O
0.88 O
0.88 O
0.88 O
0.95 O
0.86 O
0.83 O
0.89 O
0.86 O
0.83 O
0.88 O
TRUNlow O
0.95 O
0.95 O
0.96 O
0.96 O
0.97 O
0.96 O
0.97 O
0.94 O
0.96 O
0.95 O
mid O
0.94 O
0.94 O
0.95 O
0.94 O
0.97 O
0.95 O
0.96 O
0.93 O
0.95 O
0.94 O
high O
0.94 O
0.94 O
0.95 O
0.94 O
0.97 O
0.95 O
0.96 O
0.93 O
0.95 O
0.94 O
SEGlow O
0.96 O
0.96 O
0.96 O
0.96 O
0.97 O
0.97 O
0.97 O
0.96 O
0.97 O
0.95 O
mid O
0.95 O
0.95 O
0.95 O
0.96 O
0.96 O
0.97 O
0.96 O
0.95 O
0.96 O
0.94 O
high O
0.94 O
0.94 O
0.94 O
0.95 O
0.95 O
0.97 O
0.95 O
0.93 O
0.95 O
0.93 O
KEYlow O
0.95 O
0.95 O
0.95 O
0.95 O
0.96 O
0.96 O
0.96 O
0.95 O
0.96 O
0.95 O
mid O
0.92 O
0.92 O
0.93 O
0.93 O
0.93 O
0.94 O
0.95 O
0.92 O
0.93 O
0.92 O
high O
0.88 O
0.88 O
0.91 O
0.89 O
0.90 O
0.89 O
0.95 O
0.89 O
0.87 O
0.90 O
NATlow O
0.96 O
0.96 O
0.96 O
0.96 O
0.97 O
0.97 O
0.97 O
0.96 O
0.97 O
0.95 O
mid O
0.95 O
0.95 O
0.96 O
0.96 O
0.96 O
0.97 O
0.97 O
0.96 O
0.96 O
0.95 O
high O
0.95 O
0.95 O
0.95 O
0.95 O
0.96 O
0.96 O
0.96 O
0.96 O
0.96 O
0.94 O
PHlow O
0.96 O
0.96 O
0.96 O
0.96 O
0.97 O
0.97 O
0.97 O
0.95 O
0.97 O
0.95 O
mid O
0.95 O
0.95 O
0.95 O
0.96 O
0.96 O
0.97 O
0.96 O
0.95 O
0.97 O
0.95 O
high O
0.95 O
0.95 O
0.95 O
0.95 O
0.96 O
0.96 O
0.96 O
0.94 O
0.97 O
0.94 O
VISlow O
0.92 O
0.93 O
0.94 O
0.93 O
0.93 O
0.93 O
0.94 O
0.91 O
0.92 O
0.93 O
mid O
0.82 O
0.81 O
0.86 O
0.80 O
0.78 O
0.77 O
0.83 O
0.76 O
0.71 O
0.90 O
high O
0.70 O
0.69 O
0.75 O
0.65 O
0.62 O
0.62 O
0.66 O
0.64 O
0.55 O
0.85 O
Table O
10 O
: O
Toxic O
comment O
adversarial O
training O
: O
1 O
- O
1.803Proceedings O
of O
the O
1st O
Conference O
of O
the O
Asia O
- O
PaciÔ¨Åc O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
10th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
804‚Äì810 O
December O
4 O
- O
7 O
, O
2020 O
. O
¬© O
2020 O
Association O
for O
Computational O
Linguistics O
Point O
- O
of O
- O
Interest O
Type O
Inference O
from O
Social O
Media O
Text O
Danae O
S O
¬¥ O
anchez O
VillegasŒ±Daniel O
Preot O
¬∏iuc O
- O
PietroŒ≤Nikolaos O
AletrasŒ± O
Œ±Computer O
Science O
Department O
, O
University O
of O
ShefÔ¨Åeld O
, O
UK O
Œ≤Bloomberg O
{ O
dsanchezvillegas1 O
, O
n.aletras O
} O
@sheffield.ac.uk O
dpreotiucpie@bloomberg.net O
Abstract O
Physical O
places O
help O
shape O
how O
we O
perceive O
the O
experiences O
we O
have O
there O
. O
We O
study O
the O
relationship O
between O
social O
media O
text O
and O
the O
type O
of O
the O
place O
from O
where O
it O
was O
posted O
, O
whether O
a O
park O
, O
restaurant O
, O
or O
someplace O
else O
. O
To O
facilitate O
this O
, O
we O
introduce O
a O
novel O
data O
set O
of‚àº200,000 O
English O
tweets O
published O
from O
2,761 O
different O
points O
- O
of O
- O
interest O
in O
the O
U.S. O
, O
enriched O
with O
place O
type O
information O
. O
We O
train O
classiÔ¨Åers O
to O
predict O
the O
type O
of O
the O
location O
a O
tweet O
was O
sent O
from O
that O
reach O
a O
macro O
F1 O
of O
43.67 O
across O
eight O
classes O
and O
uncover O
the O
linguistic O
markers O
associated O
with O
each O
type O
of O
place O
. O
The O
ability O
to O
predict O
semantic O
place O
information O
from O
a O
tweet O
has O
applications O
in O
recommendation O
systems O
, O
personalization O
services O
and O
cultural O
geography.1 O
1 O
Introduction O
Social O
networks O
such O
as O
Twitter O
allow O
users O
to O
share O
information O
about O
different O
aspects O
of O
their O
lives O
including O
feelings O
and O
experiences O
from O
places O
that O
they O
visit O
, O
from O
local O
restaurants O
to O
sport O
stadiums O
and O
parks O
. O
Feelings O
and O
emotions O
triggered O
by O
performing O
an O
activity O
or O
living O
an O
experience O
in O
a O
Point O
- O
of O
- O
Interest O
( O
POI O
) O
can O
give O
a O
glimpse O
of O
the O
atmosphere O
in O
that O
place O
( O
Tanasescu O
et O
al O
. O
, O
2013 O
) O
. O
In O
particular O
, O
the O
language O
used O
in O
posts O
from O
POIs O
is O
an O
important O
component O
that O
contributes O
toward O
the O
place O
‚Äôs O
identity O
and O
has O
been O
extensively O
studied O
in O
the O
context O
of O
social O
and O
cultural O
geography O
( O
Tuan O
, O
1991 O
; O
Scollon O
and O
Scollon O
, O
2003 O
; O
Benwell O
and O
Stokoe O
, O
2006 O
) O
. O
Social O
media O
posts O
from O
a O
particular O
location O
are O
usually O
focused O
on O
the O
person O
posting O
the O
content O
, O
rather O
than O
on O
providing O
explicit O
information O
about O
the O
place O
. O
Table O
1 O
displays O
example O
Twitter O
posts O
from O
different O
POIs O
. O
Users O
express O
their O
feelings O
related O
to O
a O
certain O
1Data O
is O
available O
here O
: O
https://archive.org/ O
details O
/ O
poi O
- O
dataplace O
( O
‚Äò O
this O
places O
gives O
me O
war O
Ô¨Çashbacks O
‚Äô O
) O
, O
comments O
and O
thoughts O
associated O
with O
the O
place O
they O
are O
in O
( O
‚Äò O
few O
of O
us O
dressed O
appropriately O
‚Äô O
) O
or O
activities O
they O
are O
performing O
( O
‚Äò O
leaving O
the O
news O
station O
‚Äô O
, O
‚Äò O
on O
the O
way O
to O
the O
APCE O
Annual O
‚Äô O
) O
. O
In O
this O
paper O
, O
we O
aim O
to O
study O
the O
language O
that O
people O
on O
Twitter O
use O
to O
share O
information O
about O
a O
speciÔ¨Åc O
place O
they O
are O
visiting O
. O
Thus O
, O
we O
deÔ¨Åne O
the O
prediction O
of O
a O
POI O
type O
given O
a O
post O
( O
i.e. O
tweet O
) O
as O
a O
multi O
- O
class O
classiÔ¨Åcation O
task O
using O
only O
information O
available O
at O
posting O
time O
. O
Given O
the O
text O
from O
a O
user O
‚Äôs O
post O
, O
our O
goal O
is O
to O
predict O
the O
correct O
type O
of O
the O
location O
it O
was O
posted O
, O
e.g. O
park O
, O
bar O
or O
shop O
. O
Inferring O
the O
type O
of O
place O
from O
a O
user O
‚Äôs O
post O
using O
linguistic O
information O
, O
is O
useful O
for O
cultural O
geographers O
to O
study O
a O
place O
‚Äôs O
identity O
( O
Tuan O
, O
1991 O
) O
and O
has O
downstream O
geosocial O
applications O
such O
as O
POI O
visualisation O
( O
McKenzie O
et O
al O
. O
, O
2015 O
) O
and O
recommendation O
( O
Alazzawi O
et O
al O
. O
, O
2012 O
; O
Yuan O
et O
al O
. O
, O
2013 O
; O
Preo O
t O
¬∏iuc O
- O
Pietro O
and O
Cohn O
, O
2013 O
; O
Gao O
et O
al O
. O
, O
2015 O
) O
. O
Predicting O
the O
type O
of O
a O
POI O
is O
inherently O
different O
to O
predicting O
the O
POI O
type O
from O
comments O
or O
reviews O
. O
The O
role O
of O
the O
latter O
is O
to O
provide O
opinions O
or O
descriptions O
of O
the O
places O
, O
rather O
than O
the O
activities O
and O
feelings O
of O
the O
user O
posting O
the O
text O
( O
McKenzie O
et O
al O
. O
, O
2015 O
) O
, O
as O
illustrated O
in O
Table O
1 O
. O
This O
is O
also O
different O
, O
albeit O
related O
, O
to O
the O
popular O
task O
of O
geolocation O
prediction O
( O
Cheng O
et O
al O
. O
, O
2010 O
; O
Eisenstein O
et O
al O
. O
, O
2010 O
; O
Han O
et O
al O
. O
, O
2012 O
; O
Roller O
et O
al O
. O
, O
2012 O
; O
Rahimi O
et O
al O
. O
, O
2015 O
; O
Dredze O
et O
al O
. O
, O
2016 O
) O
, O
as O
this O
aims O
to O
infer O
the O
exact O
geographical O
location O
of O
a O
post O
using O
language O
variation O
and O
geographical O
cues O
rather O
than O
inferring O
the O
place O
‚Äôs O
type O
. O
Our O
task O
aims O
to O
uncover O
the O
geographic O
agnostic O
features O
associated O
with O
POIs O
of O
different O
types O
. O
Our O
contributions O
are O
as O
follows O
: O
( O
1 O
) O
We O
provide O
the O
Ô¨Årst O
study O
of O
POI O
type O
prediction O
in O
computational O
linguistics O
; O
( O
2 O
) O
A O
large O
data O
set O
made O
out O
of804Category O
Sample O
Tweet O
Train O
Dev O
Test O
Tokens O
Arts O
& O
Entertainment O
i O
‚Äôm O
back O
in O
central O
park O
. O
this O
place O
gives O
me O
war O
Ô¨Çashbacks O
now O
lol O
40,417 O
4,755 O
5,284 O
14.41 O
College O
& O
University O
currently O
visiting O
my O
dream O
school O
  O
21,275 O
2,418 O
2,884 O
15.52 O
Food O
Some O
Breakfast O
, O
it O
‚Äôs O
only O
right O
! O
# O
LA O
6,676 O
869 O
724 O
14.34 O
Great O
OutdoorsSorry O
Southport O
, O
Billy O
is O
dishing O
out O
donuts O
at O
# O
donutfest O
today O
. O
See O
you O
next O
weekend!27,763 O
4,173 O
3,653 O
13.49 O
Nightlife O
SpotChicago O
really O
needs O
to O
step O
up O
their O
Aloha O
shirt O
game O
. O
Only O
a O
few O
of O
us O
dressed O
‚Äú O
appropriately O
‚Äù O
tonight O
. O
:) O
5,545 O
876 O
656 O
15.46 O
Professional O
& O
Other O
Places O
Leaving O
the O
news O
station O
after O
a O
long O
day O
30,640 O
3,381 O
3,762 O
16.46 O
Shop O
& O
Service O
Came O
to O
get O
an O
old O
fashioned O
tape O
measures O
and O
a O
button O
for O
my O
coat O
8,285 O
886 O
812 O
15.31 O
Travel O
& O
TransportShoutout O
to O
anyone O
currently O
on O
the O
way O
to O
the O
APCE O
Annual O
Event O
in O
Louisville O
, O
KY O
! O
# O
APCE201816,428 O
2,201 O
1,872 O
14.88 O
Table O
1 O
: O
Place O
categories O
with O
sample O
tweets O
and O
data O
set O
statistics O
. O
tweets O
linked O
to O
particular O
POI O
categories O
; O
( O
3 O
) O
Linguistic O
and O
temporal O
analyses O
related O
to O
the O
place O
the O
text O
was O
posted O
from O
; O
( O
4 O
) O
Predictive O
models O
using O
text O
and O
temporal O
information O
reaching O
up O
to O
43.67 O
F1 O
across O
eight O
different O
POI O
types O
. O
2 O
Point O
- O
of O
- O
Interest O
Type O
Data O
We O
deÔ¨Åne O
the O
POI O
type O
prediction O
as O
a O
multi O
- O
class O
classiÔ¨Åcation O
task O
performed O
at O
the O
social O
media O
post O
level O
. O
Given O
a O
post O
T O
, O
deÔ¨Åned O
as O
a O
sequence O
of O
tokensT={t1, O
... O
,tn O
} O
, O
the O
goal O
is O
to O
label O
T O
as O
one O
of O
the O
MPOI O
categories O
. O
We O
create O
a O
novel O
data O
set O
for O
POI O
type O
prediction O
containing O
text O
and O
the O
location O
type O
it O
was O
posted O
from O
as O
, O
to O
the O
best O
of O
our O
knowledge O
, O
no O
such O
data O
set O
is O
available O
. O
We O
use O
Twitter O
as O
our O
data O
source O
because O
it O
contains O
a O
large O
variety O
of O
linguistic O
information O
such O
as O
expression O
of O
thoughts O
, O
opinions O
and O
emotions O
( O
Java O
et O
al O
. O
, O
2007 O
; O
Kouloumpis O
et O
al O
. O
, O
2011 O
) O
. O
2.1 O
Types O
of O
POIs O
Foursquare O
is O
a O
location O
data O
platform O
that O
manages O
‚Äò O
Places O
by O
Foursquare O
‚Äô O
, O
a O
database O
of O
more O
than O
105 O
million O
POIs O
worldwide O
. O
The O
place O
information O
includes O
veriÔ¨Åed O
metadata O
such O
as O
name O
, O
geo O
- O
coordinates O
and O
categories O
as O
well O
as O
other O
user O
- O
sourced O
metadata O
such O
as O
tags O
, O
comments O
or O
photos O
. O
POIs O
are O
organized O
into O
9 O
top O
level O
primary O
categories O
with O
multiple O
subcategories O
. O
We O
only O
focus O
on O
8 O
primary O
top O
- O
level O
POI O
categories O
since O
the O
category O
‚Äò O
Residence O
‚Äô O
has O
a O
considerably O
smaller O
number O
of O
tweets O
compared O
to O
the O
other O
categories O
( O
0.78 O
% O
tweets O
from O
the O
total O
) O
. O
We O
leave O
Ô¨Åner O
- O
grained O
place O
category O
inference O
as O
well O
as O
using O
other O
metadata O
for O
future O
work O
since O
the O
scope O
of O
this O
work O
is O
to O
study O
the O
language O
of O
posts O
associated O
with O
semantic O
type O
places.2.2 O
Associating O
Tweets O
with O
POI O
Types O
Twitter O
users O
can O
tag O
their O
tweets O
to O
the O
locations O
they O
are O
posted O
from O
by O
linking O
to O
Foursquare O
places.2In O
this O
way O
, O
we O
collect O
tweets O
assigned O
to O
the O
POIs O
and O
associated O
metadata O
( O
see O
Table O
1 O
) O
. O
We O
select O
a O
broad O
range O
of O
locations O
for O
our O
experiments O
. O
There O
is O
no O
public O
list O
of O
all O
Foursquare O
locations O
that O
can O
be O
used O
through O
Twitter O
and O
can O
be O
programmatically O
accessed O
. O
Hence O
, O
in O
order O
to O
discover O
Foursquare O
places O
that O
are O
actually O
used O
in O
tweets O
, O
we O
start O
with O
all O
places O
found O
in O
a O
1 O
% O
sample O
of O
the O
Twitter O
feed O
between O
31 O
July O
2016 O
and O
24 O
January O
2017 O
leading O
us O
to O
a O
total O
of O
9,125 O
different O
places O
. O
Then O
, O
we O
collect O
all O
tweets O
from O
these O
places O
between O
17 O
August O
2016 O
and O
1 O
March O
2018 O
using O
the O
Twitter O
Search O
API3 O
. O
We O
collect O
the O
place O
metadata O
from O
the O
public O
Foursquare O
Venues O
API O
. O
This O
resulted O
in O
a O
total O
data O
set O
of O
1,648,963 O
tweets O
tagged O
to O
a O
Foursquare O
place O
. O
In O
order O
to O
extract O
metadata O
about O
each O
location O
, O
we O
crawled O
the O
Twitter O
website O
to O
identify O
the O
corresponding O
Foursquare O
Place O
ID O
of O
each O
Twitter O
place O
. O
Then O
, O
we O
used O
the O
public O
Foursquare O
Venues O
API4to O
download O
all O
the O
place O
metadata O
. O
2.3 O
Data O
Filtering O
To O
limit O
variation O
in O
our O
data O
, O
we O
Ô¨Ålter O
out O
all O
nonEnglish O
tweets O
and O
non O
- O
US O
places O
, O
as O
these O
were O
very O
limited O
in O
number O
. O
We O
keep O
POIs O
with O
at O
least O
20 O
tweets O
and O
randomly O
subsample O
100 O
tweets O
from O
POIs O
with O
more O
tweets O
to O
avoid O
skewing O
our O
data O
. O
Our O
Ô¨Ånal O
data O
set O
consists O
of O
196,235 O
tweets O
from O
2https://developer.foursquare.com/ O
places O
3https://developer.twitter.com/ O
en O
/ O
docs O
/ O
tweets O
/ O
search O
/ O
guides/ O
tweets O
- O
by O
- O
place O
4https://developer.foursquare.com/ O
overview O
/ O
venues.html8052,761 O
POIs O
. O
2.4 O
Data O
Split O
We O
create O
our O
data O
split O
at O
a O
location O
- O
level O
to O
ensure O
that O
our O
models O
are O
robust O
and O
generalize O
to O
locations O
held O
- O
out O
in O
training O
. O
We O
split O
the O
locations O
in O
train O
( O
80 O
% O
) O
, O
development O
( O
10 O
% O
) O
and O
test O
( O
10 O
% O
) O
sets O
and O
assign O
tweets O
to O
one O
of O
the O
three O
splits O
based O
on O
the O
location O
they O
were O
posted O
from O
( O
see O
Table O
1 O
for O
detailed O
statistics O
) O
. O
2.5 O
Text O
Processing O
We O
lower O
- O
case O
text O
and O
replace O
all O
URLs O
and O
mentions O
of O
users O
with O
placeholders O
. O
We O
preserve O
emoticons O
and O
punctuation O
and O
replace O
tokens O
that O
appear O
in O
less O
than O
Ô¨Åve O
tweets O
with O
an O
‚Äò O
unknown O
‚Äô O
token O
. O
We O
tokenize O
text O
using O
a O
Twitter O
- O
aware O
tokenizer O
( O
Schwartz O
et O
al O
. O
, O
2017 O
) O
. O
3 O
Analysis O
We O
Ô¨Årst O
analyze O
our O
data O
set O
to O
understand O
the O
relationship O
between O
location O
type O
, O
language O
and O
posting O
time O
. O
3.1 O
Linguistic O
Analysis O
We O
analyze O
the O
linguistic O
features O
speciÔ¨Åc O
to O
each O
category O
by O
ranking O
unigrams O
that O
appear O
in O
at O
least O
5 O
different O
locations O
, O
such O
that O
these O
are O
representative O
of O
the O
larger O
POI O
category O
rather O
than O
a O
few O
speciÔ¨Åc O
places O
. O
Features O
are O
normalized O
to O
sum O
up O
to O
unit O
for O
each O
tweet O
, O
then O
we O
compute O
the O
( O
Pearson)œá2coefÔ¨Åcient O
independently O
between O
its O
distribution O
across O
posts O
and O
the O
binary O
category O
label O
of O
the O
post O
similar O
to O
the O
approach O
followed O
by O
Maronikolakis O
et O
al O
. O
( O
2020 O
) O
and O
Preo O
t O
¬∏iuc O
- O
Pietro O
et O
al O
. O
( O
2019 O
) O
. O
Table O
2 O
presents O
the O
top O
unigram O
features O
for O
each O
category O
. O
We O
note O
that O
most O
top O
unigrams O
speciÔ¨Åc O
of O
a O
category O
naturally O
refer O
to O
types O
of O
places O
( O
e.g. O
‚Äò O
campus O
‚Äô O
, O
‚Äò O
beach O
‚Äô O
, O
‚Äò O
mall O
‚Äô O
, O
‚Äò O
airport O
‚Äô O
) O
that O
are O
part O
of O
that O
category O
. O
All O
categories O
also O
contain O
words O
that O
refer O
to O
activities O
that O
the O
poster O
of O
the O
tweet O
is O
performing O
or O
observing O
while O
at O
a O
location O
( O
e.g. O
‚Äò O
camp O
‚Äô O
and O
‚Äò O
football O
‚Äô O
for O
College O
, O
‚Äò O
concert O
‚Äô O
and O
‚Äò O
show O
‚Äô O
for O
Arts O
& O
Entertainment O
, O
‚Äò O
party O
‚Äô O
for O
Nightlife O
Spot O
, O
‚Äò O
landed O
‚Äô O
for O
Travel O
& O
Transport O
, O
‚Äò O
hike O
‚Äô O
for O
Greater O
Outdoors O
) O
. O
Nightlife O
Spot O
and O
Food O
categories O
are O
represented O
by O
types O
of O
food O
or O
drinks O
that O
are O
typically O
consumed O
at O
these O
locations O
. O
Beyond O
these O
typical O
associations O
, O
we O
highlight O
that O
usernames O
are O
more O
likely O
mentioned O
inthe O
Arts O
& O
Entertainment O
category O
, O
usually O
indicating O
activities O
involving O
groups O
of O
users O
, O
emojis O
indicative O
of O
the O
user O
state O
( O
e.g. O
happy O
emoji O
in O
Food O
places O
) O
and O
adjectives O
indicative O
of O
the O
user O
‚Äôs O
surroundings O
( O
e.g. O
‚Äò O
beautiful O
‚Äô O
in O
Greater O
Outdoors O
places O
) O
. O
Finally O
, O
we O
also O
uncover O
words O
indicative O
of O
the O
time O
the O
user O
is O
at O
a O
place O
, O
such O
as O
‚Äò O
tonight O
‚Äô O
for O
Arts O
& O
Entertainment O
, O
‚Äò O
sunset O
‚Äô O
for O
the O
Greater O
Outdoors O
and O
‚Äò O
night O
‚Äô O
for O
Nightlife O
Spots O
and O
Arts O
& O
Entertainment O
. O
3.2 O
Temporal O
Analysis O
We O
further O
examine O
the O
relationship O
between O
the O
time O
a O
tweet O
was O
posted O
and O
the O
POI O
type O
it O
was O
posted O
from O
. O
Figure O
1 O
shows O
the O
percentage O
of O
tweets O
by O
day O
of O
week O
( O
top O
) O
and O
hour O
of O
day O
( O
bottom O
) O
. O
We O
observe O
that O
tweets O
posted O
from O
the O
‚Äò O
Professional O
& O
Other O
Places O
‚Äô O
, O
‚Äò O
Travel O
& O
Transport O
‚Äô O
and O
‚Äò O
College O
& O
University O
‚Äô O
categories O
are O
more O
prevalent O
on O
weekdays O
, O
peaking O
on O
Wednesday O
, O
while O
on O
weekends O
more O
tweets O
are O
posted O
from O
the O
‚Äò O
Great O
Outdoors O
‚Äô O
, O
‚Äò O
Arts O
& O
Entertainment O
‚Äô O
, O
‚Äò O
Nightlife O
& O
Spot O
‚Äô O
and O
‚Äò O
Food O
‚Äô O
categories O
when O
people O
focus O
less O
on O
professional O
activities O
and O
dedicate O
more O
time O
to O
leisure O
as O
expected O
. O
The O
hour O
of O
day O
pattern O
follows O
the O
daily O
human O
activity O
rhythm O
, O
but O
the O
differences O
between O
categories O
are O
less O
prominent O
, O
perhaps O
with O
the O
exception O
of O
the O
‚Äò O
Arts O
& O
Entertainment O
‚Äô O
category O
peaks O
around O
8PM O
and O
‚Äò O
Nightlife O
Spots O
‚Äô O
that O
see O
a O
higher O
percent O
of O
tweets O
in O
the O
early O
hours O
of O
the O
day O
( O
between O
1 O
- O
5am O
) O
than O
other O
categories O
. O
4 O
Predicting O
POI O
Types O
of O
Tweets O
4.1 O
Methods O
Logistic O
Regression O
We O
Ô¨Årst O
experiment O
with O
logistic O
regression O
using O
a O
standard O
bag O
of O
n O
- O
grams O
representation O
of O
the O
tweet O
( O
LR O
- O
W O
) O
, O
including O
unigrams O
to O
trigrams O
weighted O
using O
TF O
- O
IDF O
. O
We O
identiÔ¨Åed O
in O
the O
analysis O
section O
that O
temporal O
information O
about O
the O
tweet O
may O
be O
useful O
for O
classiÔ¨Åcation O
. O
Hence O
, O
to O
add O
temporal O
information O
extracted O
from O
a O
tweet O
, O
we O
create O
a O
31 O
- O
dimensional O
vector O
encoding O
the O
hour O
of O
the O
day O
and O
the O
day O
of O
the O
week O
it O
was O
sent O
from O
. O
We O
experiment O
with O
only O
using O
the O
temporal O
features O
( O
LR O
- O
T O
) O
and O
in O
combination O
with O
the O
text O
features O
( O
LR O
- O
W+T O
) O
. O
We O
use O
L1 O
regularization O
( O
Hoerl O
and O
Kennard O
, O
1970 O
) O
with O
hyperparameter O
Œ±=.01(selected O
based O
on O
dev O
set O
from{.001 O
, O
.01 O
, O
.1}).806Arts O
College O
Food O
Outdoors O
Nightlife O
Professional O
Shop O
Travel O
Featureœá2Feature O
œá2Featureœá2Featureœá2Feature O
œá2Featureœá2Featureœá2Feature O
œá2 O
concert O
167.20 O
campus O
298.74 O
chicken O
375.52 O
beach O
591.81 O
# O
craftbeer O
425.97 O
school O
87.46 O
mall O
462.03 O
airport O
394.20 O
museum O
152.14 O
college O
266.63 O
# O
nola O
340.64 O
  O
239.00 O
  O
311.68 O
students O
79.93 O
store O
403.00 O
  O
343.30 O
show O
134.39 O
university O
155.65 O
lunch O
255.98 O
hike O
227.91 O
beer O
203.57 O
grade O
66.05 O
shopping O
359.00 O
Ô¨Çight O
292.94 O
night O
104.48 O
class O
112.23 O
fried O
216.49 O
lake O
193.58 O
bar O
93.90 O
vote O
65.80 O
shop O
132.39 O
hotel O
168.38 O
tonight O
80.76 O
semester O
103.19 O
dinner O
203.65 O
park O
165.92 O
  O
67.00 O
our O
63.12 O
  O
126.07 O
conference O
141.74 O
game O
73.56 O
football O
59.24 O
  O
195.41 O
island O
151.45 O
  O
56.94 O
jv O
60.64 O
  O
95.32 O
landed O
118.05 O
art O
69.77 O
student O
57.86 O
pizza O
190.83 O
sunset O
142.44 O
dj O
56.56 O
church O
52.97 O
apple O
88.74 O
plane O
88.42 O
USER O
66.14 O
classes O
57.37 O
shrimp O
188.77 O
hiking O
137.74 O
tonight O
53.39 O
hs O
50.63 O
market O
76.60 O
bound O
78.43 O
zoo O
66.09 O
students O
56.98 O
  O
179.39 O
beautiful O
109.45 O
ale O
52.62 O
senior O
50.05 O
auto O
73.52 O
heading O
62.09 O
baseball O
62.90 O
camp O
44.19 O
  O
151.00 O
bridge O
108.56 O
party O
51.14 O
ss O
44.46 O
stock O
72.31 O
headed O
57.12 O
Table O
2 O
: O
Unigrams O
associated O
with O
each O
category O
, O
sorted O
by O
œá2value O
computed O
between O
the O
normalized O
frequency O
of O
each O
feature O
and O
the O
category O
label O
across O
all O
tweets O
in O
the O
training O
set O
( O
p<0.001 O
) O
. O
Figure O
1 O
: O
Percentage O
of O
tweets O
by O
day O
of O
week O
( O
top O
) O
and O
by O
hour O
of O
day O
( O
bottom O
) O
. O
BiLSTM O
We O
train O
models O
based O
on O
bidirectional O
Long O
- O
Short O
Term O
Memory O
( O
LSTM O
) O
networks O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
, O
which O
are O
popular O
in O
text O
classiÔ¨Åcation O
tasks O
. O
Tokens O
in O
a O
tweet O
are O
mapped O
to O
embeddings O
and O
passed O
through O
the O
two O
LSTM O
networks O
, O
each O
processing O
the O
input O
in O
opposite O
directions O
. O
The O
outputs O
are O
concatenated O
and O
passed O
to O
the O
output O
layer O
using O
a O
softmax O
activation O
function O
( O
BiLSTM O
) O
. O
We O
extend O
the O
BiLSTM O
to O
encode O
temporal O
one O
- O
hot O
representation O
by O
: O
( O
a O
) O
concatenating O
the O
temporal O
vector O
to O
the O
tweet O
representation O
( O
BiLSTM O
- O
TC O
) O
; O
and O
( O
b O
) O
projecting O
the O
time O
vector O
into O
a O
dense O
representation O
using O
a O
fully O
connected O
layer O
which O
is O
added O
to O
the O
tweet O
representation O
before O
passing O
it O
through O
the O
output O
layer O
using O
a O
softmax O
activation O
function O
( O
BiLSTM O
- O
TS O
) O
. O
We O
use O
200dimensional O
GloVe O
embeddings O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
pre O
- O
trained O
on O
Twitter O
data O
. O
The O
maximum O
sequence O
length O
is O
set O
to O
26 O
, O
covering O
95 O
% O
of O
thetweets O
in O
the O
training O
set O
. O
The O
LSTM O
size O
is O
h= O
32 O
whereh‚àà{32,64,100,300}with O
dropout O
d O
= O
0.5 O
where O
d‚àà{.2,.5 O
} O
. O
We O
use O
Adam O
( O
Kingma O
and O
Ba O
, O
2014 O
) O
with O
default O
learning O
rate O
, O
minimizing O
cross O
- O
entropy O
using O
a O
batch O
size O
of O
32 O
over O
10 O
epochs O
with O
early O
stopping O
. O
BERT O
Bidirectional O
Encoder O
Representations O
from O
Transformers O
( O
BERT O
) O
is O
a O
pre O
- O
trained O
language O
model O
based O
on O
transformer O
networks O
( O
Vaswani O
et O
al O
. O
, O
2017 O
; O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O
BERT O
consists O
of O
multiple O
multi O
- O
head O
attention O
layers O
to O
learn O
bidirectional O
embeddings O
for O
input O
tokens O
. O
The O
model O
is O
trained O
on O
masked O
language O
modeling O
, O
where O
a O
fraction O
of O
the O
input O
tokens O
in O
a O
given O
sequence O
is O
replaced O
with O
a O
mask O
token O
, O
and O
the O
model O
attempts O
to O
predict O
the O
masked O
tokens O
based O
on O
the O
context O
provided O
by O
the O
non O
- O
masked O
tokens O
in O
the O
sequence O
. O
We O
Ô¨Åne O
- O
tune O
BERT O
for O
predicting O
the O
POI O
type O
of O
a O
tweet O
by O
adding O
a O
classiÔ¨Åcation O
layer O
with O
softmax O
activation O
function O
on O
top O
of O
the O
Transformer O
output O
for O
the O
‚Äò O
classiÔ¨Åcation O
‚Äô O
[ O
CLS]token O
( O
BERT O
) O
. O
Similarly O
to O
the O
previous O
models O
, O
we O
extend O
BERT O
to O
make O
use O
of O
the O
time O
vector O
in O
two O
ways O
, O
by O
concatenating O
( O
BERT O
- O
TC O
) O
, O
and O
by O
adding O
it O
( O
BERTTS O
) O
to O
the O
output O
of O
the O
Transformer O
before O
passing O
it O
to O
through O
the O
classiÔ¨Åcation O
layer O
with O
softmax O
activation O
function O
. O
We O
use O
the O
base O
model O
( O
12layer O
, O
110 O
M O
parameters O
) O
trained O
on O
lower O
- O
cased O
English O
text O
. O
We O
Ô¨Åne O
- O
tune O
it O
for O
2 O
epochs O
with O
a O
learning O
rate O
l= O
2e‚àí5,l‚àà{2e‚àí5,3e‚àí5,5e‚àí5 O
} O
and O
a O
batch O
size O
of O
32 O
. O
4.2 O
Results O
Table O
3 O
presents O
the O
results O
of O
POI O
type O
prediction O
measured O
using O
accuracy O
, O
macro O
F1 O
, O
precision O
and O
recall O
across O
three O
runs O
. O
In O
general O
, O
we O
observe O
that O
we O
can O
predict O
POI O
types O
of O
tweets O
with O
good O
accuracy O
, O
considering O
the O
classiÔ¨Åcation O
is O
across O
eight O
relatively O
well O
balanced O
classes.807Model O
Acc O
F1 O
P O
R O
Major O
. O
Class O
26.89 O
5.30 O
3.36 O
12.50 O
Random O
13.63 O
12.64 O
13.63 O
15.68 O
LR O
- O
T O
27.93 O
14.01 O
15.78 O
16.06 O
LR O
- O
W O
43.04 O
37.33 O
37.06 O
38.03 O
LR O
- O
W+T O
43.73 O
37.83 O
37.68 O
38.37 O
BiLSTM O
44.38 O
35.77 O
45.29 O
33.78 O
BiLSTM O
- O
TC O
44.01 O
38.07 O
41.51 O
36.46 O
BiLSTM O
- O
TS O
44.72 O
38.26 O
42.91 O
36.30 O
BERT O
48.89 O
43.67 O
48.44 O
41.33 O
BERT O
- O
TC O
46.13 O
41.19 O
46.81 O
39.03 O
BERT O
- O
TS O
49.17 O
43.47 O
48.40 O
41.26 O
Table O
3 O
: O
Accuracy O
( O
Acc O
) O
, O
Macro O
- O
F1 O
Score O
( O
F1 O
) O
, O
Precision O
macro O
( O
P O
) O
, O
and O
Recall O
macro O
( O
R O
) O
for O
POI O
type O
prediction O
( O
all O
std O
. O
dev O
< O
0.01 O
) O
. O
Best O
results O
are O
in O
bold O
. O
Best O
results O
are O
obtained O
using O
BERT O
- O
based O
models O
( O
BERT O
, O
BERT O
- O
TC O
and O
BERT O
- O
TS O
) O
, O
with O
the O
highest O
accuracy O
of O
49.17 O
( O
compared O
to O
26.89 O
majority O
class O
) O
and O
highest O
macro O
- O
F1 O
of O
43.67 O
( O
compared O
to O
12.64 O
random O
) O
. O
We O
observe O
that O
BERT O
models O
outperform O
both O
BiLSTM O
and O
linear O
methods O
across O
all O
metrics O
, O
with O
over O
4 O
% O
improvement O
in O
accuracy O
and O
5 O
points O
F1 O
. O
The O
BiLSTM O
models O
perform O
marginally O
better O
than O
the O
linear O
models O
. O
Temporal O
features O
alone O
are O
marginally O
useful O
when O
models O
are O
evaluated O
using O
accuracy O
( O
+0.28 O
BERT O
, O
+0.34 O
for O
BiLSTMs O
, O
+0.69 O
for O
LR O
) O
and O
perform O
similarly O
on O
F1 O
, O
with O
the O
notable O
exception O
of O
the O
BiLSTM O
models O
. O
We O
Ô¨Ånd O
that O
adding O
these O
features O
is O
more O
beneÔ¨Åcial O
than O
concatenating O
them O
, O
with O
concatenation O
hurting O
performance O
on O
accuracy O
for O
both O
BiLSTM O
and O
BERT O
. O
Figure O
2 O
shows O
the O
confusion O
matrix O
of O
our O
best O
performing O
model O
, O
BERT O
, O
according O
to O
the O
macroF1 O
score O
. O
The O
confusion O
matrix O
is O
normalized O
over O
the O
actual O
values O
( O
rows O
) O
. O
The O
category O
‚Äò O
Arts O
& O
Entertainment O
‚Äò O
has O
the O
greatest O
percentage O
( O
62 O
% O
) O
of O
correctly O
classiÔ¨Åed O
tweets O
, O
followed O
by O
the O
‚Äò O
Great O
Outdoors O
‚Äò O
category O
with O
54 O
% O
, O
and O
the O
‚Äò O
College O
& O
University O
‚Äò O
category O
with O
44 O
% O
. O
On O
the O
other O
hand O
, O
the O
categories O
‚Äò O
Nightlife O
Spot O
‚Äò O
and O
‚Äò O
Shop O
& O
Service O
‚Äò O
have O
the O
lowest O
results O
, O
where O
30 O
% O
of O
the O
tweets O
predicted O
as O
each O
of O
these O
classes O
is O
correctly O
classiÔ¨Åed O
. O
Most O
common O
error O
is O
when O
the O
model O
classiÔ¨Åes O
tweets O
from O
the O
category O
‚Äò O
College O
& O
University O
‚Äô O
as O
‚Äò O
Professional O
& O
Other O
Places O
‚Äô O
, O
as O
tweets O
from O
these O
places O
contain O
similar O
terms O
such O
as O
‚Äò O
students O
‚Äô O
or O
‚Äò O
class O
‚Äô O
. O
5 O
Conclusion O
We O
presented O
the O
Ô¨Årst O
study O
on O
predicting O
the O
POI O
type O
a O
social O
media O
message O
was O
posted O
from O
Figure O
2 O
: O
Confusion O
Matrix O
of O
the O
best O
performing O
model O
( O
BERT O
) O
. O
and O
developed O
a O
large O
- O
scale O
data O
set O
with O
tweets O
mapped O
to O
their O
POI O
category O
. O
We O
conducted O
an O
analysis O
to O
uncover O
features O
speciÔ¨Åc O
to O
place O
type O
and O
trained O
predictive O
models O
to O
infer O
the O
POI O
category O
using O
only O
tweet O
text O
and O
posting O
time O
with O
accuracy O
close O
to O
50 O
% O
across O
eight O
categories O
. O
Future O
work O
will O
focus O
on O
using O
other O
modalities O
such O
as O
network O
( O
Aletras O
and O
Chamberlain O
, O
2018 O
; O
Tsakalidis O
et O
al O
. O
, O
2018 O
) O
or O
image O
information O
( O
Vempala O
and O
Preo O
t O
¬∏iuc O
- O
Pietro O
, O
2019 O
; O
Alikhani O
et O
al O
. O
, O
2019 O
) O
and O
prediction O
at O
a O
more O
granular O
level O
of O
POI O
types O
. O
Acknowledgments O
DSV O
is O
supported O
by O
the O
Centre O
for O
Doctoral O
Training O
in O
Speech O
and O
Language O
Technologies O
( O
SLT O
) O
and O
their O
Applications O
funded O
by O
the O
UK O
Research O
and O
Innovation O
grant O
EP O
/ O
S023062/1 O
. O
NA O
is O
supported O
by O
ESRC O
grant O
ES O
/ O
T012714/1 O
. O
Abstract O
Event O
information O
is O
usually O
scattered O
across O
multiple O
sentences O
within O
a O
document O
. O
The O
local O
sentence O
- O
level O
event O
extractors O
often O
yield O
many O
noisy O
event O
role O
Ô¨Åller O
extractions O
in O
the O
absence O
of O
a O
broader O
view O
of O
the O
documentlevel O
context O
. O
Filtering O
spurious O
extractions O
and O
aggregating O
event O
information O
in O
a O
document O
remains O
a O
challenging O
problem O
. O
Following O
the O
observation O
that O
a O
document O
has O
several O
relevant O
event O
regions O
densely O
populated O
with O
event O
role O
Ô¨Ållers O
, O
we O
build O
graphs O
with O
candidate O
role O
Ô¨Åller O
extractions O
enriched O
by O
sentential O
embeddings O
as O
nodes O
, O
and O
use O
graph O
attention O
networks O
to O
identify O
event O
regions O
in O
a O
document O
and O
aggregate O
event O
information O
. O
We O
characterize O
edges O
between O
candidate O
extractions O
in O
a O
graph O
into O
rich O
vector O
representations O
to O
facilitate O
event O
region O
identiÔ¨Åcation O
. O
The O
experimental O
results O
on O
two O
datasets O
of O
two O
languages O
show O
that O
our O
approach O
yields O
new O
state O
- O
of O
- O
the O
- O
art O
performance O
for O
the O
challenging O
event O
extraction O
task O
. O
1 O
Introduction O
Event O
Extraction O
( O
EE O
) O
, O
a O
challenging O
task O
in O
Natural O
Language O
Processing O
, O
aims O
to O
extract O
key O
types O
of O
information O
( O
aka O
event O
roles O
, O
e.g. O
, O
perpetrators O
andvictims O
of O
an O
attack O
event O
) O
that O
can O
represent O
an O
event O
in O
texts O
and O
plays O
a O
critical O
role O
in O
downstream O
applications O
such O
as O
Question O
Answer O
( O
Yang O
et O
al O
. O
, O
2003 O
) O
and O
Summarizing O
( O
Filatova O
and O
Hatzivassiloglou O
, O
2004 O
) O
. O
Existing O
research O
on O
EE O
mostly O
focused O
on O
sentence O
- O
level O
, O
such O
as O
the O
evaluation O
in O
Automatic O
Content O
Extraction O
( O
ACE O
) O
20051 O
. O
However O
, O
an O
event O
is O
usually O
described O
in O
‚àóMost O
of O
the O
work O
was O
done O
when O
the O
Ô¨Årst O
author O
was O
a O
research O
engineer O
in O
the O
Institute O
of O
Automation O
, O
CAS O
. O
1http://projects.ldc.upenn.edu/ace/ O
Event O
Template O
Event O
Roles O
Role O
Fillers O
PerpI O
nd O
TERRORISTS O
, O
HOODED O
INDIVIDUALS O
PerpOrg O
SHINING O
PATH O
Victim O
DOLORES O
HINOSTROZA O
, O
  O
HINOSTROZA O
Original O
Document O
S1 O
: O
That O
alleged O
  O
TERRORISTS O
today O
killed O
  O
DOLORES O
HINOSTROZA O
, O
the O
  O
mayor O
of O
  O
Mulqui O
district O
. O
  O
S2 O
: O
  O
HINOSTROZA O
, O
who O
was O
at O
home O
, O
was O
shot O
five O
times O
. O
S3 O
: O
H O
inostroza O
's O
children O
told O
police O
that O
  O
four O
HOODED O
INDIVIDUALS O
  O
broke O
into O
the O
  O
HOUSE O
and O
shot O
their O
mother O
after O
having O
insulted O
her O
. O
  O
S4 O
: O
  O
And O
their O
  O
FATHER O
was O
on O
a O
business O
trip O
then O
. O
S5 O
: O
  O
DOLORES O
  O
HINOSTROZA O
  O
deceased O
  O
when O
the O
ambulance O
came O
. O
S6 O
: O
  O
She O
is O
the O
second O
woman O
mayor O
killed O
this O
week O
by O
alleged O
  O
commando O
groups O
of O
the O
Maoist O
  O
SHINING O
PATH O
. O
Region1 O
Region2Figure O
1 O
: O
An O
example O
of O
document O
- O
level O
event O
extraction O
. O
We O
need O
to O
extract O
noun O
phrases O
from O
the O
document O
as O
role O
Ô¨Ållers O
for O
the O
event O
roles O
in O
the O
predeÔ¨Åned O
event O
template O
. O
The O
uppercased O
noun O
phrases O
in O
the O
document O
are O
role O
Ô¨Ållers O
extracted O
by O
the O
sentence O
- O
level O
extractor O
. O
Red O
phrases O
are O
correct O
while O
green O
phrases O
are O
noises O
compared O
to O
the O
standard O
in O
the O
template O
. O
There O
are O
two O
event O
regions O
in O
the O
sample O
document O
. O
multiple O
sentences O
in O
a O
document O
. O
As O
illustrated O
in O
Figure O
1 O
, O
relevant O
event O
information O
( O
noun O
phrases O
in O
green O
color O
) O
is O
scattered O
across O
the O
whole O
document O
. O
To O
extract O
event O
information O
accurately O
and O
comprehensively O
at O
document O
- O
level O
, O
it O
is O
necessary O
to O
understand O
the O
wider O
context O
spanning O
over O
multiple O
sentences O
. O
The O
existing O
approaches O
for O
event O
extraction O
( O
EE O
) O
often O
decompose O
the O
document O
- O
level O
EE O
into O
sentence O
- O
level O
EE O
, O
and O
extract O
candidate O
event O
role O
Ô¨Ållers O
from O
individual O
sentences O
one O
by O
one O
. O
The O
event O
role O
Ô¨Åller O
extractors O
often O
use O
extraction O
patterns O
( O
Riloff O
, O
1996 O
) O
or O
classiÔ¨Åers O
( O
Boros O
et O
al O
. O
, O
2014 O
) O
to O
identify O
typical O
local O
contexts O
containing O
a O
certain O
type O
of O
event O
role O
Ô¨Ållers O
. O
However O
, O
local O
event O
role O
Ô¨Åller O
extractors O
often O
produce O
many O
false811candidates O
, O
e.g. O
, O
the O
red O
noun O
phrases O
shown O
in O
the O
example O
document O
of O
Figure O
1 O
. O
As O
shown O
in O
the O
example O
, O
one O
document O
often O
mentions O
a O
target O
event O
multiple O
times O
and O
each O
time O
it O
takes O
one O
or O
more O
sentences O
to O
articulate O
the O
event O
. O
The O
target O
event O
role O
Ô¨Ållers O
tend O
to O
be O
mentioned O
in O
several O
groups O
of O
adjacent O
sentences O
, O
and O
we O
deÔ¨Åne O
those O
adjacent O
relevant O
sentences O
as O
different O
event O
regions O
. O
For O
example O
, O
in O
Figure O
1 O
, O
the O
document O
mentions O
the O
target O
event O
twice O
in O
two O
regions O
. O
The O
correct O
role O
Ô¨Ållers O
are O
crowding O
in O
the O
Ô¨Årst O
event O
region O
S1,S2,S3and O
the O
second O
one O
S5,S6respectively O
. O
Nevertheless O
, O
the O
sentence O
- O
level O
extractor O
will O
extract O
noise O
from O
both O
the O
event O
regions O
like O
HOUSE O
from O
S3and O
irrelevant O
sentence O
like O
FATHER O
inS4 O
, O
destroying O
the O
layout O
of O
the O
original O
regions O
. O
Many O
previous O
efforts O
try O
to O
avoid O
aggregating O
the O
noisy O
candidates O
by O
detecting O
such O
event O
regions O
. O
The O
popular O
approach O
is O
to O
apply O
sentential O
classiÔ¨Åcation O
to O
Ô¨Ålter O
the O
sentences O
and O
recognize O
role O
Ô¨Ållers O
from O
the O
chosen O
sentences O
( O
Patwardhan O
and O
Riloff O
, O
2009 O
; O
Huang O
and O
Riloff O
, O
2012 O
) O
. O
However O
, O
these O
approaches O
only O
detect O
regions O
at O
single O
sentence O
- O
level O
and O
ignore O
the O
crowding O
of O
relevant O
sentences O
. O
Also O
, O
they O
also O
suffer O
from O
the O
accumulative O
error O
of O
sentential O
classiÔ¨Åcation O
. O
For O
example O
, O
they O
may O
identify O
S2as O
a O
relevant O
event O
region O
but O
S3as O
irrelevant O
because O
they O
fail O
to O
take O
into O
account O
the O
similarity O
of O
S2and O
S3 O
. O
Another O
solution O
proposed O
by O
Yang O
et O
al O
. O
( O
2018 O
) O
tries O
to O
detect O
the O
primary O
event O
description O
sentence O
and O
supplement O
the O
missing O
event O
roles O
with O
Ô¨Ållers O
from O
adjacent O
sentences O
. O
This O
method O
considers O
the O
multiple O
sentences O
in O
an O
event O
region O
but O
is O
limited O
to O
one O
region O
per O
document O
. O
For O
instance O
, O
it O
may O
detect O
S1as O
the O
primary O
sentence O
and O
supplement O
it O
with O
S2 O
, O
missing O
the O
valid O
items O
like O
SHINING O
PATH O
from O
region O
2 O
. O
Moreover O
, O
it O
also O
suffers O
from O
the O
errors O
selecting O
primary O
sentence O
, O
and O
the O
supplementing O
strategy O
is O
coarse O
- O
grained O
and O
fails O
to O
take O
into O
account O
every O
candidate O
Ô¨Åller O
individually O
. O
We O
build O
a O
graph O
for O
each O
document O
to O
directly O
model O
the O
multiple O
event O
regions O
in O
a O
document O
, O
each O
region O
potentially O
consisting O
of O
multiple O
sentences O
. O
In O
each O
document O
graph O
, O
the O
nodes O
are O
candidate O
event O
role O
Ô¨Ållers O
and O
we O
insert O
an O
edge O
between O
two O
nodes O
based O
on O
either O
positional O
proximity O
( O
in O
adjacent O
sentences O
or O
within O
the O
same O
sentence O
) O
or O
the O
coreference O
relation O
between O
two O
candidate O
extractions O
. O
The O
document O
graphs O
capturesentence O
similarities O
and O
sophisticated O
discourse O
connections O
among O
the O
candidate O
event O
role O
Ô¨Ållers O
to O
reconstruct O
the O
original O
event O
regions O
, O
which O
can O
recognize O
false O
event O
role O
Ô¨Åller O
extractions O
from O
irrelevant O
sentences O
. O
For O
example O
, O
after O
identifying O
the O
differences O
between O
S4and O
adjacent O
sentences O
S3and O
S5 O
, O
our O
model O
will O
Ô¨Ålter O
the O
noisy O
candidate O
FATHER O
inS4 O
. O
Furthermore O
, O
constructing O
document O
graphs O
formed O
by O
candidate O
event O
role O
Ô¨Ållers O
and O
applying O
graph O
neural O
networks O
will O
enable O
recognizing O
false O
event O
role O
Ô¨Åller O
extractions O
within O
an O
event O
region O
. O
We O
employ O
attentional O
networks O
on O
the O
graphs O
to O
reinforce O
each O
candidate O
‚Äôs O
representations O
by O
global O
contextual O
information O
and O
then O
classify O
the O
candidates O
in O
a O
Ô¨Åne O
- O
grained O
manner O
. O
SpeciÔ¨Åcally O
, O
we O
characterize O
the O
edges O
into O
vector O
representations O
with O
rich O
features O
to O
control O
the O
information O
Ô¨Çowing O
between O
any O
two O
nodes O
. O
For O
instance O
, O
this O
mechanism O
will O
be O
likely O
to O
recognize O
that O
it O
is O
a O
murder O
event O
based O
on O
the O
sentential O
contexts O
of O
sentences O
S2and O
S3 O
, O
and O
therefore O
determine O
that O
the O
candidate O
extraction O
HOUSE O
is O
a O
false O
extraction O
because O
the O
Targets O
of O
a O
murder O
are O
individuals O
most O
commonly O
, O
but O
not O
physical O
targets O
or O
buildings O
. O
We O
evaluate O
our O
approach O
on O
two O
documentlevel O
event O
extraction O
datasets O
: O
the O
MUC-4 O
dataset O
and O
a O
newly O
created O
dataset O
CFEED2 O
. O
Experimental O
results O
show O
that O
the O
proposed O
approach O
successfully O
reconstructs O
70 O
% O
of O
the O
event O
regions O
and O
yields O
new O
state O
- O
of O
- O
the O
- O
art O
performance O
for O
event O
extraction O
on O
both O
datasets O
. O
In O
summary O
, O
the O
main O
contributions O
of O
this O
paper O
are O
as O
follows O
: O
‚Ä¢We O
propose O
graphs O
directly O
modeling O
the O
multiple O
regions O
with O
multiple O
sentences O
, O
which O
successfully O
help O
to O
reconstruct O
event O
regions O
naturally O
avoid O
redundant O
extractions O
irrelevant O
sources O
. O
‚Ä¢We O
propose O
an O
edge O
- O
enriched O
graph O
attention O
algorithm O
that O
can O
blend O
both O
the O
local O
clues O
and O
global O
context O
to O
enforce O
semantic O
representations O
for O
each O
candidate O
and O
help O
to O
Ô¨Ålter O
noises O
in O
the O
event O
regions O
. O
‚Ä¢Experimental O
results O
show O
that O
our O
method O
outperforms O
the O
existing O
state O
- O
of O
- O
the O
- O
arts O
on O
two O
datasets O
with O
different O
languages O
, O
including O
a O
public O
English O
MUC-4 O
dataset O
and O
a O
large O
- O
scale O
Chinese O
CFEED O
dataset O
. O
2http://www.nlpr.ia.ac.cn/cip/ O
Àúliukang O
/ O
dataset O
/ O
documentevent1.html8122 O
Related O
Work O
Sentence O
- O
level O
EE O
has O
achieved O
a O
lot O
of O
advancement O
in O
recent O
work O
( O
Chen O
et O
al O
. O
, O
2015 O
; O
Nguyen O
et O
al O
. O
, O
2016 O
; O
Chen O
et O
al O
. O
, O
2018 O
) O
and O
can O
be O
classiÔ¨Åed O
into O
template O
- O
based O
approaches O
( O
Jungermann O
and O
Morik O
, O
2008 O
; O
Bjorne O
et O
al O
. O
, O
2010 O
; O
Hogenboom O
et O
al O
. O
, O
2016 O
) O
and O
statistical O
approaches O
. O
Templatebased O
methods O
require O
human O
- O
crafted O
templates O
to O
match O
the O
events O
. O
Most O
of O
the O
statistical O
methods O
are O
supervised O
and O
either O
based O
on O
feature O
engineering O
( O
Ahn O
, O
2006 O
; O
Ji O
and O
Grishman O
, O
2008 O
; O
Liao O
and O
Grishman O
, O
2010 O
; O
Reichart O
and O
Barzilay O
, O
2012 O
) O
or O
Neural O
network O
algorithm O
( O
Chen O
et O
al O
. O
, O
2015 O
; O
Nguyen O
et O
al O
. O
, O
2016 O
; O
Chen O
et O
al O
. O
, O
2018 O
; O
Liu O
et O
al O
. O
, O
2018 O
; O
Sha O
et O
al O
. O
, O
2018 O
; O
Liu O
et O
al O
. O
, O
2018 O
) O
. O
However O
, O
these O
supervised O
methods O
rely O
on O
intensive O
manual O
annotations O
. O
To O
alleviate O
this O
problem O
, O
many O
weak O
supervised O
methods O
( O
Chen O
et O
al O
. O
, O
2017 O
; O
Zeng O
et O
al O
. O
, O
2018 O
) O
have O
arisen O
and O
achieved O
good O
performance O
in O
ACE O
2005 O
evaluation O
. O
However O
, O
most O
of O
the O
time O
, O
people O
care O
about O
the O
events O
discussed O
across O
a O
whole O
document O
. O
So O
research O
on O
document O
- O
level O
EE O
also O
prevails O
. O
Traditionally O
, O
pattern O
- O
based O
and O
classiÔ¨Åer O
- O
based O
methods O
are O
popular O
to O
solve O
this O
task O
. O
Systems O
like O
AutoSlog O
( O
Riloff O
et O
al O
. O
, O
1993 O
) O
and O
AutoSlogTS O
( O
Riloff O
, O
1996 O
) O
directly O
applied O
regular O
patterns O
to O
extract O
role O
Ô¨Ållers O
. O
Many O
works O
( O
Patwardhan O
and O
Riloff O
, O
2007 O
, O
2009 O
; O
Huang O
and O
Riloff O
, O
2011 O
, O
2012 O
; O
Boros O
et O
al O
. O
, O
2014 O
) O
relied O
on O
feature O
- O
based O
classiÔ¨Åers O
to O
distinguish O
candidate O
role O
Ô¨Ållers O
from O
texts O
and O
achieved O
better O
performance O
. O
Until O
recent O
years O
, O
researchers O
( O
Hsi O
, O
2018 O
; O
Yang O
et O
al O
. O
, O
2018 O
; O
Zheng O
et O
al O
. O
, O
2019 O
) O
began O
to O
utilize O
multiple O
neuralbased O
methods O
to O
solve O
the O
task O
. O
Notably O
, O
among O
the O
document O
- O
level O
EE O
research O
, O
some O
works O
( O
Patwardhan O
and O
Riloff O
, O
2009 O
; O
Huang O
and O
Riloff O
, O
2012 O
; O
Yang O
et O
al O
. O
, O
2018 O
) O
have O
noticed O
the O
importance O
of O
identifying O
event O
regions O
to O
improve O
performance O
. O
Traditional O
neural O
networks O
such O
as O
Convolutional O
Neural O
Networks O
and O
Recursive O
Neural O
Networks O
are O
hard O
to O
deal O
with O
graphical O
data O
structures O
, O
so O
many O
graph O
- O
based O
neural O
networks O
( O
GNNs O
) O
emerge O
( O
Gori O
et O
al O
. O
, O
2005 O
; O
Bruna O
et O
al O
. O
, O
2013 O
; O
Kipf O
and O
Welling O
, O
2016 O
) O
. O
In O
order O
to O
deal O
with O
graphs O
with O
different O
edge O
types O
, O
relational O
GNNs O
( O
Schlichtkrull O
et O
al O
. O
, O
2018 O
; O
Marcheggiani O
and O
Titov O
, O
2017 O
; O
Vashishth O
et O
al O
. O
, O
2019 O
; O
Bastings O
et O
al O
. O
, O
2017 O
) O
try O
to O
use O
separate O
weights O
for O
different O
edges O
. O
However O
, O
one O
limitation O
of O
these O
GNNs O
is O
that O
the O
weights O
are O
Ô¨Åxed O
for O
allneighbors O
. O
So O
Veli O
Àáckovi O
¬¥ O
c O
et O
al O
. O
( O
2017 O
) O
leveraged O
masked O
attentional O
layers O
( O
GATs O
) O
to O
learn O
adaptive O
weights O
for O
different O
neighbors O
. O
By O
now O
, O
some O
works O
( O
Schlichtkrull O
et O
al O
. O
, O
2018 O
; O
Vashishth O
et O
al O
. O
, O
2019 O
) O
have O
successfully O
applied O
GNNs O
to O
model O
the O
document O
- O
level O
information O
within O
texts O
and O
achieved O
state O
- O
of O
- O
the O
- O
art O
performance O
. O
Our O
model O
is O
distinguishing O
because O
we O
not O
only O
utilize O
these O
recent O
advances O
but O
also O
turns O
the O
relational O
edges O
to O
feature O
- O
enriched O
nodes O
and O
extends O
GATs O
on O
such O
heterogeneous O
graphs O
. O
3 O
Fine O
- O
grained O
Filtering O
Framework O
3.1 O
Overall O
Framework O
Our O
method O
for O
document O
- O
level O
Event O
Extraction O
follows O
three O
main O
procedures O
. O
Extracting O
role O
candidates O
by O
sentence O
- O
level O
event O
extractor O
( O
SEE O
) O
: O
Given O
a O
document O
, O
we O
disintegrate O
it O
into O
a O
series O
of O
sentences O
and O
apply O
sentence O
- O
level O
event O
extractors O
to O
identify O
candidate O
role O
Ô¨Ållers O
. O
Constructing O
graphs O
to O
model O
event O
regions O
: O
Based O
on O
the O
primitive O
results O
from O
the O
last O
step O
and O
the O
properties O
of O
event O
regions O
, O
we O
build O
graphs O
to O
capture O
both O
the O
local O
clues O
and O
global O
context O
among O
those O
candidates O
. O
Selecting O
role O
Ô¨Ållers O
via O
edge O
- O
enriched O
graph O
attention O
networks O
( O
EE O
- O
GAT O
) O
: O
We O
encode O
the O
different O
edges O
into O
vectors O
and O
then O
leverage O
the O
attention O
mechanism O
on O
the O
edge O
- O
enriched O
graphs O
to O
update O
the O
nodes O
‚Äô O
representations O
. O
After O
that O
, O
we O
feed O
the O
candidates O
to O
classiÔ¨Åers O
for O
Ô¨Åltering O
. O
3.2 O
Extracting O
Role O
Candidates O
by O
Sentence O
- O
level O
Event O
Extractor O
Sentence O
- O
level O
Event O
Extractor O
aims O
at O
extracting O
event O
roles O
from O
each O
sentence O
in O
a O
document O
. O
We O
reproduce O
the O
SEE O
introduced O
by O
Yang O
et O
al O
. O
( O
2018 O
) O
and O
employ O
BiLSTM O
- O
CRF O
to O
identify O
candidates O
from O
each O
sentence O
. O
The O
model O
uses O
the O
word O
embedding O
as O
the O
input O
features O
, O
and O
this O
method O
is O
compatible O
with O
both O
the O
English O
and O
Chinese O
corpus O
. O
3.3 O
Constructing O
Graphs O
to O
Model O
Event O
Regions O
For O
each O
document O
, O
we O
want O
to O
utilize O
the O
observed O
event O
region O
information O
in O
our O
model O
. O
As O
discussed O
before O
, O
the O
original O
event O
region O
information O
of O
the O
candidates O
from O
the O
SEE O
is O
destroyed O
. O
So O
we O
make O
use O
of O
the O
properties O
of O
the O
original813Candidate O
Role O
Fillers O
from O
SEE O
S1 O
: O
T O
hat O
alleged O
  O
[ O
c1 O
: O
TERRORISTS O
] O
  O
PerpInd O
today O
  O
killed O
   O
[ O
c2 O
: O
  O
DOLORES O
HINOSTROZA O
  O
] O
Victim O
, O
the O
  O
mayor O
of O
  O
Mulqui O
district O
. O
  O
S2 O
: O
[ O
c3 O
: O
  O
HINOSTROZA O
] O
Victim O
, O
who O
was O
at O
home O
, O
  O
was O
shot O
five O
times O
. O
S3 O
: O
‚Ä¶ O
  O
that O
four O
  O
[ O
c O
4 O
: O
HOODED O
INDIVIDUALS O
] O
  O
PerpInd O
broke O
into O
the O
  O
[ O
c O
5 O
: O
HOUSE O
] O
  O
Target O
and O
shot O
‚Ä¶ O
S4 O
: O
‚Ä¶ O
  O
their O
  O
[ O
c6 O
: O
FATHER O
] O
PerpInd O
was O
on O
‚Ä¶ O
S5 O
: O
[ O
c7 O
: O
DOLORES O
  O
HINOSTROZA O
] O
Victim O
  O
deceased O
  O
when O
the O
ambulance O
came O
. O
S6 O
: O
She O
  O
is O
the O
second O
woman O
mayor O
killed O
this O
  O
week O
by O
alleged O
commando O
groups O
of O
the O
  O
Maoist O
  O
[ O
c8 O
: O
SHINING O
PATH O
] O
PerpOrg O
. O
c6 O
c8 O
c1 O
c7 O
c2 O
c5 O
+ O
c2 O
‚Äô O
Attention O
Classify O
Update O
c3 O
Ôºö O
Within O
regional O
Affinity O
( O
Strong O
) O
Ôºö O
Within O
regional O
  O
Affinity O
  O
( O
Weak O
) O
Ôºö O
Across O
regional O
  O
Coreference O
From O
  O
Region O
  O
2 O
c4 O
Region O
1 O
Region O
2 O
From O
  O
Region O
  O
1Figure O
2 O
: O
The O
overall O
framework O
of O
Ô¨Åne O
- O
grained O
Ô¨Åltering O
framework O
. O
8 O
candidate O
role O
Ô¨Ållers O
( O
c1‚àíc8 O
) O
with O
sentential O
clues O
and O
speciÔ¨Åc O
role O
types O
are O
extracted O
by O
SEE O
as O
nodes O
. O
3 O
types O
of O
edges O
are O
deÔ¨Åned O
to O
connect O
those O
nodes O
: O
within O
- O
regional O
afÔ¨Ånity O
( O
Strong O
) O
, O
within O
- O
regional O
afÔ¨Ånity O
( O
weak O
) O
, O
across O
- O
regional O
coreference O
. O
Then O
we O
employ O
edge O
- O
enriched O
attention O
mechanism O
to O
update O
the O
representation O
of O
each O
candidate O
for O
classiÔ¨Åcation O
, O
like O
nodec2 O
/ O
primefromc2 O
. O
Ideally O
, O
the O
framework O
will O
Ô¨Ålter O
noisy O
candidates O
c5,c6and O
reconstruct O
the O
original O
two O
event O
regions O
. O
event O
regions O
and O
, O
according O
to O
them O
, O
build O
a O
graph O
to O
link O
those O
candidates O
. O
SpeciÔ¨Åcally O
, O
we O
Ô¨Årst O
take O
each O
candidate O
role O
Ô¨Åller O
as O
the O
node O
in O
the O
graph O
. O
These O
nodes O
can O
easily O
take O
rich O
candidates O
‚Äô O
rich O
features O
as O
initial O
representation O
, O
such O
as O
the O
entity O
embeddings O
and O
the O
local O
sentential O
information O
. O
For O
example O
, O
in O
Figure O
2 O
, O
we O
extract O
8 O
candidate O
role O
Ô¨Ållers O
with O
speciÔ¨Åc O
role O
type O
from O
a O
document O
using O
the O
aforementioned O
SEE O
. O
We O
mark O
them O
as O
c1‚àíc8and O
regard O
them O
as O
the O
nodes O
. O
As O
we O
know O
from O
the O
property O
of O
event O
regions O
, O
the O
correct O
role O
Ô¨Ållers O
tend O
to O
crowd O
within O
the O
same O
or O
adjacent O
sentences O
, O
such O
as O
c1,c2,c3and O
c4 O
in O
Figure O
2 O
. O
Also O
, O
one O
event O
may O
be O
mentioned O
by O
multiple O
event O
regions O
, O
and O
there O
can O
be O
coreferential O
role O
Ô¨Åller O
across O
these O
regions O
, O
like O
c2and O
c7 O
. O
We O
employ O
such O
properties O
of O
event O
regions O
to O
construct O
the O
graphs O
so O
as O
to O
utilize O
regional O
information O
. O
In O
detail O
, O
we O
deÔ¨Åne O
the O
following O
2 O
types O
of O
relations O
( O
3 O
types O
of O
edges O
) O
in O
the O
graphs O
: O
Within O
- O
regional O
AfÔ¨Ånity O
When O
two O
candidates O
appear O
in O
the O
same O
or O
adjacent O
sentences O
, O
they O
have O
a O
within O
- O
regional O
afÔ¨Ånity O
. O
We O
use O
such O
afÔ¨Ånities O
to O
model O
the O
phenomenon O
that O
multiple O
event O
role O
Ô¨Ållers O
tend O
to O
crowd O
in O
an O
event O
region O
. O
When O
one O
candidate O
Ô¨Åller O
in O
the O
region O
has O
high O
conÔ¨Ådence O
to O
be O
a O
positive O
one O
, O
other O
candidates O
can O
share O
this O
conÔ¨Ådence O
and O
vice O
versa O
. O
Furthermore O
, O
we O
distinguish O
the O
same O
sentence O
afÔ¨Ånity O
from O
the O
adjacent O
sentences O
afÔ¨Ånity O
using O
different O
edges O
because O
we O
believe O
such O
afÔ¨Ånity O
is O
stronger O
within O
the O
same O
sentence O
. O
For O
instance O
, O
in O
Figure O
2 O
, O
we O
assign O
c1 O
andc2with O
strong O
within O
- O
regional O
afÔ¨Ånity O
sincethey O
are O
both O
in O
S1 O
, O
and O
use O
a O
single O
solid O
line O
to O
represent O
this O
afÔ¨Ånity O
. O
And O
we O
assign O
c6andc7 O
with O
the O
weak O
within O
- O
regional O
afÔ¨Ånity O
because O
they O
occur O
in O
adjacent O
sentences O
S4and O
S5respectively O
. O
A O
single O
dotted O
line O
is O
used O
to O
illustrate O
it O
. O
The O
weak O
afÔ¨Ånity O
may O
have O
less O
conÔ¨Ådence O
sharing O
and O
help O
Ô¨Ålter O
nosy O
candidate O
c6while O
keeping O
c7 O
. O
Across O
- O
regional O
Coreference O
When O
two O
candidates O
are O
the O
same O
to O
each O
other O
lexically O
and O
also O
recognized O
as O
the O
same O
event O
role O
type O
, O
we O
assume O
that O
they O
have O
a O
coreference O
relationship O
. O
When O
these O
two O
coreferential O
candidates O
are O
not O
in O
the O
same O
or O
adjacent O
sentences O
( O
they O
do O
not O
have O
within O
- O
regional O
afÔ¨Ånity O
) O
, O
we O
assign O
them O
with O
across O
- O
regional O
coreference O
so O
as O
to O
bridge O
different O
regions O
. O
This O
is O
because O
a O
document O
usually O
mentions O
the O
target O
event O
in O
multiple O
event O
regions O
, O
and O
the O
same O
event O
role O
Ô¨Ållers O
may O
repeat O
in O
these O
regions O
. O
We O
connect O
these O
regions O
by O
utilizing O
such O
cross O
- O
region O
coreference O
relationships O
. O
Such O
connections O
will O
help O
exchange O
semantic O
information O
and O
share O
classiÔ¨Åcation O
conÔ¨Ådence O
among O
different O
regions O
. O
Here O
in O
Figure O
2 O
, O
we O
assign O
c2andc7with O
across O
- O
regional O
coreference O
relationship O
and O
use O
a O
double O
solid O
line O
to O
represent O
corresponding O
edge O
in O
the O
graph O
. O
Although O
the O
constructed O
graphs O
do O
not O
precisely O
demonstrate O
the O
original O
event O
regions O
, O
the O
GNNs O
models O
will O
synthesize O
comprehensive O
context O
from O
such O
connections O
to O
enforce O
each O
candidate O
‚Äôs O
representations O
, O
identify O
the O
noises O
, O
and O
reconstruct O
the O
original O
regions O
as O
a O
result.8143.4 O
Selecting O
Role O
Fillers O
via O
Edge O
- O
enriched O
Graph O
Attention O
Networks O
After O
building O
graphs O
from O
the O
documents O
, O
we O
classify O
the O
nodes O
via O
supervised O
learning O
. O
We O
Ô¨Årst O
encode O
the O
nodes O
and O
edges O
into O
vectors O
and O
then O
apply O
the O
attention O
mechanism O
to O
update O
the O
representation O
of O
each O
node O
from O
its O
neighbors O
, O
and O
Ô¨Ånally O
feed O
the O
updated O
representation O
into O
classiÔ¨Åers O
for O
Ô¨Åltering O
. O
Encoding O
Each O
graph O
is O
represented O
by O
its O
nodes O
and O
edges O
, O
as O
G= O
( O
C O
, O
E O
) O
, O
whereCrepresents O
nodes O
andErepresents O
edges O
. O
We O
Ô¨Årst O
initialize O
all O
nodes O
with O
their O
feature O
representations O
and O
get O
C={c1,c2, O
... O
,c O
n},ci‚ààRF O
, O
wherecirepresents O
the O
features O
of O
node O
i O
, O
nis O
the O
number O
of O
nodes O
andFis O
the O
embedding O
size O
for O
each O
node O
. O
Each O
node O
is O
featured O
by O
4 O
types O
of O
embeddings O
ci= O
[ O
wi O
, O
pi O
, O
ti O
, O
si O
] O
, O
wherewiis O
the O
average O
word O
embedding O
of O
each O
candidate O
entity O
, O
piis O
the O
position O
embedding O
of O
the O
candidate O
with O
respect O
to O
the O
sentence O
, O
tiis O
the O
embedding O
of O
role O
type O
, O
and O
si O
is O
the O
sentence O
embedding O
by O
averaging O
all O
words O
in O
the O
sentence O
. O
For O
edges O
, O
the O
plain O
graph O
attention O
mechanism O
does O
not O
encode O
them O
into O
vectors O
. O
Such O
a O
mechanism O
equally O
treating O
the O
edges O
suffers O
from O
losing O
the O
information O
of O
distinguishing O
edges O
. O
A O
popular O
way O
to O
deal O
with O
this O
problem O
is O
to O
use O
different O
weights O
for O
different O
edges O
in O
the O
attention O
operation O
( O
Relational O
GAT O
, O
R O
- O
GAT O
) O
. O
However O
, O
R O
- O
GAT O
does O
not O
have O
edge O
representation O
nor O
controls O
the O
information O
Ô¨Çow O
equally O
for O
the O
same O
type O
edges O
. O
Our O
edge O
- O
enriched O
attention O
model O
characterizes O
the O
edges O
into O
vector O
representations O
, O
which O
can O
especially O
control O
the O
information O
between O
each O
candidate O
node O
pair O
. O
Initially O
, O
we O
regard O
each O
edge O
as O
a O
new O
type O
of O
node O
featuring O
its O
edge O
type O
and O
make O
a O
new O
set O
of O
nodes O
E O
/ O
prime O
. O
For O
example O
in O
Figure O
3 O
, O
we O
use O
the O
new O
node O
e1,2‚ààE O
/ O
primeto O
represents O
the O
original O
within O
- O
regional O
afÔ¨Ånity O
edge O
between O
nodesc1andc2 O
. O
Here O
the O
same O
type O
of O
edges O
will O
share O
the O
same O
initial O
vector O
representation O
. O
c O
1 O
c O
2 O
c O
1 O
c O
2 O
e12 O
Figure O
3 O
: O
Encoding O
of O
EdgesIn O
this O
way O
, O
we O
construct O
a O
new O
graph O
/tildewideG= O
( O
/tildewideC,/tildewideE)in O
which O
all O
the O
new O
edges O
in O
the O
graph O
are O
the O
same O
, O
but O
we O
have O
two O
types O
of O
nodes O
/tildewideC= O
{ O
C O
, O
E O
/ O
prime O
} O
, O
which O
means O
the O
graph O
is O
heterogeneous O
now O
. O
To O
update O
all O
nodes O
in O
the O
same O
attention O
mechanism O
, O
we O
combine O
the O
feature O
spaces O
of O
both O
the O
original O
nodes O
and O
new O
edge O
- O
enriched O
nodes O
. O
In O
this O
way O
, O
any O
new O
node O
within O
the O
new O
graph O
will O
have O
5 O
types O
of O
embedding O
: O
Àúci= O
[ O
wi O
, O
pi O
, O
ti O
, O
si O
, O
ei O
] O
, O
where O
[ O
ei]is O
the O
edge O
type O
representation O
. O
We O
initializeeias O
zero O
vectors O
for O
original O
candidate O
nodes O
and O
the O
other O
4 O
embeddings O
as O
zero O
vectors O
for O
the O
new O
edge O
nodes O
. O
Updating O
Then O
we O
update O
the O
edge O
- O
enriched O
graph O
based O
on O
GAT O
proposed O
by O
( O
Veli O
Àáckovi O
¬¥ O
c O
et O
al O
. O
, O
2017 O
) O
. O
GAT O
is O
in O
essence O
masked O
attention O
operation O
on O
graphs O
. O
For O
each O
layer O
of O
graph O
attention O
, O
it O
updates O
the O
representation O
of O
node O
Àúciby O
computing O
the O
linear O
combinations O
of O
its O
neighbors O
‚Äô O
normalized O
attention O
scores O
and O
their O
corresponding O
transformed O
representations O
: O
Àúc O
/ O
prime O
i O
= O
H O
/bardbl O
h=1œÉÔ£´ O
Ô£≠/summationdisplay O
j‚ààNiŒ±h O
ijWhÀúcjÔ£∂ O
Ô£∏ O
( O
1 O
) O
Here O
we O
concatenate O
( O
signiÔ¨Åed O
by O
/bardbl)Hheads O
of O
the O
attentions O
results O
. O
œÉrepresents O
the O
activation O
functions O
andNirepresents O
the O
neighbor O
nodes O
of O
Àúci O
, O
including O
itself O
. O
Transformation O
Whis O
shared O
for O
all O
nodes O
within O
each O
head O
. O
We O
obtain O
the O
attention O
scoreŒ±h O
ijin O
headhas O
followed O
: O
Œ±h O
ij O
= O
exp O
/ O
parenleftbig O
LeakyReLU O
/ O
parenleftbig O
aT O
/ O
parenleftbig O
WhÀúci O
/ O
bardblWhÀúcj O
/ O
parenrightbig O
/ O
parenrightbig O
/ O
parenrightbig O
/summationtext O
k‚ààN(i)exp O
( O
LeakyReLU O
( O
aT(WhÀúci O
/ O
bardblWhÀúck O
) O
) O
) O
( O
2 O
) O
Hereais O
a O
single O
- O
layer O
feedforward O
neural O
network O
. O
We O
apply O
two O
layers O
of O
the O
GAT O
to O
update O
on O
the O
graphs O
. O
The O
Ô¨Årst O
layer O
will O
exchange O
the O
information O
between O
candidate O
nodes O
and O
edge O
nodes O
, O
which O
will O
characterize O
the O
edge O
representation O
with O
the O
semantic O
context O
. O
Now O
each O
edge O
node O
will O
have O
unique O
vector O
representations O
. O
Then O
in O
the O
second O
layer O
, O
the O
candidate O
nodes O
will O
incorporate O
information O
from O
the O
updated O
edge O
nodes O
, O
indirectly O
blend O
in O
the O
features O
of O
adjacent O
candidate O
nodes O
in O
the O
original O
graph O
G. O
The O
enriched O
edges O
play O
the O
role O
to O
control O
the O
information O
Ô¨Çowing O
between O
neighbor O
candidate O
nodes O
uniquely O
. O
For O
comparison O
, O
the O
R O
- O
GAT O
model O
uses O
different O
weights O
for O
different O
edges O
as O
followed O
, O
where O
Ris O
the O
set O
of O
edge O
types O
. O
Here O
different O
edges O
control815SystemsEvent O
Roles O
in O
MUC-4 O
Dataset O
PerpInd O
PerpOrg O
Target O
Victim O
Weapon O
Average O
( O
Riloff O
, O
1996 O
) O
33/49/40 O
53/33/41 O
54/59/56 O
49/54/51 O
38/44/41 O
45/48/46 O
( O
Patwardhan O
and O
Riloff O
, O
2009 O
) O
51/58/54 O
34/45/38 O
43/72/53 O
55/58/56 O
57/53/55 O
48/57/52 O
( O
Huang O
and O
Riloff O
, O
2011 O
) O
48/57/52 O
46/53/50 O
51/73/60 O
56/60/58 O
53/64/58 O
51/62/56 O
( O
Huang O
and O
Riloff O
, O
2012 O
) O
54/57/56 O
55/49/51 O
55/68/61 O
63/59/61 O
62/64/63 O
58/60/59 O
( O
Boros O
et O
al O
. O
, O
2014 O
) O
53/58/55 O
56/67/ O
61 O
59/63/61 O
56/55/55 O
72/65/68 O
59/61/60 O
( O
Yang O
et O
al O
. O
, O
2018 O
) O
48/60/54 O
52/74/ O
61 O
52/70/59 O
56/62/59 O
70/77/74 O
56/69/61 O
SEE O
35/77/48 O
28/ O
88/42 O
44/ O
80/57 O
38/ O
83/53 O
59/ O
86/70 O
41/83/55 O
GAT O
62/52/57 O
57/53/55 O
60/61/60 O
61/58/59 O
78/78/78 O
64/60/62 O
R O
- O
GAT O
58/62/ O
60 O
57/61/59 O
60/63/62 O
57/67/61 O
71/75/73 O
61/66/63 O
EE O
- O
GAT O
60/59/ O
60 O
58 O
/61/60 O
61/68/64 O
62/65/ O
63 O
75/75/75 O
63/66/ O
65 O
Table O
1 O
: O
Evaluation O
on O
MUC-4 O
test O
set O
, O
P O
/ O
R O
/ O
F1 O
( O
Precision O
/ O
Recall O
/ O
F1 O
- O
Score,% O
) O
. O
Event O
Types O
SystemsEvent O
Roles O
in O
CFEED O
Dataset O
NAME O
NUM O
BEG O
END O
ORG O
Average O
Freeze(Boros O
et O
al O
. O
, O
2014 O
) O
71/76/74 O
56/57/56 O
77/54/63 O
83/ O
80/81 O
70/80/ O
75 O
72/69/70 O
( O
Yang O
et O
al O
. O
, O
2018 O
) O
83/71/76 O
70 O
/49/58 O
75/67/71 O
85/65/74 O
71/67/69 O
77/64/70 O
EE O
- O
GAT O
68/82/75 O
57/ O
63/60 O
71/77/74 O
84/79/ O
81 O
65/82/72 O
69/77/73 O
Pledge(Boros O
et O
al O
. O
, O
2014 O
) O
74/95/83 O
60/46/52 O
68/ O
81/74 O
74/ O
30/42 O
83/ O
92/87 O
72/69/70 O
( O
Yang O
et O
al O
. O
, O
2018 O
) O
84/87/86 O
76/54/63 O
81/72/76 O
85/28/42 O
88/82/85 O
83/64/72 O
EE O
- O
GAT O
77/95/85 O
79/55/65 O
76/78/ O
77 O
83/30/44 O
84/91/ O
88 O
80/70/75 O
OW O
/ O
UW(Boros O
et O
al O
. O
, O
2014 O
) O
49/89/63 O
63/ O
65/64 O
39/ O
79/52 O
62/ O
45/53 O
‚Äî O
54/70/61 O
( O
Yang O
et O
al O
. O
, O
2018 O
) O
77/70/73 O
79/54/64 O
66/68/67 O
74/39/51 O
‚Äî O
74/58/65 O
EE O
- O
GAT O
66/82/ O
73 O
80 O
/60/68 O
73 O
/79/76 O
77 O
/44/56 O
‚Äî O
74/66/70 O
Total(Boros O
et O
al O
. O
, O
2014 O
) O
65/87/74 O
60/56/58 O
61/71/66 O
73/ O
52/61 O
77/86/ O
81 O
66/69/67 O
( O
Yang O
et O
al O
. O
, O
2018 O
) O
81/76/78 O
75 O
/52/61 O
74/69/71 O
81/44/57 O
80/75/77 O
78/62/69 O
EE O
- O
GAT O
70/86/77 O
72/ O
59/65 O
73/78/75 O
81 O
/51/63 O
75/87/81 O
74/71/72 O
Table O
2 O
: O
Evaluation O
on O
the O
CFEED O
test O
set O
, O
P O
/ O
R O
/ O
F1 O
( O
Precision O
/ O
Recall O
/ O
F1 O
- O
Score,% O
) O
. O
the O
information O
exchange O
differently O
. O
However O
, O
this O
mechanism O
is O
not O
as O
effective O
as O
the O
enriched O
edges O
in O
our O
EE O
- O
GAT O
model O
. O
c O
/ O
prime O
i O
= O
H O
/bardbl O
h=1œÉÔ£´ O
Ô£≠/summationdisplay O
r‚ààR O
/ O
summationdisplay O
j‚ààNiŒ±h O
ijWr O
, O
hcjÔ£∂ O
Ô£∏ O
( O
3 O
) O
ClassiÔ¨Åcation O
After O
updating O
the O
candidate O
nodes O
via O
the O
two O
layers O
multi O
- O
head O
attention O
mechanism O
, O
we O
need O
to O
classify O
each O
candidate O
node O
as O
either O
positive O
or O
negative O
. O
Now O
we O
average O
the O
vectors O
of O
multiple O
heads O
to O
get O
the O
Ô¨Ånal O
representation O
of O
each O
node O
and O
then O
project O
the O
results O
into O
a O
softmax O
classiÔ¨Åcation O
layer O
. O
As O
a O
result O
, O
we O
will O
get O
the O
probabilities O
of O
the O
node O
as O
either O
positive O
or O
negative O
. O
This O
process O
is O
illustrated O
in O
equation O
( O
4 O
) O
, O
where O
yi‚àà{0,1}is O
the O
label O
of O
node O
i O
, O
Œ∏represents O
all O
the O
parameters O
, O
pis O
the O
probability O
of O
yiequals O
to O
0 O
or O
1 O
. O
p(yi|/tildewideG;Œ∏ O
) O
= O
softmaxÔ£´ O
Ô£≠1 O
H O
/ O
summationdisplay O
h=1 O
/ O
summationdisplay O
j‚ààN(i)Œ±h O
ijWhÀúc O
/ O
prime O
jÔ£∂ O
Ô£∏ O
( O
4 O
) O
We O
train O
our O
model O
to O
minimize O
the O
crossentropy O
loss O
in O
the O
data O
and O
use O
the O
Adam O
optimization O
method O
proposed O
by O
Kingma O
and O
Ba O
( O
2014 O
) O
toupdate O
the O
parameters O
Œ∏ O
. O
The O
loss O
function O
is O
as O
followed O
in O
equation O
( O
4 O
) O
where O
ÀÜyi O
= O
p(yi= O
1|G;Œ∏ O
) O
is O
the O
predicted O
probability O
of O
node O
ias O
positive O
, O
N O
is O
the O
number O
of O
samples O
. O
L(Œ∏ O
) O
= O
‚àíN O
/ O
summationdisplay O
i=1(yilog O
ÀÜyi+ O
( O
1‚àíyi O
) O
log O
( O
1‚àíÀÜyi O
) O
) O
( O
5 O
) O
4 O
Experiments O
4.1 O
MUC-4 O
MUC-4 O
dataset O
was O
released O
by O
Message O
Understanding O
Conferences O
in O
1992 O
. O
It O
is O
about O
terrorism O
events O
and O
consists O
of O
1700 O
documents O
as O
in O
Table O
4 O
. O
We O
follow O
the O
same O
evaluation O
paradigm O
as O
previous O
work O
and O
evaluate O
the O
5 O
kinds O
of O
event O
roles O
: O
PerpInd O
, O
( O
individual O
perpetrator O
) O
, O
PerpOrg O
( O
organizational O
perpetrator O
) O
, O
Target O
( O
physical O
target O
) O
, O
Victim O
( O
human O
target O
name O
or O
description O
) O
Datasets O
Event O
Types O
Train O
Dev O
Test O
Total O
MUC-4 O
Terrorism O
1300 O
200 O
200 O
1700 O
CFEEDFreeze O
589 O
150 O
300 O
1039 O
Pledge O
3602 O
300 O
300 O
4202 O
OW O
/ O
UW O
1303 O
300 O
300 O
1903 O
Table O
3 O
: O
Statistics O
of O
MUC-4 O
and O
CFEED816andWeapon O
( O
instrument O
i O
d O
or O
type O
) O
. O
We O
use O
head O
noun O
matching O
( O
e.g. O
HINOSTROZA O
is O
considered O
to O
match O
DOLORES O
HINOSTROZA O
) O
as O
before O
too O
. O
Baselines O
For O
comparison O
, O
we O
choose O
the O
following O
6 O
previous O
state O
- O
of O
- O
the O
- O
art O
systems O
as O
the O
baselines O
for O
MUC-4 O
. O
Riloff O
( O
1996 O
) O
automatically O
produced O
many O
domain O
- O
speciÔ¨Åc O
extraction O
patterns O
for O
role O
Ô¨Ållers O
extraction O
. O
Patwardhan O
and O
Riloff O
( O
2009 O
) O
incorporated O
both O
phrasal O
and O
sentential O
evidence O
to O
label O
role O
Ô¨Ållers O
. O
They O
Ô¨Årst O
used O
a O
sentential O
event O
recognizer O
to O
select O
sentences O
and O
then O
applied O
a O
plausible O
roleÔ¨Åller O
recognizer O
to O
extract O
role O
Ô¨Ållers O
. O
Huang O
and O
Riloff O
( O
2011 O
) O
designed O
TIER O
system O
to O
better O
extract O
role O
Ô¨Ållers O
from O
Secondary O
Context O
, O
regardless O
of O
whether O
a O
relevant O
event O
is O
mentioned O
. O
Huang O
and O
Riloff O
( O
2012 O
) O
deÔ¨Åned O
many O
features O
and O
used O
SVMs O
to O
extract O
local O
candidate O
role O
Ô¨Ållers O
and O
CRF O
to O
choose O
sentences O
for O
Ô¨Ånal O
results O
. O
Boros O
et O
al O
. O
( O
2014 O
) O
utilized O
domain O
- O
relevant O
word O
representations O
as O
the O
features O
of O
noun O
phrases O
and O
then O
applied O
randomized O
decision O
trees O
to O
identify O
role O
Ô¨Ållers O
. O
Here O
we O
adopt O
the O
same O
idea O
but O
use O
a O
different O
classiÔ¨Åer O
MLP O
. O
Besides O
, O
we O
use O
the O
same O
node O
features O
as O
in O
EE O
- O
GAT O
instead O
of O
just O
domain O
word O
vectors O
for O
comparison O
with O
our O
model O
. O
Yang O
et O
al O
. O
( O
2018 O
) O
proposed O
a O
document O
- O
level O
EE O
system O
following O
three O
steps O
. O
It O
Ô¨Årst O
extracted O
candidate O
role O
Ô¨Ållers O
from O
each O
sentence O
via O
sequence O
tagging O
model O
; O
then O
it O
applied O
Convolutional O
Neural O
Networks O
to O
detect O
the O
primary O
sentence O
that O
mentions O
the O
target O
event O
; O
Ô¨Ånally O
, O
it O
aggregated O
the O
candidate O
role O
Ô¨Ållers O
from O
the O
primary O
sentence O
and O
supplements O
the O
missing O
even O
roles O
from O
adjacent O
sentences O
. O
Experiments O
on O
MUC-4 O
For O
node O
representations O
, O
we O
randomly O
initialize O
pi O
, O
tias O
50 O
- O
dim O
vectors O
and O
eias O
200 O
- O
dim O
, O
and O
use O
the O
100 O
- O
dim O
Glove3word O
embedding O
for O
wi O
, O
si O
. O
Each O
layer O
of O
the O
attention O
mechanism O
has O
8 O
heads O
and O
the O
learning O
rate O
is O
set O
as O
5e-4 O
. O
We O
train O
on O
MUC-4 O
training O
data O
for O
100 O
epochs O
and O
choose O
the O
best O
model O
performed O
on O
the O
development O
set O
for O
testing O
. O
We O
report O
Precision O
/ O
Recall O
/ O
F1 O
- O
score O
of O
the O
test O
results O
for O
each O
event O
role O
individually O
and O
the O
macro O
- O
average O
over O
all O
Ô¨Åve O
roles O
. O
The O
test O
results O
3https://nlp.stanford.edu/projects/ O
glove O
/ O
are O
shown O
in O
Table O
2 O
. O
From O
the O
table O
, O
we O
have O
the O
following O
observations O
: O
( O
1 O
) O
In O
general O
, O
our O
EE O
- O
GAT O
framework O
achieves O
the O
best O
performance O
compared O
with O
previous O
state O
- O
of O
- O
the O
- O
art O
methods O
. O
It O
signiÔ¨Åcantly O
improves O
the O
previous O
best O
method O
by O
4.0 O
% O
( O
65 O
% O
vs. O
61 O
% O
) O
on O
average O
F1 O
score O
and O
most O
of O
the O
improvement O
is O
contributed O
by O
the O
better O
precision O
7.0 O
% O
( O
63 O
% O
vs. O
56 O
% O
) O
as O
opposed O
to O
Yang O
et O
al O
. O
( O
2018 O
) O
. O
( O
2 O
) O
The O
SEE O
results O
have O
high O
recall O
but O
very O
low O
precision O
because O
of O
the O
noisy O
candidates O
. O
Plain O
GAT O
Ô¨Ålters O
some O
noises O
and O
improves O
precision O
a O
lot O
. O
R O
- O
GAT O
and O
EE O
- O
GAT O
balance O
the O
trade O
- O
off O
between O
precision O
and O
recall O
and O
achieve O
a O
better O
overall O
F1 O
score O
. O
( O
3 O
) O
In O
detail O
, O
our O
method O
achieves O
the O
best O
performance O
nearly O
on O
most O
of O
the O
event O
roles O
. O
We O
signiÔ¨Åcantly O
improve O
the O
F1 O
score O
of O
4.0 O
% O
( O
60 O
% O
vs. O
56 O
% O
) O
in O
PerInd O
and O
3.0 O
% O
inTarget O
( O
64 O
% O
vs. O
61 O
% O
) O
compared O
to O
previous O
best O
in O
Huang O
and O
Riloff O
( O
2012 O
) O
. O
4.2 O
CFEED O
CFEED O
C O
hinese O
Financial O
Event O
Extraction O
Dataset O
is O
a O
larger O
dataset O
in O
Chinese O
about O
the O
major O
events O
in O
the O
announcements O
of O
listed O
companies O
. O
We O
construct O
it O
by O
the O
same O
method O
proposed O
by O
Yang O
et O
al O
. O
( O
2018 O
) O
. O
We O
crawled O
the O
public O
announcements O
from O
sohu.com4and O
the O
event O
templates O
from O
eastmoney.com5 O
, O
and O
then O
align O
them O
. O
We O
assume O
that O
if O
the O
key O
role O
Ô¨Ållers O
in O
a O
template O
appear O
in O
an O
announcement O
, O
the O
announcement O
is O
describing O
the O
event O
in O
the O
template O
. O
As O
in O
Table O
3 O
, O
it O
consists O
of O
a O
total O
of O
7144 O
documents O
and O
3 O
types O
of O
Ô¨Ånancial O
events O
: O
freezing O
shares O
( O
freeze O
) O
, O
pledging O
shares O
( O
pledge O
) O
and O
overweighting O
and O
underweighting O
shares O
( O
OW&UW O
) O
. O
We O
deÔ¨Åned O
5 O
types O
of O
event O
role O
in O
these O
Ô¨Ånancial O
events O
: O
shareholder O
‚Äôs O
name O
( O
NAME O
) O
, O
organization O
( O
ORG O
) O
, O
number O
of O
shares O
( O
NUM O
) O
, O
event O
starting O
date O
( O
BEG O
) O
, O
event O
ending O
date O
( O
END O
) O
. O
Note O
that O
the O
ORG O
is O
not O
included O
in O
OW&UW O
event O
. O
Baselines O
For O
comparison O
, O
we O
select O
the O
two O
methods O
mentioned O
above O
as O
the O
baselines O
for O
CFEED O
: O
Boros O
et O
al O
. O
( O
2014 O
) O
andYang O
et O
al O
. O
( O
2018 O
) O
. O
Experiments O
on O
CFEED O
We O
use O
the O
same O
settings O
as O
in O
MUC-4 O
to O
evaluate O
on O
the O
CFEED O
except O
that O
we O
use O
the O
character O
- O
level O
100 O
- O
dim O
embeddings O
trained O
on O
Chinese O
wiki O
corpus6 O
. O
We O
sep4http://q.stock.sohu.com/index.shtml O
5http://choice.eastmoney.com/ O
6https://github.com/Embedding/ O
Chinese O
- O
Word O
- O
Vectors817StatisticsMUC-4 O
CFEED O
Gold O
SEE O
EE O
- O
GAT O
Gold O
SEE O
EE O
- O
GAT O
Avg O
# O
Fillers O
/Doc O
8.21 O
11.17 O
6.30 O
11.72 O
29.95 O
10.43 O
Avg O
# O
Regions O
/Doc O
1.76 O
2.86 O
1.57 O
2.53 O
2.21 O
2.58 O
Avg O
# O
Fillers O
/Region O
5.32 O
5.54 O
4.57 O
5.88 O
16.94 O
5.51 O
Eval O
for O
Regions O
‚Äî O
21/87/34 O
65/70/68 O
‚Äî O
16/96/27 O
68/77/72 O
Table O
4 O
: O
Distributions O
of O
role O
Ô¨Ållers O
in O
the O
golden O
data O
and O
results O
of O
SEE O
and O
EE O
- O
GAT O
on O
the O
test O
set O
of O
MUC-4 O
and O
CFEED O
. O
The O
last O
row O
is O
the O
evaluation O
( O
Precision O
/ O
Recall O
/ O
F1 O
- O
Score,% O
) O
of O
the O
regions O
sentence O
by O
sentence O
. O
The O
statistics O
demonstrate O
the O
salient O
Event O
Regions O
in O
golden O
data O
and O
its O
reconstruction O
by O
EE O
- O
GAT O
. O
Settings O
MUC-4CFEED O
Freeze O
Pledge O
OW&UW O
Total O
( O
Yang O
et O
al O
. O
, O
2018 O
) O
56/69/61 O
77/64/70 O
83/64/72 O
74/58/65 O
78/62/69 O
EE O
- O
GAT O
w/ O
1st O
Rel O
63/59/61 O
71/72/71 O
77/68/72 O
64/68/66 O
71/69/70 O
EE O
- O
GAT O
w/ O
1st O
& O
2nd O
Rels O
62/64/63 O
66/77/71 O
76/ O
71/73 O
64/ O
70/67 O
69/73/71 O
EE O
- O
GAT O
63/66/65 O
69/77/73 O
80/70/ O
75 O
74 O
/66/70 O
74/71/ O
72 O
Table O
5 O
: O
Effectiveness O
of O
the O
Regional O
Relations O
in O
EE O
- O
GAT O
( O
Average O
P O
/ O
R O
/ O
F1 O
, O
Precision O
/ O
Recall O
/ O
F1 O
- O
Score,% O
) O
. O
1st O
Relmeans O
strong O
within O
- O
regional O
afÔ¨Ånity O
and O
2nd O
Rel O
means O
weak O
within O
- O
regional O
afÔ¨Ånity O
. O
arately O
evaluate O
the O
3 O
types O
of O
events O
and O
the O
results O
are O
in O
Table O
3 O
. O
We O
can O
observe O
that O
our O
EE O
- O
GAT O
can O
achieve O
the O
best O
performance O
on O
all O
the O
3 O
types O
of O
events O
when O
compared O
with O
the O
baselines O
. O
The O
results O
verify O
the O
robustness O
of O
our O
method O
in O
Chinese O
corpus O
. O
Besides O
, O
compared O
with O
the O
method O
in O
Yang O
et O
al O
. O
( O
2018 O
) O
, O
the O
major O
improvement O
comes O
from O
recall O
rather O
than O
precision O
as O
on O
MUC-4 O
. O
This O
is O
because O
the O
Ô¨Ånancial O
announcement O
documents O
in O
CFEED O
usually O
have O
one O
main O
sentence O
describing O
the O
target O
event O
, O
so O
Yang O
‚Äôs O
method O
can O
achieve O
high O
precision O
by O
detecting O
the O
primary O
event O
mention O
. O
However O
, O
MUC-4 O
dataset O
does O
not O
have O
such O
characteristics O
. O
4.3 O
Reconstructing O
Event O
Regions O
As O
in O
Table O
4 O
about O
event O
regions O
, O
test O
if O
a O
sentence O
in O
the O
new O
regions O
appears O
in O
the O
golden O
regions O
and O
get O
the O
evaluation O
Precision O
, O
Recall O
, O
andF1scores O
. O
We O
can O
observe O
that O
in O
both O
of O
the O
datasets O
: O
( O
1 O
) O
EE O
- O
GAT O
successfully O
reconstructs O
70 O
% O
of O
the O
event O
regions O
during O
the O
evaluation O
, O
which O
improves O
about O
40 O
% O
from O
the O
SEE O
results O
. O
The O
detection O
of O
the O
event O
regions O
contributes O
to O
most O
of O
the O
Ô¨Åltering O
process O
. O
( O
2 O
) O
SEE O
extracted O
too O
many O
noisy O
role O
Ô¨Ållers O
compared O
to O
the O
golden O
standard O
. O
EE O
- O
GAT O
Ô¨Ålters O
many O
noises O
and O
the O
counts O
of O
remaining O
Ô¨Ållers O
are O
similar O
to O
the O
golden O
standard O
. O
( O
3 O
) O
The O
distribution O
of O
role O
Ô¨Ållers O
and O
event O
regions O
are O
more O
close O
to O
the O
golden O
standard O
after O
EE O
- O
GAT O
Ô¨Åltering O
. O
In O
detail O
, O
on O
the O
gold O
test O
sets O
, O
there O
are O
about O
1.76 O
regions O
in O
a O
document O
and O
5.32 O
Ô¨Ållers O
in O
each O
region O
on O
MUC-4 O
, O
and O
2.53 O
regions O
and O
5.88 O
Ô¨Ållers O
per O
region O
on O
CFEED O
. O
However O
, O
the O
eventregion O
distribution O
diverges O
after O
SEE O
because O
of O
the O
noisy O
candidates O
, O
and O
we O
have O
about O
2.86 O
regions O
in O
a O
document O
and O
5.54 O
Ô¨Ållers O
in O
each O
region O
on O
MUC-4 O
, O
and O
2.21 O
regions O
and O
16.94 O
Ô¨Ållers O
per O
region O
on O
CFEED O
. O
Then O
these O
statistics O
recover O
back O
to O
normal O
after O
the O
Ô¨Åltering O
of O
EE O
- O
GAT O
, O
and O
there O
are O
about O
1.57 O
regions O
in O
a O
document O
and O
4.57 O
Ô¨Ållers O
in O
each O
region O
on O
MUC-4 O
, O
and O
2.58 O
regions O
and O
5.51 O
Ô¨Ållers O
per O
region O
on O
CFEED O
. O
4.4 O
Effectiveness O
of O
Regional O
Relations O
We O
set O
the O
following O
control O
experiments O
to O
demonstrate O
the O
effectiveness O
of O
the O
regional O
relations O
in O
Ô¨Åltering O
the O
noise O
. O
We O
add O
the O
three O
types O
of O
edges O
one O
by O
one O
and O
test O
the O
performance O
of O
EE O
- O
GAT O
. O
As O
in O
Table O
5 O
, O
we O
can O
observe O
that O
the O
overall O
performance O
on O
all O
the O
datasets O
improves O
when O
more O
types O
of O
relations O
are O
used O
. O
( O
1 O
) O
Particularly O
, O
even O
the O
utilization O
of O
strong O
within O
- O
regional O
afÔ¨Ånity O
( O
1st O
Rel O
) O
only O
in O
EE O
- O
GAT O
achieves O
slightly O
better O
performance O
compared O
to O
the O
previous O
state O
- O
of O
- O
theart O
( O
Yang O
et O
al O
. O
, O
2018 O
) O
. O
( O
2 O
) O
Adding O
the O
weak O
withinregional O
afÔ¨Ånity O
( O
2nd O
Rel O
) O
further O
improves O
the O
overall O
performance O
, O
especially O
the O
average O
4.5pp O
improvement O
in O
recall O
score O
. O
( O
3 O
) O
And O
the O
complete O
EE O
- O
GAT O
model O
connecting O
the O
multiple O
event O
regions O
achieves O
even O
better O
overall O
performance O
. O
These O
results O
demonstrate O
that O
the O
event O
region O
relations O
can O
capture O
the O
global O
contextual O
information O
and O
help O
to O
Ô¨Ålter O
the O
noisy O
candidates O
. O
5 O
Conclusion O
We O
propose O
a O
Ô¨Åne O
- O
grained O
Ô¨Åltering O
framework O
to O
address O
the O
aggregating O
problem O
in O
document O
- O
level818event O
extraction O
by O
reconstructing O
event O
regions O
. O
Our O
method O
can O
Ô¨Ålter O
those O
noise O
both O
in O
irrelevant O
sentences O
and O
in O
the O
event O
regions O
and O
achieve O
stateof O
- O
the O
- O
art O
performance O
on O
both O
the O
MUC-4 O
and O
CFEED O
datasets O
. O
Future O
work O
may O
consider O
using O
an O
end2end O
model O
to O
avoid O
error O
propagation O
from O
SEE O
. O
Acknowledgments O
This O
work O
is O
supported O
by O
the O
Natural O
Key O
RD O
Program O
of O
China O
( O
No.2018YFB1005100 O
) O
, O
the O
National O
Natural O
Science O
Foundation O
of O
China O
( O
No.61922085 O
, O
No O
. O
U1936207 O
, O
No.61806201 O
) O
and O
the O
Key O
Research O
Program O
of O
the O
Chinese O
Academy O
of O
Sciences O
( O
Grant O
NO O
. O
ZDBS O
- O
SSW O
- O
JSC006 O
) O
. O
This O
work O
is O
also O
supported O
by O
CCF O
- O
Tencent O
Open O
Research O
Fund O
, O
Beijing O
Academy O
of O
ArtiÔ¨Åcial O
Intelligence O
( O
BAAI2019QN0301 O
) O
and O
independent O
research O
project O
of O
National O
Laboratory O
of O
Pattern O
Recognition O
. O
Abstract O
We O
propose O
a O
newly O
annotated O
dataset O
for O
information O
extraction O
on O
recipes O
. O
Unlike O
previous O
approaches O
to O
machine O
comprehension O
of O
procedural O
texts O
, O
we O
avoid O
a O
priori O
pre O
- O
deÔ¨Åning O
domain O
- O
speciÔ¨Åc O
predicates O
to O
recognize O
( O
e.g. O
, O
the O
primitive O
instructions O
in O
MILK O
) O
and O
focus O
on O
basic O
understanding O
of O
the O
expressed O
semantics O
rather O
than O
directly O
reduce O
them O
to O
a O
simpliÔ¨Åed O
state O
representation O
( O
e.g. O
, O
ProPara O
) O
. O
We O
thus O
frame O
the O
semantic O
comprehension O
of O
procedural O
text O
such O
as O
recipes O
, O
as O
fairly O
generic O
NLP O
subtasks O
, O
covering O
( O
i O
) O
entity O
recognition O
( O
ingredients O
, O
tools O
and O
actions O
) O
, O
( O
ii O
) O
relation O
extraction O
( O
what O
ingredients O
and O
tools O
are O
involved O
in O
the O
actions O
) O
, O
and O
( O
iii O
) O
zero O
anaphora O
resolution O
( O
link O
actions O
to O
implicit O
arguments O
, O
e.g. O
, O
results O
from O
previous O
recipe O
steps O
) O
. O
Further O
, O
our O
Recipe O
Instruction O
Semantic O
Corpus O
( O
RISeC O
) O
dataset O
includes O
textual O
descriptions O
for O
the O
zero O
anaphora O
, O
to O
facilitate O
language O
generation O
thereof O
. O
Besides O
the O
dataset O
itself O
, O
we O
contribute O
a O
pipeline O
neural O
architecture O
that O
addresses O
entity O
and O
relation O
extraction O
as O
well O
as O
identiÔ¨Åcation O
of O
zero O
anaphora O
. O
These O
basic O
building O
blocks O
can O
facilitate O
more O
advanced O
downstream O
applications O
( O
e.g. O
, O
question O
answering O
, O
conversational O
agents O
) O
. O
1 O
Introduction O
Recently O
, O
several O
efforts O
have O
aimed O
at O
understanding O
recipe O
instructions O
( O
see O
Section O
2 O
) O
. O
We O
consider O
such O
recipes O
as O
prototypical O
for O
procedural O
texts O
, O
for O
which O
processing O
is O
complex O
due O
to O
the O
need O
to O
( O
i O
) O
understand O
the O
ordering O
of O
steps O
( O
not O
unlike O
, O
e.g. O
, O
event O
ordering O
in O
news O
) O
, O
( O
ii O
) O
solve O
frequent O
ellipsis O
( O
i.e. O
, O
zero O
anaphora O
) O
and O
coreference O
resolution O
, O
and O
( O
iii O
) O
track O
the O
state O
changes O
they O
involve O
( O
e.g. O
, O
ingredients O
processed O
/ O
combined O
to O
new O
entities O
) O
. O
Especially O
the O
latter O
distinguishes O
procedural O
text O
processingfrom O
more O
traditional O
information O
extraction O
( O
e.g. O
, O
from O
news O
) O
. O
Most O
existing O
works O
on O
recipes O
focus O
on O
recognizing O
pre O
- O
deÔ¨Åned O
predicates O
, O
typically O
in O
the O
form O
of O
a O
limited O
set O
of O
instruction O
types O
( O
e.g. O
, O
to O
convert O
the O
recipe O
to O
robot O
instructions O
) O
with O
predeÔ¨Åned O
argument O
slots O
to O
Ô¨Åll O
. O
Further O
, O
they O
often O
rely O
on O
an O
available O
starting O
list O
of O
ingredients O
( O
which O
may O
not O
be O
available O
in O
other O
procedural O
text O
) O
. O
Hence O
, O
current O
approaches O
towards O
recipe O
understanding O
make O
assumptions O
that O
are O
rather O
domain O
speciÔ¨Åc O
. O
In O
contrast O
, O
we O
aim O
for O
a O
more O
basic O
and O
generic O
structured O
representation O
of O
the O
procedural O
text O
, O
limiting O
domain O
- O
speciÔ¨Åc O
knowledge O
and O
building O
on O
more O
general O
semantic O
concepts O
. O
In O
particular O
, O
we O
build O
on O
semantic O
concepts O
as O
deÔ¨Åned O
in O
PropBank O
( O
Kingsbury O
and O
Palmer O
, O
2002 O
) O
, O
which O
are O
not O
domain O
- O
speciÔ¨Åc O
. O
Note O
that O
our O
proposed O
form O
of O
structured O
representations O
not O
necessarily O
allows O
directly O
solving O
informational O
queries O
that O
require O
explicit O
reasoning O
and/or O
state O
tracking O
( O
e.g. O
, O
‚Äú O
Where O
are O
the O
tomatoes O
after O
step O
5 O
? O
‚Äù O
) O
. O
We O
however O
pose O
that O
properly O
detecting O
the O
various O
entities O
( O
e.g. O
, O
ingredients O
and O
their O
derivations O
) O
and O
the O
actions O
that O
are O
executed O
on O
them O
( O
as O
described O
by O
verbs O
) O
, O
with O
the O
appropriate O
coreference O
and O
zero O
anaphora O
resolution O
, O
would O
enable O
constructing O
a O
graph O
that O
facilitates O
such O
tracking O
. O
Thus O
, O
while O
our O
proposed O
representation O
based O
on O
the O
idea O
of O
joint O
entity O
and O
relation O
extraction O
( O
Bekoulis O
et O
al O
. O
, O
2018 O
) O
, O
provides O
useful O
input O
for O
it O
, O
such O
explicit O
state O
tracking O
and O
representation O
( O
e.g. O
, O
as O
in O
ProPara O
, O
Dalvi O
et O
al O
. O
, O
2018 O
) O
is O
left O
out O
of O
scope O
here O
. O
In O
summary O
, O
this O
paper O
reports O
on O
our O
work O
- O
inprogress O
and O
makes O
two O
main O
contributions O
. O
First O
, O
we O
present O
our O
newly O
annotated O
Recipe O
Instruction O
Semantic O
Corpus O
( O
RISeC O
) O
dataset O
( O
Section O
3 O
) O
, O
following O
the O
frame O
- O
semantic O
representation O
of O
PropBank O
( O
Kingsbury O
and O
Palmer O
, O
2002 O
) O
. O
Since821PropBank O
is O
domain O
- O
agnostic O
, O
the O
approach O
should O
be O
largely O
generalizable1to O
other O
procedural O
text O
settings O
. O
Second O
, O
we O
introduce O
a O
baseline O
framework O
( O
Section O
4 O
) O
to O
solve O
( O
i O
) O
entity O
recognition O
( O
ingredients O
, O
tools O
and O
actions O
) O
, O
( O
ii O
) O
relation O
extraction O
( O
ingredients O
and O
tools O
linked O
to O
the O
actions O
) O
, O
( O
iii O
) O
zero O
anaphora O
identiÔ¨Åcation O
. O
Experimental O
evaluation O
thereof O
on O
RISeC O
is O
provided O
( O
Section O
5 O
) O
. O
2 O
Related O
work O
From O
the O
perspective O
of O
structured O
representation O
, O
Tasse O
and O
Smith O
( O
2008 O
) O
deÔ¨Åne O
the O
Minimal O
Instruction O
Language O
for O
the O
Kitchen O
( O
MILK O
) O
, O
which O
is O
based O
on O
Ô¨Årst O
- O
order O
logic O
to O
describe O
the O
evolution O
of O
ingredients O
throughout O
a O
recipe O
, O
and O
use O
it O
for O
annotation O
in O
the O
CURD O
dataset O
. O
Building O
on O
this O
effort O
, O
Jermsurawong O
and O
Habash O
( O
2015 O
) O
extend O
CURD O
toward O
ingredientinstruction O
dependency O
tree O
parsing O
in O
SIMMR O
: O
they O
present O
an O
ingredient O
- O
instruction O
dependency O
tree O
representation O
of O
the O
recipe O
, O
but O
do O
not O
model O
instruction O
semantics O
. O
This O
contrasts O
with O
Maeta O
et O
al O
. O
( O
2015 O
) O
, O
who O
propose O
a O
pipeline O
framework O
for O
information O
extraction O
on O
Japanese O
recipes O
from O
the O
the O
recipe O
Ô¨Çow O
graph O
( O
r O
- O
FG O
) O
dataset O
( O
Mori O
et O
al O
. O
, O
2014 O
) O
. O
Maeta O
et O
al O
. O
use O
word O
segmentation O
, O
named O
entity O
recognition O
and O
syntactic O
analysis O
to O
extract O
predicate O
- O
argument O
structures O
and O
build O
a O
recipe O
Ô¨Çow O
graph O
that O
is O
conceptually O
similar O
to O
a O
SIMMR O
tree O
. O
Their O
work O
is O
conceptually O
closest O
to O
ours O
, O
in O
that O
they O
propose O
a O
chain O
of O
NLP O
subtasks O
( O
but O
we O
do O
not O
need O
word O
boundary O
identiÔ¨Åcation O
in O
our O
English O
corpus O
) O
. O
Yet O
, O
we O
build O
on O
a O
more O
elaborate O
and O
generic O
semantic O
relation O
scheme O
, O
PropBank O
( O
Kingsbury O
and O
Palmer O
, O
2002 O
) O
. O
Further O
, O
methodologically O
we O
adopt O
neural O
network O
models O
as O
opposed O
to O
their O
logistic O
regression O
for O
NER O
and O
a O
maximum O
spanning O
tree O
( O
MST O
) O
parser O
for O
the O
relations O
( O
i.e. O
, O
graph O
arcs O
) O
. O
Tracking O
state O
changes O
is O
another O
key O
to O
understanding O
recipe O
language O
. O
Bosselut O
et O
al O
. O
( O
2018 O
) O
predict O
the O
dynamics O
of O
action O
and O
entity O
attributes O
in O
recipes O
by O
employing O
a O
recurrent O
memory O
network O
. O
Their O
work O
includes O
sentence O
generation O
, O
but O
does O
not O
address O
the O
zero O
anaphora O
problem O
( O
see O
further O
) O
directly O
. O
Besides O
recipes O
, O
other O
works O
focus O
on O
different O
procedural O
tasks O
. O
The O
ProPara2project O
aims O
at O
1While O
some O
of O
our O
entity O
types O
are O
speciÔ¨Åc O
to O
the O
cooking O
domain O
( O
e.g. O
, O
‚Äú O
food O
‚Äù O
, O
‚Äú O
temperature O
‚Äù O
) O
, O
the O
relations O
that O
link O
action O
verbs O
to O
them O
are O
not O
( O
cf O
. O
PropBank O
) O
. O
2http://data.allenai.org/propara O
Preheat O
oven O
to O
350 O
degrees O
F.In O
a O
casserole O
, O
combine O
soup O
mix O
, O
artichoke O
hearts O
, O
cheese O
and O
crab O
meat O
. O
Bake O
[ O
the O
crab O
mixture O
] O
for O
30 O
minutes O
; O
then O
serve O
[ O
the O
baked O
crab O
] O
immediately O
. O
ACTION O
TOOL O
TEMPERATUREArg_PPTArgM_MNR O
TOOL O
ACTION O
FOOD O
FOOD O
FOOD O
FOODArg_PPT O
ArgM_LOCArg_PPTArg_PPTArg_PPT O
ACTION O
DUR O
ACTIONArgM_TMP123 O
ZAV O
ZAVFigure O
1 O
: O
An O
annotated O
recipe O
. O
The O
fragments O
between O
brackets O
are O
manually O
added O
anaphora O
descriptions O
. O
comprehending O
scientiÔ¨Åc O
processes O
and O
tracking O
the O
status O
of O
entities O
in O
them O
: O
Dalvi O
et O
al O
. O
( O
2018 O
) O
focus O
on O
tracking O
entity O
locations O
( O
as O
well O
as O
their O
creation O
/ O
destruction O
) O
using O
a O
speciÔ¨Åc O
matrix O
state O
representation O
( O
with O
a O
row O
per O
step O
, O
a O
column O
per O
entity O
) O
. O
The O
proposed O
models O
however O
do O
not O
incorporate O
entity O
recognition O
and O
are O
speciÔ¨Åcally O
Ô¨Ålling O
the O
chosen O
state O
representation O
. O
In O
our O
work O
, O
we O
rather O
stick O
to O
a O
more O
‚Äú O
basic O
‚Äù O
understanding O
, O
which O
is O
broader O
in O
scope O
than O
location O
tracking O
. O
In O
terms O
of O
datasets O
beyond O
the O
recipe O
domain O
, O
the O
work O
of O
Mysore O
et O
al O
. O
( O
2019 O
) O
is O
noteworthy O
: O
it O
focuses O
on O
material O
synthesis O
and O
annotates O
domain O
- O
speciÔ¨Åc O
entities O
( O
materials O
, O
operations O
, O
conditions O
, O
etc O
. O
) O
and O
relations O
. O
The O
latter O
in O
our O
case O
are O
rather O
domain O
- O
agnostic O
( O
using O
PropBank O
) O
. O
3 O
The O
RISeC O
Dataset O
The O
following O
paragraphs O
describe O
our O
dataset O
and O
the O
annotations O
underlying O
the O
presented O
extraction O
task3 O
. O
3.1 O
Dataset O
Collection O
Recipes O
in O
our O
RISeC O
dataset O
are O
those O
from O
the O
SIMMR O
dataset.4Unlike O
SIMMR O
, O
we O
only O
use O
the O
instruction O
text O
of O
each O
recipe O
, O
and O
rather O
detect O
ingredients O
( O
as O
well O
as O
derived O
entities O
) O
from O
the O
text O
itself O
. O
We O
annotate O
the O
dataset O
using O
BRAT O
( O
Stenetorp O
et O
al O
. O
, O
2012 O
) O
, O
which O
eventually O
creates O
a O
directed O
acyclic O
graph O
where O
( O
i O
) O
vertices O
areentities O
( O
text O
spans O
) O
such O
as O
ingredients O
, O
tools O
, O
actions O
, O
intermediate O
products O
, O
and O
( O
ii O
) O
edges O
denote O
relations O
between O
entity O
spans O
. O
An O
example O
of O
our O
annotation O
is O
given O
in O
Fig O
. O
1 O
. O
Three O
expert O
annotators O
are O
involved O
in O
this O
task O
, O
who O
were O
are O
in O
close O
communication O
during O
the O
entire O
annotation O
process O
to O
maximize O
annotation O
consistency O
. O
3The O
annotated O
data O
is O
available O
for O
research O
at O
https:// O
github.com/YiweiJiang2015/RISeC O
4https://camel.abudhabi.nyu.edu/simmr/8223.2 O
Annotation O
Structure O
Entity O
Types O
Action O
: O
Most O
verbs O
, O
their O
present O
/ O
past O
participles O
and O
verb O
phrases O
fall O
in O
this O
category O
. O
In O
addition O
to O
the O
Action O
label O
, O
speciÔ¨Åc O
verbs O
also O
carry O
a O
Zero O
Anaphora O
Verb O
( O
ZA O
V O
) O
label O
( O
see O
further O
) O
. O
Food O
: O
Ingredients O
, O
spices O
( O
salt O
, O
sugar O
, O
etc O
. O
) O
, O
intermediate O
products O
( O
e.g. O
, O
‚Äú O
the O
meat O
mixture O
‚Äù O
) O
. O
If O
a O
sequence O
of O
ingredients O
is O
involved O
in O
an O
action O
, O
we O
label O
each O
of O
them O
individually O
, O
as O
in O
Fig O
. O
1 O
. O
Tool O
: O
Appliances O
( O
e.g. O
, O
oven O
) O
, O
recipients O
( O
e.g. O
, O
bowl O
) O
, O
utensils O
( O
e.g. O
, O
fork O
) O
used O
to O
perform O
an O
action O
involved O
in O
the O
cooking O
process O
. O
Duration O
: O
Time O
interval O
for O
which O
an O
action O
lasts O
( O
e.g. O
, O
‚Äò O
20 O
minutes O
‚Äô O
, O
‚Äò O
half O
an O
hour O
‚Äô O
) O
. O
Temperature O
: O
E.g. O
, O
‚Äú O
400 O
degrees O
F O
‚Äù O
. O
Other O
: O
This O
label O
is O
used O
for O
entities O
that O
can O
not O
be O
attributed O
to O
any O
entity O
label O
above O
. O
Further O
, O
we O
also O
annotate O
subclauses O
that O
provide O
information O
on O
certain O
actions O
as O
‚Äú O
entities O
‚Äù O
. O
Thus O
, O
we O
abuse O
entity O
labeling O
to O
indicate O
them O
and O
thus O
limit O
their O
annotation O
to O
shallow O
parsing O
: O
Condition O
Clause O
: O
Sub O
- O
clauses O
led O
by O
conjunctions O
like O
‚Äú O
until O
‚Äù O
, O
‚Äú O
till O
‚Äù O
, O
‚Äú O
when O
‚Äù O
, O
‚Äú O
before O
‚Äù O
, O
usually O
expressing O
timing O
. O
Purpose O
Clause O
: O
InÔ¨Ånitives O
and O
sub O
- O
clauses O
led O
by O
for O
example O
‚Äú O
so O
that O
‚Äù O
, O
‚Äú O
to O
make O
sure O
that O
‚Äù O
. O
Relation O
Types O
Following O
the O
methodology O
of O
PropBank O
, O
we O
deÔ¨Åne O
a O
set O
of O
relations O
for O
the O
semantic O
roles O
in O
recipe O
instructions O
. O
These O
relations O
have O
the O
verb O
as O
origin O
and O
link O
an O
action O
to O
its O
arguments O
( O
Arg O
* O
) O
or O
modiÔ¨Åers O
( O
ArgM O
* O
) O
. O
For O
details O
on O
their O
meanings O
, O
see O
PropBank O
‚Äôs O
annotation O
guidelines O
( O
Babko O
- O
Malaya O
, O
2005 O
) O
. O
However O
, O
to O
make O
the O
annotating O
schema O
self O
- O
consistent O
and O
adaptive O
to O
the O
cooking O
domain O
, O
we O
create O
( O
or O
extend O
) O
verb O
frames O
that O
are O
not O
( O
yet O
) O
included O
by O
PropBank O
. O
E.g. O
, O
for O
the O
verb O
phrase O
‚Äú O
beat O
in O
‚Äù O
, O
we O
borrow O
the O
argument O
structure O
from O
its O
main O
verb O
, O
i.e. O
, O
‚Äú O
beat O
‚Äù O
. O
ArgPPT O
: O
Participant O
, O
used O
for O
the O
argument O
which O
undergoes O
a O
change O
of O
state O
or O
is O
being O
affected O
by O
an O
action O
. O
ArgGOL O
: O
Goal O
, O
destination O
where O
an O
action O
ends O
. O
ArgDIR O
: O
Direction O
, O
the O
source O
where O
an O
action O
starts O
from O
. O
E.g. O
, O
‚Äú O
Remove O
the O
pan O
from O
oven O
to O
a O
rack O
‚Äù O
where O
‚Äú O
oven O
‚Äù O
is O
ArgDIR O
of O
the O
action O
‚Äú O
remove O
‚Äù O
. O
ArgPRD O
: O
Predicate O
, O
used O
for O
the O
end O
product O
of O
an O
action O
. O
E.g. O
, O
‚Äú O
Roll O
the O
cool O
dough O
into O
3 O
- O
inch O
ball O
‚Äù O
where O
the O
dough O
is O
transformed O
into O
‚Äú O
3 O
- O
inchballs O
‚Äù O
, O
ArgPRD O
of O
the O
action O
‚Äú O
roll O
‚Äù O
. O
ArgPAG O
: O
Agent O
, O
the O
subject O
that O
performs O
an O
action O
. O
ArgM O
MNR O
: O
Manner O
, O
describing O
how O
or O
in O
what O
condition O
we O
execute O
an O
action O
. O
E.g. O
, O
in‚ÄúPreheat O
the O
oven O
at O
340 O
degrees O
‚Äù O
, O
the O
relation O
ArgM O
MNR O
linksAction O
‚Äú O
preheat O
‚Äù O
to O
Temperature O
‚Äú O
340 O
degrees O
F O
‚Äù O
. O
ArgM O
LOC O
: O
Location O
where O
an O
action O
takes O
place O
. O
This O
notion O
is O
not O
restricted O
to O
physical O
locations O
, O
but O
abstract O
locations O
are O
being O
marked O
asArgM O
LOC O
as O
well O
. O
E.g. O
, O
in O
‚Äú O
Beat O
2 O
eggs O
in O
the O
Ô¨Çour O
‚Äù O
, O
ArgM O
LOC O
linksAction O
‚Äú O
beat O
‚Äù O
to O
Food O
‚Äú O
the O
Ô¨Çour O
‚Äù O
ArgM O
TMP O
: O
Temporal O
relation O
between O
action O
and O
timing O
nodes O
( O
Duration O
, O
Condition O
clause O
) O
. O
ArgM O
PRP O
: O
Purpose O
relation O
between O
action O
and O
purpose O
clause O
nodes O
. O
ArgM O
INT O
: O
Instrument O
, O
e.g. O
, O
the O
utensil O
to O
accomplish O
the O
action O
. O
ArgM O
SIM O
: O
Simultaneous O
, O
linking O
two O
actions O
performed O
at O
the O
same O
time O
. O
E.g. O
, O
in O
‚Äú O
Broil O
the O
lamb O
, O
moving O
pan O
so O
entire O
surface O
browns O
evenly O
‚Äù O
, O
ArgM O
SIM O
links O
‚Äú O
broil O
‚Äù O
to O
‚Äú O
moving O
‚Äù O
. O
Zero O
Anaphora O
Rephrasing O
Zero O
anaphora O
is O
the O
phenomenon O
of O
implicit O
, O
unmentioned O
references O
to O
earlier O
concepts O
. O
Figure O
1 O
gives O
two O
examples O
where O
explicit O
anaphors O
are O
manually O
added O
inside O
the O
brackets O
. O
The O
last O
sentence O
in O
Fig O
. O
1 O
would O
be O
ungrammatical O
without O
the O
unmentioned O
‚Äú O
the O
crab O
mixture O
‚Äù O
and O
‚Äú O
the O
baked O
crab O
‚Äù O
. O
In O
our O
annotations O
, O
we O
annotated O
1,526 O
Zero O
Anaphora O
Verbs O
with O
candidate O
expressions O
for O
the O
zero O
anaphora O
, O
providing O
at O
least O
two O
alternatives O
: O
a O
succinct O
noun O
, O
as O
well O
as O
a O
more O
detailed O
noun O
phrase O
. O
4 O
Model O
We O
focus O
on O
two O
tasks O
: O
( O
1 O
) O
joint O
entity O
recognition O
, O
relation O
extraction O
and O
zero O
anaphora O
identiÔ¨Åcation O
, O
and O
( O
2 O
) O
zero O
anaphora O
description O
generation O
. O
Next O
we O
present O
our O
models O
for O
each O
. O
4.1 O
Entity O
recognition O
, O
relation O
extraction O
& O
zero O
anaphora O
identiÔ¨Åcation O
We O
use O
a O
span O
- O
based O
model O
, O
taking O
the O
input O
sequence O
of O
words O
as O
input O
, O
and O
passing O
it O
through O
4 O
components O
: O
( O
i O
) O
word O
representation O
, O
( O
ii O
) O
span O
representation O
, O
( O
iii O
) O
entity O
recognition O
, O
and O
( O
iv O
) O
relation O
identiÔ¨Åcation O
. O
Word O
Representation O
: O
We O
use O
a O
BiLSTM O
as O
the823base O
encoder O
. O
The O
inputs O
are O
vector O
representations O
of O
the O
sentence O
tokens O
obtained O
by O
concatenating O
pre O
- O
trained O
GLoVe O
embeddings O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
and O
character O
representations O
( O
using O
a O
CNN O
, O
ReLU O
and O
max O
pooling O
, O
as O
proposed O
by O
dos O
Santos O
and O
Guimar O
Àúaes O
, O
2015 O
) O
. O
Further O
, O
we O
also O
experimented O
with O
pre O
- O
trained O
BERT O
models O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
instead O
of O
Glove O
embeddings O
. O
Span O
Representation O
: O
We O
enumerate O
all O
possible O
word O
spans O
from O
the O
input O
sentence O
and O
concatenate O
the O
aforementioned O
BiLSTM O
( O
hleft O
, O
hright O
) O
encoder O
outputs O
at O
Ô¨Årst O
( O
f O
) O
and O
last O
( O
l O
) O
end O
- O
point O
tokens O
of O
each O
span O
, O
together O
with O
its O
length O
( O
elen O
) O
to O
obtain O
a O
span O
representation O
( O
si= O
( O
hleft O
, O
f O
, O
hright O
, O
f O
, O
hleft O
, O
l O
, O
hright O
, O
l O
, O
elen O
) O
) O
. O
Entity O
Recognition O
& O
Zero O
Anaphora O
Verb O
IdentiÔ¨Åcation O
: O
We O
pass O
the O
selected O
span O
representations O
sithrough O
a O
feed O
- O
forward O
neural O
network O
( O
FFNN O
) O
yielding O
per O
- O
class O
scores O
for O
predicting O
entity O
types O
as O
well O
as O
binary O
Zero O
Anaphora O
Verb O
labels O
( O
with O
kentity O
classes O
, O
the O
FFNN O
thus O
has O
k+ O
1outputs O
) O
. O
Relation O
IdentiÔ¨Åcation O
: O
The O
concatenation O
of O
two O
span O
representations O
( O
si O
, O
sj O
) O
is O
passed O
through O
another O
FFNN O
to O
derive O
per O
- O
class O
relation O
scores O
. O
Since O
this O
is O
quadratic O
, O
we O
only O
pass O
the O
top O
20 O
% O
highest O
scored O
spans O
to O
the O
Relation O
FFNN O
: O
every O
span O
pair O
( O
si O
, O
sj)is O
Ô¨Årst O
passed O
through O
a O
pruning O
FFNN O
, O
and O
only O
its O
top O
- O
scored O
pairs O
are O
pushed O
through O
the O
Relation O
FFNN O
. O
Training O
: O
For O
each O
recipe O
instance O
, O
the O
objective O
is O
to O
optimize O
the O
weighted O
sum O
of O
the O
negative O
log O
likelihood O
of O
span O
representation O
, O
entity O
classiÔ¨Åcation O
and O
relation O
identiÔ¨Åcation O
. O
We O
use O
Adam O
to O
optimize O
the O
model O
with O
learning O
rate O
0.001 O
. O
4.2 O
Zero O
anaphora O
description O
generation O
For O
the O
generation O
task O
, O
we O
build O
a O
baseline O
model O
corresponding O
to O
the O
sequence O
- O
to O
- O
sequence O
architecture O
used O
in O
Bahdanau O
et O
al O
. O
( O
2015 O
) O
. O
The O
input O
is O
the O
entire O
recipe O
, O
which O
we O
pass O
to O
an O
LSTM O
encoder O
taking O
the O
pre O
- O
trained O
GloVE O
embedding O
, O
concatenated O
with O
a O
binary O
label O
indicating O
whether O
it O
is O
a O
zero O
anaphora O
verb O
( O
ZA O
V O
) O
, O
and O
( O
optionally O
) O
an O
entity O
type O
embedding O
if O
the O
token O
is O
of O
a O
given O
type O
. O
Since O
usually O
the O
target O
description O
that O
the O
decoder O
needs O
to O
generate O
is O
much O
shorter O
than O
the O
full O
recipe O
, O
we O
adopt O
bilinear O
attention O
( O
Luong O
et O
al O
. O
, O
2015 O
) O
. O
The O
model O
is O
trained O
to O
minimize O
the O
negative O
log O
likelihood O
ofGlove O
BertbaseBertlarge O
Entity O
89.8 O
91.7 O
92.6 O
Zero O
Anaphora O
Verb O
89.1 O
89.0 O
89.8 O
Relation O
65.5 O
67.1 O
67.5 O
Table O
1 O
: O
Micro O
- O
F1 O
scores O
of O
models O
with O
Glove O
, O
Bert O
base O
and O
Bert O
large O
on O
the O
test O
set O
. O
Full O
Test O
set O
Count O
Prec O
. O
Recall O
F1 O
Food O
3,232 O
92.5 O
95.9 O
94.2 O
Action O
3,061 O
96.6 O
97.4 O
97.0 O
Tool O
1,138 O
92.9 O
86.8 O
89.8 O
Condition O
clause O
487 O
93.0 O
71.1 O
80.5 O
Duration O
411 O
85.7 O
87.4 O
86.5 O
Temperature O
381 O
87.4 O
89.3 O
88.4 O
Other O
270 O
54.2 O
34.7 O
41.9 O
Purpose O
clause O
147 O
78.0 O
59.2 O
67.2 O
Table O
2 O
: O
Entity O
counts O
in O
full O
dataset O
and O
extraction O
results O
with O
Bert O
large O
on O
test O
set O
. O
an O
emitted O
token O
given O
the O
full O
input O
and O
predicted O
tokens O
. O
5 O
Experiments O
and O
results O
We O
split O
our O
RISeC O
dataset O
into O
50 O
% O
training O
, O
20 O
% O
development O
and O
30 O
% O
test O
sets O
, O
using O
the O
same O
splits O
as O
SIMMR O
( O
Jermsurawong O
and O
Habash O
, O
2015 O
) O
. O
We O
tune O
hyperparameters O
on O
the O
development O
set O
. O
Reported O
performance O
metrics O
are O
obtained O
on O
the O
test O
set O
. O
In O
general O
, O
our O
span O
- O
based O
model O
shows O
good O
performance O
in O
the O
extraction O
task O
, O
as O
shown O
in O
Table O
1 O
. O
We O
obtain O
micro O
- O
F1 O
scores O
for O
the O
joint O
entity O
, O
zero O
anaphora O
verbs O
and O
relation O
identiÔ¨Åcation O
tasks O
of O
respectively O
89.8 O
, O
89.1 O
and O
65.5 O
when O
using O
Glove O
word O
embeddings O
. O
With O
Bert O
large O
word O
encodings O
, O
performance O
consistently O
improves O
by O
2.8 O
, O
0.7 O
and O
2.0 O
percentage O
points O
respectively O
, O
indicating O
the O
applicability O
of O
the O
general O
linguistic O
knowledge O
from O
Bert O
on O
a O
cooking O
- O
domain O
task O
. O
Individual O
entity O
and O
relation O
type O
performance O
is O
reported O
in O
Tables O
2‚Äì3 O
. O
As O
expected O
, O
Table O
2 O
shows O
that O
entity O
F1 O
scores O
are O
positively O
correlated O
with O
the O
occurrence O
frequency O
, O
except O
forDuration O
andTemperature O
, O
of O
which O
the O
Ô¨Åxed O
pattern O
is O
easy O
to O
learn O
. O
The O
high O
precision O
and O
recall O
of O
important O
entities O
like O
Food O
andAction O
shows O
promising O
potential O
of O
our O
model O
for O
downstream O
applications O
like O
a O
question O
answering O
system O
in O
smart O
kitchen O
settings O
. O
The O
F1824Full O
Test O
set O
Count O
Prec O
. O
Recall O
F1 O
Arg O
PPT O
3,196 O
94.1 O
69.3 O
79.8 O
Argument O
Arg O
GOL O
557 O
79.6 O
35.8 O
49.1 O
Relations O
Arg O
DIR O
91 O
93.9 O
34.5 O
50.4 O
Arg O
PRD O
74 O
77.8 O
27.4 O
40.0 O
Arg O
PAG O
25 O
0.0 O
0.0 O
0.0 O
ArgM O
TMP O
884 O
91.7 O
33.2 O
48.7 O
ArgM O
LOC O
515 O
87.8 O
49.7 O
63.3 O
ModiÔ¨Åer O
ArgM O
MNR O
432 O
86.7 O
35.6 O
50.1 O
Relations O
ArgM O
PRP O
137 O
85.2 O
9.1 O
15.8 O
ArgM O
SIM O
92 O
66.7 O
11.1 O
18.6 O
ArgM O
INT O
73 O
77.4 O
20.3 O
31.8 O
Table O
3 O
: O
Relation O
counts O
in O
full O
dataset O
and O
extraction O
results O
with O
Bert O
large O
on O
test O
set O
. O
scores O
of O
relation O
predictions O
in O
Table O
3 O
show O
that O
the O
imbalanced O
distribution O
of O
relation O
types O
causes O
detection O
of O
several O
relations O
to O
be O
difÔ¨Åcult O
, O
e.g. O
, O
the O
low O
recall O
rates O
for O
ArgPAG O
andArgM O
PRP O
. O
Future O
work O
should O
address O
this O
, O
e.g. O
, O
using O
a O
larger O
dataset O
( O
or O
pretraining O
on O
non O
- O
recipe O
corpora O
) O
. O
While O
the O
detection O
of O
zero O
anaphora O
verbs O
( O
ZA O
V O
) O
performs O
well O
, O
our O
Seq2seq O
based O
description O
generation O
largely O
failed O
, O
with O
very O
low O
performance O
and O
oftentimes O
outputting O
the O
same O
descriptions O
( O
e.g. O
, O
‚Äú O
mixture O
‚Äù O
or O
‚Äú O
chicken O
‚Äù O
) O
. O
In O
hindsight O
, O
given O
the O
limited O
dataset O
size O
( O
order O
of O
1.5k O
ZA O
V O
occurences O
in O
the O
full O
dataset O
) O
and O
the O
typically O
large O
training O
dataset O
needed O
for O
seq2seq O
models O
, O
this O
is O
not O
entirely O
unexpected O
. O
Further O
work O
on O
this O
task O
is O
clearly O
required O
. O
6 O
Conclusion O
and O
Future O
Work O
This O
paper O
introduced O
RISeC O
, O
a O
dataset O
for O
extracting O
structural O
information O
and O
resolving O
zero O
anaphora O
from O
unstructured O
recipes O
. O
The O
corpus O
consists O
of O
260 O
recipes O
from O
SIMMR O
and O
provides O
semantic O
graph O
annotations O
of O
( O
i O
) O
recipe O
- O
related O
entities O
, O
( O
ii O
) O
generic O
verb O
relations O
( O
from O
PropBank O
) O
connecting O
these O
entities O
, O
( O
iii O
) O
zero O
anaphora O
verbs O
having O
implicit O
arguments O
, O
and O
( O
iv O
) O
textual O
descriptions O
of O
those O
implicit O
arguments O
. O
We O
reported O
on O
our O
work O
- O
in O
- O
progress O
with O
two O
baseline O
models O
using O
our O
corpus O
: O
( O
i O
) O
a O
neural O
span O
- O
based O
model O
extracting O
entities O
, O
zero O
anaphora O
verbs O
and O
relations O
, O
and O
( O
ii O
) O
a O
sequence O
- O
to O
- O
sequence O
attention O
model O
generating O
noun O
phrases O
for O
zero O
anaphora O
verbs O
. O
We O
plan O
to O
continue O
working O
in O
this O
direction O
, O
making O
the O
dataset O
larger O
and O
more O
Ô¨Ånegrained O
, O
and O
especially O
, O
to O
investigate O
how O
itcan O
be O
leveraged O
for O
human O
- O
machine O
interaction O
experiments O
. O
Acknowledgments O
The O
Ô¨Årst O
author O
was O
supported O
by O
China O
Scholarship O
Council O
( O
201806020194 O
) O
. O
This O
research O
received O
funding O
from O
the O
Flemish O
Government O
under O
the O
‚Äú O
Onderzoeksprogramma O
ArtiÔ¨Åci O
¬®ele O
Intelligentie O
( O
AI O
) O
Vlaanderen O
‚Äù O
programme O
. O
We O
would O
like O
to O
thank O
anonymous O
reviewers O
who O
helped O
to O
improve O
the O
draft O
. O
Abstract O
Studies O
on O
grammatical O
error O
correction O
( O
GEC O
) O
have O
reported O
the O
effectiveness O
of O
pretraining O
a O
Seq2Seq O
model O
with O
a O
large O
amount O
of O
pseudodata O
. O
However O
, O
this O
approach O
requires O
time O
- O
consuming O
pretraining O
for O
GEC O
because O
of O
the O
size O
of O
the O
pseudodata O
. O
In O
this O
study O
, O
we O
explore O
the O
utility O
of O
bidirectional O
and O
auto O
- O
regressive O
transformers O
( O
BART O
) O
as O
a O
generic O
pretrained O
encoder O
‚Äì O
decoder O
model O
for O
GEC O
. O
With O
the O
use O
of O
this O
generic O
pretrained O
model O
for O
GEC O
, O
the O
time O
- O
consuming O
pretraining O
can O
be O
eliminated O
. O
We O
Ô¨Ånd O
that O
monolingual O
and O
multilingual O
BART O
models O
achieve O
high O
performance O
in O
GEC O
, O
with O
one O
of O
the O
results O
being O
comparable O
to O
the O
current O
strong O
results O
in O
English O
GEC O
. O
Our O
implementations O
are O
publicly O
available O
at O
GitHub1 O
. O
1 O
Introduction O
Grammatical O
error O
correction O
( O
GEC O
) O
is O
the O
automatic O
correction O
of O
grammatical O
and O
other O
language O
- O
related O
errors O
in O
text O
. O
Most O
works O
regard O
this O
task O
as O
a O
translation O
task O
and O
use O
encoder O
‚Äì O
decoder O
( O
Enc O
‚Äì O
Dec O
) O
architectures O
to O
convert O
ungrammatical O
sentences O
to O
grammatical O
ones O
. O
This O
Enc O
‚Äì O
Dec O
approach O
often O
does O
not O
require O
linguistic O
knowledge O
of O
the O
target O
language O
. O
Strong O
Enc O
‚Äì O
Dec O
models O
for O
GEC O
are O
pretrained O
with O
a O
large O
amount O
of O
artiÔ¨Åcially O
generated O
data O
, O
commonly O
referred O
to O
as O
‚Äò O
pseudodata O
‚Äô O
, O
that O
is O
created O
by O
introducing O
artiÔ¨Åcial O
error O
to O
a O
monolingual O
corpus O
. O
Hereafter O
, O
pretraining O
using O
pseudodata O
aimed O
at O
the O
GEC O
task O
is O
referred O
to O
as O
task O
- O
oriented O
pretraining O
( O
Kiyono O
et O
al O
. O
, O
2019 O
; O
Grundkiewicz O
et O
al O
. O
, O
2019 O
; O
N O
¬¥ O
aplava O
and O
Straka O
, O
2019 O
; O
Kaneko O
et O
al O
. O
, O
2020 O
) O
. O
For O
example O
, O
Kiyono O
et O
al O
. O
( O
2019 O
) O
generated O
a O
pseudo O
corpus O
using O
back O
- O
translation O
and O
‚àóCurrently O
working O
at O
Retrieva O
, O
Inc. O
1https://github.com/Katsumata420/generic-pretrainedGECachieved O
strong O
results O
for O
English O
GEC O
. O
N O
¬¥ O
aplava O
and O
Straka O
( O
2019 O
) O
generated O
a O
pseudo O
corpus O
by O
introducing O
artiÔ¨Åcial O
errors O
into O
monolingual O
corpora O
and O
achieved O
the O
best O
scores O
for O
GEC O
in O
several O
languages O
by O
adopting O
the O
methods O
proposed O
by O
Grundkiewicz O
et O
al O
. O
( O
2019 O
) O
. O
These O
task O
- O
oriented O
pretraining O
approaches O
require O
extensive O
use O
of O
a O
pseudo O
- O
parallel O
corpus O
. O
SpeciÔ¨Åcally O
, O
Grundkiewicz O
et O
al O
. O
( O
2019 O
) O
used O
100 O
M O
ungrammatical O
and O
grammatical O
sentence O
pairs O
, O
while O
Kiyono O
et O
al O
. O
( O
2019 O
) O
and O
Kaneko O
et O
al O
. O
( O
2020 O
) O
used O
70 O
M O
sentence O
pairs O
, O
which O
required O
time O
- O
consuming O
pretraining O
of O
GEC O
models O
using O
the O
pseudo O
corpus O
. O
In O
this O
study O
, O
we O
determined O
the O
effectiveness O
of O
publicly O
available O
pretrained O
Enc O
‚Äì O
Dec O
models O
for O
GEC O
. O
SpeciÔ¨Åcally O
, O
we O
investigated O
pretrained O
models O
without O
the O
need O
for O
pseudodata O
. O
We O
explored O
a O
pretrained O
model O
proposed O
by O
Lewis O
et O
al O
. O
( O
2020 O
) O
called O
bidirectional O
and O
auto O
- O
regressive O
transformers O
( O
BART O
) O
. O
Liu O
et O
al O
. O
( O
2020 O
) O
also O
proposed O
multilingual O
BART O
. O
These O
models O
were O
pretrained O
by O
predicting O
the O
original O
sequence O
, O
given O
a O
masked O
and O
shufÔ¨Çed O
sentence O
. O
The O
motivation O
for O
using O
these O
models O
for O
GEC O
was O
that O
it O
achieved O
strong O
results O
for O
several O
text O
generation O
tasks O
, O
such O
as O
summarization O
; O
we O
refer O
to O
it O
as O
a O
generic O
pretrained O
model O
. O
We O
used O
generic O
pretrained O
BART O
models O
to O
compare O
with O
GEC O
models O
using O
a O
pseudo O
- O
corpus O
approach O
( O
Kiyono O
et O
al O
. O
, O
2019 O
; O
Kaneko O
et O
al O
. O
, O
2020 O
; O
N O
¬¥ O
aplava O
and O
Straka O
, O
2019 O
) O
. O
We O
conducted O
GEC O
experiments O
for O
four O
languages O
: O
English O
, O
German O
, O
Czech O
, O
and O
Russian O
. O
The O
Enc O
‚Äì O
Dec O
model O
based O
on O
BART O
achieved O
results O
comparable O
with O
those O
of O
current O
strong O
Enc O
‚Äì O
Dec O
models O
for O
English O
GEC O
. O
The O
multilingual O
model O
also O
showed O
high O
performance O
in O
other O
languages O
, O
despite O
only O
requiring O
Ô¨Åne O
- O
tuning O
. O
These O
results O
suggest O
that O
BART O
can O
be O
used O
as O
a O
simple O
baseline827for O
GEC O
. O
2 O
Previous O
Work O
The O
Enc O
‚Äì O
Dec O
approach O
for O
GEC O
often O
uses O
the O
task O
- O
oriented O
pretraining O
strategy O
. O
For O
example O
, O
Zhao O
et O
al O
. O
( O
2019 O
) O
and O
Grundkiewicz O
et O
al O
. O
( O
2019 O
) O
reported O
that O
pretraining O
of O
the O
Enc O
‚Äì O
Dec O
model O
using O
a O
pseudo O
corpus O
is O
effective O
for O
the O
GEC O
task O
. O
In O
particular O
, O
they O
introduced O
word- O
and O
character O
- O
level O
errors O
into O
a O
sentence O
in O
monolingual O
corpora O
. O
They O
developed O
a O
confusion O
set O
derived O
from O
a O
spellchecker O
and O
randomly O
replaced O
a O
word O
in O
a O
sentence O
. O
They O
also O
randomly O
deleted O
a O
word O
, O
inserted O
a O
random O
word O
, O
and O
swapped O
a O
word O
with O
an O
adjacent O
word O
. O
They O
performed O
these O
same O
operations O
, O
i.e. O
, O
replacing O
, O
deleting O
, O
inserting O
, O
and O
swapping O
, O
for O
characters O
. O
The O
pseudo O
corpus O
made O
by O
the O
above O
methods O
consisted O
of O
100 O
M O
training O
samples O
. O
Our O
study O
aims O
to O
investigate O
whether O
the O
generic O
pretrained O
models O
are O
effective O
for O
GEC O
, O
because O
pretraining O
with O
such O
a O
large O
corpus O
is O
time O
- O
consuming O
. O
N¬¥aplava O
and O
Straka O
( O
2019 O
) O
adopted O
Grundkiewicz O
et O
al O
. O
( O
2019 O
) O
‚Äôs O
method O
for O
several O
languages O
, O
including O
German O
, O
Czech O
, O
and O
Russian O
. O
They O
trained O
a O
Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
with O
pseudo O
corpora O
( O
10 O
M O
sentence O
pairs O
) O
, O
and O
achieved O
current O
state O
- O
of O
- O
the O
- O
art O
( O
SOTA O
) O
results O
for O
German O
, O
Czech O
, O
and O
Russian O
GEC O
. O
We O
compared O
their O
results O
with O
those O
of O
the O
generic O
pretrained O
model O
to O
conÔ¨Årm O
whether O
the O
model O
was O
effective O
for O
GEC O
in O
several O
languages O
. O
Kiyono O
et O
al O
. O
( O
2019 O
) O
explored O
the O
generation O
of O
a O
pseudo O
corpus O
by O
introducing O
random O
errors O
or O
using O
back O
- O
translation O
. O
They O
reported O
that O
a O
taskoriented O
pretraining O
with O
back O
- O
translation O
data O
and O
character O
errors O
is O
better O
than O
that O
with O
pseudodata O
based O
on O
random O
errors O
. O
Kaneko O
et O
al O
. O
( O
2020 O
) O
combined O
Kiyono O
et O
al O
. O
( O
2019 O
) O
‚Äôs O
pretraining O
approach O
with O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
and O
improved O
Kiyono O
et O
al O
. O
( O
2019 O
) O
‚Äôs O
results O
. O
SpeciÔ¨Åcally O
, O
Kaneko O
et O
al O
. O
( O
2020 O
) O
Ô¨Åne O
- O
tuned O
BERT O
with O
a O
grammatical O
error O
detection O
task O
. O
The O
Ô¨Åne O
- O
tuned O
BERT O
outputs O
for O
each O
token O
were O
combined O
with O
the O
original O
tokens O
as O
a O
GEC O
input O
. O
Their O
study O
is O
similar O
to O
our O
research O
in O
that O
both O
studies O
use O
publicly O
available O
generic O
pretrained O
models O
to O
perform O
GEC O
. O
The O
difference O
between O
these O
studies O
is O
that O
Kaneko O
et O
al O
. O
( O
2020 O
) O
used O
the O
architecture O
of O
the O
pretrained O
model O
as O
an O
encoder O
. O
Therefore O
, O
their O
method O
still O
requires O
pretraining O
with O
a O
largeamount O
of O
pseudodata O
. O
The O
current O
SOTA O
approach O
for O
English O
GEC O
uses O
the O
sequence O
tagging O
model O
proposed O
by O
Omelianchuk O
et O
al O
. O
( O
2020 O
) O
. O
They O
designed O
tokenlevel O
transformations O
to O
map O
input O
tokens O
to O
target O
corrections O
to O
produce O
training O
data O
. O
The O
sequence O
tagging O
model O
then O
predicts O
the O
transformation O
corresponding O
to O
the O
input O
token O
. O
We O
do O
not O
attempt O
to O
make O
a O
comparison O
with O
this O
approach O
, O
as O
the O
purpose O
of O
our O
study O
is O
to O
create O
a O
strong O
GEC O
model O
without O
using O
pseudodata O
or O
linguistic O
knowledge O
. O
3 O
Generic O
Pretrained O
Model O
BART O
( O
Lewis O
et O
al O
. O
, O
2020 O
) O
is O
pretrained O
by O
predicting O
an O
original O
sequence O
, O
given O
a O
masked O
and O
shufÔ¨Çed O
sequence O
using O
a O
Transformer O
. O
They O
introduced O
masked O
tokens O
with O
various O
lengths O
based O
on O
the O
Poisson O
distribution O
, O
inspired O
by O
SpanBERT O
( O
Joshi O
et O
al O
. O
, O
2020 O
) O
, O
at O
multiple O
positions O
. O
BART O
is O
pretrained O
with O
large O
monolingual O
corpora O
( O
160 O
GB O
) O
, O
including O
news O
, O
books O
, O
stories O
, O
and O
web O
- O
text O
domains O
. O
This O
model O
achieved O
strong O
results O
in O
several O
generation O
tasks O
; O
thus O
, O
it O
is O
regarded O
as O
a O
generic O
model O
. O
They O
released O
pretrained O
models O
using O
English O
monolingual O
corpora O
for O
several O
tasks O
, O
including O
summarization O
, O
which O
we O
used O
for O
English O
GEC O
. O
Liu O
et O
al O
. O
( O
2020 O
) O
proposed O
multilingual O
BART O
( O
mBART O
) O
for O
a O
machine O
translation O
task O
, O
which O
we O
used O
for O
GEC O
of O
several O
languages O
. O
The O
latter O
model O
was O
trained O
using O
monolingual O
corpora O
for O
25 O
languages O
simultaneously O
. O
They O
used O
a O
special O
token O
for O
representing O
the O
language O
of O
a O
sentence O
. O
For O
example O
, O
they O
added O
< O
de_DE O
> O
and O
< O
ru_RU O
> O
into O
the O
initial O
token O
of O
the O
encoder O
and O
decoder O
for O
De O
‚Äì O
Ru O
translation O
. O
To O
Ô¨Åne O
- O
tune O
mBART O
for O
German O
, O
Czech O
, O
and O
Russian O
GEC O
, O
we O
set O
the O
target O
language O
for O
the O
special O
token O
referring O
to O
that O
language O
. O
4 O
Experiment O
4.1 O
Settings O
Common O
Settings O
. O
As O
presented O
in O
Table O
1 O
, O
we O
used O
learner O
corpora O
, O
including O
BEA2(Bryant O
et O
al O
. O
, O
2019 O
; O
Granger O
, O
1998 O
; O
Mizumoto O
et O
al O
. O
, O
2011 O
; O
Tajiri O
et O
al O
. O
, O
2012 O
; O
Yannakoudakis O
et O
al O
. O
, O
2011 O
; O
Dahlmeier O
et O
al O
. O
, O
2013 O
) O
, O
JFLEG O
( O
Napoles O
et O
al O
. O
, O
2017 O
) O
, O
and O
CoNLL-14 O
( O
Ng O
et O
al O
. O
, O
2014 O
) O
data O
for O
2BEA O
corpus O
is O
made O
of O
several O
corpora O
. O
Details O
can O
be O
found O
in O
Bryant O
et O
al O
. O
( O
2019).828lang O
Corpus O
Train O
Dev O
Test O
BEA O
1,157,370 O
4,384 O
4,477 O
En O
JFLEG O
- O
- O
747 O
CoNLL-2014 O
- O
- O
1,312 O
De O
Falko+MERLIN O
19,237 O
2,503 O
2,337 O
Cz O
AKCES O
- O
GEC O
42,210 O
2,485 O
2,676 O
Ru O
RULEC O
- O
GEC O
4,980 O
2,500 O
5,000 O
Table O
1 O
: O
Data O
statistics O
. O
English O
; O
Falko+MERLIN O
data O
( O
Boyd O
et O
al O
. O
, O
2014 O
) O
for O
German O
; O
AKCES O
- O
GEC O
( O
N O
¬¥ O
aplava O
and O
Straka O
, O
2019 O
) O
for O
Czech O
; O
and O
RULEC O
- O
GEC O
( O
Rozovskaya O
and O
Roth O
, O
2019 O
) O
for O
Russian O
. O
Our O
models O
were O
Ô¨Åne O
- O
tuned O
using O
a O
single O
GPU O
( O
NVIDIA O
TITAN O
RTX O
) O
, O
and O
our O
implementations O
were O
based O
on O
publicly O
available O
code3 O
. O
We O
used O
the O
hyperparameters O
provided O
in O
some O
previous O
works O
( O
Lewis O
et O
al O
. O
, O
2020 O
; O
Liu O
et O
al O
. O
, O
2020 O
) O
, O
unless O
otherwise O
noted O
. O
The O
scores O
excluding O
the O
ensemble O
method O
were O
averaged O
in O
Ô¨Åve O
Ô¨Åne O
- O
tuned O
experiments O
with O
random O
seeds O
. O
English O
. O
Our O
setting O
for O
the O
English O
datasets O
was O
almost O
the O
same O
as O
that O
of O
Kiyono O
et O
al O
. O
( O
2019 O
) O
. O
We O
extracted O
the O
training O
data O
from O
BEA O
- O
train O
for O
English O
GEC O
. O
Similar O
to O
Kiyono O
et O
al O
. O
( O
2019 O
) O
, O
we O
did O
not O
use O
the O
unchanged O
sentences O
in O
the O
source O
and O
target O
sides O
; O
thus O
, O
the O
training O
data O
consisted O
of O
561,525 O
sentences O
. O
We O
used O
BEA O
- O
dev O
to O
determine O
the O
best O
model O
. O
We O
trained O
the O
BART O
- O
based O
models O
by O
using O
bart.large O
. O
This O
model O
was O
proposed O
for O
the O
summarization O
task O
, O
which O
required O
some O
constraints O
in O
inference O
to O
ensure O
appropriate O
outputs O
; O
however O
, O
we O
did O
not O
impose O
any O
constraints O
because O
our O
task O
was O
different O
. O
We O
applied O
byte O
pair O
encoding O
( O
BPE O
) O
( O
Sennrich O
et O
al O
. O
, O
2016 O
) O
to O
the O
training O
data O
for O
the O
BART O
- O
based O
model O
by O
using O
the O
BPE O
model O
of O
Lewis O
et O
al O
. O
( O
2020 O
) O
. O
We O
used O
the O
M2scorer O
( O
Dahlmeier O
and O
Ng O
, O
2012 O
) O
and O
GLEU O
( O
Napoles O
et O
al O
. O
, O
2015 O
) O
for O
CoNLL-14 O
and O
JFLEG O
, O
respectively O
, O
and O
used O
the O
ERRANT O
scorer O
( O
Bryant O
et O
al O
. O
, O
2017 O
) O
for O
BEAtest O
. O
We O
compared O
these O
scores O
with O
strong O
results O
( O
Kiyono O
et O
al O
. O
, O
2019 O
; O
Kaneko O
et O
al O
. O
, O
2020 O
) O
. O
German O
, O
Czech O
, O
and O
Russian O
. O
The O
dataset O
settings O
in O
this O
study O
were O
almost O
the O
same O
as O
those O
3BART O
, O
mBART O
: O
https://github.com/pytorch/fairseqused O
by O
N O
¬¥ O
aplava O
and O
Straka O
( O
2019 O
) O
for O
each O
language O
. O
We O
used O
ofÔ¨Åcial O
training O
data O
and O
decided O
the O
best O
model O
by O
using O
the O
development O
data O
. O
In O
addition O
, O
we O
trained O
the O
mBART O
- O
based O
models O
for O
German O
, O
Czech O
, O
and O
Russian O
GEC O
. O
We O
usedmbart.cc25 O
for O
the O
mBART O
- O
based O
models O
. O
For O
the O
mBART O
- O
based O
model O
, O
we O
followed O
Liu O
et O
al O
. O
( O
2020 O
) O
; O
we O
detokenized4the O
GEC O
training O
data O
for O
the O
mBART O
- O
based O
model O
and O
applied O
SentencePiece O
( O
Kudo O
and O
Richardson O
, O
2018 O
) O
with O
the O
SentencePiece O
model O
shared O
by O
Liu O
et O
al O
. O
( O
2020 O
) O
. O
Using O
this O
preprocessing O
, O
the O
input O
sentence O
may O
not O
represent O
grammatical O
information O
, O
compared O
with O
the O
sentence O
tokenized O
using O
a O
morphological O
analysis O
tool O
and O
subword O
tokenizer O
. O
However O
, O
what O
preprocessing O
is O
appropriate O
for O
GEC O
is O
beyond O
this O
paper O
‚Äôs O
scope O
and O
will O
be O
treated O
as O
future O
work O
. O
For O
evaluation O
, O
we O
tokenized O
the O
outputs O
after O
recovering O
the O
subwords O
. O
Then O
, O
we O
used O
a O
spaCy O
- O
based5tokenizer O
for O
German6and O
Russian7 O
, O
and O
the O
MorphoDiTa O
tokenizer8for O
Czech O
. O
Moreover O
, O
the O
M2scorer O
was O
used O
for O
each O
language O
. O
We O
compared O
these O
scores O
with O
the O
current O
SOTA O
results O
( O
N O
¬¥ O
aplava O
and O
Straka O
, O
2019 O
) O
. O
4.2 O
Results O
English O
. O
Table O
2 O
presents O
the O
results O
of O
the O
English O
GEC O
task O
. O
When O
using O
a O
single O
model O
, O
the O
BART O
- O
based O
model O
is O
better O
than O
the O
model O
proposed O
by O
Kiyono O
et O
al O
. O
( O
2019 O
) O
, O
and O
the O
results O
are O
comparable O
to O
those O
reported O
by O
Kaneko O
et O
al O
. O
( O
2020 O
) O
in O
terms O
of O
CoNLL-14 O
and O
BEA O
- O
test O
. O
Kiyono O
et O
al O
. O
( O
2019 O
) O
and O
Kaneko O
et O
al O
. O
( O
2020 O
) O
incorporated O
several O
techniques O
to O
improve O
the O
accuracy O
of O
GEC O
. O
To O
compare O
these O
models O
, O
we O
experimented O
with O
an O
ensemble O
of O
Ô¨Åve O
models O
. O
Our O
ensemble O
model O
was O
slightly O
better O
than O
our O
single O
model O
, O
but O
worse O
than O
the O
ensemble O
models O
by O
Kiyono O
et O
al O
. O
( O
2019 O
) O
and O
Kaneko O
et O
al O
. O
( O
2020 O
) O
. O
The O
BART O
- O
based O
model O
along O
with O
the O
ensemble O
model O
achieved O
results O
comparable O
to O
current O
strong O
results O
despite O
only O
requiring O
Ô¨Åne O
- O
tuning O
of O
the O
BART O
model O
. O
We O
believe O
that O
the O
reason O
for O
the O
ineffectiveness O
of O
the O
ensemble O
method O
is O
that O
the O
Ô¨Åve O
models O
are O
not O
signiÔ¨Åcantly O
different O
as O
the O
4We O
used O
detokenizer.perl O
in O
the O
Moses O
script O
( O
Koehn O
et O
al O
. O
, O
2007 O
) O
. O
5https://spacy.io O
6We O
used O
the O
built O
- O
in O
de O
model O
. O
7https://github.com/aatimofeev/spacy O
russian O
tokenizer O
8https://github.com/ufal/morphodita829CoNLL-14 O
( O
M2 O
) O
JFLEG O
BEA O
- O
test O
P O
R O
F0.5 O
GLEU O
P O
R O
F0.5 O
Kiyono O
et O
al O
. O
( O
2019 O
) O
67.9/73.3 O
44.1/44.2 O
61.3/64.7 O
59.7/61.2 O
65.5/74.7 O
59.4/56.7 O
64.2/70.2 O
Kaneko O
et O
al O
. O
( O
2020 O
) O
69.2/72.6 O
45.6/46.4 O
62.6/65.2 O
61.3/62.0 O
67.1/72.3 O
60.1/61.4 O
65.6/69.8 O
BART O
- O
based O
69.3/69.9 O
45.0/45.1 O
62.6/63.0 O
57.3/57.2 O
68.3/68.8 O
57.1/57.1 O
65.6/66.1 O
Table O
2 O
: O
English O
GEC O
results O
. O
Left O
and O
right O
scores O
represent O
single O
and O
ensemble O
model O
results O
, O
respectively O
. O
Bold O
scores O
represent O
the O
best O
score O
in O
the O
single O
models O
, O
and O
underlined O
scores O
represent O
the O
best O
overall O
score O
. O
P O
R O
F0.5 O
DeN¬¥aplava O
and O
Straka O
( O
2019 O
) O
78.21 O
59.94 O
73.31 O
mBART O
- O
based O
73.97 O
53.98 O
68.86 O
CzN¬¥aplava O
and O
Straka O
( O
2019 O
) O
83.75 O
68.48 O
80.17 O
mBART O
- O
based O
78.48 O
58.70 O
73.52 O
N¬¥aplava O
and O
Straka O
( O
2019 O
) O
63.26 O
27.50 O
50.20 O
Ru O
mBART O
- O
based O
32.13 O
4.99 O
15.38 O
with O
pseudo O
corpus O
53.50 O
26.35 O
44.36 O
Table O
3 O
: O
German O
, O
Czech O
, O
and O
Russian O
GEC O
results O
. O
These O
models O
are O
not O
an O
ensemble O
of O
multiple O
models O
. O
initial O
weights O
are O
the O
same O
as O
those O
of O
the O
BART O
model O
, O
and O
seeds O
only O
affect O
minor O
changes O
, O
such O
as O
training O
data O
order O
, O
and O
so O
on O
. O
German O
, O
Czech O
, O
and O
Russian O
. O
Table O
3 O
presents O
the O
results O
for O
German O
, O
Czech O
, O
and O
Russian O
GEC O
. O
In O
the O
German O
GEC O
task O
, O
the O
mBART O
- O
based O
model O
achieves O
4.45 O
F0.5points O
lower O
than O
the O
model O
by O
N O
¬¥ O
aplava O
and O
Straka O
( O
2019 O
) O
. O
This O
may O
be O
because O
N O
¬¥ O
aplava O
and O
Straka O
( O
2019 O
) O
pretrains O
the O
GEC O
model O
with O
only O
the O
target O
language O
, O
whereas O
mBART O
is O
pretrained O
with O
25 O
languages O
, O
resulting O
in O
the O
information O
of O
other O
languages O
being O
included O
as O
noise O
. O
In O
the O
Czech O
GEC O
task O
, O
the O
mBART O
- O
based O
model O
achieves O
6.65 O
F0.5points O
lower O
than O
the O
model O
by O
N O
¬¥ O
aplava O
and O
Straka O
( O
2019 O
) O
. O
Similar O
to O
the O
case O
of O
the O
German O
GEC O
results O
, O
we O
suppose O
that O
mBART O
includes O
noisy O
information O
. O
Considering O
Russian O
GEC O
, O
the O
mBART O
- O
based O
model O
shows O
much O
lower O
scores O
than O
N O
¬¥ O
aplava O
and O
Straka O
( O
2019 O
) O
‚Äôs O
model O
. O
This O
may O
be O
because O
the O
training O
data O
for O
Russian O
GEC O
are O
scarce O
compared O
to O
those O
of O
German O
or O
Czech O
. O
To O
investigate O
the O
effect O
of O
corpus O
size O
, O
we O
additionally O
trained O
the O
mBART O
model O
with O
a O
10 O
M O
pseudo O
corpus O
, O
using O
the O
method O
proposed O
by O
Grundkiewicz O
et O
al O
. O
( O
2019 O
) O
, O
and O
Ô¨Åne O
- O
tuned O
it O
with O
the O
learner O
corpus O
to O
compensate O
for O
the O
low O
- O
resource O
scenario O
. O
The O
results O
presented O
in O
Table O
3 O
support O
our O
hypothesis O
. O
Kaneko O
et O
al O
. O
( O
2020 O
) O
BART O
- O
based O
Error O
Type O
P O
R O
F0.5 O
P O
R O
F0.5 O
PUNCT O
74.1 O
52.7 O
68.5 O
79.2 O
59.0 O
74.1 O
DET O
73.7 O
72.9 O
73.5 O
76.3 O
71.1 O
75.2 O
PREP O
73.4 O
69.1 O
72.5 O
71.2 O
64.8 O
69.9 O
ORTH O
86.9 O
62.9 O
80.8 O
84.2 O
52.9 O
75.3 O
SPELL O
83.1 O
79.5 O
82.3 O
84.7 O
55.2 O
76.5 O
Table O
4 O
: O
BEA O
- O
test O
scores O
for O
the O
top O
Ô¨Åve O
error O
types O
, O
except O
for O
OTHER O
. O
Kaneko O
et O
al O
. O
( O
2020 O
) O
and O
BARTbased O
are O
ensemble O
models O
. O
Bold O
scores O
represent O
the O
best O
score O
for O
each O
error O
type O
. O
5 O
Discussion O
BART O
as O
a O
simple O
baseline O
model O
. O
According O
to O
the O
German O
and O
Czech O
GEC O
results O
, O
the O
mBART O
- O
based O
model O
, O
in O
which O
we O
only O
Ô¨Åne O
- O
tuned O
the O
pretrained O
mBART O
model O
, O
achieves O
comparable O
scores O
with O
SOTA O
models O
. O
In O
other O
words O
, O
mBART O
- O
based O
models O
are O
considered O
to O
show O
sufÔ¨Åciently O
high O
performance O
for O
several O
languages O
without O
using O
a O
pseudo O
corpus O
. O
These O
results O
indicate O
that O
the O
mBART O
- O
based O
model O
can O
be O
used O
as O
a O
simple O
GEC O
baseline O
for O
several O
languages O
. O
Performance O
comparison O
for O
each O
error O
type O
. O
We O
compare O
the O
BART O
- O
based O
model O
with O
Kaneko O
et O
al O
. O
( O
2020 O
) O
‚Äôs O
model O
for O
common O
error O
types O
using O
a O
generic O
pretrained O
model O
. O
Table O
4 O
presents O
the O
results O
for O
the O
top O
Ô¨Åve O
error O
types O
in O
BEA O
- O
test O
. O
According O
to O
these O
results O
, O
BART O
- O
based O
is O
superior O
to O
Kaneko O
et O
al O
. O
( O
2020 O
) O
in O
PUNCT O
and O
DET O
errors O
; O
in O
particular O
, O
PUNCT O
is O
5.6 O
F0.5points O
better O
. O
BART O
is O
pretrained O
to O
correct O
the O
shufÔ¨Çed O
and O
masked O
sequence O
, O
so O
that O
this O
model O
learns O
to O
place O
punctuation O
adequately O
. O
In O
contrast O
, O
Kaneko O
et O
al O
. O
( O
2020 O
) O
uses O
an O
encoder O
that O
is O
not O
pretrained O
with O
correcting O
shufÔ¨Çed O
sequences O
. O
Conversely O
, O
Kaneko O
et O
al O
. O
( O
2020 O
) O
report O
better O
results O
for O
other O
errors O
, O
except O
for O
DET O
. O
Regarding O
ORTH O
and O
SPELL O
, O
their O
model O
is O
more O
than O
5F0.5points O
better O
than O
the O
BART O
- O
based O
one O
. O
It O
is O
difÔ¨Åcult O
for O
the O
BART O
- O
based O
model O
to O
cor-830rect O
these O
errors O
because O
BART O
uses O
shufÔ¨Çed O
and O
masked O
sequences O
as O
noise O
in O
pretraining O
; O
not O
using O
character O
- O
level O
errors O
. O
Kaneko O
et O
al O
. O
( O
2020 O
) O
introduce O
character O
errors O
into O
a O
pseudo O
corpus O
as O
task O
- O
oriented O
Enc O
‚Äì O
Dec O
pretraining O
; O
this O
is O
the O
reason O
why O
the O
BART O
- O
based O
model O
is O
inferior O
to O
Kaneko O
et O
al O
. O
( O
2020 O
) O
in O
these O
errors O
. O
6 O
Conclusion O
We O
introduced O
a O
generic O
pretrained O
Enc O
‚Äì O
Dec O
model O
, O
BART O
, O
for O
GEC O
. O
The O
experimental O
results O
indicated O
that O
BART O
better O
initialized O
the O
Enc O
‚Äì O
Dec O
model O
parameters O
. O
The O
Ô¨Åne O
- O
tuned O
BART O
achieved O
remarkable O
results O
, O
which O
were O
comparable O
to O
the O
current O
strong O
results O
in O
English O
GEC O
. O
Indeed O
, O
the O
monolingual O
BART O
seems O
to O
be O
more O
effective O
for O
GEC O
than O
the O
model O
with O
a O
multilingual O
setting O
. O
However O
, O
although O
it O
is O
not O
as O
good O
as O
SOTA O
, O
Ô¨Åne O
- O
tuned O
mBART O
exhibited O
high O
performance O
in O
other O
languages O
. O
This O
implies O
that O
BART O
is O
a O
simple O
baseline O
model O
for O
pretraining O
GEC O
methods O
because O
it O
only O
requires O
Ô¨Åne O
- O
tuning O
as O
training O
. O
Acknowledgements O
We O
thank O
the O
anonymous O
reviewers O
for O
their O
insightful O
comments O
. O
This O
work O
has O
been O
partly O
supported O
by O
the O
programs O
of O
the O
Grant O
- O
in O
- O
Aid O
for O
ScientiÔ¨Åc O
Research O
from O
the O
Japan O
Society O
for O
the O
Promotion O
of O
Science O
( O
JSPS O
KAKENHI O
) O
Grant O
Numbers O
19K12099 O
and O
19KK0286 O
. O
Abstract O
Mandarin O
Alphabetical O
Word O
( O
MAW O
) O
is O
one O
indispensable O
component O
of O
Modern O
Chinese O
that O
demonstrates O
unique O
code O
- O
mixing O
idiosyncrasies O
inÔ¨Çuenced O
by O
language O
exchanges O
. O
Yet O
, O
this O
interesting O
phenomenon O
has O
not O
been O
properly O
addressed O
and O
is O
mostly O
excluded O
from O
the O
Chinese O
language O
system O
. O
This O
paper O
addresses O
the O
core O
problem O
of O
MAW O
identiÔ¨Åcation O
and O
proposes O
to O
construct O
a O
large O
collection O
of O
MAWs O
from O
Sina O
Weibo O
( O
SMAW O
) O
using O
an O
automatic O
web O
- O
based O
technique O
which O
includes O
rule O
- O
based O
identiÔ¨Åcation O
, O
informaticsbased O
extraction O
, O
as O
well O
as O
Baidu O
search O
engine O
validation O
. O
A O
collection O
of O
16,207 O
qualiÔ¨Åed O
SMAWs O
are O
obtained O
using O
this O
technique O
along O
with O
an O
annotated O
corpus O
of O
more O
than O
200,000 O
sentences O
for O
linguistic O
research O
and O
applicable O
inquiries O
. O
1 O
Introduction O
Mandarin O
Alphabetic O
Words O
( O
MAWs O
) O
, O
also O
known O
as O
lettered O
words O
( O
Liu O
, O
1994 O
) O
or O
code O
- O
mixing O
words O
( O
Nguyen O
and O
Cornips O
, O
2016 O
) O
, O
are O
usually O
formed O
by O
Latin O
, O
Greek O
, O
Arabic O
alphabets O
in O
combination O
with O
Chinese O
characters O
, O
e.g. O
‚Äú O
X- O
ÂÖâ O
/ O
XÂ∞Ñ O
Á∫ø‚Äù,X O
- O
ray O
. O
Although O
pure O
alphabets O
( O
e.g. O
‚Äú O
NBA O
‚Äù O
) O
used O
in O
Chinese O
context O
have O
also O
been O
regarded O
as O
MAWs O
in O
some O
previous O
work O
( O
Liu O
, O
1994 O
; O
Huang O
and O
Liu O
, O
2017 O
) O
, O
they O
are O
more O
like O
switching O
- O
codes O
that O
retain O
the O
orthography O
and O
linguistic O
behaviors O
of O
the O
original O
language O
, O
instead O
of O
showing O
typical O
Chinese O
lexical O
characteristics O
. O
It O
is O
noteworthy O
that O
MAWs O
shall O
be O
taken O
as O
a O
code O
- O
mixing O
phenomenon O
instead O
of O
code O
- O
switching O
as O
a O
MAW O
is O
still O
a O
Chinese O
word O
which O
is O
not O
switched O
into O
another O
language O
. O
Therefore O
, O
in O
this O
work O
, O
MAWS O
refer O
to O
the O
combined O
type O
which O
encodes O
both O
alphabet(s O
) O
and O
Chinese O
character(s O
) O
in O
one O
word O
, O
such O
as O
‚Äú O
A O
Âûã‚Äù,A O
- O
type O
, O
‚Äú O
PO‰∏ª‚Äù,post O
owner O
, O
and O
‚Äú O
Œ≥Á∫ø‚Äù,Gamma O
Ray O
.It O
is O
linguistically O
- O
interesting O
and O
applicablysigniÔ¨Åcant O
to O
investigate O
MAWs O
due O
to O
two O
main O
reasons O
. O
First O
, O
A O
MAW O
maintains O
part O
of O
the O
Chinese O
characteristics O
in O
morphology O
, O
phonology O
and O
orthography O
( O
e.g. O
‚Äú O
PK O
Ëøá",player O
killed O
, O
past O
tense O
) O
. O
Meanwhile O
, O
it O
also O
demonstrates O
some O
properties O
of O
the O
foreigner O
language O
( O
e.g. O
‚Äú O
Áª¥ÁîüÁ¥†ing O
" O
, O
supplementing O
Vitamin O
, O
progressive O
) O
) O
, O
providing O
a O
unique O
lexical O
resource O
for O
studying O
morphophonological O
idiosyncrasies O
of O
code O
- O
mixing O
words O
. O
Second O
, O
MAWs O
serve O
as O
an O
indispensable O
part O
of O
people O
‚Äôs O
daily O
vocabulary O
, O
especially O
under O
the O
rapid O
development O
of O
social O
media O
communication O
. O
Yet O
, O
being O
out O
- O
liars O
of O
the O
Chinese O
lexicon O
, O
they O
can O
cause O
problems O
to O
existing O
word O
segmentation O
/ O
new O
word O
extraction O
tools O
that O
are O
trained O
on O
traditional O
words O
( O
Chen O
and O
Liu O
, O
1992 O
; O
Xue O
and O
Shen O
, O
2003 O
) O
. O
Consider O
the O
following O
example O
: O
E1 O
: O
PO‰∏ª O
‰∏ª O
‰∏ª‰πü‰∏çÁü•ÈÅìÈìæÊé•Ë¢´Âêû‰∫Ü O
( O
The O
post O
owner O
did O
n‚Äôt O
know O
that O
the O
link O
has O
been O
hacked O
off O
) O
Seg O
: O
PO O
/ O
‰∏ª O
‰∏ª O
‰∏ª O
/ O
‰πü O
/ O
‰∏ç O
/ O
Áü•ÈÅì O
/ O
ÈìæÊé• O
/ O
Ë¢´ O
/ O
Âêû O
/ O
‰∫Ü O
Golden O
Seg O
: O
PO‰∏ª O
‰∏ª O
‰∏ª O
/ O
‰πü O
/ O
‰∏ç O
/ O
Áü•ÈÅì O
/ O
ÈìæÊé• O
/ O
Ë¢´ O
/ O
Âêû O
/ O
‰∫Ü O
The O
sentence O
in O
E1 O
( O
example O
1 O
) O
is O
segmented O
using O
Stanford O
Parser O
( O
Manning O
et O
al O
. O
, O
2014 O
) O
which O
fails O
to O
identify O
the O
word O
‚Äú O
PO O
‰∏ª‚Äù,post O
owner O
and O
breaks O
it O
into O
two O
parts O
. O
The O
same O
type O
of O
error O
also O
occurs O
in O
other O
popular O
segmentation O
tools O
. O
Although O
Huang O
et O
al O
. O
( O
2007 O
) O
proposed O
a O
radical O
method O
of O
word O
segmentation O
to O
meet O
the O
challenge O
, O
using O
a O
concept O
of O
classifying O
a O
string O
of O
character O
- O
boundaries O
into O
either O
word O
- O
boundaries O
or O
non O
- O
word O
- O
boundaries O
, O
their O
work O
did O
not O
address O
the O
cases O
of O
code O
- O
mixing O
words O
, O
whose O
word O
boundaries O
can O
also O
fall O
on O
foreigner O
alphabets O
. O
Some O
other O
methods O
mainly O
rely O
on O
unsupervised O
methods O
( O
Chang O
and O
Su O
, O
1997 O
) O
or O
simple O
statistical O
methods O
based O
on O
N O
- O
gram O
frequencies O
, O
with O
indices O
of O
collocation O
and O
co O
- O
occurrence O
( O
Chang833and O
Su O
, O
1997 O
; O
Chen O
and O
Ma O
, O
2002 O
; O
Dias O
, O
2003 O
) O
. O
However O
, O
these O
works O
are O
mainly O
designed O
for O
new O
words O
of O
pure O
Chinese O
characters O
, O
which O
are O
not O
applicable O
to O
MAWs O
. O
In O
this O
paper O
, O
we O
address O
the O
issue O
of O
MAW O
identiÔ¨Åcation O
and O
present O
the O
construction O
of O
theSina O
MA O
W O
lexicon O
( O
SMA O
W O
) O
( O
available O
at O
https://github.com/Christainx/SMAW O
) O
using O
a O
fully O
automatic O
information O
extraction O
technique O
. O
The O
quality O
of O
the O
MAWs O
( O
accurateness O
and O
inter O
- O
rater O
agreement O
) O
are O
rated O
by O
three O
experts O
for O
system O
evaluation O
. O
Compared O
to O
previous O
resources O
, O
this O
dataset O
provides O
an O
unprecedentedly O
large O
, O
balanced O
, O
and O
structured O
MAWs O
as O
well O
as O
a O
MAW O
annotated O
corpus O
. O
With O
the O
availability O
of O
a O
comprehensive O
MAWs O
as O
a O
valuable O
Chinese O
lexical O
resource O
as O
well O
as O
corpus O
resource O
, O
it O
shall O
beneÔ¨Åt O
many O
Chinese O
language O
processing O
tasks O
which O
need O
to O
deal O
with O
code O
- O
mixing O
, O
such O
as O
word O
segmentation O
and O
information O
extraction O
. O
2 O
Related O
Works O
The O
earliest O
MAW O
was O
probably O
‚Äú O
X O
Â∞ÑÁ∫ø O
/ O
XÂÖâ‚Äù,X O
- O
ray O
, O
which O
was O
ofÔ¨Åcially O
documented O
in O
1903 O
( O
Zhang O
, O
2005 O
) O
. O
For O
over O
60 O
years O
, O
such O
words O
had O
been O
largely O
conÔ¨Åned O
to O
technical O
and O
medical O
domains O
with O
very O
few O
lexicalized O
and O
registered O
terms O
in O
dictionaries O
. O
The O
authoritative O
Xiandai O
Hanyu O
Cidian O
/ O
XianHan O
( O
‚Äú O
Áé∞‰ª£Ê±âËØ≠ËØçÂÖ∏ O
‚Äù O
) O
, O
for O
instance O
, O
initiated O
a O
separate O
section O
to O
include O
39 O
MAW O
entries O
in O
1996 O
. O
This O
list O
has O
grown O
rapidly O
with O
each O
subsequent O
XianHan O
dictionary O
edition O
, O
reaching O
239 O
entries O
by O
the O
2012 O
edition O
. O
This O
in O
turn O
generated O
a O
Ô¨Çurry O
of O
related O
linguistic O
studies O
, O
which O
were O
mainly O
focused O
on O
lexicological O
and O
language O
policy O
issues O
( O
Su O
and O
Wu O
, O
2013 O
; O
Zhang O
, O
2013 O
) O
. O
Some O
works O
have O
dealt O
with O
the O
emergence O
of O
MAWs O
in O
light O
of O
globalization O
, O
placing O
them O
in O
a O
socio O
- O
cultural O
context O
( O
Kozha O
, O
2012 O
; O
Miao O
, O
2005 O
) O
, O
and O
a O
few O
are O
also O
interested O
in O
studying O
the O
morpho O
- O
lexical O
status O
of O
MAWs O
( O
Lun O
, O
2013 O
; O
Riha O
and O
Baker O
, O
2010 O
; O
Riha O
, O
2010 O
) O
. O
In O
the O
age O
of O
Internet O
and O
social O
media O
, O
the O
scale O
of O
MAWs O
, O
their O
extraction O
methods O
, O
and O
resources O
of O
MAWs O
have O
changed O
drastically O
since O
the O
last O
decade O
. O
For O
example O
, O
Zheng O
et O
al O
. O
( O
2005 O
) O
extracted O
a O
small O
set O
of O
MAWs O
with O
manual O
validation O
from O
the O
corpus O
of O
People O
‚Äôs O
Daily O
( O
Year O
2002 O
) O
. O
Jiang O
and O
Dang O
( O
2007 O
) O
extracted O
93 O
MAWs O
( O
out O
of O
1,053 O
new O
domain O
- O
speciÔ¨Åc O
terms O
) O
using O
a O
statistical O
approach O
with O
rule O
- O
based O
validation O
. O
Recently O
, O
Huangand O
Liu O
( O
2017 O
) O
extracted O
over O
1,157 O
MAWs O
from O
both O
the O
Sinica O
Corpus O
( O
Chen O
et O
al O
. O
, O
1996 O
) O
and O
the O
Chinese O
Gigaword O
Corpus O
( O
Huang O
, O
2009 O
) O
based O
on O
manually O
segmented O
MAWs O
in O
the O
corpora O
. O
Although O
they O
have O
extracted O
60,000 O
tokens O
with O
alphabetical O
letters O
. O
However O
, O
the O
list O
mainly O
includes O
pure O
alphabets O
those O
are O
indeed O
switching O
codes O
of O
other O
languages O
. O
In O
our O
study O
, O
these O
pure O
code O
- O
switching O
words O
are O
excluded O
according O
to O
our O
deÔ¨Ånition O
. O
Their O
work O
has O
established O
a O
taxonomy O
of O
distributional O
patterns O
of O
alphabetical O
letters O
in O
MAWs O
and O
found O
that O
typical O
MAWs O
follow O
Chinese O
modiÔ¨Åer O
- O
modiÔ¨Åed O
( O
head O
) O
morphological O
rule O
and O
the O
most O
frequent O
and O
productive O
pattern O
is O
alphabetical O
letter+ O
mandarin O
character O
( O
AC O
) O
, O
such O
astype O
B O
in O
the O
form O
of O
‚Äú O
B O
Âûã O
‚Äù O
. O
Besides O
the O
above O
investigations O
, O
MAWs O
have O
not O
been O
identiÔ¨Åed O
in O
a O
systemic O
and O
automatic O
way O
. O
The O
problem O
of O
identifying O
MAWs O
can O
be O
generalized O
as O
an O
issue O
of O
new O
/ O
unknown O
/ O
out O
- O
ofvocabulary O
word O
extraction O
( O
code O
- O
mixing O
Chinese O
words O
in O
particular O
) O
( O
Chen O
and O
Ma O
, O
2002 O
; O
Zhang O
et O
al O
. O
, O
2010 O
) O
. O
A O
commonly O
adopted O
way O
of O
identifying O
a O
new O
word O
usually O
rely O
on O
word O
segmentation O
at O
the O
Ô¨Årst O
step O
and O
then O
map O
the O
valid O
MAWs O
to O
an O
existing O
dictionary O
. O
Those O
not O
mapped O
in O
the O
dictionary O
will O
be O
identiÔ¨Åed O
as O
new O
words O
. O
This O
is O
actually O
problematic O
for O
identifying O
MAWs O
( O
cf O
. O
example O
in O
Section O
1 O
) O
. O
In O
addition O
, O
previous O
studies O
mainly O
extract O
MAWs O
from O
manually O
segmented O
newspapers O
in O
pre-1990s O
( O
Huang O
and O
Liu O
, O
2017 O
) O
. O
Hence O
, O
the O
resources O
are O
domain O
- O
constrained O
and O
usage O
- O
outdated O
. O
3 O
Construction O
of O
SMA O
W O
To O
address O
the O
bias O
in O
previous O
works O
, O
we O
propose O
to O
collect O
an O
MAW O
list O
using O
social O
- O
media O
text O
commonly O
commonly O
available O
on O
Sina O
Weibo O
platform O
( O
Weibo O
for O
short O
, O
or O
micro O
- O
blogs O
) O
, O
a O
near O
- O
natural O
context O
. O
Weibo O
is O
one O
of O
the O
most O
popular O
social O
media O
platform O
in O
China O
with O
over O
400 O
million O
active O
users O
on O
monthly O
basis O
. O
This O
platform O
becomes O
the O
enabler O
for O
generating O
tons O
of O
online O
data O
, O
which O
can O
serve O
as O
a O
huge O
Web O
corpus O
. O
The O
raw O
dataset O
crawled O
from O
Weibo O
consists O
of O
over O
226 O
million O
posts O
( O
around O
20 O
gigabytes O
data O
) O
. O
On O
the O
other O
hand O
, O
as O
there O
are O
many O
debates O
among O
linguists O
about O
the O
deÔ¨Ånition O
of O
a O
MAW O
( O
Ding O
et O
al O
. O
, O
2017 O
; O
Liu O
, O
1994 O
; O
Tan O
et O
al O
. O
, O
2005 O
; O
Xue O
, O
2007 O
; O
Liu O
, O
2002 O
) O
, O
this O
work O
uses O
a O
datadriven O
statistical O
approach O
as O
well O
as O
leveraging834on O
search O
engine O
hits O
to O
exclude O
pseudo O
- O
MAWs O
of O
low O
- O
vitality O
. O
Details O
of O
the O
methodology O
are O
given O
in O
the O
next O
section O
. O
Figure O
1 O
: O
The O
framework O
of O
SMAW O
construction O
Figure O
1 O
depicts O
the O
framework O
of O
SMAW O
construction O
. O
Collecting O
the O
SMAW O
dataset O
is O
carried O
out O
through O
a O
two O
stage O
process O
: O
Candidate O
Extraction O
andCandidate O
Filtering O
. O
In O
our O
system O
, O
Candidate O
Extraction O
uses O
an O
alphabet O
- O
anchored O
brute O
- O
force O
extraction O
of O
N O
- O
grams O
tokens O
which O
contains O
both O
alphabets O
and O
Chinese O
. O
To O
eliminate O
as O
many O
false O
positive O
cases O
as O
possible O
, O
Candidate O
Filtering O
uses O
three O
methods O
to O
remove O
noisy O
candidates O
using O
( O
1 O
) O
Rule O
- O
based O
ReÔ¨Ånement O
, O
( O
2 O
) O
Informatics O
- O
based O
Elimination O
, O
as O
well O
as O
( O
3 O
) O
Search O
Engine O
Validation O
. O
In O
rule O
- O
based O
reÔ¨Ånement O
, O
a O
number O
of O
rules O
are O
selected O
as O
preliminary O
reÔ¨Ånement O
for O
Candidate O
Filtering O
. O
These O
rules O
are O
easy O
implemented O
and O
fast O
in O
execution O
. O
Then O
, O
in O
informatics O
- O
based O
elimination O
, O
PMI O
( O
Point O
- O
wise O
Mutual O
Information O
) O
and O
entropy O
are O
calculated O
to O
select O
candidates O
of O
high O
co O
- O
occurrence O
rate O
and O
informative O
Ô¨Çexibility O
. O
Using O
informatics O
- O
based O
methods O
can O
greatly O
help O
narrow O
down O
the O
scope O
of O
MAW O
candidates O
and O
remove O
false O
positive O
cases O
. O
Lastly O
, O
search O
engine O
based O
validation O
is O
adopted O
to O
Ô¨Ålter O
out O
low O
- O
vitality O
terms O
based O
on O
user O
links O
. O
This O
intellectual O
agent O
provide O
use O
cases O
about O
a O
candidate O
word O
as O
extra O
evidence O
. O
Details O
of O
these O
steps O
are O
described O
in O
the O
following O
subsections.3.1 O
Rule O
- O
based O
ReÔ¨Ånement O
Brute O
- O
force O
based O
Candidate O
Extraction O
can O
ensure O
highest O
recall O
. O
Yet O
, O
it O
can O
create O
a O
substantial O
list O
of O
false O
positive O
candidates O
, O
such O
as O
the O
subcomponent O
of O
a O
positive O
case O
: O
‚Äú O
Âï¶AÊ¢¶ O
‚Äù O
, O
whose O
correct O
MAW O
should O
be O
‚Äú O
ÂìÜÂï¶AÊ¢¶‚Äù,Doraemon O
; O
and O
the O
under O
segmented O
token O
: O
‚Äú O
A O
ËÇ° O
/ O
ÂèçÂºπ‚Äù,rally O
of O
Shanghai O
SE O
Composite O
Index O
, O
although O
the O
correct O
MAW O
should O
be O
‚Äú O
A O
ËÇ°‚Äù,Shanghai O
SE O
Composite O
Index O
, O
etc O
. O
Below O
is O
a O
typical O
example O
of O
a O
user O
post O
in O
this O
dataset O
which O
includes O
a O
number O
of O
web O
- O
speciÔ¨Åc O
linguistic O
usages O
. O
E2 O
: O
# O
BMWËµõËΩ¶Á∫™ÂΩïÁâá O
# O
# O
‰∫öÊ¥≤ÂÖ¨Ë∑ØÊë©ÊâòÈî¶Ê†áËµõÁè†Êµ∑Á´ôÂÖ®ËÆ∞ÂΩï O
# O
@UNIQ O
- O
Áéã‰∏ÄÂçöhttp://t.cn O
/ O
EPdahkI O
( O
# O
BMW O
Racing O
Documentary#Records O
Zhuhai O
( O
in O
Asian O
Highway O
Motorcycle O
Championship O
. O
@AX12FZ32 O
http://t.cn/EPdahkI O
) O
As O
shown O
in O
E2 O
, O
among O
all O
alphabetical O
chunks O
, O
many O
candidates O
are O
URL O
links O
, O
tags O
related O
to O
topics O
( O
surrounded O
by O
# O
) O
, O
or O
user O
names O
( O
introduced O
by O
the O
‚Äú O
@ O
‚Äù O
symbol O
) O
. O
These O
alphabetical O
sequences O
is O
noise O
for O
MAWS O
and O
should O
be O
readily O
excluded O
from O
the O
Ô¨Ånal O
data O
using O
some O
simple O
rules O
. O
other O
false O
MAW O
candidates O
also O
demonstrate O
obvious O
patterns O
. O
For O
example O
, O
candidates O
of O
emoji O
( O
e.g. O
‚Äú O
QAQ O
‚Äù O
, O
‚Äú O
LOL O
‚Äù O
, O
‚Äú O
:P O
‚Äù O
, O
‚Äú O
T_T O
‚Äù O
) O
are O
transformed O
symbols O
that O
encode O
no O
lexical O
meanings O
and O
shall O
be O
eliminated O
from O
the O
MAW O
list O
. O
Using O
a O
set O
of O
9 O
different O
pattern O
- O
based O
rules O
to O
Ô¨Ålter O
out O
these O
unambiguous O
noises O
can O
largely O
reduce O
noisy O
data O
without O
compromising O
the O
coverage O
of O
the O
MAW O
lexicon O
. O
Detailed O
description O
of O
these O
patterns O
shall O
be O
introduced O
in O
Section O
4.1 O
. O
3.2 O
Informatics O
- O
based O
Elimination O
As O
will O
be O
shown O
in O
the O
evaluation O
that O
even O
after O
Rule O
- O
based O
ReÔ¨Ånement O
, O
the O
candidate O
list O
it O
is O
still O
too O
large O
to O
be O
correct O
even O
by O
common O
sense O
. O
Informatics O
- O
based O
elimination O
works O
on O
this O
set O
of O
candidates O
to O
further O
remove O
noise O
. O
Term O
- O
frequency O
( O
TF O
) O
is O
a O
commonly O
used O
metric O
to O
Ô¨Ålter O
out O
low O
- O
occurrence O
candidates O
. O
However O
, O
using O
TF O
alone O
is O
insufÔ¨Åcient O
to O
identify O
MAWs O
. O
For O
instance O
, O
both O
‚Äú O
A O
ËÇ°‚Äù,Shanghai O
SE O
Composite O
Index O
and O
‚Äú O
AËÇ° O
/ O
ÂèçÂºπ‚Äù,rally O
of O
Shanghai O
SE O
Composite O
Index O
have O
high O
TF O
but O
only O
‚Äú O
A O
ËÇ° O
‚Äù O
is O
a O
valid O
MAW O
. O
In O
this O
work O
, O
informatics O
- O
based O
methods O
are O
used O
to O
automatically O
Ô¨Ålter O
the O
negative O
cases O
, O
including O
PMI O
for O
measuring O
the O
internal O
cohesion,835and O
entropy O
for O
measuring O
the O
external O
uncertainty O
of O
the O
candidates O
. O
Point O
- O
wise O
mutual O
information O
( O
PMI O
) O
is O
proposed O
by O
Bouma O
( O
2009 O
) O
to O
measure O
the O
cooccurrence O
probability O
of O
two O
variables O
. O
It O
is O
used O
to O
measure O
the O
internal O
‚Äú O
Ô¨Åxedness O
‚Äù O
of O
a O
word O
. O
Let O
wbe O
an O
MAW O
candidate O
that O
consists O
of O
two O
componentsc1,c2 O
. O
The O
PMI O
of O
wwith O
respect O
to O
c1and O
c2can O
be O
calculated O
via O
Formula O
1 O
given O
below O
. O
PMI O
( O
c1;c2 O
) O
= O
‚àílog(p(c1 O
, O
c2 O
) O
p(c1)‚àóp(c2 O
) O
) O
( O
1 O
) O
In O
practice O
, O
at O
least O
one O
component O
, O
denoted O
as O
camust O
contain O
alphabet O
character(s O
) O
. O
If O
wconsists O
of O
more O
than O
three O
components O
, O
we O
use O
the O
combination O
coordinated O
by O
ca O
. O
For O
example O
, O
‚Äú O
ÂìÜ O
Âï¶ O
/ O
A O
/ O
Ê¢¶‚ÄùDoraemon O
can O
be O
computed O
by O
using O
‚Äú O
ÂìÜÂï¶A O
/ O
Ê¢¶ O
‚Äù O
and O
‚Äú O
ÂìÜÂï¶ O
/ O
AÊ¢¶ O
‚Äù O
. O
Formula O
1 O
can O
be O
extended O
to O
Formula O
2 O
to O
handle O
three O
components O
. O
PMI O
( O
w O
) O
= O
min O
( O
PMI O
( O
c1;ca O
) O
, O
PMI O
( O
ca;c2 O
) O
) O
( O
2 O
) O
The O
threshold O
of O
PMI O
is O
experimentally O
set O
. O
Another O
dimension O
for O
identifying O
word O
boundaries O
is O
to O
use O
information O
entropy O
of O
its O
collocation O
environment O
. O
As O
proposed O
by O
He O
and O
Jun O
- O
Fang O
( O
2006 O
) O
, O
information O
entropy O
can O
be O
used O
to O
measure O
the O
uncertainty O
( O
Ô¨Çexibility O
) O
of O
a O
candidate O
‚Äôs O
environment O
, O
the O
larger O
the O
more O
Ô¨Çexible O
, O
and O
the O
more O
likely O
the O
candidate O
being O
a O
word O
. O
Consider O
the O
negative O
case O
of O
‚Äú O
Á¥†C O
‚Äù O
which O
only O
occurs O
in O
the O
context O
of O
‚Äú O
Áª¥ÁîüÁ¥†C‚Äù,Vitamin O
C O
( O
entropy O
in O
this O
case O
is O
low O
) O
. O
In O
contrast O
, O
the O
positive O
case O
‚Äú O
Áª¥ÁîüÁ¥†C O
‚Äù O
occur O
in O
many O
different O
contexts O
: O
‚Äú O
Ë°•ÂÖÖ O
/ O
Áª¥ÁîüÁ¥†C O
‚Äù O
, O
Take O
Vitamin O
C O
, O
‚Äú O
È´òÂâÇÈáè O
/ O
Áª¥ÁîüÁ¥†C‚Äù,High O
- O
dosage O
Vitamin O
C O
, O
‚Äú O
Áª¥ÁîüÁ¥†C O
/ O
ÂØπ O
/ O
ÊÑüÂÜí O
/ O
ÊúâÊïà‚Äù,Vitamin O
C O
copes O
with O
colds O
, O
etc O
. O
( O
entropy O
in O
this O
case O
is O
high O
) O
. O
Letchandctbe O
the O
respective O
head O
and O
tail O
components O
surrounding O
w. O
The O
head O
entropy O
of O
w O
, O
denoted O
byH(h O
) O
, O
is O
deÔ¨Åned O
by O
Formula O
3 O
. O
The O
tail O
entropyH(t)can O
be O
obtained O
similarly O
. O
Based O
on O
Formula O
3 O
, O
the O
Ô¨Ånal O
entropy O
of O
wis O
obtained O
by O
min O
( O
H(h),H(t O
) O
) O
. O
H(h O
) O
= O
‚àí/summationdisplay O
p(ch)i‚àólog(p(ch)i O
) O
( O
3 O
) O
3.3 O
Search O
Engine O
Validation O
Search O
Engine O
Validation O
aims O
to O
further O
Ô¨Ålter O
out O
candidate O
MAWs O
which O
are O
either O
less O
frequently O
used O
or O
in O
proper O
word O
forms O
that O
are O
not O
necessarily O
meaningful O
as O
lexical O
terms O
. O
A O
search O
engine O
such O
as O
Google O
, O
Bing O
and O
Baidu O
provide O
access O
toa O
large O
knowledge O
base O
to O
validate O
the O
semantic O
information O
of O
a O
MAW O
candidate O
. O
Active O
MAW O
candidates O
with O
more O
links O
are O
more O
likely O
to O
carry O
proper O
semantic O
meanings O
. O
semantic O
information O
can O
help O
to O
exclude O
non O
- O
lexicon O
candidates O
. O
For O
instance O
, O
" O
UNIQ- O
Áéã‰∏ÄÂçö O
" O
, O
refers O
to O
Wang O
Yi O
Bo O
, O
a O
famous O
Chinese O
actor O
in O
the O
band O
" O
UNIQ O
" O
. O
The O
features O
of O
this O
false O
candidate O
can O
pass O
previous O
Ô¨Åltering O
methods O
perfectly O
. O
This O
indicates O
the O
need O
for O
a O
more O
intelligent O
validation O
scheme O
. O
As O
the O
data O
source O
in O
this O
work O
is O
Sina O
Weibo O
, O
it O
is O
more O
appropriate O
to O
use O
Baidu O
, O
the O
most O
popular O
search O
engine O
in O
China O
, O
as O
the O
knowledge O
agent O
for O
retrieving O
the O
validation O
evidence O
of O
the O
remaining O
candidates O
. O
Figure O
2 O
is O
the O
Ô¨Çowchart O
of O
Search O
Engine O
Validation O
module O
. O
Figure O
2 O
: O
Flowchart O
of O
Search O
Engine O
Validation O
Let O
us O
examine O
a O
user O
name O
as O
an O
example O
. O
‚Äú O
Êùé O
Ê¥ãÊ¥ã O
kelly O
‚Äù O
, O
" O
Yangyang O
Li O
, O
Kelly O
" O
is O
a O
username O
combined O
with O
a O
Chinese O
name O
and O
an O
English O
nickname O
) O
. O
The O
top O
Nlinks O
are O
Ô¨Årst O
collected O
as O
external O
evidence O
. O
The O
linked O
text O
is O
then O
cleaned O
and O
parsed O
to O
check O
whether O
this O
MAW O
candidate O
is O
meaningful O
. O
In O
the O
case O
of O
‚Äú O
ÊùéÊ¥ãÊ¥ã O
kelly O
‚Äù O
occurs O
only O
as O
‚Äú O
@ O
ÊùéÊ¥ãÊ¥ã O
kelly O
‚Äù O
. O
Thus O
, O
it O
is O
validated O
as O
a O
username O
, O
not O
a O
real O
MAW O
. O
In O
addition O
to O
username O
checking O
, O
stickers O
and O
in O
sufÔ¨Åcient O
occurrences O
are O
also O
used O
as O
indication O
of O
invalid O
MAWs O
. O
4 O
Results O
and O
Evaluation O
In O
our O
system O
, O
every O
Ô¨Åltering O
method O
is O
executed O
sequentially O
. O
Due O
to O
length O
limitation O
of O
this O
paper O
, O
we O
are O
giving O
the O
Ô¨Ånal O
selected O
parameters O
of O
our O
modules O
without O
showing O
the O
tuning O
process O
. O
The O
N O
- O
gram O
token O
window O
size O
of O
bruteforce O
method O
in O
Candidate O
Extraction O
is O
set O
to O
5 O
because O
most O
new O
terms O
are O
not O
longer O
than O
5 O
as O
a O
common O
practice O
. O
In O
Candidate O
Filtering O
, O
836LEN O
_ O
THRES O
andFREQ O
_ O
THRES O
( O
detailed O
in O
Table O
2 O
) O
in O
rule O
- O
based O
reÔ¨Ånement O
are O
tuned O
to O
15 O
and O
3 O
, O
respectively O
. O
The O
upper O
bound O
of O
PMI O
and O
entropy O
in O
informatics O
- O
based O
elimination O
are O
experimentally O
set O
to O
-16.2 O
and O
0.2 O
, O
respectively O
. O
In O
search O
engine O
validation O
, O
we O
use O
the O
top O
10 O
links O
as O
external O
evidence O
. O
If O
the O
number O
of O
valid O
links O
is O
less O
than O
5 O
, O
the O
corresponding O
MAW O
candidate O
is O
Ô¨Åltered O
out O
. O
4.1 O
Evaluation O
of O
SMA O
W O
This O
section O
gives O
an O
estimate O
on O
the O
quality O
of O
SMAW O
in O
terms O
of O
Accuracy O
, O
Candidate O
Size O
and O
Inter O
- O
rater O
agreement O
through O
evaluation O
by O
human O
raters O
. O
As O
MAWs O
demonstrate O
a O
dynamic O
role O
in O
the O
Chinese O
lexicon O
, O
it O
is O
infeasible O
to O
refer O
to O
a O
full O
reference O
set O
for O
calculating O
Recall O
and O
Precision O
. O
That O
is O
the O
reason O
accuracy O
is O
used O
to O
measure O
quality O
of O
SMAW O
. O
In O
the O
evaluation O
, O
three O
groups O
of O
SMAWs O
( O
100 O
each O
group O
, O
300 O
in O
total O
) O
are O
randomly O
sampled O
from O
each O
step O
for O
the O
participants O
to O
judge O
the O
acceptance O
of O
the O
candidates O
. O
Raters O
are O
asked O
to O
make O
judgements O
and O
give O
1 O
if O
they O
think O
a O
candidate O
is O
a O
MAW O
, O
or O
0 O
otherwise O
. O
Then O
, O
Accuracy O
( O
Acc O
. O
) O
is O
calculated O
as O
the O
average O
of O
the O
three O
groups O
‚Äô O
acceptance O
rates O
. O
Incrementally O
, O
the O
Candidate O
Size O
( O
Size O
. O
) O
is O
also O
studied O
for O
each O
Ô¨Åltering O
method O
. O
Inter O
- O
rater O
agreement O
among O
the O
three O
raters O
is O
also O
measured O
using O
Cohen O
‚Äôs O
Kappa O
CoefÔ¨Åcient O
( O
K. O
) O
( O
Kraemer O
, O
2014 O
) O
. O
The O
evaluation O
results O
are O
given O
in O
Table O
1 O
. O
Step O
Method O
Acc O
. O
K. O
Size O
. O
1 O
BF O
NA O
.56 O
25,594k O
2 O
+ O
Rule O
- O
based O
.22 O
.58 O
1,470k O
3 O
+ O
PMI O
.62 O
.65 O
592k O
4 O
+ O
Entropy O
.77 O
.70 O
32k O
5 O
+ O
Baidu O
.82 O
.78 O
16k O
B0 O
TF+Max O
. O
.15 O
.59 O
1,935k O
Table O
1 O
: O
The O
Evaluation O
Results O
Staring O
from O
Brute O
- O
force O
, O
referred O
as O
BF O
, O
Table O
1 O
summarizes O
the O
accumulative O
performances O
of O
using O
various O
metrics O
for O
candidate O
selection O
after O
each O
step O
. O
B0 O
is O
a O
baseline O
method O
that O
simply O
employs O
term O
frequency O
and O
the O
maximal O
sequence O
principle O
. O
For O
example O
, O
the O
maximal O
sequence O
principle O
will O
select O
‚Äú O
ÂìÜÂï¶AÊ¢¶‚Äù,Doraemon O
over O
components O
‚Äú O
Âï¶AÊ¢¶ O
‚Äù O
or O
‚Äú O
AÊ¢¶ O
‚Äù O
. O
However O
, O
B0 O
ismore O
error O
- O
prone O
, O
For O
example O
, O
in O
‚Äú O
ÂÆâÂÖ® O
/ O
‰ΩøÁî® O
/ O
ÂÖç O
Ë¥π O
/ O
WiFi O
‚Äù O
, O
Safely O
use O
free O
wiÔ¨Å O
where O
‚Äú O
ÂÖçË¥πWiFi O
‚Äù O
, O
free O
wiÔ¨Å O
shall O
be O
a O
positive O
instance O
. O
In O
general O
, O
the O
accuracy O
increases O
when O
more O
Ô¨Åltering O
methods O
applied O
. O
It O
is O
worth O
mentioning O
that O
the O
accuracy O
shows O
a O
great O
boosting O
after O
using O
PMI O
and O
entropy O
, O
indicating O
the O
usefulness O
of O
informatics O
- O
based O
metrics O
for O
word O
identiÔ¨Åcation O
. O
In O
addition O
, O
the O
incremental O
K.of O
each O
phase O
suggests O
the O
increased O
agreement O
methods O
the O
three O
raters O
by O
adopting O
the O
several O
metrics O
, O
especially O
after O
the O
Baidu O
search O
engine O
validation O
. O
Compared O
with O
baseline O
method O
, O
our O
system O
makes O
use O
of O
a O
more O
reliable O
extraction O
approach O
that O
is O
obviously O
more O
effective O
for O
the O
identiÔ¨Åcation O
of O
alphabetical O
words O
( O
Acc O
. O
= O
0.82 O
, O
K.= O
0.78 O
) O
. O
The O
high O
accuracy O
score O
and O
agreement O
in O
the O
evaluation O
has O
proven O
the O
effectiveness O
of O
the O
extraction O
method O
, O
as O
well O
as O
demonstrating O
a O
good O
quality O
of O
the O
lexicon O
. O
As O
for O
the O
candidate O
size O
, O
it O
can O
be O
observed O
that O
the O
candidate O
size O
drastically O
decreases O
after O
Ô¨Åltering O
methods O
. O
The O
total O
number O
of O
tokens O
obtained O
after O
brute O
- O
force O
candidate O
extraction O
reaches O
25,594 O
K O
, O
obviously O
too O
large O
and O
too O
noisy O
for O
direct O
use O
. O
After O
Rule O
- O
based O
ReÔ¨Ånement O
, O
a O
set O
of O
1,470k O
potential O
MAW O
candidates O
is O
obtained O
, O
only O
5.7 O
% O
of O
complete O
candidate O
collection O
. O
To O
provide O
more O
detail O
of O
rule O
- O
based O
reÔ¨Ånement O
, O
Table O
2 O
shows O
the O
process O
of O
constructing O
SMAW O
list O
of O
patterns O
used O
and O
the O
information O
on O
the O
reduction O
in O
data O
sizes O
. O
By O
using O
PMI O
and O
entropy O
, O
878k O
and O
560k O
invalid O
MAW O
candidates O
are O
eliminated O
, O
respectively O
. O
The O
97.8 O
% O
reduction O
further O
narrow O
down O
the O
candidate O
set O
, O
only O
33k O
candidates O
remain O
in O
the O
list O
. O
After O
processing O
this O
list O
based O
on O
search O
engine O
validation O
, O
the O
Ô¨Ånal O
collection O
of O
SMAWS O
has O
16,207 O
tokens O
. O
4.2 O
The O
Lexical O
Characteristics O
This O
section O
analyses O
the O
lexical O
properties O
of O
the O
SMAW O
lexicon O
. O
Comparisons O
between O
the O
SMAW O
list O
( O
‚Äú O
Web O
‚Äù O
hereinafter O
) O
and O
the O
MAWs O
in O
Huang O
and O
Liu O
( O
2017 O
) O
( O
‚Äú O
Giga O
‚Äù O
hereinafter O
) O
will O
be O
made O
in O
terms O
of O
key O
vocabulary O
, O
length O
distribution O
, O
word O
formation O
types O
and O
lexical O
diversity O
so O
as O
to O
highlight O
the O
lexical O
differences O
of O
MAWs O
between O
social O
media O
and O
newspaper O
as O
well O
as O
the O
lexical O
development O
of O
alphabetical O
words O
in O
the O
recent O
two O
decades.837Rule O
Description O
Quantity O
NONE O
brute O
force O
candidates O
collection O
25,594k O
Topic O
remove O
candidates O
with O
‚Äô O
# O
‚Äô O
165k O
Username O
remove O
candidates O
with O
‚Äô O
@ O
‚Äô O
297k O
No O
Chinese O
remove O
candidates O
without O
Chinese O
character O
1,302k O
Too O
Short O
Length O
remove O
candidates O
less O
than O
LEN_THRES O
characters O
595k O
Rare O
Occurrence O
remove O
candidates O
which O
count O
less O
than O
FREQ_THRES O
18,443k O
English O
Expression O
remove O
candidates O
contain O
two O
or O
more O
English O
words O
1,421k O
Symbol O
remove O
candidates O
contain O
symbols O
such O
as O
‚Äô O
& O
‚Äô O
and O
‚Äô O
* O
‚Äô O
419k O
Emoji O
remove O
candidates O
contain O
emoji O
such O
as O
" O
XDD O
" O
193k O
POS O
tag O
remove O
candidates O
with O
invalid O
POS O
tag O
such O
as O
‚Äô O
DET O
‚Äô O
1k O
ALL O
RULES O
Remains O
after O
using O
all O
rule O
- O
based O
reÔ¨Ånement O
1,470k O
Table O
2 O
: O
Noise O
Reduction O
Statistics O
by O
Rule O
- O
based O
ReÔ¨Ånement O
. O
4.2.1 O
Vocabulary O
Figure O
3 O
visualizes O
the O
top O
50 O
MAW O
vocabularies O
of O
the O
two O
lexicons O
. O
The O
sizes O
of O
the O
words O
reÔ¨Çect O
its O
usage O
frequency O
. O
It O
can O
be O
observed O
that O
the O
most O
frequent O
MAW O
in O
the O
Giga O
list O
is O
‚Äú O
B O
Âûã O
‚Äù O
( O
B O
- O
type O
) O
, O
while O
in O
the O
Web O
list O
, O
the O
most O
frequent O
MAW O
is O
‚Äú O
HOLD O
‰Ωè O
‚Äù O
( O
To O
endure O
) O
, O
which O
is O
a O
typical O
Internet O
neology O
. O
Moreover O
, O
most O
MAWs O
in O
Giga O
are O
disyllabic O
, O
e.g. O
‚Äú O
AÂûã O
‚Äù O
( O
A O
- O
type O
) O
and O
‚Äú O
A O
Á∫ß‚Äù(A O
- O
level O
) O
, O
while O
SMAWs O
tend O
to O
be O
more O
lengthy O
, O
containing O
words O
of O
a O
wider O
range O
of O
syllables O
( O
e.g. O
‚Äú O
NBA O
ÂÖ®ÊòéÊòü O
‚Äù O
( O
NBA O
all O
- O
star O
) O
) O
. O
SpeciÔ¨Åcally O
, O
MAWs O
in O
Giga O
show O
a O
dominant O
( O
rigid O
) O
pattern O
of O
‚Äú O
X O
Á±ª O
/ O
Âûã O
‚Äù O
( O
Type O
- O
X O
) O
. O
However O
, O
in O
Web O
, O
MAWs O
has O
more O
Part O
- O
of O
- O
Speech O
diversity O
, O
including O
verbs O
( O
e.g. O
‚Äú O
Hold O
‰Ωè O
‚Äù O
) O
, O
nouns O
( O
e.g. O
‚Äú O
BB O
Èúú O
‚Äù O
( O
BB O
cream O
) O
) O
, O
or O
adjectives O
( O
e.g. O
‚Äú O
ÁâõX O
‚Äù O
( O
incredibly O
awesome O
) O
) O
, O
indicating O
the O
trend O
of O
MAWs O
accounting O
for O
different O
grammatical O
roles O
in O
the O
Chinese O
language O
. O
Lastly O
, O
the O
lexical O
senses O
of O
Giga O
MAWs O
are O
more O
concentrated O
to O
the O
" O
type O
/ O
classiÔ¨Åcation O
" O
meaning O
, O
while O
MAWs O
in O
Web O
encode O
a O
wider O
range O
of O
meanings O
, O
including O
name O
entities O
, O
swear O
words O
, O
economics O
, O
entertainment O
, O
etc O
. O
The O
above O
keyword O
differences O
reÔ¨Çect O
a O
dramatic O
change O
of O
MAWs O
at O
syllabic O
, O
lexical O
, O
grammatical O
and O
semantic O
levels O
in O
recent O
decades O
. O
4.2.2 O
Length O
Distribution O
The O
box O
- O
plots O
in O
Figure O
4 O
give O
an O
overview O
of O
the O
length O
distribution O
of O
MAWs O
in O
Giga O
( O
Huang O
and O
Liu O
, O
2017 O
) O
and O
Web O
( O
SMAW O
) O
. O
As O
shown O
in O
Figure O
4 O
, O
MAWs O
in O
Web O
are O
much O
longer O
and O
more O
scattered O
than O
that O
in O
Giga O
. O
The O
mean O
length O
of O
MAWs O
in O
Giga O
is O
2 O
- O
3 O
. O
But O
, O
the O
Figure O
3 O
: O
Word O
clouds O
of O
MAWs O
in O
Web O
and O
Giga838Figure O
4 O
: O
Length O
distribution O
of O
MAWs O
in O
Giga O
and O
Web O
mean O
length O
in O
SNAW O
is O
around O
5 O
. O
Overall O
, O
the O
MAWs O
in O
Web O
are O
distributed O
across O
a O
wider O
span O
. O
This O
may O
imply O
a O
tendency O
of O
code O
- O
mixing O
words O
being O
longer O
and O
richer O
in O
Modern O
Chinese O
. O
4.2.3 O
Word O
Formation O
In O
line O
with O
the O
work O
of O
Huang O
and O
Liu O
( O
2017 O
) O
, O
word O
formation O
of O
MAWs O
is O
classiÔ¨Åed O
into O
four O
major O
types O
according O
to O
the O
positions O
of O
the O
A O
( O
alphabet O
) O
and O
C O
( O
character O
) O
, O
including O
AC O
( O
e.g. O
‚Äú O
x O
- O
ÂÖâ O
‚Äù O
) O
, O
CA O
( O
e.g. O
‚Äú O
Áâõb O
‚Äù O
) O
, O
CAC O
( O
e.g. O
‚Äú O
Á®ãIÈùí O
‚Äù O
( O
a O
Chinese O
Name O
) O
) O
and O
other O
types O
. O
The O
number O
of O
the O
four O
types O
of O
MAWs O
in O
Giga O
and O
Web O
is O
shown O
in O
Table O
3 O
for O
comparison O
. O
AC O
CA O
CAC O
Other O
Total O
Giga O
665 O
283 O
185 O
18 O
1151 O
( O
pct O
) O
57.8 O
% O
24.6 O
% O
16.1 O
% O
1.5 O
% O
100.0 O
% O
Web O
6971 O
6994 O
2242 O
0 O
16207 O
( O
pct O
) O
43.0 O
% O
43.2 O
% O
13.8 O
% O
0.0 O
% O
100.0 O
% O
Table O
3 O
: O
Word O
formation O
comparison O
As O
highlighted O
in O
Table O
3 O
, O
the O
dominant O
type O
in O
Giga O
is O
AC O
, O
while O
CA O
is O
more O
prevalent O
in O
Web O
. O
Huang O
and O
Liu O
( O
2017 O
) O
argued O
that O
the O
dominance O
of O
AC O
type O
with O
the O
modiÔ¨Åer O
- O
modiÔ¨Åed O
compound O
structure O
in O
Chinese O
is O
because O
heads O
of O
nouns O
are O
usually O
right O
positioned O
( O
Sun O
, O
2006 O
) O
. O
However O
, O
MAWs O
in O
Web O
have O
wider O
grammatical O
roles O
and O
more O
verbs O
are O
found O
in O
SMAW O
. O
Contrary O
to O
nouns O
, O
verbs O
are O
left O
headed O
, O
such O
as O
in O
‚Äú O
Êâìcall O
‚Äù O
( O
cheer O
up O
) O
, O
where O
‚Äú O
Êâì O
‚Äù O
( O
beat O
) O
is O
the O
head O
. O
In O
addition O
, O
cases O
like O
‚Äú O
Áª¥c O
‚Äù O
( O
Vitamin O
C O
) O
, O
‚Äú O
Âèåc O
‚Äù O
( O
double O
cores O
) O
, O
and O
‚Äú O
ÊúÄIn O
‚Äù O
( O
Most O
popular O
) O
are O
headed O
on O
alphabetsinstead O
of O
the O
Chinese O
character O
, O
indicating O
that O
heads O
are O
not O
necessarily O
positioned O
at O
the O
Chinese O
characters O
. O
4.2.4 O
Lexical O
Diversity O
TTR O
( O
type O
‚Äì O
token O
ratio O
) O
is O
used O
to O
measure O
the O
lexical O
diversity O
/ O
richness O
of O
a O
language O
( O
Dur√°n O
et O
al O
. O
, O
2004 O
) O
. O
This O
metric O
is O
adopted O
here O
with O
normalized O
data O
( O
STTR O
) O
, O
for O
measuring O
the O
lexical O
diversity O
of O
the O
MAWs O
in O
Giga O
and O
Web O
, O
as O
shown O
in O
Table O
4 O
. O
Data O
STTR O
AC.STTR O
CA.STTR O
Web O
14.53 O
16.9 O
12.3 O
Giga O
8.77 O
7.6 O
15.2 O
Table O
4 O
: O
Lexical O
Diversity O
Comparison O
Table O
4 O
seems O
to O
suggest O
a O
reverse O
relation O
between O
the O
frequency O
of O
the O
MAW O
types O
and O
their O
lexical O
richness O
: O
the O
‚Äú O
AC O
‚Äù O
type O
is O
dominant O
in O
Giga O
, O
but O
it O
demonstrates O
a O
lower O
STTR O
; O
similarly O
, O
the O
‚Äú O
CA O
‚Äù O
type O
is O
dominant O
in O
Web O
, O
and O
it O
also O
shows O
a O
lower O
STTR O
. O
Overall O
, O
the O
Web O
MAWs O
show O
a O
richer O
vocabulary O
compared O
to O
the O
newspaper O
MAWs O
( O
Giga O
) O
, O
indicating O
the O
higher O
productivity O
of O
social O
media O
language O
. O
4.3 O
The O
Corpus O
In O
addition O
to O
the O
SMAW O
lexicon O
, O
we O
have O
also O
retrieved O
more O
than O
200,000 O
sentences O
( O
around O
2,000,000 O
tokens O
) O
for O
the O
16,207 O
SMAW O
( O
each O
SMAW O
contains O
10 O
or O
so O
sentences O
) O
to O
construct O
a O
SMAW O
corpus O
which O
can O
support O
code O
- O
mixing O
words O
inquiries O
. O
‰∏ÄÂÆö(D)Ë¶Å(D O
) O
HOLD‰Ωè O
‰Ωè O
‰Ωè(V O
A O
) O
! O
ÁñØÁãÇ(D)Â∫óÂ∫Ü O
( O
V O
A O
) O
11Â§©(Nd)ÔºåËøò(D)ËÉΩ(D O
) O
HOLD‰Ωè O
‰Ωè O
‰Ωè(V O
A)Âêó(T O
) O
KITTYÊéß(Na)‰ª¨(Na)Ëøò(D O
) O
HOLD‰Ωè O
‰Ωè O
‰Ωè(V O
A)Âêó(T O
) O
ÂæÆÊó∂‰ª£(Na)ÔºåÂ§ß(A)Ë∂ãÂäø(Na)ÔºåÂèØÂæó(VH O
) O
HOLD‰Ωè O
‰Ωè O
‰Ωè(V O
A O
) O
! O
‰∫≤(I)ÔºÅ‰Ω†(Nh)Ë¶Å(D O
) O
HOLD‰Ωè O
‰Ωè O
‰Ωè(V O
A)Âì¶(T O
) O
Â§ßÂÆ∂(Nh O
) O
HOLD‰Ωè O
‰Ωè O
‰Ωè(V O
A)Âì¶(T O
) O
ÂêÑ‰Ωç(Nes)ÁúãÂÆò(Na)Ë¶Å(D O
) O
HOLD‰Ωè O
‰Ωè O
‰Ωè(V O
A)‰∫Ü(Di O
) O
Interface O
1 O
: O
Corpus O
samples O
of O
‚Äú O
HOLD O
‰Ωè O
‚Äù O
( O
KWIC O
) O
The O
characters O
in O
the O
sentences O
are O
all O
transferred O
into O
simpliÔ¨Åed O
Chinese O
for O
consistency O
. O
All O
sentences O
are O
automatically O
segmented O
using O
Stanford O
CoreNLP1(Manning O
et O
al O
. O
, O
2014 O
) O
. O
The O
automatic O
word O
segmentation O
is O
enabled O
as O
the O
alphabetical O
words O
are O
pre O
- O
identiÔ¨Åed O
in O
our O
SMAW O
lexicon O
. O
With O
conÔ¨Årmed O
boundaries O
of O
the O
alphabetical O
1https://stanfordnlp.github.io/ O
CoreNLP/839words O
, O
it O
becomes O
an O
ordinary O
task O
of O
segmenting O
the O
remaining O
Chinese O
characters O
. O
On O
the O
basis O
of O
the O
raw O
sentences O
, O
we O
are O
building O
a O
concordance O
engine O
for O
loading O
the O
content O
of O
the O
corpus O
following O
the O
Chinese O
Word O
Sketch O
schema O
( O
Hong O
and O
Huang O
, O
2006 O
) O
, O
which O
can O
support O
users O
‚Äô O
inquires O
of O
word O
and O
grammatical O
collocations O
of O
code O
- O
mixing O
words O
. O
Samples O
of O
the O
corpus O
are O
shown O
in O
Interface O
1 O
. O
Figure O
5 O
: O
POS O
distribution O
of O
MAWs O
in O
Giga O
and O
Web O
Besides O
, O
the O
corpus O
is O
undergoing O
a O
POS O
tagging O
process O
using O
the O
Academia O
Sinica O
segmentation O
and O
tagging O
system O
( O
Chen O
et O
al O
. O
, O
1996 O
; O
Zhao O
et O
al O
. O
, O
2006 O
) O
in O
order O
to O
support O
grammatical O
inquiries O
of O
linguistic O
accounts O
. O
Tagging O
is O
conducted O
automatically O
with O
manual O
post O
- O
checking O
on O
the O
SMAWs O
. O
The O
precision O
accuracy O
is O
estimated O
to O
be O
over O
85 O
% O
. O
Since O
tagging O
is O
still O
in O
progress O
, O
we O
provide O
the O
POS O
distribution2of O
the O
most O
frequent O
50 O
SMAWs O
to O
show O
a O
general O
view O
of O
the O
grammatical O
distribution O
of O
popular O
SMAWs O
. O
Figure O
5 O
shows O
the O
POS O
distribution O
of O
MAWs O
in O
Web O
and O
Giga O
for O
comparison O
purpose O
. O
2https://catalog.ldc.upenn.edu/LDC2009T14The O
POS O
distribution O
in O
Figure O
5 O
shows O
that O
MAWs O
have O
developed O
a O
more O
salient O
role O
in O
the O
Chinese O
lexicon O
: O
from O
mainly O
nouns O
( O
Na O
, O
Nb O
, O
Nd O
) O
to O
verbs O
( O
V O
A O
, O
VH O
) O
, O
from O
modiÔ¨Åers O
( O
A O
) O
to O
core O
lexical O
components O
( O
heads O
and O
arguments O
) O
, O
and O
the O
graph O
demonstrates O
a O
more O
diversiÔ¨Åed O
lexical O
categories O
( O
more O
divisions O
and O
colorful O
) O
of O
new O
MAWs O
. O
5 O
Conclusion O
and O
Future O
Work O
This O
work O
uses O
social O
media O
platform O
( O
Sina O
Weibo O
) O
and O
search O
engine O
( O
Baidu O
) O
for O
collection O
and O
validation O
of O
code O
- O
mixing O
words O
to O
tackle O
the O
under O
- O
representation O
and O
identiÔ¨Åcation O
problems O
of O
MAWs O
. O
The O
evaluation O
of O
the O
new O
Sina O
MAW O
dataset O
( O
SMAW O
) O
, O
proves O
the O
high O
performance O
( O
Acc O
. O
= O
0.82 O
, O
K.= O
0.78 O
) O
of O
the O
proposed O
extraction O
method O
as O
well O
as O
the O
effectiveness O
our O
proposed O
candidate O
Ô¨Åltering O
techniques O
in O
terms O
of O
reducing O
number O
of O
noisy O
candidates O
. O
The O
contribution O
of O
this O
work O
is O
two O
- O
fold O
: O
it O
proposes O
an O
innovative O
method O
of O
leveraging O
the O
Web O
for O
MAW O
extraction O
without O
involvement O
of O
manual O
mediation O
, O
yet O
achieving O
promising O
performance O
in O
identifying O
out O
- O
of O
- O
vocabulary O
code O
- O
mixing O
words O
; O
it O
provides O
a O
unique O
MAW O
dataset O
and O
corresponding O
corpus O
which O
are O
most O
updated O
, O
scaled O
, O
structured O
and O
comprehensive O
for O
supporting O
linguistic O
inquiries O
of O
code O
- O
mixing O
words O
, O
as O
well O
as O
for O
facilitating O
related O
NLP O
tasks O
. O
The O
preliminary O
analysis O
to O
the O
lexical O
and O
grammatical O
characteristics O
of O
SMAWs O
and O
the O
corpus O
imply O
the O
development O
of O
code O
- O
mixing O
words O
into O
being O
a O
more O
important O
and O
diversiÔ¨Åed O
component O
in O
the O
Chinese O
lexicon O
. O
Future O
work O
will O
continue O
the O
annotation O
of O
the O
lexicon O
and O
the O
corpus O
with O
information O
of O
domains O
, O
sources O
, O
active O
time O
, O
semantic O
classes O
, O
etc O
. O
, O
and O
conduct O
deeper O
linguistic O
analyses O
for O
uncovering O
the O
phonological O
and O
morpho O
- O
lexical O
characteristics O
of O
code O
- O
mixing O
words O
. O
Acknowledgments O
We O
acknowledge O
the O
research O
grants O
from O
Hong O
Kong O
Polytechnic O
University O
( O
PolyU O
RTVU O
) O
and O
GRF O
grant O
( O
CERG O
PolyU O
15211/14E O
, O
PolyU O
152006/16E O
and O
PolyU O
156086/18H O
) O
. O
This O
work O
is O
also O
funded O
by O
the O
Post O
- O
doctoral O
project O
( O
no O
. O
4ZZKE O
) O
at O
the O
Hong O
Kong O
Polytechnic O
University.840References O
Gerlof O
Bouma O
. O
2009 O
. O
Normalized O
( O
pointwise O
) O
mutual O
information O
in O
collocation O
extraction O
. O
Proceedings O
of O
GSCL O
, O
pages O
31‚Äì40 O
. O
Jing O
- O
Shin O
Chang O
and O
Keh O
- O
Yih O
Su O
. O
1997 O
. O
An O
unsupervised O
iterative O
method O
for O
chinese O
new O
lexicon O
extraction O
. O
In O
International O
Journal O
of O
Computational O
Linguistics O
& O
Chinese O
Language O
Processing O
, O
Volume O
2 O
, O
Number O
2 O
, O
August O
1997 O
, O
pages O
97‚Äì148 O
. O
Keh O
- O
Jiann O
Chen O
, O
Chu O
- O
Ren O
Huang O
, O
Li O
- O
Ping O
Chang O
, O
and O
Hui O
- O
Li O
Hsu O
. O
1996 O
. O
Sinica O
corpus O
: O
Design O
methodology O
for O
balanced O
corpora O
. O
In O
Proceedings O
of O
the O
11th O
PaciÔ¨Åc O
Asia O
Conference O
on O
Language O
, O
Information O
and O
Computation O
, O
pages O
167‚Äì176 O
. O
Keh O
- O
Jiann O
Chen O
and O
Shing O
- O
Huan O
Liu O
. O
1992 O
. O
Word O
identiÔ¨Åcation O
for O
mandarin O
chinese O
sentences O
. O
In O
Proceedings O
of O
the O
14th O
conference O
on O
Computational O
linguistics O
- O
Volume O
1 O
, O
pages O
101‚Äì107 O
. O
Association O
for O
Computational O
Linguistics O
. O
Keh O
- O
Jiann O
Chen O
and O
Wei O
- O
Yun O
Ma O
. O
2002 O
. O
Unknown O
word O
extraction O
for O
chinese O
documents O
. O
In O
Proceedings O
of O
the O
19th O
international O
conference O
on O
Computational O
linguistics O
- O
Volume O
1 O
, O
pages O
1‚Äì7 O
. O
Association O
for O
Computational O
Linguistics O
. O
Ga√´l O
Dias O
. O
2003 O
. O
Multiword O
unit O
hybrid O
extraction O
. O
In O
Proceedings O
of O
the O
ACL O
2003 O
workshop O
on O
Multiword O
expressions O
: O
analysis O
, O
acquisition O
and O
treatment O
- O
Volume O
18 O
, O
pages O
41‚Äì48 O
. O
Association O
for O
Computational O
Linguistics O
. O
Hongwei O
Ding O
, O
Yuanyuan O
Zhang O
, O
Hongchao O
Liu O
, O
and O
Chu O
- O
Ren O
Huang O
. O
2017 O
. O
A O
preliminary O
phonetic O
investigation O
of O
alphabetic O
words O
in O
mandarin O
chinese O
. O
InINTERSPEECH O
, O
pages O
3028‚Äì3032 O
. O
Pilar O
Dur√°n O
, O
David O
Malvern O
, O
Brian O
Richards O
, O
and O
Ngoni O
Chipere O
. O
2004 O
. O
Developmental O
trends O
in O
lexical O
diversity O
. O
Applied O
Linguistics O
, O
25(2):220‚Äì242 O
. O
Jia O
- O
Fei O
Hong O
and O
Chu O
- O
Ren O
Huang O
. O
2006 O
. O
Using O
chinese O
gigaword O
corpus O
and O
chinese O
word O
sketch O
in O
linguistic O
research O
. O
In O
Proceedings O
of O
the O
20th O
PaciÔ¨Åc O
Asia O
Conference O
on O
Language O
, O
Information O
and O
Computation O
, O
pages O
183‚Äì190 O
. O
Chu O
- O
Ren O
Huang O
. O
2009 O
. O
Tagged O
chinese O
gigaword O
version O
2.0 O
, O
ldc2009t14 O
. O
Linguistic O
Data O
Consortium O
. O
Chu O
- O
Ren O
Huang O
and O
Hongchao O
Liu O
. O
2017 O
. O
Corpusbased O
automatic O
extraction O
and O
analysis O
of O
mandarin O
alphabetic O
words O
( O
in O
chinese O
) O
. O
Journal O
of O
Yunnan O
Teachers O
University O
. O
Philosophy O
and O
social O
science O
section O
. O
Chu O
- O
Ren O
Huang O
, O
Petr O
≈†imon O
, O
Shu O
- O
Kai O
Hsieh O
, O
and O
Laurent O
Pr√©vot O
. O
2007 O
. O
Rethinking O
chinese O
word O
segmentation O
: O
tokenization O
, O
character O
classiÔ¨Åcation O
, O
or O
wordbreak O
identiÔ¨Åcation O
. O
In O
Proceedings O
of O
the O
45th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
Companion O
Volume O
Proceedings O
of O
the O
Demo O
and O
Poster O
Sessions O
, O
pages O
69‚Äì72.Shaohua O
Jiang O
and O
Yanzhong O
Dang O
. O
2007 O
. O
Automatic O
extraction O
of O
new O
- O
domain O
terms O
containing O
chinese O
lettered O
words O
( O
in O
chinese O
) O
. O
Computing O
Engineering O
, O
33(2):47‚Äì49 O
. O
Ksenia O
Kozha O
. O
2012 O
. O
Chinese O
via O
english O
: O
A O
case O
study O
of O
‚Äú O
lettered O
- O
words O
‚Äù O
as O
a O
way O
of O
integration O
into O
global O
communication O
. O
In O
Chinese O
Under O
Globalization O
: O
Emerging O
Trends O
in O
Language O
Use O
in O
China O
, O
pages O
105‚Äì125 O
. O
World O
ScientiÔ¨Åc O
. O
Helena O
C O
Kraemer O
. O
2014 O
. O
Kappa O
coefÔ¨Åcient O
. O
Wiley O
StatsRef O
: O
Statistics O
Reference O
Online O
, O
pages O
1‚Äì4 O
. O
Yongquan O
Liu O
. O
1994 O
. O
Survey O
on O
chinese O
lettered O
words O
( O
in O
chinese O
) O
. O
Language O
Planning O
, O
( O
10):7‚Äì9 O
. O
Yongquan O
Liu O
. O
2002 O
. O
The O
issue O
of O
lettered O
words O
in O
chinese O
. O
Applied O
Linguistics O
, O
1:8S‚Äì90 O
. O
Ka O
Yee O
Lun O
. O
2013 O
. O
Morphological O
structure O
of O
the O
chinese O
lettered O
words O
. O
University O
of O
Washington O
Working O
Papers O
in O
Linguistics O
. O
Christopher O
Manning O
, O
Mihai O
Surdeanu O
, O
John O
Bauer O
, O
Jenny O
Finkel O
, O
Steven O
Bethard O
, O
and O
David O
McClosky O
. O
2014 O
. O
The O
stanford O
corenlp O
natural O
language O
processing O
toolkit O
. O
In O
Proceedings O
of O
52nd O
annual O
meeting O
of O
the O
association O
for O
computational O
linguistics O
: O
system O
demonstrations O
, O
pages O
55‚Äì60 O
. O
Ruiqin O
Miao O
. O
2005 O
. O
Loanword O
adaptation O
in O
Mandarin O
Chinese O
: O
Perceptual O
, O
phonological O
and O
sociolinguistic O
factors O
. O
Ph.D. O
thesis O
, O
Stony O
Brook O
University O
. O
Dong O
Nguyen O
and O
Leonie O
Cornips O
. O
2016 O
. O
Automatic O
detection O
of O
intra O
- O
word O
code O
- O
switching O
. O
In O
Proceedings O
of O
the O
14th O
SIGMORPHON O
Workshop O
on O
Computational O
Research O
in O
Phonetics O
, O
Phonology O
, O
and O
Morphology O
, O
pages O
82‚Äì86 O
. O
He O
Ren O
and O
Jun O
- O
fang O
Zeng O
. O
2006 O
. O
A O
chinese O
word O
extraction O
algorithm O
based O
on O
information O
entropy O
. O
Journal O
of O
Chinese O
Information O
Processing O
, O
20(5):40‚Äì43 O
. O
Helena O
Riha O
. O
2010 O
. O
Lettered O
words O
in O
chinese O
: O
Roman O
letters O
as O
morpheme O
- O
syllables O
. O
Helena O
Riha O
and O
Kirk O
Baker O
. O
2010 O
. O
Using O
roman O
letters O
to O
create O
words O
in O
chinese O
. O
In O
Variation O
and O
Change O
in O
Morphology O
: O
Selected O
papers O
from O
the O
13th O
International O
Morphology O
Meeting O
, O
Vienna O
, O
February O
2008 O
, O
volume O
310 O
, O
page O
193 O
. O
John O
Benjamins O
Publishing O
. O
Xinchun O
Su O
and O
Xiaofang O
Wu O
. O
2013 O
. O
Vitality O
and O
limitation O
of O
chinese O
lettered O
words O
( O
in O
chinese O
) O
. O
Journal O
of O
Beihua O
University(Social O
Sciences O
) O
, O
2 O
. O
Chaofen O
Sun O
. O
2006 O
. O
Chinese O
: O
A O
linguistic O
introduction O
. O
Cambridge O
University O
Press O
. O
Li O
Hai O
Tan O
, O
Angela O
R O
Laird O
, O
Karl O
Li O
, O
and O
Peter O
T O
Fox O
. O
2005 O
. O
Neuroanatomical O
correlates O
of O
phonological O
processing O
of O
chinese O
characters O
and O
alphabetic O
words O
: O
A O
meta O
- O
analysis O
. O
Human O
brain O
mapping O
, O
25(1):83‚Äì91.841Nianwen O
Xue O
and O
Libin O
Shen O
. O
2003 O
. O
Chinese O
word O
segmentation O
as O
lmr O
tagging O
. O
In O
Proceedings O
of O
the O
second O
SIGHAN O
workshop O
on O
Chinese O
language O
processing O
- O
Volume O
17 O
, O
pages O
176‚Äì179 O
. O
Association O
for O
Computational O
Linguistics O
. O
Xiaocong O
Xue O
. O
2007 O
. O
A O
review O
on O
studies O
of O
letteredwords O
in O
contemporary O
chinese O
. O
Chinese O
Language O
Learning O
, O
2 O
. O
Haijun O
Zhang O
, O
Heyan O
Huang O
, O
Chaoyong O
Zhu O
, O
and O
Shumin O
Shi O
. O
2010 O
. O
A O
pragmatic O
model O
for O
new O
chinese O
word O
extraction O
. O
In O
Proceedings O
of O
the O
6th O
International O
Conference O
on O
Natural O
Language O
Processing O
and O
Knowledge O
Engineering O
( O
NLPKE2010 O
) O
, O
pages O
1‚Äì8 O
. O
IEEE O
. O
Tiewen O
Zhang O
. O
2005 O
. O
Study O
of O
the O
word O
family O
‚Äò O
x O
- O
ray O
‚Äô O
in O
chinese O
( O
in O
chinese O
) O
. O
Terminology O
Standardization O
& O
Information O
Technology O
, O
1 O
. O
Tiewen O
Zhang O
. O
2013 O
. O
The O
use O
of O
chinese O
letteredwords O
is O
a O
normal O
phenomenon O
of O
language O
contact O
. O
( O
in O
chinese O
) O
. O
Journal O
of O
Beihua O
University(Social O
Sciences O
) O
, O
2 O
. O
Hai O
Zhao O
, O
Changning O
Huang O
, O
and O
Mu O
Li O
. O
2006 O
. O
An O
improved O
chinese O
word O
segmentation O
system O
with O
conditional O
random O
Ô¨Åeld O
. O
In O
Proceedings O
of O
the O
Fifth O
SIGHAN O
Workshop O
on O
Chinese O
Language O
Processing O
, O
pages O
162‚Äì165 O
. O
Zezhi O
Zheng O
, O
Pu O
Zhang O
, O
and O
Jianguo O
Yang O
. O
2005 O
. O
Corpus O
- O
based O
extraction O
of O
chinese O
lettered O
words O
( O
in O
chinese O
) O
. O
Journal O
of O
Chinese O
Information O
Processing O
, O
19(2):79‚Äì86.842Proceedings O
of O
the O
1st O
Conference O
of O
the O
Asia O
- O
PaciÔ¨Åc O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
10th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
843‚Äì857 O
December O
4 O
- O
7 O
, O
2020 O
. O
¬© O
2020 O
Association O
for O
Computational O
Linguistics O
IndoNLU O
: O
Benchmark O
and O
Resources O
for O
Evaluating O
Indonesian O
Natural O
Language O
Understanding O
Bryan O
Wilie1‚àó O
, O
Karissa O
Vincentio2‚àó O
, O
Genta O
Indra O
Winata3‚àó O
, O
Samuel O
Cahyawijaya3‚àó O
, O
Xiaohong O
Li4,Zhi O
Yuan O
Lim4,Sidik O
Soleman5,Rahmad O
Mahendra6 O
, O
Pascale O
Fung3,Syafri O
Bahar4,Ayu O
Purwarianti1,5 O
1Institut O
Teknologi O
Bandung2Universitas O
Multimedia O
Nusantara O
3The O
Hong O
Kong O
University O
of O
Science O
and O
Technology O
4Gojek5Prosa.ai6Universitas O
Indonesia O
{ O
bryanwilie92 O
, O
karissavin O
} O
@gmail.com O
, O
{ O
giwinata O
, O
scahyawijaya O
} O
@connect.ust.hk O
Abstract O
Although O
Indonesian O
is O
known O
to O
be O
the O
fourth O
most O
frequently O
used O
language O
over O
the O
internet O
, O
the O
research O
progress O
on O
this O
language O
in O
natural O
language O
processing O
( O
NLP O
) O
is O
slowmoving O
due O
to O
a O
lack O
of O
available O
resources O
. O
In O
response O
, O
we O
introduce O
the O
Ô¨Årst O
- O
ever O
vast O
resource O
for O
training O
, O
evaluation O
, O
and O
benchmarking O
on O
Indonesian O
natural O
language O
understanding O
( O
IndoNLU O
) O
tasks O
. O
IndoNLU O
includes O
twelve O
tasks O
, O
ranging O
from O
single O
sentence O
classiÔ¨Åcation O
to O
pair O
- O
sentences O
sequence O
labeling O
with O
different O
levels O
of O
complexity O
. O
The O
datasets O
for O
the O
tasks O
lie O
in O
different O
domains O
and O
styles O
to O
ensure O
task O
diversity O
. O
We O
also O
provide O
a O
set O
of O
Indonesian O
pre O
- O
trained O
models O
( O
IndoBERT O
) O
trained O
from O
a O
large O
and O
clean O
Indonesian O
dataset O
( O
Indo4B O
) O
collected O
from O
publicly O
available O
sources O
such O
as O
social O
media O
texts O
, O
blogs O
, O
news O
, O
and O
websites O
. O
We O
release O
baseline O
models O
for O
all O
twelve O
tasks O
, O
as O
well O
as O
the O
framework O
for O
benchmark O
evaluation O
, O
thus O
enabling O
everyone O
to O
benchmark O
their O
system O
performances O
. O
1 O
Introduction O
Following O
the O
notable O
success O
of O
contextual O
pretrained O
language O
methods O
( O
Peters O
et O
al O
. O
, O
2018 O
; O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
several O
benchmarks O
to O
gauge O
the O
progress O
of O
general O
- O
purpose O
NLP O
research O
, O
such O
as O
GLUE O
( O
Wang O
et O
al O
. O
, O
2018 O
) O
, O
SuperGLUE O
( O
Wang O
et O
al O
. O
, O
2019 O
) O
, O
and O
CLUE O
( O
Xu O
et O
al O
. O
, O
2020 O
) O
, O
have O
been O
proposed O
. O
These O
benchmarks O
cover O
a O
large O
range O
of O
tasks O
to O
measure O
how O
well O
pre O
- O
trained O
models O
achieve O
compared O
to O
humans O
. O
However O
, O
these O
metrics O
are O
limited O
to O
high O
- O
resource O
languages O
, O
such O
as O
English O
and O
Chinese O
, O
that O
already O
have O
existing O
datasets O
available O
and O
are O
accessible O
to O
the O
research O
community O
. O
Most O
languages O
, O
by O
contrast O
, O
suffer O
from O
limited O
data O
collection O
and O
low O
awareness O
of O
‚àóThese O
authors O
contributed O
equally.published O
data O
for O
research O
. O
One O
of O
the O
languages O
which O
suffer O
from O
this O
resource O
scarcity O
problem O
is O
Indonesian O
. O
Indonesian O
is O
the O
fourth O
largest O
language O
used O
over O
the O
internet O
, O
with O
around O
171 O
million O
users O
across O
the O
globe.1Despite O
a O
large O
amount O
of O
Indonesian O
data O
available O
over O
the O
internet O
, O
the O
advancement O
of O
NLP O
research O
in O
Indonesian O
is O
slowmoving O
. O
This O
problem O
occurs O
because O
available O
datasets O
are O
scattered O
, O
with O
a O
lack O
of O
documentation O
and O
minimal O
community O
engagement O
. O
Moreover O
, O
many O
existing O
studies O
in O
Indonesian O
NLP O
do O
not O
provide O
codes O
and O
test O
splits O
, O
making O
it O
impossible O
to O
reproduce O
results O
. O
To O
address O
the O
data O
scarcity O
problem O
, O
we O
propose O
the O
Ô¨Årst O
- O
ever O
Indonesian O
natural O
language O
understanding O
benchmark O
, O
IndoNLU O
, O
a O
collection O
of O
twelve O
diverse O
tasks O
. O
The O
tasks O
are O
mainly O
categorized O
based O
on O
the O
input O
, O
such O
as O
single O
- O
sentences O
and O
sentence O
- O
pairs O
, O
and O
objectives O
, O
such O
as O
sentence O
classiÔ¨Åcation O
tasks O
and O
sequence O
labeling O
tasks O
. O
The O
benchmark O
is O
designed O
to O
cater O
to O
a O
range O
of O
styles O
in O
both O
formal O
and O
colloquial O
Indonesian O
, O
which O
are O
highly O
diverse O
. O
We O
collect O
a O
range O
of O
datasets O
from O
existing O
works O
: O
an O
emotion O
classiÔ¨Åcation O
dataset O
( O
Saputri O
et O
al O
. O
, O
2018 O
) O
, O
QA O
factoid O
dataset O
( O
Purwarianti O
et O
al O
. O
, O
2007 O
) O
, O
sentiment O
analysis O
dataset O
( O
Purwarianti O
and O
Crisdayanti O
, O
2019 O
) O
, O
aspect O
- O
based O
sentiment O
analysis O
dataset O
( O
Ilmania O
et O
al O
. O
, O
2018 O
; O
Azhar O
et O
al O
. O
, O
2019 O
) O
, O
part O
- O
ofspeech O
( O
POS O
) O
tag O
dataset O
( O
Dinakaramani O
et O
al O
. O
, O
2014 O
; O
Hoesen O
and O
Purwarianti O
, O
2018 O
) O
, O
named O
entity O
recognition O
( O
NER O
) O
dataset O
( O
Hoesen O
and O
Purwarianti O
, O
2018 O
) O
, O
span O
extraction O
dataset O
( O
Mahfuzh O
et O
al O
. O
, O
2019 O
; O
Septiandri O
and O
Sutiono O
, O
2019 O
; O
Fernando O
et O
al O
. O
, O
2019 O
) O
, O
and O
textual O
entailment O
dataset O
( O
Setya O
and O
Mahendra O
, O
2018 O
) O
. O
It O
is O
difÔ¨Åcult O
to O
compare O
model O
performance O
since O
there O
is O
no O
ofÔ¨Åcial O
1https://www.internetworldstats.com/stats3.htm843split O
of O
information O
for O
existing O
datasets O
. O
Therefore O
we O
standardize O
the O
benchmark O
by O
resplitting O
the O
datasets O
on O
each O
task O
for O
reproducibility O
purposes O
. O
To O
expedite O
the O
modeling O
and O
evaluation O
processes O
for O
this O
benchmark O
, O
we O
present O
samples O
of O
the O
model O
pre O
- O
training O
code O
and O
a O
framework O
to O
evaluate O
models O
in O
all O
downstream O
tasks O
. O
We O
will O
publish O
the O
score O
of O
our O
benchmark O
on O
a O
publicly O
accessible O
leaderboard O
to O
provide O
better O
community O
engagement O
and O
benchmark O
transparency O
. O
To O
further O
advance O
Indonesian O
NLP O
research O
, O
we O
collect O
around O
four O
billion O
words O
from O
Indonesian O
preprocessed O
text O
data O
( O
‚âà23 O
GB O
) O
, O
as O
a O
new O
standard O
dataset O
, O
called O
Indo4B O
, O
for O
self O
- O
supervised O
learning O
. O
The O
dataset O
comes O
from O
sources O
like O
online O
news O
, O
social O
media O
, O
Wikipedia O
, O
online O
articles O
, O
subtitles O
from O
video O
recordings O
, O
and O
parallel O
datasets O
. O
We O
then O
introduce O
an O
Indonesian O
BERTbased O
model O
, O
IndoBERT O
, O
which O
is O
trained O
on O
our O
Indo4B O
dataset O
. O
We O
also O
introduce O
another O
IndoBERT O
variant O
based O
on O
the O
ALBERT O
model O
( O
Lan O
et O
al O
. O
, O
2020 O
) O
, O
called O
IndoBERT O
- O
lite O
. O
The O
two O
variants O
of O
IndoBERT O
are O
used O
as O
baseline O
models O
in O
theIndoNLU O
benchmark O
. O
In O
this O
work O
, O
we O
also O
extensively O
compare O
our O
IndoBERT O
models O
to O
different O
pre O
- O
trained O
word O
embeddings O
and O
existing O
multilingual O
pre O
- O
trained O
models O
, O
such O
as O
Multilingual O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
and O
XLM O
- O
R O
( O
Conneau O
et O
al O
. O
, O
2019 O
) O
, O
to O
measure O
their O
effectiveness O
. O
Results O
show O
that O
our O
pre O
- O
trained O
models O
outperform O
most O
of O
the O
existing O
pre O
- O
trained O
models O
. O
2 O
Related O
Work O
Benchmarks O
GLUE O
( O
Wang O
et O
al O
. O
, O
2018 O
) O
is O
a O
multi O
- O
task O
benchmark O
for O
natural O
language O
understanding O
( O
NLU O
) O
in O
the O
English O
language O
. O
It O
consists O
of O
nine O
tasks O
: O
single O
- O
sentence O
input O
, O
semantic O
similarity O
detection O
, O
and O
natural O
language O
inference O
( O
NLI O
) O
tasks O
. O
GLUE O
‚Äôs O
harder O
counterpart O
SuperGLUE O
( O
Wang O
et O
al O
. O
, O
2019 O
) O
covers O
question O
answering O
, O
NLI O
, O
co O
- O
reference O
resolution O
, O
and O
word O
sense O
disambiguation O
tasks O
. O
CLUE O
( O
Xu O
et O
al O
. O
, O
2020 O
) O
is O
a O
Chinese O
NLU O
benchmark O
that O
includes O
a O
test O
set O
designed O
to O
probe O
a O
unique O
and O
speciÔ¨Åc O
linguistic O
phenomenon O
in O
the O
Chinese O
language O
. O
It O
consists O
of O
eight O
diverse O
tasks O
, O
including O
single O
- O
sentence O
, O
sentence O
- O
pair O
, O
and O
machine O
reading O
comprehension O
tasks O
. O
FLUE O
( O
Le O
et O
al O
. O
, O
2019 O
) O
is O
an O
evaluation O
NLP O
benchmark O
for O
the O
French O
language O
which O
is O
divided O
into O
six O
different O
task O
categories O
: O
text O
classiÔ¨Åcation O
, O
paraphrasing O
, O
NLI O
, O
parsing O
, O
POS O
tagging O
, O
and O
word O
sense O
disambiguation O
. O
Contextual O
Language O
Models O
In O
recent O
years O
, O
contextual O
pre O
- O
trained O
language O
models O
have O
shown O
a O
major O
breakthrough O
in O
NLP O
, O
starting O
from O
ELMo O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
. O
With O
the O
emergence O
of O
the O
transformer O
model O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
Devlin O
et O
al O
. O
( O
2019 O
) O
proposed O
BERT O
, O
a O
faster O
architecture O
to O
train O
a O
language O
model O
that O
eliminates O
recurrences O
by O
applying O
a O
multi O
- O
head O
attention O
layer O
. O
Liu O
et O
al O
. O
( O
2019 O
) O
later O
proposed O
RoBERTa O
, O
which O
improves O
the O
performance O
of O
BERT O
by O
applying O
dynamic O
masking O
, O
increasing O
the O
batch O
size O
, O
and O
removing O
the O
next O
- O
sentence O
prediction O
. O
Lan O
et O
al O
. O
( O
2020 O
) O
proposed O
ALBERT O
, O
which O
extends O
the O
BERT O
model O
by O
applying O
factorization O
and O
weight O
sharing O
to O
reduce O
the O
number O
of O
parameters O
and O
time O
. O
Many O
research O
studies O
have O
introduced O
contextual O
pre O
- O
trained O
language O
models O
on O
languages O
other O
than O
English O
. O
Cui O
et O
al O
. O
( O
2019 O
) O
introduced O
the O
Chinese O
BERT O
and O
RoBERTa O
models O
, O
while O
Martin O
et O
al O
. O
( O
2019 O
) O
and O
Le O
et O
al O
. O
( O
2019 O
) O
introduced O
CamemBERT O
and O
FLAUBert O
respectively O
, O
which O
are O
BERT O
- O
based O
models O
for O
the O
French O
language O
. O
Devlin O
et O
al O
. O
( O
2019 O
) O
introduced O
the O
Multilingual O
BERT O
model O
, O
a O
BERT O
model O
trained O
on O
monolingual O
Wikipedia O
data O
in O
many O
languages O
. O
Meanwhile O
, O
Lample O
and O
Conneau O
( O
2019 O
) O
introduced O
XLM O
, O
a O
cross O
- O
lingual O
pre O
- O
trained O
language O
model O
that O
uses O
parallel O
data O
as O
a O
new O
translation O
masked O
loss O
to O
improve O
the O
cross O
- O
linguality O
. O
Finally O
, O
Conneau O
et O
al O
. O
( O
2019 O
) O
introduced O
XLM O
- O
R O
, O
a O
RoBERTabased O
XLM O
model O
. O
3IndoNLU O
Benchmark O
In O
this O
section O
, O
we O
describe O
our O
benchmark O
as O
four O
components O
. O
Firstly O
, O
we O
introduce O
the O
12 O
tasks O
inIndoNLU O
for O
Indonesian O
natural O
language O
understanding O
. O
Secondly O
, O
we O
introduce O
a O
large O
- O
scale O
Indonesian O
dataset O
for O
self O
- O
supervised O
pre O
- O
training O
models O
. O
Thirdly O
, O
we O
explain O
the O
various O
kinds O
of O
baseline O
models O
used O
in O
our O
IndoNLU O
benchmark O
. O
Lastly O
, O
we O
describe O
the O
evaluation O
metric O
used O
to O
standardize O
the O
scoring O
over O
different O
models O
in O
our O
IndoNLU O
benchmark O
. O
3.1 O
Downstream O
Tasks O
TheIndoNLU O
downstream O
tasks O
covers O
12 O
tasks O
divided O
into O
four O
categories O
: O
( O
a O
) O
single O
- O
sentence O
classiÔ¨Åcation O
, O
( O
b O
) O
single O
- O
sentence O
sequencetagging O
, O
( O
c O
) O
sentence O
- O
pair O
classiÔ¨Åcation O
, O
and O
( O
d)844Dataset|Train| O
|Valid| O
|Test|Task O
Description O
# O
Label O
# O
Class O
Domain O
Style O
Single O
- O
Sentence O
ClassiÔ¨Åcation O
Tasks O
EmoT‚Ä†3,521 O
440 O
442 O
emotion O
classiÔ¨Åcation O
1 O
5 O
tweets O
colloquial O
SmSA O
11,000 O
1,260 O
500 O
sentiment O
analysis O
1 O
3 O
general O
colloquial O
CASA O
810 O
90 O
180 O
aspect O
- O
based O
sentiment O
analysis O
6 O
3 O
automobile O
colloquial O
HoASA‚Ä†2,283 O
285 O
286 O
aspect O
- O
based O
sentiment O
analysis O
10 O
4 O
hotel O
colloquial O
Sentence O
- O
Pair O
ClassiÔ¨Åcation O
Tasks O
WReTE‚Ä†300 O
50 O
100 O
textual O
entailment O
1 O
2 O
wiki O
formal O
Single O
- O
Sentence O
Sequence O
Labeling O
Tasks O
POSP‚Ä†6,720 O
840 O
840 O
part O
- O
of O
- O
speech O
tagging O
1 O
26 O
news O
formal O
BaPOS O
8,000 O
1,000 O
1,029 O
part O
- O
of O
- O
speech O
tagging O
1 O
41 O
news O
formal O
TermA O
3,000 O
1,000 O
1,000 O
span O
extraction O
1 O
5 O
hotel O
colloquial O
KEPS O
800 O
200 O
247 O
span O
extraction O
1 O
3 O
banking O
colloquial O
NERGrit‚Ä†1,672 O
209 O
209 O
named O
entity O
recognition O
1 O
7 O
wiki O
formal O
NERP‚Ä†6,720 O
840 O
840 O
named O
entity O
recognition O
1 O
11 O
news O
formal O
Sentence O
- O
Pair O
Sequence O
Labeling O
Tasks O
FacQA O
2,495 O
311 O
311 O
span O
extraction O
1 O
3 O
news O
formal O
Table O
1 O
: O
Task O
statistics O
and O
descriptions.‚Ä†We O
create O
new O
splits O
for O
the O
dataset O
. O
sentence O
- O
pair O
sequence O
labeling O
. O
The O
data O
samples O
for O
each O
task O
are O
shown O
in O
Appendix O
A. O
3.1.1 O
Single O
- O
Sentence O
ClassiÔ¨Åcation O
Tasks O
EmoT O
An O
emotion O
classiÔ¨Åcation O
dataset O
collected O
from O
the O
social O
media O
platform O
Twitter O
( O
Saputri O
et O
al O
. O
, O
2018 O
) O
. O
The O
dataset O
consists O
of O
around O
4000 O
Indonesian O
colloquial O
language O
tweets O
, O
covering O
Ô¨Åve O
different O
emotion O
labels O
: O
anger O
, O
fear O
, O
happiness O
, O
love O
, O
and O
sadness O
. O
SmSA O
This O
sentence O
- O
level O
sentiment O
analysis O
dataset O
( O
Purwarianti O
and O
Crisdayanti O
, O
2019 O
) O
is O
a O
collection O
of O
comments O
and O
reviews O
in O
Indonesian O
obtained O
from O
multiple O
online O
platforms O
. O
The O
text O
was O
crawled O
and O
then O
annotated O
by O
several O
Indonesian O
linguists O
to O
construct O
this O
dataset O
. O
There O
are O
three O
possible O
sentiments O
on O
the O
SmSA O
dataset O
: O
positive O
, O
negative O
, O
and O
neutral O
. O
CASA O
An O
aspect O
- O
based O
sentiment O
analysis O
dataset O
consisting O
of O
around O
a O
thousand O
car O
reviews O
collected O
from O
multiple O
Indonesian O
online O
automobile O
platforms O
( O
Ilmania O
et O
al O
. O
, O
2018 O
) O
. O
The O
dataset O
covers O
six O
aspects O
of O
car O
quality O
. O
We O
deÔ¨Åne O
the O
task O
to O
be O
a O
multi O
- O
label O
classiÔ¨Åcation O
task O
, O
where O
each O
label O
represents O
a O
sentiment O
for O
a O
single O
aspect O
with O
three O
possible O
values O
: O
positive O
, O
negative O
, O
and O
neutral O
. O
HoASA O
An O
aspect O
- O
based O
sentiment O
analysis O
dataset O
consisting O
of O
hotel O
reviews O
collected O
from O
the O
hotel O
aggregator O
platform O
, O
AiryRooms O
( O
Azharet O
al O
. O
, O
2019).2The O
dataset O
covers O
ten O
different O
aspects O
of O
hotel O
quality O
. O
Similar O
to O
the O
CASA O
dataset O
, O
each O
review O
is O
labeled O
with O
a O
single O
sentiment O
label O
for O
each O
aspect O
. O
There O
are O
four O
possible O
sentiment O
classes O
for O
each O
sentiment O
label O
: O
positive O
, O
negative O
, O
neutral O
, O
and O
positive O
- O
negative O
. O
The O
positivenegative O
label O
is O
given O
to O
a O
review O
that O
contains O
multiple O
sentiments O
of O
the O
same O
aspect O
but O
for O
different O
objects O
( O
e.g. O
, O
cleanliness O
of O
bed O
and O
toilet O
) O
. O
3.1.2 O
Sentence O
- O
Pair O
ClassiÔ¨Åcation O
Task O
WReTE O
The O
Wiki O
Revision O
Edits O
Textual O
Entailment O
dataset O
( O
Setya O
and O
Mahendra O
, O
2018 O
) O
consists O
of O
450 O
sentence O
pairs O
constructed O
from O
Wikipedia O
revision O
history O
. O
The O
dataset O
contains O
pairs O
of O
sentences O
and O
binary O
semantic O
relations O
between O
the O
pairs O
. O
The O
data O
are O
labeled O
as O
entailed O
when O
the O
meaning O
of O
the O
second O
sentence O
can O
be O
derived O
from O
the O
Ô¨Årst O
one O
, O
and O
not O
entailed O
otherwise O
. O
3.1.3 O
Single O
- O
Sentence O
Sequence O
Labeling O
Tasks O
POSP O
This O
Indonesian O
part O
- O
of O
- O
speech O
tagging O
( O
POS O
) O
dataset O
( O
Hoesen O
and O
Purwarianti O
, O
2018 O
) O
is O
collected O
from O
Indonesian O
news O
websites O
. O
The O
dataset O
consists O
of O
around O
8000 O
sentences O
with O
26 O
POS O
tags O
. O
The O
POS O
tag O
labels O
follow O
the O
Indonesian O
Association O
of O
Computational O
Linguistics O
( O
INACL O
) O
POS O
Tagging O
Convention.3 O
2https://github.com/annisanurulazhar/absa-playground O
3http://inacl.id/inacl/wp-content/uploads/2017/06/INACLPOS-Tagging-Convention-26-Mei.pdf845Model O
# O
Params O
# O
Layers O
# O
HeadsEmb O
. O
SizeHidden O
SizeFFN O
SizeLanguage O
TypePre O
- O
train O
Emb O
. O
Type O
Scratch O
15.1 O
M O
6 O
10 O
300 O
300 O
3072 O
Mono O
fastText O
- O
cc O
- O
id O
15.1 O
M O
6 O
10 O
300 O
300 O
3072 O
Mono O
Word O
Emb O
. O
fastText O
- O
indo4b O
15.1 O
M O
6 O
10 O
300 O
300 O
3072 O
Mono O
Word O
Emb O
. O
IndoBERT O
- O
lite O
BASE O
11.7 O
M O
12 O
12 O
128 O
768 O
3072 O
Mono O
Contextual O
IndoBERT O
BASE O
124.5 O
M O
12 O
12 O
768 O
768 O
3072 O
Mono O
Contextual O
IndoBERT O
- O
lite O
LARGE O
17.7 O
M O
24 O
16 O
128 O
1024 O
4096 O
Mono O
Contextual O
IndoBERT O
LARGE O
335.2 O
M O
24 O
16 O
1024 O
1024 O
4096 O
Mono O
Contextual O
mBERT O
167.4 O
M O
12 O
12 O
768 O
768 O
3072 O
Multi O
Contextual O
XLM O
- O
R O
BASE O
278.7 O
M O
12 O
12 O
768 O
768 O
3072 O
Multi O
Contextual O
XLM O
- O
R O
LARGE O
561.0 O
M O
24 O
16 O
1024 O
1024 O
4096 O
Multi O
Contextual O
XLM O
- O
MLM O
LARGE O
573.2 O
M O
16 O
16 O
1280 O
1280 O
5120 O
Multi O
Contextual O
Table O
2 O
: O
The O
details O
of O
baseline O
models O
used O
in O
IndoNLU O
benchmark O
BaPOS O
This O
POS O
tagging O
dataset O
( O
Dinakaramani O
et O
al O
. O
, O
2014 O
) O
contains O
about O
1000 O
sentences O
, O
collected O
from O
the O
PAN O
Localization O
Project.4In O
this O
dataset O
, O
each O
word O
is O
tagged O
by O
one O
of O
23 O
POS O
tag O
classes.5Data O
splitting O
used O
in O
this O
benchmark O
follows O
the O
experimental O
setting O
used O
by O
Kurniawan O
and O
Aji O
( O
2018 O
) O
. O
TermA O
This O
span O
- O
extraction O
dataset O
is O
collected O
from O
the O
hotel O
aggregator O
platform O
, O
AiryRooms O
( O
Septiandri O
and O
Sutiono O
, O
2019 O
; O
Fernando O
et O
al O
. O
, O
2019).6The O
dataset O
consists O
of O
thousands O
of O
hotel O
reviews O
, O
which O
each O
contain O
a O
span O
label O
for O
aspect O
and O
sentiment O
words O
representing O
the O
opinion O
of O
the O
reviewer O
on O
the O
corresponding O
aspect O
. O
The O
labels O
use O
Inside O
- O
Outside O
- O
Beginning O
( O
IOB O
) O
tagging O
representation O
with O
two O
kinds O
of O
tags O
, O
aspect O
and O
sentiment O
. O
KEPS O
This O
keyphrase O
extraction O
dataset O
( O
Mahfuzh O
et O
al O
. O
, O
2019 O
) O
consists O
of O
text O
from O
Twitter O
discussing O
banking O
products O
and O
services O
and O
is O
written O
in O
the O
Indonesian O
language O
. O
A O
phrase O
containing O
important O
information O
is O
considered O
a O
keyphrase O
. O
Text O
may O
contain O
one O
or O
more O
keyphrases O
since O
important O
phrases O
can O
be O
located O
at O
different O
positions O
. O
The O
dataset O
follows O
the O
IOB O
chunking O
format O
, O
which O
represents O
the O
position O
of O
the O
keyphrase O
. O
NERGrit O
This O
NER O
dataset O
is O
taken O
from O
the O
Grit O
- O
ID O
repository,7and O
the O
labels O
are O
spans O
in O
IOB O
chunking O
representation O
. O
The O
dataset O
consists O
of O
4http://www.panl10n.net/ O
5http://bahasa.cs.ui.ac.id/postag/downloads/Tagset.pdf O
6https://github.com/jordhy97/Ô¨Ånal O
project O
7https://github.com/grit-id/nergrit-corpusthree O
kinds O
of O
named O
entity O
tags O
, O
PERSON O
( O
name O
of O
person O
) O
, O
PLACE O
( O
name O
of O
location O
) O
, O
and O
ORGANIZATION O
( O
name O
of O
organization O
) O
. O
NERP O
This O
NER O
dataset O
( O
Hoesen O
and O
Purwarianti O
, O
2018 O
) O
contains O
texts O
collected O
from O
several O
Indonesian O
news O
websites O
. O
There O
are O
Ô¨Åve O
labels O
available O
in O
this O
dataset O
, O
PER O
( O
name O
of O
person O
) O
, O
LOC O
( O
name O
of O
location O
) O
, O
IND O
( O
name O
of O
product O
or O
brand O
) O
, O
EVT O
( O
name O
of O
the O
event O
) O
, O
and O
FNB O
( O
name O
of O
food O
and O
beverage O
) O
. O
Similar O
to O
the O
TermA O
dataset O
, O
the O
NERP O
dataset O
uses O
the O
IOB O
chunking O
format O
. O
3.1.4 O
Sentence O
- O
Pair O
Sequence O
Labeling O
Task O
FacQA O
The O
goal O
of O
the O
FacQA O
dataset O
is O
to O
Ô¨Ånd O
the O
answer O
to O
a O
question O
from O
a O
provided O
short O
passage O
from O
a O
news O
article O
( O
Purwarianti O
et O
al O
. O
, O
2007 O
) O
. O
Each O
row O
in O
the O
FacQA O
dataset O
consists O
of O
a O
question O
, O
a O
short O
passage O
, O
and O
a O
label O
phrase O
, O
which O
can O
be O
found O
inside O
the O
corresponding O
short O
passage O
. O
There O
are O
six O
categories O
of O
questions O
: O
date O
, O
location O
, O
name O
, O
organization O
, O
person O
, O
and O
quantitative O
. O
3.2Indo4B O
Dataset O
Indonesian O
NLP O
development O
has O
struggled O
with O
the O
availability O
of O
data O
. O
To O
cope O
with O
this O
issue O
, O
we O
provide O
a O
large O
- O
scale O
dataset O
called O
Indo4B O
for O
building O
a O
self O
- O
supervised O
pre O
- O
trained O
model O
. O
Our O
self O
- O
supervised O
dataset O
consists O
of O
around O
4B O
words O
, O
with O
around O
250 O
M O
sentences O
. O
The O
Indo4B O
dataset O
covers O
both O
formal O
and O
colloquial O
Indonesian O
sentences O
compiled O
from O
12 O
datasets O
, O
of O
which O
two O
cover O
Indonesian O
colloquial O
language O
, O
eight O
cover O
formal O
Indonesian O
language O
, O
and O
the O
rest O
have O
a O
mixed O
style O
of O
both O
colloquial O
and O
formal O
. O
The O
statistics O
of O
our O
large O
- O
scale O
dataset O
can O
be846Dataset O
# O
Words O
# O
Sentences O
Size O
Style O
Source O
OSCAR O
( O
Ortiz O
Su O
¬¥ O
arez O
et O
al O
. O
, O
2019 O
) O
2,279,761,186 O
148,698,472 O
14.9 O
GB O
mixed O
OSCAR O
CoNLLu O
Common O
Crawl O
( O
Ginter O
et O
al O
. O
, O
2017 O
) O
905,920,488 O
77,715,412 O
6.1 O
GB O
mixed O
LINDAT O
/ O
CLARIAH O
- O
CZ O
OpenSubtitles O
( O
Lison O
and O
Tiedemann O
, O
2016 O
) O
105,061,204 O
25,255,662 O
664.8 O
MB O
mixed O
OPUS O
OpenSubtitles O
Twitter O
Crawl2115,205,737 O
11,605,310 O
597.5 O
MB O
colloquial O
Twitter O
Wikipedia O
Dump176,263,857 O
4,768,444 O
528.1 O
MB O
formal O
Wikipedia O
Wikipedia O
CoNLLu O
( O
Ginter O
et O
al O
. O
, O
2017 O
) O
62,373,352 O
4,461,162 O
423.2 O
MB O
formal O
LINDAT O
/ O
CLARIAH O
- O
CZ O
Twitter O
UI2(Saputri O
et O
al O
. O
, O
2018 O
) O
16,637,641 O
1,423,212 O
88 O
MB O
colloquial O
Twitter O
OPUS O
JW300 O
( O
Agi O
¬¥ O
c O
and O
Vuli O
¬¥ O
c O
, O
2019 O
) O
8,002,490 O
586,911 O
52 O
MB O
formal O
OPUS O
Tempo35,899,252 O
391,591 O
40.8 O
MB O
formal O
ILSP O
Kompas33,671,715 O
220,555 O
25.5 O
MB O
formal O
ILSP O
TED O
1,483,786 O
111,759 O
9.9 O
MB O
mixed O
TED O
BPPT O
500,032 O
25,943 O
3.5 O
MB O
formal O
BPPT O
Parallel O
Corpus O
510,396 O
35,174 O
3.4 O
MB O
formal O
PAN O
Localization O
TALPCo O
( O
Nomoto O
et O
al O
. O
, O
2018 O
) O
8,795 O
1,392 O
56.1 O
KB O
formal O
Tokyo O
University O
Frog O
Storytelling O
( O
Moeljadi O
, O
2012 O
) O
1,545 O
177 O
10.1 O
KB O
mixed O
Tokyo O
University O
TOTAL O
3,581,301,476 O
275,301,176 O
23.43 O
GB O
Table O
3 O
: O
Indo4B O
dataset O
statistics.1https://dumps.wikimedia.org/backup-index.html.2We O
crawl O
tweets O
from O
Twitter O
. O
The O
Twitter O
data O
will O
not O
be O
shared O
publicly O
due O
to O
restrictions O
of O
the O
Twitter O
Developer O
Policy O
and O
Agreement.3https://ilps.science.uva.nl/. O
found O
in O
Table O
3 O
. O
We O
share O
the O
datasets O
that O
are O
listed O
in O
the O
table O
, O
except O
for O
those O
from O
Twitter O
due O
to O
restrictions O
of O
the O
Twitter O
Developer O
Policy O
and O
Agreement O
. O
The O
details O
of O
Indo4B O
dataset O
sources O
are O
shown O
in O
Appendix O
B. O
3.3 O
Baselines O
In O
this O
section O
, O
we O
explain O
the O
baseline O
models O
and O
the O
Ô¨Åne O
- O
tuning O
settings O
that O
we O
use O
in O
the O
IndoNLU O
benchmark O
. O
3.3.1 O
Models O
We O
provide O
a O
diverse O
set O
of O
baseline O
models O
, O
from O
a O
non O
- O
pre O
- O
trained O
model O
( O
scratch O
) O
, O
to O
a O
wordembedding O
- O
based O
model O
, O
to O
contextualized O
language O
models O
. O
For O
the O
word O
- O
embeddings O
- O
based O
model O
, O
we O
use O
an O
existing O
fastText O
model O
trained O
on O
the O
Indonesian O
Common O
Crawl O
( O
CC O
- O
ID O
) O
dataset O
( O
Joulin O
et O
al O
. O
, O
2016 O
; O
Grave O
et O
al O
. O
, O
2018 O
) O
. O
fastText O
We O
build O
a O
fastText O
model O
with O
our O
large O
- O
scale O
self O
- O
supervised O
dataset O
, O
Indo4B O
, O
for O
comparison O
with O
the O
CC O
- O
ID O
fastText O
model O
and O
contextualized O
language O
model O
. O
For O
the O
models O
above O
and O
the O
fastText O
model O
, O
we O
use O
the O
transformer O
architecture O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O
We O
experiment O
with O
different O
numbers O
of O
layers O
, O
2 O
, O
4 O
, O
and O
6 O
, O
for O
the O
transformer O
encoder O
. O
For O
the O
fastText O
model O
, O
we O
Ô¨Årst O
pre O
- O
train O
the O
fastText O
embeddings O
with O
skipgram O
word O
representation O
and O
produce O
a O
300 O
- O
dimensional O
embedding O
vector O
. O
We O
then O
generate O
all O
required O
embeddings O
for O
each O
downstream O
task O
from O
the O
pre O
- O
trained O
fastText O
embeddings O
andcover O
all O
words O
in O
the O
vocabulary O
. O
Contextualized O
Language O
Models O
We O
build O
our O
own O
Indonesian O
BERT O
and O
ALBERT O
models O
, O
named O
IndoBERT O
and O
IndoBERT O
- O
lite O
, O
respectively O
, O
in O
both O
base O
and O
large O
sizes O
. O
The O
details O
of O
our O
IndoBERT O
and O
IndoBERT O
- O
lite O
models O
are O
explained O
in O
Section O
4 O
. O
Aside O
from O
a O
monolingual O
model O
, O
we O
also O
provide O
multilingual O
model O
baselines O
such O
as O
Multilingual O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
XLM O
( O
Lample O
and O
Conneau O
, O
2019 O
) O
, O
and O
XLM O
- O
R O
( O
Conneau O
et O
al O
. O
, O
2019 O
) O
. O
The O
details O
of O
each O
model O
are O
shown O
in O
Table O
2 O
. O
3.3.2 O
Fine O
- O
tuning O
Settings O
We O
Ô¨Åne O
- O
tune O
a O
pre O
- O
trained O
model O
for O
each O
task O
with O
initial O
learning O
with O
a O
range O
of O
learning O
rates O
[ O
1e-5 O
, O
4e-5 O
] O
. O
We O
apply O
a O
decay O
rate O
of O
[ O
0.8 O
, O
0.9 O
] O
for O
every O
epoch O
, O
and O
sample O
each O
batch O
with O
a O
size O
of O
16 O
for O
all O
datasets O
except O
FacQA O
and O
POSP O
, O
for O
which O
we O
use O
a O
batch O
size O
of O
8 O
. O
To O
establish O
a O
benchmark O
, O
we O
keep O
a O
Ô¨Åxed O
setting O
, O
and O
we O
use O
an O
early O
stop O
on O
the O
validation O
score O
to O
choose O
the O
best O
model O
. O
The O
details O
of O
the O
Ô¨Åne O
- O
tuning O
hyperparameter O
settings O
used O
are O
shown O
in O
Appendix O
D. O
3.4 O
Evaluation O
Metrics O
We O
use O
the O
F1 O
score O
to O
measure O
the O
evaluation O
performance O
of O
all O
tasks O
. O
For O
the O
binary O
and O
multilabel O
classiÔ¨Åcation O
tasks O
, O
we O
measure O
the O
macroaveraged O
F1 O
score O
by O
taking O
the O
top-1 O
prediction O
from O
the O
model O
. O
For O
the O
sequence O
labeling O
task O
, O
we O
calculate O
word O
- O
level O
sequence O
labeling O
macro-847ModelMaximum O
Sequence O
Length O
= O
128 O
Maximum O
Sequence O
Length O
= O
512 O
Batch O
Size O
Learning O
Rate O
Steps O
Duration O
( O
Hr O
. O
) O
Batch O
Size O
Learning O
Rate O
Steps O
Duration O
( O
Hr O
. O
) O
IndoBERT O
- O
lite O
BASE O
4096 O
0.00176 O
112.5 O
K O
38 O
1024 O
0.00088 O
50 O
K O
23 O
IndoBERT O
BASE O
256 O
0.00002 O
1 O
M O
35 O
256 O
0.00002 O
68 O
K O
9 O
IndoBERT O
- O
lite O
LARGE O
1024 O
0.00044 O
500 O
K O
134 O
256 O
0.00044 O
129 O
K O
45 O
IndoBERT O
LARGE O
256 O
0.0001 O
1 O
M O
89 O
128 O
0.00008 O
120 O
K O
32 O
Table O
4 O
: O
Hyperparameters O
and O
training O
duration O
for O
IndoBERT O
model O
pre O
- O
training O
. O
averaged O
F1 O
- O
score O
for O
all O
models O
by O
following O
the O
sequence O
labeling O
evaluation O
method O
described O
in O
the O
CoNLL O
evaluation O
script O
. O
We O
calculate O
two O
mean O
F1 O
- O
scores O
separately O
for O
classiÔ¨Åcation O
and O
sequence O
labeling O
tasks O
to O
evaluate O
models O
on O
our O
IndoNLU O
benchmark O
. O
4 O
IndoBERT O
In O
this O
section O
, O
we O
describe O
the O
details O
of O
our O
Indonesian O
contextualized O
models O
, O
IndoBERT O
and O
IndoBERT O
- O
lite O
, O
which O
are O
trained O
using O
our O
Indo4B O
dataset O
. O
We O
elucidate O
the O
extensive O
details O
of O
the O
models O
‚Äô O
development O
, O
Ô¨Årst O
the O
dataset O
preprocessing O
, O
followed O
by O
the O
pre O
- O
training O
setup O
. O
4.1 O
Preprocessing O
Dataset O
Preparation O
To O
get O
the O
most O
beneÔ¨Åcial O
next O
sentence O
prediction O
task O
training O
from O
the O
Indo4B O
dataset O
, O
we O
do O
either O
a O
paragraph O
separation O
or O
line O
separation O
if O
we O
notice O
document O
separator O
absence O
in O
the O
dataset O
. O
This O
document O
separation O
is O
crucial O
as O
it O
is O
used O
in O
the O
BERT O
architecture O
to O
extract O
long O
contiguous O
sequences O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O
A O
separation O
between O
sentences O
with O
a O
new O
line O
is O
also O
required O
to O
differentiate O
each O
sentence O
. O
These O
are O
used O
by O
BERT O
to O
create O
input O
embeddings O
out O
of O
sentence O
pairs O
that O
are O
compacted O
into O
a O
single O
sequence O
. O
We O
specify O
the O
number O
of O
duplication O
factors O
for O
each O
of O
the O
datasets O
differently O
due O
to O
the O
various O
formats O
of O
the O
datasets O
that O
we O
collected O
. O
We O
create O
duplicates O
on O
datasets O
with O
the O
end O
of O
document O
separators O
with O
a O
higher O
duplication O
factor O
. O
The O
preprocessing O
method O
is O
applied O
in O
both O
the O
IndoBERT O
and O
IndoBERT O
- O
lite O
models O
. O
We O
keep O
the O
original O
form O
of O
a O
word O
to O
hold O
its O
contextual O
information O
since O
Indonesian O
words O
are O
built O
with O
rich O
morphological O
operations O
, O
such O
as O
compounding O
, O
afÔ¨Åxation O
, O
and O
reduplication O
( O
Pisceldo O
et O
al O
. O
, O
2008 O
) O
. O
In O
addition O
, O
this O
setting O
is O
also O
suitable O
for O
contextual O
pre O
- O
training O
models O
that O
leverage O
inÔ¨Çections O
to O
improve O
the O
sentence O
- O
level O
representations.(Kutuzov O
and O
Kuzmenko O
, O
2019)Twitter O
data O
contains O
speciÔ¨Åc O
details O
, O
such O
as O
usernames O
, O
hashtags O
, O
emails O
, O
and O
URL O
hyperlinks O
. O
To O
preserve O
privacy O
and O
also O
to O
reduce O
noise O
, O
this O
private O
information O
in O
the O
Twitter O
UI O
dataset O
( O
Saputri O
et O
al O
. O
, O
2018 O
) O
is O
masked O
into O
generics O
tokens O
such O
as O
< O
username O
> O
, O
< O
hashtag O
> O
, O
< O
email O
> O
and O
< O
links O
> O
. O
On O
the O
other O
hand O
, O
this O
information O
is O
discarded O
in O
the O
larger O
Twitter O
Crawl O
dataset O
. O
Vocabulary O
For O
both O
the O
IndoBERT O
and O
the O
IndoBERT O
- O
lite O
models O
, O
we O
utilize O
SentencePiece O
( O
Kudo O
and O
Richardson O
, O
2018 O
) O
with O
a O
byte O
pair O
encoding O
( O
BPE O
) O
tokenizer O
as O
the O
vocabulary O
generation O
method O
. O
We O
use O
a O
vocab O
size O
of O
30.522 O
for O
the O
IndoBERT O
models O
and O
vocab O
size O
of O
30.000 O
for O
the O
IndoBERT O
- O
lite O
models O
. O
4.2 O
Pre O
- O
training O
Setup O
All O
IndoBERT O
models O
are O
trained O
on O
TPUv38 O
in O
two O
phases O
. O
In O
the O
Ô¨Årst O
phase O
, O
we O
train O
the O
models O
with O
a O
maximum O
sequence O
length O
of O
128 O
. O
The O
training O
takes O
around O
35 O
, O
89 O
, O
38 O
and O
134 O
hours O
on O
IndoBERT O
BASE O
, O
IndoBERT O
LARGE O
, O
IndoBERT O
- O
lite O
BASE O
, O
and O
IndoBERT O
- O
lite O
LARGE O
, O
respectively O
. O
In O
the O
second O
phase O
, O
we O
continue O
the O
training O
of O
the O
IndoBERT O
models O
with O
a O
maximum O
sequence O
length O
of O
512 O
. O
It O
takes O
9 O
, O
32 O
, O
23 O
and O
45 O
hours O
on O
IndoBERT O
BASE O
, O
IndoBERT O
LARGE O
, O
IndoBERT O
- O
lite O
BASE O
, O
and O
IndoBERT O
- O
lite O
LARGE O
, O
respectively O
. O
The O
details O
of O
the O
pre O
- O
training O
hyperparameter O
settings O
are O
shown O
in O
Appendix O
D. O
IndoBERT O
We O
use O
a O
batch O
size O
of O
256 O
and O
a O
learning O
rate O
of O
2e-5 O
in O
both O
training O
phases O
for O
IndoBERT O
BASE O
, O
and O
we O
adjust O
the O
learning O
rate O
to O
1e-4 O
for O
IndoBERT O
LARGE O
to O
stabilize O
the O
training O
. O
Due O
to O
memory O
limitation O
, O
we O
scale O
down O
the O
batch O
size O
to O
128 O
and O
the O
learning O
rate O
to O
8e-5 O
in O
the O
second O
phase O
of O
the O
training O
, O
with O
a O
number O
of O
training O
steps O
adapted O
accordingly O
. O
The O
base O
and O
large O
models O
are O
trained O
using O
the O
masked O
language O
modeling O
loss O
. O
We O
limit O
the O
maximum O
prediction O
per O
sequence O
into O
20 O
tokens.848ModelClassiÔ¨Åcation O
Sequence O
Labeling O
EmoT O
SmSA O
CASA O
HoASA O
WReTE O
A O
VG O
POSP O
BaPOS O
TermA O
KEPS O
NERGrit O
NERP O
FacQA O
A O
VG O
Scratch O
57.31 O
67.35 O
67.15 O
76.28 O
64.35 O
66.49 O
86.78 O
70.24 O
70.36 O
39.40 O
5.80 O
30.66 O
5.00 O
44.03 O
fastText O
- O
cc O
- O
id O
65.36 O
76.92 O
79.02 O
85.32 O
67.36 O
74.79 O
94.35 O
79.85 O
76.12 O
56.39 O
37.32 O
46.46 O
15.29 O
57.97 O
fastText O
- O
indo4b O
69.23 O
82.13 O
82.20 O
85.88 O
60.42 O
75.97 O
94.94 O
81.77 O
74.43 O
56.70 O
38.69 O
46.79 O
14.65 O
58.28 O
mBERT O
67.30 O
84.14 O
72.23 O
84.63 O
84.40 O
78.54 O
91.85 O
83.25 O
89.51 O
64.31 O
75.02 O
69.27 O
61.29 O
76.36 O
XLM O
- O
MLM O
65.75 O
86.33 O
82.17 O
88.89 O
64.35 O
77.50 O
95.87 O
88.40 O
90.55 O
65.35 O
74.75 O
75.06 O
62.15 O
78.88 O
XLM O
- O
R O
BASE O
71.15 O
91.39 O
91.71 O
91.57 O
79.95 O
85.15 O
95.16 O
84.64 O
90.99 O
68.82 O
79.09 O
75.03 O
64.58 O
79.76 O
XLM O
- O
R O
LARGE O
78.51 O
92.35 O
92.40 O
94.27 O
83.82 O
88.27 O
92.73 O
87.03 O
91.45 O
70.88 O
78.26 O
78.52 O
74.61 O
81.92 O
IndoBERT O
- O
lite O
BASE‚Ä†73.88 O
90.85 O
89.68 O
88.07 O
82.17 O
84.93 O
91.40 O
75.10 O
89.29 O
69.02 O
66.62 O
46.58 O
54.99 O
70.43 O
+ O
phase O
two O
72.27 O
90.29 O
87.63 O
87.62 O
83.62 O
84.29 O
90.05 O
77.59 O
89.19 O
69.13 O
66.71 O
50.52 O
49.18 O
70.34 O
IndoBERT O
BASE‚Ä†75.48 O
87.73 O
93.23 O
92.07 O
78.55 O
85.41 O
95.26 O
87.09 O
90.73 O
70.36 O
69.87 O
75.52 O
53.45 O
77.47 O
+ O
phase O
two O
76.28 O
87.66 O
93.24 O
92.70 O
78.68 O
85.71 O
95.23 O
85.72 O
91.13 O
69.17 O
67.42 O
75.68 O
57.06 O
77.34 O
IndoBERT O
- O
lite O
LARGE O
75.19 O
88.66 O
90.99 O
89.53 O
78.98 O
84.67 O
91.56 O
83.74 O
90.23 O
67.89 O
71.19 O
74.37 O
65.50 O
77.78 O
+ O
phase O
two O
70.80 O
88.61 O
88.13 O
91.05 O
85.41 O
84.80 O
94.53 O
84.91 O
90.72 O
68.55 O
73.07 O
74.89 O
62.87 O
78.51 O
IndoBERT O
LARGE O
77.08 O
92.72 O
95.69 O
93.75 O
82.91 O
88.43 O
95.71 O
90.35 O
91.87 O
71.18 O
77.60 O
79.25 O
62.48 O
81.21 O
+ O
phase O
two O
79.47 O
92.03 O
94.94 O
93.38 O
80.30 O
88.02 O
95.34 O
87.36 O
92.14 O
71.27 O
76.63 O
77.99 O
68.09 O
81.26 O
Table O
5 O
: O
Results O
of O
baseline O
models O
with O
best O
performing O
conÔ¨Åguration O
on O
the O
IndoNLU O
benchmark O
. O
Extensive O
experimental O
results O
are O
shown O
in O
Appendix O
E. O
Bold O
numbers O
are O
the O
best O
results O
among O
all.‚Ä†The O
IndoBERT O
models O
are O
trained O
using O
two O
training O
phases O
. O
IndoBERT O
- O
lite O
We O
follow O
the O
ALBERT O
pretraining O
hyperparameters O
setup O
( O
Lan O
et O
al O
. O
, O
2020 O
) O
to O
pre O
- O
train O
the O
IndoBERT O
- O
lite O
models O
. O
We O
limit O
the O
maximum O
prediction O
per O
sequence O
into O
20 O
tokens O
on O
the O
models O
, O
pre O
- O
training O
with O
whole O
word O
masked O
loss O
. O
We O
train O
the O
base O
model O
with O
a O
batch O
size O
of O
4096 O
in O
the O
Ô¨Årst O
phase O
, O
and O
1024 O
in O
the O
second O
phase O
. O
Since O
we O
have O
a O
limitation O
in O
computation O
power O
, O
we O
use O
a O
smaller O
batch O
size O
of O
1024 O
in O
the O
Ô¨Årst O
phase O
and O
256 O
in O
the O
second O
phase O
in O
training O
our O
large O
model O
. O
5 O
Results O
and O
Analysis O
In O
this O
section O
, O
we O
show O
the O
results O
of O
the O
IndoNLU O
benchmark O
and O
analyze O
the O
performance O
of O
our O
models O
in O
terms O
of O
downstream O
tasks O
score O
and O
performance O
- O
space O
trade O
- O
off O
. O
In O
addition O
, O
we O
show O
an O
analysis O
of O
the O
effectiveness O
of O
using O
our O
collected O
data O
compared O
to O
existing O
baselines O
. O
5.1 O
Benchmark O
Results O
Overall O
Performance O
As O
mentioned O
in O
Section O
3 O
, O
we O
Ô¨Åne O
- O
tune O
all O
baseline O
models O
mentioned O
in O
Section O
3.3 O
, O
and O
evaluate O
the O
model O
performance O
over O
all O
tasks O
, O
grouped O
into O
two O
categories O
, O
classiÔ¨Åcation O
and O
sequence O
labeling O
. O
We O
can O
see O
in O
Table O
5 O
, O
that O
IndoBERT O
LARGE O
, O
XLM O
- O
R O
LARGE O
, O
and O
IndoBERT O
BASE O
achieve O
the O
top-3 O
best O
performance O
results O
on O
the O
classiÔ¨Åcation O
tasks O
, O
and O
XLM O
- O
R O
LARGE O
, O
IndoBERT O
LARGE O
, O
and O
XLM O
- O
R O
BASE O
achieve O
the O
top-3 O
best O
performance O
results O
on O
the O
sequence O
labeling O
tasks O
. O
The O
experimental O
results O
also O
suggest O
that O
larger O
models O
have O
a O
performance O
advantage O
over O
smaller O
models O
. O
It O
is O
also O
evidentthat O
all O
pre O
- O
trained O
models O
outperform O
the O
scratch O
model O
, O
which O
shows O
the O
effectiveness O
of O
model O
pretraining O
. O
Another O
interesting O
observation O
is O
that O
all O
contextualized O
pre O
- O
trained O
models O
outperform O
word O
embeddings O
- O
based O
models O
by O
signiÔ¨Åcant O
margins O
. O
This O
shows O
the O
superiority O
of O
the O
contextualized O
embeddings O
approach O
over O
the O
word O
embeddings O
approach O
. O
5.2 O
Performance O
- O
Space O
Trade O
- O
off O
Figure O
1 O
shows O
the O
model O
performance O
with O
respect O
to O
the O
number O
of O
parameters O
. O
We O
can O
see O
two O
large O
clusters O
. O
On O
the O
bottom O
left O
, O
the O
scratch O
and O
fastText O
models O
appear O
, O
and O
they O
have O
the O
lowest O
F1 O
scores O
and O
the O
least O
Ô¨Çoating O
points O
in O
the O
inference O
time O
. O
On O
the O
top O
right O
, O
we O
can O
see O
that O
the O
pre O
- O
trained O
models O
achieve O
decent O
performance O
, O
but O
in O
the O
inference O
time O
, O
they O
incur O
a O
high O
computation O
cost O
. O
Interestingly O
, O
in O
the O
top O
- O
left O
region O
, O
we O
can O
see O
the O
IndoBERT O
- O
lite O
models O
, O
which O
achieve O
similar O
performance O
to O
the O
IndoBERT O
models O
, O
but O
with O
many O
fewer O
parameters O
and O
a O
slightly O
lower O
computation O
cost O
. O
5.3 O
Multilingual O
vs. O
Monolingual O
Models O
Based O
on O
Table O
5 O
, O
we O
can O
conclude O
that O
contextualized O
monolingual O
models O
outperform O
contextualized O
multilingual O
models O
on O
the O
classiÔ¨Åcation O
tasks O
by O
a O
large O
margin O
, O
but O
on O
the O
sequence O
labeling O
tasks O
, O
multilingual O
models O
tend O
to O
perform O
better O
compared O
to O
monolingual O
models O
and O
even O
perform O
much O
better O
on O
the O
NERGrit O
and O
FacQA O
tasks O
. O
As O
shown O
in O
Appendix O
A O
, O
both O
the O
NERGrit O
and O
FacQA O
tasks O
contain O
many O
entity O
names O
which849Figure O
1 O
: O
Performance O
- O
space O
trade O
- O
off O
for O
all O
baseline O
models O
on O
classiÔ¨Åcation O
tasks O
( O
left O
) O
and O
sequence O
labeling O
tasks O
( O
right O
) O
. O
We O
take O
the O
best O
model O
for O
each O
model O
size O
. O
2L O
, O
4L O
, O
and O
6L O
denote O
the O
number O
of O
layers O
used O
in O
the O
model O
. O
The O
size O
of O
the O
dots O
represents O
the O
number O
of O
FLOPs O
of O
the O
model O
. O
We O
use O
python O
package O
thop O
taken O
from O
https://pypi.org/project/thop/ O
to O
calculate O
the O
number O
of O
FLOPs O
. O
come O
from O
other O
languages O
, O
especially O
English O
. O
These O
facts O
suggest O
that O
monolingual O
models O
capture O
the O
semantic O
meaning O
of O
a O
word O
better O
than O
multilingual O
models O
, O
but O
multilingual O
models O
identify O
foreign O
terms O
better O
than O
monolingual O
models O
. O
5.4 O
Effectiveness O
of O
Indo4B O
Dataset O
Tasks O
# O
Layer O
fastText O
- O
cc O
- O
id O
fastText O
- O
indo4b O
ClassiÔ¨Åcation2 O
72.00 O
74.17 O
4 O
74.79 O
75.97 O
6 O
74.80 O
76.00 O
Sequence O
Labeling2 O
56.26 O
55.55 O
4 O
57.97 O
58.28 O
6 O
56.82 O
57.42 O
Table O
6 O
: O
Experiment O
results O
on O
fastText O
embeddings O
on O
IndoNLU O
tasks O
with O
different O
number O
of O
transformer O
layers O
According O
to O
Grave O
et O
al O
. O
( O
2018 O
) O
, O
Common O
Crawl O
is O
a O
corpus O
containing O
over O
24 O
TB.8We O
estimate O
the O
size O
of O
the O
CC O
- O
ID O
dataset O
to O
be O
around O
‚âà180 O
GB O
uncompressed O
. O
Although O
the O
Indo4B O
dataset O
size O
is O
much O
smaller O
( O
‚âà23 O
GB O
) O
, O
Table O
6 O
shows O
us O
that O
the O
fastText O
models O
trained O
on O
the O
Indo4B O
dataset O
( O
fastText O
- O
indo4b O
) O
consistently O
outperform O
fastText O
models O
trained O
on O
the O
CC O
- O
ID O
dataset O
( O
fastText O
- O
cc O
- O
id O
) O
in O
both O
classiÔ¨Åcation O
and O
sequence O
labeling O
tasks O
in O
all O
model O
settings O
. O
Based O
8https://commoncrawl.github.io/cc-crawlstatistics/plots/languageson O
Table O
5 O
, O
the O
fact O
that O
fastText O
- O
indo4b O
outperforms O
fastText O
- O
cc O
- O
id O
with O
a O
higher O
score O
on O
10 O
out O
of O
12 O
tasks O
suggests O
that O
a O
relatively O
smaller O
dataset O
( O
‚âà23 O
GB O
) O
can O
signiÔ¨Åcantly O
outperform O
its O
larger O
counterpart O
( O
‚âà180 O
GB O
) O
. O
We O
conclude O
that O
even O
though O
our O
Indo4B O
dataset O
is O
smaller O
, O
it O
covers O
more O
variety O
of O
the O
Indonesian O
language O
and O
has O
better O
text O
quality O
compared O
to O
the O
CC O
- O
ID O
dataset O
. O
5.5 O
Effectiveness O
of O
IndoBERT O
and O
IndoBERT O
- O
lite O
Table O
5 O
shows O
that O
the O
IndoBERT O
models O
outperform O
the O
multilingual O
models O
on O
8 O
out O
of O
12 O
tasks O
. O
In O
general O
, O
the O
IndoBERT O
models O
achieve O
the O
highest O
average O
score O
on O
the O
classiÔ¨Åcation O
task O
. O
We O
conjecture O
that O
monolingual O
models O
learn O
better O
sentiment O
- O
level O
semantics O
on O
both O
colloquial O
and O
formal O
language O
styles O
than O
multilingual O
models O
, O
even O
though O
the O
IndoBERT O
models O
‚Äô O
size O
is O
40 O
% O
‚Äì O
60 O
% O
smaller O
. O
On O
sequence O
labeling O
tasks O
, O
the O
IndoBERT O
models O
can O
not O
perform O
as O
well O
as O
the O
multilingual O
models O
( O
XLM O
- O
R O
) O
in O
three O
sequence O
labeling O
tasks O
: O
POSP O
, O
NERGrit O
, O
and O
FacQA O
. O
One O
of O
the O
possible O
explanations O
is O
that O
these O
datasets O
have O
many O
borrowed O
words O
from O
English O
, O
and O
multilingual O
models O
have O
the O
advantage O
in O
transferring O
learning O
from O
English O
. O
Meanwhile O
, O
the O
IndoBERT O
- O
lite O
models O
achieve O
a O
decent O
performance O
on O
both O
classiÔ¨Åcation O
and O
sequence O
labeling O
tasks O
with O
the O
advantage O
of O
compact O
size O
. O
Interestingly O
, O
the O
IndoBERT O
- O
lite O
LARGE850model O
performance O
is O
on O
par O
with O
that O
of O
XLM O
- O
R O
BASE O
while O
having O
16x O
fewer O
parameters O
. O
We O
also O
observe O
that O
increasing O
the O
maximum O
sequence O
length O
to O
512 O
in O
phase O
two O
improves O
the O
performance O
on O
the O
sequence O
labeling O
tasks O
. O
Moreover O
, O
training O
the O
model O
with O
longer O
input O
sequences O
enables O
it O
to O
learn O
temporal O
information O
from O
a O
given O
text O
input O
. O
6 O
Conclusion O
We O
introduce O
the O
Ô¨Årst O
Indonesian O
benchmark O
for O
natural O
language O
understanding O
, O
IndoNLU O
, O
which O
consists O
of O
12 O
tasks O
, O
with O
different O
levels O
of O
difÔ¨Åculty O
, O
domains O
, O
and O
styles O
. O
To O
establish O
a O
strong O
baseline O
, O
we O
collect O
large O
clean O
Indonesian O
datasets O
into O
a O
dataset O
called O
Indo4B O
, O
which O
we O
use O
for O
training O
monolingual O
contextual O
pre O
- O
trained O
language O
models O
, O
called O
IndoBERT O
and O
IndoBERTlite O
. O
We O
demonstrate O
the O
effectiveness O
of O
our O
dataset O
and O
our O
pre O
- O
trained O
models O
in O
capturing O
sentence O
- O
level O
semantics O
, O
and O
apply O
them O
to O
the O
classiÔ¨Åcation O
and O
sequence O
labeling O
tasks O
. O
To O
help O
with O
the O
reproducibility O
of O
the O
benchmark O
, O
we O
release O
the O
pre O
- O
trained O
models O
, O
including O
the O
collected O
data O
and O
code O
. O
In O
order O
to O
accelerate O
the O
community O
engagement O
and O
benchmark O
transparency O
, O
we O
have O
set O
up O
a O
leaderboard O
website O
for O
the O
NLP O
community O
. O
We O
publish O
our O
leaderboard O
website O
at O
https://indobenchmark.com/ O
. O
Acknowledgments O
We O
want O
to O
thank O
Cahya O
Wirawan O
, O
Pallavi O
Jain O
, O
Irene O
Gianni O
, O
Martijn O
Wieriks O
, O
Ade O
Romadhony O
, O
and O
Andrea O
Madotto O
for O
insightful O
discussions O
about O
this O
project O
. O
We O
sincerely O
thank O
the O
three O
anonymous O
reviewers O
for O
their O
insightful O
comments O
on O
our O
paper O
. O
Abstract O
The O
gaze O
behaviour O
of O
a O
reader O
is O
helpful O
in O
solving O
several O
NLP O
tasks O
such O
as O
automatic O
essay O
grading O
. O
However O
, O
collecting O
gaze O
behaviour O
from O
readers O
is O
costly O
in O
terms O
of O
time O
and O
money O
. O
In O
this O
paper O
, O
we O
propose O
a O
way O
to O
improve O
automatic O
essay O
grading O
using O
gaze O
behaviour O
, O
which O
is O
learnt O
at O
run O
time O
using O
a O
multi O
- O
task O
learning O
framework O
. O
To O
demonstrate O
the O
efÔ¨Åcacy O
of O
this O
multi O
- O
task O
learning O
based O
approach O
to O
automatic O
essay O
grading O
, O
we O
collect O
gaze O
behaviour O
for O
48 O
essays O
across O
4 O
essay O
sets O
, O
and O
learn O
gaze O
behaviour O
for O
the O
rest O
of O
the O
essays O
, O
numbering O
over O
7000 O
essays O
. O
Using O
the O
learnt O
gaze O
behaviour O
, O
we O
can O
achieve O
a O
statistically O
signiÔ¨Åcant O
improvement O
in O
performance O
over O
the O
state O
- O
of O
- O
the O
- O
art O
system O
for O
the O
essay O
sets O
where O
we O
have O
gaze O
data O
. O
We O
also O
achieve O
a O
statistically O
signiÔ¨Åcant O
improvement O
for O
4 O
other O
essay O
sets O
, O
numbering O
about O
6000 O
essays O
, O
where O
we O
have O
no O
gaze O
behaviour O
data O
available O
. O
Our O
approach O
establishes O
that O
learning O
gaze O
behaviour O
improves O
automatic O
essay O
grading O
. O
1 O
Introduction O
Collecting O
a O
reader O
‚Äôs O
psychological O
input O
can O
be O
very O
beneÔ¨Åcial O
to O
a O
number O
of O
Natural O
Language O
Processing O
( O
NLP O
) O
tasks O
, O
like O
complexity O
( O
Mishra O
et O
al O
. O
, O
2017 O
; O
Gonz O
¬¥ O
alez O
- O
Gardu O
Àúno O
and O
S√∏gaard O
, O
2017 O
) O
, O
sentence O
simpliÔ¨Åcation O
( O
Klerke O
et O
al O
. O
, O
2016 O
) O
, O
text O
understanding O
( O
Mishra O
et O
al O
. O
, O
2016 O
) O
, O
text O
quality O
( O
Mathias O
et O
al O
. O
, O
2018 O
) O
, O
parsing O
( O
Hale O
et O
al O
. O
, O
2018 O
) O
, O
etc O
. O
This O
psychological O
information O
can O
be O
extracted O
using O
devices O
like O
eye O
- O
trackers O
, O
and O
electroencephalogram O
( O
EEG O
) O
machines O
. O
However O
, O
one O
of O
the O
challenges O
in O
using O
reader O
‚Äôs O
information O
involves O
collecting O
the O
psycholinguistic O
data O
itself O
. O
In O
this O
paper O
, O
we O
choose O
the O
task O
of O
automatic O
essay O
grading O
and O
show O
how O
we O
can O
predict O
the O
score O
that O
a O
human O
rater O
would O
give O
using O
both O
text O
andlearnt O
gaze O
behaviour O
. O
An O
essay O
is O
a O
piece O
oftext O
, O
written O
in O
response O
to O
a O
topic O
, O
called O
a O
prompt O
. O
Automatic O
essay O
grading O
is O
assigning O
a O
score O
to O
the O
essay O
using O
a O
machine O
. O
An O
essay O
set O
is O
a O
set O
of O
essays O
written O
in O
response O
to O
the O
same O
prompt O
. O
Multi O
- O
task O
learning O
( O
Caruana O
, O
1998 O
) O
is O
a O
machine O
learning O
paradigm O
where O
we O
utilize O
auxiliary O
tasks O
to O
aid O
in O
solving O
a O
primary O
task O
. O
This O
is O
done O
by O
exploiting O
similarities O
between O
the O
primary O
task O
and O
the O
auxiliary O
tasks O
. O
Scoring O
the O
essay O
is O
the O
primary O
task O
andlearning O
gaze O
behaviour O
is O
the O
auxiliary O
task O
. O
Using O
gaze O
behaviour O
for O
a O
very O
small O
number O
of O
essays O
( O
less O
than O
0.7 O
% O
of O
the O
essays O
in O
an O
essay O
set O
) O
, O
we O
see O
an O
improvement O
in O
predicting O
the O
overall O
score O
of O
the O
essays O
. O
We O
also O
use O
our O
gaze O
behaviour O
dataset O
to O
run O
experiments O
on O
unseen O
essay O
sets O
- O
i.e. O
,essay O
sets O
which O
have O
no O
gaze O
behaviour O
data O
- O
and O
observe O
improvements O
in O
the O
system O
‚Äôs O
performance O
in O
automatically O
grading O
essays O
. O
Contributions O
The O
main O
contribution O
of O
our O
paper O
is O
describing O
how O
we O
use O
gaze O
behaviour O
information O
, O
in O
a O
multi O
- O
task O
learning O
framework O
, O
to O
automatically O
score O
essays O
outperforming O
the O
stateof O
- O
the O
- O
art O
systems O
. O
We O
will O
also O
release O
the O
gaze O
behaviour O
dataset1and O
code2- O
the O
Ô¨Årst O
of O
its O
kind O
, O
for O
automatic O
essay O
grading O
- O
to O
facilitate O
further O
research O
in O
using O
gaze O
behaviour O
for O
automatic O
essay O
grading O
and O
other O
similar O
NLP O
tasks O
. O
1.1 O
Gaze O
Behaviour O
Terminology O
AnInterest O
Area O
( O
IA O
) O
is O
an O
area O
of O
the O
screen O
that O
we O
are O
interested O
in O
. O
These O
areas O
are O
where O
some O
text O
is O
displayed O
, O
and O
not O
the O
white O
background O
on O
the O
left O
/ O
right O
, O
as O
well O
as O
above O
/ O
below O
the O
text O
. O
Each O
word O
is O
a O
separate O
and O
unique O
IA O
. O
1Gaze O
behaviour O
dataset O
: O
http://www.cfilt.iitb O
. O
ac.in/cognitive-nlp/ O
Essays O
: O
https://www.kaggle.com/c/asap-aes O
2https://github.com/lwsam/ASAP-Gaze858AFixation O
is O
an O
event O
when O
the O
reader O
‚Äôs O
eye O
is O
focused O
on O
a O
part O
of O
the O
screen O
. O
For O
our O
experiments O
, O
we O
are O
concerned O
only O
with O
Ô¨Åxations O
that O
occur O
within O
the O
interest O
areas O
. O
Fixations O
that O
occur O
in O
the O
background O
are O
ignored O
. O
ASaccade O
is O
the O
path O
of O
the O
eye O
movement O
, O
as O
it O
goes O
from O
one O
Ô¨Åxation O
to O
the O
next O
. O
There O
are O
two O
types O
of O
saccades O
- O
Progressions O
and O
Regressions O
. O
Progressions O
are O
saccades O
where O
the O
reader O
moves O
from O
the O
current O
interest O
area O
to O
a O
later O
one O
. O
Regressions O
are O
saccades O
where O
the O
reader O
moves O
from O
the O
current O
interest O
area O
to O
an O
earlier O
one O
. O
The O
rest O
of O
the O
paper O
is O
organized O
as O
follows O
. O
Section O
2 O
describes O
our O
motivation O
for O
using O
eyetracking O
and O
learning O
gaze O
behaviour O
from O
readers O
, O
over O
unseen O
texts O
. O
Section O
3 O
describes O
some O
of O
the O
related O
work O
in O
the O
area O
of O
automatic O
essay O
grading O
, O
eye O
tracking O
and O
multi O
- O
task O
learning O
. O
Section O
4 O
describes O
the O
gaze O
behaviour O
attributes O
used O
in O
our O
experiments O
, O
and O
the O
intuition O
behind O
them O
. O
We O
describe O
our O
dataset O
creation O
and O
experiment O
setup O
in O
Section O
5 O
. O
In O
Section O
6 O
, O
we O
report O
our O
results O
and O
present O
a O
detailed O
analysis O
. O
We O
present O
our O
conclusions O
and O
discuss O
possible O
future O
work O
in O
Section O
7 O
. O
2 O
Motivation O
Mishra O
and O
Bhattacharyya O
( O
2018 O
) O
, O
for O
instance O
, O
describe O
a O
lot O
of O
research O
in O
solving O
multiple O
problems O
in O
NLP O
using O
gaze O
behaviour O
of O
readers O
. O
However O
, O
most O
of O
their O
work O
involves O
collecting O
the O
gaze O
behaviour O
data O
Ô¨Årst O
, O
and O
then O
splitting O
the O
data O
into O
training O
and O
testing O
data O
, O
before O
performing O
their O
experiments O
. O
While O
their O
work O
did O
show O
signiÔ¨Åcant O
improvements O
over O
baseline O
approaches O
, O
across O
multiple O
NLP O
tasks O
, O
collecting O
the O
gaze O
behaviour O
data O
would O
be O
quite O
expensive O
, O
both O
in O
terms O
of O
time O
and O
money O
. O
Therefore O
, O
we O
ask O
ourselves O
: O
‚Äú O
Can O
we O
learn O
gaze O
behaviour O
, O
using O
a O
small O
amount O
of O
seed O
data O
, O
to O
help O
solve O
an O
NLP O
task O
? O
‚Äù O
In O
order O
to O
use O
gaze O
behaviour O
on O
a O
large O
scale O
, O
we O
need O
to O
be O
able O
to O
learn O
it O
, O
since O
we O
can O
not O
ask O
a O
user O
to O
read O
texts O
every O
time O
we O
wish O
to O
use O
gaze O
behaviour O
data O
. O
Mathias O
et O
al O
. O
( O
2018 O
) O
describe O
using O
gaze O
behaviour O
to O
predict O
how O
a O
reader O
would O
rate O
a O
piece O
of O
text O
( O
which O
is O
similar O
to O
our O
chosen O
application O
) O
. O
Since O
they O
showed O
that O
gaze O
behaviour O
can O
help O
in O
predicting O
text O
quality O
, O
we O
use O
multi O
- O
task O
learning O
to O
simultaneously O
learn O
gaze O
behaviour O
information O
( O
auxiliary O
task O
) O
as O
well O
as O
score O
the O
essay O
( O
theprimary O
task O
) O
. O
However O
, O
they O
collect O
all O
their O
gaze O
behaviour O
data O
a O
priori O
, O
while O
we O
try O
to O
learn O
the O
gaze O
behaviour O
of O
a O
reader O
and O
use O
what O
we O
learn O
from O
our O
system O
, O
for O
grading O
the O
essays O
. O
Hence O
, O
while O
they O
showed O
that O
gaze O
behaviour O
could O
help O
in O
predicting O
how O
a O
reader O
would O
score O
a O
text O
, O
their O
approach O
requires O
a O
reader O
to O
read O
the O
text O
, O
while O
our O
approach O
does O
not O
do O
so O
, O
during O
testing O
/ O
deployment O
. O
3 O
Related O
Work O
3.1 O
Automatic O
Essay O
Grading O
( O
AEG O
) O
The O
very O
Ô¨Årst O
AEG O
system O
was O
proposed O
by O
Page O
( O
1966 O
) O
. O
Since O
then O
, O
there O
have O
been O
a O
lot O
of O
other O
AEG O
systems O
( O
see O
Shermis O
and O
Burstein O
( O
2013 O
) O
for O
more O
details O
) O
. O
In O
2012 O
, O
the O
Hewlett O
Foundation O
released O
a O
dataset O
called O
the O
Automatic O
Student O
Assessment O
Prize O
( O
ASAP O
) O
AEG O
dataset O
. O
The O
dataset O
contains O
about O
13,000 O
essays O
across O
eight O
different O
essay O
sets O
. O
We O
discuss O
more O
about O
that O
dataset O
later O
. O
With O
the O
availability O
of O
a O
large O
dataset O
, O
there O
has O
been O
a O
lot O
of O
research O
, O
especially O
using O
neural O
networks O
, O
in O
automatically O
grading O
essays O
- O
like O
using O
Long O
Short O
Term O
Memory O
( O
LSTM O
) O
Networks O
( O
Taghipour O
and O
Ng O
, O
2016 O
; O
Tay O
et O
al O
. O
, O
2018 O
) O
, O
Convolutional O
Neural O
Networks O
( O
CNNs O
) O
( O
Dong O
and O
Zhang O
, O
2016 O
) O
, O
or O
both O
( O
Dong O
et O
al O
. O
, O
2017 O
) O
. O
Zhang O
and O
Litman O
( O
2018 O
) O
improve O
on O
the O
results O
of O
Dong O
et O
al O
. O
( O
2017 O
) O
using O
co O
- O
attention O
between O
the O
source O
article O
and O
the O
essay O
for O
one O
of O
the O
types O
of O
essay O
sets O
. O
3.2 O
Eye O
- O
Tracking O
Capturing O
the O
gaze O
behaviour O
of O
readers O
has O
been O
found O
to O
be O
quite O
useful O
in O
improving O
the O
performance O
of O
NLP O
tasks O
( O
Mishra O
and O
Bhattacharyya O
, O
2018 O
) O
. O
The O
main O
idea O
behind O
using O
gaze O
behaviour O
is O
the O
eye O
- O
mind O
hypothesis O
( O
Just O
and O
Carpenter O
, O
1980 O
) O
, O
which O
states O
that O
whatever O
text O
the O
eye O
reads O
, O
that O
is O
what O
the O
mind O
processes O
. O
This O
hypothesis O
has O
led O
to O
a O
large O
body O
of O
work O
in O
psycholinguistic O
research O
that O
shows O
a O
relationship O
between O
text O
processing O
and O
gaze O
behaviour O
. O
Mishra O
and O
Bhattacharyya O
( O
2018 O
) O
also O
describe O
some O
of O
the O
ways O
that O
eye O
- O
tracking O
can O
be O
used O
for O
multiple O
NLP O
tasks O
like O
translation O
complexity O
, O
sentiment O
analysis O
, O
etc O
. O
Research O
has O
been O
done O
on O
using O
gaze O
behaviour O
at O
run O
time O
to O
solve O
downstream O
NLP O
tasks O
like O
sentence O
simpliÔ¨Åcation O
( O
Klerke O
et O
al O
. O
, O
2016 O
) O
, O
readability O
( O
Gonz O
¬¥ O
alez O
- O
Gardu O
Àúno O
and O
S√∏gaard O
, O
2018 O
; O
Singh859et O
al O
. O
, O
2016 O
) O
, O
part O
- O
of O
- O
speech O
tagging O
( O
Barrett O
et O
al O
. O
, O
2016 O
) O
, O
sentiment O
analysis O
( O
Mishra O
et O
al O
. O
, O
2018 O
; O
Barrett O
et O
al O
. O
, O
2018 O
; O
Long O
et O
al O
. O
, O
2019 O
) O
, O
grammatical O
error O
detection O
( O
Barrett O
et O
al O
. O
, O
2018 O
) O
, O
hate O
speech O
detection O
( O
Barrett O
et O
al O
. O
, O
2018 O
) O
and O
named O
entity O
recognition O
( O
Hollenstein O
and O
Zhang O
, O
2019 O
) O
. O
Different O
strategies O
have O
been O
adopted O
to O
alleviate O
the O
need O
for O
gaze O
behaviour O
at O
run O
time O
. O
Barrett O
et O
al O
. O
( O
2016 O
) O
use O
token O
level O
averages O
of O
gaze O
features O
at O
run O
time O
from O
the O
Dundee O
Corpus O
( O
Kennedy O
et O
al O
. O
, O
2003 O
) O
, O
to O
alleviate O
the O
need O
for O
gaze O
behaviour O
at O
run O
time O
. O
Singh O
et O
al O
. O
( O
2016 O
) O
and O
Long O
et O
al O
. O
( O
2019 O
) O
predict O
gaze O
behaviour O
at O
the O
tokenlevel O
prior O
to O
using O
it O
at O
run O
time O
. O
Mishra O
et O
al O
. O
( O
2018 O
) O
, O
Gonz O
¬¥ O
alez O
- O
Gardu O
Àúno O
and O
S√∏gaard O
( O
2018 O
) O
, O
Barrett O
et O
al O
. O
( O
2018 O
) O
, O
and O
Klerke O
et O
al O
. O
( O
2016 O
) O
, O
use O
multi O
- O
task O
learning O
to O
learn O
gaze O
behaviour O
along O
with O
solving O
the O
primary O
NLP O
task O
. O
4 O
Gaze O
Behaviour O
Attributes O
In O
our O
experiments O
, O
we O
use O
only O
a O
subset O
of O
gaze O
behaviour O
attributes O
described O
by O
Mathias O
et O
al O
. O
( O
2018 O
) O
because O
most O
of O
the O
other O
attributes O
( O
like O
Second O
Fixation O
Duration3 O
) O
were O
mostly O
0 O
, O
for O
most O
of O
the O
interest O
areas O
, O
and O
learning O
over O
them O
would O
not O
have O
yielded O
any O
meaningful O
results O
. O
Fixation O
Based O
Attributes O
In O
our O
experiments O
, O
we O
use O
the O
Dwell O
Time O
( O
DT O
) O
and O
First O
Fixation O
Duration O
( O
FFD O
) O
as O
Ô¨Åxation O
- O
based O
gaze O
behaviour O
attributes O
. O
Dwell O
Time O
is O
the O
total O
amount O
of O
time O
a O
user O
spends O
focusing O
on O
an O
interest O
area O
. O
First O
Fixation O
Duration O
is O
amount O
of O
time O
that O
a O
reader O
initially O
focuses O
on O
an O
interest O
area O
. O
Larger O
values O
for O
Ô¨Åxation O
durations O
( O
for O
both O
DT O
and O
FFD O
) O
usually O
indicate O
that O
a O
word O
could O
be O
wrong O
( O
either O
a O
spelling O
mistake O
or O
grammar O
error O
) O
. O
Errors O
would O
force O
a O
reader O
to O
pause O
, O
as O
they O
try O
to O
understand O
why O
the O
error O
was O
made O
( O
For O
example O
, O
if O
the O
writer O
wrote O
‚Äú O
short O
cat O
‚Äù O
instead O
of O
‚Äú O
short O
cut O
‚Äù O
. O
Saccade O
Based O
Attribute O
In O
addition O
to O
the O
Fixation O
based O
attributes O
, O
we O
also O
look O
at O
a O
regressionbased O
attribute O
- O
IsRegression O
( O
IR O
) O
. O
This O
attribute O
is O
used O
to O
check O
whether O
or O
not O
a O
regression O
occurred O
from O
a O
given O
interest O
area O
. O
We O
do O
n‚Äôt O
focus O
on O
progression O
- O
based O
attributes O
, O
because O
the O
usual O
direction O
of O
reading O
is O
progressions O
. O
We O
are O
mainly O
concerned O
with O
regressions O
because O
they O
often O
occur O
when O
there O
is O
a O
mistake O
, O
or O
a O
need O
for O
disam3The O
duration O
of O
the O
Ô¨Åxation O
when O
the O
reader O
Ô¨Åxates O
on O
an O
interest O
area O
for O
the O
second O
time.biguation O
( O
like O
trying O
to O
resolve O
the O
antecedent O
of O
an O
anaphora O
) O
. O
Interest O
Area O
Based O
Attributes O
Lastly O
, O
we O
also O
use O
IA O
- O
based O
attributes O
, O
such O
as O
the O
Run O
Count O
( O
RC O
) O
and O
if O
the O
IA O
was O
Skipped O
( O
Skip O
) O
. O
The O
Run O
Count O
is O
the O
number O
of O
times O
a O
particular O
IA O
was O
Ô¨Åxated O
on O
, O
and O
Skip O
is O
whether O
or O
not O
the O
IA O
was O
skipped O
. O
A O
well O
- O
written O
text O
would O
be O
read O
more O
easily O
, O
meaning O
a O
lower O
RC O
, O
and O
higher O
Skip O
( O
Mathias O
et O
al O
. O
, O
2018 O
) O
. O
5 O
Dataset O
and O
Experiment O
Setup O
5.1 O
Essay O
Dataset O
Details O
We O
perform O
our O
experiments O
on O
the O
ASAP O
AEG O
dataset O
. O
The O
dataset O
has O
approximately O
13,000 O
essays O
, O
across O
8 O
essay O
sets O
. O
Table O
1 O
reports O
the O
statistics O
of O
the O
dataset O
in O
terms O
of O
Number O
of O
Essays O
, O
Score O
Range O
, O
and O
Mean O
Word O
Count O
. O
The O
Ô¨Årst O
4 O
rows O
in O
Table O
1 O
are O
source O
- O
dependent O
response O
( O
SDR O
) O
essay O
sets O
, O
which O
we O
use O
to O
collect O
our O
gaze O
behaviour O
data O
. O
The O
other O
essays O
are O
used O
as O
unseen O
essay O
sets O
. O
SDRs O
are O
essays O
written O
in O
response O
to O
a O
question O
about O
a O
source O
article O
. O
For O
example O
, O
one O
of O
the O
essay O
sets O
that O
we O
use O
is O
based O
on O
an O
article O
called O
The O
Mooring O
Mast O
, O
by O
Marcia O
Amidon O
L O
¬®usted4 O
. O
5.2 O
Evaluation O
Metric O
Essay O
Set O
Number O
of O
Essays O
Score O
Range O
Mean O
Word O
Count O
Prompt O
3 O
1726 O
0 O
- O
3 O
150 O
Prompt O
4 O
1770 O
0 O
- O
3 O
150 O
Prompt O
5 O
1805 O
0 O
- O
4 O
150 O
Prompt O
6 O
1800 O
0 O
- O
4 O
150 O
Prompt O
1 O
1783 O
2 O
- O
12 O
350 O
Prompt O
2 O
1800 O
1 O
- O
6 O
350 O
Prompt O
7 O
1569 O
0 O
- O
30 O
250 O
Prompt O
8 O
723 O
0 O
- O
60 O
650 O
Total O
12976 O
0 O
- O
60 O
250 O
Table O
1 O
: O
Statistics O
of O
the O
8 O
essay O
sets O
from O
the O
ASAP O
AEG O
dataset O
. O
We O
collect O
gaze O
behaviour O
data O
only O
for O
Prompts O
3 O
- O
6 O
, O
as O
explained O
in O
Section O
5.3 O
. O
The O
other O
4 O
prompts O
comprise O
our O
unseen O
essay O
sets O
. O
For O
measuring O
our O
system O
‚Äôs O
performance O
, O
we O
use O
Cohen O
‚Äôs O
Kappa O
with O
quadratic O
weights O
- O
Quadratic O
Weighted O
Kappa O
( O
QWK O
) O
( O
Cohen O
, O
1968 O
) O
for O
the O
following O
reasons O
. O
Firstly O
, O
irrespective O
of O
whether O
we O
4The O
prompt O
is O
‚Äú O
Based O
on O
the O
excerpt O
, O
describe O
the O
obstacles O
the O
builders O
of O
the O
Empire O
State O
Building O
faced O
in O
attempting O
to O
allow O
dirigibles O
to O
dock O
there O
. O
Support O
your O
answer O
with O
relevant O
and O
speciÔ¨Åc O
information O
from O
the O
excerpt O
. O
‚Äù O
The O
original O
article O
is O
present O
in O
Appendix O
A.860use O
regression O
, O
or O
ordinal O
classiÔ¨Åcation O
, O
the O
Ô¨Ånal O
scores O
that O
are O
predicted O
by O
the O
system O
should O
be O
discrete O
scores O
. O
Hence O
, O
using O
Pearson O
Correlation O
would O
not O
be O
appropriate O
for O
our O
system O
. O
Secondly O
, O
F O
- O
Score O
and O
accuracy O
do O
not O
consider O
chance O
agreements O
unlike O
Cohen O
‚Äôs O
Kappa O
. O
If O
we O
were O
to O
give O
everyone O
an O
average O
grade O
, O
we O
would O
get O
a O
positive O
value O
for O
accuracy O
and O
F O
- O
Score O
, O
but O
a O
Kappa O
value O
of O
0 O
. O
Thirdly O
, O
weighted O
Kappa O
takes O
into O
account O
the O
fact O
that O
the O
classes O
are O
ordered O
, O
i.e. O
0<1<2 O
.... O
Using O
unweighted O
Kappa O
would O
penalize O
a O
0graded O
as O
a O
4 O
, O
as O
much O
as O
a O
1 O
. O
We O
use O
quadratic O
weights O
, O
as O
opposed O
to O
linear O
weights O
, O
because O
quadratic O
weights O
reward O
agreements O
and O
penalize O
mismatches O
more O
than O
linear O
weights O
. O
5.3 O
Creation O
of O
the O
Gaze O
Behaviour O
Dataset O
In O
this O
subsection O
, O
we O
describe O
how O
we O
created O
our O
gaze O
behaviour O
dataset O
, O
how O
we O
chose O
our O
essays O
for O
eye O
- O
tracking O
, O
and O
how O
they O
were O
annotated O
. O
5.3.1 O
Details O
of O
Texts O
Essay O
Set O
0 O
1 O
2 O
3 O
4 O
Total O
Prompt O
3 O
2 O
4 O
5 O
1 O
N O
/ O
A O
12 O
Prompt O
4 O
2 O
3 O
4 O
3 O
N O
/ O
A O
12 O
Prompt O
5 O
2 O
1 O
3 O
5 O
1 O
12 O
Prompt O
6 O
2 O
2 O
3 O
4 O
1 O
12 O
Total O
8 O
10 O
15 O
13 O
2 O
48 O
Table O
2 O
: O
Number O
of O
essays O
for O
each O
essay O
set O
which O
we O
collected O
gaze O
behaviour O
, O
scored O
between O
0 O
to O
3 O
( O
or O
4 O
) O
. O
As O
mentioned O
earlier O
in O
Section O
5 O
, O
we O
used O
only O
essays O
corresponding O
to O
prompts O
3 O
to O
6 O
of O
the O
ASAP O
AEG O
dataset O
. O
From O
each O
of O
the O
four O
essay O
sets O
, O
we O
selected O
12 O
essays O
with O
a O
diverse O
vocabulary O
as O
well O
as O
all O
possible O
scores O
. O
We O
use O
a O
greedy O
algorithm O
to O
select O
essays O
i.e. O
, O
For O
each O
essay O
set O
, O
we O
pick O
12 O
essays O
, O
covering O
all O
score O
points O
with O
maximum O
number O
of O
unique O
tokens O
, O
as O
well O
as O
being O
under O
250 O
words O
. O
Table O
2 O
reports O
the O
distribution O
of O
essays O
with O
each O
score O
, O
for O
each O
of O
the O
4 O
essay O
sets O
that O
we O
use O
to O
create O
our O
gaze O
behaviour O
dataset O
. O
To O
display O
the O
essay O
text O
on O
the O
screen O
, O
we O
use O
a O
large O
font O
size O
, O
so O
that O
( O
a O
) O
the O
text O
is O
clear O
, O
and O
( O
b O
) O
the O
reader O
‚Äôs O
gaze O
is O
captured O
on O
the O
words O
which O
they O
are O
currently O
reading O
. O
Although O
, O
this O
ensures O
the O
clarity O
in O
reading O
and O
recording O
the O
gaze O
pattern O
in O
a O
more O
accurate O
manner O
, O
it O
also O
imposes O
a O
limitation O
on O
the O
size O
of O
the O
essay O
which O
can O
be O
used O
forour O
experiment O
. O
This O
is O
why O
, O
the O
longest O
essay O
in O
our O
gaze O
behaviour O
dataset O
is O
about O
250 O
words O
. O
The O
original O
essays O
have O
their O
named O
entities O
anonymized O
. O
Hence O
, O
before O
running O
the O
experiments O
, O
we O
replaced O
the O
required O
named O
entities O
with O
placeholders O
( O
Eg O
. O
@NAME1 O
‚Üí‚ÄúAl O
Smith O
‚Äù O
, O
@PLACE1 O
‚Üí‚ÄúNew O
Jersey O
‚Äù O
, O
@MONTH1 O
‚Üí‚ÄúMay O
‚Äù O
, O
etc.)5 O
. O
5.3.2 O
Annotator O
Details O
We O
used O
a O
total O
of O
8 O
annotators O
, O
aged O
between O
18 O
and O
31 O
, O
with O
an O
average O
age O
of O
25 O
years O
. O
All O
of O
them O
were O
either O
in O
college O
, O
or O
had O
completed O
a O
Bachelor O
‚Äôs O
degree O
. O
All O
but O
one O
of O
them O
also O
had O
experience O
as O
a O
teaching O
assistant O
. O
The O
annotators O
were O
Ô¨Çuent O
in O
English O
, O
and O
about O
half O
of O
them O
had O
participated O
earlier O
, O
in O
similar O
experiments O
. O
The O
annotators O
were O
adequately O
compensated O
for O
their O
work6 O
. O
To O
assess O
the O
quality O
of O
the O
individual O
annotators O
, O
we O
evaluated O
the O
scores O
they O
provided O
against O
the O
ground O
truth O
scores O
- O
i.e. O
,the O
scores O
given O
by O
the O
original O
annotators O
. O
The O
QWK O
measures O
the O
agreement O
between O
the O
annotators O
and O
the O
ground O
truth O
score O
. O
Close O
is O
the O
number O
of O
times O
( O
out O
of O
48 O
) O
in O
which O
the O
annotators O
either O
agreed O
with O
the O
ground O
truth O
scores O
, O
or O
differed O
from O
them O
by O
at O
most O
1 O
score O
point O
. O
Correct O
is O
the O
number O
of O
times O
( O
out O
of O
48 O
) O
in O
which O
the O
annotators O
agreed O
with O
the O
ground O
truth O
scores O
. O
The O
mean O
values O
for O
the O
3 O
measures O
were O
0.646 O
( O
QWK O
) O
, O
42.75 O
( O
Close O
) O
and O
22.25 O
( O
Correct O
) O
. O
5.4 O
System O
Details O
We O
conduct O
our O
experiments O
using O
well O
- O
established O
norms O
in O
eye O
- O
tracking O
research O
( O
Holmqvist O
et O
al O
. O
, O
2011 O
) O
. O
The O
essays O
are O
displayed O
on O
a O
screen O
that O
is O
kept O
about O
2 O
feet O
in O
front O
of O
the O
participant O
. O
The O
workÔ¨Çow O
of O
the O
experiment O
is O
as O
follows O
. O
First O
, O
the O
camera O
is O
calibrated O
. O
This O
is O
done O
by O
having O
the O
annotator O
look O
at O
13 O
points O
on O
the O
screen O
, O
while O
the O
camera O
tracks O
their O
eyes O
. O
Next O
, O
the O
calibration O
is O
validated O
. O
In O
this O
step O
, O
the O
participant O
looks O
at O
the O
same O
points O
they O
saw O
earlier O
. O
If O
there O
is O
a O
big O
difference O
between O
the O
participant O
‚Äôs O
Ô¨Åxation O
points O
tracked O
by O
the O
camera O
and O
the O
actual O
points O
, O
calibration O
is O
repeated O
. O
Then O
, O
the O
reader O
5Another O
advantage O
of O
using O
source O
- O
dependent O
essays O
is O
that O
there O
is O
a O
source O
article O
which O
we O
can O
use O
to O
correctly O
replace O
the O
anonymized O
named O
entities O
6We O
report O
details O
on O
individual O
annotators O
in O
Appendix O
B.861performs O
a O
self O
- O
paced O
reading O
of O
the O
essay O
while O
we O
supervise O
the O
tracking O
of O
their O
eyes O
. O
After O
reading O
and O
scoring O
an O
essay O
, O
the O
participant O
takes O
a O
small O
break O
of O
about O
a O
minute O
, O
before O
continuing O
. O
Before O
the O
next O
essay O
is O
read O
, O
the O
camera O
has O
to O
again O
be O
calibrated O
and O
validated7 O
. O
The O
essay O
is O
displayed O
on O
the O
screen O
in O
Times O
New O
Roman O
typeface O
with O
a O
font O
size O
of O
23 O
. O
Finally O
, O
the O
reader O
scores O
the O
essay O
andprovides O
a O
justiÔ¨Åcation O
for O
their O
score8 O
. O
This O
entire O
process O
is O
done O
using O
an O
SR O
Research O
Eye O
Link O
1000 O
eye O
- O
tracker O
( O
monocular O
stabilized O
head O
mode O
, O
with O
a O
sampling O
rate O
of O
500Hz O
) O
. O
The O
machine O
collects O
all O
the O
gaze O
details O
that O
we O
need O
for O
our O
experiments O
. O
An O
interest O
area O
report O
is O
generated O
for O
gaze O
behaviour O
using O
the O
SR O
Research O
Data O
Viewer O
software O
. O
5.5 O
Experiment O
Details O
We O
use O
Ô¨Åve O
- O
fold O
cross O
- O
validation O
to O
evaluate O
our O
system O
. O
For O
each O
fold O
, O
60 O
% O
is O
used O
as O
training O
, O
20 O
% O
for O
validation O
, O
and O
20 O
% O
for O
testing O
. O
The O
folds O
are O
the O
same O
as O
those O
used O
by O
Taghipour O
and O
Ng O
( O
2016 O
) O
. O
Prior O
to O
running O
our O
experiments O
, O
we O
convert O
the O
scores O
from O
their O
original O
score O
range O
( O
given O
in O
Table O
1 O
) O
to O
the O
range O
of O
[ O
0,1]as O
described O
by O
Taghipour O
and O
Ng O
( O
2016 O
) O
. O
In O
order O
to O
normalize O
idiosyncratic O
reading O
patterns O
across O
different O
readers O
, O
we O
perform O
binning O
for O
each O
of O
the O
features O
for O
each O
of O
the O
readers O
. O
For O
IR O
and O
Skip O
we O
use O
only O
two O
bins O
- O
0 O
and O
1 O
- O
corresponding O
to O
their O
values O
. O
For O
the O
run O
count O
, O
we O
use O
six O
bins O
( O
from O
0 O
to O
5 O
) O
, O
where O
each O
bin O
is O
the O
run O
count O
( O
up O
to O
4 O
) O
, O
and O
bin O
5 O
contains O
run O
counts O
more O
than O
4 O
. O
For O
the O
Ô¨Åxation O
attributes O
- O
DT O
and O
FFD O
- O
we O
use O
the O
same O
binning O
scheme O
as O
described O
in O
Klerke O
et O
al O
. O
( O
2016 O
) O
. O
The O
binning O
scheme O
for O
Ô¨Åxation O
attributes O
is O
as O
follows O
: O
0ifFV=0 O
, O
1ifFV>0andFV‚â§¬µ‚àíœÉ O
, O
2ifFV>¬µ‚àíœÉandFV‚â§¬µ‚àí0.5√óœÉ O
, O
3ifFV>¬µ‚àí0.5√óœÉandFV‚â§¬µ+0.5√óœÉ O
, O
4ifFV>¬µ+0.5√óœÉandFV‚â§¬µ+œÉ O
, O
5ifFV>¬µ+œÉ O
, O
whereFV O
is O
the O
value O
of O
the O
given O
Ô¨Åxation O
attribute,¬µis O
the O
average O
Ô¨Åxation O
attribute O
value O
for O
7The O
average O
time O
for O
the O
participants O
was O
about O
2 O
hours O
, O
with O
the O
fastest O
completing O
the O
task O
in O
slightly O
under O
one O
and O
a O
half O
hours O
. O
8As O
part O
of O
our O
data O
release O
, O
we O
will O
release O
the O
scores O
given O
by O
each O
annotator O
, O
as O
well O
as O
their O
justiÔ¨Åcations O
for O
their O
scorethe O
reader O
and O
œÉis O
the O
standard O
deviation O
. O
5.6 O
Network O
Architecture O
Figure O
1 O
( O
b O
) O
shows O
the O
architecture O
of O
our O
proposed O
system O
, O
based O
on O
the O
co O
- O
attention O
based O
architecture O
described O
by O
Zhang O
and O
Litman O
( O
2018 O
) O
. O
Given O
an O
essay O
, O
we O
split O
the O
essay O
into O
sentences O
. O
For O
each O
sentence O
, O
we O
look O
- O
up O
the O
word O
embeddings O
for O
all O
words O
in O
the O
Word O
Embedding O
layer O
. O
The O
4000 O
most O
frequent O
words O
are O
used O
as O
the O
vocabulary O
, O
with O
all O
other O
words O
mapped O
to O
a O
special O
unknown O
token O
. O
This O
sequence O
of O
word O
embeddings O
is O
then O
sent O
through O
a O
Time O
- O
Delay O
Neural O
Network O
( O
TDNN O
) O
, O
or O
1 O
- O
d O
Convolutional O
Neural O
Network O
( O
CNN O
) O
, O
of O
Ô¨Ålter O
width O
k. O
The O
output O
from O
CNN O
is O
pooled O
using O
an O
attention O
layer O
- O
the O
Word O
Level O
Attention O
Pooling O
Layer O
- O
which O
results O
in O
a O
representation O
for O
every O
sentence O
. O
These O
sentence O
representations O
are O
then O
sent O
through O
a O
Sentence O
Level O
LSTM O
Layer O
and O
their O
output O
pooled O
in O
the O
Sentence O
Level O
Attention O
Pooling O
Layer O
to O
obtain O
the O
sentence O
representation O
for O
the O
essay O
. O
A O
similar O
procedure O
is O
repeated O
for O
the O
source O
article O
. O
We O
then O
perform O
co O
- O
attention O
between O
the O
sentence O
representations O
of O
the O
essay O
andthe O
source O
article O
.Co O
- O
attention O
is O
performed O
to O
learn O
similarities O
between O
the O
sentences O
in O
the O
essay O
and O
the O
source O
article O
. O
This O
is O
done O
as O
a O
way O
to O
ensure O
that O
the O
writer O
sticks O
to O
answering O
the O
prompt O
, O
rather O
than O
drifting O
off O
topic O
. O
We O
now O
represent O
every O
sentence O
in O
the O
essay O
as O
a O
weighted O
combination O
of O
the O
sentence O
representation O
between O
the O
essay O
and O
the O
source O
article O
( O
Essay2Article O
) O
. O
The O
weights O
are O
obtained O
from O
the O
output O
of O
the O
co O
- O
attention O
layer O
. O
The O
weights O
represent O
how O
each O
sentence O
in O
the O
essay O
are O
similar O
to O
the O
sentences O
in O
the O
source O
article O
. O
If O
a O
sentence O
in O
the O
essay O
has O
low O
weights O
this O
indicates O
that O
the O
sentence O
would O
be O
off O
topic O
. O
A O
similar O
procedure O
is O
repeated O
to O
get O
a O
weighted O
representation O
of O
sentences O
in O
the O
source O
article O
with O
respect O
to O
the O
essay O
( O
Article2Essay O
) O
. O
Finally O
, O
we O
send O
the O
sentence O
representation O
of O
the O
essay O
and O
article O
, O
through O
a O
dense O
layer O
( O
i.e. O
the O
Modeling O
Layer O
) O
to O
predict O
the O
Ô¨Ånal O
essay O
score O
, O
with O
a O
sigmoid O
activation O
function O
. O
As O
the O
essay O
scores O
are O
in O
the O
range O
[ O
0,1 O
] O
, O
we O
use O
sigmoid O
activation O
at O
the O
output O
layer O
. O
During O
prediction O
, O
we O
map O
the O
output O
scores O
from O
the O
sigmoid O
layer O
back O
to O
the O
original O
score O
range O
, O
minimizing O
the O
mean O
squared O
error O
( O
MSE O
) O
loss O
.862Figure O
1 O
: O
Architecture O
of O
the O
proposed O
gaze O
behaviour O
and O
essay O
scoring O
multi O
- O
task O
learning O
systems O
, O
namely O
( O
a O
) O
theSelf O
- O
Attention O
multi O
- O
task O
learning O
system O
, O
for O
an O
essay O
of O
nsentences O
- O
and O
( O
b O
) O
- O
the O
Co O
- O
Attention O
system O
for O
an O
essay O
ofnsentences O
and O
a O
source O
article O
of O
msentences O
. O
For O
essay O
sets O
without O
a O
source O
article O
, O
we O
use O
theSelf O
- O
Attention O
model O
proposed O
by O
Dong O
et O
al O
. O
( O
2017 O
) O
. O
This O
is O
a O
simpler O
model O
which O
does O
not O
consider O
the O
source O
article O
, O
and O
uses O
only O
the O
essay O
text O
. O
This O
is O
applicable O
whenever O
a O
source O
article O
is O
not O
present O
. O
Figure O
1 O
( O
a O
) O
shows O
the O
architecture O
of O
the O
model O
. O
Like O
the O
earlier O
system O
, O
we O
get O
the O
sentence O
representation O
of O
the O
essay O
from O
the O
Sentence O
Level O
LSTM O
Layer O
and O
send O
it O
through O
the O
Dense O
Layer O
with O
a O
sigmoid O
activation O
function O
. O
Gaze O
behaviour O
is O
learnt O
at O
the O
Word O
- O
Level O
Convolutional O
Layer O
in O
both O
the O
models O
because O
the O
gaze O
attributes O
are O
deÔ¨Åned O
at O
the O
word O
- O
level O
, O
while O
the O
essay O
is O
scored O
at O
the O
document O
- O
level O
. O
The O
output O
from O
the O
CNN O
layer O
is O
sent O
through O
a O
linear O
layer O
followed O
by O
sigmoid O
activation O
for O
a O
particular O
gaze O
behaviour O
. O
For O
learning O
multiple O
gaze O
attributes O
simultaneously O
, O
we O
have O
multiple O
linear O
layers O
for O
each O
of O
the O
gaze O
attributes O
. O
In O
the O
multitask O
setting O
, O
we O
also O
minimize O
the O
mean O
squared O
error O
of O
the O
learnt O
gaze O
behaviour O
and O
the O
actual O
gaze O
behaviour O
attribute O
value O
. O
We O
assign O
weights O
to O
each O
of O
the O
gaze O
behaviour O
loss O
functions O
to O
control O
the O
importance O
given O
to O
individual O
gaze O
behaviour O
learning O
tasks O
. O
5.7 O
Network O
Hyperparameters O
Table O
3 O
gives O
the O
different O
hyperparameters O
which O
we O
used O
in O
our O
experiment O
. O
We O
use O
the O
50 O
dimension O
GloVe O
pre O
- O
trained O
word O
embeddings O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
trained O
on O
the O
Wikipedia O
2014 O
+ O
Gigawords O
5 O
Corpus O
( O
6B O
tokens O
, O
4 O
K O
vocabulary O
, O
uncased O
) O
. O
We O
run O
our O
experiments O
over O
a O
batch O
size O
of O
100 O
, O
for O
100 O
epochs O
, O
and O
set O
the O
learningLayer O
Hyperparameter O
Value O
Embedding O
layer O
Pre O
- O
trained O
embeddings O
GloVe O
Embeddings O
dimensions O
50 O
Word O
- O
level O
CNN O
Kernel O
size O
5 O
Filters O
100 O
Sentence O
- O
level O
LSTM O
Hidden O
units O
100 O
Network O
- O
wide O
Batch O
size O
100 O
Epochs O
100 O
Learning O
rate O
0.001 O
Dropout O
rate O
0.5 O
Momentum O
0.9 O
Table O
3 O
: O
Hyperparameters O
for O
our O
experiment O
. O
rate O
as O
0.001 O
, O
and O
a O
dropout O
rate O
of O
0.5 O
. O
The O
Wordlevel O
CNN O
layer O
has O
a O
kernel O
size O
of O
5 O
, O
with O
100 O
Ô¨Ålters O
. O
The O
Sentence O
- O
level O
LSTM O
layer O
and O
modeling O
layer O
both O
have O
100 O
hidden O
units O
. O
We O
use O
the O
RMSProp O
Optimizer O
( O
Dauphin O
et O
al O
. O
, O
2015 O
) O
with O
a O
0.001 O
initial O
learning O
rate O
and O
momentum O
of O
0.9 O
. O
Gaze O
Feature O
Gaze O
Feature O
Weight O
Dwell O
Time O
0.05 O
First O
Fixation O
Duration O
0.05 O
IsRegression O
0.01 O
Run O
Count O
0.01 O
Skip O
0.1 O
Table O
4 O
: O
This O
table O
shows O
the O
best O
weights O
assigned O
to O
the O
different O
gaze O
features O
from O
our O
grid O
search O
. O
In O
addition O
to O
the O
network O
hyper O
- O
parameters O
, O
we O
also O
weigh O
the O
loss O
functions O
of O
the O
different O
gaze863behaviours O
differently O
, O
with O
weight O
levels O
of O
0.5 O
, O
0.1,0.05,0.01 O
and0.001 O
. O
We O
use O
grid O
search O
and O
pick O
the O
weight O
giving O
the O
lowest O
mean O
- O
squared O
error O
on O
the O
development O
set O
. O
The O
best O
weights O
from O
grid O
search O
are O
0.05 O
for O
DT O
and O
FFD O
, O
0.01 O
for O
IR O
and O
RC O
, O
and O
0.1for O
Skip O
. O
5.8 O
Experiment O
ConÔ¨Ågurations O
To O
test O
our O
system O
on O
essay O
sets O
which O
we O
collected O
gaze O
behaviour O
, O
we O
run O
experiments O
using O
the O
following O
conÔ¨Ågurations O
. O
( O
a O
) O
Self O
- O
Attention O
- O
This O
is O
the O
implementation O
of O
Dong O
et O
al O
. O
( O
2017 O
) O
‚Äôs O
system O
in O
TensorÔ¨Çow O
by O
Zhang O
and O
Litman O
( O
2018 O
) O
. O
( O
b O
) O
Co O
- O
Attention O
. O
This O
is O
Zhang O
and O
Litman O
( O
2018 O
) O
‚Äôs O
system9 O
. O
( O
c O
) O
Co O
- O
Attention+Gaze O
. O
This O
is O
our O
system O
, O
which O
uses O
gaze O
behaviour O
. O
In O
addition O
to O
this O
, O
we O
also O
run O
experiments O
on O
theunseen O
essay O
sets O
using O
the O
following O
trainingconÔ¨Ågurations O
. O
( O
a O
) O
Only O
Prompt O
- O
This O
uses O
our O
self O
- O
attention O
model O
, O
with O
the O
training O
data O
being O
only O
the O
essays O
from O
that O
essay O
set O
. O
We O
use O
this O
model O
, O
because O
there O
are O
no O
source O
articles O
for O
these O
essay O
sets O
. O
( O
b O
) O
Extra O
Essays O
- O
Here O
, O
we O
augment O
the O
training O
data O
of O
( O
a O
) O
with O
the O
48 O
essays O
for O
which O
we O
collect O
gaze O
behaviour O
data O
. O
( O
c O
) O
Essays+Gaze O
- O
Here O
, O
we O
augment O
the O
training O
data O
of O
( O
a O
) O
with O
the O
48 O
essays O
which O
we O
collect O
gaze O
behaviour O
data O
, O
and O
their O
corresponding O
gaze O
data O
. O
We O
also O
compare O
our O
results O
with O
a O
string O
kernel O
based O
system O
proposed O
by O
Cozma O
et O
al O
. O
( O
2018 O
) O
. O
6 O
Results O
and O
Analysis O
Table O
5 O
reports O
the O
results O
of O
our O
experiments O
on O
the O
essay O
sets O
for O
which O
we O
collect O
the O
gaze O
behaviour O
data O
. O
The O
table O
is O
divided O
into O
3 O
parts O
. O
The O
Ô¨Årst O
part O
( O
i.e. O
,Ô¨Årst O
3 O
rows O
) O
are O
the O
reported O
results O
previously O
available O
deep O
- O
learning O
systems O
, O
namely O
Taghipour O
and O
Ng O
( O
2016 O
) O
, O
Dong O
and O
Zhang O
( O
2016 O
) O
, O
and O
Tay O
et O
al O
. O
( O
2018 O
) O
. O
The O
next O
2 O
rows O
feature O
results O
using O
the O
self O
- O
attention O
( O
Dong O
et O
al O
. O
, O
2017 O
) O
and O
co O
- O
attention O
( O
Zhang O
and O
Litman O
, O
2018 O
) O
. O
The O
last O
row O
reports O
results O
using O
gaze O
behaviour O
on O
top O
of O
co O
- O
attention O
, O
i.e. O
,Co O
- O
Attention+Gaze O
. O
The O
Ô¨Årst O
column O
is O
the O
different O
systems O
. O
The O
next O
4 O
columns O
report O
the O
QWK O
results O
of O
each O
system O
for O
each O
of O
the O
4 O
essay O
sets O
. O
The O
last O
column O
reports O
the O
Mean O
QWK O
value O
across O
all O
4 O
essay O
sets O
. O
Our O
system O
is O
able O
to O
outperform O
the O
CoAttention O
system O
( O
Zhang O
and O
Litman O
, O
2018 O
) O
in O
all O
9The O
implementation O
of O
both O
systems O
can O
be O
downloaded O
from O
here.the O
essay O
sets O
. O
Overall O
, O
it O
is O
also O
the O
best O
system O
achieving O
the O
highest O
QWK O
results O
among O
all O
the O
systems O
in O
3 O
out O
of O
the O
4 O
essay O
sets O
( O
and O
the O
secondbest O
in O
the O
other O
essay O
set O
) O
. O
To O
test O
our O
hypothesis O
that O
the O
model O
trained O
by O
learning O
gaze O
behaviour O
helps O
in O
automatic O
essay O
grading O
- O
we O
run O
the O
Paired O
T O
- O
Test O
. O
Our O
null O
hypothesis O
is O
: O
‚Äú O
Learning O
gaze O
behaviour O
to O
score O
an O
essay O
does O
not O
help O
any O
more O
than O
the O
self O
- O
attention O
and O
co O
- O
attention O
systems O
and O
whatever O
improvements O
we O
see O
are O
due O
to O
chance O
. O
‚Äù O
We O
choose O
a O
signiÔ¨Åcance O
level O
of O
p<0.05 O
, O
and O
observe O
that O
the O
improvements O
of O
our O
system O
are O
found O
to O
be O
statistically O
signiÔ¨Åcant O
- O
rejecting O
the O
null O
hypothesis O
. O
6.1 O
Results O
for O
Unseen O
Essay O
Sets O
In O
order O
to O
run O
our O
experiments O
on O
unseen O
essay O
sets O
, O
we O
augment O
the O
training O
data O
with O
the O
gaze O
behaviour O
data O
collected O
. O
Since O
none O
of O
these O
essays O
have O
source O
articles O
, O
we O
use O
the O
self O
- O
attention O
model O
of O
Dong O
et O
al O
. O
( O
2017 O
) O
as O
the O
baseline O
system O
. O
We O
now O
augment O
the O
gaze O
behaviour O
learning O
task O
as O
the O
auxiliary O
task O
and O
report O
the O
results O
in O
Table O
6 O
. O
The O
Ô¨Årst O
column O
in O
the O
table O
is O
the O
different O
systems O
. O
The O
next O
4 O
columns O
are O
the O
results O
for O
each O
of O
the O
unseen O
essay O
sets O
, O
and O
the O
last O
column O
is O
the O
mean O
QWK O
. O
From O
Table O
6 O
, O
we O
observe O
that O
our O
system O
which O
uses O
both O
the O
extra O
48 O
essays O
and O
their O
gaze O
behaviour O
outperforms O
the O
other O
2 O
conÔ¨Ågurations O
( O
Only O
Prompt O
andExtra O
Essays O
) O
across O
all O
4 O
unseen O
essay O
sets O
. O
The O
improvement O
when O
learning O
gaze O
behaviour O
for O
unseen O
essay O
sets O
is O
statistically O
signiÔ¨Åcant O
for O
p<0.05 O
. O
6.2 O
Comparison O
with O
String O
Kernel O
System O
Since O
Cozma O
et O
al O
. O
( O
2018 O
) O
have O
n‚Äôt O
released O
their O
data O
splits O
( O
train O
/ O
test O
/ O
dev O
) O
, O
we O
ran O
their O
system O
with O
our O
data O
splits O
. O
We O
observed O
a O
mean O
QWK O
of O
0.750 O
with O
the O
string O
kernel O
- O
based O
system O
on O
the O
essay O
sets O
where O
we O
have O
gaze O
behaviour O
data O
, O
and O
0.685 O
on O
the O
unseen O
essay O
sets O
. O
One O
possible O
reason O
for O
this O
could O
be O
that O
while O
they O
used O
cross O
- O
validation O
, O
they O
may O
have O
used O
only O
a O
training O
- O
testing O
split O
( O
as O
compared O
to O
a O
train O
/ O
test O
/ O
dev O
split O
) O
. O
6.3 O
Analysis O
of O
Gaze O
Attributes O
In O
order O
to O
see O
which O
of O
the O
gaze O
attributes O
are O
the O
most O
important O
, O
we O
ran O
ablation O
tests O
, O
where O
we O
ablate O
each O
gaze O
attribute O
. O
We O
found O
that O
the O
most O
important O
gaze O
behaviour O
attribute O
across O
all O
the O
essay O
sets O
is O
the O
Dwell O
Time O
, O
followed O
closely O
by O
the O
First O
Fixation O
Duration O
. O
One O
of O
the O
reasons864System O
Prompt O
3 O
Prompt O
4 O
Prompt O
5 O
Prompt O
6 O
Mean O
QWK O
Taghipour O
and O
Ng O
( O
2016 O
) O
0.683 O
0.795 O
0.818 O
0.813 O
0.777 O
Dong O
and O
Zhang O
( O
2016 O
) O
0.662 O
0.778 O
0.800 O
0.809 O
0.762 O
Tay O
et O
al O
. O
( O
2018 O
) O
0.695 O
0.788 O
0.815 O
0.810 O
0.777 O
Self O
- O
Attention O
( O
Dong O
et O
al O
. O
, O
2017 O
) O
0.677 O
0.807 O
0.806 O
0.809 O
0.775 O
Co O
- O
Attention O
( O
Zhang O
and O
Litman O
, O
2018 O
) O
0.689‚Ä† O
0.809‚Ä† O
0.812‚Ä† O
0.813‚Ä† O
0.780‚Ä† O
Co O
- O
Attention+Gaze O
0.698 O
* O
0.818 O
* O
0.815 O
* O
0.821 O
* O
0.788 O
* O
Table O
5 O
: O
Results O
of O
our O
experiments O
in O
scoring O
the O
essays O
( O
QWK O
values O
) O
from O
the O
essay O
sets O
where O
we O
collected O
gaze O
behaviour O
. O
The O
Ô¨Årst O
3 O
rows O
are O
results O
reported O
from O
other O
state O
- O
of O
- O
the O
- O
art O
deep O
learning O
systems O
. O
The O
next O
2 O
rows O
are O
the O
results O
we O
obtained O
on O
existing O
systems O
- O
self O
- O
attention O
and O
co O
- O
attention O
- O
without O
gaze O
behaviour O
. O
The O
last O
row O
is O
the O
results O
from O
our O
system O
using O
gaze O
behaviour O
data O
( O
Co O
- O
Attention+Gaze O
) O
. O
‚Ä† O
denotes O
the O
baseline O
system O
performance O
, O
and O
* O
denotes O
a O
statistically O
signiÔ¨Åcant O
result O
of O
p<0.05for O
the O
gaze O
behaviour O
system O
. O
System O
Prompt O
1 O
Prompt O
2 O
Prompt O
7 O
Prompt O
8 O
Mean O
QWK O
Taghipour O
and O
Ng O
( O
2016 O
) O
0.775 O
0.687 O
0.805 O
0.594 O
0.715 O
Dong O
and O
Zhang O
( O
2016 O
) O
0.805 O
0.613 O
0.758 O
0.644 O
0.705 O
Tay O
et O
al O
. O
( O
2018 O
) O
0.832 O
0.684 O
0.800 O
0.697 O
0.753 O
Only O
Prompt O
( O
Dong O
et O
al O
. O
( O
2017 O
) O
) O
0.816 O
0.667 O
0.792 O
0.678 O
0.738 O
Extra O
Essays O
0.828‚Ä† O
0.672‚Ä† O
0.802‚Ä† O
0.685‚Ä† O
0.747‚Ä† O
Extra O
Essays O
+ O
Gaze O
0.833 O
0.681 O
0.806 O
* O
0.699 O
* O
0.754 O
* O
Table O
6 O
: O
Results O
of O
our O
experiments O
on O
the O
unseen O
essay O
sets O
our O
dataset O
. O
The O
Ô¨Årst O
3 O
rows O
are O
results O
reported O
from O
other O
state O
- O
of O
- O
the O
- O
art O
deep O
learning O
systems O
. O
The O
next O
2 O
rows O
are O
the O
results O
obtained O
without O
using O
gaze O
behaviour O
( O
without O
and O
with O
the O
extra O
essays O
) O
. O
The O
last O
row O
is O
the O
results O
from O
our O
system O
. O
‚Ä† O
denotes O
the O
baseline O
system O
without O
gaze O
behaviour O
, O
and O
* O
denotes O
a O
statistically O
signiÔ¨Åcant O
result O
of O
p<0.05for O
the O
gaze O
behaviour O
system O
. O
Gaze O
Feature O
Diff O
. O
in O
QWK O
Dwell O
Time O
0.0137 O
First O
Fixation O
Duration O
0.0136 O
IsRegression O
0.0090 O
Run O
Count O
0.0110 O
Skip O
0.0091 O
Table O
7 O
: O
Results O
of O
ablation O
tests O
for O
each O
gaze O
behaviour O
attribute O
across O
all O
the O
essay O
sets O
. O
The O
reported O
numbers O
are O
the O
difference O
in O
QWK O
before O
and O
after O
ablating O
the O
given O
gaze O
attribute O
. O
The O
number O
in O
bold O
denotes O
the O
best O
gaze O
attribute O
. O
for O
this O
is O
the O
fact O
that O
both O
DT O
and O
FFD O
were O
very O
useful O
in O
detecting O
errors O
made O
by O
the O
essay O
writers O
. O
From O
Figure O
210 O
, O
we O
observe O
that O
most O
of O
the O
longest O
dwell O
times O
have O
come O
at O
/ O
around O
spelling O
mistakes O
( O
tock O
instead O
of O
took O
) O
, O
or O
outof O
- O
context O
words O
( O
bayinstead O
of O
by O
) O
, O
or O
incorrect O
phrases O
( O
short O
cat O
, O
instead O
of O
short O
cut O
) O
. O
These O
errors O
force O
the O
reader O
to O
spend O
more O
time O
Ô¨Åxating O
on O
the O
word O
which O
we O
also O
mentioned O
earlier O
. O
10We O
have O
given O
more O
examples O
in O
Appendix O
C.The O
normalized O
MSE O
of O
each O
of O
the O
gaze O
features O
learnt O
by O
our O
system O
was O
between O
0.125 O
to O
0.128 O
for O
all O
the O
gaze O
behaviour O
attributes O
. O
6.4 O
Analysis O
Using O
Only O
a O
Native O
English O
Speaker O
System O
No O
Native O
All O
Prompt O
1 O
0.816 O
0.824 O
0.833 O
Prompt O
2 O
0.667 O
0.679 O
0.681 O
Prompt O
3 O
0.677 O
0.679 O
0.698 O
Prompt O
4 O
0.807 O
0.812 O
0.818 O
Prompt O
5 O
0.806 O
0.810 O
0.815 O
Prompt O
6 O
0.809 O
0.815 O
0.821 O
Prompt O
7 O
0.792 O
0.809 O
0.806 O
Prompt O
8 O
0.678 O
0.679 O
0.699 O
Mean O
QWK O
0.757 O
0.764 O
0.771 O
Table O
8 O
: O
Result O
using O
only O
gaze O
behaviour O
of O
the O
native O
speaker O
( O
Native O
) O
, O
compared O
using O
no O
gaze O
behaviour O
( O
No O
) O
and O
gaze O
behaviour O
of O
all O
the O
readers O
( O
All O
) O
. O
We O
also O
ran O
our O
experiments O
using O
only O
the O
gaze O
behaviour O
of O
an O
annotator O
who O
was O
a O
native O
En-865Figure O
2 O
: O
Dwell O
Time O
of O
one O
of O
the O
readers O
for O
one O
of O
the O
essays O
. O
The O
darker O
the O
background O
, O
the O
larger O
the O
bin O
. O
glish O
speaker O
( O
as O
opposed O
to O
the O
rest O
of O
our O
annotators O
who O
were O
just O
Ô¨Çuent O
English O
speakers O
) O
. O
Table O
8 O
shows O
the O
results O
of O
those O
experiments O
. O
We O
observed O
a O
mean O
QWK O
of O
0.779 O
for O
the O
seen O
essay O
sets O
, O
and O
a O
mean O
QWK O
of O
0.748 O
for O
the O
essays O
sets O
where O
we O
have O
no O
gaze O
data O
. O
The O
difference O
in O
performance O
between O
both O
our O
systems O
( O
i.e. O
with O
only O
native O
speaker O
and O
with O
all O
annotators O
) O
were O
found O
to O
be O
statistically O
signiÔ¨Åcant O
with O
p=0.024511 O
. O
Similarly O
, O
the O
improvement O
in O
performance O
using O
the O
native O
English O
speaker O
, O
compared O
to O
not O
using O
any O
gaze O
behaviour O
was O
also O
found O
to O
be O
statistically O
signiÔ¨Åcant O
for O
p=0.0084 O
. O
7 O
Conclusion O
and O
Future O
Work O
In O
this O
paper O
, O
we O
describe O
how O
learning O
gaze O
behaviour O
can O
help O
AEG O
in O
a O
multi O
- O
task O
learning O
setup O
. O
We O
explained O
how O
we O
created O
a O
resource O
by O
collecting O
gaze O
behaviour O
data O
, O
and O
using O
multitask O
learning O
we O
are O
able O
to O
achieve O
better O
results O
over O
a O
state O
- O
of O
- O
the O
- O
art O
system O
developed O
by O
Zhang O
and O
Litman O
( O
2018 O
) O
for O
the O
essay O
sets O
which O
we O
collected O
gaze O
behaviour O
data O
from O
. O
We O
also O
analyze O
the O
transferability O
of O
gaze O
behaviour O
patterns O
across O
essay O
sets O
by O
training O
a O
multi O
- O
task O
learning O
model O
onunseen O
essay O
sets O
( O
i.e. O
essay O
sets O
where O
we O
have O
no O
gaze O
behaviour O
data O
) O
, O
thereby O
establishing O
that O
learning O
gaze O
behaviour O
improves O
automatic O
essay O
grading O
. O
In O
the O
future O
, O
we O
would O
like O
to O
look O
at O
using O
gaze O
behaviour O
to O
help O
in O
cross O
- O
domain O
AEG O
. O
This O
is O
done O
mainly O
when O
we O
do O
n‚Äôt O
have O
enough O
training O
examples O
in O
our O
essay O
set O
. O
We O
would O
also O
like O
to O
explore O
the O
possibility O
of O
generating O
textual O
feedback O
( O
rather O
than O
just O
a O
number O
, O
denoting O
the O
score O
of O
the O
essay O
) O
based O
on O
the O
justiÔ¨Åcations O
that O
the O
annotators O
gave O
for O
their O
grades O
. O
11The O
p O
- O
values O
for O
the O
different O
experiments O
are O
in O
Appendix O
D.References O
Maria O
Barrett O
, O
Joachim O
Bingel O
, O
Nora O
Hollenstein O
, O
Marek O
Rei O
, O
and O
Anders O
S√∏gaard O
. O
2018 O
. O
Sequence O
classiÔ¨Åcation O
with O
human O
attention O
. O
In O
Proceedings O
of O
the O
22nd O
Conference O
on O
Computational O
Natural O
Language O
Learning O
, O
pages O
302‚Äì312 O
, O
Brussels O
, O
Belgium O
. O
Association O
for O
Computational O
Linguistics O
. O
Maria O
Barrett O
, O
Joachim O
Bingel O
, O
Frank O
Keller O
, O
and O
Anders O
S√∏gaard O
. O
2016 O
. O
Weakly O
supervised O
part O
- O
ofspeech O
tagging O
using O
eye O
- O
tracking O
data O
. O
In O
Proceedings O
of O
the O
54th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
2 O
: O
Short O
Papers O
) O
, O
pages O
579‚Äì584 O
, O
Berlin O
, O
Germany O
. O
Association O
for O
Computational O
Linguistics O
. O
Rich O
Caruana O
. O
1998 O
. O
Multitask O
Learning O
, O
pages O
95 O
‚Äì O
133 O
. O
Springer O
US O
, O
Boston O
, O
MA O
. O
Jacob O
Cohen O
. O
1968 O
. O
Weighted O
kappa O
: O
Nominal O
scale O
agreement O
provision O
for O
scaled O
disagreement O
or O
partial O
credit O
. O
Psychological O
bulletin O
, O
70(4):213 O
. O
MÀòadÀòalina O
Cozma O
, O
Andrei O
Butnaru O
, O
and O
Radu O
Tudor O
Ionescu O
. O
2018 O
. O
Automated O
essay O
scoring O
with O
string O
kernels O
and O
word O
embeddings O
. O
In O
Proceedings O
of O
the O
56th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
2 O
: O
Short O
Papers O
) O
, O
pages O
503‚Äì509 O
, O
Melbourne O
, O
Australia O
. O
Association O
for O
Computational O
Linguistics O
. O
Yann O
Dauphin O
, O
Harm O
De O
Vries O
, O
and O
Yoshua O
Bengio O
. O
2015 O
. O
Equilibrated O
adaptive O
learning O
rates O
for O
nonconvex O
optimization O
. O
In O
Advances O
in O
neural O
information O
processing O
systems O
, O
pages O
1504‚Äì1512 O
. O
Fei O
Dong O
and O
Yue O
Zhang O
. O
2016 O
. O
Automatic O
features O
for O
essay O
scoring O
‚Äì O
an O
empirical O
study O
. O
In O
Proceedings O
of O
the O
2016 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
1072‚Äì1077 O
, O
Austin O
, O
Texas O
. O
Association O
for O
Computational O
Linguistics O
. O
Fei O
Dong O
, O
Yue O
Zhang O
, O
and O
Jie O
Yang O
. O
2017 O
. O
Attentionbased O
recurrent O
convolutional O
neural O
network O
for O
automatic O
essay O
scoring O
. O
In O
Proceedings O
of O
the O
21st O
Conference O
on O
Computational O
Natural O
Language O
Learning O
( O
CoNLL O
2017 O
) O
, O
pages O
153‚Äì162 O
, O
Vancouver O
, O
Canada O
. O
Association O
for O
Computational O
Linguistics O
. O
Ana O
V O
Gonz O
¬¥ O
alez O
- O
Gardu O
Àúno O
and O
Anders O
S√∏gaard O
. O
2018 O
. O
Learning O
to O
predict O
readability O
using O
eye O
- O
movement O
data O
from O
natives O
and O
learners O
. O
In O
Thirty O
- O
Second O
AAAI O
Conference O
on O
ArtiÔ¨Åcial O
Intelligence O
.866Ana O
Valeria O
Gonz O
¬¥ O
alez O
- O
Gardu O
Àúno O
and O
Anders O
S√∏gaard O
. O
2017 O
. O
Using O
gaze O
to O
predict O
text O
readability O
. O
In O
Proceedings O
of O
the O
12th O
Workshop O
on O
Innovative O
Use O
of O
NLP O
for O
Building O
Educational O
Applications O
, O
pages O
438‚Äì443 O
, O
Copenhagen O
, O
Denmark O
. O
Association O
for O
Computational O
Linguistics O
. O
John O
Hale O
, O
Chris O
Dyer O
, O
Adhiguna O
Kuncoro O
, O
and O
Jonathan O
Brennan O
. O
2018 O
. O
Finding O
syntax O
in O
human O
encephalography O
with O
beam O
search O
. O
In O
Proceedings O
of O
the O
56th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
2727‚Äì2736 O
, O
Melbourne O
, O
Australia O
. O
Association O
for O
Computational O
Linguistics O
. O
Nora O
Hollenstein O
and O
Ce O
Zhang O
. O
2019 O
. O
Entity O
recognition O
at O
Ô¨Årst O
sight O
: O
Improving O
NER O
with O
eye O
movement O
information O
. O
In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
1‚Äì10 O
, O
Minneapolis O
, O
Minnesota O
. O
Association O
for O
Computational O
Linguistics O
. O
Kenneth O
Holmqvist O
, O
Marcus O
Nystr O
¬®om O
, O
Richard O
Andersson O
, O
Richard O
Dewhurst O
, O
Halszka O
Jarodzka O
, O
and O
Joost O
Van O
de O
Weijer O
. O
2011 O
. O
Eye O
tracking O
: O
A O
comprehensive O
guide O
to O
methods O
and O
measures O
. O
OUP O
Oxford O
. O
Marcel O
A O
Just O
and O
Patricia O
A O
Carpenter O
. O
1980 O
. O
A O
theory O
of O
reading O
: O
From O
eye O
Ô¨Åxations O
to O
comprehension O
. O
Psychological O
review O
, O
87(4):329 O
. O
Alan O
Kennedy O
, O
Robin O
Hill O
, O
and O
Jo O
¬®el O
Pynte O
. O
2003 O
. O
The O
dundee O
corpus O
. O
In O
Proceedings O
of O
the O
12th O
European O
conference O
on O
eye O
movement O
. O
Sigrid O
Klerke O
, O
Yoav O
Goldberg O
, O
and O
Anders O
S√∏gaard O
. O
2016 O
. O
Improving O
sentence O
compression O
by O
learning O
to O
predict O
gaze O
. O
In O
Proceedings O
of O
the O
2016 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
pages O
1528‚Äì1533 O
, O
San O
Diego O
, O
California O
. O
Association O
for O
Computational O
Linguistics O
. O
Yunfei O
Long O
, O
Rong O
Xiang O
, O
Qin O
Lu O
, O
Chu O
- O
Ren O
Huang O
, O
and O
Minglei O
Li O
. O
2019 O
. O
Improving O
attention O
model O
based O
on O
cognition O
grounded O
data O
for O
sentiment O
analysis O
. O
IEEE O
Transactions O
on O
Affective O
Computing O
. O
Sandeep O
Mathias O
, O
Diptesh O
Kanojia O
, O
Kevin O
Patel O
, O
Samarth O
Agrawal O
, O
Abhijit O
Mishra O
, O
and O
Pushpak O
Bhattacharyya O
. O
2018 O
. O
Eyes O
are O
the O
windows O
to O
the O
soul O
: O
Predicting O
the O
rating O
of O
text O
quality O
using O
gaze O
behaviour O
. O
In O
Proceedings O
of O
the O
56th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
2352‚Äì2362 O
, O
Melbourne O
, O
Australia O
. O
Association O
for O
Computational O
Linguistics O
. O
Abhijit O
Mishra O
and O
Pushpak O
Bhattacharyya O
. O
2018 O
. O
Cognitively O
Inspired O
Natural O
Language O
Processing O
: O
An O
Investigation O
Based O
on O
Eye O
- O
tracking O
. O
Springer O
. O
Abhijit O
Mishra O
, O
Diptesh O
Kanojia O
, O
and O
Pushpak O
Bhattacharyya O
. O
2016 O
. O
Predicting O
readers O
‚Äô O
sarcasm O
understandability O
by O
modeling O
gaze O
behavior O
. O
Abhijit O
Mishra O
, O
Diptesh O
Kanojia O
, O
Seema O
Nagar O
, O
Kuntal O
Dey O
, O
and O
Pushpak O
Bhattacharyya O
. O
2017 O
. O
Scanpath O
complexity O
: O
Modeling O
reading O
effort O
using O
gaze O
information O
. O
Abhijit O
Mishra O
, O
Srikanth O
Tamilselvam O
, O
Riddhiman O
Dasgupta O
, O
Seema O
Nagar O
, O
and O
Kuntal O
Dey O
. O
2018 O
. O
Cognition O
- O
cognizant O
sentiment O
analysis O
with O
multitask O
subjectivity O
summarization O
based O
on O
annotators O
‚Äô O
gaze O
behavior O
. O
In O
Thirty O
- O
Second O
AAAI O
Conference O
on O
ArtiÔ¨Åcial O
Intelligence O
. O
Ellis O
B O
Page O
. O
1966 O
. O
The O
imminence O
of O
... O
grading O
essays O
by O
computer O
. O
The O
Phi O
Delta O
Kappan O
, O
47(5):238 O
‚Äì O
243 O
. O
Jeffrey O
Pennington O
, O
Richard O
Socher O
, O
and O
Christopher O
Manning O
. O
2014 O
. O
Glove O
: O
Global O
vectors O
for O
word O
representation O
. O
In O
Proceedings O
of O
the O
2014 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
( O
EMNLP O
) O
, O
pages O
1532‚Äì1543 O
, O
Doha O
, O
Qatar O
. O
Association O
for O
Computational O
Linguistics O
. O
Mark O
D O
Shermis O
and O
Jill O
Burstein O
. O
2013 O
. O
Handbook O
of O
automated O
essay O
evaluation O
: O
Current O
applications O
and O
new O
directions O
. O
Routledge O
. O
Abhinav O
Deep O
Singh O
, O
Poojan O
Mehta O
, O
Samar O
Husain O
, O
and O
Rajkumar O
Rajakrishnan O
. O
2016 O
. O
Quantifying O
sentence O
complexity O
based O
on O
eye O
- O
tracking O
measures O
. O
In O
Proceedings O
of O
the O
Workshop O
on O
Computational O
Linguistics O
for O
Linguistic O
Complexity O
( O
CL4LC O
) O
, O
pages O
202‚Äì212 O
, O
Osaka O
, O
Japan O
. O
The O
COLING O
2016 O
Organizing O
Committee O
. O
Kaveh O
Taghipour O
and O
Hwee O
Tou O
Ng O
. O
2016 O
. O
A O
neural O
approach O
to O
automated O
essay O
scoring O
. O
In O
Proceedings O
of O
the O
2016 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
1882‚Äì1891 O
, O
Austin O
, O
Texas O
. O
Association O
for O
Computational O
Linguistics O
. O
Yi O
Tay O
, O
Minh O
Phan O
, O
Luu O
Anh O
Tuan O
, O
and O
Siu O
Cheung O
Hui O
. O
2018 O
. O
SkipÔ¨Çow O
: O
Incorporating O
neural O
coherence O
features O
for O
end O
- O
to O
- O
end O
automatic O
text O
scoring O
. O
Haoran O
Zhang O
and O
Diane O
Litman O
. O
2018 O
. O
Co O
- O
attention O
based O
neural O
network O
for O
source O
- O
dependent O
essay O
scoring O
. O
In O
Proceedings O
of O
the O
Thirteenth O
Workshop O
on O
Innovative O
Use O
of O
NLP O
for O
Building O
Educational O
Applications O
, O
pages O
399‚Äì409 O
, O
New O
Orleans O
, O
Louisiana O
. O
Association O
for O
Computational O
Linguistics O
. O
A O
Source O
Article O
( O
Prompt O
6 O
) O
The O
Mooring O
Mast O
, O
by O
Marcia O
Amidon O
L O
¬®usted O
When O
the O
Empire O
State O
Building O
was O
conceived O
, O
it O
was O
planned O
as O
the O
world O
‚Äôs O
tallest O
building O
, O
taller O
even O
than O
the O
new O
Chrysler O
Building O
that O
was O
being O
constructed O
at O
Forty O
- O
second O
Street O
and O
Lexington867Avenue O
in O
New O
York O
. O
At O
seventy O
- O
seven O
stories O
, O
it O
was O
the O
tallest O
building O
before O
the O
Empire O
State O
began O
construction O
, O
and O
Al O
Smith O
was O
determined O
to O
outstrip O
it O
in O
height O
. O
The O
architect O
building O
the O
Chrysler O
Building O
, O
however O
, O
had O
a O
trick O
up O
his O
sleeve O
. O
He O
secretly O
constructed O
a O
185 O
- O
foot O
spire O
inside O
the O
building O
, O
and O
then O
shocked O
the O
public O
and O
the O
media O
by O
hoisting O
it O
up O
to O
the O
top O
of O
the O
Chrysler O
Building O
, O
bringing O
it O
to O
a O
height O
of O
1,046 O
feet O
, O
46 O
feet O
taller O
than O
the O
originally O
announced O
height O
of O
the O
Empire O
State O
Building O
. O
Al O
Smith O
realized O
that O
he O
was O
close O
to O
losing O
the O
title O
of O
world O
‚Äôs O
tallest O
building O
, O
and O
on O
December O
11 O
, O
1929 O
, O
he O
announced O
that O
the O
Empire O
State O
would O
now O
reach O
the O
height O
of O
1,250 O
feet O
. O
He O
would O
add O
a O
top O
or O
a O
hat O
to O
the O
building O
that O
would O
be O
even O
more O
distinctive O
than O
any O
other O
building O
in O
the O
city O
. O
John O
Tauranac O
describes O
the O
plan O
: O
‚Äú O
[ O
The O
top O
of O
the O
Empire O
State O
Building O
] O
would O
be O
more O
than O
ornamental O
, O
more O
than O
a O
spire O
or O
dome O
or O
a O
pyramid O
put O
there O
to O
add O
a O
desired O
few O
feet O
to O
the O
height O
of O
the O
building O
or O
to O
mask O
something O
as O
mundane O
as O
a O
water O
tank O
. O
Their O
top O
, O
they O
said O
, O
would O
serve O
a O
higher O
calling O
. O
The O
Empire O
State O
Building O
would O
be O
equipped O
for O
an O
age O
of O
transportation O
that O
was O
then O
only O
the O
dream O
of O
aviation O
pioneers O
. O
‚Äù O
This O
dream O
of O
the O
aviation O
pioneers O
was O
travel O
by O
dirigible O
, O
or O
zeppelin O
, O
and O
the O
Empire O
State O
Building O
was O
going O
to O
have O
a O
mooring O
mast O
at O
its O
top O
for O
docking O
these O
new O
airships O
, O
which O
would O
accommodate O
passengers O
on O
already O
existing O
transatlantic O
routes O
and O
new O
routes O
that O
were O
yet O
to O
come O
. O
A.1 O
The O
Age O
of O
Dirigibles O
By O
the O
1920s O
, O
dirigibles O
were O
being O
hailed O
as O
the O
transportation O
of O
the O
future O
. O
Also O
known O
today O
as O
blimps O
, O
dirigibles O
were O
actually O
enormous O
steelframed O
balloons O
, O
with O
envelopes O
of O
cotton O
fabric O
Ô¨Ålled O
with O
hydrogen O
and O
helium O
to O
make O
them O
lighter O
than O
air O
. O
Unlike O
a O
balloon O
, O
a O
dirigible O
could O
be O
maneuvered O
by O
the O
use O
of O
propellers O
and O
rudders O
, O
and O
passengers O
could O
ride O
in O
the O
gondola O
, O
or O
enclosed O
compartment O
, O
under O
the O
balloon O
. O
Dirigibles O
had O
a O
top O
speed O
of O
eighty O
miles O
per O
hour O
, O
and O
they O
could O
cruise O
at O
seventy O
miles O
per O
hour O
for O
thousands O
of O
miles O
without O
needing O
refueling O
. O
Some O
were O
as O
long O
as O
one O
thousand O
feet O
, O
the O
same O
length O
as O
four O
blocks O
in O
New O
York O
City O
. O
The O
one O
obstacle O
to O
their O
expanded O
use O
in O
NewYork O
City O
was O
the O
lack O
of O
a O
suitable O
landing O
area O
. O
Al O
Smith O
saw O
an O
opportunity O
for O
his O
Empire O
State O
Building O
: O
A O
mooring O
mast O
added O
to O
the O
top O
of O
the O
building O
would O
allow O
dirigibles O
to O
anchor O
there O
for O
several O
hours O
for O
refueling O
or O
service O
, O
and O
to O
let O
passengers O
off O
and O
on O
. O
Dirigibles O
were O
docked O
by O
means O
of O
an O
electric O
winch O
, O
which O
hauled O
in O
a O
line O
from O
the O
front O
of O
the O
ship O
and O
then O
tied O
it O
to O
a O
mast O
. O
The O
body O
of O
the O
dirigible O
could O
swing O
in O
the O
breeze O
, O
and O
yet O
passengers O
could O
safely O
get O
on O
and O
off O
the O
dirigible O
by O
walking O
down O
a O
gangplank O
to O
an O
open O
observation O
platform O
. O
The O
architects O
and O
engineers O
of O
the O
Empire O
State O
Building O
consulted O
with O
experts O
, O
taking O
tours O
of O
the O
equipment O
and O
mooring O
operations O
at O
the O
U.S. O
Naval O
Air O
Station O
in O
Lakehurst O
, O
New O
Jersey O
. O
The O
navy O
was O
the O
leader O
in O
the O
research O
and O
development O
of O
dirigibles O
in O
the O
United O
States O
. O
The O
navy O
even O
offered O
its O
dirigible O
, O
the O
Los O
Angeles O
, O
to O
be O
used O
in O
testing O
the O
mast O
. O
The O
architects O
also O
met O
with O
the O
president O
of O
a O
recently O
formed O
airship O
transport O
company O
that O
planned O
to O
offer O
dirigible O
service O
across O
the O
PaciÔ¨Åc O
Ocean O
. O
When O
asked O
about O
the O
mooring O
mast O
, O
Al O
Smith O
commented O
: O
‚Äú O
[ O
It O
‚Äôs O
] O
on O
the O
level O
, O
all O
right O
. O
No O
kidding O
. O
We O
‚Äôre O
working O
on O
the O
thing O
now O
. O
One O
set O
of O
engineers O
here O
in O
New O
York O
is O
trying O
to O
dope O
out O
a O
practical O
, O
workable O
arrangement O
and O
the O
Government O
people O
in O
Washington O
are O
Ô¨Åguring O
on O
some O
safe O
way O
of O
mooring O
airships O
to O
this O
mast O
. O
‚Äù O
A.2 O
Designing O
the O
Mast O
The O
architects O
could O
not O
simply O
drop O
a O
mooring O
mast O
on O
top O
of O
the O
Empire O
State O
Building O
‚Äôs O
Ô¨Çat O
roof O
. O
A O
thousand O
- O
foot O
dirigible O
moored O
at O
the O
top O
of O
the O
building O
, O
held O
by O
a O
single O
cable O
tether O
, O
would O
add O
stress O
to O
the O
building O
‚Äôs O
frame O
. O
The O
stress O
of O
the O
dirigible O
‚Äôs O
load O
and O
the O
wind O
pressure O
would O
have O
to O
be O
transmitted O
all O
the O
way O
to O
the O
building O
‚Äôs O
foundation O
, O
which O
was O
nearly O
eleven O
hundred O
feet O
below O
. O
The O
steel O
frame O
of O
the O
Empire O
State O
Building O
would O
have O
to O
be O
modiÔ¨Åed O
and O
strengthened O
to O
accommodate O
this O
new O
situation O
. O
Over O
sixty O
thousand O
dollars O
‚Äô O
worth O
of O
modiÔ¨Åcations O
had O
to O
be O
made O
to O
the O
building O
‚Äôs O
framework O
. O
Rather O
than O
building O
a O
utilitarian O
mast O
without O
any O
ornamentation O
, O
the O
architects O
designed O
a O
shiny O
glass O
and O
chrome O
- O
nickel O
stainless O
steel O
tower O
that O
would O
be O
illuminated O
from O
inside O
, O
with O
a O
steppedback O
design O
that O
imitated O
the O
overall O
shape O
of O
the868building O
itself O
. O
The O
rocket O
- O
shaped O
mast O
would O
have O
four O
wings O
at O
its O
corners O
, O
of O
shiny O
aluminum O
, O
and O
would O
rise O
to O
a O
conical O
roof O
that O
would O
house O
the O
mooring O
arm O
. O
The O
winches O
and O
control O
machinery O
for O
the O
dirigible O
mooring O
would O
be O
housed O
in O
the O
base O
of O
the O
shaft O
itself O
, O
which O
also O
housed O
elevators O
and O
stairs O
to O
bring O
passengers O
down O
to O
the O
eightysixth O
Ô¨Çoor O
, O
where O
baggage O
and O
ticket O
areas O
would O
be O
located O
. O
The O
building O
would O
now O
be O
102 O
Ô¨Çoors O
, O
with O
a O
glassed O
- O
in O
observation O
area O
on O
the O
101st O
Ô¨Çoor O
and O
an O
open O
observation O
platform O
on O
the O
102nd O
Ô¨Çoor O
. O
This O
observation O
area O
was O
to O
double O
as O
the O
boarding O
area O
for O
dirigible O
passengers O
. O
Once O
the O
architects O
had O
designed O
the O
mooring O
mast O
and O
made O
changes O
to O
the O
existing O
plans O
for O
the O
building O
‚Äôs O
skeleton O
, O
construction O
proceeded O
as O
planned O
. O
When O
the O
building O
had O
been O
framed O
to O
the O
85th O
Ô¨Çoor O
, O
the O
roof O
had O
to O
be O
completed O
before O
the O
framing O
for O
the O
mooring O
mast O
could O
take O
place O
. O
The O
mast O
also O
had O
a O
skeleton O
of O
steel O
and O
was O
clad O
in O
stainless O
steel O
with O
glass O
windows O
. O
Two O
months O
after O
the O
workers O
celebrated O
framing O
the O
entire O
building O
, O
they O
were O
back O
to O
raise O
an O
American O
Ô¨Çag O
again O
‚Äî O
this O
time O
at O
the O
top O
of O
the O
frame O
for O
the O
mooring O
mast O
. O
A.3 O
The O
Fate O
of O
the O
Mast O
The O
mooring O
mast O
of O
the O
Empire O
State O
Building O
was O
destined O
to O
never O
fulÔ¨Åll O
its O
purpose O
, O
for O
reasons O
that O
should O
have O
been O
apparent O
before O
it O
was O
ever O
constructed O
. O
The O
greatest O
reason O
was O
one O
of O
safety O
: O
Most O
dirigibles O
from O
outside O
of O
the O
United O
States O
used O
hydrogen O
rather O
than O
helium O
, O
and O
hydrogen O
is O
highly O
Ô¨Çammable O
. O
When O
the O
German O
dirigible O
Hindenburg O
was O
destroyed O
by O
Ô¨Åre O
in O
Lakehurst O
, O
New O
Jersey O
, O
on O
May O
6 O
, O
1937 O
, O
the O
owners O
of O
the O
Empire O
State O
Building O
realized O
how O
much O
worse O
that O
accident O
could O
have O
been O
if O
it O
had O
taken O
place O
above O
a O
densely O
populated O
area O
such O
as O
downtown O
New O
York O
. O
The O
greatest O
obstacle O
to O
the O
successful O
use O
of O
the O
mooring O
mast O
was O
nature O
itself O
. O
The O
winds O
on O
top O
of O
the O
building O
were O
constantly O
shifting O
due O
to O
violent O
air O
currents O
. O
Even O
if O
the O
dirigible O
were O
tethered O
to O
the O
mooring O
mast O
, O
the O
back O
of O
the O
ship O
would O
swivel O
around O
and O
around O
the O
mooring O
mast O
. O
Dirigibles O
moored O
in O
open O
landing O
Ô¨Åelds O
could O
be O
weighted O
down O
in O
the O
back O
with O
lead O
weights O
, O
but O
using O
these O
at O
the O
Empire O
State O
Building O
, O
where O
they O
would O
be O
dangling O
high O
above O
pedestrians O
onthe O
street O
, O
was O
neither O
practical O
nor O
safe O
. O
The O
other O
practical O
reason O
why O
dirigibles O
could O
not O
moor O
at O
the O
Empire O
State O
Building O
was O
an O
existing O
law O
against O
airships O
Ô¨Çying O
too O
low O
over O
urban O
areas O
. O
This O
law O
would O
make O
it O
illegal O
for O
a O
ship O
to O
ever O
tie O
up O
to O
the O
building O
or O
even O
approach O
the O
area O
, O
although O
two O
dirigibles O
did O
attempt O
to O
reach O
the O
building O
before O
the O
entire O
idea O
was O
dropped O
. O
In O
December O
1930 O
, O
the O
U.S. O
Navy O
dirigible O
Los O
Angeles O
approached O
the O
mooring O
mast O
but O
could O
not O
get O
close O
enough O
to O
tie O
up O
because O
of O
forceful O
winds O
. O
Fearing O
that O
the O
wind O
would O
blow O
the O
dirigible O
onto O
the O
sharp O
spires O
of O
other O
buildings O
in O
the O
area O
, O
which O
would O
puncture O
the O
dirigible O
‚Äôs O
shell O
, O
the O
captain O
could O
not O
even O
take O
his O
hands O
off O
the O
control O
levers O
. O
Two O
weeks O
later O
, O
another O
dirigible O
, O
the O
Goodyear O
blimp O
Columbia O
, O
attempted O
a O
publicity O
stunt O
where O
it O
would O
tie O
up O
and O
deliver O
a O
bundle O
of O
newspapers O
to O
the O
Empire O
State O
Building O
. O
Because O
the O
complete O
dirigible O
mooring O
equipment O
had O
never O
been O
installed O
, O
a O
worker O
atop O
the O
mooring O
mast O
would O
have O
to O
catch O
the O
bundle O
of O
papers O
on O
a O
rope O
dangling O
from O
the O
blimp O
. O
The O
papers O
were O
delivered O
in O
this O
fashion O
, O
but O
after O
this O
stunt O
the O
idea O
of O
using O
the O
mooring O
mast O
was O
shelved O
. O
In O
February O
1931 O
, O
Irving O
Clavan O
of O
the O
building O
‚Äôs O
architectural O
ofÔ¨Åce O
said O
, O
‚Äú O
The O
as O
yet O
unsolved O
problems O
of O
mooring O
air O
ships O
to O
a O
Ô¨Åxed O
mast O
at O
such O
a O
height O
made O
it O
desirable O
to O
postpone O
to O
a O
later O
date O
the O
Ô¨Ånal O
installation O
of O
the O
landing O
gear O
. O
‚Äù O
By O
the O
late O
1930s O
, O
the O
idea O
of O
using O
the O
mooring O
mast O
for O
dirigibles O
and O
their O
passengers O
had O
quietly O
disappeared O
. O
Dirigibles O
, O
instead O
of O
becoming O
the O
transportation O
of O
the O
future O
, O
had O
given O
way O
to O
airplanes O
. O
The O
rooms O
in O
the O
Empire O
State O
Building O
that O
had O
been O
set O
aside O
for O
the O
ticketing O
and O
baggage O
of O
dirigible O
passengers O
were O
made O
over O
into O
the O
world O
‚Äôs O
highest O
soda O
fountain O
and O
tea O
garden O
for O
use O
by O
the O
sightseers O
who O
Ô¨Çocked O
to O
the O
observation O
decks O
. O
The O
highest O
open O
observation O
deck O
, O
intended O
for O
disembarking O
passengers O
, O
has O
never O
been O
open O
to O
the O
public O
. O
B O
Annotator O
ProÔ¨Åles O
Table O
9 O
summarizes O
the O
proÔ¨Åles O
of O
the O
different O
annotators O
. O
It O
details O
each O
of O
the O
8 O
annotators O
, O
their O
sex O
, O
age O
, O
occupations O
, O
L1 O
/ O
native O
languages O
, O
their O
performance O
in O
a O
high O
school O
Examination O
in O
English O
and O
whether O
or O
not O
they O
have O
had O
experience O
as O
a O
TA O
. O
The O
last O
3 O
columns O
are O
their O
performance869ID O
Sex O
Age O
Occupation O
TA O
? O
L1 O
Language O
English O
Score O
QWK O
Correct O
Close O
Annotator O
1 O
Male O
23 O
Masters O
student O
Yes O
Hindi O
94 O
% O
0.611 O
19 O
41 O
Annotator O
2 O
Male O
18 O
Undergraduate O
Yes O
Marathi O
95 O
% O
0.587 O
24 O
41 O
Annotator O
3 O
Male O
31 O
Research O
scholar O
Yes O
Marathi O
85 O
% O
0.659 O
21 O
43 O
Annotator O
4 O
Male O
28 O
Software O
engineer O
Yes O
English O
96 O
% O
0.659 O
26 O
44 O
Annotator O
5 O
Male O
30 O
Research O
scholar O
Yes O
Gujarati O
92 O
% O
0.600 O
19 O
42 O
Annotator O
6 O
Female O
22 O
Masters O
student O
Yes O
Marathi O
95 O
% O
0.548 O
19 O
40 O
Annotator O
7 O
Male O
19 O
Undergraduate O
Yes O
Marathi O
93 O
% O
0.732 O
21 O
46 O
Annotator O
8 O
Male O
28 O
Masters O
student O
Yes O
Gujarati O
94 O
% O
0.768 O
29 O
45 O
Table O
9 O
: O
ProÔ¨Åle O
of O
the O
annotators O
on O
the O
annotation O
grading O
task O
, O
where O
QWK O
is O
their O
agreement O
with O
the O
ground O
truth O
scores O
, O
Correct O
is O
the O
number O
of O
times O
( O
out O
of O
48 O
) O
where O
their O
essay O
scores O
matched O
with O
the O
ground O
truth O
scores O
, O
and O
Close O
is O
the O
number O
of O
times O
( O
out O
of O
48 O
) O
where O
they O
disagreed O
with O
the O
ground O
truth O
score O
by O
at O
most O
1 O
grade O
point O
. O
C O
Heat O
Map O
Examples O
C.1 O
Different O
Gaze O
Features O
Here O
, O
we O
show O
examples O
of O
heat O
maps O
for O
different O
gaze O
behaviour O
attributes O
of O
one O
of O
our O
readers O
. O
1 O
. O
Figure O
3 O
shows O
the O
dwell O
time O
of O
the O
reader O
. O
2.Figure O
4 O
shows O
the O
heat O
map O
of O
the O
Ô¨Årst O
Ô¨Åxation O
duration O
of O
a O
reader O
. O
3.Figure O
5 O
shows O
the O
heat O
map O
of O
the O
IsRegression O
feature O
- O
i.e. O
whether O
or O
not O
the O
reader O
regressed O
from O
a O
particular O
word O
. O
4.Figure O
6 O
shows O
the O
heat O
map O
of O
the O
Run O
Count O
of O
the O
reader O
. O
5.Figure O
7 O
shows O
the O
words O
that O
the O
reader O
read O
( O
highlighted O
) O
and O
skipped O
( O
unhighlighted O
) O
. O
C.2 O
Dwell O
Times O
of O
Good O
and O
Bad O
Essays O
Figures O
8 O
and O
9 O
show O
the O
dwell O
time O
heat O
maps O
of O
a O
reader O
as O
he O
reads O
a O
good O
essay O
and O
a O
bad O
essay O
respectively O
. O
For O
the O
bad O
essay O
, O
notice O
the O
amount O
of O
a O
lot O
more O
darker O
blues O
compared O
to O
the O
good O
essay O
. O
D O
P O
- O
Values O
In O
this O
section O
, O
we O
report O
the O
p O
- O
values O
and O
other O
results O
for O
our O
experiments O
. O
D.1 O
Source O
- O
Dependent O
Essay O
Set O
‚Äôs O
p O
- O
values O
The O
results O
shown O
here O
in O
Table O
10 O
are O
the O
p O
- O
values O
for O
the O
different O
essay O
sets O
with O
and O
without O
gaze O
from O
Table O
5.Essay O
Set O
p O
- O
value O
Prompt O
3 O
0.0042 O
Prompt O
4 O
0.0109 O
Prompt O
5 O
0.0133 O
Prompt O
6 O
0.0003 O
Table O
10 O
: O
Source O
- O
Dependent O
essay O
set O
‚Äôs O
p O
- O
values O
D.2 O
Unseen O
Essay O
Set O
‚Äôs O
p O
- O
values O
The O
results O
shown O
here O
in O
Table O
10 O
are O
the O
p O
- O
values O
for O
the O
different O
essay O
sets O
with O
and O
without O
gaze O
from O
Table O
6 O
. O
Essay O
Set O
p O
- O
value O
Prompt O
1 O
0.0887 O
Prompt O
2 O
0.1380 O
Prompt O
7 O
0.0393 O
Prompt O
8 O
0.0315 O
Table O
11 O
: O
Unseen O
Essay O
‚Äôs O
p O
- O
values O
D.3 O
Native O
Gaze O
vs. O
No O
Gaze O
& O
All O
Gaze O
p O
- O
values O
The O
results O
shown O
in O
Table O
12 O
are O
the O
p O
- O
values O
for O
the O
essay O
sets O
using O
the O
gaze O
behaviour O
of O
a O
native O
English O
speaker O
compared O
to O
not O
using O
gaze O
behaviour O
, O
and O
using O
gaze O
behaviour O
of O
all O
readers O
. O
Essay O
Set O
No O
vs. O
Native O
Native O
vs. O
All O
Prompt O
1 O
0.1407 O
0.0471 O
Prompt O
2 O
0.0161 O
0.9161 O
Prompt O
3 O
0.3239 O
0.0239 O
Prompt O
4 O
0.0810 O
0.0805 O
Prompt O
5 O
0.4971 O
0.4010 O
Prompt O
6 O
0.2462 O
0.2961 O
Prompt O
7 O
0.0189 O
0.0098 O
Prompt O
8 O
0.8768 O
0.0068 O
Table O
12 O
: O
No O
gaze O
vs. O
native O
gaze O
and O
native O
gaze O
vs. O
all O
gaze O
p O
- O
values.870Figure O
3 O
: O
Sample O
heat O
map O
of O
the O
dwell O
of O
a O
reader O
for O
the O
text O
. O
The O
darker O
the O
blue O
, O
the O
larger O
the O
bin O
, O
and O
the O
longer O
the O
dwell O
time O
. O
Figure O
4 O
: O
Sample O
heat O
map O
of O
the O
Ô¨Årst O
Ô¨Åxation O
duration O
of O
a O
reader O
for O
the O
text O
. O
The O
darker O
the O
blue O
, O
the O
larger O
the O
bin O
, O
and O
the O
longer O
the O
Ô¨Årst O
Ô¨Åxation O
duration O
. O
Figure O
5 O
: O
Sample O
heat O
map O
of O
the O
Is O
Regression O
feature O
of O
a O
reader O
for O
the O
text O
. O
The O
highlighted O
words O
denote O
words O
that O
the O
reader O
regressed O
from O
. O
Figure O
6 O
: O
Sample O
heat O
map O
of O
the O
run O
count O
of O
a O
reader O
for O
the O
text O
. O
The O
darker O
the O
blue O
, O
the O
larger O
the O
bin O
, O
and O
the O
higher O
the O
run O
count O
. O
Figure O
7 O
: O
Sample O
heat O
map O
of O
the O
Skip O
feature O
of O
a O
reader O
for O
the O
text O
. O
The O
unhighlighted O
words O
denote O
words O
that O
the O
reader O
skipped.871Figure O
8 O
: O
Dwell O
Time O
for O
a O
reader O
for O
an O
essay O
which O
he O
scored O
well O
. O
Figure O
9 O
: O
Dwell O
Time O
for O
a O
reader O
for O
an O
essay O
which O
he O
scored O
badly.872Proceedings O
of O
the O
1st O
Conference O
of O
the O
Asia O
- O
PaciÔ¨Åc O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
10th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
873‚Äì883 O
December O
4 O
- O
7 O
, O
2020 O
. O
¬© O
2020 O
Association O
for O
Computational O
Linguistics O
Multi O
- O
Source O
Attention O
for O
Unsupervised O
Domain O
Adaptation O
Xia O
Cui O
University O
of O
Liverpool O
Xia O
. O
Cui@liverpool.ac.ukDanushka O
Bollegala‚àó O
University O
of O
Liverpool O
, O
Amazon O
danushka@liverpool.ac.uk O
Abstract O
We O
model O
source O
- O
selection O
in O
multi O
- O
source O
Unsupervised O
Domain O
Adaptation O
( O
UDA O
) O
as O
an O
attention O
- O
learning O
problem O
, O
where O
we O
learn O
attention O
over O
the O
sources O
per O
given O
target O
instance O
. O
We O
Ô¨Årst O
independently O
learn O
sourcespeciÔ¨Åc O
classiÔ¨Åcation O
models O
, O
and O
a O
relatedness O
map O
between O
sources O
and O
target O
domains O
using O
pseudo O
- O
labelled O
target O
domain O
instances O
. O
Next O
, O
we O
learn O
domain O
- O
attention O
scores O
over O
the O
sources O
for O
aggregating O
the O
predictions O
of O
the O
source O
- O
speciÔ¨Åc O
models O
. O
Experimental O
results O
on O
two O
cross O
- O
domain O
sentiment O
classiÔ¨Åcation O
datasets O
show O
that O
the O
proposed O
method O
reports O
consistently O
good O
performance O
across O
domains O
, O
and O
at O
times O
outperforming O
more O
complex O
prior O
proposals O
. O
Moreover O
, O
the O
computed O
domain O
- O
attention O
scores O
enable O
us O
to O
Ô¨Ånd O
explanations O
for O
the O
predictions O
made O
by O
the O
proposed O
method.1 O
1 O
Introduction O
Domain O
adaptation O
( O
DA O
) O
considers O
the O
problem O
of O
generalising O
a O
model O
learnt O
using O
the O
data O
from O
a O
particular O
source O
domain O
to O
a O
different O
target O
domain O
( O
Zhang O
et O
al O
. O
, O
2015 O
) O
. O
Although O
most O
DA O
methods O
consider O
adapting O
to O
a O
target O
domain O
from O
a O
single O
source O
domain O
( O
Blitzer O
et O
al O
. O
, O
2006 O
, O
2007 O
; O
Ganin O
et O
al O
. O
, O
2016 O
) O
, O
often O
it O
is O
difÔ¨Åcult O
to O
Ô¨Ånd O
a O
suitable O
single O
source O
to O
adapt O
from O
, O
and O
one O
must O
consider O
multiple O
sources O
. O
For O
example O
, O
in O
sentiment O
classiÔ¨Åcation O
, O
each O
product O
category O
is O
considered O
as O
a O
domain O
( O
Blitzer O
et O
al O
. O
, O
2006 O
) O
, O
resulting O
in O
a O
multi O
- O
domain O
adaptation O
setting O
. O
Unsupervised O
DA O
( O
UDA O
) O
is O
a O
special O
case O
of O
DA O
where O
labelled O
instances O
are O
not O
available O
for O
‚àóDanushka O
Bollegala O
holds O
concurrent O
appointments O
as O
a O
Professor O
at O
University O
of O
Liverpool O
and O
as O
an O
Amazon O
Scholar O
. O
This O
paper O
describes O
work O
performed O
at O
the O
University O
of O
Liverpool O
and O
is O
not O
associated O
with O
Amazon O
. O
1Source O
code O
available O
at O
https://github.com/ O
LivNLP O
/ O
multi O
- O
source O
- O
attentionthe O
target O
domain O
. O
Existing O
approaches O
for O
UDA O
can O
be O
categorised O
into O
pivot O
- O
based O
and O
instancebased O
methods O
. O
Pivots O
refer O
to O
the O
features O
common O
to O
both O
source O
and O
target O
domains O
( O
Blitzer O
et O
al O
. O
, O
2006 O
) O
. O
Pivot O
- O
based O
single O
- O
source O
domain O
adaptation O
methods O
, O
such O
as O
Structural O
Correspondence O
Learning O
( O
SCL O
; O
Blitzer O
et O
al O
. O
, O
2006 O
, O
2007 O
) O
and O
Spectral O
Feature O
Alignment O
( O
SFA O
; O
Pan O
et O
al O
. O
, O
2010 O
) O
, O
Ô¨Årst O
select O
a O
set O
of O
pivots O
and O
then O
project O
the O
source O
and O
target O
domain O
documents O
into O
a O
shared O
space O
. O
Next O
, O
a O
prediction O
model O
is O
learnt O
in O
this O
shared O
space O
. O
However O
, O
these O
methods O
fail O
in O
multi O
- O
source O
settings O
because O
it O
is O
challenging O
to O
Ô¨Ånd O
pivots O
across O
all O
sources O
such O
that O
a O
single O
shared O
projection O
can O
be O
learnt O
. O
Similarly O
, O
instance O
- O
based O
methods O
, O
such O
as O
Stacked O
Denoising O
Autoencoders O
( O
SDA O
; O
Glorot O
et O
al O
. O
, O
2011 O
) O
and O
marginalised O
SDA O
( O
mSDA O
; O
Chen O
et O
al O
. O
, O
2012 O
) O
minimise O
the O
loss O
between O
the O
original O
inputs O
and O
their O
reconstructions O
. O
Not O
all O
of O
the O
source O
domains O
are O
appropriate O
for O
learning O
transferable O
projections O
for O
a O
particular O
target O
domain O
. O
Adapting O
from O
an O
unrelated O
source O
can O
result O
in O
poor O
performance O
on O
the O
given O
target O
, O
which O
is O
known O
as O
negative O
transfer O
( O
Rosenstein O
et O
al O
. O
, O
2005 O
; O
Pan O
and O
Yang O
, O
2010 O
; O
Guo O
et O
al O
. O
, O
2018 O
) O
. O
Prior O
proposals O
for O
multi O
- O
source O
UDA O
can O
be O
broadly O
classiÔ¨Åed O
into O
methods O
that O
: O
( O
a O
) O
Ô¨Årst O
select O
a O
source O
domain O
and O
then O
select O
instances O
from O
that O
source O
domain O
to O
adapt O
to O
a O
given O
target O
domain O
test O
instance O
( O
Ganin O
et O
al O
. O
, O
2016 O
; O
Kim O
et O
al O
. O
, O
2017 O
; O
Zhao O
et O
al O
. O
, O
2018 O
; O
Guo O
et O
al O
. O
, O
2018 O
) O
; O
( O
b O
) O
pool O
all O
source O
domain O
instances O
together O
and O
from O
this O
pool O
select O
instances O
to O
adapt O
to O
a O
given O
target O
domain O
test O
instance O
( O
Chattopadhyay O
et O
al O
. O
, O
2012 O
) O
; O
( O
c O
) O
pick O
a O
source O
domain O
and O
use O
all O
instances O
in O
that O
source O
( O
source O
domain O
selection O
) O
( O
Schultz O
et O
al O
. O
, O
2018 O
) O
; O
and O
( O
d O
) O
pick O
all O
source O
domains O
and O
use O
all O
instances O
( O
utilising O
all O
instances O
) O
( O
Aue O
and O
Gamon O
, O
2005 O
; O
Bollegala O
et O
al O
. O
, O
2011 O
; O
Wu O
and O
Huang O
, O
2016 O
) O
. O
In O
contrast O
, O
we O
propose O
a O
multi O
- O
source O
UDA873method O
that O
systematically O
addresses O
the O
various O
challenges O
in O
multi O
- O
source O
UDA O
. O
‚Ä¢Although O
in O
UDA O
we O
have O
labelled O
instances O
in O
each O
source O
domain O
, O
its O
number O
is O
signiÔ¨Åcantly O
smaller O
than O
that O
of O
the O
unlabelled O
instances O
in O
the O
same O
domain O
. O
For O
example O
, O
in O
the O
Amazon O
product O
review O
dataset O
released O
by O
Blitzer O
et O
al O
. O
( O
2007 O
) O
there O
are O
73679 O
unlabelled O
instances O
in O
total O
across O
the O
four O
domains O
, O
whereas O
there O
are O
only O
4800 O
labelled O
instances O
. O
To O
increase O
the O
labelled O
instances O
in O
a O
source O
domain O
, O
we O
induce O
pseudo O
- O
labels O
for O
the O
unlabelled O
instances O
in O
each O
source O
domain O
using O
self O
- O
training O
as O
in O
¬ß O
3.1 O
. O
‚Ä¢In O
UDA O
, O
we O
have O
no O
labelled O
data O
for O
the O
target O
domain O
. O
To O
address O
this O
challenge O
, O
we O
infer O
pseudo O
- O
labels O
for O
the O
target O
domain O
‚Äôs O
unlabelled O
training O
instances O
by O
majority O
voting O
over O
the O
classiÔ¨Åers O
trained O
from O
each O
source O
domain O
, O
using O
both O
labelled O
and O
pseudolabelled O
instances O
as O
in O
¬ß O
3.1 O
. O
‚Ä¢Given O
that O
the O
pseudo O
- O
labels O
inferred O
for O
the O
target O
domain O
instances O
are O
inherently O
more O
noisier O
compared O
to O
the O
manually O
labelled O
source O
domain O
instances O
, O
we O
propose O
a O
method O
to O
identify O
a O
subset O
of O
prototypical O
target O
domain O
instances O
for O
DA O
using O
document O
embedding O
similarities O
as O
described O
in O
¬ß O
3.2 O
. O
‚Ä¢The O
accuracy O
of O
UDA O
is O
upper O
- O
bounded O
by O
theH O
- O
divergence O
between O
a O
source O
and O
a O
target O
domain O
( O
Kifer O
et O
al O
. O
, O
2004 O
; O
Ben O
- O
David O
et O
al O
. O
, O
2006 O
, O
2009 O
) O
. O
Therefore O
, O
when O
predicting O
the O
label O
of O
a O
target O
domain O
test O
instance O
, O
we O
must O
select O
only O
the O
relevant O
labelled O
instances O
from O
a O
source O
domain O
. O
We O
propose O
a O
method O
to O
learn O
such O
a O
relatedness O
map O
between O
source O
and O
target O
domains O
in O
¬ß O
3.3 O
. O
‚Ä¢To O
reduce O
negative O
transfer O
, O
for O
each O
target O
domain O
test O
instance O
we O
dynamically O
compute O
a O
domain O
- O
attention O
score O
that O
expresses O
the O
relevance O
of O
a O
source O
domain O
. O
For O
this O
purpose O
, O
we O
represent O
each O
domain O
by O
a O
domain O
embedding O
, O
which O
we O
learn O
in O
an O
end O
- O
to O
- O
end O
fashion O
using O
the O
target O
domain O
‚Äôs O
pseudo O
- O
labelled O
instances O
as O
detailed O
in O
¬ß O
3.4 O
. O
We O
evaluate O
the O
proposed O
method O
on O
two O
standard O
cross O
- O
domain O
sentiment O
classiÔ¨Åcation O
benchmarks O
for O
UDA O
. O
We O
Ô¨Ånd O
that O
both O
pseudo O
- O
labels O
and O
domain O
- O
attention O
scores O
contribute O
toward O
improving O
the O
classiÔ¨Åcation O
accuracy O
for O
a O
target O
domain O
. O
The O
proposed O
method O
reports O
consistently O
goodperformance O
in O
both O
datasets O
and O
across O
multiple O
domains O
. O
Although O
the O
proposed O
method O
does O
not O
outperform O
more O
complex O
UDA O
methods O
in O
some O
cases O
, O
using O
the O
domain O
- O
attention O
scores O
, O
we O
are O
able O
to O
retrieve O
justiÔ¨Åcations O
for O
the O
predicted O
labels O
. O
2 O
Related O
Work O
In¬ß1 O
we O
already O
mentioned O
prior O
proposals O
for O
single O
- O
source O
DA O
and O
this O
section O
discusses O
multisource O
DA O
, O
which O
is O
the O
main O
focus O
of O
this O
paper O
. O
Bollegala O
et O
al O
. O
( O
2011 O
) O
created O
a O
sentiment O
sensitive O
thesaurus O
( O
SST O
) O
using O
the O
data O
from O
the O
union O
of O
multiple O
source O
domains O
to O
train O
a O
cross O
- O
domain O
sentiment O
classiÔ¨Åer O
. O
The O
SST O
is O
used O
to O
expand O
feature O
spaces O
during O
train O
and O
test O
times O
. O
The O
performance O
of O
SST O
depends O
heavily O
on O
the O
selection O
of O
pivots O
( O
Cui O
et O
al O
. O
, O
2017 O
; O
Li O
et O
al O
. O
, O
2017 O
) O
. O
Wu O
and O
Huang O
( O
2016 O
) O
proposed O
a O
sentiment O
DA O
method O
from O
multiple O
sources O
( O
SDAMS O
) O
by O
introducing O
two O
components O
: O
a O
sentiment O
graph O
and O
a O
domain O
similarity O
measure O
. O
The O
sentiment O
graph O
is O
extracted O
from O
unlabelled O
data O
. O
Similar O
to O
SST O
, O
SDAMS O
uses O
data O
from O
multiple O
sources O
to O
maximise O
the O
available O
labelled O
data O
. O
Guo O
et O
al O
. O
( O
2020 O
) O
proposed O
a O
mixture O
of O
distance O
measures O
and O
used O
a O
multi O
- O
arm O
bandit O
to O
dynamically O
select O
a O
single O
source O
during O
training O
. O
However O
, O
in O
our O
proposed O
method O
all O
domains O
are O
selected O
and O
contributing O
differently O
as O
speciÔ¨Åed O
by O
their O
domain O
- O
attention O
weights O
for O
each O
train O
and O
test O
instance O
. O
Moreover O
, O
we O
use O
only O
one O
distance O
measure O
and O
is O
easier O
to O
implement O
. O
Recently O
, O
Adversarial O
NNs O
have O
become O
popular O
in O
DA O
( O
Ganin O
et O
al O
. O
, O
2016 O
; O
Zhao O
et O
al O
. O
, O
2018 O
; O
Guo O
et O
al O
. O
, O
2018 O
) O
. O
Adversarial O
training O
is O
used O
to O
reduce O
the O
discrepancy O
between O
source O
and O
target O
domains O
( O
Ding O
et O
al O
. O
, O
2019 O
) O
. O
Domain O
- O
Adversarial O
Neural O
Networks O
( O
DANN O
; O
Ganin O
et O
al O
. O
, O
2016 O
) O
use O
a O
gradient O
reversal O
layer O
to O
learn O
domain O
independent O
features O
for O
a O
given O
task O
. O
Multiple O
Source O
Domain O
Adaptation O
with O
Adversarial O
Learning O
( O
MDAN O
; O
Zhao O
et O
al O
. O
, O
2018 O
) O
generalises O
DANN O
and O
aims O
to O
learn O
domain O
independent O
features O
while O
being O
relevant O
to O
the O
target O
task O
. O
Li O
et O
al O
. O
( O
2017 O
) O
proposed O
End O
- O
to O
- O
End O
Adversarial O
Memory O
Network O
( O
AMN O
) O
, O
inspired O
by O
memory O
networks O
( O
Sukhbaatar O
et O
al O
. O
, O
2015 O
) O
, O
and O
automatically O
capture O
pivots O
using O
an O
attention O
mechanism O
. O
Guo O
et O
al O
. O
( O
2018 O
) O
proposed O
an O
UDA O
method O
using O
a O
mixture O
of O
experts O
for O
each O
domain O
. O
They O
model O
the O
domain O
relations874using O
a O
point O
- O
to O
- O
set O
distance O
metric O
to O
the O
encoded O
training O
matrix O
for O
source O
domains O
. O
Next O
, O
they O
perform O
joint O
training O
over O
all O
domain O
- O
pairs O
to O
update O
the O
parameters O
in O
the O
model O
by O
meta O
- O
training O
. O
However O
, O
they O
ignore O
the O
available O
unlabelled O
instances O
for O
the O
source O
domain O
. O
Adversarial O
training O
methods O
have O
shown O
to O
be O
sensitive O
to O
the O
hyper O
parameter O
values O
and O
require O
problem O
- O
speciÔ¨Åc O
techniques O
( O
Mukherjee O
et O
al O
. O
, O
2018 O
) O
. O
Kim O
et O
al O
. O
( O
2017 O
) O
modeled O
domain O
relations O
using O
exampleto O
- O
domain O
based O
on O
an O
attention O
mechanism O
. O
However O
, O
the O
attention O
weights O
are O
learnt O
using O
source O
domain O
training O
data O
in O
a O
supervised O
manner O
. O
Following O
a O
self O
- O
training O
approach O
, O
Chattopadhyay O
et O
al O
. O
( O
2012 O
) O
proposed O
a O
two O
- O
stage O
weighting O
framework O
for O
multi O
- O
source O
DA O
that O
Ô¨Årst O
computes O
the O
weights O
for O
features O
from O
different O
source O
domains O
using O
Maximum O
Mean O
Discrepancy O
( O
MMD O
; O
Borgwardt O
et O
al O
. O
, O
2006 O
) O
. O
Next O
, O
they O
generate O
pseudo O
labels O
for O
the O
target O
unlabelled O
instances O
using O
a O
classiÔ¨Åer O
learnt O
from O
the O
multiple O
source O
domains O
. O
Finally O
, O
a O
classiÔ¨Åer O
is O
trained O
on O
the O
pseudo O
- O
labelled O
instances O
for O
the O
target O
domain O
. O
Their O
method O
requires O
labelled O
data O
for O
the O
target O
domain O
, O
which O
is O
asupervised O
DA O
setting O
, O
different O
from O
the O
UDA O
setting O
we O
consider O
in O
this O
paper O
. O
Our O
proposed O
method O
uses O
self O
- O
training O
to O
assign O
pseudo O
- O
labels O
for O
the O
unlabelled O
target O
instances O
, O
and O
learn O
an O
embedding O
for O
each O
domain O
using O
an O
attention O
mechanism O
. O
3 O
Multi O
- O
Source O
Domain O
Attention O
Let O
us O
assume O
that O
we O
are O
given O
Nsource O
domains O
, O
S1,S2, O
... O
,S O
N O
, O
and O
required O
to O
adapt O
to O
a O
target O
domainT. O
Moreover O
, O
let O
us O
denote O
the O
labelled O
instances O
in O
SibySL O
iand O
unlabelled O
instances O
by O
SU O
i. O
ForTwe O
have O
only O
unlabelled O
instances O
TU O
in O
UDA O
. O
Our O
goal O
is O
to O
learn O
a O
binary O
classiÔ¨Åer2 O
to O
predict O
labels O
( O
‚àà{0,1 O
} O
) O
for O
the O
target O
domain O
instances O
usingSL=‚à™N O
i=1SL O
i O
, O
SU=‚à™N O
i=1SU O
iand O
TU O
. O
We O
denote O
labelled O
and O
unlabelled O
instances O
in O
Siby O
respectively O
xL O
iandxU O
i O
, O
whereas O
instances O
in O
Tare O
denoted O
by O
xT. O
To O
simplify O
the O
notation O
, O
we O
drop O
the O
superscripts O
LandUwhen O
it O
is O
clear O
from O
the O
context O
whether O
the O
instance O
is O
respectively O
labelled O
or O
not O
. O
The O
steps O
of O
our O
proposed O
method O
can O
be O
summarised O
as O
follows O
: O
( O
a O
) O
use O
labelled O
and O
unlabelled O
2Although O
we O
consider O
binary O
sentiment O
classiÔ¨Åcation O
as O
an O
evaluation O
task O
in O
this O
paper O
, O
the O
proposed O
method O
can O
be O
easily O
extended O
to O
multi O
- O
class O
classiÔ¨Åcation O
settings O
by O
making O
1 O
- O
vs O
- O
rest O
prediction O
tasks O
( O
Rifkin O
and O
Klautau O
, O
2004).instances O
from O
each O
of O
the O
source O
domains O
to O
learn O
classiÔ¨Åers O
that O
can O
predict O
the O
label O
for O
a O
given O
instance O
. O
Next O
, O
develop O
a O
majority O
voter O
and O
use O
it O
to O
predict O
the O
pseudo O
- O
labels O
for O
the O
target O
domain O
unlabelled O
instances O
TU(¬ß3.1 O
) O
; O
( O
b O
) O
compute O
a O
relatedness O
map O
between O
the O
target O
domain O
‚Äôs O
pseudolabelled O
instances O
, O
TL‚àó O
, O
and O
source O
domains O
‚Äô O
labelled O
instancesSL(¬ß3.3 O
) O
; O
( O
c O
) O
compute O
domainattention O
weights O
for O
each O
source O
domain O
( O
¬ß O
3.4 O
) O
; O
( O
d O
) O
jointly O
learn O
a O
model O
based O
on O
the O
relatedness O
map O
and O
the O
domain O
- O
attention O
weights O
for O
predicting O
labels O
for O
the O
target O
domain O
‚Äôs O
test O
instances O
( O
¬ß O
3.5 O
) O
. O
3.1 O
Pseudo O
- O
Label O
Generation O
In O
UDA O
, O
we O
have O
only O
unlabelled O
data O
for O
the O
target O
domain O
. O
Therefore O
, O
we O
Ô¨Årst O
infer O
pseudolabels O
for O
the O
target O
domain O
instances O
TUby O
selftraining O
( O
Abney O
, O
2007 O
) O
following O
Algorithm O
1 O
. O
SpeciÔ¨Åcally O
, O
we O
Ô¨Årst O
train O
a O
predictor O
fifor O
the O
i O
- O
th O
source O
domain O
using O
only O
its O
labelled O
instances O
SL O
iusing O
a O
base O
learner O
Œì(Line O
1 O
- O
2 O
) O
. O
Any O
classiÔ¨Åcation O
algorithm O
that O
can O
learn O
a O
predictor O
fithat O
can O
compute O
the O
probability O
, O
fi(x O
, O
y O
) O
, O
of O
a O
given O
instancexbelonging O
to O
the O
class O
ycan O
be O
used O
asŒì. O
In O
our O
experiments O
, O
we O
use O
logistic O
regression O
for O
its O
simplicity O
and O
popularity O
in O
prior O
UDA O
work O
( O
Bollegala O
et O
al O
. O
, O
2011 O
; O
Bollegala O
et O
al O
. O
, O
2013 O
) O
. O
Next O
, O
for O
each O
unlabelled O
instance O
in O
the O
selected O
source O
domain O
, O
we O
compute O
the O
probability O
of O
it O
belonging O
to O
each O
class O
and O
Ô¨Ånd O
the O
most O
probable O
class O
label O
. O
If O
the O
probability O
of O
the O
most O
likely O
class O
is O
greater O
than O
the O
given O
conÔ¨Ådence O
thresholdœÑ‚àà[0,1 O
] O
, O
we O
will O
append O
that O
instance O
to O
the O
current O
labelled O
training O
set O
. O
This O
enables O
us O
to O
increase O
the O
labelled O
instances O
for O
the O
source O
domains O
, O
which O
is O
important O
for O
learning O
accurate O
classiÔ¨Åers O
when O
the O
amount O
of O
labelled O
instances O
available O
is O
small O
. O
After O
processing O
all O
unlabelled O
instances O
inSi O
, O
we O
train O
the O
Ô¨Ånal O
classiÔ¨Åer O
fiforSi O
using O
both O
original O
and O
pseudo O
- O
labelled O
instances O
. O
Finally O
, O
we O
predict O
a O
pseudo O
- O
label O
for O
a O
target O
domain O
instance O
as O
the O
majority O
vote O
, O
f‚àó‚àà{0,1 O
} O
, O
over O
the O
predictions O
made O
by O
the O
individual O
classiÔ¨Åersfi O
. O
3.2 O
Prototype O
Selection O
Selecting O
the O
highest O
conÔ¨Ådent O
pseudo O
- O
labelled O
instances O
for O
training O
a O
classiÔ¨Åer O
for O
the O
target O
domain O
as O
done O
in O
prior O
work O
( O
Zhou O
and O
Li O
, O
2005 O
; O
Abney O
, O
2007 O
; O
S√∏gaard O
, O
2010 O
; O
Ruder O
and O
Plank O
, O
2018 O
) O
does O
not O
guarantee O
that O
those O
instances O
will O
be O
the O
most875Algorithm O
1 O
Multi O
- O
Source O
Self O
- O
Training O
Input O
: O
source O
domains O
‚Äô O
labelled O
instances O
SL O
1, O
... O
,SL O
N O
, O
source O
domains O
‚Äô O
unlabelled O
instances O
SU O
1, O
... O
,SU O
Nand O
target O
domain O
‚Äôs O
unlabelled O
instancesTU O
, O
target O
classesY O
, O
base O
learner O
Œìand O
the O
classiÔ¨Åcation O
conÔ¨Ådence O
threshold O
œÑ O
. O
Output O
: O
multi O
- O
source O
self O
- O
training O
classiÔ¨Åer O
f‚àó O
1 O
: O
fori= O
1toNdo O
2 O
: O
Li‚ÜêSL O
i O
3 O
: O
fi‚ÜêŒì(Li O
) O
4 O
: O
forx‚ààSU O
ido O
5 O
: O
ÀÜy= O
arg O
max O
y‚ààYfi(x O
, O
y O
) O
6 O
: O
iffi(x,ÀÜy)>œÑthen O
7 O
: O
Li‚ÜêL O
i‚à™{(x,ÀÜy O
) O
} O
8 O
: O
end O
if O
9 O
: O
end O
for O
10 O
: O
fi‚ÜêŒì(Li O
) O
11 O
: O
end O
for O
12 O
: O
return O
majority O
voter O
f‚àóoverf1, O
... O
,f O
N. O
suitable O
ones O
for O
adapting O
to O
the O
target O
domain O
, O
which O
was O
not O
considered O
during O
the O
self O
- O
training O
stage O
. O
For O
example O
, O
some O
target O
instances O
might O
not O
be O
good O
prototypical O
examples O
of O
the O
target O
domain O
and O
we O
would O
not O
want O
to O
use O
the O
pseudolabels O
induced O
for O
those O
instances O
when O
training O
a O
classiÔ¨Åer O
for O
the O
target O
domain O
. O
To O
identify O
instances O
in O
the O
target O
domain O
that O
are O
better O
prototypes O
, O
we O
Ô¨Årst O
encode O
each O
target O
instance O
by O
a O
vector O
and O
select O
the O
instances O
that O
are O
closest O
to O
the O
centroid O
, O
cT O
, O
of O
the O
target O
domain O
instances O
given O
by O
( O
1 O
) O
. O
cT=1 O
|TU|/summationdisplay O
x‚ààTUx O
( O
1 O
) O
In O
the O
case O
of O
text O
documents O
x O
, O
their O
embeddings O
, O
x O
, O
can O
be O
computed O
using O
numerous O
approaches O
such O
as O
using O
bi O
- O
directional O
LSTMs O
( O
Melamud O
et O
al O
. O
, O
2016 O
) O
or O
transformers O
( O
Reimers O
and O
Gurevych O
, O
2019 O
) O
. O
In O
our O
experiments O
, O
we O
use O
the O
Smoothed O
Inversed O
Frequency O
( O
SIF O
; O
Arora O
et O
al O
. O
, O
2017 O
) O
, O
which O
computes O
document O
embeddings O
as O
the O
weighted O
- O
average O
of O
the O
pre O
- O
trained O
word O
embeddings O
for O
the O
words O
in O
a O
document O
. O
Despite O
being O
unsupervised O
, O
SIF O
has O
shown O
strong O
performance O
in O
numerous O
semantic O
textual O
similarity O
benchmarks O
( O
Agirre O
et O
al O
. O
, O
2015 O
) O
. O
Using O
the O
centroid O
computed O
in O
( O
1 O
) O
, O
similarity O
for O
target O
instance O
to O
the O
centroid O
is O
computed O
usingthe O
cosine O
similarity O
given O
in O
( O
2 O
) O
. O
sim(x O
, O
cT O
) O
= O
x O
/ O
latticetopcT O
||x||||cT||(2 O
) O
Other O
distance O
measures O
such O
as O
the O
Euclidean O
distance O
can O
also O
be O
used O
. O
We O
use O
cosine O
similarity O
here O
for O
its O
simplicity O
. O
We O
predict O
the O
labels O
for O
the O
target O
domain O
unlabelled O
instances O
, O
TU O
, O
using O
f‚àó O
, O
and O
select O
the O
instances O
with O
the O
top- O
khighest O
similarities O
to O
the O
target O
domain O
according O
to O
( O
2)as O
the O
target O
domain O
‚Äôs O
pseudo O
- O
labelled O
instances O
TL‚àó. O
3.3 O
Relatedness O
Map O
Learning O
Not O
all O
of O
the O
source O
domain O
instances O
are O
relevant O
to O
a O
given O
target O
domain O
instance O
and O
the O
performance O
of O
a O
classiÔ¨Åer O
under O
domain O
shift O
can O
be O
upper O
bounded O
by O
the O
H O
- O
divergence O
between O
a O
source O
and O
a O
target O
domain O
( O
Kifer O
et O
al O
. O
, O
2004 O
; O
Ben O
- O
David O
et O
al O
. O
, O
2006 O
, O
2009 O
) O
. O
To O
model O
the O
relatedness O
between O
a O
target O
domain O
instance O
and O
each O
instance O
from O
theNsource O
domains O
, O
we O
use O
the O
pseudolabelled O
target O
domain O
instances O
TL‚àóand O
source O
domains O
‚Äô O
labelled O
instances O
SL O
ito O
learn O
a O
relatedness O
map O
, O
œài O
, O
between O
a O
target O
domain O
instance O
xT(‚ààTL‚àó O
) O
and O
a O
source O
domain O
labelled O
instance O
xL O
i(‚ààSL O
i O
) O
as O
given O
by O
( O
3 O
) O
. O
œài(xT O
, O
xL O
i O
) O
= O
exp(xT O
/ O
latticetopxL O
i)/summationtext O
x O
/ O
prime‚ààSL O
iexp(xT O
/ O
latticetopx O
/ O
prime)(3 O
) O
Usingœài O
, O
we O
can O
determine O
how O
well O
each O
instance O
in O
a O
source O
domain O
contributes O
to O
the O
prediction O
of O
the O
label O
of O
a O
target O
domain O
‚Äôs O
instance O
. O
3.4 O
Instance O
- O
based O
Domain O
- O
Attention O
To O
avoid O
negative O
transfer O
, O
we O
dynamically O
select O
the O
source O
domain(s O
) O
to O
use O
when O
predicting O
the O
label O
for O
a O
given O
target O
domain O
instance O
. O
Specifically O
, O
we O
learn O
domain O
- O
attention O
, O
Œ∏(xT O
, O
Si O
) O
, O
for O
each O
source O
domain O
, O
Si O
, O
conditioned O
on O
xTas O
given O
by O
( O
4 O
) O
. O
Œ∏(xT O
, O
Si O
) O
= O
exp(xT O
/ O
latticetopœÜi)/summationtextN O
j=1exp(xT O
/ O
latticetopœÜj)(4 O
) O
œÜican O
be O
considered O
as O
a O
domain O
embedding O
for O
Siand O
has O
the O
same O
dimensionality O
as O
the O
instance O
embeddings O
. O
During O
training O
we O
initialise O
œÜiusing O
Xavier O
initialisation O
( O
Glorot O
and O
Bengio O
, O
2010 O
) O
and O
normalise O
such O
that O
‚àÄxT,/summationtextN O
i=1Œ∏(xT O
, O
Si O
) O
= O
1 O
.8763.5 O
Training O
We O
combine O
the O
relatedness O
map O
( O
¬ß O
3.3 O
) O
and O
domain O
- O
attention O
( O
¬ß O
3.4 O
) O
and O
predict O
the O
label O
, O
ÀÜy(xT O
) O
, O
of O
a O
target O
domain O
instance O
xTusing O
( O
5 O
) O
. O
ÀÜy(xT O
) O
= O
œÉÔ£´ O
Ô£≠N O
/ O
summationdisplay O
i=1 O
/ O
summationdisplay O
xL O
i‚ààSL O
iy(xL O
i)œài(xT O
, O
xL O
i)Œ∏(xT O
, O
Si)Ô£∂ O
Ô£∏ O
( O
5 O
) O
Here O
, O
œÉ(z O
) O
= O
1/(1 O
+ O
exp(‚àíz))is O
the O
logistic O
sigmoid O
function O
and O
y(xL O
i)is O
the O
label O
of O
the O
source O
domain O
labelled O
instance O
xL O
i. O
First O
, O
we O
use O
the O
target O
instances O
, O
x‚ààTL‚àó O
, O
with O
inferred O
labels O
y‚àó(x O
) O
( O
computed O
using O
f‚àófrom O
Algorithm O
1 O
) O
as O
the O
training O
instances O
and O
predict O
their O
labels O
, O
ÀÜy(x O
) O
, O
by(5 O
) O
. O
The O
cross O
entropy O
error O
, O
E(ÀÜy(x),y‚àó(x))for O
this O
prediction O
is O
given O
by O
( O
6 O
): O
E(ÀÜy(x),y‚àó(x O
) O
) O
= O
‚àíŒª(x)(1‚àíy‚àó(x O
) O
) O
log(1‚àíÀÜy(x O
) O
) O
‚àíŒª(x)y‚àó(x O
) O
log(ÀÜy(x O
) O
) O
( O
6 O
) O
Here O
, O
Œª(x)is O
a O
rescaling O
factor O
computed O
using O
the O
normalised O
similarity O
score O
as O
in O
( O
7 O
): O
Œª(x O
) O
= O
sim(x O
, O
cT)/summationtext O
x O
/ O
prime‚ààTL‚àósim(x O
/ O
prime O
, O
cT)(7 O
) O
We O
minimise O
( O
6)using O
ADAM O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
for O
learning O
the O
domain O
- O
embeddings O
, O
œÜi O
. O
The O
initial O
learning O
rate O
is O
set O
to O
10‚àí3using O
a O
subset O
ofTL‚àóheld O
- O
out O
as O
a O
validation O
dataset O
. O
4 O
Experiments O
To O
evaluate O
the O
proposed O
method O
, O
we O
use O
the O
multi O
- O
domain O
Amazon O
product O
review O
dataset O
compiled O
by O
Blitzer O
et O
al O
. O
( O
2007 O
) O
. O
This O
dataset O
contains O
product O
reviews O
from O
four O
domains O
: O
Books O
( O
B O
) O
, O
DVD O
( O
D O
) O
, O
Electronics O
( O
E O
) O
and O
Kitchen O
Appliances O
( O
K O
) O
. O
Following O
Guo O
et O
al O
. O
( O
2018 O
) O
, O
we O
conduct O
experiments O
under O
two O
different O
splits O
of O
this O
dataset O
as O
originally O
proposed O
by O
Blitzer O
et O
al O
. O
( O
2007 O
) O
( O
Blitzer2007 O
) O
and O
by O
Chen O
et O
al O
. O
( O
2012 O
) O
( O
Chen2012 O
) O
. O
Table O
1 O
shows O
the O
number O
of O
instances O
in O
each O
dataset O
. O
By O
using O
these O
two O
versions O
of O
the O
Amazon O
review O
dataset O
, O
we O
can O
directly O
compare O
the O
proposed O
method O
against O
relevant O
prior O
work O
. O
Next O
, O
we O
describe O
how O
the O
proposed O
method O
was O
trained O
on O
each O
dataset O
. O
ForBlitzer2007 O
, O
we O
use O
the O
ofÔ¨Åcial O
train O
and O
test O
splits O
where O
each O
domain O
contains O
1600 O
labelled O
training O
instances O
( O
800positive O
and O
800negative O
) O
, O
and400target O
test O
instances O
( O
200positive O
and O
200negative O
) O
. O
In O
addition O
, O
each O
domain O
also O
contains O
6K-35 O
K O
unlabelled O
instances O
. O
We O
use O
300 O
dimensional O
pre O
- O
trained O
GloVe O
embeddings O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
following O
prior O
work O
( O
Bollegala O
et O
al O
. O
, O
2011 O
; O
Wu O
and O
Huang O
, O
2016 O
) O
with O
SIF O
to O
create O
document O
embeddings O
for O
the O
reviews O
. O
InChen2012 O
, O
each O
domain O
contains O
2000 O
labelled O
training O
instances O
( O
1000 O
positive O
and O
1000 O
negative O
) O
, O
and O
2000 O
target O
test O
instances O
( O
1000 O
positive O
and O
1000 O
negative O
) O
. O
The O
remainder O
of O
the O
instances O
are O
used O
as O
unlabelled O
instances O
( O
ca O
. O
4K-6 O
K O
for O
each O
domain O
) O
. O
We O
use O
the O
publicly O
available3 O
5000 O
dimensional O
tf O
- O
idf O
vectors O
produced O
by O
Zhao O
et O
al O
. O
( O
2018 O
) O
. O
We O
use O
a O
multilayer O
perceptron O
( O
MLP O
) O
with O
an O
input O
layer O
of O
5000 O
dimensions O
and O
3hidden O
layers O
with O
500dimensions O
. O
We O
use O
Ô¨Ånal O
output O
layer O
with O
500dimensions O
as O
the O
representation O
of O
an O
instance O
. O
For O
each O
setting O
, O
we O
follow O
the O
standard O
input O
representation O
methods O
as O
used O
in O
prior O
work O
. O
It O
also O
shows O
the O
Ô¨Çexibility O
of O
the O
proposed O
method O
to O
use O
different O
( O
embedding O
vs. O
BoW O
) O
text O
representation O
methods O
. O
We O
conduct O
experiments O
for O
cross O
- O
domain O
sentiment O
classiÔ¨Åcation O
with O
multiple O
sources O
by O
selecting O
one O
domain O
as O
the O
target O
and O
the O
remaining O
three O
as O
sources O
. O
The O
statistics O
for O
the O
two O
settings O
are O
shown O
in O
Table O
1 O
. O
4.1 O
Effect O
of O
Self O
- O
Training O
As O
described O
in O
¬ß O
3.1 O
, O
our O
proposed O
method O
uses O
self O
- O
training O
to O
generate O
pseudo O
- O
labels O
for O
the O
target O
domain O
unlabelled O
instances O
. O
In O
Table O
2 O
, O
we O
compare O
self O
- O
training O
against O
alternative O
pseudo O
- O
labelling O
methods O
on O
Chen2012 O
: O
SelfTraining O
( O
Self O
; O
Abney O
, O
2007 O
; O
Chattopadhyay O
et O
al O
. O
, O
2012 O
) O
, O
Union O
Self O
- O
Training O
( O
uni O
- O
Self O
; O
Aue O
and O
Gamon O
, O
2005 O
) O
, O
Tri O
- O
Training O
( O
Tri O
; O
Zhou O
and O
Li O
, O
2005 O
) O
and O
Tri O
- O
Training O
with O
Disagreement O
( O
TriD O
; O
S√∏gaard O
, O
2010 O
) O
. O
We O
observe O
that O
all O
semisupervised O
learning O
methods O
improve O
only O
slightly O
over O
uni O
- O
MS O
, O
the O
baseline O
model O
trained O
on O
the O
union O
of O
all O
sources O
and O
tested O
directly O
on O
a O
target O
domain O
without O
any O
DA O
, O
which O
has O
been O
identiÔ¨Åed O
as O
a O
strong O
baseline O
for O
multi O
- O
source O
DA O
( O
Aue O
and O
Gamon O
, O
2005 O
; O
Zhao O
et O
al O
. O
, O
2018 O
; O
Guo O
et O
al O
. O
, O
2018 O
) O
. O
Therefore O
, O
pseudo O
- O
labelling O
step O
alone O
is O
insufÔ¨Åcient O
for O
DA O
. O
Moreover O
, O
we O
observe O
that O
all O
semi O
- O
supervised O
methods O
perform O
comparably O
. O
3https://github.com/KeiraZhao/MDAN/877Target O
Source O
Train O
Test O
Unlabel O
Train O
Test O
Unlabel O
Blitzer2007 O
( O
Blitzer O
et O
al O
. O
, O
2006 O
) O
Chen2012 O
( O
Chen O
et O
al O
. O
, O
2012 O
) O
B O
D O
, O
E O
, O
K O
1600√ó3400 O
6000 O
2000√ó32000 O
4465 O
D O
B O
, O
E O
, O
K O
1600√ó3400 O
34741 O
2000√ó32000 O
5586 O
E O
B O
, O
D O
, O
K O
1600√ó3400 O
13153 O
2000√ó32000 O
5681 O
K O
B O
, O
D O
, O
E O
1600√ó3400 O
16785 O
2000√ó32000 O
5945 O
Table O
1 O
: O
Number O
of O
train O
, O
test O
and O
unlabelled O
instances O
for O
the O
two O
Amazon O
product O
review O
datasets O
. O
( O
a O
) O
prob O
sorted O
in O
ascending O
order O
  O
( O
b O
) O
prob O
sorted O
in O
descending O
order O
Figure O
1 O
: O
The O
number O
of O
selected O
pseudo O
- O
labelled O
instances O
konBlitzer2007 O
is O
shown O
on O
the O
x O
- O
axis O
. O
prob O
denotes O
prediction O
conÔ¨Ådence O
from O
the O
pseudo O
classiÔ¨Åer O
trained O
on O
the O
source O
domains O
, O
sim O
denotes O
the O
similarity O
to O
the O
target O
domain O
, O
asc O
and O
dsc O
respectively O
denote O
sorted O
in O
ascending O
and O
descending O
order O
( O
only O
applied O
to O
prob O
related O
selection O
methods O
, O
sim O
is O
always O
sorted O
in O
dsc O
) O
. O
prob O
only O
denotes O
using O
only O
prediction O
conÔ¨Ådence O
, O
simonly O
denotes O
using O
only O
target O
similarity O
. O
prob O
sim O
indicates O
selecting O
by O
prob O
Ô¨Årst O
and O
then O
sim O
( O
likewise O
for O
sim O
prob O
) O
. O
prob√ósim O
denotes O
using O
the O
product O
of O
prob O
and O
sim O
, O
and O
prob+sim O
denotes O
using O
their O
sum O
. O
The O
marker O
for O
the O
best O
result O
of O
each O
method O
is O
Ô¨Ålled O
. O
Example O
( O
1 O
) O
Why O
anybody O
everest O
feet O
would O
want O
reading O
this O
? O
... O
pure O
pleasure O
why O
29028 O
feet O
account O
this O
? O
... O
It O
‚Äôs O
a O
pleasure O
to O
read O
. O
( O
a O
) O
  O
( O
b O
) O
  O
( O
c O
) O
Figure O
2 O
: O
A O
positively O
labelled O
a O
target O
test O
instance O
in O
B(top O
) O
and O
resulted O
Œ∏ O
, O
œàiand O
the O
product O
of O
œàiand O
Œ∏(bottom O
) O
. O
Here O
, O
the O
x O
- O
axis O
represents O
the O
instances O
and O
the O
y O
- O
axis O
represents O
the O
prediction O
scores O
. O
Instance O
speciÔ¨Åc O
values O
in O
( O
a O
) O
and O
( O
c O
) O
are O
shown O
as O
> O
0for O
positive O
labelled O
instances O
and O
otherwise O
< O
0 O
. O
Source O
instances O
from O
D O
, O
EandKare O
shown O
in O
blue O
, O
green O
and O
red O
respectively O
. O
The O
contributions O
from O
top- O
150instances O
from O
three O
source O
domains O
are O
shown O
. O
4.2 O
Pseudo O
- O
labelled O
Instances O
Selection O
When O
selecting O
the O
pseudo O
- O
labelled O
instances O
from O
the O
target O
domain O
for O
training O
a O
classiÔ¨Åer O
for O
the O
target O
domain O
, O
we O
have O
two O
complementary O
strategies O
: O
( O
a O
) O
select O
the O
most O
conÔ¨Ådent O
instances O
according O
tof‚àó(denoted O
by O
prob O
) O
or O
( O
b O
) O
select O
the O
most O
sim O
- O
ilar O
instances O
to O
the O
target O
domain O
‚Äôs O
centroid O
( O
denoted O
by O
sim O
) O
. O
To O
evaluate O
the O
effect O
of O
these O
two O
strategies O
and O
their O
combinations O
( O
i.e O
prob+sim O
and O
prob√ósim O
) O
, O
in O
Figure O
1 O
, O
we O
select O
target O
instances O
with O
each O
strategy O
and O
measure O
the O
accuracy O
on O
the O
target O
domain O
Bfor O
increasing O
numbers O
of O
in-878Example O
( O
2 O
) O
Her O
relationship O
limited O
own O
pass O
her O
own O
analysis O
, O
there O
‚Äôre O
issues O
mainly O
focus O
in O
turn O
for O
codependency O
. O
Disappointing O
, O
dysfunctional O
. O
Mother‚Äôll O
book O
her O
daughter O
‚Äôs O
turn O
the O
pass O
, O
message O
turn O
the O
message O
issues O
analysis O
of O
very O
disappointing O
information O
. O
( O
a O
) O
  O
( O
b O
) O
  O
( O
c O
) O
Figure O
3 O
: O
A O
negatively O
labelled O
target O
test O
instance O
in O
B. O
DM O
L O
Score O
Evidences O
( O
Reviews O
) O
E O
- O
0.16943 O
Serious O
problems O
. O
E O
- O
0.02823 O
Sound O
great O
but O
lacking O
isolation O
in O
other O
areas O
. O
E O
+ O
0.02801 O
Cases O
for O
the O
cats O
walking O
years O
, O
no O
around O
and O
knocking O
... O
walking O
on O
similar O
cases O
of O
cats O
. O
E O
+ O
0.02233 O
Cord O
supposed O
to O
no O
problems O
, O
this O
extension O
extension O
not O
worked O
as O
cord O
did O
... O
whatever O
expected O
just O
worked O
Ô¨Åne O
. O
E O
- O
0.02209 O
Buy O
this O
like O
characters O
not O
used O
names O
... O
be O
aware O
of O
many O
commonly O
used O
characters O
before O
you O
accept O
Ô¨Åle O
like O
drive O
. O
Table O
3 O
: O
The O
top- O
5evidences O
for O
Example O
( O
2 O
) O
selected O
from O
the O
source O
domains O
. O
DM O
denotes O
the O
domain O
of O
the O
instance O
. O
L O
denotes O
the O
label O
for O
the O
instance O
. O
Score O
is O
œài(x)Œ∏(x O
) O
. O
T O
uni O
- O
MS O
Self O
uni O
- O
Self O
Tri O
Tri O
- O
D O
B O
79.46 O
79.60 O
79.46 O
79.61 O
79.51 O
D O
82.32 O
82.49 O
82.35 O
82.35 O
82.35 O
E O
84.93 O
84.97 O
84.93 O
84.99 O
84.93 O
K O
87.17 O
87.18 O
87.17 O
87.15 O
87.23 O
Table O
2 O
: O
ClassiÔ¨Åcation O
accuracies O
( O
% O
) O
for O
semisupervised O
methods O
on O
Chen2012 O
. O
stanceskin O
the O
descending O
( O
dsc O
) O
and O
ascending O
( O
asc O
) O
order O
of O
the O
selection O
scores O
. O
From O
Figure O
1b O
we O
observe O
that O
selecting O
the O
highest O
conÔ¨Ådent O
instances O
does O
not O
produce O
the O
best O
UDA O
accuracies O
. O
In O
fact O
, O
merely O
selecting O
instances O
based O
on O
conÔ¨Ådence O
scores O
only O
( O
corresponds O
to O
prob O
only O
) O
reports O
the O
worst O
performance O
. O
On O
the O
other O
hand O
, O
instances O
that O
are O
highly O
similar O
to O
the O
target O
domain O
‚Äôs O
centroid O
are O
more O
effective O
for O
DA O
. O
We O
observe O
that O
with O
only O
k= O
1000 O
instances O
, O
sim O
only O
reaches O
almost O
its O
optimal O
accuracy O
. O
Using O
validation O
data O
, O
we O
estimated O
that O
k= O
2000 O
to O
be O
sufÔ¨Åcient O
for O
all O
domains O
to O
reach O
the O
peak O
performance O
regardless O
of O
the O
selection O
strategy O
. O
Therefore O
, O
we O
selected O
2000 O
pseudolabelled O
instances O
for O
the O
attention O
step O
. O
In O
ourexperiments O
, O
we O
used O
sim O
only O
to O
select O
pseudolabelled O
instances O
because O
it O
steadily O
improves O
the O
classiÔ¨Åcation O
accuracy O
with O
kfor O
all O
target O
domains O
, O
and O
is O
competitive O
against O
other O
methods O
. O
4.3 O
Effect O
of O
the O
Relatedness O
Map O
In O
Table O
4 O
we O
report O
the O
classiÔ¨Åcation O
accuracy O
on O
the O
test O
instances O
in O
the O
target O
domain O
over O
the O
different O
steps O
: O
uni O
- O
MS O
( O
no O
adapt O
baseline O
) O
, O
Self(selftraining O
) O
, O
PL(pseudo O
- O
labelling O
) O
and O
Att(attention O
) O
. O
We O
use O
the O
self O
- O
training O
method O
described O
in O
Algorithm O
1 O
. O
The O
results O
clearly O
demonstrate O
a O
consistent O
improvement O
over O
all O
the O
steps O
in O
the O
proposed O
method O
. O
For O
Selfstep O
, O
the O
proposed O
method O
improves O
the O
accuracy O
only O
slightly O
without O
any O
information O
from O
the O
target O
domain O
. O
In O
the O
PLstep O
, O
we O
report O
the O
results O
of O
a O
predictor O
trained O
on O
target O
pseudo O
- O
labelled O
instances O
. O
We O
report O
the O
evaluation O
results O
for O
the O
trained O
attention O
model O
in O
Att O
. O
InAttstep O
, O
we O
use O
the O
relatedness O
map O
œàito O
express O
the O
similarity O
between O
a O
target O
instance O
and O
each O
of O
source O
domain O
instances O
, O
and O
the O
domain O
attention O
score O
Œ∏to O
express O
the O
relation O
between O
a O
target O
instance O
and O
each O
of O
the O
source O
domain O
instances O
. O
Two O
example O
test O
instances O
( O
one O
positive O
and O
one O
negative O
) O
from O
the O
target O
domain O
B879 O
T O
uni O
- O
MS O
Self O
PL O
Att O
B O
79.46 O
79.60 O
79.57 O
79.68 O
D O
82.32 O
82.49 O
82.71 O
82.96 O
E O
84.93 O
84.97 O
85.30 O
85.30 O
K O
87.17 O
87.18 O
87.30 O
87.48 O
Table O
4 O
: O
ClassiÔ¨Åcation O
accuracies O
( O
% O
) O
across O
different O
steps O
of O
the O
proposed O
method O
, O
evaluated O
on O
Chen2012 O
. O
are O
shown O
in O
Figures O
2 O
and O
3 O
. O
We O
observe O
that O
different O
source O
instances O
contribute O
to O
the O
predicted O
labels O
in O
different O
ways O
. O
As O
expected O
, O
in O
Figure O
2a O
more O
positive O
source O
instances O
are O
selected O
using O
the O
relatedness O
map O
for O
a O
positive O
target O
instance O
, O
and O
Figure O
3a O
more O
negative O
source O
instances O
are O
selected O
for O
a O
negative O
target O
instance O
. O
After O
training O
, O
we O
Ô¨Ånd O
that O
the O
proposed O
method O
identiÔ¨Åes O
the O
level O
of O
importance O
of O
different O
source O
domains O
. O
Example O
( O
1 O
) O
is O
closer O
to O
D O
, O
whereas O
Example O
( O
2 O
) O
is O
closer O
to O
Ewith O
a O
very O
high O
value O
of O
Œ∏ O
. O
Figures O
2c O
and O
3c O
show O
that O
the O
instance O
speciÔ¨Åc O
contribution O
to O
the O
target O
instance O
. O
The O
proposed O
method O
also O
identiÔ¨Åes O
the O
level O
of O
importance O
within O
the O
most O
relevant O
source O
domain O
. O
Figure O
3 O
shows O
the O
actual O
reviews O
as O
the O
top- O
5evidences O
from O
the O
source O
domains O
in O
Example O
( O
2 O
) O
. O
Negative O
labelled O
source O
training O
instance O
from O
E:‚ÄúSerious O
problem O
. O
‚Äù O
is O
the O
most O
important O
instance O
with O
the O
highest O
contribution O
ofœài(x)Œ∏(x)to O
the O
decision O
. O
4.4 O
Comparisons O
against O
Prior O
Work O
Table O
5 O
compares O
the O
proposed O
method O
against O
the O
following O
methods O
on O
Blitzer2007 O
dataset O
. O
SCL O
: O
Structural O
Correspondence O
Learning O
( O
Blitzer O
et O
al O
. O
, O
2006 O
, O
2007 O
) O
is O
a O
single O
- O
source O
DA O
method O
, O
trained O
on O
the O
union O
of O
all O
source O
domains O
and O
tested O
on O
the O
target O
domain O
. O
We O
report O
the O
published O
results O
from O
Wu O
and O
Huang O
( O
2016 O
) O
. O
SFA O
: O
Spectral O
Feature O
Alignment O
( O
Pan O
et O
al O
. O
, O
2010 O
) O
is O
a O
single O
- O
source O
DA O
method O
, O
trained O
on O
the O
union O
of O
all O
source O
domains O
, O
and O
tested O
on O
the O
target O
domain O
. O
We O
report O
the O
published O
results O
from O
Wu O
and O
Huang O
( O
2016 O
) O
. O
SST O
: O
Sensitive O
Sentiment O
Thesaurus O
( O
Bollegala O
et O
al O
. O
, O
2011 O
; O
Bollegala O
et O
al O
. O
, O
2013 O
) O
is O
the O
SoTA O
multi O
- O
source O
DA O
method O
on O
Blitzer2007 O
. O
We O
report O
the O
published O
results O
from O
Bollegala O
et O
al O
. O
( O
2011 O
) O
. O
SDAMS O
: O
Sentiment O
Domain O
Adaptation O
with O
Multiple O
Sources O
proposed O
by O
Wu O
and O
Huang O
( O
2016 O
) O
. O
We O
report O
the O
results O
from O
the O
original O
paper O
. O
T O
uni O
- O
MS O
SCL O
SFA O
SST O
SDAMS O
AMN O
Proposed O
B O
80.00 O
74.57 O
75.98 O
76.32 O
78.29 O
79.75 O
83.50 O
D O
76.00 O
76.30 O
78.48 O
78.77 O
79.13 O
79.83 O
80.50 O
E O
74.75 O
78.93 O
78.08 O
83.63 O
* O
84.18 O
* O
* O
80.92 O
* O
80.00 O
* O
K O
85.25 O
82.07 O
82.10 O
85.18 O
86.29 O
85.00 O
86.00 O
Table O
5 O
: O
ClassiÔ¨Åcation O
accuracies O
( O
% O
) O
for O
the O
proposed O
method O
and O
prior O
work O
on O
Blitzer2007 O
. O
Statistically O
signiÔ¨Åcant O
improvements O
over O
uni O
- O
MS O
according O
to O
the O
Binomial O
exact O
test O
are O
shown O
by O
‚Äú O
* O
‚Äù O
and O
‚Äú O
* O
* O
‚Äù O
respectively O
at O
p= O
0.01andp= O
0.001levels O
. O
T O
uni O
- O
MS O
mSDA O
DANN O
MDAN O
MoE O
Proposed O
B O
79.46 O
76.98 O
76.50 O
78.63 O
79.42 O
79.68 O
D O
82.32 O
78.61 O
77.32 O
80.65 O
83.35 O
82.96 O
E O
84.93 O
81.98 O
83.81 O
85.34 O
86.62 O
85.30 O
K O
86.71 O
84.26 O
84.33 O
86.26 O
87.96 O
87.48 O
Table O
6 O
: O
ClassiÔ¨Åcation O
accuracies O
( O
% O
) O
for O
the O
proposed O
method O
and O
prior O
work O
on O
Chen2012 O
. O
AMN O
: O
End O
- O
to O
- O
End O
Adversarial O
Memory O
Network O
( O
Li O
et O
al O
. O
, O
2017 O
) O
is O
a O
single O
- O
source O
DA O
method O
, O
trained O
on O
the O
union O
of O
all O
source O
domains O
, O
and O
tested O
on O
the O
target O
domain O
. O
We O
report O
the O
published O
results O
from O
Ding O
et O
al O
. O
( O
2019 O
) O
. O
In O
Table O
6 O
, O
we O
compare O
our O
proposed O
method O
against O
the O
following O
methods O
on O
Chen2012 O
. O
mSDA O
: O
Marginalized O
Stacked O
Denoising O
Autoencoders O
proposed O
by O
Chen O
et O
al O
. O
( O
2012 O
) O
. O
We O
report O
the O
published O
results O
from O
Guo O
et O
al O
. O
( O
2018 O
) O
. O
DANN O
: O
Domain O
- O
Adversarial O
Neural O
Networks O
proposed O
by O
Ganin O
et O
al O
. O
( O
2016 O
) O
. O
We O
report O
the O
published O
results O
from O
Zhao O
et O
al O
. O
( O
2018 O
) O
. O
MDAN O
: O
Multiple O
Source O
Domain O
Adaptation O
with O
Adversarial O
Learning O
proposed O
by O
Zhao O
et O
al O
. O
( O
2018 O
) O
. O
We O
report O
the O
published O
results O
from O
the O
original O
paper O
. O
MoE O
: O
Mixture O
of O
Experts O
proposed O
by O
Guo O
et O
al O
. O
( O
2018 O
) O
. O
We O
report O
the O
published O
results O
from O
the O
original O
paper O
. O
From O
Tables O
5 O
and O
6 O
, O
we O
observe O
that O
the O
proposed O
method O
obtains O
the O
best O
classiÔ¨Åcation O
accuracy O
on O
Books O
domain O
( O
B O
) O
in O
both O
settings O
, O
which O
is O
the O
domain O
with O
the O
smallest O
number O
of O
unlabelled O
instances O
. O
In O
particular O
, O
when O
the O
amount O
of O
training O
instances O
are O
small O
, O
pseudo O
- O
labelling O
and O
domain O
- O
attention O
in O
our O
proposed O
method O
play O
a O
vital O
role O
in O
multi O
- O
source O
UDA O
. O
Although O
SDAMS O
( O
inBlitzer2007 O
) O
and O
MoE O
( O
inChen2012 O
) O
outperform O
the O
proposed O
method O
, O
the O
simplicity O
and O
the O
ability O
to O
provide O
explanations O
are O
attractive O
properties O
for O
a O
UDA O
method O
when O
applying O
in O
an O
industrial O
setting O
involving O
a O
massive O
number O
of880source O
domains O
such O
as O
sentiment O
classiÔ¨Åcation O
in O
E O
- O
commerce O
reviews O
. O
5 O
Conclusions O
We O
propose O
a O
multi O
- O
source O
UDA O
method O
that O
combines O
self O
- O
training O
with O
an O
attention O
module O
. O
In O
contrast O
to O
prior O
works O
that O
select O
pseudo O
- O
labelled O
instances O
based O
on O
prediction O
conÔ¨Ådence O
of O
a O
predictor O
learnt O
from O
source O
domains O
, O
our O
proposed O
method O
uses O
similarity O
to O
the O
target O
domain O
during O
adaptation O
. O
Our O
proposed O
method O
reports O
competitive O
performance O
against O
previously O
proposed O
multi O
- O
source O
UDA O
methods O
on O
two O
splits O
on O
a O
standard O
benchmark O
dataset O
. O
Abstract O
Large O
pre O
- O
trained O
language O
models O
reach O
stateof O
- O
the O
- O
art O
results O
on O
many O
different O
NLP O
tasks O
when O
Ô¨Åne O
- O
tuned O
individually O
; O
They O
also O
come O
with O
a O
signiÔ¨Åcant O
memory O
and O
computational O
requirements O
, O
calling O
for O
methods O
to O
reduce O
model O
sizes O
( O
green O
AI O
) O
. O
We O
propose O
a O
twostage O
model O
- O
compression O
method O
to O
reduce O
a O
model O
‚Äôs O
inference O
time O
cost O
. O
We O
Ô¨Årst O
decompose O
the O
matrices O
in O
the O
model O
into O
smaller O
matrices O
and O
then O
perform O
feature O
distillation O
on O
the O
internal O
representation O
to O
recover O
from O
the O
decomposition O
. O
This O
approach O
has O
the O
beneÔ¨Åt O
of O
reducing O
the O
number O
of O
parameters O
while O
preserving O
much O
of O
the O
information O
within O
the O
model O
. O
We O
experimented O
on O
BERTbase O
model O
with O
the O
GLUE O
benchmark O
dataset O
and O
show O
that O
we O
can O
reduce O
the O
number O
of O
parameters O
by O
a O
factor O
of O
0.4x O
, O
and O
increase O
inference O
speed O
by O
a O
factor O
of O
1.45x O
, O
while O
maintaining O
a O
minimal O
loss O
in O
metric O
performance O
. O
1 O
Introduction O
Deep O
learning O
models O
have O
been O
demonstrated O
to O
achieve O
state O
- O
of O
- O
the O
- O
art O
results O
, O
but O
require O
large O
parameter O
storage O
and O
computation O
. O
It O
‚Äôs O
estimated O
that O
training O
a O
Transformer O
model O
with O
a O
neural O
architecture O
search O
has O
a O
CO O
2emissions O
equivalent O
to O
nearly O
Ô¨Åve O
times O
the O
lifetime O
emissions O
of O
the O
average O
U.S. O
car O
, O
including O
its O
manufacturing O
( O
Strubell O
et O
al O
. O
, O
2019 O
) O
. O
Alongside O
the O
increase O
in O
deep O
learning O
models O
complexity O
, O
in O
the O
NLP O
domain O
, O
there O
has O
been O
a O
shift O
in O
the O
NLP O
modeling O
paradigm O
from O
training O
a O
randomly O
initialized O
model O
to O
Ô¨Åne O
- O
tuning O
a O
large O
and O
computational O
heavy O
pre O
- O
trained O
language O
model O
( O
Howard O
and O
Ruder O
, O
2018 O
; O
Peters O
et O
al O
. O
, O
2018 O
; O
Devlin O
et O
al O
. O
, O
2018 O
; O
Radford O
, O
2018 O
; O
Radford O
et O
al O
. O
, O
2019 O
; O
Dai O
et O
al O
. O
, O
2019 O
; O
Yang O
et O
al O
. O
, O
2019 O
; O
Lample O
and O
Conneau O
, O
2019 O
; O
Liu O
et O
al O
. O
, O
2019b O
; O
Raffel O
et O
al O
. O
, O
2019 O
; O
Lan O
et O
al O
. O
, O
2019 O
; O
Lewis O
et O
al O
. O
, O
2019).While O
re O
- O
using O
pre O
- O
trained O
models O
offsets O
the O
training O
costs O
, O
inference O
time O
costs O
of O
the O
Ô¨Ånetuned O
models O
remain O
signiÔ¨Åcant O
, O
and O
are O
showstoppers O
in O
many O
applications O
. O
The O
main O
challenge O
with O
pre O
- O
trained O
models O
is O
how O
can O
we O
reduce O
their O
size O
while O
saving O
the O
information O
contained O
within O
them O
. O
Recent O
work O
, O
approached O
this O
by O
keeping O
some O
of O
the O
layers O
while O
removing O
others O
( O
Sanh O
et O
al O
. O
, O
2019 O
; O
Sun O
et O
al O
. O
, O
2019 O
; O
Xu O
et O
al O
. O
, O
2020 O
) O
. O
A O
main O
drawback O
of O
such O
approach O
is O
in O
its O
coarse O
- O
grained O
nature O
: O
removing O
entire O
layers O
might O
discard O
important O
information O
contained O
within O
the O
model O
, O
and O
working O
at O
the O
granularity O
of O
layers O
makes O
the O
trade O
- O
off O
between O
compression O
and O
accuracy O
of O
a O
model O
hard O
to O
control O
. O
Motivated O
by O
this O
, O
in O
this O
work O
we O
suggest O
a O
more O
Ô¨Ånegrained O
approach O
which O
decomposes O
each O
matrix O
to O
two O
smaller O
matrices O
and O
then O
perform O
feature O
distillation O
on O
the O
internal O
representation O
to O
recover O
from O
the O
decomposition O
. O
This O
approach O
has O
the O
beneÔ¨Åt O
of O
preserving O
much O
of O
the O
information O
while O
reducing O
the O
number O
of O
parameters O
. O
Alongside O
the O
advantage O
of O
preserving O
the O
information O
within O
each O
layer O
, O
there O
is O
also O
a O
memory O
Ô¨Çexibility O
advantage O
compared O
to O
removing O
entire O
layers O
; O
As O
a O
result O
of O
decomposing O
each O
matrix O
to O
two O
smaller O
matrices O
, O
we O
can O
store O
each O
of O
the O
two O
matrices O
in O
two O
different O
memory O
blocks O
. O
This O
has O
the O
beneÔ¨Åt O
of O
distributing O
the O
model O
matrices O
in O
many O
small O
memory O
blocks O
, O
which O
is O
useful O
when O
working O
in O
shared O
CPU O
- O
based O
environments O
. O
We O
evaluated O
our O
approach O
on O
the O
General O
Language O
Understanding O
Evaluation O
( O
GLUE O
) O
benchmark O
dataset O
( O
Wang O
et O
al O
. O
, O
2018 O
) O
and O
show O
that O
our O
approach O
is O
superior O
or O
competitive O
in O
the O
different O
GLUE O
tasks O
to O
previous O
approaches O
which O
remove O
entire O
layers O
. O
Furthermore O
, O
we O
study O
the O
effects O
of O
different O
base O
models O
to O
decompose O
and O
show O
the O
superiority O
of O
decomposing O
a O
Ô¨Åne O
- O
tuned O
model O
compared O
to O
a O
pre O
- O
trained O
model O
or O
a O
ran-884domly O
initialized O
model O
. O
Finally O
, O
we O
demonstrate O
the O
trade O
- O
off O
between O
compression O
and O
accuracy O
of O
a O
model O
. O
2 O
Related O
Work O
In O
the O
past O
year O
, O
there O
have O
been O
many O
attempts O
to O
compress O
transformer O
models O
involving O
pruning O
( O
McCarley O
, O
2019 O
; O
Guo O
et O
al O
. O
, O
2019 O
; O
Wang O
et O
al O
. O
, O
2019 O
; O
Michel O
et O
al O
. O
, O
2019 O
; O
V O
oita O
et O
al O
. O
, O
2019 O
; O
Gordon O
et O
al O
. O
, O
2020 O
) O
, O
quantization O
( O
Zafrir O
et O
al O
. O
, O
2019 O
; O
Shen O
et O
al O
. O
, O
2019 O
) O
and O
distillation O
( O
Sanh O
et O
al O
. O
, O
2019 O
; O
Zhao O
et O
al O
. O
, O
2019 O
; O
Tang O
et O
al O
. O
, O
2019 O
; O
Mukherjee O
and O
Awadallah O
, O
2019 O
; O
Sun O
et O
al O
. O
, O
2019 O
; O
Liu O
et O
al O
. O
, O
2019a O
; O
Jiao O
et O
al O
. O
, O
2019 O
; O
Izsak O
et O
al O
. O
, O
2019 O
) O
. O
SpeciÔ¨Åcally O
, O
works O
on O
compressing O
pretrained O
transformer O
language O
models O
focused O
on O
pruning O
layers O
. O
Sun O
et O
al O
. O
( O
2019 O
) O
suggested O
to O
prune O
layers O
while O
distilling O
information O
from O
the O
unpruned O
model O
layers O
. O
Xu O
et O
al O
. O
( O
2020 O
) O
proposed O
to O
gradually O
remove O
layers O
during O
training O
. O
We O
also O
note O
that O
very O
recently O
a O
work O
similar O
to O
ours O
was O
uploaded O
to O
arxiv O
( O
Mao O
et O
al O
. O
, O
2020 O
) O
. O
There O
are O
a O
few O
differences O
from O
their O
work O
to O
ours O
. O
Firstly O
, O
we O
distill O
different O
parts O
of O
the O
model O
( O
see O
Section O
3 O
for O
details O
) O
. O
Secondly O
, O
we O
focus O
on O
training O
the O
decomposed O
model O
and O
do O
not O
prune O
the O
model O
parameters O
. O
Thirdly O
, O
our O
base O
model O
, O
which O
is O
used O
for O
decomposition O
and O
as O
a O
teacher O
, O
is O
a O
Ô¨Åne O
- O
tuned O
model O
; O
This O
has O
the O
beneÔ¨Åt O
of O
task O
- O
speciÔ¨Åc O
information O
as O
we O
show O
in O
our O
experiments O
in O
Section O
4.2 O
. O
3 O
Method O
Our O
goal O
is O
to O
decompose O
each O
matrix O
W‚ààRn√ód O
as O
two O
smaller O
matrices O
, O
obtaining O
an O
approximated O
matrix O
W O
/ O
prime O
= O
AB O
, O
A‚ààRn√ór O
, O
B‚ààRr√ód O
, O
wherer O
< O
nd O
n+d O
. O
We O
seek O
a O
decomposition O
s.t O
. O
W O
/ O
primeis O
close O
toWin O
the O
sense O
that O
d(Wx O
, O
W O
/ O
primex O
) O
is O
small O
for O
all O
x O
, O
wheredis O
a O
distance O
metric O
between O
vectors O
. O
In O
practice O
, O
we O
require O
the O
condition O
to O
hold O
not O
for O
all O
x O
, O
but O
for O
vectors O
seen O
in O
a O
Ô¨Ånite O
relevant O
sample O
( O
in O
our O
case O
, O
the O
training O
data O
) O
. O
While O
one O
could O
start O
with O
random O
matrices O
and O
optimize O
the O
objective O
using O
gradient O
descent O
, O
we O
show O
that O
a O
two O
- O
staged O
approach O
performs O
better O
: O
we O
Ô¨Årst O
decompose O
the O
matrices O
using O
SVD O
, O
obtaining O
A O
/ O
prime O
, O
B O
/ O
primes.t.||A O
/ O
primeB O
/ O
prime‚àíW||2 O
2 O
is O
small O
( O
SVD O
is O
guaranteed O
to O
produce O
the O
best O
rank O
- O
r O
approximation O
to O
W O
, O
( O
Stewart O
, O
1991 O
) O
) O
. O
We O
then O
use O
these O
matrices O
as O
initialization O
and O
optimized(Wx O
, O
W O
/ O
primex)(feature O
distillation O
) O
, O
whilealso O
optimizing O
for O
task O
loss O
. O
We O
show O
that O
this O
process O
works O
substantially O
better O
in O
practice O
. O
Our O
loss O
function O
is O
thus O
composed O
of O
three O
different O
objectives O
: O
Cross O
Entropy O
Loss O
The O
cross O
entropy O
loss O
over O
an O
example O
xwith O
labelyis O
deÔ¨Åned O
likewise O
: O
LCE=‚àílogps(y|x O
) O
, O
wherepsis O
the O
probability O
for O
label O
ygiven O
by O
the O
decomposed O
student O
model O
. O
Knowledge O
Distillation O
Loss O
The O
goal O
of O
knowledge O
distillation O
is O
to O
imitate O
the O
output O
layer O
of O
a O
teacher O
model O
by O
a O
student O
model O
. O
The O
Knowledge O
Distillation O
Loss O
is O
deÔ¨Åned O
likewise O
: O
LKD=/vextenddouble O
/ O
vextenddoublezs‚àízt O
T O
/ O
vextenddouble O
/ O
vextenddouble O
2 O
, O
where O
zsandztare O
the O
logits O
of O
the O
decomposed O
and O
original O
models O
respectively O
and O
T O
is O
a O
temperature O
hyper O
- O
parameter O
. O
Feature O
Distillation O
Loss O
The O
goal O
of O
feature O
distillation O
is O
to O
imitate O
the O
intermediate O
layers O
of O
a O
teacher O
model O
by O
a O
student O
model O
. O
we O
use O
the O
following O
intermediate O
representations O
to O
distill O
the O
knowledge O
from1 O
: O
‚Ä¢Query O
, O
Key O
and O
Value O
Layers O
- O
The O
dot O
product O
of O
a O
matrix O
of O
concatenated O
tokens O
representation O
vectors O
Xby O
the O
query O
, O
key O
and O
value O
parameter O
matrices O
, O
Zq O
= O
X¬∑WQ O
, O
Zk O
= O
X¬∑WK O
, O
Zv O
= O
X¬∑WV O
‚Ä¢Attention O
Matrix O
- O
The O
attention O
matrix O
probabilities O
. O
Zatt O
= O
softmax O
( O
Zq¬∑ZT O
k O
) O
‚Ä¢Attention O
Heads O
- O
The O
output O
of O
the O
attention O
heads O
. O
ZH O
= O
Zatt¬∑Zv O
‚Ä¢The O
Multihead O
Attention O
Layer O
Output O
- O
The O
dot O
product O
of O
the O
attention O
heads O
by O
the O
matrixWO.ZMH O
= O
ZH¬∑WO O
‚Ä¢The O
Ô¨Årst O
feed O
forward O
layer O
- O
The O
dot O
product O
of O
the O
multihead O
attention O
layer O
by O
the O
Ô¨Årst O
feed O
forward O
layer O
. O
Zf1 O
= O
ZMH¬∑W1 O
‚Ä¢The O
second O
feed O
forward O
layer O
- O
The O
dot O
product O
of O
the O
Ô¨Årst O
feed O
forward O
layer O
by O
the O
second O
feed O
forward O
layer O
. O
Zf2 O
= O
Zf1¬∑W2 O
We O
denoteSi O
zandTi O
zas O
the O
intermediate O
representations O
which O
were O
described O
above O
of O
layer O
ifor O
1We O
follow O
the O
notations O
of O
Vaswani O
et O
al O
. O
( O
2017 O
) O
for O
the O
transformer O
parameters O
and O
omit O
biases O
for O
notation O
convenience.885the O
decomposed O
student O
and O
original O
teacher O
models O
respectively O
. O
Our O
loss O
function O
then O
is O
deÔ¨Åned O
by O
: O
LFD=/summationtext O
iTi O
z O
, O
Si O
z O
/ O
summationtext O
Tz O
, O
Sz O
/ O
bardblTz‚àíSz O
/ O
bardbl2 O
Full O
Objective O
Our O
loss O
function O
is O
then O
deÔ¨Åned O
by O
a O
weighted O
combination O
of O
these O
three O
loss O
functions O
likewise O
: O
L O
= O
Œ±LCE+ O
( O
1‚àí O
Œ±)LKD+LFDwhereŒ±‚àà[0,1]is O
a O
chosen O
hyperparameter O
. O
4 O
Experiments O
We O
compare O
various O
variants O
of O
our O
compression O
method O
, O
corresponding O
to O
different O
subsets O
of O
our O
loss O
. O
All O
variants O
decompose O
the O
matrices O
using O
SVD O
, O
but O
differ O
in O
their O
objective O
functions O
. O
These O
correspond O
to O
the O
four O
last O
lines O
in O
Table O
1 O
. O
Low O
Rank O
BERT O
Fine O
- O
tuning O
( O
LRBF O
) O
corresponds O
toL O
= O
LCE O
. O
LRBF+KD O
corresponds O
to O
L O
= O
Œ±LCE+ O
( O
1‚àíŒ±)LKD O
. O
LRBF+FD O
corresponds O
toL O
= O
LCE+LFD O
, O
while O
LRBF+FD+KD O
corresponds O
to O
the O
complete O
objective O
. O
The O
other O
lines O
in O
the O
table O
correspond O
to O
uncompressed O
model O
( O
Ô¨Årst O
line O
) O
and O
to O
baselines O
which O
prune O
layers O
and O
distill O
. O
Fine O
- O
tuning O
Ô¨Ånetunes O
a O
six O
layered O
BERT O
model O
. O
Vanilla O
KD O
trains O
a O
six O
- O
layered O
BERT O
model O
with O
L= O
Œ±LCE+(1‚àíŒ±)LKD O
. O
BERT O
- O
PKD O
trains O
a O
six O
layered O
BERT O
model O
with O
L O
= O
Œ±LCE+(1‚àíŒ±)LKD O
while O
also O
adding O
an O
LFDobjective O
, O
but O
on O
the O
hidden O
states O
between O
every O
consecutive O
layer O
. O
BERT O
- O
of O
- O
Theseus O
Ô¨Åne O
- O
tunes O
BERT O
model O
while O
gradually O
pruning O
half O
of O
the O
layers O
. O
We O
chose O
this O
baselines O
for O
several O
reasons O
: O
like O
our O
method O
they O
result O
in O
a O
practical O
reduction O
of O
parameters;2 O
, O
they O
are O
task O
- O
speciÔ¨Åc;3and O
they O
do O
not O
require O
the O
pretraining O
stage O
, O
which O
is O
expensive O
and O
not O
practical O
for O
most O
practitioners O
. O
Datasets O
We O
evaluate O
our O
proposed O
approach O
on O
the O
General O
Language O
Understanding O
Evaluation O
( O
GLUE O
) O
benchmark O
( O
Wang O
et O
al O
. O
, O
2018 O
) O
, O
a O
collection O
of O
diverse O
NLP O
tasks O
. O
Training O
Details O
We O
Ô¨Åne O
- O
tune O
a O
pre O
- O
trained O
BERT O
model O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
for O
each O
task O
with O
a O
batch O
size O
of O
8and O
a O
learning O
rate O
of O
2e‚àí5 O
for3epochs O
with O
an O
early O
stop O
mechanism O
according O
to O
the O
validation O
set O
. O
We O
perform O
the O
matrix O
2Unlike O
, O
e.g. O
, O
pruning O
, O
which O
sets O
parameters O
to O
zero O
and O
requires O
specialized O
hardware O
to O
fully O
take O
advantage O
of O
. O
3Unlike O
, O
e.g. O
, O
DistillBERT O
which O
is O
meant O
to O
be O
run O
before O
Ô¨Åne-tuning.decomposition O
on O
every O
parametric O
weight O
matrix O
of O
the O
encoder O
( O
excluding O
the O
embedding O
matrix O
) O
in O
a O
Ô¨Åne O
- O
tuned O
model O
and O
train O
the O
decomposed O
model O
as O
the O
student O
model O
and O
the O
original O
Ô¨Åne O
- O
tuned O
model O
as O
the O
teacher O
. O
For O
each O
task O
we O
train O
for O
3epochs O
with O
an O
early O
stopping O
mechanism O
according O
to O
the O
task O
validation O
set O
, O
the O
maximum O
sequence O
length O
is O
128 O
and O
we O
perform O
a O
grid O
search O
over O
the O
learning O
rates O
{ O
2e‚àí6,5e‚àí6,2e‚àí5,5e‚àí5,2e‚àí4,5e‚àí4}and5different O
seeds O
and O
choose O
the O
best O
model O
according O
to O
the O
validation O
set O
of O
each O
task.4For O
knowledge O
distillation O
hyper O
- O
parameters O
we O
used O
a O
temperature O
hyper O
- O
parameter O
T= O
10 O
andŒ±= O
0.7.5 O
4.1 O
Main O
Results O
Table O
1 O
compares O
the O
results O
for O
validation O
and O
test O
of O
other O
compression O
approaches O
which O
prune O
layers O
, O
along O
with O
low O
rank O
models O
which O
were O
Ô¨Ånetuned O
and O
trained O
with O
one O
or O
more O
of O
the O
distillation O
objectives O
described O
in O
Section O
3 O
. O
As O
can O
be O
seen O
, O
Low O
Rank O
BERT O
Feature O
Distillation O
+ O
KD O
and O
Low O
Rank O
BERT O
Feature O
Distillation O
surpass O
all O
of O
results O
of O
all O
methods O
in O
both O
validation O
and O
test O
sets O
except O
BERT O
- O
of O
- O
Theseus O
method O
in O
the O
test O
set O
, O
in O
which O
Low O
Rank O
BERT O
Feature O
Distillation O
+ O
KD O
surpasses O
the O
results O
in O
5 O
of O
the O
tasks O
and O
reach O
comparable O
results O
in O
2of O
the O
tasks O
. O
Also O
, O
as O
can O
be O
seen O
knowledge O
distillation O
alone O
is O
not O
sufÔ¨Åcient O
to O
compensate O
for O
the O
decomposition O
, O
but O
it O
slightly O
improves O
the O
results O
when O
incorporating O
feature O
distillation O
alone O
. O
4.2 O
Further O
Analysis O
Effect O
of O
Base O
Model O
and O
Decomposition O
In O
this O
experiment O
we O
test O
the O
importance O
of O
the O
base O
model O
we O
use O
to O
decompose O
and O
use O
as O
a O
teacher O
. O
We O
compared O
between O
three O
types O
of O
distillation O
sources O
: O
Ô¨Åne O
- O
tuned O
teacher O
, O
pre O
- O
trained O
teacher O
and O
no O
teacher O
. O
Furthermore O
, O
we O
compared O
between O
three O
types O
of O
model O
initializations O
: O
a O
decomposed O
Ô¨Åne O
- O
tuned O
model O
, O
a O
decomposed O
pretrained O
model O
and O
a O
randomly O
initialized O
model O
with O
the O
same O
architecture O
as O
the O
decomposed O
models O
. O
The O
results O
are O
shown O
in O
Table O
2 O
, O
on O
all O
tasks O
when O
training O
with O
no O
teacher O
distilla4We O
detailed O
the O
changes O
we O
made O
to O
the O
original O
Ô¨Ånetuning O
procedure O
, O
every O
other O
hyper O
- O
parameters O
which O
were O
not O
mentioned O
, O
is O
set O
as O
described O
in O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
. O
5We O
chose O
those O
hyper O
- O
parameters O
from O
a O
grid O
search O
over O
T={5,10,20}andŒ±={0.2,0.5,0.7}on O
the O
MRPC O
validation O
set.886MethodCoLA O
MNLI O
MRPC O
QNLI O
QQP O
RTE O
SST-2 O
STS O
- O
B O
Macro O
Score O
Dev O
Test O
Dev O
Test O
Dev O
Test O
Dev O
Test O
Dev O
Test O
Dev O
Test O
Dev O
Test O
Dev O
Test O
Dev O
Test O
Uncompressed O
Models O
- O
110 O
M O
Parameters O
BERT O
- O
base O
( O
uncompressed O
) O
59.9 O
53.9 O
84.6 O
83.9 O
89.0 O
85.6 O
91.6 O
90.9 O
88.1 O
80.2 O
71.5 O
67.2 O
93.5 O
93.6 O
89.8 O
84.8 O
83.5 O
80.0 O
6 O
Layers O
Transformer O
Models O
- O
66 O
M O
Parameters O
Fine O
- O
tuning O
43.4 O
41.5 O
80.1 O
80.1 O
86.0 O
83.1 O
86.9 O
86.7 O
87.8 O
78.7 O
62.1 O
63.6 O
89.6 O
90.7 O
81.9 O
81.1 O
77.2 O
75.7 O
Vanilla O
KD O
( O
Hinton O
et O
al O
. O
, O
2015 O
) O
45.1 O
42.9 O
80.1 O
80.0 O
86.2 O
83.4 O
88.0 O
88.3 O
88.1 O
79.5 O
64.9 O
64.7 O
90.5 O
91.5 O
84.9 O
81.2 O
78.5 O
76.4 O
BERT O
- O
PKD O
( O
Sun O
et O
al O
. O
, O
2019 O
) O
45.5 O
43.5 O
81.3 O
81.3 O
85.7 O
82.5 O
88.4 O
89.0 O
88.4 O
79.8 O
66.5 O
65.5 O
91.3 O
92.0 O
86.2 O
82.5 O
79.2 O
77.0 O
BERT O
- O
of O
- O
Theseus O
( O
Xu O
et O
al O
. O
, O
2020 O
) O
51.1 O
47.8 O
82.3 O
82.3 O
89.0 O
85.4 O
89.5 O
89.6 O
89.6 O
80.5 O
68.2 O
66.2 O
91.5 O
92.2 O
88.7 O
84.9 O
81.2 O
78.6 O
Low O
Rank O
Approximated O
Models O
- O
65.2 O
M O
Parameters O
( O
This O
Work O
) O
Low O
Rank O
BERT O
Fine O
- O
tuning O
41.0 O
40.5 O
82.9 O
82.3 O
82.4 O
79.8 O
89.4 O
88.8 O
89.0 O
79.5 O
65.0 O
60.4 O
91.3 O
92.0 O
87.0 O
81.2 O
78.5 O
75.6 O
Low O
Rank O
BERT O
+ O
KD O
44.7 O
34.0 O
83.1 O
82.4 O
83.4 O
80.4 O
89.1 O
88.7 O
89.0 O
79.9 O
64.3 O
60.6 O
91.3 O
91.5 O
86.6 O
80.9 O
78.9 O
74.8 O
Low O
Rank O
BERT O
Feature O
Distillation O
51.2 O
43.4 O
84.9 O
83.8 O
89.4 O
86.1 O
91.4 O
90.7 O
89.8 O
80.5 O
70.8 O
66.0 O
92.2 O
92.9 O
89.3 O
84.2 O
82.4 O
78.4 O
Low O
Rank O
BERT O
Feature O
Distillation O
+ O
KD O
53.0 O
42.9 O
84.8 O
83.7 O
90.4 O
86.2 O
91.4 O
90.8 O
89.7 O
80.5 O
71.1 O
67.8 O
92.4 O
92.9 O
89.4 O
84.6 O
82.8 O
78.7 O
Table O
1 O
: O
Results O
on O
GLUE O
dev O
and O
test O
sets O
. O
Metrics O
are O
Accuracy O
( O
MNLI O
( O
average O
of O
MNLI O
match O
and O
MNLI O
mis O
- O
match O
) O
, O
QNLI O
, O
RTE O
, O
SST-2 O
) O
, O
Avg O
of O
Accuracy O
and O
F1 O
( O
MRPC O
, O
QQP O
) O
, O
Matthew O
‚Äôs O
correlation O
( O
CoLA O
) O
, O
Avg O
of O
Pearson O
and O
Spearman O
correlations O
( O
STS O
- O
B O
) O
. O
BERT O
- O
base O
( O
Teacher O
) O
is O
our O
Ô¨Åne O
- O
tuned O
BERT O
model O
. O
The O
numbers O
for O
the O
6 O
layered O
models O
are O
taken O
from O
( O
Xu O
et O
al O
. O
, O
2020 O
) O
, O
Best O
results O
are O
indicated O
in O
Bold O
. O
CoLA O
MRPC O
SST-2 O
Base O
Model O
/ O
Teacher O
Model O
Fine O
- O
tuned O
Pre O
- O
trained O
None O
Fine O
- O
tuned O
Pre O
- O
trained O
None O
Fine O
- O
tuned O
Pre O
- O
trained O
None O
Fine O
- O
tuned O
48.7¬±2.4 O
47.5¬±0.7 O
40.1¬±0.6 O
88.5¬±0.5 O
85.8¬±0.5 O
81.6¬±1.3 O
91.8¬±0.4 O
91.3¬±0.5 O
90.9¬±0.4 O
Pre O
- O
trained O
49.4¬±1.7 O
44.8¬±2.1 O
10.8¬±2.6 O
89.2¬±0.4 O
86.3¬±1.0 O
77.1¬±0.6 O
91.7¬±0.2 O
91.2¬±0.4 O
89.6¬±1.1 O
Random O
3.6¬±5.1 O
0.0¬±0.0 O
0.6¬±0.6 O
75.9¬±1.1 O
75.3¬±0.7 O
75.0¬±0.4 O
88.2¬±0.5 O
87.2¬±0.7 O
81.2¬±0.5 O
Table O
2 O
: O
Results O
on O
the O
dev O
set O
of O
CoLA O
, O
MRPC O
and O
SST-2 O
tasks O
with O
different O
initializations O
and O
different O
teachers O
. O
The O
results O
are O
averages O
and O
standard O
deviations O
of O
Ô¨Åve O
runs O
with O
different O
seeds O
. O
tion O
, O
the O
results O
are O
best O
when O
decomposing O
a O
Ô¨Ånetuned O
model O
and O
decomposing O
a O
pre O
- O
trained O
model O
is O
better O
than O
randomly O
initializing O
a O
model O
; O
This O
indicates O
that O
the O
decomposition O
saves O
the O
information O
within O
the O
model O
and O
when O
decomposing O
a O
Ô¨Åne O
- O
tuned O
model O
it O
saves O
some O
of O
the O
more O
task O
speciÔ¨Åc O
information O
. O
Furthermore O
, O
on O
all O
tasks O
and O
all O
initialization O
the O
best O
results O
are O
when O
using O
a O
Ô¨Åne O
- O
tuned O
model O
as O
a O
teacher O
. O
Rank O
( O
Parameter O
Count O
) O
CoLA O
MRPC O
SST-2 O
Full O
Rank O
( O
110 O
M O
) O
58.4¬±1.2 O
88.3¬±0.7 O
92.8¬±0.5 O
350 O
( O
82.6 O
M O
) O
57.7¬±0.9 O
88.9¬±0.7 O
92.0¬±0.5 O
245 O
( O
65.2 O
M O
) O
48.7¬±2.4 O
88.5¬±0.5 O
91.8¬±0.4 O
150 O
( O
49.4 O
M O
) O
38.7¬±1.6 O
87.8¬±0.6 O
91.3¬±0.4 O
Table O
3 O
: O
Results O
on O
the O
dev O
set O
of O
CoLA O
, O
MRPC O
and O
SST-2 O
tasks O
with O
different O
ranks O
. O
The O
results O
are O
averages O
and O
standard O
deviations O
of O
Ô¨Åve O
runs O
with O
different O
seeds O
. O
Compression O
vs. O
Performance O
Trade O
- O
off O
Our O
method O
requires O
to O
determine O
a O
rank O
for O
the O
compression O
. O
But O
can O
we O
achieve O
better O
results O
when O
choosing O
a O
higher O
rank O
? O
Can O
we O
choose O
a O
lower O
rank O
for O
smaller O
models O
and O
still O
achieve O
satisfactory O
results O
? O
To O
determine O
this O
we O
experimented O
on O
three O
different O
ranks O
. O
As O
shown O
in O
Table O
3 O
, O
higher O
ranks O
achieve O
better O
results O
, O
while O
lower O
ranks O
achieve O
satisfactory O
results O
while O
compromising O
metric O
performance O
. O
Figure O
1 O
: O
Average O
time O
in O
milliseconds O
to O
run O
a O
batch O
of O
samples O
from O
all O
of O
the O
GLUE O
tasks O
, O
when O
running O
on O
a O
Intel(R O
) O
Xeon(R O
) O
Platinum O
8180 O
CPU O
@ O
2.50GHz O
and O
on O
a O
single O
TITAN O
V O
12 O
GB O
GPU O
. O
Run O
- O
time O
Savings O
In O
this O
experiment O
we O
measured O
the O
average O
time O
in O
milliseconds O
it O
takes O
for O
BERT O
- O
base O
compared O
to O
its O
decomposed O
and O
six O
- O
layered O
counterparts O
to O
output O
predictions O
for O
a O
batch O
of O
samples O
with O
varying O
batch O
sizes O
. O
As O
shown O
in O
Figure O
1 O
, O
we O
still O
gain O
a O
signiÔ¨Åcant O
time O
performance O
improvement O
when O
running O
on O
both O
CPU O
and O
GPU O
architectures O
over O
a O
BERT O
- O
base887model O
. O
Models O
that O
are O
decomposed O
to O
a O
rank O
r= O
245are O
about O
1.45faster O
than O
their O
uncompressed O
counterpart O
for O
batches O
larger O
than O
one O
when O
running O
on O
a O
GPU O
and O
around O
1.2‚àí1.55faster O
for O
batches O
8,16,32,64when O
running O
on O
a O
CPU O
. O
Furthermore O
, O
higher O
ranks O
still O
beneÔ¨Åt O
running O
time O
and O
lower O
ranks O
improve O
the O
running O
time O
further O
. O
Also O
, O
we O
note O
that O
although O
a O
six O
- O
layered O
BERT O
does O
achieve O
faster O
inference O
time O
, O
due O
to O
the O
coarse O
- O
grained O
compression O
, O
it O
losses O
more O
information O
contained O
within O
it O
and O
thus O
achieves O
inferior O
results O
; O
As O
shown O
in O
the O
results O
in O
Table O
1 O
, O
a O
six O
- O
layered O
model O
trained O
with O
distillation O
( O
e.g. O
BERT O
- O
PKD O
( O
Sun O
et O
al O
. O
, O
2019 O
) O
) O
achieves O
signiÔ¨Åcantly O
lower O
results O
and O
the O
BERT O
- O
of O
- O
Theseus O
model O
, O
which O
does O
improve O
upon O
BERT O
- O
PKD O
, O
requires O
many O
training O
iterations O
to O
achieve O
this O
to O
overcome O
the O
loss O
of O
information O
when O
gradually O
removing O
entire O
layers O
, O
which O
result O
in O
higher O
training O
times O
. O
5 O
Conclusions O
We O
presented O
a O
way O
to O
compress O
pre O
- O
trained O
large O
language O
models O
Ô¨Åne O
- O
tuned O
for O
speciÔ¨Åc O
tasks O
, O
while O
preserving O
much O
of O
the O
information O
contained O
within O
them O
, O
by O
using O
matrix O
decomposition O
to O
two O
small O
matrices O
. O
For O
future O
work O
it O
might O
be O
interesting O
to O
combine O
this O
approach O
with O
another O
approach O
such O
as O
pruning O
or O
quantization O
to O
achieve O
smaller O
models O
. O
Acknowledgements O
This O
project O
has O
received O
funding O
from O
the O
Europoean O
Research O
Council O
( O
ERC O
) O
under O
the O
Europoean O
Union O
‚Äôs O
Horizon O
2020 O
research O
and O
innovation O
programme O
, O
grant O
agreement O
No O
. O
802774 O
( O
iEXTRACT O
) O
and O
was O
sponsored O
in O
part O
by O
an O
Intel O
AI O
grant O
to O
the O
Bar O
- O
Ilan O
University O
NLP O
lab O
. O
Abstract O
Explainable O
recommendation O
is O
a O
good O
way O
to O
improve O
user O
satisfaction O
. O
However O
, O
explainable O
recommendation O
in O
dialogue O
is O
challenging O
since O
it O
has O
to O
handle O
natural O
language O
as O
both O
input O
and O
output O
. O
To O
tackle O
the O
challenge O
, O
this O
paper O
proposes O
a O
novel O
and O
practical O
task O
to O
explain O
evidences O
in O
recommending O
hotels O
given O
vague O
requests O
expressed O
freely O
in O
natural O
language O
. O
We O
decompose O
the O
process O
into O
two O
subtasks O
on O
hotel O
reviews O
: O
evidence O
identiÔ¨Åcation O
andevidence O
explanation O
. O
The O
former O
predicts O
whether O
or O
not O
a O
sentence O
contains O
evidence O
that O
expresses O
why O
a O
given O
request O
is O
satisÔ¨Åed O
. O
The O
latter O
generates O
a O
recommendation O
sentence O
given O
a O
request O
and O
an O
evidence O
sentence O
. O
In O
order O
to O
address O
these O
subtasks O
, O
we O
build O
an O
Evidence O
- O
based O
Explanation O
dataset O
, O
which O
is O
the O
largest O
dataset O
for O
explaining O
evidences O
in O
recommending O
hotels O
for O
vague O
requests O
. O
The O
experimental O
results O
demonstrate O
that O
the O
BERT O
model O
can O
Ô¨Ånd O
evidence O
sentences O
with O
respect O
to O
various O
vague O
requests O
and O
that O
the O
LSTM O
- O
based O
model O
can O
generate O
recommendation O
sentences O
. O
1 O
Introduction O
Recently O
, O
dialog O
systems O
using O
Natural O
Language O
Processing O
technology O
have O
been O
adopted O
in O
interactive O
services O
such O
as O
call O
centers O
( O
Zumstein O
and O
Hundertmark O
, O
2017 O
) O
. O
One O
challenging O
issue O
in O
a O
real O
- O
world O
scenario O
is O
vague O
requests1from O
users O
. O
For O
example O
, O
in O
a O
hotel O
booking O
service O
, O
users O
often O
ask O
operators O
for O
‚Äú O
a O
child O
- O
friendly O
hotel O
‚Äù O
or O
‚Äú O
a O
convenient O
inn O
. O
‚Äù O
To O
respond O
to O
such O
vague O
requests O
, O
human O
operators O
need O
to O
explain O
the O
reason O
why O
the O
given O
request O
1In O
this O
study O
, O
a O
vague O
request O
means O
one O
that O
does O
not O
specify O
a O
speciÔ¨Åc O
product O
, O
experience O
or O
service.is O
satisÔ¨Åed O
. O
An O
example O
response O
would O
be O
, O
‚Äú O
This O
hotel O
has O
a O
large O
kids O
‚Äô O
space O
, O
so O
I O
recommend O
it O
for O
families O
with O
children O
like O
you O
. O
‚Äù O
Responding O
to O
vague O
requests O
with O
evidences O
is O
effective O
because O
it O
not O
only O
strengthens O
the O
recommendation O
, O
but O
also O
urges O
users O
to O
make O
more O
concrete O
requests O
such O
as O
‚Äú O
I O
do O
n‚Äôt O
need O
a O
kids O
‚Äô O
space O
but O
want O
a O
baby O
stroller O
rental O
service O
. O
‚Äù O
Several O
studies O
have O
addressed O
explainable O
recommendations O
that O
produce O
natural O
language O
sentences O
( O
Zhao O
et O
al O
. O
, O
2014 O
; O
Zhang O
et O
al O
. O
, O
2014 O
; O
Wang O
et O
al O
. O
, O
2018 O
; O
Zhao O
et O
al O
. O
, O
2019 O
) O
. O
One O
major O
approach O
is O
feature O
- O
based O
explanations O
. O
Zhang O
et O
al O
. O
( O
2014 O
) O
generated O
explanation O
sentences O
using O
templates O
with O
slots O
, O
for O
example O
, O
‚Äú O
You O
might O
be O
interested O
in O
[ O
feature O
] O
, O
on O
which O
this O
product O
performs O
well O
. O
‚Äù O
However O
, O
by O
handling O
only O
predeÔ¨Åned O
and O
limited O
features O
, O
this O
study O
can O
not O
explain O
detailed O
evidences O
for O
each O
hotel O
such O
as O
‚Äú O
a O
view O
of O
Mount O
Fuji O
and O
Lake O
Kawaguchi O
. O
‚Äù O
Furthermore O
, O
this O
study O
does O
not O
accept O
natural O
language O
requests O
as O
inputs O
, O
which O
is O
a O
major O
bottleneck O
for O
building O
dialog O
- O
based O
interactive O
systems O
. O
In O
this O
study O
, O
we O
propose O
a O
novel O
and O
practical O
task O
to O
identify O
and O
explain O
evidences O
that O
satisfy O
a O
given O
vague O
request O
expressed O
freely O
in O
natural O
language O
. O
SpeciÔ¨Åcally O
, O
assuming O
a O
practical O
situation O
of O
recommendation O
, O
we O
address O
a O
hotel O
booking O
service O
. O
When O
choosing O
a O
hotel O
on O
an O
interactive O
service O
, O
users O
make O
a O
wide O
range O
of O
vague O
requests O
, O
which O
differ O
from O
predeÔ¨Åned O
aspects O
( O
Wang O
et O
al O
. O
, O
2010 O
) O
, O
emotional O
expressions O
( O
Chen O
et O
al O
. O
, O
2010 O
) O
and O
questions O
( O
Rajani O
et O
al O
. O
, O
2019 O
) O
. O
In O
order O
to O
satisfy O
vague O
requests O
by O
recommending O
hotels O
with O
evidences O
, O
the O
system O
must O
understand O
a O
given O
request O
, O
associate O
the O
request O
to O
a O
hotel O
with O
spe-890Figure O
1 O
: O
Pipeline O
for O
building O
the O
Evidence O
- O
based O
Explanation O
dataset O
ciÔ¨Åc O
evidence O
, O
and O
generate O
an O
explanation O
( O
recommendation O
sentence O
) O
for O
the O
evidence O
. O
To O
address O
these O
challenges O
, O
we O
decompose O
the O
process O
into O
two O
subtasks O
: O
Evidence O
IdentiÔ¨ÅcationandEvidence O
Explanation O
. O
The O
former O
predicts O
whether O
a O
sentence O
contains O
evidence O
that O
expresses O
why O
a O
given O
request O
is O
satisÔ¨Åed O
. O
The O
latter O
generates O
a O
recommendation O
sentence O
given O
the O
evidence O
sentence O
. O
In O
order O
to O
focus O
on O
evidence O
explanations O
for O
requests O
, O
we O
assume O
that O
recommending O
hotels O
are O
given O
in O
advance O
in O
this O
study O
. O
For O
these O
subtasks O
, O
we O
present O
an O
Evidencebased O
Explanation O
dataset O
, O
which O
is O
the O
largest O
dataset O
for O
explaining O
evidences O
in O
recommending O
hotels O
for O
vague O
requests O
. O
Assuming O
that O
titles O
of O
hotel O
reviews O
often O
correspond O
to O
vague O
requests O
, O
the O
dataset O
includes O
37,280 O
hotel O
reviews O
with O
annotations O
for O
vague O
requests O
, O
evidence O
sentences O
for O
the O
requests O
, O
recommendation O
sentences O
based O
on O
the O
evidence O
sentences O
. O
The O
key O
feature O
of O
the O
dataset O
is O
the O
variety O
of O
requests O
: O
it O
includes O
15,767 O
unique O
types O
of O
requests O
written O
in O
natural O
language O
. O
This O
dataset O
is O
publicly O
available2 O
. O
We O
report O
experiments O
for O
the O
two O
subtasks O
in O
Section O
3 O
. O
We O
build O
a O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
model O
for O
the O
Ô¨Årst O
subtask O
, O
which O
predicts O
whether O
a O
sentence O
contains O
evidence O
for O
a O
request O
. O
Experimental O
results O
show O
that O
the O
model O
can O
detect O
evidence O
sentence O
for O
various O
requests O
with O
a O
high O
( O
79.94 O
) O
F1 O
- O
score O
, O
and O
that O
the O
score O
does O
not O
drop O
so O
much O
even O
for O
requests O
unseen O
in O
the O
training O
data O
. O
We O
present O
encoderdecoder O
models O
for O
the O
second O
subtask O
, O
which O
rewrite O
an O
evidence O
sentence O
into O
a O
recommendation O
sentence O
. O
The O
experiments O
demonstrate O
that O
an O
LSTM O
( O
Luong O
et O
al O
. O
, O
2015 O
) O
based O
model O
achieves O
the O
BLEU O
score O
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
of O
56.09 O
with O
a O
gold O
evidence O
sentence O
given O
and O
that O
of O
45.38 O
without O
a O
gold O
sentence O
( O
only O
a O
re2https://github.com/megagonlabs/ebe-datasetview O
and O
a O
request O
is O
given O
) O
. O
We O
also O
report O
experiments O
when O
the O
two O
subtasks O
are O
combined O
to O
generate O
a O
recommendation O
sentence O
for O
a O
given O
review O
. O
The O
contributions O
of O
this O
paper O
are O
as O
follows O
: O
1.We O
propose O
a O
novel O
and O
practical O
task O
to O
explain O
evidences O
given O
vague O
requests O
expressed O
freely O
in O
natural O
language O
. O
2.We O
create O
a O
new O
dataset O
by O
annotating O
review O
sentences O
with O
evidences O
and O
rewriting O
each O
evidence O
into O
a O
recommendation O
sentence O
. O
This O
is O
the O
largest O
dataset O
for O
explaining O
evidences O
in O
recommending O
hotels O
for O
vague O
requests O
. O
3.Experiments O
show O
that O
our O
dataset O
enables O
to O
train O
models O
that O
can O
effectively O
Ô¨Ånd O
evidences O
to O
various O
vague O
requests O
and O
generate O
recommendation O
sentences O
. O
2 O
Dataset O
Creation O
In O
this O
section O
, O
we O
describe O
the O
procedure O
to O
create O
the O
Evidence O
- O
based O
Explanation O
dataset O
. O
The O
dataset O
is O
expected O
to O
include O
( O
i O
) O
vague O
requests O
from O
users O
, O
( O
ii O
) O
items O
( O
in O
this O
study O
, O
hotel O
candidates O
) O
, O
( O
iii O
) O
evidence O
where O
an O
item O
satisÔ¨Åes O
an O
request O
, O
( O
iv O
) O
and O
a O
recommendation O
sentence O
based O
on O
each O
evidence O
. O
As O
a O
corpus O
that O
meets O
these O
requirements O
, O
we O
use O
review O
data O
on O
Jalan3 O
, O
which O
is O
a O
major O
hotel O
booking O
service O
in O
Japan O
. O
On O
jalan O
, O
users O
can O
enter O
reviews O
after O
their O
stay O
at O
the O
hotel O
. O
In O
addition O
to O
review O
texts O
, O
Jalan O
accepts O
ratings O
for O
some O
speciÔ¨Åc O
aspects O
( O
e.g. O
, O
‚Äò O
Service O
‚Äô O
and O
‚Äò O
Cleanliness O
‚Äô O
) O
, O
similarly O
to O
other O
booking O
services O
( O
Wang O
et O
al O
. O
, O
2010 O
) O
. O
Although O
some O
aspects O
are O
similar O
to O
vague O
requests(e.g O
. O
, O
‚Äú O
good O
service O
‚Äù O
or O
‚Äú O
cheap O
hotel O
‚Äù O
) O
, O
the O
number O
of O
such O
pre3https://www.jalan.net/891CategoryExamples O
of O
requests O
# O
of O
collection O
# O
of O
annotations O
With O
‚Äú O
inn O
‚Äù O
or O
‚Äú O
hotel O
‚Äù O
Additional O
titles O
‚Äú O
inn O
‚Äù O
Additional O
( O
Types O
) O
Clean O
Clean O
hotel O
Clean O
15k O
71k O
3.6k O
( O
0.8k O
) O
Relax O
Relaxing O
inn O
Grate O
place O
to O
relax O
8k O
80k O
3.6k O
( O
1.1k O
) O
Service O
Helpful O
hotel O
staff O
were O
very O
helpful O
10k O
143k O
3.4k O
( O
2.0k O
) O
Useful O
Useful O
inn O
Useful O
for O
sightseeing O
4k O
113k O
2.7k O
( O
1.1k O
) O
Child O
friendly O
Child O
friendly O
hotel O
Child O
friendly O
3k O
81k O
2.6k O
( O
1.3k O
) O
Good O
view O
Good O
view O
hotel O
Good O
view O
1k O
34k O
2.3k O
( O
0.7k O
) O
Delicious O
Hotel O
with O
delicious O
food O
Delicious O
dinner O
1k O
145k O
2.3k O
( O
1.0k O
) O
Cost O
Good O
low O
cost O
hotel O
Low O
cost O
but O
very O
good O
5k O
89k O
2.2k O
( O
1.3k O
) O
Good O
Perfect O
hotel O
Perfect O
33k O
278k O
2.6k O
( O
1.4k O
) O
Others O
Historic O
hotel O
Historic O
atmosphere O
19k O
297k O
11.8k O
( O
5.2k O
) O
Total O
‚Äî O
‚Äî O
99k O
1.3 O
M O
37.3k O
( O
15.8k O
) O
Table O
1 O
: O
Examples O
of O
collected O
vague O
requests O
and O
the O
number O
of O
collections O
, O
uses O
, O
and O
types O
deÔ¨Åned O
aspects O
is O
very O
limited O
and O
can O
not O
cover O
diverse O
requests O
, O
such O
as O
‚Äú O
dog O
- O
friendly O
hotel O
. O
‚Äù O
Consequently O
, O
we O
created O
a O
new O
dataset O
using O
review O
titles O
and O
review O
texts O
. O
In O
the O
review O
texts O
, O
users O
describe O
their O
impressions O
on O
the O
service O
of O
the O
hotel O
based O
on O
their O
real O
experiences O
. O
Additionally O
, O
the O
review O
titles O
often O
summarize O
the O
most O
salient O
point O
of O
the O
experiences O
and O
often O
include O
similar O
expressions O
to O
vague O
requests O
such O
as O
‚Äú O
dog O
- O
friendly O
hotel O
. O
‚Äù O
Hence O
, O
assuming O
that O
some O
review O
titles O
express O
vague O
requests O
and O
that O
the O
corresponding O
review O
texts O
contain O
evidence O
, O
we O
extracted O
vague O
requests O
from O
review O
titles O
and O
annotated O
evidence O
sentences O
for O
requests O
in O
review O
texts O
. O
Finally O
, O
we O
rewrote O
the O
evidence O
sentences O
into O
recommendation O
sentences O
. O
Figure O
1illustrates O
the O
overall O
pipeline O
to O
construct O
the O
dataset O
. O
It O
consists O
of O
three O
steps O
. O
1.Collect O
vague O
requests O
from O
review O
titles O
: O
Use O
rules O
to O
Ô¨Ånd O
review O
titles O
that O
correspond O
to O
vague O
requests O
. O
2.Identify O
evidence O
: O
Ask O
crowdworkers O
to O
identify O
whether O
each O
review O
sentence O
contains O
evidence O
for O
the O
request O
corresponding O
to O
the O
review O
title O
. O
3.Explain O
evidence O
: O
Ask O
crowdworkers O
to O
write O
recommendation O
sentences O
based O
on O
the O
evidence O
sentences O
. O
2.1 O
Collecting O
Vague O
Requests O
Based O
on O
the O
fact O
that O
some O
titles O
have O
similar O
expressions O
to O
vague O
requests O
, O
we O
collected O
vague O
requests O
by O
selecting O
review O
titles O
. O
Some O
review O
titles O
are O
inappropriate O
as O
requests O
, O
for O
example O
, O
‚Äú O
Thanks O
‚Äù O
or O
‚Äú O
Stayed O
for O
the O
Ô¨Årst O
time O
. O
‚Äù O
Therefore O
, O
to O
comprehensively O
collect O
vague O
requests O
for O
hotels O
with O
less O
noise O
, O
we O
Ô¨Årst O
extracted O
review O
titlesthat O
included O
words O
representing O
accommodations O
such O
as O
‚Äú O
inn O
‚Äù O
or O
‚Äú O
hotel O
. O
‚Äù O
In O
addition O
, O
we O
applied O
Ô¨Åltering O
rules O
to O
remove O
other O
unuseful O
titles4 O
. O
Considering O
the O
possibility O
of O
data O
imbalance O
, O
we O
performed O
a O
categorical O
analysis O
. O
First O
, O
we O
applied O
morphological O
analysis O
of O
the O
collected O
requests O
using O
SudachiPy O
( O
Takaoka O
et O
al O
. O
, O
2018 O
) O
to O
normalize O
surface O
variations O
in O
the O
requests O
. O
We O
manually O
checked O
and O
categorized O
all O
Ô¨Åltered O
titles O
appearing O
more O
than O
twenty O
times O
in O
the O
corpus O
, O
which O
resulted O
in O
ten O
categories O
of O
vague O
requests O
. O
The O
distribution O
of O
categories O
in O
the O
dataset O
was O
skewed O
; O
the O
numbers O
of O
instances O
for O
some O
categories O
were O
small O
. O
For O
example O
, O
‚Äú O
Good O
hotel O
‚Äù O
is O
common O
but O
not O
‚Äú O
Hotel O
with O
delicious O
food O
. O
‚Äù O
This O
is O
because O
a O
small O
percentage O
of O
requests O
appear O
with O
the O
expression O
‚Äú O
inn O
‚Äù O
or O
‚Äú O
hotel O
. O
‚Äù O
Titles O
such O
as O
‚Äú O
Delicious O
dinner O
‚Äù O
are O
more O
frequent O
than O
‚Äú O
Hotel O
with O
delicious O
food O
. O
‚Äù O
Therefore O
, O
we O
extracted O
additional O
titles O
that O
contained O
the O
same O
content O
words O
as O
the O
extracted O
titles O
, O
excluding O
the O
accommodation O
expressions O
such O
as O
‚Äú O
inn O
‚Äù O
or O
‚Äú O
hotel O
. O
‚Äù O
For O
example O
, O
‚Äú O
Hotel O
with O
delicious O
food O
‚Äù O
‚Üí‚Äúdelicious O
‚Äù O
( O
excluding O
hotel O
and O
extracting O
a O
content O
word O
) O
‚Üí‚ÄúDelicious O
dinner O
‚Äù O
( O
additional O
titles O
) O
. O
Table O
1shows O
examples O
of O
vague O
requests O
collected O
from O
review O
titles O
. O
We O
extracted O
about O
1.4 O
million O
reviews O
( O
99k O
+ O
1.3 O
M O
) O
that O
have O
the O
collected O
requests O
in O
titles O
( O
# O
of O
collection O
) O
. O
For O
annotation O
in O
the O
next O
subsection O
, O
we O
selected O
37,280 O
reviews O
( O
# O
of O
annotations O
) O
. O
By O
expanding O
the O
collection O
rules O
, O
the O
number O
of O
requests O
increased O
greatly O
, O
and O
the O
data O
imbalance O
problem O
reduced O
. O
Furthermore O
, O
it O
also O
increased O
the O
variation O
of O
the O
request O
expressions O
. O
Overall O
, O
we O
collected O
15,767 O
unique O
kinds O
of O
titles O
in O
37,280 O
reviews O
. O
4The O
rules O
include O
, O
for O
example O
, O
titles O
must O
not O
contain O
proper O
nouns O
and O
must O
contain O
one O
or O
more O
content O
words.892Clean O
Relax O
Service O
Useful O
Child O
View O
Delicious O
Cost O
Good O
Others O
All O
Ratio O
of O
Relevant O
[ O
% O
] O
82.1 O
69.7 O
85.3 O
83.8 O
71.9 O
82.6 O
91.6 O
69.6 O
72.9 O
68.6 O
75.6 O
Ratio O
of O
Evidence O
[ O
% O
] O
48.3 O
55.4 O
71.4 O
74.1 O
58.8 O
60.1 O
68.6 O
44.3 O
56.0 O
46.1 O
55.3 O
Table O
2 O
: O
Ratio O
of O
relevant O
and O
evidence O
sentences O
included O
in O
the O
review O
text O
for O
a O
request O
Amount O
of O
evidence O
sentences O
# O
of O
reviews O
No O
evidence O
16,654 O
( O
44.7 O
% O
) O
1 O
evidence O
sentence O
16,456 O
( O
44.1 O
% O
) O
2 O
evidence O
sentences O
3,382 O
( O
9.1 O
% O
) O
‚â•3 O
evidence O
sentences O
788 O
( O
2.1 O
% O
) O
Table O
3 O
: O
Amount O
of O
evidence O
sentences O
in O
each O
review O
2.2 O
Evidence O
IdentiÔ¨Åcation O
Dataset O
We O
used O
Yahoo O
Crowdsourcing5to O
annotate O
review O
data O
with O
evidence O
for O
requests O
. O
Workers O
were O
shown O
a O
review O
title O
and O
a O
single O
sentence O
of O
the O
review O
text O
. O
Then O
they O
were O
asked O
, O
‚Äú O
Is O
the O
following O
sentence O
relevant O
to O
the O
title O
, O
and O
does O
it O
contain O
evidence O
for O
the O
title O
? O
‚Äù O
There O
were O
three O
options O
for O
the O
answer O
: O
Evidence O
, O
Relevant O
( O
not O
as O
Evidence O
) O
, O
and O
Irrelevant O
. O
Relevant O
( O
not O
as O
Evidence O
) O
means O
that O
the O
sentence O
contains O
the O
same O
expression O
as O
the O
request O
or O
its O
synonymous O
expression O
, O
but O
it O
does O
not O
present O
an O
evidence O
to O
support O
the O
request O
( O
title O
) O
. O
Although O
the O
evidence O
may O
make O
sense O
by O
combining O
two O
or O
more O
sentences O
, O
we O
annotated O
each O
sentence O
of O
the O
review O
independently O
to O
simplify O
the O
annotation O
work O
. O
We O
annotated O
37,280 O
reviews O
in O
total O
( O
‚Äú O
# O
of O
annotations O
‚Äù O
in O
Table O
1 O
) O
. O
For O
a O
higher O
quality O
, O
each O
task O
was O
annotated O
by O
Ô¨Åve O
people O
. O
We O
also O
prepared O
check O
questions O
for O
each O
task O
. O
Table O
2reports O
the O
ratios O
of O
the O
Evidence O
and O
Relevant O
instances O
by O
category O
. O
In O
the O
‚Äò O
Useful O
‚Äô O
category O
, O
74 O
% O
of O
the O
reviews O
contained O
evidence O
in O
the O
text O
, O
while O
only O
44 O
% O
of O
the O
reviews O
in O
the O
‚Äò O
Cost O
‚Äô O
category O
did O
. O
This O
is O
because O
users O
apt O
to O
explain O
the O
reason O
for O
an O
‚Äò O
useful O
‚Äô O
hotel O
in O
a O
review O
, O
but O
because O
the O
necessity O
of O
explaining O
the O
reason O
for O
‚Äò O
cheap O
‚Äô O
hotel O
is O
relatively O
low O
. O
Table O
3shows O
the O
number O
of O
evidence O
sentences O
for O
each O
review O
request O
. O
Approximately O
half O
of O
the O
reviews O
contained O
evidence O
. O
Requests O
that O
have O
a O
lot O
of O
evidence O
per O
review O
were O
an O
unique O
feature O
of O
this O
dataset O
. O
For O
example O
, O
requests O
that O
express O
5It O
is O
a O
microtask O
crowdsourcing O
service O
in O
Japan O
. O
We O
mixed O
some O
check O
questions O
in O
the O
tasks O
and O
receive O
annotated O
data O
from O
only O
workers O
who O
answered O
the O
check O
questions O
correctly O
. O
We O
did O
not O
set O
gender O
or O
attribute O
limits O
of O
workers O
in O
all O
our O
tasks O
. O
https://crowdsourcing.yahoo.co.jp/general O
goodness O
such O
as O
‚Äú O
good O
hotel O
‚Äù O
have O
lots O
of O
evidence O
. O
In O
this O
case O
, O
the O
task O
of O
labeling O
evidence O
sentences O
was O
similar O
to O
annotation O
efforts O
for O
sentiment O
analysis O
. O
2.3 O
Evidence O
Explanation O
Dataset O
Using O
crowdsourcing O
, O
we O
rewrote O
evidence O
sentences O
into O
recommendation O
sentences O
. O
First O
, O
we O
showed O
workers O
a O
review O
title O
and O
an O
evidence O
sentence O
. O
Then O
we O
asked O
them O
to O
write O
a O
recommendation O
sentence O
so O
that O
the O
sentence O
can O
be O
used O
to O
explain O
the O
evidence O
in O
recommending O
the O
hotel O
to O
a O
user O
. O
We O
annotated O
25,804 O
sentences O
that O
at O
least O
three O
of O
the O
Ô¨Åve O
workers O
judged O
to O
contain O
evidence O
in O
Section O
2.2 O
. O
We O
asked O
workers O
to O
report O
the O
following O
two O
cases O
. O
( O
1 O
) O
The O
request O
is O
a O
negative O
expression O
such O
as O
‚Äú O
bad O
view O
. O
‚Äù O
( O
2 O
) O
There O
is O
no O
evidence O
in O
a O
given O
sentence6 O
. O
To O
ensure O
the O
quality O
of O
the O
annotation O
, O
each O
sentence O
was O
annotated O
by O
Ô¨Åve O
workers O
, O
and O
we O
prepared O
check O
questions O
for O
each O
task O
. O
In O
the O
check O
questions O
, O
we O
prepared O
negative O
expressions O
for O
requests O
, O
and O
conÔ¨Årmed O
that O
the O
workers O
followed O
the O
instructions O
properly O
. O
Table O
4shows O
the O
number O
of O
the O
exact O
matches O
of O
Ô¨Åve O
workers O
for O
the O
created O
recommendation O
sentence O
. O
When O
only O
extracting O
a O
phrase O
from O
a O
review O
is O
sufÔ¨Åcient O
as O
a O
recommendation O
sentence O
, O
the O
Ô¨Åve O
workers O
tended O
to O
produce O
an O
identical O
result O
. O
On O
the O
other O
hand O
, O
when O
a O
certain O
part O
in O
a O
review O
had O
to O
be O
rewritten O
, O
recommendation O
sentences O
from O
the O
Ô¨Åve O
workers O
tended O
to O
differ O
. O
3 O
Experiments O
Using O
the O
annotated O
dataset O
, O
we O
conducted O
two O
experiments O
. O
( O
1 O
) O
Evidence O
IdentiÔ¨Åcation O
and O
( O
2 O
) O
Evidence O
Explanation O
. O
The O
former O
predicts O
whether O
a O
sentence O
contains O
evidence O
for O
a O
request O
, O
whereas O
the O
latter O
generates O
a O
recommendation O
sentence O
. O
6We O
targeted O
sentences O
where O
at O
least O
three O
people O
judged O
to O
contain O
evidence O
. O
However O
, O
it O
was O
sometimes O
difÔ¨Åcult O
to O
write O
recommendation O
sentences O
when O
two O
out O
of O
Ô¨Åve O
workers O
judged O
that O
the O
sentence O
has O
no O
evidence.893 O
# O
of O
sameanswers O
# O
of O
sent O
. O
Examples O
Title O
Evidence O
sentence O
Recommendation O
sentence O
‚â•2 O
matches O
13,100 O
PetfriendlyThis O
hotel O
is O
tolerant O
of O
dog O
lovers O
because O
you O
can O
sleep O
in O
a O
bed O
with O
your O
dog.(We O
recommend O
this O
) O
because O
you O
can O
sleep O
in O
a O
bed O
with O
your O
dog O
. O
All O
different O
9,889 O
Nice O
openair O
bathThe O
temperature O
of O
the O
bath O
was O
just O
right O
, O
and O
we O
spent O
a O
long O
time O
in O
the O
open O
- O
air O
bath O
watching O
the O
stars.(We O
recommend O
this O
) O
because O
you O
can O
take O
a O
long O
open O
- O
air O
bath O
while O
gazing O
at O
the O
stars O
. O
Negative O
req O
. O
( O
‚â•3)1,651 O
The O
scenery O
... O
We O
booked O
a O
Bay O
Bridge O
view O
, O
but O
it O
was O
only O
visible O
from O
the O
edge O
of O
the O
window. î O
No O
Evidence O
1,164 O
Mountain O
side O
viewIt O
was O
an O
ocean O
view O
hotel O
but O
we O
stayed O
on O
the O
mountain O
side. î O
Table O
4 O
: O
Examples O
of O
recommendation O
sentences O
rewritten O
by O
workers O
and O
matching O
rate O
of O
rewriting O
Reviews O
Sentences O
Positive O
( O
% O
) O
Train O
29,826 O
148,671 O
20,709 O
( O
13.9 O
) O
Dev O
3,726 O
18,549 O
2,606 O
( O
14.0 O
) O
Test O
3,728 O
18,823 O
2,489 O
( O
13.2 O
) O
Total O
37,280 O
186,043 O
25,804 O
( O
13.9 O
) O
Table O
5 O
: O
Evaluation O
data O
for O
evidence O
identiÔ¨Åcation O
3.1 O
Evidence O
IdentiÔ¨Åcation O
Task O
Task O
Description O
The O
task O
is O
to O
predict O
whether O
or O
not O
a O
sentence O
contains O
an O
evidence O
for O
a O
request O
. O
This O
is O
a O
binary O
classiÔ¨Åcation O
problem O
. O
A O
positive O
example O
is O
a O
sentence O
to O
which O
at O
least O
three O
out O
of O
the O
Ô¨Åve O
workers O
labeled O
evidence O
. O
All O
other O
sentences O
are O
treated O
as O
negative O
examples O
. O
We O
randomly O
divided O
the O
data O
by O
review O
into O
training O
, O
development O
, O
and O
test O
set O
( O
see O
Table O
5 O
) O
. O
We O
used O
the O
same O
data O
split O
in O
all O
experiments O
. O
Experimental O
Settings O
We O
explored O
logistic O
regression7and O
BERT O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
as O
classiÔ¨Åcation O
models O
. O
For O
the O
tokenization O
, O
we O
used O
juman++8(Tolmachev O
et O
al O
. O
, O
2018 O
) O
and O
Byte O
pair O
encoding O
( O
BPE O
) O
( O
Sennrich O
et O
al O
. O
, O
2016 O
) O
with O
the O
vocabulary O
size O
of O
8k O
. O
We O
pre O
- O
trained O
word2vec O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
CBOW O
model O
, O
and O
the O
BERT O
model O
on O
two O
million O
review O
sentences O
in O
Jalan O
. O
For O
the O
logistic O
regression O
, O
we O
calculated O
the O
TF O
- O
IDF9(Jones O
, O
1972 O
) O
vector O
and O
the O
average O
vector O
of O
word2vec O
for O
requests O
and O
sentences O
respectively O
. O
We O
used O
the O
request O
vector O
, O
the O
sentence O
vector O
, O
and O
the O
difference O
between O
the O
two O
vectors O
as O
features O
. O
The O
input O
to O
the O
BERT O
model O
was O
in O
the O
following O
order O
: O
request O
sentence O
, O
[ O
SEP O
] O
, O
and O
evidence O
sentence O
. O
Hyperparameters O
of O
each O
model O
were O
tuned O
by O
the O
F1score O
on O
the O
development O
set O
. O
7Implemented O
in O
: O
https://scikit-learn.org O
8https://github.com/ku-nlp/jumanpp O
9We O
used O
the O
word O
frequency O
in O
the O
sentence O
as O
TF O
, O
and O
the O
word O
frequency O
of O
the O
review O
text O
as O
DF.Results O
and O
Analysis O
Table O
6reports O
F1 O
- O
score O
of O
both O
models O
for O
each O
category O
and O
all O
categories O
. O
The O
F1 O
- O
score O
of O
BERT O
for O
all O
the O
data O
was O
79.94 O
, O
which O
is O
33.15 O
points O
higher O
than O
the O
logistic O
regression O
. O
Results O
in O
each O
category O
show O
that O
BERT O
had O
the O
highest O
F1 O
- O
score O
for O
‚Äò O
Useful O
‚Äô O
and O
the O
lowest O
for O
‚Äò O
Good O
‚Äô O
. O
We O
analyze O
the O
results O
of O
the O
evidence O
identiÔ¨Åcation O
by O
the O
BERT O
model O
from O
different O
perspectives O
in O
the O
following O
paragraphs O
. O
Evidence O
IdentiÔ¨Åcation O
without O
Requests O
The O
F1 O
- O
score O
of O
the O
BERT O
model O
was O
relatively O
high O
, O
considering O
the O
nature O
of O
this O
task O
, O
i.e. O
, O
associating O
evidences O
to O
requests O
. O
However O
, O
we O
need O
to O
make O
sure O
whether O
the O
BERT O
model O
considers O
a O
request O
when O
identifying O
an O
evidence O
. O
Thus O
, O
we O
trained O
another O
BERT O
model O
without O
a O
request O
( O
only O
a O
sentence O
is O
given O
) O
as O
an O
input O
. O
The O
model O
trained O
without O
a O
request O
resulted O
in O
the O
F1 O
- O
score O
of O
43.22 O
, O
which O
is O
37 O
points O
lower O
than O
that O
with O
a O
request O
. O
This O
huge O
gap O
indicates O
that O
evidences O
in O
our O
dataset O
depend O
on O
requests O
and O
that O
the O
BERT O
model O
pays O
attention O
to O
requests O
properly O
. O
For O
example O
, O
a O
model O
trained O
with O
a O
request O
predicts O
that O
the O
sentence O
, O
‚Äú O
It O
was O
pleasant O
in O
the O
room O
with O
a O
view O
of O
the O
sea O
‚Äù O
is O
evidence O
for O
a O
request O
‚Äú O
good O
view O
‚Äù O
but O
not O
for O
‚Äú O
good O
food O
‚Äù O
. O
In O
contrast O
, O
a O
model O
trained O
without O
a O
request O
predicts O
that O
the O
both O
are O
evidence O
sentences O
. O
Unseen O
Requests O
Since O
the O
dataset O
contains O
a O
wide O
range O
of O
requests O
, O
30 O
% O
of O
the O
requests O
in O
the O
test O
set O
are O
unseen O
, O
not O
appearing O
in O
the O
training O
set O
. O
Thus O
, O
we O
divided O
the O
test O
set O
in O
terms O
whether O
a O
request O
is O
unseen O
or O
not O
, O
and O
computed O
the O
F1score O
in O
Table O
7 O
. O
Although O
the O
F1 O
- O
score O
for O
unseen O
requests O
drops O
by O
6.44 O
points O
, O
it O
is O
still O
high O
compared O
to O
the O
score O
trained O
without O
a O
request O
( O
described O
in O
the O
previous O
paragraph O
) O
. O
This O
indicates O
that O
the O
model O
makes O
a O
successful O
prediction O
for894Model O
Clean O
Relax O
Service O
Useful O
Child O
View O
Delicious O
Cost O
Good O
Others O
All O
Logistic O
regression O
44.39 O
46.03 O
51.21 O
61.30 O
49.39 O
61.02 O
54.34 O
30.24 O
34.76 O
37.15 O
46.79 O
BERT O
79.52 O
82.89 O
84.98 O
89.48 O
85.04 O
81.23 O
82.54 O
73.59 O
68.85 O
73.89 O
79.94 O
Table O
6 O
: O
F1 O
- O
score O
for O
evidence O
identiÔ¨Åcation O
for O
each O
category O
Figure O
2 O
: O
Characteristics O
of O
evidence O
sentencesQuadrant O
F1 O
A O
87.11 O
B O
82.64 O
C O
79.01 O
D O
75.87 O
A+B O
83.12 O
C+D O
76.30 O
A+C O
82.82 O
B+D O
79.54 O
All O
79.94 O
Table O
8 O
: O
F1 O
- O
score O
for O
each O
quadrant O
F1 O
# O
of O
instances O
Unseen O
requests O
75.40 O
5,857 O
Seen O
requests O
81.84 O
12,966 O
Table O
7 O
: O
F1 O
- O
score O
for O
unseen O
/ O
seen O
requests O
majority O
of O
the O
unknown O
requests O
. O
Examining O
successful O
predictions O
for O
unseen O
requests O
, O
we O
found O
that O
the O
same O
expression O
to O
the O
request O
often O
appears O
in O
the O
evidence O
sentence O
, O
For O
example O
, O
in O
response O
to O
a O
request O
for O
‚Äú O
a O
good O
location O
to O
watch O
a O
football O
game O
, O
‚Äù O
the O
evidence O
sentence O
includes O
, O
‚Äú O
It O
‚Äôs O
located O
in O
front O
of O
Tosu O
Station O
in O
Saga O
, O
and O
it O
‚Äôs O
a O
good O
location O
to O
watch O
the O
Tosu O
football O
game O
. O
‚Äù O
The O
expression O
in O
italic O
is O
considered O
to O
be O
a O
clue O
for O
predicting O
the O
evidence O
label O
for O
the O
sentence O
. O
The O
analysis O
of O
whether O
the O
request O
is O
included O
in O
the O
evidence O
sentence O
is O
discussed O
in O
detail O
in O
the O
next O
paragraph O
. O
In O
contrast O
, O
we O
observed O
difÔ¨Åcult O
instances O
as O
well O
. O
For O
example O
, O
the O
request O
( O
review O
title O
) O
is O
, O
‚Äú O
You O
can O
fully O
enjoy O
an O
extraordinary O
experience O
, O
‚Äù O
and O
the O
evidence O
sentence O
is O
, O
‚Äú O
I O
was O
refreshed O
by O
soaking O
in O
a O
hot O
spring O
while O
listening O
to O
the O
chirping O
of O
birds O
and O
the O
sound O
of O
insects O
. O
‚Äù O
The O
BERT O
model O
could O
not O
infer O
that O
the O
experience O
( O
hot O
spring O
, O
chirping O
birds O
) O
is O
extraordinary O
and O
that O
the O
sentence O
is O
an O
evidence O
for O
the O
request O
. O
Characteristics O
of O
Evidence O
Sentences O
There O
are O
various O
ways O
to O
express O
an O
evidence O
sentence O
, O
for O
example O
, O
with O
and O
without O
a O
use O
of O
conjunctions O
. O
Figure O
2illustrates O
four O
categories O
( O
decomposed O
into O
two O
axes O
) O
of O
how O
a O
sentence O
presents O
an O
evidence O
for O
a O
request O
. O
The O
y O
- O
axis O
is O
whether O
a O
request O
expression O
appears O
in O
an O
evidence O
sentence O
. O
The O
x O
- O
axis O
is O
whether O
there O
is O
an O
explicit O
conjunction O
( O
e.g. O
, O
‚Äò O
because O
‚Äô O
) O
expressing O
the O
discourse O
relation O
between O
a O
request O
and O
evidence O
. O
We O
have O
automatically O
divided O
these O
categories O
by O
rules O
. O
The O
top O
- O
left O
quadrant O
A O
includes O
a O
request O
expression O
and O
an O
explicit O
conjunction O
in O
the O
sentence O
. O
Although O
60 O
% O
contain O
evidence O
for O
a O
request O
, O
quadrant O
A O
has O
the O
smallest O
volume O
. O
On O
the O
other O
hand O
, O
the O
lower O
- O
right O
quadrant O
D O
has O
the O
largest O
volume O
, O
but O
has O
the O
smallest O
ratio O
of O
including O
evidence O
for O
the O
request O
( O
only O
7 O
% O
) O
. O
The O
evidence O
for O
quadrant O
A O
can O
be O
collected O
by O
a O
simple O
rule O
, O
but O
it O
is O
comprised O
of O
only O
about O
6 O
% O
of O
the O
total O
evidence O
. O
Our O
dataset O
successfully O
extracts O
other O
evidence O
expressions O
using O
the O
relationship O
between O
the O
review O
title O
and O
the O
text O
. O
Table O
8shows O
the O
F1 O
- O
score O
of O
the O
BERT O
model O
for O
each O
quadrant O
. O
The O
F1 O
- O
score O
of O
quadrant O
A O
, O
which O
contains O
an O
explicit O
conjunction O
and O
request O
words O
, O
was O
highest O
( O
87.11 O
) O
. O
It O
was O
7.17 O
points O
higher O
than O
the O
average O
F1 O
- O
score O
of O
all O
test O
data O
. O
On O
the O
other O
hand O
, O
the O
F1 O
- O
score O
of O
quadrant O
D O
, O
which O
does O
not O
contain O
an O
explicit O
conjunction O
nor O
any O
request O
words O
, O
was O
lowest O
( O
75.87 O
) O
. O
It O
was O
4.07 O
points O
lower O
than O
the O
average O
F1 O
- O
score O
of O
all O
test O
data O
. O
In O
addition O
, O
the O
F1 O
- O
score O
of O
quadrant O
A+B O
was O
6.82 O
points O
higher O
than O
the O
F1 O
- O
score O
of O
quadrant O
C+D O
, O
indicating O
that O
the O
presence O
of O
the O
request O
expression O
in O
the O
evidence O
sentence O
significantly O
impacts O
on O
the O
performance O
of O
predicting O
evidence O
. O
We O
examined O
successful O
cases O
in O
quadrant O
D O
, O
which O
is O
the O
most O
difÔ¨Åcult O
of O
all O
. O
In O
these O
cases O
, O
we O
found O
that O
expressions O
similar O
to O
the O
requests O
often O
appear O
in O
the O
evidence O
sentence O
. O
For O
exam-895ple O
, O
in O
response O
to O
the O
request O
‚Äú O
I O
am O
soothed O
by O
a O
meal O
, O
‚Äù O
the O
evidence O
sentence O
is O
‚Äú O
I O
was O
impressed O
by O
the O
deliciousness O
of O
the O
freshly O
made O
egg O
rolls O
forbreakfast O
. O
‚Äù O
In O
the O
example O
, O
the O
word O
‚Äò O
meal O
‚Äô O
in O
the O
request O
is O
related O
to O
the O
word O
‚Äò O
breakfast O
. O
‚Äô O
However O
, O
the O
model O
could O
not O
recognize O
that O
the O
sentence O
, O
‚Äú O
We O
have O
a O
foot O
washing O
place O
next O
to O
the O
entrance O
, O
gum O
roller O
and O
wet O
tissue O
, O
it O
was O
very O
thorough O
, O
‚Äù O
contains O
an O
evidence O
for O
the O
request O
, O
‚Äú O
An O
inn O
where O
I O
can O
stay O
with O
my O
pet O
dog O
. O
‚Äù O
This O
may O
be O
due O
to O
the O
lack O
of O
similar O
expressions O
for O
the O
request O
in O
the O
sentence O
, O
and O
the O
failure O
to O
associate O
dog O
and O
dog O
amenities O
. O
3.2 O
Evidence O
Explanation O
Task O
Task O
Description O
The O
task O
generates O
a O
recommendation O
sentence O
given O
request O
and O
evidence O
sentences O
. O
We O
used O
only O
the O
data O
that O
three O
or O
more O
workers O
rewrote O
into O
recommendation O
sentences O
in O
Section O
2.3 O
. O
Each O
evidence O
sentence O
had O
multiple O
recommendation O
sentences O
rewritten O
by O
the O
workers O
, O
and O
we O
use O
all O
of O
them O
as O
training O
data O
. O
We O
use O
BLEU O
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
to O
evaluate O
generated O
sentences O
. O
Experiment O
Settings O
We O
compared O
three O
models O
: O
a O
rule O
- O
based O
model O
and O
two O
neural O
network O
models O
. O
The O
rule O
- O
based O
model O
rewrites O
an O
evidence O
sentence O
into O
a O
recommendation O
sentence O
by O
focusing O
on O
the O
root O
node O
in O
the O
parse O
tree O
of O
the O
evidence O
sentence O
. O
The O
rules O
include O
: O
if O
the O
root O
node O
is O
a O
verb O
, O
adjective O
, O
or O
auxiliary O
verb O
, O
add O
‚Äú O
because O
‚Äù O
at O
the O
beginning O
; O
if O
the O
root O
node O
is O
a O
noun O
, O
add O
‚Äú O
because O
of O
‚Äù O
at O
the O
beginning O
; O
and O
if O
the O
root O
node O
in O
an O
adverb O
, O
add O
‚Äú O
because O
you O
can O
do O
‚Äù O
at O
the O
beginning O
. O
For O
neural O
network O
models O
, O
we O
employed O
an O
LSTM O
model O
with O
attention O
( O
Luong O
et O
al O
. O
, O
2015 O
) O
and O
a O
Transformer O
model O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
assuming O
that O
the O
task O
is O
translation O
from O
an O
evidence O
sentence O
into O
a O
recommendation O
sentence O
. O
We O
used O
the O
FAIRSEQ O
( O
Ott O
et O
al O
. O
, O
2019 O
) O
to O
implement O
the O
models O
. O
We O
tokenized O
it O
using O
Juman++ O
and O
BPE O
. O
The O
input O
to O
the O
model O
was O
in O
the O
following O
order O
: O
request O
sentence O
, O
[ O
SEP O
] O
, O
and O
evidence O
sentence O
. O
Hyper O
- O
parameters O
of O
the O
models O
were O
tuned O
by O
the O
BLEU O
score O
on O
the O
development O
set O
. O
For O
the O
evaluation O
, O
we O
used O
the O
BLEU O
score O
on O
sentences O
tokenized O
by O
Juman++ O
( O
not O
by O
BPE O
) O
. O
Since O
the O
number O
of O
references O
for O
each O
evidence O
sentence O
was O
not O
constant O
, O
we O
randomly O
selected O
one O
. O
Method O
BLEU O
No O
- O
rewrite O
47.17 O
Rule O
- O
based O
50.26 O
LSTM O
56.09 O
Transformer O
55.79 O
Table O
9 O
: O
BLEU O
score O
to O
generate O
recommendation O
given O
evidence O
and O
a O
request O
Method O
BLEU O
F1 O
Pipeline O
( O
BERT O
‚ÜíLSTM O
) O
45.38 O
63.30 O
End O
- O
to O
- O
end O
( O
LSTM O
) O
16.27 O
49.13 O
Table O
10 O
: O
BLEU O
score O
to O
generate O
recommendation O
given O
review O
text O
and O
a O
request O
Results O
and O
Analysis O
Table O
9shows O
BLEU O
scores O
of O
generated O
recommendation O
sentences O
. O
‚Äò O
No O
- O
rewrite O
‚Äô O
is O
the O
baseline O
where O
the O
evidence O
sentence O
is O
treated O
as O
the O
recommendation O
sentence O
without O
a O
rewrite O
. O
Compared O
with O
this O
baseline O
( O
47.17 O
BLEU O
) O
, O
all O
generation O
methods O
obtained O
higher O
BLEU O
scores O
. O
The O
score O
of O
the O
LSTM O
- O
based O
model O
( O
56.09 O
) O
was O
0.30 O
points O
higher O
than O
that O
of O
the O
Transformer O
- O
based O
model O
( O
55.79 O
) O
. O
However O
, O
the O
BLEU O
score O
of O
the O
rulebased O
model O
was O
only O
5.83 O
point O
lower O
than O
the O
LSTM O
- O
based O
model O
. O
This O
implies O
that O
this O
task O
requires O
fewer O
rewrites O
than O
we O
expected O
. O
There O
are O
some O
differences O
between O
the O
outputs O
of O
the O
rule O
- O
based O
model O
and O
the O
LSTM O
- O
based O
model O
. O
The O
rule O
- O
based O
model O
tends O
to O
produce O
longer O
sentences O
because O
it O
can O
not O
generate O
a O
sentence O
from O
scratch O
. O
In O
addition O
, O
the O
rule O
- O
based O
model O
fails O
when O
an O
evidence O
sentence O
includes O
unnecessary O
information O
, O
for O
example O
, O
‚Äú O
it O
‚Äôs O
close O
to O
the O
station O
and O
it O
‚Äôs O
convenient O
, O
so O
we O
‚Äôd O
like O
to O
use O
it O
again O
. O
‚Äù O
The O
LSTM O
- O
based O
model O
could O
successfully O
generate O
‚Äú O
( O
We O
recommend O
this O
hotel O
) O
Because O
it O
‚Äôs O
also O
close O
to O
the O
station O
and O
it O
‚Äôs O
convenient O
, O
‚Äù O
although O
the O
rule O
- O
base O
model O
kept O
‚Äú O
so O
we O
‚Äôd O
like O
to O
use O
it O
again O
‚Äù O
and O
generated O
, O
‚Äú O
Because O
it O
‚Äôs O
close O
to O
the O
station O
and O
it O
‚Äôs O
convenient O
, O
so O
we O
‚Äôd O
like O
to O
use O
it O
again O
. O
‚Äù O
3.3 O
End O
- O
to O
- O
end O
Experiment O
In O
this O
section O
, O
we O
present O
an O
experiment O
to O
generate O
a O
recommendation O
sentence O
given O
review O
data O
( O
a O
request O
and O
review O
sentences O
) O
as O
an O
input O
. O
Combining O
the O
subtasks O
1 O
and O
2 O
, O
this O
end O
- O
to O
- O
end O
experiment O
converts O
a O
hotel O
review O
into O
a O
recommendation O
sentence O
. O
When O
a O
given O
review O
does O
not O
have O
evidence O
, O
a O
system O
is O
expected O
to O
yield O
a O
special896Review O
title O
: O
The O
view O
was O
great O
! O
Review O
text O
: O
On O
the O
Ô¨Årst O
day O
, O
I O
purchased O
a O
premium O
sightseeing O
ticket O
before O
check O
- O
in O
. O
Then O
I O
went O
to O
the O
Higashiyama O
Zoo O
to O
meet O
a O
handsome O
gorilla O
, O
Shabani O
. O
On O
the O
next O
day O
, O
I O
went O
to O
the O
Maglev O
and O
Railway O
Museum O
, O
and O
enjoyed O
it O
. O
In O
the O
room O
, O
where O
Nagoya O
Castle O
can O
be O
seen O
beautifully O
, O
the O
children O
were O
delighted O
. O
... O
Reference O
: O
( O
I O
recommend O
this O
hotel O
) O
Because O
the O
room O
has O
a O
beautiful O
view O
of O
Nagoya O
Castle O
Pipeline O
: O
Because O
the O
children O
were O
overjoyed O
because O
they O
had O
a O
beautiful O
view O
of O
Nagoya O
Castle O
. O
End O
- O
to O
- O
end O
: O
Because O
you O
have O
a O
beautiful O
view O
of O
Nagoya O
Castle O
from O
your O
room O
. O
Review O
title O
: O
Happy O
with O
my O
doggy O
Review O
text O
: O
The O
couple O
, O
father O
and O
two O
dogs O
used O
it O
. O
... O
The O
rooms O
were O
spacious O
, O
and O
the O
large O
windows O
overlooked O
the O
sea O
. O
I O
was O
very O
satisÔ¨Åed O
. O
... O
While O
many O
pet O
- O
friendly O
hotels O
are O
disappointing O
, O
this O
one O
is O
recommended O
. O
... O
Reference O
: O
While O
many O
pet O
- O
friendly O
hotels O
are O
disappointing O
, O
this O
one O
is O
recommended O
Pipeline O
: O
Because O
pet O
- O
friendly O
hotels O
are O
very O
convenient O
End O
- O
to O
- O
end O
: O
Because O
the O
rooms O
were O
spacious O
the O
large O
windows O
overlooked O
the O
sea O
, O
and O
I O
was O
very O
satisÔ¨Åed O
Review O
title O
: O
A O
hotel O
recommended O
for O
families O
with O
children O
Review O
text O
: O
Because O
I O
have O
a O
16 O
- O
month O
old O
child O
, O
I O
was O
drawn O
to O
the O
plan O
that O
included O
room O
service O
and O
a O
private O
hot O
spring O
... O
The O
mattress O
was O
thin O
because O
of O
its O
age O
, O
so O
it O
would O
have O
been O
better O
if O
it O
were O
thicker O
. O
The O
hot O
springs O
and O
customer O
service O
were O
good O
, O
and O
it O
was O
good O
that O
the O
staff O
treated O
my O
children O
kindly O
. O
Reference O
: O
Because O
the O
hot O
springs O
and O
customer O
service O
were O
good O
, O
and O
it O
was O
good O
for O
children O
. O
Pipeline O
: O
Because O
the O
hot O
springs O
and O
customer O
service O
were O
good O
, O
and O
it O
was O
good O
for O
children O
. O
End O
- O
to O
- O
end O
: O
Because O
the O
pool O
and O
customer O
service O
were O
good O
, O
and O
it O
was O
good O
for O
children O
. O
Table O
11 O
: O
Examples O
of O
generating O
recommendation O
sentences O
given O
the O
review O
data O
token O
[ O
no O
- O
evidence O
] O
. O
We O
explored O
two O
approaches O
, O
pipeline O
and O
endto O
- O
end O
. O
The O
pipeline O
method O
is O
simply O
a O
combination O
of O
the O
models O
from O
Sections O
3.1and3.2 O
. O
The O
method O
Ô¨Årst O
predicts O
whether O
a O
sentence O
in O
a O
review O
present O
an O
evidence O
for O
a O
request O
by O
using O
the O
BERT O
model O
. O
It O
then O
generates O
a O
recommendation O
sentence O
by O
using O
the O
LSTM O
- O
based O
model O
for O
the O
request O
and O
the O
predicted O
evidence O
sentence O
with O
the O
highest O
score O
assigned O
by O
the O
BERT O
model O
only O
when O
the O
review O
includes O
evidence O
sentences O
. O
If O
the O
BERT O
model O
predicts O
no O
sentence O
in O
the O
review O
as O
evidence O
, O
the O
method O
generates O
[ O
no O
- O
evidence O
] O
. O
The O
end O
- O
to O
- O
end O
method O
is O
an O
encoder O
- O
decoder O
LSTM O
model O
that O
directly O
generates O
a O
recommendation O
sentence O
given O
a O
review O
title O
and O
text O
. O
An O
input O
to O
the O
model O
is O
request O
and O
[ O
SEP O
] O
, O
followed O
by O
multiple O
sentences O
of O
the O
review O
. O
When O
a O
review O
did O
not O
contain O
an O
evidence O
for O
the O
request O
, O
the O
model O
is O
trained O
to O
generate O
[ O
no O
- O
evidence O
] O
. O
Table O
10shows O
the O
BLEU O
scores O
and O
the O
macro O
- O
average O
F1 O
- O
scores O
of O
the O
methods O
. O
The O
macro O
- O
average O
F1 O
- O
score O
is O
deÔ¨Åned O
similarly O
to O
the O
evaluation O
conducted O
by O
Rajpurkar O
et O
al O
. O
( O
2016 O
) O
10 O
. O
The O
pipeline O
method O
outperformed O
the O
end O
- O
to O
- O
end O
method O
, O
achiving O
a O
BLEU O
score O
of O
45.38 O
, O
29.11 O
points O
higher O
than O
the O
end2end O
model O
. O
This O
is O
probably O
because O
the O
pipeline O
model O
could O
utilize O
10The O
metric O
measures O
matches O
of O
bag O
- O
of O
- O
tokens O
in O
the O
reference O
and O
generated O
sentences O
. O
For O
reviews O
without O
an O
evidence O
, O
we O
regard O
that O
the O
system O
output O
is O
correct O
if O
the O
generated O
output O
is O
no-evidence.the O
pre O
- O
trained O
BERT O
model O
and O
because O
training O
the O
end O
- O
to O
- O
end O
method O
was O
difÔ¨Åcult O
with O
very O
long O
sequences O
of O
tokens O
given O
as O
inputs O
. O
In O
addition O
, O
the O
end O
- O
to O
- O
end O
method O
tends O
to O
output O
too O
many O
[ O
no O
- O
evidence O
] O
and O
the O
total O
number O
of O
output O
words O
is O
low O
, O
so O
the O
BLEU O
score O
is O
also O
low O
due O
to O
brevity O
penalty O
. O
Table O
11presents O
examples O
of O
the O
generated O
sentences O
. O
In O
the O
Ô¨Årst O
example O
, O
the O
both O
models O
successfully O
generated O
appropriate O
recommendation O
sentences O
. O
Although O
the O
end O
- O
to O
- O
end O
method O
generated O
the O
natural O
sentence O
in O
the O
second O
example O
, O
the O
recommendation O
is O
nothing O
to O
do O
with O
the O
request O
, O
‚Äú O
happy O
with O
my O
doggy O
. O
‚Äù O
In O
the O
third O
example O
, O
the O
end O
- O
to O
- O
end O
method O
generated O
the O
word O
‚Äú O
pool O
‚Äù O
, O
which O
was O
actually O
false O
because O
the O
the O
review O
text O
only O
refers O
to O
‚Äú O
hot O
spring O
. O
‚Äù O
We O
observed O
these O
incorrect O
generations O
from O
the O
end O
- O
toend O
method O
more O
than O
from O
the O
pipeline O
method O
. O
4 O
Related O
Work O
Several O
studies O
addressed O
explainable O
recommendations O
( O
Sarwar O
et O
al O
. O
, O
2001 O
; O
Diao O
et O
al O
. O
, O
2014 O
; O
Zhao O
et O
al O
. O
, O
2014 O
; O
Zhang O
et O
al O
. O
, O
2014 O
; O
Wang O
et O
al O
. O
, O
2018 O
; O
Zhang O
et O
al O
. O
, O
2020 O
; O
Zhao O
et O
al O
. O
, O
2019 O
) O
. O
In O
feature O
- O
based O
explanations O
, O
Zhang O
et O
al O
. O
( O
2014 O
) O
generated O
textual O
sentences O
as O
explanations O
using O
templates O
such O
as O
‚Äú O
You O
might O
be O
interested O
in O
[ O
feature O
] O
, O
on O
which O
this O
product O
performs O
well O
. O
‚Äù O
In O
aspect O
- O
based O
explanations O
, O
Wang O
et O
al O
. O
( O
2010 O
) O
discovered O
latent O
ratings O
on O
each O
aspect O
, O
and O
selected O
sentences O
related O
to O
each O
aspect O
to O
help O
users O
better O
understand O
the O
opinions O
given O
a O
set O
of O
review897texts O
with O
the O
overall O
ratings O
. O
Zhao O
et O
al O
. O
( O
2019 O
) O
formulated O
a O
problem O
called O
personalized O
reason O
generation O
and O
generated O
a O
recommendation O
sentence O
given O
a O
song O
name O
, O
author O
, O
and O
user O
tag O
as O
input O
. O
The O
inputs O
of O
those O
studies O
were O
user O
vectors O
created O
from O
the O
user O
‚Äôs O
action O
history O
or O
limited O
aspects O
. O
However O
, O
our O
study O
deals O
with O
a O
wide O
range O
of O
natural O
language O
requests O
for O
a O
dialog O
system O
in O
the O
hotel O
booking O
domain O
. O
In O
the O
Ô¨Åeld O
of O
sentiment O
analysis O
, O
research O
that O
extracts O
evidence O
based O
on O
sentiment O
expressions O
has O
attracted O
attention O
( O
Chen O
et O
al O
. O
, O
2010 O
; O
Gui O
et O
al O
. O
, O
2016 O
; O
Kim O
and O
Klinger O
, O
2018 O
) O
. O
Chen O
et O
al O
. O
( O
2010 O
) O
extracted O
the O
cause O
of O
a O
target O
emotional O
expression O
based O
on O
a O
rule O
. O
Gui O
et O
al O
. O
( O
2016 O
) O
annotated O
an O
emotional O
expression O
and O
its O
cause O
. O
These O
studies O
aimed O
to O
gather O
useful O
information O
to O
extract O
emotional O
expressions O
and O
provide O
evidence O
simultaneously O
by O
examining O
the O
reputations O
for O
speciÔ¨Åc O
products O
. O
Although O
our O
study O
also O
aims O
to O
collect O
useful O
information O
, O
the O
requests O
are O
not O
limited O
to O
emotional O
expressions O
. O
In O
addition O
, O
we O
generate O
recommendation O
sentences O
. O
Our O
study O
can O
be O
viewed O
as O
a O
special O
application O
of O
argument O
mining O
in O
the O
domain O
of O
hotel O
review O
. O
Liu O
et O
al O
. O
( O
2017 O
) O
used O
manually O
annotated O
arguments O
of O
evidence O
- O
conclusion O
discourse O
relations O
in O
110 O
hotel O
reviews O
. O
The O
study O
showed O
the O
effectiveness O
of O
several O
combinations O
of O
argumentbased O
features O
. O
In O
Japanese O
, O
Murakami O
et O
al O
. O
( O
2009 O
) O
proposed O
a O
method O
to O
collect O
consents O
and O
dissents O
for O
queries O
that O
can O
be O
answered O
with O
Yes O
or O
No O
. O
As O
part O
of O
that O
, O
they O
extracted O
evidence O
using O
rules O
. O
Our O
dataset O
is O
useful O
as O
training O
data O
to O
extract O
evidence O
in O
argument O
mining O
. O
5 O
Conclusion O
We O
proposed O
a O
novel O
task O
of O
predicting O
an O
evidence O
to O
satisfy O
a O
request O
and O
generating O
a O
recommendation O
sentence O
. O
We O
built O
an O
Evidence O
- O
based O
Explanation O
dataset O
for O
the O
task O
. O
The O
experimental O
results O
demonstrated O
that O
the O
BERT O
model O
could O
Ô¨Ånd O
evidence O
sentences O
with O
respect O
to O
various O
vague O
requests O
and O
that O
the O
LSTM O
- O
based O
model O
could O
generate O
recommendation O
sentences O
. O
Future O
directions O
of O
this O
study O
include O
choosing O
the O
best O
evidence O
sentence O
from O
multiple O
candidate O
sentences O
for O
a O
vague O
request O
from O
a O
user O
and O
developing O
a O
concierge O
service O
that O
can O
recommend O
a O
hotel O
with O
evidence O
. O
Acknowledgments O
We O
would O
like O
to O
thank O
the O
anonymous O
reviewers O
for O
their O
valuable O
feedback O
. O
Abstract O
In O
this O
paper O
, O
we O
propose O
an O
effective O
deep O
learningframeworkformultilingualandcodemixed O
visual O
question O
answering O
. O
The O
proposed O
model O
is O
capable O
of O
predicting O
answers O
from O
the O
questions O
in O
Hindi O
, O
English O
or O
Codemixed O
( O
Hinglish O
: O
Hindi O
- O
English O
) O
languages O
. O
ThemajorityoftheexistingtechniquesonVisualQuestionAnswering(VQA)focusonEnglishquestionsonly O
. O
However O
, O
manyapplicationssuchasmedicalimaging O
, O
tourism O
, O
visual O
assistants O
require O
a O
multilinguality O
- O
enabled O
module O
for O
their O
widespread O
usages O
. O
As O
there O
is O
no O
available O
dataset O
in O
English O
- O
Hindi O
VQA O
, O
we O
firstly O
create O
Hindi O
and O
Code O
- O
mixed O
VQA O
datasetsbyexploitingthelinguisticproperties O
oftheselanguages O
. O
Weproposearobusttechniquecapableofhandlingthemultilingualand O
code O
- O
mixed O
question O
to O
provide O
the O
answer O
against O
the O
visual O
information O
( O
image O
) O
. O
To O
betterencodethemultilingualandcode O
- O
mixed O
questions O
, O
we O
introduce O
a O
hierarchy O
of O
shared O
layers O
. O
We O
control O
the O
behaviour O
of O
these O
shared O
layers O
by O
an O
attention O
- O
based O
soft O
layer O
sharing O
mechanism O
, O
which O
learns O
how O
shared O
layersareappliedindifferentwaysforthedifferent O
languages O
of O
the O
question O
. O
Further O
, O
our O
model O
uses O
bi O
- O
linear O
attention O
with O
a O
residual O
connectiontofusethelanguageandimagefeatures O
. O
We O
perform O
extensive O
evaluation O
and O
ablation O
studies O
for O
English O
, O
Hindi O
and O
Codemixed O
VQA O
. O
The O
evaluation O
shows O
that O
the O
proposedmultilingualmodelachievesstate O
- O
ofthe O
- O
artperformanceinallthesesettings O
. O
1 O
Introduction O
VisualQuestionAnswering(VQA)isachallengingproblemthatrequirescomplexreasoningover O
visualelementstoprovideanaccurateanswertoa O
naturallanguagequestion O
. O
AnefficientVQAsystemcanbeusedtobuildanArtificialIntelligence O
( O
AI)agentwhichtakesanaturallanguagequestion O
‚àóWorkcarriedoutduringtheinternshipatIITPatnaandpredictsthedecisionbyanalyzingthecomplex O
scene(s O
) O
. O
VQA O
requires O
language O
understanding O
, O
fine O
- O
grainedvisualprocessingandmultiplesteps O
of O
reasoning O
to O
produce O
the O
correct O
answer O
. O
As O
theexistingresearchonVQAaremainlyfocused O
on O
natural O
language O
questions O
written O
in O
English O
( O
Antol O
et O
al O
. O
, O
2015;Hu O
et O
al O
. O
,2017;Fukui O
et O
al O
. O
, O
2016;Andersonetal O
. O
, O
2018;Lietal O
. O
,2018;Xuand O
Saenko,2016;Shihetal O
. O
,2016),theirapplications O
areoftenlimited O
. O
QE O
: O
What O
color O
are O
the O
trees O
? O
QH O
: O
‡§™‡•á‡§°‡§º O
‡§ï‡•á O
‡§ï‡•ç‡§Ø‡§æ O
‡§∞‡§Ç‡§ó O
‡§π‡•à‡§Ç O
? O
( O
Trans O
: O
What O
color O
are O
the O
trees O
? O
) O
QCM O
: O
Trees O
ke O
kya O
color O
hain O
? O
( O
Trans O
: O
What O
color O
are O
the O
trees O
? O
) O
Answer(English O
): O
Green O
Answer(Hindi O
): O
‡§π‡§∞‡§æ O
QE O
: O
Where O
is O
this O
picture O
? O
QH O
: O
‡§Ø‡§π O
‡§§‡§∏‡•ç‡§µ‡•Ä‡§∞ O
‡§ï‡§π‡§æ‡§Ç O
‡§ï‡•Ä O
‡§π‡•à O
? O
( O
Trans O
: O
Where O
is O
this O
picture O
? O
) O
QCM O
: O
Yahpicture O
kahan O
ki O
hai O
? O
( O
Trans O
: O
Where O
is O
this O
picture O
? O
) O
Answer O
: O
Market O
Answer(Hindi O
): O
‡§¨‡§æ‡§ú‡§æ‡§∞ O
Figure O
1 O
: O
Examples O
of O
questions O
( O
English O
, O
Hindi O
and O
Code O
- O
mixed O
) O
with O
their O
corresponding O
images O
and O
answers O
Multilingual O
speakers O
often O
switch O
back O
and O
forthbetweentheirnativeandforeign(popular)languagestoexpressthemselves O
. O
Thisphenomenon O
ofembeddingthemorphemes O
, O
words O
, O
phrases O
, O
etc O
. O
, O
ofonelanguageintoanotherispopularlyknownas O
code O
- O
mixing O
( O
Myers O
- O
Scotton O
, O
1997,2002 O
) O
. O
Codemixing O
phenomena O
is O
common O
in O
chats O
, O
conversations O
, O
and O
messages O
posted O
over O
social O
media O
, O
especiallyinbilingual O
/ O
multilingualcountrieslike O
India O
, O
China O
, O
Singapore O
, O
andmostoftheotherEuropeancountries O
. O
Sectorsliketourism O
, O
food O
, O
education O
, O
marketing O
, O
etc O
. O
haverecentlystartedusing O
code O
- O
mixed O
languages O
in O
their O
advertisements O
to O
attract O
their O
consumer O
base O
. O
In O
order O
to O
build O
an O
AIagent O
whichcan O
serve O
multilingual O
end O
users,900aVQAsystemshouldbeputinplacethatwould O
belanguageagnosticandtailoredtodealwiththe O
code O
- O
mixed O
and O
multilingual O
environment O
. O
It O
is O
worthstudyingtheVQAsysteminthesesettings O
whichwouldbeimmenselyusefultoaverylarge O
numberofpopulationwhospeak O
/ O
writeinmorethan O
onelanguage O
. O
Arecentstudy O
( O
Parshadetal O
. O
, O
2016 O
) O
alsoshowsthepopularityofcode O
- O
mixedEnglishHindilanguageandthedynamicsoflanguageshift O
inIndia O
. O
Ourcurrentworkfocusesondevelopinga O
languageagnosticVQAsystemforHindi O
, O
English O
andcode O
- O
mixedEnglish O
- O
Hindilanguages O
. O
LetusconsidertheexamplesshowninFig O
1 O
. O
The O
majorityoftheVQAmodels O
( O
Andersonetal O
. O
, O
2018 O
; O
Lietal O
. O
,2018;Yuetal O
. O
,2018)arecapableenough O
to O
provide O
correct O
answers O
for O
English O
questions O
QE O
, O
butourevaluationshowsthatthesamemodel O
could O
not O
predict O
correct O
answers O
for O
Hindi O
QH O
andCode O
- O
mixedquestion O
QCM O
. O
Thequestions O
QH O
andQCMcorrespondtothesamequestion O
QE O
, O
but O
areformulatedintwodifferentlanguages O
. O
Inthis O
paper O
, O
weinvestigatetheissueofmultilingualand O
code O
- O
mixedVQA.Weassumethatthereareseveral O
techniquesavailableformonolingual(especially O
, O
English)VQAsuchthatastrongVQAmodelcan O
bebuilt O
. O
However O
, O
weareinterestedinbuildinga O
systemthatcananswerthequestionsfromdifferent O
languages(multilingual)andthelanguageformed O
bymixingupofmultiplelanguages(code O
- O
mixed O
) O
. O
Weshowthatinacross O
- O
lingualscenarioduetolanguagemismatch O
, O
applyingdirectlyalearnedsystem O
fromonelanguagetoanotherlanguageresultsin O
poorperformance O
. O
Thus O
, O
weproposeatechnique O
for O
multilingual O
and O
code O
- O
mixed O
VQA O
. O
Our O
proposedmethodmainlyconsistsofthreecomponents O
. O
The O
first O
component O
is O
the O
multilingual O
question O
encodingwhichtransformsagivenquestiontoits O
feature O
representation O
. O
This O
component O
handles O
themultilingualityandcode O
- O
mixinginquestions O
. O
Weusemultilingualembeddingcoupledwithahierarchy O
of O
shared O
layers O
to O
encode O
the O
questions O
. O
Todoso O
, O
weemployanattentionmechanismon O
thesharedlayerstolearnlanguagespecificquestion O
representation O
. O
Furthermore O
, O
we O
utilize O
the O
self O
- O
attentiontoobtainanimprovedquestionrepresentationbyconsideringtheotherwordsinthe O
question O
. O
Thesecondcomponent O
( O
imagefeatures O
) O
obtainstheeffectiveimagerepresentationfromobject O
level O
and O
pixel O
level O
features O
. O
The O
last O
componentis O
multimodalfusion O
whichisaccountable O
toencodethequestion O
- O
imagepairrepresentationbyensuingthatthelearnedrepresentationistightly O
coupledwithboththequestion(language)andimage(vision)feature O
. O
ItistobenotedthatdesigningaVQAsystemfor O
eachlanguageseparatelyiscomputationallyvery O
expensive O
( O
both O
time O
and O
cost O
) O
, O
especially O
when O
multiple O
languages O
are O
involved O
. O
Hence O
, O
an O
endto O
- O
end O
model O
that O
integrates O
multilinguality O
and O
code O
- O
mixinginitscomponentsisextremelyuseful O
. O
Wesummarizeourcontributionasfollows O
: O
1.We O
create O
linguistically O
- O
driven O
Hindi O
and O
English O
- O
Hindicode O
- O
mixedVQAdatasets O
. O
To O
thebestofourknowledge O
, O
thisistheveryfirst O
attempttowardsthisdirection O
. O
2.Weproposeaunifiedneuralmodelformultilingualandcode O
- O
mixedVQA O
, O
whichcanpredictanswerofamultilingualorcode O
- O
mixed O
question O
. O
3.Toeffectivelyansweraquestion O
, O
weenhance O
thevisionunderstandingbycombininglocal O
image O
grid O
and O
object O
- O
level O
visual O
features O
. O
We O
propose O
a O
simple O
, O
yet O
powerful O
mechanismbasedonsoft O
- O
sharingofsharedlayersto O
betterencodethemultilingualandcode O
- O
mixed O
questions O
. O
ThisbridgesthegapbetweenVQA O
andmultilinguality O
. O
4.Weperformextensiveevaluationandablation O
studies O
for O
English O
, O
Hindi O
and O
Code O
- O
mixed O
VQA.Theevaluationshowsthatourproposed O
multilingual O
model O
achieves O
state O
- O
of O
- O
the O
- O
art O
performanceinallthesesettings O
. O
2 O
Related O
Work O
Multilingual O
and O
Code O
- O
Mixing O
: O
Recently O
, O
researchers O
have O
started O
investigating O
methods O
for O
creating O
tools O
and O
resources O
for O
various O
Natural O
Language O
Processing O
( O
NLP O
) O
applications O
involvingmultilingual O
( O
GarciaandGamallo O
, O
2015;Gupta O
et O
al O
. O
,2019;Agerri O
et O
al O
. O
, O
2014 O
) O
and O
code O
- O
mixed O
languages O
( O
Gupta O
et O
al O
. O
, O
2018a;Bali O
et O
al O
. O
,2014 O
; O
Guptaetal O
. O
, O
2016;Rudraetal O
. O
, O
2016;Guptaetal O
. O
, O
2014 O
) O
. O
DevelopingaVQAsysteminacode O
- O
mixed O
scenariois O
, O
itself O
, O
verynovelinthesensethatthere O
hasnotbeenanypriorresearchtowardsthisdirection O
. O
VQA O
Datasets O
: O
Quite O
a O
few O
VQA O
datasets O
( O
Gaoetal O
. O
,2015;Antoletal O
. O
, O
2015;Goyaletal O
. O
, O
2017;Johnson O
et O
al O
. O
, O
2017;Shimizu O
et O
al O
. O
, O
2018 O
; O
Hasanetal O
. O
, O
2018;Wangetal O
. O
, O
2018)havebeen O
createdtoencouragemulti O
- O
disciplinaryresearchinvolvingNaturalLanguageProcessing(NLP)and901Computer O
Vision O
. O
In O
majority O
of O
these O
datasets O
, O
the O
images O
are O
taken O
from O
the O
large O
- O
scale O
image O
databaseMSCOCO O
( O
Linetal O
. O
,2014)orartificially O
constructed O
( O
Antoletal O
. O
, O
2015;Andreasetal O
. O
, O
2016 O
; O
Johnsonetal O
. O
, O
2017 O
) O
. O
Thereareafewdatasets O
( O
Gao O
etal O
. O
,2015;Shimizuetal O
. O
, O
2018)formultilingual O
VQA O
, O
but O
these O
are O
limited O
only O
to O
some O
chosen O
languages O
, O
andunlikeourdatasettheydonotoffer O
anycode O
- O
mixedchallenges O
. O
VQA O
Models O
: O
The O
popular O
frameworks O
for O
VQAintheliteraturearebuilttolearnthejointrepresentationofimageandquestionusingtheattentionmechanism O
( O
Kimetal O
. O
,2018;Luetal O
. O
,2016 O
; O
Yuetal O
. O
,2017;KafleandKanan O
, O
2017;Zhaoetal O
. O
, O
2017).Hu O
et O
al.(2018 O
) O
proposed O
a O
technique O
to O
separatelylearntheanswerembeddingwithbest O
parameterssuchthatthecorrectanswerhashigher O
likelihoodamongallpossibleanswers O
. O
Thereare O
someworks O
( O
Chaoetal O
. O
,2018;Liuetal O
. O
,2018;Wu O
etal O
. O
,2018)whichexploittheadversariallearning O
strategyinVQA.VQAhasalsobeenexploredin O
medicaldomains O
( O
Zhouetal O
. O
, O
2018;Guptaetal O
. O
, O
2021;Abachaetal O
. O
, O
2018;BenAbachaetal O
. O
, O
2019 O
) O
. O
Theselearnedrepresentationsarepassedtoamultilabelclassifierwhoselabelsarethemostfrequent O
answersinthedataset O
. O
Ouranalysis(c.f O
. O
Section O
5.5)revealsthatthesemodelsperformverypoorly O
inacross O
- O
lingualsetting O
. O
3MCVQADataset O
DatasetCreation O
: O
ThepopularVQAdatasetreleasedbyAntoletal O
. O
( O
2015)containsimages O
, O
with O
theircorrespondingquestions(inEnglish)andanswers O
( O
in O
English O
) O
. O
This O
is O
a O
challenging O
large O
scaledatasetfortheVQAtask O
. O
Tocreateacomparable O
version O
of O
this O
English O
VQA O
dataset O
in O
Hindi O
and O
code O
- O
mixed O
Hinglish O
, O
we O
introduce O
a O
newVQAdatasetnamed O
‚Äú O
Multilingualand O
CodemixedVisualQuestionAnswering O
‚Äù O
( O
MCVQA)which O
comprisesofquestionsinHindiandHinglish O
. O
Our O
dataset1 O
, O
inadditiontotheoriginalEnglishquestions O
, O
also O
presents O
the O
questions O
in O
Hindi O
and O
Hinglishlanguages O
. O
Thismakesour O
MCVQAdataset O
suitable O
for O
multilingual O
and O
code O
- O
mixed O
VQA O
tasks O
. O
Asampleofquestion O
- O
answerpairsandimagesfromourdatasetareshowninFig O
2 O
. O
Wedonotconstructtheanswerincode O
- O
mixed O
language O
because O
a O
recent O
study O
( O
Gupta O
et O
al O
. O
, O
2018b)hasshownthatcode O
- O
mixedsentencesand O
1Thedatasetcanbefoundhere O
: O
http://www.iitp.ac O
. O
in/~ai O
- O
nlp O
- O
ml O
/ O
resources.htmltheir O
corresponding O
English O
sentences O
share O
the O
samenouns(commonnouns O
, O
propernouns O
, O
spatiotemporal O
nouns O
) O
, O
adjectives O
, O
etc O
. O
For O
example O
, O
givenanEnglishanditscorrespondingcode O
- O
mixed O
question O
: O
QE O
: O
Where O
is O
the O
treein O
thispicture O
? O
QCM O
: O
Ispicturemetreekahan O
hai O
? O
Itcanbeobservedthatboth O
QEandQCMsharethe O
same O
noun O
{ O
picture O
, O
tree O
} O
. O
The O
majority O
of O
answersintheVQAv1.0datasetareoftype O
‚Äò O
yes O
/ O
no O
‚Äô O
, O
‚Äò O
numbers‚Äô,‚Äònouns‚Äô,‚Äòverbs‚Äôand‚Äòadjectives O
‚Äô O
. O
Therefore O
, O
wekeepthesameanswerinbothEnglishand O
Code O
- O
mixedVQAdataset O
. O
Wefollowthetechniquessimilarto O
Guptaetal O
. O
( O
2018b O
) O
for O
our O
code O
- O
mixed O
question O
generation O
, O
which O
takes O
a O
Hindi O
sentence O
as O
input O
and O
generates O
the O
corresponding O
Hinglish O
sentence O
as O
the O
output O
. O
We O
translate O
original O
English O
questions O
andanswersusingtheGoogleTranslate2thathas O
shownremarkableperformanceintranslatingshort O
sentences O
( O
Wuetal O
. O
,2016 O
) O
. O
Weusethisserviceas O
ouroriginalquestionsandanswersinEnglishare O
veryshort O
. O
Forthecode O
- O
mixedquestiongeneration O
, O
wefirstobtainthePart O
- O
of O
- O
Speech3(PoS)and O
NamedEntity4(NE)tagsofeachquestion O
. O
Thereafter O
, O
wereplacetheHindiwordshavingthePoS O
tags(commonnoun O
, O
propernoun O
, O
spatio O
- O
temporal O
noun O
, O
adjective)withtheirbestlexicaltranslation O
. O
SamestrategyisalsofollowedforthewordshavingtheNEtagsas O
LOCATION O
andORGANIZATION O
. O
The O
remaining O
Hindi O
words O
are O
replaced O
withtheirRomantransliteration O
. O
Inordertoobtain O
thebestlexicaltranslation O
, O
wefollowtheiterative O
disambiguationalgorithm O
( O
MonzandDorr O
, O
2005 O
) O
. O
Wegeneratethelexicaltranslationbytrainingthe O
StatisticalMachineTranslation(SMT)modelon O
thepubliclyavailableEnglish O
- O
Hindi(EN O
- O
HI)parallel O
corpus O
( O
Bojar O
et O
al O
. O
, O
2014 O
) O
. O
Please O
refer O
to O
theAppendix O
forthecomparisonwithotherVQA O
datasets O
. O
DatasetAnalysis O
: O
TheMCVQAdatasetconsistsof O
248,349trainingquestionsand O
121,512validation O
questionsforrealimagesinHindiandCode O
- O
mixed O
. O
ForeachHindiquestion O
, O
wealsoprovideits O
10correspondinganswersinHindi O
. O
Inordertoanalyze O
thecomplexityofthegeneratedcode O
- O
mixedquestions O
, O
wecomputetheCode O
- O
mixingIndex(CMI O
) O
( O
Gamb√§ckandDas O
, O
2014)andComplexityFactor O
2https://cloud.google.com/translate O
3https://bit.ly/2rpNBJR O
4https://bit.ly/2Qljan59023200 O
201 O
202 O
203 O
204 O
205 O
206 O
207 O
208 O
209 O
210 O
211 O
212 O
213 O
214 O
215 O
216 O
217 O
218 O
219 O
220 O
221 O
222 O
223 O
224 O
225 O
226 O
227 O
228 O
229 O
230 O
231 O
232 O
233 O
234 O
235 O
236 O
237 O
238 O
239 O
240 O
241 O
242 O
243 O
244 O
245 O
246 O
247 O
248 O
249250 O
251 O
252 O
253 O
254 O
255 O
256 O
257 O
258 O
259 O
260 O
261 O
262 O
263 O
264 O
265 O
266 O
267 O
268 O
269 O
270 O
271 O
272 O
273 O
274 O
275 O
276 O
277 O
278 O
279 O
280 O
281 O
282 O
283 O
284 O
285 O
286 O
287 O
288 O
289 O
290 O
291 O
292 O
293 O
294 O
295 O
296 O
297 O
298 O
299NAACL¬≠HLT O
2019 O
Submission O
* O
* O
* O
. O
ConÔ¨Ådential O
Review O
Copy O
. O
DO O
NOT O
DISTRIBUTE O
. O
Metrics O
Manually O
AnnotatedAutomatically O
Generated O
Training O
Validation O
CMI O
Score O
0.2946 O
0.3223 O
0.3228 O
CF2 O
14.765 O
16.094 O
16.114 O
CF3 O
13.122 O
14.0708 O
14.096 O
Table O
1 O
: O
Performance O
comparison O
between O
manually O
annotated O
code O
- O
mixed O
questions O
and O
automatically O
generated O
code O
- O
mixed O
questions O
. O
Language O
BLEU-1 O
BLEU-2 O
BLEU-3 O
ROUGE-1 O
ROUGE-2 O
ROUGE O
- O
L O
TER O
Hindi O
92.18 O
84.19 O
80.22 O
93.14 O
83.16 O
92.20 O
9.63 O
Table O
2 O
: O
Performance O
comparison O
between O
manually O
annotated O
Hindi O
questions O
and O
automatically O
generated O
Hindi O
questions O
. O
Dataset O
Images O
used O
Created O
by O
Multilingual O
Code O
- O
Mixed O
DAQUAR O
( O
Malinowski O
and O
Fritz O
, O
2014 O
) O
NYU O
Depth O
V2In O
- O
house O
participants O
, O
Automatically O
generated7 O
7 O
FM O
- O
IQA O
( O
Gao O
et O
al O
. O
, O
2015 O
) O
MSCOCO O
Crowd O
workers O
( O
Baidu O
) O
" O
7 O
VQA O
v1.0 O
( O
Antol O
et O
al O
. O
, O
2015 O
) O
MSCOCO O
Crowd O
workers O
( O
AMT O
) O
7 O
7 O
Visual7W O
( O
Zhu O
et O
al O
. O
, O
2016 O
) O
MSCOCO O
Crowd O
workers O
( O
AMT O
) O
7 O
7 O
CLEVR O
( O
Johnson O
et O
al O
. O
, O
2017 O
) O
Synthetic O
Shapes O
Automatically O
generated O
7 O
7 O
KB O
- O
VQA O
( O
Wang O
et O
al O
. O
, O
2017 O
) O
MSCOCO O
In O
- O
house O
participants O
7 O
7 O
FVQA O
( O
Wang O
et O
al O
. O
, O
2018 O
) O
MSCOCO O
In O
- O
house O
participants O
7 O
7 O
Japanese O
VQA O
( O
Shimizu O
et O
al O
. O
, O
2018 O
) O
MSCOCO O
Crowd O
workers O
( O
Yahoo O
) O
" O
7 O
MCVQA O
( O
Ours O
) O
MSCOCO O
Automatically O
generated O
" O
" O
Table O
3 O
: O
Comparison O
of O
VQA O
datasets O
with O
our O
MCVQA O
dataset O
QE O
: O
Is O
this O
a O
salad O
? O
QH:‘π‡§æ O
‡§Ø‡§π O
‡§è‡§ï O
‡§∏‡§≤‡§æ‡§¶ O
‡§π O
‡•à O
? O
( O
Trans O
: O
Is O
this O
a O
salad O
? O
) O
QCM O
: O
Kya O
yah O
ek O
salad O
hai O
? O
( O
Trans O
: O
Is O
this O
a O
salad O
? O
) O
Answer(E O
): O
no O
Answer(H O
): O
‡§®‡§π“∞‡§Ç O
QE O
: O
What O
time O
is O
it O
? O
QH:‘π‡§æ O
‡§∏‡§Æ‡§Ø O
‡§π O
‡•Å‡§Ü O
‡§π O
‡•à O
? O
( O
Trans O
: O
What O
time O
is O
it O
? O
) O
QCM O
: O
Kya O
time O
hua O
hai O
? O
( O
Trans O
: O
What O
time O
is O
it O
? O
) O
Answer(E O
): O
1:08 O
Answer(H O
): O
1:08 O
QE O
: O
What O
is O
in O
the O
window O
? O
QH O
: O
‡§ø‡§ñ‡•ú‡§ï“¥ O
‡§Æ”í O
‘π‡§æ O
‡§π O
‡•à O
? O
? O
( O
Trans O
: O
What O
is O
in O
the O
window O
? O
) O
QCM O
: O
Window O
me O
kya O
hai O
? O
( O
Trans O
: O
What O
is O
in O
the O
window O
? O
) O
Answer(E O
): O
cat O
Answer(H O
): O
—ü‡§¨’©‡•Ä O
QE O
: O
What O
sport O
is O
this O
? O
QH O
: O
‡§Ø‡§π O
‡§ï‡•å‡§® O
‡§∏‡§æ O
‡§ñ O
‡•á‡§≤ O
‡§π‡•à O
? O
( O
Trans O
: O
What O
sport O
is O
this O
? O
) O
QCM O
: O
Yah O
kaun O
sa O
sport O
hai O
? O
( O
Trans O
: O
What O
sport O
is O
this O
? O
) O
Answer(E O
): O
baseball O
Answer(H O
): O
‡§¨‡•á‡§∏‡§¨‡•â‡§≤ O
QE O
: O
Where O
is O
the O
tree O
? O
QH O
: O
‡§™‡•á‡•ú O
‡§ï‡§π‡§æ O
‡§Ç‡§π‡•à O
? O
( O
Trans O
: O
Where O
is O
the O
tree O
? O
) O
QCM O
: O
Tree O
kahan O
hai O
? O
( O
Trans O
: O
Where O
is O
the O
tree O
? O
) O
Answer(E O
): O
wall O
Answer(H O
): O
‡§¶“∞‡§µ‡§æ‡§∞ O
where O
S O
is O
the O
number O
of O
code O
- O
switches O
and O
W O
is O
the O
number O
of O
words O
in O
the O
sentences O
or O
block O
of O
text O
. O
MF O
= O
W‚Ä≤ maxfwg O
W‚Ä≤ O
, O
if O
W O
‚Äô O
> O
0 O
MF O
= O
0 O
, O
if O
W O
‚Äô O
= O
0 O
where O
W O
‚Äô O
is O
the O
number O
of O
words O
in O
distinct O
languages O
, O
i.e. O
, O
the O
number O
of O
words O
except O
the O
undefined O
ones O
, O
max{w O
} O
is O
the O
maximum O
number O
of O
words O
belonging O
to O
the O
most O
frequent O
language O
in O
the O
sentence O
. O
LF O
= O
W O
Nwhere O
W O
is O
the O
number O
of O
words O
and O
N O
is O
the O
number O
of O
distinct O
languages O
in O
the O
sentence O
. O
A O
Appendices O
Thedetailedcomparisonofautomaticallycreated O
andmanuallycode O
- O
mixedquestionsw.r.ttheCodemixingIndex(CMI)score O
, O
ComplexityFactor(CF2 O
and O
CF3 O
) O
are O
shown O
in O
Table O
4 O
. O
We O
also O
show O
thecomparisonofourMCVQAdatasetwithother O
VQAdatasetsinTable O
5 O
. O
TheanalysisofMCVQA O
datasetareillustratedinFig O
5and6.912Figure5 O
: O
Analysisofquestiondistributionw.r.tthequestionlengthbetweenVQAv1.0Englishandcode O
- O
mixed O
, O
trainandtestdataset O
. O
( O
a O
) O
  O
( O
b O
) O
  O
( O
c O
) O
Figure O
6 O
: O
Analysis O
of O
code O
- O
mixed O
VQA O
dataset O
on O
various O
code O
- O
mixing O
metrics O
: O
( O
a),(b)and(c)show O
the O
distribution O
of O
code O
- O
mixed O
questions O
from O
training O
and O
test O
set O
w.r.t O
the O
Code O
- O
mixing O
Index O
( O
CMI O
) O
score O
, O
Complexity O
Factor(CF2andCF3),respectively O
. O
Metrics O
ManuallyAnnotatedAutomaticallyGenerated O
Training O
Testing O
CMIScore O
0.2946 O
0.3223 O
0.3228 O
CF2 O
14.765 O
16.094 O
16.114 O
CF3 O
13.122 O
14.0708 O
14.096 O
Table4 O
: O
Comparisonbetweenmanuallyannotatedcode O
- O
mixedquestionsandautomaticallygeneratedcode O
- O
mixed O
questionsw.r.ttheCMIscore O
, O
CF2,andCF3 O
. O
Dataset O
Images O
used O
Created O
by O
Multilingual O
Code O
- O
Mixed O
DAQUAR O
( O
MalinowskiandFritz O
, O
2014)NYUDepthV2In O
- O
houseparticipants O
, O
Automaticallygenerated O
 O
FM O
- O
IQA(Gaoetal O
. O
,2015 O
) O
MSCOCO O
Crowdworkers(Baidu O
) O
/enc-34 O
 O
VQAv1.0 O
( O
Antoletal O
. O
, O
2015 O
) O
MSCOCO O
Crowdworkers(AMT O
) O
 O
 O
Visual7W O
( O
Zhuetal O
. O
,2016 O
) O
MSCOCO O
Crowdworkers(AMT O
) O
 O
 O
CLEVR(Johnsonetal O
. O
, O
2017)SyntheticShapes O
Automaticallygenerated O
 O
 O
KB O
- O
VQA O
( O
Wangetal O
. O
, O
2017 O
) O
MSCOCO O
In O
- O
houseparticipants O
 O
 O
FVQA(Wangetal O
. O
, O
2018 O
) O
MSCOCO O
In O
- O
houseparticipants O
 O
 O
JapaneseVQA O
( O
Shimizuetal O
. O
, O
2018)MSCOCO O
Crowdworkers(Yahoo O
) O
/enc-34 O
 O
MCVQA(Ours O
) O
MSCOCO O
Automatically O
generated O
/enc-34 O
/enc-34 O
Table5 O
: O
ComparisonofVQAdatasetswithour O
MCVQAdataset O
. O
Theimagesusedare O
: O
MSCOCO O
( O
Linetal O
. O
,2014 O
) O
andNYUDepthv2 O
( O
NathanSilbermanandFergus O
, O
2012 O
) O
.913Proceedings O
of O
the O
1st O
Conference O
of O
the O
Asia O
- O
PaciÔ¨Åc O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
10th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
914‚Äì924 O
December O
4 O
- O
7 O
, O
2020 O
. O
¬© O
2020 O
Association O
for O
Computational O
Linguistics O
Toxic O
Language O
Detection O
in O
Social O
Media O
for O
Brazilian O
Portuguese O
: O
New O
Dataset O
and O
Multilingual O
Analysis O
JoÀúao O
A. O
Leite O
, O
Diego O
F. O
Silva O
Departamento O
de O
Computac O
¬∏ O
Àúao O
Federal O
University O
of O
S O
Àúao O
Carlos O
SÀúao O
Carlos O
, O
Brazil O
joaoaugustobr@hotmail.com O
diegofs@ufscar.brKalina O
Bontcheva O
, O
Carolina O
Scarton O
Department O
of O
Computer O
Science O
University O
of O
ShefÔ¨Åeld O
ShefÔ¨Åeld O
, O
UK O
k.bontcheva@sheffield.ac.uk O
c.scarton@sheffield.ac.uk O
Abstract O
Hate O
speech O
and O
toxic O
comments O
are O
a O
common O
concern O
of O
social O
media O
platform O
users O
. O
Although O
these O
comments O
are O
, O
fortunately O
, O
the O
minority O
in O
these O
platforms O
, O
they O
are O
still O
capable O
of O
causing O
harm O
. O
Therefore O
, O
identifying O
these O
comments O
is O
an O
important O
task O
for O
studying O
and O
preventing O
the O
proliferation O
of O
toxicity O
in O
social O
media O
. O
Previous O
work O
in O
automatically O
detecting O
toxic O
comments O
focus O
mainly O
in O
English O
, O
with O
very O
few O
work O
in O
languages O
like O
Brazilian O
Portuguese O
. O
In O
this O
paper O
, O
we O
propose O
a O
new O
large O
- O
scale O
dataset O
for O
Brazilian O
Portuguese O
with O
tweets O
annotated O
as O
either O
toxic O
or O
non O
- O
toxic O
or O
in O
different O
types O
of O
toxicity O
. O
We O
present O
our O
dataset O
collection O
and O
annotation O
process O
, O
where O
we O
aimed O
to O
select O
candidates O
covering O
multiple O
demographic O
groups O
. O
State O
- O
of O
- O
the O
- O
art O
BERT O
models O
were O
able O
to O
achieve O
76 O
% O
macro- O
F1score O
using O
monolingual O
data O
in O
the O
binary O
case O
. O
We O
also O
show O
that O
large O
- O
scale O
monolingual O
data O
is O
still O
needed O
to O
create O
more O
accurate O
models O
, O
despite O
recent O
advances O
in O
multilingual O
approaches O
. O
An O
error O
analysis O
and O
experiments O
with O
multi O
- O
label O
classiÔ¨Åcation O
show O
the O
difÔ¨Åculty O
of O
classifying O
certain O
types O
of O
toxic O
comments O
that O
appear O
less O
frequently O
in O
our O
data O
and O
highlights O
the O
need O
to O
develop O
models O
that O
are O
aware O
of O
different O
categories O
of O
toxicity O
. O
1 O
Introduction O
Social O
media O
can O
be O
a O
powerful O
tool O
that O
enables O
virtual O
human O
interactions O
, O
connecting O
people O
and O
enhancing O
businesses O
‚Äô O
presence O
. O
On O
the O
other O
hand O
, O
since O
users O
feel O
somehow O
protected O
under O
their O
virtual O
identities O
, O
social O
media O
has O
also O
become O
a O
platform O
for O
hate O
speech O
and O
use O
of O
toxic O
language O
. O
Although O
hate O
speech O
is O
a O
crime O
in O
most O
countries O
, O
identifying O
cases O
in O
social O
media O
is O
not O
an O
easy O
task O
, O
given O
the O
massive O
amounts O
of O
data O
posted O
every O
day O
. O
Therefore O
, O
automatic O
approaches O
for O
detecting O
online O
hate O
speech O
have O
received O
signiÔ¨Åcant O
attentionin O
recent O
years O
( O
Waseem O
and O
Hovy O
, O
2016 O
; O
Davidson O
et O
al O
. O
, O
2017 O
; O
Zampieri O
et O
al O
. O
, O
2019b O
) O
. O
In O
this O
paper O
, O
we O
focus O
on O
the O
analysis O
and O
automatic O
detection O
oftoxic O
comments O
. O
Our O
deÔ¨Ånition O
of O
toxic O
is O
similar O
to O
the O
one O
used O
by O
the O
Jigsaw O
competition,1 O
where O
comments O
containing O
insults O
and O
obscene O
language O
are O
also O
considered O
, O
besides O
hate O
speech.2 O
Systems O
capable O
of O
automatically O
identifying O
toxic O
comments O
are O
useful O
for O
platform O
‚Äôs O
moderators O
and O
to O
select O
content O
for O
speciÔ¨Åc O
users O
( O
e.g. O
children O
) O
. O
Nevertheless O
, O
there O
are O
multiple O
challenges O
speciÔ¨Åc O
to O
process O
toxic O
comments O
automatically O
, O
e.g. O
( O
i O
) O
toxic O
language O
may O
not O
be O
explicit O
, O
i.e. O
may O
not O
contain O
explicit O
toxic O
terms O
; O
( O
ii O
) O
there O
is O
a O
large O
spectrum O
of O
types O
of O
toxicity O
( O
e.g. O
sexism O
, O
racism O
, O
insult O
) O
; O
( O
iii O
) O
toxic O
comments O
correspond O
to O
a O
minority O
of O
comments O
, O
which O
is O
fortunate O
, O
but O
means O
that O
automatic O
data O
- O
driven O
approaches O
need O
to O
deal O
with O
highly O
unbalanced O
data O
. O
Although O
there O
is O
some O
work O
on O
this O
topic O
for O
other O
languages O
‚Äì O
e.g. O
Arabic O
( O
Mubarak O
et O
al O
. O
, O
2017 O
) O
and O
German O
( O
Wiegand O
et O
al O
. O
, O
2018 O
) O
‚Äì O
, O
most O
of O
the O
resources O
and O
studies O
available O
are O
for O
English O
( O
Davidson O
et O
al O
. O
, O
2017 O
; O
Wulczyn O
et O
al O
. O
, O
2017 O
; O
Founta O
et O
al O
. O
, O
2018 O
; O
Mandl O
et O
al O
. O
, O
2019 O
; O
Zampieri O
et O
al O
. O
, O
2019b).3For O
Portuguese O
, O
only O
two O
previous O
works O
are O
available O
( O
Fortuna O
et O
al O
. O
, O
2019 O
; O
de O
Pelle O
and O
Moreira O
, O
2017 O
) O
and O
their O
datasets O
are O
considerably O
small O
, O
mainly O
when O
compared O
to O
resources O
available O
for O
English O
. O
We O
present O
ToLD O
- O
Br O
( O
Toxic O
Language O
Dataset O
for O
Brazilian O
Portuguese O
) O
, O
a O
new O
dataset O
with O
Twitter O
posts O
in O
the O
Brazilian O
Portuguese O
language.4 O
1https://www.kaggle.com/c/ O
jigsaw O
- O
toxic O
- O
comment O
- O
classification O
- O
challenge/ O
overview O
2This O
is O
also O
similar O
to O
the O
usage O
of O
offensive O
comments O
in O
OffensEval O
( O
Zampieri O
et O
al O
. O
, O
2019b O
, O
2020 O
) O
. O
3A O
large O
list O
of O
resources O
is O
available O
at O
http:// O
hatespeechdata.com O
. O
4It O
is O
important O
to O
distinguish O
the O
language O
variant O
, O
since914A O
total O
of O
21 O
K O
tweets O
were O
manually O
annotated O
into O
seven O
categories O
: O
non O
- O
toxic O
, O
LGBTQ+phobia O
, O
obscene O
, O
insult O
, O
racism O
, O
misogyny O
andxenophobia O
. O
Each O
tweet O
has O
three O
annotations O
that O
were O
made O
by O
volunteers O
from O
a O
university O
in O
Brazil O
. O
V O
olunteers O
were O
selected O
taking O
into O
account O
demographic O
information O
, O
aiming O
to O
create O
a O
dataset O
as O
balanced O
as O
possible O
in O
regarding O
to O
demographic O
group O
biases O
. O
This O
is O
then O
the O
largest O
dataset O
available O
for O
toxic O
data O
analysis O
in O
social O
media O
for O
the O
Portuguese O
language O
and O
the O
Ô¨Årst O
dataset O
with O
demographic O
information O
about O
annotators.5 O
We O
experiment O
with O
Brazilian O
Portuguese O
( O
Souza O
et O
al O
. O
, O
2019 O
) O
and O
Multilingual O
( O
Wolf O
et O
al O
. O
, O
2019 O
) O
BERT O
models O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
for O
the O
binary O
task O
of O
automatically O
classifying O
toxic O
comments O
, O
since O
similar O
models O
achieve O
state O
- O
of O
- O
the O
- O
art O
results O
for O
the O
same O
task O
in O
other O
languages O
( O
Zampieri O
et O
al O
. O
, O
2019b O
) O
. O
Models O
Ô¨Åne O
- O
tuned O
on O
monolingual O
data O
achieve O
up O
to O
76 O
% O
of O
macro O
- O
F1 O
, O
improving O
3points O
over O
a O
baseline O
. O
Besides O
, O
BERT O
- O
based O
approaches O
with O
multilingual O
pre O
- O
trained O
models O
enable O
transfer O
learning O
and O
zero O
- O
shot O
learning O
. O
The O
OffensEval O
2019 O
OLID O
dataset O
( O
Zampieri O
et O
al O
. O
, O
2019a O
) O
is O
then O
used O
to O
experiment O
with O
( O
i O
) O
transfer O
- O
learning O
: O
where O
both O
OLID O
and O
ToLD O
- O
Br O
are O
used O
to O
Ô¨Åne O
- O
tune O
BERT O
; O
and O
, O
( O
ii O
) O
zero O
- O
shot O
learning O
: O
where O
BERT O
is O
Ô¨Åne O
- O
tuned O
using O
only O
OLID O
. O
Results O
highlight O
the O
importance O
of O
language O
- O
speciÔ¨Åc O
datasets O
, O
since O
transfer O
learning O
does O
not O
improve O
over O
monolingual O
models O
and O
zero O
- O
shot O
learning O
achieves O
only O
a O
macro- O
F1of56 O
% O
. O
An O
error O
analysis O
is O
performed O
using O
our O
best O
model O
, O
where O
the O
worst O
- O
case O
scenario O
, O
i.e. O
, O
classifying O
toxic O
comments O
as O
non O
- O
toxic O
, O
is O
further O
investigated O
, O
taking O
into O
account O
the O
Ô¨Åne O
- O
grained O
categories O
. O
Results O
show O
that O
categories O
with O
fewer O
examples O
in O
the O
dataset O
( O
racism O
andxenophobia O
) O
are O
more O
likely O
to O
be O
mislabelled O
than O
other O
classes O
, O
with O
the O
best O
performance O
being O
achieved O
by O
majority O
classes O
( O
insult O
andobscene O
) O
. O
We O
also O
analyse O
theamount O
of O
data O
needed O
in O
order O
to O
achieve O
the O
best O
performance O
in O
binary O
classiÔ¨Åcation O
. O
Models O
trained O
with O
few O
examples O
are O
only O
accurate O
in O
predicting O
the O
majority O
class O
( O
non O
- O
toxic O
) O
. O
As O
the O
number O
of O
instances O
grow O
, O
the O
performance O
on O
the O
minority O
class O
( O
toxic O
) O
improves O
signiÔ¨Åcantly O
. O
there O
are O
multiple O
differences O
between O
Brazilian O
Portuguese O
lexicon O
and O
other O
variants O
of O
Portuguese O
. O
5ToLD O
- O
Br O
is O
available O
at O
: O
https://github.com/ O
JAugusto97 O
/ O
ToLD O
- O
BrFinally O
, O
we O
experiment O
with O
multi O
- O
label O
classiÔ¨Åcation O
, O
where O
each O
different O
type O
of O
toxicity O
is O
automatically O
classiÔ¨Åed O
. O
This O
is O
a O
considerably O
harder O
problem O
than O
binary O
classiÔ¨Åcation O
, O
where O
BERTbased O
models O
do O
not O
outperform O
the O
baseline O
. O
Section O
2 O
presents O
an O
overview O
of O
relevant O
previous O
work O
. O
Section O
3 O
shows O
details O
about O
the O
ToLDBr O
dataset O
. O
Material O
and O
methods O
are O
presented O
in O
Section O
4 O
, O
whilst O
results O
are O
discussed O
in O
Section O
5 O
. O
Finally O
, O
Section O
6 O
shows O
a O
Ô¨Ånal O
discussion O
and O
future O
work O
. O
2 O
Related O
Work O
Although O
multiple O
researchers O
have O
addressed O
the O
topic O
of O
hate O
speech O
( O
e.g. O
Waseem O
and O
Hovy O
( O
2016 O
) O
, O
Chung O
et O
al O
. O
( O
2019 O
) O
, O
Basile O
et O
al O
. O
( O
2019 O
) O
) O
, O
we O
focus O
the O
literature O
review O
on O
previous O
work O
related O
to O
toxic O
comments O
detection O
, O
the O
topic O
of O
our O
paper O
. O
Due O
to O
space O
constraints O
, O
we O
only O
describe O
papers O
that O
create O
and O
use O
Twitter O
- O
based O
datasets O
and/or O
focus O
on O
the O
Brazilian O
Portuguese O
language O
. O
English O
Davidson O
et O
al O
. O
( O
2017 O
) O
present O
a O
dataset O
with O
around O
25 O
K O
tweets O
annotated O
by O
crowdworkers O
as O
containing O
hate O
, O
offensive O
language O
, O
orneither O
. O
They O
build O
a O
feature O
- O
based O
classiÔ¨Åer O
with O
TF O
- O
IDF O
transformation O
over O
n O
- O
grams O
, O
part O
- O
ofspeech O
information O
, O
sentiment O
analysis O
, O
network O
information O
( O
e.g. O
, O
number O
of O
replies O
) O
, O
among O
other O
features O
. O
Their O
best O
model O
, O
trained O
using O
logistic O
regression O
, O
achieves O
a O
macro- O
F1of90 O
. O
Founta O
et O
al O
. O
( O
2018 O
) O
also O
rely O
on O
crowd O
- O
workers O
to O
annotate O
80 O
K O
tweets O
into O
eight O
categories O
: O
offensive O
, O
abusive O
, O
hateful O
speech O
, O
aggressive O
, O
cyberbullying O
, O
spam O
, O
andnormal O
. O
They O
perform O
an O
exploratory O
approach O
to O
identify O
the O
categories O
that O
cause O
most O
confusion O
to O
crowd O
- O
workers O
. O
Their O
Ô¨Ånal O
, O
large O
- O
scale O
annotation O
is O
done O
using O
four O
categories O
: O
abusive O
, O
hateful O
, O
normal O
, O
orspam O
. O
OffensEval O
is O
a O
series O
of O
shared O
tasks O
focusing O
on O
offensive O
comments O
detection O
( O
Zampieri O
et O
al O
. O
, O
2019b O
, O
2020 O
) O
. O
The O
OLID O
dataset O
( O
used O
in O
the O
2019 O
edition O
) O
has O
around O
14 O
K O
tweets O
in O
English O
manually O
annotated O
as O
offensive O
ornonoffensive O
. O
The O
best O
model O
for O
the O
relevant O
task O
A O
( O
offensive O
versus O
non O
- O
offensive O
) O
uses O
a O
BERT O
- O
based O
classiÔ¨Åer O
and O
achieves O
82.9of O
macro O
- O
F1 O
. O
German O
A O
shared O
task O
( O
organized O
as O
part O
of O
GermEval O
2018 O
) O
aimed O
to O
classify O
tweets O
in O
German O
categorized O
into O
offensive O
ornon O
- O
offensive O
( O
Wiegand O
et O
al O
. O
, O
2018 O
) O
. O
They O
make O
available O
a O
manually O
annotated O
dataset O
with O
approximately O
8.5 O
K O
tweets.915The O
best O
system O
achieved O
76.77ofF1 O
- O
score O
and O
was O
a O
feature O
- O
based O
ensemble O
approach O
. O
Arabic O
Mubarak O
et O
al O
. O
( O
2017 O
) O
present O
a O
dataset O
with O
1.1 O
K O
manually O
annotated O
tweets O
into O
obscene O
, O
offensive O
, O
orclean O
. O
They O
experiment O
with O
lexicalbased O
approaches O
that O
achieve O
a O
maximum O
of O
60 O
F1 O
- O
score O
. O
Mulki O
et O
al O
. O
( O
2019 O
) O
create O
a O
dataset O
with O
tweets O
in O
the O
Levantine O
dialect O
of O
Arabic O
manually O
annotated O
into O
normal O
, O
abusive O
, O
orhate O
( O
with O
approximately O
5 O
K O
tweets O
) O
. O
The O
authors O
use O
featurebased O
approaches O
to O
induce O
models O
for O
ternary O
and O
binary O
scenarios O
, O
with O
best O
systems O
achieving O
74.4 O
and89.6ofF1 O
- O
score O
, O
respectively O
. O
Spanish O
Carmona O
et O
al O
. O
( O
2018 O
) O
present O
a O
shared O
task O
aiming O
to O
detect O
aggressive O
tweets O
in O
Mexican O
Spanish O
. O
They O
manually O
annotate O
11 O
K O
tweets O
into O
aggressive O
ornon O
- O
aggressive O
. O
The O
best O
system O
is O
a O
feature O
- O
based O
approach O
with O
macro- O
F1of62 O
. O
Hindi O
Mathur O
et O
al O
. O
( O
2018 O
) O
present O
a O
dataset O
of O
around O
3.6 O
K O
tweets O
in O
Hinglish O
( O
spoken O
Hindi O
written O
using O
the O
Roman O
script O
) O
. O
The O
dataset O
was O
annotated O
into O
three O
classes O
not O
offensive O
, O
abusive O
and O
hate O
- O
inducing O
by O
ten O
NLP O
researchers O
. O
A O
Convolutional O
Neural O
Network O
( O
CNN O
) O
architecture O
with O
transfer O
learning O
is O
used O
, O
where O
the O
model O
is O
trained O
with O
both O
Hinglish O
and O
English O
data O
( O
from O
( O
Davidson O
et O
al O
. O
, O
2017 O
) O
) O
, O
achieving O
71.4 O
% O
ofF1 O
- O
score O
. O
Portuguese O
de O
Pelle O
and O
Moreira O
( O
2017 O
) O
make O
available O
a O
dataset O
with O
1,250comments O
, O
extracted O
from O
comment O
sessions O
of O
g1.globo.com O
website O
, O
and O
annotated O
them O
into O
categories O
of O
offensive O
or O
non O
- O
offensive O
. O
The O
offensive O
class O
was O
also O
subdivided O
into O
racism O
, O
sexism O
, O
LGBTQ+phobia O
, O
xenophobia O
, O
religious O
in O
- O
tolerance O
, O
orcursing O
. O
They O
experiment O
with O
binary O
classiÔ¨Åcation O
, O
using O
n O
- O
grams O
as O
features O
to O
SVM O
and O
NaiveBayes O
models O
. O
Best O
results O
are O
achieved O
with O
SVM O
reaching O
a O
weighted O
F1 O
score O
between O
77and82 O
, O
depending O
on O
different O
label O
interpretations O
. O
Fortuna O
et O
al O
. O
( O
2019 O
) O
describe O
a O
dataset O
with O
5,668tweets O
classiÔ¨Åed O
as O
hate O
vs. O
non O
- O
hate O
, O
with O
the O
hate O
class O
further O
classiÔ¨Åed O
following O
a O
Ô¨Åne O
- O
grained O
hierarchy O
. O
Experiments O
with O
binary O
classiÔ¨Åcation O
show O
a O
F1score O
of O
78using O
an O
LSTM O
- O
based O
architecture O
. O
Multilingual O
HASOC O
was O
a O
shared O
task O
aiming O
to O
classify O
hate O
speech O
and O
offensive O
comments O
in O
English O
, O
German O
, O
and O
Hindi O
( O
Mandl O
et O
al O
. O
, O
2019 O
) O
. O
Their O
dataset O
contains O
around O
7 O
K O
tweets O
and O
Facebook O
posts O
manually O
annotated O
. O
Sub O
- O
task O
A O
sep O
- O
arates O
posts O
into O
hate O
speech O
oroffensive O
versus O
neither O
; O
and O
, O
sub O
- O
task O
B O
separates O
posts O
containinghate O
speech O
oroffence O
into O
three O
categories O
: O
hate O
speech O
, O
offensive O
orprofane O
. O
Best O
performing O
systems O
in O
all O
languages O
used O
deep O
learning O
approaches O
. O
For O
OffensEval O
2020 O
( O
Zampieri O
et O
al O
. O
, O
2020 O
) O
, O
a O
more O
extensive O
training O
data O
is O
available O
for O
English O
( O
over O
9 O
M O
tweets O
) O
, O
although O
the O
annotation O
was O
made O
semi O
- O
automatically O
. O
Arabic O
, O
Danish O
, O
Greek O
, O
and O
Turkish O
datasets O
are O
also O
available O
with O
manually O
annotated O
labels O
. O
For O
all O
languages O
, O
best O
models O
are O
achieved O
using O
some O
variation O
of O
BERT O
. O
Our O
work O
is O
different O
from O
previous O
approaches O
because O
we O
( O
i O
) O
release O
a O
large O
- O
scale O
dataset O
for O
a O
language O
other O
than O
English O
, O
that O
was O
created O
with O
the O
aim O
to O
reduce O
demographic O
biases O
; O
( O
ii O
) O
experiment O
with O
multilingual O
approaches O
, O
including O
transfer O
learning O
and O
zero O
- O
shot O
- O
learning O
; O
( O
iii O
) O
perform O
an O
analysis O
of O
the O
amount O
of O
data O
needed O
to O
train O
reliable O
models O
; O
and O
, O
( O
iv O
) O
experiment O
with O
multilabel O
classiÔ¨Åcation O
, O
providing O
Ô¨Årst O
insights O
into O
this O
challenge O
task O
. O
3 O
Dataset O
In O
this O
section O
, O
we O
describe O
the O
procedure O
adopted O
to O
create O
ToLD O
- O
Br O
and O
present O
its O
main O
features O
. O
3.1 O
Data O
collection O
We O
used O
the O
GATE O
Cloud O
‚Äôs O
Twitter O
Collector6to O
collect O
posts O
on O
the O
Twitter O
platform O
from O
July O
to O
August O
2019 O
. O
We O
used O
two O
different O
strategies O
to O
select O
tweets O
for O
ToLD O
- O
Br O
, O
aiming O
to O
increase O
the O
probability O
of O
obtaining O
posts O
with O
toxic O
content O
, O
given O
that O
the O
volume O
of O
toxic O
tweets O
is O
signiÔ¨Åcantly O
smaller O
than O
data O
without O
offensive O
language O
. O
Our O
Ô¨Årst O
strategy O
searches O
for O
tweets O
that O
mention O
predeÔ¨Åned O
hashtags O
or O
keywords O
. O
We O
chose O
predeÔ¨Åned O
terms O
highly O
likely O
to O
belong O
to O
a O
toxic O
tweet O
in O
Brazilian O
Twitter O
, O
such O
as O
gay(‚ÄúGay O
tem O
que O
apanhar O
‚Äù O
‚Äì O
‚Äú O
Gay O
should O
be O
beaten O
up O
‚Äù O
) O
, O
mulherzinha O
( O
‚Äú O
Mulherzinha O
, O
vai O
lavar O
lou O
c O
¬∏a O
‚Äù O
‚Äì O
‚Äú O
Sissy O
, O
go O
wash O
dishes O
‚Äù O
) O
, O
and O
nordestino O
( O
‚Äú O
Nordestino O
pregui O
c O
¬∏oso O
‚Äù O
‚Äì O
‚Äú O
Lazy O
Northeastern O
‚Äù O
) O
. O
However O
, O
using O
this O
strategy O
alone O
may O
hinder O
learning O
a O
model O
capable O
of O
generalising O
the O
concept O
of O
toxicity O
beyond O
the O
scope O
of O
keywords O
. O
Consequently O
, O
another O
strategy O
was O
adopted O
: O
we O
scraped O
tweets O
that O
mention O
inÔ¨Çuential O
users O
like O
Brazil O
‚Äôs O
president O
Jair O
Bolsonaro O
and O
soccer O
player O
Neymar O
Jr O
, O
6https://cloud.gate.ac.uk/shopfront/ O
displayItem O
/ O
twitter O
- O
collector916prone O
to O
receive O
abuse O
( O
around O
50inÔ¨Çuential O
users O
were O
monitored O
) O
. O
Tweets O
collected O
through O
this O
method O
have O
no O
restrictions O
in O
terms O
of O
keywords O
and O
should O
broaden O
the O
scope O
of O
the O
data O
. O
We O
collected O
more O
than O
10 O
M O
unique O
tweets O
and O
randomly O
selected O
21 O
K O
examples O
to O
compose O
the O
annotated O
corpus O
. O
We O
note O
that O
12,600of O
these O
posts O
( O
60 O
% O
) O
comes O
from O
the O
Ô¨Årst O
strategy O
‚Äì O
predeÔ¨Åned O
keywords O
‚Äì O
and O
the O
remaining O
are O
tweets O
from O
threads O
of O
predeÔ¨Åned O
users O
. O
The O
data O
was O
pseudoanonymised O
before O
being O
sent O
for O
annotation O
, O
with O
all O
@mentions O
replaced O
by O
@user O
. O
3.2 O
Corpus O
annotation O
The O
annotation O
process O
started O
by O
choosing O
volunteers O
to O
perform O
the O
task O
of O
assigning O
labels O
for O
each O
example O
. O
For O
this O
, O
we O
made O
a O
public O
consultation O
at O
the O
Federal O
University O
of O
S O
Àúao O
Carlos O
( O
Brazil O
) O
to O
Ô¨Ånd O
candidate O
annotators O
( O
129volunteers O
registered O
for O
the O
task O
) O
. O
From O
these O
candidates O
, O
42 O
were O
selected O
based O
on O
their O
demographic O
information O
, O
aiming O
to O
balance O
annotation O
bias O
as O
the O
interpretation O
of O
toxicity O
may O
vary O
. O
Each O
annotator O
labelled O
1,500tweets O
, O
selecting O
one O
of O
the O
following O
categories O
: O
LGBTQ+phobia O
, O
obscene O
, O
insult O
, O
racism O
, O
misogyny O
and/or O
xenophobia O
( O
or O
leaving O
it O
blank O
for O
none O
) O
. O
Each O
tweet O
was O
annotated O
by O
three O
different O
annotators O
. O
To O
evaluate O
the O
diversity O
among O
the O
annotators O
, O
we O
explore O
their O
proÔ¨Åle O
. O
We O
emphasise O
that O
the O
identity O
of O
all O
annotators O
has O
been O
preserved O
. O
At O
this O
stage O
, O
we O
only O
survey O
general O
aspects O
of O
the O
volunteers O
who O
joined O
the O
labelling O
process O
. O
Table O
1 O
presents O
the O
distribution O
of O
annotators O
regarding O
sex O
, O
sexual O
orientation O
, O
and O
ethnicity O
. O
To O
deÔ¨Åne O
these O
categories O
, O
we O
use O
the O
same O
values O
as O
the O
Brazilian O
Institute O
of O
Geography O
and O
Statistics,7 O
in O
addition O
to O
giving O
the O
candidate O
the O
option O
of O
not O
declaring O
a O
value O
for O
each O
characteristic O
. O
Although O
we O
tried O
to O
keep O
the O
demographic O
aspects O
as O
balanced O
as O
possible O
when O
selecting O
the O
annotators O
, O
our O
pool O
of O
volunteers O
was O
still O
biased O
towards O
people O
identiÔ¨Åed O
as O
white O
andheterosexual O
( O
sexis O
a O
more O
balanced O
aspect O
than O
the O
others O
) O
. O
The O
age O
of O
the O
annotators O
varies O
between O
18and37years O
, O
with O
most O
of O
them O
in O
the O
range O
between O
19and23 O
. O
Figure O
1 O
illustrates O
the O
age O
distribution O
. O
We O
perform O
different O
data O
analysis O
over O
the O
dataset O
to O
better O
understand O
its O
properties O
. O
Inter7https://www.ibge.gov.br/en/home-eng O
. O
htmlCategories O
# O
annotators O
SexMale O
18 O
Female O
24 O
Heterosexual O
22 O
Sexual O
Bisexual O
12 O
orientation O
Homosexual O
5 O
Pansexual O
3 O
EthnicityWhite O
25 O
Brown O
9 O
Black O
5 O
Asian O
2 O
Non O
- O
Declared O
1 O
Table O
1 O
: O
Annotators O
demographic O
information O
. O
1819202122232425262729303537 O
age02468count O
Figure O
1 O
: O
Annotators O
age O
distribution O
. O
Œ± O
LGBTQ+phobia O
0.68 O
Insult O
0.56 O
Xenophobia O
0.57 O
Misogyny O
0.52 O
Obscene O
0.49 O
Racism O
0.48 O
Mean O
0.55 O
Table O
2 O
: O
Krippendorff O
‚Äô O
sŒ±for O
each O
label O
. O
annotator O
agreement O
is O
calculated O
in O
terms O
of O
Krippendorf O
‚Äô O
sŒ±(Table O
2 O
) O
, O
since O
Œ±is O
robust O
to O
multiple O
annotators O
, O
different O
degrees O
of O
disagreement O
and O
, O
missing O
values O
( O
Artstein O
and O
Poesio O
, O
2008 O
) O
. O
The O
LGBTQ+phobia O
class O
shows O
the O
highest O
agreement O
, O
which O
may O
indicate O
that O
comments O
in O
this O
class O
have O
a O
more O
distinctive O
lexicon O
than O
other O
classes O
. O
The O
lowest O
agreement O
is O
seem O
in O
obscene O
andracism O
classes O
. O
Besides O
, O
we O
observed O
in O
the O
annotations O
many O
cases O
in O
which O
some O
examples O
were O
labelled O
as O
separate O
classes O
, O
although O
they O
intend917Ann O
1 O
Ann O
2 O
Ann O
3 O
o O
fdp O
do O
Ô¨Ålho O
dela O
nao O
parava O
de O
tocar O
auto O
pra O
c*****o O
[ O
... O
] O
Insult O
None O
Obscene O
her O
sob O
son O
did O
not O
stop O
to O
play O
loud O
as O
f**k O
[ O
... O
] O
[ O
... O
] O
VAI O
SE O
F***R O
IRM O
ÀúAO O
VC O
N O
ÀúAO¬¥E O
FELIZ O
PQ O
NAO O
QUER O
Obscene O
Insult O
Insult O
[ O
... O
] O
f**k O
you O
brother O
you O
are O
not O
happy O
because O
you O
do O
not O
want O
to O
be O
‚Äú O
Aonde O
tem O
um O
monte O
que O
fala O
mal O
, O
mas O
ningu O
¬¥ O
em O
vai O
embora O
do O
morro O
. O
‚Äù O
acha O
que O
algu O
¬¥ O
em O
mora O
aqui O
por O
que O
quer O
, O
c*****o O
! O
? O
Que O
i O
d O
¬¥ O
eia O
. O
[ O
... O
] O
Obscene O
Obscene O
Insult O
‚Äú O
Where O
there O
are O
loads O
saying O
bad O
things O
, O
but O
nobody O
leaves O
the O
slum O
. O
‚Äù O
who O
thinks O
that O
someone O
lives O
here O
because O
they O
want O
, O
f**k O
! O
? O
What O
an O
idea O
. O
[ O
... O
] O
Table O
3 O
: O
Example O
of O
annotation O
divergence O
. O
LGBTQ+phobia O
Obscene O
Insult O
Racism O
Misogyny O
Xenophobia O
viado O
( O
59 O
) O
porra O
( O
332 O
) O
puta O
( O
221 O
) O
nego O
( O
6 O
) O
putinha O
( O
38 O
) O
sulista O
( O
12 O
) O
boiola O
( O
15 O
) O
caralho O
( O
317 O
) O
caralho O
( O
150 O
) O
branco O
( O
6 O
) O
puta O
( O
22 O
) O
carioca O
( O
7 O
) O
viadinho O
( O
13 O
) O
puta O
( O
268 O
) O
cara O
( O
135 O
) O
preto O
( O
4 O
) O
piranha O
( O
19 O
) O
fala O
( O
4 O
) O
sapat O
Àúao O
( O
12 O
) O
tomar O
( O
136 O
) O
porra O
( O
122 O
) O
nada O
( O
4 O
) O
mulher O
( O
11 O
) O
paulista O
( O
4 O
) O
caralho O
( O
11 O
) O
fuder O
( O
98 O
) O
lixo O
( O
101 O
) O
neg O
Àúao O
( O
3 O
) O
vagabunda O
( O
11 O
) O
gente O
( O
3 O
) O
cara O
( O
10 O
) O
cara O
( O
94 O
) O
Ô¨Ålho O
( O
92 O
) O
cara O
( O
3 O
) O
quer O
( O
8) O
nordestino O
( O
3 O
) O
quer O
( O
9 O
) O
merda O
( O
90 O
) O
burro O
( O
87 O
) O
falando O
( O
3 O
) O
vaca O
( O
8) O
todo O
( O
3 O
) O
homem O
( O
9 O
) O
mano O
( O
87 O
) O
tomar O
( O
86 O
) O
vida O
( O
3 O
) O
Ô¨Åca O
( O
6 O
) O
ainda O
( O
3 O
) O
todo O
( O
9 O
) O
toma O
( O
85 O
) O
merda O
( O
78 O
) O
segue O
( O
2 O
) O
onde O
( O
5 O
) O
sendo O
( O
2 O
) O
bicha O
( O
9 O
) O
fazer O
( O
77 O
) O
idiota O
( O
76 O
) O
p O
¬¥ O
agina O
( O
2 O
) O
tudo O
( O
5 O
) O
danc O
¬∏a O
( O
2 O
) O
Table O
4 O
: O
The O
most O
common O
words O
of O
each O
class O
and O
the O
number O
of O
sentences O
they O
occur O
( O
within O
parentheses O
) O
. O
to O
point O
the O
same O
concept O
. O
Classes O
like O
obscene O
andinsult O
seem O
to O
have O
confused O
the O
annotators O
, O
which O
may O
indicate O
an O
intersection O
in O
these O
concepts O
. O
Table O
3 O
shows O
examples O
of O
disagreements O
in O
the O
classiÔ¨Åcation O
of O
obscene O
andinsult O
. O
Table O
4 O
presents O
the O
ten O
most O
frequent O
words O
for O
each O
class O
, O
after O
removing O
stopwords O
. O
It O
conÔ¨Årms O
the O
intersection O
between O
classes O
obscene O
andinsult O
, O
with O
six O
out O
of O
ten O
words O
in O
common O
. O
For O
a O
quantitative O
analysis O
, O
Table O
5 O
presents O
the O
Jaccard O
distance O
between O
the O
100most O
frequent O
words O
for O
each O
class O
. O
Obscene O
andinsult O
show O
a O
considerably O
lower O
distance O
than O
other O
pairs O
( O
0.57 O
) O
, O
indicating O
that O
they O
have O
more O
words O
in O
common O
. O
3.3 O
Dataset O
characteristics O
For O
the O
purpose O
of O
training O
models O
for O
automatically O
classifying O
toxic O
comments O
, O
we O
must O
create O
aggregated O
annotations O
to O
provide O
only O
one O
binary O
label O
for O
each O
class O
. O
Different O
rules O
can O
be O
employed O
to O
aggregate O
the O
annotations O
, O
with O
different O
semantics O
. O
When O
we O
set O
an O
example O
as O
positive O
for O
toxicity O
only O
when O
all O
the O
annotators O
consider O
it O
to O
have O
the O
same O
category O
of O
offence O
, O
we O
insert O
bias O
toa O
b O
c O
d O
e O
f O
a0.00 O
0.73 O
0.78 O
0.90 O
0.80 O
0.94 O
b O
- O
0.00 O
0.57 O
0.84 O
0.77 O
0.90 O
c O
- O
- O
0.00 O
0.86 O
0.75 O
0.92 O
d O
- O
- O
- O
0.00 O
0.87 O
0.95 O
e O
- O
- O
- O
- O
0.00 O
0.94 O
Table O
5 O
: O
Jaccard O
distance O
between O
all O
pair O
of O
classes O
. O
( O
a O
) O
LGBTQ+phobia O
; O
( O
b O
) O
Obscene O
; O
( O
c O
) O
Insult O
; O
( O
d O
) O
Racism O
; O
( O
e O
) O
Misogyny O
; O
( O
f O
) O
Xenophobia O
. O
the O
model O
to O
not O
accuse O
a O
comment O
as O
toxic O
unless O
the O
offence O
is O
evident O
. O
Since O
this O
is O
very O
restrictive O
, O
we O
can O
also O
use O
the O
majority O
rule O
, O
but O
there O
must O
still O
be O
a O
consensus O
among O
the O
annotators O
. O
A O
last O
option O
is O
to O
consider O
that O
only O
a O
positive O
annotation O
is O
sufÔ¨Åcient O
to O
label O
the O
example O
as O
positive O
. O
This O
procedure O
acknowledges O
that O
annotators O
may O
have O
divergent O
views O
about O
what O
was O
said O
. O
It O
is O
a O
risky O
rule O
if O
we O
intend O
to O
create O
rigid O
systems O
that O
classify O
the O
tweets O
and O
take O
corrective O
or O
prohibitive O
actions O
. O
However O
, O
it O
is O
beneÔ¨Åcial O
for O
training O
a O
model O
that O
‚Äú O
raises O
a O
Ô¨Çag O
‚Äù O
to O
help O
moderators O
to O
assess O
the O
com-918LGBTQ+phobia O
Insult O
Xenophobia O
Misogyny O
Obscene O
Racism O
Toxic O
At O
least O
one O
annotator O
0 O
20656 O
16615 O
20849 O
20537 O
14348 O
20862 O
11745 O
1 O
344 O
4385 O
151 O
463 O
6652 O
138 O
9255 O
At O
least O
two O
annotators O
0 O
20824 O
19131 O
20958 O
20867 O
18597 O
20967 O
16566 O
1 O
176 O
1869 O
42 O
133 O
2403 O
33 O
4424 O
Three O
annotators O
0 O
20926 O
20483 O
20985 O
20971 O
20388 O
20994 O
19510 O
1 O
74 O
517 O
15 O
29 O
612 O
6 O
1490 O
Table O
6 O
: O
Dataset O
distribution O
considering O
different O
types O
of O
label O
aggregation O
. O
ments O
. O
Table O
6 O
shows O
the O
data O
distribution O
for O
each O
label O
and O
each O
aggregation O
strategy O
. O
For O
the O
sake O
of O
reproducibility O
and O
further O
usage O
, O
ToLD O
- O
Br O
is O
split O
into O
default O
training O
( O
80 O
% O
) O
, O
development O
( O
10 O
% O
) O
and O
test O
( O
10 O
% O
) O
sets O
using O
a O
stratiÔ¨Åed O
strategy O
. O
Besides O
, O
the O
corpus O
is O
released O
with O
all O
the O
annotations O
. O
Thus O
, O
future O
users O
of O
ToLD O
- O
Br O
will O
be O
able O
to O
use O
it O
with O
all O
the O
labels O
and O
with O
varying O
levels O
of O
agreement O
between O
the O
annotators O
. O
In O
this O
paper O
, O
we O
consider O
the O
least O
restrictive O
case O
, O
where O
if O
at O
least O
one O
annotator O
marked O
any O
offence O
category O
in O
an O
example O
, O
the O
example O
is O
positive O
for O
toxicity O
. O
Likewise O
, O
if O
a O
tweet O
was O
not O
tagged O
in O
any O
of O
these O
categories O
, O
it O
is O
considered O
non O
- O
toxic O
. O
We O
believe O
that O
it O
is O
essential O
that O
if O
any O
person O
feels O
uncomfortable O
with O
a O
post O
, O
it O
should O
be O
Ô¨Çagged O
as O
having O
a O
certain O
degree O
of O
toxicity O
. O
Therefore O
, O
a O
model O
built O
with O
this O
data O
must O
be O
able O
to O
identify O
offensive O
posts O
, O
even O
for O
a O
speciÔ¨Åc O
group O
of O
people O
. O
4 O
Materials O
and O
Methods O
In O
this O
section O
, O
we O
describe O
the O
techniques O
, O
tools O
, O
and O
other O
materials O
used O
in O
our O
experimental O
evaluation O
. O
As O
mentioned O
before O
, O
we O
restrict O
our O
experiments O
on O
the O
dataset O
labelled O
as O
positive O
when O
at O
least O
one O
annotator O
considers O
the O
example O
as O
toxic O
. O
We O
then O
investigate O
the O
effects O
of O
the O
number O
of O
instances O
in O
the O
training O
data O
, O
different O
algorithms O
to O
train O
a O
classiÔ¨Åcation O
model O
, O
various O
scenarios O
considering O
single- O
and O
multilingual O
models O
, O
and O
perform O
an O
initial O
experiment O
with O
multi O
- O
label O
classiÔ¨Åcation O
. O
We O
use O
Bag O
- O
of O
- O
Words O
( O
BoW O
) O
to O
represent O
the O
examples O
and O
an O
AutoML O
model O
to O
build O
the O
baseline O
model O
( O
BoW+AutoML O
) O
. O
For O
this O
, O
weuse O
the O
auto O
- O
sklearn8library O
( O
Feurer O
et O
al O
. O
, O
2019 O
) O
. O
For O
our O
BERT O
- O
based O
models O
, O
we O
use O
thesimpletransformers9library O
, O
that O
allows O
easy O
training O
and O
evaluation O
. O
We O
use O
default O
arguments O
for O
parameter O
tuning O
and O
deÔ¨Åne O
a O
seed O
to O
allow O
for O
reproducibility O
. O
Two O
versions O
of O
pretrained O
BERT O
language O
models O
are O
applied O
: O
Brazilian O
Portuguese O
BERT10(Souza O
et O
al O
. O
, O
2019 O
) O
, O
and O
Multilingual O
BERT11(Wolf O
et O
al O
. O
, O
2019 O
) O
. O
ToLD O
- O
Br O
is O
used O
to O
Ô¨Åne O
- O
tune O
BERT O
- O
based O
models O
for O
our O
monolingual O
experiments O
, O
with O
monolingual O
BERT O
( O
BR O
- O
BERT O
) O
and O
multilingual O
BERT O
( O
M O
- O
BERT O
- O
BR O
) O
. O
Although O
M O
- O
BERT O
- O
BR O
refers O
to O
the O
multilingual O
version O
of O
BERT O
, O
we O
refer O
to O
these O
two O
models O
as O
‚Äú O
monolingual O
models O
, O
‚Äù O
as O
we O
trained O
using O
the O
dataset O
with O
Brazilian O
Portuguese O
sentences O
alone O
. O
Using O
the O
multilingual O
model O
, O
we O
also O
carry O
out O
experiments O
in O
which O
we O
add O
data O
in O
English O
to O
train O
the O
models O
either O
through O
transfer O
learning O
or O
zero O
- O
shot O
learning O
. O
For O
these O
experiments O
we O
use O
the O
OLID O
data O
, O
concatenating O
the O
training O
and O
test O
splits O
into O
a O
single O
dataset O
. O
For O
transfer O
learning O
, O
we O
merged O
OLID O
and O
ToLD O
- O
Br O
to O
obtain O
a O
model O
with O
both O
languages O
as O
input O
, O
aiming O
to O
assess O
whether O
extra O
data O
in O
English O
helps O
in O
building O
better O
models O
( O
M O
- O
BERT(transfer O
) O
) O
. O
For O
zero O
- O
shot O
learning O
, O
OLID O
is O
used O
alone O
at O
training O
time O
, O
building O
a O
model O
that O
did O
not O
have O
access O
to O
any O
data O
in O
Brazilian O
Portuguese O
( O
M O
- O
BERT(zero O
- O
shot O
) O
) O
. O
8https://automl.github.io/auto-sklearn O
9github.com/ThilinaRajapakse/ O
simpletransformers O
10huggingface.co/neuralmind/ O
bert O
- O
base O
- O
portuguese O
- O
cased O
11huggingface.co/bert-base-multilingual-cased919Through O
these O
experiments O
, O
we O
can O
assess O
the O
advantages O
of O
monolingual O
models O
, O
whether O
data O
from O
another O
language O
can O
directly O
beneÔ¨Åt O
the O
classiÔ¨Åcation O
, O
and O
whether O
a O
speciÔ¨Åc O
monolingual O
dataset O
is O
necessary O
or O
not O
. O
We O
experiment O
with O
different O
sizes O
of O
the O
training O
set O
to O
assess O
the O
inÔ¨Çuence O
of O
the O
volume O
of O
data O
on O
the O
classiÔ¨Åcation O
. O
For O
that O
, O
we O
evaluate O
the O
results O
on O
random O
subsets O
of O
the O
data O
. O
The O
size O
of O
each O
partition O
varies O
in O
a O
range O
between O
10 O
% O
and O
100 O
% O
adding O
10 O
% O
of O
the O
data O
at O
each O
iteration O
. O
For O
each O
step O
, O
we O
repeat O
the O
classiÔ¨Åcation O
three O
times O
to O
minimise O
the O
probability O
of O
reporting O
results O
obtained O
by O
chance O
. O
Our O
best O
model O
( O
M O
- O
BERT O
- O
BR O
) O
is O
used O
for O
this O
experiment O
( O
c.f O
. O
Section O
5 O
) O
. O
Evaluation O
for O
binary O
classiÔ¨Åcation O
is O
done O
in O
terms O
of O
precision O
, O
recall O
and O
, O
F1 O
- O
score O
per O
class O
and O
macro O
- O
F1 O
. O
We O
also O
analyse O
the O
confusion O
matrices O
of O
our O
systems O
in O
order O
to O
better O
visualise O
the O
performance O
of O
our O
models O
in O
each O
class O
, O
mainly O
focusing O
on O
an O
analysis O
of O
false O
negatives O
. O
Although O
we O
mainly O
focus O
on O
binary O
classiÔ¨Åcation O
, O
an O
initial O
approach O
for O
multi O
- O
label O
classiÔ¨Åcation O
is O
also O
presented O
. O
We O
use O
the O
adaptation O
for O
the O
multi O
- O
label O
classiÔ¨Åcation O
scenario O
available O
in O
simpletransformers O
. O
In O
this O
case O
, O
the O
transformer O
‚Äôs O
output O
consists O
of O
six O
neurons O
, O
each O
representing O
one O
of O
the O
labels O
. O
These O
neurons O
are O
considered O
independent O
in O
the O
training O
and O
prediction O
process O
. O
Thus O
, O
when O
an O
output O
neuron O
is O
activated O
, O
we O
set O
the O
label O
represented O
by O
this O
neuron O
to O
positive O
. O
Besides O
, O
we O
evaluate O
the O
performance O
of O
a O
baseline O
based O
on O
BoW+AutoML O
, O
where O
we O
train O
an O
AutoML O
model O
for O
multilabel O
classiÔ¨Åcation O
. O
Evaluation O
is O
done O
in O
terms O
of O
Hamming O
loss O
and O
average O
precision O
( O
Tsoumakas O
et O
al O
. O
, O
2009 O
) O
. O
5 O
Results O
and O
Discussion O
This O
section O
shows O
the O
results O
of O
our O
experiments O
in O
classifying O
toxic O
comments O
using O
ToLD O
- O
Br O
. O
5.1 O
Binary O
ClassiÔ¨Åcation O
For O
evaluating O
our O
models O
, O
we O
are O
particularly O
interested O
in O
models O
with O
high O
performance O
in O
the O
positive O
class O
( O
classiÔ¨Åcation O
of O
toxic O
comments O
) O
. O
The O
worst O
case O
scenario O
are O
false O
negatives O
, O
i.e. O
toxic O
comments O
classiÔ¨Åed O
as O
non O
- O
toxic O
. O
Tables O
7 O
through O
11 O
summarises O
the O
results O
for O
each O
model O
. O
BoW+AutoML O
is O
already O
a O
competitive O
model O
, O
achieving O
74 O
% O
of O
macro- O
F1 O
, O
as O
shown O
in O
Table O
7 O
and O
Figure O
2a O
. O
Precision O
Recall O
F1 O
- O
score O
0 O
0.76 O
0.75 O
0.75 O
1 O
0.71 O
0.73 O
0.72 O
Macro O
Avg O
0.74 O
0.74 O
0.74 O
Weighted O
Avg O
0.74 O
0.74 O
0.74 O
Table O
7 O
: O
BoW O
+ O
AutoML O
Precision O
Recall O
F1 O
- O
score O
0 O
0.77 O
0.80 O
0.79 O
1 O
0.76 O
0.73 O
0.74 O
Macro O
Avg O
0.76 O
0.76 O
0.76 O
Weighted O
Avg O
0.76 O
0.77 O
0.76 O
Table O
8 O
: O
BR O
- O
BERT O
Precision O
Recall O
F1 O
- O
score O
0 O
0.81 O
0.69 O
0.75 O
1 O
0.69 O
0.82 O
0.75 O
Macro O
Avg O
0.75 O
0.75 O
0.75 O
Weighted O
Avg O
0.76 O
0.75 O
0.75 O
Table O
9 O
: O
M O
- O
BERT O
- O
BR O
Precision O
Recall O
F1 O
- O
score O
0 O
0.80 O
0.74 O
0.77 O
1 O
0.72 O
0.79 O
0.75 O
Macro O
Avg O
0.76 O
0.76 O
0.76 O
Weighted O
Avg O
0.77 O
0.76 O
0.76 O
Table O
10 O
: O
M O
- O
BERT(transfer O
) O
Precision O
Recall O
F1 O
- O
score O
0 O
0.59 O
0.83 O
0.69 O
1 O
0.63 O
0.32 O
0.43 O
Macro O
Avg O
0.61 O
0.58 O
0.56 O
Weighted O
Avg O
0.61 O
0.60 O
0.57 O
Table O
11 O
: O
M O
- O
BERT(zero O
- O
shot O
) O
The O
monolingual O
models O
BR O
- O
BERT O
and O
M O
- O
BERT O
- O
BR O
( O
Tables O
8 O
and O
9 O
, O
respectively O
) O
show O
very O
similar O
performances O
in O
all O
metrics O
, O
withBR O
- O
BERT O
being O
slightly O
better O
in O
terms O
of O
macro O
- O
F1 O
. O
However O
, O
M O
- O
BERT O
- O
BR O
is O
better O
in O
terms O
ofF1 O
- O
score O
for O
the O
positive O
class O
and O
shows O
fewer O
false O
negatives O
than O
BR O
- O
BERT O
( O
Figure O
2b O
forBR O
- O
BERT O
and O
Figure O
2c O
for O
M O
- O
BERT O
- O
BR O
) O
. O
M O
- O
BERT(transfer O
) O
( O
Table O
10 O
) O
does O
not O
out-9200 O
1 O
Predicted0 O
1True843 O
285 O
263 O
709 O
0.20.40.60.81.0 O
( O
a O
) O
0 O
1 O
Predicted0 O
1True902 O
226 O
267 O
705 O
0.20.40.60.81.0 O
  O
( O
b O
) O
0 O
1 O
Predicted0 O
1True778 O
350 O
179 O
793 O
0.20.40.60.81.0 O
  O
( O
c O
) O
0 O
1 O
Predicted0 O
1True837 O
291 O
207 O
765 O
0.20.40.60.81.0 O
  O
( O
d O
) O
0 O
1 O
Predicted0 O
1True940 O
188 O
657 O
315 O
0.20.40.60.81.0 O
  O
( O
e O
) O
Figure O
2 O
: O
Confusion O
matrices O
for O
each O
model O
( O
a O
) O
BoW+AutoML O
( O
Baseline O
) O
; O
( O
b O
) O
BR O
- O
BERT O
; O
( O
c O
) O
M O
- O
BERT O
- O
BR O
; O
( O
d O
) O
M O
- O
BERT(transfer O
) O
; O
( O
e O
) O
M O
- O
BERT(zero O
- O
shot O
) O
2500 O
5000 O
7500 O
10000 O
12500 O
15000 O
17500 O
20000 O
examples0.20.30.40.50.60.70.8scoreRecall O
Precision O
( O
a O
) O
2500 O
5000 O
7500 O
10000 O
12500 O
15000 O
17500 O
20000 O
examples0.550.600.650.700.750.800.850.900.95scoreRecall O
Precision O
( O
b O
) O
Figure O
3 O
: O
Precision O
and O
recall O
for O
different O
sizes O
of O
the O
training O
dataset O
for O
the O
( O
a O
) O
positive O
and O
( O
b O
) O
negative O
classes O
. O
perform O
the O
monolingual O
models O
and O
it O
also O
shows O
more O
false O
negatives O
than O
M O
- O
BERT O
- O
BR O
( O
Figure O
2e O
) O
. O
On O
the O
other O
hand O
, O
the O
number O
of O
false O
negatives O
in O
BR O
- O
BERT O
( O
267 O
) O
is O
slightly O
higher O
than O
the O
number O
of O
false O
negatives O
in O
M O
- O
BERT(transfer O
) O
( O
207 O
) O
. O
Finally O
, O
M O
- O
BERT(zero O
- O
shot O
) O
( O
Table O
11 O
) O
is O
the O
worst O
model O
, O
as O
expected O
. O
It O
performs O
particularly O
bad O
when O
classifying O
the O
positive O
class O
, O
achieving O
only O
43 O
% O
ofF1 O
- O
score O
for O
this O
class O
, O
mainly O
caused O
by O
its O
high O
number O
of O
false O
negatives O
( O
Figure O
2d O
) O
. O
In O
summary O
, O
transfer O
learning O
does O
not O
seem O
to O
improve O
over O
the O
overall O
performance O
of O
monolingual O
models O
. O
Based O
on O
the O
analysis O
of O
false O
negatives O
, O
M O
- O
BERT O
- O
BR O
appears O
as O
our O
best O
model O
. O
Zero O
- O
shot O
learning O
shows O
a O
very O
low O
performance O
, O
being O
particularly O
bad O
in O
the O
positive O
class O
. O
Error O
Analysis O
We O
also O
analyse O
the O
performance O
of O
our O
best O
model O
( O
M O
- O
BERT O
- O
BR O
) O
in O
each O
Ô¨Ånegrained O
class O
. O
The O
idea O
is O
to O
identify O
which O
toxic O
classes O
are O
most O
difÔ¨Åcult O
to O
be O
classiÔ¨Åed O
as O
toxic O
by O
our O
binary O
classiÔ¨Åer O
. O
As O
false O
negatives O
are O
a O
critical O
type O
of O
error O
in O
our O
application O
, O
Table O
12 O
shows O
the O
false O
negative O
rate O
( O
false O
negatives O
/ O
expected O
positives O
) O
for O
each O
toxic O
class O
. O
The O
ratio O
of O
false O
negatives O
is O
inversely O
proportional O
to O
the O
number O
of O
examples O
for O
a O
speciÔ¨Åc O
class O
. O
Insult O
and O
obscene O
, O
the O
largest O
classes O
, O
show O
the O
lowest O
falsenegative O
rate O
, O
whilst O
the O
highest O
rates O
are O
shown O
by O
classes O
with O
less O
examples O
( O
racism O
andxenophobia O
) O
. O
Therefore O
, O
in O
order O
to O
improve O
classiÔ¨Åcation O
models O
, O
these O
aspects O
of O
the O
imbalanced O
data O
need O
to O
be O
taken O
into O
account O
and O
further O
studied O
. O
False O
negative O
rate O
LGBTQ+phobia O
7/35 O
( O
0.2 O
) O
Insult O
67/448 O
( O
0.15 O
) O
Xenophobia O
13/19 O
( O
0.68 O
) O
Misogyny O
7/45 O
( O
0.15 O
) O
Obscene O
117/701 O
( O
0.17 O
) O
Racism O
8/17 O
( O
0.47 O
) O
Table O
12 O
: O
Error O
analysis O
for O
each O
label O
. O
5.2 O
Importance O
of O
Large O
Datasets O
In O
this O
experiment O
, O
we O
highlight O
the O
importance O
of O
collecting O
a O
considerable O
amount O
of O
examples O
, O
as O
toxicity O
can O
be O
expressed O
in O
many O
different O
ways O
. O
We O
separated O
the O
training O
data O
into O
10random O
splits O
from O
10 O
% O
to O
100 O
% O
of O
the O
data O
, O
increasing O
10 O
% O
of O
data O
at O
each O
step O
, O
and O
trained O
M O
- O
BERT O
- O
BR O
with O
three O
random O
samples O
for O
each O
step O
. O
Figure O
3 O
shows O
the O
mean O
recall O
, O
precision O
and O
F1 O
- O
score O
for O
the O
positive O
and O
negative O
classes O
, O
respectively O
, O
for O
each921data O
split O
. O
With O
few O
training O
examples O
, O
the O
model O
only O
performs O
well O
on O
the O
majority O
class O
, O
but O
as O
the O
number O
of O
instances O
grows O
, O
recall O
for O
the O
negative O
class O
starts O
decreasing O
while O
recall O
for O
the O
positive O
class O
increases O
, O
and O
precision O
rises O
for O
both O
classes O
. O
At O
least O
6 O
K O
examples O
seems O
to O
be O
necessary O
to O
achieve O
reliable O
results O
, O
while O
previous O
work O
for O
Portuguese O
reports O
the O
largest O
dataset O
with O
only O
5,668examples O
. O
This O
highlights O
the O
importance O
of O
ToLD O
- O
Br O
, O
as O
a O
large O
- O
scale O
dataset O
. O
5.3 O
Multi O
- O
Label O
ClassiÔ¨Åcation O
We O
experiment O
with O
multi O
- O
label O
classiÔ¨Åcation O
, O
building O
a O
model O
using O
the O
Multilingual O
BERT O
( O
similar O
to O
M O
- O
BERT O
- O
BR O
) O
. O
Our O
baseline O
is O
a O
set O
of O
BoW+AutoML O
models O
trained O
using O
Binary O
Relevance O
( O
Tsoumakas O
et O
al O
. O
, O
2009 O
) O
for O
multi O
- O
label O
classiÔ¨Åcation O
. O
The O
BERT O
- O
based O
models O
adopt O
a O
score O
threshold O
of O
0.5 O
in O
the O
output O
neuron O
to O
deal O
with O
multi O
- O
label O
. O
If O
the O
activation O
for O
a O
label O
in O
the O
output O
layer O
is O
higher O
than O
the O
threshold O
, O
we O
consider O
it O
positive O
. O
The O
baseline O
model O
obtained O
0.08and0.20of O
Hamming O
loss O
and O
average O
precision O
, O
respectively O
, O
while O
M O
- O
BERT O
- O
BR O
resulted O
in O
0.07and0.19for O
these O
measures O
, O
respectively O
. O
Figure O
4 O
displays O
the O
confusion O
matrices O
obtained O
by O
M O
- O
BERT O
- O
BR O
. O
0 O
1 O
Predicted0 O
1True2072 O
2 O
25 O
1 O
0.20.40.60.81.0 O
( O
a O
) O
0 O
1 O
Predicted0 O
1True1430 O
38 O
427 O
205 O
0.20.40.60.81.0 O
  O
( O
b O
) O
0 O
1 O
Predicted0 O
1True1635 O
41 O
290 O
134 O
0.20.40.60.81.0 O
( O
c O
) O
0 O
1 O
Predicted0 O
1True2089 O
0 O
11 O
0 O
0.20.40.60.81.0 O
  O
( O
d O
) O
0 O
1 O
Predicted0 O
1True2057 O
0 O
39 O
4 O
0.20.40.60.81.0 O
( O
e O
) O
0 O
1 O
Predicted0 O
1True2081 O
0 O
19 O
0 O
0.20.40.60.81.0 O
  O
( O
f O
) O
Figure O
4 O
: O
Confusion O
matrices O
for O
each O
label O
( O
a O
) O
LGBTQ+phobia O
; O
( O
b O
) O
Obscene O
; O
( O
c O
) O
Insult O
; O
( O
d O
) O
Racism O
; O
( O
e O
) O
Misogyny O
; O
( O
f O
) O
Xenophobia O
. O
This O
scenario O
is O
considerably O
more O
challenging O
than O
binary O
classiÔ¨Åcation O
. O
The O
positive O
class O
of O
each O
label O
corresponds O
to O
a O
subset O
of O
the O
examples O
labelled O
as O
toxic O
. O
Thus O
, O
it O
is O
likely O
that O
the O
number O
of O
instances O
for O
these O
classes O
will O
be O
insufÔ¨Åcient O
for O
the O
model O
to O
learn O
. O
Besides O
, O
the O
problem O
of O
unbalanced O
classes O
becomes O
evident O
( O
c.f O
. O
Table O
6 O
) O
. O
As O
a O
consequence O
, O
it O
is O
clear O
that O
labels O
with O
a O
small O
number O
of O
positive O
examples O
, O
like O
racism O
, O
misogyny O
, O
xenophobia O
, O
and O
LGBTQ+phobia O
were O
almost O
entirely O
classiÔ¨Åed O
as O
negative O
. O
In O
contrast O
, O
for O
obscene O
andinsult O
, O
labels O
with O
a O
considerable O
amount O
of O
positive O
examples O
, O
the O
model O
was O
capable O
of O
classifying O
some O
examples O
correctly O
. O
In O
all O
cases O
, O
besides O
insult O
, O
the O
baseline O
performs O
slightly O
better O
for O
the O
positive O
class O
( O
which O
justify O
the O
higher O
Hamming O
loss O
) O
. O
This O
setback O
is O
likely O
due O
to O
the O
difÔ¨Åculty O
of O
the O
neural O
model O
to O
learn O
with O
few O
examples O
. O
6 O
Concluding O
Remarks O
In O
this O
paper O
, O
we O
present O
ToLD O
- O
Br O
: O
a O
dataset O
for O
the O
classiÔ¨Åcation O
of O
toxic O
comments O
on O
Twitter O
in O
Brazilian O
Portuguese O
. O
Through O
a O
wide O
and O
comprehensive O
analysis O
, O
we O
demonstrated O
the O
need O
for O
this O
dataset O
for O
studies O
on O
automatic O
classiÔ¨Åcation O
of O
toxic O
comments O
. O
We O
highlight O
that O
monolingual O
approaches O
for O
this O
task O
still O
outperform O
multilingual O
experiments O
and O
that O
large O
- O
scale O
datasets O
are O
needed O
for O
building O
reliable O
models O
. O
Also O
, O
we O
show O
that O
there O
are O
still O
challenges O
to O
be O
overcome O
, O
such O
as O
the O
naturally O
signiÔ¨Åcant O
class O
imbalance O
when O
dealing O
with O
multi O
- O
label O
classiÔ¨Åcation O
. O
As O
future O
work O
, O
in O
addition O
to O
deal O
with O
class O
imbalance O
, O
we O
intend O
to O
evaluate O
if O
aggregating O
classes O
with O
high O
divergences O
between O
annotators O
can O
build O
more O
reliable O
models O
. O
Besides O
, O
we O
intend O
to O
assess O
the O
beneÔ¨Åts O
of O
adding O
unlabelled O
data O
to O
ToLD O
- O
Br O
to O
use O
semi O
- O
supervised O
techniques O
. O
7 O
Acknowledgements O
We O
thank O
the O
volunteers O
from O
UFSCar O
that O
made O
this O
research O
possible O
. O
The O
MIDAS O
group12from O
the O
Federal O
University O
of O
S O
Àúao O
Carlos O
( O
UFSCar O
) O
, O
Brazil O
, O
funded O
the O
annotation O
process O
. O
The O
SoBigData O
TransNational O
Access O
program O
( O
EU O
H2020 O
, O
grant O
agreement O
: O
654024 O
) O
funded O
Diego O
Silva O
and O
JoÀúao O
Leite O
‚Äôs O
visits O
to O
the O
University O
of O
ShefÔ¨Åeld O
. O
12midas.ufscar.br922References O
Ron O
Artstein O
and O
Massimo O
Poesio O
. O
2008 O
. O
Survey O
article O
: O
Inter O
- O
coder O
agreement O
for O
computational O
linguistics O
. O
Computational O
Linguistics O
, O
34(4):555 O
‚Äì O
596 O
. O
Valerio O
Basile O
, O
Cristina O
Bosco O
, O
Elisabetta O
Fersini O
, O
Debora O
Nozza O
, O
Viviana O
Patti O
, O
Francisco O
Manuel O
Rangel O
Pardo O
, O
Paolo O
Rosso O
, O
and O
Manuela O
Sanguinetti O
. O
2019 O
. O
SemEval-2019 O
task O
5 O
: O
Multilingual O
detection O
of O
hate O
speech O
against O
immigrants O
and O
women O
in O
twitter O
. O
In O
Proceedings O
of O
the O
13th O
International O
Workshop O
on O
Semantic O
Evaluation O
, O
pages O
54‚Äì63 O
, O
Minneapolis O
, O
Minnesota O
, O
USA O
. O
Association O
for O
Computational O
Linguistics O
. O
Miguel O
¬¥ O
Angel O
¬¥ O
Alvarez O
Carmona O
, O
Estefan O
¬¥ O
ƒ±a O
Guzm O
¬¥ O
anFalc¬¥on O
, O
Manuel O
Montes O
y O
G O
¬¥ O
omez O
, O
Hugo O
Jair O
Escalante O
, O
Luis O
Villase O
Àúnor O
Pineda O
, O
Ver O
¬¥ O
onica O
ReyesMeza O
, O
and O
Antonio O
Rico O
Sulayes O
. O
2018 O
. O
Overview O
of O
mex O
- O
a3 O
t O
at O
ibereval O
2018 O
: O
Authorship O
and O
aggressiveness O
analysis O
in O
mexican O
spanish O
tweets O
. O
In O
Proceedings O
of O
the O
Third O
Workshop O
on O
Evaluation O
of O
Human O
Language O
Technologies O
for O
Iberian O
Languages O
( O
IberEval O
2018 O
) O
, O
volume O
2150 O
of O
CEUR O
Workshop O
Proceedings O
, O
pages O
74‚Äì96 O
. O
CEUR-WS.org O
. O
Yi O
- O
Ling O
Chung O
, O
Elizaveta O
Kuzmenko O
, O
Serra O
Sinem O
Tekiroglu O
, O
and O
Marco O
Guerini O
. O
2019 O
. O
CONAN O
COunter O
NArratives O
through O
nichesourcing O
: O
a O
multilingual O
dataset O
of O
responses O
to O
Ô¨Åght O
online O
hate O
speech O
. O
In O
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
2819‚Äì2829 O
, O
Florence O
, O
Italy O
. O
Association O
for O
Computational O
Linguistics O
. O
Thomas O
Davidson O
, O
Dana O
Warmsley O
, O
Michael O
W. O
Macy O
, O
and O
Ingmar O
Weber O
. O
2017 O
. O
Automated O
hate O
speech O
detection O
and O
the O
problem O
of O
offensive O
language O
. O
In O
Proceedings O
of O
the O
11th O
International O
AAAI O
Conference O
on O
Web O
and O
Social O
Media O
, O
pages O
512‚Äì515 O
. O
AAAI O
Press O
. O
Rogers O
Prates O
de O
Pelle O
and O
Viviane O
P. O
Moreira O
. O
2017 O
. O
Offensive O
comments O
in O
the O
Brazilian O
web O
: O
a O
dataset O
and O
baseline O
results O
. O
In O
Proceedings O
of O
the O
VI O
Brazilian O
Workshop O
on O
Social O
Network O
Analysis O
and O
Mining O
, O
pages O
510‚Äì519 O
, O
Porto O
Alegre O
, O
RS O
, O
Brazil O
. O
SBC O
. O
Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O
BERT O
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
understanding O
. O
In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
4171‚Äì4186 O
, O
Minneapolis O
, O
Minnesota O
. O
Association O
for O
Computational O
Linguistics O
. O
Matthias O
Feurer O
, O
Aaron O
Klein O
, O
Katharina O
Eggensperger O
, O
Jost O
Tobias O
Springenberg O
, O
Manuel O
Blum O
, O
and O
Frank O
Hutter O
. O
2019 O
. O
Auto O
- O
sklearn O
: O
efÔ¨Åcient O
and O
robust O
automated O
machine O
learning O
. O
In O
Automated O
Machine O
Learning O
, O
pages O
113‚Äì134 O
. O
Springer O
, O
Cham O
. O
Paula O
Fortuna O
, O
Jo O
Àúao O
Rocha O
da O
Silva O
, O
Juan O
SolerCompany O
, O
Leo O
Wanner O
, O
and O
S O
¬¥ O
ergio O
Nunes O
. O
2019 O
. O
A O
hierarchically O
- O
labeled O
Portuguese O
hate O
speech O
dataset O
. O
In O
Proceedings O
of O
the O
Third O
Workshop O
on O
Abusive O
Language O
Online O
, O
pages O
94‚Äì104 O
, O
Florence O
, O
Italy O
. O
Association O
for O
Computational O
Linguistics O
. O
Antigoni O
Maria O
Founta O
, O
Constantinos O
Djouvas O
, O
Despoina O
Chatzakou O
, O
Ilias O
Leontiadis O
, O
Jeremy O
Blackburn O
, O
Gianluca O
Stringhini O
, O
Athena O
Vakali O
, O
Michael O
Sirivianos O
, O
and O
Nicolas O
Kourtellis O
. O
2018 O
. O
Large O
scale O
crowdsourcing O
and O
characterization O
of O
twitter O
abusive O
behavior O
. O
In O
Proceedings O
of O
the O
Twelfth O
International O
AAAI O
Conference O
on O
Web O
and O
Social O
Media O
, O
pages O
491‚Äì500 O
, O
Stanford O
, O
California O
. O
AAAI O
Press O
. O
Thomas O
Mandl O
, O
Sandip O
Modha O
, O
Prasenjit O
Majumder O
, O
Daksh O
Patel O
, O
Mohana O
Dave O
, O
Chintak O
Mandlia O
, O
and O
Aditya O
Patel O
. O
2019 O
. O
Overview O
of O
the O
HASOC O
Track O
at O
FIRE O
2019 O
: O
Hate O
Speech O
and O
Offensive O
Content O
IdentiÔ¨Åcation O
in O
Indo O
- O
European O
Languages O
. O
In O
Proceedings O
of O
the O
11th O
Forum O
for O
Information O
Retrieval O
Evaluation O
, O
page O
14‚Äì17 O
, O
Kolkata O
, O
India O
. O
Puneet O
Mathur O
, O
Rajiv O
Shah O
, O
Ramit O
Sawhney O
, O
and O
Debanjan O
Mahata O
. O
2018 O
. O
Detecting O
offensive O
tweets O
in O
Hindi O
- O
English O
code O
- O
switched O
language O
. O
In O
Proceedings O
of O
the O
Sixth O
International O
Workshop O
on O
Natural O
Language O
Processing O
for O
Social O
Media O
, O
pages O
18 O
‚Äì O
26 O
, O
Melbourne O
, O
Australia O
. O
Association O
for O
Computational O
Linguistics O
. O
Hamdy O
Mubarak O
, O
Kareem O
Darwish O
, O
and O
Walid O
Magdy O
. O
2017 O
. O
Abusive O
language O
detection O
on O
Arabic O
social O
media O
. O
In O
Proceedings O
of O
the O
First O
Workshop O
on O
Abusive O
Language O
Online O
, O
pages O
52‚Äì56 O
, O
Vancouver O
, O
BC O
, O
Canada O
. O
Association O
for O
Computational O
Linguistics O
. O
Hala O
Mulki O
, O
Hatem O
Haddad O
, O
Chedi O
Bechikh O
Ali O
, O
and O
Halima O
Alshabani O
. O
2019 O
. O
L O
- O
HSAB O
: O
A O
Levantine O
twitter O
dataset O
for O
hate O
speech O
and O
abusive O
language O
. O
InProceedings O
of O
the O
Third O
Workshop O
on O
Abusive O
Language O
Online O
, O
pages O
111‚Äì118 O
, O
Florence O
, O
Italy O
. O
Association O
for O
Computational O
Linguistics O
. O
Fabio O
Souza O
, O
Rodrigo O
Nogueira O
, O
and O
Roberto O
Lotufo O
. O
2019 O
. O
Portuguese O
named O
entity O
recognition O
using O
bert O
- O
crf O
. O
arXiv O
preprint O
arXiv:1909.10649 O
, O
pages O
1 O
‚Äì O
8 O
. O
Grigorios O
Tsoumakas O
, O
Ioannis O
Katakis O
, O
and O
Ioannis O
Vlahavas O
. O
2009 O
. O
Mining O
multi O
- O
label O
data O
. O
In O
Data O
mining O
and O
knowledge O
discovery O
handbook O
, O
pages O
667‚Äì685 O
. O
Springer O
. O
Zeerak O
Waseem O
and O
Dirk O
Hovy O
. O
2016 O
. O
Hateful O
symbols O
or O
hateful O
people O
? O
predictive O
features O
for O
hate O
speech O
detection O
on O
twitter O
. O
In O
Proceedings O
of O
the O
NAACL O
Student O
Research O
Workshop O
, O
pages O
88‚Äì93 O
, O
San O
Diego O
, O
California O
. O
Association O
for O
Computational O
Linguistics O
. O
Michael O
Wiegand O
, O
Melanie O
Siegel O
, O
and O
Josef O
Ruppenhofer O
. O
2018 O
. O
Overview O
of O
the O
GermEval O
2018923Shared O
Task O
on O
the O
IdentiÔ¨Åcation O
of O
Offensive O
Language O
. O
In O
Proceedings O
of O
GermEval O
2018 O
, O
14th O
Conference O
on O
Natural O
Language O
Processing O
( O
KONVENS O
2018 O
) O
, O
pages O
1‚Äì10 O
, O
Vienna O
, O
Austria O
. O
Thomas O
Wolf O
, O
Lysandre O
Debut O
, O
Victor O
Sanh O
, O
Julien O
Chaumond O
, O
Clement O
Delangue O
, O
Anthony O
Moi O
, O
Pierric O
Cistac O
, O
Tim O
Rault O
, O
R‚Äôemi O
Louf O
, O
Morgan O
Funtowicz O
, O
and O
Jamie O
Brew O
. O
2019 O
. O
HuggingFace O
‚Äôs O
Transformers O
: O
State O
- O
of O
- O
the O
- O
art O
natural O
language O
processing O
. O
arXiv O
preprint O
arXiv:1910.03771 O
, O
abs/1910.03771:1‚Äì11 O
. O
Ellery O
Wulczyn O
, O
Nithum O
Thain O
, O
and O
Lucas O
Dixon O
. O
2017 O
. O
Ex O
Machina O
: O
Personal O
Attacks O
Seen O
at O
Scale O
. O
In O
Proceedings O
of O
the O
26th O
International O
Conference O
on O
World O
Wide O
Web O
, O
page O
1391‚Äì1399 O
, O
Perth O
, O
Australia O
. O
Marcos O
Zampieri O
, O
Shervin O
Malmasi O
, O
Preslav O
Nakov O
, O
Sara O
Rosenthal O
, O
Noura O
Farra O
, O
and O
Ritesh O
Kumar O
. O
2019a O
. O
Predicting O
the O
type O
and O
target O
of O
offensive O
posts O
in O
social O
media O
. O
In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
1415‚Äì1420 O
, O
Minneapolis O
, O
Minnesota O
. O
Association O
for O
Computational O
Linguistics O
. O
Marcos O
Zampieri O
, O
Shervin O
Malmasi O
, O
Preslav O
Nakov O
, O
Sara O
Rosenthal O
, O
Noura O
Farra O
, O
and O
Ritesh O
Kumar O
. O
2019b O
. O
SemEval-2019 O
task O
6 O
: O
Identifying O
and O
categorizing O
offensive O
language O
in O
social O
media O
( O
OffensEval O
) O
. O
In O
Proceedings O
of O
the O
13th O
International O
Workshop O
on O
Semantic O
Evaluation O
, O
pages O
75 O
‚Äì O
86 O
, O
Minneapolis O
, O
Minnesota O
, O
USA O
. O
Association O
for O
Computational O
Linguistics O
. O
Marcos O
Zampieri O
, O
Shervin O
Malmasi O
, O
Preslav O
Nakov O
, O
Sara O
Rosenthal O
, O
Noura O
Farra O
, O
and O
Ritesh O
Kumar O
. O
2020 O
. O
SemEval-2020 O
Task O
12 O
: O
Multilingual O
Offensive O
Language O
IdentiÔ¨Åcationin O
Social O
Media O
( O
OffensEval O
2020 O
) O
. O
In O
To O
appear O
in O
the O
Proceedings O
of O
the O
14th O
International O
Workshop O
on O
Semantic O
Evaluation O
, O
Barcelona O
, O
Spain.924Proceedings O
of O
the O
1st O
Conference O
of O
the O
Asia O
- O
PaciÔ¨Åc O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
10th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
925‚Äì932 O
December O
4 O
- O
7 O
, O
2020 O
. O
¬© O
2020 O
Association O
for O
Computational O
Linguistics O
Measuring O
What O
Counts O
: O
The O
Case O
of O
Rumour O
Stance O
ClassiÔ¨Åcation O
Carolina O
Scarton O
University O
of O
ShefÔ¨Åeld O
, O
UK O
c.scarton@sheffield.ac.ukDiego O
Furtado O
Silva O
Federal O
University O
of O
S O
Àúao O
Carlos O
, O
Brazil O
diegofs@ufscar.br O
Kalina O
Bontcheva O
University O
of O
ShefÔ¨Åeld O
, O
UK O
k.bontcheva.sheffield.ac.uk O
Abstract O
Stance O
classiÔ¨Åcation O
can O
be O
a O
powerful O
tool O
for O
understanding O
whether O
and O
which O
users O
believe O
in O
online O
rumours O
. O
The O
task O
aims O
to O
automatically O
predict O
the O
stance O
of O
replies O
towards O
a O
given O
rumour O
, O
namely O
support O
, O
deny O
, O
question O
, O
or O
comment O
. O
Numerous O
methods O
have O
been O
proposed O
and O
their O
performance O
compared O
in O
the O
RumourEval O
shared O
tasks O
in O
2017 O
and O
2019 O
. O
Results O
demonstrated O
that O
this O
is O
a O
challenging O
problem O
since O
naturally O
occurring O
rumour O
stance O
data O
is O
highly O
imbalanced O
. O
This O
paper O
speciÔ¨Åcally O
questions O
the O
evaluation O
metrics O
used O
in O
these O
shared O
tasks O
. O
We O
reevaluate O
the O
systems O
submitted O
to O
the O
two O
RumourEval O
tasks O
and O
show O
that O
the O
two O
widely O
adopted O
metrics O
‚Äì O
accuracy O
and O
macro- O
F1 O
‚Äì O
are O
not O
robust O
for O
the O
four O
- O
class O
imbalanced O
task O
of O
rumour O
stance O
classiÔ¨Åcation O
, O
as O
they O
wrongly O
favour O
systems O
with O
highly O
skewed O
accuracy O
towards O
the O
majority O
class O
. O
To O
overcome O
this O
problem O
, O
we O
propose O
new O
evaluation O
metrics O
for O
rumour O
stance O
detection O
. O
These O
are O
not O
only O
robust O
to O
imbalanced O
data O
but O
also O
score O
higher O
systems O
that O
are O
capable O
of O
recognising O
the O
two O
most O
informative O
minority O
classes O
( O
support O
anddeny O
) O
. O
1 O
Introduction O
The O
automatic O
analysis O
of O
online O
rumours O
has O
emerged O
as O
an O
important O
and O
challenging O
Natural O
Language O
Processing O
( O
NLP O
) O
task O
. O
Rumours O
in O
social O
media O
can O
be O
deÔ¨Åned O
as O
claims O
that O
can O
not O
be O
veriÔ¨Åed O
as O
true O
or O
false O
at O
the O
time O
of O
posting O
( O
Zubiaga O
et O
al O
. O
, O
2018 O
) O
. O
Prior O
research O
( O
Mendoza O
et O
al O
. O
, O
2010 O
; O
Kumar O
and O
Carley O
, O
2019 O
) O
has O
shown O
that O
the O
stances O
of O
user O
replies O
are O
often O
a O
useful O
predictor O
of O
a O
rumour O
‚Äôs O
likely O
veracity O
, O
specially O
in O
the O
case O
of O
false O
rumours O
that O
tend O
to O
receive O
a O
higher O
number O
of O
replies O
denying O
them O
( O
Zubiaga O
et O
al O
. O
, O
2016 O
) O
. O
However O
, O
their O
automatic O
classiÔ¨Åcation O
is O
far O
from O
trivial O
as O
demonstrated O
by O
theresults O
of O
two O
shared O
tasks O
‚Äì O
RumourEval O
2017 O
and O
2019 O
( O
Derczynski O
et O
al O
. O
, O
2017 O
; O
Gorrell O
et O
al O
. O
, O
2019 O
) O
. O
More O
speciÔ¨Åcally O
, O
sub O
- O
task O
A O
models O
rumour O
stance O
classiÔ¨Åcation O
( O
RSC O
) O
as O
a O
four O
- O
class O
problem O
, O
where O
replies O
can O
: O
‚Ä¢support O
/agree O
with O
the O
rumour O
; O
‚Ä¢deny O
the O
veracity O
of O
the O
rumour O
; O
‚Ä¢query O
/ask O
for O
additional O
evidence O
; O
‚Ä¢comment O
without O
clear O
contribution O
to O
assessing O
the O
veracity O
of O
the O
rumour O
. O
Figure O
1 O
shows O
an O
example O
of O
a O
reply O
denying O
a O
post O
on O
Twitter O
. O
Figure O
1 O
: O
Example O
of O
a O
deny O
stance O
. O
In O
RumourEval O
2017 O
the O
training O
data O
contains O
297 O
rumourous O
threads O
about O
eight O
events O
. O
The O
test O
set O
has O
28 O
threads O
, O
with O
20 O
threads O
about O
the O
same O
events O
as O
the O
training O
data O
and O
eight O
threads O
about O
unseen O
events O
. O
In O
2019 O
, O
the O
2017 O
training O
data O
is O
augmented O
with O
40 O
Reddit O
threads O
. O
The O
new O
2019 O
test O
set O
has O
56 O
threads O
about O
natural O
disasters O
from O
Twitter O
and O
a O
set O
of O
Reddit O
data O
( O
25 O
threads O
) O
. O
These O
datasets O
for O
RSC O
are O
highly O
imbalanced O
: O
thecomment O
class O
is O
considerably O
larger O
than O
the O
other O
classes O
. O
Table O
1 O
shows O
the O
distribution O
of O
stances O
per O
class O
in O
both O
2017 O
and O
2019 O
datasets O
, O
where O
66 O
% O
and O
72 O
% O
of O
the O
data O
( O
respectively O
) O
correspond O
to O
comments O
.Comments O
arguably O
are O
the9252017 O
2019 O
support O
1,004 O
( O
18 O
% O
) O
1,184 O
( O
14 O
% O
) O
deny O
415 O
( O
7 O
% O
) O
606 O
( O
7 O
% O
) O
query O
464 O
( O
8 O
% O
) O
608 O
( O
7 O
% O
) O
comment O
3,685 O
( O
66 O
% O
) O
6,176 O
( O
72 O
% O
) O
total O
5,568 O
8,574 O
Table O
1 O
: O
Distribution O
of O
stances O
per O
class O
‚Äì O
with O
percentages O
between O
parenthesis O
. O
least O
useful O
when O
it O
comes O
to O
assessing O
overall O
rumour O
veracity O
, O
unlike O
support O
anddeny O
which O
have O
been O
shown O
to O
help O
with O
rumour O
veriÔ¨Åcation O
( O
Mendoza O
et O
al O
. O
, O
2010 O
) O
. O
Therefore O
, O
RSC O
is O
not O
only O
an O
imbalanced O
, O
multi O
- O
class O
problem O
, O
but O
it O
also O
has O
classes O
with O
different O
importance O
. O
This O
is O
different O
from O
standard O
stance O
classiÔ¨Åcation O
tasks O
( O
e.g. O
SemEval O
2016 O
task O
6 O
( O
Mohammad O
et O
al O
. O
, O
2016 O
) O
) O
, O
where O
classes O
have O
arguably O
the O
same O
importance O
. O
It O
also O
differs O
from O
the O
veracity O
task O
( O
RumourEval O
sub O
- O
task O
B O
) O
, O
where O
the O
problem O
is O
binary O
and O
it O
is O
not O
as O
an O
imbalanced O
problem O
as O
RSC.1 O
RumourEval O
2017 O
evaluated O
systems O
based O
on O
accuracy O
( O
ACC O
) O
, O
which O
is O
not O
sufÔ¨Åciently O
robust O
on O
imbalanced O
datasets O
( O
Huang O
and O
Ling O
, O
2005 O
) O
. O
This O
prompted O
the O
adoption O
of O
macro- O
F1 O
in O
the O
2019 O
evaluation O
. O
Kumar O
and O
Carley O
( O
2019 O
) O
also O
argue O
that O
macro- O
F1is O
a O
more O
reliable O
evaluation O
metric O
for O
RSC O
. O
Previous O
work O
on O
RSC O
also O
adopted O
these O
metrics O
( O
Li O
et O
al O
. O
, O
2019b O
; O
Kochkina O
et O
al O
. O
, O
2018 O
; O
Dungs O
et O
al O
. O
, O
2018 O
) O
. O
This O
paper O
re O
- O
evaluates O
the O
sub O
- O
task O
A O
results O
of O
RumourEval O
2017 O
and O
2019.2It O
analyses O
the O
performance O
of O
the O
participating O
systems O
according O
to O
different O
evaluation O
metrics O
and O
shows O
that O
even O
macro O
- O
F1 O
, O
that O
is O
robust O
for O
evaluating O
binary O
classiÔ¨Åcation O
on O
imbalanced O
datasets O
, O
fails O
to O
reliably O
evaluate O
the O
performance O
on O
RSC O
. O
This O
is O
particularly O
critical O
in O
RumourEval O
where O
not O
only O
is O
data O
imbalanced O
, O
but O
also O
two O
minority O
classes O
( O
deny O
andsupport O
) O
are O
the O
most O
important O
to O
classify O
well O
. O
Based O
on O
prior O
research O
on O
imbalanced O
datasets O
in O
areas O
other O
that O
NLP O
( O
e.g. O
Yijing O
et O
al O
. O
( O
2016 O
) O
and O
Elrahman O
and O
Abraham O
( O
2013 O
) O
) O
, O
we O
propose O
four O
alternative O
metrics O
for O
evaluating O
RSC O
. O
These O
metrics O
change O
the O
systems O
ranking O
for O
RSC O
in O
RumourEval O
2017 O
and O
2019 O
, O
rewarding O
systems O
with O
high O
performance O
on O
the O
minority O
classes O
. O
1Other O
NLP O
tasks O
, O
like O
sentiment O
analysis O
are O
also O
not O
comparable O
, O
since O
these O
tasks O
are O
either O
binary O
classiÔ¨Åcation O
( O
which O
is O
then O
solved O
by O
using O
macro- O
F1 O
) O
or O
do O
not O
have O
a O
clear O
priority O
over O
classes O
. O
2We O
thank O
the O
organisers O
for O
making O
the O
data O
available.2 O
Evaluation O
metrics O
for O
classiÔ¨Åcation O
We O
deÔ¨ÅneTP O
= O
true O
positives O
, O
TN O
= O
true O
negatives O
, O
FP O
= O
false O
positives O
and O
FN O
= O
false O
negatives O
, O
where O
TPc(FPc O
) O
is O
equivalent O
to O
the O
true O
( O
false O
) O
positives O
and O
TNc(FNc O
) O
is O
equivalent O
to O
the O
true O
( O
false O
) O
negatives O
for O
a O
given O
class O
c. O
Accuracy O
( O
ACC O
) O
is O
the O
ratio O
between O
the O
number O
of O
correct O
predictions O
and O
the O
total O
number O
of O
predictions O
( O
N):ACC O
= O
/summationtextC O
c=1TPc O
N O
, O
whereCis O
the O
number O
of O
classes O
. O
ACC O
only O
considers O
the O
values O
that O
were O
classiÔ¨Åed O
correctly O
, O
disregarding O
the O
mistakes O
. O
This O
is O
inadequate O
for O
imbalanced O
problems O
like O
RSC O
where O
, O
as O
shown O
in O
Table O
1 O
, O
most O
of O
the O
data O
is O
classiÔ¨Åed O
as O
comments O
. O
As O
shown O
in O
Section O
3 O
, O
most O
systems O
will O
fail O
to O
classify O
the O
deny O
class O
and O
still O
achieve O
high O
scores O
in O
terms O
of O
ACC O
. O
In O
fact O
, O
the O
best O
system O
for O
2017 O
according O
toACC O
( O
Turing O
) O
fails O
to O
classify O
all O
denies O
. O
Precision O
( O
Pc O
) O
and O
Recall O
( O
Rc)Pcis O
the O
ratio O
between O
the O
number O
of O
correctly O
predicted O
instances O
and O
all O
the O
predicted O
values O
for O
c O
: O
Pc= O
TPc O
TPc+FPc O
. O
Rcis O
the O
ratio O
between O
correctly O
predicted O
instances O
and O
the O
number O
of O
instances O
that O
actually O
belongs O
to O
the O
class O
c O
: O
Rc O
= O
TPc O
TPc+FNc O
. O
macro O
- O
FŒ≤FŒ≤cscore O
is O
deÔ¨Åned O
as O
the O
harmonic O
mean O
of O
precision O
and O
recall O
, O
where O
the O
per O
- O
class O
score O
can O
be O
deÔ¨Åned O
as O
: O
FŒ≤c= O
( O
1 O
+ O
Œ≤2)Pc¬∑Rc O
Œ≤2Pc+Rc O
. O
IfŒ≤= O
1,FŒ≤is O
theF1score O
. O
IfŒ≤ O
> O
1,Ris O
given O
a O
higher O
weight O
and O
if O
Œ≤ O
< O
1,Pis O
given O
a O
higher O
weight O
. O
The O
macro- O
FŒ≤is O
the O
arithmetic O
mean O
between O
the O
FŒ≤scores O
for O
each O
class O
: O
macroFŒ≤c=/summationtextC O
c=1FŒ≤c O
C. O
Although O
macro- O
F1is O
expected O
to O
perform O
better O
than O
ACC O
for O
imbalanced O
binary O
problems O
, O
its O
beneÔ¨Åts O
in O
the O
scenario O
of O
multi O
- O
class O
classiÔ¨Åcation O
are O
not O
clear O
. O
SpeciÔ¨Åcally O
, O
as O
it O
relies O
on O
the O
arithmetic O
mean O
over O
the O
classes O
, O
it O
may O
hide O
the O
poor O
performance O
of O
a O
model O
in O
one O
of O
the O
classes O
if O
it O
performs O
well O
on O
the O
majority O
class O
( O
i.e. O
comments O
in O
this O
case O
) O
. O
For O
instance O
, O
as O
shown O
in O
Table O
2 O
, O
according O
to O
macro- O
F1the O
best O
performing O
system O
would O
be O
ECNU O
, O
which O
still O
fails O
to O
classify O
correctly O
almost O
all O
deny O
instances O
. O
Geometric O
mean O
Metrics O
like O
the O
geometric O
mean O
ofR O
: O
GMR O
= O
C O
/ O
radicaltp O
/ O
radicalvertex O
/ O
radicalvertex O
/ O
radicalbtC O
/ O
productdisplay O
c=1Rc.926ACC O
macro O
- O
F1GMRwAUC O
wF1wF2 O
Turing O
a O
0.784 O
( O
1 O
) O
0.434 O
( O
5 O
) O
0.000 O
( O
8) O
0.583 O
( O
7 O
) O
0.274 O
( O
6 O
) O
0.230 O
( O
7 O
) O
UWaterloo O
( O
Bahuleyan O
and O
Vechtomova O
, O
2017 O
) O
0.780 O
( O
2 O
) O
0.455 O
( O
2 O
) O
0.237 O
( O
5 O
) O
0.595 O
( O
5 O
) O
0.300 O
( O
2 O
) O
0.255 O
( O
6 O
) O
ECNU O
( O
Wang O
et O
al O
. O
, O
2017 O
) O
0.778 O
( O
3 O
) O
0.467 O
( O
1 O
) O
0.214 O
( O
7 O
) O
0.599 O
( O
4 O
) O
0.289 O
( O
4 O
) O
0.263 O
( O
4 O
) O
Mama O
Edha O
( O
Garc O
¬¥ O
ƒ±a O
Lozano O
et O
al O
. O
, O
2017 O
) O
0.749 O
( O
4 O
) O
0.453 O
( O
3 O
) O
0.220 O
( O
6 O
) O
0.607 O
( O
1 O
) O
0.299 O
( O
3 O
) O
0.283 O
( O
3 O
) O
NileTMRG O
( O
Enayet O
and O
El O
- O
Beltagy O
, O
2017 O
) O
0.709 O
( O
5 O
) O
0.452 O
( O
4 O
) O
0.363 O
( O
1 O
) O
0.606 O
( O
2 O
) O
0.306 O
( O
1 O
) O
0.296 O
( O
1 O
) O
IKM O
( O
Chen O
et O
al O
. O
, O
2017 O
) O
0.701 O
( O
6 O
) O
0.408 O
( O
7 O
) O
0.272 O
( O
4 O
) O
0.570 O
( O
8) O
0.241 O
( O
7 O
) O
0.226 O
( O
8) O
IITP O
( O
Singh O
et O
al O
. O
, O
2017 O
) O
0.641 O
( O
7 O
) O
0.403 O
( O
8) O
0.345 O
( O
2 O
) O
0.602 O
( O
3 O
) O
0.276 O
( O
5 O
) O
0.294 O
( O
2 O
) O
DFKI O
DKT O
( O
Srivastava O
et O
al O
. O
, O
2017 O
) O
0.635 O
( O
8) O
0.409 O
( O
6 O
) O
0.316 O
( O
3 O
) O
0.589 O
( O
6 O
) O
0.234 O
( O
8) O
0.256 O
( O
5 O
) O
majority O
class O
0.742 O
0.213 O
0.000 O
0.500 O
0.043 O
0.047 O
all O
denies O
0.068 O
0.032 O
0.000 O
0.500 O
0.051 O
0.107 O
all O
support O
0.090 O
0.041 O
0.000 O
0.500 O
0.066 O
0.132 O
Table O
2 O
: O
Evaluation O
of O
RumourEval O
2017 O
submissions O
. O
Values O
between O
parenthesis O
are O
the O
ranking O
of O
the O
system O
according O
to O
the O
metric O
. O
The O
ofÔ¨Åcial O
evaluation O
metric O
column O
( O
ACC O
) O
is O
highlighted O
in O
bold O
. O
ACC O
macro O
- O
F1GMR O
wAUC O
wF1wF2 O
BLCU O
NLP O
( O
Yang O
et O
al O
. O
, O
2019 O
) O
0.841 O
( O
2 O
) O
0.619 O
( O
1 O
) O
0.571 O
( O
2 O
) O
0.722 O
( O
2 O
) O
0.520 O
( O
1 O
) O
0.500 O
( O
2 O
) O
BUT O
- O
FIT O
( O
Fajcik O
et O
al O
. O
, O
2019 O
) O
0.852 O
( O
1 O
) O
0.607 O
( O
2 O
) O
0.519 O
( O
3 O
) O
0.689 O
( O
3 O
) O
0.492 O
( O
3 O
) O
0.441 O
( O
3 O
) O
eventAI O
( O
Li O
et O
al O
. O
, O
2019a O
) O
0.735 O
( O
11 O
) O
0.578 O
( O
3 O
) O
0.726 O
( O
1 O
) O
0.807 O
( O
1 O
) O
0.502 O
( O
2 O
) O
0.602 O
( O
1 O
) O
UPV O
( O
Ghanem O
et O
al O
. O
, O
2019 O
) O
0.832 O
( O
4 O
) O
0.490 O
( O
4 O
) O
0.333 O
( O
5 O
) O
0.614 O
( O
5 O
) O
0.340 O
( O
4 O
) O
0.292 O
( O
5 O
) O
GWU O
( O
Hamidian O
and O
Diab O
, O
2019 O
) O
0.797 O
( O
9 O
) O
0.435 O
( O
5 O
) O
0.000 O
( O
7 O
) O
0.604 O
( O
6 O
) O
0.284 O
( O
5 O
) O
0.265 O
( O
6 O
) O
SINAI O
- O
DL O
( O
Garc O
¬¥ O
ƒ±a O
- O
Cumbreras O
et O
al O
. O
, O
2019 O
) O
0.830 O
( O
5 O
) O
0.430 O
( O
6 O
) O
0.000 O
( O
8) O
0.577 O
( O
7 O
) O
0.255 O
( O
7 O
) O
0.215 O
( O
7 O
) O
wshuyi O
0.538 O
( O
13 O
) O
0.370 O
( O
7 O
) O
0.467 O
( O
4 O
) O
0.627 O
( O
4 O
) O
0.261 O
( O
6 O
) O
0.325 O
( O
4 O
) O
Columbia O
( O
Liu O
et O
al O
. O
, O
2019 O
) O
0.789 O
( O
10 O
) O
0.363 O
( O
8) O
0.000 O
( O
9 O
) O
0.562 O
( O
10 O
) O
0.221 O
( O
10 O
) O
0.191 O
( O
9 O
) O
jurebb O
0.806 O
( O
8) O
0.354 O
( O
9 O
) O
0.122 O
( O
6 O
) O
0.567 O
( O
9 O
) O
0.229 O
( O
8) O
0.120 O
( O
12 O
) O
mukundyr O
0.837 O
( O
3 O
) O
0.340 O
( O
10 O
) O
0.000 O
( O
10 O
) O
0.570 O
( O
8) O
0.224 O
( O
9 O
) O
0.198 O
( O
8) O
nx1 O
0.828 O
( O
7 O
) O
0.327 O
( O
11 O
) O
0.000 O
( O
11 O
) O
0.557 O
( O
11 O
) O
0.206 O
( O
11 O
) O
0.173 O
( O
10 O
) O
WeST O
( O
Baris O
et O
al O
. O
, O
2019 O
) O
0.829 O
( O
6 O
) O
0.321 O
( O
12 O
) O
0.000 O
( O
12 O
) O
0.551 O
( O
12 O
) O
0.197 O
( O
12 O
) O
0.161 O
( O
11 O
) O
Xinthl O
0.725 O
( O
12 O
) O
0.230 O
( O
13 O
) O
0.000 O
( O
13 O
) O
0.493 O
( O
13 O
) O
0.072 O
( O
13 O
) O
0.071 O
( O
13 O
) O
majority O
class O
0.808 O
0.223 O
0.000 O
0.500 O
0.045 O
0.048 O
all O
denies O
0.055 O
0.026 O
0.000 O
0.500 O
0.042 O
0.091 O
all O
support O
0.086 O
0.040 O
0.000 O
0.500 O
0.063 O
0.128 O
Table O
3 O
: O
Evaluation O
of O
RumourEval O
2019 O
submissions O
. O
Values O
between O
parenthesis O
are O
the O
ranking O
of O
the O
system O
according O
to O
the O
metric O
. O
The O
ofÔ¨Åcial O
evaluation O
metric O
column O
( O
macro- O
F1 O
) O
is O
highlighted O
in O
bold O
. O
are O
proposed O
for O
evaluating O
speciÔ¨Åc O
types O
of O
errors O
. O
AsFNs O
may O
be O
more O
relevant O
than O
FPs O
for O
imbalanced O
data O
, O
assessing O
models O
using O
Ris O
an O
option O
to O
measure O
this O
speciÔ¨Åc O
type O
of O
error O
. O
Moreover O
, O
applyingGMR O
for O
each O
class O
severely O
penalises O
a O
model O
that O
achieves O
a O
low O
score O
for O
a O
given O
class O
. O
Area O
under O
the O
ROC O
curve O
Receiver O
operating O
characteristic O
( O
ROC O
) O
( O
Fawcett O
, O
2006 O
) O
assesses O
the O
performance O
of O
classiÔ¨Åers O
considering O
the O
relation O
between O
Rcand O
the O
false O
positive O
rate O
, O
deÔ¨Åned O
as O
( O
per O
class O
): O
FPRc O
= O
FPc O
TNc+FPc O
. O
Since O
RSC O
consists O
of O
discrete O
classiÔ¨Åcations O
, O
ROC O
charts O
for O
each O
ccontain O
only O
one O
point O
regarding O
the O
coordinate O
( O
FPRc O
, O
Rc O
) O
. O
Area O
under O
the O
ROC O
curve O
( O
AUC O
) O
measures O
the O
area O
of O
the O
curve O
produced O
by O
the O
points O
in O
an O
ROC O
space O
. O
In O
the O
discrete O
case O
, O
it O
measures O
the O
area O
of O
the O
polygon O
drawn O
by O
the O
segments O
connecting O
the O
vertices O
( O
( O
0,0),(FPRc O
, O
Rc),(1,1),(0,1 O
) O
) O
. O
High O
AUC O
scores O
are O
achieved O
when O
R(probability O
of O
detection O
) O
is O
maximised O
, O
while O
FPR O
( O
probability O
of O
false O
alarm O
) O
is O
minimised O
. O
We O
experiment O
with O
a O
weighted O
variation O
of O
AUC O
: O
wAUC O
= O
C O
/ O
summationdisplay O
c=1wc¬∑AUCc O
. O
Weighted O
macro- O
FŒ≤ O
a O
variation O
of O
macro- O
FŒ≤ O
, O
where O
each O
class O
also O
receives O
different O
weights O
, O
is O
also O
considered O
: O
wFŒ≤ O
= O
C O
/ O
summationdisplay O
c=1wc¬∑FŒ≤c O
, O
We O
useŒ≤= O
1(PandRhave O
the O
same O
importance O
) O
andŒ≤= O
2(Ris O
more O
important O
) O
. O
Arguably O
, O
misclassifying O
denies O
andsupports O
( O
FNDandFNS O
, O
respectively O
) O
is O
equivalent O
to O
ignore O
relevant O
information O
for O
debunking O
a O
rumour O
. O
Since O
FNs O
negatively O
impact O
R O
, O
we O
hypothesise O
that O
Œ≤= O
2is O
more O
robust O
for O
the O
RSC O
case O
. O
wAUC O
andwFŒ≤ O
are O
inspired O
by O
empirical O
evidence O
that O
different O
classes O
have O
different O
importance O
for O
RSC.3Weights O
should O
be O
manually O
deÔ¨Åned O
, O
since O
they O
can O
not O
be O
automatically O
learnt O
. O
3Similarly O
, O
previous O
work O
proposes O
metrics O
( O
Elkan O
, O
2001 O
) O
and O
learning O
algorithms O
( O
Chawla O
et O
al O
. O
, O
2008 O
) O
based O
on O
classspeciÔ¨Åc O
mis O
- O
classiÔ¨Åcation O
costs.927Figure O
2 O
: O
Confusion O
matrix O
for O
systems O
from O
RumourEval O
2017 O
. O
Figure O
3 O
: O
Confusion O
matrix O
for O
selected O
systems O
from O
RumourEval O
2019 O
. O
All O
other O
systems O
failed O
to O
classify O
correctly O
either O
all O
or O
the O
vast O
majority O
of O
deny O
instances O
. O
We O
follow O
the O
hypothesis O
that O
support O
anddeny O
classes O
are O
more O
informative O
than O
others.4 O
3 O
Re O
- O
evaluating O
RumourEval O
task O
A O
Tables O
2 O
and O
3 O
report O
the O
different O
evaluation O
scores O
per O
metric O
for O
each O
of O
the O
RumourEval O
2017 O
and O
2019 O
systems.5ACC O
and O
macro O
- O
F1are O
reported O
in O
the O
second O
and O
third O
columns O
respectively O
, O
followed O
by O
a O
column O
for O
each O
of O
the O
four O
proposed O
metrics O
. O
Besides O
evaluating O
the O
participating O
systems O
, O
we O
also O
computed O
scores O
for O
three O
baselines O
: O
majority O
class O
( O
all O
stances O
are O
considered O
comments O
) O
, O
all O
denies O
andall O
support O
( O
all O
replies O
are O
classed O
as O
deny O
/support O
) O
. O
Our O
results O
show O
that O
the O
choice O
of O
evaluation O
metric O
has O
a O
signiÔ¨Åcant O
impact O
on O
system O
ranking O
. O
In O
RumourEval O
2017 O
, O
the O
winning O
system O
based O
onACC O
wasTuring O
. O
However O
, O
Figure O
2 O
shows O
that O
this O
system O
classiÔ¨Åed O
all O
denies O
in4wsupport O
= O
wdeny O
= O
0.40,wquery O
= O
0.15and O
wcomment O
= O
0.05 O
. O
5The O
systems O
HLT(HITSZ O
) O
, O
LECS O
, O
magc O
, O
UI O
- O
AI O
, O
shaheyu O
andNimbusTwoThousand O
are O
omitted O
because O
they O
do O
not O
provide O
the O
same O
number O
of O
inputs O
as O
the O
test O
set.correctly O
, O
favouring O
the O
majority O
class O
( O
comment O
) O
. O
When O
looking O
at O
the O
macro- O
F1score O
, O
Turing O
is O
classiÔ¨Åed O
as O
Ô¨Åfth O
, O
whilst O
the O
winner O
is O
ECNU O
, O
followed O
by O
UWaterloo O
. O
Both O
systems O
also O
perform O
very O
poorly O
on O
denies O
, O
classifying O
only O
1 O
% O
and O
3 O
% O
of O
them O
correctly O
. O
On O
the O
other O
hand O
, O
the O
four O
proposed O
metrics O
penalise O
these O
systems O
for O
these O
errors O
and O
rank O
higher O
those O
that O
perform O
better O
on O
classes O
other O
than O
the O
majority O
one O
. O
For O
example O
, O
the O
winner O
according O
to O
GMR O
, O
wF1 O
andwF2isNileTMRG O
that O
, O
according O
to O
Figure O
2 O
, O
shows O
higher O
accuracy O
on O
the O
deny O
, O
support O
andquery O
classes O
, O
without O
considerably O
degraded O
performance O
on O
the O
majority O
class O
. O
wAUC O
still O
favours O
the O
Mama O
Edha O
system O
which O
has O
very O
limited O
performance O
on O
the O
important O
deny O
class O
. O
As O
is O
evident O
from O
Figure O
2 O
, O
NileTMRG O
is O
arguably O
the O
best O
system O
in O
predicting O
all O
classes O
: O
it O
has O
the O
highest O
accuracy O
for O
denies O
, O
and O
a O
sufÔ¨Åciently O
high O
accuracy O
for O
support O
, O
queries O
and O
comments O
. O
Using O
the O
same O
criteria O
, O
the O
second O
best O
system O
should O
be O
IITP O
. O
The O
only O
two O
metrics O
that O
reÔ¨Çect O
this O
ranking O
are O
GMR O
andwF2 O
. O
In O
the O
case O
ofwF1 O
, O
the O
second O
system O
is O
UWaterloo O
, O
928which O
has O
a O
very O
low O
accuracy O
on O
the O
deny O
class O
. O
For O
RumourEval O
2019 O
, O
the O
best O
system O
according O
to O
macro- O
F1(the O
ofÔ¨Åcial O
metric O
) O
is O
BLCU O
NLP O
, O
followed O
by O
BUT O
- O
FIT O
. O
However O
, O
after O
analysing O
the O
confusion O
matrices O
in O
Figure O
3 O
, O
we O
can O
conclude O
that O
eventAI O
is O
a O
more O
suitable O
model O
due O
to O
its O
high O
accuracy O
on O
support O
anddeny O
. O
MetricsGMR O
, O
wAUC O
andwF2show O
eventAI O
as O
the O
best O
system O
. O
Finally O
, O
wshuyi O
is O
ranked O
as O
fourth O
according O
to O
GMR O
, O
wAUC O
andwF2 O
, O
while O
it O
ranked O
seventh O
in O
terms O
of O
macro- O
F1 O
, O
behind O
systems O
like O
GWU O
andSINAI O
- O
DL O
that O
fail O
to O
classify O
all O
deny O
instances O
. O
Although O
wshuyi O
is O
clearly O
worse O
than O
eventAI O
, O
BLCU O
NLP O
and O
BUT O
- O
FIT O
, O
it O
is O
arguably O
more O
reliable O
than O
systems O
that O
misclassify O
the O
large O
majority O
of O
denies O
.6 O
Our O
analyses O
suggest O
that O
GMR O
andwF2are O
the O
most O
reliable O
for O
evaluating O
RSC O
tasks O
. O
4 O
Weight O
selection O
In O
Section O
3 O
, O
wAUC O
, O
wF1andwF2have O
been O
obtained O
using O
empirically O
deÔ¨Åned O
weights O
( O
wsupport O
= O
wdeny O
= O
0.40,wquery O
= O
0.15and O
wcomment O
= O
0.05 O
) O
. O
These O
values O
reÔ¨Çect O
the O
key O
importance O
of O
the O
support O
anddeny O
classes O
. O
Although O
query O
is O
less O
important O
than O
the O
Ô¨Årst O
two O
, O
it O
is O
nevertheless O
more O
informative O
than O
comment O
. O
Previous O
work O
tried O
to O
adjust O
the O
learning O
weights O
in O
order O
to O
minimise O
the O
effect O
of O
the O
imbalanced O
data O
. O
Garc O
¬¥ O
ƒ±a O
Lozano O
et O
al O
. O
( O
2017 O
) O
( O
Mama O
Edha O
) O
, O
change O
the O
weights O
of O
their O
Convolutional O
Neural O
Network O
( O
CNN O
) O
architecture O
, O
giving O
higher O
importance O
to O
support O
, O
deny O
andquery O
classes O
, O
to O
better O
reÔ¨Çect O
their O
class O
distribution.7Ghanem O
et O
al O
. O
( O
2019 O
) O
( O
UPV O
) O
also O
change O
the O
weights O
in O
their O
Logistic O
Regression O
model O
in O
accordance O
with O
the O
data O
distribution O
criterion.8Nevertheless O
, O
these O
systems O
misclassify O
almost O
all O
deny O
instances O
. O
Table O
4 O
shows O
the O
RumourEval O
2017 O
systems O
ranked O
according O
to O
wF2using O
the O
Mama O
Edha O
andUPV O
weights O
. O
In O
these O
cases O
, O
wF2beneÔ¨Åts O
DFKI O
DKT O
, O
ranking O
it O
Ô¨Årst O
, O
since O
queries O
receive O
a O
higher O
weight O
than O
support O
. O
However O
, O
this O
system O
only O
correctly O
classiÔ¨Åes O
6 O
% O
of O
support O
instances O
, O
which O
makes O
it O
less O
suitable O
for O
our O
task O
thanNileTMRG O
for O
instance O
. O
ECNU O
is O
also O
ranked O
6Confusion O
matrices O
for O
all O
systems O
of O
RumourEval O
2019 O
are O
presented O
in O
Appendix O
A. O
7wsupport O
= O
0.157 O
, O
wdeny O
= O
0.396,wquery O
= O
0.399 O
andwcomment O
= O
0.048 O
8wsupport O
= O
0.2 O
, O
wdeny O
= O
0.35,wquery O
= O
0.35and O
wcomment O
= O
0.1better O
than O
Mama O
Edha O
andIITP O
, O
likely O
due O
to O
its O
higher O
performance O
on O
query O
instances O
. O
wF2wF2 O
Mama O
Edha O
UPV O
Turing O
0.246 O
( O
8) O
0.289 O
( O
8) O
UWaterloo O
0.283 O
( O
7 O
) O
0.322 O
( O
5 O
) O
ECNU O
0.334 O
( O
3 O
) O
0.364 O
( O
3 O
) O
Mama O
Edha O
0.312 O
( O
4 O
) O
0.349 O
( O
4 O
) O
NileTMRG O
0.350 O
( O
2 O
) O
0.374 O
( O
2 O
) O
IKM O
0.293 O
( O
5 O
) O
0.318 O
( O
7 O
) O
IITP O
0.289 O
( O
6 O
) O
0.321 O
( O
6 O
) O
DFKI O
DKT O
0.399 O
( O
1 O
) O
0.398 O
( O
1 O
) O
Table O
4 O
: O
RumourEval O
2017 O
evaluated O
using O
wF2with O
weights O
from O
Mama O
Edha O
andUPV O
. O
Arguably O
, O
deÔ¨Åning O
weights O
based O
purely O
on O
data O
distribution O
is O
not O
sufÔ¨Åcient O
for O
RSC O
. O
Thus O
our O
empirically O
deÔ¨Åned O
weights O
seem O
to O
be O
more O
suitable O
than O
those O
derived O
from O
data O
distribution O
alone O
, O
as O
the O
former O
accurately O
reÔ¨Çect O
that O
support O
anddeny O
are O
the O
most O
important O
, O
albeit O
minority O
distributed O
classes O
. O
Further O
research O
is O
required O
in O
order O
to O
identify O
the O
most O
suitable O
weights O
for O
this O
task O
. O
5 O
Discussion O
This O
paper O
re O
- O
evaluated O
the O
systems O
that O
participated O
in O
the O
two O
editions O
of O
RumourEval O
task O
A O
( O
stance O
classiÔ¨Åcation O
) O
. O
We O
showed O
that O
the O
choice O
of O
evaluation O
metric O
for O
assessing O
the O
task O
has O
a O
signiÔ¨Åcant O
impact O
on O
system O
rankings O
. O
The O
metrics O
proposed O
here O
are O
better O
suited O
to O
evaluating O
tasks O
with O
imbalanced O
data O
, O
since O
they O
do O
not O
favour O
the O
majority O
class O
. O
We O
also O
suggest O
variations O
of O
AUC O
and O
macro O
- O
FŒ≤that O
give O
different O
weights O
for O
each O
class O
, O
which O
is O
desirable O
for O
scenarios O
where O
some O
classes O
are O
more O
important O
than O
others O
. O
The O
main O
lesson O
from O
this O
paper O
is O
that O
evaluation O
is O
an O
important O
aspect O
of O
NLP O
tasks O
and O
it O
needs O
to O
be O
done O
accordingly O
, O
after O
a O
careful O
consideration O
of O
the O
problem O
and O
the O
data O
available O
. O
In O
particular O
, O
we O
recommend O
that O
future O
work O
on O
RSC O
usesGMR O
and O
/ O
orwFŒ≤ O
( O
preferablyŒ≤= O
2 O
) O
as O
evaluation O
metrics O
. O
Best O
practices O
on O
evaluation O
rely O
on O
several O
metrics O
that O
can O
assess O
different O
aspects O
of O
quality O
. O
Therefore O
, O
relying O
on O
several O
metrics O
is O
likely O
the O
best O
approach O
for O
RSC O
evaluation O
. O
Acknowledgments O
This O
work O
was O
funded O
by O
the O
WeVerify O
project O
( O
EU O
H2020 O
, O
grant O
agreement O
: O
825297 O
) O
. O
The O
SoBigData O
TransNational O
Access O
program O
( O
EU O
H2020 O
, O
grant O
agreement O
: O
654024 O
) O
funded O
Diego O
Silva O
‚Äôs O
visit O
to O
the O
University O
of O
ShefÔ¨Åeld.929References O
Hareesh O
Bahuleyan O
and O
Olga O
Vechtomova O
. O
2017 O
. O
UWaterloo O
at O
SemEval-2017 O
task O
8 O
: O
Detecting O
stance O
towards O
rumours O
with O
topic O
independent O
features O
. O
In O
Proceedings O
of O
the O
11th O
International O
Workshop O
on O
Semantic O
Evaluation O
( O
SemEval-2017 O
) O
, O
pages O
461 O
‚Äì O
464 O
, O
Vancouver O
, O
Canada O
. O
Association O
for O
Computational O
Linguistics O
. O
Ipek O
Baris O
, O
Lukas O
Schmelzeisen O
, O
and O
Steffen O
Staab O
. O
2019 O
. O
CLEARumor O
at O
SemEval-2019 O
task O
7 O
: O
ConvoLving O
ELMo O
against O
rumors O
. O
In O
Proceedings O
of O
the O
13th O
International O
Workshop O
on O
Semantic O
Evaluation O
, O
pages O
1105‚Äì1109 O
, O
Minneapolis O
, O
Minnesota O
, O
USA O
. O
Association O
for O
Computational O
Linguistics O
. O
Nitesh O
V O
. O
Chawla O
, O
David O
A. O
Cieslak O
, O
Lawrence O
O. O
Hall O
, O
and O
Ajay O
Joshi O
. O
2008 O
. O
Automatically O
countering O
imbalance O
and O
its O
empirical O
relationship O
to O
cost O
. O
Data O
Mining O
and O
Knowledge O
Discovery O
, O
17(2):225‚Äì252 O
. O
Yi O
- O
Chin O
Chen O
, O
Zhao O
- O
Yang O
Liu O
, O
and O
Hung O
- O
Yu O
Kao O
. O
2017 O
. O
IKM O
at O
SemEval-2017 O
task O
8 O
: O
Convolutional O
neural O
networks O
for O
stance O
detection O
and O
rumor O
veriÔ¨Åcation O
. O
In O
Proceedings O
of O
the O
11th O
International O
Workshop O
on O
Semantic O
Evaluation O
( O
SemEval-2017 O
) O
, O
pages O
465‚Äì469 O
, O
Vancouver O
, O
Canada O
. O
Association O
for O
Computational O
Linguistics O
. O
Leon O
Derczynski O
, O
Kalina O
Bontcheva O
, O
Maria O
Liakata O
, O
Rob O
Procter O
, O
Geraldine O
Wong O
Sak O
Hoi O
, O
and O
Arkaitz O
Zubiaga O
. O
2017 O
. O
SemEval-2017 O
task O
8 O
: O
RumourEval O
: O
Determining O
rumour O
veracity O
and O
support O
for O
rumours O
. O
In O
Proceedings O
of O
the O
11th O
International O
Workshop O
on O
Semantic O
Evaluation O
( O
SemEval-2017 O
) O
, O
pages O
69‚Äì76 O
, O
Vancouver O
, O
Canada O
. O
Association O
for O
Computational O
Linguistics O
. O
Sebastian O
Dungs O
, O
Ahmet O
Aker O
, O
Norbert O
Fuhr O
, O
and O
Kalina O
Bontcheva O
. O
2018 O
. O
Can O
rumour O
stance O
alone O
predict O
veracity O
? O
In O
Proceedings O
of O
the O
27th O
International O
Conference O
on O
Computational O
Linguistics O
, O
pages O
3360‚Äì3370 O
, O
Santa O
Fe O
, O
New O
Mexico O
, O
USA O
. O
Association O
for O
Computational O
Linguistics O
. O
Charles O
Elkan O
. O
2001 O
. O
The O
foundations O
of O
cost O
- O
sensitive O
learning O
. O
In O
Proceedings O
of O
the O
17th O
International O
Joint O
Conference O
on O
ArtiÔ¨Åcial O
Intelligence O
- O
Volume O
2 O
, O
pages O
973‚Äì978 O
, O
Seattle O
, O
Washington O
, O
USA O
. O
International O
Joint O
Conferences O
on O
ArtiÔ¨Åcial O
Intelligence O
. O
Shaza O
M. O
Abd O
Elrahman O
and O
Ajith O
Abraham O
. O
2013 O
. O
A O
Review O
of O
Class O
Imbalance O
Problem O
. O
Journal O
of O
Network O
and O
Innovative O
Computing O
, O
1:332‚Äì340 O
. O
Omar O
Enayet O
and O
Samhaa O
R. O
El O
- O
Beltagy O
. O
2017 O
. O
NileTMRG O
at O
SemEval-2017 O
task O
8 O
: O
Determining O
rumour O
and O
veracity O
support O
for O
rumours O
on O
twitter O
. O
In O
Proceedings O
of O
the O
11th O
International O
Workshop O
on O
Semantic O
Evaluation O
( O
SemEval-2017 O
) O
, O
pages O
470‚Äì474 O
, O
Vancouver O
, O
Canada O
. O
Association O
for O
Computational O
Linguistics O
. O
Martin O
Fajcik O
, O
Pavel O
Smrz O
, O
and O
Lukas O
Burget O
. O
2019 O
. O
BUT O
- O
FIT O
at O
SemEval-2019 O
task O
7 O
: O
Determining O
therumour O
stance O
with O
pre O
- O
trained O
deep O
bidirectional O
transformers O
. O
In O
Proceedings O
of O
the O
13th O
International O
Workshop O
on O
Semantic O
Evaluation O
, O
pages O
1097‚Äì1104 O
, O
Minneapolis O
, O
Minnesota O
, O
USA O
. O
Association O
for O
Computational O
Linguistics O
. O
Tom O
Fawcett O
. O
2006 O
. O
An O
introduction O
to O
ROC O
analysis O
. O
Pattern O
Recognition O
Letters O
, O
27(8):861‚Äì874 O
. O
Miguel O
A. O
Garc O
¬¥ O
ƒ±a O
- O
Cumbreras O
, O
Salud O
Mar O
¬¥ O
ƒ±a O
Jim O
¬¥ O
enezZafra O
, O
Arturo O
Montejo O
- O
R O
¬¥ O
aez O
, O
Manuel O
Carlos O
D O
¬¥ O
ƒ±azGaliano O
, O
and O
Estela O
Saquete O
. O
2019 O
. O
SINAI O
- O
DL O
at O
SemEval-2019 O
task O
7 O
: O
Data O
augmentation O
and O
temporal O
expressions O
. O
In O
Proceedings O
of O
the O
13th O
International O
Workshop O
on O
Semantic O
Evaluation O
, O
pages O
1120‚Äì1124 O
, O
Minneapolis O
, O
Minnesota O
, O
USA O
. O
Association O
for O
Computational O
Linguistics O
. O
Marianela O
Garc O
¬¥ O
ƒ±a O
Lozano O
, O
Hanna O
Lilja O
, O
Edward O
Tj¬®ornhammar O
, O
and O
Maja O
Karasalo O
. O
2017 O
. O
Mama O
edha O
at O
SemEval-2017 O
task O
8 O
: O
Stance O
classiÔ¨Åcation O
with O
CNN O
and O
rules O
. O
In O
Proceedings O
of O
the O
11th O
International O
Workshop O
on O
Semantic O
Evaluation O
( O
SemEval-2017 O
) O
, O
pages O
481‚Äì485 O
, O
Vancouver O
, O
Canada O
. O
Association O
for O
Computational O
Linguistics O
. O
Bilal O
Ghanem O
, O
Alessandra O
Teresa O
Cignarella O
, O
Cristina O
Bosco O
, O
Paolo O
Rosso O
, O
and O
Francisco O
Manuel O
Rangel O
Pardo O
. O
2019 O
. O
UPV-28 O
- O
UNITO O
at O
SemEval2019 O
task O
7 O
: O
Exploiting O
post O
‚Äôs O
nesting O
and O
syntax O
information O
for O
rumor O
stance O
classiÔ¨Åcation O
. O
In O
Proceedings O
of O
the O
13th O
International O
Workshop O
on O
Semantic O
Evaluation O
, O
pages O
1125‚Äì1131 O
, O
Minneapolis O
, O
Minnesota O
, O
USA O
. O
Association O
for O
Computational O
Linguistics O
. O
Genevieve O
Gorrell O
, O
Elena O
Kochkina O
, O
Maria O
Liakata O
, O
Ahmet O
Aker O
, O
Arkaitz O
Zubiaga O
, O
Kalina O
Bontcheva O
, O
and O
Leon O
Derczynski O
. O
2019 O
. O
SemEval-2019 O
task O
7 O
: O
RumourEval O
, O
determining O
rumour O
veracity O
and O
support O
for O
rumours O
. O
In O
Proceedings O
of O
the O
13th O
International O
Workshop O
on O
Semantic O
Evaluation O
, O
pages O
845‚Äì854 O
, O
Minneapolis O
, O
Minnesota O
, O
USA O
. O
Association O
for O
Computational O
Linguistics O
. O
Sardar O
Hamidian O
and O
Mona O
Diab O
. O
2019 O
. O
GWU O
NLP O
at O
SemEval-2019 O
task O
7 O
: O
Hybrid O
pipeline O
for O
rumour O
veracity O
and O
stance O
classiÔ¨Åcation O
on O
social O
media O
. O
In O
Proceedings O
of O
the O
13th O
International O
Workshop O
on O
Semantic O
Evaluation O
, O
pages O
1115‚Äì1119 O
, O
Minneapolis O
, O
Minnesota O
, O
USA O
. O
Association O
for O
Computational O
Linguistics O
. O
Jin O
Huang O
and O
Charles O
X O
Ling O
. O
2005 O
. O
Using O
AUC O
and O
accuracy O
in O
evaluating O
learning O
algorithms O
. O
IEEE O
Transactions O
on O
Knowledge O
and O
Data O
Engineering O
, O
17(3):299‚Äì310 O
. O
Elena O
Kochkina O
, O
Maria O
Liakata O
, O
and O
Arkaitz O
Zubiaga O
. O
2018 O
. O
All O
- O
in O
- O
one O
: O
Multi O
- O
task O
learning O
for O
rumour O
veriÔ¨Åcation O
. O
In O
Proceedings O
of O
the O
27th O
International O
Conference O
on O
Computational O
Linguistics O
, O
pages O
3402‚Äì3413 O
, O
Santa O
Fe O
, O
New O
Mexico O
, O
USA O
. O
Association O
for O
Computational O
Linguistics.930Sumeet O
Kumar O
and O
Kathleen O
Carley O
. O
2019 O
. O
Tree O
LSTMs O
with O
convolution O
units O
to O
predict O
stance O
and O
rumor O
veracity O
in O
social O
media O
conversations O
. O
InProceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
5047‚Äì5058 O
, O
Florence O
, O
Italy O
. O
Association O
for O
Computational O
Linguistics O
. O
Quanzhi O
Li O
, O
Qiong O
Zhang O
, O
and O
Luo O
Si O
. O
2019a O
. O
eventAI O
at O
SemEval-2019 O
task O
7 O
: O
Rumor O
detection O
on O
social O
media O
by O
exploiting O
content O
, O
user O
credibility O
and O
propagation O
information O
. O
In O
Proceedings O
of O
the O
13th O
International O
Workshop O
on O
Semantic O
Evaluation O
, O
pages O
855‚Äì859 O
, O
Minneapolis O
, O
Minnesota O
, O
USA O
. O
Association O
for O
Computational O
Linguistics O
. O
Quanzhi O
Li O
, O
Qiong O
Zhang O
, O
and O
Luo O
Si O
. O
2019b O
. O
Rumor O
detection O
by O
exploiting O
user O
credibility O
information O
, O
attention O
and O
multi O
- O
task O
learning O
. O
In O
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
1173‚Äì1179 O
, O
Florence O
, O
Italy O
. O
Association O
for O
Computational O
Linguistics O
. O
Zhuoran O
Liu O
, O
Shivali O
Goel O
, O
Mukund O
Yelahanka O
Raghuprasad O
, O
and O
Smaranda O
Muresan O
. O
2019 O
. O
Columbia O
at O
SemEval-2019 O
task O
7 O
: O
Multi O
- O
task O
learning O
for O
stance O
classiÔ¨Åcation O
and O
rumour O
veriÔ¨Åcation O
. O
InProceedings O
of O
the O
13th O
International O
Workshop O
on O
Semantic O
Evaluation O
, O
pages O
1110‚Äì1114 O
, O
Minneapolis O
, O
Minnesota O
, O
USA O
. O
Association O
for O
Computational O
Linguistics O
. O
Marcelo O
Mendoza O
, O
Barbara O
Poblete O
, O
and O
Carlos O
Castillo O
. O
2010 O
. O
Twitter O
under O
crisis O
: O
can O
we O
trust O
what O
we O
RT O
? O
In O
Proceedings O
of O
the O
First O
Workshop O
on O
Social O
Media O
Analytics O
, O
pages O
71‚Äì79 O
, O
Washington O
, O
DC O
, O
USA O
. O
Association O
for O
Computing O
Machinery O
. O
Saif O
Mohammad O
, O
Svetlana O
Kiritchenko O
, O
Parinaz O
Sobhani O
, O
Xiaodan O
Zhu O
, O
and O
Colin O
Cherry O
. O
2016 O
. O
SemEval-2016 O
task O
6 O
: O
Detecting O
stance O
in O
tweets O
. O
InProceedings O
of O
the O
10th O
International O
Workshop O
on O
Semantic O
Evaluation O
( O
SemEval-2016 O
) O
, O
pages O
31 O
‚Äì O
41 O
, O
San O
Diego O
, O
California O
. O
Association O
for O
Computational O
Linguistics O
. O
Vikram O
Singh O
, O
Sunny O
Narayan O
, O
Md O
Shad O
Akhtar O
, O
Asif O
Ekbal O
, O
and O
Pushpak O
Bhattacharyya O
. O
2017 O
. O
IITP O
at O
SemEval-2017 O
task O
8 O
: O
A O
supervised O
approach O
for O
rumour O
evaluation O
. O
In O
Proceedings O
of O
the O
11th O
International O
Workshop O
on O
Semantic O
Evaluation O
( O
SemEval2017 O
) O
, O
pages O
497‚Äì501 O
, O
Vancouver O
, O
Canada O
. O
Association O
for O
Computational O
Linguistics O
. O
Ankit O
Srivastava O
, O
Georg O
Rehm O
, O
and O
Julian O
Moreno O
Schneider O
. O
2017 O
. O
DFKI O
- O
DKT O
at O
SemEval2017 O
task O
8 O
: O
Rumour O
detection O
and O
classiÔ¨Åcation O
using O
cascading O
heuristics O
. O
In O
Proceedings O
of O
the O
11th O
International O
Workshop O
on O
Semantic O
Evaluation O
( O
SemEval-2017 O
) O
, O
pages O
486‚Äì490 O
, O
Vancouver O
, O
Canada O
. O
Association O
for O
Computational O
Linguistics O
. O
Feixiang O
Wang O
, O
Man O
Lan O
, O
and O
Yuanbin O
Wu O
. O
2017 O
. O
ECNU O
at O
SemEval-2017 O
task O
8 O
: O
Rumour O
evalua O
- O
tion O
using O
effective O
features O
and O
supervised O
ensemble O
models O
. O
In O
Proceedings O
of O
the O
11th O
International O
Workshop O
on O
Semantic O
Evaluation O
( O
SemEval-2017 O
) O
, O
pages O
491‚Äì496 O
, O
Vancouver O
, O
Canada O
. O
Association O
for O
Computational O
Linguistics O
. O
Ruoyao O
Yang O
, O
Wanying O
Xie O
, O
Chunhua O
Liu O
, O
and O
Dong O
Yu O
. O
2019 O
. O
BLCU O
NLP O
at O
SemEval-2019 O
task O
7 O
: O
An O
inference O
chain O
- O
based O
GPT O
model O
for O
rumour O
evaluation O
. O
In O
Proceedings O
of O
the O
13th O
International O
Workshop O
on O
Semantic O
Evaluation O
, O
pages O
1090 O
‚Äì O
1096 O
, O
Minneapolis O
, O
Minnesota O
, O
USA O
. O
Association O
for O
Computational O
Linguistics O
. O
Li O
Yijing O
, O
Guo O
Haixiang O
, O
Liu O
Xiao O
, O
Li O
Yanan O
, O
and O
Li O
Jinling O
. O
2016 O
. O
Adapted O
ensemble O
classiÔ¨Åcation O
algorithm O
based O
on O
multiple O
classiÔ¨Åer O
system O
and O
feature O
selection O
for O
classifying O
multi O
- O
class O
imbalanced O
data O
. O
Knowledge O
- O
Based O
Systems O
, O
94:88‚Äì104 O
. O
Arkaitz O
Zubiaga O
, O
Ahmet O
Aker O
, O
Kalina O
Bontcheva O
, O
Maria O
Liakata O
, O
and O
Rob O
Procter O
. O
2018 O
. O
Detection O
and O
resolution O
of O
rumours O
in O
social O
media O
: O
A O
survey O
. O
ACM O
Computing O
Surveys O
, O
51(2):32:1‚Äì32:36 O
. O
Arkaitz O
Zubiaga O
, O
Maria O
Liakata O
, O
Rob O
Procter O
, O
Geraldine O
Wong O
Sak O
Hoi O
, O
and O
Peter O
Tolmie O
. O
2016 O
. O
Analysing O
how O
people O
orient O
to O
and O
spread O
rumours O
in O
social O
media O
by O
looking O
at O
conversational O
threads O
. O
Plos O
One O
, O
11(3 O
) O
. O
A O
Confusion O
matrices O
for O
all O
RumourEval O
2019 O
systems O
For O
completeness O
, O
Figure O
4 O
shows O
the O
confusion O
matrices O
of O
all O
systems O
submitted O
to O
RumourEval O
2019 O
. O
Apart O
from O
the O
four O
systems O
discussed O
in O
Section O
3 O
, O
all O
other O
systems O
fails O
to O
correctly O
classify O
the O
large O
majority O
of O
deny O
instances.931Figure O
4 O
: O
Confusion O
matrix O
for O
all O
systems O
from O
RumourEval O
2019.932Author O
Index O
Aharonov O
, O
Ranit O
, O
447 O
Ahmad O
, O
Zishan O
, O
303 O
Aletras O
, O
Nikolaos O
, O
804 O
Amplayo O
, O
Reinald O
Kim O
, O
409 O
Awadallah O
, O
Ahmed O
Hassan O
, O
551 O
Bahar O
, O
Syafri O
, O
843 O
Baldwin O
, O
Timothy O
, O
598 O
Banea O
, O
Carmen O
, O
425 O
Bateni O
, O
Peyman O
, O
664 O
Ben O
Noach O
, O
Matan O
, O
884 O
Bennett O
, O
Paul O
, O
551 O
Benz O
, O
Yannik O
, O
786 O
Bhattacharyya O
, O
Pushpak O
, O
281 O
, O
303 O
, O
858 O
, O
900 O
Biester O
, O
Laura O
, O
425 O
Bing O
, O
Lidong O
, O
258 O
Blache O
, O
Philippe O
, O
224 O
Blain O
, O
Fr√©d√©ric O
, O
366 O
Bollegala O
, O
Danushka O
, O
873 O
Bontcheva O
, O
Kalina O
, O
914 O
, O
925 O
Bowman O
, O
Samuel O
R. O
, O
557 O
, O
672 O
Brahman O
, O
Faeze O
, O
588 O
Cahyawijaya O
, O
Samuel O
, O
843 O
Calixto O
, O
Iacer O
, O
504 O
, O
557 O
Callison O
- O
Burch O
, O
Chris O
, O
328 O
Cardie O
, O
Claire O
, O
551 O
Carenini O
, O
Giuseppe O
, O
516 O
, O
626 O
, O
664 O
Chang O
, O
Kai O
- O
Wei O
, O
609 O
Chang O
, O
Walter O
, O
529 O
Chaturvedi O
, O
Snigdha O
, O
588 O
Chaudhary O
, O
Vishrav O
, O
366 O
Chauhan O
, O
Dushyant O
Singh O
, O
281 O
Chen O
, O
Haotian O
, O
637 O
Chen O
, O
Pei O
, O
811 O
Chen O
, O
Ruijie O
, O
672 O
Chen O
, O
Xiao O
, O
191 O
Chen O
, O
Yubo O
, O
181 O
, O
811 O
Chen O
, O
Yufeng O
, O
726 O
Chen O
, O
Yun O
, O
191 O
Chersoni O
, O
Emmanuele O
, O
224 O
Chi O
, O
Zewen O
, O
12 O
Cho O
, O
Kyunghyun O
, O
334 O
Choi O
, O
Jinho O
D. O
, O
358Chu O
, O
Eric O
, O
643 O
Chua O
, O
Tat O
- O
Seng O
, O
122 O
Cohen O
, O
Shay O
B. O
, O
378 O
Cui O
, O
Xia O
, O
873 O
Dai O
, O
Wenliang O
, O
269 O
Danilevsky O
, O
Marina O
, O
447 O
Deleu O
, O
Johannes O
, O
821 O
Demeester O
, O
Thomas O
, O
821 O
Dernoncourt O
, O
Franck O
, O
529 O
Develder O
, O
Chris O
, O
821 O
Ding O
, O
Yuning O
, O
347 O
Dong O
, O
Li O
, O
12 O
, O
87 O
Du O
, O
Xinya O
, O
551 O
Eger O
, O
Steffen O
, O
786 O
Ekbal O
, O
Asif O
, O
281 O
, O
303 O
, O
900 O
El O
- O
Kishky O
, O
Ahmed O
, O
366 O
, O
616 O
Fei O
, O
Hao O
, O
100 O
Feng O
, O
Qihang O
, O
70 O
Feng O
, O
Yukun O
, O
80 O
Feng O
, O
Zhifan O
, O
735 O
FitzGerald O
, O
Jack O
, O
576 O
Fomicheva O
, O
Marina O
, O
366 O
Fourney O
, O
Adam O
, O
551 O
Fu O
, O
Zihao O
, O
258 O
Fung O
, O
Pascale O
, O
269 O
, O
843 O
Gao O
, O
Tianyu O
, O
745 O
Gao O
, O
Tong O
, O
491 O
Gao O
, O
Yingbo O
, O
212 O
, O
389 O
Garg O
, O
Siddhant O
, O
460 O
Ge O
, O
Tao O
, O
201 O
Glass O
, O
James O
, O
334 O
Goldberg O
, O
Yoav O
, O
884 O
Goldstein O
, O
Felicia O
, O
358 O
Gong O
, O
Ming O
, O
687 O
Gosangi O
, O
Rakesh O
, O
706 O
Guo O
, O
Mengfei O
, O
726 O
Guo O
, O
Yingmei O
, O
37 O
Gupta O
, O
Deepak O
, O
900 O
Gupta O
, O
Vivek O
, O
706 O
Guz O
, O
Grigorii O
, O
664 O
Guzm√°n O
, O
Francisco O
, O
366 O
, O
616 O
933Hackinen O
, O
Brad O
, O
626 O
Hajjar O
, O
Ihab O
, O
358 O
Han O
, O
Xu O
, O
169 O
, O
745 O
Hao O
, O
Yaru O
, O
87 O
Hayashibe O
, O
Yuta O
, O
890 O
He O
, O
Tianxing O
, O
334 O
He O
, O
Yuan O
, O
378 O
Hernandez O
Abrego O
, O
Gustavo O
, O
435 O
Herold O
, O
Christian O
, O
212 O
Horbach O
, O
Andrea O
, O
347 O
Hou O
, O
Lei O
, O
770 O
Htut O
, O
Phu O
Mon O
, O
557 O
Hu O
, O
Chenlong O
, O
80 O
Huang O
, O
Chu O
- O
Ren O
, O
224 O
, O
833 O
Huang O
, O
Gabriel O
, O
470 O
Huang O
, O
Guoping O
, O
1 O
Huang O
, O
Heyan O
, O
12 O
Huang O
, O
Junhong O
, O
70 O
Huang O
, O
Kuan O
- O
Hao O
, O
609 O
Huang O
, O
Minlie O
, O
122 O
, O
248 O
, O
770 O
Huang O
, O
Qi O
, O
491 O
Huang O
, O
Ruihong O
, O
811 O
Huang O
, O
Shaohan O
, O
248 O
Huang O
, O
Xuedong O
, O
536 O
Huang O
, O
Yan O
, O
122 O
Huang O
, O
Yongfeng O
, O
44 O
, O
181 O
Huang O
, O
Zhong O
- O
Yu O
, O
720 O
Iwakura O
, O
Tomoya O
, O
154 O
Jang O
, O
Seongbo O
, O
133 O
Ji O
, O
Donghong O
, O
100 O
Ji O
, O
Haozhe O
, O
248 O
Jia O
, O
Shengyu O
, O
169 O
Jiang O
, O
Wenbin O
, O
726 O
, O
735 O
Jiang O
, O
Xin O
, O
191 O
Jiang O
, O
Yiwei O
, O
821 O
Jin O
, O
Lifeng O
, O
396 O
Joshi O
, O
Manish O
, O
781 O
Jung O
, O
Dawoon O
, O
133 O
Kamigaito O
, O
Hidetaka O
, O
80 O
Kann O
, O
Katharina O
, O
557 O
Kano O
, O
Ryuji O
, O
291 O
Kanojia O
, O
Diptesh O
, O
858 O
Kanouchi O
, O
Shin O
, O
890 O
Kao O
, O
Hung O
- O
Yu O
, O
18 O
, O
143 O
Katsis O
, O
Yannis O
, O
447 O
Katsumata O
, O
Satoru O
, O
163 O
, O
827 O
Kawas O
, O
Ban O
, O
447 O
Ke O
, O
Pei O
, O
248 O
Keller O
, O
Frank O
, O
409Kim O
, O
Doo O
Soon O
, O
529 O
Kim O
, O
Taeuk O
, O
409 O
Kim O
, O
Yu O
- O
Seop O
, O
63 O
Koehn O
, O
Philipp O
, O
582 O
Komachi O
, O
Mamoru O
, O
163 O
, O
827 O
Koto O
, O
Fajri O
, O
598 O
Kumar O
, O
Vishwajeet O
, O
781 O
Kurosawa O
, O
Michiki O
, O
163 O
Lam O
, O
Wai O
, O
258 O
, O
542 O
, O
696 O
Lau O
, O
Jey O
Han O
, O
598 O
Le O
, O
Yuquan O
, O
54 O
Lebanoff O
, O
Logan O
, O
529 O
Lee O
, O
Cheng O
- O
Syuan O
, O
720 O
Lee O
, O
Joohong O
, O
133 O
Leite O
, O
Jo√£o O
Augusto O
, O
914 O
Lenci O
, O
Alessandro O
, O
224 O
Lenka O
, O
Pabitra O
, O
900 O
Li O
, O
Bowen O
, O
409 O
Li O
, O
Chen O
, O
609 O
Li O
, O
Chenliang O
, O
201 O
Li O
, O
Jiawen O
, O
18 O
Li O
, O
Juanzi O
, O
169 O
, O
770 O
Li O
, O
Liangyou O
, O
191 O
Li O
, O
MengYuan O
, O
70 O
Li O
, O
Peng O
, O
169 O
, O
745 O
Li O
, O
Renxuan O
Albert O
, O
358 O
Li O
, O
Tianrui O
, O
687 O
Li O
, O
Weikang O
, O
106 O
Li O
, O
Xi O
, O
637 O
Li O
, O
Xiaohong O
, O
843 O
Li O
, O
Ying O
, O
726 O
Li O
, O
Yuan O
- O
Fang O
, O
781 O
Liang O
, O
Bowen O
, O
435 O
Liang O
, O
Yingyu O
, O
460 O
Liao O
, O
Keng O
- O
Te O
, O
720 O
Liao O
, O
Lizi O
, O
122 O
Lim O
, O
Zhi O
Yuan O
, O
843 O
Lin O
, O
Shou O
- O
de O
, O
720 O
Lin O
, O
Yankai O
, O
745 O
Liu O
, O
Fei O
, O
529 O
Liu O
, O
Haokun O
, O
557 O
Liu O
, O
Kang O
, O
811 O
Liu O
, O
Lemao O
, O
1 O
Liu O
, O
Qun O
, O
191 O
Liu O
, O
Zhiyuan O
, O
169 O
, O
745 O
, O
770 O
Liu O
, O
Zihan O
, O
269 O
Liu O
, O
Zitao O
, O
122 O
Lorr√© O
, O
Jean O
- O
Pierre O
, O
313 O
Lu O
, O
Jing O
, O
653 O
Lu O
, O
Qin O
, O
833 O
Luo O
, O
Gan O
, O
770Luo O
, O
Huaishao O
, O
687 O
Lyu O
, O
Qing O
, O
328 O
Lyu O
, O
Yajuan O
, O
726 O
, O
735 O
Ma O
, O
Xutai O
, O
582 O
Mahata O
, O
Debanjan O
, O
706 O
Mahendra O
, O
Rahmad O
, O
843 O
Mao O
, O
Xian O
- O
Ling O
, O
12 O
Mathias O
, O
Sandeep O
, O
858 O
Matsushita O
, O
Kyoumoto O
, O
154 O
Mihalcea O
, O
Rada O
, O
425 O
Milewski O
, O
Victor O
, O
504 O
Mishra O
, O
Abhijit O
, O
858 O
Miura O
, O
Yasuhide O
, O
291 O
Moens O
, O
Marie O
- O
Francine O
, O
504 O
Mooney O
, O
Raymond O
, O
491 O
Muglich O
, O
Darius O
, O
664 O
Murthy O
, O
Rudra O
, O
858 O
Nadeem O
, O
Moin O
, O
334 O
Narayanan O
Sundararaman O
, O
Mukuntha O
, O
303 O
Neishi O
, O
Masato O
, O
890 O
Ney O
, O
Hermann O
, O
212 O
, O
389 O
Ng O
, O
Vincent O
, O
653 O
Ninomiya O
, O
Takashi O
, O
154 O
Oh O
, O
Byoung O
- O
Doo O
, O
63 O
Ohkuma O
, O
Tomoko O
, O
291 O
Okazaki O
, O
Naoaki O
, O
890 O
Okumura O
, O
Manabu O
, O
80 O
Okura O
, O
Shumpei O
, O
116 O
Omote O
, O
Yutaro O
, O
154 O
Ono O
, O
Shingo O
, O
116 O
Opitz O
, O
Juri O
, O
235 O
Ouchi O
, O
Hiroki O
, O
890 O
Pang O
, O
Bo O
, O
470 O
Parekh O
, O
Zarana O
, O
435 O
Park O
, O
Kyubyong O
, O
133 O
Peng O
, O
Hao O
, O
745 O
Petrusca O
, O
Alexandru O
, O
588 O
Phang O
, O
Jason O
, O
557 O
Pino O
, O
Juan O
, O
582 O
Preotiuc O
- O
Pietro O
, O
Daniel O
, O
804 O
Pruksachatkun O
, O
Yada O
, O
557 O
Pugoy O
, O
Reinald O
Adrian O
, O
143 O
Purwarianti O
, O
Ayu O
, O
843 O
Qi O
, O
Tao O
, O
44 O
, O
181 O
Qian O
, O
Kun O
, O
447 O
Ramakrishnan O
, O
Ganesh O
, O
781 O
Rambelli O
, O
Giulia O
, O
224Ren O
, O
Yafeng O
, O
100 O
Renduchintala O
, O
Adithya O
, O
366 O
Rivera O
, O
Clara O
, O
470 O
Roy O
, O
Deb O
, O
643 O
S O
R O
, O
Dhanush O
, O
281 O
S√°nchez O
Villegas O
, O
Danae O
, O
804 O
Sasaki O
, O
Mei O
, O
116 O
Scarton O
, O
Carolina O
, O
914 O
, O
925 O
Schuler O
, O
William O
, O
396 O
Sen O
, O
Prithviraj O
, O
447 O
Shah O
, O
Rajiv O
Ratn O
, O
706 O
Shang O
, O
Guokan O
, O
313 O
Sharma O
, O
Rohit O
Kumar O
, O
460 O
Shi O
, O
Bei O
, O
258 O
Shi O
, O
Yu O
, O
536 O
, O
687 O
Shou O
, O
Linjun O
, O
687 O
Silva O
, O
Diego O
, O
914 O
, O
925 O
Sim O
, O
Robert O
, O
551 O
Sneyd O
, O
Alison O
, O
759 O
Soleman O
, O
Sidik O
, O
843 O
Soricut O
, O
Radu O
, O
470 O
Specia O
, O
Lucia O
, O
366 O
Stent O
, O
Amanda O
, O
706 O
Stevenson O
, O
Mark O
, O
759 O
Su O
, O
Qi O
, O
833 O
Sujana O
, O
Yudianto O
, O
18 O
Sun O
, O
Maosong O
, O
745 O
Sun O
, O
Shuo O
, O
366 O
Sung O
, O
Yunhsuan O
, O
435 O
Swaminathan O
, O
Avinash O
, O
706 O
Takamura O
, O
Hiroya O
, O
80 O
Tamura O
, O
Akihiro O
, O
154 O
Tang O
, O
Jie O
, O
770 O
Taniguchi O
, O
Tomoki O
, O
291 O
Tixier O
, O
Antoine O
, O
313 O
Trebbi O
, O
Francesco O
, O
626 O
Tu O
, O
Kewei O
, O
93 O
Uppal O
, O
Shagun O
, O
706 O
Vania O
, O
Clara O
, O
557 O
, O
672 O
Vazirgiannis O
, O
Michalis O
, O
313 O
Vijayaraghavan O
, O
Prashanth O
, O
643 O
Vincentio O
, O
Karissa O
, O
843 O
Wadhwa O
, O
Sahil O
, O
637 O
Wan O
, O
Mingyu O
, O
833 O
Wang O
, O
Baoxun O
, O
70 O
Wang O
, O
Chenyu O
, O
770 O
Wang O
, O
Hongfei O
, O
163 O
Wang O
, O
Jin O
, O
27Wang O
, O
Qi O
, O
735 O
Wang O
, O
Qian O
, O
1 O
Wang O
, O
Taifeng O
, O
811 O
Wang O
, O
Wei O
, O
435 O
Wang O
, O
Weiyue O
, O
212 O
, O
389 O
Wang O
, O
Xiaozhi O
, O
169 O
Wang O
, O
Xinyu O
, O
93 O
Wang O
, O
Zongsheng O
, O
70 O
Wei O
, O
Furu O
, O
12 O
, O
87 O
, O
201 O
, O
248 O
Wenjie O
, O
Ying O
, O
54 O
Wilie O
, O
Bryan O
, O
843 O
Winata O
, O
Genta O
, O
843 O
Wu O
, O
Bowen O
, O
70 O
Wu O
, O
Chuhan O
, O
44 O
, O
181 O
Wu O
, O
Fangzhao O
, O
44 O
Wu O
, O
Yunfang O
, O
106 O
Wu O
, O
Zhiyong O
, O
37 O
Xiang O
, O
Rong O
, O
833 O
Xiao O
, O
Chaojun O
, O
745 O
Xiao O
, O
Wen O
, O
516 O
Xing O
, O
Linzi O
, O
626 O
Xiong O
, O
Hantao O
, O
54 O
Xu O
, O
Canwen O
, O
201 O
Xu O
, O
Jinan O
, O
726 O
Xu O
, O
Ke O
, O
87 O
Xu O
, O
Mingxing O
, O
37 O
Xu O
, O
Ruochen O
, O
536 O
Yang O
, O
Hang O
, O
811 O
Yang O
, O
Yaoliang O
, O
745 O
Yang O
, O
Yinfei O
, O
435 O
Yang O
, O
Zijian O
, O
212 O
, O
389 O
Yu O
, O
Jifan O
, O
770 O
Yu O
, O
Liang O
- O
Chih O
, O
27 O
Yu O
, O
Qian O
, O
696 O
Yu O
, O
Tiezheng O
, O
269 O
Yuan O
, O
Li O
, O
27 O
Yuan O
, O
Yifei O
, O
542 O
Yuan O
, O
Zhigang O
, O
181 O
Zaporojets O
, O
Klim O
, O
821 O
Zeng O
, O
Michael O
, O
536 O
Zesch O
, O
Torsten O
, O
347 O
Zhang O
, O
Haimin O
, O
706 O
Zhang O
, O
Haiyang O
, O
759 O
Zhang O
, O
Huan O
, O
70 O
Zhang O
, O
Jiajun O
, O
1 O
Zhang O
, O
Li O
, O
328 O
Zhang O
, O
Wenxuan O
, O
696 O
Zhang O
, O
Xuejie O
, O
27 O
Zhang O
, O
Zheng O
, O
122Zhao O
, O
Jun O
, O
811 O
Zhou O
, O
Jie O
, O
169 O
, O
745 O
Zhou O
, O
Jingbo O
, O
542 O
Zhu O
, O
Chenguang O
, O
536 O
Zhu O
, O
Xiaoyan O
, O
122 O
Zhu O
, O
Yong O
, O
726 O
, O
735 O
Zhu O
, O
Zhenhai O
, O
470 O
Zong O
, O
Chengqing O
, O
1 O
Zukov O
Gregoric O
, O
Andrej O
, O
637 O
