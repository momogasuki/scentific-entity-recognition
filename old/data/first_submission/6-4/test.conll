Neural B-TaskName
discrete I-TaskName
reasoning I-TaskName
( O
Dua O
et O
al O
. O
, O
2019 O
) O
is O
an O
emerging O
technique O
for O
machine O
reading O
comprehension O
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
which O
aims O
at O
answering O
numerical O
questions O
from O
textual O
( O
Dua O
et O
al O
. O
, O
2019 O
) O
or O
hybrid O
( O
Zhu O
et O
al O
. O
, O
2021 O
) O
context O
1 O
. O

The O
pre O
- O
training O
objective O
of O
PEGASUS B-MethodName
( O
Zhang O
et O
al O
. O
, O
2020 O
) O
is O
tailored O
for O
the O
summarization O
task O
, O
which O
predicts O
the O
most O
" O
summary O
worthy O
" O
sentences O
in O
a O
document O
. O

We O
present O
results O
of O
students O
trained O
with O
gold O
labels O
( O
Gold O
) O
and O
regular O
pseudo O
labels O
( O
Regular O
) O
as O
well O
as O
pseudo O
labels O
with O
higher O
and O
random O
attention O
temperatures O
( O
PLATE B-MethodName
B12 I-MethodName
- I-MethodName
3 I-MethodName
λ=1.5 I-MethodName
, O
PLATE B-MethodName
B12 I-MethodName
- I-MethodName
3 I-MethodName
λ=2.0 I-MethodName
and O
PLATE B-MethodName
B12 I-MethodName
- I-MethodName
3 I-MethodName
rnd I-MethodName
) O
. O

Language O
models O
pretrained O
on O
unlabeled O
texts O
have O
substantially O
advanced O
the O
state O
of O
the O
art O
in O
various O
NLP O
tasks O
, O
ranging O
from O
natural B-TaskName
language I-TaskName
understanding I-TaskName
( B-TaskName
NLU I-TaskName
) O
to O
text B-TaskName
generation I-TaskName
( O
Radford O
et O
al O
. O
, O
2018a;Devlin O
et O
al O
. O
, O
2019;Yang O
et O
al O
. O
, O
2019;Radford O
et O
al O
. O
, O
2018b;Raffel O
et O
al O
. O
, O
2020;Brown O
et O
al O
. O
, O
2020 O
) O
. O

. O

NumNet+ B-MethodName
V2 I-MethodName
( O
Ran O
et O
al O
. O
, O
2019 O
) O
, O
a O
numerical B-TaskName
QA I-TaskName
method O
with O
numerically O
- O
aware O
graph O
neural O
network O
. O

However O
, O
because O
the O
hidden B-HyperparameterName
embedding I-HyperparameterName
dimensions I-HyperparameterName
of O
teachers O
and O
students O
are O
different O
in O
our O
setting O
, O
we O
applied O
a O
linear O
transformation O
to O
the O
teacher O
's O
classification O
embedding O
to O
match O
the O
dimension O
with O
the O
student O
model O
. O

Moreover O
, O
we O
conduct O
each O
experiment O
5 B-HyperparameterValue
times O
and O
report O
the O
mean O
F1 B-MetricName
- O
score O
. O

These O
two O
models O
are O
two O
parallel O
tasks O
, O
wherein O
the O
entity O
recognition O
teacher O
focuses O
on O
identifying O
the O
named O
entities O
and O
the O
similarity O
evaluator O
teacher O
is O
to O
decide O
if O
two O
tokens O
are O
in O
the O
same O
type O
. O

Mehri O
et O
al O
. O
( O
2019 O
) O
uses O
supervised O
learning O
to O
bootstrap O
followed O
by O
RL O
fine O
tuning O
, O
whereas O
Zhao O
et O
al O
. O
( O
2019 O
) O
uses O
policy O
gradient O
on O
latent O
action O
space O
as O
against O
handcrafted O
ones O
. O

During O
decoding O
, O
we O
use O
beam B-HyperparameterValue
search I-HyperparameterValue
with O
beam B-HyperparameterName
size I-HyperparameterName
5 B-HyperparameterValue
and O
tweak O
the O
value O
of O
length O
penalty O
on O
the O
development O
set O
. O

as O
Attention(Q O
k O
, O
K O
q O
, O
V O
q O
) O
= O
softmax B-HyperparameterValue
( O
Q O
k O
K O
T O
q O
√ O
dv O
) O
V O
q O

On O
June O
2 O
, O
1970 O
he O
was O
cut O
by O
the O
Cardinals O
. O

In O
the O
second O
example O
, O
our O
model O
attends O
to O
the O
correct O
knowledge O
hyperedges O
considering O
the O
multi O
- O
hop O
facts O
about O
place O
of O
birth O
of O
the O
people O
shown O
in O
the O
given O
image O
, O
and O
infers O
the O
correct O
answer O
. O

The O
average O
document O
length O
is O
78 O
words O
. O

The O
above O
described O
architecture O
is O
illustrated O
in O
Fig O
: O
10 O
.To O
evaluate O
our O
proposed O
method O
on O
Multi B-DatasetName
- I-DatasetName
domain I-DatasetName
Wizard I-DatasetName
- I-DatasetName
of I-DatasetName
- I-DatasetName
Oz I-DatasetName
( O
MultiWoz B-DatasetName
) O
( O
Budzianowski O
et O
al O
. O
, O
2018a O
) O
dataset O
. O

Terry O
grew O
up O
in O
Surrey O
, O
England O
and O
attended O
the O
University O
of O
Sussex O
in O
the O
United O
Kingdom O
, O
graduating O
with O
a O
degree O
in O
english O
literature O
. O

Table O
4 O
shows O
the O
performance O
of O
the O
compared O
methods O
on O
the O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
dataset O
. O

Without O
the O
similarity O
knowledge O
fed O
into O
the O
student O
model O
, O
the O
performance O
drops O
significantly O
. O

We O
have O
two O
considerations O
for O
the O
module O
design O
: O
1 O
) O
the O
module O
should O
recognize O
the O
semantic O
connection O
between O
the O
assumption O
and O
the O
context O
, O
and O
2 O
) O
the O
module O
should O
uniformly O
support O
various O
discrete O
operations O
to O
enable O
accurate O
derivation O
. O

( O
Tsai O
et O
al O
. O
, O
2016 O
) O
48.12 B-MetricValue
60.55 B-MetricValue
61.56 B-MetricValue
WS B-MethodName
( O
Ni O
et O
al O
. O
, O
2017 O
) O
58.50 B-MetricValue
65.10 B-MetricValue
65.40 B-MetricValue
TMP B-MethodName
( O
Jain O
et O
al O
. O
, O
2019 O
) O
61.50 B-MetricValue
73.50 B-MetricValue
69.9 B-MetricValue
BERT B-MethodName
- I-MethodName
f I-MethodName
( O
Wu O
and O
Dredze O
, O
2019 O
) O
69.56 B-MetricValue
74.96 B-MetricValue
77.57 B-MetricValue
AdvCE B-MethodName
( O
Keung O
et O
al O
. O
, O
2019 O
) O
71.90 B-MetricValue
74.3 B-MetricValue
77.6 B-MetricValue
TSL B-MethodName
( O
Wu O
et O
al O
. O
, O
2020a O
) O
73.16 B-MetricValue
76.75 B-MetricValue
80.44 B-MetricValue
Unitrans B-MethodName
( O
Wu O
et O
al O
. O
, O
2020b O
) O
74 O
TSL B-MethodName
( O
Wu O
et O
al O
. O
, O
2020c O
) O
proposes O
a O
teacher O
- O
student O
learning O
model O
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER.Unitrans I-TaskName
( O
Wu O
et O
al O
. O
, O
2020b O
) O
unifies O
a O
data O
transfer O
and O
model O
transfer O
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
. O

Bilinear B-MethodName
attention I-MethodName
networks I-MethodName
exploit O
a O
multi O
- O
head O
co O
- O
attention O
mechanism O
between O
knowledge O
and O
question O
. O

[ O
S O
] O

Particularly O
for O
the O
two O
most O
frequently O
used O
terms O
, O
reproducibility O
and O
replicability O
, O
multiple O
divergent O
definitions O
are O
in O
use O
, O
variously O
conditioned O
on O
same O
vs. O
different O
teams O
, O
methods O
, O
artifacts O
, O
code O
, O
and O
data O
. O

The O
encoders O
produce O
contextualized O
representations O
that O
suit O
natural O
language O
understanding O
tasks O
, O
but O
could O
not O
be O
directly O
applied O
for O
text B-TaskName
generation I-TaskName
. O

To O
reduce O
surface O
- O
level O
variability O
in O
the O
responses O
, O
we O
use O
domain O
- O
adaptive O
delexicalization O
preprocess O
- O
ing O
proposed O
in O
Wen O
et O
al O
. O
( O
2016 O
) O
. O

The O
CC B-DatasetName
- I-DatasetName
News I-DatasetName
dataset O
is O
not O
publicly O
available O
and O
we O
use O
the O
CC B-DatasetName
- I-DatasetName
News I-DatasetName
- I-DatasetName
en I-DatasetName
published O
by O
( O
Mackenzie O
et O
al O
. O
, O
2020 O
) O
. O

Then O
we O
finetune O
the O
pretrained O
GLM B-MethodName
models O
on O
each O
task O
as O
described O
in O
Section O
2.3 O
. O

However O
, O
these O
models O
must O
be O
further O
improved O
for O
tasks O
requiring O
domain O
knowledge O
, O
such O
as O
those O
in O
the O
biomedical O
or O
financial O
domains O
, O
as O
the O
pre O
- O
training O
data O
usually O
consist O
of O
general O
domain O
text O
( O
e.g. O
, O
Wikipedia O
) O
. O

Position O
1 O
1 O
2 O
3 O
4 O
5 O
5 O
5 O
5 O
3 O
3 O
Position O
2 O
0 O
0 O
0 O
0 O
0 O
1 O
2 O
3 O
1 O
2 O
output O
respectively O
. O

Based O
on O
the O
attention O
map O
A O
, O
the O
joint O
feature O
is O
obtained O
as O
follows O
: O
z O

x O
5 O

This O
indicates O
that O
the O
documentlevel O
objective O
, O
which O
teaches O
the O
model O
to O
extend O
the O
given O
contexts O
, O
is O
less O
helpful O
to O
conditional B-TaskName
generation I-TaskName
, O
which O
aims O
to O
extract O
useful O
information O
from O
the O
context O
. O

Especially O
, O
the O
model O
attends O
to O
Q3476753 O
, O
Q290666 O
and O
Ireland O
with O
the O
high O
attention O
score O
0.237 O
, O
0.221 O
, O
and O
0.202 O
. O

Summary O
. O

This O
is O
demonstrated O
in O
Fig O
: O
1 O
. O

We O
stack O
two B-HyperparameterValue
guided B-HyperparameterName
- I-HyperparameterName
attention I-HyperparameterName
blocks I-HyperparameterName
and O
three B-HyperparameterValue
self B-HyperparameterName
- I-HyperparameterName
attention I-HyperparameterName
blocks I-HyperparameterName
, O
respectively O
. O

We O
note O
that O
T5 B-MethodName
is O
pretrained O
with O
a O
similar O
blank O
infilling O
objective O
. O

We O
do O
not O
freeze O
any O
layers O
and O
we O
use O
the O
output O
of O
the O
last O
layer O
as O
our O
hidden O
feature O
vector O
. O

For O
all O
models O
above O
we O
apply O
a O
label B-HyperparameterName
smoothing I-HyperparameterName
of O
0.1 B-HyperparameterValue
to O
prevent O
overfitting O
( O
Pereyra O
et O
al O
. O
, O
2017 O
) O
. O

During O
inference O
, O
as O
common O
wisdom O
, O
we O
apply O
beam O
search O
. O

Recently O
, O
PET O
( O
Schick O
and O
Schütze O
, O
2020a O
, O
b O
) O
proposes O
to O
reformulate O
input O
examples O
as O
cloze B-TaskName
questions I-TaskName
with O
patterns O
similar O
to O
the O
pretraining O
corpus O
in O
the O
few O
- O
shot O
setting O
. O

AdvPicker B-MethodName
proposes O
a O
adversarial O
discriminator O
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
. O

The O
conditional O
probability O
of O
predicting O
y O
given O
x O
is O

For O
β B-HyperparameterName
analysis O
, O
we O
observe O
that O
F1 B-MetricName
- O
score O
are O
increasing O
with O
the O
entity O
similarity O
score O
from O
0.5 O
to O
both O
sides O
0 O
and O
1 O
in O
Figure O
6b O
. O

We O
propose O
2D O
positional O
encodings O
to O
address O
the O
challenge O
. O

The O
first O
positional O
i O
d O
represents O
the O
position O
in O
the O
corrupted O
text O
x O
corrupt O
. O

4 O
) O
PQL-3H B-DatasetName
has O
a O
quite O
limited O
number O
of O
QA O
pairs O
( O
1,031 O
) O
. O

Results O
of O
our O
BART B-MethodName
12 I-MethodName
- I-MethodName
3 I-MethodName
and O
BART B-MethodName
12 I-MethodName
- I-MethodName
6 I-MethodName
student O
models O
are O
in O
the O
third O
and O
fourth O
block O
. O

For O
HAN B-MethodName
, O
the O
hyperedges O
sampled O
by O
stochastic O
graph O
walk O
are O
fed O
into O
the O
co O
- O
attention O
mechanism O
. O

However O
, O
if O
the O
governor O
desires O
to O
appoint O
a O
member O
to O
the O
Wyoming O
state O
senate O
, O
a O
law O
authorizes O
the O
governor O
to O
do O
so O
. O

During O
inference O
, O
we O
need O
to O
either O
know O
or O
enumerate O
the O
length O
of O
the O
answer O
, O
the O
same O
problem O
as O
BERT B-MethodName
. O

The O
qualitative O
results O
indicate O
that O
our O
model O
draws O
reasonable O
inferences O
across O
diverse O
question O
categories O
. O

We O
follow O
the O
experimental O
settings O
suggested O
in O
( O
Wang O
et O
al O
. O
, O
2018 O
) O
. O

Specifically O
, O
GLM B-MethodName
RoBERTa I-MethodName
outperforms O
T5 B-MethodName
Large I-MethodName
but O
is O
only O
half O
its O
size O
. O

Thus O
for O
T5 B-MethodName
and O
GLM B-MethodName
, O
we O
report O
the O
performance O
after O
such O
conversion O
in O
our O
main O
results O
. O

The O
results O
for O
models O
trained O
on O
larger O
corpora O
are O
shown O
in O
Table O
2 O
. O

Attention O
We O
have O
shown O
earlier O
in O
Figure O
1 O
that O
with O
higher O
attention B-HyperparameterName
temperature I-HyperparameterName
, O
cross O
- O
attention O
modules O
of O
a O
teacher O
can O
attend O
to O
later O
parts O
in O
documents O
. O

GLM B-MethodName
515 I-MethodName
M I-MethodName
( O
1.5× O
of O
GPT O
Large O
) O
can O
further O
outperform O
GPT B-MethodName
Large I-MethodName
. O

( O
b O
- O
e O
) O
. O

In O
other O
words O
, O
we O
successfully O
transferred O
domain O
- O
specific O
knowledge O
to O
AL B-MethodName
- I-MethodName
BERT I-MethodName
while O
maintaining O
its O
existing O
advantages O
. O

5 O
Quantitative O
ResultsWe O
all O
settings O
. O

We O
reproduce O
end O
- O
to O
- O
end O
memory O
networks O
( O
Sukhbaatar O
et O
al O
. O
, O
2015 O
) O
proposed O
as O
a O
baseline O
model O
in O
. O

True O
objective O
of O
ToD B-TaskName
is O
human B-MetricName
experience I-MetricName
while O
interacting O
with O
the O
dialogue O
systems O
, O
which O
automatic O
evaluation O
metrics O
might O
fall O
short O
to O
capture O
. O

Other O
things O
being O
equal O
, O
a O
more O
similar O
reproduction O
can O
be O
expected O
to O
produce O
more O
similar O
results O
, O
and O
such O
( O
dis)similarities O
should O
be O
factored O
into O
reproduction O
analysis O
and O
conclusions O
, O
but O
NLP O
lacks O
a O
method O
for O
doing O
so O
. O

In O
the O
1980s O
, O
two O
stations O
were O
constructed O
on O
the O
line O
, O
Corona O
Road O
and O
Corona O
Park O
. O

Thus O
, O
we O
apply O
a O
confidence O
penalty O
regularization O
in O
the O
refinement O
step O
. O

The O
F1 B-MetricName
- O
score O
and O
similarity O
score O
of O
teachers O
are O
all O
higher O
in O
the O
higher O
γ B-HyperparameterName
intervals O
, O
as O
shown O
in O
Figure O
6c O
. O

PQL-3H B-DatasetName
- I-DatasetName
More I-DatasetName
has O
twice O
more O
QA O
pairs O
( O
2,062 O
) O
with O
the O
same O
number O
of O
entities O
, O
relations O
, O
knowledge O
facts O
and O
answers O
as O
PQL-3H.Here B-DatasetName
, O
we O
analyze O
more O
in O
- O
depth O
on O
KVQA B-DatasetName
dataset O
concerning O
i O
) O
categories O
of O
question O
, O
and O
ii O
) O
types O
of O
answer O
selector O
. O

Figure O
1 O
illustrates O
one O
such O
example O
, O
where O
a O
sentence O
from O
the O
BOOKS O
domain O
is O
translated O
to O
the O
MOVIE O
domain O
. O

Although O
the O
above O
- O
mentioned O
models O
solve O
the O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
problem O
to O
some O
extent O
, O
the O
auxiliary O
tasks O
, O
as O
in O
multi O
- O
task O
learning O
, O
have O
not O
been O
studied O
in O
this O
problem O
. O

The O
hyperparameters O
for O
GLM B-MethodName
Doc I-MethodName
and O
GLM B-MethodName
Sent I-MethodName
are O
the O
same O
as O
those O
of O
GLM B-MethodName
Large I-MethodName
. O

When O
applying O
L2I B-MethodName
to O
an O
existing O
NDR B-TaskName
method O
, O
we O
keep O
its O
question O
- O
answering O
objective O
unchanged O
. O

Generally O
, O
a O
transformer O
- O
based O
model O
is O
pretrained O
with O
a O
large O
amount O
of O
text O
data O
in O
an O
unsu- O
* O
Hyunju O
Lee O
is O
the O
corresponding O
author O
. O

In O
either O
case O
, O
the O
attention O
distribution O
is O
too O
sharp O
( O
i.e. O
, O
attention O
weights O
of O
the O
next O
word O
position O
or O
the O
leading O
part O
is O
much O
larger O
than O
other O
positions O
) O
, O
which O
means O
our O
teacher O
model O
is O
over O
- O
confident O
. O

We O
use O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
around O
80 B-HyperparameterValue
documents O
( O
we O
limit O
the O
max B-HyperparameterName
number I-HyperparameterName
of I-HyperparameterName
tokens I-HyperparameterName
on O
each O
GPU O
to O
2048 B-HyperparameterValue
) O
and O
train O
our O
models O
for O
20,000/15,000/6,000 B-HyperparameterValue
steps O
with O
500 B-HyperparameterValue
warmup O
steps O
for O
CNNDM B-DatasetName
, O
XSum B-DatasetName
, O
and O
NYT B-DatasetName
, O
respectively O
. O

In O
this O
paper O
, O
we O
argue O
that O
attention O
distributions O
of O
a O
Seq2Seq O
teacher O
model O
might O
be O
too O
sharp O
. O

The O
model O
generates O
the O
text O
of O
Part O
B O
autoregressively O
. O

The O
ACM O
originally O
had O
these O
definitions O
the O
other O
way O
around O
until O
asked O
by O
ISO O
to O
bring O
them O
in O
line O
with O
the O
scientific O
standard O
( O
ibid O
. O

The O
details O
of O
two O
modules O
, O
guided O
- O
attention O
blocks O
and O
selfattention O
blocks O
, O
are O
described O
as O
below O
. O

Position O
1 O
1 O
2 O
3 O
4 O
5 O
5 O
5 O
5 O
3 O
3 O
Position O
2 O
0 O
0 O
0 O
0 O
0 O
1 O
2 O
3 O
1 O
2 O

The O
performance O
drops O
to O
EM B-MetricName
48.5 B-MetricValue
and O
F B-MetricName
1 I-MetricName
49.0 B-MetricValue
. O

The O
results O
Sequence O
- O
to O
- O
Sequence O
. O

Here O
, O
H O
( O
0 O
) O
is O
the O
word O
embeddings O
of O
each O
entity O
in O
the O
knowledge O
and O
question O
graph O
. O

In O
this O
way O
, O
our O
model O
automatically O
learns O
a O
bidirectional O
encoder O
( O
for O
Part O
A O
) O
and O
a O
unidirectional O
decoder O
( O
for O
Part O
B O
) O
in O
a O
unified O
model O
. O

We O
formulate O
it O
as O
: O
c O
= O
g(c O
, O
a O
) O
, O
where O
the O
counterfactual O
context O
c O
is O
the O
status O
of O
the O
context O
c O
after O
the O
assumption O
a O
is O
executed O
. O

In O
particular O
, O
it O
outperforms O
the O
best O
baselines O
by O
19.8 B-MetricValue
% I-MetricValue
and O
19.7 B-MetricValue
% I-MetricValue
on O
EM B-MetricName
and O
F B-MetricName
1 I-MetricName
, O
respectively O
. O

Each O
span O
is O
replaced O
with O
a O
single O
[ O
MASK O
] O
token O
, O
forming O
a O
corrupted O
text O
x O
corrupt O
. O

CoNLL2002 B-DatasetName
includes O
Spanish O
and O
Dutch O
, O
CoNLL2003 B-DatasetName
includes O
English O
and O
German O
, O
and O
WikiAnn B-DatasetName
includes O
English O
and O
three O
non O
- O
western O
languages O
: O
Arabic O
, O
Hindi O
, O
and O
Chinese O
. O

Recent O
progress O
of O
abstractive B-TaskName
summarization I-TaskName
largely O
relies O
on O
large O
pre O
- O
trained O
Transformer O
models O
( O
Raffel O
et O
al O
. O
, O
2020;Lewis O
et O
al O
. O
, O
2020;Zhang O
et O
al O
. O
, O
2020;Liu O
and O
Lapata O
, O
2019 O
; O
. O

x O
1 O
x O
2 O
x O
3 O
x O
4 O
x O
5 O
x O
6 O
x O
1 O
x O
2 O
[ O
M O
] O
x O
4 O
[ O
M O
] O
[ O
S O
] O
x O
5 O
x O
6 O
[ O
S O
] O
x O
3 O
x O
5 O
x O
6 O
[ O
E O
] O
x O
3 O
[ O
E O
] O
x O
1 O
x O
2 O
[ O
M O
] O
x O
4 O
[ O
M O
] O
[ O
S O
] O
x O
5 O
x O
6 O
[ O
S O
] O
x O
3 O
x O
1 O
x O
2 O
[ O
M O
] O

By O
use O
of O
these O
two O
methods O
, O
we O
demonstrate O
performance O
and O
sample O
efficiency O
. O

As O
shown O
in O
Table O
5 O
, O
the O
application O
of O
the O
calibrated O
teacher O
training O
reduces O
the O
L O
AT O
and O
improves O
the O
classification O
performance O
. O

In O
this O
study O
, O
we O
proposed O
the O
DoKTra B-MethodName
framework O
as O
a O
domain O
knowledge O
transfer O
method O
for O
PLMs O
. O

Downstream O
task O
performance O
as O
well O
as O
the O
scale O
of O
the O
parameters O
have O
also O
constantly O
increased O
in O
the O
past O
few O
years O
. O

In O
order O
to O
have O
an O
intuitive O
feeling O
, O
we O
select O
a O
rep O
- O
resentative O
example O
1 O
and O
visualize O
its O
cross O
attention O
weights O
2 O
( O
see O
the O
left O
graph O
in O
Figure O
1 O
) O
. O

] O
T O
+ O
b O
where O
the O
matrix O
A O
determines O
how O
nodes O
in O
the O
graph O
communicate O
each O
other O
and O
b O
is O
a O
bias O
vector O
. O

In O
summary O
, O
the O
main O
contributions O
are O
as O
follows O
: O

x O
1 O
x O
2 O
x O
3 O
x O
4 O
x O
5 O
x O
6 O
x O
1 O
x O
2 O
[ O
M O
] O
x O
4 O
[ O
M O
] O
[ O
S O
] O
x O
5 O
x O
6 O
[ O
S O
] O
x O
3 O
x O
5 O
x O
6 O
[ O
E O
] O
x O
3 O
[ O
E O
] O
x O
1 O
x O
2 O
[ O
M O
] O
x O
4 O
[ O
M O
] O
[ O
S O
] O
x O
5 O
x O
6 O
[ O
S O
] O
x O
3 O
x O
1 O
x O
2 O
[ O
M O
] O

Less O
copy O
bias O
in O
pseudo O
summaries O
encourages O
student O
models O
to O
be O
more O
abstractive O
, O
while O
less O
leading O
bias O
in O
pseudo O
summaries O
encourages O
student O
models O
to O
take O
advantage O
of O
longer O
context O
in O
documents O
. O

The O
only O
exception O
is O
WiC B-TaskName
( O
word B-TaskName
sense I-TaskName
disambiguation I-TaskName
) O
. O

Fot O
the O
text O
summarization O
task O
, O
we O
use O
the O
dataset O
Gigaword B-DatasetName
( O
Rush O
et O
al O
. O
, O
2015 O
) O
for O
model O
fine O
- O
tuning O
and O
evaluation O
. O

The O
Financial B-DatasetName
PhraseBank I-DatasetName
( O
FPB B-DatasetName
) O
( O
Malo O
et O
al O
. O
, O
2014 O
) O
contains O
sentences O
from O
financial O
news O
annotated O
for O
positive O
, O
neutral O
, O
and O
negative O
sentiments O
. O

The O
experiments O
were O
run O
on O
a O
single O
RTX O
3090 O
24 O
GB O
GPU O
, O
and O
the O
training O
codes O
were O
implemented O
in O
PyTorch O
. O

The O
FinTextSen B-DatasetName
originally O
includes O
2,488 O
tweets O
, O
but O
only O
1,700 O
tweets O
are O
available O
now O
. O

Figure O
3(a O
) O
shows O
the O
validation O
result O
of O
TAGOP B-MethodName
- I-MethodName
L2I I-MethodName
as O
increasing O
the O
matching B-HyperparameterName
block I-HyperparameterName
from O
1 B-HyperparameterValue
to O
4 B-HyperparameterValue
layers O
. O

Here O
, O
we O
note O
that O
BLSTM B-MethodName
and O
MemNN B-MethodName
of O
the O
first O
section O
in O
the O
table O
are O
based O
on O
the O
different O
entity O
linking O
modules O
with O
top-1 O
precision B-MetricName
81.1 B-MetricValue
% I-MetricValue
and O
top-1 O
recall B-MetricName
82.2 B-MetricValue
% I-MetricValue
1 O
. O

For O
tokens O
in O
Part O
A O
, O
their O
second O
positional O
ids O
are O
0 O
. O

We O
tweak O
the O
value O
of O
length O
penalty O
on O
the O
development O
set O
. O

It O
has O
been O
successfully O
applied O
to O
transfer O
learning O
such O
as O
one B-TaskName
- I-TaskName
shot I-TaskName
image I-TaskName
recognition I-TaskName
( O
Koch O
et O
al O
. O
, O
2015 O
) O
, O
text B-TaskName
similarity I-TaskName
( O
Neculoiu O
et O
al O
. O
, O
2016 O
) O
. O

When O
the O
number O
of O
candidate O
answers O
increases O
, O
the O
MLP O
needs O
more O
parameters O
, O
but O
SIM O
does O
not O
. O

By O
doing O
so O
, O
we O
obtain O
the O
result O
of O
entity O
linking O
with O
top-1 O
precision B-MetricName
65.0 B-MetricValue
% I-MetricValue
and O
top-1 O
recall B-MetricName
72.8 B-MetricValue
% I-MetricValue
. O

The O
student O
model O
learns O
two O
source O
language O
patterns O
of O
entity B-TaskName
recognition I-TaskName
and O
entity B-TaskName
similarity I-TaskName
evaluation I-TaskName
. O

In O
the O
setting O
of O
RoBERTa B-MethodName
Large I-MethodName
, O
GLM B-MethodName
RoBERTa I-MethodName
can O
still O
achieve O
improvements O
over O
the O
baselines O
, O
but O
with O
a O
smaller O
margin O
. O

Example O
of O
such O
annotated O
belief O
- O
state O
are O
shown O
in O
Fig O
: O
1 O
. O

The O
benchmark O
is O
usually O
considered O
as O
less O
challenging O
than O
Super B-DatasetName
- I-DatasetName
GLUE I-DatasetName
. O

Formally O
, O

The O
second O
section O
contains O
weakly O
- O
supervised O
models O
learning O
to O
infer O
the O
multi O
- O
hop O
reasoning O
paths O
without O
the O
groundtruth O
path O
annotation O
. O

MinTL B-MethodName
does O
n't O
explicitly O
predict O
dialogue B-TaskName
act I-TaskName
. O

α O
( O
• O
) O
= O
( O
max(ŷ O
T O
i O
) O
) O
2 O
β O
= O
( O
2 O
t O
T O
( O
x O
T O
, O
x O
T O
, O
i O
, O
j O
) O
− O
1 O
) O
2 O
γ O
= O
1 O
− O
|σ(cos(ŷ O
T O
i O
, O
ŷ O
T O
j O
) O
) O
−t O
T O
( O
x O
T O
, O
x O
T O
, O
i O
, O
j)|In O
this O
section O
, O
we O
evaluate O
our O
multiple O
- O
task O
and O
multiple O
- O
teacher O
model O
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
and O
compare O
our O
model O
with O
a O
series O
of O
state O
- O
of O
- O
the O
- O
art O
models O
. O

Part O
A O
tokens O
can O
attend O
to O
each O
other O
, O
but O
can O
not O
attend O
to O
any O
tokens O
in O
B. O
Part O
B O
tokens O
can O
attend O
to O
Part O
A O
and O
antecedents O
in O
B O
, O
but O
can O
not O
attend O
to O
any O
subsequent O
tokens O
in O
B. O
To O
enable O
autoregressive O
generation O
, O
each O
span O
is O
padded O
with O
special O
tokens O
[ O
START O
] O
and O
[ O
END O
] O
, O
for O
input O
and O
Token O

To O
do O
this O
, O
assessment O
has O
to O
be O
done O
in O
a O
way O
that O
is O
also O
comparable O
across O
reproduction O
studies O
of O
different O
original O
studies O
, O
e.g. O
to O
develop O
common O
expectations O
of O
how O
similar O
original O
and O
reproduction B-TaskName
results O
should O
be O
for O
different O
types O
of O
system O
, O
task O
and O
evaluation O
. O

GLM B-MethodName
also O
significantly O
outperforms O
T5 B-MethodName
on O
NLU B-TaskName
and O
generation B-TaskName
tasks O
with O
fewer O
parameters O
and O
data O
. O

We O
evaluate O
the O
model O
's O
ability O
of O
language B-TaskName
modeling I-TaskName
with O
perplexity B-MetricName
on O
BookWiki B-DatasetName
and O
accuracy B-MetricName
on O
the O
LAMBDA B-DatasetName
dataset O
( O
Paperno O
et O
al O
. O
, O
2016 O
) O
. O

Such O
traversal O
, O
called O
graph O
walk O
, O
starts O
from O
the O
node O
linked O
from O
the O
previous O
module O
( O
see O
section O
3.2 O
) O
and O
considers O
all O
entity O
nodes O
associated O
with O
the O
start O
node O
. O

Experiments O
on B-DatasetName
CNN I-DatasetName
/ I-DatasetName
DailyMail I-DatasetName
, O
XSum B-DatasetName
, O
and O
New B-DatasetName
York I-DatasetName
Times I-DatasetName
datasets O
with O
student O
models O
of O
different O
sizes O
show O
PLATE B-MethodName
consistently O
outperforms O
vanilla O
pseudo O
- O
labeling O
methods O
. O

In O
1966 O
, O
the O
NFL O
gave O
players O
$ O
300,000 O
a O
season O
to O
play O
football O
. O

Moreover O
, O
T5 B-MethodName
always O
predicts O
spans O
in O
a O
fixed O
left O
- O
to O
- O
right O
order O
. O

( O
3 O
) O
MTMT B-MethodName
w/o O
similarity O
, O
which O
removes O
the O
similarity O
teacher O
model O
. O

To O
demonstrate O
the O
versatility O
of O
our O
method O
to O
adapt O
to O
different O
metrics O
, O
we O
use O
all O
the O
discussed O
variants O
of O
the O
metric O
. O

Our O
design O
fits O
downstream O
tasks O
as O
usually O
the O
length O
of O
the O
generated O
text O
is O
unknown O
beforehand O
. O

Most O
methods O
above O
are O
designed O
for O
classification O
models O
. O

To O
demonstrate O
the O
effectiveness O
of O
our O
approach O
, O
we O
designed O
the O
following O
ablation O
studies O
. O

SuperGLUE B-DatasetName
consists O
of O
8 O
challenging O
NLU B-TaskName
tasks O
. O

Interestingly O
, O
our O
student O
models O
PLATE B-MethodName
B12 I-MethodName
- I-MethodName
3 I-MethodName
λ=2.0 I-MethodName
and O
PLATE B-MethodName
B12 I-MethodName
- I-MethodName
6 I-MethodName
λ=2.0 I-MethodName
outperform O
all O
models O
in O
comparison O
( O
including O
student O
models O
and O
even O
the O
teacher O
model O
) O
on O
CNNDM B-DatasetName
. O

The O
memory O
- O
based O
methods O
represent O
knowledge O
facts O
in O
a O
form O
of O
memory O
and O
calculate O
soft O
attention O
scores O
of O
each O
memory O
with O
respect O
to O
a O
question O
. O

x O
3 O

The O
proposed O
QRA B-TaskName
method O
produces O
degree B-MetricName
- I-MetricName
of I-MetricName
- I-MetricName
reproducibility I-MetricName
scores O
that O
are O
comparable O
across O
multiple O
reproductions O
not O
only O
of O
the O
same O
, O
but O
of O
different O
original O
studies O
. O

Usually O
, O
the O
assumption O
appears O
either O
before O
of O
after O
the O
factual O
question O
. O

There O
were O
plans O
to O
build O
a O
subway O
extension O
to O
Corona O
, O
but O
it O
was O
never O
built O
. O

Here O
the O
user O
has O
requested O
for O
a O
taxi O
, O
before O
enough O
information O
such O
as O
destination O
or O
time O
of O
departure O
are O
gathered O
, O
the O
agent O
books O
the O
taxi O
. O

All O
the O
datasets O
used O
total O
158 O
GB O
of O
uncompressed O
texts O
, O
close O
in O
size O
to O
RoBERTa B-MethodName
's O
160 O
GB O
datasets O
. O

We O
have O
four O
kinds O
of O
student O
models O
. O

Empirically O
, O
we O
show O
that O
with O
the O
same O
amount O
of O
parameters O
and O
computational O
cost O
, O
GLM B-MethodName
significantly O
outperforms O
BERT B-MethodName
on O
the O
SuperGLUE B-DatasetName
benchmark O
by O
a O
large O
margin O
of O
4.6 O
% O
-5.0 O
% O
and O
outperforms O
RoBERTa B-MethodName
and O
BART B-MethodName
when O
pretrained O
on O
a O
corpus O
of O
similar O
size O
( O
158 O
GB O
) O
. O

Question O
hypergraph O
We O
transform O
a O
question O
sentence O
into O
a O
question O
hypergraph O
H O
q O
consisting O
of O
a O
node O
set O
V O
q O
and O
a O
hyperedge O
set O
E O
q O
. O

As O
shown O
in O
Figure O
2(a O
) O
, O
entity O
linking O
module O
first O
links O
concepts O
from O
query O
( O
a O
given O
image O
- O
question O
pair O
) O
to O
knowledge O
base O
. O

Knowledge O
distillation O
based O
models O
train O
a O
student O
model O
using O
soft O
labels O
of O
the O
target O
language O
( O
Wu O
et O
al O
. O
, O
2020a O
, O
b;Liang O
et O
al O
. O
, O
2021 O
) O
. O

Similar O
to O
the O
sequence O
- O
to O
- O
sequence O
experiments O
, O
we O
use O
an O
AdamW B-HyperparameterValue
optimizer B-HyperparameterName
with O
a O
peak B-HyperparameterName
learning I-HyperparameterName
rate I-HyperparameterName
1e-5 B-HyperparameterValue
and O
6 B-HyperparameterValue
% I-HyperparameterValue
warm B-HyperparameterName
- I-HyperparameterName
up I-HyperparameterName
linear I-HyperparameterName
scheduler I-HyperparameterName
. O

These O
results O
are O
remarkable O
since O
our O
approach O
spent O
only O
a O
few O
hours O
on O
each O
task O
, O
whereas O
RoBERTa B-MethodName
- I-MethodName
PM I-MethodName
may O
require O
several O
days O
and O
billions O
of O
words O
to O
be O
pre O
- O
trained O
. O

Perhaps O
not O
surprisingly O
, O
the O
styles O
of O
summaries O
from O
students O
are O
similar O
with O
these O
from O
their O
teachers O
. O

where O
the O
subscript O
i O
denotes O
the O
i O
- O
th O
index O
of O
column O
vectors O
in O
each O
matrix O
. O

The O
summaries O
are O
extremely O
abstractive O
. O

For O
the O
metric O
M B-MetricName
used O
in O
pairwise O
causal O
reward O
learning O
, O
we O
use O
the O
following O
: O

Our O
framework O
is O
consist O
of O
two O
models O
: O
teacher O
training O
model O
learned O
from O
the O
source O
language O
and O
teacher O
- O
student O
distillation O
learning O
model O
learned O
from O
the O
target O
language O
. O

As O
a O
result O
, O
pseudo O
labels O
generated O
from O
it O
are O
sub O
- O
optimal O
for O
student O
models O
. O

We O
assume O
that O
each O
word O
unit O
( O
a O
word O
or O
named O
entity O
) O
of O
the O
question O
is O
defined O
as O
a O
node O
, O
and O
has O
edges O
to O
adjacent O
nodes O
. O

On O
the O
other O
hand O
, O
MemNN B-MethodName
† I-MethodName
, O
HAN B-MethodName
( O
Kim O
et O
al O
. O
, O
2020 O
) O
, O
and O
BAN B-MethodName
( O
Kim O
et O
al O
. O
, O
2018 O
) O
achieve O
comparatively O
high O
performance O
because O
MemNN B-MethodName
† I-MethodName
adopts O
question O
- O
guided O
soft O
attention O
over O
knowledge O
memories O
. O

Compared O
with O
the O
left O
graph O
, O
the O
right O
graph O
with O
higher O
attention O
temperature O
has O
shorter O
lines O
( O
less O
copy O
bias O
) O
with O
high O
attention O
weights O
, O
and O
positions O
of O
high O
attention O
weights O
extend O
to O
the O
first O
450 O
words O
( O
less O
leading O
bias O
) O
. O

The O
procedure O
of O
the O
DoKTra B-MethodName
framework O
is O
summarized O
in O
Algorithm O
1.Input O
: O
Downstream O
task O
data O
D O
= O
{ O
x O
k O
, O
y O
k O
} O
N O
k=1 O
, O
hyperparameter O
β B-HyperparameterName
1 I-HyperparameterName
, O
β B-HyperparameterName
2 I-HyperparameterName
, O
γ B-HyperparameterName
1 I-HyperparameterName
: O

The O
more O
implementation O
details O
of O
the O
above O
comparative O
models O
is O
described O
as O
follows O
. O

He O
was O
a O
guest O
lecturer O
at O
King O
's O
College O
London O
, O
and O
then O
took O
two O
years O
of O
acting O
courses O
at O
the O
brit O
school O
of O
acting O
to O
prepare O
for O
his O
future O
career O
in O
the O
entertainment O
industry O
. O

In O
particular O
, O
entity O
anonymization O
is O
applied O
to O
all O
relation O
extraction O
datasets O
, O
which O
replace O
the O
entity O
mentions O
with O
anonymous O
tokens O
( O
e.g. O
, O
@GENE$ O
, O
@DISEASE$ O
) O
to O
avoid O
confusion O
in O
using O
complex O
entity O
names O
. O

We O
follow O
the O
same O
experimental O
settings O
suggested O
in O
( O
Zhou O
et O
al O
. O
, O
2018 O
) O
. O

We O
describe O
details O
of O
the O
experimental O
settings O
and O
the O
tuned O
hyperparameters O
for O
each O
dataset O
in O
Appendix O
D. O

Unlike O
the O
competing O
losses O
used O
in O
GANs O
, O
we O
introduce O
cooperative B-HyperparameterValue
losses I-HyperparameterValue
where O
the O
discriminator O
and O
the O
generator O
cooperate O
and O
reduce O
the O
same O
loss O
. O

Therefore O
, O
we O
evaluate O
the O
language B-TaskName
modeling I-TaskName
perplexity B-MetricName
on O
a O
held O
- O
out O
test O
set O
of O
our O
pretraining O
dataset O
, O
which O
contains O
about O
20 B-HyperparameterValue
M I-HyperparameterValue
tokens I-HyperparameterValue
, O
denoted O
as O
BookWiki B-DatasetName
. O

We O
devise O
a O
Learning O
to O
Imagine O
module O
to O
model O
counterfactual O
thinking O
( O
Section O
3.1 O
) O
, O
and O
then O
incorporate O
the O
L2I B-MethodName
module O
( O
Section O
3.2 O
) O
into O
existing O
NRD B-TaskName
methods O
, O
followed O
by O
a O
discussion O
about O
potential O
extensions O
( O
Section O
3.3).Functionally O
speaking O
, O
the O
L2I B-MethodName
module O
aims O
to O
construct O
a O
counterfactual O
context O
based O
on O
the O
factual O
context O
and O
the O
assumption O
. O

In O
total O
, O
we O
obtain O
8,283 O
hypothetical O
questions O
, O
naming O
it O
as O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
. O

We O
use O
teacher B-HyperparameterValue
forcing I-HyperparameterValue
and O
consider O
the O
prediction O
correct O
only O
when O
all O
the O
predicted O
tokens O
are O
correct O
. O

Considering O
the O
available O
baseline O
results O
, O
we O
use O
the O
Gigaword B-DatasetName
dataset O
( O
Rush O
et O
al O
. O
, O
2015 O
) O
for O
abstractive B-TaskName
summarization I-TaskName
and O
the O
SQuAD B-DatasetName
1.1 I-DatasetName
dataset O
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
for O
question B-TaskName
generation I-TaskName
( O
Du O
et O
al O
. O
, O
2017 O
) O
as O
the O
benchmarks O
for O
models O
pretrained O
on O
BookCorpus B-DatasetName
and O
Wikipedia B-DatasetName
. O

However O
, O
since O
the O
autoencoding O
and O
autoregressive O
objectives O
differ O
by O
nature O
, O
a O
simple O
unification O
can O
not O
fully O
inherit O
the O
advantages O
of O
both O
frameworks O
. O

The O
hyperparameters O
for O
GLM B-MethodName
Base I-MethodName
and O
GLM B-MethodName
Large I-MethodName
are O
similar O
to O
those O
used O
by O
BERT B-MethodName
. O

This O
framing O
, O
whether O
the O
same O
conclusions O
can O
be O
drawn O
, O
involves O
subjective O
judgments O
and O
different O
researchers O
can O
come O
to O
contradictory O
con O
- O
clusions O
: O
e.g. O
the O
four O
papers O
( O
Arhiliuc O
et O
al O
. O
, O
2020;Bestgen O
, O
2020;Caines O
and O
Buttery O
, O
2020;Huber O
and O
Çöltekin O
, O
2020 O
) O
reproducing O
Vajjala O
and O
Rama O
( O
2018 O
) O
in O
REPROLANG O
all O
report O
similarly O
large O
differences O
, O
but O
only O
Arhiliuc O
et O
al O
. O
conclude O
that O
reproduction O
was O
unsuccessful O
. O

Firstly O
, O
we O
decide O
to O
choose O
the O
approach O
of O
explicitly O
modeling O
discrete O
operations O
, O
since O
existing O
NDR B-TaskName
solutions O
have O
demonstrated O
its O
superiority O
( O
Dua O
et O
al O
. O
, O
2019;Ran O
et O
al O
. O
, O
2019;Herzig O
et O
al O
. O
, O
2020;Zhu O
et O
al O
. O
, O
2021 O
) O
. O

Hence O
such O
rewards O
are O
sparse O
and O
under O
- O
specified O
( O
Wang O
et O
al O
. O
, O
2020 O
) O
. O

Table O
5 O
shows O
the O
experimental O
results O
on O
four O
relation O
extraction O
tasks O
with O
ALBERT B-MethodName
students O
. O

Since O
then O
information O
retrieval O
- O
based O
methods O
which O
retrieve O
knowledge O
facts O
associated O
with O
a O
question O
and O
conduct O
semantic O
matching O
between O
the O
facts O
and O
the O
question O
are O
introduced O
. O

He O
graduated O
from O
Michigan O
State O
University O
in O
1958 O
with O
a O
degree O
in O
business O
administration O
. O

CASPI(DAMD B-MethodName
) I-MethodName
with O
its O
light O
weight O
model O
architecture O
and O
no O
pretraining O
on O
any O
external O
corpus O
, O
except O
for O
( O
Lubis O
et O
al O
. O
, O
2020 O
) O
, O
out O
perform O
all O
other O
previous O
methods O
, O
these O
includes O
methods O
that O
use O
large O
pretrained O
language O
models O
such O
as O
Hosseini O
- O
Asl O
et O
al O
. O
( O
2020 O
) O
, O
Peng O
et O
al O
. O
( O
2020 O
) O
and O
Lin O
et O
al O
. O
( O
2020 O
) O
. O

In O
Seq2Seq O
learning O
tasks O
such O
as O
summarization B-TaskName
, O
we O
can O
apply O
distillation O
methods O
above O
to O
each O
step O
of O
sequence O
model O
predictions O
. O

For O
question O
hyperedges O
E O
q O
, O
self O
- O
attention O
is O
performed O
in O
a O
similar O
manner O
: O
Attention(Q O
q O
, O
K O
q O
, O
V O
q O
) O
. O

The O
candidate O
labels O
y O
∈ O
Y O
are O
also O
mapped O
to O
answers O
to O
the O
cloze O
, O
called O
verbalizer O
v(y O
) O
. O

Note O
that O
applying O
the O
confidence O
regularizer O
to O
the O
fine O
- O
tuning O
of O
the O
student O
model O
only O
slightly O
improved O
the O
performance O
, O
suggesting O
that O
the O
observed O
gains O
in O
our O
model O
are O
only O
partially O
because O
of O
the O
calibration O
regularizer O
. O

Of O
these O
tasks O
, O
in O
this O
work O
we O
focus O
on O
dialogue O
policy O
management O
to O
improve O
the O
endto O
- O
end O
performance O
of O
ToD. O
The O
need O
for O
sample O
efficiency O
is O
key O
for O
learning O
offline O
task O
- O
oriented O
dialogue O
system O
, O
as O
access O
to O
data O
are O
finite O
and O
expensive O
. O

In O
downstream O
tasks O
, O
only O
one O
of O
the O
sentinel O
tokens O
is O
used O
, O
leading O
to O
a O
waste O
of O
model O
capacity O
and O
inconsistency O
between O
pretraining O
and O
finetuning O
. O

We O
conduct O
experiments O
on O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
dataset O
to O
answer O
the O
following O
questions O
: O
RQ1 O
: O
How O
does O
L2I B-MethodName
perform O
on O
HQA B-TaskName
? O
RQ2 O
: O
What O
factors O
influ- O
Compared O
methods O
. O

In O
conventional O
causal O
inference O
, O
such O
successors O
will O
also O
be O
omitted O
according O
to O
the O
local O
surgery O
principle O
( O
Pearl O
, O
2009 O
) O
. O

The O
models O
are O
trained O
on O
64 B-HyperparameterValue
V100 I-HyperparameterValue
GPUs B-HyperparameterName
for O
200 B-HyperparameterValue
K I-HyperparameterValue
steps B-HyperparameterName
with O
batch B-HyperparameterName
size I-HyperparameterName
of O
1024 B-HyperparameterValue
and O
maximum B-HyperparameterName
sequence I-HyperparameterName
length I-HyperparameterName
of O
512 B-HyperparameterValue
, O
which O
takes O
about O
2.5 O
days O
for O
GLM B-MethodName
Large I-MethodName
. O

Teacher O
/ O
Student O
model O
settings O
We O
use O
BART B-MethodName
Large I-MethodName
( O
Lewis O
et O
al O
. O
, O
2020 O
) O
as O
our O
teacher O
model O
, O
which O
has O
12 B-HyperparameterValue
layers O
in O
the O
encoder O
and O
decoder O
. O

On O
the O
other O
hand O
, O
Transformer B-MethodName
( I-MethodName
SA+GA I-MethodName
) I-MethodName
strongly O
attends O
to O
the O
knowledge O
entity O
of O
person O
( O
Q2439789 O
) O
presented O
in O
the O
image O
with O
undesired O
attention O
score O
0.788 O
. O

Here O
, O
we O
remark O
that O
the O
upper O
bound O
of O
QA B-TaskName
performance O
is O
72.8 B-MetricValue
% I-MetricValue
due O
to O
the O
error O
rate O
of O
entity O
linking O
module O
. O

Here O
, O
each O
node O
representation O
in O
the O
hypergraphs O
is O
updated O
by O
inter O
- O
and O
intra O
- O
attention O
mechanisms O
in O
two O
hypergraphs O
, O
rather O
than O
by O
iterative O
message O
passing O
scheme O
. O

The O
weight B-HyperparameterName
decay I-HyperparameterName
is O
set O
to O
0.0001 B-HyperparameterValue
. O

Currently O
, O
we O
omit O
the O
following O
iterations O
in O
Step O
2 O
of O
L2I B-MethodName
( O
cf O
. O

We O
address O
aforementioned O
shortcomings O
with O
following O
key O
contributions O
: O

To O
observe O
how O
each O
component O
contributed O
to O
the O
proposed O
framework O
, O
we O
conducted O
an O
ablation O
study O
. O

x O
4 O

• O
Sentence O
- O
level O
. O

The O
value O
of O
Equation O
4 O
directly O
refers O
to O
the O
number O
of O
neurons O
activated O
differently O
than O
the O
teacher O
model O
. O

Text O
style O
transfer O
, O
a O
popular O
form O
of O
attribute O
transfer O
, O
regards O
" O
style O
" O
as O
any O
attribute O
that O
changes O
between O
datasets O
( O
Jin O
et O
al O
. O
, O
2020a O
) O
. O

On O
the O
other O
hand O
, O
to O
explicitly O
consider O
relational O
structure O
between O
knowledge O
facts O
, O
graph O
- O
based O
methods O
construct O
a O
query O
- O
aware O
knowledge O
graph O
by O
retrieving O
facts O
from O
KB O
and O
perform O
graph O
reasoning O
for O
a O
question O
. O

Table O
3 O
shows O
the O
classification O
performance O
of O
BioBERT B-MethodName
, O
RoBERTa B-MethodName
- I-MethodName
PM I-MethodName
, O
and O
our O
approach O
in O
five O
biomedical O
and O
clinical O
tasks O
. O

Perplexity B-MetricName
is O
an O
evaluation O
criterion O
that O
has O
been O
changes O
to O
the O
senate O
. O

Following O
the O
paper O
, O
we O
split B-HyperparameterName
the O
dataset O
into O
train O
, O
validation O
, O
and O
test O
sets O
with O
a O
proportion O
of O
8:1:1 B-HyperparameterValue
, O
and O
report O
the O
average B-MetricName
accuracy I-MetricName
of O
five O
repeated O
runs O
on O
different O
data O
split O
. O

The O
hyperparameters O
except O
Transformer O
architecture O
for O
GLM B-MethodName
410 I-MethodName
M I-MethodName
and O
GLM B-MethodName
515 I-MethodName
M I-MethodName
are O
the O
same O
as O
those O
of O
GLM B-MethodName
Large I-MethodName
. O

[ O
S O
] O

This O
paper O
aims O
to O
distill O
these O
large O
Transformer O
summarization O
models O
into O
smaller O
ones O
with O
minimal O
loss O
in O
performance O
. O

[ O
S O
] O

In O
all O
cases O
, O
we O
set O
the O
batch B-HyperparameterName
size I-HyperparameterName
to O
the O
maximum O
that O
a O
single O
GPU O
can O
process O
, O
with O
128 B-HyperparameterValue
being O
the O
maximum B-HyperparameterName
sequence I-HyperparameterName
length I-HyperparameterName
. O

To O
this O
end O
, O
we O
devise O
four O
key O
building O
blocks O
for O
the O
L2I B-MethodName
module O
: O
• O
Encoder O
. O

Assume O
that O
NDR B-TaskName
model O
equipped O
with O
L2I B-MethodName
can O
answer O
the O
hypothetical O
questions O
requiring O
one O
- O
iteration O
derivation O
( O
i.e. O
, O
c O
i O
→ O
c O
i O
) O
. O

For O
entity O
linking O
, O
we O
apply O
well O
- O
known O
pre O
- O
trained O
models O
for O
face B-TaskName
identification I-TaskName
: O
RetinaFace B-MethodName
( O
Deng O
et O
al O
. O
, O
2020 O
) O
for O
face B-TaskName
detection I-TaskName
and O
ArcFace B-MethodName
( O
Deng O
et O
al O
. O
, O
2019 O
) O
for O
face B-TaskName
feature I-TaskName
extraction I-TaskName
. O

All O
compared O
methods O
are O
initialized O
with O
the O
model O
trained O
on O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
and O
then O
fine O
- O
tuned O
on O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
. O

For O
more O
details O
on O
the O
model O
architecture O
and O
parameter O
setting O
refer O
Zhang O
et O
al O
. O
( O
2019 O
) O
. O

1 O
) O
Appropriateness B-MetricName
: O
Are O
the O
generated O
responses O
appropriate O
for O
the O
given O
context O
in O
the O
dialogue O
turn O
? O
2 O
) O
Fluency B-MetricName
: O
Are O
the O
generated O
responses O
coherent O
and O
comprehensible O
? O

We O
find O
that O
the O
proposed O
method O
facilitates O
insights O
into O
causes O
of O
variation O
between O
reproductions O
, O
and O
allows O
conclusions O
to O
be O
drawn O
about O
what O
changes O
to O
system O
and/or O
evaluation O
design O
might O
lead O
to O
improved O
reproducibility O
. O

Comparison O
with O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

We O
fine O
- O
tuned O
the O
RoBERTa B-MethodName
- I-MethodName
PM I-MethodName
for O
each O
task O
. O

Since O
GCN B-MethodName
( O
Kipf O
and O
Welling O
, O
2017 O
) O
and O
GGNN B-MethodName
encode O
question O
and O
knowledge O
graph O
separately O
, O
they O
do O
not O
learn O
interactions O
between O
question O
and O
knowledge O
. O

To O
this O
end O
we O
conduct O
human O
evaluation O
on O
the O
quality O
of O
the O
generated O
response O
. O

He O
was O
also O
a O
voice O
actor O
for O
" O
the O
Simpsons O
" O
as O
well O
as O
" O
the O
marvelous O
misadventures O
of O
superman O
. O

We O
formulate O
them O
as O
blank B-TaskName
infilling I-TaskName
tasks O
, O
following O
( O
Schick O
and O
Schütze O
, O
2020b O
) O
. O

All O
the O
numbers O
of O
CASPI B-MethodName
reported O
in O
this O
work O
are O
median O
of O
5 B-HyperparameterValue
runs O
with O
different O
seeds O
. O

Since O
GLM B-MethodName
learns O
the O
bidirectional O
attention O
, O
we O
also O
evaluate O
GLM B-MethodName
under O
the O
setting O
in O
which O
the O
contexts O
are O
encoded O
with O
bidirectional O
attention O
. O

× O
× O
× O
× O
× O
× O
× O
× O
× O
× O
× O
× O
× O
× O
× O
× O
× O
× O
× O
× O
× O
× O
× O
× O
× O
× O
× O
× O
× O
× O
× O
× O
× O
× O
× O
( O

We O
observe O
that O
GLM B-MethodName
Large I-MethodName
can O
achieve O
performance O
matching O
the O
other O
pretraining O
models O
on O
the O
two O
generation O
tasks O
. O

It O
has O
been O
shown O
that O
combined O
with O
gradient O
- O
based O
finetuning O
, O
PET O
can O
achieve O
better O
performance O
in O
the O
few O
- O
shot O
setting O
than O
GPT-3 B-MethodName
while O
requiring O
only O
0.1 O
% O
of O
its O
parameters O
. O

In O
sentiment B-TaskName
classification I-TaskName
, O
the O
labels O
" O
positive O
" O
and O
" O
negative O
" O
are O
mapped O
to O
the O
words O
" O
good O
" O
and O
" O
bad O
" O
. O

For O
similarity O
- O
based O
answer O
, O
we O
calculate O
a O
dot O
product O
similarity O
p O
= O
zC O
T O
between O
z O
and O
answer O
candidate O
set O
C O
∈ O
R O
|A|×w O
where O
|A| O
is O
a O
number O
of O
candidate O
answers O
and O
w O
is O
a O
dimension O
of O
representation O
for O
each O
answer O
. O

In O
the O
teacher O
training O
model O
, O
there O
are O
two O
sub O
- O
models O
, O
i.e. O
an O
entity O
recognizer O
teacher O
and O
a O
similarity O
evaluator O
teacher O
. O

x O
5 O

These O
methods O
mainly O
adopt O
an O
iterative O
message O
passing O
process O
to O
propagate O
information O
between O
adjacent O
nodes O
in O
the O
graph O
. O

To O
compare O
with O
SOTA O
models O
, O
we O
also O
train O
a O
Large O
- O
sized O
model O
with O
the O
same O
data O
, O
tokenization O
, O
and O
hyperparameters O
as O
RoBERTa B-MethodName
, O
denoted O
as O
GLM B-MethodName
RoBERTa I-MethodName
. O

We O
introduced O
hyperparamter O
λ B-HyperparameterName
to O
normalize O
the O
achievable O
scale O
of O
BLEU B-MetricName
. O

Note O
that O
we O
also O
tried O
more O
extreme O
λ B-HyperparameterName
values O
as O
shown O
in O
Appendix O
B O
, O
and O
we O
find O
the O
value O
of O
1.5 B-HyperparameterValue
or O
2.0 B-HyperparameterValue
works O
better O
than O
others O
. O

He O
plays O
characters O
in O
several O
films O
, O
including O
" O
" O
, O
" O
" O
, O
" O
" O
and O
" O
" O
. O

Following O
the O
previous O
work O
( O
Zhou O
et O
al O
. O
, O
2018 O
) O
, O
we O
call O
this O
setting O
under O
weak O
supervision O
. O

To O
encourage O
teacher O
models O
to O
generate O
pseudo O
labels O
with O
more O
diversity O
, O
we O
further O
propose O
to O
use O
a O
random O
λ B-HyperparameterName
for O
each O
input O
document O
( O
λ B-HyperparameterName
∼ O
U O
[ O
a B-HyperparameterName
, O
b B-HyperparameterName
] O
) O
. O

The O
only O
difference O
is O
the O
number B-HyperparameterName
of I-HyperparameterName
spans I-HyperparameterName
and O
the O
span B-HyperparameterName
lengths I-HyperparameterName
. O

To O
validate O
the O
effectiveness O
of O
our O
proposed O
L2I B-MethodName
module O
, O
we O
apply O
it O
to O
TAGOP B-MethodName
, O
obtaining O
an O
NDR B-TaskName
model O
for O
HQA B-TaskName
, O
named O
TAGOP B-MethodName
- I-MethodName
L2I. I-MethodName
In O
addition O
to O
the O
vanilla O
TAGOP B-MethodName
, O
we O
compare O
our O
method O
against O
representative O
methods O
of O
traditional B-TaskName
QA I-TaskName
, O
numerical B-TaskName
QA I-TaskName
, B-TaskName
tabular I-TaskName
QA I-TaskName
, O
and O
hybrid B-TaskName
QA I-TaskName
. O

This O
objective O
aims O
for O
seq2seq O
tasks O
whose O
predictions O
are O
often O
complete O
sentences O
or O
paragraphs O
. O

We O
observe O
that O
the O
accuracy B-MetricName
on O
PQL-3H B-DatasetName
is O
relatively O
lower O
than O
the O
other O
splits O
. O

Each O
GGNN B-MethodName
model O
consists O
of O
three B-HyperparameterValue
gated B-HyperparameterName
recurrent I-HyperparameterName
propagation I-HyperparameterName
layers I-HyperparameterName
and O
a O
graphlevel O
aggregator O
. O

We O
follow O
( O
Shen O
et O
al O
. O
, O
2020 O
) O
and O
evaluate O
text B-TaskName
infilling I-TaskName
performance O
on O
the O
Yahoo B-DatasetName
Answers I-DatasetName
dataset O
( O
Yang O
et O
al O
. O
, O
2017 O
) O
, O
which O
contains O
100K/10K/10 B-HyperparameterValue
K I-HyperparameterValue
documents I-HyperparameterValue
for O
train B-HyperparameterName
/ I-HyperparameterName
valid I-HyperparameterName
/ I-HyperparameterName
test I-HyperparameterName
respectively O
. O

In O
this O
light O
, O
we O
consider O
modeling O
counterfactual O
thinking O
as O
neural O
network O
modules O
that O
can O
be O
seamlessly O
incorporated O
into O
existing O
NDR B-TaskName
models O
. O

Siamese B-MethodName
Network I-MethodName
is O
originally O
introduced O
by O
( O
Bromley O
et O
al O
. O
, O
1994 O
) O
to O
treat O
signature B-TaskName
verification I-TaskName
as O
a O
matching O
problem O
. O

Specifically O
, O
we O
re O
- O
scale O
attention O
weights O
in O
all O
attention O
modules O
with O
a O
higher O
temperature O
, O
which O
leads O
to O
softer O
attention O
distributions O
. O

T5 B-MethodName
uses O
independent O
positional O
encodings O
for O
the O
encoder O
and O
decoder O
, O
and O
relies O
on O
multiple O
sentinel O
tokens O
to O
differentiate O
the O
masked O
spans O
. O

In O
the O
fifth O
block O
, O
we O
additionally O
conduct O
selfdistillation O
experiments O
, O
which O
is O
not O
the O
focus O
of O
this O
work O
. O

Then O
, O
it O
infers O
an O
answer O
by O
attending O
to O
knowledge O
evidence O
with O
high O
attention O
scores O
. O

Smith O
was O
born O
in O
La O
Canada O
Flintridge O
, O
Michigan O
, O
in O
1938 O
. O

The O
student O
model O
learns O
less O
from O
unreasonable O
results O
, O
and O
it O
can O
make O
more O
accurate O
entity B-TaskName
recognition I-TaskName
for O
the O
target O
language O
. O

GLM B-MethodName
: O
In O
his O
four O
- O
year O
NFL O
career O
, O
he O
played O
in O
33 O
games O
and O
started O
14 O
, O
registering O
62 O
career O
interceptions O
. O

e O
k O
= O
ϕ O
k O
• O
f O
k O
( O
h O
k O
) O
∈ O
R O
d O
, O
e O
q O
= O
ϕ O
q O
• O
f O
q O
( O
h O
q O
) O
∈ O
R O
d O
where O
h O
[ O
• O
] O
is O
a O
hyperedge O
in O
E O
[ O
• O
] O
. O

This O
show O
using O
CASPI B-MethodName
to O
shepard O
the O
gradient O
update O
process O
as O
sample O
weights O
for O
each O
dialogue O
turn O
leads O
to O
a O
model O
that O
's O
well O
aligned O
with O
true O
objective O
of O
the O
task O
. O

Table O
5 O
presents O
the O
results O
. O

2 O
. O

GLM B-MethodName
Sent I-MethodName
can O
perform O
better O
than O
GLM B-MethodName
Large I-MethodName
, O
while O
GLM B-MethodName
Doc I-MethodName
performs O
slightly O
worse O
than O
GLM B-MethodName
Large I-MethodName
. O

Our O
model O
shows O
notable O
strengths O
especially O
on O
complex O
problems O
such O
as O
Comparison O
, O
Multi O
- O
entity O
or O
Subtraction O
. O

Because O
the O
entropy O
regularizer O
in O
calibrated O
teacher O
training O
issues O
penalties O
based O
on O
the O
output O
probability O
distribution O
, O
it O
is O
difficult O
to O
intuitively O
understand O
how O
it O
positively O
affects O
activation O
boundary O
distillation O
, O
which O
uses O
hidden O
representation O
. O

We O
set O
our O
hyperparameters O
empirically O
following O
( O
Wu O
et O
al O
. O
, O
2020c O
) O
with O
some O
modifications O
. O

The O
first O
section O
in O
the O
table O
includes O
fully O
- O
supervised O
models O
which O
require O
a O
ground O
- O
truth O
path O
annotation O
as O
an O
additional O
supervision O
. O

Although O
the O
TAPT B-MethodName
performance O
improved O
when O
the O
batch B-HyperparameterName
size I-HyperparameterName
increased O
through O
distributed O
training O
, O
the O
improvement O
was O
inadequate O
. O

i O
) O
To O
answer O
a O
complex O
question O
, O
multi O
- O
hop O
reasoning O
over O
multiple O
knowledge O
evidences O
is O
necessary O
. O

We O
obtain O
the O
two O
numbers O
above O
by O
matching O
each O
sentence O
in O
a O
summary O
with O
the O
sentence O
in O
its O
original O
document O
that O
can O
produce O
maximum O
ROUGE B-MetricName
( O
Lin O
, O
2004 O
) O
score O
between O
them O
. O

After O
applying O
the O
pre O
- O
processing O
procedures O
described O
in O
Durrett O
et O
al O
. O
( O
2016 O
) O
; O
Liu O
and O
Lapata O
( O
2019 O
) O
, O
we O
first O
obtain O
110,540 O
articles O
with O
abstractive O
summaries O
. O

In O
the O
previous O
section O
, O
GLM B-MethodName
masks O
short O
spans O
and O
is O
suited O
for O
NLU B-TaskName
tasks O
. O

x O
3 O

4 O
) O
The O
performance O
achieved O
is O
still O
low O
w.r.t O
. O

x3 O

For O
TAPT B-MethodName
, O
we O
additionally O
pre O
- O
trained O
the O
RoBERTa B-MethodName
- I-MethodName
large I-MethodName
model O
with O
each O
pre O
- O
processed O
downstream O
task O
's O
training O
data O
. O

Greedy O
agent O
: O
In O
certain O
domains O
, O
the O
agents O
has O
a O
tendency O
to O
book O
a O
service O
before O
it O
has O
gathered O
all O
the O
required O
information O
or O
before O
the O
user O
requested O
or O
agreed O
for O
booking O
a O
service O
. O

Empirically O
we O
show O
that O
GLM B-MethodName
outperforms O
previous O
methods O
for O
NLU B-TaskName
tasks O
and O
can O
effectively O
share O
parameters O
for O
different O
tasks O
. O

2 O
) O
Across O
the O
groups O
, O
TAGOP B-MethodName
achieves O
relatively O
good O
performance O
on O
the O
SWAP O
group O
, O
which O
replaces O
the O
target O
fact O
with O
a O
number O
in O
the O
assumption O
. O

He O
was O
also O
traded O
to O
the O
St. O
Louis O
Cardinals O
for O
a O
second O
round O
pick O
in O
the O
1970 O
draft O
. O

This O
is O
because O
not O
all O
successors O
are O
necessary O
for O
answering O
the O
question O
. O

We O
finetune O
GLM B-MethodName
LARGE I-MethodName
on O
the O
training O
set O
for O
4 B-HyperparameterValue
epochs B-HyperparameterName
with O
AdamW B-HyperparameterValue
optimizer B-HyperparameterName
. O

x O
= O
[ O
x O
1 O
, O
• O
• O
• O
, O
x O
n O
] O
, O
multiple O
text O
spans O
{ O
s O
1 O
, O
• O
• O
• O
, O
s O
m O
} O
are O
sampled O
, O
where O
each O
span O
s O
i O
corresponds O
to O
a O
series O
of O
consecutive O
tokens O
[ O
s O
i,1 O
, O
• O
• O
• O
, O
s O
i O
, O
l O
i O
] O
in O
x. O

Both O
GLM B-MethodName
and O
XLNet B-MethodName
are O
pretrained O
with O
autoregressive O
objectives O
, O
but O
there O
are O
two O
differences O
between O
them O
. O

Autoregressive O
models O
, O
such O
as O
GPT B-MethodName
( O
Radford O
et O
al O
. O
, O
2018a O
) O
, O
learn O
left O
- O
to O
- O
right O
language O
models O
. O

In O
his O
two O
years O
as O
a O
reserve O
cornerback O
, O
he O
led O
the O
conference O
in O
interceptions O
with O
five O
. O

is O
wrong O
even O
though O
it O
attends O
correctly O
to O
the O
first O
knowledge O
hyperedge O
{ O
Wallace O
Reid O
⪯ O
spouse O
⪯ O
Dorothy O
Davenport O
⪯ O
parents O
⪯ O
Harry O
Davenport O
⪯ O
cause O
of O
death O
⪯ O
Myocardial O
Infarction O
} O
. O

[ O
S O
] O

In O
the O
student O
model O
, O
we O
then O
borrow O
the O
idea O
of O
multitask O
learning O
to O
incorporate O
a O
similarity B-TaskName
evaluation I-TaskName
task O
as O
an O
auxiliary O
task O
into O
the O
entity B-TaskName
recognition I-TaskName
classifier O
. O

The O
output O
of O
this O
optimized O
using O
binary O
crossentopy O
loss O
described O
in O
Eqn:4 O
. O

The O
governor O
can O
also O
appoint O
members O
of O
the O
wyoming O
senate O
. O

For O
instance O
, O
it O
can O
be O
a O
combination O
of O
the O
cross B-MetricName
- I-MetricName
entropy I-MetricName
( O
CE B-MetricName
) O
loss O
over O
the O
operand O
look O
- O
up O
and O
the O
CE B-MetricName
loss O
over O
the O
choice O
of O
discrete O
operation O
( O
Herzig O
et O
al O
. O
, O
2020;Zhu O
et O
al O
. O
, O
2021 O
) O
. O

To O
compare O
with O
GLM B-MethodName
RoBERTa B-MethodName
, O
we O
choose O
T5 B-MethodName
, O
BART B-MethodName
Large I-MethodName
, O
and O
RoBERTa B-MethodName
Large I-MethodName
as O
our O
baselines O
. O

Our O
model O
responds O
by O
focusing O
on O
{ O
second O
⪯ O
from O
⪯ O
left O
} O
phrase O
of O
the O
question O
and O
four O
facts O
having O
a O
left O
relation O
among O
86 O
knowledge O
hyperedges O
. O

Note O
that O
the O
HoC B-DatasetName
dataset O
is O
a O
multi O
- O
label O
document B-MethodName
classification I-MethodName
task O
predicting O
the O
combination O
of O
labels O
from O
an O
input O
text O
. O

To O
further O
investigate O
the O
difference O
in O
answering O
factual O
and O
hypothetical O
questions O
, O
we O
test O
TAGOP B-MethodName
- I-MethodName
L2I I-MethodName
on O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
. O

T5 B-MethodName
has O
no O
direct O
match O
in O
the O
number O
of O
parameters O
for O
BERT B-MethodName
Large I-MethodName
, O
so O
we O
present O
the O
results O
of O
both O
T5 B-MethodName
Base I-MethodName
( O
220 O
M O
parameters O
) O
and O
T5 B-MethodName
Large I-MethodName
( O
770 O
M O
parameters O
) O
. O

Although O
not O
covered O
in O
this O
paper O
, O
an O
interesting O
future O
work O
is O
to O
construct O
heterogeneous O
knowledge O
graph O
that O
includes O
more O
diverse O
knowledge O
sources O
( O
e.g. O
documents O
on O
web O
) O
. O

x O
3 O

Actually O
, O
it O
is O
a O
common O
approach O
for O
current O
state O
- O
of O
- O
the O
- O
art O
NDR B-TaskName
models O
to O
apply O
a O
set O
of O
defined O
operators O
( O
Ran O
et O
al O
. O
, O
2019;Chen O
et O
al O
. O
, O
2020a;Zhu O
et O
al O
. O
, O
2021 O
) O
. O

Query O
- O
aware O
knowledge O
hypergraph O
A O
knowledge O
base O
( O
KB O
) O
, O
a O
vast O
amount O
of O
general O
knowledge O
facts O
, O
contains O
not O
only O
knowledge O
facts O
required O
to O
answer O
a O
given O
question O
but O
also O
unnecessary O
knowledge O
facts O
. O

+ O
A O
Q O
J O
o O
0 O
q O
7 O
7 O
r O
c O
1 O
N O
7 O
+ O
w O
u O
L O
R O
c O
W O
L O
F O
X O
1 O
9 O
Y O
3 O
N O
o O
t O
b O
2 O
3 O
U O
l O
U O
o O
l O
J O
D O
Q O
s O
m O
Z O
D O
N O
A O
i O
j O
D O
K O
S O
U O
1 O
T O
z O
U O
g O
z O
k O
Q O
T O
F O
A O
S O
O O
N O
o O
H O
+ O
R O
+ O
4 O
1 O
b O
I O
h O
U O
V O
/ O
F O
o O
P O
E O
u O
L O
H O
q O
M O
t O
p O
R O
D O
H O
S O
R O
r O
p O
p O
B O
4 O
K O
F O
a O
h O
C O
b O
K O
7 O
s O
f O
d O
o O
o O
l O
t O
+ O
y O
O O
4 O
M O
w O
S O
b O
0 O
J O
K O
Z O
x O
/ O
2 O
a O
f O
L O
y O
Z O
V O
c O
7 O
x O
c O
9 O
2 O
K O
H O
A O
a O
E O
6 O
4 O
x O
Q O
0 O
q O
1 O
P O
D O
f O
R O
f O
o O
a O
k O
p O
p O
i O
R O
o O
d O
1 O
O O
F O
U O
k O
Q O
7 O
q O
M O
u O
a O
R O
n O
K O
U O
U O
y O
U O
n O
4 O
1 O
S O
D O
5 O
1 O
9 O
o O
4 O
R O
O O
J O
K O
Q O
5 O
X O
D O
s O
j O
9 O
f O
d O
G O
h O
m O
K O
V O
R O
z O
O O
T O
M O
d O
I O
9 O
N O
e O
3 O
l O
4 O
n O
9 O
e O
K O
9 O
X O
R O
i O
Z O
9 O
R O
n O
q O
S O
a O
c O
D O
x O
+ O
K O
E O
q O
Z O
o O
4 O
W O
T O
V O
+ O
C O
E O
V O
B O
K O
s O
2 O
c O
A O
Q O
h O
C O
U O
1 O
W O
R O
3 O
c O
Q O
x O
J O
h O
b O
Y O
q O
y O
T O
Q O
n O
e O
9 O
J O
d O
n O
S O
f O
2 O
w O
7 O
B O
2 O
V O
3 O
S O
u O
3 O
V O
D O
m O
H O
M O
Q O
q O
w O
C O
3 O
t O
w O
A O
B O
4 O
c O
Q O
w O
U O
u O
o O
Q O
o O
1 O
w O
C O
D O
h O
A O
Z O
7 O
g O
2 O
b O
q O
z O
HTypically O
, O
for O
downstream O
NLU B-TaskName
tasks O
, O
a O
linear O
classifier O
takes O
the O
representations O
of O
sequences O
or O
tokens O
produced O
by O
pretrained O
models O
as O
input O
and O
predicts O
the O
correct O
labels O
. O

) O
where O
i O
and O
j O
are O
a B-HyperparameterValue
single I-HyperparameterValue
layer I-HyperparameterValue
feed B-HyperparameterName
- I-HyperparameterName
forward I-HyperparameterName
layer I-HyperparameterName
, O
respectively O
. O

We O
observe O
that O
students O
behave O
similarly O
, O
and O
we O
put O
more O
cross O
attention O
visualization O
of O
students O
in O
Appendix O
F. O
To O
obtain O
corpus O
- O
level O
statistics O
, O
we O
further O
calculate O
the O
evident B-MetricName
crossattention I-MetricName
weight I-MetricName
distributions I-MetricName
of O
the O
teacher O
when O
generating O
pseudo O
labels O
on O
the O
training O
set O
of O
CN B-DatasetName
- I-DatasetName
NDM I-DatasetName
. O

Each O
hyperedge O
connects O
an O
arbitrary O
number O
of O
nodes O
and O
has O
partial O
order O
itself O
, O
i.e. O
, O
h O

To O
analyze O
the O
effectiveness O
of O
hypergraph O
- O
based O
input O
representation O
, O
we O
conduct O
comparative O
experiments O
on O
the O
different O
types O
of O
input O
formats O
for O
Transformer O
architecture O
. O

The O
target O
of O
HQA B-TaskName
is O
to O
learn O
y O
= O
f O
( O
q O
, O
c O
, O
a O
) O
where O
a O
denotes O
the O
assumption O
. O

Our O
main O
contributions O
are O
as O
follows O
: O

x O
3 O

For O
multi O
- O
task O
pretraining O
, O
we O
train O
two O
Largesized O
models O
with O
a O
mixture O
of O
the O
blank O
infilling O
objective O
and O
the O
document O
- O
level O
or O
sentencelevel O
objective O
, O
denoted O
as O
GLM B-MethodName
Doc I-MethodName
and O
GLM B-MethodName
Sent I-MethodName
. O

Position O
1 O
1 O
2 O
3 O
4 O
5 O
5 O
5 O
5 O
3 O
3 O
Position O
2 O
0 O
0 O
0 O
0 O
0 O
1 O
2 O
3 O
1 O
2 O

p(y|x O
) O
= O
p(v(y)|c(x O
) O
) O
y O
∈Y O
p(v(y O
) O
|c(x O
) O
) O
( O
3 O

Table O
1 O
shows O
the O
results O
. O

While O
Turn#4 O
contributes O
least O
to O
successful O
outcome O
. O

We O
first O
assign O
a O
name O
of O
the O
detected O
faces O
with O
the O
label O
of O
the O
closest O
distance O
compared O
to O
all O
of O
the O
face O
embeddings O
of O
18,880 O
named O
entities O
. O

For O
the O
5 O
single O
- O
token O
tasks O
, O
the O
score O
is O
defined O
to O
be O
the O
logit O
of O
the O
verbalizer O
token O
. O

Given O
a O
document O
X O
= O
( O
x O
1 O
, O
x O
2 O
, O
. O

Each O
example O
is O
a O
passage O
consisting O
of O
4 O
- O
5 O
sentences O
with O
the O
last O
word O
missing O
and O
the O
model O
is O
required O
to O
predict O
the O
last O
word O
of O
the O
passage O
. O

Other O
reasons O
for O
low O
BLEU B-MetricName
score O
includes O
: O
lack O
of O
diversity O
in O
the O
responses O
or O
malformation O
of O
response O
. O

For O
example O
, O
in O
the O
biomedical O
domain O
, O
several O
domainspecific O
PLMs O
trained O
with O
large O
biomedical O
texts O
, O
such O
as O
BioBERT B-MethodName
, O
PubMedBERT B-MethodName
( O
Gu O
et O
al O
. O
, O
2020 O
) O
and O
BlueBERT B-MethodName
( O
Peng O
et O
al O
. O
, O
2019 O
) O
, O
have O
been O
successfully O
used O
as O
strong O
baselines O
for O
several O
downstream O
tasks O
. O

For O
the O
training O
of O
recognition O
teacher O
model O
and O
similarity O
teacher O
model O
, O
we O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
be O
1e-5 B-HyperparameterValue
and O
5e-6 B-HyperparameterValue
separately O
. O

In O
this O
section O
, O
we O
introduce O
our O
framework O
and O
its O
detailed O
implementation O
. O

. O

We O
always O
generate O
the O
tokens O
in O
each O
blank O
following O
a O
left O
- O
to O
- O
right O
order O
, O
i.e. O
the O
probability O
of O
generating O
the O
span O
s O
i O
is O
factorized O
as O
: O

It O
confirms O
the O
superiority O
of O
GLM B-MethodName
over O
Masked O
LM O
pretraining O
on O
NLU O
tasks O
. O

Each O
GCN B-MethodName
model O
consists O
of O
two B-HyperparameterValue
propagation B-HyperparameterName
layers I-HyperparameterName
and O
a B-HyperparameterValue
sum B-HyperparameterName
pooling I-HyperparameterName
layer I-HyperparameterName
across O
the O
nodes O
in O
the O
graph O
. O

We O
also O
conduct O
experiments O
on O
fixing O
the O
parameter O
of O
PLM O
during O
training O
on O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
as O
initialized O
by O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
. O

ii O
) O
Learning O
a O
complex O
reasoning O
process O
is O
difficult O
especially O
in O
a O
condition O
where O
only O
QA B-TaskName
is O
provided O
without O
extra O
supervision O
on O
how O
to O
capture O
any O
evidence O
from O
the O
KB O
and O
infer O
based O
on O
them O
. O

The O
current O
state O
house O
members O
are O
: O
The O
Wyoming O
Constitution O
assigns O
certain O
powers O
to O
the O
governor O
. O

After O
his O
rookie O
season O
, O
he O
was O
not O
selected O
to O
play O
in O
the O
1966 O
pro O
bowl O
. O

Temperature B-HyperparameterName
in O
the O
Final O
Decoder O
Layer O

An O
empirical O
evidence O
on O
such O
vulnerability O
is O
that O
the O
state O
- O
of O
- O
the O
- O
art O
model O
( O
Zhu O
et O
al O
. O
, O
2021 O
) O
encounters O
a O
sharp O
performance O
drop O
( O
F1 O
score O
drops O
from O
68.6 O
% O
to O
3.8 O
% O
) O
on O
the O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
dataset O
when O
changing O
the O
questions O
to O
be O
hypothetical O
by O
adding O
a O
related O
assumption O
( O
see O
details O
in O
Section O
2 O
, O
Table O
3 O
) O
. O

In O
other O
words O
, O
this O
implies O
our O
approach O
has O
a O
room O
for O
further O
improvement O
when O
a O
better O
in O
- O
domain O
model O
is O
set O
as O
a O
teacher O
. O

where O
α B-HyperparameterName
1 I-HyperparameterName
, O
α B-HyperparameterName
2 I-HyperparameterName
, O
β B-HyperparameterName
, O
and O
γ B-HyperparameterName
are O
weights O
in O
loss O
function O
which O
are O
set O
to O
make O
the O
student O
model O
learns O
less O
noisy O
knowledge O
from O
teachers O
. O

Their O
results O
are O
all O
decent O
and O
close O
to O
each O
other O
( O
at O
least O
for O
ROUGE-1 B-MetricName
and O
ROUGE B-MetricName
- I-MetricName
L I-MetricName
) O
. O

x1 O
x2 O
x3 O
x4 O
x5 O
x6 O
x1 O
x2 O
[ O
M O
] O
x4 O
[ O
M O
] O
[ O
S O
] O
x5 O
x6 O
[ O
S O
] O
x3 O
x5 O
x6 O
[ O
E O
] O
x3 O
[ O
E O
] O
x1 O
x2 O
[ O
M O
] O
x4 O
[ O
M O
] O
[ O
S O
] O
x5 O
x6 O
[ O
S O
] O
x3 O
x1 O
x2 O
[ O
M O
] O
x4 O
[ O
M O
] O
[ O
S O
] O
x5 O
x6 O
[ O
S O
] O

Therefore O
, O
the O
student O
model O
is O
better O
suited O
to O
the O
target O
language O
with O
learning O
fewer O
low O
- O
confidence O
misrecognitions O
for O
the O
target O
language O
. O

Specifically O
, O
compared O
with O
the O
remarkable O
RIKD B-MethodName
, O
AdvPicker B-MethodName
, O
and O
Unitrans B-MethodName
, O
which O
also O
use O
knowledge O
distillation O
but O
ignore O
the O
entity O
similarity O
knowledge O
, O
our O
model O
obtains O
significant O
and O
consistent O
improvements O
in O
F1 B-MetricName
- O
score O
ranging O
from O
0.23 B-MetricValue
for O
German O
[ O
de O
] O
to O
6.81 B-MetricValue
for O
Arabic O
[ O
ar O
] O
. O

Instead O
, O
T5 B-MethodName
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
formulates O
most O
language O
tasks O
in O
the O
text O
- O
to O
- O
text O
framework O
. O

We O
follow O
the O
experimental O
settings O
suggested O
in O
. O

We O
reformulate O
the O
classification B-TaskName
tasks O
as O
blank B-TaskName
infilling I-TaskName
with O
human O
- O
crafted O
cloze B-TaskName
questions I-TaskName
, O
following O
PET O
( O
Schick O
and O
Schütze O
, O
2020b O
) O
. O

Kim O
and O
Rush O
( O
2016 O
) O
and O
later O
work O
( O
Kasai O
et O
al O
. O
, O
2020;Gu O
et O
al O
. O
, O
2017;Denkowski O
and O
Neubig O
, O
2017 O
) O
show O
pseudo O
- O
labeling O
achieves O
competitive O
performance O
for O
Seq2Seq O
tasks O
such O
as O
machine O
translation O
. O

Then O
they O
phrase O
the O
intervention O
into O
an O
assumption O
, O
forming O
a O
" O
what O
if O
" O
type O
of O
question O
, O
and O
calculate O
the O
answer O
( O
see O
an O
example O
in O
Figure O
1 O
) O
. O

We O
conduct O
experiments O
on O
Fact B-DatasetName
- I-DatasetName
based I-DatasetName
Visual I-DatasetName
Question I-DatasetName
Answering I-DatasetName
( O
FVQA B-DatasetName
) O
as O
an O
additional O
benchmark O
dataset O
for O
knowledge B-TaskName
- I-TaskName
based I-TaskName
VQA I-TaskName
. O

We O
include O
our O
code O
in O
the O
supplementary O
material O
. O

We O
use O
top B-HyperparameterName
- I-HyperparameterName
k I-HyperparameterName
random I-HyperparameterName
sampling I-HyperparameterName
with O
k B-HyperparameterName
= O
40 B-HyperparameterValue
for O
generation O
and O
set O
maximum B-HyperparameterName
sequence I-HyperparameterName
length I-HyperparameterName
to I-HyperparameterName
512 I-HyperparameterName
. O

For O
more O
details O
of O
the O
model O
architecture O
and O
parameter O
setting O
, O
we O
suggest O
referring O
to O
( O
Lin O
et O
al O
. O
, O
2020 O
) O
( O
Lewis O
et O
al O
. O
, O
2019 O
) O
. O

The O
knowledge O
and O
question O
graph O
are O
encoded O
separately O
by O
two O
graph B-MethodName
convolutional I-MethodName
networks I-MethodName
( O
GCN B-MethodName
) O
( O
Kipf O
and O
Welling O
, O
2017 O
) O
. O

Use O
of O
under O
- O
specified O
reward O
will O
often O
lead O
to O
policy O
that O
suffers O
from O
high O
variance O
( O
Agarwal O
et O
al O
. O
, O
2019 O
) O
. O

This O
student O
is O
randomly O
initialized O
and O
denoted O
by O
Transformer O
. O

12 O
annotators O
are O
invited O
( O
they O
are O
either O
native O
English O
speakers O
or O
graduate O
students O
with O
IELTS O
test O
score O
over O
6.5 O
) O
. O

With O
the O
same O
amount O
of O
parameters O
, O
GLM B-MethodName
Doc I-MethodName
performs O
worse O
than O
GPT B-MethodName
Large I-MethodName
. O

The O
relation B-TaskName
extraction I-TaskName
task O
aims O
to O
classify O
the O
relationship O
between O
two O
entities O
( O
e.g. O
, O
gene O
, O
chemical O
, O
and O
disease O
) O
that O
are O
already O
annotated O
. O

Inspired O
by O
previous O
work O
on O
constructing O
counterfactual O
samples O
( O
Kaushik O
et O
al O
. O
, O
2019 O
) O
, O
we O
recruit O
college O
students O
with O
finance O
- O
related O
majors O
to O
imagine O
an O
intervention O
based O
on O
the O
factual O
question O
and O
context O
from O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
which O
involves O
numerical O
thinking O
, O
e.g. O
, O
a O
change O
of O
number O
. O

We O
consider O
a O
huge O
number O
of O
knowledge O
facts O
in O
the O
KB O
as O
a O
huge O
knowledge O
graph O
, O
and O
construct O
a O
hypergraph O
by O
traversing O
the O
knowledge O
graph O
. O

We O
speculate O
the O
reason O
may O
be O
that O
, O
unlike O
summarization B-TaskName
, O
outputs O
of O
the O
machine B-TaskName
translation I-TaskName
task O
are O
relatively O
fixed O
. O

We O
can O
directly O
apply O
the O
pretrained O
GLM B-MethodName
for O
unconditional B-TaskName
generation I-TaskName
, O
or O
finetune O
it O
on O
downstream O
conditional B-TaskName
generation I-TaskName
tasks O
. O

Position O
1 O
1 O
2 O
3 O
4 O
5 O
5 O
5 O
5 O
3 O
3 O
Position O
2 O
0 O
0 O
0 O
0 O
0 O
1 O
2 O
3 O
1 O
2 O

To O
validate O
the O
impact O
of O
similarity O
- O
based O
answer O
selector O
, O
we O
replace O
the O
similarity O
- O
based O
answer O
selector O
( O
SIM O
) O
with O
a O
multi O
- O
layer O
perceptron O
( O
MLP O
) O
. O

We O
suspect O
the O
reason O
is O
that O
the O
operation O
of O
SWAP O
MIN O
NUM O
is O
very O
close O
to O
SWAP O
, O
which O
may O
confuse O
the O
deriving O
head O
when O
making O
classification O
over O
the O
operators O
. O

To O
sum O
up O
, O
teachers O
with O
higher O
attention B-HyperparameterName
temperatures I-HyperparameterName
can O
generate O
more O
concise O
and O
abstractive O
pseudo O
summaries O
, O
which O
makes O
the O
teacher O
provide O
more O
summary O
- O
like O
pseudo O
labels O
to O
students O
. O

In O
knowledge O
distillation O
, O
besides O
learning O
from O
gold O
labels O
in O
the O
training O
set O
, O
student O
models O
can O
learn O
from O
soft O
targets O
( O
Ba O
and O
Caruana O
, O
2014;Hinton O
et O
al O
. O
, O
2015 O
) O
, O
intermediate O
hidden O
states O
( O
Romero O
et O
al O
. O
, O
2014 O
) O
, O
attentions O
( O
Zagoruyko O
and O
Komodakis O
, O
2017 O
; O
, O
and O
target O
output O
derivatives O
( O
Czarnecki O
et O
al O
. O
, O
2017 O
) O
of O
teacher O
models O
. O

Effect O
of O
multi O
- O
hop O
graph O
walk O
We O
compare O
the O
performances O
with O
different O
number B-HyperparameterName
of I-HyperparameterName
graph I-HyperparameterName
walks I-HyperparameterName
used O
to O
construct O
a O
knowledge O
hypergraph O
( O
i.e. O
, O
1 B-HyperparameterValue
- I-HyperparameterValue
hop I-HyperparameterValue
, O
2 B-HyperparameterValue
- I-HyperparameterValue
hop I-HyperparameterValue
, O
and O
3 B-HyperparameterValue
- I-HyperparameterValue
hop I-HyperparameterValue
) O
. O

Many O
studies O
have O
been O
done O
to O
solve O
this O
crosslingual B-TaskName
NER I-TaskName
problem O
. O

We O
use O
a O
pre O
- O
processed O
version O
of O
the O
GAD B-DatasetName
dataset O
provided O
by O
BioBERT B-MethodName
, O
which O
is O
split O
for O
10 O
- O
fold O
cross O
- O
validation O
. O

Increasing B-MethodName
GLM I-MethodName
Doc I-MethodName
's O
parameters O
to O
410 O
M O
leads O
to O
the O
best O
performance O
on O
both O
tasks O
. O

Our O
future O
studies O
would O
focus O
on O
developing O
the O
proposed O
framework O
as O
a O
task O
- O
agnostic O
method O
and O
evaluating O
it O
on O
various O
tasks O
. O

We O
employed O
advanced O
models O
as O
the O
student O
model O
and O
verified O
the O
future O
applicability O
of O
our O
framework O
to O
emerging O
language O
models O
by O
achieving O
even O
higher O
performances O
than O
the O
teacher O
model O
. O

We O
define O
a O
triplet O
as O
a O
basic O
unit O
of O
graph O
walk O
to O
preserve O
high O
- O
order O
semantics O
inherent O
in O
knowledge O
graph O
, O
i.e. O
, O
every O
single O
graph O
walk O
contains O
three B-HyperparameterValue
nodes B-HyperparameterName
{ B-HyperparameterValue
head I-HyperparameterValue
, I-HyperparameterValue
predicate I-HyperparameterValue
, I-HyperparameterValue
tail I-HyperparameterValue
} I-HyperparameterValue
, O
rather O
than O
having O
only O
one O
of O
these O
three O
nodes O
. O

PLATE B-MethodName
B12 I-MethodName
- I-MethodName
3 I-MethodName
λ=1.5 I-MethodName
means O
that O
the O
student O
uses O
attention B-HyperparameterName
temperature I-HyperparameterName
coefficient I-HyperparameterName
λ B-HyperparameterName
= O
1.5 B-HyperparameterValue
with O
architecture O
setting O
BART B-MethodName
12 I-MethodName
- I-MethodName
3 I-MethodName
. O

In O
this O
section O
, O
we O
describe O
quantified B-TaskName
reproducibility I-TaskName
assessment I-TaskName
( O
QRA B-TaskName
) O
, O
an O
approach O
that O
is O
directly O
derived O
from O
the O
concepts O
and O
definitions O
of O
metrology O
, O
adopting O
the O
latter O
exactly O
as O
they O
are O
, O
and O
yields O
assessments O
of O
the O
degree O
of O
similarity O
between O
numerical O
results O
and O
between O
the O
studies O
that O
produced O
them O
. O

In O
this O
setting O
, O
we O
use O
the O
neural O
model O
proposed O
by O
Zhang O
et O
al O
. O
( O
2019 O
) O
. O

Each O
layer O
has O
a O
hidden B-HyperparameterName
size I-HyperparameterName
of O
512 B-HyperparameterValue
and O
8 O
attention O
heads O
. O

For O
NLU B-TaskName
tasks O
, O
we O
evaluate O
models O
on O
the O
SuperGLUE B-DatasetName
benchmark O
. O

The O
existence O
of O
an O
assumption O
calls O
for O
the O
imagination O
of O
a O
counterfactual O
context O
before O
inferring O
the O
answer O
, O
pushing O
the O
NDR B-TaskName
model O
to O
grasp O
both O
semantic O
understanding O
and O
counterfactual O
thinking O
. O

However O
, O
it O
does O
not O
explicitly O
maintain O
other O
attributes O
between O
the O
source O
and O
translated O
text O
, O
for O
e.g. O
, O
text O
length O
and O
descriptiveness O
. O

Following O
the O
standard O
structure O
of O
the O
transformer O
, O
we O
build O
up O
guided O
- O
attention O
block O
and O
selfattention O
block O
where O
each O
block O
consists O
of O
each O
attention O
operation O
with O
layer O
normalization O
, O
residual O
connection O
, O
and O
a O
single B-HyperparameterValue
feed B-HyperparameterName
- I-HyperparameterName
forward I-HyperparameterName
layer I-HyperparameterName
. O

The O
soft O
attention O
over O
the O
knowledge O
facts O
and O
the O
given O
question O
is O
computed O
as O
follows O
: O
p O
ij O
= O
softmax(q O
T O
i−1 O
m O
ij O
) O
where O
m O
is O
the O
embeddings O
of O
knowledge O
facts O
, O
i O
is O
a O
number O
of O
layer O
and O
j O
is O
an O
index O
of O
knowledge O
facts O
. O

Moreover O
, O
to O
guarantee O
the O
student O
learning O
performance O
, O
we O
also O
propose O
a O
weighting O
strategy O
to O
take O
into O
consideration O
the O
reliability O
of O
the O
teachers O
. O

Let O
λ B-HyperparameterName
enc I-HyperparameterName
, O
λ B-HyperparameterName
cross I-HyperparameterName
, O
and O
λ B-HyperparameterName
dec I-HyperparameterName
denote O
the O
attention B-HyperparameterName
temperature I-HyperparameterName
coefficient I-HyperparameterName
of O
the O
encoder O
self O
- O
attention O
module O
, O
the O
decoder O
cross O
- O
attention O
module O
, O
and O
the O
decoder O
self O
- O
attention O
module O
, O
respectively O
. O

Shleifer O
and O
Rush O
( O
2020 O
) O
propose O
the O
shrink O
and O
fine O
- O
tune O
( O
SFT O
) O
approach O
for O
pre O
- O
trained O
summarization O
distillation O
, O
which O
re O
- O
finetunes O
a O
teacher O
model O
with O
some O
layers O
removed O
, O
and O
they O
show O
SFT O
outperforms O
pseudo O
- O
labeling O
and O
a O
modification O
of O
direct O
knowledge O
distillation O
( O
Jiao O
et O
al O
. O
, O
2020 O
) O
on O
one O
of O
their O
datasets O
, O
but O
not O
others O
. O

The O
conversations O
span O
across O
7 O
domains O
including O
attraction O
, O
hospital O
, O
hotel O
, O
police O
, O
restaurant O
, O
taxi O
and O
train O
. O

For O
example O
, O
T5 B-MethodName
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
is O
pre O
- O
trained O
by O
predicting O
corrupted O
text O
spans O
. O

Compared O
to O
BERT B-MethodName
with O
cloze O
- O
style O
finetuning O
, O
GLM B-MethodName
benefits O
from O
the O
autoregressive O
pretraining O
. O

We O
implement O
the O
autoregressive O
blank O
infilling O
objective O
with O
the O
following O
techniques O
. O

• O
We O
devise O
the O
L2I B-MethodName
module O
, O
which O
is O
designed O
as O
neural O
network O
operations O
and O
can O
be O
seamlessly O
incorporated O
into O
the O
NDR B-TaskName
model O
for O
answering O
hypothetical O
questions O
. O

Formally O
, O
it O
is O
to O
learn O
a O
function O
y O
= O
f O
( O
q O
, O
c O
) O
, O
where O
y O
, O
q O
, O
and O
c O
are O
the O
word O
list O
representing O
the O
answer O
, O
the O
question O
, O
and O
the O
context O
2 O
respectively O
. O

Here O
, O
we O
highlight O
four O
aspects O
as O
follows O
: O
1 O
) O
KVQA B-DatasetName
dataset O
covers O
the O
large O
number O
of O
entities O
( O
at O
least O
5 O
times O
more O
) O
and O
knowledge O
facts O
( O
at O
least O
17 O
times O
more O
) O
than O
FVQA B-DatasetName
, O
PQ B-DatasetName
and O
PQL B-DatasetName
. O

Jonathan O
Terry O
was O
born O
in O
London O
. O

Nonetheless O
, O
our O
model O
shows O
robust O
reasoning O
performance O
when O
a O
large O
and O
noisy O
knowledge O
facts O
are O
given O
. O

Specifically O
, O
we O
first O
introduce O
the O
knowledge O
distillation O
to O
build O
entity O
recognizer O
and O
similarity O
evaluator O
teachers O
in O
the O
source O
language O
and O
transfer O
the O
learned O
patterns O
to O
the O
student O
in O
the O
target O
language O
. O

While O
blanking O
filling O
has O
been O
used O
in O
T5 B-MethodName
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
for O
text O
- O
to O
- O
text O
pretraining O
, O
we O
propose O
two O
improvements O
, O
namely O
span O
shuffling O
and O
2D O
positional O
encoding O
. O

However O
, O
such O
indirect O
guidance O
on O
imagination O
fails O
on O
the O
groups O
requiring O
more O
complex O
imagination O
, O
e.g. O
, O
requiring O
add O
or O
minus O
. O

To O
ablate O
the O
calibrated O
teacher O
training O
, O
we O
trained O
the O
teacher O
model O
using O
only O
L O
CE O
. O

For O
α B-HyperparameterName
analysis O
, O
we O
calculate O
the O
F1 B-MetricName
- O
score O
in O
different O
probability B-HyperparameterName
intervals I-HyperparameterName
of O
entity O
recognizer O
teacher O
, O
we O
find O
that O
the O
recognizer O
teacher O
tends O
to O
predict O
more O
correct O
in O
higher O
probability B-HyperparameterName
interval I-HyperparameterName
, O
as O
illustrated O
in O
Figure O
6a O
. O

Then O
, O
scaled O
dot O
product O
at O
- O
tention O
using O
the O
query O
, O
key O
, O
and O
value O
is O
calculated O

Nevertheless O
, O
additional O
pre O
- O
training O
has O
several O
limitations O
, O
such O
as O
the O
need O
for O
sufficient O
training O
data O
and O
resources O
, O
and O
a O
longer O
training O
time O
. O

In O
this O
case O
, O
our O
approach O
degrades O
into O
the O
single O
teacherstudent O
learning O
model O
as O
in O
TSL O
( O
Wu O
et O
al O
. O
, O
2020a O
) O
. O

The O
most O
similar O
answer O
to O
the O
joint O
representation O
is O
selected O
as O
an O
answer O
among O
the O
answer O
candidates O
. O

He O
appeared O
in O
the O
first O
few O
episodes O
of O
" O
" O
as O
the O
character O
major O
Jack O
Ryan O
. O

Though O
the O
suitable O
λ B-HyperparameterName
values O
may O
vary O
across O
datasets O
, O
we O
recommend O
considering O
the B-HyperparameterName
λ I-HyperparameterName
value O
1.5 B-HyperparameterValue
or O
2.0 B-HyperparameterValue
firstly O
in O
most O
cases O
. O

Moreover O
, O
we O
believe O
that O
the O
following O
iterations O
can O
be O
achieved O
by O
the O
current O
L2I B-MethodName
module O
in O
an O
iterative O
manner O
. O

Building O
on O
the O
progress O
of O
supervised O
transfer O
models O
, O
recent O
works O
have O
focused O
on O
unsupervised O
style O
transfer O
that O
avoids O
costly O
annotation O
of O
parallel O
sentences O
. O

However O
, O
it O
is O
difficult O
to O
capture O
multi O
- O
hop O
relationships O
containing O
long O
- O
distance O
nodes O
from O
the O
graph O
due O
to O
the O
well O
- O
known O
over O
- O
smoothing O
problem O
, O
where O
repetitive O
message O
passing O
process O
to O
propagate O
information O
across O
long O
distance O
makes O
features O
of O
connected O
nodes O
too O
similar O
and O
undiscriminating O
( O
Li O
et O
al O
. O
, O
2018 O
; O
. O

Our O
method O
, O
which O
builds O
on O
top O
of O
pseudo O
- O
labeling O
, O
is O
conceptually O
simple O
and O
improves O
pseudo O
- O
labeling O
across O
different O
summarization B-TaskName
datasets O
. O

We O
show O
that O
the O
NLU B-TaskName
tasks O
can O
be O
formulated O
as O
conditional B-TaskName
generation I-TaskName
tasks I-TaskName
, O
and O
therefore O
solvable O
by O
autoregressive O
models O
. O

Each O
node O
is O
represented O
as O
a O
w B-HyperparameterValue
- I-HyperparameterValue
dimensional I-HyperparameterValue
embedding B-HyperparameterName
vector I-HyperparameterName
, O
i.e. O
, O
v O
i O
∈ O
R O
w O
. O

Similarly O
, O
Athiwaratkun O
et O
al O
. O
( O
2020 O
) O
and O
Paolini O
et O
al O
. O
( O
2020 O
) O
convert O
structured B-TaskName
prediction I-TaskName
tasks O
, O
such O
as O
sequence B-TaskName
tagging I-TaskName
and O
relation B-TaskName
extraction I-TaskName
, O
to O
sequence B-TaskName
generation I-TaskName
tasks O
. O

Then O
we O
compute O
the O
cross B-HyperparameterValue
entropy I-HyperparameterValue
loss B-HyperparameterName
using O
the O
groundtruth O
label O
and O
update O
the O
model O
parameters O
. O

High O
- O
temperature B-HyperparameterName
teachers O
can O
alleviate O
the O
leading O
bias O
problems O
by O
providing O
pseudo O
labels O
with O
better O
coverage O
of O
source O
documents O
to O
students O
. O

We O
adopt O
the O
Seq2Seq O
Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
model O
. O

Our O
source O
code O
is O
available O
at O
https://github.com/yujungheo/ O
kbvqa O
- O
public O
. O

Using O
a O
pair O
of O
dialogue O
rewards O
R(τ O
1 O
) O
and O
R(τ O
2 O
) O
, O
we O
compute O
the O
probabilistic O
preference O
between O
the O
roll O
- O
outs O
P O
[ O
τ O
1 O
≻ O
τ O
2 O
] O
either O
by O
standard O
normalization O
or O
a O
softmax O
function O
. O

The O
model O
predicts O
the O
missing O
tokens O
in O
the O
spans O
from O
the O
corrupted O
text O
in O
an O
autoregressive O
manner O
, O
which O
means O
when O
predicting O
the O
missing O
tokens O
in O
a O
span O
, O
the O
model O
has O
access O
to O
the O
corrupted O
text O
and O
the O
previously O
predicted O
spans O
. O

To O
investigate O
the O
impacts O
of O
each O
attention O
block O
( O
i.e. O
, O
GA O
and O
SA O
) O
, O
ablation O
studies O
are O
shown O
in O
Table O
3(e O
- O
g O
) O
. O

While O
the O
translated O
sentence O
" O
Loved O
the O
movie O
" O
has O
correctly O
transferred O
the O
attribute O
( O
style O
) O
, O
it O
does O
not O
have O
the O
same O
length O
, O
does O
not O
retain O
the O
personal O
noun O
( O
" O
I O
" O
) O
, O
nor O
use O
a O
domain O
- O
appropriate O
proper O
noun O
. O

Applying O
KL O
- O
divergence O
- O
based O
distillation O
yielded O
positive O
results O
in O
terms O
of O
classification O
performance O
. O

Note O
that O
the O
improvement O
is O
not O
as O
significant O
as O
in O
summarization B-TaskName
tasks O
. O

The O
hyperparameters O
for O
all O
the O
pre O
- O
training O
settings O
are O
summarized O
in O
Table O
7.Our O
pretraining O
implementation O
is O
based O
on O
Megatron B-MethodName
- I-MethodName
LM I-MethodName
( O
Shoeybi O
et O
al O
. O
, O
2019 O
) O
and O
Deep B-MethodName
- I-MethodName
Speed I-MethodName
( O
Rasley O
et O
al O
. O
, O
2020 O
) O
. O

Summaries O
generated O
by O
abstractive O
models O
may O
be O
ungrammatical O
or O
unfaithful O
to O
the O
original O
document O
. O

Joint O
representation O
is O
obtained O
based O
on O
the O
attention O
as O
well O
. O

Taking O
the O
hypothetical O
question O
in O
Figure O
1 O
as O
an O
example O
, O
an O
ideal O
L2I B-MethodName
should O
recognize O
the O
target O
variable O
( O
finished O
goods O
in O
2019 O
) O
, O
identify O
the O
corresponding O
fact O
( O
$ O
133,682 O
) O
, O
and O
replace O
the O
fact O
with O
the O
assumed O
value O
( O
$ O
132,935 O
) O
. O

( O
Taigman O
et O
al O
. O
, O
2017 O
) O
, O
but O
these O
issues O
- O
to O
the O
best O
of O
our O
knowledge O
- O
are O
unexplored O
in O
NLP O
. O

The O
results O
on O
GLUE B-DatasetName
and O
SQuAD B-DatasetName
are O
shown O
in O
Tables O
9 O
and O
10 O
. O

The O
pattern O
is O
written O
in O
natural O
language O
to O
represent O
the O
semantics O
of O
the O
task O
. O

Here O
, O
we O
set O
up O
the O
model O
as O
three B-HyperparameterValue
layers O
with O
adjacent O
and O
layer O
- O
wise O
weight O
tying O
. O

The O
confidence B-HyperparameterName
penalty I-HyperparameterName
strength I-HyperparameterName
β B-HyperparameterName
2 I-HyperparameterName
in O
the O
refinement O
step O
and O
loss B-HyperparameterName
switch I-HyperparameterName
rate I-HyperparameterName
γ B-HyperparameterName
were O
chosen O
from O
{ O
0 B-HyperparameterValue
, O
0.3 B-HyperparameterValue
, B-HyperparameterValue
0.5 I-HyperparameterValue
, O
0.7 B-HyperparameterValue
} O
and O
{ O
0.6 B-HyperparameterValue
, O
0.7 B-HyperparameterValue
, O
0.8 B-HyperparameterValue
, O
0.9 B-HyperparameterValue
} O
, O
respectively O
. O

It O
was O
one O
of O
two O
stations O
built O
by O
the O
flushing O
railroad O
in O
Corona O
, O
this O
one O
having O
been O
at O
Grand O
Avenue O
( O
later O
called O
National O
Avenue O
, O
now O
National O
Street O
) O
and O
45th O
Avenue O
. O

Rastogi O
et O
al O
. O
( O
2019 O
) O
and O
Hosseini O
- O
Asl O
et O
al O
. O
( O
2020 O
) O
frame O
dialogue O
policy O
learning O
as O
language B-TaskName
modeling I-TaskName
task O
. O

With O
the O
same O
amount O
of O
training O
data O
, O
GLM B-MethodName
consistently O
outperforms O
BERT B-MethodName
on O
most O
tasks O
with O
either O
base O
or O
large O
architecture O
. O

These O
results O
validate O
that O
it O
is O
meaningful O
to O
consider O
not O
only O
knowledge O
but O
also O
question O
as O
hypergraphs O
. O

Autoencoding O
models O
, O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
learn O
bidirectional O
context O
encoders O
via O
denoising O
objectives O
, O
e.g. O
Masked O
Language O
Model O
( O
MLM O
) O
. O

In O
calibrated O
teacher O
training O
, O
we O
trained O
for O
3 B-HyperparameterValue
- O
10 B-HyperparameterValue
epochs B-HyperparameterName
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
2e-5 B-HyperparameterValue
. O

We O
also O
investigate O
the O
general O
applicability O
of O
our O
framework O
by O
applying O
it O
to O
a O
financial O
domain O
PLM O
and O
downstream O
tasks O
. O

Smith O
' O
s O
father O
was O
a O
pharmacist O
. O

where O
M O
q O
, O
M O
k O
are O
a O
row O
- O
wise O
concatenated O
question O
words O
and O
knowledge O
entities O
, O
W O
[ O
• O
] O
is O
learn O
- O
Automatic O
transfer O
of O
text O
between O
domains O
has O
become O
popular O
in O
recent O
times O
. O

The O
three O
encoded O
representations O
are O
concatenate O
and O
are O
fed O
through O
a O
couple O
of O
feed O
- O
forward O
layers O
before O
making O
a O
bounded O
reward O
prediction O
R(s O
t O
, O
a O
t O
, O
g O
) O
∈ O
[ O
0 O
, O
1 O
] O
for O
each O
turn O
using O
a O
sigmoid O
function O
. O

The O
ChemProt B-DatasetName
( O
Krallinger O
et O
al O
. O
, O
2017 O
) O
dataset O
contains O
PubMed O
abstracts O
with O
10 O
types O
of O
chemicalprotein O
interaction O
annotations O
and O
only O
five O
of O
the O
types O
are O
used O
for O
evaluation O
. O

The O
experiments O
were O
performed O
on O
the O
ChemProt B-DatasetName
dataset O
, O
using O
the O
ALBERT B-MethodName
- I-MethodName
xlarge I-MethodName
model O
as O
the O
student O
architecture O
. O

We O
evaluate O
the O
multi O
- O
task O
model O
for O
NLU B-TaskName
, O
seq2seq B-TaskName
, O
blank B-TaskName
infilling I-TaskName
, O
and O
zero B-TaskName
- I-TaskName
shot I-TaskName
language O
modeling O
. O

The O
only O
difference O
is O
the O
abstraction O
level O
of O
input O
. O

Offline O
task B-TaskName
- I-TaskName
oriented I-TaskName
dialogue I-TaskName
( O
ToD B-TaskName
) O
systems O
involves O
solving O
disparate O
tasks O
of O
belief O
states O
tracking O
, O
dialogue O
policy O
management O
, O
and O
response O
generation O
. O

reconstructing O
them O
. O
It O
is O
an O
important O
difference O
as O
compared O
to O
other O
models O
. O

In O
particular O
, O
we O
evaluate O
TAGOP B-MethodName
( O
Zhu O
et O
al O
. O
, O
2021 O
) O
, O
which O
is O
the O
state O
- O
of O
- O
the O
- O
art O
model O
on O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
( O
see O
detailed O
settings O
in O
Section O
4.1 O
) O
by O
training O
on O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
and O
testing O
on B-DatasetName
TAT I-DatasetName
- I-DatasetName
HQA I-DatasetName
. O

To O
address O
this O
issue O
, O
it O
is O
worth O
considering O
the O
operator O
relation O
in O
the O
deriving O
head O
in O
the O
future O
. O

T5 B-MethodName
proposes O
a O
similar O
blank O
infilling O
objective O
to O
pretrain O
an O
encoder O
- O
decoder O
Transformer O
. O

We O
then O
leverage O
this O
enciphered O
training O
data O
along O
with O
the O
original O
parallel O
data O
via O
multi O
- O
source O
training O
to O
improve O
neural B-TaskName
machine I-TaskName
translation I-TaskName
. O

Module O
Design O
. O
Based O
on O
the O
two O
- O
step O
formulation O
, O
we O
then O
design O
the O
L2I B-MethodName
module O
as O
neural O
network O
operations O
. O

i O
) O
We O
propose O
Hypergraph B-MethodName
Transformer I-MethodName
which O
enhances O
multi O
- O
hop O
reasoning O
ability O
by O
encoding O
high O
- O
order O
semantics O
in O
the O
form O
of O
a O
hypergraph O
and O
learning O
inter O
- O
and O
intrahigh O
- O
order O
associations O
in O
hypergraphs O
using O
the O
attention O
mechanism O
. O

[ O
S O
] O

Are O
these O
pseudo O
summaries O
of O
good O
quality O
? O
set O
is O
shown O
in O
Table O
7 O
. O

Perhaps O
this O
is O
not O
surprising O
, O
since O
cross O
attentions O
are O
directly O
related O
to O
the O
selection O
of O
document O
contents O
for O
summarization B-TaskName
. O

Maintaining O
constraints O
in O
transfer O
has O
several O
downstream O
applications O
, O
including O
data B-TaskName
augmentation I-TaskName
and O
de B-TaskName
- I-TaskName
biasing I-TaskName
. O

The O
per O
turn O
rewards O
are O
summed O
to O
form O
a O
global O
reward O
R(τ O
) O
for O
the O
roll O
- O
out O
τ O
. O

Reproducibility O
more O
generally O
is O
becoming O
more O
of O
a O
research O
focus O
. O

The O
main O
difference O
baselines O
on O
seq2seq B-TaskName
tasks O
are O
obtained O
from O
the O
corresponding O
papers O
. O

Attention(Q O
q O
, O
K O
k O
, O
V O
k O
) O
. O

x O
1 O
x O
2 O
x O
3 O
x O
4 O
x O
5 O
x O
6 O
x O
1 O
x O
2 O
[ O
M O
] O
x O
4 O
[ O
M O
] O
[ O
S O
] O
x O
5 O
x O
6 O
[ O
S O
] O
x O
3 O
x O
5 O
x O
6 O
[ O
E O
] O
x O
3 O
[ O
E O
] O
x O
1 O
x O
2 O
[ O
M O
] O
x O
4 O
[ O
M O
] O
[ O
S O
] O
x O
5 O
x O
6 O
[ O
S O
] O
x O
3 O
x O
1 O
x O
2 O
[ O
M O
] O

However O
, O
these O
methods O
are O
complicated O
to O
encode O
inherent O
high O
- O
order O
semantics O
and O
multi O
- O
hop O
relationships O
present O
in O
the O
knowledge O
graph O
. O

These O
models O
are O
pre O
- O
trained O
using O
unsupervised O
text O
- O
to O
- O
text O
objectives O
. O

[ O
M O
] O

Here O
, O
f O
[ O
• O
] O

Formally O
, O
let O
Z O
m O
be O
the O
set O
of O
all O
possible O
permutations O
of O
the O
length O
- O
m O
index O
sequence O
[ O
1 O
, O
2 O
, O
• O
• O
• O
, O
m O
] O
, O
and O
s O
z O
< O
i O
be O
[ O
s O
z O
1 O
, O
• O
• O
• O
, O
s O
z O
i−1 O
] O
, O
we O
define O
the O
pretraining O
objective O
as O

We O
use O
the O
official O
splits O
of O
Narayan O
et O
al O
. O
( O
2018 O
) O
. O

The O
test O
set O
is O
constructed O
by O
including O
the O
9,076 O
articles O
published O
after O
January O
1 O
, O
2007 O
. O

. O

First O
, O
autoencoding O
models O
learn O
a O
bidirectional O
contextualized O
encoder O
for O
natural O
language O
understanding O
via O
denoising O
objectives O
( O
Devlin O
et O
al O
. O
, O
2019;Joshi O
et O
al O
. O
, O
2020;Yang O
et O
al O
. O
, O
2019;Lan O
et O
al O
. O
, O
2020;Clark O
et O
al O
. O
, O
2020 O
) O
. O

Note O
that O
we O
use O
Q O
, O
K O
, O
and O
V O
for O
query O
, O
key O
, O
value O
, O
and O
q O
, O
k O
as O
subscripts O
to O
represent O
question O
and O
knowledge O
, O
respectively O
. O

Table O
8 O
shows O
the O
cloze O
questions O
and O
verbalizers O
we O
used O
in O
our O
experiments O
. O

Interestingly O
, O
compared O
with O
λ B-HyperparameterName
= O
1.0 B-HyperparameterValue
, O
the O
performance O
of O
the O
teacher O
with B-HyperparameterName
λ I-HyperparameterName
= O
2.0 B-HyperparameterValue
is O
worse O
, O
but O
the O
resulting O
student O
is O
much O
better O
( O
see O
Table O
2 O
) O
. O

We O
train O
our O
model O
on O
5 O
% O
, O
10 O
% O
, O
and O
20 O
% O
of O
the O
training O
data O
and O
compared O
with O
other O
baselines O
on O
end O
- O
to O
- O
end O
dialogue O
task O
, O
Table O
2 O
list O
the O
results O
. O

Therefore O
, O
we O
heuristically O
devises O
the O
three O
weights O
scheduling O
as O
functions O
of O
the O
inputs O
, O

The O
TAPT B-MethodName
approach O
additionally O
pre O
- O
trains O
an O
existing O
PLM O
before O
fine O
- O
tuning O
it O
with O
the O
training O
samples O
of O
each O
task O
. O

However O
, O
the O
sequence O
- O
level O
knowledge O
of O
teacher O
mod O
- O
els O
is O
not O
well O
utilized O
. O

[ O
S O
] O

their O
ROUGE B-MetricName
scores O
on O
validation O
sets O
. O

In O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
, O
the O
training O
set O
without O
entity O
label O
of O
the O
target O
language O
is O
also O
available O
when O
training O
the O
model O
. O

. O

Example O
D.3 O
. O
Corona O
was O
a O
station O
along O
the O
port O
Washington O
branch O
of O
the O
long O
island O
rail O
road O
in O
the O
Corona O
section O
of O
queens O
, O
New O
York O
City O
. O

They O
are O
typically O
deployed O
in O
conditional O
generation O
tasks O
, O
such O
as O
text B-TaskName
summarization I-TaskName
and O
response B-TaskName
generation I-TaskName
. O

In O
the O
literature O
, O
there O
are O
mainly O
two O
kinds O
of O
methods O
for O
summarization B-TaskName
: O
extractive B-TaskName
summarization I-TaskName
and O
abstractive B-TaskName
summarization I-TaskName
( O
Nenkova O
and O
McKeown O
, O
2011 O
) O
. O

+ O
d O
K O
V O
L O
z O
g O
7 O
3 O
6 O
M O
6 O
s O
f O
h O
c O
= O
" O
> O
A O
A O
A O
B O
9 O
X O
i O
c O
b O
V O
C O
7 O
T O
s O
M O
w O
F O
L O
3 O
h O
W O
c O
K O
r O
w O
M O
g O
S O
U O
S O
E O
x O
V O
Q O
k O
D O
s O
C O
A O
q O
W O
B O
i O
L O
R O
B O
9 O
S O
G O
y O
r O
H O
c O
V O
q O
r O
j O
h O
3 O
Z O
D O
l O
B O
F O
/ O
Q O
8 O
W O
B O
h O
5 O
i O
5 O
T O
P O
Y O
W O
R O
B O
/ O
g O
9 O
N O
2 O
g O
J O
Y O
j O
W O
T O

However O
, O
current B-TaskName
NDR I-TaskName
models O
face O
severe O
generalization O
failure O
on O
hypothetical O
questions O
. O

We O
conducted O
an O
experiment O
to O
verify O
the O
positive O
effect O
of O
combining O
calibrated O
teacher O
training O
and O
activation O
boundary O
distillation O
. O

GLM B-MethodName
is O
trained O
with O
an O
autoregressive O
blank O
infilling O
objective O
, O
thus O
can O
straightforwardly O
solve O
this O
task O
. O

To O
pursue O
accurate O
context O
, O
we O
derive O
the O
intervention O
with O
a O
set O
of O
discrete O
operators O
such O
as O
SWAP O
and O
ADD O
for O
imagination O
. O

Transformer B-MethodName
( I-MethodName
SA+GA I-MethodName
) I-MethodName
fails O
to O
focus O
on O
the O
multi O
- O
hop O
facts O
required O
to O
answer O
the O
given O
question O
and O
predicts O
the O
answer O
with O
the O
wrong O
number O
at O
the O
end O
. O

We O
set O
the O
number B-HyperparameterName
of I-HyperparameterName
crossattention I-HyperparameterName
layers I-HyperparameterName
to O
3 B-HyperparameterValue
, O
and O
fine O
tune O
from O
TAGOP B-MethodName
trained O
on O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
5e-5 B-HyperparameterValue
, O
batch B-HyperparameterName
size I-HyperparameterName
of O
32 B-HyperparameterValue
, O
and O
gradient B-HyperparameterName
accumulation I-HyperparameterName
step I-HyperparameterName
of O
4 B-HyperparameterValue
. O

x O
3 O

While O
we O
can O
not O
directly O
compare O
GLM B-MethodName
with O
T5 B-MethodName
due O
to O
the O
differences O
in O
training O
data O
and O
the O
number B-HyperparameterName
of I-HyperparameterName
parameters I-HyperparameterName
, O
the O
results O
in O
Tables O
1 O
and O
6 O
have O
demonstrated O
the O
advantage O
of O
GLM.Pretrained B-MethodName
Language O
Models O
. O

He O
was O
elected O
to O
the O
Michigan O
sports O
hall O
of O
fame O
in O
1995 O
. O

Recent O
researches O
have O
introduced O
the O
concept O
of O
hypergraph O
for O
multi O
- O
hop O
graph O
reasoning O
( O
Kim O
et O
al O
. O
, O
2020;Han O
et O
al O
. O
, O
2020b O
, O
a;Yadati O
et O
al O
. O
, O
, O
2021 O
. O

We O
find O
that O
λ B-HyperparameterName
= O
1.5 B-HyperparameterValue
or O
λ B-HyperparameterName
= O
2.0 B-HyperparameterValue
usually O
works O
well O
in O
practice O
. O

We O
find O
that O
adjusting O
the O
student O
's O
attention O
temperature O
does O
not O
work O
. O

The O
last O
two O
datasets O
, O
PQ B-DatasetName
and O
PQL B-DatasetName
, O
focus O
on O
evaluating O
multi O
- O
hop O
reasoning O
ability O
in O
the O
knowledgebased B-TaskName
textual I-TaskName
QA I-TaskName
task I-TaskName
. O

Thus O
, O
we O
construct O
a O
queryaware O
knowledge O
hypergraph O
H O
k O
= O
{ O
V O
k O
, O
E O
k O
} O
to O
extract O
related O
information O
for O
answering O
a O
given O
question O
. O

Additional O
pre O
- O
training O
with O
in O
- O
domain O
text O
has O
been O
proposed O
to O
provide O
the O
PLMs O
with O
domain O
- O
specific O
knowledge O
. O

When O
we O
train O
our O
student O
model O
with O
pseudo O
labels O
, O
we O
still O
use O
a O
normal O
temperature O
( O
i.e. O
, O
τ O
= O
√ O
d O
) O
. O

Formally O
, O

First O
, O
XLNet B-MethodName
uses O
the O
original O
position O
encodings O
before O
corruption O
. O

Increasing O
the O
model O
's O
parameters B-HyperparameterName
to O
410 B-HyperparameterValue
M I-HyperparameterValue
( O
1.25× O
of O
GPT O
Large O
) O
leads O
to O
a O
performance O
close O
to O
GPT B-MethodName
Large I-MethodName
. O

2 O
) O
TAGOP B-MethodName
- I-MethodName
CLO I-MethodName
outperforms O
TAGOP B-MethodName
by O
10.5 B-MetricValue
% I-MetricValue
and O
10.4 B-MetricValue
% I-MetricValue
on O
EM B-MetricName
and O
F B-MetricName
1 I-MetricName
. O

We O
verify O
the O
effectiveness O
of O
each O
module O
in O
Hypergraph B-MethodName
Transformer I-MethodName
. O

For O
a O
fair O
comparison O
, O
we O
compare O
our O
model O
against O
the O
version O
of O
TOF B-MethodName
w/o O
continual O
learning O
( O
Zhang O
et O
2021 O
) O
, O
RIKD B-MethodName
w/o O
IKD O
( O
Liang O
et O
al O
. O
, O
2021 O
) O
and O
Unitrans B-MethodName
w/o O
translation O
( O
Wu O
et O
al O
. O
, O
2020b O
) O
as O
reported O
in O
their O
paper O
. O

Specifically O
, O
each O
token O
is O
encoded O
with O
two O
positional O
ids O
. O

If O
the O
length O
l O
is O
unknown O
, O
we O
may O
need O
to O
enumerate O
all O
possible O
lengths O
, O
since O
BERT B-MethodName
needs O
to O
change O
the O
number O
of O
[ O
MASK O
] O
tokens O
according O
to O
the O
length O
. O

However O
, O
for O
arithmetic O
questions O
, O
the O
question B-TaskName
- I-TaskName
answering I-TaskName
label O
for O
one O
pair O
of O
c O
and O
q O
remains O
the O
same O
between O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
and O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
, O
and O
the O
intervention O
is O
achieved O
explicitly O
by O
deriving O
operators O
and O
tagging O
head O
. O

Results O
with O
the O
Transformer B-MethodName
student O
( O
the O
sixth O
block O
) O
follow O
a O
similar O
trend O
, O
although O
the O
improvements O
are O
smaller O
. O

It O
corresponds O
to O
the O
simplest O
imagination O
since O
the O
assumed O
value O
( O
i.e. O
, O
c O
i O
) O
is O
explicitly O
mentioned O
in O
the O
assumption O
. O

We O
also O
find O
that O
BART B-MethodName
does O
not O
perform O
well O
on O
the O
challenging O
SuperGLUE B-DatasetName
benchmark O
. O

, O
in O
a O
given O
sentence O
. O

We O
also O
study O
the O
contribution O
of O
2D B-HyperparameterValue
positional I-HyperparameterValue
encoding I-HyperparameterValue
to O
long B-TaskName
text I-TaskName
generation I-TaskName
. O

We O
train B-MethodName
Transformer I-MethodName
for O
100 B-HyperparameterValue
epochs O
and O
select O
the O
best O
model O
w.r.t O
. O

GLM B-MethodName

This O
goes O
to O
show O
that O
having O
the O
right O
reward O
function O
to O
guide O
the O
budget O
of O
the O
gradient O
update O
process O
to O
reach O
the O
true O
objective O
is O
important O
in O
extremely O
low O
resource O
setting O
. O

We O
notice O
that O
GLM B-MethodName
Doc I-MethodName
slightly O
underperforms O
GLM B-MethodName
Large I-MethodName
, O
which O
is O
consistent O
with O
our O
observations O
in O
the O
seq2seq O
experiments O
. O

All O
our O
models O
are O
trained O
on O
8 O
NVIDIA O
V100 O
GPUs O
. O

o O
∈ O
R B-HyperparameterName
O I-HyperparameterName
is O
a O
distribution O
over O
the O
operators O
where O
O B-HyperparameterName
denotes O
the O
number B-HyperparameterName
of I-HyperparameterName
operators I-HyperparameterName
. O

Meanwhile O
, O
HAN B-MethodName
employs O
stochastic O
graph O
walk O
in O
a O
knowledge O
and O
question O
graph O
to O
encode O
high O
- O
order O
semantics O
( O
e.g. O
, O
knowledge O
facts O
and O
question O
phrases O
) O
, O
and O
considers O
attention O
scores O
between O
knowledge O
facts O
and O
question O
phrases O
. O

In O
classification O
tasks O
, O
it O
is O
typically O
done O
by O
minimizing O
the O
distance O
between O
the O
teacher O
and O
student O
predictions O
( O
Hinton O
et O
al O
. O
, O
2015 O
) O
. O

wherep O
j O
∈ O
{ O
0 O
, O
1 O
} O
denotes O
the O
label O
of O
the O
target O
fact O
( O
token O
j O
in O
context O
) O
or O
the O
premise O
( O
token O
j O
in O
assumption O
) O
; O
andō O
∈ O
R O
O O
is O
the O
label O
of O
the O
deriving O
operator O
( O
see O
Appendix O
C O
for O
the O
details O
of O
label O
construction).Readers O
might O
have O
raised O
the O
following O
two O
concerns O
for O
L2I B-MethodName
: O
1 O
) O
the O
operators O
defined O
are O
limited O
, O
and O
2 O
) O
the O
operators O
are O
tailored O
to O
one O
step O
of O
derivation O
on O
one O
target O
fact O
. O

Existing O
models O
can O
be O
separated O
into O
three O
categories O
, O
shared O
feature O
space O
based O
, O
translation O
based O
and O
knowledge O
distillation O
based O
. O

Position O
1 O
1 O
2 O
3 O
4 O
5 O
5 O
5 O
5 O
3 O
3 O
Position O
2 O
0 O
0 O
0 O
0 O
0 O
1 O
2 O
3 O
1 O
2 O

However O
, O
both O
models O
require O
more O
parameters O
to O
outperform O
autoencoding O
models O
such O
as O
RoBERTa B-MethodName
. O

This O
work O
is O
supported O
partly O
by O
the O
Fundamental O
Research O
Funds O
for O
the O
Central O
Universities O
and O
by O
the O
State O
Key O
Laboratory O
of O
Software O
Development O
Environment O
. O

In O
particular O
, O
we O
adopt O
a O
tagging O
head O
to O
identify O
the O
premise O
and O
a O
multi O
- O
way O
classifier O
for O
choosing O
operators O
, O
which O
is O
formulated O
as O
: O
o O
= O
sof O
tmax(MLP(h O
CLS O
) O
) O
. O

When O
both O
data O
and O
code O
are O
provided O
, O
the O
number O
of O
potential O
causes O
of O
such O
differences O
is O
limited O
, O
and O
the O
NLP O
field O
has O
shared O
increasingly O
detailed O
information O
about O
system O
, O
dependencies O
and O
evaluation O
to O
chase O
down O
sources O
of O
differences O
. O

x O
1 O
x O
2 O
x O
3 O
x O
4 O
x O
5 O
x O
6 O
x O
1 O
x O
2 O
[ O
M O
] O
x O
4 O
[ O
M O
] O
[ O
S O
] O
x O
5 O
x O
6 O
[ O
S O
] O
x O
3 O
x O
5 O
x O
6 O
[ O
E O
] O
x O
3 O
[ O
E O
] O
x O
1 O
x O
2 O
[ O
M O
] O
x O
4 O
[ O
M O
] O
[ O
S O
] O
x O
5 O
x O
6 O
[ O
S O
] O
x O
3 O
x O
1 O
x O
2 O
[ O
M O
] O

A O
low O
BLEU B-MetricName
score O
and O
relatively O
high O
inform B-MetricName
and O
success B-MetricName
rate I-MetricName
might O
indicate O
greedy O
agent O
behaviour O
. O

The O
margin B-HyperparameterName
µ B-HyperparameterName
of O
the O
activation O
transfer O
loss O
was O
set O
to O
1.0 B-HyperparameterValue
. O

To O
construct O
the O
text B-TaskName
infilling I-TaskName
task O
, O
we O
randomly O
mask O
a O
given O
ratio B-HyperparameterName
r O
∈ O
{ O
10 B-HyperparameterValue
% I-HyperparameterValue
• O
• O
• O
50 B-HyperparameterValue
% I-HyperparameterValue
} O
of O
each O
document O
's O
tokens O
and O
the O
contiguous O
masked O
tokens O
are O
collapsed O
into O
a O
single O
blank O
. O

We O
present O
the O
attention O
map O
from O
the O
guided O
- O
attention O
block O
, O
and O
visualize O
top O
- O
k O
attended O
knowledge O
facts O
or O
entities O
with O
the O
attention O
scores O
. O

In O
calibrated O
teacher O
training O
, O
we O
first O
select O
the O
number B-HyperparameterName
of I-HyperparameterName
epochs I-HyperparameterName
and O
the O
learning B-HyperparameterName
rate I-HyperparameterName
as O
the O
default O
values O
of O
the O
BioBERT B-MethodName
code O
and O
slightly O
change O
the O
number B-HyperparameterName
of I-HyperparameterName
epochs I-HyperparameterName
( O
e B-HyperparameterName
) O
for O
the O
unreported O
tasks O
from O
BioBERT B-MethodName
. O

The O
detailed O
equation O
of O
gated O
recurrent O
propagation O
is O
as O
follows O
: O
h O

Sharing O
code O
and O
data O
together O
with O
detailed O
information O
about O
them O
is O
now O
expected O
as O
standard O
, O
and O
checklists O
and O
datasheets O
have O
been O
proposed O
to O
standardise O
information O
sharing O
( O
Pineau O
, O
2020;Shimorina O
and O
Belz O
, O
2021 O
) O
. O

This O
situation O
is O
more O
severe O
for O
zero O
- O
resource O
languages O
. O

Translation O
based O
models O
generate O
pseudo O
labeled O
target O
language O
data O
to O
train O
the O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
model O
, O
but O
the O
noise O
from O
translation O
process O
restrains O
its O
performance O
. O

[ O
M O
] O

The O
dataset O
has O
10438 O
dialogues O
split O
into O
8438 O
dialogues O
for O
training O
set O
and O
1000 O
dialogues O
each O
for O
validation O
and O
test O
set O
. O

To O
facilitate O
the O
evaluation O
of B-TaskName
HQA I-TaskName
and O
diagnose O
counterfactual O
thinking O
, O
we O
construct O
an O
HQA B-TaskName
dataset O
based O
on O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
( O
Zhu O
et O
al O
. O
, O
2021 O
) O
, O
which O
is O
a O
QA B-TaskName
dataset O
with O
a O
mix O
of O
tabular O
and O
textual O
context O
extracted O
from O
financial O
reports O
. O

It O
clearly O
demonstrates O
the O
advantage O
of O
our O
method O
in O
NLU B-TaskName
tasks O
. O

Especially O
, O
when O
we O
convert O
the O
one O
of O
both O
hyperedge B-HyperparameterValue
- I-HyperparameterValue
level I-HyperparameterValue
representation B-HyperparameterName
to O
single B-HyperparameterValue
- I-HyperparameterValue
word I-HyperparameterValue
- I-HyperparameterValue
unit I-HyperparameterValue
- I-HyperparameterValue
based I-HyperparameterValue
representation B-HyperparameterName
, O
the O
mean B-MetricName
accuracy I-MetricName
of O
QA B-TaskName
is O
82.7 B-MetricValue
% I-MetricValue
and O
88.7 B-MetricValue
% I-MetricValue
, O
respectively O
. O

Study O
on B-MethodName
L2I I-MethodName
module O
design O
. O

Our O
distillation O
method O
PLATE O
works O
as O
follows O
. O

Positive O
< O
l O
a O
t O
e O
x O
i O
t O
s O
h O
a O
1 O
_ O
b O
a O
s O
e O
6 O
4 O
= O
" O
c O
b O
5 O
S O
9 O

In O
this O
paper O
, O
we O
proposed O
Hypergraph B-MethodName
Transformer I-MethodName
for O
multi O
- O
hop O
reasoning O
over O
knowledge O
graph O
under O
weak O
supervision O
. O

The O
results O
for O
models O
trained O
on O
BookCorpus B-DatasetName
and O
Wikipedia B-DatasetName
are O
shown O
in O
Tables O
3 O
and O
4 O
. O

However O
, O
generative O
models O
require O
much O
more O
parameters O
to O
work O
due O
to O
the O
limit O
of O
unidirectional O
attention O
. O

Within O
one O
training O
batch O
, O
we O
sample O
short O
spans O
and O
longer O
spans O
( O
document O
- O
level O
or O
sentence O
- O
level O
) O
with O
equal O
chances O
. O

These O
methods O
, O
while O
capable O
of O
achieving O
the O
target O
domain O
characteristics O
, O
often O
fail O
to O
maintain O
the O
invariant O
content O
. O

Simpsons O
" O
as O
the O
character O
captain O
Billy O
Higgledypig O
, O
but O
his O
character O
was O
only O
a O
one O
- O
time O
recurring O
character O
in O
the O
series O
' O
first O
six O
seasons O
. O

Note O
that O
we O
only O
change O
the O
teacher O
's O
attention O
temperature O
during O
inference O
time O
. O

It O
derives O
the O
intervention O
result O
for O
the O
target O
fact O
. O

The O
comparison O
results O
are O
shown O
in O
Table O
4 O
. O

3 O
The O
resulting O
numbers O
of O
document O
- O
summary O
pairs O
for O
training O
, O
validation O
, O
and O
test O
are O
287,227 O
, O
13,368 O
, O
and O
11,490 O
, O
respectively O
. O

Moreover O
, O
compared O
with O
the O
latest O
model O
TOF B-MethodName
, O
RIKD B-MethodName
, O
Unitrans B-MethodName
, O
our O
model O
requires O
much O
lower O
computational O
costs O
for O
both O
translation O
and O
iterative O
knowledge O
distillation O
, O
meanwhile O
reaching O
superior O
performance O
. O

Also O
, O
our O
model O
shows O
significant O
improvement O
in O
spatial O
question O
compared O
to O
other O
models O
. O

Following O
( O
Tjong O
Kim O
Sang O
, O
2002 O
) O
, O
we O
use O
the O
entity B-MetricName
level I-MetricName
F1 I-MetricName
- O
score O
as O
the O
evaluation O
metric O
. O

For O
activation O
boundary O
distillation O
, O
we O
first O
fine O
- O
tuned O
the O
initial O
student O
model O
for O
5 B-HyperparameterValue
- O
10 B-HyperparameterValue
epochs B-HyperparameterName
with O
learning B-HyperparameterName
rates I-HyperparameterName
of O
{ O
6e-6 B-HyperparameterValue
, O
8e-6 B-HyperparameterValue
, O
1e-5 B-HyperparameterValue
} O
. O

RoBERTa B-MethodName
's O
performance O
was O
already O
similar O
to O
the O
teacher O
model O
in O
the O
initial O
fine O
- O
tuning O
stage O
because O
it O
was O
pre O
- O
trained O
with O
more O
data O
than O
BERT B-MethodName
and O
exhibited O
a O
greater O
robustness O
. O

We O
now O
describe O
our O
pretraining O
setup O
and O
the O
evaluation O
of O
downstream O
tasks O
. O

To O
evaluate O
a O
pure O
reasoning O
ability O
of O
the O
models O
, O
we O
conduct O
experiments O
in O
the O
oracle O
setting O
. O

With O
the O
release O
of O
multi O
- O
domain O
, O
multi O
- O
turn O
Multi B-DatasetName
- I-DatasetName
Woz2.0 I-DatasetName
dataset O
( O
Budzianowski O
et O
al O
. O
, O
2018a O
) O
, O
there O
has O
been O
flurry O
of O
recent O
works O
, O
of O
which O
Zhang O
et O
al O
. O
( O
2019 O
) O
uses O
data O
augmentation O
. O

Two O
- O
step O
Formulation O
. O

) O

We O
truncate O
all O
documents O
and O
summaries O
to O
1024 B-HyperparameterValue
sub O
- O
word O
tokens O
. O

The O
contents O
of O
this O
appendix O
are O
as O
follows O
: O
In O
Section O
A O
, O
we O
show O
the O
detailed O
statistics O
for O
the O
diverse O
splits O
of O
four O
benchmark O
datasets O
, O
i.e. O
, O
KVQA B-DatasetName
, O
FVQA B-DatasetName
, O
PQ B-DatasetName
and O
PQL B-DatasetName
. O

All O
models O
except O
ours O
show O
slightly O
lower O
performance O
on O
the O
3 B-HyperparameterValue
- I-HyperparameterValue
hop I-HyperparameterValue
graph O
than O
on O
the O
2 B-HyperparameterValue
- I-HyperparameterValue
hop I-HyperparameterValue
graph O
. O

The O
evaluation O
metrics O
are O
the O
scores O
of O
BLEU-1 B-MetricName
, O
BLEU-2 B-MetricName
, O
BLEU-3 B-MetricName
, O
BLEU-4 B-MetricName
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
, O
METEOR B-MetricName
( O
Denkowski O
and O
Lavie O
, O
2014 O
) O
and O
Rouge B-MetricName
- I-MetricName
L I-MetricName
( O
Lin O
, O
2004 O
) O
. O

We O
start O
below O
with O
the O
concepts O
and O
definitions O
that O
QRA B-TaskName
is O
based O
on O
, O
followed O
by O
an O
overview O
of O
the O
framework O
( O
Section O
3.2 O
) O
and O
steps O
in O
applying O
it O
in O
practice O
( O
Section O
3.3).The O
International O
Vocabulary O
of O
Metrology O
( O
VIM O
) O
( O
JCGM O
, O
2012 O
) O
defines O
repeatability O
and O
reproducibility O
as O
follows O
( O
defined O
terms O
in O
bold O
, O
see O
VIM O
for O
subsidiary O
defined O
terms O
): O

