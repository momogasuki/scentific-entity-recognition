In O
comparison O
, O
Transformer B-MethodName
( I-MethodName
SA+GA I-MethodName
) I-MethodName
strongly O
attends O
to O
the O
knowledge O
entities O
which O
appear O
repetitive O
in O
the O
knowledge O
facts O
. O

Table O
6 O
shows O
our O
ablation O
analysis O
for O
GLM B-MethodName
. O

XSum B-DatasetName
The O
XSum B-DatasetName
dataset O
is O
collected O
by O
harvesting O
online O
articles O
from O
the O
BBC O
with O
single O
sentence O
summaries O
, O
which O
is O
professionally O
written O
. O

He O
has O
also O
appeared O
as O
part O
of O
the O
supporting O
cast O
of O
several O
episodes O
of O
" O
the O
secret O
life O
of O
pets O
" O
. O

The O
results O
are O
shown O
in O
Figure O
4 O
. O

We O
postulate O
that O
the O
failure O
is O
due O
to O
unable O
of O
imagining O
the O
counterfactual O
context O
according O
to O
the O
assumption O
( O
Figure O
1 O
) O
. O

Note O
that O
an O
attention O
weight O
is O
evident O
if O
it O
is O
greater O
than O
0.15 O
, O
and O
these O
evident O
attention O
weights O
account O
for O
around O
15 B-MetricValue
% I-MetricValue
of O
all O
attention O
weights O
. O

This O
problem O
setting O
is O
an O
important O
application O
of O
text O
transfer O
, O
as O
enforcing O
constraints O
of O
identity O
can O
help O
maintain O
the O
brand O
identity O
when O
the O
product O
descriptions O
are O
mapped O
from O
one O
commercial O
product O
to O
another O
. O

Above O
all O
, O
we O
conclude O
that O
GLM B-MethodName
effectively O
shares O
model O
parameters O
across O
natural B-TaskName
language I-TaskName
understanding I-TaskName
and O
generation B-TaskName
tasks O
, O
achieving O
better O
performance O
than O
a O
standalone B-MethodName
BERT I-MethodName
, O
encoder B-MethodName
- I-MethodName
decoder I-MethodName
, O
or O
GPT B-MethodName
model I-MethodName
. O

The O
DDI B-DatasetName
( O
Herrero O
- O
Zazo O
et O
al O
. O
, O
2013 O
) O
dataset O
consists O
of O
text O
from O
the O
DrugBank O
database O
and O
Medline O
abstracts O
, O
with O
four O
types O
of O
drug O
- O
drug O
interaction O
annotations O
. O

That O
is O
, O
the O
teacher O
model O
has O
the O
same O
as O
the O
neural O
network O
structure O
of O
the O
student O
model O
. O

All O
the O
other O
results O
of O
well O
studied O
for O
language O
modeling O
. O

Then O
, O
the O
two O
aggregated O
graph O
representations O
are O
concatenated O
and O
fed O
into O
another O
single B-HyperparameterValue
layer I-HyperparameterValue
feed B-HyperparameterName
- I-HyperparameterName
forward I-HyperparameterName
layer I-HyperparameterName
to O
get O
joint O
representation O
of O
question O
and O
knowledge O
graph O
. O

Then O
, O
we O
model O
the O
derivation O
as O
making O
a O
choice O
across O
the O
operators O
and O
tagging O
the O
premise O
for O
executing O
the O
operator O
. O

For O
instance O
, O
if O
L O
AT O
= O
500 O
for O
an O
ALBERT B-MethodName
model O
( O
H=2,048 B-HyperparameterName
) O
, O
it O
indicates O
that O
500 O
of O
the O
2,048 O
elements O
in O
the O
hidden O
representation O
vector O
exhibited O
signs O
different O
to O
those O
of O
the O
teacher O
. O

Irrespective O
of O
the O
use O
of O
an O
alternative O
version O
( O
Equation O
5 O
) O
during O
the O
training O
, O
the O
extent O
to O
which O
the O
activation O
pattern O
is O
distilled O
can O
be O
intuitively O
observed O
by O
calculating O
the O
original O
" O
activation O
transfer O
loss O
" O
( O
Equation O
4 O
) O
. O

Multi O
- O
hop O
knowledge O
graph O
reasoning O
is O
a O
process O
of O
sequential O
reasoning O
based O
on O
multiple O
evidences O
of O
a O
knowledge O
graph O
, O
and O
has O
been O
broadly O
used O
in O
various O
downstream O
tasks O
such O
as O
question B-TaskName
answering I-TaskName
( O
Lin O
et O
al O
. O
, O
2019;Saxena O
et O
al O
. O
, O
2020;Han O
et O
al O
. O
, O
2020b O
, O
a;Yadati O
et O
al O
. O
, O
2021 O
) O
, O
or O
knowledge B-TaskName
- I-TaskName
enhanced I-TaskName
text I-TaskName
generation I-TaskName
( O
Liu O
et O
al O
. O
, O
2019;Moon O
et O
al O
. O
, O
2019;Ji O
et O
al O
. O
, O
2020 O
) O
. O

In O
this O
paper O
, O
we O
introduce O
a O
concept O
of O
hypergraph O
to O
encode O
highlevel O
semantics O
of O
a O
question O
and O
a O
knowledge O
base O
, O
and O
to O
learn O
high O
- O
order O
associations O
between O
them O
. O

Each O
language O
is O
divided O
into O
a O
training O
set O
, O
a O
development O
set O
and O
a O
test O
set O
. O

Hence O
, O
we O
also O
use O
a O
soft O
version O
of O
the O
metric O
M B-MetricName
sof I-MetricName
t I-MetricName
, O
where O
the O
success B-MetricName
rate I-MetricName
measures O
a O
fraction O
of O
requested O
information O
provided O
in O
a O
dialogue O
. O

ii O
) O
We O
conduct O
extensive O
experiments O
on O
two O
knowledge B-TaskName
- I-TaskName
based I-TaskName
VQA I-TaskName
datasets O
( O
KVQA B-DatasetName
and O
FVQA B-DatasetName
) O
and O
two O
knowledge B-TaskName
- I-TaskName
based I-TaskName
textual I-TaskName
QA I-TaskName
datasets O
( O
PQ B-DatasetName
and O
PQL B-DatasetName
) O
and O
show O
superior O
performances O
on O
all O
datasets O
, O
especially O
multi O
- O
hop O
reasoning O
problem O
. O

Ablation O
study O
In O
a O
Transformer O
, O
there O
are O
three O
types O
of O
attention O
modules O
( O
i.e. O
, O
encoder O
selfattention O
, O
decoder O
self O
- O
attention O
and O
decoder O
crossattention O
) O
, O
and O
we O
can O
scale O
attention B-HyperparameterName
temperatures I-HyperparameterName
for O
all O
of O
them O
or O
some O
of O
them O
. O

Answering O
complex O
questions O
that O
require O
multi O
- O
hop O
reasoning O
under O
weak O
supervision O
is O
considered O
as O
a O
challenging O
problem O
since O
i O
) O
no O
supervision O
is O
given O
to O
the O
reasoning O
process O
and O
ii O
) O
highorder O
semantics O
of O
multi O
- O
hop O
knowledge O
facts O
need O
to O
be O
captured O
. O

In O
other O
words O
, O
domain O
- O
specific O
knowledge O
can O
be O
transferred O
into O
advanced O
models O
without O
a O
time O
- O
consuming O
pre- O
training O
and O
perturbing O
the O
model O
's O
efficacy O
in O
the O
general O
domain O
. O

Thus O
, O
Hypergraph B-MethodName
Transformer I-MethodName
can O
mitigate O
the O
well O
- O
known O
over O
- O
smoothing O
problem O
in O
the O
previous O
graph O
- O
based O
methods O
exploiting O
the O
message O
passing O
scheme O
. O

In O
this O
paper O
, O
we O
( O
i O
) O
describe O
a O
method O
for O
quantified B-TaskName
reproducibility I-TaskName
assessment I-TaskName
( O
QRA B-TaskName
) O
directly O
derived O
from O
standard O
concepts O
and O
definitions O
from O
metrology O
which O
addresses O
the O
above O
issues O
, O
and O
( O
ii O
) O
test O
it O
on O
diverse O
sets O
of O
NLP O
results O
. O

In O
worst O
case O
, O
turns O
like O
Turn#4 O
will O
appear O
more O
often O
than O
turns O
Turn#2 O
and O
# O
3 O
in O
a O
ToD B-TaskName
dataset O
, O
there O
by O
taking O
greater O
share O
of O
the O
gradient O
budget O
. O

Another O
observation O
is O
that O
the O
cloze O
formulation O
is O
critical O
for O
GLM B-MethodName
's O
performance O
on O
NLU B-TaskName
tasks O
. O

We O
expect O
that O
the O
performance O
will O
be O
improved O
when O
the O
entity O
linking O
module O
is O
enhanced O
. O

The O
encoder O
of O
the O
student O
model O
obtains O
the O
clustering O
information O
of O
the O
target O
language O
with O
the O
help O
of O
β B-HyperparameterName
. O

GLM B-MethodName
differs O
in O
three O
aspects O
: O

When O
hypergraph B-HyperparameterValue
- I-HyperparameterValue
based I-HyperparameterValue
representations B-HyperparameterName
are O
used O
for O
both O
knowledge O
and O
question O
, O
the O
results O
show O
the O
best O
performance O
across O
all O
settings O
over O
question O
types O
( O
ORG O
and O
PRP O
) O
and O
a O
number O
of O
graph B-HyperparameterName
walk I-HyperparameterName
( O
1 B-HyperparameterValue
- I-HyperparameterValue
hop I-HyperparameterValue
, O
2 B-HyperparameterValue
- I-HyperparameterValue
hop I-HyperparameterValue
, O
and O
3 B-HyperparameterValue
- I-HyperparameterValue
hop I-HyperparameterValue
) O
. O

x O
4 O

There O
are O
204,045 O
articles O
for O
training O
; O
11,332 O
articles O
for O
validation O
; O
and O
11,334 O
articles O
for O
test O
. O

All O
datasets O
were O
annotated O
with O
four O
entity O
types O
: O
LOC O
, O
MISC O
, O
ORG O
, O
and O
PER O
. O

We O
show O
that O
the O
complementary O
cooperative O
losses O
improve O
text O
quality O
, O
according O
to O
both O
automated O
and O
human O
evaluation O
measures O
. O

The O
learning B-HyperparameterName
rate I-HyperparameterName
has O
a O
peak O
value O
of O
3e-5 B-HyperparameterValue
, O
warmup B-HyperparameterName
over O
the O
6 B-HyperparameterValue
% I-HyperparameterValue
training I-HyperparameterValue
steps I-HyperparameterValue
and O
a O
linear B-HyperparameterValue
decay I-HyperparameterValue
. O

x O
4 O

From O
the O
results O
in O
Table O
5 O
, O
GLM B-MethodName
outperforms O
previous O
methods O
by O
large O
margins O
( O
1.3 B-MetricValue
to O
3.9 B-MetricValue
BLEU B-MetricName
) O
and O
achieves O
the O
state O
- O
of O
- O
the O
- O
art O
result O
on O
this O
dataset O
. O

Then O
we O
evaluate O
the O
GLM B-MethodName
's O
performance O
in O
a O
multi O
- O
task O
setting O
( O
Section O
2.1 O
) O
. O

First O
, O
they O
are O
asked O
to O
evaluate O
the O
summaries O
on O
three O
dimensions O
: O
fluency O
( O
is O
the O
summary O
grammatically O
correct O
? O
) O
, O
faithfulness O
( O
is O
the O
summary O
faithful O
to O
the O
original O
document O
? O
) O
, O
and O
coverage O
( O
does O
the O
summary O
coverage O
important O
information O
of O
the O
document O
? O
) O
. O

Further O
empirical O
analysis O
shows O
that O
, O
with O
PLATE B-MethodName
, O
both O
pseudo O
summaries O
generated O
by O
teacher O
models O
and O
summaries O
generated O
by O
student O
models O
are O
shorter O
and O
more O
abstractive O
, O
which O
matches O
the O
goal O
of O
abstractive O
summarization O
. O

With O
the O
help O
of O
transfer O
learning O
( O
Ruder O
et O
al O
. O
, O
2019 O
) O
and O
multilingual B-MethodName
BERT I-MethodName
( O
short O
as O
mBERT O
) O
( O
Devlin O
et O
al O
. O
, O
* O
NER O
/ O
NER O
tea O
: O
learned O
NER B-TaskName
model O
for O
source O
language O
; O
NER O
stu O
: O
learned O
NER B-TaskName
model O
for O
target O
language O
; O
SIM O
tea O
learned O
similarity O
model O
for O
source O
language O
; O
{ O
X O
, O
Y O
} O
src O
: O
labeled O
data O
in O
source O
language O
; O
{ O
X O
} O
tgt O
: O
unlabeled O
data O
in O
target O
language O
; O
{ O
X O
, O
P O
} O
tgt O
: O
labeled O
data O
in O
target O
language O
with O
probability O
; O
{ O
X O
, O
S O
} O
tgt O
: O
labeled O
data O
in O
target O
language O
with O
entity O
similarity O
score O
. O

Following O
Durrett O
et O
al O
. O
( O
2016 O
) O
; O
Liu O
and O
Lapata O
( O
2019 O
) O
, O
we O
report O
limited O
- O
length O
recall O
based O
ROUGE-1 B-MetricName
, O
ROUGE-2 B-MetricName
, O
and O
ROUGE B-MetricName
- I-MetricName
L I-MetricName
, O
where O
generated O
summaries O
are O
truncated O
to O
the O
lengths O
of O
gold O
summaries O
. O

In O
Section O
3.3 O
, O
we O
also O
propose O
a O
variant O
of O
our O
method O
, O
which O
employs O
random O
attention B-HyperparameterName
temperatures I-HyperparameterName
( O
PLATE B-MethodName
rnd I-MethodName
in O
Table O
2 O
) O
. O

For O
3 O
tasks O
( O
ReCoRD B-DatasetName
, O
COPA B-DatasetName
, O
and O
WSC B-DatasetName
) O
, O
the O
answer O
may O
consist O
of O
multiple O
tokens O
, O
and O
for O
the O
other O
5 O
tasks O
, O
the O
answer O
is O
always O
a O
single O
token O
. O

The O
HoC B-DatasetName
( O
Baker O
et O
al O
. O
, O
2016 O
) O
corpus O
consists O
of O
PubMed O
abstracts O
with O
ten O
types O
of O
hallmarks O
of O
cancer O
annotation O
. O

He O
has O
also O
worked O
on O
" O
the O
simpsons O
" O
TV O
show O
since O
" O
the O
simpsons O
movie O
" O
, O
most O
notably O
playing O
the O
roles O
of O
Captain O
Skeletor O
and O
the O
ghost O
of O
the O
same O
name O
. O

We O
used O
two O
pre O
- O
trained O
models O
as O
the O
initial O
student O
model O
: O
ALBERT B-MethodName
- I-MethodName
xlarge I-MethodName
( O
L=24 B-HyperparameterName
, O
H=2048 B-HyperparameterName
, O
A=32 B-HyperparameterName
) O
and O
RoBERTa B-MethodName
- I-MethodName
large I-MethodName
( O
L=24 B-HyperparameterName
, O
H=1024 B-HyperparameterName
, O
A=16 B-HyperparameterName
) O
. O

For O
the O
masked O
spans O
, O
it O
is O
the O
position O
of O
the O
corresponding O
[ O
MASK O
] O
token O
. O

We O
want O
the O
student O
model O
to O
learn O
from O
the O
two O
teachers O
as O
follows O
: O
the O
higher O
the O
prediction O
of O
the O
entity O
recognizer O
teacher O
is O
( O
the O
further O
away O
from O
0.5 O
the O
prediction O
of O
the O
entity O
similarity O
teacher O
is O
, O
the O
higher O
the O
consistency B-HyperparameterName
level I-HyperparameterName
is O
) O
, O
the O
more O
accurate O
the O
prediction O
is O
, O
thus O
the O
more O
attention O
the O
student O
model O
pays O
attention O
to O
the O
input O
tokens O
, O
and O
vice O
versa O
. O

Proportions O
of O
novel B-MetricName
n I-MetricName
- I-MetricName
grams I-MetricName
are O
used O
to O
measure O
the O
abstractiveness O
of O
summaries O
( O
See O
et O
al O
. O
, O
2017;Liu O
and O
Lapata O
, O
2019 O
) O
. O

BERTSUM B-MethodName
( O
Liu O
and O
Lapata O
, O
2019 O
) O
employs O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
as O
its O
encoder O
and O
uses O
randomly O
initialized O
decoder O
. O

x O
5 O

The O
result O
in O
Figure O
4 O
shows O
that O
training O
on O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
causes O
a O
performance O
drop O
in O
counting O
, O
span O
and O
multi O
- O
span O
groups O
of O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
, O
and O
performs O
similar O
on O
the O
in O
arithmetic O
group O
. O

We O
trained O
the O
model O
with O
the O
labeled O
training O
set O
of O
the O
source O
language O
and O
evaluated O
the O
model O
on O
the O
test O
set O
of O
each O
target O
language O
. O

The O
governor O
of O
Wyoming O
holds O
no O
legislative O
power O
but O
has O
the O
power O
to O
veto O
lawmakers O
, O
which O
is O
not O
limited O
to O
the O
veto O
of O
laws O
. O

Thus O
, O
we O
ablate O
the O
calibrated O
teacher O
training O
steps O
in O
our O
framework O
and O
compare O
the O
final O
performances O
and O
loss O
values O
. O

All O
the O
other O
baselines O
are O
of O
similar O
size O
to O
BERT B-MethodName
Large I-MethodName
. O

However O
, O
the O
limitations O
of O
our O
approach O
are O
that O
it O
is O
task O
- O
specific O
and O
was O
evaluated O
only O
in O
classification O
tasks O
. O

In O
all O
our O
experiments O
, O
we O
use B-HyperparameterName
K I-HyperparameterName
= O
10 B-HyperparameterValue
. O

We O
try O
to O
bring O
up O
insights O
on O
why O
the O
proposed O
multiple B-MethodName
- I-MethodName
task I-MethodName
and I-MethodName
multiple I-MethodName
- I-MethodName
teacher I-MethodName
model I-MethodName
works O
. O

Visual B-TaskName
question I-TaskName
answering I-TaskName
( O
VQA B-TaskName
) O
is O
a O
semantic B-TaskName
reasoning I-TaskName
task O
that O
aims O
to O
answer O
questions O
about O
visual O
content O
depicted O
in O
images O
( O
Antol O
et O
al O
. O
, O
2015;Zhu O
et O
al O
. O
, O
2016;Hudson O
and O
Manning O
, O
2019 O
) O
, O
and O
has O
become O
one O
of O
the O
most O
active O
areas O
of O
research O
with O
advances O
in O
natural O
language O
processing O
and O
computer O
vision O
. O

Different O
from O
the O
BERTbased O
models O
used O
by O
PET O
, O
GLM B-MethodName
can O
naturally O
handle O
multi O
- O
token O
answers O
to O
the O
cloze O
question O
via O
autoregressive O
blank O
filling O
. O

For O
trade O
- O
off O
of O
training O
speed O
and O
fair O
comparison O
with O
BERT B-MethodName
( O
batch B-HyperparameterName
size I-HyperparameterName
256 B-HyperparameterValue
and O
1,000,000 B-HyperparameterValue
training B-HyperparameterName
steps I-HyperparameterName
) O
, O
we O
use O
batch B-HyperparameterName
size I-HyperparameterName
of O
1024 B-HyperparameterValue
and O
200,000 B-HyperparameterValue
training B-HyperparameterName
steps I-HyperparameterName
for O
GLM B-MethodName
Large I-MethodName
. O

Detailed O
performance O
. O
To O
further O
investigate O
the O
effectiveness O
of O
the O
proposed O
L2I B-MethodName
module O
, O
we O
perform O
a O
detailed O
comparison O
between O
TAGOP B-MethodName
- I-MethodName
L2I I-MethodName
and O
TAGOP B-MethodName
w.r.t O
. O

Empirically O
, O
compared O
with O
standalone O
baselines O
, O
GLM B-MethodName
with O
multi O
- O
task O
pretraining O
achieves O
improvements O
in O
NLU B-TaskName
, O
conditional B-TaskName
text I-TaskName
generation I-TaskName
, O
and O
language B-TaskName
modeling I-TaskName
tasks O
altogether O
by O
sharing O
the O
parameters O
. O

He O
attended O
Bishop O
O O
' O
Dowd O
high O
school O
in O
Flintridge O
. O

We O
call O
the O
two O
biases O
above O
the O
copy O
bias O
and O
the O
leading O
bias O
. O

The O
MLP O
uses O
17 O
% O
more O
parameters O
than O
SIM O
because O
KVQA O
has O
a O
large O
number O
of O
answer O
candidates O
( O
19,360 O
) O
. O

There O
have O
been O
several O
workshops O
and O
initiatives O
on O
reproducibility O
, O
including O
workshops O
at O
ICML O
2017 O
and O
2018 O
, O
the O
reproducibility O
challenge O
at O
ICLR O
2018 O
and O
2019 O
, O
and O
at O
NeurIPS O
2019 O
and O
2020 O
, O
the O
RE O
- O
PROLANG O
( O
Branco O
et O
al O
. O
, O
2020 O
) O
initiative O
at O
LREC O
2020 O
, O
and O
the O
ReproGen O
shared O
task O
on O
reproducibility O
in O
NLG O
( O
Belz O
et O
al O
. O
, O
2021b O
) O
. O

For O
text B-TaskName
generation I-TaskName
tasks O
, O
the O
given O
context O
constitutes O
the O
Part O
A O
of O
the O
input O
, O
with O
a O
mask O
token O
appended O
at O
the O
end O
. O

3 O
) O
As O
to O
the O
remaining O
methods O
, O
their O
performance O
has O
a O
clear O
gap O
between O
TAGOP B-MethodName
, O
which O
is O
consistent O
with O
the O
result O
on O
the O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
dataset O
( O
Zhu O
et O
al O
. O
, O
2021 O
) O
. O

First O
, O
we O
use O
Bag O
- O
of O
- O
words O
( O
BoW O
) O
representation O
for O
knowledge O
facts O
and O
a O
question O
. O

To O
fully O
capture O
the O
interdependencies O
between O
different O
spans O
, O
we O
randomly O
permute O
the O
order O
of O
the O
spans O
, O
similar O
to O
the O
permutation O
language O
model O
( O
Yang O
et O
al O
. O
, O
2019 O
) O
. O

Additionally O
, O
we O
train O
two O
larger O
GLM B-MethodName
models O
of O
410 O
M O
( O
30 B-HyperparameterValue
layers B-HyperparameterName
, O
hidden B-HyperparameterName
size I-HyperparameterName
1024 B-HyperparameterValue
, O
and O
16 B-HyperparameterValue
attention B-HyperparameterName
heads I-HyperparameterName
) O
and O
515 O
M O
( O
30 B-HyperparameterValue
layers B-HyperparameterName
, O
hidden B-HyperparameterName
size I-HyperparameterName
1152 B-HyperparameterValue
, O
and O
18 B-HyperparameterValue
attention B-HyperparameterName
heads I-HyperparameterName
) O
parameters O
with O
documentlevel O
multi O
- O
task O
pretraining O
, O
denoted O
as O
GLM B-MethodName
410 I-MethodName
M I-MethodName
and O
GLM B-MethodName
515 I-MethodName
M I-MethodName
. O

In O
other O
words O
, O
calibration O
on O
the O
teacher O
training O
clearly O
aids O
the O
supervision O
of O
the O
teacher O
in O
activation O
boundary O
distillation O
, O
even O
though O
the O
output O
probability O
information O
is O
not O
directly O
used O
in O
distillation O
. O

We O
use O
the O
uncased O
wordpiece O
tokenizer O
of O
BERT B-MethodName
with O
30k O
vocabulary O
. O

Second O
, O
XLNet B-MethodName
uses O
a O
two O
- O
stream O
self O
- O
attention O
mechanism O
, O
instead O
of O
the O
right O
- O
shift O
, O
to O
avoid O
the O
information O
leak O
within O
Transformer O
. O

The O
fourth O
student O
is O
the O
Transformer B-MethodName
base O
model O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
which O
has O
6 O
layers O
in O
each O
of O
the O
encoder O
and O
decoder O
. O

This O
demonstrates O
the O
ease O
of O
adaptation O
of O
existing O
methods O
with O
CASPI.Inverse B-MethodName
reinforcement O
learning O
, O
coupled O
with O
offpolicy O
policy O
learning O
and O
evaluation O
are O
proven O
to O
be O
sample O
efficient O
( O
Thomas O
and O
Brunskill O
, O
2016 O
) O
. O

Hypergraph B-MethodName
Transformer I-MethodName
achieves O
the O
best O
accuracy B-MetricName
in O
all O
categories O
except O
Multi O
- O
hop O
( O
slightly O
low O
at O
second O
- O
best O
) O
. O

First O
, O
to O
provide O
an O
apple O
- O
to O
- O
apple O
comparison O
with O
BERT B-MethodName
, O
we O
train O
a O
BERT B-MethodName
Large I-MethodName
model O
with O
our O
implementation O
, O
data O
, O
and O
hyperparameters O
( O
row O
2 O
) O
. O

Table O
2 O
shows O
the O
overall O
experimental O
F1 B-MetricName
score O
results O
of O
the O
DoKTra B-MethodName
framework O
on O
five O
biomedical B-TaskName
and O
clinical B-TaskName
classification I-TaskName
tasks O
. O

A O
n O
- O
gram O
phrase O
is O
considered O
as O
a O
hyperedge O
in O
the O
question O
hypergraph O
( O
see O
Figure O
2(b)).To O
consider O
high O
- O
order O
associations O
between O
knowledge O
and O
question O
, O
we O
devise O
structural O
semantic O
matching O
between O
the O
query O
- O
aware O
knowledge O
hypergraph O
and O
the O
question O
hypergraph O
. O

Latent O
state O
information O
such O
as O
prosody O
, O
richness O
of O
natural O
language O
and O
among O
others O
induces O
stochasticity O
in O
the O
agents O
response O
. O

The O
latency O
statistics O
( O
Milliseconds O
) O
and O
numbers O
of O
parameters O
of O
above O
four O
models O
are O
in O
Table O
1 O
. O

Named B-TaskName
entity I-TaskName
recognition I-TaskName
, O
NER B-TaskName
in O
short O
, O
refers O
to O
identifying O
entity O
types O
, O
i.e. O
location O
, O
person O
, O
organization O
, O
etc O
. O

Notably O
, O
calibrated O
teacher O
training O
also O
improved O
the O
KL O
- O
divergence O
- O
based O
distillation O
because O
it O
enabled O
the O
distillation O
of O
a O
considerably O
more O
reliable O
output O
probability O
, O
as O
reported O
in O
Menon O
et O
al O
. O
( O
2021 O
) O
. O

Table O
6 O
presents O
the O
results O
of O
the O
ablation O
study O
. O

GLM B-MethodName
unifies O
the O
pretraining O
objectives O
for O
different O
tasks O
as O
autoregressive O
blank O
infilling O
, O
with O
mixed O
attention O
masks O
and O
the O
novel O
2D B-HyperparameterValue
position I-HyperparameterValue
encodings I-HyperparameterValue
. O

Each O
dialogue O
is O
generated O
by O
users O
with O
a O
defined O
goal O
which O
may O
cover O
1 O
- O
5 O
domains O
with O
a O
maximum O
of O
13 O
turns O
in O
a O
conversation O
. O

We O
uses O
three O
evaluations O
metrics O
proposed O
by O
( O
Budzianowski O
et O
al O
. O
, O
2018a O
) O
. O

GLM B-MethodName
is O
a O
general O
pretraining O
framework O
for O
natural B-TaskName
language I-TaskName
understanding I-TaskName
and I-TaskName
generation I-TaskName
. O

NDR B-TaskName
combines O
deep O
neural O
network O
with O
discrete B-TaskName
and I-TaskName
symbolic I-TaskName
reasoning I-TaskName
( O
e.g. O
, O
addition B-TaskName
, O
sorting B-TaskName
, O
or O
counting B-TaskName
) O
( O
Dua O
et O
al O
. O
, O
2019 O
) O
and O
enables O
the O
comprehension O
of O
complex O
contexts O
and O
compositional O
questions O
, O
which O
is O
critical O
for O
many O
practical O
applications O
such O
as O
automatic O
diagnosis O
( O
Wei O
et O
al O
. O
, O
2018 O
) O
and O
robo O
- O
advisor O
( O
Fisch O
et O
al O
. O
, O
2019 O
) O
. O

We O
compared O
the O
activation O
boundary O
distillation O
with O
KL O
- O
divergence O
based O
distillation O
( O
KLD O
) O
, O
which O
penalizes O
the O
difference O
between O
the O
output O
probability O
distributions O
of O
the O
two O
models O
. O

Row O
7 O
uses O
different O
sentinel O
tokens O
instead O
of O
a O
single O
[ O
MASK O
] O
token O
to O
represent O
different O
masked O
spans O
. O

Since O
we O
use O
WordPiece B-HyperparameterValue
tokenization B-HyperparameterName
, O
a O
word O
can O
be O
split O
into O
several O
subword O
units O
. O

The O
conversations O
are O
between O
a O
tourist O
and O
a O
clerk O
at O
an O
information O
center O
. O

The O
first O
block O
includes O
several O
recent O
abstractive O
summarization O
models O
based O
on O
large O
pre O
- O
trained O
Transformers O
. O

The O
aim O
is O
to O
maximally O
preserve O
the O
semantics O
of O
the O
source O
sentence O
( O
" O
content O
" O
) O
but O
change O
other O
properties O
( O
" O
attributes O
" O
) O
, O
such O
as O
sentiment O
( O
Jin O
et O
al O
. O
, O
2020b O
) O
, O
expertise O
( O
Cao O
et O
al O
. O
, O
2020 O
) O
, O
formality O
( O
Rao O
and O
Tetreault O
, O
2018 O
) O
or O
a O
combination O
of O
them O
( O
Subramanian O
et O
al O
. O
, O
2018 O
) O
. O

For O
the O
pairwise O
casual O
reward O
learning O
network O
, O
we O
use O
three B-HyperparameterValue
single O
bi O
- O
LSTM O
layers O
, O
one O
each O
to O
encode O
goal O
, O
belief O
state O
and O
either O
dialogue O
act O
or O
response O
sequences O
at O
each O
dialogue O
turn O
on O
each O
of O
the O
sampled O
roll O
- O
outs O
pairs O
, O
τ O
1 O
and O
τ O
2 O
. O

Then O
, O
we O
perform O
hyperedge B-HyperparameterValue
matching I-HyperparameterValue
between O
the O
two O
hypergraphs O
by O
leveraging O
transformer O
- O
based O
attention O
mechanism O
. O

As O
shown O
in O
Table O
7 O
, O
ALBERT B-MethodName
- I-MethodName
DoKTRa I-MethodName
and O
RoBERTa B-MethodName
- I-MethodName
DoKTRa I-MethodName
outperformed O
the O
FinBERT B-MethodName
- I-MethodName
ft I-MethodName
teacher O
on O
financial O
downstream O
tasks O
. O

BART B-MethodName
is O
trained O
using O
cross O
- O
entropy O
loss O
between O
the O
decoder O
output O
and O
the O
original O
document O
. O

For O
training O
, O
we O
use O
only O
supervision O
from O
QA O
pairs O
without O
annotations O
for O
ground O
- O
truth O
reasoning O
paths O
. O

Each O
hyperedge O
connects O
the O
subset O
of O
vertices O

Here O
, O
we O
use O
four O
attention O
heads O
as O
multi O
- O
head O
. O

As O
shown O
in O
Figure O
1 O
, O
a O
hypothetical O
question O
includes O
an O
assumption O
, O
e.g. O
, O
" O
if O
the O
amount O
in O
2019 O
was O
$ O
132,935 O
thousand O
instead O
" O
. O

It O
may O
because O
the O
model- O

We O
compare O
both O
adaptation O
of O
our O
methods O
CASPI(DAMD B-MethodName
) I-MethodName
and O
CASPI(MinTL B-MethodName
) I-MethodName
on O
the O
endto O
- O
end O
dialogue O
tasks O
defined O
by O
MultiWoz2.0 B-DatasetName
( O
Budzianowski O
et O
al O
. O
, O
2018a O
) O
. O

For O
Transformer B-MethodName
, O
the O
hyperparameters O
of O
the O
Adam O
optimizer O
is O
a O
bit O
different O
, O
and O
we O
use O
β B-HyperparameterName
1 I-HyperparameterName
= O
0.9 B-HyperparameterValue
, O
β B-HyperparameterName
2 I-HyperparameterName
= O
0.98 B-HyperparameterValue
. O

Following O
a O
review O
of O
related O
research O
( O
Section O
2 O
) O
, O
we O
present O
the O
method O
( O
Section O
3 O
) O
, O
tests O
and O
results O
( O
Section O
4 O
) O
, O
discuss O
method O
and O
results O
( O
Section O
5 O
) O
, O
and O
finish O
with O
some O
conclusions O
( O
Section O
6).The O
situation O
memorably O
caricatured O
by O
Pedersen O
( O
2008 O
) O
still O
happens O
all O
the O
time O
: O
you O
download O
some O
code O
you O
read O
about O
in O
a O
paper O
and O
liked O
the O
sound O
of O
, O
you O
run O
it O
on O
the O
data O
provided O
, O
only O
to O
find O
that O
the O
results O
are O
not O
the O
same O
as O
reported O
in O
the O
paper O
, O
in O
fact O
they O
are O
likely O
to O
be O
worse O
( O
Belz O
et O
al O
. O
, O
2021a O
) O
. O

Mean O
pooling O
is O
applied O
when O
a O
node O
consists O
of O
multiple O
words O
. O

We O
can O
observe O
that O
: O
1 O
) O
Stacking O
more O
layers O
does O
not O
always O
bring O
performance O
gain O
. O

The O
second O
positional O
i O
d O
represents O
the O
intra O
- O
span O
position O
. O

Knowledge O
distillation O
is O
a O
class O
of O
methods O
that O
leverage O
the O
output O
of O
a O
( O
large O
) O
teacher O
model O
to O
guide O
the O
training O
of O
a O
( O
small O
) O
student O
model O
. O

Note O
that O
QA(• O
) O
measures O
the O
discrepancy O
between O
the O
ground O
- O
truth O
and O
the O
predicted O
answers O
which O
can O
have O
different O
formats O
. O

We O
can O
see O
that O
though O
random O
temperature B-HyperparameterName
based O
method O
is O
not O
as O
good O
as O
our O
best O
fixed O
- O
temperature B-HyperparameterName
method O
, O
it O
in O
general O
produces O
decent O
results O
. O

We O
conjecture O
this O
can O
be O
attributed O
to O
the O
low O
parameter O
efficiency O
of O
the O
encoder O
- O
decoder O
architecture O
and O
the O
denoising O
sequence O
- O
to O
- O
sequence O
objective O
. O

It O
is O
more O
accurate O
than O
ours O
around O
9.4 B-MetricValue
% I-MetricValue
in O
the O
recall B-MetricName
metric O
. O

To O
this O
end O
we O
choose O
two O
ToD B-TaskName
methods O
that O
are O
at O
the O
extremes O
of O
model O
architecture O
spectrum O
1 O
) O
One O
uses O
a O
light O
weight O
custom O
model O
and O
2 O
) O
Other O
uses O
a O
large O
standard O
pre O
- O
trained O
out O
- O
of O
- O
the O
box O
universal O
language O
model O
. O

This O
work O
was O
partly O
supported O
by O
the O
IITP O
( O
2015 O
- O
0 O
- O
00310 O
- O
SW.StarLab/20 O
% O
, O
2017 O
- O
0 O
- O
01772 O
- O
VTT/20 O
% O
, O
2019 O
- O
0 O
- O
01371 O
- O
BabyMind/10 O
% O
, O
2021 O
- O
0 O
- O
02068 O
- O
AIHub/10 O
% O
, O
2021 O
- O
0 O
- O
01343 O
- O
GSAI/10 O
% O
, O
2020 O
- O
0 O
- O
01373/10 O
% O
) O
grants O
, O
the O
KIAT O
( O
P0006720 O
- O
ILIAS/10 O
% O
) O
grant O
funded O
by O
the O
Korean O
government O
, O
and O
the O
Hanyang O
University O
( O
HY-202100000003160/10%).As O
the O
same O
as O
graph B-MethodName
convolutional I-MethodName
networks I-MethodName
, O
the O
knowledge O
and O
question O
graph O
are O
encoded O
separately O
by O
two B-MethodName
gated I-MethodName
graph I-MethodName
neural I-MethodName
networks I-MethodName
( O
GGNN B-MethodName
) O
. O

However O
, O
we O
are O
interested O
in O
pretraining O
a O
single O
model O
that O
can O
handle O
both O
NLU B-TaskName
and O
text B-TaskName
generation I-TaskName
. O

Fixing O
the O
parameter O
of O
PLM O
largely O
impedes O
the O
performance O
of O
TAGOP B-MethodName
- I-MethodName
L2I I-MethodName
on O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
, O
showing O
that O
encoding O
factual O
and O
hypothetical O
questions O
requires O
different O
mechanisms O
. O

To O
ensure O
the O
diversity O
of O
the O
phrasing O
, O
annotators O
are O
free O
to O
generate O
various O
phrasing O
of O
the O
assumption O
, O
and O
there O
is O
no O
restriction O
on O
the O
position O
of O
the O
assumption O
. O

GLM B-MethodName
: O
He O
was O
a O
voice O
actor O
for O
the O
" O
X O
- O
Men O
" O
cartoon O
series O
. O

We O
apply O
our O
method O
on O
the O
WMT16 B-DatasetName
En B-TaskName
- I-TaskName
De I-TaskName
translation I-TaskName
task O
. O

Here O
, O
we O
mainly O
focus O
on O
two O
aspects O
: O
i O
) O
effect O
of O
hypergraph O
and O
ii O
) O
effect O
of O
attention O
mechanism O
. O

All O
of O
the O
feature O
encoders O
mentioned O
in O
this O
paper O
use O
pre O
- O
trained O
mBERT B-MethodName
model O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
in O
HuggingFace O
Transformer O
1 O
, O
which O
has O
12 B-HyperparameterValue
Transformer B-HyperparameterName
blocks I-HyperparameterName
, O
12 B-HyperparameterValue
attention B-HyperparameterName
heads I-HyperparameterName
, O
and O
768 B-HyperparameterValue
hidden B-HyperparameterName
units I-HyperparameterName
. O

Finally O
, O
we O
compare O
GLM B-MethodName
variants O
with O
different O
pretraining B-HyperparameterName
designs I-HyperparameterName
to O
understand O
their O
importance O
. O

2.We O
propose O
a O
safe O
policy O
improvement O
method O
for O
task B-TaskName
oriented I-TaskName
dialogue I-TaskName
setting O
that O
guarantees O
performance O
against O
a O
baseline O
. O

Examples O
of O
QA B-TaskName
on O
diverse O
question O
categories O
are O
depicted O
in O
Figure O
4 O
. O

We O
sample O
a O
single O
span O
whose O
length O
is O
sampled O
from O
a O
uniform O
distribution O
over O
50%-100 B-HyperparameterValue
% I-HyperparameterValue
of I-HyperparameterValue
the I-HyperparameterValue
original I-HyperparameterValue
length I-HyperparameterValue
. O

, O
y O
|Y O
| O
) O
, O
we O
estimate O
the O
following O
conditional O
probability O
: O

For O
all O
datasets O
, O
we O
follow O
the O
experimental O
settings O
as O
in O
previous O
works O
. O

We O
argue O
that O
this O
update O
process O
effectively O
learns O
the O
high O
- O
order O
semantics O
inherent O
in O
each O
hypergraph O
and O
the O
high O
- O
order O
associations O
between O
two O
hypergraphs O
. O

Then O
, O
we O
will O
give O
an O
in O
- O
depth O
explanation O
. O

For O
DST B-TaskName
and O
response B-TaskName
generation I-TaskName
, O
we O
retain O
the O
cross O
entropy O
loss O
as O
is O
from O
DAMD B-MethodName
( O
Zhang O
et O
al O
. O
, O
2019).On O
the O
other O
extreme O
of O
model O
complexity O
, O
we O
use O
the O
Task O
oriented O
Dialogue O
model O
, O
MinTL B-MethodName
( O
Lin O
et O
al O
. O
, O
2020 O
) O
. O

† O
Corresponding O
authors O
. O

γ B-HyperparameterName
indicates O
consistency B-HyperparameterName
level I-HyperparameterName
between O
the O
outputs O
from O
two O
teacher O
models O
, O
e.g. O
for O
two O
input O
tokens O
, O
if O
the O
output O
from O
entity O
similarity O
teacher O
is O
high O
, O
and O
the O
similarity O
level O
computed O
from O
the O
outputs O
of O
the O
entity O
recognizer O
teacher O
is O
low O
, O
then O
their O
consistency B-HyperparameterName
level I-HyperparameterName
is O
low O
. O

The O
outputs O
of O
these O
attention O
layer O
are O
used O
as O
representation O
for O
predicting O
series O
of O
tokens O
for O
their O
respective O
modules O
. O

We O
also O
report O
the O
combined O
score O
( O
Inf B-MetricName
orm I-MetricName
+ O
Success B-MetricName
) O
× O
0.5 O
+ O
BLEU B-MetricName
proposed O
by O
Mehri O
et O
al O
. O
( O
2019 O
) O
. O

This O
paper O
describes O
and O
tests O
a O
method O
for O
carrying O
out O
quantified B-TaskName
reproducibility I-TaskName
assessment I-TaskName
( O
QRA B-TaskName
) O
that O
is O
based O
on O
concepts O
and O
definitions O
from O
metrology O
. O

T O
where O
x O
v O
is O
the O
v O
- O
th O
word O
embedding O
of O
each O
en O
- O
tity O
in O
the O
knowledge O
and O
question O
graph O
, O
a O

Following O
( O
Wu O
and O
Dredze O
, O
2019 O
) O
, O
all O
datasets O
are O
annotated O
using O
the O
BIO O
entity O
labelling O
scheme O
. O

As O
the O
baseline O
, O
we O
train O
a O
GPT B-MethodName
Large I-MethodName
model O
( O
Radford O
et O
al O
. O
, O
2018b;Brown O
et O
al O
. O
, O
2020 O
) O
with O
the O
same O
data O
and O
tokenization O
as O
GLM B-MethodName
Large I-MethodName
. O

[ O
S O
] O

The O
detailed O
statistics O
of O
the O
datasets O
are O
shown O
in O
Appendix O
A.Each O
node O
in O
the O
knowledge O
hypergraph O
and O
the O
question O
hypergraph O
is O
represented O
as O
a O
300dimensional B-HyperparameterValue
vector B-HyperparameterName
( O
i.e. O
, O
w B-HyperparameterName
= O
300 B-HyperparameterValue
) O
initialized O
using O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
. O

PQ B-DatasetName
and O
PQL B-DatasetName
contain O
7,106 O
and O
2,625 O
QA O
pairs O
on O
4,050 O
and O
9,844 O
knowledge O
facts O
from O
the O
subset O
of O
Freebase B-DatasetName
( O
Bollacker O
et O
al O
. O
, O
2008 O
) O
, O
respectively O
. O

Under O
weak O
supervision O
, O
previous O
studies O
proposed O
memory O
- O
based O
methods O
( O
Narasimhan O
and O
Schwing O
, O
2018 O
; O
and O
graph O
- O
based O
methods O
( O
Narasimhan O
et O
al O
. O
, O
2018 O
; O
to O
learn O
to O
selectively O
focus O
on O
necessary O
pieces O
of O
knowledge O
. O

Besides O
the O
λ B-HyperparameterName
values O
of O
1.5 B-HyperparameterValue
and O
2.0 B-HyperparameterValue
, O
we O
also O
try O
more O
values O
in O
a O
broader O
range O
. O

pervised O
manner O
, O
and O
then O
fine O
- O
tuned O
with O
a O
small O
dataset O
for O
several O
downstream O
tasks O
. O

the O
two O
metrics O
( O
e.g. O
, O
54.4→100 B-MetricValue
) O
, O
showing O
a O
large O
space O
for O
future O
exploration O
on O
the O
challenging O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
dataset O
. O

Siamese B-MethodName
network I-MethodName
assumes O
the O
input O
is O
a O
pair O
, O
and O
the O
output O
is O
a O
similarity O
score O
. O

We O
report O
the O
average B-MetricName
accuracy I-MetricName
of O
five O
repeated O
runs O
on O
different O
data O
split O
: O
76.55 B-MetricValue
as O
top-1 O
accuracy O
( O
average O
of O
76.93 B-MetricValue
, O
75.92 B-MetricValue
, O
76.24 B-MetricValue
, O
76.16 B-MetricValue
, O
and O
77.50 B-MetricValue
) O
and O
82.20 B-MetricValue
as O
top-3 O
accuracy B-MetricName
( O
average O
of O
82.90 B-MetricValue
, O
81.45 B-MetricValue
, O
81.70 B-MetricValue
, O
81.74 B-MetricValue
and O
83.20 B-MetricValue
) O
. O

As O
shown O
in O
Table O
7 O
of O
Appendix O
E O
, O
our O
model O
shows O
the O
best O
performances O
for O
both O
original O
and O
paraphrased O
questions O
. O

None O
of O
these O
pretraining O
frameworks O
is O
flexible O
enough O
to O
perform O
competitively O
across O
all O
NLP O
tasks O
. O

where O
Y O
is O
the O
label O
set O
. O

[ O
S O
] O

In O
the O
first O
example O
, O
both O
model O
, O
Hypergraph B-MethodName
Transformer I-MethodName
and O
Transformer B-MethodName
( I-MethodName
SA+GA I-MethodName
) I-MethodName
, O
infer O
the O
correct O
answer O
, O
Q5075293 O
. O

All O
the O
models O
are O
evaluated O
in O
the O
zero O
- O
shot O
setting O
. O

Since O
the O
results O
of O
the O
RoBERTa B-MethodName
- I-MethodName
large I-MethodName
model O
with O
a O
small O
batch B-HyperparameterName
size I-HyperparameterName
were O
unstable O
, O
we O
also O
performed O
a O
distributed O
training O
with O
three O
GPUs O
, O
resulting O
in O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
108 B-HyperparameterValue
. O

We O
use O
PyTorch O
1.7.1 O
to O
implement O
our O
model O
. O

However O
, O
Hypergraph B-MethodName
Transformer I-MethodName
shows O
robust O
reasoning O
capacity O
over O
the O
noisy O
inputs O
. O

To O
draw O
inferences O
for O
these O
question O
categories O
, O
the O
model O
needs O
to O
attend O
to O
multiple O
knowledge O
facts O
related O
to O
a O
given O
question O
, O
and O
conducts O
multi O
- O
hop O
reasoning O
based O
on O
the O
facts O
. O

Jaques O
et O
al O
. O
( O
2019 O
) O
and O
Wang O
et O
al O
. O
( O
2020 O
) O
uses O
Batch O
- O
RL O
for O
dialogue O
policy O
learning O
. O

BAN B-MethodName
calculates O
soft O
attention O
scores O
between O
knowledge O
entities O
and O
question O
words O
. O

( O
Narasimhan O
and O
Schwing O
, O
2018 O
; O
proposed O
memory O
- O
based O
methods O
that O
represent O
knowledge O
facts O
in O
the O
form O
of O
memory O
and O
calculate O
soft O
attention O
scores O
of O
the O
memory O
with O
a O
question O
. O

Multi O
- O
iteration O
derivation O
. O

As O
both O
TAPT B-MethodName
and O
DoK B-MethodName
- I-MethodName
Tra I-MethodName
only O
utilize O
the O
task O
- O
specific O
training O
data O
, O
they O
can O
be O
fairly O
compared O
in O
terms O
of O
performance O
and O
training O
resources O
. O

We O
set O
a O
query O

It O
's O
really O
[ O
MASK O
] O
" O
. O

where O
d B-HyperparameterName
v I-HyperparameterName
is O
the O
dimension B-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
query I-HyperparameterName
and O
the O
key O
vector O
. O

We O
find O
that O
removing O
the O
2D B-HyperparameterValue
positional I-HyperparameterValue
encoding I-HyperparameterValue
leads O
to O
lower O
accuracy B-MetricName
and O
higher O
perplexity B-MetricName
in O
language O
modeling O
. O

We O
apply O
L2I B-MethodName
to O
TAGOP B-MethodName
( O
Zhu O
et O
al O
. O
, O
2021 O
) O
, O
and O
obtain O
a O
promising O
solution O
for O
HQA B-TaskName
. O

• O
Deriving O
head O
. O

Our O
method O
improves O
the O
teacher O
model O
on O
CNNDM B-DatasetName
; O
ROUGE-2 B-MetricName
/ O
L B-MetricName
scores O
are O
improved O
on O
XSum B-DatasetName
; O
while O
on O
NYT B-DatasetName
, O
there O
are O
improvements O
on O
ROUGE-1 B-MetricName
/ O
L. B-MetricName

The O
weight B-HyperparameterName
for I-HyperparameterName
the I-HyperparameterName
contrastive I-HyperparameterName
loss I-HyperparameterName
is O
0.1.Overall B-HyperparameterValue
performance O
. O

SQuAD B-DatasetName
( O
Rajpurkar O
et O
al O
. O
, O
2016(Rajpurkar O
et O
al O
. O
, O
, O
2018 O
) O
is O
an O
extractive B-TaskName
question I-TaskName
answering I-TaskName
benchmark O
. O

Therefore O
, O
we O
introduce O
a O
concept O
of O
hypergraph O
and O
propose O
transformer O
- O
based O
attention O
mechanism O
over O
hypergraphs O
. O

When O
finetuning O
GLM B-MethodName
on O
the O
SuperGLUE B-DatasetName
tasks O
, O
we O
construct O
the O
input O
using O
the O
cloze O
questions O
in O
Table O
8 O
and O
replace O
the O
blank O
with O
a O
[ O
MASK O
] O
token O
. O

1 O
. O

Due O
to O
the O
size O
limit O
of O
supplementary O
material O
, O
we O
can O
not O
include O
the O
pretrained O
models O
, O
but O
will O
make O
them O
public O
available O
in O
the O
future O
. O

Therefore O
, O
the O
result O
shows O
that O
the O
NDR B-TaskName
model O
can O
achieve O
simple O
counterfactual O
thinking O
by O
learning O
to O
answer O
hypothetical O
questions O
. O

It O
consists O
of O
a O
node O
set O
V O
k O
and O
hyperedge O
set O
E O
k O
, O
which O
represent O
a O
set O
of O
entities O
in O
knowledge O
facts O
and O
a O
set O
of O
hyperedges O
, O
respectively O
. O

x O
6 O

We O
conjecture O
the O
performance O
drop O
in O
the O
first O
three O
groups O
is O
because O
the O
question B-TaskName
- I-TaskName
answering I-TaskName
label O
in O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
under O
the O
same O
c O
and O
q O
is O
different O
from O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
. O

To O
answer O
the O
given O
question O
as O
shown O
in O
Figure O
1 O
, O
a O
model O
should O
understand O
the O
semantics O
of O
the O
given O
question O
, O
link O
visual O
entities O
appearing O
in O
the O
given O
image O
to O
the O
KB O
, O
extract O
a O
number O
of O
evidences O
from O
the O
KB O
and O
predict O
an O
answer O
by O
aggregating O
semantics O
of O
both O
the O
question O
and O
the O
extracted O
evidences O
. O

Smith O
also O
won O
the O
Dick O
Butkus O
award O
as O
the O
nation O
's O
outstanding O
linebacker O
. O

Note O
that O
BERT B-MethodName
- I-MethodName
f I-MethodName
performs O
better O
than O
our O
model O
on O
the O
Chinese O
dataset O
due O
to O
their O
re O
- O
tokenization O
of O
the O
dataset O
. O

The O
multi O
- O
hop O
graph O
walk O
is O
conducted O
in O
the O
same O
manner O
as O
the O
knowledge O
hypergraph O
. O

Under O
this O
setting O
, O
GLM B-MethodName
410 I-MethodName
M I-MethodName
outperforms O
GPT B-MethodName
Large I-MethodName
. O

Our O
model O
shows O
comparable O
performances O
on O
PQ-{2H B-DatasetName
, I-DatasetName
3H I-DatasetName
, I-DatasetName
M I-DatasetName
} I-DatasetName
to O
the O
state O
- O
of O
- O
the O
- O
art O
weaklysupervised O
model O
, O
SRN O
. O

Entity O
linking O
setting O
We O
also O
present O
the O
experimental O
results O
on O
the O
entity O
linking O
setting O
where O
the O
named O
entities O
are O
not O
provided O
as O
the O
oracle O
setting O
, O
but O
detected O
by O
the O
module O
as O
described O
in O
Section O
3.2 O
. O

The O
model O
architecture O
and O
detailed O
operation O
of O
hypergraph B-MethodName
attention I-MethodName
networks I-MethodName
are O
similar O
to O
that O
of O
BAN B-MethodName
. O

x O
1 O
x O
2 O
x O
3 O
x O
4 O
x O
5 O
x O
6 O
x O
1 O
x O
2 O
[ O
M O
] O
x O
4 O
[ O
M O
] O
[ O
S O
] O
x O
5 O
x O
6 O
[ O
S O
] O
x O
3 O
x O
5 O
x O
6 O
[ O
E O
] O
x O
3 O
[ O
E O
] O
x O
1 O
x O
2 O
[ O
M O
] O
x O
4 O
[ O
M O
] O
[ O
S O
] O
x O
5 O
x O
6 O
[ O
S O
] O
x O
3 O
x O
1 O
x O
2 O
[ O
M O
] O

1 O

All O
transformer O
variant O
models O
described O
in O
this O
paper O
have O
the O
same O
fixed O
- O
number B-HyperparameterName
of I-HyperparameterName
sequence I-HyperparameterName
length I-HyperparameterName
as O
follows O
: O
300 B-HyperparameterValue
for O
1 O
- O
hop O
, O
1,000 B-HyperparameterValue
for O
2 O
- O
hop O
and O
1,800 B-HyperparameterValue
for O
3 O
- O
hop O
graphs O
. O

Among O
encoder O
- O
decoder O
models O
, O
BART B-MethodName
conducts O
NLU B-TaskName
tasks O
by O
feeding O
the O
same O
input O
into O
the O
encoder O
and O
decoder O
, O
and O
taking O
the O
final O
hidden O
states O
of O
the O
decoder O
. O

Example O
D.2 O
. O
Jonathan O
Terry O
is O
a O
television O
and O
film O
actor O
. O

Due O
to O
the O
two O
characteristics O
, O
the O
model O
shows O
better O
reasoning O
performance O
focusing O
on O
the O
evidences O
necessary O
for O
reasoning O
under O
weak O
supervision O
. O

The O
operation O
of O
the O
propagation B-HyperparameterName
layer I-HyperparameterName
is O
as O
follows O
: O
f O
( O
H O
( O
l O
) O
, O
A O
) O
= O
σ(D O
− O
1 O
2ÂD O
− O
1 O
2 O
H O
( O
l O
) O
W O
( O
l O
) O
) O
wherê O
A O
= O
A O
+ O
I O
, O
A O
is O
an O
adjacency O
matrix O
of O
the O
graph O
, O
I O
is O
an O
identity O
matrix O
, O
D O
is O
a O
degree O
matrix O
of O
A O
, O
W O
( O
l O
) O
is O
the O
model O
parameters O
of O
l O
- O
th O
layer O
, O
and O
H O
( O
l O
) O
is O
the O
representations O
of O
the O
graph O
in O
the O
l O
- O
th O
layer O
. O

Our O
encoding O
method O
ensures O
that O
the O
model O
is O
not O
aware O
of O
the O
length B-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
masked I-HyperparameterName
span I-HyperparameterName
when O
Coronet O
has O
the O
best O
lines O
of O
all O
day O
cruisers O
. O

To O
solve O
the O
task O
, O
two O
pioneering O
studies O
( O
Wang O
et O
al O
. O
, O
2017(Wang O
et O
al O
. O
, O
, O
2018 O
suggested O
logical O
parsing O
- O
based O
methods O
which O
convert O
a O
question O
to O
a O
KB O
logic O
query O
using O
predefined O
query O
templates O
and O
execute O
the O
generated O
query O
on O
KB O
for O
searching O
an O
answer O
. O

For O
TAGOP B-MethodName
- I-MethodName
CLO I-MethodName
, O
we O
conduct O
max O
pooling O
for O
H O
and O
adopt O
cosine B-MetricName
similarity I-MetricName
as O
the O
distance O
metric O
. O

On O
November O
15 O
, O
1970 O
, O
the O
Los O
Angeles O
Rams O
acquired O
Smith O
from O
the O
Lions O
in O
exchange O
for O
Linebacker O
Tony O
Harris O
. O

x O
6 O

TOF B-MethodName
( O
Zhang O
et O
al O
. O
, O
2021 O
) O
transfers O
knowledge O
from O
three O
aspects O
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
. O

Another O
disadvantage O
of O
BERT B-MethodName
is O
that O
it O
can O
not O
fill O
in O
the O
blanks O
of O
multiple O
tokens O
properly O
. O

Furthermore O
, O
whenever O
a O
new O
PLM O
emerges O
, O
it O
must O
be O
re O
- O
trained O
to O
create O
more O
advanced O
domain O
- O
specific O
models O
. O

He O
wrote O
for O
" O
the O
Guardian O
" O
newspaper O
. O

According O
to O
the O
ACM O
's O
definitions O
( O
Association O
for O
Computing O
Machinery O
, O
2020 O
) O
, O
results O
have O
been O
reproduced O
if O
obtained O
in O
a O
different O
study O
by O
a O
different O
team O
using O
artifacts O
supplied O
in O
part O
by O
the O
original O
authors O
, O
and O
replicated O
if O
obtained O
in O
a O
different O
study O
by O
a O
different O
team O
using O
artifacts O
not O
supplied O
by O
the O
original O
authors O
. O

To O
optimize O
the O
L2I B-MethodName
module O
, O
we O
incorporate O
supervision O
on O
the O
classifiers O
in O
the O
tagging O
head O
and O
deriving O
head O
. O

Our O
own O
fine O
- O
tuning O
version O
of O
BART B-MethodName
( O
BART B-MethodName
( I-MethodName
ours I-MethodName
) I-MethodName
) O
is O
comparable O
or O
slightly O
better O
than O
the O
original O
reported O
BART B-MethodName
results O
, O
and O
we O
use O
it O
as O
the O
teacher O
model O
on O
the O
three O
datasets O
. O

( O
3 O
) O
we O
replace O
ReLU B-HyperparameterValue
activation B-HyperparameterName
functions I-HyperparameterName
with O
GeLUs B-HyperparameterValue
( O
Hendrycks O
and O
Gimpel O
, O
2016).One O
of O
the O
challenges O
of O
the O
autoregressive O
blank O
infilling O
task O
is O
how O
to O
encode O
the O
positional O
information O
. O

The O
Transformer B-MethodName
( I-MethodName
SA I-MethodName
) I-MethodName
and O
Transformer B-MethodName
( I-MethodName
SA+GA I-MethodName
) I-MethodName
take O
single B-HyperparameterValue
- I-HyperparameterValue
word I-HyperparameterValue
- I-HyperparameterValue
unit I-HyperparameterValue
as O
input B-HyperparameterName
tokens I-HyperparameterName
, O
and O
Hypergraph B-MethodName
Transformer I-MethodName
takes O
hyperedges O
as O
input O
tokens O
. O

In O
ToD B-TaskName
, O
per O
- O
turn O
annotated O
belief O
- O
state O
does O
not O
capture O
the O
true O
state O
of O
the O
MDP O
. O

We O
finetune O
GLM B-MethodName
Large I-MethodName
on O
the O
training O
set O
for O
5 B-HyperparameterValue
epochs B-HyperparameterName
with O
dynamic O
masking O
, O
i.e. O
the O
blanks O
are O
randomly O
generated O
at O
training O
time O
. O

Consequent O
to O
applying O
our O
framework O
to O
ALBERT B-MethodName
and O
RoBERTa B-MethodName
student O
models O
, O
we O
were O
able O
to O
obtain O
models O
that O
retained O
most O
of O
the O
teacher O
model O
's O
performance O
with O
fewer O
model O
parameters O
( O
ALBERT B-MethodName
) O
, O
and O
models O
with O
a O
higher O
performance O
than O
both O
students O
and O
teachers O
( O
RoBERTa B-MethodName
) O
. O

One O
straightforward O
solution O
is O
to O
model O
counterfactual O
thinking O
as O
a O
generation O
procedure O
with O
the O
fact O
and O
assumption O
as O
inputs O
by O
using O
a O
generation O
model O
such O
as O
GPT B-MethodName
( O
Brown O
et O
al O
. O
, O
2020 O
) O
. O

Especially O
, O
HAN B-MethodName
employs O
stochastic O
graph O
walk O
to O
construct O
question O
and O
knowledge O
hypergraph O
. O

We O
propose O
Learning O
to O
Imagine O
, O
where O
the O
counterfactual O
thinking O
is O
implemented O
with O
two O
intervening O
steps O
: O
1 O
) O
identifying O
the O
facts O
to O
intervene O
, O
and O
2 O
) O
deriving O
the O
result O
of O
intervention O
. O

We O
use O
the O
similarity O
- O
based O
answer O
predictor O
for O
KVQA B-DatasetName
, O
and O
MLP O
for O
the O
others O
. O

In O
the O
evaluation O
, O
participants O
are O
presented O
with O
a O
document O
and O
a O
list O
of O
outputs O
by O
different O
models O
. O

The O
new O
Corona O
station O
opened O
in O
1988 O
, O
and O
the O
original O
Corona O
station O
was O
demolished O
. O

[ O
M O
] O

LAMBDA B-DatasetName
is O
a O
cloze O
- O
style O
dataset O
to O
test O
the O
ability O
of O
long O
- O
range O
dependency O
modeling O
. O

[ O
M O
] O

Comparison O
with O
sampling O
and O
tuning O
output O
layer O
temperature O
Sampling O
based O
methods O
can O
produce O
more O
diverse O
and O
richer O
outputs O
than O
its O
beam O
search O
based O
counterpart O
and O
has O
been O
proven O
useful O
in O
back B-TaskName
translation I-TaskName
( O
Edunov O
et O
al O
. O
, O
2018 O
) O
. O

Both O
models O
share O
the O
similar O
motivation O
, O
but O
the O
core O
operations O
are O
quite O
different O
. O

Different O
from O
KVQA B-DatasetName
focusing O
on O
world O
knowledge O
for O
named O
entities O
, O
FVQA B-DatasetName
considers O
commonsense O
knowledge O
about O
common O
nouns O
in O
a O
given O
image O
. O

It O
doubles O
the O
time O
cost O
of O
pretraining O
. O

The O
GAD B-DatasetName
dataset O
( O
Bravo O
et O
al O
. O
, O
2015 O
) O
consists O
of O
gene O
- O
disease O
binary O
relation O
annotations O
. O

Learning B-HyperparameterName
rates I-HyperparameterName
are O
picked O
from O
1e-4 B-HyperparameterValue
, O
3e-4 B-HyperparameterValue
, O
5e-4 B-HyperparameterValue
, O
7e-4 B-HyperparameterValue
accord O
- O
ing O
to O
validation O
sets O
. O

1 O
The O
code O
and O
pre O
- O
trained O
models O
are O
available O
at O
https O
: O
//github.com O
/ O
THUDM O
/ O
GLM O
In O
general O
, O
existing O
pretraining O
frameworks O
can O
be O
categorized O
into O
three O
families O
: O
autoregressive O
, O
autoencoding O
, O
and O
encoder O
- O
decoder O
models O
. O

More O
details O
on O
contrasting O
the O
merits O
and O
limitations O
of O
these O
methods O
can O
be O
found O
in O
Sec O
: O
A.1 O

Recently O
there O
's O
has O
been O
proliferation O
in O
use O
of O
large O
pretrained O
language O
model O
based O
systems O
like O
Hosseini O
- O
Asl O
et O
al O
. O
( O
2020 O
) O
, O
Lin O
et O
al O
. O
( O
2020 O
) O
, O
etc O
. O

Training O
on O
CNNDM B-DatasetName
with O
the O
teacher O
model O
( O
i.e. O
, O
BART B-MethodName
) O
is O
most O
time O
- O
consuming O
. O

This O
result O
suggests O
that O
DoKTra B-MethodName
can O
be O
applied O
regardless O
of O
the O
domain O
and O
can O
be O
an O
efficient O
alternative O
to O
in O
- O
domain O
pre O
- O
training O
. O

Comparison O
with O
XLNet B-MethodName
( O
Yang O
et O
al O
. O
, O
2019 O
) O
. O

When O
setting O
the O
coefficient B-HyperparameterName
of O
the O
cross O
attention O
module O
to B-HyperparameterName
λ I-HyperparameterName
cross I-HyperparameterName
= O
1.0 B-HyperparameterValue
, O
the O
ROUGE B-MetricName
scores O
drop O
most O
. O

Abstractive B-TaskName
summarization I-TaskName
aims O
to O
rewrite O
a O
document O
into O
its O
shorter O
form O
( O
i.e. O
, O
summary O
) O
, O
which O
is O
a O
typical O
Seq2Seq O
learning O
problem O
. O

We O
follow O
the O
standard O
pre O
- O
processing O
steps O
described O
in O
See O
et O
al O
. O
( O
2017 O
) O
; O
Liu O
and O
Lapata O
( O
2019 O
) O
. O

To O
sum O
up O
, O
Hypergraph B-MethodName
Transformer I-MethodName
takes O
graph O
- O
level O
inputs O
, O
i.e. O
, O
hyperedge O
, O
and O
conducts O
semantic O
matching O
between O
hyperedges O
by O
the O
attention O
mechanism O
. O

GLM B-MethodName
formulates O
NLU B-TaskName
tasks O
as O
cloze B-TaskName
questions I-TaskName
that I-TaskName
contain I-TaskName
task I-TaskName
descriptions I-TaskName
, O
which O
can O
be O
answered O
by O
autoregressive O
generation O
. O

Despite O
this O
growing O
body O
of O
research O
, O
no O
consensus O
has O
emerged O
about O
standards O
, O
terminology O
and O
definitions O
. O

Position O
1 O
1 O
2 O
3 O
4 O
5 O
5 O
5 O
5 O
3 O
3 O
Position O
2 O
0 O
0 O
0 O
0 O
0 O
1 O
2 O
3 O
1 O
2 O

For O
Wieling O
et O
al O
. O
( O
2018 O
) O
, O
reproducibility O
is O
achieving O
the O
same O
results O
using O
the O
same O
data O
and O
methods O
. O

We O
repeatedly O
sample O
new O
spans O
until O
at O
least O
15 B-HyperparameterValue
% I-HyperparameterValue
of O
the O
original B-HyperparameterName
tokens I-HyperparameterName
are I-HyperparameterName
masked I-HyperparameterName
. O

The O
key O
to O
achieving O
counterfactual O
thinking O
in O
NDR B-TaskName
lies O
in O
: O
1 O
) O
parsing O
the O
assumption O
to O
identify O
the O
target O
fact O
to O
intervene O
; O
and O
2 O
) O
deriving O
the O
assumed O
value O
to O
construct O
the O
counterfactual O
context O
. O

For O
example O
, O
XL B-MethodName
- I-MethodName
Net I-MethodName
( O
Yang O
et O
al O
. O
, O
2019 O
) O
encodes O
the O
original O
position O
so O
that O
it O
can O
perceive O
the O
number O
of O
missing O
tokens O
, O
and O
SpanBERT B-MethodName
( O
Joshi O
et O
al O
. O
, O
2020 O
) O
replaces O
the O
span O
with O
multiple O
[ O
MASK O
] O
tokens O
and O
keeps O
the O
length O
unchanged O
. O

Recently O
, O
transformer O
( O
Vaswani O
et O
al O
. O
, O
2017)-based O
language O
models O
have O
been O
successfully O
applied O
in O
the O
field O
of O
natural O
language O
processing O
( O
NLP O
) O
. O

We O
also O
employ O
a O
weight B-HyperparameterName
decay I-HyperparameterName
of O
0.01 B-HyperparameterValue
. O

As O
mentioned O
before O
, O
our O
best O
model O
outperformed O
the O
BioBERT B-MethodName
( O
teacher O
) O
model O
on O
four O
of O
the O
five O
tasks O
. O

These O
include O
: O
1 O
) O
inform B-MetricName
ratemeasures I-MetricName
the O
fraction O
of O
dialogue O
, O
the O
system O
has O
provided O
the O
correct O
entity O
, O
2 O
) O
success B-MetricName
rate I-MetricName
-fraction O
of O
dialogues O
, O
the O
system O
has O
answered O
all O
the O
requested O
information O
and O
3 O
) O
BLEU B-MetricName
( O
Papineni O
et O
al O
. O
, O
2002 O
) O
-measures O
the O
fluency O
of O
the O
generated O
response O
. O

What O
HAN B-MethodName
and O
our O
model O
have O
in O
common O
is O
introducing O
a O
hypergraph O
to O
consider O
high O
- O
order O
relationships O
in O
question O
graph O
and O
knowledge O
graph O
. O

Following O
( O
Vaswani O
et O
al O
. O
, O
2017;Tsai O
et O
al O
. O
, O
2019 O
) O
, O
we O
apply O
positional O
embeddings O
to O
the O
input O
sequence O
of O
both O
models O
. O

Therefore O
the O
probability O
of O
the O
sentence O
being O
positive O
or O
negative O
is O
proportional O
to O
predicting O
" O
good O
" O
or O
" O
bad O
" O
in O
the O
blank O
. O

All O
ROUGE B-MetricName
scores O
are O
computed O
using O
the O
ROUGE-1.5.5.pl O
script O
4 O
. O

The O
station O
burned O
down O
and O
was O
rebuilt O
in O
1908 O
. O

To O
compare O
our O
approach O
with O
the O
in O
- O
domain O
pre O
- O
training O
method O
, O
we O
used O
RoBERTa B-MethodName
- I-MethodName
PM I-MethodName
- I-MethodName
large I-MethodName
( O
Lewis O
et O
al O
. O
, O
2020 O
) O
, O
which O
is O
a O
RoBERTa B-MethodName
- I-MethodName
large I-MethodName
model O
additionally O
pre O
- O
trained O
with O
a O
large O
biomedical O
and O
clinical O
corpus O
consisting O
of O
14 O
billion O
words O
. O

Finetuning O
UniLM B-MethodName
on O
downstream O
generation O
tasks O
also O
relies O
on O
masked O
language O
modeling O
, O
which O
is O
less O
efficient O
. O

The O
remaining O
100,834 O
articles O
are O
further O
split O
into O
training O
and O
validation O
sets O
. O

We O
use O
Transformer B-MethodName
- I-MethodName
Big I-MethodName
model O
as O
the O
teacher O
and O
Transformer B-MethodName
- I-MethodName
Base I-MethodName
as O
the O
student O
. O

Resorting O
to O
the O
language O
of O
causality O
, O
it O
can O
be O
expressed O
as O
the O
do O
- O
operation O
that O
intervenes O
a O
variable O
to O
execute O
the O
assumption O
and O
the O
action O
to O
derive O
the O
outcome O
of O
the O
intervention O
3 O
( O
Pearl O
, O
2009 O
) O
. O

Instead O
, O
we O
reformulate O
NLU B-TaskName
classification O
tasks O
as O
generation O
tasks O
of O
blank O
infilling O
, O
following O
PET O
( O
Schick O
and O
Schütze O
, O
2020a O
) O
. O

GLM B-MethodName
RoBERTa I-MethodName
can O
achieve O
performance O
matching O
the O
seq2seq B-MethodName
BART I-MethodName
model O
, O
and O
outperform O
T5 B-MethodName
and O
UniLMv2 B-MethodName
. O

TAGOP B-MethodName
- I-MethodName
CLO I-MethodName
, O
incorporating O
the O
Contrastive O
Learning O
Objective O
( O
CLO O
) O
into O
the O
training O
objective O
of O
TAGOP B-MethodName
, O
which O
is O
shown O
to O
be O
effective O
in O
learning O
the O
relationship O
between O
factual O
and O
counterfactual O
samples O
( O
Liang O
et O
al O
. O
, O
2020 O
) O
. O

With O
the O
same O
amount O
of O
parameters O
, O
encoding O
the O
context O
with O
bidirectional O
attention O
can O
improve O
the O
performance O
of O
language O
modeling O
. O

In O
NLP O
, O
reproduction O
studies O
generally O
address O
the O
following O
question O
: O
if O
we O
create O
and/or O
evaluate O
this O
system O
multiple O
times O
, O
will O
we O
obtain O
the O
same O
results O
? O

By O
passing O
the O
guided O
- O
attention O
blocks O
and O
selfattention O
blocks O
sequentially O
, O
representations O
of O
knowledge O
hyperedges O
and O
question O
hyperedges O
are O
updated O
and O
finally O
aggregated O
to O
single O
vector O
representation O
as O
z O
k O
∈ O
R O
dv B-HyperparameterName
and O
z O
q O
∈ O
R O
dv B-HyperparameterName
, O
respectively O
. O

[ O
S O
] O

PLATE B-MethodName
B12 I-MethodName
- I-MethodName
3 I-MethodName
rnd I-MethodName
means O
that O
we O
use O
random O
attention B-HyperparameterName
temperature I-HyperparameterName
of O
λ B-HyperparameterName
∼ O
U O
[ O
1.0 B-HyperparameterValue
, O
2.0 B-HyperparameterValue
] O
. O

( O
Kim O
et O
al O
. O
, O
2018 O
) O
and O
hypergraph B-MethodName
attention I-MethodName
networks I-MethodName
( O
HAN B-MethodName
) O
( O
Kim O
et O
al O
. O
, O
2020 O
) O
consider O
interactions O
between O
knowledge O
and O
question O
based O
on O
co O
- O
attention O
mechanism O
. O

The O
improvement O
indicates O
that O
learning O
the O
relationship O
between O
the O
factual O
and O
counterfactual O
samples O
with O
CLO O
provides O
some O
clue O
for O
counterfactual O
imagination O
, O
yet O
it O
is O
still O
worse O
than O
directly O
learning O
to O
imagine O
with O
neural O
network O
modules O
. O

the O
discrete O
operation O
required O
in O
answering O
the O
question O
or O
counterfactual O
thinking O
. O

We O
apply O
PLATE B-MethodName
to O
the O
WMT16 B-DatasetName
( O
Bojar O
et O
al O
. O
, O
2016 O
) O
English B-TaskName
- I-TaskName
German I-TaskName
translation I-TaskName
task O
and O
use O
Transformer B-MethodName
- I-MethodName
big I-MethodName
as O
the O
teacher O
and O
Transformer B-MethodName
- I-MethodName
base I-MethodName
as O
the O
student O
. O

The O
mean O
proportions O
of O
evident O
attentions O
for O
all O
bins O
are O
shown O
in O
Figure O
2 O
. O

We O
report O
the O
performance O
of O
standard O
finetuning O
( O
i.e. O
classification O
on O
the O
[ O
CLS O
] O
token O
representation O
) O
. O

As O
a O
result O
, O
GLM B-MethodName
can O
significantly O
outperform O
T5 B-MethodName
on O
NLU B-TaskName
and O
seq2seq B-TaskName
tasks I-TaskName
with O
fewer O
parameters O
and O
data O
, O
as O
stated O
in O
Sections O
3.2 O
and O
3.3 O
. O

Moreover O
, O
some O
research O
introduces O
new O
components O
on O
top O
of O
the O
mBERT B-MethodName
by O
directly O
transferring O
the O
model O
learned O
from O
the O
labeled O
source O
language O
to O
that O
of O
target O
languages O
( O
Keung O
et O
al O
. O
, O
2019 O
) O
. O

Smith O
played O
for O
the O
Michigan O
Wolverines O
football O
team O
from O
1959 O
to O
1963 O
. O

Second O
, O
autoregressive O
models O
are O
trained O
with O
a O
left O
- O
to O
- O
right O
language O
modeling O
objective O
( O
Radford O
et O
al O
. O
, O
2018a O
, O
b;Brown O
et O
al O
. O
, O
2020 O
) O
. O

As O
shown O
in O
Table O
4 O
, O
using O
large O
attention B-HyperparameterName
temperature I-HyperparameterName
coefficients I-HyperparameterName
( O
2.0 B-HyperparameterValue
) O
for O
all O
three O
types O
of O
attention O
modules O
leads O
to O
the O
best O
result O
. O

Most O
language B-TaskName
modeling I-TaskName
datasets O
such O
as O
WikiText103 B-DatasetName
are O
constructed O
from O
Wikipedia O
documents O
, O
which O
our O
pretraining O
dataset O
already O
contains O
. O

GPT-2 B-MethodName
( O
Radford O
et O
al O
. O
, O
2018b O
) O
and O
GPT-3 B-MethodName
( O
Brown O
et O
al O
. O
, O
2020 O
) O
show O
that O
generative O
language O
models O
can O
complete O
NLU B-TaskName
tasks O
such O
as O
question B-TaskName
answering I-TaskName
by O
directly O
predicting O
the O
correct O
answers O
without O
finetuning O
, O
given O
task O
instructions O
or O
a O
few O
labeled O
examples O
. O

In O
Section O
D O
, O
we O
describe O
the O
experimental O
details O
for O
each O
dataset O
. O

The O
initially O
fine O
- O
tuned O
student O
models O
are O
in O
the O
second O
and O
fourth O
rows O
and O
the O
DoKTra B-MethodName
framework O
is O
applied O
to O
both O
, O
as O
shown O
in O
the O
third O
and O
fifth O
rows O
. O

In O
entity O
linking O
setting O
, O
the O
constructed O
knowledge O
hypergraph O
can O
be O
incomplete O
and O
quite O
noisy O
due O
to O
the O
undetected O
entities O
or O
misclassified O
entity O
labels O
. O

Modern O
neural O
networks O
methods O
are O
capable O
of O
mapping O
data O
from O
one O
domain O
to O
another O
. O

2H B-HyperparameterValue
and O
3H B-HyperparameterValue
denote O
the O
number B-HyperparameterName
of I-HyperparameterName
hops I-HyperparameterName
( O
i.e. O
, O
2 B-HyperparameterValue
- I-HyperparameterValue
hop I-HyperparameterValue
and O
3 B-HyperparameterValue
- I-HyperparameterValue
hop I-HyperparameterValue
) O
in O
ground O
- O
truth O
reasoning O
paths O
. O

3 O
) O
The O
performance O
of O
TAGOP B-MethodName
on O
arithmetic O
has O
a O
large O
gap O
with O
other O
types O
, O
showing O
that O
arithmetic O
questions O
are O
more O
difficult O
to O
conduct O
imagination O
and O
reasoning O
even O
though O
arithmetic O
makes O
up O
the O
majority O
of O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
data O
. O

On O
September O
10 O
, O
1968 O
, O
he O
was O
traded O
back O
to O
Los O
Angeles O
for O
a O
second O
round O
pick O
in O
the O
1970 O
draft O
. O

However O
, O
models O
built O
using O
unsupervised O
methods O
perform O
poorly O
when O
compared O
to O
supervised O
( O
parallel O
) O
training O
( O
Artetxe O
et O
al O
. O
, O
2020 O
) O
. O

We O
adopt O
Adam B-HyperparameterValue
( O
Kingma O
and O
Ba O
, O
2015 O
) O
to O
optimize O
all O
learnable O
parameters O
in O
the O
model O
. O

( O
1 O
) O
MTST B-MethodName
, O
which O
combines O
the O
multiple O
- O
teacher O
to O
single O
- O
teacher O
. O

Position O
1 O
1 O
2 O
3 O
4 O
5 O
5 O
5 O
5 O
3 O
3 O
Position O
2 O
0 O
0 O
0 O
0 O
0 O
1 O
2 O
3 O
1 O
2 O

[ O
S O
] O

We O
then O
explore O
the O
influence O
of O
network O
architecture O
on O
the O
effectiveness O
of O
the O
L2I B-MethodName
module O
from O
three O
perspectives O
: O
1 O
) O
module B-HyperparameterName
depth I-HyperparameterName
; O
2 O
) O
configuration O
of O
the O
matching B-HyperparameterName
block I-HyperparameterName
; O
and O
3 O
) O
the O
setting O
of O
PLM O
. O

able O
matrices O
, O
and O
• O
is O
element O
- O
wise O
multiplication O
. O

The O
second O
block O
presents O
results O
of O
student O
models O
. O

Eq O
1 O
) O
. O

( O
Wang O
et O
al O
. O
, O
2020 O
) O
is O
first O
to O
argue O
the O
use O
of O
automated O
evaluation O
metrics O
directly O
as O
reward O
is O
under O
- O
specified O
for O
ToD B-TaskName
policy O
learning O
. O

This O
demonstrates O
GLM B-MethodName
's O
advantage O
in O
handling O
variable O
- O
length O
blank O
. O

Consequently O
, O
the O
distilled O
ALBERT B-MethodName
achieved O
a O
higher O
performance O
than O
the O
teacher O
model O
on O
ChemProt B-DatasetName
and O
DDI B-DatasetName
. O

h O
CLS O
corresponds O
to O
the O
CLS O
token O
in O
H.Most O
recent O
NDR B-TaskName
models O
( O
Ran O
et O
al O
. O
, O
2019;Andor O
et O
al O
. O
, O
2019;Chen O
et O
al O
. O
, O
2020a;Herzig O
et O
al O
. O
, O
2020;Zhu O
et O
al O
. O
, O
2021 O
) O
( O
Dua O
et O
al O
. O
, O
2019 O
) O
. O

It O
can O
be O
seen O
that O
the O
performance O
decrease O
in O
terms O
of O
F1 B-MetricName
- O
score O
ranges O
from O
0.45 B-MetricValue
for O
Dutch(nl O
) O
to O
0.98 B-MetricValue
for O
Spanish(es O
) O
, O
which O
validates O
that O
weighting O
loss O
can O
bring O
more O
confident O
knowledge O
to O
the O
student O
model O
. O

However O
, O
such O
uncontrollable O
model O
( O
Zou O
et O
al O
. O
, O
2021 O
) O
can O
hardly O
generate O
high O
- O
quality O
context O
for O
two O
reasons O
: O
1 O
) O
the O
context O
is O
more O
complex O
than O
plain O
text O
, O
which O
can O
include O
a O
table O
( O
Figure O
1 O
) O
; O
and O
2 O
) O
NDR B-TaskName
requires O
a O
precise O
context O
with O
the O
correct O
numbers O
( O
Figure O
1 O
, O
$ O
132,935 O
for O
the O
finished O
goods O
in O
2019 O
) O
. O

In O
addition O
to O
the O
triplet O
- O
based O
graph O
walks O
, O
a O
multihop O
graph O
walk O
is O
proposed O
to O
encode O
multiple O
relational O
facts O
that O
are O
interconnected O
. O

In O
addition O
, O
we O
refine O
a O
list O
of O
de O
- O
tected O
named O
entities O
by O
matching O
the O
associated O
image O
caption O
( O
i.e. O
, O
Wikipedia O
caption O
) O
. O

ALBERT B-MethodName
has O
two O
advantages O
: O
a O
small O
number O
of O
parameters O
and O
high O
performance O
( O
Lan O
et O
al O
. O
, O
2019 O
) O
. O

We O
consider O
that O
the O
reason O
why O
Hypergraph B-MethodName
Transformer I-MethodName
failed O
to O
infer O
the O
correct O
answer O
despite O
focusing O
on O
the O
exact O
knowledge O
fact O
is O
that O
the O
correct O
answer O
word O
( O
Myocardial O
Infarction O
) O
appears O
rarely O
in O
QA O
pairs O
. O

( O
Auer O
et O
al O
. O
, O
2007 O
) O
, O
ConceptNet B-DatasetName
( O
Liu O
and O
Singh O
, O
2004 O
) O
, O
and O
WebChild B-DatasetName
( O
Tandon O
et O
al O
. O
, O
2014 O
) O
. O

( O
Mayhew O
et O
al O
. O
, O
2017;Xie O
et O
al O
. O
, O
2018;Wu O
et O
al O
. O
, O
2020b O
) O
. O

In O
this O
work O
, O
we O
extend O
NDR B-TaskName
to O
hypothetical B-TaskName
question I-TaskName
answering I-TaskName
( O
HQA B-TaskName
) O
, O
where O
the O
question O
consists O
of O
an O
assumption O
beyond O
the O
context O
( O
Figure O
1 O
) O
. O

Besides O
, O
the O
performance O
difference O
between O
SIM O
and O
MLP O
in O
one O
- O
shot O
answer O
( O
appeared O
in O
the O
only O
one O
time O
in O
training O
phase O
) O
is O
more O
than O
18 O
% O
. O

The O
student O
models O
are O
with O
the O
BART B-MethodName
12 I-MethodName
- I-MethodName
6 I-MethodName
setting O
and O
are O
trained O
on O
CNNDM B-DatasetName
and O
the O
following O
examples O
are O
from O
the O
validation O
set O
of O
CNNDM B-DatasetName
. O

We O
first O
note O
that O
KVQA B-DatasetName
dataset O
includes O
a O
large O
number O
of O
unique O
answers O
( O
19,360 O
) O
, O
and O
contains O
a O
lot O
of O
zero O
- O
shot O
and O
few O
- O
shot O
answers O
in O
test O
phase O
. O

Then O
, O
we O
select O
the O
strength O
of O
the O
confidence B-HyperparameterName
regularization I-HyperparameterName
( O
β B-HyperparameterName
1 I-HyperparameterName
) O
by O
a O
grid O
search O
in O
terms O
of O
the O
F1 B-MetricName
score O
and O
expected B-MetricName
calibration I-MetricName
error I-MetricName
( O
ECE B-MetricName
) O
on O
the O
development O
set O
. O

There O
are O
three O
types O
of O
pretrained O
models O
. O

In O
the O
clinical O
domain O
, O
the O
i2b2 B-DatasetName
dataset O
( O
Uzuner O
et O
al O
. O
, O
2011 O
) O
contains O
texts O
from O
clinical O
documents O
, O
and O
eight O
types O
of O
relations O
between O
medical O
problems O
and O
treatments O
have O
been O
annotated O
. O

The O
proposed O
model O
, O
Hypergraph B-MethodName
Transformer I-MethodName
, O
constructs O
a O
question O
hypergraph O
and O
a O
query O
- O
aware O
knowledge O
hypergraph O
, O
and O
infers O
an O
answer O
by O
encoding O
inter O
- O
associations O
between O
two O
hypergraphs O
and O
intra O
- O
associations O
in O
both O
hypergraph O
itself O
. O

1 O

It O
can O
be O
seen O
that O
our O
model O
outperforms O
the O
state O
- O
of O
- O
the O
- O
arts O
. O

While O
they O
succeed O
in O
long B-TaskName
- I-TaskName
text I-TaskName
generation I-TaskName
and O
show O
fewshot O
learning O
ability O
when O
scaled O
to O
billions O
of O
parameters O
( O
Radford O
et O
al O
. O
, O
2018b;Brown O
et O
al O
. O
, O
2020 O
) O
, O
the O
inherent O
disadvantage O
is O
the O
unidirectional O
attention O
mechanism O
, O
which O
can O
not O
fully O
capture O
the O
dependencies O
between O
the O
context O
words O
in O
NLU B-TaskName
tasks O
. O

We O
also O
implemented O
cloze O
- O
style O
finetuning O
for O
the O
other O
pre O
- O
trained O
models O
, O
but O
the O
performance O
was O
usually O
similar O
to O
the O
standard O
classifier O
, O
as O
we O
shown O
in O
the O
ablation O
study O
. O

For O
the O
question B-TaskName
generation I-TaskName
task O
, O
we O
use O
the O
SQuAD B-DatasetName
1.1 I-DatasetName
dataset O
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
and O
follow O
the O
dataset B-HyperparameterName
split I-HyperparameterName
of O
( O
Du O
et O
al O
. O
, O
2017 O
) O
. O

" O
Smith O
was O
inducted O
into O
the O
university O
of O
Michigan O
athletic O
hall O
of O
honor O
in O
1989 O
and O
the O
national O
football O
foundation O
hall O
of O
fame O
in O
1991 O
. O

It O
indicates O
that O
the O
teachers O
can O
produce O
more O
concise O
and O
abstractive O
summaries O
, O
which O
matches O
the O
goal O
of B-TaskName
abstractive I-TaskName
summarization I-TaskName
. O

Note O
that O
the O
performance O
on O
GAD B-DatasetName
in O
Table O
4 O
was O
evaluated O
with O
the O
first O
split O
of O
a O
10 B-HyperparameterValue
- O
fold O
crossvalidation O
, O
while O
the O
main O
result O
in O
Table O
3 O
was O
evaluated O
with O
all O
splits O
. O

Under O
the O
wyoming O
state O
constitution O
, O
the O
governor O
can O
veto O
the O
actions O
of O
the O
other O
members O
of O
the O
wyoming O
house O
of O
representatives O
. O

In O
Section O
E O
, O
we O
depict O
the O
implementation O
details O
of O
comparative O
models O
for O
KVQA.The B-DatasetName
diverse O
split B-HyperparameterName
statistics I-HyperparameterName
for O
four O
benchmark O
datasets O
, O
KVQA B-DatasetName
, O
FVQA B-DatasetName
( O
Wang O
et O
al O
. O
, O
2018 O
) O
, O
PQ B-DatasetName
and O
PQL B-DatasetName
( O
Zhou O
et O
al O
. O
, O
2018 O
) O
, O
are O
shown O
in O
Table O
4 O
. O

The O
work O
use O
variable O
distribution O
: O
via O
pretraining O
, O
to O
obtain O
an O
informed O
prior O
, O
and O
uses O
autoencoding O
as O
the O
auxiliary O
task O
, O
to O
capture O
generative O
factors O
of O
dialogue O
responses O
. O

We O
set O
the B-HyperparameterName
batch I-HyperparameterName
size I-HyperparameterName
to O
be O
32 B-HyperparameterValue
, O
maximum B-HyperparameterName
sequence I-HyperparameterName
length I-HyperparameterName
to O
be O
128 B-HyperparameterValue
, O
dropout B-HyperparameterName
rate I-HyperparameterName
to O
be O
0.2 B-HyperparameterValue
, O
and O
we O
use O
Adam B-HyperparameterValue
as O
optimizer B-HyperparameterName
( O
Kingma O
and O
Ba O
, O
2014 O
) O
. O

Extensive O
experiments O
on O
various O
datasets O
, O
KVQA B-DatasetName
, O
FVQA B-DatasetName
, O
PQ B-DatasetName
, O
and O
PQL B-DatasetName
validated O
that O
Hypergraph B-MethodName
Transformer I-MethodName
conducts O
accurate O
inference O
by O
focusing O
on O
knowledge O
evidences O
necessary O
for O
question O
from O
a O
large O
knowledge O
graph O
. O

For O
BART B-MethodName
12 I-MethodName
- I-MethodName
6 I-MethodName
( O
or O
BART B-MethodName
12 I-MethodName
- I-MethodName
3 I-MethodName
) O
, O
the O
decoder O
is O
initialized O
from O
the O
first O
6 B-HyperparameterValue
( O
or O
3 B-HyperparameterValue
) O
layers O
or O
the O
maximally O
spaced O
6 B-HyperparameterValue
( O
or O
3 B-HyperparameterValue
) O
layers O
of O
BART B-MethodName
decoder O
. O

T5 B-MethodName
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
, O
PEGASUS B-MethodName
( O
Zhang O
et O
al O
. O
, O
2020 O
) O
and O
BART B-MethodName
( O
Lewis O
et O
al O
. O
, O
2020 O
) O
are O
three O
popular O
large O
Seq2Seq O
Transformer O
models O
with O
different O
pretraining O
objectives O
. O

The O
SuperGLUE B-DatasetName
benchmark O
consists O
of O
8 O
NLU O
tasks O
. O

The O
possible O
maximum O
pre O
- O
training O
batch B-HyperparameterName
size I-HyperparameterName
with O
the O
given O
computing O
resource O
for O
the O
RoBERTa B-MethodName
- I-MethodName
large I-MethodName
model O
was O
36 B-HyperparameterValue
. O

We O
select O
the O
corresponding O
factual O
question O
as O
the O
positive O
sample O
and O
a O
randomly O
selected O
factual O
question O
as O
the O
negative O
sample O
. O

Our O
best O
performing O
student O
model O
PLATE B-MethodName
B12 I-MethodName
- I-MethodName
3 I-MethodName
λ=1.5 I-MethodName
outperforms O
BART B-MethodName
- I-MethodName
PL I-MethodName
, O
BART B-MethodName
- I-MethodName
SFT I-MethodName
, O
and O
BART B-MethodName
- I-MethodName
KD I-MethodName
on O
XSum B-DatasetName
. O

Therefore O
, O
Kim O
and O
Rush O
( O
2016 O
) O
introduce O
a O
sequence O
- O
level O
knowledge O
distillation O
method O
( O
i.e. O
, O
pseudo O
- O
labeling O
) O
, O
where O
a O
student O
model O
is O
trained O
with O
pseudo O
labels O
generated O
by O
the O
teacher O
model O
using O
beam O
search O
decoding O
. O

For O
tokens O
in O
Part O
B O
, O
they O
range O
from O
1 O
to O
the O
length O
of O
the O
span O
. O

Q O
k O
= O
E O
k O
W O
Q O
k O
, O
a O
key O
K O
q O
= O
E O
q O
W O
Kq O
, O
and O
a O
value O
V O
q O
= O
E O
q O
W O
Vq O
, O

The O
statistics O
of O
the O
pre O
- O
processed O
downstream O
task O
datasets O
are O
listed O
in O
For O
the O
experiments O
, O
we O
used O
the O
pre O
- O
trained O
BioBERT B-MethodName
- I-MethodName
base I-MethodName
model O
( O
L=12 B-HyperparameterName
, O
H=768 B-HyperparameterName
, O
A=12 B-HyperparameterName
) O
as O
the O
initial O
teacher O
model O
. O

It O
is O
pre O
- O
trained O
on O
the O
task O
of O
denoising O
corrupt O
documents O
. O

Our O
method O
aims O
to O
make O
these O
large O
models O
faster O
. O

We O
randomly O
sample O
spans O
of O
length O
drawn O
from O
a O
Poisson O
distribution O
with O
λ B-HyperparameterName
= O
3 B-HyperparameterValue
. O

They O
also O
use O
BART B-MethodName
as O
teacher O
models O
. O

As O
can O
be O
seen O
, O
both O
of O
the O
sampling O
based O
methods O
above O
perform O
worse O
than O
the O
regular O
beam O
search O
based O
pseudolabeling O
method O
( O
Regular O
) O
, O
let O
alone O
ours O
. O

) O
where O
σ B-HyperparameterName
is O
a O
logistic B-HyperparameterValue
sigmoid I-HyperparameterValue
function I-HyperparameterValue
, O
and O
W O
[ O
• O
] O
and O
U O
[ O
• O
] O
are O
learnable O
parameters O
. O

Comparatively O
, O
the O
higher O
- O
fidelity O
transfer O
" O
I O
absolutely O
enjoyed O
Spielberg O
's O
direction O
" O
, O
maintains O
such O
constraints O
of O
identity O
, O
in O
addition O
to O
being O
apt O
. O

The O
warmup B-HyperparameterName
step I-HyperparameterName
we O
use O
is O
4000 B-HyperparameterValue
. O

As O
shown O
in O
Figure O
2(c O
) O
, O
the O
guided O
- O
attention O
blocks O
are O
introduced O
to O
learn O
correlations O
between O
knowledge O
hyperedges O
and O
question O
hyperedges O
by O
interattention O
mechanism O
, O
and O
then O
intra O
- O
relationships O
of O
in O
knowledge O
or O
question O
hyperedges O
are O
trained O
with O
the O
following O
self O
- O
attention O
blocks O
. O

Based O
on O
the O
observations O
above O
, O
we O
propose O
a O
simple O
method O
called B-MethodName
PLATE I-MethodName
( O
as O
shorthand O
for O
Pseudo B-MethodName
- I-MethodName
labeling I-MethodName
with I-MethodName
Larger I-MethodName
Attention I-MethodName
TEmperature I-MethodName
) O
to O
smooth O
attention O
distributions O
of O
teacher O
models O
. O

They O
can O
also O
help O
in O
data O
augmentation O
for O
downstream O
domain O
adaptation O
NLP O
applications O
( O
§ O
5 O
) O
. O

x O
6 O

Being O
able O
to O
assess O
reproducibility O
of O
results O
objectively O
and O
comparably O
is O
important O
not O
only O
to O
establish O
that O
results O
are O
valid O
, O
but O
to O
provide O
evidence O
about O
which O
methods O
have O
better O
/ O
worse O
reproducibility O
and O
what O
may O
need O
to O
be O
changed O
to O
improve O
reproducibility O
. O

The O
first O
example O
in O
Fig O
: O
9 O
demonstrate O
this O
behaviour O
. O

Note O
that O
RoBERTa B-MethodName
- I-MethodName
PM I-MethodName
has O
an O
advantage O
in O
the O
i2b2 B-DatasetName
task O
since O
its O
pre O
- O
training O
data O
contains O
MIMIC B-DatasetName
- I-DatasetName
III I-DatasetName
clinical O
text O
data O
, O
while O
our O
teacher O
model O
was O
pretrained O
with O
only O
biomedical O
texts O
. O

This O
technique O
combines O
easily O
with O
existing O
approaches O
to O
data O
augmentation O
, O
and O
yields O
particularly O
strong O
results O
in O
low O
- O
resource O
settings O
. O

As O
to O
operator O
types O
( O
the O
right O
half O
) O
, O
we O
observe O
that O
: O
1 O
) O
TAGOP B-MethodName
- I-MethodName
L2I I-MethodName
achieves O
imagination O
on O
the O
majority O
of O
operator O
types O
with O
better O
performance O
than O
TAGOP B-MethodName
, O
yet O
TAGOP B-MethodName
can O
only O
achieve O
imagination O
on O
a O
few O
operator O
types O
. O

We O
argue O
that O
introducing O
the O
concept O
of O
hypergraph O
is O
powerful O
for O
multi O
- O
hop O
reasoning O
problem O
in O
that O
it O
can O
encode O
high O
- O
order O
semantics O
without O
the O
constraint O
of O
length O
and O
learn O
cross O
- O
modal O
high O
- O
order O
associations O
. O

We O
randomly O
blank O
out O
continuous O
spans O
of O
tokens O
from O
the O
input O
text O
, O
following O
the O
idea O
of O
autoencoding O
, O
and O
train O
the O
model O
to O
sequentially O
reconstruct O
the O
spans O
, O
following O
the O
idea O
of O
autoregressive O
pretraining O
( O
see O
Figure O
1 O
) O
. O

Constraints O
of O
identity O
are O
explored O
extensively O
in O
the O
computer O
vision O
task O
of O
cross O
- O
domain O
image O
generation O
. O

x O
6 O

On O
CNNDM B-DatasetName
and O
XSum B-DatasetName
datasets O
, O
we O
report O
full B-MetricName
- I-MetricName
length I-MetricName
F1 I-MetricName
based O
ROUGE-1 B-MetricName
( O
R1 B-MetricName
) O
, O
ROUGE-2 B-MetricName
( O
R2 B-MetricName
) O
, O
and O
ROUGE B-MetricName
- I-MetricName
L I-MetricName
( O
RL B-MetricName
) O
scores O
. O

x O
3 O

UniLM B-MethodName
combines O
different O
pretraining O
objectives O
under O
the O
autoencoding O
framework O
by O
changing O
the O
attention O
mask O
among O
bidirectional O
, O
unidirectional O
, O
and O
cross O
attention O
. O

Table O
9 O
shows O
the O
distillation O
performance O
of O
BART B-MethodName
12 I-MethodName
- I-MethodName
6 I-MethodName
student O
models O
with O
more O
values O
of O
λ B-HyperparameterName
we O
try O
on O
CNNDM B-DatasetName
dataset O
( O
we O
also O
include O
the O
values O
of O
1.0 B-HyperparameterValue
, O
1.5 B-HyperparameterValue
, O
and O
2.0 B-HyperparameterValue
in O
table O
for O
convenient O
comparison O
) O
. O

Answers O
, O
inferred O
by O
five O
comparative O
models O
and O
the O
proposed O
model O
, O
are O
presented O
with O
corresponding O
image O
and O
question O
. O

For O
instance O
, O
answering O
the O
question O
in O
Figure O
1 O
does O
not O
require O
the O
post O
- O
intervention O
value O
of O
total O
inventories O
in O
2019 O
. O

Specifically O
, O
all O
models O
are O
optimized O
using O
Adam O
( O
Kingma O
and O
with O
β B-HyperparameterName
1 I-HyperparameterName
= O
0.9 B-HyperparameterValue
, O
β B-HyperparameterName
2 I-HyperparameterName
= O
0.999 B-HyperparameterValue
. O

Extensive O
experiments O
on O
two O
knowledgebased B-TaskName
visual I-TaskName
QA I-TaskName
and O
two O
knowledge B-TaskName
- I-TaskName
based I-TaskName
textual I-TaskName
QA I-TaskName
demonstrate O
the O
effectiveness O
of O
our O
method O
, O
especially O
for O
multi O
- O
hop O
reasoning O
problem O
. O

Namely O
, O
the O
student O
is O
refined O
with O
L O
cls O
after O
the O
distillation O
steps O
. O

More O
results O
with O
different O
T B-HyperparameterName
s O
are O
in O
Appendix O
C.Why O
does O
our O
distillation O
method O
work O
? O
To O
answer O
this O
question O
, O
we O
first O
try O
to O
analyze O
the O
reasons O
from O
both O
the O
external O
characteristics O
of O
the O
summaries O
generated O
by O
the O
teacher O
model O
and O
the O
internal O
characteristics O
of O
the O
teacher O
's O
attention O
mechanism O
. O

Therefore O
, O
we O
recommend O
using O
this O
method O
when O
the O
computing O
budget O
is O
limited O
. O

However O
, O
there O
is O
a O
dilemma O
to O
adapt O
the O
siamese B-MethodName
network I-MethodName
to O
tokenlevel B-TaskName
recognition I-TaskName
tasks O
such O
as O
NER B-TaskName
. O

We O
argue O
CASPI B-MethodName
is O
competitive O
with O
other O
sample O
efficiency O
techniques O
, O
such O
as O
data B-MethodName
augmentation I-MethodName
and O
transfer B-MethodName
learning I-MethodName
as O
performed O
by O
Zhang O
et O
al O
. O
( O
2019 O
) O
and O
Lin O
et O
al O
. O
( O
2020 O
) O
respectively O
. O

For O
all O
comparative O
models O
, O
we O
use O
the O
same O
knowledge O
hypergraph O
extracted O
by O
the O
3 B-HyperparameterValue
- I-HyperparameterValue
hop I-HyperparameterValue
graph B-HyperparameterName
walk I-HyperparameterName
. O

We O
implement O
the O
sampling O
method O
in O
Edunov O
et O
al O
. O
( O
2018 O
) O
and O
Nucleus O
Sampling O
( O
Holtzman O
et O
al O
. O
, O
2019 O
) O
, O
a O
more O
advanced O
sampling O
method O
, O
to O
generate O
pseudo O
labels O
for O
distillation O
. O

Each O
attention O
block O
has O
multi O
- O
head O
attention O
with O
four B-HyperparameterValue
attention B-HyperparameterName
heads I-HyperparameterName
followed O
by O
layer O
normalization O
, O
residual O
connections O
and O
a O
single O
multi O
- O
layer O
perceptron O
. O

The O
training O
is O
fairly O
fast O
. O

The O
performance O
is O
slightly O
worse O
than O
the O
official O
BERT B-MethodName
Large I-MethodName
and O
significantly O
worse O
than O
GLM B-MethodName
Large I-MethodName
. O

Through O
multi O
- O
task O
learning O
of O
different O
pretraining O
objectives O
, O
a O
single O
GLM B-MethodName
can O
excel O
in O
both O
NLU B-TaskName
and O
( O
conditional O
and O
unconditional O
) O
text B-TaskName
generation I-TaskName
. O

The O
model O
architectures O
of O
Transformer B-MethodName
( I-MethodName
SA I-MethodName
) I-MethodName
and O
Transformer B-MethodName
( I-MethodName
SA+GA I-MethodName
) I-MethodName
presented O
in O
this O
paper O
are O
the O
same O
as O
Hypergraph B-MethodName
Transformer I-MethodName
. O

The O
effective O
use O
of O
these O
techniques O
are O
hindered O
by O
the O
nature O
of O
ToD. B-TaskName
For O
instance O
, O
bias O
correction O
in O
off O
- O
policy O
based O
methods O
usually O
requires O
estimation O
of O
behaviour O
policy O
for O
a O
given O
state O
of O
Markov O
Decision O
Process O
( O
MDP O
) O
. O

We O
also O
use O
label B-HyperparameterName
smoothing I-HyperparameterName
with O
rate O
0.1 B-HyperparameterValue
( O
Pereyra O
et O
al O
. O
, O
2017 O
) O
. O

However O
, O
UniLM B-MethodName
always O
replaces O
masked O
spans O
with O
[ O
MASK O
] O
tokens O
, O
which O
limits O
its O
ability O
to O
model O
the O
dependencies O
between O
the O
masked O
spans O
and O
their O
context O
. O

There O
is O
no O
standard O
way O
of O
going O
about O
a O
reproduction B-TaskName
study O
in O
NLP O
, O
and O
different O
reproduction B-TaskName
studies O
of O
the O
same O
original O
set O
of O
results O
can O
differ O
substantially O
in O
terms O
of O
their O
similarity O
in O
system O
and/or O
evaluation O
design O
( O
as O
is O
the O
case O
with O
the O
Vajjala O
and O
Rama O
( O
2018 O
) O
reproductions O
, O
see O
Section O
4 O
for O
details O
) O
. O

We O
are O
mainly O
concerned O
with O
how O
they O
can O
be O
adapted O
to O
downstream O
blank O
infilling O
tasks O
. O

One O
of O
its O
aims O
is O
to O
preserve O
the O
semantic O
content O
of O
text O
being O
translated O
from O
source O
to O
target O
domain O
. O

We O
first O
generate O
multiple O
ROT O
- O
k O
ciphertexts O
using O
different O
values O
of O
k O
for O
the O
plaintext O
which O
is O
the O
source O
side O
of O
the O
parallel O
data O
. O

This O
is O
very O
similar O
to O
combined O
score O
used O
in O
evaluation O
and O
both O
are O
equivalent O
when O
λ B-HyperparameterName
= O
2 B-HyperparameterValue
. O

x O
3 O
Target O
x O
5 O
x O
6 O
[ O
E O
] O
x O
3 O
[ O
E O
] O
Position O
1 O
1 O
2 O
3 O
4 O
5 O
5 O
5 O
5 O
3 O
3 O
Position O
2 O
0 O
0 O
0 O
0 O
0 O
1 O
2 O
3 O
1 O
2 O
x O
1 O
x O
2 O
x O
3 O
x O
4 O
x O
5 O
x O
6 O
x O
1 O
x O
2 O
[ O
M O
] O
x O
4 O
[ O
M O
] O
[ O
S O
] O
x O
5 O
x O
6 O
[ O
S O
] O
x O
3 O
x O
1 O
x O
2 O
[ O
M O
] O
x O
4 O
[ O
M O
] O
[ O
S O
] O
x O
5 O
x O
6 O
[ O
S O
] O
x O
3 O
x O
1 O
x O
2 O
[ O
M O
] O

[ O
S O
] O

The O
model O
performs O
worse O
than O
the O
standard B-MethodName
GLM I-MethodName
. O

For O
question O
hypergraph O
, O
each O
word O
unit O
is O
used O
as O
a O
start O
node O
of O
a O
graph O
walk O
. O

A O
hyperedge O
is O
flexible O
to O
encode O
different O
kinds O
of O
semantics O
in O
the O
underlying O
graph O
without O
the O
constraint O
of O
length O
. O

ROT O
- O
k O
is O
a O
simple O
letter O
substitution O
cipher O
that O
replaces O
a O
letter O
in O
the O
plaintext O
with O
the O
kth O
letter O
after O
it O
in O
the O
alphabet O
. O

These O
models O
have O
a O
similar O
motivation O
to O
the O
Hypergraph B-MethodName
Transformer I-MethodName
proposed O
in O
this O
paper O
, O
but O
core O
operations O
are O
vastly O
different O
. O

In O
causal O
inference O
, O
a O
rigorous O
derivation O
of O
an O
intervention O
considers O
the O
successors O
of O
the O
target O
variable O
, O
e.g. O
, O
finished O
goods O
in O
2019 O
affects O
total O
inventories O
in O
2019 O
. O

The O
implementation O
of O
GLM B-MethodName
is O
illustrated O
in O
Figure O
2 O
. O

More O
experiment O
details O
can O
be O
found O
in O
Appendix O
A.To O
evaluate O
our O
pretrained O
GLM B-MethodName
models O
, O
we O
conduct O
experiments O
on O
the O
SuperGLUE B-DatasetName
bench O
- O
mark O
and O
report O
the O
standard B-MetricName
metrics I-MetricName
. O

Training O
and O
inference O
Hyper O
- O
parameters O
for O
BART B-MethodName
, O
BART B-MethodName
12 I-MethodName
- I-MethodName
6 I-MethodName
, O
BART B-MethodName
12 I-MethodName
- I-MethodName
3 I-MethodName
, O
and O
BART B-MethodName
12 I-MethodName
- I-MethodName
12 I-MethodName
are O
similar O
. O

GLM O
: O
It O
had O
a O
brick O
entrance O
building O
next O
to O
the O
tracks O
, O
and O
one O
platform O
with O
2 O
side O
platforms O
. O

[ O
S O
] O

For O
the O
baseline O
classifiers O
, O
we O
follow O
the O
standard O
practice O
to O
concatenate O
the O
input O
parts O
of O
each O
task O
( O
such O
as O
the O
premise O
and O
hypothesis O
for O
textual B-TaskName
entailment I-TaskName
, O
or O
the O
passage O
, O
question B-TaskName
and I-TaskName
answer I-TaskName
for O
ReCORD O
and O
MultiRC O
) O
and O
add O
a O
classification B-HyperparameterName
layer I-HyperparameterName
on O
top O
of O
the O
[ O
CLS O
] O
token O
representation O
. O

The O
Stories B-DatasetName
dataset O
is O
no O
longer O
publicly O
available O
4 O
. O

GLM B-MethodName
uses O
a O
single O
Transformer B-HyperparameterName
with O
several O
modifications O
to O
the O
architecture O
: O
( O
1 O
) O
we O
rearrange O
the O
order O
of O
layer O
normalization O
and O
the O
residual O
connection O
, O
which O
has O
been O
shown O
critical O
for O
large O
- O
scale O
language O
models O
to O
avoid O
numerical O
errors O
( O
Shoeybi O
et O
al O
. O
, O
2019 O
) O
; O
( O
2 O
) O
we O
use O
a O
single O
linear B-HyperparameterName
layer I-HyperparameterName
for O
the O
output O
token O
prediction O
; O

Following O
these O
, O
there O
are O
two O
fundamental O
challenges O
in O
this O
task O
. O

p O
θ O
( O
s O
i O
|x O
corrupt O
, O
s O
z O
< O
i O
) O
= O
l O
i O
j=1 O
p(s O
i O
, O
j O
|x O
corrupt O
, O
s O
z O
< O
i O
, O
s O
i,<j O
) O
( O
2 O
) O

To O
train O
GLM B-MethodName
RoBERTa I-MethodName
, O
we O
follow O
the O
pretraining O
datasets O
of O
RoBERTa B-MethodName
, O
which O
consist O
of O
BookCorups B-DatasetName
( O
Zhu O
et O
al O
. O
, O
2015),Wikipedia B-DatasetName
( O
16 O
GB O
) O
, O
CC B-DatasetName
- I-DatasetName
News I-DatasetName
( O
the O
English B-DatasetName
portion I-DatasetName
of I-DatasetName
the I-DatasetName
Com I-DatasetName
- I-DatasetName
monCrawl I-DatasetName
News I-DatasetName
dataset O
3 O
76 O
GB O
) O
, O
OpenWebText B-DatasetName
( O
web O
content O
extracted O
from O
URLs O
shared O
on O
Reddit O
with O
at O
least O
three O
upvotes O
( O
Gokaslan O
and O
Cohen O
, O
2019 O
) O
, O
38 O
GB O
) O
and O
Stories B-DatasetName
( O
subset O
of O
Common B-DatasetName
- I-DatasetName
Crawl I-DatasetName
data O
filtered O
to O
match O
the O
story O
- O
like O
style O
of O
Winograd B-DatasetName
schemas I-DatasetName
( O
Trinh O
and O
Le O
, O
2019 O
) O
, O
31 O
GB O
) O
. O

The O
most O
recent O
is O
the O
creation O
of O
a O
six O
- O
seat O
district O
that O
includes O
all O
or O
part O
of O
the O
following O
: O
In O
the O
2009 O
elections O
, O
the O
state O
senate O
members O
were O
elected O
to O
six O
- O
year O
terms O
. O

The O
station O
closed O
on O
september O
15 O
, O
1927 O
, O
with O
the O
train O
service O
transferred O
from O
Grand O
Avenue O
to O
45th O
Avenue O
. O

To O
distil O
the O
knowledge O
from O
a O
teacher O
model O
, O
we O
first O
fine O
- O
tune O
the O
student O
model O
to O
provide O
initial O
knowledge O
about O
the O
task O
. O

The O
input O
x O
is O
divided O
into O
two O
parts O
: O
Part O
A O
is O
the O
corrupted O
text O
x O
corrupt O
, O
and O
Part O
B O
consists O
of O
the O
masked O
spans O
. O

The O
choice O
of O
action O
in O
reward O
function O
R(s O
t O
, O
a O
t O
, O
g O
) O
can O
either O
be O
dialogue B-TaskName
act I-TaskName
or O
generate B-TaskName
response I-TaskName
, O
we O
refer O
corresponding O
variants O
of O
metrics O
as O
M B-MetricName
( I-MetricName
act I-MetricName
) I-MetricName
and O
M B-MetricName
( I-MetricName
resp I-MetricName
) I-MetricName
. O

Since O
GLM B-MethodName
Base I-MethodName
is O
smaller O
, O
we O
reduce O
the O
number B-HyperparameterName
of I-HyperparameterName
training I-HyperparameterName
steps I-HyperparameterName
to O
120,000 B-HyperparameterValue
to O
speed O
up O
pre O
- O
training O
. O

Every O
hyperparameter O
was O
tuned O
on O
the O
development O
set O
. O

Finally O
, O
the O
hidden O
states O
of O
nodes O
in O
the O
given O
graph O
are O
updates O
as O
h O

For O
example O
, O
we O
set O
query O
, O
key O
, O
and O
value O
based O
on O
the O
knowledge O
hyperedges O
E O
k O
, O
and O
the O
self O
- O
attention O
for O
knowledge O
hyperedges O
is O
conducted O
by O

We O
randomly O
sample O
50 O
documents O
from O
the O
test O
set O
of O
CNNDM B-DatasetName
. O

[ O
M O
] O

To O
pursue O
such B-TaskName
reasoning I-TaskName
ability O
, O
we O
resort O
to O
the O
concept O
of O
counterfactual O
thinking O
( O
Pearl O
, O
2019 O
) O
from O
the O
theory O
of O
causality O
, O
which O
is O
the O
ability O
to O
imagine O
and O
reason O
over O
unseen O
cases O
based O
on O
the O
seen O
facts O
and O
counterfactual O
assumptions O
. O

We O
propose O
a O
novel O
data O
- O
augmentation O
technique O
for O
neural B-TaskName
machine I-TaskName
translation I-TaskName
based O
on O
ROT O
- O
k O
ciphertexts O
. O

We O
consider O
the O
following O
two O
objectives O
: O

2 O
) O
In O
particular O
, O
three B-HyperparameterValue
layers O
of O
matching O
block O
achieve O
the O
best O
performance O
on O
TAGOP B-MethodName
- I-MethodName
L2I. I-MethodName
The O
result O
indicates O
that O
three B-HyperparameterValue
layers O
should O
be O
sufficient O
to O
capture O
the O
semantic O
connection O
across O
the O
context O
, O
question O
and O
assumption O
. O

This O
result O
thus O
reflects O
the O
advantage O
of O
the O
unified O
operator O
framework O
adopted O
by O
the O
L2I B-MethodName
module O
, O
which O
is O
consistent O
with O
previous O
work O
( O
Andor O
et O
al O
. O
, O
2019 O
) O
. O

This O
is O
the O
advantage O
of O
GLM B-MethodName
over O
unidirectional B-MethodName
GPT I-MethodName
. O

x3 O

We O
detect O
visual O
concepts O
( O
e.g. O
, O
objects O
, O
attributes O
, O
person O
names O
) O
in O
a O
given O
image O
and O
named O
entities O
in O
a O
given O
question O
. O

The O
others O
were O
Joe O
Namath O
, O
Bill O
Nelsen O
, O
and O
Jerry O
Kramer O
. O

Our O
method O
can O
also O
be O
applied O
in O
selfdistillation O
and O
can O
potentially O
be O
combined O
with O
the O
self O
- O
distillation O
methods O
above O
. O

In O
particular O
, O
the O
calibrated O
teacher O
model O
was O
able O
to O
distil O
its O
activation O
boundary O
to O
the O
student O
model O
much O
more O
effectively O
, O
thus O
improving O
the O
performance O
of O
the O
student O
model O
, O
as O
we O
hypothesized O
in O
the O
previous O
section O
. O

Empirically O
, O
we O
have O
found O
that O
the O
15 B-HyperparameterValue
% I-HyperparameterValue
ratio O
is O
critical O
for O
good O
performance O
on O
downstream O
NLU B-TaskName
tasks O
. O

We O
show O
texts O
generated O
by O
GLM B-MethodName
Doc I-MethodName
given O
unseen O
contexts O
randomly O
sampled O
from O
the O
test O
set O
. O

All O
the O
three O
students O
have O
the O
12 B-HyperparameterValue
layers O
of O
BART B-MethodName
encoder O
and O
differ O
in O
the O
number O
of O
decoder O
layers O
. O

The O
beam B-HyperparameterName
size I-HyperparameterName
, O
length B-HyperparameterName
penalty I-HyperparameterName
, O
and O
minimal B-HyperparameterName
length I-HyperparameterName
are O
4 B-HyperparameterValue
, O
2.0 B-HyperparameterValue
, O
and O
55 B-HyperparameterValue
on O
CNNDM B-DatasetName
; O
6 B-HyperparameterValue
, O
0.1 B-HyperparameterValue
, O
and O
1 B-HyperparameterValue
on O
XSum B-DatasetName
; O
and O
4 B-HyperparameterValue
, O
0.7 B-HyperparameterValue
, O
and O
80 B-HyperparameterValue
on B-DatasetName
NYT I-DatasetName
, O
respectively O
. O

) O
. O

We O
hypothesize O
that O
it O
wastes O
some O
modeling O
capacity O
to O
learn O
the O
different O
sentinel O
tokens O
which O
are O
not O
used O
in O
downstream O
tasks O
with O
only O
one O
blank O
. O

[ O
S O
] O

UniLMv2 B-MethodName
( O
Bao O
et O
al O
. O
, O
2020 O
) O
adopts O
partially O
autoregressive O
modeling O
for O
generation O
tasks O
, O
along O
with O
the O
autoencoding O
objective O
for O
NLU B-TaskName
tasks O
. O

That O
is O
, O
the O
model O
should O
learn O
which O
knowledge O
facts O
to O
be O
attended O
to O
and O
how O
to O
combine O
them O
to O
infer O
the O
correct O
answer O
on O
its O
own O
. O

For O
comparison O
with O
previous O
work O
, O
we O
use O
the O
same O
test O
set O
constructed O
by O
( O
Shen O
et O
al O
. O
, O
2020 O
) O
. O

Large O
pre O
- O
trained O
Seq2Seq O
Transformer O
models O
largely O
improve O
results O
of O
generation O
tasks O
including O
text B-TaskName
summarization I-TaskName
( O
Song O
et O
al O
. O
, O
2019;Lewis O
et O
al O
. O
, O
2020;Raffel O
et O
al O
. O
, O
2020 O
; O
Token O
index O
in O
summary O
Zhang O
et O
al O
. O
, O
2020 O
) O
. O

Assume O
we O
have O
a O
teacher O
model O
trained O
with O
τ O
= O
√ O
d. O
When O
the O
teacher O
generates O
pseudo O
labels O
with O
beam O
search O
, O
we O
use O
a O
higher O
attention O
temperature O
and O
set O
τ O
= O
√ O
λ B-HyperparameterName
d O
where O
λ B-HyperparameterName
> O
1 B-HyperparameterValue
( O
λ B-HyperparameterName
is O
the O
attention B-HyperparameterName
temperature I-HyperparameterName
coefficient I-HyperparameterName
) O
. O

We O
propose O
a O
general O
pretraining O
framework O
GLM B-MethodName
based O
on O
a O
novel O
autoregressive O
blank O
infilling O
objective O
. O

We O
introduce O
an O
attention O
mechanism O
over O
two O
hypergraphs O
based O
on O
guided O
- O
attention O
( O
Tsai O
et O
al O
. O
, O
2019 O
) O
and O
self O
- O
attention O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O

Concise O
and O
abstractive O
teachers O
lead O
to O
concise O
and O
abstractive O
students O
( O
see O
Table O
6 O
) O
. O

Moreover O
, O
DoKTra B-MethodName
required O
less O
training O
time O
than O
TAPT B-MethodName
while O
both O
methods O
were O
task O
- O
specific O
. O

It O
takes O
about O
45 O
minutes O
for O
one O
epoch O
, O
and O
we O
need O
6 B-HyperparameterValue
epochs O
in O
total O
. O

Such O
significant O
performance O
gain O
validates O
the O
effectiveness O
of O
the O
L2I B-MethodName
module O
and O
reveal O
the O
rationality O
of O
modeling O
counterfactual O
thinking O
as O
a O
neural O
network O
module O
. O

On O
January O
13 O
, O
1966 O
, O
the O
Rams O
traded O
smith O
to O
the O
Detroit O
Lions O
for O
Paul O
Hornung O
, O
and O
later O
that O
year O
he O
was O
traded O
to O
the O
Lions O
for O
Ray O
" O
the O
Lion O
" O
Jones O
in O
exchange O
for O
Linebacker O
Jim O
" O
the O
Hawk O
" O
Johnson O
. O

The O
better O
performance O
of O
TAGOP B-MethodName
- I-MethodName
L2I I-MethodName
is O
attributed O
to O
modeling O
the O
deriving O
operations O
as O
specific O
operators O
. O

We O
minimize O
negative O
log O
- O
likelihood O
using O
Adam B-HyperparameterValue
optimizer B-HyperparameterName
( O
Kingma O
and O
Ba O
, O
2015 O
) O
with O
an O
initial B-HyperparameterName
learning I-HyperparameterName
rate I-HyperparameterName
from O
1e B-HyperparameterValue
− I-HyperparameterValue
4 I-HyperparameterValue
to O
1e B-HyperparameterValue
− I-HyperparameterValue
5 I-HyperparameterValue
with O
batch B-HyperparameterName
size I-HyperparameterName
from O
128 B-HyperparameterValue
to O
256 B-HyperparameterValue
. O

To O
predict O
an O
answer O
, O
we O
first O
concatenate O
the O
representation O
z O
k O
and O
z O
q O
obtained O
from O
the O
attention O
blocks O
and O
feed O
into O
a O
single B-HyperparameterValue
feed B-HyperparameterName
- I-HyperparameterName
forward I-HyperparameterName
layer I-HyperparameterName
( O
i.e. O
, O
R O
2dv O
→ O
R O
w O
) O
to O
make O
a O
joint O
representation O
z. O
We O
then O
consider O
two O
types O
of O
answer O
predictor O
: O
multi O
- O
layer O
perceptron O
and O
similarity O
- O
based O
answer O
predictor O
. O

• O
We O
highlight O
the O
importance O
of O
counterfactual O
thinking O
in O
NDR B-TaskName
and O
formulate O
counterfactual O
thinking O
as O
an O
intervening O
procedure O
to O
achieve O
precise O
imagination O
. O

Appendix O
B O
) O
. O

Row O
6 O
shows O
that O
removing O
the O
span O
shuffling O
( O
always O
predicting O
the O
masked O
spans O
from O
left O
to O
right O
) O
leads O
to O
a O
severe O
performance O
drop O
on O
SuperGLUE B-DatasetName
. O

SuperGLUE B-DatasetName
. O

It O
leverages O
the O
pretrained O
T5 B-MethodName
and O
BART B-MethodName
( O
Lewis O
et O
al O
. O
, O
2019 O
) O
as O
backbone O
for O
model O
architecture O
. O

Here O
, O
we O
consider O
the O
two O
types O
of O
input B-HyperparameterName
format I-HyperparameterName
, O
which O
are O
single B-HyperparameterValue
- I-HyperparameterValue
wordunit I-HyperparameterValue
and O
hyperedge B-HyperparameterValue
- I-HyperparameterValue
based I-HyperparameterValue
representations B-HyperparameterName
. O

Then O
, O
they O
are O
asked O
to O
rank O
the O
summaries O
from O
best O
to O
worst O
as O
a O
way O
of O
determining O
the O
overall O
quality O
of O
summaries O
. O

For O
the O
large O
model O
, O
clozestyle O
finetuning O
can O
improve O
the O
performance O
by O
7 B-MetricValue
points I-MetricValue
. O

For O
example O
, O
for O
Rougier O
et O
al O
. O
( O
2017 O
) O
, O
reproducing O
a O
result O
means O
running O
the O
same O
code O
on O
the O
same O
data O
and O
obtaining O
the O
same O
result O
, O
while O
replicating O
the O
result O
is O
writing O
and O
running O
new O
code O
based O
on O
the O
information O
provided O
by O
the O
original O
publication O
. O

As O
we O
proposed O
, O
applying O
both O
calibrated O
teacher O
training O
and O
activation O
boundary O
distillation O
resulted O
in O
a O
superior O
performance O
. O

And O
β B-HyperparameterName
is O
set O
such O
that O
it O
is O
high O
when O
the O
output O
of O
the O
entity O
similarity O
teacher O
is O
close O
to O
0 O
or O
1 O
, O
and O
it O
is O
low O
when O
the O
output O
is O
close O
to O
0.5 O
. O

With O
λ B-HyperparameterName
= O
1.5 B-HyperparameterValue
, O
we O
obtain O
a O
BLEU B-MetricName
of O
27.90 B-MetricValue
, O
while O
the O
result O
of O
the O
regular O
pseudo O
- O
labeling O
is O
27.79 B-MetricValue
( O
more O
details O
are O
in O
Appendix O
A O
) O
. O

As O
pointed O
out O
by O
( O
Yang O
et O
al O
. O
, O
2019 O
) O
, O
BERT B-MethodName
fails O
to O
capture O
the O
interdependencies O
of O
masked O
tokens O
due O
to O
the O
independence O
assumption O
of O
MLM O
. O

This O
causes O
a O
performance O
drop O
across O
all O
languages O
due O
to O
two O
single O
teachers O
can O
not O
make O
a O
difference O
with O
the O
combination O
. O

BAN B-MethodName
calculates O
soft O
attention O
scores O
between O
knowledge O
entities O
and O
question O
words O
as O
follows O
: O

Donahue O
et O
al O
. O
( O
2020 O
) O
and O
Shen O
et O
al O
. O
( O
2020 O
) O
also O
study O
blanking O
infilling O
models O
. O

Therefore O
, O
we O
resort O
to O
an O
alternative O
approach O
: O
constructing O
the O
counterfactual O
( O
Pearl O
, O
2009 O
) O
where O
the O
target O
variable O
is O
intervened O
according O
to O
the O
hypothetical O
condition O
to O
infer O
a O
counterfactual O
. O

Note O
that O
if O
a O
word O
is O
divided O
into O
several O
subwords O
after O
tokenization O
, O
then O
only O
the O
first O
subword O
is O
considered O
in O
the O
loss O
function O
. O

Particularly O
, O
the O
mean B-MetricName
accuracy I-MetricName
of O
QA B-TaskName
is O
decreased O
by O
6.0 B-MetricValue
% I-MetricValue
( O
89.7 B-MetricValue
% I-MetricValue
→ O
83.7 B-MetricValue
% I-MetricValue
) O
, O
2.6 B-MetricValue
% I-MetricValue
( O
89.7 B-MetricValue
% I-MetricValue
→ O
87.1 B-MetricValue
% I-MetricValue
) O
for O
cutting O
out O
the O
GA O
and O
the O
SA O
block O
, O
respectively O
. O

The O
Financial B-DatasetName
PhraseBank I-DatasetName
dataset O
contains O
4,846 O
sentences O
, O
and O
we O
set O
10 O
% O
of O
the O
examples O
as O
the O
test O
set O
while O
preserving O
the O
label O
distribution O
. O

Our O
experimental O
results O
show O
that O
the O
proposed O
model O
yields O
significant O
improvements O
on O
six O
target O
language O
datasets O
and O
outperforms O
the O
existing O
state O
- O
of O
- O
the O
- O
art O
approaches O
. O

x O
1 O
x O
2 O
x O
3 O
x O
4 O
x O
5 O
x O
6 O
x O
1 O
x O
2 O
[ O
M O
] O
x O
4 O
[ O
M O
] O
[ O
S O
] O
x O
5 O
x O
6 O
[ O
S O
] O
x O
3 O
x O
5 O
x O
6 O
[ O
E O
] O
x O
3 O
[ O
E O
] O
x O
1 O
x O
2 O
[ O
M O
] O
x O
4 O
[ O
M O
] O
[ O
S O
] O
x O
5 O
x O
6 O
[ O
S O
] O
x O
3 O
x O
1 O
x O
2 O
[ O
M O
] O

* O
The O
first O
two O
authors O
contributed O
equally O
. O

Then O
, O
the O
update O
gate O
and O
reset O
gate O
are O
computed O
as O
follows O
: O

We O
group O
the O
questions O
according O
to O
1 O
) O
the O
answer O
type O
and O
2 O
) O
the O
operator O
to O
derive O
the O
intervention O
. O

To O
imitate O
the O
zero B-TaskName
- I-TaskName
resource I-TaskName
cross I-TaskName
lingual I-TaskName
NER I-TaskName
case O
, O
following O
( O
Wu O
and O
Dredze O
, O
2019 O
) O
, O
we O
used O
English O
as O
the O
source O
language O
and O
other O
languages O
as O
the O
target O
language O
. O

He O
also O
has O
voiced O
characters O
in O
" O
the O
legend O
of O
korra O
" O
He O
has O
appeared O
on O
several O
television O
series O
, O
including O
" O
the O
simpsons O
" O
, O
" O
the O
x O
- O
files O
" O
, O
" O
heroes O
" O
and O
" O
the O
simpsons O
movie O
" O
as O
the O
character O
captain O
billy O
higgledy O
- O
pig O
, O
and O
the O
web O
series O
" O
krusty O
mysteries O
" O
as O
the O
character O
Colonel O
Trungus O
. O

Following O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
, O
the O
hypothetical O
questions O
are O
also O
labeled O
with O
four O
answer O
types O
: O
arithmetic O
, O
span O
, O
count O
, O
and O
multi O
- O
span O
, O
three O
types O
of O
answer O
sources O
: O
table O
, O
text O
and O
table O
- O
text O
, O
and O
a O
derivation O
on O
how O
the O
answer O
is O
derived O
from O
the O
context O
. O

Besides O
, O
the O
attention B-HyperparameterName
temperature I-HyperparameterName
of O
the O
decoder O
self O
- O
attention O
is O
also O
crucial O
but O
not O
as O
important O
as O
the O
cross O
- O
attention O
( O
see O
the O
fourth O
row O
) O
. O

Also O
, O
M O
denotes O
a O
mixture O
of O
the O
2H O
and O
3H O
questions O
. O

T5 B-MethodName
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
unifies O
NLU B-TaskName
and O
conditional B-TaskName
generation I-TaskName
via O
encoder O
- O
decoder O
models O
but O
requires O
more O
parameters O
to O
match O
the O
performance O
of O
BRET B-MethodName
- O
based O
models O
such O
as O
RoBERTa B-MethodName
and O
DeBERTa B-MethodName
( O
He O
et O
al O
. O
, O
2021 O
) O
. O

Previously O
, O
pretrained O
language O
models O
complete O
classification B-TaskName
tasks O
for O
NLU B-TaskName
with O
linear O
classifiers O
on O
the O
learned O
representations O
. O

Self O
- O
attention O
The O
only O
difference O
between O
guided O
- O
attention O
and O
self O
- O
attention O
is O
that O
the O
same O
input O
is O
used O
for O
both O
query O
and O
key O
- O
value O
within O
self O
- O
attention O
. O

For O
a O
fair O
comparison O
with O
GLM B-MethodName
Base I-MethodName
and O
GLM B-MethodName
Large I-MethodName
, O
we O
choose O
BERT B-MethodName
Base I-MethodName
and O
BERT B-MethodName
Large I-MethodName
as O
our O
baselines O
, O
which O
are O
pretrained O
on O
the O
same O
corpus O
and O
for O
a O
similar O
amount O
of O
time O
. O

Each O
document O
is O
ensured O
to O
be O
annotated O
by O
3 O
different O
subjects O
. O

To O
demonstrate O
the O
hypothesis O
, O
we O
test O
our O
method O
against O
baseline O
in O
a O
low O
sample O
complexity O
regime O
. O

3 O
) O
TAGOP B-MethodName
- I-MethodName
L2I I-MethodName
achieves O
the O
worst O
performance O
on O
SWAP O
MIN O
NUM O
, O
which O
is O
merely O
comparable O
to O
TAGOP B-MethodName
. O

The O
statistics O
of O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
are O
shown O
in O
Table O
1 O
. O

To O
evaluate O
the O
counterfactual O
thinking O
ability O
, O
we O
recruit O
volunteers O
with O
domain O
expertise O
to O
construct O
an O
HQA B-TaskName
dataset O
based O
on O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
( O
Zhu O
et O
al O
. O
, O
2021 O
) O
by O
posting O
an O
assumption O
for O
each O
original O
question O
, O
named O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
. O

This O
work O
studies O
a O
new O
and O
more O
challenging O
task O
that O
focuses O
on O
hypothetical O
question O
. O

The O
first O
three O
student O
models O
are O
initialized O
from O
BART B-MethodName
weights O
( O
therefore O
, O
their O
hidden O
sizes O
are O
the O
same O
as O
that O
of O
BART B-MethodName
) O
. O

We O
also O
add O
a O
few O
refinement O
steps O
to O
refine O
the O
classification O
layer O
of O
the O
student O
model O
. O

• O
We O
construct O
a O
challenging O
HQA B-TaskName
dataset O
and O
conduct O
extensive O
experiments O
on O
the O
dataset O
, O
where O
the O
performance O
validates O
the O
rationality O
and O
effectiveness O
of O
the O
proposed B-MethodName
L2I.In I-MethodName
the O
general O
setting O
of O
machine O
reading O
comprehension O
, O
the O
task O
is O
to O
answer O
a O
question O
according O
to O
the O
facts O
in O
a O
given O
context O
. O

The O
practices O
are O
different O
from O
the O
generative O
pretraining O
task O
, O
leading O
to O
inconsistency O
between O
pretraining O
and O
finetuning O
. O

NLU B-TaskName
as O
Generation O
. O

Without O
generative O
objective O
during O
pretraining O
, O
GLM B-MethodName
Large I-MethodName
can O
not O
complete O
the O
language B-TaskName
modeling I-TaskName
tasks O
, O
with O
perplexity B-MetricName
larger O
than O
100 B-MetricValue
. O

) O
. O
After O
the O
propagation O
phase O
, O
the O
nodes O
in O
the O
graph O
are O
aggregated O
to O
a O
graph O
- O
level O
representation O
as O
h O
G O
= O
tanh B-HyperparameterValue
( O
v∈V O
σ(i(h O

CASPI(MinTL B-MethodName
) I-MethodName
trained O
only O
on O
20 O
% O
of O
data O
was O
able O
to O
out O
perform O
previous O
state O
of O
the O
art O
method O
, O
LAVA B-MethodName
( O
Lubis O
et O
al O
. O
, O
2020 O
) O
and O
MINTL B-MethodName
( O
Lin O
et O
al O
. O
, O
2020 O
) O
trained O
on O
100 O
% O
data O
on O
two O
of O
the O
three O
performance O
metrics O
. O

Though O
the O
turns O
have O
varying O
levels O
of O
importance O
, O
each O
of O
the O
turns O
are O
treated O
equally O
in O
imitation O
learning O
. O

The O
experimental O
results O
are O
shown O
in O
Table O
8 O
. O

In O
particular O
, O
the O
two O
- O
stage O
approach O
of O
" O
pre O
- O
training O
and O
fine O
- O
tuning O
, O
" O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
has O
become O
the O
standard O
for O
NLP O
applications O
. O

Turns#3 O
and O
# O
2 O
are O
rich O
in O
semantic O
information O
and O
Turn#3 O
is O
key O
to O
success O
of O
the O
booking O
process O
. O

For O
the O
3 O
multi O
- O
token O
tasks O
, O
we O
use O
the O
sum O
of O
the O
log O
- O
probabilities O
of O
the O
verbalizer O
tokens O
. O

Shared O
feature O
space O
based O
models O
generally O
train O
a O
language O
- O
independent O
encoder O
using O
source O
and O
target O
language O
data O
( O
Tsai O
et O
al O
. O
, O
2016 O
) O
. O

We O
also O
compared O
our O
framework O
with O
taskadaptive B-MethodName
pre I-MethodName
- I-MethodName
training I-MethodName
( O
TAPT B-MethodName
) O
( O
Gururangan O
et O
al O
. O
, O
2020 O
) O
, O
an O
additional O
pre O
- O
training O
method O
for O
PLMs O
. O

The O
difference O
between O
BAN B-MethodName
and O
HAN B-MethodName
is O
the O
abstraction B-HyperparameterName
level I-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
input I-HyperparameterName
. O

( O
2 O
) O
MTMT B-MethodName
w/o O
weighting O
, O
which O
set O
the O
α B-HyperparameterName
( I-HyperparameterName
• I-HyperparameterName
) I-HyperparameterName
, O
β B-HyperparameterName
and O
γ B-HyperparameterName
all O
to O
be O
1 B-HyperparameterValue
in O
the O
loss O
of O
student O
learning O
. O

These O
models O
mainly O
update O
node O
representations O
in O
the O
hypergraph O
through O
a O
message O
passing O
process O
using O
graph O
convolution O
operation O
. O

To O
transform O
it O
into O
a O
classification O
task O
, O
we O
clustered O
the O
sentiment O
score O
into O
a O
3 O
- O
class O
label O
, O
following O
Daudert O
et O
al O
. O
( O
2018 O
) O
. O

Previous O
works O
have O
tried O
to O
unify O
different O
frameworks O
by O
combining O
their O
objectives O
via O
multi O
- O
task O
learning O
( O
Dong O
et O
al O
. O
, O
2019;Bao O
et O
al O
. O
, O
2020 O
) O
. O

This O
is O
expected O
since O
GLM B-MethodName
Doc I-MethodName
also O
optimizes O
the O
blank O
infilling O
objective O
. O

To O
analyze O
the O
performances O
of O
the O
variants O
in O
our O
model O
, O
we O
use O
KVQA B-DatasetName
which O
is O
a O
representative O
and O
large O
- O
scale O
dataset O
for O
knowledge B-TaskName
- I-TaskName
based I-TaskName
VQA I-TaskName
. O

We O
can O
thus O
derive O
the O
value O
of O
successors O
( O
e.g. O
, O
c O
i O
→ O
c O
j O
) O
by O
forming O
a O
simple O
hypothetical O
question O
: O
" O
What O
c O
j O
would O
be O
if O
c O
i O
is O
c O
i O
? O
" O
and O
answering O
it O
with O
the O
NDR B-TaskName
model O
. O

We O
further O
compare O
GLM B-MethodName
with O
BERT B-MethodName
on O
the O
two O
benchmarks O
. O

Learning B-HyperparameterName
rates I-HyperparameterName
are O
tuned O
on O
validation O
sets O
( O
choose O
from O
1e-5 B-HyperparameterValue
, O
3e-5 B-HyperparameterValue
, O
5e-5 B-HyperparameterValue
, O
7e-5 B-HyperparameterValue
) O
. O

Note O
that O
if O
an O
n O
- O
gram O
appears O
in O
the O
summary O
, O
but O
not O
in O
the O
original O
document O
, O
we O
call O
it O
a O
novel B-MetricName
n I-MetricName
- I-MetricName
gram I-MetricName
. O

On O
the O
two O
benchmarks O
, O
GLM B-MethodName
can O
still O
outperform O
BERT B-MethodName
with O
the O
same O
amount O
of O
parameters O
, O
but O
with O
a O
smaller O
margin O
. O

Table O
1 O
and O
2 O
shows O
the O
statistics O
of O
all O
datasets O
. O

This O
happens O
because O
there O
are O
gaps O
in O
automatic O
evaluation O
metrics O
. O

iii O
) O
We O
qualitatively O
observe O
that O
Hypergraph B-MethodName
Transformer I-MethodName
performs O
robust O
in O
- O
ference O
by O
focusing O
on O
correct O
reasoning O
evidences O
under O
weak O
supervision O
. O

The O
experimental O
results O
on O
diverse O
split B-HyperparameterName
of O
PQ B-DatasetName
and O
PQL B-DatasetName
datasets O
are O
provided O
in O
Table O
2 O
. O

Meanwhile O
, O
our O
method O
is O
conceptually O
simpler O
and O
can O
further O
be O
combined O
with O
their O
methods O
with O
additional O
train- O
ing O
objectives O
. O

As O
to O
TAGOP B-MethodName
- I-MethodName
L2I I-MethodName
, O
the O
gap O
between O
arithmetic O
question O
and O
other O
types O
of O
question O
largely O
reduces O
, O
validating O
the O
effectiveness O
of O
learning O
intervention O
with O
discrete O
operators O
and O
neural O
network O
modules O
. O

( O
Narasimhan O
et O
al O
. O
, O
2018 O
; O
represented O
the O
retrieved O
facts O
as O
a O
graph O
and O
performed O
graph O
reasoning O
through O
message O
passing O
scheme O
utilizing O
graph O
convolution O
. O

All O
experiments O
were O
repeated O
three B-HyperparameterValue
times O
with O
different O
random O
seeds O
, O
and O
the O
average O
performances O
and O
standard O
deviations O
have O
been O
reported O
. O

To O
infer O
the O
probability O
of O
an O
answer O
of O
length O
l O
, O
BERT B-MethodName
needs O
to O
perform O
l O
consecutive O
predictions O
. O

where O
all O
projection O
matrices O
W O
[ O
• O
] O
∈ O
R O
d×dv O
are O
learnable O
parameters O
. O

RIKD B-MethodName
( O
Liang O
et O
al O
. O
, O
2021 O
) O
develops O
a O
reinforced O
iterative O
knowledge O
distillation O
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
. O

The O
objective O
aims O
for O
long O
text O
generation O
. O

The O
performance O
is O
still O
weak O
due O
to O
the O
lack O
of O
annotations O
of O
target O
languages O
. O

x O
4 O

An O
attention O
layers O
is O
used O
to O
attend O
the O
outputs O
of O
the O
seq2seq O
models O
with O
the O
context O
vector O
of O
previous O
turn O
for O
copy O
over O
mechanism O
. O

Our O
method O
, O
CipherDAug B-MethodName
, O
uses O
a O
co O
- O
regularization O
- O
inspired O
training O
procedure O
, O
requires O
no O
external O
data O
sources O
other O
than O
the O
original O
training O
data O
, O
and O
uses O
a O
standard O
Transformer B-HyperparameterName
to O
outperform O
strong O
data O
augmentation O
techniques O
on O
several O
datasets O
by O
a O
significant O
margin O
. O

We O
restrict O
that O
the O
masked O
spans O
must O
be O
full O
sentences O
. O

HAN B-MethodName
and O
BAN B-MethodName
utilize O
multi O
- O
head O
co O
- O
attention O
between O
question O
and O
knowledge O
. O

He O
was O
one O
of O
four O
Michigan O
players O
honored O
as O
first O
- O
overall O
selections O
in O
the O
1964 O
NFL O
draft O
. O

We O
validate O
the O
model O
performance O
on O
the O
three O
commonly O
- O
used O
datasets O
across O
7 O
languages O
and O
the O
experimental O
results O
show O
the O
superiority O
of O
our O
presented O
MTMT B-MethodName
model O
. O

Figure O
1 O
intuitively O
shows O
the O
effect O
of O
using O
higher O
attention O
temperatures O
. O

We O
compare O
with O
two O
baselines O
: O
( O
1 O
) O
BERT B-MethodName
, O
which O
learns O
a O
left O
- O
to O
- O
right O
language O
model O
to O
generate O
the O
masked O
tokens O
on O
top O
of O
the O
blank O
representation O
, O
and O
( O
2 O
) O
BLM B-MethodName
proposed O
by O
( O
Shen O
et O
al O
. O
, O
2020 O
) O
, O
which O
can O
fill O
in O
the O
blank O
with O
arbitrary O
trajectories O
. O

x O
5 O

We O
then O
study O
a O
multi O
- O
task O
pretraining O
setup O
, O
in O
which O
a O
second O
objective O
of O
generating O
longer O
text O
is O
jointly O
optimized O
with O
the O
blank O
infilling O
objective O
. O

To O
the O
best O
of O
our O
knowledge O
, O
we O
are O
the O
first O
to O
learn O
the O
entity O
similarity O
by O
siamese B-MethodName
network I-MethodName
. O

Figure O
3 O
provides O
the O
qualitative O
analysis O
on O
effectiveness O
of O
using O
a O
hypergraph O
as O
an O
input O
format O
to O
Transformer O
architecture O
. O

We O
observe O
that O
using O
pseudo O
- O
labeling O
methods O
with O
higher O
attention B-HyperparameterName
temperatures I-HyperparameterName
consistently O
improves O
over O
its O
counterpart O
with O
normal O
attention B-HyperparameterName
temperatures I-HyperparameterName
( O
Regular O
) O
across O
all O
three O
datasets O
, O
and O
the O
differences O
between O
them O
are O
almost O
always O
significant O
measured O
with O
the O
ROUGE B-MetricName
script O
5 O
( O
see O
details O
in O
Table O
2 O
) O
. O

• O
Document O
- O
level O
. O

As O
proposed O
in O
Zhang O
et O
al O
. O
( O
2019 O
) O
, O
We O
generate O
delexicalized O
responses O
with O
placeholders O
for O
specific O
values O
which O
can O
be O
filled O
with O
information O
in O
DST O
and O
database O
. O

Compared O
to O
hyperedge B-HyperparameterValue
- I-HyperparameterValue
based I-HyperparameterValue
inputs O
considering O
multiple O
relational O
facts O
as O
a O
input O
token O
, O
single B-HyperparameterValue
- I-HyperparameterValue
wordunit I-HyperparameterValue
takes O
every O
entity O
and O
relation O
tokens O
as O
separate O
input O
tokens O
. O

We O
observe O
that O
the O
number B-HyperparameterName
of I-HyperparameterName
extracted I-HyperparameterName
knowledge I-HyperparameterName
facts O
increases O
when O
the O
number B-HyperparameterName
of I-HyperparameterName
graph I-HyperparameterName
walk I-HyperparameterName
increases O
, O
and O
unnecessary O
facts O
for O
answering O
a O
given O
question O
are O
usually O
included O
. O

i O
= O
{ O
v O
′ O
1 O
⪯ O
... O
⪯ O
v O
′ O
l O
} O
where O
V O
′ O
= O
{ O
v O
′ O
1 O
, O
... O
, O
v O
′ O
l O
} O
is O
a O
subset O
of O
V O
and O
⪯ O
is O
a O
binary O
relation O
which O
denotes O
an O
element O
( O
v O
′ O
i O
) O
precedes O
the O
other O
( O
v O
′ O
j O
) O
in O
the O
ordering O
when O
v O
′ O
i O
⪯ O
v O
′ O
j O
. O

The O
weights O
are O
set O
as O
follows O
: O
α B-HyperparameterName
1 I-HyperparameterName
( O
α B-HyperparameterName
2 I-HyperparameterName
) O
is O
an O
increasing O
function O
concerning O
the O
output O
of O
the O
entity O
recognizer O
teacher O
as O
shown O
in O
Figure O
4 O
. O

GLM B-MethodName
feeds O
in O
the O
previous O
token O
and O
autoregressively O
generates O
the O
next O
token O
. O

Different O
from O
their O
work O
, O
we O
pre O
- O
train O
language O
models O
with O
blank O
infilling O
objectives O
and O
evaluate O
their O
performance O
in O
downstream O
NLU B-TaskName
and O
generation B-TaskName
tasks O
. O

1.We O
introduce O
pairwise O
causal O
reward O
learning O
to O
learn O
fine O
grained O
per O
turn O
reward O
that O
reason O
the O
intention O
of O
human O
utterance O
. O

That O
may O
be O
the O
reason O
why O
multiple O
continuous O
spans O
of O
text O
are O
copied O
. O

The O
only O
difference O
between O
these O
two O
methods O
is O
that O
TAGOP B-MethodName
- I-MethodName
CLO I-MethodName
incorporates O
an O
extra O
CLO O
. O

Applying O
our O
framework O
to O
ALBERT B-MethodName
allowed O
us O
to O
obtain O
a O
student O
model O
with O
performance O
comparable O
to O
that O
of O
the O
teacher O
with O
half O
the O
parameters O
. O

After O
propagation O
and O
aggregation O
phase O
, O
the O
knowledge O
and O
question O
graph O
representations O
are O
obtained O
. O

The O
Rams O
waived O
Smith O
during O
the O
September O
1 O
, O
1972 O
offseason O
. O

GLM B-MethodName
is O
trained O
by O
optimizing O
an O
autoregressive O
blank O
infilling O
objective O
. O

This O
is O
reasonable O
since O
the O
average O
length O
of O
both O
assumption O
and O
question O
are O
only O
around O
10 O
words O
( O
cf O
. O

where O
a O
massive O
number O
of O
knowledge O
facts O
from O
a O
general O
knowledge O
base O
( O
KB O
) O
is O
given O
with O
an O
image O
- O
question O
pair O
. O

We O
conduct O
a O
pilot O
study O
on O
the O
generalization O
ability O
of O
existing O
NDR B-TaskName
models O
on O
hypothetical O
questions O
. O

We O
follow O
the O
split O
of O
training O
, O
testing O
and O
validation O
set O
of O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
as O
shown O
in O
Table O
2 O
. O

We O
test O
QRA B-TaskName
on O
18 O
system O
and O
evaluation O
measure O
combinations O
( O
involving O
diverse O
NLP O
tasks O
and O
types O
of O
evaluation O
) O
, O
for O
each O
of O
which O
we O
have O
the O
original O
results O
and O
one O
to O
seven O
reproduction O
results O
. O

To O
this O
end O
, O
we O
propose O
a O
two O
- O
step O
formulation O
of O
counterfactual O
thinking O
for O
HQA B-TaskName
to O
perform O
the O
identification O
and O
derivation O
. O

Parameter O
settings O
. O
We O
implement O
TAGOP B-MethodName
- I-MethodName
L2I I-MethodName
based O
on O
TAGOP B-MethodName
4 O
. O

Models O
with O
blank O
- O
infilling O
objectives O
, O
such O
as O
T5 B-MethodName
and O
our O
GLM B-MethodName
, O
benefits O
more O
from O
converting O
the O
NLU B-TaskName
tasks O
into O
cloze B-TaskName
questions I-TaskName
. O

In O
addition O
, O
the O
guided O
- O
attention O
which O
uses O
the O
question O
hyperedges O
as O
query O
and O
the O
knowledge O
hyperedges O
as O
key O
- O
value O
pairs O
is O
performed O
in O
a O
similar O
manner O
: O

We O
observe O
that O
success B-MetricName
rate I-MetricName
, O
if O
used O
as O
is O
, O
will O
result O
in O
non O
- O
markovian O
and O
stochastic O
per O
turn O
reward O
function O
. O

In O
addition O
, O
the O
governor O
can O
appoint O
members O
of O
the O
Wyoming O
house O
of O
representatives O
. O

Note O
their O
settings O
of O
student O
models O
are O
BART B-MethodName
12 I-MethodName
- I-MethodName
6 I-MethodName
on O
CNNDM B-DatasetName
and O
BART B-MethodName
12 I-MethodName
- I-MethodName
3 I-MethodName
on O
XSum B-DatasetName
. O

Besides O
, O
we O
want O
to O
select O
baselines O
that O
are O
effective O
for O
learning O
counterfactual O
samples O
. O

Pretraining O
largescale O
language O
models O
significantly O
improves O
the O
performance O
of O
downstream O
tasks O
. O

This O
may O
be O
because O
of O
the O
batch B-HyperparameterName
size I-HyperparameterName
being O
smaller O
than O
that O
in O
the O
TPU O
environment O
. O

This O
is O
because O
both O
datasets O
have O
textual O
and O
tabular O
texts O
, O
where O
the O
ability O
of O
TAGOP B-MethodName
to O
perform O
discrete O
reasoning O
across O
hybrid O
contexts O
brings O
significant O
advantages O
. O

To O
calculate O
the O
intervention O
result O
, O
we O
select O
a O
set O
of O
commonly O
used O
discrete O
operators O
such O
as O
SWAP O
, O
ADD O
, O
and O
MINUS O
( O
cf O
. O

Length B-MetricName
and O
novel B-MetricName
n I-MetricName
- I-MetricName
grams I-MetricName
We O
first O
analyze O
the O
pseudo O
summaries O
generated O
by O
the O
teacher O
models O
. O

As O
shown O
in O
Table O
6 O
, O
when O
using O
a O
larger O
λ B-HyperparameterName
, O
pseudo O
summaries O
are O
shorter O
6 O
and O
contain O
a O
larger O
portion O
of O
novel B-MetricName
n I-MetricName
- I-MetricName
grams I-MetricName
. O

x O
6 O

In O
this O
paper O
, O
we O
focus O
on O
the O
task O
which O
is O
called O
knowledge B-TaskName
- I-TaskName
based I-TaskName
visual I-TaskName
question I-TaskName
answering I-TaskName
, O
To O
answer O
the O
given O
question O
, O
the O
multiple O
reasoning O
evidences O
( O
marked O
as O
orange O
) O
are O
required O
. O

Then O
the O
student O
model O
is O
trained O
with O
L O
AT O
. O

Motivated O
by O
Gated O
Recurrent O
Units O
( O
Cho O
et O
al O
. O
, O
2014 O
) O
, O
GGNN B-MethodName
adopts O
a O
update O
gate O
and O
a O
reset O
gate O
to O
renew O
each O
node O
's O
hidden O
state O
. O

The O
results O
are O
tabulated O
at O
Table O
:1 O
. O

In O
this O
section O
, O
we O
report O
the O
searching O
scheme O
and O
actual O
values O
of O
the O
hyperparameters O
used O
by O
us O
. O

GLUE B-DatasetName
( O
Wang O
et O
al O
. O
, O
2018 O
) O
is O
another O
widely O
- O
used O
NLU B-TaskName
benchmark O
, O
including O
single O
sentence O
tasks O
( O
e.g. O
sentiment B-TaskName
analysis I-TaskName
( O
Socher O
et O
al O
. O
, O
2013 O
) O
) O
and O
sentence O
pair O
tasks O
( O
e.g. O
text B-TaskName
similarity I-TaskName
( O
Cer O
et O
al O
. O
, O
2017 O
) O
and O
natural B-TaskName
language I-TaskName
inference I-TaskName
( O
Williams O
et O
al O
. O
, O
2018;Dagan O
et O
al O
. O
, O
2005 O
) O
) O
. O

The O
evaluation O
metric O
is O
the O
BLEU B-MetricName
score O
of O
the O
infilled O
text O
against O
the O
original O
document O
. O

We O
evaluate O
GLM B-MethodName
on O
the O
Yahoo B-DatasetName
Answers I-DatasetName
dataset I-DatasetName
( O
Yang O
et O
al O
. O
, O
2017 O
) O
and O
compare O
it O
with O
Blank B-MethodName
Language I-MethodName
Model I-MethodName
( O
BLM B-MethodName
) O
( O
Shen O
et O
al O
. O
, O
2020 O
) O
, O
which O
is O
a O
specifically O
designed O
model O
for O
text B-TaskName
infilling I-TaskName
. O

It O
is O
a O
large O
scale O
multidomain O
, O
task O
oriented O
dataset O
generated O
by O
human O
- O
to O
- O
human O
conversation O
, O
where O
one O
participant O
plays O
the O
role O
of O
a O
user O
while O
the O
other O
plays O
the O
agent O
. O

We O
note O
that O
using O
single B-HyperparameterValue
- I-HyperparameterValue
wordunit I-HyperparameterValue
- I-HyperparameterValue
based I-HyperparameterValue
input O
format O
for O
both O
knowledge O
and O
question O
is O
the O
standard O
settings O
for O
the O
Transformer O
network O
and O
using O
hyperedge B-HyperparameterValue
- I-HyperparameterValue
based I-HyperparameterValue
input O
format O
for O
both O
is O
the O
proposed O
model O
, O
Hypergraph B-MethodName
Transformer I-MethodName
. O

Table O
5 O
shows O
the O
group O
- O
wise O
It O
should O
be O
noted O
that O
the O
separation O
also O
facilitates O
the O
generalization O
to O
new O
operations O
since O
the O
modules O
can O
be O
separately O
updated O
. O

In O
the O
previous O
description O
, O
we O
have O
assumed O
that O
the O
embedding B-HyperparameterName
dimensions I-HyperparameterName
of O
teachers O
and O
students O
are O
identical O
. O

Furthermore O
, O
we O
show O
that O
by O
varying O
the B-HyperparameterName
number I-HyperparameterName
and I-HyperparameterName
lengths I-HyperparameterName
of I-HyperparameterName
missing I-HyperparameterName
spans I-HyperparameterName
, O
the O
autoregressive O
blank O
filling O
objective O
can O
pretrain O
language O
models O
for O
conditional B-TaskName
and I-TaskName
unconditional I-TaskName
generation I-TaskName
. O

Knowledge B-DatasetName
- I-DatasetName
based I-DatasetName
visual I-DatasetName
question I-DatasetName
answering I-DatasetName
( O
Wang O
et O
al O
. O
, O
2017(Wang O
et O
al O
. O
, O
, O
2018Marino O
et O
al O
. O
, O
2019;Sampat O
et O
al O
. O
, O
2020 O
) O
proposed O
benchmark O
datasets O
for O
knowledge B-TaskName
- I-TaskName
based I-TaskName
visual I-TaskName
question I-TaskName
answering I-TaskName
that O
requires O
reasoning O
about O
an O
image O
on O
the O
basis O
of O
facts O
from O
a O
large O
- O
scale O
knowledge O
base O
( O
KB O
) O
such O
as O
Freebase B-DatasetName
( O
Bollacker O
et O
al O
. O
, O
2008 O
) O
or O
DBPedia B-DatasetName
( O
Auer O
et O
al O
. O
, O
2007 O
) O
. O

It O
's O
a O
more O
direct O
idea O
to O
change O
the O
softmax B-HyperparameterName
temperature I-HyperparameterName
in O
the O
final O
decoder O
layer O
rather O
than O
attention B-HyperparameterName
temperatures I-HyperparameterName
, O
namely O
changing O
the O
T B-HyperparameterName
in O
equation O
5 O
to O
some O
other O
values O
rather O
than O
the O
default O
value O
1.0 B-HyperparameterValue
. O

Further O
, O
advanced O
pre O
- O
trained O
language O
models O
( O
PLMs O
) O
with O
improved O
architectures O
or O
training O
methods O
continue O
to O
emerge O
, O
including B-MethodName
ALBERT I-MethodName
( O
Lan O
et O
al O
. O
, O
2019 O
) O
or O
RoBERTa B-MethodName
. O

In O
this O
setting O
we O
use O
both O
stochastic O
, O
L O
sto O
and O
deterministic O
, O
L O
det O
loss O
functions O
on O
dialogue B-TaskName
act I-TaskName
. O

x O
5 O

Inspired O
by O
Pattern O
- O
Exploiting O
Training O
( O
PET O
) O
( O
Schick O
and O
Schütze O
, O
2020a O
) O
, O
we O
reformulate O
NLU B-TaskName
tasks O
as O
manually B-TaskName
- I-TaskName
crafted I-TaskName
cloze I-TaskName
questions I-TaskName
that O
mimic O
human O
language O
. O

Based O
on O
the O
two O
experiments O
, O
we O
identify O
that O
not O
only O
the O
guided O
- O
attention O
which O
captures O
inter O
- O
relationships O
between O
question O
and O
knowledge O
but O
also O
the O
selfattention O
which O
learns O
intra O
- O
relationship O
in O
them O
are O
crucial O
to O
the O
complex B-TaskName
QA I-TaskName
. O

The O
formula O
for O
calculating O
ECE B-MetricName
is O
as O
follows O
: O

x O
6 O

The O
maximum B-HyperparameterName
document I-HyperparameterName
length I-HyperparameterName
is O
192 B-HyperparameterValue
and O
the O
maximum B-HyperparameterName
summary I-HyperparameterName
length I-HyperparameterName
is O
32 B-HyperparameterValue
. O

MinTL B-MethodName
- I-MethodName
BART I-MethodName
( O
Lin O
et O
al O
. O
, O
2020 O
) O
, O
introduced O
Levenshtein O
belief O
spans O
framework O
that O
predicts O
only O
the O
incremental O
change O
in O
dialogue O
state O
per O
turn O
. O

Encoder O
- O
decoder O
models O
adopt O
bidirectional O
attention O
for O
the O
encoder O
, O
unidirectional O
attention O
for O
the O
decoder O
, O
and O
cross O
attention O
between O
them O
( O
Song O
et O
al O
. O
, O
2019;Bi O
et O
al O
. O
, O
2020 O
; O
. O

Therefore O
, O
we O
remove O
the O
Stories B-DatasetName
dataset O
and O
replace O
OpenWebText B-DatasetName
with O
OpenWebText2 B-DatasetName
5 I-DatasetName
( O
66 O
GB O
) O
. O

There O
is O
an O
interesting O
line O
of O
work O
called O
selfdistillation O
or O
self O
- O
training O
( O
Furlanello O
et O
al O
. O
, O
2018;Xie O
et O
al O
. O
, O
2020;Deng O
et O
al O
. O
, O
2009;He O
et O
al O
. O
, O
2019 O
) O
, O
where O
the O
size O
of O
the O
student O
model O
is O
identical O
to O
the O
size O
of O
the O
teacher O
model O
. O

Due O
to O
resource O
limitations O
, O
we O
only O
pretrain O
the O
model O
for O
250,000 B-HyperparameterValue
steps I-HyperparameterValue
, O
which O
are O
half O
of O
RoBERTa B-MethodName
and O
BART B-MethodName
's O
training O
steps O
and O
close O
to O
T5 B-MethodName
in O
the O
number B-HyperparameterName
of I-HyperparameterName
trained I-HyperparameterName
tokens I-HyperparameterName
. O

Hypergraph B-MethodName
Transformer I-MethodName
adopts O
hypergraph O
- O
based O
representation O
to O
encode O
high O
- O
order O
semantics O
of O
knowledge O
and O
questions O
and O
considers O
associations O
between O
a O
knowledge O
hypergraph O
and O
a O
question O
hypergraph O
. O

x1 O
x2 O
x3 O
x4 O
x5 O
x6 O
x1 O
x2 O
[ O
M O
] O
x4 O
[ O
M O
] O
[ O
S O
] O
x5 O
x6 O
[ O
S O
] O
x3 O
x5 O
x6 O
[ O
E O
] O
x3 O
[ O
E O
] O
x1 O
x2 O
[ O
M O
] O
x4 O
[ O
M O
] O
[ O
S O
] O
x5 O
x6 O
[ O
S O
] O
x3 O
x1 O
x2 O
[ O
M O
] O
x4 O
[ O
M O
] O
[ O
S O
] O
x5 O
x6 O
[ O
S O
] O

The O
main O
contributions O
of O
this O
paper O
can O
be O
summarized O
as O
follows O
. O

On O
the O
contrary O
, O
our O
method O
update O
node O
representations O
via O
hyperedge O
matching O
of O
hypergraphs O
instead O
of O
message O
passing O
scheme O
. O

We O
set O
10 O
% O
of O
the O
entire O
data O
as O
the O
test O
set O
, O
which O
is O
similar O
to O
FPB B-DatasetName
. O

We O
pre O
- O
process O
every O
classification O
dataset O
except O
for O
GAD B-DatasetName
in O
the O
same O
manner O
as O
the O
BLUE B-MethodName
( O
Peng O
et O
al O
. O
, O
2019 O
) O
benchmark O
. O

We O
ablated O
two O
major O
components O
: O
calibrated O
teacher O
training O
( O
CTT O
) O
and O
activation O
boundary O
distillation O
( O
ABD O
) O
. O

[ O
S O
] O

To O
verify O
the O
general O
applicability O
of O
our O
approach O
, O
we O
conducted O
experiments O
on O
financial B-TaskName
sentiment I-TaskName
classification I-TaskName
tasks O
. O

To O
capture O
high O
- O
order O
semantics O
inherent O
in O
the O
knowledge O
sources O
, O
we O
adopt O
the O
concept O
of O
hypergraph O
. O

The O
two O
positional O
ids O
are O
projected O
into O
two O
vectors O
via O
learnable O
embedding O
tables O
, O
which O
are O
both O
added O
to O
the O
input O
token O
embeddings O
. O

Language B-TaskName
Modeling I-TaskName
. O

V O
′k O
⊂ O
V O
k O
. O

Especially O
on O
ReCoRD B-DatasetName
and O
WSC B-DatasetName
, O
where O
the O
verbalizer O
consists O
of O
multiple O
tokens O
, O
GLM B-MethodName
consistently O
outperforms O
BERT B-MethodName
. O

He O
played O
for O
the O
Los O
Angeles O
Rams O
( O
1962)(1963)(1964)(1965 O
) O
and O
the O
Detroit O
Lions O
( O
1965)(1966 O
) O
. O

Specifically O
, O
we O
normalize O
the O
token O
positions O
of O
each O
document O
to O
( O
0.0 O
, O
1.0 O
] O
and O
divide O
the O
normalized O
positions O
into O
five O
bins O
. O

Among O
the O
works O
that O
uses O
reinforcement O
learning O
. O

In O
this O
study O
, O
we O
selected O
the O
FinBERT B-MethodName
( O
Yang O
et O
al O
. O
, O
2020 O
) O
model O
as O
a O
teacher O
in O
the O
DoKTra B-MethodName
framework O
and O
evaluated O
our O
approach O
on O
two O
tasks O
, O
the O
Financial B-DatasetName
PhraseBank I-DatasetName
( O
FPB B-DatasetName
) O
and O
Fin B-DatasetName
- I-DatasetName
TextSen I-DatasetName
( O
FTS B-DatasetName
) O
. O

Our O
main O
results O
are O
shown O
in O
Table O
2 O
. O

The O
optimizer B-HyperparameterName
hyperparameters I-HyperparameterName
are O
the O
same O
as O
those O
of O
abstractive B-TaskName
summarization I-TaskName
. O

We O
present O
the O
detailed O
content O
of O
the O
example O
in O
Section O
1 O
in O
table O
12.We O
present O
more O
examples O
of O
student O
models O
' O
outputs O
and O
cross O
attention O
visualization O
here O
. O

We O
also O
evaluate O
GLM B-MethodName
on O
the O
LAMBADA B-DatasetName
dataset O
( O
Paperno O
The O
task O
is O
to O
predict O
the O
final O
word O
of O
a O
passage O
. O

We O
evaluate O
the O
quality O
of O
different O
summarization O
systems O
using O
ROUGE B-MetricName
. O

The O
performance O
of O
BERT B-MethodName
with O
cloze B-TaskName
questions I-TaskName
is O
reported O
in O
Section O
3.4 O
. O

In O
this O
work O
, O
we O
focus O
on O
abstractive B-TaskName
summarization I-TaskName
, O
which O
is O
viewed O
as O
a O
sequence O
- O
tosequence O
( O
Seq2Seq O
) O
learning O
problem O
, O
since O
recent O
abstractive O
models O
outperform O
their O
extractive O
counterparts O
and O
can O
produce O
more O
concise O
summaries O
( O
Raffel O
et O
al O
. O
, O
2020;Lewis O
et O
al O
. O
, O
2020;Zhang O
et O
al O
. O
, O
2020;Liu O
and O
Lapata O
, O
2019 O
) O
. O

Whereas O
spatial O
question O
is O
quite O
simple O
, O
it O
is O
required O
to O
understand O
a O
correct O
spatial O
relationship O
between O
multiple O
entities O
in O
a O
given O
image O
. O

By O
applying O
the O
DoKTra B-MethodName
framework O
, O
the O
ALBERT B-MethodName
- I-MethodName
xlarge I-MethodName
student O
model O
was O
able O
to O
retain O
99.72 B-MetricValue
% I-MetricValue
of O
the O
teacher O
model O
performance O
on O
an O
average O
. O

After O
removing O
articles O
with O
summaries O
less O
than O
50 O
words O
, O
we O
obtain O
the O
final O
dataset O
with O
38,264 O
articles O
for O
training O
; O
4,002 O
articles O
for O
validation O
; O
and O
3,421 O
articles O
for O
test O
. O

He O
was O
also O
a O
kick O
and O
punt O
returner O
. O

Then O
we O
compute O
the O
score O
of O
generating O
each O
answer O
candidate O
. O

In O
this O
paper O
, O
we O
propose O
a O
pretraining O
framework O
named O
GLM B-MethodName
( O
General B-MethodName
Language I-MethodName
Model I-MethodName
) O
, O
based O
on O
autoregressive O
blank O
infilling O
. O

Smith O
's O
number O
at O
Michigan O
State O
was O
# O
7 O
in O
1969.The O
work O
is O
supported O
by O
the O
NSFC O
for O
Distinguished O
Young O
Scholar(61825602 O
) O
, O
and O
Beijing O
Academy O
of O
Artificial O
Intelligence O
( O
BAAI).To O
train O
GLM B-MethodName
Base I-MethodName
and O
GLM B-MethodName
Large I-MethodName
, O
we O
use O
Book B-DatasetName
- I-DatasetName
Corpus I-DatasetName
( O
Zhu O
et O
al O
. O
, O
2015 O
) O
and O
Wikipedia B-DatasetName
used O
by O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

However O
, O
Transformer B-MethodName
( I-MethodName
SA+GA I-MethodName
) I-MethodName
attends O
to O
only O
the O
second O
and O
seventh O
word O
( O
Dorothy O
Davenport O
) O
and O
the O
fourth O
and O
ninth O
word O
( O
Harry O
Davenport O
) O
in O
knowledge O
with O
high O
attention O
score O
, O
not O
the O
answer O
entity O
, O
Myocardial O
Infarction O
. O

We O
observe O
that O
attention O
weights O
form O
three O
" O
lines O
" O
, O
which O
indicates O
very O
time O
the O
decoder O
predicts O
the O
next O
word O
, O
its O
attention O
points O
to O
the O
next O
word O
in O
the O
input O
document O
. O

All O
models O
are O
under O
the O
same O
setting O
of O
ORG+3 B-HyperparameterValue
- I-HyperparameterValue
hop I-HyperparameterValue
reported O
in O
Table O
1.We O
analyze O
QA B-TaskName
performances O
over O
different O
question O
categories O
in O
Table O
5 O
. O

In O
this O
paper O
, O
we O
propose O
an O
unsupervised O
multipletask B-MethodName
and I-MethodName
multiple I-MethodName
- I-MethodName
teacher I-MethodName
model I-MethodName
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
. O

In O
Section O
B O
and O
C O
, O
we O
present O
the O
additional O
quantitative O
and O
qualitative O
analyses O
on O
KVQA B-DatasetName
and O
PQ B-DatasetName
datasets O
, O
respectively O
. O

2 O
) O
PQ B-DatasetName
and O
PQL B-DatasetName
datasets O
have O
annotations O
of O
a O
ground O
- O
truth O
reasoning O
path O
to O
answer O
a O
given O
question O
. O

We O
follow O
the O
quality O
control O
approaches O
of O
annotator O
training O
and O
two O
- O
round O
validation O
in O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
to O
guarantee O
the O
quality O
of O
the O
hypothetical O
questions O
. O

We O
add O
a O
hyperparameter B-HyperparameterName
γ I-HyperparameterName
∈ O
[ O
0 B-HyperparameterValue
, O
1 B-HyperparameterValue
] O
, O
which O
determines O
when O
the O
training O
loss O
is O
switched O
from O
distillation O
to O
refinement O
. O

i O
= O
( O
M O
q O
W O
q O
) O
i O
⊤ O
A(M O
k O
W O
k O
) O
i O

Reproduction B-TaskName
studies O
are O
becoming O
more O
common O
in O
Natural O
Language O
Processing O
( O
NLP O
) O
, O
with O
the O
first O
shared O
tasks O
being O
organised O
, O
including O
RE B-MethodName
- I-MethodName
PROLANG I-MethodName
( O
Branco O
et O
al O
. O
, O
2020 O
) O
and O
ReproGen B-MethodName
( O
Belz O
et O
al O
. O
, O
2021b O
) O
. O

Recently O
, O
the O
pre O
- O
trained O
multilingual O
language O
model O
is O
effective O
to O
address O
the O
challenge O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

As O
shown O
in O
Table O
3 O
, O
the O
mean B-MetricName
accuracy I-MetricName
of O
QA B-TaskName
achieves O
89.7 B-MetricValue
% I-MetricValue
when O
both O
are O
encoded O
using O
hyperedges O
, O
while O
using O
single B-HyperparameterValue
- I-HyperparameterValue
word I-HyperparameterValue
- I-HyperparameterValue
unit I-HyperparameterValue
- I-HyperparameterValue
based I-HyperparameterValue
representation B-HyperparameterName
causes O
performance O
to O
drop O
to O
81.6 B-MetricValue
% I-MetricValue
. O

In O
1962 O
, O
he O
set O
the O
Wolverines O
' O
all O
- O
time O
interception O
record O
with O
13 O
, O
and O
was O
second O
overall O
in O
the O
1962 O
season O
's O
Heisman O
Trophy O
voting O
. O

TAPAS B-MethodName
- I-MethodName
WTQ I-MethodName
( O
Herzig O
et O
al O
. O
, O
2020 O
) O
, O
a O
tabular B-TaskName
QA I-TaskName
method O
that O
focuses O
on O
parsing O
and O
understanding O
tables O
, O
pre O
- O
trained O
over O
tables O
collected O
from O
Wikipedia O
before O
training O
on O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
. B-MethodName
HyBrider I-MethodName
( O
Chen O
et O
al O
. O
, O
2020c O
) O
, O
a O
hybrid B-TaskName
QA I-TaskName
method O
that O
considers O
the O
connection O
between O
the O
table O
and O
text O
. O

The O
comparison O
of O
TAPT B-MethodName
and O
DoKTra B-MethodName
using O
more O
advanced O
computing O
resources O
is O
left O
as O
a O
future O
work O
. O

It O
projects O
the O
raw O
content O
into O
latent O
representation O
. O

Then O
, O
the O
two O
graph O
representations O
are O
concatenated O
and O
fed O
into O
a B-HyperparameterValue
single I-HyperparameterValue
layer I-HyperparameterValue
feed B-HyperparameterName
- I-HyperparameterName
forward I-HyperparameterName
layer I-HyperparameterName
to O
get O
joint O
representation O
. O

Each O
hypothetical O
question O
is O
related O
to O
one O
factual O
question O
from O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
, O
but O
each O
factual O
question O
in O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
is O
not O
guaranteed O
to O
have O
one O
hypothetical O
question O
. O

Comparison O
with O
UniLM B-MethodName
( O
Dong O
et O
al O
. O
, O
2019 O
) O
. O

We O
would O
like O
to O
thank O
Woo O
Young O
Kang O
, O
Kyoung O
- O
Woon O
On O
, O
Seonil O
Son O
, O
Gi O
- O
Cheon O
Kang O
, O
Christina O
Baek O
, O
Junseok O
Park O
, O
Min O
Whoo O
Lee O
, O
Hwiyeol O
Jo O
and O
Sang O
- O
Woo O
Lee O
for O
their O
helpful O
comments O
and O
discussion O
. O

Transformers O
rely O
on O
positional O
encodings O
to O
inject O
the O
absolute O
and O
relative O
positions O
of O
the O
tokens O
. O

We O
aim O
to O
empower O
NDR B-TaskName
models O
with O
counterfactual O
thinking O
ability O
. O

For O
a O
fair O
comparison O
with O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
we O
use O
BooksCorpus B-DatasetName
( O
Zhu O
et O
al O
. O
, O
2015 O
) O
and O
English B-DatasetName
Wikipedia I-DatasetName
as O
our O
pretraining O
data O
. O

Conciseness O
and O
abstractiveness O
are O
good O
properties O
for O
summarization B-TaskName
, O
which O
however O
may O
not O
be O
the O
case O
for O
other O
generation O
tasks O
such O
as O
machine B-TaskName
translation I-TaskName
. O

3 O
) O
PQL B-DatasetName
covers O
more O
knowledge O
facts O
including O
a O
large O
number O
of O
entities O
and O
relations O
than O
PQ B-DatasetName
, O
but O
has O
fewer O
QA O
pairs O
. O

The O
selected O
hyperparameters O
are O
shown O
in O
the O
Appendix O
. O

Shared O
feature O
space O
based O
models O
exploit O
language O
- O
independent O
features O
, O
which O
lacks O
the O
domain O
- O
specific O
features O
for O
the O
target O
language O
( O
Tsai O
et O
al O
. O
, O
2016;Wu O
and O
Dredze O
, O
2019;Keung O
et O
al O
. O
, O
2019 O
) O
. O

Thanks O
to O
the O
autoregressive O
blank O
infilling O
mechanism O
we O
proposed O
, O
we O
can O
obtain O
all O
the O
log O
- O
probabilities O
in O
one O
pass O
. O

The O
hyperparameter O
β B-HyperparameterName
1 I-HyperparameterName
, O
the O
strength B-HyperparameterName
of I-HyperparameterName
the I-HyperparameterName
confidence I-HyperparameterName
penalty I-HyperparameterName
in O
teacher O
training O
, O
was O
chosen O
from O
{ O
0 B-HyperparameterValue
, O
0.3 B-HyperparameterValue
, O
0.5 B-HyperparameterValue
, O
0.7 B-HyperparameterValue
} O
. O

The O
exploiting O
of O
deep O
neural O
networks O
, O
such O
as O
Bi B-MethodName
- I-MethodName
LSTM I-MethodName
- I-MethodName
CRF I-MethodName
( O
Lample O
et O
al O
. O
, O
2016 O
) O
, O
Bi B-MethodName
- I-MethodName
LSTM- I-MethodName
CNN I-MethodName
( O
Chiu O
and O
Nichols O
, O
2016 O
) O
makes O
this O
task O
achieve O
significant O
performances O
. O

The O
scores O
across O
all O
settings O
drop O
when O
GA O
or O
SA O
is O
removed O
. O

Most O
notably O
, O
the O
governor O
is O
president O
of O
the O
senate O
and O
governor O
. O

We O
followed O
the O
hyperparameters O
used O
in O
TAPT B-MethodName
except O
for O
batch B-HyperparameterName
size I-HyperparameterName
and O
the O
maximum B-HyperparameterName
sequence I-HyperparameterName
length I-HyperparameterName
because O
we O
used O
the O
same O
computing O
resource O
as O
DoKTra B-MethodName
for O
a O
fair O
comparison O
. O

We O
evaluate O
performance O
of O
our O
method O
on O
end O
- O
to O
- O
end O
dialogue O
modeling O
task O
of O
Multi B-DatasetName
- I-DatasetName
woz2.0 I-DatasetName
( O
Budzianowski O
et O
al O
. O
, O
2018a O
) O
. O

θ O
: O
= O
θ O
− O
R(s O
t O
, O
a O
t O
, O
g)∇π O
blackbox O
( O
a O
t O
|s O
t O
; O
θ O
) O
( O
6 O
) O
Hence O
we O
believe O
our O
pairwise O
casual O
reward O
learning O
and O
associated O
improvement O
in O
sample O
efficiency O
are O
independent O
of O
model O
architecture O
. O

Recent O
work O
for O
distillation O
of O
pre O
- O
trained O
Transformers O
( O
e.g. O
, O
DistilBERT B-MethodName
( O
Sanh O
et O
al O
. O
, O
2019 O
) O
, O
TinyBERT B-MethodName
( O
Jiao O
et O
al O
. O
, O
2020 O
) O
, O
Mobile B-MethodName
- I-MethodName
BERT I-MethodName
( O
Sun O
et O
al O
. O
, O
2020 O
) O
, O
BERT B-MethodName
- I-MethodName
of I-MethodName
- I-MethodName
Theseus I-MethodName
( O
Xu O
et O
al O
. O
, O
2020a O
) O
, O
MINILM B-MethodName
) O
focuses O
on O
natural O
language O
understanding O
tasks O
such O
as O
GLUE B-DatasetName
( O
Wang O
et O
al O
. O
, O
2018 O
) O
or O
SQuAD B-DatasetName
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
benchmarks O
. O

Example O
D.4 O
. O
Robert O
Lee O
Smith O
( O
born O
July O
5 O
, O
1938 O
) O
is O
a O
former O
American O
football O
cornerback O
in O
the O
national O
football O
league O
. O

Results O
of O
T5 B-MethodName
Large I-MethodName
on O
XSum B-DatasetName
are O
obtained O
by O
running O
the O
summarization O
script O
provided O
by O
Huggingface B-HyperparameterValue
transformers I-HyperparameterValue
6 I-HyperparameterValue
. O

Hypergraph B-MethodName
Transformer I-MethodName
is O
involved O
in O
the O
weakly O
- O
supervised O
models O
because O
it O
only O
exploits O
an O
answer O
as O
a O
supervision O
. O

[ O
M O
] O

Besides O
the O
attention B-HyperparameterName
temperatures I-HyperparameterName
, O
we O
can O
also O
tune O
the O
temperature B-HyperparameterName
T B-HyperparameterName
in O
the O
decoder O
output O
softmax O
layer O
. O

Notably O
, O
our O
approach O
even O
outperformed O
RoBERTa B-MethodName
- I-MethodName
PM I-MethodName
on O
two O
tasks O
and O
demonstrated O
comparable O
performances O
on O
the O
others O
. O

We O
confirm O
that O
our O
model O
works O
effectively O
as O
a O
general O
reasoning O
framework O
without O
considering O
characteristics O
of O
different O
knowledge O
sources O
( O
i.e. O
, O
Wikidata B-DatasetName
for O
KVQA B-TaskName
, O
DBpedia B-DatasetName
, O
ConceptNet B-DatasetName
, O
WebChild B-DatasetName
for O
FVQA).To B-TaskName
required O
to O
answer O
a O
given O
question O
is O
unknown O
. O

CASPI(MinTL B-MethodName
) I-MethodName
with O
its O
robust O
pretrained O
model O
out O
performs O
CASPI(DAMD B-MethodName
) I-MethodName
and O
LAVA B-MethodName
( O
Lubis O
et O
al O
. O
, O
2020 O
) O
by O
a O
large O
margin O
. O

BART B-MethodName
use O
as O
a O
standard O
encoder O
decoder O
transformer O
architecture O
with O
a O
bidirectional O
encoder O
and O
an O
autoregressive O
decoder O
. O

Comparison O
with O
T5 B-MethodName
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
. O

Shleifer O
and O
Rush O
( O
2020 O
) O
compare O
pseudolabeling O
( O
BART B-MethodName
- I-MethodName
PL I-MethodName
) O
, O
knowledge O
distillation O
using O
both O
output O
and O
intermediate O
layers O
( O
BART B-MethodName
- I-MethodName
KD I-MethodName
) O
as O
well O
as O
shrink O
and O
fine O
- O
tuning O
( O
BART B-MethodName
- I-MethodName
SFT I-MethodName
) O
methods O
. O

We O
demonstrate O
that O
such O
training O
retains O
lexical O
, O
syntactic O
, O
and O
domain O
- O
specific O
constraints O
between O
domains O
for O
multiple O
benchmark O
datasets O
, O
including O
ones O
where O
more O
than O
one O
attribute O
change O
. O

Note O
that O
we O
used O
the O
RoBERTa B-MethodName
- I-MethodName
base I-MethodName
model O
in O
this O
section O
because O
of O
the O
training O
stability O
. O

We O
give O
a O
case O
study O
to O
show O
that O
the O
failed O
cases O
of O
baseline O
models O
can O
be O
corrected O
by O
our O
model O
. O

We O
set O
the O
dropout B-HyperparameterName
applied O
on O
the O
token O
embedding O
weights O
, O
query O
and O
key O
- O
value O
embedding O
weights O
, O
attention O
weights O
and O
residual O
connections O
from O
0.05 B-HyperparameterValue
to O
0.2 B-HyperparameterValue
. O

The O
RoBERTa B-MethodName
model O
that O
was O
applied O
to O
the O
proposed O
framework O
outperformed O
the O
teacher O
model O
on O
an O
average O
, O
specifically O
in O
four O
of O
five O
downstream O
tasks O
( O
ChemProt B-DatasetName
, O
DDI B-DatasetName
, O
i2b2 B-DatasetName
, O
and O
HoC B-DatasetName
) O
. O

Perplexity B-MetricName
is O
the O
exponentiation O
of O
the O
average O
cross O
entropy O
of O
a O
corpus O
. O

Then O
we O
finetune O
GLM B-MethodName
with O
a O
cross B-HyperparameterValue
- I-HyperparameterValue
entropy I-HyperparameterValue
loss B-HyperparameterName
( O
see O
Figure O
3 O
) O
. O

We O
thus O
believe O
that O
TAGOP B-MethodName
- I-MethodName
L2I I-MethodName
can O
generalize O
well O
to O
more O
deriving O
operations O
by O
simply O
incorporating O
the O
operators O
, O
as O
long O
as O
the O
corresponding O
training O
questions O
are O
not O
rare O
. O

To O
leverage O
the O
similarity O
between O
the O
tokens O
of O
the O
source O
languages O
, O
we O
design O
an O
multiple B-MethodName
- I-MethodName
task I-MethodName
and I-MethodName
multiple I-MethodName
- I-MethodName
teacher I-MethodName
model O
( O
short O
as O
MTMT B-MethodName
, O
as O
shown O
in O
Figure O
1 O
) O
, O
which O
helps O
the O
NER B-TaskName
learning O
process O
on O
the O
target O
languages O
. O

On O
average O
, O
GLM B-MethodName
Base I-MethodName
scores O
4.6 B-MetricValue
% I-MetricValue
higher O
than O
BERT B-MethodName
Base I-MethodName
, O
and O
GLM B-MethodName
Large I-MethodName
scores O
5.0 B-MetricValue
% I-MetricValue
higher O
than O
BERT B-MethodName
Large I-MethodName
. O

In O
addition O
to O
these O
short O
comings O
, O
the O
direct O
use O
of O
automatic O
evaluation O
metric O
as O
reward O
for O
policy O
learning O
is O
not O
desirable O
, O
since O
these O
automatic O
evaluation O
metrics O
are O
often O
for O
the O
entire O
dialogue O
and O
not O
per O
turn O
. O

( O
1 O
) O
GLM B-MethodName
consists O
of O
a O
single B-HyperparameterValue
encoder B-HyperparameterName
, O
( O
2 O
) O
GLM B-MethodName
shuffles O
the O
masked O
spans O
, O
and O
( O
3 O
) O
GLM B-MethodName
uses O
a O
single O
[ O
MASK O
] O
instead O
of O
multiple O
sentinel O
tokens O
. O

All O
datasets O
are O
tokenized O
with O
the O
GPT-2 B-MethodName
tokenizer O
( O
Radford O
et O
al O
. O
, O
2019 O
) O
, O
which O
is O
based O
on O
UTF-8 B-MethodName
BPE I-MethodName
( O
Sennrich O
et O
al O
. O
, O
2016).CNN B-DatasetName
/ I-DatasetName
DailyMail I-DatasetName
dataset O
( O
CNNDM B-DatasetName
; O
Hermann O
et O
al O
. O
, O
2015 O
) O
contains O
online O
news O
articles O
from O
the O
CNN O
and O
DailyMail O
websites O
paired O
with O
their O
associated O
highlights O
as O
reference O
summaries O
. O

x O
5 O

We O
train O
GLM B-MethodName
Base I-MethodName
and O
GLM B-MethodName
Large I-MethodName
with O
the O
same O
architectures O
as O
BERT B-MethodName
Base I-MethodName
and O
BERT B-MethodName
Large I-MethodName
, O
containing O
110 O
M O
and O
340 O
M O
parameters O
respectively O
. O

Given O
an O
input O
text O

Especially O
, O
Hypergraph B-MethodName
Transformer I-MethodName
shows O
significant O
performance O
improvement O
( O
78.6 B-MetricValue
% I-MetricValue
→ O
90.5 B-MetricValue
% I-MetricValue
for O
PQL-2H B-DatasetName
, O
78.3 B-MetricValue
% I-MetricValue
→ O
94.5 B-MetricValue
% I-MetricValue
for O
PQL B-DatasetName
- I-DatasetName
M I-DatasetName
) O
on O
PQL B-DatasetName
. O

For O
instance O
, O
TAPT B-MethodName
required O
a O
total O
of O
seven O
hours O
of O
training O
, O
while O
DoKTRa B-MethodName
was O
completed O
in O
only O
1.1 O
hours O
for O
the O
ChemProt B-DatasetName
task O
. O

Compared O
to O
the O
teacher O
with O
normal O
attention B-HyperparameterName
temperature I-HyperparameterName
( O
pink O
bar O
) O
, O
teachers O
with O
higher O
attention B-HyperparameterName
temperatures I-HyperparameterName
( O
blue O
and O
green O
bars O
) O
attend O
less O
on O
the O
heading O
parts O
of O
documents O
while O
more O
on O
the O
tail O
parts O
of O
documents O
. O

Multi O
- O
layer O
perceptron O
as O
an O
answer O
classifier O
p O
= O
ψ(z O
) O
is O
a O
prevalent O
for O
visual B-TaskName
question I-TaskName
answering I-TaskName
problems O
. O

To O
this O
end O
, O
cross B-HyperparameterValue
- I-HyperparameterValue
entropy I-HyperparameterValue
between O
prediction O
p O
and O
ground O
- O
truth O
t O
is O
utilized O
as O
a O
loss B-HyperparameterName
function I-HyperparameterName
. O

x O
4 O

The O
cloze B-TaskName
questions I-TaskName
and O
other O
details O
can O
be O
found O
in O
Appendix O
B.1 O
. O

Existing O
state O
- O
of O
- O
the O
- O
art B-TaskName
NDR I-TaskName
models O
implement O
the O
nu B-TaskName
- I-TaskName
merical I-TaskName
reasoning I-TaskName
process O
as O
neural O
network O
modules O
( O
Ran O
et O
al O
. O
, O
2019;Herzig O
et O
al O
. O
, O
2020;Zhu O
et O
al O
. O
, O
2021 O
) O
, O
e.g. O
, O
a O
graph O
neural O
network O
for O
sorting O
( O
Ran O
et O
al O
. O
, O
2019;Chen O
et O
al O
. O
, O
2020a O
) O
. O

We O
refer O
the O
original O
metric O
that O
uses O
the O
discrete O
variant O
of O
success B-MetricName
rate I-MetricName
as O
M B-MetricName
hard I-MetricName
. O

He O
later O
appeared O
as O
a O
regular O
for O
the O
show O
's O
final O
six O
seasons O
, O
and O
has O
been O
a O
frequent O
guest O
in O
the O
show O
since O
. O

As O
can O
be O
seen O
from O
the O
table O
, O
teachers O
with O
smaller O
length B-HyperparameterName
penalty I-HyperparameterName
( O
i.e. O
, O
1.0 B-HyperparameterValue
or O
0.5 B-HyperparameterValue
) O
can O
not O
teach O
better O
students O
than O
the O
Regular O
pseudo O
- O
labeling O
or O
our O
method O
. O

This O
is O
due O
to O
the O
insufficient O
number O
of O
training O
QA O
pairs O
in O
PQL-3H. B-DatasetName
When O
we O
use O
PQL-3H B-DatasetName
- I-DatasetName
More I-DatasetName
which O
has O
twice O
more O
QA O
pairs O
( O
1031 O
→ O
2062 O
) O
on O
the O
same O
knowledge O
base O
as O
PQL-3H B-DatasetName
, O
our O
model O
achieves O
95.4 B-MetricValue
% I-MetricValue
accuracy B-MetricName
. O

x O
4 O

From O
the O
results O
, O
we O
find O
that O
the O
attention O
mechanism O
between O
question O
and O
knowledge O
is O
crucial O
for O
complex O
QA B-TaskName
. O

Third O
, O
encoder O
- O
decoder O
models O
are O
pretrained O
for O
sequence O
- O
to O
- O
sequence O
tasks O
( O
Song O
et O
al O
. O
, O
2019;Bi O
et O
al O
. O
, O
2020;Zhang O
et O
al O
. O
, O
2020 O
) O
. O

[ O
M O
] O

The O
evaluation O
metrics O
are O
the O
F1 B-MetricName
scores O
of O
Rouge-1 B-MetricName
, O
Rouge-2 B-MetricName
, O
and O
Rouge B-MetricName
- I-MetricName
L I-MetricName
( O
Lin O
, O
2004 O
) O
on O
the O
test O
set O
. O

As O
revealed O
in O
the O
results O
, O
even O
though O
TAPT B-MethodName
showed O
improved O
results O
in O
the O
original O
study O
with O
Google O
Cloud O
TPU O
, O
it O
was O
unstable O
with O
the O
small O
batch B-HyperparameterName
size I-HyperparameterName
and O
sequence B-HyperparameterName
length I-HyperparameterName
; O
the O
performances O
were O
even O
degraded O
in O
the O
general O
GPU O
environment O
. O

NYT B-DatasetName
The O
New B-DatasetName
York I-DatasetName
Times I-DatasetName
dataset O
( O
NYT B-DatasetName
; O
Sandhaus O
, O
2008 O
) O
is O
composed O
of O
articles O
published O
by O
New O
York O
Times O
, O
and O
the O
summaries O
are O
written O
by O
library O
scientists O
. O

The O
ability O
of O
HQA B-TaskName
will O
undoubtedly O
enhance O
the O
practical O
use O
of O
NDR B-TaskName
due O
to O
the O
universality O
of O
hypothetical O
questions O
. O

( O
Araci O
, O
2019;Yang O
et O
al O
. O
, O
2020;Liu O
et O
al O
. O
, O
2021 O
) O
to O
fill O
the O
gap O
between O
the O
general O
and O
financial O
domains O
. O

Note O
that O
a O
weighting O
strategy O
is O
also O
provide O
therein O
to O
take O
into O
consideration O
of O
the O
reliability O
of O
the O
teachers O
. O

For O
experimental O
setup O
, O
we O
adopt O
the O
low O
resource O
testing O
strategy O
from O
Lin O
et O
al O
. O
( O
2020 O
) O
. O

In O
the O
summarization B-TaskName
task O
, O
we O
observe O
that O
1 O
) O
pseudo O
summaries O
generated O
from O
our O
teacher O
model O
copy O
more O
continuous O
text O
spans O
from O
original O
documents O
than O
reference O
summaries O
( O
56 B-MetricValue
% I-MetricValue
4 B-MetricName
- I-MetricName
grams I-MetricName
in O
pseudo O
summaries O
and O
15 B-MetricValue
% I-MetricValue
4 B-MetricName
- I-MetricName
grams I-MetricName
in O
reference O
summaries O
are O
copied O
from O
their O
original O
documents O
on O
CNN B-DatasetName
/ I-DatasetName
DailyMail I-DatasetName
dataset O
) O
; O
2 O
) O
pseudo O
summaries O
tend O
to O
summarize O
the O
leading O
part O
of O
a O
document O
( O
measured O
on O
CNN B-DatasetName
/ I-DatasetName
DailyMail I-DatasetName
, O
74 O
% O
of O
sentences O
in O
pseudo O
summaries O
and O
64 O
% O
of O
sentences O
in O
reference O
summaries O
are O
from O
the O
leading O
40 O
% O
sentences O
in O
original O
documents O
) O
. O

During O
the O
student O
learning O
process O
, O
we O
input O
unlabelled O
samples O
from O
the O
target O
languages O
into O
the O
entity O
recognizer O
and O
evaluator O
, O
and O
take O
output O
pesudo O
labels O
as O
supervisory O
signals O
for O
these O
two O
tasks O
in O
the O
student O
model O
. O

Both O
new O
objectives O
are O
defined O
in O
the O
same O
way O
as O
the O
original O
objective O
, O
i.e. O
Eq O
. O

Some O
of O
the O
texts O
are O
cut O
short O
. O

Inspired O
by O
the O
recent O
research O
on O
NDR B-TaskName
, O
we O
employ O
a O
pre O
- O
trained O
language O
model O
( O
PLM O
) O
, O
i.e. O
, O
RoBERTa B-MethodName
, O
as O
the O
encoder O
to O
learn O
an O
overall O
representation O
of O
the O
context O
, O
question O
, O
and O
assumption O
; O

The O
second O
and O
third O
attended O
knowledge O
entities O
are O
the O
other O
person O
( O
Q7141361 O
) O
and O
Iran O
. O

The O
three O
seq2seq O
models O
are O
one O
each O
for O
belief O
state O
, O
dialogue O
act O
and O
response O
generation O
modules O
. O

In O
this O
section O
, O
we O
discuss O
the O
differences O
between O
GLM B-MethodName
and O
other O
pretraining O
models O
. O

They O
are O
denoted O
by O
BART B-MethodName
12 I-MethodName
- I-MethodName
6 I-MethodName
, O
BART B-MethodName
12 I-MethodName
- I-MethodName
3 I-MethodName
, O
and O
BART B-MethodName
12 I-MethodName
- I-MethodName
12 I-MethodName
with O
6 B-HyperparameterValue
, O
3 B-HyperparameterValue
, O
and O
12 B-HyperparameterValue
decoder O
layers O
, O
respectively O
. O

Appendix O
. O
This O
supplementary O
material O
provides O
additional O
information O
not O
described O
in O
the O
main O
text O
due O
to O
the O
page O
limit O
. O

Recent O
advancements O
in O
off O
- O
policy O
reinforcement O
learning O
methods O
that O
uses O
offline O
data O
as O
against O
a O
simulator O
has O
proven O
to O
be O
sample O
efficient O
( O
Thomas O
and O
Brunskill O
, O
2016 O
) O
. O

Alternatively O
use O
of O
imitation O
learning O
based O
methods O
falls O
short O
of O
reasoning O
on O
the O
outcome O
. O

To O
address O
the O
above O
limitation O
, O
we O
propose O
a O
novel O
method O
, O
Hypergraph B-MethodName
Transformer I-MethodName
, O
which O
exploits O
hypergraph O
structure O
to O
encode O
multi O
- O
hop O
relationships O
and O
transformer O
- O
based O
attention O
mechanism O
to O
learn O
to O
pay O
attention O
to O
important O
knowledge O
evidences O
for O
a O
question O
. O

x O
4 O

For O
knowledge O
distillation O
, O
we O
use O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1e-6 B-HyperparameterValue
for O
the O
student O
models O
training O
. O

Hence O
we O
only O
use O
the O
deterministic O
loss O
, O
L O
det O
directly O
on O
the O
generated O
response O
and O
for O
DST B-TaskName
we O
retain O
the O
loss O
as O
is O
from O
MintTL B-MethodName
( O
Lin O
et O
al O
. O
, O
2020).For O
k B-HyperparameterName
- O
model O
training O
of O
pairwise O
casual O
reward O
learning O
illustrated O
in O
Fig O
: O
3 O
, O
we O
chose O
DAMD B-MethodName
( O
Zhang O
et O
al O
. O
, O
2019 O
) O
model O
for O
it O
's O
light O
weight O
model O
architecture O
. O

Table O
2 O
) O
. O

Specifically O
, O
given O
a O
labeled O
example O
( O
x O
, O
y O
) O
, O
we O
convert O
the O
input O
text O
x O
to O
a O
cloze O
question O
c(x O
) O
via O
a O
pattern O
containing O
a O
single O
mask O
token O
. O

He O
completed O
his O
NFL O
career O
with O
five O
interceptions O
. O

This O
is O
because O
the O
reward O
of O
current O
state O
will O
depend O
on O
the O
performance O
of O
future O
states O
. O

During O
decoding O
, O
we O
use O
beam B-HyperparameterValue
search I-HyperparameterValue
with O
beam B-HyperparameterName
size I-HyperparameterName
of O
5 B-HyperparameterValue
and O
remove O
repeated O
trigrams O
. O

Random O
initialization O
is O
applied O
when O
a O
word O
for O
a O
node O
does O
not O
exist O
in O
the O
vocabulary O
of O
GloVe B-MethodName
. O

In O
Table O
3 O
, O
the O
huge O
performance O
drop O
shows O
that O
even O
the O
state O
- O
of O
- O
the O
- O
art O
NDR B-TaskName
model O
lacks O
counterfactual O
thinking O
ability O
. O

The O
maximum B-HyperparameterName
passage I-HyperparameterName
length I-HyperparameterName
is O
464 B-HyperparameterValue
and O
the O
maximum B-HyperparameterName
question I-HyperparameterName
length I-HyperparameterName
is O
48 B-HyperparameterValue
. O

The O
first O
is O
a O
contrastive B-HyperparameterValue
loss I-HyperparameterValue
and O
the O
second O
is O
a O
classification B-HyperparameterValue
loss I-HyperparameterValue
-aiming O
to O
regularize O
the O
latent O
space O
further O
and O
bring O
similar O
sentences O
across O
domains O
closer O
together O
. O

With O
these O
extremely O
large O
models O
, O
we O
can O
obtain O
state O
- O
of O
- O
theart O
summarization O
results O
, O
but O
they O
are O
slow O
for O
online O
inference O
, O
which O
makes O
them O
difficult O
to O
be O
used O
in O
the O
production O
environment O
even O
with O
cutting O
- O
edge O
hardware O
. O

, O
x O
|X| O
) O
and O
its O
gold O
summary O
Y O
= O
( O
y O
1 O
, O
y O
2 O
, O
. O

2019 O
) O
, O
it O
is O
possible O
to O
transfer O
the O
annotated O
training O
samples O
or O
trained O
models O
from O
a O
rich O
- O
resource O
domain O
to O
a O
zero O
- O
resource O
domain O
. O

To O
train O
GLM B-MethodName
RoBERTa I-MethodName
, O
we O
follow O
most O
of O
the O
hyperparameters O
of O
RoBERTa B-MethodName
. O

QRA B-TaskName
produces O
a O
single O
score O
estimating O
the O
degree O
of O
reproducibility O
of O
a O
given O
system O
and O
evaluation O
measure O
, O
on O
the O
basis O
of O
the O
scores O
from O
, O
and O
differences O
between O
, O
different O
reproductions O
. O

We O
highlight O
that O
PQL B-DatasetName
is O
more O
challenging O
dataset O
than O
PQ B-DatasetName
in O
that O
PQL B-DatasetName
not O
only O
covers O
more O
knowledge O
facts O
but O
also O
has O
fewer O
QA O
instances O
. O

Formally O
, O
directed O
hypergraph O
H O
= O
{ O
V O
, O
E O
} O
is O
defined O
by O
a O
set O
of O
nodes O
V O
= O
{ O
v O
1 O
, O
... O
, O
v O
|V| O
} O
and O
a O
set O
of O
hyperedges O
E O
= O
{ O
h O
1 O
, O
... O
, O
h O
|E| O
} O
. O

The O
results O
on O
RoBERTa B-MethodName
imply O
that O
our O
proposed O
framework O
can O
be O
effectively O
applied O
to O
emerging O
and O
advanced O
pre O
- O
trained O
language O
models O
. O

Multi O
- O
hop O
graph O
walk O
connects O
multiple O
facts O
by O
setting O
the O
arrival O
node O
( O
tail O
) O
of O
the O
preceding O
walk O
as O
the O
starting O
( O
head O
) O
node O
of O
the O
next O
walk O
, O
thus O
, O
n O
- O
hop O
graph O
walk O
combines O
n O
facts O
as O
a O
hyperedge O
. O

Recently O
, O
researches O
for O
VQA B-TaskName
have O
advanced O
, O
from O
inferring O
visual O
properties O
on O
entities O
in O
a O
given O
image O
, O
to O
inferring O
commonsense O
or O
world O
knowledge O
about O
those O
entities O
( O
Wang O
et O
al O
. O
, O
2017(Wang O
et O
al O
. O
, O
, O
2018Marino O
et O
al O
. O
, O
2019;Zellers O
et O
al O
. O
, O
2019 O
) O
. O

Instead O
, O
GLM B-MethodName
unifies O
NLU B-TaskName
and O
generation O
tasks O
with O
autoregressive O
pretraining O
. O

To O
sum O
up O
, O
the O
similarity O
- O
based O
answer O
selector O
( O
SIM O
) O
contributes O
to O
infer O
few O
- O
shot O
and O
zero O
- O
shot O
answers O
in O
parameter O
- O
efficient O
manner O
. O

He O
has O
appeared O
in O
music O
videos O
for O
the O
killers O
in O
1993 O
, O
the O
pretenders O
in O
1995 O
, O
and O
in O
the O
TV O
shows O
" O
the O
royal O
" O
and O
" O
the O
bill O
" O
. O

To O
handle O
this O
issue O
, O
we O
reconstruct O
the O
data O
to O
pair O
format O
. O

The O
FinTextSen B-DatasetName
( O
FTS B-DatasetName
) O
( O
Cortis O
et O
al O
. O
, O
2017 O
) O
consists O
of O
financial O
tweets O
from O
Twitter O
and O
StockTwits O
with O
real O
- O
valued O
sentiment O
scores O
. O

The O
output O
representation O
of O
i O
- O
th O
layer O
is O
O O
i O
= O
j O
p O
ij O
o O
ij O
where O
o O
is O
the O
another O
embeddings O
of O
knowledge O
facts O
different O
from O
m. O
The O
updated O
question O
representation O
is O
q O
k+1 O
= O
O O
k+1 O
+ O
q O
k O
, O
and O
based O
on O
the O
output O
representation O
and O
question O
representation O
, O
answer O
is O
predicted O
as O
follows O
: O
a O
= O
softmax(f O
( O
O O
K O
+ O
q O
K−1 O
) O
) O
where O
f O
is O
a B-HyperparameterValue
single I-HyperparameterValue
layer I-HyperparameterValue
feed B-HyperparameterName
- I-HyperparameterName
forward I-HyperparameterName
layer I-HyperparameterName
. O

Smith O
earned O
the O
honor O
because O
of O
his O
accomplishments O
prior O
to O
his O
NFL O
career O
. O

The O
semantic O
labels O
of O
visual O
concepts O
or O
named O
entities O
are O
then O
linked O
with O
knowledge O
entities O
in O
the O
knowledge O
base O
using O
exact O
keyword O
matching O
. O

BART B-MethodName
( O
Lewis O
et O
al O
. O
, O
2020 O
) O
employs O
denoising O
auto O
- O
encoding O
objectives O
such O
as O
text O
infilling O
and O
sentence O
permutation O
during O
its O
pre O
- O
training O
. O

We O
set O
the O
Transformer B-MethodName
( I-MethodName
SA+GA I-MethodName
) I-MethodName
as O
a O
backbone O
model O
, O
and O
present O
the O
results O
in O
Table O
3 O

The O
contributions O
of O
this O
study O
can O
be O
summarized O
as O
follows O
: O

We O
apply O
our O
framework O
to O
the O
biomedical O
domain O
and O
verify O
its O
effectiveness O
by O
conducting O
experiments O
on O
several O
biomedical O
and O
clinical O
downstream O
tasks O
. O

We O
use O
the O
BART B-MethodName
12 I-MethodName
- I-MethodName
6 I-MethodName
as O
the O
student O
model O
, O
and O
the O
distillation O
results O
on O
CNNDM B-DatasetName
are O
in O
Table O
5 O
. O

We O
construct O
a O
question O
hypergraph O
and O
a O
knowledge O
hypergraph O
to O
explicitly O
encode O
high O
- O
order O
semantics O
present O
in O
the O
question O
and O
each O
knowledge O
fact O
, O
and O
capture O
multi O
- O
hop O
relational O
knowledge O
facts O
effectively O
. O

Here O
, O
we O
assume O
that O
the O
performance O
of O
entity O
linking O
is O
perfect O
, O
and O
evaluate O
the O
pure O
reasoning O
ability O
of O
our O
model O
. O

QA B-TaskName
performances O
in O
the O
entity O
linking O
setting O
on O
KVQA B-DatasetName
are O
shown O
in O
Table O
7 O
. O

As O
shown O
in O
Table O
8 O
of O
Appendix O
D O
, O
Hypergraph B-MethodName
Transformer I-MethodName
shows O
comparable O
performance O
in O
both O
top-1 O
and O
top-3 O
accuracy B-MetricName
in O
comparison O
with O
the O
state O
- O
of O
- O
the O
- O
art O
methods O
. O

For O
γ B-HyperparameterName
analysis O
, O
we O
consider O
the O
consistency O
of O
recognition O
results O
and O
similarity O
score O
by O
teachers O
. O

As O
to O
Seq2Seq O
models O
, O
an O
effective O
distillation O
method O
is O
called O
pseudo O
- O
labeling O
( O
Kim O
and O
Rush O
, O
2016 O
) O
, O
where O
the O
teacher O
model O
generates O
pseudo O
summaries O
for O
all O
documents O
in O
the O
training O
set O
and O
the O
resulting O
document O
- O
pseudo O
- O
summary O
pairs O
are O
used O
to O
train O
the O
student O
model O
. O

Specifically O
, O
we O
select O
two O
PLMs O
as O
the O
initial O
student O
model O
: O
ALBERT B-MethodName
- I-MethodName
xlarge I-MethodName
( O
Lan O
et O
al O
. O
, O
2019 O
) O
, O
which O
has O
a O
smaller O
number O
of O
parameters O
but O
performs O
better O
than O
BERT B-MethodName
, O
and O
RoBERTa B-MethodName
- I-MethodName
large I-MethodName
, O
which O
has O
a O
larger O
number O
of O
parameters O
and O
is O
known O
to O
outperform O
BERT B-MethodName
significantly O
for O
most O
of O
the O
tasks O
. O

Due O
to O
the O
randomness O
of O
the O
stochasticity O
, O
misinformed O
or O
incomplete O
hyperedges O
can O
be O
extracted O
. O

Text B-TaskName
Infilling I-TaskName
. O
Text B-TaskName
infilling I-TaskName
is O
the O
task O
of O
predicting O
missing O
spans O
of O
text O
which O
are O
consistent O
with O
the O
surrounding O
context O
( O
Zhu O
et O
al O
. O
, O
2019;Donahue O
et O
al O
. O
, O
2020;Shen O
et O
al O
. O
, O
2020 O
) O
. O

This O
is O
because O
DoKTra B-MethodName
leverages O
the O
knowledge O
of O
an O
existing O
indomain O
PLM O
, O
thus O
requiring O
only O
a O
few O
fine O
- O
tuning O
and O
distillation O
steps O
. O

Additionally O
, O
we O
also O
measure O
the O
quality O
of O
generated O
summaries O
by O
eliciting O
human O
judgements O
. O

In O
Natural O
Language O
Processing O
( O
NLP O
) O
, O
the O
umbrella O
term O
attribute O
transfer O
( O
Jin O
et O
al O
. O
, O
2020b O
) O
transfer O
) O
refers O
to O
similar O
methods O
2 O
. O

The O
hidden B-HyperparameterName
size I-HyperparameterName
of O
each O
layer O
is O
1024 B-HyperparameterValue
, O
and O
each O
layer O
contains O
16 B-HyperparameterValue
attention O
heads O
with O
a O
hidden B-HyperparameterName
size I-HyperparameterName
of O
64 B-HyperparameterValue
. O

The O
student O
models O
with O
our O
method O
( O
λ B-HyperparameterName
= O
1.5 B-HyperparameterValue
and O
λ B-HyperparameterName
= O
2.0 B-HyperparameterValue
) O
slightly O
outperform O
the O
student O
with O
regular O
pseudo O
- O
labeling O
method O
( O
λ B-HyperparameterName
= O
1.0 B-HyperparameterValue
) O
. O

Suppose O
we O
have O
a O
set O
of O
labeled O
questions O
D O
= O
{ O
< O
ȳ O
, O
( O
q O
, O
c O
, O
a O
) O
> O
} O
, O
the O
training O
objective O
can O
be O
abstracted O
as O
min O
θ O
D O
QA O
( O
ȳ O
, O
f O
( O
q O
, O
c O
, O
a O
) O
) O
where O
θ O
denotes O
model O
parameters O
. O

As O
shown O
in O
Table O
6 O
, O
the O
MLP O
fails O
to O
infer O
zeroshot O
answers O
which O
are O
not O
appeared O
in O
the O
training O
phase O
at O
all O
. O

Note O
that O
U O
[ O
a B-HyperparameterName
, O
b B-HyperparameterName
] O
is O
a O
uniform O
distribution O
and O
we O
typically O
set O
a B-HyperparameterName
= O
1.0 B-HyperparameterValue
and O
b B-HyperparameterName
= O
2.0.We B-HyperparameterValue
conduct O
our O
experiments O
on O
three O
popular O
document O
summarization O
datasets O
: O
CNN B-DatasetName
/ I-DatasetName
DailyMail I-DatasetName
( O
Hermann O
et O
al O
. O
, O
2015 O
) O
, O
XSum B-DatasetName
( O
Narayan O
et O
al O
. O
, O
2018 O
) O
, O
and O
New B-DatasetName
York I-DatasetName
Times I-DatasetName
( O
Sandhaus O
, O
2008 O
) O
. O

The O
strength O
of O
our O
methodconciseness O
and O
abstractiveness O
are O
good O
properties O
for O
summarization B-TaskName
but O
seem O
not O
very O
beneficial O
to O
the O
translation B-TaskName
task O
. O

We O
calculate O
novel B-MetricName
n I-MetricName
- I-MetricName
grams I-MetricName
and B-MetricName
lengths I-MetricName
of O
generated O
summaries O
. O

Position O
1 O
1 O
2 O
3 O
4 O
5 O
5 O
5 O
5 O
3 O
3 O
Position O
2 O
0 O
0 O
0 O
0 O
0 O
1 O
2 O
3 O
1 O
2 O

In O
Figure O
4 O
, O
we O
show O
that O
removing O
the O
second O
dimension O
of O
2D O
positional O
encoding O
hurts O
the O
performance O
of O
long B-TaskName
text I-TaskName
generation I-TaskName
. O

Blank B-MethodName
Language I-MethodName
Modeling I-MethodName
. O

Automatic B-TaskName
document I-TaskName
summarization I-TaskName
is O
the O
task O
of O
rewriting O
a O
long O
document O
into O
its O
shorter O
form O
while O
still O
retaining O
its O
most O
important O
content O
. O

Guided O
- O
attention O
To O
learn O
inter O
- O
association O
between O
two O
hypergraphs O
, O
we O
first O
embed O
a O
knowledge O
hyperedge O
and O
a O
question O
hyperedge O
as O
follows O
: O
We O
define O
the O
knowledge O
hyperedges O
E O
k O
and O
the O
question O
hyperedges O
E O
q O
as O
a O
query O
and O
key O
- O
value O
pairs O
, O
respectively O
. O

With O
a O
proper O
T B-HyperparameterName
( O
i.e. O
, O
T B-HyperparameterName
= O
0.5 B-HyperparameterValue
) O
during O
pseudo O
label O
generation O
, O
the O
resulting O
student O
model O
slightly O
outperforms O
the O
baseline O
student O
model O
with O
regular O
pseudo O
labeling O
method O
on O
ROUGE-2 B-MetricName
/ O
L B-MetricName
( O
see O
Table O
5 O
) O
, O
but O
worse O
than O
PLATE B-MethodName
λ=2.0 I-MethodName
. O

x O
6 O

In O
1961 O
, O
the O
" O
Los O
Angeles O
Times O
" O
wrote O
that O
Smith O
" O
is O
an O
outstanding O
pass O
rusher O
, O
with O
an O
average O
of O
almost O
100 O
yards O
per O
punt O
return O
. O

Second O
, O
we O
show O
the O
SuperGLUE B-DatasetName
performance O
of O
GLM B-MethodName
finetuned O
as O
sequence O
classifiers O
( O
row O
5 O
) O
and O
BERT B-MethodName
with O
clozestyle O
finetuning O
( O
row O
3 O
) O
. O

We O
introduce O
a O
method O
for O
such O
constrained O
unsupervised O
text O
style O
transfer O
by O
introducing O
two O
complementary B-HyperparameterValue
losses I-HyperparameterValue
to O
the O
generative O
adversarial O
network O
( O
GAN O
) O
family O
of O
models O
. O

However O
, O
since O
deep O
neural O
networks O
highly O
rely O
on O
a O
large O
amount O
of O
labelled O
training O
data O
, O
the O
annotation O
acquiring O
process O
is O
expensive O
and O
time O
consuming O
. O

Because O
the O
student O
model O
is O
already O
fine O
- O
tuned O
before O
the O
distillation O
step O
, O
this O
additional O
refinement O
may O
cause O
overconfidence O
. O

Our O
results O
on O
newstest2014 B-DatasetName
are O
shown O
in O
Table O
8 O
. O

TAGOP B-MethodName
, O
a O
hybrid B-TaskName
QA I-TaskName
method O
that O
performs O
discrete O
reasoning O
over O
both O
the O
tabular O
and O
textual O
contexts O
. O

The O
baselines O
are O
: O
BERT B-MethodName
- I-MethodName
RC I-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
a O
traditional B-TaskName
QA I-TaskName
method O
that O
selects O
answer O
spans O
from O
the O
context O
. O

UniLM B-MethodName
( O
Dong O
et O
al O
. O
, O
2019;Bao O
et O
al O
. O
, O
2020 O
) O
unifies O
three O
pretraining O
models O
under O
the O
masked O
language O
modeling O
objective O
with O
different O
attention O
masks O
. O

Thus O
, O
GCN B-MethodName
and O
GGNN B-MethodName
show O
quite O
low O
performance O
under O
74 B-MetricValue
% I-MetricValue
mean O
accuracy B-MetricName
. O

It O
is O
the O
state O
- O
of O
- O
the O
- O
art O
method O
on O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
dataset O
. O

Then O
, O
we O
distilled O
for O
10 B-HyperparameterValue
epochs B-HyperparameterName
with B-HyperparameterName
learning I-HyperparameterName
rates I-HyperparameterName
of O
{ O
6e-6 B-HyperparameterValue
, O
8e-6 B-HyperparameterValue
, O
1e-5 B-HyperparameterValue
} O
. O

We O
can O
observe O
that O
: O
1 O
) O
TAGOP B-MethodName
- I-MethodName
L2I I-MethodName
achieves O
the O
best O
performance O
among O
all O
the O
compared O
methods O
. O

Additionally O
, O
we O
use O
the O
CNN B-DatasetName
/ I-DatasetName
DailyMail I-DatasetName
( O
See O
et O
al O
. O
, O
2017 O
) O
and O
XSum B-DatasetName
( O
Narayan O
et O
al O
. O
, O
2018 O
) O
datasets O
for O
abstractive B-TaskName
summarization I-TaskName
as O
the O
benchmarks O
for O
models O
pretrained O
on O
larger O
corpora O
. O

Probably O
because O
the O
student O
can O
easily O
adapt O
to O
the O
scaled O
attention O
temperature O
during O
training O
. O

We O
represent O
DB O
results O
as O
one O
- O
hot O
vectors O
as O
proposed O
by O
Budzianowski O
et O
al O
. O
( O
2018b O
) O
. O

x O
5 O

For O
entity O
linking O
for O
KVQA B-DatasetName
, O
we O
apply O
the O
wellknown O
pre O
- O
trained O
models O
for O
face O
identification O
: O
RetinaFace B-MethodName
( O
Deng O
et O
al O
. O
, O
2020 O
) O
for O
face B-TaskName
detection I-TaskName
and O
ArcFace B-MethodName
( O
Deng O
et O
al O
. O
, O
2019 O
) O
for O
face B-TaskName
feature I-TaskName
extraction I-TaskName
. O

We O
conducted O
experiments O
on O
three O
benchmark O
datasets O
: O
CoNLL2002 B-DatasetName
( O
Tjong O
Kim O
Sang O
, O
2002 O
) O
, O
CoNLL2003 B-DatasetName
( O
Tjong O
Kim O
Sang O
and O
De O
Meulder O
, O
2003 O
) O
and O
WikiAnn B-DatasetName
( O
Pan O
et O
al O
. O
, O
2017 O
) O
. O

Terry O
first O
appeared O
in O
the O
TV O
series O
" O
theKnowledge O
- O
based O
visual O
question B-TaskName
answering I-TaskName
( O
QA B-TaskName
) O
aims O
to O
answer O
a O
question O
which O
requires O
visually O
- O
grounded O
external O
knowledge O
beyond O
image O
content O
itself O
. O

We O
define O
quality O
by O
the O
following O
criterias O
: O

Following O
the O
paper O
, O
the O
dataset O
provides O
five O
splits O
of O
train O
and O
test O
data O
. O

LAVA B-MethodName
( O
Lubis O
et O
al O
. O
, O
2020 O
) O
, O
reduces O
the O
action O
space O
of O
policy O
in O
end O
- O
to O
- O
end O
ToD B-TaskName
, O
by O
using O
the O
latent O
space O
of O
a O
variational O
model O
with O
an O
informed O
prior O
. O

Multiple B-HyperparameterName
spans I-HyperparameterName
( O
sentences O
) O
are O
sampled O
to O
cover O
15 B-HyperparameterValue
% I-HyperparameterValue
of I-HyperparameterValue
the I-HyperparameterValue
original I-HyperparameterValue
tokens I-HyperparameterValue
. O

DAMD B-MethodName
is O
composed O
of O
three O
seq2seq O
generative O
model O
using O
GRUs O
. O

Attention(Q O
k O
, O
K O
k O
, O
V O
k O
) O
. O

Another O
phenomenon O
we O
observe O
is O
that O
all O
high O
- O
value O
attention O
weights O
( O
in O
deeper O
color O
) O
concentrate O
on O
the O
first O
200 O
words O
in O
the O
input O
document O
, O
which O
reflects O
the O
leading O
bias O
. O

To O
answer O
this O
question O
for O
a O
given O
specific O
system O
, O
typically O
( O
Wieling O
et O
al O
. O
, O
2018;Arhiliuc O
et O
al O
. O
, O
2020;Popović O
and O
Belz O
, O
2021 O
) O
an O
original O
study O
is O
selected O
and O
repeated O
more O
or O
less O
closely O
, O
before O
comparing O
the O
results O
obtained O
in O
the O
original O
study O
with O
those O
obtained O
in O
the O
repeat O
, O
and O
deciding O
whether O
the O
two O
sets O
of O
results O
are O
similar O
enough O
to O
support O
the O
same O
conclusions O
. O

That O
demonstrates O
the O
benefits O
of O
our O
proposed O
MTMT B-MethodName
model O
, O
compared O
to O
direct O
model O
transfer O
( O
Wu O
and O
Dredze O
, O
2019 O
) O
. O

Due O
to O
the O
distributed O
representation O
of O
natural O
languages O
, O
the O
relatedness O
among O
the O
embedding O
of O
target O
languages O
, O
which O
is O
measured O
by O
the O
similarity O
, O
can O
be O
utilized O
to O
further O
boost O
the O
learned O
encoder O
and O
improve O
the O
final O
NER B-TaskName
performance O
on O
target O
languages O
. O

For O
example O
, O
a O
sentiment B-TaskName
classification I-TaskName
task O
can O
be O
formulated O
as O
" O
{ O
SENTENCE O
} O
. O

Automatic O
evaluation O
metrics O
have O
their O
own O
biases O
. O

For O
multi O
- O
head O
attention O
, O
the O
attended O
outputs O
with O
different O
heads O
are O
concatenated O
and O
fed O
into O
a B-HyperparameterValue
single I-HyperparameterValue
layer I-HyperparameterValue
feedforward B-HyperparameterName
layer I-HyperparameterName
to O
make O
a O
final O
representation O
. O

Wyoming O
's O
constitution O
provides O
that O
the O
governor O
can O
appoint O
a O
member O
of O
the O
wyoming O
state O
senate O
to O
the O
wyoming O
supreme O
court O
, O
and O
the O
chairman O
of O
the O
wyoming O
senate O
. O

The O
experimental O
results O
from O
the O
biomedical O
, O
clinical O
, O
and O
financial O
domain O
downstream O
tasks O
demonstrated O
that O
our O
proposed O
framework O
could O
transfer O
domain O
- O
specific O
knowledge O
into O
a O
PLM O
, O
while O
preserving O
its O
own O
expressive O
advantages O
without O
any O
further O
pre O
- O
training O
with O
additional O
in O
- O
domain O
data O
. O

max O
θ O
E O
z∼Zm O
m O
i=1 O
log O
p O
θ O
( O
s O
z O
i O
|x O
corrupt O
, O
s O
z O
< O
i O
) O
( O
1 O
) O

As O
can O
be O
seen O
, O
both O
lower O
and O
larger O
λ B-HyperparameterName
values O
are O
not O
helpful O
to O
the O
distillation O
. O

MinTL B-MethodName
uses O
a O
large O
pretrained O
language O
model O
BART B-MethodName
( O
Lewis O
et O
al O
. O
, O
2019 O
) O
. O

Prominent O
examples O
include O
translation B-TaskName
of O
text O
between O
languages O
( O
Vaswani O
et O
al O
. O
, O
2017;Artetxe O
et O
al O
. O
, O
2018;Lample O
et O
al O
. O
, O
2017 O
) O
, O
emoji O
creation O
from O
human O
faces O
( O
Taigman O
et O
al O
. O
, O
2017 O
) O
, O
and O
stylistic O
transfer O
of O
speech O
( O
Yuan O
et O
al O
. O
, O
2021 O
) O
. O

