We O
use O
teacher B-HyperparameterValue
forcing I-HyperparameterValue
and O
consider O
the O
prediction O
correct O
only O
when O
all O
the O
predicted O
tokens O
are O
correct O
. O

For O
tokens O
in O
Part O
A O
, O
their O
second O
positional O
ids O
are O
0 O
. O

We O
conducted O
an O
experiment O
to O
verify O
the O
positive O
effect O
of O
combining O
calibrated O
teacher O
training O
and O
activation O
boundary O
distillation O
. O

Training O
on O
CNNDM B-DatasetName
with O
the O
teacher O
model O
( O
i.e. O
, O
BART B-MethodName
) O
is O
most O
time O
- O
consuming O
. O

It O
indicates O
that O
the O
teachers O
can O
produce O
more O
concise O
and O
abstractive O
summaries O
, O
which O
matches O
the O
goal O
of B-TaskName
abstractive I-TaskName
summarization I-TaskName
. O

x O
5 O

We O
trained O
the O
model O
with O
the O
labeled O
training O
set O
of O
the O
source O
language O
and O
evaluated O
the O
model O
on O
the O
test O
set O
of O
each O
target O
language O
. O

) O
where O
σ B-HyperparameterName
is O
a O
logistic B-HyperparameterValue
sigmoid I-HyperparameterValue
function I-HyperparameterValue
, O
and O
W O
[ O
• O
] O
and O
U O
[ O
• O
] O
are O
learnable O
parameters O
. O

Perhaps O
not O
surprisingly O
, O
the O
styles O
of O
summaries O
from O
students O
are O
similar O
with O
these O
from O
their O
teachers O
. O

Autoregressive O
models O
, O
such O
as O
GPT B-MethodName
( O
Radford O
et O
al O
. O
, O
2018a O
) O
, O
learn O
left O
- O
to O
- O
right O
language O
models O
. O

We O
note O
that O
using O
single B-HyperparameterValue
- I-HyperparameterValue
wordunit I-HyperparameterValue
- I-HyperparameterValue
based I-HyperparameterValue
input O
format O
for O
both O
knowledge O
and O
question O
is O
the O
standard O
settings O
for O
the O
Transformer O
network O
and O
using O
hyperedge B-HyperparameterValue
- I-HyperparameterValue
based I-HyperparameterValue
input O
format O
for O
both O
is O
the O
proposed O
model O
, O
Hypergraph B-MethodName
Transformer I-MethodName
. O

This O
work O
studies O
a O
new O
and O
more O
challenging O
task O
that O
focuses O
on O
hypothetical O
question O
. O

i O
= O
{ O
v O
′ O
1 O
⪯ O
... O
⪯ O
v O
′ O
l O
} O
where O
V O
′ O
= O
{ O
v O
′ O
1 O
, O
... O
, O
v O
′ O
l O
} O
is O
a O
subset O
of O
V O
and O
⪯ O
is O
a O
binary O
relation O
which O
denotes O
an O
element O
( O
v O
′ O
i O
) O
precedes O
the O
other O
( O
v O
′ O
j O
) O
in O
the O
ordering O
when O
v O
′ O
i O
⪯ O
v O
′ O
j O
. O

The O
first O
positional O
i O
d O
represents O
the O
position O
in O
the O
corrupted O
text O
x O
corrupt O
. O

We O
set O
our O
hyperparameters O
empirically O
following O
( O
Wu O
et O
al O
. O
, O
2020c O
) O
with O
some O
modifications O
. O

With O
a O
proper O
T B-HyperparameterName
( O
i.e. O
, O
T B-HyperparameterName
= O
0.5 B-HyperparameterValue
) O
during O
pseudo O
label O
generation O
, O
the O
resulting O
student O
model O
slightly O
outperforms O
the O
baseline O
student O
model O
with O
regular O
pseudo O
labeling O
method O
on O
ROUGE-2 B-MetricName
/ O
L B-MetricName
( O
see O
Table O
5 O
) O
, O
but O
worse O
than O
PLATE B-MethodName
λ=2.0 I-MethodName
. O

To O
reduce O
surface O
- O
level O
variability O
in O
the O
responses O
, O
we O
use O
domain O
- O
adaptive O
delexicalization O
preprocess O
- O
ing O
proposed O
in O
Wen O
et O
al O
. O
( O
2016 O
) O
. O

However O
, O
for O
arithmetic O
questions O
, O
the O
question B-TaskName
- I-TaskName
answering I-TaskName
label O
for O
one O
pair O
of O
c O
and O
q O
remains O
the O
same O
between O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
and O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
, O
and O
the O
intervention O
is O
achieved O
explicitly O
by O
deriving O
operators O
and O
tagging O
head O
. O

However O
, O
we O
are O
interested O
in O
pretraining O
a O
single O
model O
that O
can O
handle O
both O
NLU B-TaskName
and O
text B-TaskName
generation I-TaskName
. O

The O
weight B-HyperparameterName
for I-HyperparameterName
the I-HyperparameterName
contrastive I-HyperparameterName
loss I-HyperparameterName
is O
0.1.Overall B-HyperparameterValue
performance O
. O

Example O
D.2 O
. O
Jonathan O
Terry O
is O
a O
television O
and O
film O
actor O
. O

[ O
M O
] O

Text O
style O
transfer O
, O
a O
popular O
form O
of O
attribute O
transfer O
, O
regards O
" O
style O
" O
as O
any O
attribute O
that O
changes O
between O
datasets O
( O
Jin O
et O
al O
. O
, O
2020a O
) O
. O

In O
this O
section O
, O
we O
report O
the O
searching O
scheme O
and O
actual O
values O
of O
the O
hyperparameters O
used O
by O
us O
. O

This O
demonstrates O
the O
ease O
of O
adaptation O
of O
existing O
methods O
with O
CASPI.Inverse B-MethodName
reinforcement O
learning O
, O
coupled O
with O
offpolicy O
policy O
learning O
and O
evaluation O
are O
proven O
to O
be O
sample O
efficient O
( O
Thomas O
and O
Brunskill O
, O
2016 O
) O
. O

The O
scores O
across O
all O
settings O
drop O
when O
GA O
or O
SA O
is O
removed O
. O

In O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
, O
the O
training O
set O
without O
entity O
label O
of O
the O
target O
language O
is O
also O
available O
when O
training O
the O
model O
. O

[ O
M O
] O

Detailed O
performance O
. O
To O
further O
investigate O
the O
effectiveness O
of O
the O
proposed O
L2I B-MethodName
module O
, O
we O
perform O
a O
detailed O
comparison O
between O
TAGOP B-MethodName
- I-MethodName
L2I I-MethodName
and O
TAGOP B-MethodName
w.r.t O
. O

This O
is O
because O
both O
datasets O
have O
textual O
and O
tabular O
texts O
, O
where O
the O
ability O
of O
TAGOP B-MethodName
to O
perform O
discrete O
reasoning O
across O
hybrid O
contexts O
brings O
significant O
advantages O
. O

Though O
the O
suitable O
λ B-HyperparameterName
values O
may O
vary O
across O
datasets O
, O
we O
recommend O
considering O
the B-HyperparameterName
λ I-HyperparameterName
value O
1.5 B-HyperparameterValue
or O
2.0 B-HyperparameterValue
firstly O
in O
most O
cases O
. O

Comparison O
with O
UniLM B-MethodName
( O
Dong O
et O
al O
. O
, O
2019 O
) O
. O

( O
Narasimhan O
and O
Schwing O
, O
2018 O
; O
proposed O
memory O
- O
based O
methods O
that O
represent O
knowledge O
facts O
in O
the O
form O
of O
memory O
and O
calculate O
soft O
attention O
scores O
of O
the O
memory O
with O
a O
question O
. O

x O
4 O

We O
present O
the O
detailed O
content O
of O
the O
example O
in O
Section O
1 O
in O
table O
12.We O
present O
more O
examples O
of O
student O
models O
' O
outputs O
and O
cross O
attention O
visualization O
here O
. O

For O
text B-TaskName
generation I-TaskName
tasks O
, O
the O
given O
context O
constitutes O
the O
Part O
A O
of O
the O
input O
, O
with O
a O
mask O
token O
appended O
at O
the O
end O
. O

Then O
we O
compute O
the O
score O
of O
generating O
each O
answer O
candidate O
. O

p O
θ O
( O
s O
i O
|x O
corrupt O
, O
s O
z O
< O
i O
) O
= O
l O
i O
j=1 O
p(s O
i O
, O
j O
|x O
corrupt O
, O
s O
z O
< O
i O
, O
s O
i,<j O
) O
( O
2 O
) O

Position O
1 O
1 O
2 O
3 O
4 O
5 O
5 O
5 O
5 O
3 O
3 O
Position O
2 O
0 O
0 O
0 O
0 O
0 O
1 O
2 O
3 O
1 O
2 O

When O
we O
train O
our O
student O
model O
with O
pseudo O
labels O
, O
we O
still O
use O
a O
normal O
temperature O
( O
i.e. O
, O
τ O
= O
√ O
d O
) O
. O

The O
evaluation O
metrics O
are O
the O
F1 B-MetricName
scores O
of O
Rouge-1 B-MetricName
, O
Rouge-2 B-MetricName
, O
and O
Rouge B-MetricName
- I-MetricName
L I-MetricName
( O
Lin O
, O
2004 O
) O
on O
the O
test O
set O
. O

We O
apply O
our O
framework O
to O
the O
biomedical O
domain O
and O
verify O
its O
effectiveness O
by O
conducting O
experiments O
on O
several O
biomedical O
and O
clinical O
downstream O
tasks O
. O

Here O
, O
we O
set O
up O
the O
model O
as O
three B-HyperparameterValue
layers O
with O
adjacent O
and O
layer O
- O
wise O
weight O
tying O
. O

The O
weight B-HyperparameterName
decay I-HyperparameterName
is O
set O
to O
0.0001 B-HyperparameterValue
. O

CoNLL2002 B-DatasetName
includes O
Spanish O
and O
Dutch O
, O
CoNLL2003 B-DatasetName
includes O
English O
and O
German O
, O
and O
WikiAnn B-DatasetName
includes O
English O
and O
three O
non O
- O
western O
languages O
: O
Arabic O
, O
Hindi O
, O
and O
Chinese O
. O

For O
example O
, O
in O
the O
biomedical O
domain O
, O
several O
domainspecific O
PLMs O
trained O
with O
large O
biomedical O
texts O
, O
such O
as O
BioBERT B-MethodName
, O
PubMedBERT B-MethodName
( O
Gu O
et O
al O
. O
, O
2020 O
) O
and O
BlueBERT B-MethodName
( O
Peng O
et O
al O
. O
, O
2019 O
) O
, O
have O
been O
successfully O
used O
as O
strong O
baselines O
for O
several O
downstream O
tasks O
. O

He O
has O
appeared O
in O
music O
videos O
for O
the O
killers O
in O
1993 O
, O
the O
pretenders O
in O
1995 O
, O
and O
in O
the O
TV O
shows O
" O
the O
royal O
" O
and O
" O
the O
bill O
" O
. O

[ O
M O
] O

Each O
hypothetical O
question O
is O
related O
to O
one O
factual O
question O
from O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
, O
but O
each O
factual O
question O
in O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
is O
not O
guaranteed O
to O
have O
one O
hypothetical O
question O
. O

In O
other O
words O
, O
this O
implies O
our O
approach O
has O
a O
room O
for O
further O
improvement O
when O
a O
better O
in O
- O
domain O
model O
is O
set O
as O
a O
teacher O
. O

The O
three O
encoded O
representations O
are O
concatenate O
and O
are O
fed O
through O
a O
couple O
of O
feed O
- O
forward O
layers O
before O
making O
a O
bounded O
reward O
prediction O
R(s O
t O
, O
a O
t O
, O
g O
) O
∈ O
[ O
0 O
, O
1 O
] O
for O
each O
turn O
using O
a O
sigmoid O
function O
. O

We O
observe O
that O
the O
number B-HyperparameterName
of I-HyperparameterName
extracted I-HyperparameterName
knowledge I-HyperparameterName
facts O
increases O
when O
the O
number B-HyperparameterName
of I-HyperparameterName
graph I-HyperparameterName
walk I-HyperparameterName
increases O
, O
and O
unnecessary O
facts O
for O
answering O
a O
given O
question O
are O
usually O
included O
. O

To O
evaluate O
a O
pure O
reasoning O
ability O
of O
the O
models O
, O
we O
conduct O
experiments O
in O
the O
oracle O
setting O
. O

Compared O
to O
the O
teacher O
with O
normal O
attention B-HyperparameterName
temperature I-HyperparameterName
( O
pink O
bar O
) O
, O
teachers O
with O
higher O
attention B-HyperparameterName
temperatures I-HyperparameterName
( O
blue O
and O
green O
bars O
) O
attend O
less O
on O
the O
heading O
parts O
of O
documents O
while O
more O
on O
the O
tail O
parts O
of O
documents O
. O

There O
are O
204,045 O
articles O
for O
training O
; O
11,332 O
articles O
for O
validation O
; O
and O
11,334 O
articles O
for O
test O
. O

Our O
method O
aims O
to O
make O
these O
large O
models O
faster O
. O

Using O
a O
pair O
of O
dialogue O
rewards O
R(τ O
1 O
) O
and O
R(τ O
2 O
) O
, O
we O
compute O
the O
probabilistic O
preference O
between O
the O
roll O
- O
outs O
P O
[ O
τ O
1 O
≻ O
τ O
2 O
] O
either O
by O
standard O
normalization O
or O
a O
softmax O
function O
. O

The O
hyperparameters O
except O
Transformer O
architecture O
for O
GLM B-MethodName
410 I-MethodName
M I-MethodName
and O
GLM B-MethodName
515 I-MethodName
M I-MethodName
are O
the O
same O
as O
those O
of O
GLM B-MethodName
Large I-MethodName
. O

Comparison O
with O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

GLM B-MethodName
RoBERTa I-MethodName
can O
achieve O
performance O
matching O
the O
seq2seq B-MethodName
BART I-MethodName
model O
, O
and O
outperform O
T5 B-MethodName
and O
UniLMv2 B-MethodName
. O

The O
latency O
statistics O
( O
Milliseconds O
) O
and O
numbers O
of O
parameters O
of O
above O
four O
models O
are O
in O
Table O
1 O
. O

Third O
, O
encoder O
- O
decoder O
models O
are O
pretrained O
for O
sequence O
- O
to O
- O
sequence O
tasks O
( O
Song O
et O
al O
. O
, O
2019;Bi O
et O
al O
. O
, O
2020;Zhang O
et O
al O
. O
, O
2020 O
) O
. O

To O
construct O
the O
text B-TaskName
infilling I-TaskName
task O
, O
we O
randomly O
mask O
a O
given O
ratio B-HyperparameterName
r O
∈ O
{ O
10 B-HyperparameterValue
% I-HyperparameterValue
• O
• O
• O
50 B-HyperparameterValue
% I-HyperparameterValue
} O
of O
each O
document O
's O
tokens O
and O
the O
contiguous O
masked O
tokens O
are O
collapsed O
into O
a O
single O
blank O
. O

Results O
of O
T5 B-MethodName
Large I-MethodName
on O
XSum B-DatasetName
are O
obtained O
by O
running O
the O
summarization O
script O
provided O
by O
Huggingface B-HyperparameterValue
transformers I-HyperparameterValue
6 I-HyperparameterValue
. O

And O
β B-HyperparameterName
is O
set O
such O
that O
it O
is O
high O
when O
the O
output O
of O
the O
entity O
similarity O
teacher O
is O
close O
to O
0 O
or O
1 O
, O
and O
it O
is O
low O
when O
the O
output O
is O
close O
to O
0.5 O
. O

The O
performance O
is O
still O
weak O
due O
to O
the O
lack O
of O
annotations O
of O
target O
languages O
. O

Recent O
work O
for O
distillation O
of O
pre O
- O
trained O
Transformers O
( O
e.g. O
, O
DistilBERT B-MethodName
( O
Sanh O
et O
al O
. O
, O
2019 O
) O
, O
TinyBERT B-MethodName
( O
Jiao O
et O
al O
. O
, O
2020 O
) O
, O
Mobile B-MethodName
- I-MethodName
BERT I-MethodName
( O
Sun O
et O
al O
. O
, O
2020 O
) O
, O
BERT B-MethodName
- I-MethodName
of I-MethodName
- I-MethodName
Theseus I-MethodName
( O
Xu O
et O
al O
. O
, O
2020a O
) O
, O
MINILM B-MethodName
) O
focuses O
on O
natural O
language O
understanding O
tasks O
such O
as O
GLUE B-DatasetName
( O
Wang O
et O
al O
. O
, O
2018 O
) O
or O
SQuAD B-DatasetName
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
benchmarks O
. O

Most O
notably O
, O
the O
governor O
is O
president O
of O
the O
senate O
and O
governor O
. O

) O

To O
capture O
high O
- O
order O
semantics O
inherent O
in O
the O
knowledge O
sources O
, O
we O
adopt O
the O
concept O
of O
hypergraph O
. O

where O
M O
q O
, O
M O
k O
are O
a O
row O
- O
wise O
concatenated O
question O
words O
and O
knowledge O
entities O
, O
W O
[ O
• O
] O
is O
learn O
- O
Automatic O
transfer O
of O
text O
between O
domains O
has O
become O
popular O
in O
recent O
times O
. O

For O
instance O
, O
answering O
the O
question O
in O
Figure O
1 O
does O
not O
require O
the O
post O
- O
intervention O
value O
of O
total O
inventories O
in O
2019 O
. O

GLM B-MethodName
: O
He O
was O
a O
voice O
actor O
for O
the O
" O
X O
- O
Men O
" O
cartoon O
series O
. O

Then O
, O
we O
distilled O
for O
10 B-HyperparameterValue
epochs B-HyperparameterName
with B-HyperparameterName
learning I-HyperparameterName
rates I-HyperparameterName
of O
{ O
6e-6 B-HyperparameterValue
, O
8e-6 B-HyperparameterValue
, O
1e-5 B-HyperparameterValue
} O
. O

In O
conventional O
causal O
inference O
, O
such O
successors O
will O
also O
be O
omitted O
according O
to O
the O
local O
surgery O
principle O
( O
Pearl O
, O
2009 O
) O
. O

Under O
the O
wyoming O
state O
constitution O
, O
the O
governor O
can O
veto O
the O
actions O
of O
the O
other O
members O
of O
the O
wyoming O
house O
of O
representatives O
. O

First O
, O
XLNet B-MethodName
uses O
the O
original O
position O
encodings O
before O
corruption O
. O

When O
both O
data O
and O
code O
are O
provided O
, O
the O
number O
of O
potential O
causes O
of O
such O
differences O
is O
limited O
, O
and O
the O
NLP O
field O
has O
shared O
increasingly O
detailed O
information O
about O
system O
, O
dependencies O
and O
evaluation O
to O
chase O
down O
sources O
of O
differences O
. O

For O
all O
models O
above O
we O
apply O
a O
label B-HyperparameterName
smoothing I-HyperparameterName
of O
0.1 B-HyperparameterValue
to O
prevent O
overfitting O
( O
Pereyra O
et O
al O
. O
, O
2017 O
) O
. O

Our O
results O
on O
newstest2014 B-DatasetName
are O
shown O
in O
Table O
8 O
. O

We O
have O
two O
considerations O
for O
the O
module O
design O
: O
1 O
) O
the O
module O
should O
recognize O
the O
semantic O
connection O
between O
the O
assumption O
and O
the O
context O
, O
and O
2 O
) O
the O
module O
should O
uniformly O
support O
various O
discrete O
operations O
to O
enable O
accurate O
derivation O
. O

These O
methods O
mainly O
adopt O
an O
iterative O
message O
passing O
process O
to O
propagate O
information O
between O
adjacent O
nodes O
in O
the O
graph O
. O

An O
attention O
layers O
is O
used O
to O
attend O
the O
outputs O
of O
the O
seq2seq O
models O
with O
the O
context O
vector O
of O
previous O
turn O
for O
copy O
over O
mechanism O
. O

The O
first O
three O
student O
models O
are O
initialized O
from O
BART B-MethodName
weights O
( O
therefore O
, O
their O
hidden O
sizes O
are O
the O
same O
as O
that O
of O
BART B-MethodName
) O
. O

To O
draw O
inferences O
for O
these O
question O
categories O
, O
the O
model O
needs O
to O
attend O
to O
multiple O
knowledge O
facts O
related O
to O
a O
given O
question O
, O
and O
conducts O
multi O
- O
hop O
reasoning O
based O
on O
the O
facts O
. O

Although O
the O
above O
- O
mentioned O
models O
solve O
the O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
problem O
to O
some O
extent O
, O
the O
auxiliary O
tasks O
, O
as O
in O
multi O
- O
task O
learning O
, O
have O
not O
been O
studied O
in O
this O
problem O
. O

In O
this O
study O
, O
we O
selected O
the O
FinBERT B-MethodName
( O
Yang O
et O
al O
. O
, O
2020 O
) O
model O
as O
a O
teacher O
in O
the O
DoKTra B-MethodName
framework O
and O
evaluated O
our O
approach O
on O
two O
tasks O
, O
the O
Financial B-DatasetName
PhraseBank I-DatasetName
( O
FPB B-DatasetName
) O
and O
Fin B-DatasetName
- I-DatasetName
TextSen I-DatasetName
( O
FTS B-DatasetName
) O
. O

The O
RoBERTa B-MethodName
model O
that O
was O
applied O
to O
the O
proposed O
framework O
outperformed O
the O
teacher O
model O
on O
an O
average O
, O
specifically O
in O
four O
of O
five O
downstream O
tasks O
( O
ChemProt B-DatasetName
, O
DDI B-DatasetName
, O
i2b2 B-DatasetName
, O
and O
HoC B-DatasetName
) O
. O

These O
models O
have O
a O
similar O
motivation O
to O
the O
Hypergraph B-MethodName
Transformer I-MethodName
proposed O
in O
this O
paper O
, O
but O
core O
operations O
are O
vastly O
different O
. O

Parameter O
settings O
. O
We O
implement O
TAGOP B-MethodName
- I-MethodName
L2I I-MethodName
based O
on O
TAGOP B-MethodName
4 O
. O

[ O
S O
] O

Note O
that O
QA(• O
) O
measures O
the O
discrepancy O
between O
the O
ground O
- O
truth O
and O
the O
predicted O
answers O
which O
can O
have O
different O
formats O
. O

Smith O
's O
number O
at O
Michigan O
State O
was O
# O
7 O
in O
1969.The O
work O
is O
supported O
by O
the O
NSFC O
for O
Distinguished O
Young O
Scholar(61825602 O
) O
, O
and O
Beijing O
Academy O
of O
Artificial O
Intelligence O
( O
BAAI).To O
train O
GLM B-MethodName
Base I-MethodName
and O
GLM B-MethodName
Large I-MethodName
, O
we O
use O
Book B-DatasetName
- I-DatasetName
Corpus I-DatasetName
( O
Zhu O
et O
al O
. O
, O
2015 O
) O
and O
Wikipedia B-DatasetName
used O
by O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

Note O
that O
RoBERTa B-MethodName
- I-MethodName
PM I-MethodName
has O
an O
advantage O
in O
the O
i2b2 B-DatasetName
task O
since O
its O
pre O
- O
training O
data O
contains O
MIMIC B-DatasetName
- I-DatasetName
III I-DatasetName
clinical O
text O
data O
, O
while O
our O
teacher O
model O
was O
pretrained O
with O
only O
biomedical O
texts O
. O

The O
detailed O
equation O
of O
gated O
recurrent O
propagation O
is O
as O
follows O
: O
h O

Multi O
- O
hop O
graph O
walk O
connects O
multiple O
facts O
by O
setting O
the O
arrival O
node O
( O
tail O
) O
of O
the O
preceding O
walk O
as O
the O
starting O
( O
head O
) O
node O
of O
the O
next O
walk O
, O
thus O
, O
n O
- O
hop O
graph O
walk O
combines O
n O
facts O
as O
a O
hyperedge O
. O

The O
initially O
fine O
- O
tuned O
student O
models O
are O
in O
the O
second O
and O
fourth O
rows O
and O
the O
DoKTra B-MethodName
framework O
is O
applied O
to O
both O
, O
as O
shown O
in O
the O
third O
and O
fifth O
rows O
. O

For O
comparison O
with O
previous O
work O
, O
we O
use O
the O
same O
test O
set O
constructed O
by O
( O
Shen O
et O
al O
. O
, O
2020 O
) O
. O

We O
implement O
the O
sampling O
method O
in O
Edunov O
et O
al O
. O
( O
2018 O
) O
and O
Nucleus O
Sampling O
( O
Holtzman O
et O
al O
. O
, O
2019 O
) O
, O
a O
more O
advanced O
sampling O
method O
, O
to O
generate O
pseudo O
labels O
for O
distillation O
. O

Compared O
to O
BERT B-MethodName
with O
cloze O
- O
style O
finetuning O
, O
GLM B-MethodName
benefits O
from O
the O
autoregressive O
pretraining O
. O

( O
1 O
) O
GLM B-MethodName
consists O
of O
a O
single B-HyperparameterValue
encoder B-HyperparameterName
, O
( O
2 O
) O
GLM B-MethodName
shuffles O
the O
masked O
spans O
, O
and O
( O
3 O
) O
GLM B-MethodName
uses O
a O
single O
[ O
MASK O
] O
instead O
of O
multiple O
sentinel O
tokens O
. O

Autoencoding O
models O
, O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
learn O
bidirectional O
context O
encoders O
via O
denoising O
objectives O
, O
e.g. O
Masked O
Language O
Model O
( O
MLM O
) O
. O

To O
this O
end O
, O
we O
propose O
a O
two O
- O
step O
formulation O
of O
counterfactual O
thinking O
for O
HQA B-TaskName
to O
perform O
the O
identification O
and O
derivation O
. O

We O
also O
conduct O
experiments O
on O
fixing O
the O
parameter O
of O
PLM O
during O
training O
on O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
as O
initialized O
by O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
. O

After O
removing O
articles O
with O
summaries O
less O
than O
50 O
words O
, O
we O
obtain O
the O
final O
dataset O
with O
38,264 O
articles O
for O
training O
; O
4,002 O
articles O
for O
validation O
; O
and O
3,421 O
articles O
for O
test O
. O

The O
candidate O
labels O
y O
∈ O
Y O
are O
also O
mapped O
to O
answers O
to O
the O
cloze O
, O
called O
verbalizer O
v(y O
) O
. O

The O
details O
of O
two O
modules O
, O
guided O
- O
attention O
blocks O
and O
selfattention O
blocks O
, O
are O
described O
as O
below O
. O

While O
they O
succeed O
in O
long B-TaskName
- I-TaskName
text I-TaskName
generation I-TaskName
and O
show O
fewshot O
learning O
ability O
when O
scaled O
to O
billions O
of O
parameters O
( O
Radford O
et O
al O
. O
, O
2018b;Brown O
et O
al O
. O
, O
2020 O
) O
, O
the O
inherent O
disadvantage O
is O
the O
unidirectional O
attention O
mechanism O
, O
which O
can O
not O
fully O
capture O
the O
dependencies O
between O
the O
context O
words O
in O
NLU B-TaskName
tasks O
. O

The O
dataset O
has O
10438 O
dialogues O
split O
into O
8438 O
dialogues O
for O
training O
set O
and O
1000 O
dialogues O
each O
for O
validation O
and O
test O
set O
. O

In O
addition O
to O
the O
triplet O
- O
based O
graph O
walks O
, O
a O
multihop O
graph O
walk O
is O
proposed O
to O
encode O
multiple O
relational O
facts O
that O
are O
interconnected O
. O

pervised O
manner O
, O
and O
then O
fine O
- O
tuned O
with O
a O
small O
dataset O
for O
several O
downstream O
tasks O
. O

Figure O
1 O
illustrates O
one O
such O
example O
, O
where O
a O
sentence O
from O
the O
BOOKS O
domain O
is O
translated O
to O
the O
MOVIE O
domain O
. O

More O
details O
on O
contrasting O
the O
merits O
and O
limitations O
of O
these O
methods O
can O
be O
found O
in O
Sec O
: O
A.1 O

We O
use O
the O
BART B-MethodName
12 I-MethodName
- I-MethodName
6 I-MethodName
as O
the O
student O
model O
, O
and O
the O
distillation O
results O
on O
CNNDM B-DatasetName
are O
in O
Table O
5 O
. O

Additionally O
, O
we O
also O
measure O
the O
quality O
of O
generated O
summaries O
by O
eliciting O
human O
judgements O
. O

Besides O
, O
we O
want O
to O
select O
baselines O
that O
are O
effective O
for O
learning O
counterfactual O
samples O
. O

[ O
S O
] O

We O
construct O
a O
question O
hypergraph O
and O
a O
knowledge O
hypergraph O
to O
explicitly O
encode O
high O
- O
order O
semantics O
present O
in O
the O
question O
and O
each O
knowledge O
fact O
, O
and O
capture O
multi O
- O
hop O
relational O
knowledge O
facts O
effectively O
. O

However O
, O
because O
the O
hidden B-HyperparameterName
embedding I-HyperparameterName
dimensions I-HyperparameterName
of O
teachers O
and O
students O
are O
different O
in O
our O
setting O
, O
we O
applied O
a O
linear O
transformation O
to O
the O
teacher O
's O
classification O
embedding O
to O
match O
the O
dimension O
with O
the O
student O
model O
. O

We O
set O
10 O
% O
of O
the O
entire O
data O
as O
the O
test O
set O
, O
which O
is O
similar O
to O
FPB B-DatasetName
. O

Blank B-MethodName
Language I-MethodName
Modeling I-MethodName
. O

Within O
one O
training O
batch O
, O
we O
sample O
short O
spans O
and O
longer O
spans O
( O
document O
- O
level O
or O
sentence O
- O
level O
) O
with O
equal O
chances O
. O

The O
warmup B-HyperparameterName
step I-HyperparameterName
we O
use O
is O
4000 B-HyperparameterValue
. O

There O
is O
no O
standard O
way O
of O
going O
about O
a O
reproduction B-TaskName
study O
in O
NLP O
, O
and O
different O
reproduction B-TaskName
studies O
of O
the O
same O
original O
set O
of O
results O
can O
differ O
substantially O
in O
terms O
of O
their O
similarity O
in O
system O
and/or O
evaluation O
design O
( O
as O
is O
the O
case O
with O
the O
Vajjala O
and O
Rama O
( O
2018 O
) O
reproductions O
, O
see O
Section O
4 O
for O
details O
) O
. O

More O
results O
with O
different O
T B-HyperparameterName
s O
are O
in O
Appendix O
C.Why O
does O
our O
distillation O
method O
work O
? O
To O
answer O
this O
question O
, O
we O
first O
try O
to O
analyze O
the O
reasons O
from O
both O
the O
external O
characteristics O
of O
the O
summaries O
generated O
by O
the O
teacher O
model O
and O
the O
internal O
characteristics O
of O
the O
teacher O
's O
attention O
mechanism O
. O

To O
facilitate O
the O
evaluation O
of B-TaskName
HQA I-TaskName
and O
diagnose O
counterfactual O
thinking O
, O
we O
construct O
an O
HQA B-TaskName
dataset O
based O
on O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
( O
Zhu O
et O
al O
. O
, O
2021 O
) O
, O
which O
is O
a O
QA B-TaskName
dataset O
with O
a O
mix O
of O
tabular O
and O
textual O
context O
extracted O
from O
financial O
reports O
. O

Training O
and O
inference O
Hyper O
- O
parameters O
for O
BART B-MethodName
, O
BART B-MethodName
12 I-MethodName
- I-MethodName
6 I-MethodName
, O
BART B-MethodName
12 I-MethodName
- I-MethodName
3 I-MethodName
, O
and O
BART B-MethodName
12 I-MethodName
- I-MethodName
12 I-MethodName
are O
similar O
. O

The O
test O
set O
is O
constructed O
by O
including O
the O
9,076 O
articles O
published O
after O
January O
1 O
, O
2007 O
. O

It O
derives O
the O
intervention O
result O
for O
the O
target O
fact O
. O

Eq O
1 O
) O
. O

Position O
1 O
1 O
2 O
3 O
4 O
5 O
5 O
5 O
5 O
3 O
3 O
Position O
2 O
0 O
0 O
0 O
0 O
0 O
1 O
2 O
3 O
1 O
2 O

Here O
, O
we O
use O
four O
attention O
heads O
as O
multi O
- O
head O
. O

The O
knowledge O
and O
question O
graph O
are O
encoded O
separately O
by O
two O
graph B-MethodName
convolutional I-MethodName
networks I-MethodName
( O
GCN B-MethodName
) O
( O
Kipf O
and O
Welling O
, O
2017 O
) O
. O

We O
reformulate O
the O
classification B-TaskName
tasks O
as O
blank B-TaskName
infilling I-TaskName
with O
human O
- O
crafted O
cloze B-TaskName
questions I-TaskName
, O
following O
PET O
( O
Schick O
and O
Schütze O
, O
2020b O
) O
. O

Language O
models O
pretrained O
on O
unlabeled O
texts O
have O
substantially O
advanced O
the O
state O
of O
the O
art O
in O
various O
NLP O
tasks O
, O
ranging O
from O
natural B-TaskName
language I-TaskName
understanding I-TaskName
( B-TaskName
NLU I-TaskName
) O
to O
text B-TaskName
generation I-TaskName
( O
Radford O
et O
al O
. O
, O
2018a;Devlin O
et O
al O
. O
, O
2019;Yang O
et O
al O
. O
, O
2019;Radford O
et O
al O
. O
, O
2018b;Raffel O
et O
al O
. O
, O
2020;Brown O
et O
al O
. O
, O
2020 O
) O
. O

Terry O
grew O
up O
in O
Surrey O
, O
England O
and O
attended O
the O
University O
of O
Sussex O
in O
the O
United O
Kingdom O
, O
graduating O
with O
a O
degree O
in O
english O
literature O
. O

We O
formulate O
it O
as O
: O
c O
= O
g(c O
, O
a O
) O
, O
where O
the O
counterfactual O
context O
c O
is O
the O
status O
of O
the O
context O
c O
after O
the O
assumption O
a O
is O
executed O
. O

We O
try O
to O
bring O
up O
insights O
on O
why O
the O
proposed O
multiple B-MethodName
- I-MethodName
task I-MethodName
and I-MethodName
multiple I-MethodName
- I-MethodName
teacher I-MethodName
model I-MethodName
works O
. O

In O
addition O
, O
we O
refine O
a O
list O
of O
de O
- O
tected O
named O
entities O
by O
matching O
the O
associated O
image O
caption O
( O
i.e. O
, O
Wikipedia O
caption O
) O
. O

However O
, O
it O
does O
not O
explicitly O
maintain O
other O
attributes O
between O
the O
source O
and O
translated O
text O
, O
for O
e.g. O
, O
text O
length O
and O
descriptiveness O
. O

The O
proposed O
model O
, O
Hypergraph B-MethodName
Transformer I-MethodName
, O
constructs O
a O
question O
hypergraph O
and O
a O
query O
- O
aware O
knowledge O
hypergraph O
, O
and O
infers O
an O
answer O
by O
encoding O
inter O
- O
associations O
between O
two O
hypergraphs O
and O
intra O
- O
associations O
in O
both O
hypergraph O
itself O
. O

Simpsons O
" O
as O
the O
character O
captain O
Billy O
Higgledypig O
, O
but O
his O
character O
was O
only O
a O
one O
- O
time O
recurring O
character O
in O
the O
series O
' O
first O
six O
seasons O
. O

GLM B-MethodName
is O
trained O
by O
optimizing O
an O
autoregressive O
blank O
infilling O
objective O
. O

As O
shown O
in O
Table O
8 O
of O
Appendix O
D O
, O
Hypergraph B-MethodName
Transformer I-MethodName
shows O
comparable O
performance O
in O
both O
top-1 O
and O
top-3 O
accuracy B-MetricName
in O
comparison O
with O
the O
state O
- O
of O
- O
the O
- O
art O
methods O
. O

Recently O
, O
the O
pre O
- O
trained O
multilingual O
language O
model O
is O
effective O
to O
address O
the O
challenge O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

In O
calibrated O
teacher O
training O
, O
we O
first O
select O
the O
number B-HyperparameterName
of I-HyperparameterName
epochs I-HyperparameterName
and O
the O
learning B-HyperparameterName
rate I-HyperparameterName
as O
the O
default O
values O
of O
the O
BioBERT B-MethodName
code O
and O
slightly O
change O
the O
number B-HyperparameterName
of I-HyperparameterName
epochs I-HyperparameterName
( O
e B-HyperparameterName
) O
for O
the O
unreported O
tasks O
from O
BioBERT B-MethodName
. O

Usually O
, O
the O
assumption O
appears O
either O
before O
of O
after O
the O
factual O
question O
. O

We O
propose O
Learning O
to O
Imagine O
, O
where O
the O
counterfactual O
thinking O
is O
implemented O
with O
two O
intervening O
steps O
: O
1 O
) O
identifying O
the O
facts O
to O
intervene O
, O
and O
2 O
) O
deriving O
the O
result O
of O
intervention O
. O

As O
both O
TAPT B-MethodName
and O
DoK B-MethodName
- I-MethodName
Tra I-MethodName
only O
utilize O
the O
task O
- O
specific O
training O
data O
, O
they O
can O
be O
fairly O
compared O
in O
terms O
of O
performance O
and O
training O
resources O
. O

Specifically O
, O
all O
models O
are O
optimized O
using O
Adam O
( O
Kingma O
and O
with O
β B-HyperparameterName
1 I-HyperparameterName
= O
0.9 B-HyperparameterValue
, O
β B-HyperparameterName
2 I-HyperparameterName
= O
0.999 B-HyperparameterValue
. O

The O
benchmark O
is O
usually O
considered O
as O
less O
challenging O
than O
Super B-DatasetName
- I-DatasetName
GLUE I-DatasetName
. O

x O
3 O

However O
, O
models O
built O
using O
unsupervised O
methods O
perform O
poorly O
when O
compared O
to O
supervised O
( O
parallel O
) O
training O
( O
Artetxe O
et O
al O
. O
, O
2020 O
) O
. O

Our O
model O
shows O
notable O
strengths O
especially O
on O
complex O
problems O
such O
as O
Comparison O
, O
Multi O
- O
entity O
or O
Subtraction O
. O

He O
later O
appeared O
as O
a O
regular O
for O
the O
show O
's O
final O
six O
seasons O
, O
and O
has O
been O
a O
frequent O
guest O
in O
the O
show O
since O
. O

MinTL B-MethodName
- I-MethodName
BART I-MethodName
( O
Lin O
et O
al O
. O
, O
2020 O
) O
, O
introduced O
Levenshtein O
belief O
spans O
framework O
that O
predicts O
only O
the O
incremental O
change O
in O
dialogue O
state O
per O
turn O
. O

To O
verify O
the O
general O
applicability O
of O
our O
approach O
, O
we O
conducted O
experiments O
on O
financial B-TaskName
sentiment I-TaskName
classification I-TaskName
tasks O
. O

To O
answer O
this O
question O
for O
a O
given O
specific O
system O
, O
typically O
( O
Wieling O
et O
al O
. O
, O
2018;Arhiliuc O
et O
al O
. O
, O
2020;Popović O
and O
Belz O
, O
2021 O
) O
an O
original O
study O
is O
selected O
and O
repeated O
more O
or O
less O
closely O
, O
before O
comparing O
the O
results O
obtained O
in O
the O
original O
study O
with O
those O
obtained O
in O
the O
repeat O
, O
and O
deciding O
whether O
the O
two O
sets O
of O
results O
are O
similar O
enough O
to O
support O
the O
same O
conclusions O
. O

Multi O
- O
iteration O
derivation O
. O

Due O
to O
the O
size O
limit O
of O
supplementary O
material O
, O
we O
can O
not O
include O
the O
pretrained O
models O
, O
but O
will O
make O
them O
public O
available O
in O
the O
future O
. O

To O
answer O
the O
given O
question O
as O
shown O
in O
Figure O
1 O
, O
a O
model O
should O
understand O
the O
semantics O
of O
the O
given O
question O
, O
link O
visual O
entities O
appearing O
in O
the O
given O
image O
to O
the O
KB O
, O
extract O
a O
number O
of O
evidences O
from O
the O
KB O
and O
predict O
an O
answer O
by O
aggregating O
semantics O
of O
both O
the O
question O
and O
the O
extracted O
evidences O
. O

Here O
the O
user O
has O
requested O
for O
a O
taxi O
, O
before O
enough O
information O
such O
as O
destination O
or O
time O
of O
departure O
are O
gathered O
, O
the O
agent O
books O
the O
taxi O
. O

In O
the O
summarization B-TaskName
task O
, O
we O
observe O
that O
1 O
) O
pseudo O
summaries O
generated O
from O
our O
teacher O
model O
copy O
more O
continuous O
text O
spans O
from O
original O
documents O
than O
reference O
summaries O
( O
56 B-MetricValue
% I-MetricValue
4 B-MetricName
- I-MetricName
grams I-MetricName
in O
pseudo O
summaries O
and O
15 B-MetricValue
% I-MetricValue
4 B-MetricName
- I-MetricName
grams I-MetricName
in O
reference O
summaries O
are O
copied O
from O
their O
original O
documents O
on O
CNN B-DatasetName
/ I-DatasetName
DailyMail I-DatasetName
dataset O
) O
; O
2 O
) O
pseudo O
summaries O
tend O
to O
summarize O
the O
leading O
part O
of O
a O
document O
( O
measured O
on O
CNN B-DatasetName
/ I-DatasetName
DailyMail I-DatasetName
, O
74 O
% O
of O
sentences O
in O
pseudo O
summaries O
and O
64 O
% O
of O
sentences O
in O
reference O
summaries O
are O
from O
the O
leading O
40 O
% O
sentences O
in O
original O
documents O
) O
. O

Existing O
models O
can O
be O
separated O
into O
three O
categories O
, O
shared O
feature O
space O
based O
, O
translation O
based O
and O
knowledge O
distillation O
based O
. O

The O
key O
to O
achieving O
counterfactual O
thinking O
in O
NDR B-TaskName
lies O
in O
: O
1 O
) O
parsing O
the O
assumption O
to O
identify O
the O
target O
fact O
to O
intervene O
; O
and O
2 O
) O
deriving O
the O
assumed O
value O
to O
construct O
the O
counterfactual O
context O
. O

BART B-MethodName
is O
trained O
using O
cross O
- O
entropy O
loss O
between O
the O
decoder O
output O
and O
the O
original O
document O
. O

However O
, O
there O
is O
a O
dilemma O
to O
adapt O
the O
siamese B-MethodName
network I-MethodName
to O
tokenlevel B-TaskName
recognition I-TaskName
tasks O
such O
as O
NER B-TaskName
. O

We O
pre O
- O
process O
every O
classification O
dataset O
except O
for O
GAD B-DatasetName
in O
the O
same O
manner O
as O
the O
BLUE B-MethodName
( O
Peng O
et O
al O
. O
, O
2019 O
) O
benchmark O
. O

Part O
A O
tokens O
can O
attend O
to O
each O
other O
, O
but O
can O
not O
attend O
to O
any O
tokens O
in O
B. O
Part O
B O
tokens O
can O
attend O
to O
Part O
A O
and O
antecedents O
in O
B O
, O
but O
can O
not O
attend O
to O
any O
subsequent O
tokens O
in O
B. O
To O
enable O
autoregressive O
generation O
, O
each O
span O
is O
padded O
with O
special O
tokens O
[ O
START O
] O
and O
[ O
END O
] O
, O
for O
input O
and O
Token O

Different O
from O
KVQA B-DatasetName
focusing O
on O
world O
knowledge O
for O
named O
entities O
, O
FVQA B-DatasetName
considers O
commonsense O
knowledge O
about O
common O
nouns O
in O
a O
given O
image O
. O

We O
conduct O
experiments O
on O
Fact B-DatasetName
- I-DatasetName
based I-DatasetName
Visual I-DatasetName
Question I-DatasetName
Answering I-DatasetName
( O
FVQA B-DatasetName
) O
as O
an O
additional O
benchmark O
dataset O
for O
knowledge B-TaskName
- I-TaskName
based I-TaskName
VQA I-TaskName
. O

In O
the O
teacher O
training O
model O
, O
there O
are O
two O
sub O
- O
models O
, O
i.e. O
an O
entity O
recognizer O
teacher O
and O
a O
similarity O
evaluator O
teacher O
. O

While O
the O
translated O
sentence O
" O
Loved O
the O
movie O
" O
has O
correctly O
transferred O
the O
attribute O
( O
style O
) O
, O
it O
does O
not O
have O
the O
same O
length O
, O
does O
not O
retain O
the O
personal O
noun O
( O
" O
I O
" O
) O
, O
nor O
use O
a O
domain O
- O
appropriate O
proper O
noun O
. O

[ O
M O
] O

The O
model O
generates O
the O
text O
of O
Part O
B O
autoregressively O
. O

Perplexity B-MetricName
is O
an O
evaluation O
criterion O
that O
has O
been O
changes O
to O
the O
senate O
. O

Formally O
, O
let O
Z O
m O
be O
the O
set O
of O
all O
possible O
permutations O
of O
the O
length O
- O
m O
index O
sequence O
[ O
1 O
, O
2 O
, O
• O
• O
• O
, O
m O
] O
, O
and O
s O
z O
< O
i O
be O
[ O
s O
z O
1 O
, O
• O
• O
• O
, O
s O
z O
i−1 O
] O
, O
we O
define O
the O
pretraining O
objective O
as O

Use O
of O
under O
- O
specified O
reward O
will O
often O
lead O
to O
policy O
that O
suffers O
from O
high O
variance O
( O
Agarwal O
et O
al O
. O
, O
2019 O
) O
. O

Note O
that O
a O
weighting O
strategy O
is O
also O
provide O
therein O
to O
take O
into O
consideration O
of O
the O
reliability O
of O
the O
teachers O
. O

However O
, O
both O
models O
require O
more O
parameters O
to O
outperform O
autoencoding O
models O
such O
as O
RoBERTa B-MethodName
. O

Then O
, O
we O
model O
the O
derivation O
as O
making O
a O
choice O
across O
the O
operators O
and O
tagging O
the O
premise O
for O
executing O
the O
operator O
. O

In O
comparison O
, O
Transformer B-MethodName
( I-MethodName
SA+GA I-MethodName
) I-MethodName
strongly O
attends O
to O
the O
knowledge O
entities O
which O
appear O
repetitive O
in O
the O
knowledge O
facts O
. O

Offline O
task B-TaskName
- I-TaskName
oriented I-TaskName
dialogue I-TaskName
( O
ToD B-TaskName
) O
systems O
involves O
solving O
disparate O
tasks O
of O
belief O
states O
tracking O
, O
dialogue O
policy O
management O
, O
and O
response O
generation O
. O

In O
sentiment B-TaskName
classification I-TaskName
, O
the O
labels O
" O
positive O
" O
and O
" O
negative O
" O
are O
mapped O
to O
the O
words O
" O
good O
" O
and O
" O
bad O
" O
. O

Neural B-TaskName
discrete I-TaskName
reasoning I-TaskName
( O
Dua O
et O
al O
. O
, O
2019 O
) O
is O
an O
emerging O
technique O
for O
machine O
reading O
comprehension O
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
which O
aims O
at O
answering O
numerical O
questions O
from O
textual O
( O
Dua O
et O
al O
. O
, O
2019 O
) O
or O
hybrid O
( O
Zhu O
et O
al O
. O
, O
2021 O
) O
context O
1 O
. O

CASPI(MinTL B-MethodName
) I-MethodName
with O
its O
robust O
pretrained O
model O
out O
performs O
CASPI(DAMD B-MethodName
) I-MethodName
and O
LAVA B-MethodName
( O
Lubis O
et O
al O
. O
, O
2020 O
) O
by O
a O
large O
margin O
. O

The O
improvement O
indicates O
that O
learning O
the O
relationship O
between O
the O
factual O
and O
counterfactual O
samples O
with O
CLO O
provides O
some O
clue O
for O
counterfactual O
imagination O
, O
yet O
it O
is O
still O
worse O
than O
directly O
learning O
to O
imagine O
with O
neural O
network O
modules O
. O

Our O
best O
performing O
student O
model O
PLATE B-MethodName
B12 I-MethodName
- I-MethodName
3 I-MethodName
λ=1.5 I-MethodName
outperforms O
BART B-MethodName
- I-MethodName
PL I-MethodName
, O
BART B-MethodName
- I-MethodName
SFT I-MethodName
, O
and O
BART B-MethodName
- I-MethodName
KD I-MethodName
on O
XSum B-DatasetName
. O

In O
this O
study O
, O
we O
proposed O
the O
DoKTra B-MethodName
framework O
as O
a O
domain O
knowledge O
transfer O
method O
for O
PLMs O
. O

Without O
the O
similarity O
knowledge O
fed O
into O
the O
student O
model O
, O
the O
performance O
drops O
significantly O
. O

Conciseness O
and O
abstractiveness O
are O
good O
properties O
for O
summarization B-TaskName
, O
which O
however O
may O
not O
be O
the O
case O
for O
other O
generation O
tasks O
such O
as O
machine B-TaskName
translation I-TaskName
. O

We O
present O
the O
attention O
map O
from O
the O
guided O
- O
attention O
block O
, O
and O
visualize O
top O
- O
k O
attended O
knowledge O
facts O
or O
entities O
with O
the O
attention O
scores O
. O

We O
follow O
the O
experimental O
settings O
suggested O
in O
. O

All O
the O
models O
are O
evaluated O
in O
the O
zero O
- O
shot O
setting O
. O

The O
conditional O
probability O
of O
predicting O
y O
given O
x O
is O

Thus O
, O
Hypergraph B-MethodName
Transformer I-MethodName
can O
mitigate O
the O
well O
- O
known O
over O
- O
smoothing O
problem O
in O
the O
previous O
graph O
- O
based O
methods O
exploiting O
the O
message O
passing O
scheme O
. O

To O
the O
best O
of O
our O
knowledge O
, O
we O
are O
the O
first O
to O
learn O
the O
entity O
similarity O
by O
siamese B-MethodName
network I-MethodName
. O

TAPAS B-MethodName
- I-MethodName
WTQ I-MethodName
( O
Herzig O
et O
al O
. O
, O
2020 O
) O
, O
a O
tabular B-TaskName
QA I-TaskName
method O
that O
focuses O
on O
parsing O
and O
understanding O
tables O
, O
pre O
- O
trained O
over O
tables O
collected O
from O
Wikipedia O
before O
training O
on O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
. B-MethodName
HyBrider I-MethodName
( O
Chen O
et O
al O
. O
, O
2020c O
) O
, O
a O
hybrid B-TaskName
QA I-TaskName
method O
that O
considers O
the O
connection O
between O
the O
table O
and O
text O
. O

To O
analyze O
the O
performances O
of O
the O
variants O
in O
our O
model O
, O
we O
use O
KVQA B-DatasetName
which O
is O
a O
representative O
and O
large O
- O
scale O
dataset O
for O
knowledge B-TaskName
- I-TaskName
based I-TaskName
VQA I-TaskName
. O

Learning B-HyperparameterName
rates I-HyperparameterName
are O
picked O
from O
1e-4 B-HyperparameterValue
, O
3e-4 B-HyperparameterValue
, O
5e-4 B-HyperparameterValue
, O
7e-4 B-HyperparameterValue
accord O
- O
ing O
to O
validation O
sets O
. O

Also O
, O
M O
denotes O
a O
mixture O
of O
the O
2H O
and O
3H O
questions O
. O

Based O
on O
the O
two O
experiments O
, O
we O
identify O
that O
not O
only O
the O
guided O
- O
attention O
which O
captures O
inter O
- O
relationships O
between O
question O
and O
knowledge O
but O
also O
the O
selfattention O
which O
learns O
intra O
- O
relationship O
in O
them O
are O
crucial O
to O
the O
complex B-TaskName
QA I-TaskName
. O

We O
uses O
three O
evaluations O
metrics O
proposed O
by O
( O
Budzianowski O
et O
al O
. O
, O
2018a O
) O
. O

x O
4 O

MinTL B-MethodName
uses O
a O
large O
pretrained O
language O
model O
BART B-MethodName
( O
Lewis O
et O
al O
. O
, O
2019 O
) O
. O

Each O
GGNN B-MethodName
model O
consists O
of O
three B-HyperparameterValue
gated B-HyperparameterName
recurrent I-HyperparameterName
propagation I-HyperparameterName
layers I-HyperparameterName
and O
a O
graphlevel O
aggregator O
. O

The O
multi O
- O
hop O
graph O
walk O
is O
conducted O
in O
the O
same O
manner O
as O
the O
knowledge O
hypergraph O
. O

Especially O
, O
when O
we O
convert O
the O
one O
of O
both O
hyperedge B-HyperparameterValue
- I-HyperparameterValue
level I-HyperparameterValue
representation B-HyperparameterName
to O
single B-HyperparameterValue
- I-HyperparameterValue
word I-HyperparameterValue
- I-HyperparameterValue
unit I-HyperparameterValue
- I-HyperparameterValue
based I-HyperparameterValue
representation B-HyperparameterName
, O
the O
mean B-MetricName
accuracy I-MetricName
of O
QA B-TaskName
is O
82.7 B-MetricValue
% I-MetricValue
and O
88.7 B-MetricValue
% I-MetricValue
, O
respectively O
. O

To O
validate O
the O
impact O
of O
similarity O
- O
based O
answer O
selector O
, O
we O
replace O
the O
similarity O
- O
based O
answer O
selector O
( O
SIM O
) O
with O
a O
multi O
- O
layer O
perceptron O
( O
MLP O
) O
. O

Row O
6 O
shows O
that O
removing O
the O
span O
shuffling O
( O
always O
predicting O
the O
masked O
spans O
from O
left O
to O
right O
) O
leads O
to O
a O
severe O
performance O
drop O
on O
SuperGLUE B-DatasetName
. O

Encoder O
- O
decoder O
models O
adopt O
bidirectional O
attention O
for O
the O
encoder O
, O
unidirectional O
attention O
for O
the O
decoder O
, O
and O
cross O
attention O
between O
them O
( O
Song O
et O
al O
. O
, O
2019;Bi O
et O
al O
. O
, O
2020 O
; O
. O

Second O
, O
autoregressive O
models O
are O
trained O
with O
a O
left O
- O
to O
- O
right O
language O
modeling O
objective O
( O
Radford O
et O
al O
. O
, O
2018a O
, O
b;Brown O
et O
al O
. O
, O
2020 O
) O
. O

Instead O
, O
GLM B-MethodName
unifies O
NLU B-TaskName
and O
generation O
tasks O
with O
autoregressive O
pretraining O
. O

Large O
pre O
- O
trained O
Seq2Seq O
Transformer O
models O
largely O
improve O
results O
of O
generation O
tasks O
including O
text B-TaskName
summarization I-TaskName
( O
Song O
et O
al O
. O
, O
2019;Lewis O
et O
al O
. O
, O
2020;Raffel O
et O
al O
. O
, O
2020 O
; O
Token O
index O
in O
summary O
Zhang O
et O
al O
. O
, O
2020 O
) O
. O

The O
hyperparameters O
for O
GLM B-MethodName
Base I-MethodName
and O
GLM B-MethodName
Large I-MethodName
are O
similar O
to O
those O
used O
by O
BERT B-MethodName
. O

Table O
2 O
) O
. O

We O
consider O
that O
the O
reason O
why O
Hypergraph B-MethodName
Transformer I-MethodName
failed O
to O
infer O
the O
correct O
answer O
despite O
focusing O
on O
the O
exact O
knowledge O
fact O
is O
that O
the O
correct O
answer O
word O
( O
Myocardial O
Infarction O
) O
appears O
rarely O
in O
QA O
pairs O
. O

Mehri O
et O
al O
. O
( O
2019 O
) O
uses O
supervised O
learning O
to O
bootstrap O
followed O
by O
RL O
fine O
tuning O
, O
whereas O
Zhao O
et O
al O
. O
( O
2019 O
) O
uses O
policy O
gradient O
on O
latent O
action O
space O
as O
against O
handcrafted O
ones O
. O

The O
output O
of O
this O
optimized O
using O
binary O
crossentopy O
loss O
described O
in O
Eqn:4 O
. O

Most O
methods O
above O
are O
designed O
for O
classification O
models O
. O

Motivated O
by O
Gated O
Recurrent O
Units O
( O
Cho O
et O
al O
. O
, O
2014 O
) O
, O
GGNN B-MethodName
adopts O
a O
update O
gate O
and O
a O
reset O
gate O
to O
renew O
each O
node O
's O
hidden O
state O
. O

All O
transformer O
variant O
models O
described O
in O
this O
paper O
have O
the O
same O
fixed O
- O
number B-HyperparameterName
of I-HyperparameterName
sequence I-HyperparameterName
length I-HyperparameterName
as O
follows O
: O
300 B-HyperparameterValue
for O
1 O
- O
hop O
, O
1,000 B-HyperparameterValue
for O
2 O
- O
hop O
and O
1,800 B-HyperparameterValue
for O
3 O
- O
hop O
graphs O
. O

All O
experiments O
were O
repeated O
three B-HyperparameterValue
times O
with O
different O
random O
seeds O
, O
and O
the O
average O
performances O
and O
standard O
deviations O
have O
been O
reported O
. O

The O
most O
similar O
answer O
to O
the O
joint O
representation O
is O
selected O
as O
an O
answer O
among O
the O
answer O
candidates O
. O

BART B-MethodName
( O
Lewis O
et O
al O
. O
, O
2020 O
) O
employs O
denoising O
auto O
- O
encoding O
objectives O
such O
as O
text O
infilling O
and O
sentence O
permutation O
during O
its O
pre O
- O
training O
. O

GLM B-MethodName
differs O
in O
three O
aspects O
: O

Under O
this O
setting O
, O
GLM B-MethodName
410 I-MethodName
M I-MethodName
outperforms O
GPT B-MethodName
Large I-MethodName
. O

Our O
model O
responds O
by O
focusing O
on O
{ O
second O
⪯ O
from O
⪯ O
left O
} O
phrase O
of O
the O
question O
and O
four O
facts O
having O
a O
left O
relation O
among O
86 O
knowledge O
hyperedges O
. O

We O
suspect O
the O
reason O
is O
that O
the O
operation O
of O
SWAP O
MIN O
NUM O
is O
very O
close O
to O
SWAP O
, O
which O
may O
confuse O
the O
deriving O
head O
when O
making O
classification O
over O
the O
operators O
. O

He O
was O
also O
a O
kick O
and O
punt O
returner O
. O

Answering O
complex O
questions O
that O
require O
multi O
- O
hop O
reasoning O
under O
weak O
supervision O
is O
considered O
as O
a O
challenging O
problem O
since O
i O
) O
no O
supervision O
is O
given O
to O
the O
reasoning O
process O
and O
ii O
) O
highorder O
semantics O
of O
multi O
- O
hop O
knowledge O
facts O
need O
to O
be O
captured O
. O

By O
passing O
the O
guided O
- O
attention O
blocks O
and O
selfattention O
blocks O
sequentially O
, O
representations O
of O
knowledge O
hyperedges O
and O
question O
hyperedges O
are O
updated O
and O
finally O
aggregated O
to O
single O
vector O
representation O
as O
z O
k O
∈ O
R O
dv B-HyperparameterName
and O
z O
q O
∈ O
R O
dv B-HyperparameterName
, O
respectively O
. O

The O
result O
in O
Figure O
4 O
shows O
that O
training O
on O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
causes O
a O
performance O
drop O
in O
counting O
, O
span O
and O
multi O
- O
span O
groups O
of O
TAT B-DatasetName
- I-DatasetName
QA I-DatasetName
, O
and O
performs O
similar O
on O
the O
in O
arithmetic O
group O
. O

It O
can O
be O
seen O
that O
our O
model O
outperforms O
the O
state O
- O
of O
- O
the O
- O
arts O
. O

To O
demonstrate O
the O
versatility O
of O
our O
method O
to O
adapt O
to O
different O
metrics O
, O
we O
use O
all O
the O
discussed O
variants O
of O
the O
metric O
. O

their O
ROUGE B-MetricName
scores O
on O
validation O
sets O
. O

In O
knowledge O
distillation O
, O
besides O
learning O
from O
gold O
labels O
in O
the O
training O
set O
, O
student O
models O
can O
learn O
from O
soft O
targets O
( O
Ba O
and O
Caruana O
, O
2014;Hinton O
et O
al O
. O
, O
2015 O
) O
, O
intermediate O
hidden O
states O
( O
Romero O
et O
al O
. O
, O
2014 O
) O
, O
attentions O
( O
Zagoruyko O
and O
Komodakis O
, O
2017 O
; O
, O
and O
target O
output O
derivatives O
( O
Czarnecki O
et O
al O
. O
, O
2017 O
) O
of O
teacher O
models O
. O

To O
this O
end O
we O
choose O
two O
ToD B-TaskName
methods O
that O
are O
at O
the O
extremes O
of O
model O
architecture O
spectrum O
1 O
) O
One O
uses O
a O
light O
weight O
custom O
model O
and O
2 O
) O
Other O
uses O
a O
large O
standard O
pre O
- O
trained O
out O
- O
of O
- O
the O
box O
universal O
language O
model O
. O

Note O
that O
applying O
the O
confidence O
regularizer O
to O
the O
fine O
- O
tuning O
of O
the O
student O
model O
only O
slightly O
improved O
the O
performance O
, O
suggesting O
that O
the O
observed O
gains O
in O
our O
model O
are O
only O
partially O
because O
of O
the O
calibration O
regularizer O
. O

During O
inference O
, O
as O
common O
wisdom O
, O
we O
apply O
beam O
search O
. O

* O
The O
first O
two O
authors O
contributed O
equally O
. O

Donahue O
et O
al O
. O
( O
2020 O
) O
and O
Shen O
et O
al O
. O
( O
2020 O
) O
also O
study O
blanking O
infilling O
models O
. O

T O
where O
x O
v O
is O
the O
v O
- O
th O
word O
embedding O
of O
each O
en O
- O
tity O
in O
the O
knowledge O
and O
question O
graph O
, O
a O

We O
set O
the O
Transformer B-MethodName
( I-MethodName
SA+GA I-MethodName
) I-MethodName
as O
a O
backbone O
model O
, O
and O
present O
the O
results O
in O
Table O
3 O

Moreover O
, O
compared O
with O
the O
latest O
model O
TOF B-MethodName
, O
RIKD B-MethodName
, O
Unitrans B-MethodName
, O
our O
model O
requires O
much O
lower O
computational O
costs O
for O
both O
translation O
and O
iterative O
knowledge O
distillation O
, O
meanwhile O
reaching O
superior O
performance O
. O

First O
, O
autoencoding O
models O
learn O
a O
bidirectional O
contextualized O
encoder O
for O
natural O
language O
understanding O
via O
denoising O
objectives O
( O
Devlin O
et O
al O
. O
, O
2019;Joshi O
et O
al O
. O
, O
2020;Yang O
et O
al O
. O
, O
2019;Lan O
et O
al O
. O
, O
2020;Clark O
et O
al O
. O
, O
2020 O
) O
. O

Siamese B-MethodName
Network I-MethodName
is O
originally O
introduced O
by O
( O
Bromley O
et O
al O
. O
, O
1994 O
) O
to O
treat O
signature B-TaskName
verification I-TaskName
as O
a O
matching O
problem O
. O

We O
then O
leverage O
this O
enciphered O
training O
data O
along O
with O
the O
original O
parallel O
data O
via O
multi O
- O
source O
training O
to O
improve O
neural B-TaskName
machine I-TaskName
translation I-TaskName
. O

, O
in O
a O
given O
sentence O
. O

To O
this O
end O
, O
we O
devise O
four O
key O
building O
blocks O
for O
the O
L2I B-MethodName
module O
: O
• O
Encoder O
. O

All O
the O
datasets O
used O
total O
158 O
GB O
of O
uncompressed O
texts O
, O
close O
in O
size O
to O
RoBERTa B-MethodName
's O
160 O
GB O
datasets O
. O

Notably O
, O
our O
approach O
even O
outperformed O
RoBERTa B-MethodName
- I-MethodName
PM I-MethodName
on O
two O
tasks O
and O
demonstrated O
comparable O
performances O
on O
the O
others O
. O

We O
follow O
the O
standard O
pre O
- O
processing O
steps O
described O
in O
See O
et O
al O
. O
( O
2017 O
) O
; O
Liu O
and O
Lapata O
( O
2019 O
) O
. O

The O
second O
block O
presents O
results O
of O
student O
models O
. O

Each O
GCN B-MethodName
model O
consists O
of O
two B-HyperparameterValue
propagation B-HyperparameterName
layers I-HyperparameterName
and O
a B-HyperparameterValue
sum B-HyperparameterName
pooling I-HyperparameterName
layer I-HyperparameterName
across O
the O
nodes O
in O
the O
graph O
. O

In O
Natural O
Language O
Processing O
( O
NLP O
) O
, O
the O
umbrella O
term O
attribute O
transfer O
( O
Jin O
et O
al O
. O
, O
2020b O
) O
transfer O
) O
refers O
to O
similar O
methods O
2 O
. O

Our O
experimental O
results O
show O
that O
the O
proposed O
model O
yields O
significant O
improvements O
on O
six O
target O
language O
datasets O
and O
outperforms O
the O
existing O
state O
- O
of O
- O
the O
- O
art O
approaches O
. O

We O
randomly O
sample O
spans O
of O
length O
drawn O
from O
a O
Poisson O
distribution O
with O
λ B-HyperparameterName
= O
3 B-HyperparameterValue
. O

i O
) O
We O
propose O
Hypergraph B-MethodName
Transformer I-MethodName
which O
enhances O
multi O
- O
hop O
reasoning O
ability O
by O
encoding O
high O
- O
order O
semantics O
in O
the O
form O
of O
a O
hypergraph O
and O
learning O
inter O
- O
and O
intrahigh O
- O
order O
associations O
in O
hypergraphs O
using O
the O
attention O
mechanism O
. O

We O
can O
observe O
that O
: O
1 O
) O
TAGOP B-MethodName
- I-MethodName
L2I I-MethodName
achieves O
the O
best O
performance O
among O
all O
the O
compared O
methods O
. O

( O
b O
- O
e O
) O
. O

The O
TAPT B-MethodName
approach O
additionally O
pre O
- O
trains O
an O
existing O
PLM O
before O
fine O
- O
tuning O
it O
with O
the O
training O
samples O
of O
each O
task O
. O

He O
has O
also O
worked O
on O
" O
the O
simpsons O
" O
TV O
show O
since O
" O
the O
simpsons O
movie O
" O
, O
most O
notably O
playing O
the O
roles O
of O
Captain O
Skeletor O
and O
the O
ghost O
of O
the O
same O
name O
. O

Recent O
progress O
of O
abstractive B-TaskName
summarization I-TaskName
largely O
relies O
on O
large O
pre O
- O
trained O
Transformer O
models O
( O
Raffel O
et O
al O
. O
, O
2020;Lewis O
et O
al O
. O
, O
2020;Zhang O
et O
al O
. O
, O
2020;Liu O
and O
Lapata O
, O
2019 O
; O
. O

The O
current O
state O
house O
members O
are O
: O
The O
Wyoming O
Constitution O
assigns O
certain O
powers O
to O
the O
governor O
. O

Among O
encoder O
- O
decoder O
models O
, O
BART B-MethodName
conducts O
NLU B-TaskName
tasks O
by O
feeding O
the O
same O
input O
into O
the O
encoder O
and O
decoder O
, O
and O
taking O
the O
final O
hidden O
states O
of O
the O
decoder O
. O

All O
models O
except O
ours O
show O
slightly O
lower O
performance O
on O
the O
3 B-HyperparameterValue
- I-HyperparameterValue
hop I-HyperparameterValue
graph O
than O
on O
the O
2 B-HyperparameterValue
- I-HyperparameterValue
hop I-HyperparameterValue
graph O
. O

The O
model O
architecture O
and O
detailed O
operation O
of O
hypergraph B-MethodName
attention I-MethodName
networks I-MethodName
are O
similar O
to O
that O
of O
BAN B-MethodName
. O

It O
is O
more O
accurate O
than O
ours O
around O
9.4 B-MetricValue
% I-MetricValue
in O
the O
recall B-MetricName
metric O
. O

We O
use O
the O
similarity O
- O
based O
answer O
predictor O
for O
KVQA B-DatasetName
, O
and O
MLP O
for O
the O
others O
. O

Note O
that O
if O
an O
n O
- O
gram O
appears O
in O
the O
summary O
, O
but O
not O
in O
the O
original O
document O
, O
we O
call O
it O
a O
novel B-MetricName
n I-MetricName
- I-MetricName
gram I-MetricName
. O

All O
the O
other O
baselines O
are O
of O
similar O
size O
to O
BERT B-MethodName
Large I-MethodName
. O

These O
models O
are O
pre O
- O
trained O
using O
unsupervised O
text O
- O
to O
- O
text O
objectives O
. O

We O
formulate O
them O
as O
blank B-TaskName
infilling I-TaskName
tasks O
, O
following O
( O
Schick O
and O
Schütze O
, O
2020b O
) O
. O

To O
address O
this O
issue O
, O
it O
is O
worth O
considering O
the O
operator O
relation O
in O
the O
deriving O
head O
in O
the O
future O
. O

He O
completed O
his O
NFL O
career O
with O
five O
interceptions O
. O

Finally O
, O
the O
hidden O
states O
of O
nodes O
in O
the O
given O
graph O
are O
updates O
as O
h O

Position O
1 O
1 O
2 O
3 O
4 O
5 O
5 O
5 O
5 O
3 O
3 O
Position O
2 O
0 O
0 O
0 O
0 O
0 O
1 O
2 O
3 O
1 O
2 O

Pretraining O
largescale O
language O
models O
significantly O
improves O
the O
performance O
of O
downstream O
tasks O
. O

The O
maximum B-HyperparameterName
passage I-HyperparameterName
length I-HyperparameterName
is O
464 B-HyperparameterValue
and O
the O
maximum B-HyperparameterName
question I-HyperparameterName
length I-HyperparameterName
is O
48 B-HyperparameterValue
. O

The O
statistics O
of O
TAT B-DatasetName
- I-DatasetName
HQA I-DatasetName
are O
shown O
in O
Table O
1 O
. O

Table O
6 O
shows O
our O
ablation O
analysis O
for O
GLM B-MethodName
. O

Example O
of O
such O
annotated O
belief O
- O
state O
are O
shown O
in O
Fig O
: O
1 O
. O

